Methods for Measuring, Updating, and Visualizing Factual Beliefs in
Language Models
PeterHase1,2 MonaDiab1 AsliCelikyilmaz1 XianLi1
ZornitsaKozareva1 VeselinStoyanov1 MohitBansal2 SrinivasanIyer1
1MetaAI 2UNCChapelHill
{peter, mbansal}@cs.unc.edu
{mdiab, aslic, xianl, zori, ves, sviyer}@fb.com
Abstract Whilepretrainedmodelsclearlystorefactualbe-
liefs, it is not well understood how to efficiently
Languagemodelscanmemorizeaconsiderable
editthestoredbeliefs. Modeleditingisanexciting
amountoffactualinformationduringpretrain-
recentdirectionofresearchwithseveralpractical
ingthatcanbeelicitedthroughpromptingor
usescases(Sinitsinetal.,2020;Zhuetal.,2020;
finetuning models on tasks like question an-
swering. Inthispaper,wediscussapproaches De Cao et al., 2021; Mitchell et al., 2021). For
to measuring model factual beliefs, updating LMs,theseusesincludeupdatingfactuallyinaccu-
incorrectfactualbeliefsinmodels,andvisualiz- rateoutputsandpreventingotherunwantedmodel
inggraphicalrelationshipsbetweenfactualbe-
outputs(e.g. toxicgeneratedtext)withoutexpen-
liefs. Ourmaincontributionsinclude: (1)new
sivedatacurationandretrainingefforts. Theseare
metrics for evaluating belief-updating meth-
importantapplicationsgiventhatLMs(1)struggle
odsfocusingonthelogicalconsistencyofbe-
withfuturedatawhentrainedondatafromthepast
liefs, (2) a training objective for Sequential,
Local,andGeneralizingupdates(SLAG)that (Lazaridouetal.,2021;Dhingraetal.,2021), (2)
improvestheperformanceofexistinghypernet- oftengeneratemorallyundesirabletext(Gehman
work approaches, and (3) the introduction of et al., 2020; Bender et al., 2021), and (3) simply
the belief graph, a new form of visualization giveinaccurateoutputsfortaskslikequestionan-
forlanguagemodelsthatshowsrelationships
swering(Linetal.,2021). Notably,thereisgood
betweenstoredmodelbeliefs. Ourexperiments
evidencethatscalingmodelstolargersizeswillnot
suggestthatmodelsshowonlylimitedconsis-
fixtheseparticularproblemsormayevenexacer-
tencybetweenfactualbeliefs,butupdatemeth-
batethem(Lazaridouetal.,2021;Gehmanetal.,
ods can both fix incorrect model beliefs and
greatly improve their consistency. Although 2020;Linetal.,2021).
off-the-shelfoptimizersaresurprisinglystrong Intheremainderofthispaper, wepresentnew
belief-updatingbaselines,ourlearnedoptimiz-
methodsformeasuring,updating,andvisualizing
erscanoutperformtheminmoredifficultset-
factual beliefs in LMs. We further describe each
tingsthanhavebeenconsideredinpastwork.1
ofthesethreecontributionsbelow. Figure1repre-
1 Introduction sentsthecoreideasbehindmeasuringandupdating
factual beliefs, while belief visualization is done
Pretrained language models have been shown to
viabeliefgraphs(shownlaterinFigure2).
storealargeamountoffactualinformationabout
Measuring factual beliefs. We measure the de-
theworldthatcanbeelicitedbyclozeprompting
gree to which LMs possess consistent factual be-
(Petroni et al., 2019), few-shot learning (Brown
liefsusingmodelsfinetunedonfactverificationand
et al., 2020), or finetuning models for question
questionansweringtasks. Beyondsimplychecking
answering or true/false statement classification
individualmodelresponses,wewanttoassessthe
(Roberts et al., 2020). We refer to this kind of
structural properties of model outputs: Are they
storedinformationasmodelfactualbeliefs.2
consistent under paraphrase? Are they logically
1Code is available at https://github.com/ consistent? Does changing one belief correctly
peterbhase/SLAG-Belief-Updating.
changeotherentailedbeliefs? Doesiterroneously
2We use the term factual belief rather than knowledge
as in related work (Zhu et al., 2020; De Cao et al., 2021) changeotherunrelatedbeliefs? Pastworkhasfo-
because “belief” is a weaker term than “knowledge.” In a cused primarily on consistency under paraphrase
traditionalviewofknowledgeasJustifiedTrueBelief,itis
(Elazaretal.,2021;DeCaoetal.,2021;Mitchell
moredifficulttodescribeinformationasknowledgethanasa
belief(Schwitzgebel,2019). et al., 2021). Here, we adapt data from Talmor
2714
Proceedingsofthe17thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics,pages2714–2731
May2-6,2023©2023AssociationforComputationalLinguistics
SLAG: Sequential, Local, and Generalizing Model Updates
(Main Input) A viper is a vertebrate.
(Entailed Data) A viper has a brain.
(Local Neutral Data) A viper is venemous.
(Paraphase Data) Vipers are vertebrates.
(Random Data) Chile is a country.
Figure1: RelyingonlyonaMainInputM ,wewanttoupdatealanguagemodel’sweightsinorderto(1)change
i
theoutputforM toadesiredoutputy ,(2)changetheoutputforparaphrasesofM ,(3)appropriatelychange
i i∗ i
outputsfordataE entailedbythetuple(M ,y ),and(4)avoidchangingoutputsforotherlogicallyneutraldata
i i i∗
LN ,evenifitissimilar(local)toM . ThisisdoneiterativelyforT requestedupdates.
i i
etal.(2020)tomeasureconsistencyunderentail- directededgesbetweennodesshowthatupdating
ment (including for contrapositives), and we use onebeliefchangestheother. Wediscussgraphmet-
theWikidata5mdataset(Wangetal.,2021)tocon- ricsthathelpsummarizethedependenciesbetween
struct logically neutral belief pairs for checking modelbeliefs.
thatmodelsdotreatthesebeliefsasindependent. Wesummarizeourmainconclusionsasfollows:
Updatingfactualbeliefs. WeproposeaSequen- 1. 100Mparametermodelsexhibitlimitedbelief-
∼
tial, Local, and Generalizing belief update objec- likequalities,asparaphraseconsistencyscores
tive (SLAG) that substantially improves the per- areunder70%,andmodelsshowmixedlevels
formance of the comparable KNOWLEDGEEDI- ofconsistencyunderentailment(Sec. 5.1).
TOR methodfromDeCaoetal.(2021). KNOWL- 2. Off-the-shelfoptimizersarequiteeffectiveup-
EDGEEDITOR is a learned optimizer that edits a datemethods,oftenoutperforminglearnedopti-
model’sweightstochangeitspredictiononanin- mizerswhenupdatingasinglebelief(Sec.5.2).
put while satisfying other desiderata, like consis- 3. When updating multiple beliefs in a row, per-
tency under paraphrase. Principally, we identify formancegreatlydeclinesacrossmethods,but
more difficult training data for the learned opti- SLAGcanimprovelearnedoptimizers’perfor-
mizer, and we learn to apply many small edits mancebeyondstrongbaselines(Sec. 5.2).
ratherthanonebigedit. Thesechangesmarkedly 4. Belief graphs reveal many nonsensical depen-
improve the update success rate and lower the denciesbetweenmodelbeliefs,andtheyshow
rateatwhichotherbeliefsarecorrupted. Wealso the presence of “core” model beliefs that are
findthat KNOWLEDGEEDITORalmosttotallyfails connectedtomanyotherstoredfacts(Sec. 6).
whenupdatingmultiplebeliefsinarowasopposed
2 RelatedWork
toachangingasinglebelief. However,byexplic-
itlytrainingtheoptimizertoupdatemultiplebeliefs
Measuring factual beliefs in language models.
sequentially, we recover much of the lost perfor-
Muchpastworkhasexploredhowinformationis
mance. Lastly,weadvocatethatthesemethodsbe
storedandrepresentedinpretrainedlanguagemod-
evaluated for their ability to fix false or morally
els(Rogersetal.,2020). Petronietal.(2019)pro-
undesirablemodelbeliefs,ratherthantoarbitrarily
videevidencethatLMsstorerelationalinformation
change beliefs to plausible alternatives as in past
between entities, and Roberts et al. (2020) show
work(DeCaoetal.,2021;Mitchelletal.,2021).
thatLMscanansweropen-endedquestions. Subse-
Visualizingbeliefgraphs. Weexploreanewform quentworkhasfurtherexploredhowmuchknowl-
ofvisualizationforunderstandinglanguagemod- edgeisstoredinLMs(HeinzerlingandInui,2021).
els,thebeliefgraph. Givenasetoffactualbeliefs, Most relevant to our work are studies from Tal-
weconstructbeliefgraphsbychangingeachmodel moretal.(2020)andElazaretal.(2021). Talmor
beliefandcheckingwhatotherbeliefsaresensitive etal.(2020)trainLMstoperformTrue/Falseclas-
tothosechanges. Eachbeliefbecomesanode,and sificationoffactualclaims,andtheymeasurehow
2715
beliefs correlate between entailed facts. We use modeltochangeonebeliefalsoresultsinachange
theirLeapOfThoughtdataasapartofourSLAG to the other belief, rather than there being a
objective (Eq. 1) and to measure model consis- probabilistic model specifying the relationship
tency under entailment before and after updating betweenthetwobeliefs.
beliefsinmodels. Meanwhile,Elazaretal.(2021)
3 UpdatingBeliefsinLanguageModels
measuretheconsistencyofmodelpredictionsfor
paraphrasedinputs. Weadopttheirmetricforpara-
Herewedescribetheproblemofupdatingmodel
phraseconsistencyasameasureofbelief. Inother
beliefsandourlearnedoptimizermethod. Wealso
recentwork,Kassneretal.(2021)measureconsis-
discussmetricsformeasuringfactualbeliefsbelow,
tencyunderentailmentandparaphraseforfactual
whileourBeliefGraphsarepresentedinSec. 6.
beliefwithanewsmall-scaledataset,BeliefBank.
Problem statement and metrics. We suppose
Updating factual beliefs in language models. we have a model f = p (y x) parametrized by
θ θ
|
Approachestomakingtargetedupdatestomodel θ. Foraninputx thathassomeundesiredmodel
i
beliefs vary along a few dimensions. First is outputyˆ = argmax p (y x),wewishtoobtain
i y θ |
whetherthemethodsaltermodeltrainingoroper- anewmodelθ thatproducesadesiredoutputy
∗ i∗
ateinapost-trainingsetting. Sinitsinetal.(2020) for x . This new model θ should also fulfill a
i ∗
use a meta-learning objective during training to few other desiderata. As in past work (De Cao
encourage ease of editing afterwards. A larger etal.,2021;Mitchelletal.,2021),weoperational-
family of methods perform post-training model izethesedesideratainthefollowingmetrics:
updates: Daietal.(2021)proposeahand-crafted
algorithm that edits model weights, while Zhu 1. UpdateSuccessRate(MainInput): Thepro-
et al. (2020) use projected gradient descent for portion of Main Inputs x i for which the up-
batches of points. De Cao et al. (2021) train a datedmodelgivesthedesiredoutputy i∗.
hypernetwork (learned optimizer) that processes
2. UpdateSuccessRate(Paraphrase): Thepro-
modelgradientsinordertoproduceanewmodel
portion of paraphrases of x for which the
i
that(1)givesthedesiredoutputforaninput,while
updatedmodelgivesthesamenewprediction
(2) satisfying other objectives like minimizing
asitdoesforx (averagedacrossx ).
i i
changesinpredictionsforotherdata. Mitchelletal.
(2021)focusonscalinguptheunderlyinghypernet- 3. Retain Rate (All Data): The proportion of
workarchitecture, whichisacomplementarybut theupdatedmodel’spredictionswhichareun-
orthogonalresearchdirectionthatisnotthefocus changedforalldatabesidestheMainInput.
of this paper. In a different approach, Kassner
etal.(2021)“update”modelbeliefsbyaddingin 4. ∆-Acc(AllData): Thechangeinaccuracyon
relevantinformationtotheinputattesttime. But allotherdatabesidestheMainInput.
thisapproachdoesnotchangethemodelweights
Inpractice,RetainRate(AllData)and∆-Accare
and hence does not influence model outputs
computedwithrandomsubsetsofadataset,since
on all other potentially relevant inputs. Lastly,
thesemustbecomputedaftereverybeliefupdate.
Meng et al. (2022) provide a specialized method
Weaddtwometricstothoseusedinpastwork:
focused on rank-one updates to MLP matrices in
Transformer-based LMs, but they do not address 5. Update Success Rate (Entailed Data): The
the problem of updating multiple model beliefs newmodel’saccuracyondatathatislogically
and do not measure model consistency under entailedbythenewMainInputprediction.
entailment or unintended corruption of local
6. RetainRate(LocalNeutral): Theproportion
neutralbeliefs(metrics(5)and(6)inSec. 3).
oftheupdatedmodel’spredictionswhichare
Visualizing factual beliefs in language models.
unchangedfordatathatissimilartotheMain
Wedonotknowofanypriorworkonvisualizing
Inputbutstilllogicallyneutral.
dependenciesbetweenfactualbeliefsinlanguage
models,althoughourapproachisnotablyinspired We use Update Success Rate (Entailed Data) to
byolderAImethodslikeBayesNets(Pearl,2009). measurelogicalconsistencyforanupdatedmodel,
DifferentfromBayesNets,wedrawdependencies sincechangingonebeliefentailschangesinlogi-
between two individual nodes when editing the callyentailedbeliefs. RetainRate(LocalNeutral)
2716
Dataset DataType Input Label(s)
MainInput PlayerAliKanaanplaysforwhatteam?
zsRE {SportingAlRiyadiBeirut}
Paraphrase WhatteamisAliKanaanassociatedwith?
MainInput MaryGoodhasrelation‘awardreceived’to {Garvan-OlinMedal;Arkansas
Paraphrase MaryLoweGoodhasrelation‘winnerof’to Women’sHallofFame;etc.}
Wikidata5m
LocalNeutral MaryGoodhasrelation‘educatedat’to {TheUniversityofArkansas;U
Arkansas;etc.}
MainInput Tardigradesarealsoknownasspacebears. True
FEVER
MainInput TheLionbelongstothegenusVulpes. False
MainInput Aviperisavertebrate. True
LeapOfThought
EntailedData Aviperhasabrain. True
Table1: Exampledatapointfromeachdataset,andauxiliarydatathataccompaniestheMainInput.
uses special Local Neutral data. Unlike random usingthelearnedoptimizerg ,currentLMweights
ϕ
data, Local Neutral data is guaranteed to be logi- θ,MainInputx ,currentpredictionyˆ,anddesired
i i
callyindependentoftheMainInput,whilestillbe- modeloutputy . Then,wecanpackagetheabove
i∗
ingsimilar(local)toit,whichweensurebyusing updateasθ(k+1) = θ(k)+g (x ,yˆ,y ,θ(k)),and
ϕ i i i∗
datawiththesamesubjectentity. Together,these obtainnewmodelparametersviaaloopedupdate,
sixmetricsbettercoverthecriteriaforbeliefout-
K 1
linedbyNewenandStarzak(2020). Wecompute θ = θ(k)+ − g (x ,yˆ,y ,θ(k+j))
∗ ϕ i i i∗
themetricsusingdataofthekindshowninTable1.
j=0
X
Evaluation procedure. To date, methods have = Update(x i,yˆ i,y i∗,θ(k);ϕ,K)
been evaluated on the basis of their ability to
takingK smallstepsfrominitialparametersθ(k).
change model predictions for all data. Moreover,
thedesiredlabels y n onsequenceprediction De Cao et al. (2021) use such a loop at test time;
{ i∗ }i=1 we incorporate the loop into training to align the
taskshaveeachbeenselectedfromthemodel’spre-
trainandtest-timedistributions.
dictivebeamsearch(DeCaoetal.,2021;Mitchell
et al., 2021). We propose for evaluation to focus Learnedoptimizertraining. Thetrainingobjec-
onamorevaluablebutdifficultsetting: changing tive for KNOWLEDGEEDITOR includes differen-
thepredictionsonincorrectpointstobecorrect. tiabletermscorrespondingtoUpdateSuccessfor
theMainInputandparaphrases,aswellasRetain
Sequential updates. The standard evaluation in
Rate for all other data. We also consider terms
pastworkistoupdateasinglemodelbelief,evalu-
forUpdateSuccessonentaileddataandtheLocal
atethenewmodel,thenrollbacktheupdatebefore
Neutral Retain Rate, when this is possible given
repeating the process for each test point. We ob-
availabledata. Theoverallobjectiverequiressev-
tainsequentialversionsofallmetricsbyapplyingr
eralkindsofadditionaldataforeachpoint,which
modelupdatesinarowbeforecheckingthemetrics,
wedenoteby forotherrandomdata, for
meaningtherearefloor(n/r)measurementsfora R LN
D D
localneutraldata, forentaileddata,and for
test set of n points. We consider it important to E P
D D
paraphrasesofx . Foradatapointx withdesired
evaluateasequentialsettingbecause,inpractice,it i i
predictiony ,thefullobjectiveisthen:
islikelythatmodeldeveloperswillwanttoupdate i∗
manyfactualbeliefsofatrainedmodelovertime. (ϕ;x ,yˆ,y ,θ) = λ (f (x ),y )
L
i i i∗ 1 LTask θ
∗
i i∗
Beliefupdatingmethod. Asourbasearchitecture, 1
+λ (f (x ),y )
weusetheKNOWLEDGEEDITORarchitecturefrom 2 LTask θ ∗ P i∗
P
DeCaoetal.(2021),whichisahypernetworkthat
|D | xPX∈DP
1
takes in model gradients as inputs and outputs a +λ (f (x ),y )
3 Task θ E E
L ∗
newupdatetoapplytothemodelparameters. For |DE | xE, XyE∈DE
furtherdetailsofthismethod,wereferreadersto 1
+λ KL(f (x ) f (x ))
AppendixA.Letitsufficefornowtoobservethat 4 θ ∗ LN || θ LN
LN
anewmodelisgivenasadifferentiablefunction
|D | xLNX∈DLN
1
+λ 5 KL(f θ (x R) f θ(x R)) (1)
∗ ||
θ = θ+g (x ,yˆ,y ,θ) R
∗ ϕ i i i∗ |D | xRX∈DR
2717
where Task isthelossusedtogetgradientsforf θ. BeliefConsistency
L ↑
WeusetheCrossEntropylossforbinaryclassifica-
Dataset Paraphrase Entailed Contrapos.
tionandsequence-to-sequencetasks.
LeapOfThought - 85.6(1.1) 16.5(2.7)
We optimize this objective w.r.t. ϕ using zsRE 69.5(1.1) - -
AdamW(LoshchilovandHutter,2019). Toobtain Wikidata5m 25.8(0.5) - -
update labels y n , we always use the oppo-
{ i∗ }i=1 Table2: Beliefmetricresultsacrossdatasets.
siteclassinbinaryclassification. Forsequence-to-
sequence tasks, we use the correct label when yˆ
i ParaphraseConsistency
↑
isincorrect,andwhenyˆ iscorrect,werandomly
i Dataset ModelIncorrect ModelCorrect
select another label from the training data. This
zsRE 61.39(1.33) 91.82(1.17)
choice is in contrast to De Cao et al. (2021) and
Wikidata5m 24.55(0.48) 37.20(2.06)
Mitchell et al. (2021), who use samples from the
Table3: Paraphraseconsistencybythecorrectnessof
modelbeamsearchasupdatelabelsforallpoints.
themodelpredictionontheMainInput.
SLAGobjective. Topreparetheupdatemethodfor
a sequential-update setting, we consider training fact. Wefilterthedatasothatthecontextfactsare
g ϕ to update multiple datapoints in a row. Using unique,thenshuffletheresulting14,939pointsinto
theper-datapointlossinEq. 1,weobtainourSe- train/dev/testsplitswith60/10/30%ofthedata.
quential,Local,andGeneralizing(SLAG)lossfor
InordertogetLocalNeutraldata,weconstruct
asetofr MainInputs = x ,yˆ,y r as
D { i i i∗ }i=1 (4)asequencepredictiontaskusingWikidata5m,
r arelationalknowledgebasewithover20million
LSequential(ϕ; D,θ t)= L(ϕ;x i,yˆ i,y i∗,θ t+i) (2) triplets (Wang et al., 2021). Each input consists
i=1 of an entity e and relation r, and the label is an-
X 1
whereθ t+i = Update(x i,yˆ i,y i∗,θ t+i 1;ϕ,K)are otherentitye 2 thatcompletesthetriplet. Allinputs
the model parameters obtained from− updating on comeinpairsthatsharethesameentitye 1 butuse
thefirstipointsin (startingfromθ ). Thisobjec- differentrelationswithdifferentlabels. Ingeneral,
t
D
tiveallowsustotraing ϕ toupdatemultiplebeliefs thecompletione 2 totheMainInputtriplet(e 1,r 1,
inarow. Toensuretrainingwiththisobjectiveis e 2) has no logical consequences for its paired in-
stillefficient,welimithowfarbackthroughtheup- put,(e 1,r 2,?). Thepairedpointsarealsolocalto
datehistorywebackpropagatewhencomputingthe theMainInput,i.e. theypertaintothesameentity
gradientw.r.t. ϕforeachtermintheRHSsumof e 1 astheMainInput. Weobtainfourparaphrases
Eq.2. Eachparametervectorθ dependsonϕand foreachMainInputusingdifferentaliasesforthe
t
θ . Wealwaysapplythestop-gradientfunction entityandsynonymsoftherelation. Weconstruct
t 1
to− themostrecentvectorθ topreventbackprop- atrainsetof150kpointsanddev/testsetsof10k
t 1
agatingthroughit(visualiz− edinAppendixFig. 3). pointseach. SeeAppendixBforfurtherdetails.
Thischoiceallowsourmemoryusetoremaincon- Models. We train five models with different ran-
stantinr (seeAppendixFig. 4). domseedsforeachdataset,usingRoBERTa-base
for binary tasks and BART-base for sequence-to-
4 ExperimentSetup
sequencetasks(accuraciesinAppendixTable14).
Datasets. Werunexperimentswithfourdatasets Foreachofthefivemodels,wetrainonelearned
(example data shown in Appendix Table 15). (1) optimizerusingSLAGandonewiththeobjective
FEVERincludes115,409True/Falsefactualclaims fromDeCaoetal.(2021),whichwelistasKEin
(Thorneetal.,2018). Weusetheoriginaltestset tablesbelow. Ourmodelselectioncriterionisthe
of10,444points,andwerandomlysplitthetrain- meanof: averageUpdateSuccessRate(acrossdata
ing data into 94,469 train points and 10,496 dev types),RetainRate(onlyforLocalNeutraldata),
points. (2)zsREincludes151,631questionsbased and∆-AccforAllData. WetunetheSLAGobjec-
on relational knowledge from Wikipedia, which tivetermsforeachtaskseparately(seeAppendix
werandomlyshuffleintotrain/dev/testsplitswith Table 10 for final selections; results discussed in
80/10/10% of the data (Levy et al., 2017). Tal- Appendix E). Other hyperparameters are given
moretal.(2020)introduce(3)theLeapOfThought in Appendix B. To summarize the differences
dataset, consisting of factual claims that are en- betweenSLAGandKNOWLEDGEEDITOR: (1)we
tailed to be true or false depending on a context useK =K ratherthanK =1;(2)weadopt
train test train
2718
Single-UpdateSetting UpdateSuccessRate RetainRate ∆-Acc
Dataset Method MainInput Paraphrases EntailedData LocalNeutral AllData AllData
AdamW 100(0.0) - - - 98.80(0.2) 0.22(0.1)
FEVER KE 99.98(<0.1) - - - 98.28(0.3) -0.24(0.1)
SLAG 99.99(<0.1) - - - 98.41(0.2) -0.20(0.1)
SGD 100(0.0) - 72.48(4.6) - 95.52(0.4) 1.23(0.8)
LeapOfThought KE 99.78(0.4) - 74.48(4.4) - 93.50(1.3) -1.33(1.1)
SLAG 100(0.0) - 75.50(4.3) - 94.92(1.4) -1.31(1.2)
SGD 99.36(0.1) 94.44(0.6) - - 74.73(0.4) -0.43(0.1)
zsRE KE 84.73(1.4) 89.26(1.8) - - 71.55(2.4) -2.19(0.4)
SLAG 94.29(0.4) 94.71(0.5) - - 80.48(1.3) -0.29(0.1)
SGD 98.05(0.3) 68.78(0.8) - 41.46(1.0) 58.62(0.6) -1.97(0.3)
Wikidata5m KE 74.57(2.9) 58.05(2.2) - 40.84(1.8) 53.58(2.2) -3.03(0.5)
SLAG 87.59(0.6) 80.70(0.9) - 47.85(1.0) 63.51(1.3) -1.71(0.3)
Table4: Beliefupdatemetricsforoff-the-shelfoptimizers,KNOWLEDGEEDITOR(KE)fromDeCaoetal.(2021),
and SLAG, with r = 1. Bolded numbers are the best in their group at a statistical significance threshold of
test
p<.05(orlower). OurSLAGobjectiveimprovesoverKE,butoff-the-shelfoptimizersperformsurprisinglywell.
training labels using real data labels rather than UpdateSuccessRate ∆-Acc
↑ ↑
alternativesfromthemodel’sbeamsearch;(3)our
DesiredLabel MainInput Paraphrase AllData
objectivetermsdifferfollowingtuning;and(4)we
BeamLabel 97.41(0.3) 97.03(0.4) -0.30(0.1)
canoptimizeforupdatingmultiplebeliefsinarow. CorrectLabel 94.46(0.7) 94.45(0.7) -0.24(0.1)
Baselines. Weuseoff-the-shelfoptimizersasbase-
Table5: Evaluationdifficultybydesiredmodeloutput,
lines. We tune the baseline hyperparameters sep- foralearnedoptimizertrainedwithSLAGonzsRE.
arately for each dataset, selecting among several
kinds of optimizers, learning rates, and the num- that“MainInputx isfalse.” SoforContrapositive
i
berofupdatesteps. Theselectioncriterionisthe Acc,wemeasurehowoftenthemodelfollowsthis
sameasthecriterionoutlinedforlearnedoptimiz- rule,whentheantecedentholdsofitsprediction.
ersabove. Theresultingbaselinesaresurprisingly
Beliefmeasurementresults. Table2showsthebe-
strong(seeAppendixTable12forfinalselections).
liefmetricsforeachdataset. Wefindthat 100M
∼
Hypothesis testing. We obtain 95% confidence parametermodelsshowlimitedevidenceofhaving
intervals and perform hypothesis tests via block consistentfactualbeliefs. Paraphraseconsistency
bootstrap,resamplingmodelseedsanddatapoints is69.50%( 1.09)forzsREandmuchlowerfor
±
(EfronandTibshirani,1994). Forablationexperi- Wikidata5m(25.84% 0.53). Whileentailmentac-
±
ments,werunonlyonemodelseedpercondition. curacyishighforLeapOfThought(85.63% 1.08),
±
the model is consistent under the contrapositive
5 ExperimentResults only16.51%( 2.71)ofthetime. Overall,these
±
results are not nearly as consistent as we would
5.1 DoLMshaveconsistentfactualbeliefs?
hope for factual beliefs to be. Interestingly, the
We measure Paraphrase Consistency, Entailment metrics are much higher when the model predic-
Acc, and Contrapositive Acc for finetuned task tionontheMainInputiscorrect(Table3).
models. ParaphraseCons. isthefractionofpara-
5.2 CanweupdatefactualbeliefsinLMs?
phrase pairs where the model produces the same
output(Elazaretal.,2021). EntailmentAccisthe First, we compare two evaluation procedures for
modelaccuracyondatathatisentailedbytheMain sequence prediction tasks: correcting model be-
Input. For LeapOfThought (see Table 1), “Main liefsversuschangingthemtoanalternativefrom
Input x is true” implies “entailed input x has the model’s beam search. We do so for zsRE us-
i E
label y ,” but the inverse ( A B) does not ing SLAG. Next, we compare belief update per-
E
¬ ⇒ ¬
necessarily hold. Therefore, we compute Entail- formancebetween KNOWLEDGEEDITOR,SLAG,
mentAcconlywheretheMainInputpredictionis andoff-the-shelfoptimizers. Wereportresultsin
correct. Wedoknowthatthecontrapositiveholds: single-update (r = 1) and sequential-update
test
“Entailedinputx doesnothavelabely ”implies (r = 10) settings. See Appendix Fig. 5 for
E E test
2719
Sequential-UpdateSetting UpdateSuccessRate RetainRate ∆-Acc
Dataset Method MainInput Paraphrases EntailedData LocalNeutral AllData AllData
AdamW 92.81(1.3) - - - 91.86(1.4) 1.16(0.6)
FEVER KE 74.13(1.8) - - - 39.86(0.7) -27.13(1.3)
SLAG 91.27(2.9) - - - 70.30(5.8) -11.96(4.5)
SGD 100(0.0) - 61.34(5.0) - 82.62(0.8) -4.93(1.0)
LeapOfThought KE 96.14(2.3) - 49.27(6.0) - 72.45(0.9) -15.03(1.0)
SLAG 100(0.0) - 50.46(5.5) - 74.02(1.1) -13.03(1.3)
SGD 82.71(0.6) 90.81(0.7) - - 40.49(0.6) -2.38(0.3)
zsRE KE 0.10(<0.1) 36.55(1.4) - - 0.05(<0.1) -20.98(0.7)
SLAG 87.57(0.6) 92.20(0.7) - - 47.19(0.7) -1.74(0.3)
SGD 56.82(0.8) 54.49(0.7) - 6.40(0.4) 26.37(0.6) -3.96(0.4)
Wikidata5m KE 0(0.0) 40.84(0.9) - 0(0.0) 0(0.0) -10.05(0.6)
SLAG 58.27(1.0) 65.51(0.9) - 7.36(0.5) 27.76(0.7) -3.62(0.4)
Table6: Beliefupdateresultswhenamodelissequentiallyupdatedr =10times. Here,SLAGusesr =R. On
test train
sequencepredictiontasksinthissetting,SLAGcanoutperformtheoff-the-shelfoptimizersacrossmetrics.
anablationacrossr test. Metric BeforeUpdate AfterUpdate
Correcting beliefs vs. changing factual beliefs. EntailmentAcc 58.30(5.7) 75.50(4.3)
Para.Cons(zsRE) 61.39(1.3) 94.53(0.6)
GiventheresultsinTable5,wefindthatcorrecting
Para.Cons(Wiki) 24.69(0.5) 84.56(0.9)
modeloutputsisharderthansimplychangingthem
toaplausiblealternative. UpdateSuccessrisesbya Table7: EntailmentAccandParaphraseConsistency
full2.96( 0.48;p<1e 4)pointsforMainInputs risegreatlyaftermodelupdatestoincorrectpoints.
± −
and2.58( 0.81;p<1e 4)forParaphrases,while
± −
∆-Accisvirtuallyunchanged. Thissuggeststhat
that past work has overestimated the efficacy of
zsRE and 59.87 1.09 on Wikidata5m, while En-
belief update methods for actually fixing models. ±
tailmentAccrisesby17.20 7.10points.
Henceforthweevaluatemethodsaccordingtotheir ±
abilitytoupdatemodelbeliefstobetrue.
Update method results (sequential updates).
Updatemethodresults(singleupdate). Table4 We give results for a sequential update setting
showstheresultsinasingle-updatesetting. First, (r =10)inTable6. Immediatelyweseethisisa
test
wefindthatoff-the-shelfoptimizersareveryeffec- muchmoredifficultevaluation,asmetricsaregen-
tive across the board. The baselines show Main erallyfarlowerforeachdataset. Next,weobserve
InputUpdateSuccessRatesof98%+acrosstasks thatlearnedoptimizerswithSLAG(r =10)out-
train
withcompetitiveorevenpositive∆-Accscores.3
perform baselines on sequence prediction tasks.
Whenstronglytuned,thesebaselinesoutperform On zsRE, we improve Update Success for Main
learnedoptimizersonmostmetricshere. Inputs by 4.86 ( 0.83; p=1e 4) and for Para-
± −
However,SLAGsurpassesthebaselinesinafew phrases by 1.39 ( 0.93; p=.004), with better ∆-
±
places. All Data Retain Rate on zsRE rises by Acc by 0.64 ( 0.35; p=.0005). Improvements
±
5.77points( 1.43;p<1e 4),andonWikidata5m trendinthesamedirectionforWikidata5mandare
± −
ParaphraseUpdateSuccessrisesby11.92( 1.20; allstatisticallysignificantexceptforthegainin∆-
±
p<1e 4) and the Local Neutral Retain Rate by Acc. ThejumponParaphrasesinparticularislarge
−
6.40 ( 1.41; p<1e 4). SLAG also greatly im- (11.02 1.17;p<1e 4). Incomparison,usingthe
± − ± −
provesoverKEforsequencepredictiontasks. KNOWLEDGEEDITOR trainingobjectiveleadsto
Interestingly, we observe that belief updates drasticdropsinperformance.
greatlyimproveparaphraseconsistencyandentail-
mentaccuracy(SLAGresultsinTable7). Updates Learned optimizers still struggle compared to
improveParaphraseconsistencyby33.14 1.46on baselines on binary datasets. Here, AdamW and
±
SGD achieve high update update success with
3Positive∆-Accvaluesarepossiblyduetodistribution much better ∆-Acc scores, by 13.12 ( 4.51;
shiftinthetestsplit. InFEVER,forinstance,thetrainand ±
p=1e 4) on FEVER and 8.16 ( 1.63; p=1e 4)
devdataare73%True,whiletestdatais50%True. Onthe − ± −
devsplit,AdamWachievesanegative∆-Acc,-0.18( 0.11). onLeapOfThought.
±
2720
Shane McMahon officially
The Chrysler Building was retired on the first day of
always the world's shortest Middle-earth is a real place. 2010.
building. [y: false] [y: false]
[y: false]
Despicable Me 2 was written
There are no musical or creative by Cinco Paul.
works in existence that have [y: true]
been created by Phillip Glass.
[y: false]
Hot Right Now is mistakenly
attributed to DJ Fresh.
[y: false]
The Daily Show is incapable
of focusing on recent news Hot Right Now is from Nextlevelism.
stories. [y: true] Bessie Smith died on April
[y: false] 26, 1937.
[y: false]
Figure2: Anon-randomsubgraphofthebeliefgraphforamodeltrainedonFEVER.Directededgesfromuto
vindicatethatchangingthemodelbeliefinucausesthebeliefinvtochange. Theground-truthlabelisgivenin
bracketsforeachpoint,andnodecolorshowsthemodel’saccuracybeforeanyupdates(green=correct).
6 BeliefGraphs Dataset
Metric FEVER LeapOfThought
We now construct belief graphs to better under-
#Nodes 10,444 8,642
standtheconnectionsbetweenmodelbeliefs. We
%Edgeless 0.0 0.0
formagraphfromasetofdatapointsbyupdating #EdgesTotal 1.88m 9.71m
each prediction and checking what other predic- #InEdges(95thperc.) 1,088 5,347
#OutEdges(95thperc.) 390 3,087
tions change. We represent each datapoint as its
%Update-Transitivity 66.64 24.38*
own node in a belief graph. Whenever updating
a datapoint u changes the prediction for point v, Table8: Beliefgraphsummarystatistics. *Wecompute
Update-TransitivityforLeapOfThoughtwithn=4000
we draw a directed edge from u to v. Following
pointsduetocomputationalcost.
Sec. 5.2,weuseoff-the-shelfoptimizerstochange
themodeloutputtotheoppositeofitsoriginalpre-
dictionforeverydatapoint. ForFEVERweobtain 61.51 1.33, while for Local Neutral data it is a
±
agraphof10,444nodes,andforLeapOfThought full15.66pointslower.
weobtainagraphwith8642nodes,whichisdou- Wehighlightafewsummarystatisticsherefrom
blethetestsetsizebecauseweincludebothMain Table 8 for a broader view of the graphs. First,
InputsandEntailedDataastheirownnodes. %Edgelessistheproportionofnodeswhichhave
WevisualizepartofabeliefgraphinFig. 2. This noinoroutedges. Sincethisis0forbothdatasets,
figureshowsanon-randomsubgraphintendedto every belief can be changed by editing the right
givearepresentativeviewofthedata(wegivethree belief. #InEdgesisthenumberofinedgesatthe
randomsubgraphsinAppendixE).Oninspection, 95thpercentile,meaning5%ofbeliefshavemorein
we do not see any clear reasons for beliefs being edgesthanthisvalue,andthesameholdsof#Out
connectedornotconnected. Wecometothesame Edges. Thesevaluesgrowtoaratherlargefraction
conclusionlookingatotherrandomsubgraphs(see of the overall datasets, suggesting that (1) some
AppendixFigures9,10,and11). Whetherornot beliefsaresensitivetochangesinalargefraction
changingonebeliefchangesanotherappearsessen- of all beliefs, and (2) some beliefs are influential
tiallyrandom,whichisanovelnegativeresulton tohundredsofotherbeliefswhenchanged. Inter-
the organization of internal model beliefs. How- estingly,thisimpliesthatsomefactualbeliefsare
ever,wedoobservesomeaggregatetrends. First, “core”beliefsinthemodel,suchthatchangingthese
it appears that incorrect predictions are the most individual beliefs requires greatly changing the
sensitivetomodelupdates. OnFEVER,incorrect overalldistributionoffactualbeliefsinthemodel.
beliefschangearound4%ofthetimewhenother Lastly, % Update-Transitivity represents the an-
beliefs are updated, while correct beliefs change swertothequestion: ifupdatingbeliefAchanges
only2.5%ofthetime. Second,wefindthatLocal belief B, and updating belief B changes belief C,
Neutralbeliefsaremuchhardertoavoidchanging whatproportionofthetimedoesupdatingAchange
thansimplyrandomdata. OnWikidata5m(Table C?Forthesedatasets,alogicallyconsistentmodel
4),weobservethattheRetainRateonAllDatais shoulddisplay100%Update-Transitivity(seeAp-
2721
pendix D for a caveat on this metric). We find ourresults,itwillalsobevaluableforfuturework
that belief updates often yield intransitive results toscaletolargermodelswhichmayexhibitmore
forbothdatasets,anothernegativeresultforbelief consistentfactualbeliefs. Thatsaid,webelieveour
consistency. Itwouldbevaluableforfuturework contributions are still valuable since our metrics,
toextendthisanalysisofbeliefgraphstoexplore objectives,andbeliefvisualizationmethodcanall
whylanguagemodelsdemonstratethesesurprising be easily applied to larger models, and hypernet-
connectionsandinconsistenciesbetweenbeliefs. works have already been extended to work with
largermodels(Mitchelletal.,2021).
7 Conclusion
(4)Currently,modelsmayhaveseeminglyran-
dom interdependencies between factual beliefs,
Wefirstmeasurethepresenceofconsistentfactual
limiting the insights available from our belief
beliefsinlanguagemodels,andweproposetoeval-
graphs. We believe that as models become more
uatelearnedoptimizersforwhethertheycanmake
consistentandmoretruthful,theusefulnessofbe-
model beliefs more truthful. Then we show that
liefgraphsasatoolforunderstandingconnections
ourSLAGobjectivegreatlyimproveslearnedop-
betweenbeliefswillincrease.
timizerperformance,outperformingoff-the-shelf
(5) Lastly, we do not currently account for un-
optimizerswhenupdatingmultiplemodelbeliefs
certaintyinfactualbeliefs. Thedataweusecomes
in a row. Finally, we introduce belief graphs to
intheformofdeclarativestatementsandanswers
visualizeconnectionsbetweenmodelbeliefs. We
to questions which take what is called a veridi-
findthatmodelbeliefsarehighlyinterconnected,
calstancetowardaproposition,displayinga“full
withsome“core”beliefsinfluencinghundredsof
commitment”tothatproposition’struthfulness(Gi-
otherbeliefs.
annakidou and Mari, 2020). It will be valuable
EthicsStatement for future work to explore two dimensions of un-
certaintyinbeliefs: (1)expressionofuncertainty
Beliefupdatemethodsmaybeusedtoeithercor-
in language, via partial or trivial commitments
rectundesiredbeliefsorinduceproblematicbeliefs
(like“XmightbeY”)and(2)expressionofuncer-
in LMs, and it is not clear whether these capabil-
tainty mathematically, via probabilities assigned
ities could be separated. We propose to evaluate
byamodeltoutterancesorTrue/Falsevalues. In
methodsonlyonthebasisoftheirabilitytocorrect
thispaperwetreatabeliefas“updated”whenthe
mistakenmodelbeliefs,butthemalicioususecase
modeloutputchanges,butthisignoresanyunder-
remains. Weareuncertainabouthowabadbelief
lyingchangeinthedistributionp (y x)thatcould
θ
would influence the general behavior of a model |
occurevenifitsmodedoesnotchange.
(e.g. answerstomanyquestions),butitispossible
thatabeliefupdatemethodcouldinstillbadbeliefs
in a capable LM with far-reaching implications References
formodelbehavior. Thatsaid,wehopethatthese
EmilyMBender, TimnitGebru, AngelinaMcMillan-
methodswillinsteadbeusedtoupdateundesirable
Major, and Shmargaret Shmitchell. 2021. On the
moral,social,andfactualbeliefsinlargeLMs. dangersofstochasticparrots: Canlanguagemodels
betoobig? InProceedingsofthe2021ACMConfer-
Limitations enceonFairness,Accountability,andTransparency,
pages610–623.
Wenoteafewlimitationsofourwork:
TomB.Brown,BenjaminMann,NickRyder,Melanie
(1) Neural learned optimizers require large
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
amountsoftrainingdatatosuccessfullyeditevena
Neelakantan,PranavShyam,GirishSastry,Amanda
fewmodelbeliefsattesttime. Askell, Sandhini Agarwal, Ariel Herbert-Voss,
(2) Our experiments are limited by available Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
datasetsintermsofbothmetricswecancalculate
ClemensWinter,ChristopherHesse,MarkChen,Eric
andobjectiveswecanoptimizefor. Thereisalso
Sigler,MateuszLitwin,ScottGray,BenjaminChess,
somenoiseineachdatasetwhichwecataloguein Jack Clark, Christopher Berner, Sam McCandlish,
AppendixC. Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
(3) We conduct experiments with 100M pa-
∼ NeurIPS.
rametermodelsasinpastwork. Whilethebelief-
updating problem is still clearly unsolved given DamaiDai,LiDong,YaruHao,ZhifangSui,andFuru
2722
Wei.2021. Knowledgeneuronsinpretrainedtrans- KevinMeng,DavidBau,AlexAndonian,andYonatan
formers. arXivpreprintarXiv:2104.08696. Belinkov.2022. Locatingandeditingfactualknowl-
edgeingpt. arXivpreprintarXiv:2202.05262.
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021.
Editing factual knowledge in language models. In EricMitchell,CharlesLin,AntoineBosselut,Chelsea
EMNLP,pages6491–6506.AssociationforCompu- Finn,andChristopherDManning.2021. Fastmodel
tationalLinguistics. editingatscale. arXivpreprintarXiv:2110.11309.
Bhuwan Dhingra, Jeremy R Cole, Julian Martin AlbertNewenandTobiasStarzak.2020. Howtoascribe
Eisenschlos, Daniel Gillick, Jacob Eisenstein, and beliefstoanimals. Mind&Language.
WilliamWCohen.2021. Time-awarelanguagemod-
els as temporal knowledge bases. arXiv preprint J.Pearl.2009. Causality. CambridgeUniversityPress.
arXiv:2106.15110.
FabioPetroni,AleksandraPiktus,AngelaFan,Patrick
BradleyEfronandRobertJTibshirani.1994. AnIntro- Lewis,MajidYazdani,NicolaDeCao,JamesThorne,
ductiontotheBootstrap. CRCpress. YacineJernite,VladimirKarpukhin,JeanMaillard,
VassilisPlachouras,TimRocktäschel,andSebastian
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi- Riedel. 2021. KILT: a benchmark for knowledge
lashaRavichander, EduardHovy, HinrichSchütze, intensivelanguagetasks. InProceedingsofthe2021
andYoavGoldberg.2021. Measuringandimproving Conference of the North American Chapter of the
consistencyinpretrainedlanguagemodels. Transac- AssociationforComputationalLinguistics: Human
tionsoftheAssociationforComputationalLinguis- LanguageTechnologies,pages2523–2544,Online.
tics,9:1012–1031. AssociationforComputationalLinguistics.
Samuel Gehman, Suchin Gururangan, Maarten Sap, Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Yejin Choi, and Noah A Smith. 2020. Realtoxici- Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
typrompts: Evaluatingneuraltoxicdegenerationin AlexanderMiller.2019. Languagemodelsasknowl-
languagemodels. InFindingsofEMNLP. edge bases? In Proceedings of the 2019 Confer-
enceonEmpiricalMethodsinNaturalLanguagePro-
AnastasiaGiannakidouandAldaMari.2020. Alinguis- cessingandthe9thInternationalJointConference
ticframeworkforknowledge,belief,andveridicality onNaturalLanguageProcessing(EMNLP-IJCNLP),
judgement. HAL. pages2463–2473,HongKong,China.Association
forComputationalLinguistics.
Benjamin Heinzerling and Kentaro Inui. 2021. Lan-
guagemodelsasknowledgebases: Onentityrepre- AdamRoberts,ColinRaffel,andNoamShazeer.2020.
sentations,storagecapacity,andparaphrasedqueries. Howmuchknowledgecanyoupackintotheparam-
InProceedingsofthe16thConferenceoftheEuro- eters of a language model? In Proceedings of the
peanChapteroftheAssociationforComputational 2020ConferenceonEmpiricalMethodsinNatural
Linguistics: MainVolume,pages1772–1791,Online. LanguageProcessing(EMNLP),pages5418–5426,
AssociationforComputationalLinguistics. Online.AssociationforComputationalLinguistics.
Nora Kassner, Oyvind Tafjord, Hinrich Schütze, and Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
PeterClark.2021. Beliefbank: Addingmemorytoa 2020. AprimerinBERTology:Whatweknowabout
pre-trainedlanguagemodelforasystematicnotion howBERTworks. TransactionsoftheAssociation
ofbelief. arXivpreprintarXiv:2109.14723. forComputationalLinguistics,8:842–866.
Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gri- EricSchwitzgebel.2019. Belief. InEdwardN.Zalta,
bovskaya, Devang Agrawal, Adam Liska, Tayfun editor,TheStanfordEncyclopediaofPhilosophy,Fall
Terzi,MaiGimenez,CypriendeMassond’Autume, 2019edition.MetaphysicsResearchLab,Stanford
SebastianRuder,DaniYogatama,etal.2021. Mind University.
thegap: Assessingtemporalgeneralizationinneural
languagemodels. InNeurIPS. AntonSinitsin,VsevolodPlokhotnyuk,DmitriyPyrkin,
SergeiPopov,andArtemBabenko.2020. Editable
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke neuralnetworks. InICLR.
Zettlemoyer.2017. Zero-shotrelationextractionvia
readingcomprehension. InProceedingsofthe21st AlonTalmor,OyvindTafjord,PeterClark,YoavGold-
Conference on Computational Natural Language berg,andJonathanBerant.2020. Leap-of-thought:
Learning(CoNLL2017),pages333–342,Vancouver, Teachingpre-trainedmodelstosystematicallyreason
Canada.AssociationforComputationalLinguistics. overimplicitknowledge. InNeurIPS.
StephanieLin,JacobHilton,andOwainEvans.2021. James Thorne, Andreas Vlachos, Christos
Truthfulqa: Measuring how models mimic human Christodoulopoulos, and Arpit Mittal. 2018.
falsehoods. arXivpreprintarXiv:2109.07958. FEVER: a large-scale dataset for fact extraction
and VERification. In Proceedings of the 2018
Ilya Loshchilov and Frank Hutter. 2019. Decoupled Conference of the North American Chapter of
weightdecayregularization. InICLR. the Association for Computational Linguistics:
2723
Human Language Technologies, Volume 1 (Long SymbolGlossary
Papers), pages 809–819, New Orleans, Louisiana.
f LanguageModel
AssociationforComputationalLinguistics. θ
g Learnedoptimizer
ϕ
XiaozhiWang,TianyuGao,ZhaochengZhu,Zhengyan x i MainInput
Zhang,ZhiyuanLiu,JuanziLi,andJianTang.2021.
yˆ
i
LMoutputonx
i
Kepler: Aunifiedmodelforknowledgeembedding
y i∗ Desiredoutput
and pre-trained language representation. Transac- ∇θ
L(x i,y i∗) GradientofLM
tionsoftheAssociationforComputationalLinguis-
Update(x i,yˆ i,y i∗,θ) UpdateoneLMbelief
tics,9:176–194.
L(ϕ;x i,yˆ i,y i∗,θ) Beliefupdateobjectiveforx
i
LSequential(ϕ; D,θ t) Sequentialobjective(SLAG)
ChenZhu,AnkitSinghRawat,ManzilZaheer,Srinadh K #gradientstepsinUpdate()
·
r #beliefsupdatedin
Bhojanapalli,DaliangLi,FelixYu,andSanjivKumar. LSequential
2020. Modifyingmemoriesintransformermodels.
Table9: Symboldescriptionsforthelearnedoptimizer.
arXivpreprintarXiv:2012.00363.
tasks,weusethecorrectlabelwhenyˆ isincorrect,
i
A LearnedOptimizerDetails andwhenyˆ i iscorrect,werandomlyselectanother
labelfromthetrainingdata. Thischoiceisincon-
Architecture. KNOWLEDGEEDITOR isalearned trast to De Cao et al. (2021) and Mitchell et al.
optimizerg : Θ Θthatproduces (2021), who use samples from the model beam
X ×Y ×Y × →
new model weights by applying an adjusted gra- searchasupdatelabelsforallpoints.
dient step to a model. For reference, we give a
glossaryofsymbolsusedhereinTable9. Forad- B AdditionalTrainingDetails
ditionaldetailsbeyondwhatispresentedhere,we
B.1 ComputeCosts.
referreaderstoDeCaoetal.(2021).
Atahighlevel,g ϕ firstencodesaninputx i and Learnedoptimizermemory. Thehypernetwork
requestedpredictionchangeintoavectorh, then has92mtrainableparametersforRoBERTa-base
processes h into two low-rank matrices A and B (which is 125m parameters), and 105m param-
thatareusedtotransformthemodelgradientonx i, eters for BART-base (which is 139m parame-
∇θ L(x i,y i∗). ForTransformermodels,themethod ters). To increase training efficiency, we limit
editsonlyattentionandfeed-forwardweights,so howfarintothetaskmodelhistorywebackprop-
allmodelgradientsmatchtheshapeofanassoci- agate. As shown in Fig. 3, when backpropagat-
atedweightmatrixofshaped 1 ×d 2. Formally,a ing through task model parameters θ t = θ t 1 +
newmodelθ ∗isobtainedusingalearnedoptimizer Update(x i,yˆ i,y i∗,θ
t
1;ϕ),wecontinuebackp− rop-
g ϕ asfollows: agating through Up− date(x i,yˆ i,y i∗,θ t 1) but not
−
θ ,whichisalsodependentonϕ. Thatis,weap-
t 1
h = LSTM([x;yˆ;y ∗]) pl−
yastop-gradientfunctiontoθ . Thisway,we
t 1
{u,v,γ,δ
}
= {MLP i(h) }4
i=1
computethederivative ∇ϕUpdate− (x i,yˆ i,y i∗,θ t;ϕ).
A = softmax(u)vT onlyonceforeacht,ratherthanrecomputingthese
gradients for all subsequent time steps. These
B = softmax(γ)δT
choicesallowthememoryuseofourtrainingalgo-
η = σ(MLP(h)) rithmtoremainconstantinr. Wemakethesame
θ = θ+η(A (x ,y )+B) choiceforourK loopedstepsinasingleapplica-
∗ ◦∇θ
L
i i∗
tionoftheUpdatefunction,sothegradientforthe
whereϕconsistsofallLSTMandMLPparameters. updateatstepkdependsonlyong (x ,yˆ,y ,θ(k))
ϕ i i i∗
Training Algorithm. The learned optimizer ob- andnotθ(k −1). SeeFig. 4foragraphofmemory
jectiveisoptimizedw.r.t. ϕwithAdamWthrough usedependingonr andk.
astandardprocedureofrandomlysamplingmini- Experiment runtimes. We now give runtimes
batcheswithoutreplacement(LoshchilovandHut- for experiments in the paper. Building the belief
ter, 2019). Within each batch, one datapoint is graphs takes 25 hours for FEVER (n = 10,444)
randomly selected as the Main Input, and the re- and 17.5 hours for LeapOfThought (n = 8642)
mainingpointsareusedas . Toobtainupdate onanNVIDIARTX2080GPU.Computingsum-
R
D
labels y n , we always use the opposite class marystatisticsforgraphstakes3hoursonFEVER
{ i∗ }i=1
inbinaryclassification. Forsequence-to-sequence and 3 hours for LeapOfThought for statistics be-
2724
Sequential Backprop Graph
Task Model
Optimizer Backprop
Stop Gradient
Figure3: Thebackpropagationgraphforsequentialmodelupdates.
sides Update-Transitivity. We compute Update- Dataset r K Objective
test
Transitivity for LeapOfThought with a subset of
1 5 Main
FEVER
4000points,whichtakes45hours. 10 1 Main
All other experiments are run on a NVIDIA 1 5 Main
LeapOfThought
V10032GBGPU.Trainingthetaskmodelstakes 10 1 Main
7 minutes for LeapOfThought, 45 minutes for 1 5 Main
zsRE
10 5 Main
FEVER,4hoursforzsRE,and10hoursforWiki-
data5m. Trainingthelearnedoptimizerwithr = 1 1 5 Main+Para
Wikidata5m
10 5 Main+Para
takes 2.3 hours for LeapOfThought, 5 hours for
FEVER, 9.5 hours for zsRE, and 16 hours for Table10: Finalhyperparametersandobjectivetermsof
Wikidata5m. Trainingthelearnedoptimizerwith thelearnedoptimizerforeachtask.
r = 10takes53minutesforLeapOfThought,2.9
hoursforFEVER,7hoursforzsRE,and12.5hours
LeapOfThought, which we train for 10 epochs
forWikidata5m. Computingupdatestatisticswith
givenitssmallersize. Thelearnedoptimizersare
theoff-the-shelfoptimizerswithr = 1takes4min-
also selected basedon dev set performance, with
utesforLeapOfThought,30minutesforFEVER,
checkpointingaftereachtrainingepoch. Theirse-
2.3hoursforzsRE,and3.9hoursforWikidata5m.
lectioncriterionisarawaverageofUpdateSuccess
With r = 10, the baselines require 1 minute for
Rate(averagedovereachkindofdata),RetainRate
LeapOfThought,15minutesforFEVER,54min-
(Local Neutral) and ∆-Acc, with terms dropped
utesforzsRE,and1.8hoursforWikidata5m. Total
whentheycannotbecomputedgiventheavailable
runtimesforeachexperimentshouldtakeintoac-
data. Note that dev epochs with zsRE and Wiki-
count multiple conditions and multiple seeds of
data5marefairlyslow,soinordertospeedupour
eachmodelbeingrun.
experimentswecomputedevepochswithasubset
B.2 HyperparametersandObjectiveTerms. of4000devpoints.
Learnedoptimizer. Wegivethefinalhyperparam-
Traininghyperparameters. WefitourRoBERTa-
eterandobjectivetermsusedineachexperimentin
base and BART-base task models to their respec-
Table10. Ourobjectiveablationisgivenin17,and
tivedatasetswiththefollowinghyperparameters:
we select the best performing condition for each
We train for 10 epochs on the binary tasks, and
datasetaccordingtodevsetperformance,usingthe
20forthesequence-to-sequencetasks. Whenpre-
same selection criterion outlined previously. We
dicting with BART-base, we use a beam search
keep all weight coefficients λ equal rather than
withwidth5. Ineachcase,weuseAdamWfrom i
tuning them. Main refers to the first term in Eq.
torch.optimwithaLRof1e-5andweightde-
1, plus the KL term with random data. We use
cay of 1e-4. We select the best model according
K 5 for all experiments. For results across
to the best dev set accuracy, checkpointing after train ≤
K valuesonzsRE,seeFig. 8.
each training epoch. The learned optimizers are
optimized with AdamW, using a learning rate of Baselineupdatemethod. Wetuneabaselineoff-
3e-4andweightdecayof0. Wetrainthelearned the-shelfoptimizerseparatelyforeachdataset,us-
optimizerfor5epochsoneachdatasetexceptfor ing r = 1. Our performance criterion is the
test
2725
Relation %TestData tailsremain. Somerelationsareone-to-many,and
thereforeweaccumulatevalidcompletingentities
PlaceofBirth 11.00
AwardReceived 11.00 from the data as possible answers; later we com-
CauseofDeath 5.66
puteaccuracyasanexactmatchwithanypossible
PlaceofDeath 11.00
PlaceofBurial 8.33 answer. All10relationsappearineachsplitofthe
EducatedAt 11.00 data. Only 33.80% and 37.18% of the entities in
Child 11.00
thedevandtestsplitsareseeninthetrainingdata,
Occupation 11.00
Spouse 11.00 thoughwedonotfindthatmodelsperformbetter
Sibling 9.01 onentitiesseenintraining.
Table11: Wikidatarelationsandtheirproportionofthe
B.4 LeapOfThoughtAdditionalDetails
testdata.
TheLeapOfThoughtdatasetconsistsofafactanda
Dataset Optimizer LR Num.Steps
claimforeachdatapoint,wherethetruthofthefact
FEVER AdamW 1e-6 100 impliesthattheclaimhaslabely (True/False). All
i
LeapOfThought SGD 1e-2 100
of the facts in the data are true, while half of the
zsRE SGD 1e-1 10
Wikidata5m SGD 1e-1 10 claims are true and half are false. When training
thelearnedoptimizer,wetreatthethefactsasthe
Table12: Finalhyperparametersofthebaselineupdate
Main Input when training the learned optimizer
methodforeachtask.
and claims as entailed data. When training the
True/Falseclassifier,wefittotheclaims,forwhich
sameaswithlearnedoptimizers,arawaverageof
test accuracy is 83.65 ( 1.05). This seems to
UpdateSuccessRate(averagedovereachkindof ±
generalizewelltothefacts,astestaccuracyhereis
data),RetainRate(LocalNeutral)and∆-Acc. The
93.66( 0.87),althoughasthelowcontrapositive
gridsearchisoverthefollowingparameters: The ±
accuracysuggests(Table3),themodelseemstobe
off-the-shelfoptimizersarefromtorch.optim
toopronetopredictingtrueforthisdata.
andinclude{AdamW,SGD,andRMSProp}with
SinceveryfewoftheMainInputsarepredicted
default arguments (except for the learning rate).
as false, we run into a small dilemma when fit-
We consider a number of maximum steps in {5,
ting the learned optimizer with the use of the en-
10, 100}. The learning rates we consider depend
taileddataobjectiveterm. Theentailmentbetween
ontheoptimizer: {1e-4,1e-5,1e-6}forAdamW,
factandclaimonlyholdswhenthefactistrue,so
{1e-4,1e-5,1e-6}forRMSProp,and{1e-1,1e-2,
we can only compute the objective when updat-
1e-3} for SGD. The LR ranges were selected af-
ingapointfromfalsetotrue. Thisendsupbeing
ter some initial manual exploration of the space.
lessthan10%ofthetrainingdata. Weultimately
OurfinalhyperparametervaluesareshowninTa-
choose to oversample points that fit this descrip-
ble 12 for each dataset. For comparison, De Cao
tionduringtrainingofthelearnedoptimizer,which
etal.(2021)useRMSPropwith100updatesteps.
allowsthelearnedoptimizertofullyfittotheen-
TheLRforzsREandWikidata5mmayseemquite
taileddata. Alsonotethatduringlearnedoptimizer
high,butthisistheconditionthatactuallydoesthe
training,weincludeEntailedDatafromotherdata
leastdamagetothemodel’saccuracyonotherdata,
pointsbesidestheMainInputintheKLterminEq.
∆-Acc. The baseline optimizes all of the train-
1,andwemeasure∆-AccusingbothMainInputs
ableparametersinthelanguagemodel,unlikethe
andEntailedData.
learnedoptimizerwhichoptimizesonlyattention
andfeedforwardweightsforpurposesofparameter C DatasetSourcesandNoise
efficiency.
Herewegivesourcesandlicensesforeachdataset,
and we document some shortcomings of each
B.3 Wikidata5mAdditionalDetails.
dataset,withreferencetoexamplesinTable15.
WeconstructfourparaphrasesperMainInputby
Datasetsourcesandlicenses. FEVERandzsRE
selectingfromasetofalternativephrasingsforthe
areavailablethroughtheKILT4 resourceandare
entityandrelationintheMainInput. Thesyntax
foreachparaphrasefollowsthesamesimpletem-
4https://github.com/
facebookresearch/KILT/?fbclid=
plateastheMainInput,incontrasttozsREwhere
IwAR2WiFkl-7KLIQAoNI9bJgBVKWgsAQEDV342vV5_
syntaxdiffersbetweenparaphrases. Acouplede- PcsKA881vpuXaELKBz0
2726
K Memory Usage byr
30
20
10
0
1 2 4 6 8 10
K r
Figure 4: Training memory usage in terms of K and r hyperparameters in our implementation, for a learned
optimizertrainedforaBART-basemodelonzsRE,usingabatchsizeof16. Forcomparison,theorangedashed
lineshowsthememoryuseoftrainingtheBART-basemodelonzsRE,usingthesamebatchsize. Ouruseofthe
stop-gradientfunctionlimitsthegrowthofruntimeandmemoryw.r.t. bothK andr. Byaccumulatinggradients
acrosspoints,memoryw.r.t. riskeptconstant. ThesametrickcouldbeappliedtotheK loopedgradientsteps
insidetheUpdatefunction,atthetrade-offofbackpropagatingK timesperpointratherthanonetime.
Ours DeCaoetal.(2021) Mitchelletal.(2021)
UpdateSuccessRate(MainInput) Successrate Editsuccess
UpdateSuccessRate(Paraphrase) Equivalenceaccuracy Editsuccess
UpdateSuccessRate(EntailedData) - -
RetainRate(LocalNeutral) - -
RetainRate(AllData) Retainaccuracy -
∆-Acc(AllData) Performancedeterioration Drawdown
Table13: Aglossaryoftermsusedinworkonmodelupdatemethods. Notemetricsarenotalwayscalculated
inexactlythesameway. Forinstance,Performancedeteriorationisaratioinaccuraciesratherthandifferencein
accuracies,andeditsuccessfromMitchelletal.(2021)combinestwometricsinourcase. Theperformancemetric
inZhuetal.(2020)isanaverageofUpdateSuccessRate(MainInput)and∆-Acc.
available under the MIT license (Petroni et al., LeapOfThought. Many examples use an “is a”
2021). LeapOfThought data can be constructed relation,producingsentenceslike“Asunlightisa
throughtheiravailablecode5 andisalsoavailable good health.” This could bemore false thantrue,
undertheMITlicense. ThesourcedataforWiki- butit’safairlynonsensicalstatementtobeginwith.
data5m data can be downloaded through the KE- Therearealsoothernonsensicalorvagueexamples
PLER6 code repository (Wang et al., 2021) and inthedata: ”Afriaristheoppositeofmineral”is
is available under the MIT license. Use of each labeledFalse. “Adetectivedesiresequalopportu-
datasetisinaccordancewiththeirintendedlicensed nity.” islabeledTrue. Itisnotimmediatelyclear
uses. ThezsREandWikidata5mdatasetsdorefer whatconditionswouldmakethesestatementstrue
topeoplebynameastheyreferencepublicfigures orfalse.
onWikipedia. AlldatasetsareinEnglish.
zsRE. Some questions invoke potentially one-to-
FEVER. Some claims are slightly vague or am-
many or temporally dependent relations, though
biguous when taken on their own. For instance
there is only one ground-truth answer per ques-
“DougDuceywastheCEOofColdStoneCream-
tion in this dataset. For instance, a paraphrase of
ery and offered many opportunities to new hires”
thequestionaboutGiffordPinchotinTable15is:
is rated True, though this will depend heavily on
”What disease did Gifford Pinchot have?” A per-
whatonethinks“manyopportunities”means. Sim-
son might have had many diseases over their life
ilar whether or not “L.A. Guns is a tattoo shop”
whichcouldallbevalidresponses. Theansweris
dependsonwhich“L.A.Guns”oneisreferringto,
especiallyambiguousforspatialrelations,wherea
thetattooshopormetalband. Ofcourse,thisisa
validanswermightrefertoacity,region,country,
generic issue of language, and not unique to this
province,orcontinent.
dataset. Someinputsseemtobeamatterofperson
opinion: “Los Angeles is known for its food” is Wikidata. Aliases sometimes vary greatly even
ratedFalse. as they refer to the same person, or they are sim-
ply noisy. For example, as shown in Table 15,
5https://github.com/alontalmor/
“SusunW” appears in an entity name, but this is
LeapOfThought
6https://github.com/THU-KEG/KEPLER actuallyausernameofsomeonewhocontributed
2727
)BG(desU
yromeM
Dataset Model Acc ParaphraseCons EntailmentAcc ContrapositiveAcc
↑ ↑ ↑
FEVER RoBERTa-base 78.29(0.86) - - -
LeapOfThought RoBERTa-base 93.66(0.87) - 85.63(1.08) 16.51(2.71)
zsRE BART-base 21.01(0.64) 69.50(1.09) - -
Wikidata5m BART-base 10.21(0.59) 25.84(0.53) - -
Table14: Modelaccuracyandbeliefmetricresultsandforfourdatasets.
Dataset DataType Input Label(s)
MainInput WhatdidGiffordPinchotdieof?
{Leukemia}
Paraphrase HowdidGiffordPinchotdie?
zsRE
MainInput PlayerAliKanaanplaysforwhatteam?
{SportingAlRiyadiBeirut}
Paraphrase WhatteamisAliKanaanassociatedwith?
MainInput MargaritaNolascoArmashasrelation‘place
{Orizaba,Veracruz;Orizaba;
ofbirth’to
etc.}
Paraphrase SusunW/Margarita Nolasco Armas has rela-
tion‘bornat’to
LocalNeutral MargaritaNolascoArmashasrelation‘place MexicoCity;CiudaddeMexico;
Wikidata5m
ofdeath’to etc.
MainInput MaryGoodhasrelation‘awardreceived’to {Garvan-OlinMedal;Arkansas
Paraphrase MaryLoweGoodhasrelation‘winnerof’to Women’sHallofFame;etc.}
LocalNeutral MaryGoodhasrelation‘educatedat’to {TheUniversityofArkansas;U
Arkansas;etc.}
MainInput Tardigradesarealsoknownasspacebears. True
FEVER
MainInput TheLionbelongstothegenusVulpes. False
MainInput Aviperisavertebrate. True
EntailedData Aviperhasabrain. True
LeapOfThought
MainInput Aamaranthisaherb. True
EntailedData Aamaranthhasanose. False
Table15: Exampledatapointfromeachdataset,andauxiliarydatathataccompaniestheMainInput.
totheWikipediaarticleforMargaritaNolascoAr- samplingbypreferentiallyselectingdatathathas
mas. Meanwhile, other aliases for J.R.R Tolkien been infrequently selected before. We note that
include “Tolkienian” and “Mabel Suffield,” his paraphrase consistency is easy to evaluate for a
mother. Rephrasingsofrelationsmightalsocreate smallnumberofparaphrasesperdatapoint,aswe
confusinginputs,e.g. switching“child”with“has haveforbothzsREandWikidata5m. Additionally,
kids,”“daughter”,or“son.” SimilartozsRE,some onLeapOfThought,wecompute∆-Accusingboth
relationsarealsoone-to-manyandtemporallyde- MainInputsandEntailedData.
pendent (like occupation), though we hope that
by using many valid answers we circumvent this Update-Transitivity caveat. The % Update-
issue to some extent when calculating prediction Transitivity metric represents the answer to the
correctness. question: if updating belief A changes belief B,
andupdatingbeliefBchangesbeliefC,whatpro-
D MetricComputationandBootstrap portion of the time does updating A change C?
Details Wewouldtreatthisasanormativemetricthatwe
hopetomaximize,exceptwedonotknowingen-
Metric computation. The only computationally eral whether there is a confounding belief D that
difficult metric to calculate is ∆-Acc, which re- determines the relationship between B and C. If
quires computing the updated language model’s changingAalsochangedaconfoundingbeliefD,
accuracyonotherdataaftereverysinglebeliefup- thenwemightnotbeabletoexpectthatCshould
date. We randomly sample other data after every changetoo. Thatsaid,whenwehavenoreasonto
update for this purpose, using n = 30 points for thinktherearesuchconfoundingbeliefs,wewould
zsRE and Wikidata5m and n = 200 points for expectalogicallyconsistentmodeltodisplay100%
FEVER and LeapOfThought. We ensure that all Update-Transitivityoftheirbeliefs. InFig. 2,for
evaluation data is used at some point during this instance,weseenoreasontosuspecttherearecon-
2728
Ablation by r
FEVER ZSRE
1
0.9
0.8
0.7 Method by r train
0.6
0.5 Baseline
r=1
0.4
r matches test
0.3
0.2
0.1
0
1 2 4 6 8 10 1 2 4 6 8 10
r
test
Figure5: Ablationacrossvaluesofrfortrainingandtesting. OnzsRE,ourmethodoutperformsthebaselinewhen
r =10,andthegapislikelytoincreaseasr risesfurther. Whenusinganon-sequentialobjectivefrompast
test test
work,performancedeclinesdrasticallyasr rises.
test
founding beliefs for the relationship between the UpdateSuccessRate ∆-Acc
dateBessieSmithdiedandthewriterofDespicable
DesiredLabel MainInput Paraphrases AllData
Me 2, and therefore we would expect that updat-
BeamLabel 91.19(0.5) 92.07(0.8) -0.39(0.1)
ingthebeliefaboutwhatalbumHotRightNowis HardLabel 94.46(0.7) 94.45(0.7) -0.24(0.1)
onwouldchangethebeliefinDespicableMe2’s
authorship(whichitdoes). Table16: Updatemetricsbyoptimizertraininglabels.
Bootstrapcomputation. Weaccountforsample
and seed variance by block bootstrap (Efron and
where r = 1, and a SLAG condition where
train
Tibshirani,1994). Whenthereisasinglestatistic
r = r . Itiscriticaltothesuccessoflearned
train test
perdatapoint,likeMainInputUpdateSuccess,we
optimizers to train them to update points sequen-
formamatrixofshapen sforndatapointsand
× tially when this is a desired application. Further,
smodelseeds(wheretheseedwasusedforboth
sequentialupdatingwithsequencepredictiontasks
task model training and learned optimizer train-
istheonlysettingwhereweseelearnedoptimizers
ing). Wethenresamplerowsandcolumnsofthis
outperformbaselinesacrossallrelevantmetrics.
matrix10,000times,whichwassufficientforcon-
vergence. When we perform hypothesis tests for Choosingtraininglabelsforlearnedoptimizers.
thedifferenceinstatisticsbetweenconditions,we Inearlyexperiments,wefoundthatitisbeneficial
pairthedatapointsbyusingthesamerowsofthis tousealldatapoints(includingcorrectlypredicted
matrixateachstepofthebootstrap(i.e. weconduct points)asMainInputsduringtraining,ratherthan
pairedtests). Formetricsinvolvingmultipledata restricting training to only incorrectly predicted
points per Main Input, like paraphrases or other points. Westillfocusoncorrectingwrongoutputs
random data, we make a simplifying assumption at test time. But so we must select what label to
wherewedonotresamplethemultipledatapoints useduringoptimizertraining. TogetaHardLabel,
butjustcomputetheaveragemetricforthosedata we use the correct label for incorrectly predicted
points and treat that as the ground-truth statistics points,andforcorrectlypredictedpoints,wesim-
for the Main Input. We explored using a full 3- ply draw a label randomly from the labels in the
dimensionalbootstrap,whereweresampleamong training data. The alternative Beam Label condi-
theseextradatapointsbyconstructingamatrixof tionusesasamplefromthemodel’sbeamsearch
shape n s n, but it was quite slow and gave for a data point, as done in past work (De Cao
× ×
similarresultstotheblockbootstrap. et al., 2021; Mitchell et al., 2021). We show up-
datemetricsforzsREsplitbythedesiredlabelin
E AdditionalResults Table16. Ifone’sgoalistofixwrongmodelout-
puts,thenitismuchbettertouseeitherthecorrect
Ablation across num. sequential steps. Fig. label or a random label as the desired model out-
5 shows the results for an ablation across r putduringtrainingratherthanasamplefromthe
test
using two kinds of learned optimizers: SLAG , model’sbeamsearch. Updatesuccessimprovesby
1
2729
etaR
sseccuS
etadpU
ObjectiveTermAblation UpdateSuccessRate RetainPredictions ∆Acc
Dataset Objective MainInput Paraphrases EntailedData LocalNeutral AllData AllData
Main 100(0.0) - - - 98.27(0.1) -0.15(0.1)
FEVER
(noKL) 100(0.0) - - - 40.42(0.6) -27.19(1.2)
Main 100(0.0) - 76.43(5.3) - 96.84(0.3) -1.22(0.8)
LeapOfThought
+Ent 100(0.0) - 71.87(5.3) - 96.52(0.3) -0.40(0.8)
Main 94.46(0.4) 94.44(0.7) - - 81.96(0.4) -0.24(0.1)
zsRE
+Para 93.75(0.4) 94.41(0.7) - - 75.24(0.5) -0.42(0.2)
Main 88.67(0.7) 64.12(0.7) - 49.78(1.0) 71.04(0.5) -1.54(0.3)
+Para 87.46(0.7) 81.06(0.7) - 47.15(1.0) 63.02(0.6) -1.55(0.3)
Wikidata5m
+LN 87.73(0.7) 59.75(0.7) - 60.49(1.0) 72.69(0.6) -1.57(0.3)
+Para+LN 87.02(0.7) 81.18(0.7) - 56.86(1.0) 68.42(0.6) -1.65(0.3)
Table17: Beliefupdateresultsbytheobjectivetermsusedforthelearnedoptimizer. Wedonotboldanynumbers
basedonstatisticalsignificance. Fortuningpurposesweselectwhicheverconditionachievesthehigherselection
criterionwithouttestingforstatisticalsignificance.
3.27 ( 0.65; p<1e 4) points for the Main Input Which Beliefs Are Hard to Update?
± −
and2.38( 1.05;p<1e 4)forParaphrases,while Wikidata5m
± − 1.00
∆-Accrisesby0.15( 0.18;p=.09).
± 0.95
Whichbeliefsarehardtoupdate? Wehypothe- 0.90
sizethatbeliefswillbeeasiertoupdatewhenthey 0.85
aremorebelief-liketobeginwith. Weprincipally
ZSRE
measurethisviathecorrelationbetweenupdatesuc- 1.00
cessrateandabelief’sconsistencyonparaphrases 0.95
0.90
before the update, for our learned optimizer in a
0.85
single-updatesetting(r = 1). Surprisingly,weob-
0.00 0.25 0.50 0.75 1.00
servenorelationshipbetweenupdatesuccessand
Pre−Update Consistency
the belief consistency. The correlation between
Figure6: Beliefsareneithereasiernorhardertoupdate
consistencyandupdatesuccessisnear0forboth
dependingontheirconsistencybeforehand.
zsRE (ρ = .027) and Wikidata5m (ρ = .013);
−
see Fig. 6 for a plot of the relationship. So it ap-
Learning Curve for zsRE
pearsthatthelearnedoptimizercanupdatemodel 100
beliefsindependentlyofhowbelief-liketheyareto
beginwith. Wewouldalsobeinterestedinconsid- 95
eringconsistencyunderentailment,buttheupdate
successrateonLeapOfThoughtisalready100%, 90
sothereisnovariancetoexplain.
85
Learning curve. In Fig. 7 we show the learning
103 103.5 104 104.5 105
n
curve of a learned optimizer trained with SLAG
Figure7: MainInputUpdateSuccessRateacrosstrain-
on zsRE. The Main Input Update Success Rate ingsetsizes,usingSLAGonzsRE.
steadilyrisesasafunctionofthetrainingsetsize.‘
Ablation by objective term. We give objective data5m, however, the paraphrase term improves
ablation results in Table 17. Surprisingly, we do paraphrase update success by a large margin of
notalwaysseethattheobjectivetermshelpforthe 16.94( 1.03;p<1e 4)points. AddingtheLocal
± −
data they are intended to help with. First, we ob- Neutral(LN)termwiththeparaphrasetermgreatly
tainmixedresultsfortheparaphraseobjective. On improvestheLNRetainRateforWikidata5m,by
zsRE, the objective term seems to hinder perfor- 9.71points( 1.44;p<1e 4),thoughbothofthese
± −
mance,withupdatesuccessdroppingonMainIn- termscomeatacosttoMainInputUpdateSuccess,
putsby0.71( 0.60;p=.021)and∆-Accdropping similartozsRE.Lastly,wedonotfindthattheen-
±
by0.18( 0.19;p=.069),whiletheparaphraseUp- tailmentobjectiveimprovesEntailedDataUpdate
±
dateSuccessRateitselfisunaffected. WithWiki- Success; infact, thismetricfallsby4.56( 7.22;
±
2730
etaR
sseccuS
etadpU
sseccuS
etadpU
tupnI
niaM
Ablation by K
0.96
0.94 Training Obj. (Ktrain)
1
Matches Test 0.92
Dan O'Bannon work was primarily Margaret Thatcher was the
science fiction and horror, Starrcade was originally most senior politician within
serving as a screenwriter broadcast via television. the Conservative Party in
and director. [y: true] the UK in 1975.
[y: true] [y: true]
0.90 1 2 4 Ktest6 8 10 R e ua v nc e th ire ly l t heG ep [r y ie fs :2e i o n 0 fn d aa 0 lea l 2 s p e . eop pB ]f e ieF s a aor cr iv edee e nend r t d tiei no [sn r yn :i, s f O ainr lse d eg e ]o cn li' ns ec .ity H gu rm eap t h e Ar s me ty em r [B yia co :l a g e tna rs ur ctt ea i n ]w r e a o ms f aCr .a lan sk se icd
Sidse Babett Knudsen graduated
Figure8: AblationacrossvaluesofK fortrainingand on Nove [m yb : e far l2 s De2 a]n nd , O 1 'B9 a6 n8 n . 1o 7 [n yt :hd t,i re 2 ud 0 e 0o ]9n . DA ecn eg mel ba e B [ rya :s ts re ut et ] is alive. ofA a F ttl rho aip n np say mn [d id ysi :ss f ik l toe r i n uxs i e b mc ]lo e em dm ip uao mgs n .e ed tic
testing, usingSLAGonzsRE.Itisusefultotrainthe
Magic Johnson was a tap dancer.
optimizerusingthevalueofK itwilluseattesttime. [y: false] Saturn is [ yo :n fly a la sn e] asteroid. 1978 is Ia on f B br ie rtn hn .an's year
[y: true]
I Kissed a Girl was only recorded
by Donald Trump.
[y: false]
in ThT ea y Blo er r nL i [a e yu :M t fn aae lc sr eSa ]hp op we a inre 2d 001H.ighway tt eo
l
eH ve ia sv ioe nn si es ra ien
s
A .mericanC ao l si tn a F K r ota oee t [r b yp i a :e n l tr l rtn L uhi ec eek a
]
N gb a ue t ec i .oa nm ae l
[y: true]
weB se tr em rnu d pa a [ r yT t :r oi fa f a n lt sg h el ee ] His im in a lt ah ye as.Ri c ah pa pr ed a D r [ a o yw n : fk t ai hn lses e ih ]na tes ry ne et t .toCarlos Santa [n ya : fi as la se U ]S president. Ju l ti ea ln en ve i Ws M i [o o yo n r :lo d fsr a ee T lr suc i ee rr ]ne s a sA .t se d th t ehe Queen (b ra [o yn c :d k f) a bi ls a s n ea ]d C .anadianAleister Crow [ c yl i :e t tiy z r e uw n ea . ]s an English
SoV s ui oc thnto egr r)i n a w H( a [D es ya m :r n e tic rsle ue pa e hE s ]ex e rp d eo i in n ne t 1n h 9t es 82 p.laT yh e in N te h w e o [ fEO y ta :r h sl fe e atea lN srn en Bs ] A CP .oe nli fc ea rn es nceSa t ku nr on w C n [o yar :p s fo T ar loa syt ei oo ]tn a i Ls L a Cl .so
CHiPs is an American comedy Figure10: Arandomsubgraphofthebeliefgraphfor
New Orleans Pelicans compete film.
Emma W [a yt :s to rn u ew ]as bor nin.[ yt :h te r uN eB ]A. [y: true] FEVER.Noteallnodesactuallyareconnectedtoatleast
Croatia has a king. oneanothernode.
[y: false] Starrcade waJo s h en v eD ne ti ug ah lt lo yn [ yw : o tr rk ue ed ] in CaliforniTah.e Cinci [n yn :a ft ai lK seid ] is a boy.
broadcast via pay-per-view
umbrella.
[y: false]
Asylum R re e [c c yo o :r r fd d as ll sai es b ]ea ln . English Paramore [y f :o fr am lse ed ] in 2007. Basildon
[
E yis n
:
gf fa alar lsn a edw ].ay from
Telemu tn ed leo v i [is s y i :a o fnE a n ln sg e el t ]is wh o- rla kn .guageHa r oo nl d F eM ba [r yc u :m a fri aXl yl lHa s 2 ena 0 ]m w , s 1at 8se 9 r [bc4 yoop .:nrr tno t red unu etc ].es online
Filming for Boyhood was stopped
between 2002 and 2013.
[y: false]
Figure 9: A random subgraph of the belief graph for
F onE eV aE nR o. thN eo rt ne oa dll en .odesactuallyareconnectedtoatleast b a ss cN e reda e t uu npr pa o lal n [yB yT o :wa r fr in ata h lnK so eti ul i ]l n te o r r's es v w o isa ri is g onin .Daelre ik n H Mo au [k yg e :h fY ab o la sur ere ]Mly o s vt ea .rredKuching is [a y :c i ft ay l si en ] Singapore.
The Mirny (sloop-of-war)
was a ship without allegianHceea.vy Metal music was developAeudstralia (2008 film) production
[y: false] in the early 1970's. took place in Bowen.
[y: true] [y: true]
p=.213)pointswiththeobjective.
Ablationbynum. updatesteps. Fig. 8showsthe Camden, New JerseC ya ir sl o as l aS ra gn e[t ya :n ta r uis e a ] musician. On Febru Ka [yyr :y le t2 rd u, i e2 e ]0 d1 .3, Chris
human settlement.
[y: true]
results of an ablation across values of K using a
learnedoptimizertrainedusingSLAGwithr = 1 Knocked Up is a work of aD ra ta .[g y :i s f aa l sh eo ]me. Chile [ i ys : a tr c uo eu ]ntry. St. A Sn ug be r [P yw o : pa fa s R l sr ee ecl ]oe ra ds se .d by
[y: true]
onzsRE.MainInputUpdateSuccessrisesbyover
The Lincoln-Douglas debates
threepointsbyincreasingK test from1toatleast Despicab ble y M a e co 2 m w pa as n yp .roM due cl eB d[ h y:a d tr ua e c ]areer. happened [ yin : tQ ru ui en ]cy, Illinois.
5. UsingavalueofK thatmatchesK gives [y: true] James o V f I a a sn id n gI lw e a ps a a rl im amaj eo nr ta fd ov rocate
train test Scotland and England.
The Chrysler Building has Harold Macmillan was born [y: true]
afurtherincreaseofabout0.5points. yet to be surpassed in height. on February 20, 1894.
[y: false] [y: false]
A River Runs Through It has
lost every Academy Award.
[y: false]
Figure11: Arandomsubgraphofthebeliefgraphfor
FEVER.Noteallnodesactuallyareconnectedtoatleast
oneanothernode.
2731
etaR
sseccuS
etadpU

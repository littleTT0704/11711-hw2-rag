Paraphrastic Representations at Scale
JohnWieting1,KevinGimpel2,GrahamNeubig3,andTaylorBerg-Kirkpatrick4
1GoogleResearch
2ToyotaTechnologicalInstituteatChicago,Chicago,IL,60637,USA
3CarnegieMellonUniversity,Pittsburgh,PA,15213,USA
4UniversityofCaliforniaSanDiego,SanDiego,CA,92093,USA
jwieting@alumni.cmu.edu,kgimpel@ttic.edu,gneubig@cs.cmu.edu,tberg@eng.ucsd.edu
Abstract tasks(Wietingetal.,2019a). Withinthiscontext,
fastandlight-weightmethodsareparticularlyuse-
We present a system that allows users to
fulastheymakeiteasytocomputesimilarityover
train their own state-of-the-art paraphrastic
sentence representations in a variety of lan- theever-increasingvolumesofwebtextavailable.
guages.WereleasetrainedmodelsforEnglish, Forinstance,wemaywanttomineahundredmil-
Arabic, German, Spanish, French, Russian, lion parallel sentences (Schwenk et al., 2021) or
Turkish, and Chinese. We train these mod- useasemanticsimilarityrewardwhenfine-tuning
elsonlargeamountsofdata,achievingsignif-
language generation models on tens of millions
icantly improved performance from our orig-
oftrainingexamples. Thesetasksaremuchmore
inal papers on a suite of monolingual seman-
feasiblewhenusingapproachesthatarefast, can
ticsimilarity,cross-lingualsemanticsimilarity,
andbitextminingtasks. Moreover, theresult- berunonCPU,anduselittleRAM,allowingfor
ing models surpass all prior work on efficient increasedbatchsize.
unsupervisedsemantictextualsimilarity,even
Thisneedforfastinferenceisonemotivationfor
significantly outperforming supervised BERT-
usingsentenceembeddings. Sentenceembeddings
based models like Sentence-BERT (Reimers
and Gurevych, 2019). Most importantly, our allowthesearchforsimilarsentencestobelinearin
models are orders of magnitude faster than thenumberofsentences,orevensub-linearwhen
otherstrongsimilaritymodelsandcanbeused usinghighlyoptimizedtoolslikeFaiss(Johnson
on CPU with little difference in inference etal.,2017)thatallowforefficientnearestneigh-
speed (even improved speed over GPU when
borsearch. Thisiscontrasttomodels, likecross-
usingmoreCPUcores),makingthesemodels
attentionmodels,whicharequadraticduringinfer-
an attractive choice for users without access
ence as they require both of the texts being com-
to GPUs or for use on embedded devices. Fi-
paredasinputs. Asweshowinthispaper,oursim-
nally, weaddsignificantlyincreasedfunction-
ality to the code bases for training paraphras- pleandinterpretableword-averagingsentenceem-
tic sentence models, easing their use for both beddingmodels(Wietingetal.,2016b;Wietingand
inferenceandfortrainingthemforanydesired Gimpel, 2018; Wieting et al., 2019b), are orders
language with parallel data. We also include ofmagnitudefastertocomputethanpriorembed-
code to automatically download and prepro-
dingapproacheswhilesimultaneouslypossessing
cesstrainingdata.1
significantlystrongerperformanceonmonolingual
1 Introduction andcross-lingualsemanticsimilaritytasks. Since
wearesimplyaveragingembeddingsandhaveno
Measuringsentencesimilarity(Agirreetal.,2012)
neural architecture, any models based on neural
is an important task in natural language pro-
architectures,especiallylargepretrainedneuralar-
cessing, and has found many uses including
chitectureswhichareincreasinglyused,willnotbe
paraphrase detection (Dolan et al., 2004), bitext
asfastasthemodelsdescribedinthispaper. Lastly,
mining (Schwenk and Douze, 2017), language
wealsoshowthatthisapproachiscompetitivewith
modelling (Khandelwal et al., 2019), question-
LASER(ArtetxeandSchwenk,2019),astate-of-
answering(Lewisetal.,2021),andasrewardfunc-
the-art multilingual model, on mining bitext and
tionsorevaluationmetricsforlanguagegeneration
hasstrongerperformanceoncross-lingualseman-
1Code, including an easy to install PyPi package, re- tic similarity, while having inference speeds that
leased models including Hugging Face implementations,
aretwiceasfastonGPUandordersofmagnitude
demo,anddataareavailableathttps://github.com/jwieting/
paraphrastic-representations-at-scale. fasteronCPU.
379
ProceedingsoftheThe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages379-388
December7-11,2022(cid:13)c2022AssociationforComputationalLinguistics
Wemakeseveralcontributionsinthispaperthat 2 RelatedWork
gobeyondourpriorwork. Firstly,wereformatthe
codetosupporttrainingmodelsontensofmillions
2.1 EnglishSemanticSimilarity
ofsentencepairsefficientlyandwithlowRAMus-
age. Secondly,wetrainanEnglishmodelon25.85
Ourlearningandevaluationsettingisthesameas
millionparaphrasepairsfromParaNMT(Wieting
that of our earlier work that seeks to learn para-
andGimpel,2018),aparaphrasecorpusweprevi-
phrasticsentenceembeddingsthatcanbeusedfor
ously constructed automatically from bitext. We
downstreamtasks(Wietingetal.,2016b,a;Wieting
thentrainmodelsdirectlyonX-Englishbitextfor
and Gimpel, 2017; Wieting et al., 2017; Wieting
Arabic,German,Spanish,French,Russian,Turk-
and Gimpel, 2018). We trained models on noisy
ish,andChinese,producingmodelsthatareableto
paraphrasepairsandevaluatedthemprimarilyon
distinguish both paraphrases in English and their
semantic textual similarity (STS) tasks. More re-
respective languages as well as cross-lingual X-
cently,wemadeuseofparallelbitextfortraining
Englishparaphrases. Eventhoughallmodelsare
paraphrastic representations for other languages
able to model semantic similarity in English, we
as well that are also able to model cross-lingual
findthattrainingonParaNMTspecificallyleadsto
semantic similarity (Wieting et al., 2019a, 2020).
stronger models as it is easier to filter the data to
Prior work in learning general sentence embed-
removenoiseandsentencepairswithlittletonodi-
dingshasusedautoencoders(Socheretal.,2011;
versity. Werefertoourmodelsas PARAGRAM-SP,
abbreviatedas P-SP,2 referringtohowthemodels Hill et al., 2016), encoder-decoder architectures
(Kiros et al., 2015; Gan et al., 2017), and other
are based on averaging subword units generated
sources of supervision and learning frameworks
bysentencepiece(KudoandRichardson,2018).
(LeandMikolov,2014;Phametal.,2015;Arora
Wemakeallofthesemodelsavailabletothecom-
etal.,2017;Pagliardinietal.,2017).
munityforuseondownstreamtasks.
ForEnglishsemanticsimilarity,wecompareto
We also add functionality to our implementa-
well known sentence embedding models such as
tion. Besidesthesupportforefficient,low-memory
InferSent(Conneauetal.,2017),GenSen(Subra-
training on tens of million of sentence pairs de-
manian et al., 2018), the Universal Sentence En-
scribedabove,weaddcodetosupport(1)reading
coder (USE) (Cer et al., 2018), as well as BERT
inalistofsentencesandproducingasavednumpy (Devlinetal.,2019).3 WeusethepretrainedBERT
arrayofthesentenceembeddings;(2)readingina
modelintwowaystocreateasentenceembedding.
listofsentencepairsandproducingcosinesimilar-
The first way is to concatenate the hidden states
ityscores;and(3)downloadingandpreprocessing
fortheCLStokeninthelastfourlayers. Thesec-
evaluation data, bitext, and paraphrase data. For
ondwayistoconcatenatethehiddenstatesofall
bitextandparaphrasedata,weprovidesupportfor
wordtokensinthelastfourlayersandmeanpool
trainingusingeithertextfilesorHDF5files.
these representations. Both methods result in a
4096dimensionembedding. Wealsocompareto
Lastly, this paper contains new experiments
a more recently released model called Sentence-
showcasing the limits of these scaled-up models
BERT(ReimersandGurevych,2019). Thismodel
and detailed comparisons with prior work on a
issimilartoInferSentinthatitistrainedonnatu-
suite of semantic similarity tasks in a variety of
rallanguageinferencedata(SNLI;Bowmanetal.,
languages. We release our code and models to
2015). However,insteadofusingpretrainedword
thecommunityinthehopethattheywillbefound
embeddings, they fine-tune BERT in a way to in-
usefulforresearchandapplications,aswellasus-
duce sentence embeddings. Lastly, we also com-
ingthemasabasetobuildstronger,fastermodels
paretotheunsupervisedversionofSimCSE(Gao
coveringmoreofthelanguagesoftheworld.
etal.,2021),whichfine-tunesapretrainedencoder
on contrastive pairs, where positive pairs are ob-
tainedbyusingdropoutonasingleinputsentence.
2OurEnglishmodelisP-SP,andthecross-lingualmodels
areP-SP-AR,P-SP-DE,P-SP-ES,P-SP-FR,P-SP-RU,P-SP- 3Note that in all experiments using BERT, including
TR,andP-SP-ZH. Sentence-BERT,thelarge,uncasedversionisused.
380
2.2 Cross-LingualSemanticSimilarityand we choose the most similar sentence in some set
SemanticSimilarityinNon-English accordingtothecurrentmodelparameters,i.e.,the
Languages onewiththehighestcosinesimilarity. Wefoundwe
couldachievethestrongestperformancebytying
Most previous work for cross-lingual representa-
all parameters together for each language, more
tions has focused on models based on encoders
precisely,θ andθ arethesame.
from neural machine translation (Espana-Bonet src tgt
etal.,2017;SchwenkandDouze,2017;Schwenk,
Negative Sampling. Negative examples are se-
2018)ordeeparchitecturesusingcontrastivelosses
lectedfromthesentencesinthebatchfromtheop-
(Grégoire and Langlais, 2018; Guo et al., 2018;
posinglanguagewhentrainingwithbitextandfrom
Chidambaram et al., 2019). Recently, other ap-
any sentence in the batch when using paraphrase
proachesusinglargeTransformer(Vaswanietal.,
data. In all cases, we choose the negative exam-
2017)havebeenproposed,trainedonvastquanti-
plewiththehighestcosinesimilaritytothegiven
tiesoftext(Conneauetal.,2020;Liuetal.,2020;
sentences,ensuringthatthenegativeisnotinfact
Tran et al., 2020). We primarily focus our com-
pairedwithsinthebatch. Toselectevenmorediffi-
parisonforthesesettingsonLASER(Artetxeand
cultnegativeexamplesthataidtraining,weusethe
Schwenk,2019),amodeltrainedforsemanticsim-
mega-batchingprocedureofWietingandGimpel
ilarity across more than 100 languages. Their
(2018),whichaggregatesM mini-batchestocreate
modelusesanLSTMencoder-decodertrainedon
one “mega-batch” and selects negative examples
hundreds of millions of parallel sentences. They
fromthismega-batch. Onceeachpairinthemega-
achievestate-of-the-artperformanceonavarietyof
batch has a negative example, the mega-batch is
multilingualsentenceembeddingstasksincluding
splitbackupintoM mini-batchesfortraining. Ad-
bitextmining. WealsocomparetoLaBSE(Feng
ditionally,weannealthemega-batchsizebyslowly
etal.,2022),acontrastivemodeltrainedonsixbil-
increasingitduringtraining. Thisyieldsimproved
lion parallel pairs across languages and was also
performancebyasignificantmargin.
trained on monolingual text using a masked lan-
guagemodellingobjective. Encoder. Our sentence encoder g simply aver-
ages the embeddings of subword units generated
3 Methods bysentencepiece(KudoandRichardson,2018);
werefertoourmodelasPARAGRAM-SP,abbrevi-
We first describe our objective function and then
atedas P-SP.Thismeansthatthesentencepiece
describeourencoder.
embeddingsthemselvesaretheonlylearnedparam-
Training. The training data consists of a se- etersofthismodel.
quenceofparallelsentencepairs(s ,t )insource
i i
and target languages respectively. Note that for 4 CodeandUsage
trainingourEnglishmodel,thesourceandtarget
We added a number of features to the code base
languagesarebothEnglishasweareabletomake
toimproveperformanceandmakeiteasiertouse.
useofanexistingparaphrasecorpus. Foreachsen-
First, we added code to support easier inference.
tence pair, we randomly choose a negative target
Examples of using the code programmatically to
sentencet duringtrainingthatisnotatranslation
0i
embed sentences and score sentence pairs (using
orparaphraseofs . Ourobjectiveistohavesource
i
cosinesimilarity)areshowninFigure1.
and target sentences be more similar than source
Our code base also supports functionality that
andnegativetargetexamplesbyamarginδ:
allowsonetoreadinalistofsentencesandproduce
a saved numpy array of the sentence embeddings.
θm src,i θn
tgt
δ −f θ(s i,t i)+f θ(s i,t 0i))
+
(1)
Wealsoincludedfunctionalitythatallowsoneto
Xi h i
read in a list of sentence pairs and produce the
wherethesimilarityfunctionisdefinedas: sentence pairs along with their cosine similarity
scores in an output file. These scripts allow our
f (s,t) = cos g(s;θ ),g(t;θ ) (2) modelstobeusedwithoutanyprogrammingforthe
θ src tgt
twomostcommonusecases: embeddingsentences
(cid:16) (cid:17)
where g is the sentence encoder with parameters andscoringsentencepairs. Examplesoftheirusage
for each language θ = (θ ,θ ). To select t withatrainedmodelareshowninFigure2.
src tgt 0i
381
from models import load_model cd preprocess/bilingual && bash do_all.
1 1
sh fr-es-de
2
text1 = ’This is a test.’ cd ../..
3 2
text2 = ’This is another test.’ cd preprocess/paranmt && bash do_all.sh
4 3
0.4 1.0 0.7
5
# Load English paraphrase model
6
7 model_name = ’paraphrase-at-scale/model. Figure3: Usageexamplestodownloadandpreprocess
para.lc.100.pt’
bilingualandParaNMTdata.Thefirstcommanddown-
sp_model = ’paraphrase-at-scale/paranmt.
8 loads and preprocesses (filters, trains sentencepiece
model’
models, tokenizes if language is zh, converts files to
9
10 model, _ = load_model(model_name= hdf5format)en-Xbilingualdata. Thethirdcommand
model_name, sp_model=sp_model) downloads and preprocesses ParaNMT data. The ar-
11 gumentsareusedtofilterthedata(semanticsimilarity
# Obtain sentence embedding
12
scoresbetween0.4and1.0andtrigramoverlapbelow
embeddings = model.embed_raw_text([text1
13
, text2]) # 2D numpy array of 0.7, which have been used in prior papers when gen-
sentence embeddings erating training data for paraphrase generation (Iyyer
14 cosine_scores = model.score_raw_text([( etal.,2018;Krishnaetal.,2020)).
text1, text2)]) # list of cosine
scores
en ar de es fr ru tr zh
25.85M8.23M6.47M6.75M6.46M9.09M5.12M4.18M
Figure1: Usageexampleofprogrammaticallyloading
one of our pretrained models and obtaining sentence
Table 1: The number of sentence pairs used to train
embeddingsandscoresfortwosentences.
ourmodels. ForEnglish,thedataisParaNMT,andfor
the other languages, the data is a collection of bitext
python -u embed_sentences.py --sentence-
1
detailedinSection5.1.
file paraphrase-at-scale/example-
sentences.txt --load-file paraphrase
-at-scale/model.para.lc.100.pt --
data, including scripts to download and evaluate
output-file sentence_embeds.np
ontheSTSdata(English,non-English,andcross-
2
3 python score_sentence_pairs.py -- lingual), as well as code to download and pro-
sentence-pair-file paraphrase-at-
cess bitext and ParaNMT automatically. For bi-
scale/example-sentences-pairs.txt --
load-file paraphrase-at-scale/model. text,ourscriptsdownloadthedata,filterthedata
para.lc.100.pt by length,5 lowercase, remove duplicates, train a
sentencepiece model, encode the data with the
Figure 2: Usage examples to embed sentences and
sentencepiece model, shuffle the data, and pro-
score sentence pairs. The first command is a usage
example of scoring a list of sentence pairs. The file cess the data into HDF5 format for efficient use.
example-sentences-pairs.txtcontainsalistofsen- ForParaNMT,ourscriptsdownloadthedata,use
tences,oneperline. Theoutputofthescriptisasaved alanguageclassifiertofilteroutnon-Englishsen-
numpyarrayofsentenceembeddingsinthesameorder tences6 (Joulinetal.,2017),filterthedatabypara-
of the input sentences. The second command is a us- phrasescore,trigramoverlap,andlength,7 traina
age example of scoring a list of sentence pairs. The
sentencepiece model, encode the data with the
fileexample-sentences-pairs.txtcontainspairsof
sentencepiecemodel, andprocessthedatainto
tab-separated sentences, one per line. The output of
HDF5format. ExamplesareshowninFigure3.
thescriptisatextfilecontainingthetabseparatedlist
ofsentencesalongwiththeircosinescoresinthesame
5 Experiments
orderoftheinputsentences.
5.1 ExperimentalSetup
Secondly,weaddedatrainingmodeusingHDF54
Data. For our English model, we train on se-
format, allowing training data to remain on disk
lectedsentencepairsfromParaNMT(Wietingand
during training. This leads to a significant reduc-
Gimpel,2018). Wefilterthecorpusbyonlyinclud-
tioninRAMusageduringtraining,whichisespe-
ingsentencepairswheretheparaphrasescorefor
ciallytruewhenusingmorethan10milliontrain-
thetwosentencesis 0.4. Weadditionallyfiltered
ingexamples. Efficienttrainingcannowbedone ≥
onCPUonlyusingonlyafewgigabytesofRAM.
5Weremovesentenceswiththenumberoftokens(untok-
enized)smallerthan3orgreaterthan100.
Lastly, we also added code for preprocessing 6https://fasttext.cc
7Weremovesentenceswiththenumberoftokens(untok-
4https://docs.h5py.org/en/stable/ enized)smallerthan5orgreaterthan40.
382
SemanticTextualSimilarity(STS)
Model
2012 2013 2014 2015 2016 Avg.
BERT(CLS) 33.2 29.6 34.3 45.1 48.4 38.1
BERT(Mean) 48.8 46.5 54.0 59.2 63.4 54.4
InferSent 61.1 51.4 68.1 70.9 70.7 64.4
GenSen 60.7 50.8 64.1 73.3 66.0 63.0
USE 61.4 59.0 70.6 74.3 73.9 67.8
Sentence-BERT 66.9 63.2 74.2 77.3 72.8 70.9
LASER 63.1 47.0 67.7 74.9 71.9 64.9
P-SP 68.7 64.7 78.1 81.4 80.0 74.6
Sentence-BERT 71.0 76.5 73.2 79.1 74.3 74.8
P-SP 71.2 76.5 74.6 83.0 79.1 76.9
Table 2: Results of our models and models from prior work on English STS. In the first part of the table, we
show results, measured in Pearson’s r 100, for each year of the STS tasks 2012-2016 as well as the average
×
performanceacrossallyears.Inthesecondpart,weevaluatebasedontheSpearman’sρ 100oftheconcatenation
×
ofthedatasetsofeachyearwiththe2013SMTdatasetremovedfollowing(ReimersandGurevych,2019).
Model Dim. ar-ar ar-en es-es es-en tr-en
LASER 1024 69.3\68.8 65.5\66.5 79.7\79.7 59.7\58.0 72.0\72.1
LaBSE 768 68.6\69.1 72.2\74.5 79.5\80.8 65.5\65.7 72.9\72.1
Espana-Bonetetal.(2017) 2048 59 44 78 49 76
Chidambarametal.(2019) 512 - - 64.2 58.7 -
2017STS1stPlace - 75.4 74.9 85.6 83.0 77.1
2017STS2ndPlace - 75.4 71.3 85.0 81.3 74.2
2017STS3rdPlace - 74.6 70.0 84.9 79.1 73.6
P-SP 1024 76.2\76.7 78.3\78.4 85.8\85.6 78.4\77.8 79.2\79.5
Table 3: Comparison of our models with those in the literature on non-English and cross-lingual STS. We also
includethetop3systemsforeachdatasetfromtheSemEval2017STSsharedtask. Performanceismeasuredin
Pearson’sr 100. WealsoincluderesultsinSpearmans’sρ 100afteraslashforLASER,LaBSE,andP-SP.
× ×
sentence pairs by their trigram overlap (Wieting Oneexception,though,iswedonotincludetrain-
et al., 2017), which is calculated by counting tri- ingdatafromTatoeba13 (Tiedemann,2012)asthey
gramsinthetwosentences,andthendividingthe do, since this domain is also in the bitext mining
numberofsharedtrigramsbythetotalnumberin evaluation set. The amount of data used to train
the sentence with fewer tokens. We only include eachofourmodelsisshowninTable1.
sentencepairswherethetrigramoverlapscoreis
0.7. Theparaphrasescoreiscalculatedbyaver- Hyperparameters. For all models, we fix the
≤
aging PARAGRAM-PHRASE embeddings(Wieting batch size to 128, margin δ to 0.4, and the an-
etal.,2016b)forthetwosentencesineachpairand nealing rate to 150.14 We set the size of the
thencomputingtheircosinesimilarity. Thepurpose sentencepiece vocabulary to 50,000, using a
ofthelowerthresholdistoremovenoisewhilethe sharedvocabularyforthemodelstrainedonbitext.
higher threshold is meant to remove paraphrases Ifawordisnotinvocabulary,wesimplyexcludeit,
thataretoosimilar. unlessthetextonlyconsistsofunknownwordsin
whichcaseweuseasingleunknown-wordtoken.
OurtrainingdataisamixtureofOpenSubtitles
20188 (Lison and Tiedemann, 2016), Tanzil cor- WeoptimizeourmodelsusingAdam(Kingmaand
pus9 (Tiedemann, 2012), Europarl10 for Spanish, Ba, 2014) with a learning rate of 0.001 and train
GlobalVoices11 (Tiedemann,2012),andtheMul- modelsfor25epochs.
tiUN corpus.12 We follow the same distribution For training on the bilingual corpora, we tune
forourlanguagesofinterestacrossdatasourcesas eachmodelonthe250example2017EnglishSTS
ArtetxeandSchwenk(2019)forafaircomparison. task (Cer et al., 2017). We vary dropout on the
embeddingsover 0,0.1,0.3 andthemega-batch
{ }
8http://opus.nlpl.eu/OpenSubtitles.php sizeM over 60,100,140 .
{ }
9http://opus.nlpl.eu/Tanzil.php
10http://opus.nlpl.eu/Europarl.php 13https://opus.nlpl.eu/Tatoeba.php
11https://opus.nlpl.eu/GlobalVoices.php 14Annealing rate is the number of minibatches that are
12http://opus.nlpl.eu/MultiUN.php processedbeforethemegabatchsizeisincreasedby1.
383
For training on ParaNMT, we fix the hyperpa- LanguageLASERXLM-RmBARTCRISSLaBSE P-SP
rametersinourmodelduetotheincreaseddatasize ar 7.8 52.5 61.0 22.0 9.1 8.8
making tuning more expensive. We use a mega- de 1.0 11.1 13.2 2.0 0.7 1.5
es 2.1 24.3 39.6 3.7 1.6 2.4
batch size M of 100 and set the dropout on the
fr 4.3 26.3 39.6 7.3 4.0 5.4
embeddingsto0.0. ru 5.9 25.9 31.6 9.7 4.7 5.6
tr 2.6 34.3 48.8 7.1 1.6 1.4
5.2 Evaluation Avg. 4.0 29.1 39.0 8.6 3.6 4.2
Table 4: Results on the Tatoeba bitext mining task
WeevaluatesentenceembeddingsusingtheSem-
(Artetxe and Schwenk, 2019). Results are measured
Evalsemantictextualsimilarity(STS)tasksfrom
inerrorrate 100.
2012to2016(Agirreetal.,2012,2013,2014,2015, ×
2016) as was done initially for sentence embed-
dings in (Wieting et al., 2016b). Given two sen- similarity. Performanceismeasuredbycomputing
tences,theaimoftheSTStasksistopredicttheir theerrorrate.
similarityona0-5scale,where0indicatesthesen-
tencesareondifferenttopicsand5meanstheyare 6 Results
completely equivalent. As our test set, we report
English Semantic Similarity. The results for
theaveragePearson’sr overeachyearoftheSTS
our English semantic similarity evaluation are
tasksfrom2012-2016asisconvention.
shown in Table 2. Our P-SP model has the best
Most work evaluating accuracy on STS tasks
performance across each year of the task, signif-
has averaged the Pearson’s r over each individ-
icantly outperforming all prior work. We outper-
ual dataset for each year of the STS competition.
form methods that use large pre-trained models
However,ReimersandGurevych(2019)computed
includingSentence-BERTwhichissupervised,as
Spearman’sρoverconcatenateddatasetsforeach
itistrainedonNLIdata(Bowmanetal.,2015).
year of the STS competition. To be consistent
WealsoincluderesultsfromSimCSE(Gaoetal.,
with previous work, we re-ran their model and
2021). We compare to the unsupervised version,
calculatedresultsusingthestandardmethod,and
sinceourmodelisalsounsupervised. Weevaluate
thusourresultsarenotthesameasthosereported
usingtheSpearman’sρoftheconcatenationofthe
ReimersandGurevych(2019). However,wealso
datasetsforeachyear,andfindouraverageperfor-
includeresultsusingtheirapproachforcomplete-
manceoverthe2012-2016datasetstobe76.9,com-
ness. Oneotherdifferencebetweenthesetwoways
paredto77.4and77.9fortheRoBERTa-base(Liu
of calculating the results is the inclusion of the
etal.,2019)andRoBERTa-largeversionsofSim-
SMT dataset of the 2013 task, which we also ex-
CSE.Whileourperformanceisslightlylower,we
clude when replicating the approach in Reimers
note that they tune their model on the dev set of
andGurevych(2019).
theSTSBenchmark(Ceretal.,2017),whichcon-
Forcross-lingualsemanticsimilarityandseman-
tainsasubsetofthedatafromSTStaskswhichwe
tic similarity in non-English languages, we eval-
useforevaluation. Therefore,theyaretuningona
uate on the STS tasks from SemEval 2017. This
subsetoftheevaluationdata,anditisunclearhow
evaluationcontainsArabic-Arabic,Arabic-English,
tuningonthistestdataaffectsmodelperformance.
Spanish-Spanish, Spanish-English, and Turkish-
English datasets. The datasets were created by Cross-Lingual Semantic Similarity. The re-
translating one or both pairs of an English STS sultsforthenon-Englishandcross-lingualseman-
pairintoArabic(ar),Spanish(es),orTurkish(tr).
ticsimilarityevaluationareshowninTable3. From
Followingconvention,wereportresultswithPear- theresults,ourmodelagainoutperformsallprior
son’sr forallsystems,butalsoincluderesultsin work using sentence embeddings. The only sys-
Spearman’sρforLASER,LaBSE,and P-SP. temsthathavebetterperformancearethetop(non-
We also evaluate on the Tatoeba bitext mining embeddingbased)systemsfromSemEval2017for
task introduced by Artetxe and Schwenk (2019). Spanish-English.15
Thedatasetconsistsofupto1,000English-aligned
sentence pairs for over 100 languages. The aim
15Thetopsystemsforthistaskusedsupervisionandrelied
ofthetaskistofindthenearestneighborforeach
onstate-of-the-arttranslationmodelstofirsttranslatethenon-
sentenceintheotherlanguageaccordingtocosine EnglishsentencestoEnglish.
384
BitextMining. TheresultsontheTatoebabitext using32coresandachievedaspeedof15,316sen-
miningtaskfromArtetxeandSchwenk(2019)are tences/second. Thisisevenfasterthanwhenusing
shown in Table 4. The results show that our em- aGPUandindicatesthatourmodelcaneffectively
beddingsarecompetitive,buthaveslightlyhigher be used at scale when GPUs are not available. It
errorratesthanLASER.Themodelsaresoclose alsosuggestsourmodelwouldbeappropriatefor
thatthedifferenceinerrorrateforthetwomodels useonembeddeddevices.
acrossthe6evaluationsis0.2,correspondingtoa
difference of about 2 mismatched sentence pairs 8 Conclusion
perdataset. LaBSEperformsabitbetter,butwas
Inthispaper,wepresentasystemforthelearning
trainedonmuchmoredatathenbothLASERand
andinferenceofparaphrasticsentenceembeddings
ourmethod. WealsocomparetomBART,XLM-R,
in any language for which there is paraphrase or
andCRISS.16
bilingualparalleldata. Additionally,wereleaseour
Thisbitextminingresultisincontrasttothere-
trainedsentenceembeddingmodelsinEnglish,as
sultsoncross-lingualsemanticsimilarity,suggest-
wellasArabic,German,Spanish,French,Russian,
ingthatourembeddingsaccountforalessliteral
Turkish, and Chinese. These models are trained
semantic similarity, making them more adept at
on tens of million of sentence pairs resulting in
detectingparaphrasesbutslightlyweakeratiden-
models that achieve state-of-the-art performance
tifying translations. It is also worth noting that
on unsupervised English semantic similarity and
LASER was trained on Tatoeba data outside the
arestate-of-the-artorcompetitiveonnon-English
testsets,whichcouldalsoaccountforsomeofthe
semanticsimilarity,cross-lingualsemanticsimilar-
slightimprovementoverourmodel.
ity,andbitextmining.
Moreover, our models are significantly faster
7 SpeedAnalysis
thanpriorworkowingtotheirsimplearchitecture.
TheycanalsoberunonCPUwithlittletonoloss
inspeedfromrunningthemonGPU—-something
Model GPU CPU
thatnostrongmodelsfrompriorworkareableto
P-SP 13,863 12,776
do. Lastly,wereleaseourcodethathasbeenmod-
LASER 6,033 26
Sentence-Bert 288 2 ified to make training and inference easier, with
InferSent 4,445 16 supportfortrainingonlargecorpora, preprocess-
Table 5: Speed as measured in sentences/second on ingparaphraseandbilingualcorporaandevaluation
bothGPU(Nvidia1080TI)andCPU(singlecore). data,aswellasscriptsforeasyinferencethatcan
generateembeddingsorsemanticsimilarityscores
Weanalyzethespeedofourmodelsaswellasse-
forsentencessuppliedinatextfile.
lected popular sentence embedding models from
priorwork. Toevaluateinferencespeed,wemea-
surethetimerequiredtoembed120,000sentences References
fromtheTorontoBookCorpus(Zhuetal.,2015).
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Preprocessingofsentencesisnotfactoredintothe Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
timing, and each method sorts the sentences by Guo,IñigoLopez-Gazpio,MontseMaritxalar,Rada
length prior to computing the embeddings to re- Mihalcea,GermanRigau,LarraitzUria,andJanyce
Wiebe. 2015. SemEval-2015 task 2: Semantic tex-
duce padding and extra computation. We use a
tual similarity, English, Spanish and pilot on inter-
batch size of 64 for each model. The number of pretability. In Proceedings of the 9th International
sentences embedded per second is shown in Ta- Workshop on Semantic Evaluation (SemEval 2015),
ble5. pages 252–263, Denver, Colorado. Association for
ComputationalLinguistics.
From the results, we see that our model is eas-
ilythefastestonGPU,sometimesbyanorderof Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
magnitude. Interestingly, using a single core of Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
CPU, we achieve similar speeds to inference on
Wiebe. 2014. SemEval-2014 task 10: Multilingual
GPU, which is not the case for any other model.
semantic textual similarity. In Proceedings of the
Moreover, we repeated the experiment, this time 8thInternationalWorkshoponSemanticEvaluation
(SemEval 2014), pages 81–91, Dublin, Ireland. As-
16Resultsarecopiedfrom(Tranetal.,2020). sociationforComputationalLinguistics.
385
EnekoAgirre,CarmenBanea,DanielCer,MonaDiab, Muthu Chidambaram, Yinfei Yang, Daniel Cer, Steve
Aitor Gonzalez-Agirre, Rada Mihalcea, German Yuan, Yunhsuan Sung, Brian Strope, and Ray
Rigau, and Janyce Wiebe. 2016. SemEval-2016 Kurzweil. 2019. Learning cross-lingual sentence
task 1: Semantic textual similarity, monolingual representationsviaamulti-taskdual-encodermodel.
and cross-lingual evaluation. In Proceedings of the In Proceedings of the 4th Workshop on Represen-
10th International Workshop on Semantic Evalua- tation Learning for NLP (RepL4NLP-2019), pages
tion (SemEval-2016), pages 497–511, San Diego, 250–259, Florence, Italy. Association for Computa-
California. Association for Computational Linguis- tionalLinguistics.
tics.
AlexisConneau, KartikayKhandelwal, NamanGoyal,
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
pilotonsemantictextualsimilarity. In*SEM2012:
moyer, and Veselin Stoyanov. 2020. Unsupervised
The First Joint Conference on Lexical and Compu-
cross-lingual representation learning at scale. In
tational Semantics – Volume 1: Proceedings of the
Proceedingsofthe58thAnnualMeetingoftheAsso-
main conference and the shared task, and Volume
ciation for Computational Linguistics, pages 8440–
2: ProceedingsoftheSixthInternationalWorkshop
8451, Online. Association for Computational Lin-
onSemanticEvaluation(SemEval2012),pages385–
guistics.
393, Montréal, Canada. Association for Computa-
tionalLinguistics. AlexisConneau,DouweKiela,HolgerSchwenk,Loïc
Barrault, and Antoine Bordes. 2017. Supervised
EnekoAgirre,DanielCer,MonaDiab,AitorGonzalez-
learning of universal sentence representations from
Agirre,andWeiweiGuo.2013. *SEM2013shared
natural language inference data. In Proceedings of
task: Semantic textual similarity. In Second Joint
the 2017 Conference on Empirical Methods in Nat-
Conference on Lexical and Computational Seman-
ural Language Processing, pages 670–680, Copen-
tics (*SEM), Volume 1: Proceedings of the Main
hagen, Denmark. Association for Computational
Conference and the Shared Task: Semantic Textual
Linguistics.
Similarity,pages32–43,Atlanta,Georgia,USA.As-
sociationforComputationalLinguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. Kristina Toutanova. 2019. BERT: Pre-training of
Asimplebuttough-to-beatbaselineforsentenceem- deep bidirectional transformers for language under-
beddings. In Proceedings of the International Con- standing. In Proceedings of the 2019 Conference
ferenceonLearningRepresentations. of the North American Chapter of the Association
for Computational Linguistics: Human Language
Mikel Artetxe and Holger Schwenk. 2019. Mas- Technologies, Volume 1 (Long and Short Papers),
sively multilingual sentence embeddings for zero- pages4171–4186,Minneapolis,Minnesota.Associ-
shot cross-lingual transfer and beyond. Transac- ationforComputationalLinguistics.
tions of the Association for Computational Linguis-
tics,7:597–610. Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
pora: Exploiting massively parallel news sources.
and Christopher D. Manning. 2015. A large anno-
In COLING 2004: Proceedings of the 20th Inter-
tatedcorpusforlearningnaturallanguageinference.
national Conference on Computational Linguistics,
In Proceedings of the 2015 Conference on Empiri-
pages350–356,Geneva,Switzerland.COLING.
calMethodsinNaturalLanguageProcessing,pages
632–642,Lisbon,Portugal.AssociationforCompu-
Cristina Espana-Bonet, Adám Csaba Varga, Alberto
tationalLinguistics.
Barrón-Cedeño, and Josef van Genabith. 2017. An
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez- empirical analysis of nmt-derived interlingual em-
Gazpio, and Lucia Specia. 2017. SemEval-2017 beddings and their use in parallel sentence identifi-
task1: Semantictextualsimilaritymultilingualand cation. IEEE Journal of Selected Topics in Signal
crosslingual focused evaluation. In Proceedings Processing,11(8):1340–1350.
of the 11th International Workshop on Semantic
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen
Evaluation(SemEval-2017),pages1–14,Vancouver,
Arivazhagan, and Wei Wang. 2022. Language-
Canada.AssociationforComputationalLinguistics.
agnostic BERT sentence embedding. In Proceed-
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, ings of the 60th Annual Meeting of the Association
Nicole Limtiaco, Rhomni St. John, Noah Constant, forComputationalLinguistics(Volume1: LongPa-
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, pers), pages 878–891, Dublin, Ireland. Association
Brian Strope, and Ray Kurzweil. 2018. Universal forComputationalLinguistics.
sentence encoder for English. In Proceedings of
the 2018 Conference on Empirical Methods in Nat- Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li,
uralLanguageProcessing: SystemDemonstrations, Xiaodong He, and Lawrence Carin. 2017. Learn-
pages 169–174, Brussels, Belgium. Association for ing generic sentence representations using convolu-
ComputationalLinguistics. tional neural networks. In Proceedings of the 2017
386
Conference on Empirical Methods in Natural Lan- Diederik Kingma and Jimmy Ba. 2014. Adam: A
guage Processing, pages 2390–2400, Copenhagen, method for stochastic optimization. arXiv preprint
Denmark. Association for Computational Linguis- arXiv:1412.6980.
tics.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Richard Zemel, Raquel Urtasun, Antonio Torralba,
SimCSE: Simple contrastive learning of sentence and Sanja Fidler. 2015. Skip-thought vectors. In
embeddings. InProceedingsofthe2021Conference AdvancesinNeuralInformationProcessingSystems
onEmpiricalMethodsinNaturalLanguageProcess- 28,pages3294–3302.
ing, pages6894–6910, OnlineandPuntaCana, Do-
KalpeshKrishna,JohnWieting,andMohitIyyer.2020.
minican Republic. Association for Computational
Reformulating unsupervised style transfer as para-
Linguistics.
phrasegeneration. InProceedingsofthe2020Con-
FrancisGrégoireandPhilippeLanglais.2018. Extract- ferenceonEmpiricalMethodsinNaturalLanguage
ing parallel sentences with bidirectional recurrent Processing(EMNLP),pages737–762,Online.Asso-
neuralnetworkstoimprovemachinetranslation. In ciationforComputationalLinguistics.
Proceedingsofthe27thInternationalConferenceon
TakuKudoandJohnRichardson.2018. SentencePiece:
ComputationalLinguistics,pages1442–1453,Santa
A simple and language independent subword tok-
Fe, New Mexico, USA. Association for Computa-
enizeranddetokenizerforneuraltextprocessing. In
tionalLinguistics.
Proceedings of the 2018 Conference on Empirical
Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Methods in Natural Language Processing: System
Ge, Daniel Cer, Gustavo Hernandez Abrego, Keith Demonstrations, pages 66–71, Brussels, Belgium.
Stevens, Noah Constant, Yun-Hsuan Sung, Brian AssociationforComputationalLinguistics.
Strope, and Ray Kurzweil. 2018. Effective parallel
Quoc V. Le and Tomas Mikolov. 2014. Distributed
corpusminingusingbilingualsentenceembeddings.
representations of sentences and documents. arXiv
InProceedingsoftheThirdConferenceonMachine
preprintarXiv:1405.4053.
Translation:ResearchPapers,pages165–176,Brus-
sels, Belgium. Association for Computational Lin- Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale
guistics. Minervini,HeinrichKüttler,AleksandraPiktus,Pon-
tusStenetorp,andSebastianRiedel.2021. PAQ:65
Felix Hill, Kyunghyun Cho, and Anna Korhonen.
millionprobably-askedquestionsandwhatyoucan
2016. Learning distributed representations of sen-
do with them. Transactions of the Association for
tences from unlabelled data. In Proceedings of the
ComputationalLinguistics,9:1098–1115.
2016ConferenceoftheNorthAmericanChapterof
the Association for Computational Linguistics: Hu- Pierre Lison and Jörg Tiedemann. 2016. OpenSub-
manLanguageTechnologies,pages1367–1377,San titles2016: Extracting large parallel corpora from
Diego, California. Association for Computational movieandTVsubtitles. InProceedingsoftheTenth
Linguistics. International Conference on Language Resources
and Evaluation (LREC’16), pages 923–929, Por-
Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
torož, Slovenia. European Language Resources As-
Zettlemoyer.2018. Adversarialexamplegeneration
sociation(ELRA).
with syntactically controlled paraphrase networks.
InProceedingsofthe2018ConferenceoftheNorth YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey
American Chapter of the Association for Computa- Edunov, Marjan Ghazvininejad, Mike Lewis, and
tional Linguistics: Human Language Technologies, Luke Zettlemoyer. 2020. Multilingual denoising
Volume 1 (Long Papers), pages 1875–1885, New pre-trainingforneuralmachinetranslation. Transac-
Orleans, Louisiana. Association for Computational tions of the Association for Computational Linguis-
Linguistics. tics,8:726–742.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
Billion-scale similarity search with GPUs. arXiv dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
preprintarXiv:1702.08734. Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
ArmandJoulin,EdouardGrave,PiotrBojanowski,and
proach. arXivpreprintarXiv:1907.11692.
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con- Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi.
ferenceoftheEuropeanChapteroftheAssociation 2017. Unsupervised learning of sentence embed-
forComputationalLinguistics: Volume2, ShortPa- dings using compositional n-gram features. arXiv
pers, pages 427–431, Valencia, Spain. Association preprintarXiv:1703.02507.
forComputationalLinguistics.
Nghia The Pham, Germán Kruszewski, Angeliki
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Lazaridou, and Marco Baroni. 2015. Jointly opti-
Zettlemoyer,andMikeLewis.2019. Generalization mizing word representations for lexical and senten-
through memorization: Nearest neighbor language tial tasks with the C-PHRASE model. In Proceed-
models. arXivpreprintarXiv:1911.00172. ings of the 53rd Annual Meeting of the Association
387
for Computational Linguistics and the 7th Interna- JohnWieting,MohitBansal,KevinGimpel,andKaren
tional Joint Conference on Natural Language Pro- Livescu.2016a. Charagram: Embeddingwordsand
cessing (Volume 1: Long Papers), pages 971–981, sentencesviacharactern-grams. InProceedingsof
Beijing, China. Association for Computational Lin- the2016ConferenceonEmpiricalMethodsinNatu-
guistics. ralLanguageProcessing,pages1504–1515,Austin,
Texas.AssociationforComputationalLinguistics.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT:SentenceembeddingsusingSiameseBERT- JohnWieting,MohitBansal,KevinGimpel,andKaren
networks. InProceedingsofthe2019Conferenceon Livescu.2016b. Towardsuniversalparaphrasticsen-
EmpiricalMethodsinNaturalLanguageProcessing tence embeddings. In Proceedings of the Interna-
andthe9thInternationalJointConferenceonNatu- tionalConferenceonLearningRepresentations.
ralLanguageProcessing(EMNLP-IJCNLP),pages
3982–3992, Hong Kong, China. Association for JohnWieting,TaylorBerg-Kirkpatrick,KevinGimpel,
ComputationalLinguistics. andGrahamNeubig.2019a. BeyondBLEU:training
neuralmachinetranslationwithsemanticsimilarity.
Holger Schwenk. 2018. Filtering and mining paral- In Proceedings of the 57th Annual Meeting of the
lel data in a joint multilingual space. In Proceed- Association for Computational Linguistics, pages
ings of the 56th Annual Meeting of the Association 4344–4355,Florence,Italy.AssociationforCompu-
forComputationalLinguistics(Volume2: ShortPa- tationalLinguistics.
pers), pages 228–234, Melbourne, Australia. Asso-
ciationforComputationalLinguistics. John Wieting and Kevin Gimpel. 2017. Revisiting re-
current networks for paraphrastic sentence embed-
Holger Schwenk, Vishrav Chaudhary, Shuo Sun, dings. InProceedingsofthe55thAnnualMeetingof
HongyuGong,andFranciscoGuzmán.2021. Wiki- the Association for Computational Linguistics (Vol-
Matrix: Mining 135M parallel sentences in 1620 ume1: LongPapers),pages2078–2088,Vancouver,
language pairs from Wikipedia. In Proceedings of Canada.AssociationforComputationalLinguistics.
the16thConferenceoftheEuropeanChapterofthe
Association for Computational Linguistics: Main John Wieting and Kevin Gimpel. 2018. ParaNMT-
Volume, pages 1351–1361, Online. Association for 50M:Pushingthelimitsofparaphrasticsentenceem-
ComputationalLinguistics. beddings with millions of machine translations. In
Proceedings of the 56th Annual Meeting of the As-
Holger Schwenk and Matthijs Douze. 2017. Learn- sociation for Computational Linguistics (Volume 1:
ing joint multilingual sentence representations with LongPapers),pages451–462,Melbourne,Australia.
neural machine translation. In Proceedings of the AssociationforComputationalLinguistics.
2ndWorkshoponRepresentationLearningforNLP,
pages157–167,Vancouver,Canada.Associationfor JohnWieting,KevinGimpel,GrahamNeubig,andTay-
ComputationalLinguistics. lor Berg-Kirkpatrick. 2019b. Simple and effective
paraphrasticsimilarityfromparalleltranslations. In
Richard Socher, Eric H. Huang, Jeffrey Pennington, Proceedingsofthe57thAnnualMeetingoftheAsso-
Andrew Y. Ng, and Christopher D. Manning. 2011. ciation for Computational Linguistics, pages 4602–
Dynamic pooling and unfolding recursive autoen- 4608,Florence,Italy.AssociationforComputational
codersforparaphrasedetection. InAdvancesinNeu- Linguistics.
ralInformationProcessingSystems.
JohnWieting, JonathanMallinson, andKevinGimpel.
Sandeep Subramanian, Adam Trischler, Yoshua Ben- 2017. Learning paraphrastic sentence embeddings
gio, and Christopher J Pal. 2018. Learning gen- from back-translated bitext. In Proceedings of the
eralpurposedistributedsentencerepresentationsvia 2017 Conference on Empirical Methods in Natural
large scale multi-task learning. arXiv preprint LanguageProcessing,pages274–285,Copenhagen,
arXiv:1804.00079. Denmark. Association for Computational Linguis-
tics.
Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In Proceedings of the Eighth In- John Wieting, Graham Neubig, and Taylor Berg-
ternationalConferenceonLanguageResourcesand Kirkpatrick. 2020. A bilingual generative trans-
Evaluation (LREC’12), pages 2214–2218, Istanbul, former for semantic sentence embedding. In Pro-
Turkey. European Language Resources Association ceedingsofthe2020ConferenceonEmpiricalMeth-
(ELRA). ods in Natural Language Processing (EMNLP),
pages 1581–1594, Online. Association for Compu-
ChauTran,YuqingTang,XianLi,andJiataoGu.2020. tationalLinguistics.
Cross-lingual retrieval for iterative self-supervised
training. arXivpreprintarXiv:2006.09526. YukunZhu,RyanKiros,RichZemel,RuslanSalakhut-
dinov,RaquelUrtasun,AntonioTorralba,andSanja
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Fidler.2015. Aligningbooksandmovies: Towards
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz story-like visual explanations by watching movies
Kaiser, and Illia Polosukhin. 2017. Attention is all andreadingbooks. InTheIEEEInternationalCon-
you need. In Advances in Neural Information Pro- ferenceonComputerVision(ICCV).
cessingSystems,pages5998–6008.
388

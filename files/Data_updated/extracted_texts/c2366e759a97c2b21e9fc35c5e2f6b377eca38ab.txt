Transformers are Adaptable Task Planners
VidhiJain1,2 YixinLin1 EricUndersander1 YonatanBisk2
AksharaRai1
1MetaAI,2CarnegieMellonUniversity
Abstract: Every home is different, and every person likes things done in their
particular way. Therefore, home robots of the future need to both reason about
thesequentialnatureofday-to-daytasksandgeneralizetouser’spreferences. To
thisend,weproposeaTransformerTaskPlanner(TTP)thatlearnshigh-levelac-
tions from demonstrations by leveraging object attribute-based representations.
TTP can be pre-trained on multiple preferences and shows generalization to un-
seen preferences using a single demonstration as a prompt in a simulated dish-
washer loading task. Further, we demonstrate real-world dish rearrangement
using TTP with a Franka Panda robotic arm, prompted using a single human
demonstration. Code: https://anonymous.4open.science/r/temporal_
task_planner-Paper148/
Keywords: TaskPlanning,Prompt,Preferences,Object-centricRepresentation
1 Introduction
Considerarobottaskedwithloadingadishwasher. Sucharobothastoaccountfortaskconstraints
(e.g. only an open dishwasher rack can be loaded), and dynamic environments (e.g. more dishes
mayarriveoncetherobotstartsloadingthedishwasher).Dishwasherloadingisacanonicalexample
of personal preferences, where everyone has a different approach which the robot should adapt
to. Classicaltaskplanningdealswithtaskconstraintsthroughsymbolictaskdescription, butsuch
descriptionsaredifficulttodesignandmodifyfornewpreferencesincomplextasks.Buildingeasily
adaptablelong-horizontaskplans,underconstraintsanduncertainty,isanopenprobleminrobotics.
Machine learning (ML) enables learning complex tasks without extensive expert intervention:
robotic navigation [1, 2, 3], in-hand manipulation [4, 5, 6, 7, 8], and planning [9, 10, 11, 12, 13].
Withintaskplanning,MLisusedtoreplaceuser-definedsymbolicdescriptions[14],dealwithun-
certainty [15], and adapt to preferences [16]. Recent work [17] has shown Transformer networks
[18] can learn temporally-consistent representations, and exhibit generalization to new scenarios
[19,20,21]. Ourcentralquestionis: CanaTransformernetworklearntaskstructure,adapttouser
preferences,andachievecomplexlong-horizontasksusingnosymbolictaskrepresentations?
Wehypothesizethattaskstructureandpreferenceareimplicitlyencodedindemonstrations. When
loading a dishwasher, a user pulls out a rack before loading it, inherently encoding a structural
constraint. They may place mugs on the top rack and plates on the bottom, encoding their prefer-
ence. Learninguserpreferencesfromlong-horizondemonstrationsrequirespolicieswithtemporal
context. For example, a user might prefer to load the top rack before the bottom tray. The policy
needstoconsiderthesequenceofactionsdemonstrated,ratherthanindividualactions.Transformers
arewell-suitedtothisproblem,astheyhavebeenshowntolearnlong-rangerelationships[22],al-
thoughnotintemporalrobotictasks.WeproposeTransformerTaskPlanner(TTP)-anadaptationof
aclassictransformerarchitecturethatincludes temporal-, pose- and category-specificembeddings
tolearnobject-orientedrelationshipsoverspaceandtime.TTPgeneralizesbeyondwhatwasseenin
demonstrations–tovariablenumbersofobjectsanddynamicenvironments.Bypre-trainingTTPon
multiplepreferences,webuildtemporalrepresentationsthatcanbegeneralizedtonewpreferences.
The main contributions of our work are: (1) Introduce transformers as a promising architecture
forlearningtaskplansfromdemonstrations,usingobject-centricembeddings(2)Demonstratethat
preferenceconditionedpre-traininggeneralizesattesttimetonew,unseenpreferenceswithasingle
2202
luJ
6
]OR.sc[
1v24420.7022:viXra
Figure 1: (Left-to-Right, Top-to-bottom) A Franka Emika Panda arm organizing 4 dishes into 2
drawers, following preference shown in a demonstration. The robot opens a top drawer, places
objectsinthetopdrawer,closesit,anddoesthesameforthebottomdrawer. Thehigh-levelpolicy
thatmakesdecisionsaboutwhentoopendrawers,whatobjectstopick,etcislearnedinsimulation
andtransferredzero-shottothereal-world.
user demonstration. Our experiments use a complex high-dimensional dishwasher loading envi-
ronment (Fig. 4) with several challenges: complex task structure, dynamically appearing objects
andhuman-specificpreferences. TTPsuccessfullylearnsthistaskfromsevenpreferenceswith80
demonstrationseach,andgeneralizestounseenscenesandpreferencesandoutperformscompetitive
baselines[23,24]. Finally,wetransferTTPtoarearrangementprobleminthereal-world,wherea
Frankaarmplacesdishesintwodrawers,usingasinglehumandemonstration(Fig. 1).
2 TransformerTaskPlanner(TTP)
We introduce TTP, a Transformer-based policy architecture for learning sequential manipulation
tasks. We assume low-level ‘generalized’ pick-place primitive actions that apply to both objects
likeplates,bowls,andalsotodishwasherdoor,racks,etc. TTPlearnsahigh-levelpolicyforpick-
placeinaccordancewiththetaskstructureandpreferenceshownindemonstrations. Thefollowing
sections are described with dishwasher-loading as an example, but our setup is applicable to most
long-horizonmanipulationtaskswith‘generalized’pick-place.
State-ActionRepresentations Weconsiderahigh-levelpolicythatinteractswiththeenvironment
atdiscretetimesteps. Ateverytimestept,wereceiveobservationo fromtheenvironmentwhichis
t
passedthroughaperceptionpipelinetoproduceasetofrigid-bodyinstances{x }n ,corresponding
i i=1
tothenobjectscurrentlyvisible. Weexpressaninstanceas: x = {p ,c ,t},wherep isthepose
i i i i
of the object, c is its category, and t is the timestep while recording o (Fig. 2a). For example,
i t
for a bowl at the start of an episode, p is its current location w.r.t a global frame, c is its category
(bowl),andt = 0. ThepickstateSpick isdescribedasthesetofinstancesvisibleino :Spick =
t t t
{x ,x ,··· ,x }. Spick ispassedtoalearnedpolicyπtopredictapickactiona . Wedescribethe
0 1 n t t
actiona intermsofwhattopickandwheretoplaceit. Specifically, thepickactionchoosesone
t
instancefromx observedino : π(Spick)→x , wherex ∈{x }n .
i t target target i i=1
Once a pick object is chosen, a similar procedure determines where to place it. We pre-compile
alistofdiscreteplacementposescorrespondingtoviableplacelocationsforeachobject-category.
These poses densely cover the dishwasher racks, and are created by randomly placing objects in
thedishwasherandmeasuringthefinalposetheylandin. Allpossibleplacementlocationsforthe
pickedobjectcategory,whetherfreeoroccupied,areusedtocreateasetofplaceinstances{g }l .
j j=1
Similartox ,g = {p ,c ,t},consistsofapose,categoryandtimestep. Abooleanvaluer inthe
i j j j
attributesetdistinguishesthetwoinstancetypes. TheplacestateisSplace =Spick∪{g }l .The
j j=1
samepolicyπthenchooseswheretoplacetheobject(Fig.2).Notethattheinputforpredictingplace
includes both objects and place instances, since the objects determine whether a place instance is
freetoplaceornot.x andg togethermakeactiona,senttoalow-levelpick-placepolicy.
target target
ThepolicyπismodeledusingaTransformer[18].
2
(a)ScenetoInstanceembeddings
(b)Instancestoprediction
Figure 2: (Left) Architecture overview of how a scene is converted to a set of instances. Each
instance is comprised of attributes, i.e. pose, category, timestep, and whether it is an object or
placeinstance. (Right)Instanceattributes(gray)arepassedtotheencoder, whichreturnsinstance
embeddingsforpickableobjects(red),placeblelocations(green),and<ACT>embeddings(blue).
Thetransformeroutputsachosenpick(red)andplace(green)instanceembedding.
Instance Encoder Each object and goal instance x , g is projected into a higher-dimensional
i j
vector space (Fig. 2a). Such embeddings improve the performance of learned policies [18, 25].
FortheposeembeddingΦ ,weuseapositionalencodingschemesimilartoNeRF[25]toencode
p
the3Dpositionalcoordinatesand4Dquaternionrotationofaninstance. Forcategory, weusethe
dimensions of the 3D bounding box of the object to build a continuous space of object types and
process this through an MLP Φ . For each discrete 1D timestep, we model Φ as a learnable em-
c t
beddinginalookuptable,similartothepositionalencodingsinBERT[26]. Toindicatewhetheran
instanceisanobjectorplacementlocation,weadda1Dbooleanvaluevectorizedusingalearnable
embeddingfunctionΦ . Theconcatenatedd-dimensionalembeddingforaninstanceattimestept
r
isrepresentedasf (x ) = Φ ||Φ ||Φ ||Φ = e . Theencodedstateattimetcanberepresentedin
e i t c p r i
termsofinstancesas: Senc =[e ,e ,···e ]. Wedrop()encforbrevity.
t 0 1 N
Demonstrations are state-action sequences C = {(S ,a ),(S ,a ),··· ,(S ,a ),(S )}.
0 0 1 1 T−1 T−1 T
HereS isthesetofobjectandplaceinstancesanda arethepick-placeactionschosenbyexpertat
i i
timei. Ateverytimestep,werecordthestateoftheobjects,thepickinstancechosenbytheexpert,
placeinstancesforthecorrespondingcategory,andtheplaceinstancechosenbytheexpert. Expert
actions are assumed to belong to the set of visible instances in S . However, different experts can
i
exhibit different preferences over a . For example, one expert might choose to load bowls first in
i
thetoprack,whileanotherexpertmightloadbowlslastinthebottomrack. Inthetrainingdataset,
weassumelabelsforwhichpreferenceeachdemonstrationbelongsto,basedontheexpertusedfor
collectingthedemonstration. GivenK demonstrationsperpreferencem ∈ M, wehaveadataset
forpreferencem: D = {C ,··· ,C }. Thecompletetrainingdatasetconsistsofdemonstrations
m 1 K
from all preferences: D =
(cid:83)M
D . During training, we learn a policy that can reproduce all
m=1 m
demonstrationsinourdataset. Thisischallenging, sincetheactionstakenbydifferentexpertsare
differentforthesameinput,andthepolicyneedstodisambiguatethepreference. Attesttime,we
generalizethepolicytobothunseenscenesandunseenpreferences.
2.1 Prompt-SituationTransformer
We use Transformers [18], a deep neural network that operates on sequences of data, for learn-
ing a high-level pick-place policy π. The input to the encoder is a d-dimensional token1 per in-
stance e ∈ [1,..N] in the state S. In addition to instances, we introduce special <ACT> to-
i
kens2,azerovectorforallattributes,todemarcatetheendofonestateandstartofthenext. These
tokens help maintain the temporal structure; all instances between two <ACT> tokens in a se-
(cid:2)
quence represent one observed state. A trajectory τ, without actions, is: τ = S ,<ACT>
t=0
(cid:3)
,··· ,S ,<ACT>,S . To learn a common policy for multiple preferences, we propose
t=T−1 t=T
1Terminologyborrowedfromnaturallanguageprocessingwheretokensarewords;here,theyareinstances.
2Similarto<CLS >tokensusedforsentenceclassification.
3
a prompt-situation architecture (Fig. 3). The prompt encoder receives one demonstration trajec-
tory as input, and outputs a learned representation of the preference. These output prompt tokens
are input to a situation decoder, which also receives the current state as input. The decoder π is
trained to predict the action chosen by the expert for the situation, given a prompt demonstration.
The left half is prompt encoder ψ and the right half
is the situation decoder or policy π acting on given
state. The prompt encoder ψ : f ◦f ◦f con-
slot te e
sists of an instance encoder f , transformer encoder
e Slot Attention
f , andaslot-attentionlayerf [27]. ψ takesthe
te slot
whole demonstration trajectory τ as input and
prompt Transformer Decoder
returns a fixed and reduced preference embedding
Transformer Encoder
γ =ψ(τ )ofsequencelengthH.Slotattentionis
prompt
aninformationbottleneck,whichlearnssemantically
meaningfulrepresentationsoftheprompt.
Instance Encoder Instance Encoder
The situation decoder is a policy π : f ◦ f that
td e
receivesasinputN instancetokensfromthecurrent Prompt Situation
sceneS, consistingofobjects, aswellas, placement
Figure 3: Prompt-Situation Architecture.
instances separated by <ACT> tokens (Fig. 2b).
The left is the prompt encoder which takes
Thepolicyarchitectureisatransformerdecoder[18]
as input a prompt demonstration and out-
with self-attention layers over the N input tokens.
puts a learned preference embedding. The
Thisisfollowedbyacross-attentionlayerwithpref-
right half is the situation decoder, which
erenceembeddingγ (H tokens)fromtheprompten-
conditionedonpreferenceembeddingfrom
coder.Weselecttheoutputofthesituationdecoderat
promptencoder,actsonthecurrentstate.
the<ACT>tokenandcalculatedot-productsimilar-
itywiththeN inputtokense . Thetokenwiththemaximumdot-productischosenasthepredicted
i (cid:0) (cid:1)
instance: x = max xˆ ·e . Thetrainingtargetisextractedfromthedemon-
pred ei,i∈{1,N} <ACT> i
stration dataset D, and the policy trained with cross-entropy to maximize the similarity of output
latentwiththeexpert’schoseninstanceembedding.
2.2 Multi-PreferenceTaskLearning
We adopt a prompt-situation architecture for multi-preference learning. This design (1) enables
multi-preferencetrainingbydisambiguatingpreferences,(2)learnstask-levelrulessharedbetween
preferences (e.g. dishwasher should be open before placing objects), (3) can generalize to unseen
preferencesattesttime,withoutfine-tuning. Givena‘prompt’demonstrationofpreferencem,our
policy semantically imitates it in a different ‘situation’ (i.e. a different initialization of the scene).
Tothisend,welearnarepresentationofthepromptγmwhichconditionsπtoimitatetheexpert.
ψ(τm )→γm (1)
prompt
π(S |γm)→am ={xm ,gm } (2)
situation t pred pred
Wetrainneuralnetworksψandπtogethertominimizethetotalpredictionlossoverallpreferences
usingthemulti-preferencetrainingdatasetD. Theoverallobjectivebecomes:
min L (a,π(S,ψ(τ))) (3)
CE
m∼M,τ∼Dm,(S,a)∼Dm
Foreverypreferenceminthedataset,wesampleademonstrationfromD anduseitasaprompt
m
forallstate-actionpairs(S,a)inD . Thisincludesthestate-actionpairsfromτ andcreates
m prompt
a combinatorially large training dataset. At test time, we record one prompt demo from a seen or
unseenpreferenceanduseittoconditionπandψ:a=π(S,ψ(τ )).Allpolicyweightsarekept
prompt
fixedduringtesting,andgeneralizationtonewpreferencesiszero-shotusingthelearnedpreference
representation γ. Unlike [24], γ captures not just the final state, but a temporal representation of
the whole demonstration. Building a temporal representation is crucial to encode demonstration
preferenceslikeorderofloadingracksandobjects. Eventhoughthefinalstateisthesamefortwo
preferencesthatonlydifferinwhichrackisloadedfirst,ourapproachisabletodistinguishbetween
them using the temporal information in τ . To the best of our knowledge, our approach is the
prompt
firsttotemporallyencodepreferencesinferredfromademonstrationinlearnedtaskplanners.
4
Figure 4: Dishwasher Loading demonstration in AI Habitat Kitchen Arrange Simulator. Objects
dynamicallyappearonthecounter-top(ii-iv),andneedtobeplacedinthedishwasher. Ifthedish-
washerracksarefull,theylandinsink(v)
3 Experiments
We present the “Replica Synthetic Apartment 0 Kitchen”3 (see figure 4, appendix and video), an
artist-authored interactive recreation of the kitchen of the “Apartment 0” space from the Replica
dataset[28]. WeuseselectedobjectsfromtheReplicaCAD[29]dataset, includingseventypesof
dishes, and generate dishwasher loading demonstrations using an expert-designed data generation
script (see Appendix B). Given 7 categories of dishes and two choices in which rack to load first,
the hypothesis space of possible preferences is 2×7!. Our dataset consists of 12 preferences (7
train, 5held-outtest)with100sessionsperpreference. Inasession, n ∈ {3,...,10}instancesare
loadedineachrack. Thetrainingdataconsistsofsessionswith6or7objectsallowedperrack. The
held-out test set contains 5 unseen preferences and settings for {3,4,5,8,9,10} objects per rack.
Additionally, to simulate a dynamic environment, we randomly initialize new objects mid-session
onthekitchencounter. Thissimulatessituationswherethepolicydoesnothavefullinformationof
everyobjecttobeloadedatthestartofthesession,andhastolearntobereactivetonewinformation.
Wetraina2-head2-layerTransformerencoder-decoderwith256inputand512hiddendimensions,
and50slotsand3iterationsforSlotAttention(moredetailsinAppendixC).Wetestbothin-and
out-of-distributionperformanceinsimulation. Forin-distributionevaluation, 10sessionsareheld-
out for testing for each training preference. For out-of-distribution evaluation, we create sessions
with unseen preferences and unseen number of objects. We evaluate trained policies on ‘rollouts’
in the simulation, a more complex setting than accuracy of prediction. Rollouts require repeated
decisions in the environment, without any resets. A mistake made early on in a rollout session
can be catastrophic, and result in poor performance, even if the prediction accuracy is high. For
example, if a policy mistakenly fails to open a dishwasher rack, the rollout performance will be
poor, despite good prediction accuracy. To measure success, we rollout the policy from an initial
stateandcomparetheperformanceofthepolicywithanexpertdemonstrationfromthesameinitial
state. Notethatthepolicydoesnothaveaccesstotheexpertdemonstration,andthedemonstration
isonlyusedforevaluation. Specifically,wemeasure(1)howwellisthefinalstatepackedand(2)
howmuchdidthepolicydeviatefromtheexpertdemonstration?
Packingefficiency:Wecomparethenumberofobjectsplacedinthedishwasherbythepolicytothat
intheexpert’sdemonstration. Leta benumberofobjectsintoprackandb beobjectsonbottom
i i
rackplacedbytheexpertintheithdemonstration. Ifthepolicyadherestothepreferenceandplaces
(cid:16)
aˆ i andˆb i onthetopandbottomrespectively,thenthepackingefficiency(PE) = (cid:80) i max(aˆ aˆi i,ai) +
(cid:17)
ˆbi
.Packingefficiencyisbetween0to1,andhigherisbetter.Notethatifthepolicyfollows
max(ˆbi,bi)
thewrongpreference,thenPEis0,evenifthedishwasherisfull.
InverseEditdistance:Wealsocalculateistheinverseeditdistancebetweenthesequenceofactions
takenbytheexpertversusthelearnedpolicy. WecomputetheLevenshteindistance4 (LD)between
the policy’s and expert’s sequence of pick and place instances. Inverse edit distance is defined as
ED = 1−LD;higherisbetter. Thismeasuresthetemporaldeviationfromtheexpert,insteadof
justthefinalstate. Iftheexpertpreferencewastoloadthetoprackfirstandthelearnedpolicyloads
thebottomfirst,PE wouldbeperfect,butinverseeditdistancewouldbelow.
3“ReplicaSyntheticApartment0Kitchen”wascreatedwiththeconsentofandcompensationtoartists,and
willbesharedunderaCreativeCommonslicensefornon-commercialusewithattribution(CC-BY-NC).
4Levenshtein distance= textdistance(learned seq, expert seq) / len(expert seq)
5
(a)Packingefficiency (b)InverseEditDistance
Figure5: ComparisonsofTTP,GNN-IL/RLandaRandompolicyinsimulationacrosstwometrics.
TTPshowsgoodperformanceatthetaskofdishwasherloadingonseenandunseenpreferences,and
outperformstheGNNandrandombaselines. HoweverTTP’sgeneralizationtounseen#objectsif
worse,butstillclosetoGNN-IL,andbetterthanGNN-RLandrandom.
3.1 Evaluationonsimulateddishwasherloading
Baselines: We compare our approach against Graph Neural Network (GNN) based preference
learning from [23] and [24]. Neither of these works are directly suitable for our task, so we com-
bine them to create a stronger baseline. We use ground-truth preference labels, represented as a
categoricaldistributionandaddthemtotheinputfeaturesoftheGNN,similarto[24]. Forunseen
preferences,thisisprivilegedinformationthatourapproachdoesnothaveaccessto. Weusethe
output of the GNN to make sequential predictions, as in [23]. Thus, by combining the two works
andaddingprivilegedinformationaboutground-truthpreferencecategory,wecreateaGNNbase-
line which can act according to preference in a dishwasher loading scenario. We train this policy
usingimitationlearning(IL),andreinforcementlearning(RL),following[23]. GNN-IListrained
fromthesamesetofdemonstrationsasTTPusingbehaviorcloning.ForGNN-RL,weuseProximal
Policy Optimization [30] from [31]. GNN-RL learns from scratch by directly interacting with the
dishwasherenvironmentandobtainingadensereward. Fordetails,seeAppendixC.1.
GNN-IL does not reach the same performance as TTP for in-distribution tasks (PE of 0.34 for
GNN-ILvs0.62forTTP).Notethatunseenpreferencesarealsoin-distributionforGNN-ILsince
we provide ground-truth preference labels to the GNN. Hence, there isn’t a drop in performance
for unseen preferences for GNN, unlike TTP. Despite having no privileged information, TTP out-
performsGNN-ILinunseenpreferences,andperformscomparablyonunseen#objects. Duetothe
significantly long time horizons per session (more than 30 steps), GNN-RL fails to learn a mean-
ingfulpolicyevenafteralargebudgetof32,000environmentinteractionsandadensereward(PE
0.017±0.002usingGNN-RL).Lastly, wefindthattherandompolicyRPisnotabletosolvethe
task at all due to the large state and action space. TTP is able to solve dishwasher loading using
unseenpreferenceswell(PE0.54). Incontrast,classicaltaskplannerslike[32]needtobeadapted
per new preference. This experiment shows that Transformers make adaptable task planners, us-
ing our proposed prompt-situation architecture. However, TTP’s performance on unseen #objects
deteriorates(0.62forseenversus0.34onunseen#objects),andwelookmorecloselyatthatnext.
Generalizationtounseen#objects:Fig.7aexaminesPEonout-of-distributionsessionswithlesser
i.e. 3-5ormorei.e. 8-10objectsperrack. Thetrainingsetconsistsofdemonstrationswith6or7
objectsperrack.Thepolicyperformswellon5-7objects,butpooreraswegofurtherawayfromthe
trainingdistribution. PoorerPEisexpectedforlargernumberofobjects,astheactionspaceofthe
policyincreases,andthepolicyismorelikelytopickthewrongobjecttypeforagivenpreference.
Poorer performance on 3-4 objects is caused by the policy closing the dishwasher early, as it has
neverseenthisstateduringtraining. Trainingwithricherdatasets,andaddingmorerandomization
intheformofobjectmaskingmightimproveout-of-distributionperformanceofTTP.
3.2 Real-worlddish-rearrangementexperiments
We zero-shot transfer our policy trained in simulation to robotic hardware, by assuming low-level
controllers. We use a Franka Panda equipped with a Robotiq 2F-85 gripper, controlled using the
Polymetis control framework [33]. For perception, we find the extrinsics of three Intel Realsense
D435 RGBD cameras [34] using ARTags [35]. The camera output, extrinsics, and intrinsics are
6
Figure6: PipelineforRealHardwareExperiments
combinedusingOpen3D[36]andfedintoasegmentationpipeline[37]togenerateobjectcategories.
Forlow-levelpick,weuseagraspcandidategenerator[38]appliedtotheobjectpointcloud,anduse
ittograspthetargetobject. Placeisapproximatedasa‘drop’actioninapre-definedlocation. Our
hardware setup mirrors our simulation, with different categories of dishware (bowls, cups, plates)
onatable, a“dishwasher”(cabinetwithtwodrawers). Theobjectiveistoselectanobjecttopick
andplaceitintoadrawer(rack)(seeFig. 1).
We use a policy trained in simulation and apply it to a scene with four objects (2 bowls, 1 cup,
1 plate) through the hardware pipeline described above. We start by collecting a prompt human
demonstration(moredetailsinAppendixA).,Thelearnedpolicy,conditionedapromptdemonstra-
tion,isappliedtotwovariationsofthesamescene,andthepredictedactionsexecuted. Thepolicy
wassuccessfuloncewith100%successrate,andoncewith75%,showninFigure1,bottom. The
failurecasewascausedduetoaperceptionerror–abowlwasclassifiedasaplate.Thisdemonstrates
thatsuchTTPcanbetrainedinsimulationandapplieddirectlytohardware. Thepolicyisrobustto
minorhardwareerrors,suchasifabowlgraspfails,itjustrepeatsthegraspingaction(seevideoand
AppendixA).However,itreliesonaccurateperceptionofthestate. Inthefuture,wewouldliketo
furtherevaluateourapproachonmorediversereal-worldsettingsandmeasureitssensitivitytothe
differenthardwarecomponents,informingfuturechoicesforlearningrobustpolicies.
3.3 Ablations
(a) Performance decays for OOD(b)Performanceimproveswiththe (c)Performanceimproveswiththe
numberofobjects,i.e3-5&8-10 #ofuniquesessionsintraining #ofuniquepreferencesintraining
Figure7: (a)Out-of-distributiongeneralizationto#objects. (b-c)ablationexperiments.
Westudythesensitivityofourapproachtotraininghyperparameters. First,wevarythenumberof
trainingsessionsperpreferenceandstudygeneralizationtounseenscenariosofthesamepreference.
Figure7bshowsthattheperformanceofTTPimprovesasthenumberofdemonstrationsincrease,
indicatingthatourmodelisnotoverfittingtothetrainingset,andmightbenefitfromfurthertraining
samples. Next,wevarythenumberoftrainingpreferencesandevaluategeneralizationperformance
to unseen preferences. Figure 7c shows that the benefits of adding additional preferences beyond
4 are minor, and similar performance is observed when training from 4-7 preferences. This is an
7
interestingresult,sinceonewouldassumethatmorepreferencesimprovegeneralizationtounseen
preferences. But for the kinds of preferences considered in our problem, 4-7 preference types are
enoughforgeneralization.
Finally,weanalyzewhichinstanceattributesarethemostimportant Table1: AttributeAblations
forlearninginaobject-centricsequentialdecisionmakingtask. We
maskdifferentinstanceattributestoremovesourcesofinformation Pose Cat Time PE
from the instance tokens. As seen in Table 1, all components of × × × 0.0
ourinstancetokensplaysignificantrole(0.606withall,versusthe × × (cid:88) 0.0
nexthighestof0.517). Themostimportantattributeisposeofthe × (cid:88) × 0.027
objects (without the pose, top PE is 0.142), followed by the cate- × (cid:88) (cid:88) 0.142
gory. The timestep is the least important, but the best PE comes (cid:88) × × 0.411
fromcombiningallthree. (moredetailsinAppendixD.1). (cid:88) × (cid:88) 0.419
(cid:88) (cid:88) × 0.517
4 PriorWork (cid:88) (cid:88) (cid:88) 0.606
Object-centric representations for sequential manipulation
Several works build object-centric pick-place representations using off-the-shelf perception meth-
ods [39, 40, 41, 42, 43, 44]. Once estimated, object states are used by a task and motion planner
forsequentialmanipulation[32,45],butobjectsinthesceneareknown. [46]combinemotorlearn-
ingwithobject-centricrepresentation,buttransferofpoliciesischallenging. TransporterNets[47]
useavisualencoder-decoderfortable-topmanipulationtasks;SORNet[48]extractsobject-centric
representationsfromRGBimagesanddemonstratesgeneralizationinsequentialmanipulationtask.
Inspiredfromthese,welearnpoliciesfordishwasherloading,choosingfromvisibleobjectstomake
pick-placedecisions.
Transformers for sequence modeling Transformers in NLP [18] and vision [49] have focused
on self-supervised pretraining due to abundant unsupervised data available. Recent works have
repurposedtransformersforothersequencemodelingtasks[50,51,52,53,54,55].PromptDecision
Transformer[56]considersasinglemodeltoencodethepromptandthesuccessivesequenceofstate.
WeconsiderstateasvariablenumberofinstancePlaTe[51]proposesplanningfromvideos,while
[53,54,55]modelasequentialdecision-makingtask. Weconsiderlong-horizontaskswithpartially
observablestatefeatures,anduser-specificpreferences.
Preferences and prompt training In literature, there are several ways of encoding preferences.
[24]proposeVAEtolearnuserpreferencesforspatialarrangementbasedonjustthefinalstate,while
ourapproachmodelstemporalpreferencefromdemonstrations.Preference-basedRLlearnsrewards
basedonhumanpreferences[57,58,59,60],butdonotgeneralizetounseenpreferences. Oncom-
plex long-horizon tasks, modeling human preferences enables faster learning than RL, even with
carefullydesignedrewards[61]. Weshowgeneralizationtounseenpreferencesbyusingprompts.
Largelanguageandvisionmodelshaveshowngeneralizationthroughprompting[62,63]. Prompt-
ingcanalsobeusedtoguideamodeltoquicklyswitchbetweenmultipletaskobjectives[64,65,66].
Specifically, language models learn representations that be easy transferred to new tasks in a few-
shotsetting[67,68,19,62]. Ourapproachsimilarlyutilizespromptsforpreferencegeneralization
insequentialdecision-makingrobotictasks.
5 ConclusionsandLimitations
WepresentTranformerTaskPlanner(TTP):ahigh-level,sequential,preference-basedpolicyfrom
asingledemonstrationusingaprompt-situationarchitecture. Weintroducedasimulateddishwasher
loadingdatasetwithdemonstrationsthatadheretovaryingpreferences. TTPcansolveacomplex,
long-horizondishwasher-loadingtaskinsimulationandtransfertotherealworld.
Wehavedemonstratedthe TTP’sstrong performancein thedishwasher setting. This environment
isbothcomplexbyvirtueofitsstrictsequentialnatureandyetincompleteasweassumedoorsand
drawers can be easily opened and perception is perfect. In real settings, the policy needs to learn
how to recover from its low-level execution mistakes. More complex preferences may depend on
differences in visual or textural patterns on objects, for which the instance encoder would require
modifications to encode such attributes. An important question to address is how more complex
motion plans interact with or hinder the learning objective, especially due to different human and
8
robotaffordances. Finally,promptsareonlypresentedviademonstration,whilelanguagemightbe
amorenaturalinterfaceforusers.
References
[1] A.Kadian,J.Truong,A.Gokaslan,A.Clegg,E.Wijmans,S.Lee,M.Savva,S.Chernova,and
D.Batra.Sim2realpredictivity:Doesevaluationinsimulationpredictreal-worldperformance?
IEEERoboticsandAutomationLetters(RA-L),2020.
[2] J.Truong,D.Yarats,T.Li,F.Meier,S.Chernova,D.Batra,andA.Rai. Learningnavigation
skillsforleggedrobotswithlearnedrobotembeddings.InternationalConferenceonIntelligent
RobotsandSystems(IROS),2020.
[3] A. Kumar, Z. Fu, D. Pathak, and J. Malik. Rma: Rapid motor adaptation for legged robots.
Robotics: ScienceandSystems(RSS),2021.
[4] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly,
M.Kalakrishnan,V.Vanhoucke,etal.Qt-opt:Scalabledeepreinforcementlearningforvision-
basedroboticmanipulation(2018). arXivpreprintarXiv:1806.10293,2018.
[5] A. Nagabandi, K. Konolige, S. Levine, and V. Kumar. Deep dynamics models for learning
dexterousmanipulation. InConferenceonRobotLearning,pages1101–1112.PMLR,2020.
[6] F.Wirnshofer,P.S.Schmitt,G.vonWichert,andW.Burgard. Controllingcontact-richmanip-
ulationunderpartialobservability. InRobotics: ScienceandSystems,2020.
[7] Z.Qin,K.Fang,Y.Zhu,L.Fei-Fei,andS.Savarese. Keto: Learningkeypointrepresentations
for tool manipulation. In 2020 IEEE International Conference on Robotics and Automation
(ICRA),pages7278–7285.IEEE,2020.
[8] A.Simeonov, Y.Du, B.Kim, F.R.Hogan, J.Tenenbaum, P.Agrawal, andA.Rodriguez. A
long horizon planning framework for manipulating rigid pointcloud objects. arXiv preprint
arXiv:2011.08177,2020.
[9] G.Yang,A.Zhang,A.Morcos,J.Pineau,P.Abbeel,andR.Calandra.Plan2vec:Unsupervised
representationlearningbylatentplans.InLearningforDynamicsandControl,pages935–946.
PMLR,2020.
[10] K.Pertsch,Y.Lee,andJ.J.Lim.Acceleratingreinforcementlearningwithlearnedskillpriors.
arXivpreprintarXiv:2010.11944,2020.
[11] A.Singh,H.Liu,G.Zhou,A.Yu,N.Rhinehart,andS.Levine. Parrot: Data-drivenbehavioral
priorsforreinforcementlearning. arXivpreprintarXiv:2011.10024,2020.
[12] D. Driess, J.-S. Ha, and M. Toussaint. Deep visual reasoning: Learning to predict ac-
tion sequences for task and motion planning from an initial scene image. arXiv preprint
arXiv:2006.05398,2020.
[13] J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy
sketches. InInternationalConferenceonMachineLearning,pages166–175.PMLR,2017.
[14] T.Silver,R.Chitnis,N.Kumar,W.McClinton,T.Lozano-Perez,L.P.Kaelbling,andJ.Tenen-
baum. Inventingrelationalstateandactionabstractionsforeffectiveandefficientbilevelplan-
ning. arXivpreprintarXiv:2203.09634,2022.
[15] D.Gordon,D.Fox,andA.Farhadi. Whatshouldidonow? marryingreinforcementlearning
andsymbolicplanning. arXivpreprintarXiv:1901.01492,2019.
[16] R. Kaushik, T. Anne, and J.-B. Mouret. Fast online adaptation in robotics through meta-
learningembeddingsofsimulatedpriors. In2020IEEE/RSJInternationalConferenceonIn-
telligentRobotsandSystems(IROS),pages5269–5276.IEEE,2020.
[17] R. Kaplan, C. Sauer, and A. Sosa. Beating atari with natural language guided reinforcement
learning. arXivpreprintarXiv:1704.05539,2017.
9
[18] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polo-
sukhin. Attention is all you need. Advances in neural information processing systems, 30,
2017.
[19] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances
inneuralinformationprocessingsystems,33:1877–1901,2020.
[20] V.Sanh,A.Webson,C.Raffel,S.H.Bach,L.Sutawika,Z.Alyafeai,A.Chaffin,A.Stiegler,
T.L.Scao,A.Raja,etal. Multitaskpromptedtrainingenableszero-shottaskgeneralization.
arXivpreprintarXiv:2110.08207,2021.
[21] X.Liu,K.Ji,Y.Fu,Z.Du,Z.Yang,andJ.Tang.P-tuningv2:Prompttuningcanbecomparable
tofine-tuninguniversallyacrossscalesandtasks. arXivpreprintarXiv:2110.07602,2021.
[22] D.S.Chaplot,D.Pathak,andJ.Malik. Differentiablespatialplanningusingtransformers. In
InternationalConferenceonMachineLearning,pages1484–1495.PMLR,2021.
[23] Y.Lin,A.S.Wang,E.Undersander,andA.Rai. Efficientandinterpretablerobotmanipulation
withgraphneuralnetworks. IEEERoboticsandAutomationLetters,2022.
[24] I. Kapelyukh and E. Johns. My house, my rules: Learning tidying preferences with graph
neuralnetworks. InConferenceonRobotLearning,pages740–749.PMLR,2022.
[25] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:
Representingscenesasneuralradiancefieldsforviewsynthesis. InEuropeanconferenceon
computervision,pages405–421.Springer,2020.
[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
[27] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit,
A. Dosovitskiy, and T. Kipf. Object-centric learning with slot attention. Advances in Neu-
ralInformationProcessingSystems,33:11525–11538,2020.
[28] J.Straub,T.Whelan,L.Ma,Y.Chen,E.Wijmans,S.Green,J.J.Engel,R.Mur-Artal,C.Ren,
S.Verma,A.Clarkson,M.Yan,B.Budge,Y.Yan,X.Pan,J.Yon,Y.Zou,K.Leon,N.Carter,
J.Briales,T.Gillingham,E.Mueggler,L.Pesqueira,M.Savva,D.Batra,H.M.Strasdat,R.D.
Nardi,M.Goesele,S.Lovegrove,andR.Newcombe. TheReplicadataset: Adigitalreplicaof
indoorspaces. arXivpreprintarXiv:1906.05797,2019.
[29] A.Szot,A.Clegg,E.Undersander,E.Wijmans,Y.Zhao,J.Turner,N.Maestre,M.Mukadam,
D. Chaplot, O. Maksymets, A. Gokaslan, V. Vondrus, S. Dharur, F. Meier, W. Galuba,
A.Chang, Z.Kira, V.Koltun, J.Malik, M.Savva, andD.Batra. Habitat2.0: Traininghome
assistants to rearrange their habitat. In Advances in Neural Information Processing Systems
(NeurIPS),2021.
[30] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov.Proximalpolicyoptimization
algorithms. arXivpreprintarXiv:1707.06347,2017.
[31] A.Raffin,A.Hill,A.Gleave,A.Kanervisto,M.Ernestus,andN.Dormann.Stable-baselines3:
Reliable reinforcement learning implementations. Journal of Machine Learning Research,
2021.
[32] C.R.Garrett,R.Chitnis,R.Holladay,B.Kim,T.Silver,L.P.Kaelbling,andT.Lozano-Pe´rez.
Integratedtaskandmotionplanning. arXivpreprintarXiv:2010.01083,2020.
[33] Y. Lin, A. S. Wang, G. Sutanto, A. Rai, and F. Meier. Polymetis. https://
facebookresearch.github.io/fairo/polymetis/,2021.
[34] L.Keselman,J.IselinWoodfill,A.Grunnet-Jepsen,andA.Bhowmik. Intelrealsensestereo-
scopicdepthcameras. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognitionworkshops,pages1–10,2017.
10
[35] M.Fiala. Artag, afiducialmarker systemusingdigitaltechniques. In 2005IEEEComputer
SocietyConferenceonComputerVisionandPatternRecognition(CVPR’05),volume2,pages
590–596.IEEE,2005.
[36] Q.-Y.Zhou,J.Park,andV.Koltun. Open3d: Amodernlibraryfor3ddataprocessing. arXiv
preprintarXiv:1801.09847,2018.
[37] Y.Xiang,C.Xie,A.Mousavian,andD.Fox. Learningrgb-dfeatureembeddingsforunseen
objectinstancesegmentation. arXivpreprintarXiv:2007.15157,2020.
[38] H.-S. Fang, C. Wang, M. Gou, and C. Lu. Graspnet-1billion: A large-scale benchmark for
generalobjectgrasping. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages11444–11453,2020.
[39] H. Wang, S. Sridhar, J. Huang, J. Valentin, S. Song, and L. J. Guibas. Normalized object
coordinatespaceforcategory-level6dobjectposeandsizeestimation. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages2642–2651,2019.
[40] X.Deng,Y.Xiang,A.Mousavian,C.Eppner,T.Bretl,andD.Fox. Self-supervised6dobject
poseestimationforrobotmanipulation. In2020IEEEInternationalConferenceonRobotics
andAutomation(ICRA),pages3665–3671.IEEE,2020.
[41] A.Zeng, K.-T.Yu, S.Song, D.Suo, E. Walker, A.Rodriguez, and J.Xiao. Multi-view self-
supervised deep learning for 6d pose estimation in the amazon picking challenge. In 2017
IEEEinternationalconferenceonroboticsandautomation(ICRA),pages1386–1383.IEEE,
2017.
[42] M. Zhu, K. G. Derpanis, Y. Yang, S. Brahmbhatt, M. Zhang, C. Phillips, M. Lecce, and
K. Daniilidis. Single image 3d object detection and pose estimation for grasping. In 2014
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages3936–3943.IEEE,
2014.
[43] Y.Yoon,G.N.DeSouza,andA.C.Kak. Real-timetrackingandposeestimationforindustrial
objects using geometric features. In 2003 IEEE International conference on robotics and
automation(cat.no.03CH37422),volume3,pages3473–3478.IEEE,2003.
[44] V.Jain,P.Agarwal,S.Patil,andK.Sycara.Learningembeddingsthatcapturespatialsemantics
forindoornavigation. arXivpreprintarXiv:2108.00159,2021.
[45] C.Paxton,N.Ratliff,C.Eppner,andD.Fox. Representingrobottaskplansasrobustlogical-
dynamical systems. In 2019 IEEE/RSJ International Conference on Intelligent Robots and
Systems(IROS),pages5588–5595.IEEE,2019.
[46] P. Florence, L. Manuelli, and R. Tedrake. Self-supervised correspondence in visuomot or
policylearning. IEEERoboticsandAutomationLetters,5(2):492–499,2019.
[47] A.Zeng, P.Florence, J.Tompson, S.Welker, J.Chien, M.Attarian, T.Armstrong, I.Krasin,
D.Duong,V.Sindhwani,andJ.Lee. Transporternetworks: Rearrangingthevisualworldfor
roboticmanipulation. ConferenceonRobotLearning(CoRL),2020.
[48] W.Yuan,C.Paxton,K.Desingh,andD.Fox. SORNet: Spatialobject-centricrepresentations
forsequentialmanipulation.In5thAnnualConferenceonRobotLearning,2021.URLhttps:
//openreview.net/forum?id=mOLu2rODIJF.
[49] K.He, X.Chen, S.Xie, Y.Li, P.Dolla´r, andR.Girshick. Maskedautoencodersarescalable
visionlearners. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages16000–16009,2022.
[50] W. Liu, C. Paxton, T. Hermans, and D. Fox. Structformer: Learning spatial structure for
language-guidedsemanticrearrangementofnovelobjects. ArXiv,abs/2110.10189,2021.
[51] J. Sun, D.-A. Huang, B. Lu, Y.-H. Liu, B. Zhou, and A. Garg. Plate: Visually-grounded
planningwithtransformersinproceduraltasks. IEEERoboticsandAutomationLetters,7(2):
4924–4930,2022.
11
[52] V. Jain, R. Jena, H. Li, T. Gupta, D. Hughes, M. Lewis, and K. Sycara. Predicting human
strategiesinsimulatedsearchandrescuetask. arXivpreprintarXiv:2011.07656,2020.
[53] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and
I.Mordatch. Decisiontransformer: Reinforcementlearningviasequencemodeling. Advances
inneuralinformationprocessingsystems,34,2021.
[54] M.Janner,Q.Li,andS.Levine. Offlinereinforcementlearningasonebigsequencemodeling
problem. InAdvancesinNeuralInformationProcessingSystems,2021.
[55] A.L.Putterman,K.Lu,I.Mordatch,andP.Abbeel. Pretrainingforlanguageconditionedim-
itationwithtransformers,2022. URLhttps://openreview.net/forum?id=eCPCn25gat.
[56] M.Xu,Y.Shen,S.Zhang,Y.Lu,D.Zhao,B.J.Tenenbaum,andC.Gan. Promptingdecision
transformer for few-shot policy generalization. In Thirty-ninth International Conference on
MachineLearning,2022.
[57] X. Wang, K. Lee, K. Hakhamaneshi, P. Abbeel, and M. Laskin. Skill preferences: Learning
toextractandexecuteroboticskillsfromhumanfeedback. InConferenceonRobotLearning,
pages1259–1268.PMLR,2022.
[58] K. Lee, L. Smith, A. Dragan, and P. Abbeel. B-pref: Benchmarking preference-based rein-
forcementlearning. arXivpreprintarXiv:2111.03026,2021.
[59] X. Liang, K. Shu, K. Lee, and P. Abbeel. Reward uncertainty for exploration in preference-
basedreinforcementlearning. arXivpreprintarXiv:2205.12401,2022.
[60] W. B. Knox, S. Hatgis-Kessell, S. Booth, S. Niekum, P. Stone, and A. Allievi. Models of
humanpreferenceforlearningrewardfunctions. arXivpreprintarXiv:2206.02231,2022.
[61] P.F.Christiano,J.Leike,T.Brown,M.Martic,S.Legg,andD.Amodei. Deepreinforcement
learning from human preferences. Advances in neural information processing systems, 30,
2017.
[62] Y. Chen, Y. Liu, L. Dong, S. Wang, C. Zhu, M. Zeng, and Y. Zhang. Adaprompt: Adaptive
modeltrainingforprompt-basednlp. arXivpreprintarXiv:2202.04824,2022.
[63] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.
Zero-shottext-to-imagegeneration. InInternationalConferenceonMachineLearning,pages
8821–8831.PMLR,2021.
[64] C.Raffel,N.M.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.
Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv,
abs/1910.10683,2020.
[65] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and
L.Zettlemoyer. Bart: Denoisingsequence-to-sequencepre-trainingfornaturallanguagegen-
eration,translation,andcomprehension. InACL,2020.
[66] K.Song,X.Tan,T.Qin,J.Lu,andT.-Y.Liu.Mass:Maskedsequencetosequencepre-training
forlanguagegeneration. InICML,2019.
[67] T. Schick and H. Schu¨tze. It’s not just size that matters: Small language models are also
few-shotlearners. arXivpreprintarXiv:2009.07118,2020.
[68] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict:
A systematic survey of prompting methods in natural language processing. arXiv preprint
arXiv:2107.13586,2021.
[69] K. He, G. Gkioxari, P. Dolla´r, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE
internationalconferenceoncomputervision,pages2961–2969,2017.
[70] P.Anderson,A.Chang,D.S.Chaplot,A.Dosovitskiy,S.Gupta,V.Koltun,J.Kosecka,J.Ma-
lik,R.Mottaghi,M.Savva,etal. Onevaluationofembodiednavigationagents. arXivpreprint
arXiv:1807.06757,2018.
12
A HardwareExperiments
A.1 Real-worldpromptdemonstration
Herewedescribehowwecollectedandprocessedavisual,humandemonstrationinthereal-world
to treat as a prompt for the trained TTP policy (Fig. 8). Essentially, we collect demonstration
pointcloudsequencesandmanuallysegmentthemintodifferentpick-placesegments, followedby
extractingobjectstates. Ateachhigh-levelstep,wemeasurethestateusingthreeRealSenseRGBD
cameras[34], whicharecalibratedtotherobotframeofreferenceusingARTags[35]. Thecamera
output, extrinsics, and intrinsics are combined using Open3D [36] to generate a combined point-
cloud. This pointcloud is segmented and clustered to give objects’ pose and category using the
algorithmfrom[37]andDBScan. Foreachobjectpointcloudcluster, weidentifytheobjectpose
basedonthemean ofthepointcloud. For categoryinformationweusemedianRGBvalue ofthe
pointcloud, and map it to apriori known set of objects. In the future this can be replaced by more
advanced techniques like MaskRCNN [69]. Placement poses are approximated as a fixed, known
location,astheplaceactiononhardwareisafixed‘drop’positionandorientation.Theperstepstate
of the objects is used to create the input prompt tokens used to condition the policy rollout in the
real-world,asdescribedinSection3.2.
Figure8: Humandemonstrationofreal-worldrearrangementofhouseholddishes.
A.2 Hardwarepolicyrollout
We zero-shot transfer our policy π trained in simulation to robotic hardware, by assuming low-
levelcontrollers. WeuseaFrankaPandaequippedwithaRobotiq2F-85gripper,controlledusing
the Polymetis control framework [33]. Our hardware setup mirrors our simulation, with different
categoriesofdishware(bowls,cups,plates)onatable,a“dishwasher”(cabinetwithtwodrawers).
Theobjectiveistoselectanobjecttopickandplaceitintoadrawer(rack)(seeFig. 8).
Oncewecollectthehumanpromptdemonstrationtokens,wecanusethemtoconditionthelearned
policy π from simulation. Converting the hardware state to tokens input to π follows the same
pipelineastheonesusedforcollectinghumandemonstrations. Ateachstep,thesceneiscaptured
using3Realsensecameras,andthecombinedpointcoundissegmentedandclusteredtogetobject
posesandcategories. Thisinformationalongwiththetimestepisusedtogenerateinstancetokens
asdescribedinSection2forallobjectsvisibletothecameras. Forvisiblealreadyplacedobjects,
theplaceposeisapproximatedasafixedlocation. Thepolicyπ,conditionedonthehumandemo,
reasons about the state of the environment, and chooses which object to pick. Next, we use a
graspgeneratorfrom[38]thatoperatesonpointcloudstogeneratecandidategrasplocationsonthe
chosenobject.Wefilteroutgrasplocationsthatarekinematicallynotreachablebytherobot,aswell
as grasp locations located on points that intersect with other objects in the scene. Next, we select
thetop5mostconfidentgrasps,asestimatedbythegraspgenerator,andchoosethemosttop-down
grasp. We design an pre-grasp approach pose for the robot which is the same final orientation as
thegrasp,locatedhigheronthegraspingplane. Therobotmovestotheapproachposefollowinga
minimum-jerktrajectory,andthenfollowsastraightlinepathalongtheapproachaxestograspthe
object. Once grasped, the object is moved to the pre-defined place pose and dropped in a drawer.
Theprimitivesforopeningandclosingthedrawersaremanuallydesignedonhardware.
Thelearnedpolicy,conditionedonpromptdemonstrations,isappliedtotwovariationsofthesame
scene, and the predicted pick actions are executed. Fig.9 shows the captured image from one of
13
Figure9: Pointcloudandgraspsfordifferentobjectsduringpolicyrollout.
the three cameras, the merged point cloud and the chosen object to pick and selected grasp for
the same. The policy was successful once with 100% success rate, and once with 75%, shown in
Fig.1. Thefailurecasewascausedduetoaperceptionerror–abowlwasclassifiedasaplate. This
demonstratesthatourapproach(TTP)canbetrainedinsimulationandapplieddirectlytohardware.
Thepolicyisrobusttominorhardwareerrorslikeafailedgrasp;itjustmeasuresthenewstateofthe
environmentandchoosesthenextobjecttograsp. Forexample, iftherobotfailstograspabowl,
and slightly shifts the bowl, the cameras measure the new pose of the bowl, which is sent to the
policy. However,TTPreliesonaccurateperceptionofthestate. Ifanobjectisincorrectlyclassified,
thepolicymightchoosetopickthewrongobject,deviatingfromthedemonstrationpreference. In
thefuture, wewouldliketofurtherevaluateourapproachonmorediversereal-worldsettingsand
measureitssensitivitytothedifferenthardwarecomponents,informingfuturechoicesforlearning
robustpolicies.
A.3 Transforminghardwaretosimulationdatadistribution
The policy trained in simulation applies zero-shot to real-world scenarios, but it requires a co-
ordinate transform. Fig. 10 shows the coordinate frame of reference in simulation and real
world setting. Since our instance embedding uses the poses of objects, it is dependant on
the coordinate frame that the training data was collected in. Since hardware and simulation
are significantly different, this coordinate frame is not the same between sim and real. We
build a transformation that converts hardware measured poses to the simulation frame of ref-
erence, which is then used to create the instance tokens. This ensures that there is no sim-
to-real gap in object positions, reducing the challenges involved in applying such a simulation
trained policy to hardware. In this section we describe how we convert the real world coor-
dinates to simulation frame coordinates for running the trained TTP policy on a Franka arm.
14
We use the semantic work area in simulation and hardware
to transform the hardware position coordinates to simulation
position coordinates. We measure the extremes of the real
workspace by manually moving the robot to record positions
and orientations that define the extents of the workspace for
the table. The extents of the drawers are measured by plac-
ing ARTag markers. We build 3 real-to-sim transformations
Figure 10: Coordinate Frame of using the extents for counter, top rack and bottom rack: Let
reference in simulation (left) and
X ∈ R3×N containhomogeneousxz−coordinatesofawork
real world setting (right). Red is area,alongitscolumn,asfollows:
x-axis, green is y-axis and blue is
 
z-axis. x(1) x(2) ···
(cid:2) (cid:3)
X =z(1) z(2) ···= x(1) x(2) ··· (4)
1 1 ···
Astherequiredtransformationfromrealtosimulationinvolves
scaling and translation only, we have 4 unknowns, namely, a = [α ,α ,x ,z ]. Here
x y trans trans
α ,α arescalingfactorsandx ,z aretranslationoffsetforxandz axisrespectively. To
x z trans trans
(cid:34)α 0 x (cid:35)
x trans
solveX =AX ,weneedtofindthetransformationmatrixA=aˆ = 0 α z .
sim hw z trans
0 0 1
X =aˆX (5)
sim hw
Rewritingthe systemoflinearequations, (6)
 x(1)   x(1) 0 1 0
sim hw
z(1)   0 z(1) 0 1
 sim  hw 
=⇒  x( s2 im)  = x( h2 w) 0 1 0 aT (7)
 z(2)     0 z(2) 0 1 
 sim  hw 
. . . . .
. . . . .
. . . . .
(8)
LettheaboveequationbeexpressedasY =Z aT whereY ∈R2N×1,Z ∈R2N×4,and
sim hw sim hw
aT ∈ R4×1. Assuming we have sufficient number of pairs of corresponding points in simulation
andrealworld,wecansolveforabyleastsquaresa=(ZT Z )−1ZT Y . Theheighty is
hw hw hw sim sim
chosenfromalook-uptablebasedony . OncewecomputethetransformationA,westoreitfor
hw
latertoprocessarbitrarycoordinatesfromrealtosim,asshownbelow.
def get_simulation_coordinates(xyz_hw: List[float], A: np.array) -> List:
xz_hw = [xyz_hw[0], xyz_hw[2]]
X_hw = get_homogenous_coordinates(xz_hw)
X_sim_homo = np.matmul(A, X_hw)
y_sim = process_height(xyz_hw[1])
X_sim = [X_sim_homo[0]/X_sim_homo[2], y_sim, X_sim_homo[1]/X_sim_homo[2]]
return X_sim
Theobjectsusedinsimulationtrainingaredifferentfromhardwareobjects,eventhoughtheybelong
tothesamecategories. Forexample,whilebothsimandrealhaveasmallplate,thesizesofthese
platesaredifferent. Wecanestimatethesizeoftheobjectsbasedonactualboundingboxfromthe
segmentationpipeline. However,itissignificantlyout-of-distributionfromthetrainingdata,dueto
objectmismatch. So,wemapeachdetectedobjecttothenearestmatchingobjectinsimulationand
usethesimulationsizeastheinputtothepolicy. Thisisnon-ideal, astheplacingmightdifferfor
simversusrealobjects. Inthefuture,wewouldliketotrainwithrichvariationsofobjectbounding
boxsizeinsimulationsothatthepolicycangeneralizetounseenobjectshapesintherealworld.
15
Figure11: Humandemonstrationwithpointandclickinsimulation
B SimulationSetup
Dataset “ReplicaSyntheticApartment0Kitchen”consistsofafully-interactivedishwasherwith
a door and two sliding racks, an adjacent counter with a sink, and a “stage” with walls, floors,
and ceiling. We use selected objects from the ReplicaCAD [29] dataset, including seven types of
dishes(cups,glasses,trays,smallbowls,bigbowls,smallplates,bigplates)whichareloadedinto
the dishwasher. Fig. 11 shows a human demonstration recorded in simulation by pointing and
clicking on desired object to pick and place. We initialize every scene with an empty dishwasher
and random objects placed on the counter. Next, we generate dishwasher loading demonstrations,
adhering to a given preference, using an expert-designed data generation script. Expert actions
include,opening/closingdishwasher/racks,picking/placingobjectsinfeasiblelocationsorthesink
iftherearenofeasiblelocationsleft. Expertsdifferintheirpreferences,andmightchoosedifferent
objectarrangementsinthedishwasher.
Expert Preferences We define a preference in
Table2:Threeexamplepreferencesfordish-
terms of expert demonstration ‘properties’, like
washerloading. Rackorderandtheirrespec-
whichrackisloadedfirstwithwhatobjects? There
tivecontents(orderedbypreference).
are combinatorially many preferences possible, de-
pendingonhowmanyobjectsweuseinthetraining First? Top Bottom
set. Forexample, Table2describesthepreferences 1.bigplates
1.cups
ofdishwasherloadingintermsofthreeproperties- 2.smallplates
Top 2.glasses
firstloadedtray,objectsintopandbottomtray.Each 3.trays
3.smallbowl
preference specifies properties such as which rack 4.bigbowls
to load first and their contents. In Table 2, Prefer-
1.bigplates
ences1&2varyintheorderofwhichrackisloaded 1.cups 2.smallplates
Bottom 2.glasses
first, while 2 & 3 both load the bottom rack first 3.trays
3.smallbowl
with similar categories on top and bottom but with 4.bigbowl
differentorderingsforthesecategories. Otherpref-
1.bigbowls
erences can have different combinations of objects 1.smallplate
2.trays
loadedperrack. Bottom 2.glasses 3.bigplates
3.cups
4.smallbowl
To describe a preference, let there be k properties,
where each can take m values respectively. For
k
example, a property to describe preference can be
whichrackisloadedfirst,andthiscantaketwovalues;eithertoporbottomrack. Thetotalnumber
ofpossiblepreferencesisG=(cid:81)k
m .
i=1 i
Inourdemonstrationdataset,wehave100uniquesessionsperpreference. Eachsessioncanactas
a prompt to indicate preference as well as provide situation for the policy. Each session is about
∼ 30 steps long. With 7 preferences, this leads to 70,000×30 = 2,100,000 ∼ 2 million total
training samples, creating a relatively large training dataset from only 100 unique demonstrations
perpreference. Individualtaskpreferencesdifferinthesequenceofexpertactions,butcollectively,
preferencessharetheunderlyingtasksemantics.
Dynamicallyappearingobjects Toaddadditionalcomplexitytooursimulationenvironment,we
simulate a setting with dynamically appearing objects later in the episode. During each session,
the scene is initialized with p% of maximum objects allowed. The policy/expert starts filling a
16
dishwasher using these initialized objects. After all the initial objects are loaded and both racks
are closed, new objects are initialized one-per-timestep to the policy. The goal is to simulate an
environmentwherethepolicydoesnothaveperfectknowledgeofthescene,andneedstoreactively
reason about new information. The policy reasons on both object configurations in the racks, and
thenewobjecttypetodecidewhetherto‘openarackandplacetheutensil’or‘droptheobjectin
thesink’.
C Training
InthisSectionwedescribedetailsofthedifferentcomponentsofourlearningpipeline.
C.1 Baseline: GNN
Architecture WeuseGNNwithattention. Theinputconsistsof12dimensionalattributeinputs
(1D-timestep, 3D-category bounding box extents, 7D-pose, 1D-is object or not?) and 12 dimen-
sionalone-hotencodingforthepreference.
input_dim: 24
hidden_dim: 128
epochs: 200
batch_size: 32
Optimizer : Adamwithlr =0.01andweight decay=1e−3.
RewardfunctionforGNN-RL RewardfunctionfortheRLpolicyisdefinedintermsofprefer-
ence. Thepolicygetsarewardof+1everytimeitpredictstheinstancetopickthathasthecategory
accordingtothepreferenceorderandwhetheritisplacedonthepreferredrack.
C.2 Ourproposedapproach: TTP
Architecture Weusea2-layer2-headTransformernetworkforencoderanddecoder. Theinput
dimension of instance embedding is 256 and the hidden layer dimension is 512. The attributes
contributetotheinstanceembeddingasfollows:
C_embed: 16
category_embed_size: 64
pose_embed_size: 128
temporal_embed_size: 32
marker_embed_size: 32
FortheslotattentionlayerattheheadofTransformerencoder,weuse:
num_slots: 50
slot_iters: 3
Optimizer Weuseabatch-sizeof64sequences. Withineachbatch,weusepadtheinputswith0
uptothemaxsequencelength. OuroptimizerofchoiceisSGDwithmomentum0.9,weightdecay
0.0001anddampening0.1. Theinitiallearningrateis0.01, withexponentialdecayof0.9995per
10gradientupdates. Weusedearlystoppingwithpatience100.
C.3 Metrics
In Section 3, we presented packing efficiency (PE) and edit distance (ED) metrics collected on a
policyrollout. Wepresentadditionalmetricsabouttrainingprogressandrollouthere.
Category-token Accuracy indicates how well the policy can mimic the expert’s action, given the
currentstate. Wemonitortrainingprogressbymatchingthepredictedinstancetothetargetchosen
in demonstration (Fig. 12a). We see that TTP is able to predict the same category object to pick
perfectly(accuracycloseto1.0). However,thisisasimplersettingthatsequentialdecisionmaking.
17
(a) Category level accuracy
grouped by batch size for
prompt-situationtraining. (b)TE(SPL)metricforheld-outtestsettings.
(a)Temporalencoding (b)Categoryencoding (c)Poseencoding
Figure 13: [Left-to-Right] Comparing different design choices of attribute encoders in terms of
categorytokenaccuracyonheld-outtestprompt-situationsessionpairs.
During rollout, any error in a state could create a setting that is out-of-distribution for the policy.
Thus, category token accuracy sets an upper bound for rollout performance, that is, while having
highcategorytokenaccuracyisnecessary,itisnotsufficientforhighpackingefficiencyandinverse
editdistance.
Temporalefficiency: JustlikeSPL[70]fornavigationagents,wedefinetheefficiencyoftemporal
tasks in policy rollout, in order to study how efficient the agent was at achieving the task. For
episodei ∈ [1,..N],lettheagenttakep numberofhigh-levelinteractionstoexecutethetask,and
i
the demonstration consists of l interactions for the initial state. We scale the packing efficiency
i
PE ofthepolicybytheratioofstepstakenbyexpertversuspolicy. Temporalefficiencyisdefined
i
between0to1,andhigherisbetter. Thisvaluewillbeequaltoorlowerthanthepackingefficiency.
This especially penalizes policies that present a ‘looping’ behavior, such as repeatedly open/close
dishwasher trays, over policies that reach a low PE in shorter episodes (for example, by placing
mostobjectsinthesink). Fig12bshowsthetemporalefficiencyorSPLoverour4mainheld-out
testsettings.
D AdditionalAblationExperiments
In Section 3.3, we presented ablation experiments over number of demonstrations per preference
usedfortraining,andthenumberofuniquepreferencesused. InthisSection,wepresentadditional
ablation experiments over the design of instance encodings in TTP. Additionally, we also present
resultswhereweincreasethetemporalcontextofTTPandstudyitseffectonperformance.
D.1 DesignofInstanceEncoding
Howmuchdoestemporalencodingdesignmatter? Fig. 13ashowsthatlearninganembedding
pertimesteporexpandingitasfouriertransformedvectorofsufficientsizeachieveshighsuccess.
Ontheotherhand, havingnotimestepinputshowsslightlylowerperformance. Timestephelpsin
18
Figure15: ProcessingwithpreviousContextHistoryk
encodingtheorderofthepromptstates. Thenotionoftimestepisalsoincorportatedbyautoregres-
sivemaskinginboththeencoderandthedecoder.
How much does category encoding design matter? In our work, we represent category as the
extents of an objects’ bounding box. An alternative would be to denote the category as a discrete
setofcategoricallabels. Intuitively,boundingboxextentscapturesshapesimilaritybetweenobjects
and their placement implicitly, which discrete category labels do not. Fig. 13b shows that fourier
transformoftheboundingboxachievesbetterperformancethandiscretelabels,whichexceedsthe
performancewithnocategoryinput.
Howmuchdoesposeencodingdesignmatter? Weencodeposeasa7-dimvectorthatincludes
3d position and 4d quaternion. Fig. 13c shows that the fourier transform of the pose encoding
performsbetterthanfeedingthe7dimthroughMLP.Fouriertransformoftheposeperformsbetter
because such a vector encodes the fine and coarse nuances appropriately, which otherwise either
requirecarefulscalingorcanbelostduringSGDtraining.
D.2 Markovassumptiononthecurrentstateinpartialvisibilityscenarios
Dynamic settings, as used in our simulation,
canbepartiallyobservable.Forexample,when
the rack is closed, the policy doesn’t know
whether it is full or not from just the current
state. Ifanewobjectarrives, thepolicyneeds
to decide between opening the rack if there is
space,ordroppingtheobjectinsinkiftherack
isfull. Insuchpartiallyobservedsettings, the
currentstatemayormaynotcontainallthein-
formation needed to reason about the next ac-
tion. However, given information from states
in previous timesteps, the policy can decide
what action to take (whether to open the rack
ordirectlyplacetheobjectinthesink). Tothis
end,wetrainasinglepreferencepickonlypol-
icy for different context history. As shown in Figure14:Categorylevelaccuracyforsinglepref-
Fig. 15, context window of size k processes erencetrainingwithvaryingcontextwindows.
thecurrentstateaswellaskpredecessorstates,
thatis,intotalk+1states. Whilelargercontextwindowsizelearnsfaster,theasymptoticperfor-
manceforallcontextwindowsconvergesinoursetting.
Let context history k refer to the number of previous states included in the input. Then the input
is a sequence of previous k states’ instances (including the current state), as shown in Fig. 15.
Fig14showsthatTTPgets> 90%categorylevelpredictionaccuracyinvalidationforallcontext
windows. While larger context windows result in faster learning at the start of the training, the
asymptoticperformanceofallcontextsisthesame. Thispointstothedatasetbeinglargelyvisible,
and a single context window capturing the required information. In the future, we would like to
experimentwithmorecomplexsettingslikemobilerobots,whichmightrequirealongercontext.
19
E LimitationsandFuturescope
InSection6,webrieflydiscussedthelimitationsandrisks.Hereweenlistmoredetailsandhighlight
futuredirections.
Pickgraspingdependsonaccuratesegmentationandedgedetection Graspingpolicydepends
on quality of segmentation and edge detection of the selected object. Due to noise in calibration,
shadows and reflections, there are errors in detecting the correct edge to successfully grasp the
object. Forexample,itishardtograspaplateinrealsetting. Plateisveryclosetothegroundand
the depth cameras cannot detect a clean edge for grasping. Therefore, in our work, we place the
plateonanelevatedstandforeasygrasping. Graspingsuccessalsodependsonthesizeandkindof
gripperused.
Placement in real setting For placement, the orientation of final pose is often different from
initialposeandmayrequirere-grasping.Theplacementposeatfinalsettlementisdifferentfromthe
robot’s end-effector pose while releasing the object from its grasp. Similar to picking, placement
accuracywilllargelydependonapproperiatesizeandshapeofgripperused. Duetothesereasons,
placementinrealworldisanopenchallengingproblemandwehopetoaddressthisfuturework.
Hardwarepipelineissuesduetocalibration Theresultingpointcloudisgeneratednoisydueto
tworeasons. First,incorrectdepthestimationduetocamerahardware,lightingconditions,shadows
andreflections. Second, anysmallmovementsamongcamerasthataffectscalibration. Ifwehave
anoisypointcloud,itismorelikelytohaveerrorsinsubsequentsegmentationandedgedetection
forgrasppolicy. Havingsufficientcoverageoftheworkspacewithcamerasisimportanttomitigate
issuesduetoocclusionsandincompletepointclouds.
Incomplete information in prompt The prompt session may not contain all the information to
executeonthesituation.Forexample,inapromptsessiontheremightbenolargeplatesseen,which
inincompleteorambiguousinformationforthepolicy. Thiscanbemitigatedbyensuringcomplete
informationinpromptdemoorhavingmultiplepromptsinslightlydifferentinitialization.
20

Retrospectives on the Embodied AI Workshop
MattDeitke1,17,DhruvBatra5,8,YonatanBisk3,TommasoCampari4,16,AngelX.Chang13,DevendraSinghChaplot8,
ChanganChen19,ClaudiaPérez-D’Arpino9,KianaEhsani1,AliFarhadi2,17,LiFei-Fei14,AnthonyFrancis6,ChuangGan11,15,
KristenGrauman19,8,DavidHall20,WinsonHan1,UnnatJain8,AniruddhaKembhavi1,17,JacobKrantz12,StefanLee12,ChengshuLi14,
SagnikMajumder19,OleksandrMaksymets8,RobertoMartín-Martín19,RoozbehMottaghi8,17,SoniaRaychaudhuri13,
MikeRoberts7,SilvioSavarese14,ManolisSavva13,MohitShridhar17,NikoSünderhauf20,AndrewSzot5,BenTalbot20,
JoshuaB.Tenenbaum10,JesseThomason18,AlexanderToshev2,JoanneTruong5,LucaWeihs1,JiajunWu14
1AllenInstituteforAI,2Apple,3CarnegieMellonUniversity,4FBK,5GeorgiaTech,6Google,7IntelLabs,8MetaAI,9NVIDIA,10MIT,
11MIT-IBMWatsonAILab,12OregonStateUniversity,13SimonFraserUniversity,14StanfordUniversity,15UMassAmherst,
16UniversityofPadova,17UniversityofWashington,18UniversityofSouthernCalifornia,19UTAustin,20QUTCentreforRobotics
Abstract ofresearchersandresearchchallenges.
Consideraskingarobotto‘Cleanmyroom’or‘Drive
We present a retrospective on the state of Embodied AI
metomyfavoriterestaurant’. Tosucceedatthesetasks
research. Our analysis focuses on 13 challenges presented
in the real world, the robots need skills like visual
at the Embodied AI Workshop at CVPR. These challenges
perception (to recognize scenes and objects), audio per-
aregroupedintothreethemes: (1)visualnavigation,(2)re-
ception (to receive the speech spoken by the human),
arrangement, and (3) embodied vision-and-language. We
language understanding (to translate questions and in-
discuss the dominant datasets within each theme, evalua-
structions into actions), memory (to recall how items
tionmetricsforthechallenges,andtheperformanceofstate-
should be arranged or to recall previously encoun-
of-the-artmodels. Wehighlightcommonalitiesbetweentop
teredsituations),physicalintuition(tounderstandhow
approaches to the challenges and identify potential future
tointeractwithotherobjects),multi-agentreasoning(to
directionsforEmbodiedAIresearch.
predict and interact with other agents), and naviga-
tion (to safely move through the environment). The
study of embodied agents both provides a challeng-
1.Introduction
ing testbed for building intelligent systems and tries
to understand how intelligence emerges through in-
Within the last decade, advances in deep learn-
teraction with an environment. As such, it involves
ing, coupled with the creation of massive datasets
many disciplines, such as computer vision, natural
and high-capacity models, have resulted in remark-
languageprocessing,acousticlearning,reinforcement
able progress in computer vision, audio, NLP, and
learning, developmental psychology, cognitive sci-
the broader field of AI. This progress has enabled
ence,neuroscience,androbotics.
modelstoobtainsuperhumanperformanceonawide
variety of passive tasks (e.g. image classification). In this paper, we present a retrospective on
However, this progress has also enabled a paradigm the state of embodied AI, focusing on the chal-
shift towards embodied agents (e.g. robots) which lenges highlighted at the 2020–2022 CVPR em-
learn, through interaction and exploration, to cre- bodied AI workshops. The challenges presented
atively solve challenging tasks within their environ- in the workshop have focused on benchmarking
ments. The field of embodied AI focuses on how in- progress in navigation, rearrangement, and em-
telligence emerges from an agent’s interactions with bodied vision-and-language. The navigation chal-
itsenvironment.Aninteractionintheenvironmentin- lenges include Habitat PointNav [1] and Object-
volvesanagenttakinganactionthataffectsitsfuture Nav [17], Interactive and Social Navigation with
state. Forinstance,theagentmayperformnavigation iGibson [210], RoboTHOR ObjectNav [51], Mul-
actions to move around the environment or take ma- tiON [198], RVSU Semantic SLAM [82], and Audio-
nipulation actions to open or pick up objects within Visual Navigation with SoundSpaces [38]; rear-
reach. EmbodiedAIisafocusofagrowingcollection rangement challenges include AI2-THOR Rearrange-
1
2202
ceD
5
]VC.sc[
3v94860.0122:viXra
Hey , is
there any
cereal left?
Cereal
Figure1.AnillustrationofascenariodepictingmanytasksofinteresttoresearchersinEmbodiedAI.Here,wehavemultiple
robotsoperatinginakitchenenvironment,withahumanaskingoneoftherobotsifthereisanycerealleft,whiletheother
onecleansthedishes. Therobotsmustusetheirnavigation,manipulation,andreasoningskillstoanswerandachievetasks
intheenvironment.
ment [200], TDW-Transport [67], and RVSU Scene transfer[188,220].
Change Detection [82]; and embodied vision-and- Abstracting away from real or simulated embodi-
language challenges include RxR-Habitat [102], AL- ments, embodied AI can be defined as the study of
FRED [177], and TEACh [133]. We discuss the setup intelligent agents that can see (or more generally per-
ofeachchallengeanditsstate-of-the-artperformance, ceive their environment through vision, audition, or
analyze common approaches between winning en- othersenses), talk (i.e.holdanaturallanguagedialog
tries across the challenges, and conclude with a dis- grounded in the environment), listen (i.e. understand
cussionofpromisingfuturedirectionsinthefield. andreacttoaudioinputanywhereinascene.),act(i.e.
navigatetheirenvironmentandinteractwithittoac-
2.WhatisEmbodiedAI? complishgoals),andreason(i.e.considerthelong-term
consequences of their actions). Embodied AI focuses
Embodied AI studies artificial systems that express
on tasks which break the clean input/output formal-
intelligent behavior through bodies interacting with
ism of passive tasks such as object classification and
theirenvironments. Thefirstgenerationofembodied
speech understanding, and require agents to interact
AIresearchersfocusedonroboticembodiments[142],
with - and sometimes even modify - their environ-
arguing that robots need to interact with their noisy
ments over time (Fig. 2). Furthermore, embodied AI
environments with a rich set of sensors and effec-
environmentsgenerallyviolatethecleandynamicsof
tors, creating high-bandwidth interaction that breaks
structuredenvironmentssuchasgamesandassembly
the fundamental assumptions of clean inputs, clean
lines, and require agents to cope with noisy sensors,
outputs, and static world states required by classical
effectors, dynamics, and other agents, which creates
AI approaches [206]. More recent embodied AI re-
unpredictableoutcomes.
searchhasbeenempoweredbyrichsimulationframe-
works,oftenderivedfromscansofrealbuildingsand
models of real robots, to recreate environments more Why is Embodiment Important? Embodied AI can
closely resembling the real world than those previ- be viewed as a reaction against extreme forms of
ously available. These environments have enabled the mind-body duality in philosophy, which some per-
both discoveries about the properties of intelligence ceive to view intelligence as a purely mental phe-
[135] and systems which show excellent sim-to-real nomenon.Themind-bodyproblemhasfacedphiloso-
2
Input Predictions
Environment + Goal (Interactive) Action Trajectory
TV PFuosuhn!d!
Rotate Move
Right Ahead
“C“Floinsde tthhee TdVra”wer”
Figure2. PassiveAItasksarebasedonpredictionsoverindependentsamplesoftheworld,suchasimagescollectedwithout
a closed loop with a decision-making agent. In contrast, embodied AI tasks include an active artificial agent, such as a
robot,thatmustperceiveandinteractwiththeenvironmentpurposelytoachieveitsgoals,includinginunstructuredoreven
uncooperativesettings.Enabledbytheprogressincomputervisionandrobotics,embodiedAIrepresentsthenextfrontierof
challengestostudyandbenchmarkintelligentmodelsandalgorithmsforthephysicalworld.
phersandscientistsformillennia[45]: humansaresi- creatingproblemswithgroundingsymbolsinpercep-
multaneously “physical agents” with mass, volume tion[84,184]andoftenleadingtobrittleness[47,108,
and other bodily properties, and at the same time 114]. However, symbolic reasoning problems them-
“mental agents” that think, perceive, and reason in a selvesoftenprovedtoberelativelyeasy, whereasthe
conceptualdomainwhichseemstolackphysicalem- physical problems of perceiving the environment or
bodiment. Some scholars argue in favor of a strict actinginitwereactuallythemostchallenging:whatis
mind-body duality in which intelligence is a purely unconsciousforhumansoftenrequiressurprisingin-
mentalqualityonlylooselyconnectedtobodilyexpe- telligence,oftenknownasMoravec’sParadox[3,70].
rience [161]. Other scholars, across philosophy, psy- Some researchers challenged this approach, arguing
chology, cognitive science and artificial intelligence, that for machines to be intelligent, they must inter-
havechallengedthismind-bodyduality,arguingthat act with noisy environments via rich sets of sensors
intelligence is intrinsically connected to embodiment and effectors, creating high-bandwidth interactions
in bodily experience, and that separating them has thatbreaktheassumptionofcleaninputsandoutputs
distortingeffectsonresearch[23,117,138,161,197]. anddiscretestatesrequiredbyclassicalAI[206];these
Thehistoryofresearchinartificialintelligencehas ideas were echoed by roboticists already concerned
mirrored this debate over mind and body, focusing with connecting sensors and actuators more directly
first on computational solutions for symbolic prob- [12, 23, 124]. Much as neural network concepts hi-
lems which appear hard to humans, a strategy often bernated through several AI winters before enjoying
called GOFAI ("Good Old Fashioned AI", [22, 116]). a renaissance, embodied AI ideas have now been re-
The computational theory of mind argued that if in- vived by new interest from fields such as computer
telligencewasreasoningoperationsinthemind,com- vision, machine learning and robotics - often in com-
putersperformingsimilarcomputationscouldalsobe binationwithneuralnetworkideas. Newgenerations
intelligent [143, 169]. Purely symbolic artificial in- ofartificialneuralnetworksarenowabletodigestraw
telligence were often disconnected from the physical sensor signals, generate commands to actuators, and
world, requiring symbolic representations as input, autonomouslylearnproblemrepresentations,linking
3
IA
evissaP
IA
deidobmE
"classicalAI"taskstoembodiedsetups. Machinelearningisoneofthemostcommonlyused
Thus, embodied AI is more than just the study of techniques for building embodied agents. However,
agents that are active and situated in their environ- machinelearningisavastfieldencompassingprimar-
ments: it is an exploration of the properties of in- ilypassivetasks,andmostembodiedAItasksarefor-
telligence. Embodied AI research has demonstrated mulated in such a way that they are learning agnos-
that intelligent systems that perform well at embod- tic. For example, the iGibson 2020 challenge [175]
iedtasksoftenlookdifferentthantheirpassivecoun- allowed training in simulated environments but de-
terparts[61]-but,conversely,thathighlyperforming ployment in holdout environments in both real and
passive AI tasks can often contribute greatly to em- simulation; nothing required the solutions to use a
bodied systems as components [176]. Furthermore, learnedapproachasopposedtoaclassicalnavigation
the control over embodied agents provided by mod- stack (though learned approaches were the ones de-
ernsimulatorsanddeeplearninglibrariesenablesab- ployed).
lation studies that reveal fine-grained details about Artificialintelligenceiswrittenintothenameofem-
the properties needed for individual embodied tasks bodiedAI,butthefieldofembodiedAIwascreatedto
[135]. addresstheperceivedlimitationsofclassicalartificial
intelligence [142], and much of artificial intelligence
is focused on problems like causal reasoning or au-
What is not Embodied AI? Embodied AI overlaps
tomated programming which are hard enough with-
with many other fields, including robotics, computer
out introducing the messiness of real embodiments.
vision, machine learning, artificial intelligence, and
More recently, techniques from more traditional arti-
simulation. However, there are differences in focus
ficial intelligence domains like natural language un-
which make embodied AI a research area in its own
derstandinghavebeenappliedtoembodiedproblems
right.
withgreatsuccess[4].
All robotic systems are embodied; however, not all
Simulation and embodied AI are intimately inter-
embodied systems are robots (e.g., AR glasses), and
twined; while simulations of real-world systems go
robotics requires a great deal of work beyond purely
farbeyondthetopicsofrobotics,andthefirstgenera-
tryingtomakesystemsintelligent. EmbodiedAIalso
tionofembodiedAIfocusedonroboticembodiments
includes work that focuses on exploring the proper-
[142], muchofmodernembodiedAIresearchhasex-
tiesofintelligenceinrealisticenvironmentswhileab-
panded to simulated benchmarks, emulating or even
stractingsomeofthedetailsoflow-levelcontrol. For
scannedfromrealenvironments,whichprovidechal-
example, the ALFRED [177] benchmark uses simula-
lenging problems for traditional AI approaches, with
tion to abstract away low-level robotic manipulation
or without physical embodiments. Despite not start-
(e.g. moving a gripper to grasp an object) to focus on
ing with robots, systems that have resulted from this
high-level task planning. Here, the agent is tasked
work have nevertheless found success in real-world
withcompletinganaturallanguageinstruction, such
environments [188, 220], providing hope that simu-
as rinse the egg to put it in the microwave, and it can
latedbenchmarkswillproveafruitfulwaytodevelop
openorpickupanobjectbyissuingahigh-levelOpen
morecapablereal-worldintelligentsystems.
or Pickup action that succeeds if the agent is looking
at the object and is sufficiently close to it. Addition-
ally,[135]providesanexampleofstudyingproperties Why focus on real-world environments? Many re-
ofintelligence,wheretheyattempttoanswerwhether searchers are exploring intelligence in areas such as
mappingisstrictlyrequiredforaformofroboticnav- imagerecognitionornaturallanguageunderstanding
igation. Conversely, robotics includes work that fo- where at first blush interaction with an environment
cusesdirectlyontheaspectsoftherealworld,suchas appearsnottoberequired.Genuinediscoveriesabout
low-level control, real-time response, or sensor pro- intelligent systems appear to have been made here,
cessing. such as the role of convolutions in image processing
Computer vision has contributed greatly to embod- and the role of recurrent networks and attention in
ied AI research; however, computer vision is a vast languageprocessing.Soareasonablequestionis,why
field, much of which is focused purely on improving doweneedtofocusoninteractiveandrealistic(ifnot
performanceonpassiveAItaskssuchasclassification, real-world) environments if we want to understand
segmentation,andimagetransformation. Conversely, intelligence?
embodiedAIresearchoftenexploresproblemsthatre- Focusingoninteractiveenvironmentsisimportant
quireothermodalitieswithorwithoutvision,suchas because each new modality of intelligence we con-
navigationwithsound[38]orpureLiDARimages. sider - classification, image processing, natural lan-
4
guage understanding, and so on - has required new
architectures for learning systems [71], [41]. Interact-
ingwithanenvironmentovertimerequiresthetech-
niquesofreinforcementlearning. Deepreinforcement
learninghasmademassivestridesincreatinglearning
systems for synthetic environments, including tradi-
tional board games, Atari games, and even environ-
mentswithsimulatedphysicssuchastheMujocoen-
vironments.
However, embodied AI research focuses on envi-
ronmentsthatareeithermorerealistic[214]orwhich
requireactualdeploymentsintherealworld[1,175]).
Thisshiftinemphasishastwoprimaryreasons. First,
many embodied AI researchers believe that the chal- Figure3. ThePointNavtaskrequiresanagenttonavigate
lenges of realistic environments are critical for devel- to a goal coordinate in a novel environment (potentially
opingsystemsthatcanbedeployedintherealworld. with noisysensory inputs), withoutaccess to apre-built
Second, many embodied AI researchers believe that mapoftheenvironment.
there are genuine discoveries to be made about the
3.1.1 PointNav
propertiesofintelligenceneededtohandlerealworld
environmentsthatcanonlybemadebyattemptingto In PointNav, the agent’s goal is to navigate to target
solveproblemsinenvironmentsthatareasclosetothe coordinatesinanovelenvironmentthatarerelativeto
realworldasisfeasibleatthistime. its starting location (e.g. navigate 5m north, 3m west
relative to its starting pose), without access to a pre-
3.ChallengeDetails built map of the environment. The agent has access
to egocentric sensory inputs (RGB images, depth im-
In this section, we discuss the 13 challenges
ages, or both), and an egomotion sensor (sometimes
presentatourEmbodiedAIWorkshopbetween2020–
referred to as GPS+Compass sensor) for localization.
2022. The challenges are partitioned into naviga-
The action space for the robot consists of: Move For-
tion challenges, rearrangement challenges, and em-
ward0.25m,RotateRight30◦,RotateLeft30◦,andDone.
bodied vision-and-language challenges. Most chal-
Anepisodeisconsideredsuccessfuliftheagentissues
lengespresentadistinctivetasks,metricsandtraining
the Done command within 0.2 metersof the goal and
datasets,thoughmanychallengessharesimilarobser-
within 500 maximum steps. The agent is evaluated
vationspaces,actionspaces,andenvironments.
using the Success Rate (SR) and "Success weighted
3.1.NavigationChallenges by Path Length" (SPL) [9] metrics, which measures
the success and efficiency of the path taken by the
Ourworkshophasfeaturedanumberofchallenges
agent. For training and evaluation, challenge partic-
relating to embodied visual navigation. At a high-
ipantsusethetrainandvalsplitsfromtheGibson3D
level,thetasksconsistofanagentoperatinginasim-
dataset[223].
ulated 3D environment (e.g. a household), where its
In 2019, AI Habitat hosted its first challenge on
goalistomovetosometarget.Foreachtask,theagent
PointNav.1 The winning submission [31] utilized a
has access to an egocentric camera and observes the
combinationofclassicalandlearning-basedmethods,
environment from a first-person’s perspective. The
and achieved a high test SPL of 0.948 in the RGB-D
agentmustlearntonavigatetheenvironmentfromits
track,and0.805intheRGBtrack.In2020and2021,the
visualobservations.
PointNav challenge was modified to emphasize in-
The challenges primarily differ based on how the
creasedrealismandonsim2realpredictivity(theabil-
target is encoded (e.g. ObjectGoal, PointGoal, Audio-
itytopredictperformanceonarealrobotfromitsper-
Goal), how the agent is expected to interact with the
formance in simulation) based on findings from Ka-
environment (e.g. static navigation, interactive navi-
dianetal. [92]. Specifically,thechallenge(PointNav-
gation,socialnavigation),thetrainingandevaluation
v2)introduced(1)noGPS+Compasssensor,(2)noisy
scenes (e.g. 3D scans, video-game environments, the
actuation and sensing, (3) collision dynamics and
realworld),theobservationspace(e.g.RGBvs. RGB-
‘sliding’, and (4) minor changes to the robot embod-
D, whether to provide localization information), and
iment/size,cameraresolution,heighttobettermatch
theactionspace(e.g.outputtingdiscretehigh-levelac-
tionsorcontinuousjointmovementactions). 1https://aihabitat.org/challenge/2019/
5
theLoCoBotrobot. Thesechangesprovedtobemuch interactandpushawayobjectstoachievemeaningful
more challenging, with the winning submission in navigation. Note that all objects in the scenes are as-
2020 [149] achieving a SPL of 0.21 and SR of 0.28. In signed realistic physical weight and are interactable.
2021, therewasamajorbreakthroughwitha3× per- Asintherealworld,whilesomeobjectsarelightand
formanceimprovementoverthewinnersin2020; the movablebytherobot, othersarenot. Alongwiththe
winningsubmissionachievedaSPLof0.74andSRof furniture objects originally in the scenes, additional
0.96 [1]. Since an agent with perfect GPS + Compass objects(e.g. shoesandtoys)fromtheGoogleScanned
sensors in this PointNav-v2 setting can only achieve Objects dataset [54] are added to simulate real-world
amaximumof0.76SPLand0.99SR,thePointNav-v2 clutter. Theperformanceoftheagentisevaluatedus-
challengewasconsideredsolved,anddiscontinuedin ing a novel Interactive Navigation Score (INS) [210]
futureyears. that measures both navigation success as well as the
level of disturbance to the scene an agent has caused
alongtheway.
3.1.2 InteractiveandSocialPointNav
In Social Navigation, the agent navigates among
In Interactive and Social Navigation, the agent is re- walking humans in a home environment. The hu-
quiredtoreachaPointGoalindynamicenvironments mans in the scene move towards randomly sampled
thatcontaindynamicobjects(furniture,clutter,etc)or locations, and their 2D trajectories are simulated us-
dynamic agents (pedestrians). Although robot nav- ingthemodelofOptimalReciprocalCollisionAvoid-
igation achieves remarkable success in static, struc- ance(ORCA)[18]integratediniGibson[105,140,175].
tured environments like warehouses, it still remains Theagentshallavoidcollisionsorproximitytopedes-
a challenging research question in dynamic environ- trians beyond a threshold (distance <0.3 meter) to
ments like homes and offices. In 2020 and 2021, the avoid episode termination. It should also maintain a
Stanford Vision and Learning Lab in collaboration comfortabledistancetopedestrians(distance<0.5me-
with Robotics@Google hosted challenges on Interac- ter),beyondwhichthescoreispenalizedbutepisodes
tive and Social (Dynamic) Navigation2. These chal- are not terminated. Social Navigation Score (SNS),
lengesusedthesimulationenvironmentiGibson[105, which is the average of STL (Success weighted by
175]withanumberofrealisticindoorscenes,asillus- Time Length) and PSC (Personal Space Compliance),
trated in Fig. 4. The 2020 Challenge3 also featured a isusedtoevaluateperformanceoftheagent.
Sim2Real component where the participants trained
The agent takes in the current RGB-D images, the
their policies in the iGibson simulation environment
targetcoordinatesinitslocalframe,andcurrentveloc-
anddeployedintherealworld.
ities as observations, and outputs a continuous twist
command(desiredlinearandangularvelocities)asac-
tions. Thedatasetincludeseighttrainingscenes,two
validation scenes and five testing scenes. All scenes
arefullyinteractive.
In the 2020edition we saw4 submissions while in
the subsequent 2021 edition we had 6 submissions.
The current state-of-the-art learning based methods
achievedsomelevelofsuccessforInteractiveandSo-
cial Navigation tasks (around 0.5 INS and 0.45 SNS),
but they are still far from being solved. In both com-
(a)InteractiveNavigation (b)SocialNavigation
petitions participants improved over navigation suc-
Figure 4. Interactive Navigation (left) requires the agent to cess rate while keeping environment disturbance rel-
push aside small obstacles (e.g. shoes, boxes) whereas So- atively constant. The common failure cases include
cialNavigation(right)requirestheagenttonavigateamong
the agent being too conservative and not being able
pedestriansandrespecttheirpersonalspace.
tocleartheobstaclesintime,andtheagentbeingtoo
aggressiveandcollidingwiththeothermovingpedes-
In Interactive Navigation, we challenge the notion
trians.
that navigating agents are to avoid collision at any
One of the challenges for the Social Nav part was
cost. We argue for the contrary – in clutter-filled real
the difficulty in simulating the trajectories of the hu-
environments, such as homes, an agent will have to
man agents, including reactivity and interaction be-
2https://svl.stanford.edu/igibson/challenge2021.html tweenagents.Oftentimes,gettingtothegoalrequires
3https://svl.stanford.edu/igibson/challenge2020.html negotiation of the space or the agent would require
6
to go over the desired personal space threshold; or intheUnitygameengine. Habitathousesaresig-
thesimulatedhumanagentsbehaveerraticallydueto nificantly larger than those in RoboTHOR, often
limitationsonthebehaviormodelsandthespacecon- consistingofmultiplefloors.
straints. For future editions, we are to emphasize on
the importance of high fidelity simulation of naviga- • Target Objects. The RoboTHOR Challenge uses
tionwithhuman-likebehaviors. 13 relatively small objects as target object types
(e.g.AlarmClock,Basketball,Laptop). TheHabi-
FortheSim2Realcomponentofthe2020Challenge,
tat 2021 Challenge used 21 target objects types
a significant performance drop was observed during
andtheHabitat2022Challengeused6targetob-
the Sim2Real transfer, due to the reality gap in vi-
jecttypes. ThetargetobjecttypesinbothHabitat
sualsensorreadings,dynamics(e.g.motoractuation),
Challenges typically represent larger objects (e.g.
and 3D modeling (e.g. soft carpets). More analysis of
Bed,Fireplace,Sofa).
thetakeawayscanbefoundintheiGibsonChallenge
20204 and20215 videos,alongwiththewinningentry
For the RoboTHOR Challenge, state-of-the-art is
paper[218].
currently held by ProcTHOR [52], which has a test
SPL[9]of0.2884andasuccessrateof65%onunseen
3.1.3 ObjectNav scenesduringtraining.ProcTHORusesafairlysimple
modelthatembedsimageswithCLIP,feedsitthrough
In ObjectNav, the agent is tasked with navigating to aGRU,andusesanactor-criticoutputoptimizedwith
oneofasetoftargetobjecttypes(e.g.navigatetothe DD-PPO. Its novelty is pre-training on 10K procedu-
bed)givenego-centricsensoryinputs.Thesensoryin- rally generated houses (ProcTHOR-10K). It then fine
put can be an RGB image, a depth image, or combi- tunesinRoboTHOR.FortheHabitat2022Challenge,
nation of both. At each time step the agent must is- state-of-the art by SPL is also held by ProcTHOR,
sueoneofthefollowingactions: MoveForward,Rotate achieving 0.32 SPL and a success rate of 54% on un-
Right, Rotate Left, Look Up, Look Down, and Done. The seen scenes. For the Habitat 2022 Challenge, Proc-
Move Forward action moves the agent by 0.25m and THORpre-trainsonProcTHOR-10Kandfine-tuneson
therotateandlookactionsareperformedin30◦incre-
theHM3DSemanticsscenes. WhensortingtheHabi-
ments. tat 2022 Challenge entries by success rate, imitation
Episodesareconsideredsuccessfulif(1)theobject learning with Habitat-Web [154], fine-tuned with RL,
is visible in the camera’s frame (2) the distance be- achievesastate-of-the-art60%successrateandanSPL
tweentheagentandthetargetobjectiswithin1meter of0.30onunseenscenes. Habitat-Webbuiltawebin-
and(3)theagentissuestheDoneaction. Thestarting terfacetocollecthumandemonstrationsofObjectNav
locationoftheagentisarandomlocationinthescene. withAmazonMechanicalTurk. Italsoachievedstate-
Our workshop has held 2 ObjectNav challenges: of-the-artintheHabitat2021Challenge, withanSPL
the RoboTHOR ObjectNav Challenge [51] and the of0.146andasuccessrateof34%.
Habitat ObjectNav Challenge [166, 214]. Both chal-
lenges use the mentioned action and observation
space, as well as a simulated LoCoBot robotic agent.
3.1.4 Multi-ObjectNav
Incomparison:
InMulti-ObjectNav(MultiON)[198], theagentisini-
• Scenes. The RoboTHOR Challenge6 includes tialized at a random starting location in an environ-
89 room-sized dorm-like scenes. The Habi- ment and asked to navigate to an ordered sequence
tat 2021 Challenge7 90 houses from the Matter- of objects placed within realistic 3D interiors (Fig-
port3D dataset [27] and the Habitat 2022 Chal- ures 6a, 6b). The agent must navigate to each target
lenge8uses120housesfromtheHM3DSemantics
objectinthegivensequenceandcalltheFoundaction
dataset[151]. BothiterationsoftheHabitatChal- to signal the object’s discovery. This task is a gener-
lengeusescenescollectedfromreal-worldscans. alized variant of ObjectNav, whereby the agent must
In contrast, RoboTHOR scenes were hand-built navigate to a sequence of objects rather than a sin-
by 3D artists to be accessible in AI2-THOR [99] gle object. MultiON explicitly tests the agent’s navi-
gationcapabilityinlocatingpreviouslyobservedgoal
4https://www.youtube.com/watch?v=0BvUSjcc0jw
objectsandis,therefore,asuitabletestbedforevalu-
5https://www.youtube.com/watch?v=1uSsds7HSrQ
atingmemory-basedarchitecturesforEmbodiedAI.
6https://ai2thor.allenai.org/robothor/challenge
7https://aihabitat.org/challenge/2021/ TheagentisequippedwithanRGB-Dcameraanda
8https://aihabitat.org/challenge/2022/ (noiseless)GPS+Compasssensor. TheGPS+Compass
7
Task: FindtheBed
(a)
Figure 5. ObjectNav tasks the agent with navigating to a
givenobjecttypeinthescene.Thisexampleshowstheagent
taskedwithnavigatingtotheBedinthescene. Thehouseis
curtousyoftheArchitecTHORdataset[52].
sensorprovidestheagent’scurrentlocationandorien-
tationrelativetoitsinitiallocationandorientationin
theepisode. Itisnotprovidedwithamapoftheenvi-
ronment. TheactionspacecomprisesofMoveForward
by 0.25 meters, Rotate Left by 30◦, Rotate Right by 30◦
andFound.
The MultiON dataset is created by syntheti-
cally adding objects in the Habitat-Matterport 3D
(HM3D)[152]scenes. Theobjectsareeithercylinder-
shapedornatural-looking(real)objects. Asshownin
Figure 6a, the cylinder objects are of the same height
and radius, with different colors. However, such ob-
jects do not appear realistic in the indoor scenes of
Matterport houses. Furthermore, detecting the same
objectwithdifferentcolorsmightbeeasyfortheagent
to learn. This has led us to include realistic-looking (b)
objectsthatcannaturallyoccurinhouses(Figure6b).
Theseobjectsareofvaryingsizesandshapesandpose Figure 6. Multi-ObjectNav: (a) Top-down visualization of
a MultiON episode with 5 target cylinder objects in a par-
amoredemandingdetectionchallenge. Thereare800
ticularsequence; (b)Top-downvisualizationofaMultiON
HM3D scenes and 8M episodes in the training split,
episodewith5targetrealobjectsinaparticularsequence.
30 unseen scenes and 1050 episodes in the validation
split, and 70 unseen scenes and 1050 episodes in the
testsplit.Theepisodesaregeneratedbysamplingran-
domnavigablepointsasstartandgoallocations,such orRealobjectstobeinsertedbetweenthestartandthe
that the locations are on the same floor and a navi- goal, maintaining a minimum pairwise geodesic dis-
gable path exists between them. Next, five goal ob- tancebetweenthemtoavoidcluttering. Furthermore,
jects are randomly sampled from the set of Cylinder tomakethetaskevenmorerealisticandchallenging,
8
three distractor objects (which are not goals) are in- of 55%. This model is an evolution of Proj-Neural,
sertedineachepisode.Thepresenceofdistractorswill where three auxiliary tasks were used to inject infor-
encouragenewagentstodistinguishbetweengoalob- mationaboutthemapandobjectsintotheagent’sin-
jectsandotherobjectsintheenvironment.Anepisode ternalrepresentation.
is considered successful if the agent is able to reach In the 2022 challenge, instead, we noticed some
within1meterofeverygoalinthespecifiedorderand similarities between the Baseline method, Mem-
generatetheFOUNDactionateachgoalobject. Apart SLAM, and the winning entry in the 2022 MultiON
from the standard evaluation metrics used in Object- challenge, Exploration and Semantic Mapping for
Nav,suchasSuccessRate(SR)andSuccessweighted Multi Object-Goal Navigation (EXP-MAP). Both the
bypathlength(SPL)[9],weadditionallyuseProgress methods are modular, consisting of detection (iden-
and Progress weighted by path length (PPL) to mea- tifying objects from raw RGB images), Mapping (in-
sureagentperformance. Theleaderboardforthechal- crementallybuildingatop-downmapoftheenviron-
lengeisbasedonthePPLmetric. MultiONchallenge ment using Depth observations and relative poses),
was hosted on evalAI, an open-source platform for andPlanning(navigatingtoadetectedgoalobjectby
evaluatingandcomparingartificialintelligencemeth- generatinglow-levelactions)modules.Allthesemod-
ods. The participants implemented their methods in els record previously seen objects in some memory
docker images and submitted them to evalAI. The (e.g., semantic map of the environment). The EXP-
dockerimageswereevaluatedonevaluationservers, MAPcanachieve70%Progressand60%Successinthe
andtheresultswereuploadedtoevalAI. Test-ChallengesplitoftheCylinderobjectstrackofthe
challengewhileachieving55%Progressand40%Suc-
The MultiON task is similar to ObjectNav, but at
cessintheRealobjectstrack. Theseresultsshowthat
the same time, it tries to solve different challenges.
episodeswithnaturalobjectsaremorechallengingto
Notably,itaimstoinjectlong-termplanningcapabili-
detectthanthecylinders.
ties into the agents. In the ObjectNav task, the object
detection task takes on a fundamental role. Still, the
agent does not have to remember all the objects (and
their semantic information) encountered in the past.
In MultiON, on the other hand, we assume a more
limitedpartofthedetection(e.g., detectingcylinders
orasetofalimitednumberofnaturalobjects). Paral-
lelly, the agent must be able to remember the objects
already seen. Thus, this task is more tailored to the
real world than ObjectNav. In fact, the agents oper-
ateinthesameenvironmentforaverylongtimeand,
therefore,mustbeabletorememberwhathasalready
beenseen. Forthisreason,theapproachesdeveloped
forMultiON,unlikethoseforObjectNav,alwaysadd Figure 7. In the RVSU Semantic SLAM task, an au-
acomponentthatstoresthesemanticinformationob- tonomousagentexploresenvironmenttocreateaseman-
tainedthroughexploration. tic3Dcuboidmapofobjects.
For the 2021 challenge, a simpler setup was used.
3.1.5 NavigatingtoIdentifyAllObjectsinaScene
Thedistractorswereabsent,theobjectswereonlythe
cylinders, and the dataset was developed on Matter- The RVSU semantic SLAM challenge tasks partici-
port3D[26].TheProj-NeuralmodelwasusedasBase- pants with exploring a simulation environment to
line [198]. This model takes advantage of an ego- mapoutallobjectsofinteresttherein. Thischallenge
centric map that is used as an input for an end-to- asks a robot agent the question, “what objects are
end model that achieved 29% Progress and 12% Suc- where?” within the scene. Robot agents traverse a
cess. Surprisingly,twomodelsbasedonmappingand scene,createanaxisaligned3Dcuboidsemanticmap
pathplanning, SgoLAM(64%progress, 52%Success) of the objects within that scene, and are evaluated
and Memory Augmented SLAM (Mem-SLAM) (57% based on their map’s accuracy. Providing a semantic
Progress,36%Success),exceededtheresultsobtained understandingofobjectscanassistarobot’sabilityto
from the Baseline by a large margin demonstrating interpret attributes of its environment such as know-
that this type of model works well on long-horizon ing how to interact with objects and understanding
tasks. Instead, the model proposed in [113] won the what type of room it might be in. This semantic un-
2021 challenge, with a progress of 67% and a success derstanding is typically viewed as a semantic simul-
9
taneous localization and mapping (SLAM) problem. bestscore.
ThetaskofsemanticSLAMhasalreadyseengreatin- Current results from the RVSU Semantic SLAM
vestigation using static datasets such as KITTI [68], challengehaveshownthatwhilethechallengeissim-
SunRGBD[181]andSceneNet[115]. However,these ple in concept, there is still room for improvement
static datasets ignore the active capabilities of robots fromcurrentstate-of-the-artmethods. Thehighestre-
andforegosearchingthephysicalactionspaceforthe sultforsemanticSLAMachievedwas0.39OMQwhen
actions that best explore and understand an environ- using ground-truth pose data and passive control.
ment. Addressing this limitation, the RVSU seman- When digging deeper into the results provided, we
ticSLAMchallenge[82]helpsbridgethegapbetween canseethatalthoughthequalityofmatchingcuboids
passiveandactivesemanticSLAMsystemsbyprovid- is often good (pairwise quality of up to 0.72) there
ingaframeworkandsimulationenvironmentsforre- aretoomanyunmatchedcuboidstogetahighscore.
peatable,quantitativecomparisonofbothpassiveand Whencompetitorsbridgethegapfrompassivetoac-
activeapproaches. tive control, we also commonly see a drop in OMQ
Participationinthechallengeisconductedthrough ofapproximately0.06despitehavingmorecontrolof
simulated environments, accessed and controlled us- the robot’s observations. Those who participated in
ing the BenchBot framework [187]. The environ- bothpassiveandactivecontrolversionsoftheseman-
ments used are a version of the BenchBot environ- tic SLAM task, focused their research in how to map
ments for active robotics (BEAR) [83] rendered us- a scene given a sequence of inputs, rather than how
ing the NVIDIA Omniverse Isaac Simulator9. BEAR toactivelyexploretomaximizeunderstandingofthe
provides 25 high-fidelity indoor environments com- scene. Theseresultssuggestthatpotentiallythemost
prisingoffivebaseenvironmentswithfivevariations fruitful areas for future research lie in better filtering
thereof. Betweenvariations,objectsareaddedandre- outcuboidsthatdonotmatchanytrueobject,andin
moved, and lighting conditions are changed. Across how to best exploit active robot control to improve
environments there are 25 object classes of interest scene understanding. There is also yet to be an at-
to be mapped within the challenge. The challenge tempt at solving this challenge using active control
splits BEAR into 2 base environments for algorithm and noisy pose estimation which adds further diffi-
development and 3 for final testing and evaluation. cultytothechallenge.
The BenchBot framework enables a simulated robot
to explore BEAR using either passive or active con-
trolthroughdiscretisedactionsthatarepre-definedor 3.1.6 Audio-VisualNavigation
actively chosen by the agent respectively. The action
Moving around in the real world is a multi-sensory
space for robot agents is MOVE_NEXT for passive
experience, and an intelligent agent should be able
modeandMOVE_DISTANCEandMOVE_ANGLEfor
to see, hear and move to successfully interact with
active mode with magnitude of movement being de-
its surroundings. While current navigation mod-
fined by users with a minimum distance of 0.01 m
els tightly integrate seeing and moving, they are
and a minimum angle of 1°. BenchBot provides the
deaf to the world around them, motivated by these
robotagentaccesstoRGB-Dcamera,laser,andeither
factors, the audio-visual navigation task was intro-
ground-truth or estimated pose information for the
duced [38, 66], where an embodied agent is tasked
robotimmediatelyaftercompletinganygivenaction.
to navigate to a sounding object in an unknown un-
Theprogressionofpassivecontrolwithground-truth
mapped environment with its egocentric visual and
pose data, through to active control with estimated
audio perception (Figure 8). This audio-visual navi-
posedataisdesignedtograduallybridgethegapfrom
gationtaskcanfindapplicationsinassistiveandmo-
passive to active semantic SLAM. The final cuboid
bilerobotics, e.g., robotsforsearchandrescueopera-
mapcreatedbytheagentwithinthechallengeiseval-
tionsandassistivehomerobots. Alongwiththetask,
uatedusingthenewobjectmapquality(OMQ)mea-
theSoundSpacesplatformwasalsointroduced,afirst-
sure outlined in [82]. This evaluation measure con-
of-its-kindaudio-visualsimulatorwhereanembodied
sidersthequalityofeveryprovidedobjectcuboid, in
agent could move around in the simulated environ-
termsofbothgeometricandsemanticaccuracy,when
mentwhileseeingandhearing.
compared to its best match in the ground-truth map,
Audio-visual navigation is a challenging task be-
as well as the number of provided cuboids with no
cause the agent not only needs to perceive the sur-
matchingground-truthequivalentandviceverse.The
rounding environment, but also to reason about the
final OMQ score is between 0 and 1 with 1 being the
spatial location of the sound emitter in the environ-
9https://developer.nvidia.com/isaac-sim ment via the received sound. This new multimodal
10
We set up the AudioGoal navigation task on the
Where is the
phone? Matterport3D (MP3D) [27] scene dataset, split into
train/val/test splits in 59/10/12 for this challenge
due to its large scale. SoundSpaces provides au-
diorenderingsforMP3Dintheformofpre-rendered
Source room impulse responses (RIRs), which are transfer
Agent View
functions that characterize how sound propagates
Door
from one point in space to another point in space.
For all MP3D scenes, SoundSpaces discretizes them
intogridsofspatialresolution1meter×1meterand
Left Right
Door Audio Spectrogram provide RIRs for all pairs of grid points. For the
sourcesound,weuse73/11/18disjointsoundsinour
Figure 8. AudioGoal tasks an autonomous agent to find an train/val/testsplits,respectively. Eachsoundclipis1
audiosourceinanunmapped3Denvironmentbynavigat-
second long. The received sound at every step is the
ingtoagoal. Herethetopdownmapisoverlaidwiththe
result of convolution between the source sound and
acousticpressurefieldheatmap. Whileaudioprovidesrich
theRIRcorrespondingtothesourcelocationandcur-
directionalinformationaboutthegoal,andaudiointensity
rentagentposeinthescene. WhiletheMoveForward
variationiscorrelatedwiththeshortestpathdistance,vision
actiontakestheagentforwardby1meterinthedirec-
reveals the surrounding geometry in the form of obstacles
andfreespace. AnAudioGoalnavigationagentshouldin- tionit’scurrentlyfacingifthereisanavigablenodein
telligentlyleveragethesynergyofthesetwocomplementary the scene grid in that direction, Rotate Left and Rotate
signalstosuccessfullynavigateintheenvironment.
Rightrotatetheagentby90◦intheclockwiseandanti-
clockwisedirections,respectively. Theepisodetermi-
nates when the agent issues the Done action, or it ex-
embodied navigation task has gained attention over ceedsabudgetof500steps.
the past few years and different methods have been
In SoundSpaces Challenge 2021 and 2022, a total
proposed to solve this task, including learning hier-
of25teamsshowedinterestand8teamsparticipated.
archicalpolicies[39],trainingrobustpolicieswithad-
ForSoundSpacesChallenge2022’sleadingteams,we
versarialattack[222]ordataaugmentationforgener-
observedsomesimilaritiesbetweenthemodeldesign
alizationtonovelsounds[219]. However,theperfor-
of the top two teams. Both models used a hierarchi-
manceofSOTAaudio-visualnavigationmodelsisstill
calnavigationarchitecture(inspiredbyAV-WaN[39]),
not perfect, and thus we organized the SoundSpaces
whereahigh-level(long-term)plannerpredictsanav-
Challenge 10 at CVPR 2021 and 2022, which aims
igation waypoint in the local neighborhood of the
to promote research in the field of developing au-
agentateachstep, andalow-level(short-term)plan-
tonomous embodied agents that are capable of navi-
ner executes atomic actions, such as Move Forward
gatingtosoundingobjectsofinterestusingaudioand
and Rotate Left, to take the agent to the predicted
vision.
waypoint. Further, agents that leverage the audio-
More specifically, in an AudioGoal navigation visualcuesfromthefull360◦FoVandtrainaseparate
episode, a sound source is placed at a random loca-
model for stopping are more successful and efficient
tion in the environment, and the agent is also posi-
than the others. Moreover, training an AudioGoal
tionedwitharandompose(locationandorientation)
navigation agent in the presence of distractor sound
atthestartoftheepisode. Theagentistaskedtonavi-
sourcesalsoresultsinlearningrobustnavigationpoli-
gatetothesoundingobjectwithoneofthefouractions
cies that boost navigation performance. The presen-
fromtheactionspace:MoveForward,RotateLeft,Rotate
tationvideosfromtheleadingteamscanbefoundon
Right, and Done. At each episode step, the agent re-
thechallengewebsite.
ceives egocentric (noiseless) RGB-D images captured
with a 90◦ field-of-view (FoV) camera, the binaural OneofthelimitationsoftheSoundSpacesplatform
is that it provides pre-rendered RIRs for fixed grid
audio received by the agent. The episode terminates
pointsanddoesnotallowuserstorendersoundsfor
whentheagentexecutestheDoneaction,oritrunsout
arbitrarylocationsorenvironments. Totacklethisis-
ofapre-specifiedtimebudget. Theagentisevaluated
sue,wehaveintroducedSoundSpaces2.0[40](Fig.9,
usingstandardembodiednavigationmetrics,suchas
a continuous, configurable and generalizable simu-
SuccessRate(SR)andSPL[9]. WeuseSPLasthemet-
lator. This new simulator has enabled continuous
ricforrankingchallengeparticipants.
audio-visualnavigationaswellasmanyotherembod-
10https://soundspaces.org/challenge iedaudio-visualtasks. Webelievethissimulatorwill
11
Gypsum
Visual
Reflection
HRTF Render
Reverb Direct sound RGB
Transmission
Carpet
Acoustic Left Ear
Render
Hardwood
Right Ear
Figure9. SoundSpaces2.0,acontinuous,configurable,and
generalizable audio-visual simulation platform. It models
variousacousticphenomenaandrendersvisualandaudio
observationswithspatialandacousticcorrespondence.
taketheaudio-visualnavigationtasktothenextstep.
Anotherimportantdirectionforfutureresearchisfor
the agent to reason about the semantics between the Figure10.Exampleofthescenechangedetectionchallenge.
soundandobjects(e.g.semanticaudio-visualnaviga- Between two scenes some objects are added (blue) and re-
tion [37] and finding fallen objects [64]). If the agent moved(orange)andtheseneedtobeidentifiedandmapped
could leverage the semantics of sounding objects, it out.
couldnavigatefasterbyreasoningwheretheobjectis
locatedinspacebasedonitscategoryinformation.
We believe studying audio-visual embodied AI is The setup for SCD is similar to that shown for the
of vital importance for building truly autonomous RVSUSemanticSLAMchallengedescribedpreviously
robots with rich perception modalities in the real but with some differences in challenge setup within
world. BenchBot. The SCD challenge also uses the BEAR
dataset [83] which already has multiple variants of
3.2.RearrangementChallenges
a set of base scenes. Variants differ in some objects
This section discusses rearrangement challenges. areaddedandsomeremoved(asdesiredfortheSCD
RearrangementisdescribedasacanonicaltaskinEm- task), and also there are some lighting variations to
bodied AI that may lead to learning representations increase challenge difficulty. BenchBot enables the
that are for many downstream tasks [16]. Here, the switching between environment variants within one
agentsgoalistomoveordetectthechangesfromone SCDsubmissionassoonastherobotagentdetermines
stateofthescenetoanother.Forexamples,severalob- itisfinishedwithitsfirsttraversal. BenchBotsupplies
jects, such as an apple and a banana may move, and the same robot control that progresses from passive
the agent is tasked with detecting that they moved toactivecontrolviadiscreteactionsandwhereineach
andputtingthembacktotheircorrectlocations. actionisfollowedbyanobservationcontainingRGB-
D images, laser scan, and either ground-truth or es-
timated robot pose data. The SCD challenge utilises
a variant of the OMQ evaluation measure [82] which
3.2.1 SceneChangeDetection
evaluates the final 3D object cuboid map output as
part of the challenge. This variant introduces the ne-
The RVSU scene change detection (SCD) challenge,
cessityforthemaptoprovideanestimateforthelike-
an extension to the RVSU semantic SLAM challenge,
lihoodthatanobjecthasbeenaddedorremovedfrom
requires identification and mapping of objects which
thescenebetweentraversals. Thisstateestimationfor
have been added and removed between two traver-
theobjectisthencombinedwiththeestimationofthe
salsofthesamebasescene[82].Humanenvironments
labelandlocationoftheobjecttomakeuptheobject-
are inherently non-static with objects frequently be-
levelqualityscore.Asbefore,thebestOMQscorepos-
ing added, removed, or shifted. In order to operate
sibleis1andtheworstis0.
withinsaidenvironmentswhilstutilisingobjectmaps,
itbecomesimportanttobeabletoidentifywhenthese There has been limited engagement with the SCD
changeshaveoccurred. Thischallengeexaminesper- challenge and there is much room for improvement.
haps the simplest of such scenarios, where some ob- IntheCVPR2022iterationofthischallengethehigh-
jects are added or removed while all others remain estOMQscoreachievedwas0.25. Thisisquitelower
fixedinplace. thanthebestOMQscoreofthesemanticSLAMchal-
12
lengewhichwasabletoreachOMQof0.39. Thiscan tween goal and initial environment states is confined
beattributedsomewhattotheapproachthatcompeti- toobjectspose(position/rotation)andattributes(e.g.
tors used in solving SCD. All SCD submissions per- istheobjectopenedorclosed?). Successfulrearrange-
formedsemanticSLAMonthetwodifferenttraversals mentinthesetasksrequiresagentstoflexiblyencode
and did a naive comparison of the resultant cuboid environment environment states, to dynamically up-
maps. This led to an accumulation of the errors seen date these encodings as they interact with their en-
across the maps for both traversals. This simple be- vironment, and also to making long-term plans (fre-
ginningshowsthattherearemanydirectionsthatcan quently of the traveling-salesman variety) to maxi-
still be experimented with in order to improve SCD mize the efficiency of rearrangement. We now detail
in future years. This may include more targeted ap- the two rearrangement challenges, AI2-THOR Visual
proaches to navigation and/or mapping within the RoomRearrangementandTDW-Transport,heldatthe
second traversal which utilises the scene knowledge EAIworkshopinpastyears.
from the first traversal. There is still much research The AI2-THOR Visual Room Rearrangement
to be done in how to reliably identify and map out (RoomR) task [200] occurs in two phases, see Fig-
changesbetweenscenes. ure11. IntheWalkthroughphasethetheagentexplores
a room and builds an internal representation of
the room’s configuration (sgoal). Then, in the Un-
shuffle phase, the agent is placed within the same
environment but objects within this environment
have been randomly moved to different locations
and opened/closed (sinit), the agent must now re-
store objects back to their original states. As this
2-phase RoomR is quite challenging, a 1-phase vari-
ant was also proposed where the agent enacts the
Walkthrough and Unshuffle phases simultaneously,
receiving egocentric RGB-D images of the environ-
ment in both the sinit and sgoal states at each step.
In the 2021 RoomR challenge, no participants were
able to outperform the baseline model, which used
Figure11. AI2-THORVisualRoomRearrangementChal-
lenge. Anagentmustchangeposeandattributesofobjects a 2D semantic mapping approach along with imita-
in a household environment to restore the environment to tion learning from a heuristic expert agent. In 2022
aninitialstate. however, several exciting approaches were released
resulting in dramatic improvements in performance.
Forthe1-phasevariant,performanceleaptfrom≈9%
3.2.2 InteractiveRearrangement to ≈24% on the FIXEDSTRICT metric on the test-set.
Advances making this possible included (1) the
While the PointNav and ObjectNav tasks have led to
use of CLIP-pretrained visual encoders [96] and (2)
substantialadvancesinembodiedAI,performanceon
large-scalepre-trainingusingprocedurallygenerated
these tasks has steadily improved with PointNav be-
environments[52]. Unliketheend-to-endapproaches
ingnearlysolved [152]. Inlightofthisfastprogress,
used for the 1-phase variant, the most successful
researchers from nine institutions proposed the rear-
methods for the 2-phase variant used powerful
ragementasthenextfrontierforresearchinembodied
inductive biases in the form of semantic mapping
AI [16]. At a high level, in rearrangement, an em-
andplanningalgorithms. Inayetunpublishedwork,
bodied agent must interact with its environment to
2022 2-phase challenge winner used voxel-based 3D
transform the environment from it’s initial state sinit
semantic map and shortest path planners to bulid an
to a goal state sgoal. This general formulation of re-
agent attaining ≈15% FIXEDSTRICT on the test-set
arrangement leaves much unspecified, namely: (1)
(dramatically beating the baseline performance of
which environment? (2) what affectors/actions are
< 1%). Thedifferencesbetweentheapproachesused
available to agent? (3) how are states sinit,sgoal spec-
inthe1-and2-phasevariantsisstriking: itseemsthat
ified? Given the EAI community’s focus on building
newalgorithmsarerequiredtobringfullyend-to-end
agentscapableofassistinghumansineverydaytasks,
methodstothechallenging2-phasesetting.
all existing instantiations of the rearrangement task
embody agents in household environments and fo-
cusonobject-basedrearrangement: thedifferencebe- TDW-Transport Challenge [67] is an object-goal
13
trained agents. Additionally, language imposes a
data-sparseregime,asexamplescannotbecreatedau-
tomatically,asprecisioninlanguageisdirectlytiedto
aspecificscenelayout(e.g. “ontheleft/right"),andit
isanopenchallengeastoifunimodalrepresentations
canbeleveragedinthisembodiedspace[20].
Figure12.TDW-TransportChallenge.Inthisexampletask,
the agent must transport two objects on the table in one RxR-Habitat
roomandplacethemonthebedinthebedroom. Theagent RxR
canfirst pickup thecontainer, puttwo objectsinto it, and
thentransportthemtothetargetlocation.
Youareinabedroom.Turnaroundtotheleftuntilyouseea
doorleadingoutintoahallway,gothroughit.Hangarightand
driveninteractivenavigationtask(seeFigure 12). In walkbetweentheislandandthecouchonyourleft.Whenyou
this challenge, an embodied agent is spawned ran- arebetweenthesecondandthirdchairsfortheislandstop.
domlyinahouseandisrequiredtofindasmallsetof
objectsscatteredaroundthehouseandtransportthem Figure 13. The Room-Across-Room Habitat Challenge
to a desired final location. We also position various (RxR-Habitat)isamultilingualinstruction-followingtask
containersaroundthehouse;theagentcanfindthese set in simulated indoor environments requiring realistic
containersandplacesomeobjectsintothem. Without navigationoverlongactionsequences.
using a container as a tool, the agent can only trans-
3.3.1 NavigationInstructionFollowing
portuptotwoobjectsatatime.However,usingacon-
tainer, the agent can collect several objects and then Navigationguidedbynaturallanguagehaslongbeen
transport them together. While the containers help a desired foundational ability of intelligent agents.
theagenttransportmorethantwoitems,italsotakes In Vision-and-Language Navigation (VLN), an agent
some time to find them. Therefore, the agent has to is given egocentric vision in a realistic, previously-
decidetousecontainersornot. unseenenvironmentandtaskedwithfollowingapath
The embodied agent is equipped with an RGB-D described in natural language, e.g., Move toward the
camera. There are two types of actions of the agent: dining table. Go down the hallway toward the kitchen
navigation and interactive actions. Navigation ac- and stop at the sink. The Room-Across-Room Habi-
tions include Move Forward(α meters), Turn Left(θ tatChallenge(RxR-Habitat)instantiatesVLNinsimu-
degrees),TurnRight(θdegrees).Interactiveactionsin- lated indoor environments, provides multilingual in-
clude Reach to object, Put into container, Grasp, and structions, and requires agents to navigate via long
Drop. The objective of this challenge is to transport action sequences in a realistic, continuous 3D world
the maximum number of objects in fixed steps as ef- (Figure13). SolvingRxR-Habitatwouldhaveapplica-
ficiently as possible. We use the transport rate as an tionsinmanydomains,suchaspersonalroboticassis-
evaluation metric, which measure the fraction of the tants, and lead to a better scientific understanding of
objects successfully transported to the desired posi- theconnectionbetweenlanguage,vision,andaction.
tionwithinagivenbudget. TheRxR-HabitatChallengetakesplacein3Drecon-
structions of Matterport3D scenes [28] and interacts
3.3.EmbodiedVision-and-Language
with those scenes using the Habitat Simulator [166].
This section discusses the embodied vision-and- We model the agent embodiment after a robot of ra-
language challenges. In each challenge, natural lan- dius0.18mandheight0.88mwithacameramountat
guage is used to convey the goal to the agent. For 0.88m. An episode is specified by a scene, a start lo-
example, the agent may be tasked with following in- cation, a language instruction, and the implied path.
structions to complete a task. Since language is the At each time step, the agent observes egocentric vi-
primary means of human communication, advances sion in the form of a single forward-facing, noiseless
inembodiedvision-and-languageresearchwillmake 480x640 RGB-D image with a 79◦ HFOV. The agent
it easier for a human to naturally interact with the also receives the natural language instruction from
14
one of three languages: English, Hindi, or Telugu. candidateprediction,waypointselection(thediscrete
The action space is discrete and noiseless, consisting VLN task), and waypoint navigation. For waypoint
of actions {MOVE_FORWARD, TURN_LEFT, TURN_RIGHT, STOP, selection, a history-aware transformer was trained in
LOOK_UP,LOOK_DOWN}. Forwardmovementis0.25mand discreteVLNwithaugmentationsincludingsynthetic
turning and looking actions are performed in 30◦ in- instructions,environmentediting,andensembling. It
crements. Actions that result in collision terminate was then transferred and tuned in continuous envi-
upon collision, i.e., no wall sliding. An episode ends ronments. Despite this remarkable improvement, a
whentheagentcallsSTOP. performancegapstillexistsbetweenSOTAincontin-
The dataset used in RxR-Habitat is the Room- uous versus discrete environments, with human per-
Across-Room (RxR) dataset [102] ported from high- formance even higher. Evidently, this direction of re-
level discrete VLN environments [11] to the continu- searchisstillfarfromsaturated.
ousVLN-CEenvironments[101]usedinHabitat. The
Goal: "Rinseoffamugandplaceitinthecoffeemaker"
datasetissplitintotraining(Train:60,300episodes,59
scenes),validationinenvironmentsseenduringtrain- 2
3
ing(Val-Seen: 6,746episodes,57scenes),validationin 1 " frp oic mk tu hp et ch oe ffd ei erty mm aku eg r" "turnandwalktothesink"
"walktothecoffee
environments not seen during training (Val-Unseen: makerontheright"
11,006 episodes, 11 scenes), and testing in environ-
mentsnotseenduringtraining(Test-Challenge: 9,557
episodes, 17 scenes), each with a roughly equal dis- t=0 t=10 t=21
visualnavigation objectinteraction visualnavigation
tributionbetweenEnglish,Hindi,andTeluguinstruc-
5 6
tions. To submit to the RxR-Habitat leaderboard 11, 4 "washthemuginthesink" " bp ai cc kk tu op thth ee cm ofu feg ea mnd akg eo r" " inpu tht eth ce oc ffl ee ean mm aku eg r"
participants run inference on the Test-Challenge split
andsubmittheinferredagentpaths. Theleaderboard
evaluates these paths against held-out ground-truth
paths. Agent performance is reported as the average
ofepisodicperformance.Theofficialcomparisonmet- t=27 objectinteraction t=36 visualnavigation t=50 objectinteraction
ricbetweentheagent’spathandthegroundtruthpath statechanges memory
is normalized dynamic time warping (nDTW) [111]
Figure 14. ALFRED involves interactions with objects,
which scores path alignment between 0 and 1 with
keeping track of state changes, and references to previ-
1 indicating identical paths. Additional metrics re-
ousinstructions. Thedatasetconsistsof25klanguagedi-
ported for analysis include path length (PL), naviga-
rectivescorrespondingtoexpertdemonstrationsofhouse-
tionerror(NE),successrate(SR)andsuccessweighted
holdtasks. Wehighlightseveralframescorrespondingto
byinversepathlength(SPL)[9]. portionsoftheaccompanyinglanguageinstruction.
RxR-Habitatisincrediblydifficult;theinterplaybe-
tweenperception,control,andlanguageunderstand- 3.3.2 InteractiveInstructionFollowing.
ing makes instruction-following an interdisciplinary
ALFRED is a benchmark for connecting human lan-
problem. Realistic environments and unconstrained
guage to actions, behaviors, and objects in interactive
natural language lead to a long tail of vision and
visual environments. Planner-based expert demon-
language grounding, and the low-level action space
strationsareaccompaniedbybothhigh-andlow-level
makes learning the relationship between instructions
human language instructions in 120 indoor scenes in
and actions highly implicit. The RxR-Habitat Chal-
AI2-THOR.Thesedemonstrationsinvolvepartialob-
lengetookplacein2021andagainin2022. Thebase-
servability,longactionhorizons,underspecifiednatu-
line model is a cross-modal attention (CMA) model
rallanguage,andirreversibleactions.
[101]thatattendsbetweenvisionandlanguageencod-
The dataset includes over 25K English language
ings, predicts actions end-to-end from observation,
directives describing 8K expert demonstrations av-
andistrainedwithbehaviorcloning(nDTW:0.3086).
eraging 50 steps each, resulting in >428K image-
In the first year, teams failed to surpass the perfor-
action pairs. Motivated by work in robotics on
mance of this baseline. However, a significant im-
segmentation-based grasping, agents in ALFRED in-
provementinSOTAwasattainedin2022;thetopsub-
teractwithobjectsvisually, specifyingapixelwisein-
mission(Reborn[8])producedannDTWof0.5543—
teraction mask of the target object. This inference
an 80% relative improvement over the baseline. This
is more realistic than simple object class prediction,
was enabled by an effective hierarchy of waypoint
wherelocalizationistreatedasasolvedproblem. Ex-
11https://ai.google.com/research/rxr/habitat istingbeam-searchandbacktrackingsolutionsarein-
15
feasibleduetothelargeractionandstatespaces,long 3.3.3 InteractiveInstructionFollowingwithDialog
horizon,andinabilitytoundocertainactions. Agents
Task-driven Embodied Agents that Chat (TEACh) is
are evaluated on their ability to achieve directives in
adatasetofover3,000human–human,interactivedi-
both seen and unseen rooms. Evaluation metrics in-
alogues and demonstrations of household task com-
clude: success rate (SR), success weighted by path-
pletion in the AI2-THOR simulator. Robots operat-
length(SPL),andGoal-Conditionsuccesswhichmea-
ing in human spaces must be able to engage in such
surescompletedsubtasks.
naturallanguageinteractionwithpeople,bothunder-
Currentstate-of-the-artapproachesinALFREDuse standing and executing instructions and using con-
spatial-semantic mapping [21, 119] to explore and versation[159,190]toresolveambiguity[131]andre-
build persistent representations of the environment coverfrommistakes. ACommanderwithaccesstoor-
before grounding instructions. These representations acle information about a task communicates in natu-
have also been coupled with symbolic planners and ral language with a Follower. The Follower navigates
modular policies for better generalization to unseen through and interacts with the environment to com-
rooms. Currently,thebestperformingagentachieves pletetasksvaryingincomplexityfromMake Coffeeto
40%successinseenroomsand36%inunseenrooms. Prepare Breakfast,askingquestionsandgettingaddi-
tionalinformationfromtheCommander(Figure15).
Thereare12tasktypesinTEAChwith438unique
combinationsoftaskparameters(e.g.,Make Saladwith
1versus2slicesofTomato)in109AI2-THORenviron-
Let’s put all the forks in the sink. ments. Onaverage,therearemorethan13utterances
Check the fridge to your left. ineachcooperativedialogue,withtaskstakinganav-
erageof131Followeractionstocompletecomparedto
(a) (b) ALFRED’s 50 due to both task complexity and non-
optimal planning. A major difference between the
TEACh and ALFRED challenge is edge cases in the
environmentsduetoALFRED’srejectionsampling: if
a PDDL planner could not resolve an ALFRED task
Put all Fork In any Sink (c) given an initial scene configuration, it was rejected
[ ] The Fork needs to be fromdata,whereTEAChsceneconfigurationsarere-
put into a Sink jected only when a human cannot resolve them. This
[ ] The Fork needs to be decisionresultsinmany“cornercases”inTEAChthat
put into a Sink
require human ingenuity, for example filling a pot
withwaterusingacupasanintermediatevesselwhen
(d) thepotitselfistoolargetofitinthesinkbasin.
TheTwo-AgentTaskCompletion(TATC)challenge
is based on the TEACh data, and involves modeling
both the Commander and Follower agents, which have
distinctactionandobservationspacesbutacommon
household task goal. The Commander agent has ac-
cess to a structured representation of the goal and its
componentparts, aswellassearchfunctionstoiden-
tify the locations and physical appearance of objects
in the environment by class or id. The Follower is
Found a fork on the top
shelf. Where’s the sink? analogoustoanALFREDagent,butwithawiderac-
tionspacethatincludes,forexample,pouringliquids
Figure15. IntheTEAChTwo-AgentTaskCompletionchal- from one container to another. Further, object inter-
lenge,theCommanderhasoracletaskdetails(a),objectloca- actions are done via individual (x,y) coordinate pre-
tions(b),amap(c),andegocentricviewsfrombothagents, dictions,ratherthanthefullobjectmasksusedinAL-
butcannotactintheenvironment,onlycommunicate. The
FRED, analogous to the click inputs of human users
Follower carries out the task and asks questions (d). The
whoprovideddemonstrations. Theagentsbothhave
agentscanonlycommunicateviatext.
a communicate action that adds to a mutually-visible
dialoguehistory,andrequiresgeneratingtext.
TATC agents are evaluated via SR and SPL, simi-
16
lar to ALFRED agents. Rule-based, planning agents hundreds of millions to billions of steps. DD-PPO
forTATCachieveabout24%SR,withplanningcorner presented an on-policy RL method that has been
cases dominating failures. A learned Follower based used throughout embodied AI to train agents in a
on the Episodic Transformer [136] with a rule-based, distributed manner [202, 204]. It scaled PointNav
simple Commander that simply reports the raw text to train for billions of steps across 64 GPUs and
of the next task subgoal as a communication action showednear-perfectperformanceinPointNavinun-
achieves nearly 0%. We are eager to see whether seenenvironmentswithjustanRGB-Dcameraanda
mapping-based approaches like those succeeding at GPS+Compass sensor. For ObjectNav training, Proc-
ALFRED can adapt to the wider space of tasks and THOR similarly trained with DD-PPO for 420 mil-
environmentcornercasesinTEACh. lion steps, and was later fine-tuned for 195M steps
onHM3D[151]andfine-tunedfor29millionstepson
4.CommonApproaches RoboTHOR[51]. Habitat-Webusedbehaviorcloning
to train for 400 million steps for their 2021 Habitat
Thissectionpresentscommonapproachesusedby
Challenge entry. With the growing size of datasets,
the winners of the challenges. We discuss large-scale
andtheaddedperformancegainedbytrainingforor-
trainingbyscalingupdatasetsandcompute,leverag-
ders of magnitude longer, we suspect further scaling
ing visual pre-trained models such as CLIP, the use
computetoleadtobetterperformingagents.
ofinductivebiasessuchasmaps,goalembeddingsto
representdifferenttasks,andvisualanddynamicaug- 4.2.VisualPre-Training
mentation to make simulators more noisy and closer
toreality. Initial successes in deep reinforcement learning
were largely focused on graphically simplistic envi-
4.1.Large-ScaleTraining
ronments,e.g.Atarigames,forwhichcomplexvisual
processing was, in large part, unnecessary. For in-
Embodied AI is seeing the same trend as com-
stance,theseminalworkofMnihetal.[122]achieved
puter vision and natural language processing, where
human-level performance on dozens of Atari games
massive datasets and more computing power enable
usingusedamodelwithonlythreeconvolutionallay-
higher-performingmodels.
ers. Several initial works in embodied AI, in part
Massive datasets have been obtained by Proc-
due to computational constraints when training RL
THOR, which trains on 10K procedurally generated
agents, adopted this mindset; for instance, Savva et
houses [52]; HM3D, which captures 1K static scans
al. [167] trained models using a 3-layer image pro-
of real-world environments [151]; and Habitat-Web,
cessing CNN for Point Navigation. As embodied
whichbuildsanAmazonMechanicalTurktasktocol-
agents are, ostensibly, meant to be embodied in the
lect 80K imitation learning examples of humans per-
real-world, one might expected the they would ben-
forming ObjectNav in simulated environments [154].
efitfromimageprocessingarchitecturesdesignedfor
ProcTHOR supports object interaction, and training
use with real images and, indeed, this has proven to
a simple RGB model with it using on-policy RL led
be the case. A recent work has shown that modi-
to state-of-the-art results for the Habitat 2022 Object-
fying existing embodied baseline models by replac-
NavChallenge,theRoboTHORObjectNavchallenge,
ing their visual backbones with a CLIP-pretrained
andtheAI2-THORRearrangementChallenge. More-
ResNet-50 can result in dramatic improvements [96].
over,0-Shotperformance,withmodelspre-trainedon
The top performing models of 1-Phase Rearrange-
ProcTHOR, often beats the same models trained on
ment,RoboTHORObjectNavleaderboards,andHabi-
the training data from the benchmark it is evaluated
tatObjectNavleaderboard,usevariantsofthis“Emb-
on. The scale and diversity of HM3D led to models
CLIP”architecture[52]. Severalothertopperforming
trained on it achieving state-of-the-art performance
modelstootherchallengesusepretrainedvisionmod-
forPointNavmodelswhenevaluatedonGibson[209],
els for object detection and semantic segmentation
MP3D[26],andHM3D[151].Usingimitationlearning
(RVSUSemanticSLAM,MultiON,andTwo-PhaseRe-
to train on Habitat-Web led to state-of-the-art results
arrangement).
inthe2021HabitatObjectNavChallenge,whichlater
improveditsperformancewithonlinefine-tuning.We
4.3.End-to-endvsModular
expectthetrendofbuildingandtrainingonmassively
larger datasets to continue leading to better general- In the last few years, two classes of methods have
ization. emerged for various embodied AI tasks: (1) end-to-
Simultaneously, many of the top approaches to end and (2) modular. The end-to-end methods learn
these challenges are scaling compute to train on topredictlow-levelactionsdirectlyfrominputobser-
17
BestEnd-to-end BestModular
Challenge Simulator Method Success Rank Method Success Rank
ObjectNav Habitat Habitat-Web 60 2 Stretch 60 1
Audio-VisualNavigation SoundSpaces FreiburgSound 73 2 colab_buaa 78 1
Multi-ON Habitat - - - exp_map 39 1
NavigationInstructionFollowing VLN-RxR CMABaseline 13.93 10 Reborn 45.82 1
InteractiveInstructionFollowing AI2-THOR APM 15.43 14 EPA 36.07 1
Rearrangement AI2-THOR ResNet18+ANM 0.5 6 TIDEE 28.94 1
Table1.Tablesummarizingtheperformanceofbestend-to-endandbestmodularmethodsacrossvariouschallenges.
vations. They typically use a deep neural network lar methods on easier and relatively shorter horizon
consisting of a visual encoder followed by a recur- tasks such as ObjectNav and Audio-Visual Naviga-
rent layer for memory and are trained using imita- tion, the performance gap increases as the complex-
tionlearningorreinforcementlearning.Earliestappli- ityofthetaskincreasessuchasinInteractiveNaviga-
cation of end-to-end methods on embodied AI tasks tionandRearrangement. Thisislikelybecauseasthe
include [32, 35, 86, 104, 120, 165, 227]. End-to-end taskhorizonincreases,theexplorationcomplexityin-
RL methods have also been scaled to train with bil- creases exponentially when training end-to-end with
lions of samples using distributed training [205] or justreinforcementlearning.
using tens of thousands of procedurally generated
4.4.VisualandDynamicAugmentation
scenes [52]. Researchers have also introduced some
structureinend-to-endpoliciessuchusingspatialrep- Visual and dynamic augmentation of real-world
resentations[33,72,78,85,134]andtopologicalrepre- datasets has proven to be a key technique for en-
sentations[163,164,216]. abling robotic systems trained in simulation to trans-
Modular methods use multiple modules to break fer to unseen environments and even to reality. For
downtheembodiedAItasks. Eachmoduleistrained yearsintheroboticsandlearningcommunity,apreva-
for a specific subtask using direct supervision. The lentattitudehasbeenthatsimulationtransferspoorly
modular decomposition typically includes separate to reality. One justification for this perspective is
modules for perception (mapping, pose estimation, thatthedynamicsmodelsofmostsimulationsarenot
SLAM), encoding goals, global waypoint selection good enough to reveal problems that typically occur
policies, planning and local obstacle avoidance poli- in real robotic deployments, such as wheel slippage,
cies. Rather than training all modules end-to-end, odometry drift, floor irregularities, nonlinear motor
eachmoduleistrainedseparatelyusingdirectsuper- anddynamicresponses,andcomponentbreakageand
vision, which also allows use of non-differentiable burnout. Anotherjustification isthatsimulated eval-
classical modules within the embodied AI pipeline. uation can reveal problems with systems, but can-
Earliestlearning-basedmodularmethodsinclude [30, notvalidatethem: validationtestsforroboticsystems
31,34]whichshowtheireffectivenessonvariousnav- mustultimatelybeperformedon-robot.
igationtaskssuchasExploration,ImageNavandOb- Nevertheless, many existing systems have shown
jectNav. Variants of these methods include improve- successful transfer to novel and to real-world envi-
ments in mapping by anticipating unseen parts [149] ronmentsbyaugmentingtrainingdatasetswithnoise,
or by using density-based maps [19]; and learning staticobstacles,dynamicobstacles,andchangestovi-
global waypoint selection policies in ObjectNav and sualappearance. Manyapproachesaddnoisetosen-
ImageNaventirelyusingofflineorpassivedatasetsto sors, actions and even environment dynamics, effec-
improvesampleandcomputeefficiency[81,118,150, tively making each episode occur in a distinctive en-
199]. Recently, modular methods have also been ap- vironment; these techniques have proved useful for
plied to longer horizon tasks such as Navigation In- translating LiDAR-based policies trained in simula-
struction Following in VLN-CE [8, 100, 157], Interac- tion to the real world [58, 59] and for estimating the
tiveInstructionFollowinginALFRED[107,119,127], safety of plans prior to deployment [213]. Other ap-
RearrangementinAI2Thor[162,192],andRearrange- proaches improve performance by adding static ob-
mentinHabitat[93]. stacles to the environment in simulation, also effec-
In Table 1, we show the performance of best end- tivelyincreasingthespaceofenvironmentstrainedon
to-endandmodularmethodsinvarious2022Embod- [213]. Aninterestingexampleofthispresentedatthe
ied AI challenges. The table shows that while end- workshop involves training in a simulated environ-
to-end method performance is comparable to modu- mentwithvariabledynamicsandusinganadaptation
18
moduletoperformsystemidentificationinrealenvi- showssupervisedpre-trainingiseffectivefornaviga-
ronments[103],[61]. tion and manipulation tasks [168, 172, 217]. How-
However, visual policies present other difficulties: ever,alarge-scalestudy[204]showedthatatscale,su-
apolicytrainedononesetofobjectsandlightingcon- pervisedpre-trainingvisualrepresentationsfromIm-
ditionsisunlikelytotransfertootherobjectsandcon- ageNetcouldhurtdownstreamperformanceinPoint-
ditions [52]. Adding noise has been used to improve Nav. EmbCLIPshowsthatunsupervisedpre-training
robustness[57],andtheRSVUchallengesadddistrac- with a pre-trained CLIP visual encoder is effective
torobjectstoreducetheeffectsofdistractors[82]. The for various embodied AI tasks [96]. Other works ex-
RL-CycleGan approach uses style transfer to make plore pre-training with masked auto-encoders [212],
simulated environments appear more like the real contrastivelearning[55,128,171],orotherSSLobjec-
world [156]. Most recently, ProcTHOR [52] attempts tives [215]. Future work may explore tailoring pre-
to address the visual diversity issue by generating training objectives specifically for control. For exam-
largenumbersofsyntheticenvironments. ple, pre-training may account for the temporal as-
Finally, while the pandemic disrupted many pect of decision making [75], be embodiment agnos-
plans for real-world deployments, both the iGibson, tic [183], curiosity-driven [55],or avoid pixel recon-
RoboTHOR, and Habitat challenges included tests of struction [225]. Analogous to pretrained visual rep-
simulation-trained policies in real deployments [17, resentation for visual navigation, audio-visual repre-
51, 210]. These environments proved challenging for sentations [7, 121, 125] can be adopted for tasks with
many policies; nevertheless, many policies were still multi-modalinputs[38,66]infuturework.
able to function, and going forward tests in the real Another way pre-training may benefit embodied
willbeanimportantvalidationstepforembodiedAI AI is with scaling model and dataset size. Currently,
agents. Asdatasetscollectedfromrealevaluationsin- works use a variety of datasets for pre-training such
crease,theopportunityexiststotrainpoliciesdirectly as Epic Kitchens [48, 49, 50], YouTube 100 days of
over this real-world data, which has already proved hands [173], Something-Something [73], Ego4D [74],
useful in a grasping and manipulation context [13] andRealEstate10k[226]datasets. Thecurationofdata
andforleggedlocomotion[180]. for pre-training matters, with pre-training on unla-
beledcurateddatasetsoutperforminglabeleddatasets
5.FutureDirections ondownstreamtasks[212].Increasingmodelsizealso
promisesbenefits, withlargerResNetshowingbetter
In this section, we discuss promising future di- performance [203]. Prior work pre-trains ResNet-50
rections for embodied AI, including further leverag- [96, 128, 215], CLIP [96], or ViT models [212]. With
ing pre-trained models, world models and inverse the success of neural scaling laws [94] in vision and
graphics, simulation and dataset advances, sim2real language, future work in embodied AI may translate
approaches,proceduralgeneration,generalistagents, theselessonstopre-traininglargermodelswithlarger
andmulti-agentandhumaninteraction. datasets.
Pre-training also provides a way to specify di-
5.1.Pre-training
versetasksforagentseasily. Open-worldagentsmust
Pre-training has powered impressive results from be able to flexibly complete tasks with unseen goals
visual recognition [69], natural language [53, 148], or task specifications. Prior work shows that pre-
and audio [132]. Pre-trained models can be repur- trainedmodelscanprovidedenserewardsupervision
posed through fine-tuning, zero-shot generalization, [36,46,174]. Otherworkshowsthatpre-trainedmod-
or prompting to perform diverse tasks. However, els can be leveraged for open-world object detection,
pre-training has not yet found such levels of success allowing for zero-shot generalization to new goals in
in embodied AI. Recent work has begun to explore navigation tasks [5, 63, 112]. Finally, some methods
this direction, showing that pre-trained models can explore generalization to new language instructions
helpimproveperformance,efficiencyandexpandthe byemployingpre-trainedmodels[176]. Therearefur-
scope of solvable tasks. This section discusses how ther opportunities to use such models for zero-shot
pre-training can help embodied AI with visual pre- generalizationtocompletingnewtasks,newgoals,or
training objectives, the role of scale in pre-training, flexiblyspecifyinggoalsindifferentinputmodalities.
pre-trainingfortaskspecification,andpre-trainedbe- Finally,pre-trainingcanlearnbehavioralpriorsfor
havioralpriors. interaction. Thepreviouslydiscussedpre-trainingob-
One promising area is new pre-training objectives jectives primarily focus on learning representations
forvisualrepresentationsinembodiedAI.Priorwork of input modalities. However, this leaves out a crit-
19
ical part of embodied AI, interacting with the envi- cally,(3)manydetailsencodedinastateareirrelevant
ronment. Rather than pre-training representations, to task completion (e.g. minor color or texture varia-
pre-training can also learn models of behavior that tions of objects) and attempting to predict these de-
account for agent actions. One line of work pre- tailsneedlesslycomplicatestraining,and(4)collecting
trainsmodelswithsupervisedlearningtopredictac- high-qualitytrainingdatafortheend-to-endtraining
tionsfromsensorinputsonlargeinteractiondatasets of world models may require the design of increas-
andthenfine-tunethismodeltospecificdownstream inglycomplexphysicalstates(e.g.atowerofplatesto
tasks [14]. Other work learns skills or reusable be- beknockedover). Whilemoreworkisneededbefore
haviorsfromofflinedatasetsthatcanadapttodown- world models will become a ubiquitous tool for em-
streamtasks[77,141]. Futureworkmayexplorehow bodied AI agents, recent work has shown that world
scalingdatasetsize,modelsize,andcomputecanpre- modelscanbesuccessfullyusedtotrainingagentsto
train behavioral policies better suited for fine-tuning play Atari games [80] and to build navigation-only
ondownstreamtasks. modelsofembodiedenvironments[97].
As world models are meant to be broadly appli-
5.2.Worldmodelsandinversegraphics
cable and learned from data, they frequently eschew
inductive biases and use general purpose architec-
As previously discussed, semantic and free-space
tures. Thedisadvantageofthisapproachisclear: we
maps have been hugely successful in enabling high
have well-understood models of physics that should
performance and efficient learning across embodied-
not have to be re-learned from data for every task.
AI tasks (e.g. in navigation [31] and rearrange-
Moreover, we have simulators designed explicitly to
ment [193]). These mapping approaches are success-
simulate 3D objects and their physical interactions,
fulastheyprovideasimple,highly-structured,model
video game engines. These observations suggest an-
oftheagent’senvironmentthatenablesexplicitplan-
otherapproach:ratherthanlearninganimplicitworld
ning. Thesimplicityof existingmappingapproaches
model, can we use techniques from inverse-graphics
is also one of their major limitations: as embodied
to back-project an agent’s observations to 3D assets
tasks become more complex they require agents to
within a scene in a game engine? Once this back-
reasonaboutnewsemanticcategoriesandnewtypes
projection is complete, the game engine can be used
ofinteraction(e.g.arm-basedmanipulation). Extend-
to perform physical simulations and planning. This
ingexistingapproachestoincludenewcapabilitiesis
approach,whichcanbethoughtofasworldmodeling
generallypossiblebutnon-trivial,oftenrequiringsub-
withstronginductivebiases,hasusedsuccessfullyto
stantive human effort. For instance, a 2D free-space
build models of intuitive physics in constrained set-
mapping approach successful for PointGoal Naviga-
tings[208]. Whilethisapproachappearsverypromis-
tion [31] was explicitly extended to include semantic
ing it does present some challenges: (1) the problem
mapping channels so as to enable training agents for
of inverse graphics is especially challenging in this
ObjectGoalNavigation[30]. Thesechallengesinmap-
setting as de-rendered objects must be in physically
ping raise an important question: how can we build
plausible relationships with one another for simula-
flexiblemodelsofanagent’senvironmentthatcanbe
tiontobemeaningfuland(2)game-enginesare,gener-
used for general purpose task planning? We identify
ally,non-differentiableandcanbeslow. Nevertheless,
two exciting directions toward answering this ques-
this approach of explicitly bringing our understand-
tion: end-to-end trainable world models and game-
ingofphysicallawstoworldmodelsseemsapromis-
enginesimulationviainverse-graphics.
ing direction toward building embodied models that
At a high-level, a world models W is a function
canphysicallyreasonandplan.
that, given the state of the environment s at time t
t
andanagentactiona,producesapredictionW(s,a) =
5.3.SimulationandDatasetAdvances
(cid:98)s t+1 ofthestateoftheworldattimet+1iftheagent
were to take action a [79]. Iterative applications of One factor towards improving the reliability and
the world model can thus be used to simulate agent scope of embodied AI research in the future will be
trajectories and, thus, for model-based planning. As the continued improvement of simulation capabili-
may be expected, building and training world mod- tiesandrealism,andincreaseinthescaleandquality
elsmadechallengingbyseveralfactors: (1)generally of 3D assets used in simulation. Repeatable, quanti-
full state information (s ) is not available as agent’s tative analysis of embodied AI systems at scale has
t
have access only to partial, egocentric, observations, been made possible through the use of simulation.
(2) the dynamics of an environment are frequently As research in embodied AI continues to grow and
stochastic and thus cannot be predicted deterministi- tackleincreasinglycomplexproblemswithinincreas-
20
inglycomplexscenes,theneedsplacedonsimulation heavily utilized by embodied AI researchers in the
environmentsandassetswillincrease. pastyears[25,29,43,54,123,182,211]. Althoughin-
One important area of improvement for simula- creased scale and quality has been the general trend
tion environments is physics realism during agent- for these datasets, it still remains extremely costly to
object interaction. Past simulation environments makethemuseableforinteractivetasks. Forexample,
have solidly supported both abstracted [99, 144] and most of the objects in these datasets do not support
rigid-body physics-based agent interactions [56, 65, interaction,suchastheabilitytoopencabinets. Such
175, 186]. There has been quite some progress in worknotonlyrequiresmodifyingmeshes,butalsore-
physics simulation of flexible material (rope, cloth, quiresatremendousamountofannotationtoprovide
softbody)[106,170],fluids[60],andcontact-richinter- part-levelandarticulationannotation,aswasdonein
action(e.g.nut-and-bot)[129],leveragingstate-of-the- the PartNet and PartNet-Mobility datasets [123, 211].
art physics engines like PyBullet [44] and NVIDIA’s Similarly, it requires additional annotation and mesh
PhysX/FleX.SomeenvironmentlikeiGibson2.0[105] editingtosupportobjectstates(e.g.whethertheobject
even attempts to go beyond kinodynamic simula- iscookable,sliceable)fortheBEHAVIORdataset[182]
tion and use approximate models to simulate more or in AI2-THOR [99]. Yet, these annotations are es-
complexphysicalprocessessuchasthermodynamics. sentialaswerampupthecomplexityofembodiedAI
However,allofthesesimulationsarestillfarfromper- tasks.
fect and oftentimes face a grim trade-off between fi- Another important aspect of realistic simulation
delityandefficiency. Moreefficientandrealisticsim- is its multimodal nature, one of the most important
ulation of physical interaction of agents with all ele- onesisauditoryperception. Existingacousticsimula-
ments of their environment can greatly assist in the tion like SoundSpaces [38] allows the agent to move
applicability of embodied AI trained using simula- around in the environment with both visual and au-
tion,tosolvingreal-worldproblems. ditory sensing to search for a sounding object. How-
With the prevalence of vision sensors for solving ever,itpre-computestheroomimpulseresponse(RIR)
problems, the need for increased visual realism has basedonscenegeometryandcan’tbeconfigured. Re-
also become imperative for research that is to trans- centworklikeSoundSpaces2.0[40](Fig.9)extended
late to the real world. This has been aided in recent the simulation to make it continuous, configurable
years through aspects like new graphics technology and generalizable to arbitrary scene datasets, which
like real-time ray tracing. An example of how these enablestheagenttoexploretheacousticsofthespace
advances can improve visual realism can be found evenfurther.
within iterations of the RVSU challenge [82] that re- In addition, tactile sensing is also super impor-
centlymigratedtoNVIDIA’sIsaacOmniverse12. Yet, tanttofuturesimulationenvironments. Asthesesen-
the rendering speed can still become a bottleneck as sors become more cost-efficient, robots will likely be
thenumberofobjectsandlightsourcesincreaseinthe equipped with these new sensing capabilities in the
scenes. foreseeable future. Researchers have made tremen-
Aside from advances in computer graphics, vi- dous progress in tactile simulation [2, 130] in the
sual realism also relies on high-quality 3D assets of pastyears,whichcanunlocktremendouspotentialfor
scenes and objects. It has been a standard practice multi-modalembodiedAIresearch.
for embodied AI researchers to benchmark naviga-
tionagentsinlarge-scalestaticscenedatasetslikeMat- 5.4.Sim2RealApproaches
terport3D [28], Gibson [209], and HM3D [151]. On
AstheembodiedAIcommunitygrows,andbench-
the other hand, interactive scenes have been quite
marks in simulation continue to improve, a funda-
limited. iGibson 2.0 [105] provides fifteen fully in-
mental question that remains is: how well does this
teractive scenes with added clutter that aim to cap-
progresstranslatetotherealworld? Towardsanswer-
ture the messiness of the real world, and Habitat
ing this question, the embodied AI community has
2.0 [186] also similarly converts a subset of an ex-
madesignificanteffortsin1)buildinginfrastructureto
isting static dataset [185] to become fully interactive.
facilitate sim2real transfer on hardware, 2) providing
ProcTHOR[52]recentlyattemptedtoscaleuptheef-
support for researchers across the world to evaluate
fortandprocedurallygeneratefullyinteractivescenes
policiesinthereal-world,and3)developingsim2real
withrealisticroomstructuresandobjectlayout.
adaptationtechniques.
Many object datasets have been proposed and
Significant advances have been made in recent
12see https://developer.nvidia.com/blog/making-robotics- yearsonreal-worldhardwaretargets, withtheemer-
easier-with-benchbot-and-isaac-sim/fordetails gence of low-cost robots for evaluation [95, 126] and
21
open-sourceinfrastructureforsim2robotdeployment available used during training. The use of large-
[1, 51, 187]. These advances have lowered the bar- scale training data has resulted in models that are
rier to entry for robotics, and enable the embodied increasingly more powerful and generalizable across
AIcommunitytoevaluatetheperformanceofvarious AI [6, 42, 147, 153]. Yet, most works in embodied
research algorithms both in simulation and on real- AI often suffer from massive overfitting to the train-
world robots. Currently, each approach is limited to ing scene datasets. For instance, in the RoboTHOR
aspecificsimulatororalimitedsetofrobotplatforms. andHabitatObjectNavchallengesalone, itisnotun-
A key future direction is for these translation tech- commontoobtainnear100%successonthe100orso
nologies to become ubiquitous interfaces, with sup- scenes seen during training while only obtaining 30-
port for any simulator or physical robot platform re- 50%successwhenevaluatingonunseenscenes.
quiredbytheresearcher. Early work in embodied AI either trained agents
Bycomparingtheperformanceofpoliciesinsimu- onhand-designedscenescreatedby3Dartists[227]or
lationandthereal-world,researchersareabletoiden- fromstatic3Dscansofreal-worldenvironments[26].
tify flaws in the simulator design that lead to poor However, there are several drawbacks for both ap-
sim2real transfer [1], and develop novel methods to proaches. Manually creating 3D scenes is incredibly
overcomethesim2realgap. Commonapproachesfor time intensive work and requires graphics experts to
bridgingthesim2realgapincludedomainrandomiza- create3Dassets,possiblymaketheminteractive,and
tion [10, 191], or domain adaptation, a technique in arrange the objects to construct scenes. It took 3D
whichdatafromasourcedomainisadaptedtomore artists about 32 hours to develop each house of the
closely resemble data from a target domain. Prior ArchitecTHORdataset[52],andresultsinpresent-day
worksleveragedGANtechniquestoadaptthevisual simulators,withscenesdesignedby3Dartists,toonly
appearanceofobjectsfromsim-to-real[156],andother having on the order of 100 scenes available for train-
works built models [51, 194, 195], or learned latent ing[51,105,186].Static3Dscansdonotsupportinter-
embeddingsoftherobot’sdynamics[103,196,221]to actingormanipulatingobjectsandmaybeincredibly
adapt to the actuation noise found in the real world. timeconsumingtoannotatesemantically,capturethe
Models of real-world camera and actuation noises scenes, and clean up the meshes. To create HM3D-
have since been integrated into simulators, and in- Semantics [151], it took over 100 hours to semanti-
cludedaspartoftheHabitat,RoboTHOR,RVSUand cally annotate each of 120 scenes to make them use-
iGibsonChallenges,therebyimprovingtherealismof able for ObjectNav. Thus, it is incredibly difficult to
the challenge and decreasing the sim2real gap. Con- scale the creation of both hand-designed scenes and
tinuingthiscloseintegrationbetweenreal-worldeval- 3Dscannedscenes.
uation and improving simulators and benchmarks
will help accelerate the speed of progress in robotics
research.
A final future direction, is in addressing the dif-
ferences between simulated and real-world sensori-
motor interfaces. It is common currently for actua-
tion to be broken into discretised chunks, and simu-
Figure16.ExamplesofProcedurallygeneratedhousesfrom
latedsensorinputstreatedthesameasreal-worldin-
ProcTHOR[52].
puts. While simulators and datasets will continue to
advance, there will likely always be a difference be-
Procedurallygeneratingenvironmentsoffersanal-
tween emulated and real-world sensorimotor experi-
ternative approach towards scaling data in embod-
ences. Research approaches that leverage simulated
ied AI, which randomly samples scenes with respect
datatolearnpolicies,thenembracethelimitationsof
to a prior distribution. ProcTHOR [52] procedurally
thesepolicieswhentransferringtoreal-worldscenar-
generates 10K houses to train embodied agents, and
ios, have begun to emerge in recent years [155]. This
achieves remarkable generalization results on many
isastart,butapproacheslikethesewillneedtobeex-
downstream navigation and interaction tasks [51, 56,
pandeduponinthefuture.
151,200]. Atahigh-level,eachhouseisgeneratedby
sampling a floorplan (defining which rooms appear
5.5.ProceduralGeneration
in a house and where) and then sampling the place-
InembodiedAI,proceduralgenerationcanbeused ment of objects in rooms of the house. The objects
to create environments with respect to some priors. largely come from AI2-THOR’s asset database, mak-
The purpose is often to scale up the diversity of data ing them interactive. All the objects are modularly
22
placed in each scene, which also provides semantic Binary Mask Segmentation Language Image
What sport are
annotations of the scenes for free. Procedural gener- they playing?
ation also allows for sampling scenes with respect to
preferences. Forexample,ifIwanttotrainanObject- input sequence of discrete tokens
Language
Navagenttofindabasketballinahomeortooperate Bounding Boxes
A wooden shelf
filled with colorful
inaroomwithmanymirrors,itissignificantlyeasier ceramic dishes
to procedurally generate environments that fit such
criteria than it is to either build such environments
from scratch or find and scan such environments in Segmentation Depth Mask Binary Mask Image
therealworld. [15,62,189]havealsoshownimpres-
sive generalization results from procedurally gener-
ating more simplistic embodied environments. We
suspectthetrendofprocedurallygeneratingenviron- Key Points Surface Normals Bounding Boxes
mentstocontinuetogrowintheyearstocome.
5.6.GeneralistAgents output sequence of discrete tokens
Embodiedagentsthatcanlearnfrommanytypesof Figure 17. Unified-IO [109] is a generalist model in com-
inputs and produce many different types of outputs puter vision, which can be used to solve tasks with many
offer a promising approach to generalize to interact- differentinputandoutputmodalities.
ing with humans and being able to quickly adapt to
new tasks. Here, we may want our agents to be able
to learn from watching videos, viewing a geographic modaltasks. Whilecurrentlytheresultsindicatethat
map,readingatutorial,orlisteningtosomebodytalk, Gatodoesnotfullybenefitfromasharedframework,
and be able to communicate through navigation or in the future, with sufficient scaling and better prob-
manipulation actions, text, or voice. The promise of lem formulation, the authors hypothesize that it will
generalist agents in embodied AI is that they should achieve significant gains in generalization. In com-
benefit from knowledge transfer between tasks and puter vision, there has recently been a line of work
modalities, while being much easier to instruct and exploringunifiedmodels,includingUnified-IO[109],
adapttonewtasks. Perceiver-IO [88], and UViM [98]. These models are
AnemergingphenomenoningenerativeNLPmod- abletoperformawidevarietyoftasksinbothvision
els,suchasGPT-3[24]andPALM[42],isthattheycan and language out of the box, including performing
be used to solve arbitrary NLP tasks in a 0-shot set- wellwithimageimpainting,segmentation,andvisual
ting by prompting the models with language tokens question answering. Moreover, their results begin to
asinput,andhavingitgeneratelanguagetokensfrom showmodelsthattransferknowledgebetweentasks.
the same vocabulary as output. Such prompting can In the years to come, we suspect the rise of unified
beusedtoevaluatethemodelsonmanytasks,suchas models will continue advancing, growing to support
question answering, summarization, and mathemati- 0-shottaskcompletionformanymoretasksinembod-
calreasoning. However,intherealmofcomputervi- iedAI.
sion, the input and output modalities are incredibly In parallel to multi-task learning, recent works
different. Forexample,opticalflowmodelsmightin- have also experimented with re-purposing existing
put a video and output a flow mask of each frame; pre-trained models. In Socratic Models [224], GPT-
object detection models input an image and output 3 (a language model) [24], CLIP (a vision-language
asetofboundingboxeswiththeirassociatedclasses; model) [147], and CLIPort (a vision-language-action
andimagegenerationmodelsinputatextdescription, model)[176]areusedtogethertosolvetabletoppick-
andoutputanRGBimage. Thusitismuchharderto and-place tasks, with language as the common in-
build unified computer vision models. Building uni- terface to prompt the pre-trained models. Similarly,
fiedmodelstoachievearbitrarytasksinembodiedAI in SayCan [4], a language model is used to generate
issimilarlydifficultduetothemanyinputandoutput high-levelplansthatbecanexecutedwithpre-trained
modalitiesthatarepossible. action skills. Likewise, in Inner Monologue [87] and
Gato[158]isthefirstbigattemptatbuildingauni- ALFWorld [178], textual state descriptions as used as
fied model that works for embodied agents. It con- amediumforsequentialdecision-making.
sistsofasingletransformeragentthatwastrainedon Overall,whilegeneralistagentsarelessprevalentin
awidevarietyofvision,language,control,andmulti- EmbodiedAI,inthenearfuture,wemightseegreater
23
consolidationoftasksandagentarchitecturesfollow- from large-scale human-activity datasets [49, 74, 76,
ingsimilarprogressinvisionandNLP. 179] is an exciting prospect for modeling human be-
haviorsinsimulation. Totrainandtransferthesepoli-
5.7.Multi-Agent&HumanInteraction
cies to the real world, we must develop low-shot ap-
proachesandrealisticbenchmarkstolearnsociallyin-
Analogoustothesociallearninginhumans,itisde-
telligentagents.
sirablethatembodiedagentscanobserve,learnfrom,
andcollaboratewithotheragents(includinghumans)
intheirenvironment. Theadvancedandrealisticsim-
ulated environments being developed for Embodied Alice’s task: set up a dinner table Bob’s task: guess Alice’s goal and help her
WATCH stage: Bob watches Alice’s behaviors and infers her goal
AI research will serve as virtual worlds for agent-
Alice may want
agent and human-agent interaction. The two pillars to set up a
dinner table
for social, multi-agent, and human-in-the-loop em-
HELP stage: Bob works with Alice to achieve her goal
bodied agents are (1) accurately simulating a subset
ofagentandhumanbehaviorrelevanttoagivenem-
bodied task and (2) creating realistic benchmarks for
multi-agentandhuman-AIcollaboration.
Figure 19. Watch and Help encourages social intelligence
whereanagentlearnsinthepresenceofhuman-liketeach-
Goal object
Vieisw oOccblusdtreudcted ers(imagecredits:Puigetal.[146]).
StartingPosition
EndingPosition
Agent 1 view Several benchmarks have helped make progress
Goal object
Goaisl vOisbibselerved within the space of multi-agent and social learning
GoalObject
inembodiedAI.WithinAI2-THOR,collaborativetask
completion[91]andfurnituremoving[90]wereoneof
Agent 2 view the first benchmarks for multi-agent learning in em-
bodied AI, focussed on task that cannot be done by
Figure 18. Furniture Moving [90] is a collaborative multi-
asingleagent. Whileabstractedgridworlds[89]pro-
agenttaskforagentstomoveaheavyfurnitureitem.
videafastertraininggroundforsuchtasks,efficiently
going beyond 2-3 agents models with high visual fi-
Immersing humans in simulation creates an op-
delityischallenging. Emergentcommunication[137]
portunity for a new class of experiences and user
and emergent visual representations [201] show ex-
studies that involve human-virtual agent interaction,
amples of learning heterogeneous agents possessing
data collection of human demonstrations at scale
specializedskills. SocialNaviniGibsonpresentsearly
in controlled environments, and creation of events
stepstowardsrobotlearningformobilityaroundhu-
and visualizations that are impossible or irrepro-
mans and other moving objects within the environ-
ducible in real scenarios. Some examples towards
ment. WithinVirtualHome,thewatch-and-help[146]
this goal include VirtualHome [145] where programs
benchmarkwillenabledfew-shotlearningofpolicies
are collected and created to model human behav-
thatcaninteractwithahuman-likeagenttoreplicate
iors along with animated atomic actions such as
demonstrationsinanunseenenvironment.
walk/run, grab, switch-on/off, open/close, place,
look-at,sit/standup,touch.TEACh[133]collectsboth Overall, simulated environments offer a scalable
humaninstructions,demonstrations,andquestionan- platform for procedural training and testing of inter-
swers from human who interact with the simulator activepolicies,potentiallyaddressingsomeofthelim-
throughawebinterface[133],whileBEHAVIORuses iting challenges inherent to research on human inter-
virtual reality to collect high-fidelity human demon- action: scalingupwithsafetyandspeed,standardize
strations directly in the action space of a simulated environments to support reproducible research, and
robotagent[182]. Totrainpolicies,modelingthetask- procedural testing and benchmarking of a minimum
relevantaspectsofhumanbehaviorisofprimefocus. set of tests before deploying on real robots. Progress
In challenges such as SocialNav, human agents are on all these fronts requires the integration and con-
simulated following a simple interaction model that vergence of contributions from diverse fields such as
considers interactions between agents. Looking for- graphics, animation, and simulation, towards fully
ward,withrobustmotionsolutionsmodels[110,160] functional, realistic and interactive virtual environ-
andhumanbehavioranimation[139,207], emulating ments.
24
5.8.ImpactofEmbodiedAI Tommaso Campari co-wrote the section on Multi-
ObjectNavchallenge.
Whether in simulation or reality, embodied AI re-
search focuses on embodied tasks in the hope of de-
livering on the fundamental promise of AI: the cre- Devendra Singh Chaplot worked on Habitat Chal-
ationofembodiedagents,suchasrobots,whichlearn, lengesectionsandtheend-to-endvsmodularsubsec-
through interaction and exploration, to creatively tion.
solve challenging tasks within their environments.
Many embodied AI researchers believe that creating
ChanganChen workedontheaudio-visualnaviga-
intelligent agents that can solve embodied tasks will
tionsectionandthesimulationanddatasetadvances
produceoutsizedreal-worldimpacts. Increasinglyca-
section.
pableroboticplatformsandeffectivesim-to-realtech-
niquesmakeiteasiertotransferlearnedpoliciestothe
realworld.Evensmalladvancesatinterestingembod- Claudia Pérez-D’Arpino worked on the Introduc-
ied tasks could serve as the foundation for technolo- tion, Interactive and Social PointNav, and the Multi-
gies that could improve the lives of people with dis- Agent&HumanInteractionsections.
abilities or free able-bodied humans from mundane
tasks. However, these advances, as with all automa-
AnthonyFrancis workedontheIntroduction,What
tion, could result in disruptions such as the elimina-
IsEmbodiedAI,andSimtoRealsections,andedited
tion of jobs or disempowerment of individuals. We
othersections.
must be careful to ensure that the benefits of embod-
iedAIbecomeavailabletoallanddonotreinforcein-
equality. Therefore,theembodiedAIcommunityhas Chuang Gan worked on the rearrangement chal-
promoteddiscussionoftheseissuesinthehopethatit lengessection.
willguideustowardsmoreequitablesolutions.
DavidHall workedontheRVSUchallengesections
6.Conclusion and provided some editing on the simulation and
datasetadvancessection.
In this paper, we presented a retrospective on the
state of Embodied AI research. We discussed 13 dif-
ferent challenges that make up a testbed for a suite WinsonHan createdtheFigure1covergraphic.
of embodied navigation, interaction, and vision-and-
language tasks. Over the past 3 years, we observed
Unnat Jain worked on audio-visual navigation,
large-scaletraining, visualpre-training, modularand
multi-objectnavigation,andmulti-agentsections.
end-to-end training, and visual & dynamic augmen-
tation as common approaches to many of the top
challenge entries. We discuss improvements to pre- Jacob Krantz worked on the challenge section on
training,worldmodelsandinversegraphics,simula- NavigationInstructionFollowing.
tion and dataset advances, sim2real, procedural gen-
eration, generalist agents, and multi-agent & human
Chengshu Li worked on the Interactive and Social
interactionaspromisingfuturedirectionsinthefield.
PointNavsectionandtheSimulationandDatasetAd-
vancessectioninFutureDirections.
Contributions
Matt Deitke led the planning, outline, and coordi- SagnikMajumder workedontheaudio-visualnav-
nationofthepaper;workedontheabstract,introduc- igationsection.
tion,&conclusionandworkedontheObjectNavsec-
tion, the large-scale training section, the procedural
RobertoMartín-Martín workedontheWhatIsEm-
generation,andthegeneralistagentssection.
bodied AI section, and the Interactive and Social
PointNavsection.
YonatanBisk saidweshoulddothis,attendedafew
planningmeetings,butthendelegatedandMattreally SoniaRaychaudhuri co-wrotethesectiononMulti-
ranwithit. ObjectNavchallenge.
25
Mohit Shridhar worked on the interactive instruc- [2] Arpit Agarwal, Timothy Man, and Wenzhen
tion following section for challenge details and the Yuan. Simulationofvision-basedtactilesensors
generalistagentssectionforfuturedirections. usingphysicsbasedrendering. InICRA,pages
1–7.IEEE,2021. 21
Niko Sünderhauf worked on the RVSU challenge
[3] Kush Agrawal. To study the phenomenon
sections.
of the moravec’s paradox. arXiv preprint
arXiv:1012.3148,2010. 3
AndrewSzot workedonthepre-trainingsectionfor
[4] Michael Ahn, Anthony Brohan, Noah Brown,
futuredirections.
Yevgen Chebotar, Omar Cortes, Byron David,
ChelseaFinn,KeerthanaGopalakrishnan,Karol
BenTalbot workedontheRVSUchallengesections Hausman, Alex Herzog, et al. Do as i can, not
andSim2RealApproachesadvancessection. as i say: Grounding language in robotic affor-
dances. arXivpreprintarXiv:2204.01691,2022. 4,
23
Jesse Thomason worked on the interactive instruc-
tion following and interactive instruction following
[5] Ziad Al-Halah, Santhosh Kumar Ramakrish-
with dialog sections of the challenge details, and the
nan,andKristenGrauman. Zeroexperiencere-
multi-agent&humaninteractionsectionoffuturedi-
quired: Plug & play modular transfer learning
rections.
forsemanticvisualnavigation. InCVPR,pages
17031–17041,2022. 19
AlexanderToshev workedonSocialandInteractive
Navigationsection. [6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline
Luc, Antoine Miech, Iain Barr, Yana Hasson,
KarelLenc,ArthurMensch,KatieMillican,Mal-
JoanneTruong workedonthePointNavsectionfor
colm Reynolds, et al. Flamingo: a visual
challenge details, and the Sim2Real approaches sec-
language model for few-shot learning. arXiv
tionforfuturedirections.
preprintarXiv:2204.14198,2022. 22
[7] Humam Alwassel, Dhruv Mahajan, Bruno Ko-
Luca Weihs worked on the rearrangement section
rbar, Lorenzo Torresani, Bernard Ghanem, and
for challenge details, the visual pre-training section
Du Tran. Self-supervised learning by cross-
for common approaches, and the world models and
modalaudio-videoclustering.NeurIPS,2020.19
inversegraphicssectionforfuturedirections.
[8] Dong An, Zun Wang, Yangguang Li, Yi Wang,
Dhruv Batra, Angel X. Chang, Kiana Ehsani, Ali YicongHong,YanHuang,LiangWang,andJing
Farhadi, Li Fei-Fei, Kristen Grauman, Anirud- Shao. 1st place solutions for rxr-habitat vision-
dha Kembhavi, Stefan Lee, Oleksandr Maksymets, and-language navigation competition (cvpr
Roozbeh Mottaghi, Mike Roberts, Manolis Savva, 2022). arXivpreprintarXiv:2206.11610,2022. 15,
Silvio Savarese, Joshua B. Tenenbaum, Jiajun Wu 18
advised and provided feedback on the draft, work-
shop,and/orchallenges. [9] PeterAnderson,AngelChang,DevendraSingh
Chaplot, Alexey Dosovitskiy, Saurabh Gupta,
References Vladlen Koltun, Jana Kosecka, Jitendra Ma-
lik, Roozbeh Mottaghi, Manolis Savva, et al.
On evaluation of embodied navigation agents.
arXivpreprintarXiv:1807.06757,2018. 5,7,9,11,
[1] Abhishek Kadian*, Joanne Truong*, Aaron 15
Gokaslan, AlexanderClegg, ErikWijmans, Ste-
fan Lee, Manolis Savva, Sonia Chernova, and [10] Peter Anderson, Ayush Shrivastava, Joanne
DhruvBatra. Sim2RealPredictivity: DoesEval- Truong, Arjun Majumdar, Devi Parikh, Dhruv
uationinSimulationPredictReal-WorldPerfor- Batra, and Stefan Lee. Sim-to-real transfer for
mance? InRA-L,pages6670–6677,2020. 1,5,6, vision-and-languagenavigation. InCoRL,2020.
22 22
26
[11] Peter Anderson, Qi Wu, Damien Teney, Jake Joseph Turian. Experience Grounds Language.
Bruce, Mark Johnson, Niko Sünderhauf, Ian InEMNLP,2020. 14
Reid,StephenGould,andAntonvandenHen-
[21] ValtsBlukis,ChrisPaxton,DieterFox,Animesh
gel. Vision-and-languagenavigation: Interpret-
Garg, and Yoav Artzi. A persistent spatial se-
ing visually-grounded navigation instructions
inrealenvironments. InCVPR,2018. 15 manticrepresentationforhigh-levelnaturallan-
guage instruction execution. In CoRL, pages
[12] RonaldCArkin,RonaldCArkin,etal.Behavior- 706–717,2022. 16
basedrobotics. MITpress,1998. 3
[22] MargaretABoden. 4gofai. TheCambridgehand-
[13] Shikhar Bahl, Abhinav Gupta, and Deepak bookofartificialintelligence,page89,2014. 3
Pathak. Human-to-robot imitation in the wild.
RSS,2022. 19 [23] Rodney A Brooks. Elephants don’t play chess.
Robotics and autonomous systems, 6(1-2):3–15,
[14] BowenBaker,IlgeAkkaya,PeterZhokhov,Joost 1990. 3
Huizinga, Jie Tang, Adrien Ecoffet, Brandon
[24] Tom Brown, Benjamin Mann, Nick Ryder,
Houghton, Raul Sampedro, and Jeff Clune.
Melanie Subbiah, Jared D Kaplan, Prafulla
Video pretraining (vpt): Learning to act by
watching unlabeled online videos. arXiv Dhariwal,ArvindNeelakantan,PranavShyam,
preprintarXiv:2206.11795,2022. 20 Girish Sastry, Amanda Askell, et al. Language
modelsarefew-shotlearners. NeurIPS,33:1877–
[15] Bowen Baker, Ingmar Kanitscheider, Todor 1901,2020. 23
Markov, Yi Wu, Glenn Powell, Bob Mc-
[25] Berk Calli, Arjun Singh, James Bruce, Aaron
Grew, and Igor Mordatch. Emergent tool use
from multi-agent autocurricula. arXiv preprint Walsman,KurtKonolige,SiddharthaSrinivasa,
arXiv:1909.07528,2019. 23 Pieter Abbeel, and Aaron M Dollar. Yale-cmu-
berkeley dataset for robotic manipulation re-
[16] DhruvBatra,AngelX.Chang,SoniaChernova, search. IJRR,36(3):261–268,2017. 21
Andrew J. Davison, Jia Deng, Vladlen Koltun,
[26] AngelChang,AngelaDai,ThomasFunkhouser,
Sergey Levine, Jitendra Malik, Igor Mordatch,
Maciej Halber, Matthias Niebner, Manolis
RoozbehMottaghi,ManolisSavva,andHaoSu.
Savva, Shuran Song, Andy Zeng, and Yinda
Rearrangement: A challenge for embodied AI.
CoRR,abs/2011.01975,2020. 12,13 Zhang.Matterport3d:Learningfromrgb-ddata
inindoorenvironments.In3DV,pages667–676.
[17] DhruvBatra,AaronGokaslan,AniruddhaKem- IEEE,2017. 9,17,22
bhavi, Oleksandr Maksymets, Roozbeh Mot-
[27] AngelChang,AngelaDai,ThomasFunkhouser,
taghi, Manolis Savva, Alexander Toshev, and
Maciej Halber, Matthias Niessner, Manolis
Erik Wijmans. Objectnav revisited: On evalu-
Savva, Shuran Song, Andy Zeng, and Yinda
ationofembodiedagentsnavigatingtoobjects.
arXivpreprintarXiv:2006.13171,2020. 1,19 Zhang. Matterport3D: Learning from RGB-D
data in indoor environments. In 3DV, 2017. 7,
[18] Jur van den Berg, Stephen J Guy, Ming Lin, 11
and Dinesh Manocha. Reciprocal n-body colli-
sionavoidance. InRoboticsresearch,pages3–19. [28] AngelChang,AngelaDai,ThomasFunkhouser,
Maciej Halber, Matthias Niessner, Manolis
Springer,2011. 6
Savva, Shuran Song, Andy Zeng, and Yinda
[19] Roberto Bigazzi, Federico Landi, Silvia Cas- Zhang. Matterport3d: Learning from RGB-D
cianelli, Lorenzo Baraldi, Marcella Cornia, and data in indoor environments. 3DV, 2017. 14,
Rita Cucchiara. Focus on impact: indoor 21
exploration with intrinsic motivation. RA-L,
[29] AngelXChang,ThomasFunkhouser,Leonidas
7(2):2985–2992,2022. 18
Guibas,PatHanrahan,QixingHuang,ZimoLi,
[20] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Silvio Savarese, Manolis Savva, Shuran Song,
Jacob Andreas, Yoshua Bengio, Joyce Chai, Hao Su, et al. Shapenet: An information-
Mirella Lapata, Angeliki Lazaridou, Jonathan rich 3d model repository. arXiv preprint
May, Aleksandr Nisnevich, Nicolas Pinto, and arXiv:1512.03012,2015. 21
27
[30] Devendra Singh Chaplot, Dhiraj Gandhi, Ab- Kristen Grauman. Soundspaces 2.0: A sim-
hinav Gupta, and Ruslan Salakhutdinov. Ob- ulation platform for visual-acoustic learning.
jectgoalnavigationusinggoal-orientedseman- arXiv,2022. 11,21
ticexploration. InConferenceonNeuralInforma-
[41] FrancoisChollet. DeeplearningwithPython. Si-
tionProcessingSystems,2020. 18,20
monandSchuster,2021. 5
[31] Devendra Singh Chaplot, Dhiraj Gandhi,
[42] Aakanksha Chowdhery, Sharan Narang, Jacob
Saurabh Gupta, Abhinav Gupta, and Ruslan
Devlin,MaartenBosma,GauravMishra,Adam
Salakhutdinov. Learningtoexploreusingactive
Roberts, Paul Barham, Hyung Won Chung,
neuralslam. InICLR,2020. 5,18,20
Charles Sutton, Sebastian Gehrmann, et al.
[32] Devendra Singh Chaplot and Guillaume Lam- Palm: Scaling language modeling with path-
ple. Arnold: Anautonomousagenttoplayfps ways. arXivpreprintarXiv:2204.02311,2022. 22,
games. InThirty-FirstAAAIConferenceonArtifi- 23
cialIntelligence,2017. 18
[43] Jasmine Collins, Shubham Goel, Kenan Deng,
[33] DevendraSinghChaplot, EmilioParisotto, and Achleshwar Luthra, Leon Xu, Erhan Gun-
Ruslan Salakhutdinov. Active neural localiza- dogdu, Xi Zhang, Tomas F Yago Vicente,
tion. ICLR,2018. 18 Thomas Dideriksen, Himanshu Arora, et al.
Abo:Datasetandbenchmarksforreal-world3d
[34] Devendra Singh Chaplot, Ruslan Salakhutdi-
object understanding. In CVPR, pages 21126–
nov,AbhinavGupta,andSaurabhGupta. Neu-
21136,2022. 21
ral topological slam for visual navigation. In
CVPR,2020. 18 [44] Erwin Coumans and Yunfei Bai. Pybullet,
a python module for physics simulation for
[35] Devendra Singh Chaplot, Kanthashree Mysore
games, robotics and machine learning. http:
Sathyendra, Rama Kumar Pasumarthi,
//pybullet.org,2016–2021. 21
Dheeraj Rajagopal, and Ruslan Salakhutdi-
nov. Gated-attention architectures for task- [45] Tim Crane and Sarah Patterson. History of the
oriented language grounding. arXiv preprint mind-bodyproblem. Routledge,2012. 3
arXiv:1706.07230,2017. 18
[46] Yuchen Cui, Scott Niekum, Abhinav Gupta,
[36] Annie S Chen, Suraj Nair, and Chelsea Finn. Vikash Kumar, and Aravind Rajeswaran. Can
Learninggeneralizableroboticrewardfunctions foundationmodelsperformzero-shottaskspec-
from"in-the-wild"humanvideos. arXivpreprint ificationforrobotmanipulation? InLearningfor
arXiv:2103.16817,2021. 19 DynamicsandControlConference,pages893–905.
PMLR,2022. 19
[37] Changan Chen, Ziad Al-Halah, and Kristen
Grauman. Semanticaudio-visualnavigation. In [47] MLCummings. Thesurprisingbrittlenessofai.
CVPR,2021. 12 WomenCorporateDirectors,2020. 3
[38] Changan Chen, Unnat Jain, Carl Schissler, Se- [48] Dima Damen, Hazel Doughty, Giovanni Maria
bastia Vicenc Amengual Gari, Ziad Al-Halah, Farinella,SanjaFidler,AntoninoFurnari,Evan-
Vamsi Krishna Ithapu, Philip Robinson, and gelos Kazakos, Davide Moltisanti, Jonathan
Kristen Grauman. SoundSpaces: Audio-visual Munro, Toby Perrett, Will Price, et al. Scaling
navigationin3denvironments. InECCV,2020. egocentricvision: Theepic-kitchensdataset. In
1,4,10,19,21 ECCV,pages720–736,2018. 19
[39] Changan Chen, Sagnik Majumder, Ziad Al- [49] Dima Damen, Hazel Doughty, Giovanni Maria
Halah, Ruohan Gao, Santhosh Kumar Ramakr- Farinella, Antonino Furnari, Evangelos Kaza-
ishnan, and Kristen Grauman. Learning to set kos, Jian Ma, Davide Moltisanti, Jonathan
waypointsforaudio-visualnavigation.InICLR, Munro,TobyPerrett,WillPrice,etal. Rescaling
2021. 11 egocentricvision: collection, pipelineandchal-
lengesforepic-kitchens-100. IJCV,2022. 19,24
[40] Changan Chen, Carl Schissler, Sanchit Garg,
Philip Kobernik, Alexander Clegg, Paul [50] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian
Calamia, DhruvBatra, PhilipWRobinson, and Ma, Amlan Kar, Richard Higgins, Sanja Fidler,
28
DavidFouhey,andDimaDamen. Epic-kitchens [59] Anthony Francis, Aleksandra Faust, Hao-
visorbenchmark: Videosegmentationsandob- Tien Lewis Chiang, Jasmine Hsu, J Chase Kew,
ject relations. In NeurIPS Track on Datasets and MarekFiser,andTsang-WeiEdwardLee. Long-
Benchmarks,2022. 19 range indoor navigation with prm-rl. IEEE
Transactions on Robotics, 36(4):1115–1134, 2020.
[51] Matt Deitke, Winson Han, Alvaro Herrasti, 18
Aniruddha Kembhavi, Eric Kolve, Roozbeh
Mottaghi, Jordi Salvador, Dustin Schwenk, [60] Haoyuan Fu, Wenqiang Xu, Han Xue, Huinan
Eli VanderBilt, Matthew Wallingford, et al. Yang,RuolinYe,YongxiHuang,ZhendongXue,
Robothor: An open simulation-to-real embod- Yanfeng Wang, and Cewu Lu. Rfuniverse:
ied ai platform. In CVPR, pages 3164–3174, Aphysics-basedaction-centricinteractiveenvi-
2020. 1,7,17,19,22 ronment for everyday household tasks. arXiv
preprintarXiv:2202.00199,2022. 21
[52] Matt Deitke, Eli VanderBilt, Alvaro Herrasti,
LucaWeihs,JordiSalvador,KianaEhsani,Win- [61] Zipeng Fu, Ashish Kumar, Ananye Agarwal,
son Han, Eric Kolve, Ali Farhadi, Anirud- HaozhiQi,JitendraMalik,andDeepakPathak.
dha Kembhavi, and Roozbeh Mottaghi. Proc- Couplingvisionandproprioceptionfornaviga-
THOR: Large-Scale Embodied AI Using Proce- tion of legged robots. In CVPR, pages 17273–
duralGeneration. InNeurIPS,2022. 7,8,13,17, 17283,2022. 4,19
18,19,21,22
[62] Zipeng Fu, Ashish Kumar, Jitendra Malik, and
[53] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Deepak Pathak. Minimizing energy consump-
and Kristina Toutanova. Bert: Pre-training of tion leads to the emergence of gaits in legged
deep bidirectional transformers for language robots. CoRL,2021. 23
understanding. arXivpreprintarXiv:1810.04805,
2018. 19 [63] Samir Yitzhak Gadre, Mitchell Wortsman,
Gabriel Ilharco, Ludwig Schmidt, and Shuran
[54] Laura Downs, Anthony Francis, Nate Koenig, Song. Cliponwheels: Zero-shotobjectnaviga-
Brandon Kinman, Ryan Hickman, Krista Rey- tionasobjectlocalizationandexploration.arXiv
mann, Thomas B McHugh, and Vincent Van- preprintarXiv:2203.10421,2022. 19
houcke. Googlescannedobjects:Ahigh-quality
dataset of 3d scanned household items. arXiv [64] Chuang Gan, Yi Gu, Siyuan Zhou, Jeremy
preprintarXiv:2204.11918,2022. 6,21 Schwartz, Seth Alter, James Traer, Dan Gutfre-
und, Joshua B Tenenbaum, Josh H McDermott,
[55] YilunDu,ChuangGan,andPhillipIsola. Curi- and Antonio Torralba. Finding fallen objects
ousrepresentationlearningforembodiedintel- via asynchronous audio-visual integration. In
ligence. InICCV,pages10408–10417,2021. 19 CVPR,pages10523–10533,2022. 12
[56] KianaEhsani,WinsonHan,AlvaroHerrasti,Eli [65] ChuangGan,JeremySchwartz,SethAlter,Mar-
VanderBilt,LucaWeihs,EricKolve,Aniruddha tin Schrimpf, James Traer, Julian De Freitas,
Kembhavi, and Roozbeh Mottaghi. Manipu- Jonas Kubilius, Abhishek Bhandwaldar, Nick
lathor: A framework for visual object manipu- Haber, Megumi Sano, et al. Threedworld:
lation. InCVPR,pages4497–4506,2021. 21,22 A platform for interactive multi-modal physi-
cal simulation. arXiv preprint arXiv:2007.04954,
[57] Kuan Fang, Alexander Toshev, Li Fei-Fei, and 2020. 21
Silvio Savarese. Scene memory transformer
for embodied agents in long-horizon tasks. In [66] Chuang Gan, Yiwei Zhang, Jiajun Wu, Boqing
CVPR,2019. 19 Gong, and Joshua B Tenenbaum. Look, listen,
and act: Towards audio-visual embodied navi-
[58] Aleksandra Faust, Kenneth Oslund, Oscar gation.arXivpreprintarXiv:1912.11684,2019.10,
Ramirez, Anthony Francis, Lydia Tapia, Marek 19
Fiser, and James Davidson. Prm-rl: Long-
rangeroboticnavigationtasksbycombiningre- [67] Chuang Gan, Siyuan Zhou, Jeremy Schwartz,
inforcementlearningandsampling-basedplan- SethAlter,AbhishekBhandwaldar,DanGutfre-
ning. InICRA,pages5113–5120.IEEE,2018. 18 und, Daniel LK Yamins, James J DiCarlo, Josh
29
McDermott, AntonioTorralba, etal. Thethree- [77] Abhishek Gupta, Vikash Kumar, Corey Lynch,
dworld transport challenge: A visually guided SergeyLevine,andKarolHausman. Relaypol-
task-and-motion planning benchmark towards icylearning:Solvinglong-horizontasksviaimi-
physicallyrealisticembodiedai. InICRA,pages tationandreinforcementlearning.arXivpreprint
8847–8854,2022. 2,13 arXiv:1910.11956,2019. 20
[68] Andreas Geiger, Philip Lenz, Christoph Stiller, [78] Saurabh Gupta, James Davidson, Sergey
andRaquelUrtasun. Visionmeetsrobotics: The Levine, Rahul Sukthankar, and Jitendra Malik.
kittidataset. IJRR,2013. 10 Cognitive mapping and planning for visual
navigation. InCVPR,pages2616–2625,2017. 18
[69] RossGirshick,JeffDonahue,TrevorDarrell,and
Jitendra Malik. Rich feature hierarchies for ac- [79] David Ha and Jürgen Schmidhuber. World
curateobjectdetectionandsemanticsegmenta- models. CoRR,abs/1803.10122,2018. 20
tion. InCVPR,pages580–587,2014. 19
[80] Danijar Hafner, Timothy P. Lillicrap, Moham-
mad Norouzi, and Jimmy Ba. Mastering atari
[70] KenGoldberg.Robotics:Counteringsingularity
sensationalism.Nature,526(7573):320–321,2015. with discrete world models. In ICLR. OpenRe-
view.net,2021. 20
3
[81] Meera Hahn, Devendra Chaplot, Mustafa
[71] Ian Goodfellow, Yoshua Bengio, and Aaron
Mukadam, James M. Rehg, Shubham Tulsiani,
Courville. Deeplearning. MITpress,2016. 5
and Abhinav Gupta. No rl, no simulation:
[72] DanielGordon,AniruddhaKembhavi,Moham- Learning to navigate without navigating. In
madRastegari,JosephRedmon,DieterFox,and NeurIPS,2021. 18
Ali Farhadi. Iqa: Visual question answering in
[82] David Hall, Ben Talbot, Suman Raj Bista,
interactiveenvironments.InCVPR,pages4089–
Haoyang Zhang, Rohan Smith, Feras Dayoub,
4098,2018. 18
and Niko Sünderhauf. The robotic vision
scene understanding challenge. arXiv preprint
[73] Raghav Goyal, Samira Ebrahimi Kahou, Vin-
arXiv:2009.05246,2020. 1,2,10,12,19,21
cent Michalski, Joanna Materzynska, Susanne
Westphal, Heuna Kim, Valentin Haenel, Ingo
[83] David Hall, Ben Talbot, Suman Raj Bista,
Fruend, Peter Yianilos, Moritz Mueller-Freitag,
Haoyang Zhang, Rohan Smith, Feras Dayoub,
et al. The" something something" video
and Niko Sünderhauf. Benchbot environments
database for learning and evaluating visual
for active robotics (bear): Simulated data for
commonsense.InICCV,pages5842–5850,2017.
active scene understanding research. IJRR,
19
41(3):259–269,2022. 10,12
[74] Kristen Grauman, Andrew Westbury, Eugene
[84] Stevan Harnad. The symbol grounding prob-
Byrne,ZacharyChavis,AntoninoFurnari,Rohit
lem. Physica D: Nonlinear Phenomena, 42(1-
Girdhar, Jackson Hamburger, Hao Jiang, Miao
3):335–346,1990. 3
Liu,XingyuLiu,etal.Ego4d:Aroundtheworld
in 3,000 hours of egocentric video. In CVPR, [85] JoaoFHenriquesandAndreaVedaldi. Mapnet:
2022. 19,24 Anallocentricspatialmemoryformappingen-
vironments. In CVPR, pages 8476–8484, 2018.
[75] Karol Gregor, George Papamakarios, Frederic
18
Besse, Lars Buesing, and Theophane Weber.
Temporal difference variational auto-encoder. [86] KarlMoritzHermann, FelixHill, SimonGreen,
arXivpreprintarXiv:1806.03107,2018. 19 Fumin Wang, Ryan Faulkner, Hubert Soyer,
David Szepesvari, Wojtek Czarnecki, Max
[76] Chunhui Gu, Chen Sun, David A Ross, Carl Jaderberg, Denis Teplyashin, et al. Grounded
Vondrick, Caroline Pantofaru, Yeqing Li, Sud- language learning in a simulated 3d world.
heendra Vijayanarasimhan, George Toderici, arXivpreprintarXiv:1706.06551,2017. 18
Susanna Ricco, Rahul Sukthankar, et al. Ava:
A video dataset of spatio-temporally localized [87] Wenlong Huang, Fei Xia, Ted Xiao, Har-
atomicvisualactions. InCVPR,2018. 24 ris Chan, Jacky Liang, Pete Florence, Andy
30
Zeng, Jonathan Tompson, Igor Mordatch, Yev- stretch:Acompact,lightweightmobilemanipu-
gen Chebotar, Pierre Sermanet, Noah Brown, latorforindoorhumanenvironments. InICRA,
Tomas Jackson, Linda Luu, Sergey Levine, pages3150–3157.IEEE,2022. 21
Karol Hausman, and Brian Ichter. Inner
[96] Apoorv Khandelwal, Luca Weihs, Roozbeh
monologue:Embodiedreasoningthroughplan-
Mottaghi, and Aniruddha Kembhavi. Simple
ning with language models. In arXiv preprint
but effective: CLIP embeddings for embodied
arXiv:2207.05608,2022. 23
AI. CoRR,abs/2111.09888,2021. 13,17,19
[88] Andrew Jaegle, Sebastian Borgeaud, Jean-
[97] Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason
BaptisteAlayrac,CarlDoersch,CatalinIonescu,
Baldridge, and Peter Anderson. Pathdreamer:
David Ding, Skanda Koppula, Daniel Zoran,
Aworldmodelforindoornavigation. InICCV,
AndrewBrock,EvanShelhamer,etal. Perceiver
pages14718–14728.ICCV,2021. 20
io: A general architecture for structured inputs
&outputs.arXivpreprintarXiv:2107.14795,2021.
[98] AlexanderKolesnikov,AndréSusanoPinto,Lu-
23
cas Beyer, Xiaohua Zhai, Jeremiah Harmsen,
and Neil Houlsby. Uvim: A unified modeling
[89] Unnat Jain, Iou-Jen Liu, Svetlana Lazebnik,
approachforvisionwithlearnedguidingcodes.
AniruddhaKembhavi,LucaWeihs,andAlexan-
arXivpreprintarXiv:2205.10337,2022. 23
der G Schwing. Gridtopix: Training embod-
ied agents with minimal supervision. In ICCV, [99] EricKolve,RoozbehMottaghi,WinsonHan,Eli
2021. 24 VanderBilt, Luca Weihs, Alvaro Herrasti, Matt
Deitke, Kiana Ehsani, Daniel Gordon, Yuke
[90] UnnatJain,LucaWeihs,EricKolve,AliFarhadi,
Zhu, Aniruddha Kembhavi, Abhinav Gupta,
Svetlana Lazebnik, Aniruddha Kembhavi, and
andAliFarhadi. AI2-THOR:AnInteractive3D
Alexander Schwing. A cordial sync: Going be-
EnvironmentforVisualAI. arXiv,2022. 7,21
yond marginal policies for multi-agent embod-
iedtasks. InECCV,2020. 24 [100] Jacob Krantz, Aaron Gokaslan, Dhruv Batra,
Stefan Lee, and Oleksandr Maksymets. Way-
[91] Unnat Jain, Luca Weihs, Eric Kolve, Moham-
pointmodelsforinstruction-guidednavigation
mad Rastegari, Svetlana Lazebnik, Ali Farhadi,
incontinuousenvironments. InICCV,2021. 18
Alexander G Schwing, and Aniruddha Kemb-
havi. Two body problem: Collaborative visual [101] Jacob Krantz, Erik Wijmans, Arjun Majundar,
taskcompletion. InCVPR,2019. 24 Dhruv Batra, and Stefan Lee. Beyond the nav-
graph: Vision and language navigation in con-
[92] Abhishek Kadian, Joanne Truong, Aaron
tinuousenvironments. InECCV,2020. 15
Gokaslan, Alexander Clegg, Erik Wijmans,
Stefan Lee, Manolis Savva, Sonia Chernova, [102] Alex Ku*, Peter Anderson*, Roma Patel, Eu-
and Dhruv Batra. Sim2real predictivity: Does gene Ie, and Jason Baldridge. Room-Across-
evaluation in simulation predict real-world Room: Multilingual vision-and-language navi-
performance? RA-L,2020. 5 gationwithdensespatiotemporalgrounding.In
EMNLP,2020. 2,15
[93] Yash Kant, Arun Ramachandran, Sriram Yena-
mandra, Igor Gilitschenski, Dhruv Batra, An- [103] AshishKumar,ZipengFu,DeepakPathak,and
drew Szot, and Harsh Agrawal. Housekeep: Jitendra Malik. Rma: Rapid motor adaptation
Tidyingvirtualhouseholdsusingcommonsense forleggedrobots. RSS,2021. 19,22
reasoning. arXivpreprintarXiv:2205.10712,2022.
[104] Guillaume Lample and Devendra Singh Chap-
18
lot.PlayingFPSgameswithdeepreinforcement
[94] JaredKaplan,SamMcCandlish,TomHenighan, learning. InThirty-FirstAAAIConferenceonAr-
Tom B Brown, Benjamin Chess, Rewon Child, tificialIntelligence,2017. 18
ScottGray,AlecRadford,JeffreyWu,andDario
[105] Chengshu Li, Fei Xia, Roberto Martín-Martín,
Amodei.Scalinglawsforneurallanguagemod-
Michael Lingelbach, Sanjana Srivastava, Bokui
els. arXivpreprintarXiv:2001.08361,2020. 19
Shen, Kent Vainio, Cem Gokmen, Gokul
[95] Charles C Kemp, Aaron Edsinger, Henry M Dharan, Tanish Jain, et al. igibson 2.0:
Clever, and Blaine Matulevich. The design of Object-centric simulation for robot learning
31
of everyday household tasks. arXiv preprint imagenetpre-trainingonindoorsegmentation?
arXiv:2108.03272,2021. 6,21,22 ICCV,2017. 10
[106] XingyuLin,YufeiWang,JakeOlkin,andDavid [116] Drew McDermott. Gofai considered harmful
Held. Softgym: Benchmarking deep reinforce- (andmythical),2015. 3
ment learning for deformable object manipula-
[117] Neeta Mehta. Mind-body dualism: A critique
tion. arXivpreprintarXiv:2011.07215,2020. 21
from a health perspective. Mens sana mono-
[107] Xiaotian Liu, Hector Palacios, and Christian graphs,9(1):202,2011. 3
Muise. A planning based neural-symbolic ap-
proachforembodiedinstructionfollowing. In- [118] Lina Mezghani, Sainbayar Sukhbaatar, Thibaut
teractions,9:8,2022. 18 Lavril,OleksandrMaksymets,DhruvBatra,Pi-
otrBojanowski,andKarteekAlahari. Memory-
[108] AndrewJLohn. Estimatingthebrittlenessofai: augmented reinforcement learning for image-
Safety integrity levels and the need for testing goalnavigation. InIROS,2022. 18
out-of-distribution performance. arXiv preprint
arXiv:2009.00802,2020. 3 [119] SoYeonMin,DevendraSinghChaplot,Pradeep
Ravikumar,YonatanBisk,andRuslanSalakhut-
[109] Jiasen Lu, Christopher Clark, Rowan Zellers, dinov. Film: Followinginstructionsinlanguage
Roozbeh Mottaghi, and Aniruddha Kembhavi. withmodularmethods. InICLR,2022. 16,18
Unified-io: A unified model for vision, lan-
guage, and multi-modal tasks. arXiv preprint [120] Piotr Mirowski, Razvan Pascanu, Fabio Viola,
arXiv:2206.08916,2022. 23 Hubert Soyer, Andrew J Ballard, Andrea Ban-
ino, MishaDenil, RossGoroshin, LaurentSifre,
[110] Camillo Lugaresi, Jiuqiang Tang, Hadon KorayKavukcuoglu,etal. Learningtonavigate
Nash, Chris McClanahan, Esha Uboweja, incomplexenvironments. ICLR,2017. 18
Michael Hays, Fan Zhang, Chuo-Ling Chang,
Ming Guang Yong, Juhyun Lee, et al. Medi- [121] Himangi Mittal, Pedro Morgado, Unnat Jain,
apipe: A framework for building perception and Abhinav Gupta. Learning state-aware vi-
pipelines. arXivpreprintarXiv:1906.08172,2019. sual representations from audible interactions.
24 arXivpreprintarXiv:2209.13583,2022. 19
[111] Gabriel Magalhaes, Vihan Jain, Alexander Ku, [122] Volodymyr Mnih, Koray Kavukcuoglu, David
EugeneIe,andJasonBaldridge. Generalevalu- Silver, Andrei A. Rusu, Joel Veness, Marc G.
ationforinstructionconditionednavigationus- Bellemare,AlexGraves,MartinRiedmiller,An-
ingdynamictimewarping. InNeurIPSVisually dreas K. Fidjeland, Georg Ostrovski, Stig Pe-
GroundedInteractionandLanguage(ViGIL)Work- tersen, Charles Beattie, Amir Sadik, Ioannis
shop,2019. 15 Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hass-
[112] Arjun Majumdar, Gunjan Aggarwal, Bhavika abis. Human-level control through deep rein-
Devnani, Judy Hoffman, and Dhruv Batra. forcement learning. Nature, 518(7540):529–533,
Zson: Zero-shot object-goal navigation using 022015. 17
multimodal goal embeddings. arXiv preprint
arXiv:2206.12403,2022. 19 [123] KaichunMo,ShilinZhu,AngelXChang,LiYi,
Subarna Tripathi, Leonidas J Guibas, and Hao
[113] Pierre Marza, Laetitia Matignon, Olivier Si- Su. Partnet: A large-scale benchmark for fine-
monin, and Christian Wolf. Teaching agents grainedandhierarchicalpart-level3dobjectun-
how to map: Spatial reasoning for multi-object derstanding. InCVPR,pages909–918,2019. 21
navigation. arXiv preprint arXiv:2107.06011,
2021. 9 [124] HansMoravec. Ripplesandpuddles,2000. 3
[114] John McCarthy. From here to human-level ai. [125] Pedro Morgado, Nuno Vasconcelos, and Ishan
ArtificialIntelligence,171(18):1174–1182,2007. 3 Misra. Audio-visual instance discrimination
withcross-modalagreement. CVPR,2021. 19
[115] John McCormac, Ankur Handa, Stefan
Leutenegger, and Andrew J.Davison. Scenenet [126] AdithyavairavanMurali,TaoChen,KalyanVa-
rgb-d: Can 5m synthetic images beat generic sudev Alwala, Dhiraj Gandhi, Lerrel Pinto,
32
Saurabh Gupta, and Abhinav Gupta. Py- [136] Alexander Pashevich, Cordelia Schmid, and
robot: An open-source robotics framework for ChenSun.EpisodicTransformerforVision-and-
research and benchmarking. arXiv preprint LanguageNavigation. InICCV,2021. 17
arXiv:1906.08236,2019. 21
[137] ShivanshPatel,SaimWani,UnnatJain,Alexan-
[127] Michael Murray and Maya Cakmak. Follow- der G. Schwing, Svetlana Lazebnik, Manolis
ingnaturallanguageinstructionsforhousehold Savva, and Angel X. Chang. Interpretation of
tasks with landmark guided search and rein- emergentcommunicationinheterogeneouscol-
forcedposeadjustment. RA-L,2022. 18 laborativeembodiedagents. InICCV,2021. 24
[128] SurajNair,AravindRajeswaran,VikashKumar, [138] Annie Murphy Paul. The extended mind: The
ChelseaFinn,andAbhinavGupta. R3m: Auni- powerofthinkingoutsidethebrain. EamonDolan
versalvisualrepresentationforrobotmanipula- Books,2021. 3
tion. arXivpreprintarXiv:2203.12601,2022. 19
[139] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey
[129] Yashraj Narang, Kier Storey, Iretiayo Aki- Levine, and Angjoo Kanazawa. Amp: Adver-
nola, Miles Macklin, Philipp Reist, Lukasz sarial motion priors for stylized physics-based
Wawrzyniak, Yunrong Guo, Adam Moravan- charactercontrol. ACMTrans.Graph.,2021. 24
szky, Gavriel State, Michelle Lu, et al. Factory:
Fastcontactforroboticassembly. arXivpreprint [140] Claudia Pérez-D’Arpino, Can Liu, Patrick
arXiv:2205.03532,2022. 21 Goebel, Roberto Martín-Martín, and Silvio
Savarese. Robot navigation in constrained
[130] Yashraj Narang, Balakumar Sundaralingam, pedestrian environments using reinforcement
Miles Macklin, Arsalan Mousavian, and Di- learning. InICRA,pages1140–1146.IEEE,2021.
eter Fox. Sim-to-real for robotic tactile sensing 6
viaphysics-basedsimulationandlearnedlatent
projections. In ICRA, pages 6444–6451. IEEE, [141] Karl Pertsch, Youngwoon Lee, and Joseph J
2021. 21 Lim. Accelerating reinforcement learning
with learned skill priors. arXiv preprint
[131] Khanh Nguyen, Yonatan Bisk, and Hal Daume arXiv:2010.11944,2020. 20
III. AFrameworkforLearningtoRequestRich
andContextuallyUsefulInformationfromHu- [142] Rolf Pfeifer and Fumiya Iida. Embodied artifi-
mans. InICML,2022. 16 cialintelligence: Trendsandchallenges. Embod-
iedartificialintelligence,pages1–26,2004. 2,4
[132] Aaron van den Oord, Sander Dieleman, Heiga
Zen, Karen Simonyan, Oriol Vinyals, Alex [143] GualtieroPiccinini. Thefirstcomputationalthe-
Graves, NalKalchbrenner, AndrewSenior, and oryofmindandbrain:acloselookatmcculloch
Koray Kavukcuoglu. Wavenet: A genera- and pitts’s “logical calculus of ideas immanent
tive model for raw audio. arXiv preprint in nervous activity”. Synthese, 141(2):175–215,
arXiv:1609.03499,2016. 19 2004. 3
[133] Aishwarya Padmakumar, Jesse Thomason, [144] XavierPuig,KevinRa,MarkoBoben,JiamanLi,
Ayush Shrivastava, Patrick Lange, Anjali Tingwu Wang, Sanja Fidler, and Antonio Tor-
Narayan-Chen, Spandana Gella, Robinson ralba. Virtualhome: Simulating household ac-
Piramithu,GokhanTur,andDilekHakkani-Tur. tivities via programs. In CVPR, pages 8494–
TEACh: Task-driven embodied agents that 8502,2018. 21
chat. arXiv,2021. 2,24
[145] XavierPuig,KevinRa,MarkoBoben,JiamanLi,
[134] Emilio Parisotto and Ruslan Salakhutdinov. Tingwu Wang, Sanja Fidler, and Antonio Tor-
Neuralmap: Structuredmemoryfordeeprein- ralba. Virtualhome: Simulating household ac-
forcementlearning. ICLR,2018. 18 tivitiesviaprograms. InCVPR,2018. 24
[135] Ruslan Partsey, Erik Wijmans, Naoki [146] Xavier Puig, Tianmin Shu, Shuang Li, Zilin
Yokoyama, Oles Dobosevych, Dhruv Batra, Wang, Yuan-Hong Liao, Joshua B. Tenenbaum,
and Oleksandr Maksymets. Is mapping nec- Sanja Fidler, and Antonio Torralba. Watch-
essary for realistic pointgoal navigation? In and-help: Achallengeforsocialperceptionand
CVPR,pages17232–17241,2022. 2,4 human-{ai}collaboration. InICLR,2021. 24
33
[147] Alec Radford, Jong Wook Kim, Chris Hallacy, Sünderhauf. Zero-shot uncertainty-aware de-
Aditya Ramesh, Gabriel Goh, Sandhini Agar- ploymentofsimulationtrainedpoliciesonreal-
wal, Girish Sastry, Amanda Askell, Pamela world robots. arXiv preprint arXiv:2112.05299,
Mishkin,JackClark,etal. Learningtransferable 2021. 22
visual models from natural language supervi-
[156] KanishkaRao,ChrisHarris,AlexIrpan,Sergey
sion. In ICML, pages 8748–8763. PMLR, 2021.
Levine, Julian Ibarz, and Mohi Khansari.
22,23
Rl-cyclegan: Reinforcement learning aware
[148] Alec Radford, Jeffrey Wu, Rewon Child, David simulation-to-real. In CVPR, pages 11157–
Luan, Dario Amodei, Ilya Sutskever, et al. 11166,2020. 19,22
Language models are unsupervised multitask
[157] Sonia Raychaudhuri, Saim Wani, Shivansh Pa-
learners. OpenAIblog,1(8):9,2019. 19
tel, Unnat Jain, and Angel Chang. Language-
[149] SanthoshK.Ramakrishnan,ZiadAl-Halah,and aligned waypoint (law) supervision for vision-
Kristen Grauman. Occupancy anticipation for and-language navigation in continuous envi-
efficient exploration and navigation. In ECCV, ronments. InEMNLP,2021. 18
2020. 6,18
[158] ScottReed,KonradZolna,EmilioParisotto,Ser-
[150] Santhosh K. Ramakrishnan, Devendra Singh gio Gomez Colmenarejo, Alexander Novikov,
Chaplot, Ziad Al-Halah, Jitendra Malik, and Gabriel Barth-Maron, Mai Gimenez, Yury Sul-
Kristen Grauman. Poni: Potential functions sky, Jackie Kay, Jost Tobias Springenberg,
for objectgoal navigation with interaction-free et al. A generalist agent. arXiv preprint
learning. InCVPR.IEEE,2022. 18 arXiv:2205.06175,2022. 23
[151] Santhosh K Ramakrishnan, Aaron Gokaslan, [159] Homero Roman Roman, Yonatan Bisk, Jesse
Erik Wijmans, Oleksandr Maksymets, Alex Thomason,AsliCelikyilmaz,andJianfengGao.
Clegg,JohnTurner,EricUndersander,Wojciech RMM: A Recursive Mental Model for Dialog
Galuba, Andrew Westbury, Angel X Chang, Navigation. InEMNLP,2020. 16
et al. Habitat-matterport 3d dataset (hm3d):
[160] Yu Rong, Takaaki Shiratori, and Hanbyul Joo.
1000large-scale3denvironmentsforembodied
Frankmocap:Amonocular3dwhole-bodypose
ai. arXiv preprint arXiv:2109.08238, 2021. 7, 17,
estimation system via regression and integra-
21,22
tion. InICCVWorkshops,2021. 24
[152] Santhosh K. Ramakrishnan, Aaron Gokaslan,
[161] Gilbert Ryle. The concept of mind. Routledge,
Erik Wijmans, Oleksandr Maksymets, Alexan-
2009. 3
der Clegg, John Turner, Eric Undersander, Wo-
jciech Galuba, Andrew Westbury, Angel X. [162] Gabriel Sarch, Zhaoyuan Fang, Adam W.
Chang, Manolis Savva, Yili Zhao, and Dhruv Harley, Paul Schydlo, Michael J. Tarr, Saurabh
Batra. Habitat-matterport 3d dataset (HM3D): Gupta, and Katerina Fragkiadaki. Tidee: Tidy-
1000large-scale3denvironmentsforembodied ing up novel rooms using visuo-semantic com-
AI. CoRR,abs/2109.08238,2021. 8,13 monsensepriors. InECCV,2022. 18
[153] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, [163] Nikolay Savinov, Alexey Dosovitskiy, and
Scott Gray, Chelsea Voss, Alec Radford, Mark Vladlen Koltun. Semi-parametric topological
Chen, and Ilya Sutskever. Zero-shot text-to- memoryfornavigation. InICLR,2018. 18
image generation. In ICML, pages 8821–8831.
[164] Nikolay Savinov, Anton Raichuk, Raphaël
PMLR,2021. 22
Marinier,DamienVincent,MarcPollefeys,Tim-
[154] RamRamrakhya, EricUndersander, DhruvBa- othy Lillicrap, and Sylvain Gelly. Episodic cu-
tra, and Abhishek Das. Habitat-web: Learning riositythroughreachability. InICLR,2019. 18
embodied object-search strategies from human
[165] Manolis Savva, Angel X. Chang, Alexey Doso-
demonstrationsatscale. InCVPR,pages5173–
vitskiy, Thomas Funkhouser, and Vladlen
5183,2022. 7,17
Koltun. MINOS: Multimodal indoor simula-
[155] Krishan Rana, Vibhavari Dasagi, Jesse Havi- tor for navigation in complex environments.
land, Ben Talbot, MIchael Milford, and Niko arXiv:1712.03931,2017. 18
34
[166] Manolis Savva, Abhishek Kadian, Oleksandr Srivastava, Lyne P Tchapmi, et al. igibson
Maksymets, Yili Zhao, Erik Wijmans, Bhavana 1.0, a simulation environment for interactive
Jain, Julian Straub, Jia Liu, Vladlen Koltun, Ji- tasks in large realistic scenes. IROS 2021,
tendraMalik,etal. Habitat: Aplatformforem- arXiv:2012.02924,2021. 4,5,6,21
bodied ai research. In ICCV, pages 9339–9347,
[176] MohitShridhar,LucasManuelli,andDieterFox.
2019. 7,14
Cliport: What and where pathways for robotic
[167] Manolis Savva, Abhishek Kadian, Oleksandr manipulation. In CoRL, pages 894–906. PMLR,
Maksymets, Yili Zhao, Erik Wijmans, Bhavana 2022. 4,19,23
Jain, Julian Straub, Jia Liu, Vladlen Koltun, Ji-
tendra Malik, Devi Parikh, and Dhruv Batra. [177] Mohit Shridhar, Jesse Thomason, Daniel Gor-
Habitat: APlatformforEmbodiedAIResearch. don,YonatanBisk,WinsonHan,RoozbehMot-
InICCV,2019. 17 taghi, Luke Zettlemoyer, and Dieter Fox. AL-
FRED:ABenchmarkforInterpretingGrounded
[168] Alexander Sax, Bradley Emi, Amir R Zamir, InstructionsforEverydayTasks. InCVPR,2020.
Leonidas Guibas, Silvio Savarese, and Jitendra 2,4
Malik. Mid-level visual representations im-
prove generalization and sample efficiency for [178] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre
learning visuomotor policies. arXiv preprint Côté, Yonatan Bisk, Adam Trischler, and
arXiv:1812.11971,2018. 19 Matthew Hausknecht. ALFWorld: Aligning
Text and Embodied Environments for Interac-
[169] Melanie Sclar, Graham Neubig, and Yonatan
tiveLearning. InICLR,2021. 23
Bisk. Symmetric Machine Theory of Mind. In
ICML,2022. 3 [179] LucasSmaira,JoãoCarreira,EricNoland,Ellen
Clancy, Amy Wu, and Andrew Zisserman. A
[170] Daniel Seita, Pete Florence, Jonathan Tomp-
short note on the kinetics-700-2020 human ac-
son, Erwin Coumans, Vikas Sindhwani, Ken
tion dataset. arXiv preprint arXiv:2010.10864,
Goldberg, and Andy Zeng. Learning to Re-
2020. 24
arrange Deformable Cables, Fabrics, and Bags
with Goal-Conditioned Transporter Networks. [180] LauraSmith,IlyaKostrikov,andSergeyLevine.
InICRA,2021. 21 A walk in the park: Learning to walk in 20
minutes with model-free reinforcement learn-
[171] Pierre Sermanet, Corey Lynch, Yevgen Cheb-
ing. arXivpreprintarXiv:2208.07860,2022. 19
otar, Jasmine Hsu, Eric Jang, Stefan Schaal,
Sergey Levine, and Google Brain. Time-
[181] ShuranSong,SamuelPLichtenberg,andJianx-
contrastive networks: Self-supervised learning
iong Xiao. Sun rgb-d: A rgb-d scene under-
from video. In ICRA, pages 1134–1141. IEEE,
standingbenchmarksuite. InCVPR,pages567–
2018. 19
576,2015. 10
[172] Rutav Shah and Vikash Kumar. Rrl: Resnet as
[182] Sanjana Srivastava, Chengshu Li, Michael
representationforreinforcementlearning.arXiv
Lingelbach, Roberto Martín-Martín, Fei Xia,
preprintarXiv:2107.03380,2021. 19
Kent Elliott Vainio, Zheng Lian, Cem Gokmen,
[173] Dandan Shan, Jiaqi Geng, Michelle Shu, and Shyamal Buch, Karen Liu, et al. Behavior:
DavidFFouhey. Understandinghumanhands Benchmark for everyday household activities
in contact at internet scale. In CVPR, pages in virtual, interactive, and ecological environ-
9869–9878,2020. 19 ments.InCoRL,pages477–490.PMLR,2022.21,
24
[174] LinShao,TokiMigimatsu,QiangZhang,Karen
Yang, and Jeannette Bohg. Concept2robot: [183] Bradly C Stadie, Pieter Abbeel, and Ilya
Learning manipulation concepts from instruc- Sutskever. Third-person imitation learning.
tions and human demonstrations. IJRR, 40(12- arXivpreprintarXiv:1703.01703,2017. 19
14):1419–1434,2021. 19
[184] LucSteels. Thesymbolgroundingproblemhas
[175] Bokui Shen, Fei Xia, Chengshu Li, Roberto beensolved.sowhat’snext. Symbolsandembod-
Martín-Martín,LinxiFan,GuanzhiWang,Clau- iment: Debates on meaning and cognition, pages
dia Pérez-D’Arpino, Shyamal Buch, Sanjana 223–244,2008. 3
35
[185] Julian Straub, Thomas Whelan, Lingni Ma, Yu- [193] Brandon Trabucco, Gunnar A. Sigurdsson,
fan Chen, Erik Wijmans, Simon Green, Jakob J. RobinsonPiramuthu,GauravS.Sukhatme,and
Engel, Raul Mur-Artal, Carl Ren, Shobhit Ruslan Salakhutdinov. A simple approach for
Verma, Anton Clarkson, Mingfei Yan, Brian visualrearrangement: 3dmappingandseman-
Budge, Yajie Yan, Xiaqing Pan, June Yon, ticsearch. CoRR,abs/2206.13396,2022. 20
Yuyang Zou, Kimberly Leon, Nigel Carter, Je-
sus Briales, Tyler Gillingham, Elias Mueggler, [194] Joanne Truong, Sonia Chernova, and Dhruv
Luis Pesqueira, Manolis Savva, Dhruv Batra, Batra. Bi-directional domain adaptation for
Hauke M. Strasdat, Renzo De Nardi, Michael sim2real transfer of embodied navigation
Goesele, Steven Lovegrove, and Richard New- agents. RA-L,6(2):2634–2641,2021. 22
combe. TheReplicadataset: Adigitalreplicaof
[195] Joanne Truong, Max Rudolph, Naoki
indoor spaces. arXiv preprint arXiv:1906.05797,
Yokoyama, Sonia Chernova, Dhruv Batra,
2019. 21
and Akshara Rai. Rethinking sim2real: Lower
[186] Andrew Szot, Alexander Clegg, Eric Under- fidelity simulation leads to higher sim2real
sander, Erik Wijmans, Yili Zhao, John Turner, transferinnavigation. InCoRL,2022. 22
Noah Maestre, Mustafa Mukadam, Deven-
draSinghChaplot,OleksandrMaksymets,etal. [196] Joanne Truong, Denis Yarats, Tianyu Li,
Habitat 2.0: Training home assistants to rear- FranziskaMeier,SoniaChernova,DhruvBatra,
range their habitat. NeurIPS, 34:251–266, 2021. andAksharaRai. Learningnavigationskillsfor
21,22 legged robots with learned robot embeddings.
InIROS,2020. 22
[187] Ben Talbot, David Hall, Haoyang Zhang,
Suman Raj Bista, Rohan Smith, Feras Day-
[197] Francisco J Varela, Evan Thompson, and
oub, and Niko Sünderhauf. Benchbot: Eval-
Eleanor Rosch. The embodied mind, revised edi-
uating robotics research in photorealistic 3d
tion: Cognitivescienceandhumanexperience. MIT
simulation and on real robots. arXiv preprint
press,2017. 3
arXiv:2008.00635,2020. 10,22
[198] SaimWani,ShivanshPatel,UnnatJain,AngelX.
[188] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil
Chang, and Manolis Savva. Multi-ON: Bench-
Iscen,YunfeiBai,DanijarHafner,StevenBohez,
marking Semantic Map Memory using Multi-
and Vincent Vanhoucke. Sim-to-real: Learning
ObjectNavigation. InNeurIPS,2020. 1,7,9
agile locomotion for quadruped robots. CoRR,
abs/1804.10332,2018. 2,4
[199] Justin Wasserman, Karmesh Yadav, Girish
[189] Open Ended Learning Team, Adam Stooke, Chowdhary, Abhinav Gupta, and Unnat Jain.
Anuj Mahajan, Catarina Barros, Charlie Deck, Last-mileembodiedvisualnavigation. InCoRL,
Jakob Bauer, Jakub Sygnowski, Maja Trebacz, 2022. 18
Max Jaderberg, Michael Mathieu, et al. Open-
ended learning leads to generally capable [200] Luca Weihs, Matt Deitke, Aniruddha Kemb-
agents. arXivpreprintarXiv:2107.12808,2021. 23 havi, and Roozbeh Mottaghi. Visual room re-
arrangement. InCVPR,pages5922–5931.Com-
[190] Jesse Thomason, Michael Murray, Maya Cak-
puterVisionFoundation/IEEE,2021. 2,13,22
mak,andLukeZettlemoyer. Vision-and-dialog
navigation. InCoRL,2019. 16 [201] Luca Weihs, Aniruddha Kembhavi, Kiana
Ehsani, Sarah M Pratt, Winson Han, Alvaro
[191] JoshTobin,RachelFong,AlexRay,JonasSchnei-
Herrasti,EricKolve,DustinSchwenk,Roozbeh
der,WojciechZaremba,andPieterAbbeel. Do-
Mottaghi,andAliFarhadi. Learninggeneraliz-
main randomization for transferring deep neu-
ablevisualrepresentationsviainteractivegame-
ralnetworksfromsimulationtotherealworld.
play. InICLR,2020. 24
InIROS,pages23–30.IEEE,2017. 22
[192] Brandon Trabucco, Gunnar Sigurdsson, Robin- [202] Luca Weihs, Jordi Salvador, Klemen Kotar, Un-
son Piramuthu, Gaurav S Sukhatme, and Rus- nat Jain, Kuo-Hao Zeng, Roozbeh Mottaghi,
lan Salakhutdinov. A simple approach for vi- and Aniruddha Kembhavi. Allenact: A frame-
sual rearrangement: 3d mapping and semantic work for embodied ai research. arXiv preprint
search. arXivpreprintarXiv:2206.13396,2022. 18 arXiv:2008.12760,2020. 17
36
[203] Erik Wijmans, Irfan Essa, and Dhruv Batra. motor control. arXiv preprint arXiv:2203.06173,
How to train pointgoal navigation agents on a 2022. 19
(sample and compute) budget. arXiv preprint
arXiv:2012.06117,2020. 19 [213] Xuesu Xiao, Bo Liu, Garrett Warnell, and Peter
Stone. Toward agile maneuvers in highly con-
[204] Erik Wijmans, Abhishek Kadian, Ari Morcos, strained spaces: Learning from hallucination.
Stefan Lee, Irfan Essa, Devi Parikh, Manolis RA-L,6(2):1503–1510,2021. 18
Savva, and Dhruv Batra. Dd-ppo: Learning
near-perfect pointgoal navigators from 2.5 bil- [214] Karmesh Yadav, Santhosh Kumar Ramakrish-
lion frames. arXiv preprint arXiv:1911.00357, nan, John Turner, Aaron Gokaslan, Oleksandr
2019. 17,19 Maksymets,RishabhJain,RamRamrakhya,An-
gel X Chang, Alexander Clegg, Manolis Savva,
[205] Erik Wijmans, Abhishek Kadian, Ari Morcos,
Eric Undersander, Devendra Singh Chaplot,
Stefan Lee, Irfan Essa, Devi Parikh, Manolis
andDhruvBatra.Habitatchallenge2022.https:
Savva, and Dhruv Batra. Decentralized dis-
//aihabitat.org/challenge/2022/,2022. 5,7
tributed ppo: Solving pointgoal navigation.
arXivpreprintarXiv:1911.00357,2019. 18
[215] Karmesh Yadav, Ram Ramrakhya, Arjun Ma-
jumdar, Vincent-Pierre Berges, Sachit Kuhar,
[206] DavidEWilkins.Practicalplanning:extendingthe
Dhruv Batra, Alexei Baevski, and Oleksandr
classicalAIplanningparadigm. Elsevier, 1988. 2,
Maksymets. Offlinevisualrepresentationlearn-
3
ing for embodied navigation. arXiv preprint
[207] Jungdam Won, Deepak Gopinath, and Jessica arXiv:2204.13226,2022. 19
Hodgins.Ascalableapproachtocontroldiverse
behaviors for physically simulated characters. [216] WeiYang,XiaolongWang,AliFarhadi,Abhinav
ACMTransactionsonGraphics,2020. 24 Gupta, and Roozbeh Mottaghi. Visual seman-
ticnavigationusingscenepriors. arXivpreprint
[208] Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill arXiv:1810.06543,2018. 18
Freeman, and Josh Tenenbaum. Learning to
see physics via visual de-animation. In Is- [217] LinYen-Chen,AndyZeng,ShuranSong,Phillip
abelle Guyon, Ulrike von Luxburg, Samy Ben- Isola, andTsung-YiLin. Learningtoseebefore
gio, Hanna M. Wallach, Rob Fergus, S. V. N. learning to act: Visual pre-training for manip-
Vishwanathan, and Roman Garnett, editors, ulation. In ICRA, pages 7286–7293. IEEE, 2020.
NeurIPS,pages153–164,2017. 20 19
[209] FeiXia,AmirR.Zamir,Zhi-YangHe,Alexander
[218] Naoki Yokoyama, Qian Luo, Dhruv Batra, and
Sax, Jitendra Malik, and Silvio Savarese. Gib-
SehoonHa.Benchmarkingaugmentationmeth-
son Env: real-world perception for embodied
ods for learning robust navigation agents: the
agents. InCVPR.IEEE,2018. 17,21
winning entry of the 2021 igibson challenge.
arXivpreprintarXiv:2109.10493,2021. 7
[210] Fei Xia, William B Shen, Chengshu Li, Priya
Kasimbeg, Micael Edmond Tchapmi, Alexan-
[219] AbdelrahmanYounes,DanielHonerkamp,Tim
der Toshev, Roberto Martín-Martín, and Silvio
Welschehold, and Abhinav Valada. Catch me
Savarese. Interactive gibson benchmark: A
if you hear me: Audio-visual navigation in
benchmark for interactive navigation in clut-
complexunmappedenvironmentswithmoving
tered environments. RA-L, 5(2):713–720, 2020.
sounds. arxiv,2022. 11
1,6,19
[211] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan [220] Wenhao Yu, Visak CV Kumar, Greg Turk, and
Xia, Hao Zhu, Fangchen Liu, Minghua Liu, C. Karen Liu. Sim-to-real transfer for biped lo-
Hanxiao Jiang, Yifu Yuan, He Wang, et al. comotion. In IROS, pages 3503–3510, 2019. 2,
Sapien: A simulated part-based interactive en- 4
vironment. In CVPR, pages 11097–11107, 2020.
[221] Wenhao Yu, Jie Tan, C Karen Liu, and Greg
21
Turk. Preparing for the unknown: Learning a
[212] TeteXiao,IlijaRadosavovic,TrevorDarrell,and universal policy with online system identifica-
Jitendra Malik. Masked visual pre-training for tion. RoboticsScienceandSystems,2017. 22
37
[222] Yinfeng Yu, Wenbing Huang, Fuchun Sun,
ChanganChen,YikaiWang,andXiaohongLiu.
Sound adversarial audio-visual navigation. In
ICLR,2022. 11
[223] Amir R Zamir, Fei Xia, Jerry He, Sasha Sax, Ji-
tendraMalik,andSilvioSavarese. GibsonEnv:
Real-worldperceptionforembodiedagents. In
CVPR,2018. 5
[224] Andy Zeng, Adrian Wong, Stefan Welker,
Krzysztof Choromanski, Federico Tombari,
AveekPurohit,MichaelRyoo,VikasSindhwani,
Johnny Lee, Vincent Vanhoucke, et al. So-
cratic models: Composing zero-shot multi-
modal reasoning with language. arXiv preprint
arXiv:2204.00598,2022. 23
[225] Amy Zhang, Rowan McAllister, Roberto Ca-
landra, Yarin Gal, and Sergey Levine. Learn-
ing invariant representations for reinforcement
learning without reconstruction. arXiv preprint
arXiv:2006.10742,2020. 19
[226] TinghuiZhou,RichardTucker,JohnFlynn,Gra-
hamFyffe,andNoahSnavely.Stereomagnifica-
tion: Learningviewsynthesisusingmultiplane
images. ACM Trans. Graph. (Proc. SIGGRAPH),
37,2018. 19
[227] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve,
JosephJLim,AbhinavGupta,LiFei-Fei,andAli
Farhadi. Target-driven visual navigation in in-
doorscenesusingdeepreinforcementlearning.
InICRA,2017. 18,22
38

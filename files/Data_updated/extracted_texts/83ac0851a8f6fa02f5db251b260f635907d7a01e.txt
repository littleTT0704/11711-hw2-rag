Tactical Rewind: Self-Correction via Backtracking
in Vision-and-Language Navigation
LiyimingKe1∗ XiujunLi1,2 YonatanBisk1 AriHoltzman1 ZheGan2
JingjingLiu2 JianfengGao2 YejinChoi1,3 SiddharthaSrinivasa1
1PaulG.AllenSchoolofComputerScience&Engineering,UniversityofWashington
2MicrosoftResearchAI 3AllenInstituteforArtificialIntelligence
{kayke, xiujun, ybisk, ahai, yejin, siddh}@cs.washington.edu
{xiul, zhgan, jingjl, jfgao}@microsoft.com
Abstract
We present the Frontier Aware Search with backTrack-
ing (FAST) Navigator, a general framework for action de-
coding, that achieves state-of-the-art results on the Room-
to-Room(R2R)Vision-and-Languagenavigationchallenge
of Anderson et. al. (2018). Given a natural language in-
struction and photo-realistic image views of a previously
unseen environment, the agent was tasked with navigating
fromsourcetotargetlocationasquicklyaspossible. While
allcurrentapproachesmakelocalactiondecisionsorscore
entire trajectories using beam search, ours balances local
andglobalsignalswhenexploringanunobservedenviron-
ment. Importantly, this lets us act greedily but use global
signalstobacktrackwhennecessary.ApplyingFASTframe-
worktoexistingstate-of-the-artmodelsachieveda17%rel- (a) SoTABeamSearch (b) FASTNAVIGATOR
ative gain, an absolute 6% gain on Success rate weighted Figure1. Top-downviewofthetrajectorygraphsforbeamsearch
byPathLength(SPL).1 andFAST.BlueStaristhestartandRedStopisthetarget.
1.Introduction decoded as output. Several subsequent architectures also
use this framing; however, they augment it with impor-
When reading an instruction (e.g. “Exit the bathroom, tantadvancesinattentionmechanisms,globalscoring,and
taketheseconddooronyourright, passthesofaandstop beamsearch[2,13,10].
atthetopofthestairs.”),apersonbuildsamentalmapof Inherent to the seq2seq formulation is the problem of
how to arrive at a specific location. This map can include exposure bias [19]: a model that has been trained to pre-
landmarks, such as the second door, and markers such as dict one-step into the future given the ground-truth se-
reaching the top of the stairs. Training an embodied agent quence cannot perform accurately given its self-generated
to accomplish such a task with access to only ego-centric sequence. Previousworkwithseq2seqmodelsattemptedto
visionandindividuallysupervisedactionsrequiresbuilding addressthisusingstudentforcingandbeamsearch.
richmulti-modalrepresentationsfromlimiteddata[2]. Studentforcingexposesamodeltoitsowngeneratedse-
Most current approaches to Vision-and-Language Nav- quence during training, teaching the agent how to recover.
igation (VLN) formulate the task to use the seq2seq (or However,oncetheagenthasdeviatedfromthecorrectpath,
encoder-decoder) framework [21], where language and vi- theoriginalinstructionnolongerapplies. TheSupplemen-
sionareencodedasinputandanoptimalactionsequenceis taryMaterials(§A.1)showthatstudentforcingcannotsolve
∗WorkdonepartiallyasaninternatMSR the exposure bias problem, causing the confused agent to
1Thecodeisavailableathttps://github.com/Kelym/FAST. fallintoloops.
9102
rpA
2
]LC.sc[
2v74520.3091:viXra
Beam search, at the other extreme, collects multiple movingtowardsoneoftheK views,insteadofR2R’sorig-
globaltrajectoriestoscoreandincursacostproportionalto inal primitive action space (i.e, left, right, etc.) [2, 23]. In
thenumberoftrajectories,whichcanbeprohibitivelyhigh. addition,thisformulationincludesastopactiontoindicate
Thisapproachrunscountertothegoalofbuildinganagent thattheagenthasreacheditsgoal.
thatcanefficientlynavigateanenvironment: Noonewould
likely deploy a household robot that re-navigates an entire Greedy FAST BeamSearch
house100times2 beforeexecutingeachcommand, evenif
it ultimately arrives at the correct location. The top per-
formingsystemsontheVLNleaderboard3allrequirebroad
exploration that yields long trajectories, causing poor SPL
performance(SuccessweightedbyPathLength[1]).
To alleviate the issues of exposure bias and expen-
sive, inefficient beam-search decoding, we propose the Figure 2. All VLN agents are performing a search. The orange
FrontierAwareSearchwithbackTracking(FAST NAVIGA- areashighlightthefrontierfordifferentnavigationmethods.
TOR). This framework lets agents compare partial paths
of different lengths based on local and global information
2.1.LearningSignals
andthenbacktrackifitdiscernsamistake. Figure1shows
trajectorygraphscreatedbythecurrentpublishedstate-of-
KeytoprogressinvisualnavigationisthatallVLNap-
the-art(SoTA)agentusingbeamsearchversusourown.
proaches performs a search (Figure 2). Current work of-
Our method is a form of asynchronous search, which
ten goes toward two extremes: using only local informa-
combines global and local knowledge to score and com-
tion,e.g.greedydecoding,orfullysweepingmultiplepaths
pare partial trajectories of different lengths. We evaluate
simultaneously, e.g. beam search. To build an agent that
our progress to the goal by modeling how closely our pre-
can navigate an environment successfully and efficiently,
vious actions align with the given text instructions. To
we leverage both local and global information, letting the
achieve this, we use a fusion function, which converts lo-
agent make a local decision while remaining aware of its
calactionknowledgeandhistoryintoanestimatedscoreof
globalprogressandefficientlybacktrackingwhentheagent
progress. Thisscoredetermineswhichlocalactiontotake
discernsamistake. Inspiredbypreviouswork[10,13],our
andwhethertheagentshouldbacktrack. Thisinsightyields
workusesthreelearningsignals:
significant gains on evaluation metrics relative to existing
models. Theprimarycontributionsofourworkare:
LOGIT l t: local distribution over action. The logit of
the action chosen at time t is denoted l . Specifically, the
t
• Amethodtoalleviatetheexposurebiasofactiondecod-
original language instruction is encoded via LSTM. An-
ingandexpensivenessofbeamsearch.
other LSTM acts as a decoder, using attention mechanism
• Analgorithmthatmakesuseofasynchronoussearchwith
to generate logits over actions. At each time step t of de-
neuraldecoding.
coding, logits are calculated by taking the dot product of
• Anextensibleframeworkthatcanbeappliedtoexisting thedecoder’shiddenstateandeachcandidateactionai.
t
modelstoachievesignificantgainsonSPL. PM ppm: global progress monitor. It tracks how much
t
of an instruction has been completed [13]. Formally, the
2.Method
model takes as input the (decoder) LSTM’s current cell
TheVLNchallengerequiresanagenttocarryoutanat- state, c t, previous hidden state, h t−1, visual inputs, V t,
ural language instruction in photo-realistic environments. and attention over language embeddings, α t to compute
The agent takes an input instruction X, which contains a score pp tm. The score ranges between [-1,1], indicating
several sentences describing a desired trajectory. At each the agent’s normalized progress. Training this indicator
stept,theagentobservesitssurroundingsV . Becausethe regularizes attention alignments, helping the model learn
t
agent can look around for 360 degrees, V is in fact a set language-to-visioncorrespondencesthatitcanusetocom-
t
of K = 36 different views. We denote each view as Vk. paremultipletrajectories.
t
Usingthismultimodalinput,theagentistrainedtoexecute SPEAKER S: global scoring. Given a sequence of vi-
asequenceofactionsa ,a ,....,a ∈Atoreachadesired sualobservationsandactions,wetrainaseq2seqcaptioning
1 2 T
location. Consistent with recent work [13, 10], we use a modelasa“speaker”[10]toproduceatextualdescription.
panoramic action space, where each action corresponds to Doing so provides two benefits: (1) the new speaker can
automaticallyannotatenewtrajectoriesintheenvironment
2ThisiscalculatedbasedonthelengthofSPEAKER-FOLLOWERagent
withthesyntheticinstructions,and(2)thespeakercanscore
pathsandhumanpathsontheR2Rdataset.
3https://evalai.cloudcv.org/web/challenges/challenge- thelikelihoodthatagiventrajectorywillcorrespondtothe
page/97/leaderboard/270 originalinstruction.
(a) Instructions and visual observations are encoded as hidden vectors (b) Ateachtimestep,thepredictedactionsequenceandvisualobservation
definingmultiplepathsthroughtheworld. Thesevectorscanthenbeac- arefedintoanattentionmodulewiththeencodedinstruction,toproduce
cumulatedtoscoreasequenceofactions. boththelogitsforthenextactionsandaprogressmonitorscore.
Figure3.(a). Howthethreesignalsareextractedfromthepartialtrajectoryinaseq2seqVLNframework;(b). Howtocomputethethree
signals.
2.2.Framework destination. Theagentmovesintheenvironmentbychoos-
ingtoextendapartialtrajectory: itdoesthisbymovingto
We now introduce an extendible framework4 that inte-
the last node of the partial trajectory and executing its last
gratestheprecedingthreesignals(l ,ppm,S)5 andtotrain
t t actionto arrive ata newnode. Theagent thenrealizes the
newindicators,equippinganagenttoanswer:
actionsavailableatthenewnodeandcollectsthemtobuild
1. Shouldwebacktrack? asetofnewpartialtrajectories.
At each time step, the agent must (1) access the set of
2. Whereshouldwebacktrackto?
partialtrajectoriesithasnotexpanded,(2)accessthecom-
3. Whichvisitednodeismostlikelytobethegoal?
pletedtrajectoriesthatmightconstitutethecandidatepath,
4. Whendoesitterminatethissearch?
(3)calculatetheaccumulatedcostofpartialtrajectoriesand
Thesequestionspertaintoallexistingapproachesinnav- theexpectedgainofitsproposedaction, and(4)compares
igation task. In particular, greedy approaches never back- allpartialtrajectories.
trackanddonotcomparepartialtrajectories. Globalbeam To do so, we maintain two priority queues: a frontier
search techniques always backtrack but can waste efforts. queue, Q F, for partial trajectories, and a global candidate
Bytakingamoreprincipledapproachtomodelingnaviga- queue, Q C, for completed trajectories. These queues are
tionasgraphtraversal,ourframeworkpermitsnuancedand sortedbylocalLandglobalGscores,respectively.Lscores
adaptiveanswerstoeachofthesequestions. thequalityofallpartialtrajectorieswiththeirproposedac-
For navigation, the graph isdefined by aseries of loca- tionsandmaintainstheirorderinQ F;G scoresthequality
tions in the environment, called nodes. For each task, the ofcompletedtrajectoriesandmaintainstheorderinQ C.
agentisplacedatastartingnode,andtheagent’smovement In§4.3,weexplorealternativeformulasforLandG.For
inthehousecreatesatrajectorycomprisedofasequenceof example,wedefineLandG usingthesignalsdescribedin
<nodeu, actiona >pairs. Wedenoteapartialtrajectory §2.1andafunction,f,thatisimplementedasaneuralnet-
uptotimetasτ ,orthesetofphysicallocationsvisitedand work.
t
theactiontakenateachpoint:
L ←Σ l (2)
0→t i
τ t ={(u i,a i)}t i=1 (1) G ←f(S,pp tm,Σ 0→tl i,...) (3)
Foranypartialtrajectory,thelastactionisproposedand To allow the agent to efficiently navigate and follow
evaluated, but not executed. Instead, the model chooses theinstruction,weuseanapproximationoftheD*search.
whethertoexpandapartialtrajectoryorexecuteastopac- FAST expands its optimal partial trajectory until it decides
tiontocompletethetrajectory. Importantly,thismeansthat to backtrack (Q1). It decides on where to backtrack (Q2)
every node the agent visited can serve as a possible final byrankingallpartialtrajectories. Toproposethefinalgoal
location (Q3 & Q4), the agent ranks the completed global
4Figure 3(a) shows an example of integrating the three signals in a
trajectoriesincandidatequeueQ . Weexploretheseques-
seq2seqframework. C
5Figure3(b)showshowtocomputethethreesignals. tionsinmoredetailbelow.
(a) BothlocalLandglobalGscorescanbetrainedtoconditiononar- (b) Anexpansionqueuemaintainsallpossiblenextactionsfromallpar-
bitrary information. Here, we show the fusion of historical logits and tialtrajectories. Theoptionsaresortedbytheirscores(Figure4(a))in
progressmonitorinformationintoasinglescore. ordertoselectthenextaction.
Figure4.Arbitrarysignalscanbecomputedfrompartialtrajectoriestolearnascoringfunction(left)thatranksallpossibleactionsinour
expansionqueue(right).Thisprovidesaflexibleandextendibleframeworkforoptimalactiondecoding.
Q1:Shouldwebacktrack? Whenanagentmakesamis- how well the action was aligned with the target de-
takeorgetslost,backtrackingletsitmovetoamorepromis- scription(thisinformationislostduringnormalization).6
ingpartialtrajectory;however,retracingstepsincreasesthe
lengthofthefinalpath.Todeterminewhenitisworthincur- Finally, during exploration, the agent implicitly con-
ring this cost, we proposed two simple strategies: explore structs a “mental map” of the visited space. This lets it
andexploit. search more efficient by refusing to revisit nodes, unless
1. Explorealwaysbacktrackstothemostpromisingpartial theyleadtoahigh-valueunexploredpath.
trajectory. This approach resembles beam search, but,
ratherthansimplymovingtothenextpartialtrajectoryin Q3: Which visited node is most likely to be the goal?
the beam, the agent computes the most promising node Unlikeexistingapproaches,FASTconsiderseverypointthat
tobacktrackto(Q2). theagenthasvisitedasacandidateforthefinaldestination,7
2. Exploit, in contrast, commits to the current partial tra- meaningwemustrerankallcandidates. Weachievethisus-
jectory, always executing the best action available at ingG,atrainableneuralnetworkfunctionthatincorporates
the agent’s current location. This approach resembles all global information for each candidate and ranks them
greedydecoding,exceptthattheagentbacktrackswhen accordingly. Figure4(a)showsasimplevisualization.
itisconfused(i.e,whenthebestlocalactioncausesthe WeexperimentedwithseveralapproachestocomputeG,
agent to revisit a node, creating a loop; see the SMNA e.g., byintegratingL, theprogressmonitor, speakerscore,
examplesinSupplementaryMaterials§A.1). andatrainableensemblein(§4.3).
Q4: Whendoweterminatethesearch? Theflexibility
Q2: Whereshouldwebacktrackto? Makingthisdeci-
of FAST allowsittorecoverboththegreedydecodingand
sioninvolvesusingLtoscoreallpartialtrajectories. Intu-
beamsearchframework.Inaddition,wedefinetwoalterna-
itively,thebetterapartialtrajectoryalignswithagivende-
tivestoppingcriteria:
scription,thehigherthevalueofL. Thus,ifwecanassume
1. Whenapartialtrajectorydecidestoterminate.
the veracity of L, the agent simply returns to the highest
2. WhenwehaveexpandedM nodes.In§3weablatethe
scoringnodewhenbacktracking.Throughoutthispaper,we
effectofchoosingadifferentM.
explore several functions for computing L, but we present
twosimpletechniqueshere, eachactingoverthesequence 2.3.Algorithm
ofactionsthatcompriseatrajectory:
(cid:80) We present the algorithm flow of our FAST framework.
1. Sum-of-log logp sums the log-probabilities of
0→t i Whenanagentisinitializedandplacedonthestartingnode,
every previous action, thereby computing the probabil-
boththecandidateandfrontierqueuesareempty.Theagent
ityofapartialtrajectory.
(cid:80)
2. Sum-of-logits 0→tl i sums the unnormalized logits 6Thisisparticularlyproblematicwhenanagentislost. Normalizing
of previous actions, which outperforms summing manylow-valuelogitscanyieldacomparativelyhighprobability(e.g.uni-
formorrandom).Wealsoexperimentwithvariationsofthisapproach(e.g.
probabilities. These values are computed using an
meansinsteadofsums)in§4.
attentionmechanismoverthehiddenstate,observations, 7Therecanbemorethanonetrajectoryconnectingthestartingnodeto
and language. In this way, their magnitude captures eachvisitednode.
Algorithm1FASTNAVIGATOR 3.Experiments
1: procedureFASTNAVIGATOR
We evaluate our approach using the Room-to-Room
2: Qsort=L,Qsort=G ={},{}
F C (R2R) dataset [2]. At the beginning of the task, the agent
3: Q F ←(u 0,a 0 =None) (cid:46)InitialProposal
4:
τˆ←∅ receives a natural language instruction and a specific start
5: M ←∅ (cid:46)MentalMap locationintheenvironment;theagentmustnavigatetothe
6: whileQ F (cid:54)=∅andstopcriteriondo targetlocationspecifiedintheinstructionasquicklyaspos-
7:
ifneedbacktrackorτˆ==∅then sible.R2RisbuiltupontheMatterport3Ddataset[5],which
consists of >194K images, yielding 10,800 panoramic
8: τˆ←Q F.pop
views(“nodes”)and7,189paths.Eachpathismatchedwith
9: endif
threenaturallanguageinstructions.
10: uˆ t−1,aˆ t−1 ←τˆ.last
11: if(uˆ t−1,aˆ t−1)∈M then
3.1.EvaluationCriteria
12: u t ←M(uˆ t−1,aˆ t−1)
13: else Weevaluateourapproachonthefollowingmetricsinthe
14: u t ←movetou t−1andexecutea t−1 R2Rdataset:
15: M(uˆ t−1,aˆ t−1)←u t TL TrajectoryLengthmeasurestheaveragelengthofthe
16: endif navigationtrajectory.
17: fora k inbestK nextactionsdo NE NavigationErroristhemeanoftheshortestpathdis-
18: Q F ←Q F ∪{τˆ+(u t,a k)} tanceinmetersbetweentheagent’sfinallocationand
19: endfor thegoallocation.
20: Q C ←Q C ∪τˆ SR SuccessRateisthepercentageoftheagent’sfinallo-
21: τˆ←τˆ+(u t,a∗)wherea∗isthebestaction cation that is less than 3 meters away from the goal
22: endwhile location.
23: returnQ C.pop SPL Success weighted by Path Length [1] trades-off SR
24: endprocedure againstTL.Higherscorerepresentsmoreefficiencyin
navigation.
thenaddsallpossiblenextactionstothefrontierqueueand 3.2.Baselines
addsitscurrentlocationtothecandidatequeue:
We compare our results to four published baselines for
thistask.8
Q ←Q +∀ {τ ∪(u ,a )} (4)
F F i∈K 0 0 i
• RANDOM: an agent that randomly selects a direction
Q ←Q +τ (5)
C C 0 andmovesfivestepinthatdirection [2].
• SEQ2SEQ: the best performing model in the R2R
Now that the Q is not empty and the stop criterion is
F
datasetpaper[2].
not met, FAST can choose the best partial trajectory from
• SPEAKER-FOLLOWER [10]: an agent trained with
thefrontierqueueunderthelocalscoringfunction:
data augmentation from a speaker model on the
panoramicactionspace.
τˆ ←argmaxL(Q ) (6)
F
τi • SMNA [13]: an agent trained with a visual-textual
co-grounding module and a progress monitor on the
Following τˆ, we perform the final action proposal, a , to
t panoramicactionspace.9
movetoanewnode(locationinthehouse). FASTcannow
updatethecandidatequeuewiththislocationandthefron- 3.3.OurModel
tier queue with all possible new actions. We then either
continue, byexploitingtheavailableactionsatthenewlo- As our framework provides a flexible design space, we
cation,orbacktrack,dependingonthechoiceofbacktrack reportperformancefortwoversions:
criteria. We repeat this process until the model chooses to • FAST(short)usestheexploitstrategy. Weusethesum
stopandreturnsthebestcandidatetrajectory. of logits fusion method to compute L and terminate
whenthebestlocalactionisstop.
τ∗ ←argmaxG(Q ) (7)
C
τ
8Somebaselinesontheleader-boardarenotyetpublicwhensubmit-
ted;therefore,wecannotcomparewiththemdirectlyonthetrainingand
Algorithm 1 more precisely outlines the full procedure for
validationsets.
ourapproach. §4.3detailsthedifferentapproachestoscor- 9Our SMNA implementationmatchespublishedvalidationnumbers.
ingpartialandcompletetrajectories. Allourexperimentsarebasedonfullre-implementations.
ValidationSeen ValidationUnseen TestUnseen
Model TL NE SR SPL TL NE SR SPL TL NE SR SPL
RANDOM 9.58 9.45 0.16 - 9.77 9.23 0.16 - 9.93 9.77 0.13 0.12
Seq2seq 11.33 6.01 0.39 - 8.39 7.81 0.22 - 8.13 7.85 0.20 0.18
OurbaselineSMNA 11.69 3.31 0.69 0.63 12.61 5.48 0.47 0.41 - - - -
SMNA - - - - - - - - 18.04 5.67 0.48 0.35
SPEAKER-FOLLOWER - - - - - - - - 14.82 6.62 0.35 0.28
+FAST (short) 21.17 4.97 0.56 0.43 22.08 5.14 0.54 0.41
SMNA - 3.23 0.70 - - 5.04 0.57 - 373.09 4.48 0.61 0.02
SPEAKER-FOLLOWER - 3.88 0.63 - - 5.24 0.50 - 1,257.30 4.87 0.53 0.01
+FAST (long) 188.06 3.13 0.70 0.04 224.42 4.03 0.63 0.02 196.53 4.29 0.61 0.03
Human - - - - - - - - 11.85 1.61 0.86 0.76
Table1.OurresultsandSMNAre-implementationareshowningrayhighlightedrows. Boldingindicatesthebestvaluepersectionand
blueindicatesbestvaluesoverall. Weincludebothashortandlongversionofourapproachtocomparetoexistingmodelsgreedyand
beamsearchapproaches.
• FAST(long) uses the explore strategy. We again use
thesumoflogitsforfusion,terminatingthesearchaf-
ter fixed number of nodes and using a trained neural
networkrerankertoselectthegoalstateG.
3.4.Results
Table1comparestheperformanceofourmodelagainst
published numbers of existing models. Our approach sig-
nificantly outperforms the existing model in terms of effi-
ciency, matching the best overall success rate despite tak-
ing 150 - 1,000 fewer steps. This efficiency gain can be
seenintheSPLmetric,whereourmodelsoutperformpre-
vious approaches in every setting. Note that our short tra-
jectory model appreciably outperforms current approaches Figure5.Circlesizesrepresentthewhatpercentageofagentsdi-
inbothSRandSPL.Ifouragentcouldcontinueexploring, vergeonstepN.Mostdivergencesoccurintheearlysteps. FAST
recoversfromearlydivergences.
it matches existing peak success rates in half of the steps
(196vs373).
rate for SPEAKER-FOLLOWER and SMNA, respectively.
ValidationUnseen SR(%) SPL(%) TL
Due to those models’ new ability to backtrack, the trajec-
SPEAKER-FOLLOWER 37 28 15.32 torylengthsincreaseslightly. However,thesuccessratein-
+FAST 43(+6) 29(+1) 20.63 creasessomuchthatSPLincreases,aswell.
SMNA 47 41 12.61
+FAST 56(+9) 43(+2) 21.17
4.Analysis
Table2.Plug-n-playperformancegainsachievedbyaddingFAST
tocurrentSoTAmodels. Here, we isolate the effects of local and global knowl-
edge,theimportanceofbacktracking,andvariousstopping
Anotherkeyadvantageofourtechniqueishowsimpleit criteria. In addition, we include three qualitative intuitive
istointegratewithcurrentapproachestoachievedramatic examples to illustrate the model’s behavior in the Supple-
performance gains. Table 2 shows how the sum-of-logits mentaryMaterials(§A.1). Wecanperformthisanalysisbe-
fusion method enhances the two previously best perform- cause our approach has access to the same information as
ingmodels.SimplychangingtheirgreedydecoderstoFAST previous architectures, but it is more efficient. Our claims
withnoaddedglobalinformationandthereforenorerank- andresultsaregeneral,andourFASTapproachshouldben-
ing yields immediate gains of 6 and 9 points in success efitfutureVLNarchitectures.
ydeerG
maeB
4.1.FixingYourMistakes
To investigate the degree to which models benefit from
backtracking, Figure 5 plots a model’s likelihood of suc-
cessfully completing the task after making its first mis-
take at each step. We use SMNA as our greedy baseline.
Our analysis finds that the previous SoTA model makes a
mistake at the very first action 40% of the time. Figure
5 shows the effect of this error: the greedy approach, if
madeamistakeatitsfirststep,hasa<30%chanceofsuc-
cessfully completing the task. In contrast, because FAST
detects its mistake, it returns to the starting position and
tries again. This simple one-step backtracking increases
Figure6.TheSRincreaseswiththenumberofnodesexploredbe-
its likelihood of success by over 10%. In fact, the greedy
foreplateauing,whileSPL(whichisextremelysensitivetolength)
approach is equally successful only if it progresses over
continuallydecreaseswithaddedexploration.
halfwaythroughtheinstructionwithoutmakingamistake.
Heur/step Combine SR SPL Len
4.2.KnowingWhenToStopExploring
logit mean 53.89 44.74 14.80
Thestoppingcriterionbalancesexplorationandexploita-
logprob mean 53.85 44.14 15.57
tion. Unlike previous approaches, our framework lets us
logit sum 56.66 43.64 21.17
comparedifferentcriteriaandofferstheflexibilitytodeter-
logprob sum 56.23 42.66 21.70
minewhichisoptimalforagivendomain. Thebestavail-
able stopping criterion for VLN is not necessarily the best logit mean/pm 53.00 44.51 13.67
ingeneral. Weinvestigatedthenumberofnodestoexpand logprob mean/pm 53.72 44.64 13.85
beforeterminatingthealgorithm,andweplottheresulting logit mean*pm 54.78 44.70 15.91
successrateandSPLinFigure6. Oneimportantfindingis logprob mean*pm 55.04 43.70 17.45
thatthemodel’ssuccessrate, thoughincreasingwithmore logit sum*pm 50.95 41.28 20.25
nodesexpanded,doesnotmatchtheoracle’srate,i.e.,asthe logprob sum*pm 56.15 43.19 21.55
agent expands 40 nodes, it has visited the true target node
Table3.Performanceofdifferentfusionmethodsforscoringpar-
over 90% of the time but cannot recognize it as the final
tialtrajectories.Testedonthevalidationunseenset.
destination. Thismotivatesananalysisoftheutilityofour
globalinformationandwhetheritistrulypredictive(Table
trajectory achieves an SR score of 56.66. Note although
4),whichweinvestigatefurtherin§4.3.
allinformationoriginateswiththesamehiddenvectors,the
4.3.LocalandGlobalScoring valuescomputedandhowtheyareaggregatedsubstantially
affect performance. Overall, we find that summing unnor-
As noted in §2.3, core to our approach are two queues, malized logits (the 3rd row) performs the best considering
frontier queue for expansion and the candidate queue for its outstanding SR. This suggests that important activation
proposing the final candidate. Each queue can use arbi- informationinthenetworkoutputsisbeingthrownawayby
traryinformationforscoring(partial)trajectories. Wenow normalizationandthereforediscardedbyothertechniques.
comparetheeffectsofcombiningdifferentsetofsignalsfor ThebottompartofTable3exploreswaysofcombining
scoringeachqueue. localandglobalinformationproviders.Thesearemotivated
bybeam-rescoringtechniquesinpreviouswork(e.g., mul-
Fusion methods for scoring partial trajectories An tiplying by the normalized progress monitor score). Cor-
ideal model would include as much global information as rectly integrating signals is challenging, in part due to dif-
possiblewhenscoringpartialtrajectoriesinthefrontierex- ferencesinscale. Forexample,thelogitisunbounded(+/-
pansion queue. Thus, we investigated several sources of ), log probabilities are unbounded in the negative, and the
pseudo-global information and ten different ways to com- progress monitor is normalized to a score between 0 and
binethem. Thefirstfouruseonlylocalinformation,while 1. Unfortunately,directintegrationoftheprogressmonitor
theothersattemptstofuselocalandglobalinformation. didnotyieldpromisingresults,butfuturesignalsmayprove
ThetophalfofTable3showstheperformancewhencon- morepowerful.
sideringonlylocalinformationproviders. Forexample,the
third row of the table shows that summing the logit scores Fusion methods for ranking complete trajectories .
ofnodesalongthepartialtrajectoryastheLscoreforthat Previous work [10] used state-factored beam search to
generate M candidates and rank the complete trajec- munity has explored instruction following using 2D maps
tories using probability of speaker and follower scores [17,14]andcomputer-rendered3Denvironments[16].Due
argmax P (d|r)λ ∗P (d|r)(1−λ). In addition to the totheenormousvisualcomplexityofreal-worldscenes,the
r∈R(d) S F
speakerandprogressmonitorscoresusedbypreviousmod- VLNliteratureusuallybuildsoncomputervisionworkfrom
els, we also experiment with using L to compute G. To referring expressions [15, 24], visual question answering
inspecttheperformanceofusingdifferentfusionmethods, [3], andego-centricQAthatrequiresnavigationtoanswer
weran FAST NAVIGATOR toexpand40nodesonthefron- questions [11, 8, 9]. Finally, core to the our work is the
tier and collect candidate trajectories. Table 4 shows the fieldofsearchalgorithm,datingbacktotheearliestdaysof
performance of different fusion scores that rank complete AI [18, 20], but largely absent from recent VLN literature
trajectories. Weseethatmosttechniqueshavealimitedun- thattendstofocusesmoreonneuralarchitecturedesign.
derstanding of the global task’s goal and formulation. We During publishing the Room-to-Room dataset (VLN),
do,however,findasignificantimprovementonunseentra- [2] introduced the “student forcing” method for seq2seq
jectorieswhenallsignalsarecombined. Forthiswetraina model. Later work integrated a planning module to com-
multi-layerperceptrontoaggregateandweightourpredic- bined model-based and model-free reinforcement learning
tors. Notethatanyimprovementstotheunderlyingmodels to better generalize to unseen environments [23], and a
ornewfeaturesintroducedbyfutureworkwilldirectlycor- Cross-Modal Matching method that enforces cross-modal
relatetogainsinthiscomponentofthepipeline. grounding both locally and globally via reinforcement
The top line of Table 4, shows oracle’s performance. learning [22]. Two substantial improvements came from
Thisindicateshowfarcurrentglobalinformationproviders panoramic action spaces and a “speaker” model trained
haveyettoachieve. Closingthisgapisanimportantdirec- to enable data augmentation and trajectory reranking for
tionforfuturework. beam search [10]. Most recently, [13] leverages a visual-
textual co-grounding attention mechanism to better align
Train ValSeen ValUnseen theinstructionandvisualscenesandincorporatesaprogress
monitor to estimate the agent’s current progress towards a
Oracle 99.13 92.85 90.20
goal. These approaches require beam search for peak SR.
Σl i 78.78 62.49 56.49 Beamsearchtechniquescanunfortunatelyleadtolongtra-
µl i 85.78 66.99 54.41 jectorieswhenexploringunknownenvironments. Thislim-
Σp i 91.25 68.56 56.15 itation motivates the work we present here. Existing ap-
µp i 91.60 69.34 58.75 proachestradeoffahighsuccessrateandlongtrajectories:
pp tm 66.71 53.67 50.15 greedy decoding provides short, often incorrect paths, the
S 69.99 53.77 43.68 beamsearchyieldshighsuccessratesbutlongtrajectories.
All 90.16 71.00 64.03
6.Conclusion
Table4.SuccessrateusingsevendifferentfusionscoresasG to
rerankthedestinationnodefromthecandidatepool. We present FAST NAVIGATOR, a framework for using
asynchronous search to boost any VLN navigator by en-
ablingexplicitbacktrackwhenanagentdetectsifitislost.
4.4.IntuitiveBehavior
This framework can be easily plugged into the most ad-
The Supplementary Materials (§A.1) provide three real vancedagentstoimmediatelyimprovetheirefficiency. Fur-
examplestoshowhowourmodelperformswhencompared ther, empirical results on the Room-to-Room dataset show
to greedy decoding (SMNA model). It highlights how the that our agent achieves state-of-the-art Success Rates and
sameobservationscanleadtodrasticallydifferentbehaviors SPLs.Oursearch-basedmethodiseasilyextendibletomore
during an agent’s rollout. Specifically, in Figures A1 and challenging settings, e.g., when an agent is given a goal
A2,thegreedydecoderisforcedintoabehavioralloopbe- withoutanyrouteinstruction[6,12],oracomplicatedreal
causeonlylocalimprovementsareconsidered. UsingFAST visualenvironment[7].
clearlyshowsthatevenasinglebacktrackingstepcanfree
theagentofpoorbehavioralchoices. Acknowledgments
Partial funding provided by DARPA’s CwC program
5.RelatedWork
through ARO (W911NF-15-1-0543), NSF (IIS-1524371,
Ourworkfocusesonandcomplementsrecentadvances 1703166), National Institute of Health (R01EB019335),
in Vision-and-Language Navigation (VLN) as introduced NationalScienceFoundationCPS(1544797),NationalSci-
by [2], but many aspects of the task and core technolo- ence Foundation NRI (1637748), the Office of Naval Re-
gies date back much further. The natural language com- search,theRCTA,Amazon,andHonda.
References iliary progress estimation. In International Conference on
LearningRepresentations(ICLR),2019. 1,2,5,8,11
[1] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy,
[14] H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and
S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi,
walk: Neuralmappingofnavigationalinstructionstoaction
M. Savva, and A. Zamir. On evaluation of embodied nav-
sequences. InAAAI,volume1,page2,2016. 8
igationagents. arXivpreprintarXiv:1807.06757, 2018. 2,
[15] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. Ballard,
5
A.Banino,M.Denil,R.Goroshin,L.Sifre,K.Kavukcuoglu,
[2] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson,
D.Kumaran,andR.Hadsell. Learningtonavigateincom-
N. Su¨nderhauf, I. Reid, S. Gould, and A. van den Hen-
plexenvironments.InInternationalConferenceonLearning
gel. Vision-and-languagenavigation: Interpretingvisually-
Representations(ICLR),2017. 8
grounded navigation instructions in real environments. In
[16] D.Misra,A.Bennett,V.Blukis,E.Niklasson,M.Shatkhin,
Proceedings of the IEEE Conference on Computer Vision
andY.Artzi. Mappinginstructionstoactionsin3denviron-
andPatternRecognition(CVPR),volume2,2018. 1,2,5,8
mentswithvisualgoalprediction.InProceedingsofthe2018
[3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,
ConferenceonEmpiricalMethodsinNaturalLanguagePro-
C.LawrenceZitnick, andD.Parikh. Vqa: Visualquestion
cessing,2018. 8
answering.InProceedingsoftheIEEEInternationalConfer-
[17] D. Misra, J. Langford, and Y. Artzi. Mapping instructions
enceonComputerVision(CVPR),pages2425–2433,2015.
andvisualobservationstoactionswithreinforcementlearn-
8
ing.Proceedingsofthe2017ConferenceonEmpiricalMeth-
[4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, odsinNaturalLanguageProcessing(EMNLP),2017. 8
N.Hamilton,andG.Hullender. Learningtorankusinggra-
[18] J.Pearl.Heuristics:intelligentsearchstrategiesforcomputer
dientdescent.InProceedingsofthe22ndInternationalCon-
problemsolving. 1984. 8
ferenceonMachineLearning,pages89–96.ACM,2005. 10
[19] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Se-
[5] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Nießner, quenceleveltrainingwithrecurrentneuralnetworks. InIn-
M.Savva, S.Song, A.Zeng, andY.Zhang. Matterport3d: ternationalConferenceonLearningRepresentations(ICLR),
Learningfromrgb-ddatainindoorenvironments. Interna-
2016. 1
tionalConferenceon3DVision(3DV),2017. 5
[20] S.J.RussellandP.Norvig. Artificialintelligence:amodern
[6] D.S.Chaplot,K.M.Sathyendra,R.K.Pasumarthi,D.Ra- approach. Malaysia;PearsonEducationLimited,,2016. 8
jagopal,andR.Salakhutdinov. Gated-attentionarchitectures
[21] I.Sutskever,O.Vinyals,andQ.V.Le.Sequencetosequence
fortask-orientedlanguagegrounding.In32ndAAAIConfer-
learningwithneuralnetworks. InAdvancesinNeuralInfor-
enceonArtificialIntelligence(AAAI),2018. 8
mationProcessingSystems(NIPS),pages3104–3112,2014.
[7] H.Chen,A.Shur,D.Misra,N.Snavely,andY.Artzi.Touch- 1
down: Naturallanguagenavigationandspatialreasoningin
[22] X.Wang,Q.Huang,A.Celikyilmaz,J.Gao,D.Shen,Y.-F.
visualstreetenvironments. InConferenceonComputerVi-
Wang,W.Y.Wang,andL.Zhang. Reinforcedcross-modal
sionandPatternRecognition(CVPR),2019. 8
matching and self-supervised imitation learning for vision-
[8] A.Das,S.Datta,G.Gkioxari,S.Lee,D.Parikh,andD.Ba- languagenavigation.InConferenceonComputerVisionand
tra. Embodied question answering. In Proceedings of the PatternRecognition(CVPR),2019. 8
IEEEConferenceonComputerVisionandPatternRecogni- [23] X.Wang, W.Xiong, H.Wang, andW.Y.Wang. Lookbe-
tion(CVPR),volume5,page6,2018. 8 fore you leap: Bridging model-free and model-based rein-
[9] H.deVries,K.Shuster,D.Batra,D.Parikh,J.Weston,and forcement learning for planned-ahead vision-and-language
D.Kiela. Talkthewalk: Navigatingnewyorkcitythrough navigation. In European Conference on Computer Vision
groundeddialogue. arXivpreprintarXiv:1807.03367,2018. (ECCV),2018. 2,8
8 [24] Y.Zhu,R.Mottaghi,E.Kolve,J.J.Lim,A.Gupta,L.Fei-
[10] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.- Fei, and A. Farhadi. Target-driven visual navigation in in-
P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, doorscenesusingdeepreinforcementlearning. InIEEEIn-
and T. Darrell. Speaker-follower models for vision-and- ternationalConferenceonRoboticsandAutomation(ICRA),
languagenavigation. InNeuralInformationProcessingSys- pages3357–3364.IEEE,2017. 8
tems(NeurIPS),2018. 1,2,5,7,8
[11] D.Gordon,A.Kembhavi,M.Rastegari,J.Redmon,D.Fox,
andA.Farhadi. IQA:Visualquestionansweringininterac-
tiveenvironments.InComputerVisionandPatternRecogni-
tion(CVPR),volume1,2018. 8
[12] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner,
H. Soyer, D. Szepesvari, W. M. Czarnecki, M. Jaderberg,
D.Teplyashin,etal. Groundedlanguagelearninginasimu-
lated3dworld. arXivpreprintarXiv:1706.06551,2017. 8
[13] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher,
and C. Xiong. Self-monitoring navigation agent via aux-
A.SupplementaryMaterial
Our appendix is structured to provide both correspond-
ing qualitative examples for the quantitative results in the
paperandadditionalimplementationdetailsforreplication.
A.1.Qualitativecomparison
FiguresA1throughA3showthreeexamplescomparing
our approach to the previous state-of-the-art. In addition,
thefollowingURLincludesa90secondvideo(https://
youtu.be/AD9TNohXoPA)showingafirst-personview
of several agents navigating the environment with corre-
spondingbirds-eye-viewmaps.
A.2.CandidateReranker
Givenacollectionofcandidatetrajectories,ourreranker
moduleassignsascoretoeachofthetrajectories.Thehigh-
est scoring trajectory is selected for the FAST agent’s next
step. In our implementation, we use a 2-layer MLP as the
reranker. Wetraintheneuralnetworkusingpairwisecross-
entropyloss[4].
As input to the reranker, we concatenate the following
featurestoobtaina6-dimensionalvector:
• Sumofscorelogitsforactionsonthetrajectory.
• Meanofscorelogitsforactionsonthetrajectory.
• Sumoflogprobabilitiesforactionsonthetrajectory.
• Meanoflogprobabilityforactionsonthetrajectory.
• Progressmonitorscoreforthecompletedtrajector.
• Speakerscoreforthecompletedtrajectory.
We feed the 6-dimensional vector through an MLP:
BN → FC → BN → Tanh → FC, where BN is a
layer of Batch Normalization, FC is a Fully
Connectedlayer,andTanhisthenonlinearityused. The
first FC layer transforms the 6-dimensional input vector to
a6-dimensionalhiddenvector.ThesecondFClayerproject
the 6-dimensional vector to a single floating-point value,
whichisusedasthescoreforthegivenpartialtrajectory.
To train the MLP, we cache the candidate queue after
runningFASTfor40steps. Eachcandidatetrajectoryinthe
queue has a corresponding score s . To calculate the loss,
i
weminimizethepairwisecross-entropyloss:
−(s −s )+log(1+exp(s −s ))
1 2 1 2
where s is the score for a qualified candidate and s is
1 2
thescoreforanunqualifiedcandidate. Wedefinequalified
candidate trajectories as those that end within 3 meters of
groundtruthdestination.Inourcachedtrainingset,wehave
4,378,729pairsoftrainingdata.Wetrainusingabatchsize
of 3600, SGD optimizer with a learning rate of 5e−5, and
momentum0.6;Wetrainfor30epochs.
FigureA1.Comparisonofthepreviouslystate-of-the-artSMNAmodel[13]toourFAST NAVIGATORmethod,withthegroundtruthas
reference. NotehowSMNAretracesitsstepsmultipletimesduetothelackofglobalinformation. ThisexampleistakenfromRoom-to-
Room,path2617,instructionset3.Youcanviewavideoofthistrajectoryhere:https://youtu.be/AD9TNohXoPA.
FigureA2.IdenticaltopreviousfigureA1,exceptthatthisexampleistakenfromRoom-to-Room,path15,instructionset1.
FigureA3.IdenticaltopreviousfigureA1,exceptthatthisexampleistakenfromRoom-to-Room,path1759,instructionset1. Thetypo
’direclty’comesfromthedataset.

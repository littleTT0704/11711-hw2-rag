Learn-to-Race: A Multimodal Control Environment for Autonomous Racing
JamesHerman1* JonathanFrancis1,2∗ SiddhaGanju3 BingqingChen1 AnirudhKoul4
AbhinavGupta1 AlexeySkabelkin5 IvanZhukov5 MaxKumskoy5 EricNyberg1
1SchoolofComputerScience,CarnegieMellonUniversity,Pittsburgh,PA,USA
2Human-MachineCollaboration,BoschResearch,Pittsburgh,PA,USA
3 NVIDIA,SantaClara,CA,USA
4 Pinterest,SanFrancisco,CA,USA
5 AutonomousDriving,Arrival,London,UK
{jamesher, jmf1, bingqinc, agupta6, ehn}@cs.cmu.edu, {sganju1, akoul}@alumni.cmu.edu,
{skabelkin, zhukov, kumskoy}@arrival.com
Abstract
Existing research on autonomous driving primarily fo-
cuses on urban driving, which is insufficient for charac-
terising the complex driving behaviour underlying high-
speed racing. At the same time, existing racing simula-
tionframeworksstruggleincapturingrealism,withrespect
to visual rendering, vehicular dynamics, and task objec-
tives, inhibiting the transfer of learning agents to real-
world contexts. We introduce a new environment, where
agentsLearn-to-Race(L2R)insimulatedcompetition-
style racing, using multimodal information—from virtual
cameras to a comprehensive array of inertial measure-
ment sensors. Our environment, which includes a sim-
ulator and an interfacing training framework, accurately
models vehicle dynamics and racing conditions. In this
paper, we release the Arrival simulator for autonomous
racing. Next, we propose the L2R task with challenging
metrics,inspiredbylearning-to-drivechallenges,Formula-
style racing, and multimodal trajectory prediction for au-
tonomousdriving. Additionally,weprovidetheL2Rframe-
work suite, facilitating simulated racing on high-precision
models of real-world tracks. Finally, we provide an offi-
cial L2R task dataset of expert demonstrations, as well as Figure1: Learn-to-Raceinterfaceswitharacingsimulator,
aseriesofbaselineexperimentsandreferenceimplementa- whichfeaturesnumerousreal-worldracetrackssuchastheThrux-
tions. We make all code available: https://github. tonCircuit(top-left)andLasVegasMotorSpeedway(top-right).
com/learn-to-race/l2r. Simulatedracecars(bottom)areempoweredwithlearningagents,
tasked with the challenge of learning to race for the fastest lap-
timesandbestmetrics.
1.Introduction
Progress in the field of autonomous driving relies on tionmetrics,whichenableresearcherstoeffectivelyassess
theexistenceofchallengingtasksandwell-definedevalua- andimprovealgorithms. Modelsdevelopedinlearning-to-
drive settings continue to struggle with issues in sample-
*Equalcontribution. complexity, safety, and unseen generalisation, calling for
1202
guA
81
]OR.sc[
3v57511.3012:viXra
moresuitablebenchmarks[9,16,28]. Wehypothesisethat 2.RelatedWork
high-fidelity simulation environments, together with well-
2.1.ReinforcementLearningEnvironments
defined metrics and evaluation procedures, are conducive
todevelopingmoresophisticatedagents;and,inturn,such
ResearchprogressinthefieldsofReinforcementLearn-
agentswillbebetter-suitedtoreal-worlddeployment.
ing(RL),Planning,andControlhasreliedonvarioussim-
Simulated autonomous racing exhibits task complexity ulationenvironments,forbenchmarkingagentperformance
on several factors: (i) agents must perform real-time deci- ongame-playingandrobotcontroltasks[32,8,33,18,30].
sionmaking,requiringcomputationally-efficientpolicyup- These tasks require sequential decision-making in order
dates as well as robustness to latency; (ii) agents must be to complete objectives and are generally characterised by
abletodealwithrealisticvehicleandenvironmentaldynam- their state dimensionality, the nature of their action space
ics(whereasagentsinless-realisticenvironmentshavebeen (e.g.,discreteorcontinuous),agentcardinality(i.e.,single-
abletoachievesuper-humanperformance);(iii)agentsmust or multi-agent), and by the capability of the underly-
leveragemoreinformativeintrinsicrewardschemesthaten- ing simulator in capturing real-world physical dynamics
ablereplicationofhuman-likedrivingbehaviour,e.g.,trad- [18]. Whereas the vast majority of tasks offered by, e.g.,
ing off safety and performance; and (iv) agents must use the DeepMind Control Suite [32], OpenAI Gym [8], and
offline demonstrations effectively, without overfitting, and the MuJoCo physics engine [33] have been solved—with
must leverage interactions with the environment sample- agentsoftenachievingsuperhumanperformance—noexist-
efficiently. We highlight simulated racing (Figure 1) as an ingenvironmentsfocusonhigh-fidelitysimulationofhigh-
opportunityfordevelopinglearningstrategiesthatarecapa- speeddriving,indynamicallyunstablecontexts.
bleofmeetingthesestringentrequirements.
In this work, we release the Arrival Autonomous Rac- 2.2.SimulationofAutonomousDriving
ingSimulator,whichincludesnumerousinterfacesforboth
Urbandriving. CARLA[14]isanopen-sourcesimulator
simulated and real vehicle instrumentation. Furthermore,
for autonomous driving, wherein various tasks have been
weintroduceLearn-to-Race(L2R),amultimodaland
definedtochallengeagents’street-legalurbandrivingabili-
continuous control environment for training and evaluat-
ties. Duckietown[29,10]providesacustomisableplatform
ing autonomous racing agents. Through the L2R envi-
forurbanautonomousdriving,aswellashardwaresupport
ronment, wesimulatecompetition-styleracetracksthatare
forminiaturevehiclescontrolledviaRaspberryPi’s. Inthis
basedoffreal-worldcounterparts, weprovidemechanisms
paper, we focus primarily on autonomous racing environ-
forfully-characterisingrealisticracingagents(e.g.,flexible
ments, which present challenges outside the conventional
sensor placements, multimodal cameras, and various ve-
scopeofurbanandhighwaydriving.
hicle dynamics profiles), and we provide numerous tools
Track racing. In autonomous racing, agents must make
for fine-grained agent evaluation (e.g., random and fixed
sub-second decisions in regimes of unstable physical dy-
spawn locations, custom racing map construction, and in-
namics,whereintheramificationsofcontrolactionscanbe
jectionofexternaldisturbances). Usingthesefacilities,we
amplifiedorsuppressed,non-linearly,dependingonvehicu-
enable research in problems that require agents to make
larandenvironmentalstate.CarRacing-v0,anOpenAI-gym
safety-critical, sub-second decisions in dynamically unsta-
environment [8], is a simple racing environment, which
ble contexts, such as autonomous racing, real-time uncer-
usesonlybird’s-eye-view(BEV)observations. In[17],re-
taintyanalysisinhighwaydriving, andtrajectoryforecast-
searchers trained agents to race in the video game Gran
ing.Inthispaper,weexemplifyalgorithmdevelopmentand
TurismoSport,buthavenotyetreleasedtheirenvironment.
benchmarkingofmethodsunderlearningfromdemonstra-
Moreover,insteadofusingsensoryperception,agentswere
tions,reinforcementlearning,andmodel-predictivecontrol.
directlyprovidedwithprivilegedinformation,e.g.,distance
Concretely,ourcontributionsinclude:(i)theArrivalAu-
to obstacles and road boundaries. TORCS [3] is an open-
tonomous Racing Simulator, which simulates high-fidelity
source simulator and is used by the Simulated Car Racing
competition-style tracks, vehicles, and various sensor sig-
Championship[27], despiteitsgame-likequalities. Asthe
nals;(ii)theLearn-to-Race(L2R)framework,aplug-
goalofsimulatorsshouldbetoaccuratelymodelthedynam-
and-play environment, which defines interfaces for vari-
icsofthereal-world,weassertthatthepotentialformodel
ous sensor modalities and provides an OpenAI-gym com-
transferfromtheseframeworksremainslimited.
pliant training and testing environment for learning-based
agents; (iii) an official L2R task and dataset with expert
2.3.LearningParadigms
demonstrations, metrics, and reference evaluation proce-
dures; and (iv) an academic release of the simulator, code Wediscussvariouslearningparadigmsthatareenabledby
for the L2R framework, and implementations of baseline thesimulationofautonomousdriving.
agentstofacilitatefullreproducibilityandextension.
Learn-to-Race (L2R)
Racing Simulator Agent
Task Framework
Sensor Models + Placement Camera Tracker + Obs. Policy
Interface Metrics
Signals
IMU Localisation
Environment / Maps
Interface Interface
Actions
Vehicle Building + Config. Commands Action Gym
Interface
Simulator Reward Rewards
API
Control Function
Figure2: Learn-to-Raceallowsagentstointeractwiththeracingsimulatorthroughaseries
ofinterfacesforobservations,actions,andsimulatorcontrol.
Simulation-to-realtransfer. DeepRacer[5],developedby OpenAISafetyGym)lackrealisticdynamicsandtheyeval-
Amazon WebServices, providesan end-to-end framework uateagentsatmuchlowerspeeds; thus,thenumerouslim-
for training and deploying 1/18th-scale autonomous rac- itations of existing methods cannot be studied comprehen-
ing cars. The Indy Autonomous Challenge [1] encourages sively.WeassertthatthephysicalrealismthatL2Rprovides
institutions to create autonomous vehicle technology; par- facilitatesimprovementofthoseunderlyingapproaches.
ticipants are given the proprietary VRXPERIENCE driv-
ing simulator, which focuses more on optimising human- 3.SimulationEnvironment
machine interactions within the vehicle, in the context of
situational highway driving, which contrasts with our fo- 3.1.ArrivalAutonomousRacingSimulator
cus in this work on autonomous racing. Roborace [2] is
The Arrival simulator is a powerful tool for the devel-
thefirstinternationalchampionshipforfull-size,real-world
opmentandtestingofautonomousvehicles. Itisbasedon
autonomousracing. Here, teamsdevelopself-drivingsoft-
Unreal Engine 4 and includes such features as: (i) a ve-
wareandcompeteinchallenges,usingRoborace-ownedve-
hicle prototyping framework; (ii) full software-in-the-loop
hicles. Roboraceprovidesteamswithproprietarysoftware-
(SIL)simulation,tomodelallvehiclecontroldevices; (iii)
in-the-loop (SIL) and hardware-in-the-loop (HIL) simula-
controller area network (CAN) bus interface; (iv) camera,
tors, with a base driving stack. These simulators are pre-
inertialmeasurementunit(IMU),lightdetectionandrang-
dominately used for developing classical control methods,
ing (LiDAR), ultrasonic, and radar sensor models; (v) se-
however,anddonotincludefacilitiesfortraininglearning-
mantic segmentation; (vi) sensor placement and configu-
basedagents[7,22,23,31].Theauthorsobtainedapodium
ration facilities; (vii) V2V/V2I interface subsystem; (vii)
finishattheThruxtonCircuit(UK),intheSeasonBetaRob-
dynamic racing scenario creation; (viii) race track genera-
oracecompetitions(2020-2021),andwenowwishtoenable
tionfromscanneddatasets;(ix)supportforfullintegration
new technologies through open-sourcing our autonomous
withtheCARLAsimulator[14];and(x)anapplicationpro-
racingresearch: toourknowledge, wepubliclyreleasethe
gramminginterface(API),whichisautomaticallygenerated
firstenvironmentthatisspecificallyintendedforsimulating
basedonC++codeanalysis. Detailsinsupplementary.
autonomouscompetition-styletrackracingandfortransfer-
ringlearning-basedagentstotherealworld.
3.2.Learn-to-RaceEnvironment
Safe and efficient learning. Imposing safety constraints
in, e.g., RLalgorithms, hasbecomepopularforthepoten- Learn-to-Race(L2R)isamultimodalcontrolenvi-
tial of reducing failures in simulation-to-real transfer set- ronmentthatprovidesaseriesofinterfacesforanagentto
tings and for enabling agent robustness to environmental interactwiththeracingsimulator,includingthecapabilities
stochasticity [20]. The goal is to embed safety guaran- tosendcontrolcommandsandmakeobservationsoftheen-
tees in policies, without compromising their performance vironmentandvehiclestateviadifferentsensors(Figure2).
orsample-efficiency. Whileafewworksconsiderdetection L2R is implemented as a Gym environment [8], enabling
and avoidance of unsafe states, in urban driving [13] and quickprototypingofcontrolpolicies. Whilewereleasethe
human-assistive robotics [19], no existing works focus on L2Renvironmentandtask(Section4)alongsidetheArrival
safelearningandcontrol,forautonomousracingindynami- simulator, we note that other simulators may be used with
callyunstablecontexts. PopularSafe-RLbenchmarks(e.g., L2R,includingthoseprovidedby[2].
Agent-SimulatorInteraction. Ateachstept,anagentse- 4.1.TaskOverview
lects an action a , based on its current observation s , us-
t t
L2R provides an OpenAI Gym [8] compliant learn-
ing its policy π : a ∼ π (·|s ). The control action from
θ t θ t
ing environment, where researchers could flexibly select
theagentisforwardedtothesimulatorasaUDPmessage.
among the available sensor modalities. This early version
L2Rreceivesupdatesfromthesimulator,i.e.,imagesfrom
of the environment enables single-agent racing on three
thevirtualcameraand/ormeasurementsfromothervehicle
racetracks (with custom track construction facility), mod-
sensors, through TCP and UDP socket connections. As in
eled after their real-world counterparts. Included tracks
reality,updatefrequenciesacrossthevarioussensormodal-
aretheThruxtonCircuit(Track01:Thruxton)andAn-
ities are not equal, so L2R synchronizes observations by
glesey National Circuit (Track02:Anglesey) from the
providing agents with the most recent data from each (Al-
United Kingdom, and the North Road Track at Las Ve-
gorithm 1). The step method of the environment returns
gas Motor Speedway (Track03:Vegas), located in the
the new observation s , along with a calculated reward
t+1
United States. Analogous to having separate town maps
to the agent, r = R(s ,a ,s ), and a Boolean terminal
t t t t+1
for training and testing in other simulation environments,
state flag. The reward function and evaluation metrics are
e.g., CARLA [14], we use Track01 and Track02 for
definedinSection4.3.
trainingandTrack03fortesting. Consequently, wegen-
erateexperttracesfromthetrainingtracks,forinclusionin
Algorithm1Agent-SimulatorInteraction
ourinitialdatasetrelease(seeSection4.2). Manyavenues
1: functionSENSORTHREAD forresearchcanbeexploredwithinL2R,includingvarious
2: data←Initialvalue learning paradigms, such as: (constrained) reinforcement
3: functionGETDATA learning,learningfromdemonstrations,multitasklearning,
4: returndata transferlearninganddomainadaptation,simulation-to-real
5: whilenotterminateddo transfer,fastdecision-making,classical/neuralhybridmod-
6: data←ReceiveData eling,etc. Regardlessofthemethodchosen,agents’multi-
7: modalperceptioncapabilities—i.e.,theirabilitytofuseand
8: functionSTEP(a t) alignsensoryinformation—areofcriticalimportance.
9: Senda tasUDPmessage
10: s t+1 ←GetData∀SensorThreads 4.2.Learn-to-RaceDataset
11: r t ←R(s t,a t,s t+1)
12: done←IsTerminal(s t,s t+1) Wegeneratearich,multimodaldatasetofexpertdemon-
13: returns t+1, r t, done strations from training tracks, Track01:Thruxton and
Track02:Anglesey, in order to facilitate pre-training
ofagentsvia,e.g.,imitationlearning(IL).TheL2Rdataset
Episodiccontrol.Thecontrolinterfacecommunicateswith contains multi-sensory input at a 100-millisecond resolu-
the simulator to automatically setup and execute simula- tion,inboththeobservationandactionspaces. Depending
tions in an episodic manner. L2R conveniently allows for ontheselectedsimulatorperceptionmode,agentshaveac-
training to be launched in one command, as all aspects of cesstoone(vision-onlymode)orallmodalities(multimodal
theracingsimulatorandlearningenvironmentareparame- mode). SeeTable1foracompletelistofavailablemodal-
terised.Astateisconsideredterminalifalllapsaresuccess- ities. The actionspace is defined by continuous values for
fullycompleted,ifatleast2ofthevehicle’swheelsgoout- acceleration and steering, in the ranges [−1.0,1.0], where
sideofthedrivablearea,orifprogressisminimallyinsuffi- negativeaccelerationvalueswilldeceleratethevehicletoa
cient.Theepisodebeginsbyresettingthevehicletoastand- haltedposition. NotethatGearisacontrollableaction,but
ing start position, at a parameterised location along with fixedtodriveinallourexperiments.
configuredsensorinterfacesandinitialisedrewardfunction.
Theexpertdemonstrationswerecollectedusingamodel
Discretestepsaretakenbytheagentuntiloneoftheafore-
predictivecontroller(MPC)(Section5),whichfollowsthe
mentionedepisodeterminationcriteriaismet.
centerline of the race track at a pre-specified reference
speed.Thisfirstversionofthedatasetcontains10,600sam-
4.Task: Learn-to-Race ples of each sensor and action dimension, for 9 complete
lapsaroundeachtrack. FutureversionreleasesofL2Rwill
TheLearn-to-Race(L2R)tasktestsanagent’sabil- includeaccesstonewsimulatedtracks(modeledafterother
itytoexecutetherequisitebehavioursforcompetition-style realtracks, aroundtheworld)aswellasexperttracesgen-
track racing, through multimodal perceptual input. In this eratedfromtheseadditionaltracks—acrossvariousweather
section,weprovideataskoverviewanddescribetaskprop- conditions, in challenging multi-agent settings, and within
erties,datasetcharacteristics,andmetrics. dangerousobstacle-avoidancescenarios.
Table 1: Summaryoftheobservationandcontinuousactionspaces, fortheLearn-to-Racetask. When
thesimulatorisinitialisedinvision-onlymode,theobservationspaceconsistsofjusttheimagesfromtheego-
vehicle’sfront-facingcamera. Theadditionalobservationdata,allofwhichisrealisticallyaccessibleonareal
racingcar,isavailableinmultimodalmode. *Whereasgearispermittedasacontrollableparameter,wedonot
useitinourexperiments.
Signal Description Dimension
Acceleration Commandin[-1.0,1.0] R1
Action
Steering Commandin[-1.0,1.0] R1
Gear {park,drive,neutral,reverse} —
Image RGBimage RW×H×3
Steering Observedsteeringdirection R1
Gear {park,drive,neutral,reverse} —
Mode Vehiclemode R1
Velocity InENUcoordinate(m/s) R3
Acceleration InENUcoordinate(m/s2) R3
Observation
Yaw,Pitch,Roll Orientationofthecar(rad) R3
AngularVelocity Rateofchangeoftheorientation(rad/s) R3
Location LocationofthevehiclecenterinENU(m) R3
WheelRotationalSpeed perwheel(RPM) R4
Braking Brakepressureperwheel(Pa) R4
Torque perwheel(Nm˙) R4
4.3.TaskMetrics mentsforenvironmentalfactors,suchaswheelslippageand
weathereffectsasthetaskmatures. AverageDisplacement
TheprimaryobjectiveoftheL2Rtaskistominimisethe Error (ADE), a common metric in trajectory forecasting
timetakenforanagenttosuccessfullycompleteracinglaps, [28], measures the agent’s average deviation from a refer-
withadditionalrequirementsontheagent’sdrivingquality. encepath—inthiscase,thecenterlineofthetrack. Trajec-
We do not restrict the agent’s learning paradigms to, e.g., toryAdmissibility(TrA)isthedimensionlessmetricα,de-
ILorRL;onthecontrary,wecanenvisionawealthofcom- finedinEquation1,wheret isthedurationoftheepisode
e
binationstrategiesandothermethodsthatareapplicableto and t is the cumulative time spent driving unsafely with
u
thetask.Whilewedonotincludeplanning-onlyapproaches exactlyonewheeloutsideofthedrivablearea.
asthosethatareconsistentwiththeofficialL2Rtask,(i)we
(cid:114)
t
do encourage hybrid or model-based learning approaches; α=1− u (1)
t
furthermore, (ii) we do encourage the simulator and the e
L2Rinterfacetobeusedtofurtherresearchintheseareas, We also utilise metrics that measure the smoothness of
more generally. Agnostic to the learning paradigm used, agent behaviour: Trajectory Efficiency (TrE) measures the
and inspired by concepts from high-speed driving, robot ratiooftrackcurvaturetoagenttrajectorycurvature,i.e.,in
navigation [18], and trajectory forecasting [28], we define terms of agent heading deviations; Movement Smoothness
the core modalities, metrics, and objectives that shall be (MS)quantifiesthesmoothnessoftheagent’sacceleration
usedtotrainL2Ragentsandassesstheirperformance. We profile, adjusted for gravity, using the negated log dimen-
summariseagents’actionandobservationspacesinTable1 sionlessjerk,η inEquation2,inspiredby[6]:
ldj
and Wth ee do ef fifi nc eial thL e2 sR ut ca cs ek ssm fue ltr cic os min plT ea tib ol ne2 o. f an episode in η ldj =ln(cid:32) (t 2 v− 2 t 1)3 (cid:90) t2(cid:12) (cid:12) (cid:12) (cid:12)d d2 t2v(cid:12) (cid:12) (cid:12) (cid:12)2 dt(cid:33) (2)
the L2R task to be 3 completed laps, from a standing peak t1
start; Episode Completion Percentage (ECP) measures the Rather than restricting agents to predefined incentive poli-
amount of the episode completed, and Episode Duration cies, input dimensions, or even input modalities, L2R al-
(ED)measurestheminimumamountoftimethattheagent lows and encourages flexibility so that agents can learn to
tooktoprogresstoitsfurthestextent, throughtheepisode. raceeffectively. ThedefaultrewardfunctionforL2Risin-
WedefineAverageAdjustedTrackSpeed(AATS)asamet- spiredby[17]: thispolicyprovidesdenserewardsforpro-
ricthatmeasurestheaveragespeedoftheagent,acrossall gression, consistentwiththegoalofminimisinglaptimes,
threelapsoftheepisode. Metricsmayalsoincludeadjust- andnegativerewardsforgoingout-of-bounds.
4.4.TaskEvaluationProcedure of L2R via classical control approaches. The MPC mini-
mizesthetrackingerrorwithrespecttothecenterlineofthe
Agent assessment is conducted through a leaderboard
racetrackatapre-specifiedreferencespeed. Weusetheit-
competition, with two distinct stages: (1) pre-evaluation
erative linear quadratic regulator (iLQR) proposed in [26],
and (2) evaluation. Predicated on industry standards, we
which iteratively linearizes the non-linear dynamics along
adopt a racing-centric pre-evaluation step, for assessing
the current estimate of trajectory, solves a linear quadratic
agent performance, giving agents a warm start on the test
regulatorproblembasedonthelinearizeddynamics,andre-
trackbeforeformalevaluation. Muchlikehowhumanrac-
peats the process until convergence. Specifically, we used
ingdriversarepermittedtoacquaintthemselveswithanew
theimplementationforiLQRfrom[4]. Weadoptthekine-
racing track, before competition, we run a pre-evaluation
maticbikemodel[25]tocharacterizethevehicledynamics.
onmodels,withunfrozenweights,allowingforsomeinitial
FurtherMPCdetailsareprovidedinthesupplementary.
(albeit constrained) exploration. In this pre-evaluation pe-
While MPC implies optimal control performance, we
riod,agentsmayexploretheenvironmentforafixedtimeof
wanttopointoutthelimitationsofourcurrentimplementa-
60minutes,definedinthenumberoftime-stepsofdiscrete
tion. Firstly, the ground truth vehicle parameters were not
observationfromtheL2Rframework.Inthepre-evaluation,
known to us and we used estimated values. Secondly, we
we further define a “competency check” that agents must
askedtheMPCtofollowthecenterlineofthetrack,which
pass, in order to successfully proceed through to the main
isnotthetrajectoryexpertdriverswouldhavetaken, espe-
evaluation phase. For the North Road track at Las Ve-
cially when cornering. Finally, we pre-specified the MPC
gas Motor Speedway, the only competency check is that
todriveataconservativespeed(12.5m/s),whichmakesthe
agents are able to successfully complete a lap during the
expertdemonstrationseasiertolearnfrom.
pre-evaluation period with acceleration capped at 50% of
Conditional Imitation Learning. We adopted the same
maximumallowedintheactionspace.Asuccessfulepisode
neural architecture from [11], except that we do not have
isdefinedthecompletionof3lapsfromastandingstartand
different commands in our case, e.g., turn left, turn right,
the agent not going out of the driveable areas of the track.
go straight, and stop. Thus, we used a single branch for
If the agent is unsuccessful in the pre-evaluation phase, it
decoding actions. We assume both front view images and
is disqualified and not evaluated further. As we continue
sensormeasurementsareavailablefortheILagent. Ineach
toprovidesupportfornewtracks(necessitatingmorenovel
sample, the input consists of a 512 × 384 image and 30
driving maneuvers), we will also continue to add and per-
sensor measurements, and output is 2 actions (as listed in
mute the driving competency checks, to maintain fairness
Table1). TheimplementationofCILautomaticallyadjusts
ofevaluationonthosetracks.
the neural network architecture based on specified input-
Postasuccessfulpre-evaluationstage,thefinalteststage
output dimensions. The imitation loss (Equation 3) is the
occurs: agents are provided all the various input modal-
mean squared error between the predicted action, aˆ , and
t
ities and have to compete on the metrics defined Section
theactiontakenbytheexpert,a .
t
4.3. When the agent successfully passes through the pre-
evaluationstage,theuserisnotprovidedwiththeresultsof n
(cid:88)
the competency checks and instead is able to view the re- L= ||aˆ i−a i||2 2 (3)
sultsofthecompleteevaluationdirectlyontheleaderboard. i=1
SoftActor-Critic. Weprovideareferenceimplementation
5.BaselineAgents ofSoft-ActorCritic(SAC)[12,21],whichisgenerallyper-
formant and known to be robust [15]. SAC belongs to the
Wedefineaseriesoflearning-free(e.g.,RANDOM,MPC) family of maximum entropy reinforcement learning (RL)
and learning-based (e.g., reinforcement learning, imitation algorithms, wherein an agent maximizes expected return,
learning)baselines,toillustratetheperformanceofvarious subjecttoanentropyregularizationterm(Equation4),asa
algorithmic classes on the L2R task. We also benchmark principledwaytotrade-offexplorationandexploitation.
humanperformance,throughaseriesofexperttrials.
T
(cid:88)
J(θ)= E [R(s ,a )−H(π (a |s ))] (4)
Random. TheRANDOMagentismainlyintendedasasim- πθ t t θ t t
ple demonstration of how to interface with the L2R envi- t=1
ronment. TheRANDOMagentisspawnedatthestartofthe OurRL-SACagentdemonstratesseveraloffeaturesinthe
track, anduniformlysamplesactions, i.e., steeringandac- environment: it operates in vision-only mode, but rather
celeration,fromtheactionspace. Theagentthenproceeds than learning directly from pixels, we pre-trained a con-
toexecutetheserandomactions. volutional, variational auto-encoder [24] made on sample
MPC. The MPC was used to generate expert demonstra- cameraimages. Therefore,ouragentonlyneedtolearnsto
tions (Section 4.2) and is intended as a reference solution decodeactionsfromimageembeddingsusingamulti-layer
Table 2: Learn-to-Race defines multiple metrics for the assessment of agent performance. These metrics
measure overall success—e.g., whether and how fast the task is completed—along with more specific properties,
suchastrajectoryadmissibilityandsmoothness.
Metric Definition
EpisodeCompletionPercentage Percentageofthe3-lapepisodecompleted
EpisodeDuration Durationoftheepisode,inseconds
AverageAdjustedTrackSpeed Averagespeed,acrossallthreelaps,adjustedforenvironmentalconditions,inkm/h
AverageDisplacementError Euclideandisplacementfrom(unobserved)trackcenterline,inmeters
TrajectoryAdmissibility Complementofthesquarerootoftheproportionofcumulativetimespentunsafe
TrajectoryEfficiency Ratiooftrackcurvaturetotrajectorycurvature(i.e.,inagentheading)
MovementSmoothness Logdimensionlessjerkbasedonaccelerometerdata,adjustedforgravity
Table 3: Baseline agent results on Learn-to-Race task while training on Thruxton track, with respect
tothetaskmetricsinTable2: EpisodeCompletionPercentage(ECP),EpisodeDuration(ED),AverageAd-
justedTrackSpeed(AATS),AverageDisplacementError(ADE),TrajectoryAdmissibility(TrA),Trajectory
Efficiency (TrE), and Movement Smoothness (MS). Arrows (↑↓) indicate directions of better performance.
Asterisks(*)inTables3and4indicatemetricswhichmaybemisleading,forincompleteracingepisodes.
Agent ECP(↑) ED(↓) AATS(↑) ADE(↓) TrA(↑) TrE(↑) MS(↑)
HUMAN 100.0(±0.0) 235.8(±1.7) 171.2(±3.4) 2.4(±0.1) 0.93(±0.01) 1.00(±0.02) 11.7(±0.1)
RANDOM 0.5(±0.3) 14.0(±5.5) 11.9(±3.8) 1.5(±0.6) 0.81(±0.04) 0.33(±0.38)∗ 6.7(±1.1)
MPC 100.0(±0.0) 904.2(±0.7) 45.1(±0.0) 0.9(±0.1) 0.98(±0.01) 0.85(±0.03) 10.4(±0.6)
RL-SAC 31.1(±0.0) 251.2(±1.4) 50.5(±0.3) 0.5(±0.0) 0.97(±0.0) 0.48(±0.0)∗ 11.1(±0.4)
Table4: BaselineagentresultsonLearn-to-RacetaskwhiletestingonLasVegastrack.
Agent ECP(↑) ED(↓) AATS(↑) ADE(↓) TrA(↑) TrE(↑) MS(↑)
HUMAN 100.0(±0.0) 176.2(±3.4) 114.2(±2.3) 1.7(±0.1) 0.88(±0.01) 1.09(±0.02) 10.1(±0.3)
RANDOM 1.0(±0.6) 21.9(±9.6) 9.2(±1.5) 1.4(±0.3) 0.74(±0.01) 0.18(±0.05)∗ 8.4(±1.0)
MPC 69.5(±10.7) 353.2(±54.8) 40.5(±0.1) 0.8(±0.1) 0.91(±0.02) 1.07(±0.01)∗ 10.4(±0.2)
RL-SAC 11.8(±0.1) 109.9(±7.5) 22.1(±1.5) 1.3(±0.1) 0.95(±0.01) 0.58(±0.01)∗ 9.9(±0.2)
perceptronwithtwohiddenlayersof64hiddenunitseach. 6.ExperimentsandResults
Ouragent’srewardfunctionwastheenvironment’sdefault
We evaluate each of the baseline agents—HUMAN,
withtheinclusionofabonusiftheagentremainednearthe
RANDOM, MPC, and RL-SAC—on the L2R task, with the
centerofthetrack.
objective of finishing 3 consecutive laps in minimal time.
Human. We additionally establish a HUMAN performance For all approaches, agents complete model training and
baseline, by collecting simulated racing results from hu- tuning on Track01:Thruxton. We present the aver-
man expert players. The collection procedure involved a age of each metric across 3 consecutive episodes, in Ta-
privatecrowd-sourcingevent,whichwassplitintotwosep- ble3. Afterwards,agentsareevaluatedbasedontheirper-
arate phases—practice/training and recording/testing. Ex- formanceonTrack03:Vegas,followingthe1-hourpre-
pertplayerswerealreadyfamiliarwiththesimulator, task, evaluation period described in Section 4.4. Learning-free
and objective, prior to engaging in the event. In the train- agents,RANDOMandMPC,simplyperforminferenceinthe
ing phase, players were instructed to engage in the race, testingenvironment. TheRL-SACagent,alearning-based
until the variance in finished lap-times, for three consecu- approach,operatesinvision-onlymodeandutilizesthepre-
tiveruns,fellbelowacertainthreshold. Afterthistraining evaluation stage to perform simple transfer learning to the
phase,playerswereallowedtoproceedtothetestingphase, new racetrack. The agent’s image encoder does not have
forwhichtheirtop-3lapswererecorded. Weaveragedthe accesstothetesttrackpriortopre-evaluationandisnotup-
top-3resultsinthetestingphase,fromallexperts,foreach datedduringthisphase,butthemodelweightsoftheagent
track;thetrainingresultswerediscarded. doupdateasnewexperiencebecomesavailable. Following
the pre-evaluation phase, agents completed 3 consecutive will effectively assess models, based on their general un-
episodes,andwepresentmetricaveragesinTable4. derstandingofvehicledynamics, high-speedandhigh-risk
Humanexperts. Humanexpertsstronglyoutperformdur- control,racetrackperception,andintelligentracingtactics.
ing both training and testing, suggesting a general under-
Tochallengestate-of-the-artlearningapproaches,which
standing of racing: they can quickly adapt to a new track,
continue to demonstrate superhuman performance in sim-
despite different features, including frequent and severe
plisticenvironments,webelievethatthedirectionoffuture
turns. Human experts fully complete 3 lap episodes at
tasksmustbetowardshighercomplexityandrealism. Our
speedsnearthevehicle’sphysicallimitsandestimatetheir
racingsimulatorhasbeenusedasaprimarymodelingtool
lap-timeperformancetobewithin10%ofoptimal. Weex-
forautonomousagentswhichhavedemonstratedreal-world
pectstrongagentstoexecutetrajectorieswhichareoflower
racingspeedsinexcessof200km/h,anorderofmagnitude
curvaturethantheracetrack’scenterline,oraTrEofatleast
faster, and more complex, than comparable environments.
1.0, allowing the vehicle to maintain higher AATS. Only
Limitations of our simulation environment with respect to
human experts were able to achieve this, considering that
competingsimulatorsincludemulti-agentracinganda(cur-
failuretocompleteanepisodedistortsthemetric. However,
rently)limitedsupplyoftracks—however,bothmulti-agent
suchtrajectoriesareaggressiveandrisky, becausetheyof-
racing and additional tracks will follow. Future enhance-
ten involve cutting corners with the wheels nearly outside
mentsalsoincludeadditionalvehiclesensors, domainran-
ofthedriveablearea;thisisapparentbyhigherADEvalues
domization,andsupportfordistributedtraininginlearning-
and relatively low TrA. Additionally, human experts per-
basedapproaches. Webelievetheseenhancementsserveas
formed well relative to other agents terms of MS, demon-
aprecursortoreal-worldtransferandsafetylearning.
strating the ability to anticipate the need for acceleration
andtoapplysmoothcontrol.
Baselineagents. Thereareseveralnotableconclusionsthat 8.Conclusion
we make based on the performance of our baseline agents
which we do not claim to be state-of-the-art. The first is We have presented: (i) a high-fidelity simulator for the
that the task is indeed challenging, as even the MPC agent development and testing of autonomous race cars, (ii) the
withanapproximatecarmodelfailedtoconsistentlycom- Learn-to-Race environment which enables rapid pro-
pletelapsonthetesttrack. Evenafterover1millionsteps totyping, training, and testing in this simulated environ-
environmentstepsonthetrainingtrack,theRL-SACagent ment, and (iii) the L2R task which defined dataset char-
only completes about 90% of a lap due to the challenging acteristics and concrete driving-inspired metrics for eval-
speed trap near the finish line at Thruxton. However, the uation. L2R addresses the lack of complex learning en-
RL-SACagentdemonstratesbettercontrolthantheMPCin vironments and introduces the challenging task of simu-
traininginbothADEandMS.Second,wenotethelackof lated,high-performanceracing. Whilehumanexpertshave
generalization and poor sample efficiency of the RL-SAC demonstrated strong results on this task, both using the
agentwhoseperformancedroppedsignificantlyintermsof L2R framework as well as in competition racing, learning
ability to progress down the track, ECP, and stay near the agentshavenot. Wehaveprovidedrelevantracingmetrics
centerline, ADE, despite being directly incentivised to do andbaselineresultsforclassicalcontrol,RL,andILagents
so.Theagentlearnstosimplystopaltogethertoavoidgoing aswellashumanexperts,andwearereleasingreferenceim-
out-of-boundsabout1/3rdofthewayaroundthetesttrack. plementationsandmodelcheckpointstofurtheradvancethe
We note that imitation learning has potential for providing research. TheL2Rsuiteoftasksandmetricswillcontinue
agentswithstrongpriors. However,inourexperiments,au- toexpandinthefutureincludingtheintroductionofmulti-
tomatic network sizing based on input/output dimensions agentracing. Wehopetosomedayseeagentsreachsuper-
andstep-wisesupervisionalone,suggestedby[14],didnot human,real-worldperformanceinautonomousracing.
yield good performance. This demonstrates the challenge
that L2R poses to this family of approaches, necessitating
Acknowledgements
considerationof,e.g.,jointIL/RLstrategies.
Therearemanypeoplethathelpedcreatethistaskenvi-
7.Discussion
ronment. We are grateful to the RoboRace community, in
We are confident that agents can achieve superhuman particular,theHiveteamwhichhasdevelopedthesimulated
performanceforanygiventrackgiventhat(1)theyaresuf- racetrackmaps. WethankdevelopersBohuiFang,Ignacio
ficientlycomplexand(2)thattheyhaveinteractedwiththat MaronnaMusetti,JikaiLu,ZihangZhang,andXinnanDu
environment enough times. What is not clear, is how well fortheirsupport. WealsothankMaximIntegratedProducts
agentscangeneralizetonewracetracksinarealisticsimula- forsupportingourefforts.Thisworkwassupported,inpart,
tion environment. We believe the Learn-to-Race task byafellowshipfromBoschResearchPittsburgh.
References [13] Wenhao Ding, Baiming Chen, Bo Li, Kim Ji Eun,
and Ding Zhao. Multimodal safety-critical scenarios
[1] Indy autonomous challenge. https://www.
generationfordecision-makingalgorithmsevaluation.
indyautonomouschallenge.com/. Last ac-
IEEE Robotics and Automation Letters, 6(2):1551–
cessed: 2021-01-30.
1558,2021.
[2] Roborace. https://roborace.com/. Last ac-
[14] Alexey Dosovitskiy, German Ros, Felipe Codevilla,
cessed: 2021-01-30.
Antonio Lopez, and Vladlen Koltun. CARLA: An
[3] Torcs, the open racing car simulator. http: open urban driving simulator. In Proceedings of the
//torcs.sourceforge.net/index.php? 1st Annual Conference on Robot Learning, pages 1–
name=Sections&op=viewarticle&artid= 16,2017.
19. Lastaccessed: 2021-01-30. [15] Benjamin Eysenbach and Sergey Levine. Maximum
[4] Brandon Amos, Ivan Dario Jimenez Rodriguez, Ja- entropyrl(provably)solvessomerobustrlproblems.
cobSacks,ByronBoots,andJZicoKolter. Differen- arXivpreprintarXiv:2103.06257,2021.
tiablempcforend-to-endplanningandcontrol. arXiv [16] Angelos Filos, Panagiotis Tigas, Rowan McAllister,
preprintarXiv:1810.13400,2018. Nicholas Rhinehart, Sergey Levine, and Yarin Gal.
Can autonomous vehicles identify, recover from, and
[5] B. Balaji, S. Mallya, S. Genc, S. Gupta, L. Dirac,
adapt to distributionshifts? In International Confer-
V. Khare, G. Roy, T. Sun, Y. Tao, B. Townsend, E.
enceonMachineLearning(ICML),2020.
Calleja,S.Muralidhara,andD.Karuppasamy. Deep-
racer: Autonomous racing platform for experimenta- [17] F.Florian,S.Yunlong,E.Kaufmann,D.Scaramuzza,
tion with sim2real reinforcement learning. In 2020 and P. Duerr. Super-human performance in gran tur-
IEEE International Conference on Robotics and Au- ismosportusingdeepreinforcementlearning,2020.
tomation(ICRA),pages2746–2754,2020. [18] Jonathan Francis, Nariaki Kitamura, Felix Labelle,
[6] S. Balasubramanian, A. Melendez-Calderon, and E. XiaopengLu,IngridNavarro,andJeanOh.Corechal-
Burdet. A robust and sensitive metric for quantify- lenges in embodied vision-language planning. arXiv
ing movement smoothness. IEEE Transactions on preprintarXiv:2106.13948,2021.
BiomedicalEngineering,59(8):2126–2136,2012. [19] David Fridovich-Keil, Andrea Bajcsy, Jaime F Fisac,
[7] J. Betz, A. Wischnewski, A. Heilmeier, F. Nobis, SylviaLHerbert, StevenWang, AncaDDragan, and
T. Stahl, L. Hermansdorfer, and M. Lienkamp. A ClaireJTomlin. Confidence-awaremotionprediction
software architecture for an autonomous racecar. In for real-time collision avoidance1. The International
2019 IEEE 89th Vehicular Technology Conference JournalofRoboticsResearch,39(2-3):250–265,2020.
(VTC2019-Spring),pages1–6,2019. [20] JavierGarcıaandFernandoFerna´ndez.Acomprehen-
sivesurveyonsafereinforcementlearning. Journalof
[8] G.Brockman,V.Cheung,L.Pettersson,J.Schneider,
MachineLearningResearch,16(1):1437–1480,2015.
J.Schulman,J.Tang,andW.Zaremba. Openaigym,
2016. [21] T.Haarnoja,A.Zhou,P.Abbeel,andS.Levine. Soft
actor-critic: Off-policy maximum entropy deep rein-
[9] DianChen,BradyZhou,VladlenKoltun,andPhilipp
forcementlearningwithastochasticactor. InJennifer
Kra¨henbu¨hl. Learningbycheating. InConferenceon
Dy and Andreas Krause, editors, Proceedings of the
RobotLearning,pages66–75.PMLR,2020.
35th International Conference on Machine Learning,
[10] M.Chevalier-Boisvert,F.Golemo,Y.Cao,B.Mehta, volume 80 of Proceedings of Machine Learning Re-
and L. Paull. Duckietown environments for openai search,pages1861–1870,Stockholmsma¨ssan,Stock-
gym. https://github.com/duckietown/ holmSweden,10–15Jul2018.PMLR.
gym-duckietown,2018.
[22] T.Herrmann,F.Passigato,J.Betz,andM.Lienkamp.
[11] Felipe Codevilla, Matthias M”uller, Antonio L’opez, Minimum race-time planning-strategy for an au-
VladlenKoltun,andAlexeyDosovitskiy. End-to-end tonomous electric racecar. 2020 IEEE 23rd Interna-
driving via conditional imitation learning. In 2018 tional Conference on Intelligent Transportation Sys-
IEEE International Conference on Robotics and Au- tems(ITSC),Sep2020.
tomation(ICRA),pages4693–4700.IEEE,2018.
[23] T. Herrmann, A. Wischnewski, L. Hermansdorfer, J.
[12] P.Dhariwal,C.Hesse,O.Klimov,A.Nichol,M.Plap- Betz, and M. Lienkamp. Real-time adaptive velocity
pert,A.Radford,J.Schulman,S.Sidor,Y.Wu,andP. optimizationforautonomouselectriccarsatthelimits
Zhokhov. Openai baselines. https://github. of handling. IEEE Transactions on Intelligent Vehi-
com/openai/baselines,2017. cles,pages1–1,2020.
[24] D. P. Kingma and M. Welling. Auto-Encoding Vari-
ational Bayes. In 2nd International Conference on
Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Pro-
ceedings,2014.
[25] Jason Kong, Mark Pfeiffer, Georg Schildbach, and
Francesco Borrelli. Kinematic and dynamic vehicle
models for autonomous driving control design. In
2015IEEEIntelligentVehiclesSymposium(IV),pages
1094–1099.IEEE,2015.
[26] Weiwei Li and Emanuel Todorov. Iterative linear
quadratic regulator design for nonlinear biological
movement systems. In ICINCO (1), pages 222–229.
Citeseer,2004.
[27] D. Loiacono, L. Cardamone, and P. L. Lanzi. Simu-
latedcarracingchampionship: Competitionsoftware
manual. arXivpreprintarXiv:1304.1672,2013.
[28] Seong Hyeon Park, Gyubok Lee, Manoj Bhat, Jimin
Seo,MinseokKang,JonathanFrancis,AshwinRJad-
hav,PaulPuLiang,andLouis-PhilippeMorency. Di-
verse and admissible trajectory forecasting through
multimodalcontextunderstanding. InEuropeanCon-
ferenceonComputerVision(ECCV),2020.
[29] L. Paull, J. Tani, H. Ahn, J. Alonso-Mora, L. Car-
lone,M.Cap,Y.F.Chen,C.Choi,J.Dusek,Y.Fang,
D. Hoehener, S. Liu, M. Novitzky, I. F. Okuyama, J.
Pazis, G. Rosman, V. Varricchio, H. Wang, D. Yer-
shov, H. Zhao, M. Benjamin, C. Carr, M. Zuber, S.
Karaman,E.Frazzoli,D.DelVecchio,D.Rus,J.How,
J.Leonard, andA.Censi. Duckietown: Anopen, in-
expensive and flexible platform for autonomy educa-
tion and research. In 2017 IEEE International Con-
ference on Robotics and Automation (ICRA), pages
1497–1504,2017.
[30] K. Shao, Z. Tang, Y. Zhu, N. Li, and D. Zhao. A
surveyofdeepreinforcementlearninginvideogames,
2019.
[31] T.Stahl,A.Wischnewski,J.Betz,andM.Lienkamp.
Multilayer graph-based trajectory planning for race
vehicles in dynamic scenarios. In 2019 IEEE In-
telligent Transportation Systems Conference (ITSC),
pages3149–3154,2019.
[32] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. de
Las Casas, D. Budden, A. Abdolmaleki, J. Merel, A.
Lefrancq, T.Lillicrap, andM.Riedmiller. Deepmind
controlsuite,2018.
[33] E.Todorov,T.Erez,andY.Tassa. Mujoco: Aphysics
engineformodel-basedcontrol.In2012IEEE/RSJIn-
ternationalConferenceonIntelligentRobotsandSys-
tems,pages5026–5033,2012.
A.RacingSimulatorDetails A.1.3 Agent-to-SimulatorCommunication
A.1.ClientandServerInformationExchange Agentscancommunicateracingactionstothesimulatorin
avarietyofways:
The agents and the racing simulator act together as a
client-server system. The racing simulator includes both a
• Viaakeyboardorjoystickforhumandrivers
physics and graphics engine and provides numerous com-
municationsmechanismsforavarietyofusecases. Figure • UDP packets with steering, acceleration, and gear re-
3summarisesthesimulatorsystemarchitecture. quests
• Through a safety layer with longitudinal acceleration
A.1.1 SimulatorState
andcurvaturerequests
Managementofthesimulator’sstateisdonethroughaweb-
socketinterface,allowingfortwo-waycommunicationand • Via various API modes which allow for more granu-
forclientstoupdatethestateofthesimulatorincludingthe lar control of the vehicle including individual motor
abilityto: torquesandbrakepressurerequests
• Changethemap Consistent with the simulator-to-agent above, agent-to-
simulatorcommunicationcanbedoneovervirtualorphys-
• Changethetypeofvehicle
icalCANbuses.
• Changetheposeofthevehicle
A.2.AdditionalVisualisation
• Changetheinputmode
The racing simulator features multiple real world race-
• Turnon/offdebuggingroutines trackseachwithuniquefeaturesthatchallengehumanand
autonomousagentsalike(seeFigure4).
• Turnon/offsensors
• Modifysensorparameters B.DatasetDetails
• Modifyvehicleparameters We generate a rich, multimodal dataset of ex-
pert demonstrations from the training racetracks
A.1.2 Simulator-to-AgentCommunication (Track01:Thruxton and Track02:Anglesey), in
order to facilitate pre-training of agents via, e.g., imitation
Thesimulatorcommunicatestoagentsprimarilywithsen- learning (IL). The L2R dataset contains multi-sensory
soryinformationincluding: input at a 100-millisecond resolution, in both the obser-
vation and action spaces. See Table 1 in the main paper
• LiDARdatafrom4independentsensors
for a complete list of available modalities. The expert
• RADARdatafromthevehicle’sradarsensor demonstrations were collected using a model predictive
controller(MPC)thattracksthecenterlineoftheracetrack
• Imagesfromthefront-facingcamera
at a pre-specified reference speed. Important parameters
• Pose information from the inertial measurement unit forthiscenterlineMPCexpertincludedaccelerationrange
onthevehicle of [-1, 1], steering range of [-1, 1], and image H×W
dimensions of 384 × 512. This training dataset contains
• Additional data about the state of the vehicle such as
10,600 samples of each sensory and action dimension, in
brakepressureandtirespeedperwheel
this first version, which includes 9 complete laps around
• Ground-truthinformationaboutothervehicles thetrack. Demonstrationsweresavedasinvidualstep-wise
transitions, using numpy.savez compressed1, with
• Ground-truthinformationaboutvirtualobjects the following as dict fields in the data: (i) img with
shape (384,512,3); (ii) multimodal data with shape
The camera publishes images using Transmission Con-
(30,); (iii) and action with shape (2,). The fields in
trol Protocol (TCP) while the others publish sensory data
multimodal data correspond to the vector dimension
usingUserDataProtocol(UDP)oroveraControllerArea
mappings,indicatedinTable5.
Network(CAN).WhiletheLearn-to-Raceframework
Future version releases of L2R will include access to
exclusively supports software-in-the-loop simulation, and
newsimulatedtracks(alsomodelledafterrealtracks,from
therefore, only virtual CAN buses, the racing simulator
alsosupportshardware-in-the-loopsimulationandphysical 1https://numpy.org/doc/stable/reference/
CANbuses. generated/numpy.savez_compressed.html
External
Auto-Generated API
(based on WebSocket & JSON RPC) WebSocket
Vehicle Builder
Vehicle Dynamic Model Abstraction Layer Mechanics Dynamic Model HIL/SIL Abstraction Layer
Abstraction Layer
PhysX Model
PhysX Model or Arrival MSi om dp el le 4WD or Steering Braking Motors CAN/UDP
Arr 4iv Wal D A Mdv oa dn ec led or Gaze Mb oo d C eo lpter Differential Trans. ... or Arrival MSi om dp el le 4WD CAN
or Arr 4iv Wal D A Mdv oa dn ec led UDP
GNSS & IMU Sensors
GPSd
Vendor-specific Protocols (via CAN and Ethernet)
LiDAR Sensors
Environment General LiDAR based on DepthMap
(UE4 Maps) General LiDAR based on RayCast
Vendor-specific Protocols (via Ethernet)
Camera Sensors
General Pinhole via ZMQ
General Fisheye via ZMQ
Radar Sensor Model
General Radar
Vendor-specific Protocols (via CAN)
Generic Ultrasonic Sensor
General Ultrasonic (via Ethernet)
Input Sub-system CAN Bus Sub-system Sensors Manager Seria Sl uiz be -/ sd ye ss te er mialize
V2X/V2V
Sub-system UDP
Editor Mode Sensor Ms oP dla ecement R Ga ec ne e rT ar ta ioc nk A Uu sto e- rg Ie nn tee rr fa at ce ed
Vehicle Configuration
(Van Alpha, Van Beta, Bus Alpha, DevBot2, RoboCar, P1, Mustang)
Figure3: Overviewoftheracingsimulator.
around the world) as well as expert traces generated from trained to encode RGB images of with a width and height
these additional tracks—across various weather scenarios, of 144 pixels each into a latent space of size 32. The en-
in challenging multi-agent settings, and within dangerous coderarchitectureconsistedof4convolutionallayers,each
obstacle-avoidanceconditions. followedbyaReLuactivation,withakernelsizeandstride
of4and2,respectively. Theresultoftheconvolutionswas
passed through a single fully connected layer to the com-
C.AdditionalAgentDetails
pressed representation. Binary cross entropy loss and an
C.1.RL-SACModelDetails Adamoptimiserwereusedfortraining.
The RL-SAC agent learns from image embeddings rather TheRL-SACagentwastrainedfor1,000episodeswhich
than raw pixels. The encoder used is a convolutional vari- wasapproximately1millionstepsintheenvironment. We
ationalautoencoder(VAE)whichwastrainedpriorto,and trainedthisagentinvision-onlymode,soitonlyhadaccess
frozen during, the RL-SAC agent learning. The VAE was tothecamera’simages. Theagentpassedtheencodedim-
Figure 4: Firstcolumn, topfourrows: theThruxtonCircuitracetrack, UnitedKingdom, isinfamousforitslongstraightaways, high
speeds, and a difficult speed-trap near the finish line. Second column, top four rows: the North Road race track at Las Vegas Motor
Speedway,UnitedStates,includesthesharpturnsandmercilessspeedtrapsandaddsavision-processingchallengeforlearningagents,
duetothelowercontrastbetweenthetrackanditssurroundings. Thirdcolumn,topfourrows: theAngleseyCircuitracetrack, United
Kingdom,featurestwoprominentstraightsandseveralharrowingturns.Lastrow:theracingsimulatorfeaturesmultiplecarmodels,sensor
placements,weatherconditions,andadditionaltracks.
agesthroughtwofullyconnectedlayerswith64unitseach C.2.MPCAgentDetails
and a final layer with an output shape of 2, matching the
environment’sactionspace. Gradientupdatesweretakenat The MPC problem is summarised by Equation 5. The
the conclusion of episodes, and the training hyperparame- objective (Equation 5a) is to minimise the tracking error
tersarelistedinTable6. with respect to a reference trajectory, in this case the cen-
terline of the race track at a pre-specified reference speed,
with regularisation on actuations, over a planning horizon
Table 5: Vector dimension mappings, to which the data tion,andδisthesteeringangleatthefrontaxle.
fieldsinmultimodal data(30,)correspond.
x˙ =vcos(φ) (6a)
Arrayindices Description y˙ =vsin(φ) (6b)
0 steeringrequest
v˙ =a (6c)
1 gearrequest
2 mode φ˙ =vtanδ/L (6d)
3,4,5 directionalvelocityinm/s
6,7,8 directionalaccelerationinm/s2 A key challenge is that the ground truth vehicle param-
9,10,11 directionalangularvelocity eters were not known to us. Aside from L defined as the
12,13,14 vehicleyaw,pitch,androll,respectively distancebetweenthefrontandrearaxle,thekinematicbike
15,16,17 centerofvehiclecoordinatesin(y,x,z) model expects actions, i.e. acceleration and steering, in
18,19,20,21 wheelrevolutionsperminute(perwheel) physical units, while the environment expects commands
22,23,24,25 wheelbraking(perwheel)
in [−1,1]. The mapping is unknown to us, and non-linear
26,27,28,29 wheeltorque(perwheel)
basedonourobservations. Forinstance,accelerationcom-
mand=1resultsinsmalleraccelerationathigherspeed. In
Table6: RL-SACmodelhyperparameters thecurrentimplementation,wemakeasimplifyingassump-
tion that a = k × acceleration command, and δ = k ×
1 2
steeringcommand.
Hyperparameter Value
We use the iterative linear quadratic regulator (iLQR)
Buffersize 100,000
proposedin[26],whichiterativelylinearizesthenon-linear
Gamma 0.99
dynamics (Equation 6) along the current estimate of tra-
Polyak 0.995
jectory, solves a linear quadratic regulator problem based
Learningrate 0.001
on the linearized dynamics, and repeats the process until
Alpha 0.2
convergence. Specifically, weusedtheimplementationfor
Batchsize 256
iLQRfrom[4]. TheparametersusedbytheMPCaresum-
Startsteps 1000
marisedinTable7.
Learningsteps 5
Table7: MPCparameters
of T time steps. Q and R are both diagonal matrices cor- Parameter Value
respondingtocostweightsfortrackingreferencestatesand Q diag([1,1,1,16])
regularising actions. At the same time, the MPC respects R diag([0.1,1])
the system dynamics of the vehicle (Equation 5b), and al- v 12.5m/s
ref
lowableactionrange(Equation5c). ¯a [1,0.2]
a [−1,−0.2]
L 2.7m
min
(cid:88)T (cid:104)
(s t−s ref,i)TQ(s i−s ref,i)+aT
i
Ra
i(cid:105)
(5a)
k k1
2
1 60
a1:T
t=1 T 6
s.t. s =f(s ,a ), ∀t=1,...,T (5b)
t+1 t t
a≤a ≤a¯ (5c)
t
D.MetricEquations
Specifically, we characterise the vehicle with the kine-
We quantify the parametric curvature of a trajectory in
maticbikemodel2[25]giveninEquation6,wherethestate
Eqn. 7, with x(cid:48) denoting dx at time t, and we summarise
iss = [x,y,v,φ],andtheactionisa = [a,δ]. x,y arethe t dt
thecurvatureoftheentirepathasκ inEqn. 8:
vehiclelocationinlocaleast,north,up(ENU)coordinates, rms
v is the vehicle speed, and φ is the yaw angle (measured x(cid:48)y(cid:48)(cid:48)−y(cid:48)x(cid:48)(cid:48)
anti-clockwise from the local east-axis). a is the accelera- κ t = (cid:16) t t t t (cid:17)3 (7)
(x(cid:48))2+(y(cid:48))2 2
t t
2Thissetofequationsisdefinedwithrespecttothebackaxleofthe (cid:118)
vehicleandisusedforgeneratingexpertdemonstrations. Thekinematic (cid:117) (cid:117)1 (cid:32) (cid:88)T (cid:33)
bikemodeldefinedwithrespecttothecentreofthevehicleisalsoincluded κ rms =(cid:116) T κ2 t (8)
inourcodebase.
t=0
We measure Trajectory efficiency as ρ in Eqn. 9 based on
thecurvature,κ ,oftheracetrackandtheracingagent’s
rms
trajectory.
κ
ρ= rms,racetrack (9)
κ
rms,trajectory

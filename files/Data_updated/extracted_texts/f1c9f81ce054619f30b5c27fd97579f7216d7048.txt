Robust Long-Term Object Tracking via
Improved Discriminative Model Prediction
Seokeon Choi(cid:63)1, Junhyun Lee(cid:63)2, Yunsung Lee(cid:63)2, and Alexander Hauptmann3
1 KAIST, South Korea
seokeon@kaist.ac.kr
2 Korea University, South Korea
{ljhyun33, swack9751}@korea.ac.kr
3 Carnegie Mellon University, USA
alex@cs.cmu.edu
Abstract. We propose an improved discriminative model prediction
methodforrobustlong-termtrackingbasedonapre-trainedshort-term
tracker.Thebaselinepre-trainedshort-termtrackerisSuperDiMPwhich
combinesthebounding-boxregressorofPrDiMPwiththestandardDiMP
classifier. Our tracker RLT-DiMP improves SuperDiMP in the follow-
ing three aspects: (1) Uncertainty reduction using random erasing: To
make our model robust, we exploit an agreement from multiple im-
ages after erasing random small rectangular areas as a certainty. And
then, we correct the tracking state of our model accordingly. (2) Ran-
dom search with spatio-temporal constraints: we propose a robust ran-
dom search method with a score penalty applied to prevent the prob-
lemofsuddendetectionatadistance.(3)Backgroundaugmentationfor
more discriminative feature learning: We augment various backgrounds
that are not included in the search area to train a more robust model
in the background clutter. In experiments on the VOT-LT2020 bench-
markdataset,theproposedmethodachievescomparableperformanceto
the state-of-the-art long-term trackers. The source code is available at:
https://github.com/bismex/RLT-DIMP.
Keywords: Long-termobjecttracking,Robustobjecttracking,Uncer-
taintyreduction,Randomerasing,Randomsearch,Backgroundaugmen-
tation, Discriminative feature learning
1 Introduction
Visual Object Tracking (VOT), the task of continuously locating an arbitrary
target in the first frame of a video, has been drawn attention in both aca-
demic and industrial fields over the last decade [28,16,18]. It is because VOT
can be widely used in real-world applications such as autonomous vehicles [20],
robotics[29],andvideosurveillancesystems[1].Withtheadvanceofdeeplearn-
ing techniques, trackers are not only getting better performance but also used
at long-term tracking (minute-level) beyond short-term tracking (second-level).
(cid:63) This work was done while the authors were visiting researchers at CMU.
0202
guA
52
]VC.sc[
2v22740.8002:viXra
2 Seokeon Choi, Junhyun Lee, Yunsung Lee and Alexander Hauptmann
Besides the length of input videos, the clear difference between short-term
trackingandlong-termtrackingiswhetherthetargetexistsinthefieldofview,as
reflectedinstandardbenchmarkdatasets[33,25].Ingeneral,short-termtrackers
are designed on the assumption that the target always appears in every single
frame, otherwise, the short-term tracker will drift and fail. Long-term trackers,
ontheotherhand,needtokeeptrackoftheobjectevenifitdisappearsfromthe
fieldofviewinthemiddleoftheframes.Consequently,there-detectionmodule,
which localizes the target with a confidence score of its absence, is the essential
part of long-term trackers.
Because long-term trackers encounter unpredictable abrupt changes during
relativelylongsequences,therobustnessisthemostimportantpropertyoflong-
term trackers. If the long-term tracker misestimates the location of a target
because of visual deformation, there is a high risk of incorrect estimation in
the following frame. Previous researches tried to construct robust modules and
strategiesinvariouswayssuchasP-Nlearning[15],memorymodel[21],anddy-
namic programming [30]. Those methods are focusing on the robustness against
visual deformation.
Inthiswork,wefocusontherobustnessagainstthere-detectionmoduleitself
(i.e. reliability of the trackerâ€™s prediction) as well as against visual deformation.
First, we propose a way to reduce the uncertainty of our model and correct
thepredictionaccordingly.Theestimatedlocationofthetarget,whichisrobust
againstthebackgroundnoise,wouldnotbechangedevenifweremoveacertain
small area of the background. If the estimation is not robust then it would
be changed and not reliable even though the confidence score is high. In view
of these characteristics, our model estimates the location of the target from
multiple images with randomly erased the small rectangular area. Secondly, we
utilize spatio-temporal constraints to adjust the confidence score for robust re-
detection.Whenthetargetre-appearafterocclusionordisappear,thetime-space
gap should be related in physical. For instance, if the estimated location of the
target that re-appears in a short time is far from the last observation location,
wecansaythattheestimationisunreliable.Tooffsetthedistortionofbothtime
and space and to make a robust estimation as a result, we explicitly adjust the
confidence score by penalizing it. Finally, we perform background augmentation
for more discriminative feature learning in the online stage.
Section 2 provides a brief description of existing short- and long-term track-
ers. In Section 3, we explain the details about how our approach can handle the
robustness issues of long-term trackers. The experimental results with analysis
and the conclusion are in Section 4 and 5, respectively.
2 Related Work
2.1 Short-term object tracking
Visualobjecttracking(VOT)isatasktotrackanobjectinavideowhenthefirst
frameboundingboxofthetargetobjectisgiven.Anumberofdeepconvolutional
RLT-DiMP 3
neuralnetwork(DCNN)basedstudieshavebeenconducted,suchasMDNet[26]
and FCNT [31], showing successful results in the VOT Challenge [17]. Among
DCNN-based studies, Siamese architecture [5,23,30,32,22] satisfies end-to-end
training capabilities while also showing high efficiency [16,18]. DiMP [6] and
PrDiMP [11], motivated from the idea of ATOM [9] that solved the limited
targetestimationproblemofthepreviousstudies,arealsoSiamesearchitecture-
based model that showed performance improvement in the VOT Challenge.
Most tracking models before ATOM [9] were only focused on the develop-
ment of powerful classifiers. The problem of accurate target state estimation
has been overlooked. To this end, ATOM architecture consists of dedicated tar-
getestimationandclassificationcomponents.Thistargetestimationcomponent
is trained to predict the overlap between the target object and an estimated
bounding box. High-level knowledge is incorporated into the target estimation
through extensive offline learning.
Siamese networks [5,23,30,32,22] have received much attention due to their
end-to-endtrainingcapabilitiesandhighefficiency.However,Siameseapproaches
arelimitedintheirinabilitytoincorporateinformationfromthebackgroundre-
gion or previous tracked frames into the model prediction. To deal with this
issue, DiMP [6] takes inspiration from the discriminative online learning proce-
dures [9,10,26]. DiMP tracking architecture consists of two branches: a target
classification branch for distinguishing the target from the background, and a
bounding box estimation branch for predicting an accurate target box. The tar-
getclassificationbranchisderivedfromtwomainprinciples:(i)adiscriminative
learninglosspromotingrobustnessinthelearnedtargetmodeland(ii)apower-
ful optimization strategy ensuring rapid convergence. Bounding box estimation
branch is utilized from the overlap maximization based architecture introduced
in ATOM [9].
Most tacking models rely on estimating a state-dependent confidence score,
but this value lacks a clear probabilistic interpretation, complicating its use.
Therefore, in PrDiMP [11], a probabilistic regression formulation is proposed,
and it is applied to track the target. PrDiMP network predicts the conditional
probability density of the target state given an input image. Their formulation
helpsthemodeltoberobustfrominaccurateannotationsandambiguitiesinthe
task.
2.2 Long-term object tracking
The target may disappear in the long-term tracking setting, so a re-detection
module is essential. In addition, a robust online update method capable of ac-
commodatingchangesinthevisualappearanceofthetargetdramaticallyaffects
performance. Therefore, we shortly introduce how long-term trackers overcome
significant problems.
The most representative long-term object tracker is TLD [15], which is di-
vided into tracking (T), learning (L), and detection (D). In the tracking part,
the tracker predicts the position of the target object based on the median-flow
4 Seokeon Choi, Junhyun Lee, Yunsung Lee and Alexander Hauptmann
tracker [14]. In the detection part, the detector judges the existence of the tar-
get in a cascade manner over the entire area of the image. Assuming that the
trackeranddetectorcanfail,thelearningmoduleestimateserrorsbasedonP-N
learning and trains the trackers and detectors more robustly.
Due to its good performance in both accuracy and speed, various object
trackers based on Siamese networks have been proposed. One of those meth-
ods, MMLT [21], is designed for long-term tracking to handle visual deforma-
tion and target disappearance. In the tracking step, to accommodate changes in
the visual appearance, the target position is estimated by the Siamese features
obtained from short-term and long-term memory stores inspired by Atkinson-
Shiffrin Memory Model (ASMM) [2]. In the re-detection step, the target is de-
tected in the entire image without the dependency of the previous position. In
particular, the coarse-to-fine strategy is adopted for improving speed.
Tracking by re-detection paradigm [3,4,12,22] has a long history, but re-
detection is challenging due to the existence of distractor objects that are very
similartothetemplateobject.SiamR-CNN[30],anadaptationofFasterR-CNN
[27] with a Siamese architecture, has two key methods. First, they introduced a
hardexampleminingprocedurewhichtrainsthere-detectorspecificallyfordiffi-
cultdistractors.Secondly,dynamicprogrammingisusedtoselectthebestobject
in the current time step based on the complete history of all target objects and
distractor object tracklets(short object tracks). While being resistant to tracker
drift and being able to immediately re-detect object after the disappearance,
Siam R-CNN is able to effectively perform long-term object tracking.
3 Proposed Method
3.1 Baseline Short-Term Tracker
We propose an improved Discriminative Model Prediction method for robust
long-term tracking based on a pre-trained short-term tracker. Our baseline pre-
trained short-term tracker is SuperDiMP 1 combining the bounding-box regres-
sorofPrDiMP[11]withthestandardDiMPclassifier[6]forbettertrainingand
inference.
3.2 Uncertainty Reduction using Random Erasing
Wefocusonrobustness,whichistheconsistentgeneralization(tracking)ability,
particularly against the artifact of background features. The robust model can
consistently track the target whatever occurs in the background, even whenever
weremovesomeregionofbackground.Weconsideruncertaintyasanagreement
(or consistency) and estimate the location of the target from multiple images
after erasing random small rectangular areas. Because the target usually has
a small portion of the whole image, even though we remove a small area, the
targetmightberarelyremoved.Ifthepredictionofourmodelchangeswhenthe
1 The pre-trained model is provided at https://github.com/visionml/pytracking.
RLT-DiMP 5
Fig.1.(a)illustratesourassumptionthatthepredictionoftherandomlyerasedimage
will be changed when the prediction is affected by background features or vice versa.
(b)illustratestheconceptofhowtoreducetheuncertaintyandcorrecttheprediction.
smallregionofbackgroundisrandomlydeleted,thepredictionisaffectedbythe
background, so we can say that the certainty is low. Adapting this assumption,
we correct the flag of our model according to the agreement.
3.3 Random Search with Spatio-Temporal Constraints
Generalshort-termtrackerscanonlyfindthetargetifitisincludedinthesearch-
ing area. Accordingly, global re-detection capacity is required for robust long-
termtrackingtodealwithocclusionsandtargetdisappearances.Mostlong-term
trackers adopt the global sliding window method, as shown in Fig. 2. However,
this approach not only requires high computation costs, but is not robust. To
tackle this problem, we propose a robust random search method with spatio-
temporal constraints.
6 Seokeon Choi, Junhyun Lee, Yunsung Lee and Alexander Hauptmann
Fig.2. A re-detection example using a general global search method. It takes a long
time to find objects in all areas.
Fig.3. Re-detection examples using the proposed random search method. From a
stochastic point of view, it is possible to search the entire area within a few frames.
Random search. First, we create global searching templates with a predeter-
minedinterval.Next,weadaptivelydeterminethenumberofsearchesaccording
totheratiooftheimagesizetothetargetsize.Whenthetargetsizeisrelatively
large, we set the fewer number to search. Otherwise, we assign more numbers
to search when the target size is relatively small. Then, an object is detected
within a randomly selected searching area. As visualized in Fig. 3, it is possible
tosearchwholeregionswithinafewframesfromastochasticpointofview.This
stochasticapproachimprovesthere-detectionspeedcomparedtogeneralsliding
window methods, which is discussed in Section 4.3.
Score penalty. Whentheconfidencescores ofthenewlydetectedtargetis
new
higher than a predetermined threshold, the frame in which the new object ap-
pearedisdesignatedasthenewfirstframe,andtheobjectisagaintracked.Here,
we note that this naÂ¨Ä±ve re-detection scheme is not robust. Figure 4 shows the
examplesofsuddendetectionofothersimilarobjectsorbackgrounddistractors.
Once the target is missing, a new object can be re-detected immediately, even if
the new object is located far from the original target. However, the probability
of an object disappearing and suddenly appearing at a distant location is very
low. To prevent this sudden detection, we penalize a confidence score through
spatio-temporal constraints, which is expressed as follows:
RLT-DiMP 7
Fig.4. Examples of the need for score penalty. In the naÂ¨Ä±ve re-detection scheme, a
similar object can be re-detected immediately, even if the new object is located far
fromtheoriginaltarget.Inotherwords,thismethodisquitevulnerabletobackground
clutter.
||p âˆ’p ||
s(cid:48)
new
=w b(1âˆ’w
d
new
d
old 2 Â·eâˆ’wt|tnewâˆ’told|)Â·s new, (1)
max
where w ,w , and w are hyper-parameters for basic re-detection, distance, and
b d t
time, respectively. d , p, and t indicate a diagonal length of an image (i.e.
max
the maximum distance), a position vector, and a frame number, respectively.
In terms of a spatial constraint, the score is more penalized when the distance
between new and old positions is large. This distance penalty term prevents
the problem of abnormal detection at a long distance. Meanwhile, this distance
penaltyiscompensatedbythetimepenalty(i.e.atemporalconstraint),astime
passes by not finding the target. This is because, if the period during which the
objectcannotbefoundbecomeslonger,theobjectmaynewlyappearatalonger
distance.Thistemporalconstraintallowsobjectstobedetectedatrelativelyfar
locations. The revised re-detection module makes the tracker more robust.
3.4 Background Augmentation for More Discriminative Feature
Learning
In the short-term baseline tracker, we extract features and trains the model by
applying some transforms (blur, rotate, horizontal flip) in the searching area of
the target. To train a robust model against the background clutter, we try to
augment various backgrounds that are not included in the search area by com-
biningthetargetimagewithanotherbackground.Thisaugmentationskillmakes
the model capable of fully exploiting various background appearance informa-
tion.Figure5showsourdataaugmentationmethodsforenhanceddiscriminative
feature learning.
Learning at the first frame. The bounding box is given in the first frame,
which means that the target image is completely confident. Therefore, we train
the short-term tracker using new data augmentation and additionally store the
newly generated images in the memory system. This approach allows for im-
proved discriminative learning in the first frame.
8 Seokeon Choi, Junhyun Lee, Yunsung Lee and Alexander Hauptmann
Fig.5. General data augmentation and additional samples for more discriminative
learning.
Online learning. We also perform background augmentation during tracking
intheonlinestageassameasinthefirstframe.However,thisprocessisonlyused
when the reliability of the bounding box is high since the estimated bounding
box is not always confident. The background augmentation process is the same
as the first frame one, and the filter is trained if more demanding conditions are
met.Unlikeinthefirstframesettings,imageswithbackgroundaugmentationare
not stored in memory. This prevents the problem of including negative features
that cause drifting to other objects.
3.5 Confidence Score Assignment
The baseline short-term tracker classifies the state of object tracking into four
typesaccordingtothescoreandvariousconditions:normal,hardnegative,uncer-
tain,andnot found.Inthecaseofnormal andhard negative,theobjecttracking
result is reliable, so the confidence score is given as 1. In the case of uncertain,
it is difficult to determine whether it is an object or not, so the confidence score
is given as 0.5. Lastly, in the case of not found, it is predicted that there is no
object, so the confidence score is given as 0.
4 Experiments
4.1 Dataset and Settings
Dataset We experiment with a long-term object tracking dataset, LTB50 [24].
ThisdatasetisanextensionoftheLTB35[24]usedintheVOT-LT2018challenge
[16], and it is officially used in the VOT-LT2019 [18] challenge. In the VOT-
LT2020 challenge, the LTB50 is used unchanged from last year. The LTB50
RLT-DiMP 9
Benchmark Tracker F-score Precision Recall
FuCoLoT 0.411 0.507 0.346
ASINT 0.505 0.517 0.494
CooSiam 0.508 0.482 0.537
Siamfcos-LT 0.520 0.493 0.549
VOT-LT2019 [18] SiamRPNsLT 0.556 0.749 0.443
mbdet 0.567 0.609 0.530
SiamDW LT 0.665 0.697 0.636
CLGS 0.674 0.739 0.619
LT DSE 0.695 0.715 0.677
VOT-LT2020 RLT-DiMP 0.681 0.667 0.695
Table 1. Comparison with the state-of-the-art methods on VOT-LT2019 and VOT-
LT2020 benchmarks. Both benchmarks are based on the LTB50 dataset.
dataset contains 50 sequences of various objects with a total length of 215,294
frames for single-target object tracking. In each sequence, the target disappears
anaverageof10times,andthedisappearedtargetlastsanaverageof52frames.
The resolution of video sequences is between 1280 Ã— 720 and 290 Ã— 217. All
targets are marked with an axis-aligned bounding box.
Evaluation protocol TheproposedRLT-DiMPisevaluatedbytheevaluation
protocol of the VOT-LT2020 benchmark. An evaluation protocol for long-term
trackersfollowsano-resetprotocol,whichmeansthattheobjecttrackerwillnot
restart even if the object tracking fails. Three evaluation metrics are adopted
for the long-term tracking benchmark: tracking precision, tracking recall, and
tracking F-measure. For additional information, please see [24]. This evaluation
is automatically performed by the VOT toolkit [8,19]. All experiments are per-
formed on a system with Intel(R) core(TM) i7-4770 3.40 GHz processor and a
single NVIDIA GTX 1080 Ti with 11GB RAM.
4.2 Quantitative Evaluation
Overall comparison with long-term trackers. We compare our proposed
method with the state-of-the-art methods in the VOT-LT2019 benchmark [18].
TheVOT-LT2019andVOT-LT2020benchmarksarebasedontheLTB50dataset
[24]. Both of competing methods and our method are re-detecting long-term
trackers (LT1). This term means that all of the trackers detect tracking failure,
and an explicit re-detection technique is implemented, unlike the pseudo long-
term tracker (LT0). The following taxonomy has been introduced explicitly in
[24].
Table1showsthequantitativeevaluationoflong-termtrackersontheLTB50
dataset. In the VOT-LT2019 benchmark, LT DSE tracker has achieved the best
F-score and the best tracking recall, and CLGS has achieved the best track-
ing precision. The tracker LT DSE is designed based on target localization by
10 Seokeon Choi, Junhyun Lee, Yunsung Lee and Alexander Hauptmann
1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
following (164) yamaha (42) carchase (27) freestyle (27) liverRun (27) bicycle (19) dog (18) longboard (18) !ghtrope (16) cat2 (13)
RLT_dimp super_dimp prdimp50 prdimp18 dimp50 dimp18 dimp50_vot
1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
warmup (13) bird1 (12) dragon (12) skiing (11) boat (10) ballet (9) volkswagen (9) sitcom (8) freesbiedog (6) helicopter (6)
RLT_dimp super_dimp prdimp50 prdimp18 dimp50 dimp18 dimp50_vot
1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
person19 (6) wingsuit (6) bull (5) group2 (5) group3 (5) uav1 (5) cat1 (4) deer (3) car1 (2) f1 (2)
RLT_dimp super_dimp prdimp50 prdimp18 dimp50 dimp18 dimp50_vot
1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
horseride (2) nissan (2) parachute (2) person7 (2) person17 (2) rollerman (2) person14 (1) sup (1) bike1 (0) car3 (0)
RLT_dimp super_dimp prdimp50 prdimp18 dimp50 dimp18 dimp50_vot
1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
car6 (0) car8 (0) car9 (0) car16 (0) group1 (0) kitesurfing (0) person2 (0) person4 (0) person5 (0) person20 (0)
RLT-DiMP SuperDiMP PrDiMP50 PrDiMP18 DiMP50 DiMP18 DiMP50-VOT
RLT-DiMP SuperDiMP PrDiMP50 PrDiMP18 DiMP50 DiMP18 DiMP50-VOT
Fig.6. The maximum F-score for each sequence on the LTB50 dataset. Sequences
are sorted based on the number of target disappearances, which are indicated by the
number in parentheses.
ATOM [9], bounding box refinement by SiamMask [32], and a verifier network
by RT-MDNet [13]. The tracker CLGS is designed based on target localization
bySiamMask[32],globaldetectionbycascadeR-CNN[7],andanonlineverifier
by RT-MDNet [13].
Our RLT-DiMP achieves comparable performance to the state-of-the-art
long-term trackers in the VOT-LT2019 benchmark. Our method has an F-score
of 0.007 higher than that of CLGS and an F-score of 0.014 lower than that
of LT DSE. For tracking precision, the proposed method achieves lower perfor-
mance than the top three trackers of the VOT-LT2019 benchmark. Notably,
we achieve the highest score in the tracking recall metric, which means that
our tracker is well modeled to be robust for long-term tracking through the
erocs-F
erocs-F
erocs-F
erocs-F
erocs-F
RLT-DiMP 11
threecontributions:uncertaintyreductionusingrandomerasing,robustrandom
searchwithspatio-temporalconstraints,andbackgroundaugmentationformore
discriminative feature learning.
Comparison by sequence with short-term trackers. Our RLT-DiMP
methodisanextendedversionbasedonapre-trainedshort-termtrackerwithim-
provedrobustnessforlong-termobjecttracking.Ourbaselinepre-trainedshort-
term tracker is SuperDiMP, which is a combination of the standard DiMP clas-
sifier [6] and the bounding-box regressor of PrDiMP [11] for better tracking. In
thissection,wecompareourlong-termtrackerwithvariousshort-termtrackers,
including the baseline tracker SuperDiMP and the individual methods of DiMP
[6]andPrDiMP[11].PrDiMP50andDiMP50aremethodsusingResNet50,and
PrDiMP18 and DiMP18 are methods using ResNet18 as a backbone. DiMP50-
VOT is another version designed to follow a reset protocol for the VOT-ST
benchmark. The rest of the PrDiMP- and DiMP-family all follow the no-reset
protocol, so these methods can be applied well in a long-term object tracking
environment.
Figure 6 shows the maximum F-score for each sequence on LTB50. All se-
quences are listed in order of the number of target disappearances. We can
observe that the baseline tracker SuperDiMP has better reasoning skills than
the individual methods of DiMP [6] and PrDiMP [11]. We emphasize that our
RLT-DiMP method outperforms the baseline tracker in almost all sequences.
Thisprovesthat ourmethod is more suitable forlong-term objecttracking, and
robust modeling via uncertainty reduction, robust random search, and back-
ground augmentation plays a significant role. This is analyzed in more detail
through ablation studies in the next section.
4.3 Further Evaluations and Analysis
Visual attributes on LTB50. In the LTB50 dataset, a total of 50 sequences
are annotated by nine visual attributes as follows: 1) Full occlusion, 2) Out-of-
view,3)Partialocclusion,4)Cameramotion,5)Fastmotion,6)Scalechange,7)
Aspectratiochange,8)Viewpointchange,9)Similarobjects.Whilealongboard
sequence even has as many as 8 visual attributes, and a person5 sequence has
no visual attribute. As described above, each sequence includes several visual
attributes,andweperformanablationstudybyaveragingperformanceforeach
visual characteristic.
Ablation studies. Figure 7 shows the overall F-score and the average F-score
foreachvisualattributewithrespecttovariousversionsoftheproposedmethod.
RLT-DiMP and SuperDimp indicate our final version and baseline version, re-
spectively. In this section, we compare performance by adding one proposed
module each from the baseline.
Asdescribedintheoverallperformanceinthebargraph,Whenapplyingun-
certaintyreduction,robustrandomsearch,backgroundaugmentationalgorithms
12 Seokeon Choi, Junhyun Lee, Yunsung Lee and Alexander Hauptmann
0.700
0.675
0.650
0.625
0.600
0.575
0.550
0.525
0.500
Full occlusion Aspect ra!o Fast mo!on Par!al Camera
change occlusion mo!on
0.700
0.675
0.650
0.625
0.600
0.575
0.550
0.525
0.500
Out-of-view Scale change Viewpoint Similar objects Overall
change
RLT-DiMP baseline + uncertainty measurement baseline + re-detec!on
baseline + more discrimina!ve learning baseline (SuperDiMP)
Fig.7. The overall F-score and the average F-score for each attribute on the LTB50
dataset.
tobaseline,theF-scoreisimprovedby0.025,0.031,and0.018.Accordingly,these
results prove that our sub-algorithms enable our tracker to estimate the target
position more robustly. We note that our RLT-DiMP method outperforms the
F-score by 0.038 compared to the baseline tracker.
In the visual attribute analysis, the proposed method surpasses the baseline
tracker in all cases. Especially in situations with visual characteristics of full-
occlusion, aspect ratio change, fast motion, and partial occlusion, our method
has an F-score performance of 0.05 or higher than the baseline.
Processing time analysis. In the LTB50 dataset, our method records 14.17
fps, which is somewhat lower than the object tracking speed of PrDiMP [11]
at 21.83 fps and DiMP [6] at 30.22 fps. All three proposed modules inevitably
reducethespeedofobjecttracking.However,inthecaseofre-detection,wenote
that the random search method can improve the speed of about 3 fps compared
to the global sliding window method.
erocs-F
erocs-F
RLT-DiMP 13
4.4 Qualitative Evaluation
Inthissection,weperformqualitativeevaluationbyselectingseveralsequences,
which are visualized in Fig 8. The total of 10 selected sequences is sorted based
on the number of times the target disappeared. Besides, not only the number of
timesdisappearedforeachsequence,butalsothenumberofframes,theduration
ofthetargetdisappearance,andthevisualattributesareorganizedinthefigure.
We show through various examples that our method RLT-DiMP estimates
a target position more accurately than SuperDiMP, PrDiMP [11], and DiMP
[6],withoutdependingontheabovesituations.Inparticular,weemphasizethat
our method has a robust re-detection ability with the aid of spatio-temporal
contraints in dog, bird1, boat, and person7 sequences where the target object
suddenly re-appears in a different location. In addition, we note that the target
is robustly tracked in carchase, warmup, uav1, and group2 sequences with sim-
ilar objects or background clutter through uncertainty reduction using random
erasing and enhanced discriminative feature learning with background augmen-
tation.
5 Conclusion
Wehaveproposedarobustlong-termobjecttrackerviaanimproveddiscrimina-
tive model prediction method. Conventional object trackers easily follow other
objectswhenthetargetobjectdisappearsfromvieworispartiallyobscureddue
to the presence of background distractors or similar objects. Long-term object
trackersneedtoberobusttothesechallengingissuesbecausetheyhavetotrack
objectswithoutrestartingforalongperiod.Tothisend,ourapproachimproves
robustness for long-term tracking through uncertainty reduction using random
erasing,robustrandomsearchwithspatio-temporalconstraints,andbackground
augmentation for more discriminative feature learning. Quantitative and quali-
tative evaluation on the VOT-LT2020 benchmark dataset demonstrates the su-
periority of our method over the state-of-the-art long-term trackers.
Acknowledgment
This work was supported in part through NSF grant IIS-1650994, the financial
assistanceaward60NANB17D156fromU.S.DepartmentofCommerce,National
InstituteofStandardsandTechnology(NIST)andbytheIntelligenceAdvanced
Research Projects Activity (IARPA) via Department of Interior/Interior Busi-
ness Center (DOI/IBC) contract number D17PC0034. The U.S. Government
is authorized to reproduce and distribute reprints for Governmental purposes
notwithstanding any copy-right annotation/herein. Disclaimer: The views and
conclusions contained herein are those of the authors and should not be inter-
preted as representing the official policies or endorsements, either expressed or
implied, of NIST, IARPA, NSF, DOI/IBC, or the U.S. Government.
14 Seokeon Choi, Junhyun Lee, Yunsung Lee and Alexander Hauptmann
O Full occlusion V Out-of-view P Par!al occlusion C Camera mo!on F Fast mo!on
S Scale change A Aspect ra!o change W Viewpoint change I Similar objects
carchase (9928 frames, 27 disappearances, 13% target missing)
O
V
C
S
I
dog (1829 frames, 18 disappearances, 8% target missing)
O
P
C
F
A
S
W
cat2 (2756 frames, 13 disappearances, 7% target missing)
O
V
P
F
A
warmup (3961 frames, 13 disappearances, 7% target missing)
V
P
W
I
bird1 (2437 frames, 12 disappearances, 43% target missing)
V
P
F
A
S
dragon (4474 frames, 12 disappearances, 20% target missing)
O
V
P
A
boat (7524 frames, 10 disappearances, 28% target missing)
V
C
F
A
S
W
uav1 (3469 frames, 5 disappearances, 6% target missing)
V
C
F
S
group2 (2683 frames, 5 disappearances, 8% target missing)
O
C
person7 (2065 frames, 2 disappearances, 3% target missing)
V
C
A
Ground truth RLT- DiMP SuperDiMP PrDiMP50 Dimp50
Fig.8. Qualitative results of the proposed RLT-DiMP, SuperDiMP, PrDiMP [11],
DiMP [6]. Best viewed in color.
RLT-DiMP 15
References
1. Ali, A., Jalil, A., Niu, J., Zhao, X., Rathore, S., Ahmed, J., Iftikhar, M.A.: Vi-
sualobjecttrackingclassicalandcontemporaryapproaches.FrontiersofComputer
Science 10(1), 167â€“188 (2016)
2. Atkinson,R.C.,Shiffrin,R.M.:Humanmemory:Aproposedsystemanditscontrol
processes (1968)
3. Avidan, S.: Support vector tracking. IEEE transactions on pattern analysis and
machine intelligence 26(8), 1064â€“1072 (2004)
4. Babenko,B.,Yang,M.H.,Belongie,S.:Robustobjecttrackingwithonlinemultiple
instancelearning.IEEEtransactionsonpatternanalysisandmachineintelligence
33(8), 1619â€“1632 (2010)
5. Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H.: Fully-
convolutional siamese networks for object tracking. In: European conference on
computer vision. pp. 850â€“865. Springer (2016)
6. Bhat, G., Danelljan, M., Gool, L.V., Timofte, R.: Learning discriminative model
prediction for tracking. In: Proceedings of the IEEE International Conference on
Computer Vision. pp. 6182â€“6191 (2019)
7. Cai,Z.,Vasconcelos,N.:Cascader-cnn:Delvingintohighqualityobjectdetection.
In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
pp. 6154â€“6162 (2018)
8. CË‡ehovin, L.: Trax: The visual tracking exchange protocol and library. Neurocom-
puting 260, 5â€“8 (2017)
9. Danelljan, M., Bhat, G., Khan, F.S., Felsberg, M.: Atom: Accurate tracking by
overlapmaximization.In:ProceedingsoftheIEEEConferenceonComputerVision
and Pattern Recognition. pp. 4660â€“4669 (2019)
10. Danelljan, M., Robinson, A., Khan, F.S., Felsberg, M.: Beyond correlation filters:
Learning continuous convolution operators for visual tracking. In: European con-
ference on computer vision. pp. 472â€“488. Springer (2016)
11. Danelljan,M.,VanGool,L.,Timofte,R.:Probabilisticregressionforvisualtrack-
ing. arXiv preprint arXiv:2003.12565 (2020)
12. Grabner,H.,Grabner,M.,Bischof,H.:Real-timetrackingviaon-lineboosting.In:
Bmvc. vol. 1, p. 6 (2006)
13. Jung, I., Son, J., Baek, M., Han, B.: Real-time mdnet. In: Proceedings of the
European Conference on Computer Vision (ECCV). pp. 83â€“98 (2018)
14. Kalal, Z., Mikolajczyk, K., Matas, J.: Forward-backward error: Automatic detec-
tionoftrackingfailures.In:201020thInternationalConferenceonPatternRecog-
nition. pp. 2756â€“2759. IEEE (2010)
15. Kalal, Z., Mikolajczyk, K., Matas, J.: Tracking-learning-detection. IEEE transac-
tions on pattern analysis and machine intelligence 34(7), 1409â€“1422 (2011)
16. Kristan,M.,Leonardis,A.,Matas,J.,Felsberg,M.,Pflugfelder,R.,CehovinZajc,
L., Vojir, T., Bhat, G., Lukezic, A., Eldesokey, A., et al.: The sixth visual object
trackingvot2018challengeresults.In:ProceedingsoftheEuropeanConferenceon
Computer Vision (ECCV). pp. 0â€“0 (2018)
17. Kristan,M.,Matas,J.,Leonardis,A.,Felsberg,M.,Cehovin,L.,Fernandez,G.,Vo-
jir,T.,Hager,G.,Nebehay,G.,Pflugfelder,R.:Thevisualobjecttrackingvot2015
challengeresults.In:ProceedingsoftheIEEEinternationalconferenceoncomputer
vision workshops. pp. 1â€“23 (2015)
18. Kristan, M., Matas, J., Leonardis, A., Felsberg, M., Pflugfelder, R., Kamarainen,
J.K., Cehovin Zajc, L., Drbohlav, O., Lukezic, A., Berg, A., et al.: The seventh
16 Seokeon Choi, Junhyun Lee, Yunsung Lee and Alexander Hauptmann
visual object tracking vot2019 challenge results. In: Proceedings of the IEEE In-
ternational Conference on Computer Vision Workshops. pp. 0â€“0 (2019)
19. Kristan, M., Matas, J., Leonardis, A., VojÂ´Ä±Ë‡r, T., Pflugfelder, R., Fernandez, G.,
Nebehay, G., Porikli, F., CË‡ehovin, L.: A novel performance evaluation methodol-
ogy for single-target trackers. IEEE transactions on pattern analysis and machine
intelligence 38(11), 2137â€“2155 (2016)
20. Laurense,V.A.,Goh,J.Y.,Gerdes,J.C.:Path-trackingforautonomousvehiclesat
thelimitoffriction.In:2017AmericanControlConference(ACC).pp.5586â€“5591.
IEEE (2017)
21. Lee,H.,Choi,S.,Kim,C.:Amemorymodelbasedonthesiamesenetworkforlong-
term tracking. In: Proceedings of the European Conference on Computer Vision
(ECCV). pp. 0â€“0 (2018)
22. Li, B., Wu, W., Wang, Q., Zhang, F., Xing, J., Yan, J.: Siamrpn++: Evolution
of siamese visual tracking with very deep networks. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. pp. 4282â€“4291 (2019)
23. Li, B., Yan, J., Wu, W., Zhu, Z., Hu, X.: High performance visual tracking with
siameseregionproposalnetwork.In:ProceedingsoftheIEEEConferenceonCom-
puter Vision and Pattern Recognition. pp. 8971â€“8980 (2018)
24. Lukezic, A., Zajc, L.C., VojÄ±r, T., Matas, J., Kristan, M.: Now you see me: eval-
uating performance in long-term visual tracking. arXiv preprint arXiv:1804.07056
4 (2018)
25. Moudgil, A., Gandhi, V.: Long-term visual object tracking benchmark. In: Asian
Conference on Computer Vision. pp. 629â€“645. Springer (2018)
26. Nam,H.,Han,B.:Learningmulti-domainconvolutionalneuralnetworksforvisual
tracking. In: Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 4293â€“4302 (2016)
27. Ren,S.,He,K.,Girshick,R.,Sun,J.:Fasterr-cnn:Towardsreal-timeobjectdetec-
tionwithregionproposalnetworks.In:Advancesinneuralinformationprocessing
systems. pp. 91â€“99 (2015)
28. Smeulders,A.W.,Chu,D.M.,Cucchiara,R.,Calderara,S.,Dehghan,A.,Shah,M.:
Visual tracking: An experimental survey. IEEE transactions on pattern analysis
and machine intelligence 36(7), 1442â€“1468 (2013)
29. SË‡uligoj,F.,SË‡ekoranja,B.,SË‡vaco,M.,JerbiÂ´c,B.:Objecttrackingwithamultiagent
robotsystemandastereovisioncamera.ProcediaEngineeringpp.968â€“973(2014)
30. Voigtlaender, P., Luiten, J., Torr, P.H., Leibe, B.: Siam r-cnn: Visual tracking by
re-detection. arXiv preprint arXiv:1911.12836 (2019)
31. Wang,L.,Ouyang,W.,Wang,X.,Lu,H.:Visualtrackingwithfullyconvolutional
networks.In:ProceedingsoftheIEEEinternationalconferenceoncomputervision.
pp. 3119â€“3127 (2015)
32. Wang,Q.,Zhang,L.,Bertinetto,L.,Hu,W.,Torr,P.H.:Fastonlineobjecttracking
and segmentation: A unifying approach. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 1328â€“1338 (2019)
33. Wu, Y., Lim, J., Yang, M.H.: Object tracking benchmark. IEEE Transactions on
Pattern Analysis and Machine Intelligence 37(9), 1834â€“1848 (2015)

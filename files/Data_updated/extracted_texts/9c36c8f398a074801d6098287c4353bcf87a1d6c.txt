Enabling Classifiers to Make Judgements
Explicitly Aligned with Human Values
YejinBang1∗ TiezhengYu1∗ AndreaMadotto2
ZhaojiangLin2 MonaDiab2 PascaleFung1†
1TheHongKongUniversityofScienceandTechnology 2MetaAI
{yjbang,tyuah}@connect.ust.hk
Abstract
Many NLP classification tasks, such as sex-
ism/racismdetectionortoxicitydetection,are
basedonhumanvalues.Yet,humanvaluescan
vary underdiverse culturalconditions. There-
fore, we introduce a framework for value-
alignedclassificationthatperformsprediction
basedonexplicitlywrittenhumanvaluesinthe
command. Along with the task, we propose
apracticalapproachthatdistillsvalue-aligned
knowledge from large-scale language models
(LLMs) to construct value-aligned classifiers
intwosteps. First, wegeneratevalue-aligned
training data from LLMs by prompt-based
few-shotlearning. Next, wefine-tunesmaller
classification models with the generated data
for the task. Empirical results show that our
Figure1:Illustrationofproposedvaluealignmenttask.
VA-MODELs surpass multiple baselines by at
Given the same content, VA-MODEL makes variable
least 15.56% on the F1-score, including few-
predictionsbasedonexplicitlyprovidedhumanvalues.
shotlearningwithOPT-175Bandexistingtext
augmentationmethods. Wesuggestthatusing
classifierswithexplicithumanvalueinputim-
provesbothinclusivity&explainabilityinAI. (Hendrycks et al., 2020) or human preferences
(Christianoetal.,2017;Koren,2008).
1 Introduction
Value-alignment of AI systems is not a trivial
problem as human values are non-consensual by
The demand for responsible NLP technology –
nature(Haneletal.,2018). Valuescanbeverydi-
to make it more robust, inclusive and fair, as
verse and most existing works have attempted to
well as more explainable and trustworthy – has
alignmachineswithsharedhumanvaluesoraver-
increased since pre-trained large-scale language
age norms, or from a certain cultural perspective
models (LLMs) have brought about significant
withcrowdsourcedannotations(Jiangetal.,2021).
progressinmakingNLPtasksmoreefficientand
Thesedays,forinstance,manysocietiesagreethat
broad-ranging (Brown et al., 2020; Zhang et al.,
sexism should be eliminated, and we expect ma-
2022;Chowdheryetal.,2022;Radfordetal.,2019;
chines to be non-sexist, but different individuals
Brown et al., 2020; Petroni et al., 2019; Madotto
andculturesmayperceivesexismdifferently. As
et al., 2020). Researchers have studied how to
isshowninFigure1,thesamecontentcanbecon-
align machines with human values as one of the
sideredtobesexistornon-sexistdependingonthe
directionstoachieveresponsibleAItechnologyby
valuesprovidedtomakethejudgements.
teaching machines about moral and social norms
(Forbes et al., 2020; Emelin et al., 2020; Jiang Inthispaper,weproposeavalue-alignedjudge-
et al., 2021), ethics and common human values menttaskthatseparatesthevaluedefinitionprocess
from the development of the models for more in-
∗ Equalcontribution.
clusive and explainable value-aligned NLP. Our
† Theauthorcontributedtotheoriginalideaasapart
ofresponsibleAIprojectforMetaAI. proposedtaskaimstobuildasinglemodeltomake
2202
tcO
41
]LC.sc[
1v25670.0122:viXra
dynamicjudgementsbasedonexplicitlyprovided 2 RelatedWork
humanvalues,requiringthemodeltounderstand
Human Value Alignment One challenge in
thevalueanditscorrespondingentailmentonthe
valuealignmentisvaluedefinition,andtherehas
givencontent. Thevalueisprovidedintheformof
beenaprofusionofdocumentsonAIethicalstan-
instructions,allowingcoarse-to-finecustomization.
dards(Gabriel,2020;Dignum,2017). Jobinetal.
We start with value-aligned sexism classification
(2019) identified 11 clusters of ethical principles
asaproofofconceptfortheproposedapproach,as
among84documents,andFjeldetal.(2020)found
sexismisoneofthemostrepresentativeexamples
eightkeythemesacross36ofthemostinfluential
ofvaryingculturalperspectives.
of them. However, since human values are vari-
able with culture, we anticipate value definition
to be dynamic. Meanwhile, the values should be
We also present Value-Aligned Models (VA-
definedexternallytothedevelopmentoftheNLP
MODELs) that leverage value-based knowledge
algorithms,likehowweadoptdefinitionsofsexism
fromLLMs. LLMsaretrainedfromvastamounts
categoriesbasedonsocialstudies.
of human data with embedded human values
Toteachmodelsvalue-alignment,theliterature
(Hendrycks et al., 2020). However, they are not
has focused on improving the model’s reason-
controllableanditisdifficulttofine-tunesuchlarge
ing ability relating to human values and morality
modelswithexplicitvaluealignment. Instead,we
(Forbes et al., 2020; Emelin et al., 2020; Lourie
distillvalue-basedtrainingdatafromtheLLMsus-
et al., 2021; Hendrycks et al., 2020). Recently,
ingprompt-baseddatagenerationwithexampleval-
Solaiman and Dennison (2021) proposed to fine-
ues,andbuildVA-MODEL byfine-tuningsmaller
tuneGPT-3toadapttoamanuallycraftedvalues-
classification models with the distilled data. Ex-
targeteddatasettoarriveatavalues-targetedmodel.
perimentalresultsshowthatourapproachismore
However, in their approach, value alignment and
stableandaccuratethandirectlyapplyingfew-shot
definitionareintertwinedandentangledinaniter-
learning on LLMs. Moreover, our methodology
ative process. We instead separate the value defi-
avoidscostlyhumanlabelingorcrowdsourcingof
nitionandalignmentprocessmodelsaboutvalue-
values, allowing easier extensions to other value-
alignedjudgementwithexplicitvalueprovision.
alignedtasksindifferentdomains. Wefurtherin-
vestigatemodelperformanceusingdatagenerated
Prompt-basedLearning Recently,LLMshave
fromdifferentscalesandtypesofLLMs,andstudy
showngreatperformanceonprompt-basedlearning
theeffectofdatasizeforfine-tuning,andanalyze
(Brownetal.,2020;Chowdheryetal.,2022),which
the quality of the generated data. Moreover, we
doesn’trequirefine-tuning. Instead, themodelis
studythegeneralizationabilityof VA-MODELsby
directly fed a prompt that includes some exam-
testingitsperformanceonunseenvaluesets.
ples,andthemodelcangenerateresultsasifithas
“learned”. Studies on efficient prompt-learning/-
construction include Lu et al. (2021); Reynolds
Our contributions are as follows: 1) we intro-
andMcDonell(2021);Zhaoetal.(2021);Schick
duce the value-aligned classification task, where
andSchütze(2020). Weconsidertheliteraturefor
we first define human values externally and then
prompt-constructioninourmethodology.
use them at the instruction level in an in-context
learningparadigmandconstructvalue-alignedclas- KnowledgeDistillation Knowledgedistillation
sifierstomakepredictions;2)weproposetolever- isthetransferofknowledgefromteachertostudent
ageprompt-baseddatagenerationtodistillvalue- distribution (Hinton et al., 2015). Recent works
alignedknowledgefromLLMsforsmallerclassifi- haveattemptedtoperformdistillationfromLLMs
cationmodels;3)experimentalresultsindicatethat by prompting for text generation to show that it
ourapproachsignificantlyoutperformsstrongbase- outperforms existing text augmentation methods
lines,includingin-contextfew-shotlearningwith (Yooetal.,2021;Wangetal.,2022). (Westetal.,
LLMsandexistingtextaugmentationmethods;4) 2021)retrievescommonsenseknowledgesymbol-
wesystematicallystudyfactorsthatimpactprompt- icallyinatextformfromGPT-3fordownstream
baseddatagenerationandhighlightresearchques- taskswithhelpofsmallerfilteringclassifiers. We
tionsandchallengesinthevalue-alignedjudgement distill value-specific knowledge, not all abilities
taskthroughthoroughanalysis. of general language model, from LLMs through
value-alignedtrainingdatagenerationfortraining
smallervalue-alignedclassifiers. Thisreducesthe
costofhumanlabelingandalsoenablesbuilding
smallermodelsspecializedforvalue-alignedjudg-
menttask.
3 Value-AlignedJudgementTask
3.1 TaskDescription
Asanefforttoalignmachineswithhumanvalues, Figure 2: Illustration of the construction of our pro-
ourtaskfocusesonteachingthemodelthatdiffer- posed VA-MODEL. Using LLMs, we first create syn-
ent values can lead to different judgements even thetic value-aligned training data. Then, we transfer
given the same content. The task is formulated theknowledgeintosmallermodelsbyfine-tuningthem
onthedata,soValueAlignedModelscanmakevalue-
as follows. A model needs to make a judgement
alignedjudgements.
Y oncontentC basedonanexplicithumanvalue
V
V. In this work, “value” refers to any qualities,
standardsofbehavior,orbeliefsthatindividualsor
derstanding of sexism has been emphasized (Jha
societieshold,andisexpressedinnaturallanguage
andMamidi,2017;Sharifiradetal.,2018;Parikh
phrasesorsentences. Thesetofvaluesisexternally
et al., 2019). This aligns with our motivation for
definedbyahumanuserofthesystemorfromex-
explicit value-aligned judgement. Lastly, values
istingrelevantliteratureonmoralphilosophy,and
related to sexism are complicated, involving reli-
is independent of the development of algorithms.
gious,cultural,andpersonalbeliefsorvalues. We
Thedistinctionfromtheexistingvalue-alignedclas-
thusbelieveitisataskwithenoughcomplexityto
sificationtaskandconventionalclassificationtasks
actasacasestudy.
is that our task expects the model to incorporate
explicitlyprovidedvaluesalongwithotherinputs
4 Methodology
formakingjudgements.
Weseparatetheprocessofvaluedefinitionfrom
There is no existing resource for training value-
the development of the value-aligned models so
aligned classification models. We therefore pro-
thatthemodelscanlearntomakedynamicjudge-
pose to leverage LLMs for generating synthetic
mentsbasedonexternalvalues. Forinstance, ex-
trainingdata. LLMshavebeenfoundtolearnsig-
istingsexismclassifiersimplicitlylearnafixedset
nificantamountsofinherentknowledgeaswellas
ofdefinitionsofsexismfromlabeleddata,sothe
human values during pre-training (Petroni et al.,
contentwillbejudgedbasedonthesestaticvalues.
2019; Hendrycks et al., 2020; West et al., 2021;
Ourtaskrequiresthemodeltopredictdynamicla-
Robertsetal.,2020). However,thedirectusageof
belsdependingonthedifferentexplicitvalueseven
LLMsinzero-shotsettingforNLPtaskscanbeun-
whenthecontentisthesame.
stableandstilllimited(Weietal.,2021). Therichly
embeddedknowledgeinLLMsneverthelessmakes
3.2 Value-alignedSexismClassification
themgoodresourcegenerators. Therefore,weat-
We showcase the value-aligned judgement task
tempttobuildvalue-alignedmodels(VA-MODELs)
with an application to sexism classification. The
through fine-tuning smaller models on the value-
model needs to judge whether natural language
alignedtrainingdatageneratedbyLLM(s).
content is sexist or non-sexist based on a given
Ourproposedmethod(Figure2)consistsoftwo
valueV. Ifthevalueisnotapplicableorirrelevant,
steps: 1)promptinghumanvalue-alignedcontents
the model needs to predict that it is not applica-
fromLLMsbyprovidingexplicithumanvaluesand
ble (NA). Our rationale for choosing the sexism
instructions,and2)fine-tuningsmallerLMsonthe
classification task is that the definition of sexism
generateddatatoteachthemaboutvalue-aligned
haschangedovertimeasvalueshaveevolvedand
judgements. Formally,webuildVA-MODEL (pa-
alteredanditstillvariesacrosscultures. Thus,we
rameterizedbyθ)tomaximizethefollowinglikeli-
can verify the effect of varying values in a more
hood:
evident manner in the sexism classification task.
Furthermore,theimportanceofafine-grainedun- L(θ) = logP(Y|V,C;θ). (1)
4.1 Value-AlignedKnowledgeDistillation: 4.2 Fine-tuningSmallerModels–
Prompt-basedDataGeneration Value-AlignedModels
Inthenextphase,webuildclassifiersbyfine-tuning
Prompt Construction with Few-shot Examples
relativelysmallertransformer-basedmodels(e.g.,
The prompt construction of in-context few-shot
ALBERT-base,RoBERTa-base,BART-base)with
examplesaffectperformance. Thuswerefertothe
thegeneratedtrainingdatatoenablethemtomake
existingliteratureondifferentprompt-techniques
value-aligned judgements. We add a linear layer
(ReynoldsandMcDonell,2021;Zhaoetal.,2021;
ontopofthepooledoutputofthesmallermodels
Yoo et al., 2021). For the few-shot examples, we
toconstructourproposedVA-MODEL. Inorderto
createapoolof10human-labeledsamples(value,
makethemodelintakebothvaluesandcontentin
content, and value-aligned labels) for each value.
thelearningphase,theinputtextisformattedinto
AccordingtoLuetal.(2021),theorderofthefew-
“value [sep] content [sep]”andtheoutputis
shot samples in the prompt affects the in-context
avalue-alignedjudgement.
learningforLLMs. Therefore,werandomlyselect
The classifiers need to predict different labels
andorderfivesamplesoutofthepool.
according to explicitly provided values given the
Toselectthemostappropriatepromptforgener- same content. Recalling the example of value-
atingvalue-alignedsynthesizeddata, wetestfive alignedsexismclassificationinFigure1,thesame
candidate prompt templates with reference to lit- contentcanbeconsideredtobesexist,non-sexist
erature. All prompt templates consist of a label, orNAdependingontheconsideredvalues.
avalue,andvalue-alignedcontentexamples. The
5 Experiments
best-performingprompttemplateisselectedbased
ontestingwithasmallersizeofthesamples. The
Inthispaper,weconductvalue-alignedsexismclas-
prompttemplatesandtheirperformanceareavail-
sification. Models are expected to label content
ableinAppendixB.
withlabelchoicessexist, non-sexist, NAdepend-
ingonexplicitlyprovidedvalues.
Generation We feed the prompts to LLMs to
5.1 Dataset
generatevalue-alignedsynthetictrainingsamples.
Weborrowmulti-labelsexismcategorizationdata
Our method is model agnostic in that any LLMs
(multi-sexism)(Parikhetal.,2019),whichoffers
canbeadoptedforthisstep. Recently,LLMshave
fine-grainedsexismcategorizationforsexistcon-
scaledtomorethan500billionparameters(Chowd-
tent. Examplecategoriesinclude,butarenotlim-
hery et al., 2022; Smith et al., 2022), and some
itedto,Role-stereotyping,Paygap,andMansplain-
modelswithmorethan100billionparametersare
ing. We select 10 items of content per category
availablepublicly,suchasJurassic-1Jumbo(GPT-
to have a small set of human-labeled data for the
Jurassic Lieber et al. (2021)), Open Pre-trained
prompt-constructioninourmethodologyandbase-
Transformer(OPTZhangetal.(2022)),andGPT-3.
lines. Therestofthedataareusedasthetestset.
In this paper, we choose OPT-175B for the main
Based on the description of each category, we
experimentandprovideananalysisontheeffects
manuallycomposetwoopposingvalues–onemak-
ofthesizeandtypesofLLMs.
ing the content sexist (value) and another mak-
ing the content non-sexist (counter-value). For
Generated Content Extraction & Processing instance, any Role Stereotyping contents will be
Thegeneratedcontentisgeneratedinsuccessionaf- considered to be sexist based on the value “Men
terthepromptasnaturaltext,andextractedthrough andwomenareequallycapableforanyrole,”but
patternmatching. Wegatherallextractedcontent can also be considered to be non-sexist with the
toconstructasynthetictrainingsetforteachingthe differentvalue“Menandwomenarebiologically
smaller models in the next step, and process the different; hence certain roles are more appropri-
generated data as follows. Firstly, we keep only ate for women.” A full list of values and counter
uniquesamplesbydroppingallduplicates. Then, values is available in Appendix A.1. In total, we
weremoveexactcopiesofthefew-shotexamples consider 19 categories of sexism and two corre-
usedintheprompts. Finally,anycontentlessthan sponding values (value, counter-value) for each
threewordsisfilteredoutasitislessinformative. category,translatedinto38(19×2)humanvalues.
Test set We use the original multi-label sexism construct VA-ALBERT, VA-ROBERTA and VA-
content (human-labeled, non-synthetic) for creat- BART,respectively. RoBERTahasbeenprovedto
ingatestsetforthevalue-alignedjudgementtask, berobustinvariousNLPtasksandBARTshows
excludingthatusedforprompt-constructioninthe comparable performance to RoBERTa on GLUE
training data generation. Originally, each item tasks.
of content is labelled with one/multiple sexism
5.2.2 Baselines
categories. For our task setup, we translate the
data into the form of triplet {content, value, la- Toexamineourproposedapproach,wecompareit
bel},andweassignvalue-dependentlabelstoeach withmultiplebaselines,includingarandombase-
sample. Forinstance,ifcontentC wasoriginally line, prompt-based few-shot learning with OPT-
labelledasRole-stereotyping(RS),weconvertinto 175B, and fine-tuning transformer-based models.
three testing samples, {C, value , Sexist}, {C, For the fine-tuning setting, we fine-tune on dif-
RS
counter-value , Non-Sexist}, and {C, random ferentdatasetups–onlywithhuman-labeleddata
RS
value/counter-value,NA}. NotethatvaluesforNA (withoutgenerateddata)andwithsemanticallyaug-
labelsaretotallyunrelatedtothecontentcategory. menteddata.
In this way, we can inspect the model’s perfor-
Random Baseline We randomly select the pre-
manceinmakingavaluejudgementonthesame
dicted label for each test sample with the same
content with different values. In total, there are
labelprobabilitydistributionasinthetrainingdata.
17,720testsamples,withalabelratioof1:1:1.
OPT-175B (few-shot) This baseline uses OPT-
5.2 Models
175B with a prompt-based few-shot learning for
5.2.1 VA-MODELs(Ours) labelprediction.2 Weprovide20few-shotsamples
Generatingvalue-alignedtrainingdata Using inthecontext.
the method explained in Section 4.1, we get 100
Human-Labeled(HL)-Models Weonlyusethe
contentpiecesfromeachofthevalueandcounter-
smallsubsetofhuman-labeledsamplesastraining
valueprompts. Insum,thereare200uniquepieces
data to fine-tune smaller transformer-based LMs
of content per category.1 Then, all content per
with a linear layer trained on top. We choose the
categoryispairedwithavalueandcountervalue
base versions of ALBERT, RoBERTa and BART
andcorrespondinglabels{content,value,‘Sexist’}
asthebackbonemodelsforafaircomparisonwith
and {content, counter-value, ‘Non-Sexist’}. So,
our VA-MODELs.
eachcontentitemhasaduplicatebutispairedwith
differentvaluesandvalue-alignedjudgements. To Nlpaug-Models Nlpaug(Ma,2019)issemantic
preventthemodelfromonlylearningtwovalueand augmentationmethodusingBERT-baseembedding.
labelassociations,wesyntheticallymaketheclass We conduct augmentation with prompt construc-
‘NA’byassigningirrelevantvalues/counter-values tion examples by insertion and substitution. For
tothecontent(e.g.,assigningthevalueofPayGap each examples, we make 10 augmented samples
to a content of Role Stereotyping so the label is (five insertions and five substitutions). Then, we
‘NA’).Intotal,thereare10,722samples,including fine-tunethebaseversionsofALBERT,BARTand
the prompt construction samples. We split them RoBERTaonthesemanticallyaugmenteddataand
intotrainingandvalidationsetswitharatioof4:1. prompt-constructionexamplessowecanevaluate
theeffectivenessoftheprompt-basedaugmentation
Building VA-MODELs We finetune smaller
inourmethod.
modelswiththegeneratedvalue-alignedtraining
data. We build VA-MODELs to incorporate ex- 5.3 Experimentalsetup
plicithumanvaluestomakejudgementsforvalue-
Evaluationmetric Weevaluateourexperiments
alignedsexismclassificationfollowingSection4.2.
with both F1 score and accuracy. For the main
For the smaller models, we take base versions
results,wereportallaccuracy,weightedF1-score
of ALBERT (12M params.) (Lan et al., 2019),
(W-F1),precisionandrecall.
RoBERTa (125M params.) (Liu et al., 2019) and
BART (110M params.) (Lewis et al., 2019) to 2Weuseprompt-basedfew-shotlearningwithOPT-175B
forgeneratingvalue-alignedcontentinourmethodologywhile
1Reflectingtheoriginalratioofmulti-sexism,wekeep the baseline used it for directly predicting label. Refer to
theoriginalnumberofsamplesiftherearelessthan100. AppendixCfordetails.
Model Accuracy W-F1
RandomBaseline 33.53 33.53
0.41 0.41
OPT-175B(few-shot) 55.18 54.78
7.75 7.20
HL-ALBERT 58.70 51.67
4.43 3.96
HL-RoBERTa 64.53 55.23
2.54 1.91
HL-BART 63.23 54.93
1.87 1.47
Nlpaug-ALBERT 62.87 58.80
2.13 3.44
Nlpaug-RoBERTa 61.52 58.67
3.03 2.89
Nlpaug-BART 59.03 58.49
1.38 1.60
VA-ALBERT 70.10 1.65 70.75 1.48 Figure3: EvaluationresultsofVA-BARTpersexism
VA-ROBERTA 73.24 0.39 73.82 0.32 category on the test set. Only the top and bottom five
VA-BART 74.07 0.82 74.36 0.60 categories(basedonW-F1)aredisplayed. Theperfor-
mancefortheninecategoriesinthemiddleare∼80%
Table 1: Evaluation results of baselines and our pro- for Acc. and W-F1. The full results for the 19 cate-
posed VA-MODELs, on the value-alignment task. We goriesareavailableinAppendixE.
use200value-alignedtrainingdatasamplesgenerated
from the LLMs per category to fine-tune VA-MODEL.
Experiments are ran with five random seeds and re- performanceisstilllow. HL-ModelssurpassOPT-
sults are reported in mean format. All our VA- 175B (few-shot) under all evaluation metrics ex-
std
MODELperformancesarestatisticallysignificant(t-test ceptHL-ALBERTinW-F1score,showingthatthe
withp-value<0.05). Scoresareallinpercentage(%). models can capture our task with limited human-
labeleddataduetotheeffectivenessoffine-tuning.
Nlpaugisoneoftheconventionaldataaugmenta-
Implementation Details For generating value-
tionapproachesandweaugmentthesameamount
aligned training data, we conduct the main ex-
of data as VA-MODEL. In comparison with HL-
periment with OPT-175B model with top-p 0.7
Models,Nlpaug-modelsshowhigherW-F1scores
andtemperature1. ForourVA-MODELsandHL-
withsmalldropsinaccuracy.
Modelsbaselines,weusepre-trainedtransformer-
Overall,theexperimentalresultssupportourpro-
basedLMsavailablethroughtheHuggingFaceAPI.
posed approach for the value-aligned judgement
Furtherimplementationdetailssuchashyperparam-
task. OPT-175B(few-shot)showsmuchlowerand
etersaregiveninAppendixC.
unstableperformancethanVA-MODELsalthough
6 ResultsandAnalysis
thevalue-alignedtrainingdataofVA-MODELsis
generatedfromOPT-175B.Fortheprompt-based
6.1 MainResults few-shotapproach,especiallywhenthetasksetup
iscomplicatedlikevalue-alignedclassification,the
Effectivenessofourmethod Table1showsthe
modelcannoteasilyoverfitthetaskbygivingsev-
performance of the models on the value-aligned
eralprompts,leadingtoahigherchancetopredict
sexismclassificationtask. Ourmodelsachievebet-
randomlabels. Instead,weusedaknowledgedis-
terscoresonW-F1andaccuracythanthebaselines
tillationapproachthroughtrainingdatageneration,
by large gaps (15.56 ∼ 40.83% gain in W-F1),
whichisasimplertaskforthemodelasthemain
which signifies the robustness and superiority of
objectiveofthegenerallanguagemodelistextgen-
ourapproach. OurVA-ALBERTalsosurpassesall
eration. Moreover,utilizingtheLLMsforgenerat-
baselines,includingthoseback-bonedwithbigger
ingknowledgedistilleddataismoreeffectivethan
models(e.g.,Nlp-RoBERTa,HL-RoBERTa). This
simplesemantictextaugmentation(e.g.,Nlpaug).
highlights the effectiveness of the value-aligned
knowledgedistillationwithLLMs. Per-Category performance Figure 3 presents
WeobservethattheOPT-175Bfew-shotlearning theper-categoryevaluationscoresof VA-BART.
approachperformsbetterthanrandomlabelassign- Theresultsvarysignificantlybetweencategories,
ments on the test set and HL-ALBERT, but still indicatingthecomplexityofourproposedtask. The
performsworsethanorascomparableastheother resultsforbothMenstruation-relatedDiscrimina-
baselines. ThisindicatesthatLLMswithprompt- tionandPayGapachievescoreshigherthan90%,
basedfew-shotlearningcanunderstandthevalue- while the results for Internalized Sexism are rela-
alignedclassificationtasktosomeextent,butthe tivelylow. Weconjecturereasonsforthehighper-
Model Accuracy W-F1
VA-ALBERT 70.101.65 70.751.48
w/ohumanlabeleddata 70.791.40 71.291.33
VA-ROBERTA 73.240.39 73.820.32
w/ohumanlabeleddata 72.902.06 73.191.68
VA-BART 74.070.82 74.360.60
w/ohumanlabeleddata 72.301.24 72.710.90
Table 2: Effectiveness of generated data. We remove
human-labelleddatafromthetrainingsetandonlyuse
synthetic samples generated from LLM for training
Figure 4: Vocabulary overlaps (%) of the generated (w/o human-labeled data). The minimal drops in per-
dataamongsexismcategories. Onlytop-3andbottom- formanceshowtheeffectivenessofvalue-alignedtrain-
3categoriesaredisplayedindescendingorderofW-F1 ingdatageneratedfromLLMsforthevaluealignment
(toptobottom;left-to-right).FullsetisinAppendixD. task.
formanceofcertaincategoriesarevaryingquality dataareevaluatedtoberelevantandsexist(R&S)
ofgeneratedtrainingdatapercategoriesandmore 69.44%ofthetime(R: 75.93%,S: 87.03%)while
distinguishablefeaturesthanother. Weinvestigate internalized sexism data are evaluated to be 25%
thispointfurtherinSection6.2. (R&S) (R: 34.50%, S: 67.50%). We observe
thatthequalityofPaygapgenerateddataismuch
6.2 QualityAnalysisforGenerated
better than that of Internalized sexism, which is
Value-AlignedTrainingData
consistentwiththepercategoryresultsinFigure3.
Distinction between generated data & test set This highlights the difficulty of our task and the
Thevocabularyoverlapbetweenallgenerateddata needformorerobustprompttemplatesforprompt-
(trainingsetfor VA-MODELs)andtestsetdatais baseddatageneration. Andthehuman-in-the-loop
51.79%. Moreover, we check how many of gen- methodmayfurtherboosttheperformanceofour
erated data samples that share more than 80% of approachwithlessnoisydata.
vocabularywithatleastoneofthetestdatasamples,
findingthatonly0.01%ofgenerateddatasamples Effectiveness of generated training data To
reachthethreshold(80%). Therefore,thedatagen- investigate the standalone effectiveness of the
eratedfromOPT-175BfortrainingVA-MODELsis generatedtrainingdata(value-alignedknowledge
clearlydistinctfromthetestset. distillation), we study the performance of VA-
MODELs when they are trained without any of
Diversity of Data We calculate the vocabulary
human-labeled data but only with generated data
overlaps for each sexism category of the gener-
(Table2). Minorperformancedegradationsinboth
ateddatainFigure4. Weobservethatthevocabu-
VA-BART and VA-ROBERTA are investigated,
laryoverlapsaregenerallysmall,whichillustrates
−1.65%and−0.63%W-F1respectively. However,
that OPT-175B can generate diverse data for dif-
thesevaluesarestillabovethoseofthebaselines.
ferentvalues(e.g.,sexismcategories)providedin
Interestingly,VA-ALBERTshowedaminimalper-
prompts. Wecanobservethetrendthattheoverlaps
formance gain on both accuracy and W-F1. This
amonghighperformingcategoriesaresmall,espe-
indicatesthatthevaluealignmentknowledgedis-
cially Pay gap and Menstruation related, which
tilledfromLLMsisthemaincontributorfor VA-
make data sample distinguishable to others. In
MODELtounderstandthetask.
contrast, lowperformingcategories, overlapsare
relativelyhigher. 6.3 GeneralizationAbilityonUnseenValues
Humanevaluation LLMsarepowerfulfew-shot To understand capacity of models to generalize
learners,yettheyarenotperfect. Thus,weconduct value-aligned judgement over unseen values, we
human evaluation on two categories’ data (Inter- conduct an experiment in which three randomly
nalizedsexismandPaygap)tofurtherinvestigate selectedsexismcategoriesareseparatedfromthe
the augmented data quality. We assess generated trainingprocess(i.e.,modelshaveneverseenval-
contentsfromtwoaspects: 1)relevancetothecor- ues related to the three categories in the training
respondingcategory(R); 2)sexism(S). Paygap phaseandareevaluatedontestsetonlycomposed
Model Accuracy W-F1 Models Accuracy W-F1
OPT-175B(fewshot) 32.97 30.23 VA-BART(OPT-1.3B) 65.72 2.03 66.50 2.15
HL-ALBERT 40.25 6.05 37.52 7.68 VA-BART(OPT-6.7B) 65.69 2.28 66.44 2.45
HL-RoBERTa 47.79 5.65 45.35 6.09 VA-BART(OPT-175B) 74.07 0.82 74.36 0.60
HL-BART 46.09 3.42 45.97 3.68 VA-BART(GPT-Jurassic6B) 69.09 1.59 69.89 1.49
Nlpaug-ALBERT 48.10 11.0 40.62 8.05 VA-BART(GPT-Jurassic17B) 71.03 1.01 71.68 0.90
Nlpaug-RoBERTa 40.14 2.00 30.64 3.56 VA-BART(GPT-Jurassic178B) 74.04 1.16 74.24 0.91
Nlpaug-BART 47.76 42.40
3.89 5.45
VA-ALBERT 55.15 53.14 Table 4: Effect of size and types of LLMs on value-
6.83 9.05
VA-ROBERTA 58.13
5.33
56.60
6.56
aligned training data generation. We prompted OPT
VA-BART 57.98 55.94 and GPT-Jurassic ranging 1.3B ∼ 178B. The bigger
5.12 5.48
themodel,thebetterthefinalperformanceinthevalue
Table 3: Performances of VA-MODELs and baselines alignment task. All VA-BART variations are fine-
onunseenvaluesinvalue-alignedsexismclassification. tunedwiththesamenumberoftrainingsamples.
In the training phase, models did not see any of the
valuesinthetestset.
of those unseen values) and the results are pre-
sentedinTable3. Overall,therearedropsinper-
formance compared to the main experiment (Ta-
ble 1), while all of our VA-MODELs continue to
outperform all baselines. The baselines experi-
encelargerdrops(maximum43.18%forNlpaug-
RoBERTa)thanthe VA-MODELs(17.22%forVA- Figure 5: Evaluation results (W-F1) of VA-
ROBERTA). Considering the model was never MODELsoverdifferentsizeofgeneratedtrainingdata.
taughtorreceivedanydirectsupervisiononthetest
values,itisexpectedbehaviorasothergeneraliza-
generateddatacangainfurtherimprovements,we
tionproblem. Weleavehowtoimprovethemodels’
fine-tuneVA-MODELswithdifferenttrainingdata
generalizationabilityinvalue-alignedjudgement
size. In Figure 5, we show that the W-F1 score
taskforfuturework.
doesnotshowanygainwhenthesizeexceeds200
samplesexceptforVA-ALBERT.Asweanalysed
6.4 AblationStudies
in Section 6.2, the generated data has noise. We
LLMscapacityforprompting Wefirstinvesti- conjecture that when using more generated data,
gatehowthesizeofLLMsaffectsthecapacityfor theadditionaldatawillnotonlybringmorevalue
generatingvalue-alignedtrainingdatabyevaluat- alignmentknowledge,butalsoaddmorenoiseto
ing the final performance of VA-MODEL trained thetrainingset. Therefore,whenthedegradation
on data from varying sizes of LLMs. Unsurpris- in model performance caused by the noisy data
ingly, as is shown in Table 4, we can continually is greater than the improvement in model perfor-
boost the model’s performance when the LLMs mancefromtheadditionalknowledge,theoverall
sizeincrease. resultsdecrease.
WealsotrainVA-BARTwiththedataprompted
from GPT-Jurassic. Results for GPT-Jurassic 6B 7 ConclusionandFutureWork
are slightly higher than those of OPT-6.7B, al-
In this paper, we propose a task that focuses on
thoughthemodelsizeissmaller. However,when
teaching a model human value alignment knowl-
theLLMsbecomeextremelylarge,GPT-Jurassic
edge. We also introduce value-aligned models
178B performs similar to OPT-175B with only
0.12%difference. Sincesimilarmodelsizesshow
(VA-MODEL)thatgeneratevalue-alignedtraining
datafromLLMsbyprompt-baseddatageneration
similarperformancewithminimaldifferences,the
and fine-tune smaller classification models with
types of LLMs do not have much effect on the
the value-aligned generated training data. Exper-
generateddataqualityforourtask.
imental results show that VA-MODEL generally
Effectofthesizeofgeneratedtrainingdata To outperformsstrongbaselines. Furtheranalysisil-
investigate whether increasing the the number of lustratesthatthegenerateddatafromlargerLLMs
helps increase the performance, and more gener- We propose that the human value definition
ated data can cause performance reduction when shouldbedecoupledfromthevalue-alignmenttask.
thedatasizeistoolarge. Inaddition,wealsotest Theformershouldbedefinedtogetherwithsociety,
thepromisinggeneralizationabilityofVA-MODEL. ethicists,socialscientists,andsoon. However,the
Finally,wehighlightsseveralresearchchallenges mechanicsofvaluealignmentshouldbeindepen-
forfuturework: improvementsin1)therobustness dentofthefirstpartsothatengineersdonotdefine
ofthemodelondiversevalues,2)themodels’gen- thesevaluesdirectlyinthetrainingdataorinthe
eralizationabilityforourvalue-alignedjudgement codeitself.
task, 3) higher quality generated data with more
humancuration.
References
Limitations
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
OurmethodologyiscurrentlytestedwithonlyEn-
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
glish. Weconjecturethatthemethodologyshould Amanda Askell, Sandhini Agarwal, Ariel Herbert-
beapplicabletootherlanguages,butmaybelim- Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
itedbythecapacityofLLMsinthosespecificlan-
Clemens Winter, Chris Hesse, Mark Chen, Eric
guages. Itispossiblethatvalue-alignedknowledge
Sigler,MateuszLitwin,ScottGray,BenjaminChess,
distillation may be more difficult with languages Jack Clark, Christopher Berner, Sam McCandlish,
fromcountriesandregionsthatdonothaveacom- Alec Radford, Ilya Sutskever, and Dario Amodei.
pletesetofhumanvaluedefinitions. Thus,explor- 2020. Language models are few-shot learners. In
AdvancesinNeuralInformationProcessingSystems,
ing the value-aligned task in different languages
volume 33, pages 1877–1901. Curran Associates,
other than English is a promising research direc- Inc.
tion.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Our main experimental results are based on a
Maarten Bosma, Gaurav Mishra, Adam Roberts,
175Bparametermodel,whichrequireslargeGPU
Paul Barham, Hyung Won Chung, Charles Sutton,
resourcesoraccessthroughanAPI.Thismayhin- Sebastian Gehrmann, et al. 2022. Palm: Scaling
derotherresearchersfromreproducingexperimen- language modeling with pathways. arXiv preprint
arXiv:2204.02311.
talresults. Additionally,weexploreddifferentsizes
of LLM including 1B and 6B models, which do
PaulFChristiano,JanLeike,TomBrown,MiljanMar-
notrequirelargeGPUresources,andshowedthey tic, Shane Legg, and Dario Amodei. 2017. Deep
canachievecomparableresults. Wehopetheycan reinforcementlearningfromhumanpreferences. In
AdvancesinNeuralInformationProcessingSystems,
bepossiblealternativeoptionsforresearcherswho
volume30.CurranAssociates,Inc.
maynothaveaccessto100B+models.
Althoughsexismisasuitablecasestudyforus Virginia Dignum. 2017. Responsible artificial intelli-
toinvestigatethefeasibilityofthevaluealignment gence: designing ai for human values. Daffodil In-
ternationalUniversity.
taskaswehaveshownthroughoutthiswork,itis
stillonedomain. Furtherexpansiontodifferentdo- Denis Emelin, Ronan Le Bras, Jena D Hwang,
mainsorvalue-alignedclassificationtaskssuchas Maxwell Forbes, and Yejin Choi. 2020. Moral
stories: Situated reasoning about norms, intents,
thedetectionofracism,toxicity,otherthansexism,
actions, and their consequences. arXiv preprint
areneeded.
arXiv:2012.15738.
EthicsStatement Jessica Fjeld, Nele Achten, Hannah Hilligoss, Adam
Nagy, and Madhulika Srikumar. 2020. Principled
In this work, we conduct experiments with some artificial intelligence: Mapping consensus in ethi-
values that are unconventional in and counter to cal and rights-based approaches to principles for ai.
BerkmanKleinCenterResearchPublication.
the current contemporary society. However, we
arenotsuggestingtoleranceonsexistbehaviorsor Maxwell Forbes, Jena D. Hwang, Vered Shwartz,
beliefs. Instead,weexplaintheexistenceofdiffer- Maarten Sap, and Yejin Choi. 2020. Social chem-
entperspectivesinthediscussionofsexismacross istry101: Learningtoreasonaboutsocialandmoral
norms. In Proceedings of the 2020 Conference on
cultures or religions. Our value-aligned sexism
EmpiricalMethodsinNaturalLanguageProcessing
classificationtaskisacasestudyofthisdecoupled
(EMNLP), pages 653–670, Online. Association for
process. ComputationalLinguistics.
Iason Gabriel. 2020. Artificial Intelligence, Values, NicholasLourie,RonanLeBras,andYejinChoi.2021.
and Alignment. Minds and Machines, 30(3):411– Scruples: A corpus of community ethical judg-
437. ments on 32,000 real-life anecdotes. Proceedings
of the AAAI Conference on Artificial Intelligence,
Paul HP Hanel, Gregory R Maio, Ana KS Soares, 35(15):13470–13479.
Katia C Vione, Gabriel L de Holanda Coelho,
Valdiney V Gouveia, Appasaheb C Patil, Shan- Yao Lu, Max Bartolo, Alastair Moore, Sebastian
mukh V Kamble, and Antony SR Manstead. 2018. Riedel, and Pontus Stenetorp. 2021. Fantastically
Cross-culturaldifferencesandsimilaritiesinhuman orderedpromptsandwheretofindthem: Overcom-
valueinstantiation. FrontiersinPsychology,9:849. ingfew-shotpromptordersensitivity. arXivpreprint
arXiv:2104.08786.
Dan Hendrycks, Collin Burns, Steven Basart, Andrew
Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Edward Ma. 2019. Nlp augmentation.
2020. Aligningaiwithsharedhumanvalues. arXiv https://github.com/makcedward/nlpaug.
preprintarXiv:2008.02275.
Andrea Madotto, Zihan Liu, Zhaojiang Lin, and Pas-
cale Fung. 2020. Language models as few-shot
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
learner for task-oriented dialogue systems. arXiv
Distillingtheknowledgeinaneuralnetwork. arXiv
preprintarXiv:2008.06239.
preprintarXiv:1503.02531.
Pulkit Parikh, Harika Abburi, Pinkesh Badjatiya, Rad-
AkshitaJhaandRadhikaMamidi.2017. Whendoesa
hika Krishnan, Niyati Chhaya, Manish Gupta, and
compliment become sexist? analysis and classifica-
Vasudeva Varma. 2019. Multi-label categorization
tionofambivalentsexismusingtwitterdata. InPro-
ofaccountsofsexismusinganeuralframework. In
ceedings of the second workshop on NLP and com-
Proceedings of the 2019 Conference on Empirical
putationalsocialscience,pages7–16.
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
LiweiJiang,JenaDHwang,ChandraBhagavatula,Ro-
guage Processing (EMNLP-IJCNLP), pages 1642–
nanLeBras,MaxwellForbes,JonBorchardt,Jenny
1652,HongKong,China.AssociationforComputa-
Liang, Oren Etzioni, Maarten Sap, and Yejin Choi.
tionalLinguistics.
2021. Delphi: Towards machine ethics and norms.
arXivpreprintarXiv:2110.07574.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Anna Jobin, Marcello Ienca, and Effy Vayena. 2019.
AlexanderMiller.2019. Languagemodelsasknowl-
Thegloballandscapeofaiethicsguidelines. Nature
edge bases? In Proceedings of the 2019 Confer-
MachineIntelligence,1(9):389–399.
ence on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
YehudaKoren.2008. Factorizationmeetstheneighbor-
ence on Natural Language Processing (EMNLP-
hood: A multifaceted collaborative filtering model.
IJCNLP),pages2463–2473,HongKong,China.As-
In Proceedings of the 14th ACM SIGKDD Inter-
sociationforComputationalLinguistics.
national Conference on Knowledge Discovery and
Data Mining, KDD ’08, page 426–434, New York,
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
NY,USA.AssociationforComputingMachinery.
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
OpenAIblog,1(8):9.
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learn-
LariaReynoldsandKyleMcDonell.2021. Promptpro-
ing of language representations. arXiv preprint
gramming for large language models: Beyond the
arXiv:1909.11942.
few-shot paradigm. In Extended Abstracts of the
2021 CHI Conference on Human Factors in Com-
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
puting Systems, CHI EA ’21, New York, NY, USA.
jan Ghazvininejad, Abdelrahman Mohamed, Omer
AssociationforComputingMachinery.
Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.
Bart: Denoising sequence-to-sequence pre-training AdamRoberts,ColinRaffel,andNoamShazeer.2020.
for natural language generation, translation, and How much knowledge can you pack into the pa-
comprehension. arXivpreprintarXiv:1910.13461. rameters of a language model? arXiv preprint
arXiv:2002.08910.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav
Shoham. 2021. Jurassic-1: Technical details and TimoSchickandHinrichSchütze.2020. Few-shottext
evaluation. WhitePaper.AI21Labs. generation with pattern-exploiting training. arXiv
preprintarXiv:2012.11926.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Sima Sharifirad, Borna Jafarpour, and Stan Matwin.
Luke Zettlemoyer, and Veselin Stoyanov. 2019. 2018. Boosting text classification performance on
Roberta: A robustly optimized bert pretraining ap- sexist tweets by text augmentation and text genera-
proach. arXivpreprintarXiv:1907.11692. tion using a combination of knowledge graphs. In
Proceedings of the 2nd workshop on abusive lan- A AdditionalDatadetails
guageonline(ALW2),pages107–114.
A.1 ValueandCounterValue
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared Intotal,wehave19categoriesofsexismandtwo
Casper, Zhun Liu, Shrimai Prabhumoye, George correspondingvaluesforeachofthem. Weprovide
Zerveas, Vijay Korthikanti, et al. 2022. Using
thelistofallvaluesandcounter-valuespersexism
deepspeed and megatron to train megatron-turing
categoryinTable6.
nlg 530b, a large-scale generative language model.
arXivpreprintarXiv:2201.11990.
B PromptTemplateChoicefor
Irene Solaiman and Christy Dennison. 2021. Process
Value-alignedDataGeneration
for adapting language models to society (palms)
withvalues-targeteddatasets. InAdvancesinNeural
Final Prompt For prompting thevalue-aligned
Information Processing Systems, volume 34, pages
5861–5873.CurranAssociates,Inc. trainingsamples,weselectedthefinalprompttem-
plateoutofaset-offiveprompttemplatesbasedon
Boxin Wang, Wei Ping, Chaowei Xiao, Peng Xu,
experimental results with the OPT-175B model.
Mostofa Patwary, Mohammad Shoeybi, Bo Li, An-
The prompt template consists of an instruction
ima Anandkumar, and Bryan Catanzaro. 2022. Ex-
ploring the limits of domain-adaptive training for headerandanenumerationofcontentasfollows:
detoxifying large-scale language models. arXiv Generate {label} content that is relevant
preprintarXiv:2202.04173.
to the Value. Value:{value}\n.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Herethelabelandvalueinbracketsisreplaced
Guu, Adams Wei Yu, Brian Lester, Nan Du, An- withthetargetlabel, andweprovidefivecontent
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
examples with format Content:{content}, con-
guagemodelsarezero-shotlearners. arXivpreprint
catenatedtothepreviousprompt. Fortheseexam-
arXiv:2109.01652.
ples,Wemanuallycreatethetenmostrepresenta-
PeterWest,ChandraBhagavatula,JackHessel,JenaD tive examples that align with the described value
Hwang, Liwei Jiang, Ronan Le Bras, Ximing
andrandomlyselectfiveofthemforeachprompt.
Lu, Sean Welleck, and Yejin Choi. 2021. Sym-
Then,themodelisencouragedtogeneratecontent
bolicknowledgedistillation: fromgenerallanguage
models to commonsense models. arXiv preprint relevanttotheprovidedvalueandlabelitwiththe
arXiv:2110.07178. promptContent:.
Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-
Tried Prompt Templates We tried five prompt
Woo Lee, and Woomyoung Park. 2021. GPT3Mix:
templates,includingthefinalprompttemplateas
Leveraginglarge-scalelanguagemodelsfortextaug-
mentation. In FindingsoftheAssociationforCom- follows:
putational Linguistics: EMNLP 2021, pages 2225–
2239, Punta Cana, Dominican Republic. Associa- 1. Generate {label} content that
tionforComputationalLinguistics.
is relevant to the Value.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Value:{value}\n.
Artetxe,MoyaChen,ShuohuiChen,ChristopherDe-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2. “Each item in the following
2022. Opt: Open pre-trained transformer language
list contains a value and the
models. arXivpreprintarXiv:2205.01068.
respective "{label}" content
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and according to the value.Value:{value}
SameerSingh.2021. Calibratebeforeuse: Improv-
Content:{content}”
ing few-shot performance of language models. In
Proceedings of the 38th International Conference
3. “value="{value}"\n label="{label}"\n
on Machine Learning, volume 139 of Proceedings
ofMachineLearningResearch,pages12697–12706. content={content}”
PMLR.
4. “Value:{value} Label:{label}
Content:”
5. “Generate label content that is
relevant to the Value.\nValue:{value}
Content:{content}”
Wemainlyinvestigatedtheeffectivenessofthe for the Content based on the given Value:
different prompt templates with the OPT-175B Value. Content: Content Label:”
modelasweconductedthemainexperimentwith Intheprompt,theboldwordswillbereplacedby
it. WealsodidinvestigationwiththeGPT-Jurassic theactualdata. Thefirstsentenceisthefew-shot
6B model. Interestingly, the GPT-Jurassic mod- example and we repeat it N times by randomly
elsshowedbetterperformancewithdataprompted selectingfivesamplesforeachlabelcategory. The
withprompttemplate#2,whichwasdifferentfrom secondsentenceisthetestsample,andthemodel
OPT-175B.Thismayhaveresultedfromthediffer- will generate the corresponding label in the text.
enttrainingobjectivesandpre-trainingresources Duringgeneration,wesettop-p0.9andgenerate
of the models. Although the overall structure of labelsfivetimes. Finally,wecalculatetheaverage
ourmethodologyismodelagnostic,thereshould scoresamongtheresults.
besomeexplorationmadeonprompttemplatecon-
D Vocabularyoverlapsofgenerated
structiondependentonmodels.
trainingdataamongsexismcategories
TheexperimentalresultsareshowninTable5.
Figure 6 presents the vocabulary overlaps of the
VA-ROBERTA value-aligned training data generated from OPT-
Acc. Prec. Rec. W-F1
w/prompttype
175amongthesexismcategories. Wecalculatethe
1 73.91% 73.91% 75.14% 74.31% vocabulary overlaps for each sexism category of
2 72.71% 72.71% 75.22% 73.34%
thegenerateddata.
3 71.25% 71.25% 75.48% 72.06%
4 69.75% 69.75% 74.12% 70.60%
E PerCategoryResults
5 72.07% 72.07% 73.82% 72.61%
Figure 7 presents the evaluation results of VA-
Table5: Evaluationresultsof VA-ROBERTA trained
BART for each sexism category on the test set.
on OPT-175B generated data with different prompt
types. Weprompted120datasamplespercategories.
C ExperimentalDetails
Hyperparameters For hyper-parameters, we
performagridsearchtofindthebestperforming
setofparametersamongthelearningrates{1e-5,
5e-5}andbatchsizes{32,64}.
TrainingDetails Foreachmodelwetrainfora
maximumof 30epochs withearly-stopping with
patience of 5. Each experiment is conducted on
anNvidiaRTX3090device,andeachepochtakes
around2–10minutesdependingonthenumberof
trainingsamples.
RandomSeeds Weraneachofexperimentsfive
times with different random seeds and reported
the mean and standard deviation in a format of
mean .
std
OPT-175B (few-shot) Baseline Prompt For
eachtestsample, weconstructapromptwiththe
taskinstructionandseveralexamplesasshownbe-
low:
“Predict a Label for the Content based
on the given Value: Value. Content:
Content Label: Label \n Predict a Label
Category Value Counter-value
Rolestereotyping MenandWomenareequallycapableforanyrole. Menandwomenarebiologicallydifferenthencecer-
tainrolesaremoreappropriateforwomen.
Attributestereotyping Thedifferencesbetweenpeople’sphysical,psycho- Womenandmenlike/dislikedifferentthingsandthey
logical,orbehavioralqualitiesorlikes/dislikesare behavedifferentlybecausetheyhavebeenraisedand
notbasedongender. educateddifferently.
Bodyshaming Objectionable comments or behaviour concerning Womenandmenhavedifferentbodytypesorstan-
appearanceincludingthepromotionofcertainbody dardsandtheyshouldbepromoted.
typesorstandardsarenotacceptable.
Hyper-sexualization Unwarrantedfocusonphysicalaspectsorsexualacts Attention and appreciation of the female physical
(excluding body towomenisnotacceptable. aspectsorsexualactsisacceptable.
shaming)
Internalizedsexism Womenmakingsexistcommentsandactionsarenot Ifwomenmakethesecommentsoractionsthenthey
acceptable. arenotsexist.;Womencannotbesexist.
Paygap Salariesformenandwomenforthesameworkpro- Women are paid less than men because they con-
fileshouldbeequal. tributelessthanmenatwork.
Denialortrivialization Noneofsexistcommentorbehaviorisacceptable. Somecommentsorbehaviorscanbeconsideredban-
ofsexistmisconduct ter.
Threats Anythingthatmakeswomenfeelthreatenedisinap- Somethreatstowomencanbeseenasajoke.
propriate.
Sexual assault (ex- Anysexualcontactshouldhappenwithexplicitcon- Willingenssforsexualcontactisimpliedbythings
cludingrape) sent. likepastbehavior,whatyouwear,orwhereyougo.
Sexual harassment Anysexuallyobjectionablebehaviortowomenisnot Some culturally and traditionally based actions
(excludingassault) acceptable. againstwomencannotbeconsideredsexualharass-
ment.
Tonepolicing Commentsoractionsthatcauseoraggravaterestric- Women should pay attention to their language in
tionsonhowwomencommunicateisnotallowed. communication.
Moralpolicing(exclud- MoralprinciplesshouldbethesameforWomenand Therearedifferentcodesofconductforwomenand
ingtonepolicing) Men. menindifferentculturesandreligions.
Victimblaming Victimsdoesnotholdresponsibilityforanyofsexual In some cases, victims are responsible for sexual
harassment, violence, or other sexism perpetrated harassment, violence, or other sexism perpetrated
againsther. againsther.
Slutshaming Individualsexualfreedomshouldberespectedand Sexshouldbemoresubjecttoself-control.;Women
accepted.; Womenarefreetowearwhateverthey shouldbemoreconservativeintermsofsexualcon-
want. ductandbehavior.; Womenshoulddressinaway
thatisnotlinkedtosexualavailability.
Motherhood-related Shaming,prejudices,orotherdiscriminationormis- Motherhoodishonourable,womenshouldbeproud
discrimination conductrelatedtothenotionofmotherhoodisnot ofthis.Awomanshouldbeamother.
acceptable.
Menstruation-related Shaming, prejudices, or other discrimination or Womenshouldunderstandthattheyareoftenmore
discrimination wrongdoingsrelatedtomenstruationisnotaccept- emotional during menstruation due to hormones.;
able. Womenshouldnotentercertainvenuesduringmen-
struation.;Womenshouldbeexemptedfromdifficult
taskswhentheyareinmenstruation.
Religion-basedsexism Sexistdiscriminationorprejudicesstemmingfrom Womenshouldrespectandfollowreligiousscriptures
religiousscripturesorconstructsisnotallowed. orconstructs.
Physical violence Physicalviolenceagainstanyoneisobjectoinable. Womenneedtobephysicallypunishedforcertain
(excluding sexual misbehavior,suchasadultery.
violence)
Mansplaining Givingunsolicitedadviceorexplanationtowomen Womenshouldhumblyconsidertheadviceofothers,
forsomethingthattheyactuallyknowwellordisap- whetheritisappropriateornot.
proveisnotacceptable.
Table6: Listofthevaluesandcounter-valuesforeachsexismcategory.
Figure 6: Vocabulary overlaps of the value-aligned training data generated from OPT-175 among sexism cate-
gories. CategoriesareindescendingorderofW-F1(toptobottom;left-to-right).
Figure7: EvaluationresultsofVA-BARTforeachsexismcategoryonthetestset.

SQuAT: Sharpness- and Quantization-Aware Training for BERT
ZhengWang∗,JunchengBLi∗,ShuhuiQu,FlorianMetze,EmmaStrubell
{zhengwan,junchenl,fmetze,strubell}@cs.cmu.edu
Abstract
Quantization is an effective technique to re-
ducememoryfootprint,inferencelatency,and
power consumption of deep learning models.
However,existingquantizationmethodssuffer
from accuracy degradation compared to full-
precision (FP) models due to the errors intro-
duced by coarse gradient estimation through
non-differentiable quantization layers. The
existence of sharp local minima in the loss
landscapesofoverparameterizedmodels(e.g.,
Transformers) tends to aggravate such per-
formance penalty in low-bit (2, 4 bits) set-
tings. Inthiswork,weproposesharpness-and
quantization-aware training (SQuAT), which
would encourage the model to converge to
flatter minima while performing quantization-
aware training. Our proposed method alter-
natestrainingbetweensharpnessobjectiveand
Figure 1: Illustration of our SQuAT alternate training
step-sizeobjective,whichcouldpotentiallylet
in a 2D toy example, where W ,W indicate the 2 di-
the model learn the most suitable parameter 1 2
mensions. (A)whenstep-sizeisfixed,updateweights
update magnitude to reach convergence near-
intheSharpness-awareminimizing(SAM)directionin
flatminima. Extensiveexperimentsshowthat
thequantizedlosslandscape. (B)Alternatingstep: fix
ourmethodcanconsistentlyoutperformstate-
weight,updatethestep-size.
of-the-artquantizedBERTmodelsunder2, 3,
and 4-bit settings on GLUE benchmarks by
1%, and can sometimes even outperform full
thetraininganddeploymentofdeeplearningmod-
precision(32-bit)models. Ourexperimentson
elswithhighcompressionrates,resultinginasig-
empiricalmeasurementofsharpnessalsosug-
gestthatourmethodwouldleadtoflattermin- nificantreductioninmemorybandwidthandless
imacomparedtootherquantizationmethods. carbonfootprint.
There are two primary strategies for quantiza-
tion: Post-training quantization (PTQ) quantizes
1 Introduction
theparametersofamodeltrainedinfullprecision
posthocandtendstosufferaheavypenaltyonthe
Asstate-of-the-artdeeplearningmodelsforNLP
accuracy,asitsinferencegraphdifferssubstantially
and speech (e.g. Transformers, BERT) grow in-
fromthetraining(Jacobetal.,2018). Quantization-
creasinglylargeandcomputationallyburdensome
aware training (QAT) (Bhuwalka et al., 2020)
(Devlinetal.,2018;Karitaetal.,2019),thereisin-
combatsthisdiscrepancybysimulatingquantiza-
creasingantitheticaldemand,motivatedbylatency
tionduringtraining: Allweightsarequantizedto
andenergyconsumptionconcerns,todevelopcom-
lowbit-widthduringbothforwardandbackward
putationallyefficientmodels. Modelquantization
passes of training, so that the model parameters
has emerged as a promising approach to enable
willworkwellwheninferenceisperformedinlow
∗Co-firstauthor bit-width. Note that in the backward pass, QAT
2202
tcO
31
]GL.sc[
1v17170.0122:viXra
leverages straight through estimator (STE; (Ben- ingscheduletoletLSQworkstablytogether
gioetal.,2013))toestimatethegradientofthese withSAMoptimization,incontrasttoanaive
non-differentiablequantizerlayers. joint training method that leads to unstable
Todate,QATofBERTmodelsstillintroducesa performance (sometimes worse than LSQ
non-negligibleperformancelosscomparedtothe alone). Botharepioneeringefforts.
full-precisionmodels(Zafriretal.,2019;Baietal.,
2. Our experiments suggest that our SQuAT
2020). Thisdropinaccuracyiscausedbypoores-
quantizationachievesasizeableperformance
timationofgradientsduetotheSTE-stepthrough
gain over other state-of-the-art quantization
nondifferentiablelayers. Toalleviateerrorscaused
methodsatlowbitwidth.
bysuchcoarseestimation,LSQ(Esseretal.,2020)
proposedtohavethequantizationbinwidth,orstep
3. Measurement of sharpness verifies that the
size,updateproportionaltotheweightsupdatedur-
modelstrainedwithourSQuATindeedcon-
ingtraining. Suchfinergrainestimationachieves
vergeintoflatterminimacomparedtoLSQ.
impressiveperformancewhenquantizingResNet
models(Esseretal.,2020),butsuffersheavierac- 2 Background&Relatedworks
curacy loss when quantizing larger Transformer
2.1 LearnableparametersQuantization
models. Thisperformancegapbetweenquantized
ResNetsandTransformerscouldbeexplainedby Althoughthecommunityseessomerenewedinter-
thedifferentgeometryoftheirlosssurface,where estinpost-trainingquantization(PTQ),itstilllags
Transformerswereshowntohaveamuchsharper behind quantization-aware training (QAT) meth-
losslandscapethanResNets(Chenetal.,2021). odsinaccuracy,asisconcludedbyGholamietal.
In the meantime, the line of research that fo- (2021). Hence, in this work, we focus on QAT,
cuses on the flatness of minima has garnered in- whichwasfirstintroducedbyJacobetal.(2018).
creasinginterest. Strongempiricalevidenceshows Severalworksproposedlearning-basedapproaches
that SAM (Foret et al., 2020) optimization can toimproveQATaroundthesametime: Jainetal.
improve model generalization by simultaneously (2020)proposedtolearnthequantizers’sdynamic
minimizinglossvalueandlosssharpness. Models rangewhiletraining;Uhlichetal.(2020)advocated
trainedwiththisobjectiveachievedbettergeneral- for learning the optimal bit-width. Among them,
ization across many tasks in the NLP and vision LSQ(Esseretal.,2020)wasthesimplestandbest
domain(Chenetal.,2021;Mehtaetal.,2021). performingapproach,whichproposeslearningthe
In this work, we incorporate the intuition of optimal quantizer step size (bin width). In their
SAM and combine it with step-size quantization. work,atrainablescaleparametersisproposedfor
However,thecombinationofthetwostrategiesis bothweightsandactivations. Thisschemedefines
nontrivial: naivelymergingthesharpnesstermand theelementwisequantizationfunctionas:
step-size optimization and jointly optimizing the
Q(w) = (cid:98)clip(w/s,Q ,Q )(cid:101)·s (1)
loss function results in unstable performance, in N P
some cases underperforming LSQ. The reason is
where(cid:98)·(cid:101)indicatesroundingandtheclip(·)func-
explainedin§3. Toovercomesuchinstability,we tionclampsallvaluesbetweenQ = −2n−1 and
N
introduce an alternate training schedule SQuAT, Q = −2n−1−1inanbitsetting. Theparameter
P
whichoptimizesforthestepsizeinonepassand w istheweightvaluetobequantizedandsisthe
learnedscalingscalar(onepermodule). Following
switches gear to optimize according to the SAM
therulesofSTE,thisquantizernaturallyresultsin
objective in the next pass, and keeps alternating astep-sizegradientof:
untiltheconvergenceofthemodel.

Inourexperiments,wefine-tuneapre-trainedun- ∂Q(w) −w/s+(cid:98)w/s(cid:101) if−Q N <w<Q P
casedBERTbaselinemodeloneachGLUEbench- ∂s = −Q N ifw≤−Q N (2)
Q ifw≥Q
mark (Wang et al., 2018) task using our SQuAT P P
quantization. The result shows that our method
achieves about 1% performance gain over SOTA
2.2 QuantizationofBERT
quantized models consistently across 8 different
SincetrainingBERTusuallyinvolvesadaptiveopti-
tasks. Ourcontributionsarelistedasfollows:
mizerslikeAdamratherthanSGD,andBERTland-
1. Wesuccessfullyleveragedanalternatetrain- scape is shown to be sharper than ResNet (Chen
Task W/ABits COLA SST-2 MRPC STS-B QQP mNLI qNLI RTE
Metrics MatthewsCorr. Acc. Acc. PeasonCorr. Acc. MatchedAcc. Acc. Acc.
FP32 32/32 56.5 93.1 82.84 88.6 90.8 84.34 91.4 67.2
Q8BERT 8/8 58.5±1.3 92.2±0.3 - 89.0±0.2 88.0±0.4 - 90.6±0.3 68.8±3.5
GOBO 2/32 - - - 82.7 - 71.0 - -
Q-BERT 2/8 - 84.6 - - - 76.6 - -
LSQ* 2/8 51.3±0.7 92.2±0.1 83.5±0.7 87.2±0.1 91.1±0.1 83.6±0.1 91.1±0.1 66.8±0.9
SQuAT* 2/8 53.3±0.2 92.7±0.1 84.0±0.7 88.0±0.1 91.1±0.2 84.0±0.1 91.3±0.1 67.4±0.4
GOBO 3/32 - - - 88.3 - 83.7 - -
Q-BERT 3/8 - 92.5 - - - 83.4 - -
LSQ* 3/8 58.8±0.4 92.6±0.2 84.2±0.5 88.4±0.1 91.2±0.2 84.2±0.2 91.8±0.1 68.6±0.3
SQuAT* 3/8 59.2±0.6 93.0±0.2 86.2±0.7 89.1±0.1 91.5±0.1 84.6±0.1 92.1±0.1 70.6±0.6
Q-BERT 4/8 - 92.7 - - - 83.9 - -
LSQ* 4/8 58.7±0.5 93.2±0.1 83.4±0.6 89.1±0.1 91.5±0.1 84.6±0.1 91.6±0.1 68.6±0.4
SQuAT* 4/8 59.1±0.4 93.6±0.2 85.1±0.5 89.3±0.1 91.5±0.1 85.1±0.1 91.9±0.2 69.4±0.7
Table1:PerformancecomparisonsofdifferentquantizationmethodsonGLUEbenchmark. Thescoreisevaluated
onthedevelopmentsetofthetaskusingthespecifiedmetric.WecompareourSQuATagainstGOBO(Zadehetal.,
2020)(PTQbaseline),Q-BERT(Shenetal.,2020),andLSQ(Esseretal.,2020). Wealsolistfull-precisionmodel
(FP32),andthe8-bitsQ8Bert(Zafriretal.,2019)modelasreferences. Wereportthemeanandstandarddeviation
oftheperformancecalculatedover3randomseeds."-"denotesresultswerenotreportedintheoriginalpaper.Note
here (Bai et al., 2020; Zhang et al., 2020; Kim et al., 2021) all initialized with different BERT models than our
BERT model,andthusarenotlistedincomparisonhere. Fortaskswithmultiplemetrics,wereportthemain
base
metrichere. ThealternativemetricsareshowninTable2oftheappendix. *indicatesourownimplementation
et al., 2021), many QAT methods that worked quantization be aware of loss landscape. A more
with ResNet on vision tasks would not perform recentunpublishedwork(Liuetal.,2022)(concur-
equallywellwiththeheavierparameterizedBERT renttoourSQuAT),whichalsoleveragesSAMto
on the NLP benchmark (Gholami et al., 2021). performquantizationadoptsmanuallydefinedstep-
GOBO (Zadeh et al., 2020) is a recent bench- sizeandonlyranexperimentsonvisiondatasets.
markforPTQinBERT.ForQATbaselines,there
3 Methodology
are Zafrir et al. (2019) and Q-BERT (Shen et al.,
2020). BinaryBERT(Baietal.,2020)andTenary
ThemostintuitivewaytoapplySAMtothequan-
BERT(Zhangetal.,2020)attemptedthechalleng-
tizedmodelistooptimizeforthefollowingobjec-
ing“binarization"taskofBERTmodels. Themost
tive, where w is the collection of model weights
recenteffortistoperformintegerquantizationof
andsthecollectionofstep-sizeparameters:
theRoBERTamodel(Kimetal.,2021).
min max L(Q(w,s)+(cid:15)) (4)
2.3 FlatMinima
w,s (cid:107)(cid:15)(cid:107)2<ρ
A flat minimum in the loss landscape is a local
WeaddtheSAMperturbationdirectlytothequan-
optima where the loss remains low in a nearby
tized weight because the element-wise gradient
region. We follow the (cid:15)-sharpness definition of
from Q(w,s) to w and s can only be estimated
(Keskar et al., 2016) which defines sharpness as through STE, therefore the SAM perturbation, (cid:15),
maximumlosswithinaneighborhoodboundedby cannotbeaccuratelyevaluated. Anaturalapproach
to optimize Eq. 4 is to jointly update s and w,
(cid:15). Inmathexpression,letw denotesthecollection
whichresultsinthefollowingupdates:
ofallmodelweights,max L(w+(cid:15))−L(w)
(cid:107)(cid:15)(cid:107)2<ρ
issmallunderagivenradius,ρ. (cid:15)≈ρ∇ L(Q(w,s))/(cid:107)∇ L(Q(w,s))(cid:107) (5)
Q Q 2
Inordertoachieveaflatteroptima,SAM(Foret s←s−η∇ QL(Q(w,s)+(cid:15))∇ sQ(w,s) (6)
et al., 2020) introduces a minimax objective: w←w−η∇ QL(Q(w,s)+(cid:15)) (7)
min max L(w + (cid:15)) to push models into
w (cid:107)(cid:15)(cid:107)2<ρ
flat minima and proposes the following gradient
Noticethatweareomittingtheterm∇ Q(w,s)
update under (cid:96) norm (Note that to reduce w
2
whenupdatingw sinceforeachelementbeingup-
computational cost, (cid:15)(w) in Eq. 3 is regarded as
dated, the STE gradient ∂Q(w,s)/∂w = 1 . If
constantandnogradientflowstoit):
wetakeQ(w,s) → w,Eq.7breaksdowntothe
(cid:15)(w) ≈ ρ∇ L(w)/(cid:107)∇ L(w)(cid:107) update in Eq. 3. However, this is not the case
w w 2
(3)
for step-size parameter s (Eq. 6). The joint up-
w ← w−η·∇ L(w+(cid:15)(w))
w
dateofeachstep-sizeparameterhaselementwise
Nahshan et al. (2020) pioneered the effort to let gradient∂Q(w,s)/∂swhichmustbeevaluatedat
Figure2: SharpnessofSQuATVS.LSQforallGLUETasks. Thelowersharpnessmeansaflatterlocalminima.
run-time. Thus the joint update of step-size can-
notapproximatetheeffectofEq. 3similartothe
weightupdate. Thismakestheupdateofstep-size
asymmetricw.r.ttheupdateofmodelweightwand
intuitively, may not be able to catch up with the
weight updates in the sharpness-aware direction.
Tofixthis,wemaketheweightupdatesto“wait”
forthestep-sizetoadapttothepropermagnitude.
Figure3: JointTrainingVS.AlternateTrainingperfor-
Specifically,weinheritthesharpness-awareupdate
mancedifferencerelativetoLSQ,whichisthe0line
of w, while during the step-size updating phase,
4 Experiment&Discussion
wefixthemodelweightsw,andonlyupdates,as
summarizedinAlgorithm1. Becausetheweights
We apply SQuAT to quantize the pre-trained un-
arealreadysharpness-awareterms,wealsosimpli-
casedBERTbaselinemodelandevaluatetheperfor-
fied updates of the highly shared step-sizes term
manceofourproposedquantizationontheGLUE
toregularSGD.Comparatively,ouralgorithmex-
benchmark(Wangetal.,2018),whichconsistsofa
hibitsbetterstability,whichiscorroboratedbyour
collectionofNLPtasks. Forallthesetasks,werun
strongempiricalresultsinTable1. Empirically,we
the experiments with three random seeds and re-
alsoobservejointupdateresultinginlowerperfor-
portthemeanandstandarddeviationoftheresult1
mancecomparedtoalternatetrainingasshownin
As shown in Table1, our SQuAT outperforms all
Figure3,withjointtrainingunderperformingLSQ
existing quantization methods in all GLUE tasks
baseline in majority of the GLUE (Wang et al.,
under the 2,3 and 4 bits scheme. SQuAT signifi-
2018)tasks.
cantlyoutperformsGOBO(Zadehetal.,2020)by
atleast5%in2bitsand1%in3bitsinGLUEtasks,
which shows the necessity of quantization-aware
training (QAT) over PQT. We compare SQuAT
Algorithm1:AlternateTraining
withotherQATmethods,includingQ-BERT(Shen
1 whilenotconvergingdo etal.,2020)andthecurrentSOTALSQ(Esseretal.,
→sampleBatchB;
2020). Whenquantizingto2-bit,3-bit,and4-bit,
→computegradient∇ L(Q(w ,s ))
Q t t BERTmodelquantizedwithSQuAToutperforms
→compute(cid:15)(Q(w ,s ))
t t LSQ quantized BERT by 2% on average for all
→updatew withSTEgradient
8 GLUE tasks. Remarkably, our 3-bit and 4-bit
w ← w −η·∇ L(Q(w ,s )+(cid:15))
t+1 t Q t t performanceexceedthefull-precisionscoreby1%
→updatesw.r.tweights
atseveralGLUEtasks.
s ← s +η·∇ L(Q(w ,s ))
t+1 t s t+1 t To show that the SQuAT quantization method
reducesthesharpnessoftheminima,wemeasure
1Moredetailsofthesetupisincludedintheappendix
the sharpness score of the local minimum in the
losslandscapefollowing(Foretetal.,2020;Mehta
et al., 2021). The result is shown in Figure. 2 2.
Compared to LSQ, we observe that the models
trainedwithourSQuATquantizationconvergeto
muchflatterminimaacrossallGLUEtasks.
5 Limitation
SQuATmethodoptimizesforthesharpness-aware
perturbation and step size alternatively, and thus
wouldincurmorecomputationaloverheadsandpo-
tentiallylongertraintimecomparedtotheclassical
QAT methods, but there is no additional cost in
inferencetime. Inevitably,asamethodofQAT,we
inheritthelimitationsofQAT,whichis“fake"quan-
tization, meaning all computations are still done
with full-precision floating point numbers. This
approachismoreexpensivetocarryoutinpractice
thanposttrainingquantization,butgeneratesbetter
results.
2RefertoAppendixforhowwecomputedthescore,and
moredetailsinTable.3&4
References Shigeki Karita, Nanxin Chen, Tomoki Hayashi,
Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang,
HaoliBai,WeiZhang,LuHou,LifengShang,JingJin,
Masao Someki, Nelson Enrique Yalta Soplin,
Xin Jiang, Qun Liu, Michael Lyu, and Irwin King.
Ryuichi Yamamoto, Xiaofei Wang, et al. 2019. A
2020. Binarybert: Pushing the limit of bert quanti-
comparative study on transformer vs rnn in speech
zation. arXivpreprintarXiv:2012.15701.
applications. In 2019 IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU),
Yoshua Bengio, Nicholas Léonard, and Aaron
pages449–456.IEEE.
Courville. 2013. Estimating or propagating gradi-
entsthroughstochasticneuronsforconditionalcom-
NitishShirishKeskar, DheevatsaMudigere, JorgeNo-
putation. arXivpreprintarXiv:1308.3432.
cedal, Mikhail Smelyanskiy, and Ping Tak Peter
PulkitBhuwalka,AlanChiao,SuharshSivakumar,Ra- Tang. 2016. Onlarge-batch training for deep learn-
ziel Alvarez, Feng Liu, Lawrence Chan, Skirman- ing: Generalization gap and sharp minima. arXiv
tas Kligys, Yunlu Li, Khanh LeViet, Billy Lam- preprintarXiv:1609.04836.
bert,MarkDaoust,TimDavis,SarahSirajuddin,and
FrançoisChollet.2020. Quantizationawaretraining SehoonKim,AmirGholami,ZheweiYao,MichaelW
withtensorflowmodeloptimizationtoolkit-perfor- Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-
mancewithaccuracy. only bert quantization. In International conference
onmachinelearning,pages5506–5518.PMLR.
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia
Liu, Yang Zhang, Zhangyang Wang, and Michael Jing Liu, Jianfei Cai, and Bohan Zhuang. 2022.
Carbin.2020. Thelotterytickethypothesis forpre- Sharpness-aware quantization for deep neural net-
trained bert networks. Advances in neural informa- works. arXivpreprintarXiv:2111.12273.
tionprocessingsystems,33:15834–15846.
SanketVaibhavMehta,DarshanPatil,SarathChandar,
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. and Emma Strubell. 2021. An empirical investiga-
2021. Whenvisiontransformersoutperformresnets tion of the role of pre-training in lifelong learning.
without pre-training or strong data augmentations. arXivpreprintarXiv:2112.09153.
arXivpreprintarXiv:2106.01548.
Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Zheltonozhskii, Ron Banner, Alex M. Bronstein,
KristinaToutanova.2018. Bert:Pre-trainingofdeep andAviMendelson.2020. Lossawarepost-training
bidirectional transformers for language understand- quantization.
ing. arXivpreprintarXiv:1810.04805.
ShengShen,ZhenDong,JiayuYe,LinjianMa,Zhewei
StevenKEsser,JeffreyLMcKinstry,DeepikaBablani,
Yao,AmirGholami,MichaelWMahoney,andKurt
Rathinakumar Appuswamy, and Dharmendra S
Keutzer. 2020. Q-bert: Hessian based ultra low
Modha. 2020. Learned step size quantization. In
precision quantization of bert. In Proceedings of
International Conference on Learning Representa-
the AAAI Conference on Artificial Intelligence, vol-
tions.
ume34,pages8815–8821.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and
Stefan Uhlich, Lukas Mauch, Fabien Cardinaux,
Behnam Neyshabur. 2020. Sharpness-aware min-
Kazuki Yoshiyama, Javier Alonso Garcia, Stephen
imization for efficiently improving generalization.
Tiedemann, Thomas Kemp, and Akira Nakamura.
arXivpreprintarXiv:2010.01412.
2020. Mixed precision dnns: All you need is a
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei good parametrization. In International Conference
Yao, Michael W Mahoney, and Kurt Keutzer. onLearningRepresentations.
2021. A survey of quantization methods for ef-
Alex Wang, Amanpreet Singh, Julian Michael, Felix
ficient neural network inference. arXiv preprint
Hill, Omer Levy, and Samuel R Bowman. 2018.
arXiv:2103.13630.
Glue:Amulti-taskbenchmarkandanalysisplatform
Benoit Jacob, Skirmantas Kligys, Bo Chen, Meng- for natural language understanding. arXiv preprint
longZhu,MatthewTang,AndrewHoward,Hartwig arXiv:1804.07461.
Adam, and Dmitry Kalenichenko. 2018. Quanti-
zation and training of neural networks for efficient Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
integer-arithmetic-onlyinference. InProceedingsof Chaumond, ClementDelangue, AnthonyMoi, Pier-
the IEEE Conference on Computer Vision and Pat- ric Cistac, Tim Rault, Rémi Louf, Morgan Fun-
ternRecognition,pages2704–2713. towicz, et al. 2019. Huggingface’s transformers:
State-of-the-art natural language processing. arXiv
Sambhav Jain, Albert Gural, Michael Wu, and Chris preprintarXiv:1910.03771.
Dick.2020. Trainedquantizationthresholdsforac-
curate and efficient fixed-point inference of deep KoheiYamamoto.2021. Learnablecompandingquan-
neural networks. In I. Dhillon, D. Papailiopoulos, tizationforaccuratelow-bitneuralnetworks. InPro-
andV.Sze, editors, ProceedingsofMachineLearn- ceedingsoftheIEEE/CVFConferenceonComputer
ingandSystems,volume2,pages112–128. VisionandPatternRecognition,pages5029–5038.
Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad,
and Andreas Moshovos. 2020. Gobo: Quantiz-
ing attention-based nlp models for low latency and
energy efficient inference. In 2020 53rd Annual
IEEE/ACMInternationalSymposiumonMicroarchi-
tecture(MICRO),pages811–824.IEEE.
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019. Q8bert: Quantized 8bit bert. In
2019 Fifth Workshop on Energy Efficient Machine
Learning and Cognitive Computing-NeurIPS Edi-
tion(EMC2-NIPS),pages36–39.IEEE.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao
Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert:
Distillation-awareultra-lowbitbert. arXivpreprint
arXiv:2009.12812.
A UnformVS.non-Uniform
Quantization
Quantization approaches can be subdivided into
uniform and non-uniform quantization. Non-
uniformquantizationtendstoachievebetteraccu-
racythanuniformquantization(Yamamoto,2021),
butrequiresnonstandardhardwaresupporttostore
codebooksorquantizationintervals,hencenotprac-
ticalwithexistinghardware(Gholamietal.,2021).
Inthescopeofthispaper,wereferQATasuniform
quantizationviaquantization-awaretraining.
B ExperimentSetup
We initialized the model with uncased BERT
base
model (Wolf et al., 2019) from the HuggingFace
librarywithpre-trainedweights,andfine-tunedon
eachGLUEbenchmarktask. Wefollowthesame
setupas(Chenetal.,2020)andreportvalidation
setaccuracyforQQP,QNLI,MRPC,RTE,SST-2,
matched accuracy for MNLI, Matthew’s correla-
Algorithm2:SharpnessMeasurement
tionforCoLA,andPearsoncorrelationforSTS-B,
the alternative metrics are included below in Ta- 1 bufferinitialweightasw 0
ble2. Each task was trained on its train set for 5 2 whilenotconvergingdo
epochstoobtainthestartingcheckpointfortheall if(cid:107)W t+1−W 0(cid:107) 2 ≤ ρthen
W = W +η·∇ L
of the quantization model. To train the quantiza- t+1 t W
else
tion model, we use Adam optimizer with initial
W = W +η·∇ L
learningratesetat1e-5andusecosineannealing t+1 t W
W = ρ· Wt+1−W0 +W
LRscheduletoadjustthelearningrateduringthe t+1 (cid:107)Wt+1−W0(cid:107)2 0
trainingprocess. ToperformtheSQuATandLSQ t = t+1
fine-tuning,weruneachmodelfor32epochsfor 3 ReturnL(W T+1)−L(W 0);
eachtasks. Thehyperparameterρweusedfortrain-
ingSQuATis0.1for2-bitsand3-bitsmodels,and
0.15for4-bitsmodels,whicharedeterminedbya
gridsearchfrom0.0to0.25at0.05incrementon
MNLItask.
C Measuringsharpness
FollowingMehtaetal.(2021),Algorithm2shows
howwecomputethesharpnessscoreforourquan-
tized model checkpoints. W is the weight at T
t
step,η islearningrate,ρisasmallradius. ∇ is
W
thegradient,Listhetasklossfunction(inourcase,
differsatdifferentGLUE(Wangetal.,2018)task).
Task Bits MRPC STS-B QQP mNLI
Metrics F1 Spearmancorr. F1 MismatchedAcc.
FP32 32 88.01 88.2 87.7 84.53
Q8BERT 8 89.6 - - -
±0.2
Q-BERT 2 - - - 77.0
LSQ 2 88.6 86.7 88.0 83.6
±0.4 ±0.1 ±0.1 ±0.1
SQuAT 2 88.9 87.4 88.0 84.1
±0.5 ±0.1 ±0.1 ±0.2
Q-BERT 3 - - - 83.8
LSQ 3 88.7 87.9 88.4 84.0
±0.3 ±0.1 ±0.1 ±0.2
SQuAT 3 90.6 88.6 88.6 84.7
±0.5 ±0.1 ±0.1 ±0.1
Q-BERT 4 - - - 84.2
LSQ 4 88.4 88.7 88.6 84.7
±0.4 ±0.1 ±0.1 ±0.2
SQuAT 4 89.5 88.8 88.6 84.8
±0.5 ±0.1 ±0.1 ±0.1
Table2: GLUEbenchmarkresultsonauxiliarymetric. Wereportmeanandstandarddeviationcalculatedover3
randomseeds.
Task Bits COLA SST-2 MRPC STS-B QQP mNLI qNLI RTE
LSQ 2 0.04393 0.02461 0.13278 0.03866 0.00659 0.01969 0.01189 0.09071
SQuAT 2 0.03564 0.01656 0.04578 0.01524 0.00533 0.00783 0.00833 0.02861
LSQ 3 0.04835 0.01660 0.08314 0.02848 0.01046 0.02036 0.01520 0.05867
SQuAT 3 0.01968 0.00773 0.02964 0.01789 0.00626 0.00905 0.01111 0.04527
LSQ 4 0.04174 0.01783 0.02867 0.02268 0.00850 0.01851 0.01771 0.21308
SQuAT 4 0.03092 0.01594 0.02517 0.01232 0.00715 0.00805 0.01154 0.03905
Table3: TheSharpnesscomparisonbetweenLSQandSQuATonGLUEbenchmark. Here,ρ = 0.01. Thelocal
minimumofSQuATisclearlyflatterthanLSQbyalargemarginacrossalltheGLUEtasks.
Task Bits COLA SST-2 MRPC STS-B QQP mNLI qNLI RTE
LSQ 2 0.23983 0.13650 0.57738 0.39474 0.03593 0.10609 0.09377 0.45210
SQuAT 2 0.17667 0.08475 0.23903 0.09545 0.03421 0.04913 0.04665 0.19441
LSQ 3 0.25096 0.09075 0.34070 0.27663 0.07237 0.13044 0.08277 0.53654
SQuAT 3 0.11754 0.05985 0.15784 0.12288 0.03506 0.05780 0.05913 0.28605
LSQ 4 0.21309 0.10925 0.19889 0.25601 0.04632 0.10354 0.09807 0.63298
SQuAT 4 0.16224 0.08386 0.14872 0.07025 0.03275 0.05179 0.06944 0.36209
Table4: TheSharpnesscomparisonbetweenLSQandSQuATonGLUEbenchmark. Here,ρ=0.05. Inthiscase,
thelocalminimumofSQuATisflatterthanLSQacrossalltheGLUEtasksaswell.

Regularizing Self-training for Unsupervised Domain Adaptation
via Structural Constraints
RajshekharDas1*† JonathanFrancis1,2∗ SanketVaibhavMehta1∗ JeanOh1 EmmaStrubell1 Jose´ Moura1
1CarnegieMellonUniversity 2BoschCenterforArtificialIntelligence
{rajshekd, jmf1, svmehta, hyaejino, strubell, moura}@andrew.cmu.edu
Abstract
Self-training based on pseudo-labels has emerged as a
dominantapproachforaddressingconditionaldistribution
shifts in unsupervised domain adaptation (UDA) for se-
mantic segmentation problems. A notable drawback, how-
ever, is that this family of approaches is susceptible to er-
roneous pseudo labels that arise from confirmation biases
in the source domain and that manifest as nuisance fac-
tors in the target domain. A possible source for this mis-
match is the reliance on only photometric cues provided
by RGB image inputs, which may ultimately lead to sub-
optimal adaptation. To mitigate the effect of mismatched
pseudo-labels, we propose to incorporate structural cues
fromauxiliarymodalities,suchasdepth,toregularisecon-
ventionalself-trainingobjectives.Specifically,weintroduce
acontrastivepixel-levelobjectnessconstraintthatpullsthe
pixel representations within a region of an object instance
closer,whilepushingthosefromdifferentobjectcategories
apart. To obtain object regions consistent with the true Figure 1. Motivation for Objectness Constraints: The above
underlying object, we extract information from both depth examplescomparetarget-domainground-truthsegmentation,pre-
maps and RGB-images in the form of multimodal cluster- dicted segmentation and prediction confidence (brighter regions
ing. Crucially, the objectness constraint is agnostic to the aremoreconfident)ofaseedmodelthatwasadaptedfromsource
ground-truth semantic labels and, hence, appropriate for totargetdomainviaadversarialadaptation[48].Mostself-training
unsupervised domain adaptation. In this work, we show approaches use such a seed model to predict pixelwise pseudo-
labels. Theblue-dashed-boxeshighlightethehigh-confidencere-
that our regularizer significantly improves top performing
gions that are likely to be included in the set of a pseudo-labels
self-training methods (by up to 2 points) in various UDA
despitebeingmis-classified. Weproposetomitigatetheadverse
benchmarksforsemanticsegmentation.Weincludeallcode
effectofsuchnoisypseudo-labelsonself-trainingbasedadapta-
inthesupplementary.
tionviaobjectnessconstraints.
1.Introduction ing [9]. However, an important limitation arises from the
excessivecostandtimetakentoannotateimagesatapixel-
Semanticsegmentationisacrucialandchallengingtask
level(reportedtobe1.5hoursperimageinapopulardataset
for applications such as autonomous driving [2,18,51,60,
[12]). Further, most real-world datasets do not have suffi-
61]thatrelyonpixel-levelsemanticsofthescene. Perfor-
cient coverage over all variations in outdoor scenes such
manceonthistaskhassignificantlyimprovedoverthepast
as weather conditions and geography-specific layouts that
fewyearsfollowingtheadvancesindeepsupervisedlearn-
canbecrucialforlarge-scaledeploymentoflearning-based
modelsinautonomousvehicles. Acquiringtrainingdatato
*Equalcontribution.
†Correspondence. catertosuchscenevariationswouldsignificantlyaddtothe
1
3202
rpA
92
]VC.sc[
1v13100.5032:viXra
costofannotation. ingbothdepth-basedhistogramsandRGB-imagesthatare
Toaddresstheannotationproblem,syntheticdatasetscu- fused together to yield multiple object-regions per image.
ratedfrom3DsimulationenvironmentslikeGTA[37]and These regions respect actual object boundaries, based on
SYNTHIA[38]havebeenproposedwherelargeamountsof the structural information depth provides, as well as vi-
annotateddatacanbeeasilygenerated. However,generated sual similarity. In the second step, the object-regions are
data introduces domain shift due to differences in visual leveraged to formulate a contrastive objective [10,22,44]
characteristics of simulated images (source domain) and thatpullstogetherpixelrepresentationswithinanobjectre-
realimages(targetdomain). Tomitigatesuchshifts,unsu- gion and pushes apart those from different semantic cate-
perviseddomainadaptationstrategies[2,5,18,48,60,61,64] gories. Such an objective can improve semantic segmen-
for semantic segmentation have been extensively studied tation by causing the pixel representations of a semantic
intherecentyears. Amongtheseapproaches, self-training category to form a compact cluster that is well separated
[16]hasemergedasaparticularlypromisingapproachthat from other categories. We empirically demonstrate the ef-
involves pseudo labelling the (unlabelled) target-domain fectiveness of our constraint on popular benchmark tasks,
data using a seed model trained solely on the source do- GTA→Cityscapes and SYNTHIA→Cityscapes, on
main. Pseudo-label predictions for which the confidence which we achieve competitive segmentation performance.
exceedsapredefinedthresholdarethenusedtofurthertrain Tosummariseourcontributions:
themodelandultimatelyimprovethetarget-domainperfor- • Weproposeanovelobjectnessconstraintderivedfrom
mance. depthandRGBinformationtoregulariseself-training
Whileself-trainingbasedadaptationisquiteeffective,it approachesinunsuperviseddomainadaptationforse-
issusceptibletoerroneouspseudolabelsarisingfromcon- mantic segmentation. The use of multiple modalities
firmationbias[3]intheseedmodel. Confirmationbiasre- introduces implicit model supervision that is comple-
sults from training on source domain semantics that might mentarytothepseudo-labelsandhence,leadtoamore
introduce factors of representation that serve as nuisance robustself-training.
factors for the target domain. In the context of semantic
• We empirically validate the most important aspect of
segmentation,suchabiasmanifestsaspixel-wiseseedpre-
our regulariser, i.e., its ability to improve a variety
dictions that are highly confident but incorrect (see Figure
of self-training methods. Specifically, our approach
1). Forinstance,ifthesourcedomainimagesusuallyhave
achieves 1.2%-2.9% (GTA) and 2.2%-4.4% (SYN-
brightregions(highintensityoftheRGBchannels)forthe
THIA)relativeimprovementsoverthreedifferentself-
skyclass,thenbrightregionsintargetdomainimagesmight
training baselines. Interestingly, we observe that reg-
be predicted as the sky with high confidence, irrespective
ularisationimprovesperformanceonboth“stuff”and
oftheactualsemanticlabel. Sincehighlyconfidentpredic-
“things” classes, somewhat normalising the effects of
tionsqualifyaspseudo-labels,trainingthemodelonpoten-
classwisestatistics.
tially noisy predictions can ultimately lead to sub-optimal
performance in the target domain. Thus, in this work, we • Further, our regularised self-training method
seek to reduce the heavy reliance of self-training methods achieves state-of-the-art mIoU of 54.2% in GTA
on photometric cues for predicting pixel-wise semantic la- → Cityscapes settings and improves classwise
bels. IoUsbyupto4.8%overbestpriorresults.
Tothatend,weproposetoincorporateauxiliarymodal-
2.RelatedWork
ity information such as depth maps that can provide struc-
turalcues[11,24,51,53],complementarytothephotometric Unsuperviseddomainadaptation. Unsuperviseddomain
cues. Semantic segmentation datasets are usually accom- adaptation (UDA) is of particular importance in complex
panied by depth maps that can be easily acquired in prac- structured-prediction problems, such as semantic segmen-
tice[12,39].Sincena¨ıvefusionoffeaturesthatareextracted tation in autonomous driving, where the domain gap be-
fromdepthinformationcanalsointroducenuisance[24,51], tweenasourcedomain(e.g.,anurbandrivingdataset)and
an important question is raised — How can we leverage targetdomain(real-worlddrivingscenarios)canhavedev-
the depth modality to counter the effect of noisy pseudo- astating consequences on the efficacy of deployed mod-
labels during self-training? In this work, we propose a els. Several approaches [5,14,18,30,35] have been pro-
contrastive objectness constraint derived from depth maps posed for learning domain invariant representations, e.g.,
and RGB-images in the target domain that is used to reg- throughadversarialfeaturealignment[6,14,49,54],which
ularise conventional self-training methods. The constraint addresses the domain gap by minimising a distance met-
iscomputedintwosteps: anobject-regionestimationstep, ric that characterises the divergence between the two do-
followedbypixel-wisecontrastivelosscomputation. Inthe mains[4,13,28,29,33,36,40,43].Problematically,suchap-
firststep,weperformunsupervisedimagesegmentationus- proachesaddressonlyshiftsinthemarginaldistributionof
2
thecovariatesorthelabelsand,therefore,proveinsufficient use depth only for the source domain, we explore its ap-
for handling the more complex shifts in the conditionals plication exclusively to the target domain. Contemporary
[20,55,62]. Self-training approaches have been proposed to our setting, [53] improves adaptation by extracting the
toinducecategory-awareness[60]orclusterdensity-based correlation between depth and RGB in both domains. An
assumptions [42], in order to anchor or regularise condi- importantdistinctionofourapproachwithregardstoabove
tionalshiftadaptation,respectively. Inthispaper,webuild works is that we exploit the complementarity of RGB and
upontheseworksbyjointlyintroducingcategory-awareness depth instead of the correlation to formulate a contrastive
through the use of pseudo-labeling strategies and regulari- regularizer. Theimportanceofmultimodalinformationhas
sationthroughthedefinitionofcontrastivedepth-basedob- also been considered in other contexts such as indoor se-
jectnessconstraints. manticsegmentation[45]andadaptationfor3Dsegmenta-
tionusing2Dimagesand3Dpointsclouds[19]. Whilenot
Self-training with pseudo-labels. Application of self-
directly related to our experimental settings, they provide
traininghasbecomepopularinthesphereofdomainadap-
insightandinspirationforourapproach.
tation for semantic segmentation [23,25,60,64]. Here,
pseudo-labels are assigned to observations from the target
domain, based on the semantic classes of high-confidence
(e.g., the closest or least-contrastive) category centroids
[57,60], prototypes [8], cluster centers [21], or superpixel
representations[61]thatarelearnedbyamodeltrainedon 3.Self-TrainingwithObjectnessConstraints
thesourcedomain. Often,toensurethereliabilityofinitial
pseudolabelsfortargetdomain, themodelisfirstwarmed
upviaadversarialadaptation[60,61]. Moreover,forstabil- We begin by introducing preliminary concepts on self-
itypurposes,pseudolabelsareupdatedinastagewisefash- training based adaptation. These concepts serve as bases
ion,thusresultinginanoverallcomplexadaptationscheme. for introducing our objectness constraint in Section 3 that
Towards streamlining this complex adaptation process, re- isusedtoregularisetheself-trainingmethods. Wereferto
centapproacheslike[2,47]proposetotrainwithoutadver- ourframeworkasPAC-UDAwhichusesPseudo-labelsAnd
sarialwarmupandwithamomentumnetworktocircumvent objectness Constraints for self-training in Unsupervised
stagewisetrainingissue. Acommonfactorunderlyingmost Domain Adaptation for semantic segmentation. Although,
self-training methods is their reliance on just RGB inputs we describe a canonical form of self-training for formalis-
thatmaynotprovidesufficientsignalforpredictingrobust ingourregularisationconstraint,PAC-UDAshouldbeseen
target-domainpseudolabels. Thismotivatesustolookfor asageneralapproachthatcanencompassvariousformsof
alternate forms of input like depth that is easily accessible self-training(asshowninexperiments).
andprovideamorerobustsignal.
Adaptationwithmultiplemodalities. Learningandadap- Unsupervised Domain Adaptation (UDA) for Semantic
tation using multimodal contexts presents an opportunity Segmentation: Consider a dataset Ds = {(xs,ys)}Ns
i i i=1
forleveragingcomplementaritybetweendifferentviewsof of input-label pairs sampled from a source domain dis-
the input space, to improve model robustness and general- tribution, Ps . The input and labels share the same
X×Y
isability. Inthecontextofunsuperviseddomainadaptation, spatial dimensions, H × W, where each pixel of the la-
use of mutimodal information has recently become more bel is assigned a class c ∈ {1,...,C} and is represented
popularwithpioneeringworkslike[24]. Specifically,[24] via a C dimensional one-hot encoding. We also have a
usesdepthregressionasawaytoregularisetheGANbased dataset Dt = {(xt,yt)}Nt sampled from a target distri-
i i i=1
domaintranslationresultinginbettercaptureofsourcese- bution,Pt wherethecorrespondinglabels,{yt}areun-
X×Y i
manticsinthegeneratedtargetimages. Anotherrelatedap- observed during training. Here, the target domain is sepa-
proach[51]proposestheuseofdepthviaanauxiliaryobjec- ratedfromthesourcedomainduetodomainshiftexpressed
tivetolearnfeaturesthatwhenfusedwithprimarysemantic as Ps (cid:54)= Pt . Under such a shift, the goal of un-
X×Y X×Y
segmentationpredictionbranchprovidesamorerobustrep- supervised domain adaptation is to leverage Ds and Dt to
resentationforadaptation.Whilesharingourmotivationfor learn a parametric model that performs well in the target
useofauxiliaryinformation,theiruseoffusedfeaturesfor domain. The model is defined as a composition of an en-
adaptation does not address the susceptibility of adversar- coder,E :X →Z andaclassifier,G :Z →Z where,
φ ψ P
ialadaptationtoconditionaldistributionshifts. Incontrast Z ∈ RH×W×d representsthespaceofd-dimensionalspa-
to this method, we propose a depth based objectness con- tialembeddings,Z ∈ RH×W×C givestheun-normalized
P
straint for adaptation via self-training that not only lever- distributionovertheC classesateachspatiallocation,and
agesmultimodalcontextbutalsohandlesconditionalshifts {φ,ψ}arethemodelparameters. Tolearnasuitabletarget
moreeffectively. Moreover,unlikethepreviousworksthat model, the parameters are optimised using a cross-entropy
3
Figure2.ObjectnessConstraintFormulation:Overallpipelineforcomputingtheobjectnessconstraintusingmulti-modalobject-region
estimatesderivedfromRGB-ImageandDepth-Map.DepthsegmentationisobtainedbyclusteringthehistogramofdepthvaluesandRGB
segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object
regionsthataremoreconsistentwiththeactualobject.Forexample,aportionofthecarinthemiddleiswronglyclusteredwiththeroadin
depthsegmentationandwiththeleft-wallunderRGBsegmentation. However,thefusedsegmentationyieldscar-regionsthatcompletely
respecttheboundaryofthecar.
objectiveonthesourcedomain, NotethatwhileEqn.3usesaclass-agnosticfixedthreshold
inpractice,thisthresholdcanbemadeclass-specificanddy-
1
(cid:88)Ns H (cid:88)×W (cid:88)C namically updated over the course of self-training. Such a
Ls =− ys logps (ψ,φ) thresholdensuresthatonlythehighly-confidentpredictions
cls N imc imc
s i=1 m=1 c=1 contributetosuccessivetraining. Thefinalself-trainingob-
(1) jectivecanbewrittenintermsofpseudo-labelsas
ps (ψ,φ)=σ(G ◦E (xs))| , (2)
imc ψ φ i m,c
Lt =− 1
(cid:88)Nt H (cid:88)×W (cid:88)C
y˜t log(cid:0) pt (cid:1) (4)
where σ denotes softmax operation and an adaptation ob- st N imc(cid:48) imc(cid:48)
t
i=1 m=1 c(cid:48)=1
jectiveoverthetargetdomainasdescribednext.
Pseudo-labelself-training(PLST):Followingpriorworks TheoverallUDAobjectiveissimply,L = Ls +α Lt,
uda cls st st
[60,64], we describe a simple and effective approach to whereα istherelativeweightingcoefficients.
st
PLSTthatleveragesasourcetrainedseedmodeltopseudo-
label unlabelled target data, via confidence thresholding. 3.1.SupervisionForObjectnessConstraint
Specifically, the seed model is first trained on Ds using
An important issue with the self-training scheme de-
Eqn. 2 to obtain a good parameter initialisation, {φ ,ψ }.
0 0 scribedaboveisthatitisusuallypronetoconfirmationbias
Then,thismodelisusedtocomputepixel-wiseclassprob-
that can lead to compounding errors in target model pre-
abilities,pt (ψ ,φ )usingtoEqn.2foreachtargetimage,
im 0 0 dictions when trained on noisy pseudo-labels. To alleviate
xt ∈Dt. Theseprobabilitiesareusedinconjunctionwitha
i target performance, we introduce auxiliary modality infor-
predefined threshold δ, to obtain one-hot encoded pseudo-
mation (like, depth) that can provide indirect supervision
labels
for semantic labels in the target domain and improve the
(cid:40) robustness of self-training. In this section we describe our
1ifc=argmaxpt andpt ≥δ
y˜t = c(cid:48) imc(cid:48) imc (3) multimodalobjectnessconstraintthatextractsobject-region
imc
0otherwise estimatestoformulateacontrastiveobjective.Theoverview
4
ofourobjectnessconstraintformulationispresentedinFig. thesameclusterlabeleventoobjectsatdistinctdepths,for
2. example,thebackofthebusandthesmallcarattheback.In
Supervision via Depth: Segmentation datasets are often contrast, objectregionsderivedfromafusionofthesetwo
accompaniedwithdepthmapsregisteredwiththeRGBim- modalities can lead to object regions that are more consis-
ages. In practice, depth maps can be obtained from stereo tentwithindividualobjectinstances(forexample,thesmall
pairs[12,39]orsequenceofimages[15].Thesedepthmaps caratthebackaswellasthecarinthefront). Weempiri-
can reveal the presence of distinct objects in a scene. We callydemonstratetheeffectivenessofobjectnessconstraint
particularlyseektoextractobjectregionsfromthesedepth derivedfromsuchmultimodalfusioninSection4.3.
maps by first computing a histogram of depth values with
3.2.ObjectnessConstraintsthroughContrast
predefined, b number of bins. We then leverage the prop-
ertyofobjectsunder”things”categories[17]whoserange Our objectness constraint is formulated using a con-
of the depth is usually much smaller than the range of en- trastive objective that pulls together pixel representations
tire scene depth. Examples of such categories in outdoor within an object region and pushes apart those that belong
scene segmentation include persons, cars, poles etc. This todifferentobjectcategories. Formally,weassignaregion
property translates into high density regions (or peaks) in index and a region label to every pixel associated with an
the histogram corresponding to distinct objects at distinct object region of the input scene. Each region index is a
depths. Among these peaks, we use the ones with promi- uniquenaturalnumberin{1,...,K}whereK isthenum-
nence[27]aboveathreshold,δ ascenterstoclusterthe ber of object regions. A region label is assigned as the
peak
histogramsintodiscreteregionswithuniquelabels. These most frequent pseudo-label class within the object region.
labels are then assigned to every pixel whose depth values Inpractice,noisypseudo-labelscanleadtoregionlabelling
lie in the associated region. An example of the resulting thatisinconsistentwithtruesemanticlabels. Tominimise
depth-basedsegmentationforb = 200andδ = 0.0025 such inconsistencies, we introduce a threshold τ that se-
peak p
isvisualisedinFig. 2. lectsvalidobjectregionsforwhichtheproportionofpixels
with pseudo-label class same as the region label is above
Supervision via RGB: Another important form of self-
this threshold. This selection excludes the object regions
supervision for object region estimates is based on RGB-
with no dominant pseudo-label class from contributing to
input clustering. We adopt SLIC [1] as a fast algorithm
theobjectnessconstraint. Sincethecostofcomputingpair-
for partitioning images into multiple segments that respect
wise constraints is quadratic in the number of pixels, we
objectboundaries; theSLICmethodappliesk-meansclus-
recastthepairwiseconstraintintoaprotoypicallossthatre-
tering in pixel space to group together adjacent pixels that
duces the time complexity to linear. Towards the end, we
are visually similar. An important design decision is the
numberofSLICsegments,k : smallk leadstolargeclus- first compute a prototypical representation for each region
s s
usingtheassociatedpixelembeddings,
ter sizes that is agnostic to the variation in object scales,
acrossdifferentobjectcategoriesandinstancesofthescene.
1 (cid:88)
Consequently,pixelsfromdistinctobjectinstancesmaybe ν k = |U | z p (5)
grouped together regardless of the semantic class, thus vi- k p∈Uk
olating the notion of object region. Conversely, a large k
s where, U is the set of pixel locations with the kth object-
will over-segment each object in the scene, resulting in a k
region. Thenasimilarityscore(basedonCosinemetric)is
trivial objectness constraint. Triviality arises from enforc-
computed between each pixel and prototypical representa-
ingsimilarityofpixel-embeddingsthatshareroughlyiden-
tionthatformsthebasisforourcontrastiveobjectnesscon-
ticalpixelneighbourhoodsandhencearelikelytoyieldthe
straintas
sameclasspredictionsanyway.
Thus, to formulate a non-trivial constraint with suffi- 1 (cid:88) (cid:88)
Lt = Lt (p) (6)
ciently small k that also respects object boundaries, we obj S obj
s
proposetofuseregionestimatesfrombothdepthandRGB
k p∈Uk
 
modalities.Wefirstobtaink segmentsusingSLICoverthe
s exp(z˜ ·ν˜ )
RGB image followed by further partitioning of each seg- Lt obj(p)=−log  (cid:80) expp (z˜k ·ν˜ )  (7)
ment into smaller ones based on the depth segmentation. p k(cid:48)
k(cid:48)∈Ω(k)
Theprocess,visualisedinFig. 2highlightstheimportance
ofourmultimodalapproach. Purelydepthbasedsegments where,S isthetotalnumberofvalidpixels,Ω(k)istheset
areagnostictopixelintensitiesandmayclustertogetherdis- ofvalidobjectregionsthathaveregionlabelsotherthank,
tinctobjectcategoriesthatlieatsimilardepths,forinstance, and z˜ and ν˜ represent L normalised embeddings. Note
p k 2
the car in the front and the sidewalk. On the other hand, that the objectness constraints are only computed for the
purelyRGBsegmentswithsufficientlysmallk mayassign targetdomainimagessinceweareinterestedinimproving
s
5
Table1. TestofGenerality: Wecomparetheperformanceofregularisedandun-regularisedversionsofthreeself-trainingapproachesfortwodomain
settings,namely,GTA → CityscapesandSYNTHIA → Cityscapes. Bothper-classIoUandmeanIoUsarepresented. Thenumbersinbold
indicatehigheraccuraciesinthepairwisecomparisons,betweenabase-methodandthebase-method+PAC.
SourceDomain Method mIoU
CAG[60] 87.0 44.6 82.9 32.1 35.7 40.6 38.9 45.5 82.6 23.5 78.7 64.0 27.2 84.4 17.5 34.8 35.8 26.7 32.8 48.2
CAG+PAC(ours) 86.3 45.7 84.5 30.5 35.5 38.9 40.3 49.9 86.0 33.5 81.1 64.1 25.5 84.5 21.3 32.9 36.3 26.7 40.0 49.6
SAC[2] 89.9 54.0 86.2 37.8 28.9 45.9 46.9 47.7 88.0 44.8 85.5 66.4 30.3 88.6 50.5 54.5 1.5 17.0 39.3 52.8
SAC+PAC(ours) 93.3 63.6 87.2 42.0 25.4 44.9 49.0 50.6 88.1 45.2 87.6 64.0 28.1 83.6 37.5 43.9 13.7 20.1 46.2 53.4
DACS[47] 93.4 54.3 86.3 28.6 33.7 37.0 41.1 50.6 86.1 42.6 87.6 63.5 28.9 88.1 44.2 52.7 1.7 34.7 48.1 52.8
DACS+PAC(ours) 93.2 58.8 87.2 33.3 35.1 38.6 41.8 51.4 87.4 45.8 88.3 64.8 31.6 84.3 51.7 53.4 0.6 31.3 50.6 54.2
CAG 87.0 41.0 79.0 9.0 1.0 34.0 15.0 11.0 81.0 - 81.0 55.0 16.0 77.0 - 17.0 - 2.0 47.0 40.8
CAG+PAC(ours) 87.0 42.0 80.0 12.0 3.0 30.0 17.0 17.0 80.0 - 88.0 57.0 5.0 75.0 - 20.0 - 1.0 52.0 41.7
SAC[2] 91.7 52.7 85.1 22.6 1.5 42.2 44.1 30.9 82.5 - 73.8 63.0 20.9 84.9 - 29.5 - 26.9 52.2 50.3
SAC+PAC(ours) 83.2 40.5 85.4 30.0 2.0 43.0 42.2 33.8 86.3 - 89.8 65.3 33.5 85.1 - 35.2 - 29.9 55.3 52.5
DACS[47] 84.9 23.0 83.7 16.0 1.0 36.3 35.0 42.8 81.7 - 89.5 63.5 34.5 85.3 - 41.5 - 31.2 50.8 50.0
DACS+PAC(ours) 90.6 46.7 83.3 18.7 1.3 35.1 34.5 32.0 85.1 - 88.5 66.0 35.0 83.8 - 43.1 - 28.8 46.7 51.2
target domain performance using self-training. Addition- prior works [2,61]. The performance metrics used are per
ally, the constraint in Eqn. 7 is defined for a single image class Intersection over Union (IoU) and mean IoU (mIoU)
but can be easily extended to multiple images by simply overalltheclasses.
averagingoverthem; thefinalregularisedself-trainingob- Implementation Details: For object region estimates,
jectiveisthendefinedasL pac = Ls uda +α obj ∗Lt obj, where we experiment with three different numbers, k s ∈
α objcontrolstheeffectoftheconstraintonoveralltraining. {25,50,100} of RGB-clusters, two values of prominence
thresholds, δ ∈ {0.001,0.0025} and three numbers of
3.3.LearningandOptimization peak
histogrambins,b ∈ {100,200,400}. Depthmapsobtained
To train the our model, PAC-UDA with a base self- fromstereopairscanhavemissingvaluesatpixel-level,as
training approach, we follow the exact procedure outlined is the case with Cityscapes. These missing values have a
bythecorrespondingapproach. Theonlydifferenceisthat valuezeroandareignoredwhilegeneratingdepthsegments
we plug in our constraint as a regularise to the base ob- using depth-histogram. Finally, due to high computational
jective, L . One important consideration is that our reg- costofcomputingthecontrastiveobjectivefrompixel-wise
uda
ularise depends on reasonable quality of pseudo labels to embedding, we set the spatial resolution of these embed-
defineregionlabelsthatarenotrandom. Thustheregular- dingsto256×470inCAGandSACand300×300inDACS.
isation weight, α is set to zero for a few initial training We fixed the relative weighting of the regularizer, α to
obj obj
iterations,postwhichitswitchestotheactualvalue. 1.0 as the target performance was found to be insensitive
to the exact value. For hyperameter choices regarding ar-
4.Experiments chitectureandoptimizers,weexactlyfollowtherespective
self-training base methods [2,47,60]. Experiments were
Datasets and Evaluation Metric: We evaluate the PAC-
conductedon4×11GBRTX2080TiGPUswithPyTorch
UDA framework in two common scenarios: the GTA
implementation. Furtherdetailsinthesupplementary.
[37]→Cityscapes [12] transfer semantic segmenta-
tion task and the SYNTHIA [38]→Cityscapes [12]
4.1.GeneralityofObjectnessConstraint
task. GTA5 is composed of 24,966 synthetic images with
resolution1914×1052andhasannotationsfor19classes In Table 1, we test the generality of our proposed reg-
thatarecompatiblewiththecategoriesinCityscapes. Sim- ularizer on three base methods, namely, CAG [60], SAC
ilarly,SYNTHIAconsistsof9,400syntheticimagesofur- [2] and DACS [47] that generate pseudo labels in differ-
ban scenes at resolution 1280 × 760 with annotations for ent ways. We use official implementations of each base
only 16 common categories. Cityscapes has of 5,000 real method with almost same configurations for data prepro-
images and aligned depth maps of urban scenes at resolu- cessing,modelarchitecture,andoptimizerexceptforafew
tion2048×1024andissplitintothreesetsof2,975train, modifications as follows. In the case of CAG, we replace
500 validation and 1,525 test images. Of the 2,975, we the Euclidean metric with a Cosine metric as it was found
use 2,475 randomly selected images for self-training and to generate more reliable pseudo-labels. Also, we run it
remaining 500 images for validation. We report the final forasingleself-trainingiterationinsteadofthree[60]. For
test performance of our method on the 500 images of the theSACmethod,wereducetheGROUP SIZEfromdefault
official validation split. The data-splits are consistent with valueof4to2followingGPUconstraints. Finally, forthe
6
ATG
AIHTNYS
daor
klawedis gnidliub
llaw ecnef elop thgil ngis .egev niarret yks nosrep redir rac kcurt sub niart rotom ekib
Table2.GTA → Cityscapesresults:Classwiseandmean(over16classes)IoUcomparisonofourDACS+PACwithpriorworks.†denotestheuseof
PSPNet[63],*denotesourimplementationofSACwitharestrictedconfiguration(GROUPSIZE=2)comparedtooriginalSACmethod(GROUPSIZE=4).
AllothermethodsuseDeepLabV2[9]architecture.
mIoU
AdvEnt[50] 89.4 33.1 81.0 26.6 26.8 27.2 33.5 24.7 83.9 36.7 78.8 58.7 30.5 84.8 38.5 44.5 1.7 31.6 32.4 45.5
DISE[7] 91.5 47.5 82.5 31.3 25.6 33.0 33.7 25.8 82.7 28.8 82.7 62.4 30.8 85.2 27.7 34.5 6.4 25.2 24.4 45.4
Cycada[18] 86.7 35.6 80.1 19.8 17.5 38.0 39.9 41.5 82.7 27.9 73.6 64.9 19.0 65.0 12.0 28.6 4.5 31.1 42.0 42.7
BLF[25] 91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5
CAG-UDA[60] 90.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.2 50.2
PyCDA†[26] 90.5 36.3 84.4 32.4 28.7 34.6 36.4 31.5 86.8 37.9 78.5 62.3 21.5 85.6 27.9 34.8 18.0 22.9 49.3 47.4
CD-AM[58] 91.3 46.0 84.5 34.4 29.7 32.6 35.8 36.4 84.5 43.2 83.0 60.0 32.2 83.2 35.0 46.7 0.0 33.7 42.2 49.2
FADA[52] 92.5 47.5 85.1 37.6 32.8 33.4 33.8 18.4 85.3 37.7 83.5 63.2 39.7 87.5 32.9 47.8 1.6 34.9 39.5 49.2
FDA[59] 92.5 53.3 82.4 26.5 27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7 46.4 50.5
SA-I2I[34] 91.2 43.3 85.2 38.6 25.9 34.7 41.3 41.0 85.5 46.0 86.5 61.7 33.8 85.5 34.4 48.7 0.0 36.1 37.8 50.4
PIT[31] 87.5 43.4 78.8 31.2 30.2 36.3 39.9 42.0 79.2 37.1 79.3 65.4 37.5 83.2 46.0 45.6 25.7 23.5 49.9 50.6
IAST[32] 93.8 57.8 85.1 39.5 26.7 26.2 43.1 34.7 84.9 32.9 88.0 62.6 29.0 87.3 39.2 49.6 23.2 34.7 39.6 51.5
DACS[47] 89.9 39.7 87.9 30.7 39.5 38.5 46.4 52.8 88.0 44.0 88.7 67.0 35.8 84.4 45.7 50.2 0.0 27.2 34.0 52.1
RPT†[61] 89.2 43.3 86.1 39.5 29.9 40.2 49.6 33.1 87.4 38.5 86.0 64.4 25.1 88.5 36.6 45.8 23.9 36.5 56.8 52.6
SAC[2] 90.4 53.9 86.6 42.4 27.3 45.1 48.5 42.7 87.4 40.1 86.1 67.5 29.7 88.5 49.1 54.6 9.8 26.6 45.3 53.8
SAC*[2] 89.9 54.0 86.2 37.8 28.9 45.9 46.9 47.7 88.0 44.8 85.5 66.4 30.3 88.6 50.5 54.5 1.5 17.0 39.3 52.8
DACS+PAC(ours) 93.2 58.8 87.2 33.3 35.1 38.6 41.8 51.4 87.4 45.8 88.3 64.8 31.6 84.3 51.7 53.4 0.6 31.3 50.6 54.2
Table 3. SYNTHIA → Cityscapes results: Classwise and mean (over 16 classes) IoU comparison of our PAC-UDA with prior works. † de-
notestheuseofPSPNet[63],*denotesourimplementationofSACwitharestrictedconfiguration(GROUPSIZE=2)comparedtooriginalSACmethod
(GROUPSIZE=4).AllothermethodsuseDeepLabV2[9]architecture.
mIoU
SPIGAN[24] 71.1 29.8 71.4 3.7 0.3 33.2 6.4 15.6 81.2 78.9 52.7 13.1 75.9 25.5 10.0 20.5 36.8
DCAN[56] 82.8 36.4 75.7 5.1 0.1 25.8 8.0 18.7 74.7 76.9 51.1 15.9 77.7 24.8 4.1 37.3 38.4
DISE[7] 91.7 53.5 77.1 2.5 0.2 27.1 6.2 7.6 78.4 81.2 55.8 19.2 82.3 30.3 17.1 34.3 41.5
AdvEnt[50] 85.6 42.2 79.7 8.7 0.4 25.9 5.4 8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0 41.2
DADA[51] 89.2 44.8 81.4 6.8 0.3 26.2 8.6 11.1 81.8 84.0 54.7 19.3 79.7 40.7 14.0 38.8 42.6
CAG-UDA[60] 84.7 40.8 81.7 7.8 0.0 35.1 13.3 22.7 84.5 77.6 64.2 27.8 80.9 19.7 22.7 48.3 44.5
PIT[31] 83.1 27.6 81.5 8.9 0.3 21.8 26.4 33.8 76.4 78.8 64.2 27.6 79.6 31.2 31.0 31.3 44.0
PyCDA†[26] 75.5 30.9 83.3 20.8 0.7 32.7 27.3 33.5 84.7 85.0 64.1 25.4 85.0 45.2 21.2 32.0 46.7
FADA[52] 84.5 40.1 83.1 4.8 0.0 34.3 20.1 27.2 84.8 84.0 53.5 22.6 85.4 43.7 26.8 27.8 45.2
DACS[47] 80.6 25.1 81.9 21.5 2.9 37.2 22.7 24.0 83.7 90.8 67.6 38.3 82.9 38.9 28.5 47.6 48.3
IAST[32] 81.9 41.5 83.3 17.7 4.6 32.3 30.9 28.8 83.4 85.0 65.5 30.8 86.5 38.2 33.1 52.7 49.8
RPT†[61] 88.9 46.5 84.5 15.1 0.5 38.5 39.5 30.1 85.9 85.8 59.8 26.1 88.1 46.8 27.7 56.1 51.2
SAC[2] 89.3 47.2 85.5 26.5 1.3 43.0 45.5 32.0 87.1 89.3 63.6 25.4 86.9 35.6 30.4 53.0 52.6
SAC*[2] 91.7 52.7 85.1 22.6 1.5 42.2 44.1 30.9 82.5 73.8 63.0 20.9 84.9 29.5 26.9 52.2 50.3
SAC+PAC(ours) 83.2 40.5 85.4 30.0 2.0 43.0 42.2 33.8 86.3 89.8 65.3 33.5 85.1 35.2 29.9 55.3 52.5
DACSapproach,weadoptthetrainingandvalidationsplits FromTable1,weobservethatbasemethodsregularised
of Cityscapes used in SAC to maintain benchmark consis- with our constraint always, and sometimes significantly,
tency across different base methods. In terms of architec- outperformstheunregularisedversionintermsofmIoU(by
ture, DACS and SAC use a standard DeepLabv2 [9] back- up to 2.2%). Secondly, the improvement is across various
bonewhereasCAGaugmentsthisbackbonewithadecoder categories of both stuffs and things type. Some of these
model (see [60] for details). For the sake of fair compari- include sidewalk (up to 9.6%), sky (up to 2.4%), traffic
son,wetryourbesttoachievebaselineaccuraciesthatare light (up to 2.1%), traffic sign (up to 4.4%) and bike (up
atleastasgoodasthepublishedresults. Whileweachieved to 7.2%) classes under GTA→Cityscapes while wall
slightly lower performance on SAC due to resource con- (upto7.4%), fence(upto2.0%), person(upto2.5%)and
straints,weachievesuperioraccuraciesforDACSandCAG bus(upto5.7%)classesunderSYNTHIA→Cityscapes.
baselines.Thus,thesemethodsserveasstrongbaselinesfor Whiledifferentadaptationsettingsfavourdifferentclasses,
evaluatingourapproach. aparticularlystrikingobservationisthatlargegainsareob-
7
daor
daor
klawedis
klawedis
gnidliub
gnidliub
llaw
llaw
ecnef elop
ecnef
thgil
elop
ngis
thgil
.egev
ngis
niarret
elbategev
yks
yks
nosrep
nosrep
redir rac
redir
kcurt
rac
sub
sub
niart
rotom
rotom
ekib
ekib
Figure3.QualitativeresultsonCityscapes[12]postadaptationfromGTA[37]:Bluedashedboxeshighlightthesemanticclassesthat
ourregularizedversion(DACS+PAC)isabletopredictmoreaccuratelythanthebasemethod(DACS).Furthervisualisationsareprovided
inthesupplementary.
tained in both frequent (sidewalk, wall) and less-frequent 4.2.PriorWorksComparison
(bus, bike)classes. Wesuspectthatsuchuniformityarises
Inthissection,wecompareourbestperformingmethod
from our object-region aware constraint that is agnostic to
withpriorworksundereachdomainsettings
thestatisticaldominanceofspecificclasses. Finally,Fig. 3
visualisestheseobservationsbycomparingthepredictions GTA→Cityscapes (Table 2): In terms of mIoU, our
ofDACSandDACS+PACmodels(trainedonGTA)onran- DACS+PAC outperforms the state-of-the-art (SAC) by
domlyselectedexamplesfromCityscapesvalidationsplit. 0.4% despite having a simpler training objective (no fo-
cal loss regularizer or importance sampling) and no adap-
tive batch normalisation. In particular, our approach out-
Table4. Ablations: Comparingtheeffectsofindividualcomponentsof
performsSACsignificantlyinroad,sidewalk,fence,terrain,
theregulariser(PAC)onfinalperformance(mIoU).Here,thefullmodel
isDACS+PAC,andtheadaptationsettingisGTA→Cityscapes; hy- sky, rider, motorcycle and bike classes by 1.9%−11.3%.
perparametersare: ks = 50,b = 200,δ peak = 0.0025;“PL”refersto More interestingly, this observation holds when compared
pseudo-labelling.WeincludeclasswiseIoUsinthesupplementary.
to other prior works as well, wherein our model improves
IoUs for both dominant categories like road and sidewalk
Configuration mIoU
as well as less frequent categories like traffic-sign and ter-
rain. For classes like sidewalk, we suspect that structural
All 54.2
constraintsbasedonourregularizerreducescontextualbias
Only PL 49.3
[41],responsibleforcoarseboundaries.
Only Depth + RGB segments 51.9 SYNTHIA→Cityscapes (Table 3): In this setting, our
Only Depth segments w/ PL 51.7 bestperformingmethodoutperformsallbutonepriormeth-
Only RGB segments w/ PL 52.1 ods,oftenbysignificantmargins. WhileourSAC+PACun-
derresourceconstraintscomparesfavourablytotheofficial
implementationofSAC(withlargerGROUP SIZE),itsig-
8
nificantlyoutperformsourimplementationofSACwhichis JointConferenceonNeuralNetworks(IJCNN),pages1–8.
a more fair comparison due to same resource constraints. IEEE,2020. 2
Nevertheless, our approach improves the best previous re- [4] M. Baktashmotlagh, M. T. Harandi, B. C. Lovell, and M.
sultsonwallclassby3.5%andachievesstate-of-the-arton Salzmann. Unsuperviseddomainadaptationbydomainin-
poleandsignclasses. variantprojection. InICCV,2013. 2
[5] Konstantinos Bousmalis, Nathan Silberman, David Dohan,
4.3.Ablations Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-
level domain adaptation with generative adversarial net-
In this section, we deconstruct our multi-modal regu-
works. CoRR,2016. 2
larizer (PAC) to quantify the effect of individual compo-
[6] KonstantinosBousmalis,GeorgeTrigeorgis,NathanSilber-
nents on final performance. In Table 4, the ‘ALL’ con-
man, DilipKrishnan, andDumitruErhan. Domainsepara-
figuration corresponds to our original formulation. ‘Only tionnetworks. NeurIPS,2016. 2
PL’configurationestimatestheobject-regionsusingjustthe [7] Wei-LunChang,Hui-PoWang,Wen-HsiaoPeng,andWei-
pseudo-labels and hence ignores complementary informa- Chen Chiu. All about structure: Adapting structural infor-
tion from depth. ‘Only Depth+RGB segments’ do not use mationacrossdomainsforboostingsemanticsegmentation.
pseudo-labelstodefineregionlabelsandinsteadtreatseach CoRR,2019. 7
Depth+RGBsegmentasauniqueobjectcategory. Thecon- [8] Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong,
figurationsinnexttworowsuseonlyoneofthetwomodal- Xinghao Ding, Yue Huang, Tingyang Xu, and Junzhou
itiesforestimatingobjectregionswhilestillusingpseudo- Huang. Progressivefeaturealignmentforunsuperviseddo-
mainadaptation. InProceedingsoftheIEEE/CVFConfer-
labels to define region labels. We observe that contrastive
ence on Computer Vision and Pattern Recognition, pages
regulariserbasedononlypseudo-labelsperformstheworst
627–636,2019. 3
and significantly below the one based on just multimodal
[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
segments. This is intuitive because reusing pseudo-labels
KevinP.Murphy,andAlanLoddonYuille.Deeplab:Seman-
asaregularisationwithoutauxiliaryinformationreinforces
ticimagesegmentationwithdeepconvolutionalnets,atrous
theconfirmationbias. While, purelyRGBbasedsegments
convolution,andfullyconnectedcrfs.IEEETransactionson
leadtobetterobjectnessconstraintthanpurelydepth-based PatternAnalysisandMachineIntelligence,2018. 1,7
ones (as can be seen in Fig. 2), combining the two (ALL [10] TingChen,SimonKornblith,MohammadNorouzi,andGe-
config.) yieldsthebestresults. offreyHinton. Asimpleframeworkforcontrastivelearning
ofvisualrepresentations,2020. 2
5.Conclusion [11] Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.
Learningsemanticsegmentationfromsyntheticdata:Ageo-
In this work, we proposed a multi-modal regularisa-
metricallyguidedinput-outputadaptationapproach. CVPR,
tion scheme for self-training approaches in unsupervised 2019. 2
domain adaptation for semantic segmentation. We de- [12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
rive an objectness constraint from multi-modal clustering Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
that is then used to formulate a contrastive objective for Franke, Stefan Roth, and Bernt Schiele. The cityscapes
regularisation. We show that this regularisation consis- datasetforsemanticurbansceneunderstanding. InCVPR,
tently improves upon different types of self-training meth- 2016. 1,2,5,6,8,14
odsandevenachievesstate-of-the-artperformanceinpopu- [13] NicolasCourty,Re´miFlamary,AmauryHabrard,andAlain
larbenchmarks. Inthefuture,weplantostudytheeffectof Rakotomamonjy. Joint distribution optimal transportation
fordomainadaptation. InNeurIPS,2017. 2
othermodalitieslike3Dpoint-cloudsinsemanticsegmen-
[14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
tation.
cal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario
Marchand,andVictorLempitsky. Domain-adversarialtrain-
References
ing of neural networks. Advances in Computer Vision and
[1] RadhakrishnaAchanta, AppuShaji, KevinSmith, Aurelien PatternRecognition,2017. 2
Lucchi, Pascal Fua, and Sabine Su¨sstrunk. Slic superpix- [15] Cle´ment Godard, Oisin Mac Aodha, and Gabriel J. Bros-
els compared to state-of-the-art superpixel methods. IEEE tow. Diggingintoself-supervisedmonoculardepthestima-
TransactionsonPatternAnalysisandMachineIntelligence, tion. ICCV,2019. 5
2012. 5,15 [16] Yves Grandvalet and Yoshua Bengio. Semi-supervised
[2] Nikita Araslanov, , and Stefan Roth. Self-supervised aug- learningbyentropyminimization. InNeurIPS,2005. 2
mentation consistency for adapting semantic segmentation. [17] Bharath Hariharan, Pablo Arbela´ez, Ross B. Girshick, and
InCVPR,2021. 1,2,3,6,7,12 Jitendra Malik. Simultaneous detection and segmentation.
[3] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, InECCV,2014. 5
and Kevin McGuinness. Pseudo-labeling and confirmation [18] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
biasindeepsemi-supervisedlearning.In2020International Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Dar-
9
rell. Cycada: Cycle-consistent adversarial domain adap- [34] Luigi Musto and Andrea Zinelli. Semantically adaptive
tation. In International Conference on Machine Learning image-to-imagetranslationfordomainadaptationofseman-
(ICML),2018. 1,2,7 ticsegmentation. InBMVC,2020. 7
[19] MaximilianJaritz, Tuan-HungVu, RaouldeCharette, Em- [35] Fei Pan, Inkyu Shin, Franc¸ois Rameau, Seokju Lee, and
ilieWirbel, andPatrickPerez. xmuda: Cross-modalunsu- InSoKweon. Unsupervisedintra-domainadaptationforse-
pervised domain adaptation for 3d semantic segmentation. manticsegmentationthroughself-supervision. CVPR,2020.
CVPR,2020. 3 2
[20] Fredrik D. Johansson, David A Sontag, and Rajesh Ran- [36] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain
ganath. Support and invertibility in domain-invariant rep- adaptationviatransfercomponentanalysis. IEEETransac-
resentations. InAISTATS,2019. 3 tionsonNeuralNetworks,2011. 2
[21] GuoliangKang,LuJiang,YiYang,andAlexanderGHaupt- [37] StephanRRichter,VibhavVineet,StefanRoth,andVladlen
mann. Contrastiveadaptationnetworkforunsuperviseddo- Koltun. Playing for data: Ground truth from computer
mainadaptation. InProceedingsoftheIEEE/CVFConfer- games.InProceedingsoftheEuropeanConferenceonCom-
ence on Computer Vision and Pattern Recognition, pages puterVision(ECCV),2016. 2,6,8,14
4893–4902,2019. 3 [38] German Ros, Laura Sellart, Joanna Materzynska, David
[22] PrannayKhosla,PiotrTeterwak,ChenWang,AaronSarna, Vazquez,andAntonioMLopez.Thesynthiadataset:Alarge
YonglongTian,PhillipIsola,AaronMaschinot,CeLiu,and collectionofsyntheticimagesforsemanticsegmentationof
DilipKrishnan.Supervisedcontrastivelearning.InNeurIPS, urbanscenes. InCVPR,2016. 2,6
2020. 2 [39] AshutoshSaxena,JamieSchulte,AndrewYNg,etal.Depth
estimationusingmonocularandstereocues. InIJCAI,vol-
[23] MyeongjinKimandHyeranByun. Learningtextureinvari-
ume7,pages2197–2203,2007. 2,5
ant representation for domain adaptation of semantic seg-
[40] Uri Shalit, Fredrik D. Johansson, and David Sontag. Es-
mentation. CVPR,2020. 3
timatingindividualtreatmenteffect: Generalizationbounds
[24] Kuan-HuiLee,Germa´nRos,JieLi,andAdrienGaidon.Spi-
andalgorithms. InICML,2017. 2
gan:Privilegedadversariallearningfromsimulation. ArXiv,
[41] Rakshith Shetty, Bernt Schiele, and Mario Fritz. Not us-
2019. 2,3,7
ingthecartoseethesidewalk–quantifyingandcontrolling
[25] YunshengLi,LuYuan,andNunoVasconcelos.Bidirectional
theeffectsofcontextinclassificationandsegmentation. In
learningfordomainadaptationofsemanticsegmentation.In
CVPR,June2019. 8
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
[42] RuiShu,HungHBui,HirokazuNarui,andStefanoErmon.
sion and Pattern Recognition, pages 6936–6945, 2019. 3,
Adirt-tapproachtounsuperviseddomainadaptation. arXiv,
7
2018. 3
[26] Qing Lian, Fengmao Lv, Lixin Duan, and Boqing Gong.
[43] S.Si,D.Tao,andB.Geng.Bregmandivergence-basedregu-
Constructingself-motivatedpyramidcurriculumsforcross-
larizationfortransfersubspacelearning. IEEETransactions
domainsemanticsegmentation:Anon-adversarialapproach.
onKnowledgeandDataEngineering,2010. 2
In Proceedings of the IEEE/CVF International Conference
[44] Kihyuk Sohn. Improved deep metric learning with multi-
onComputerVision,pages6758–6767,2019. 7
classn-pairlossobjective. InNeurIPS,2016. 2
[27] Marcos Llobera. Building past landscape perception with
[45] Sinisa Stekovic, Friedrich Fraundorfer, and Vincent Lep-
gis: Understandingtopographicprominence. JournalofAr-
etit. Casting geometric constraints in semantic segmenta-
chaeologicalScience-JARCHAEOLSCI,2001. 5
tion as semi-supervised learning. In Proceedings of the
[28] MingshengLong,YueCao,JianminWang,andMichaelJor-
IEEE/CVFWinterConferenceonApplicationsofComputer
dan.Learningtransferablefeatureswithdeepadaptationnet-
Vision,pages1854–1863,2020. 3
works. In International conference on machine learning,
[46] Subhashree Subudhi, Ram Narayan Patro, Pradyut Kumar
pages97–105.PMLR,2015. 2
Biswal,andFabioDell’Acqua. Asurveyonsuperpixelseg-
[29] MingshengLong, HanZhu, JianminWang, andMichaelI. mentation as a preprocessing step in hyperspectral image
Jordan. Deep transfer learning with joint adaptation net- analysis. IEEEJournalofSelectedTopicsinAppliedEarth
works. InICML,2017. 2 ObservationsandRemoteSensing,14:5015–5035,2021. 12
[30] Fengmao Lv, Tao Liang, Xiang Chen, and Guosheng Lin. [47] Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, and
Cross-domain semantic segmentation via domain-invariant Lennart Svensson. Dacs: Domain adaptation via cross-
interactiverelationtransfer. InCVPR,2020. 2 domainmixedsampling. WACV,2021. 3,6,7
[31] Fengmao Lv, Tao Liang, Xiang Chen, and Guosheng Lin. [48] Y.-H.Tsai,W.-C.Hung,S.Schulter,K.Sohn,M.-H.Yang,
Cross-domain semantic segmentation via domain-invariant and M. Chandraker. Learning to adapt structured output
interactiverelationtransfer. InCVPR,2020. 7 spaceforsemanticsegmentation. InCVPR,2018. 1,2
[32] KeMei,ChuangZhu,JiaqiZou,andShanghangZhang. In- [49] E.Tzeng,J.Hoffman,K.Saenko,andT.Darrell.Adversarial
stanceadaptiveself-trainingforunsuperviseddomainadap- discriminativedomainadaptation. InCVPR,2017. 2
tation. InECCV,2020. 7 [50] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Mathieu
[33] Krikamol Muandet, David Balduzzi, and Bernhard Cord,andPatrickPe´rez. Advent: Adversarialentropymin-
Scho¨lkopf. Domain generalization via invariant feature imization for domain adaptation in semantic segmentation.
representation. ICML,2013. 2 arXiv,2018. 7
10
[51] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu
Cord,andPatrickPe´rez. Dada: Depth-awaredomainadap-
tation in semantic segmentation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages7364–7373,2019. 1,2,3,7
[52] HaoranWang, TongShen, Wei Zhang, Ling-Yu Duan, and
Tao Mei. Classes matter: A fine-grained adversarial ap-
proachtocross-domainsemanticsegmentation.InEuropean
Conference on Computer Vision, pages 642–659. Springer,
2020. 7
[53] QinWang,DengxinDai,LukasHoyer,OlgaFink,andLuc
Van Gool. Domain adaptive semantic segmentation with
self-superviseddepthestimation. InICCV,2021. 2,3
[54] Zhonghao Wang, Mo Yu, Yunchao Wei, Roge´rio Schmidt
Feris, Jinjun Xiong, Wen mei W. Hwu, Thomas S. Huang,
and Humphrey Shi. Differential treatment for stuff and
things: A simple unsupervised domain adaptation method
forsemanticsegmentation. CVPR,2020. 2
[55] Yifan Wu, Ezra Winston, Divyansh Kaushik, and
Zachary Chase Lipton. Domain adaptation with
asymmetrically-relaxed distribution alignment. ICML,
2019. 3
[56] Zuxuan Wu, Xintong Han, Yen-Liang Lin, Mustafa
GokhanUzunbas,TomGoldstein,SerNamLim,andLarryS
Davis.Dcan:Dualchannel-wisealignmentnetworksforun-
supervisedsceneadaptation. InECCV,2018. 7
[57] Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen.
Learningsemanticrepresentationsforunsuperviseddomain
adaptation.InInternationalconferenceonmachinelearning,
pages5423–5432.PMLR,2018. 3
[58] JinyuYang,WeizhiAn,ChaochaoYan,PeilinZhao,andJun-
zhouHuang. Context-awaredomainadaptationinsemantic
segmentation. InWACV,2021. 7
[59] Yanchao Yang and Stefano Soatto. FDA: Fourier domain
adaptationforsemanticsegmentation. InCVPR,2020. 7
[60] QimingZhang,JingZhang,WenyuLiu,andD.Tao. Cate-
goryanchor-guidedunsuperviseddomainadaptationforse-
manticsegmentation. InNeurIPS,2019. 1,2,3,4,6,7
[61] Yiheng Zhang, Zhaofan Qiu, Ting Yao, Chong-Wah Ngo,
Dong Liu, and Tao Mei. Transferring and regularizing
prediction for semantic segmentation. In Proceedings of
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages9621–9630,2020. 1,2,3,6,7,12
[62] HanZhao, RemiTachetdesCombes, KunZhang, andGe-
offrey J. Gordon. On learning invariant representation for
domainadaptation. ICML,2019. 3
[63] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In
CVPR,2017. 7
[64] Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang.
Unsuperviseddomainadaptationforsemanticsegmentation
via class-balanced self-training. In Proceedings of the Eu-
ropeanconferenceoncomputervision(ECCV),pages289–
305,2018. 2,3,4,12
11
Inthissupplementary,weprovideadditionaldetailsand and pseudo-labels as described in Section 4.3. While, Ta-
analysisforourproposedmethod,PAC-UDA.Algorithm1, ble6highlightstheimportanceofcombiningallmodalities
providesastep-by-stepprocedureforunsuperviseddomain with pseudo-labels for the best mean IoU, there are a few
adaptationviaPAC-UDA. otherimportantobservationswithrespecttoclasswiseIoUs.
For instance, using “PL” for objectness constraints sig-
A.HyperparametersforMainExperiments
nificantly underperforms other settings (by upto 49 IoU)
in rare source-classes, like motorcycle and bike (Figure
4). This gap is surprisingly large (by upto 38.5 IoU) even
Table5.HyperparametersusedinTable1
when compared to “Depth-RGB”. We attribute this large
method k b δ τ performance gap to the class-imbalance problem [64] that
S peak p
is known to adversely affect self-training in the absence
CAG+PAC 50 200 0.0025 0.90
of class-balanced losses. However, incorporating our ob-
SAC+PAC 50 200 0.0025 0.90
jectness constraint alleviates the rare-class IoUs signifi-
DACS+PAC 25 200 0.001 0.90
cantly without losing performance in frequent classes (ex-
cept, sky). These results provide strong evidence for the
ToreporttheresultsinTable1,Table2andTable3,we
normalisationofclass-relatedstatisticaleffectsinthepres-
choosethebesthyperparametersfollowingstandardcross-
enceofmultimodalobjectness-constraints.
validation on a random subset of Cityscapes train-split in-
troduced in [2]. For base methods, we use the default hy- Another interesting insight arises from comparing
perparametersfromrespectivepapers. InTable5,wesum- “Depth-PL” and “RGB-PL” settings that demonstrates the
marisethehyperparametersforTable1. Sincetheresultsof complementarity of the two modalities. Among the more
ourapproachinTable2andTable3areasubsetofTable1, frequentsource-classes(Figure4),purelyRGB-basedcon-
theabovehyperparametersapplythereaswell. straint considerably outperforms purely depth-based con-
straintincategoriessuchasroad,sidewalkandcarwhereas
B.AdditionalAblations theconverseistrueforothercategorieslikewall,pole,ter-
rain and person. The outperformance of depth-based con-
Inthissection,weprovideadditionalablationstudiesfor straint on pole and person is intuitive since these objects
DACS+PAC on GTA→Cityscapes. The default hyper- have very small depth range compared to the scene depth
parameter configuration is: k = 25, b = 200, δ = andhencecanbeeasilydetectedusingthedepthhistogram
s peak
0.001, τ = 0.9; unless otherwise stated. Also, we train (seeSection3.1formorediscussion).
p
eachsettingforT (=125000)iterations.
train
B.1. Importance of Multiple Modalities and
B.2.ImportanceofRGB-segments
Pseudo-Labels
In the past, image clustering has been often used as an
effective preprocessing step for segmentation [46,61]. In-
spiredbytheseworks,inTable7,wetesttheextenttowhich
purelySLICbasedRGB-segmentscaninfluencetheobject-
ness constraint and consequently, the final performance of
our PAC-UDA. Specifically, we tabulate the performance
withvaryingnumberofSLICsegments,k andcompareit
s
toourdefaultconfiguration,“ALL(k =25)”.
s
WeobservethatwhenusingonlyRGB-segments(with-
outdepth)forobject-regionestimates,thereexistsaninter-
mediatevaluealongtherangeofk ∈ {25,50,100}where
s
the semantic segmentation performance peaks. This trend
empiricallyvalidatesourintuitionforchoosingthebestk
s
as discussed in Section 3.1. In fact, too small a value can
be highly undesirable as it can lead to worse results (52.1
Figure4.Pixel-wiseclassdistributioninGTAdataset
mIoU) than even the base method (52.8 mIoU). It is how-
ever, interesting to note that even with the most optimal
In Table 6, we provide the complete results (including k = 50, just RGB based objectness constraint underper-
s
classwise IoUs) for the ablations on individial modalities formsourmultimodalconstraint(“ALL”)by∼1mIoU.
12
Table6.EffectofIndividualModalitiesandPseudoLabels:Comparingtheeffectsofindividualmodalitiesusedtoestimateobject-regionsandpseudo-
labelsonfinalperformance(mIoU).ThistableisanextendedversionofTable4withclasswiseIoUs.MappingofconfigurationnamestothoseinTable4-
PL:OnlyPL;Depth-RGB:OnlyDepth+RGBsegments;Depth-PL:OnlyDepthsegmentsw/PL;RGB-PL:OnlyRGBsegmentsw/PL.RefertoSection
4.3forconfigurationspecificdetails.
Configuration mIoU
All 93.2 58.8 87.2 33.3 35.1 38.6 41.8 51.4 87.4 45.8 88.3 64.8 31.6 84.3 51.7 53.4 0.6 31.3 50.6 54.2
PL 93.7 58.7 86.8 27.3 29.7 35.4 41.6 50.6 87.1 46.7 89.2 65.2 37.1 87.4 41.3 49.8 0.0 7.0 1.6 49.3
Depth-RGB 94.1 58.1 86.2 38.2 30.3 34.8 37.8 41.3 86.7 46.1 87.5 62.4 31.0 86.8 52.5 49.1 0.0 24.5 40.1 51.9
Depth-PL 93.3 61.9 86.7 31.8 35.9 36.1 43.3 50.2 86.2 41.2 86.4 65.0 32.2 82.1 31.9 50.4 0.9 23.1 43.6 51.7
RGB-PL 95.1 65.3 86.1 25.9 30.1 35.4 39.1 41.2 85.2 37.9 86.2 61.4 26.7 87.9 50.9 50.6 0.0 35.8 50.4 52.1
Table7. ImportanceofRGB-segments: ComparingtheeffectofonlyRGB-segmentswithdifferentvaluesofks. here,PL:Pseudo-Labels;RGB-PL:
Objectness-constraintwithonlyRGBsegmentsandPL;ALL:Objectness-constraintwithRGB-segments+Depth-segmentsandPL
Configuration mIoU
All(ks=25) 93.2 58.8 87.2 33.3 35.1 38.6 41.8 51.4 87.4 45.8 88.3 64.8 31.6 84.3 51.7 53.4 0.6 31.3 50.6 54.2
RGB-PL(ks=25) 95.1 65.3 86.1 25.9 30.1 35.4 39.1 41.2 85.2 37.9 86.2 61.4 26.7 87.9 50.9 50.6 0.0 35.8 50.4 52.1
RGB-PL(ks=50) 94.6 63.4 86.8 28.7 30.7 37.6 42.8 51.3 86.8 44.9 87.9 64.9 32.5 87.8 42.7 45.4 0.0 32.6 51.2 53.3
RGB-PL(ks=100) 94.4 62.1 86.2 29.2 32.5 34.2 40.0 50.2 86.2 47.1 87.4 63.0 32.7 87.9 39.4 45.3 0.1 32.6 52.8 52.8
Table8. EffectoftheContrastiveObjective:ComparingtwodifferentformulationsofcontrastiveobjectiveasdefinedinEqn.7andSectionB.3andan
upperboundconfiguration,GTlab(target-domainground-truthlabels
Configuration mIoU
Lt 93.2 58.8 87.2 33.3 35.1 38.6 41.8 51.4 87.4 45.8 88.3 64.8 31.6 84.3 51.7 53.4 0.6 31.3 50.6 54.2
obj
Lt+ 94.2 59.4 86.7 35.8 32.1 36.8 40.5 49.4 86.5 41.9 86.0 63.5 27.1 89.1 53.7 54.5 2.5 27.3 45.7 53.3
obj
Table9.Effectofregion-labelthreshold,τponFinalPerformance:
Threshold mIoU
0.70 93.9 60.4 86.5 32.5 30.4 34.9 39.9 48.8 86.4 45.6 88.0 63.0 27.6 87.0 39.9 49.2 1.9 32.5 47.9 52.4
0.80 92.9 51.3 86.6 31.5 32.4 36.7 42.8 52.1 86.8 44.7 87.5 65.4 34.5 89.2 48.8 56.3 0.2 23.8 45.1 53.1
0.90 93.2 58.8 87.2 33.3 35.1 38.6 41.8 51.4 87.4 45.8 88.3 64.8 31.6 84.3 51.7 53.4 0.6 31.3 50.6 54.2
0.95 93.4 55.9 86.1 28.7 30.0 33.2 40.5 45.3 86.6 45.7 87.8 64.2 31.6 89.1 50.4 50.7 0.0 10.5 28.3 50.4
B.3.ContrastiveLossAnalysis into the classwise IoUs reveal that less-common classes
primarily contribute the the overall worse performance of
Weanalyzetheeffectofspecificformofthecontrastive Lt+(p). We suspect that increasing the influence of, and
obj
lossfunctioninTable8. RecallthatinEqn.7,ourformula- consequently the noise in, region-labels affect these less-
tionofthecontrastivelossmaximizesthesimilarityofeach commonclassesmoreadverselythancommonclasseslike
pixel embedding, z˜ p to only a prototype of the region, U k road, sidewalk, wall and car. Finally, this ablation guides
thatincludespixelp. Here,weintroduceanothervariantof our decision to adopt Lt (p) as the default form of con-
obj
thatloss,Lt o+ bj(p)thatmaximizesthesimilarityofz˜ ptopro- trastivelossinEqn.7.
totypesofallvalidregions,{U }K \Ω(k)thatsharethe
k k=1
same region-label. While, originially, region-labels could B.4.ImportanceofRegion-LabelThreshold
influencethelossfunctiononlyviadissimilarityscores, in
An important hyperparameter of our objectness-
Lt+(p),theycaninfluenceviabothsimilarityanddissimi-
obj constraint is the region-label threshold, τ . At higher val-
p
larityscores.
uesofthisthreshold,validobject-regionsaremorelikelyto
We observe that allowing greater region-label influence beapartofasingleobjectandconsistentwiththeground-
in Lt+(p) leads to worse mIoU than Lt (p). Zooming truth semantic category. This will positively influence the
obj obj
13
daor
daor
daor
daor
klawedis
klawedis
klawedis
klawedis
gnidliub
gnidliub
gnidliub
gnidliub
llaw
llaw
llaw
llaw
ecnef
ecnef
ecnef
ecnef
elop
elop
elop
elop
thgil
thgil
thgil
thgil
ngis
ngis
ngis
ngis
.egev
.egev
.egev
.egev
niarret
niarret
niarret
niarret
yks
yks
yks
yks
nosrep
nosrep
nosrep
nosrep
redir
redir
redir
redir
rac
rac
rac
rac
kcurt
kcurt
kcurt
kcurt
sub
sub
sub
sub
niart
niart
niart
niart
rotom
rotom
rotom
rotom
ekib
ekib
ekib
ekib
Figure5.AdditionalqualitativeresultsonCityscapes[12]postadaptationfromGTA[37]:Bluedashedboxeshighlightthesemantic
classesthatourregularizedversion(DACS+PAC)isabletopredictmorereliablythanthebasemethod(DACS).
target-domainperformance. Atthesame, timethenumber
of such valid object-regions is likely to be small, which,
may reduce the overall effect of the objectness-constraint
ontarget-domainperformance.Asonedecreasesthethresh-
old,thenumberofvalid-regionswillincreaseattheexpense
ofregion-labelconsistencywithground-truth. Thus,evalu-
atingtheperformanceoverarangeofvaluesiscrucial.
Indeed, we observe in Table 9 that the mIoU increases
withincreaseinthresholduptoacertainpoint(τ = 0.90),
p
beyond which the performance deteriorates. We, thus, set
0.90asourdefaultthresholdforallourexperiments.
C.AdditionalVisualisations
In Figure 5, we provide additional qualitative compari-
son between DACS+PAC, DACS and the ground-truth un-
derGTA→Cityscapessettings.
14
Algorithm1UnsuperviseddomainadaptationviaPAC-UDA
Input: Pseudo-label (y˜); Target training dataset with depth (Dt = {(xt,ht,y˜t)}Nt ); Initial model parameters (θ =
depth i i i i=1 0
{ψ ,φ }); Number of histogram bins (b); Peak prominence threshold (δ ); Number of RGB-segments (k ); Spatial
0 0 peak s
dimensions of depth map (H ×W); Region-label threshold (τ ); Objectness constraint loss weight (α ); Number of
p obj
trainingiterations(T )
train
Output: Target-domainadaptedparameters(θ ={ψ ,φ })
∗ ∗ ∗
1: fort tr←1toT train do
2: {(xt,ht,yt)}N tB ∼Dt (cid:46)Randomlysampleatrainingbatchfromtarget-domain
i i i i=1 h
3: ComputeL uda (cid:46)Self-trainingbasedadaptationobjective(seeSection3)
4: L obj =0 (cid:46)Initialiseobjectness-constraint
5: fori←1toNB do
t
6:
7: InitializeVd ={} (cid:46)Emptylistofdepth-segments
8: Hist(cid:0) {h im}H mW =1; b(cid:1) → Fd (cid:46)Histogramofdepthvalues(HOD)
9: FindPeaks(Fd; δ peak) → {µ k}k kd
=1
(cid:46)Cluster-centerassignmentusingHOD
10: fork ← 1tok d do
11: V kd ={m|m∈{1,...,HW}, |h m−µ k|<|h m−µ k(cid:48)|∀k(cid:48) (cid:54)=k} (cid:46)Depthsegments
12: Vd.append(Vd) (cid:46)Depth-segmentlistupdate
k
13: endfor
14:
15: InitializeVs ={} (cid:46)EmptylistofRGB-segments
16: SLIC(x i; k s) → {L k}k ks
=1
(cid:46)RGB-segmentlabellingusingSLIC[1]
17: fork ← 1tok s do
18: V ks ={m|m∈{1,...,HW}, label(m)=L k} (cid:46)RGB-segments
19: Vs.append(Vs) (cid:46)RGB-segmentlistupdate
k
20: endfor
21:
22: InitializeV ={} (cid:46)Emptylistofobject-regions
23: Initializek =0 (cid:46)region-index
24: fori(cid:48)←1tok s do
25: forj(cid:48)←1tok d do
26: k ← k+1 (cid:46)Region-indexupdate
27: V k ={m|m∈V is (cid:48), m∈V jd (cid:48)} (cid:46)Uniqueobject-regionassignment
28: V.append(V k) (cid:46)Object-regionlistupdate
29: endfor
30: endfor
31:
32: F k =Histogram({y˜ it m} m∈Vk)∀k ={1,...,K(cid:48)} (cid:46)Region-wisefrequencyofpseudo-labelclasses
33: InitializeU ={} (cid:46)Emptylistofvalidregions
34: InitializeL={} (cid:46)Emptylistofvalidregionlabels
35: fork ← 1toK(cid:48) do
maxFk[c]
36: if then (cid:80)c Fk[c] ≥τ p (cid:46)Thresholdonmajority-votingbasedregion-label
c
37: U k =V k (cid:46)Validregionassignment
38: U.append(U k) (cid:46)Valid-regionlistupdate
39: L k =argmaxF k[c] (cid:46)Region-labelassignment
c
40: L.append(L k) (cid:46)Valid-regionlabellistupdate
41: endif
42: endfor
43: UsingU andL,computeLt (cid:46)Objectnessconstraint,Eqn.. 7
obj,i
44: L obj =L obj+Lt obj,i
45: endfor
46: L pac =L uda+ Nαo Bbj ∗Lt obj (cid:46)OverallPAC-UDAobjective
t
47: θ t ← θ t−1−η∇L pac (cid:46)Parameterupdate
48: endfor 15

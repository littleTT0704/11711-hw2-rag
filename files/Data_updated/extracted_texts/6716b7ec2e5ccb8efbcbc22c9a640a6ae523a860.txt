R3 : Refined Retriever-Reader Pipeline for Multidoc2dial
SrijanBansal SumitAgarwal SurajTripathi SireeshGururaja
∗ ∗ ∗ ∗
AdityaVeerubhotla RitamDutt TerukoMitamura EricNyberg
∗
srijanb, sumita, surajt, sgururaj @andrew.cmu.edu
{ }
adityasv, rdutt, teruko @andrew.cmu.edu, ehn@cs.cmu.edu
{ }
LanguageTechnologiesInstitute,CarnegieMellonUniversity
Abstract modelstounderstanduserqueriesandtheirasso-
ciateddialoguecontext,usethemtofindrelevant
In this paper, we present our submission to
groundingdocuments,andthengeneratecoherent
the DialDoc shared task based on the Multi-
responses to user queries. This pipe-lined archi-
Doc2Dialdataset.MultiDoc2Dialisaconversa-
tectureformsthebackboneofthebaselinemodel,
tionalquestionansweringdatasetthatgrounds
dialogues in multiple documents. The task henceforthcalledretriever-reader.
involves grounding a user’s query in a docu- In this paper, we present our approach to the
ment followed by generating an appropriate
MultiDoc2Dial (MDD) task (Feng et al., 2021),
response. We propose several improvements
thesuccessortoDoc2Dial,whichcomplicatesthe
over the baseline’s retriever-reader architec-
Doc2Dialsettingbyconstructingdialoguesthatare
turetoaidinmodelinggoal-orienteddialogues
groundedinmultipledocuments. Eachdocument
grounded in multiple documents. Our pro-
posedapproachemployssparserepresentations issegmentedintomultiplepassages,andthusdoc-
forpassageretrieval,apassagere-ranker,the umentandpassageareinterchangeablyusedinthis
fusion-in-decoderarchitectureforgeneration, paper. Asaresult,modelsmustretrievethedocu-
and a curriculum learning training paradigm. mentsrelevanttothecurrentdialogueturn. These
Ourapproachshowsa12pointimprovement
groundingdocumentscouldpotentiallybedifferent
inBLEUscorecomparedtothebaselineRAG
fromthosegroundedinpreviousdialogueturns.
model.
Weproposeamodelthatimprovesoverthebase-
1 Introduction linemodelbyfocusingoneachcomponentofits
retriever-readerarchitecture. Firstly,weintroduce
Thetaskframeworkofdocument-grounded,conver-
sparse lexical representations in the retriever for
sationalquestionansweringunifiesseveralclosely
matching,asoutlinedinFormaletal.(2021). Sec-
related task frameworks, including open-domain
ondly,wereranktheretriever’sresultsusingtech-
questionanswering(QA),conversationalQA,and
niquesfromFajciketal.(2021). Furthermore,we
knowledge-groundedgeneration. Inopen-domain
update the decoding process to incorporate the
question answering tasks, such as SQuAD (Ra-
fusion-in-decoder (FiD) technique (Izacard and
jpurkar et al., 2018), models are required to re-
Grave,2021). Finally,weusecurriculumlearning
spond to a question with knowledge that may be
totrainourmodels. Weobserveanimprovement
locatedwithinapotentiallylargecollectionofdoc-
of11.9(BLEU)and9.5(F1)pointsonthevalida-
uments. For conversational QA tasks like QuAC
tion;andanimprovementof9.5(BLEU)and10.3
(Choietal.,2018b),thequeriesposedtothemodel
(F1)pointsontestsetintheMDD-SEENsetting
take the form of a dialogue, where previous dia-
comparedtothebaselineRAGmodel. Weachieve
logueturnscontainnecessarycontexttoanswerthe
18.7 and 13.7 points improvement in the BLEU
currentturn’squestion. Bothofthesetaskframe-
andF1metricrespectivelyontheMDD-UNSEEN
workscanbeframedaseitherextractiveQAorab-
testsetcomparedtothebaselineRAGmodel. Our
stractiveQA.Document-groundedconversational
submission (CMU-QA) stands 2nd and 3rd on the
questionansweringtaskslikeCoQA(Reddyetal.,
unseenandseenleaderboards1 respectively.
2019a)andDoc2Dial(Fengetal.,2020b)combine
the above two frameworks. This setting requires
1https://eval.ai/web/challenges/challenge-
∗Equalcontribution page/1437/leaderboard/3577
148
ProceedingsoftheSecondDialDocWorkshoponDocument-groundedDialogueandConversationalQuestionAnswering,pages148-154
May26,2022©2022AssociationforComputationalLinguistics
2 RelatedWorks ery passage is computationally infeasible. As a
compromise,re-rankingmethodssuchasthosein
TheMultiDoc2Dialsettingdrawsonrelatedtasks
Fajciketal.(2021)trainare-rankingmodulethat
likeopen-domainQAandconversationalQA.Con-
canjointlyembedthequeryandretrievedpassages.
sequently, we investigate techniques that have
Because the set of retrieved passages is signifi-
shownsuccessonthosetasks. ConversationalQA
cantly smaller than the whole corpus, re-ranking
tasks, which typically assume that the grounding
methods can model more complex relationships
document is provided, use transformer-based ar-
betweenthequeryandretrievedpassages,andsig-
chitectures;theleadingsubmissionstotheQuAC
nificantlyboostretrievalperformance.
andCoQAleaderboardsuseRoBERTa(Liuetal.,
Reader: Encoder-decoderbasedabstractiveread-
2019) and DeBERTa (He et al., 2020), respec-
ers have been widely used in QA tasks. RAG
tively. Retriever-readerarchitecturessuchasRAG
(Lewisetal.,2020b)usestheBART-largemodel
(Lewisetal.,2020b)havebecomeapopularchoice
(Lewisetal.,2020a)whichispre-trainedusinga
foropen-domainQA,increasinglyusingdensere-
denoisingobjectiveandavarietyofdifferentnois-
trieval methods such as DPR (Karpukhin et al.,
ingfunctions. Moreover,RAGmarginalizesoutput
2020). We study works related to four areas
fromeach(query,passage)pairbasedonretrieval
for modeling improvement: retrieval, reranking,
scores. It has obtained state-of-the-art results on
reader,andtraining.
adiversesetofgenerationtasksandoutperforms
Retriever: AstheMultiDoc2Dialtaskisformu-
comparably-sizedT5models.
lated in an open-domain setting, it requires the
Fusion in Decoder (FiD) (Izacard and Grave,
retrievalofrelevantsources(passages)fromalarge
2021)performswellinextractive-basedQAtasks
poolofdocumentsforgeneratingtherightoutput.
likeNaturalQuestions(Kwiatkowskietal.,2019).
Hence, we investigate the strides in information
UnlikeRAGmodel,theindependentprocessingof
retrievalinrecentyears.
the passages on the encoder side allows the FiD
Recently,denseretrievalbasedapproacheshave
modeltoscaletoalargenumberofpassages,while
showncompetitiveperformance(Karpukhinetal.,
thefusioninthedecodereffectivelycombinesevi-
2020; Xiong et al., 2020; Hofsta¨tter et al., 2021)
dencefrommultiplepassages.
while also scaling to large corpora, like MS-
Training : Works such as Xu et al. (2020) have
MARCOdataset(Nguyenetal.,2016). Theyuse
shownthatfine-tuningatransformermodelonex-
anearestneighborindex,suchasFAISS(Johnson
amples,orderedonthebasisoftheirdifficulty,re-
etal.,2019)toensurescalability. Denseretrieval
sultsinsignificantperformancegainsacrossdiffer-
techniquesaimstoencodethequeryandpassage
enttasks. Kimetal.(2021)showmorespecifically
intoasharedsemanticspacewheretherelevance
thatthistypeofcurriculumdesigngeneralizeswell
of a passage for a query can be computed by the
tothedocument-groundedQAsetting.
innerproductoftheirrepresentations.
Incontrast,sparseretrievaltechniquesperform
3 TaskDescription
exacttoken-levelmatchinginthevocabularyspace.
Therehasbeenagrowinginterestinthisfield,with MultiDoc2Dial is a conversational QA task that
many advances achieving state-of-the-art results requires generating responses to user queries. In
(DaiandCallan,2020;Baietal.,2020;Gaoetal., contrast to tasks like its predecessor, Doc2Dial
2021;Formaletal.,2021;MacAvaneyetal.,2020). (Fengetal.,2020a),andrelatedtaskslikeQuAC
Thesemodelsareadvantageousduetotheirinter- (Choi et al., 2018a), ShARC (Saeidi et al.), and
pretable representations, efficient lookup, highly CoQA(Reddyetal.,2019b),whichassumethatthe
scalable inverted-list indexing, and excellent per- groundingdocumentforthedialogueisgiven,Mul-
formanceinexactterm-basedmatchingscenarios. tiDoc2Dialconstructsdialoguesthataregrounded
Like dense retrieval based approaches, matches in multiple documents. Each dialogue is con-
arecomputedviathedotproductofthequeryand structed from a number of segments. Different
passagerepresentations. segments are grounded in different documents;
Reranking: Whilebothdenseandsparseretrieval while all the dialogue turns within a segment are
methodshaveshowngoodprogress,theymuststill groundedinasingledocument. Thedatasetaddi-
embedthequeryandpassageseparately,because tionallymarksthespecificpassagethatisrelevant
computingamatchscorebetweenaqueryandev- to the current dialogue turn. However, the tran-
149
Figure1: Theproposedsystemarchitectureusesabi-encoder(DistilSPLADE)retrieverwhichfetchesthetop100
relevantpassagesfromthepassageindex,followedbyaRoBERTA-basedcross-encoderforreranking. Thetop10
passagesarepassedtoFiDwithT5tooutputthefinalresponse. Thismodelisalsousedtoperformcurriculum
learningasdiscussedinSection4.
sitions between segments, which we refer to as anddialoguecontext(previousturns)asthequery.
topic shifts, are not marked. As a result, models The query is then passed to the retriever which
are required not only to determine the grounding selectsthetop-npassageswhicharefurtherpassed
passagesforeachturnbutalsotodeterminewhich to a reranker. The top-k (out of top-n) reranked
parts of the dialogue context continue to be rele- passagesarethenfedtothereaderalongwiththe
vantacrosstopicshifts. Theoriginaldatasetalso querytofinallygeneratetheagent’sresponse.
presentsseveraldistinctdomainsofgroundingdoc- Inourexperiments,weuseDistilSPLADE(For-
umentsfromdifferentpublic-facingwebsitesthat maletal.,2021)asourretriever,whichaugments
exhibit different writing styles. Each dialogue is the query and passages, subsequently projecting
groundedindocumentsdrawnfromthesamedo- them to a sparse vector in the vocabulary space.
main. Eachcoordinateintheprojectedvectorrepresents
Thesharedtaskdefinestwosettings: onewhere the semantic importance of a term (also called
all of the dialogues are grounded in documents “termimpact”(Malliaetal.,2021))formatching.
fromdomainsseenduringtraining(MDD-SEEN), Theinputsareaugmentedbyapplyingasparsity-
and another where the grounding domain is un- inducing activation function on the logits of a
seen (MDD-UNSEEN). Due to the open domain Masked Language Model such as BERT (Devlin
evidence retrieval and natural language response et al., 2019), which selects the important words
generationsettingofthetask,itlendsitselfwellto presentinthepassageandaddsadditionalexpan-
aretriever-readerarchitecture. Broadlyspeaking, siontocombatthevocabularymismatchproblem.
theMultiDoc2Dialtaskcanbebrokendowninto Thesparsityoftheactivationiscomplementedwith
two distinct subtasks. Models must first retrieve the FLOPS regularizer (Paria et al., 2020) which
the correct grounding passage from the provided minimizes the expected floating point operations
corpora. Theymustthenusetheretrievedpassages required to perform matching. In addition to the
togeneratearesponsetotheuserqueryinthemost training data provided in MS-MARCO (Campos
recentdialogueturn. WhiletheMultiDoc2Dialpa- etal.,2016),themodelistrainedusingthepseudo-
perdefinesbothretrievalandagenerationtask,the labelsfromamoreexpressivecross-encodermodel,
shared task only evaluates reader outputs. Mod- whichimprovestheperformanceoftheSPLADE
els are evaluated on the sum of different metrics: model. Thistechniquehasshownstate-of-the-art
F1, BLEU (as implemented in Post 2018), ME- performanceacrossseveraldatasetsandobtained
TEOR (Banerjee and Lavie, 2005), and RougeL thehighestperformanceinourexperiments.
(Lin,2004). Thepassagesretrievedbythebi-encoderbased
retrievalarethenpassedthroughaRoBERTA(Liu
4 Methodology
etal.,2019)basedcross-encoder. TheRoBERTA
For this task, we employ the standard retriever- modelistrainedtooutputascorethatdenotesthe
readerarchitectureusedinopen-domainquestion relevance of a passage to the given query. Due
answering. Themodeltakestheuser’scurrentturn to the cross-attention between the query and the
150
passage, the reranking proved to be effective by 5.2 Setup
pullingupgoldenpassagesinthetop-kdocuments
Our experimental setup refines both the retriever
thatarepassedontothereader.
andreadercomponentsoftheexistingarchitecture.
Anabstractivereaderisusedtogenerateagentre-
RetrievalWeanalyzetheperformanceofdiffer-
sponses. WeuseaT5basedfusionindecoder(FiD)
ent dense and sparse retrieval methods in a zero-
model which encodes all the top-k reranked pas-
shotsettingontheMultiDoc2Dialdataset. Forour
sages one-by-one and concatenates them to form
denseretrieverbaselines,weconductexperiments
theinputtothedecoder. Thedecoderthenlearnsto
with DPR, ANCE (Xiong et al., 2020) and TAS-
collectevidencefrommultiplepassagestogenerate
B (Hofsta¨tter et al., 2021). For sparse retrieval
theresponse.
methods,weexperimentwithSPLADE-maxand
Wealsoexperimentwithtrainingourmodelus- DistilSPLDAE(Formaletal.,2021). Duringtrain-
ingacurriculumlearningapproachoriginallypro- ing,welabeltheretrievedpassages(excludingthe
posedbyXuetal.(2020)andthenimplementedon goldenpassage)fromBM25ashardnegatives. We
Doc2DialbyKimetal.(2021). Todoso,wedivide alsoexperimentwiththefinetunedDPRmodelto
ourtrainingdatarandomlyinto4buckets,andtrain minehardernegatives.
ateachermodeloneachbucketusingFiD-T5. We RerankerFollowing(Fajciketal.,2021),wese-
then calculate each teacher model’s performance lectthetop100passagesfromtheDistilSPLADE
(BLEU,RougeLandMETEORscores)ontheother retrievertobererankedusingRoBERTAasacross
3 buckets, which the teacher model has not seen encoder. We use this reranking only during vali-
during training. The training instances are then dation time. The top 10 reranked documents are
partitionedinto”easy”,”medium”,and”hard”ex- passedtothereader.
amples based on the scores chosen in Kim et al.
ReaderWeexperimentwithbothT5andBART
(2021). We train in four phases, and each phase
modelsasthereader. WeusetheT5basedreader
istraineduntilconvergence. Inthefirstphase,we
model to circumvent the limited tokens used for
trainonathirdoftheeasyexamples;inthesecond,
BART along with the FiD model pretrained on
onadisjointthirdoftheeasyexamples,andathird natural questions 2. We further experimented by
ofthemediumexamples;inthethirdphase,adis-
placing the golden passage at the top-most posi-
jointthirdofallofthethreepartitions,andinthe
tion(Goldsetting)intheretrievedpassagesbefore
finalphase,wetrainontheentiretrainingset.
passing it to the reader during training. We also
applycurriculumlearning(CL)inthereaderasper
5 Experiments
describedinSection4.
Dataset : The MultiDoc2Dial dataset consists
6 Results&Discussion
of4796dialogues, consisting29,748queryturns
andgroundedin4283passagesacross4domains Table 1 shows our model’s performance on the
(Social Security Administration, Veteran Affairs, validation split. Applying DistilSPLADE as the
Student-Aid,andDMV).MDD-UNSEENtestcor- retrieverwithFiD+T5asthereaderwesawa10
pususedinsharedtaskisbasedonCOVIDdomain. pointimprovementinBLEUcomparedtothebase-
line. Reranking(RR)theretrievaloutputsleadsto
5.1 Baseline further increase in the overall metrics. Addition-
The proposed baseline for the MultiDoc2Dial ally,curriculumlearning(CL)booststhemodel’s
sharedtaskcomprisesaretrieval-augmentedgener- performance. SettingM1showsaBLEUscorethat
ator(RAG)model(Lewisetal.,2020b). Themodel is 1 point higher than the ”DistillSplade + Fid +
uses a fine-tuned dense passage retrieval (DPR) RR”model. WeusetheM1settingforevaluation
model(Karpukhinetal.,2020)tofindrelevantpas- ontheTestSEENdataset. FortheGoldsetting,we
sagesandapretrainedsequence-to-sequenceBART sawadecreaseinmetricsfortheRRandRR+CL
(Lewis et al., 2020a) to generate the response by settings.
marginalizingitaccordingtodocumentscores.
6.1 Retrievalimprovement
Weusestructure-basedsegmentation,withthe
Wepresenttheresultsfordifferentretrieverconfig-
original and reranking original scoring functions.
urations at Recall@10 and Recall@100 in Table
WeuseDPRencoderfinetunedonMultiDoc2Dial
forretrieval,andapretrainedBART-largemodel. 2https://github.com/facebookresearch/FiD
151
Model Reader EM F1 BLEU RougeL
Baseline BART 3.6 33.8 19.2 31.4
DistilSPLADE+RAG BART 4.8 38.5 23.7 36.2
DistilSPLADE+FiD T5 5.1 42.3 29.7 40.2
DistilSPLADE+FiD+RR T5 5.5 43.1 30.1 41.1
DistilSPLADE+FiD+RR+CL(M1) T5 5.3 43.3 31.1 41.4
DistilSPLADE+FiD+Gold T5 5.3 42.4 30.5 40.6
DistilSPLADE+FiD+Gold+RR T5 5.5 42.5 30.4 40.7
DistilSPLADE+FiD+Gold+RR+CL(M2) T5 5.6 43.0 30.5 41.0
M1(onSharedTaskMDD-SEENtest) T5 - 46.2 31.8 44.2
M2(onSharedTaskMDD-UNSEENtest) T5 - 33.0 25.0 32.0
Table1:ModelperformanceonthevalidationsplitforEM,F1,BLEUandRougeL.Weseeaconsistentimprovement
acrossallmetricswithDistilSPLADEastheretrieverandFiDasthereader. Goldmeanstheground-truthpassage
waspassedduringtraining. Reranking(RR)andcurriculumlearning(CL)furtherboostperformanceonallmetrics.
Model R@10 R@100 6.2 Readerimprovement
DPR-PT 33.9 69.4
Ouranalysis,inTable1,indicatesthattheFiD(T5-
ANCE-PT 53.8 80.7
based)modeloutperformsthecurrentBART-based
TAS-B-PT 53.9 85.0
baseline modelon all the evaluation metrics. We
SPLADE-max-PT 58.5 85.9
DistilSPLADE-PT 61.6 86.9 observed an improvement of around 10 points in
BLEU score in the FiD setting compared to the
DPR-FT(Baseline) 73.2 92.8
RAGmodel. FiDextractsrelevantevidencefrom
SPLADE-max-FT 75.1 93.9
concatenatedpassagesdisregardingtheirretrieval
DistilSPLADE-FT 77.0 94.8
DistilSPLADE-FT+DPR-FT(Neg) 78.6 94.9 scores,unlikeRAGwhichusesthemformarginal-
DistilSPLADE-FT+DPR-FT(Neg) 85.7 94.9 ization. Reinforcingsignalsfromtheretrieverfor
+Reranker the reader component might be the cause of the
dipinperformanceofRAGcomparedtoFiD.We
Table2:Performanceoftheretrieverfordifferentmodel
alsoobservedthatincreasingthenumberofinput
configurations at Recall@10 and Recall@100. X-PT
tokenstothereadermodelhelpscapturedialogue
refers to the pretrained X model while X-FT implies
andpassagecontextrelevanttotheinputquery.
thatXwasfinetunedonMultiDoc2dial. DPR-FTwas
theretrieveremployedfortheMultiDoc2Dialbaseline.
7 Conclusion
2. Itisevidentthatthepretrainedsparseretrieval
frameworks, Splade and DistilSPLADE, achieve We introduced our submission (CMU-QA) for the
betterretrievalperformanceincomparisontothe Multidoc2Dial shared task. Our approach (R3)
pretrainedDPRmodel. Thissuggeststhattheexact focuses on improving the overall retriever-reader
matchingoverkeywordsandovertheparaphrases pipelineusingthesparseretrieverDistilSPLADE
generated for functional words achieves good re- andFusion-in-decoder(FID)asthereader. Weuse
trieval performance. Unsurprisingly, the perfor- across-attentionbasedrerankertofurtherboostre-
manceforallmodelsimprovesignificantlywhen callscores. Werefinethetrainingprocessthrough
theyarefine-tunedonMultidoc2Dialdataset,with curriculumlearningtohandlethediversecomplex-
thesparse-retrieversstilloutperformingDPR.The ity of this dataset. For future work, we plan to
performance shows a further boost when we use improveresultsthroughbetterdialoguemodelling
thefine-tunedDPRmodeltominehard-negatives. andreducingnoiseorirrelevantinformationinthe
Rerankingthevalidationpassagesincreasesthe passagesbytakingtoptextspans. Further,wewill
R@10to85%(RefTable2). Thisfurtherleadsto aim to select the best of all candidate responses
improvements in metrics in both the normal and usingaresponsere-ranker.
theGoldsetting.
152
References pages8118–8128,Online.AssociationforComputa-
tionalLinguistics.
YangBai,XiaoguangLi,GangWang,ChaoliangZhang,
Lifeng Shang, Jun Xu, Zhaowei Wang, Fangshan Song Feng, Hui Wan, Chulaka Gunasekara,
Wang, and Qun Liu. 2020. Sparterm: Learning Siva Sankalp Patel, Sachindra Joshi, and Luis A
term-basedsparserepresentationforfasttextretrieval. Lastras. 2020b. doc2dial: A goal-oriented
ArXiv,abs/2010.00768. document-groundeddialoguedataset. arXivpreprint
arXiv:2011.06623.
SatanjeevBanerjeeandAlonLavie.2005. Meteor: An
automaticmetricformtevaluationwithimprovedcor- Thibault Formal, Carlos Lassance, Benjamin Pi-
relationwithhumanjudgments. InProceedingsof wowarski, and Ste´phane Clinchant. 2021. Splade
theaclworkshoponintrinsicandextrinsicevaluation v2: Sparselexicalandexpansionmodelforinforma-
measuresformachinetranslationand/orsummariza- tionretrieval. arXivpreprintarXiv:2109.10086.
tion,pages65–72.
LuyuGao,ZhuyunDai,andJamieCallan.2021. Coil:
DanielFernandoCampos,TriNguyen,MirRosenberg, Revisitexactlexicalmatchininformationretrieval
Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan withcontextualizedinvertedlist. InNAACL.
Majumder,LiDeng,andBhaskarMitra.2016. Ms
marco: Ahumangeneratedmachinereadingcompre- Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
hensiondataset. ArXiv,abs/1611.09268. WeizhuChen.2020. Deberta: Decoding-enhanced
bert with disentangled attention. arXiv preprint
EunsolChoi,HeHe,MohitIyyer,MarkYatskar,Wen arXiv:2006.03654.
tauYih, YejinChoi, PercyLiang, andLukeZettle-
moyer.2018a. Quac: Questionansweringincontext. Sebastian Hofsta¨tter, Sheng-Chieh Lin, Jheng-Hong
InEMNLP. Yang, Jimmy Lin, and Allan Hanbury. 2021. Ef-
ficiently teaching an effective dense retriever with
EunsolChoi,HeHe,MohitIyyer,MarkYatskar,Wen- balanced topic aware sampling. In Proceedings of
tauYih, YejinChoi, PercyLiang, andLukeZettle- the 44th International ACM SIGIR Conference on
moyer.2018b. Quac: Questionansweringincontext. ResearchandDevelopmentinInformationRetrieval,
In Proceedings of the 2018 Conference on Empiri- pages113–122.
calMethodsinNaturalLanguageProcessing,pages
2174–2184. GautierIzacardandEdouardGrave.2021. Leveraging
passage retrieval with generative models for open
Zhuyun Dai and Jamie Callan. 2020. Context-Aware domain question answering. In EACL 2021-16th
DocumentTermWeightingforAd-HocSearch,page ConferenceoftheEuropeanChapteroftheAssocia-
1897–1907.AssociationforComputingMachinery, tionforComputationalLinguistics,pages874–880.
NewYork,NY,USA. AssociationforComputationalLinguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and JeffJohnson,MatthijsDouze,andHerve´ Je´gou.2019.
KristinaToutanova.2019. Bert: Pre-trainingofdeep Billion-scale similarity search with GPUs. IEEE
bidirectionaltransformersforlanguageunderstand- TransactionsonBigData,7(3):535–547.
ing. InProceedingsofthe2019Conferenceofthe
NorthAmericanChapteroftheAssociationforCom- VladimirKarpukhin,BarlasOguz,SewonMin,Patrick
putationalLinguistics: HumanLanguageTechnolo- Lewis,LedellWu,SergeyEdunov,DanqiChen,and
gies,Volume1(LongandShortPapers),pages4171– Wen-tauYih.2020. Densepassageretrievalforopen-
4186. domainquestionanswering. InProceedingsofthe
2020ConferenceonEmpiricalMethodsinNatural
MartinFajcik,MartinDocekal,KarelOndrej,andPavel LanguageProcessing(EMNLP),pages6769–6781.
Smrz. 2021. R2-d2: A modular baseline for open-
domainquestionanswering. InFindingsoftheAsso- Boeun Kim, Dohaeng Lee, Sihyung Kim, Yejin Lee,
ciationforComputationalLinguistics:EMNLP2021, Jin-XiaHuang,Oh-WoogKwon,andHarksooKim.
pages854–870. 2021. Document-groundedgoal-oriented dialogue
systemsonpre-trainedlanguagemodelwithdiverse
SongFeng,SivaSankalpPatel,HuiWan,andSachindra inputrepresentation. InProceedingsofthe1stWork-
Joshi. 2021. MultiDoc2Dial: Modeling dialogues shoponDocument-groundedDialogueandConver-
groundedinmultipledocuments. InProceedingsof sationalQuestionAnswering(DialDoc2021),pages
the2021ConferenceonEmpiricalMethodsinNatu- 98–102,Online.AssociationforComputationalLin-
ralLanguageProcessing,pages6162–6176,Online guistics.
andPuntaCana,DominicanRepublic.Association
forComputationalLinguistics. TomKwiatkowski, JennimariaPalomaki, OliviaRed-
field,MichaelCollins,AnkurParikh,ChrisAlberti,
SongFeng,HuiWan,ChulakaGunasekara,SivaPatel, DanielleEpstein,IlliaPolosukhin,MatthewKelcey,
SachindraJoshi,andLuisLastras.2020a. doc2dial: Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Agoal-orienteddocument-groundeddialoguedataset. LlionJones,Ming-WeiChang,AndrewDai,Jakob
InProceedingsofthe2020ConferenceonEmpirical Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
MethodsinNaturalLanguageProcessing(EMNLP), ralquestions: abenchmarkforquestionanswering
153
research. TransactionsoftheAssociationofCompu- SivaReddy,DanqiChen,andChristopherDManning.
tationalLinguistics. 2019a. Coqa: Aconversationalquestionanswering
challenge. TransactionsoftheAssociationforCom-
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan putationalLinguistics,7:249–266.
Ghazvininejad,AbdelrahmanMohamed,OmerLevy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020a. SivaReddy,DanqiChen,andChristopherD.Manning.
Bart: Denoisingsequence-to-sequencepre-training 2019b. CoQA: A conversational question answer-
fornaturallanguagegeneration,translation,andcom- ing challenge. Transactions of the Association for
prehension. InProceedingsofthe58thAnnualMeet- ComputationalLinguistics,7:249–266.
ingoftheAssociationforComputationalLinguistics,
pages7871–7880. Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer
Singh,TimRockta¨schel,MikeSheldon,Guillaume
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio Bouchard, and Sebastian Riedel. Interpretation of
Petroni,VladimirKarpukhin,NamanGoyal,Hein- natural language rules in conversational machine
rich Ku¨ttler, Mike Lewis, Wen-tau Yih, Tim reading.
Rockta¨schel,etal.2020b. Retrieval-augmentedgen-
LeeXiong,ChenyanXiong,YeLi,Kwok-FungTang,
erationforknowledge-intensivenlptasks. Advances
Jialin Liu, Paul N. Bennett, Junaid Ahmed, and
inNeuralInformationProcessingSystems,33:9459–
ArnoldOverwijk.2020. Approximatenearestneigh-
9474.
bor negative contrastive learning for dense text re-
Chin-YewLin.2004. Lookingforafewgoodmetrics: trieval. CoRR,abs/2007.00808.
Rougeanditsevaluation.
Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan
Wang, Hongtao Xie, and Yongdong Zhang. 2020.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
Curriculumlearningfornaturallanguageunderstand-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
ing. InProceedingsofthe58thAnnualMeetingof
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
theAssociationforComputationalLinguistics,pages
Roberta: A robustly optimized bert pretraining ap-
proach. arXivpreprintarXiv:1907.11692. 6095–6104.
Sean MacAvaney, Franco Maria Nardini, Raffaele
Perego,NicolaTonellotto,NazliGoharian,andOphir
Frieder. 2020. Expansion via prediction of impor-
tancewithcontextualization. InProceedingsofthe
43rd International ACM SIGIR conference on re-
search and development in Information Retrieval,
pages1573–1576.
Antonio Mallia, O. Khattab, Nicola Tonellotto, and
TorstenSuel.2021. Learningpassageimpactsforin-
vertedindexes. Proceedingsofthe44thInternational
ACMSIGIRConferenceonResearchandDevelop-
mentinInformationRetrieval.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng
Gao, Saurabh Tiwary, Rangan Majumder, and
Li Deng. 2016. MS MARCO: A human gener-
atedmachinereadingcomprehensiondataset. CoRR,
abs/1611.09268.
BiswajitParia,Chih-KuanYeh,IanEn-HsuYen,Ning
Xu,PradeepRavikumar,andBarnaba´sPo´czos.2020.
Minimizingflopstolearnefficientsparserepresenta-
tions. CoRR,abs/2004.05665.
MattPost.2018. AcallforclarityinreportingBLEU
scores. InProceedingsoftheThirdConferenceon
MachineTranslation: ResearchPapers,pages186–
191, Belgium, Brussels. Association for Computa-
tionalLinguistics.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tionsforsquad. InProceedingsofthe56thAnnual
Meeting of the Association for Computational Lin-
guistics(Volume2: ShortPapers),pages784–789.
154

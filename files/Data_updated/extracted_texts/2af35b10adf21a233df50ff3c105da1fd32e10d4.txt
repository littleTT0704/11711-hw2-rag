The phonetic bases of vocal expressed emotion: natural versus acted
HiraDhamyal,ShahanA.Memon,BhikshaRaj,RitaSingh
CarnegieMellonUniversity
hyd@cs.cmu.edu, samemon@cs.cmu.edu, bhiksha@cs.cmu.edu, rsingh@cs.cmu.edu
Abstract allcases, thedecoderisanobserver, who’sonlycueinterms
ofvocalemotionsisthevocalstimuli. Basedonthese,wepro-
Canvocalemotionsbeemulated? Thisquestionhasbeenare- pose the non-actor, actor, and observer (NAO) model (Figure
current concern of the speech community, and has also been 1),whichrepresentsallthreeentitiesandtherelationbetween
vigorously investigated. It has been fueled further by its link them. Theactoraimstoencodesyntheticemotioninamanner
to the issue of validity of acted emotion databases. Much of thattheobservercannotdistinguishfromthegenuineemotion
thespeechandvocalemotionresearchhasreliedonactedemo- encoded by the non-actor. This enables us to formulate a hy-
tion databases as valid proxies for studying natural emotions. pothesis that can be formally tested – that there nevertheless
Tocreatemodelsthatgeneralizetonaturalsettings,itiscrucial remainidentifiablefundamentaldifferencesintheencodedsig-
to work with valid prototypes – ones that can be assumed to nalsinthetwocases. Ifthetestfails,thatwouldmeannatural
reliablyrepresentnaturalemotions. Moreconcretely,itisim- emotionscanbeemulated,andthatactedemotionscanbeused
portanttostudyemulatedemotionsagainstnaturalemotionsin as proxies for natural emotions. If the test passes, however,
terms of their physiological, and psychological concomitants. thatwouldsignaltowardsdichotomybetweenactedandnatu-
In this paper, we present an on-scale systematic study of the ralemotions,leadingtoalowvalidityandvalueinusingacted
differencesbetweennaturalandactedvocalemotions. Weuse stimuli.
a self-attention based emotion classification model to under-
stand the phonetic bases of emotions by discovering the most
‘attended’phonemesforeachclassofemotions.Wethencom-
paretheseattended-phonemesintheirimportanceanddistribu-
tionacrossactedandnaturalclasses.Ourtestsshowsignificant
differencesinthemannerandchoiceofphonemesinactedand
naturalspeech,concludingmoderatetolowvalidityandvalue
inusingactedspeechdatabasesforemotionclassificationtasks.
IndexTerms:emotion,phonemes,neuralnetwork,attention
1. Introduction
Canvocalemotionsbeemulated?Thisquestionhasledtolong
standing debates in the speech community regarding natural
Figure1: Thenon-actor,actorandobserver(NAO)framework
versusactedemotions,inthecontextofemotionclassification
forthecommunicationofemotions.
andemotioncategorizationtasks.Toconductanyspeechbased
emotionresearch,animportantfactoristhenatureofthespeech
samples or the vocal stimuli, and whether those samples are Wenotethatnon-actorandtheactordifferintheirencod-
representativeofnaturalemotions. Naturalemotionscanbest ing of emotions. Because natural emotions are, for the most
be defined as emotions that are spontaneous and involuntary. part,involuntary,theyincludephysiologicalandpsychological
Acted emotions, on the other hand, are prompted and volun- responses as concomitants, such as heart rate, breathing rate,
tary. Becauseactedemotionsarevolitional, researchersargue muscletension,andmood. Thesephysiologicalchangesmani-
thatthephysiologicalandpsychologicalresponsesthatnatural festinthevoicebychangingthespectro-temporalstructureof
emotions induce are absent from acted emotions [1, 2]. Nev- individual sounds [5, 6]. For example, [7, 8] argue that vow-
ertheless,researchonemotionperceptionusesactedemotions els and consonants produced in fear are often more precisely
as convenient proxies for natural emotions. While many past articulatedthantheyareinneutralsituations. Thephysiologi-
studieshavefocusedonpresentingperceptiontestsfornatural calchangesalongwiththepsychologicalfactorsalsodefinethe
versusactedemotionswithmixedconclusions[2,3,4],thereis choiceofphonemes(i.e. thelexicalcontent)andprosodiccues
alackofanat-scalesystematicframeworktostudythediffer- [9, 10]. As an example, words of aggressive nature are more
encesandsimilaritiesinthoseclasses. likelytobeusedbyanindividualinanaggressivemood[11].
Inordertodevelopsuchaframework, wemustrecognize Theencodingofemotionishenceanaggregate,orperceptual
thatthereare,infactthreeentitiestobeconsidered. Thecom- sumoftheseacoustic,phoneticandlinguisticinfluences,orfac-
municationofvocalemotionsis,atitsessence,acombinationof tors. Theobserverdecodestheperceptualsumofthesefactors
anencodingandadecodingprocess.Thesubjectexpressingthe tomaketheirinferenceabouttheemotionalstateofthespeaker.
emotionencodestheiremotionalstateintothelow-dimensional In emulating an emotion, the actor attempts to produce a
speechsignal. Thesubjectperceivingthesignaldecodesitto somewhat similar aggregate of these factors as the non-actor,
makeinferencesaboutthestateofthespeaker. Wewilldistin- i.e. a combination of factors that he expects the observer to
guishbetweentwotypesofencoders: thenon-actorwhoactu- decode into a near identical perceptual sum. If the actor suc-
ally experiences the emotion, and the actor who may not. In ceeds, he conveys the target emotion to the observer. We hy-
0202
luJ
52
]SA.ssee[
3v33750.1191:viXra
pothesizethatindoingso,theindividualfactorsthattheactor criminator between acted and natural emotion, hypothesizing
produceswill, however, stillbeincorrectorevenimplausible, insteadthatthefactorsthatcomprisenaturalandactedemotion
eventhoughtheperceptualsummaybeplausible. differsignificantlyirrespectiveoftheobserver’sresponse. The
The distinction is, perhaps, best illustrated by the study, followingstudiessupportsourhypothesisalthoughnotwithina
“ThePresident’sSpeech”,narratedbywell-knownneurologist systematicframeworktoanalyzethedifference.[14]concludes
OliverSacks[12],inwhichtwotypesofpatients,aphasic,and that acted and natural speech innately differs based on voice
tonal agnostic, both find a presidential candidate’s posturing quality. Actedspeechisconsideredtobedeliveredinamore
on TV to be screamingly implausible, although normal view- emotionally intense fashion but also that acted speech affects
ers have no problem with it. Aphasic patients are highly sen- the vocal expression in a more general way, without the nu-
sitive to expression and tone, but cannot interpret the words. ancesofthechangescausedbythenaturalemotion[15].Some
On the other hand, tonal agnostic patients lack any sense of studies have focused on only particular aspects of vocal emo-
expression and tone, and pay attention to exactness of words tionlike[16]whichconcludesthatthetwoaredifferentbased
andwordusetocapturetheemotion. Theskilledactor,inthis ontheprosodicpropertiesofthespeech.
casethepresidentialcandidate,attemptstoconveyfeelingsand
emotions through a combination of affect that is given a pass 3. NeuralModel
bynormalviewers. However, bothtypesofpatientswho, un-
like normal people, only perceive some of these factors, find Inordertoextractthephoneticbasesofvocalemotion,wepro-
themsufficientlyimplausibleastocausethemdistress. Simply poseaneuralnetworkmodel.Wedesignthemodeltotakeinto
by their inability to consider the totality of affect that normal consideration both the lexical and acoustic aspects of the ut-
peoplecanperceive,thepatientscannotbeliedtoordeceived. terance and also the relationship between the two, to capture
Vocal(orindeedany)expressionofemotionis,ofcourse, thephoneticbasesofemotion.Thelinguisticsshouldguidethe
acomplexphenomenon,andthecompletesetofacoustic,pho- model about the important parts of the acoustic. To create a
netic,linguisticandprosodicfactorsusedinexpressingitisstill vectorrepresentationofthelinguisticpartoftheinput,wepass
notfullyunderstood.Totestourhypothesis,wemustneverthe- itthroughanLSTMwhichcapturesthecontextualinformation
lessidentifyoneormoreofthesefactorsthatcanbestatistically ofthelinguistics.Thisformsacontext-sensitivelexicalvector.
quantified. Asmentionedearlier,physiologicalandpsycholog- Tocapturetherelationshipbetweenthetwomodalities,we
icalchangesconcomitantwithemotionareknowntoaffectthe utilizeanattention-basedmechanism.Thisenablesthecontext-
choiceofphonemesandtheirmannerofdelivery.Wewillrefer sensitivelexicalvectortoputattentiononsomepartsoftheau-
totheseasthephoneticbasesofvocalemotions.Byourhypoth- dio, forming importance weights. The weights, when applied
esis,therewillbeastatisticallymeasurabledifferenceinthese backtotheinputaudio,maketheoutputhighinpartsthatthe
betweentheactorandnon-actor. lexicalvectorpointstoandothersbecomelowinvalue. Afea-
Toverifyourhypothesis,werequireamechanismtoquan- ture vector is created from this weighted output, which there-
tiablyextractthesebasesfromthespeechsignal. Todoso,we aftergoesintotheclassificationlayerforemotion.Trainingthe
train a neural network model for emotion classification tasks model maximizes the classification accuracy, but in doing so,
on two datasets, one of natural speech and the other of acted it teaches the model to create feature vectors which would be
emotionalspeech,usinganattentionmechanism.Theattention differentiablefortheemotionclasses. This,inturn,optimizes
aimstoidentifythemostimportantphonemesinanutterancein theattentionmechanism,therebyallowingthelexicalvectorto
ordertoclassifyitsemotion.Wecomparethestatisticalpatterns focusonlyonthosepartsoftheaudiowhichwouldleadtothe
ofthemostattentedphonemesacrossactorandnon-actor. As highestclassificationaccuracy.Thislaysthebasisofthemodel
wewillseeinthefinalsectionsofourpaper, thesefactorsdo wehaveused,asshowninFigure2.
indeeddifferinastatisticallysignificantmanner, bringingthe
validityofconclusionsdrawnfromactedemotionalspeechasa
proxyfornaturalemotionintoquestion.
.
2. BackgroundWork
Therehasbeensomeworkdonetounderstandthedifferences
betweenactedandnaturalemotionalspeech. Theresultsfound
from these studies are somewhat contradictory to each other.
Studies like [13], which analyze acted and natural emotional
speechwiththehelpofhumanlistenershaveconcludedthatthe
listenersarenotabletodistinguishbetweenthetwocategories.
Theproblemisalsostudiedinthedomainoffalseexpression,
wherethetruthfulnessoftheexpressedemotionisstudied. It
alsoreachestheconclusionthathumansarelesslikelytodif-
ferentiatebetweenthetwo. Ontheotherhand,studieslike[4],
whichalsousehumanlisteners,concludethatabout78%oflis-
tenerswereabletodifferentiatebetweenthenaturalandacted
emotionwithonlyaudiocluesandevenmorecoulddifferenti-
atewhenprovidedwithaudio-visualcues.
The above studies primarily analyze the effect of acted
Figure2: Neuralnetworkmodelusedfortheemotionclassifi-
emotiononthelistener.Inourworkwedonotconsiderthelis-
cationwithattentionmechanism
tener(observerintheproposedNAOmodel)tobeavaliddis-
Sinceweneedboththeacousticandlexicalcontentofan 4.1.4. Characterizingcontent-dependentbias
utterance, to train the model, we require the transcription of
Itisalsopossibleforouranalysestobeinfluencedbythedif-
therecording. Togetthetranscription,therecordingispassed
ferencesinthecontentofthetwodatasets. Toensurethatthe
throughASR.WeusedGoogleAPI[17]toextractthetranscrip-
differenceincontentisnotaconfoundingfactor,westudythe
tion. EachwordinthetranscriptisrepresentedasaBERT[18]
phonemedistributionsofthetwodatasets.Figure3presentsthe
contextualizedwordembedding. Theseembeddingsarepassed
phonemedistributionsofbothdatasets.Todetermineifthedif-
throughanLSTMlayer(showninblue).Fortheattentionlayer
ferencebetweenthedistributionsisstatisticallysignificant,we
werepresentthekeysastheoutputoftheconvolutionallayer;a
runaWilcoxonranktest[25].TheWilcoxonranktestisanon-
3-dimensionaloutput. Thequeryisthelasthiddenstateofthe
parametric test, used to compare two related samples. In this
LSTMpassedthroughalinearprojection.
case,thenullhypothesisH isthatthereisnodifferenceinthe
The network is optimized using the Cross-Entropy loss, 0
distributions of the phonemes under the two datasets, and the
withweightsforindividuallabelsduetotheclassimbalancein
alternativehypothesisH isthatthereisadifferencebetween
thedatasets. TheASRbasedtranscriptoftheutteranceisseg- 1
thedistributionsofthetwodatasets. Weobtainap-valueof.3,
mentedintodifferentphonemesusinganHMMbasedphoneme
thereforewithα = .05,wefailtorejecttheH . Thisensures
segmentor [19]. Once the model is trained, the attended- 0
thatthedifferenceinthephonemiccontentofthetwodatasets
phonemeoutputsareinspected.
isunlikelytoaffectthedistributionoftheattended-phonemes.
4. Experiment
4.1. Dataset
We use two types of datasets; acted and natural. We run our
experimentsforonlyfouremotions:angry,happy,sadandneu-
tral.
4.1.1. ActedData
The acted dataset used in the experiment is IEMOCAP [20].
Itconsistsoftensessions,eachofwhichisaconversationbe-
tween two actors. The conversations are divided into labeled
Figure3: Totalphonemedistributionsunderbothnaturaland
sentences. We implement a 10-fold cross validation training
acteddataset
setup. In each fold, data from 9 speakers is used for training
themodelanddatafrom1speakerisusedfortesting.Thedata
consistsof1103angry,1636happy,1708neutraland1084sad
utterances. Theaveragedurationofutterancesinthisdatasetis 5. AnalysisandResults
4seconds.
We base our results on the output of the attention mechanism
4.1.2. NaturalData from the neural model described earlier. Since the model is
trainedforemotionclassificationoverfouremotions,itisuse-
For natural-speech, we used the CMU-SER data [21]. This
fultonotetheclassificationaccuraciesachievedbythemodel
datasethasbeencollectedfromNPRpodcasts[22],andtelevi-
overthetwodatasets. Ontheacteddata, weachieveaclassi-
sionprogramshostedbytheInternetArchive[23]. Thedataset
ficationaccuracyof72%andonthenaturaldata,weachievea
is annotated using the Amazon Mechanical Turk [24]. It has
classificationaccuracyof52.4%.
6000 utterances in the training set and 2571 utterances in the
To perform analysis on the attended-phonemes, for each
test set, with a total of 1099 angry, 3028 happy, 1262 neu-
emotionweaggregatethephonemeswiththehighestattention
tral,and611sadutterances.Theaveragedurationofutterances
output.Wenormalizetheirfrequenciesbythetotalfrequencyof
in this dataset is 5 seconds. Further details of the CMU-SER
thephonemeinthedata.Figures4and5showthedistributions
datasetcanbefoundin[21].
of these attended-phonemes for acted and natural conditions
(from the corresponding datasets) respectively, for each emo-
4.1.3. Alleviatingspeaker-dependentbias
tion.Wenoteseveraldifferencesbetweenthetwodistributions.
Because we compare acted versus natural emotions based on Thefrequencyoffricativesandstopsishigherinnaturalspeech
the two datasets with difference in speakers, it is possible for than in acted speech. We also observe that the frequency of
our phonemic content and, hence, phoneme distributions and vowelsishigherinactedspeechthaninnaturalspeech.Specifi-
theattendedphonemestobeinfluencedbythewordchoicesof cally,thephonemes/AA/,/B/and/IH/occurmorefrequentlyin
differentspeakers. Toensurethatouranalysesonlyreflectthe actedspeech. Moreover,anoverallhigherpercentageofnasal
differencesintheemotionalcontentratherthanthedifferences phonemesoccursinnaturalspeech.
inspeakers, weeliminatespeakerdependenciesatthetimeof Wealsostudytheattended-phonemedistributionunderdif-
training our model. Because the natural dataset is collected ferenttestsubsetscreatedforthe10-foldcrossvalidationproce-
fromadiversesetofonlinesources,itisreasonabletoassume duretoensureconsistencyoftheattended-phonemedistribution
that there are fewer cases of a speaker represented more than withinthedataset.Variationofthephonemefrequencyfromthe
onceinthedata.Ontheotherhand,theacteddatasetconsistsof 10differentcrossvalidationresultsareshownintheboxplots
10speakersonly. Hence,weperformleave-one-outcrossval- infigure6forbothdatasets. Itcanbeobservedthattheresults
idationtoalleviatespeakerdependenciesintheresults. These havelowervariationinnaturalspeechthanintheactedspeech.
stepsensurethatourmodelsandanalysesarerobusttothedif- Ingeneral, thesamevariationtrendsholdforotherphonemes
ferenceofspeakers. aswell.
Figure 4: Distribution of attended-phonemes (and phonetic Figure 5: Distribution of attended-phonemes (and phonetic
groupings)acrossfouremotionclassesonacteddata groupings)acrossfouremotionclassesonnaturaldata
Theboxplotsalsoillustratethedifferenceinthefrequen-
naltowardsadichotomybetweennaturalandactedemotions.
ciesofeachphonemeinthetwodatasets. Inparticular,weob-
Thisstudyhasapplicationsinspeechemotionrecognition,emo-
serve the frequencies of the vowels like /IY/, nasal phonemes
tionalspeechsynthesis,andhumancomputerinteraction. The
/M/,/N/,stopphonemes/T/,andfricativeslike/DH/(figure6)
approachtakeninthispaper,i.e,exploitingthedynamicsofthe
tobedifferentamongthetwodatasets. Tocalculatethesignifi-
neural model, allow us to not only use it for marking distinc-
canceofthesedifferencesforallemotions, werunastandard
tionsbetweenactedversusnaturalspeech, butalsotoapplyit
t-test. We find a statistically significant result for all four
to other problems, such as of exploring the phonetic bases of
emotions(p<.05).
voicedisorders,e.gvocalpalsy.
Therefore, in the context of natural versus acted classes,
ouranalysisconcludesthattherearesignificantdifferencesbe-
tween the phonetic bases of the two classes. Consequently,
weconcludemoderatetolowvalidityandvalueinusingacted
emotions as proxies for natural emotions, suggesting that re-
searchersshouldbewaryofarrivingatconclusionsaboutnatu-
ralemotionsusingactedemotiondatasets.
We would like to note that this study has only inspected
Englishlanguagedata. However,theframeworkprovidedcan
easilybeappliedtoanyotherlanguage. Weleavetheinvesti-
gationofthephoneticcorrelatesofemotioninotherlanguages,
anditscomparisonwiththeconclusionsprovidedinthisstudy,
asapossiblefuturework.
Onelimitationofthisstudyisthelackofthesamesetofob-
serversacrosstheactedandnaturaldatasets.Whilewehaveno
controloverthiswithinouranalysis,giventhediversityofob-
serversforthetwodatasets,weexpectlittlestatisticalobserver
bias.However,thisremainstobeverifiedbyfuturestudies.
6. Conclusion
Inthispaper,wepresentastudyofthedifferencesobservedbe-
tweennaturalandactedemotionwithrespecttotheirphonetic
bases. Phonetic bases of emotion comprise ‘what’ phonemes
areusedandthe‘manner’theyaredeliveredintoexpressthe
emotion. To run a quantifiable test, we model the task as an
attention-based emotion classification problem. The attention
mechanismaimstocapturethe“attended”phonemesinorderto
getthecorrectclassification. Wethencalculatethedistribution
of these attended-phonemes, and examine how their distribu- Figure 6: Box plot of attented phoneme /T/, /AY/, /DH/, /EY/
tionvariesbetweennaturalversusactedemotions. Weobserve in natural versus acted dataset. The figure highlights the fre-
severaldifferences, forexample, ahigheroccurrenceoffrica- quency difference among the two datasets. Note the median
tivesandstopsinnaturalspeechthaninactedspeech. Weob- valuesfortheboxplotsareverydifferentforbothdatasetsfor
tainstatisticallysignificantdifferenceintheattended-phoneme agivenemotion.
distribution amongnatural and actedemotion. Therefore, our
hypothesisstandstrue. Thedifferencesinphoneticbasessig-
7. References
[21] S. A. Memon, H. Dhamyal, O. Wright, D. Justice, V. Palat,
W. Boler, B. Raj, and R. Singh, “Detecting gender differences
[1] R.Ju¨rgens,A.Grass,M.Drolet,andJ.Fischer,“Effectofacting
inperceptionofemotionincrowdsourceddata,” arXivpreprint
experienceonemotionexpressionandrecognitioninvoice:Non-
arXiv:1910.11386,2019.
actorsprovidebetterstimulithanexpected,”Journalofnonverbal
behavior,vol.39,no.3,pp.195–214,2015. [22] “PodcastDirectory,”https://www.npr.org/podcasts/.
[2] J.Wilting,E.Krahmer,andM.Swerts,“Realvs.actedemotional [23] “Topcollectionsatthearchive,”https://archive.org/.
speech,”inNinthInternationalConferenceonSpokenLanguage [24] A.M.Turk,“Amazonmechanicalturk,”RetrievedAugust,vol.17,
Processing,2006. p.2012,2012.
[3] K.R.Scherer,“Vocalmarkersofemotion: Comparinginduction [25] F. Wilcoxon, “Individual comparisons by ranking methods,” in
andactingelicitation,” ComputerSpeech&Language, vol.27, Breakthroughsinstatistics. Springer,1992,pp.196–202.
no.1,pp.40–58,2013.
[4] N.Audibert,V.Auberge´,andA.Rilliard,“Howwearenotequally
competentfordiscriminatingactedfromspontaneousexpressive
speech,”inProceedingsofspeechprosody. Citeseer,2008,pp.
693–696.
[5] T.JohnstoneandK.R.Scherer,“Theeffectsofemotionsonvoice
quality,” in Proceedings of the XIVth international congress of
phoneticsciences. Citeseer,1999,pp.2029–2032.
[6] K.R.SchererandJ.S.Oshinsky,“Cueutilizationinemotionat-
tributionfromauditorystimuli,”Motivationandemotion,vol.1,
no.4,pp.331–346,1977.
[7] C.E.WilliamsandK.N.Stevens,“Emotionsandspeech: Some
acoustical correlates,” The Journal of the Acoustical Society of
America,vol.52,no.4B,pp.1238–1250,1972.
[8] B.Myers-Schulz, M.Pujara, R.C.Wolf, andM.Koenigs, “In-
herentemotionalqualityofhumanspeechsounds,”Cognition&
emotion,vol.27,no.6,pp.1105–1113,2013.
[9] K.R.Scherer,T.Johnstone,andG.Klasmeyer,“Vocalexpression
ofemotion,”Handbookofaffectivesciences,pp.433–456,2003.
[10] S.Patel,K.R.Scherer,E.Bjo¨rkner,andJ.Sundberg,“Mapping
emotionsintoacousticspace:Theroleofvoiceproduction,”Bio-
logicalpsychology,vol.87,no.1,pp.93–98,2011.
[11] C. Whissell, “Phonosymbolism and the emotional nature of
sounds: evidenceofthepreferentialuseofparticularphonemes
intextsofdifferingemotionaltone,”PerceptualandMotorSkills,
vol.89,no.1,pp.19–48,1999.
[12] O.Sacks, “Thepresident’sspeech,” Language, Communication
andEducation,p.23,1985.
[13] E.ScheinerandJ.Fischer,“Emotionexpression: Theevolution-
aryheritageinthehumanvoice,”inInterdisciplinaryanthropol-
ogy. Springer,2011,pp.105–129.
[14] K.P.Truong, “Howdoesrealaffectaffectaffectrecognitionin
speech?”2009.
[15] R.Ju¨rgens, K.Hammerschmidt, andJ.Fischer, “Authenticand
play-actedvocalemotionexpressionsrevealacousticdifferences,”
Frontiersinpsychology,vol.2,p.180,2011.
[16] N.Audibert,V.Auberge´,andA.Rilliard,“Prosodiccorrelatesof
actedvs.spontaneousdiscriminationofexpressivespeech:apilot
study,” inSpeechProsody2010-FifthInternationalConference,
2010.
[17] “Google Speech To Test,” https://cloud.google.com/
speech-to-text/.
[18] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“Bert: Pre-
training of deep bidirectional transformers for language under-
standing,”arXivpreprintarXiv:1810.04805,2018.
[19] P. Lamere, P. Kwok, E. Gouvea, B. Raj, R. Singh, W. Walker,
M.Warmuth, andP.Wolf, “Thecmusphinx-4speechrecogni-
tionsystem,”inIEEEIntl.Conf.onAcoustics,SpeechandSignal
Processing(ICASSP2003),HongKong,vol.1,2003,pp.2–5.
[20] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,
S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:
Interactiveemotionaldyadicmotioncapturedatabase,”Language
resourcesandevaluation,vol.42,no.4,p.335,2008.

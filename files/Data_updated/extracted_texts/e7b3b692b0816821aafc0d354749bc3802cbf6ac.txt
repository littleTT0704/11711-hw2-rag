PublishedasaconferencepaperatICLR2023
COMPUTATIONAL LANGUAGE ACQUISITION WITH
THEORY OF MIND
AndyLiu HaoZhu,EmmyLiu,YonatanBisk,GrahamNeubig
HarveyMuddCollege LanguageTechnologiesInstitute
Claremont,CA,USA CarnegieMellonUniversity
{ajliu}@g.hmc.edu Pittsburgh,PA,USA
{zhuhao, mengyan3, ybisk, gneubig}@cs.}cmu.edu
ABSTRACT
Unlike current state-of-the-art language models, young children actively acquire
languagethroughinteractionswiththeirsurroundingenvironmentandcaretakers.
Onemechanismthathasbeenarguedtobecriticaltolanguagelearningistheabil-
itytoinferthementalstatesofotheragentsinsocialenvironments,coinedTheory
of Mind (ToM) by Premack & Woodruff (1978). Drawing inspiration from the
modernoperationalizedversionsofToMimplementedinRabinowitzetal.(2018)
andZhuetal.(2021),webuildlanguage-learningagentsequippedwithToM,and
measureitseffectsonthelearningprocess. WemodelToMbygivingthespeaker
agent an internal listener model that is trained alongside the speaker and used
to rerank potential utterances. We experiment with varying task difficulty, hy-
pothesizingthatmodelswillacquiremorecomplexlanguagetoadapttostronger
environmental pressures. We find that training speakers with a highly weighted
ToMlistenercomponentleadstoperformancegainsinourimagereferentialgame
setting. Wealsofindsomeevidencethatincreasingtaskdifficultyinthetraining
processresultsinmorefluentandpreciseutterancesinevaluation. Thissuggests
the potential utility of further incorporating ToM, as well as other insights from
childlanguageacquisition,intocomputationalmodelsoflanguageacquisition1.
1 INTRODUCTION
Humanlanguagesarefundamentallyshapedbysocial-communicativegoalsinthegroundedworld.
Moderntheoriesfromdevelopmentalpsychologyoftenattributehumans’uniqueabilitytoquickly
acquireandadaptlanguagetotheirabilitytoascribementalstatestootheragents(Tomasello,2005),
anabilityalsoknownasTheoryofMind(ToM).
Some previous studies have attempted to perform computational modeling of ToM. For instance,
ToM-like mechanisms have been demonstrated to allow models to better predict the behavior of a
future agent (Rabinowitz et al., 2018), model agents’ beliefs in a negotiation (Cao et al., 2018) or
acooperativegame(Bardetal.,2020),orchoosegoodutterancesbasedonthelistener’slinguistic
abilities(Zhuetal.,2021).However,theeffectsofToMhavenotyetbeenstudiedinthehigher-level
contextofcomputationallanguageacquisition.
Inthispaper,westudyhowaninternalToMmechanismandexternalenvironmentalpressurecon-
tribute to language learning. We use an image referential game setting consisting of a series of
trainingepisodesbetweenaspeaker, whichrepresentsalanguagelearner(Zhuetal.,2022), anda
listener,whichrepresentsafluentteacher. Whenpresentedwithasetofimages,oneofwhichisthe
target referent, the speaker must learn to generate an English utterance that the listener can use to
selectthetarget. Thespeakerisrewardedforgeneratingutterancesthatareusedtocorrectlyguess
the target image. Additionally, the speaker may be given feedback depending on the confidence
the listener has in the selection. This setting provides an attractive test-bed for testing the effects
of various reward signals or model designs on the speaker’s learned language; previous studies of
pragmaticsinlanguageacquisition,suchasAndreas&Klein(2016),haveusedsimilarsettings.
1Codeanddatacanbefoundathttps://github.com/neulab/ToM-Language-Acquisition.
1
3202
raM
2
]LC.sc[
1v20510.3032:viXra
PublishedasaconferencepaperatICLR2023
Target Image Distractor Image
1. Yellow shirt man throw frisbee
2. Frisbee front of trees
1. Three people play frisbee
3. Three people play frisbee
2. Frisbee front of trees
3. Yellow shirt man throw frisbee
yellow shirt man
throw frisbee
Candidate Utterance Sampling ToM Listener Reranking Output Utterance
Figure 1: An example of how ToM is used by our speaker models in our implementation. The
speakerfirstgeneratescandidateutterancesbeforererankingthemaccordingtotheprobabilitygiven
toeachtargetimage, utterancepairbytheinternalToMlistener. Itthenselectseitherthehighest-
scoringorarandomutterancefromthecandidatepooltooutput.
Withinthissetting, weseektobetterunderstandhowmodelswithandwithoutToMadapttotheir
environments. Wefocusontwospecificresearchquestionsinthisarea:
RQ1. HowdoestheinclusionofToMinlanguageacquisitionspeakermodelsaffecttheirperfor-
manceandlearnedlanguage? (internalToMmechanism)
RQ2. Howdoourmodelsadapttomoredifficultreferentialgameenvironmentsduringthelan-
guageacquisitionprocess? (externalenvironmentalpressure)
WestudytheimpactofToM(RQ1)bymodelinganinternallistenermodulewithinourspeakersthat
aimstopredictwhichutterancesaremostlikelytoresultinthedesiredlistenerbehavior. Byincor-
poratingtheprobabilitiesgiventothetargetimagebytheToMlistenerintotheutterancereranking
process, as shown in Fig. 1, we select for more pragmatic utterances sampled from the speaker’s
distribution. Tostudytheimpactofenvironmentalpressure(RQ2)onourspeakers, wecreateref-
erentialgameswithdifferentdifficultiesbysamplingdistractorsfromdifferentdistributions. These
distributionsarebasedonthesimilaritybetweenimagescalculatedbyvariousimagerepresentation
models,includingCLIP(Radfordetal.,2021),RoBERTa(Liuetal.,2020),andTF-IDFvariants.
In experiments, we find that (RQ1) speaker models including ToM components generally outper-
form those that do not in terms of fluency and final accuracy. We also find that (RQ2) training
withmorevisuallyandsemanticallysimilardistractorreferentscausesthespeakermodeltodevelop
longer,morefluent,andmorepreciseutterancestodistinguishbetweenpotentialreferents,although
thesedonotalwaystranslatetogainsinreferentialgameperformance. Theseresultssuggestcon-
tributions to language acquisition from both ToM and environmental pressures in this setting. We
stillfindsignificantgapsbetweenthelanguagethatourspeakermodelacquiresandhumancaptions.
Additionally,werestrictboththevocabularyandthemaximumlengthofourspeakers’utterances.
These both suggest that there is still room for improvement in this class of models. However, we
hopeourresultsstillhintatbothbettertrainingmethodsforpragmaticlanguagemodelsandadeeper
computationalunderstandingofhumanlanguageacquisition.
2 IMAGE REFERENTIAL GAME ENVIRONMENT
Following Lazaridou et al. (2016); Lowe et al. (2019); Zhu et al. (2022), we consider image ref-
erentialgameswithrealworldimages, whereaspeakerandalistenercollaborateonidentifyinga
targetimagex ∈ C amongdistractorsrandomlysampledfromasetofcandidatesC. Theidentity
ofthetargetisknownonlytothespeaker. ThespeakergeneratesanEnglish-languageutteranceu
2
PublishedasaconferencepaperatICLR2023
thatdescribesthetargetimage,whichisthenpassedtothelistener. Thelistenerwilleitherselectan
imagexˆ, inthecasewheretheyunderstandtheutterancewithahighenoughprobabilityorrefuse
to act, in the case where they do not. The listener can also determine whether or not to provide
feedback(linguisticinputintheformoftheground-truthcaptionofthetargetimage)tothespeaker.
Speakertrainingismotivatedbyarewardfunctionthatseekstomodelthecommunicativegoalof
directingthelistenertothecorrectreferent. Thespeakergetsapositiverewardof1ifthelistener
chooses the target image and a penalty of −1 if the listener chooses the wrong target image. Ad-
ditionally,asmallerpenaltyw isappliedtothespeakerifthelistenerchoosestonotchoosean
noop
image;thisisdonetopenalizethespeakerforgeneratingunclearutterances.
2.1 AGENTFORMULATION
WefollowthespeakerandlistenerformulationsdefinedinZhuetal.(2022). Thespeakerisamodel
f :I →Σ∗thatgeneratesutterances(sequencesoftokensfromitsvocabulary)thatcorrespondtoa
providedimage. WedefineΣ∗asthespeakervocabularyandI asthesetofcandidateimages.
ThelistenerisdefinedonΣ∗ andIN,whichisasetofN candidateimagessampledfromI. Given
an utterance ∈ Σ∗ and an observation space ∈ IN, the listener will either return the index of its
predictedimageornoop,ifitcannotpredictanimage. Itmayalsoreturntheground-truthcaption
fromitsvocabulary. Wedefineitwiththemodelg :Σ∗×IN →([0,1,...N −1]∪noop)×Σ∗.
2.2 SPEAKERDESIGN
The speaker that we use is a captioning model composed of a pretrained ResNet (He et al., 2016)
model and an LSTM-based utterance generation model. After generating an embedding vector
of the target image using the ResNet model, the speaker autoregressively generates an utterance
u={u }M usingtheLSTMnetwork:
i i=1
P(u |u ,u ,...,u ,x)
i 1 2 i−1
(1)
∝exp(wT LSTM(w ,w ,...,w ,h =ResNet(x))),
ui u1 u2 ui−1 0
where w
ui
∈ Rdw is the word embedding of u i. The speaker generates a sequence of tokens to
eitherthemaximumlength,whichissetto20inourexperiments,oranend-of-sequencetoken. In
ourexperiments, weuseavocabularysizeof200tolimitthesizeoftheactionspaceandtomore
realisticallymimicthevocabularyofalanguagelearner.
2.3 LISTENERDESIGN
Given a setof candidate images I (j ∈ 1,2...n+1), where n is thenumber of distractors, and
j
anutteranceu,thelistenercomputesembeddingsforeachimage,L(I ),fortheutterance,L(u). It
j
thentakesthedotproductofeachL(I )withL(u),andthesoftmaxofthedotproductstocompute
j
theprobabilityP(I |u)∝exp(L(I )L(u))oftheutterancereferringtoimageI foreachimage.
j j j
Thelisteneralsohasarule-basedcomponenttocontrolwhentogivelinguisticinputintheformof
theground-truthcaptionC(x). Weintroducetwothresholds,θ andθ . Afterthelistenercomputes
1 2
themostlikelytargetimage,t=argmaxP(I |u),andtheprobabilitygiventothischoice,P =
j max
maxP(I |u),itreturnsitschoicexˆandinputf accordingtothefollowinglogic:
j listener
 (noop,0) P <θ
 max 1
(xˆ,f )= (t,C(x)) θ <P <θ (2)
listener 1 max 2
(t,0) P >θ
max 2
This control strategy mimics a caregiver who only gives feedback when they understand what the
language learner is referring to, but wants to direct it to generate higher-quality utterances. When
thelistener’sconfidenceisverylow,itwillneitherselectatargetimagenorprovidelinguisticinput,
asthespeakerutteranceistoolow-quality. Meanwhile,whenthelistener’sconfidenceisveryhigh,
itwillstopgivinglinguisticinput,asfurtherimprovementisdeemedunnecessary.
3
PublishedasaconferencepaperatICLR2023
WeuseanLSTM-basedmodeltolearnembeddingsofspeakerutterancescombinedwithaResNet
modeltoderiveimagefeatures. Becausewewantourlistenertoatleastmodelacompetentuserof
thelanguagethatthespeakeristryingtoacquire,somepretrainingisrequiredbeforeinitializinga
listenerinanewenvironment. Thecontrollervaluesθ ,θ andtheResNetparametersaresetafter
1 2
preliminary experimentation on the dataset. The listener’s language network parameters are then
trainedwithmini-batchstochasticgradientdescenttooptimizethenetworkvalues
N
1 (cid:88)
θ =argmaxE logP (i|U∗,C;θ) (3)
listener
θ
C∼U(I)NN listener i
i=1
3 SPEAKER TRAINING PROCESS
3.1 COMMUNICATIVEGOALS
Wemodelcommunicativegoalsandlearningfromlinguisticinputastwoseparatelearningobjec-
tivesforourspeakernetwork,whichwecombinewithanadjustablecoefficientλ. Thecommunica-
tivegoalrewardO issimplytheaveragereward(definedin§2)overeachgameinthetraining
CG
episode. Thespaceofpossibleoutpututterancesisdiscreteandnon-differentiable, soourspeaker
learnsitspolicyπviathereinforcementlearningmethodPPO(Schulmanetal.,2017).
3.2 LEARNINGFROMLINGUISTICINPUT
We model the learning from linguistic input objective as the maximum likelihood of the listener
inputinthespeakermodelsandoptimizethiscontinuousfunctionwithstochasticgradientdescent.
O =E logπ(U∗ |x,C) (4)
LI x,C,u∼π(u|x) flistener(u,C)
Wethencombinetheseparatelearningobjectivesforeachtaskusingacoefficientλ∈[0,1]:
O =λO +(1−λ)O (5)
joint CG LI
Next,weintroduceToMModeling,andanadditionaltaskobjective,O . Thisisacross-entropy
ToM
objectivethatrepresentshowaccuratethespeaker’sinternallistenermodelis. Similartotheobjec-
tiveabove,wecombinetheseintoasinglelearningobjectivefortheentirespeakernetwork.
4 INTRODUCING TOM MODELING
We incorporate into the speaker architecture an LSTM-based ToM listener. This listener model is
trainedalongsideoftherestofthespeakernetworktolearnthe“mentalstate”oftheactuallistener–
theneuralnetworkparametersthatallowittobestreplicatetheprobabilitiesthatthelistenerwould
assign to image-utterance pairs. It uses a similar architecture to the actual listener, combining its
ownlearnedsentenceembeddingsfromanLSTMnetworkwithpretrainedimageembeddingsusing
ResNet. Inotherwords,ourToMlistenerseekstolearnthesentenceembeddingmodelθ that
ToM
willmaximizetheprobabilitiesassignedtothelistener’schoicegivenanutterance.
θ =argmaxP (u |xˆ)∝exp(θT (u )·ResNet(xˆ)) (6)
ToM ToM i ToM i
θ
WeintroduceToMintothespeakermodelbyhavingourspeakersamplecandidateutterancesand
rerankthemwiththehelpoftheToMlistener.OurToMspeakerfirstsamplesN candidateutterances
U ={u(i)}N fromitsdistribution. Next,wegenerateaspeakerandalistenerscoreforeach:
i=1
(cid:89)
P (u)∝ P(u |u ,u ,...u ,x) (7)
speaker i 1 2 i−1
i
P (x|u)∝exp(LSTMT(u)Resnet(x)) (8)
ToM
4
PublishedasaconferencepaperatICLR2023
Here,theprobabilityP(u |u ,u ,...u ,x)iscomputedaccordingto1. Wethencombinethese
i 1 2 i−1
scoresusingalistenerweighthyperparameterw togettheoverallscoreassignedtoeachutterance.
l
Finally,weselectthebestutterance,ub,astheargmaxofthisscore:
ub =argmax(P ToM(x|uj)wl ·p speaker(uj)) (9)
U
Inourexperiments,wetrainmodelswiththreedifferentsettingsofw .Wetrainmodelswithw =0
l l
to isolate the effects of the ToM listener from the effects of the new model architecture. We train
models with w = 1 which weigh the speaker and ToM listener input equally. Finally, we train
l
models where w is the arbitrarily high constant 1000. In this case, the listener score dominates,
l
and our speaker essentially seeks to maximize P (x|ui), which it approximates with the
listener
ToM listener score. If we replace the learned ToM listener with a copy of the external listener,
thisutterancesamplingprocessisclosetothatofthelearnedRationalSpeechAct(RSA)modelin
Andreas & Klein (2016), which also trains pragmatic speakers on referential game environments.
WecompareourToMspeakerstothese“RSA”speakerstoevaluatetheimpactofusingourlearned
approximationratherthantheactuallistener.
Finally,weintroduceahyperparameterσ: ourspeakeroutputsarandomutterancewithprobability
σ andub withprobability1−σ. σ issettodecaylinearlyovertime;thisrandomizationisdoneto
promoteearlyexploration.
Bydefault, webeginwithanuntrainedlistenerthatwillbetrainedtoemulatetheactuallistener’s
outputs over time. To train the ToM listener, we introduce a third training objective in addition to
O andO ,theToMobjectiveO ,definedasthecross-entropylossbetweenthedistribution
LI CG ToM
oftheToMlistenerandthatofthetruelistener. Thus,wearetrainingittogivehigherprobabilities
tothelistener’schoicexˆbasedonthespeaker’sutterances. Formally,
(cid:26)
−logP (xˆ|u) P >θ
O = ToM max 1 (10)
ToM 0 P <θ
max 1
Notethatthisrequiresachoicetobemadebythelistener,sowemaskoutthecasewheretheactual
listener does not choose an image by setting it to zero. This is done using the same parameter θ
1
andlistenerconfidenceP thatwereusedtocontrollinguisticinputin2. Weaddthistothejoint
max
learningobjectivepreviouslydefinedin5tocomputeourcombinedobjectivefortheToMspeaker
systemasawhole:
O =λO +(1−λ)O +O (11)
joint CG LI ToM
However, an untrained listener can introduce significant noise to the utterance generation process.
To counteract this, we anneal the influence of the listener, by linearly increasing w from 0 to the
l
final w value over a fixed number of steps. This allows our speaker to only begin using its ToM
l
listenerwhenthelistenerisatleastsomewhatcapable.
5 INCREASING DISTRACTOR DIFFICULTY
Previous studies on language acquisition have found that infants initially look at familiar objects
whenhearingsemanticallyrelatedlabelsfordifferentobjects,butadapttomoredifficulttasksover
timebylearningmorecomplexsemanticrepresentationsofobjects(Bergelson&Aslin,2017a).Ad-
ditionally,Yuksekgonuletal.(2022)showedthatcontrastivetrainingonharderdistractorsimproved
visiolinguistic models’ performance on tasks involving compositionality. We hypothesize that the
usageofmoresimilardistractorimagesmightsimilarlyforceourspeakertogeneratemorecomplex
utterances due to the need to further distinguish between images. This motivated us to generate
moresimilardistractorimagesinthetrainingprocessinordertoachievesuchaneffect. Todoso,
wecomputedasimilarityrankbetweenimagesbasedonvisualandsemanticsimilarity. Then,after
selectinga“target”imageinthetrainingprocess, wesampledimageswithhighsimilaritiestothe
targetimagetouseasdistractorsduringtraining.
Weexperimentwiththreeoptionsforcalculatingimage/captionsimilarity:
5
PublishedasaconferencepaperatICLR2023
Original Image Random Distractor Image Similarity Caption Similarity Hybrid Similarity
a little girl holding a two sheep standing a little dog sitting on a woman hugs a a kitten that is sitting
kitten next to a blue next to each other in a wooden bench. gray cat to her down by a door.
fence. the snow. chest.
Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics.
CaptionandHybridsimilaritiesarecomputedwithTF-IDF-weightedRoBERTaembeddings.
• Visual Cosine Similarity. We compute the most visually similar images based on the cosine
similarity of image embeddings from a pretrained ResNet model. For each image, we save a
similarityrankingofthenext1000mostvisuallysimilarimages,andthenselectdistractorsfrom
thissetduringtrainingwhenevertheoriginalimageisselectedasatarget.
• Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine
similarityofvectorrepresentationsofthetext. Weexperimentwithboth(1)densevectorsusing
embeddingscalculatedusingeitherthepretrainedRoBERTaorthepretrainedCLIPmeanpooled
modelsfromthesentence-transformerslibrary(Reimers&Gurevych,2019),and(2)sparsevec-
tors using one-hot vectors of each word. We also experimented with using term frequency –
inverse document frequency (TF-IDF) to weight the vector for each token in our captions when
computingcaptionsimilarityasawaytoupweightrarercontentwords.
• Visual+Textual Similarity. In cases where both image and caption similarity were used, we
addedacaptionweightparameterw ∈[0,1]andusedittocomputeaweightedaverageofimage
c
andcaptionsimilarity. Wehopedthatusinghybridmethods(w =0.5)wouldcapturedistractors
c
withbothvisualandsemanticsimilarities.
Wealsotrainmodelsonrandomlyselected, or“easy”, distractorstocreateabaselinetostudythe
effectsofdistractordifficulty. ExamplesofaselectionofthedistractorsettingsareshowninFig.2.
6 EXPERIMENTAL SETUP AND RESULTS
6.1 EXPERIMENTALSETUP
Wefocusontwomainresults: Aspeaker’sabilitytouselanguagetocorrectlydiscriminatebetween
thetargetanddistractorimages,aswellasthequalityofthelanguagespeakersusetodoso.
Toevaluatetheformer,weprimarilyconsideraverageaccuracy,orhowoftenthelistenerselectsthe
correctreferent. Weevaluatethequalityofourspeakers’learnedlanguagesusingfluency,average
length,F1scoreforsalientpartsofspeech,andcaptionqualityscores. Weuseafluencyscorethat
measures the grammatical quality of output utterances (Kann et al., 2018). This is defined as the
averagegaininlogprobabilitywhenmovingfromaunigrammodeltoapretrainedlanguagemodel.
1
fluency= (ln(p (u))−ln(p (u))) (12)
|u| M U
Wetrainaunigrammodel,p ,andfine-tuneGPT-2large(Radfordetal.,2019),p ,onourtraining
U M
set.
In addition to fluency and accuracy, we consider two additional performance metrics. To better
evaluatetheoverallcaptionqualityofthesystem,wecomputeBLEUscore(Papinenietal.,2002)
usingtheimplementationfoundinBirdetal.(2009). Additionally,toevaluatetheaccuracyofthe
learnedlistenermodel,wecomputeToMAccuracy,definedashowoftenthecandidateimagethat
theToMlistenerbelieveshasthehighestprobabilityofbeingchosenistheactuallistener’schoice.
Wealsoconsiderpart-of-speechstatisticstoevaluateutterancequality. Partsofspeechforwordsin
speakerutterancesandground-truthcaptionsaretaggedwithspaCy(Honnibaletal.,2020). Weuse
6
PublishedasaconferencepaperatICLR2023
Target Distractor Distractor
a group of men on a field playing a person on some skis. a white building
baseball. with a clock on the
front and side of it.
Base Speaker: A baseball player holding a holding umbrella.
Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.
ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside.
Figure 3: Examples of output utterances generated by various speaker models. Here, the “Hard
Distractors” referenced are selected with hybrid metrics computed with CLIP embeddings. Both
trainingonmoredifficultdistractorsandtrainingwithToMleadstomoreaccurate,fluentutterances.
Table1: PerformanceandlanguagefeaturesofvariousToMspeakers.
Model Performance POSF1
ToMWeight Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB
Baseline(NoToM) Easy 0.81 0.20 1.50 N/A 0.16 0.52 0.41 0.38
Baseline(NoToM) Hard 0.81 0.24 1.87 N/A 0.24 0.58 0.46 0.45
GoldStandard N/A 0.92 1.00 2.52 N/A 1.00 1.00 1.00 1.00
Zero Hard 0.83 0.26 1.99 0.81 0.22 0.64 0.49 0.47
Normal Hard 0.85 0.26 2.25 0.88 0.22 0.65 0.52 0.49
High Hard 0.88 0.27 2.23 0.89 0.22 0.66 0.52 0.50
HighRSA Hard 0.87 0.28 2.26 0.93 0.23 0.65 0.50 0.49
Zero Easy 0.85 0.25 1.73 0.85 0.21 0.57 0.48 0.49
Normal Easy 0.88 0.26 2.09 0.91 0.21 0.64 0.50 0.52
High Easy 0.88 0.27 2.07 0.91 0.22 0.65 0.51 0.50
HighRSA Easy 0.89 0.29 1.91 0.94 0.17 0.65 0.52 0.49
this to compute F1 scores between speaker utterances and ground-truth captions over each part of
speech. Thisallowsustoevaluatehoweffectivelyourspeakersidentifyreferentswithinthetarget
imagethatahumancaptionerwouldconsidersalient. HigherF1scoresoverapartofspeechwould
suggestthataspeakerismorecapableofidentifyingconceptsthatfallunderthatpartofspeech.
Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The
train-val-testsplitgivenbytheMSCOCO2017datasetisextendedtoourexperimentalsetup.
6.2 EFFECTSOFTOM
WetrainmodelsequippedwithToMlistenerswithnormalweight(i.e.equallyweightingthespeaker
andthelistenerscoreswhenrerankingoutpututterances)andwithhighweight(wherethelistener
weightissetarbitrarilyto1000andthespeakeressentiallyoptimizesforP(x|ui)).Inordertoisolate
theeffectsofusingaToMlistenerfromtheeffectsofmodifyingtheutteranceselectionprocess,we
also train a model equipped with a ToM listener that it does not give any weight to. Additionally,
wetrainamodelwithoutanyToMcomponentforcomparison. Wedothisforbotheasyandhard
distractors,wheretheharddistractorswerethosethatwerechosenbythehybridsimilaritybetween
visual and semantic (CLIP) features outlined in 5. Finally, we give the listener the ground-truth
captionsoverthetestsettocomputegold-standardmetricsofaccuracyandfluency.
WefindsignificantperformanceimprovementsinTable1whenspeakermodelsaretrainedtorerank
utterancessolelybyToMlistenerscore. Such“high-weightToM”speakermodelsachieveaccuracy
gainsof3.0%and4.6%oneasyandharddistractors,respectively. Thissuggeststhattheinclusion
ofasufficientlyinfluentialToMrerankerduringthespeakertrainingprocessimprovesspeakerper-
formance, although the relative gains appear to be much higher when training on easy distractors.
7
PublishedasaconferencepaperatICLR2023
Table 2: Performance and language features of speakers trained on various distractors. We only
showthemostperformantvariantsofCaptionandHybridsimilarity,withtheothersshowninA.1.
Model Performance POSF1 Average
Distractors Acc Fluency ADJ ADP NOUN VERB Length
Base 0.81 1.50 0.16 0.52 0.41 0.38 8.97
GoldStandard 0.92 2.52 1.00 1.00 1.00 1.00 10.79
Image 0.80 2.09 0.19 0.61 0.45 0.49 10.33
Caption 0.86 2.01 0.19 0.62 0.49 0.48 10.04
Hybrid 0.85 2.19 0.20 0.62 0.49 0.47 10.18
However,wefindthatspeakermodelsthatrerankutterancesusingacombinedspeaker-ToMscore
generallyfailtooutperformmodelsthatdonotusetheirToMlistenerintraining.
Wealsofindthattheusageofahighly-weightedToMlistenerleadstosignificantfluencygainswhen
training on both easy (15.6% relative increase in fluency score) and hard (11.6%) distractors. We
alsoseelongerandmorecomplexutteranceswhenusingnormallyorhighlyweightedToMlisteners.
Additionally,wefindlimitedgainsingeneralcaptioningabilitybetweenbaselineandhigh-weight
models, as measured by BLEU score. However, these effects are more subtle, and do not always
leadtosignificantaccuracygains,suggestingthatthemaindriverofToMaccuracygainsisincreased
pragmatic ability. We conclude that usage of a highly influential ToM listener during the training
processleadstosignificantperformanceandfluencygains. Wearealsoabletoqualitativelyobserve
theimprovementinmodelperformancefromToM.AsseeninonerepresentativeexampleinFig.3,
ourToMSpeakerisabletoidentifytwoelementsthatclearlydistinguishthetargetimagefromthe
distractors(i.e.thattherearemultiplemenwhoareplayingbaseball)inafluentutterance.
Finally,wefindthattheToMlistenersuccessfullyapproximatestheexternallistener. Modelswith
learnedlistenersandRSAmodelswiththepretrainedlistenerperformcomparablyinaccuracyand
fluency. BecausetheRSAmodelsrepresenttheupperboundofhowgoodaspeaker’slistenermodel
canbe,thissuggeststhatourlearnedlistenersareverybeneficialtothespeakers. Thisisalsoshown
through the high ToM accuracies reported, especially in the most performant models, those with
highlistenerweight. Thesequalitativeandquantitativeresultsprovidecomputationalevidencethat
ToM can play an important role in simulated language acquisition, similarly to how it has been
hypothesizedtoplayacriticalroleinhumanlanguageacquisition.
6.3 EFFECTSOFDISTRACTORDIFFICULTY
As shown in Table 2, we generally find significant improvements in language quality in models
trained on more difficult distractors. The largest gains are those seen in the fluency score, where
difficult distractors achieve gains ranging between 25% to 46%. We also find that models trained
onmoredifficultdistractorsusemoresimilarvocabularytotheground-truthcaptions,asmeasured
byF1scoreintheground-truthcaptionsandutterancesproduced. Thisissignificantlyhigherover
adpositions,nouns,andverbsonmodelstrainedwithmoredifficultdistractors. Finally,allspeakers
trainedondifficultdistractorsgeneratemorecomplexutterancescomparedtothebasespeaker,with
utterances that are at least one word longer on average. This supports our hypothesis that when
confronted with increased environmental pressure, the speaker adapts by becoming more precise,
fluent,andcomplexwithitslanguage.Thesecanbeseenqualitativelyinonerepresentativeexample
in Fig. 3, which shows that a speaker trained on hard distractors is able to generate more fluent
utterancesthatmorepreciselydescribetheimage(inthisexample,correctlyidentifyinganobjectin
theimageasabaseballbat,asopposedtoanumbrella).
Wefindsmallerdifferencesbetweenthelanguageofmodelstrainedwithvarioustypesofharddis-
tractors. Speaker models trained with visually similar distractors achieve the highest fluency, at
2.094, and form the longest utterances. They also have more precise verb selection, as measured
byF1. Speakerstrainedondistractorsthatwereselectedwithhybridorcaptionsimilarity,achieved
highnounF1scoresof0.49comparedto0.41forthebasespeakermodel,indicatingthatsemanti-
callysimilardistractorsintrainingmaybebetterforidentifyingsalientnouns. Wefindthattraining
onmoredifficultdistractorsdoesnotconsistentlyimprovemodelperformancewhenevaluatingon
easierdistractors. Thissuggestssomedisconnectbetweenalanguage’sfluencyanditssuitabilityto
8
PublishedasaconferencepaperatICLR2023
the image referential game environment. However, speakers that train on more semantically simi-
lar distractors still achieve up to 5 percent higher accuracy than the base speaker, indicating some
benefitstoperformancefromtrainingoncertainharderdistractors.
7 RELATED WORK
Parallels in Human Language Acquisition. The concept of learning language through repeated
exposure to referents is a popular model within the psychology community. Smith & Yu (2008)
foundthatinfantsresolvetheuncertaintyofdeterminingwhichreferentinasceneawordreferstoby
statisticallearningoverword-scenepairings.Yu&Ballard(2007)incorporatedasocialelementinto
modelsbyconsidering“social-cognitive”capacities,suchasattentionreading,thatactasaformof
ToM.Yu&Smith(2012)studiedcaregivers’socialimpactonearlylanguageacquisitionusinghead-
mounted cameras, finding that caregiver feedback during moments where a referent was visually
dominant directly led to language learning in the infant. Bergelson & Aslin (2017b) found that
unrelatedimageswereeasierforinfantstodifferentiatebetweenthansemanticallysimilarimages.
Pragmatic Modelling. Andreas & Klein (2016) used pragmatic models to jointly train neural
speakerandlistenermodelstoplayareferentialgamethatinvolveddescribingcontrastingscenes.
They also use a sample-and-rerank method for selecting utterances in this setup. However, they
use the actual listener, rather than a learned listener, to rerank utterances. Additionally, we apply
this process to a computational model of language acquisition. Nematzadeh et al. (2018) created
a dataset to evaluate question-answering models’ ability to keep track of inconsistent worldviews.
They found that state-of-the-art neural models lack this ability, indicating that they cannot solve
tasks with ToM. Monroe et al. (2017) studied the effects of visually similar distractors on a prag-
maticmodelofcoloridentification,findingthatpragmaticmodelshadthelargestgainsinthemost
difficultsettings.Vedantametal.(2017)alsoconsidertheeffectsofincludingpragmaticcomponents
inimagecaptioningmodels,namelyaninternalmoduletobetterdiscriminatebetweenimages.
EmergentLanguage. Lazaridou&Baroni(2020)surveyedrecentprogressinemergentlanguage
from multi-agent communication, claiming that further progress can help deep networks become
moreinteractiveandinterpretable. Lazaridouetal.(2020)placeagenerallytrainedlanguagemodel
in a multi-agent environment with task-specific rewards. Similarly to our work, this results in a
task-conditionallanguagemodelwhichtheauthorsclaimcanbettercommunicatewithhumansover
visualtasks. Chaabounietal.(2022)analyzedtheeffectsoftaskdifficultyonemergentcommuni-
cationtasksbyvaryingthenumberofdistractors,leadingtonegativeeffectsonmodelperformance
during evaluation. Mu & Goodman (2021) attempted to improve interpretability of learned lan-
guages in referential games by forcing speakers to communicate over sets of objects representing
abstractvisualconcepts,andanalyzedthecompositionalityoftheensuingemergentlanguages.
8 CONCLUSION AND FUTURE WORK
Inthispaper,weextendanexistingcomputationalframeworkthatmodelsthelanguageacquisition
processinanimagereferentialgameenvironment. Mostnotably,weaddaToMcomponenttoour
speakermodels,allowingourspeakerstopragmaticallyrerankcandidateutteranceswithalearned
internallistener. Wealsoexperimentwithincreasingdistractordifficultybyupweightingmorese-
manticallyandvisuallysimilardistractorimages. WefindthatincorporatingToMintoourspeaker
modelsleadstoimprovementsinspeakerperformance. Wealsofindthatincorporatingharderdis-
tractorsleadstothedevelopmentofmorecomplexandfluentlanguages.
Futureworkcouldmeasurethesimilarityofthelearningprocessbetweenhumanlearnersandour
models, and whether the changes implemented in this paper lead to a more humanlike learning
process. Furtherworkcouldalsoconsidertheimplicationsofmoredynamicdifficultyadjustment
or curriculum design – for instance, studying whether models trained on a variety of distractor
difficulties are able to adjust their utterances to fit a context. Finally, we can study these effects
inmorecomplexenvironmentsbyvaryingthelistenerarchitectureorbyconsideringmoredifficult
settings, such as object referential games. We encourage the machine learning and psychological
modellingcommunitiestoconsiderthefurtherincorporationofToMintocomputationalmodelsof
languageacquisition,whichcouldhelpdevelopmorepragmatically-awaremodels.
9
PublishedasaconferencepaperatICLR2023
REFERENCES
Jacob Andreas and Dan Klein. Reasoning about pragmatics with neural listeners and speakers.
Proceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,1:
1173–1182,2016.
NolanBard,JakobNFoerster,SarathChandar,NeilBurch,MarcLanctot,HFrancisSong,Emilio
Parisotto,VincentDumoulin,SubhodeepMoitra,EdwardHughes,etal. Thehanabichallenge: A
newfrontierforairesearch. ArtificialIntelligence,280:103216,2020.
Elika Bergelson and Richard Aslin. Semantic specificity in one-year-olds’ word comprehen-
sion. Language Learning and Development, 13:1–21, 06 2017a. doi: 10.1080/15475441.2017.
1324308.
Elika Bergelson and Richard N. Aslin. Nature and origins of the lexicon in 6-mo-olds. Pro-
ceedings of the National Academy of Sciences, 114(49):12916–12921, 2017b. doi: 10.
1073/pnas.1712966114. URL https://www.pnas.org/doi/abs/10.1073/pnas.
1712966114.
StevenBird,EwanKlein,andEdwardLoper.NaturalLanguageProcessingwithPython:Analyzing
TextwiththeNaturalLanguageToolkit. O’Reilly,Beijing,2009. ISBN978-0-596-51649-9. doi:
http://my.safaribooksonline.com/9780596516499. URLhttp://www.nltk.org/book.
KrisCao,AngelikiLazaridou,MarcLanctot,JoelZLeibo,KarlTuyls,andStephenClark.Emergent
communicationthroughnegotiation. arXivpreprintarXiv:1804.03980,2018.
RahmaChaabouni,FlorianStrub,FlorentAltche´,EugeneTarassov,CorentinTallec,ElnazDavoodi,
KoryWallaceMathewson,OlivierTieleman,AngelikiLazaridou,andBilalPiot. Emergentcom-
munication at scale. In International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=AUGBfDIV9rL.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778,2016.
Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spacy: Industrial-
strengthnaturallanguageprocessinginpython. 2020. doi: 10.5281/zenodo.1212303.
KatharinaKann,SaschaRothe,andKatjaFilippova. Sentence-levelfluencyevaluation: References
help, but can be spared! In Proceedings of the 22nd Conference on Computational Natural
LanguageLearning,pp.313–323,2018.
AngelikiLazaridouandMarcoBaroni. Emergentmulti-agentcommunicationinthedeeplearning
era. CoRR,abs/2006.02419,2020. URLhttps://arxiv.org/abs/2006.02419.
AngelikiLazaridou,AlexanderPeysakhovich,andMarcoBaroni. Multi-agentcooperationandthe
emergenceof(natural)language. arXivpreprintarXiv:1612.07182,2016.
Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets
natural language: Synergies between functional and structural language learning. CoRR,
abs/2005.07064,2020. URLhttps://arxiv.org/abs/2005.07064.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conferenceoncomputervision,pp.740–755.Springer,2014.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Ro{bert}a: A robustly optimized {bert} pre-
trainingapproach,2020. URLhttps://openreview.net/forum?id=SyxS0T4tvS.
Ryan Lowe, Abhinav Gupta, Jakob Foerster, Douwe Kiela, and Joelle Pineau. On the interaction
betweensupervisionandself-playinemergentcommunication. InInternationalConferenceon
LearningRepresentations,2019.
10
PublishedasaconferencepaperatICLR2023
WillMonroe,RobertX.D.Hawkins,NoahD.Goodman,andChristopherPotts. Colorsincontext:
Apragmaticneuralmodelforgroundedlanguageunderstanding. TransactionsoftheAssociation
for Computational Linguistics, 5:325–338, 2017. doi: 10.1162/tacl a 00064. URL https:
//aclanthology.org/Q17-1023.
Jesse Mu and Noah Goodman. Emergent communication of generalizations. In M. Ran-
zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances
in Neural Information Processing Systems, volume 34, pp. 17994–18007. Curran Asso-
ciates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/
9597353e41e6957b5e7aa79214fcb256-Paper.pdf.
Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Tom Griffiths. Evaluating
theory of mind in question answering. In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pp. 2392–2400, Brussels, Belgium, October-
November2018.AssociationforComputationalLinguistics. doi: 10.18653/v1/D18-1261. URL
https://aclanthology.org/D18-1261.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ’02, pp. 311–318, USA, 2002. Association for Computa-
tional Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10.3115/
1073083.1073135.
DavidPremackandGuyWoodruff. Doesthechimpanzeehaveatheoryofmind? Behavioraland
brainsciences,1(4):515–526,1978.
Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew
Botvinick. Machinetheoryofmind. InInternationalconferenceonmachinelearning,pp.4218–
4227.PMLR,2018.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision, 2021. URL
https://arxiv.org/abs/2103.00020.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.
org/abs/1908.10084.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms,2017. URLhttps://arxiv.org/abs/1707.06347.
Linda Smith and Chen Yu. Infants rapidly learn word-referent mappings via cross-situational
statistics. Cognition, 106(3):1558–1568, 2008. ISSN 0010-0277. doi: https://doi.org/
10.1016/j.cognition.2007.06.010. URLhttps://www.sciencedirect.com/science/
article/pii/S0010027707001795.
Michael Tomasello. Constructing a Language: A Usage-Based Theory of Language Acquisition,
volume1. HarvardUniversityPress,2005.
Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, and Gal Chechik. Context-
awarecaptionsfromcontext-agnosticsupervision. CoRR,abs/1701.02870, 2017. URLhttp:
//arxiv.org/abs/1701.02870.
ChenYuandDanaH.Ballard. Aunifiedmodelofearlywordlearning: Integratingstatisticaland
social cues. Neurocomputing, 70(13):2149–2165, 2007. ISSN 0925-2312. doi: https://doi.org/
10.1016/j.neucom.2006.01.034. URL https://www.sciencedirect.com/science/
article/pii/S092523120600508X. Selected papers from the 3rd International Confer-
enceonDevelopmentandLearning(ICDL2004)Timeseriespredictioncompetition: theCATS
benchmark.
11
PublishedasaconferencepaperatICLR2023
Chen Yu and Linda B. Smith. Embodied attention and word learning by toddlers. Cog-
nition, 125(2):244–262, 2012. ISSN 0010-0277. doi: https://doi.org/10.1016/j.cognition.
2012.06.016. URL https://www.sciencedirect.com/science/article/pii/
S0010027712001369.
MertYuksekgonul,FedericoBianchi,PratyushaKalluri,DanJurafsky,andJamesZou. Whenand
why vision-language models behave like bags-of-words, and what to do about it?, 2022. URL
https://arxiv.org/abs/2210.01936.
HaoZhu,GrahamNeubig,andYonatanBisk. Few-shotlanguagecoordinationbymodelingtheory
ofmind. InInternationalConferenceonMachineLearning,pp.12901–12911.PMLR,2021.
HaoZhu,YonatanBisk,andGrahamNeubig. Simulatedlanguagelearningthroughcommunicative
goalsandlinguisticinput. ProceedingsoftheAnnualMeetingoftheCognitiveScienceSociety,
44:1351–1358,2022.
A APPENDIX
A.1 FULLEFFECTSOFDISTRACTORDIFFICULTY
InTable2,weselectthecaptionandhybriddistractorsinvolvedinthetrainingofthemostperfor-
mant models to be reported. Here, as promised, we report the results of experiments over a fuller
rangeofdistractorvariants.
Table3: Performanceandlanguagefeaturesofspeakerstrainedonalldistractorvariants.
Model Performance POSF1 Average
Distractors Acc Fluency ADJ ADP NOUN VERB Length
Base 0.81 1.50 0.16 0.52 0.41 0.38 8.97
GoldStandard 0.92 2.52 1.00 1.00 1.00 1.00 10.79
Image 0.80 2.09 0.19 0.61 0.45 0.49 10.33
CaptionCLIP 0.83 1.87 0.18 0.57 0.46 0.43 9.56
CaptionRoBERTa 0.83 1.93 0.20 0.60 0.48 0.47 10.06
CaptionRoBERTaTFIDF 0.86 2.01 0.19 0.62 0.49 0.48 10.04
CaptionTFIDF 0.83 2.05 0.21 0.63 0.46 0.45 10.30
Hybrid CLIP 0.81 1.87 0.24 0.58 0.46 0.45 9.78
Hybrid RoBERTa 0.83 1.94 0.20 0.61 0.48 0.47 10.09
Hybrid RoBERTaTFIDF 0.84 2.09 0.20 0.65 0.48 0.46 10.29
Hybrid TFIDF 0.85 2.19 0.20 0.62 0.49 0.47 10.18
12

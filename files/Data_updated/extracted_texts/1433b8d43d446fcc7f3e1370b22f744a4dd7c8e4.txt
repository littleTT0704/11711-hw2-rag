To Build Our Future, We Must Know Our Past:
Contextualizing Paradigm Shifts in Natural Language Processing
SireeshGururaja1∗ AmandaBertsch1∗ ClaraNa1∗
DavidGrayWidder2 EmmaStrubell1,3
1LanguageTechnologiesInstitute,CarnegieMellonUniversity,Pittsburgh,PA,USA
2DigitalLifeInitiative,CornellTech,CornellUniversity,NewYorkCity,NY,USA
3AllenInstituteforArtificialIntelligence,Seattle,WA,USA
{sgururaj, abertsch, csna, estrubel}@cs.cmu.edu, david.g.widder@gmail.com
Abstract
Unique authors publishing in *CL venues
17500 1+ papers
NLPisinaperiodofdisruptivechangethatis 2+ papers
3+ papers
impactingourmethodologies,fundingsources, 15000 5+ papers
andpublicperception. Inthiswork,weseekto
12500
understandhowtoshapeourfuturebybetter
understandingourpast. Westudyfactorsthat 10000
shapeNLPasafield,includingculture,incen-
7500 tives, and infrastructure by conducting long-
form interviews with 26 NLP researchers of
5000
varyingseniority,researcharea,institution,and
socialidentity. Ourintervieweesidentifycycli- 2500
calpatternsinthefield,aswellasnewshifts
0
withouthistoricalparallel,includingchangesin 1980 1990 2000 2010 2020
Figure1: Thenumberofuniqueresearcherspublishing
benchmarkcultureandsoftwareinfrastructure.
in ACL venues has increased dramatically, from 715
Wecomplementthisdiscussionwithquantita-
uniqueauthorsin1980to17,829in2022.
tive analysis of citation, authorship, and lan-
guage use in the ACL Anthology over time. of dubious scientific value,” and that AI technol-
Weconcludebydiscussingsharedvisions,con-
ogycouldleadtoacatastrophiceventthiscentury
cerns,andhopesforthefutureofNLP.Wehope
(Michael et al., 2022). More recently, there has
that this study of our field’s past and present
been discussion of the increasing prevalence of
can prompt informed discussion of our com-
munity’s implicit norms and more deliberate closed-source models in NLP and how that will
actiontoconsciouslyshapethefuture. shapethefieldanditsinnovations(Rogers,2023;
Solaiman,2023;LiaoandVaughan,2023). Inorder
1 Introduction totacklethesebigchallenges,wemustunderstand
the factors — norms, incentives, technology and
Naturallanguageprocessing(NLP)isinaperiod
culture—thatledtoourcurrentcrossroads.
offlux. Theunprecedentedadvancesofdeepneu-
Wepresentastudyofthecommunityinitscur-
ralnetworksandlargelanguagemodels(LLMs)in
rent state, informed by a series of long-form ret-
NLPcoincideswithashiftnotonlyinthenatureof
rospectiveinterviewswithNLPresearchers. Our
ourresearchquestionsandmethodology,butalso
interviewees identify patterns throughout the his-
inthesizeandvisibilityofourfield. Sincethemid-
tory of NLP, describing periods of research pro-
2010s,thenumberoffirst-timeauthorspublishing
ductivity and stagnation that recur over decades
in the ACL Anthology has been increasing expo-
andappearatsmallerscalearoundprominentmeth-
nentially(Figure1). RecentpublicityaroundNLP
ods(§3). Intervieweesalsopointoutunparalleled
technology, most notably ChatGPT, has brought
shifts in the community’s norms and incentives.
ourfieldintothepublicspotlight,withcorrespond-
Aggregatingtrendsacrossinterviews,weidentify
ing(over-)excitementandscrutiny.
keyfactorsshapingtheseshifts,includingtherise
Inthe2022NLPCommunityMetasurvey,many
andpersistenceofbenchmarkingculture(§4)and
NLPpracticionersexpressedfearsthatprivatefirms
the maturation and centralization of software in-
exert excessive influence on the field, that “a ma-
frastructure(§5). Ourquantitativeanalysisofcita-
jority of the research being published in NLP is
tionpatterns,authorship,andlanguageuseinthe
∗Denotesequalcontribution. ACLAnthologyovertimeprovidesacomplemen-
3202
tcO
11
]LC.sc[
1v51770.0132:viXra
tary view of the shifts described by interviewees, wasnotpresentintheinterviewappliedtheclosed
groundingtheirnarrativesandourinterpretationin coding frame to the data. In weekly analysis
measurable trends. Through our characterization meetings,newcodesarosetocapturenewthemes
ofthecurrentstateoftheNLPresearchcommunity or provide greater specificity, in which case the
and the factors that have led us here, we aim to closed coding scheme was revised, categories
offer a foundation for informed reflection on the refined, and data re-coded in an iterative process.
futurethatweasacommunitymightwishtosee. Analysis reported here emerged first from this
coded data, was refined by subsequent review of
2 Methods rawtranscriptstocheckcontext,anddevelopedin
discussionbetweenallauthors.
2.1 Qualitativemethods
Werecruited26researcherstoparticipateininter- 2.2 QuantitativeMethods
viewsusingpurposive(Campbelletal.,2020)and Weusequantitativemethodsprimarilyasacoher-
snowball sampling (Parker et al., 2019): asking ence check on our qualitative results. While our
participants to recommend other candidates, and workislargelyconcernedwiththecausesandcom-
purposivelyselectingfordiversityinaffiliation,ca- munityreceptionofchangesinthecommunity,our
reerstage,geographicposition,andresearcharea quantitative analyses provide evidence that these
(see participant demographics in Appendix A.1). changes have occurred. This includes analyzing
Oursamplehada69-31%academia-industrysplit, authorshipshifts(Figure4),citationpatterns(Fig-
was19%women, and27%ofparticipantsidenti- ure 2), terminology use (Figure 2,3) in the ACL
fiedaspartofaminoritizedgroupinthefield. Of anthologyovertime;formoredetailsonreproduc-
ouracademicparticipants,wehadanear-evensplit ingthisanalysis,seeAppendixB.
ofearly-,mid-,andlate-careerresearchers;indus-
try researchers were 75% individual contributers 3 Exploit-explorecyclesofwork
and25%managers.
Our participants described cyclical behavior in
Interviewsweresemi-structured(Weiss,1995),
NLPresearchfollowingmethodologicalshiftsev-
includingadedicatednotetaker,andrecordedwith
eryfewyears. Manyparticipantsreferredtothese
participantconsent,lastingbetween45-73minutes
methodological shifts as “paradigm shifts”, with
(mean: 58 minutes). Interviews followed an in-
similarstructure,whichwecharacterizeasexplore
terview guide (see Appendix A.3) and began by
andexploitphases.1
contrastingtheparticipant’sexperienceoftheNLP
communityatthestartoftheircareerandthecur- Firstwave: exploit. Participantssuggestedthat
rentmoment,thenmovedtodiscussionofshiftsin after a key paper, a wave of work is published
thecommunityduringtheircareer. Interviewswere thatdemonstratestheutilityofthatmethodacross
conductedbetweenNovember2022andMay2023; varying tasks or benchmarks. Interviewees vari-
coincidentally,thiswascontemporaneouswiththe ously describe this stage as “following the band-
releaseofChatGPTinNovember2022andGPT-4 wagon” (17), “land grab stuff” (9), or “picking
inMarch2023,whichfrequentlyprovidedpoints thelow-hangingfruit”(8). Aresearcherwithprior
ofreflectionforourparticipants. experience in computer vision drew parallels be-
Following procedures of grounded the- tween the rise of BERT and the computer vision
ory(StraussandCorbin,1990),theauthorspresent communityafterAlexNetwasreleased,where“it
for the interview produced analytical memos feltlikeeveryotherpaperwas,‘Ihavefinetuned
for early interviews (Glaser et al., 2004). As ImageNet trained CNN on some new dataset’ ”
interviewsproceeded,authorsbeganaprocessof (17). However, participants identified benefits of
independently open coding the data, an interpre- this “initial wave of showing things work” (17)
tive(Lincolnetal.,2011)analyticalprocesswhere in demonstrating the value of techniques across
researchers assign conceptual labels to segments tasks or domains; in finding the seemingly obvi-
of data (Strauss and Corbin, 1990). After this, ous ideas which do not work, thus exposing new
authors convened to discuss their open codes, areastoinvestigate;andindevelopingdownstream
systematizing recurring themes and contrasts to applications. When one researcher was asked to
constructapreliminaryclosedcodingframe(Miles
1Inspiredbytheverbiageofreinforcementlearning,e.g.
and Huberman, 1994). After this, an author who SuttonandBarto(2018).
Figure 2: Quantitative and qualitative timeline. The lower half of this diagram captures historical information
thatourparticipantsfeltwasrelevant,alongwiththeirreporteddateranges. Theupperhalfcapturesquantitative
informationthatparallelsthattimeline. Barchartsindicatefractionofpapersthatciteagivenpaper,whileline
chartsindicatethefractionofpapersthatuseaparticularterm.
identify their own exploit work, they joked we work may focus on interpretability, bias, data or
could simply “sort [their] Google Scholar from compute efficiency. While some participants see
most cited to least cited” (9), describing another thisasatimeof“stalled”(8)progress,othersde-
incentivetopublishthefirstpaperapplyinganew scribedthisas“themoreinterestingresearchafter
methodologytoaparticulartask. Participantsad- theinitialwaveofshowingthingswork”(18). A
ditionally described doing exploit work early in mid-career participant identified this as the work
theircareersasahedgeagainstriskier,longer-term they choose to focus on: “I’m at the stage of my
projects. However,mostparticipantsascribedlow career where I don’t want to just push numbers,
status to exploit work2, with participants calling you know. I’ll let the grad students do that. I
these projects “low difficulty” (4) and “obvious wanttodointerestingstuff”(22). Participantsof-
idea[s]”(9)with“highprobabilityofsuccess”(4). tensaw“pushingnumbers”aslower-statuswork,
Participantsalsodiscussedincreasingriskofbeing appropriateforgraduatestudentsandimportantfor
“scooped”(7,9,4)onexploitworkasthecommunity advancingthefieldandone’scareer,butultimately
growslarger. notwhatresearchershopetoexplore.
Participantsfeltthattheexploitphaseofwork Someworktoimprovebenchmarkperformance
isnotsustainable. Eventually,“thelowhanging was also perceived as explore work, particularly
fruit has been picked” (8); this style of work be- when it involved developing new architectures.
comesunsurprisingandlesspublishable. Asone One participant described a distinction between
researcherputit: “ifIfinetune[BERT]forsome “enteringarace”(4)and“forginginanewdirection”
newtask,it’sprobablygoingtogetsomehighac- (4)withaproject,whichfocusestheexploit/explore
curacy. Andthat’sfine. That’stheend.” (17). dividemoreontheperceivedsurprisingnessofan
idearatherthanthetypeofcontributionmade. Ex-
Second wave: explore After some time in the
ploration often leads to a new breakthrough,
exploitphase,participantsdescribedastatewhere
causingthecycletobeginanew.
obviousextensionstothedominantmethodology
have already been proposed, and fewer papers 3.1 Wherearewenow?
demonstratedramaticimprovementsoverthestate
Placing the current state of the field along the
oftheartonpopularbenchmarks.
exploit-explorecyclerequiresdefiningthecurrent
Inthisphase,workonidentifyingthewaysthat methodological “paradigm”. Participants identi-
thenewmethodisflawedgainsprominence. This fiedsimilar patternsat varying scales, with some
disagreementontheplacementofrecenttrends.
2Data work is also commonly perceived as low-status
(Sambasivanetal.,2021);participantsagreeddataworkwas
Prompting as a methodological shift Several
previouslyundervaluedinNLPbutdescribedatrendofin-
creasingrespect,onecallingdatasetcuration“valorized.”(7) participants described prompting as a paradigm
shift or a direction that the community found featuringgreatercentralizationonfewermethods
promising, but most participants viewed current (see§5formorediscussion). Someexpressedcon-
workonpromptengineeringor“ChatGPTforX” cern: “atechniqueshowssomepromise,andthen
(9) as something that people are working on “in- morepeopleinvestigateit. That’sperfectlyappro-
stead[...] ofsomethingthatmightmakeafunda- priateandreasonable,butIthinkithappensalittle
mentaldifference”(14). Oneparticipantdescribed too much. [...] Everybody collapses on this one
bothpromptengineeringandpreviousworkonfea- approach [...] everything else gets abandoned.”
tureengineeringas“psuedoscience[...] justpoking (19). Anotherparticipantdescribedpeersfromlin-
atthemodel”(6). Thecurrentflurryofprompting guistics departments who left NLP because they
workwasviewedbyseveralparticipantsaslower- feltalienatedbythefocusonmachinelearning.
statusworkexploitingaknownmethod.
Issueswithpeerreview Somefeltthatpeerre-
“Era of scale” For participants who discussed view was inherently biased toward incremental
larger-scale cycles, pre-trained models were fre- workbecausepeerreviewersareinvestedinthesuc-
quentlyidentifiedasthemostrecentmethodologi- cessofthecurrentmethodologicaltrends,withone
calshift. Participantsdisagreedonwhetherscaling participantarguingthat“ifyouwanttobreakthe
uppre-trainedmodels(intermsofparametercount, paradigmanddosomethingdifferent,you’regonna
trainingtime,and/orpre-trainingdata)wasaform getbadreviews,andthat’sfatalthesedays”(21).
ofexploitingorexploringthismethod. Somepar- Farmorecommonly,participantsdidnotexpress
ticipants found current approaches to scale to be inherentoppositiontopeerreviewbutraisedcon-
“areliablerecipewherewe,whenweputmorere- cernsbecauseoftherecentexpansionofthefield,
sourcesin, weget[...] moreusefulbehaviorand withoneseniorindustryresearcherremarkingthat
capabilitiesout”(4)andrelativelyeasytoperform: peerreviewersarenowprimarilyjuniorresearchers
“OnceyouhavethatGPU[...] it’slike,supersim- who“have not seen the effort that went into [ear-
ple” (5). This perception of scaling up as both lier] papers” (12). Another participant asserted
highlikelihoodofsuccessandlowdifficultyplaces that“mypeersneverreviewmypapers”(22). Par-
itasexploitwork,andresearcherswhodescribed ticipants additionally suggested that the pressure
scale in this way tended to view it as exploiting on junior researchers to publish more causes an
“obvious”trends. Oneresearcherdescribedscale accelerationinthepaceofresearchandreinforce-
asawayofestablishingwhatispossiblebut“ac- ment of current norms, as research that is farther
tuallyabadwaytoachieveourgoals.” (4), with fromcurrentnorms/methodologiesrequireshigher
further(explore-wave)worknecessarytofindeffi- upfronttimeinvestment.
cientwaystoachievethesameperformance. This competitiveness can manifest in harsher
A minority of participants argued that, while reviews, and one participant described a “deadly
historical efforts to scale models or extract large combination”(19)ofhigherstandardsforpapers
noisycorporafromtheinternetwereexploitwork, and lower quality of reviews. Some participants
current efforts to scale are different, displaying described this as a reason they were choosing to
“emergenceinadditiontoscale,whereaspreviously engagelesswithNLPconferences;oneindustryre-
wejustsaw[...] diminishingreturns”(22). Some searcherstatedthat“Ijustfinditdifficulttopublish
participantsalsoemphasizedtheengineeringwork papersin*CLthathaveideasinthem.” (22).
required to scale models, saying that some were
4 Benchmarkingculture
“underestimatingtheamountofworkthatgoesinto
trainingalargemodel”(8)andidentifyingpeople
4.1 Theriseofbenchmarks
orengineeringteamsasamajorresourcenecessary
toperformscalingwork. Theseparticipantswho Senior and emeritus faculty shared a consistent
describedscalingworkasproducingsurprisingre- recollection of the ACL community before the
sultsandbeinghigherdifficultyalsodescribedscal- prominenceofbenchmarksascentralizedaround
ingashigherstatus,moreexploratorywork. a few US institutions and characterized by “pa-
tientmoney”(21): fundingfromDARPAthatdid
“Deep learning monoculture” There was a notrequireanydeliverablesorstatementsofwork.
sensefromseveralparticipantsthatthecurrentcy- Capabilitiesinlanguagetechnologieswereshow-
clehaschangedthefieldmorethanpreviousones, casedwithtechnical“toy”(26,19)demonstrations
thatwereevaluatedqualitatively: “theperformance reallyhelpedthefieldtomoveforward.” (2). Other
metrics were, ‘Oh my God it does that? No ma- participantssimilarlyarguedthatacultureofquan-
chine ever did that before.’ ” (21). Participants titativemeasurementwaskeyformovingonfrom
repeatedly mentioned how small the community techniquesthatwereappealingfortheir“elegance”
was;atconferences,“everybodykneweachother. (14)butempiricallyunderperforming.
Everybodywasconversing,inalltheissues”(26).
The field was described as “higher trust” (22), 4.2 Thecurrentstateofbenchmarks
withsocialmediationofresearchquality–ableto
Roughly twenty years on from the establishment
functionintheabsenceofstandardizationbecause
ofbenchmarksasafield-widepriority,ourpartici-
ofthestronginterconnectednessofthecommunity.
pants’attitudestowardsbenchmarkshadbecome
Many participants recalled the rise of bench-
significantly morecomplex. Many of ourpartici-
marksinthelate1990sandearly2000s,coinciding
pantsstillfoundbenchmarksnecessary,butnearly
with a major expansion in the NLP community
allofthemfoundthemincreasinglyinsufficient.
in the wake of the “statistical revolution,” where
participantsdescribedstatisticalmodelsdisplacing
Misaligned incentives Many participants, par-
more traditional rules-based work (see Figure 2).
ticular early- and late-career faculty, argued that
Inthewordsofoneparticipant,thefieldbecameof
thefieldincentivizestheproductionofbenchmark
“suchabigsnowballingsizethatnobodyownedthe
resultstotheexclusionofallelse: “thetypicalre-
firsteversanymore.” (26). Instead,aftertherelease
searchpaper...theirimmediategoalhastobetoget
of the Penn Treebank in 1993 and the reporting
another2%andgettheboldfaceblackentryonthe
ofinitialresultsonthedataset,“theclimbstarted”
table.” (21). Forourparticipants,improvements
(25) to increase performance. Some participants
onbenchmarksinNLParetheonlyresultsthat
attributed these changes to an influx of methods
areself-justifyingtoreviewers. Someparticipants
frommachinelearningandstatistics,whileothers
felt this encourages researchers to exploit model-
describedthemasmethodstounderstandandorga-
ing tricks to get state-of-the-art results on bench-
nizeprogresswhendoingthiscoordinationthrough
marks,ratherthanexplorethedeepermechanisms
one’ssocialnetworkwasnolongerfeasible.
bywhichmodelsfunction(see§3).
Overtime,thisfocusonmetricsseemstohave
overtakentherestofthefield,inpartthroughthe
“We’re solving NLP” Some participants per-
operationalizationofmetricsasakeyconditionof
ceiveadegradationinthevalueofbenchmarksbe-
DARPAfunding. Oneparticipantcreditedthisto
causeofthestrengthofnewermodels. Participants
AnthonyTether,whobecamedirectorofDARPA
appreciated both the increased diversity and fre-
in2001: theydescribedearlierDARPAgrantsas
quencyofnewbenchmarkintroduction,butnoted
funding “the crazy [...] stuff that just might be
thatthetimefornewapproachestoreach“superhu-
a breakthrough” (21) and DARPA under Tether
man”(6,22)levelsofperformanceonanyspecific
as“showmethemetrics. We’regoingtorunthese
benchmark is shortening. One common compar-
metricseveryyear.” (21).3
ison was between part of speech tagging (“a hill
Someparticipantsmournedtheriskappetiteofa
that was climbed for [...] about 20 years” (15))
culturethatprioritized“first-evers,”criticizingthe
andmostmodernbenchmarks(“solved”withina
lackoffundingforideasthatdidnotimmediately
few years, or even months). Some went further,
performwellatevaluations(notablyleadingtothe
describing“solvingNLP”(8)ornaming2020as
recessionofneuralnetworksuntil2011). However,
thetimewhen“classificationwassolved”(15).
there was sharp disagreement here; many other
However,whenparticipantswereaskedforclar-
participantscelebratedtheintroductionofbench-
ification on what it meant to “solve” a problem,
marks,withonestatingthatcomparingresultson
most participants hedged in similar ways; that
benchmarksbetweenmethods“reallybroughtpeo-
datasets and benchmarks could be solved, with
ple together to exchange ideas. [...] I think this
the correct scoping, but problems could rarely or
3Other participants named DARPA program managers neverbesolved. Manyparticipantsarguedthatthe
CharlesWayneandJ.AllenSearsasadditionalkeyplayers. standardforsolvingataskshouldbehumanequiv-
ArecenttributetoWayne(Church,2018)providesadditional
alency,andthatthiswasnotpossiblewithoutnew
contextreflectingonDARPA’sshiftinprioritiesinthemid-
1980s. benchmarks,metrics,ortaskdefinition.
NLPinthewild Someparticipantsarguedthat
Mentions of software libraries by year
many benchmarks reflect tasks that “aren’t that 0.30
HuggingFace
usefulin the world”(13), and thatthishasled to PyTorch
0.25 Theano
asituationwhere“[NLP],afieldthat,likefunda- DyNet
mentally,isaboutsomethingaboutpeople,knows 0.20 Moses
Tensorflow
remarkablylittleaboutpeople”(3). Industrypar-
0.15
ticipantsoftenviewedthisasadistinctionbetween
theirworkandtheacademiccommunity,withone 0.10
statingthat“mostoftheacademicbenchmarksout
0.05
there are not real tasks” (12). Many academics
articulatedadesireformorehuman-centeredNLP, 0.00
2010 2012 2014 2016 2018 2020 2022
and most participants described feeling pressure Year
over the unprecedented level of outside interest Figure3: MentionsoflibrariesovertimeintheACL
Anthology. Notethecyclicpatternandincreasingcon-
in the field. One participant contrasted the inter-
centrationonthedominantframeworkovertime. While
nationalattentiononChatGPTwiththevisibility
somelibrariesarebuiltonothers,theshiftinmentions
of earlier NLP work: “It’s not like anyone ever
overtimecapturestheprimarylevelofabstractionthat
wenttolikeparser.yahoo.comtorunaparseron
researchersconsiderimportant. SeeappendixBforde-
something”(3). Participantsarguedthat,giventhis tailsonhowwehandleambiguityinmentions.
outsideattention,thebenchmarkfocusofNLPis
toonarrow,thatbenchmarksfailtocaptureno- simpler to manage, simpler to train” (17). Previ-
tionsoflanguageunderstandingthattranslate ously, participants described spending “like 90%
towideraudiences,andthatweshouldmoveon
ofourtimere-implementingpapers”(12);asmore
frombenchmarksnotwhentheyaresaturatedbut papersbeganreleasingcodeimplementedinpop-
when“itwouldn’treallyimprovetheworldtoim- ularframeworks,thecostofusingthosemethods
provethisperformanceanymore”(9). Thisechoed asbaselinesdecreased. Oneparticipantstatedthat
“thingsthatsoftwaremakeseasy,peoplearegoing
a common refrain: many participants, especially
early-andmid-careerresearchers,sawpositiveso- to do” (18); this further compounds centraliza-
cialchangeasagoalofprogressinNLP. tionontothemostpopularlibraries,withlittle
incentive to stray from the mainstream: “ev-
5 Softwarelotteries erybodyusesPyTorch,sonowIusePyTorchtoo”
(8);“wejustuseHuggingFaceforprettymuchev-
Hooker (2021) argues that machine learning re-
erything” (18).4 Figure 3 visualizes mentions of
search has been shaped by a hardware lottery:
frameworksacrosspapersintheACLAnthology,
anidea’ssuccessispartiallytiedtoitssuitability
showing both the rise and fall in their popularity.
foravailablehardware. Severalparticipantsspoke
The rising peaks of popularity reflect the central-
aboutsoftwareinwaysthatindicateananalogous
izationovertime. Whilesomecommunitieswithin
softwarelotteryinNLPresearch: asthecommunity
NLP had previously seen some centralization on
centralizesinitssoftwareuse,italsocentralizesin
toolkits (e.g. machine translation’s use of Moses
methodology,withresearchers’choicesofmethods
(Koehnetal.,2007)),thecurrentcentralizationis
influencedbyrelativeeaseofimplementation. This
transcendssubfields.
appearedtobearelativelynewphenomenon;partic-
ipantsdescribedpreviouslyusingcustom-designed Centralizationonspecificmodels Participants
softwarefromtheirownresearchgroup,orwriting identified another shift after the release of BERT
codefromscratchforeachnewproject. andsubsequentdevelopmentofHuggingFace. Be-
cause of pre-training, participants moved from
Centralization on frameworks As deep learn-
merely using the same libraries to “everyone
ingbecamemorepopularinNLP,participantsde-
us[ing] the same base models” (9). Participants
scribed the landscape shifting. As TensorFlow
expressedconcernthatthisledtofurthercentraliza-
(Abadi et al., 2015) increased support for NLP
modeling, PyTorch (Paszke et al., 2019) was re- 4Inthissection,weprimarilydiscussopensourceframe-
leased, along with NLP-specific toolkits such as workscommonlyusedbyacademicandindustryresearchers.
However,manyofourparticipantsworkinginindustryalso
DyNet(Neubigetal.,2017)andAllenNLP(Gard-
describeaffordancesandconstraintsofinternaltoolkitsthat
neretal.,2018),and“everythingstartedbeing[...] areusedattheirrespectivecompanies.
noitnem
taht
srepap
fo
noitcarF
tioninthecommunity,withoneparticipantidenti- citing the lack of well-supported NLP toolkits or
fyingatrendofsomepeople“notus[ing]anything community use in other languages. This is an in-
else than BERT [...] that’s a problem” (5). This stanceofasoftwarelotteryatahigherlevel,where
concentrationaroundparticularmodelingchoices thedominanceofasingleprogramminglanguage
has reached greater heights than any previous hassnowballedwiththecontinueddevelopmentof
concentration on a method; in 2021, 46.7% of researchartifactsinthatlanguage.
papersintheACLanthologycitedBERT(Devlin
5.1 Consequencesofcentralization
etal.(2019);Figure2)5.
Other large models are only available to re- ThisincreasingcentralizationofthemodernNLP
searchersviaanAPI.Oneparticipantwhoworks stackhasseveralconsequences. Oneoftheprimary
onLLMsinindustryarguedthatblack-boxLLMs, ones,however,isthelossofcontrolofdesigndeci-
while “non-scientific” (15) in many ways, were sionsforthemajorityofresearchersinthecommu-
likelarge-scaletechnicaltoolsconstructedinother nity. Practically,researcherscannowchoosefrom
disciplines,drawingaparalleltoparticlephysics: ahandfulofwell-establishedimplementations,but
“computerscienceisgettingtohaveitsCERNmo- onlyhaveaccesstosoftwareandmodelsoncethe
mentnow[...] there’sonlyoneLargeHadronCol- decisionsonhowtobuildthemhavealreadybeen
lider, right?” (15). This participant argued that reifiedinwaysthataredifficulttochange.
NLP has become a field whose frontiers require
Lowerbarriers Beyondtimesaved(re-)imple-
toolsthatarebeyondmostorganizations’resources
menting methods, many participants identified a
and capabilities to construct, but nonetheless are
lowerbarriertoentryintothefieldasanotableben-
widely adopted and set bounding parameters for
efitofcentralizationonspecificsoftwareinfrastruc-
futureresearchastheyseewideadoption. Inthis
ture. Participantsdescribedstudentsgettingstateof
vision, black-boxLLMstakeonthesameroleas
theartresultswithinanhouroftacklingaproblem;
theLHCortheHubbleSpaceTelescope(bothno-
seeing the average startup time for new students
tably public endeavors, unlike most LLMs), as
decreasing from six months to a few weeks; and
toolswhosespecificationsaredecidedonbyafew
teachingstudentswithnocomputerscienceback-
well-resourcedorganizationswhoshapesignificant
groundtobuildNLPapplicationswithprompting.
partsofthefield. Butmostparticipantsexpressed
skepticism about the scientific validity of experi- Obscuringwhat’s“underthehood” Onepar-
ments on black-box LLMs, with one participant ticipant recalled trying to convince their earlier
referencingcritiquesofearly-2000sIRresearchon students to implement things from scratch in or-
Google(Kilgarriff,2007). dertounderstandallthedetailsofthemethod,but
nolongerdoingsobecause“Idon’tthinkit’spos-
Centralization on Python While most early- sible [...] it’s just too complicated” (11); others
careerandlate-careerparticipantsdidnotexpress attributedthistospeedmorethancomplexity,stat-
strong opinions about programming languages, ingthat“thepaceissofastthatthereisnotimeto
manymid-careerparticipantsexpressedstrongdis- properlydocument,thereisnotimetoproperlyen-
like for Python, describing it as “a horrible lan- gagewiththiscode,you’rejustusingthemdirectly”
guage” (22) with efficiency issues that are “an (5). However, this can cause issues on an opera-
impediment to us getting things done” (20) and tionallevel;severalparticipantsrecalledinstances
datastructureimplementationsthatare“acomplete whereabugorpoordocumentationofsharedsoft-
disasterintermsofmemory”(9). Oneparticipant waretoolsresultedininvalidresearchresults. One
describedtheiridealnextparadigmshiftinthefield participantdescribedusingawidelysharedpieceof
asashiftawayfromusingPythonforNLP. evaluationcodethatmadeanunstatedassumption
Yeteventheparticipantswhomostvehementlyop- abouttheinputdataformat,leadingto“massively
posed Python used it for most of their research, inflated evaluation numbers” (3) on a well-cited
dataset. Anotherparticipantdescribedworkingon
5While not every paper that cites BERT uses a BERT
apaperwheretheyrealized,anhourbeforethepa-
model,thisindicateshowcentralBERTisbothasamodel
andasaframeforthediscussionofotherwork.Forcompari- perdeadline,thatthestudentauthorshadusedtwo
son,onlytwootherpapershavebeencitedbymorethan20% differenttokenizersinthepipelinebymistake: “we
ofanthologypapersinasingleyear: “AttentionisAllYou
decidedthatwell,theresultswerestillvalid,and
Need”(Vaswanietal.,2017)with27%in2021andGloVe
(Penningtonetal.,2014)with21%in2019. theresultswouldonlygetbetterif[itwasfixed]...so
thepaperwentout. Itwaspublishedthatway.” (26) of reach for academics, echoing concerns previ-
Softwarebugsinresearchcodearenotanewprob- ously described by Ahmed and Wahed (2020).
lem,6 butparticipantsdescribedbugsintoolkitsas Participantsworriedthat“wearebuildinganup-
difficult to diagnose because they “trust that the perclassofAI”(6)wheremostresearchersmust
libraryiscorrectmostofthetime”(8),evenasthey “buildontopof[largemodels]”(15)thattheycan-
spokeoffinding“many,many,many”(8)bugsin not replicate, though others expressed optimism
toolkitsincludingHuggingFaceandPyTorch. that “clever people who are motivated to solve
these problems” (22) will develop new efficient
Software is implicit funding Participants sug-
methods(Bartoldsonetal.,2023). Industrypartici-
gested that tools that win the software lottery act
pantsfromlargetechcompaniesalsofeltresource-
asasortofimplicitfunding: theyenableresearch
constrained: “modernmachinelearningexpands
groupstoconductworkthatwouldnotbepossible
tofittheavailablecompute.” (4).
inthetools’absence,andmanyofourparticipants
assertedthatthescopeoftheirprojectsexpanded
6 RelatedWork
asaresult. However,theyalsosignificantlyraise
the relative cost of doing research that does not The shifts we explore in this paper have not hap-
fallneatlyintoexistingtools’purview. Asonepar- penedinavacuum,withadjacentresearchcommu-
ticipantstated,“You’renotgonnajustbuildyour nitiessuchascomputervision(CV)andmachine
ownsystemthat’sgonnacompeteonthesemajor learning(ML)experiencingsimilarphenomena,in-
benchmarksyourself. Youhavetostart[with]the spiringanumberofrecentpapersdiscussingnorms
infrastructurethatisthere”(19). Thissetofincen- inAImorebroadly. Birhaneetal.(2022)analyze
tivespushesresearcherstofollowcurrentmethod- asetofthemosthighlycitedpapersatrecentML
ologicalpractice,andsomeparticipantsfearedthis conferences,findingthattheyeschewdiscussionof
ledtowardmoreincrementalwork. societalneedandnegativepotential,insteadempha-
sizingalimitedsetofvaluesbenefitingrelatively
5.2 ImpactonReproducibility
few entities. Others have noticed that corporate
Acommonsentimentamongparticipantswasthat interestshaveplayedanincreasingroleinshaping
centralizationhashadanoverallpositiveimpacton research,andquantifiedthiswithstudiesofauthor
reproducibility,becauseusingsharedtoolsmakes affiliationsovertimeinmachinelearning(Ahmed
iteasiertoevaluateanduseothers’researchcode. andWahed,2020)andNLP(Abdallaetal.,2023).
However,participantsalsoexpressedconcernsthat Su and Crandall (2021) study the tangible emo-
theincreasingsecrecyofindustryresearchcompli- tionalimpactofrecentdramaticgrowthintheCV
catesthatoverallnarrative: “thingsaremoreopen, communitybyaskingcommunitymemberstowrite
reproducible... except for those tech companies storiesaboutemotionaleventstheyexperiencedas
whosharenothing”(14). membersoftheirresearchcommunity.
Whilewefocusonsummarizingandsynthesiz-
Shiftsinexpectations Oneparticipantdescribed
ingtheviewsofourparticipants,someoftheover-
a general shift in focus to “making sure that you
arching themes identified in this work have been
makeclaimsthataresupportedratherthanrepro-
discussed more critically. Fishman and Hancox-
ducingpriorworkexactly”(12)inordertomatch
Li(2022)critiquetheunificationofMLresearch
reviewers’shiftingexpectations. However,partic-
aroundtransformermodelsonbothepistemicand
ipantsalsofeltthattheexpectationsforbaselines
ethicalgrounds. Positionpapershavecritiquedthe
hadincreased: “[inthepast,]everybodyknewthat
notionofgeneralpurposebenchmarksforAI(Raji
the Google system was better because they were
et al., 2021), and emphasized the importance of
running on the entire Internet. But like that was
morecarefulanddeliberatedatacurationinNLP
not a requirement [to] match Google’s accuracy.
(Rogers,2021;BowmanandDahl,2021).
Butnowitis,right?” (8).
The NLP Community Metasurvey (Michael
Disparities in compute access Many felt that et al., 2022) provides a complementary view to
buildinglarge-scalesystemswasincreasinglyout thiswork,withtheirsurveyelicitingopinionsfrom
abroadswathofthe*CLcommunityonasetof32
6Tambonetal.(2023)describesilentbugsinpopulardeep
controversial statements related to the field. The
learningframeworksthatescapenoticeduetoundetectederror
propagation. surveyalsoaskedrespondentstoguessatwhatthe
mostpopularbeliefswouldbe,elicitingsociologi- is not the first period of flux in the field, nor are
calbeliefsaboutthecommunity. Whilethereisno thefundamentalforcesenablingLLMs’dominance
directoverlapbetweenourquestionsandMetasur- andotherchangesentirelynew.
veyquestions,participantsraisedthetopicsofscal-
ingup,benchmarkingculture,anonymouspeerre- Our participants described cycles of change
view,andtheroleofindustryresearch,whichwere in the NLP community from mid-80s to the
the subject of Metasurvey questions. Where we present, with common themes of first exploiting
canmapbetweenourthematicanalysisandMeta- andthenexploringpromisingmethodologies. Each
surveyquestions,weseeagreement–e.g. manyof methodological shift brought corresponding cul-
ourparticipantsdiscussedothersvaluingscale,but turalchange: theshiftfromsymbolictostatistical
fewplacedhighvaluethemselvesonscalingupas methodsbroughtabouttheriseofbenchmarkcul-
aresearchcontribution. ture and the end of the socially mediated, small-
TheavailabilityoftheACLAnthologyhasen- network ACL community. Neural methods be-
abledquantitativestudiesofourcommunityviapat- ganthecentralizationonsoftwaretoolkitsandthe
ternsofcitation,authorship,andlanguageuseover methodologiestheysupport. Pre-trainingintensi-
time. Andersonetal.(2012)performatopicmodel fied this software lottery, causing unprecedented
analysis over the Anthology to identify different levelsofcentralizationonindividualmethodsand
eras of research and better understand how they models. Currentmodelshavecalledintoquestion
developovertime,andanalyzefactorsleadingau- thevalueofbenchmarksandcatapultedNLPinto
thorstojoinandleavethecommunity. Mohammad the public eye. Our participants largely agree on
(2020)analyzecitationpatternsin*CLconferences theresultingincentives–tobeatbenchmarkresults,
acrossresearchtopicsandpapertypes,andSingh todotheeasiestthingratherthanthemostfulfilling,
etal.(2023)specificallyinspectthephenomenon to produce work faster and faster – while largely
whereinmorerecentpapersarelesslikelytocite expressingfrustrationwiththeconsequences.
olderwork. Pramanicketal.(2023)provideaview
ofparadigmshiftsintheNLPcommunitycomple- We hope that this contextualization of the cur-
mentarytooursbasedonadiachronicanalysisof rentstateofNLPwillbothservetoinformnewer
theACLAnthology,inferringcausallinksbetween membersofthecommunityandstirinformeddis-
datasets,methods,tasksandmetrics. cussion on the condition of the field. While we
Shifts in norms and methods in science more donotprescribespecificsolutions,sometopicsof
broadlyhasbeenstudiedoutsidecomputing-related discussionemergefromthethemesofthiswork:
fields. Mostnotably,Kuhn(1970)coinedtheterm
• Whoholdsthepowertoshapethefield? How
paradigmshiftinTheStructureofScientificRevo-
canabroadrangeofvoicesbeheard?
lutions. Histheoryofthecyclicprocessofscience
• Dotheincentivesinplaceencouragethebe-
overdecadesorcenturieshassomeparallelswith
havior we would like to see? How can we
the(shortertimescale)exploit-explorecyclesdis-
improvereviewingtoalignwithourvalues?
cussed in this work. Note that in this work, we
• What affects the ability to do longer-term
didnotprimeparticipantswithanaprioridefini-
workthatmaydeviatefromcurrentnorms?
tionofparadigmshift,allowingeachparticipantto
• Howcanthecommunityarriveatanactively
engagewiththetermaccordingtotheirowninter-
mediatedconsensus,ratherthanpassivelybe-
pretation,whichoftendifferedfromKuhn’snotion
ingshapedbyforcesliketheoneswediscuss?
ofaparadigmshift.
7 TheFuture We personally take great hope for our commu-
nity from this project. The care with which all
The rise of large language models has coincided participantsreflectedontheshapeofthefieldsug-
withdisruptivechangeinNLP:acceleratingcentral- geststousthatmanypeopleareconcernedabout
izationofsoftwareandmethodologies,questioning theseissues,investedinthecommunity,andhope-
ofthevalueofbenchmarks,unprecedentedpublic ful for the future. By sharing publicly what peo-
scrutiny of the field, and dramatic growth of the plesothoughtfullyarticulateprivately,wehopeto
community. Ashiftlikethiscanfeelthreateningto promptfurtherdiscussionofwhatthecommunity
thefundamentalnatureofNLPresearch, butthis candotobuildourfuture.
Limitations earlycareerresearchersandthosewhohavenotyet
publishedbutareinterestedinNLPresearch.
Westernbias Themostnotablyirrepresentative
samplingbiasinourparticipantpoolisthelackof
non-Westerninstitutionalaffiliation(andthestrong EthicsStatement
skew toward North American affiliations). This
biashasarisenlikelyinpartduetoourowninstitu- FollowingInstitutionalReviewBoardrecommen-
tionalaffiliationandconceptionsofthecommunity. dations, we take steps to preserve the anonymity
Thatbeingsaid,giventheAssociationforCompu- ofourparticipants, includingaggregatingorgen-
tationalLinguistics’historicallyUS-andEnglish- eralizingacrossdemographicinformation, avoid-
centric skews, this allows us to gather historical ing the typical practice of providing a table of
perspectives. Additionally,consideringthatWest- per-intervieweedemographics,usingdiscretionto
erninstitutionsconstituteacitationnetworklargely redactornotreportquotesthatmaybeidentifying,
distinctfromAsiannetworks(Rungtaetal.,2022), andrandomizingparticipantnumbers. Participants
webelievethatoursampleallowsustotellarich consented to the interview and to being quoted
and thorough story of factors which have shaped anonymouslyinthiswork. Thisworkunderwent
theWesternNLPresearchcommunity,whichboth additionalIRBscreeningforinterviewingpartici-
informsandisinformedbyothercommunitiesof pantsinGDPR-protectedzones.
NLPresearchers.
Weviewourworkashavingpotentialforposi-
Lackofearlycareervoices Ourinclusioncrite- tiveimpactontheACLcommunity,asweprompt
riaforourparticipants–threeormorepublications itsmemberstoengageinactivereflection. Webe-
in*CL,IR,orspeechvenues7–necessarilymeans lieve that, given recent developments in the field
thatwehavelimitedperspectivesonandfrommore andtheco-occuringexternalscrutiny,thecurrent
junior NLP researchers (such as junior graduate momentisaparticularlyappropriatetimeforsuch
students), those hoping to conduct NLP research reflection. Additionally,wehopethatourworkcan
in the future, those who have engaged with NLP servethosecurrentlyexternaltothecommunityas
research in the past and decided not to continue anaccessible,human-centeredsurveyofthefield
beforedevelopingapublicationrecord,andthose andfactorsthathaveshapeditoverthedecades,pri-
whohaveconsciouslydecidednottoengagewith oritizingsharingofanecdotesandotherin-group
NLPresearchinthefirstplace. Ingeneral,although knowledge that may be difficult for outsiders to
wegatheredperspectivesfromparticipantsacross learnaboutotherwise.
avarietyofdemographicbackgrounds,ourpartic-
ipants represent those who have been successful
Acknowledgements
andpersistedinthefield. Thisisespeciallytruefor
our participants in academia; of our participants’
This work would not have been possible without
currentacademicaffiliations,onlyR1institutions
ourparticipants,whoweregenerouswiththeirtime
(if in the US) and institutions of comparable re-
andengageddeeplywiththismaterial. Wewould
search output (if outside the US) are represented.
additionally like to thank Jill Fain Lehman, An-
We therefore may be missing perspectives from
jalie Field, Dawn Nafus, Momin M. Malik, Gra-
certaingroupsofresearchers,includingthosewho
hamNeubigandtheanonymousreviewersfortheir
primarily engage with undergraduate students or
helpfulcomments.
face more limited resource constraints than most
oftheacademicfacultyweinterviewed. This work was supported in part by a grant
Future research could further examine differ- from the National Science Foundation Graduate
encesbetweengeographicsubcommunitiesinNLP Research Fellowship Program under Grant No.
andmorecloselyexamineinfluencesonpeople’s DGE2140739. Widder gratefully acknowledges
participationinanddisengagementfromthecom- thesupportoftheDigitalLifeInitiativeatCornell
munity. Additionally, we leave to future work a Tech. Anyopinions,findings,andconclusionsor
moreintentionalexplorationofperspectivesfrom recommendations expressed in this material are
thoseoftheauthorsanddonotnecessarilyreflect
7Inordertocaptureperspectivesofthecommunitychang-
theviewsofthesponsors.
ingovertime,andtoselectforpeoplewhoarepartofthese
communities.
References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
MartínAbadi,AshishAgarwal,PaulBarham,Eugene
deepbidirectionaltransformersforlanguageunder-
Brevdo,ZhifengChen,CraigCitro,GregS.Corrado,
standing. InProceedingsofthe2019Conferenceof
AndyDavis,JeffreyDean,MatthieuDevin,Sanjay
theNorthAmericanChapteroftheAssociationfor
Ghemawat,IanGoodfellow,AndrewHarp,Geoffrey
ComputationalLinguistics: HumanLanguageTech-
Irving,MichaelIsard,YangqingJia,RafalJozefow-
nologies,Volume1(LongandShortPapers),pages
icz,LukaszKaiser,ManjunathKudlur,JoshLeven-
4171–4186,Minneapolis,Minnesota.Associationfor
berg,DandelionMané,RajatMonga,SherryMoore,
ComputationalLinguistics.
DerekMurray,ChrisOlah,MikeSchuster,Jonathon
Shlens,BenoitSteiner,IlyaSutskever,KunalTalwar, Nic Fishman and Leif Hancox-Li. 2022. Should at-
PaulTucker,VincentVanhoucke,VijayVasudevan, tention be all we need? the epistemic and ethical
FernandaViégas,OriolVinyals,PeteWarden,Mar- implicationsofunificationinmachinelearning. In
tinWattenberg,MartinWicke,YuanYu,andXiao- 2022ACMConferenceonFairness,Accountability,
qiang Zheng. 2015. TensorFlow: Large-scale ma- andTransparency,pages1516–1527.
chinelearningonheterogeneoussystems. Software
availablefromtensorflow.org. Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord,PradeepDasigi,NelsonF.Liu,MatthewPe-
MohamedAbdalla,JanPhilipWahle,TerryRuas,Au- ters,MichaelSchmitz,andLukeZettlemoyer.2018.
rélieNévéol,FannyDucel,SaifM.Mohammad,and AllenNLP: A deep semantic natural language pro-
KarënFort.2023. Theelephantintheroom: Ana- cessing platform. In Proceedings of Workshop for
lyzingthepresenceofbigtechinnaturallanguage NLPOpenSourceSoftware(NLP-OSS),pages1–6,
processingresearch. Melbourne,Australia.AssociationforComputational
Linguistics.
Nur Ahmed and Muntasir Wahed. 2020. The de-
democratizationofai:Deeplearningandthecompute BarneyGGlaser, JudithHolton, etal.2004. Remod-
divideinartificialintelligenceresearch. elinggroundedtheory. InForumqualitativesozial-
forschung/forum: qualitative social research, vol-
Ashton Anderson, Dan Jurafsky, and Daniel A. Mc-
ume5.
Farland.2012. Towardsacomputationalhistoryof
the ACL: 1980-2008. In Proceedings of the ACL- SaraHooker.2021. Thehardwarelottery. Communica-
2012SpecialWorkshoponRediscovering50Yearsof tionsoftheACM,64(12):58–65.
Discoveries,pages13–21,JejuIsland,Korea.Asso-
ciationforComputationalLinguistics. AdamKilgarriff.2007. Lastwords:Googleologyisbad
science. ComputationalLinguistics,33(1):147–151.
Brian R. Bartoldson, Bhavya Kailkhura, and Davis
Blalock.2023. Compute-efficientdeeplearning: Al- Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
gorithmictrendsandopportunities. JournalofMa- Callison-Burch,MarcelloFederico,NicolaBertoldi,
chineLearningResearch,24(122):1–77. Brooke Cowan, Wade Shen, Christine Moran,
RichardZens,ChrisDyer,OndˇrejBojar,Alexandra
AbebaBirhane,PratyushaKalluri,DallasCard,William Constantin, and Evan Herbst. 2007. Moses: Open
Agnew,RavitDotan,andMichelleBao.2022. The sourcetoolkitforstatisticalmachinetranslation. In
valuesencodedinmachinelearningresearch. InPro- Proceedings of the45th Annual Meeting of the As-
ceedingsofthe2022ACMConferenceonFairness, sociationforComputationalLinguisticsCompanion
Accountability,andTransparency,FAccT’22,page VolumeProceedingsoftheDemoandPosterSessions,
173–184,NewYork,NY,USA.AssociationforCom- pages177–180,Prague,CzechRepublic.Association
putingMachinery. forComputationalLinguistics.
SamuelR.BowmanandGeorgeDahl.2021. Whatwill ThomasS.Kuhn.1970. Thestructureofscientificrevo-
ittaketofixbenchmarkinginnaturallanguageunder- lutions. UniversityofChicagoPress,Chicago.
standing? InProceedingsofthe2021Conferenceof
theNorthAmericanChapteroftheAssociationfor Q.VeraLiaoandJenniferWortmanVaughan.2023. Ai
ComputationalLinguistics: HumanLanguageTech- transparencyintheageofllms: Ahuman-centered
nologies,pages4843–4855,Online.Associationfor researchroadmap.
ComputationalLinguistics.
YvonnaSLincoln,SusanALynham,andEgonGGuba.
Steve Campbell, Melanie Greenwood, Sarah Prior, 2011. Paradigmatic controversies, contradictions,
Toniele Shearer, Kerrie Walkem, Sarah Young, andemergingconfluences,revisited. TheSagehand-
DanielleBywaters, andKimWalker.2020. Purpo- bookofqualitativeresearch,4:97–128.
sive sampling: complex or simple? research case
examples. JournalofresearchinNursing,25(8):652– KyleLo,LucyLuWang,MarkNeumann,RodneyKin-
661. ney,andDanielWeld.2020. S2ORC:Thesemantic
scholaropenresearchcorpus. InProceedingsofthe
KennethWardChurch.2018. Emergingtrends: Atrib- 58thAnnualMeetingoftheAssociationforCompu-
utetocharleswayne. NaturalLanguageEngineering, tationalLinguistics,pages4969–4983,Online.Asso-
24(1):155–160. ciationforComputationalLinguistics.
Julian Michael, Ari Holtzman, Alicia Parrish, Aaron pages2182–2194,Online.AssociationforComputa-
Mueller, Alex Wang, Angelica Chen, Divyam tionalLinguistics.
Madaan,NikitaNangia,RichardYuanzhePang,Ja-
sonPhang,andSamuelR.Bowman.2022. Whatdo AnnaRogers.2023. Closedaimodelsmakebadbase-
nlpresearchersbelieve? resultsofthenlpcommunity lines.
metasurvey.
MukundRungta,JanvijaySingh,SaifM.Mohammad,
Matthew B Miles and A Michael Huberman. 1994. and Diyi Yang. 2022. Geographic citation gaps in
Qualitativedataanalysis: Anexpandedsourcebook. NLPresearch. InProceedingsofthe2022Confer-
sage. enceonEmpiricalMethodsinNaturalLanguagePro-
cessing,pages1371–1383,AbuDhabi,UnitedArab
SaifM.Mohammad.2020. Examiningcitationsofnat- Emirates.AssociationforComputationalLinguistics.
urallanguageprocessingliterature. InProceedings
of the 58th Annual Meeting of the Association for NithyaSambasivan,ShivaniKapania,HannahHighfill,
Computational Linguistics, pages 5199–5209, On- DianaAkrong,PraveenParitosh,andLoraMAroyo.
line.AssociationforComputationalLinguistics. 2021. “everyone wants to do the model work, not
thedatawork”: Datacascadesinhigh-stakesai. In
Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Proceedingsofthe2021CHIConferenceonHuman
Matthews,WaleedAmmar,AntoniosAnastasopou- FactorsinComputingSystems,CHI’21,NewYork,
los,MiguelBallesteros,DavidChiang,DanielCloth- NY,USA.AssociationforComputingMachinery.
iaux, Trevor Cohn, Kevin Duh, Manaal Faruqui,
CynthiaGan,DanGarrette,YangfengJi,Lingpeng JanvijaySingh,MukundRungta,DiyiYang,andSaifM.
Kong, Adhiguna Kuncoro, Gaurav Kumar, Chai- Mohammad.2023. Forgottenknowledge: Examin-
tanyaMalaviya,PaulMichel,YusukeOda,Matthew ingthecitationalamnesiainnlp.
Richardson,NaomiSaphra,SwabhaSwayamdipta,
and Pengcheng Yin. 2017. Dynet: The dy- Irene Solaiman. 2023. The gradient of generative ai
namic neural network toolkit. arXiv preprint release: Methodsandconsiderations.
arXiv:1701.03980.
AnselmStraussandJulietCorbin.1990. Basicsofqual-
CharlieParker,SamScott,andAlistairGeddes.2019. itativeresearch. Sagepublications.
Snowballsampling. SAGEresearchmethodsfounda-
NormanMakotoSuandDavidJ.Crandall.2021. The
tions.
affectivegrowthofcomputervision. InProceedings
Adam Paszke, Sam Gross, Francisco Massa, Adam of the IEEE/CVF Conference on Computer Vision
Lerer, James Bradbury, Gregory Chanan, Trevor andPatternRecognition(CVPR),pages9291–9300.
Killeen, Zeming Lin, Natalia Gimelshein, Luca
RichardSSuttonandAndrewGBarto.2018. Reinforce-
Antiga, Alban Desmaison, Andreas Köpf, Edward
mentlearning: Anintroduction. MITpress.
Yang,ZachDeVito,MartinRaison,AlykhanTejani,
SasankChilamkurthy,BenoitSteiner,LuFang,Junjie
Florian Tambon, Amin Nikanjam, Le An, Foutse
Bai,andSoumithChintala.2019. PyTorch: AnIm-
Khomh, andGiulianoAntoniol.2023. Silentbugs
perativeStyle,High-PerformanceDeepLearningLi-
indeeplearningframeworks: Anempiricalstudyof
brary.CurranAssociatesInc.,RedHook,NY,USA.
kerasandtensorflow.
Jeffrey Pennington, Richard Socher, and Christopher
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Manning. 2014. GloVe: Global vectors for word
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
representation. InProceedingsofthe2014Confer-
Kaiser,andIlliaPolosukhin.2017. Attentionisall
enceonEmpiricalMethodsinNaturalLanguagePro-
youneed. InAdvancesinNeuralInformationPro-
cessing (EMNLP), pages 1532–1543, Doha, Qatar.
cessingSystems,volume30.CurranAssociates,Inc.
AssociationforComputationalLinguistics.
Alex D Wade. 2022. The semantic scholar academic
Aniket Pramanick, Yufang Hou, and Iryna Gurevych.
graph (s2ag). Companion Proceedings of the Web
2023. A diachronic analysis of the nlp research
Conference2022.
paradigmshift: When,how,andwhy?
Samuel Way, Daniel Larremore, and Aaron Clauset.
DeborahRaji,EmilyDenton,EmilyM.Bender,Alex
2016. Gender,productivity,andprestigeincomputer
Hanna, and Amandalynne Paullada. 2021. Ai and
sciencefacultyhiringnetworks.
theeverythinginthewholewideworldbenchmark.
In Proceedings of the Neural Information Process-
RobertSWeiss.1995. Learningfromstrangers:Theart
ingSystemsTrackonDatasetsandBenchmarks,vol-
andmethodofqualitativeinterviewstudies. Simon
ume1.Curran.
andSchuster.
AnnaRogers.2021. Changingtheworldbychanging
the data. In Proceedings of the 59th Annual Meet-
ingoftheAssociationforComputationalLinguistics
andthe11thInternationalJointConferenceonNatu-
ralLanguageProcessing(Volume1: LongPapers),
A DetailsofQualitativeMethodologies
A.1 Participantdemographics
Oftheacademicsinterviewed,therewasanevensplitbetweenearly,mid,andlatecareer(6/33%each).
Of those in industry, 6 (75%) were individual contributors and 2 (25%) were research managers. Our
sampleisonly19%women,whichislikelyrepresentative,aswomencompriseapproximately15%of
tenure-trackcomputersciencefacultyintheUS(Wayetal.,2016). Formorediscussionofthesample
characteristics,see7. Ourpositiveresponseratewas82%.
Demographictrait %ofsample
Inacademia 69%
Women 19%
Minoritizedgroup 27%
BornoutsideUS 38%
CurrentlyworksoutsideUS 4%
Table1: Self-reporteddemographicmakeupofsubjects.
A.2 Consentprotocol
IntervieweeswereaskedforverbalconsentunlesstheywereinaGDPR-protectedregionatthetimeof
theinterview,inwhichcasetheyprovidedwrittenconsentviaDocuSigninstead. Thisconsentscriptwas
IRB-approved.
Hi,thanksfortakingthetimetotalkwithme! MycollaboratorsonthisprojectandIworkfor
CMU.Wecanbereachedat[emails]shouldyouhaveanyquestionsforusduringthestudyor
after.
Thisinterviewwilltakebetween45minutesandanhour. Therewillbenocompensationfor
participation.
Participationisalwaysvoluntary,andyoumayrefusetoparticipateintheresearchstudyorstop
participationatanytime.
You will not be identified in any reports we release from this research. This data will be
deidentified,whichmeansyouwillnotbeidentifiedbynameoranyotherspecificcharacteristic.
We may quote you anonymously. Just in case, though, please do not reveal any private or
personally-identifiableinformationaboutyourselforothersinyouranswertoourquestions.
I’d like to record the audio of this interview as a memory aid. You can ask me to stop the
recording at any time. Only the members of our research team will have access to these
recordings.
Wereallyappreciateyourparticipation,andwehopetopublishthisresearchtoadvanceour
understandingofthefactorsshapingtheNLPresearchcommunity.
Doyouhaveanyquestions? Ifnot,doIhaveyourconsenttoparticipateinthisstudy?
[answeranyquestions;ifconsented,beginrecording]
Alright,I’vestartedtherecording. Justtoconfirm,doIhaveyourconsenttoparticipateinthis
study,andtorecordthisinterview?
A.3 InterviewGuide
These questions are intentionally open-ended, and the interviewers asked non-scripted followups or
additionalquestionswhereappropriate. Overtime,asearlythemesemerged,additionalquestionswere
added,particularlyonfundingandpaceofwork.
1. First,IhaveafewquestionsaboutyourrelationshiptotheNLPcommunity. Youcanbeasspecific
orasvagueasyou’dlikewithyourresponses.
(a) Whatdoyouconsidertobeyourmain/home/primaryresearchcommunity?
(b) Morespecifically,whatvenuesdoyoufollowand/orpublishin?
(c) Whatsubarea(s)orsubfieldsareyoumostactivein?
(d) Isthisdifferentfromwhatyouhaveconsideredyourmaincommunityatotherpointsinyour
career? (Prompt: ifso,whatchanged?)
(e) WhatwouldyoudefineasthestartofyourNLPresearchcareer? (e.g. startofPhD,researchas
anundergrad,etc). (Prompt: Whenwasthis?)
2. I’dliketohearyourthoughtsonwhatthefieldwaslikenearthebeginningofyourcareer.
(a) When you started in your field, what did people generally think were the most promising
directions? (Prompt: doyouagree?)
(b) Howdoyouinterprettheterm“promising”?
(c) Whatdoyouthinktheresearchcommunityprioritizedwhenyoustarted?
(d) Whatwasthescopeoftheworkthatyourresearchgroupdid?
(e) Whatwasyourrelationshiptocomputingresourcesatthestartofyourcareer?
(f) What did your software workflow look like when you first started doing research? (Prompt:
Whattools,frameworks,librariesdidyouuse?)
(g) Wheredidfundingforyourworkcomefrom? (Prompt: whatwerethemajorcostsinvolved
withyourresearch?)
Now,I’dliketocomparethiswiththecurrentstateofthefield.
(a) Whatdoyouthinkothersinyourfieldwouldsayarethemostpromisingdirections? (Prompt:
doyouagree?)
(b) Whatdoyouthinktheresearchcommunityprioritizesnow?
(c) Whatisthescopeoftheworkthatyourresearchgroupdoes?
(d) Howdocomputingresourcesaffectyourgroup’sworknow?
(e) Howdoesthesoftwareworkflowlooklikeforyouoryourstudentsnow?
(f) Whattools,frameworks,librariesdoyouoryourstudentsuse?
(g) Havethesetools,frameworks,andlibrariesmadeanimpactonyour(oryourstudents’)research?
(h) Whatimpactsdoyouthinkthesetools,frameworks,andlibrarieshavemadeonyourcommu-
nity’sresearch?
(i) Wheredoesthefundingforyourworkcomefrom? (Prompt: whatarethemajorcostsinvolved
inyourresearch?)
(j) Whenyouoryourstudentsstartanewproject,howlongonaveragedoyouexpectittotake,
fromthestartofworktoapapersubmission?
(k) Hasthischangedoveryourcareer? (Prompt: whathasledtothischange?)
NowI’dliketohearyourthoughtsonthechangesyou’veobservedinyourcareer.
(a) Arethereparadigmshiftsthatyouwouldidentifyinthefieldoverthecourseofyourcareer?
(b) Didyourcommunitychangeatall,asaresultoftheseparadigmshifts?
(c) Prompt: whatyearswouldyouascribetoeachshift?
(d) How frequently do you feel the community undergoes a paradigm shift? (Prompt: is this
frequencychanging?)
(e) Arethereconcernsyouhavewiththedirectionofthefield?
(f) Ifthereweretobeaparadigmshiftinthenextfewyears,inwhatdirectionshoulditgo?
(g) Doyouthinkchangesintheresearchcommunityhavechangedyourteaching? (Prompts: how?
Howdoyoufeelaboutthisshift?)
NowIhavesomedemographicquestions,whichwillhelpusunderstandtherangeofpeoplethatwe
talkto. Ifyou’dprefernottoansweranyofthem,justletmeknow.
(a) Withwhichgenderdoyouidentify?
i. Man
ii. Woman
iii. Or,feelfreetospecifyasyouwish
(b) Iamgoingtoreadsomeagebrackets. CanyouindicatewhenIreadabracketthatyourage
fallsinto?
i. 18-24
ii. 25-34
iii. 35-44
iv. 45-54
v. 55-64
vi. 65+
(c) Whichcountrywereyoubornin?
(d) (ifnotalreadyknown)Whichcountryareyoucurrentlybasedin?
(e) Whatstageofyouracademiccareerwouldyouconsideryourselfin?
(f) Doyouconsideryourselfapartofaminoritizedgroupinyourfield?
(g) Anythingelseinyourbackgroundthatfeelsrelevantorthatyouwanttoadd?
3. Finally,we’dliketohearfrommorepeopleabouttheseissues. Isthereanyoneyoucouldintroduce
ustowhoyouthinkwouldhaveinterestinganswerstothesequestions?
Active authors in *CL venues over time
8000 All active authors
Newcomers
Outgoers
6000
4000
2000
0
2000
1990 1995 2000 2005 2010 2015 2020
Year
Figure4: Thenumberof“active”researcherspublishinginACLvenueshasincreaseddramatically,withmore
newcomerstothefieldyearoveryear
B Detailedquantitativemethodology
WebeginwiththeACLAnthologyandfocusonpapersbetween1980and2022. UsingtheSemantic-
Scholar(S2)API(Wade,2022),weobtainauthorandcitationinformationofpapersindexedbyS2(some
venues,suchassomeworkshops,andsomeFindings8 papers,arenotindexed,leaving77,235papers),
withafocusoncitationsofpapersidentifiedbyourparticipantsashavingbeeninfluentialtotheNLP
community. WeselectfromthissetofinfluentialpaperstogeneratethebarplotsforFigure2. Figure1
usesauthorpublicationinformationlinkedtotheindividualpapersconsidered.
WerelyonS2ORC(Loetal.,2020)forfull-textPDFparsesofasubsetofthesepapers,whichweuse
matchformentionsofsoftwaretoolkitsidentifiedbyourparticipantsashavingbeeninfluentialtothe
community, for Figure 3, as well as for mentions of influential techniques in the line plot of Figure 2.
Notethatquantifyingmentionsisnoisy: framework/librarynamescanbespelledinavarietyofways,
andnameslike"Moses"arealsousedforauthorsintheACLAnthology. Forfigure3,wenormalizeby
lowercasingalltext,andusingthemostcommonnormalizedspelling,e.g. tensorflow,orhuggingface.
WeestimatethatthiswilloverestimatethepresenceofMoses,duetoitsotherusages. andunderestimate
the presence of Hugging Face, which is officially spelled “Hugging Face”, but much more often used
as“HuggingFace”. Despitethis,theincrediblegrowthandpopularityofHuggingFacerelativetoother
frameworksisstillprominentlyvisible.
Wepresentanalternativeviewofdata,similartothatseeninFigure1,inFigure4. Here,wedefine
membersofthecommunityasauthorswithatleastthreepaperstotalin*CLvenues. Authorsarecounted
as“leaving”thecommunitytheyearaftertheirlast*CLpublication. Weonlyconsidertrendsinauthorship
until2020,asitisdifficulttodetermineifauthorswhodidnotpublishinthelastfewyearshaveleftthe
communityindefinitely.
8AsofOctober2023,whilesomeFindingspapers,suchasfromACL2021andEMNLP2020,areautomaticallyindexedby
S2,FindingspapersfromsomeconferencessuchasEMNLP2022arenot.Whilesomeofthesepapers(orversionsofthem)
maystillbeindexedbyS2duetoalsobeingonArXivorasimilarservice,wedonotincludetheminoursetofpapers.
srohtua
fo
rebmuN

Hierarchical Routing Mixture of Experts
WenboZhao1 YangGao1 ShahanAliMemon2 BhikshaRaj2 RitaSingh2
Abstract sub-region.
In regression tasks the distribution of the data Many models take this strategy. For example decision
is often too complex to be fitted by a single trees(Loh,2014)andrandomforests(Breiman,2001)divide
model. In contrast, partition-based models are theinputspacebyhard-partitionoffeaturedimensions,and
developed where data is divided and fitted by makepiece-wiselinearpredictionsoneachpartition. Mix-
local models. These models partition the input turemodels(Baileyetal.,1994)andmixtures-of-experts(Ja-
spaceanddonotleveragetheinput-outputdepen- cobsetal.,1991)performsoft-partitionontheinputspace
dencyofmultimodal-distributeddata,andstrong andassignregressionmodelstoeachofthepartitions. In
local models are needed to make good predic- particular,themixtureofexpertsmodelsaretree-structured
tions. Addressing these problems, we propose modelswithagatingmechanismtopartitiontheinputspace
abinarytree-structuredhierarchicalroutingmix- andacollectionofexpertsattheleavestomakelocalpre-
tureofexperts(HRME)modelthathasclassifiers dictions.
as non-leaf node experts and simple regression
Althoughwell-studiedandhavebeenproveneffective,these
modelsasleafnodeexperts. Theclassifiernodes
modelsdonotleveragetheinput-outputdependencyofthe
jointlysoft-partitiontheinput-outputspacebased
datadistributions. Forinstance,asinourtoyexample,dif-
on the natural separateness of multimodal data.
ferentregionsoftheoutputspace(theylabel)correspond
This enables simple leaf experts to be effective
todifferentmodesofthedata. Solelypartitioningtheinput
forprediction. Further,wedevelopaprobabilistic
spacewouldfitmultiplemodesofthedataintoeachparti-
frameworkfortheHRMEmodel,andproposea
tion,stillrequiringcomplexregressionmodelstocapture
recursiveExpectation-Maximization(EM)based
theinput-outputrelationineachofthem. Thisproblemcan
algorithmtolearnboththetreestructureandthe
beavoidedbyjointlypartitioningboththeinputandoutput
expert models. Experiments on a collection of
spaces,suchthateachpartitiononlyrequiresasimplerlocal
regressiontasksvalidatetheeffectivenessofour
regression. Thisisthemotivationbehindourwork.
methodcomparedtoavarietyofotherregression
models. Addressing the above-mentioned issues of conventional
partition-based regression methods, we propose a hierar-
chical routing mixture of experts (HRME) model, which
separatesoutputvariablesmodesbyjointlypartitioningthe
1.Introduction
inputandoutputspaces,andmakesprobabilisticinferences
Oneofthechallengesinmodelingaregressiontaskisthat byassigningsimpleregressionmodelstoeachoftheresul-
ofdealingwithdatawithcomplexdistributions. Thedistri- tantpartitions. OurHRMEmodelcanbeviewedasanew
butioncanbemulti-modal,renderinganysingleregression member of the family of hierarchical mixture of experts
modelhighlybiased. Forinstance,Figure1ashowsasyn- (HME)(Jordan&Jacobs,1994)models. Itisbinary-tree
thetic data set uniformly sampled from three intersecting structured,andhastwotypesofexperts—thenon-leafnode
lines with different amount of noise. A single regression expertsandleafnodeexperts. Thenon-leafnodeexperts
modelwouldfailtocapturethemulti-modalityofthisdata function as a new gating mechanism to soft-partition the
andyieldpoorperformance. Thisnecessitatesanotherstrat- databasedontheirmodes,definedonthejointdistribution
egy,ofdivideandconquer,topartitiontheinputspaceinto ofinputandoutputvariables. Thepartitioningisperformed
simplesub-regionsandassignaregressionmodeltoeach bynode-specificbinaryclassifier. Together,theclassifiers
inthenon-leafnodeshierarchicallypartitionthespaceinto
1DepartmentofElectricalandComputerEngineering,Carnegie
numberofregions,eachofwhichcorrespondstoaleafin
Mellon University, Pittsburgh, PA, USA 2School of Computer
thetree,andwithinwhichtherelationshipbetweeninput
Science,CarnegieMellonUniversity,Pittsburgh,PA,USA.Corre-
spondenceto:WenboZhao<wzhao1@andrew.cmu.edu>. and output variables is ideally unimodal. The leaf node
expertsmakepredictionsoneachresultingpartition. Ifthe
Preliminarywork.Copyright2019bytheauthor(s).
9102
raM
81
]GL.sc[
1v65770.3091:viXra
HierarchicalRoutingMixtureofExperts
10.0 10.0
original
10 pred
7.5 7.5
5.0 5 5.0
2.5 2.5
0
0.0 0.0
2.5 5 2.5
5.0 5.0
y=0.8x-1, noise=0.2 10
y=1.2x+6, noise=0.15
7.5 y=-2x+5, noise=0.25 7.5
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x
(a) (b) (c)
Figure1.(a)Atoyexample:synthetic3-linesdatawithdifferentamountofnoises.(b)PredictionsmadebyexpertsinourHRMEmodel.
Eachcurverepresentsthepredictionmadebyoneexpert.Darkercolorindicatesstrongerpredictionconfidence.(c)Predictionmadeby
ourHRMEmodelviaselectingthetop-1experts.
dataiswellpartitioned,theseleafnodeexpertscannowbe linearpredictions. Basedonthem,randomforests(Breiman,
relativelysimple. 2001;Liawetal.,2002)takeanensemblelearningapproach
byaggregatingacollectionofdecisiontreestoreducethe
However,theactualdistributionofthedataanditsmodes
over-fittingtendencyofasingledecisiontree. Apertaining
areunknownapriori. Consequently,thebinaryclassesfor
issuewiththesetree-basedmethodsisthattheyrelyonhard
eachclassifier(non-leaf)nodeareunknown.Thiseffectively
partitionsandpiece-wiselinearpredictions,whichcanlead
makesthepartitionoftheoutputspaceitselfavariableto
todiscontinuitiesandhighbiasesinpredictions.
be determined. To address this, we develop a probabilis-
ticframeworkforourHRMEmodel,andproposearecur- On the other hand, the mixture of experts (ME) models
sive Expectation-Maximization (EM) based algorithm to areafamilyofprobabilistictree-structuredmodelswitha
optimizethejointinput-outputpartition,thevariousexpert gatingmechanismandacollectionofexpertsattheleaves.
models, as well as the tree structure. To the best of our Thegatingmechanismisresponsibleforsoftpartitioning
knowledge,thisnewjoint-partitionbasedgatingmechanism the input space into sub-regions such that a local expert
forHMEmodelshasnotbeenstudiedyet. Theclosestrele- models the distribution of each sub-region (Yuksel et al.,
vantliteratureisbyMemonetal.(2018)whichpartitions 2012). The flexibility of the ME family embraces a rich
the space solely based on the output value to determine varietyofgatingmechanismsandexpertmodels. Examples
itsoptimaldiscretization. OurHRMEmodel,ontheother includehierarchicalmixtureofexperts(HME)(Jordan&Ja-
hand, uses a joint partition to determine the optimal data cobs,1994)whichemploysabinarytreestructure,Bayesian
allocation to the leaf experts, and our model is globally HME(Bishop&Svenskn,2002)withaBayesiantreatment,
optimizedratherthanlocallyoptimized. mixture of Gaussian processes (HME-GP) (Tresp, 2001;
Rasmussen&Ghahramani,2002;Yuan&Neubauer,2009;
We test our model on a collection of standard regression
Nguyen & Bonilla, 2014), mixture of support vector ma-
tasks,andtheresultsvalidatetheeffectivenessofourmodel
chines(HME-SVM)(Limaetal.,2007;Cao,2003),toname
comparedtoHMEandotherregressionmodels. Ourcon-
onlyafew.
tributionsare: (1)weproposeanewgatingmechanismvia
joint-partitionofbothinputspaceandoutputspacetosepa- The ME models have three issues: (1) the gating mecha-
ratethemodesofcomplexlydistributeddata,makingsimple nism does not explicitly leverage the input-output depen-
regressionmodelseffectiveforpredictions;(2)wedevelop denciesofthedata. Rather,itperformsprobabilisticinput-
arecursiveEMalgorithmtojointlyoptimizethepartition, spacepartitioning,basedonassumeddatadistributionssuch
theexpertmodels,aswellasthetreestructure. as the multinomial distribution (Jordan & Jacobs, 1994),
Gaussiandistribution(Yuan&Neubauer,2009),Dirichlet
process(Rasmussen&Ghahramani,2002),Gaussianpro-
2.RelatedWork
cess (Tresp, 2001), etc; (2) in ME models strong experts
Decisiontrees(Loh,2014;Breiman,2017;Quinlan,1986) areoftenneededtogaingoodperformance(Yukseletal.,
areafamilyofsupervisedlearningmethodsthatutilizea 2012);(3)thestructureoftheMEmodels,namelythetree
partition on the input feature space and make piece-wise depthandthenumberofexperts,isoftenoptimizedthrough
y y y
HierarchicalRoutingMixtureofExperts
eachnon-leafnode. However,wedonothavethedataclass
informationbeforehand,i.e.,wedonotknowhowdatacan
belocallyseparated. Asaremedy,weadoptathresholding
strategy—settingathresholdtonysuchthaty =0ify <t
andy =1otherwise. Asaresult,weassignbinaryclasses
todataviathresholdingony. However,notethatbydoing
soweeffectivelymaketavariabletobeoptimized,namely,
wearenotonlypartitioningonx,butalsopartitioningon
y. Wewillexplainoptimizationofthisjoint-partitioninthe
latersection.
Atthispoint,let’sassumewehaveknowntheoptimaltree
settings—thatis,weknowthetreestructure(thedepthand
thenumberofnodes),andforeachnon-leafnode,theopti-
malsplittingthresholdt∗andtheclassifierh parameter-
β∗
izedbytheoptimalparameterβ∗,andforeachleafnode,
theregressorr parameterizedbytheoptimalparameter
Figure2.Illustration of the HRME model. It is a probabilistic θ∗
θ∗. Wethenexplainthepredictionofygivenaninputx.
binary tree. Each non-leaf node (circle) carries a classifier h
β
andapartitionthresholdt,andeachleafnode(square)carriesa Specifically,fornotationconvenience,weassumethenodes
regressorr θ.Predictionismadeviaprobabilisticcombinationof arenumberedsuchthatforanytwonodesn andn ,ifi<j,
i j
leafregressors.ModelislearnedviarecursiveEM.
n occurseithertotheleftofn oraboveitinthetree. Each
i j
noden carriesaclassifierh :x(cid:55)→{n ,n },which
i β∗ i+1 i+2
assigns any instance with inn pi ut x to one of the children
extraprocedures,suchaspruning(Waterhouse&Robinson,
nodesn orn . Weintroduceabinary-valuedrandom
i+1 i+2
1995) and Bayesian model selection (Bishop & Svenskn,
variablez ∈{0,1}toindicatexbeingassignedton or
2002;Kanaujia&Metaxas,2006). Thisincreasesthecom-
ni i
not. Then,thecorrespondinglikelihoodofxbeingassigned
plexityofmodellearning.
tonoden isestimatedbytheclassifieronnoden
i i−1
Weaddresstheissueswiththeseconventionalmethodsby
q(z |x)≡q(z =1|x)←−h (x). (1)
(1)jointsoft-partitionoftheinput-outputspacebasedon ni ni β n∗
i−1
thenaturalseparabilityofthemulti-modaldataand(2)joint
optimization on the tree structure and the expert models Next,wewouldliketoknowthelikelihoodofadatapoint
withoutextrapruningprocedures. xbeingroutedtoaspecificleaf. Denotethechainfromroot
l ≡n toleafl asl →...→l ,thenthelikelihoodof
1 0 k 1 k
xbeingassignedtoleafl is
3.HierarchicalRoutingMixtureofExperts k
(cid:88) (cid:88)
Inthissection,wepresentthespecificationsoftheHRME q(z lk |x)= ... q(z l1,...,z lk |x). (2)
model,formulatetheoptimizationobjective,anddevelop zl1 zlk−1
theoptimizationalgorithm.
Applying the sum-product rule and using the conditional
dependencyto(2)yield
3.1.ModelSpecification
Figure2showsthestructureofthetreemodel. Itisabinary k (cid:89)−1
q(z |x)= q(z |z ,x). (3)
tree. Eachnon-leafnodeisequippedwithaclassification lk lj+1 lj
expert,whichisabinaryclassifierinthiscase. Eachleaf j=1
nodeisequippedwitharegressionexpert,whichisasimple
linear model. The basic assumption here is that the com- Forleafl k,itcarriesaregressorr θ l∗ k suchthattheprediction
plexlydistributedmulti-modaldataisneverthelesslocally y (cid:98)lk = r θ l∗ k(x). Then, an estimate of y is given by the
(andnon-linearly)separable,andhencethenon-leafexperts expectationofthepredictionsoverallleaves
ofthetreefunctionasaroutingmechanismtopartitionthe
(cid:88)
dataintosubsetsofsimple(uni-modal)distributions,and y (cid:98)= r θ l∗ k(x)q(z lk |x), (4)
routeeachsubsettoasimpleleafexperttomakepredictions. lk∈leaves
Wedenotetheinputfeaturesasx∈Rdandthecontinuous andthecorrespondingconditionaldensityforleafl is
k
output label as y ∈ R. In order to route the data in such
fashion, it requires to determine the optimal classifier at p(y|z lk,x)←r θ l∗ k(x). (5)
HierarchicalRoutingMixtureofExperts
3.2.LearningAlgorithm
Input: [data],[root]
Parameter:{t},classifierparameters,regressor
Fromtheprevioussection,wehaveshownthatinorderto
parameters
makepredictionsusingthetree,weneedtodeterminethe
Output: HRMETree
optimaltreesettings,i.e.,thetreestructure{n },thenon-
i
FunctionGrowTree(data list,nodes per level)
leafnodethresholds{t },theclassifierparameters{β },
ni ni
fornodeinnodes per leveldo
andtheleafnoderegressorparameters{θ }.
ni D←data list
Weadoptamaximum-likelihoodapproach. Specifically,our node l,node r←GrowSubtree(node)
objectiveistomaximizethelog-likelihoodforeachx fortdo
D ,D ←SplitData(D,t)
maxlogp(y|x) (6) l r
if
q(z|x)(cid:88) min(|D l|,|D r|) <min leaf sample ratio
=logp(y|x) q(z|x) #oftotalsamples
q(z|x)
thencontinue;
z
(cid:88) p(y,z|x)q(z|x)
node.TrainClassifier(D,t)
= q(z|x)log
PropagateconditionalsusingEquation(3)
p(z|y,x)q(z|x)
z node l.TrainLeaf(D )
l
(cid:88) p(y,z|x) node r.TrainLeaf(D )
= q(z|x)log + (7) r
q(z|x) Q←ComputeQ usingEquation(10)
z
end
(cid:88) q(z|x)
q(z|x)log , (8) ifQ>Q∗then
p(z|y,x)
z Q∗ ←Q
whereq(z|x)isanestimateforthetrueassignmentmass data list←[D l,D r]
p(z|x);(7)iscommonlyreferredtoastheevidencelower
nodes per level←[node l,node r]
GrowTree(data list,nodes per level)
bound(ELBO)whichneedtobeimprovedtomaximizethe
else
log-likelihood(6);(8)istheKullback-Leiblerdivergence
Deletethesubtree
whichmeasuresthedistanceoftwoprobabilitymassesand
continue
isalwaysgreaterthanorequaltozero.
end
Therefore, it is natural to apply the expectation- end
maximization(EM)methodtooptimize(6). Specifically,in Algorithm1:RecursiveEMLearningofHRME
theE-step, wecomputetheELBO(7)forallthetraining
instances
(cid:88)(cid:88) p(y,z|x) difficulty. Foreachnon-leafnode,weperformgrid-search
Q(p,q)= q(z|x)log
over the possible values of t, and for each t, we perform
q(z|x)
x z the M-step. The best t value is then obtained as the one
(cid:88)(cid:88) p(y|z,x)p(z|x) withmaximumQ-value. Althoughdifferentsamplingstrate-
= q(z|x)log , (9)
q(z|x) giescanbeusedwhensearchingfort,inpracticewefind
x z
grid-searchworkswell.
whereq(z|x)isgivenby(3), andp(y|z,x)isgivenby
(5)(forexample,theleafnodegivesaGaussiandistribution Aswementionedearlier,itisdifficulttoestimatethetrue
overy ifweassumealinearmodelwithGaussiannoise). leafnodeassignmentmassp(z|x). Althoughvariational
Thetrueleafnodeassignmentmassp(z|x)isyetunknown. approximation may be used, we propose an empirically
However,wecanestimateitusingtheempiricalfrequency simplerstrategy. InsteadofusingtheQ-valueasaglobal
of the number of samples at the leaf node over the total indicatoroftheoptimalityofthetree,weproposetousethe
numberoftrainingsamples. Thisisacrudeestimation,but negativemean-square-error
we will provide a better strategy in the later part of this
Q =−mean(y−y)2, (10)
section. alternative (cid:98)
IntheM-step,weoptimizetheparameterstoincreasethe whereyisgivenby(4).
(cid:98)
ELBO (7). Specifically, we optimize the non-leaf node
TherecursiveEMalgorithmissummarizedinAlgorithm1.
experttomaximizetheclassificationaccuracy,andoptimize
Westartfromtherootnode,andgrowthetreerecursively
theleaf-nodeexperttominimizetheregressionerror.
inadepth-firstmanner,i.e.,fromtoptobottom,fromleft
However,aswementionedinSection3.1,thedataclasses toright. Eachtimewegrowathree-nodesubtree. Wekeep
arenotavailable,andthenon-leafnodethresholdtisun- increasing the number of nodes until the lower bound Q
known. Weprovideanalternativeapproachtomitigatethis stopsincreasingortheratioofthenumberofsamplesatthe
HierarchicalRoutingMixtureofExperts
leaf to the total number of samples is below some preset literatureunderthesameexperimentsettings.
threshold.
HRME For our HRME model, we train it following Al-
gorithm1. Inourinstantiationofthemodel,thenon-leaf
4.Experiments expertsaresupportvectormachineswithradialbasisfunc-
tion kernels (SVM-RBF). We choose two simple models
Inthissection, weevaluateourHRMEmodelandthere-
forleafexperts,thelinearregressionmodel(referredtoas
cursiveEMalgorithmonacollectionofstandardregression
HRME-LR) or the support vector regression model with
datasets. Wedescribetheexperimentsettingsandpresent
radial basis function kernel (referred to as HRME-SVR).
the results for our method and a wide range of baseline
Similar to the training of baselines, our models are also
methods.
trainedandfine-tunedonthesametrainingsetsfollowing
thesamestrategywiththebaselinemethods. Inaddition,all
Table1. DatasetStatistics non-leafexpertsonthetreesharethesamehyperparameters,
soaretheleafexperts. Althoughitwouldbedesirableto
DATASET FEATUREDIM TRAIN TEST
usedifferenthyperparametersfornodesondifferentdepth
3-LINES 1 1750 750
of thetree asthe datasize shrinkswith the treedepth, in
HOUSING 13 354 152
CONCRETE 8 721 309 practicewefindourmodelisrobusttosuchvariations.
CCPP 4 6697 2871
ENERGY 28 14803 4932 4.3.Results
KIN40K 8 10000 30000
Weevaluateourmethodsandthebaselinemethodswithtwo
metrics: themeanabsoluteerror(MAE)andtherootmean
4.1.Data squarederror(RMSE).
Fordemonstrationpurpose,wesynthesizea3-linesdataset On the synthetic 3-lines data, Figure 3 shows the fitting
(as shown in Figure 1a). For further evaluation, we se- resultsonthetestsetforourmethodsandbaselinemethods.
lect five other standard datasets that are commonly used WeobservethatourHRMEmodelsprovideamoreaccurate
in regression tasks. Four of these datasets are from the predictionthanthebaselines. Specifically,thelinearmodel
UCImachinelearningrepository(Dheeru&KarraTaniski- isjustpredictingthemeanofthethreedifferentdistributions;
dou,2017): theCCPPdataset(Tu¨fekci,2014;Kayaetal., thedecisiontreeandrandomforestprovideabetterfitthan
2012), the concrete dataset (Yeh, 1998), the Boston linear regression, but discontinuities and higher variance
housing dataset (Belsley et al., 2005) and the energy occurduetothepiece-wiselinearnatureofthesetwomod-
dataset (Candanedo et al., 2017), and one kin40k els. MLPachievessmallerpredictionerrorthanDTandRF,
dataset(Seegeretal.,2003;Deisenroth&Ng,2015). The butitalsoshowsdiscontinuitiesandfailuretocapturethe
datasetsrangefromsmall-sizedtolarge-sizedandfromlow- datamodality. Incomparison,ourHRMEmodelsprovide
dimensionaltohigh-dimensional. Thestatisticsareshown much smoother fitting with lower bias and variance than
inTable1.Thedivisionoftrainandtestsetsareeitherusing thebaselines. Notethatevenwithlinearleafexperts,the
thedefaultsplitorusing0.7:0.3split. HRME-LRmodelisabletocapturethenonlinearmodality
ofthedataandmakeregionalpredictionsbysoft-switching
4.2.Models its experts among the three distributions. Further, by us-
ingnon-linearleafexperts,theHRME-SVRmodelyields
Baselines To promote a fair evaluation, we compare our
smootherpredictionsthantheHRME-LRmodelwithlower
method with a wide range of baselines: linear regression
biasandvariance. Additionally,weobservethatallmodels
(LR),supportvectorregression(SVR),decisiontrees(DT),
hereprefertheupperlinetothelowerlineduetothehigher
randomforests(RF),hierarchicalmixtureofexperts(HME)
noiselevelinthelowerline.
withstrongGaussianorGaussianprocessexperts,andmulti-
layerperceptron(MLP).Eachmodelcarriesasetofparame- Figure 1b shows the predictions made by the experts in
terstobeestimatedaswellashyperparameters(e.g.,margin HRME-SVR model. We see that there are total fourteen
andkernelsinSVR,depthandnumberofnodesinDTand experts (indicated by colored curves) being allocated to
RF,numberofneuronsandlearningrateinMLP,etc.) tobe different regions of the data. Each expert is confident of
tuned. Wetrainthemodelsontrainingsets,andfine-tune makingpredictionswithinonedatamode,asindicatedby
thehyperparametersusinggrid-searchandthree-foldcross higherposteriorprobabilities(darkercolors),andalldata
validationonthetrainingsetstoobtainthebestperformance. modesaresuccessfullycaptured. Consequently,ifwehave
Themodelsareimplementedwithscikit-learntoolkit(Buit- priorknowledgeofthedatadistribution,thiscouldbeused
inck et al., 2013) or PyTorch (Paszke et al., 2017). For toselecttheexpertsformakingthebestpredictions. Fur-
HMEmodels,weobtainthebestavailableresultsfromthe ther,insteadofusingweighted-averageoverallexperts,we
HierarchicalRoutingMixtureofExperts
10.0 oprriegdinal 10.0 oprriegdinal 10.0 oprriegdinal 2.86
7.5 7.5 7.5
5.0 5.0 5.0 -1.9
2.5 2.5 2.5
0.0 0.0 0.0 2.85 2.827
2.5 2.5 2.5
-3.09 7.7
5.0 5.0 5.0
7.5 7.5 7.5 2.846 2.842 2.822
10.07.5 5.0 2.50x.0 2.5 5.0 7.510.0 10.07.5 5.0 2.50x.0 2.5 5.0 7.510.0 10.07.5 5.0 2.50x.0 2.5 5.0 7.510.0
(a)LR (b)DT (c)RF -7 -2.5 5.3
10.0 oprriegdinal 10.0 oprriegdinal 10.0 oprriegdinal 2.845 2.822
7.5 7.5 7.5 -3.7 6.8
5.0 5.0 5.0
2.821
2.5 2.5 2.5 2.844
0.0 0.0 0.0 5.6
-6.4
2.5 2.5 2.5
5.0 5.0 5.0 2.843 2.820
7.5 10.07.5 5.0 2.50x.0 2.5 5.0 7.510.0 7.5 10.07.5 5.0 2.50x.0 2.5 5.0 7.510.0 7.5 10.07.5 5.0 2.50x.0 2.5 5.0 7.510.0 -4.3 5.9
(d)MLP (e)HRME-LR (f)HRME-SVR 2.843
-4.9
Figure3.Fittingresultsonsyntheticdatawithdifferentmodels:
linearregression(LR),decisiontree(DT),randomforest(RF),
multi-layerperceptron(MLP),ourHRMEwithlinearregressor
(HRME-LR)andSVRregressor(HRME-SVR),respectively. Figure4.TheHRMEtreeaftertrainingonthesyntheticdata.The
treeisgrownrecursivelyinadepth-firstmanner—toptobottom,
left to right. Each circle represents a classifier node, and the
numberwithinitisthepartitionthresholdt. Thenumberonthe
selectthetop-1experttomakepredictions. Figure1cshows
edgerepresentstherootmeansquareerrorifstopgrowingatthat
thecorrespondingfittingresults. Weseeamuchbetterfit
node.Eachdashededgeleadstoaleafregressor.
than that in Figure 3f—in the former all data modes are
successfullypredictedbyourHRME-SVRmodel.
vationindicatesthelinearnatureofdatadistributions,and
WefurthershowthegrowthoftheHRMEtreeonthetrain-
henceanonlinearregressionexpertwouldbeinappropriate
ingset. InFigure4,thenumberineachcirclenodeisthe
forthisdataset. Thisobservationisalsoconfirmedbythe
partition threshold t. The number besides each circle is
poorperformanceofthenonlinearMLPmodel. Further,the
the RMSE if growth stops at that node. Note the tree is
data is small (506 samples) but has high dimension (13),
grown recursively in a depth-first manner (top to bottom,
makingitdifficulttoseparatethemodesbySVM.Instead,
left to right). We observe that the RMSE reduces as the
otherclassifierscanbeusedtoimprovetheperformanceof
treegrows. Thisvalidatesourhypothesisthatouralgorithm
ourmodel. Wealsoobservethatourmethodscanreducethe
canlearntheoptimaltreestructureautomaticallywithout
variance(lowRMSE)onamajorityoftasks. Thisshows
pruning afterwards, and the proposed Q-value is a good
thatourmethodsareabletomitigatetheproblemofhigh
indicatoroftheglobaloptimalityofthetree. Further,we
varianceofconventionaltreemodels. Inaddition,wesee
notice our HRME model also successfully partitions the
even with simple linear leaf experts, our method can sig-
outputspacebasedonseparabilityofdatamodesbyfinding
nificantlyoutperformLR,andcancompetewithnonlinear
thethresholdslike−1.9,5.6,−6.4,etc.
modelslikeSVR,RFandMLP.Thisvalidatesourhypoth-
Table2showscomprehensiveresultsforallthemethodson esisthatwithourdatamodality-awareroutingmechanism
allthedatasets. Weobserveanoverallimprovementofour simple leaf experts can make good predictions. At last,
HRMEmethodsoverthebaselinemethods. Specifically,for TheMLPperformspoorlyinmostofthetasksevenwith
largedatasetslikeEngeryandKin40k,ourmethodsoutper- fine-tuning. ThisshowsthatMLPisnotabletocapturethe
formallotherbaselinesintermsofbothbias(MAE)and complexmodalityofdatadistributions.
variance (RMSE) even for the HME models with strong
Tothispoint,comprehensiveexperimentresultsshowthat
GaussianprocessexpertsandtheMLP.Formedium-sized
our HRME methods perform well on a wide range of re-
datasets like CCPP and Concrete our methods generally
gressiontasks, especiallyonlarge, high-dimensionaland
outperformotherbaselinesexceptRF.Butasanensemble
difficult datasets. Our HRME methods can capture the
method like RF, our method can also be boosted (now is
complex data hierarchy, reduce variance, and make good
averaged) to improve performance (Avnimelech & Intra-
predictions with simple leaf experts. We further explore
tor,1999). ForsmalldatasetlikeHousing,ourmethodsdo
sometheoreticalpropertiesofourHRMEmodel.
not outperform DT and RF. But at a closer look we find
thatHRME-LRyieldsmuchsmallerMAEandRMSEthan 1usingGaussianexperts;resultstakenfromFerrari&Milioni
HRME-SVRandisonaparwithDTandRF.Thisobser- (2011)
y
y
y
y
y
y
HierarchicalRoutingMixtureofExperts
Table2. ExperimentResults
DATASET METRIC LR SVR DT RF HME MLP HRME
LR SVR
MAE 3.352 2.006 2.224 2.131 — 1.960 2.337 2.250
3-LINES
RMSE 4.104 3.173 3.291 3.072 — 2.795 2.885 2.859
MAE 3.651 3.498 2.537 2.103 4.1701 6.711 2.682 3.266
HOUSING
RMSE 4.911 5.126 3.665 3.043 5.6102 8.535 3.857 4.376
MAE 8.088 8.013 4.919 3.436 — 5.394 4.121 4.020
CONCRETE
RMSE 10.204 10.772 8.000 4.806 6.2503 6.594 5.664 5.609
MAE 3.601 2.746 2.941 2.383 — 4.013 2.965 2.712
CCPP
RMSE 4.578 3.856 4.151 3.409 4.1004 5.078 3.951 3.805
MAE 52.075 43.141 43.996 52.002 — 40.521 42.121 40.009
ENERGY
RMSE 93.564 101.267 99.654 95.558 — 88.191 89.203 87.022
MAE 0.806 0.092 0.592 0.433 — 0.237 0.150 0.071
KIN40K
RMSE 0.996 0.161 0.773 0.548 0.2305 0.312 0.212 0.114
4.4.ConvergenceandComplexityAnalysis O(n3+n2d).Asaresult,toattain(cid:15)-errorweneedO((cid:15)−d/2)
experts. FortheHMEmodels,wecanassumeuniformdata
Convergence:Letn,d,kbethenumberoftrainingsamples,
partitionamongexperts,andthetotalcostisO(n3(cid:15)d). For
thedimensionofeachsampleandthenumberofexperts,
ourHRMEmodel, thedataforeachnodedecreaseswith
respectively. Zeevietal.(1998)provethatwithlargesam-
depth,andwecantaketheaverageamongnodes. There-
ples,theMEmodelscanuniformlyapproximateSobolev
sultanttotalcostisO(n3(cid:15)d+dn2(cid:15)d/2). Althoughthetotal
class functions of order r in the L norm at a rate of at
p complexityincreasesforouralgorithm,however,thecom-
leastO(Ck−r/d)withconstantC. Thisupper-boundsthe
putationcanbeacceleratedusingdynamicprogrammingat
approximationerrorofgeneralMEfamily. Further,Jiang
thepriceofstoragecost. Moreover,thecomputationateach
&Tanner(1999a)provethattheHMEmeanfunctionscan
nodecanbedoneinparallel.
approximatethetruemeanfunctionatarateofO(k−2/d)
in the L norm. Jiang & Tanner (1999b) also show that Consistency: Zeevietal.(1998)provethatunderregular-
p
theHMEprobabilitydensityfunctionscanapproximatethe ityconditions,least-squaresestimatorsfortheMEmodels
datadensityatarateofO(k−4/d)inKLdivergence. For areconsistent. Further, Jiang&Tanner(2000)showthat
ourHRMEmodel,sincethegeneralassumptionsofthese maximumlikelihoodestimatorsareconsistentandasymptot-
resultshold,theuniformconvergencealsoholds. icallynormal. Therefore,ourHRMEmodelalsoproduces
consistentestimators.
Complexity: ThecomplexityofEMbasedalgorithmsfor
HME models mainly lies in the M-step, where the re- Identifiability: Jiang&Tanner(1999c)provethattheME
estimationofparametersinvolvessolvingasystemofequa- modelsareidentifiableunderregularityconditionsthatthe
tions using the Newton (or Newton-like) update. In the expertsareorderedandthemodelparametersarecarefully
HMEmodels,aNewtoniterationcostisO(n3). Inourcase, initialized.
the complexity of M-step is in solving the SVM. Specif-
Inthefuturework,wewouldliketoprovidemorerigorous
ically, for standard SVM solver with primal-dual interior
studyonthetheoreticallybehaviorsofourHRMEmodel.
pointmethod,thecomplexityisintheNewtonupdateand
evaluation of the kernel, and hence the iteration cost is
5.Conclusions
2usingGaussianexperts;resultstakenfromFerrari&Milioni
(2011) In this paper, we propose a hierarchical routing mixture
3usingGaussianprocessexperts;resultstakenfromTrappetal. ofexperts(HRME)modeltoaddressthedifficultyofdata
(2018)
partitioningandexpertassigninginconventionalregression
4usingGaussianprocessexperts;resultstakenfromTrappetal.
models.Byutilizingnon-leafclassifierexperts,ourmodelis
(2018)
5usingGaussianprocessexperts;resultstakenfromNguyen& abletocapturethenaturaldatahierarchyandroutethedata
Bonilla(2014) tosimpleregressorsforeffectivepredictions. Furthermore,
HierarchicalRoutingMixtureofExperts
wedevelopaprobabilisticframeworkfortheHRMEmodel, Jacobs,R.A.,Jordan,M.I.,Nowlan,S.J.,andHinton,G.E.
and propose a recursive Expectation-Maximization (EM) Adaptivemixturesoflocalexperts. Neuralcomputation,
basedalgorithmtooptimizeboththetreestructureaswell 3(1):79–87,1991.
as the expert models. Comprehensive experiment results
validatetheeffectivenessandsomenicepropertiesofour Jiang, W. and Tanner, M. A. On the approximation rate
model. ofhierarchicalmixtures-of-expertsforgeneralizedlinear
models. Neuralcomputation,11(5):1183–1198,1999a.
References
Jiang, W. and Tanner, M. A. Hierarchical mixtures-of-
experts for generalized linear models: some results on
Avnimelech,R.andIntrator,N. Boostedmixtureofexperts:
densenessandconsistency. InAISTATS.Citeseer,1999b.
anensemblelearningscheme. Neuralcomputation,11
(2):483–497,1999.
Jiang, W. and Tanner, M. A. On the identifiability of
Bailey,T.L.,Elkan,C.,etal. Fittingamixturemodelbyex- mixtures-of-experts. NeuralNetworks,12(9):1253–1258,
pectationmaximizationtodiscovermotifsinbipolymers. 1999c.
1994.
Jiang,W.andTanner,M.A. Ontheasymptoticnormality
Belsley, D. A., Kuh, E., and Welsch, R. E. Regression ofhierarchicalmixtures-of-expertsforgeneralizedlinear
diagnostics: Identifyinginfluentialdataandsourcesof models. IEEETransactionsonInformationTheory,46
collinearity,volume571. JohnWiley&Sons,2005. (3):1005–1013,2000.
Bishop,C.M.andSvenskn,M. Bayesianhierarchicalmix- Jordan, M.I.andJacobs, R.A. Hierarchicalmixturesof
turesofexperts. InProceedingsoftheNineteenthconfer- expertsandtheemalgorithm. Neuralcomputation,6(2):
enceonUncertaintyinArtificialIntelligence,pp.57–64. 181–214,1994.
MorganKaufmannPublishersInc.,2002.
Kanaujia, A. and Metaxas, D. Learning ambiguities us-
Breiman, L. Random forests. Machine learning, 45(1): ingbayesianmixtureofexperts. InToolswithArtificial
5–32,2001. Intelligence, 2006. ICTAI’06. 18th IEEE International
Conferenceon,pp.436–440.IEEE,2006.
Breiman,L. Classificationandregressiontrees. Routledge,
2017.
Kaya,H.,Tu¨fekci,P.,andGu¨rgen,F.S. Localandglobal
learningmethodsforpredictingpowerofacombinedgas
Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F.,
&steamturbine.InProceedingsoftheInternationalCon-
Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P.,
ferenceonEmergingTrendsinComputerandElectronics
Gramfort, A., Grobler, J., Layton, R., VanderPlas, J.,
EngineeringICETCEE,pp.13–18,2012.
Joly,A.,Holt,B.,andVaroquaux,G. APIdesignforma-
chinelearningsoftware:experiencesfromthescikit-learn
Liaw,A.,Wiener,M.,etal. Classificationandregressionby
project. InECMLPKDDWorkshop: LanguagesforData
randomforest. Rnews,2(3):18–22,2002.
MiningandMachineLearning,pp.108–122,2013.
Lima,C.A.,Coelho,A.L.,andVonZuben,F.J. Hybridiz-
Candanedo,L.M.,Feldheim,V.,andDeramaix,D. Data
ing mixtures of experts with support vector machines:
drivenpredictionmodelsofenergyuseofappliancesin
Investigationintononlineardynamicsystemsidentifica-
alow-energyhouse. Energyandbuildings,140:81–97,
tion. InformationSciences,177(10):2049–2074,2007.
2017.
Loh,W.-Y. Fiftyyearsofclassificationandregressiontrees.
Cao, L. Support vector machines experts for time series
InternationalStatisticalReview,82(3):329–348,2014.
forecasting. Neurocomputing,51:321–339,2003.
Deisenroth,M.P.andNg,J.W. Distributedgaussianpro- Memon, S. A., Zhao, W., Raj, B., and Singh, R. Neural
cesses. arXivpreprintarXiv:1502.02843,2015. regressiontrees. arXivpreprintarXiv:1810.00974,2018.
Dheeru,D.andKarraTaniskidou,E. UCImachinelearning Nguyen, T. and Bonilla, E. Fast allocation of gaussian
repository,2017. URLhttp://archive.ics.uci. processexperts. InInternationalConferenceonMachine
edu/ml. Learning,pp.145–153,2014.
Ferrari,D.B.andMilioni,A.Z. Choicesandpitfallscon- Paszke,A.,Gross,S.,Chintala,S.,Chanan,G.,Yang,E.,
cerningmixture-of-expertsmodeling. PesquisaOpera- DeVito,Z.,Lin,Z.,Desmaison,A.,Antiga,L.,andLerer,
cional,31(1):95–111,2011. A. Automaticdifferentiationinpytorch. 2017.
HierarchicalRoutingMixtureofExperts
Quinlan,J.R.Inductionofdecisiontrees.Machinelearning,
1(1):81–106,1986.
Rasmussen,C.E.andGhahramani,Z. Infinitemixturesof
gaussianprocessexperts. InAdvancesinneuralinforma-
tionprocessingsystems,pp.881–888,2002.
Seeger,M.,Williams,C.,andLawrence,N. Fastforward
selectiontospeedupsparsegaussianprocessregression.
InArtificialIntelligenceandStatistics9,numberEPFL-
CONF-161318,2003.
Trapp,M.,Peharz,R.,Rasmussen,C.E.,andPernkopf,F.
Learningdeepmixturesofgaussianprocessexpertsusing
sum-productnetworks. arXivpreprintarXiv:1809.04400,
2018.
Tresp, V. Mixtures of gaussian processes. In Advances
inneuralinformationprocessingsystems,pp.654–660,
2001.
Tu¨fekci,P. Predictionoffullloadelectricalpoweroutput
ofabaseloadoperatedcombinedcyclepowerplantus-
ingmachinelearningmethods. InternationalJournalof
ElectricalPower&EnergySystems,60:126–140,2014.
Waterhouse, S. and Robinson, A. Pruning and growing
hierachicalmixturesofexperts. 1995.
Yeh,I.-C. Modelingofstrengthofhigh-performancecon-
creteusingartificialneuralnetworks. CementandCon-
creteresearch,28(12):1797–1808,1998.
Yuan, C. and Neubauer, C. Variational mixture of gaus-
sianprocessexperts. InAdvancesinNeuralInformation
ProcessingSystems,pp.1897–1904,2009.
Yuksel, S. E., Wilson, J. N., and Gader, P. D. Twenty
yearsofmixtureofexperts. IEEEtransactionsonneural
networksandlearningsystems,23(8):1177–1193,2012.
Zeevi, A. J., Meir, R., and Maiorov, V. Error bounds for
functionalapproximationandestimationusingmixtures
ofexperts. IEEETransactionsonInformationTheory,44
(3):1010–1025,1998.

TheThirty-FifthAAAIConferenceonArtificialIntelligence(AAAI-21)
Knowledge-driven Data Construction for Zero-shot Evaluation
in Commonsense Question Answering
KaixinMa1,FilipIlievski2,JonathanFrancis1,3,
YonatanBisk1,EricNyberg1,AlessandroOltramari3
1LanguageTechnologiesInstitute,SchoolofComputerScience,CarnegieMellonUniversity
2InformationSciencesInstitute,ViterbiSchoolofEngineering,UniversityofSouthernCalifornia
3Human-MachineCollaboration,BoschResearchPittsburgh
fkaixinm,jmf1,ybisk,ehng@cs.cmu.edu,ilievski@isi.edu,alessando.oltramari@us.bosch.com
Abstract led to leaps in accuracy—closing the gap between human
andmachineperformancetosingle-digitpercentagepoints.1
Recent developments in pre-trained neural language
However,duetoincreasingconcernthatlarge-capacityneu-
modeling have led to leaps in accuracy on common-
sensequestion-answeringbenchmarks.However, there ral systems are modeling individual datasets, rather than
is increasing concern that models overfit to specific learninghowtoperformlogicalreasoningortoutilizeexter-
tasks,withoutlearningtoutilizeexternalknowledgeor nalknowledgeeffectively(Mitraetal.2019),focusisshift-
performgeneralsemanticreasoning.Incontrast,zero- ingtoalternativetrainingandevaluationstrategies.Inpartic-
shotevaluationshaveshownpromiseasamorerobust ular,zero-shotevaluationshowspromiseasanefficientmea-
measureofamodel’sgeneralreasoningabilities.Inthis sure of model generalisability across tasks (Shwartz et al.
paper, we propose a novel neuro-symbolic framework
2020;Lietal.2020).Here,modelsaretrainedandvalidated
forzero-shotquestionansweringacrosscommonsense
ontaskA,andtestedonadifferenttaskB,withoutaccessto
tasks. Guided by a set of hypotheses, the framework
B’strainingdataorlabels.Thisleadsstate-of-the-artmod-
studies how to transform various pre-existing knowl-
elsfromindividualtaskstofalter,sometimesbyasmuchas
edgeresourcesintoaformthatismosteffectiveforpre-
training models. We vary the set of language models, a50%decreaseinperformance(Shwartzetal.2020).
trainingregimes,knowledgesources,anddatagenera- Repositories of commonsense knowledge, like
tion strategies, and measure their impact across tasks. ConceptNet (Speer, Chin, and Havasi 2017) and
Extendingonpriorwork,wedeviseandcomparefour ATOMIC(Sapetal.2019a),canbebeneficialforcommon-
constrained distractor-sampling strategies. We provide senseQA,especiallywhenlittleornotrainingdataisavail-
empirical results across five commonsense question- able. Enriching the training data with ConceptNet and
answeringtaskswithdatageneratedfromfiveexternal ATOMIChasbeenshown(Maetal.2019;Mitraetal.2019)
knowledgeresources.Weshowthat,whileanindivid-
toimproveaccuracyondatasetsderivedfromthesegraphs:
ualknowledgegraphisbettersuitedforspecifictasks,a
CommonSenseQA (Talmor et al. 2019) and SocialIQA.
globalknowledgegraphbringsconsistentgainsacross
Knowledge bases (KBs) can be used to generate question-
differenttasks.Inaddition,bothpreservingthestructure
of the task as well as generating fair and informative answerpairsanddistractorsautomatically,inordertotesta
questionshelplanguagemodelslearnmoreeffectively. model’s reasoning ability (Petroni et al. 2019; Richardson
andSabharwal2019)orprovideadditionalsupervision(Ye
Introduction etal.2019;Yangetal.2020). WhileKBshavebeenshown
tohelpinazero-shottransfersettingrecently(Banerjeeand
Common sense is key to efficient communication in every-
Baral 2020), no comprehensive study exists on the relation
day situations, as it enables natural language understand-
between various knowledge, its usage method, and neural
ingthroughcontextualreasoning.Machinequestionanswer-
models for zero-shot transfer across commonsense tasks.
ing(QA)benchmarks,likeSocialIQA(Sapetal.2019b)
Moreover,whileadversarialfilteringtechniques(Brasetal.
and PhysicalIQA (Bisk et al. 2020), are effective be-
2020) improve the quality of a manually created question
havioraltestsofcommonsensereasoninginmachines,each
set,theirimpactonautomaticallygeneratedquestionsfrom
focusing on different capabilities. Answering a question in
avarietyofKBshasnotbeeninvestigatedyet.
SocialIQAmightrequiretheknowledgethatreaderstyp-
Inthispaper,(1)wecompileasetofhypothesesandde-
icallypreferheroesovervillainsinfantasynovels;whereas,
signanovelneuro-symbolicframeworkthatinvestigatesthe
in PhysicalIQA, the knowledge that metal stools can
dependency between knowledge sources, question genera-
break windows, because windows are made of glass and
tion techniques, language model (LM) variants, and tasks.
metalisamoreenduringmaterialthanglass.Althoughsuch
Our framework leverages a wide range of KBs, covering
taskshadbeentraditionallydifficultformachines,recentde-
visual, social, and concept-based knowledge, to pre-train
velopments in pre-trained neural language modeling have
Copyright(cid:13)c 2021,AssociationfortheAdvancementofArtificial 1For example (accessed 4 August, 2020): https://leaderboard.
Intelligence(www.aaai.org).Allrightsreserved. allenai.org/socialiqa/submissions/public
13507
LMs for zero-shot evaluation on multiple-choice common- generatequestionsusingConceptNetandWikipedia.Ko-
senseQAtasks.(2)Recognizingthattheaspectofquestion cijan et al. (2019) constructed a large set of pronoun res-
generation is especially understudied, we expand on prior olution questions using Wikipedia sentences. Yang et al.
work to devise and test four distractor-sampling strategies (2020) generate QA pair and distractors using generative
for effective question generation. We analyze their impact models.Regardingzero-shotevaluation,theSelf-Talkmodel
on model performance across tasks, conditioned on model of (Shwartz et al. 2020) generates clarification prompts
class and (pre-)training regime, and show that generating based on a template prefix, which are leveraged to elicit
questionsthataresimultaneouslyfairandinformativeisdif- knowledgefromanotherLM,whichisusedjointlywiththe
ficult but beneficial for LM pre-training. (3) We determine original context and question to score each answer candi-
which combination of knowledge graphs (KGs), data con- date. Given a task context, one can use COMET (Bosselut
struction/training,andarchitecturesismosteffectiveandcan and Choi 2019), a generative model trained on common-
utilizeappropriatelyrichcontextsacrossfivetasks.Weob- sense KGs, to generate background knowledge statements,
serve that diversifying knowledge generally improves per- and to compute scores for each answer candidate based on
formance, under the condition of it being aligned with the the context, question, and generated knowledge. Banerjee
task,andthatpreservingthestructureofthetaskisdesired. and Baral (2020) pre-train the LM with three representa-
(4)Wemakeourcodeandresultingdatasetsavailabletothe tionlearningfunctionswhichaimtocompleteaknowledge
communitytofacilitatefutureresearchinthisdirection.2 triplegiventwoofitselements.Thesefunctionsjointlycom-
pute the distance for each answer candidate. The ambition
RelatedWork of this paper is to provide a comprehensive framework for
suchprioreffortsonzero-shotQAwithKGs.Bycoveringa
KnowledgeInjection
widersetofKGs,questiongenerationtechniques,andtasks,
Strongperformanceonstandardmultiple-choiceQAbench- wecansystematicallyinvestigatetheeffectofusing differ-
marks, like SocialIQA and PhysicalIQA, has been entKGs,generationmethods,andtechniquesacrosstasks.
achieved by fine-tuning a task-specific prediction layer,
placed atop pre-trained LMs, such as BERT (Devlin et al. Zero-ShotQAFramework
2019),RoBERTa(Liuetal.2019),andGPT(Radfordetal.
GivenanaturallanguagequestionQ,andnpossibleanswers
2019). As shown by Ma et al. (2019) and Mitra et al.
A ;:::;A ,thetask istoselectthemostprobablesinglean-
(2019), combining neural methods with structured back- 1 n
swerA.Werefertotheremainingn(cid:0)1possibleanswers:
ground knowledge from ConceptNet, WordNet (Miller
D ;:::;D as distractors. In a zero-shot QA evaluation
1995),andATOMIC workswellforcommonsensedatasets 1 n(cid:0)1
mode, the system has no access to the task training or de-
that have been partially derived from these resources, such
velopmentdata.Weassumeasetupwherethesystemispre-
as SocialIQA and CommonSenseQA. Here, the struc-
trainedonceandthenappliedacrossdifferenttasksinazero-
turedknowledge,formalizedaslexicalizedtask-targetedev-
shotmanner.Ourzero-shotevaluationframeworkaddresses
idencepaths,isinjectedintoanLM,eitherviaanattention
this task by variants of pre-training an LM on an artificial
mechanism (Bauer, Wang, and Bansal 2018) or through an
QAset,createdfromKGdata.Next,wedescribeitscovered
auxiliarytrainingobjective(Xia,Wu,andYan2019).Graph
tasks,sourcesofknowledge,questiongenerationstrategies,
andrelationnetworkscanalsobeusedtoscoreanswercan-
LMtechniques,andtrainingregimes,inturn.
didates, by informing the graph structure with data from
LMs(Linetal.2019;Zhongetal.2019).Finally,complete
SyntheticQAGeneration
KGs can be incorporated directly in training by introduc-
We generate questions, answers, and distractor op-
ing additional modeling objectives, to teach a model about
tions from five KGs: ATOMIC, ConceptNet,
generalcommonsenseregardlessofthetaskathand(Peters
WordNet, VisualGenome (Krishna et al. 2017), and
etal.2019;Levineetal.2020;Liuetal.2020;Zhangetal.
Wikidata (Vrandecˇic´ and Kro¨tzsch 2014), found in the
2019; Talmor et al. 2020). This line of work resembles our
Commonsense Knowledge Graph (CSKG) (Ilievski
approach of including background knowledge in a general,
et al. 2020). Notably, ATOMIC differs from the other KGs
task-agnosticway;however,itstillreliesonthetasktraining
in two ways: 1) its relations have a different focus than
dataandhasgenerallynotbeentestedinazero-shotregime.
thoseoftheothersources;and2)itsnodelabelsarelonger
and formalized as templates. Due to these considerations,
GeneratingCommonsenseQuestionsandAnswers
wepreparetwosetsofQAsets:onebasedonATOMICand
RichardsonandSabharwal(2019)uselinksinWordNetto
onebasedontheremainingfourknowledgesources.Figure
generate question-answer pairs, then leverage the resulting
1illustratesourquestiongenerationpipeline.
dataset to evaluate language models. Petroni et al. (2019)
prompttheskillsoflanguagemodelsbysentencesinsteadof Data partitions ATOMIC expresses pre- and post-states
questions, generated from sources like ConceptNet and foreventsandtheirparticipantswithninerelations.Itshead
SQuAD(Rajpurkaretal.2016).Previousworkshavegener- nodes are events, whereas the tail nodes are either events
atedsyntheticQAsetstocomplementexistingtrainingdata. or attributes. Its nodes have two particularities: 1) irrele-
Yeetal.(2019)proposedan‘align-mask-select’methodto vant parts of the node text are replaced with blanks (‘ ’);
and2)referencestofictionalagentsareindicatedwithspe-
2https://github.com/Mayer123/HyKAS-CSKG cial tokens (e.g., PersonX). We follow the SocialIQA’s
13508
Question:Robintakesthefifth.Asaresult,Robinwantedto
A1:gotothecinema.
A2:withholdinformation.(*)
A3:hearwhattheythink.
Question:losingweightisfor
A1:beinghealthier.(*)
A2:embeddedsoftware.
A3:buyingthingsinstore.
Figure1:Anillustrationofourquestiongenerationpipeline.
Table1:GeneratedquestionsfromATOMIC (top)andCWWV
(bottom).(*)denotesthecorrectanswer.
ATOMIC train/dev/test splits, to ensure that the facts of the
devandtestpartitionsareexcludedintraining.
Our second partition, CWWV, covers three other KGs in r,t0). ConsideringtheexampleinFigure1,thetriple(gain-
CSKG that express commonsense facts between concepts: ing weight, CausesDesire, change appearance) will be fil-
ConceptNet, WordNet, and Wikidata. We use them teredoutbyrule(1),(losingweight,UsedFor,feelingbetter)
jointly to generate questions, and we enrich them with ad- will be ruled out by both (2) and (3), and (relaxing, Used-
ditional distractors from VisualGenome. Treating these For,feelingbetter)willberuledoutby(3).Here,wereplace
foursourcesasasingleoneisenabledbytheirCSKGmap- anyreferencestofictionalATOMICagentsinthedistractors
ping to a single set of relations, defined by ConceptNet. withthesamenamesusedinthequestion.Wethenrandomly
We focus on 14 semantic relations that are grounded on selecttwodistractors(D ,D )fromD.Werefertothisdis-
1 2
strong psycholinguistic and pragmatic evidence (Murphy tractorpoolingstrategyasrandom,andproposethreealter-
2003), like /r/Causes and /r/HasPrerequisite. nativestrategiesinthenextSection.
Since there is no pre-defined train/dev/test split for CSKG, ExamplequestionswitheachpartitionareshowninTable
we randomly sample 5% of generated questions as devel- 1.ForATOMIC,thisproceduregenerates535KQApairsfor
opment set, while the other 95% are used for training, to training and 60K for development. For CWWV, the training
maximizethecoverageoftheknowledge. setcontains157Kandthedevsethas8KQApairs.
Generatingquestionsandanswers Ifatriple(h,r,t)has
DistractorSampling
an associated sentence, we directly employ it for question
generation; otherwise, we generate a sentence in a lexical- Existing data generation procedures are likely to introduce
ization step, using a set of pre-defined templates. Next, we annotation artifacts in datasets (Zellers et al. 2019; Sak-
generatethequestionQbyremovingthetailofthesentence, aguchi et al. 2019). Models may exploit these artifacts to
and extract this tail as the correct answer, A. Here, we en- achieve spuriously strong performance during training, at
surethatthereisnotokenoverlapbetweentheheadandthe theexpenseofdegradationinrobustness.Togeneratemore
correct answer. For ATOMIC, we: 1) compare the keyword challenging QA pairs from KGs and to alleviate potential
tokensinsteadofalltokens,inordertoavoidstopwords;and biasesinoursyntheticsets,wetesttwootherdistractorsam-
2) the agent templates (e.g., ‘PersonX’) are replaced with pling strategies in addition to the random strategy: 1) we
randomlysampledgender-neutralnamesfromapre-defined select distractors that are as similar as possible to the an-
set.ForCWWV,wefilteroutquestionswhereeitherthehead swer, while being under a certain threshold (adv-answer);
orthetailarenotcommonconceptsortheyarenamedenti- and 2) we select distractors that are as similar as possible
ties.Weusecorpusfrequencyasaproxyforcommonness,3 tothequestion,whilebeingunderacertainthreshold(adv-
while named entities are filtered by removing all concepts question).Herewedefinesimilarityoftwonodestobetheir
whoselabelsstartwithacapitalletter. proximityintheembeddingspace,measuredbycosinesim-
ilarity.Theintuitionisthat,bygeneratingmorechallenging
Generating negative samples (distractors) We seek to QA pairs for the models, we could achieve better general-
generate distractor options that satisfy two criteria: infor- izationacrosstasks.WeusetheRoBERTasentenceembed-
mativeness and fairness. Namely, a good distractor has se- ding model (Reimers and Gurevych 2020) to compute em-
manticrelatednesswiththecontext(informative),whilebe- beddingsforallKGnodes.Forthesetwostrategies,weset
ing relatively easy to discriminate from the correct answer an upper bound on the similarity score to avoid unfair dis-
(fair). We create the pool of distractors D for every sam- tractors, i.e., paraphrases of the correct answer. Based on
ple as follows: 1. The distractor candidates are the tails of manualobservations,wesettheirdistractorsimilarityupper
knowledgetriples(h0,r0,t0)withthesamerelationr0 = r, boundtobe0.6forCWWVand0.4forATOMIC.
randomlysampledfromtheKGs.Thiswouldensurethatthe
distractorscanfillthesamesemanticroleasthecorrectan- Samplefiltering Besidesthesedistractorsamplingstrate-
swer. 2. The head h0 of the sampled triples does not have gies,wetestanothercondition(3),whereweselectthedis-
non-stopwordoverlapwithh.3.Thedistractortailt0 isnot tractors randomly, but only keep the questions whose dis-
partofthecorrectanswerset,i.e.,thereexistnotriples,(h, tractors are sufficiently challenging at training time (adv-
filter). The intuition is that QA pairs generated using the
3https://pypi.org/project/wordfreq/(Accessed9Sept.2020) aforementionedmethodsmightstillbetooeasyforthemod-
13509
els, thus we would like to only keep the most challenging Here,(cid:17) representsthemarginandy istheindexofthecor-
subsettotrainourmodels.WeemploytheAFLitealgorithm rectanswer.ForaMLMmodel,thecomputationcostforthe
(Sakaguchi et al. 2019) for our purpose. Given a train and scoring function scales quadratically with the input length.
devsplitofoursyntheticQAset,weuse5%ofthetrainset Tomakethetrainingmoreefficient,weonlymaskoutnon-
tofinetuneaRoBERTamodelwithaclassificationhead(4% stoptokensintheheadandtailnodes.
training,1%validation).These5%arediscardedfromtrain
TrainingRegimes Inordertodisentanglethecontribution
after this step. We then compute the fixed embeddings for
oftheKGsfromthestructureoftheQApairs,weconsider
theremaining95%oftrainandtheentiredev,denotedasTrn
different training methods for augmentation of language
andDev.Next,wefeedTrnandDevalongwiththeirlabels
modelswithKGs.Specifically,wecomparemarginalrank-
to the AFLite algorithm, which iteratively filters out easy
ing(MR)trainingwithmaskedlanguagemodeling(MLM)
examplesusinganensembleoflinearclassifiers.Finally,we
training.ForMLM,wedirectlyconcatenatethequestionand
retain (101K training, 11K dev) samples for ATOMIC and
the correct answer in our synthetic QA set and then train
(29Ktraining,1.5Kdev)samplesforCWWVsubset.Thede-
RoBERTaonthethesesentencesusingtheMLMobjective.
tailsofAFLitecanbefoundintheappendix.
Tasks
LanguageModels
Weselectcommonsensetasksbasedontwocriteria.Firstly,
We consider 2 types of language models: auto-regressive westrivetocoveradiversesetoftasks,bothintermsoftheir
language models and masked language models (MLM). format(questionanswering,pronounresolution,naturallan-
Specifically,weuseGPT-2andRoBERTatoselectthebest guage inference), as well as their type of knowledge (e.g.,
answer candidate. Given a context C, a question Q, and a social or physical knowledge). Secondly, we prefer larger
listofansweroptions(A ;A :::),weconcatenateC andQ taskdatasetsthataremanuallyconstructed.Forthisreason,
1 2
witheachansweroptiontobuildinputsequences(T ;T :::). we do not include datasets like COPA (Gordon, Kozareva,
1 2
WealsousetemplatestoconvertasequenceT intoanatu- and Roemmele 2012), or HellaSwag (Zellers et al. 2019).
ral language sentence following (Shwartz et al. 2020). For Weoptforthefollowingfivetaskdatasets:
example,wetransformthesequence:[C]WhatwillXwant
1. Abductive NLI (aNLI) (Bhagavatula et al. 2019) is
todonext?[A ]into:[C],asaresult,Xwantto[A ].The
i i posedasanaturallanguageinferencetask.Giventhebegin-
score S for the resulting sequence using an auto-regressive
ningandtheendingofastory,thetaskistochoosethemore
LMiscomputedasfollows:
plausiblehypothesisoutoftwooptions.Thedatasetconsists
n ofnearly170kentries.
1 X
S (T)=(cid:0) logP (t jt :::t ) (1)
LM n i 1 i(cid:0)1 2.CommonsenseQA(CSQA) (Talmoretal.2019)eval-
i=1 uates a broad range of common sense aspects. Each entry
where n is the number of tokens in the sequence and P is containsaquestionand5answercandidates.Thequestions
the conditional probability provided by the LM. To evalu-
arecrowdsourcedbasedonasubgraphfromConceptNet.
ate MLMs, we mask out one token at a time and compute
TheanswercandidatescombineConceptNetnodeswith
itsloss(Zhouetal.2020).Werepeatthisprocessforevery additionalcrowdsourceddistractors.
tokeninthesequence.ThefinalMLMscoreis: 3.PhysicalIQA(PIQA) (Bisketal.2020)isatwo-choice
question answering dataset which focuses on physical rea-
n
1 X soning.Givenaquestion,thesystem(orhuman)isaskedto
S (T)=(cid:0) logP (t j:::t ;t :::) (2)
MLM n i i(cid:0)1 t+1 pickthemoreplausibleoutoftwopossiblecontinuations.
i=1
4. SocialIQA (SIQA) (Sap et al. 2019b) is a question-
Thepredictedoptionistheonewiththelowestscore.
answeringdatasetwhichrequiresreasoningaboutsocialin-
LMFinetuning Inthetypicalmodelarchitectureforfine- teractions. Each entry contains a context, a question, and 3
tuningLMformultiple-choicetasks,alinearlayerisadded answercandidates.ThecontextisderivedfromtheATOMIC
ontopoftheLMencodertopredicttheanswer.Themodel knowledgegraph,thequestionsaregeneratedbasedonnine
inputs are separated by a model-specific delimiter. How- templates (corresponding to the relations in ATOMIC), and
ever,asthisarchitectureintroducesrandomlyinitializedpa- theanswersarecrowdsourced.
rameters, it may not be able to fully utilize the pre-trained 5. WinoGrande (WG) (Sakaguchi et al. 2019) contains
weights (Tamborrino et al. 2020). Instead, we re-use the 44 thousand pronoun resolution problems. Each entry con-
GPT-2andRoBERTawithLMheadforfinetuning.Bykeep- sists of a context description with an emphasized pronoun,
ingthemodelintact,wecanreusethesameconvertingtem- andtwooptionsareofferedasitspossiblereferences.
plates and scoring functions. To train the model, given the
scores computed for each answer candidate S 1;S 2;:::S m, ExperimentalSetup
weusethemarginalranking(MR)lossdefinedas:
Baselines
1 Xm We compare our results with the following baselines. Ma-
L=
m
max(0;(cid:17)(cid:0)S y+S i) (3) jority answers each question with the most frequent op-
i=1 tionintheentiredataset.‘Vanilla’versionsofthelanguage
i6=y
13510
Model KG aNLI CSQA PIQA SIQA WG
Majority - 50.8 20.9 50.5 33.6 50.4
GPT2-L - 56.5 41.4 68.9 44.6 53.2
RoBERTa-L - 65.5 45.0 67.6 47.3 57.5
Self-talk (Shwartzetal.2020) - - 32.4 70.2 46.2 54.7
COMET-DynaGen(BosselutandChoi2019) ATOMIC - - - 50.1 -
SMLM (BanerjeeandBaral2020) * 65.3 38.8 - 48.5 -
GPT2-L(MR) ATOMIC 59:2((cid:6)0:3) 48:0((cid:6)0:9) 67:5((cid:6)0:7) 53:5((cid:6)0:4) 54:7((cid:6)0:6)
GPT2-L(MR) CWWV 58:3((cid:6)0:4) 46:2((cid:6)1:0) 68:6((cid:6)0:7) 48:0((cid:6)0:7) 52:8((cid:6)0:9)
GPT2-L(MR) CSKG 59:0((cid:6)0:5) 48:6((cid:6)1:0) 68:6((cid:6)0:9) 53:3((cid:6)0:5) 54:1((cid:6)0:5)
RoBERTa-L(MR) ATOMIC 70:8((cid:6)1:2) 64:2((cid:6)0:7) 72:1((cid:6)0:5) 63:1((cid:6)1:5) 59:6((cid:6)0:3)
RoBERTa-L(MR) CWWV 70:0((cid:6)0:3) 67:9((cid:6)0:8) 72:0((cid:6)0:7) 54:8((cid:6)1:2) 59:4((cid:6)0:5)
RoBERTa-L(MR) CSKG 70:5((cid:6)0:2) 67:4((cid:6)0:8) 72:4((cid:6)0:4) 63:2((cid:6)0:7) 60:9((cid:6)0:8)
RoBERTa-L(supervised) - 85.6 78.5 79.2 76.6 79.3
Human - 91.4 88.9 94.9 86.9 94.1
Table2:Zero-shotevaluationresultswithdifferentcombinationsofmodelsandknowledgesources,acrossfivecommonsense
tasks. CSKG represent the combination of ATOMIC and CWWV. We run our experiments three times with different seeds and
report average accuracy with 95% confidence interval. SMLM (*) used OMCS for CSQA, ROCStories (Mostafazadeh et al.
2016)foraNLIandATOMICforSIQAasknowledgeresources.
models are used in order to understand the impact of fur- H4: Adding diverse knowledge (from different KGs) im-
ther tuning. Here we directly uses the LMs to score the provesperformance.Thisistheinitialmotivationbehind
QA pairs without any finetuning. We also show the results the creation of CSKG (Ilievski et al. 2020), but has not
ofotherunsupervisedsystemsthatleverageKGs:Self-talk, beeninvestigatedindetail.
COMET-DynaGen,andSMLM.Toindicatetheupperbound
H5: When selecting negative samples for a question, it
of this work, we include results of a supervised fine-tuned
helpstouseanadversarialstrategythatensurestheques-
RoBERTasystemandofhumanevaluation.
tion is not trivial for a language model. H5 is inspired
by adversarial filtering, which has not been investigated
Implementation
indetailforautomatically-generatedquestionsandacross
FortheLMbaselines,wedirectlyloadtheweightsfromthe KGs.
Transformerslibrary(Wolfetal.2019)andevaluateonthe H6: Preserving the task structure when generating syn-
downstreamtasks.ThefinetunedLMsaretrainedforasin- thetic data leads to better accuracy. This is implicitly
gle epoch on our synthetic QA set. For Adv-filter, we train assumed in prior data augmentation work (Kocijan et al.
themodelsfor5epochstocompensateforlesstrainingdata. 2019).
We use our synthetic dev set to select the best model. We
H7:Theautomaticallycreatedquestionsarenotablyeasier
describeotherhyper-parametersusedandcomputinginfras-
forhumansthantheyareformachines-ageneralassump-
tructureintheappendix.
tion made by commonsense task creators and typically
correctforanyexisting,human-generatedbenchmark.
Hypotheses
Basedonindividualpriorfindingsandunderstandingofdif- Results
ferent components of our framework, we put forward a set
We evaluate various combinations of: knowledge sources,
ofhypotheseswhichwillbevalidatedinourexperiments:
question generation strategies, LMs, training regimes, and
H1:RoBERTawouldhavebetterperformancethanGPT-2. tasks. We use accuracy as a metric. All our experiments
This is in line with prior findings that RoBERTa has the areperformedinazero-shotsetting,i.e.,themodelsdonot
advantageofbi-directionalcontext(Zhouetal.2020). leverage the official training data of the task. We report re-
sults on the dev sets of these tasks, as the official test sets
H2:Pre-trainingalanguagemodelwithartificiallycreated
arenotpubliclyavailable.Wenotethat,sincewedidnotuse
question-answer sets enhances zero-shot performance.
thetasks’devsetsforhyperparametertuningorcheckpoint
This is also supported in previous study about unsuper-
selection,thedevsetscanbeusedeffectivelyastestsets.
visedQA(Lietal.2020)
H3: The impact of more knowledge depends on the align- MainResults
ment between KGs and the task, partial evidence for Table2showsthatGPT-2andRoBERTaoutperformthema-
whichisprovidedby(Maetal.2019;Mitraetal.2019). joritybaselinebyalargemarginonalltasks,indicatingthat
13511
RoBERTa-L Strategy aNLI CSQA PIQA SIQA WG
+ATOMIC Random 70:8((cid:6)1:2) 64:2((cid:6)0:7) 72:1((cid:6)0:5) 63:1((cid:6)1:5) 59:6((cid:6)0:3)
+ATOMIC Adv-answer 70:4((cid:6)0:8) 62:3((cid:6)0:9) 72:6((cid:6)1:8) 61:6((cid:6)0:3) 60:5((cid:6)0:5)
+ATOMIC Adv-question 70:8((cid:6)0:6) 55:6((cid:6)0:9) 70:6((cid:6)0:8) 51:6((cid:6)0:8) 58:5((cid:6)0:3)
+ATOMIC Adv-filter 68:6((cid:6)1:8) 46:4((cid:6)1:5) 67:9((cid:6)1:1) 51:8((cid:6)1:2) 60:8((cid:6)0:6)
+CWWV Random 70:0((cid:6)0:3) 67:9((cid:6)0:8) 72:0((cid:6)0:7) 54:8((cid:6)1:2) 59:4((cid:6)0:5)
+CWWV Adv-answer 69:5((cid:6)1:1) 68:5((cid:6)0:8) 72:7((cid:6)0:3) 53:8((cid:6)0:6) 60:7((cid:6)0:7)
+CWWV Adv-question 68:3((cid:6)2:3) 60:9((cid:6)2:3) 69:6((cid:6)0:6) 47:0((cid:6)2:0) 59:0((cid:6)1:4)
+CWWV Adv-filter 69:7((cid:6)0:7) 64:7((cid:6)2:3) 72:0((cid:6)1:3) 50:1((cid:6)1:0) 59:4((cid:6)1:4)
Table3:ComparisonofdifferentQAgenerationstrategies.
the LMs have already learned relevant knowledge during ComparisonofTrainingRegimes
pretraining. Despite being a smaller model, RoBERTa out-
Table4presentsresultswithtwodifferenttrainingregimes.
performsGPT-2on4outof5taskswithoutpretraining,and
In comparison to the baseline without additional training,
on all tasks when pretraining over different synthetic QA
MLMtrainingonATOMIConlyimprovesontheSIQAtask,
sets. This shows the advantage of leveraging bi-directional
and harms on the rest. With CWWV, it brings large gain on
context,andconfirmsourhypothesisH1.Asexpected(H2),
CSQA and small improvements on SIQA and WG. At the
training RoBERTa on our ATOMIC or CWWV synthetic sets
sametime,marginalrankingtrainingoneitherquestionset
bringsnotableperformancegainonall5tasks.Weobserve
consistently outperforms MLM training by a large margin,
thatmodelstrainedonATOMICsetshavealargeadvantage
suggestingthatpreservingthetaskstructureisbeneficialin
onSIQAcomparetomodelstrainedonCWWV,whileCWWV
additiontothequestioncontentandvalidatingH6.
brings advantage on the CSQA task. This is not surpris-
ing asthese two tasks arederived from ConceptNet and
DifficultyoftheSyntheticQASets
ATOMIC,respectively.ThedifferencebetweenATOMICand
CWWVontheremainingthreetasksisrelativelysmall.This Ideally,thegeneratedquestion-answerpairsshouldbechal-
supportsourhypothesisH3:knowledgealignmentiscrucial lenging for the models but easy for humans to solve (H7).
forobtainingbetterperformance. Here,weprobethishypothesisbyassessingthedifficultyof
Trainingonthecombinedquestionset(CSKG)ismostly our synthetic QA sets both by humans and ‘vanilla’ LMs.
able to retain the best of its both partitions. Training on We evaluated both models on the dev sets of our synthetic
CSKG leads to best performance on three out of five tasks, data. For human evaluation, we randomly sample 50 ques-
showingthataglobalcommonsenseresourceisabletobring tionsfromATOMICand50questionsfromCWWV.Atotalof
consistentgainacrossdifferenttasks.Thissupportsourhy- five researchers were asked to first provide the correct an-
pothesis H4: adding more diverse knowledge is beneficial swer,thenratethequestiondifficulty.Forthelatter,thean-
forlanguagemodels.Finally,evenwiththisknowledge,we notatorchosebetweeneasy,moderate,hard,ornon-sensical
recognizethatthereisstillalargegapbetweenourmodel’s - as a guideline, nonsensical questions have unfair distrac-
accuracyandthatofthesupervisedRoBERTamodel. torsandcannotbeeasilyunderstood.Followingthisproce-
dure,weobtainedthreejudgmentsforeachquestion.
ComparisonofQAGenerationStrategies Theinter-annotatoragreementonselectingthecorrectan-
Table3showstheresultswithdifferentsamplingstrategies, swer is 0.62 using Fleiss Kappa score, which is substan-
thus addressing H5. The best performing adversarial algo- tial agreement. The Kripendorf alpha (Krippendorff 2004)
rithm, Adv-answer, yields comparable accuracy to the ran- for rating question difficulty is 0.35, which is fair agree-
domstrategy,revealingthatdistractorssampledwithamore ment. The results of the baseline LMs and human perfor-
sophisticated strategy are not necessarily more informative mance (Table 5) show that the ATOMIC subset presents a
for the LMs. Adv-question and Adv-filter typically lead to harder challenge for both models, as well as for humans.
declines in accuracy. Considering Adv-question, this could Overall,theresultssupportourhypothesisH7:thesynthetic
be due to the similarity of the distractors to the question, questionsarerelativelyeasyforhumanstosolveandmuch
whichmightguidethemodeltolearntopickthemostdis- harderformodels.However,theannotationpointedtosev-
similarcandidateasthecorrectanswer,whichisanartifact eraldirectionsforimprovingthesyntheticQAsets.Anum-
of our question generation and cannot be expected to work ber of questions generated from ATOMIC are ungrammat-
wellfordownstreamtasks. Ourmanualinspectionofthere- ical, which makes them harder to understand, while some
mainingquestionspreferedbyAdv-filterindicatesthatmany questionsfromCWWVwereratedasunfair.Forexample,all
questionsareunfair,assomedistractorsarealsocorrectan- answer options for the question A person can are valid: (a)
swers,whichisaconsequenceoftheincompletenessofthe cost several thousand dollars (b) expressing boredom (c)
KGs. Adv-filterprioritizesthesequestionsastheyare“dif- check snow level. As discussed earlier, this is due to the
ficult” for LMs, however, training on them might teach the incompleteness of our KGs, and the current lack of under-
LMincorrectknowledgeandharmdownstreamaccuracy. standingonhowtogeneratefair,yetinformative,distractors.
13512
RoBERTa-L Train aNLI CSQA PIQA SIQA WG Model ATOMIC CWWV
baseline - 65.5 45.0 67.6 47.3 57.5 GPT2-L 43.2 69.5
+ATOMIC MLM 62.9 43.8 65.8 53.9 55.5 RoBERTa-L 45.9 64.5
+ATOMIC MR 70.8 64.2 72.1 63.1 59.6 Human 78.0 80.7
+CWWV MLM 65.3 57.3 67.2 49.3 59.4
+CWWV MR 70.0 67.9 72.0 54.8 59.4 Table5:LMandhumanaccuracyonoursyntheticQAsets.
Table4:ComparisonbetweenMLMandMRtraining.
thisfindingpointstoamoresubstantialchallenge:generat-
ingfairandinformativemultiple-choicequestionsisnotyet
Discussion well-understood. Adversarial strategies yield more plausi-
ble candidates than random sampling, making the task less
TowardsaCommonsenseService
fair; yet, fully relying on random sampling would generate
The overarching pursuit of this paper is to understand distractorsthataretriviallydiscerniblefromthecorrectop-
whethergeneratingartificialQAsetswithKGsimprovesthe tion.Balancingbetweenfairnessandinformativeness,thus,
zero-shotQAperformanceofLMs.Weobserveaconsistent isessentialformultiple-choicequestiongeneration.Ourem-
leapinaccuracyacrosstasks,LMs,knowledgesources,and pirical evidence suggests that it could be achieved by a
question generation strategies. While these accuracies are mixedapproach,wherepartofdistractorsisgeneratedran-
notablybelowsupervisedLMaccuracies,theymightfurther domly,andpartbyadoptingsuitableadversarialstrategies.5
improvebyarchitecturalimprovementsoftheLMs,knowl-
edge sources with wider coverage and stronger semantics, Conclusions
and well-tuned scoring functions.4 In addition, despite its
Whilezero-shotQAisgainingfocus,nostudysofarhasin-
complexity and diversity, commonsense knowledge (unlike
vestigatedsystematicallythedependencybetweenthetask,
knowledgeonentitiesandevents,whichchangesrapidly)is
the technique, and any additional knowledge used. To ad-
largelystaticandevolvesslowlyovertime,thusmakingthe
dress this gap, this paper proposed a framework for zero-
dataset-specific finetuning unnecessary in theory. A natural
shotQAbypretrainingLMsonartificialdatacreatedbased
questionarises:canwebuildasufficientlyreliable,general
on KGs. We studied the interplay between five knowledge
commonsenseservice,bypretrainingaLMonarichsetof
sources,fourdatagenerationstrategies,twoLMs,twotrain-
questionscoveringawidespectrumofknowledgetypes?
ingregimes,andfivetasks.Weputforwardsevenhypothe-
ses, which guided our experiments. We observed that lan-
ImpactofKnowledge guagemodelsbenefitfrompretrainingonartificiallycreated
In general, we observed that using knowledge from a QAsets.Similartopriorwork,weobservedthatthebestper-
wider set of sources is beneficial. However, on aNLI and formanceisobtainedbytheRoBERTamodel.Furthermore,
CommonSenseQA, the best accuracy was obtained with combiningknowledgesourcesleadstobestperformance,in-
a subset of all questions. This could be due to the dicating that diverse relevant knowledge is desired for pre-
kinds of knowledge covered: 1. aNLI focuses on ex- training. However, this is conditioned on a good alignment
pectations of agents and events, for which ATOMIC is between knowledge and the task. The training regime had
directly useful, whereas the other KGs might be delu- a role too: marginal ranking is superior to vanilla language
sive; 2. CommonSenseQA mostly requires knowledge modeling,asitpreservedbetterthestructureofthetask.Our
about properties of objects (e.g., function or appearance) analysis and human evaluation indicated that the generated
which is the focus of ConceptNet, Wikidata, and questionsweretypicallyeasierforhumansthanforlanguage
VisualGenome,butnotofATOMIC.Thisindicatesaten- models,whichisappropriateforcommonsensequestions.
sionbetweenH3andH4:whilemoreknowledgeoftenhelps, Yet,thehumanevaluationrevealedthatanotableportion
itmightnotbethecasewhenthetaskandtheknowledgeare of the questions is nonsensical or difficult, hinting that au-
not well-aligned. Our current understanding of the dimen- tomatically generating high-quality, informative common-
sionsofcommonsenseknowledgeinknowledgesourcesand sensequestions isnon-trivial andshould be revisedin sub-
benchmarksislimited,andwouldbenefitfromfurtherstudy. sequent work. Future work should also investigate the im-
pact of this approach on knowledge injection systems (Ma
etal.2019)andgraphrelationalnetworks(Linetal.2019).
GeneratingFairandInformativeQuestions
It should also consider: (1) other, less structured knowl-
Alternatively, this result may be explained by our human edge sources, like WikiHow; (2) different distractor sam-
evaluation:notallautomaticallygeneratedquestionsarefair plingstrategies,e.g.,basedoniterativesampling(Niuetal.
and a subset has more than one correct answer, as a direct 2020);and(3)additionalLMscoringfunctions,e.g.,based
consequence of the inherent incompleteness of KGs. Be- onscoringsequencesoftokens(Tamborrinoetal.2020).
sidesbeingadisadvantageofautomaticquestiongeneration,
5Question formulation is another challenge. Template-based
4Forexample,scoringsequencesoftokensbyalanguagemodel questionsmaybetriviallyeasyforLMstosolve,asdiscussedin
mightimprovetheperformanceofLMs(Tamborrinoetal.2020). https://cs.nyu.edu/faculty/davise/papers/CYCQns.html.
13513
Acknowledgements Y. 2020. SenseBERT: Driving Some Sense into BERT. In
Proc.ofACL,4656–4667.
We would like to thank all of the anonymous reviewers
for their valuable feedback. We also thank Ehsan Qasemi Li,Z.;Wang,W.;Dong,L.;Wei,F.;andXu,K.2020. Har-
and Soumyaroop Nandi for participating in our human an- vesting and Refining Question-Answer Pairs for Unsuper-
notation study. This material is based upon work spon- visedQA. InProc.ofACL,6719–6728.
sored by the DARPA MCS program under Contract No.
Lin,B.Y.;Chen,X.;Chen,J.;andRen,X.2019. KagNet:
N660011924033withtheUnitedStatesOfficeOfNavalRe-
Knowledge-AwareGraphNetworksforCommonsenseRea-
search.
soning. InProc.ofEMNLP-IJCNLP,2829–2839.
References Liu,W.;Zhou,P.;Zhao,Z.;Wang,Z.;Ju,Q.;Deng,H.;and
Banerjee, P.; and Baral, C. 2020. Self-supervised Knowl- Wang, P. 2020. K-BERT: Enabling Language Representa-
edge Triplet Learning for Zero-shot Question Answering. tionwithKnowledgeGraph. InAAAI.
ArXivabs/2005.00316. Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
Bauer, L.; Wang, Y.; and Bansal, M. 2018. Commonsense Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.
for Generative Multi-Hop Question Answering Tasks. In 2019. RoBERTa:ARobustlyOptimizedBERTPretraining
Proc.ofEMNLP,4220–4230. Approach. ArXivabs/1907.11692.
Bhagavatula, C.; Bras, R. L.; Malaviya, C.; Sakaguchi, K.; Ma, K.; Francis, J.; Lu, Q.; Nyberg, E.; and Oltramari, A.
Holtzman, A.; Rashkin, H.; Downey, D.; Yih, S. W.-t.; and 2019. TowardsGeneralizableNeuro-SymbolicSystemsfor
Choi, Y. 2019. Abductive commonsense reasoning. arXiv Commonsense Question Answering. In Proc. of the First
preprintarXiv:1908.05739. WorkshoponCommonsenseInferenceinNaturalLanguage
Processing,22–32.
Bisk,Y.;Zellers,R.;Bras,R.L.;Gao,J.;andChoi,Y.2020.
PIQA: Reasoning about Physical Commonsense in Natural Miller, G. A. 1995. WordNet: A Lexical Database for En-
Language. In Thirty-Fourth AAAI Conference on Artificial glish. Commun.ACM38(11):3941. ISSN0001-0782.
Intelligence,7432–7439.
Mitra, A.; Banerjee, P.; Pal, K. K.; Mishra, S.; and Baral,
Bosselut, A.; and Choi, Y. 2019. Dynamic Knowledge C. 2019. Exploring ways to incorporate additional knowl-
Graph Construction for Zero-shot Commonsense Question edgetoimproveNaturalLanguageCommonsenseQuestion
Answering. ArXivabs/1911.03876. Answering. arXivpreprintarXiv:1909.08855.
Bras, R. L.; Swayamdipta, S.; Bhagavatula, C.; Zellers, R.; Mostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Ba-
Peters,M.E.;Sabharwal,A.;andChoi,Y.2020.Adversarial tra, D.; Vanderwende, L.; Kohli, P.; and Allen, J. 2016. A
FiltersofDatasetBiases. arXivpreprintarXiv:2002.04108 Corpus and Cloze Evaluation for Deeper Understanding of
. Commonsense Stories. In Proceedings of the 2016 Con-
Devlin,J.;Chang,M.-W.;Lee,K.;andToutanova,K.2019. ference of the North American Chapter of the Association
BERT:Pre-trainingofDeepBidirectionalTransformersfor forComputationalLinguistics:HumanLanguageTechnolo-
LanguageUnderstanding. InProc.ofNAACL,4171–4186. gies,839–849.SanDiego,California:AssociationforCom-
putational Linguistics. doi:10.18653/v1/N16-1098. URL
Gordon, A.; Kozareva, Z.; and Roemmele, M. 2012.
https://www.aclweb.org/anthology/N16-1098.
SemEval-2012Task7:ChoiceofPlausibleAlternatives:An
EvaluationofCommonsenseCausalReasoning. InProc.of Murphy, M. L. 2003. Semantic relations and the lexicon:
*SEM),394–398. Antonymy,synonymyandotherparadigms.CambridgeUni-
versityPress.
Ilievski, F.; Szekely, P.; Cheng, J.; Zhang, F.; and Qasemi,
E. 2020. Consolidating Commonsense Knowledge. arXiv Niu, Y.; Jiao, F.; Zhou, M.; Yao, T.; Xu, J.; and Huang, M.
preprintarXiv:2006.06114. 2020. ASelf-TrainingMethodforMachineReadingCom-
prehension with Soft Evidence Extraction. arXiv preprint
Kocijan, V.; Cretu, A.-M.; Camburu, O.-M.; Yordanov, Y.;
arXiv:2005.05189.
andLukasiewicz,T.2019. ASurprisinglyRobustTrickfor
the Winograd Schema Challenge. In Proc. of ACL, 4837– Peters,M.E.;Neumann,M.;Logan,R.;Schwartz,R.;Joshi,
4842. V.;Singh,S.;andSmith,N.A.2019. KnowledgeEnhanced
Krippendorff, K. 2004. Content Analysis: An Introduction Contextual Word Representations. In Proc. of EMNLP-
toItsMethodology(secondedition). SagePublications. IJCNLP,43–54.
Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; Petroni,F.;Rockta¨schel,T.;Lewis,P.;Bakhtin,A.;Wu,Y.;
Kravitz, J.; Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma, Miller, A. H.; and Riedel, S. 2019. Language models as
D.A.;etal.2017. Visualgenome:Connectinglanguageand knowledgebases? arXivpreprintarXiv:1909.01066.
visionusingcrowdsourceddenseimageannotations. Inter-
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and
nationaljournalofcomputervision123(1):32–73.
Sutskever, I. 2019. Language Models are Unsupervised
Levine, Y.; Lenz, B.; Dagan, O.; Ram, O.; Padnos, D.; Multitask Learners. OpenAI URL https://openai.com/blog/
Sharir, O.; Shalev-Shwartz, S.; Shashua, A.; and Shoham, better-language-models/.
13514
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. Ye,Z.-X.;Chen,Q.;Wang,W.;andLing,Z.-H.2019.Align,
Squad: 100,000+ questions for machine comprehension of mask and select: A simple method for incorporating com-
text. arXivpreprintarXiv:1606.05250. monsense knowledge into language representation models.
arXivpreprintarXiv:1908.06725.
Reimers, N.; and Gurevych, I. 2020. Making Monolin-
gual Sentence Embeddings Multilingual using Knowledge Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi,
Distillation. arXiv preprint arXiv:2004.09813 URL http: Y. 2019. HellaSwag: Can a Machine Really Finish Your
//arxiv.org/abs/2004.09813. Sentence? InProc.ofACL,4791–4800.
Zhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu,
Richardson, K.; and Sabharwal, A. 2019. What does my
Q.2019. ERNIE:EnhancedLanguageRepresentationwith
QA model know? devising controlled probes using expert
InformativeEntities. InProc.ofACL,1441–1451.
knowledge. arXivpreprintarXiv:1912.13337.
Zhong,W.;Tang,D.;Duan,N.;Zhou,M.;Wang,J.;andYin,
Sakaguchi, K.; Bras, R. L.; Bhagavatula, C.; and Choi, Y.
J.2019. ImprovingQuestionAnsweringbyCommonsense-
2019. Winogrande: An adversarial winograd schema chal-
BasedPre-training.InTang,J.;Kan,M.-Y.;Zhao,D.;Li,S.;
lengeatscale. arXivpreprintarXiv:1907.10641.
and Zan, H., eds., Natural Language Processing and Chi-
Sap,M.;Bras,R.L.;Allaway,E.;Bhagavatula,C.;Lourie, neseComputing,16–28.Cham:SpringerInternationalPub-
N.;Rashkin,H.;Roof,B.;Smith,N.A.;andChoi,Y.2019a. lishing.
ATOMIC:AnAtlasofMachineCommonsenseforIf-Then
Zhou, X.; Zhang, Y.; Cui, L.; and Huang, D. 2020. Eval-
Reasoning. InProc.ofAAAI,3027–3035.
uating Commonsense in Pre-trained Language Models. In
Sap, M.; Rashkin, H.; Chen, D.; Le Bras, R.; and Choi, Y. AAAI.
2019b. Social IQa: Commonsense Reasoning about Social
Interactions. InProc.ofEMNLP-IJCNLP,4463–4473.
Shwartz,V.;West,P.;Bras,R.L.;Bhagavatula,C.;andChoi,
Y.2020. UnsupervisedCommonsenseQuestionAnswering
withSelf-Talk. ArXivabs/2004.05483.
Speer,R.;Chin,J.;andHavasi,C.2017.ConceptNet5.5:An
Open Multilingual Graph of General Knowledge. In Proc.
ofAAAI,AAAI17,44444451.
Talmor,A.;Herzig,J.;Lourie,N.;andBerant,J.2019.Com-
monsenseQA: A Question Answering Challenge Targeting
CommonsenseKnowledge.InProc.ofNAACL,4149–4158.
Talmor, A.; Tafjord, O.; Clark, P.; Goldberg, Y.; and Be-
rant, J. 2020. Teaching Pre-Trained Models to System-
atically Reason Over Implicit Knowledge. arXiv preprint
arXiv:2006.06609.
Tamborrino, A.; Pellicano`, N.; Pannier, B.; Voitot, P.; and
Naudin, L. 2020. Pre-training Is (Almost) All You Need:
An Application to Commonsense Reasoning. In Proc. of
ACL,3878–3887.
Vrandecˇic´, D.; and Kro¨tzsch, M. 2014. Wikidata: a free
collaborativeknowledgebase. CommunicationsoftheACM
57(10):78–85.
Wolf,T.;Debut,L.;Sanh,V.;Chaumond,J.;Delangue,C.;
Moi,A.;Cistac,P.;Rault,T.;Louf,R.;Funtowicz,M.;and
Brew, J. 2019. HuggingFace’s Transformers: State-of-the-
artNaturalLanguageProcessing. ArXivabs/1910.03771.
Xia, J.; Wu, C.; and Yan, M. 2019. Incorporating Rela-
tion Knowledge into Commonsense Reading Comprehen-
sion with Multi-Task Learning. In Proc. of CIKM, CIKM
19,23932396. ISBN9781450369763.
Yang, Y.; Malaviya, C.; Fernandez, J.; Swayamdipta, S.;
Bras, R. L.; Wang, J.; Bhagavatula, C.; Choi, Y.; and
Downey, D. 2020. G-DAUG: Generative Data Augmenta-
tionforCommonsenseReasoning. ArXivabs/2004.11546.
13515

Panoramic Video Salient Object Detection with Ambisonic Audio Guidance
XiangLi1*,HaoyuanCao2,ShijieZhao3(cid:0),JunlinLi2,LiZhang2,BhikshaRaj1,
1DepartmentofElectricalandComputerEngineering,CarnegieMellonUniversity,PA,USA.
2BytedanceInc.,SanDiego,CA,USA.3BytedanceInc.,Shenzhen,China.
{xl6,bhiksha}@andrew.cmu.edu,{haoyuan.cao,zhaoshijie.0526,lijunlin.li,lizhang.idm}@bytedance.com
Abstract
VisualScene
Videosalientobjectdetection(VSOD),asafundamentalcom- Understanding
putervisionproblem,hasbeenextensivelydiscussedinthe
lastdecade.However,allexistingworksfocusonaddressing
theVSODproblemin2Dscenarios.Withtherapiddevelop- ERFrame Multimodal Fusion
mentofVRdevices,panoramicvideoshavebeenapromising 3D Sound
Source
alternativeto2Dvideostoprovideimmersivefeelingsofthe Localization
realworld.Inthispaper,weaimtotacklethevideosalient Multi-channel Audios Audio-VisualSceneUnderstanding
objectdetectionproblemforpanoramicvideos,withtheircor-
respondingambisonicaudios.Amultimodalfusionmodule Figure 1: We study the problem of how to utilize the am-
equippedwithtwopseudo-siameseaudio-visualcontextfusion bisonicaudiotofacilitatethepanoramicvideosalientobject
(ACF)blocksisproposedtoeffectivelyconductaudio-visual detection.Sinceambisonicaudiocontainsrichspatialinfor-
interaction.TheACFblockequippedwithsphericalpositional
mation,wecandirectlylocalizethesoundsourcesandthen
encodingenablesthefusioninthe3Dcontexttocapturethe
fusethemwithvisualcues.Ontheotherhand,sincetheER
spatialcorrespondencebetweenpixelsandsoundsourcesfrom
framehasdistortionsandcannotreflectthe3Dpositionof
theequirectangularframesandambisonicaudios.Experimen-
pixels,projectingthepixelsbackto3Dspaceisessentialfor
talresultsverifytheeffectivenessofourproposedcomponents
anddemonstratethatourmethodachievesstate-of-the-artper- effectivemultimodalfusion.
formanceontheASOD60Kdataset.
Introduction The panoramic video contains omnidirectional contexts
andrepresentseachpixelona3Dsphere.Unlike2Dvideos
Videosalientobjectdetection(VSOD)aimstofindthemost
thatdisplaywithastableviewpointandfixedenvironmen-
visuallydistinctiveobjectsinavideo.VSODfor2Dvideos
talaudios,panoramicvideosaretypicallysupportedbyVR
has been attracting considerable attention (Su et al. 2022;
headsets which provide a head direction-adaptive field of
Liu et al. 2021; Ren et al. 2021b) due to its wide appli-
view(FOV)andambisonicaudios.Withmulti-channelau-
cations in real-world scenarios, such as video editing and
diorecordings,ambisonicaudiosencodethe3Dlocationof
videocompression.While,forpanoramicvideoswhichhave
thesoundsourceandtheVRheadsetcanfurtheradjustthe
a very different format and viewing environment, how to
originalaudiotoprovidearealsenseofsoundsourcetothe
effectively detect salient objects is still an open problem.
usergiventhemovementofthehead.Previousworks(Tsi-
Sincehumanattentionisusuallyinfluencedbyacousticsig-
ami,Koutras,andMaragos2020;Chengetal.2021)have
naturesthatarenaturallysynchronizedwithvisualobjects
demonstratedtheaudioshowsnon-trivialinfluenceonhuman
inaudio-bearingvideorecordings,someVSODmethodsfor
attentionin2Dvideos.ForVSODinpanoramicvideos,as
2Dvideos(Tsiami,Koutras,andMaragos2020;Chengetal.
shown in Figure 1, since the ambisonic audio can directly
2021)introduceacousticmodalitytofacilitatethesaliency
reflectthe3Dsoundsourcelocation,weconsideritshould
discrimination.Differentfrommonoorbinauralaudioused
performamoreimportantrolecomparedto2Dscenarios.
in2Dvideos,ambisonicaudioisutilizedtocreateimmersive
On the other hand, previous panoramic video process-
feelingsoftherealworldinpanoramicvideos.Inthiswork,
ing approaches cannot preserve the spatial relationship of
wefocusonhowtodetectthesalientobjectsinpanoramic
panoramicvideos.Duetotheuniqueformatofpanoramic
videos with their corresponding ambisonic audios. To the
videos,itisdifficulttostoreortransmittherawvideousing
bestofourknowledge,wearethefirsttotackletheVSOD
current video coding techniques. Therefore, equirectangu-
problemforpanoramicscenarios.
lar(ER)projectioniscommonlyleveragedtotransformthe
panoramicvideointoaregular2Dformat.However,theER
*ThisworkwasdonewhenXiangLiwasaninternatBytedance.
Copyright¬©2023,AssociationfortheAdvancementofArtificial projectionwillinvolvenotonlyaseparationalongthelon-
Intelligence(www.aaai.org).Allrightsreserved. gitude of the sphere but also distortions in the polar area,
2202
voN
62
]VC.sc[
1v91441.1122:viXra
whichseverelybarriertheeffectiveVSOD.Previousmethods moreadvancedstructuresareleveragedtobetterunderstand
addressthepolardistortionsbyintroducingacubeprojec- the spatio-temporal correspondence. For example, ConvL-
tion(Chengetal.2018)whichprojectsthespheretoacube STM (Li et al. 2018; Song et al. 2018; Fan et al. 2019) is
andexpandseachfacetoeasethedistortions.However,the adoptedtoconstructlong-termtemporalrelation.Withthe
paddedcubemapseverelydestroysthespatialrelationship strongabilityoftransformer(Vaswanietal.2017)tomodel
betweeneachfacewhichobstaclestheglobalunderstanding thecomplexrelationship,ithasachievedpromisingresultin
oftheframe. VSOD(Liuetal.2021;Renetal.2021b).
In this paper, we purpose a framework for audio-visual
salient object detection in panoramic scenarios. Consider- PanoramicSaliencyDetection
ing the rich spatial and semantic information encoded in Saliencydetectionaimstopredicttheregionofhumaneye
theambisonicaudios,wefirstuseapretrainedacousticen- fixation in the video. For image-level saliency prediction
coder to extract location and semantic embeddings of 3D (Cheng et al. 2018; Suzuki and Yamanaka 2018),(Cheng
soundsources,andthenintroduceaudio-visualcontextfu- etal.2018)proposeacubepaddingoperationtoprojectthe
sion(ACF)blockstoenhancethevisualfeaturesbyacoustic panoramic frame to a cube with fewer distortions on each
cues.Ideally,the3Dlocationofsoundsourcesshouldbeuti- face.(Zhu,Zhai,andMin2018)firstmaptheERframeto
lizedasground-truthtofinetunetheacousticnetworkwhile aspherethenpredictthesaliencyfromeachviewport.For
itisverydifficulttoobtainforcommonpanoramicvideos. video-levelsaliencyprediction(Chengetal.2018;Nguyen,
Totacklethisproblem,inspiredbylabel-guideddistillation Yan,andNahrstedt2018),Nguyenetal.(Nguyen,Yan,and
(Zhangetal.2022),theground-truthlabelofsalientobjectis Nahrstedt2018;Zhangetal.2018)proposesamethodthat
employedinthemultimodalfusionbyenforcingconsistency leveragesCNNandLSTMforsaliencyprediction.In(Zhang
betweenteacher(equippedwithlabel)andstudentbranch.By et al. 2018), spherical CNN is introduced to directly han-
learningbetteraudio-visualcorrespondence,wecantransfer dle panoramic videos. In addition, audio is introduced in
thespatialinformationencodedinthevisualobjectstosu- panoramicsaliencyprediction(Chaoetal.2020)givenits
perviseacousticmodality.Inparticular,toreflectthetrue3D strongabilitytoinfluencehumanattention.
locationofobjectsandmitigatetheinfluenceofdistortionsin
ERframes,wemapthe2Dcoordinatesofpixelsbacktothe VideoObjectSegmentation
3Dsphereandencodetheminthepositionalencodingduring
Video object segmentation (VOS) can be categorized as
thefusion.Inthisway,themodelcancapturethetruespatial
unsupervised (Wang et al. 2019; Ren et al. 2021a), semi-
positionofeachpixel.Ourcontributionsaresummarizedas:
supervised(Wangetal.2021)andreferring(Wuetal.2022;
‚Ä¢ Weproposeanaudio-visualvideosalientobjectdetection Li et al. 2022c) VOS. The most relevant type to this work
framework for panoramic scenarios. To the best of our is the unsupervised VOS (UVOS) which aims to segment
knowledge,wearethefirsttotacklethisproblem. primaryobjectregionsfromthebackgroundinvideos.Early
methodstackletheUVOSproblembyobjectproposal(Kim
‚Ä¢ We introduce a label-guided audio-visual fusion mod-
andHwang2002),temporaltrajectory(Fragkiadaki,Zhang,
ule to effectively utilize the rich spatial and semantic
andShi2012)andsaliencyprior(Wangetal.2015;Wang,
informationencodedintheambisonicaudiorecordings
Shen,andPorikli2015).Morerecently,deeplearning-based
synchronizedwithpanoramicvideos.
methodsareproposedformodelingthespatio-temporalinfor-
‚Ä¢ Our model achieves state-of-the-art results on the
mation.MATNet(Zhouetal.2020)usesamotion-attentive
ASOD60Kdataset.Extensiveexperimentsareconducted
transitiontomodelmotioncuesandspatio-temporalrepre-
toillustratetheeffectivenessofourmethod.
sentation.RTN(Renetal.2021a)leverageslong-rangeintra-
framecontrast,temporalcoherence,andmotion-appearance
RelatedWorks similaritytoenhancetheappearancefeaturerepresentation.
In addition to VOS, video instance segmentation (Li et al.
VideoSalientObjectDetection
2022a,b,d)isalsorelevanttothiswork.Recently,someworks
VSODaimstofindthemostvisuallysalientobjectsinthe extendthevideosegmentationtasktomultimodalbyconsid-
video.Conventionalmethodsusuallyleveragecolorcontrast eringaudio(Zhouetal.2022)orsignals(Zhaoetal.2022;
(Achanta et al. 2009), motion prior (Zhang, Yu, and Cran- Huangetal.2021b,a).
dall2019),backgroundprior(Yangetal.2013)andcenter
prior(JiangandDavis2013;Bertasiusetal.2017)todistin- Method
guishthesalientregions.However,mostofthosemethods
Overview
arelimitedbythelowrepresentativeabilityofhand-crafted
features.Recentmethodsleveragedeep-learningapproaches Given a video clip V = {I t}T
t=1
of T frames and its cor-
totackletheVSODproblem.Toutilizethetemporalinfor- responding multi-channel audio recordings A = {H i}4 i=1,
mation,FCNS(Wang,Shen,andShao2017)firstleverages we predictthe salientobject {M }T effectivelywith our
t t=1
FCNforstaticsaliencypredictionandthenpost-processthem method.ThemethodoverviewisillustratedinFigure2.The
byanotherdynamicFCN.Similarly,DSRFCN3D(Leand pipelinecanbeboileddownintothreeparts:acousticandvi-
Sugimoto2017)introduces3Dconvolutionfortemporalag- sualencoders,alabel-guidedmultimodalfusionmodule,and
gregation.Opticalflowreflectingthemotionalsoservesasa decoders.Wefirstleverageavisualandanacousticencoder
strongcueforVSODtaskin(Lietal.2018,2019).Recently, to extract visual {f }T and acoustic features gsem, gloc.
t t=1
Skip Connections
Student
Visual Enc.
Visual Dec.
Video ùëâ={ùêº !‚ãØùêº "} Visual Feature ùëì! "% #$ GroundTruth Student Feature ùëì!&!‚Äô "% #$ Student Prediction ùëÄ!&!‚Äô "% #$
(Label-guided)
Multimodal Fusion
‚Ñí‚Äô(!)($$ ‚Ñí!)*+&
Skip Connections GroundTruth
Sem. Head
Acoustic Enc. Sem Embed. ùëî!"# Teacher
Visual Dec.
Loc. Head
Multi-channelAudioA= ùêª # #% $! Loc. Embed. ùëî$%& Teacher Feature ùëì!!() "%
#$
Teacher Prediction ùëÄ!!() "%
#$
Figure2:PipelineOverview.Weuseseparateencoderstoextractmultimodalfeatures.ForavideoclipV = {I ¬∑¬∑¬∑I },a
1 T
visualencoderisutilizedtoextractvisualfeature{f }T .ForaudioinputA = {H }4 ,atwo-brunchacousticencoderis
t t=1 i i=1
employedtoextractthesemanticembeddinggsemandlocationembeddinggloc.Afterthat,alabel-guidedmultimodalfusion
module is introduced to effectively fuse the multimodal features, which outputs a student feature {fstu}T and a teacher
t t=1
feature{ftch}T .Twodecodersareleveragedtodecodethefinalpredictions{Mstu}T and{Mtch}T fromcompacted
t t=1 t t=1 t t=1
features{fstu}T and{ftch}T respectively.Inparticular,toenhancemultimodalcommunication,adistillationlossL is
t t=1 t t=1 ditill
adoptedbetweenstudentfeature{fstu}T and(label-guided)teacherfeature{ftch}T .AstructurelossL isutilizedas
t t=1 t t=1 struc
theobjective.Thegraycolorindicatescomponentsthatareonlyusedduringtraining.
Theacousticfeaturescontainsasemanticembeddinggsem
andalocationembeddinggloc whichencodesthecategory
and3Dlocationofsoundsourcesrespectively.Afterthat,a ER Projection
multimodalfusionmoduleisutilizedtoenableaudio-visual
interaction.Specifically,themultimodalfusionmodulecon-
PanoramicFrame ER Frame
tainstwopseudo-siameseblocks-astudentblockthatfuses
Figure3:IllustrationofERProjection.Severedistortions
audio-visualinformationusingmultimodalattentionanda
canbeobservedinthepolarareas.
teacherblockthatsharesthesamestructureofthestudent
block while taking an additional ground truth as input to
guidethefusion.Theoutputstudentfeatures{fstu}T and
t t=1 ferent from regular positional encoding (Vaswani et al.
(label-guided)teacherfeatures{ftch}T aresenttovisual
t t=1 2017), spherical positional encoding first re-projects the
decodersequippedwithskipconnectionstogeneratethefi-
plane coordinates of each pixel back to the 3D sphere
nal salient object predictions {Mstu}T and {Mtch}T .
t t=1 t t=1 and generates positional encoding based on the 3D coor-
A distillation loss between student feature {fstu}T and
t t=1 dinates. In this way, each pixel can reflect its true 3D po-
(label-guided)teacherfeatures{ftch}T isadoptedtohelp
t t=1 sition thus avoiding the severe distortion in the polar re-
themultimodalinteractionandastructurelossbetweenpre-
gionandinevitableseparationalongalongitudeintheER
dictionMÀÜ andgroundtruthM isusedasthetaskobjective
t t frame.Inparticular,thesphericalpositionalencodingcanbe
forsalientobjectdetection.
computedasPE(pos 3D,2i) = sin(pos 3D/100002i/d 3)and
Label-guidedMultimodalFusion PE(pos 3D,2i+1)=cos(pos 3D/100002i/d 3)wherepos 3D
canbex,y,zcoordinatesandiisthedimension.The2D-to-
The label-guided multimodal fusion module contains two
3Dtransformationcanbecomputedas
pseudo-siamese audio-visual context fusion (ACF) blocks
v u v u v
equippedwithsphericalpositionalencodingtoalignthecor-
x=sin cos , y =sin sin , z =cos (1)
respondence between 3D sound sources and pixels. The R R R R R
output of student and teacher block are student feature where(u,v)and(x,y,z)arethe2Dand3Dcoordinateof
{fstu}T andteacherfeature{ftch}T respectively. eachpixelrespectively.R= W whereW isthewidthofthe
t t=1 t t=1 2œÄ
frame.Sphericalpositionalencoding(SPE)isemployedto
Sphericalpositionalencoding. ERframeisacommonly
encodespatialinformationforvisualrepresentationduring
used format to transmit and store panoramic videos (Cai
cross-modalattention.
etal.2022).However,asshowninFigure3,theERframe
suffers from severe distortions in the polar regions. To Student block. As shown in Figure 4 (without the gray
tackle this problem, we adopt the position-agnostic atten- parts), to fuse the rich information encoded in visual and
tion mechanism and propose a spherical positional encod- acoustic features, we utilize multimodal attention to en-
ing to compensate for the distortion in the ER frame. Dif- able audio-visual context interaction. We first concatenate
fusion.AsshowninFigure4(withthegrayparts),theaddi-
TeacherPath C Concatenation
tionalgroundtruthisdownsampledandconcatenatedwith
Stop Gradient F Flatten visualfeature{f }T intheteacherblockbeforeconducting
GroundTruth Element-wise Multiplication R Reshape t t=1
themultimodalinteraction.Similartostudentblock,thefinal
CC F z z outputofteacherblockisdenotedas{f ttch}T t=1.Notethat
the teacher block does not share weights with the student
Visual Feature ùëì!"% #$ Visual Featureùêπ blockandthegradientofteacherblockistruncatedtoavoid
Multimodal z influencingtheencoder.
Attention z R
Sem Embed. ùëî&‚Äô( ùêπ&‚Äô(
Multimodal z
ùúë!‚Ü¶# S (Ttu ed ace hn et rF Fea et au tr ue reùëì! ùëì& !! !, +-"% # "% #$ $) WEn ec uo sd ee sr
eparateencoderstoextractvisualandacousticfea-
Attention z
Loc. Embed. ùëî)*+ ùêπ)*+ tures.
Figure4:IllustrationofACFblockusedinthe(label-guided)
multimodalfusionprocess.Studentblock:Thevisualfea-
ture{f }T isfirstflattenedandaddedwithpositionalen-
t t=1
coding then conducts multimodal attention with acoustic F Transformer
semanticembeddinggsem andlocationembeddinggloc re- P
spectively.Afterthat,weformapixel-wiseweightingfrom Frame ùêº! Visual Featureùëì!"
Backbone
fusedfeatureFloctomodulateFsem.Theoutputofstudent
Addition F Flatten P Positional Encoding
block is denoted as {fstu}T . Teacher block: Different
t t=1
fromstudentblock,weconcatenateground-truthmaskwith Figure5:Single-framevisualfeatureextraction.TheER
the visual feature {f t}T t=1 to help the network learn better frame I t is first processed by a backbone and then the ex-
representationinthemultimodalfusion.Sinceteacherblock tracted features are fed to a transformer to capture spatial
isonlyemployedduringtraining,wetruncatethegradients information.Thesingle-framefeatureisdenotedasf(cid:48).
t
beforethemultimodalfusionintheteacherblock.Theoutput
ofteacherblockisdenotedas{ftch}T .
t t=1 Visual encoder. We first project the panoramic video to
2D frames {I ¬∑¬∑¬∑I } using ER projection and then feed
1 T
themtothebackbone.AsshowninFigure5,atransformer
and then flatten the visual feature {f }T to form F =
t t=1 encoderontopoftheResNet-50(Heetal.2016)isadoptedto
flatten(f ‚äï¬∑¬∑¬∑‚äïf ) ‚àà RC√óTHW.Afterthat,spherical
1 T mitigatetheseveredistortionintheERframe.Inaddition,a
andregularpositionalencodings(Vaswanietal.2017)are
temporalnon-localblockas(Yanetal.2019)isalsoleveraged
addedtovisualfeatureF,andacousticfeaturegsemandgloc,
toenablethetemporalinteractionontheextractedfeatures
respectively,tohelpthenetworkcapturespatialinformation.
{f(cid:48)}T from the backbone. We denote the features after
Themultimodalattentioncanbecomputedby temt pt o= r1 alaggregationas{f }T wheref ‚ààRC√óH√óW.
t t=1 t
haud =LN(MCA(F,gaud)+F) (2)
Acousticencoder. Multi-channelaudiocontainsthe3Dlo-
Faud =LN(FFN(haud)+haud) (3) cationandcategoryinformationofthesoundsourcewhich
haveagreatimpactonthechoosingofthesalientobjects.To
where MCA, FFN and LN are multi-head cross-attention extracttheacousticfeatures,weleveragethreeCNNlayers
(Ye et al. 2019), feed-forward network and layer- followedbytwobi-directionalGRUlayersasouracoustic
normalization respectively. In particular, we generate the encoder and three linear layers for both semantic and lo-
query Q from F and key K, value V from gaud by lin- cationhead(detailedstructureavailableinsupplementary)
ear projections. The MCA(F,gaud) can be computed by (Adavanneetal.2018).Sinceitisdifficulttoobtainthereal-
Softmax(K ‚àöTQ)V, where d is the dimension of query Q. world sound source location in panoramic videos, we first
d pretraintheacousticencoderona3Dsoundsourcelocaliza-
ThestudentfeatureFstucanbecomputedby
tionandsoundeventclassificationdataset,L3DAS(Guizzo
Fstu =Fsem(cid:12)œï (Floc) (4) et al. 2021). We remove final linear layer in each head to
C(cid:55)‚Üí1
formthesemanticembeddinggsem ‚àà RC√óL andlocation
whereœï denotesaconvolutiontoreducechannelfrom
C(cid:55)‚Üí1 embeddinggloc ‚ààRC√óL.
C to1and(cid:12)denoteselement-wisemultiplication.Thefinal
outputis{fstu}T =Reshape(Fstu).
t t=1 Decoder
Teacherblock. Theaudioencoderispretrainedona3D We adopt the same structure for decoding {fstu}T and
t t=1
soundsourcelocalizationdataset(Guizzoetal.2021)while {ftch}T . For decoding the salient object prediction, we
t t=1
it is difficult to obtain a 3D location of vocal objects in follow the FPN structure (Lin et al. 2017) to fuse the low-
panoramicvideosduringmaintraining.Inspiredbyprevious level features. Let the output salient object prediction be
work(Zhangetal.2022),webuildapseudo-siameseteacher {Mstu}T ‚ààRHo√óWo and{Mtch}T ‚ààRHo√óWo forstu-
t t=1 t t=1
blocktohelpthenetworkcaptureaccuratespatialinforma- dentandteacherbranchrespectively,whereH andW are
o o
tionbyintroducingground-truthannotationtothemultimodal theheightandwidthoftheoutput.
Miscellanea(Test1) Music(Test2) Speaking(Test3) ASOD60K-TestAll
Method
F ‚Üë S ‚Üë E ‚Üë M‚Üì F ‚Üë S ‚Üë E ‚Üë M‚Üì F ‚Üë S ‚Üë E ‚Üë M‚Üì F ‚Üë S ‚Üë E ‚Üë M‚Üì
Œ≤ Œ± œÜ Œ≤ Œ± œÜ Œ≤ Œ± œÜ Œ≤ Œ± œÜ
Image-LevelMethods
CPD-R .248 .654 .645 .035 .272 .608 .632 .018 .228 .588 .657 .026 .243 .609 .648 .026
SCRN .250 .665 .615 .046 .341 .683 .664 .023 .276 .636 .642 .034 .286 .655 .641 .034
F3Net .257 .655 .629 .040 .358 .662 .749 .021 .308 .626 .692 .027 .310 .642 .691 .029
MINet .238 .650 .625 .050 .380 .670 .716 .020 .261 .590 .635 .053 .286 .624 .652 .044
LDF .280 .663 .626 .044 .389 .671 .753 .023 .309 .625 .711 .037 .322 .645 .701 .035
CSFR2 .238 .652 .642 .033 .347 .665 .693 .018 .285 .636 .700 .026 .290 .646 .684 .026
GateNet .285 .677 .651 .044 .290 .673 .616 .018 .260 .633 .638 .034 .273 .653 .636 .033
Video-LevelMethods
COSNet .147 .610 .553 .031 .220 .557 .541 .016 .176 .572 .570 .023 .181 .582 .559 .023
RCRNet .272 .661 .640 .034 .403 .695 .738 .019 .282 .632 .687 .030 .310 .654 .688 .029
PCSA .123 .604 .574 .034 .310 .657 .645 .022 .150 .571 .534 .026 .184 .600 .570 .027
3DC-Seg .300 .668 .618 .062 .326 .635 .632 .046 .289 .629 .592 .056 .300 .640 .608 .055
RTNet .240 .622 .634 .038 .365 .638 .766 .020 .194 .555 .668 .028 .247 .591 .683 .025
Ours .355 .714 .617 .037 .483 .682 .834 .016 .382 .658 .742 .024 .404 .678 .732 .026
Table1:Comparisontostate-of-the-artsalientobjectdetectionmethodsonASOD60Kdataset.‚Üëmeanslargerisbetter
and‚Üìmeanssmallerisbetter.Boldmeansthestate-of-the-artperformance.
LossFunction inducedsalientobjectdetectionbenchmarkforpanoramic
Theoverallobjectiveofourproposedmethodiscomposedof videos.Thereare62,455frameswith10,465instance-level
structurelossesforstudentandteacherbranchLstu ,Ltch ground truths in the dataset. In particular, each video cor-
struc struc
andadistillationlossL responds to a 4-channel ambisonic audio recording. The
distill
ground-truth salient objects are determined by the eye fix-
L=Lstu +Ltch +Œª L (5)
struc struc distill distill ation of 40 participants who viewed the video with HTC
whereŒª distill isascalartobalancethelosses. ViveHMDheadset.ThetestsetofASOD60Kcontainsthree
Structure loss. Following previous method (Chen et al. subsetssplitbysoundeventclasses-miscellanea,music,and
2022), we leverage a combination of binary cross entropy speaking.
lossandDiceloss(Milletari,Navab,andAhmadi2016)as Metrics. To evaluate the performance of video salient
theobjectiveforsalientobjectdetection. object detection, we employ the adaptive F-Measure F
Œ≤
(Achanta et al. 2009), adaptive E-Measure E (Fan et al.
T T œÜ
L struc =(cid:88) L bce(M t,MÀÜ t)+Œª dice(cid:88) L dice(M t,MÀÜ t) 2018),S-MeasureS Œ± (Fanetal.2017)andMeanAbsolute
Error(MAE)M(Borjietal.2015).
t=1 t=1
(6)
where M and MÀÜ are predicted and ground-truth salient ImplementationDetails
t t
maps respectively. Œª is a scalar. M can be Mstu and Followingthebenchmarksetting(Zhang,Chao,andZhang
dice t t
MtchforcomputingLstu andLtch . 2021),ourmodelisfirstpre-trainedonDUSTdataset(Wang
t struc struc
etal.2017)andthenfinetunedonASOD60K(Zhang,Chao,
Distillationloss. Tohelpthestudentblocklearntheaudio-
andZhang2021).Themodelistrainedfor20epochswith
visual correspondence, we enforce a consistency between
a learning rate of 1e-4. We adopt a batchsize of 2 and an
ftchandfstu.AMSElossisutilizedastheconstraint
t t AdamW(LoshchilovandHutter2017)optimizerwithweight
(cid:88)T decay0.Allimagesarecroppedtohavethelongestsideof
L distill = L MSE(f ttch,f tstu) (7) 832pixelsandtheshortestsideof416pixelsduringtraining
t=1 andevaluation.Thewindowsizeissetto3.TheŒª isset
distill
to5.0andŒª issetto1ifnospecification.Weleveragea
Inference dice
3-layertransformerencoder(Dosovitskiyetal.2020)ontop
Sincethepurposeofintroducingteacherblockistofacilitate
oftheResNet-50(Heetal.2016)toextractvisualfeatures.
networklearnaccurateaudio-visualcorrespondenceduring
WeleverageanaugmentedSELDNet(Adavanneetal.2018)
training, we disable the teacher block and only keep the
as our acoustic encoder. Our method is implemented with
studentblockduringinference.
PyTorch.
Experiment
MainResults
DatasetandMetrics Inthissection,wecompareourmethodwithpreviousstate-
Dataset. We conduct experiments on the ASOD60K of-the-artmethods,includingCPD-R(Wu,Su,andHuang
dataset(Zhang,Chao,andZhang2021)whichisanaudio- 2019a),MINet(Pangetal.2020),SCRN(Wu,Su,andHuang
Frames GT CPD-R LDF CSFR2 RCRNet Ours
Figure6:Qualitativecomparisontostate-of-the-artVSODmethodsonASOD60Kdataset.
2019b), F3Net (Wei, Wang, and Huang 2020), LDF (Wei ASOD60K-TestAll
MultimodalFusion
etal.2020),CSFR2(Gaoetal.2020),GateNet(Zhaoetal. F ‚Üë S ‚Üë E ‚Üë M‚Üì
Œ≤ Œ± œÜ
2020),COSNet(Luetal.2019),RCRNet(Yanetal.2019), None .396 .660 .714 .037
PCSA(Guetal.2020),3DC-Seg(Mahadevanetal.2020) +ACF(Concat) .385 .667 .697 .027
andRTNet(Renetal.2021a)onASOD60Kdataset. +ACF(MMAttn) .397 .670 .722 .026
+ACF(MMAttn)+SPE .404 .678 .732 .026
Quantitativeresults. Wecompareourmethodwithstate-
of-the-artmethodsontheASOD60KdatasetinTable1.In
Table2:Impactofdifferentmultimodalfusionmethods.
general, our method achieves the best result of 0.404 F ,
Œ≤
Thecontentinthebracketindicatesdifferentfusionmethods
0.678 S , 0.732 E and 0.026 M on the ASOD60K test
Œ± œÜ
inACFblock.Concat:Concatenate,MMAttn:multimodal
set. For each sound event split, all metrics of our method
attention,SPE:sphericalpositionalencoding.
eclipse other methods on both Music and Speaking splits.
Whilethe0.617E ofourmethodontheMiscellaneasplitis
œÜ
slightlylowerthanthe0.644E ofRCRNet(Yanetal.2019).
œÜ
Tworeasonsmaybeaccountfortheinferiorperformanceof Qualitative results. We present our qualitative result in
ourmethod.First,theaudiorecordingsinMiscellaneasplit Figure 6 and compare it against previous 2D methods on
containbackgroundmusicwhichbarriersourmodeltoaccu- ASOD60Kdataset.Theresultindicatesthatpreviousmeth-
ratelylocatingthesoundsources.Second,thereareseveral odsfailtodetectthecorrectsalientobjects.Incontrast,our
unseensoundeventclassesintheMiscellaneatestsplit.In method shows great accuracy and robustness even in very
this way, it is difficult for the model to construct correct challengingscenarios,e.g.,withseveredistortionsinthepo-
multimodalcorrespondencebetweenvideosandaudioswith larareaasshowninthefirstlineinFigure6.Thisimplies
unseen classes. In the music and speaking scenario where thatournetworkequippedwithACFblockandSPEgener-
soundsourcescanbeeasilylocalized,ourmethodachieves atesmoreaccurateresultsthansimplyadoptingprevious2D
obviousimprovementagainstpreviousmethods. methodsonthepanoramicscenario.Wevisualizetheaudio-
guidedlocationheatmapœï (floc)inFigure7.Wenotice
C(cid:55)‚Üí1 t
thattheaudio-guidedlocationheatmapœï (floc)reflects
C(cid:55)‚Üí1 t
thecorrectlocationofsoundsourcesthushelpingthefinal
salientobjectdetection.
AblationExperiments
Frame GT Location Heatmap
We conduct extensive ablation studies on the ASOD60K
Figure7:Visualizationofsoundsourcelocalizationheatmap.
datasettoverifytheeffectivenessofdifferentcomponents.
ASOD60K-TestAll
ASOD60K-TestAll Œª
Backbone distill F ‚Üë S ‚Üë E ‚Üë M‚Üì
F ‚Üë S ‚Üë E ‚Üë M‚Üì Œ≤ Œ± œÜ
Œ≤ Œ± œÜ 0.0 .389 .667 .716 .027
Backbone .396 .660 .714 .037
1.0 .402 .676 .725 .029
+Transformer .404 .678 .732 .026
2.0 .410 .670 .715 .035
+Transformer+SPE .403 .676 .742 .026
5.0 .404 .678 .732 .026
Table 3: Impact of visual feature extraction methods.
Table4:Impactofdistillationlossweight.
Componentsareaddedstepbystep.
ASOD60K-TestAll
WindowSize
ASOD60K-TestAll F ‚Üë S ‚Üë E ‚Üë M‚Üì
3DLocalization Œ≤ Œ± œÜ
F Œ≤ ‚Üë S Œ± ‚Üë E œÜ ‚Üë M‚Üì 1 .392 .670 .725 .026
(cid:37) .391 .667 .723 .030 2 .396 .672 .719 .027
(cid:33) .404 .678 .732 .026 3 .404 .678 .732 .026
4 .402 .680 .728 .028
Table5:Impactof3Dsoundsourcelocalizationbranch.
Table6:Impactofinputwindowsize.
ASOD60K-TestAll differentŒª .AsshowninTable4,wenoticethataweight
SoundType distill
F ‚Üë S ‚Üë E ‚Üë M‚Üì of5leadstothebestresultof0.404F ,0.678S ,0.732E
Œ≤ Œ± œÜ Œ≤ Œ± œÜ
None .396 .660 .714 .037 and0.26M.TheŒª distill =0meansthatnoteacherbranch
Mono .397 .662 .717 .029 isadoptedwhichleadstotheworstresult.
Ambisonic .404 .678 .732 .026
3Dsoundsourcelocalization. Todemonstratetheeffec-
tivenessofemploying3DsoundsourcelocalizationinVSOD,
Table7:Impactofsoundtype. weconductanexperimenttodisablethesoundsourcelocal-
izationbranchinourmethod.TheresultinTable5indicates
Multimodalfusionmethod. Toinvestigatetheeffective- that3Dsoundsourcelocalizationfromambisonicaudiocan
nessofourproposedACFblock,weconductexperiments helpsalientobjectdetectioninpanoramicscenarios.
withdifferentmultimodalfusionschemes.AsshowninTa-
Windowsize. Sincetemporalinformationisessentialfor
ble2,wecomparetheACFblockequippedwithmultimodal
thevideosalientobjectdetection,weconductexperimentson
attentionwithtwobaselinesettings.‚ÄòNone‚Äôand‚ÄòACF(Con-
differentinputwindowsizesasshowninTable6.Wenotice
cat)‚Äômeansnomultimodalfusionandsimplyconcatenating
thatthewindowsizeof3achievesthebestperformancein
visualandacousticfeaturesintheACFblockrespectively.
termsofF ,E andMandthewindowsizeof4achieves
Œ≤ œÜ
Ingeneral,themodelwithoutmultimodalfusionleadstoan
thebestresultintermsofS .
Œ±
inferiorperformancewhichindicatestheacousticmodalityis
essentialtosalientobjectdetection.WenoticethatACFblock Soundtype. Weconductexperimentstoshowthebenefit
equipped with multimodal attention outperforms the ACF ofutilizingambisonicaudios.Table7showsthatmonoaudio
blockwithsimplymultimodalfeatureconcatenation.Addi- only has a trivial improvement compared to the baseline
tional spherical positional encoding (SPE) brings another setting(nomultimodalfusion).Weconsiderthisisbecause
0.07F ,0.08S and0.1E gaincomparedtomultimodal monoaudiocannotexplicitlymodelspatialinformationof
Œ≤ Œ± œÜ
attention. thesoundsource.
Visualfeatureextraction. Weconductexperimentstoab- Conclusion
latetheinfluenceofdifferentvisualfeatureextractionmeth-
Inthispaper,weproposeaframeworkforaudio-visualvideo
ods.WefirstbuildabaselinemodelthatleveragesResNet-50
salientobjectdetectioninpanoramicscenarios.Inparticular,
(He et al.2016) backbone to extract visual featureswhich
weproposeanaudio-visualcontextfusionblocktoenhance
leads to 0.396 F , 0.660 S , 0.714 E and 0.37 M. By
Œ≤ Œ± œÜ visual features by ambisonic audios. To better utilize the
employing a transformer encoder on top of the backbone,
spatial information encoded in the audios, a label-guided
weobservenon-trivialgainsonallmetrics.Weconsiderthe
distillationschemeisintroducedtohelpthemultimodalin-
improvementcomesfromthestrongglobalunderstandingca-
teraction.Inaddition,duetotheseveredistortionsintheER
pabilityoftransformer.Byreplacingthestandardpositional
frame,weleverageposition-agnosticattentionmechanism
encodinginthetransformerwithoursphericalpositionalen-
equipped with spherical positional encoding to map each
coder,theE metricimproves0.1whileF andS slightly
œÜ Œ≤ Œ± pixelbackto3Dspacethuscapturingthetruespatiallocation
drop.Weconsidertheperformancedropmainlybecausethe
ofeachpixel.Notably,ourmethodachievesthebestresulton
pretrainingisconductedonthe2Ddataset.
theASOD60Kbenchmark.Moreover,extensivestudyshows
Distillation loss weight. To investigate the influence of thatambisonicaudiocanhelpthesalientobjectdetectionin
distillationlossweight,weconductexperimentsbyablating panoramicvideos.
References Guizzo,E.;Gramaccioni,R.F.;Jamili,S.;Marinoni,C.;Massaro,
Achanta, R.; Hemami, S.; Estrada, F.; and Susstrunk, S. 2009. E.;Medaglia,C.;Nachira,G.;Nucciarelli,L.;Paglialunga,L.;Pen-
Frequency-tunedsalientregiondetection. In2009IEEEconference nese,M.;etal.2021.L3DAS21Challenge:Machinelearningfor3D
oncomputervisionandpatternrecognition,1597‚Äì1604.IEEE. audiosignalprocessing. In2021IEEE31stInternationalWorkshop
onMachineLearningforSignalProcessing(MLSP),1‚Äì6.IEEE.
Adavanne,S.;Politis,A.;Nikunen,J.;andVirtanen,T.2018.Sound
eventlocalizationanddetectionofoverlappingsourcesusingconvo- He,K.;Zhang,X.;Ren,S.;andSun,J.2016.Deepresiduallearning
lutionalrecurrentneuralnetworks. IEEEJournalofSelectedTopics forimagerecognition. InProceedingsoftheIEEEconferenceon
inSignalProcessing,13(1):34‚Äì48. computervisionandpatternrecognition,770‚Äì778.
Bertasius,G.;SooPark,H.;Yu,S.X.;andShi,J.2017. Unsuper- Huang, Y.; Li, X.; Wang, W.; Jiang, T.; and Zhang, Q. 2021a.
vised learning of important objects from first-person videos. In Forgery Attack Detection in Surveillance Video Streams Using
ProceedingsoftheIEEEInternationalConferenceonComputer Wi-FiChannelStateInformation. IEEETransactionsonWireless
Vision,1956‚Äì1964. Communications.
Borji,A.;Cheng,M.-M.;Jiang,H.;andLi,J.2015. Salientobject Huang,Y.;Li,X.;Wang,W.;Jiang,T.;andZhang,Q.2021b. To-
detection:Abenchmark. IEEEtransactionsonimageprocessing, wardscross-modalforgerydetectionandlocalizationonlivesurveil-
24(12):5706‚Äì5722. lancevideos. InIEEEINFOCOM2021-IEEEConferenceonCom-
Cai,Y.;Li,X.;Wang,Y.;andWang,R.2022. AnOverviewof puterCommunications,1‚Äì10.IEEE.
Panoramic Video Projection Schemes in the IEEE 1857.9 Stan- Jiang,Z.;andDavis,L.S.2013. Submodularsalientregiondetec-
dardforImmersiveVisualContentCoding. IEEETransactionson tion. InProceedingsoftheIEEEconferenceoncomputervision
CircuitsandSystemsforVideoTechnology. andpatternrecognition,2043‚Äì2050.
Chao,F.-Y.;Ozcinar,C.;Wang,C.;Zerman,E.;Zhang,L.;Hami-
Kim,C.;andHwang,J.-N.2002. Fastandautomaticvideoobject
douche, W.; Deforges, O.; and Smolic, A. 2020. Audio-visual
segmentationandtrackingforcontent-basedapplications. IEEE
perceptionofomnidirectionalvideoforvirtualrealityapplications.
transactionsoncircuitsandsystemsforvideotechnology,12(2):
In 2020 IEEE International Conference on Multimedia & Expo
122‚Äì129.
Workshops(ICMEW),1‚Äì6.IEEE.
Le,T.-N.;andSugimoto,A.2017.DeeplySupervised3DRecurrent
Chen,Y.-W.;Jin,X.;Shen,X.;andYang,M.-H.2022.VideoSalient
FCNforSalientObjectDetectioninVideos. InBMVC,volume1,
ObjectDetectionviaContrastiveFeaturesandAttentionModules.
3.
InProceedingsoftheIEEE/CVFWinterConferenceonApplications
ofComputerVision,1320‚Äì1329. Li,G.;Xie,Y.;Wei,T.;Wang,K.;andLin,L.2018. Flowguided
recurrentneuralencoderforvideosalientobjectdetection. InPro-
Cheng,H.-T.;Chao,C.-H.;Dong,J.-D.;Wen,H.-K.;Liu,T.-L.;
ceedingsoftheIEEEconferenceoncomputervisionandpattern
andSun,M.2018. Cubepaddingforweakly-supervisedsaliency
recognition,3243‚Äì3252.
predictionin360videos. InProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition,1420‚Äì1429. Li,H.;Chen,G.;Li,G.;andYu,Y.2019. Motionguidedattention
Cheng, S.; Song, L.; Tang, J.; and Guo, S. 2021. Audio-Visual forvideosalientobjectdetection. InProceedingsoftheIEEE/CVF
SalientObjectDetection.InInternationalConferenceonIntelligent internationalconferenceoncomputervision,7274‚Äì7283.
Computing,510‚Äì521.Springer. Li,X.;Wang,J.;Li,X.;andLu,Y.2022a. Hybridinstance-aware
Dosovitskiy,A.;Beyer,L.;Kolesnikov,A.;Weissenborn,D.;Zhai, temporalfusionforonlinevideoinstancesegmentation. InProceed-
X.;Unterthiner,T.;Dehghani,M.;Minderer,M.;Heigold,G.;Gelly, ingsoftheAAAIConferenceonArtificialIntelligence,volume36,
S.;etal.2020. Animageisworth16x16words:Transformersfor 1429‚Äì1437.
imagerecognitionatscale. arXivpreprintarXiv:2010.11929. Li,X.;Wang,J.;Li,X.;andLu,Y.2022b.Videoinstancesegmenta-
Fan, D.-P.; Cheng, M.-M.; Liu, Y.; Li, T.; and Borji, A. 2017. tionbyinstanceflowassembly. IEEETransactionsonMultimedia.
Structure-measure:Anewwaytoevaluateforegroundmaps.InPro-
Li, X.; Wang, J.; Xu, X.; Li, X.; Lu, Y.; and Raj, B. 2022c. RÀÜ
ceedingsoftheIEEEinternationalconferenceoncomputervision,
2VOS:RobustReferringVideoObjectSegmentationviaRelational
4548‚Äì4557.
MultimodalCycleConsistency. arXivpreprintarXiv:2207.01203.
Fan,D.-P.;Gong,C.;Cao,Y.;Ren,B.;Cheng,M.-M.;andBorji,
Li,X.;Wang,J.;Xu,X.;Raj,B.;andLu,Y.2022d. OnlineVideo
A.2018. Enhanced-alignmentmeasureforbinaryforegroundmap
InstanceSegmentationviaRobustContextFusion. arXivpreprint
evaluation. arXivpreprintarXiv:1805.10421.
arXiv:2207.05580.
Fan,D.-P.;Wang,W.;Cheng,M.-M.;andShen,J.2019. Shifting
Lin,T.-Y.;Doll√°r,P.;Girshick,R.;He,K.;Hariharan,B.;andBe-
moreattentiontovideosalientobjectdetection. InProceedingsof
longie,S.2017. Featurepyramidnetworksforobjectdetection. In
theIEEE/CVFconferenceoncomputervisionandpatternrecogni-
ProceedingsoftheIEEEconferenceoncomputervisionandpattern
tion,8554‚Äì8564.
recognition,2117‚Äì2125.
Fragkiadaki,K.;Zhang,G.;andShi,J.2012. Videosegmentation
bytracingdiscontinuitiesinatrajectoryembedding. In2012IEEE Liu,Z.;Tan,Y.;He,Q.;andXiao,Y.2021. SwinNet:SwinTrans-
ConferenceonComputerVisionandPatternRecognition,1846‚Äì former drives edge-aware RGB-D and RGB-T salient object de-
1853.IEEE. tection. IEEE Transactions on Circuits and Systems for Video
Technology.
Gao,S.-H.;Tan,Y.-Q.;Cheng,M.-M.;Lu,C.;Chen,Y.;andYan,S.
2020.Highlyefficientsalientobjectdetectionwith100kparameters. Loshchilov,I.;andHutter,F.2017. Decoupledweightdecayregu-
InEuropeanConferenceonComputerVision,702‚Äì721.Springer. larization. arXivpreprintarXiv:1711.05101.
Gu,Y.;Wang,L.;Wang,Z.;Liu,Y.;Cheng,M.-M.;andLu,S.-P. Lu,X.;Wang,W.;Ma,C.;Shen,J.;Shao,L.;andPorikli,F.2019.
2020. Pyramidconstrainedself-attentionnetworkforfastvideo Seemore,knowmore:Unsupervisedvideoobjectsegmentationwith
salientobjectdetection. InProceedingsoftheAAAIconferenceon co-attentionsiamesenetworks. InProceedingsoftheIEEE/CVF
artificialintelligence,volume34,10869‚Äì10876. conferenceoncomputervisionandpatternrecognition,3623‚Äì3632.
Mahadevan,S.;Athar,A.;O≈°ep,A.;Hennen,S.;Leal-Taix√©,L.; Wei,J.;Wang,S.;andHuang,Q.2020. F3Net:fusion,feedback
andLeibe,B.2020. Makingacasefor3dconvolutionsforobject andfocusforsalientobjectdetection. InProceedingsoftheAAAI
segmentationinvideos. arXivpreprintarXiv:2008.11516. ConferenceonArtificialIntelligence,volume34,12321‚Äì12328.
Milletari, F.; Navab, N.; and Ahmadi, S.-A. 2016. V-net: Fully Wei,J.;Wang,S.;Wu,Z.;Su,C.;Huang,Q.;andTian,Q.2020.
convolutionalneuralnetworksforvolumetricmedicalimageseg- Labeldecouplingframeworkforsalientobjectdetection.InProceed-
mentation. In2016fourthinternationalconferenceon3Dvision ingsoftheIEEE/CVFconferenceoncomputervisionandpattern
(3DV),565‚Äì571.IEEE. recognition,13025‚Äì13034.
Nguyen,A.;Yan,Z.;andNahrstedt,K.2018. Yourattentionis Wu,J.;Jiang,Y.;Sun,P.;Yuan,Z.;andLuo,P.2022. Language
unique:Detecting360-degreevideosaliencyinhead-mounteddis- asQueriesforReferringVideoObjectSegmentation. InProceed-
playforheadmovementprediction. InProceedingsofthe26th ingsoftheIEEE/CVFConferenceonComputerVisionandPattern
ACMinternationalconferenceonMultimedia,1190‚Äì1198. Recognition,4974‚Äì4984.
Pang,Y.;Zhao,X.;Zhang,L.;andLu,H.2020. Multi-scaleinter- Wu,Z.;Su,L.;andHuang,Q.2019a. Cascadedpartialdecoder
activenetworkforsalientobjectdetection. InProceedingsofthe forfastandaccuratesalientobjectdetection. InProceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition, IEEE/CVFconferenceoncomputervisionandpatternrecognition,
9413‚Äì9422. 3907‚Äì3916.
Ren, S.; Liu, W.; Liu, Y.; Chen, H.; Han, G.; and He, S. 2021a. Wu,Z.;Su,L.;andHuang,Q.2019b. Stackedcrossrefinement
Reciprocaltransformationsforunsupervisedvideoobjectsegmen- networkforedge-awaresalientobjectdetection. InProceedingsof
tation. InProceedingsoftheIEEE/CVFconferenceoncomputer theIEEE/CVFinternationalconferenceoncomputervision,7264‚Äì
visionandpatternrecognition,15455‚Äì15464. 7273.
Ren,S.;Wen,Q.;Zhao,N.;Han,G.;andHe,S.2021b. Unify- Yan, P.; Li, G.; Xie, Y.; Li, Z.; Wang, C.; Chen, T.; and Lin, L.
ingGlobal-LocalRepresentationsinSalientObjectDetectionwith 2019. Semi-supervisedvideosalientobjectdetectionusingpseudo-
Transformer. arXivpreprintarXiv:2108.02759. labels. InProceedingsoftheIEEE/CVFinternationalconference
Song, H.; Wang, W.; Zhao, S.; Shen, J.; and Lam, K.-M. 2018. oncomputervision,7284‚Äì7293.
Pyramiddilateddeeperconvlstmforvideosalientobjectdetection.
Yang, C.; Zhang, L.; Lu, H.; Ruan, X.; and Yang, M.-H. 2013.
In Proceedings of the European conference on computer vision
Saliencydetectionviagraph-basedmanifoldranking. InProceed-
(ECCV),715‚Äì731.
ingsoftheIEEEconferenceoncomputervisionandpatternrecog-
Su, Y.; Deng, J.; Sun, R.; Lin, G.; and Wu, Q. 2022. A Uni- nition,3166‚Äì3173.
fiedTransformerFrameworkforGroup-basedSegmentation:Co-
Ye,L.;Rochan,M.;Liu,Z.;andWang,Y.2019. Cross-modalself-
Segmentation, Co-Saliency Detection and Video Salient Object
attentionnetworkforreferringimagesegmentation. InProceedings
Detection. arXivpreprintarXiv:2203.04708.
oftheIEEE/CVFconferenceoncomputervisionandpatternrecog-
Suzuki,T.;andYamanaka,T.2018. Saliencymapestimationfor nition,10502‚Äì10511.
omni-directional image considering prior distributions. In 2018
Zhang,P.;Kang,Z.;Yang,T.;Zhang,X.;Zheng,N.;andSun,J.
IEEEInternationalConferenceonSystems,Man,andCybernetics
2022. LGD:Label-GuidedSelf-DistillationforObjectDetection.
(SMC),2079‚Äì2084.IEEE.
InProceedingsoftheAAAIConferenceonArtificialIntelligence,
Tsiami, A.; Koutras, P.; and Maragos, P. 2020. Stavis: Spatio- volume36,3309‚Äì3317.
temporal audiovisual saliency network. In Proceedings of the
Zhang,Y.;Chao,F.-Y.;andZhang,L.2021. ASOD60K:AnAudio-
IEEE/CVF Conference on Computer Vision and Pattern Recog-
InducedSalientObjectDetectionDatasetforPanoramicVideos.
nition,4766‚Äì4776.
arXivpreprintarXiv:2107.11629.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;
Zhang,Z.;Xu,Y.;Yu,J.;andGao,S.2018. Saliencydetectionin
Gomez,A.N.;Kaiser,≈Å.;andPolosukhin,I.2017. Attentionisall
360videos.InProceedingsoftheEuropeanconferenceoncomputer
youneed. Advancesinneuralinformationprocessingsystems,30.
vision(ECCV),488‚Äì503.
Wang, H.; Jiang, X.; Ren, H.; Hu, Y.; and Bai, S. 2021. Swift-
Zhang,Z.;Yu,C.;andCrandall,D.2019. Aselfvalidationnetwork
net:Real-timevideoobjectsegmentation. InProceedingsofthe
forobject-levelhumanattentionestimation. AdvancesinNeural
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
InformationProcessingSystems,32.
1296‚Äì1305.
Wang, L.; Lu, H.; Wang, Y.; Feng, M.; Wang, D.; Yin, B.; and Zhao,C.;Li,X.;Dong,S.;andYounes,R.2022. Self-supervised
Ruan,X.2017. Learningtodetectsalientobjectswithimage-level Multi-Modal Video Forgery Attack Detection. arXiv preprint
supervision. InProceedingsoftheIEEEconferenceoncomputer arXiv:2209.06345.
visionandpatternrecognition,136‚Äì145. Zhao,X.;Pang,Y.;Zhang,L.;Lu,H.;andZhang,L.2020.Suppress
Wang,W.;Shen,J.;Li,X.;andPorikli,F.2015.Robustvideoobject andbalance:Asimplegatednetworkforsalientobjectdetection. In
cosegmentation. IEEETransactionsonImageProcessing,24(10): Europeanconferenceoncomputervision,35‚Äì51.Springer.
3137‚Äì3148. Zhou,J.;Wang,J.;Zhang,J.;Sun,W.;Zhang,J.;Birchfield,S.;
Wang,W.;Shen,J.;andPorikli,F.2015. Saliency-awaregeodesic Guo,D.;Kong,L.;Wang,M.;andZhong,Y.2022. Audio‚ÄìVisual
videoobjectsegmentation. InProceedingsoftheIEEEconference Segmentation. InEuropeanConferenceonComputerVision,386‚Äì
oncomputervisionandpatternrecognition,3395‚Äì3402. 403.Springer.
Wang,W.;Shen,J.;andShao,L.2017. Videosalientobjectdetec- Zhou, T.; Li, J.; Wang, S.; Tao, R.; and Shen, J. 2020. Matnet:
tionviafullyconvolutionalnetworks. IEEETransactionsonImage Motion-attentivetransitionnetworkforzero-shotvideoobjectseg-
Processing,27(1):38‚Äì49. mentation.IEEETransactionsonImageProcessing,29:8326‚Äì8338.
Wang,W.;Song,H.;Zhao,S.;Shen,J.;Zhao,S.;Hoi,S.C.;and Zhu,Y.;Zhai,G.;andMin,X.2018. Thepredictionofheadand
Ling,H.2019. Learningunsupervisedvideoobjectsegmentation eyemovementfor360degreeimages. SignalProcessing:Image
throughvisualattention. InProceedingsoftheIEEE/CVFConfer- Communication,69:15‚Äì25.
enceonComputerVisionandPatternRecognition,3064‚Äì3074.

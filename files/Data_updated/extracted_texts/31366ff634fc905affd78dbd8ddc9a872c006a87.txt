CodeBERTScore: Evaluating Code Generation
with Pretrained Models of Code
ShuyanZhou∗ UriAlon∗† SumitAgarwal GrahamNeubig
LanguageTechnologiesInstitute,CarnegieMellonUniversity
{shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu
Abstract developers’ time when implemented in tools such
asGitHub’sCopilot. ThissharpriseinLLMs’us-
Since the rise of neural natural-language-to-
ability was achieved thanks to their ability to ac-
code models (NL→Code) that can generate
curately generate long completions, which span
long expressions and statements rather than
multiple tokens and even lines, rather than only
a single next-token, one of the major prob-
lems has been reliably evaluating their gen- a single next-token as in early models (Allama-
erated output. In this paper, we propose nis and Sutton, 2013; Movshovitz-Attias and Co-
CodeBERTScore: an evaluation metric for hen,2013). Nevertheless, evaluatingandcompar-
codegeneration, whichbuildsonBERTScore ing different models has remained a challenging
(Zhang et al., 2020). Instead of encoding
problem(Xuetal.,2022)thatrequiresanaccurate
only the generated tokens as in BERTScore,
and reliable evaluation metric for the quality of
CodeBERTScorealsoencodesthenaturallan-
the models’ generated outputs, and existing met-
guageinputprecedingthegeneratedcode,thus
ricsaresub-optimal.
modeling the consistency between the gener-
ated code and its given natural language con-
text as well. We perform an extensive eval- Existing evaluation approaches The most
uation of CodeBERTScore across four pro-
common evaluation metrics are token-matching
gramming languages. We find that Code-
methods such as BLEU (Papineni et al., 2002),
BERTScoreachievesahighercorrelationwith
adopted from natural language processing. These
humanpreferenceandwithfunctionalcorrect-
ness than all existing metrics. That is, gener- metrics are based on counting overlapping n-
atedcodethatreceivesahigherscorebyCode- grams in the generated code and the reference
BERTScore is more likely to be preferred by code. CrystalBLEU (Eghbali and Pradel, 2022)
humans,aswellastofunctioncorrectlywhen extends BLEU by ignoring the 500 most oc-
executed. We release five language-specific
curring n-grams, arguing that they are trivially
pretrained models to use with our publicly
shared between the prediction and the reference.
availablecode. Ourlanguage-specificmodels
Nonetheless, both BLEU and CrystalBLEU rely
have been downloaded more than 1,000,000
timesfromtheHuggingfaceHub.1 on the lexical exact match of tokens, which does
not account for diversity in implementation,
1 Introduction variable names, and code conventions. Figure 1
shows an example: given the reference code
Natural-language-to-codegeneration(NL→Code)
in Figure 1(a), both BLEU and CrystalBLEU
has seen sharply growing popularity recently
prefer (rank higher) the non-equivalent code in
due to the emergence of large language models
Figure 1(b) over the functionally equivalent code
(LLMs) trained on vast amounts of natural lan-
inFigure1(c).
guage and code (Chen et al., 2021; Fried et al.,
CodeBLEU(Renetal.,2020)attemptstolower
2022; Zhou et al., 2023; Austin et al., 2021; Al-
therequirementforalexicalexactmatch,byrely-
lal et al., 2023). LLMs have reached such a high
ing on data-flow and Abstract Syntax Tree (AST)
NL→Code accuracy that they are now useful for
matching as well; nevertheless, valid generations
thebroadprogrammingaudienceandactuallysave
may have different ASTs and data flow from the
∗Equalcontribution
referencecode,whichmayleadtolowCodeBLEU
†NowatGoogleDeepMind
scoreevenwhenthepredictioniscorrect. Further,
1The code and data are available at https://github.com/
neulab/code-bert-score partial predictions may be useful for a program-
3202
tcO
13
]ES.sc[
2v72550.2032:viXra
Reference:
int f(Object target) {
int i = 0;
for (Object elem: this.elements) {
if (elem.equals(target)) {
return i;
}
i++;
}
return -1;
}
(a)Thegroundtruthreference–findtheindexoftargetinthis.elements.
Non-equivalentcandidate: Equivalentcandidate:
boolean f(Object target) { int f(Object target) {
for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) {
if (elem.equals(target)) { Object elem = this.elements.get(i);
return true; if (elem.equals(target)) {
} return i;
} }
}
return false; return -1;
} }
(b) Preferred by BLEU & CrystalBLEU – find (c) Preferred by CodeBERTScore – find the index of target in
whetherornottargetisinthis.elements. this.elements.
Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a)
shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among
these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in
Figure1(b),whichisnotfunctionallyequivalenttothereference,whileourproposedCodeBERTScoreprefersthe
codeinFigure1(c),whichisfunctionallyequivalenttothecodeinFigure1(a).
mer, but accepting them may lead to partial code et al., 2020). First, CodeBERTScore encodes
that doesnot parse, andthus cannot befully eval- the generated code and the reference code inde-
uated by CodeBLEU (for example, predicting the pendently with pretrained models, with the inclu-
firstlineofaforloop,withouttheloop’sbody). sionofnaturallanguageinstructionsorcomments.
Execution-basedevaluationattemptstoaddress Then, we compute the cosine similarity between
these problems by running tests on the generated the encoded representations of each token in the
code to verify its functional correctness (Chen generated code and each token in the reference
et al., 2021; Athiwaratkun et al., 2022; Li et al., code. Finally,thebestmatchingtokenvectorpairs
2022; Wang et al., 2022; Lai et al., 2022). This are used to compute precision and recall. Code-
provides a direct measure of the functionality of BERTScore allows comparing code pairs that are
the generated code while being agnostic to di- lexically different while taking into account the
versity in implementation and style. However, (1) programmatic- or natural-language-context, if
execution-based evaluation requires datasets that such provided; the (2) contextual information of
areprovidedwithhand-writtentestcasesforeach eachtoken;and(3)implementationdiversity. Our
example, which is costly and labor-intensive to approachisillustratedinFigure2.
create; thus, only few such datasets exist. Addi-
Example A concrete example is shown in Fig-
tionally, executing model-generated code is sus-
ure1: whileBLEUandCrystalBLEUprefer(rank
ceptibletosecuritythreats,andthusshouldberun
higher) the non-equivalent code in Figure 1(b)
inanisolatedsandbox,whichmakesittechnically
given the reference code in Figure 1(a), Code-
cumbersometoworkwithiteratively.
BERTScoreprefersthecodeinFigure1(c),which
Ourapproach Inthiswork,weintroduceCode- is functionally equivalent to the reference (Fig-
BERTScore,anevaluationmetricforcodegenera- ure 1(a)). We note that in this example, the vari-
tion,leveragingself-supervisedpretrainedmodels ablenamesareidenticalacrossallthreecodesnip-
of code such as CodeBERT (Feng et al., 2020), pets. When the variable names of the reference
and adopting best practices BERTScore (Zhang aredifferentthanthecandidate’s,itisevenharder
Natural Language Instruction Pairwise Cosine Similarity Similarity Matrix
(only between non-punctuation
# Find the square root of x
code tokens) Generated Code
maths qrt x
Reference Code Encode
x CodeBERTScore
x ** 0.5 Precision
CodeBERT
** CodeBERTScore
Generated Code Recall
math.sqrt(x) CodeBERT 0.5 CodeBERTScore
F-score
Figure2: AdiagramillustratingCodeBERTScore: Weusealanguage-specificCodeBERTmodeltoencodeeach
of⟨natural_language, reference_code⟩and⟨natural_language, generated_code⟩. Wethencomputethepairwise
cosine similarity between every encoded token in the reference and every encoded token in the generated code,
ignoringtheencodednaturallanguagecontexttokensandencodedpunctuationtokens; finally, wetakethemax
acrosstherowsoftheresultingmatrixtocomputePrecisionandacrosscolumnstocomputeRecall.
fortoken-matchingapproachessuchasBLEUand didates is more important than the absolute value
CrystalBLEU to compare the reference with the of f(yˆ,y∗). That is, ideally, if a prediction yˆ
1
candidates, while CodeBERTScore can trivially is more functionally equivalent to y∗ and more
match variable names according to their semantic preferable by human programmers over a predic-
similarityandtheirfunctionalroleinthecode. tionyˆ ,wewishthatagoodmetricwouldrankyˆ
2 1
higherthanyˆ . Thatis,weseekanf functionsuch
2
Contributions In summary, our main contribu-
thatf(yˆ ,y∗) > f(yˆ ,y∗).
1 2
tions are: (a) CodeBERTScore: a self-supervised
metric for NL→Code evaluation, based on
BERTScore, which leverages the benefits of pre-
trained models, while not requiring labeling or 2.2 Background: BERTScore
manually annotated data. (b) An extensive em-
pirical evaluation across four programming lan- BERTScore (Zhang et al., 2020) was proposed as
guages, showing that CodeBERTScore is more a method for evaluating mainly machine transla-
correlated with human preference and more cor- tion outputs. The idea in BERTScore is to en-
related with execution correctness than all pre- code the candidate sentence (the prediction) and
vious approaches including BLEU, CodeBLEU, the reference sentence (the ground truth) sepa-
andCrystalBLEU.(c)Wepretrainandreleasefive rately,usingaBERT-basedmodel,whichencodes
language-specific CodeBERT models to use with eachsequenceoftokensasasequenceofvectors.
our publicly available code, for Java, Python, C, Then, BERTScore computes the cosine similarity
C++, and JavaScript. As of the time of this sub- betweeneveryvectorfromthecandidatesequence
mission, our models have been downloaded from andeveryvectorfromthereferencesequences.
theHuggingfaceHubmorethan1,000,000times.
Giventhesesimilarityscores,BERTScorecom-
2 EvaluatingGeneratedCode putes sentence-level precision by taking the max-
imum similarity score for every candidate vec-
2.1 ProblemFormulation
tor and averaging, and computes recall by tak-
Given a context x ∈ X (e.g., a natural language ing the average of the maximum similarity scores
instructionorcomment),acodegenerationmodel for every reference vector. Intuitively, a high
M : X → Y produces a code snippet yˆ ∈ Y BERTScore-recall is obtained, for example, if ev-
by conditioning on the intent specified by x. The eryvectorfromthereferencesentencehasatleast
quality of the generation is evaluated by compar- one vector from the candidate sentence that is
ing yˆ ∈ Y with the reference implementation highly cosine-similar to it; a high BERTScore-
y∗ ∈ Y, usingametricfunctionf : Y ×Y → R, precisionisobtainedifeveryvectorfromthecan-
essentiallycomputingf(yˆ,y∗). didatesentenceishighlycosine-similartoatleast
Alargervalueoff(yˆ,y∗)indicatesthatthegen- one vector from the reference sentence. Ulti-
erated code is more accurate with respect to the mately, the final score is the F score, computed
1
referencecode,andthewayf ranksdifferentcan- astheharmonicmeanofprecisionandrecall.
edoC
ecnerefeR
1 (cid:88)
CodeBERTScore = max sim(y∗,yˆ ) (4)
P |yˆ[mˆ]| y∗ ∈y∗[m∗] i j
yˆj∈yˆ[mˆ] i
1 (cid:88)
CodeBERTScore = max sim(y∗,yˆ ) (5)
R |y∗[m]|
y∗∈y∗[m∗]yˆj ∈yˆ[mˆ]
i j
i
2·CodeBERTScore ·CodeBERTScore
P R
CodeBERTScore = (6)
F1
CodeBERTScore +CodeBERTScore
P R
10·CodeBERTScore ·CodeBERTScore
P R
CodeBERTScore = (7)
F3
9·CodeBERTScore +CodeBERTScore
P R
Figure3: MainequationsforCodeBERTScore
2.3 CodeBERTScore enizedsequence,resultinginsequencesofvectors:
OurapproachgenerallyfollowsBERTScore,with B(⟨x ,...,x ,y∗,...,y∗ ⟩)=⟨x ,...,x ,y∗,...,y∗ ⟩
1 k 1 m 1 k 1 m
thefollowingmaindifferences:
B(⟨x ,...,x ,yˆ ,...,yˆ ⟩)=⟨x ,...,x ,yˆ ,...,yˆ ⟩
1 k 1 n 1 k 1 n
1. We encode the context (the natural language (2)
instruction or comment) along with each of Finally, we mask out the encoded context tokens
the generated and reference code snippets, x ,...,x as well as all non-alphanumeric tokens
1 k
but without using the encoded context in the (parentheses, brackets, dots, commas, whites-
finalsimilaritycomputation,essentiallycom- paces, etc.) except for arithmetic operators, from
putingf(yˆ,y∗,x)ratherthanf(yˆ,y∗).
each of the encoded reference and encoded can-
2. Given the precision and recall, instead of didate. This results in encoded reference tokens
computing the F 1 score, we also compute F 3 y∗ = ⟨y 1∗,...,y m∗ ⟩,encodedcandidatetokensyˆ =
toweighrecallhigherthanprecision,follow- ⟨yˆ ,...,yˆ ⟩, and their corresponding masks m∗
1 n
ingMETEOR(BanerjeeandLavie,2005). andmˆ. Wedenotey[m]astheremainingencoded
3. As our underlying BERT-like model, we use tokens in y after selecting only alphanumeric to-
programming language-specific models that kenvectorsaccordingtothemaskm.
we pretrain and release, rather than models
Similarity Computation We compute the co-
thatwereintendedfornaturallanguageonly.
sinesimilaritybetweentheencodedreferenceand
We use a BERT-like pretrained model B to en-
candidatetokens,followingZhangetal.(2020):
code the reference and candidate. In our exper-
iments, B is a CodeBERT model that we fur-
y∗⊤·yˆ
ther pretrained using the masked language mod- sim(y∗,yˆ ) = i j (3)
i j ∥y∗∥·∥yˆ ∥
eling objective (Devlin et al., 2019) on language- i j
specific corpora, but B can be any transformer- Although this compares the individual tokens y∗
i
based model which we have access to its internal and yˆ , their vector representations y∗ and yˆ
j i j
hiddenstates.
contain information about their context, and thus
abouttheirsemanticroleinthecode.
TokenRepresentation Weconcatenatethecon-
textxwitheachofthereferenceandthecandidate, CodeBERTScore We use the similarity matrix
resulting in x·y∗ and x·yˆ. We use the tokenizer (seeFigure2),formedbythesimilarityscoresbe-
T providedwiththemodelB: tweeny∗ andyˆ, tocomputeprecision, recall, and
B
F , by taking the maximum across the rows and
1
columns of the similarity matrix, and then aver-
T (x·y∗) = ⟨x ,...,x ,y∗,...,y∗ ⟩
B 1 k 1 m (1) aging. Following Banerjee and Lavie (2005), we
T (x·yˆ) = ⟨x ,...,x ,yˆ ,...,yˆ ⟩
B 1 k 1 n also compute F by giving more weight to recall,
3
asshowninFigure3. Additionaldetailsregarding
to get a sequences of tokens. We run a standard token weighting and scaling are provided in Ap-
“forward pass” with the model B for each tok- pendixA.
3 ExperimentalSetup Correlation metrics We used three major cor-
relationmetrics. Followingbestpracticesinnatu-
We evaluate CodeBERTScore across multiple
rallanguageevaluation,weusedKendall-Tau(τ),
datasets and programming languages. We first
Pearson (r ) and Spearman (r ) to measure the
p s
show that CodeBERTScore is more correlated
correlation between each metric’s scores and the
with human preference than previous metrics, us-
references. Thedetailedequationscanbefoundin
inghuman-ratedsolutionsfortheCoNaLadataset
AppendixC.
(Yinetal.,2018a;Evtikhievetal.,2022). Wethen
show that CodeBERTScore is more correlated Human preference experiments We evaluate
withfunctionalcorrectness,usingtheHumanEval differentmetricsonCoNaLa(Yinetal.,2018b),a
dataset (Chen et al., 2021). We also show that naturallanguagetoPythoncodegenerationbench-
CodeBERTScore achieves a higher newly pro- mark collected from StackOverflow. We use the
posed distinguishability than other metrics (Ap- human annotation of Evtikhiev et al. (2022) to
pendixF).Finally,weanalyzesomeofthedesign measure the correlation between each metric and
decisionsandtheirimplications. human preference. More details are provided in
AppendixB.1.
3.1 TrainingLanguage-specificCodeBERT
models Functional correctness experiments We
evaluate functional correctness using the Hu-
Training We used CodeBERT (Feng et al.,
manEval (Chen et al., 2021) benchmark. Each
2020)asourbasemodel(B)andcontinueditsself-
example in HumanEval contains a natural
supervised pretraining (Gururangan et al., 2020)
language goal, hand-written input-output test
with the masked language modeling (MLM) ob-
cases, and a human-written reference solution.
jective(Devlinetal.,2019)onPython,Java,C++,
While the original HumanEval is in Python,
C, and JavaScript corpora. We trained a sepa-
Cassano et al. (2022) translated HumanEval to
rate model for each programming language, for
18 programming languages, and provided the
1,000,000 steps for each language, using a batch
predictionsoftheCodexmodel(Chenetal.,2021)
sizeof32,aninitiallearningrateof5e−5,decayed
(code-davinci-002)andtheircorresponding
linearly to 3e−5. Our implementation is based on
functional correctness.4 We used Java, C++,
the widely used HuggingFace Transformers
Python, and JavaScript for these experiments,
library (Wolf et al., 2019) and BERTScore2, and
whicharesomeofthemostpopularprogramming
itsupportsanytransformer-basedmodelavailable
languages in open-source projects.5 More details
ontheHuggingFacehub.
areprovidedinAppendixB.2.
Dataset Wetrainedeachmodelonthelanguage-
Hyperparameters Wetunedonlythefollowing
specific subset of the CodeParrot (Tunstall et al.,
hyperparametersforCodeBERTScore: whetherto
2022) dataset3, which consists of overall 115M
use F or F , and which layer of the underlying
code files from GitHub, further filtered by keep- 1 3
model to extract the encoded tokens from, which
ingonlyfileshavingaveragelinelengthlowerthan
we examine in Section 5. We used F in the hu-
100,morethan25%alphanumericcharacters,and 1
man preference experiments and F in the func-
non-auto-generated files. Even after 1,000,000 3
tionalcorrectnessexperiments. Weperform3-fold
trainingsteps,noneofthemodelshavecompleted
cross-validation and report average results across
even a single epoch, meaning that every training
the three folds. As for the layer to extract the to-
examplewasseenonlyonceatmost.
ken vectors from, we used layer 7 for CoNaLa,
3.2 ComparingDifferentMetrics andinHumanEvalweusedlayer7forJava,10for
C++,11forJavaScript,and9forPython.
We compare CodeBERTScore with existing met-
rics that are commonly used on code generation
4 Results
evaluation. We use human annotated preference
and execution-based results as the ground truth Correlation with human preference Table 2
andmeasuretheircorrelationwiththesemetrics. shows the correlation between different metrics
2https://github.com/Tiiiger/bert_score 4https://huggingface.co/datasets/nuprl/MultiPL-E
3https://huggingface.co/datasets/codeparrot/ 5https://octoverse.github.com/2022/
github-code-clean top-programming-languages
Java C++ Python JavaScript
Metric τ r τ r τ r τ r
s s s s
BLEU .481 .361 .112 .301 .393 .352 .248 .343
CodeBLEU .496 .324 .175 .201 .366 .326 .261 .299
ROUGE-1 .516 .318 .262 .260 .368 .334 .279 .280
ROUGE-2 .525 .315 .270 .273 .365 .322 .261 .292
ROUGE-L .508 .344 .258 .288 .338 .350 .271 .293
METEOR .558 .383 .301 .321 .418 .402 .324 .415
chrF .532 .319 .319 .321 .394 .379 .302 .374
CrystalBLEU .471 .273 .046 .095 .391 .309 .118 .059
CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402
Table 1: Kendall-Tau (τ) and Spearman (r ) correlations of each metric with the functional correctness on Hu-
s
manEvalinmultiplelanguages.Thecorrelationcoefficientsarereportedastheaverageacrossthreeruns.Standard
deviationisprovidedinTable3.
Metric τ r r achieves the highest or comparable Kendall-Tau
p s
andSpearmancorrelationwithfunctionalcorrect-
BLEU .374 .604 .543
nessacrossallfourlanguages. METEORachieves
CodeBLEU .350 .539 .495
acomparablecorrelationwithCodeBERTScorein
ROUGE-1 .397 .604 .570
Java and JavaScript, and its correlation is surpris-
ROUGE-2 .429 .629 .588
inglybetterthanotherbaselinemetrics. However,
ROUGE-L .420 .619 .574
in C++ and Python, CodeBERTScore is strictly
METEOR .366 .581 .540
better. Overallonaverageacrosslanguages,Code-
chrF .470 .635 .623
BERTScore is more correlated with functional
CrystalBLEU .411 .598 .576
correctnessthanallbaselines.
CodeBertScore .517 .674 .662
5 Analysis
Table2: TheKendall-Tau(τ),Pearson(r )andSpear-
p
man(r )correlationwithhumanpreference. Thebest We conducted a series of additional experiments
s
performance is bold. The correlation coefficients are to understand the importance of different design
reportedastheaverageacrossthreeruns. Standardde- decisions, and to gain insights on applying Code-
viationsareprovidedinTable4.
BERTScoretonewdatasetsandscenarios.
Can we use CodeBERTScore in a new lan-
andhumanpreference. CodeBERTScoreachieves
guagewithoutalanguage-specificCodeBERT?
the highest correlation with human preference,
In all experiments in Section 4, we used the
across all correlation metrics. While Evtikhiev
language-specific model which we continued to
et al. (2022) suggested that chrF and ROUGE-L
pretrain on each language. But what if we wish
are the most suitable metrics for evaluating code
touseCodeBERTScoreinalanguageinwhichwe
generation models in CoNaLa, CodeBERTScore
don’t have a language-specific model? We com-
outperforms these metrics by a significant mar-
pare the language-specific models to CodeBERT-
gin. For example, CodeBERTScore achieves
base in Figure 4. Generally, CodeBERT-base
Kendall-Tau correlation of 0.517 compared to
achievescloseperformancetoalanguage-specific
0.470 of chrF and 0.420 of ROUGE-L. These re-
model. However, in most HumanEval experi-
sultsshowthatgeneratedcodethatispreferredby
mentsandcorrelationmetrics,usingthelanguage-
CodeBERTScore— also tends to be preferred by
specific model is beneficial. These results show
humanprogrammers.
that language-specific models are often preferred
Correlation with functional correctness Ta- if such models are available, but the CodeBERT-
ble1showsthecorrelationbetweendifferentmet- base can still provide close performance even
rics and functional correctness: CodeBERTScore withoutlanguage-specificpretraining.
τ-Lang-specific τ-Basemodel r -Lang-specific r -Basemodel
s s
0.7
0.650.65
0.6
0.560.56
0.520.52
0.5
0.460.45
0.4 0.380.39 0.37 0.370.38
0.340.35 0.330.32 0.35 0.34
0.3
0.3
0.2
0.1
0
CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript
Figure4: TheKendall-TauandSpearmanonthedevelopmentsetofdifferentdatasetswiththelanguage-specific
pretrainedmodel(Lang-specific)andwiththebaseCodeBERT(Basemodel).
0.45 tion, while higher layers encode deeper semantic
meaninginnaturallanguage.
Does encoding natural language context help?
0.4
One major difference between CodeBERTScore
and BERTScore is that CodeBERTScore lever-
0.35 ages the context for the generated code, such as
the natural language instruction or intent that was
given as input for generation. We find that us-
Java
0.3 C++ ingcontextincreasesthecorrelation,forexample,
JavaScript the Kendall-Tauof CodeBERTScore from0.50 to
Python 0.52. While this paper mainly focuses on natu-
0.25 ral language instructions, we believe that Code-
0 2 4 6 8 10 12
BERTScore can thus benefit other programming
Layer
scenarios as well, for example when generating
code given the human-written comments, or gen-
Figure 5: The average of Kendall-Tau and Spearman
onthedevelopmentsetofHumanEvalwhenusingthe eratingcodegiventheprecedingcodecontext.
embeddingsfromdifferentlayers.
CodeBERTScore allows soft matching of
tokens The heatmaps in Figure 6 show the sim-
ilarityscoresbetweentokensinCodeBERTScore.
Which transformer layer should we use? We For example, both shutil.rmtree and
further investigate the impact of using hidden os.rmdir in Figure 6(a) delete a folder;
states from different layers of the model — the CodeBERTScorealignseachtokentoarespective
layerwhichthevectorsinEquation(2)comefrom, tokenintheotherexpression,eventhoughthetwo
in the computation of CodeBERTScore. The re- spansdonotsharemanyidenticaltokens.
sults are shown in Figure 5: generally, the deeper In Figure 6(b), both code snippets calculate a
the layer – the higher the average correlation be- squareroot,whereoneusesmath.sqrt(x)and
tween CodeBERTScore and functional correct- the other uses x ** 0.5. An exact surface-
ness,acrossallprogramminglanguages. However form-matchingmetricsuchaschrFwouldassigna
in almost all languages, performance reaches its lowsimilarityscoretothiscodepair,astheyonly
maximum before the last layer, and decreases at sharethetokenx. However, CodeBERTScoreas-
thefollowinglayers. Thissuggeststhathigherlay- signsnon-zeroscorestoeachtokenwithmeaning-
ersencodethesemanticinformationofeachtoken ful alignments, such as matching [sq,rt] with
more accurately, but the final layers may be more [_0,5],sinceasquarerootisthe0.5-thpower.
task-specific. These observations are consistent Additionally, we study the robustness of Code-
with Tenney et al. (2019), who found that lower BERTScore to adversarial perturbations. We
layers in BERT tend to process shallow informa- found that token-based metrics such as chrF are
0.8
os
math
0.7
.
.
0.6
r
sq
0.5
md
rt 0.4
ir
( 0.3
(
0.2
x
folder
0.1
) )
0.0
sh util . r mt ree ( folder ) x _** _0 . 5
(a) (b)
Figure 6: Heatmaps of the similarity scores between two pieces of code that achieve the same goal. Figure 6(a)
shows the similarity scores between os.rmdir(folder) and shutil.rmtree(folder). Figure 6(b)
showsthesimilarityscoresbetweenmath.sqrt(x)andx ** 0.5.
muchmorepronetomatchingtrivialtokensrather correlatewellwithexecutionaccuracy.
thantokensthatpreservethesemanticmeaningof
Execution-based Metrics To alleviate previous
thecode. ExamplescanbefoundinAppendixE.
issues,execution-basedevaluationcountsagener-
Additional discussion and experiments regard-
ated code snippet as correct if it produces the re-
ing the distinguishability of CodeBERTScore are
quired outputs when run with given inputs (Chen
providedinAppendixF.Additionalgeneralexam-
et al., 2021; Athiwaratkun et al., 2022; Li et al.,
plesareprovidedinAppendixG.
2022; Wang et al., 2022; Lai et al., 2022; Huang
6 RelatedWork et al., 2022). However, execution-based evalua-
tion requires datasets that are provided with man-
Token-based metrics Metrics such as BLEU uallycraftedtestcasesforeachexample,whichis
(Papineni et al., 2002) evaluate code generation costlyandlabor-intensivetocreate;thus,onlyfew
bycountingmatchingn-gramsbetweengenerated such datasets exist. In contrast, CodeBERTScore
and reference code. CrystalBLEU (Eghbali and is completely unsupervised and does not depend
Pradel, 2022) refines this approach by disregard- onanyspecificdataset. Further,executingmodel-
ing trivially shared n-grams, while ROUGE (Lin, generated code is susceptible to security threats,
2004) and METEOR (Banerjee and Lavie, 2005) and thus should be run in an isolated sandbox,
emphasize recall and balance of precision and re- which makes it technically cumbersome to work
call respectively. However, these metrics, relying withiteratively.
on exact lexical matches, often fail to capture se-
mantically equivalent but lexically different code 7 Conclusion
snippets. Unlike these, CodeBERTScore captures
Inthispaper,wepresentCodeBERTScore,asim-
the wide, two-sided context of each token, which
ple evaluation metric for code generation, which
n-gramscannotcapture.
builds on BERTScore (Zhang et al., 2020), using
Static analysis-based metrics CodeBLEU pretrained language models of code, and lever-
(Ren et al., 2020) incorporates data-flow and aging the natural language context of the gen-
AbstractSyntaxTree(AST)matching,inaddition erated code. We perform an extensive evalua-
to token-matching. However, valid code may not tion across four programming languages which
always align in ASTs and data-flows. Addition- shows that CodeBERTScore is more correlated
ally, partial code, although potentially useful, withhumanpreferencethanallpriormetrics. Fur-
may not parse, thus cannot be fully evaluated by ther, we show that generated code that receives a
CodeBLEU.Further,ashighlightedbysubsequent higher score by CodeBERTScore is more likely
studies (Wang et al., 2022), CodeBLEU does not to function correctly when executed. Finally, we
release five programming language-specific pre- evaluatingencoder-decoderordecoder-onlyasthe
trained models to use with our publicly available NL→Codegeneratormodels.
code. These models were downloaded more than Another point to consider is that Code-
1,000,000 times from the HuggingFace Hub. Our BERTScore relies on a strong underlying BERT-
code and data are available at https://github.com/ basedmodel,whilemethodssuchasBLEUdonot
neulab/code-bert-score. have many “moving parts” or hyperparameters to
tune. However, this is mostly an advantage, since
Acknowledgement
CodeBERTScore can be further improved in the
futureusingstrongerbasemodels.
WethankMishaEvtikhiev, EgorBogomolov, and
Timofey Bryksin for the discussions, and for the
data from their paper (Evtikhiev et al., 2022). We
References
thankanonymousreviewersforthevaluablefeed-
back. We are grateful to Yiwei Qin for the dis- Loubna Ben Allal, Raymond Li, Denis Kocetkov,
cussions regarding the T5Score paper (Qin et al., Chenghao Mou, Christopher Akiki, Carlos Munoz
Ferrandis, Niklas Muennighoff, Mayank Mishra,
2022); the idea to use functional correctness as
Alex Gu, Manan Dey, et al. 2023. Santa-
a meta-metric was born thanks to the discus- coder: don’t reach for the stars! arXiv preprint
sion with her. We are also grateful to Aryaz arXiv:2301.03988.
Eghbali and Michael Pradel for the discussions
MiltiadisAllamanisandCharlesSutton.2013. Mining
about CrystalBLEU (Eghbali and Pradel, 2022).
sourcecoderepositoriesatmassivescaleusinglan-
This material is partly based on research spon-
guage modeling. In 2013 10th working conference
sored in part by the Air Force Research Labora- onminingsoftwarerepositories(MSR),pages207–
toryunderagreementnumberFA8750-19-2-0200. 216.IEEE.
The U.S. Government is authorized to reproduce
Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian
anddistributereprintsforGovernmentalpurposes
Wang, Xiaopeng Li, Yuchen Tian, Ming Tan,
notwithstanding any copyright notation thereon.
Wasi Uddin Ahmad, Shiqi Wang, Qing Sun,
The views and conclusions contained herein are Mingyue Shang, et al. 2022. Multi-lingual evalu-
those of the authors and should not be interpreted ation of code generation models. ArXiv preprint,
abs/2210.14868.
as necessarily representing the official policies or
endorsements, either expressed or implied, of the
JacobAustin,AugustusOdena,MaxwellNye,Maarten
Air Force Research Laboratory or the U.S. Gov- Bosma, Henryk Michalewski, David Dohan, Ellen
ernment. Thisprojectwasalsopartiallysupported Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.
byagiftfromAWSAI. 2021. Programsynthesiswithlargelanguagemod-
els. ArXivpreprint,abs/2108.07732.
Limitations
SatanjeevBanerjeeandAlonLavie.2005. METEOR:
An automatic metric for MT evaluation with im-
CodeBERTScore requires a GPU for computing
proved correlation with human judgments. In Pro-
themetric,whiletraditionalmetricssuchasBLEU
ceedingsoftheACLWorkshoponIntrinsicandEx-
require only a CPU. This adds a hardware re- trinsic Evaluation Measures for Machine Transla-
quirement to the evaluation of models of code, tion and/or Summarization, pages 65–72, Ann Ar-
bor, Michigan. Association for Computational Lin-
whilemostpreviousapproachesarecomputation-
guistics.
ally cheaper (e.g., by counting n-grams). How-
ever, since training and testing neural models re-
FedericoCassano,JohnGouwar,DanielNguyen,Syd-
quire GPU anyways, we can safely assume that a ney Nguyen, Luna Phipps-Costin, Donald Pinck-
GPUisavailable. Further, BERT-basemodelsare ney, Ming Ho Yee, Yangtian Zi, Carolyn Jane An-
derson, Molly Q Feldman, et al. 2022. A scalable
encoder-only and non-autoregressive; this means
and extensible approach to benchmarking nl2code
that they require only a single “forward pass”,
for 18 programming languages. ArXiv preprint,
compared to encoder-decoder models (e.g., T5) abs/2208.08227.
and decoder-only models (e.g., GPT-3) that need
toautoregressivelygeneratetokenaftertoken,us- MarkChen,JerryTworek,HeewooJun,QimingYuan,
HenriquePonde,JaredKaplan,HarriEdwards,Yura
ing a forward pass for each output token. Thus,
Burda, Nicholas Joseph, Greg Brockman, et al.
the additional time consumption by encoder-only
2021. Evaluatinglargelanguagemodelstrainedon
models(e.g.,BERT)isnegligible,especiallywhen code. ArXivpreprint,abs/2107.03374.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and DalLago,etal.2022. Competition-levelcodegen-
Kristina Toutanova. 2019. BERT: Pre-training of eration with alphacode. Science, 378(6624):1092–
deepbidirectionaltransformersforlanguageunder- 1097.
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association Chin-Yew Lin. 2004. Rouge: A package for auto-
for Computational Linguistics: Human Language maticevaluationofsummaries. InTextsummariza-
Technologies, Volume 1 (Long and Short Papers), tionbranchesout,pages74–81.
pages4171–4186,Minneapolis,Minnesota.Associ-
Dana Movshovitz-Attias and William Cohen. 2013.
ationforComputationalLinguistics.
Natural language models for predicting program-
mingcomments. InProceedingsofthe51stAnnual
AryazEghbaliandMichaelPradel.2022. Crystalbleu:
Meeting of the Association for Computational Lin-
preciselyandefficientlymeasuringthesimilarityof
guistics(Volume2: ShortPapers),pages35–40.
code. In37thIEEE/ACMInternationalConference
onAutomatedSoftwareEngineering,pages1–12.
KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu: amethodforautomaticeval-
Mikhail Evtikhiev, Egor Bogomolov, Yaroslav
uationofmachinetranslation. InProceedingsofthe
Sokolov, and Timofey Bryksin. 2022. Out of the
40th Annual Meeting of the Association for Com-
bleu: howshouldweassessqualityofthecodegen-
putationalLinguistics,pages311–318,Philadelphia,
erationmodels? ArXivpreprint,abs/2208.03133.
Pennsylvania,USA.AssociationforComputational
Linguistics.
ZhangyinFeng,DayaGuo,DuyuTang,NanDuan,Xi-
aochengFeng,MingGong,LinjunShou,BingQin,
YiweiQin,WeizheYuan,GrahamNeubig,andPengfei
TingLiu,DaxinJiang,andMingZhou.2020. Code-
Liu. 2022. T5score: Discriminative fine-tuning
BERT: A pre-trained model for programming and of generative evaluation metrics. arXiv preprint
natural languages. In Findings of the Association arXiv:2212.05726.
forComputationalLinguistics:EMNLP2020,pages
1536–1547, Online. Association for Computational Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie
Linguistics. Liu,DuyuTang,NeelSundaresan,MingZhou,Am-
brosio Blanco, and Shuai Ma. 2020. Codebleu: a
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida method for automatic evaluation of code synthesis.
Wang,EricWallace,FredaShi,RuiqiZhong,Wen- ArXivpreprint,abs/2009.10297.
tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022.
Incoder: A generative model for code infilling and Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.
synthesis. ArXivpreprint,abs/2204.05999. BERT rediscovers the classical NLP pipeline. In
Proceedingsofthe57thAnnualMeetingoftheAsso-
Suchin Gururangan, Ana Marasovic´, Swabha ciationforComputationalLinguistics,pages4593–
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, 4601, Florence, Italy. Association for Computa-
and Noah A. Smith. 2020. Don’t stop pretraining: tionalLinguistics.
Adapt language models to domains and tasks. In
Proceedings of the 58th Annual Meeting of the LewisTunstall,LeandrovonWerra,andThomasWolf.
Association for Computational Linguistics, pages 2022. Natural Language Processing with Trans-
formers. "O’ReillyMedia,Inc.".
8342–8360, Online. Association for Computational
Linguistics.
Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Gra-
ham Neubig. 2022. Execution-based evaluation
Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong
for open-domain code generation. ArXiv preprint,
Yan, Haotian Cui, Jeevana Priya Inala, Colin
abs/2212.10481.
Clement, and Nan Duan. 2022. Execution-based
evaluation for data science code generation mod-
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
els. In Proceedings of the Fourth Workshop on
Chaumond,ClementDelangue,AnthonyMoi,Pier-
Data Science with Human-in-the-Loop (Language
ric Cistac, Tim Rault, Rémi Louf, Morgan Fun-
Advances), pages 28–36, Abu Dhabi, United Arab
towicz, et al. 2019. Huggingface’s transformers:
Emirates (Hybrid). Association for Computational
State-of-the-art natural language processing. ArXiv
Linguistics.
preprint,abs/1910.03771.
YuhangLai,ChengxiLi,YimingWang,TianyiZhang, FrankF.Xu,UriAlon,GrahamNeubig,andVincentJ.
Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Hellendoorn.2022. Asystematicevaluationoflarge
Yih, Daniel Fried, Sida Wang, and Tao Yu. 2022. languagemodelsofcode.
Ds-1000: A natural and reliable benchmark for
data science code generation. ArXiv preprint, Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan
abs/2211.11501. Vasilescu,andGrahamNeubig.2018a. Learningto
mine aligned code and natural language pairs from
Yujia Li, David Choi, Junyoung Chung, Nate Kush- stackoverflow. InInternationalConferenceonMin-
man, Julian Schrittwieser, Rémi Leblond, Tom ing Software Repositories, MSR, pages 476–486.
Eccles, James Keeling, Felix Gimeno, Agustin ACM.
Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan
Vasilescu,andGrahamNeubig.2018b. Learningto
mine aligned code and natural language pairs from
stackoverflow. InProceedingsofthe15thInterna-
tionalConferenceonMiningSoftwareRepositories,
pages476–486.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
WendiZheng,XiaoXia,etal.2022. Glm-130b: An
open bilingual pre-trained model. ArXiv preprint,
abs/2210.02414.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger,andYoavArtzi.2020. Bertscore: Eval-
uating text generation with BERT. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020.OpenReview.net.
ShuyanZhou,UriAlon,FrankF.Xu,ZhengbaoJiang,
andGrahamNeubig.2023. Docprompting: Gener-
ating code by retrieving the docs. In International
Conference on Learning Representations (ICLR),
Kigali,Rwanda.
A AdditionalDetails 2860 annotated code snippets (5 generations ×
472examples)whereeachsnippetisgradedby4.5
F Thewell-knownF scoreiscomputedas:
β 1 annotators.
2 2·precision·recall
F 1 = 1 + 1 = precision+recall B.2 FunctionalCorrectness
recall precision
(4) We evaluate functional correctness using the Hu-
A more general F score F uses a positive factor manEval(Chenetal.,2021)benchmark. Eachex-
β
β,whererecallisconsideredβ timesasimportant ample in HumanEval contains a natural language
asprecision: goal, hand-written input-output test cases, and a
human-written reference solution. On average,
(cid:0) 1+β2(cid:1)
·precision·recall each example has 7.7 test cases and there are 164
F = (5)
β β2·precision+recall examples in total. While the original HumanEval
is in Python, Cassano et al. (2022) translated Hu-
As found in METEOR (Banerjee and Lavie,
manEval to 18 programming languages, and pro-
2005), using F with β = 3, thus preferring re-
β vided the predictions of the Codex model (Chen
call over precision, results in a higher correlation
et al., 2021) (code-davinci-002) and their
with human preference in machine translation. In
corresponding functional correctness.6 We used
our experiments, we found that this applies to
Java, C++, Python, and JavaScript for these ex-
NL→Codeaswell.
periments, which are some of the most popular
Token Weighting Following Zhang et al.
programminglanguagesinopen-sourceprojects.7
(2020), we compute the inverse document fre- Notably,Cassanoetal.(2022)didnottranslatethe
quency (idf), according to a language-specific reference solutions to the other languages, so, we
test set, and weigh each token according to its collected these from HumanEval-X (Zeng et al.,
negativelogfrequency. 2022).8 The reference score of every example is
either 1 (“correct”, if it passes all test cases) or 0
Scaling Following Zhang et al. (2020), the co-
(“incorrect”,otherwise).
sine similarity scores of hidden states tend to lie
inalimitedrange. Thus,wecanlinearlyscalethe C CorrelationMetrics
resultingscores,usinganempiricalbasescalarb:
Kendall-Tau (τ) τ measures the ordinal/rank
(cid:92) CodeBERTScore−b association between a metric such as Code-
CodeBERTScore =
1−b
BERTScore and the reference measurement. It is
(6)
calculatedas:
This typically spreads the CodeBERTScore F
1
scores to the [0,1] range, and is merely a cos- |concordant|−|discordant|
τ =
metical change: this scaling does not change the
|concordant|+|discordant|
way CodeBERTScore ranks different prediction,
butcanbeslightlymoreintuitiveandeasiertoin- where|concordant|representsthenumberofpairs
terpret. We computed b empirically by sampling where two measurements agree on their relative
random unrelated code pairs and measuring their rank. That is, if f(yˆ,y∗) > f(yˆ,y∗), the
1 1 2 2
average similarity score. For Java, the empirical reference measurement also yields f∗(yˆ,y∗) >
1 1
b Java was0.78andforC++,b C++ itwas0.76. f∗(yˆ 2,c∗ 2). Similarly, |discordant| represents the
number of pairs where two measurements yield
B EvaluationDetails
opposite ranks. Notably, in our experiments, we
B.1 HumanPreference restrictthecomparisonsofrankswithinthegener-
ationsofthesamequestion.
For each example, Evtikhiev et al. (2022) asked
experiencedsoftwaredeveloperstogradethegen- Pearson (r ) r measures the linear correlation
p p
erated code snippets from five different models.
between a metric and the reference measurement.
The grade scales from zero to four, with zero de-
notingthatthegeneratedcodeisirrelevantandun-
6https://huggingface.co/datasets/nuprl/MultiPL-E
7https://octoverse.github.com/2022/
helpful,andfourmeaningthatthegeneratedcode
top-programming-languages
solves the problem accurately. Overall, there are 8https://huggingface.co/datasets/THUDM/humaneval-x
Itisdefinedas: hasthesamevariablenamebutwithoutanyfunc-
tion call. Contrarily, chrF assigns a higher rank-
r = (cid:80)N i=1(f(yˆ i,y i∗)−f¯)(f∗(yˆ i,y i∗)−f¯∗) ing to (folder) which has a longer common
p (cid:113)
(cid:80)N i=1(f(yˆ i,y i∗)−f¯)2(cid:80)N i=1(f∗(yˆ i,y i∗)−f¯∗)2 sequence of characters, although semantically in-
equivalent.
where N is the number of generations in the
dataset, f¯ is the mean CodeBERTScore of the F DistinguishingCodewithDifferent
dataset, and f¯∗ is the mean similarity score cal- Semantics
culatedbythereferencemeasurement.
WestudyhowwellcanCodeBERTScoreperform
Spearman (r ) r measures the Pearson corre- as a generic similarity function that measures the
s s
lationcoefficientbetweentheranksproducedbya similarity between two arbitrary code snippets y i
metricandthereferencemeasurement: andy j.
F.1 DistinguishabilityMetric
cov(R(f(Yˆ),R(f∗(Y∗)))
r p = We evaluate CodeBERTScore using the distin-
σ σ
R(f(Yˆ)) R(f∗(Y∗))
guishability metric d proposed by Eghbali and
where R returns the ranks of code snippets in a Pradel(2022)whichiscalculatedasfollows:
collection of code snippets Y. cov(·,·) is the co- (cid:80)
f(y ,y )
variance of two variables and σ(·) is the standard d = yi,yj∈Pairsintra i j (7)
(cid:80)
f(y ,y )
deviation. yi,yj∈Pairsinter i j
wherePair definesasetofcodepairsfromthe
D StandardDeviation intra
samesemanticallyequivalentclusters,andPair
inter
Table 3 shows the same results as in Table 1, but definesasetofcodepairsfromtwoclustersofdif-
withstandarddeviations. Table4showstheresults ferentfunctionality. Formally,
fromTable2,withstandarddeviations.
Pair ={(y ,y ) | ∃k suchthaty ,y ∈ C }
intra i j i j k
E Robustnesstoadversarial Pair ={(y ,y ) | ∃k suchthaty ∈ C ,y ∈/ C }
inter i j i k j k
perturbations
where C is the k-th cluster with semantically
k
equivalent code snippets. Intuitively, a similar-
Ref:shutil.rmtree(folder) ityfunctionf thatcandistinguishbetweensimilar
and dissimilar code will produce d larger than 1,
Candidate CodeBERTScore chrF
meaningthatapairofcodesnippetsfromthesame
os.rmdir(folder) 1st 1st
semanticclusterhasahighersimilarityscorethan
os.rmdir(f) 2nd 3rd
(folder) 3rd 2nd apairofsnippetsfromdifferentclusters. Sincethe
number of intra-class and inter-class pairs grows
Figure 7: The similarity rankings of three quadraticallywiththenumberofcodesnippets,in
code snippets given the reference code our experiments we followed Eghbali and Pradel
shutil.rmtree(folder). While Code-
(2022) to sample N inter- and N intra-class pairs
BERTScore correctly ranks os.rmdir(f) over
instead.
the the non-equivalent (folder), chrF prefers just
(folder)overos.rmdir(f).
F.2 DatasetwithSemanticallyequivalent
clusters
WeconductedaqualitativeevaluationofCode-
We follow Eghbali and Pradel (2022) to evaluate
BERTScore under various perturbations. An
whether CodeBERTScore can distinguish similar
example is shown in Figure 7, which shows
and dissimilar code mined from ShareCode9, an
the CodeBERTScore and chrF rankings of
onlinecodingcompetitionplatform. Semantically
three code snippets based on the similarity
equivalentcodesnippetsarefromthesamecoding
to the reference shutil.rmtree(folder).
problem, and they all pass the unit tests provided
CodeBERTScore gives a higher ranking to
by the platform. The dataset consists 6958 code
the code snippet that employs the appropriate
API(os.rmdir)thanthetrivial(folder)that 9https://sharecode.io/
Java C++ Python JavaScript
Metric τ r τ r τ r τ r
s s s s
BLEU .481(±.030) .361(±.037) .112(±.059) .301(±.054) .393(±.083) .352(±.064) .248(±.075) .343(±.052)
CodeBLEU .496(±.034) .324(±.037) .175(±.021) .201(±.037) .366(±.079) .326(±.075) .261(±.065) .299(±.043)
ROUGE-1 .516(±.052) .318(±.043) .262(±.073) .260(±.024) .368(±.092) .334(±.054) .279(±.092) .280(±.068)
ROUGE-2 .525(±.049) .315(±.047) .270(±.073) .273(±.036) .365(±.094) .322(±.077) .261(±.077) .292(±.057)
ROUGE-L .508(±.060) .344(±.038) .258(±.091) .288(±.027) .338(±.103) .350(±.064) .271(±.078) .293(±.046)
METEOR .558(±.058) .383(±.027) .301(±.061) .321(±.023) .418(±.090) .402(±.049) .324(±.075) .415(±.022)
chrF .532(±.067) .319(±.035) .319(±.056) .321(±.020) .394(±.096) .379(±.058) .302(±.073) .374(±.044)
CrystalBLEU .471(±.024) .273(±.067) .046(±.009) .095(±.064) .391(±.080) .309(±.073) .118(±.057) .059(±.069)
CodeBERTScore .553(±.068) .369(±.049) .327(±.086) .393(±.048) .422(±.090) .415(±.071) .319(±.054) .402(±.030)
Table 3: Kendall-Tau (τ) and Spearman (r ) correlations of each metric with the functional correctness on Hu-
s
manEval in multiple languages. The correlation coefficients are reported as the average across three runs, along
withthestandarddeviation.
Metric τ r r
p s
BLEU .374(±.025) .604(±.016) .543(±.018)
CodeBLEU .350(±.037) .539(±.033) .495(±.037)
ROUGE-1 .397(±.023) .604(±.016) .570(±.018)
ROUGE-2 .429(±.025) .629(±.015) .588(±.022)
ROUGE-L .420(±.037) .619(±.014) .574(±.022)
METEOR .366(±.033) .581(±.016) .540(±.022)
chrF .470(±.029) .635(±.023) .623(±.018)
CrystalBLEU .411(±.030) .598(±.019) .576(±.034)
CodeBertScore .517(±.024) .674(±.012) .662(±.012)
Table 4: The Kendall-Tau (τ), Pearson (r ) and Spearman (r ) correlation with human preference. The best
p s
performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside
parenthesesindicatethestandarddeviations.
Metric Java C++ ThisresultconfirmsthatCodeBERTScoreassigns
higher similarity scores to semantically similar
BLEU 2.36 2.51
CodeBLEU 1.44 1.42 codepairs,comparedtorandomlypairedsnippets
CrystalBLEU 5.96 6.94
thatbelongtodifferentsemanticclasses.
CodeBERTScore 9.56 9.13
Can We Hack the Distinguishability Metric?
Table5:Distinguishabilitywithdifferentmetricsasthe
DespitetheencouragingresultsinTable5,wealso
similarityfunction. CodeBERTScoreachievesahigher
foundthatdistinguishabilitycanbeeasilymanipu-
distinguishability than CrystalBLEU, which proposed
latedsinceitcomparesabsolutescoresacrossdif-
thismeta-metric,onthesamedatasets.
ferent metrics. For example, while CrystalBLEU
achievesadistinguishabilityscoreof5.96,wecan
snippets covering 278 problems in Java and C++. craftavariantofCodeBERTScorethatachievesa
We use CodeBERTScore to calculate the similar- distinguishability score of 120,000 by simple ex-
ityscoreforcodepairsthatsharethesameseman- ponentiationofCodeBERTScore’soutputscore.
ticclassandcodepairsthatdonot. Wethenmea- To illustrate this, we conducted a distinguisha-
surethedistinguishabilityofCodeBERTScoreac- bility evaluation with the same configurations as
cording to Equation 7. The results are shown in before, but with a variant of CodeBERTScore
Table5. thatwecallCodeBERTScorek,anddefinedasthe
Table 5 shows that CodeBERTScore achieves compositionofCodeBERTScorewiththef(x) =
a higher distinguishability than CrystalBLEU, xk function, that is: CodeBERTScorek(y 1,y 2) =
which proposed this meta-metric, in both Java (CodeBERTScore(y 1,y 2))k.
andC++. CodeBERTScoreachievesdistinguisha- As Figure 8 shows, distinguishability of
bility scores of 9.56 in Java while CrystalBLEU CodeBERTScorek increases almost exponentially
achieves 5.96; in C++, CodeBERTScore achieves while increasing k, although the base Code-
9.13 while CrystalBLEU achieves only 6.94. BERTScoremetrichasnotchanged.
130,000
120,000 CodeBERTScorek
110,000
100,000
90,000
80,000
70,000
60,000
50,000
40,000
30,000
20,000
10,000
0
0 10 20 30 40 50
k
Figure8:Distinguishabilitybyexponentiatingtheorig-
inalCodeBERTScorebyk.
We thus argue that distinguishability is not
a reliable meta-metric and is no substitute for
execution-based-orhuman-rating. Wefurthersus-
pectthatanymeta-metricthatcomparesexact,ab-
solute, scores across different metrics is suscepti-
bletosuchmanipulations,andthereliablewayto
comparemetricsisaccordingtothewaytheyrank
differentexamples,ratherthantheexactscores.
The distinguishability results of
CodeBERTScorek with different values of k
are shown in Figure 8. As Figure 8 shows, the
distinguishability increases almost exponentially
with the increasing value of k. We thus argue
that distinguishability is not a reliable meta-
metric and is no substitute for execution-based-
or human-rating. We further suspect that any
meta-metric that compares exact, absolute, scores
across different metrics is susceptible to such
manipulations, and the reliable way to compare
metricsisaccordingtothewaytheyrankdifferent
examples,ratherthantheexactscores.
G AdditionalExamples
In this section, we provide additional examples
inwhichCodeBERTScoreprefersthefunctionally
correct prediction, while the best baseline metric
in each language ranks higher a functionally in-
correctprediction,whichisinequivalenttotheref-
erence. Figure 9 shows an example in Java, and
Figure10showsaC++example.
NaturalLanguageQuestion: Reference:
/** public static int howManyTimes(String string,
Find how many times a given String substring) {
substring can be found in int times = 0;
the original string. for (int i = 0; i < string.length() -
Count overlaping cases. substring.length() + 1; i++) {
>>> howManyTimes("", "a") if (string.substring(i, i + substring.length())
0 .equals(substring)) {
>>> howManyTimes("aaa", "a") times += 1;
3 }
>>> howManyTimes("aaaa", "aa") }
3 return times;
*/ }
(a)Thenaturallanguagequestion. (b)Thegroundtruthreference.
Non-equivalentcandidate: Equivalentcandidate:
public static int howManyTimes( public static int howManyTimes(
String string, String substring) { String string, String substring) {
int count = 0; int counter = 0;
int index = 0; int index = 0;
while ((index = string.indexOf( while (true) {
substring, index)) != -1) { index = string.indexOf(substring, index);
count++; if (index == -1)
index += substring.length(); break;
} counter += 1;
return count; index += 1;
} }
return counter;
}
(c)PreferredbyMETEOR. (d)PreferredbyCodeBERTScore.
Figure 9: An example in HumanEval-Java, in which METEOR assigns a higher score to Figure 9(c) which is
notfunctionallyequivalenttothereference(Figure9(b)),whileCodeBERTScoreprefersthefunctionallycorrect
candidateFigure9(d).
NaturalLanguageQuestion: Reference:
/* long specialFilter(
Write a function that takes a vector std::vector<long> nums) {
of numbers as input and returns the int num=0;
number of elements in the vector that for (int i=0;i<nums.size();i++)
are greater than 10 and both first if (nums[i]>10) {
and last digits of a number are odd string w=to_string(nums[i]);
(1, 3, 5, 7, 9). For example: if (w[0]%2==1 and
specialFilter({15, -73, 14, -15}) w[w.length()-1] % 2 == 1)
=> 1 num+=1;
specialFilter({33, -2, -3, 45, 21, 109}) }
=> 2 return num;
*/ }
(a)Thenaturallanguagequestion. (b)Thegroundtruthreference.
Non-equivalentcandidate: Equivalentcandidate:
long specialFilter( long specialFilter(
std::vector<long> nums) { std::vector<long> nums) {
// Write your code here long count = 0;
long sum = 0; for(int i=0; i<nums.size(); i++){
for(int i=0; i<nums.size(); i++){ if(nums[i] >= 10) {
std::string str = int last = nums[i] % 10;
std::to_string(nums[i]); int first = nums[i];
long firstDigit = nums[i] / while(first >= 10) {
abs(nums[i]); first /= 10;
long lastDigit = nums[i]%10; }
if(nums[i] > 10 && if (last % 2 != 0 &&
(firstDigit % 2 == 1 || first % 2 != 0) {
lastDigit % 2 == 1)){ count++;
sum++; }
} }
} }
return sum; return count;
} }
(c)PreferredbychrF. (d)PreferredbyCodeBERTScore.
Figure 10: An example in HumanEval-C++, in which chrF assigns a higher score to Figure 10(c) which is not
functionallyequivalenttothereference(Figure10(b)),whileCodeBERTScoreassignsahigherscoretothefunc-
tionallycorrectcandidateFigure10(d).

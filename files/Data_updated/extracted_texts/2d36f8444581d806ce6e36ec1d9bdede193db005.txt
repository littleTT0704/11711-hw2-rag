Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)
Visual Memory QA: Your Personal Photo and Video Search Agent
LuJiang,1 LiangLiangCao,2 YannisKalantidis,2 SachinFarfade,2 AlexanderG.Hauptmann,1
1CarnegieMellonUniversity,UnitedStates
2YahooResearch,UnitedStates
{lujiang,alex}@cs.cmu.edu,liangliang.cao@gmail.com,
ykalant@image.ntua.gr,fsachin@yahoo-inc.com
Abstract
Theboomofmobiledevicesandcloudserviceshasled
toanexplosionofpersonalphotoandvideodata.How-
ever, due to the missing user-generated metadata such
astitlesordescriptions,itusuallytakesauseralotof
swipestofindsomevideoonthecellphone.Tosolve
the problem, we present an innovative idea called Vi-
sualMemoryQAwhichallowausernotonlytosearch
butalsotoaskquestionsaboutherdailylifecapturedin
thepersonalvideos.Theproposedsystemautomatically
analyzes the content of personal videos without user-
generated metadata, and offers a conversational inter-
facetoacceptandanswerquestions.Tothebestofour Figure1:ComparisonofVisualQA&VisualMemoryQA.
knowledge, it is the first to answer personal questions
discovered in personal photos or videos. The example
questionsare“whatwasthelattimewewenthikingin
a question in VQA. However, it is considerably more diffi-
theforestnearSanFrancisco?”;“didwehavepizzalast
cultforthesameadulttoanswerquestionsinVMQA.This
week?”;“withwhomdidIhavedinnerinAAAI2015?”.
isparticularlydifficulttoanswerquestionsoveracollection
videos.Second,thequestionspaceinVMQAisasubsetof
Introduction
thatinVQA,whichonlyincludesthequestionsausermight
Theprevailingofmobiledevicesandcloudserviceshasled ask later to recall his or her memories. Because of the two
to an unprecedented growth of personal photo and video differences,VisualMemoryQAisexpectedtobemoreuse-
data. A recent study shows that the queries over personal fulinpractice.
photosorvideosareusuallytask-orquestion-driven(Jiang Toaddressthisnovelproblem,thispaperintroducesapro-
et al. 2017). For question-driven queries, users seem to be totype system that can automatically analyzes the content
using photos or videos as a mean to recover pieces from ofpersonalvideos/photoswithoutuser-generatedmetadata,
their own memories, i.e. looking for a specific name, place andoffersaconversationalinterfacetoanswerquestionsdis-
or date. For example, a user might ask “what was the last coveredfromtheuser’spersonalvideos/photos.Technically,
time we went hiking?”; “did we have pizza last week?” or itcanberegardedasanend-to-endneuralnetwork,consist-
“withwhomdidIhavedinnerinAAAI2015?”. ingofthreemajorcomponents:arecurrentneuralnetworkto
Wedefinetheproblemofseekinganswersabouttheuser’s understand the user question, a content-based video engine
dailylifediscoveredinhisorherpersonalphotoandvideo toanalyzeandfindrelevantvideos,andamulti-channelat-
collectionasVMQA(VisualMemoryQuestionAnswering). tentionneuralnetworktoextracttheanswer.Tothebestof
As about 80% of personal photos and videos do not have our knowledge, the proposed system is the first to answer
metadatasuchastagsortitles(Jiangetal.2017),thisfunc- personalquestionsdiscoveredinpersonalphotosorvideos.
tionalitycanbeveryusefulinhelpingusersfindinformation
intheirpersonalphotosandvideos.VisualMemoryQAisa VisualMemoryQuestionAnswering
novelproblemandhastwokeydifferencesfromVQA(Vi-
As shown in Fig. 2, the proposed model is inspired by the
sualQA)(Antoletal.2015):firsttheuserisabletoaskques-
classicaltextQAmodel(Ferruccietal.2010),consistingof
tions over a collection of photos or videos in Visual Mem-
three major components: a recurrent neural network to un-
ory QA as opposed to a single image in VQA. As shown
derstandtheuserquestion,acontent-basedvideoengineto
in Fig. 1, given an image it is trivial for an adult to answer
findtherelevantvideos,andamulti-channelattentionfeed-
Copyright(cid:2)c 2017,AssociationfortheAdvancementofArtificial forwardneuralnetworktoextracttheanswer.Eachcompo-
Intelligence(www.aaai.org).Allrightsreserved. nent is pre-trained on its own task, and then the first and
5093
switchestolocalizingtheanswerinthemultipleinputchan-
Table1:Questionandanswertypesintheproposedsystem.
nels.Forexample,theattentionshouldbeontimestampfor
Question
AnswerType Example “when” questions, and on food concepts for “what did we
Type
eat”questions.Thisisnowachievedbyamulti-channelat-
which photo,video showmethephotoofmydog?
tentionfeed-forwardneuralnetwork.Forthecurrentproto-
date, year, sea- What was the last time we went
when type, a few manual templates are also employed to further
son,hour,etc. hiking?
improvetheaccuracy.
scene,gps,city, Wherewasmybrother’sgradua-
where
country,etc. tionceremonyin2013?
Demonstration
action, object, What did we play during this
what
activity,etc. springbreak? The demonstration will be organized in two phases: a) a
With whom did I have dinner in briefintroduction,andb)ahands-onphase.Ina),themain
who name,face,etc.
AAAI2015? featuresoftheVisualMemoryQAsystemwillbeexplained
how number HowmanytimeshaveIhadsushi and some example queries will be demonstrated. In b), the
many lastmonth?
publicisinvitedtointeractdirectlywiththesystemandtest
yes/no yes,no DidIdoyogayesterday?
itscapabilitiesoveralaptoporonacellphone.Specifically.
allofthepersonalvideosinYFCCdataset(about0.8M)will
be employed as a giant collection of a single anonymous
thethirdcomponentsarefine-tunedonourannotatedbench-
user, and the public can ask questions and examine results
markdatabyBackPropagation.
inlessthan2seconds.Thedemowillberunningonalaptop
andwewillbringcellphonesandotherlaptopstoshowthe
hidden state results.Noadditionaldevicesareneededforthisdemo.
Qu le as s ht ti i o ktn i im n: g ew ih nwa e St Fww ?ea ns tt he (cid:15)(cid:3) (cid:16)(cid:9) (cid:17)(cid:11)(cid:2) (cid:18)(cid:4)(cid:3) (cid:12) (cid:10)(cid:4) (cid:9)(cid:5)(cid:5) (cid:6) (cid:4)(cid:13)(cid:6) (cid:6)(cid:7) (cid:9) (cid:19)(cid:8) (cid:11)(cid:9) (cid:8)(cid:7)(cid:10) (cid:9) (cid:12)(cid:20)(cid:14) (cid:5) (cid:13)(cid:6)(cid:18) (cid:6)(cid:4)(cid:3) (cid:9)(cid:9)(cid:21) (cid:4)(cid:6) (cid:6) (cid:6)(cid:7) (cid:7)(cid:22) (cid:8) (cid:19)(cid:23) (cid:9)(cid:24) (cid:8)(cid:10)(cid:13) (cid:9) (cid:12)(cid:9) (cid:20)(cid:4) (cid:5)(cid:9) (cid:3)(cid:4) (cid:12)(cid:13)(cid:21)(cid:10) (cid:21)(cid:10) M 2a 0y 1 616 ConclusionsandFutureWork
video content engine relevant answer This demo paper presents a novel and promising Visual
photos/videos
MemoryQAsystem,anintelligentagentorchatbotthatcan
Figure 2: Framework of the proposed Visual Memory QA answerquestionsaboutusers’dailylivesdiscoveredintheir
system. personalphotosandvideos.Wehavedevelopedaprototype
system that can efficiently answer questions over 1 million
personalvideos.Wearestillworkingonobtainingmorean-
Intherecurrentneuralnetwork,thetaskistounderstand
notateddatatoqualitativelyevaluatetheaccuracyofonthe
thequestionandclassifyitintoapredefinedanswertype.We
end-to-end task. In the future, we plan to release a bench-
predefineasetofquestionandanswertypesbasedontheir
markonthisnovelandinterestingproblem.
frequencies in Flickr visual search logs (Jiang et al. 2017).
SeeTable1.Atwo-layerLSTMneuralnetworkisincorpo-
Acknowledgments
ratedastheclassifierwheretheembeddingofeachwordin
thequestionissequentiallyfedintotheLSTMunits.Asthe ThisworkwaspartiallysupportedbyYahooInMindProject
answertypesaremutuallyexclusive,asoftmaxlogisticloss andtheIARPAviaDepartmentofInteriorNationalBusiness
isemployedtotrainthenetwork.Besides,thisquestionun- CentercontractnumberD11PC20068.
derstanding component is also responsible for parsing the
question to extract the named entity (person, organization, References
placeandtime).
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.;
The second component is a content video/photo engine
Lawrence Zitnick, C.; and Parikh, D. 2015. Vqa: Visual
thatcanautomaticallyunderstandandindexpersonalvideos
questionanswering. InICCV.
purelybasedonthevideocontent.Ittakesanaturallanguage
Ferrucci, D.; Brown, E.; Chu-Carroll, J.; Fan, J.; Gondek,
sentenceastheinput,andoutputsalistofsemanticallyrel-
D.; Kalyanpur, A. A.; Lally, A.; Murdock, J. W.; Nyberg,
evantvideos,i.e.text-to-video(Jiangetal.2015a).Thetop
E.;Prager,J.;etal. 2010. Buildingwatson:Anoverviewof
rankedrelevantvideosarefedintothethirdcomponent.We
thedeepqaproject. AImagazine31(3):59–79.
employ a state-of-the-art engine called E-Lamp Lite which
not only provides accurate video search and understanding Jiang, L.; Yu, S.-I.; Meng, D.; Mitamura, T.; and Haupt-
but also can scale up to 100 million videos (Jiang et al. mann, A. G. 2015a. Bridging the ultimate semantic gap:
2015b). Asemanticsearchengineforinternetvideos. InICMR.
The last component is a neural network to extract the Jiang, L.; Yu, S.-I.; Meng, D.; Yang, Y.; Mitamura, T.; and
answer. It receives, from the question understanding net- Hauptmann,A.G. 2015b. Fastandaccuratecontent-based
work,ahiddenstatethatembedstheinformationaboutthe semanticsearchin100minternetvideos. InMM.
predicted answer type, and the top ranked relevant videos Jiang,L.;Cao,L.;Kalantidis,Y.;Farfade,S.;Tang,J.;and
fromthevideocontentengine.Eachrelevantvideoisasso- Hauptmann,A.G. 2017. Delvingdeepintopersonalphoto
ciatedwithinformationorganizedintochannels,suchasthe andvideosearch. InWSDM.
timestamp,theactionconcepts,sceneconcepts,objectcon-
ceptsand,insomecases,theGPScoordinates.Thetasknow
5094

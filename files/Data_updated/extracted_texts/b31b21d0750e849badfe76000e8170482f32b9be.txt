PublishedasaconferencepaperatICLR2023
DocPrompting: GENERATING CODE BY RETRIEVING
THE DOCS
ShuyanZhou‚Ä†,UriAlon‚Ä†
FrankF.Xu‚Ä†,ZhiruoWang‚Ä†,ZhengbaoJiang‚Ä†,GrahamNeubig‚Ä†‚Ä°
‚Ä†LanguageTechnologiesInstitute,CarnegieMellonUniversity,
‚Ä°InspiredCognition
{shuyanzh,ualon,fangzhex,zhiruow,zhengbaj,gneubig}@cs.cmu.edu
ABSTRACT
Publiclyavailablesource-codelibrariesarecontinuouslygrowingandchanging.
This makes it impossible for models of code to keep current with all available
APIsbysimplytrainingthesemodelsonexistingcoderepositories. Thus,exist-
ingmodelsinherentlycannotgeneralizetousingunseenfunctionsandlibraries,
becausethesewouldneverappearintheirtrainingdata. Incontrast,whenhuman
programmersusefunctionsandlibrariesforthefirsttime,theyfrequentlyrefer
to textual resources such as code manuals and documentation, to explore and
understandtheavailablefunctionality. Inspiredbythisobservation,weintroduce
DocPrompting: anatural-language-to-codegenerationapproachthatexplicitly
leveragescodedocumentationby(1)retrievingtherelevantdocumentationpieces
given a natural language (NL) intent, and (2) generating code based on the NL
intentandtheretrieveddocumentation. DocPromptingisgeneral: itcanbeap-
pliedtoanyprogramminglanguage,andisagnostictotheunderlyingneuralmodel.
WedemonstratethatDocPromptingconsistentlyimprovesNL-to-codemodels:
DocPromptingimprovesstrongbasemodelssuchasCodeT5by2.85%inpass@1
(52%relativegain)and4.39%inpass@10(30%relativegain)inexecution-based
evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset
tldr,DocPromptingimprovesCodeT5andGPT-Neo-1.3Bbyuptoabsolute
6.9%exactmatch. 1
1 INTRODUCTION
Weaddressthetaskofnaturallanguagetocodegeneration(NL‚Üícode): generatingacodesnippet,
writteninageneral-purposeprogramminglanguagesuchasPythonorBash,givenanaturallanguage
intent. Thistaskhasseensharplygrowingpopularityrecentlyduetotheemergenceoflargelanguage
modelstrainedonvastamountsofnaturallanguageandcode(Chenetal.,2021;Xuetal.,2022;
Friedetal.,2022). NL‚Üícodemodelsfacilitateprogrammingforbothprofessionalandinexperienced
programmers,byallowingprogrammerstowritecodebyonlyexpressingtheirhigher-levelintent.
Many existing code generation models either learn directly from input-output pairs provided as
trainingdata(Allamanisetal.,2015;YinandNeubig,2017;Iyeretal.,2018;Brockschmidtetal.,
2019;Xuetal.,2020;Alonetal.,2020;Wangetal.,2021), orlearnthemappingbetweeninput
and output implicitly from naturally occurring corpora of intertwined natural language and code
(Austinetal.,2021;Nijkampetal.,2022). Nevertheless,alltheseworksassumethatalllibraries
andfunctioncallswereseeninthetrainingdata;andthatattesttime,thetrainedmodelwillneedto
generateonlyseenlibrariesandfunctioncalls. However,newfunctionsandlibrariesareintroduced
allthetime,andevenaseenfunctioncallcanhaveunseenarguments. Thus,theseexistingmodels
inherentlycannotgeneralizetogeneratesuchunseenusages.
In contrast to these existing models, human programmers frequently refer to manuals and docu-
mentationwhenwritingcode(Nykazaetal.,2002;Lethbridgeetal.,2003). Thisallowshumans
toeasilyusefunctionsandlibrariestheyhaveneverseennorusedbefore. Inspiredbythisability,
1Dataandcodeareavailableathttps://github.com/shuyanzhou/docprompting.
1
3202
beF
81
]LC.sc[
3v78950.7022:viXra
PublishedasaconferencepaperatICLR2023
c
n from pygments import *
Generate HTML with python
code = ‚Äòprint(‚Äúreading docs‚Äù)‚Äô
syntax highlighting for Re!iever Genera"r s = highlight(code, PythonLexer(),
‚Äúprint(‚Äòreading docs‚Äô)‚Äù
HtmlFormatter())
Pygment is a generic syntax highlighter A formatter takes the token stream and writes it d 3
d to an output file ‚Ä¶
1
A lexer splits the source into tokens, fragments ‚Ä¶
class HtmlFormatter
Format tokens as HTML 4 <span> tags with ‚Ä¶
class PythonLexer
For Python source code d
2
ùíü
Figure1: DocPrompting: givenanNLintent(cid:13)n,theretrieverretrievesasetofrelevantdocumenta-
tion{(cid:13)d1,(cid:13)d2,(cid:13)d3}fromadocumentationpool(cid:13)D. Then,thegeneratorgeneratesthecode(cid:13)c based
ontheNLandretrieveddocs. DocPromptingallowsthemodeltogeneralizetopreviouslyunseen
usagesbyreadingthosedocs. ItalicbluehighlightsthesharedtokensbetweenNLanddocs;Bold
showssharedtokensbetweendocsandthecodesnippet.
weproposeDocPrompting: acodegenerationapproachthatlearnstoretrievecodedocumentation
beforegeneratingthecode. AnoverviewofourapproachisillustratedinFigure1: First,adocument
retrieverusestheNLintent(cid:13)n toretrieverelevantcodedocumentation{(cid:13)d1,(cid:13)d2,(cid:13)d3}fromadocumen-
tationpool(cid:13)D. Then,acodegeneratorusesthesedocsinitsprompttogeneratethecorresponding
code(cid:13)c. Thedocumentationpoolservesasanexternaldatastorethatcanbeupdatedfrequently
withnewcontents(e.g.,documentationofnewlyreleasedlibraries),withoutre-traininganymodel
component. Thisway,DocPromptingcanleveragenewlyaddeddocumentation,anditcangenerate
codecontainingunseenandunusedfunctionsandlibraries. DocPromptingisgeneralandapplicable
toanyprogramminglanguageandunderlyingbasearchitecture. Tothebestofourknowledge,thisis
thefirstdemonstrationofleveragingdocumentationinmodelsofcodeexplicitlyandeffectively.
WedemonstratetheeffectivenessofDocPromptingontwoNL‚Üícodebenchmarksandtasks,across
twoprogramminglanguages,andusingseveralbasemodels:GPT-Neo(Blacketal.,2021),T5(Raffel
etal.,2020),CodeT5(Wangetal.,2021),Fusion-in-Decoder(IzacardandGrave,2021)),andCodex
(Chenetal.,2021). Further,weexperimentwithbothsparseretrieverssuchasBM25(Robertsonand
Jones,1976)anddenseretrievalmodelssuchasSimCSE(Gaoetal.,2021). Finally,weintroduce
twonewbenchmarksforretrieval-basedcodegeneration: (a)inBash,wecurateanewbenchmark
by crawling the tldr repository, and constructing the training/development/test splits without
overlappingcommands;(b)inPython,were-splitthepopularCoNaLabenchmark(Yinetal.,2018)
bymakingeverytestexamplecontainatleastonePythonfunctionthatisnotseeninthetrainingdata.
ModelsthatuseDocPromptingconsistentlyoutperformtheirbasemodelsthatgeneratecodesolely
basedontheNLintents. UsingDocPromptingimprovesstrongbasemodelssuchasCodeT5by
2.85%inpass@1(52%relativegain)and4.39%inpass@10(30%relativegain)inexecution-based
evaluationinCoNaLa;onthenewtldrdataset,DocPromptingimprovesCodeT5andGPT-Neo-
1.3Bbyuptoabsolute6.9%exactmatch. Wereleaseournewbenchmarks,includingannotation
oforacledocumentsforeachexampleandpoolsofdocumentation,toserveasatest-bedforfuture
retrieval-basedcodegenerationmodels.
2 CODE GENERATION BY READING THE DOCS
Ourunderlyingassumptionisthatcodedocumentationisthemostexhaustiveyetsuccinctresource
formostlibrariesandprogramminglanguages(Roehmetal.,2012),andthatdocumentationallows
toeffectivelygeneralizetounseenlibrariesandfunctions(ForwardandLethbridge,2002). Wefollow
theretrieve-then-generateparadigm(Lewisetal.,2020;Guuetal.,2020),focusingonretrieving
documentation. Inthissection,wedescribethegeneralapproachofDocPrompting;in¬ß3and¬ß6.2,
weelaborateandexperimentwithpracticalimplementationsofDocPrompting.
FormulationGivenNLintentn,ourgoalistogenerateacorrespondingcodesnippetcwrittenin
someprogramminglanguage(PL)suchasPython. Weassumethatamodelhasaccesstoacollection
ofcodedocumentationD. Eachdocumentd ‚ààDdescribestheusageofalibrary,afunction,oran
i
2
PublishedasaconferencepaperatICLR2023
argumentinthatPL.TheconstructionofDisflexible: itcaneitherbeacomprehensivesetofall
availablelibrariesandfunctionsinaPL,oracustomizedsubsetforthescopeofaspecificproject.
2.1 BACKGROUND: RETRIEVAL-CONDITIONEDGENERATION
AlthoughamodelmayusetheentirecollectionofdocumentsD,onlyafewdocumentsinD are
relevantforanyparticularintent. Further,itisusuallycomputationallyinfeasibletodirectlycondition
ontheentire,unbounded,collectionofdocumentswhilemakingpredictions. Thus,wefirstletthe
modelselectasubsetofdocumentsD ={d ,d ,..,d }‚äÜDthatarepotentiallyrelevantgivenn,
n 1 2 k
andrefertothissubsetwhilegeneratingc.
Overall,wedecomposetheprobabilityofgeneratingcintotheprobabilityofchoosingaparticular
subsetofdocumentsP(D ‚à£D,n),andtheprobabilityofgeneratingthecodeconditionedonthe
n
intentandtheselecteddocumentsP(c‚à£D ,n);finally,wemarginalizingoverallD ‚äÜD:
n n
P(c‚à£D,n)=‚àë P(c‚à£D ,n)‚ãÖP(D ‚à£D,n) (1)
Dn‚äÜD n n
assumingthatcisindependentofDgivenD (thatis,(c√ÜD‚à£D )). Sinceenumeratingallpossible
n n
subsets D is computationally infeasible, we follow the common practice and approximate the
n
marginalizationoverD inEquation(1)bytakingthemostprobablesubsetofretrieveddocuments
n
DÀÜ ,andthenconditioningthepredictionofconthesemostlikelydocuments:
n
DÀÜ ‚à∂=argmax P(D ‚à£D,n) P(c‚à£D,n)‚âàP(c‚à£DÀÜ ,n)‚ãÖP(DÀÜ ‚à£D,n) (2)
n Dn‚äÜD n n n
2.2 DocPrompting: GENERATINGCODEBYRETRIEVINGTHEDOCS
Equation 2 implies that DocPrompting relies of two main components: A retriever R retrieves
relevantdocumentsDÀÜ giventheintentn;andageneratorGgeneratesthecodesnippetcconditioned
n
on the retrieved documents DÀÜ and the intent n, which compose a new prompt. Specifically, R
n
computesasimilarityscores(d ,n)betweenaintentnandeverydocumentd ‚ààD. Thus,thesubset
i i
DÀÜ ‚äÜDisthetop-kdocumentswiththehighestsimilarityscores: DÀÜ =top-k (s(d ,n)).
n n di‚ààD i
AnoverviewofourapproachisillustratedinFigure1: giventheintentGenerateHTMLwithpython
syntaxhighlightingfor‚Äúprint(‚Äôreadingdocs‚Äô)‚Äù,theretrieverRretrievesthreerelevantdocuments:
d describesthesyntaxhighlightinglibrarypygments,d describestheclassPythonLexer,and
1 2
d describestheHtmlFormatterclass. Giventhesedocsandtheintent,thegeneratorG generates
3
thecodesnippetc,whichusesPythonLexerandHtmlFormatterfromthepygmentlibrary.
3 PRACTICAL INSTANTIATIONS OF DocPrompting
DocPromptingisageneralapproachthatisnotboundtoanyspecificmodelchoices,anditcanbe
instantiatedwithanybaseretrieverandgenerator. Thissectionpresentstheconcreteinstantiationsof
RandG thatwefoundtoprovidethebestperformanceinourexperiments.
3.1 RETRIEVERINSTANTIATION
Weexperimentwithtwomaintypesofretrievers:sparseretrieversanddenseretrievers. Asoursparse
retriever,weuseElasticsearch2withthestandardBM25(RobertsonandJones,1976). Thisretriever
representsdocumentsusingsparsefeaturesthatrelyonwordfrequencies,suchasBM25andTF-IDF.
Asourdenseretriever,wefollowpriorwork(Chenetal.,2020;Karpukhinetal.,2020;Gaoetal.,
2021): givenatriplet(n,c,D‚àó),whereD‚àó aretheoracledocsforn,eachd+ ‚ààD‚àó andnforma
n n i n
positivepair(n,d+),whileeachd‚àí‚àâD‚àó andnformanegativepair(n ,d‚àí). Wetraintheretriever
i j n i j
in a contrastive fashion where the similarity score of a positive pair is maximized while that of
in-batchnegativepairsisminimized. Forapair(n ,d+),thelossfunctionisdefinedas:
i i
Lr=‚àílog
exp(sim(h n,h d+ i))
(3)
exp(sim(h n,h d+ i))+‚àë d‚àí j‚ààB/Dn‚àó exp(sim(h n,h d‚àí j))
2https://github.com/elastic/elasticsearch
3
PublishedasaconferencepaperatICLR2023
whereh istherepresentationofxcomputedbyaneuralencoder,andBarepositivedocsforother
x
examplesinthebatch. Wedefinesim(h ,h )asthecosinesimilaritybetweenh andh .
x y x y
Weuseall(n ,d+)inthetrainingsetasoursupervisedtrainingdataset. Additionally,weuseall
i i
sentencesinthedocumentationpoolforweaksupervision:FollowingChenetal.(2020)andGaoetal.
(2021),representationsofthesamesentencewithdifferentdropoutmasksaretreatedasapositive
example. InsteadofusingeithersupervisedorweaklysupervisedtrainingasinGaoetal.(2021),we
simplymixthetworesultingsupervisionsignals,andexamplesarerandomlydistributedintobatches.
Thismixtureoftasksnotonlyfacilitatesthelearningprocess(¬ß6.2),butalsoreducestheengineering
effortrequiredtostoreandreloadmodelsforseparatesupervisedandunsupervisedtrainingphases.
WeinitializetheretrieverencoderwitheitherthebestmodelofGaoetal.(2021)ortheencoderof
CodeT5-base(Wangetal.,2021). AdditionaltrainingdetailsareprovidedinAppendixC
3.2 GENERATORINSTANTIATION
Weexperimentedwithavarietyofgeneratormodels.WeusedGPT-Neo-125M,GPT-Neo-1.3B(Black
etal.,2021)andCodex(Chenetal.,2021),whereweconcatenatetheretrieveddocumentsandthe
NLintentasasingle, long, prompt. T5-base(Raffeletal.,2019)andCodeT5-base(Wangetal.,
2021)haveashorterinputsizeof512tokens,whichissometimestooshortfortheconcatenationof
multipledocs. Thus,forT5andCodeT5weapplythefusion-in-decoderapproach(FiD; Izacardand
Grave,2021): wefirstconcatenatetheintentnwitheachretrievedd ‚ààDÀÜ andencodeeach(n,d )
i n i
pairindependently. Then,thedecoderattendstoallencodedNL-documentpairs. Wefinetunethe
generatortomaximizethelog-likelihoodofthereferencecodecgivennandDÀÜ .
n
WithCodex(Chenetal.,2021),weperformedfew-shotlearningratherthanfinetuningbecausethe
modelparametersarenotpubliclyavailable. Weconstructedthepromptwiththreestaticexamples,
eachofwhichisaconcatenationofretrieveddocumentation,anNLintentandthereferencecode
snippet. Wethenappendedthetestexampleanditsretrieveddocumentationtothefew-shotexamples.
Weusedthecode-davinci-001versionbecausewesuspectpotentialleakageofthetestsetintothe
trainingsetofcode-davinci-002. SeemoredetailsinAppendixH.Trainingdetails,hyper-parameter
settingsandexamplepromptscanbefoundinAppendicesEandD.
4 EXPERIMENTAL SETUP
WeevaluateDocPromptingontwoNL‚Üícodetasks: shellscripting(¬ß4.1),inwhichwegenerate
complex shell commands given an intent, and Python programming (¬ß4.2), where we generate
answersinPythonforNLquestions. Inthissection,wefirstintroduceanewlycuratedbenchmark
tldr;wethendescribeourre-splitofthepopularCoNaLabenchmark(Yinetal.,2018). Foreach
benchmark,weprovideaglobaldocumentationpoolD thatissharedforallexamplesandoracle
documentsD‚àó whichweusetotraintheretriever. Wereleaseournewlycuratedbenchmarkstoserve
n
astest-bedforfutureretrieval-basedcodegenerationmodels.
4.1 SHELLSCRIPTING
tldr is a community-driven project that maintains easily-
readable help pages with examples for over 2.5k Bash com-
mands in over 25 natural languages3. We collected pairs of
EnglishintentsandBashcommandlines. TheNLintentsare
writtenbyhumanusers,andtheBashcommandsrangefrom
popularoneslikecatandtar,touncommoncommandssuch
astoiletandfaketime. Ourresultingtldrbenchmark
contains1,879uniqueBashcommandsand9,187NL‚ÜíBash
pairs. Weconstructedthetraining,developmentandthetestset
withcompletelydisjointcommandstotestthegeneralizability
ofacodegenerationmodel. Theshareddocumentationpool
D is made up of the 400k paragraphs from the 1,879 Bash Figure2:AnexampleNL-codepair
manuals. Eachparagraphdescribesasingleconceptsuchasan
fromtldr,alongwiththreeoracle
documentationitems.
3https://github.com/tldr-pages/tldr
4
PublishedasaconferencepaperatICLR2023
argumentflag. WefurthercuratedtheoracledocumentsD‚àó foreachexampleusingsimplestring
n
matching. AnexamplefromtldrisshowninFigure2. Tothebestofourknowledge,thisisthe
firstworktoleveragetldrasanNL‚Üícodebenchmark. Detailedstatisticsandadditionaldetails
are provided in Appendix A. In tldr, each NL intent results in a single Bash command with a
combinationofargumentflags. WethereforefirstretrieveanentireBashmanual;then,wetakethe
topmanualandretrievethetop-10paragraphsfromthatmanual.
EvaluationmetricsWemeasure: (a)commandnameaccuracy(CMDAcc)‚Äìwhetherthecommand
name(e.g.,cat)isanexactmatch;(b)exactmatch(EM)‚Äìexactmatchbetweenthereferenceand
thegeneration;(c)token-levelF1;and(d)character-levelBLEU(charBLEU; Linetal.,2018;Shi
et al., 2022). In all metrics, we disregard user-specific variable names in the references and the
modelsoutputs. Forexample,‚Äúmycli -u [user] -h [host] [database]‚Äùisevaluated
as‚Äúmycli -u $1 -h $2 $3‚Äù.
4.2 PYTHONPROGRAMMING
CoNaLa (Yin et al., 2018) is a popular benchmark for NL‚ÜíPython generation. NL intents are
StackOverflowquestions,andcodesnippetsaretheiranswers. Bothintentsandcodesnippetsare
rewrittenbyhumanannotators.Were-splitthedatasettotestmodels‚ÄôgeneralizationtounseenPython
functions. Inourre-split,weverifedthateveryexampleinthedevelopmentorthetestsetusesatleast
onePythonfunction(e.g.,plt.plot)thatwasnotseeninthetrainingdata. Inaddition,wemake
surethattheexamplesfromthesameStackOverflowpostsareinthesamesettopreventleakage.
Thisre-splitresultsin2,135/201/543examplesinthetraining/development/testsets,respectively.
TheCoNaLadocumentationpoolDcontains35,763documents,eachdescribingasinglefunction,
fromallPythonlibrariesavailableonDevDocs(https://devdocs.io). Theseincludebuilt-in
librariesandotherpopularlibrariessuchasnumpy. WeconstructedtheoracledocsD‚àó foreach
n
examplebymatchingallfunctionnamesinthetargetcodecwithdocs. MoredetailsinAppendixB.
EvaluationmetricsWefollowYinetal.(2018)andmeasureBLEU-4. Sincewefocusongeneral-
izationtounseenfunctions,weadditionallyreportfunctionnamerecall(recall)andunseenfunction
recall(recall ),whichmeasuresrecallamongfunctioncallsthatdonotappearinthetraining
unseen
set. Finally,followingChenetal.(2021);Austinetal.(2021),weusedthemanuallywrittenunit
testsfromWangetal.(2022)for100examplesfromCoNaLa‚Äôstestsetandmeasurepass@k. We
followed Chenetal.(2021)andperformednucleussampling(Holtzmanetal.,2019)withp=0.95.
Foreachk,wesearchedforthebesttemperatureforeachmodelfrom{0.2,0.4,0.6,0.8,1.0}. On
average,eachexamplehas2.03tests. TheconcatenationofmultiplePythondocsoftenexceededthe
lengthlimitofGPT-Neo,wehenceexperimentedinthisdatasetwithFiD,whichallowslongerinputs.
AdditionaldetailsareprovidedinAppendixB.
5 RESULTS
Inallfollowingresults,allmodelswithDocPromptingusethetop-10retrieveddocsfromthebest
retrieveronthatdataset(Table4). Everybaselineusestheexactsamesetupasits‚Äú+DocPrompting‚Äù
version,exceptfornotusingthedocumentation.
5.1 SHELLSCRIPTINGRESULTS
ResultsfortldrareshowninTable1. DocPromptingconsistentlyimprovesthebasemodels. For
example,T5+DocPromptingachievesmorethantwicehigheraccuracyinpredictingthecommand
name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact
matchgain,comparedtothevanillaT5. Inthefew-shotlearningsettingwithCodex,DocPrompting
bringsgainsof6.7charBLEUpoints,andconsistentimprovementacrossallmetricsoverthebaseline
thatobservesonlyNL-codepairsinitsprompt. Theseresultsshowthatretrievingdocumentation
alsobenefitsstrongmodelssuchasCodex,andwithonlyfewexamplesinthecontext.
Code generation with oracle command names In realistic settings, a human programmer may
knowthecommandnametheyneedtouse(e.g.,awk),butnotknowtheexactusageandflags. In
fact,betterunderstandingoftheusageofknowncommandsisthepurposeofUnixmanpagesandthe
5
PublishedasaconferencepaperatICLR2023
Table1: Resultsonshellscripting,usingaBM25retrieverwithtop-10retrieveddocs,onthetestset
oftldr. Forthe‚Äúoraclecommandname‚Äùexperiments,weselectedthebestmodelofeachtype.
Model CMDAcc(%) EM(%) TokenF1 charBLEU
- 11.96 1.94 28.75 19.99
GPT-Neo-125M
+DocPrompting 25.32 3.56 31.23 24.43
- 14.55 3.12 32.46 24.70
GPT-Neo-1.3B
+DocPrompting 27.59 9.05 37.24 30.57
- 10.02 0.76 19.90 25.48
T5
+DocPrompting 30.28 9.16 37.58 31.97
- 14.60 2.18 30.00 21.50
CodeT5
+DocPrompting 30.72 9.15 36.71 33.83
- 27.48 8.94 36.04 16.94
Codex3-shots
+DocPrompting 31.21 9.29 36.77 23.72
Withtheoraclecommandname
- - 12.96 59.36 45.05
T5
+DocPrompting - 22.55 64.84 54.28
- - 22.44 62.26 50.29
Codex3-shots
+DocPrompting - 32.43 69.73 55.21
Table2: Comparisontoapproachesthatretrieveexamples(Parvezetal.,2021;Pasupatetal.,2021)
.
Model CMDAcc(%) EM(%) TokenF1 charBLEU
+ExPrompting 6.68 0.32 20.49 11.15
GPT-Neo-125M
+DocPrompting 25.32 3.56 31.23 24.43
+ExPrompting 14.01 2.8 30.07 22.11
GPT-Neo-1.3B
+DocPrompting 27.59 9.05 37.24 30.57
tldrproject. WeconductedanoracleexperimentwhereweprovidedT5(whichwasthestrongest
modelusingDocPrompting)andCodexwiththeoraclecommandname(e.g.,awk). Thisoracle
informationisprovidedtoboththebaselineandthemodelthatusesDocPrompting. Theresultsare
shownonthebottompartofTable1. Whentheoraclecommandisgiven,DocPromptingfurther
improvesoverthebasemodels. Forexample,whenprovidingCodexwiththegroundtruthcommand
name,DocPromptingimprovesitsexactmatchfrom22.44%to32.43%.
Should we retrieve documentation or examples? All existing retrieval-based models of code
retrieveNL-codepairsorcodesnippets,ratherthandocumentation. Tosimulatethisscenario,we
followed Parvezetal.(2021)and Pasupatetal.(2021)toretrieveNL-codepairsfromthetraining
setoftldr,andrefertothisbaselineasExPrompting. WefinetunedthebestretrieverRoBERTa
andtwogenerators,andretrievedthetop-30NL-codepairsforeveryexample. AsshowninTable2,
retrievingdocumentation(DocPrompting)providesmuchhighergainsthanretrievingexamples
(ExPrompting). Theoretically,addingexamplesofunseencommandscanhelpExPrompting
generalizetothemaswell. However,newlibrariesandfunctionsmaynothaveavailableexampleson
thewebyet,whiledocumentationoftendoesbecomesavailablewhenthelibraryisreleased.
5.2 PYTHONPROGRAMMINGRESULTS
Table3showstheresultsonCoNaLa. CodeT5+DocPromptingyieldsa1.65BLEUimprovement
overthestate-of-the-artbaselinethatwasinitializedwithCodeT5.4 Whenmeasuringtherecallofthe
generatedfunctionnames,thebenefitofDocPromptingisespeciallyhigherforunseenfunctions
(recall ).Forexample,DocPromptingachieves18.30comparedtoonly9.03ofthebaseCodeT5
unseen
inunseenfunctions. Additionally,DocPromptingimprovesin-contextlearningsettingwithCodex.
4InaseparateexperimentontheoriginalsplitofCoNaLa,thisbaselineachievedaBLEUscoreof39.12,
whichoutperformsthepreviousstate-of-the-art(BeauandCrabbe¬¥,2022)by4.92BLEUpoints.
6
PublishedasaconferencepaperatICLR2023
Table3: ResultsonCoNaLa,usingaCodeT5retrieverwithtop-10retrieveddocs. Functionrecall
(Recall) measures how many functions in the reference codeare correctly predicted, and unseen
functionrecall(Recall )onlyconsidersthesubsetheldoutfromthetrainingdata.
unseen
Model BLEU Recall Recall
unseen
- 43.16 39.52 -
Codex3-shots +DocPrompting 43.47 39.87 -
+DocPromptingoracledocs 50.59 57.84 -
- 28.07 14.36 2.57
T5
+DocPrompting 30.04 21.34 8.24
- 34.57 24.24 9.03
CodeT5 +DocPrompting 36.22 27.80 18.30
+DocPromptingoracledocs 49.04 72.20 63.91
tldr CoNaLa
40
35.46
35 31.87 100% 91% NL‚Üê‚ÜíCode
30 27.54 80% (NL+Docs)‚Üê‚ÜíCode
22 05 18.70 23.38 25.54 27.08 60% 52%
15 14.31 40% 24% 30% 28%
10 8.26 +DocPrompting 20% 12% 14% 11% 9%16% 7%11%
5
5.41 CodeT5 0%
0%2%0%0%
0
110 50 100 200 1 2 3 1 2 3 4 5
n-gram n-gram
k
Figure4: Usingdocumentationsignificantlyin-
Figure3: Pass@k ofCodeT5withandwithout
creases the n-gram overlap recall between the
DocPromptingon100CoNaLaexamples.
inputandtheoutput,intldrandCoNaLa.
WehypothesisthattheminorgainismainlyduetothepotentialdataleakageofCodex,whichviolates
the split of seen and unseen functions. Another reason is that a strong generator such as Codex
mayrequireanequallystrongretrieveraswell. WefindthatCodexcanachieveevenhigherresults
withanoracleretriever,whichshowsthepotentialfurtherimprovementbyimprovingtheretrievers.
Finally,CodeT5performsbetterthanT5,withandwithoutusingDocPrompting. Thisemphasizes
theimportanceofusingcode-specificpretrainedmodels.
Execution-basedevaluationTheresultsareshowninFigure3. UsingDocPromptingconsistently
outperformsthe baselineCodeT5 forall values ofpass@k. For example, DocPrompting yields
2.85%improvementonpass@1and4.45%improvementonpass@5,whicharerealisticnumbers
of completions that can be suggested in an IDE. When k = 200, DocPrompting widens the gap
to8.38%. TheseresultsdemonstratethatDocPromptingdoesnotonlyimprovethequalityofthe
generatedcodeinitssurfaceform,butalsoincreaseitsfunctionalcorrectness. Additionaldetailsand
resultsareprovidedinAppendixG.
6 ANALYSIS
6.1 WHYDOESREADINGTHEDOCUMENTATIONHELPGENERATINGMOREACCURATECODE?
We believe that one of the major reasons is that documentation eases the mapping between NL
intentsandcode,sincethedocumentationcontainsbothNLdescriptionsandfunctionsignatures.
We calculated the n-gram overlap between the NL intents and their corresponding code snippets
(NL‚Üê‚Üícode), and the overlap between the NL intents with their top-10 retrieved documents and
theircodesnippets((NL+docs)‚Üê‚Üícode). AsshowninFigure4,addingdocumentationsignificantly
increasestheoverlapacrossn-grams,andincrease,forexample,theunigramoverlapfrom12%to
7
k@ssap llaceR
PublishedasaconferencepaperatICLR2023
Table4:Retrievalperformanceofmultiplemodelsonthedevsetoftldr(top)andCoNaLa(bottom).
RoBERTaisthebestmodeltakenfromfromGaoetal.(2021),andCodeT5istheencoderofCodeT5-
base(Wangetal.,2021). Modelswiththesubscript‚Äúoff-shelf‚Äùaretheoff-the-shelfmodels,andthe
othermodelswerefinetunedwiththeobjectiveinEquation3. Thelastcolumnisthebestmodel
(RoBERTafortldrandCodeT5forCoNaLa)trainedwithouttheweaksupervisioncorpus.
n BM25 RoBERTa RoBERTa CodeT5 CodeT5 Bestw/oweaksup.
off-shelf off-shelf
1 32.81 17.53 30.03 10.45 18.10 28.30
5 51.73 37.89 52.50 20.26 38.52 50.50
tldr
10 59.86 46.80 60.33 25.73 51.03 59.84
20 62.01 56.11 64.30 33.65 57.26 62.30
1 3.01 4.46 13.49 4.60 16.54 10.51
5 7.16 7.58 26.38 8.63 42.35 21.15
CoNaLa
10 9.73 10.93 34.86 12.25 55.81 29.34
20 11.46 13.89 45.46 18.46 66.79 42.21
24%intldr.Thatis,oneofthereasonsthatretrievingdocumentationhelpsgeneratingaccuratecode
isthatdocumentationbridgesthegapbetweenthe‚Äúintentterminology‚Äùandthe‚Äúcodeterminology‚Äù.
6.2 ABLATIONSTUDY
We compared different configurations of the retriever, to gather more insights for effective
DocPrompting. Table4showsacomparisonbetweendifferentretrieversandtheirsetups. First,the
performanceofBM25variesamongdatasets: Intldr,BM25matchestherecalloftraineddense
retrievers;howeverinCoNaLa,BM25achievesonlyrecall@10of9.73%,andstrongdenseretrievers
such as the encoder of CodeT5 achieve recall@10 of 55.81. We hypothesize that this difference
betweendatasetsstemsfromthewaysthesedatasetswerecreated: tldrintentswerewrittenbased
onexistingBashcommandsandmanuals;whileCoNaLaexampleswereminedfromStackOverflow
posts,whereusersaskquestionswithlimitedornocontext. Thus,NLintentsinCoNaLarequire
abettersemanticalignmentwiththedocuments,andthusbenefitfromdenseretrievers. Thegap
resultingfromdifferentdatacurationprocesseswasalsoobservedbyRodriguezandBoyd-Graber
(2021)inopen-domainquestionanswering(QA).
Second,retrieversthatwerepretrainedonthetargetprogramminglanguagearegenerallystronger.
For example in CoNaLa, CodeT5 which was pretrained on Python, is both a better off-the-shelf
retrieverandabetterfinetuned-retrieverthanRoBERTa,whichwaspretrainedmainlyontext. In
contrast,tldrisbasedonBash,whichneitherCodeT5norRoBERTawereexplicitlypretrainedon.
Thus,tldrbenefitsmostlyfromBM25andRoBERTaratherthanCodeT5asretrievers.
Finally, training the retriever using weak supervision on the documentation pool (Section 3.1)
dramatically improves the retriever. The recall of the best retrievers of each dataset without this
corpus is shown in the last column of Table 4 (‚ÄúBest w/o weak sup.‚Äù). On CoNaLa, removing
thiscorpusresultsinsevereperformancedegradation. Onepossibleexplanationisthatthisweak
supervisionhelpstheretrieverperformdomainadaptationmoreeffectively.
6.3 CASESTUDY
We examine the models‚Äô outputs and show two representative examples in Table 5. In the first
example, Image.open was not seen in the training set, and the baseline CodeT5 incorrectly
predictsos.open. Incontrast,usingDocPromptingallowstoretrievethedocsandtocorrectly
predictImage.open.Inthesecondexample,df.to csvwasnotseenintraining,andthebaseline
CodeT5failstocorrectlypredictit.Incontrast,DocPromptingdoespredictmostofthedf.to csv
call correctly, thanks to the retrieved docs. Nevertheless, DocPrompting generates an incorrect
argumentskiprows=1,insteadofheader=False. Thereasonisthatalongwiththeretrieved
documentationofdf.to csv,theretrieveralsoretrievedthedocumentationofdf.read csv,
whichhasaskiprowsargument. Thatis,thegeneratorusesanargumentofdf.read csvwith
thefunctiondf.to csv. Furtherimprovingtheretrieversandthegenerators, andpost-filtering
basedonthevalidityofargumentnames,maymitigatesuchmistakes.
8
PublishedasaconferencepaperatICLR2023
Table 5: Examples of predictions from CoNaLa, of the base CodeT5 compared to
CodeT5+DocPrompting. Unseenfunctionsareunderscored.
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
NLIntent:Openimage‚Äùpicture.jpg‚Äù
Groundtruth: img = Image.open(‚Äôpicture.jpg‚Äô) \n Img.show
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
CodeT5: os.open(‚Äôpicture.jpg‚Äô, ‚Äôr‚Äô)
CodeT5+DocPrompting: image = Image.open(‚Äôpicture.jpg‚Äô, ‚Äôrb‚Äô)
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
NLIntent:Excludecolumnnameswhenwritingdataframe‚Äòdf‚Äôtoacsvfile‚Äòfilename.csv‚Äô
Groundtruth: df.to csv (‚Äôfilename.csv‚Äô, header=False)
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
CodeT5: df.drop([‚Äôcol1‚Äô, ‚Äôcol2‚Äô], axis=1, inplace=True)
CodeT5+DocPrompting: df.to csv(‚Äôfilename.csv‚Äô, skiprows=1)
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
7 RELATED WORK
CodegenerationThemostcommonpracticeinNL‚Üícodegenerationistrainingamodelonadataset
ofNL-codepairs (Allamanisetal.,2015;YinandNeubig,2017;Rabinovichetal.,2017;Iyeretal.,
2018). Nevertheless,alltheseworksassumethattheirtrainingcorpuscoversallrequiredlibrariesand
functions,andtheirmodelsareinherentlyincapableofgeneratinglibrariesandfunctionsthatwerenot
seeninthetrainingdata. Onthecontrary,DocPromptingallowsmodelstogeneratecallstounseen
function,byretrievingthesefunctions‚Äôdocumentationandreadingthemattesttime. Hayatietal.
(2018);Parvezetal.(2021);Hashimotoetal.(2018)andLuetal.(2017)learntoretrieveexamplesat
testtime;Pasupatetal.(2021)alsoconsideredsettingswherethetestdatahasadistributionshiftfrom
thetrainingdata. However,whennewlibrariesarereleasedtheyoftencomewithdocumentation,
andthusweassumethatdocumentationfornewlibrariesismuchmorelikelytobeavailablethan
concretenaturallanguageintentandcodesnippetpairs(n,c)thatusetheselibrariesalready. The
modelsofShrivastavaetal. andWuetal.(2021)retrievecodesnippetsfromrelevantfilesinthe
sameproject;contrarily,whenpredictingnewlibrariesandfunctionsthatareexternaltotheuser‚Äôs
project,documentationisthesourcethatisthemostlikelytobeavailable.
RetrievalaugmentedgenerationTheparadigmofretrieve-then-generatehasgainedpopularityin
thefieldofopen-domainquestionanswering(Guuetal.,2020;Lewisetal.,2020;Karpukhinetal.,
2020),wheretheanswerforanopen-domainquestionexistsinonlyfewdocumentsoutofamuch
larger pool. Although DocPrompting takes a similar approach, documentation retrieval in code
generationisevenmorevaluable,sincecodelibrariesareupdatedconstantly,andnewlibrariesare
introduceddaily. Thus,DocPromptingallowsupdatingthedocumentationpoolfrequentlywithnew
contents,withoutre-traininganymodelcomponents.
Documentation conditioned generation The model of Zhong et al. (2019) reads documents to
understandenvironmentdynamicsinagrid-worldgame,andBranavanetal.(2011)controlssituated
agentsinagame(CivilizationII)byreadingthegame‚Äôsmanual. However,alltheirmodelswere
tailoredtospecificgames;incontrast,DocPromptingisgeneralandisapplicableforavarietyof
programminglanguagesanddatasets.
8 CONCLUSION
WeproposeDocPrompting,asimpleandeffectiveapproachforcodegenerationbyretrievingthe
relevantdocumentation. DocPromptingconsistentlyimprovesNL‚Üícodemodelsintwotasks,in
twoPLs,andacrossmultiplestrongbasemodels. DocPromptingimprovesstrongbasemodelssuch
asCodeT5by2.85%inpass@1(52%relativegain)inexecution-basedevaluationonthepopular
PythonCoNaLabenchmark;onanewBashdatasettldr,DocPromptingimprovesCodeT5and
GPT-Neo-1.3Bbyupto6.9%exactmatch,andCodexby6.78charBLEUscore.
TheseresultsopenapromisingdirectionforNL‚Üícodegeneration. Webelievethatourresultscanbe
furtherimprovedusingmorecleverencodingofthestructurednatureoflongdocuments,andusing
jointtrainingoftheretrieverandthegenerator,whichhopefullywillavoidcascadingerrors. Further,
webelievethattheprinciplesandthemethodspresentedinthispaperareapplicabletoadditional
code-relatedtasks,andotherdocumentation-likeresourcessuchastutorialsandblogposts. Tothese
ends,wemakeallourcode,data,andmodelspubliclyavailable.
9
PublishedasaconferencepaperatICLR2023
9 ACKNOWLEDGEMENT
We thanks the anonymous reviewers for their useful comments and suggestions. This work is
supported by a gift from Amazon AI and a contract from the Air Force Research Laboratory
underagreementnumberFA8750-19-2-0200. TheU.S.Governmentisauthorizedtoreproduceand
distributereprintsforGovernmentalpurposesnotwithstandinganycopyrightnotationthereon. The
viewsandconclusionscontainedhereinarethoseoftheauthorsandshouldnotbeinterpretedas
necessarilyrepresentingtheofficialpoliciesorendorsements,eitherexpressedorimplied,oftheAir
ForceResearchLaboratoryortheU.S.Government.
REFERENCES
MiltiadisAllamanis,DanielTarlow,AndrewD.Gordon,andYiWei. Bimodalmodellingofsourcecodeand
naturallanguage. InFrancisR.BachandDavidM.Blei,editors,Proceedingsofthe32ndInternational
ConferenceonMachineLearning,ICML2015,Lille,France,6-11July2015,volume37ofJMLRWorkshop
andConferenceProceedings,pages2123‚Äì2132.JMLR.org,2015. URLhttp://proceedings.mlr.
press/v37/allamanis15.html.
UriAlon,RoySadaka,OmerLevy,andEranYahav. Structurallanguagemodelsofcode. InInternational
conferenceonmachinelearning,pages245‚Äì256.PMLR,2020.
JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,DavidDohan,EllenJiang,
CarrieCai,MichaelTerry,QuocLe,etal. Programsynthesiswithlargelanguagemodels. ArXivpreprint,
abs/2108.07732,2021. URLhttps://arxiv.org/abs/2108.07732.
Nathanae¬®lBeauandBenoÀÜƒ±tCrabbe¬¥. Theimpactoflexicalandgrammaticalprocessingongeneratingcode
fromnaturallanguage. ArXivpreprint,abs/2202.13972,2022. URLhttps://arxiv.org/abs/2202.
13972.
SidBlack, LeoGao, PhilWang, ConnorLeahy, andStellaBiderman. GPT-Neo: LargeScaleAutoregres-
siveLanguageModelingwithMesh-Tensorflow,2021. URLhttps://doi.org/10.5281/zenodo.
5297715. Ifyouusethissoftware,pleaseciteitusingthesemetadata.
S.R.K.Branavan,DavidSilver,andReginaBarzilay. LearningtowinbyreadingmanualsinaMonte-Carlo
framework. InProceedingsofthe49thAnnualMeetingoftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,pages268‚Äì277,Portland,Oregon,USA,2011.AssociationforComputational
Linguistics. URLhttps://aclanthology.org/P11-1028.
Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr Polozov. Generative code
modeling with graphs. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=Bke4KsA5FX.
MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePonde,JaredKaplan,HarriEdwards,Yura
Burda,NicholasJoseph,GregBrockman,etal. Evaluatinglargelanguagemodelstrainedoncode. ArXiv
preprint,abs/2107.03374,2021. URLhttps://arxiv.org/abs/2107.03374.
TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyE.Hinton. Asimpleframeworkforcontrastive
learningofvisualrepresentations. InProceedingsofthe37thInternationalConferenceonMachineLearning,
ICML2020,13-18July2020,VirtualEvent,volume119ofProceedingsofMachineLearningResearch,
pages1597‚Äì1607.PMLR,2020. URLhttp://proceedings.mlr.press/v119/chen20j.html.
AndrewForwardandTimothyCLethbridge. Therelevanceofsoftwaredocumentation,toolsandtechnologies:
asurvey. InProceedingsofthe2002ACMsymposiumonDocumentengineering,pages26‚Äì33,2002.
DanielFried,ArmenAghajanyan,JessyLin,SidaWang,EricWallace,FredaShi,RuiqiZhong,Wen-tauYih,
LukeZettlemoyer,andMikeLewis. Incoder: Agenerativemodelforcodeinfillingandsynthesis. ArXiv
preprint,abs/2204.05999,2022. URLhttps://arxiv.org/abs/2204.05999.
TianyuGao,XingchengYao,andDanqiChen. SimCSE:Simplecontrastivelearningofsentenceembeddings.
In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
6894‚Äì6910,OnlineandPuntaCana,DominicanRepublic,2021.AssociationforComputationalLinguistics.
doi:10.18653/v1/2021.emnlp-main.552. URLhttps://aclanthology.org/2021.emnlp-main.
552.
RuiqiGuo,PhilipSun,ErikLindgren,QuanGeng,DavidSimcha,FelixChern,andSanjivKumar. Accelerating
large-scaleinferencewithanisotropicvectorquantization. InInternationalConferenceonMachineLearning,
2020. URLhttps://arxiv.org/abs/1908.10396.
10
PublishedasaconferencepaperatICLR2023
KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMing-WeiChang. Realm:Retrieval-augmented
languagemodelpre-training. ArXivpreprint,abs/2002.08909,2020. URLhttps://arxiv.org/abs/
2002.08909.
TatsunoriBHashimoto,KelvinGuu,YonatanOren,andPercySLiang. Aretrieve-and-editframeworkfor
predictingstructuredoutputs. AdvancesinNeuralInformationProcessingSystems,31,2018.
ShirleyAnugrahHayati,RaphaelOlivier,PravalikaAvvaru,PengchengYin,AnthonyTomasic,andGraham
Neubig.Retrieval-basedneuralcodegeneration.InProceedingsofthe2018ConferenceonEmpiricalMethods
inNaturalLanguageProcessing,pages925‚Äì930,Brussels,Belgium,2018.AssociationforComputational
Linguistics. doi:10.18653/v1/D18-1111. URLhttps://aclanthology.org/D18-1111.
AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecuriouscaseofneuraltextdegeneration.
arXivpreprintarXiv:1904.09751,2019.
SrinivasanIyer,IoannisKonstas,AlvinCheung,andLukeZettlemoyer. Mappinglanguagetocodeinpro-
grammaticcontext. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages1643‚Äì1652,Brussels,Belgium,2018.AssociationforComputationalLinguistics. doi:
10.18653/v1/D18-1192. URLhttps://aclanthology.org/D18-1192.
GautierIzacardandEdouardGrave. Leveragingpassageretrievalwithgenerativemodelsforopendomainques-
tionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationforCompu-
tationalLinguistics:MainVolume,pages874‚Äì880,Online,2021.AssociationforComputationalLinguistics.
doi:10.18653/v1/2021.eacl-main.74. URLhttps://aclanthology.org/2021.eacl-main.74.
JeffJohnson,MatthijsDouze,andHerve¬¥Je¬¥gou. Billion-scalesimilaritysearchwithGPUs. IEEETransactions
onBigData,7(3):535‚Äì547,2019.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,
andWen-tauYih. Densepassageretrievalforopen-domainquestionanswering. InProceedingsofthe
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769‚Äì6781,
Online,2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL
https://aclanthology.org/2020.emnlp-main.550.
TimothyCLethbridge,JaniceSinger,andAndrewForward. Howsoftwareengineersusedocumentation:The
stateofthepractice. IEEEsoftware,20(6):35‚Äì39,2003.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
HeinrichKu¬®ttler,MikeLewis,Wen-tauYih,TimRockta¬®schel,SebastianRiedel,andDouweKiela. Retrieval-
augmentedgenerationforknowledge-intensiveNLPtasks. InHugoLarochelle,Marc‚ÄôAurelioRanzato,Raia
Hadsell,Maria-FlorinaBalcan,andHsuan-TienLin,editors,AdvancesinNeuralInformationProcessing
Systems33: AnnualConferenceonNeuralInformationProcessingSystems2020,NeurIPS2020,Decem-
ber6-12,2020,virtual,2020. URLhttps://proceedings.neurips.cc/paper/2020/hash/
6b493230205f780e1bc26945df7481e5-Abstract.html.
XiVictoriaLin,ChenglongWang,LukeZettlemoyer,andMichaelD.Ernst. NL2Bash:Acorpusandsemantic
parserfornaturallanguageinterfacetothelinuxoperatingsystem.InProceedingsoftheEleventhInternational
ConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan,2018.EuropeanLanguage
ResourcesAssociation(ELRA). URLhttps://aclanthology.org/L18-1491.
YanxinLu,SwaratChaudhuri,ChrisJermaine,andDavidMelski. Data-drivenprogramcompletion. arXiv
preprintarXiv:1705.09042,2017.
ErikNijkamp,BoPang,HiroakiHayashi,LifuTu,HuanWang,YingboZhou,SilvioSavarese,andCaiming
Xiong. Aconversationalparadigmforprogramsynthesis. arXivpreprint,2022.
JanetNykaza,RhondaMessinger,FranBoehme,CherieLNorman,MatthewMace,andManuelGordon. What
programmersreallywant:resultsofaneedsassessmentforsdkdocumentation. InProceedingsofthe20th
annualinternationalconferenceonComputerdocumentation,pages133‚Äì141,2002.
MdRizwanParvez,WasiAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang. Retrievalaug-
mentedcodegenerationandsummarization. InFindingsoftheAssociationforComputationalLinguistics:
EMNLP2021,pages2719‚Äì2734,PuntaCana,DominicanRepublic,2021.AssociationforComputational
Linguistics. doi:10.18653/v1/2021.findings-emnlp.232. URLhttps://aclanthology.org/2021.
findings-emnlp.232.
11
PublishedasaconferencepaperatICLR2023
PanupongPasupat,YuanZhang,andKelvinGuu. Controllablesemanticparsingviaretrievalaugmentation.
In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
7683‚Äì7698,OnlineandPuntaCana,DominicanRepublic,2021.AssociationforComputationalLinguistics.
doi:10.18653/v1/2021.emnlp-main.607. URLhttps://aclanthology.org/2021.emnlp-main.
607.
MaximRabinovich,MitchellStern,andDanKlein. Abstractsyntaxnetworksforcodegenerationandsemantic
parsing. InProceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume
1:LongPapers),pages1139‚Äì1149,Vancouver,Canada,2017.AssociationforComputationalLinguistics.
doi:10.18653/v1/P17-1105. URLhttps://aclanthology.org/P17-1105.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,Wei
Li,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. ArXiv
preprint,abs/1910.10683,2019. URLhttps://arxiv.org/abs/1910.10683.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,Wei
Li,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. Journalof
MachineLearningResearch,21:1‚Äì67,2020.
StephenERobertsonandKSparckJones.Relevanceweightingofsearchterms.JournaloftheAmericanSociety
forInformationscience,27(3):129‚Äì146,1976.
PedroRodriguezandJordanBoyd-Graber. Evaluationparadigmsinquestionanswering. InProceedingsof
the2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages9630‚Äì9642,Onlineand
PuntaCana,DominicanRepublic,2021.AssociationforComputationalLinguistics. doi:10.18653/v1/2021.
emnlp-main.758. URLhttps://aclanthology.org/2021.emnlp-main.758.
TobiasRoehm,RebeccaTiarks,RainerKoschke,andWalidMaalej.Howdoprofessionaldeveloperscomprehend
software? In201234thInternationalConferenceonSoftwareEngineering(ICSE),pages255‚Äì265.IEEE,
2012.
FredaShi,DanielFried,MarjanGhazvininejad,LukeZettlemoyer,andSidaI.Wang. Naturallanguagetocode
translationwithexecution,2022. URLhttps://arxiv.org/abs/2204.11454.
DishaShrivastava,HugoLarochelle,andDanielTarlow. Repository-levelpromptgenerationforlargelanguage
modelsofcode. InICML2022WorkshoponKnowledgeRetrievalandLanguageModels.
YueWang, WeishiWang, ShafiqJoty, andStevenC.H.Hoi. CodeT5: Identifier-awareunifiedpre-trained
encoder-decodermodelsforcodeunderstandingandgeneration. InProceedingsofthe2021Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,pages8696‚Äì8708,OnlineandPuntaCana,Dominican
Republic,2021.AssociationforComputationalLinguistics. doi:10.18653/v1/2021.emnlp-main.685. URL
https://aclanthology.org/2021.emnlp-main.685.
ZhiruoWang,ShuyanZhou,DanielFried,andGrahamNeubig. Execution-basedevaluationforopen-domain
codegeneration. arXivpreprintarXiv:2212.10481,2022.
YuhuaiWu,MarkusNormanRabe,DeLesleyHutchins,andChristianSzegedy. Memorizingtransformers. In
InternationalConferenceonLearningRepresentations,2021.
FrankF.Xu,ZhengbaoJiang,PengchengYin,BogdanVasilescu,andGrahamNeubig. Incorporatingexternal
knowledgethroughpre-trainingfornaturallanguagetocodegeneration. InProceedingsofthe58thAnnual
MeetingoftheAssociationforComputationalLinguistics,pages6045‚Äì6052,Online,2020.Associationfor
ComputationalLinguistics.doi:10.18653/v1/2020.acl-main.538.URLhttps://aclanthology.org/
2020.acl-main.538.
FrankFXu, UriAlon, GrahamNeubig, andVincentJHellendoorn. Asystematicevaluationoflargelan-
guagemodelsofcode. ArXivpreprint,abs/2202.13169,2022. URLhttps://arxiv.org/abs/2202.
13169.
Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. In
Proceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long
Papers),pages440‚Äì450,Vancouver,Canada,2017.AssociationforComputationalLinguistics. doi: 10.
18653/v1/P17-1041. URLhttps://aclanthology.org/P17-1041.
PengchengYin,BowenDeng,EdgarChen,BogdanVasilescu,andGrahamNeubig. Learningtominealigned
codeandnaturallanguagepairsfromstackoverflow. In2018IEEE/ACM15thinternationalconferenceon
miningsoftwarerepositories(MSR),pages476‚Äì486.IEEE,2018.
VictorZhong,TimRockta¬®schel,andEdwardGrefenstette. Rtfm:Generalisingtonovelenvironmentdynamics
viareading. ArXivpreprint,abs/1910.08210,2019. URLhttps://arxiv.org/abs/1910.08210.
12
PublishedasaconferencepaperatICLR2023
A TLDR: A NEWLY CURATED SHELL SCRIPTING BENCHMARK
NL‚ÜíBashpairs Foreachcommand(e.g.,cat),userscontributeexamplesofpairsofNLdescriptionsand
bashcode(mainlyone-liners),includingvariousflagsandarguments,whichcoverthecommonusagesofthat
command.AnexampleisshowninFigure2.
We crawl NL-code pairs from the markdown files5 in the linux and common folders. We discard Bash
commandswhosemanualisunavailable(discussedbelow). ThedetailedstatisticsareshowninTable6. On
average, each command has 4.84 NL‚ÜíBash pairs and there is a total of 9187 NL-code pairs. To test the
generalizabilityofamodel,weconstructthetraining,developmentandthetestsetwithcompletelydifferent
commands.
Table6: Thestatisticsofthetldrshellscriptingbenchmark
#Commands NL‚ÜíBashpairs
train 1315 6414
dev 376 1845
test 188 928
total 1879 9187
Documentation pool D Wetakethebashmanualofthe1897bashcommandsintldrtoconstructa
documentationpool.Wesearcheachcommandnameatmanned.org6,awebsitewhicharchivesUnixmanual
pages(thesameastheUnix‚Äòman <command>command),andthenextractthetextcontentsfromthereturned
manualpage. Wefurtherbreakeachmanualintomultipleparagraphsbylinebreakssothateachparagraph
delicatelydescribesasingleconceptsuchasacommandfunctionalityoraflagusage.Wemakethisdecision
duetothelargevolumeofcontenteachmanualhas,whichistoolongtofitthelengthlimitationofaneural
model,andtoonoisyanddistractsthemodelwithirrelevantinformation.Thisresultsin400kindividualentries
inthepoolintotal.
Oracle manual D‚àó We find the ground truth documentation for each (n,c) pair through command
i
name and flag matching heuristics. For instance, given a code snippet toilet ‚Äôinput text‚Äô -f
‚Äôfont filename‚Äô,weconstrainoursearchtothedocumentationfromtoiletmanualpageandselect
documentationthatstartswith-fflagasanoracleparagraph.Alongwiththefirstparagraphthatcommonly
summarizesacommand,theseparagraphsformsD‚àó.
n
Evaluationmetrics Weusefourevaluationmetricstomeasurethequalityofthegeneratedcode:(a)com-
mandnameaccuracy(CMDAcc)‚Äìmeasureswhetherthecommandname(e.g.,cat)ispredictedcorrectly;
(b)token-levelF1‚Äìconvertsthereferencecodeandthegeneratedcodetobagofwordsandmeasuresthe
token-levelprecision,recall,andF1overlap;(c)exactmatch(EM)‚Äìmeasurestheexactmatchbetweenthe
referenceandthegeneration;and(d)character-levelBLEU(charBLEU; Linetal.,2018;Shietal.,2022).
FortokenlevelF1,exactmatch,andcharBLEU,wedisregardalluser-specificvariablesinthereferencesand
the system outputs. For example, ‚Äùmycli -u [user] -h [host] [database]‚Äù is converted into
‚Äùmycli -u $1 -h $2 $3‚Äù. Thisismainlybecausethevariablesarenotinstantiatedintldrandthe
styleoftheplaceholdervariesamongcontributors. Forexample,somecontributorsmightwrite[user]as
[username]or[your name]. Therefore,measuringthesurfaceformofuser-specificvariablenamesis
lessmeaningful.
B RE-SPLITTING CONALA
NL‚ÜíPythonpairs WeadaptthepopularCoNaLabenchmark andre-splitthedatasettotestthegeneraliza-
tionscenario.Thisre-splitmakeseveryexampleinthedevelopmentandthetestsethaveatleastonePython
function(e.g.,plt.plot)thatwasnotseeninthetrainingdata. Thereare2135,201,and543examplesin
thetraining,developmentandtestsets,respectively.WefollowtheoriginalworkYinetal.(2018)toevaluate
thesystemoutputswithBLEU-4.Sincewefocusonthegeneralizationsetting,weadditionallyreportunseen
functionaccuracy,whichmeasuresthepercentageofcorrectlypredictedheld-outfunctionsthatdonotappearin
thetrainingset.
5e.g.,https://github.com/tldr-pages/tldr/blob/main/pages/linux/toilet.md
6https://manned.org
13
PublishedasaconferencepaperatICLR2023
Human-annotatedunittests FollowingChenetal.(2021)andAustinetal.(2021),weconductexecution-
basedevaluationonCoNaLatomeasurethefunctionalcorrectnessofthegeneratedcode.Werandomlyselected
100examplesfromthetestsetandmanuallyannotatedunittestforeachexample.Forexample,wewrotetests
suchasassert gen code("abcds", 2) == 4andassert gen code("abde", 2) == -1to
verifywhetherthefunctiongen codecouldperform‚Äúfindtheindexofsubstring‚Äôs‚Äôinstring‚Äòstr‚Äòstarting
fromindex2‚Äù.Eachexamplewasannotatedbyasingleannotator.Theannotationwasdonebytwoauthorsof
thepaperwhoprogramwithPythondaily.Onaverage,weannotate2.03unittestsforeachexample.
Documentation pool D Our documentation pool contains 35763 manuals. These functions are from
all Python libraries that are available on DevDocs7. These libraries contains the Python built-in library,
and popular libraries like numpy and pandas. The documentation on DevDocs are curated and further
transformedandindexedtoallowforquicksearchingofAPIs. WethenextracteachAPIsignatureandthe
correspondingdocumentationineverylibrary,removeanycontentinthedocumentationthatisnottext,and
segmentthedocumentationintomultipleparagraphsbasedonthe<p>HTMLtags.Thedocumentationpool
thencontainspairsoftheAPIsignatureandasingleparagraphinthecorrespondingdocumentation.Although
thedocumentationpoolisnotcomprehensivetocoverallPythonlibrariesandfunctions,wefindithasahigh
coveragerateontheCoNaLadataset.Thischoicereflectstheflexibilityofourapproachuponthecharacteristics
ofatargetscenario.
Oracle manual D‚àó To find the oracle documents for a given NL intent D‚àó from the original
i i
(n,c) example, we first index the function names with absolute path (e.g., plot is indexed with
matplotlib.pyplot.plot) with Elasticsearch. Then we query the search engine with clean version
ofcwherevariablenameareremoved.Thetop-5functionsafterde-duplicationaretreatedasoraclemanuals
D‚àó.
i
Naturallanguageandcodeassociationsduringpretraining Despiteourefforts,itispossiblethat
someoftheheld-outfunctionsinthetestsetwereseentoassociatewithNLcontexts(e.g.,comments)during
thepretrainingofaretrieverandagenerator.Sincethegeneratorswereinitializedfromthesamecheckpointin
boththebaselinesandtheDocPromptingmodels,suchapossibleassociationisexpectedtoequallyhelpboth
models.Intheretriever,suchapossibleassociationdidnotcausetheretrievertoseetheexactNLintentstogether
withthecorrespondingdocumentation,andthusthematchingbetweenNL‚Üê‚Üídocwasnotleaked.However,it
ispossiblethattherehadbeensemanticallysimilarintentsseenalongwiththecodesnippetsoftheheld-out
functions.Nevertheless,suchco-occurrenceis‚Äúindirect‚Äùand‚Äúunsupervised‚Äù.
C DENSE RETRIEVER TRAINING
Wefinetunethemodelfor10epochswithbatchsizeof512andlearningrateof1e‚àí5.SinceCodeT5doesnot
use[CLS]token,wealternativelytaketheaverageofthehiddenstateofthelastlayerasthetextrepresentation.
ForCoNaLa,wealsousethefirst100k‚Äùmined‚ÄùexamplesprovidedaspartofCoNaLaasthesupervisedcorpus.
ForCoNaLa,weonlyapplyasinglesearchstepbecauseeachcodesnippetcommonlycontainsmorethanone
function.Wealsoobservedthatusingthefirstsentencethatnormallysummarizestheusageofafunctionachieve
thebestretrievalperformancethanotheralternativessuchasusingthefirstparagraph,orsimplytruncatingto
themaximumtokenlength.Thetrainingtakesupto15hoursonasingleA6000GPU.
D GENERATOR TRAINING
Wetrainoursingle-sourcegeneratorsfor20epochswithlearningrate4e‚àí5.WetrainourFiD-basedgenerators
for10000steps. Thedoclengthissetto200,anyfurthercontentwillbetruncated. Wefollow(Izacardand
Grave,2021)tosetlearningrateto5e‚àí5with2000stepswarmupandlinearlearningratedecay.Thebatchsize
issetto8.Thebestmodelisselectedbasedonthetoken-levelF1scoreonthedevelopmentsetfortldrand
BLEUscoreforCoNaLa.Thetrainingtakes 8hoursonasingleA6000GPU.
E CODEX PROMPTS
Forthebaseline,wepromptCodexwiththreeNL-codepairsandappendthetestquerytotheend.Anexampleon
tldrisshownontopofTable7.Onthebottom,welistthepromptwithDocPromptingwheredocumentation
isprovidedalongtoo. Intheoraclecommandnamesetting,weprependthecommandnamebeforeeachNL
7https://devdocs.io
14
PublishedasaconferencepaperatICLR2023
CoNaLa
70.1 70 36.2 36.2 66.7 73.7
60 35.9 36.3
60.4 35.6
50 55.8
40
42.3 34.9
30 33.1
20 33.7 16.5 Recall
BLEU
33.4
11 33 55 1100 1155 2200 25 3300
RetrievedDocs
Figure 5: The recall@k (%) and the corresponding BLEU score by using these top-k docs on
CoNaLadataset(usingCodeT5).
intentforthebaselineprompt. ForDocPromptingprompt,wereplacethepotentialdocswiththeretrieved
docsfromtheoraclemanual.
F ADDITIONAL ANALYSIS
Parameterefficiency AsshowninTable1, underagivenparameterbudget, wefindthatDocPrompting
mostlybenefitsfromparallelencoding(FiD).Forexample,theparallelencodingT5+DocPrompting(220M
parameters)significantlyoutperformsthe125MparametersjointencodingNeo-125M+DocPrompting.Only
scalingupNeo+DocPromptingto1.3Bparametersmanagestomatchthe220MparameterT5+DocPrompting.
ApossibleexplanationisthatalthoughthebaseNeo-1.3B(withoutDocPrompting)generallyperformsbetter
thanthebaseT5(withoutDocPrompting),parallelencodingallowstoutilizetheretrieveddocumentsbetter,
sincedocumentsareencodedindependentlyontheencoderside.
TheimpactofthenumberofdocumentsFigure5showstherecall@kandtheBLEUscorecomparedtok,
thenumberofretrieveddocuments.Increasingkconsistentlyyieldsahigherrecall;however,asmoreirrelevant
documentsareretrieved,thegeneratorcannoteffectivelydistinguishthemfromtherelevantonesandtheoverall
performanceremainsimilar. Forexample,CodeT5achievesthehighestBLEUscoreusing5 ‚â§ k ‚â§ 10. In
contrast,whenthegeneratorisprovidedwiththeoracledocsonly,itsBLEUscorereaches49.04(Table3).This
suggeststhatbothprecisionandrecallofdocsareimportant,andthebenefitofusinglargervaluesofkinopen
domainQA(IzacardandGrave,2021)doesnotnecessarilyholdincodegeneration.
Fulln-gramoverlap Table8showsthatusingdocumentationsignificantlyincreasesthen-gramoverlap
recallbetweentheinputandtheoutput,intldrandCoNaLa.SinceweusedBM25toretrievedocsintldr,
theNL‚Üê‚ÜíRetrieveddocsoverlapishighbyconstruction.InCoNaLa,theNL‚Üê‚ÜíRetrieveddocsunigramoverlap
ishighaswell,butsinceweusedadenseretriever,thegeneraln-gramoverlapdoesnothavetobehighfor
DocPromptingtoworkwell.
Retrieval latency Although retrieving docs results in additional test-time computation, the increase in
latencyisnotprohibitive.First,encodingtheinputfortheretrievalstep‚Äúcosts‚Äùasingleforwardpassthrough
theretriever‚Äôsencoder,whichissignificantlylessexpensivethangeneration(whichrequiresmultipletimesteps
ofthedecoder).Allthedocumentationintheretrievalpoolcanbeencodedinadvance,andfindingthetop-k
resultscanbeperformedquicklyusinglibrariessuchasFAISSJohnsonetal.(2019)ontheGPUorScaNNGuo
etal.(2020)onCPU.Thecostofthistop-k searchissub-linearinthesizeofthedocumentpool. Second,
theadditionalinputtothegeneratorresultsinanincreasedmemoryconsumption,butonlyasmallincrease
inlatencysincethetokensofagiveninputcanbeencodedinparallel.Ifthisdifferenceiscrucialinpractical
settings,wecandecreasethenumberofretrieveddocuments.Figure5showsthatretrievingasfewasfivedocs
maybesufficientinmanycases.
G FULL PASS@k PLOTS
Inthemainexecution-basedevaluation,pass@kresultsinSection5.2andFigure3,wetookthebesttemperature
foreverymodelandvalueofk.Here,weshowallthepass@kplotswithdifferenttemperaturesinFigure6.
15
k@llaceRlaveirteR UELBnoitareneG
PublishedasaconferencepaperatICLR2023
#getthelabelofafat32partition
fatlabel /dev/sda1
#END
#displayinformationwithoutincludingthelogin,jcpuandpcpucolumns
w --short
#END
#sortacsvfilebycolumn9
csvsort -c 9 data.csv
#END
#searchforapackageinyourcurrentsources
Potentialdocument0:fatlabelwilldisplayorchangethevolumelabelorvolumeIDontheMS-DOS
filesystemlocatedonDEVICE...
#getthelabelofafat32partition
fatlabel /dev/sda1
#END
Potentialdocument0: wdisplaysinformationabouttheuserscurrentlyonthemachine,andtheir
processes.Theheadershows,inthisorder...
Potentialdocument1: -s,‚ÄìshortUsetheshortformat. Don‚Äôtprintthelogintime,JCPUorPCPU
times.
#displayinformationwithoutincludingthelogin,jcpuandpcpucolumns
w --short
#END
Potentialdocument0:SortCSVfiles.LiketheUnix‚Äúsort‚Äùcommand,butfortabulardata
Potentialdocument1:usage:csvsort[-h][-dDELIMITER][-t][-qQUOTECHAR][-u0,1,2,3][-b]
[-pESCAPECHAR]...
Potentialdocument2: optionalarguments: -h,‚Äìhelshowthishelpmessageandexit-n,‚Äìnames
DisplaycolumnnamesandindicesfromtheinputCSVandexit.-cCOLUMNS...
Potentialdocument3:csvsort-c9examples/realdata/FY09 EDU Recipients by State.csv
Potentialdocument4:csvcut-c1,9examples/realdata/FY09 EDU Recipients by State.csv‚Äîcsvsort
-r-c2‚Äîhead-n5
#sortacsvfilebycolumn9
csvsort -c 9 data.csv
#END
Potentialdocument1:...
Potentialdocument2:...
...
#searchforapackageinyourcurrentsources
Table7:Top:baselineCodexpromptwiththreeNL-codepairsandatestintent.Bottom:DocPrompt-
ingpromptforCodex. Ineachin-contextlearningexample,theoracledocs,theNLintentandthe
correspondingbashcommandareprovided. Weuseuptofiveoracledocsfortheseexamples. For
a test example, the top-5 paragraphs from the retriever are represented with the NL intent. The
documents‚Äôcontentswereomitted(‚Äú...‚Äù) tosavespace.
16
PublishedasaconferencepaperatICLR2023
Table8: n-gramoverlapbetweendifferentcontents(%). Usingdocumentationsignificantlyincreases
then-gramoverlaprecallbetweentheinputandtheoutput,intldrandCoNaLa.
tldr 1 2 3 CoNaLa 1 2 3 4 5
NL‚Üê‚ÜíCode 12 0 0 NL‚Üê‚ÜíCode 30 14 11 9 7
(NL+retrieveddocs)‚Üê‚ÜíCode 24 2 0 (NL+retrieveddocs)‚Üê‚ÜíCode 91 52 28 16 11
NL‚Üê‚ÜíRetrieveddocs 39 8 3 NL‚Üê‚ÜíRetrieveddocs 72 14 3 1 1
temperature=0.2 temperature=0.4
0.18 0.225 CodeT5
+DocPrompting
0.16 0.200
0.175
0.14
0.150
0.12
0.125
0.10
0.100
0.08
0.075
0.06 CodeT5
+DocPrompting 0.050
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
k k
temperature=0.6 temperature=0.8
0.30 CodeT5 0.35 CodeT5
+DocPrompting +DocPrompting
0.25 0.30
0.25
0.20
0.20
0.15
0.15
0.10 0.10
0.05 0.05
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
k k
temperature=1.0
0.35 CodeT5
+DocPrompting
0.30
0.25
0.20
0.15
0.10
0.05
0 25 50 75 100 125 150 175 200
k
Figure6: Pass@kon100examplesonthetestsetwithdifferenttemperatures.
17
k@ssap
k@ssap
k@ssap
k@ssap
k@ssap
PublishedasaconferencepaperatICLR2023
Table9: ResultsontldrandCoNaLawithcode-davinci-002.
tldr
Model CMDAcc(%) EM(%) TokenF1 charBLEU
Codex - 39.01 14.55 44.89 33.93
3-shots +DocPrompting 36.10 13.97 42.55 32.93
Withtheoraclecommandname
- - 20.22 59.22 38.14
+DocPrompting - 33.15 68.59 44.76
CoNaLa
BLEU Recall
- 48.39 43.35
+DocPrompting 47.21 44.70
+DocPromptingoracledocs 54.67 59.68
H EXPERIMENTS WITH code-davinci-002
Theresultswithcode-davinci-002underfew-shotlearningsettingisshowninTable9.Inthenon-oraclesettings,
Codex+DocPromptingdidnotimproveoverthebaseCodex;oneexplanationmightbethatthedatasetsare
leakedintothetrainingcorpusoftheCodex.Forexample,CoNaLawasextractedfromStackOverflow,whichis
includedinthelargeCommonCrawlcorpus8thatwasusedtotrainGPT-3,andpossiblyCodex.Therefore,Codex
mighthavememorizedthetargetcode,andthusdidnotneedtheadditionaldocumentation.Althoughthedata
leakageissuemighthavehappenedincode-davinci-001aswell,wesuspectthatthisissuehasworsenedinthe
stronger002version.Regardless,webelievethatthelargecapacityofCodexrequiresanequallystrongretriever
toimproveoverthebasemodel.Withanoracleretriever,DocPromptingyieldssignificantimprovementon
bothdatasets.Thus,thenon-oracleresultscouldbefurtherimprovedusingastrongernon-oracleretriever.
I EXAMPLES
I.1 TLDR
ExamplesontldrareinTable10.Inthetopthreecases,thebaselineT5couldnotgeneratethecorrectbash
commandwhileT5+DocPromptingretrievesthecorrectbashdocumentationandusethecorrectarguments.In
thebottomtwocases,T5alsopredictsthebashcommandcorrectly.However,inthepuppetcase,itmisses
concreteusagesofexecutingthecode.Intheelixircase,ithalluciatetheargumentflags--exampleand
--argumentswhichelixirdoesnotsupportatall.Ontheotherhand,T5+DocPromptingdoesground
itsgenerationtotheretrieveddocsandthereforeitgeneratesmoreaccuratecode.
I.2 CONALA
MoreexamplesonCoNaLaareshowninTable11.
8https://commoncrawl.org/the-data/
18
PublishedasaconferencepaperatICLR2023
Table10: Examplesofpredictionsfromtldr,ofthebaseT5comparedtoT5+DocPrompting. In
tldr,thebashcommandsinthetestsetarecompletelydifferentfromthoseinthetrainingset.
playamidifileinaspecifickey(0=cmajor/aminor,1=fmajor/dminor,+1=gmajor/eminor,etc.)
Groundtruth: timidity --force-keysig=‚Äô-flats|+sharps‚Äô ‚Äôpath/to/file.mid‚Äô
T5: midiplay ‚Äôkey‚Äô
T5+DocPrompting: timidity --force-keysig ‚Äôf|f|f‚Äô ‚Äôpath/to/file.midi‚Äô
cleanupalltemporarytexfilesinthecurrentdirectory
Groundtruth: latexmk -c
T5: tex clean
T5+DocPrompting: latexmk -c
specifythehashalgorithmfortheindexfile(experimental)
Groundtruth: git show-index --object-format=‚Äôsha1|sha256‚Äô ‚Äôpath/to/file‚Äô
T5: hash --experimental ‚Äôpath/to/index.hashes‚Äô
T5+DocPrompting: git show-index --object-format= ‚Äôpath/to/hash-algorithm‚Äô
‚Äôpath/to/experimental‚Äô
executepuppetcode
Groundtruth: puppet apply --execute ‚Äôcode‚Äô
T5: puppet
T5+DocPrompting: puppet apply --execute ‚Äôpath/to/file.puppet‚Äô
evaluateelixircodebypassingitasanargument
Groundtruth: elixir -e ‚Äôcode‚Äô
T5: elixir --example --arguments ‚Äôpath/to/file.elixir‚Äô
T5+DocPrompting: elixir -e ‚Äôpath/to/file.elixir‚Äô
Table 11: Examples of predictions from CoNaLa, of the base CodeT5 compared to
CodeT5+DocPrompting. Unseenfunctionsareunderscored.
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
setthecurrentworkingdirectoryto‚Äôc:\Users\uname\desktop\python‚Äô
Groundtruth: os.chdir(‚Äôc:\Users\uname\desktop\python‚Äô)
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
CodeT5: os.system(‚Äôc:\Users\uname\desktop\python‚Äô)
CodeT5+DocPrompting: os.chdir(‚Äôc:\Users\uname\desktop\python‚Äô)
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
convertdataframe‚Äôdf‚Äôtointeger-typesparseobject
Groundtruth: df.to sparse(0)
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
CodeT5: np.isinstance(df, np.integer)
CodeT5+DocPrompting: df.to sparse(‚Äôi‚Äô)
(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)
19

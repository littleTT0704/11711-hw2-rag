TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment
JianweiYang YonatanBisk JianfengGao
MicrosoftResearch CarnegieMellonUniversity MicrosoftResearch
jianwyan@microsoft.com ybisk@cs.cmu.edu jfgao@microsoft.com
Abstract
ùêæ‚Ä≤hard negatives
Contrastive Lossùêø3
Contrastive learning has been widely used to train
‚Ä¶
transformer-based vision-language models for video-text Multi-modal Fusion Layers Hard negative mining
H
alignment and multi-modal representation learning. This alignment
paper presents a new algorithm called Token-Aware Contrastive Lossùêø2 scores
(add, tomatoes, pan, stir)
Cascade contrastive learning (TACo) that improves con- Contrastive Lossùêø1
trastivelearningusingtwonoveltechniques. Thefirstisthe
Language Video
token-aware contrastive loss which is computed by taking Encoder Encoder
into account the syntactic classes of words. This is mo-
Addblended tomatoes
tivated by the observation that for a video-text pair, the to panand stir.
‚Ä¶
content words in the text, such as nouns and verbs, are Text-video pair ùêæ‚àí1negatives
more likely to be aligned with the visual contents in the
Figure 1: The proposed token-aware cascade contrastive
videothanthefunctionwords. Second,acascadesampling
learningpipeline. Wecomputethreecontrastivelosses: 1)
method is applied to generate a small set of hard nega-
sentence-levellossL overallnegativeexamples;2)token-
tive examples for efficient loss estimation for multi-modal 1
level loss L on content words (noun, verb) over all nega-
fusion layers. To validate the effectiveness of TACo, in 2
tiveexamples;3)sentence-levellossL overhardnegative
ourexperimentswefinetunepretrainedmodelsforasetof 3
examplessampledbasedonL andL online.
downstreamtasksincludingtext-videoretrieval(YouCook2, 1 2
MSR-VTT and ActivityNet), video action step localization
answering[44,27]andvideocaptioning[58].
(CrossTask),videoactionsegmentation(COIN).Theresults
In this paper, we present a new variant of con-
showthatourmodelsattainconsistentimprovementsacross
trastive learning, Token-Aware Cascade contrastive learn-
different experimental settings over previous methods, set-
ing (TACo) to improve the video-text alignment for both
tingnewstate-of-the-artonthreepublictext-videoretrieval
large-scale pretraining and downstream specific tasks. As
benchmarksofYouCook2,MSR-VTTandActivityNet.
the name indicates, TACo makes two modifications to the
conventional contrastive learning used in video-language
1.Introduction domain. Thefirstisthetoken-awarecontrastivelosswhich
iscomputedbytakingintoaccountthesyntacticclassesof
Aligningorgroundinglanguagetovideosisachalleng- words. This is motivated by the observation that, given
ing topic in the context of vision-language (VL) research a video and its corresponding text, content words, such
as it requires the model to understand contents, dynamics, as nouns and verbs, are more likely than function words
and causality presented in videos [3]. Inspired by the suc- to be aligned with (or grounded to) visual contents in the
cessofBERT[10]innaturallanguageprocessing,thereisa video. Conventionalcontrastivelearningtypicallycompute
growinginterestinapplyingtransformer-basedmulti-modal thelossafteraggregatingoverallthewordsinthetextand
models for video-text alignment and representation learn- frames in the video (loss L or L in Fig. 1). In contrast,
1 3
ing [41, 40, 60, 33, 14, 28]. These models are typically the token-aware contrastive loss is computed using only a
pretrainedonlargeamountsofnoisyvideo-textpairsusing subset of words whose syntactic classes belong to a pre-
contrastive learning [35, 34], and then applied in a zero- definedset(e.g.,nounsandverbs),whichforcestheground-
shot manner or finetuned for various downstream tasks, ingofindividualwordstothevideo(lossL2).Forexample,
suchastext-videoretrieval[52],videoactionsteplocaliza- we pay particular attention to the words ‚Äúadd‚Äù, ‚Äútomatos‚Äù,
tion [61], video action segmentation [43], video question ‚Äúpan‚Äùand‚Äústir‚ÄùinFig.1.
1202
guA
32
]VC.sc[
1v08990.8012:viXra
The second technique we introduce is a cascade sam- ousworks,thesetwoapproacheswereexploredseparately.
pling method to find a small set of hard negative exam- Veryrecently,anupdatedversionof[33]usedtwoindepen-
ples for training the multi-modal fusion layers. Consider dentalignmentlossesbeforeandaftermulti-modalfusionin
a batch of K video-text pairs. For each of the video-text asingleframework.Inthispaper,however,thesetwolosses
pairs, the ideal case is that we use the remaining K ‚àí 1 cooperatecloselywitheachotherduringtraininginthatthe
negativevideosortextstocomputethecontrastivelossaf- earlierstagehelpstodiscoverthehardnegativeswhilethe
termulti-modalfusion. However,thecostofcomputingthe multi-modallayerswithmorecapacityhelptotacklethose
contrastivelossquicklybecomesprohibitivewhenitiscou- hardsamplesparticularly.
pled with multi-modal fusion layers, considering its high
Video-textalignment. Aligningvideostotextrequiresthe
complexityO(K2√óL2)whereListotalnumberofvisual
modeltounderstandmotionandtemporalcoherence.Some
and textual tokens. A conventional way to address this is
works have relied on attention mechanisms to extract key
usingrandomsamplingtoselectasmallsubsetofnegative
informationfromvideos[45,55],whileotherspreservevi-
pairs. In this paper, instead of random sampling, we pro-
sual information by composing pairwise joint representa-
pose a cascade sampling method as shown in the top-right
tionusing3Dtensors[53]orusemulti-levelvideoencoders
of Fig. 1 to efficiently select a small set of hard negative
to separately encode the spatial and temporal cues [11].
examplesontheflyduringtraining. Itleveragesthevideo-
Thesemodelsusuallyrelyonarankormarginlosstolearn
textalignmentscorescomputedinL andL beforemulti-
1 2 the correct alignment for video-text pairs. Another line
modalfusionlayers,andhelpstolearnthemulti-modalfu-
of work learns fine-grained or hierarchical alignment be-
sionlayersmoreeffectivelywithoutanyextraoverhead.
tween videos and texts [56, 49, 6]. In [49], the authors
We perform a comprehensive empirical study to val-
proposed a fine-grained alignment by extracting the nouns
idate the effectiveness of TACo in both pretraining and
and verbs from action phrase in a sentence and projecting
dataset-specific scenarios. We apply TACo and different
themintoasharedspacewithvideos. Alternatively,theau-
variants of contrastive losses to train or pretrain and fine-
thorsin[6]extractahierarchicalsemanticgraphandapply
tune on various downstream tasks including text-video re-
graph reasoning to achieve the alignment at different lev-
trieval (YouCook2, MSR-VTT and ActivityNet) [58, 52,
els. Similar ideas have been also proposed in the image-
12],videoactionsteplocalization(CrossTask)[61]andac-
text alignment by decomposing the images and texts into
tionsegmentation(COIN)[43].OurresultsshowthatTACo
sub-tokens [26, 50]. Thus far, it has not been studied how
improves the text-video retrieval performance over current
thesetask-specificarchitecturescanbeintegratedintolarge-
state-of-the-art across three benchmarks. Furthermore, the
scale pretraining. In this paper, we are the first to propose
learned multi-modal representation and video representa-
asimpleyeteffectivetoken-awarecontrastivelossforfine-
tioncanbeeffectivelytransferredtoCrossTaskandCOIN,
grainedalignmentforpretraininganddownstreamtasks.
and achieve better or comparable performance to current
Negative sampling. Key to efficient contrastive training
state-of-the-artmethods.
isagoodsourceofnegativeexamples. Mostofcurrentap-
proachesuserandomsamplingstrategiesfortrainingvideo-
2.Relatedwork
textalignment[60,33]. However,inthedomainofimage-
Video-language pretraining. Realistic application sce- text retrieval, a few works tried hard negative sampling to
narios around videos have prompted emergence of vari- choose the hardest negatives for training. In [2, 13], the
ous video-language tasks, such as text-video retrieval [30, authors computed the alignment scores for all image-text
55, 53], video question answering [21, 27], video caption- pairs in a mini-batch and use the hardest negative sample
ing[54,59],etc.InspiredbythesuccessofBERTforlarge- to compute the marginal loss. However, this strategy can
scale pretraining in language domain [10], transformers onlybeappliedwithoutmulti-modalfusion. Inthosemod-
havebeenemployedinthevideo-languagedomain[41,60, els which have multi-modal fusion layers for better repre-
33,28]aswellasimage-languagedomain[42,32,57,29]. sentations [32, 8], the authors instead compute the match-
Combinedwithlargescaledatasets, e.g. Howto100M[35] ing score offline and then use it to sample hard negatives
this approach has proven to be effective on various down- forfinetuningimage-textretrievalmodel,whichhoweveris
streamtasks. Dependingonthetasksofinterest, someap- difficult for large-scale pretraining due to the high compu-
proachestrainamulti-modaltransformerusingacombina- tationalcost. Inthispaper,ourcascadehardnegativemin-
tion of multiple losses including video-text alignment [41, ing is particularly designed to address these issues as we
60, 33, 28], masked token (words/frames/objects) predic- efficiently select the hard negative samples online before
tion [41, 60, 33], and frame order prediction [28], etc. multi-modal fusion and send them to the fusion layers for
Someotherapproachesexploitedvariouscontrastivelearn- computing the loss. As we will show in our experiments,
ing techniques to directly optimize the feature space with- thistechniquecanbeseamlesslyappliedtobothlarge-scale
out multi-modal fusion [35, 34, 31, 14]. In most of previ- pretraininganddownstreamtasks.
2
3.Method ingto[15,37],p(v |t )canbeapproximatedby:
j i
3.1.Framework exps(vj,ti)
p(v |t )‚àº (1)
j i (cid:80)N exps(vk,ti)
AsdepictedinFig.1,ourmodelhasthreecomponents: k=1
Videoencodingmodulef Œ∏v. Itisimplementedbyastack where s(v,t) is the alignment score between v and t; the
of self-attention layers parameterized by Œ∏ v. Here, we as- denominator is a sum over all possible videos, which is a
sume the input video features have been already extracted partitionfunctionfornormalization. Addingcross-entropy
using some pre-trained models such as 2D CNN (e.g., lossonp(v |t ),wecanthenderivetheNCEloss[15]:
j i
ResNet [18])or 3D CNN(e.g., I3D[4], S3D[51]). Given
theinputvideoembeddings,videoencoderstartswithalin- N
(cid:88)
ear layer to project them to the same dimension d as fol- L nce = ‚àílogp(v i|t i)
lowing self-attention layers. We denote the output of our i=1 (2)
videoencoderforavideoclipbyasequenceofmfeatures, (cid:88)N (cid:32) exps(vi,ti) (cid:33)
‚àº ‚àílog
x = {x1,...,xm} ‚àà Rm√ód. Thenumberoffeaturesmde- exps(vi,ti)+(cid:80) exps(vk,ti)
i=1 k(cid:54)=i
pends on the choice of sampling frame rate and the video
featureextractor,whichwewilldiscussinSec.4. ThedenominatorinEq.2requiresasumoverallvideos
Language encoding module f . We use pretrained tok- inadataset,whichisintractableinpractice. Therefore,we
Œ∏t
enizer [48] and BERT [10] to tokenize the input texts and usuallycomputetheNCElossonamini-batchofK(K (cid:28)
extracttextualfeatures,respectively. Givenarawsentence, N) video-text pairs sampled from the whole dataset. Ide-
we append a ‚Äú[CLS]‚Äù and ‚Äú[SEP]‚Äù to the beginning and ally, we want to learn the parameters Œ∏ = {Œ∏ v,Œ∏ t,Œ∏ m}
end, respectively. At the top, we can obtain a sequence of the model to minimize the above NCE loss, such that
of n textual features y = {y1,...,yn} ‚àà Rn√ód. We en- ‚àÜ = s(v i,t i) ‚àí s(v j,t i) is maximized over all tuples
sure the output feature dimension of video encoder to be (t i,v i,v j),j (cid:54)= i. A number of previous works used the
identical to that of language encoder. During training, we above formula for contrastive learning [34, 60]. Mean-
update the parameters Œ∏ in our language encoder to adapt while,therearesomevariantsofcomputingcontrastiveloss
t
tothetextsinspecificdomain,e.g.,cookinginstructionsin in video-langauge representation learning. For example,
YouCook2[58]. [28, 14] omits the denominator and incorporate a margin
s.t. s(v ,t ) > s(v ,t )+Œ¥,‚àÄj (cid:54)= i in a mini-batch. [33]
Multi-modal fusion module f . It also consists of self- i i j i
Œ∏m
optimizesbinarycross-entropy(BCE)byassigning(v ,t )
attentionlayerswithlearnableparametersŒ∏ .Ittakesvideo i i
m
featuresx ‚àà Rm√ód andtextfeaturesy ‚àà Rn√ód fromtwo apositivelabel(1)andotherpairsanegativelabel(0).
separate modalities as inputs and output the (m+n) fea-
3.3.TACo: ourapproach
turesz = {z ,...,z } ‚àà R(m+n)√ód. Tohelpittodis-
1 (m+n)
tinguishthevideoandlanguagetokens,weuseatokentype
Thewayofusingcontrastivelearninginpreviousworks
embeddinglayertolearntwoembeddingsandaddthemto
has two issues. First, the loss is computed at sentence-
thevisualandtextualtokens,separately. Similartooriginal
level by taking ‚Äò[CLS]‚Äô token [14] or the maximum over
Transformer[47],weincludeapositionalembeddinglayer
all tokens [34] in a sentence. Clearly, the content words
toencodetheabsolutetokenpositionsintheinputsequence. (e.g.,nouns,verbs)aremorelikelytoalignwiththevisual
The above three components comprise our video-text contentsorconceptsinthevideoscomparedwithfunction
alignment model which is then trained with the proposed words (e.g., stop words). Second, the high computational
token-awarecascadecontrastiveloss. Westartwithabrief costinmulti-modalfusionlayershindertheusageoflarge
reviewofconventionalcontrastivelearningandthenintro- batch of negative samples, which however is essential to
ducetheproposedtechnique. contrastivelearning[34,17,7]. Motivatedbythesetwois-
sues,weintroduceTACo,asimpleyeteffectivemethodto
improvethecontrastivelearning. Weelaboratebelowhow
3.2.Contrastivelearning: arevisit
thesecontrastivelossesarecomputed.
GivenasetofN video-textpairs{(v ,t )}N ,ourgoal Given the K video-text pairs {(v ,t )}K in a mini-
i i i=1 i i i=1
is to learn an optimal scoring function s such that paired batch, we first use our video encoder f and lan-
Œ∏v
videoandtext(v ,t )havehigherscoresthanalltheother guage encoder f to obtain a batch of video features
i i Œ∏t
unmatched pairs (v ,t ),j (cid:54)= k. From the probabilistic X = {x ,...,x } ‚àà RK√óm√ód and text features Y =
j k 1 K
perspective, aligning v to t is equivalent to maximizing {y ,...,y } ‚àà RK√ón√ód, respectively. Then, we average
i i 1 K
the conditional probability p(v |t ) while minimizing the alltokensofavideoclipv togetx¬Ø ‚àà R1√ód,andtakethe
i i i i
probability for all negative pairs p(v |t ),j (cid:54)= i. Accord- first‚Äò[CLS]‚Äôtokenforeachtextt togety¬Ø ‚ààR1√ód. Based
j i i i
3
onx¬Øandy¬Ø,wecomputethesentence-levelcontrastiveloss: thisasthesummaryoftwomodalitiesandthencomputethe
L 1 =‚àí(cid:88)K log(cid:32) expx¬Øi¬∑y¬Øi/œÑ1ex +px (cid:80)¬Øi¬∑y¬Øi/œÑ e1 xpx¬Øj¬∑y¬Øi/œÑ1(cid:33) (3) contrastiveloss (cid:88): K Ô£´ expw¬∑zic ,l is Ô£∂
where œÑ
1
isi= a1
scalar temperature
parj a(cid:54)= mi
eter. In Eq. 3, the
L 3 =‚àí i=1logÔ£≠ expw¬∑z ic ,l is +(cid:80) j(cid:54)=iexpw¬∑z jc ,l isÔ£∏ (5)
computation is simply a number of dot-products between wherezclsisthemulti-modalfusionoutputfor‚Äú[CLS]‚Äùto-
j,i
videoandtextfeatures. Givingsuchefficiency,wecanuse
ken taking x and y as inputs; w ‚àà R1√ód is the parame-
alltheK ‚àí1negativesamplesinamini-batchtocompute j i
ter in a linear layer1. Based on Eq. 5, we optimize all pa-
theloss.Throughthis,weoptimizeŒ∏ andŒ∏ soastoproject
v t rameters in our model Œ∏ = {Œ∏ ,Œ∏ ,Œ∏ } in collaboration
v t m
thevideoandtextsamplesintoanalignedfeaturespace.
withEq.3andEq.4.
The‚Äò[CLS]‚ÄôtokenandaverageofvideotokensinEq.3
InEq.5, apracticalchallengeisthatwecanhardlyuse
overlooksthedifferencesacrosstokensandframes,andthus
all(K ‚àí1)negativesamplesinthemini-batch, duetothe
maynotprovidethepressuretopushindividualtokens(e.g.,
highcomputationalandmemorycostinthemulti-modalfu-
nounsandverbs)togroundonthespecificvideocontents.
sion. TheO(d(m+n)2)complexityofself-attentionlayer
Toencouragecorrectalignment,inadditiontothesentence-
makesitintractabletopassallK √óK pairsintothemulti-
levelloss,weintroduceatoken-levelcontrastiveloss:
modallayers. Previousworksolvedthisbyperformingran-
Ô£´ Ô£∂
L
=‚àí(cid:88)K (cid:88)
logÔ£¨
exps(xi,y ip)/œÑ2
Ô£∑
Hdo om wes va em r,pl ri an ng dt oo mc lu yt cth he oon su im ngbe nr eo gf atn ive ega st aiv me ps la em sp mle as yt ro eK sul(cid:48) t.
2
i=1p‚ààPi
Ô£≠ exps(xi,y ip)/œÑ2+ j(cid:80) (cid:54)=iexps(xj,y ip)/œÑ2Ô£∏
insub-optimallearningsincethepairsarescarce.Wethere-
(4) foreintroduceacascadesamplingstrategytofindhardneg-
whereœÑ isanotherscalartemperatureparameter;P isthe ativesinsteadofrandomones.
2 i
indices of tokens of interest in i-th text, and yp is the p-th Cascade hard negative sampling. To reduce the compu-
i
token embedding in i-th text. s(¬∑) measures the similarity tationalcostinEq.5,wechooseamongallpossiblevideo-
betweenvideofeaturesandspecifictokenembeddingyp. It textpairsasmallsubsetwhicharemostdifficult. However,
i
firstcomputesthedot-productbetweenyp ‚àà R1√ód andall computingthealignmentscoresforallpairsusingEq.5and
i
m video tokens x ‚àà Rm√ód, and then take the maximum thenselectthehardnegativesisa‚Äúchicken-and-egg‚Äùprob-
over m scores to get the final alignment score. Through lem. Instead,weproposetousethesimilaritiesbetweenall
Eq.4,themodelusesindividualtokensasanchorstoalign video-text pairs computed in Eq. 3 and Eq. 4 as the guid-
with video, which is complementary to the sentence-level ance. Specifically,foreachtext-videopair(v ,t ),wetake
j i
lossinEq.3. SimilartoEq.3,wecancomputethistoken- theirglobalsimilarityx¬Ø ¬∑y¬Ø computedin Eq.3andtoken-
j i
levelcontrastivelossefficiently,andthususealltheK ‚àí1 level similarity by aggregating (cid:80) s(x ,yp) for all to-
p‚ààPi j i
negativesamples. Asawhole,thesetwolossesareusedto kens of interest in t . Then we sum the two similarities as
i
optimizeŒ∏ andŒ∏ inatoken-awaremanner. the alignment score for the given pair. For each text, we
v t
Tokenofinterest.InEq.4,weneedtodecidewhichtokens choose the top K(cid:48) aligned negative videos and vice versa.
should be included in P . In this paper, we heuristically The resulting 2K √ó (K(cid:48) + 1) pairs are then fed into the
i
select nouns and verbs as the targets considering they are multi-modalfusionlayers.Throughthisstrategy,wecanef-
more‚Äúconcrete‚Äù inthevideos. Inpractice, nouns or verbs fectivelyselectthedifficultnegativesamplesontheflyatno
usuallyhavedifferentdiscriminativenesseveniftheyareall extracost.Sincethemulti-modalfusionlayershasmoreca-
thesametype. Forexample,‚Äúman‚Äùisanounbutislessin- pacity(parameters)todistinguishthesehardnegativesfrom
formativethan‚Äúgymnast‚Äù. Toreflectthis,wefurtherassign positive ones, our sampling strategy naturally prompts the
different words with different weights by computing their cooperationbetweenthethreecontrastivelosses.
inversedocumentfrequency(idf)[22]. Ahigheridfmeans Finally, wepresentacomprehensivecomparisontodif-
it is more unique across the corpus, and hence will weigh ferentiateourmodelwithpreviousworkswithrespecttothe
morewhencomputingthetoken-levelcontrastiveloss. An- usedcontrastivelearningmethodinTable1.
otherpracticalissueforcomputingthelossisthatthetokens
3.4.Objective
are usually sub-words due to the BERT tokenizer. Hence,
foralltokensthatbelongstothesameword,wewillassign The training objective in our method is finding optimal
thesameweightsaccordingly. Œ∏ = {Œ∏ ,Œ∏ ,Œ∏ } by minimizing the combination of the
v t m
After computing the token-aware contrastive loss, we abovethreecontrastivelosses:
feed the features from separate modalities to multi-modal N
(cid:88)
fusionlayerstoenablemoreinteractionsbetweenthemtwo. arg min (L 1+Œª tL 2+L 3) (6)
Similar to previous work [60], we take the feature corre-
Œ∏v,Œ∏t,Œ∏mi=1
spondingtothe‚Äú[CLS]‚Äùinthe(m+n)outputs. Weregard 1forclarity,weomitthebiastermintheformula
4
Method Token-aware Earlystage Laterstage Cascade Loss 4.2.Settings
VideoBert[41] (cid:55) (cid:55) (cid:51) (cid:55) BCE Previous work use a variety of different video and lan-
CBT[40] (cid:55) (cid:55) (cid:51) (cid:55) NCE
guagerepresentationswhichwefindsignificantlyaffectthe
TJVE[35] (cid:55) (cid:51) (cid:55) (cid:55) Margin
MIL-NCE[34] (cid:55) (cid:51) (cid:55) (cid:55) NCE finalperformance. Wesummarizedifferentchoicesbelow:
ActBert[60] (cid:55) (cid:55) (cid:51) (cid:55) BCE ‚Ä¢ Video representations. For 2D CNN, Resnet-152 [18]
UniVL[33] (cid:55) (cid:51) (cid:51) (cid:55) NCE
is used to extract feature map and then globally pooled to
MMT[14] (cid:55) (cid:51) (cid:55) (cid:55) Margin
TACo(Ours) (cid:51) (cid:51) (cid:51) (cid:51) NCE 2048-d [35, 33]. For 3D features, commonly used mod-
els are I3D [5], R(2+1)D [46] and S3D [51]. In [60],
Table1:Acomparisonofvideo-languagepretrainingmeth-
the authors further extract objects from the video clips.
odsregardingcontrastivelearningstrategies. ‚ÄúEarlystage‚Äù
In[31,14], theauthorsusecollaborativeexpertstoextract
and‚ÄúLaterstage‚Äùmeancomputingthelossbeforeandafter
featuresfromaudio,scene,OCR,face,speech,etc.
multi-modal fusion, respectively. ‚ÄúCascade‚Äù means using
‚Ä¢Languagerepresentations.Thereareprimarilyfourvari-
cascadehardnegativesampling.
ants: 1)GoogleNewspretrainedword2vec(w2v)[36]used
in [31, 35, 34]; 2) LSTM or Bidirectional LSTM [19];
whereŒª istheweightoftoken-levelloss(0.5bydefault). 3) pretrained BERT [10] used in [41, 60, 33, 14] and 4)
t
Duringinference, wemakethepredictionbysummingthe OpenAI-GPT[38]usedin[31].
alignmentscoresfromallthethreescoringfunctions. Inthispaper,weuseapretrainedBERT-basemodelfor
language representation as in [60, 33]. For video features,
following [35, 34, 33], we extract 2D CNN features using
4.Experimentalsetup
Resnet-152 (R-152) pretrained on ImageNet [9]. For 3D
CNN features, we use I3D (with Resnext-101 backbone)
4.1.Datasets
pretrainedonKinetics-400[23]andS3D[51]pretrainedon
Inourexperiments, wetrainandevaluateourmodelon Howto100M[34]. Theoff-the-shelfpretrainedweightsare
thefollowingestablishedbenchmarks: providedby[16]and[34]. Forsimplicity,wedenotethem
byI3D-X101andS3D-HMinthefollowing.
‚Ä¢YouCook2[58]consistsof2kvideosaboutroutinecook-
Another discrepancy among different methods is the
ing activities of 89 recipes. Each video contains multiple
numberofself-attentionlayersusedinthemodel. In[60],
videoclipsannotatedwithtextdescriptionsbyhumananno-
the authors use 12 multi-modal self-attention layers while
tators. Following[35,34],wetrainourmodelsonthetrain-
6videoencoderlayersand2multi-modalfusionlayersare
ingsplit,andreportthetext-videoretrievalperformanceon
usedin[33].Differently,4multi-modalself-attentionlayers
around3.5kvalidationclips.
are used in [14]. In this paper, for all our ablation studies
‚Ä¢MSR-VTT[52]contains10kvideoclipsassociatedwith
below, we use 1 and 2 self-attention layers for our video
200ksentences. Therearetwovalidationsplitsusedinpre-
encoderandmulti-modalfusion, respectively. Tocompare
viouswork.In[31,14],thetrainingsethas9kclip-textpairs
with previous work on specific dataset, we use 2 video
withtheremaining1kpairsforevaluation,whichwedenote
encoding layers. While pretraining the model with large-
bysplit1.In[53,35,34],1kclip-textpairsaresampledfrom
scaledatasetHowto100M[35], weincreaseto4videoen-
the3kpairsintestsetforevaluation,whiletheoriginal7k
coding layers for comparable model capacity to previous
pairs are used for training. We denote this by split2. We
works [60, 33, 14]. Note that this largest model is still
reporttext-videoretrievalresultsusingbothsplits.
smallerthanoronparwiththeaforementionedmethods.
‚Ä¢ActivityNet[25].Itconsistsof20KYouTubevideos,each
ofwhichisassociatedwithmultiplehuman-annotatedcap- 4.3.Implementationdetails
tions. Following [56, 14], we concatenate all the captions
ForYouCook2andMSR-VTT,themaximumnumberof
for a video into a paragraph and evaluate the paragraph-
videoandtexttokensaresetto48and30,respectively. For
videoretrievalonthe‚Äúval1‚Äùsplit.
paragraph-videoretrievalonActivityNet,wesetthemboth
‚Ä¢Howto100M[35]. Wecomparewithpreviousworkunder to 256. The 2D R-152 feature is extracted for one frame
thepretrainingprotocolonHowto100M[35,34,60,33]. It per second, and then globally pooled to 2048-d. For 3D
was collected from YouTube and contains over 1.2M nar- CNNfeatures,wefollow[35]tosamplevideoframesat24
rated videos associated with automatically generated tran- fpsandextractanI3D-X101featureevery16frames. This
scripts. Eachvideocontainsover100clipsonaverage. results in 1.5 2048-d feature per second. For Eq. 3 and 4,
Tofurtherverifythetransferrabilityorourlearnedmulti- wesetthetemperaturesœÑ andœÑ bothequalto1.
1 2
modal representation from Howto100M, we also evalu- Training on separate datasets. In this setting, we train
atetheactionsteplocalizationandactionsegmentationon models from scratch using the training set provided in
CrossTask[61]andCOIN[43],respectively. YouCook2,MSR-VTTandActivityNetseparately.Wetrain
5
YouCook2 MSR-VTT(split1) YouCook2 MSR-VTT(split1)
VideoRepresentation R1‚ÜëR5‚ÜëR10‚ÜëMR‚Üì R1‚Üë R5‚Üë R10‚ÜëMR‚Üì Losses Cascade R1‚Üë R5‚ÜëR10‚ÜëMR‚Üì R1‚Üë R5‚ÜëR10‚ÜëMR‚Üì
R-152,Baseline 4.1 13.2 19.4 81.0 16.4 42.6 55.8 8.0 L n/a 14.135.7 48.8 11.0 22.949.7 61.7 6.0
1
R-152,Ours 4.6 14.1 20.4 71.0 18.9 46.2 58.8 7.0
L n/a 13.335.8 48.9 11.0 21.448.1 61.5 6.0
3
I3D-X101,Baseline 2.1 8.1 12.7 125.0 14.740.83 53.2 9.0 L +L (cid:55) 13.937.4 50.7 10.0 22.550.8 64.1 5.0
1 3
I3D-X101,Ours 2.6 8.9 13.2 115.0 20.6 44.0 56.9 7.0 L +L (cid:51) 15.038.7 51.3 10.0 23.751.3 63.9 5.0
1 3
R-152+I3D-X101,Baseline 4.2 13.5 20.0 75.0 16.6 45.4 58.5 7.0 L 1+L 2+L 3 (cid:51) 15.839.8 52.4 10.0 24.552.8 65.5 5.0
R-152+I3D-X101,Ours 4.7 14.3 21.9 68.0 23.1 50.5 64.0 5.0
Table 3: Text-video retrieval performance with different
S3D-HM,Baseline 13.837.2 51.1 10.0 18.7 47.2 62.2 6.0
techniqueensembles. Itshowsthatusingourproposedtwo
S3D-HM,Ours 16.140.3 52.2 9.0 23.9 51.4 65.0 5.0
techniques produce best results. All experiments use R-
R-152+S3D-HM,Baseline 13.335.8 48.9 11.0 21.4 48.1 61.5 6.0
152+S3D-HMvideofeatures.
R-152+S3D-HM,Ours 15.839.8 52.4 10.0 24.5 52.8 65.5 5.0
Table 2: Text-video retrieval performance on YouCook2
YouCook2 MSR-VTT(split1)
andMSR-VTTwithdifferentfeaturetypes. S3Dpretrained
onHowTo100Moutperformsotherswithlargemargin. TokenofInterest R1‚Üë R5‚Üë R10‚ÜëMR‚Üì R1‚Üë R5‚Üë R10‚ÜëMR‚Üì
None 15.038.7 51.3 10.0 23.751.3 63.9 5.0
det+adp 14.738.5 51.2 10.0 23.351.0 63.5 5.0
themodelfor30kiterationswithbatchsize128. Foreach noun 15.439.3 51.8 10.0 24.051.8 65.1 5.0
training sample, we use our cascade sampling strategy to verb 15.339.0 51.4 10.0 23.952.1 64.8 5.0
sample 8 hard negatives. We use Adam [24] as the opti- noun+verb 15.839.8 52.4 10.0 24.552.8 65.5 5.0
mizerwithinitiallearningrate1e‚àí4. Alinearlearningrate Table4: Text-videoretrievalperformancewithdifferentto-
decay is applied after 5k warm-up iterations. The weight kensofinterestforcomputingtoken-levelcontrastiveloss.
decayissetto1e‚àí5.
‚Äúdet‚Äù means determiner; ‚Äúadp‚Äù means adposition. We use
Pretraining and finetuning. We pretrain our model on thesamevideofeaturesasinTable3.
Howto100M[35]. Sincetheoriginalannotatedvideoclips
in Howto100M are usually short with a few seconds, we
contrastive learning method has been adopted in a number
mergetheadjacentclipssothattheresultedtexthasatleast
ofpreviousworks[60,33]. Thiscomparisoncanverifythe
10 words. We use Adam [24] as the optimizer with ini-
tial learning rate 1e‚àí4. We train the model for 500k itera- effectiveness of our proposed contrastive learning method
considering two models have exactly the same number of
tions with batch size 64, and also sample 8 hard negatives
parameters. In Table 2, we can see our proposed method
foreachsampleusingourcascadesamplingstrategy. After
outperformsbaselineacrossallfeaturetypes introducedin
pretraining, wefinetunethepretrainedmodelsondifferent
Sec. 4.2 on both YouCook2 and MSR-VTT. Note that our
datasetsusingthesamesettingasaboveexceptforalower
initiallearningrate2e‚àí5andlessfinetuningiterations20k. model uses exactly the same number of parameters to the
baseline model. These consistent improvements demon-
Evaluation metrics. For text-video retrieval, we use Re-
stratetheeffectivenessandgeneralizationabilityofourpro-
calls at different points (Recall@n or Rn, with n as a spe-
posed method. As mentioned above, we also observe the
cificnumber)andMedianRank(MR)asthemetricsfollow-
text-video retrieval performance significantly depends on
ingpreviousworks[60,33]. Inalltables,weuse‚Üëor‚Üìto
thefeaturetypes. Wecanfind3Dfeatures(I3D-X101and
indicatehigherorlowerisbetter,respectively.
S3D-HM)ingeneraloutperform2Dfeature(R-152),which
5.Results isexpectedsince2Dfeaturedoesnotcapturethemotionsin
thevideos. Amongallthreefeaturetypes,S3D-HMoutper-
We first evaluate text-video retrieval performance and
formstheothertwowithlargemargin,whichdemonstrates
thenstudywhetherthelearnedrepresentationscanbetrans-
thepotentialtolearngoodvideorepresentationbypretrain-
ferredtoothertasksonCrossTaskandCOIN.
ing on large-scale noisy dataset (Howto100M [35]). Be-
cause Howto100M mainly contains instructional videos, it
5.1.Text-videoretrieval
ismoreclosetoYouCook2thanMSR-VTT,andhencewe
5.1.1 Comparingwithbaselines
see more gain on YouCook2. These comparisons indicate
Wefirstshowthecomparisonswithbaselinestoinspectthe videorepresentationsmattermuchtothefinalperformance.
effectsofdifferentcomponentsinourmodel. ComponentAnalysis. Inourmethod,wecombineL ,L ,
1 2
Video representations. We train our model with differ- andL duringtrainingandinference. Here, westudyhow
3
entvideorepresentationsasdescribedaboveandcompareit they perform separately and contribute to the final perfor-
withthebaselinemodelwhichhasidenticalarchitecturebut mance. In Table 2, we use R-152+S3D-HM as the video
merely trained with L as depicted in Eq. 5. The baseline feature and report the results with different loss combina-
3
6
YouCook2
Model Lang. Video
R1‚Üë R5‚Üë R10‚Üë MR‚Üì
Random ‚Äì ‚Äì 0.0 0.2 0.3 1675
TVJE[35] w2v R-152+I3D-X101 4.2 13.7 21.5 65
UniVL(v1)[33] BERT R-152+I3D-X101 3.4 10.8 17.8 76
TACo(Ours) BERT R-152+I3D-X101 4.9 14.7 21.7 63
UniVL(v3)[33] BERT S3D-HM 7.7 23.9 34.7 21
TACo(Ours) BERT S3D-HM 16.6 40.3 53.1 9.0
Table5: Comparingtext-videoretrievalonYouCook2.
MSR-VTT
Model Lang. Video
R1‚Üë R5‚Üë R10‚Üë MR‚Üì
Random ‚Äì ‚Äì 0.1 0.5 1.0 500.0
JSFusion[53] BiLSTM R-152 10.2 31.2 43.2 13.0
JPoSE[49] w2v TSN+Flow 14.3 38.1 53.0 9.0
TVJE[35] w2v R-152+I-101 12.1 35.0 48.0 12.0
UniVL(v1)‚àó[33] BERT R-152+I-101 14.6 39.0 52.6 10.0
TACo(Ours) BERT R-152+I-101 19.2 44.7 57.2 7.0
CE[31] GPT CollaborativeExperts 20.9 48.8 62.4 6.0 Figure2: Zero-shotperformanceonYouCook2andMSR-
MMT[14] BERT CollaborativeExperts 24.6 54.0 67.1 4.0 VTTfordifferentsettings.score-1-5correspondtothefive
TACo(Ours) BERT R-152+S3D-HM 26.7 54.5 68.2 4.0
settingsinTable3fromtoptobottom.
Table6: Comparingtext-videoretrievalonMSR-VTT.The
upperblockandbottomblockusesplit2andsplit1,respec- with‚Äúdet+adp‚Äù,‚Äúnoun‚Äùand‚Äúverb‚Äùandreportthenumbers
tively. Wereportthemseparatelyforfaircomparison. on two text-video retrieval datasets. As we can see, using
‚Äúdet+adp‚Äù as the target tokens is worse than the baseline
withoutanytoken-levelcontrastiveloss.‚Äúnoun‚Äùand‚Äúverb‚Äù
ActivityNet
Model Lang. Video canbothimprovetheperformancewhile‚Äúnoun‚Äùisslightly
R1‚Üë R5‚Üë R10‚Üë MR‚Üì
better than ‚Äúverb‚Äù. Finally, combining noun and verb to-
Random - - 0.02 0.1 1.02 2458
gether achieves the best performance. These results align
DenseCap[25] LSTM C3D 14.0 32.0 65.0 34
FSE[56] GRU C3D+TSN-Inception 18.2 44.8 89.1 7.0 withourintuitiontousenounsandverbsasthetargettoken
CE[31] GPT CollaborativeExperts 18.2 47.7 91.4 6.0 forfine-grainedalignmentbetweentextsandvideosconsid-
MMT[14] BERT CollaborativeExperts 22.7 54.2 93.2 5.0 eringtheyareusuallygroundedtovideocontents.
TACo(Ours) BERT R-152+S3D-HM 25.8 56.3 93.8 4.0
Table7: Comparingtext-videoretrievalonActivityNet.
5.1.2 Comparingwithstate-of-the-art
Wecomparewithpreviousworksunderthreeprotocols: 1)
tions. As we can see, solely using L 1 (row 1) or L 2 (row trainingandevaluatingonseparatedatasets; 2)pretraining
2)forcontrastivelearningresultsinsub-optimalvideo-text on Howto100M and evaluating zero-shot performance and
alignment. Simply combining them together (row 3) im- 3)finetuningpretrainedmodelonseparatedatasets.
proves the performance on two datasets. This implies that Results on separate datasets. We separately show the
different levels of contrastive learning can be complemen- comparisonson YouCook2, MSR-VTT andActivityNet in
tarytoeachother,whichsupportsourearlierhypothesisthat Table5,6and7.Forafaircomparisonwithpreviousworks,
thesetwolossesaresynergisticwitheachotherforabetter we use the same or similar features as listed in the tables.
video-textalignment.Whenincorporatingthehardnegative As we can see, our method outperforms all previous work
mining via our cascade sampling strategy (row 4), it fur- acrossalldatasets. Theseresultsvalidatesitseffectiveness
therimprovestheperformance. Finally,wecanseeadding tolearnvideo-textalignment. Notethatpreviousworksei-
token-levelcontrastivelossL 3canfurtherimprovetheper- theruseavarietyoflossfunctions[33,28]oracollectionof
formanceacrossallsettings(row5). multiplefeatures[31,14]. Incontrast, weachievethebest
TokensofInterest. Wefurtherstudytheeffectofdifferent performance using a simpler contrastive learning pipeline
tokens of interest on the model performance. By default, with smaller model size. This supports our earlier claim
our model uses the noun and verb as the tokens of inter- on the efficiency. Comparing the numbers in Table 2, Ta-
esttocomputethetoken-levelcontrastloss. Here,wevary ble5andTable6,wecanfindourmodelachievesbetterper-
themtoothertypessuchasadposition(adp)anddeterminer formance with the same video features when using deeper
(det)forinvestigation. InTable4,wereplace‚Äúnoun+verb‚Äù videoencoder(2layersv.s. 1layer).
7
YouCook2 MSR-VTT ActivityNet Method CrossTask COIN
Model Video
R1‚Üë R5‚Üë R10‚Üë MR‚Üì R1‚Üë R5‚Üë R10‚Üë MR‚Üì R1‚Üë R5‚Üë R50‚Üë MR‚Üì Alayracetal.[1] 13.3 ‚Äì
TJVE[35] R-152+I-101 6.1 17.3 24.8 46.0 7.5 21.2 29.6 38.0 ‚Äì ‚Äì ‚Äì ‚Äì Zhukovetal.[61] 22.4 ‚Äì
ActBERT[60] O-101+R(2+1)D 9.6 26.7 38.0 19.0 8.6 23.4 33.1 36.0 ‚Äì ‚Äì ‚Äì ‚Äì Supervised[61] 31.6 ‚Äì
MIL-NCE[34] S3D-HM 15.1 38.0 51.2 10.0 9.9 24.0 32.4 29.5 ‚Äì ‚Äì ‚Äì ‚Äì NN-Viterbi[39] ‚Äì 21.2
TACo(Ours) S3D-HM 19.9 43.2 55.7 8.0 9.8 25.0 33.4 29.0 ‚Äì ‚Äì ‚Äì ‚Äì CBT[40] ‚Äì 53.9
TVJE[35] 33.6 ‚Äì
TJVE[35] R-152+I3D-X101 8.2 24.5 35.3 24.0 14.9 40.2 52.8 9.0 ‚Äì ‚Äì ‚Äì ‚Äì
UniVL(v3)[33] S3D-HM 28.9 57.6 70.0 4.0 21.2 49.6 63.1 6.0 ‚Äì ‚Äì ‚Äì ‚Äì MIL-NCE[34] 40.5 61.0
TACo(Ours) S3D-HM 29.6 59.7 72.7 4.0 24.8 52.1 64.0 5.0 28.3 56.8 92.6 4.0 ActBert[60] 41.4 57.0
UniVL(v3)[33] 42.0 70.0
MMT[14] CollaborativeExperts ‚Äì ‚Äì ‚Äì ‚Äì 26.6 57.1 69.6 4.0 28.7 61.4 94.5 3.3
TACo(Ours) R-152+S3D-HM 27.3 56.5 68.8 4.0 28.4 57.8 71.2 4.0 30.4 61.2 93.4 3.0 TACo(Ours) 42.5 68.4
Table 8: A complete comparison of TACo under zero-shot and finetuning evaluation Table 9: Action step localization
protocols. Note that the zero-shot and upper part of finetuned performance for MSR- onCrossTask(avg. recall)andac-
VTTisonsplit2,whilethebottomisonsplit1forfaircomparison. tionsegmentationonCOIN(acc.).
Zero-shot and finetuned performance. In Table 8, we 5.2.Othervideo-relatedtasks
showthecomparisonsacrossdifferentmodelspretrainedon
Following[35,60,33],weevaluateactionsteplocaliza-
Howto100M.Intheupperpartofthetable,wecomparethe
tion performance on CrossTask dataset [61]. It covers 18
zero-shotperformanceonYouCook2andMSR-VTT.Wedo
tasksandeachvideocontainsmultiplevideosegmentsan-
notevaluateonActivityNetsinceithasdifferentnumberof
notatedwithactionstepsandnaturallanguagedescriptions.
inputvideotokenscomparedwiththepretrainedmodeland
Similar to [35, 60, 33], we use our model to compute the
thusisnotdirectlycompatibletothepretrainedmodel. As
similarity between each frame and the action step descrip-
wecansee,TACooutperformspreviousworkssignificantly
tions, which results in a score matrix. Using the official
onYouCook2andslightlyonMSR-VTT.SinceYouCook2
algorithmprovidedby[61],wecanfindtheoptimalframe-
hascloserdomaingaptoHowto100MthanMSR-VTT,the
wise order of action steps for a video. By comparing it
improvementbroughtbylarge-scalepretrainingismoresig-
withtheground-truthannotations,wecomputetherecallfor
nificant. However, on MSR-VTT, our model still outper-
eachtaskandthendotheaverage. Accordingtotheresults
forms MIL-NCE [34] which uses the same video features.
in Table 9, our model achieves the best performance com-
InFig.2,weshowthezero-shotperformanceonYouCook2
pared with previous works. This indicates that our model
and MSR-VTT when pretraining our models with differ-
canlearngoodvideo-languagerepresentations.
ent contrastive losses as listed in Table 3. Accordingly, it
Wefurtherevaluateourpretrainedmodelonactionseg-
shows our proposed contrastive losses gradually improve
mentationtaskonCOINdataset,following[34,60]. Unlike
theperformance,andcombiningalltechniquesachievesthe
theabovetask, actionsegmentationdoesnotrelyontexts,
best performance. Based on the pretrained model, we fur-
and thus can be used to evaluate the learned video repre-
therfinetuneitonspecificdatasets. Inourexperiments,we
sentation. As shown in Table 9, our method significantly
usetwofeatureS3D-HMandR-152+S3D-HM,tocompare
outperforms MIL-NCE and ActBert, and achieves compa-
withthemethodswiththesame/similarsettings. Aswecan
rableperformancetoUniVL.Thisindicatesthatourmodel
see, our model using S3D-HM outperforms UniVL [33]
isalsoagoodvideorepresentationlearner.
using the same feature but more video encoder layers (6).
Differentfromzero-shotresults,weobservemoreimprove- 6.Conclusion
ment on MSR-VTT than YouCook2 after finetuning. This
In this paper, we introduced TACo, a simple yet ef-
impliesthatfinetuningonspecificdatasetscancompensate
fective contrastive learning method for learning video-text
thedomaingaptothepretrainingdatasets.Tocomparewith
alignment. It is aimed at addressing two existing issues
themethodsusingfeaturesextractedfromcollaborativeex-
in current contrastive learning pipelines: missing fine-
perts [14], we enrich our video representation by adding
grainedalignmentandinefficientsamplingformulti-modal
2D R-152 feature, which achieves better performance on
fusion. Without introducing any extra parameters, our
MSR-VTT,andbetterRecall@1andMedianRankonAc-
method achieved promising results on three text-video re-
tivityNet. Notethatthiscombinationhurtstheperformance
trievalbenchmarksundervariousevaluationprotocols. We
onYouCook2,andwewitnessedasimilartrendformodels
furtherdemonstratedthelearnedrepresentationscanbeef-
withoutpretraininginTable2. Finally,comparingwiththe
fectivelytransferredtoothertaskssuchasactionsteplocal-
results without pretraining in Table 5, 6 and 7, we clearly
ization and segmentation. Based on all these encouraging
find large-scale pretraining and finetuning brings substan-
results, we believe TACo is a good alternative to conven-
tialimprovementsconsistently.
tionalcontrastivelearningpipeline.
8
tohs-oreZ
denuteniF
References [13] FartashFaghri,DavidJFleet,JamieRyanKiros,andSanja
Fidler. Vse++:Improvingvisual-semanticembeddingswith
[1] Jean-BaptisteAlayrac,PiotrBojanowski,NishantAgrawal,
hardnegatives. arXivpreprintarXiv:1707.05612,2017. 2
JosefSivic, IvanLaptev, andSimonLacoste-Julien. Unsu-
[14] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia
pervisedlearningfromnarratedinstructionvideos. InPro-
Schmid. Multi-modaltransformerforvideoretrieval. InEu-
ceedings of the IEEE Conference on Computer Vision and
ropeanConferenceonComputerVision(ECCV),volume5.
PatternRecognition,pages4575‚Äì4583,2016. 8
Springer,2020. 1,2,3,5,7,8,12
[2] Srikar Appalaraju and Vineet Chaoji. Image similarity
[15] Michael Gutmann and Aapo Hyva¬®rinen. Noise-contrastive
using deep cnn and curriculum learning. arXiv preprint
estimation: A new estimation principle for unnormalized
arXiv:1709.08761,2017. 2
statistical models. In Proceedings of the Thirteenth Inter-
[3] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob An-
nationalConferenceonArtificialIntelligenceandStatistics,
dreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Ange-
pages297‚Äì304,2010. 3
likiLazaridou,JonathanMay,AleksandrNisnevich,Nicolas
[16] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can
Pinto,andJosephTurian. Experiencegroundslanguage. In
spatiotemporal 3d cnns retrace the history of 2d cnns and
Proceedingsofthe2020ConferenceonEmpiricalMethods
imagenet? InProceedingsoftheIEEEConferenceonCom-
inNaturalLanguageProcessing(EMNLP),2020. 1
puterVisionandPatternRecognition(CVPR),pages6546‚Äì
[4] Joao Carreira and Andrew Zisserman. Quo Vadis, Action
6555,2018. 5
Recognition? A New Model and the Kinetics Dataset. In
[17] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRoss
IEEEConferenceonComputerVisionandPatternRecogni-
Girshick. Momentumcontrastforunsupervisedvisualrep-
tion(CVPR),052017. 3
resentationlearning. InProceedingsoftheIEEE/CVFCon-
[5] Joao Carreira and Andrew Zisserman. Quo vadis, action
ferenceonComputerVisionandPatternRecognition,pages
recognition? anewmodelandthekineticsdataset. Inpro-
9729‚Äì9738,2020. 3
ceedings of the IEEE Conference on Computer Vision and
[18] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
PatternRecognition,pages6299‚Äì6308,2017. 5
Deepresiduallearningforimagerecognition. 122015. 3,5
[6] ShizheChen,YidaZhao,QinJin,andQiWu. Fine-grained
[19] SeppHochreiterandJu¬®rgenSchmidhuber. Longshort-term
video-text retrieval with hierarchical graph reasoning. In
memory. Neuralcomputation,9(8):1735‚Äì1780,1997. 5
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,pages10638‚Äì10647,2020. 2 [20] MatthewHonnibalandInesMontani. spaCy2:Naturallan-
guageunderstandingwithBloomembeddings,convolutional
[7] TingChen,SimonKornblith,MohammadNorouzi,andGe-
neuralnetworksandincrementalparsing. Toappear,2017.
offreyHinton. Asimpleframeworkforcontrastivelearning
11
ofvisualrepresentations. arXivpreprintarXiv:2002.05709,
2020. 3 [21] YunseokJang,YaleSong,YoungjaeYu,YoungjinKim,and
GunheeKim. Tgif-qa:Towardspatio-temporalreasoningin
[8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
visualquestionanswering. InProceedingsoftheIEEECon-
FaisalAhmed,ZheGan,YuCheng,andJingjingLiu.Uniter:
ferenceonComputerVisionandPatternRecognition,pages
Universal image-text representation learning. In European
2758‚Äì2766,2017. 2
Conference on Computer Vision, pages 104‚Äì120. Springer,
2020. 2 [22] Karen Spa¬®rck Jones. A statistical interpretation of term
specificity and its application in retrieval. Journal of Doc-
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
umentation,28:11‚Äì21,1972. 4
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage
database. In2009IEEEconferenceoncomputervisionand [23] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
patternrecognition,pages248‚Äì255.Ieee,2009. 5 Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,
Toutanova.BERT:Pre-trainingofDeepBidirectionalTrans- andAndrewZisserman. TheKineticsHumanActionVideo
formersforLanguageUnderstanding. InProceedingsofthe Dataset. arXiv:1705.06950,052017. 5
2019ConferenceoftheNorthAmericanChapteroftheAs- [24] Diederik P Kingma and Jimmy Ba. Adam: A method for
sociationforComputationalLinguistics: HumanLanguage stochastic optimization. arXiv preprint arXiv:1412.6980,
Technologies, Volume1(LongandShortPapers), 102019. 2014. 6
1,2,3,5 [25] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
[11] Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan JuanCarlosNiebles. Dense-captioningeventsinvideos. In
He, Gang Yang, and Xun Wang. Dual encoding for zero- Proceedings of the IEEE international conference on com-
example video retrieval. In Proceedings of the IEEE Con- putervision,pages706‚Äì715,2017. 5,7,11
ferenceonComputerVisionandPatternRecognition,pages [26] Kuang-HueiLee,XiChen,GangHua,HoudongHu,andXi-
9346‚Äì9355,2019. 2 aodongHe.Stackedcrossattentionforimage-textmatching.
[12] BernardGhanemFabianCabaHeilbron,VictorEscorciaand InProceedingsoftheEuropeanConferenceonComputerVi-
JuanCarlosNiebles.Activitynet:Alarge-scalevideobench- sion(ECCV),pages201‚Äì216,2018. 2
mark for human activity understanding. In Proceedings [27] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.
of the IEEE Conference on Computer Vision and Pattern Tvqa: Localized, compositional video question answering.
Recognition,pages961‚Äì970,2015. 2 arXivpreprintarXiv:1809.01696,2018. 1,2
9
[28] LinjieLi,Yen-ChunChen,YuCheng,ZheGan,LichengYu, [41] C.Sun,A.Myers,C.Vondrick,K.Murphy,andC.Schmid.
and Jingjing Liu. Hero: Hierarchical encoder for video+ Videobert:Ajointmodelforvideoandlanguagerepresenta-
language omni-representation pre-training. arXiv preprint tionlearning. In2019IEEE/CVFInternationalConference
arXiv:2005.00200,2020. 1,2,3,7 onComputerVision(ICCV),pages7463‚Äì7472,2019.1,2,5
[29] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan [42] Hao Tan and Mohit Bansal. LXMERT: Learning Cross-
Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Modality Encoder Representations from Transformers. In
Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object- EMNLP,082019. 2
semantics aligned pre-training for vision-language tasks. [43] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,
arXivpreprintarXiv:2004.06165,2020. 2 DanyangZhang,LiliZhao,JiwenLu,andJieZhou. Coin:
[30] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, Alarge-scaledatasetforcomprehensiveinstructionalvideo
PietroPerona,DevaRamanan,PiotrDolla¬¥r,andCLawrence analysis. InProceedingsoftheIEEEConferenceonCom-
Zitnick. Microsoft coco: Common objects in context. In puter Vision and Pattern Recognition, pages 1207‚Äì1216,
European conference on computer vision, pages 740‚Äì755. 2019. 1,2,5
Springer,2014. 2 [44] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,
[31] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Antonio Torralba, Raquel Urtasun, and Sanja Fidler.
Zisserman. Use what you have: Video retrieval using Movieqa:Understandingstoriesinmoviesthroughquestion-
representations from collaborative experts. arXiv preprint answering. In Proceedings of the IEEE conference on
arXiv:1907.13487,2019. 2,5,7 computervisionandpatternrecognition,pages4631‚Äì4640,
2016. 1
[32] JiasenLu,DhruvBatra,DeviParikh,andStefanLee. ViL-
[45] Atousa Torabi, Niket Tandon, and Leonid Sigal. Learning
BERT:PretrainingTask-AgnosticVisiolinguisticRepresen-
tationsforVision-and-LanguageTasks. In11pages, 5fig- language-visual embedding for movie understanding with
ures,082019. 2 natural-language. arXiv preprint arXiv:1609.08124, 2016.
2
[33] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan
[46] DuTran,HengWang,LorenzoTorresani,JamieRay,Yann
Duan, Tianrui Li, Xilin Chen, and Ming Zhou. Univilm:
LeCun,andManoharPaluri.Acloserlookatspatiotemporal
A unified video and language pre-training model for mul-
convolutions for action recognition. In Proceedings of the
timodal understanding and generation. arXiv:2002.06353,
IEEEconferenceonComputerVisionandPatternRecogni-
2020. 1,2,3,5,6,7,8,12
tion,pages6450‚Äì6459,2018. 5
[34] A.Miech,J.B.Alayrac,L.Smaira,I.Laptev,J.Sivic,and
[47] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
A.Zisserman. End-to-endlearningofvisualrepresentations
reit,LlionJones,AidanNGomez,LukaszKaiser,andIllia
from uncurated instructional videos. In 2020 IEEE/CVF
Polosukhin. Attentionisallyouneed. 062017. 3
Conference on Computer Vision and Pattern Recognition
[48] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
(CVPR),pages9876‚Äì9886,June2020. 1,2,3,5,8
mond,ClementDelangue,AnthonyMoi,PierricCistac,Tim
[35] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Rault, Re¬¥mi Louf, Morgan Funtowicz, Joe Davison, Sam
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Shleifer,PatrickvonPlaten,ClaraMa,YacineJernite,Julien
HowTo100M: Learning a Text-Video Embedding by
Plu,CanwenXu,TevenLeScao,SylvainGugger,Mariama
WatchingHundredMillionNarratedVideoClips. InICCV,
Drame,QuentinLhoest,andAlexanderM.Rush. Hugging-
062019. 1,2,5,6,7,8
face‚Äôs transformers: State-of-the-art natural language pro-
[36] Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jeffrey cessing. ArXiv,abs/1910.03771,2019. 3
Dean. Efficientestimationofwordrepresentationsinvector
[49] Michael Wray, Diane Larlus, Gabriela Csurka, and Dima
space.InInternationalConferenceonLearningRepresenta-
Damen.Fine-grainedactionretrievalthroughmultipleparts-
tions,2013. 5
of-speech embeddings. In Proceedings of the IEEE Inter-
[37] Andriy Mnih and Yee Whye Teh. A fast and simple al- national Conference on Computer Vision, pages 450‚Äì459,
gorithm for training neural probabilistic language models. 2019. 2,7
arXivpreprintarXiv:1206.6426,2012. 3
[50] Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei
[38] AlecRadford,KarthikNarasimhan,TimSalimans,andIlya Li,WeiweiSun,andWei-YingMa. Unifiedvisual-semantic
Sutskever. Improvinglanguageunderstandingbygenerative embeddings: Bridging vision and language with structured
pre-training,2018. 5 meaningrepresentations. InProceedingsoftheIEEECon-
[39] Alexander Richard, Hilde Kuehne, Ahsan Iqbal, and Juer- ferenceonComputerVisionandPatternRecognition,pages
genGall. Neuralnetwork-viterbi: Aframeworkforweakly 6609‚Äì6618,2019. 2
supervisedvideolearning. InProceedingsoftheIEEECon- [51] SainingXie,ChenSun,JonathanHuang,ZhuowenTu,and
ferenceonComputerVisionandPatternRecognition,pages KevinMurphy. Rethinkingspatiotemporalfeaturelearning:
7386‚Äì7395,2018. 8 Speed-accuracy trade-offs in video classification. In Pro-
[40] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia ceedings of the European Conference on Computer Vision
Schmid. Learning video representations using contrastive (ECCV),pages305‚Äì321,2018. 3,5
bidirectionaltransformer. arXivpreprintarXiv:1906.05743, [52] JunXu,TaoMei,TingYao,andYongRui. Msr-vtt:Alarge
2019. 1,5,8 video description dataset for bridging video and language.
10
In2016IEEEConferenceonComputerVisionandPattern where D is the full set of corpus, which are the captions
Recognition(CVPR),pages5288‚Äì5296.IEEE,2016.1,2,5, inthetrainingsetforadataset;thedenominatorcountsthe
11 number of captions which contain a specific token. Based
[53] YoungjaeYu,JongseokKim,andGunheeKim. Ajointse- on Eq. 7, we can compute the idf for each token of inter-
quence fusion model for video question answering and re- est. Thesmallertheidf,themorefrequentitappearsinthe
trieval.InProceedingsoftheEuropeanConferenceonCom-
corpus. We do not compute the tf term since usually a to-
puterVision(ECCV),pages471‚Äì487,2018. 2,5,7
kenonlyappearsonceinasinglesentence. Thefulllistof
[54] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee
tokens and corresponding idfs can be found in Fig. 4. For
Kim. Videocaptioningandretrievalmodelswithsemantic
a given sentence, we first assign the computed idfs to its
attention. arXivpreprintarXiv:1610.02947,6(7),2016. 2
nounsandverbsandthennormalizetheidfs,whicharethen
[55] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee
usedtoweighthetoken-levelcontrastivelosses.
Kim. End-to-endconceptworddetectionforvideocaption-
ing,retrieval,andquestionanswering. InProceedingsofthe
B.Contributionofthreecontrastivelosses
IEEEConferenceonComputerVisionandPatternRecogni-
tion,pages3165‚Äì3173,2017. 2
[56] BowenZhang,HexiangHu,andFeiSha. Cross-modaland Loss R@1 R@5 R@10 MR
hierarchical modeling of video and text. In Proceedings
Earlystageonly 14.1 35.7 48.8 11.0
of the European Conference on Computer Vision (ECCV),
Laterstageonly 13.3 35.8 48.9 11.0
pages374‚Äì390,2018. 2,5,7
[57] LuoweiZhou,HamidPalangi,LeiZhang,HoudongHu,Ja- Earlystage 15.3 39.3 51.9 10.0
sonJ.Corso,andJianfengGao.Unifiedvision-languagepre- Token-level 15.0 39.5 51.4 11.0
trainingforimagecaptioningandvqa.InThirty-FourthAAAI Laterstage 14.3 38.4 50.6 11.0
ConferenceonArtificialIntelligence,2019. 2 Fused 15.8 39.8 52.4 10.0
[58] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards Table 11: Text-video retrieval performance using separate
automatic learning of procedures from web instructional
alignmentscoresonYouCook2.
videos. InAAAI2018,2017. 1,2,3,5,11
[59] LuoweiZhou,YingboZhou,JasonJCorso,RichardSocher, Inthispart,weinvestigatethecontributionsofthreecon-
andCaimingXiong.End-to-enddensevideocaptioningwith trastivelossesusedinourmodel. Afterwetrainthevideo-
maskedtransformer.InProceedingsoftheIEEEConference text alignment model using all three losses, we report the
on Computer Vision and Pattern Recognition, pages 8739‚Äì performance using separate alignment scores in Table 11.
8748,2018. 2 Forreference,thetoptworowsaretheperformanceforus-
[60] Linchao Zhu and Yi Yang. Actbert: Learning global-local ingearlystageonlyandlaterstageonlycontrastivelearning
video-textrepresentations. InCVPR,2020. 1,2,3,4,5,6, to train the model. The bottom four rows are the separate
8,12
performance at different stages for our model. As we can
[61] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk see, combiningthreecontrastivelossesduringtrainingcan
Cinbis,DavidFouhey,IvanLaptev,andJosefSivic. Cross-
boost the performance for both early and later stage (row
task weakly supervised learning from instructional videos.
3 v.s. row 1, row 5 v.s. row 2). This indicates that the
InProceedingsoftheIEEEConferenceonComputerVision
threelossesaresynergistictoeachotherforabettervideo-
andPatternRecognition,pages3537‚Äì3545,2019. 1,2,5,8
textalignment.Ontheotherhand,theearlystagealignment
achievesbetterperformancethanothertwo(token-leveland
A.Tokensofinterest
later stage), while the fused score is the best. We suspect
thatthisisbecauseearlystagealignmentistrainedwithall
Dataset Noun Verb All
text-video pairs at sentence-level. In contrast, token-level
YouCook2[58] 378 168 2,144 contrast focuses on single tokens and the multi-modal fu-
MSR-VTT[52] 4,415 1,463 15,740 sionlayersmerelyseeasmallpartofhardtext-videopairs.
ActivityNet[25] 2,602 1,021 9,059
C.Effectofcascadesampling
Table10: Tokenstatisticsforeachdataset.
Theproposedcascadesamplinghelpsthelaterstagecon-
Weextracttokensofinterest(T.O.I)usingthepos-tagger
trastive learning to focus on hard negative samples. As
providedbySpacy[20]. InTable10,weshowthestatistics
shown in our main submission, adding cascade sampling
of tokens for three datasets. For each token that is tagged
will improve the performance. We suspect this is because
atVERBorNOUN,wecomputetheinversedocumentfre-
cascadesamplinghelpslearnabetterlaterstagealignment.
quency(idf)by:
Toverifythis,wecomparethelaterstagealignmentacross
threedifferentsettings: 1)merelyapplyinglaterstagecon-
|D|
idf(token)=log (7) trastive loss; 2) combine early state and later stage con-
1+|{d‚ààD:token‚ààd}|
11
trastivelossesand3)usingcascadesamplingforlaterstage E.ComparingmodelsizeandFLOPs
contrastiveloss. WereporttheresultsonYouCook2inTa- Finally,weattempttocomparethemodelsizesandcom-
ble12.Here,notethatweonlyusethelaterstagealignment putational costs for different methods. Unfortunately, all
scoresforevaluatingtheperformance. Aswecansee,com- previousmethodsdidnotreportFLOPsandonlyMMT[14]
biningearlystageandlaterstagetogetherslightlyimproves discussed#params. However,theresultsinTable13imply
theperformance. Thisisprobablybecauseearlystagecon- that bigger model can usually achieve better performance.
trastive loss helps to learn a better video and language en- Therefore, it is necessary to have a comparison of model
coder,fromwhichthemulti-modalmoduletakesbetterrep- size and computational cost between our model and those
resentationsforcross-modalfusion. Afterapplyingthecas- fromothermethods. Forothermethodswhichdonotreport
cade sampling for the later stage contrastive loss, the per- thenumbers,weestimatethembasedonthedescriptionsin
formanceisfurtherimproved. Sinceourcascadesampling the original paper. Table 14 summarizes the comparisons
strategy can send more difficult samples to the later stage, and also reports the #params and FLOPs (all underlined
the cross-modal fusion layers can learn more discrimina- numbers are estimated based on the descriptions in origi-
tive representations for video-text alignment. These re- nal papers). As shown, our largest model has comparable
sultsvalidatethatthehardnegativeminingthroughcascade sizeandFLOPstoothers.
samplingindeedhelpstoimprovethelater-stagetext-video
alignment,andhencethefinalperformance. mm #params. FLOPs
method text video
(M) (G)
self cross
Setting R@1 R@5 R@10 MR
ActBert[60] 12 12 0 24 369.1 13.80
Laterstageonly 13.3 35.7 48.8 11.0 MMT[14] 12 4 0 0 133.3 4.63
Earlystage+Laterstage 13.6 35.9 49.1 11.0 UniVL[33] 12 6 2 0 169.0 5.82
Cascadesampling 14.5 38.3 50.7 11.0 Ourlargest 12 4 2 0 154.9 5.14
Table 12: Text-video retrieval performance on YouCook2
Table 14: Comparison of model size and FLOPs. ‚Äúmm‚Äù
onlyusinglaterstagealignmentscorefordifferentsettings.
meansmulti-modalfusion, and‚Äúself‚Äùmeansself-attention
layerswhile‚Äúcross‚Äùmeanscross-modalattention.
D.Effectofvideoencoderlayers
F.Visualizations
In our main paper, we noticed the number of video en- We visualize the text-video retrieval results by varying
coderlayersaffectsthefinalperformance. Tohaveamore theweightsforthetoken-levelalignmentscoresduringtest-
comprehensivestudy,weuseR-152andS3D-HMasthe2D ing. In Fig. 3, we show two text-video retrieval examples
and3Dfeaturesandtrainthevideo-textalignmentmodelon on YouCook (top) and MSR-VTT (bottom). From top to
YouCook2 with different video encoder layers. As shown bottom, the five rows in each block correspond to the top
in Table 13, using more video encoder layers can signifi- five retrieved results from the whole test set. As we can
cantly boost the text-video retrieval performance. Particu- see, when we gradually increase the weight for the token-
larly,whennovideoencoderlayersareused,themodelcan levelalignmentscore,therearemorerelatedvideosappear-
hardy capture the long-range temporal dynamics, and thus inginthetopfivecandidates. ForYouCook2,whenweset
performspoorly. Onceweaddonevideoencoderlayer,the the weight equal to 0.0, the third and fifth video are not
performance improves significantly. With the increase of well-aligned with the query since they are both not about
encoderlayers,theperformanceisfurtherimproved,which ‚Äútomato‚Äù. Whenweincreasetheweightto0.1,wecanob-
is reasonable since more video encoder layers can encode servethethefourthvideomovestothethirdplace.Afterwe
morecomplicatedvideocontentsanddynamics. increase the weight to 0.5, we can see all top-5 videos are
aboutcuttingtomato. Similarly,forMSR-VTT,wecansee
#video #params.FLOPs YouCook2 thelastthreevideosarenotabout‚Äútwopeopletalkingona
enc. layers (M) (G) R@1R@5R@10 MR table‚Äù.Whenweincreasetheweightto0.1,thefifthvideois
replacedwithamorematchedvideo. Keepingincreasethe
0 126.5 3.86 14.0 35.7 49.5 11.0
weightto0.5,wecanobtainthetop5videosallabout‚Äútwo
1 133.6 4.11 15.8 39.8 52.4 10.0
peopletalkingwitheachotheronatable‚Äù. Thesevisualiza-
2 140.7 4.45 15.9 40.5 53.8 9.0
tions demonstrate the efficacy of our proposed token-level
4 154.9 5.14 16.4 40.5 54.3 9.0
contrastivelearning.
Table 13: Text-video retrieval performance on YouCook2
withdifferentvideoencoderlayersusingR-152+S3D-HM.
12
Query: cut the tomato and put it inside a bowl
cut a tomato into quarters remove the seeds chop finely and add to the bowl cut a tomato into quarters remove the seeds chop finely and add to the bowl cut a tomato into quarters remove the seeds chop finely and add to the bowl
chop a tomato into thin slices chop a tomato into thin slices chop a tomato into thin slices
chop some red onions red pepper and green pepper into square pieces chop up the tomatoes chop up the tomatoes
chop up the tomatoes chop some red onions red pepper and green pepper into square pieces slice tomatoes into thin slices
cut the pepperoni in half cut the pepperoni in half cut tomatoes and place them in a bowl
Weight=0.0 Weight=0.1 Weight=0.5
Query: two people are talking with each other on a table
a man and a woman trying some sake a man and a woman trying some sake there is a woman is talking with two guys
there is a woman is talking with two guys there is a woman is talking with two guys a man and another man speak to each other in a room
leonardodicapriois portrayed as two different characters in this film leonardodicapriois portrayed as two different characters in this film a man and a woman trying some sake
a girl in a studio singing a girl in a studio singing two men talking about investors on a show
a cartoon man plays a card game with his friends two men talking about investors on a show a man and woman arguing about fake arms used in a performance
Weight=0.0 Weight=0.1 Weight=0.5
Figure3: Text-videoretrievalresultsgivenaqueryonYouCook2(top)andMSR-VTT(bottom). Ineachblock,weshowtop
5rankedvideosfromtoptobottom.Fromlefttoright,wegraduallyincreasethetoken-levelalignmentweightfrom0.0to0.1
andthen0.5(defaultinourmainexperiments).Thechangeofthetop5resultsdemonstratethebenefitoftoken-levelcontrast
when performing text-video retrieval. Below each video (depicted by three side-by-side frames), we show the associated
descriptionsprovidedintheoriginaldataset. Betterviewedbyenlargingthefigure.
13
Figure4: Tokeninversedocumentfrequency(IDF)fornounandverbinYouCook2andMSR-VTT.Forclarity, weevenly
samplethetokensandshowtheirIDFs. Fromlefttoright,thenoun/verbbecomesmoreandmorefrequentgradually.
14

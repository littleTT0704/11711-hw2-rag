Detoxifying Text with MARCO:
Controllable Revision with Experts and Anti-Experts
SkylerHallinan♡ AlisaLiu♡ YejinChoi♡♣ MaartenSap♢♣
♡PaulG.AllenSchoolofComputerScience&Engineering,UniversityofWashington
♣AllenInstituteforAI ♢LanguageTechnologiesInstitute,CarnegieMellonUniversity
hallisky@uw.edu,maartensap@cmu.edu
Abstract
1) Given a toxic input
Text detoxification has the potential to miti-
You’ll be fine! Just talk like a white person.
gatetheharmsoftoxicitybyrephrasingtextto
removeoffensivemeaning,butsubtletoxicity
2) Identify where the toxic and
remains challenging to tackle. We introduce
non-toxic LMs disagree the most
MARCO,adetoxificationalgorithmthatcom-
binescontrollablegenerationandtextrewriting
methods using a Product of Experts with au-
Toxic LM fine bad talk curse good white
toencoderlanguagemodels(LMs). MARCO You’ll be fine! Just talklike a whiteperson.
useslikelihoodsunderanon-toxicLM(expert)
andatoxicLM(anti-expert)tofindcandidate
Non-Toxic LM fine bad talk curse good white
words to mask and replace. We evaluate our
methodonseveralsubtletoxicityandmicroag- 3) Generate a non-toxic rewrite
gressionsdatasets,andshowthatitnotonlyout- using the base LM + steering
performsbaselinesonautomaticmetrics, but
MARCO’s rewrites are preferred 2.1× more You’ll be fine! Just talklike a whiteperson.
in human evaluation. Its applicability to in-
stancesofsubtletoxicityisespeciallypromis-
Base LM fine bad talk curse good white
ing,demonstratingapathforwardforaddress-
You’ll be[mask]!Just[mask] like a[mask] person.
ingincreasinglyelusiveonlinehate.
1 Introduction
Non-Toxic LM fine bad talk curse good white
Toxic,offensive,hateful,orbiasedlanguageisin-
creasinglyprevalentandcancauseonlineandof- Toxic LM fine bad talk curse good white
flineharms,especiallytominoritygroups(Thomas
etal.,2021;OHCHR,2021). Thisischallenging
Ensemble fine bad talk curse good white
for NLP systems to detect and account for when
biasesaresubtleorwithoutexplicittoxickeywords You’ll be fine! Just talklike a goodperson.
(Hartvigsenetal.,2022;HanandTsvetkov,2020;
Figure1: AdemonstrationoftheMARCOalgorithm,
Vidgen et al., 2021). For example, the statement
whichutilizesabaselanguagemodel(LM)andafine-
"You’llbefine! Justtalklikeawhiteperson"con-
tunedtoxicandnon-toxicLMtorewritetoxictext. We
veysthebiasedimplicationthatnon-whitedialects
startwithtoxictext,identifypotentiallytoxictokensvia
arenotconducivetosuccess(Figure1),whichisa disagreement of the toxic and non-toxic LMs, and fi-
harmfulracialstereotype(Nadaletal.,2014). nallygenerateanon-toxicrewriteusingthebasemodel
Textdetoxification,i.e.,rewritingtexttobeless steeredbythetoxicandnon-toxicLM.
toxicwhilepreservingnon-toxicmeaning,provides
apromisingsolutionbysuggestingalternativeways
ofexpressingsimilarideaswithlessbiasedimplica- the quality of online conversations (e.g., through
tions(NogueiradosSantosetal.,2018). Forexam- machine-in-the-loopinterfaces;Hohensteinetal.,
ple,therewrite“You’llbefine! Justtalklikeagood 2021;Clarketal.,2018).
person" eliminates the racial bias from the origi- We present MARCO, Mask and Replace with
nalstatementwhilepreservingthenon-toxicmean- Context: a new, unsupervised algorithm for text
ing. Such methods have the potential to improve detoxificationthatcombinesmask-and-replacetext
3202
yaM
62
]LC.sc[
2v34501.2122:viXra
denoisingwithcontrollabletextgenerationusinga oftheentiresequence. Thoughinspiredby DEX-
ProductofExperts(PoE)(PoE,DEXPERTS;Hin- PERTS (Liu et al., 2021), our novelty is two-fold:
ton,2002;Liuetal.,2021). first, we tackle a more challenging task, unsuper-
MARCO jointly uses an expert and an anti- visedrevision,insteadofstyle-controlledgenera-
expert,apairoflanguagemodels(LM)fine-tuned tion,andsecond,weproposeadetectandrewrite
on a non-toxic and toxic corpus respectively, to pipeline, in contrast to simple word-distribution
identifywhichtokensmostlikelycontributetothe steeringduringautoregressivegeneration.
overalltoxicity,andthensuggestreplacementsthat
ExpertandAnti-ExpertLMs Ourmethodfor
lower toxicity. Using LMs to capture toxicity al-
unsupervised controlled revision is based on de-
lows MARCO to rewrite much subtler toxic text
noising autoencoder LMs (AE-LMs), which are
comparedtopreviousworkthatusestoxicityclas-
trainedtomaskandreconstructsequencesoftext.
sifiersortoxicwordlists(Daleetal.,2021).
OursetupconsistsofabasepretrainedAE-LMG,
WeapplyMARCOtothreedatasetsfocusedon
anexpert AE-LMG+ finetunedondatawithde-
subtlytoxicstatements,suchasmicroaggressions.
sirableattributes,andananti-expertAE-LMG−
Ourmethodoutperformsstate-of-the-artdetoxifi-
finetunedondatawithundesirableattributes.
cationbaselinesfromDaleetal.(2021)acrossall
We use BART-base (Lewis et al., 2020) as our
threedatasets,asmeasuredthroughbothautomatic
baseautoencoder. Wefinetunetheexpertandanti-
andhumanevaluation. Ourworkshowstheeffec-
expertusing1Mnon-toxicand100Kovertlytoxic
tivenessofcombiningcontrollablegenerationwith
commentsfromtheJigsawcorpus(Do,2019),as
textrewritingmethodsfortextdetoxification.1
done in Liu et al. (2021) and Dale et al. (2021).
BARTcaninfillmultipleornotokensevenifonly
2 Background: TextDetoxification
one token is masked, allowing for more flexible
maskinfilling. SeeAppendixAfortrainingdetails.
Text detoxification is a form of stylistic rewrit-
ing (Hu et al., 2017; Shen et al., 2017; Jhamtani
3.1 ContextualMasking
etal.,2017)withthegoalofproducinganon-toxic
Wefirstidentifylocationsthatcould conveytoxic
rewrite given a toxic input sentence. This task is
meaning; intuitively, these could be words or
challenging,asitrequiresbothdetoxificationand
phrases with strongly differing likelihoods under
preservationofnon-toxicmeaning, incontrastto
theexpertandanti-expert.
controllabletextgeneration,whichaimstosimply
Formally,givenasequencew,foreverytoken
generateanynon-toxiccontinuationforaprompt
w ∈ w,wetemporarilymaskitandgenerateprob-
(Prabhumoyeetal.,2020;Gehmanetal.,2020). i
abilitydistributionsoverthevocabularyV forthat
Duetoalackofsupervisionwithparalleldata,
location from G+ and G−, which we denote P+
an often effective approach to stylistic rewriting
and P− respectively. Then, we compute the dis-
reliesonunsupervisedmasking-and-reconstructing
tance d between P+ and P− using the Jensen-
approaches(Lietal.,2018;Wuetal.,2019;Malmi i
Shannondivergence,asymmetricformoftheKull-
et al., 2020; Ma et al., 2020). In this paradigm,
back–Leibler(KL)divergence:2
sourcestyle-specifictokens/spansintheinputtext
are detected and masked, then filled in with to-
d =
1(cid:0)
D
(P+∥P−)(cid:1)
+
1(cid:0)
D
(P−∥P+)(cid:1)
kens/spansfromthetarget-styleusingamaskedlan- i 2 KL 2 KL
guagemodel. Otherworkhasframeddetoxification
After normalizing all distances by the mean, we
asatranslationorparaphrasingtask,usingaclassi-
maskallw whosedistanced isaboveathreshold
i i
fiertosteerawayfromtoxiccontent(Nogueirados
τ and denote the resulting sequence wm; these
Santosetal.,2018;Daleetal.,2021).
maskedtokensarelocationswheretoxicitymaybe
presentduetoexpertandanti-expertdisagreement.
3 TextDetoxificationwith MARCO
3.2 ContextualReplacing
MARCOisanunsupervisedapproachtotextdetox-
ification,consistingoftwodiscretesteps: masking
Aftermaskingpotentiallytoxiclocations,MARCO
then replaces them with more benign tokens – if
andthenreplacingtokens,assistedbythecontext
2GivenprobabilitydistributionsAandB,theKLdiver-
1Wereleaseourcodeanddataathttps://github.com/
genceisdefinedasD (A∥B)=
(cid:80)
A(x)log(cid:16) A(x)(cid:17)
shallinan1/MarcoDetoxification. KL B(x)
x∈V
Validation Test
Method Toxicity(↓) BERTScore(↑) Fluency(↓) Toxicity(↓) BERTScore(↑) Fluency(↓)
Original 0.286 – 51.49 0.272 – 70.20
CondBERT 0.161 0.966 104.10 0.148 0.964 88.69
MAgr
ParaGeDi 0.162 0.931 104.46 0.172 0.929 120.78
MARCO 0.145 0.958 43.54 0.141 0.954 39.10
Original 0.351 – 58.46 0.344 – 88.79
CondBERT 0.202 0.961 69.51 0.190 0.961 131.12
SBF
ParaGeDi 0.186 0.921 179.88 0.192 0.923 99.96
MARCO 0.176 0.947 54.86 0.186 0.946 48.75
Original 0.563 – 205.73 0.578 – 220.42
Dyna CondBERT 0.288 0.954 190.51 0.293 0.950 200.20
Hate ParaGeDi 0.332 0.918 217.78 0.323 0.912 240.17
MARCO 0.274 0.939 110.50 0.277 0.936 128.84
Table1: AutomaticevaluationsondetoxifiedgenerationsonMAgr,SBF,andDynaHateforMARCO,ParaGeDi
andCondBERTacrossalldatasetsandsplits,MARCOachievesthelowesttoxicity,bestfluency,andsecond-best
BERTScore,whileCondBERTachievesthehighestBERTScore. Boldindicatesthebestmetric,andunderline
indicatesthesecond-bestmetricineachcolumnforeachdataset.
theyareindeedtoxic–toautoregressivelyproduce anti-expert than with the expert, the original to-
arewriteggiventheoriginalandmaskedsentences ken is most likely toxic and will be replaced in
w and wm. We transform the DEXPERTS (Liu the rewrite. On the other hand, if the differences
et al., 2021) framework, which leverages a PoE betweentheexpertandanti-expertarenotenough
to steer a model away from toxic generations by toswaythebasemodel,theoriginaltokenismost
ensemblingtokenprobabilities,toenablerewriting likelynon-toxicandwillbere-addedintherewrite.
byusingAE-LMs.
We obtain the next-token unnormalized log- 4 DetoxificationExperiments&Results
probabilities(i.e.,logits)z ,z+,andz− fromthe
i i i
baseandexpertAE-LMsG,G+,andG−,respec- In our experiments, we focus on rewriting sen-
tences from three toxicity datasets, and use both
tively,conditionedonthepreviouslygeneratedto-
automatic and human evaluations to measure
kensg ,theoriginalsequencew,andthemasked
<i
variantwm. Wethenensemblethoselogitsintoa MARCO’sperformanceatdetoxifyingtext.
modifiednext-tokenprobabilitydistribution:
4.1 Datasets
P(X |g ,w,wm)=softmax(z +α z+−α z−)
i <i i 1 i 2 i We seek to rewrite English sentences that are al-
readyknowntobeorannotatedastoxic,especially
whereX isarandomvariableoverthevocabulary
i
sentences that contain more subtle or implicit bi-
V representingthenexttokenatindexigiventhe
ases(e.g.,withoutswearwords). Incontrasttothe
previousgenerationg ,andourtwohyperparam-
<i
Jigsawcorpususedtofinetuneourexperts,weuse
etersα andα independentlycontroltheimpact
1 2
threeout-of-domaindatasetswithsubtletoxicity:
oftheexpertandanti-expertformoreflexibility.3
In our method, the expert and anti-expert use
Microagressions.com (MAgr)isapubliclyavail-
themaskedsequencew astheirinput,whilethe
m able Tumblr blog where users can anonymously
basemodelusestheunmaskedw. Intuitively,the
post about socially-biased interactions and utter-
base model tries to replicate the input sequence
ances in the wild. Each post includes an offend-
but is steered by an expert and anti-expert with
ingquoteand/oradescriptionoftheincident. We
contrastingprobabilitydistributionsatthemasked
scrape all quotes, resulting in a set of real-world
locations. Thisenablesrewriteswithminimalbut
microagressionutterances. Thevalidationandtest
meaningfuleditsontoxictokensandpreservation
setsizesare238and298respectively.
ofnon-toxiccontent. Notethatforamaskedloca-
tion, when the base model agrees more with the Social Bias Frames (SBF; Sap et al., 2020) is
a corpus of socially biased and offensive content
3AppendixEgivesfurtherintuitionintounderstandingthis
equationasaPoE. from various online sources. We use a subset of
Validation Test Original ...becauseain’tnobodygottimetopickall
MAgr 0.57 0.27 0.16 0.44 0.30 0.27 thatcotton.
SBF 0.43 0.37 0.20 0.47 0.34 0.19 MARCO ...becauseain’tnobodygottimetopickall
DynaHate 0.35 0.45 0.20 0.32 0.40 0.28 thatup.
CondBERT ... becauseain’tnobodygottimetopick
MaRCo Tie CondBERT
allthatcotton.
ParaGeDi Becausenobodyhastimetopickupallthe
Validation Test
cotton.
MAgr 0.60 0.23 0.17 0.48 0.21 0.31
SBF 0.60 0.17 0.23 0.41 0.18 0.41 Original Ugh,Ihatelivinghere. There’ssomany
DynaHate 0.36 0.33 0.31 0.43 0.24 0.33 blackpeoplearound.
MaRCo Tie ParaGeDi
MARCO Ugh,Ihatelivinghere. There’ssomany
peoplearound.
CondBERT "well, i could be living here. there’s so
Figure2: Head-to-headhumanevaluationsontoxicity
manyothernicepeoplearound.
forMARCOvsCondBERTandMARCOvsParaGeDi ParaGeDi I’vebeenlivinghereforalongtime.
acrossalldatasetsandsplits. MARCO hasless-toxic
Original Iwouldn’teatfoodthathasbeentouched
generationshead-to-headagainstbothbaselines,most
byam*ng
notablyonthesubtletoxicitydatasets(MAgrandSBF).
MARCO Iwouldn’teatfoodthathasbeentouched
byahumanbeing.
CondBERT i wouldn’t be eating food that has been
SBFfromthemicroaggressionssubreddit,4 which touchedbyam*ng
ParaGeDi Iwouldnoteatfoodtouchedbyamonk.
contains subtly biased content (Breitfeller et al.,
2019). Weuseallpostswherethemajorityofanno-
Table2: Differentrewritingmethodsonthreetoxicex-
tatorsmarkedthetextasoffensive. Thevalidation
amplesfromSBF(top),MAgr(middle),andDynaHate
andtestsetsizesare92and114respectively. (bottom). InthetoxicexamplefromSBF(containing
aracistslaveryreferencetocottonpicking). MARCO
DynaHate (Vidgen et al., 2021) is an adversar-
detectsandmasks“cotton”asatoxicityindicator,which
ially collected set of hate speech, where human baselinesfailtorewrite.Inthelastexample,CondBERT
annotatorscreateexamplesthatan iterativelyim- failstorecognizethetoxicityoftheword“m*ng”(un-
proved hate-speech classifier cannot detect. We censoredinthedata)whichisconsideredanableistslur
utilizeallfourroundsofhate-speechdataanduse (Clark,2011).
allexamplesmarkedashateful. Thevalidationand
testsetsizesare1,858and2,011respectively.
AutomaticMetrics Weassessthequalityofthe
models’ rewrites with automatic metrics used in
4.2 Baselines
previous work (Liu et al., 2021; Ma et al., 2020).
We compare MARCO to the two baseline ap-
Wereporttheaveragetoxicityscoreofrewritesus-
proaches from Dale et al. (2021), which have ingthePerspectiveAPI.5 Additionally,wemeasure
shownstate-of-the-artdetoxificationperformance.
fluencyofrewritesbycomputingtheirperplexity
SeeAppendixBforgenerationdetails.
with an external LM (GPT-2 XL; Radford et al.,
2019),andmeaningsimilaritybetweentheinput
ParaGeDi utilizesaclass-conditionedlanguage
and the rewrite using BERTScore (Zhang et al.,
model(usingcontrolcodesfortoxicandnon-toxic
2019). SeeAppendixB.3forfurtherdetails.
styles)ontopofaparaphrasinglanguagemodelto
steergeneratedtexttowardsaspecificattribute.
HumanEvaluation Weconductahead-to-head
CondBERT follows a pointwise editing setup, human evaluation (Kiritchenko and Mohammad,
firstidentifyingtokenstomaskintheinput, then 2017)ofthetoxicityoftherewritesusingAmazon
usingamask-fillingmodeltoreplacethem. Incon- MechanicalTurk. Foreachdataset’svalidationand
trastto MARCO,CondBERTusesalexicon-based testsets,wesample75promptseach,thencompare
approachtomaskingwordsbyusingweightsfrom eachpairofMARCO,ParaGeDiandCondBERT’s
awhole-word,toxiclanguagelogisticclassifier. generationsagainsteachotherandaskwhichone
islesstoxic(alongwithanoptiontoflageitherof
4.3 EvaluationSetup therewritesasungrammaticalordisfluent). Inour
Weperformautomaticandhumanevaluations,fol- evaluation, we obtained head-to-head judgments
lowingpreviouswork. fromthreeworkersperrewritepair;workersagreed
4Asubredditisatopic-focusedcommunityonReddit 5www.perspectiveapi.org,accessed06-2022.
moderately, with a Cohen’s κ=0.575 on average. forcontrollablerevision,andhighlightstheuseful-
SeeAppendixDfordetails(e.g.,MTurkinterface). nessofusingLMsforcapturingtoxicity.
4.4 Results Limitations,EthicalConsiderations,and
BroaderImpacts
Automatic metrics (Table 1) show that MARCO
isbetteratdetoxificationthanbaselinesacrossall
Despite the promising performance of MARCO
datasets and splits by 10.3% on average. Human
at detoxifying text, there are several limitations,
evaluationscorroboratethis(Figure2),asMARCO
ethicalconsiderations,andbroaderimpactsofour
is on average rated as less toxic than CondBERT
approach,whichwelistbelow.
2.2timesmoreoftenthanviceversaacrossdatasets
First,inthiswork,weseektodetoxifysentences.
andsplits,and1.9timesmoreoftenvs. ParaGeDi.
However, toxicity itself is a subjective and sensi-
Intermsofmeaningpreservationasmeasuredby
tive concept with large potential downstream im-
BERTScore, MARCO isonparwithCondBERT,
pacts caused by annotator and subsequent model
withanaveragescorewithin2.5%acrossdatasets.
biases (Sap et al., 2022). We somewhat mitigate
However,BERTScoredoesnotmeasuremeaning
this variation by selecting human evaluators that
preservationofonlynon-toxiccontent;removing
scoredhighlyonatoxicityqualificationtask(see
toxicmeaningbydefinitionrequirestrade-offsbe-
AppendixD),inlinewithaprescriptiveparadigm
tweenfluency,styleaccuracy,andmeaningpreser-
oftoxicityannotation(Rottgeretal.,2022). Future
vation as discussed in most style transfer work
workcouldinvestigatetheeffectofdemographics
(Daleetal.,2021;Laugieretal.,2021;Malmietal.,
on preference for different rewriting algorithms,
2020;Maetal.,2020;Krishnaetal.,2020,i.a.).
e.g.,inamoredescriptiveparadigm.
Compared to DynaHate, MARCO’s margin of
In addition, achieving meaningful seman-
winning is even larger on MAgr and SBF, which
tic preservation in detoxification is challenging.
contain more subtle toxicity. For instance, in the
Specifically,itisdifficulttodisentanglethetoxic
firstexamplefromTable2,thesubtlereferenceto
and non-toxic meanings from the input, making
cottonpickingandslaveryiscorrectedbyMARCO,
itchallengingtogeneratedetoxifiedrewriteswith
which replaces “cotton” with “up”; in contrast,
highpreservationofonlythenon-toxiccontent;this
both baselines fail to revise the toxic content.6
mayriskminimizingmarginalizedgroups’speech
Sinceallthreemethodslearnedtoxicityusingthe
(Xu et al., 2021). Partially, this could be due to
sameovertlytoxicdatafromJigsaw,thefactthat
a lack of context incorporation (social, conversa-
MARCOdealsespeciallywellwithsubtletoxicity
tional,precedingsentences;Yerukolaetal.,2023);
highlights the advantages of using LMs to better
future work should consider adapting detoxifica-
modelandcapturetoxicitypatterns.
tion methods in context (Cheng et al., 2020; Roy
Finally, MARCO’s rewrites were more fluent etal.,2023).
than other methods, according to both automatic
MARCOalsorequiresfinetuningtwopretrained
metricsandhumanevaluation. MARCO’srewrites
LMs, which is not computationally insignificant
weredeemedasungrammaticaltheleastamountof
(Strubelletal.,2019;Schwartzetal.,2020). Future
thetime(9.3%),versus9.7%forCondBERTand
workcouldexploreusingsmallerLMstocontrol
11.7%forParaGeDi.
a larger model (Liu et al., 2021), or even more
lightweightapproaches.
5 Conclusion
Additionally,weacknowledgethatintheevalu-
ation, we expose Turkers to toxic content, which
WepresentMARCO,anovelmethodfortextdetox-
mightharmindividuals,especiallythosewithiden-
ification, which utilizes auto-encoder language
titiesthattheoffensivecontentappliesto(Roberts,
model experts in a mask and reconstruct process.
2017; Steiger et al., 2021). However, we pay a
Our method outperforms strong baselines in au-
fair wage (US$8/h) and our work is approved by
tomatic and human evaluations, showing strong
our institution’s ethics review board (IRB). See
abilitytodetoxifyevensubtlebiases. MARCO’s
AppendixDforfurtherdetails.
successdemonstratestheeffectivenessofcontrol-
Anothermajorethicalimplicationofourworkis
lablegenerationmixedwithtextrewritingmethods
that,followingpreviouswork,weusethePerspec-
6AppendixCcontainsmoreexamplegenerations. tiveAPItoautomaticallyassesstoxicity,aclassi-
fierwhichcontainsdocumentedbiases(e.g.,demo- QuanHDo.2019. Jigsawunintendedbiasintoxicity
graphicbiasesandracialbiases;Dixonetal.,2018; classification.
Sap et al., 2019). Future research could consider
Samuel Gehman, Suchin Gururangan, Maarten Sap,
different,moreholisticviewsoftoxicityandbiases Yejin Choi, and Noah A. Smith. 2020. RealToxi-
(e.g.,Sapetal.,2020). cityPrompts: Evaluating neural toxic degeneration
inlanguagemodels. InFindingsoftheAssociation
Finally, although our application in this paper
forComputationalLinguistics: EMNLP2020,pages
is detoxification, we acknowledge that MARCO
3356–3369,Online.AssociationforComputational
couldbeappliedfortheoppositepurpose,ie.,gen- Linguistics.
eration of toxic text from non-toxic text; this is
Xiaochuang Han and Yulia Tsvetkov. 2020. Fortify-
a malicious application which we condemn. Al-
ingtoxicspeechdetectorsagainstveiledtoxicity. In
thoughthisissueismoreprevalentforcontrolled Proceedings of the 2020 Conference on Empirical
generation methods (McGuffie and Newhouse, MethodsinNaturalLanguageProcessing(EMNLP),
2020),thisisstillariskMARCOfaces. Inasimilar pages7732–7739,Online.AssociationforComputa-
tionalLinguistics.
vein, we do not endorse using the toxicity or mi-
croaggressiondatasetstodevelopmodelstogener- Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,
atemoretoxicityormicroaggressions,asthismay MaartenSap,DipankarRay,andEceKamar.2022.
Toxigen:Alarge-scalemachine-generateddatasetfor
incurharm,especiallytomarginalized/vulnerable
adversarialandimplicithatespeechdetection.
populations.
GeoffreyE.Hinton.2002. Trainingproductsofexperts
byminimizingcontrastivedivergence. NeuralCom-
References put.,14(8):1771–1800.
Luke Breitfeller, Emily Ahn, David Jurgens, and Yu- JessHohenstein,DominicDiFranzo,ReneFKizilcec,
liaTsvetkov.2019. Findingmicroaggressionsinthe ZhilaAghajari,HannahMieczkowski,KarenLevy,
wild: Acaseforlocatingelusivephenomenainso- MorNaaman,JeffHancock,andMalteJung.2021.
cialmediaposts. InProceedingsofthe2019Confer- Artificialintelligenceincommunicationimpactslan-
enceonEmpiricalMethodsinNaturalLanguagePro- guageandsocialrelationships.
cessingandthe9thInternationalJointConference
onNaturalLanguageProcessing(EMNLP-IJCNLP), Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
pages1664–1674,HongKong,China.Association Salakhutdinov,andEricP.Xing.2017. Towardcon-
forComputationalLinguistics. trolledgenerationoftext. InProceedingsofthe34th
InternationalConferenceonMachineLearning-Vol-
YuCheng,ZheGan,YizheZhang,OussamaElachqar, ume70,ICML’17,page1587–1596.JMLR.org.
DianqiLi,andJingjingLiu.2020. Contextualtext
styletransfer. InFindingsoftheAssociationforCom- HarshJhamtani,VarunGangal,EduardHovy,andEric
putationalLinguistics: EMNLP2020,pages2915– Nyberg. 2017. Shakespearizing modern language
2924. usingcopy-enrichedsequencetosequencemodels.
InProceedingsoftheWorkshoponStylisticVariation,
Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, pages10–19,Copenhagen,Denmark.Associationfor
Yangfeng Ji, and Noah A. Smith. 2018. Creative ComputationalLinguistics.
writingwithamachineintheloop: Casestudieson
slogans and stories. In 23rd International Confer- SvetlanaKiritchenkoandSaifMohammad.2017. Best-
ence on Intelligent User Interfaces, IUI ’18, page worstscalingmorereliablethanratingscales: Acase
329–340,NewYork,NY,USA.AssociationforCom- studyonsentimentintensityannotation. InProceed-
putingMachinery. ingsofthe55thAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume2: ShortPapers),
NicolaClark.2011. Rickygervais,pleasestopusingthe pages465–470,Vancouver,Canada.Associationfor
word’mong’. TheGardian. Accessed2023-05-25. ComputationalLinguistics.
DavidDale,AntonVoronov,DarynaDementieva,Var- KalpeshKrishna,JohnWieting,andMohitIyyer.2020.
varaLogacheva,OlgaKozlova,NikitaSemenov,and Reformulating unsupervised style transfer as para-
AlexanderPanchenko.2021. Textdetoxificationus- phrasegeneration. InProceedingsofthe2020Con-
inglargepre-trainedneuralmodels. InProceedings ferenceonEmpiricalMethodsinNaturalLanguage
ofthe2021ConferenceonEmpiricalMethodsinNat- Processing(EMNLP),pages737–762,Online.Asso-
uralLanguageProcessing,pages7979–7996,Online ciationforComputationalLinguistics.
andPuntaCana,DominicanRepublic.Association
forComputationalLinguistics. LéoLaugier,JohnPavlopoulos,JeffreySorensen,and
Lucas Dixon. 2021. Civil rephrases of toxic texts
LucasDixon,JohnLi,JeffreySorensen,NithumThain, with self-supervised transformers. In Proceedings
andLucyVasserman.2018. Measuringandmitigat- ofthe16thConferenceoftheEuropeanChapterof
ingunintendedbiasintextclassification. theAssociationforComputationalLinguistics: Main
Volume,pages1442–1461,Online.Associationfor Cicero Nogueira dos Santos, Igor Melnyk, and Inkit
ComputationalLinguistics. Padhi.2018. Fightingoffensivelanguageonsocial
mediawithunsupervisedtextstyletransfer. InPro-
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan ceedingsofthe56thAnnualMeetingoftheAssocia-
Ghazvininejad,AbdelrahmanMohamed,OmerLevy, tionforComputationalLinguistics(Volume2: Short
Veselin Stoyanov, and Luke Zettlemoyer. 2020. Papers),pages189–194,Melbourne,Australia.As-
BART:Denoisingsequence-to-sequencepre-training sociationforComputationalLinguistics.
fornaturallanguagegeneration,translation,andcom-
prehension. InProceedingsofthe58thAnnualMeet- OHCHR.2021. Report: Onlinehateincreasingagainst
ingoftheAssociationforComputationalLinguistics, minorities,saysexpert. Technicalreport.
pages7871–7880,Online.AssociationforComputa-
tionalLinguistics. Shrimai Prabhumoye, Alan W Black, and Ruslan
Salakhutdinov. 2020. Exploring controllable text
Juncen Li, Robin Jia, He He, and Percy Liang. 2018. generation techniques. In Proceedings of the 28th
Delete,retrieve,generate: asimpleapproachtosenti- InternationalConferenceonComputationalLinguis-
mentandstyletransfer. InProceedingsofthe2018 tics,pages1–14,Barcelona,Spain(Online).Interna-
Conference of the North American Chapter of the tionalCommitteeonComputationalLinguistics.
AssociationforComputationalLinguistics: Human
Language Technologies, Volume 1 (Long Papers), AlecRadford,JeffreyWu,RewonChild,DavidLuan,
pages1865–1874,NewOrleans,Louisiana.Associa- DarioAmodei,IlyaSutskever,etal.2019. Language
tionforComputationalLinguistics. modelsareunsupervisedmultitasklearners. OpenAI
blog,1(8):9.
Alisa Liu, Maarten Sap, Ximing Lu, Swabha
SarahTRoberts.2017. Socialmedia’ssilentfilter. The
Swayamdipta,ChandraBhagavatula,NoahA.Smith,
Atlantic.
andYejinChoi.2021. DExperts:Decoding-timecon-
trolledtextgenerationwithexpertsandanti-experts.
PaulRottger,BertieVidgen,DirkHovy,andJanetPier-
In Proceedings of the 59th Annual Meeting of the
rehumbert. 2022. Two contrasting data annotation
Association for Computational Linguistics and the
paradigmsforsubjectiveNLPtasks. InProceedings
11thInternationalJointConferenceonNaturalLan-
ofthe2022ConferenceoftheNorthAmericanChap-
guageProcessing(Volume1: LongPapers),pages
ter of the Association for Computational Linguis-
6691–6706,Online.AssociationforComputational
tics:HumanLanguageTechnologies,pages175–190,
Linguistics.
Seattle,UnitedStates.AssociationforComputational
Linguistics.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Shamik Roy, Raphael Shu, Nikolaos Pappas, Elman
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Mansimov,YiZhang,SaabMansour,andDanRoth.
Roberta: A robustly optimized bert pretraining ap-
2023. Conversation style transfer using few-shot
proach. ArXiv,abs/1907.11692.
learning. arXivpreprintarXiv:2302.08362.
XinyaoMa,MaartenSap,HannahRashkin,andYejin
MaartenSap,DallasCard,SaadiaGabriel,YejinChoi,
Choi.2020. PowerTransformer: Unsupervisedcon-
and Noah A. Smith. 2019. The risk of racial bias
trollablerevisionforbiasedlanguagecorrection. In
inhatespeechdetection. InProceedingsofthe57th
Proceedings of the 2020 Conference on Empirical
AnnualMeetingoftheAssociationforComputational
MethodsinNaturalLanguageProcessing(EMNLP),
Linguistics,pages1668–1678,Florence,Italy.Asso-
pages7426–7441,Online.AssociationforComputa-
ciationforComputationalLinguistics.
tionalLinguistics.
MaartenSap,SaadiaGabriel,LianhuiQin,DanJuraf-
EricMalmi,AliakseiSeveryn,andSaschaRothe.2020. sky, NoahA.Smith, andYejinChoi.2020. Social
Unsupervisedtextstyletransferwithpaddedmasked biasframes: Reasoningaboutsocialandpowerim-
languagemodels. InProceedingsofthe2020Con- plicationsoflanguage. InProceedingsofthe58th
ferenceonEmpiricalMethodsinNaturalLanguage AnnualMeetingoftheAssociationforComputational
Processing(EMNLP),pages8671–8680,Online.As- Linguistics,pages5477–5490,Online.Association
sociationforComputationalLinguistics. forComputationalLinguistics.
KrisMcGuffieandAlexNewhouse.2020. Theradical- Maarten Sap, Swabha Swayamdipta, Laura Vianna,
izationrisksofGPT-3andadvancedneurallanguage Xuhui Zhou, Yejin Choi, and Noah Smith. 2022.
models. CoRR,abs/2009.06807. Annotators with attitudes: How annotator beliefs
andidentitiesbiastoxiclanguagedetection. InPro-
KevinL.Nadal,KatieE.Griffin,YingleeWong,Sahran ceedingsofthe2022ConferenceoftheNorthAmer-
Hamit, andMorganRasmus.2014. Theimpactof icanChapteroftheAssociationforComputational
racialmicroaggressionsonmentalhealth: Counsel- Linguistics: HumanLanguageTechnologies,pages
ingimplicationsforclientsofcolor. JournalofCoun- 5884–5906, Seattle, UnitedStates.Associationfor
seling&Development,92(1):57–66. ComputationalLinguistics.
Roy Schwartz, Jesse Dodge, Noah A. Smith, and AlbertXu,EshaanPathak,EricWallace,SuchinGuru-
Oren Etzioni. 2020. Green ai. Commun. ACM, rangan,MaartenSap,andDanKlein.2021. Detoxi-
63(12):54–63. fyinglanguagemodelsrisksmarginalizingminority
voices. In Proceedings of the 2021 Conference of
TianxiaoShen,TaoLei,ReginaBarzilay,andTommi theNorthAmericanChapteroftheAssociationfor
Jaakkola.2017. Styletransferfromnon-paralleltext ComputationalLinguistics: HumanLanguageTech-
bycross-alignment. InProceedingsofthe31stInter- nologies,pages2390–2397,Online.Associationfor
nationalConferenceonNeuralInformationProcess- ComputationalLinguistics.
ingSystems,NIPS’17,page6833–6844,RedHook,
NY,USA.CurranAssociatesInc. AkhilaYerukola,XuhuiZhou,andMaartenSap.2023.
“don’ttakethisoutofcontext!” ontheneedforcon-
Miriah Steiger, Timir J Bharucha, Sukrit Venkatagiri, textualmodelsandevaluationsforstylisticrewriting.
MartinJ.Riedl,andMatthewLease.2021. Thepsy- arXiv.
chological well-being of content moderators: The
emotionallaborofcommercialmoderationandav- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
enuesforimprovingsupport. InProceedingsofthe Weinberger, and Yoav Artzi. 2019. Bertscore:
2021CHIConferenceonHumanFactorsinComput- Evaluating text generation with BERT. CoRR,
ingSystems,CHI’21,NewYork,NY,USA.Associa- abs/1904.09675.
tionforComputingMachinery.
A ModelingDetails
EmmaStrubell,AnanyaGanesh,andAndrewMcCal-
lum. 2019. Energy and policy considerations for A.1 Out-of-the-BoxModeling
deeplearninginnlp.
We use the HuggingFace Transformers library
KurtThomas,DevdattaAkhawe,MichaelBailey,Dan (Wolf et al., 2020) version 4.10.2 for out-of-the-
Boneh,ElieBursztein,SunnyConsolvo,NicolaDell,
box, pretrained BART models and for finetuning
ZakirDurumeric,PatrickGageKelley,DeepakKu-
using the Trainer class. It is licensed under the
mar,DamonMcCoy,SarahMeiklejohn,ThomasRis-
tenpart,andGianlucaStringhini.2021. Sok: Hate, Apache License 2.0., and the code is available at
harassment, and the changing landscape of online https://github.com/huggingface/transformers.
abuse. In 2021 IEEE Symposium on Security and
Privacy(SP),pages247–267.
A.2 FinetuningtheExperts
Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Fortheexpertandanti-expertmodels,wefurther
DouweKiela.2021. Learningfromtheworst: Dy- finetunethebaseBARTmodelwith139Mparame-
namicallygenerateddatasetstoimproveonlinehate
ters,foundathttps://huggingface.co/facebook/bart-
detection. InProceedingsofthe59thAnnualMeet-
base and licensed under the Apache License 2.0,
ingoftheAssociationforComputationalLinguistics
andthe11thInternationalJointConferenceonNatu- with the non-toxic and toxic corpus respectively.
ralLanguageProcessing(Volume1: LongPapers), Weusethesamepretrainingprocedureusedtofur-
pages1667–1682,Online.AssociationforComputa-
ther fintune BART (Lewis et al., 2020), and ran-
tionalLinguistics.
domly corrupt sequences during training, which
alignswithBART’sintendeduse.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RemiLouf,MorganFuntow- TrainingCorpus WeusetheJigsawUnintended
icz,JoeDavison,SamShleifer,PatrickvonPlaten, BiasinToxicityClassification(Do,2019)dataset
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, forfinetuningourexpertandantiexpert,acorpusof
Teven Le Scao, Sylvain Gugger, Mariama Drame,
forumcommentsonnewsarticles. Eachcomment
QuentinLhoest,andAlexanderRush.2020. Trans-
hasfivebinaryannotationsonifitistoxicornot.
formers:State-of-the-artnaturallanguageprocessing.
InProceedingsofthe2020ConferenceonEmpirical Wemarkallsequenceswithnotoxicannotationsas
Methods in Natural Language Processing: System non-toxic,andallsequenceswithmorethan50%
Demonstrations,pages38–45,Online.Association
toxic annotations as toxic. The intended use of
forComputationalLinguistics.
thisdatasetistohelpminimizeunintendedmodel
XingWu,TaoZhang,LiangjunZang,JizhongHan,and bias, which we follow in this work. Finally, we
SonglinHu.2019. Maskandinfill:Applyingmasked sample100instancesfromthevalidationset,and
languagemodelforsentimenttransfer. InProceed- findtheonlyindividualsmentionedinJigsaware
ings of the Twenty-Eighth International Joint Con-
high-profilepoliticalfigureswhoarealreadywell-
ference on Artificial Intelligence, IJCAI-19, pages
known. Wedonotperformadditionalanonymiza-
5271–5277.InternationalJointConferencesonArti-
ficialIntelligenceOrganization. tionofthedata.
Expert We finetune the expert with the hyper- followtheintendeduseofallthreedatasetsbyus-
parameters listed in Table 3, using two NVIDIA ingthemonlytorewritetoxicsentences.
RTX6000 GPUs. We select the best checkpoint, We also preprocess each of the datasets in the
basedonthelowestevaluationloss,whichisatstep same way. We use the re package built-in to
100,000. Thetotaltrainingtimeis20hours,for40 Python(weuseversion3.8.11)toremoveanyex-
GPUhoursofusage. tendedwhitespace,includingtabsandlinebreaks,
and convert them to one space. We use the html
Hyperparameter Assignment package, also built-in to our Python version, to
convert named html character references to their
model BART-base
numberofgpus 2 corresponding string, such as “&gt;” to ‘’>”. Af-
effectivebatchsize 48
terwards,weusetheftfypackage,version6.1.1,
totalsteps 100,000
stepsperevaluation 1000 foundathttps://pypi.org/project/ftfy/tofixbroken
learningrateoptimizer AdamW unicodeintext. Finally,weremoveanyverylong
AdamWinitiallearningrate 2.5e-06
sequences: wecalculatethe90%percentileoftext
AdamWepsilon 1e-06
learningrateschedule linearwithnowarmup lengthstobe44, wheretextlengthisthenumber
weightdecay 0.0 ofspace-delimitedwords,andweremoveanyse-
maxsequencelength 180
quenceslongerthanthis.
maxgenerationlength 230
paddingsequences tomaxseqlength
MAgr Wescrapeallquotesfrompostsusingthe
Table3: Hyperparametersusedtofinetunetheexpert TumblrAPI,followingtheAPILicenseAgreement
model athttps://www.tumblr.com/docs/en/api_agreement,
which grants the right to use, distribute, display,
Anti-Expert Wefinetunetheanti-expertwiththe andmodifypostedTumblrcontent.
hyperparameters listed in Table 4, using a single
SBF Thereisnolicenseforthisdataset.
NVIDIARTX6000GPU.Weselectthebestcheck-
point,basedonthelowestevaluationloss,which DynaHate Thereisnolicenseforthisdataset.
isatstep38,000. Thetotaltrainingtimeis2hours,
for2GPUhoursofusage. B.2 GenerationDetails
GenerationsareperformedusingasingleNVIDIA
Hyperparameter Assignment
RTX6000GPUforalldatasetsandmethods.
model BART-base
numberofgpus 1 MARCO
effectivebatchsize 32
totalsteps 50,000
MaskingHyperparameters Wesetamasking
stepsperevaluation 1000
learningrateoptimizer AdamW
thresholdofτ = 1.2forallexperiments.
AdamWinitiallearningrate 1e-06
AdamWepsilon 1e-06 GenerationHyperparameters Wegenerate
learningrateschedule linearwithnowarmup
withgreedysearchforalldatasetswithamaxgen-
weightdecay 0.0
maxsequencelength 180 erationlengthof128.
maxgenerationlength 230
paddingsequences tomaxseqlength MAgr We perform a search jointly over dif-
ferenthyperparametervaluesonthedevelopment
Table 4: Hyperparameters used to finetune the anti-
set. We choose the hyperparameter combination
expertmodel
thatperformsbestonautomaticmetrics,shownin
Table5,andusethistogenerateonthetestset.
B ExperimentalDetails
Hyperparameter Tested Assignment
B.1 Datasets
repetitionpenalty [1.0,1.2,1.5] 1.0
Foreachdataset,wemanuallysampleandreview α 1 [0,0.5,1.0,1.5] 1.5
75examplesfromthevalidationset,andsearchfor α 2 [3.0,3.25,...,5.0] 4.25
temperature(basemodel) [0.9,1.3,...,2.9] 2.5
anyinformationthatnamesoruniquelyidentifies
individual people. We find no examples and per- Table5: HyperparameterstestedandusedforMARCO
form no further anonymization. In addition, we onMAgr
In total, we sweep over 3×4×9×6 = 648 CondBERT Weperformabriefhyperparame-
hyperparameter combinations before choosing a tersearchandtrytwodifferentvaluesfortheCond-
bestsettorunonourtestset. Includingthissearch, BERT“numberofsubstitutewords”hyperparam-
weperformapproximately150,000rewrites. Since eter on each validation dataset. We choose the
100generationstakeabout30seconds,weuseap- hyperparameter that performs best on automatic
proximately12.5GPUhours. metrics,giveninTable8,andusethistogenerate
onthetestsets. SeeDaleetal.(2021)foradetailed
SBF Weperformasearchjointlyoverdifferent descriptionofthehyperparameter.
hyperparametervaluesonthedevelopmentset. We
choose the hyperparameter combination that per- Hyperparameter Tested Assignment
formsbestonautomaticmetrics,showninTable6,
numberofsubstitutewords 1,10 1
andusethistogenerateonthetestset.
Table 8: Hyperparameters tested and used for Cond-
BERT
Hyperparameter Tested Assignment
repetitionpenalty [1.0,1.2,1.5] 1.5
α 1 [0,0.5,1.0,1.5] 1.5 Includingourhyperparametersearch,werunap-
α 2 [3.0,3.25,...,5.0] 5.0 proximately7000rewritesacrossalldatasetsand
temperature(basemodel) [0.9,1.3,...,2.9] 2.9
splits. Given that 100 generations take approxi-
Table6: HyperparameterstestedandusedforMARCO mately30seconds,ourusageis0.6GPUhours.
onSBF CondBERT uses BERT-base, which includes
110Mparameters.
Asabove,wegoover648hyperparametercom-
ParaGeDi We use greedy decoding for Par-
binationsbeforechoosingabestsettorunonour
aGeDi and use the same hyperparameters as
testset. Intotal,werewriteapproximately65,000
MARCOforeachdataset,forfaircomparison. Ta-
sequences. Since 100 generations take about 30
ble9liststhesoleParaGedi-specifichyperparam-
seconds,weuseapproximately5.4GPUhours.
eter we modify: we do not generate and rerank
multiplesequencesforfairness.
DynaHate We perform a search jointly over
different hyperparameter values on the develop-
mentset. Wechoosethehyperparametercombina- Hyperparameter Assignment
tionthatperformsbestonautomaticmetrics,shown generatemultipleseqsandrerank false
inTable7,andusethistogenerateonthetestset.
Table9: HyperparametersusedforParaGeDi
Hyperparameter Tested Assignment
Weperformapproximately5000rewritesacross
repetitionpenalty [1.0,1.2,1.5] 1.0
α 1 [0.5,1.0,1.5] 1.5 alldatasetsandsplits. Giventhat100generations
α 2 [4.0,4.25,...,5.0] 4.75 take approximately one minute, our usage is 0.8
temperature(basemodel) [0.9,1.7,2.5] 2.5
GPUhours.
Table7: HyperparameterstestedandusedforMARCO ParaGediusesT5-baseasaparaphrasingmodel,
onDynaHate with220Mparameters,inconjunctionwithafine-
tunedGPT2-mediumdiscriminator,with355Mpa-
rameters.
Weiterateoverasmaller3×3×5×3 = 135
hyperparametercombinations,duetodatasetsize,
B.3 EvaluationMetrics
before choosing a final set to use on our test set.
In total, we rewrite approximately 240,000 texts. Toxicity To evaluate toxicity, we use the Per-
Since100generationstakeabout30seconds,we spective API, a publicly hosted toxicity classi-
useapproximately20GPUhours. fier trained on the Jigsaw corpus. Given a text,
themodeloutputsascalartoxicityscorebetween
Baselines Both of our baselines are available 0 and 1 inclusive. The model, which is lo-
onhttps://github.com/s-nlp/detoxasJupyterNote- cated at https://www.perspectiveapi.com/, is con-
books. WeadaptthemtoPythonfiles,runnablevia tinually updated and may change output over
thecommandline. Thereisnolicenseavailable. time. We query it in June, 2022, following
the API Terms of Service and intended use at evaluation. Theinterfaceforthisisthesameasour
https://developers.google.com/terms/. maintaskshowninFigure3,butwithsixsentences
insteadofone. Annotatorswhoansweratleastfive
Fluency We assess fluency by calculating the
outofsixquestionscorrectlyareapprovedandcan
perplexity of a text with an external, pretrained
work on the main task. We list the six examples
languagemodel. WeuseGPT2-base(Radfordetal.,
andcorrectanswersinTable11.
2019),foundathttps://huggingface.co/gpt2,with
Wepaidamedianwageof$8/hforthequalifi-
117Mparameters,anduseitundertheMITlicense
cationandthemaintask,whichisabovethemini-
anditsintendeduse.
mumwageandafairvalueforUSAandCanada.
We run this metric with a single NVIDIA
RTX6000GPU,whichtakesapproximately5sec- E DecodingwithProductofExperts
onds per 100 examples. With an estimate of
Hinton (2002) introduce the Product of Experts
450,000 texts processed, our usage for this met-
(PoE),anequationthatstatesgivennexperts:
ricis6.3GPUhours.
(cid:81)
Meaning Preservation We use BERTScore
p(d|θ ,...,θ ) =
mp m(d|θ m)
(1)
1 n (cid:80) (cid:81)
(Zhang et al., 2019), which outputs the co-
c
mp m(c|θ m)
sine distance between model sentence embed-
whereθ denotestheparametersofmodelm,dis
dings,tomeasurethemeaningsimilaritybetween m
somedatavector,p (d|θ )denotestheprobability
the original sentence and the rewrite. We use m m
ofdundermodelm,andciteratesoverallpossible
RoBERTa-large (Liu et al., 2019) as our model,
datavectors.
which has 354M parameters. We use the code
ApplyingthePoEtoautoregressivegeneration
locatedathttps://huggingface.co/spaces/evaluate-
equation, d represents a single token, p (d|θ )
metric/bertscore under the MIT License and its m m
representsthenexttoken-probabilityofdundera
intendeduse.
specificmodel,andciteratesoveralltokensinthe
We run this evaluation with a single NVIDIA
vocabularyV.
RTX6000 GPU, which takes approximately 15
Givenavectorx,thesoftmaxequationis:
seconds per 100 examples. With an estimate of
450,000textsprocessed,ourusageforthismetric exi
softmax(x )= fori=1,2,...,K
is18.7GPUhours. i (cid:80)K exj
j=1
B.4 TotalComputationalBudget
In the replacing step of MARCO, we perform
Summing up our computational usage from the the following ensembling of unnormalized log-
above sections, including finetuning the experts, probabilities(i.e.,logits)z ,z+,andz− fromthe
i i i
ourtotalcomputationalbudgetis106.1GPUhours. baseandexpertAE-LMsG,G+,andG−,respec-
tively,conditionedonthepreviouslygeneratedto-
C ExampleRewrites
kensg ,theoriginalsequencew,andthemasked
<i
variantwm toobtainthenext-tokenprobability:
Table 10 shows example generations from each
methodacrossallthreedatasets.
P(X |g ,w,wm)=softmax(z +α z+−α z−)
i <i i 1 i 2 i
D HumanEvaluationDetails
Considerasingleindexj correspondingtotoken
We use annotators from the USA and Canada on V inthesoftmaxoutput. Giventhateachofz ,z+,
j i i
AmazonMechanicalTurk,whovoluntarilyopt-in and z− are log-space probability vectors over V,
i
tothetask. Ourtaskwasapprovedbyourinstitu- theexponentiationofz +α z+−α z−inthesoft-
i 1 i 2 i
tion’sethicsreviewboard(IRB).Ascreenshotof maxequationconvertsthissumbacktoaproduct
ourinterfaceforthehumanevaluationisshownin in probability space, which matches the PoE nu-
Figure3. Ourinterfacedescribeshowtheannota- meratorforV . Inaddition,thedenominatorinthe
j
tors’datawillbeused. softmaxequationtransformsthelogitsumexactly
To gather annotations, we first recruit workers tothePoEdenominatorforV j. Thus,eachproba-
to do a qualification task, where annotators must bility fromthe output distribution from MARCO
answersixquestionsonwhichrewritefromapairis exactlymatchesEquation1andisthusaPoE.
lesstoxic,thesamequestionasinourmainhuman
Original ughexamssuck:’(#thisisgayerthanaids
CondBERT ’thefinalexamsinenglishlanguage:’ (#thisisgayerthanaids
ParaGeDi ughexams-’(#thisisgayerthanaids
MARCO: ughexamssuck:’(
Original Yourbrotherissuchagirl!
MAgr CondBERT myolderbrotherissuchaluckyone!
ParaGeDi thebrother’sgreat!
MARCO: Yourbrotherissuchagreatguy!
Original Ugh,Ihatelivinghere. There’ssomanyblackpeoplearound.
CondBERT "well,icouldbelivinghere. there’ssomanyothernicepeoplearound.
ParaGeDi I’vebeenlivinghereforalongtime.
MARCO: Ugh,Ihatelivinghere. There’ssomanypeoplearound.
Original What’sthedifference? AlltheseOrientalsarethesame.
CondBERT what’sthedifference? alltheseorientalsarethesame.
ParaGeDi allOrientalshavethesame.
MARCO: What’sthedifference? Allofthesethingsarethesame.
Original nofatsnofems
SBF CondBERT nodemons,nodemons.
ParaGeDi Nofatty,nofats
MARCO: nofatsnoadditives
Original ...becauseain’tnobodygottimetopickallthatcotton.
CondBERT ... becauseain’tnobodygottimetopickallthatcotton.
ParaGeDi Becausenobodyhastimetopickupallthecotton.
MARCO: ...becauseain’tnobodygottimetopickallthatup.
Original Iwouldn’teatfoodthathasbeentouchedbyamong
CondBERT iwouldn’tbeeatingfoodthathasbeentouchedbyamong
ParaGeDi Iwouldnoteatfoodtouchedbyamonk.
MARCO: Iwouldn’teatfoodthathasbeentouchedbyahumanbeing.
Original Ijustdon’tlikegaysandthatismychoice
DynaHate CondBERT ijustdon’tlikethem,andthatismychoice
ParaGeDi Ijustdon’tliketheGays.
MARCO: Ijustdon’tliketheideaandthatismychoice
Original Itistotallyvalidtosaythatblackpeopleareinferior
CondBERT itistotallyvalidtosaythatmanyofthesepeopleareinferior
ParaGeDi it’sentirelytruethattheBlackisaminority
MARCO: Itistotallyfairtosaythatsomepeoplearewrong.
Table10: Examplesofrewritesfromeachmethodonalldatasets’validationsets
Instructions
Decide which of the sentences best answers each question based on toxicity.
Participation restriction: providers/turkers for this task cannot currently be employed by or a student at the University of Washington.
Full Instructions (Expand/Collapse)
Instructions
Background
Thank you for participating in this survey! We have tasked you with selecting the rewritten text from our AI system on the
basis of toxicity compared to some original text. We feed an original sentence into an AI model and it tries to rewrite the We want to better understand the effectiveness of various
text such that it is less toxic/offensive while retaining as much meaning as possible. Please use your best judgment in rewriting methods by AI systems that can remove or
answering these survey questions. Note that some of this content may be difficult to read due to its toxic nature. reduce offensive/toxic content from text. The following
rewrites you see will compare the outputs of different
Given two generated rewrites as options for each prompt, please answer the questions below as defined on the following methods, so that we can understand the impact and
category: limitations of each one. As stated in the instructions,
Toxicity some of this content may be very difficult to read, but
A senten ce is toxic if it is rude, disrespectful, or unreasonable, and more likely to make you leave a discussion or hopefully you can help us in our mission to mitigate this
give up on sharing your perspective. phenomena in the real world!
Please note:
While we provide the option to rate the two rewrites as "equal" with respect to each category, please only choose
this option when there is truly no meaningful comparison to be made.
Task
Sentences:
original sentence:
Maybe you should let the real men deal with this problem, sweetie.
Rewrite A: Rewrite B:
Miss, maybe you should let the men deal with the construction. You should help them deal with the problem.
Q1: Which of these rewrites is less toxic?
If one of the completions is empty, select the non-empty one as less toxic.
A – Miss, maybe you should let the men deal with the construction.
B – You should help them deal with the problem.
equal –equally toxic or non-toxic
Figure3: AscreenshotofthehumanevaluationinterfaceonAmazonMechanicalTurk.
Original ithinkifpeopledon’tsucceedinthissociety,it’sbecausetheydidn’tworkhardenough.
RewriteA ifpeopledon’tsucceedinthissociety,it’sbecausetheydidn’tworkenough.
RewriteB ithinkifpeopledon’tsucceedinthissociety,it’sbecausetheydidn’tgetluckyenough.
Original whycan’tweinterviewyou? we’retryingtodosomethingforyourpeople.
RewriteA whycan’twedosomethingforyou? we’retryingtodosomethingforyou.
RewriteB we’retryingtodosomethingforyourpeople.
Original idon’tgetitwhyarepregnantwomenworking
RewriteA idon’tgetitwhyareyounot?
RewriteB idon’tunderstandwhypregnantwomenwork.
Original heyhoney,you’regoingthewrongdirection,theborder’sthatway!
RewriteA hmm... hey,honey,it’sgoingthewrongway,theborder’sthatway!
RewriteB hey,you’regoingthewrongdirection,theonethatis!
Original WhenIlookatyou,Idon’tseecolor
RewriteA Idon’tseecolorwhenIlookatyou.
RewriteB WhenIlookatyou,Igetdistracted.
Original You’vebeendoingwell. It’ssoinspiringthatyou’veovercomeyourdisability!
RewriteA You’vebeendoingdecently. It’ssoexciting!
RewriteB You’vebeendoingwellwithyourdisability. It’ssoinspiring!
Table11: ThesixrewriteexamplesusedinthedetoxificationqualificationtaskforworkersonMTurk. Theless
toxic,correctrewritesarelistedingreen.

The Devil is in the Errors: Leveraging Large Language Models for
Fine-grained Machine Translation Evaluation
PatrickFernandes∗,2,3,4 DanielDeutsch1 MaraFinkelstein1 ParkerRiley1
AndréF.T.Martins3,4,5 GrahamNeubig2,6
AnkushGarg1 JonathanH.Clark1 MarkusFreitag1 OrhanFirat1
1Google 2CarnegieMellonUniversity 3InstitutoSuperiorTécnico
4InstitutodeTelecomunicações 5Unbabel 6InspiredCognition
pfernand@cs.cmu.edu
Abstract
Source: “Avaliar tradução Candidate: “Evaluating
automática é difícil.” automatic translation are easy.”
Automaticevaluationofmachinetranslation Score Prediction
(MT)isacriticaltooldrivingtherapiditer-
Score the following translation from 0 to 100:
ative development of MT systems. While
Portuguese: {source}; English:{candidate}
considerableprogresshasbeenmadeonesti-
matingasinglescalarqualityscore,current
Score: 25
metricslacktheinformativenessofmorede-
tailed schemes that annotate individual er-
AᴜᴛᴏMQM
rors,suchasMultidimensionalQualityMet- Identify the errors in the translation
rics(MQM).Inthispaper,wehelpfillthis Portuguese: {source}; English:{candidate}
gap by proposing AUTOMQM, a prompt-
ingtechniquewhichleveragesthereasoning
Errors: ‘easy’ - major/accuracy; ‘are’ - minor/fluency
andin-contextlearningcapabilitiesoflarge
language models (LLMs) and asks them
MQM Score: -5x1(major) - 1x1(minor) = -6
to identify and categorize errors in transla-
tions. WestartbyevaluatingrecentLLMs,
suchasPaLMandPaLM-2,throughsimple
Figure 1: Illustration of how AUTOMQM uses
score prediction prompting, and we study LLMstoassessthequalityofatranslation. Rather
theimpactoflabeleddatathroughin-context thanaskingforasinglequalityscore,AUTOMQM
learning and finetuning. We then evaluate promptsmodelstoidentifyandclassifyerrors,and
AUTOMQMwithPaLM-2models,andwe usestheMQMframeworktoproduceascore.
findthatitimprovesperformancecompared
to just prompting for scores (with particu- metrics is rapidly evolving. Learned automatic
larly large gains for larger models) while metricsthatleveragehuman-judgmentstofinetune
providinginterpretabilitythrougherrorspans language models (Sellam et al., 2020; Rei et al.,
thatalignwithhumanannotations.
2022a)currentlyrepresentthestate-of-the-artinau-
tomaticevaluationbenchmarksliketheWMTMet-
1 Introduction ricstask(Freitagetal.,2022),andshowhighcorre-
lationwithhumanjudgments. However,thesemet-
Evaluatingnaturallanguagegenerationsystemshas
ricstypicallyoutputasingle,uninterpretablequal-
always been challenging, and as the output qual-
ityscore,makingitdifficulttounderstandthetype
ityofthesesystemshasimproved,evaluationhas
andextentoferrorsidentifiedbythem. Thelackof
becomeevenmorechallengingandcritical. Forex-
insightsmakesitdifficultformodeldevelopersto
ample,inMachineTranslation(MT),afieldwhere
leveragethesemetricstoimprovetheirsystems.
evaluationhasgarneredconsiderableattention,pre-
Unlike automatic metrics that only provide a
viousstandardautomaticsurface-levelmetricssuch
singlescalarvalueasqualityscore,state-of-the-art
asBLEU(Papinenietal.,2002)arebecomingless
human evaluation methodologies like Multidi-
reliable as the quality of generation systems im-
mensionalQualityMetrics(MQM;Lommeletal.,
proves, with little remaining correlation with hu-
2014; Freitag et al., 2021a) ask professional
manjudgments(Freitagetal.,2022).
annotators to identify and label error spans with
Tokeeppacewiththeconstantlyimprovingqual-
acategoryandseverity. Thismuchricherfeedback
ityofMToutput,thenextgenerationofautomatic
can be used to gain a better understanding of the
∗ Workdonewhileworkingpart-timeatGoogle. currentlimitationsofthemodelunderevaluation
3202
guA
41
]LC.sc[
1v68270.8032:viXra
andimproveit. • WearethefirsttoevaluateLLM-basedevalu-
In this paper, we ask whether large language ationmethodsonlow-resourcelanguagepairs.
models(LLMs)incombinationwithafewhuman Wefindthattheirperformanceispromising,
annotations can be used to design an automatic but lags behind state-of-the-art learned met-
metricthatgeneratesrichfeedbacksimilartothat rics.
generatedbyhumanexpertsinMQM.Thiswork
• Wefindthat,withAUTOMQM,PaLM-2mod-
is motivated by recent papers that demonstrated
els can be prompted to generate rich MQM-
thatLLMscanbeusedasautomaticmetrics(Liu
like annotations, outperforming their score
etal.,2023b)togenerateasinglequalityscore. In
predictioncounterpartsatthesegment-level.
particular,KocmiandFedermann(2023)showed
• Furthermore,annotationspredictedbyPaLM-
that LLMs can be prompted to assess the quality
2modelscorrectlyidentifyover50%ofwords
ofmachine-generatedtranslations,evenachieving
thatarepartofmajor errors,andarecompa-
state-of-the-artperformanceonassessingsystem-
rabletotheonesproducedbystate-of-the-art
level quality. However, previous work only pro-
supervised word-levelevaluators.
videsalimitedviewofthecapabilitiesofLLMsfor
machinetranslationevaluation: thefocushaspre- Our findings might have significant implica-
dominantlybeenonscoreprediction(i.e. predict- tions for not only MT evaluation, but evaluation
inganumericalvalueforquality),withoutconsid- ofmachine-generatedtextingeneral,andfurther
eringtheuseofanyannotateddata(eitherthrough highlight the potential of using LLMs to provide
in-contextlearningorfinetuning),andonlyinhigh- AIFeedback(Fernandesetal.,2023).
resourcelanguagepairs.
Weprovidealarge-scalestudyofthecapabilities 2 Background: MTEvaluation
of LLMs (from the PaLM and PaLM-2 families;
Machinetranslationevaluationisoneofthemost
Chowdheryetal.,2022;Aniletal.,2023)forma-
well-studiedevaluationproblemsinNLP(Callison-
chinetranslationevaluation(bothwithandwithout
Burchetal.,2008;Freitagetal.,2022). Inthistask,
areferencetranslation),provideanovelcompari-
given
sonbetweenpromptingandfinetuning,andinvesti-
gatetheperformanceinthelow-resourcescenario. 1. asourcesentenceina(source)language
InspiredbyfindingsthattheperformanceofLLMs
2. acandidatetranslationina(target)language
canbeimprovedbypromptingthemforrationales
of their predictions (Wei et al., 2022; Lu et al., an evaluation metric assesses the quality of the
2023), we also propose AUTOMQM, a prompt- candidate translation by how well it conveys the
ing technique for MT evaluation that asks LLMs meaningofthesourcesentencewhileconsidering
toidentifyerrorspansinatranslationandtoclas- other factors like fluency. Like many other natu-
sifytheseerrorsaccordingtotheMQMframework, rallanguagegenerationevaluationproblems,this
withaqualityscorederivedautomaticallyfromthe task is difficult because the set of correct transla-
identifiederrors. Akeyadvantageof AUTOMQM tionsforagivensourcesentenceisoftenverylarge
isitsinterpretability,asuserscaninspecttheerrors and not entirely known in advance. To simplify
responsibleforascore(Figure1). theproblemofmachinetranslationevaluation,of-
Ourcontributionscanbesummarizedasfollows: ten (3) a reference translation (typically created
byaprofessionalhumantranslator)isincludedas
• WeconfirmthefindingofKocmiandFeder-
additional information when assessing the candi-
mann(2023)thatLLMsarezero-shotstate-of-
date translation. This sub-problem is known as
the-artsystem-levelevaluators,butshowlow
reference-based evaluation(asopposedreference-
correlation with human judgment compared
lessevaluationorqualityestimation).
tolearned metricsatthesegment-level.
Upuntilrecently,humanevaluationofmachine
• We show that finetuning an LLM with hu- translationwascarriedoutpredominantlywiththe
manjudgmentmitigatesitslowsegment-level aim of assigning a single quality score to a can-
performance(particularlyforsmallerLLMs), didatetranslation. Consequently,learned metrics,
showingsimilarcorrelationswithhumanjudg- whichleveragecollectedhumanjudgmentdata,are
ment at both the system-level and segment- trainedforandevaluatedonthesametaskofscore
leveltostate-of-the-artlearnedmetrics. prediction(i.e.,assigningasinglequalityscoreto
acandidatetranslation),andcanachievehighcor- toautomaticallyscoretranslations. However,their
relationwithhuman-providedscores(Freitagetal., approachreliedonweakerencoder-onlyorencoder-
2022). decoderlanguagemodels,requiredsuperviseddata
However, framing machine translation evalu- towork,andoverallunderperformedothertopmet-
ation as a score prediction task is problematic: rics. WecompareagainsttheirMaTASemetricin
anyscoringorrankingoftranslationsisimplicitly ourexperiments. Luetal.(2023)showedthatdo-
basedonanidentificationoferrorsinthecandidate ingerroranalysis,apromptingtechniquesimilarto
translations,andaskingraterstosolelyprovidea AUTOMQM,couldleadtobetterChatGPT-based
singlescorecanleadtorushedandnoisyjudgments evaluators. However,theystillreliedontheLLM
(Freitagetal.,2021a). toprovideascoreonceitidentifiederrors(rather
This insight has led to the adoption of the thandoitautomaticallyusingsomethinglikethe
MultidimensionalQualityMetrics(MQM)frame- MQM framework). Furthermore, they provided
work (Lommel et al., 2014; Freitag et al., 2021a) a very limited meta-evaluation using only 40 ex-
asthegoldstandardforevaluatingmachinetransla- amples per language pair. Concurrently with our
tion. TheMQMframeworkaskshumanevaluators work,Xuetal.(2023)proposedINSTRUCTSCORE,
toidentifyerrorspansincandidatetranslationsand aLLaMA-basedevaluatorthatasksmodelstoiden-
classify those errors according to various dimen- tifyandcategorizeerrorsintranslation(aswellas
sions,e.g.,fluency,accuracy,... (seeAppendixA providinganaturallanguageexplanationforeach
foramoredetaileddescriptionofMQM).Impor- error). However, the authors only explore a 7B
tantly, the MQM framework does not ask anno- parametermodelanddon’tleveragezero-andfew-
tators to provide a quality score for each transla- shotcapabilitiesofmodelsasinthiswork. Instead,
tion, and instead derives one automatically from theyrelyonamorecomplexapproachofdistilling
theidentifiederrorspansandtheirclassifications. theknowledgeofamorecapableGPT-4LLM.
However,despiteitsrichness,mostautomaticmet- Additionally, WMT Word-Level Quality Esti-
ricsthatleverageMQMdataonlyusethefinalqual- mation shared tasks (Fonseca et al., 2019; Zerva
ityscoreproducedbytheframeworkanddiscard et al., 2022) leverage MQM data by converting
theerrorspaninformationandclassification. span-levelannotationsoferrors(normallyofma-
jor severity) to word-level tags and Task 2 in the
3 RelatedWork WMT19QualityEstimationsharedtaskevaluation
explicitlyevaluatedsubmissionsofspan-levelanno-
The success of learned machine translation met-
tations(althoughmostsubmissionsstillconsisted
rics(Sellametal.,2020;Reietal.,2022a;Freitag
of models that predicted word-level tags which
etal.,2022;Qinetal.,2022),whichfinetuneneu-
wereconvertedtospans). Wealsocompareagainst
ralnetworkmodelspretrainedonlargeamountsof
state-of-the-artword-levelqualityestimationmod-
(unsupervised)data,highlightedtheimportanceof
els.
leveragingtransferlearningtoachievemetricswith
bettercorrelationwithhumanjudgments. Morere- 4 UsingLLMstoPredictQualityScores
cently,generativeLLMs(OpenAI,2023;Aniletal.,
2023)haveconsistentlydemonstratedimpressive Recentworkshaveshownthatlargelanguagemod-
resultsinnaturallanguageunderstandingandzero- elsareversatile,general-purposemodelsthatcan
andfew-shottransferand,naturally,interestinem- be used to tackle many problems in NLP, includ-
ploying these models for (translation) evaluation ingevaluation(KocmiandFedermann,2023;Jain
hasincreased. KocmiandFedermann(2023)first etal.,2023;Liuetal.,2023b). Webeginbyexplor-
explored the use of GPT models for evaluating inghowLLMscanbeusedformachinetranslation
machinetranslationtasks,showingtheirpotential evaluationthroughscoreprediction.
aszero-shot evaluators, andothershavesinceex-
4.1 Prompting
tended GPT-based evaluation to other generation
problems(Jainetal.,2023;Liuetal.,2023b). We start by measuring how far we can push the
Perrellaetal.(2022)firsthighlightedthatMQM performance of LLMs with just prompting (Liu
annotationscouldbeleveragedtoallowpretrained etal.,2023a): bydefiningthetaskofMTevaluation
modelstopredictmajorandminorerrorsand,sim- and quality estimation as textual templates (with
ilarly to AUTOMQM, used the identified errors a general description of the problem and “slots”
for the inputs and outputs), we can use general- 4.2 Finetuning
purposeLLMstoperformthesetasksatinference-
IthaspreviouslybeenshownthatLLMsarecapa-
time,withoutanyparameterupdates.
bleofzero-shotevaluation(KocmiandFedermann,
Throughoutthepaper,wechoosetouseKocmi
2023),buttheextenttowhichfinetuningonhuman
and Federmann (2023)’s GEMBA-SQM prompt
judgmentdatacanfurtherboosttheperformanceof
(Figure2),whichasksmodelstogenerate(astring
LLMshasnotbeenstudied. IntheWMT’22Met-
representationof)ascorefrom0-100. Wechoose
ricsSharedTask(Freitagetal.,2022),alltopsub-
this prompt for two reasons: firstly, early explo-
missionswerelearnedmetrics;thatis,pretrained
rationswiththeirsandotherpromptsshowedthat modelsfinetunedonhumanjudgmentdata2.
this generally performed well. Secondly, using a
Thus, we investigate whether LLMs are
singlepromptensuresafairercomparisonbetween
amenable to finetuning on human judgment data.
thecapabilitiesofdifferentmodels.1
LLMs used in top-performing metrics are gener-
allymuchlargerthanthepretrainedlanguagemod-
Score the following translation from elsleveragedbypreviouslearnedmetrics(which
{src_lang} to {tgt_lang} with respect
generally have fewer than 1 billion parameters).
to the human reference on a continuous
Moreover,mostlearnedmetricsleveragepretrained
scale from 0 to 100 that starts with
"No meaning preserved", goes through encoder-onlyratherthan(decoder-only)prefixlan-
"Some meaning preserved", then "Most guage models. We experiment with finetuning
meaning preserved and few grammar mistakes",
LLMsusingtwoobjectives:
up to "Perfect meaning and grammar".
• Regression(R):Commonlyusedfortraining
{src_lang} source: "{source}"
{tgt_lang} human reference: "{reference}" learned metrics (Rei et al., 2022a), the ob-
{tgt_lang} translation: "{candidate}" jective here is a regression loss (e.g., mean
Score (0-100): {score}
squarederror)betweencontinuousscoresob-
Figure2: Thescorepredictionpromptusedinthispaper. tained from the model (for example, with a
Equivalent to the GEMBA-SQM prompt in Kocmi and regressionhead)andthehumanscores.
Federmann (2023). Parts in purple are only included
• Generative Classification (GC): We bucket
for reference-based evaluation, while parts in orange
scoresintodiscreteclasses(see§6.1)andtreat
representslotsforoutputsandareonlyincludedforin-
contextexamples. theMTevaluationtaskasatext-to-textclassi-
ficationproblem(Raffeletal.,2020).
In-ContextLearning Asurprisingemergentca-
pability of LLMs is their ability to improve on 5 UsingLLMstoPredictErrorSpans
prompting-based tasks by including a very small
amountoflabeleddataaspartoftheprompt/con- Whileproducingqualityscoresthatcorrelatewith
text(Brownetal.,2020)andwithoutparameterup- human judgments is an important part of transla-
dates,atechniquecalledin-contextlearning(ICL). tionqualityassessment,metricsthatsolelydoscore
We thus investigate the impact that ICL has on predictionsufferfromproblemsofinterpretabil-
LLMs’abilitytoassesstranslationquality. Recent ity: ifametricassignsalowscore,thedownstream
workshaveshownthattheimpactofICListightly usersareleftinthedarkaboutwhichpartsofthe
tiedwiththeexactexamplesincludedintheprompt, translationwereresponsibleforthescoreandthus
withapoorselectionprocedureleadingtonoim- needtobecorrected. Thisisespeciallyproblematic
provements or even worse performance than the incaseswherethemetricassignsawrongscoreto
zero-shotcase(Jainetal.,2023). Wethereforeex- atranslation,asitismuchhardertodiagnosewhy
ploretwosamplingapproachestoselectin-context the evaluation model made a mistake, and iden-
examplesfromapre-defined“pool”oftranslation tifyandpreventsimilarmistakesinthefuture. In
qualityassessments: uniformsamplingandstrati- fact,reducingtranslationqualitytoasinglescore
fiedsampling,wheretheexamplepoolisbucketed hasprovenproblematicevenforhumanannotators:
by score ranges and examples are sampled from asking raters to solely provide a single score can
eachbucket. leadtorushedandnoisyjudgments(Freitagetal.,
1Whilethispromptwasn’tthebestforsystem-level,itled 2Whilethesemetricsallleveragepowerfulpretrained(lan-
tothebestsegment-levelperformanceinGEMBA. guage)models,thesegenerallyaren’tconsideredLLMs
Based on the given source and reference, identify the major and minor errors in this
translation. Note that Major errors refer to actual translation or grammatical errors,
and Minor errors refer to smaller imperfections, and purely subjective opinions about
the translation.
{src_lang} source: "{source}"
{tgt_lang} human reference: "{reference}"
{tgt_lang} translation: "{candidate}"
Errors: {error1:span} - {error1:severity}/{error1:category}; {error2:span} - ...
Figure3: TheAUTOMQMpromptusedinthispaper. Partsinpurpleareonlyincludedforreference-based
evaluation,whilepartsinorangerepresentslotsforoutputs,andareonlyincludedforin-contextexamples.
2021a) and the current gold standard for transla- LP #Sys #Seg
LP #Sys #Seg
tionqualityevaluationinvolvinghumanannotators
en→kk 11 998
isinsteadbasedonmethodologiesliketheMQM en→de 13 1315 kk→en 11 1000
zh→en 14 1875
framework(see§2),whichprovidericherfeedback en→gu 11 998
en→ru 15 1315
byidentifyingerrorspans,categorizingthem,and gu→en 11 1016
evaluatingtheirseverity. Table 1: The number of systems and segments that
Interestingly,anotheremergentphenomenonin haveMQMscores(left)andDAscores(right)usedas
LLMsisthesuccessofchain-of-thought prompt- ground-truthinthiswork.
ing (Wei et al., 2022): when defining a prompt
tiveordifficulttoparse. Thusweonlyconsiderthe
for a particular task, if we instruct the model to
produce a series of intermediate reasoning steps
AUTOMQM taskinthefew-shotscenario. Based
(“let’s think step-by-step”), it tends to generate onthefindingsfrom§6.2, weexploretheimpact
a free-text rationale before generating an output, ofin-contextlearningbysamplingfromtheexam-
plepoolusingstratifiedsamplingextendedwitha
and this often improves the performance on the
set of rejection criteria (Appendix B), which en-
taskathand(Liuetal.,2023b). Furthermore,this
chain-of-thoughtpromptingcanbeusedtoobtain sures that the example set has a balance between
structured rationalesfromLLMs,andthiscanlead majorandminorerrorsaswellasdiversityinthe
categoriesoferrors.
tobetterperformancethanwithfree-textrationales
(Luetal.,2023).
6 Experiments
Motivated by these findings, we propose
AUTOMQM, a prompting technique for transla- 6.1 ExperimentalSetup
tionqualityassessmentthatinstructsLLMstoiden-
Data The metrics in this work are evaluated
tifyerrorsinatranslation,andcategorizethetypeof
onbothhigh-resourceandlow-resourcelanguage
erroraccordingtotheMQMframework(Lommel
pairs. Thethreehigh-resourcelanguagepairscome
etal.,2014). Furthermore,wedon’taskthemodel
from the WMT’22 Metrics Shared Task (Freitag
to produce a score, as the MQM framework pro-
et al., 2022): en→de, zh→en, and en→ru. The
videsanalgorithmicproceduretoobtainonefrom
ground-truthtranslationqualityscoresarederived
identifiederrors: thetotalscoreisthesumofpenal-
from MQM ratings in which expert annotators
tiesforallerrorsidentified,where(roughly)major
markederrorspansinthetranslationswithdifferent
errorsgetpenalizedwith−5andminorswith−1
severitylevelswhichareautomaticallyconverted
(seeAppendixAforamoredetaileddescriptionof
toanumericscore(see§2). Thefourlow-resource
thescoringalgorithm).3 Figure3showsthemain
language pairs come from the WMT’19 Metrics
AUTOMQM promptusedinthispaper.
SharedTask(Maetal.,2019): en↔guanden↔kk.
Importantly,obtainingmeaningfulAUTOMQM
SinceMQMratingsarenotavailableforthelow-
resultsinazero-shotsettingisasubstantiallymore
resourcepairs,thegroundtruthqualityscoresare
challengingtaskcomparedtoscoreprediction: we
directassessment(DA)scores. DAscoresarequal-
foundthat,withoutanyin-contextexamples,LLMs
ityassessmentsassignedbynon-expertratersona
tendtoproduceoutputsthatareeitheruninforma-
scalefrom0-100,thennormalizedperrater. SeeTa-
3Thisissimilartomethodsthatleverageexternalexecutors ble1forstatisticsaboutthenumberofMTsystems
toimprovetheperformanceofLLMs(Gaoetal.,2022) andsegmentsforeverylanguagepair.
Additionally,inourexperiments, AUTOMQM unabletoobtainthespan-levelpredictionsforthe
required in-context examples with MQM anno- MATESEsubmission,wealsocompareagainstthe
tations to work, so we restrict our evaluation of topsubmissiontotheWMT’22Word-LevelQual-
AUTOMQM toen→deandzh→enbecausethere ity Estimation Shared Task (Zerva et al., 2021):
areavailableMQMratingsfromtheWMT’21Met- word-level COMETKIWI (COMET-WL)(Reietal.,
ricsSharedTask(Freitagetal.,2021b)thatwecan 2022b),alsobasedonanXLM-Rmodeltrainedon
useasin-contextlearningexamplepools. acombinationofsentence-andword-leveldata. To
doso,were-runthismodelontheWMT’22Met-
Models Webasemostofourexperimentsonthe
rics Shared Task data, and convert the predicted
followingLLMs:
word-levelOK/BADtagsintospans.5
• PaLM: A 540 billion parameter autoregres-
Finetuning Forregressionfinetuning,weusea
siveTransformermodeltrainedon780billion
real-valuedlogit,extractedfromafixedindexinthe
tokensofhigh-qualitytext(Chowdheryetal.,
firsttargettoken’slogitvector,asthequalitysignal.
2022). Itshowedremarkableperformanceon
(Inparticular,weleverageaspecial,unused,vocab-
awide-rangeofNLPtasks,includingMachine
ularytoken.) Thiswasthetechniqueusedtotrain
Translation(Vilaretal.,2022).
MetricX-XXLintheWMT2022SharedTasksub-
mission(Freitagetal.,2022). Theregression-based
• PaLM-2: The successor to PaLM, the
modelwastrainedonWMTdirectassessment(DA)
PaLM-2 family of LLMs (Anil et al., 2023)
datafromtheyears2015through2020.
buildsuponrecentresearchinsights,suchas
For generative classification, we bucket the
compute-optimalscaling,amoremultilingual
scoresinthetrainingdataintofiveclasses,where
anddiversepre-trainingmixture,andarchitec-
class boundaries are assigned so that each class
tural/optimizationimprovements. Wemainly
containsanequalnumberoftrainingexamples. We
usetwomodelsizesinthefamily: PaLM-2BI-
SON and(thelarger)PaLM-2-UNICORN.4 In thenmaplabelstoverbalratingsfromthefollow-
ingset,basedontheirbucket: ["verybad","bad",
additionweexploretheimpactofinstruction-
"ok","good","verygood"]. Toevaluatethemodel,
tuningbyusinga UNICORNmodelfinetuned
predictionsaremappedbacktointegerlabelsfrom
ontheFLANdataset(Weietal.,2021).
1to5. Anypredictionsnotcontainingasubstringin
For score prediction, we compare PaLM and thelabelsetareconsideredinvalidandaremapped
PaLM-2againsttheGPTfamilyofLLMs(Brown to0. WeexperimentedwithfinetuningonbothDA
et al., 2020; OpenAI, 2023) by leveraging the and MQM 2020 (Freitag et al., 2021a) data, and
results and outputs from the GEMBA evaluator foundthatthelatterperformedslightlybetter.
(KocmiandFedermann,2023). Wethenevaluate To assess the impact of model size, we also
theperformanceofAUTOMQMwithonlyPaLM-2 finetunetwoadditional(smaller)PaLM-2models,
models(whichperformedbestinscoreprediction). whichwecallS andM,comparingtheirfinetuned
Additionally, for the high-resource languages, andzero-shotperformance.6
we compare to a set of strong baseline eval-
MetricMeta-Evaluation Thequalityofanau-
uation metrics, MetricX-XXL and COMET-22,
tomaticevaluationmetricisestimatedbycompar-
whichwerethetwotop-performingmetricsinthe
ing the agreement between the metric scores and
WMT’22MetricsSharedTask. MetricX-XXLand
ground-truth quality scores on a large number of
COMET-22arebothfinetunedregressionmodels
translationsfromdifferentMTsystems,aprocess
trainedonDAdatafromWMTthatareinitialized
known as metric meta-evaluation. This work re-
withmT5(Xueetal.,2021)andXLM-R(Conneau
portsthreedifferentagreementscores,asfollows.
etal.,2020),respectively.
Thefirstissystem-levelaccuracy,whichcalcu-
Forthe AUTOMQM experiments,wealsocom-
latesthepercentofsystempairsthatarerankedthe
pareagainst MATESE,acomparablesubmission
samebythemetricandground-truthscores,micro-
totheWMT’22MetricsSharedtaskthatfinetuned
aXLM-Rmodeltoidentifymajorandminorerrors, 5Weconsideraspanasanymaximalconsecutivesequence
andcomputedascoreautomatically. Sincewewere of words marked as BAD, assigning every span the major
severity.
4InformationaboutexactnumberofparametersofPaLM-2 6Weuseasmallvariationofthezero-shotprompt,asking
modelsisnotpubliclyavailable. modelsforscoresfromthesame5bucketsusedinfinetuning.
averagedoverasetoflanguagepairs(Kocmietal., Intuitively,wecareforoverallprecision(regard-
2021). System-levelscoresaredefinedastheaver- less of severity) since we want to make sure pre-
agescoreacrossallsegments. dicted errors tend to be marked by annotators as
At the segment-level, the standard correlation well,butforrecallwecaremostlyformajorerrors,
thatisreportedbyWMTisKendall’sτ. However, as these have a larger impact on translation qual-
recentworkpointedoutproblemswithKendall’sτ ityandaremorecriticaltoidentify. Additionally,
withrespecttoties(Deutschetal.,2023). Inshort, wealsoreportthe(3)MatthewsCorrelationCoeffi-
differentvariantsofτ areinconsistentwithrespect cient(MCC),oneoftheofficialmetricsintheword-
totiesandevenbiasedagainstmetricsthatpredict levelqualityestimationtasks(Zervaetal.,2022).
ties,asourmetricsdointhiswork. Deutschetal.
6.2 Results
(2023)recommendreportingapairwiseaccuracy
score,whichrewardsmetricsforcorrectlyranking 6.2.1 ScorePrediction
translationsaswellascorrectlypredictingties,in Table2summarizesthemeta-evaluationresults,at
combination with atie calibration procedure that thesystemandsegmentlevel,forboththezero-shot
automaticallyintroducestiesintometricscoresso promptingandfinetuningsettings.
that the meta-evaluation is fairer. This accuracy
score,denotedacc∗,rangesbetween0and1,and Prompting Afirstobservationisalmostallzero-
arandommetricwouldachieve33%accuracy. We shotLLMevaluatorshavehighersystem-levelper-
reportthe“group-by-item”variantofthepairwise formance than learned metrics (with and without
accuracy score from Deutsch et al. (2023) in ad- references), with PaLM 540B and PaLM-2 UNI-
dition to Pearson’s ρ, a complementary signal to CORNachievingthebestperformance. Attheseg-
rank-basedcorrelationsthatmeasurethestrengthof mentlevel,thestoryismorecomplicated: similarly
thelinearrelationshipbetweentwovariables(and to Kocmi et al. (2022), we find that none of the
oneofthestandardcorrelationsreportedinWMT). LLMs we explored was able to consistently out-
performthebaselinelearnedmetrics. Weseethat
SpanMeta-Evaluation Since AUTOMQM pro-
PaLM-540Bisaparticularlypoorreference-based
vides not only scores but also the identified error
evaluator,whichissurprisinggivenitssystem-level
spans,wecancomparethepredictedspanswiththe
performance. Unexpectedly, instruction-tuning
errorsmarkedbyannotatorsintheMQMannota-
with FLAN seems to degrade performance, with
tions. Weevaluatequalityofpredictedspansusing:
FLAN-PaLM-2 UNICORN achievingpoorperfor-
(1)SpanPrecision(SP),whichmeasurestheover- manceatboththesystemandsegmentlevels.7
lapofpredictedspansandgold(annotated)spans;
Nevertheless,PaLM-2modelsachievehighcor-
and(2)Majorrecall(MR),whichcapturestheper-
relationswithhumanjudgments,andthereference-
centageofgoldmajorerrorsthatwerepredictedas
lessPaLM-2BISONiscompetitivewiththelearned
errors(eitherminorormajor).
baselines,particularlyatassessingalternativetrans-
Moreformally,considerthesetofgroundtruth lations of the same sentence (acc∗). When com-
spansS⋆,whereeachspanconsistsofasequenceof
paringPaLM-2modelswithKocmietal.(2022)’s
words, i.e., s = (w ,w ,···). Let S⋆ ⊆
i (a) (a+1) maj GPT-based GEMBA evaluator (Table 3), we see
S⋆ bethesubsetcontainingonlythemajorerrors.
that both families of LLMs perform similarly,
Given a span set S, we define its positional set
with PaLM-2 models exhibiting higher system-
P(S)asthesetcontainingthepositionsofallthe
levelperformancethanGPT-basedGEMBA,while
wordsineveryspaninS. Forexample,assuminga
GEMBA achieves better segment-level accuracy,
spans = (w ,w ,···)inS startsatthenth
i (n) (n+1) particularlyinthereference-lesssetting.
positioninthetext,itscorrespondingpositionalset
Figure 4 shows the distribution of scores pro-
willincludethepositions{n,n+1,...,n+len(s )−
i duced by PaLM- and PaLM-2-based evaluators.
1}. Then for a set of predicted spans Sˆ, SP and
Wefindthat,despitebeingpromptedtogiveascore
MRaredefinedas:
inthe0-100range,thesemodelsalmostalwaysout-
|P(Sˆ)∩P(S⋆)|
putoneofaverylimitedsetofscores(e.g. 0,50,
SP(Sˆ) = (1)
|P(Sˆ)| 90, 95). Given Kocmi and Federmann (2023)’s
|P(Sˆ)∩P(S⋆ )| 7NotethatthismightbeaproblemwiththeFLANdataset
MR(Sˆ) = |P(S⋆ )m | aj (2) andnotinstruction-tuningingeneral,astheGPTmodelsare
maj alsoinstruction-tunedandperformwell.
System-Level Segment-Level
All(3LPs) EN-DE ZH-EN EN-RU
Model Ref? Accuracy ρ acc⋆ ρ acc⋆ ρ acc⋆
Baselines
MetricX-XXL ✓ 85.0% 0.549 61.1% 0.581 54.6% 0.495 60.6%
COMET-22 ✓ 83.9% 0.512 60.2% 0.585 54.1% 0.469 57.7%
COMET-QE ✗ 78.1% 0.419 56.3% 0.505 48.8% 0.439 53.4%
Prompting
PaLM540B ✓ 90.1% 0.247 55.4% 0.255 48.5% 0.180 48.6%
PaLM-2BISON ✓ 88.7% 0.394 56.8% 0.322 49.3% 0.322 52.8%
PaLM-2UNICORN ✓ 90.1% 0.401 56.3% 0.349 51.1% 0.352 55.3%
FLAN-PaLM-2UNICORN ✓ 75.9% 0.197 55.6% 0.139 46.1% 0.198 52.0%
PaLM540B ✗ 84.3% 0.239 56.1% 0.270 43.1% 0.300 51.8%
PaLM-2BISON ✗ 85.0% 0.355 57.0% 0.299 48.6% 0.303 53.1%
PaLM-2UNICORN ✗ 84.3% 0.275 56.1% 0.252 48.3% 0.209 49.8%
FLAN-PaLM-2UNICORN ✗ 69.7% 0.116 54.6% 0.112 43.8% 0.156 47.8%
Finetune
PaLM-2BISON(R) ✓ 88.0% 0.511 61.0% 0.459 51.5% 0.458 59.5%
PaLM-2BISON(GC) ✓ 86.1% 0.400 59.2% 0.444 49.3% 0.365 56.0%
PaLM-2UNICORN(R) ✓ 87.6% 0.508 61.1% 0.412 52.6% 0.460 60.4%
PaLM2BISON(R) ✗ 87.6% 0.490 59.9% 0.439 53.4% 0.437 59.2%
PaLM2BISON(GC) ✗ 86.1% 0.368 57.5% 0.420 47.3% 0.390 54.9%
PaLM2UNICORN(GC) ✗ 86.1% 0.407 57.9% 0.402 45.6% 0.411 55.3%
Table 2: Meta-evaluation results at system and segment-level for the high-resource language pairs.
Finetuned(R)and(GC)representtheregressionandgenerativeclassificationobjectives(§4.2). ✓and✗
representreference-based andreference-lessmetrics,respectively.
System Segmentacc⋆
Gold MQM (+100)
Model Ref? All EN-DE ZH-EN EN-RU 10000 PaLM-2 (Unicorn)
PaLM (540B)
GEMBA FLAN-PaLM-2 (Unicorn)
GPT-3.5 ✓ 85.4% 54.9% 49.5% 47.5% 1000
GPT-4 ✓ 88.7% 57.8% 52.6% 55.0%
GPT-3.5 ✗ 82.5% 56.1% 49.7% 49.3% 100
GPT-4 ✗ 89.1% 56.4% 53.4% 54.8%
BISON ✓ 88.7% 56.8% 49.3% 52.8% 10
UNICORN ✓ 90.1% 56.3% 51.1% 55.3%
BISON ✗ 85.0% 57.0% 48.6% 53.1% 1
UNICORN ✗ 84.3% 56.1% 48.3% 49.8% Error 0 5 10 20 50 60 70 80 85 90 95100
Score
Table3: ComparisonbetweenPaLM-2andGPT-based Figure 4: Distribution of scores for various LLM
GEMBA(Kocmietal.,2022)atthesystemandsegment reference-basedevaluators,ontheEN-DEtestset. Note
levelsforthehigh-resourcelanguagepairs. thattheyaxisisinlog-scale.
totheirlargerfinetunedcounterparts.
similar findings with GPT models, it seems that
thisisaconsequenceofthepretrainingobjective. In-contextLearning Figure6showsthemean
andinterquartilerange(IQR)oftheperformance
Finetuning Despite their already-great perfor- asweincreasethenumberofin-contextexamples
mance in the zero-shot setting, we find that fine- k(with100examplesetsperk)sampledwithstrat-
tuningLLMscanfurtherimproveLLMevaluators’ ified sampling(seeAppendixCforuniform). Sur-
segment-levelscores. Thisisparticularlyobvious prisingly, despite evidence of the benefits of in-
forthereference-lessevaluators,whereafinetuned context learning for many tasks, we found that
PaLM-2 BISON achieves state-of-the-art perfor- including in-context examples during evaluation
mance in segment-level correlations and compa- (almost) never led to better performance, either
rable system-level accuracy across all language withuniformorstratified sampling.
pairs. Moreover, when we look at how perfor- Toinvestigatethecauseofthisdisappointingper-
mancescaleswithparametercount(Figure5),we formance,welookedathowparticularin-context
observeaninterestingtrend: whilesmallermodels examplesetsaffectthedistributionofscorespro-
arenotcapableofbeingeffectivezero-shotevalu- ducedbyLLM-basedevaluators. Figure7shows
ators,finetuningthemleadstocompetitiveperfor- the distribution of scores over the whole test set
mance,andonlyaslightdecreasewhencompared for the 1-shot and 2-shot settings, with different
)ed-ne(
secnerrucco
#
0.5 Score: 79.0
10000 Score: 94.0
0.4 Score: 99.0
1000
0.3
100
0.2
0.1 10
Zero-Shot
0.0 Finetune 1
Error 0 5 10 20 50 60 70 80 85 90 95100
S M Bison Score
Model
Figure5: BehaviorofPearsonaswescaletheLLM’s
Score: 85.0+95.0
parameter count. Note that the x axis is not to-scale 10000 Score: 99.0+90.0
withregardtoparametercount.
1000
0.40 PaLM-2 (Bison) ref-based 100
PaLM-2 (Bison) ref-free
0.35
10
0.30
0.25 Error 0 5 10 20 50 Scor6 e0 70 80 85 90 95100
0.20 Figure7: DistributionofscoresforPaLM-2(BISON)
0.15 models for 1-shot (top) and 2-shot (bottom) setups,
withvariousin-contextlearningsetsforeach(andtheir
0.10
scoresinthelegend)
0 1 2 3 4
# of in-context examples
Figure 6: Mean Pearson and its interquartile range
System Segmentρ
(IQR)intheWMT22EN-DEtestset,asweincreasethe
Model Ref? All EN-KKEN-GUKK-ENGU-EN
numberofin-contextexampleswithstratifiedsampling
Baseline
MetricX-XXL⋆ ✓ 94.0% 0.666 0.701 0.539 0.409
in-context examples sets. We can see that output Prompting
distribution isheavilybiased bythe scoresin the BISON ✓ 92.2% 0.605 0.540 0.462 0.339
UNICORN ✓ 87.4% 0.609 0.621 0.495 0.384
in-contextexamples: despitenever predicting79 BISON ✗ 89.8% 0.567 0.478 0.381 0.313
in the zero-shot setting, when a single example UNICORN ✗ 84.4% 0.536 0.523 0.433 0.334
with that score is included, it starts to dominate
the model predictions. This seems to hint that
Table4: Meta-evaluationresultsforsystem-levelaccu-
racyandsegment-levelPearsononthelow-resourcelan-
LLMs“overfit”tothespecificscoresprovidedas
guages,usingPaLM-2forscoreprediction. ⋆Notethat
examples,ratherthangeneralizingtothebroader
thebaselineisslightlydifferentfromthehigh-resource
evaluationtask,whichcouldexplainthelackluster
case,beingtrainedonthesamedatabutwithoutthese
performanceofin-contextlearning. low-resourcelanguagepairs.
6.3 LowResourceLanguages
k). Contrarytotheperformancewithscorepredic-
Table 4 shows the performance of PaLM-2 mod- tion, wefindthatperformancewith AUTOMQM
els at score prediction for low-resource transla- seems to (mostly) scale with the number of in-
tion. Overall,wefindthatsimilartohigh-resource contextexamples: performanceincreasesmonoton-
LPs, these models are good zero-shot evaluators, icallywithupto4in-contextexamplesandplateaus
with system-level accuracies around 90%. How- thereafter. Additionally,thevarianceacrossthein-
ever,zero-shot LLMsunderperformlearned met- contextlearningsetsseemstobelower,withmost
rics,evenwhenthesemetricsalsoweren’texposed examplesetsexhibitinglessthan0.05Pearsondif-
todataintheselow-resourcelanguages. ferencefromthebest-performingsets. Allthissug-
geststhatLLMevaluatorsaremuchmorerobustto
6.3.1 AUTOMQM
thechoiceofin-contextexampleswhenprompted
Figure 8 shows the mean and interquartile range for AUTOMQM rather than for score prediction.
(IQR)oftheperformanceofPaLM-2BISONwith Wealsofindthatthebehaviorofin-contextlearn-
AUTOMQM, as we increase the number of in- ing is quite similar for both reference-based and
contextexamples(again,with100examplesetsper reference-lessevaluationtasks. Finally,weobserve
)ED-NE(
nosraeP
nosraeP
)ed-ne(
secnerrucco
#
)ed-ne(
secnerrucco
#
PaLM-2 (Bison) 0.35 PaLM-2 (Bison)
0.30 PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free
0.30
0.25
0.25
0.20
0.20
0.15
0.15
0.10 0.10
0.05 0.05
0.00 0.00
0 1 2 3 4 5 6 0 1 2 3 4 5 6
# of in-context examples # of in-context examples
Figure8: MeanPearsonanditsinterquartilerange(IQR),asweincreasethenumberofin-contextexamples
inthe AUTOMQM prompt,forEN-DE(left)andZH-EN(right).
System-Level Segment-Level
All(2LPs) EN-DE ZH-EN
Model Ref? Accuracy ρ acc⋆ ρ acc⋆
Baselines
MetricX-XXL ✓ 81.1% 0.549 61.1% 0.581 54.6%
MATESE ✓ 79.9% 0.391 58.8% 0.528 51.5%
COMET-QE ✗ 76.9% 0.419 56.3% 0.505 48.8%
MATESE-QE ✗ 73.4% 0.298 57.9% 0.468 50.1%
COMET-WL ✗ 71.6% 0.418 57.1% 0.406 51.5%
ScorePrediction
PaLM-2BISON ✓ 86.4% 0.394 56.8% 0.322 49.3%
PaLM-2UNICORN ✓ 86.4% 0.401 56.3% 0.349 51.1%
PaLM-2BISON ✗ 84.0% 0.355 57.0% 0.299 48.6%
PaLM-2UNICORN ✗ 80.5% 0.275 56.1% 0.252 48.3%
AutoMQM
PaLM-2BISON ✓ 84.0% 0.369 59.2% 0.355 48.4%
PaLM-2UNICORN ✓ 87.6% 0.432 59.1% 0.442 51.8%
PaLM2BISON ✗ 87.6% 0.297 55.2% 0.331 48.0%
PaLM2UNICORN ✗ 83.4% 0.368 56.4% 0.429 50.2%
Table5: Meta-evaluationresultsforPaLM-2modelsusingAutoMQM andscoreprediction,atthesystem
andsegmentlevelsformultiplelanguagepairs.
thattheexamplesetsthatperformwellforonetask Table 5 shows the meta-evaluation results for
generallyworkwellfortheother,withperformance PaLM-2 BISON and UNICORN prompted with
onbothsettingsgivenafixedin-contextsetbeing AUTOMQM(usingthebest-performingin-context
highlycorrelated,asshowninFigure9. learningsetsinFigure8). Foreaseofcomparison,
wealsoreporttheirperformancewhenprompted
for score prediction, as well as the performance
of the baselines. Overall, prompting LLMs with
Corr = 0.941
6 AUTOMQM seemstoleadtosignificantimprove-
0.35 ments in evaluating machine translation quality,
0.30 5 particularlyforlargermodels: UNICORNachieves
0.25 better performance (across all meta evaluations)
4
0.20 with it than when prompted for score prediction,
0.15 and itsreference-less version iscompetitive with
3
0.10 the best learned metric even at the segment level.
0.05 2 However, for the smaller BISON, the benefits of
0.00 AUTOMQM are less clear, with both techniques
1 performing comparably. This hints that scale is
0.0 0.1 0.2 0.3
Pearson w/o Reference necessaryforzero-andfew-shotfine-grainedevalu-
Figure 9: Scatter plot of the Pearson of PaLM-2 ation(likewithAUTOMQM).Wealsofindthatthe
(BISON)models,with/withoutincludingthereference
intheprompt,foreachin-contextlearningsettingtried.
ecnerefeR
htiw
nosraeP
)ED-NE(
nosraeP
selpmaxE
fo
rebmuN
)NE-HZ(
nosraeP
distributionofscoresproducedbyLLMsprompted LLMs (despite only leveraging a handful of an-
withAUTOMQMismuchclosertothegoldMQM notations).
distribution,withmodelsoutputtingamuchlarger
7 Conclusion
setofscores,andinthesamerangesasannotators
do(seeFigure10).
In this study, we have systematically investi-
gated the capabilities of large language models
Gold MQM for machine translation evaluation through score
10000 PaLM-2 (Bison)
PaLM-2 (Unicorn)
prediction, and proposed AUTOMQM, a novel
prompting technique that leverages the Multidi-
1000
mensionalQualityMetrics(MQM)frameworkfor
100 interpretableMTevaluationusingLLMs.
WedemonstratedthatjustpromptingLLMsfor
10 score prediction leads to state-of-the-art system-
level evaluators, but still falls short of the best
1
learned metrics at the segment-level (with fine-
40 35 30 25 20 15 10 5 0
Score tuning being necessary to close this gap). Then
Figure10: DistributionofscoresforPaLM-2models weshowedthatAUTOMQMcanfurtherimprove
usingAUTOMQM,onWMT22EN-DE theperformanceofLLMswithoutfinetuningwhile
providinginterpretabilitythrougherrorspansthat
alignwithhumanannotations.
EN-DE ZH-EN
OurfindingssurroundingfinetuningLLMsfor
Model R? SP MR MCC SP MR MCC score prediction hint that LLMs’ performance in
Baselines machinetranslationevaluationcouldbefurtherim-
COMET-WL ✗ 0.2670.2500.1610.3640.1780.152
provedbyfinetuningthesemodelsonfine-grained
AutoMQM humanjudgmentdata(likeMQM)andisadirec-
BISON ✓ 0.0950.7490.0600.2520.2550.109
tion we are actively pursuing. Additionally, the
UNICORN ✓ 0.1750.6280.1930.2380.4760.143
BISON ✗ 0.1190.5200.0920.2240.3110.091 general-purpose nature of LLMs may enable the
UNICORN ✗ 0.1500.5800.1500.2290.4880.133 applicationofsimilarpromptingtechniques(lever-
aging some fine-grained evaluation schemes) to
Table 6: Span-level meta-evaluation on WMT22 for otherevaluationproblems(Wuetal.,2023).
PaLM-2modelsusingAutoMQM.SRandMRrepresent
spanprecisionandmajorrecall,respectively. Acknowledgements
We would like to thank Ricardo Rei, Marcos
Finally, when evaluating the error spans pro-
Treviso and Chryssa Zerva for helping run the
ducedbyLLMspromptedwith AUTOMQM (Ta-
word-levelQEbaselines,andGeorgeFosterwho
ble 6), we find that PaLM-2 models are able to
provided feedback on an earlier version of this
identifymostofthemajorerrors. However,itdoes
work. ThisworkwaspartiallysupportedbyEU’s
seemtoover-predicterrors(witherrorspredicted
HorizonEuropeResearchandInnovationActions
byUNICORNhavingonaverage∼5wordsperspan
(UTTER, contract 101070631), the P2020 pro-
vs∼2wordsinthegroundtruth)andhaveoverall
gramMAIA(LISBOA-01-0247-FEDER-045909),
lowspanprecision. Similarlytooverallscorecor-
the Portuguese Recovery and Resilience Plan,
relations,scalealsoseemstobeimportantforthe
and the Fundação para a Ciência e Tecnolo-
quality of spans produced by AUTOMQM, with
giathroughcontractsSFRH/BD/150706/2020and
UNICORN outperforming BISON atmostmetrics.
UIDB/50008/2020.
Additionally,UNICORNpromptedwithAutoMQM
predicts spans of comparable quality to the ones
producedbycurrentstate-of-the-artlearned word-
References
levelevaluators(trainedonaconsiderablenumber
of fine-grained annotations derived from MQM): RohanAnil,AndrewM.Dai,OrhanFirat,Melvin
while word-level models are more precise, their Johnson, Dmitry Lepikhin, Alexandre Passos,
overallspancorrelation(MCC)iscomparable,and Siamak Shakeri, Emanuel Taropa, Paige Bai-
they miss considerably more major errors than ley,ZhifengChen,EricChu,JonathanH.Clark,
)ed-ne(
secnerrucco
#
Laurent El Shafey, Yanping Huang, Kathy few-shotlearners. InAdvancesinNeuralInfor-
Meier-Hellstern,GauravMishra,EricaMoreira, mation Processing Systems, volume 33, pages
Mark Omernick, Kevin Robinson, Sebastian 1877–1901.CurranAssociates,Inc.
Ruder,YiTay,KefanXiao,YuanzhongXu,Yu-
Chris Callison-Burch, Philipp Koehn, Christof
jing Zhang, Gustavo Hernandez Abrego, Jun-
Monz, Josh Schroeder, and Cameron Shaw
whan Ahn, Jacob Austin, Paul Barham, Jan
Fordyce, editors. 2008. Proceedings of the
Botha, James Bradbury, Siddhartha Brahma,
ThirdWorkshoponStatisticalMachineTransla-
Kevin Brooks, Michele Catasta, Yong Cheng,
tion.AssociationforComputationalLinguistics,
ColinCherry,ChristopherA.Choquette-Choo,
Columbus,Ohio.
AakankshaChowdhery,ClémentCrepy,Shachi
Dave,MostafaDehghani,SunipaDev,JacobDe- AakankshaChowdhery,SharanNarang,JacobDe-
vlin,MarkDíaz,NanDu,EthanDyer,VladFein- vlin, Maarten Bosma, Gaurav Mishra, Adam
berg, Fangxiaoyu Feng, Vlad Fienber, Markus Roberts, Paul Barham, Hyung Won Chung,
Freitag,XavierGarcia,SebastianGehrmann,Lu- Charles Sutton, Sebastian Gehrmann, Parker
casGonzalez,GuyGur-Ari,StevenHand,Hadi Schuh, Kensen Shi, Sasha Tsvyashchenko,
Hashemi, Le Hou, Joshua Howland, Andrea JoshuaMaynez,AbhishekRao,ParkerBarnes,
Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Is- Yi Tay, Noam Shazeer, Vinodkumar Prab-
ard, Abe Ittycheriah, Matthew Jagielski, Wen- hakaran, Emily Reif, Nan Du, Ben Hutchin-
hao Jia, Kathleen Kenealy, Maxim Krikun, son, Reiner Pope, James Bradbury, Jacob
Sneha Kudugunta, Chang Lan, Katherine Lee, Austin,MichaelIsard,GuyGur-Ari,Pengcheng
Benjamin Lee, Eric Li, Music Li, Wei Li, Yin, Toju Duke, Anselm Levskaya, Sanjay
YaGuangLi,JianLi,HyeontaekLim,Hanzhao Ghemawat, Sunipa Dev, Henryk Michalewski,
Lin, Zhongtao Liu, Frederick Liu, Marcello Xavier Garcia, Vedant Misra, Kevin Robin-
Maggioni, Aroma Mahendru, Joshua Maynez, son, Liam Fedus, Denny Zhou, Daphne Ip-
Vedant Misra, Maysam Moussalem, Zachary polito, David Luan, Hyeontaek Lim, Barret
Nado, John Nham, Eric Ni, Andrew Nystrom, Zoph, Alexander Spiridonov, Ryan Sepassi,
Alicia Parrish, Marie Pellat, Martin Polacek, David Dohan, Shivani Agrawal, Mark Omer-
AlexPolozov,ReinerPope,SiyuanQiao,Emily nick,AndrewM.Dai,ThanumalayanSankara-
Reif, Bryan Richter, Parker Riley, Alex Cas- narayanaPillai,MariePellat,AitorLewkowycz,
tro Ros, Aurko Roy, Brennan Saeta, Rajku- EricaMoreira,RewonChild,OleksandrPolozov,
mar Samuel, Renee Shelby, Ambrose Slone, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
DanielSmilkov,DavidR.So,DanielSohn,Si- BrennanSaeta,MarkDiaz,OrhanFirat,Michele
mon Tokumine, Dasha Valter, Vijay Vasude- Catasta,JasonWei,KathyMeier-Hellstern,Dou-
van, Kiran Vodrahalli, Xuezhi Wang, Pidong glas Eck, Jeff Dean, Slav Petrov, and Noah
Wang, Zirui Wang, Tao Wang, John Wieting, Fiedel. 2022. Palm: Scaling language model-
YuhuaiWu,KelvinXu,YunhanXu,LintingXue, ingwithpathways.
PengchengYin,JiahuiYu,QiaoZhang,Steven
Alexis Conneau, Kartikay Khandelwal, Naman
Zheng,CeZheng,WeikangZhou,DennyZhou,
Goyal,VishravChaudhary,GuillaumeWenzek,
Slav Petrov, and Yonghui Wu. 2023. Palm 2
Francisco Guzmán, Edouard Grave, Myle Ott,
technicalreport.
LukeZettlemoyer,andVeselinStoyanov.2020.
TomBrown,BenjaminMann,NickRyder,Melanie Unsupervisedcross-lingualrepresentationlearn-
Subbiah, Jared D Kaplan, Prafulla Dhariwal, ingatscale. InProceedingsofthe58thAnnual
ArvindNeelakantan,PranavShyam,GirishSas- Meeting of the Association for Computational
try, Amanda Askell, Sandhini Agarwal, Ariel Linguistics,pages8440–8451,Online.Associa-
Herbert-Voss,GretchenKrueger,TomHenighan, tionforComputationalLinguistics.
Rewon Child, Aditya Ramesh, Daniel Ziegler,
DanielDeutsch,GeorgeFoster,andMarkusFreitag.
JeffreyWu,ClemensWinter,ChrisHesse,Mark
2023. TiesMatter: ModifyingKendall’sTaufor
Chen,EricSigler,MateuszLitwin,ScottGray,
ModernMetricMeta-Evaluation.
BenjaminChess,JackClark,ChristopherBerner,
SamMcCandlish,AlecRadford,IlyaSutskever, Patrick Fernandes, Aman Madaan, Emmy Liu,
andDarioAmodei.2020. Languagemodelsare António Farinhas, Pedro Henrique Martins,
AmandaBertsch,JoséG.C.deSouza,Shuyan Zhou. 2023. Multi-dimensional evaluation of
Zhou, Tongshuang Wu, Graham Neubig, and textsummarizationwithin-contextlearning.
André F. T. Martins. 2023. Bridging the gap:
A survey on integrating (human) feedback for TomKocmi,RachelBawden,OndˇrejBojar,Anton
naturallanguagegeneration. Dvorkovich,ChristianFedermann,MarkFishel,
ThammeGowda,YvetteGraham,RomanGrund-
Erick Fonseca, Lisa Yankovskaya, AndrÃ© F. T. kiewicz, Barry Haddow, Rebecca Knowles,
Martins,MarkFishel,andChristianFedermann. Philipp Koehn, Christof Monz, Makoto Mor-
2019. Findings of the wmt 2019 shared tasks ishita, Masaaki Nagata, Toshiaki Nakazawa,
on quality estimation. In Proceedings of the MichalNovák,MartinPopel,andMajaPopovic´.
FourthConferenceonMachineTranslation(Vol- 2022. Findings of the2022 conference on ma-
ume3: SharedTaskPapers,Day2),pages1–12, chine translation (WMT22). In Proceedings
Florence,Italy.AssociationforComputational of the Seventh Conference on Machine Trans-
Linguistics. lation(WMT),pages1–45,AbuDhabi,United
Arab Emirates (Hybrid). Association for Com-
Markus Freitag, George Foster, David Grang- putationalLinguistics.
ier,VireshRatnakar,QijunTan,andWolfgang
Macherey.2021a. Experts,errors,andcontext: TomKocmiandChristianFedermann.2023. Large
Alarge-scalestudyofhumanevaluationforma- languagemodelsarestate-of-the-artevaluators
chine translation. Transactions of the Associ- oftranslationquality.
ation for Computational Linguistics, 9:1460–
1474. TomKocmi,ChristianFedermann,RomanGrund-
kiewicz, Marcin Junczys-Dowmunt, Hitokazu
MarkusFreitag,RicardoRei,NitikaMathur,Chi- Matsushita,andArulMenezes.2021. Toshipor
kiu Lo, Craig Stewart, Eleftherios Avramidis, nottoship: Anextensiveevaluationofautomatic
Tom Kocmi, George Foster, Alon Lavie, and metricsformachinetranslation. InProceedings
AndréF.T.Martins.2022. ResultsofWMT22 oftheSixthConferenceonMachineTranslation,
metrics shared task: Stop using BLEU – neu- pages478–494,Online.AssociationforCompu-
ral metrics are better and more robust. In Pro- tationalLinguistics.
ceedingsoftheSeventhConferenceonMachine
Translation (WMT), pages 46–68, Abu Dhabi, Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao
UnitedArabEmirates(Hybrid).Associationfor Jiang, Hiroaki Hayashi, and Graham Neubig.
ComputationalLinguistics. 2023a. Pre-train, prompt, and predict: A sys-
tematic survey of prompting methods in natu-
MarkusFreitag,RicardoRei,NitikaMathur,Chi- ral language processing. ACM Comput. Surv.,
kiu Lo, Craig Stewart, George Foster, Alon 55(9).
Lavie,andOndˇrejBojar.2021b. Resultsofthe
WMT21metricssharedtask: Evaluatingmetrics YangLiu,DanIter,YichongXu,ShuohangWang,
with expert-based human evaluations on TED Ruochen Xu, and Chenguang Zhu. 2023b. G-
and news domain. In Proceedings of the Sixth eval: Nlgevaluationusinggpt-4withbetterhu-
ConferenceonMachineTranslation,pages733– manalignment.
774,Online.AssociationforComputationalLin-
guistics. ArleLommel,HansUszkoreit,andAljoschaBur-
chardt.2014. Multidimensionalqualitymetrics
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri (MQM):Aframeworkfordeclaringanddescrib-
Alon,PengfeiLiu,YimingYang,JamieCallan, ing translation quality metrics. Revista Trad-
and Graham Neubig. 2022. Pal: Program- umàtica: tecnologiesdelatraducció.
aided language models. arXiv preprint
arXiv:2211.10435. Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie,
andDachengTao.2023. Erroranalysisprompt-
Sameer Jain, Vaishakh Keshava, Swar- ingenableshuman-liketranslationevaluationin
nashreeMysoreSathyendra,PatrickFernandes, largelanguagemodels: Acasestudyonchatgpt.
Pengfei Liu, Graham Neubig, and Chunting arXivpreprint.
Qingsong Ma, Johnny Wei, Ondˇrej Bojar, and André F. T. Martins. 2022b. CometKiwi: IST-
Yvette Graham. 2019. Results of the WMT19 unbabel 2022 submission for the quality esti-
metrics shared task: Segment-level and strong mation shared task. In Proceedings of the Sev-
MTsystemsposebigchallenges. InProceedings enthConferenceonMachineTranslation(WMT),
of the Fourth Conference on Machine Transla- pages 634–645, Abu Dhabi, United Arab Emi-
tion (Volume 2: Shared Task Papers, Day 1), rates (Hybrid). Association for Computational
pages 62–90, Florence, Italy. Association for Linguistics.
ComputationalLinguistics.
ThibaultSellam,DipanjanDas,andAnkurParikh.
OpenAI.2023. Gpt-4technicalreport. 2020. BLEURT:Learningrobustmetricsfortext
generation. InProceedingsofthe58thAnnual
KishorePapineni,SalimRoukos,ToddWard,and Meeting of the Association for Computational
Wei-Jing Zhu. 2002. Bleu: a method for auto- Linguistics,pages7881–7892,Online.Associa-
maticevaluationofmachinetranslation. InPro- tionforComputationalLinguistics.
ceedingsofthe40thAnnualMeetingoftheAs-
DavidVilar,MarkusFreitag,ColinCherry,Jiaming
sociationforComputationalLinguistics,pages
Luo,VireshRatnakar,andGeorgeFoster.2022.
311–318,Philadelphia,Pennsylvania,USA.As-
Promptingpalmfortranslation: Assessingstrate-
sociationforComputationalLinguistics.
giesandperformance.
Stefano Perrella, Lorenzo Proietti, Alessandro
JasonWei,MaartenBosma,VincentZhao,Kelvin
Scirè,NiccolòCampolungo,andRobertoNav-
Guu,AdamsWeiYu,BrianLester,NanDu,An-
igli.2022. MaTESe: Machinetranslationeval-
drewMDai,andQuocVLe.2021. Finetuned
uation as a sequence tagging problem. In Pro-
language models are zero-shot learners. In In-
ceedingsoftheSeventhConferenceonMachine
ternationalConferenceonLearningRepresenta-
Translation(WMT),pages569–577,AbuDhabi,
tions.
UnitedArabEmirates(Hybrid).Associationfor
ComputationalLinguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, brian ichter, Fei Xia, Ed H.
Yiwei Qin, Weizhe Yuan, Graham Neubig, and
Chi,QuocVLe,andDennyZhou.2022. Chain
PengfeiLiu.2022. T5score: Discriminativefine-
ofthoughtpromptingelicitsreasoninginlarge
tuningofgenerativeevaluationmetrics. ArXiv,
languagemodels. InAdvancesinNeuralInfor-
abs/2212.05726.
mationProcessingSystems.
Colin Raffel, Noam Shazeer, Adam Roberts,
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri,
KatherineLee,SharanNarang,MichaelMatena,
Alane Suhr, Prithviraj Ammanabrolu, Noah A.
YanqiZhou,WeiLi,andPeterJ.Liu.2020. Ex-
Smith,MariOstendorf,andHannanehHajishirzi.
ploringthelimitsoftransferlearningwithauni-
2023. Fine-grainedhumanfeedbackgivesbetter
fied text-to-text transformer. J. Mach. Learn.
rewardsforlanguagemodeltraining.
Res.,21(1).
WendaXu,DanqingWang,LiangmingPan,Zhen-
Ricardo Rei, José G. C. de Souza, Duarte Alves, qiaoSong,MarkusFreitag,WilliamYangWang,
Chrysoula Zerva, Ana C Farinha, Taisiya and Lei Li. 2023. Instructscore: Towards ex-
Glushkova,AlonLavie,LuisaCoheur,andAn- plainable text generation evaluation with auto-
dréF.T.Martins.2022a. COMET-22: Unbabel- maticfeedback.
IST2022submissionforthemetricssharedtask.
LintingXue,NoahConstant,AdamRoberts,Mihir
InProceedingsoftheSeventhConferenceonMa-
Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
chineTranslation(WMT),pages578–585,Abu
Barua, and Colin Raffel. 2021. mT5: A mas-
Dhabi,UnitedArabEmirates(Hybrid).Associa-
sivelymultilingualpre-trainedtext-to-texttrans-
tionforComputationalLinguistics.
former. InProceedingsofthe2021Conference
Ricardo Rei, Marcos Treviso, Nuno M. Guer- of the North American Chapter of the Associ-
reiro,ChrysoulaZerva,AnaCFarinha,Christine ation for Computational Linguistics: Human
Maroti,JoséG.C.deSouza,TaisiyaGlushkova, LanguageTechnologies,pages483–498,Online.
Duarte Alves, Luisa Coheur, Alon Lavie, and AssociationforComputationalLinguistics.
Chrysoula Zerva, Frédéric Blain, Ricardo Rei,
Piyawat Lertvittayakumjorn, José G. C. de
Souza, Steffen Eger, Diptesh Kanojia, Duarte
Alves, Constantin Ora˘san, Marina Fomicheva,
André F. T. Martins, and Lucia Specia. 2022.
Findings of the WMT 2022 shared task on
quality estimation. In Proceedings of the Sev-
enthConferenceonMachineTranslation(WMT),
pages69–99,AbuDhabi,UnitedArabEmirates
(Hybrid). Association for Computational Lin-
guistics.
Chrysoula Zerva, Daan van Stigt, Ricardo Rei,
Ana C Farinha, Pedro Ramos, José G. C. de
Souza,TaisiyaGlushkova,MiguelVera,Fabio
Kepler, and André F. T. Martins. 2021. IST-
unbabel2021submissionforthequalityestima-
tion shared task. In Proceedings of the Sixth
ConferenceonMachineTranslation,pages961–
972,Online.AssociationforComputationalLin-
guistics.
A MultidimensionalQualityMetric(MQM)
The Multidimensional Quality Metrics (MQM) framework is a flexible human-evaluation framework
developedtoevaluateandcategorizeerrorsintranslations. Annotatorsareinstructedtoidentifyallerrors
withineachsegmentinadocument,payingparticularattentiontodocumentcontext. SeeTable7forthe
annotatorguidelinesprovided.
You will be assessing translations at the segment level, where a segment may contain one or more
sentences. Each segment is aligned with a corresponding source segment, and both segments are
displayedwithintheirrespectivedocuments. Annotatesegmentsinnaturalorder,asifyouwerereading
thedocument. Youmayreturntoreviseprevioussegments.
Pleaseidentifyallerrorswithineachtranslatedsegment,uptoamaximumoffive. Iftherearemorethan
fiveerrors,identifyonlythefivemostsevere. Ifitisnotpossibletoreliablyidentifydistincterrorsbecause
thetranslationistoobadlygarbledorisunrelatedtothesource,thenmarkasingleNon-translationerror
thatspanstheentiresegment.
Toidentifyanerror,highlighttherelevantspanoftext,andselectacategory/sub-categoryandseverity
levelfromtheavailableoptions. (Thespanoftextmaybeinthesourcesegmentiftheerrorisasource
errororanomission.) Whenidentifyingerrors,pleasebeasfine-grainedaspossible. Forexample,ifa
sentence contains two words that are each mistranslated, two separate mistranslation errors should be
recorded. Ifasinglestretchoftextcontainsmultipleerrors,youonlyneedtoindicatetheonethatismost
severe. Ifallhavethesameseverity,choosethefirstmatchingcategorylistedintheerrortypology(eg,
Accuracy,thenFluency,thenTerminology,etc).
Pleasepayparticularattentiontodocumentcontextwhenannotating. Ifatranslationmightbequestionable
onitsownbutisfineinthecontextofthedocument,itshouldnotbeconsiderederroneous;conversely,
ifatranslationmightbeacceptableinsomecontext,butnotwithinthecurrentdocument,itshouldbe
markedaswrong.
There are two special error categories: Source error and Non-translation. Source errors should be
annotatedseparately,highlightingtherelevantspaninthesourcesegment. Theydonotcountagainstthe
five-errorlimitfortargeterrors,whichshouldbehandledintheusualway,whetherornottheyresulted
fromasourceerror. TherecanbeatmostoneNon-translationerrorpersegment,anditshouldspanthe
entiresegment. NoothererrorsshouldbeidentifiedifNon-Translationisselected.
Table7: MQMannotatorguidelines
Annotators are asked to assign both an error severity and category. Error severity (either major or
minor)isassignedindependentlyofcategory. Spanswithnomarkederrorshaveneutralseverityandno
category. PossibleerrorcategoriesaredisplayedinTable8.
SinceMQMdoesn’taskannotatorsforqualityscores,thosescoresarederivedautomaticallyfromthe
identifiederrorspansandtheirclassifications,basedonaweightingofeacherrorseverityandcategory.
Table9summarizesthisweightingscheme,inwhichsegment-levelscorescanrangefrom0(perfect)to
25(worst). Thefinalsegment-levelscoreisanaverageoverscoresfromallannotators. Insomesettings
(e.g. calculatingcorrelationforlearnedmetrics),thescoresarenegated.
Weusethesameweightingtoobtainscoresfromerrorsidentifiedby AUTOMQM.
B Samplingin-contextlearningexamplesforAutoMQM
Figure11showstherejectioncriteriausedwhensamplingexamplesetsasdiscussedin§4.
C AdditionalResults
Figures12,13,and14presentadditionalexperimentalresults.
ErrorCategory Description
Accuracy Addition Translationincludesinformationnotpresentinthesource.
Omission Translationismissingcontentfromthesource.
Mistranslation Translationdoesnotaccuratelyrepresentthesource.
Untranslatedtext Sourcetexthasbeenleftuntranslated.
Fluency Punctuation Incorrectpunctuation(forlocaleorstyle).
Spelling Incorrectspellingorcapitalization.
Grammar Problemswithgrammar,otherthanorthography.
Register Wronggrammaticalregister(eg,inappropriatelyinformalpronouns).
Inconsistency Internalinconsistency(notrelatedtoterminology).
Characterencoding Charactersaregarbledduetoincorrectencoding.
Terminology Inappropriateforcontext Terminologyisnon-standardordoesnotfitcontext.
Inconsistentuse Terminologyisusedinconsistently.
Style Awkward Translationhasstylisticproblems.
Locale Addressformat Wrongformatforaddresses.
convention Currencyformat Wrongformatforcurrency.
Dateformat Wrongformatfordates.
Nameformat Wrongformatfornames.
Telephoneformat Wrongformatfortelephonenumbers.
Timeformat Wrongformatfortimeexpressions.
Other Anyotherissues.
Sourceerror Anerrorinthesource.
Non-translation Impossibletoreliablycharacterizedistincterrors.
Table8: MQMhierarchy.
Severity Category Weight
Major Non-translation 25
allothers 5
Minor Fluency/Punctuation 0.1
allothers 1
Neutral all 0
Table9: MQMerrorweighting.
1 def check_icl_set(
2 examples: pd.DataFrame,
3 min_errors=3,
4 majmin_threshold=2,
5 cat_diversity=2,
6 min_clen=20,
7 max_clen=400,
8 ):
9 # Check if they have the same number of spans as severity/category
10 if not examples.apply(
11 lambda r:
12 len(r[’span’]) == len(r[’severity’]) and len(r[’span’]) == len(r[’category’]),
13 axis=1
14 ).all():
15 return False
16
17 # Check if there are at least min_errors
18 if examples[’severity’].apply(lambda svs: len(svs)).sum() < min_errors:
19 return False
20
21 # Check that there’s a balance of major and minor errors.
22 major_count = examples[’severity’].apply(lambda svs: sum([s==’major’ for s in svs])).sum()
23 minor_count = examples[’severity’].apply(lambda svs: sum([s==’minor’ for s in svs])).sum()
24 if abs(major_count - minor_count) > majmin_threshold:
25 return False
26
27 # Check that at least cat_diversity error types are represented.
28 categories = examples[’category’].apply(lambda cs: [c.split("/")[0] for c in cs])
29 represented_error_types = set().union(*categories.tolist())
30 if len(represented_error_types) < cat_diversity:
31 return False
32
33 top_clen = examples.apply(
34 lambda row: max(len(row[s]) for s in (’source’, ’reference’, ’candidate’)
35 ), axis=1).max()
36 bot_clen = examples.apply(
37 lambda row: min(len(row[s]) for s in (’source’, ’reference’, ’candidate’)),
38 axis=1).min()
39
40 if top_clen > max_clen or bot_clen < min_clen:
41 return False
42
43 # All checks passed.
44 return True
Figure11: Rejectioncriteriausedwhensamplingin-contextlearningexamplesfor AUTOMQM.
0.45 0.45
PaLM-2 (Bison) ref-based PaLM-2 (Bison) ref-based
0.40 0.40
PaLM-2 (Bison) ref-free PaLM-2 (Bison) ref-free
0.35 0.35
0.30 0.30
0.25 0.25
0.20 0.20
0.15 0.15
0.10 0.10
0.05 0.05
0.00 0.00
0 1 2 3 4 0 1 2 3 4
# of in-context examples # of in-context examples
Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context
examplesinthescorepredictionprompt,sampledwithuniform(left)andstratified (right)sampling,for
WMT22EN-DE.
0.35 0.35
PaLM-2 (Bison) ref-based PaLM-2 (Bison) ref-based
0.30 PaLM-2 (Bison) ref-free 0.30 PaLM-2 (Bison) ref-free
0.25 0.25
0.20 0.20
0.15 0.15
0.10 0.10
0.05 0.05
0.00 0.00
0 1 2 3 4 0 1
# of in-context examples # of in-context examples
Figure 13: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context
examplesinthescorepredictionprompt,sampledwithuniform(left)andstratified (right)sampling,for
WMT22ZH-EN.
10000
1000
100
10
1
Error 0 5 10 20 50 60 70 80 85 90 95100
Score
Figure14: DistributionofscoresforvariousLLMreference-basedevaluators,ontheZH-ENtestset. Notethatthe
yaxisisinlog-scale.
nosraeP
nosraeP
)ne-hz(
secnerrucco
#
nosraeP
nosraeP

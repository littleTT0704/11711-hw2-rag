Uncovering Surprising Event Boundaries in Narratives
ZhilinWang,AnnaJafarpour,MaartenSap
UniversityofWashington
{zhilinw, annaja}@uw.edu, msap@cs.washington.edu
Abstract
When reading stories, people can naturally
identifysentencesinwhichaneweventstarts,
i.e., event boundaries, using their knowledge
of how events typically unfold, but a compu-
tational model to detect event boundaries is
not yet available. We characterize and detect
sentences with expected or surprising event
boundaries in an annotated corpus of short
diary-likestories,usingamodelthatcombines
commonsense knowledge and narrative flow Figure 1: Example story with sentences that contain
features with a RoBERTa classifier. Our re- either a surprising event boundary, no event boundary
sults show that, while commonsense and nar- oranexpectedeventboundaryrespectively. Theanno-
rative features can help improve performance tations of reader perception are from the Hippocorpus
overall, detecting event boundaries that are dataset(Sapetal.,2022).
more subjective remains challenging for our
model. We also find that sentences marking
surprising event boundaries are less likely to
etal.,2019;Zacks,2020). Furthermore,detection
be causally related to the preceding sentence,
ofsentenceswitheventboundariescanalsobeuse-
but are more likely to express emotional re-
fulwhengeneratingengagingstorieswithagood
actions of story characters, compared to sen-
tenceswithnoeventboundary. amount of surprises. (Yao et al., 2019; Rashkin
etal.,2020;Ghazarianetal.,2021).
1 Introduction
Todifferentiatesentenceswithsurprisingevent
When people read stories, they can easily detect boundaries, expected event boundaries, and no
the start of new events through changes in cir- eventboundaries,wetrainaclassifierusing3925
cumstancesorinnarrativedevelopment,i.e.,event story sentences with human annotation of event
boundaries(Zacksetal.,2007;Brunietal.,2014; boundariesfromdiary-likestoriesaboutpeople’s
Foster and Keane, 2015; Jafarpour et al., 2019b). everyday lives (Sap et al., 2022). We extract var-
Theseeventboundariescanbeexpectedorsurpris- ious commonsense and narrative features on re-
ing. For example, in the story in Figure 1 based lationships between sentences of a story, which
oncrowdsourcedannotation,“gettingalongwitha can predict the type of event boundaries. Com-
dogwhodoesnotgenerallylikenewpeople"marks monsense features include the likelihood that ad-
asurprisingnewevent,while“theirplayingfetch jacentsentencesarelinkedbycommonsenserela-
togetherforalongtime"isanexpectednewevent. tionsfromtheknowledgegraphsAtomic(Sapetal.,
We aim to study whether machines can detect 2019a) and Glucose (Mostafazadeh et al., 2020).
thesesurprisingorexpectedeventboundaries,us- NarrativefeaturesincludeRealis(Simsetal.,2019)
ing commonsense knowledge and narrative flow thatidentifiesthenumberofevent-relatedwordsin
features. Characterizing features that are infor- asentence,Sequentiality(Radfordetal.,2019;Sap
mativeindetectingeventboundariescanhelpde- etal.,2022)basedontheprobabilityofgenerating
terminehowhumansapplyexpectationsonevent asentencewithvaryingcontextandSimGen(Ros-
relationships (Schank and Abelson, 1977; Kurby set,2020),whichmeasuresthesimilaritybetween
and Zacks, 2009; Radvansky et al., 2014; Ünal a sentence and the sentence that is most likely to
1
Proceedingsofthe3rdWordplay:WhenLanguageMeetsGamesWorkshop(Wordplay2022),pages1-12
July15,2022©2022AssociationforComputationalLinguistics
begeneratedgiventheprevioussentence. Wethen arerepresentedasacontinuousflowwithmultiple
combinethepredictionbasedonthesefeatureswith boundariesmarkingnewevents(Zacksetal.,2007;
thepredictionfromaRoBERTaclassifier(Liuetal., Graesseretal.,1981;KurbyandZacks,2008;Za-
2019),toformoverallpredictions. cks,2020);however,welackamodeltodetectthe
Weevaluatetheperformanceoftheclassification boundaryeventsthatmarkthemeaningfulsegmen-
modelbymeasuringF1ofthepredictionsandcom- tationofacontinuousstoryintodiscreteevents.
parevariousconfigurationsofthemodeltoabase-
Inthiswork,westudystoriesfromacognitive
lineRoBERTamodel. Wefindthatintegratingnar-
angletodetecteventboundaries. Sucheventbound-
rativeandcommonsensefeatureswithRoBERTa
ariesrelatetoournarrativeschemaunderstanding
leadstoasignificantimprovement(+2.2%F1)over
(SchankandAbelson,1977;ChambersandJuraf-
a simple RoBERTa classifier. There are also in-
sky,2008;Ryan,2010),commonsenseknowledge
dividual differences on the subjective judgment
(Sapetal.,2019a;Mostafazadehetal.,2020)and
of which sentences contain a surprising or an ex-
worldknowledge(Nematzadehetal.,2018;Bisk
pectedeventboundary,thatisreflectedinthedetec-
etal.,2020). Existingworkhasstudiedonsalient
tionmodel’sperformance. Theperformanceofour
(i.e. important/mostreport-able)eventboundaries
modelincreaseswithincreasingagreementacross
withinastory(OuyangandMcKeown,2015;Otake
thehumanannotators. Additionally,byinterpreting
etal.,2020;WilmotandKeller,2021). However,
thetrainedparametersofourmodel,wefindthat
missingfromliteratureishowsalienteventbound-
theabsenceofcausallinksbetweensentencesisa
arycaneitherbesurprisingorexpectedbasedon
strongpredictorofsurprisingeventboundaries.
theknowledgeofhowaflowofeventsshouldun-
Tofurtheranalyzehowsurprisingeventbound-
fold. Forexample,eventscanbesurprisingwhen
aries relate to deviation from commonsense un-
theydeviatefromcommonsenseintermsofwhat
derstanding, we compare the performance of the
peoplewouldpredict(e.g.,ifsomeonewonsome-
classification model on the related task of ROC
thing, they should not be sad; Sap et al., 2019a).
StoryClozeTest(Mostafazadehetal.,2016). This
Surprisingeventscanalsobelowlikelihoodevents
task concerns whether the ending sentence of a
(FosterandKeane,2015)suchasseeingsomeone
storyfollows/violatescommonsensebasedonear-
wear shorts outside in winter, or due to a rapid
lier sentences, which can be linked to whether
shiftinemotionalvalencebetweenevents(Wilson
sentences are expected or surprising. Our model
and Gilbert, 2008) such as seeing a protagonist
performs significantly higher on the ROC Story
being defeated. Importantly, there are individual
Cloze Test (87.9% F1 vs 78.0% F1 on our task),
differencesinhowhumanssegmentnarrativesinto
showing that surprising event boundaries go be-
events(Jafarpouretal.,2019a).
yondmerelyviolatingcommonsenseandtherefore
We tackle event boundary detection as a three-
canbeseenasmorechallengingtodetect. Together,
wayclassificationtaskthatinvolvesdistinguishing
ourresultssuggeststhatwhiledetectingsurprising
surprisingbutplausibleeventboundariesinstory
event boundaries remains a challenging task for
sentencesfromexpectedeventboundariesandno
machines, a promising direction lies in utilizing
eventboundaries. Tomirrorhowhumansreadsto-
commonsenseknowledgeandnarrativefeaturesto
ries,wepredicttheeventboundarylabelforasen-
augmentlanguagemodels.
tence using all of its preceding sentences in the
story,aswellasthegeneralstorytopicascontext.
2 EventBoundaryDetectionTask
Surprisingeventboundariesarenoveleventsthat
Events have been widely studied in Natural Lan- areunexpectedgiventheircontext,suchasadog
guage Processing. They have often been repre- getting along with someone despite not typically
sented in highly structured formats with word- likingnewpeople. Expected eventboundariesare
specifictriggersandarguments(Walkeretal.,2006; noveleventsthatarenotsurprising,suchasaper-
Lietal.,2013;Chenetal.,2017;Simsetal.,2019; sonplayinganewgamewithadogforalongtime
Mostafazadeh et al., 2020; Ahmad et al., 2021) given that they like each other. In contrast, sen-
or as Subject-Verb-Object-style (SVO) tuples ex- tenceswithnoeventboundarytypicallycontinue
tracted from syntactic parses (Chambers and Ju- orelaborateontheprecedingevent,suchasaper-
rafsky, 2008; Martin et al., 2018; Rashkin et al., sonlikingadoggiventhattheygetalongwiththe
2018; Sap et al., 2019a). In narratives, events dog(Figure1).
2
Majoritylabel #Samples(%) %majority 4 EventBoundaryDetectionModel
agreement(std)
We first describe informative commonsense and
Noevent 2255(57.5) 68.1(13.9)
narrative features that we extract for the event
Expected 650(16.6) 58.8(10.6)
boundarydetectionmodel. Then,wedescribehow
Surprising 509(13.0) 61.7(11.9)
weintegratethesefeatureswithaRoBERTaclassi-
Tied 511(13.0) 41.1(5.7)
fierinourmodelbeforedetailingourexperimental
Total 3925(100) 62.2(15.2) setup. Figure2depictsanoverviewofourmodel.
Table1: DescriptiveStatisticsforEvent-annotatedsen- 4.1 Features
tences. Majority label refers to the most common an-
We select a collection of commonsense features
notationofasamplefrom8independentannotators. If
thereisatiebetween2labels,itiscategorizedastied. (AtomicandGlucoserelations)andnarrativeflow
Majorityagreementistheproportionofsampleannota- features (Realis, Sequentiality and SimGen). A
tionsforthemajoritylabel. modelistrainedseparatelyfromourmainmodel
for Atomic relations, Glucose relations and Re-
alis while models for Sequentiality and SimGen
are used without further training. Features of
3 Event-annotatedData
storysentencesareextractedasinputintothemain
model. Becauselanguagemodellingalonemight
not be sufficient to learn such features (Gordon
We use the English-based Event-annotated sen-
andVanDurme,2013;Sapetal.,2019a),wepro-
tences from stories in the Hippocorpus dataset
vide the extracted features to the model instead
to study event boundaries. This dataset contains
of relying on the language models to learn them
240 diary-like crowdsourced stories about every-
implicitly.
day life experiences, each containing an average
of16.4sentencesandareannotatedatthesentence
Atomic relations are event relations from a so-
level(Sapetal.,2022). Storieswereinspectedfor
cialcommonsenseknowledgegraphcontainingnu-
theabsenceofoffensiveorperson-identifyingcon-
merous events that can be related to one another
tent. Fortheannotation,eightcrowdworkerswere
(Sapetal.,2019a). Theeventrelationsinthisgraph
shownastorysentencebysentenceandwereasked
consistsof:
to mark whether each sentence contained a new
EmotionalReaction,
surprisingorexpectedeventboundary,ornoevent
TheEffectofanevent,
boundaryatall,basedontheirsubjectivejudgment
Wanttodoaftertheevent,
(Sapetal.,2022). SummarizedinTable1,basedon
WhatNeedstobedonebeforeanevent,
themajoritarianvote,mostsentences(57.5%)con-
TheIntentiontodoacertainevent,
tainnoeventboundarieswhile16.6%and13.0%
WhatAttributesaneventexpresses.
ofsentencescontainsexpectedandsurprisingevent
Whenaneventaffectsthesubject,thefeaturename
boundaries,respectively.
is preceded by an x, while if it affects others, it
Duetotheinherentsubjectivityofthetask,ag- hasano. oonlyappliestoReact,EffectandWant.
gregating labels into a majority label yields low For example, an xWant of a sentence PersonX
agreement(e.g.,61.7%forsurprisingeventbound- pays PersonY a compliment is that PersonX will
aries;Table1). Therefore,attrainingtime,weuse wanttochatwithPersonY,andanoWantisthat
theproportionofannotationsforeacheventbound- PersonY will compliment PersonX back. We use
ary type as the label instead of the majority vote, Atomicrelationsbecausesurprisingeventbound-
becausesuchdistributionalinformationisabetter ariescaninvolvebreachesofcommonsenseunder-
reflectionoftheinherentdisagreementamonghu- standing(Bosselutetal.,2019;Sapetal.,2019a;
manjudgements(PavlickandKwiatkowski,2019). Mostafazadeh et al., 2020; Gabriel et al., 2021).
At test time, we use the majority vote as a gold Furthermore,someAtomicrelations(xReactand
label,sincemeasuringperformanceondistribution oReact)concernemotionalaffectandtherefore
modelling is less intuitive to interpret, and sub- can be used to capture changes in emotional va-
sequentlybreakdownperformancebyagreement lence,whichcancauseeventstobeseenassurpris-
leveltotakedisagreementsintoaccount. ing(WilsonandGilbert,2008).
3
Figure2: (Left)OurmodelinvolvesaGRUtocombinefeaturesfromsentencepairswiththreefeatureencoding
modes, RoBERTa to consider story sentences and Event Boundary Detector to combine predictions made by
thetwocomponents. S andF refertosentencenandfeaturesnrespectively,whileP andP arepredictions
n n G R
madebytheGRUandRoBERTa. Theoutputisaprobabilitydistributionovernoeventboundary,expectedevent
boundaryandsurprisingeventboundary,whichisusedtoupdatemodelparameterstogetherwiththelabelusing
the Kullback-Leibler Divergence loss function. (Right) Features (Atomic, Glucose, Realis, Sequentiality and
SimGen)canbeextractedasinputintotheGRUinthreefeatureencodingmodes: SEQUENTIAL(showninModel
Overview),ALLTOCURRENTandPREVIOUSONLY.
We train an Atomic relation classifier using a Dim-2: Emotion/humandrivethatmotivates
RoBERTa-base model (Liu et al., 2019) and the Dim-3: Changeinlocationthatenables
Atomicdatasettoclassifyevent-pairsintooneof Dim-4: Stateofpossessionthatenables
the nine possible relationship labels as well as a Dim-5: Otherattributethatenables
None label (to introduce negative samples). We Glucose relation classifier was trained on a
achievedavalidationF1of77.15%,whichishigh RoBERTa-basemodeltoclassifyevent-pairsfrom
fora10-wayclassificationtask. Wedescribetrain- theGlucosedatasetintooneoftenpossiblerelation
ingandotherexperimentaldetailsintheAppendix. labelsaswellasaNonelabel. Weusedthespecific
When making inferences on the Event-annotated version of Glucose events represented in natural
dataset,wepredictthelikelihoodthatapreceding language. As a result, we achieved a validation
sentenceinastorywillberelatedtothecurrentsen- F1 of 80.94%. Training and other experimental
tenceviaeachoftheninerelationshiplabels. Be- details are in the Appendix. During inference on
causeAtomicrelationsaredirectedrelations(e.g., the Event-annotated dataset, we predict and use
I ate some cake xEffect I am full is different asfeaturesthelikelihoodthatthecurrentsentence
fromIamfullxEffectIatesomecake),wealso will be related to a preceding sentence via each
madethereverseinferenceincasecommonsense relationlabel.
relations between sentences exist in the reverse
Realis eventsarewordsthatserveastriggers(i.e.,
direction. Together,9forwardatomicrelationfea-
head words) for structured event representations
turesand9reversefeatures(markedwith’-r’)are
(Sims et al., 2019). Realis event words denote
used.
concrete events that actually happened, meaning
Glucose relations are event relations from an- that a higher number of Realis event words sug-
other commonsense knowledge dataset contain- gestsgreaterlikelihoodofthesentencecontaining
ingrelationsbetweenevent-pairsin10dimensions aneweventboundary(expectedorsurprising). We
(Mostafazadehetal.,2020). Glucoserelationfea- trained a BERT-base model (Devlin et al., 2019)
turesareusedtocomplementAtomicrelationfea- on an annotated corpus of literary novel extracts
tures in its coverage of commonsense relations. (Sims et al., 2019). We achieved a validation F1
Dim-1to5aredescribedbelowwhileDim-6to10 of 81.85%, inspired by and on par with Sap et al.
arethereverse/passiveformofDim-1to5respec- (2020). Then, we use the trained model to make
tively. inferenceonstorysentencesintheEvent-annotated
Dim-1: Eventthatcauses/enables dataset. Finally,weusedthenumberofRealis
4
wordsineachsentenceasafeature. Trainingand sentences,whichmimicstheannotator’sprocedure
otherexperimentaldetailsareintheAppendix. of identifying event boundaries as they read one
sentence at the time. As seen in Figure 2 (right),
Sequentiality is a measure of the difference in
weusethreefeatureencodingmodestodetermine
conditionalnegativelog-likelihoodofgenerating
thefeaturesthatareusedasinputintotheGRU,as
a sentence given the previous sentence or other-
inspiredbyliteratureoneventsegmentation(Petti-
wise(Sapetal.,2020,2022). Sequentialitycanbe
johnandRadvansky,2016;Baldassanoetal.,2018;
a predictor for unlikely events, which can cause
Zacks,2020). Thesethreemodesrepresentdiffer-
surprise(FosterandKeane,2015). WeuseGPT-2
entwaysoffacilitatinginformationflowbetween
(Radfordetal.,2019)tomeasurethisnegativelog-
sentences,whichcanhavedistincteffectsoniden-
likelihoodsinceitisaLeft-to-Rightmodel,which
tifyingeventboundaries.
matchestheorderinwhichannotatorswereshown
Thefirstmode, SEQUENTIAL,encodesfeatures
sentences in a story. NLL of each sentence was
from all previous sentences in the story in a re-
obtained in two different contexts. NLL_topic
current way (1 to 2, 2 to 3 ... i 1 to i) up until
isbasedonthesentencealonewithonlythetopic −
the current sentence i. The second mode, ALL-
as prior context, while NLL_topic+prev uses
TOCURRENT,usesfeaturesfromeachoftheprevi-
the previous sentence as additional context to
oussentencestothecurrentsentencei(1toi,2to
studythelinkbetweenadjacentsentences. Finally,
i... i 1toi). Thethirdmode, PREVIOUSONLY,
Sequentialityisobtainedbytakingtheirdif- −
(i 1 to i) only feeds into the GRU the features
ference. ExperimentaldetailsareintheAppendix. −
relatingtotheprevioussentence. Forallmodes,the
dimensionofeachtimestepinputisK ,represent-
G
1
NLL = logp (s Topic) ingthetotalnumberofdistinctfeatures. Wethen
topic LM i
− |s i
|
| project the final output of the GRU, h
G ∈
RKG,
1 intoa3-dimensionalvectorspacerepresentingthe
NLL = logp (s Topic,s )
topic+prev LM i i 1
− s i | − unnormalized probability distribution over event
| |
boundarytypes.
SimGen iscomputedasthecosinesimilaritybe-
tween each sentence and the most likely gener-
RoBERTa isusedtomakepredictionsbasedon
ated sentence given the previous sentence, under
textinstorysentences. Weuseallstorysentences
alargeLeft-to-Rightlanguagemodel(specifically,
up to sentence i inclusive. We then project the
Turing-NLG;Rosset,2020). Then,weseparately
hiddenstateofthefirsttoken(alsoknownasCLS
convertedtheoriginalsentenceandgeneratedsen-
token),h
R
RKR,intoa3-dimensionalspacerep-
tenceintosentenceembeddingsusingapre-trained ∈
resentingtheunnormalizedprobabilitydistribution
MPnet-basemodel(Songetal.,2020). Finally,the
overeventboundarytypes.
generatedembeddingsandtheoriginalembeddings
arecomparedforcosinesimilarity,whichisusedas Combining predictions We combine predic-
afeature. ExperimentaldetailsareintheAppendix. tionsmadebytheGRU(P )andRoBERTa(P )
G R
byconcatenatingtheirpredictionsandmultiplying
4.2 ModelArchitecture
it with a linear classifier of size (6, 3) to output
Weproposeamodeltointegratefeature-basedpre-
logits of size (3). The logits are then normalized
diction with language-based prediction of event
using Softmax to give a distribution of the three
boundaries,illustratedinFigure2(left). Thepre-
typesofeventboundaries(P). Theweightsofthe
dictionsareindependentlymadewithextractedfea-
linearclassifierareinitializedbyconcatenatingtwo
turesusingagatedrecurrentunit(GRU)andwith
identitymatrixofsize3(I ),whichservestoper-
3
language (i.e., story sentences) using RoBERTa.
formelementwiseadditionbetweenthepredictions
Then these predictions are combined into a final
of the GRU and RoBERTa at early stages of the
predicteddistributionforthethreetypesofevent
trainingprocess.
boundaries. Our model is then trained using the
Kullback-LeiblerDivergenceloss.
W := [I ;I ] (1)
3 3
GRU isusedtocombinefeaturesrelatingthecur-
rentsentenceitopriorsentencesinastory. Itse-
quentiallyconsidersinformationconcerningprior P := Softmax(W([P ;P ])) (2)
G R
5
Lossfunction WeusetheKullback-LeiblerDi- overall noevent expected surprising
vergencelossfunctiontotrainthemodel. Weuse F1 F1 F1 F1
it over the standard Cross Entropy loss function EventDetector
(wRoBERTa)
becauseourtrainingtargetsareintheform: propor-
PREVIOUSONLY* 78.0 87.2 60.0 59.7
tionofannotationsforeachtypeofeventboundary SEQUENTIAL 77.3 86.6 57.5 60.5
(e.g., 0.75, 0.125, 0.125 for no event, expected ALLTOCURRENT 76.9 86.3 57.5 59.7
and surprising respectively). Including such dis- RoBERTa 75.8 86.2 55.8 54.3
tributionalinformationinourtrainingtargetsover EventDetector
usingthemajorityannotationonlycanreflectthe (w/oRoBERTa)
ALLTOCURRENT 63.9 81.8 32.3 24.8
inherentdisagreementamonghumanjudgements
SEQUENTIAL 63.8 82.1 34.6 19.5
(PavlickandKwiatkowski,2019),whichisimpor- PREVIOUSONLY 63.4 81.8 31.8 21.2
tanttocaptureforeventboundariesgiventhatthey
Table 2: Event detection task: Performance of Event
aresubjectivejudgements.
Detectorcomparedtobaselinemodel.*:overallF1sig-
4.3 Experimentalsetup nificantdifferentfromRoBERTabasedonMcNemar’s
test(p<0.05)(McNemar,1947)
Weseektopredicttheevent-boundaryannotation
for each Hippocorpus story sentence, using pre-
cedingsentencesinthestoryascontext,asshown 5 ResultsandDiscussion
inFigure2. Additionaltrainingandexperimental
Wefirstquantifytheperformanceofourmodelin
detailsareavailableintheAppendix.
detectingeventboundaries,usingacoarse-grained
K-foldCross-validation Becauseofthelimited performancemeasureonF1withrespecttomajor-
size of the dataset (n=3925), we split the dataset ityvote. Then,weinvestigatehowtheperformance
ink-folds(k=10),usingonefold(n=392)forval- variesbasedonannotationsubjectivity. Finally,we
idation and nine other folds combined for train- inspectthemodelparameterstoidentifycommon-
ing. From each of the 10 models, we obtained senseandnarrativefeaturesthataremostinforma-
thepredictionforthevalidationset. Together,the tiveindetectingeventboundaries.
validationsetsforthe10modelscombinetoform
Improving prediction of event boundaries As
predictionsfortheentiredataset,whichweuseto
seen in Table 2, RoBERTa alone performs fairly
conduct significance testing in order to compare
well in predicting event boundaries (F1 = 75.8%,
theperformanceofmodels.
within 2.2% F1 of our best performing model),
GRU wasaccessedfromPyTorch,withK G set butcanbefurthersupportedbyourcommonsense
to33andahiddendimensionof33. andnarrativefeaturestoimproveitsperformance.
In contrast, the commonsense and narrative fea-
RoBERTa RoBERTa-base-uncased with 12-
tures alone do not perform as well.1 Overall,
layer, 768-hidden (K ), 12-heads, 110M param-
R
our best performing set-up is the Event Detector
eters, 0.1 dropout was used, accessed from Hug-
(PREVIOUSONLY)withF1=78.0%,whichissig-
gingFaceTransformerslibrary(Wolfetal.,2020).
nificantlydifferentfromRoBERTaalonebasedon
Whenmorethan10priorsentencesareavailablein
McNemar’s test (p <0.05). 2 Its overall strong
astory,weuseonlythemostrecent10sentences
performanceislargelycontributedbyitsstrongper-
duetoRoBERTainputsequencelengthlimitations.
formanceindetectingnoeventboundariesandex-
Evaluation Metrics While capturing distribu- pectedeventboundaries. F1fornoeventboundary
tionalinformationofsubjectivejudgementlabels ishigherthanbothsurprisingandexpectedevent
(PavlickandKwiatkowski,2019)isimportantfor boundaries,likelybecausetherearemoresentences
training,itcanalsobedifficulttointerpretforeval- withnoeventboundariesasseeninTable1. The
uation. Therefore, we decided to predict for the PREVIOUSONLY configurationperformsbestfor
mostlikelylabelduringevaluationandcompareit
1We also increased learning rate to 1e-3 for better per-
againstthemajoritylabelforeachsample. Some
formancegiventheabsenceofRoBERTapredictionsinthis
511(13.0%)samplesdonothaveasinglemajority ablationset-up.
label(e.g.,equalnumberofexpectedandsurpris-
2McNemar’stestisusedtodeterminewhethersamples
that have been predicted accurately (or not) by one model
ingannotations)andthesesampleswereexcluded.
overlapwiththosethathavepredictedaccurately(ornot)by
Weusemicro-averagedF1asthemetric. anothermodel.
6
Figure 3: F1 by Event Detector (PREVIOUSONLY)
against majority agreement, on all 10 folds. * means
thatPearson’srissignificantatp<0.05and**atp<
Figure 4: Feature weights towards each label in GRU
0.001.
componentofEventDetector(PREVIOUSONLY)
noeventboundariesandexpectedeventboundaries dayinJulythatourcommunitydecidedtopainta
likelybecausedeterminingwhetherthecurrentsen- muralonanintersectionforpublicart,”only4out
tencecontinuesanexpectedevent(ornot)requires of8annotatorsindicatedthat“Ihaddecidedtovol-
retainingthelatestinformationinworkingmemory unteertohelppaint."wassurprising. Theresults
(Jafarpour et al., 2019a). However, the SEQUEN- suggest that our model performance reflects the
TIAL configuration seems to perform the best in variability and agreements in human annotations
predictingsurprisingeventboundaries. Compared ofeventboundaries. Wehypothesizethattheevent
tono/expectedeventboundaries,wehypothesize boundarieswithmoreagreementarebasedonfea-
thatpredictingsurprisingeventboundariesrequires tures that are shared across the annotators, such
takingintoaccounthowthestorydevelopedprior ascommonsenseknowledge;therefore,themodel
to theprevioussentence insetting up thecontext performs well in detecting those. Whereas, our
forthecurrentsentence. Thisfindingechoesresults model struggles with detecting event boundaries
by Townsend (2018) that showed that surprising thataremoresubjective.
sentences take a long time to read because it re-
quires changing our mental model formed from Predictive features By integrating a separate
feature-basedclassifier,theEventBoudaryDetec-
previoussentences.
tormodelallowsustoexaminethemodelparame-
F1 varies with majority agreement Since the tersanddeterminefeaturesthatareassociatedwith
annotations were subjective and did not always surprising,expectedornoeventboundaries. First,
agree,wefurtherexamineourbestmodel’sperfor- wetaketheaverageoftheGRUclassifierweights
mance(PREVIOUSONLY)withrespecttoannota- for each of the 10 cross-validated models. Then,
tionagreement. AsshowninFigure3,F1increases we plot these weights for each label in Figure 4,
withmajoritylabelagreement(Pearson’sr=0.953, andsummarizethefindingsbelow.
p<0.05). Suchpositivecorrelationsareobserved Featuresthatrelatetocommonsenserelations:
across all event boundary labels (Pearson’s r = oEffect, xEffect and Glucose Dim-6
0.869-0.994)andisespeciallystrongforsurprising (causedby)aremostpredictiveofexpectedevent
eventboundaries(Pearson’sr=0.994,p<0.001). boundaries. Thiscanindicatethateventsthatarean
Thismeansthatmosterrorsaremadeonsamples effectof/causedbyaprioreventcanbeexpectedby
that have low agreement among annotators. For annotators,asalsonotedbyGraesseretal.(1981).
exampletoshowthiscontrast,after“SheandIare An example of an expected event boundary is “I
veryclosesoitwasgreattoseehermarryingsome- toldherwecouldgoforcoffeesometime.”, asan
onesheloves,"7outof8annotatorsindicatedthat effectof“Wehadagoodtimetogether.” xNeedis
“ThemostmemorablemomentwaswhenIspilled leastindicativeofsurprisingeventboundaries. This
champagneonmydressbeforethewedding"was islikelybecausexNeedreferstowhatthesubject
surprising. Ontheotherhand,after“Itwasahot needstodobeforeanactivity,whichisprocedural
7
and unlikely to cause surprise. An example is “I tectormodelontheROCStoryClozeTest,using
wasgroceryshoppingafewweeksago.” whichis the same experimental setup as for event bound-
neededbefore“Ihadpurchasedmyitemsandwas arydetection,exceptthelossfunction;weusethe
leavingthestore.” cross-entropylosssinceonlyonelabelisavailable
FeaturesthatexplainunlikelyeventsRealis foreachsample.4
ishighestforsurprisingeventboundaries,suggest-
ing that surprising event boundaries tend to con- overall nonsense commonsense
tain the most concrete event-words. Surprising F1 endingF1 endingF1
eventboundariesalsohavethehighestlikelihood EventDetectorwRoBERTa
whenconditionedonthestorytopic(NLL_topic) ALLTOCURRENT 87.9 87.8 88.0
while expected events are highest when condi- PREVIOUSONLY 87.6 87.3 87.8
SEQUENTIAL 87.3 87.1 87.5
tionedbasedonthetopicandtheprevioussentence
(NLL_topic+prev). Thissuggeststhatsurpris- RoBERTa 87.7 87.6 87.8
ingeventsareofteninlinewiththestorytopicbut
Table3: ROCStoryClozeTest
notwiththeprevioussentence. Therefore,thelow
likelihood of transitioning between the previous
and current sentence is a strong predictor of sur-
Performance of Event Detector on ROC Story
prisingeventboundaries,inlinewithfindingsby
ClozeTest Ourcommonsenseandnarrativefea-
FosterandKeane(2015)onhowthedifficultyof
tures do not seem to significantly improve upon
linkingtwoadjacenteventsisanimportantfactor
RoBERTa’sperformanceintheROCStoryCloze
incausingsurprise.
Test(+0.2%F1),asobservedinTable3. Thisindi-
Features that explain changes in emotional
catesthatdetectingwhetherastoryendingfollows
valence Compared to sentences that contain no
commonsense can be effectively approached us-
eventboundaries,sentencesthatcontaineitherex-
ingRoBERTaalone,settingthistaskmightnotbe
pectedorsurprisingeventboundarieshavehigher
closely related to the Event Boundary Detection
xReact and oReact, which are emotional re-
Task.
sponses either by the subject or by others to an
event. Forexample,thisisthecaseforthesurpris-
7 Conclusion
ingandemotionaleventboundary"Irememberit
waslikethe3rdor4thgamewhensomethingbad
Wetacklethetaskofidentifyingeventboundaries
happened.."Thissuggeststhateventboundariesare
instories. Weproposeamodelthatcombinespre-
morelikelywhenasentenceismoreemotionally
dictions made using commonsense and narrative
charged,echoingworkbyDunsmooretal.(2018)
featureswithaRoBERTaclassifier. Wefoundthat
onhoweventsegmentationisparticularlyfrequent
integrating commonsense and narrative features
whentheemotionoffearistriggered.
cansignificantlyimprovethepredictionofsurpris-
ingeventboundariesthroughdetectingviolations
6 ComparisonwithStoryClozeTest
to commonsense relations (especially relating to
Tobetterunderstandhowsurprisingeventbound- the absence of causality), low likelihood events,
ariesrelatetodeviationfromcommonsensereason- and changes in emotional valence. Our model is
ing, we compare our Event Boundary Detection capable in detecting event boundaries with high
TasktotheROCStoryClozeTest(Mostafazadeh annotatoragreementbutlimitedindetectingthose
etal.,2016). Thistestinvolvesidentifyingwhether with lower agreement. Compared to identifying
acandidateendingsentencefollowscommonsense commonsenseandnonsensestoryendingsinStory
(commonsenseending)ordeviatesfromcommon- ClozeTest,ourtaskisfoundtobeonlytagentially
sense (nonsense ending) given the first four sen- related. Ourresultssuggestthatconsideringcom-
tences of a English short story. The ROC Story monsenseknowledgeandnarrativefeaturescanbe
Cloze Test dataset contains 3142 samples with apromisingdirectiontowardscharacterizingand
1571 commonsense endings and 1571 nonsense detectingeventboundariesinstories.
endings.3 WetrainaseparateEventBoundaryDe-
3WeusetheWinter2018version,whichcontainsadev trainourmodelonthedevportion.
andatestset.Asinpreviouswork(Schwartzetal.,2017),we 4Trainingtakes20minutesonanNvidiaP100GPU.
8
References Saadia Gabriel, Chandra Bhagavatula, Vered Shwartz,
Ronan Le Bras, Maxwell Forbes, and Yejin Choi.
Wasi Uddin Ahmad, Nanyun Peng, and Kai-Wei
2021. Paragraph-level commonsense transformers
Chang.2021. Gate: Graphattentiontransformeren-
with recurrent memory. Proceedings of the AAAI
coderforcross-lingualrelationandeventextraction.
ConferenceonArtificialIntelligence,35(14):12857–
12865.
Christopher Baldassano, Uri Hasson, and Kenneth A.
Norman. 2018. Representation of real-world event Sarik Ghazarian, Zixi Liu, Akash S M, Ralph
schemasduringnarrativeperception. TheJournalof Weischedel, Aram Galstyan, and Nanyun Peng.
Neuroscience,38(45):9689–9699. 2021. Plot-guidedadversarialexampleconstruction
forevaluatingopen-domainstorygeneration. InPro-
Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob ceedingsofthe2021ConferenceoftheNorthAmer-
Andreas, Yoshua Bengio, Joyce Chai, Mirella Lap- ican Chapter of the Association for Computational
ata, Angeliki Lazaridou, Jonathan May, Aleksandr Linguistics: Human Language Technologies, pages
Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. 4334–4344, Online. Association for Computational
Experiencegroundslanguage. InProceedingsofthe Linguistics.
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 8718–8735, JonathanGordonandBenjaminVanDurme.2013. Re-
Online.AssociationforComputationalLinguistics. portingbiasandknowledgeacquisition. InProceed-
ingsofthe2013WorkshoponAutomatedKnowledge
AntoineBosselut,HannahRashkin,MaartenSap,Chai- Base Construction, AKBC ’13, page 25–30, New
tanya Malaviya, Asli Çelikyilmaz, and Yejin Choi. York,NY,USA.AssociationforComputingMachin-
2019. Comet: Commonsense transformers for au- ery.
tomaticknowledgegraphconstruction. InProceed-
Arthur C Graesser, Scott P Robertson, and Patricia A
ings of the 57th Annual Meeting of the Association
Anderson. 1981. Incorporating inferences in narra-
forComputationalLinguistics(ACL).
tiverepresentations: Astudyofhowandwhy. Cog-
Luis Emilio Bruni, Sarune Baceviciute, and Mo- nitivePsychology,13(1):1–26.
hammed Arief. 2014. Narrative cognition in inter-
AnnaJafarpour,ElizabethABuffalo,RobertTKnight,
activesystems: Suspense-surpriseandthep300erp
and Anne GE Collins. 2019a. Event segmentation
component. In Interactive Storytelling, pages 164–
reveals working memory forgetting rate. Available
175,Cham.SpringerInternationalPublishing.
atSSRN3614120.
NathanaelChambersandDanJurafsky.2008. Unsuper- Anna Jafarpour, Sandon Griffin, Jack J Lin, and
visedlearningofnarrativeeventchains. InProceed- Robert T Knight. 2019b. Medial orbitofrontal cor-
ings of ACL-08: HLT, pages 789–797, Columbus, tex,dorsolateralprefrontalcortex,andhippocampus
Ohio.AssociationforComputationalLinguistics. differentially represent the event saliency. Journal
ofcognitiveneuroscience,31(6):874–884.
Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and
JunZhao.2017. Automaticallylabeleddatagenera- CA Kurby and JM Zacks. 2009. Segmentation in the
tionforlargescaleeventextraction. InProceedings perception and memory of events. Trends in cogni-
of the 55th Annual Meeting of the Association for tivesciences.
ComputationalLinguistics(Volume1:LongPapers),
ChristopherAKurbyandJeffreyMZacks.2008. Seg-
pages409–419,Vancouver,Canada.Associationfor
mentation in the perception and memory of events.
ComputationalLinguistics.
Trendsincognitivesciences,12(2):72–79.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
Kristina Toutanova. 2019. BERT: Pre-training of
extraction via structured prediction with global fea-
deep bidirectional transformers for language under-
tures. InProceedingsofthe51stAnnualMeetingof
standing. In Proceedings of the 2019 Conference
the Association for Computational Linguistics (Vol-
of the North American Chapter of the Association
ume1: LongPapers),pages73–82,Sofia,Bulgaria.
for Computational Linguistics: Human Language
AssociationforComputationalLinguistics.
Technologies, Volume 1 (Long and Short Papers),
pages4171–4186,Minneapolis,Minnesota.Associ- YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
ationforComputationalLinguistics. dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
JosephE.Dunsmoor,MarijnC.W.Kroes,CarolineM. Roberta: A robustly optimized bert pretraining ap-
Moscatelli, Michael D. Evans, Lila Davachi, and proach.
ElizabethA.Phelps.2018. Eventsegmentationpro-
tects emotional memories from competing experi- Lara Martin, Prithviraj Ammanabrolu, Xinyu Wang,
ences encoded close in time. Nature Human Be- WilliamHancock,ShrutiSingh,BrentHarrison,and
haviour,2(4):291–299. Mark Riedl. 2018. Event representations for auto-
mated story generation with deep neural nets. Pro-
Meadhbh I.Foster and Mark T.Keane. 2015. Predict- ceedingsoftheAAAIConferenceonArtificialIntel-
ingsurprisejudgmentsfromexplanationgraphs. ligence,32(1).
9
Quinn McNemar. 1947. Note on the sampling error Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and
of the difference between correlated proportions or Jianfeng Gao. 2020. PlotMachines: Outline-
percentages. Psychometrika,12(2):153–157. conditioned generation with dynamic plot state
tracking. In Proceedings of the 2020 Conference
NasrinMostafazadeh,NathanaelChambers,Xiaodong onEmpiricalMethodsinNaturalLanguageProcess-
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, ing (EMNLP), pages 4274–4295, Online. Associa-
Pushmeet Kohli, and James Allen. 2016. A cor- tionforComputationalLinguistics.
pusandclozeevaluationfordeeperunderstandingof
commonsense stories. In Proceedings of the 2016 Hannah Rashkin, Maarten Sap, Emily Allaway,
Conference of the North American Chapter of the NoahA.Smith,andYejinChoi.2018. Event2mind:
Association for Computational Linguistics: Human Commonsense inference on events, intents, and re-
LanguageTechnologies,pages839–849,SanDiego, actions. InACL.
California. Association for Computational Linguis-
tics. Nils Reimers and Iryna Gurevych. 2019. Sentence-
bert: Sentence embeddings using siamese bert-
Nasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, networks. InProceedingsofthe2019Conferenceon
David Buchanan, Lauren Berkowitz, Or Biran, and EmpiricalMethodsinNaturalLanguageProcessing.
Jennifer Chu-Carroll. 2020. GLUCOSE: GeneraL- AssociationforComputationalLinguistics.
ized and COntextualized story explanations. In
Proceedings of the 2020 Conference on Empirical
Corby Rosset. 2020. Turing-nlg: A 17-billion-
MethodsinNaturalLanguageProcessing(EMNLP),
parameterlanguagemodelbymicrosoft.
pages4569–4586,Online.AssociationforComputa-
tionalLinguistics.
Marie-Laure Ryan. 2010. Narratology and cognitive
science: A problematic relation. Style, 44(4):469–
Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison
495.
Gopnik,andTomGriffiths.2018. Evaluatingtheory
of mind in question answering. In Proceedings of
MaartenSap,EricHorvitz,YejinChoi,NoahASmith,
the 2018 Conference on Empirical Methods in Nat-
and James W Pennebaker. 2020. Recollection ver-
uralLanguageProcessing,pages2392–2400,Brus-
susimagination:Exploringhumanmemoryandcog-
sels, Belgium. Association for Computational Lin-
nitionvianeurallanguagemodels. InACL.
guistics.
Maarten Sap, Anna Jafarpour, Yejin Choi, Noah A.
TakakiOtake,ShoYokoi,NaoyaInoue,RyoTakahashi,
Smith, James W. Pennebaker, and Eric Horvitz.
TatsukiKuribayashi, andKentaroInui.2020. Mod-
2022. Computational lens on cognition: Study of
eling event salience in narratives via barthes’ car-
autobiographicalversusimaginedstorieswithlarge-
dinal functions. In Proceedings of the 28th Inter-
scalelanguagemodels.
national Conference on Computational Linguistics,
pages1784–1794, Barcelona, Spain (Online). Inter-
Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-
nationalCommitteeonComputationalLinguistics.
draBhagavatula,NicholasLourie,HannahRashkin,
Brendan Roof, Noah A. Smith, and Yejin Choi.
Jessica Ouyang and Kathleen McKeown. 2015. Mod-
2019a. Atomic: An atlas of machine common-
elingreportableeventsasturningpointsinnarrative.
senseforif-thenreasoning. ProceedingsoftheAAAI
In Proceedings of the 2015 Conference on Empiri-
Conference on Artificial Intelligence, 33(01):3027–
calMethodsinNaturalLanguageProcessing,pages
3035.
2149–2158,Lisbon,Portugal.AssociationforCom-
putationalLinguistics.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Ellie Pavlick and Tom Kwiatkowski. 2019. Inherent Le Bras, and Yejin Choi. 2019b. Social IQa: Com-
disagreementsinhumantextualinferences. Transac- monsense reasoning about social interactions. In
tions of the Association for Computational Linguis- Proceedings of the 2019 Conference on Empirical
tics,7:677–694. Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
Kyle A. Pettijohn and Gabriel A. Radvansky. 2016. guage Processing (EMNLP-IJCNLP), pages 4463–
Narrative event boundaries, reading times, and ex- 4473,HongKong,China.AssociationforComputa-
pectation. Memory&Cognition,44(7):1064–1075. tionalLinguistics.
Alec Radford, Jeff Wu, Rewon Child, David Luan, R.C. Schank and R. Abelson. 1977. Scripts, Plans,
DarioAmodei,andIlyaSutskever.2019. Language Goals,andUnderstanding. Hillsdale,NJ:Earlbaum
modelsareunsupervisedmultitasklearners. Assoc.
GabrielA.Radvansky,AndreaK.Tamplin,JosephAr- RoySchwartz,MaartenSap,IoannisKonstas,LiZilles,
mendarez, and Alexis N. Thompson. 2014. Differ- Yejin Choi, and Noah A Smith. 2017. The effect
entkindsofcausalityineventcognition. Discourse ofdifferentwritingtasksonlinguisticstyle: Acase
Processes,51(7):601–618. studyoftherocstoryclozetask. InCoNLL.
10
Matthew Sims, Jong Ho Park, and David Bamman.
2019. Literary event detection. In Proceedings of
the57thAnnualMeetingoftheAssociationforCom-
putational Linguistics, pages 3623–3634, Florence,
Italy.AssociationforComputationalLinguistics.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2020. Mpnet: Masked and permuted pre-
trainingforlanguageunderstanding. arXivpreprint
arXiv:2004.09297.
David J. Townsend. 2018. Stage salience and situa-
tionallikelihoodintheformationofsituationmodels
duringsentencecomprehension. Lingua,206:1–20.
Ercenur Ünal, Yue Ji, and Anna Papafragou. 2019.
From event representation to linguistic meaning.
TopicsinCognitiveScience,13(1):224–242.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. Ace 2005 multilingual
trainingcorpus.
David Wilmot and Frank Keller. 2021. Memory and
knowledge augmented language models for infer-
ringsalienceinlong-formstories. InProceedingsof
the 2021 Conference on Empirical Methods in Nat-
ural Language Processing, pages 851–865, Online
and Punta Cana, Dominican Republic. Association
forComputationalLinguistics.
Timothy D. Wilson and Daniel T. Gilbert. 2008. Ex-
plainingaway:Amodelofaffectiveadaptation. Per-
spectives on Psychological Science, 3(5):370–386.
PMID:26158955.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, ClementDelangue, AnthonyMoi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-artnaturallanguagepro-
cessing. InProceedingsofthe2020Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing:
SystemDemonstrations,pages38–45,Online.Asso-
ciationforComputationalLinguistics.
Lili Yao, Nanyun Peng, Weischedel Ralph, Kevin
Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-
and-write: Towardsbetterautomaticstorytelling. In
The Thirty-Third AAAI Conference on Artificial In-
telligence(AAAI-19).
Jeffrey M. Zacks. 2020. Event perception and mem-
ory. Annual Review of Psychology, 71(1):165–191.
PMID:31905113.
Jeffrey M Zacks, Nicole K Speer, Khena M Swallow,
ToddSBraver,andJeremyRReynolds.2007. Event
perception: a mind-brain perspective. Psychologi-
calbulletin,133(2):273.
11
A Appendix A.5 SimGenexperimentaldetails
We used the Turing-NLG model without further
A.1 Atomicrelationstrainingdetails
fine-tuning. The model has 17B and we used it
withtop-psampling(top-p=0.85),temperature=1.0
Weusedthetrain/dev/testsplitsfromtheoriginal
and max sequence length of 64 tokens. MPnet-
Atomicdataset(Sapetal.,2019a). Negativesam-
basemodelwasaccessedfromtheSentence-BERT
plesarecreatedbymatchingaAtomiceventnode
library (Reimers and Gurevych, 2019) and used
to a corresponding tail event node from another
withoutfurtherfine-tuning.
samplebasedontherelationshipinvolved. Sepcifi-
cally,negativesamplingwasperformedongroups
A.6 EventBoundaryDetectionModel
([’xWant’,’oWant’,’xNeed’,’xIntent’],[’xReact’,
trainingdetails
’oReact’,’xAttr’],[’xEffect’,’oEffect’])giventhat
AdamWoptimizerwasusedwithα = 5 10 6,fol-
thetaileventnodesineachgrouparemoresimilar, −
∗
lowingauniformsearchusingF1asthecriterionat
creatingmorediscriminatingnegativesamples,as
intervalsof 2.5,5,7.5,10 10n; 6 n 3.
inspiredbySapetal.(2019b). Onenegativesam-
{ }∗ − ≤ ≤ −
Learningratewaslinearlydecayed(8epochs)with
pleisintroducedeveryninepositivesamples,since
100warm-upsteps. Batchsizeof16wasused. Val-
there are nine labels. We used a learning rate of
idationwasdoneevery0.25epochsduringtraining.
1e-4, batch size of 64, 8 epochs and AdamW op-
Trainingeachmodeltookaround30minutesonan
timizer. Trainingtook18hoursonaNvidiaP100
NvidiaP100GPU.
GPU.
A.2 Glucoserelationstrainingdetails
BecausetheGlucosedataset(Mostafazadehetal.,
2020)wasnotsplitinitially,werandomlysplitthe
datasetintotrain/dev/testsplitsbasedona80/10/10
ratio. For each sample in Glucose, annotations
sharesimilarheadeventnodesinDim-1to5and
similartaileventnodesinDim-6to10. Therefore,
ournegativesamplingstrategyforDim-1to5in-
volvesrandomlychoosingatailnodefromDim-6
to 10 and vice-versa. As a result, one negative
sample is introduced every five samples. During
training,weusedalearningrateof1e-4,batchsize
of64, 8epochsandAdamWoptimizer. Training
took15hoursonaNvidiaP100GPU.
A.3 Realistrainingdetails
We used the train/dev/test split from the Realis
dataset (Sims et al., 2019). During training, we
usedtheAdamWoptimizer,alearningrateof2e-5,
3 epochsand batch sizeof 4, asinspired by (Sap
et al., 2020). Training took 1 hour on a Nvidia
P100GPU.
A.4 Sequentialityexperimentaldetails
GPT2-small was accessed from HuggingFace
Transformerslibraryandusedwithoutfurtherfine-
tuning. Ithas125Mparameters,acontextwindow
of1024,hiddenstatedimensionof768,12heads
anddropoutof0.1.
12

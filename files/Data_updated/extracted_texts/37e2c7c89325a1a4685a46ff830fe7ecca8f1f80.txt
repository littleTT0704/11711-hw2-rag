Learning to Scaffold:
Optimizing Model Explanations for Teaching
PatrickFernandes∗,Ψ,Ω,(cid:60) MarcosTreviso∗,Ω,(cid:60) DanishPruthi†,Λ
AndréF.T.MartinsΩ,(cid:60),Γ GrahamNeubigΨ
ΨLanguageTechnologiesInstitute,CarnegieMellonUniversity,Pittsburgh,PA
ΩInstitutoSuperiorTécnico&LUMLIS(LisbonELLISUnit),Lisbon,Portugal
(cid:60)InstitutodeTelecomunicações,Lisbon,Portugal
ΛAmazonWebServices ΓUnbabel,Lisbon,Portugal
Abstract
Modernmachinelearningmodelsareopaque,andasaresultthereisaburgeoning
academicsubfieldonmethodsthatexplainthesemodels’behavior. However,what
istheprecisegoalofprovidingsuchexplanations,andhowcanwedemonstrate
thatexplanationsachievethisgoal? Someresearcharguesthatexplanationsshould
help teach a student (either human or machine) to simulate the model being
explained,andthatthequalityofexplanationscanbemeasuredbythesimulation
accuracy of students on unexplained examples. In this work, leveraging meta-
learningtechniques,weextendthisideatoimprovethequalityoftheexplanations
themselves,specificallybyoptimizingexplanationssuchthatstudentmodelsmore
effectivelylearntosimulatetheoriginalmodel. Wetrainmodelsonthreenatural
language processing and computer vision tasks, and find that students trained
withexplanationsextractedwithourframeworkareabletosimulatetheteacher
significantlymoreeffectivelythanonesproducedwithpreviousmethods. Through
humanannotationsandauserstudy,wefurtherfindthattheselearnedexplanations
morecloselyalignwithhowhumanswouldexplaintherequireddecisionsinthese
tasks. Ourcodeisavailableathttps://github.com/coderpat/learning-scaffold.
1 Introduction
Whiledeeplearning’sperformancehasledittobecomethedominantparadigminmachinelearning,
itsrelativeopaquenesshasbroughtgreatinterestinmethodstoimprovemodelinterpretability. Many
recentworksproposemethodsforextractingexplanationsfromneuralnetworks(§7),whichvaryfrom
thehighlightingofrelevantinputfeatures[Simonyanetal.,2014,Arrasetal.,2017,Dingetal.,2019]
tomorecomplexrepresentationsofthereasoningofthenetwork[MuandAndreas,2020,Wuetal.,
2021].However,arethesemethodsactuallyachievingtheirgoalofmakingmodelsmoreinterpretable?
Someconcerningfindingshavecastdoubtonthisproposition;differentexplanationsmethodshave
beenfoundtodisagreeonthesamemodel/input[Neelyetal.,2021,Bastingsetal.,2021]andexplana-
tionsdonotnecessarilyhelppredictamodel’soutputand/oritsfailures[Chandrasekaranetal.,2018].
Infact,theresearchcommunityisstillintheprocessofunderstandingwhatexplanationsaresupposed
toachieve,andhowtoassesssuccessofanexplanationmethod[Doshi-VelezandKim,2017,Miller,
2019]. Manyearlyworksonmodelinterpretabilitydesignedtheirmethodsaroundasetofdesiderata
[Sundararajanetal.,2017,LertvittayakumjornandToni,2019]andreliedonqualitativeassessmentof
ahandfulofsampleswithrespecttothesedesiderata;aprocessthatishighlysubjectiveandishardto
reproduce. Incontrast,recentworkshavefocusedonmorequantitativecriteria: correlationbetween
∗ Equalcontribution.Corresp.topfernand@cs.cmu.eduormarcos.treviso@tecnico.ulisboa.pt
† WorkdonewhileatCarnegieMellonUniversity,priortojoiningAmazon.
36thConferenceonNeuralInformationProcessingSystems(NeurIPS2022).
2202
voN
03
]GL.sc[
2v01801.4022:viXra
studentlearning time student evaluation time
simulability loss
classifier y T = ? y S classifier y T = ? y S
e T = ? e S student student
simulability loss
explainer explainer regularizer explainer
×N
Figure 1: Illustration of our SMaT framework. First, a student model is trained to recover the
classifier’spredictionsandtomatchtheexplanationsgivenbytheexplainer. Then,theexplaineris
updatedbasedonhowwellthetrainedstudentsimulatestheclassifier(withoutaccesstoexplanations).
Inpractice,werepeatthesetwoconsecutiveprocessesforseveralsteps. Greenarrowsandboxes
representlearnablecomponents.
explainabilitymethodsformeasuringconsistency[JainandWallace,2019,SerranoandSmith,2019],
sufficiencyandcomprehensiveness[DeYoungetal.,2020],andsimulability: whetherahumanor
machineconsumerofexplanationsunderstandsthemodelbehaviorwellenoughtopredictitsoutput
onunseenexamples[Lipton,2016,Doshi-VelezandKim,2017]. Simulability,inparticular,hasa
numberofdesirableproperties,suchasbeingintuitivelyalignedwiththegoalofcommunicatingthe
underlyingmodelbehaviortohumansandbeingmeasurableinmanualandautomatedexperiments
[TrevisoandMartins,2020,HaseandBansal,2020,Pruthietal.,2020].
For instance, Pruthi et al. [2020] proposed a framework for automatic evaluation of simulability
that,givenateachermodelandexplanationsofthismodel’spredictions,trainsastudentmodelto
matchtheteacher’spredictions. Theexplanationsarethenevaluatedwithrespecttohowwellthey
helpastudentlearntosimulatetheteacher(§2). Thisisanalogoustotheconceptinpedagogyof
instructionalscaffolding[VandePoletal.,2010],aprocessthroughwhichateacheraddssupport
forstudentstoaidlearning. Moreeffectivescaffolding—inourcase,betterexplanations—isassumed
toleadtobetterstudentlearning. However,whilethispreviousworkprovidesanattractivewayto
evaluateexistingexplanationmethods,itstopsshortofproposingamethodtoactuallyimprovethem.
Inthiswork, weproposetolearntoexplainbydirectlylearningexplanationsthatprovidebetter
scaffoldingofthestudent’slearning,aframeworkwetermScaffold-MaximizingTraining(SMaT).
Figure 1 illustrates the framework: the explainer is used to scaffold the student training, and is
updatedbasedonhowwellthestudentdoesattesttimeatsimulatingtheteachermodel. Wetake
insights from research on meta-learning [Finn et al., 2017, Raghu et al., 2021], formalizing our
settingasabi-leveloptimizationproblemandoptimizingitbasedonhigher-orderdifferentiation
(§3). Importantly,ourhigh-levelframeworkmakesfewassumptionsaboutthemodelwearetrying
toexplain,thestructureoftheexplanationsorthemodalitiesconsidered. Totestourframework,we
thenintroduceaparameterizedattention-basedexplaineroptimizablewithSMaTthatworksforany
modelwithattentionmechanisms(§4).
WeexperimentwithSMaTintextclassification,imageclassification,and(multilingual)text-based
regressiontasksusingpretrainedtransformermodels(§5). Wefindthatourframeworkisableto
effectivelyoptimizeexplainersacrossalltheconsideredtasks,wherestudentstrainedwithlearned
attention explanations achieve better simulability than baselines trained with static attention or
gradient-basedexplanations. Wefurtherevaluatetheplausabilityofourexplanations(i.e.,whether
producedexplanationsalignwithhowpeoplewouldjustifyasimilarchoice)usinghuman-labeled
explanations(textclassificationandtextregression)andthroughahumanstudy(imageclassification)
andfindthatexplanationslearnedwithSMaTaremoreplausiblethanthestaticexplainersconsidered.
Overall,theresultsreinforcetheutilityofscaffoldingasacriterionforevaluatingandimproving
modelexplanations.
2 Background
ConsideramodelT : X → Y trainedonsomedatasetD = {(x ,y )}N . Forexample, this
train i i i=1
couldbeatextorimageclassifierthatwastrainedonaparticulardownstreamtask(withD being
train
thetrainingdataforthattask). Post-hocinterpretabilitymethodstypicallyintroduceanexplainer
moduleE :T ×X →E thattakesamodelandaninput,andproducesanexplanatione∈E forthe
T
outputofthemodelgiventhatinput,whereE denotesthespaceofpossibleexplanations.Forinstance,
2
interpretabilitymethodsusingsaliencymapsdefineE asthespaceofnormalized distributionsof
importanceoverLinputelementse∈(cid:52) (where(cid:52) isthe(L−1)-probabilitysimplex).
L−1 L−1
Pruthietal.[2020]proposedanautomaticframeworkforevaluatingexplainersthattrainsastudent
model S : X → Y with parameters θ to simulate the teacher (i.e., the original classifier) in a
θ
constrainedsetting. Forexample,thestudentcanbeconstrainedtohavelesscapacitythantheteacher
byusingasimplermodelortrainedwithasubsetofthedatasetusedfortheteacher(Dˆ (cid:40)D ).
train train
In this framework, a baseline student S is trained according to θ∗ =
θ
argmin θE
(x,y)∼Dˆ
train[L sim(S θ(x),T(x))], and its simulability SIM(S θ∗,T) is measured on
an unseen test set. The actual form of L
sim
and SIM(S θ∗,T) is task-specific. For example, in a
classificationtask,weusecross-entropyasthesimulationlossL overtheteacher’spredictions,
sim
whilethesimulabilityofamodelS canbedefinedasthesimulationaccuracy,i.e.,whatpercentage
θ∗
ofthestudentandteacherpredictionsmatchoveraheld-outtestsetD :
test
SIM(S θ∗,T)=E (x,y)∼Dtest[1{S θ∗(x)=T(x)}]. (1)
Next,thetrainingofthestudentisaugmentedwithexplanationsproducedbytheexplainerE. We
introduceastudentexplainerE :S×X →E,(theS-explainer)toextractexplanationsfromthe
S
student,andregularizingtheseexplanationsontheexplanationsofteacher(theT-explainer),usinga
lossL thattakesexplanationsforbothmodels:
expl
(cid:20) (cid:21)
θ∗ =argminE L (S (x),T(x))+βL (E (S ,x),E (T,x)) . (2)
E
θ
(x,y)∼Dˆ
train
(cid:124)sim θ
(cid:123)(cid:122) (cid:125)
(cid:124)expl S θ
(cid:123)(cid:122)
T
(cid:125)
simulabilityloss explainerregularizer
Forexample,Pruthietal.[2020]consideredasateacherexplainerE variousmethodssuchasLIME
T
[Ribeiroetal.,2016],IntegratedGradients[Sundararajanetal.,2017],andattentionmechanisms,and
exploredbothattentionregularization(usingKullback-Leiblerdivergence)andmulti-tasklearningto
regularizethestudent.
The key assumption surrounding this evaluation framework is that a student trained with good
explanations should learn to simulate the teacher better than a student trained with bad or no
(cid:0) (cid:1)
explanations,thatis,SIM S θ∗,T > SIM(S θ∗,T).Forclarity,wewillrefertothesimulabilityofa
E
modelS
θ∗
trainedusingexplanationsasscaffoldedsimulability.
E
3 OptimizingExplainersforTeaching
As a first contribution of this work, we extend the previously described framework to make it
possibletodirectlyoptimizetheteacherexplainersothatitcanmosteffectivelyteachthestudentthe
originalmodel’sbehavior. Tothisend,consideraparameterizedT-explainerE withparameters
φT
φ ,andequivalentlyaparameterizedS-explainerE withparametersφ . Wecanwritetheloss
T φS S
functionforthestudentandS-explaineras:
L (S ,E ,T,E ,x)=L (S (x),T(x))+βL (E (S ,x),E (T,x)). (3)
student θ φS φT sim θ expl φS θ φT
While this framework is flexible enough to rigorously and automatically evaluate many types of
explanations, calculating scaffolded simulability requires an optimization procedure to learn the
studentandS-explainerparametersθ,φ . Thismakesitnon-trivialtoachieveourgoalofdirectly
S
findingtheteacherexplainerparametersφ thatoptimizescaffoldedsimulability. Toovercomethis
T
challenge,wedrawinspirationfromtheextensiveliteratureonmeta-learning[Schmidhuber,1987,
Finnetal.,2017],andframetheoptimizationasthefollowingbi-leveloptimizationproblem(see
Grefenstetteetal.[2019]foraprimer):
θ∗(φ ),φ∗(φ )=argminE [L (S ,E ,T,E ,x)] (4)
T S T
θ,φS
(x,y)∼Dˆ
train
student θ φS φT
φ∗ =argminE (cid:2) L (cid:0) S (x),T(x)(cid:1)(cid:3) . (5)
T (x,y)∼Dtest sim θ∗(φT)
φT
Here,theinneroptimizationupdatesthestudentandtheS-explainerparameters(Equation4),and
in the outer optimization we update the T-explainer parameters (Equation 5). Importantly, our
frameworkdoesnotmodifytheteacher,asourgoalistoexplainamodelwithoutchangingitsoriginal
behavior. Noticethatwealsosimplifytheproblembyconsideringthemoretractablesimulationloss
L siminsteadofthesimulabilitymetricSIM(S θ∗,T)aspartoftheobjectivefortheouteroptimization.
3
Now, if we assume the explainers E and E are differentiable, we can use gradient-based
φT φS
optimization[Finnetal.,2017]tooptimizeboththestudent(withitsexplainer)andtheT-explainer.
Inparticular,weuseexplicitdifferentiationtosolvethisoptimizationproblem. Tocomputegradients
forφ ,wehavetodifferentiatethroughagradientoperation,whichrequiresHessian-vectorproducts,
T
anoperationsupportedbymostmoderndeeplearningframeworks[Bradburyetal.,2018,Grefenstette
et al., 2019]. However, explicitly computing gradients for φ through a large number of inner
T
optimizationstepsiscomputationallyintractable. Tocircumventthisproblem,typicallytheinner
optimizationisrunforonlyacoupleofstepsoratruncated gradientiscomputed[Shabanetal.,
2019]. Inthiswork,wetaketheapproachoftakingasingleinneroptimizationstepandlearningthe
studentandS-explainerjointlywiththeT-explainerwithoutresettingthestudent[Deryetal.,2021].
Ateachstep,weupdatethestudentandS-explainerparametersasfollows:
(cid:104) (cid:105)
θt+1 =θt−η ∇ E L (S ,E ,T,E ,x) (6)
INN θ (x,y)∼Dˆ
train
student θt φt
S
φt
T
(cid:104) (cid:105)
φt+1 =φt −η ∇ E L (S ,E ,T,E ,x) . (7)
S S INN φS (x,y)∼Dˆ train student θt φt S φt T
After updating the student, we take an extra gradient step with the new parameters but only use
theseupdatestocalculatetheouter-gradientforφ ,withoutactuallyupdatingθ. Thisapproachis
T
similartothepilotupdateproposedbyZhouetal.[2021],andweverifiedthatitledtomorestable
optimizationinpractice:
(cid:104) (cid:105)
θ(φt )=θt+1−η ∇ E L (S ,E ,T,E ,x) (8)
T INN θ (x,y)∼Dˆ
train
student θt+1 φt S+1 φt
T
(cid:104) (cid:16) (cid:17)(cid:105)
φt+1 =φt −η ∇ E L S (x),T(x) . (9)
T T OUT φT (x,y)∼Dtest sim θ(φt T)
4 ParameterizedAttentionExplainer
Asasecondcontributionofthiswork,weintroduceanovelparameterizedattention-basedexplainer
thatcanbelearnedwithourframework. Transformermodels[Vaswanietal.,2017]arecurrentlythe
mostsuccessfuldeep-learningarchitectureacrossavarietyoftasks[Shoeybietal.,2019,Wortsman
etal.,2022]. Underpinningtheirsuccessisthemulti-headattentionmechanism,whichcomputesa
normalizeddistributionoverthe1≤i≤Linputelementsinparallelforeachheadh:
Ah = SOFTMAX(Qh(Kh)(cid:62)), (10)
where Qh = [qh,··· ,qh] and Kh = [kh,··· ,kh] are the query and key linear projections over
0 L 0 L
the input element representations for head h. Attention mechanisms have been used extensively
forproducingsaliencymaps[WiegreffeandPinter,2019,Vashishthetal.,2019]andwhilesome
concernshavebeenraisedregardingtheirfaithfulness[JainandWallace,2019],overallattention-
basedexplainershavebeenfoundtoleadtorelativelygoodexplanationsintermsofplausibilityand
simulability[TrevisoandMartins,2020,Kobayashietal.,2020,Pruthietal.,2020].
However,toextractexplanationsfrommulti-headattention,wehavetwoimportantdesignchoices:
1. Singledistributionselection: Sinceself-attentionproducesanattentionmatrixAh ∈(cid:52)L ,we
L−1
needtopooltheseattentiondistributionstoproduceasinglesaliencymape∈(cid:52) . Typically,
L−1
thedistributionfromasingletoken(suchas[CLS])ortheaverageoftheattentiondistributions
fromalltokens1≤i≤Lareused.
2. Headselection: Wealsoneedtopoolthedistributionsproducedbyeachhead. Typicalad-hoc
strategiesincludeusingthemeanoverallheadsforacertainlayer[Fomichevaetal.,2021b]or
selectingasingleheadbasedonplausibilityonvalidationset[Trevisoetal.,2021]. However,
sincetransformerscanhavehundredsoreventhousandsofheads,thesechoicesrelyonhuman
intuitionorrequirelargeamountsofplausibilitylabels.
In this work, we approach the latter design choice in a more principled manner. Concretely, we
associateeachheadwithaweightandthenperformaweightedsumoverallheads. Theseweights
arelearnedsuchthattheresultingexplanationmaximizessimulability,asdescribedin§3. More
formally,givenamodelT anditsqueryandkeyprojectionsforaninputxforeachlayerandhead
θT
h≤H,wedefineaparameterized,differentiableattentionexplainerE (T ,x)as
φT θT
L (cid:32) H (cid:33)
1 (cid:88) (cid:88)
sh =
L
(q ih)(cid:62)Kh, E φT(T,x)= SOFTMAX λh Tsh , (11)
i=1 h=1
wheretheteacher’sheadcoefficientsλ
T
∈(cid:52) H−1areλ
T
= NORMALIZE(φ T)withφ
T
∈RH.
4
Inthisformulation,sh ∈RLrepresents head 1 head 2 head H
theaverageunnormalizedattentionlog- k11 k21 ... kL1 k12 k22 ... kL2 k1H k2H ... kLH
q1 q2 qH
itsoverallinputelements,whicharethen 1 1 1
q1 q2 . . . qH
combinedaccordingtoλ andnormal- 2 2 2
T
izedwithSOFTMAXtoproduceadistri- q1 q2 qH
L L L
butionin(cid:52) . Weapplyanormaliza-
L−1
row-wise mean row-wise mean row-wise mean
tionfunctionNORMALIZEtoheadcoeffi-
s1 × λ1 s2 × λ2 sH ×λH
cientsinvolvedtocreateaconvexcombi-
nationoverallheadsinalllayers. Inthis sum
softmax( ) ∈ △
workweconsiderthesparseprojection L-1
function NORMALIZE = SPARSEMAX Figure 2: Our parameterized attention-based explainer.
[MartinsandAstudillo,2016],as: Dashed red boxes represent learned parameters λ T =
SPARSEMAX(φ T)∈(cid:52) H−1,weightingaverageattention
SPARSEMAX(z)=argmin(cid:107)p−z(cid:107) 2. logits of each head 1 ≤ h ≤ H. A softmax over the
p∈(cid:52)H−1 weightedsumgeneratestheattentionprobabilities.
WechooseSPARSEMAXduetoitsben-
efitsintermsofinterpretability, sinceitleadstomanyheadshavingzeroweight. Wealsofound
itoutperformedeveryotherprojectionwetried(see§6foramoredetaileddiscussion). Figure2
illustrateseachstepofourparameterizedattentionexplainer.
5 Experiments
Toevaluateourframework,weattempttolearnexplainersfortransformermodelstrainedonthree
different tasks: text classification (§ 5.1), image classification (§ 5.2), and machine translation
qualityestimation(atext-basedregressiontask, detailedin§5.3). WeuseJAX[Bradburyetal.,
2018]toimplementthehigher-orderdifferentiation,andusepretrainedtransformermodelsfromthe
HuggingfaceTransformerslibrary[Wolfetal.,2020],togetherwithFlax[Heeketal.,2020]. For
eachtask,wetrainateachermodelwithAdamW[LoshchilovandHutter,2019]but,asexplainedin
§3,weuseSGDforthestudentmodel(innerloop). Wealsousescalarmixing[Petersetal.,2018]to
poolrepresentationsfromdifferentlayersautomatically.3 Wetrainstudentswithateacherexplainer
inthreesettings:
• NoExplainer: Noexplanationsareprovided,andnoexplanationregularizationisusedfortraining
thestudent(i.e.β =0inEquation3). Werefertostudentsinthissettingasbaselinestudents.
• Static Explainer: Explanations for the teacher model are extracted with five commonly-used
saliency-basedexplainers: (1)L2normofgradients;(2)agradient×inputexplainer[Deniletal.,
2014];(3)anintegratedgradientsexplainer[Sundararajanetal.,2017];andattentionexplainers
thatusesthemeanpoolingoverattentionfrom(4)allheadsinthemodeland(5)fromtheheadsof
thelastlayer[Fomichevaetal.,2021b,Vafaetal.,2021]. MoredetailscanbefoundinAppendixA.
• LearnedExplainer(SMaT):Explanationsareextractedwiththeexplainerdescribedin§4,with
coefficientsforeachheadthataretrainedwithSMaTjointlywiththestudent. Weinitializethe
coefficientssuchthatthemodelisinitializedtobethesameasthestaticattentionexplainer(i.e.,
performingthemeanoverallheads).
Independently of the T-explainer, we always use a learned attention-based explainer as the S-
explainer,consideringallheadsexceptwhentheT-explainerisastaticattentionexplainerthatonly
considersthelastlayers’heads,wherewedothesamefortheS-explainer. WeusetheKullback-
Leibler divergence as L , and we set β = 5 for attention-based explainers and β = 0.2 for
expl
gradient-based explainers (since we found smaller values to be better). We set L as the cross-
sim
entropy loss for classification tasks, and as the mean squared error loss for text regression. For
eachsetting,wetrainfivestudentswithdifferentseeds. Sincethereissomevarianceinstudents’
performance(wehypothesizeduetothesmalltrainingsets)wereportthemedianandinterquantile
range(IQR)aroundit(relativetothe25-75percentile).
3Whilescalarmixingreducedvarianceofstudentperformance,SMaTalsoworkedwithothercommon
poolingmethods.
5
... ... ...
Table1: ResultsfortheIMDBdatasetwithrespecttostudentsimulabilityintermsofaccuracy(%).
Underlinedvaluesindicatehighersimulabilitythanbaselinewithnon-overlappingIQR.
500 1,000 2,000
NoExplainer 81.72[81.24:81.75] 83.44[83.36:83.63] 84.84[84.80:84.88]
GradientL2 81.66[81.32:82.00] 82.98[82.72:83.08] 84.78[84.96:85.08]
Gradient×Input 84.83[84.79:84.88] 81.15[80.95:81.36] 83.84[83.59:84.99]
IntegratedGradients 82.99[82.59:82.99] 81.79[81.72:81.87] 84.20[84.03:85.03]
Attention(alllayers) 83.00[82.60:83.00] 85.72[85.72:86.23] 90.08[89.72:90.11]
Attention(lastlayer) 80.91[79.99:81.07] 83.15[82.91:83.51] 91.47[91.39:91.56]
Attention(SMaT) 91.48[91.40:91.56] 92.56[92.28:92.83] 92.84[92.84:93.08]
5.1 TextClassification
For text classification, we consider the IMDB dataset [Maas et al., 2011], a binary sentiment
classificationtaskoverhighlypolarizedEnglishmoviereviews. Asthebasepretrainedmodel,weuse
thesmallELECTRAmodel[Clarketal.,2020],with12layersand4headsineach(total48heads).
Like the setting in Pruthi et al. [2020], we use the original training set with 25,000 samples to
traintheteacher,andfurthersplitthetestsetintoatrainingsetforthestudentandadevandtest
set. Wevarythenumberofsamplesthestudentistrainedonbetween500,1,000,and2,000. We
evaluatesimulabilityusingaccuracy(i.e.,whatpercentageofstudentpredictionsmatchwithteacher
predictions). Theteachermodelobtains91%accuracyonthestudenttestset.
Table1showstheresultsinterminstegoratfedgsraidmientsu: lnaoboffienlsiettyoan(yEoneqwuhoasatwiothinsan1d)likefdoitr,btuthiethreesettings. Wecansee
that,overall,theattentionexplainheaterdittr!aitidnragegeddonwanidthonaSndMthereawTasnloetaavdersygtooodpslottu,adlsoe,ntsthatsimulatetheteacher
toosimpleandtheactingwassoso...iwouldgivethiss##nor##efe##st
modelmuchmoreaccuratelythana2sattuthedmeosnttstrainedwithoutanyexplanations,andmoreaccurately
thanstudentstrainedwithanystaattitecntioen(xallplaylears)i:nneoroffaencsertooasnysoneawllhossatwutdhiseanndtlikterdaiti,nbuitnigsetsizes. Interestingly,the
hatedit!itdraggedonandonandtherewasnotaverygoodplot,also,
gradient-basedexplainersonlyimtpoorsoimvpleeanodvtheearctintghweasbsoasos.e.l.iinweoulsdtguivedtheisnst#s#nowr##hefee#n#sttheamountoftrainingdata
isverylow,andactuallydegradesaim2atuthleambositlityforlargeramountsofdata(seediscussioninA).Using
onlyheadsfromthelastlayerseeamttenstiont(oSMhaTa):vneo otffhenese too apnyponoe wshiot esawe thfisf eandcltik,edl eit a,bdutiingtohighersimulabilitythan
hatedit! itdraggedon andon andtherewasnot a very goodplot ,also ,
allotherstaticexplainersonlyfortoloasrimgplee randt rthaeiacntinignwgasssoesots. .. . iwould give this s##nor##efe##st
a 2 at the most
Table2:Plausibilityon
integrated gradients: no offense to anyone who saw this and integratedgradients: i'veseenriver ##danceinpersonandnothingcompares
liked it , but i hated it ! it dragged on and on and there was tothevideo,buttheshowisawesome.thedancersareamazing.themusic MovieReviewsinterms
not a very good plot , also , too simple and the acting was so isimpact##ing.andtheoverallperformanceisoutstanding.i'venever
so...iwouldgivethiss##nor##efe##sta2atthemost seenanythinglikeit!isuggestthatyouseethisshowifyoucan!!! of AUC. * represents
attention (all layers): no offense to anyone who saw this and attention(alllayers): i'veseenriver##danceinpersonandnothingcompares methods that use hu-
likedit,butihatedit!itdraggedonandonandtherewas tothevideo,buttheshowisawesome.thedancersareamazing.themusic manlabels.
not a very good plot , also , too simple and the acting was so isimpact##ing.andtheoverallperformanceisoutstanding.i'venever
so . . . i would give this s ##nor ##efe ##st a 2 at the most seenanythinglikeit!isuggestthatyouseethisshowifyoucan!!!
attention (SMaT): no offense to anyone who saw this and attention(SMaT): i ' ve seen river ##dance inperson and nothing compares AUC
liked it , but i hated it ! it dragged on and on and there was to the video ,buttheshowisawesome.thedancersareamazing.themusic
Grad.L2 0.65
notaverygoodplot,also,toosimpleandtheactingwasso isimpact##ing.andtheoverallperformanceisoutstanding.i 'venever
Grad.×Input 0.51
so . . . i would give this s ##nor ##efe ##st a 2 at the most seenanythinglike it! isuggestthatyousee thisshowif you can!!!
IntegratedGrad. 0.53
Fi int geg urate rd egra 3die :nts E: i x' pve lasee nn ariv te ir o## nda snce gin ivper eso nnan bd yintegratedgradients,attention(lastlayer), Attn.(alllayers) 0.68
nothing compares to the video , but the show is awesome . Attn.(lastlayer) 0.61
a nthed danocerus rare lameaazinrgn . ethed muasict itse imnpatcit o ##ning .e axnd p thle ainer (SMaT) for two movie reviews of Attn.(SMaT) 0.73
overall performance is outstanding . i ' ve never seen anything
th lie ke itI !M i suD ggeB st thad t ya out a ses e e thit s s( hon w e if g yoa u t ci anv !e ! !a nd positive examples). Green and orange Attn.(bestlayer)*0.75
reattpenrtioen s(alel lnayetrs)p: o i 's viet sievene rivaer n ##ddancne ein g pearstoni v aned contributions,respectively. Attn.(besthead)* 0.75
nothing compares to the video , but the show is awesome .
thedancersareamazing.themusicisimpact##ing.andthe
P olvaeraull speirfbormialnicte yis oautsntanadinlgy . s i i' s ve. nWever e seens eanlytehincg tthemedianmodeltrainedwith1,000samplesandextractexplana-
like it ! i suggest that you see this show if you can ! ! !
tionsfortestsamplesfromtheMovieReviewsdataset[DeYoungetal.,2020],whichcontainsbinary
attention (learned): i ' ve seen river ##dance in person and
senonthitnigmcomepanresttomtheovvideioe,brutethveisheowwissawfersoomem. RottenTomatoesalongsidehuman-rationaleannotation. Sincethe
the dancers are amazing . the music is impact ##ing . and the
la obverealll speraforrmeancbe iisn ouatsrtayndin(gi .n i d ' viec naevteri nseegn awnythhineg theratokenispartoftheexplanationornot)andthepredictedscores
arliekeirte!aislugvgesatlthuateyosu,sewetheissfhoowllifoyowucaFn!o!m! ichevaetal.[2021a]andreportourresultsintermsoftheAreaUnder
theCurve(AUC),whichautomaticallyconsidersmultiplebinarizationthresholds. Theresultsare
showninTable2alongwithtworandomlyselectedexamplesofextractedexplanationsinFigure3.We
foundthatgradient-basedexplanationsarelessplausiblethanthoseusingattention(withtheexception
ofGrad.L2,whichissimilartostaticattention)andthatonesproducedwithSMaTachievethehighest
plausibility,indicatingthatourlearnedexplainercanproducehuman-likeexplanationswhilemaximiz-
ingsimulability. Moreover,SMaTachievesasimilarAUCscoretothebestperformingattentionlayer
andhead,4whilenotrequiringanyhumanannotations. Thisisevidencethatscaffoldedsimulability,
whilenotexplicitlydesignedforit,isagoodproxyforplausibilityand“human-like”explanations.
4AUCscoresobtainedbyindependentlytryingallattentionheadsandlayersofthemodel.
6
Table3: Simulabilityresults,intermsofaccuracy(%),ontheCIFAR100dataset. Underlinedvalues
representbetterperformancethanbaselinewithnon-overlappingIQR
2,250 4,500 9,000
NoExplainer 81.16[80.98:81.26] 84.02[83.98:84.24] 85.20[85.17:85.26]
GradientL2 80.97[80.91:81.10] 83.98[83.81:84.23] 85.13[84.97:85.50]
Gradient×Input 80.93[80.82:81.04] 83.99[83.98:84.13] 85.33[84.85:85.35]
Integratedgradients 80.22[80.17:80.35] 83.44[83.25:83.44] 84.99[84.76:85.22]
Attention(alllayers) 82.53[82.53:82.62] 84.81[84.74:84.92] 85.92[85.78:85.94]
Attention(lastlayer) 82.34[82.30:82.60] 84.65[84.56:84.81] 85.31[84.84:85.31]
Attention(SMaT) 83.09[82.77:83.28] 85.42[85.39:85.85] 85.96[85.74:86.35]
5.2 ImageClassification
To validate our framework across multiple modalities, we consider image classification on the
CIFAR-100dataset[Krizhevsky,2009]. WeuseasthebasemodeltheVisionTransformer(ViT)
[Dosovitskiyetal.,2020],inparticularthebaseversionwith16×16patchesthatwasonlypretrained
onImageNet-21k[Ridniketal.,2021]. Weup-sampleimagestotoa224×224resolution.
Since the self-attention mechanism in the ViT model only works with patch representations, the
explanationsproducedbyattention-basedexplainerswillbeatpatch-levelratherthanpixel-level. We
splittheoriginalCIFAR-100trainingsetintoanewtrainingsetwith45,000andavalidationsetwith
5,000. Unliketheprevioustask,wereusethetrainingsetforboththeteacherandstudent,varyingthe
numberofsamplesthestudentistrainedwithbetween2,250(5%),4,500(10%)and9,000(20%).
Weuseaccuracyasthesimulabilitymetricandtheteacherobtains89%ontestset.
Table3showstheresultsforthethreesettings.Similarlytotheresultsinthetextmodality,theattention
explainertrainedwithSMaTachievesthebestscaffoldingperformance,althoughthegapstostatic
attention-basedexplainersaresmaller(especiallywhenstudentsaretrainedwithmoresamples).Here,
thegradient-basedexplainersalwaysdegradesimulabilityacrossthetestedtrainingsetsizesandandit
seemsimportantthattheexplanationsincludeattentioninformationfromlayersotherthanthelastone.
Plausibilityanalysis. SincetherearenoavailablehumanannotationsforplausibilityintheCIFAR-
100 dataset, we design a user study to measure the plausability of the considered methods. The
originalimageandexplanationsextractedwithGradient×Input,IntegratedGradients,Attention
(all layers), and Attention (SMaT) are shown to the user, and the user has to rank the different
explanationstoanswerthequestion“Whichexplanationalignsthemostwithhowyouwouldexplain
asimilardecision?”. Explanationswereannotatedbythreevolunteers. Aftercollectingresults,we
computetherankandtheTrueSkillrating[Herbrichetal.,2007]foreachexplainer(roughly,the
“skill”leveliftheexplainerswhereplayersingame). FurtherdescriptioncanbefoundinAppendixB.
TheresultsareshowninTable4. Asintheprevioustask,attentiontrainedwithSMaToutperforms
allotherexplainersintermsofplausibility,anditspredictedratingismuchhigherthanallother
explainers. WealsoshowexamplesofexplanationsforasetofrandomlyselectedimagesinFigure4.
InputimageInteg.Grad.Attn.(alllx.)Attn.(SMaT)InputimageInteg.Grad.Attn.(alllx.)Attn.(SMaT)
Table 4: Plausibility
results of the human
study on visual expla-
nations.
RankTrueSkill
Grad.×Input 3-4 -2.7±.67
Figure4: Explanationsgivenbyintegratedgradients,attention(alllayers), Integ.Grad. 3-4 -2.1±.67
Attn.(alllx.) 2 0.7±.67
andlearnedattentionexplainerforasetofinputimagesofCIFAR-100.
Attn.(SMaT) 1 4.3±.70
Goldlabelsare: “television”,“butterfly”,“cockroach”,and“sunflower”.
5.3 MachineTranslationQualityEstimation
QualityEstimation(QE)isthetaskofpredictingaqualityscoregivenasentenceinasourcelanguage
andatranslationinatargetlanguagefromamachinetranslationsystem,whichrequiresmodelsthat
considerinteractionsbetweenthetwoinputs,sourceandtarget. Scorestendtobecontinuousvalues
(makingthisaregressiontask)thatwerecollectedfromexpertannotators.
7
Table5: Simulabilityresults,intermsofPearsoncorrelation,ontheML-QEdataset. Underlined
valuesrepresentbetterperformancethanbaselinewithnon-overlappingIQR.
2,100 4,200 8,400
NoExplainer .7457[.7366:.7528] .7719[.7660:.7802] .7891[.7860:.7964]
GradientL2 .8065[.8038:.8268] .8535[.7117:.8544] .8638[.8411:.8657]
Gradient×Input .6846[.6781:.6894] .6922[.6885:.6965] .7141[.7136:.7147]
Integratedgradients .6686[.6677:.6694] .7086[.6994:.7101] .7036[.6976:.7037]
Attention(alllayers) .8120[.7955:.8125] .8193[.8186:.8280] .8467[.8464:.8521]
Attention(lastlayer) .7486[.7484:.7534] .7720[.7672:.7726] .7798[.7717:.7814]
Attention(SMaT) .8156[.8096:.8183] .8630[.8412:.8724] .8561[.8512:.8689]
Table6: PlausibilityresultsforsourceandtargetinputsforeachlanguagepairoftheMLQE-PE
datasetintermsofAUC.*representssupervisedmethodsthatusehumanlabelsinsomeform.
EN-DE EN-ZH ET-EN NE-EN RO-EN RU-EN OVERALL
src. tgt. src. tgt. src. tgt. src. tgt. src. tgt. src. tgt. src. tgt.
GradientL2 0.64 0.65 0.65 0.49 0.67 0.61 0.68 0.55 0.72 0.68 0.65 0.54 0.67 0.59
Gradient×Input 0.58 0.60 0.61 0.51 0.60 0.54 0.61 0.49 0.64 0.59 0.58 0.51 0.61 0.54
IntegratedGradients 0.59 0.60 0.63 0.49 0.60 0.52 0.64 0.48 0.64 0.59 0.60 0.51 0.62 0.53
Attention(alllayers) 0.60 0.63 0.68 0.52 0.60 0.61 0.58 0.55 0.66 0.70 0.62 0.55 0.62 0.59
Attention(lastlayer) 0.51 0.49 0.61 0.49 0.51 0.50 0.55 0.48 0.52 0.57 0.56 0.50 0.54 0.50
Attention(SMaT) 0.64 0.65 0.68 0.52 0.66 0.64 0.66 0.54 0.71 0.70 0.61 0.54 0.66 0.60
Attention(bestlayer)* 0.64 0.65 0.69 0.64 0.64 0.68 0.68 0.68 0.71 0.76 0.64 0.59 0.65 0.65
Attention(besthead)* 0.67 0.67 0.70 0.65 0.70 0.70 0.70 0.69 0.73 0.75 0.67 0.60 0.67 0.66
Interpreting quality scores of machine translated outputs is a problem that has received recent
interest[Fomichevaetal.,2021a]sinceitallowsidentifyingwhichwordswereresponsibleforabad
translation. WeusetheMLQE-PEdataset[Fomichevaetal.,2020],whichcontains7,000training
samplesforeachofsevenlanguagepairsalongsideword-levelhumanannotation. Weuseasthebase
modelapretrainedXLM-R-base[Conneauetal.,2019],amultilingualmodelwith12layersand12
headsineach(totalof144heads).
Weexcludeoneofthelanguagepairsinthedataset(si-en)sincetheXLM-Rmodeldidnotsupport
it,leadingtoatrainingsetwith42,000samples. SimilartotheCIFAR-100case,wereusethesame
trainingsetforboththeteacherandstudent,samplingasubsetforthelatter. Wevarythenumberof
samplesthestudentistrainedwithbetween2,100(5%),4,200(10%)and8,400(20%). Sincethisis
aregressiontask,weevaluatesimulabilityusingthePearsoncorrelationcoefficientbetweenstudent
andteacher’spredictions.5 Theteacherachieves0.63correlationonthetestset.
Table5showstheresultsforthethreesettings. Similartoothertasks,theattentionexplainertrained
withSMaTleadstostudentswithhighersimulabilitythanbaselinestudentsandsimilarorhigher
thanstaticexplaineracrossalltrainingsetsizes. Curiously,theGrad. L2explainerachievesvery
highsimulabilityforthistask. ItevenhasahighermediansimulabilityscorethanSMaTfor8,400
samples. However,weattributethistovarianceinthestudenttrainingsetsampling(thatcouldlead
toanimbalanceinlanguagepairproportions)whichcouldexplainwhySMaTperformancedegrades
withmoresamples. Forthistask,thegradient-basedexplainersalwaysdegradesimulabilityacross
thetestedtrainingsetsize. Italsoseemsthatusingonlythelastlayer’sattentionisalsoineffectiveat
teachingstudents,achievingthesameperformanceasthebaseline.
Plausibilityanalysis. Weselectthemedianmodeltrainedwith4,200samplesandfollowtheap-
proachdevisedintheExplainableQEsharedtasktoevaluateplausibility[Fomichevaetal.,2021a],
whichconsistsofevaluatingthehuman-likenessofexplanationsintermsofAUConlyonthesubset
oftranslationsthatcontainerrors. TheresultsareshowninTable6. Wenotethatforalllanguage
pairs,SMaTperformsonparorbetterthanstaticexplainers,andonlybeingsurpassedbyGrad. L2in
thesource-sideoveralllanguages. Comparingwiththebestattentionlayer/head,anapproachusedby
Fomichevaetal.[2021b],Trevisoetal.[2021],SMaTachievessimilarAUCscoresforsourceexpla-
nations,butlagsbehindthebestattentionlayer/headfortargetexplanationson*-ENlanguagepairs.
However,asstressedpreviouslyfortextandimageclassification,SMaTsidestepshumanannotation
andavoidsthecumbersomeapproachofindependentlycomputingplausibilityscoresforallheads.
5Pearsoncorrelationisthestandardmetricusedtoevaluatesentence-levelQEmodels.
8
6 ImportanceoftheHeadProjection
Amajorcomponentofourframeworkisthenormalizationoftheheadcoefficients,asdefinedin
§4. Althoughmanyfunctionscanbeusedtomapscorestoprobabilities,wefoundempiricallythat
SPARSEMAXperformsthebest,whileothertransformationssuchasSOFTMAXand1.5-ENTMAX
[Peters et al., 2019], a sparse transformation more dense than sparsemax, usually lead to poorly
performingstudents(seeTable7).
Table7: Simulabilityresults,intermsofaccuracy(%),ontheMLQEdatasetwith4200training
examples,withdifferentnormalizationfunctions.
SPARSEMAX SOFTMAX 1.5-ENTMAX NoNormalization
NoExplainer .7719±[.7660:.7802] .7719±[.7660:.7802] .7719±[.7660:.7802] .7719±[.7660:.7802]
Attention(alllayers) .8193±[.8186:.8280] .7345±[.7335:.7390] .7152±[.7111:.7161] .7781±[.7762:.7791]
Attention(lastlayer) .7720±[.7672:.7726] .7697±[.7659:.7715] .7807±[.7652:.7821] .7768±[.7764:.7807]
Attention(SMaT) .8630±[.8412:.8724] .7439±[.7430:.7484] .7163±[.7130:.7239] .8002±[.7919:.8100]
Furthermore,anotherbenefitofSPARSEMAXisthatitproducesasmallsubsetofactiveheads. The
heatmaps of attention coefficients (λ ) learned after training, shown in Figure 5, exemplify this.
T
We can see that the dependency between head position (layer it belongs to) and its coefficient is
task/dataset/model specific, with CIFAR-100 and MLQE having opposite observations. We also
foundempiricallythatactiveheads(λh >0)usuallyleadtohigherplausibilityscoresthaninactive
T
heads,furtherreinforcingthegoodplausibilityfindingsofSMaT.
Head coefficients Head coefficients Head coefficients
0 0 0.08 0
1 0.25 1 1
0.07 0.08
2 2 2
3 0.20 3 0.06 3
4 4 0.05 4 0.06
5 0.15 5 5
6 6 0.04 6
0.04
7 0.10 7 0.03 7
8 8 8
9 0.05 9 0.02 9 0.02
10 10 0.01 10
11 11 11
0.00 0.00 0.00
0 1 2 3 0 1 2 3 4 5 6 7 8 9 1011 0 1 2 3 4 5 6 7 8 9 1011
Head Head Head
Figure5: Headcoefficientsfortextclassification(left),imageclassification(middle),andquality
estimation (right), illustrating that only a small subset of attention heads are deemed relevant by
SMaTduetoSPARSEMAX.
7 RelatedWork
Explainabilityfortext&vision.Severalworksproposeexplainabilitymethodstointerpretdecisions
madebyNLPandCVmodels. Besidesgradientandattention-basedapproachesalreadymentioned,
someextractexplanationsbyrunningthemodelswithperturbedinputs[Ribeiroetal.,2016,Feng
etal.,2018,Kimetal.,2020].Othersevendefinecustombackwardpassestoassignrelevanceforeach
feature[Bachetal.,2015]. Thesemethodsarecommonlyemployedtogetherwithpost-processing
heuristics,suchasselectingonlythetop-ktokens/pixelswithhigherscoresforvisualization. Another
lineofworkseekstobuildaclassifierwithinherentlyinterpretablecomponents,suchasmethods
basedonattentionmechanismsandrationalizers[Leietal.,2016,Bastingsetal.,2019].
Evaluationofexplainabilitymethods. Asmentionedintheintroduction, earlyworksevaluated
explanationsbasedonpropertiessuchasconsistency, sufficiencyandcomprehensiveness. Jacovi
andGoldberg[2020]recommendedtheuseofagradednotionoffaithfulness,whichtheERASER
benchmarkquantifiesusingtheideaofsufficientandcomprehensiverationales,alongsidecompiling
datasetswithhuman-annotatedrationalesforcalculatingplausibilitymetrics[DeYoungetal.,2020].
Giventhedisagreementbetweenexplainabilitymethods,Neelyetal.[2021]showedthatwithouta
faithfulground-truthexplanationitisimpossibletodeterminewhichmethodisbetter. Diagnostic
testssuchastheonesproposedbyAdebayoetal.[2018],WiegreffeandPinter[2019]andAtanasova
etal.[2020]aremoreinformativeyettheydonotcapturethemaingoalofanexplanation: theability
tocommunicateanexplanationtoapractitioner.
9
reyaL reyaL reyaL
Simulability. A new dimension for evaluating explainability methods relies on the forward pre-
diction/simulationproposedbyLipton[2016]andDoshi-VelezandKim[2017],whichstatesthat
humansshouldbeabletocorrectlysimulatethemodel’soutputgiventheinputandtheexplanation.
Chandrasekaranetal.[2018],HaseandBansal[2020],Aroraetal.[2022]analyzesimulabilityvia
humanstudiesacrosstextclassificationdatasets. TrevisoandMartins[2020]designedanautomatic
frameworkwherestudents(machineorhuman)havetopredictthemodel’soutputgivenanexplana-
tionasinput. Similarly,Pruthietal.[2020]proposedthesimulabilityframeworkthatwasextended
inourwork,whereexplanationsareusedtoregularizethestudentratherthanpassedasinput.
Learning to explain. The concept of simulability also opens a path to learning explainers. In
particularTrevisoandMartins[2020]learnanattention-basedexplainerthatmaximizessimulability.
However,directlyoptimizingforsimulabilitysometimesledtoexplainersthatlearnedtrivialprotocols
(such as selecting only punctuation symbols or stopwords to leak the label). Our approach of
optimizingateacher-studentframeworkissimilartoapproachesthatoptimizeformodeldistillation
[Zhou et al., 2021]. However, these approaches modify the original model rather than introduce
a new explainer module. Raghu et al. [2021] propose a framework similar to ours for learning
commentariesforinputsthatspeedupandimprovethetrainingofamodel. Howevercommentaries
aremodel-independentandareoptimisedtoimproveperformanceontherealtask. Rationalizers
[Chenetal.,2018,JacoviandGoldberg,2021,GuerreiroandMartins,2021]alsodirectlylearnto
extractexplanations,butcanalsosufferfromtrivialprotocols.
8 Conclusion&FutureWork
WeproposedSMaT,aframeworkfordirectlyoptimizingexplanationsofthemodel’spredictions
to improve the training of a student simulating the said model. We found that, across tasks and
domains, explanationslearnedwithSMaTbothleadtostudentsthatsimulatetheoriginalmodel
moreaccuratelyandaremorealignedwithhowpeopleexplainsimilardecisionswhencompared
topreviouslyproposedmethods. Ontopofthat,ourparameterizedattentionexplainerprovidesa
principledwayfordiscoveringrelevantattentionheadsintransformers.
Ourworkshowsthatscaffoldingisasuitablecriterionforbothevaluatingandoptimizingexplainabil-
itymethods,andwehopethatSMaTpaveswayfornewresearchtodevelopexpressiveinterpretable
componentsforneuralnetworksthatcanbedirectlytrainedwithouthuman-labeledexplanations.
However,itshouldbenotedthat“interpretability”isalooselydefinedconcept,andthereforecau-
tionshouldbeexercisedwhenmakingstatementsaboutthequalityofexplanationsbasedonlyon
simulability,especiallyifthesestatementsmighthavesocietalimpacts.
Weonlyexploredlearningattention-basedexplainers,butourmethodcanalsobeusedtooptimize
other types of explainability methods, including gradient-based ones, by introducing learnable
parametersintheirformulations. Anotherpromisingfutureresearchdirectionistoexploreusing
SMaTtolearnexplanationsotherthansaliencymaps.
Acknowledgments
ThisworkwassupportedbytheEuropeanResearchCouncil(ERCStGDeepSPIN758969),byEU’s
HorizonEuropeResearchandInnovationActions(UTTER,contract101070631),byP2020project
MAIA (LISBOA-01-0247- FEDER045909), and Fundação para a Ciência e Tecnologia through
projectPTDC/CCI-INF/4703/2021(PRELUNA)andcontractUIDB/50008/2020. Wearegratefulto
NunoSabino,ThalesBertaglia,HenricoBrum,andAntonioFarinhasfortheparticipationinhuman
evaluationexperiments.
References
JuliusAdebayo,JustinGilmer,MichaelMuelly,IanGoodfellow,MoritzHardt,andBeenKim. Sanity
checksforsaliencymaps. Advancesinneuralinformationprocessingsystems,31,2018.
SiddhantArora,DanishPruthi,NormanSadeh,WilliamCohen,ZacharyLipton,andGrahamNeubig.
Explain,edit,andunderstand: Rethinkinguserstudydesignforevaluatingmodelexplanations.
InThirty-SixthAAAIConferenceonArtificialIntelligence(AAAI),Vancouver,Canada,February
2022. URLhttps://arxiv.org/abs/2112.09669.
10
Leila Arras, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. Explaining recur-
rent neural network predictions in sentiment analysis. In Proceedings of the 8th Workshop
on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages
159–168,Copenhagen,Denmark,September2017.AssociationforComputationalLinguistics.
doi:10.18653/v1/W17-5221. URLhttps://aclanthology.org/W17-5221.
PepaAtanasova,JakobGrueSimonsen,ChristinaLioma,andIsabelleAugenstein.Adiagnosticstudy
ofexplainabilitytechniquesfortextclassification. InProceedingsofthe2020ConferenceonEm-
piricalMethodsinNaturalLanguageProcessing(EMNLP),pages3256–3274,Online,November
2020.AssociationforComputationalLinguistics. doi:10.18653/v1/2020.emnlp-main.263. URL
https://aclanthology.org/2020.emnlp-main.263.
SebastianBach,AlexanderBinder,GrégoireMontavon,FrederickKlauschen,Klaus-RobertMüller,
andWojciechSamek. Onpixel-wiseexplanationsfornon-linearclassifierdecisionsbylayer-wise
relevancepropagation. PLoSONE,10,2015.
JasmijnBastings,WilkerAziz,andIvanTitov. Interpretableneuralpredictionswithdifferentiable
binary variables. In Proceedings of the 57th Annual Meeting of the Association for Computa-
tionalLinguistics,pages2963–2977,Florence,Italy,July2019.AssociationforComputational
Linguistics. doi:10.18653/v1/P19-1284. URLhttps://aclanthology.org/P19-1284.
JasmijnBastings,SebastianEbert,PolinaZablotskaia,AndersSandholm,andKatjaFilippova. "will
youfindtheseshortcuts?"Aprotocolforevaluatingthefaithfulnessofinputsaliencemethodsfor
textclassification. CoRR,abs/2111.07367,2021. URLhttps://arxiv.org/abs/2111.07367.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
ArjunChandrasekaran,VirajPrabhu,DeshrajYadav,PrithvijitChattopadhyay,andDeviParikh. Do
explanationsmakeVQAmodelsmorepredictabletoahuman? InProceedingsofthe2018Confer-
enceonEmpiricalMethodsinNaturalLanguageProcessing,pages1036–1042,Brussels,Belgium,
October-November2018.AssociationforComputationalLinguistics. doi:10.18653/v1/D18-1128.
URLhttps://aclanthology.org/D18-1128.
JianboChen,LeSong,MartinWainwright,andMichaelJordan. Learningtoexplain:Aninformation-
theoreticperspectiveonmodelinterpretation. InInternationalConferenceonMachineLearning,
pages883–892.PMLR,2018.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-
training text encoders as discriminators rather than generators. In ICLR, 2020. URL https:
//openreview.net/pdf?id=r1xMH1BtvB.
AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,Fran-
ciscoGuzmán,EdouardGrave,MyleOtt,LukeZettlemoyer,andVeselinStoyanov. Unsupervised
cross-lingualrepresentationlearningatscale. arXivpreprintarXiv:1911.02116,2019.
MishaDenil,AlbanDemiraj,andNandodeFreitas. Extractionofsalientsentencesfromlabelled
documents. ArXiv,abs/1412.6815,2014.
LucioM.Dery,PaulMichel,AmeetS.Talwalkar,andGrahamNeubig. Shouldwebepre-training?
anargumentforend-taskawaretrainingasanalternative. ArXiv,abs/2109.07437,2021.
JayDeYoung,SarthakJain,NazneenFatemaRajani,EricLehman,CaimingXiong,RichardSocher,
andByronC.Wallace. ERASER:AbenchmarktoevaluaterationalizedNLPmodels. InProceed-
ingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages4443–4458,
Online,July2020.AssociationforComputationalLinguistics. doi:10.18653/v1/2020.acl-main.408.
URLhttps://aclanthology.org/2020.acl-main.408.
ShuoyangDing,HainanXu,andPhilippKoehn. Saliency-drivenwordalignmentinterpretationfor
neuralmachinetranslation. InProceedingsoftheFourthConferenceonMachineTranslation(Vol-
ume1:ResearchPapers),pages1–12,Florence,Italy,August2019.AssociationforComputational
Linguistics. doi:10.18653/v1/W19-5201. URLhttps://aclanthology.org/W19-5201.
11
FinaleDoshi-VelezandBeenKim. Towardsarigorousscienceofinterpretablemachinelearning.
arXivpreprintarXiv:1702.08608,2017.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929,2020.
ShiFeng,EricWallace,AlvinGrissomII,MohitIyyer,PedroRodriguez,andJordanBoyd-Graber.
Pathologiesofneuralmodelsmakeinterpretationsdifficult. InProceedingsofthe2018Conference
onEmpiricalMethodsinNaturalLanguageProcessing,pages3719–3728,Brussels,Belgium,
October-November2018.AssociationforComputationalLinguistics. doi:10.18653/v1/D18-1407.
URLhttps://aclanthology.org/D18-1407.
ChelseaFinn,PieterAbbeel,andSergeyLevine. Model-agnosticmeta-learningforfastadaptationof
deepnetworks. InDoinaPrecupandYeeWhyeTeh,editors,Proceedingsofthe34thInternational
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pages1126–1135.PMLR,06–11Aug2017. URLhttps://proceedings.mlr.press/v70/
finn17a.html.
MarinaFomicheva,ShuoSun,LisaYankovskaya,FrédéricBlain,FranciscoGuzmán,MarkFishel,
Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. Unsupervised quality estimation for
neural machine translation. Transactions of the Association for Computational Linguistics, 8:
539–555,2020.
Marina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. The
Eval4NLP shared task on explainable quality estimation: Overview and results. In Proceed-
ings of the 2nd Workshop on Evaluation and Comparison of NLP Systems, pages 165–178,
PuntaCana,DominicanRepublic,November2021a.AssociationforComputationalLinguistics.
doi:10.18653/v1/2021.eval4nlp-1.17. URLhttps://aclanthology.org/2021.eval4nlp-1.
17.
Marina Fomicheva, Lucia Specia, and Nikolaos Aletras. Translation error detection as rationale
extraction. arXivpreprintarXiv:2108.12197,2021b.
EdwardGrefenstette,BrandonAmos,DenisYarats,PhuMonHtut,ArtemMolchanov,Franziska
Meier,DouweKiela,KyunghyunCho,andSoumithChintala.Generalizedinnerloopmeta-learning.
arXivpreprintarXiv:1910.01727,2019.
Nuno M. Guerreiro and André F. T. Martins. SPECTRA: Sparse structured text rationalization.
In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-
ing, pages 6534–6550, Online and Punta Cana, Dominican Republic, November 2021. Asso-
ciation for Computational Linguistics. doi:10.18653/v1/2021.emnlp-main.525. URL https:
//aclanthology.org/2021.emnlp-main.525.
Peter Hase and Mohit Bansal. Evaluating explainable AI: Which algorithmic explanations help
userspredictmodelbehavior? InProceedingsofthe58thAnnualMeetingoftheAssociationfor
ComputationalLinguistics,pages5540–5552,Online,July2020.AssociationforComputational
Linguistics. doi:10.18653/v1/2020.acl-main.491. URL https://aclanthology.org/2020.
acl-main.491.
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner,andMarcvanZee. Flax: AneuralnetworklibraryandecosystemforJAX,2020. URL
http://github.com/google/flax.
Ralf Herbrich, Tom Minka, and Thore Graepel. Trueskill(tm): A bayesian skill rating sys-
tem. In Advances in Neural Information Processing Systems 20, pages 569–576. MIT
Press,January2007. URLhttps://www.microsoft.com/en-us/research/publication/
trueskilltm-a-bayesian-skill-rating-system/.
AlonJacoviandYoavGoldberg. TowardsfaithfullyinterpretableNLPsystems: Howshouldwe
defineandevaluatefaithfulness? InProceedingsofthe58thAnnualMeetingoftheAssociationfor
ComputationalLinguistics,pages4198–4205,Online,July2020.AssociationforComputational
12
Linguistics. doi:10.18653/v1/2020.acl-main.386. URL https://aclanthology.org/2020.
acl-main.386.
Alon Jacovi and Yoav Goldberg. Aligning Faithful Interpretations with their Social Attribution.
TransactionsoftheAssociationforComputationalLinguistics,9:294–310,032021. ISSN2307-
387X. doi:10.1162/tacl_a_00367. URLhttps://doi.org/10.1162/tacl_a_00367.
Sarthak Jain and Byron C. Wallace. Attention is not Explanation. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
HumanLanguageTechnologies,Volume1(LongandShortPapers),pages3543–3556,Minneapolis,
Minnesota,June2019.AssociationforComputationalLinguistics. doi:10.18653/v1/N19-1357.
URLhttps://www.aclweb.org/anthology/N19-1357.
SiwonKim,JihunYi,EunjiKim,andSungrohYoon. InterpretationofNLPmodelsthroughinput
marginalization.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP),pages3154–3167,Online,November2020.AssociationforComputational
Linguistics. doi:10.18653/v1/2020.emnlp-main.255. URLhttps://aclanthology.org/2020.
emnlp-main.255.
GoroKobayashi,TatsukiKuribayashi,ShoYokoi,andKentaroInui. Attentionisnotonlyaweight:
Analyzingtransformerswithvectornorms. InProceedingsofthe2020ConferenceonEmpirical
MethodsinNaturalLanguageProcessing(EMNLP),pages7057–7075,Online,November2020.
AssociationforComputationalLinguistics. doi:10.18653/v1/2020.emnlp-main.574. URLhttps:
//aclanthology.org/2020.emnlp-main.574.
AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Technicalreport,University
ofToronto,2009.
TaoLei,ReginaBarzilay,andTommiJaakkola. Rationalizingneuralpredictions. Inroceedingsofthe
2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages107–117,2016.
Piyawat Lertvittayakumjorn and Francesca Toni. Human-grounded evaluations of explanation
methodsfortextclassification. InProceedingsofthe2019ConferenceonEmpiricalMethodsin
NaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguage
Processing(EMNLP-IJCNLP),pages5195–5205,HongKong,China,November2019.Association
forComputationalLinguistics.doi:10.18653/v1/D19-1523.URLhttps://aclanthology.org/
D19-1523.
Zachary C. Lipton. The mythos of model interpretability: In machine learning, the concept of
interpretabilityisbothimportantandslippery. Queue,16(3):31–57,jun2016. ISSN1542-7730.
doi:10.1145/3236386.3241340. URLhttps://doi.org/10.1145/3236386.3241340.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
ConferenceonLearningRepresentations,2019. URLhttps://openreview.net/forum?id=
Bkg6RiCqY7.
AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopher
Potts. Learningwordvectorsforsentimentanalysis. InProceedingsofthe49thAnnualMeetingof
theAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages142–150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015.
AndreMartinsandRamonAstudillo. Fromsoftmaxtosparsemax: Asparsemodelofattentionand
multi-labelclassification. InMariaFlorinaBalcanandKilianQ.Weinberger,editors,Proceedings
ofThe33rdInternationalConferenceonMachineLearning,volume48ofProceedingsofMachine
LearningResearch,pages1614–1623,NewYork,NewYork,USA,20–22Jun2016.PMLR. URL
https://proceedings.mlr.press/v48/martins16.html.
Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial
intelligence,267:1–38,2019.
JesseMuandJacobAndreas. Compositionalexplanationsofneurons. ArXiv,abs/2006.14032,2020.
13
MichaelNeely,StefanFSchouten,MauritsJRBleeker,andAnaLucic.Orderinthecourt:Explainable
aimethodspronetodisagreement. InICMLWorkshoponTheoreticFoundation,Criticism,and
ApplicationTrendofExplainableAI,2021. URLhttps://arxiv.org/abs/2105.03287.
BenPeters,VladNiculae,andAndréF.T.Martins. Sparsesequence-to-sequencemodels. InProceed-
ingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages1504–1519,
Florence,Italy,July2019.AssociationforComputationalLinguistics. doi:10.18653/v1/P19-1146.
URLhttps://aclanthology.org/P19-1146.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,
andLukeZettlemoyer. Deepcontextualizedwordrepresentations. InProceedingsofthe2018
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers), pages 2227–2237, New Orleans,
Louisiana,June2018.AssociationforComputationalLinguistics. doi:10.18653/v1/N18-1202.
URLhttps://aclanthology.org/N18-1202.
DanishPruthi,BhuwanDhingra,LivioBaldiniSoares,MichaelCollins,ZacharyC.Lipton,Graham
Neubig,andWilliamW.Cohen. Evaluatingexplanations: Howmuchdoexplanationsfromthe
teacher aid students? CoRR, abs/2012.00893, 2020. URL https://arxiv.org/abs/2012.
00893.
AniruddhRaghu,MaithraRaghu,SimonKornblith,DavidDuvenaud,andGeoffreyHinton. Teaching
with commentaries. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=4RbdgBh9gE.
MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin. "whyshouldItrustyou?": Explainingthe
predictionsofanyclassifier. InProceedingsofthe22ndACMSIGKDDInternationalConference
onKnowledgeDiscoveryandDataMining,SanFrancisco,CA,USA,August13-17,2016,pages
1135–1144,2016.
T.Ridnik,EmanuelBen-Baruch,AsafNoy,andLihiZelnik-Manor. Imagenet-21kpretrainingforthe
masses. ArXiv,abs/2104.10972,2021.
JurgenSchmidhuber. Evolutionaryprinciplesinself-referentiallearning.onlearningnowtolearn:
Themeta-meta-meta...-hook. Diplomathesis,TechnischeUniversitatMunchen,Germany,14May
1987. URLhttp://www.idsia.ch/~juergen/diploma.html.
SofiaSerranoandNoahA.Smith. Isattentioninterpretable? InProceedingsofthe57thAnnual
Meeting of the Association for Computational Linguistics, pages 2931–2951, Florence, Italy,
July2019.AssociationforComputationalLinguistics. doi:10.18653/v1/P19-1282. URLhttps:
//aclanthology.org/P19-1282.
AmirrezaShaban,Ching-AnCheng,NathanHatch,andByronBoots. Truncatedback-propagation
forbileveloptimization. InKamalikaChaudhuriandMasashiSugiyama,editors,Proceedingsof
theTwenty-SecondInternationalConferenceonArtificialIntelligenceandStatistics,volume89of
ProceedingsofMachineLearningResearch,pages1723–1732.PMLR,16–18Apr2019. URL
https://proceedings.mlr.press/v89/shaban19a.html.
MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatan-
zaro. Megatron-lm: Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism,
2019. URLhttps://arxiv.org/abs/1909.08053.
KarenSimonyan, AndreaVedaldi, andAndrewZisserman. Deepinsideconvolutionalnetworks:
Visualisingimageclassificationmodelsandsaliencymaps. CoRR,abs/1312.6034,2014.
MukundSundararajan,AnkurTaly,andQiqiYan. Axiomaticattributionfordeepnetworks. InDoina
PrecupandYeeWhyeTeh,editors,Proceedingsofthe34thInternationalConferenceonMachine
Learning,volume70ofProceedingsofMachineLearningResearch,pages3319–3328.PMLR,
06–11Aug2017. URLhttps://proceedings.mlr.press/v70/sundararajan17a.html.
MarcosTrevisoandAndréF.T.Martins. Theexplanationgame: Towardspredictionexplainability
throughsparsecommunication. InProceedingsoftheThirdBlackboxNLPWorkshoponAnalyzing
andInterpretingNeuralNetworksforNLP,pages107–118,Online,November2020.Association
14
for Computational Linguistics. doi:10.18653/v1/2020.blackboxnlp-1.10. URL https://www.
aclweb.org/anthology/2020.blackboxnlp-1.10.
Marcos Treviso, Nuno M. Guerreiro, Ricardo Rei, and André F. T. Martins. IST-unbabel 2021
submissionfortheexplainablequalityestimationsharedtask. InProceedingsofthe2ndWorkshop
onEvaluationandComparisonofNLPSystems,pages133–145,PuntaCana,DominicanRepublic,
November2021.AssociationforComputationalLinguistics. doi:10.18653/v1/2021.eval4nlp-1.14.
URLhttps://aclanthology.org/2021.eval4nlp-1.14.
Keyon Vafa, Yuntian Deng, David Blei, and Alexander Rush. Rationales for sequential predic-
tions. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pages 10314–10332, Online and Punta Cana, Dominican Republic, November
2021.AssociationforComputationalLinguistics. doi:10.18653/v1/2021.emnlp-main.807. URL
https://aclanthology.org/2021.emnlp-main.807.
JannekeVandePol,MoniqueVolman,andJosBeishuizen.Scaffoldinginteacher–studentinteraction:
Adecadeofresearch. Educationalpsychologyreview,22(3):271–296,2010.
ShikharVashishth,ShyamUpadhyay,GauravSinghTomar,andManaalFaruqui. Attentioninter-
pretabilityacrossnlptasks. ArXiv,abs/1909.11218,2019.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InI.Guyon,U.V.Luxburg,S.Bengio,
H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeuralInformation
ProcessingSystems,volume30.CurranAssociates,Inc.,2017. URLhttps://proceedings.
neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Proceedings of the 2019
ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJoint
ConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages11–20,HongKong,China,
November2019.AssociationforComputationalLinguistics. doi:10.18653/v1/D19-1002. URL
https://aclanthology.org/D19-1002.
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,
PierricCistac,TimRault,RémiLouf,MorganFuntowicz,JoeDavison,SamShleifer,Patrickvon
Platen,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,SylvainGugger,Mariama
Drame,QuentinLhoest,andAlexanderM.Rush. Transformers: State-of-the-artnaturallanguage
processing. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguage
Processing: SystemDemonstrations,pages38–45,Online,October2020.AssociationforCompu-
tationalLinguistics. URLhttps://www.aclweb.org/anthology/2020.emnlp-demos.6.
MitchellWortsman,GabrielIlharco,SamirYitzhakGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,
AriS.Morcos,HongseokNamkoong,AliFarhadi,YairCarmon,SimonKornblith,andLudwig
Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy
withoutincreasinginferencetime,2022. URLhttps://arxiv.org/abs/2203.05482.
Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld. Polyjuice: Generating
counterfactuals for explaining, evaluating, and improving models. In Proceedings of the 59th
AnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJoint
ConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pages6707–6723,Online,
August 2021. Association for Computational Linguistics. doi:10.18653/v1/2021.acl-long.523.
URLhttps://aclanthology.org/2021.acl-long.523.
WangchunshuZhou,CanwenXu,andJulianMcAuley. Metalearningforknowledgedistillation.
arXivpreprintarXiv:2106.04570,2021.
Checklist
1. Forallauthors...
(a) Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthepaper’s
contributionsandscope? Yes
15
(b) Didyoudescribethelimitationsofyourwork? Yes,see§8
(c) Didyoudiscussanypotentialnegativesocietalimpactsofyourwork? Yes,see§8
(d) Haveyoureadtheethicsreviewguidelinesandensuredthatyourpaperconformsto
them? Yes
2. Ifyouareincludingtheoreticalresults...
(a) Didyoustatethefullsetofassumptionsofalltheoreticalresults? N/A
(b) Didyouincludecompleteproofsofalltheoreticalresults? N/A
3. Ifyouranexperiments...
(a) Didyouincludethecode,data,andinstructionsneededtoreproducethemainexperi-
mentalresults(eitherinthesupplementalmaterialorasaURL)?Yes
(b) Didyouspecifyallthetrainingdetails(e.g.,datasplits,hyperparameters,howthey
werechosen)? Yes,withunspecifiedonesincludedinthecode
(c) Didyoureporterrorbars(e.g.,withrespecttotherandomseedafterrunningexperi-
mentsmultipletimes)? Yes
(d) Didyouincludethetotalamountofcomputeandthetypeofresourcesused(e.g.,type
ofGPUs,internalcluster,orcloudprovider)? No
4. Ifyouareusingexistingassets(e.g.,code,data,models)orcurating/releasingnewassets...
(a) Ifyourworkusesexistingassets,didyoucitethecreators? Yes
(b) Didyoumentionthelicenseoftheassets? No
(c) DidyouincludeanynewassetseitherinthesupplementalmaterialorasaURL?Yes
(d) Didyoudiscusswhetherandhowconsentwasobtainedfrompeoplewhosedatayou’re
using/curating? No
(e) Didyoudiscusswhetherthedatayouareusing/curatingcontainspersonallyidentifiable
informationoroffensivecontent? No
5. Ifyouusedcrowdsourcingorconductedresearchwithhumansubjects...
(a) Didyouincludethefulltextofinstructionsgiventoparticipantsandscreenshots,if
applicable? Yes,seeAppendixB
(b) Did you describe any potential participant risks, with links to Institutional Review
Board(IRB)approvals,ifapplicable? N/A
(c) Didyouincludetheestimatedhourlywagepaidtoparticipantsandthetotalamount
spentonparticipantcompensation? N/A(Volunteers)
A ExplainerDetails
Withtheintegratedgradientsexplainer[Sundararajanetal.,2017],weuse10iterationsfortheintegral
inthesimulabilityexperiments(duetothecomputationcosts)and50iterationsfortheplausability
experiments. Weusezerovectorsasbaselineembeddings,sincewefoundlittlevariationinchanging
this.Forbothgradients-basedexplainers,weprojectintothesimplexbyusingtheSOFTMAXfunction,
similartotheattention-basedexplainers. Thisresultsinverynegativevalueshavinglowprobability
values. Moreover,forevaluatingplausibilityontextclassificationandtranslationqualityestimation,
wecomputedtheexplanationscoreofasinglewordbysummingthescoresofitswordpieces.
Wewouldliketonotethat,unlikethesettinginPruthietal.[2020],wedonotapplyatop-kpost-
processingheuristicongradients/attentionlogits, insteaddirectlyprojectingthemtothesimplex.
Thismightexplainthedifferenceinresultstotheoriginalpaper,particularlyforthelowsimulability
performanceofstaticexplainers.
B HumanStudyforVisualExplanations
The annotations were collected through an annotation webpage, built on top of Flask. Figure 6
showsthethreepagesofthesite. Duringtheannotation,userswereaskedtorankfourexplanations,
unnamedandinrandomorder. Aftercollectingtheratings,wecomputedtheTrueSkillrating,with
aninitialratingforeachmethodofµ=0,σ =0.5. Afterlearningtheratings,wethencomputethe
ranksbyobtainingthe95%confidenceintervalfortheratingeachmethod,andconstructingapartial
orderingofmethodsbasedonthis.
16
Thevolunteerswereamixtureofgraduatesorgraduatestudentsknownbytheauthors. Howeverwe
wouldliketopointoutthatduetoblindnatureofthemethodannotation,thechanceofbiasislow.
Figure6: Loginpage(left),dashboard(middle)andannotationpage(right)
InFigures7and8weshowrawattentionexplanationsextractedfromalllayers(rows)andheads
(columns) of the teacher transformer used in our CIFAR-100 experiments. Cross-checking the
explanationswiththemostrelevantheadsselectedbySMaTforimageclassification,wecanseethat
mostselectedheadsproduceplausibleexplanations(e.g.,attentionheadsfromthelastlayers).
Originalimage(“television”)
Figure7: Explanationsfromalllayers(rows)andheads(columns)oftheCIFAR-100teachermodel.
17
Originalimage(“butterfly”)
Figure8: Explanationsfromalllayers(rows)andheads(columns)oftheCIFAR-100teachermodel.
18

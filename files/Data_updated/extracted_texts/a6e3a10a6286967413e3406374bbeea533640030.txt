The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link
between Phonemes and Facial Features
LiaoQu1,‚àó,XianweiZou1,‚àó,XiangLi1,‚àó,YandongWen2,RitaSingh1,BhikshaRaj1,3
1CarnegieMellonUniversity
2MaxPlanckInstitute
3MohamedbinZayedUniversityofArtificialIntelligence
{liaoq,xianweiz,xl6,yandongw,rsingh,bhiksha}@andrew.cmu.edu
Abstract byusingadvancedGenerativeAdversarialNetworks[12]. Go-
ing beyond, recent works [13, 14] attempt to recover 3D face
This work unveils the enigmatic link between phonemes
geometrymeshesfromvoicetoavoidtheimpactofinevitable
and facial features. Traditional studies on voice-face correla-
background area modeling in 2D images. However, all these
tions typically involve using a long period of voice input, in-
approaches rely on a long period of voice and potentially ne-
cludinggeneratingfaceimagesfromvoicesandreconstructing
glecttheadvantageofexploringamorefine-grainedvoice-face
3Dfacemeshesfromvoices.However,insituationslikevoice-
correspondence.
based crimes, the available voice evidence may be short and
Rethinking the human voice production mechanism, the
limited. Additionally, from a physiological perspective, each
voice is produced by either the vibration of the vocal cord or
segmentofspeech-phonemecorrespondstodifferenttypesof
theresonanceofthepulmonaryairflow.Forbothofthemecha-
airflow and movements in the face. Therefore, it is advanta-
nisms,thevocaltrackishighlyenrolled.Thevocaltrackcanbe
geoustodiscoverthehiddenlinkbetweenphonemesandface
assumedasafilter,reflectingthecharacteristicsofhumanvoice.
attributes.Inthispaper,weproposeananalysispipelinetohelp
Withthetightbindofmuscleandbone,thevocaltrackisalso
usexplorethevoice-facerelationshipinafine-grainedmanner,
correlated with facial attributes. Specifically, each phoneme
i.e.,phonemesvs. facialanthropometricmeasurements(AM).
correspondstoadifferentvocaltrackstatusandalsoanaccord-
Webuildanestimatorforeachphoneme-AMpairandevaluate
inglyfacialmovement.Toconstructanaccuratevoice-facecor-
thecorrelationthroughhypothesistesting. Ourresultsindicate
relation, we argue that phoneme-level voice-face modeling is
thatAMsaremorepredictablefromvowelscomparedtoconso-
vital.
nants,particularlywithplosives. Additionally,weobservethat
To investigate and understand the voice-face correlation
ifaspecificAMexhibitsmoremovementduringphonemepro-
at a more fine-grained phoneme level, we propose an anal-
nunciation, it is more predictable. Our findings support those
ysis pipeline that leverages a common feature extractor with
inphysiologyregardingcorrelationandlaythegroundworkfor
a regression head to predict human anthropometric measure-
futureresearchonspeech-facemultimodallearning.
ments(AM)fromphoneme.Specifically,Humananthropomet-
IndexTerms:voice-facecorrelation,phoneme
ricmeasurementsareasetoffacialmeasurementssummarized
fromcognitivesciencestudiesthatcaneffectivelyrepresentthe
identityofahuman. Wedecomposetheaudiorecordingsinto
1. Introduction
phonemes and learn to predict AMs from phonemes. In this
way, we can quantitatively analyze the relationship between
The implicit relation between speech and anthropometry fea-
each facial AM and phoneme pairs. In this paper, we aim to
tures has been extensively researched in recent years. Nu-
answercoretwoquestions: 1)whetherthereexistsany‚Äúenig-
merous voice profiling studies [1, 2, 3, 4, 5, 6] have shown
matic‚Äùlinkbetweenphonemeandfacialfeaturesand2)whether
that human voice carries a plethora of information about the
those‚Äúenigmatic‚Äùlinkscanbequantitativelydescribed.
speaker, makingitpossibletodeducebiophysicalcharacteris-
tics of speakers, e.g., gender, age and health conditions, from
theirvoice. However,incriminalprofilingscenarios,thestudy 2. RelatedWorks
of correlations between voice and face becomes essential. In
LearningHumanAttributesfromVoice. Thereisasubstan-
voice-based crimes, such as hoax emergency calls and voice-
tialbodyofresearchoninferringhumanattributesfromaper-
based phishing, the officers seek to depict the facial features
of the criminal merely from short voice evidence. ‚ÄúMayday‚Äù son‚Äôsvoice,includingspeakeridentity[15,16],age[1,3],gen-
der [4], and emotion status [17, 6]. In addition to predicting
can be an example of the audio samples obtained by officers.
attributesdirectlyrelatedtovoice,manystudieshaveexplored
Thismotivatesustoinvestigatethephoneme-levelcorrelation
theimplicitcorrelationbetweenvoiceandfacialfeatures. One
betweenvoiceandface.
popular task is generating 2D face images from voice using
Severalrecentworkshaveattemptedtoinvestigatethecor-
GANs[12],whichhasbeenprogressedinseveralrecentworks
relationbetweenvoiceandface.Cognitivesciencestudies[7,8]
[10,9,11]. Toavoidtheimpactofinevitablebackgroundarea
suggestshumanhasastrongcapabilitytoimaginetheappear-
modeling in 2D images, recent work turns to the 3D domain:
ance of speakers based on their voice. To verify it, face re-
synthesizing3Dmeshesfromvoices[13].
constructionfromvoice,whichaimstorecoveridentity-fidelity
facesfromtheircorrespondingvoicerecordings,isintroduced PhonemePronunciationMechanism. Thehumanvocaltract
by[9]. Afterthat,greatprogresses[10,11]hasbeenachieved can be considered as a series of resonance chambers that can
bedynamicallyconfigured[18]. Whenthevocalcordsvibrate,
‚àóEqualcontribution. theyconverttheairflowfromthelungsintoacousticenergyin
3202
luJ
62
]VC.sc[
1v35931.7032:viXra
Figure1: Illustrationofourframework. Weconvertphonemeclipsintomelspectrogramsanddevelopestimatorsforeachphoneme-
acousticmodel(AM)pair.HypothesistestingisusedtodeterminethepredictabilityofAMsfromphonemes.Greendenotespredictable
AMsandredotherwise,wherethecolorshadeindicatesthedegreeofpredictability.
theformofsoundwaves, producingvoice. Theshapeanddi- 3.3. AMEstimator
mensions of the resonant chambers change as the movements
WeleverageanAMestimatorE topredictthej-thAMfrom
ofthevocaltractmodifytheacousticsignal,resultingindiffer- ij
the i-th phonemem(i) = E (p(j) as anestimator that maps
entpatterns[19,20]. Toproduceaspecificpattern,themouth ij
thej-thphonemetothei-thAM.Tobegin,wetransformeach
andnosemustformacorrespondingshape.Eachpatterncorre-
phonemeintoalogmelspectrum,whichisessentiallyanimage.
spondstoauniquecompositionalunitofspeech,oraphoneme.
Thisisaclassicregressionproblem, andtherefore, weneeda
Whenaspeakerenunciatesdifferentphonemes,thevocaltract,
modelwithstrongfeatureextractioncapabilitiestoextractin-
mouse,nose, andotherrelatedfacialstructuresactinconcert.
formation from the image. We develop a modified version of
Andeachphoneme,therefore,carriessomeinformationabout
theclassicalMNasNetmodeldevelopedbyGoogleAI.Itisde-
alltheserelatedfeatures.
signedtobeefficient,lightweight,andhighlyaccuratefortasks
such as image classification and object detection. [21]. Our
3. Methods modificationretainstheoriginalstructure,butwithafewmodi-
ficationstotheinputandoutputlayers.Specifically,wechange
theinputConv2dmoduletoacceptonly1channel,andtheout-
3.1. Overview
putLinearmoduletoproduceonly1value. Inaddition,since
this part is model-independent, other models such as ResNet
Weaimtoinvestigatethecorrelationsbetweeneachphoneme
[22]arealsocapableofachievingthesamefunction.
and AM pair. As shown in Fig 1, we first transform the seg-
mentedphonemesintologmelspectrumtobettercaptureinfor-
3.4. HypothesisTestingforPhoneme-AMPredictability
mationfromthefrequencydomain.Afterthat,anAMestimator
isemployed topredicteach AMfromphonemes. Finally, we OnceAMsarepredictedfromdifferentphonemes,thenextstep
usehypothesistestingtoanalyzethecorrelationbetweeneach istodeterminewhetheraspecificphonemecanactuallypredict
phoneme-AMpair. an AM. To do this, we use hypothesis testing for each AM-
phoneme pair separately. Firstly, we write the null hypothe-
sisandthealternativehypothesisforthei-thAMandthej-th
3.2. Notations
phonemeas
Our problem involves a set of paired voice recordings of H :AMm(i)isnotpredictablefromphonemep(j)
0
phonemes and AMs, where we aim to predict each AM from
different phonemes. We begin by segmenting the record-
H :AMm(i)ispredictablefromphonemep(j)
ings into phonemes, which can be represented as P = 1
p(1),p(2),...,p(k), where k denotes the total number of dis-
tinct phonemes. Similarly, the AMs can be represented as
TorejectthenullhypothesisH ,weneedtocompareour
AMs = m(1),m(2),...,m(n), wherenrepresentsthenum- 0
estimator E for the AM m(i) when using phoneme p(j) as
berofsummarizedAMs.WerefertotheentiredatasetasD. ij
inputwithachance-levelestimatorC . Iftheperformanceof
ij
Tosimplifythetrainingandevaluationprocess,wedivide E ij is statistically significantly better than C ij, we can reject
D into three subsets. The first subset is the training set D t, H 0 andacceptH 1. Toestimatethechancelevelforphoneme
whichisusedforestimatorlearning. Thesecondsubsetisthe p(j)inourtrainingsetD t,weusethemeanm(i)ofallinstances
validation set D , which is used for estimator selection. Fi- ofthatphonemeintheset.Specifically,wecalculateaconstant
nally, thethirdsv u1 bsetisthevalidationsetD v2, whichisused value C ij as follows: C ij = |D1 t|(cid:80) m(i)‚ààDtm(i). We can
forhypothesistestingandAM-phonemepairselection. expressthehypothesesas:
H :¬µ(Œµ /ŒµC)‚©æ1
0 ij ij
H :¬µ(Œµ /ŒµC)<1
1 ij ij
Here,¬µ(¬∑)representsthemeanfunction,andŒµ andŒµC are
ij ij
themeansquarederrors(MSE)oftheestimatorsE andC
ij ij
onthevalidationsetD ,respectively. Wecancomputethem
v2
asfollows:
Œµ = 1 (cid:88) (mÀÜ(i)‚àím(i))2
ij |D |
v2
m(i)‚ààDv2
ŒµC = 1 (cid:88) (C ‚àím(i))2
ij |D | ij
v2
m(i)‚ààDv2
Figure2: (a)Theselectedlandmarks. (b)Thevisualizationof
Toconductrepeatedexperiments,weneedtotrainthees- the6mostpredictableAMs. Theyarearrangedindescending
timators multiple times. In each iteration, we randomly split orderfromlefttorightandtoptobottom. Numbersintheface
thedatasetintoD ,D ,andD . Wethenusetheone-sided denotetheindexoflandmarks.
t v1 v2
paired-samplet-testtotestthehypothesis. Theconfidencein-
terval(CI)boundsare:
(cid:32) (cid:33) model is trained using a contrastive task on masked latent
CI =¬µ
Œµ
ij ‚àít ¬∑
œÉ(Œµ ‚àöij/ŒµC ij)
speechrepresentationsandcanlearnaquantizationofthelatent
l ŒµC ij 1‚àíŒ±,ŒΩ N sharedacrosslanguages. Itisthenfine-tunedonmulti-lingual
labeledcommonvoicedata. Sinceourdataprimarilycontains
(cid:32) (cid:33)
CI =¬µ
Œµ
ij +t ¬∑
œÉ(Œµ ‚àöij/ŒµC ij) standard English pronunciations, this model can provide rela-
u ŒµC 1‚àíŒ±,ŒΩ N tivelyhighsegmentationaccuracy.
ij We adopt the wav2vec2-xlsr-53-espeak-cv-ft
inhuggingface1inourexperiments.Aftersplitting,wechoose
Here, œÉ(¬∑) represents the standard deviation function, N
themostfrequentlyusedphonemeswhichhavenumberofsam-
representsthenumberofexperiments,Œ±representsthesignifi-
ples‚â•5000.ThedetailedlistisprovidedinthelabelofFig.3.
cancelevel,andŒΩ = N ‚àí1representsthedegreeoffreedom.
Foreachphonemerecording, wefollow[11]andperform64-
Forthisproject,wesetN = 10,andwechooseŒ± = 0.05to
dimensionallogmel-spectrogramsusingananalysiswindowof
obtainstatisticallysignificantresults. Wecanreadthevalueof
25ms, with a hop of 10ms between frames. We perform nor-
t directlyfromthet-distributiontable.Totestthehypoth-
1‚àíŒ±,ŒΩ malizationbymeanandvarianceofeachmel-frequencybin.
esis,iftheCIupperboundCI <1,wecaninferthatwesuc-
u AMsummarization. Wesummarizethemostcommonlyused
cessfullyrejectH andacceptH ,meaningthattheAMm(i)
0 1 AMs[24,25,26,27,28],includingdistances,proportions,and
ispredictablefromphonemep(j).Onthecontrary,ifCI ‚â•1,
u angles in Table 1. The selected landmark is shown in Fig. 2
wecannotrejectH ,indicatingthattheresultisnotstatistically
0 (a).TheseAMsaremorerobustthan3Dcoordinaterepresenta-
significant.
tions.Thisisattributedtothecompleteeliminationofvariations
inducedbyspatialmisalignment,thusrenderingthemmorere-
4. Experiments
liableandresistanttoperturbations. ThegroundtruthAMsare
4.1. Dataset normalizedtohaveameanofzeroandavarianceofone.
Weconductedexperimentsonaprivateaudio-visualdatasetD. Table1:ThesummarizedAMs.
Thedatasetcontains1,026individuals‚Äôpairedvoicerecordings
andscanned3Dfacialshapes. Eachrecordingisarawspeech
distance
speakingoutgeneralphonemesandsentenceswithalengthof
1-2minutes. Eachfacialdataconsistsof67903D-coordinate 31-37 32-36 40-42
pointscollectedfromoneperson. 39-43 33-35 50-53
2-7 30-53 59-53
55-63 54-61
4.2. DataProcessingandTraining
proportion
Phonemesegmentation.ToidentifypredictableAMsandtheir
correspondingphonemes, the firststepistoextractindividual 31-37/27-30 32-36/27-30 31-37/59-53
phonemesfromthedataset. However,duetothelargeamount 32-36/59-53 54-64/31-37 56-62/31-37
ofdataandthecomplexityofdistinguishingphonemeintervals, angle
manuallysegmentingphonemescanbelaborious,difficult,and
31-30-37 31-29-37 29-30-34
imprecise.
To improve the accuracy of phoneme segmentation, we
employ state-of-the-art phoneme segmentation approaches. Training details. For each phoneme-AM pair, we conduct
Specifically,weusetheWav2Vec2-Large-XLSR-53model[23] 10 repeated experiments to ensure statistical significance. In
developedbyFAIR,whichlearnspowerfulspeechrepresenta-
tionsfrommorethan50.000hoursofunlabeledspeech. This 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft
eachexperiment, werandomlysample5000datasamplesand sizethatforaspecificAM,ifitismorefrequentlymovedduring
randomly split them into the D /D /D set in the ratio of thepronunciationofphonemes,theAMisgenerallymorepre-
t v1 v2
70%/10%/20%. WefollowthetypicalsettingsofAdam[29] dictable.Wefurtherverifythishypothesisinthenextsection.
foroptimizationoftheestimator. Thelossfunctionweuseis
the mean squared error loss. The size of the mini-batch and 4.3.3. RelationshipbetweenphonemesandAMs
learningrateissetto128and0.0001,respectively.
Table2:Detailedresultsofphoneme-AMpairs.
4.3. Results
4.3.1. Analysisofphonemes AMs /E/ /D/ /f/ /i:/ /v/ /w/ /√¶/
For each phoneme, we calculate the average 1‚àíCI result
u 39-43 0.10 -0.04 0.08 0.23 0.11 0.18 0.18
witheveryAMs.AscanbeseenfromFig.3,/i:/gotthehigh-
31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09
est avg. 1‚àíCI value 0.199, and /b/ got the lowest value
u 50-53 0.05 0.09 -0.07 0.21 0.06 0.11 0.21
-0.06. When1‚àíCI islowerthan0,AMsareaveragelyun-
u
2-7 0.02 0.08 0.05 0.04 -0.03 0.08 0.09
predictable from the phoneme. The three phonemes with the
lowestandnegativevaluesare/t/,/b/and/d/,whichareall
plosive consonants. During the pronunciation of plosive con- Weinvestigatethedetailedrelationshipbetweenphoneme
sonants, we complete stoppage of airflow followed by a sud- andAMpairstoverifyourhypothesis. AsshowninTable2,
den release of air through trivial mouse open and close, and we list 4 typical AMs paired with 7 phonemes, where 39-43
thereisminimalmovementofthefacialmusclesandstructures. is an oblique distance of the lip, 31-30-37 is an angle in the
Consequently,thepredictionofanyacousticmodelbasedsolely nose, 50-53 is the distance between the lip and jaw, and 2-7
onsuchphonemesischallenging. Onthecontrary,mostvow- is the distance between eyebrow. In the case of AM 2-7, no
els achieve good performance in the test set, and all the top matter pairing with any phoneme, the value is relatively low
6 phonemes belong to vowels with 1‚àíCI u > 0.10. Com- (all1‚àíCI uvaluescloseto0). Duringthepronunciationpro-
paredwithconsonants,thereisnoconstrictionofairflowinthe cessofanyphonemes,themovementofthisparticularregionis
vocaltractwhenpronouncingvowels. Inordertoproducespe- verylimited.Therefore,phonemescanbarelycarryinformation
cific vowels, the facial muscles have relatively greater move- aboutAMsinthispart. However,for39-43,itshowsthat/i:/,
mentduringthepronunciationofthesephonemes,suchasjaw /w/,and/√¶/havethehighestvalue.Whenpronouncingthese
movementduetomouthopeningorlipspreading. Thusvowel threephonemes,themouthusuallygrinsinordertocontrolthe
phonemes may carry more information about facial features. outputairflow. Andthedistancebetweenfaciallandmarks39
This, therefore, canmakethemodelbettercapturethehidden and43couldslightlyinfluencetheairflowfromaphysicalper-
correlationwhenpredictingAMs. spective,thereforethephonemeproducedmayhavesubtledif-
ferences.Incontrast,whenpronouncing/f/and/D/,thisAM
barelymoves. Thus,hardlycanthisAMinfluencestheoutput
airflow. The results verify that it is less predictable for these
phoneme-AMpairs. Similarly,thephenomenonalsooccursin
otherAMslike50-53.Itismorepredictablewhenpairingwith
phonemesthanneedingmouseopening. Alltheseexperiments
verify that for a specific AM, if it is more frequently moved
duringthepronunciationofphonemes,thentheAMwouldbe
morepredictable.
5. Conclusions
In this work, we delve deeply into a fundamental question:
whetherthereexistsany‚Äúenigmatic‚Äùlinkbetweenphonemeand
facial features. If so, whether those ‚Äúenigmatic‚Äù links can be
quantitativelydescribed? Asaforerunnerinthisfield, wede-
signaphoneme-AMsparadigm,whichenablesustoexplorethe
speech-facerelationshipinafine-grainedmanner. Hypothesis
Figure3:Phonemesindescendingorderbyavg.1‚àíCI .
u testingisutilizedtoverifywhetheranAMispredictablefrom
a phoneme. Experiments show that AMs are averagely more
4.3.2. AnalysisofAMs
predictablewithvowelscomparedwithconsonants,especially
Similarly,foreachAM,wealsocalculatedtheaverage1‚àíCI plosives,andareconsistentwiththephysiologicalexplanation.
u
results with all phonemes. To intuitively locate the most pre- On the other hand, most of the predictable AMs are around
dictableAMs(withthehighestavg. 1‚àíCI )onthe3Dface, the nose, mouth, and jaw. Results also verify that for a spe-
u
wevisualizetheminFig. 2(b). MostofthepredictableAMs cificAM,ifitmovesmorefrequentlyduringphonemepronun-
arearoundthenoseandmouth. Onthecontrary,AMsaround ciation, the AM will be more predictable since the phonemes
the eyes are less predictable. This is consistent with the fact mightcarrythishiddeninformationduringpronunciation. We
thatthenoseandmouthshapes(distances,proportions,andan- hope our work lays a foundation for this field. In the future,
gles)affectthepronunciationofphonemes.Otherthanthenose wewouldliketoscaleuptherangeofphonemesandAMsto
and mouth, the jaw is another region frequently occurring in discover more hidden relationships. Moreover, we are inves-
themostpredictableAMs. Sincethejawisanotherregionthat tigatingmodelsthatmakeuseofthefoundhiddencorrelation
exhibitsfrequentmovementduringpronunciation,wehypothe- knowledgeinscenarioslike3Dfacereconstruction.
ùêºùê∂‚àí1 !
6. References
[20] M. T. Ghiselin, P. Ekman, and H. E. Gruber, ‚ÄúDarwin and fa-
cialexpression:Acenturyofresearchinreview.@@@darwinon
[1] M.H.Bahari,M.McLaren,H.V.hamme,andD.A.vanLeeuwen,
man: Apsychologicalstudyofscientificcreativity.‚ÄùSystematic
‚ÄúAgeestimationfromtelephonespeechusingi-vectors,‚ÄùinInter-
Biology,vol.23,p.562,1974.
speech,2012.
[21] M.Tan,B.Chen,R.Pang,V.Vasudevan,andQ.V.Le,‚ÄúMnas-
[2] S. McGilloway, R. Cowie, and E. Douglas-Cowie, ‚ÄúAutomatic
net: Platform-awareneuralarchitecturesearchformobile,‚Äù2019
recognitionofemotionfromvoice:aroughbenchmark,‚Äù2000.
IEEE/CVFConferenceonComputerVisionandPatternRecogni-
[3] P.H.PtacekandE.K.Sander,‚ÄúAgerecognitionfromvoice.‚ÄùJour- tion(CVPR),pp.2815‚Äì2823,2018.
nalofspeechandhearingresearch,vol.92,pp.273‚Äì7,1966.
[22] K.He,X.Zhang,S.Ren,andJ.Sun,‚ÄúDeepresiduallearningfor
[4] S.Li,D.Raj,X.Lu,P.Shen,T.Kawahara,andH.Kawai,‚ÄúIm- imagerecognition,‚Äù2016IEEEConferenceonComputerVision
provingtransformer-basedspeechrecognitionsystemswithcom- andPatternRecognition(CVPR),pp.770‚Äì778,2015.
pressed structure and speech attributes augmentation,‚Äù in Inter-
[23] Q.Xu,A.Baevski,andM.Auli,‚ÄúSimpleandeffectivezero-shot
speech,2019.
cross-lingualphonemerecognition,‚ÄùArXiv,vol.abs/2109.11680,
[5] H. A. Sa¬¥nchez-Hevia, R. Gil-Pita, M. Utrilla-Manso, and 2022.
M.Rosa-Zurera,‚ÄúAgegroupclassificationandgenderrecognition
[24] D.Ghafourzadeh,C.Rahgoshay,S.Fallahdoust,A.Beauchamp,
fromspeechwithtemporalconvolutionalneuralnetworks,‚ÄùMul-
A.Aubame,T.Popa,andE.Paquette,‚ÄúPart-based3dfacemor-
timediaToolsandApplications,vol.81,pp.3535‚Äì3552,2022.
phablemodelwithanthropometriclocalcontrol,‚ÄùinGraphicsIn-
[6] Z. Zhang, B. Wu, and B. Schuller, ‚ÄúAttention-augmented end- terface,2019.
to-endmulti-tasklearningforemotionpredictionfromspeech,‚Äù
[25] Z.Shan,R.T.C.Hsung,C.Zhang,J.Ji,W.S.Choi,W.Wang,
ICASSP2019-2019IEEEInternationalConferenceonAcous-
Y.Yang,M.Gu,andB.S.Khambay,‚ÄúAnthropometricaccuracyof
tics, Speech and Signal Processing (ICASSP), pp. 6705‚Äì6709,
three-dimensionalaveragefacescomparedtoconventionalfacial
2019.
measurements,‚ÄùScientificReports,vol.11,2021.
[7] P.Belin, S.Fecteau, andC.Be¬¥dard, ‚ÄúThinkingthevoice: neu-
[26] Z. Zhuang, D. Landsittel, S. M. Benson, R. J. Roberge, and
ralcorrelatesofvoiceperception,‚ÄùTrendsinCognitiveSciences,
R.Shaffer,‚ÄúFacialanthropometricdifferencesamonggender,eth-
vol.8,pp.129‚Äì135,2004.
nicity,andagegroups.‚ÄùTheAnnalsofoccupationalhygiene,vol.
[8] W. J. Hardcastle and J. Laver, ‚ÄúThe handbook of phonetic sci- 544,pp.391‚Äì402,2010.
ences,‚ÄùLanguage,vol.75,p.152,1999.
[27] Y.Wen,‚ÄúReconstructionofhumanfacesfromvoice,‚ÄùPh.D.dis-
[9] T.-H.Oh,T.Dekel,C.Kim,I.Mosseri,W.T.Freeman,M.Rubin- sertation,CarnegieMellonUniversity,2022.
stein,andW.Matusik,‚ÄúSpeech2face:Learningthefacebehinda
voice,‚Äù2019IEEE/CVFConferenceonComputerVisionandPat- [28] L.G.Farkas,O.G.Eiben,S.T.Sivkov,B.Tompson,M.Katic¬¥,
ternRecognition(CVPR),pp.7531‚Äì7540,2019. and C. R. Forrest, ‚ÄúAnthropometric measurements of the facial
frameworkinadulthood: Age-relatedchangesineightagecate-
[10] H.-S.Choi,C.Park,andK.Lee,‚ÄúFrominferencetogeneration: goriesin600healthywhitenorthamericansofeuropeanances-
End-to-endfullyself-supervisedgenerationofhumanfacefrom tryfrom16to90yearsofage,‚ÄùJournalofCraniofacialSurgery,
speech,‚ÄùArXiv,vol.abs/2004.05830,2020. vol.15,pp.288‚Äì298,2004.
[11] Y.Wen,B.Raj,andR.Singh,‚ÄúFacereconstructionfromvoiceus- [29] D.P.KingmaandJ.Ba,‚ÄúAdam: Amethodforstochasticopti-
inggenerativeadversarialnetworks,‚ÄùinNeuralInformationPro- mization,‚ÄùCoRR,vol.abs/1412.6980,2014.
cessingSystems,2019.
[12] I.J.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-
Farley,S.Ozair,A.C.Courville,andY.Bengio,‚ÄúGenerativead-
versarialnets,‚ÄùinNIPS,2014.
[13] C.-Y.Wu,C.-C.Hsu,andU.Neumann,‚ÄúCross-modalperception-
ist:Canfacegeometrybegleanedfromvoices?‚Äù2022IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),
pp.10442‚Äì10451,2022.
[14] C.-Y. Wu, K. Xu, C.-C. Hsu, and U. Neumann, ‚ÄúVoice2mesh:
Cross-modal3dfacemodelgenerationfromvoices,‚ÄùArXiv,vol.
abs/2104.10299,2021.
[15] R. H. C. Bull, H. Rathborn, and B. R. Clifford, ‚ÄúThe voice-
recognitionaccuracyofblindlisteners,‚ÄùPerception,vol.12,pp.
223‚Äì226,1983.
[16] M.RavanelliandY.Bengio,‚ÄúSpeakerrecognitionfromrawwave-
form with sincnet,‚Äù 2018 IEEE Spoken Language Technology
Workshop(SLT),pp.1021‚Äì1028,2018.
[17] Z.-Q.WangandI.Tashev,‚ÄúLearningutterance-levelrepresenta-
tionsforspeechemotionandage/genderrecognitionusingdeep
neuralnetworks,‚Äù2017IEEEInternationalConferenceonAcous-
tics, Speech and Signal Processing (ICASSP), pp. 5150‚Äì5154,
2017.
[18] T.Riede,E.Bronson,H.Hatzikirou,andK.Zuberbu¬®hler,‚ÄúVocal
productionmechanismsinanon-humanprimate: morphological
dataandamodel,‚ÄùJournalofHumanEvolution,vol.48,no.1,
pp.85‚Äì96,2005.
[19] R.Singh,B.Raj,andD.Genc¬∏aga,‚ÄúForensicanthropometryfrom
voice: An articulatory-phonetic approach,‚Äù 2016 39th Interna-
tionalConventiononInformationandCommunicationTechnol-
ogy,ElectronicsandMicroelectronics(MIPRO),pp.1375‚Äì1380,
2016.

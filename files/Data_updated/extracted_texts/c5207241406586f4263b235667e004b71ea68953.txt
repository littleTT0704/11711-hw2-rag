Syntax and Semantics Meet in the “Middle”:
Probing the Syntax-Semantics Interface of LMs Through Agentivity
LindiaTjuatja,EmmyLiu,LoriLevin,GrahamNeubig
LanguageTechnologiesInstitute
CarnegieMellonUniversity
{ltjuatja, mengyan3, lsl, gneubig}@cs.cmu.edu
Abstract anagentinthewritingevent. Ontheotherhand,
the subject of (1b), this passage, doesn’t do any
Recentadvancesinlargelanguagemodelshave
writing—itiswhatiscreatedintheeventofwrit-
promptedresearcherstoexaminetheirabilities
ing. In contrast to this author, this passage is a
across a variety of linguistic tasks, but little
hasbeendonetoinvestigatehowmodelshan- patient. The agent and patient roles are not dis-
dle the interactions in meaning across words cretecategories,butratherprototypesonopposite
andlargersyntacticforms—i.e. phenomenaat ends of a continuum. These “protoroles” have a
theintersectionofsyntaxandsemantics. We
numberofcontributingpropertiessuchascausing
presentthesemanticnotionofagentivityasa
aneventforagentsandundergoingchangeofstate
casestudyforprobingsuchinteractions. We
forpatients(Dowty,1991).
created a novel evaluation dataset by utilitiz-
The contrast between the minimal pair in (1)
ing the unique linguistic properties of a sub-
setofoptionallytransitiveEnglishverbs. This suggeststhattherearelexicalsemanticproperties
dataset was used to prompt varying sizes of of the subjects that give rise to these two distinct
threemodelclassestoseeiftheyaresensitive readings: onethatdescribeshowthesubjectgen-
to agentivity at the lexical level, and if they erally does an action as in (1a), and another that
canappropriatelyemploytheseword-levelpri-
describeshowaneventgenerallyunfoldswhenthe
ors given a specific syntactic context. Over-
subjectundergoesanactionasin(1b). Intuitively,
all, GPT-3 text-davinci-003 performs
aspeakermayknowfromthemeaningofauthor
extremelywellacrossallexperiments,outper-
thatauthorsareanimate,havesomedegreeofvoli-
formingallothermodelstestedbyfar. Infact,
theresultsareevenbettercorrelatedwithhu- tion,andtypicallywritethings,whereaspassages
manjudgementsthanbothsyntacticandseman- (of text) are inanimate, have no volition, and are
tic corpus statistics. This suggests that LMs typicallywritten. Theknowledgeoftheseaspects
maypotentiallyserveasmoreusefultoolsfor
of meaning must somehow interact with the syn-
linguistic annotation, theory testing, and dis-
tacticformofthesentencesin(1)todisambiguate
coverythanselectcorporaforcertaintasks.1
betweenthetwopossiblereadings,andanagentor
1 Introduction patientroleforthesubjectfollowsfromthemean-
ingofthestatementasawhole.
ConsidertheEnglishsentencesin(1)below:
Nowconsiderthe(somewhatunusual)sentences
(1) a. Thisauthorwriteseasily. in(2)whichusethetransitiveformofwrite:
b. Thispassagewriteseasily. (2) a. Somethingwritesthisauthoreasily.
Thesesentencesdisplayaninterestingpropertyof
b. Thispassagewritessomethingeasily.
certain optionally transitive verbs in English. Al-
Atfirstglance,theabovesentences(withthesame
though they share an identical surface syntactic
sense of write as in 1) are infelicitous unless we
structure—a noun phrase in subject position fol-
imagine some obscure context where this author
lowedbytheintransitiveformoftheverbandan
issomethinglikeacharacterinatextandthispas-
adverbphrasemodifyingtheverb—theyentailvery
sageissomehowanthropomorphizedandcapable
differentthingsabouttherolesoftheirsubjects.
of writing; these contexts go against our natural
The subject of (1a) is someone that does the
intuitions of the semantics of “passage” and “au-
action of writing; in other words, this author is
thor”.2 Unlikethesyntacticformofthesentences
1Code is available at https://github.com/
lindiatjuatja/lm_sem 2Thereisanotherreadingof(2a)thatusesadifferentsense
3202
luJ
01
]LC.sc[
2v58181.5032:viXra
in(1),theexplicitinclusionofbotharguments(sub- testsetofnoun-verb-adverbcombinationsthatdis-
ject and direct object) now forces whatever is in playthealternationshowninexample(1). Wethen
subjectpositiontobetheagentandwhateverisin compare the performance of LMs to both human
objectpositiontobemorelikeapatient,regardless judgementsandcorpusstatistics.
ofthetypicalsemanticpropertiesofthearguments. ProbingforLMsfortheirknowledgeofagentiv-
Taken together, the examples in (1) and (2) ityinsyntacticconstructionsasin(1)and(2)isa
illustrate a compelling interaction at the syntax- particularlyinsightfulcasestudyasitallowsusto
semantics interface. More specifically, we see explorethreeinterconnectedquestionsinahighly
a two-way interaction: first, near-identical sur- controlledsyntacticsetting:
faceformsacquirecompletelydifferententailments
I. Do models display sensitivity to aspects of
abouttheirsubjectssolelydependingonthechoice
word-levelsemanticsindependentofsyntactic
ofsubject,whileconverselycertainsyntacticforms
context, and is such sensitivity aligned with
caninfluencethesemanticroleofanargumentre-
humanjudgements? (§3.1)
gardless of the usual behavior of said argument.
II. Canmodelsemploylexicalsemanticstodeter-
Weaimtoinvestigatethelinguisticcapabilitiesof
minetheappropriatesemanticsofasentence
languagemodelswithregardstothisinteraction.
wherethesyntaxisambiguousbetweenread-
Prior work in studying LMs as psycholinguis-
ings(asin1)? (§3.2)
tic subjects has largely focused on syntax and
grammaticalwell-formedness(Futrelletal.2019; III. Canmodelsdeterminethesemanticsofasen-
LinzenandBaroni2021,interalia). However,asil- tencefromsyntax,disregardinglexicalseman-
lustratedintheaboveexamples,thereareinstances ticswhennecessary(asin2)? (§3.3)
ofnear-identicalsyntacticstructuresthatcangive Additionally,therelativelyinfrequentpairingsof
rise to different meanings depending on the indi- semanticfunctionandsyntacticformofsentences
viduallexicalitemsaswellassurroundingcontext. such as (1b) are also interesting from a learnabil-
ThusevaluatingLMsonsyntax,whileanecessary ity and acquisition perspective for both LMs and
startingpoint,doesnotgiveusasufficientmeasure humans. How both come to process and acquire
of LM linguistic capabilities. While other work exceptionstoageneral“rule”hasbeenatopicof
such as Ettinger (2020), Kim and Linzen (2020), debate since early connectionist models (Rumel-
and Misra et al. (2022) (among others) evaluate hartandMcClelland,1986). Hence,knowledgeof
LMsonavarietyoftestsinvolvingsemanticsand LMcapabilitiesinacquiringandprocessingthese
pragmatics,theydonotinvestigatetheinteraction linguisticanomaliesmayserveasvaluableinsight
between the meanings associated with syntactic to linguists, cognitive scientists, and NLP practi-
formsandthoseofindividuallexicalitems. tionersalike.
Thus, wenot only needtoevaluatesyntaxand
utilizationofsemanticknowledge,butwealsoneed 2 Methodology
tounderstandhowinteractionsofmeaningatdif-
Weconstructedthreeexperiments,eachtargeting
ferentlinguisticlevels—i.e. morphological,lexical,
oneoftheabovequestionsthroughthelensofagen-
phrasal—mayaltermodelbehavior. Exploringphe-
tivity. Wewillfirstgiveabroadoverviewofeach,
nomenawithinthesyntax-semanticsinterfaceisa
andthengointodetailaboutthegeneralapproach.
compelling approach as it gives us access to spe-
Experiment 1 (§3.1) tests whether language
cific aspects of semantics while allowing precise
modelsaresensitivetotheword-levelsemanticsof
controloversyntacticformbetweenlevels.
nounswithregardstoagentivity,suchaswhether
Inthiswork,weprobethesyntax-semanticsin-
nounslikeauthorandpassagearemorelikelyto
terface of several language models, focusing on
beagentsorpatientswithoutanysurroundingcon-
the semantic notion of agentivity. We do this by
text. This is analogous to the idea that speakers
promptingmodelstolabelnounsinisolationorin
have intuition for how entities prototypically act
contextaseitheragentsorpatientsfromacurated
inevents,e.g. thatauthorswriteandpassagesare
ofwrite,wherethisauthorisarecipient(Somethingwrites written,andthatthisextendstohowwecategorize
(to)thisauthoreasily).Regardless,giventhattheagentand
theirrolesinevents(RissmanandMajid,2019).
patientrolesasdefinedbyDowty(1991)areprototypeson
ascale, thisauthorintherecipientreadingisclosertothe Experiment 2 (§3.2) tests whether language
patientrole. models can disambiguate between the possible
Figure1: Promptsetupforeachexperiment. NotethattheexamplesgivenforExp1arenotmeanttobehard
labels,rathertheyare“tendencies”forthesenouns. InExp2,thenounitselfdetermineswhetherthesentenceis
consideredintr-agentorintr-patient;inExp3,weforcethenountotaketheagentorpatientrolebyplacingitin
subject(trans-agent)orobject(trans-patient)position.
readings of sentences of the form in (1)—i.e. if 2.1 Generalapproachanddatacuration
they can identify whether the syntactic subject is
Inalloftheseexperiments,werelyontheprompt-
anagentorapatientwhentheverbcanallowfor
ingparadigmtoelicitLMprobabilitiesofan“agent”
either. Sentenceswiththeintransitiveformofthe
or“patient”labelforagivennouninisolationor
verbthatdescribehowthesubject(anagent)does
within a sentence. Our prompting method con-
an action demonstrate object drop (as the direct
sists of four examples with gold labels, followed
objectofthenormallytransitiveverbis“dropped”),
bytheunlabeledtestexampleinthesameformat,
whilesentencesthatdescribehowaneventunfolds
as shown in Figure 1. As this task has not been
when the subject (a patient) undergoes an action
exploredinpriorliterature,wehadtoconstructour
arecalledmiddles,shortforthelinguistictermdis-
ownexamplestoteston.
positionalmiddle(vanOosten1977;Jaeggli1986;
The highly controlled syntactic setting that al-
Condoravdi1989;Fagan1992,interalia).3 Inour
lowsustoexplorethealternationinagentivityas
experimentalsetup,wewillrefertotheseasintr-
displayedin(1)and(2)isadouble-edgedsword—
agent and intr-patient, respectively. If a model
whilethissettingprovidesuswithaminimalpair,it
candothistasksuccessfullybyemployingseman-
alsorestrictsthetypesofverbsthatworkinthisex-
tic information about the noun, we would expect
perimentalsetup. Thesecond(intr-agentvs. intr-
not only to see that nouns in subject position are
patient)andthird(trans-agentandtrans-patient)
classifiedcorrectlyasagentsorpatients, butalso
experimentsrequireverbsthatareoptionallytransi-
thatthesepredictionsforthemostpartcorrelateto
tiveandhavenopreferenceforwhetheranagentor
thepredictionsinthefirstexperiment.
apatientisthesubjectoftheintransitiveform,asin
Finally,Experiment3(§3.3)testswhetherlan-
(1). Theserequirementstogetherhighlyconstrain
guage models can disregard word-specific priors
the class of verbs that work in this experimental
to identify whether the noun of interest in a sen-
setup,andasfarwecantellthereexistsnodefini-
tence with a transitive verb (such as those in 2)
tivelistinthelinguisticsliteratureofEnglishverbs
is an agent or patient. Since the semantic role of
thatdisplaybothproperties.
thenounmapsdirectlytoitssyntacticpositionin
As a starting point to curate a list of verbs, we
thesesentences,allsubjectsshouldbeagentsand
consulted literature on verbs that display object
allobjectsshouldbepatients. Forourtestset,we
drop(Gillon2012;Fillmore1986,aswellasLevin
createsentenceswherethepositionofthenounis
1993foranoverviewofEnglishverbclasses). We
thesubject(trans-agent)andsentenceswhereitis
compiled a list of 23 verbs (see Appendix A),
theobject(trans-patient)foreverynoun.
though this list is certainly non-exhaustive. For
3NotethatinEnglish,dispositionalmiddlesalsoallowfor eachverb,welistnounsandadverbsthatcanwork
whatareconsiderednon-patientpromotedobjects(suchas in combination with each other in all of the tem-
paths, e.g. The desert crosses easily) (Tenny 1994, 1992),
plates in Table 1. Criteria for adding nouns and
butforconveniencewewilltreatthemasbeinginthesame
categoryaspatients. adverbsarelistedintheAppendixB.
Intotal,wehave233uniquenounsandatotalof may occur in (e.g. for “model”, we give human
820noun-verb-adverbcombinations. Outofthese annotators “model (person)”).4 We then average
combinations,343formintr-agentsentencesand theratingsacrossallannotatorsandnormalizeso
477formintr-patientsentences. Sincewecanput thatthevaluesfallbetween0and1. Tocalculate
anynounintosyntacticsubjectorobjectposition inter-annotatoragreement,werandomlydividethe
forthetransitivesentences,wehave820sentences annotatorsintotwogroups(of9and10),average
eachfortrans-agentandtrans-patient. their ratings for each noun, and calculate the cor-
relation between the two; doing this seven times
Sentence Template yieldsanaverageinter-groupcorrelationof0.968.
This<noun><verb><adverb>. Thesecondmethodusesstatisticsfromlinguisti-
intr-agent Thisauthorwriteseasily. callyannotatedcorporaasaproxyforthe“typical”
intr-patient Thispaperwriteseasily.
agentivity of a noun. We do this by calculating
This<noun><verb>something<adv>. the frequency of “agenthood” for a noun (agent
Thisauthorwritessomethingeasily.
trans-agent ratio),i.e. dividingthenumberoftimesthenoun
Thispaperwritessomethingeasily.
appears as an agent by the number of times it is
Something<verb>this<noun><adv>.
eitheranagentorpatient. Theidealannotatedcor-
Somethingwritesthisauthoreasily.
trans-patient
Somethingwritesthispapereasily. pusforthiswouldbeonewithsemanticrolelabels
suchasPropbank(KingsburyandPalmer,2002),
Table1: Templatesforexperiments2and3. Sentences
wherethe“ARG0”labelcorrespondstoagentand
highlightedinpinkcontaina<noun>withan“agent”
“ARG1”topatient. However,manyofthenounsin
label,whilethoseinbluewith“patient”.
ourdataappearedonlyafewtimesinPropbankor
notatall—outofall233nouns,only166ofthem
2.2 Approximating“groundtruth”agentivity occurredwithinanARG0orARG1span.5
labelsfornounsoutofcontext Thus, we also tried utilizing syntax as a proxy
usingGoogleSyntacticNgramsbiarcs(Goldberg
Gettingagold“agent”or“patient”labelisstraight-
andOrwant,2013),asitissignificantlylarger. The
forwardintheexperimentswithnounsincontext:
biarcs portion of the corpus covers dependency
forsentenceswiththeintransitivethiswasdonead
relationsbetweenthreeconnectedcontentwords,
hocduringdatacuration,andforsentenceswiththe
whichincludestransitivepredicates. Tocalculatea
transitive this is a one-to-one mapping to syntax.
similarratio,wedividethenumberoftimesanoun
However,usingahardlabelfornounsinisolationis
occursasasubjectbythetotalnumberofsubject
problematicasasemanticrolelabelismeaningless
anddirectobjectoccurrences(wecallthisthesub-
withoutcontextoftheevent;inprinciple,givenan
ject ratio). A value closer to 1 should correlate
appropriatecontext,anythingcanactuponsome-
withatendencytooccurmoreoftenasanagent,as
thingelseorhavesomethingdonetoit(literallyor
agentsaregenerallycodedassubjectsofEnglish
figuratively).
transitiveverbsandpatientsasdirectobjects. All
To get around this, we have two methods for
butoneofournounscontainedatleastoneinstance
findinganapproximatelabelforthe“typical”agen-
ofoccurringwitha“nsubj”or“dobj”label.
tivity of a noun. The first was to collect human
judgements. 19annotators(native/fluentbilingual
3 ExperimentalResults
Englishproficiency)weregivennounswithoutany
contextandweretaskedtojudgehowlikelyeach We evaluate BLOOM (Scao et al., 2022), GPT-2
nounistobeanagentinanyarbitraryeventwhere (Radford et al., 2019), and GPT-3 (Brown et al.,
bothanagentandpatientareinvolved. Theirjudge- 2020)modelsofvaryingsizesforallexperiments.
mentswerecollectedviaratingsonascalefrom1 Since previous work has shown that models are
(very unlikely to be an agent) to 5 (very likely to highly sensitive to the ordering of examples (Lu
be an agent). For nouns that have multiple com- etal.,2021),weruneachexperimenttwice: once
monwordsenses(e.g. “model”canrefertobotha with the order shown in Figure 1 where an agent
fashionmodelormachinelearningmodel,among
4Additional details on collecting human ratings can be
otherthings)weincludeadisambiguatingdescrip- foundinAppendixC.
tion. Thisdescriptiondoesnotcontainanyverbsor 5We used Propbank annotations for BOLT, EWT,
and Ontonotes 5.0 from https://github.com/
otherexplicitindicationsofwhateventsthenoun
propbank/propbank-release.
Figure 2: Correlation between subject ratio (from Figure3: Correlationbetweenδ-LLinExperiment1
GoogleSyntacticNgrams)andhumanratingsforeach forGPT-3davinci-003andthenormalizedhuman
noun(r = 0.762). Thesemanticrolelabelistherole rating in the APAP experiment. Note that a negative
the noun takes as the subject of the intransitive verb δ-LLmeansthe“patient”labelismorelikely.
withinourtestset.
with human ratings, with the exception of
is first (APAP ordering) and again with the first GPT-3 text-davinci-003 (henceforth
example moved to the bottom (PAPA ordering). davinci-003), shown in Figure 3. We also
Wecomparemodelsbasedontheiraverageperfor- see that davinci-003 is not only both better
manceacrossbothorderings. Note,however,that correlated with human judgements than with
somemodelsaremoresensitivetoorderingsthan corpus statistics, but surprisingly there is also a
others;somemodels(liketext-davinci-003) strongercorrelationbetweenitsδ-LLandhuman
are largely invariant to example ordering. In Ap- ratings than between these proxies (syntactic
pendixD,wereportresultsfrombothexperiments. and semantic) and human ratings. In fact,
davinci-003isextremelyclosetotheaverage
3.1 Exp1: Agentivityatthelexicallevel
inter-annotatorgroupcorrelation,andfurthermore
Inordertoseeifmodelsaresensitivetothenotion thiscorrelationislargelyinvarianttotheordering
ofhow“typically”agentiveanounis,wecompare ofprompts.
thedifferenceinlog-likelihoodbetweenpredicting The observation that davinci-003 is better
“agent”or“patient”forthatnoun(δ-LL)withthe correlated with human judgement than both syn-
normalizedhumanratingsaswellascorpusstatis- tactic (Ngrams) and semantic (Propbank) corpus
ticsfromGoogleSyntacticNgramsandPropbank. statisticsisintriguingasbothtypesofcorporahave
Before we compare models with Ngrams and beenusedinmodelingpredictionofthematicfit,or
Propbank, we first ask how well-correlated both howwellanounfulfillsacertainthematicrolewith
are with human ratings. We find that the subject a verb (Sayeed et al., 2016). Thus, we may natu-
ratiocalculatedfromoccurrencecountsinGoogle rally expect this to also work well with “general
SyntacticNgramsispositivelycorrelatedwiththe tendencies”ortypicalityjudgementsfornounsby
average human rating with Pearson’s r of 0.762, themselves. However, it seems that such corpora
thoughthehumanratinghasastrongerdividebe- maybetoosmallorgenre-biasedtofullycapture
tween agents and patients. This can be seen in thenuancesofhumanjudgements,andsuchjudge-
Figure 2. When comparing with humans, using ments may be better captured by LMs that have
SyntacticNgramsforthistaskactuallyturnsoutto seenvastquantitiesofdataacrossawidevarietyof
bebetterthanusingPropbank: forthe166nouns domains,evenwithoutexplicithumanannotation.
thatoccurwithARG0/1labels,thereisacorrela-
tionof0.555withhumanratings(seeAppendixE 3.2 Exp2: Disambiguatingagentivitywiththe
fordetails). intransitive
Overall, as seen in Table 2, we find Inthisexperiment,weevaluatemodelsalongtwo
that most models have a weak correlation metrics: howaccuratethemodelisinpredictingthe
Model Human Ngrams PB text-babbage-001. This is also the case in
BLOOM560m 0.549 0.519 0.377 Experiment1.
BLOOM1b1 0.374 0.358 0.291 Wealsoevaluatehowstronglycorrelatedtheδ-
BLOOM1b7 0.340 0.288 0.278
LLbetweenpredicting“agent”or“patient”forthe
BLOOM3b 0.305 0.348 0.231
BLOOM7b1 0.016 -0.129 0.011 nouninsubjectpositionoftheintransitiveiswith
theδ-LLofthenouninisolation. Sincetheroleof
GPT-2small 0.650 0.569 0.463
GPT-2medium 0.394 0.451 0.333 thenounintheintransitiveisheavilydependenton
GPT-2large 0.499 0.544 0.412
themeaningofthenounitself,ifamodelisusing
GPT-2xl 0.358 0.349 0.227
thisinformationtodisambiguatewewouldexpect
GPT-3ada-001 0.594 0.575 0.490
thattheδ-LLinthisexperimentiscorrelatedwith
GPT-3babbage-001 0.311 0.337 0.158
GPT-3curie-001 0.107 0.181 0.128 δ-LLfromExperiment1. Furthermore,wewould
GPT-3davinci-001 0.467 0.461 0.330 alsowantittobestronglycorrelatedwithourap-
GPT-3davinci-003 0.939 0.730 0.574
proximate“groundtruth”measuresforagentivity,
Inter-annotator 0.968 – – especiallyhumanratings.
GoogleSyntacticNgrams 0.762 – –
ThesecorrelationsareshowninTable3. Asex-
Propbank 0.555 – –
pected,davinci-003displaysastrongrelation-
Table 2: Correlation between the difference in log- shipbetweentheδ-LLfromintransitivesentences
likelihoodofpredicting“agent”or“patient”withhuman withtheδ-LLfromExperiment1,andfurthermore
ratings,subjectratiocalculatedfromGoogleSyntactic also has a strong correlation with human ratings.
Ngrams (232/233 nouns), and agent ratio calculated
Like in Experiment 1, davinci-003’s perfor-
fromPropbank(166/233nouns),averagedacrossAPAP
manceisinvarianttochangesinexampleorders.
andPAPAexperiments.
Model Nounδ-LL Human Ngrams PB
correctlabelincontextandhowstronglycorrelated BLOOM560m 0.605 0.217 0.147 0.100
BLOOM1b1 0.702 -0.0344 0.0200 0.0511
theδ-LLinthisexperimentiswiththeδ-LLfrom BLOOM1b7 0.540 0.706 0.562 0.441
BLOOM3b 0.258 0.280 0.190 0.0871
Experiment1.
BLOOM7b1 0.385 0.161 0.124 0.0689
GPT-2small 0.655 0.424 0.309 0.290
GPT-2medium 0.611 0.523 0.516 0.505
GPT-2large 0.551 0.609 0.489 0.447
GPT-2xl 0.548 0.507 0.445 0.363
GPT-3ada-001 0.541 0.496 0.358 0.307
GPT-3babbage-001 0.127 -0.176 -0.170 -0.125
GPT-3curie-001 0.130 0.156 0.189 0.0953
GPT-3davinci-001 0.487 0.647 0.515 0.376
GPT-3davinci-003 0.914 0.919 0.715 0.567
Table 3: Correlation between the δ-LL from intr-
agent/intr-patient sentences with the δ-LL from the
nouninisolation,humanratings,subject(GoogleSyn-
tacticNgrams),andagentratios(Propbank).
3.3 Exp3: Agentivitywiththetransitive
Figure4: Averageaccuracyforpredictingthelabelof
nounsinintr-agent/intr-patientsentences. Theblack Aspreviouslydiscussed,thesyntacticpositionof
line indicates majority class performance; blue bars thenouninthetransitivesentences(subjectorob-
indicateabovemajorityclassperformance.
ject) directly map to their semantic roles (agent
andpatient,respectively). Figure5showsaccuracy
Figure 4 shows the accuracy of each model
splitbytrans-agentandtrans-patient.
in predicting (giving a higher probability to) the
As in the previous experiments, GPT-3
correct semantic label. Over half of the models
davinci-003 outperforms all other models
do not achieve chance performance (predicting
(0.994 for trans-agent and 0.991 for trans-
the majority class ≈ 0.582). Interestingly, we
patient—it is actually the only model which per-
find that there is no monotonic increase in per-
formssignificantlyabovechanceforbothExperi-
formance for this task with respect to model size
ments 2 and 3, and is also consistent across both
(Kaplanetal.,2020)—forexample,performance
exampleorderings.
drops drastically between text-ada-001 and
1976; Comrie 1989, inter alia). However, across
both orderings, the only noun that was misclassi-
fiedwas“model”inthesentenceThismodelpho-
tographsbeautifully/nicely. Nevertheless,itcould
bearguedthatanagentinterpretationinthiscontext
isplausible.
It appears that there are two interactions that
are occurring in the above example. First, we
must consider the selectional restrictions and of
theverb,i.e. whatargumentsareallowableinthe
eventdescribedbytheverb(Chomsky1965;Katz
andFodor1963). Whileselectionalrestrictionsare
traditionally viewed as binary features, a weaker,
gradientversionofthisisselectionalpreferences,
Figure5: Averageaccuracyacrosstrans-agent,trans-
orthedegreetowhichanargumentfulfillsthere-
patient, andalltransitivesentences. Thedashedline
strictions of the event (Resnik, 1996). A closely
indicateschanceperformance.
relatednotiontothisisthematicfit,whichishow
muchawordfulfillsthesepreferences.
4 ACloserLookatdavinci-003 Secondly,theAnimacyHierarchy—ofwhichhu-
mansareatthetop—playsaroleinsuchselectional
Given that GPT-3 davinci-003 does ex-
restrictionsandpreferences,andthusinthematic
tremely well, a natural question to ask is
fit (Trueswell et al., 1994). Since photograph re-
whether davinci-003 “fails” in similar ways
quiresahuman-likeentityasanagent,itcouldbe
tohumans—i.e. wecanseewhetherthenounsthat
arguedthattheinterpretationof“model”beingan
aremisclassifiedintheintransitivesentencesetting
agentinthissentenceisnotinvalid(thoughlikely
(§3.2)aremoreambiguoustohumansaswell.
a less salient interpretation by English speakers),
InbothAPAPandPAPAorderings,allornearly
asnothinginthe“photographing”eventrulesouta
allofwhatdavinci-003getsincorrectarepa-
subtypeofahuman“model”beingtheagent. This
tientsubjects;all78incorrectlyclassifiedsubjects
contrastswiththeexamplewith“animal”inourtest
ofsentencesintheAPAPorderingarepatients,and
set (This animal photographs beautifully/nicely),
69ofthe70incorrectsubjectsinthePAPAorder-
whichwouldbefarlessacceptablewithananimal
ingarepatients. Fromthis,onewaytoanswerthe
agentinterpretation,andfallsbelow“humanmodel”
abovequestionistocomparethissubsetofnouns
intheAnimacyHierarchy.
withthesubsetofnounswitha“patient”label(in
theintransitiveconstruction)thathumanstendto
4.2 Verbswithvehicleobjects
rateasmoreagentive.
TheotherclassofnounspresentinTable4,which
4.1 Animacyandthematicfit also happen to be the remaining nouns, are ve-
hicles. With regards to the relationship between
Table 4 lists the latter subset of nouns, i.e. the
animacy and agentivity, prior work such as Zae-
most“agent-like”nounswitha“patient”labelin
nen et al. (2004) has noted that “intelligent ma-
the intransitive construction. Recall that human
chinery” (such as computers and robots) and ve-
annotators were asked to rate each noun in isola-
hicles also often act as animates (below humans
tion from a scale from 1 (very unlikely to be an
and above inanimates). Interestingly, nearly half
agent) to 5 (very likely to be an agent) which is
of the examples that davinci-003 gets wrong
then normalized to a scale from 0 to 1, whereas
aresentencescontainingverbswithvehicleobjects
thegoldlabelsfornounsaredeterminedbyroleit
(This car/vehicle/SUV/tractor/etc. drives nicely,
takesintheconstructed(inthiscase,intransitive)
Thisjet/plane/aircraft/etc. fliessmoothly). Infact,
sentences.
theexamplesthatdavinci-003getsthe“most
Animatenouns,suchas“model(person)”,“an-
wrong” (higher LL −LL ) are sen-
imal”, and “fish” are unsurprisingly in this list, incorrect correct
tenceswiththeseverb-nouncombinations.
as many linguists have noted that the notion of
Liketheaboveexampleswith“model”,someof
agentivityiscloselyrelatedtoanimacy(Silverstein
Noun Human Ngrams Nounδ-LL this setup is similar to our dataset, they focus on
theexplicitrelationshipbetweentheeventandthe
model(person) 0.806 0.523 8.06
animal 0.722 0.699 2.97 noun, while our data is meant to focus on the re-
jet 0.583 0.562 7.27
lationshipbetweentheprototypicalroleofanoun
aircraft 0.583 0.551 3.92
fish 0.569 0.467 -4.08 (outofcontext)anditsroleinacontrolledsyntactic
vehicle 0.542 0.468 4.66 environment. Furthermore, as we would like the
bus 0.542 0.394 0.537
agent/patientdistinctiontobeaminimalpairresult-
tank 0.542 0.564 -0.639
plane 0.528 0.565 11.1 ingchangingthenouninanidenticalsurfaceform,
car 0.528 0.565 3.83 thesetsofnounsandverbsbetweentheirstudies
motorcycle 0.514 0.184 5.11
andoursonlypartiallyoverlap.
truck 0.514 0.437 13.6
SUV 0.480 0.500 -2.27 Thisstudyalsofollowsawell-establishedlineof
tractor 0.401 0.500 11.2 workonLMsaspsycholinguisticsubjects(Futrell
et al. 2019; Ettinger 2020; Linzen and Baroni
Table4: Nounsinintr-patientsentenceswithnormal-
2021, inter alia). A large portion of this work fo-
izedhumanratings≥0.5,alongwiththeirsubjectratio
cuses on probing LMs for sensitivity to the well-
fromGoogleSyntacticNgramsandtheaverageδ-LL
fromnounsinisolation(3.1). Theaverageδ-LLfor“pa- formedness of sentences containing various syn-
tient”nounsrangesfrom-15.7to13.6. Notethatmodel tactic structures such as subject-verb agreement
waspresentedtoannotatorswithadisambiguatingword (Linzen et al., 2016), relative clauses (Gulordava
sense(person). etal.2018;Ravfogeletal.2021),andfiller-gapde-
pendencies(Wilcoxetal.,2018),amongothers. A
closely-relatedworkbyPapadimitriouetal.(2022)
thesesentenceshaveapossiblealternativereading
investigateshowBERTclassifiesgrammaticalrole
and are more ambiguous compared to sentences
ofentitiesinnon-prototypicalsyntacticpositions,
with verbs like sell (as in, This car sells well.).
similartooursetupinExperiment3.
More specifically, they have a possible (though
There have also been works on evaluating and
alsolesssalient)unergativereading: e.g. inThisjet
probing LMs for semantic/pragmatic knowledge.
flies smoothly, it could be a statement about how
Ettinger (2020) created a suite of tests drawn
the jet flies on its own as opposed to about how
fromhumanlanguageexperimentstoevaluatecom-
the jet flies when someone flies it. Out of all the
monsensereasoning,eventknowledge,andnega-
sentences in the test set, these are the only ones
tion. The COGS challenge (Kim and Linzen,
(alongwithsomesentenceswith“turn”)wherethe
2020), which contains related tests to ours with
intr-agenthasthispossibleunergativereading.
regardstoargumentalternation,testsforwhether
5 RelatedWorks LMs can learn to generalize about passivization
andunnacusative-transitivealternationsinEnglish.
There has been extensive work in the psycholin-
Misraetal.(2022)testLMsfortheirabilitytoat-
guisticsliteratureinvestigatinghowhumansmake
tributepropertiestoconceptsandfurthertestprop-
use of the relationship between events described
ertyinheritance. Withregardstolexicalsemantics,
by verbs and nouns that may participate in these
Vulic´ etal.(2020)investigatehowtype-levellexi-
events,whichisespeciallyrelevanttotheanalysis
calinformationfromwordsincontextisstoredin
describedin§4.1. WorkssuchasTanenhausetal.
modelsacrosssixtypologicallydiverselanguages.
(1989)andTrueswelletal.(1994)haveshownthat
However, our work is distinct from both pre-
humans utilize information about thematic fit to
vioussyntax-andsemantics-focusedprobingand
resolveambiguityinsentenceprocessing,mainly
evaluationinitsfocusontheinteractionsbetween
focusingongarden-pathsentences.
theaspectsofmeaninginindividuallexicalitems
Alongthislineofwork,McRaeetal.(1998)and
with larger syntactic structures or constructions.
Padó(2007)createdhumanjudgementdatasetsfor
Nevertheless,methodologiesfromtheseresearch
thematicfitbyaskinghumanstoratenounsasso-
areashaveinformedtheconstructionofourexperi-
ciatedwithevents(e.g. acrookarresting/beingar-
ments. Ouruseofminimalpairstoformsentences
restedbysomeone)onascalefrom1(veryuncom-
with contrasting semantic roles is similar to the
mon/implausible) to 7 (very common/plausible).
constructionoftheBLiMPdataset(Warstadtetal.,
Asstimuli,humansaregiventhenoun,theverbde-
2020)andothertestsuites. Furthermore,wetreat
scribingtheevent,andtheroleofthenoun. While
the “agent”/“patient” labelling task as classifica- than some proxy corpus statistics. We find it sur-
tion based on the generation probabilities of the prisingthatdavinci-003isabletocapturean
labels,followingLinzenetal.(2016)’smethodof abstract notion of agentivity extremely well, but
usinggenerationprobabilitiesforgrammaticality thisabilitydoesnotappeartocomefromthesizeof
judgements. themodelaloneasperformancedoesnotincrease
Another relevant recent line of work within monotonically across any of the model families
NLPisinspiredbyConstructionGrammar(CxG), tested. What aspects of model training/data con-
a branch of theories within cognitive linguistics tributetodavinci-003’s(orothermodels’)per-
that posits that constructions—defined as form- formanceonlinguistictasksmaybeaninteresting
meaningpairings—arethebasicbuildingblocksof areaforfuturework.
language(Goldberg1995;Croft2001,interalia). Furthermore, a qualitative analysis of what
Mahowald (2023) conducted a similar prompt- davinci-003 gets incorrect reveals examples
ing experiment on the English Article-Adjective- involvinganumberoflinguisticconfoundersthat
Numeral-Noun construction, though this was fo- make them more ambiguous to humans as well.
cused on grammaticality judgements as opposed The model’s ability to “pick out” these linguisti-
toaspectsofsemantics. Weissweileretal.(2022) callyinterestingexamples,combinedwiththehigh
probeforbothsyntacticandsemanticunderstand- correlation with human ratings in Experiment 1,
ing of the English comparative correlative. Our showcasesthepotentialofLMsastoolsforlinguis-
studydiffersinthatweanalyzetheimpactofindi- ticdiscoveryfornewphenomena,suchasfinding
viduallexicalitemsinwhatotherwiseappearsto newclassesofwordsorsyntacticconstructionsthat
beanidenticalsyntacticconstruction,asopposed behaveinunexpectedways. Wehopetheseresults
to analyzing competence of the construction as a encourageamorelivelydiscussionbetweenNLP
whole. Finally,Lietal.(2022)findthatsentences researchersandlinguiststounlockthepotentialof
sharingthesameargumentstructureconstructions LMsastoolsfortheoreticallinguisticsresearch.
(ASCs) are closer in the embedding space than
7 Limitations
thosesharingthemainverb;inlightofourresults,
aninterestingdirectionwouldbetoseeifsentences
WhiletheuseofaparticularsubsetofEnglishtran-
ofthesamesurfaceconstructionmayclusterbased
sitiveverbsallowsustohaveprecisecontrolover
onfiner-grainedsemanticdistinctions.
thesurfaceformsweareevaluatingLMson,this
Oneconsequenceofourwork—specificallywith
restrictsourscopetoaspecificalternationinone
regards to davinci-003’s extremely high cor-
languageaswellasarelativelysmallevaluationset.
relationwithhumanjudgements—isthepotential
Nevertheless,wehopethemethodologypresented
for LMs as a tool for discovery in theoretical lin-
inthisworkcanbeextendedtootherphenomena
guistics. This also has been argued recently by
acrosslanguages.
PetersenandPotts(2022),whodemonstratethisin
Additionally, while we explored a variety of
therealmoflexicalsemanticsthroughacasestudy
ways to prompt these models, it may be the case
oftheEnglishverbbreak.
thatthepromptisnon-optimalandthereforedoes
notelicitthebestpossibleoutputwithrespecttothe
6 Conclusion
task. Furthermore, the “prompt” to elicit human
In order to gain insight into the behavior of LMs judgements is not the same as the prompt given
withrespecttothesyntax-semanticsinterface,we tomodels,noraretheoutputformats(humansare
created a suite of prompting experiments focus- askedtorespondonadiscretescalefrom1-5,while
ing on agentivity. We prompt varying sizes of modelsareevaluatedbytheirlabelloglikelihoods).
BLOOM,GPT-2,andGPT-3toseeiftheyaresen- Evaluating whether the methodology in this line
sitive to aspects of agentivity at the lexical level, ofworkisafaircomparisonbetweenmodelsand
andthentoseeiftheycaneitherutilizeordiscard humansmaybeaninterestingdirectionforfuture
theseword-levelpriorsgiventheappropriatesyn- work.
tactic context. GPT-3 davinci-003 performs
8 Acknowledgements
exceptionallywellinallthreeofourexperiments—
outperformingallothermodelstestedbyfar—and
This work was supported by NSF CISE RI grant
is even better correlated with human judgements
number 2211951. For helpful comments and
suggestions, we thank Kyle Mahowald and John AdeleE..Goldberg.1995. Constructions: Aconstruc-
Beavers, as well as the three anonymous review- tiongrammarapproachtoargumentstructure. Uni-
versityofChicagoPress.
ers and meta-reviewer. Finally, we would like
Yoav Goldberg and Jon Orwant. 2013. A dataset of
to thank Thomas Lu, Saujas Vaduguru, Leilani
syntactic-ngrams over time from a very large cor-
Zhang, Russell Emerine, Vijay Viswanathan,
pusofEnglishbooks. InSecondJointConference
JeremiahMilbauer,QianliMa,AlexWilf,Amanda on Lexical and Computational Semantics (*SEM),
Bertsch,SireeshGururaja,YingshanChang,Clara Volume1: ProceedingsoftheMainConferenceand
Na, Zhiruo Wang, Cathy Jiao, Simran Khanuja, theSharedTask: SemanticTextualSimilarity,pages
241–247, Atlanta, Georgia, USA. Association for
AtharvaKulkarni,AnubhaKabra,LeenaMathur,
ComputationalLinguistics.
andShuyanZhoufortheirhelpinannotatingnouns
KristinaGulordava,PiotrBojanowski,EdouardGrave,
inourdataset.
TalLinzen,andMarcoBaroni.2018. Colorlessgreen
recurrentnetworksdreamhierarchically. InProceed-
ingsofthe2018ConferenceoftheNorthAmerican
References Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
1 (Long Papers), pages 1195–1205, New Orleans,
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Louisiana.AssociationforComputationalLinguis-
Neelakantan,PranavShyam,GirishSastry,Amanda
tics.
Askell,etal.2020. Languagemodelsarefew-shot
learners. Advancesinneuralinformationprocessing Ray S Jackendoff. 1972. Semantic interpretation in
systems,33:1877–1901. generativegrammar.
NoamChomsky.1965. AspectsoftheTheoryofSyntax. OsvaldoA.Jaeggli.1986. Passive. LinguisticInquiry,
MITpress. 17:587–622.
BernardComrie.1989. Languageuniversalsandlin- JaredKaplan,SamMcCandlish,TomHenighan,TomB
guistictypology: Syntaxandmorphology. University Brown,BenjaminChess,RewonChild,ScottGray,
ofChicagopress. AlecRadford,JeffreyWu,andDarioAmodei.2020.
Scaling laws for neural language models. arXiv
CleoCondoravdi.1989. Themiddle: Wheresemantics
preprintarXiv:2001.08361.
and morphology meet. In MIT Working Papers in
Linguistics11,pages16–31.MITPress. JerroldJKatzandJerryAFodor.1963. Thestructure
ofasemantictheory. Language,39(2):170–210.
WilliamCroft.2001. Radicalconstructiongrammar:
Syntactictheoryintypologicalperspective. Oxford NajoungKimandTalLinzen.2020. COGS:Acompo-
UniversityPressonDemand. sitionalgeneralizationchallengebasedonsemantic
interpretation. In Proceedings of the 2020 Confer-
DavidDowty.1991. Thematicproto-rolesandargument
ence on Empirical Methods in Natural Language
selection. Language,67(3):547–619.
Processing(EMNLP),pages9087–9105,Online.As-
ThomasErnst.2001. Thesyntaxofadjuncts,volume96.
sociationforComputationalLinguistics.
CambridgeUniversityPress.
Paul R Kingsbury and Martha Palmer. 2002. From
Allyson Ettinger. 2020. What BERT is not: Lessons
treebanktopropbank. InLREC,pages1989–1993.
fromanewsuiteofpsycholinguisticdiagnosticsfor
languagemodels. TransactionsoftheAssociationfor Beth Levin. 1993. English verb classes and alterna-
ComputationalLinguistics,8:34–48. tions: A preliminary investigation. University of
Chicagopress.
SarahM.B.Fagan.1992. TheSyntaxandSemanticsof
MiddleConstructions. CambridgeUniversityPress, BaiLi,ZiningZhu,GuillaumeThomas,FrankRudzicz,
Cambridge. andYangXu.2022. Neuralrealityofargumentstruc-
tureconstructions. InProceedingsofthe60thAnnual
CharlesJFillmore.1986. Pragmaticallycontrolledzero
Meeting of the Association for Computational Lin-
anaphora. In Annual Meeting of the Berkeley Lin-
guistics(Volume1: LongPapers),pages7410–7423,
guisticsSociety,volume12,pages95–107.
Dublin,Ireland.AssociationforComputationalLin-
Richard Futrell, Ethan Wilcox, Takashi Morita, Peng guistics.
Qian, Miguel Ballesteros, and Roger Levy. 2019.
TalLinzenandMarcoBaroni.2021. Syntacticstructure
Neurallanguagemodelsaspsycholinguisticsubjects:
from deep learning. Annual Review of Linguistics,
Representationsofsyntacticstate. InProceedingsof
7(1):195–212.
the2019ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics: Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
HumanLanguageTechnologies,Volume1(Longand 2016. AssessingtheabilityofLSTMstolearnsyntax-
ShortPapers),pages32–42,Minneapolis,Minnesota. sensitivedependencies. TransactionsoftheAssocia-
AssociationforComputationalLinguistics. tionforComputationalLinguistics,4:521–535.
Brendan S Gillon. 2012. Implicit complements: a Yao Lu, Max Bartolo, Alastair Moore, Sebastian
dilemmaformodeltheoreticsemantics. Linguistics Riedel, and Pontus Stenetorp. 2021. Fantastically
andPhilosophy,35:313–359.
ordered prompts and where to find them: Over- Michael Silverstein. 1976. Shifters, linguistic cate-
coming few-shot prompt order sensitivity. CoRR, gories,andculturaldescription. Meaninginanthro-
abs/2104.08786. pology.
Kyle Mahowald. 2023. A discerning several thou- Michael K. Tanenhaus, Greg Carlson, and John C.
sand judgments: GPT-3 rates the Article + Adjec- Trueswell.1989. Theroleofthematicstructuresin
tive+Numeral+Nounconstruction. arXivpreprint interpretationandparsing. LanguageandCognitive
arXiv:2301.12564. Processes,4(3-4):SI211–SI234.
Ken McRae, Michael J. Spivey-Knowlton, and CarolTenny.1992. Theaspectualinterfacehypothesis.
Michael K. Tanenhaus. 1998. Modeling the influ- pages490–508.CSLIPublications,Stanford.
enceofthematicfit(andotherconstraints)inon-line
Carol Tenny. 1994. Aspectual Roles and the Syntax-
sentence comprehension. Journal of Memory and
SemanticInterface. Kluwer,Dordrecht.
Language,38(3):283–312.
J.C. Trueswell, M.K. Tanenhaus, and S.M. Garnsey.
Kanishka Misra, Julia Taylor Rayz, and Allyson Et-
1994. Semantic influences on parsing: Use of the-
tinger. 2022. COMPS: Conceptual minimal pair
maticroleinformationinsyntacticambiguityresolu-
sentencesfortestingpropertyknowledgeandinheri-
tion. JournalofMemoryandLanguage,33(3):285–
tanceinpre-trainedlanguagemodels. arXivpreprint
318.
arXiv:2210.01963.
JeannevanOosten.1977. Subjectsandagenthoodin
UlrikePadó.2007. Theintegrationofsyntaxandseman-
English. InCLS13,pages451–471.
ticplausibilityinawide-coveragemodelofhuman
sentenceprocessing. Ivan Vulic´, Edoardo Maria Ponti, Robert Litschko,
GoranGlavaš,andAnnaKorhonen.2020. Probing
Isabel Papadimitriou, Richard Futrell, and Kyle Ma-
pretrainedlanguagemodelsforlexicalsemantics. In
howald.2022. Whenclassifyingarguments,BERT
Proceedings of the 2020 Conference on Empirical
doesn’tcareaboutwordorder...exceptwhenitmat-
MethodsinNaturalLanguageProcessing(EMNLP),
ters. InProceedingsoftheSocietyforComputation
pages7222–7240,Online.AssociationforComputa-
inLinguistics2022,pages203–205,online.Associa-
tionalLinguistics.
tionforComputationalLinguistics.
Erika Petersen and Christopher Potts. 2022. Lexical AlexWarstadt,AliciaParrish,HaokunLiu,AnhadMo-
semanticswithlargelanguagemodels: Acasestudy hananey,WeiPeng,Sheng-FuWang,andSamuelR.
ofEnglishbreak. Ms.,StanfordUniversity. Bowman.2020. BLiMP:Thebenchmarkoflinguis-
tic minimal pairs for English. Transactions of the
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
Association for Computational Linguistics, 8:377–
DarioAmodei,IlyaSutskever,etal.2019. Language
392.
modelsareunsupervisedmultitasklearners. OpenAI
blog,1(8):9. LeonieWeissweiler,ValentinHofmann,AbdullatifKök-
sal,andHinrichSchütze.2022. Thebetteryoursyn-
ShauliRavfogel,GrushaPrasad,TalLinzen,andYoav
tax, the better your semantics? Probing pretrained
Goldberg. 2021. Counterfactual interventions re-
languagemodelsfortheEnglishcomparativecorrel-
veal the causal effect of relative clause representa-
ative. arXivpreprintarXiv:2210.13181.
tions on agreement prediction. In Proceedings of
the25thConferenceonComputationalNaturalLan- EthanWilcox,RogerLevy,TakashiMorita,andRichard
guageLearning,pages194–209,Online.Association Futrell. 2018. What do RNN language models
forComputationalLinguistics. learnaboutfiller-gapdependencies? arXivpreprint
arXiv:1809.00042.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational Annie Zaenen, Jean Carletta, Gregory Garretson,
realization. Cognition,61(1-2):127–159. Joan Bresnan, Andrew Koontz-Garboden, Tatiana
Nikitina,MCatherineO’Connor,andThomasWa-
LiliaRissmanandAsifaMajid.2019. Thematicroles:
sow.2004. AnimacyencodinginEnglish: Whyand
Core knowledge or linguistic construct? Psycho-
how. InProceedingsoftheworkshopondiscourse
nomicbulletin&review,26(6):1850–1869.
annotation,pages118–125.
DavidERumelhartandJamesLMcClelland.1986. On
learningthepasttensesofEnglishverbs.
AsadSayeed,ClaytonGreenberg,andVeraDemberg.
2016. Thematic fit evaluation: an aspect of selec-
tionalpreferences. InProceedingsofthe1stWork-
shoponEvaluatingVector-SpaceRepresentationsfor
NLP,pages99–105, Berlin, Germany.Association
forComputationalLinguistics.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman
Castagné,AlexandraSashaLuccioni,FrançoisYvon,
Matthias Gallé, et al. 2022. BLOOM: A 176b-
parameteropen-accessmultilinguallanguagemodel.
arXivpreprintarXiv:2211.05100.
A Noun-Verb-AdverbCombinations verb writes
patients: section,passage,proposal,
verb sells nouns
code,essay
patients: toy,book,novel,magazine,
nouns agents: student, person, notetaker,
hat, lotion, album, car, SUV, prod-
journalist,scribe,doctor,professor,
uct,make,item,CD,drug,snack
essayist,blogger,poet,novelist,au-
agents: salesman, saleswoman,
thor
businessman, businesswoman,
adverbs quickly,easily
trader,peddler,telemarketer,dealer,
shopkeeper verb performs
adverbs easily,well,quickly patients: routine,song,choreogra-
nouns
phy,sonata,concerto,scene
verb drives
agents: musician,person,actor,co-
patients: car, SUV, truck, convert-
nouns median,dancer,singer,soloist
ible,vehicle,tank,bus,tractor,van
adverbs easily
agents: driver,person,chauffeur
adverbs nicely,smoothly,well verb photographs
patients: building, animal, land-
verb flies nouns
scape,lake,mountain,model,view
patients: plane,kite,jet,aircraft
nouns agents: photographer,cameraman
agents: pilot, person, aviator, cap-
adverbs nicely,beautifully
tain
adverbs nicely,smoothly,well verb plays
patients: cello,piano,violin,instru-
verb cooks nouns
ment,flute,clarinet
patients: mushroom, pepper, fish,
nouns agents: musician, violinist, cellist,
salmon,tuna,fillet,vegetable,herb,
pianist,drummer,flutist,clarinetist
meat,ingredient,steak
adverbs nicely,beautifully
agents: chef,cook,baker,caterer
adverbs nicely,well,terribly verb cuts
patients: meat,cardboard,packag-
verb bakes nouns
ing,board,paper,fabric
patients: pizza,potato,bread,cake,
nouns agents: hairdresser,barber,butcher,
pastry,dough,pie,clay
chef
agents: patissier,chef,cook,baker,
adverbs nicely,roughly,cleanly,effortlessly
person,confectioner
adverbs nicely,well,terribly verb cleans
patients: jewelry,window,counter-
verb reads nouns
top, floor, surface, carpet, wind-
patients: passage,poem,verse,line,
nouns shield,mirror,pot,silverware,bed-
passage,script,abstract,essay,let-
ding
ter,report
agents: janitor, maid, cleaner,
agents: student, orator, person,
housekeeper, busboy, waiter, wait-
narrator, announcer, broadcaster,
ress
teacher
adverbs easily,quickly,effortlessly
adverbs nicely,well
verb washes
verb paints
patients: bottle,tub,shirt,car,wind-
patients: wall,fabric,glass,canvas, nouns
nouns shield,dish,bedding,blanket,bowl
wood,surface,panel
agents: worker,maid,cleaner,bus-
agents: painter,artist,person,illus-
boy
trator,portraitist
adverbs easily,quickly
adverbs easily,terribly,well,beautifully
verb shaves
patients: beard,stubble,sideburn
nouns
agents: barber,hairdresser
adverbs neatly,nicely,smoothly
verb packs curateadverbsandnounsthatworkinthetemplates
patients: crate,lunchbox,basket,con- asdescribedinTable1.
nouns
tainer, coat, jacket, bag, duffle, food, Adverbs must be manner adverbs, but they
suitcase,tent,backpack shouldnotbeagent-orientedadverbs(Jackendoff
agents: mover,traveller,clerk,worker, 1972;Ernst2001)thatexpressthementalstateof
backpacker,roadtripper,hiker,camper theagent. Examplesofsuchadverbsincludefuri-
adverbs well,easily ously,happily,angrily,etc.
Thenforeachverbandalistofadverbsforeach
verb stitches
verb, wecomeupwithalistofpatientandagent
patients: silk,quilt,cotton,cut,cloth,
nouns nouns. Allofthenounsmustworkinintransitive
fabric,wound
andtransitivetemplatesusingthesamesenseofthe
agents: surgeon, tailor, machine, up-
verb. Fornounsaddedaspatientsintheintransitive,
holsterer,dressmaker
thenounmustnotbeanentitythatcausestheevent
adverbs easily,smoothly,nicely,poorly
describedbytheverb. Furthermore,itshouldnot
verb embroiders
benecessarilyobliqueinthetransitiveform. Inthe
patients: cushion,thread,cloth,fabric
nouns
examplebelow,needlecannotbethedirectobject
agents: tailor,seamster,seamstress
of the transitive and can only appear in the with
adverbs well,nicely,beautifully,poorly
prepositionalphrase,sowedonotincludeitinthe
verb knits listofnouns:
patients: yarn,wool,pattern
(4) a. Thisneedlesewseasily.
nouns
agents: person,lady,man,woman
b. Thetailorsewseasilywiththisneedle.
adverbs well,nicely,beautifully,poorly,easily
c. *Thetailorsewsthisneedleeasily.
verb sews
patients: fabric,material Fornounsaddedasagents,intheintransitiveit
nouns
agents: tailor,seamster,machine must be clear that the noun is the one doing the
adverbs well,nicely,beautifully,poorly action. Forhumanagents,wetrytoaddagentsthat
aremostcloselyassociatedtotheactiondescribed
verb turns
fortheevent,especiallywiththosethattendtotake
patients: screw,knob,car,bike,motor-
nouns humandirectobjectsinthetransitiveform,suchas
cycle,valve,handle
shave.
agents: driver,racer,motorist,pilot
adverbs smoothly,easily,nicely,roughly C HumanAnnotationDetails
verb carves
We had 19 human annotators rate all 233 unique
patients: pumpkin,wood,stone,gem,
nouns nouns on Google Forms. Each annotator saw a
ice,steak,turkey
differentrandomorderofthenounsandwerepre-
agents: sculptor,person,jeweler,arti-
sented with 10 nouns on each page of the form,
san,carver
though they could go back to alter previous re-
adverbs beautifully,nicely,cleanly,flawlessly
sponses. AllannotatorsarefluentinEnglish. An-
verb sculpts notatorswerealsoaskedtoself-identifyasnativeor
patients: wood, stone, marble, ice, non-nativespeakers;14of19considerthemselves
nouns
clay nativespeakers.
agents: sculptor, person, potter, ma- Fornounsthathavemultiplecommonandhighly
son,carver distinctwordsenses,wegaveannotatorsashortdis-
adverbs beautifully,nicely,cleanly ambiguatingdescription. Thisdescriptiondoesnot
containanyverbsoranyotherindicatorforwhat
B DataCurationCriteria
types of events the entity may occur in. A list of
thesenounswiththeirdisambiguatingdescription
Aftercollectingalistofoptionallytransitiveverbs
isgiveninTable5.
that appear as intransitive via object drop (agent
subject)orobjectpromotionintheformofthemid-
dleconstruction(patientsubject), wethenhadto
Noun Description • causing an event or change of state in
anotherparticipant
make productofaparticularcompany,suchasofa
car
• movement(relativetothepositionofan-
plane airplane
kite alightframecoveredwithpaper,cloth,orplas- otherparticipant)
tic,oftenwithastabilizingtail
• (existsindependentlyoftheeventnamed
jet aircraft
line ofatext/apoem/etc. bytheverb)
passage ofatext/anessay/etc.
panel ofwood/ahardsurface/etc.
(2) ContributingpropertiesforthePatientProto-
model person
routine apartofanentertainmentact Role:
board along,thin,flatpieceofwoodorotherhard
material • undergoeschangeofstate
letter asheetofpaperwithwordsonitinanenvelope
proposal aformalplanorsuggestion • incremental theme (something that
turkey meat
changes incrementally over the course
ofanevent)
Table5: Nounsanddisambiguatingdescriptionsgiven
toannotators. • causallyaffectedbyanotherparticipant
• stationary relative to movement of an-
C.1 Instructionsprovidedtoannotators otherparticipant
An agent is something that initiates an action, • (does not exist independently of the
possibly with some degree of volition. In other event,ornotatall)
words,nounsthattendtobeagentshaveatendency
For the sake of simplicity, disregard events de-
todothings.
scribedbyreflexives(suchasJohnshavedhimself).
For each of the following nouns, rate it on the
A patient is something that undergoes an action
followingscale:
and often experiences a change. In other words,
nouns that tend to be patients have a tendency to
1=veryunlikelytobeanagent
havethingsdonetoit.
2=somewhatunlikelytobeanagent
3=nopreferencebetweenagentandpatient
In this form, you are tasked to annotate how
4=somewhatlikelytobeanagent
"agentive"youthinkanountypicallyis—inother
5=verylikelytobeanagent
words, howlikelyitistobeanagentorapatient
when an action involving both an agent and a
patientoccur.
Ex: TheplantwaswateredbyJohn.
Theplant=patient
John=agent
Ex: ThesunburnsJohn.
Thesun=agent
John=patient
AmoreformaldefinitionisgivenbyDowty(1991),
whooutlinescontributingpropertiesofagentsand
patients:
(1) Contributing properties for the Agent Proto- Figure 6: Example of Google Form question format
Role: giventoannotators.
• volitional involvement in the event or
state—sentience(and/orperception)
Model APAP PAPA δ Model APAP PAPA δ
BLOOM560m 0.566 0.531 0.036 BLOOM560m 0.214 0.219 0.005
BLOOM1b1 0.384 0.365 0.019 BLOOM1b1 -0.096 0.027 0.124
BLOOM1b7 0.308 0.371 0.062 BLOOM1b7 0.618 0.795 0.177
BLOOM3b 0.476 0.133 0.343 BLOOM3b 0.049 0.512 0.463
BLOOM7b1 -0.118 0.150 0.268 BLOOM7b1 0.050 0.272 0.223
GPT-2small 0.648 0.652 0.004 GPT-2small 0.658 0.190 0.468
GPT-2medium 0.420 0.367 0.053 GPT-2medium 0.546 0.500 0.047
GPT-2large 0.501 0.496 0.005 GPT-2large 0.632 0.586 0.045
GPT-2xl 0.486 0.231 0.255 GPT-2xl 0.484 0.531 0.047
GPT-3ada-001 0.589 0.598 0.009 GPT-3ada-001 0.574 0.417 0.157
GPT-3babbage-001 0.394 0.228 0.166 GPT-3babbage-001 -0.030 -0.322 0.292
GPT-3curie-001 0.418 -0.204 0.622 GPT-3curie-001 0.045 0.266 0.221
GPT-3davinci-001 0.579 0.356 0.223 GPT-3davinci-001 0.673 0.622 0.051
GPT-3davinci-003 0.934 0.943 0.010 GPT-3davinci-003 0.927 0.911 0.017
Table6: Experiment1: Correlationbetweenthediffer- Table7: Experiment2: Correlationbetweenthediffer-
enceinlog-likelihoodofpredicting“agent”or“patient” enceinlog-likelihoodofpredicting“agent”or“patient”
withhumanratingsfornounsinisolationinbothexam- withhumanratingsfornounsinintransitivesentences
pleorderings. inbothexampleorderings.
D ResultsbyExampleOrder somegreaternumberoftimes(withinanARG0/1
span)inPropbank. Wecalltheminimumnumber
Tables6,7,and8showperformanceinbothAPAP
oftimesthenounmustappearthecountthreshold.
and PAPA orderings in Experiments 1 (nouns in
Figure7plotsthePropbankagentratiocorrela-
isolation),2(nounsinintransitivesentences),and
tionwithhumanratingsagainstthecountthreshold
3(nounsintransitivesentences)respectively. For
(ingreen). Wealsoplotthenumberofnounsthat
simplicity,weonlyreportcorrelationswithhuman
meetthiscountthreshold(inblue). Theminimum
judgements.
countthresholdtohaveagreatercorrelationthan
Both GPT-3 davinci-001 and
Google Syntactic Ngrams (pink line) is 27, how-
davinci-003 are very robust to changes
ever only 33 nouns meet this threshold. To meet
in example ordering for all three experiments, as
meettheaveragehumaninter-annotatorgroupcor-
are BLOOM 560m and 1b1. The three largest
relation,thethresholdis268;onlytwonounsmeet
BLOOM models are remarkably sensitive to
this.
ordering,especiallyinExperiment3,asareGPT-2
xlandGPT-3curie-001andbabbage-001. F AdjustingThresholdforExp2
E PropbankStatistics We also considered the possibility that the mod-
elsmayhaveabiastowardseitherthe“agent”or
When calculating model correlations with Prop-
“patient”labelandmayactuallybecorrectlyclassi-
bank,weuseallnounswithatleastoneoccurrence
fyingnounsgivenanappropriatenon-zerothresh-
ofappearingwithinanARG0/1spanintheparse
old for δ-LL. To account for this, we recalculate
treetomaximizethenumberofnounswecancom-
accuracieswiththresholdsthatprovidethebestper-
pare with. However, we recognize that this may
formanceforeachmodelasan“upperbound”for
messwithcorrelationvaluessincenounswithonly
performance,asseeninFigure8. Afterthisadjust-
one occurrence will have values at either 0 or 1.
ment, all models do at least as well as predicting
Furthermore,dependingontherolethenounhasin
the majority class, with GPT-2 xl experiencing
thatparticularsentence,itmaypushitsagentrat-
thelargestgaininaccuracy. Nevertheless,GPT-3
ingtotheoppositeendofthespectrumcompared
davinci-003stilloutperformsallothermodels
to its “typical” behavior. Thus, we also tried cal-
byfar.
culatingthecorrelationonlyfornounsthatoccur
trans-agent trans-patient
Model APAP PAPA δ APAP PAPA δ
BLOOM560m 0.034 0.090 0.056 0.962 0.932 0.031
BLOOM1b1 0.620 0.781 0.161 0.516 0.457 0.059
BLOOM1b7 0.940 0.013 0.927 0.007 0.989 0.982
BLOOM3b 1.000 0.059 0.941 0.000 0.895 0.895
BLOOM7b1 0.974 0.017 0.957 0.088 1.000 0.912
GPT-2small 0.313 0.796 0.483 0.811 0.210 0.600
GPT-2medium 0.121 0.000 0.121 0.877 1.000 0.123
GPT-2large 0.829 0.389 0.440 0.163 0.623 0.461
GPT-2xl 0.978 0.001 0.977 0.018 1.000 0.982
GPT-3ada-001 0.313 0.089 0.224 0.611 0.933 0.322
GPT-3babbage-001 0.987 0.044 0.943 0.023 0.994 0.971
GPT-3curie-001 0.353 0.034 0.319 0.740 0.963 0.224
GPT-3davinci-001 0.987 0.968 0.018 0.413 0.427 0.013
GPT-3davinci-003 0.996 0.993 0.004 0.999 0.984 0.015
Table 8: Experiment 3: Accuracy in both example orderings for predicting the role of the noun in transitive
sentences,wheretrans-agentcorrespondstothenouninsubjectpositionandtrans-patienttoobjectposition.
Figure 7: Count threshold versus the correlation be- Figure8: Averageaccuracyforpredictingthelabelin
tweennounagentratiosandhumanratingsandthenum- intr-agent/intr-patientsentenceswithadjustedthresh-
berofuniquenounsthatsurpassthethreshold. Thepink olds. Afterthisadjustment,allmodelsareatorabove
horizontallineshowsthecorrelationofGoogleSyntac- majority class accuracy. Magenta segments show in-
ticNgramswithhumanratings;theblacklineshowsthe creaseinperformance.
averageinter-annotatorgroupcorrelation.

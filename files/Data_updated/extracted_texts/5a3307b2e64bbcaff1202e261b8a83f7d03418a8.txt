Rethinking Voice-Face Correlation: A Geometry View
XiangLi1,YandongWen2,MuqiaoYang1,JingluWang3,RitaSingh1,BhikshaRaj1,4
1CarnegieMellonUniversity,2MaxPlanckInstitute,3Microsoft,
4MohamedbinZayedUniversityofArtificialIntelligence
ABSTRACT
Previousworksonvoice-facematchingandvoice-guidedfacesyn- VocalTract Vocal Track
thesisdemonstratestrongcorrelationsbetweenvoiceandface,but
mainlyrelyoncoarsesemanticcuessuchasgender,age,andemo-
ùêª(ùëß)
tion.Inthispaper,weaimtoinvestigatethecapabilityofreconstruct-
ingthe3Dfacialshapefromvoicefromageometryperspectivewith- Air Unit Impulse LPCFilter Speech
outanysemanticinformation.Weproposeavoice-anthropometric (a) Voice Production (b) Linear Predictive Coding
measurement(AM)-faceparadigm,whichidentifiespredictablefa-
cial AMs from the voice and uses them to guide 3D face recon-
struction.ByleveragingAMsasaproxytolinkthevoiceandface
Phonatory Module
geometry,wecaneliminatetheinfluenceofunpredictableAMsand
GaussianNoise
makethefacegeometrytractable.Ourapproachisevaluatedonour Speech
proposeddatasetwithground-truth3Dfacescansandcorresponding
AM-guided
voicerecordings,andwefindsignificantcorrelationsbetweenvoice
Reconstruction
andspecificpartsofthefacegeometry,suchasthenasalcavityand
Speech Voice Code AMs Face Geometry
cranium.Ourworkoffersanewperspectiveonvoice-facecorre-
(c) Speech-AMs-Face
lationandcanserveasagoodempiricalstudyforanthropometry
science. Figure 1: (a) Human voice production. (b) Linear predictive
codingrepresentsthevoicebyaunitimpulsewithasetoflinear
CCSCONCEPTS filterswhichcanbeinterpretedasanestimationofthevocal
‚Ä¢Computingmethodologies‚ÜíAppearanceandtexturerepresen- tract.(c)Ourvoice-AM-Facepipelinefirstpredictsandverifies
predictableanthropometricmeasurements(AMs)andthenuti-
tations.
lizesAMstoguide3Dfacereconstruction.Aphonatorymodule
KEYWORDS isinvolvedtoobtainabetterrepresentationforAMprediction.
voice,face,vocaltract
ACMReferenceFormat:
XiangLi1,YandongWen2,MuqiaoYang1,JingluWang3,RitaSingh1,Bhik- synthesis, the generated faces have reasonable appearances with
shaRaj1,4.2023.RethinkingVoice-FaceCorrelation:AGeometryView.In propergender,ageandemotionstatuscorrespondingtothevoice.
ProceedingsofACMConference(Conference‚Äô23).ACM,NewYork,NY, Thosesemanticcorrelationsarestrongandeasytolearnthusdomi-
USA,10pages.https://doi.org/10.1145/nnnnnnn.nnnnnnn nantpreviousmodelswhileafundamentalquestionwewanttocast
is,arethereanyothervoice-facecorrelationsexceptforthosecoarse
1 INTRODUCTION semantics?Isreconstructingidentity-fidelity3Dfacefromvoicepos-
Thestudyofface-voicecorrelationhasbeenextensivelyinvestigated sible?Inthispaper,weaimtoexplorethevoice-facecorrelationsin
inrecentyears.Previousworksonvoice-facematching[28,44,53], ageometryviewafterconstrainingallthoseeasilylearnedsemantic
voice-guidedfacesynthesis[9,16,19,54],andvoice-guidedface biases.
modificationhaveindicatedastrongcorrelationbetweenvoiceand Thereareseveralpreviousworksinvestigatingrecoveringface
face.Themostintuitiveandcommonlyusedconsensusencodedbe- fromvoice.Mostofthemarefroma2Dperspective[16,19,54],
tweenvoiceandfaceismainlybasedonsemantics,suchasgender, whichutilizeGenerativeAdversarialNetwork(GAN)[14,26]to
ageandemotion.Mostpriorworksaimtolearnasemanticcorre- generatefaceswithvoiceasthecondition.However,facerecovering
spondencebetweenvoiceandfaceandconductcrossmodaltasksby fromvoiceisill-posed.[29]foundthattherecoverymainlyfocuses
leveragingthoseconsensuses.Forexample,forvoice-guidedface onsomesemanticsofthespeaker.Forexample,attributessuchas
ethnicityhasweakornofunctionwhilegenderandagetendtobe
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
recovered.Sincethosemodelsmainlyrelyonsemantics,theresults
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation arenotidentity-fidelitywhichmeansgeneratedfacescanlookvery
onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM differentfromtheoriginalones.Inaddition,fora2Dfaceimage,
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
identity-unrelatedfactorslikeexpressions,hairs,glasses,illumina-
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee.Requestpermissionsfrompermissions@acm.org. tion, background, etc., are also involved in the recovery process
Conference‚Äô23,July2023,Ottawa,Canada leadingtonoisyandunstableoutcomes.Differentfrom2Dimages,
¬©2023AssociationforComputingMachinery.
general3Dfacialshapeisrepresentedbythe3Dcoordinatesofa
ACMISBN978-x-xxxx-xxxx-x/YY/MM...$15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn numberofpointsonitssurfacecalledvertices[4]whichinherently
3202
luJ
62
]VC.sc[
1v84931.7032:viXra
Conference‚Äô23,July2023,Ottawa,Canada XiangLi1,YandongWen2,MuqiaoYang1,JingluWang3,RitaSingh1,BhikshaRaj1,4
excludestheidentity-unrelatedfactors.Moreover,sincethetopology WiththepredictedAMs,wereconstructthefacialshapesbyan
of3Dfacialshapeispredefinedandconsistentacrossdifferentfaces, optimization-basedmethod,whichfirstprojectsthe3Dfacialshapes
wecaneasilymeasurethereconstructionaccuracywithdistances intoalow-dimensionallinearspace[4].Byadjustingthecoefficients
betweenthepredictedverticesandtheirgroundtruths. inlow-dimensionalspace,weobtaindifferentre-projected3Dfacial
Similartoourtarget,onerecentwork[47]attemptstorecover3D shapes. Though this paper mainly focuses on understanding the
facesfromvoicewhile,duetothelackofground-truth3Dfacescans, relationshipbetweenthe3Dfacialshapeandvoicefromascientific
theyfirstgenerate2Dfaceimagesfromvoiceandthenreconstruct angle, this technique has its potential applications. For example,
3Dfacesguidedbyanoff-the-shelf3Dfacereconstructionmodel. theidentity-fidelityfacialshapecanbeusedforcriminalprofiling
Thenoiseenrolledinthe2D-to-3Dfacereconstructionmakesthe scenarios,suchashoaxcallsandvoice-basedphishing.
resultunconvincing.Forexample,anyexpressioninthe2Dface Inthispaper,wetrytoanswertwocorequestions-(1)Istherea
fromthefirststagewillforcethereconstructed3Dfacetohavethe correlationbetweenfacegeometryandvoice?(2)Ifso,whichpart
sameexpression.Inthisway,weconsiderthefaceisstilldetermined ofthefacecanberepresentedbythevoice?Tofulfillourtarget,we
bythefirst-stage2Dfaceimage. collectalarge-scaledatasetcontainingground-truth3Dfacescans
Inourmethod,weaimtodisableallpreviouslyusedsemantics, andcorrespondingvoicerecordingsfrom1026speakeridentities.
e.g.,gender,ageandemotion,andfocusonthevoice-facecorrelation Avoice-AM-faceparadigmequippedwithaphonatorymoduleis
fromapuregeometryview.Beforeintroducingourmethod,letus proposedforanalyzingthevoice-facecorrelation.Ourcontributions
gobackandunderstandhowvoiceisgeneratedbyhumanbeings. canbesummarizedasfollows.
voiceisproducedbyphonatorystructures(Fig.1(a)),e.g.,vocal ‚Ä¢ Weproposeavoice-AM-faceparadigmandacorresponding
tractandvocalcords.Specifically,whenproducingvowels,thevocal voice-facedatasetfortractable3Dfacedeductionfromvoice.
cordsvibratewithnoobstructioninthevocaltract.Incontrast,for ‚Ä¢ Weinvestigatevoice-facecorrelationinafine-grainedman-
mostconsonants,thephonationpurelydependsonthevocaltract nerbystatisticallyverifyingwhichpartofthefacecanbe
resonancewithapulmonicairflow.Thevocaltractcanbeassumed reflectedbythevoice.Theresultscanserveasagoodrefer-
asafilterthatmakesthephonemesversatileandpersonalized.With encetosupportfuturevoice-faceresearch,suchasvoice-face
thephonationmechanismofhumanbeings,asshowninFig.1(b), verification.
Markeletalintroduceslinearpredictivecoding(LPC)[25]which ‚Ä¢ Weleveragevoiceproductionasaproxytasktolearnface
modelsphonationasaunitimpulsesignalmodifiedbyastackof geometryrepresentationandverifythatvoiceproductionis
tubes(vocaltract)andencodespersonalizedvoicebyvocaltract highlyrelatedto3Dfacialshapes.
coefficients.TheLPCyieldsagoodphysicalmodelofthevocaltract
withonlyvoiceinputsinanunsupervisedmanner.Asthemouth 2 RELATEDWORKS
andnoseserveasthemostimportantpartsofthevocaltract,we
2.1 Voice-faceMatchingandVoice-guidedFace
hypothesizethattheirgeometryshouldbeencodedinthevoice.With
Synthesis
thetightbindofmusclesandskeletons,otherpartsoffacegeometry
mayalsoberepresentedbyvoice. The human voice contains rich information that can be used to
Thoughvoiceandfacegeometryshouldhavesomecorrelations, recognize personality traits, such as speaker identity [6, 24, 33],
wehavenoideaaboutwhichpartofthefacevoicecanrepresent. gender[22],age[15,30,39],andemotionstatus[43,51].Voices
Constructinguncorrelatedrelationswillleadtorandomresultsand can also be used for monitoring health conditions [1] and other
raise the model instability. To tackle this problem, we introduce medicalapplications[17].Mostexistingworksinthisareafocus
thevoice-anthropometricmeasurement(AM)-faceparadigm.Pre- onpredictingpersonalitytraitsthatareintuitivelyrelatedtovoice.
viousstudieshaveshownthatanthropometricmeasurementslike Suchpersonalitytraitsmayhaveessentialcorrelationsbetweenthe
thedimensionsofnasalcavities[42]orcranium[48,49]directly humanvoiceandtheirfaces[45].
influence the speaker‚Äôs voices. In our voice-AM-face paradigm, Cross-modalvoice-facematching[28,44,53]andcross-modal
we first summarize a set of AMs from anthropometry literature verification[27,37,41]aretaskswherevoicesareusedasqueriesto
[10,11,32,38,55],thenidentifypredictableAMsandusethemto retrievefacesorviceversa,whichhavereceivedincreasingattention
guidethe3DfacereconstructionbyconductingAM-guidedopti- inrecentyears.Voice-guidedfacesynthesisisanotherrelatedtask,
mization.ByleveragingAMsasaproxytolinkthevoiceandface whichaimstogeneratecoherentandnaturallipmovements,and
geometry,wecaneliminatetheinfluenceofunpredictableAMsand includesmethodsthatdrivetemplateimages[16,19,54]ortemplate
makethefacegeometrytractable.Inaddition,theanalysisofAMs facemeshes[9]totalkbyspeechinputs,orreplacelipmovements
alsobringsanewviewtounderstandingvoice-facecorrelationina inavideowithmovementsinferredfromanothervideoorspeech
fine-grainedfashion. [8,46].
InspiredbyLPCwhichlearnstheshapeofthevocaltractbypro- Unliketheexistingworkinrelatedfieldsthataremorefocusedon
ducingvoice,weutilizeaphonatorymoduletofacilitatevoicerepre- semanticcorrelationsbetweenvoiceandface,ourworkinvestigates
sentationlearningforfacegeometry.Similartotheauto-regressive thevoice-facecorrelationfromageometryviewbystudyingholistic
impulse-by-filtermodelusedinLPC,recentlyintroduceddenoising facialstructures.Therehasbeenrecentworkthatseekstounder-
diffusionprobabilisticmodels[18]shareasimilarstructure,which standthecorrelationsbetweenvoiceandfacialgeometrybyfirst
samplesarandomnoisewithauto-regressiveupdatingtoformthe recovering2Dfacesfromvoiceandthenreconstructing3Dfaces
finalresult.Basedonthestructuresimilarity,wechoosethediffusion fromthe2Drepresentations[47].However,duringthisprocess,itis
modelasourphonatorymodule. stillinevitablethatthesemanticcorrelationsareencodedinthe2D
RethinkingVoice-FaceCorrelation:AGeometryView Conference‚Äô23,July2023,Ottawa,Canada
faceandaffectthe2D-to-3Dfacereconstruction.Instead,weaimto empiricallyvalidatingthedependency.Specifically,thedatasetDis
modelourvoice-facecorrelationfromapuregeometryviewwithout splitintoatrainingsetDùë° formodellearning,avalidationsetDùë£1
theinfluenceofanysemantics. formodelselection,avalidationsetDùë£2forAMselection,andan
evaluationsetDùëí forevaluatingthereconstructed3Dfacialshapes.
2.2 PhonationandAnthropometry Allsplitshavenooverlap.
The human voice is generated by phonatory structures, and the
3.2 PipelineOverview
phonation of different phonemes may be dependent on different
physiologicalstructures.Forexample,thephonationofconsonants AsshowninFig.2,theproposedmethodhasthreemaincomponents
includessomeairflowobstructioninthevocaltract,whilevowelsdo -facialAMprediction,AM-guidedreconstructionandanauxiliary
not.Byutilizingsuchproperties,ithasbeenproventobeinformative phonatorymodule.Ononehand,wepredicttheAMsthatarepoten-
andhelpfulinvarioustasks,includingautomaticspeechrecogni- tiallycorrelatedwithvoiceproductionfromanthropometryliterature
tion[12],speechenhancement[50]andemotionrecognition[13]. [10,11,32,38,55].AnestimatorEistrainedwithuncertaintylearn-
Beyondthoselanguage-relatedusages,humanattributesarealso ingwithavoicecodeùëí.Ontheotherhand,inspiredbythevoice
predictablefromvoice.Thereisasubstantialbodyofresearchon productionmechanism,weintroduceaphonatorymoduleasacon-
inferringhumanattributesfromaperson‚Äôsvoice,includingspeaker strainttofacilitatethetrainingofAMprediction.Inparticular,a
identity[7,34],age[3,31],gender[23],andemotionstatus[43,52]. diffusion-basedvoicegenerationmoduleisinvolvedasthephona-
The interaction between these physiological structures may play torymodulewhichaimstoimitatethevoiceidentityconditioning
an important role in the recovery of 3D faces from voice. More onthevoicecodeùëí.Afterthat,weselecttheAMspredictablefrom
specifically,theunderlyingskeletalandarticulatorystructureofthe voiceforhypothesistesting.Thenullhypothesisismadeforeach
faceandthetissuecoveringthemmaygoverntheshapes,sizes,and AMandstatestheAMisunpredictablefromvoice.Wecansuccess-
acousticpropertiesofthevocaltractthatproducesthevoice.Linear fullyrejectthecorrespondingnullhypothesisifanyAMestimation
predictivecoding(LPC)[25]whichmodelsphonationasaunitim- is better than chance on a held-out validation set with statistical
pulsesignalmodifiedbyastackoftubes(vocaltract)andencodes significance.Thefinal3Dfacialshapescanbereconstructedbya
personalized voice by vocal tract coefficients. The LPC yields a fittingprocess[5]basedonthepredictableAMs.Thisisconducted
goodphysicalmodelofthevocaltractwithonlyvoiceinputsinan byadjustingasetofcoefficientsinlow-dimensionalspace,such
unsupervisedmanner. thatthedifferencesbetweentheAMsofthegenerated3Dfacial
To explicitly describe the correspondence between vocal and shapeandthepredictedAMsareminimized.Intuitively,ifthereare
facial features, anthropometric measurements have been used in morepredictableAMsspanningdifferentlocationsofaface,the
a wide range of applications to associate with voice production reconstructioncanbemoreindistinguishable.
[10,11,32,38,40,55].Inabroadsense,AMsmaycovervarious
bodyparametersandcharacteristics,includingskeletalproportions, 3.3 FacialAMPrediction
race,height,bodysize,etc.Thesecharacteristicsmayinfluencethe Inthissection,weillustrateourmethodtopredictfacialAMsfrom
phonationofvoicebythedifferencesintheplacementoftheglottis, voice.
lengthofvocalcords,etc.
In this work, we summarize a large set of AMs that is highly AMsummarization.Thereisalargebodyofliteratureonanthro-
associatedwithvoice-facecorrelation.Meanwhile,wealsoidentify pometry.ExtensivestudiesshowthatmanyAMsofhumanfaces
thepredictableAMstoguidethe3Dfacialshapereconstruction.The canbeassociatedwithvoiceproduction[10,11,32,38,55].We
resultscanserveasagoodreferencetosupportfuturevoice-face summarizethemostcommonlyusedAMsasshowninFig.3(the
research. complete list of AMs is available in the appendix). The chosen
AMsarecategorizedasproportion,anglesanddistanceofasetof
3 METHOD facelandmarks.Thoseintra-facefeaturesaremorerobustthan3D
coordinaterepresentationsasthevariationsresultingfromspatial
In this section, we first introduce the task formulation and then
misalignmentarecompletelyeliminated.
demonstrateourmethodindetail.
Uncertainty-aware AM estimation. The AM prediction is con-
3.1 Formulation ductedbyanestimatortrainedwithanuncertainty-awarescheme.
We aim to reconstruct any speaker‚Äôs 3D facial shape from their
Letùêπ ùëò(ùë£;Eùëò,ùúî ùëò):ùë£ ‚Ü¶‚ÜíRbeanestimatorthatmapsvoicerecording
voice recordings. Given a set of paired voice recordings and 3D ùë£ intotheùëò-thpredictedAMs,whereEùëò andùúî ùëò arethelearnable
facialshapes{(ùë£ ùëñ,ùëì ùëñ)}fromdifferentindividuals,whereùë£ ùëñ isavoice parameters.Asthisisaregressionproblem,weleverage
r s fae c cc a io n ar n ld e si dn hg af pros ep mo ùëìk t ohe fen asb npy yea st kh pee er aùëñ ko- et fh rùë£ fùëñp r.e oTr ms ho e tn hg ea o in rad l vi oùëì sùëñ icti o es rra ee cc3 ooD rn ds if t nra u gc ci ùë£a t .l th Is neha o3p uDe
r
{E ùëò‚àó,ùúî ùëò‚àó}=ar Eg ùëò,m ùúîùëòin |D1 ùë°| (ùë£,ùëö‚àëÔ∏Å (ùëò))‚ààDùë°(ùêπ ùëò(ùë£;Eùëò,ùúî ùëò)‚àíùëö(ùëò))2 (1)
method,weintroduceanthropometricmeasurements(AMs)ùëö = as the training objective for theùëò-th AM. |Dùë°| is the number of
{ùëö(1),¬∑¬∑¬∑,ùëö(ùëò)}computedfromùëì asaproxy,whereùêæisapositive thetriplets(voice,faceandAMs)indatasetDùë°.Byincorporating
integer andùëöùëò (ùëò ‚àà [1,ùêæ]) denotes the ùëò-th AM. Accordingly, uncertaintyintotheestimatorlearning,thepredictionbecomesa
theoveralldatasetisdenotedasD ={(ùë£ ùëñ,ùëì ùëñ,ùëö ùëñ)}.Tostatistically randomvariableratherthanasinglevalue.WeleverageaGaussian
analyze the results, we construct an additional validation set for distributiontotheprediction.Theestimatorùêπ ùëò(ùë£;Eùëò,ùúî ùëò) mapsùë£
Conference‚Äô23,July2023,Ottawa,Canada XiangLi1,YandongWen2,MuqiaoYang1,JingluWang3,RitaSingh1,BhikshaRaj1,4
Phonatory Module AM-guided
Bases
Reconstruction
DiffusionProcess DenoisingProcess
ùëè
$
UNetùúñ!
ùëß! ùëß"
√ó(T‚àí1) AM-guided
ùí´ ‚ãØ ùíú ùíú ùí´#$ ùëè Optimization
6
Speechùë£% Speechùë£* Face Geometry
ùë• ùë• ùë•~ùí©(0, 1) ùë•* ùë•*
Share ! $ "#$ ! ùëè
7
Identity
ùíú Attention
ùúî
‚Ñ∞ AM ùí´ Pre-Processing
ùúô [0.1 ‚ãØ0.7] Selection
Speech ùë£ Log-melSpec. Voice Code ùëí Uncertainty Predictable AM
AMs AMs
Facial AM Prediction Unpredictable AM
Figure2:Illustrationofouranalysispipelineforvoice-facecorrelation.Werandomlypicktwovoicerecordingswithsharedspeaker
identityasùë£andùë£‚Ä≤.WethenanalyzetherelationshipbetweeneachAMandvoicebypredictingeachAMfromvoicewithanestimator
andanintermediatevoicecodeùëí.Theoptionalphonatorymoduleequipsadiffusion-basedvoicegenerationmodelwithavoicecodeùëí
asaconditiontoconductvoicestylecloningtohelpusunderstandtherelationshipbetweenfacegeometryandvoicecharacteristics,
whichservesasanadditionalconstrainttoenforcetheestimatorlearnvoiceidentity.WeanalyzeandselectAMswithhypothesis
testing.ThestaticallysignificantlypredictableAMsareutilizedfor3Dfacialshapereconstructionforfurtheranalysis.
AMswhenthepredictedvariancesaresmall,anddeferthevoice
recordingstohumanexpertsotherwise.Anextremecaseisùê∫ ùëò(ùë£)‚â°
1wheretheuncertaintylearningobjectivedegradestotheregular
regressionmodel.
Temporal aggregation. In practice, following the convention of
(a) Landmarks (b) Proportion (c) Angle (d) Distance voiceunderstanding,thelongvoicerecordingùë£isfedintothenet-
workintheformofmultipleshortsegments{ùë£(1),¬∑¬∑¬∑,ùë£(ùêø)}.We
Figure3:ExamplesofsummarizedAMs.Wesummarizedthree obtain a sequence of means and variances of the predicted AM.
typesofAM:proportion,angleanddistance.ThoseAMsare Duringtraining,wecomputethelossforeachsegmentindividually
computedfromthepredefinedlandmarkonthe3Dfacerepre- andaveragethemasthetrainingloss.Whileduringevaluation,the
sentation. predictedAManditsuncertaintyaregivenbyaggregatingthepre-
dictionsamongallsegments.Assumingtheshortsegmentsfroma
longrecordingareclass-conditionallyindependent,theformulations
into the mean of theùëñ-th predicted AM. Similarly, we define an ofaggregationare
vu aban r lc i ee ar n pt c aa e rin ao mty f ete h ts eet ri sùëòm .-ta Tht ho p er rùê∫ e pd rùêø ei( c dùë£ t ie; cdE teùëò A d, Mùúô Aùëò . M) A: g aùë£ a ni‚Ü¶‚Üí dn, itER sùëò+ ga‚à™ ron u{ d0 nùúô} dùëòt th a ra uret thùë£ th bi en et clo e oat mrh ne e- ùëöÀÜ(ùëò) =‚àëÔ∏Å ùëô=ùêø
1
ùê∫
ùëòùë§ (( ùë£ùëò ()
ùëô))
¬∑ùêπ ùëò(ùë£(ùëô)),
N(ùêπ ùëò(ùë£),ùê∫ ùëò(ùë£))andN(ùëö(0),0)respectively[20].Giventworan-
ùêø
(3)
domvariables,amorereasonablelearningobjectiveistominimize 1 =‚àëÔ∏Å 1
theirKLdivergence. ùë§(ùëò)
ùëô=1
ùê∫ ùëò(ùë£(ùëô))
{E‚àó,ùúî‚àó,ùúô‚àó}= argmin 1 ‚àëÔ∏Å (ùêπ ùëò(ùë£;Eùëò,ùúî ùëò)‚àíùëö(ùëò))2 whereùëöÀÜ(ùëò) istheaggregatedmeanandalsothepredictedùëò-thAM.
ùëò ùëò ùëò Eùëò,ùúîùëò,ùúôùëò |Dùë°| (ùë£,ùëö(ùëò))‚ààDùë° ùê∫ ùëò(ùë£;Eùëò,ùúô ùëò) H taio nw tyev oe fr t, ht ehe pra eg dg icr te eg da ùëòte -d thv Aar Mian sc ine cùë§ et( hùëò e) ci os nn do it tiou nse ad linas det ph ee nu dn ec ne cr e-
+lnùê∫ ùëò(ùë£;Eùëò,ùúô ùëò)
assumptiondoesnotalwaysholdincasessuchasnoises,silences,
(2)
Forafixed (ùêπ ùëò(ùë£;Eùëò,ùúî ùëò)‚àíùëö(ùëò))2,thereisanoptimalvariance thecomputedaggregatedvariancewillbebiasedbythenumberof
ùê∫ ùëò(ùë£;Eùëò,ùúô ùëò)=(ùêπ ùëò(ùë£;Eùëò,ùúî ùëò)‚àíùëö(ùëò))2suchthatthelossfunction v asoi ùë§c ÀÜe (ùëòse )g =m ùêøen ¬∑t ùë§si (n ùëò)th .elongrecording.Sowecalibratetheuncertainty
isminimized.Therebytheuncertaintyestimatorùê∫ ùëò islearnedto
produceasmallvarianceifthepredictionerrorissmallandvice Predictable AM identification. We have collected a number of
versa.Onthecontrary,asmallervarianceindicatesthatthepredicted AMsandtrainedestimatorsforpredictingthem.However,onlya
AM is more likely to yield a small prediction error,i.e., close to fewoftheAMsareactuallypredictablefromvoice,whichwehad
thegroundtruth.Inthisway,wecanchoosetotrustthepredicted anticipated while designing the task. To identify those AMs, we
‚ãØ
‚ãØ
RethinkingVoice-FaceCorrelation:AGeometryView Conference‚Äô23,July2023,Ottawa,Canada
usehypothesistestingtothem.Formally,wecanwritethenulland onlyservesasanadditionaltrainingconstraintandisnotapplied
alternativehypothesesfortheùëò-thAMas during inference. Letùë• 0,¬∑¬∑¬∑,ùë• ùëá be a sequence of variables with
ùêª 0:theAMùëö(ùëò) isNOTpredictablefromvoice thesamedimensionwhereùë° istheindexfordiffusiontimesteps.
ùêª 1:theAMùëö(ùëò) ispredictablefromvoice Thenthediffusionprocesstransformsùë• 0intoaGaussiannoiseùë• ùëá
throughachainofMarkovtransitionswithasetofvarianceschedule
I thn ao trd ve or ict eor ie sje inc dtùêª ee0 d,w use eo funl ly inne pe rd et do icfi tin nd ga Aco Mun ùëöte (r ùëòex ).am Ap nle efto fes ch tio vw
e
iùõΩ n1 g,¬∑ t¬∑ o¬∑ t, hùõΩ eùëá M.S arp ke oc vifi trc aa nll sy it, ie oa nc ph rotr ba an bs if lo itr ym ùëûa (t ùë•io ùë°n
|ùë•
ùë°is ‚àí1p ,e ùëír )fo ar sm sue md ea dcc toor bd e-
exampleistocomparetheestimatorswithandwithoutthevoice independentofthestylecodeùëías
input. Ifthereexistsalearned estimatorùêπ ùëò(ùë£) performingbetter
thanthechance-levelestimatorùê∂ ùëò withoutusingvoiceinputand ùëû(ùë• ùë°|ùë• ùë°‚àí1,ùëí)=N(ùë• ùë°;‚àöÔ∏Å 1‚àíùõΩ ùë°ùë• ùë°‚àí1,ùõΩ ùë°ùêº). (5)
theresultsarestatisticallysignificant,wecansuccessfullyrejectùêª
0
andacceptùêª 1.Herethechance-levelestimatorfortheùëò-thAMis Unlikethediffusionprocess,thedenoisingprocessaimstorecover
a thc eo tn rast ia nn int gùê∂ ùëò set= D|D ùë°1 .ùë° S| o(cid:205) tùëö h( eùëò) n‚àà uD llùë° aùëö nd(ùëò a), ltw erh ni ac th ivi es hth ye pom the ea sn isùëö c( aùëò n) bo ef t tih oe ns ap le de ic sth ris bi ug tn ioa nlf ùëùro ùúÉm (ùë• 0G :ùëáa ‚àíu 1s |s ùë•i ùëáan ,ùëên )o .i Tse hrw ouh gic hh this ed re evfi en re sd ea trs aa nsc io tin od ni s-
rewrittenas
ùëù ùúÉ(ùë• 0:ùëá‚àí1|ùë• ùëá,ùëê),thevariablesaregraduallyrestoredtoaspeechsig-
nalwithstylecodecondition.Thephonatorymoduleactuallymodels
ùêª ùêª0 1: :ùúá ùúá( (ùúñ ùúñùëò ùëò/ /ùúñ ùúñùëò ùëòùê∂ ùê∂)
)
‚â§ ‚â•1
1
wa edi os btr ti ab iu nti to hn eùëû ad(ùë• d0 it| iùëê o) n. aB ly traa ip np il ny gin cg onth ste rap ia nr ta am seterizationtrick[21],
w wih te hr oe uùúñ tùëò voa in cd eiùúñ nùëòùê∂ pua tr se ot nhe vam lie da an tios nqu sea tre De ùë£r 2r ,o rr es so pf ece ts it vi em lya .to Tr hs ew foit rh ma un lad
-
{E‚àó,ùúÉ‚àó}=arg Em ,ùúÉin=E ùë• 0,ùúñ,ùë°‚à•ùúñ‚àíùúñ ùúÉ(‚àö ùõº¬Øùë°ùë• 0+‚àö 1‚àíùõº¬Øùë°ùúñ,ùë°,ùëí)‚à• 1 (6)
t ai no dns ùúño ùëòùê∂fùúñ =ùëòa |n Dd 1 ùë£2ùúñ |ùëòùê∂ (cid:205)a ùëöre (ùëòg )i ‚ààve Dn ùë£2a (s ùê∂ùúñ ùëòùëò ‚àí= ùëö|D (1 ùëòùë£2 )| )2(cid:205) .ùëö S( iùëò n) c‚àà eD tùë£ h2 e(ùëö tÀÜ ru(ùëò e)‚àí vaùëö ri( aùëò n) c) e2 w ah Ne er te [ùõº 3ùë° 6]= w1 it‚àí hùõΩ cùë° roa sn sd -aùõº t¬Ø tùë° en= ti(cid:206) onùë° ùë°‚Ä≤ [= 31 5ùõº ].ùë°‚Ä≤ S. iA ncs es th ho ew pn hi on naF ti og r. y2 m,t oh de eùúÉ lii ss
ofùúñ ùëò/ùúñ ùëòùê∂ isunknown,thetypeofhypothesistestingisone-sided onlyutilizedasanauxiliaryconstraintduringtraining,weomitthe
paired-samplet-test.Theupperboundoftheconfidenceinterval(CI) inferencedetailstoobtainùë£Àúhere.
isgivenby
ùê∂ùêº ùë¢ =ùúá(ùúñ ùëò/ùúñ ùëòùê∂ )+ùë° 1‚àíùõº,ùúà ¬∑
ùúé(ùúñ
‚àöùëò
ùëÅ/ùúñ ùëòùê∂)
(4)
T3 o.4 recoA nsM tru- cG ttu heid 3e Dd fa3 cD ialF sha ac pi ea ,l wS eh fia rp ste neR edec toon prs et dr icu tc Ati Mon
sof
thevoicerecordingsinDùëí first.Subsequently,wegeneratethe3D
whereùúá(¬∑)andùúé(¬∑)arethefunctionsforcomputingmeanandstan-
facialshapesbasedonthepredictedAMsbyanoptimization-based
darddeviationrespectively.ùëÅ isthenumberoftherepeatedexperi-
method.Todoso,wefirstprojectthe3Dfacialshapesintoalow-
mentsandwesetùëÅ =100here.ùõºandùúà =ùëÅ‚àí1arethesignificance
dimensionallinearspace[5].Byadjustingthecoefficientsinlow-
level and the degree of freedom respectively. For the purpose of
dimensionalspace,weobtaindifferentre-projected3Dfacialshapes.
this section, we adopt the significance level of 5% and then we
Thelearningobjectiveistofindasetofcoefficients,suchthatthe
canreadùë° 0.95,ùëÅ‚àí1fromt-distributiontable.Nowwecandetermine
differencesbetweentheAMsofthere-projected3Dfacialshapeand
whethertorejectùêª 0andacceptùêª 1,i.e.,theAMùëö(ùëò) ispredictable thepredictedAMsareminimized.Specifically,weconstructabig
fromvoice.Accordingtotheexperimentalresults,theprobability matrixùêµ= [ùëè 1,ùëè 2,¬∑¬∑¬∑] ‚ààR3ùëá√ó|Dùë°| whereeachcolumnùëè
ùëñ
‚ààR3ùëá√ó1
thattheaforementioneddecisioniscorrectishigherthan95%,i.e., isalongvectorobtainedbyflatteninga3Dfacialshapeùëì
ùëñ
‚ààRùëá√ó3.
statisticallysignificant.Incontrast,ùê∂ùêº ùë¢ ‚â•1impliesthatwefailto ùëá is the number of vertices on 3D faces. Since 3ùëá ‚â´ |Dùë°|, we
rejectùêª 0,forthecurrentexperimentalresultsarenotstatistically computetheprojectmatrixùëÉ ‚ààR3ùëá√óùëë(ùëë ‚â´3ùëá)usingeigenfaces
significantenough.Notethatfailingtorejectùêª 0doesnotimplywe [5]onùêµ.Nowanyflattened3Dfacialshapeùëècanbeapproximated
acceptùêª 0. byre-projectingalow-dimensionalvectorùõΩ ‚ààRùëè√ó1intheformof
Weemphasizethatitisnecessarytocomputeùúñ ùëòùê∂ andùúñ ùëò onDùë£2 ùëÉùõΩ.WedefinethecomputationofAMasùëÑ ùëò(ùëè) :ùëè ‚Ü¶‚Üí R,which
ratherthanDùë° orDùë£1.Thisisbecauseourestimatorsaretrainedon mapsanyflattened3Dfacialshapeùëèintotheùëò-thAMofùëè.Since
lD owùë° ea rnd
ùúñ
ùëòse al ne dct ùúñe ùëòùê∂db oy nt th he ese err so prs lito sn .Dùë£1,wecaneasilygetsignificantly ùëÑ shùëò a( p¬∑ e) ,c io tm isp aut de is ffa ed reis nt ta in ac be le,a fup nro ctp io or nt .io Tn h, eor oa pn timan ig zl ae tio of nth oe bj3 eD ctf ivac eia isl
Optionalphonatorymodule.Inspiredbylinearpredictivecoding givenbelow.
(LPC) [25] which leverages voice producing to learn vocal tract ùêæ
geometry,weaimtofacilitatefacegeometrycapturebylearning ùõΩ‚àó=argminùúÜ‚à•ùõΩ‚à•2 2+‚àëÔ∏Å (ùëÑ ùëò(ùëÉùõΩ)‚àíùëöÀÜ(ùëò))2¬∑ùëß(ùëò) (7)
characteristicsofvoice.Weenrollaphonatorymoduleservingas ùõΩ ùëò=1
an additional constraint when predicting facial AMs. In particu-
whereùúÜisthelossweightbalancingtwoterms.Thereconstructed
lar, we leverage a diffusion-based [18] voice generation method
3DfacialshapeisgivenbyùëèÀÜ=ùëÉùõΩ‚àó.
tomodelthetime-domainspeechsignals.AsshowninFig.2,the
diffusionmodelconvertsthenoisedistributiontoaspeechùë£Àú con-
trolledbythevoicecodeùëíextractedfromspeechùë£.Duringtraining 4 EXPERIMENTS
speechùë£‚Ä≤ whichsharesspeakeridentitywithùë£ isfedtothediffu- Inthissection,weelaborateonthedatasetsetting,implementation
sionmodelasground-truth.Pleasenotethatthephonatorymodule detailsandexperimentalresults.
Conference‚Äô23,July2023,Ottawa,Canada XiangLi1,YandongWen2,MuqiaoYang1,JingluWang3,RitaSingh1,BhikshaRaj1,4
Figure4:Thenormalizederrorsandùê∂ùêºsof24AMson(a)malesubset,(b)femalesubset,and(c)asmallerfemalesubset.If1‚àíùê∂ùêº
ùë¢
>0,
theAMispredictableelseunpredictable.
4.1 Dataset 4.2 ImplementationDetails
WeperformexperimentsonaprivateaudiovisualdatasetD.The We leverage a backbone E to learn voice codeùëí which is a sim-
datasetconsistsofpairedvoicerecordingsandscanned3Dfacial pleconvolutionalneuralnetwork.Thedetailednetworkstructure
shapesfrom1,026people,with364malesand662females.The ispresentedinthesupplementarymaterials.ùêπ ùëò andùê∫ ùëò sharethe
scanned3Dfaceisstoredinthemeshformatwith6790pointsfor backbone‚Äôslearnableparametersbuthaveindividualparametersfor
eachface.Thevoicerecordingsareabout2minuteslongforeach theirheads.Weuseasinglelayerfully-connectednetworkforeach
speaker. We reduce the influencing factors to the voice and face head.Forthevariancehead,weaddanexponentialactivationto
by(1)askingparticipantstospeakasetofspecifiedsentences,(2) thelastlayerofùê∫ ùëò fornon-negativepositiveoutput.Wefollowthe
askingparticipantstospeakwithoutemotion,(3)controltheageof typicalsettingsofstochasticgradientdescent(SGD)foroptimiza-
participants(roughly18-28yearsold).Inaddition,topreventthe tion.Minibatchsizeis64.Themomentum,learningrate,andweight
modelsfromtakingthegendershortcuts,wesplitthedatasetDby decayvaluesare0.9,0.1,and0.0005,respectively.Thetrainingis
gender,andexperimentsareindividuallyperformedonmaleandfe- completedat5kiterations.Sincethephonatorymodulerequiresa
malesubsets.Foreachsubset,weadopt7/1/1/1splittingforDùë°/Dùë£1 longtrainingprocedure,wefirsttrainitwiththevoicecodeencoder
/Dùë£2/Dùëí.Intraining,thevoicerecordingsarerandomlytrimmed Efor60kstepsonourtrainingsetDùë°.Wefollowthetrainingsetting
tosegmentsof6to8seconds,whileweusetheentirerecordings in[18]totrainthephonatorymodule.Theotherparametersetting
intesting.ThegroundtruthAMsarenormalizedtozeromeanand follows[18].Wedirectlynormalizedthevoicesignalasinputtothe
unitvariance.Forvoicefeatures,weextract64-dimensionallogMel- networkinsteadoffirstconvertingittoLog-Melspectrum.Toensure
spectrogramsusingananalysiswindowof25ms,withthehopof statisticalsignificance,weperformN=100repeatedexperimentsto
10msbetweenframes.Weperformmeanandvariancenormalization computetheùê∂ùêº ùë¢.Fortheexperimentsatphonemelevel,weleverage
ofeachMel-frequencybin. Wav2Vec[2]tocutthelongvoicerecordingsintophonemes.
decnalaB
)c(
tesbus
elaM
)a(
tesbus
elameF
)b(
tesbus
elameF
RethinkingVoice-FaceCorrelation:AGeometryView Conference‚Äô23,July2023,Ottawa,Canada
PhonationModule 100%ùë§ÀÜ 75%ùë§ÀÜ 50%ùë§ÀÜ PhonatoryModule Predictable Unpredictable
(cid:33) 0.953¬±0.009 0.909¬±0.024 0.842¬±0.030 (cid:33) 0.628¬±0.021 0.990¬±0.032
(cid:37) 0.952¬±0.014 0.927¬±0.030 0.879¬±0.041 (cid:37) 0.730¬±0.048 1.002¬±0.031
Table1:Effectofthephonatorymodule.Wemeasurethenormal- Table2:Effectofphonatorymoduleforpredictableandunpre-
izedmeansquarederrorbetweenpredictedandground-truth dictableAMs.Wemeasurethenormalizedmeansquarederror
AMamongallAMswithdifferentconfidencethresholds. betweenpredictedandground-truthAMamongallpredictable
andunpredictableAMs.Interestingly,wefindphonatorymod-
uleonlyimprovespredictableAMs.
Male Female
subset,i.e.,364females.Surprisingly,theresultsonthenewsubset
arestillbetterthanthoseonthemalesubset,asshowninFig.4(c).
Thisispossiblebecausethefemalesubjectshavehighernasalance
scoresonthenasalsentences[42]amongotherthings,whichpro-
videsusefulinformationforpredictingtheAMsaroundthenose.
Herewenotethatourexperimentshaverevealedthatmeasurements
aroundthenosearehighlycorrelatedtovoice.Moreinvestigations
areleftforfuturework.
Figure5:VisualizationofthepredictableAMs.Bluebox:male,
Ontheotherhand,someAMshavenotbeenshowntobepre-
Redbox:female.
dictablefromvoice.Thisobservationsuggeststhatvoicesmayonly
associatewithafewspecificregionsofthe3Dfacialshape,likethe
4.3 PredictableAMAnalysis noseandmouth.FortheAMswithhighererrorsthanchancelevel,
wedonotclaimtheyarenotpredictablefromvoice.Instead,wefail
ForAMprediction,theestimationmodelsaretrainedonDùë° and
todemonstratetheirpredictabilitybasedonourcurrentempirical
selectedbasedontheirperformanceonDùë£1(hyperparametertun-
results.Thepossiblereasonsincludeimperfectmodeling,limited
ing).ForAMselection,thepredictableAMsareselectedbasedon
data,datanoise,etc.
theupperboundoftheCI(ùê∂ùêº ùë¢)onDùë£2.Theperformancecanbe
evaluatedbythemeanerrorofeachAManditsCI.
4.4 EffectofPhonatoryModule
Fig.4showstheresults,including20AMswithhighest1‚àíùê∂ùêº
ùë¢
and4AMswithlowest1‚àíùê∂ùêº ùë¢.Thegraybarsaretheresultson AspresentedinTable1,itisevidentthatutilizingthephonatory
theentirevalidationsetDùë£2,whiletheredandyellowonesarethe moduleduringtrainingenhancestheaccuracyofpredictedAMs.
resultsof75%and50%voicesampleswithlowestuncertaintyùë§ÀÜ on Ourevaluationinvolvedcomputingthenormalizederroracrossall
Dùë£2,respectively.Theself-constructedfemalesubsethasthesame AMswithvariousconfidencethresholds.Althoughthemodelswith
sizeasthemalesubset.Higher1‚àíùê∂ùêº ùë¢ indicatesbetterresultsand andwithoutthephonatorymoduleexhibitedamarginaldifferencein
errorwhenevaluatingallthedata,theonestrainedwiththephona-
thenormalizederrorof0indicatesthechance-levelperformance.
torymoduleshowedaclearimprovementinerrorwhenconsidering
Assuggestedbyourhypothesistestingformulation,theAMswith
1‚àíùê∂ùêº ùë¢ > 0areconsideredpredictablefromvoice.Inthissense, moreconfidentsamples.
Furthermore,weconductedanerrorevaluationforpredictable
wehavediscoveredanumberofpredictablefemaleAMs(seethe
gray bars and theirùê∂ùêº ùë¢ in Fig. 4 (b)). By filtering out the voice andunpredictableAMsasdepictedinTable2.Weobservedthat
sampleswithhighuncertainties,weachieveevenhigher1‚àíùê∂ùêº ùë¢ (see utilizingthephonatorymoduleresultedina0.102-pointdecreasein
theredandyellowbarsandtheirùê∂ùêºs).Theimprovedperformance normalizederrorforpredictableAMs,highlightingitseffectiveness
inimprovingthepredictionperformance.Interestingly,thephona-
indicatesthatmoreAMsarediscoveredaspredictablefromvoice.
torymoduledidnothaveanyapparenteffectonunpredictableAMs.
The complete results of all AMs are given in the appendix. The
Overall, the results indicate that utilizing the phonatory module
resultsempiricallydemonstratethattheinformationof3Dfacial
during training is beneficial for predicting AMs, particularly for
shapeisindeedencodedinthevoicesandcanbediscoveredbyour
predictableones.
analysispipeline.
To intuitively locate the predictable AMs on the 3D face, we
4.5 Phoneme-levelAnalysis
visualizetheminFig.5.Weclearlyobservethatmostofthepre-
dictableAMsarearoundnoseandmouth,andmanyofthemare Wealsoexperimentwiththevoice-facecorrelationatthephoneme
sharedbetweenmaleandfemalesubsets.Thisisconsistentwiththe level.Forthisexperiment,wetrainandevaluateestimatorsbytaking
factthatnoseandmouthshapesaffectpronunciation. onephonemeasinputeachtime.Wecomputedtheaverage1‚àíùê∂ùêº ùë¢
Wealsonoticethattheperformanceoffemalesubsetismuch valueforeachphonemeacrossallAMs,asshowninFig.7.Our
betterthanthatofthemalesubset.Toinvestigatewhethertheim- resultsindicatethat/i:/hadthehighestaverage1‚àíùê∂ùêº ùë¢ valueof
provements come from the larger data scale (364 malesùë£.ùë†. 662 0.199,while/b/hadthelowestvalueof-0.06.Whenthe1‚àíùê∂ùêº ùë¢
females),weperformanothersetofrepeatedexperimentsonaself- valueislessthan0,itsuggeststhatAMsaregenerallyunpredictable
constructed female subset, which has the same size as the male fromthecorrespondingphoneme.
Conference‚Äô23,July2023,Ottawa,Canada XiangLi1,YandongWen2,MuqiaoYang1,JingluWang3,RitaSingh1,BhikshaRaj1,4
100 % 90 % 80 % 70 % 60 % 50 %
Male
Female
Figure6:Errormapsofthereconstructed3Dfacialshapesforthemaleandfemalesubsets.Fromlefttoright:theerrormaps
correspondingto100%(i.e.theentiretestset)to50%ofthetestset.
aboutfacialfeatures,makingiteasierforthemodeltocapturethe
hiddencorrelationwhenpredictingAMs.
4.6 3DFacialShapeReconstruction
InSection4.3,wehavediscoveredanumberofpredictableAMs,
fromwhichwechoose10AMswiththehighest1‚àíùê∂ùêº ùë¢ forthe
subsequentreconstructionsonmaleandfemalesubsets.
Toevaluatetheperformance,wecomputetheper-vertexerrors
betweenthereconstructed3Dfacialshapeandtheirgroundtruths.
Wealsofilteroutaportionofvoicesampleswiththehighestuncer-
taintiesandevaluatetheerrorsintheremainingdata.Thefilter-out
rateisfrom0%to50%,asshownfromlefttorightinFig.6.
Unsurprisingly, we achieve the lowest errors around the nose
regionformaleandfemalesubsets,consistentwiththeAMestima-
tions.Moreover,thereconstructionerrorsdecreasesignificantlyby
Phonemes
filteringoutthevoicesampleswiththehighestuncertainties.This
indicatesthatthelearneduncertaintyiseffectivelyassociatedwith
Figure7:Phonemeswithcorrespondingaveragedùê∂ùêº ùë¢indecreas- thereconstructionqualityandallowsthesystemtodecidewhether
ingorder. totrustthemodelornot.
5 CONCLUSION
Weobservedthatthethreephonemeswiththelowestandnegative
Inconclusion,thispaperpresentsanovelapproachtoexploringthe
1‚àíùê∂ùêº ùë¢ valueswere/t/,/b/,and/d/,allofwhichareplosive
voice-facecorrelationbyfocusingonthegeometricaspectsofthe
consonants.Duringthepronunciationofplosiveconsonants,there
faceratherthanrelyingonsemanticcuessuchasgender,age,and
isacompletestoppageofairflowfollowedbyasuddenreleaseof
emotion.Theproposedvoice-anthropometricmeasurement(AM)-
airthroughminimalmouthopeningandclosing.Asaresult,there
faceparadigmidentifiespredictablefacialAMsfromthevoiceto
isminimalmovementofthefacialmusclesandstructures,making
guide3Dfacereconstruction,whichresultsinsignificantcorrela-
itchallengingforthemodeltopredictAMsbasedsolelyonthese
tionsbetweenvoiceandspecificpartsofthefacegeometry,suchas
phonemes.
thenasalcavityandcranium.Thisapproachnotonlyeliminatesthe
Incontrast,mostvowelsachievedgoodperformanceinthetestset,
influenceofunpredictableAMsbutalsooffersanewperspective
withallofthetop6phonemesbelongingtovowelswith1‚àíùê∂ùêº
ùë¢
>
onvoice-facecorrelation,whichcanbevaluableforanthropometry
0.10.Comparedtoconsonants,theproductionofvowelsdoesnot
science.Theresultsofthisstudyopenuppossibilitiesforfuture
involveconstrictionofairflowinthevocaltract.Instead,thefacial
researchinthisarea,suchasdevelopingmoreaccuratevoice-guided
muscleshaverelativelygreatermovementduringthepronunciation
facesynthesistechniquesandabetterunderstandingoftherelation-
ofthesephonemes,suchasjawmovementduetomouthopeningor
shipbetweenvoiceandfacialgeometry.
lipspreading.Thus,vowelphonemesmaycarrymoreinformation
ùêºùê∂‚àí1
!
RethinkingVoice-FaceCorrelation:AGeometryView Conference‚Äô23,July2023,Ottawa,Canada
REFERENCES
[27] ShahNawaz,MuhammadSaadSaeed,PietroMorerio,ArifMahmood,Ignazio
[1] ZulfiqarAli,GhulamMuhammad,andMohammedFAlhamid.2017.Anauto- Gallo,MuhammadHaroonYousaf,andAlessioDelBue.2021. Cross-Modal
matichealthmonitoringsystemforpatientssufferingfromvoicecomplicationsin SpeakerVerificationandRecognition:AMultilingualPerspective.InCVPRW.
smartcities.IEEEAccess5(2017),3900‚Äì3908. [28] HailongNing,XiangtaoZheng,XiaoqiangLu,andYuanYuan.2021.Disentangled
[2] AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed,andMichaelAuli.2020. RepresentationLearningforCross-modalBiometricMatching.TMM(2021).
wav2vec2.0:Aframeworkforself-supervisedlearningofspeechrepresentations. [29] Tae-HyunOh,TaliDekel,ChangilKim,InbarMosseri,WilliamTFreeman,
Advancesinneuralinformationprocessingsystems33(2020),12449‚Äì12460. MichaelRubinstein,andWojciechMatusik.2019.Speech2face:Learningtheface
[3] MohamadHasanBahari,MitchellMcLaren,HugoVanhamme,andDavidA. behindavoice.InProceedingsoftheIEEE/CVFconferenceoncomputervision
vanLeeuwen.2012.AgeEstimationfromTelephoneSpeechusingi-vectors.In andpatternrecognition.7539‚Äì7548.
Interspeech. [30] PaulHPtacekandEricKSander.1966.Agerecognitionfromvoice.Journalof
[4] VolkerBlanzandThomasVetter.1999.Amorphablemodelforthesynthesisof speechandhearingResearch9,2(1966),273‚Äì277.
3Dfaces.InProceedingsofthe26thannualconferenceonComputergraphics [31] PaulH.PtacekandEricK.Sander.1966.Agerecognitionfromvoice.Journalof
andinteractivetechniques.187‚Äì194. speechandhearingresearch92(1966),273‚Äì7.
[5] VolkerBlanzandThomasVetter.2003.Facerecognitionbasedonfittinga3D [32] NarayananRamanathanandRamaChellappa.2006.Modelingageprogression
morphablemodel.IEEETransactionsonpatternanalysisandmachineintelligence inyoungfaces.In2006IEEEComputerSocietyConferenceonComputerVision
25,9(2003),1063‚Äì1074. andPatternRecognition(CVPR‚Äô06),Vol.1.IEEE,387‚Äì394.
[6] RayBull,HarrietRathborn,andBrianRClifford.1983.Thevoice-recognition [33] MircoRavanelliandYoshuaBengio.2018.Speakerrecognitionfromrawwave-
accuracyofblindlisteners.Perception12,2(1983),223‚Äì226. formwithsincnet.In2018IEEESpokenLanguageTechnologyWorkshop(SLT).
[7] R. H. C. Bull, Harriet Rathborn, and Brian R. Clifford. 1983. The Voice- IEEE,1021‚Äì1028.
RecognitionAccuracyofBlindListeners.Perception12(1983),223‚Äì226. [34] MircoRavanelliandYoshuaBengio.2018. SpeakerRecognitionfromRaw
[8] LeleChen,ZhihengLi,RossKMaddox,ZhiyaoDuan,andChenliangXu.2018. WaveformwithSincNet. 2018IEEESpokenLanguageTechnologyWorkshop
Lipmovementsgenerationataglance.InECCV.520‚Äì535. (SLT)(2018),1021‚Äì1028.
[9] DanielCudeiro,TimoBolkart,CassidyLaidlaw,AnuragRanjan,andMichaelJ [35] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rn
Black.2019. Capture,learning,andsynthesisof3Dspeakingstyles.InCVPR. Ommer.2022. High-resolutionimagesynthesiswithlatentdiffusionmodels.
10101‚Äì10111. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
[10] LeslieGFarkas,OttoGEiben,StefanSivkov,BryanTompson,MarkoJKatic, Recognition.10684‚Äì10695.
andChristopherRForrest.2004. Anthropometricmeasurementsofthefacial [36] OlafRonneberger,PhilippFischer,andThomasBrox.2015.U-net:Convolutional
frameworkinadulthood:age-relatedchangesineightagecategoriesin600healthy networksforbiomedicalimagesegmentation.InMedicalImageComputingand
whiteNorthAmericansofEuropeanancestryfrom16to90yearsofage.Journal Computer-AssistedIntervention‚ÄìMICCAI2015:18thInternationalConference,
ofCraniofacialSurgery15,2(2004),288‚Äì298. Munich,Germany,October5-9,2015,Proceedings,PartIII18.Springer,234‚Äì
[11] DonyaGhafourzadeh,CyrusRahgoshay,SahelFallahdoust,AdelineAubame, 241.
AndreBeauchamp,TiberiuPopa,andEricPaquette.2019. Part-based3Dface [37] LedaSarƒ±,KritikaSingh,JiatongZhou,LorenzoTorresani,NayanSinghal,and
morphablemodelwithanthropometriclocalcontrol.(2019). YatharthSaraf.2021.AMulti-ViewApproachtoAudio-VisualSpeakerVerifica-
[12] PrasantaKumarGhoshandShrikanthNarayanan.2011.Automaticspeechrecog- tion.InICASSP.
nitionusingarticulatoryfeaturesfromsubject-independentacoustic-to-articulatory [38] ZhiyiShan,RichardTai-ChiuHsung,CongyiZhang,JuanjuanJi,WingShanChoi,
inversion.TheJournaloftheAcousticalSocietyofAmerica130,4(2011),EL251‚Äì WenpingWang,YanqiYang,MinGu,andBalvinderSKhambay.2021.Anthro-
EL257. pometricaccuracyofthree-dimensionalaveragefacescomparedtoconventional
[13] PatrickGomezandBrigittaDanuser.2007.Relationshipsbetweenmusicalstruc- facialmeasurements.ScientificReports11,1(2021),1‚Äì12.
tureandpsychophysiologicalmeasuresofemotion.Emotion7,2(2007),377. [39] RitaSingh,JosephKeshet,DenizGencaga,andBhikshaRaj.2016.Therelation-
[14] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley, shipofvoiceonsettimeandvoiceoffsettimetophysicalage.InICASSP.IEEE,
SherjilOzair,AaronCourville,andYoshuaBengio.2020.Generativeadversarial 5390‚Äì5394.
networks.Commun.ACM63,11(2020),139‚Äì144. [40] RitaSingh,BhikshaRaj,andDenizGencaga.2016.Forensicanthropometryfrom
[15] JoannaGrzybowskaandStanislawKacprzak.2016.SpeakerAgeClassification voice:anarticulatory-phoneticapproach.In201639thInternationalConvention
andRegressionUsingi-Vectors..InINTERSPEECH.1402‚Äì1406. onInformationandCommunicationTechnology,ElectronicsandMicroelectronics
[16] YudongGuo,KeyuChen,SenLiang,YongjinLiu,HujunBao,andJuyongZhang. (MIPRO).IEEE,1375‚Äì1380.
2021.AD-NeRF:AudioDrivenNeuralRadianceFieldsforTalkingHeadSynthe- [41] RuijieTao,RohanKumarDas,andHaizhouLi.2020. Audio-visualspeaker
sis.InICCV. recognitionwithacross-modaldiscriminativenetwork.InINTERSPEECH.
[17] Jing Han, Chlo√´ Brown, Jagmohan Chauhan, Andreas Grammenos, Apinan [42] Tom√°≈°Vampola,Jarom√≠rHor√°cÀáek,VojteÀáchRadolf,JanG≈†vec,andAnne-Maria
Hasthanasombat,DimitrisSpathis,TongXia,PietroCicuta,andCeciliaMas- Laukkanen.2020.Influenceofnasalcavitiesonvoicequality:Computersimula-
colo.2021.ExploringAutomaticCOVID-19Diagnosisviavoiceandsymptoms tionsandexperiments.TheJournaloftheAcousticalSocietyofAmerica148,5
fromCrowdsourcedData.InICASSP.IEEE. (2020),3218‚Äì3231.
[18] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic [43] Zhong-QiuWangandIvanTashev.2017.Learningutterance-levelrepresentations
models.AdvancesinNeuralInformationProcessingSystems33(2020),6840‚Äì forspeechemotionandage/genderrecognitionusingdeepneuralnetworks.In
6851. ICASSP.IEEE,5150‚Äì5154.
[19] AmirJamaludin,JoonSonChung,andAndrewZisserman.2019.Yousaidthat?: [44] PeisongWen,QianqianXu,YangbangyanJiang,ZhiyongYang,YuanHe,and
Synthesisingtalkingfacesfromaudio.InternationalJournalofComputerVision QingmingHuang.2021.SeekingtheShapeofSound:AnAdaptiveFramework
(IJCV)127,11(2019),1767‚Äì1779. forLearningVoice-FaceAssociation.InCVPR.16347‚Äì16356.
[20] AlexKendallandYarinGal.2017.Whatuncertaintiesdoweneedinbayesian [45] YandongWen,BhikshaRaj,andRitaSingh.2019. FaceReconstructionfrom
deeplearningforcomputervision?Advancesinneuralinformationprocessing VoiceusingGenerativeAdversarialNetworks.InNeurIPS,Vol.32.
systems30(2017). [46] OliviaWiles,AKoepke,andAndrewZisserman.2018. X2face:Anetwork
[21] DiederikPKingmaandMaxWelling.2013. Auto-encodingvariationalbayes. forcontrollingfacegenerationusingimages,audio,andposecodes.InECCV.
arXivpreprintarXiv:1312.6114(2013). 670‚Äì686.
[22] ShengLi,DabreRaj,XugangLu,PengShen,TatsuyaKawahara,andHisashi [47] Cho-YingWu,Chin-ChengHsu,andUlrichNeumann.2022.Cross-ModalPer-
Kawai.2019.ImprovingTransformer-BasedSpeechRecognitionSystemswith ceptionist:CanFaceGeometrybeGleanedfromVoices?.InProceedingsofthe
CompressedStructureandSpeechAttributesAugmentation..InINTERSPEECH. IEEE/CVFConferenceonComputerVisionandPatternRecognition.10452‚Äì
4400‚Äì4404. 10461.
[23] ShengLi,DabreRaj,XugangLu,PengShen,TatsuyaKawahara,andHisashi [48] MarzenaWyganowska-Swikatkowska,IwonaKowalkowska,GrazynaFlicinska-
Kawai.2019.ImprovingTransformer-BasedSpeechRecognitionSystemswith Pamfil,MikolajDabrowski,PrzemyslawKopczynski,andBozenaWiskirska-
CompressedStructureandSpeechAttributesAugmentation.InInterspeech. Woznica.2017.Vocaltraininginananthropometricalaspect.LogopedicsPhoni-
[24] CorrinaMaguinness,ClaudiaRoswandowitz,andKatharinavonKriegstein.2018. atricsVocology42,4(2017),178‚Äì186.
Understandingthemechanismsoffamiliarvoice-identityrecognitioninthehuman [49] MarzenaWyganowska-Swikatkowska,IwonaKowalkowska,KatarzynaMehr,and
brain.Neuropsychologia116(2018),179‚Äì193. MikolajDkabrowski.2013.Ananthropometricanalysisoftheheadandfacein
[25] JohnDMarkel,AugustineHGray,andAugustineHGray.1976.Linearprediction vocalstudents.FoliaPhoniatricaetLogopaedica65,3(2013),136‚Äì142.
ofspeech:Communicationandcybernetics.(1976). [50] MuqiaoYang,JosephKonan,DavidBick,YunyangZeng,ShuoHan,Anurag
[26] MehdiMirzaandSimonOsindero.2014.Conditionalgenerativeadversarialnets. Kumar,ShinjiWatanabe,andBhikshaRaj.2023.PAAPLoss:APhonetic-Aligned
arXivpreprintarXiv:1411.1784(2014). AcousticParameterLossforSpeechEnhancement.Proc.ofICASSP(2023).
Conference‚Äô23,July2023,Ottawa,Canada XiangLi1,YandongWen2,MuqiaoYang1,JingluWang3,RitaSingh1,BhikshaRaj1,4
[51] ZixingZhang,BingwenWu,andBj√∂rnSchuller.2019. Attention-augmented [53] AihuaZheng,MenglanHu,BoJiang,YanHuang,YanYan,andBinLuo.2021.
end-to-endmulti-tasklearningforemotionpredictionfromspeech.InICASSP. Adversarial-metriclearningforaudio-visualcross-modalmatching.TMM(2021).
IEEE,6705‚Äì6709. [54] HangZhou,YuLiu,ZiweiLiu,PingLuo,andXiaogangWang.2019. Talking
[52] ZixingZhang,BingwenWu,andBj√∂rnSchuller.2019. Attention-augmented facegenerationbyadversariallydisentangledaudio-visualrepresentation.InAAAI,
End-to-endMulti-taskLearningforEmotionPredictionfromSpeech.ICASSP Vol.33.9299‚Äì9306.
2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignal [55] ZiqingZhuang,DouglasLandsittel,StaceyBenson,RaymondRoberge,and
Processing(ICASSP)(2019),6705‚Äì6709. RonaldShaffer.2010.Facialanthropometricdifferencesamonggender,ethnicity,
andagegroups.Annalsofoccupationalhygiene54,4(2010),391‚Äì402.

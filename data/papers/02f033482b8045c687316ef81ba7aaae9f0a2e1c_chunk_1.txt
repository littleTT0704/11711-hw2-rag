DEXPERTS: Decoding-Time Controlled Text Generation
with Experts and Anti-Experts
AlisaLiu♥ MaartenSap♥ XimingLu♥♣ SwabhaSwayamdipta♣
ChandraBhagavatula♣ NoahA.Smith♥♣ YejinChoi♥♣
♥PaulG.AllenSchoolofComputerScience&Engineering,UniversityofWashington
♣AllenInstituteforArtificialIntelligence
alisaliu@cs.washington.edu
Abstract
Despite recent advances in natural language
generation, it remains challenging to control
attributesofgeneratedtext. WeproposeDEX-
PERTS: Decoding-time Experts, a decoding-
time method for controlled text generation
that combines a pretrained language model
with “expert” LMs and/or “anti-expert” LMs
in a product of experts. Intuitively, under
the ensemble, tokens only get high probabil-
ity if they are considered likely by the ex-
pertsandunlikelybytheanti-experts. Weap-
plyDEXPERTStolanguagedetoxificationand
sentiment-controlled generation, where we
outperform existing controllable generation
methodsonbothautomaticandhumanevalua-
tions. Moreover,becauseDEXPERTSoperates
only on the output of the pretrained LM, it is
effectivewith(anti-)expertsofsmallersize,in-
cluding when operating on GPT-3. Our work
highlightsthepromiseoftuningsmallLMson
text with (un)desirable attributes for efficient
decoding-timesteering.
1 Introduction Figure1: IllustrationofDEXPERTS,whereatoxicLM
actsasan“anti-expert”andanon-toxicLMactsasan
Controllingtheoutputofpretrainedlanguagemod- “expert”. Inthistoyexample,giventheprompt,“When
els(LMs)iscrucialforachievingusefulandsafe she rejected his advance, he grabbed,” the toxic LM
language generation applications, such as non- assigns greater weight to “her” than “his”, expressing
subtlesignalsoftoxicitythat
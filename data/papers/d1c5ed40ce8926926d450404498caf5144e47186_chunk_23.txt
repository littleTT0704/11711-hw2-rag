 on QA performance using a TREC
dataset. The crawler was seeded with the home pages of educational institutions and
retrieved linked web pages in breadth-first order. The authors found that over 50 GB
of web content were required to outperform the 3 GB reference corpus used in TREC,
and that performance actually degraded if the crawl exceeded about 500 GB. Our
proposed method improves on earlier work by using statistical models to reduce the
size of the retrieved web data by two orders of magnitude and to filter out noise that
may hurt QA performance.
Balasubramanian and Cucerzan [2009] propose an algorithm for generating doc-
uments comprising useful information about given topics from web data. The use-
fulness of sentences extracted from web pages is determined by using aspect models
builtfromquerylogsoftheBing1 searchengine. Theseaspectmodelsconsistofwords
that frequently co-occur with a given topic or related topics in the query logs. The
approach is used to compile biographical information about people from Wikipedia’s
“Living people” category, but it appears to be applicable to other types of topics
as well. The authors note that the topic pages generated with their approach are
often preferable over the search result pages created by Bing. A key difference to
the proposed SE approach lies in the generation of topic models for selecting useful
content. Instead of relying on query logs, we leverage the content of existing seed
corpora to model topicality. While comprehensive query logs may be hard to come
by, particularly when starting out in a new domain, there already exist seed corpora
for many knowledge domains. For instance, Wikipedia articles can be used as seeds
when developing a QA system for trivia questions, a medical encyclopedia could serve
as a starting point for a QA system that answers questions about diseases and cures,
1http://www.bing.com/
10 CHAPTER 2. RELATED WORK
and a legal dictionary can be used for answering questions about the law. Further-
more, while the query-based aspect modeling approach is evaluated on a relatively
small dataset of 300 topics, we were able to apply our source expansion approach
efficiently to more than 400,000 topics and provide intrinsic evaluation results as well
as extrinsic results on multiple QA datasets comprising over 3,000 questions each.
For the Jeopardy! QA challenge
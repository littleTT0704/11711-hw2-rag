odalfusionmodule,and
gregation.Opticalflowreflectingthemotionalsoservesasa decoders.Wefirstleverageavisualandanacousticencoder
strongcueforVSODtaskin(Lietal.2018,2019).Recently, to extract visual {f }T and acoustic features gsem, gloc.
t t=1
Skip Connections
Student
Visual Enc.
Visual Dec.
Video 𝑉={𝐼!⋯𝐼 "} Visual Feature 𝑓! "% #$ GroundTruth Student Feature 𝑓!&!’ "% #$ Student Prediction 𝑀!&!’ "% #$
(Label-guided)
Multimodal Fusion
ℒ’(!)($$ ℒ!)*+&
Skip Connections GroundTruth
Sem. Head
Acoustic Enc. Sem Embed. 𝑔!"# Teacher
Visual Dec.
Loc. Head
Multi-channelAudioA= 𝐻 # #% $! Loc. Embed. 𝑔$%& Teacher Feature 𝑓!!() "%
#$
Teacher Prediction 𝑀!!() "%
#$
Figure2:PipelineOverview.Weuseseparateencoderstoextractmultimodalfeatures.ForavideoclipV = {I ···I },a
1 T
visualencoderisutilizedtoextractvisualfeature{f }T.ForaudioinputA = {H }4,atwo-brunchacousticencoderis
t t=1 i i=1
employedtoextractthesemanticembeddinggsemandlocationembeddinggloc.Afterthat,alabel-guidedmultimodalfusion
module is introduced to effectively fuse the multimodal features, which outputs a student feature {fstu}T and a teacher
t t=1
feature{ftch}T.Twodecodersareleveragedtodecodethefinalpredictions{Mstu}T and{Mtch}T fromcompacted
t t=1 t t=1 t t=1
features{fstu}T and{ftch}T respectively.Inparticular,toenhancemultimodalcommunication,adistillationlossL is
t t=1 t t=1 d
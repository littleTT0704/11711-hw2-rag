 explanations (McGillandKlein,1993).
(§3). We then perform a battery of experiments While contrastive and counterfactual explana-
aimedatexaminingtowhatextentthesecontrastive tions have been explored to interpret model deci-
explanationsaresuperiortotheirnon-contrastive sions(seeStepinetal.(2021)forabroadsurvey),
counterpartsfromvariousperspectives: they are relatively new to NLP and have not yet
beenstudiedtoexplainlanguagemodels.
• RQ1: Are contrastive explanations better at
Recently, Jacovi etal. (2021)produce counter-
identifyingevidencethatwebelieve,a-priori,
factual explanations for text classification mod-
tobeusefultocaptureavarietyoflinguistic
elsbyerasingcertainfeaturesfromtheinputand
phenomena(§4)?
projecting the input representation to the “con-
trastive space” that minimally separates two de-
• RQ2: Do contrastive explanations allow hu-
cisionclasses. Then,theycomparemodelprobabil-
man observers to better simulate language
itiesbeforeandaftertheintervention.
modelbehavior(§5)?
We, on the other hand, propose contrastive ex-
• RQ3: Are different types of evidence neces- planationsforlanguagemodeling,whereboththe
sarytodisambiguatedifferenttypesofwords, number of input factors and the output space are
and does the evidence needed reflect (or un- muchlarger. Whilewealsouseacounterfactualap-
cover)coherentlinguisticconcepts(§6)? proachwitherasure(§3.3),counterfactualmethods
maybecomeintractableoverlonginputsequences
2 Background
andalargefoilspace. We,therefore,alsopropose
contrastiveexplanationsusinggradient-basedmeth-
2.1 ModelExplanation
ods(§3.1,§3.2)thatmeasurethesaliencyofinput
Ourworkfocusesonmodelexplanationsthatcom-
tokensforacontrastivemode
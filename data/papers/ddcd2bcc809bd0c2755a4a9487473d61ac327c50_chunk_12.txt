ci-003 67 67 71
GPT-3.5 - 70 73
GPT-4 - 70 73
Table3: AccuracyoftherecentGPTmodelsonaran-
domsampleof400instancesfromSocialIQa(Siqa)and
ToMi. Theprobingmethodaffectstheperformance.For
Figure1: Accuraciesoftop-performingmodelsoneach
example,inSiqathereisa7%differenceintheaccuracy
of the ToM tasks, compared to a most frequent class
ofGPT-4betweenMC-probingandCoT-probing.
(MFC) baseline. For several datasets, the best model
achievesperformancecomparabletotheMFCbaseline,
suggestingverylimitedToMability.
Our findings demonstrate that while some
LLMs achieve near perfect accuracies on some
task (see Appendix 8.2 for an example for each datasets (e.g., TriangleCOPA with 96% accuracy
variation). Again, the examples were created by byflan-t5-xxl),othersdatasetsremainchalleng-
oneauthorandverifiedbyanother. ingforLLMswithconsiderablylowerperformance.
For instance, the best performing LLM on the
4 Experiments&Results FauxPasEAIdatasetsisinferiortoasimplemost-
frequent-class baseline, indicating the difficulty
Toinvestigate theToM abilities of LLMs, we de-
levelofthesedatasets.
signed experiments that explore various aspects.
Notably,thebestLLMsperformanceseemscor-
The first experiment presents a meta-evaluation
related to the dataset’s age (i.e., the older the
of 15 LLMs evaluated on multiple ToM-related
dataset, the better the performance). This trend
datasetsinazero-shotmanner(§4.1). Wethenin-
could be attributed to the fact that the increasing
vestigatetowhatextentLLMsaresensitivetothe
sophistication of LLMs is driving the creation of
probingmethod(§4.2).
morechallengingdatasets,promptingresearchers
LLMs We examine the performance of 15 tosetahigherbar. AnotherpossibilityisthatLLMs
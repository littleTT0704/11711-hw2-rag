PLM generation are shown in Table 12.
Following the recommendation of the authors,
Hyperparameter Assignment
we performed a hyperparameter search for step
embeddingsize 1280 size over the values t0.02,0.06,0.10,0.20,0.40u,
numberofsteps 10epochs
and for number of iterations over the values
learningrate 1e-4
batchsize 64 t10,20,40,60u,overasmallsampleoftwentynon-
toxic prompts. We picked step size 0.20 and 10
Table7:Hyperparametersfortrainingtheattributeclas- iterations, for the best tradeoff between toxicity
sifiersusedforPPLM.
reductionandoutputfluency. Duetotheextreme
computational expense of this method, we were
not able to repeat the hyperparameter search for
GeDi For toxicity and sentiment steering, we
sentimentprompts.
downloadtheclass-conditionedlanguagemodels
HyperparametersforGeDigenerationareshown
(basedonGPT-2Medium)madeavailablebythe
inTable13.
originalauthors. Asanexperiment,wealsoalign
the finetuning data for the sentiment GeDis and
Hyperparameter Assignment
the (anti-)experts used in DEXPERTS by finetun-
numberofsamples 25
inganewclass-conditionedLMonSST-5data(as top-p(sampling) 0.9
opposed to IMDB used by in GeDi). We found temperature 1
maxlength 20
slightly lower performance on sentiment control
(„1-2%)acrossthesettings,andthereforeusethe
Table11:Hyperparametersforgenerationwithallmod-
originalclass-conditionedLMs.
els.
A.3 DatasetDetails
We compare the runtime for each controllable
Details of datasets used for further pretraining in generationmethodusedin§3inTable14,allona
theDAPTbaselinesaregiveninTable8,andthose single NVIDIA Quadro 6000 GPU.. We see that
Hyperparameter Assignment
temperature 1
numberofiterations 10
stepsize 0.20
gamma 1
GM-scale 0.9
KL-scale 0.01
repetition
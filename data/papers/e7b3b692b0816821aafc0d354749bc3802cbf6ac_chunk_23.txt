ford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision, 2021. URL
https://arxiv.org/abs/2103.00020.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.
org/abs/1908.10084.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms,2017. URLhttps://arxiv.org/abs/1707.06347.
Linda Smith and Chen Yu. Infants rapidly learn word-referent mappings via cross-situational
statistics. Cognition, 106(3):1558–1568, 2008. ISSN 0010-0277. doi: https://doi.org/
10.1016/j.cognition.2007.06.010. URLhttps://www.sciencedirect.com/science/
article/pii/S0010027707001795.
Michael Tomasello. Constructing a Language: A Usage-Based Theory of Language Acquisition,
volume1. HarvardUniversityPress,2005.
Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, and Gal Chechik. Context-
awarecaptionsfromcontext-agnosticsupervision. CoRR,abs/1701.02870, 2017. URLhttp:
//arxiv.org/abs/1701.02870.
ChenYuandDanaH.Ballard. Aunifiedmodelofearlywordlearning: Integratingstatisticaland
social cues. Neurocomputing, 70(13):2149–2165, 2007. ISSN 0925-2312. doi: https://doi.org/
10.1016/j.
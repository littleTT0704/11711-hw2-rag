, we set r to be bottlenecked, i.e. r < 2020b), a simple down- and up-projection of in-
d, so the representation transformation network put representations. Nevertheless, beyond net-
firstcompressestheinputrepresentationandthen workarchitecture,thegoalandtrainingprocedure
projects back onto the original dimension of the of the two approaches are significantly different.
inputrepresentation. Adapters are typically trained to encode task or
AsshowninFigure2,byassumingthatthebase language-specific information by fixing the rest
modelhasN layers,asourceexample(x,y ) ∈ of the model and updating the parameters of the
s s
D passesthroughthefirstilayers, thenthrough adaptersonly. Adaptersallowtrainingparameter-
s
therepresentationtransformationnetwork,finally efficientmodelsthatcouldbeflexiblyadaptedto
throughthelastN−ilayersofthebasemodel. We multiple languages and tasks. While in our pro-
denotethefinallogitsofthisbatchasf(x ;θ,φ), posed method, we use the representation trans-
s
Algorithm1TrainingprocedureforMetaXL
Input: InputdatafromthetargetlanguageD andthesourcelanguageD
t s
1: Initialize base model parameters θ with pretrained XLM-R weights, initialize parameters of the
representationtransformationnetworkφrandomly
2: whilenotconvergeddo
3: Sampleasourcebatch(x s,y s)fromD s andatargetbatch(x t,y t)fromD t;
4: Updateθ: θ(t+1) = θ(t)−α∇ θL(x s;θ(t),φ(t))
5: Updateφ: φ(t+1) = φ(t)−β∇ φL(x t;θ(t)−α∇ θL(x s;θ(t),φ(t)))
6: endwhile
fernetworkattrainingtimetoadjustthetraining learningrate. Notethattheresultingθ(cid:48) isineffect
dynamicstomaximallyimprovetest-timeper
length
MAgr Wescrapeallquotesfrompostsusingthe
Table3: Hyperparametersusedtofinetunetheexpert TumblrAPI,followingtheAPILicenseAgreement
model athttps://www.tumblr.com/docs/en/api_agreement,
which grants the right to use, distribute, display,
Anti-Expert Wefinetunetheanti-expertwiththe andmodifypostedTumblrcontent.
hyperparameters listed in Table 4, using a single
SBF Thereisnolicenseforthisdataset.
NVIDIARTX6000GPU.Weselectthebestcheck-
point,basedonthelowestevaluationloss,which DynaHate Thereisnolicenseforthisdataset.
isatstep38,000. Thetotaltrainingtimeis2hours,
for2GPUhoursofusage. B.2 GenerationDetails
GenerationsareperformedusingasingleNVIDIA
Hyperparameter Assignment
RTX6000GPUforalldatasetsandmethods.
model BART-base
numberofgpus 1 MARCO
effectivebatchsize 32
totalsteps 50,000
MaskingHyperparameters Wesetamasking
stepsperevaluation 1000
learningrateoptimizer AdamW
thresholdofτ = 1.2forallexperiments.
AdamWinitiallearningrate 1e-06
AdamWepsilon 1e-06 GenerationHyperparameters Wegenerate
learningrateschedule linearwithnowarmup
withgreedysearchforalldatasetswithamaxgen-
weightdecay 0.0
maxsequencelength 180 erationlengthof128.
maxgenerationlength 230
paddingsequences tomaxseqlength MAgr We perform a search jointly over dif-
ferenthyperparametervaluesonthedevelopment
Table 4: Hyperparameters used to finetune the anti-
set. We choose the hyperparameter combination
expertmodel
thatperformsbestonautomaticmetrics,shownin
Table5,andusethistogenerateonthetestset.
B ExperimentalDetails
Hyperparameter Tested Assignment
B.1 Datasets
repetitionpenalty [1.0,1.2,1.5] 1.0
Foreachdataset,wemanuallysampleandreview α 1 [
max() 𝐷
𝐷 𝐷
ℎ 𝑠𝑚 𝑊 𝑠𝑚 𝑉 + ℎ 𝑑𝑠
Feed Forward FFN
Network 𝑊 𝑑𝑠 𝑁 𝑑𝑠 mask-to-k()
Layer Norm ATT In 𝑘NN-LM:
top-𝑘()
Multi Headed
Attention In 𝑘NN-LM:
𝑁 : up to 5000𝑉
𝑑𝑠
Figure1: AnillustrationofthegeneralizedformulationofkNN-LMinEquation5.
notsimplybenefitingfromaccesstomoredata. Intriguedbythis,weaskquestionslike,couldkNN-LMbe
improvingbecauseofcapacityissuesintheparametricbaseLM?Inthispaper,wesetouttounderstandwhy
kNN-LMsworkeveninthissetting.
Inthefollowingsections,wefirstelucidateconnectionsbetweentheaddedkNNcomponentandthestandard
LMcomponent. Specifically,wenotethatworddistributionsfromthetwocomponentsarebothcalculated
usingasoftmaxfunction,basedonthesimilarityofthecurrentcontextembeddingwithasetofembeddings
thatcorrespondstodifferentnextwords. Withthisintuition,weformalizeandgeneralizethenon-parametric
distributioncalculationwiththesoftmaxlayerandwordembeddinglayerusedinparametricLMs. Wethen
showthatthisgeneralizedformexposesavarietyofdesignchoices,e.g.,thenumberofcontextembeddings
inthedatastore,theinputrepresentationusedinsoftmaxlayer,differentsimilarityfunctions,aswellasthe
approximationandsparsificationimplementationsinthekNNsearch. Thisprovidesageneralframeworkfor
analyzingkNN-LMandsimilarmodelsandallowsustoperformablationstudiesthattesttheimportanceof
variousdesigndecisions.
WeproceedtoproposemultiplehypothesesforwhykNN-LMworks, whicharetestablebyadjustingthe
variousparameter
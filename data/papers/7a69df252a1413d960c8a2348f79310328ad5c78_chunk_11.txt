 4.75±.07 43.20(cid:61) 30.49±.57 22.84(cid:61)
Obj-GAN [18] - - 30.29±.33 -
OP-GAN [11] - - 27.88±.12 23.29(cid:61)
CPGAN [20] - - 52.73±.6149.92(cid:61)
CAGAN SE (ours) 4.78±.06 42.98 32.60±.75 19.88
CAGAN L+SE (ours) 4.96±.05 61.06 33.89±.69 27.40
feature repetitions (see Figure 4) and has a major negative impact on the FID
throughout training (see Figure 3). This behaviour is similar to [20] on the
COCOdatasetanddemonstratesthatasinglescorecanbemisleadingandthus
the importance of reporting both scores.
In summary, according to the experimental results, our proposed CAGAN
achieved state-of-the-art results on both the CUB dataset and COCO dataset
basedontheFIDmetric.Moreover,weobtainedstate-of-the-artISontheCUB
dataset and quite a comparative IS on the COCO dataset. All these results
indicate how our CAGAN model is effective for the text-to-image generation
task.
Qualitative Results : Figure 4 shows images generated by our models and by
several other models [11] [46] [42] on the CUB dataset and on the more chal-
lenging COCO dataset. On the CUB dataset, our model utilising SE attention
generates images of vivid details (see 1st, 4th, 5th, and 6th row), demonstrating
a strong text-image correlation (see 3th, 4th, and 5th row), avoiding feature rep-
etitions (see double beak, DM-GAN 6th row), and managing the difficult scene
(see 7th row) best. Cut-off artefacts occur in all presented models.
Ourmodelincorporatinglocalself-attentionfailstoproducerealisticlooking
image,despitescoringhigherISsthantheAttnGANandourmodelutilisingSE

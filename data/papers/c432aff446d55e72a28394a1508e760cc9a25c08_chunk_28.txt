us. InInternationalconferenceonmachinelearning,pages1302–1310.PMLR,2017.
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest neighbor
machinetranslation. arXivpreprintarXiv:2010.00710,2020a.
12
UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis. Generalizationthrough
Memorization: NearestNeighborLanguageModels. InInternationalConferenceonLearningRepresenta-
tions(ICLR),2020b.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,Luke
Zettlemoyer,andVeselinStoyanov. Roberta:Arobustlyoptimizedbertpretrainingapproach. arXivpreprint
arXiv:1907.11692,2019.
ClaraMeister,ElizabethSalesky,andRyanCotterell. Generalizedentropyregularizationor: There’snothing
specialaboutlabelsmoothing. arXivpreprintarXiv:2005.00820,2020a.
ClaraMeister,TimVieira,andRyanCotterell. Best-firstbeamsearch. TransactionsoftheAssociationfor
ComputationalLinguistics,8:795–809,2020b.
StephenMerity,CaimingXiong,JamesBradbury,andRichardSocher. Pointersentinelmixturemodels. arXiv
preprintarXiv:1609.07843,2016.
StephenMerity,NitishShirishKeskar,andRichardSocher. RegularizingandoptimizingLSTMlanguage
models. InProceedingsofICLR,2018.
Hermann Ney, Ute Essen, and Reinhard Kneser. On structuring probabilistic dependences in stochastic
languagemodelling. ComputerSpeech&Language,8(1):1–38,1994.
GabrielPereyra,GeorgeTucker,JanChorowski,ŁukaszKaiser,andGeoffreyHinton. Regularizing
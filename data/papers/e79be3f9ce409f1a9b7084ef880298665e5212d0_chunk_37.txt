alignment.
quency(idf)by:
Toverifythis,wecomparethelaterstagealignmentacross
threedifferentsettings: 1)merelyapplyinglaterstagecon-
|D|
idf(token)=log (7) trastive loss; 2) combine early state and later stage con-
1+|{d∈D:token∈d}|
11
trastivelossesand3)usingcascadesamplingforlaterstage E.ComparingmodelsizeandFLOPs
contrastiveloss. WereporttheresultsonYouCook2inTa- Finally,weattempttocomparethemodelsizesandcom-
ble12.Here,notethatweonlyusethelaterstagealignment putational costs for different methods. Unfortunately, all
scoresforevaluatingtheperformance. Aswecansee,com- previousmethodsdidnotreportFLOPsandonlyMMT[14]
biningearlystageandlaterstagetogetherslightlyimproves discussed#params. However,theresultsinTable13imply
theperformance. Thisisprobablybecauseearlystagecon- that bigger model can usually achieve better performance.
trastive loss helps to learn a better video and language en- Therefore, it is necessary to have a comparison of model
coder,fromwhichthemulti-modalmoduletakesbetterrep- size and computational cost between our model and those
resentationsforcross-modalfusion. Afterapplyingthecas- fromothermethods. Forothermethodswhichdonotreport
cade sampling for the later stage contrastive loss, the per- thenumbers,weestimatethembasedonthedescriptionsin
formanceisfurtherimproved. Sinceourcascadesampling the original paper. Table 14 summarizes the comparisons
strategy can send more difficult samples to the later stage, and also reports the #params and FLOPs (all underlined
the cross-modal fusion layers can learn more discrimina- numbers are estimated based on the descriptions in origi-
tive representations for video-text alignment. These re- nal papers). As shown, our largest model has comparable
sultsvalidatethatthehardnegativeminingthroughcascade sizeandFLOPstoothers.
samplingindeedhelpstoimprovethelater-stagetext-video

or source language. We compute accuracy using
2012to2016(Agirreetal.,2012,2013,2014,2015,
cosinesimilarityinbothdirectionsforall112lan-
2016) as was done initially for sentence embed-
guages (19 are unseen in the training data) and
dingsin(Wietingetal.,2016). Asourtestset,we
averagethisscoreforalllanguages.
report the average Pearson’s r over each year of
The goal of the BUCC task is to find the gold
the STS tasks from 2012-2016 as is convention
alignedparallelsentencesgiventwocorpora(one
in the top part of Table 9. However, some recent
being very large) in two distinct languages. Lan-
work,likeReimersandGurevych(2019)computed
guages are aligned with English and consist of
Spearman’sρoverconcatenateddatasetsforeach
German (de), French (fr), Russian (ru), and Chi-
yearoftheSTScompetition. Tobeconsistentwith
nese (zh). Following Schwenk (2018), we eval-
theseworks,wealsoincludeevaluationsusingthis
uate on the publicly available BUCC data. This
approachinthebottompartofTable9. Oneother
involvesscoringallpairsbetweenthesourcetarget
differencebetweenthesetwowaysofcalculating
sentences and finding the optimal threshold that
the results is the inclusion of the SMT dataset of
separates the data. Using the threshold, we can
the 2013 task. When computing the results us-
computetheprecision,recall,andF ofthealign-
1
ingPearson’sr,thisdatasetisincluded,butwhen
ments. WereportF ×100inourresults.
1
computingtheresultsusingSpearman’sρ,itisnot
Wecomparetwodifferentapproachesforfind-
included.
ing the sentence alignments. In the first, BUCC
Forcross-lingualsemanticsimilarityandseman-
(cosine),wecomputethecosinesimilaritybetween
tic similarity in non-English languages,
al.(2018);Holtzmanetal.(2019);Wellecketal.(2020b)).
etal.2020b;Lietal.2020),andbeamsearchoptimization
ThetaskconsistsofdecodingacontinuationY^ = F((cid:18);X)
(WisemanandRush2016),incorporatesequence-levelscores
givenaprefixX =(x ;:::;x ).
withoutreferencetoanexternalrewardfunction. 1 k
Inthistask,neurallanguagemodelssuchasGPT-2(Rad-
fordetal.2018)exhibitdegeneraterepetition(Holtzmanetal.
Drawbacks ofMLE inNLP. Severalstudies investigate
2019)andnon-terminationwithgreedydecoding;(Welleck
drawbacks of maximum likelihood training, including la-
etal.2020a)conjecturedthatthelackofadecodingalgorithm
belbias(Lafferty,McCallum,andPereira2001),exposure
inmaximum-likelihoodtrainingisthecauseofthelatter.We
bias(DaumeÂ´,Langford,andMarcu2009;Ross,Gordon,and
evaluate whether MGS, which uses a decoding algorithm
Bagnell2011;Bengioetal.2015),andlossmismatch(Lee duringtraining,canalleviatetheseissues.3
etal.2020).Neuralmachinetranslationmodelstrainedwith
maximumlikelihoodhavebeenshowntoexhibitdecreased
Experimental setup. We use the Wikitext-103 dataset
performancewithincreasedbeamsize(KoehnandKnowles
(Merity et al. 2016), a large-scale collection of Wikipedia
2017; Ott et al. 2018) and a bias towards short sequences
articles containing over 100 million words that has been
(SountsovandSarawagi2016;StahlbergandByrne2019),
used for language modeling (Baevski and Auli 2019) and
whichhavebeenattributedtolabelbiasduetolocalnormal-
textcompletion(Wellecketal.2020b).Wemodelindividual
ization(MurrayandChiang2018).
sequences by splitting the corpus according
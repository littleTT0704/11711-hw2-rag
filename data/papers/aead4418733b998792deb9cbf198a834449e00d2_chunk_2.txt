sthedifficultyofgener-
thatareoutofreachfortraditionalsymbolicsolvers,theneu-
alizingwellwiththepredominantmodelingandlearningap-
ralsequenceintegratorshowsdeficiencesinrobustness(top)
proach,andtheimportanceofevaluatingbeyondthetestset,
acrossdifferentaspectsofgeneralization. andcompositionality(middle),andfailsonadversarialprob-
lemsdiscoveredbySAGGA(bottom).
1 Introduction
Despitetheirsuccess,recentstudiesrevealundesirableprop-
– finding the integral of a mathematical function – specifi-
erties of conventional neural sequence models, such as as-
callyrequirestheseformsofgeneralization,asitinvolvesan
signing high-probabilities to unrealistic sequences (Holtz-
underlyingstructurethatextendsbeyondthisfixedtraining
manetal.2020;Wellecketal.2020),susceptibilitytoadver- R
distribution.Forinstance,therule k = kx+C appliesto
sarial attacks (Wallace et al. 2019), and limited generaliza- R R R
allconstantsk,andthesumrule f + f = (f +f )
tiononsymbolictasks(Saxtonetal.2019;Nogueira,Jiang, 1 2 1 2
meansthatintegratingtwofunctionscorrectlyshouldimply
and Li 2021), even with very large models and datasets
integratingtheirsumcorrectly.Symbolicintegrationalsoof-
(Henighan et al. 2020). Despite these drawbacks, Lample
fers a structured problem domain and a verifier for evalu-
and Charton (2019) recently demonstrated that a standard
ating whether a proposed solution is correct, making it an
sequence-to-sequence model, which we call a neural se-
effectivesettingforevaluatinggeneralization.Astheneural
quenceintegrator,performssurprisinglywellatsymbolicin-
sequenceintegratorreliesonacommonrecipe–alarge-scale
tegration,solvingproblemsthatarebeyondtraditionalsym-
transformertrainedtomaximizethelikelihoodofatraining
b
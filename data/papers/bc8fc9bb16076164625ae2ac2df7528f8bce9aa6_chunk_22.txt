-GraphandGCCinmostdatasets,showingthatIGSD fromself-supperviseddataforsupervisedtasks.
canlearnexpressivegraph-levelrepresentationsfordownstream
classifiers.Besides,ourmodelstillachievecompetitiveresultsin
datasetslikeIMDB-MandNCI1withrandomdroppingaugmen-
tation,whichdemonstratestherobustnessofIGSDwithdifferent
dataaugmentationstrategies. 5.3 AblationStudiesandAnalysis
Results on semi-supervised graph classification. We fur- Effectsofself-training.Wefirstinvestigatetheeffectsofself-
therapplyourmodeltosemi-supervisedgraphclassificationtasks trainingforourmodelperformanceinTable4.Resultsshowthat
withresultsdemonstratedinTable4,wherewesetùë§ andùë§‚Ä≤in self-trainingcanimprovetheGINbaselineandourmodelswith
Eq. (6) to be 1 and 0 as Ours (Self-supp) while 0 and 1 as Ours self-suppervisedloss(self-supp)orsupervisedcontrastiveloss(Sup-
(SupCon).Inthissetting,ourmodelperformsbetterthanMean Con).Theimprovementisevenmoresignificantcombinedwith
TeachersandInfoGraph*.Boththeself-suppervisedlossandsuper- supervisedcontrastivelosssincehigh-qualitypseudo-labelspro-
visedcontrastivelossprovideextraperformancegaincompared vide additional information of graph categories. Moreover, our
withGINusingsuperviseddataonly.Besides,bothoftheirperfor- self-training algorithm consistently outperforms the traditional
mancecanbeimprovedsignificantlycombinedusingself-training self-trainingbaseline,whichfurthervalidatesthesuperiorityofour
especiallywithsupervisedcontrastiveloss.Itmakesempiricalsense model.
oitaR
rorrE
IterativeGraphSelf-distillation Ljubljana‚Äô21,April19‚Äì23,2021,Ljubljana,Slovenia
54
Ours InfoGraph* 75.00
52 74.75
74.50
50
74.25
48
10 20 30 40 50 60
74.00
128 512
 the most popular topics
may be less useful because the documents in the seed corpus already have good
coverage for those topics. We further found that the threshold that is used to remove
text nuggets with low relevance estimates in the merging phase has little effect on
the length of the generated documents when using the crawl, which indicates that
there are large amounts of relevant text even for seeds that are ranked relatively
low. On the other hand, the top 100 web search results usually contain less relevant
information, and pseudo-documents generated from this data for the same topics are
on average shorter if the same relevance threshold is applied.
7.1.2 Experiments and Analysis
We evaluated the impact of source expansion with content extracted from the Clue-
Web09 crawl on QA performance, and compared this method to the approach that is
based on web search results. The experimental setup was described in Section 6.3.1
for search experiments and in Section 6.4.1 for end-to-end evaluations. Watson was
used as a framework for all experiments, and performance was measured on both
Jeopardy! and TREC datasets. Again, passages and document titles were retrieved
114 CHAPTER 7. UNSTRUCTURED SOURCES
from the original sources and the expanded corpora using Indri and Lucene. The final
answersreturnedbyWatsonwerejudgedmanuallyandtheanswerkeyswereextended
with additional correct answers. Search results were judged automatically using these
extended answer patterns because a manual evaluation of all results was not feasible.
We report search recall, candidate recall and QA accuracy on all datasets, and we also
use precision@70 on regular Jeopardy! data as a measure of Watsonâ€™s performance if
it only answers when it has high confidence.
Table 7.2 compares the search recall of Watson on Jeopardy! and TREC questions
when only using Wikipedia as an information source to expansions generated with
related content from web search results, the local web crawl, or both. It can be seen
that the two SE approaches have a similar impact on QA search performance, and
that the combination of these methods is most effective. This is not an obvious result
because QA performance does not necessarily improve if additional source content is
used. For instance, the effectiveness of a QA system can decline if a large, raw web
crawl is added to its sources (
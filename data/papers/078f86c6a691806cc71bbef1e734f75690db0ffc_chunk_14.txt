 The next
distributionduringthetrainingprocess. Formally,learning two terms, i.e, log(cid:16) p′ s(y tk)(cid:17) and log(cid:16) p′ s(y tk)(cid:17), denoted as
the adaptation framework of Eqn. (1) under the ideal data
ps(y tk) ps(y tk)
theL, imposethebehaviorofthemodelwithrespect
Class
distributioncanbeformulatedasinEqn. (6).
totheclassdistribution. Inparticular,theseconstraintsaim
(cid:34)
argm θin E xs∼ps(ys),yˆs∼ps(yˆs)Ls(ys,yˆs) pp s′ s (( yy ss )) pp s′ s (( yy ˆˆ ss )) t so hor ue lg dul ba eri hz ae veth fe aip rr lyed bic et ti won es enof clc al sa ss es ses ws io thth rea st pt eh ce
t
m tood the el
(6)
+E xt∼pt(xt)Lt(yt) pp t′ t (( yy tt ))(cid:35) c til oa nss,td hi est mrib ou dt ei lo in s. eU xpn ed ce tr et dhe toid ee qa ul ad lla yta trd eais tt pri rb eu dt ii co tn ioa ns ssu om fap l-
l
Figure3. TheProposedFairnessFramework. Thepredictionsoftheinputssampledfromthesourceortargetdomainsarepenalized
bythesupervisedlossL ortheself-supervisedlossL,respectively. Then,thepredictionsareimposedbythefairnessclassbalanceloss
s t
L followedbytheConditionalConstraintLossL computedviaaConditionalStructureNetwork(Bestviewincolor).
Class Cond
classes. Thus,toachievethedesiredgoal,thedistributions demand for ideal data distribution is relaxed. Fig. 3 illus-

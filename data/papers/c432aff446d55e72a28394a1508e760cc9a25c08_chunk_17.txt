 same vector,
renderingover-parameterizationuseless.
Besidessimplyincreasingthenumberofembeddingvectorsequallyforeachwordtype,wealsopropose
otheralternativestoincreasesoftmaxcapacity. First,wehypothesizethatdifferentwordtypeshavedifferent
difficultiesforthelanguagemodeltopredict. Forthosewordsthatappearveryfrequently,theymayappear
in many different contexts. As a result, instead of adding an equal number of additional embeddings to
eachwordtype,weproposetoadaptivelyincreasethenumberofembeddingsforwordtypesbasedonword
frequency,ortotaltraininglossfortheword. Second,wetrytobreakthesoftmaxbottleneckwithaMixture
of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to
producemorelinearlyindependentprobabilitydistributionsofwordsgivendifferentcontexts. Last,opposite
totrainingthewordembeddingsofincreasedsize,wealsoconsiderwaystocompressthedatastoredowntoa
similar-sizedembeddingmatrixforsoftmaxcomputationbyclusteringthewholedatastoreandallowingfor
furtherfinetuningoftheembeddingmatrixconsistingofclustercentroids. However,noneofthesealternative
methods provided additional benefits over the simple multi-embedding approach. More details on these
attemptscanbefoundinAppendixC.
7
ytixelpreP
detalopretnI
att ffn
22
21
20
19
2 4 6 8
Number of Trained Embeddings (nV)
Figure 3: The number of embeddings per word type (nV total embeddings in W ) versus interpolated
ds
perplexity. Thehorizontallineatthetop(black)representstheperplexityofthebaseLM.Thehorizontalline
atthebottom(red)representstheinterpolatedperplexityusingafulldatastorewithkNN-LM.
5 ApproximatekNNSearch&SoftmaxTemperature
5.1 ComparingApproximatekNNSearch
TocalculateP ofthen
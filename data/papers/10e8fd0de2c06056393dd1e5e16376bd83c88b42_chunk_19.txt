 see
we vary the answer distribution by changing the
in Table 3. While this demonstrates that answer
samplingdistributionoverplausibleanswerspans.
First, we extract and annotate coarse grain entity 4https://spacy.io/
priorsinfluencereaderperformance,inanunsuper- Inordertosampleanswersforwhichweshould
visedsetupwewillnothavethistruedistribution. curateStandardandClozeQApairs,wefollowthe
Therefore,weadoptthesecondbesttechnique,i.e., previoussubsectionandsampleanswerspansuni-
uniformsamplingfromacrosstheentitytypecate- formlybasedonanentitytypedistributionfromthe
goriesforotherexperimentsinthepaper(Table5). targetcorpus. Wethenqueryourcombinedindex
tocreateadatasetcontainingclozestylequestions
Augmentations Acc@100 aligned with relevant documents. We use these
Random 45.35 samesampledanswerstogeneratestandardQGen
Uniform 50.02
QApairsaswell.
Mostfrequent 39.33
We combine this augmented data with our ini-
BioASQtrainanswers 47.48
tial source domain data to train a DPR retriever
Table2: Answerdistribution: Retriverperformanceon (Table4)andaFiDreader(Table5). Weobserve
BioASQ similaraverageperformanceacrossbothinterven-
tiontypesinretrieverandreadermodels. However,
To understand the impact of the pre-trainining
cloze QA pairs are computationally much more
vsfine-tuningcorpus,wealsocomparetheperfor- efficienttogenerateastheydonotrequireanyad-
mance of the FiD reader initialized from T5 pre-
ditionalquestiongenerationmodels.
trainedoncommon-crawldataset(C4)comparedto
onethatwaspre-trainedonpubmedarticles. After
Baseline ClozeQA StandardQGen
pretraining,bothmodelsarethenfine-tunedonour
CliCR 23.87 24.88 23.99
sourcedomaindata. Inthiscase,weobservethat Bio
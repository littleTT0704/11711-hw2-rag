 scaling of training data and number of model
suchasreal-timenaturallanguagegenerationand
parameters (Kaplan et al., 2020; Alabdulmohsin
automatedspeechrecognition(Reddietal.,2020).
etal.,2022;Tayetal.,2021,2022). However,the
These concerns have motivated research in de-
accompanyingincreasesincomputationraisecon-
signingmoreefficientneuralnetworkmodelarchi-
cerns as to the efficiency of these systems due to
tectures and faster hardware accelerators. In the
associatedenvironmentalcostsofdevelopmentand
past five years alone, the number of papers that
deployment(Schwartzetal.,2020;Strubelletal.,
mentionthetermsefficientorefficiencyintopma-
2019).
chinelearningvenueshasgrownbyover2.5xand
Inparticular,efficiencyisespeciallyimportant
evenmoresoatvenuesinnaturallanguageprocess-
in inference settings where models are used re-
ing,increasingby8.3xinthesamespan.1 Thishas
peatedlyandatscale,incontrasttotrainingwhich
spurredinnovationsinthedesignofefficientneu-
posesasingleupfrontcomputationalcost. Forex-
ral network architectures for language aiming to
ample,Metareportsthatinferenceworkloadsmake
reducethenumberoftrainablemodelparameters,
up 70% of their AI power consumption, with the
floating point or multiply-accumulate operations
remaining 30% due to training and development
(MACs)(Iandolaetal.,2020;Daietal.,2020;Sun
(Wuetal.,2022),whileGoogleattributes60%of
their ML energy consumption to inference (Pat-
1BasedonpublicationsatML(ICLR,ICML,NeurIPS)
terson et al., 2022). Inference is also estimated andNLPconferences(ACL,EMNLP)between2017and2022
3202
ceD
22
]GL.sc[
2v71160.2032:viXra
etal.,2020). Overthesameperiod,GPUhardware lutionalneuralnewtorkmodelsineagerexecution
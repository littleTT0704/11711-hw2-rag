 The performance
0.6 54.30 81.27 0.1 78.18
of a model is measured by the area under the ROC curve
0.8 85.40 81.24 0.2 86.10
(AUC). Since it is important for a face recognition system 1.0 87.70 87.19 0.4 83.34
to avoid false positives, we use AUC-x [87] as the metric, 1.2 85.00 87.15 0.6 85.50
which integrates up to false positive rate of x (x âˆˆ [0,1]). 1.4 78.52 83.67 0.8 81.75
Wefindthatx=0.0005achievesthebesttrade-offbetween
stability and effectiveness. Since VGGFace2 is a relatively
well, due to the increased difficulty in optimization. In our
small training set (8.6K subjects [76]), we use the SFNet-20 experiments, m = 1.2 achieves the best trade-off between
modelasthebackboneinthissection.
featurediscriminativenessandoptimizationdifficulty,lead-
ingtothebestperformanceforalltypesofmargins.
6.2.1 NoFeatureNormalization
We start by exploring SphereFace and SphereFace-R with- 6.2.2 HardFeatureNormalization
out feature normalization. Since there is only one effective HFNintroducesanadditionalhyperparameterstotheloss
hyperparameter (i.e., the margin m), it is easy to find the function, which controls the norm of the deep features. To
optimal setting. We compare SphereFace, SphereFace-R v1 show how m and s affect the performance, we perform a
and SphereFace-R v2, and they represent different types of grid search by varying these two hyperparameters for all
margins as shown in Table 1. Note that SphereFace with threetypesofmarginsincludingSphereFace,SphereFace-R
NFN is the same as the original SphereFace [2] (with addi- v1andSphereFace-Rv2.TheresultsaregiveninTable4.
tional CGD). Our methods are equivalent to the standard We have several observations. First, there usually exists
softmax cross-entropy loss when m = 1.0.
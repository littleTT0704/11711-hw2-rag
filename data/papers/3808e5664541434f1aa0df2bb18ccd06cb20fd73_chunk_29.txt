modelincreaseinsteadexceptforthe OI
Clark et al. (2019) add an entropy penalty (H) to category and Table 9’s results behave in-line with
theloss: Table4.
R =αH(softmax{g(x )logb }) C Few-shot AAE-to-WAE Translation
i i
(cid:80) Note that we do not recommend the following
Where H(z) = − z logz is the entropy and
j j j
approach to build large scale parallel data for
αisahyperparameter.
dialects,asdiscussedunderethicalimplications
A.2 DataFiltering andlimitations(§6).
We use GPT-3 (Brown et al., 2020) to create
Forthedatafilteringmethods,wefirstfilterdatato
a few-shot AAE-to-WAE translation system, us-
50% of the original data as in Swayamdipta et al.
ing the following set of example translation pairs
(2020). Then we further downsample the dataset
drawnfromSpears(1998):
to 33% of the original data to control that each
training set has the same toxic ratio as the origi- AAE: Getyourtriflin’assoutofhere.
naltrainingset. Thisstepistoavoidconfounding WAE: Getyourtriflingselfoutofhere.
ourresultswithdifferenttoxicratioamongdiffer-
AAE: Isawhisassyesterday.
enttrainingsets.
WAE: Isawhimyesterday.
A.3 TrainingSettings AAE: Hisassisgonnagetfried.
For all the experiments, we fine-tune RoBERTa- WAE: Heisgonnagetfried
large(Liuetal.,2019)overthecorrespondingcor- AAE: Wassup,nigga?
puswithoneGTX2080Ti. Weusethedefaulthy- WAE: What’supbro?
perparametersasprovided in theHuggingFace
AAE: (cid:104)tweet(cid:105)
Transformerslibrary(Wolfetal.,2019),with
WAE:
twomajorchanges: weuse
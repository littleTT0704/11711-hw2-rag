Interpreting Language Models with Contrastive Explanations
KayoYin GrahamNeubig
∗
UniversityofCalifornia,Berkeley CarnegieMellonUniversity
kayoyin@berkeley.edu gneubig@cs.cmu.edu
Abstract Input:Canyoustopthedogfrom
Output:barking
Modelinterpretabilitymethodsareoftenused 1.Whydidthemodelpredict“barking”?
toexplainNLPmodeldecisionsontaskssuch Canyoustopthedogfrom
as text classification, where the output space 2.Whydidthemodelpredict“barking”insteadof“crying”?
isrelativelysmall. However, whenappliedto Canyoustopthedogfrom
language generation, where the output space 3.Whydidthemodelpredict“barking”insteadof“walking”?
often consists of tens of thousands of tokens, Canyoustopthedogfrom
these methods are unable to provide informa-
Table 1: Explanations for the GPT-2 prediction given
tiveexplanations. Languagemodelsmustcon-
theinput“Canyoustopthedogfrom_____". Inputto-
sider various features to predict a token, such
kensthataremeasuredtoraiseorlowertheprobability
asitspartofspeech,number,tense,orseman-
of“barking”areinredandbluerespectively,andthose
tics. Existingexplanationmethodsconflateev-
with little influence are in white. Non-contrastive ex-
idenceforallthesefeaturesintoasingleexpla-
planationssuchasgradient input(1)usuallyattribute
nation, which is less interpretable for human ×
thehighestsaliencytothetokenimmediatelypreceding
understanding.
the prediction. Contrastive explanations (2, 3) give a
To disentangle the different decisions in lan- morefine-grainedandinformativeexplanationonwhy
guage modeling, we focus on explaining lan- themodelpredictedonetokenoveranother.
guage models contrastively: we look for
salientinputtokensthatexplainwhythemodel
predicted one token instead of another. We
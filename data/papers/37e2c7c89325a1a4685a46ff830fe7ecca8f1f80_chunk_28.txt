ornewresearchtodevelopexpressiveinterpretable
componentsforneuralnetworksthatcanbedirectlytrainedwithouthuman-labeledexplanations.
However,itshouldbenotedthat“interpretability”isalooselydefinedconcept,andthereforecau-
tionshouldbeexercisedwhenmakingstatementsaboutthequalityofexplanationsbasedonlyon
simulability,especiallyifthesestatementsmighthavesocietalimpacts.
Weonlyexploredlearningattention-basedexplainers,butourmethodcanalsobeusedtooptimize
other types of explainability methods, including gradient-based ones, by introducing learnable
parametersintheirformulations. Anotherpromisingfutureresearchdirectionistoexploreusing
SMaTtolearnexplanationsotherthansaliencymaps.
Acknowledgments
ThisworkwassupportedbytheEuropeanResearchCouncil(ERCStGDeepSPIN758969),byEU’s
HorizonEuropeResearchandInnovationActions(UTTER,contract101070631),byP2020project
MAIA (LISBOA-01-0247- FEDER045909), and Fundação para a Ciência e Tecnologia through
projectPTDC/CCI-INF/4703/2021(PRELUNA)andcontractUIDB/50008/2020. Wearegratefulto
NunoSabino,ThalesBertaglia,HenricoBrum,andAntonioFarinhasfortheparticipationinhuman
evaluationexperiments.
References
JuliusAdebayo,JustinGilmer,MichaelMuelly,IanGoodfellow,MoritzHardt,andBeenKim. Sanity
checksforsaliencymaps. Advancesinneuralinformationprocessingsystems,31,2018.
SiddhantArora,DanishPruthi,NormanSadeh,WilliamCohen,ZacharyLipton,andGrahamNeubig.
Explain,edit,andunderstand: Rethinkinguserstudydesignforevaluatingmodelexplanations.
InThirty-SixthAAAIConferenceonArtificialIntelligence(AAAI),Vancouver,Canada,February
2022. URLhttps://arxiv.
 time, and label
foreach soundevent. Inbothcases, theaudiorepresentation isfrozen andusedas theinput
feature vector to a shallow downstream MLP classifier, with no fine-tuning. Fine-tuning
2. https://neuralaudio.ai/hear2021-rules.html
5
Turian et al.
improves downstream performance (Baevski et al. (2020); Shor et al. (2020)), but increases
training time. Crucially, the use of frozen embedings means that HEAR downstream evalu-
ation codecanbemaintained solely inPyTorch,regardless ofwhethertheembeddingmodel
was in TensorFlow or PyTorch.3
A timestamp-based task can be simplified to a frame-based sequence-labeling task of
the audio at regular intervals (Kelz et al., 2016), and we use a common postprocessing
step to compose predictions from multiple timesteps and extract discrete labeled events
with start and ends times (Mesaros et al., 2016). Framewise accuracy (the decomposed
multilabel prediction, computed at regular timesteps) does not always correlate well with
theperceptualquality of event-onset FMS (Hawthorne et al.,2018)becausethey ignore the
interplay between the frame representations and more sophisticated downstream inference
(Cheuk et al., 2021). See Section B for details on the downstream training regime.
3.3. Evaluation tasks
The following are the HEAR evaluation tasks. For simplicity and reproducibility, we have
preprocessedeachrelevantdatasetstoallcommonlyusedsamplerates(16000, 22050, 32000,
44100), fixed the length of the audio clips, predefined training splits, and packaged each
datasetinaself-explanatory commonformatwithhuman-readablemetadata. Theyallhave
open licenses (some of which permit commercial use), with the exception of the GTZAN
corporawhich arewidely usedbutof unknownlicense status. We encourage thecommunity
tobenchmarkon HEARdatasets, even if theydonotfollow theHEARrulesor HEAR API.
Open tasks were released early in the NeurIPS 2021 shared challenge, to encourage
participation and to allow participants to debug and refine their submissions: Speech Com-

wanathan et al., 2023), and
tion3wedescribeourreferenceimplementation.
a model retriever using BM25. We evaluate on
three tasks covering both traditional NLP bench- PromptParser Astheprimaryinputtooursys-
marksandnovelapplicationsandfindthat,empiri- tem, users provide prompts similar to those used
cally,Prompt2Modelsometimesproducessmall forLLMs. Thesepromptscompriseaninstruction
modelsthatoutperformgpt-3.5-turbowhenus- and,optionally,afewdemonstrationsoftheantic-
ing the same prompt as input. On 2 of these 3 ipatedbehavior. While thisopen-endedinterface
tasks,weobserve>20pointimprovementsoverthe is convenient for users, end-to-end ML pipelines
gpt-3.5-turbo baseline, despite the final model maybenefitfromaPromptParser thatprocesses
producedbyPrompt2Modelbeingupto700times thisinput,suchassegmentingthepromptintoan
"Answer questions given context from a relevant Wikipedia article.
Examples: <QA pairs> "
flan-t5-base
Model Retrieved
Prompt + Inst Aru nc st wio en r questions [...] Retriever Model The Children's
Few Demonstrations Book Test
Examples
<QA pairs> Dataset Retrieved
Retriever Dataset
Input Prompt
Parser Spec
Dataset Generated Model
Training Set
Generator Trainer
ChrF++: 58.9 Generated Test
EM: 61.5 Performance Set
BERTScore: 94.0 Estimate
Evaluation
Interactive Trained
Demo Model
Figure2: ThePrompt2Modelarchitectureseekstoautomatethecoremachinelearningdevelopmentpipeline,
allowingustotrainasmallyetaccuratemodelfromjustaprompt.
instructionandindividualdemonstrationsortrans- couldinsteadcoverothermodelrepositoriessuch
latinginstructionsintoEnglish. asModelZoo(Koh,2020).
DatasetRetriever Givenaprompt,wefirsttryto Training Givenretrievedandgenerateddatasets
discoverexistingmanually-annotateddatathatcan and a pretrained
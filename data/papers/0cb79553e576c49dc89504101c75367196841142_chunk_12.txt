toxic meanings from the input, making
cottonpickingandslaveryiscorrectedbyMARCO,
itchallengingtogeneratedetoxifiedrewriteswith
which replaces “cotton” with “up”; in contrast,
highpreservationofonlythenon-toxiccontent;this
both baselines fail to revise the toxic content.6
mayriskminimizingmarginalizedgroups’speech
Sinceallthreemethodslearnedtoxicityusingthe
(Xu et al., 2021). Partially, this could be due to
sameovertlytoxicdatafromJigsaw,thefactthat
a lack of context incorporation (social, conversa-
MARCOdealsespeciallywellwithsubtletoxicity
tional,precedingsentences;Yerukolaetal.,2023);
highlights the advantages of using LMs to better
future work should consider adapting detoxifica-
modelandcapturetoxicitypatterns.
tion methods in context (Cheng et al., 2020; Roy
Finally, MARCO’s rewrites were more fluent etal.,2023).
than other methods, according to both automatic
MARCOalsorequiresfinetuningtwopretrained
metricsandhumanevaluation. MARCO’srewrites
LMs, which is not computationally insignificant
weredeemedasungrammaticaltheleastamountof
(Strubelletal.,2019;Schwartzetal.,2020). Future
thetime(9.3%),versus9.7%forCondBERTand
workcouldexploreusingsmallerLMstocontrol
11.7%forParaGeDi.
a larger model (Liu et al., 2021), or even more
lightweightapproaches.
5 Conclusion
Additionally,weacknowledgethatintheevalu-
ation, we expose Turkers to toxic content, which
WepresentMARCO,anovelmethodfortextdetox-
mightharmindividuals,especiallythosewithiden-
ification, which utilizes auto-encoder language
titiesthattheoffensivecontentappliesto(Roberts,
model experts in a mask and reconstruct process.
2017; Steiger et al., 2021). However, we pay a
Our method outperforms strong baselines in au-
fair
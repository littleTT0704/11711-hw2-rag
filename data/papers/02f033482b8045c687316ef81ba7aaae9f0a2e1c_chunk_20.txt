 seePrabhumoyeetal.,2020).
text generation that reweights the predictions of
PriortousingpretrainedLMsasabackbone,most
languagemodelsbasedonexpert(andanti-expert)
workusedcustomneuralmodelstrainedfortheir
opinions. In experiments for two different tasks,
respectivedownstreamgenerationtasks,including
detoxificationandsentimentcontrol,weshowthat
emotion-awaretextgeneration(Ghoshetal.,2017;
ourmethodisabletoeffectivelysteerthelanguage
FiclerandGoldberg,2017),attribute-awareproduct
modeltowardsthedesiredgenerations,whilepre-
reviewgeneration(Dongetal.,2017),andfriendly
servingthefluencyanddiversityofgeneratedtext.
or empathetic dialogue response generation (See
Asapplicationsbuiltonlanguagemodelsbecome
etal.,2019;Rashkinetal.,2019).
ubiquitous, DEXPERTS demonstrates promise in
Since pretrained LMs have shown impressive
steeringthesemodelstowardsafeanduser-friendly
textgenerationability(Radfordetal.,2018,2019),
generations.
two directions have emerged to control their
language generation: training approaches and Acknowledgments
decoding-timeapproaches. Trainingapproachesin-
This research is supported in part by NSF (IIS-
cludefinetuningthepretrainedLMsondatasetsthat
1714566),DARPAMCSprogramthroughNIWC
contain the desired attributes (Gururangan et al.,
Pacific (N66001-19-2-4031), and Allen Institute
2020)aswellascreatingaclass-conditionedpre-
forAI.WethankOpenAI,specificallyBiancaMar-
trainedLMtrainedontextwithspecificattributes
tin and Miles Brundage, for providing access to
controlcodeprefixes(Keskaretal.,2019). Incon-
GPT-3throughtheOpenAIAPIAcademicAccess
trasttoourmethod,suchapproachescanonlysteer
Program. We also thank UW NLP, AI2 Mosaic,
towards desired text attributes, they cannot steer
andtheanonymousreviewersforhelpfulfeedback
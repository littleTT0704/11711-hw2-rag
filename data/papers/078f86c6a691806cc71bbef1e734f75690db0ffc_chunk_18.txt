 during training. First, the binary mask only con-
Figure4. TheMeanMagnitudeofNormalizedGradientsUp-
tainsoneunmaskedpixeltomodeltheconditionstructural
datedforEachClass.Configuration(A)isusedasthebaseline.
constraint p (y\k|yk). Second, the binary mask does not
s s s Implementation Two different segmentation architectures
containanyunmaskedpixels(azeromask).Inthiscase,the
areusedinourexperiments,i.e.,(1)DeepLab-V2[3]with
model is going to learn the likelihood of the segmentation
the Resnet-101 backbone and (2) Transformer with the
mapp (y ).Third,thebinarymaskcontainsmorethanone
s s MiT-B3backbone[45]. TheTransformerdesignof[5]has
unmaskedpixelthataimstoincreasethegeneralizabilityof
been adapted to our conditional network structure G. Our
theconditionalstructurenetworkinmodelingsegmentation
framework is implemented in PyTorch and trained on four
structuresconditionedontheunmaskedpixels.
48GB-VRAM NVIDIA Quadro P8000 GPUs. The model
To model conditional structure network G in a parallel
is optimized by the SGD optimizer [2] with learning rate
fashion, the network G is designed as a Transformer. In
2.5×10−4,momentum0.9,weightdecay10−4,andbatch
particular,consideringeachpixelasatoken,thenetworkG
size of 4 per GPU. The image size is set to 1280 × 720
is formed as the Transformer with L self-attention blocks
pixels. IntheproposedFREDOMframework,thelearning
where each block is designed in a residual style and the
strategiesandsamplingtechniquesof[1,19]areadoptedfor
layernormsareappliedtoboththemulti-headself-attention
the self-supervised loss L to train our model. Our imple-
andmulti-perceptronlayers. Bythisdesign, thespatialre- t
mentationisfurtherdetailedinthesupplementary.
lationship and structural dependencies can be modeled
valueforquality),withoutconsid- ofmachine-generatedtextingeneral,andfurther
eringtheuseofanyannotateddata(eitherthrough highlight the potential of using LLMs to provide
in-contextlearningorfinetuning),andonlyinhigh- AIFeedback(Fernandesetal.,2023).
resourcelanguagepairs.
Weprovidealarge-scalestudyofthecapabilities 2 Background: MTEvaluation
of LLMs (from the PaLM and PaLM-2 families;
Machinetranslationevaluationisoneofthemost
Chowdheryetal.,2022;Aniletal.,2023)forma-
well-studiedevaluationproblemsinNLP(Callison-
chinetranslationevaluation(bothwithandwithout
Burchetal.,2008;Freitagetal.,2022). Inthistask,
areferencetranslation),provideanovelcompari-
given
sonbetweenpromptingandfinetuning,andinvesti-
gatetheperformanceinthelow-resourcescenario. 1. asourcesentenceina(source)language
InspiredbyfindingsthattheperformanceofLLMs
2. acandidatetranslationina(target)language
canbeimprovedbypromptingthemforrationales
of their predictions (Wei et al., 2022; Lu et al., an evaluation metric assesses the quality of the
2023), we also propose AUTOMQM, a prompt- candidate translation by how well it conveys the
ing technique for MT evaluation that asks LLMs meaningofthesourcesentencewhileconsidering
toidentifyerrorspansinatranslationandtoclas- other factors like fluency. Like many other natu-
sifytheseerrorsaccordingtotheMQMframework, rallanguagegenerationevaluationproblems,this
withaqualityscorederivedautomaticallyfromthe task is difficult because the set of correct transla-
identifiederrors. Akeyadvantageof AUTOMQM tionsforagivensourcesentenceisoftenverylarge
isitsinterpretability,asuserscaninspecttheerrors and not entirely known in advance. To simplify
responsibleforascore(Figure1). theproblemofmachinetranslationevaluation,of-
Ourcontributionscanbes
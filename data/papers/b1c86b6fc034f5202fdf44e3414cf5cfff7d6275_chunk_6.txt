variousYouTubevideoswhichhavebeenweaklylabeled. Thefeatureembeddingsprovided
by[2]areextractedfromaVGGishmodelfollowedbyaPCAtoreducedownto128dimensions
andeachtensecondclipisrepresentedbyten128-dimensionfeaturevectors,i.e. onefeaturevector
representsonesecondofoneclip. Theentiretensecondclipcancontainmultiplelabelsoutofthe
total527classesintheAudiosetdata. Theclassesencompassawidevarietyofsoundssuchashuman
noises,music,animals,orvehicles. ThefirsttwohigherlevelclassesareshowninFigure1.
Figure1: ThetoptwohighestlevelclassesintheAudioSetontology
WeusethetrainingsetofAudiosetfortrainingourmodels,whichcontainsabout22,160tensecond
clipsorabout221,160featureembeddings. Wethenpreprocessthecorrespondinglabelsofeach
audio clip to get the labels in the top two levels of the Audioset ontology. The validation set is
3
about20%ofthetrainingsetsizeandisdrawnfromtheunbalancedtrainingsetofAudioset,and
theAudiosettestsetcontainsabout20,383audioclips. Thesamelabelprocessingisdoneforthe
validationandtestsettoobtaintwolevelsofontologylabels. Althoughtherearemorelevelstothe
Audiosetontology,wechoosetofocusonthetoptwolevelstofitourmodelarchitectures.
TogiveamoregeneralideaoftheAudiosetdata,consideraweaklylabeledaudioclipwhichcould
havemultiplelabelswithnotiminginformation,butthelabelscouldberelatedduetotheirontology.
Forexample,ahumanvoiceandhandscouldpossiblyco-occurbecausetheyarebothnaturalsounds.
Thelabelscouldalsoberelatedduetotheirdependency,suchasahumancryingcouldco-occurwith
sadmusic. Ontheotherhandthelabelscouldalsonotberelatedwitheachother. Forinstance,adog
barkingsoundandcarenginesoundcouldco-occur
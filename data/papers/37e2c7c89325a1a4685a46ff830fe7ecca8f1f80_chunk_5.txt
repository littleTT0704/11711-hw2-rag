N. Forexample, this
train i i i=1
couldbeatextorimageclassifierthatwastrainedonaparticulardownstreamtask(withD being
train
thetrainingdataforthattask). Post-hocinterpretabilitymethodstypicallyintroduceanexplainer
moduleE :T ×X →E thattakesamodelandaninput,andproducesanexplanatione∈E forthe
T
outputofthemodelgiventhatinput,whereE denotesthespaceofpossibleexplanations.Forinstance,
2
interpretabilitymethodsusingsaliencymapsdefineE asthespaceofnormalized distributionsof
importanceoverLinputelementse∈(cid:52) (where(cid:52) isthe(L−1)-probabilitysimplex).
L−1 L−1
Pruthietal.[2020]proposedanautomaticframeworkforevaluatingexplainersthattrainsastudent
model S : X → Y with parameters θ to simulate the teacher (i.e., the original classifier) in a
θ
constrainedsetting. Forexample,thestudentcanbeconstrainedtohavelesscapacitythantheteacher
byusingasimplermodelortrainedwithasubsetofthedatasetusedfortheteacher(Dˆ (cid:40)D ).
train train
In this framework, a baseline student S is trained according to θ∗ =
θ
argmin θE
(x,y)∼Dˆ
train[L sim(S θ(x),T(x))], and its simulability SIM(S θ∗,T) is measured on
an unseen test set. The actual form of L
sim
and SIM(S θ∗,T) is task-specific. For example, in a
classificationtask,weusecross-entropyasthesimulationlossL overtheteacher’spredictions,
sim
whilethesimulabilityofamodelS canbedefinedasthesimulationaccuracy,i.e.,whatpercentage
θ∗
ofthestudentandteacherpredictionsmatchoveraheld-outtestsetD :
test
SIM(S θ∗
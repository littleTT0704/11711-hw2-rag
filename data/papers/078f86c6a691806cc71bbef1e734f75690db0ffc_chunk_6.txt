2)E xs,yˆs∼ps(ys,yˆs)Ls(ys,yˆs)+E xt∼pt(xt)Lt(yt () 1(cid:3)
)
ing privileged depth information. Vu et al. [42] presented where L is the supervised cross-entropy (CE) loss in the
s
thefirstadversarialentropyminimizationapproachtoUDA sourcedomain.Meanwhile,L istheunsupervisedlearning
t
in segmentation. Then, [29,46] presented a curriculum lossinthetargetdomainthatcanbedefinedastheadversar-
adaptation training from easy to complex samples ranked ialloss[29,38,39,42],ortheself-supervisedloss[1,19,47].
by the entropy level. Truong et al. [22,35] improved the In recent studies, the self-supervised loss defined by the
performance of segmentation models by introducing a bi- cross-entropy loss with pseudo labels has achieved SOTA
jectivemaximumlikelihoodapproach. performanceandoutperformedotherpriormethods. There-
Self-supervised Approach has gained a SOTA perfor- fore, our proposed approach also defines L as the self-
t
mance in UDA in semantic segmentation in recent years supervisedloss[1,19]withthenovelfairnessguarantee.
[1,14,19,47,50]. Inself-trainingapproaches,anewmodel
3.1.TheFairnessObjectiveFunction
is trained on unlabeled data using pseudo-labels derived
from a trained model. Araslanov et al. [1] proposed an Under the fairness constraint in semantic segmentation,
augmentationconsistencyapproachtoautomaticallyevolve theperformanceofeachclassshouldbeequallytreatedby
pseudolabelswithoutusingfurthertrainingrounds. Zhang thedeepmodel. Thus,thegoaloffairnessinsemanticseg-
et al. [47] introduced a knowledge distillation approach to mentationcanbedefinedasinEqn. (2).
i tm hep sr oo fv ti pn sg et uh de op le ar bf eo lr sm oa nn lic ne eo.f
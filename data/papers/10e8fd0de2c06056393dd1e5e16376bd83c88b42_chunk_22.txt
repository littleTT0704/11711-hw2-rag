bypromptingthesamemodelas
useeightseedexamplesfromthetargetdomainto used in our data generation with 8 examples that
generateadditionaltrainingdatatohelpbootstrap demonstrate how to answer questions in the tar-
adaptation in the target domain. We observe that getdomain. InTable7,weobservethattheLLM
itiseasierforlargelanguagemodelstocondition doeswellondatasetswithtriviastylefactualques-
onasinglevariable(context)andcompress(Goyal tions, like SearchQA and Quasar-T, but in other
et al., 2022) multiple facts from the passage into casesdoesnotperformaswell. Thefew-shotdata
asinglesentence,ascomparedtoconditioningon augmentationtrainedmodel,ontheotherhand,per-
acontextandanswerspantogether. Moreover,in formsbetteracrossawiderrangeofdomainsand
section 5.1 we observed that augmentation with
datasetswiththeimprovementsupto∼24%inF1
clozestyleQApairsyieldedsimilarperformance onQuasar-Swhencomparedwithbaseline.
tousingquestion-formattedQApairs,offeringevi-
dencethatthepreciseformatisnotasimportantas Baseline Closed-Book DataGen
ReaderParams (770M) (540B) (770M)
thecontentitself.
BioASQ 45.38 32.02 50.64
We prompt the model in the following format,
CliCR 6.126 10.84 19.42
“Afterreadingthearticle,«context»thedoctorsaid Quasar-S 10.24 23.75 34.19
«sentence»."forpubmedarticles. Forothertarget Quasar-T 34.92 55.32 45.86
NewsQA 18.57 8.67 23.37
corpuswereplacedoctorwithengineer,journalist
SearchQA 34.60 61.53 37.65
and poster for stackoverflow, dailymail and red- COLIEE 46.79 53.02 61.11
ditrespectively. Tofilteroutinvalidsentences,we
applythreesimpleheur
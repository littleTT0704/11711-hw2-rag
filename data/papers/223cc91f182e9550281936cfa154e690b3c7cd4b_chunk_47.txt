alev-Shwartz, 2012) whose distribution can change over time, or the experience in lifelong
learning (Thrun, 1998) that differs across a series of tasks.
In particular, we consider an online setting: at each time τ ∈ {1,...,T}, a predictor is given an input and is
required to make a prediction (e.g., if the stock market will go up or down tomorrow where). We have access to
the recommended prediction by each of the K experts t = {1,...,K}, and make our prediction
accordingly. As a result, the environment reveals a reward based on the discrepancy between the prediction and
the true answer. The sequence of data instances follows a dynamic that is unknown and can even be
adversarially adaptive to the predictor’s behavior (e.g., in the problem of spam email filtering, or other strategic
game environments) (Shalev-Shwartz, 2012). In such cases, we can only hope the predictor to achieve some
relative performance guarantee, in particular w.r.t. the best single expert in hindsight. This is formally captured
by regret, which is the difference between the cumulative reward of the predictor and that of the best single
expert. We further consider the specific setting where we submit a ‘soft’ prediction p (t), that is, the
τ
distribution (a vector of normalized weights) over the K experts, and receive the rewards r τ(t) ∈ R for each
expert t. The goal of the learning problem is then to update the distribution p (t) at every step τ for minimal
τ
regret ∑ τT
=1
r τ(t∗) −∑ τT
=1
Ep τ(t)[r τ(t)], where t∗ is the best single expert.
The rewards from the environment naturally serves as the dynamic experience:
f τ(t) :=r τ(t). (6.7)
In the following, we show that the SE rediscovers the classical multiplicative weights (MW), or Hedge
algorithm (Freund & Schapire, 1997) to the above online problem. Similar ideas of multiplicative weights have
been widely used in diverse fields such as optimization, game theory, and economics (Arora et al., 2012).
34
Harvard Data Science Review • Issue 4.4
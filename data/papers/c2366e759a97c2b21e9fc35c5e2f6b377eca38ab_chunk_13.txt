 prompt-situation architecture. However, TTP’s performance on unseen #objects
deteriorates(0.62forseenversus0.34onunseen#objects),andwelookmorecloselyatthatnext.
Generalizationtounseen#objects:Fig.7aexaminesPEonout-of-distributionsessionswithlesser
i.e. 3-5ormorei.e. 8-10objectsperrack. Thetrainingsetconsistsofdemonstrationswith6or7
objectsperrack.Thepolicyperformswellon5-7objects,butpooreraswegofurtherawayfromthe
trainingdistribution. PoorerPEisexpectedforlargernumberofobjects,astheactionspaceofthe
policyincreases,andthepolicyismorelikelytopickthewrongobjecttypeforagivenpreference.
Poorer performance on 3-4 objects is caused by the policy closing the dishwasher early, as it has
neverseenthisstateduringtraining. Trainingwithricherdatasets,andaddingmorerandomization
intheformofobjectmaskingmightimproveout-of-distributionperformanceofTTP.
3.2 Real-worlddish-rearrangementexperiments
We zero-shot transfer our policy trained in simulation to robotic hardware, by assuming low-level
controllers. We use a Franka Panda equipped with a Robotiq 2F-85 gripper, controlled using the
Polymetis control framework [33]. For perception, we find the extrinsics of three Intel Realsense
D435 RGBD cameras [34] using ARTags [35]. The camera output, extrinsics, and intrinsics are
6
Figure6: PipelineforRealHardwareExperiments
combinedusingOpen3D[36]andfedintoasegmentationpipeline[37]togenerateobjectcategories.
Forlow-levelpick,weuseagraspcandidategenerator[38]appliedtotheobjectpointcloud,anduse
ittograspthetargetobject. Placeisapproximatedasa‘drop’actioninapre-definedlocation. Our
hardware setup mirrors our simulation, with different categories of dishware (bowls, cups, plates)
onatable, a“dishwasher”(cabinetwithtwodraw
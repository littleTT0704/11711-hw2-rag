etheT5basedreader
istraineduntilconvergence. Inthefirstphase,we
model to circumvent the limited tokens used for
trainonathirdoftheeasyexamples;inthesecond,
BART along with the FiD model pretrained on
onadisjointthirdoftheeasyexamples,andathird natural questions 2. We further experimented by
ofthemediumexamples;inthethirdphase,adis-
placing the golden passage at the top-most posi-
jointthirdofallofthethreepartitions,andinthe
tion(Goldsetting)intheretrievedpassagesbefore
finalphase,wetrainontheentiretrainingset.
passing it to the reader during training. We also
applycurriculumlearning(CL)inthereaderasper
5 Experiments
describedinSection4.
Dataset : The MultiDoc2Dial dataset consists
6 Results&Discussion
of4796dialogues, consisting29,748queryturns
andgroundedin4283passagesacross4domains Table 1 shows our model’s performance on the
(Social Security Administration, Veteran Affairs, validation split. Applying DistilSPLADE as the
Student-Aid,andDMV).MDD-UNSEENtestcor- retrieverwithFiD+T5asthereaderwesawa10
pususedinsharedtaskisbasedonCOVIDdomain. pointimprovementinBLEUcomparedtothebase-
line. Reranking(RR)theretrievaloutputsleadsto
5.1 Baseline further increase in the overall metrics. Addition-
The proposed baseline for the MultiDoc2Dial ally,curriculumlearning(CL)booststhemodel’s
sharedtaskcomprisesaretrieval-augmentedgener- performance. SettingM1showsaBLEUscorethat
ator(RAG)model(Lewisetal.,2020b). Themodel is 1 point higher than the ”DistillSplade + Fid +
uses a fine-tuned dense passage retrieval (DPR) RR”model. WeusetheM1settingforevaluation
model(Karpukhinetal.,2020)tofindrelevantpas- ontheTestSEENdataset. FortheGoldsetting,we
sagesandapretrainedsequence-
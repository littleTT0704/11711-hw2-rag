 59.68%
% Gain +11.0% +23.0% +18.8%
# Gain/Loss +322/-96 +99/-29 +61/-19
Wiktionary 14.71% 5.96% 9.23%
Expansion 28.53% 11.93% 22.75%
% Gain +93.9% +100.2% +146.5%
# Gain/Loss +586/-101 +58/-11 +71/-11
All Sources 66.08% 45.43% 59.46%
Expansion 71.12% 51.27% 63.96%
% Gain +7.6% +12.9% +7.6%
# Gain/Loss +286/-109 +82/-36 +44/-24
Table 6.16: QA accuracy of Watson on Jeopardy! and TREC questions when using
sources that were expanded with web search results. For each setup, we show the
percentage gain and the number of questions gained/lost. All improvements are sig-
nificant with p <.01 based on a one-sided sign test.
types. Comparedtothestrongestbaselinecomprisingallmanuallyselecteddocument
collections, SE improves accuracy by 7.6% on regular Jeopardy! questions, by 12.9%
on Final Jeopardy! and by 7.6% on TREC 11. These results are based on answer
keys that were extended with correct answers returned by Watson in first place and
therefore closely reflect true system performance. The improvements from source
expansion are statistically significant with p <.01, even though only about half of
the sources in the strongest baseline were expanded. It is also worth noting that
the collection of manually acquired sources includes the AQUAINT newswire corpus,
which was used as the reference source in TREC 11 and contains the answers to
all questions. Nevertheless, the setup that uses only Wikipedia and its expansion is
equally effective on the TREC test set (59.68% vs. 59.46% accuracy) even though
these sources are not guaranteed to support the answers. The gains in accuracy
exceed the gains in candidate recall on all datasets, which supports our claim at the
beginning of this thesis that SE also facilitates answer selection (failure type 3 in
Section 1.1).
The expanded
 label error spans with
Tokeeppacewiththeconstantlyimprovingqual-
acategoryandseverity. Thismuchricherfeedback
ityofMToutput,thenextgenerationofautomatic
can be used to gain a better understanding of the
∗ Workdonewhileworkingpart-timeatGoogle. currentlimitationsofthemodelunderevaluation
3202
guA
41
]LC.sc[
1v68270.8032:viXra
andimproveit. • WearethefirsttoevaluateLLM-basedevalu-
In this paper, we ask whether large language ationmethodsonlow-resourcelanguagepairs.
models(LLMs)incombinationwithafewhuman Wefindthattheirperformanceispromising,
annotations can be used to design an automatic but lags behind state-of-the-art learned met-
metricthatgeneratesrichfeedbacksimilartothat rics.
generatedbyhumanexpertsinMQM.Thiswork
• Wefindthat,withAUTOMQM,PaLM-2mod-
is motivated by recent papers that demonstrated
els can be prompted to generate rich MQM-
thatLLMscanbeusedasautomaticmetrics(Liu
like annotations, outperforming their score
etal.,2023b)togenerateasinglequalityscore. In
predictioncounterpartsatthesegment-level.
particular,KocmiandFedermann(2023)showed
• Furthermore,annotationspredictedbyPaLM-
that LLMs can be prompted to assess the quality
2modelscorrectlyidentifyover50%ofwords
ofmachine-generatedtranslations,evenachieving
thatarepartofmajor errors,andarecompa-
state-of-the-artperformanceonassessingsystem-
rabletotheonesproducedbystate-of-the-art
level quality. However, previous work only pro-
supervised word-levelevaluators.
videsalimitedviewofthecapabilitiesofLLMsfor
machinetranslationevaluation: thefocushaspre- Our findings might have significant implica-
dominantlybeenonscoreprediction(i.e. predict- tions for not only MT evaluation, but evaluation
inganumerical
33
AAE GPT-3WAETranslation Gold New
RT@userIcan’tstandabadtexterbruhlikedon’t RT@userIcan’tstandabadtexterbrolikedon’t A ¤
bemadifIforgetaboutyoass bemadifIforgetaboutyou
RT@userRetweetifyoufuckwiththis!!!! RT@userRetweetifyoulikethis! A ¤
RT@userThatnigganeedsangermanagement RT@userThatguyneedsangermanagement A ¤
RT@userohfuckinghelltakeadayoffman RT@userohfucktakeadayoffman A A
Table 6: Examples of AAE tweets with their GPT-3 based WAE translation, and original gold standard and new
annotations based on AAE-relabeled. For the first three tweets, the (biased) gold labels are changed by models
predicting the new labels on their WAE translations. A indicates presence of toxicity, and ¤ represents non-
toxic. Weanonymizetheusernamestoprotectuserprivacy.
temshasgrown,severalbiaseshavebeenfoundin tasks. Focusingontwotypesofbiases,lexicaland
datasetandmodels,spurringvariousdebiasingef- dialectal, our experiments show that these meth-
fortstomitigatetheseindividualbiases(e.g.,gen- ods face significant challenges in reducing the bi-
der bias, racial bias; Park et al., 2018; Sap et al., asedbehaviorintoxicitydetectors. Thisindicates
2019; Davidson et al., 2019). Some work tackles that biases in toxic language detection might be
identity-basedbiases,e.g.,usingdatare-balancing different in nature compared to spurious associa-
(Dixon et al., 2018), or adversarial feature learn- tions studied in typical NLU settings. We studied
ing (Vaidya et al., 2019). Less work has tackled a synthetic scheme for relabeling examples with
racial or dialectal bias. Notably, Xia et al. (2020) potential dialectal biases; our results indicate that
useadversarialtrainingtopreventthemodelfrom correctingnoisylabelsresultsinbetterbiasreduc-
associatingtoxicity
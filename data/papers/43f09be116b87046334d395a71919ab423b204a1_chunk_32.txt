OntoNotes.
AverageRecall 0.437 -
AverageF1 0.527 - We report results with singletons included and
IAA 0.691 0.97 excludedfromsystemoutput. Ourevaluationscript
canbefoundatsrc/coref/metrics.py.
TimedAnnotationExperimentCoreferenceAgreement
CNDatasetAdditionalDetails Table8liststhe
AgreementMetric Non-expert Domain-expert
Annotators Annotators specificdefinitionsforlabelsusedbyannotatorsin
Krippendorf’salpha 0.371 - theCNdataset,ascomparedtothedescriptionsin
AveragePrecision 0.275 - thei2b2/VAdatasetafterwhichtheyweremodeled.
AverageRecall 0.511 -
Table7reportsmeasuresforinter-annotatoragree-
AverageF1 0.342 -
IAA 0.368 0.73 ment for the CN dataset, compared to agreement
Table 6: Annotation agreement metrics for timed experi- reportedforcoreferenceannotationsinOntoNotes.
mentsofmentiondetectionandcoreferenceresolution.Inter-
Annotator Agreement (IAA) refers to a metric defined in
(Uzuneretal.,2012).Forcoreference,precision,recall,and
F1areaveragedoverstandardmetricsdefinedin§B. CNAnnotationAgreement
AgreementMetric Non-expertAnnotators OntoNotes
MUC 72.0 68.4
CEAFϕ 40.5 64.4
For a given number of mentions m, we gener-
C B3EAFm 6 53 7..4
8
4 78 5..0
0
ated models for min(max(6,15000/m),15) ran- Krippendorf’sMDalpha 60.5 61.9
domseeds. Theseboundswereselectedbasedon Krippendorf’sref.alpha 70.5 −
Table7: AnnotationagreementmetricsfortheCNdataset
preliminaryexperimentsassessingdeviation.
computedoverarandomsampleof20documents.Weachieve
agreementonparwithOntoNotes(Pradhanetal.,2012).
We use a
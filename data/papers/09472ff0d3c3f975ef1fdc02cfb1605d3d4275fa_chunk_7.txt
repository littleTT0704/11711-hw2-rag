 otherwise.
2. Mute: For every representation z, RSC masks out the bits associated with
larger gradients by:
z˜=z(cid:12)m (3)
3. Update: RSC computes the softmax with perturbed representation with
˜s=softmax(h(z˜;θ(cid:98)top)), (4)
t
and then use the gradient
g˜
θ
=∂l(˜s,y)/∂θ(cid:98)t (5)
toupdatetheentiremodelforθ(cid:98)t+1 withoptimizerssuchasSGDorADAM.
We summarize the procedure of RSC in Algorithm 1. No that operations
of RSC comprise of only few simple operations such as pooling, threshold and
element-wise product. Besides the weights of the original network, no extra pa-
rameter needs to be learned.
3.2 Theoretical Evidence
To expand the theoretical discussion smoothly, we will refer to the “dog” vs.
“cat” classification example repeatedly as we progress. The basic set-up, as we
introduced in the beginning of this paper, is the scenario of a child trying to
learn the concepts of “dog” vs. “cat” from illustrations in her book: while the
6 Huang et al.
Algorithm 1: RSC Update Algorithm
Input: data set (cid:104)X,Y(cid:105), percentage of representations to discard p, other
configurations such as learning rate η, maximum number of epoches T, etc;
Output: Classifier f(·;θ(cid:98));
random initialize the model θ(cid:98)0;
while t≤T do
for every sample (or batch) x,y do
calculate z through forward pass;
calculate g with Equation 1;
z
calculate q and m as in Equation 2;
p
generate z˜ with Equation 3;
calculate gradient g˜ with Equation 4 and Equation 5;
θ
update θ(cid:98)t+1 as a function of θ(cid:98)t
weightonthenon-parametriccomponent.
LookingcloselyatEquation2,wecannoticeasimilaritybetweenthecalculationofP andthestandard
kNN
P. ThekNNdistributionisbasedonthedistancesbetweenthecurrentcontextandthenearestneighbors
LM
fromthedatastore,normalizedbyasoftmaxfunction. Recallthatin(standard)parametriclanguagemodels,
thedistributionoverthevocabularyisalsobasedonameasureofdistance,theinnerproductbetweenthe
currentcontextembeddingandthewordembeddingsofeverytokeninthevocabulary. Becauseeachcontext
embeddinginthedatastore(K,V)correspondstoatargettoken,wecanalsoviewthisdatastoreasalarge
wordembeddingmatrixwithmultiplewordembeddingsforeachofthevocabularywords. Theoretically,
givenunlimitedcomputation,wecouldcalculatethedistributionbasedonthedistancestoeveryembeddingin
thedatastore,andaggregatebyvocabularyitems,makingitmorecloselyresembleP. Inthiscase,k =|D|,
LM
thesizeoftheentiredatastore,andEquation2becomesthefollowing,basedonthedistancestoeverycontext
inthedatastoreDinsteadofasubsetofnearestneighborsN.
(cid:88)
p (w |c )∝ 1 exp(−d(k,f(c ))) (4)
D t t wt=vi i t
(ki,vi)∈D
In practice, we use kNN search as a way of approximation, by limiting the calculation to only k nearest
neighborstoavoidthecomputationalcostofcalculatingthedistributionovertheentiredatastore.
Ifwere-writeandgeneralizeEquation2,boththekNN-LMofKhandelwaletal.(2020b)andalargenumber
ofrelatedmodelscanbeexpressedthroughthefollowingequation:
P =(1−λ)softmax(W ·h )+λMsoftmax(mask-to-k(W ⊗h )/τ). (5)
interp
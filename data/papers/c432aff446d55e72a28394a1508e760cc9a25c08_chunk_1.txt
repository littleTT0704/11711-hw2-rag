Why do Nearest Neighbor Language Models Work?
FrankF.Xu UriAlon GrahamNeubig
LanguageTechnologiesInstitute
CarnegieMellonUniversity
{fangzhex,ualon,gneubig}@cs.cmu.edu
Abstract
Language models (LMs) compute the probability of a text by sequentially computing
a representation of an already-seen context and using this representation to predict the
nextword. Currently,mostLMscalculatetheserepresentationsthroughaneuralnetwork
consumingtheimmediatepreviouscontext. Howeverrecently,retrieval-augmentedLMs
haveshowntoimproveoverstandardneuralLMs,byaccessinginformationretrievedfroma
largedatastore,inadditiontotheirstandard,parametric,next-wordprediction. Inthispaper,
wesetouttounderstandwhyretrieval-augmentedlanguagemodels,andspecificallywhy
k-nearestneighborlanguagemodels(kNN-LMs)performbetterthanstandardparametric
LMs, even when the k-nearest neighbor component retrieves examples from the same
training set that the LM was originally trained on. To this end, we perform a careful
analysisofthevariousdimensionsoverwhichkNN-LMdivergesfromstandardLMs,and
investigate these dimensions one by one. Empirically, we identify three main reasons
whykNN-LMperformsbetterthanstandardLMs: usingadifferentinputrepresentation
forpredictingthenexttokens, approximatekNNsearch, andtheimportanceofsoftmax
temperature for the kNN distribution. Further, we incorporate these insights into the
model architecture or the training procedure of the standard parametric LM, improving
itsresultswithouttheneedforanexplicitretrievalcomponent. Thecodeisavailableat
https://github.com/frankxu2004/knnlm-why.
1 Introduction
Languagemodelingisthetaskofpredictingtheprobabilityofatext(oftenconditionedoncontext), with
broad-spanning applications across natural language processing (Bengio et al., 2003; Merity et al., 2018;
BaevskiandAuli,2018;Brownetal.,
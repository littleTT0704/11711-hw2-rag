ical
and decoders include Long Short Term Memory reasoning tasks such as geometry problems solv-
network (LSTM) (Hochreiter and Schmidhuber, ing(Robaideketal.,2018;Chenetal.,2021a)and
1997), Gated Recurrent Unit (GRU) (Cho et al., theoremproving(YangandDeng,2019). Various
2014), and their bidirectional variants: BiLSTM attentionmechanismshavebeenstudiedtoextract
andBiGRU.Alargeamountofworkhasshownthe betterrepresentations,suchasGroup-ATT(Lietal.,
performance advantage of Seq2Seq models over 2019)whichusesdifferentmulti-headattentionto
previousstatisticallearningapproaches(Lingetal., extractvarioustypesofMWPfeatures,andgraph
2017;Wangetal.,2018a;Huangetal.,2018;Wang attention which is applied to extract knowledge-
etal.,2019;Lietal.,2019). awareinformationin(Wuetal.,2020).
3.4 OtherNeuralNetworksforMath workhasshownthepromisingperformanceofpre-
trainedlanguagemodelsinansweringmathword
Deep learning approaches to mathematical rea-
problems(Kimetal.,2020),assistingwiththeorem
soning tasks can also make use of other neural
proving(Wuetal.,2022b),aswellassolvingother
networks, such as convolutional neural networks
mathematicaltasks(Charton,2022).
(CNN)andmultimodalnetworks. Someworken-
codes the input text using a convolutional neural However, though large language models excel
networkarchitecture,givingthemodeltheability in modeling natural language, there are several
to capture long-term relationships between sym- challengestousingthemformathematicalreason-
bolsintheinput(Gehringetal.,2017;Wangetal., ing. First, pre-trained language models are not
2018a,a;Robaideketal.,2018;Alemietal.,2016; specifically trained on mathematical data. This
Loosetal.,2017). Forexample, thefirstapplica- likelycontributestothembeinglessproficientin

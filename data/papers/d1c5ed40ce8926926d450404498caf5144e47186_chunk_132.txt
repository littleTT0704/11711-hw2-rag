 on two subsets of TREC 8–15: (1) questions
from which queries consisting of individual keywords were generated, and (2) ques-
tions for which queries with at least one phrase were constructed.
In Table 6.13 we show the search recall of OpenEphyra and the impact of statistical
source expansion on these two subsets of TREC 8–15 for both passage searches and
title searches. Queries containing at least one phrase have higher recall than keyword
queries, and they also benefit more from expanded sources. This comes to no surprise
since phrase queries are often more specific and are less affected by noise added
through source expansion in addition to useful information. Thus, to maximize the
impact of our method, one should consider using more constrained queries that are
resistant to noise.
6.3.5 Robustness of Search
So far we have seen that statistical source expansion yields consistent gains in search
recall while hurting relatively few questions. This sets our method apart from query
expansion techniques, which typically have much higher variance [Collins-Thompson
and Callan, 2007]. Now we take a closer look at how search rankings generated by
Watson and OpenEphyra are affected by source expansion, following the evaluation
methodology proposed by Collins-Thompson [2008]. For each question in the Jeop-
ardy! and TREC datasets, we retrieved 100 passages and 100 document titles from
baseline corpora and expanded sources generated through statistical SE. In experi-
ments with Watson, we used the collection of manually acquired sources as a baseline
and expanded Wikipedia, Wiktionary and the two small encyclopedias. The results
for OpenEphyra are based on Wikipedia with and without source expansion. We then
examined how SE affects the rank of the first relevant passage or title. This analysis
gives us insights into how much our method helped or hurt for each question, and
into the robustness of the search rankings to source expansion. The positions of the
first relevant passage and title are important because the search ranks can be used as
features for answer scoring. If relevant results are generally retrieved at high ranks,
answer scoring accuracy can be improved by giving more weight to candidates that
were extracted from the top results.
We generated histograms that illustrate the distribution of the change in retrieval
rank of the first relevant search result if expanded sources are added. The histograms
are given in
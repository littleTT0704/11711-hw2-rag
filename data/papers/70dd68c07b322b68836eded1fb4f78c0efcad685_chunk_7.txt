smT5inmosttasks
ferencecosts. Interestingly,ourmulti-dimensional andfine-tuningsettings. WenotethatmT5-small
evaluation reveals that mBERT generally has the mightbesomewhatpenalizedasmostparameters
best performance and efficiency under most set- are allocated on the embedding layer (85% for
tings. Next,wediscusseachevaluationdimension mT5-Smallvs0.3%forByT5-Small). Also,given
indetail. thatthetasksconcernedarenotgeneration-heavy
tasks, the extra depth on the encoder side (12
4.1 Robustnesstofine-tuningdatasettings
layers in ByT5-Small vs 8 layers in mT5-Small)
InFig.1,weplotthethreeperformancenumbers might have favored ByT5 over mT5. However,
obtained under different fine-tuning data settings when comparing the two encoder-decoder based
(MULTI, SINGLE, ZERO) for each model. Over- models to mBERT, mBERT still achieves the
all,mBERTachievesthebestaccuracyforalmost best performance and robustness in all tasks and
allthreesettings. Since MULTI typicallyleadsto settingswiththeexceptionofXNLIZero-shot.
ycaruccA
ycaruccA
1F
1F
1F
1F
mBERT CANINE-S mT5 ByT5
XNLI(en) NER(en) TyDiQA(en)
80
80 80 70
70
70 60
60 50
60
50 40
40 50 30
30 40 20
102 103 104 105? 106 102 103 104 105? 102 103?
80 80
80
70
70 70 60
60 60 50
50 40
50 30
40
20
30 40
102 103 104 105? 106 102 103 104 105? 102 103?
Numberofannotatedsamplesusedforfine-tuning
Figure2:Performanceofmodelsfine-tunedwithdifferentnumbersoflabeledsamplesinthethreetasksinEnglish.
ThetoprowcomparesmBERTtotheCANINE-Smodel,whilethebottomrowcomparesmT5
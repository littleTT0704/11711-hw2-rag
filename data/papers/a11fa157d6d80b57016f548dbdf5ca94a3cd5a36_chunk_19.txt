amountofnegativesam-
ples with the same strategy in Li et al. (2016) by AspreviouslymentionedinSection§3,theresult-
random replacing entities or relations in a true ing knowledge graphs can be viewed as a sym-
knowledgetuple. Eachscoringfunctionranksthe bolicinterpretationofLMs. Weextractknowledge
samplesbasedonthescoresfromhightolow. We graphsfrom5distinctlanguagemodelsandsubmit
canthencomputeboththeprecisionandrecallof themtohumanannotationevaluation. Thefindings
positive samples at different cut-off points along are presented in Table 4 (The detailed results per
theranking,andplottheprecision-recallcurvesfor relation is listed in Table 5), which sheds some
eachmethod. newlightonseveralknowledge-relatedquestions
Theautomaticevaluationsettingongivenknowl- regardingtheLMs’knowledgecapacity.
edgetermsenablesustoadaptexistingprevalent DoesalargerLMencodebetterknowledge?
works, e.g., KG completion and factual probing ThelargeversionofBERTandRoBERTahavethe
(Table 1), for comparison with our approach: (1) samepretrainingcorpusandtasksastheirbasever-
COMET(Bosselutetal.,2019)isatransformer- sions,buthavelargermodelarchitectureinterms
basedKGcompletionmodeltrainedtopredictthe oflayers(24v.s. 12),attentionheads(16v.s. 12),
tail entity t conditioning on the head entity and andthenumberofparameters(340Mv.s. 110M).
relation (h,r) on ConceptNet. We use its log- WecanseethattheaccuraciesofBertNet-largeand
likelihoodlogP(t|h,r)asthescoreforeachgiven RoBERTaNet-largearearound7%and3%higher
knowledgetuple. (2)LPAQA(Jiangetal.,2020) than their base version, separately, indicating the
collectsasetofpromptsonLAMAwithtextmin- larger models indeed encoded better knowledge
ingandparaphrasing,andoptimizetheir
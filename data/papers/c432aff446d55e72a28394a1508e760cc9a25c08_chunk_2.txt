2020). Thismodelingisusuallydonebysequentiallyencodingacontext
c usingatrainedneuralnetworkfunctionf,andcomputingtheprobabilityofthenextwordw accordingto
t t
f(c )andavectorrepresentationofw.
t t
Recently,retrieval-augmentedLMshaveshownaseriesofimpressiveresults(Graveetal.,2017;Guuetal.,
2018;Heetal.,2020;Khandelwaletal.,2020b;Borgeaudetal.,2022;Alonetal.,2022). Retrieval-augmented
LMscomputenexttokendistributionsbasednotonlyontheimmediatelyprecedingcontextc andthemodel
t
parameters,butalsoonanexternaldatastore,fromwhichexamplesareretrievedandincorporatedintothe
baseLMâ€™sprediction.
Oneretrieval-augmentedmodelthatisnotableforbothitssimplicityandefficacyisthek-nearestneighbor
languagemodel(kNN-LM;Khandelwaletal.,2020b). ItextendsatrainedbaseLMbylinearlyinterpolating
theoutputworddistributionwithakNNmodel. Thenearestneighborsareretrievedaccordingtothedistances
betweenthecurrentcontextembeddingofthebaseLMandallthecontextembeddingsinthedatastore. The
datastoreiscreatedbyencodingallcontextsfromanytextcollection,includingtheoriginalLMtrainingdata.
OneofthemostsurprisingresultsfromKhandelwaletal.(2020b)isthatkNN-LMreducestheperplexityof
thebaseLMevenwhenthekNNcomponentisretrievingexamplesfromthesametrainingsetthattheLM
wasoriginallytrainedon,indicatingthatthekNN-LMimprovestheabilitytomodelthetrainingdataandis
Preprint.Underreview.
3202
naJ
71
]LC.sc[
2v82820.1032:viXra
ğ‘ƒ parametric component ğ‘ƒ non-parametric component
ğ¿ğ‘€ ğ‘˜ğ‘ğ‘
softmax() ğ· soft
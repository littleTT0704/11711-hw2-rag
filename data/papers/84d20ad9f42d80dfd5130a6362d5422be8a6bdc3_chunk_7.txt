linesearchengine. Thearrivalsofusers’
queriesareunpredictable,andsoisthemodel’sinferencebatchsize. AnAIassistantoperatingona
smartphonetypicallyprocessesonerequestatatime,whileanofflinetranslationsystemtranslatingan
entirebookmustuselargebatchsizestoprioritizemaximizingthroughput. Thesepracticalscenarios
arerarelyreflectedbyconventionalefficiencyevaluationsintheresearchcontext,wheremodelsare
2We plan to use the NVIDIA Jetson TX2 Module (https://developer.nvidia.com/embedded/
jetson-tx2) to simulate limited-resource settings such as on an automobile, and extend Pentathlon to a
smartphonetoevaluatemachinelearningmodelsdesignedtorunonmobiledevices.
3
typicallyassessedwithafixedbatchsize. Suchdisparityunderscoresthepressingneedforevaluation
protocolsthatbetterreflectreal-worlddeployments.
Ourapproach. InspiredbyReddietal.(2020), weincludefourdistinctevaluationscenariosto
provideacomprehensiveevaluationofNLPmodelsinavarietyofrealisticsettings:
• Fixedbatching. Theevaluationdataisfirstrandomlyshuffledbeforebeinggroupedintobatches
ofauser-specifiedbatch-size. Thissettingisintendedtomimictypicalresearchexperimental
settings. Wedefertotheuserschoosingoptimalbatchsizesfortheirmodels.
• Poisson batching is similar to the fixed batching scenario, but the size of each batch is ran-
domly drawn from a Poisson distribution with a mean of batch-size: batch-size ∼
Pois
Pois(batch-size). Thissetupaimstosimulateanonlineservicewherethevolumeofrequestsis
unpredictablebuttheaveragecanbeestimated.
• Singlestreamrandomlyshufflestheevaluationinstancesandusesabatchsizeofone,reflecting
theapplicationsprocessingonerequestatatime.
• Offline: Inthisscenario,themodelhasimmediateaccesstotheentireevaluationdataset,enabling
techniquessuch
ourspeakersidentifyreferentswithinthetarget
imagethatahumancaptionerwouldconsidersalient. HigherF1scoresoverapartofspeechwould
suggestthataspeakerismorecapableofidentifyingconceptsthatfallunderthatpartofspeech.
Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The
train-val-testsplitgivenbytheMSCOCO2017datasetisextendedtoourexperimentalsetup.
6.2 EFFECTSOFTOM
WetrainmodelsequippedwithToMlistenerswithnormalweight(i.e.equallyweightingthespeaker
andthelistenerscoreswhenrerankingoutpututterances)andwithhighweight(wherethelistener
weightissetarbitrarilyto1000andthespeakeressentiallyoptimizesforP(x|ui)).Inordertoisolate
theeffectsofusingaToMlistenerfromtheeffectsofmodifyingtheutteranceselectionprocess,we
also train a model equipped with a ToM listener that it does not give any weight to. Additionally,
wetrainamodelwithoutanyToMcomponentforcomparison. Wedothisforbotheasyandhard
distractors,wheretheharddistractorswerethosethatwerechosenbythehybridsimilaritybetween
visual and semantic (CLIP) features outlined in 5. Finally, we give the listener the ground-truth
captionsoverthetestsettocomputegold-standardmetricsofaccuracyandfluency.
WefindsignificantperformanceimprovementsinTable1whenspeakermodelsaretrainedtorerank
utterancessolelybyToMlistenerscore. Such“high-weightToM”speakermodelsachieveaccuracy
gainsof3.0%and4.6%oneasyandharddistractors,respectively. Thissuggeststhattheinclusion
ofasufficientlyinfluentialToMrerankerduringthespeakertrainingprocessimprovesspeakerper-
formance, although the relative gains appear to be much higher when training on easy distractors.
7
PublishedasaconferencepaperatICLR2023
Table 2: Performance and language features of speakers trained on various distractors. We only

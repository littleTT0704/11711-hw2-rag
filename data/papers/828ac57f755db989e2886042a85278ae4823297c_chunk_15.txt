CCAandthen
where v = W v,v = W v and p = W y,s = conductevaluationofdual-channellearning.
p vp s vs j pv j j
W svz j (for simplicity we dropped subscript i). v is the Our dual-channel ranking method improves perfor-
vector learning from our GRU encoder-decoder model for mance. We compare our dual-channel ranking approach
videoclipv i,y jistheaverageofword2vecvectorsforeach with Canonical Correlation Analysis (CCA) which com-
wordincandidatep ij,z j istheskip-thoughtvectorforde- putes the directions of maximal correlation between a pair
scriptions ij. Weconstrainthesefeaturerepresentationsto ofmulti-dimensionalvariables. TolearnCCA,wetraintwo
beinunitnorm.Î¸denotesallthetransformationparameters embeddinglayersseparately. ThefirstCCAmapsthesen-
neededtolearninthemodel,W vsandW vparetransforma- tencedescriptiontovisualsemanticjoint-embeddingspace
tionsthatmapvisualrepresentationtosemanticjointspace, and the second one maps the correct answer to the joint
whileW svandW pvtransformsthesemanticrepresentation. space. In order to answer multiple-choice questions, we
NotethatW xxcanbealineartransformationormulti-layer embed each candidate and select the answer that is most
neuralnetworkswithhiddenunits. similartothevideoclipbyEquation6. Weconductcross-
Training.Duringtrainingprocedure,wesamplefalseterms validation to choose the weight to combine two embed-
fromnegativecandidatesandpracticallystopsummingaf- dings.
ter first margin-violating term was found [9]. Empirically, For both methods, we restrict the input features to be
we choose the sentence embedding dimension to be 500 thesame. Forvisualrepresentation,weaverageframe-level
and word embedding to be 300. The model is trained by features extracted from the last fully connected layer of
stochastic gradient descent (SGD) by simply setting the GoogLeNet. Forsemanticrepresentation
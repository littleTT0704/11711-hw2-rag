 explanation generation and valence labeling abil-
UEPRISM (Table 8). The topic word cloud (Fig. 8) shows
ities of each model using 700 predefined values, rights,
thatVALUEPRISMcoversabroadspectrumofcommontop-
and duties from the test set of VALUEPRISM. Crowdwork-
icslike”save”,”kill”,and”helping”forsituationsand”re-
ersweretaskedwithevaluatingthequalityofexplanations,
spect”,”care”,and”promote”forvalues.Clusturingshows
their effectiveness in linking values to actions, and agree-
thatthecorpusencompassawidevarietyofthemes,reflect-
ment with valence labels. As depicted in Table 3, the 11B
ingthediversityandrichnessofsituationsandvalues,rights,
model’sperformancecloselyalignswiththatofGPT-4.The
andduties.Formoredataanalysis,seeApp.C.
11B model achieved Valence accuracy within a 1% differ-
ence from GPT-4 and slightly outperformed it in terms of
5 Experiments
Explanationquality.
5.1 OurSystemAgainsttheTeacher
5.2 RelevanceCorrelateswithHumanJudgments
Generating correct and complete sets of values Central
toourresearchisthecapabilitytomodelpluralisticvalues, WewouldlikeKALEIDOtopredictwhetherahumanwould
rights, and duties. Ideally, these values should be correct, find a value, right, or duty relevant. However, its training
havehighcoverage,andbealignedwithhumanpreferences. data is synthetic, so the model instead predicts whether a
Werecruitcrowdworkerstoevaluate KALEIDOSYS directly givenvaluewaslikelytobegeneratedforaparticularsitu-
againstGPT-4acrossthesethreedimensions. ation by GPT-4. To test how well this correlates with how
WerunseveralvariationsofKALEIDOSYS:allmodelsizes
humansjudgerelevance,wecollect18annotationseachfor
(60M–11B);3
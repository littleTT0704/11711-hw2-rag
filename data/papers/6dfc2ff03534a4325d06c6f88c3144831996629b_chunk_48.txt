→A
212,923
25.0 27.6 53.8
questionansweringtasks,withoutvision? QA→R 25.0 26.3 64.1
VCR
To be more general, we will consider problems where
small Q→A 25.0 25.5 39.9
amodelisgivenaquestionandanswerchoices, andpicks VCR 9,848
small QA→R 25.0 25.3 50.9
exactlyoneanswer.Theanswerchoicesaretheoutputsthat VCR
themodelisdecidingbetween(liketheresponsesin VCR) Table 5: Text-only results on the validation sets of vision
and the question is the shared input that is common to all datasets, using BERT-Base. # shows the number of
train
answer choices (the query, image, and detected objects in training examples. A corresponds to only seeing the an-
VCR).Withthisterminology,wecancategorizeunwanted swer;inQ+Athemodelalsoseesthequestion;inS+Q+A
datasetpriorsinthefollowingways: themodelalsoseessubtitlesfromthevideoclip. Thesere-
• Answer Priors: A model can select a correct answer sultssuggestthatmanymultiplechoiceQAdatasetssuffer
withoutevenlookingatthequestion. Manytext-only fromannotationartifacts,whileAdversarialMatchinghelps
datasets contain these priors. For instance, the Roc- produce a dataset with minimial biases; moreover, pro-
Stories dataset [53] (in which a model must classify viding extra text-only information (like subtitles) greatly
endingstoastoryascorrectorincorrect),amodelcan boostsperformance. Moreinfo:
obtain 75% accuracy by looking at stylistic features
♠: Stateoftheart.
(suchaswordchoiceandpunctuation)intheendings.
• Non-Visual Priors: A model can select a correct an- ♣: Only45%(879/1958)ofthequestionsintheMovieQAvali-
dationsethavetimestamps,whichareneededtoextractclip-
swer using only non-visual elements of the question.
levelsub
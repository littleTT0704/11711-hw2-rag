.2013. Mining
about CrystalBLEU (Eghbali and Pradel, 2022).
sourcecoderepositoriesatmassivescaleusinglan-
This material is partly based on research spon-
guage modeling. In 2013 10th working conference
sored in part by the Air Force Research Labora- onminingsoftwarerepositories(MSR),pages207–
toryunderagreementnumberFA8750-19-2-0200. 216.IEEE.
The U.S. Government is authorized to reproduce
Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian
anddistributereprintsforGovernmentalpurposes
Wang, Xiaopeng Li, Yuchen Tian, Ming Tan,
notwithstanding any copyright notation thereon.
Wasi Uddin Ahmad, Shiqi Wang, Qing Sun,
The views and conclusions contained herein are Mingyue Shang, et al. 2022. Multi-lingual evalu-
those of the authors and should not be interpreted ation of code generation models. ArXiv preprint,
abs/2210.14868.
as necessarily representing the official policies or
endorsements, either expressed or implied, of the
JacobAustin,AugustusOdena,MaxwellNye,Maarten
Air Force Research Laboratory or the U.S. Gov- Bosma, Henryk Michalewski, David Dohan, Ellen
ernment. Thisprojectwasalsopartiallysupported Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.
byagiftfromAWSAI. 2021. Programsynthesiswithlargelanguagemod-
els. ArXivpreprint,abs/2108.07732.
Limitations
SatanjeevBanerjeeandAlonLavie.2005. METEOR:
An automatic metric for MT evaluation with im-
CodeBERTScore requires a GPU for computing
proved correlation with human judgments. In Pro-
themetric,whiletraditionalmetricssuchasBLEU
ceedingsoftheACLWorkshoponIntrinsicandEx-
require only a CPU. This adds a hardware re- trinsic Evaluation Measures for Machine Transla-
quirement to the evaluation of models of code, tion and/or Summarization, pages 65–72, Ann Ar-

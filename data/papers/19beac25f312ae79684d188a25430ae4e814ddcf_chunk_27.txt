
Thesimulatorcommunicatestoagentsprimarilywithsen- learning (IL). The L2R dataset contains multi-sensory
soryinformationincluding: input at a 100-millisecond resolution, in both the obser-
vation and action spaces. See Table 1 in the main paper
• LiDARdatafrom4independentsensors
for a complete list of available modalities. The expert
• RADARdatafromthevehicle’sradarsensor demonstrations were collected using a model predictive
controller(MPC)thattracksthecenterlineoftheracetrack
• Imagesfromthefront-facingcamera
at a pre-specified reference speed. Important parameters
• Pose information from the inertial measurement unit forthiscenterlineMPCexpertincludedaccelerationrange
onthevehicle of [-1, 1], steering range of [-1, 1], and image H×W
dimensions of 384 × 512. This training dataset contains
• Additional data about the state of the vehicle such as
10,600 samples of each sensory and action dimension, in
brakepressureandtirespeedperwheel
this first version, which includes 9 complete laps around
• Ground-truthinformationaboutothervehicles thetrack. Demonstrationsweresavedasinvidualstep-wise
transitions, using numpy.savez compressed1, with
• Ground-truthinformationaboutvirtualobjects the following as dict fields in the data: (i) img with
shape (384,512,3); (ii) multimodal data with shape
The camera publishes images using Transmission Con-
(30,); (iii) and action with shape (2,). The fields in
trol Protocol (TCP) while the others publish sensory data
multimodal data correspond to the vector dimension
usingUserDataProtocol(UDP)oroveraControllerArea
mappings,indicatedinTable5.
Network(CAN).WhiletheLearn-to-Raceframework
Future version releases of L2R will include access to
exclusively supports software-in-the-loop simulation, and
newsimulatedtracks(alsomodelledafterrealtracks,from
therefore, only virtual CAN buses, the racing simulator
alsosupportshardware-in-the-loopsimulationandphysical 1https://numpy.org/doc/stable/reference
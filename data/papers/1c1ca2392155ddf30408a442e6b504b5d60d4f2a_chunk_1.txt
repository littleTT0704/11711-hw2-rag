When to Make Exceptions: Exploring Language
Models as Accounts of Human Moral Judgment
ZhijingJin∗ SydneyLevine∗ FernandoGonzalez∗
MPI&ETHZürich MIT&Harvard ETHZürich
zjin@tue.mpg.de smlevine@mit.edu fgonzalez@ethz.ch
OjasvKamal MaartenSap MrinmayaSachan†
IITKharagpur LTI,CarnegieMellonUniversity ETHZürich
maartensap@cmu.edu msachan@ethz.ch
kamalojasv47@iitkgp.ac.in
RadaMihalcea† JoshuaTenenbaum† BernhardSchölkopf†
UniversityofMichigan MIT MPIforIntelligentSystems
mihalcea@umich.edu jbt@mit.edu bs@tue.mpg.de
Abstract
AIsystemsarebecomingincreasinglyintertwinedwithhumanlife. Inorderto
effectively collaborate with humans and ensure safety, AI systems need to be
abletounderstand,interpretandpredicthumanmoraljudgmentsanddecisions.
Human moral judgments are often guided by rules, but not always. A central
challengeforAIsafetyiscapturingtheflexibilityofthehumanmoralmind—the
abilitytodeterminewhenaruleshouldbebroken,especiallyinnovelorunusual
situations. In this paper, we present a novel challenge set consisting of moral
exceptionquestionanswering(MoralExceptQA)ofcasesthatinvolvepotentially
permissiblemoralexceptions–inspiredbyrecentmoralpsychologystudies. Using
astate-of-the-artlargelanguagemodel(LLM)asabasis,weproposeanovelmoral
chainofthought(MORALCOT)promptingstrategythatcombinesthestrengthsof
LLMswiththeoriesofmoralreasoningdevelopedincognitivesciencetopredict
humanmoraljudgments. MORALCOToutperformssevenexistingLLMsby6.2%
F1,suggestingthatmodelinghumanreasoningmightbenecessarytocapturethe
flexibilityofthehumanmoralmind
,aswementionedinSection3.1,thedataclasses toright. Eachtimewegrowathree-nodesubtree. Wekeep
arenotavailable,andthenon-leafnodethresholdtisun- increasing the number of nodes until the lower bound Q
known. Weprovideanalternativeapproachtomitigatethis stopsincreasingortheratioofthenumberofsamplesatthe
HierarchicalRoutingMixtureofExperts
leaf to the total number of samples is below some preset literatureunderthesameexperimentsettings.
threshold.
HRME For our HRME model, we train it following Al-
gorithm1. Inourinstantiationofthemodel,thenon-leaf
4.Experiments expertsaresupportvectormachineswithradialbasisfunc-
tion kernels (SVM-RBF). We choose two simple models
Inthissection, weevaluateourHRMEmodelandthere-
forleafexperts,thelinearregressionmodel(referredtoas
cursiveEMalgorithmonacollectionofstandardregression
HRME-LR) or the support vector regression model with
datasets. Wedescribetheexperimentsettingsandpresent
radial basis function kernel (referred to as HRME-SVR).
the results for our method and a wide range of baseline
Similar to the training of baselines, our models are also
methods.
trainedandfine-tunedonthesametrainingsetsfollowing
thesamestrategywiththebaselinemethods. Inaddition,all
Table1. DatasetStatistics non-leafexpertsonthetreesharethesamehyperparameters,
soaretheleafexperts. Althoughitwouldbedesirableto
DATASET FEATUREDIM TRAIN TEST
usedifferenthyperparametersfornodesondifferentdepth
3-LINES 1 1750 750
of thetree asthe datasize shrinkswith the treedepth, in
HOUSING 13 354 152
CONCRETE 8 721 309 practicewefindourmodelisrobusttosuchvariations.
CCPP 4 6697 2871
ENERGY 28 14803 4932 4.3.Results
KIN40K 8 10000 30000
Weevaluateourmethodsand
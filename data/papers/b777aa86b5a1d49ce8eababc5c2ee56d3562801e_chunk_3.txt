
acceleratorshaveseensimilarperformancegains PyTorch, just-in-time compiled TorchScript, and
with the number of floating point operations per ahead-of-time compiled ONNX runtime using a
second(FLOPS)growingbyover175%.2 CUDAexecutionprovider. Weperformourstudy
Despitethisprogress,thesupposedgainsoffered acrosssevendifferentGPUsfromthePascal,Tur-
by higher performance hardware and more effi- ing,andAmpereNvidiaGPUmicroarchitectures.
cient models are often not realized in inference Based on our findings, we provide a series of
settings,wherewall-clockmodelspeedhasnotre- recommendationsforNLPresearchersandpracti-
liably improved, as seen in Figure 1. We show tionerspresentedthroughacollectionofcasestud-
thatthismisalignmentisprimarilyattributableto ies. Amongthese,werecommendusageofstatic
overhead incurred by deep learning frameworks or ahead-of-time inference runtimes when batch
usedtoimplementandexecutemodels. Inthepast, sizes are small as they can substantially reduce
theexecutionoflargeneuralnetworkscommonto framework overhead. Alternatively, when using
naturallanguageprocessinghasbeenassumedto eager execution-based frameworks for inference,
becompute-boundedbycomputationallyintensive we recommend increasing model width or batch
tensor operations (Li et al., 2020). However, as sizeatnocosttolatencyandtakeadvantageofper-
thespeedofhardwareincreases,theoverheadin- formance gains associated with increased model
troducedbythedeeplearningframeworksusedto capacity(ZagoruykoandKomodakis,2016). For
implement and deploy these models is no longer example,hiddendimensionsinself-attentionand
negligibleandimposesbottlenecksoninference. fullyconnectedlayerscanbedoubledtoincrease
Inthiswork,weconductasystematicinvestiga- model capacity without affecting latency, imply-
tioninwhichweshowthatimprovementstoneural ing that model designers have an extra degree of
networkarchitecturesandhardwarehavenottrans- freedomoftenoverlookedwhendesigning
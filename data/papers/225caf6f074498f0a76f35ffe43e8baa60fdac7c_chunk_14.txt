hecosineandmargin
basedscoring),NQ,andMKQA.
6.4 ExperimentalSettings
6.3 Baselines
Weexplorethreedifferentsettingsforeachoffour
WecompareVMSSTagainsttwostrongbaselines, objectivefunctionsweconsider. WeusetheTrans-
whichhavebeenusedextensivelyintheliterature. formerarchitectureforallsettings. Specifically,we
Thefirstbaselineis CONTRASTIVE,wherewe explorea6layerencoder-decodermodel,a24layer
use contrastive learning with the other sentences encoder-decoder model, and a 24 layer encoder-
inthebatch(“in-batchnegativesampling”)asneg- decoderinitializedwiththeMultilingualT5(mT5)
ative examples (Sohn, 2016). CONTRASTIVE is Large(Xueetal.,2021). Wesetthedimensionof
computedastheaverageofcomputingp(s |t )and theembeddingsandhiddenstatesfortheencoders
i i
p(t |s )forsourcesentences andtargetsentence anddecodersto1024. ThemT5Largemodelinher-
i i i
t, and their respective representations s and t entlyhasembeddingandhiddenstatedimensions
i i i
where the first term uses all the other targets as of1024. Forallmodels,weusethemT5vocabu-
negativesandthesecondusealloftheothersource lary,whichisderivedfromsentencepiece(Kudo
sentenceasnegatives. Specifically, and Richardson, 2018). The vocabulary consists
of250,000tokensandwaslearnedfrommultilin-
gual variant of the C4 dataset called mC4 which
(cid:88)
p(s |t ) = exp(s ·t )/ exps ·t includes101languages.
i i i i i j
j∈B Foroptimization,weuseAdafactor(Shazeerand
(cid:88)
p(t |s ) = exp(s ·t )/ expt ·s Stern,2018). Weusethesamelearningratesched-
i i i i i j
uleasVaswanietal
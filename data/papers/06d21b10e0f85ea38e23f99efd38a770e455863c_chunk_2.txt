in
models are orders of magnitude faster than thenumberofsentences,orevensub-linearwhen
otherstrongsimilaritymodelsandcanbeused usinghighlyoptimizedtoolslikeFaiss(Johnson
on CPU with little difference in inference etal.,2017)thatallowforefficientnearestneigh-
speed (even improved speed over GPU when
borsearch. Thisiscontrasttomodels, likecross-
usingmoreCPUcores),makingthesemodels
attentionmodels,whicharequadraticduringinfer-
an attractive choice for users without access
ence as they require both of the texts being com-
to GPUs or for use on embedded devices. Fi-
paredasinputs. Asweshowinthispaper,oursim-
nally, weaddsignificantlyincreasedfunction-
ality to the code bases for training paraphras- pleandinterpretableword-averagingsentenceem-
tic sentence models, easing their use for both beddingmodels(Wietingetal.,2016b;Wietingand
inferenceandfortrainingthemforanydesired Gimpel, 2018; Wieting et al., 2019b), are orders
language with parallel data. We also include ofmagnitudefastertocomputethanpriorembed-
code to automatically download and prepro-
dingapproacheswhilesimultaneouslypossessing
cesstrainingdata.1
significantlystrongerperformanceonmonolingual
1 Introduction andcross-lingualsemanticsimilaritytasks. Since
wearesimplyaveragingembeddingsandhaveno
Measuringsentencesimilarity(Agirreetal.,2012)
neural architecture, any models based on neural
is an important task in natural language pro-
architectures,especiallylargepretrainedneuralar-
cessing, and has found many uses including
chitectureswhichareincreasinglyused,willnotbe
paraphrase detection (Dolan et al., 2004), bitext
asfastasthemodelsdescribedinthispaper. Lastly,
mining (Schwenk and Douze, 2017), language
wealsoshowthatthisapproachiscompetitivewith
modelling (Khandelwal et al., 2019), question-
LASER(ArtetxeandSchwenk,2019),astate-of-
answering(Lewiset
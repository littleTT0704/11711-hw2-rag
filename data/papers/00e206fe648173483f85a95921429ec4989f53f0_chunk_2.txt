performanceonstandardbenchmark Coreferencemodelsstruggletoproducemeaning-
datasets such as the CoNLL-2012 shared task, fulrepresentationsfornewdomain-specificspans
where average F1 has improved by 20% since andmayrequiremanyexamplestoadapt(Uppunda
2016(DurrettandKlein,2013;Dobrovolskii,2021; etal.,2021;LuandNg,2020;Zhuetal.,2021).
Kirstain et al., 2021). Modern coreference archi- Further,coreferencemodelstrainedonstandard
tectures typically consist of an encoder, mention benchmarksarenotrobusttodifferencesinanno-
detector,andantecedentlinker. Allofthesecom- tation schemes for new domains (Bamman et al.,
ponents are optimized end-to-end, using only an 2020). Forexample,OntoNotesdoesnotannotate
antecedentlinkingobjective,soexpensivecorefer- singletonmentions,thosethatdonotcoreferwith
ence chain annotations are necessary for training anyothermention. AsystemtrainedonOntoNotes
(AralikatteandSøgaard,2020;Lietal.,2020a). would implicitly learn to detect only entities that
Theseresultshaveencouragedinterestindeploy- appearmorethanonce,eventhoughsingletonre-
ingmodelsindomainslikemedicineandchildpro- trieval is often desired in other domains (Zeldes,
tectiveservices,whereasmallnumberofpractition- 2022). Also,practitionersmayonlybeinterested
inretrievingasubsetofdomain-specificentities.
1Code is available at https://github.com/
nupoorgandhi/data-eff-coref Continuedtrainingontargetdomaindataisan
10543
Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics
Volume1:LongPapers,pages10543–10558
July9-14,2023©2023AssociationforComputationalLinguistics
effectiveapproach(XiaandVanDurme,2021),but 7-14%improvementsinF1across3domains,we
it
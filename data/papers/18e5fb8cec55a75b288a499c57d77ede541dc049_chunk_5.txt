
placed atop pre-trained LMs, such as BERT (Devlin et al. Zero-ShotQAFramework
2019),RoBERTa(Liuetal.2019),andGPT(Radfordetal.
GivenanaturallanguagequestionQ,andnpossibleanswers
2019). As shown by Ma et al. (2019) and Mitra et al.
A ;:::;A,thetask istoselectthemostprobablesinglean-
(2019), combining neural methods with structured back- 1 n
swerA.Werefertotheremainingn(cid:0)1possibleanswers:
ground knowledge from ConceptNet, WordNet (Miller
D ;:::;D as distractors. In a zero-shot QA evaluation
1995),andATOMIC workswellforcommonsensedatasets 1 n(cid:0)1
mode, the system has no access to the task training or de-
that have been partially derived from these resources, such
velopmentdata.Weassumeasetupwherethesystemispre-
as SocialIQA and CommonSenseQA. Here, the struc-
trainedonceandthenappliedacrossdifferenttasksinazero-
turedknowledge,formalizedaslexicalizedtask-targetedev-
shotmanner.Ourzero-shotevaluationframeworkaddresses
idencepaths,isinjectedintoanLM,eitherviaanattention
this task by variants of pre-training an LM on an artificial
mechanism (Bauer, Wang, and Bansal 2018) or through an
QAset,createdfromKGdata.Next,wedescribeitscovered
auxiliarytrainingobjective(Xia,Wu,andYan2019).Graph
tasks,sourcesofknowledge,questiongenerationstrategies,
andrelationnetworkscanalsobeusedtoscoreanswercan-
LMtechniques,andtrainingregimes,inturn.
didates, by informing the graph structure with data from
LMs(Linetal.2019;Zhongetal.2019).Finally,complete
SyntheticQAGeneration
KGs can be incorporated directly in training by introduc-
We generate questions, answers, and distractor op-
ing additional modeling objectives, to teach a
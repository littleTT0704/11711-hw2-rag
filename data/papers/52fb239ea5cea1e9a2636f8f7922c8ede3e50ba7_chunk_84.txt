MattGard-
ner. 2019a. Orb: An open reading benchmark for comprehensive evaluation of
machine reading comprehension. arXiv preprint arXiv:1912.12598.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh,
and Matt Gardner. 2019b. Drop: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1 (Long and Short Papers),
pages 2368–2378.
KarënFort, GillesAdda, andKevinBretonnelCohen.2011. Amazonmechanical
turk: Gold mine or coal mine? Computational Linguistics, pages 413–420.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster,JasonPhang,HoraceHe,AnishThite,NoaNabeshima,ShawnPresser,
and Connor Leahy. 2020. The Pile: An 800gb dataset of diverse text for
language modeling. arXiv preprint arXiv:2101.00027.
Jesse Michael Han, Jason M. Rute, Yuhuai Wu, Edward W. Ayers, and Stanislas
Polu. 2021. Proof artifact co-training for theorem proving with language
models. ArXiv, abs/2102.06203.
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,
Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al.
2021a. Measuring coding challenge competence with apps. arXiv preprint
arXiv:2105.09938.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart,
EricTang,DawnSong,andJacobSteinhardt.2021b. Measuringmathematical
problem solving with the math dataset. arXiv preprint arXiv:2103.03874.
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh
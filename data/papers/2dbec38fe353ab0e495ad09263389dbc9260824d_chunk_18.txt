answerstopreviouslysolvedsub- monlyfoundindailydocumentsandcontainhierar-
problems. Similarly, Khot et al. (2022) leverage chicallystructuredinformation,havealsobeenthe
diverse decomposition structures and use differ- focus of tasks that require quantitative reasoning
ent prompts to answer each sub-question. Apart overtextualandtabularcontext(Chenetal.,2021c;
from these multi-step reasoning methods, Chen Zhuetal.,2021;Zhaoetal.,2022;Luetal.,2022b).
etal.(2022b);Gaoetal.(2022)proposeprogram- Inaddition,recentdatasetshavebeendevelopedfor
of-thoughts(PoT),analternativesolutionthatuses mathematicalreasoninggroundedonconversations
large language models to express the reasoning (Sun et al., 2019; Zhang et al., 2021; Chen et al.,
process as a program. The computation is then 2022c),aswellasreports(Chenetal.,2022c).
relegatedtoanexternalcomputer,whichexecutes
Pioneeringworkisemergingintheexploration
the generated programs to derive the answer. A
oflow-resourcesettings. Despitethecreationof
more recent work, Chameleon (Lu et al., 2023),
various datasets, mathematical reasoning in low-
integratesdifferenttoolstoenhancetheabilitiesof
resource settings remains largely under-explored.
LLMsforcompositionalreasoning.
Pioneering research has developed mathematical
reasoning benchmarks for financial (Chen et al.,
Outcome-based approaches acknowledge the
2021c; Zhu et al., 2021; Zhao et al., 2022) and
potential incorrectness of an individual reason-
scientific domains (Lu et al., 2022a). Addition-
ing path, and instead use multiple reasoning
ally,therehavebeenattemptstobuildnon-English
paths (Wang et al., 2023; Li et al., 2022a). Self-
datasetsforChinese(Wangetal.,2017;Qinetal.,
consistency(Wangetal.,2023)generatesasetof
2020;Yuetal.,2021a)andArabic(Alghamdietal
iversity × Uncertainty
Diversity → Uncertainty
40%
Supervised
35%
0 200 400 600 800 1000
Steps
Figure8.3: ActivelearningcurvesforSVMswithlinearkernelsthatmakeindependent
predictions using only the original features.
85%
80%
75%
70%
65%
P
A 60%
M
Random
55%
Maximum Score
Diversity
50%
Uncertainty
45%
Diversity × Uncertainty
Diversity → Uncertainty
40%
Supervised
35%
0 200 400 600 800 1000
Steps
Figure 8.4: Active learning curves for SVMs with linear kernels that include features
of adjacent instances to capture dependencies.
8.1. ACTIVE LEARNING 129
Diversity → Uncertainty combines the relative advantages of the individual query
selection criteria by first exploring the feature space to avoid local optima and se-
lecting hard instances in later iterations to further refine the model. Note that this
hybrid approach selects the first 200 queries based on the diversity criterion and thus
the learning curves for this approach and diversity sampling are initially very similar
(though not identical because the first two queries in each active learning run are
selected at random). Once the hybrid method switches to the uncertainty criterion,
it outperforms diversity sampling in all settings. It is also more robust and on average
more effective than the single-strategy uncertainty approach when using logistic re-
gression. After 1,000 steps, the performance of the cross-over method comes close to
the strategy that continuously selects queries based on a combination of diversity and
uncertainty. However, we may have slightly overestimated its effectiveness since we
chose a cross-over point based on the learning curves for the single-strategy methods
on the same dataset.
The learning curves for LR models and SVMs with features of adjacent instances
convergemoreslowly, whichisnotsurprisingbecausemoretrainingdataisneededifa
largerfeaturesetisusedtoavoidoverfitting. After1,000steps, manyoftheselearning
curves still have distinctly positive slopes, and thus performance could be improved
further by labeling additional queries. It can also be seen that SVMs converge faster
than LR models, which makes sense because the decision boundary of an SVM can be
defined by a small number
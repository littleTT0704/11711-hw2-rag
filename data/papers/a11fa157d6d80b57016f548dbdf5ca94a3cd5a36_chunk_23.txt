
Tianqing Fang, Hongming Zhang, Weiqi Wang, YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
Yangqiu Song, and Bin He. 2021. Discos: Bridg- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
ingthegapbetweendiscourseknowledgeandcom- Luke Zettlemoyer, and Veselin Stoyanov. 2019.
monsense knowledge. In Proceedings of the Web Roberta: A robustly optimized bert pretraining ap-
Conference2021,pages2648–2659. proach. ArXiv,abs/1907.11692.
JoshuaFeldman,JoeDavison,andAlexanderM.Rush. YiLuan,DaveWadden,LuhengHe,AmyShah,Mari
2019. Commonsenseknowledgeminingfrompre- Ostendorf,andHannanehHajishirzi.2019. Ageneral
trainedmodels. InEMNLP. frameworkforinformationextractionusingdynamic
spangraphs. arXivpreprintarXiv:1904.03296.
ChristianeD.Fellbaum.2000. Wordnet: anelectronic
lexicaldatabase. Language,76:706. Benjamin Newman, Prafulla Kumar Choubey, and
NazneenRajani.2021. P-adapters: Robustlyextract-
Google.2012. Introducingtheknowledgegraph:things, ingfactualinformationfromlanguagemodelswith
notstrings. diverseprompts. ArXiv,abs/2110.07280.
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Tuan-Phong Nguyen, Simon Razniewski, Julien
ZhenWang,DaisyZheWang,andZhitingHu.2023. Romero,andGerhardWeikum.2021. Refinedcom-
Reasoning with language model is planning with monsenseknowledgefromlarge-scalewebcontents.
worldmodel. arXivpreprintarXiv:2112.04596.
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, An-
PeterHase,MonaT.Diab,AsliÇ
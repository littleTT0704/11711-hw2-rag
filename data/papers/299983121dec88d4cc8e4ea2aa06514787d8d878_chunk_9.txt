generatingfinal
gerated.Thisseepsintolargepretrainedtextmodels(Shwartz
texts.Hence,wetryexperimentsusingdifferentnumbersof
andChoi2020).Usingvisualdataandmodelsdampensthis
topcaptionswithinS,aparameterwecallNTC (Number
bias,likelyimprovingthecommonsenseofgenerations. c′
of Top Captions). We try NTC = 1,2,3,5,7,10, and do
notgoaboveNTC = 10asFigure1showsthatcoverage
4 Methodology
gainsfrom10→30areminor.Figure1alsoillustratesthat
captionshaverelativelylowindividualcoverage,especially
4.1 ImageRetrieval
comparedwithoutputsfrommodelstrainedonCommonGen,
Wefirstobtainimagesforeachconceptsetinourthreesplits. whichiswhywedonotusethemasabaseline.
ImagecaptioningdatasetssuchasMSCOCOandFlickrare Thecaptionsareconcatenatedtogetherandontothecon-
typicallytoosmallandfocusedtobeeffectiveforourpur- cept set using <s> separator tokens. These serve as aug-
posessincewemustcovernumerousdifferentconceptsets. mentedinputstoBARTandT5.Theylearntoconvertthese
Further,asearchengineismoregeneralizable.
WedecidetouseGoogleImages.Onasampleofconcept 2https://pypi.org/project/simple-image-download/
sets,theretrievedimagesusingothersearchengineswerein- 3https://pypi.org/project/Pillow/
appropriate;theydidnotincorporatemostinputkeywordsnor 4https://github.com/ruotianluo/self-critical.pytorch
handlehomonymswell.Forexample,“sports+fan+watch” 5SeeAppendixBforfurthercaptioningmodeldetails.
10620
AugmentedInput→FinalGeneration
wavefallboardsurfer<s>asurferridingawaveonasurfboard→Asurferisfallingoffhisboardintothewaves.
danc
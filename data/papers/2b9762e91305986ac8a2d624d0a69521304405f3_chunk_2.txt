anceobservedinEnglishtransferlearning(Wang
agnostic suite (MULTICHECKLIST) and fine-
etal.,2019a),whichhasrecentlybeencloseden-
grained multi-dataset evaluation capabilities
through an interactive public leaderboard to tirelyonsomeevaluationsuites(Heetal.,2021).
gainabetterunderstandingofsuchmodels. Inordertoexaminethenatureofthisprogress,
wefirstperformananalysisofstate-of-the-artmul-
1 Introduction
tilingual models on XTREME. We observe that
Most research in natural language processing progresshasnotbeenuniform,butconcentratedon
(NLP)todatehasfocusedondevelopingmethods cross-lingualretrievaltaskswherefine-tuningon
thatworkwellforEnglishandasmallsetofother othertasksandpre-trainingwithparalleldatalead
high-resource languages (Joshi et al., 2020). In to large gains. On other task categories improve-
contrast,methodsforotherlanguagescanbevastly ments are more modest. Models still generally
morebeneficialastheyenableaccesstolanguage performpoorlyonlanguageswithlimiteddataand
technologyformorethanthreebillionspeakersof non-Latinscripts. Fine-tuningonadditionaltrans-
low-resourcelanguagesandpreventtheNLPcom- lateddatagenerallyleadstothebestperformance.
munityfromoverfittingtoEnglish. Motivatedby Basedonthisanalysis,wepropose XTREME-R
these benefits, the area of multilingual NLP has (XTREME Revisited), a new benchmark with the
attractedincreasinginterestrecently. dualpurposeofensuringthatresearchinmultilin-
However,evaluatingmultilingualmodelsischal- gual NLP focuses on the most challenging prob-
lengingasitrequiresassessingperformanceona lemsandequippingresearcherswithabroaderset
wide range of typologically distinct languages in oftoolstobetterunderstandtheirmodels(seeTa-
thefaceoflimitedheterogeneousdatasources. Re- ble 1 for a brief overview). XTREME-R follows
cently large-scale benchmarks such as XTREME in its predecessorâ€™s footsteps
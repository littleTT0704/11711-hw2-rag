.94 86.81 5.92 98.87 83.33 97.17 60.33
0.1 0.2 0.6 0.7 0.0 3.4 0.1 3.8
Table2: EvaluationoflexicalbiasremovalforalldebiasingmethodsontheFountaetal.(2018)testset. Results
showthemeanands.d.(subscript)ofaccuracyandF across3runs,aswellasF andfalsepositiverateexclusively
1 1
ontestexamplescontainingspecificTOXTRIGmentions—NOI,OIandONI,alongwiththenumberofexamples
in each category. The lower the FPR, the less the model infers lexical associations for toxicity. The first block
*
shows debiased training approaches, along with the vanilla classifier, which are trained on the full dataset. The
second block shows data filtering approaches, all trained on only 33% of the training data. Best performance in
eachblockisboldfaced. Takeaway: Whiledatafilteringapproachesachieveoverallhigherperformance,debiased
trainingapproachesperformbetteronlexicalbiasreduction,inaggregate.
Gold DM- DM-
Hard Easy
@userTHIS?LMAOOOOO...doyourselfasolidandstayoutofBlackpeople’smentionsand ¤ A ¤
mindyourcaucasia...
RT@userIwishIwasn’tsoannoyinglikeIevenpissmyselfoff A ¤ A
@user If you want to attack people, attack fundamentalists of all faiths. Attack those who ¤ A ¤
condemn1.5bnpeopleoutofhand.
Table3: Examplesoftestsettweetswiththeirgold-standardannotationsandpredictionsfrommodelstrainedon
DataMaps-Hard(DM-Hard)andDataMaps-Easy(DM-Easy)subsets. Adenotestweetswithtoxiclabels,and¤
representsnon-toxiclabels. Weanonymizetheusernamestoprotectuserprivacy.
in §4.2. These annotation ambiguities might also than both LMIXIN models. Regardless, none of
impairourmeasurementformodels’performance the models we test
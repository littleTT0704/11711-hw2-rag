� aˆi i,ai) +
(cid:17)
ˆbi
.Packingefficiencyisbetween0to1,andhigherisbetter.Notethatifthepolicyfollows
max(ˆbi,bi)
thewrongpreference,thenPEis0,evenifthedishwasherisfull.
InverseEditdistance:Wealsocalculateistheinverseeditdistancebetweenthesequenceofactions
takenbytheexpertversusthelearnedpolicy. WecomputetheLevenshteindistance4 (LD)between
the policy’s and expert’s sequence of pick and place instances. Inverse edit distance is defined as
ED = 1−LD;higherisbetter. Thismeasuresthetemporaldeviationfromtheexpert,insteadof
justthefinalstate. Iftheexpertpreferencewastoloadthetoprackfirstandthelearnedpolicyloads
thebottomfirst,PE wouldbeperfect,butinverseeditdistancewouldbelow.
3“ReplicaSyntheticApartment0Kitchen”wascreatedwiththeconsentofandcompensationtoartists,and
willbesharedunderaCreativeCommonslicensefornon-commercialusewithattribution(CC-BY-NC).
4Levenshtein distance= textdistance(learned seq, expert seq) / len(expert seq)
5
(a)Packingefficiency (b)InverseEditDistance
Figure5: ComparisonsofTTP,GNN-IL/RLandaRandompolicyinsimulationacrosstwometrics.
TTPshowsgoodperformanceatthetaskofdishwasherloadingonseenandunseenpreferences,and
outperformstheGNNandrandombaselines. HoweverTTP’sgeneralizationtounseen#objectsif
worse,butstillclosetoGNN-IL,andbetterthanGNN-RLandrandom.
3.1 Evaluationonsimulateddishwasherloading
Baselines: We compare our approach against Graph Neural Network (GNN) based preference
learning from [23] and [24]. Neither of these works are directly suitable for our task, so we com-
bine them to create a stronger baseline. We use ground-truth preference labels, represented as a
categorical
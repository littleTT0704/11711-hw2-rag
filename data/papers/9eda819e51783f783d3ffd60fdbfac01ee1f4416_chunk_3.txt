 the multilingual scenario (e.g.,
∗Correspondingauthor sharedfeaturesacrosslanguages)toderivediffer-
1Codeathttps://github.com/qinyiwei/Multi-Sum.git entarchitecturesformultilinguallearning. These
2202
ceD
21
]LC.sc[
1v04750.2122:viXra
frameworks encompass some existing works on reduce the negative effect of the limited capacity
multilinguallearning,butalsoallowustopropose ofoneLMsharedbyalllanguages. Weachievea
new learning methods and perform comparisons newstate-of-the-artperformancewithsuchamulti-
betweendifferentframeworks. lingualtuningstrategy.
Figure 1 (a) shows a commonly-used frame-
2 Preliminaries
work (Hasan et al., 2021) in which one tunable
pre-trainedmodelissharedbydifferentlanguages. 2.1 TaskFormulation
Figure 1 (b) introduces a language-specific mod-
Abstractive summarization can be formulated as
ule with learnable parameters while keeping the
a conditional generation task where the input D
PLM’s parameter frozen, i.e. parameter-efficient
is a document, and the output S is a short sum-
tuning(Maoetal.,2021;Heetal.,2021). Inprac-
mary. The majority of state-of-the-art models
tice,thelanguage-specificmodulecouldbeinstan-
forabstractivesummarizationuseencoder-decoder
tiatedasanadapter,prefix,orothervarietyofextra
models(Sutskeveretal.,2014),whereanencoder
parameters. Notably,thiskindoflanguage-specific
generatesrepresentationsforthesourcedocument
moduleisindependentforeachlanguageandcan-
D = [d,...,d ],andadecoderoutputsthesum-
notshareinformation,thuslow-resourcelanguages 1 m
mary S = [s,...,s ] one target token at a time.
cannot benefit from other related languages. Fig- 1 n
The conditional probability of a single sample is
ure1(c)triestoalleviatethisproblemintwoways:
modeledasp(
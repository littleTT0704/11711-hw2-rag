.com/Mayer123/HyKAS-CSKG cial tokens (e.g., PersonX). We follow the SocialIQAâ€™s
13508
Question:Robintakesthefifth.Asaresult,Robinwantedto
A1:gotothecinema.
A2:withholdinformation.(*)
A3:hearwhattheythink.
Question:losingweightisfor
A1:beinghealthier.(*)
A2:embeddedsoftware.
A3:buyingthingsinstore.
Figure1:Anillustrationofourquestiongenerationpipeline.
Table1:GeneratedquestionsfromATOMIC (top)andCWWV
(bottom).(*)denotesthecorrectanswer.
ATOMIC train/dev/test splits, to ensure that the facts of the
devandtestpartitionsareexcludedintraining.
Our second partition, CWWV, covers three other KGs in r,t0). ConsideringtheexampleinFigure1,thetriple(gain-
CSKG that express commonsense facts between concepts: ing weight, CausesDesire, change appearance) will be fil-
ConceptNet, WordNet, and Wikidata. We use them teredoutbyrule(1),(losingweight,UsedFor,feelingbetter)
jointly to generate questions, and we enrich them with ad- will be ruled out by both (2) and (3), and (relaxing, Used-
ditional distractors from VisualGenome. Treating these For,feelingbetter)willberuledoutby(3).Here,wereplace
foursourcesasasingleoneisenabledbytheirCSKGmap- anyreferencestofictionalATOMICagentsinthedistractors
ping to a single set of relations, defined by ConceptNet. withthesamenamesusedinthequestion.Wethenrandomly
We focus on 14 semantic relations that are grounded on selecttwodistractors(D,D )fromD.Werefertothisdis-
1 2
strong psycholinguistic and pragmatic evidence (Murphy tractorpoolingstrategyasrandom,andproposethreealter-
2003), like /r/Causes and /r/HasPrerequisite. nativestrategiesinthenextSection.

 in which the labels of relevant
nuggets in the training folds were flipped with increasing probabilities P (Rel). To
Flip
ensure that the total number of positive instances remained roughly the same, we
also flipped the labels of irrelevant training nuggets with probability
# relevant nuggets
P (Irrel) = P (Rel)Ã—.
Flip Flip
# irrelevant nuggets
By flipping labels in both directions, we take into account that annotators not only
overlook relevant text but also mistakenly select irrelevant text.
Figure 5.5 shows MAP scores for varying error probabilities P (Rel). It can be
Flip
seen that relevance estimation performance is not very sensitive to label noise. For
example, even if an annotator mislabeled half of the relevant nuggets and a similar
numberofirrelevantnuggets, MAPwouldonlydegradefrom80.59%to80.11%.3 This
result may at first seem counterintuitive, but note that there would still be a strong
signal in the training data because of the imbalance between positive and negative
instances (only about 7% of all text nuggets are relevant). In the example, 50% of
the text nuggets in the positive class would actually be relevant, whereas less than
4% of the nuggets in the negative class would be relevant. If even more label noise
is added to the dataset, the variance of the evaluation results increases and ranking
performance eventually drops off sharply. The dashed horizontal line illustrates the
average MAP if text nuggets are ranked randomly (28.48%). At P (Rel) = 0.93
Flip
3Here we assume that the labels in our dataset are correct.
64 CHAPTER 5. INTRINSIC EVALUATION
100%
90%
80%
70%
60%
P
A 50%
M
40%
30%
20%
10%
0%
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
P (Rel)
Flip
Figure 5.5: Effect of label noise on relevance ranking performance. The solid curve
illustrates the MAP of logistic regression models trained on datasets with varying
degrees of label noise. The dashed line is the average MAP of random rankings.
the performance of the LR model drops below the random baseline. This makes
convertedtheoriginalsentenceandgeneratedsen-
token),h
R
RKR,intoa3-dimensionalspacerep-
tenceintosentenceembeddingsusingapre-trained âˆˆ
resentingtheunnormalizedprobabilitydistribution
MPnet-basemodel(Songetal.,2020). Finally,the
overeventboundarytypes.
generatedembeddingsandtheoriginalembeddings
arecomparedforcosinesimilarity,whichisusedas Combining predictions We combine predic-
afeature. ExperimentaldetailsareintheAppendix. tionsmadebytheGRU(P )andRoBERTa(P )
G R
byconcatenatingtheirpredictionsandmultiplying
4.2 ModelArchitecture
it with a linear classifier of size (6, 3) to output
Weproposeamodeltointegratefeature-basedpre-
logits of size (3). The logits are then normalized
diction with language-based prediction of event
using Softmax to give a distribution of the three
boundaries,illustratedinFigure2(left). Thepre-
typesofeventboundaries(P). Theweightsofthe
dictionsareindependentlymadewithextractedfea-
linearclassifierareinitializedbyconcatenatingtwo
turesusingagatedrecurrentunit(GRU)andwith
identitymatrixofsize3(I ),whichservestoper-
3
language (i.e., story sentences) using RoBERTa.
formelementwiseadditionbetweenthepredictions
Then these predictions are combined into a final
of the GRU and RoBERTa at early stages of the
predicteddistributionforthethreetypesofevent
trainingprocess.
boundaries. Our model is then trained using the
Kullback-LeiblerDivergenceloss.
W := [I ;I ] (1)
3 3
GRU isusedtocombinefeaturesrelatingthecur-
rentsentenceitopriorsentencesinastory. Itse-
quentiallyconsidersinformationconcerningprior P := Softmax(W([P ;P ])) (2)
G R
5
Lossfunction WeusetheKullback-LeiblerDi- overall noevent expected surprising
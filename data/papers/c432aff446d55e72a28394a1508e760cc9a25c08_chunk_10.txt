estheablationstudiesinthispaper.1 Thisbase
LMhas268Mparameters. Togetaperspectiveonhowlargethedatastoreis,itisbuiltonthetrainingdata
thatcontainsnearly150MBPEtokens,eachpairedwithacontextvectorofsize1024. Thisdatastorehasa
totalmemoryconsumptionofabout300GB.Ateveryretrievalstep,wetakethetop1024nearestneighbors,
i.e.,k =1024,followingKhandelwaletal.(2020b). Theinterpolatedperplexityiscomputedwithoptimal
interpolationparameterλtunedaccordingtotheperplexityonthedevelopmentset. λisfixedduringthe
inferenceforallpredictions,thesameasthestandardkNN-LM.
1BytrainingourownversionofthebaseLMfromscratchwithBPEtokenizationandastandardoutputsoftmaxlayer,
ourLM’sperplexityisworsethanthatusedintheoriginalkNN-LMpaper.However,weobservesimilarrelativegains
fromtheadditionalkNNcomponent.WearguethatthebaseLM’sperformanceisorthogonaltothestudyofthefactors
behindkNN-LM’simprovements.
4
h ⊗ +#params PPL Interp.PPL Oracle
ds
BaseLM - - 0 21.750 - -
kNN-LM-L2 att L2 N ×D ∞ 19.174 14.230
ds
kNN-LM-IP att IP N ×D ∞ 19.095 14.077
ds
kNN-LM-L2 ffn L2 N ×D ∞ 20.734 15.594
ds
kNN-LM-IP ffn IP N ×D ∞ 21.101 16.254
ds
Table1: PerformanceoftheparametriclanguagemodelandseveralkNN-LMvariants.
Results comparing multiple kNN-LM variants are shown in Table 1. The first row represents the base
parametriclanguagemodel’sperplexity. ThesecondisaformulationanalogoustothatofKhandelwaletal.
(2020b), and in the remaining rows, we
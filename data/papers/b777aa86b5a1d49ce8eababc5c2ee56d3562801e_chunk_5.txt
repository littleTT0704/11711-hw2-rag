utilization(Chowdh-
ingspaceofmodelsandhardwareplatformsisout
eryetal.,2022)arereportedastheratiobetween
ofscope,soweinsteadidentifythemostpopular
observedFLOPspersecondandahardwareplat-
model components, inference environments, and
formspeaktheoreticalFLOPs.
hardwareacceleratorsfortesting(i.e. isolatingthe
Previous works examining the relationship be-
cases most likely to mislead a practitioner). We
tween efficiency measures showed that different
analyzetheperformanceoftransformerandconvo-
costindicatorsdonotcorrelatewellwitheachother
2PertheoreticalFLOPSofNvidiaGPUs(2017-2022). during neural network training (Dehghani et al.,
2021). Inparticular,ithasbeenhypothesizedthat stractawaytheunderlyingframeworks,compilers,
discrepanciesbetweenFLOPsandwallclockinfer- backends,andhardwareplatforms. Whilegeneral
ence latency is primarily are primarily compute improvements in hardware and software kernels
boundedbykernelexecutionormemory-boundby mayleadtoimprovementsacrossallmodels,ithas
data movement as opposed to framework bottle- been argued that solely focusing on performance
necks (Langerman et al., 2020). These previous optimization of a limited set of model architec-
workshavelargelyfocusedonconvolutionalneural turesandruntimesmayleadtooverspecialization
networks(CNNs)incomputervision. Weextend (Hooker,2021).
these analyses to the inference setting and study Previous analysis of the computational proper-
transformer-based neural networks for NLP, and ties of hardware accelerators has largely focused
showthatFLOP-basedproxymetricsbreakdown onthetrainingsettinginwhichlargerkernelsand
for due to additional performance bottlenecks in- batchsizeshideframeworkoverheadthatemerges
troducedbydeeplearningframeworks. in the inference setting (Wang et al., 2020c; Zhu
et al., 2020, 2018). Other analyses of end-to-end
2.2 EfficientModelDesign
systemsanalyseshasprimarilyfocusedondomain-
Desiretodevelopcomput
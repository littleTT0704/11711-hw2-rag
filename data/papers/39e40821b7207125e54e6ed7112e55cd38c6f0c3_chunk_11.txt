. ThemainresultsarethatCOCOGEN(based
onCODEX),withjust15promptexamples,outper-
Table2: Precision,recall,andF 1 forPROSCRIPTedge- formsthefine-tunedmodelT5whichhasbeenfine-
prediction task. COCOGEN with 15 samples outper-
tuned on all 3500 samples. Further, COCOGEN
forms strong few-shot models, and T5 trained on 100
outperforms the few-shot NL-LM CURIE across
samples.
allsemanticmetricsandstructuralmetrics. COCO-
GENoutperformsDAVINCIacrossallsemanticmet-
completestheclassbygeneratingthecodebelow rics,whileDAVINCIperformsslightlybetterintwo
thecomment# edges. structuralmetrics.
Table 2 shows the results for edge predic-
ScriptGenerationmetrics Wedenotethescript tion: COCOGENsignificantlyoutperformstheNL-
thatwasgeneratedbythemodelasGˆ
,andevaluate LLMsCURIEandDAVINCI. Whencomparingwith
Gˆ vs. G for both semantic and structural similar- T5,whichwasfine-tuned,COCOGENwithonly15
ity. Toevaluatesemanticsimilarity,weuseBLEU, examplesoutperformsthefine-tunedT5whichwas
ROUGE-L, and the learned metric BLEURT to de- fine-tuned on 100 examples. The impressive per-
terminethecontentoverlap. FollowingSakaguchi formanceintheedge-generationtaskallowsusto
etal.(2021),weusethefollowingmetricsforstruc- highlightthebetterabilityof COCOGENincaptur-
turalevaluationofgeneratedscripts: ingstructure,whilefactoringoutallmodels’ability
• Graph edit distance (GED): the number of togeneratetheNLcontent.
requirededits(node/edgeremoval/additions)
totransformGˆ toG (Abu-Aishehetal.,2015); 3.3 Entitystatetracking: PROPARA
• Graph isomorphism (ISO; Cordella et al., The text inputs
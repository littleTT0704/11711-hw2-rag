, Ariel Herbert-Voss,
(2) Our experiments are limited by available Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
datasetsintermsofbothmetricswecancalculate
ClemensWinter,ChristopherHesse,MarkChen,Eric
andobjectiveswecanoptimizefor. Thereisalso
Sigler,MateuszLitwin,ScottGray,BenjaminChess,
somenoiseineachdatasetwhichwecataloguein Jack Clark, Christopher Berner, Sam McCandlish,
AppendixC. Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
(3) We conduct experiments with 100M pa-
∼ NeurIPS.
rametermodelsasinpastwork. Whilethebelief-
updating problem is still clearly unsolved given DamaiDai,LiDong,YaruHao,ZhifangSui,andFuru
2722
Wei.2021. Knowledgeneuronsinpretrainedtrans- KevinMeng,DavidBau,AlexAndonian,andYonatan
formers. arXivpreprintarXiv:2104.08696. Belinkov.2022. Locatingandeditingfactualknowl-
edgeingpt. arXivpreprintarXiv:2202.05262.
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021.
Editing factual knowledge in language models. In EricMitchell,CharlesLin,AntoineBosselut,Chelsea
EMNLP,pages6491–6506.AssociationforCompu- Finn,andChristopherDManning.2021. Fastmodel
tationalLinguistics. editingatscale. arXivpreprintarXiv:2110.11309.
Bhuwan Dhingra, Jeremy R Cole, Julian Martin AlbertNewenandTobiasStarzak.2020. Howtoascribe
Eisenschlos, Daniel Gillick, Jacob Eisenstein, and beliefstoanimals. Mind&Language.
WilliamWCohen.2021. Time-awarelanguagemod
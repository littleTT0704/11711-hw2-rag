foreachsoftmaxbeingconcatenated. Weperformexperimentswithavaryingnumberofmixtures
(R),differentdefinitionsh,andwhethertofinetunetheoutputwordembeddingsW. Weallowfinetuning
ds sm
thewordembeddingwhenweuseattentionlayeroutputascontextvector,sincethewordembeddingmatrixis
trainedwithfeedforwardlayeroutputoriginally. TheresultsforthisformulationareshowninTable7. MoS
modelsonitsownincreasetheperformanceofthelanguagemodelmarginally. WhencomparedwithTable5,
we find that these models are worse than those that simply increases the number of embeddings. This is
expectedbecauseMoShasfeweraddedparameterscomparedtothose,asitonlyrequiresseveraladditional
linearprojectionlayersfortheembeddings.
C.3 ClusteringDatastore
Oppositetotrainingthewordembeddingsofanincreasedsize,wealsoconsiderwaystocompressthedatastore
downtoasimilar-sizedembeddingmatrixforsoftmaxcomputation. Theintuitionisthatthedatastorecontains
15
h R ⊗ +#params PPL λ Interp.PPL Oracle
ds
BaseLM - - - 0 21.750 - - -
KNN att - L2 N ×D ∞ 0.271 19.174 14.230
ds
KNN att - IP N ×D ∞ 0.266 19.095 14.077
ds
KNN ffn - L2 N ×D ∞ 0.065 20.734 15.594
ds
KNN ffn - IP N ×D ∞ 0.050 21.101 16.254
ds
Ft.MoS+embed att 2 IP VD+2D2+2D 21.986 0.437 20.720 17.573
Ft.MoS+embed att 3 IP VD+3D2+3D 22.106 0.422 20.779 17.609
Ft.MoSOnly att 2 IP 2D2+2D 22.552 0.371 21.011 17.796
Ft.MoSOnly att 3 IP 3D2+3D 22.573 0.371 21.024 17.8
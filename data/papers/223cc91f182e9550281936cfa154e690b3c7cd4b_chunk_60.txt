 Negative
Original besides that, the wine selection they have is pretty awesome
as well.
(Shen et al., 2017) after that, the quality prices that doesn’t pretty much well as
.
f sc besides horrible horrible as
f sc +f data besides that, the wine selection they have is pretty
borderline as atrocious.
44
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
f sc +f data +f LM besides that, the wine selection they have is pretty horrible
as well.
Method Attribute accuracy (↑) Preservation (↑) Fluency (↓)
(Shen et al., 2017) 79.5% 12.4 51.4
f sc 99.6% 1.2 259.2
f sc +f data 87.7% 65.6 177.7
f sc +f data +f LM 91.2% 57.8 53.95
Case study: Text attribute transfer. As a case study of learning from rich experience, consider the problem of
text attribute transfer where we want to rewrite a given piece of text to possess a desired attribute (Hu et al.,
2017; Shen et al., 2017). Taking the sentiment attribute, for example, given a sentence x (e.g., a customer’s
review “the manager is a horrible person”) and a target sentiment a (e.g., positive), the goal of the problem is
to generate a new sentence y that (1) possesses the target sentiment, (2) preserves all other characteristics of
the original sentence, and (3) is fluent (e.g., the transferred sentence “the manager is a perfect person”). To
learn an attribute transfer model p (y∣x,a), a key challenge of the problem is the lack of direct supervision
θ
data (i.e., pairs of sentences that are exact the same except for sentiment), making it necessary to use other
forms of experience. Here we briefly describe an approach originally presented in Hu et al. (2017) and Yang et
al. (2018), highlighting how the approach can be built mechanically, by formulating relevant experience
directly based on the problem definition and then plugging them into the SE.
We can identify three types of experience, corresponding to the above three desiderata, respectively. First, the

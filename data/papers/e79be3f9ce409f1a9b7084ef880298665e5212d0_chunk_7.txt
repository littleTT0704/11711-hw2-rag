1 k(cid:54)=i
pends on the choice of sampling frame rate and the video
featureextractor,whichwewilldiscussinSec.4. ThedenominatorinEq.2requiresasumoverallvideos
Language encoding module f. We use pretrained tok- inadataset,whichisintractableinpractice. Therefore,we
θt
enizer [48] and BERT [10] to tokenize the input texts and usuallycomputetheNCElossonamini-batchofK(K (cid:28)
extracttextualfeatures,respectively. Givenarawsentence, N) video-text pairs sampled from the whole dataset. Ide-
we append a “[CLS]” and “[SEP]” to the beginning and ally, we want to learn the parameters θ = {θ v,θ t,θ m}
end, respectively. At the top, we can obtain a sequence of the model to minimize the above NCE loss, such that
of n textual features y = {y1,...,yn} ∈ Rn×d. We en- ∆ = s(v i,t i) − s(v j,t i) is maximized over all tuples
sure the output feature dimension of video encoder to be (t i,v i,v j),j (cid:54)= i. A number of previous works used the
identical to that of language encoder. During training, we above formula for contrastive learning [34, 60]. Mean-
update the parameters θ in our language encoder to adapt while,therearesomevariantsofcomputingcontrastiveloss
t
tothetextsinspecificdomain,e.g.,cookinginstructionsin in video-langauge representation learning. For example,
YouCook2[58]. [28, 14] omits the denominator and incorporate a margin
s.t. s(v,t ) > s(v,t )+δ,∀j (cid:54)= i in a mini-batch. [33]
Multi-modal fusion module f. It also consists of self- i i j i
θm
optimizesbinarycross-entropy(BCE)byassigning
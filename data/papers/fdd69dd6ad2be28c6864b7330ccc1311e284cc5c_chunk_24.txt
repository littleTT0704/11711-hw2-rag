iftrainedfromscratch[34,72,73]. However,afterappropriatepre-training,
ViTperformsthebestamongallthebackbones. Inaddition,weprovidetheT-SNEvisualizationof
thefeaturesinFigure3,wherethepretrainedViTmodeldemonstratesthemostseparablefeature
spaceaftertraining. Inaword,pre-trainedViTmakesthetrainingmoreefficientandimprovesthe
generalizationperformanceofSSLalgorithms. ForNLPtasks,weobservesimilarresults,yetthe
improvementcanberelativelylesssignificantsincepre-trainingisthede-factofashioninthefield.
Robustness SSL sometimes hurts the generalization performance due to the large differences
betweenthenumberoflabeleddataandthenumberofunlabeleddataasshowninTable9. Werefer
toanSSLalgorithmasarobustSSLalgorithmifitisconsistentlybetterthanthesupervisedtraining
setting. SSLalgorithmscannotalwaysoutperformsupervisedtrainingespeciallywhenlabeleddata
isscarce. WefindthatCRMatch,AdaMatchandSimMatcharerelativelyrobustSSLalgorithmsin
USB.AlthoughpreviousworkhasdonesomeresearchtowardsrobustSSLwhenusingsupportvector
8
.ccA
.ccA
tseT
.ccA
.ccA
lebaL
oduesP
(a)WRN-28-8fromscratch. (b)Pre-trainedWRN-28-8. (c)Pre-trainedViT-S-P2-32.
(d)WRN-28-8fromscratch. (e)Pre-trainedWRN-28-8. (f)Pre-trainedViT-S-P2-32.
Figure 3: T-SNE visualization of FixMatch features on training data (first row) and testing data
(secondrow)ofCIFAR-100(400labels). Differentcolorsrefertolabeleddatawithdifferentclasses
whileunlabeleddataisindicatedbygraycolor.
machine[74,75],wehopethatourfindingcanserveasthemotivationtodelveintodeeplearning
basedrobustSSLmethods.
6 Codebase
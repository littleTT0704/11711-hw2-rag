video(lossL2).Forexample,
suchastext-videoretrieval[52],videoactionsteplocaliza- we pay particular attention to the words “add”, “tomatos”,
tion [61], video action segmentation [43], video question “pan”and“stir”inFig.1.
1202
guA
32
]VC.sc[
1v08990.8012:viXra
The second technique we introduce is a cascade sam- ousworks,thesetwoapproacheswereexploredseparately.
pling method to find a small set of hard negative exam- Veryrecently,anupdatedversionof[33]usedtwoindepen-
ples for training the multi-modal fusion layers. Consider dentalignmentlossesbeforeandaftermulti-modalfusionin
a batch of K video-text pairs. For each of the video-text asingleframework.Inthispaper,however,thesetwolosses
pairs, the ideal case is that we use the remaining K − 1 cooperatecloselywitheachotherduringtraininginthatthe
negativevideosortextstocomputethecontrastivelossaf- earlierstagehelpstodiscoverthehardnegativeswhilethe
termulti-modalfusion. However,thecostofcomputingthe multi-modallayerswithmorecapacityhelptotacklethose
contrastivelossquicklybecomesprohibitivewhenitiscou- hardsamplesparticularly.
pled with multi-modal fusion layers, considering its high
Video-textalignment. Aligningvideostotextrequiresthe
complexityO(K2×L2)whereListotalnumberofvisual
modeltounderstandmotionandtemporalcoherence.Some
and textual tokens. A conventional way to address this is
works have relied on attention mechanisms to extract key
usingrandomsamplingtoselectasmallsubsetofnegative
informationfromvideos[45,55],whileotherspreservevi-
pairs. In this paper, instead of random sampling, we pro-
sual information by composing pairwise joint representa-
pose a cascade sampling method as shown in the top-right
tionusing3Dtensors[53]orusemulti-levelvideoencoders
of Fig. 1
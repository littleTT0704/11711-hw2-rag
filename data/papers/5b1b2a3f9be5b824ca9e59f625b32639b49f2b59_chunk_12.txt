e,whichrepresentsorder- RC(cid:48)×(H·W+K·K). The fused target tokens O(cid:48) ∈ RC(cid:48)×H·W
t tgt
preserving instance identities. We directly compose the final are further reshaped and fed into a mask decoder to generate
instance segmentation map M by dynamic convolution be- the segmentation map S. In addition, the whole fused tokens
t t
tweenS ande withoutanysophisticatedmatchingalgorithm. O(cid:48) are fed into a transformer decoder to extract instance-
t t
We elaborate our online VIS framework in Section III-B. specific information, which is discussed in Section III-B2.
Our pipeline is flexible to aggregate multi-modal contexts. RCF analysis. We further analyze our design of RCF from
Audio is verified to be helpful for object recognition and theoreticalandexperimentalperspectives.Followingthetrans-
localization, and we investigate the audio-visual correspon- formerstructure[3],weadoptstandardthescaleddot-product
denceandleverageittoimprovethedenseprediction(instance attention, i.e., Attention(Q,K,V) = softmax(Q √KT )V, in
dk
segmentation in this work). We align and fuse audio features thetransformerencoderofRCF.Asoftmaxfunctionisapplied
faud in the RCF the same way as reference video frames. to obtain the weights on the values. Given a much smaller
t
5
Reference Target learnable instance query is used to decode instance-specific
information from the fused tokens O(cid:48) using a transformer
decoder,whereinstancequeryisasetoflearnableembedding
as previous methods [60], [23], [63]. We term the decoded
(a) Input frames (b) Pixel weight 𝑊
instance-specificfeaturesasinstancecodee
t
∈RCe×N,where
Target-to-target Reference-to-target C eandN arethechanneldimensionandlengthoftheinstance
code respectively. For decoding the segmentation mask, we
follow the FPN structure [36] to fuse the low-level features.
Let the upsam
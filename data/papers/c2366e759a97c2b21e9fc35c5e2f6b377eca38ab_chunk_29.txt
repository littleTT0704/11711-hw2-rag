andcategories. Thisinformationalongwiththetimestepisusedtogenerateinstancetokens
asdescribedinSection2forallobjectsvisibletothecameras. Forvisiblealreadyplacedobjects,
theplaceposeisapproximatedasafixedlocation. Thepolicyπ,conditionedonthehumandemo,
reasons about the state of the environment, and chooses which object to pick. Next, we use a
graspgeneratorfrom[38]thatoperatesonpointcloudstogeneratecandidategrasplocationsonthe
chosenobject.Wefilteroutgrasplocationsthatarekinematicallynotreachablebytherobot,aswell
as grasp locations located on points that intersect with other objects in the scene. Next, we select
thetop5mostconfidentgrasps,asestimatedbythegraspgenerator,andchoosethemosttop-down
grasp. We design an pre-grasp approach pose for the robot which is the same final orientation as
thegrasp,locatedhigheronthegraspingplane. Therobotmovestotheapproachposefollowinga
minimum-jerktrajectory,andthenfollowsastraightlinepathalongtheapproachaxestograspthe
object. Once grasped, the object is moved to the pre-defined place pose and dropped in a drawer.
Theprimitivesforopeningandclosingthedrawersaremanuallydesignedonhardware.
Thelearnedpolicy,conditionedonpromptdemonstrations,isappliedtotwovariationsofthesame
scene, and the predicted pick actions are executed. Fig.9 shows the captured image from one of
13
Figure9: Pointcloudandgraspsfordifferentobjectsduringpolicyrollout.
the three cameras, the merged point cloud and the chosen object to pick and selected grasp for
the same. The policy was successful once with 100% success rate, and once with 75%, shown in
Fig.1. Thefailurecasewascausedduetoaperceptionerror–abowlwasclassifiedasaplate. This
demonstratesthatourapproach(TTP)canbetrainedinsimulationandapplieddirectlytohardware.
Thepolicyisrobusttominorhardwareerrorslikeafailedgrasp;itjust
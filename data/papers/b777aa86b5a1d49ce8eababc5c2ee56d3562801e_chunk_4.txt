around
latedintoreductionsininferencelatency. Weshow parameters or FLOPs. We hope that our analy-
thatthisbreakdownislargelyduetooverheadin- sisandrecommendationswillhelpbridgethegap
troducedbydeeplearningframeworks. Wereferto betweenefficientNLPresearchandpractice.
thisphenomenonastheframeworktax,andshow
2 RelatedWork
that it exists across all deep learning framework
paradigms(e.g. eagerexecution,just-in-time,and
2.1 EfficiencyMetrics&CostIndicators
ahead-of-timecompilation).
Previous efforts to report efficiency often utilize
At small batch sizes and sequence lengths, we
proxymetricsforamountofcomputation,suchas
showthatfixedframeworkoverheaddominatesin-
thenumberoffloatingpoint(FLOPs)ormultiply-
ferencetimeleadingproxymeasurementsofmodel
accumulate (MACs) operations (Schwartz et al.,
efficiency,suchasMACsandparametercount,to
2020). Similarly,numberoftrainableparametersis
breakdownaspredictorsofinferencelatency. Fur-
afrequentlyreportedasaproxyformemoryutiliza-
thermore,wenotethatexistingeffortstoimprove
tion(Lanetal.,2019). Unfortunately,theseproxy
modelefficiencybydesigninglowerFLOPmodel
metrics are often not predictive of realworld effi-
architecturesandfasterGPUkernelsdonotreduce
ciency. Forexample,totalFLOPsdoesnotaccount
latency,whentotalexecutionisboundbyfixed-cost
forthevaryingextenttowhichdifferentoperations
framework overhead. Moreover, we note that as
canbeparallelizedandtechniquessuchasweight
hardwareperformanceincreases,NLPsystemswill
tyingcanreduceparametercountswithoutreduc-
become increasingly framework-bound at larger
ingtheamountofrequiredcomputation(Lanetal.,
batchsizes.
2019). Fromtheperspectiveofdeviceutilization,
Anexhaustivecomparisonoftherapidlygrow-
hardwareandmodelFLOPs
 adopt a IE tasks. For other hyper-parameter settings, we
local pairwise model with the standard cross- mostlyfollowcommonpractices. Adamisutilized
entropy loss. Since the relation model is local, foroptimization,withaninitiallearningrateof1e-
nospecialtreatmentisneededforPA. 5forNERand2e-5forDPARandIE.Thelearning
rateislinearlydecayedto10%oftheinitialvalue
B DataStatisticsandMoreSettings throughout the training process. The models are
tuned for 10K steps with a batch size of roughly
Data. Our main experiments are conducted us-
512tokens. Weevaluatethemodelonthedevset
ing the CoNLL-2003 English dataset11 (Tjong
every1Kstepstochoosethebestcheckpoint. The
Kim Sang and De Meulder, 2003) for NER, the
experiments are run with one 2080Ti GPU. The
EnglishWebTreebank(EWT)fromUniversalDe-
training of one AL cycle usually takes only one
pendencies12 v2.10(Nivreetal.,2020)forDPAR,
or two hours, and the full simulation of one AL
andEnglishportionofACE200513 (Walkeretal.,
runcanbefinishedwithinoneday. Weadoptstan-
2006)forIE.WeutilizeStanza14 (Qietal.,2020)
dard evaluation metrics for the tasks: labeled F1
toassignPOStagsforcostmeasurementinNER
scoreforNER,labeledattachmentscore(LAS)for
11https://www.clips.uantwerpen.be/conll2003/ DPAR,labeledargumentandrelationF1scorefor
ner/ eventargumentsandrelations(Linetal.,2020).
12https://universaldependencies.org/
13https://catalog.ldc.upenn.edu/LDC2006T06 15http://blender.cs.illinois.edu/software/
14https://stanfordnlp.github.io/stanza/ oneie/
C DetailsofAlgorithms labeling (Baum et al., 1970) or Matrix-tree for
non-projective dependency parsing (Koo et al.,
In this section, we provide more details of
 1D-is object or not?) and 12 dimen-
sionalone-hotencodingforthepreference.
input_dim: 24
hidden_dim: 128
epochs: 200
batch_size: 32
Optimizer : Adamwithlr =0.01andweight decay=1e−3.
RewardfunctionforGNN-RL RewardfunctionfortheRLpolicyisdefinedintermsofprefer-
ence. Thepolicygetsarewardof+1everytimeitpredictstheinstancetopickthathasthecategory
accordingtothepreferenceorderandwhetheritisplacedonthepreferredrack.
C.2 Ourproposedapproach: TTP
Architecture Weusea2-layer2-headTransformernetworkforencoderanddecoder. Theinput
dimension of instance embedding is 256 and the hidden layer dimension is 512. The attributes
contributetotheinstanceembeddingasfollows:
C_embed: 16
category_embed_size: 64
pose_embed_size: 128
temporal_embed_size: 32
marker_embed_size: 32
FortheslotattentionlayerattheheadofTransformerencoder,weuse:
num_slots: 50
slot_iters: 3
Optimizer Weuseabatch-sizeof64sequences. Withineachbatch,weusepadtheinputswith0
uptothemaxsequencelength. OuroptimizerofchoiceisSGDwithmomentum0.9,weightdecay
0.0001anddampening0.1. Theinitiallearningrateis0.01, withexponentialdecayof0.9995per
10gradientupdates. Weusedearlystoppingwithpatience100.
C.3 Metrics
In Section 3, we presented packing efficiency (PE) and edit distance (ED) metrics collected on a
policyrollout. Wepresentadditionalmetricsabouttrainingprogressandrollouthere.
Category-token Accuracy indicates how well the policy can mimic the expert’s action, given the
currentstate. Wemonitortrainingprogressbymatchingthepredictedinstancetothetargetchosen
in demonstration (Fig. 12a). We see that TTP is able to predict the same category object to pick
perfectly(accuracycloseto
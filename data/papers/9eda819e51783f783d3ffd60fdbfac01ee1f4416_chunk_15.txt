differentsettingsisplottedinFigure4.
eters of the LM) for each language and tune
Withcombinationsofdifferentexperimentsettings,
adapters/prefixes separately for each language
wehavethefollowingresults:
while freezing mt5-base which is shared by all
PLF v.s. MPE prefix v.s. MPE adapter: PLF
languages. Multi-lingual PLM Fine-tuning
outperformsMPE prefixandMPE adapteroverall,
(MPF): A single model is trained with train-
with a gap larger for more available training
ing samples from multiple languages. The
samples. MPE adapter outperforms MPE prefix
training strategy proposed by (Lample and
foralmosteverylanguage,exceptafewlanguages
Conneau, 2019) to use a smoothing factor
with few samples. This conforms to the result in
(alpha) of 0.5 to balance the sampling rate of
Sec.3.1 that fine-tuning has the advantage with
low-resource languages and high-resource lan-
largetrainingsetsize,whileprefix-tuninghasthe
guagesisfollowedbyeverymultilingualsetting.7
advantagewithsmalltrainingsetsize. Onething
Multi-lingual Private-shared (Adapter/Prefix)
to notice is that adapter-tuning has comparable
Tuning (MPS adapter/MPS prefix): Private
orevenhigherperformancewhenthetrainingset
adapters/prefixes (with parameters = 2% pa-
sizeissmallerthan10k,consistentwiththeresult
rameters of LM) for each language (in total 45
inSec.3.1andadapter’ssensitivitytotrainingset
languages in XL-Sum dataset × 2% parameters
size is not as high as prefix-tuning. The latter
foreachlanguages=90%additionalparameters)
is reflected in that the adapter’s performance
areaddedtoasingleLM.TheLMistunedjointly
does not drop down dramatically as the training
formultiplelanguages,whiletheadapters/prefixes
set size increases and keeps within -2 R2 of
aretunedseparatelyforeachlanguage.8
the baseline for almost all languages, which is
even true for
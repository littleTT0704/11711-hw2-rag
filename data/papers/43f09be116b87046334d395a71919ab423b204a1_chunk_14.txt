ingletonsfromboth.
CL with coreference examples from the source
domain (CL ), and MD with examples from the
S 6 ResultsandAnalysis
target domain (MD ). We report results only
T
withMD pairedwithhigh-prec. c2fpruning(i.e. Table 4 reports results when transfering models
T
thresholdq =.5imposedonthementionscores m) trainedonONtoi2b2andmodelstrainedoni2b2
as described in §4. Without the threshold, MD toCNwithsingletonsincluded(forcompleteness
T
has almost no effect on overall coreference per- Appendix A, Table 5 reports results without sin-
formance, likely because the space of candidate gletons). Forbothi2b2→CNandON→i2b2,our
antecedentsforanygivenmentiondoesnotshrink. model performs better with mention annotations
Our model uses only mentions without target thanthecontinuedtrainingbaselinewithhalfthe
domaincoreferencelinks,whileourbaselineuses coreferenceannotations(e.g. equivalentannotator
coreferenceannotations. Accordingly,wecompare time,sincetheaveragelengthofi2b2documents
resultsforsettingswherethereis(1)anequivalent is 963 words; and timed experiments in CN sug-
numberofannotateddocumentsand(2)anequiv- gested mention annotations are ~2X faster than
alent amount of annotator time spent, estimated coreference,§5.1). CombiningMLM withMD
T T
basedonthetimedannotationexperimentsin§3. results in our best performing model, but intro-
Foreachtransfersetting,weassumethesource ducing MD T with high-precision c2f pruning is
domain has coreference examples allowing us to enoughtosurpassthebaseline. Theresultssuggest
optimize CL. In the target domain, however, in-domain mention annotation are more efficient
S
we are interested in a few different settings: (1) foradaptationthancoreferenceannotations.
100% of annotation budget is spent on corefer-
6.1 TransferAcrossAnnotationStyles
ence, (2) 100% of annotation budget is spent on
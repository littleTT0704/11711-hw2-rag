sequences
Finally, we observe that mBERT achieves the
to generate. In fact, ByT5 has a faster inference
best performance on almost all tasks when the
speedthanmT5forXNLIbecausethemodelsonly
dataset is small, which is surprising, as we ex-
needtopredictasingletoken.
pectmT5/ByT5modelstogeneralizebetterinlow-
Ingeneral, wefindthatmBERTandCANINE
resourcesettingsgiventheirmuchlargerpretrain-
are both very accessible and efficient to use,
ingcorpus.
whereas ByT5 would require more effort in tun-
4.3 Inferencecost ingthebatchsizetofitintotheGPUmemory.
Anotherkeyconcerninutilizingpretrainedmodels
5 Conclusion
fordownstreamapplicationsistheinferencecost,
suchasmemoryconsumptionandlatency. Inthe
Inthispaper,wepresentamulti-dimensionaleval-
uation of tokenizer-free multilingual pretrained
3While we initially considered four languages (Arabic,
English,Russian,Swahili)chosenbasedonthepretraining models focusing on their robustness against fine-
corpussize,weincludeotherlanguageresultsinÂ§B.2asthey tuningdatasettings,dataefficiency,andinference
tendtoexhibitsimilarperformancetrends.
cost. Surprisingly,wefindthatmBERTmightbe
4WesuspectByT5matches/surpassestheperformanceof
mT5inmanysituationsduetotheextradepthintheencoder. the most practical choice so far, considering all
ycaruccA
ycaruccA
1F
1F
1F
1F
theabovementionedfactors. Despiteourfindings, NationalScienceFoundationaswellastheCMU-
tokenizer-free models still have a significant ad- PortugalMAIAproject.
vantageinreducingengineeringeffortsandpoten-
tiallyincreasingrobustnesstonoisyandmultilin-
gualdata. Webelievemoreworkshouldbedone
indevelopingefficient tokenizer-freemodelsthat
arerobustinvariousfine-tuningsettings.
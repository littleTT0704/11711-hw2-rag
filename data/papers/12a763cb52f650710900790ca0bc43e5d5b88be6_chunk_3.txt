kid. two 0.91
Shewasalwayshelpingattheseniorcenter,itbroughtherwhat? feelbetter 0.97|0.02
CSQA
Peoplewhohelpothersareusuallyhappier. happiness 0.98
Partofgolfistryingtogetahigherpointtotalthanothers. yes 1.00|0.00
CSQA2
Theplayerwiththelowestscorewins. no 1.00
Spongeseatprimarily cartilage 0.95|0.00
QASC
Spongeseatbacteriaandothertinyorganisms. krillandplankton 0.99
Table1: Exampleswherepromptingwithgeneratedknowledgerectifiesmodelprediction. Eachsectionshowsthe
correctansweringreen,theincorrectanswerinred,andthepredictionscoresfromtheinferencemodelthatonly
seesthequestion(top)andthesamemodelthatseesthequestionpromptedwiththegivenknowledge(bottom).
supportavarietyofsettingswithoutfinetuning,the tion q ∈ Q, where the set of choices A is finite
q
qualityandflexibilityofknowledgeiscrucial. We andcanvarybyquestion,andbothquestionsand
proposeasimple,yeteffective,methodthatelicits answers are variable-length text sequences. Our
knowledge statements (i.e. knowledge expressed method answers commonsense questions in two
asnaturallanguagestatements)fromgenericlan- steps.
guagemodelsinafew-shotsetting. Comparedto Thefirststepisknowledgegeneration,wherewe
priorworkthatelicitsknowledgeviaclarification usealanguagemodelp (k|q)togenerateknowl-
G
questions(Shwartzetal.,2020)orcontrastiveex- edgestatementsconditionedonthequestion:
planations (Paranjape et al., 2021), our approach
K = {k : k ∼ p (k|q),m = 1...M},
cangenerateknowledgeflexibly,beyondthescope q m m G
ofpre-definedtemplates(Table1).
whereeachknowledgestatementk isavariable-
m
Experiments show that our method improves
length text
-LM solves the stolen probabilities problem by allowing to assign the highest
probabilitytoanyword,givenatesthiddenstatethatiscloseenoughtothatword’sdatastorekey.Nevertheless,
asshownbyGrivasetal.(2022),althoughthisproblemmighthappeninsmallRNN-basedlanguagemodels,
inmoderntransformersitrarelyhappensinpractice. UsingthecodeofGrivasetal.(2022),wecheckedthe
embeddingsmatrixofourmodelandofthecheckpointprovidedbyKhandelwaletal.(2020b). Indeed,we
foundthatinbothmodels–nowordisun-argmaxable.
18
E.5 ArekNN-LMJustEnsembling?
Our hypothesis is that kNN component only provides another model for ensembling. The interpolation
processisbasicallyanensemblemodel. TechnicallyitisunsurprisingthatkNN-LMwillhavethebenefit
from ensembling, but we perform experiments to see how it compares to other ensembling. We trained
another language model with the same architecture as the base LM we used throughout the experiments,
with some variants having more than one embedding vector for each word (similar to Section 4.2). We
interpolatethemodelswiththeoriginalbaseLM,andtheresultsareshowninTable8. Wecanseethateven
justensemblingthebaseLMwithanotheridenticalmodel,buttrainedwithadifferentrandomseed,provides
ahugeperformanceboost,bothoninterpretedperplexityandonoracleperplexity.
Prev.Layers h N ⊗ +#params PPL Interp. Oracle
ds ds
same - - - 0 21.750 - -
same att Big L2 N ×D ∞ 19.174 14.230
ds
same att Big IP N ×D ∞ 19.095 14.077
ds
same ffn Big L2 N ×D ∞ 20.734 15.594
ds
same ffn Big IP N ×D ∞ 21.101 16.254
ds
diff ffn 1x IP F +V ×D 21.569 18.941 14.980
diff ffn 2x IP F +2V ×D 21.914
Challenges in Automated Debiasing for Toxic Language Detection
XuhuiZhou♥ MaartenSap♣ SwabhaSwayamdipta♦ NoahA.Smith♣♦ YejinChoi♣♦
♥DepartmentofLinguistics,UniversityofWashington
♣PaulG.AllenSchoolofComputerScience&Engineering,UniversityofWashington
♦AllenInstituteforArtificialIntelligence
xuhuizh@uw.edu, msap@cs.washington.edu
{swabhas,noah,yejinc}@allenai.org
Abstract Detected
toxicity score
I identify as a black
Warning:thispapercontainscontentthatmay gay woman. Identity
bias
beoffensiveorupsetting. I identify as a PAePrsI. (Lexical)
straight white man.
Biased associations have been a challenge in
the development of classifiers for detecting Fucking love Swear
toxic language, hindering both fairness and this. word
accuracy. As potential solutions, we inves- Adolf Hilter is a PAePrsI. (Lb ei xa ics al)
great person.
tigate recently introduced debiasing methods
for text classification datasets and models, as
Wussup, n*gga! Dialect/
appliedtotoxiclanguagedetection. Ourfocus Racial
is on lexical (e.g., swear words, slurs, iden- What’s up, bro! PAePrsI. bias
tity mentions) and dialectal markers (specifi-
cally African American English). Our com- Figure1:Lexicalitemsanddialectmarkerscauseprob-
prehensiveexperimentsestablishthatexisting lematic behavior for toxic language detection systems
methods are limited in their ability to prevent suchasthewidelyusedPerspectiveAPI.Inthetoptwo
biased behavior in current toxicity detectors. examplepairs, statementswithminorityidentitymen-
We then propose an automatic, dialect-aware tionsandswearwordsusedinoffensivelyareflaggedas
datacorrectionmethod, asaproof-of-concept toxic,butmajorityidentitymentionsoroffensivestate-
study. Despite the use of synthetic labels, mentswithoutovertswearingaremissed.
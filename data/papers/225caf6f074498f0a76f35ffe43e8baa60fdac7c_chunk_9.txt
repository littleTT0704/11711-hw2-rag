embeddings,arefedintotheencoderactingastheirinferencenetworks. The
j
textisalsofedintothesemanticinferencenetworkwhichisaseparateencoder. Theoutputofthesenetworksare
thelanguagevariablesz andz andsemanticvariablez. Eachlanguage-specificvariableisthenconcatenated
li lj sem
toz andusedbyasingleshareddecodertoreconstructtheinputsentencepair.
sem
Decoder Architecture. The decoder models pair should be similar regardless of the language
p(x |z,z ;Î¸) for each language i (see right oftheinputsentence. Weusethemeanofthese-
li sem li
sideofFigure2). Theinputstothedecoderarethe manticencoderasthesentencerepresentationfor
language-specificvariablez andthesemanticvari- downstreamtasks.
li
ablez,whichareconcatenatedandusedtocon-
sem
6 Experiments
ditionthedecodertogeneratethereconstructionof
theobservedtextx. Weuseasingledecoderfor
li 6.1 ConstructingtheTrainingData
alllanguages.
We follow Artetxe and Schwenk (2019b) in con-
structing our training data. However, since the
EncoderArchitecture. Theencodersplayanim-
exact data is not publicly available, we expect
portant role in the source separation as well as
theirmaybesmalldifferencesduetorandomsam-
inferenceasdetailedbelow.
plinganddifferentdatasetversions. Morespecifi-
Inordertomotivatetheseparationofthelinguis-
callywesampleourdatafromEuroparl,3 United
ticandsemanticinformationwesplittheencoder
Nations(RafalovitchandDale,2009),4 OpenSub-
into two parts, only sharing the embedding table.
titles2018 (Lison et al., 2018),5 Global Voices,6
We use one of these encoders to be the semantic
Tanzil,7 andTatoebav2021-07-22.8
inference network, which produces the semantic
Wesample

4
0.20 with it than when prompted for score prediction,
0.15 and itsreference-less version iscompetitive with
3
0.10 the best learned metric even at the segment level.
0.05 2 However, for the smaller BISON, the benefits of
0.00 AUTOMQM are less clear, with both techniques
1 performing comparably. This hints that scale is
0.0 0.1 0.2 0.3
Pearson w/o Reference necessaryforzero-andfew-shotfine-grainedevalu-
Figure 9: Scatter plot of the Pearson of PaLM-2 ation(likewithAUTOMQM).Wealsofindthatthe
(BISON)models,with/withoutincludingthereference
intheprompt,foreachin-contextlearningsettingtried.
ecnerefeR
htiw
nosraeP
)ED-NE(
nosraeP
selpmaxE
fo
rebmuN
)NE-HZ(
nosraeP
distributionofscoresproducedbyLLMsprompted LLMs (despite only leveraging a handful of an-
withAUTOMQMismuchclosertothegoldMQM notations).
distribution,withmodelsoutputtingamuchlarger
7 Conclusion
setofscores,andinthesamerangesasannotators
do(seeFigure10).
In this study, we have systematically investi-
gated the capabilities of large language models
Gold MQM for machine translation evaluation through score
10000 PaLM-2 (Bison)
PaLM-2 (Unicorn)
prediction, and proposed AUTOMQM, a novel
prompting technique that leverages the Multidi-
1000
mensionalQualityMetrics(MQM)frameworkfor
100 interpretableMTevaluationusingLLMs.
WedemonstratedthatjustpromptingLLMsfor
10 score prediction leads to state-of-the-art system-
level evaluators, but still falls short of the best
1
learned metrics at the segment-level (with fine-
40 35 30 25 20 15 10 5 0
Score tuning being necessary to close this gap). Then
Figure10: DistributionofscoresforPaLM-2models weshowedthatAUTOMQMcanf
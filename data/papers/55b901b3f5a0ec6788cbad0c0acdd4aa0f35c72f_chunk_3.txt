respondenceto:ldery@andrew.cmu.edu
1Codeavailableat:https://github.com/ldery/Automating-Auxiliary-Learning.
1
3202
beF
72
]GL.sc[
2v28041.5022:viXra
PublishedasaconferencepaperatICLR2023
Model Representation ( ) Output ( ). For instance, in RL, a common auxiliary objective
R → O
istopredicttheenvironment’sforwarddynamics(Agrawaletal.,2016;Hafneretal.,2019). To
construct this objective, the current task state-action pair ( ) is corrupted ( ) and then passed
D T
throughthemodeltoproducealatentrepresentation( )whichisfinallyusedtopredictthenext
R
state( ). Similarly,inNLP,theXLNet(Yangetal.,2019)objective—whichperformslanguage
O
modellingonarandomlyfactorizedpermutationoftheinput—canbewrittenwithinourtaxonomyas
=Out-of-Domain, =No-op, =Random-Factorized, =NextToken. Thesetwoexamples
{D T R O }
(alongwithotherslistedinFigure1)fallwithinaclasswetermnamedobjectives: objectivesthat
havebeenpreviouslyproposedintheauxiliarylearningliterature.
Decomposing named objectives Data( ) Transform( ) Representation( ) Output( )
D T R O
within our taxonomy provides Out-of-domain No-Op Bidirectional NextToken
a unified view of the auxiliary In-domain Replace Left-to-Right Real/Synth
Taskdata Mask Right-to-Left DenoiseToken
learning landscape. From this NeuralLMData ⇥ Noisingembeds ⇥ Rand. factorized ⇥ TF-IDF
vantage point, it becomes clear............
that there are many unexplored
#
combinations of the various
TAPT= Taskdata BERT-Op Bidirectional DenoiseToken
primitives used across named GPT={ Out-of-dom! ain No-Op! Left-to-Righ! t NextToken }
{!!! }
objectives. This presents a New-Obj1= {Taskdata!BERT-Op
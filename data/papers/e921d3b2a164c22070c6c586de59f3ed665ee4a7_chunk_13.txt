linearleafexperts,theHRME-SVRmodelyields
Baselines To promote a fair evaluation, we compare our
smootherpredictionsthantheHRME-LRmodelwithlower
method with a wide range of baselines: linear regression
biasandvariance. Additionally,weobservethatallmodels
(LR),supportvectorregression(SVR),decisiontrees(DT),
hereprefertheupperlinetothelowerlineduetothehigher
randomforests(RF),hierarchicalmixtureofexperts(HME)
noiselevelinthelowerline.
withstrongGaussianorGaussianprocessexperts,andmulti-
layerperceptron(MLP).Eachmodelcarriesasetofparame- Figure 1b shows the predictions made by the experts in
terstobeestimatedaswellashyperparameters(e.g.,margin HRME-SVR model. We see that there are total fourteen
andkernelsinSVR,depthandnumberofnodesinDTand experts (indicated by colored curves) being allocated to
RF,numberofneuronsandlearningrateinMLP,etc.) tobe different regions of the data. Each expert is confident of
tuned. Wetrainthemodelsontrainingsets,andfine-tune makingpredictionswithinonedatamode,asindicatedby
thehyperparametersusinggrid-searchandthree-foldcross higherposteriorprobabilities(darkercolors),andalldata
validationonthetrainingsetstoobtainthebestperformance. modesaresuccessfullycaptured. Consequently,ifwehave
Themodelsareimplementedwithscikit-learntoolkit(Buit- priorknowledgeofthedatadistribution,thiscouldbeused
inck et al., 2013) or PyTorch (Paszke et al., 2017). For toselecttheexpertsformakingthebestpredictions. Fur-
HMEmodels,weobtainthebestavailableresultsfromthe ther,insteadofusingweighted-averageoverallexperts,we
HierarchicalRoutingMixtureofExperts
10.0 oprriegdinal 10.0 oprriegdinal 10.0 oprriegdinal 2.86
7.5 7.5 7
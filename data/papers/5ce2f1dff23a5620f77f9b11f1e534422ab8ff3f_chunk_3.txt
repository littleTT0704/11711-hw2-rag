receptacles and objects the agent sees. Combined with 2020) policies conditioned on natural language instruc-
growing roll-outs, the state becomes too verbose to fit tion or goal (MacMahon et al., 2006; Kollar et al.,
into any LLM. 2010). While some prior research has used pre-trained
language embeddings to improve generalization to new
In this work, we explore alternative mechanisms to
instructions(Nairetal.,2022),theylackdomainknowl-
leverage the prior knowledge encoded in LLMs without
edge that is captured in LLMs. Our PET framework
impactingthetrainablenatureoftheactor. Wepropose
enables planning, progress tracking, and observation
a 3-step framework (Figure 1): Plan, Eliminate, and
filtering through the use of LLMs, and is designed to
Track (PET). Plan module simplifies complex tasks
be compatible with any language conditional policies
by breaking them down into sub-tasks. It uses a pre-
above.
trainedLLMtogeneratealistofsub-tasksforaninput
task description employing example prompts from the
LLMs for Control LLMs have recently achieved
training set similar to Huang et al. (2022a); Ahn et al.
success in high-level planning. Huang et al. (2022a)
(2022). TheEliminatemoduleaddressesthechallenge
shows that pre-trained LLMs can generate plausible
of long observations. It uses a zero-shot QA language
plans for day-to-day tasks, but the generated sub-tasks
model to score and mask objects and receptacles that
cannot be directly executed in an end-to-end control
areirrelevanttothecurrentsub-task. TheTrackmod-
environment. Ahn et al. (2022) solves the executability
ule uses a zero-shot QA language model to determine
issue by training an action scoring model to re-weigh
if the current sub-task is complete and moves to the
LLM action choices and demonstrates success on a
next sub-task. Finally, the Action Attention agent
robot. However, LLM scores work for simple environ-
uses a transformer-based architecture to accommodate
ments with actions limited to pick/place (Ahn et
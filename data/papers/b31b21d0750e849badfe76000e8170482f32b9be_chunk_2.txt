6.9%exactmatch. 1
1 INTRODUCTION
Weaddressthetaskofnaturallanguagetocodegeneration(NL→code): generatingacodesnippet,
writteninageneral-purposeprogramminglanguagesuchasPythonorBash,givenanaturallanguage
intent. Thistaskhasseensharplygrowingpopularityrecentlyduetotheemergenceoflargelanguage
modelstrainedonvastamountsofnaturallanguageandcode(Chenetal.,2021;Xuetal.,2022;
Friedetal.,2022). NL→codemodelsfacilitateprogrammingforbothprofessionalandinexperienced
programmers,byallowingprogrammerstowritecodebyonlyexpressingtheirhigher-levelintent.
Many existing code generation models either learn directly from input-output pairs provided as
trainingdata(Allamanisetal.,2015;YinandNeubig,2017;Iyeretal.,2018;Brockschmidtetal.,
2019;Xuetal.,2020;Alonetal.,2020;Wangetal.,2021), orlearnthemappingbetweeninput
and output implicitly from naturally occurring corpora of intertwined natural language and code
(Austinetal.,2021;Nijkampetal.,2022). Nevertheless,alltheseworksassumethatalllibraries
andfunctioncallswereseeninthetrainingdata;andthatattesttime,thetrainedmodelwillneedto
generateonlyseenlibrariesandfunctioncalls. However,newfunctionsandlibrariesareintroduced
allthetime,andevenaseenfunctioncallcanhaveunseenarguments. Thus,theseexistingmodels
inherentlycannotgeneralizetogeneratesuchunseenusages.
In contrast to these existing models, human programmers frequently refer to manuals and docu-
mentationwhenwritingcode(Nykazaetal.,2002;Lethbridgeetal.,2003). Thisallowshumans
toeasilyusefunctionsandlibrariestheyhaveneverseennorusedbefore. Inspiredbythisability,
1Dataandcodeareavailableathttps://github.com/shuyanzhou/docprompting.
1
3202
beF
81
]LC.sc[
3v78950.7022:
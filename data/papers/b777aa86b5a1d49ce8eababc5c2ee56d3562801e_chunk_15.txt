oslowdowns
higherlatencythanBERT,andremainsslowereven
indeploymentsettingswheredepthwiseandpoint-
atlargerbatchsizes. Whilesomearchitecturalinno-
wiseconvolutionsmayhavelimitedhardwareand
vationsdecreasethetotalnumberofmodelFLOPs,
frameworkbackendsupport.3
someapproachesincreasethesizeoftensoropera-
torgraphs(e.g. vis-a-visadditionallayers)which
4.3 HardwareConsiderations
canultimatelyincreaseinferencelatency.
100
4.2.3 EfficientMobileArchitectures Nvidia 1080Ti
Nvidia 2080Ti
Nvidia 3090
Nvidia V100
10 1 Nvidia A100
10 1
10 2 10 2
100 101 102 100 101 102
Batch Size Batch Size
Figure 9: Framework overhead occurs across genera-
tionsofGPUhardware,withincreasingprominenceas
Figure8: LatencyofTransformermodelsusingefficient
hardwarespeedsincreasewithnewergenerations.
variationsofconvolutionandself-attentionoperations.
Allofthevariantsobservelowerlatencyatlargebatch
sizes,buthaveworseFLOPutilization. EfficientTrans- InFigure9,weobservethatframework-bounded
former variants are slower than BERT at small batch behaviorsduringinferenceacrossmultiplegener-
sizesduetotheintroductionofmorelayers.
3Pointwiseconvolutionsareoftenmemory-bound:https:
//pytorch.org/tutorials/recipes/recipes/tuning_guide.html
)s(
ycnetaL
ations of consumer, workstation, and datacenter Dynamic computational graphs can be faster
NvidiaGPUs. Asthespeedoftheacceleratorin- forinputsentenceswithvariablelengths. Dy-
creases, the relative execution speed of the com- namic computational graphs can leverage input
pute kernels decreases while the total framework sparsity to reduce latency when processing vari-
overhead due to CPU kernel dispatch operations ablelengthtext. Forexample,PyTorchwithsparse
remainsconstant. Weobserve
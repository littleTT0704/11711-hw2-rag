 fusion layers for
Someotherapproachesexploitedvariouscontrastivelearn- computing the loss. As we will show in our experiments,
ing techniques to directly optimize the feature space with- thistechniquecanbeseamlesslyappliedtobothlarge-scale
out multi-modal fusion [35, 34, 31, 14]. In most of previ- pretraininganddownstreamtasks.
2
3.Method ingto[15,37],p(v |t )canbeapproximatedby:
j i
3.1.Framework exps(vj,ti)
p(v |t )∼ (1)
j i (cid:80)N exps(vk,ti)
AsdepictedinFig.1,ourmodelhasthreecomponents: k=1
Videoencodingmodulef θv. Itisimplementedbyastack where s(v,t) is the alignment score between v and t; the
of self-attention layers parameterized by θ v. Here, we as- denominator is a sum over all possible videos, which is a
sume the input video features have been already extracted partitionfunctionfornormalization. Addingcross-entropy
using some pre-trained models such as 2D CNN (e.g., lossonp(v |t ),wecanthenderivetheNCEloss[15]:
j i
ResNet [18])or 3D CNN(e.g., I3D[4], S3D[51]). Given
theinputvideoembeddings,videoencoderstartswithalin- N
(cid:88)
ear layer to project them to the same dimension d as fol- L nce = −logp(v i|t i)
lowing self-attention layers. We denote the output of our i=1 (2)
videoencoderforavideoclipbyasequenceofmfeatures, (cid:88)N (cid:32) exps(vi,ti) (cid:33)
∼ −log
x = {x1,...,xm} ∈ Rm×d. Thenumberoffeaturesmde- exps(vi,ti)+(cid:80) exps(vk,ti)
i=
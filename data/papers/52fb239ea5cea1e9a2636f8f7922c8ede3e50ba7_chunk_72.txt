for tasks containing only one dataset.
3.4 L¯ila-Robust
In light of recent work demonstrating the brittleness of language models at
solving math problems (Patel et al., 2021), we create a high-quality evaluation
dataset,L¯ila-Robust,toevaluateperformanceonmathematicalreasoningtasks
when linguistic perturbations are introduced. Specifically, we define and apply
a set of carefully chosen augmentation templates, summarized in Table 16, on
each task, yielding a set of challenging problems that are consistent answer-wise
but stylistically different question-wise. Overall, we define a total of 9 templates
for such question perturbations: 3 from Patel et al. (2021) and 6 of our own.
From each constituent dataset, we sample 20 questions and obtain perturbed
question annotations via Amazon Mechanical Turk (AMT). Refer to Appendix
B.1 for additional details on the construction of L¯ila-Robust.
3.5 Statistics
Table 2 shows key statistics of our proposed benchmark, L¯ila. L¯ila contains
134K examples with significant diversity across question, answer, program
≈
and instruction length (see detailed statistics in Appendix C). Figure 2 shows
the diversity of questions in L¯ila. Note that we downsample (via random
selection) some datasets like AMPS (Hendrycks et al., 2021b) which contains
numeroustemplatedquestionsthatcangetover-representatedinthedistribution
of examples across categories in L¯ila.
4 Experiments
In this section, we introduce our modeling contributions for the L¯ila benchmark
and discuss the overall experimental setup.
Data partition and evaluation. For the IID setup, we randomly partition
the data in each task into training (70%), development (10%) and test (20%)
sets. Additionally, we also evaluate on L¯ila-OOD and L¯ila-Robust settings;
thus, the final evaluation scheme is a combination of the performance on all
three evaluation setups
7
Statistic Number
# Total tasks 23
# Total datasets 44
# Total instructions 44
# Total questions 133,815
# Total programs 358,769
Unique questions 132,239
Unique programs 325,597
Unique answers 271,264
Average length of instructions 31.18
Average length of questions 47.72
Average
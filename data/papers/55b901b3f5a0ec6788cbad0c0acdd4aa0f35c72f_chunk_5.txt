hefollowingprescriptionsfromourtheory: (i)auxiliarytasks
thataremoresimilartotheend-taskaredesirable. Givenasetofobjectives,AANGlearnsadaptive
weightstobringthecompositeobjectiveclosertotheend-task;(ii)ingeneral,moreauxiliarydatais
better. AANGmaximizestheeffectiveamountofdatausedintrainingbyusingallthegenerated
objectivesinsteadoftakingtask-specificsubsets.
Toempiricallyvalidateourmethodforautomaticallygeneratingandutilizingauxiliaryobjectives,
we experiment on five NLP tasks. We do so in the widely-used setting of continued pre-
training(Gururanganetal.,2020;Aghajanyanetal.,2021;Deryetal.,2021b;Zhangetal.,2022),
where a model trained with a single auxiliary objective on large-scale data is further trained on
end-taskrelateddata. Withoutintroducinganyexternaldataorarchitecturalmodifications,variants
of AANG outperform strong and widely used baselines in 4 out of 5 tasks. AANG achieves an
averageimprovementof4.2%overstandardfine-tuningofRoBERTaacrossourchosentasks. We
believeourresultswillspurfurtherresearchintoexploringautomatingauxiliarylearningacrossa
varietyofsettings. Notably,whilewefocusonNLPwhendiscussingthespaceofauxiliaryobjectives
(Section3)andinourempiricalevaluation(Section6),ourtheoreticalresults(Section4)andAANG
itselfaredomain-agnostic2.
2OurideascouldbeappliedtodomainslikeRLorcomputervision(CV),whereasimilardissectionof
existingobjectivescanbeperformed.
2
PublishedasaconferencepaperatICLR2023
2 RELATED WORK
Toproperlyscopethiswork,wedefineauxiliarylearningastrainingamodelonalternativeobjectives
with the goal of improving performance on some primary end-task. Auxiliary learning is an
instantiation of transfer learning (Caruana, 1997; Baxter, 2000; Ruder et al., 2019). It covers
thepretrain
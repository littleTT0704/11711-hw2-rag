)
S S INN φS (x,y)∼Dˆ train student θt φt S φt T
After updating the student, we take an extra gradient step with the new parameters but only use
theseupdatestocalculatetheouter-gradientforφ,withoutactuallyupdatingθ. Thisapproachis
T
similartothepilotupdateproposedbyZhouetal.[2021],andweverifiedthatitledtomorestable
optimizationinpractice:
(cid:104) (cid:105)
θ(φt )=θt+1−η ∇ E L (S,E,T,E,x) (8)
T INN θ (x,y)∼Dˆ
train
student θt+1 φt S+1 φt
T
(cid:104) (cid:16) (cid:17)(cid:105)
φt+1 =φt −η ∇ E L S (x),T(x). (9)
T T OUT φT (x,y)∼Dtest sim θ(φt T)
4 ParameterizedAttentionExplainer
Asasecondcontributionofthiswork,weintroduceanovelparameterizedattention-basedexplainer
thatcanbelearnedwithourframework. Transformermodels[Vaswanietal.,2017]arecurrentlythe
mostsuccessfuldeep-learningarchitectureacrossavarietyoftasks[Shoeybietal.,2019,Wortsman
etal.,2022]. Underpinningtheirsuccessisthemulti-headattentionmechanism,whichcomputesa
normalizeddistributionoverthe1≤i≤Linputelementsinparallelforeachheadh:
Ah = SOFTMAX(Qh(Kh)(cid:62)), (10)
where Qh = [qh,···,qh] and Kh = [kh,···,kh] are the query and key linear projections over
0 L 0 L
the input element representations for head h. Attention mechanisms have been used extensively
forproducingsaliencymaps[WiegreffeandPinter,2019,Vashishthe
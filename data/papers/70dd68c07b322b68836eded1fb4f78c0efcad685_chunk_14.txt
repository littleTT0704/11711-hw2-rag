 and models, we refer to the original
guistics. papers’codebaseforhyperparameters.567
Rico Sennrich, Barry Haddow, and Alexandra Birch. XNLI For encoder-only models, the first token
2016. Neural machine translation of rare words
([CLS])isusedtomapthesentencerepresentation
withsubwordunits. InProceedingsofthe54thAn-
tothelabeldistribution. Forencoder-decodermod-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715– els, we generate the index of the label (e.g., ‘0’)
1725, Berlin, Germany. Association for Computa- directly.
tionalLinguistics.
NER Forencoder-decodermodels,wefollowthe
Xinyi Wang, Sebastian Ruder, and Graham Neubig.
input-outputformat(e.g.,input: ‘tag: rick and
2021. Multi-view subword regularization. In Pro-
morty are cool.’, output: ‘PER: rick $$
ceedingsofthe2021ConferenceoftheNorthAmer-
ican Chapter of the Association for Computational PER: morty’)specifiedinthemT5model’sorigi-
Linguistics: Human Language Technologies, pages nalcodebase.
473–482, Online. Association for Computational
Linguistics. B Perlanguageresults
Zihan Wang, Karthikeyan K, Stephen Mayhew, and
B.1 Mainexperiments(Zero,Single,Multi)
Dan Roth. 2020. Extending multilingual BERT to
low-resource languages. In Findings of the Associ- XNLI:Tab.2,NER:Tab.3,TyDiQA-GoldP:Tab.4
ationforComputationalLinguistics: EMNLP2020,
pages2649–2656,Online.AssociationforComputa- B.2 Singlelanguagefine-tuningrobustness
tionalLinguistics.
Fig.3
Linting Xue, Aditya Barua, Noah Constant, Rami Al-
Rfou, Sharan Narang, Mihir Kale, Adam Roberts,

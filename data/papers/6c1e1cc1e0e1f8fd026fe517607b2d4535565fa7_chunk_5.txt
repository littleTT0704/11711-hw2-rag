amplesthatarepro-
butproduceanincorrectanswer.
vided as part of the test-time input (Brown et al., 2020;
We demonstrate the effectiveness of PAL across 13 arith- Liuetal.,2021;Chowdheryetal.,2022),wherek isusu-
metic and symbolic reasoning tasks. In all these tasks, allyanumberinthelowsingledigits. Theseinput-output
PAL usingCodex(Chenetal.,2021a)outperformsmuch examples {(x,y )}k are concatenated in a prompt p
i i i=1
largermodelssuchasPaLM-540Busingchain-of-thought ≡(cid:104)x ·y (cid:105)(cid:107)(cid:104)x ·y (cid:105)(cid:107)...(cid:107)(cid:104)x ·y (cid:105). where“·”denotes
1 1 2 2 k k
prompting. For example, on the popular GSM8K bench- theconcatenationofaninputandoutput,and“(cid:107)”indicate
mark, PAL achieves state-of-the-art accuracy, surpassing theconcatenationofdifferentexamples. Duringinference,
PaLM-540B withchain-of-thoughtbyabsolute15%top- atestinstancex isappendedtotheprompt,andp(cid:107)x
test test
1accuracy. Whenthequestionscontainlargenumbers,a ispassedtothemodelwhichattemptstocompletep(cid:107)x,
test
datasetwecallGSM-HARD,PALoutperformsCOTbyanab- andtherebygenerateananswery. Notethatsuchfew-
test
solute40%. Webelievethatthisseamlesssynergybetween shotpromptingdoesnotmodifytheunderlyingLLM.
aneuralLLMandasymbolicinterpreterisanessentialstep
towardsgeneralandrobustAIreasoners.
PAL:Program-aidedLanguageModels 3
Weietal.(2022)additionallyaugmenteachin-con
withcontrastivepredictive
coding. arXivpreprintarXiv:1807.03748,2018.
MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,KentonLee,and
LukeZettlemoyer. Deepcontextualizedwordrepresentations. CoRR,abs/1802.05365,2018. URL
http://arxiv.org/abs/1802.05365.
HieuPham,MelodyGuan,BarretZoph,QuocLe,andJeffDean. Efficientneuralarchitecturesearch
viaparameterssharing. InInternationalConferenceonMachineLearning,pp.4095–4104.PMLR,
2018.
XipengQiu,TianxiangSun,YigeXu,YunfanShao,NingDai,andXuanjingHuang. Pre-trained
modelsfornaturallanguageprocessing: Asurvey. ScienceChinaTechnologicalSciences, pp.
1–26,2020.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understandingbygenerativepre-training. 2018.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. arXivpreprintarXiv:1910.10683,2019.
Nicholas Roberts, Mikhail Khodak, Tri Dao, Liam Li, Christopher Re´, and Ameet Talwalkar.
Rethinkingneuraloperationsfordiversetasks. arXivpreprintarXiv:2103.15798,2021.
SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf. Transferlearning
innaturallanguage
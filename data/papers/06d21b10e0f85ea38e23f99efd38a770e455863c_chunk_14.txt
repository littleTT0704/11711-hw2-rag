.9 25.9 31.6 9.7 4.7 5.6
tr 2.6 34.3 48.8 7.1 1.6 1.4
5.2 Evaluation Avg. 4.0 29.1 39.0 8.6 3.6 4.2
Table 4: Results on the Tatoeba bitext mining task
WeevaluatesentenceembeddingsusingtheSem-
(Artetxe and Schwenk, 2019). Results are measured
Evalsemantictextualsimilarity(STS)tasksfrom
inerrorrate 100.
2012to2016(Agirreetal.,2012,2013,2014,2015, ×
2016) as was done initially for sentence embed-
dings in (Wieting et al., 2016b). Given two sen- similarity. Performanceismeasuredbycomputing
tences,theaimoftheSTStasksistopredicttheir theerrorrate.
similarityona0-5scale,where0indicatesthesen-
tencesareondifferenttopicsand5meanstheyare 6 Results
completely equivalent. As our test set, we report
English Semantic Similarity. The results for
theaveragePearson’sr overeachyearoftheSTS
our English semantic similarity evaluation are
tasksfrom2012-2016asisconvention.
shown in Table 2. Our P-SP model has the best
Most work evaluating accuracy on STS tasks
performance across each year of the task, signif-
has averaged the Pearson’s r over each individ-
icantly outperforming all prior work. We outper-
ual dataset for each year of the STS competition.
form methods that use large pre-trained models
However,ReimersandGurevych(2019)computed
includingSentence-BERTwhichissupervised,as
Spearman’sρoverconcatenateddatasetsforeach
itistrainedonNLIdata(Bowmanetal.,2015).
year of the STS competition. To be consistent
WealsoincluderesultsfromSimCSE(Gaoetal.,
with previous work, we re-ran their model and
2021). We compare to the unsupervised version,
calculatedresultsusingthestandardmethod,and
sinceourmodelisalsounsupervised. Weevaluate
thusour
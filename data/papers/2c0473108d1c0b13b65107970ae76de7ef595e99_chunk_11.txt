to1with
an increment of 0.1. While training with MP and MMP, the
initial margin β is set to 0.1 and smoothing factor α is set to
10. Inspired by the idea that balanced training samples mat-
ter[3,11,12],wealsoadoptthismannerinMPandMMP.Con-
cretely, the number of samples for each class is a fixed value
M whichisahyper-parameter. Inthisexperiment, wesetM
as2basedonthefactthatfewershotlearningmattersinAngu-
larPrototypicalloss[3]. WecallthismannerMP-Balancefor
MaskProxyandMMP-BalanceforMultinomialMaskProxy.
Weadoptthe*expectedbatchsize*Bof800and4001respec-
tively. WeapplySGDasoptimizerwithastartinglearningrate
of0.2andReduceLROnPlateauaslearningrateschedulerwith
Figure10: EERonthetestset-VoxCeleb1datasetusingdif-
afactorof0.8,patienceof3andEER%asmetric.
ferentlossobjectivesoverepochs. DetailscanbefoundinSec.
Basedontheaforementionedstatement,threeexperiments
4.3
areperformedandwedenotethemasE1(τ = 2s,B = 400),
E2(τ = 2s,B = 800), E3(τ = 4s,B = 400)respectively. AblationStudyBasedontheresultsinTable. 2,balanced
Wealsoduplicatetheexperimentsin[3]usingTriplet,Prototyp- datainputsalwaysresultinlowerEERforbothMPandMMP.
ical, GE2EandAngularPrototypicalrespectively. Toanalyze Both larger batch size B and larger training audio duration τ
thetrainingspeedoverdifferentlossobjectives,ineachepoch, givelowerEERforMP,MMP,MP-BalanceandMMP-Balance.
wecomputetheEER%ontestsetandrecordtheresultsinFig. TheseconclusionsholdforallthreeexperimentsE1, E2and
10.Basedonourexperiments
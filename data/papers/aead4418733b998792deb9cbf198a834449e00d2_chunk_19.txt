emethodsaddresssystematicitywithin-
ductivebiasesinmodelstructure(Andreasetal.2016;Bah-
Robustness. First, we study the simple-primitive robust-
danau et al. 2019), and others through the data (Hill et al.
nessproblems(e.g.k exp(k x),see2),astheseshortprob-
1 2
2020; Andreas 2020) or learning procedure (Lake 2019;
lemsresultedinasmallnumberoftimeouts,allowingusto
Vanietal.2021).Wefocusonsystematicgeneralizationde-
scaleupsearchandverification.Weincreasethebeamsize
ficienciesinastate-of-the-artmodelinanewsetting–sym-
to500candidates,andstudythemodel’sprobabilitiesonthe
bolicintegration–withadditionalaspectsofgeneralization.
500returnedcandidatesandcorrectsolutions.
When a correct solution is within the 500 returned can- Robustness and adversaries in sequence models. Sev-
didates,thecorrectsolutionoftenhasmuchlowerprobabil- eralworksstudyrobustnessinNLP,includingclassification
itythanthetopcandidate,p (y(1) |x) ≫ p(y |x).Specif- (Tuetal.2020),wordsubstitutions(Jiaetal.2019),domain
θ beam ∗
ically, correct solutions often appear at the bottom of the shift in QA (Kamath, Jia, and Liang 2020) or topic distri-
candidates(3,orange),yetonaveragethebottomcandidate butions(Orenetal.2019).Severalmethodsfindadversarial
y(500)hasprobability≈0.0000035,whilethetopcandidate examplesinNLP(Morrisetal.2020).Alzantotetal.(2018)
beam use genetic algorithms in a classification setting, while we
y(1) hasprobability≈0.92.Thesearemodeldeficiencies:
beam considergeneration.Micheletal.(2019)constraininputse-
themodelisconfidentlyincorrect,assigningveryhighprob-
quences to be similar and use a gradient
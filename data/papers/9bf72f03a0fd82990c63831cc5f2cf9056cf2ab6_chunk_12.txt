,AmirGholami,ZheweiYao,MichaelW
withtensorflowmodeloptimizationtoolkit-perfor- Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-
mancewithaccuracy. only bert quantization. In International conference
onmachinelearning,pages5506–5518.PMLR.
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia
Liu, Yang Zhang, Zhangyang Wang, and Michael Jing Liu, Jianfei Cai, and Bohan Zhuang. 2022.
Carbin.2020. Thelotterytickethypothesis forpre- Sharpness-aware quantization for deep neural net-
trained bert networks. Advances in neural informa- works. arXivpreprintarXiv:2111.12273.
tionprocessingsystems,33:15834–15846.
SanketVaibhavMehta,DarshanPatil,SarathChandar,
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. and Emma Strubell. 2021. An empirical investiga-
2021. Whenvisiontransformersoutperformresnets tion of the role of pre-training in lifelong learning.
without pre-training or strong data augmentations. arXivpreprintarXiv:2112.09153.
arXivpreprintarXiv:2106.01548.
Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Zheltonozhskii, Ron Banner, Alex M. Bronstein,
KristinaToutanova.2018. Bert:Pre-trainingofdeep andAviMendelson.2020. Lossawarepost-training
bidirectional transformers for language understand- quantization.
ing. arXivpreprintarXiv:1810.04805.
ShengShen,ZhenDong,JiayuYe,LinjianMa,Zhewei
StevenKEsser,JeffreyLMcKinstry,DeepikaBablani,
Yao,AmirGholami,MichaelWMahoney,andKurt
Rathinakumar Appuswamy
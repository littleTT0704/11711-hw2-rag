(e.g.,documentationofnewlyreleasedlibraries),withoutre-traininganymodel
component. Thisway,DocPromptingcanleveragenewlyaddeddocumentation,anditcangenerate
codecontainingunseenandunusedfunctionsandlibraries. DocPromptingisgeneralandapplicable
toanyprogramminglanguageandunderlyingbasearchitecture. Tothebestofourknowledge,thisis
thefirstdemonstrationofleveragingdocumentationinmodelsofcodeexplicitlyandeffectively.
WedemonstratetheeffectivenessofDocPromptingontwoNLâ†’codebenchmarksandtasks,across
twoprogramminglanguages,andusingseveralbasemodels:GPT-Neo(Blacketal.,2021),T5(Raffel
etal.,2020),CodeT5(Wangetal.,2021),Fusion-in-Decoder(IzacardandGrave,2021)),andCodex
(Chenetal.,2021). Further,weexperimentwithbothsparseretrieverssuchasBM25(Robertsonand
Jones,1976)anddenseretrievalmodelssuchasSimCSE(Gaoetal.,2021). Finally,weintroduce
twonewbenchmarksforretrieval-basedcodegeneration: (a)inBash,wecurateanewbenchmark
by crawling the tldr repository, and constructing the training/development/test splits without
overlappingcommands;(b)inPython,were-splitthepopularCoNaLabenchmark(Yinetal.,2018)
bymakingeverytestexamplecontainatleastonePythonfunctionthatisnotseeninthetrainingdata.
ModelsthatuseDocPromptingconsistentlyoutperformtheirbasemodelsthatgeneratecodesolely
basedontheNLintents. UsingDocPromptingimprovesstrongbasemodelssuchasCodeT5by
2.85%inpass@1(52%relativegain)and4.39%inpass@10(30%relativegain)inexecution-based
evaluationinCoNaLa;onthenewtldrdataset,DocPromptingimprovesCodeT5andGPT-Neo-
1.3B
insocialmediabylookingatconversationalthreads.
PloSone,11(3):e0150989.
A DataPreprocessing TOXICHAT SemEval2017
#words 202K 63K
As a data cleaning step, we replaced all urls in
#words/sentence 23.5 13.9
the threads with a special token. We also limited
#sentences 8623 4519
the posts to ≤ 70 words and comments to ≤ 50
avg. threadlen. 3.31 2.85
words. Onlythepostscontainingtextualdatawere
#stancelabels 12492 4519
allowed.
Table 5: Comparison of corpus statistics of TOXI-
B OffensiveSubRedditDataCollection CHAT againstSemEval2017-ChallengeTask8(Der-
czynskietal.,2017)stancedataset.
Existing datasets of offensive language (Breit-
feller et al., 2019; Sap et al., 2020) annotated allu firstu replyu
commentsfrompotentiallyoffensiveSubReddits NBOW(CE).515.623.485
to increase proportion of offensive language. To BERT(CE).633.687.618
annotate our conversation corpus, we similarly DGPT(CE).667.681.662
consider these previously used 28 SubReddits DGPT+(CE).686.704.680
in Breitfeller et al. (2019) and some additional
community-reported hateful SubReddits in Table6: Devset, OffensiveF 1 scoresforallutter-
r/AgainstHateSubReddits.6 We sample ances,firstutterancesandreplyutterancesinallthreads.
DGPT+ indicates DGPT model trained on our dataset
threads with last offensive comment using a
augmentedwithinstancesfromSBIC(Sapetal.,2020).
BERT offensive comment classifier (Devlin
et al., 2019) trained on SBIC (Sap et al., 2020),
P(offensive) ≥ 0.7. Finally,weselecttop10most andtrainedfor30epochs. BERTandDGPTmod-
offensive SubReddits based on their proportion elsarefine-tunedfor12epochs. TheDGPTmodel
and availability of the offensive threads. The fine
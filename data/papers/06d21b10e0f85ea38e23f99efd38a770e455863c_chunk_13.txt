
whichcaseweuseasingleunknown-wordtoken.
OurtrainingdataisamixtureofOpenSubtitles
20188 (Lison and Tiedemann, 2016), Tanzil cor- WeoptimizeourmodelsusingAdam(Kingmaand
pus9 (Tiedemann, 2012), Europarl10 for Spanish, Ba, 2014) with a learning rate of 0.001 and train
GlobalVoices11 (Tiedemann,2012),andtheMul- modelsfor25epochs.
tiUN corpus.12 We follow the same distribution For training on the bilingual corpora, we tune
forourlanguagesofinterestacrossdatasourcesas eachmodelonthe250example2017EnglishSTS
ArtetxeandSchwenk(2019)forafaircomparison. task (Cer et al., 2017). We vary dropout on the
embeddingsover 0,0.1,0.3 andthemega-batch
{ }
8http://opus.nlpl.eu/OpenSubtitles.php sizeM over 60,100,140.
{ }
9http://opus.nlpl.eu/Tanzil.php
10http://opus.nlpl.eu/Europarl.php 13https://opus.nlpl.eu/Tatoeba.php
11https://opus.nlpl.eu/GlobalVoices.php 14Annealing rate is the number of minibatches that are
12http://opus.nlpl.eu/MultiUN.php processedbeforethemegabatchsizeisincreasedby1.
383
For training on ParaNMT, we fix the hyperpa- LanguageLASERXLM-RmBARTCRISSLaBSE P-SP
rametersinourmodelduetotheincreaseddatasize ar 7.8 52.5 61.0 22.0 9.1 8.8
making tuning more expensive. We use a mega- de 1.0 11.1 13.2 2.0 0.7 1.5
es 2.1 24.3 39.6 3.7 1.6 2.4
batch size M of 100 and set the dropout on the
fr 4.3 26.3 39.6 7.3 4.0 5.4
embeddingsto0.0. ru 5
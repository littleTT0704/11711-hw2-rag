task(Zweigenbaumetal.,2018). WeusethetheProbablyAskedQuestionsdataset
TheTatoebadatasetconsistsof100–1000pairs (PAQ) (Lewis et al., 2021) as a knowledge base
ofdataalignedtoEnglishfor112languages. The from which we look up the nearest neighbor of
accuracy for Tatoeba can be computed in two eachquestionintheNQandMKQAtestsetsusing
ways,dependingifEnglishisthetargetlanguage cosinesimilarity. PAQisaverylargeresourceof65
or source language. We compute accuracy using million automatically generated question-answer
pairs. This is a zero-shot evaluation without any contrastive losses like triplet loss (Weston et al.,
NQsuperviseddata.9 2010)whichhasbeenusedforlearningtextembed-
dings,butweleaveacomparisonofcontrastiveob-
OverallScore Weconsolidatealloftheseevalua-
jectivesforlearningmultilingualtextembeddings
tionsintoascore,asawaytogetasenseofoverall
forfuturework.
performancesincedifferentmodelsfavordifferent
ThesecondbaselineisBITRANSLATION,where
evaluations. Whileweareaveragingdifferentmet-
weuseatranslationobjectivetolearntherepresen-
rics(accuracy,Pearson’sr,andF ),wejustifythis
1 tation Espana-Bonet et al. (2017); Schwenk and
astheydohavethesamescale,10 andasimpleav-
Douze(2017);ArtetxeandSchwenk(2019b).
eragegivesawayforustoseeoverallperformance.
We also explore an alternative to the VMSST,
Ourscoreistheaverageofsixsubtasks,twosub-
VMSST CONTRASTIVE,byincorporatingacon-
tasksforeachofsemanticsimilarity,bitextmining,
trastive loss to use in a multitask setting. Again,
andquestionretrieval: Englishsemanticsimilarity,
weweightthecontributionoftheVMSSTlossby
cross-lingualsemanticsimilarity,Tatoeba,BUCC
λ.
(weaverageperformanceoft
sts(RF),assug-
conducive to our goal of learning grammar rules. gestedbyanonymousreviewers,butfoundthedecisiontrees
We experiment with treebanks for 61 languages, (DT)tobeslightlyunderperforming((avg.) -0.12acc). But
giventhatitisstraightforwardtoextractinterpretablerules
whicharepubliclyavailablewithannotationsfor
fromDT,whichisourprimarygoal,ascomparedtoRF,we
POS tags, lemmas, dependency parses, and mor- usetheformerforallexperiments,detailsinAppendixD.
(cid:4)baseline(cid:4)syntactic specificdatasetonwhichitwastrained,wetrain
(cid:4)syntactic+lexical(cid:4)syntactic+semantic amodelononetreebankofalanguageandapply
Adj-NounWordOrder NounCaseMarking the trained model directly on the test portions of
100 100 other treebanks of the same language. There are
80
80 30 languages in the SUD which fit this require-
60
60 ment. Figure4intheAppendixdemonstratesone
40
suchsettingforunderstandingthewordorderpat-
English Spanish Greek Turkish
terns across different French corpora, where the
Figure3: Comparingtheeffectofdifferentfeatureson
models have been trained on the largest treebank
thewordorderandcasemarking.
(fr-gsd). Forsubject-verborder,alltreebanksex-
and +12.8 for the high-resource. The larger the ceptthefr-fqbshowsimilarhightestperformance
treebank size, the larger the improvement of our ( >90% acc.). Interestingly, the model severely
modelâ€™s performance over the baseline. Even in underperforms (28% acc.) on fr-fqb which is a
low-resourcesettings,againoverthebaselinesug- question-bankcorpuscomprisingofonlyquestions,
gests that our approach is extracting valid rules, andquestionsinFrenchcanhavevaryingwordor-
whichisencouragingforlanguagedocumentation derpatterns.7 Themodelfailstocorrectlypredict
efforts.
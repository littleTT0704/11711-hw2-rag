methodwouldleadtoflattermin- nificantreductioninmemorybandwidthandless
imacomparedtootherquantizationmethods. carbonfootprint.
There are two primary strategies for quantiza-
tion: Post-training quantization (PTQ) quantizes
1 Introduction
theparametersofamodeltrainedinfullprecision
posthocandtendstosufferaheavypenaltyonthe
Asstate-of-the-artdeeplearningmodelsforNLP
accuracy,asitsinferencegraphdifferssubstantially
and speech (e.g. Transformers, BERT) grow in-
fromthetraining(Jacobetal.,2018). Quantization-
creasinglylargeandcomputationallyburdensome
aware training (QAT) (Bhuwalka et al., 2020)
(Devlinetal.,2018;Karitaetal.,2019),thereisin-
combatsthisdiscrepancybysimulatingquantiza-
creasingantitheticaldemand,motivatedbylatency
tionduringtraining: Allweightsarequantizedto
andenergyconsumptionconcerns,todevelopcom-
lowbit-widthduringbothforwardandbackward
putationallyefficientmodels. Modelquantization
passes of training, so that the model parameters
has emerged as a promising approach to enable
willworkwellwheninferenceisperformedinlow
âˆ—Co-firstauthor bit-width. Note that in the backward pass, QAT
2202
tcO
31
]GL.sc[
1v17170.0122:viXra
leverages straight through estimator (STE; (Ben- ingscheduletoletLSQworkstablytogether
gioetal.,2013))toestimatethegradientofthese withSAMoptimization,incontrasttoanaive
non-differentiablequantizerlayers. joint training method that leads to unstable
Todate,QATofBERTmodelsstillintroducesa performance (sometimes worse than LSQ
non-negligibleperformancelosscomparedtothe alone). Botharepioneeringefforts.
full-precisionmodels(Zafriretal.,2019;Baietal.,
2. Our experiments suggest that our SQuAT
2020). Thisdropinaccuracyiscausedbypoores-
quantizationachievesasizeable
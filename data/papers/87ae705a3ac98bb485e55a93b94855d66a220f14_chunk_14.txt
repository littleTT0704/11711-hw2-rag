orporatedomainembeddingsasinputtotheAMRLtypes
isasoftmax,andthedashedlineistherecurrentconnection.
andpropertylayers.Inbothmodels,weonlyinvestigateserial
“type”and“prop”performsequentialprediction,while“int”
embeddingsfortheSLUslotssincetheywerefoundtobe
categorizestheintentoftheuser.
betterthantheparallelarchitectureinearlyexperiments.
Inthethirdmodelarchitecture(+SLUSlots/Domain),the
domain layer is trained as a task in parallel to the AMRL
arepredictedatacoarse-grainedlevel,afterwhichthefine- typelayerandisinputtothepropertylayer.Therationale
grained properties and the actions are predicted. Residual behind this choice is that, although domain and property
connectionsareincludedtoleverageembeddingsfromprevi- predictionareverydifferenttasks,thedomain,whencorrectly
ousstages.Forexample,thewordembeddingsareusedas classified,canrestrictwhatkindofpropertiesweexpect(e.g.,
inputtotheLSTMlayerspredictingthepropertiesandthe if a sentence is classified in the Music domain we expect
actions.Figure5showsthetopologyofthismodel. propertiessuchasbyArtistbutnotweatherCondition).There
EachblockinFigure5isabi-directionalLSTM,which is still a shared embedding space learned as input to the
isusedforbothsequentialpredictionandclassification.At typemodel.Thismodel(+SLUSlots/Domain)isshownin
theoutputlayers,weformapredictionbyfeedingthecon- Figure6c.
catenated hidden representations for the token we wish to Thefinalmodelarchitecture(+SLUSlots/Domainpipe),
labelintoanotheraffinetransformfollowedbyasoftmaxto acommonLSTMembeddinglayeristrainedforSLUslots
obtainposteriorprobabilitiesoverthesetoflabels.Dropout and domains. This layer is input to the layer that predicts
isappliedaftereachLSTMlayer.Forclassification,only
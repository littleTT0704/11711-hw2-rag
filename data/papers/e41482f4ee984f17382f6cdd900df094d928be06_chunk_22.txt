onlyachievesalimited
end-to-end task success rate of 14.41%, significantly lagging behind the human performance of
78.24%. Thesefindingsunderscoretheneedforfutureresearchtofocusonenhancingtherobustness
andefficacyofautonomousagentswithinWebArenaenvironment.
ACKNOWLEDGEMENT
We would like to thank Emmy Liu, Zhiruo Wang, Zhitong Guo for examining our annotations,
Shunyu Yao for providing the raw Amazon product data in Webshop, Pengfei Liu, Zaid Sheikh
andAmanMadaanforthehelpfuldiscussions. WearealsogratefultotheCenterforAISafetyfor
providingcomputationalresources. Thismaterialispartlybasedonresearchsponsoredinpartbythe
AirForceResearchLaboratoryunderagreementnumberFA8750-19-2-0200. TheU.S.Government
isauthorizedtoreproduceanddistributereprintsforGovernmentalpurposesnotwithstandingany
copyrightnotationthereon. Theviewsandconclusionscontainedhereinarethoseoftheauthors
andshouldnotbeinterpretedasnecessarilyrepresentingtheofficialpoliciesorendorsements,either
expressedorimplied,oftheAirForceResearchLaboratoryortheU.S.Government. Thisproject
wasalsopartiallysupportedbyagiftfromAWSAI.
REFERENCES
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian D.
Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpret-
ingvisually-groundednavigationinstructionsinrealenvironments. In2018IEEEConference
on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-
22, 2018, pp. 3674–3683. IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00387.
URLhttp://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_
Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html.
RohanAnil, AndrewM.Dai, OrhanFirat, MelvinJohnson, DmitryLepikh
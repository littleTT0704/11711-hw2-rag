 code using
thatKSTprovidesgainsaswell(AppendixB). large-codegenerationmodels.
Code representationfor procedural knowledge erating output effectively. Thus, the tasks in our
reasoning Programsinherentlyencoderichstruc- work push a model to use both its reasoning and
tures,andtheycanefficientlyrepresenttaskproce- symbolicmanipulationcapabilities.
dures. Existingworksleveragethecontrol-flows,
6 Conclusion
nestedfunctionsandAPIcallsofaprogramming
language such as Python to control the situated
Wepresentthefirstworktoemploylargelanguage
agents in the embodied environment (Sun et al.,
modelsofcodeforstructuredcommonsensegen-
2019; Zhou et al., 2022; Singh et al., 2022). In
eration. By converting the output commonsense
this work, we go beyond these procedural tasks
structures to Python code, COCOGEN provides
and show the effectiveness of using Code-LLMs
a simple and effective method for leveraging the
onbroaderstructuredcommonsensetasks.
code-generationabilitiesofCode-LLMsforstruc-
turedgeneration. Theseresultsopenapromising
Adapting Code-LLMs for reasoning As code-
direction for structural commonsense reasoning.
generation models (Code-LLMs) are getting in-
Webelievethattheprinciplesandthemethodspre-
creasingly popular, there is a growing interest in
sented in this paper are applicable to additional
adapting them for a wide range reasoning tasks.
NLP tasks that require “language understanding”
Wuetal.(2022)use CODEXandPaLM(Chowdh-
and structuredprediction.
eryetal.,2022)forconvertingmathematicalstate-
ments written in natural language into a formal Acknowledgments
structurethatcanbeusedfortheoremprovers,with
WethankKaixinMa,KeisukeSakaguchiandNiket
moderate success. The task is challenging, as it
Tandonforthoughtfuldiscussionandhelpingwith
involves understanding the concepts used in the
PROSCRIPT datasets and the anonymous review-
theorem(e.g.,setofrealnumbers)andthecomplex
ers for valuable
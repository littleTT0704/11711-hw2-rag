 about the next ac-
tion. However, given information from states
in previous timesteps, the policy can decide
what action to take (whether to open the rack
ordirectlyplacetheobjectinthesink). Tothis
end,wetrainasinglepreferencepickonlypol-
icy for different context history. As shown in Figure14:Categorylevelaccuracyforsinglepref-
Fig. 15, context window of size k processes erencetrainingwithvaryingcontextwindows.
thecurrentstateaswellaskpredecessorstates,
thatis,intotalk+1states. Whilelargercontextwindowsizelearnsfaster,theasymptoticperfor-
manceforallcontextwindowsconvergesinoursetting.
Let context history k refer to the number of previous states included in the input. Then the input
is a sequence of previous k statesâ€™ instances (including the current state), as shown in Fig. 15.
Fig14showsthatTTPgets> 90%categorylevelpredictionaccuracyinvalidationforallcontext
windows. While larger context windows result in faster learning at the start of the training, the
asymptoticperformanceofallcontextsisthesame. Thispointstothedatasetbeinglargelyvisible,
and a single context window capturing the required information. In the future, we would like to
experimentwithmorecomplexsettingslikemobilerobots,whichmightrequirealongercontext.
19
E LimitationsandFuturescope
InSection6,webrieflydiscussedthelimitationsandrisks.Hereweenlistmoredetailsandhighlight
futuredirections.
Pickgraspingdependsonaccuratesegmentationandedgedetection Graspingpolicydepends
on quality of segmentation and edge detection of the selected object. Due to noise in calibration,
shadows and reflections, there are errors in detecting the correct edge to successfully grasp the
object. Forexample,itishardtograspaplateinrealsetting. Plateisveryclosetothegroundand
the depth cameras cannot detect a clean edge for grasping. Therefore, in our work, we place the
plateonanelevatedstandforeasygrasping. Graspingsuccessalsodependsonthesizeandkindof
gripperused.
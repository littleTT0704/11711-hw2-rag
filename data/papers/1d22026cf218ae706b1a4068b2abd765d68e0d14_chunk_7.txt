ationset
respectively. Example of gabor-based STRFs are shown
below in Figure 1. Inspired by the results in (Xia et al., feed-forward layers, convolution, and self-attention. The
2021) that used STRFs to capture spectro-temporal mod- initiallayeroftheconformerencoderconsistsofa2Dconvo-
ulations for emotion recognition, we developed a similar lutionaldownsamplinglayer.Weincreasetheinputchannels
spectro-temporalmodulationfeatureformulti-tasklearning. ofthedownsamplinglayerwhenSTRFfeaturesareused.
Wedescribethespectro-temporalmodulationfeaturebelow.
Self-Attentive Pooling Decoder: Our decoder uses task-
GivenN STRFs,wedefinethespectro-temporalmodulation specificself-attentivepooling,followedbyaseriesoffully-
feature(STMF)attimeframetandfrequencybinktobe connectedlayers.
STMF[i,t,k]=STRF(i)[t,k](cid:63)LM[t,k], (2)
3.3.Trainingsetup
where LM denotes the log-mel spectrogram, (cid:63) refers to Weusethemeanabsoluteerror(MAE)objectivefunction
cross-correlation,andiistheSTRFindex. EachoftheN to compute the loss for both the emotion and age predic-
STRFsaretunedtospecificspectro-temporalmodulation tion. Forthecountryprediction, weusethenegativelog-
patterns. We follow (Vuong et al., 2020) and enable the likelihoodobjectivefunction.Thefinallossusedtooptimize
STRFmodulationparameterstobealearnableparameterby ourmultitasksystemisaweightedcombinationofthethree
thedeepneuralnetwork(DNN).Specifically,theSTRFsare lossterms. Wetunetheweightsforeachlosstermbasedon
implementedasa2DconvolutionallayerwheretheDNN theperformanceonthevalidationset.
learnstherateandscalevaluethatparameterizetheSTRF
L =α L +α L +α L (3)
ratherthantheindividual
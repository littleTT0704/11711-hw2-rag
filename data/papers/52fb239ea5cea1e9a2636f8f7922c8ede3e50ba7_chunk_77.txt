.120 0.315 0.250 0.158 0.025 0.239 0.021 0.157 0.105 0.181 0.250
22 Computersciencek. 0.262 0.128 0.425 0.408 0.151 0.137 0.147 0.134 0.232 0.304 0.220 0.278
23 Real-worldk. 0.150 - 0.472 - 0.012 - 0.100 - 0.019 - 0.160 -
Averagescore 0.384 0.384 0.604 0.586 0.204 0.177 0.394 0.238 0.252 0.268 0.480 0.448
Table 3: Evaluations of different baselines across 23 tasks in L¯ila. On most
tasks, Codex outperforms all baselines while Bha¯skara-P outperforms all
fine-tuned baselines. A model usually performs worse on the OOD data set. The
bold score refers to the best score among models with the same supervision
method; the underlined score refers to the best score among all models. GPT-3
and Codex performance is computed on 100 uniformly distributed examples
owing to their cost and usage limit. Fine-tuned model performance is calculated
on the full test set.
Lastly, we do not find any benefit to fine-tuning with instructions. Our best
instruction tuned model achieves 0.133 F1, whereas the worst non-instruction-
tuned multitask model achieves 0.290.
Programsynthesissubstantiallyoutperformsanswerprediction. Syn-
thesizing the program and evaluating it to get an answer substantially outper-
forms directly predicting the answer. For instance, multi-task program synthesis
(Bha¯skara-P) has an average score of 0.480 while multi-task answer prediction
(Bha¯skara-A) scores 0.252. This means models are often able to generate
a program that evaluates to the correct answer, even when the model cannot
directly compute the answer.
Program synthesis improves over answer prediction in all math categories
except Geometry, with the largest improvements in Statistics and Linear
Algebra; see Table 5 for examples. We even see benefits of program synthesis
in NLI, a classification-based task. L¯ila’s unified problem format decouples
synt
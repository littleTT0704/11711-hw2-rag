.Forthei2b2 CN
amountofannotatortime). →
(toprow),however,theperformanceimprovementincreases
Forourtimedannotationexperimentdescribed withmoreannotateddata.
in§3,wereportmoredetailedannotatoragreement
metricsforthetwoannotationtasksinTable6. We
expectthatagreementscoresforbothtasksarelow,
sincei2b2/VAdatasetishighlytechnical,andanno-
tatorshavenodomainexpertise. Theincreasedtask
complexityofcoreferenceresolutionmayfurther
worsenagreementforthetaskrelativetomention
detection. We do not use this annotated data be-
yondtimingannotationtasks.
B ReproducibilityDetails
ImplementationDetails Forallmodels,webe-
gan first with a pretrained SpanBERT (base) en-
Figure 6: Heatmap represents performance improvements
coder(Joshietal.,2020)andrandomlyinitialized
fromourmodelSpanBERT+high-precc2f(CLS,MDT)over
parametersfortheremainingmentiondetectorand thebaselineSpanBERT+c2f(CLS,CLT)wheresingletons
antecedentlinking. Weuse512formaximumseg- aredroppedfromthesystemoutput.Thebaselinehasaccessto
100%oftargetdomaincoreferenceexamples,andourmodel
mentlengthwithbatchsizeofonedocumentsim-
hasaccessto100%mentionannotations.
ilar to Lee et al. (2018). We first train the model
withacoreferenceobjectiveoverthesourcedomain
CLS,andthenwetrainoverthetargetdomainwith domain mentions, our baseline model performed
somesubsetofourobjectivesCLT,MDT,MLMT betterifweinterleavedtargetandsourceexamples.
Wedonotweightauxiliaryobjectives,takingthe So,weinterleavetargetandsourceexampleswith
rawsumoverlossesastheoverallloss. Whenwe fewerthan1kmentionsfromthetargetdomain.
trainoneobjectiveoverboththesourceandtarget Forexperimentswherethenumberofmentions
domain(i
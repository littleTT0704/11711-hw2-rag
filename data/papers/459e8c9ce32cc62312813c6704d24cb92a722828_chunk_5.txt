ibleforevaluationuptoanormalizingconstant,wepropose
k=1PK
k=1w((cid:1) k)
k MGS
to use self-normalized importance sampling for obtaining
where(cid:1) (cid:24)q((cid:1)j(cid:18)),eachw((cid:1) )is exp((cid:11)(C((cid:18))(cid:0)C((cid:18)+(cid:1)k))),
updatedirections. k k q~((cid:1)kj(cid:18))
The key idea behind our method is to form a proposal and q / q~. This update direction equals (cid:1) (cid:3) in the limit:
distributionthatisamixtureofrandomsearcharoundthecur- P(lim K!1(cid:1) MGS =(cid:1) (cid:3))=1.1
rentparametersandaroundthemaximum-likelihoodupdate Thesamplecomplexityofsucharandom-searchmethodis
direction.Ourexperimentsshowthattheresultingprocedure, knowntodependonthedimensionalityofthesamplespace
calledmaximum-likelihoodguidedparametersearch(MGS), (SenerandKoltun2020),thusitiscrucialtochooseagood
iseffectiveforminimizingsequence-levellossesinnatural proposaldistribution.Ourcontributionisaproposaldistribu-
language generation and machine translation, offering an tionforuseinsequencegeneration,wherewehaveaccess
alternativetopolicygradientandminimumriskmethods. tothemaximumlikelihoodgradientr (cid:18)L MLE.Specifically,
weproposeamixtureoftwoGaussians,whosecomponents
2 Maximum-LikelihoodGuidedParameter are centered at the origin and at the maximum-likelihood
gradient,respectively:
Search
q ((cid:1)j(cid:18))=N((cid:1)j0;I(cid:27)2)(cid:1)(cid:25)+ (5)
Sequencegeneration. Sequencegenerationistheproblem MGS
of mapping an input X
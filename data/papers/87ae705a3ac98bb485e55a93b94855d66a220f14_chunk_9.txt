Mlayers,residualconnec- al.2016)performfine-grainedentitylabelingusinganeural
tions,andacustomdecodertoimproveaccuracy(Heetal. attention model. (Dong et al. 2015) use a combination of
2017). NNstoembedwordsandentitiesforcoarse-grainedentity
InadditiontolinearizingtheAMRLparsegraphs,wealso labeling.Morerecently,twotypesofnetworkarchitectures
leverageembeddingstrainedfromthemuchlargerdatasets havegainedpopularity.ThefirstoneisLSTM-CNNs(Chiu
thatareavailableforSLU.Asacomparison,thelinearized andNichols2015),whichuseacombinationofword-level
AMRLcorpusconsistedof300kexamples,whiletheSLU andCNN-extractedcharacter-levelfeaturestoaugmentthe
corpusconsistedofaround3millionexamples.Toleverage inputtobi-LSTMs.ThesecondoneisLSTM-CRFs(Huang,
thesedatasets,newlayersforintentsandslotswereadded Xu, and Yu 2015), which apply a CRF constraints to bi-
to our DNN models, and were trained as new tasks using LSTMs.Recently,(MaandHovy2016)combinedthetwo
multi-tasklearning.Slotembeddingsarefoundtoproduce approaches to get the state of the art results on standard
a 2.6% improvement in IRER (full-parse accuracy), while CONLL2003NERtask.
domainembeddingsimprovetheaccuracybyanother0.1% LSTMs (Long Short Term Memory) (Hochreiter and
IRER.Anadditional1%IRERimprovementisobtainedby Schmidhuber 1997) perform well on many NLP tasks in-
usingacustomdecoderthatleveragesspan-basedIOBand cludingsequencetagging,intentclassification,andlanguage
ontologicalconstraints.Ourproposedmodelevendecreases modeling due to their inherent ability to model long term
thefull-parseerrorrateby1.5%IRERwhencomparedtoa sequential dependencies. Bi-LSTMs (Graves, rahman Mo-
5391
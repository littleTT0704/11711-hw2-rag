penalty 1
gradlength 100000
horizonlength 1
windowlength none
Table12: HyperparametersforgenerationwithPPLM.
Figure 6: A histogram of the number of positive gen-
Adescriptionofeachhyperparametercanbefoundin
erations out of 25 from GPT-2, conditioned on our
(Dathathrietal.,2020)
sentimentpromptsdatasetof100knaturallyoccurring
prompts.
Hyperparameter Assignment
posteriorweightingexponent(ω) 30
filterpp1´ρq 0.8 creation of RealToxicityPrompts (Gehman et al.,
targetppτq 0.8 2020),wespliteachsentenceintotheprompt,con-
repetitionpenaltyscale 10
sistingofthefirsthalfoftokens,andthecontinua-
repetitionpenalty 1.2
tion,consistingoftheremainingtokens. Wekeep
Table 13: Hyperparameters for generation with GeDi. only prompts that are between 4 and 10 tokens
Adescriptionofeachhyperparametercanbefoundin long (inclusive). For all tokenization, we use the
(Krauseetal.,2020) NLTKlibrary(BirdandLoper,2004). Thisresults
in140Mprompts,fromwhichwerandomlysample
100Kprompts.
DEXPERTStakes2to3timesthetimeasdecoding
Foreachofthe100Kprompts,wegenerate25
directly from the base model, depending on the
continuationsfromourbasemodel,GPT-2(large),
size of the (anti-)experts. When using the same
andscorethecontinuationsforsentimentusingthe
model size for the guiding language model as in
HuggingFacesentimentclassifierdescribedin§4.
GeDi (GPT-2 Medium), DEXPERTS is more effi-
Thedistributionofpromptswithn P r0,25sposi-
cientthanGeDi,andbothmethodsare100ˆfaster
tive continuations out of 25 is shown in Figure 6.
thanPPLM.
Interestingly,weobservethatmorepromptshave
Model Generationtime(
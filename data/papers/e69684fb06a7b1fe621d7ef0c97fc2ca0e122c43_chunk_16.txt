ensible design and modular
guagequeries,oursystemislikelytostrugglemore
implementation makes it a platform for advanc-
withlower-resourcelanguages. Weusetheunpub-
ingmodeldistillation,datasetgeneration,synthetic
lishedgpt-3.5-turbomodelforourDatasetGen-
evaluation,datasetretrieval,andmodelretrieval.
eratorinourreferenceimplementation. Thismodel
WebelieveourPrompt2Modelframeworkcan
is believed to be similar to GPT-3 (Brown et al.,
inspirevariousnovelresearchquestions. Wehope
2020), which was trained on 93% English docu-
that our platform enables future work that looks
ments, 1%Germandocuments, 1%Frenchdocu-
moredeeplyintoqualityassuranceonthegenerated
ments,and<5%documentsinanyotherlanguage.
dataandthemodel. Interestingquestionsinclude
Ouruseofthismodelmayexacerbateexistingdis-
howmuchdatashouldwegeneratefordownstream
parities in language technologies between high-
modeltrainingandhowdiverseshoulditbe? How
resourcelanguagesandlow-resourcelanguages.
doweeffectivelymixtheretrievedandgenerated
dataset such to achieve complementary strengths One potential limitation is that we have only
(e.g. using dataset generation to focus on the ex- testedourapproachon3tasks,eachwithasingle
pectedinputstothemodelthattheretrieveddataset datasetandasingleevaluationmetric. Wejustify
failstocover)? Sinceusersoftenstruggletoarticu- thisdecisionbecauseourfocusisonprovidingan
extensiblesoftwaresystemratherthanestablishing
8Thissetofmodelsconsistedof5T5-familymodels,2
state-of-the-artresultsonmanydatasets,butwebe-
BART-family models, and 1-5 additional retrieved models
fromtheModelRetriever,dependingontask. lievethatourresultssuggestbroaderapplicability.
EthicsStatement References
Any system which makes powerful technology AbubakarAbid,AliAb
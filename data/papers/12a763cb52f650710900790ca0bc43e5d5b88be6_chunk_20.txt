ranavShyam,GirishSastry,Amanda
6 Conclusion Askell, et al. 2020. Language models are few-shot
learners. arXivpreprintarXiv:2005.14165.
We introduce generated knowledge prompting, a
Ting-Yun Chang, Yang Liu, Karthik Gopalakrish-
simple method to elicit and integrate knowledge
nan, Behnam Hedayatnia, Pei Zhou, and Dilek
from language models so as to improve perfor- Hakkani-Tur. 2020. Incorporating commonsense
manceoncommonsensereasoningtasks. Inpartic- knowledge graph in pretrained models for social
commonsense tasks. In Proceedings of Deep
ular,wegenerateknowledgestatementsbyprompt-
LearningInsideOut(DeeLIO):TheFirstWorkshop
ing a language model with task-specific, human-
on Knowledge Extraction and Integration for Deep
written, few-shot demonstrations of question- LearningArchitectures,pages74–79,Online.Asso-
knowledge pairs. We show that knowledge can ciationforComputationalLinguistics.
beintegratedbysimplypluggingitinatinference
Joe Davison, Joshua Feldman, and Alexander Rush.
time,withnoneedtofinetunethemodelforknowl- 2019. Commonsense knowledge mining from
edgeintegration. Ourmethodshowseffectiveness pretrained models. In Proceedings of the 2019
acrossmultipledatasets,setsthenewstate-of-the- Conference on Empirical Methods in Natural
Language Processing and the 9th International
art on three commonsense reasoning tasks, and
Joint Conference on Natural Language Processing
works under a variety of settings. The method’s
(EMNLP-IJCNLP), pages 1173–1178, Hong Kong,
successhighlightslanguagemodelsassourcesof China.AssociationforComputationalLinguistics.
3162
Geoffrey E Hinton. 2002. Training products of ex- Kaixin Ma, Jonathan Francis, Quanyang Lu, Eric Ny-
pertsbyminimizingcontrastivedivergence. Neural berg, and Alessandro Oltramari. 2019. Towards
computation,14(8):1771–1800. generaliz
Zhouetal.(2021b).
tothepromptthatisgiventothePLMsalongwith
Results. Shown in Table 5, both Prost pro-
thedialoguecontext(seeAppendixD.2fordetails).
ducemoredisagreeingresponsescomparedtoother
Werunhead-to-headhumanevaluationsbetween
models. Incontrast,BlenderBot1andGPT-3have
PLMswithandwithoutCanary,asdonein§5.2.
muchhigherratesofresponsesthatagreewithtoxic
Results. As illustrated in Figure 5, responses
content,comparedtoProstandothers.
withCanaryarestronglypreferredoverthosewith-
Interestingly, Prost (RoT & Response) gener-
atesmoretoxicwordsoroffensiveresponses,com- 8Wecorroboratethisintuitionbycountingnegationwords
from LIWC-2015 (Pennebaker et al., 2015), and find that
7Asbeforein§5.2,wesetpromptstomakeGPT-3and negationsappearin88%ofProst(RoT&Response)outputs
InstructGPT-3tobedialogueagents. butonly72%ofProst(Response).
out Canary (×2 ∼ 3 on prosociality and over- 70% Instruct GPT-3 Tie Instruct GPT-3 + Canary
60%
all). Thepatternissimilarforallotherdimensions, 50%
wheretheresponseswithCanaryRoTsarebetter 40%
30%
or as good as responses without the RoTs. This
20%
suggeststhatwhenguidedwithsocialnormsand 10%
0%
RoTs, PLMs can be effectively steered towards
Coherency Engaged Respect Prosociality Overall
behavingmoreprosocially. 70%
GPT-3 Tie GPT-3 + Canary
Going one step further, we also compare re- 60%
50%
sponsesbetweenGPT-3andInstructGPT-3(Fig-
40%
ure 6). As expected, Instruct GPT-3 outperforms 30%
20%
GPT-3 in all five criteria. However, when GPT-
10%
3 is equipped
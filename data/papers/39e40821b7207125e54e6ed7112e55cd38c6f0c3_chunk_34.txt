Table 12: Corpus Statistics for the tasks used in this and code-davinci-002. While the exact
work. sizes of the models are unknown because
of their proprietary nature, OpenAI API
states that code-davinci-002 is the Most
E Sampleoutputs
capable Codex model Tables 16 and?? com-
Sample outputs from COCOGEN for all the pares COCOGEN +code-davinci-001
tasks are located at https://github.com/ with COCOGEN +code-davinci-002.
madaan/CoCoGen/tree/main/outputs. Note that both code-davinci-001 and
Representative examples from each task are code-davinci-002 can fit 4000 tokens,
presented in Figure 5. Surprisingly, COCO- so the number of in-context examples was
GEN (CODEX with a Python prompt) generates identical for the two settings. The results
syntacticallyvalidPythongraphsthataresimilar show that for identical prompts, COCOGEN
to the task graphs/tables in nearly 100% of the +code-davinci-002 vastly outperforms
cases. COCOGEN +code-davinci-001, showing
theimportanceofhavingabetterunderlyingcode
F Prompts generationmodel.
Thepromptsforeachtasksarepresentatthisanony- 8https://networkx.org/
mousURL: 9asofJune2022
2 RelatedWork
Datasets: Large-scale reading comprehension
datasets, e.g., SQuAD (Rajpurkar et al., 2016),
TriviaQA (Joshi et al., 2017), have successfully
drivenprogressinquestionanswering,butlargely
targetingexplicitlystatedfacts. Often,theresult-
ing systems can be fooled (Jia and Liang, 2017),
promptingeâ†µortstocreateharderdatasetswhere
adeeperunderstandingofthetextappearsneces-
Figure 2: A (simplified) annotated paragraph from
Figure5: Examplegraphsforeachofthetasksusedfor COCOGEN: PROSCRIPT (top-lesfatr)y, E(XWPeLlAbGleRtAaPlH.,S
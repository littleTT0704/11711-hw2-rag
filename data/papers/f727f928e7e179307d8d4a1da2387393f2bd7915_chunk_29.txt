ers). To increase training efficiency, we limit
editsonlyattentionandfeed-forwardweights,so howfarintothetaskmodelhistorywebackprop-
allmodelgradientsmatchtheshapeofanassoci- agate. As shown in Fig. 3, when backpropagat-
atedweightmatrixofshaped 1 ×d 2. Formally,a ing through task model parameters θ t = θ t 1 +
newmodelθ ∗isobtainedusingalearnedoptimizer Update(x i,yˆ i,y i∗,θ
t
1;ϕ),wecontinuebackp− rop-
g ϕ asfollows: agating through Up− date(x i,yˆ i,y i∗,θ t 1) but not
−
θ,whichisalsodependentonϕ. Thatis,weap-
t 1
h = LSTM([x;yˆ;y ∗]) pl−
yastop-gradientfunctiontoθ. Thisway,we
t 1
{u,v,γ,δ
}
= {MLP i(h) }4
i=1
computethederivative ∇ϕUpdate− (x i,yˆ i,y i∗,θ t;ϕ).
A = softmax(u)vT onlyonceforeacht,ratherthanrecomputingthese
gradients for all subsequent time steps. These
B = softmax(γ)δT
choicesallowthememoryuseofourtrainingalgo-
η = σ(MLP(h)) rithmtoremainconstantinr. Wemakethesame
θ = θ+η(A (x,y )+B) choiceforourK loopedstepsinasingleapplica-
∗ ◦∇θ
L
i i∗
tionoftheUpdatefunction,sothegradientforthe
whereϕconsistsofallLSTMandMLPparameters. updateatstepkdependsonlyong (x,yˆ,y,θ(k))
ϕ i i i∗
Training Algorithm. The learned optimizer ob- andnotθ(k −1
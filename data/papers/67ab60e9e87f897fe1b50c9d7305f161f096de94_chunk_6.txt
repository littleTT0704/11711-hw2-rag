antically identical augmentation (Chen et al., 2020b; Tian et al., 2020). To avoid col-
lapsedsolutions, self-supervisedapproaches historically usednegative sampleswithatriplet
loss (Chopra et al., 2005), possibly requiring large negative batches (Chen et al., 2020b;
Saeed et al., 2021), which can be expensive to train. Alternatives include quantization ap-
proaches to define uniform clusterings of representations (Baevski et al., 2020), or carefully
implementedasymmetrictrainingarchitectureslikeBYOL(Grill et al.,2020;Niizumi et al.,
2021) and SimSiam (Chen and He, 2021). More recent are self-supervised approaches that
avoid these aforementioned techniques, relying instead upon explicit and fundamental pri-
ors (Zbontar et al., 2021;Bardes et al., 2022). Inputaugmentations can beused to increase
the size of training data or provide corresponding views on the input (Salamon and Bello,
2017). Fonseca et al. (2021b) and Wang and van den Oord (2021) discuss augmentations,
including audio mixing, which Gong et al. (2021b); Wang et al. (2021b) explore in greater
depth and argue is useful both for supervised and unsupervised regimes.
Multi-modal approaches learn the correspondence between different modalities of the
input. Different modalities can accelerate compact learning in a single target modality by
exploiting cross-modality structure. OpenL3 (ยง4, Cramer et al. (2019)) is a broad-domain
audiomodeltrainedonthecorrespondencebetweenaudioandvideo. ContrastiveLanguage-
Image Pre-training (CLIP, Radford et al. (2021)) learns a model from 400M image-text
pairs, and was successully applied on zero-shot tasks. Wang and van den Oord (2021) con-
trastively induce audio representations from waveforms (1-D audio) and spectrograms, and
Wang et al. (2021b) extend that to include correspondence with video frames.
Because pretraininglarge-scale models requires large quantities of data and can becom-
putationally expensive, another research direction has been on distilling information from
existing
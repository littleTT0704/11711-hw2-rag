edcross-entropyloss(wCE)boosts
For the Stance task, we present per class F as theperformanceoftheDGPTclassifierbygetting
1
wellasmacro-F scoresforallutterancepairs. We thehighest‘Agree’labelF. However,itsperfor-
1 1
also report these metrics for adjacent pairs of ut- manceonDisagreeclassificationisstillpoor. This
terancesi.e. forpairs(u,u ),whichareeasier issueismitigatedbytrainingDGPTclassifierwith
i i+1
to predict. Hyperparameters and implementation classbalancedfocalloss(CB ),whichachieves
foc
detailsarepresentinAppendixD. thehighestoverallMacro-F.
1
6 MitigatingOffensiveBehavior kensencapsulatedifferentresponseattributes. For
example,[OFF]and[SAFE]tokensindicateof-
Our data analysis confirms that dialogue models
fensive control attributes. During training, these
cangeneratesomecontextuallyoffensivelanguage.
tokensareprependedtoresponsesandatinference
To steer the generation away from offensive con-
time,theyaremanuallyfrozentosteerthemodel’s
tent,weexperimentwithsomepreliminarystrate-
response towards the desired attribute (Niu and
gies using controlled text generation (CTG). We
Bansal, 2018; See et al., 2019; Xu et al., 2020).
considerthefollowingthreecontrolattributes: (1)
ForeachCTGexperiment,wefine-tuneDialoGPT-
Offensive - to control safe or offensive response
medium on the train split for 3 epochs and tune
generation,(2)Stance-tocontrolagreeingorneu-
hyperparametersusingdevsetperplexity.
tral response generation towards its immediately
Our goal is to test the conversation models in
precedingcomment,11 and(3)BothOffensiveand
offensivecontexts,wheretheyhaveapropensityto
Stance-tocontrolresponsegenerationwithboth
agreewithoffensivecomments,hence,wesample
controltypes.
a test set of 500 threads where the last utterance

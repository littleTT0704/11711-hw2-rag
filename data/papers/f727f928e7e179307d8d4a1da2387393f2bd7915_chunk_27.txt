geiPopov,andArtemBabenko.2020. Editable
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke neuralnetworks. InICLR.
Zettlemoyer.2017. Zero-shotrelationextractionvia
readingcomprehension. InProceedingsofthe21st AlonTalmor,OyvindTafjord,PeterClark,YoavGold-
Conference on Computational Natural Language berg,andJonathanBerant.2020. Leap-of-thought:
Learning(CoNLL2017),pages333–342,Vancouver, Teachingpre-trainedmodelstosystematicallyreason
Canada.AssociationforComputationalLinguistics. overimplicitknowledge. InNeurIPS.
StephanieLin,JacobHilton,andOwainEvans.2021. James Thorne, Andreas Vlachos, Christos
Truthfulqa: Measuring how models mimic human Christodoulopoulos, and Arpit Mittal. 2018.
falsehoods. arXivpreprintarXiv:2109.07958. FEVER: a large-scale dataset for fact extraction
and VERification. In Proceedings of the 2018
Ilya Loshchilov and Frank Hutter. 2019. Decoupled Conference of the North American Chapter of
weightdecayregularization. InICLR. the Association for Computational Linguistics:
2723
Human Language Technologies, Volume 1 (Long SymbolGlossary
Papers), pages 809–819, New Orleans, Louisiana.
f LanguageModel
AssociationforComputationalLinguistics. θ
g Learnedoptimizer
ϕ
XiaozhiWang,TianyuGao,ZhaochengZhu,Zhengyan x i MainInput
Zhang,ZhiyuanLiu,JuanziLi,andJianTang.2021.
yˆ
i
LMoutputonx
i
Kepler: Aunifiedmodelforknowledgeembedding
y i∗ Desiredoutput
and pre-trained language representation. Transac- ∇θ
L(x i,y i∗) GradientofLM
tionsoftheAssociationforComputationalLinguis-
Update(x i,yˆ i,y i∗
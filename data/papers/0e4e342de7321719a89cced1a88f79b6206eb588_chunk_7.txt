 token f query, and the volution in the backbone in [1,1, 1, 1 ] resolution stages
aggregatedkernelW isaweightedsumofthecandidatekernels. 4 8 16 32
with our query-conditioned convolution. Let us denote the
extractedvisualfeaturesfromthebackboneasf(cid:48) withchan-
v
encouragequery-awarevisualfeatureextractioninthevisual nelsizeofC(cid:48). Asinespatialpositionalencoding[Carionet
v
encoder. With query-aware visual features, the downstream al.,2020]isaddedtotheprojectedfeaturemapbeforefeed-
multi-modalfusion[Yangetal.,2020;Dengetal.,2021]be- ingittothetransformerencoders.
comes optional. Figure 2 shows the overall pipeline of our
f(cid:48)(cid:48) =Flatten(ϕ (f(cid:48))+P) (3)
proposed method. In particular, our VGQC network can be v C v(cid:48)(cid:55)→Cv v
boileddowntofourparts:atextualencoder,avisualencoder, where f(cid:48)(cid:48) is the input to the transformer encoders, P is the
v
anoptionalfusionmodule,andapredictionhead.
positionalencoding,andϕ (·)isadimensionalreduc-
C v(cid:48)(cid:55)→Cv
tion operation. A stack of transformer encoders is further
3.1 TextualEncoder
leveraged to model the non-local correspondence in the ex-
The textual encoder extracts textual features from input ex-
tracted feature map. The final output of the visual encoder
pressions at both sentence and word levels. Following pre- f
v
∈ RCv×Nv is a set of query-aware visual tokens, where
vious works [Deng et al., 2021; Yang et al., 2020], we use C isthefeaturedimensionandN isthenumberofthevi-
v v
BERT [Devlin et al., 2019] as our textual encoder. The in-
sualtok
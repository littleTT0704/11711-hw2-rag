-
but it stays positive as long as ∆(·) is always positive). dependent (cid:107)x(cid:107). There are two advantages of feature nor-
Therefore, now we need to make θ 1+m(θ 1) < θ 2 in order malization.First,itcaneffectivelyavoidpotentialbadlocal
to classify x to the first class, and the decision boundary minima. Second, it can help the loss function to better
forthefirstclassbecomesmorestringentthantheprevious balanceeasyandhardtrainingsamples.
case.Theneuralnetworkhastolearnsmallerθ 1 inorderto Forthefirstaspect,weconsiderEq.(6)inasimplebinary
correctly classify x and smaller θ 1 implies a more compact classification scenario (class 1 is the ground truth label for
representation for the first class. The same reasoning also thedeepfeaturex,i.e.,y = 1).Thelossvaluecaneasilygo
applies to the case where x belongs to the second class to zero once the deep feature x lies in the correct decision
(i.e., the second class is the target class). As a result, if we region,asdemonstratedinthefollowingequation:
can successfully train a neural network to correctly classify (cid:16) (cid:0) (cid:1)(cid:17)
training samples with these more stringent classification lim log 1+exp s·(η(θ 2)−η(θ 1)+∆(θ 1))
s→+∞
c mri at re gr ii na, fo∆ r( tθ h1 e) le> arn0 ec dan dee ef pfe fc et aiv tue rly es.produce large angular   0 if η(θ 1)−η(θ 2)>∆(θ 1) (7)
Importantly,currentpopularhypersphericalFRmethods
=

+− ∞log 21 i if
f
η η( (θ θ1 1) )− −η η( (θ θ2 2) )<= ∆∆ ((�
 on detection of
textualoffensivelanguageusingadversarialhuman- offensive context (Dinan et al., 2019a; Zhang
bot conversations, where a human intentionally et al., 2018a) and subsequent generation of non-
triestotrickthechatbotintosayingsomethingin- provocativecounter-speech(Chungetal.,2019)is
appropriate. Ontheotherhand,Pavlopoulosetal. crucial.
(2020);Xenosetal.(2021)createdlabeleddatasets
fortoxicitydetectioninsingleturnconversations 9 SocietalandEthicalConsiderations
and studied context-sensitivity in detection mod-
Thispapertacklesissuesofsafetyofneuralmod-
els. In contrast, we study the stance dynamics of
els,andspecificallyitattemptstounderstandhow
dialoguemodelresponsestooffensiveRedditcon-
dialoguesystemscanhelpcombatsocialbiasesand
versationswithmorethanoneturns.
helpmakeconversationsmore civil(Dinan etal.,
Inappropriate Language Mitigation - Sheng
2019a;Xuetal.,2020). Forthispurpose,wecrowd-
etal.(2020)manipulatetrainingobjectivesanduse
annotateadatasetofoffensiveconversationsfrom
adversarialtriggers(Wallaceetal.,2019)toreduce
publicly available Reddit conversations enriched
biasesacrossdemographicsandgeneratelessnega-
withautomaticallygeneratedresponses. Thisstudy
tivelybiasedtextoverall. Liuetal.(2020)propose
was conducted under the approval of the Institu-
adversarialtrainingtoreducegenderbias. Dinan
tionalReviewBoard(IRB)ofGeorgiaInstituteof
etal.(2020a)trainsdialoguemodelswithattribute
Technology. WepaidcrowdworkersonAmazonâ€™s
conditioningtomitigatebiasbyproducinggender-
MechanicalTurkplatform$0.8perHITandgave
neutral responses. Saleh et al. (2020) proposes a
extra bonuses to annotators with high annotation
toxicityclassifier-basedreinforcementlearningob-
quality. Weestimatethatthehourlypayofcrowd
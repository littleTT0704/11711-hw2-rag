CAGAN: Text-To-Image Generation with
Combined Attention Generative Adversarial
Networks
Henning Schulze Dogucan Yaman Alexander Waibel
Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology,
Germany
Abstract. Generating images according to natural language descrip-
tionsisachallengingtask.Priorresearchhasmainlyfocusedtoenhance
the quality of generation by investigating the use of spatial attention
and/or textual attention thereby neglecting the relationship between
channels. In this work, we propose the Combined Attention Generative
Adversarial Network (CAGAN) to generate photo-realistic images ac-
cording to textual descriptions. The proposed CAGAN utilises two at-
tentionmodels:wordattentiontodrawdifferentsub-regionsconditioned
on related words; and squeeze-and-excitation attention to capture non-
linear interaction among channels. With spectral normalisation to sta-
bilise training, our proposed CAGAN improves the state of the art on
theISandFIDontheCUBdatasetandtheFIDonthemorechallenging
COCO dataset. Furthermore, we demonstrate that judging a model by
asingleevaluationmetriccanbemisleadingbydevelopinganadditional
modeladdinglocalself-attentionwhichscoresahigherIS,outperforming
thestateoftheartontheCUBdataset,butgeneratesunrealisticimages
through feature repetition.
Keywords: text-to-imagesynthesis,generativeadversarialnetwork(GAN),at-
tention
1 Introduction
Generatingimagesaccordingtonaturallanguagedescriptionsspansawiderange
ofdifficulty,fromgeneratingsyntheticimagestosimpleandhighlycomplexreal-
world images. It has tremendous applications such as photo-editing, computer-
aided design, and may be used to reduce the complexity of or even replace
renderingengines[27].Furthermore,goodgenerativemodelsinvolvelearningnew
representations.Theseareusefulforavarietyoftasks,forexampleclassification,
clustering, or supporting transfer among tasks.
The final published version of the paper can be found here: https://
link.springer.com/chapter/10.1007/978-3-
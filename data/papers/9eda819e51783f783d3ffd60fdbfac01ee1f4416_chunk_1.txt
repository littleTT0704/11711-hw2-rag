Searching for Effective Multilingual Fine-Tuning Methods:
A Case Study in Summarization
YiweiQin♠ GrahamNeubig♠♣ PengfeiLiu♠♣∗
♠CarnegieMellonUniversity,♣InspiredCognition
{yiweiq,gneubig,pliu3}@cs.cmu.edu
Abstract
Recently, a large number of tuning strategies
have been proposed to adapt pre-trained lan-
guagemodelstodownstreamtasks. Inthispa-
per, we perform an extensive empirical evalu-
ation of various tuning strategies for multilin- (a) (b) (c)
guallearning,particularlyinthecontextoftext
Figure1: Differentframeworksformultilinguallearn-
summarization. Specifically, we explore the
ing,whereorangecirclesrepresentdifferentlanguages
relative advantages of three families of multi-
and blue circles denote pre-trained language models
lingualtuningstrategies(atotaloffivemodels)
(PLMs). Redboxesrefertoadditionallearnableparam-
andempiricallyevaluatethemforsummariza-
eters,suchasadaptersorprefixes.Doublesidedarrows
tion over 45 languages. Experimentally, we
representthattheparametersofPLMsaretunable.
not only established a new state-of-the-art on
theXL-Sumdatasetbutalsoderiveaseriesof
observations that hopefully can provide hints
Atthesametime,therehasbeenmuchprogress
for future research on the design of multilin-
gualtuningstrategies.1 inmultilingualmodelsbasedonpre-trainedLMs
(LampleandConneau,2019;Conneauetal.,2020;
1 Introduction
Liuetal.,2020). However,thereisanotablegap
Methodsthatperformfine-tuningofpre-trainedlan- in the literature – to our knowledge, there are no
guagemodels(PLMs)nowrepresentthestate-of- comprehensivecomparativestudiesonhowdiffer-
the-artacrossawidevarietyofNLPtasks(Howard ent tuning strategies behave in multi-lingual sce-
andR
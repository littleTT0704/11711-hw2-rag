 perform at 80.59% MAP. Thus
the predictive performance of models trained on only 1,000 well-chosen queries comes
close to the performance of supervised learning using over 100,000 training instances
in each step of the cross-validation. We further investigated whether the small loss
in relevance estimation performance when using active learning techniques instead of
fitting a supervised model to pre-labeled data is noticeable in a task-based evaluation
on QA datasets. For this experiment, we used a logistic regression model with ad-
8.1. ACTIVE LEARNING 131
Regular J! Final J! TREC 8–12
Wikipedia 81.33% 63.32% 76.74%
Expansion Supervised 86.23% 72.21% 82.17%
(130,119 training instances) (+6.0%) (+14.0%) (+7.1%)
Expansion Active Learning 86.06% 71.07% 81.09%
(1,000 training instances) (+5.8%) (+12.2%) (+5.7%)
Wiktionary 30.39% 13.32% 29.15%
Expansion Supervised 51.20% 27.79% 52.46%
(130,119 training instances) (+68.5%) (+108.6%) (+80.0%)
Expansion Active Learning 49.80% 25.13% 49.93%
(1,000 training instances) (+63.9%) (+88.7%) (+71.3%)
Table 8.2: Impact of source expansion on QA search recall when using relevance
models obtained through supervised learning or active learning.
jacent features because this method consistently performed well in all experiments,
and because it was easy to integrate in our SE system. Queries were selected based
on the strategy Diversity × Uncertainty because this method is most effective after
1,000 iterations when using logistic regression and the large feature set. In contrast
to the second best strategy Diversity → Uncertainty, this approach does not require
tuning a cross-over parameter.
The final model after 1,000 active learning steps was applied to expand Wikipedia
and Wiktionary seed corpora using the exact same methodology as in Chapter 6.
The relevance threshold in the merging phase of the SE system was adjusted to
ensure that the expanded corpora have the same size as those previously generated
with the
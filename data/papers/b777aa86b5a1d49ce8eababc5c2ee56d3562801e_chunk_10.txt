 100 101 102
Batch Size Batch Size Batch Size Batch Size Sequence Length
Figure 4: Latency vs. (batch size and sequence lengths) for baseline models in FP16 and FP32 on RTX-8000.
Frameworkboundednessexistsforallmodelsatsmallinputsizeswhereframeworkoverheaddominatesruntime
andresultsinconstantlatencyregardlessofinputsize. Frameworkoverheadismostprominentinsmallermodels
executedinhalfprecisiononslowerframeworks.
rangeofbatchsizestocapturecommoninference to overtake fixed framework overhead. As a re-
use cases as stated in (Reddi et al., 2020): sin- sult,halfprecisioninferenceisframeworkbound
gle example inferences is common in streaming for larger batch sizes. For inference with larger
on-deviceapplications,suchasautocompleteand compute-bound batch sizes, latency observes ex-
automaticspeechrecognition;largebatchinference pectedspeedupsfromusinghalfprecision.
istypicalinofflineserversettings. Although models based on convolutions
We simulate text data by randomly generating (ResNet-50) and self-attention (BERT-Base)
tokensequencesoflength128,ascommonlyused operationsbothexhibitframework-boundbehavior,
in sentence classification tasks (Liu et al., 2019; theytransitiontocomputeboundednessatdifferent
Izsak et al., 2021). We report averaged metrics batchsizes. Thedifferenceinmodelbehaviorcan
from100forwardpassesafter10warm-uppasses be attributed to differences in the rate at which
toinitializemodelweightsanddata. GPUcoreand computekernelsovertakeCPUdispatchoperations.
memoryutilizationaremeasuredwiththeNvidia Forthewell-optimizedoperations(Conv2Dsand
NsightComputeandNvidiaManagementLibrary. GEMMs)thatmakeupResNet-50andBERT,the
WeselectBERT-Base(Devlinetal.,2018)and timeperFLOPisreasonablyconsistent.
ResNet-50(Heetal.,2016)asrepresentativemod-
elsforencoder-onlyandconvolutionalneuralnet- 4.1 FrameworkDesignDecisions
work architectures commonly used for sentence
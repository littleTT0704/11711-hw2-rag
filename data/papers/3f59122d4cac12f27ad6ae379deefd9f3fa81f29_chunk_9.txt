
A one-hot attention over action verbs chooses which pose
ω =cos−1(2(cid:104)qˆ,qˆ (cid:105)2).
andgrippercommandshouldbeexecuted.Ineffect,theactor 1 2
learns to compute a set of pose features for predicting the
nextmanipulationgoalandlearnsasimpleperceptronmodel To avoid computing the inverse cosine as a part of the loss,
foreachactionverbinordertochoosewherethearmshould we use a squared distance metric. In addition, normalize
go and whether the gripper should be opened or closed after gripper commands to be between 0 and 1, where 0 is closed
the motion is complete. and 1 is open, and trained with an additional L2 loss on
predicted gripper commands. Given estimated pose θˆ and
D. Training
final pose θ, we calculate pose estimation loss:
Wetraintheencoderanddecoderjointlywhentrainingthe
PredictionandActormodulesandoptimizewithAdam[30], C (θˆ,θ)=λ (cid:8) (cid:107)pˆ−p(cid:107)2+(1−(cid:104)qˆ,q(cid:105))+(cid:107)gˆ−g(cid:107)2(cid:9).
actor actor 2 2
usinganinitiallearningrateof1e−3.Wefixthelatentstate
encoder and decoder functions after this step, then use the
learned hidden space to train the Subgoal module. Object Pose Estimation Loss It is important to ensure
Image Reconstruction Loss This determines how well that our learned latent states z t capture all the necessary in-
ourmodelcanreconstructanimagefromagivenhiddenstate formationtoperformthetask.Assuch,weuseanaugmented
z t, and is trained on the output of our visualization module. loss C obj(z t) that predicts the position of each of the four
WeusedanL2lossonpixelsbothforRGBanddepth.Depth blocks in the scene at the observed frame. This information
values were capped at 2 meters and were normalized to be is not used at test time, but
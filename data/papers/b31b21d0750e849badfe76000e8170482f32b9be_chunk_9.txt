 Thus,forT5andCodeT5weapplythefusion-in-decoderapproach(FiD; Izacardand
Grave,2021): wefirstconcatenatetheintentnwitheachretrievedd ∈Dˆ andencodeeach(n,d )
i n i
pairindependently. Then,thedecoderattendstoallencodedNL-documentpairs. Wefinetunethe
generatortomaximizethelog-likelihoodofthereferencecodecgivennandDˆ.
n
WithCodex(Chenetal.,2021),weperformedfew-shotlearningratherthanfinetuningbecausethe
modelparametersarenotpubliclyavailable. Weconstructedthepromptwiththreestaticexamples,
eachofwhichisaconcatenationofretrieveddocumentation,anNLintentandthereferencecode
snippet. Wethenappendedthetestexampleanditsretrieveddocumentationtothefew-shotexamples.
Weusedthecode-davinci-001versionbecausewesuspectpotentialleakageofthetestsetintothe
trainingsetofcode-davinci-002. SeemoredetailsinAppendixH.Trainingdetails,hyper-parameter
settingsandexamplepromptscanbefoundinAppendicesEandD.
4 EXPERIMENTAL SETUP
WeevaluateDocPromptingontwoNL→codetasks: shellscripting(§4.1),inwhichwegenerate
complex shell commands given an intent, and Python programming (§4.2), where we generate
answersinPythonforNLquestions. Inthissection,wefirstintroduceanewlycuratedbenchmark
tldr;wethendescribeourre-splitofthepopularCoNaLabenchmark(Yinetal.,2018). Foreach
benchmark,weprovideaglobaldocumentationpoolD thatissharedforallexamplesandoracle
documentsD∗ whichweusetotraintheretriever. Wereleaseournewlycuratedbenchmarkstoserve
n
astest-bedforfutureretrieval-basedcodegenerationmodels.
4.1 SHELLSCRIPTING
tldr is a community-driven project
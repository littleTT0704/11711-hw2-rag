(W hh
k
+b h) R3, of the model, we use sliding window and divide
| âˆˆ recenttweetsorganizedinchronologicalorderinto
wherek representthepositioninx. differentwindowswhereeachwindowcanrepre-
i
Duringtraining,werandomlydropnegativein- sentinformationwithinatimerange. Thenwetrain
stancesthatdonotcontainanyBlabelstokeepthe the model on these divided examples separately.
positive-negativesampleratiosteady. Each example contains the same prefix predicate
andTwittermetadatabutusesdifferentpartsofthe
Resultaggregation. Duringinference,foreach tweetstoinfertheattributevalue.
user,wefirstperformsequencelabelingonevery
sequencepredicatepairexhaustively. Thenweag- Result aggregation. During inference, we use
gregate sequence-level labeling results into user- thesameslidingwindowstrategyanddividethein-
levelresults. Foreachattributepredicate,weselect putintodifferentexamplestomakepredictionsin-
thespanthathasthelargestaveragedlogitasthe dependently. Then,similartotheextraction-based
finalanswer. method,weaggregatethosewindow-levelpredic-
tions into a user-level prediction. We count the
3.2 Generation-basedMethod
occurrencesofeachpredictedtextforapredicate
Extraction-basedmethodssufferfromthefactthat andthenusemajorityvotetofindtheaggregated
attributevaluesmustappearintheTwittercontext. resultofthatpredicate.
3176
DevelopmentSet TestSet
Model Precision Recall F 1 Precision Recall F 1
Random 0.22 0.22 0.22 0.23 0.23 0.23
Majority 14.56 14.56 14.56 14.19 14.19 14.19
Extraction 18.36 9.69 12.69 18.39 9.80 12.79
Generation 59.05 43.71 50.23 58.73 43.40 49.92
Table3: Systemperformance(%)onourconstructedopen-domainTwitteruserprofileinferencedataset.
Result filtering. The generation-based method Model Precision Recall F 1
aggressively generates output without
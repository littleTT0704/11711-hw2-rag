 58.1
RandomInit.(24Layer)
2048 64.4 64.6 71.6 60.0 62.7 62.8 82.5 32.8 16.0 52.2
CONTRASTIVE 4096 66.6 68.6 75.1 64.3 65.7 70.9 86.8 34.7 18.1 55.4
8192 68.0 70.2 76.2 66.2 67.7 74.2 88.3 35.2 19.4 57.0
2048 71.1 71.7 77.7 67.7 61.4 78.4 87.8 38.3 22.3 58.0
VMSST 4096 72.0 72.1 77.7 68.3 62.9 81.0 89.7 38.7 23.5 59.1
8192 72.7 74.1 79.0 70.8 64.1 82.0 90.2 39.0 24.3 60.1
Table7: ComparisonofCONTRASTIVEandVMSSTusingdifferentbatchsizesduringtraining.
Model Tat.(seen) Tat.(unseen) âˆ† amount(billionsofpairs)ofparalleldataavailable.
RandomInit.(6Layer) We experiment with batch sizes of 4096 and
8192, double and quadruple the 2048 used in all
BITRANSLATION 59.3 24.0 -
VMSST 64.4 30.6 1.5 experiments up to this point, for both the 6 layer
RandomInit.(24Layer) and24layerrandomlyinitializedversionsof CON-
BITRANSLATION 82.6 56.5 - TRASTIVE and VMSST. All models are trained
VMSST 84.9 62.2 3.4 againfor100,000steps. Theresultsareshownin
Pretrained(24Layer) Table7.
BITRANSLATION 63.7 26.5 - From the results, we see that for the 6 layer
VMSST 67.3 32.6 2.5 model,increasingthebatchsizeequalizesVMSST
and CONTRASTIVE overall, however each per-
Table 8: Results on languages seen during training forms better at different tasks. CONTRASTIVE
(seen)andlanguagesthatwerenotseenduringtraining has better performance on Tatoeba, XL semantic

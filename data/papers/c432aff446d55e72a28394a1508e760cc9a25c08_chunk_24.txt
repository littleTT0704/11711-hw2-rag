x
hull that is formed by other word embeddings and can thus never be “selected” as the argmax word. We
hypothesizedthatkNN-LMsolvesthestolenprobabilitiesproblembyallowingtoassignthehighestprobability
toanyword,givenatestcontextthatiscloseenoughtothatword’sdatastorekey. However,wefoundthat
noneofthevectorsinourembeddingmatrixandintheoriginalembeddingmatrixofKhandelwaletal.(2020b)
islocatedintheconvexhulloftheothers,whichisconsistentwiththefindingsofGrivasetal.(2022). More
detailscanbefoundinAppendixE.4.
Memorization WehypothesizedthatthekNNcomponentsimplyprovidesmemorizationofthetrainingset.
However,wecouldnotimproveastandardLMbyinterpolatingitsprobabilitywithanotherstandardLMthat
wasfurthertrainedtooverfitthetrainingset. MoredetailscanbefoundinAppendixE.6.1.
Soft Labels We hypothesized that kNN-LM’s improvement lies in reducing the “over-correction” error
whentrainingwith1-hotlabels,ashypothesizedbyYangetal.(2022),andthatretrievingneighborsisnot
important. If only “soft labels” are the key, we could hypothetically improve the performance of another
freshLMwiththesamemodelarchitecturebuttrainedwiththesoftlabelsfromthebaseLM,insteadoffrom
kNN-LM.Thisseparatestheeffectof“softlabeling”fromtheadditionalguidanceprovidedbykNN.However,
thisdoesnothelpwiththeinterpolatedperplexityatall. MoredetailscanbefoundinAppendixE.6.2.
Optimizing Interpolated Loss We hypothesized that the standard LM cross-entropy training loss does
not emphasize the examples where base LM performs badly which could benefit from kNN, and directly
optimizing the interpolated loss of standard LM and a separate trainable softmax layer could be a better
alternative. However,wecouldnotgainanybenefitsbytraininganadditional

iosbyjointlyregularizingthenetworkwithbothsupervisedand allyneedtosamplesubgraphsaslocalviewstocontrastwithglobal
self-supervisedcontrastiveloss.Finally,weshowthatfinetuning graphs.Andtheyusuallyrequireanadditionaldiscriminatorfor
theIGSD-trainedmodelswithself-trainingcanfurtherimprove scoringlocal-globalpairsandnegativesamples,whichcanbecom-
thegraphrepresentationpower.Empirically,weachievesignificant putationallyprohibitive[47].Besides,theperformanceisalsovery
andconsistentperformancegainonvariousgraphdatasetsinboth sensitivetothechoiceofencodersandMIestimators[47].More-
unsupervisedandsemi-supervisedsettings,whichwellvalidates over,context-instancecontrastiveapproachescannotbehandily
thesuperiorityofIGSD. extendedtothesemi-supervisedsettingsincelocalsubgraphslack
labelsthatcanbeutilizedfortraining.Therefore,weareseekingan
approachthatlearnstheentiregraphrepresentationbycontrasting
KEYWORDS
thewholegraphdirectlytoaddresstheabovechallenges.
graphrepresentationlearning,self-supervisedlearning,contrastive
Motivatedbyrecentprogressoninstancediscriminationcon-
learning
trastive learning [16, 51], we propose the Iterative Graph Self-
Distillation(IGSD),ateacher-studentframeworktolearngraph
ACMReferenceFormat:
HanlinZhang1,ShuaiLin2,WeiyangLiu3,4,PanZhou5,JianTang6,7,Xiao- representationsbycontrastinggraphinstancesdirectly.Thehigh-
danLiang2,EricP.Xing1.2021.IterativeGraphSelf-distillation.In.ACM, levelideaofIGSDisbasedongraphcontrastivelearningwhere
NewYork,NY,USA,9pages. wepullsimilargraphstogetherandpushdissimilargraphaway.
However,theperformanceofconventionalcontrastivelearning
largelydependsonhownegativesamplesareselected.Tolearn
1 INTRODUCTION
discriminativerepresentationsandavoidcollapsingtot
softmaxlayertogetherwitha
baseLMusingtheinterpolatedloss. MoredetailscanbefoundinAppendixE.6.3.
7 Conclusion
Inthispaper,weinvestigatewhykNN-LMimprovesperplexity,evenwhenretrievingexamplesfromthesame
trainingdatathatthebaseLMwastrainedon. Byproposingandtestingvarioushypothesesandperforming
extensiveablationstudies,wefindthatthekeytokNN-LM’ssuccessisthreefold:
1. Ensemblingdifferentinputrepresentations–thefeedforwardlayeroutputandtheattentionlayer
output–canrecover55%oftheperformance,evenwithoutretrieval.
2. One of the most unexpected discoveries in the paper is that using approximate nearest neighbor
searchallowskNN-LMstogeneralizebetterthanexactnearestneighborsearch,possiblyduetoa
regularizationeffect.
3. TuningthesoftmaxtemperatureforthekNNdistributioniscrucialtoadjustthestandardLMoutput
distributionwiththedistributioncreatedbytheretrievedneighbors’distances.
WeperformedextensiveexperimentswhichruledoutotherhypothesesastowhykNN-LMswork,suchas
over-parameterization,datastoreclustering,sparsification,overfitting,ensemblingofdistancemetrics,and
alternativetrainingmethods.
We believe that this work unlocks a variety of exciting research directions for efficient kNN alternatives.
For example, exploring methods that replace the kNN component with trainable parameters and achieve
comparableresultswithoutthelatencyburdenofkNN-LM.
Acknowledgement
WethankRameshNallapati,SudiptaSengupta,DanRoth,DanielFriedandXiaosenZhengforthehelpful
discussionsandfeedback. ThisprojectwassupportedbyagiftfromAWSAI.FrankF.Xuissupportedby
IBMPh.D.Fellowship.
11
References
Uri Alon, Frank F Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. Neuro-symbolic
languagemodelingwithautomaton-
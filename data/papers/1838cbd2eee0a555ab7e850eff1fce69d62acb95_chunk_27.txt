. Table 13 show MetaXL
etal.(2020c). Duringthebi-leveloptimizationpro-
results on mBERT with various sizes of source
cess,weadoptalearningrateof3e-05fortraining
data.
themainarchitectureandtunedthelearningrateon
Nevertheless, our method consistently brings
3e-5,1e-6and1e-7fortrainingtherepresentation
gains on both tasks. We observe an average of 2
transformationnetwork. Weuseabatchsizeof16
F1pointsimprovementonNERand2.0F1points
for NER and 12 for AS, and train 20 epochs for
improvement on SA. It shows that the improve-
each experiment on both tasks. We use a single
ment brought by our method is consistent across
NVIDIATeslaV100witha32Gmemorysizefor
differentlanguagemodels.
eachexperiment. Foreachlanguage,wepickthe
bestmodelaccordingtothevalidationperformance
aftereachepoch.
B DetailedResultsonEachLanguage
B.1 SourceDataSize
The full results of using 10k and 20k English ex-
amplesastransferdataarepresentedinTable9.
B.2 PlacementofRTN
Thefullresultsofplacingtherepresentationtrans-
formationnetworkatdifferentlayersarepresented
inTable10.
B.3 JointTrainingw/RTN
The full results of joint training with the repre-
sentationtransformationnetworkarepresentedin
Table11.
C AdditionalResultsonmBERT
WeconductexperimentsonmBERT(Devlinetal.,
2019), which covers 104 languages with most
Wikipediaarticles. Foralanguagethatisnotpre-
trained with mBERT, we train its subword tok-
enizer with the task data. Further, we combine
the vocabulary from the newly trained tokenizer
with the original mBERT vocabulary. A similar
Method tel fa
(1) targetonly 75.00 73.86
JT 75.13 74.81
(2)
MetaXL 77.36 76.69
Table 8: F1 for sentiment analysis on mBERT on two
settingsusing(1)onlythetargetlanguagedata;
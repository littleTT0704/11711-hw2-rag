<1e 4)pointsforMainInputs risegreatlyaftermodelupdatestoincorrectpoints.
± −
and2.58( 0.81;p<1e 4)forParaphrases,while
± −
∆-Accisvirtuallyunchanged. Thissuggeststhat
that past work has overestimated the efficacy of
zsRE and 59.87 1.09 on Wikidata5m, while En-
belief update methods for actually fixing models. ±
tailmentAccrisesby17.20 7.10points.
Henceforthweevaluatemethodsaccordingtotheir ±
abilitytoupdatemodelbeliefstobetrue.
Update method results (sequential updates).
Updatemethodresults(singleupdate). Table4 We give results for a sequential update setting
showstheresultsinasingle-updatesetting. First, (r =10)inTable6. Immediatelyweseethisisa
test
wefindthatoff-the-shelfoptimizersareveryeffec- muchmoredifficultevaluation,asmetricsaregen-
tive across the board. The baselines show Main erallyfarlowerforeachdataset. Next,weobserve
InputUpdateSuccessRatesof98%+acrosstasks thatlearnedoptimizerswithSLAG(r =10)out-
train
withcompetitiveorevenpositive∆-Accscores.3
perform baselines on sequence prediction tasks.
Whenstronglytuned,thesebaselinesoutperform On zsRE, we improve Update Success for Main
learnedoptimizersonmostmetricshere. Inputs by 4.86 ( 0.83; p=1e 4) and for Para-
± −
However,SLAGsurpassesthebaselinesinafew phrases by 1.39 ( 0.93; p=.004), with better ∆-
±
places. All Data Retain Rate on zsRE rises by Acc by 0.64 ( 0.35; p=.0005). Improvements
±
5.77points( 1.43;p<1e 4),andonWikidata5m trendinthesamedirectionforWikidata5mandare
± −
ParaphraseUpdateSuccessrisesby11.92( 1.20;
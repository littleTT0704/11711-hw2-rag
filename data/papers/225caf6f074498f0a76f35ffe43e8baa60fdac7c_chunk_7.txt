(Kingmaand
ELBObyλgivingthetotalobjectiveas:
Welling, 2013), which allows for the expectation
underq tobeapproximatedthroughsamplingina
(cid:88)
w ana dy et nh ca ot dp ere rsse ar rv ee ds ib sa cc uk ssp er dop ia ng fa ut ri to hn e. rT dh ee tad ile ic nod Se er cs
-
p(x li|µ
sem
lj)+p(x lj|µ
sem
li)+λELBO
(x,x )∈X
li lj
tion5.
In contrast to variational autoencoders, which Therefore, our objective resembles translation
have only a single latent variable for each exam- withaweightedsource-separationterm. Weshow
ple,wehavethreeinourmodelforeachexample. theeffectivenessofthisformulationcomparedto
Toencouragesourceseparation,wemakeseveral apuretranslationobjectiveinourexperimentsin
independenceassumptionsforq andfactoritinto Section6.
threeterms:
5 Architecture
Our architecture is an encoder-decoder model,
q(z sem,z li,z lj|x li,x lj;ϕ) = where the encoder produces a single representa-
q(z |x,x ;ϕ)q(z |x ;ϕ)q(z |x ;ϕ) tion that is fed into the decoder. Cross-attention
sem li lj li li lj lj
betweentheencoderanddecoderisnotused,there-
Lastly, we note that the ELBO contains a KL forethedecoderhasnofullsequencevisibilityand
termthatactstoregularizethelatentvariables. In morepressureisappliedontheencodertocreate
ourmodel,theKLtermencouragesz,z,and asemanticallymeaningfulrepresentation. Specif-
sem li
z to be close to a zero-centered Gaussian prior. ically, we follow the approach of Wieting et al.
lj
The KL term thus encourages source separation, (2020)whichusesaTransformer(V
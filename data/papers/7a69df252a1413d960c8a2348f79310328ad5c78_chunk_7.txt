olution,localself-attention[26]extracts
a localregion of pixels ab∈N (i,j) foreach pixel x and a givenspatial extent
k ij
k. An output pixel y computes as follows:
ij
(cid:88)
y = softmax (qTk )v. (3)
ij ab ij ab ab
a,b∈Nk(i,j)
q = W x denotes the queries, k = W x the keys, and v = W x
ij Q ij ab K ab ab V ab
the values, each obtained via linear transformations W of the pixel ij and their
neighbourhood pixels. The advantage over a simple convolution is that each
pixelvalueisaggregatedwithaconvexconvolutionofvaluevectorswithmixing
weights (softmax ) parametrised by content interactions.
ab
6 H. Schulze et al.
Squeeze-and-excitation (SE) attention Instead of focusing on the spatial
component of CNNs, SE attention aims to improve the channel component by
explicitly modelling interdependencies among channels via channel-wise weight-
ing. Thus, they can be interpreted as a light-weight self-attention function on
channels.
First, a transformation, which is typically a convolution, outputs the feature
mapU.Becauseconvolutionsuselocalreceptivefields,eachentryofU isunaware
ofcontextualinformationoutsideitsregion.AcorrespondingSE-blockaddresses
this issue by performing a feature recalibration.
A squeeze operation aggregates the feature maps of U across the spatial
dimension (H ×W) yielding a channel descriptor. The proposed squeeze oper-
ation is mean-pooling across the entire spatial dimension of each channel. The
resultingchanneldescriptorservesasanembeddingoftheglobaldistributionof
channel-wise features.
AfollowingexcitationoperationF aimstocapturechannel-wisedependen-
ex
cies, specifically non-linear interaction among channels and non-mutually exclu-
sive relationships. The latter allows multiple channels to be emphasized. The
excitation operation is a simple self-gating operation with a sigmoid activation
function:
F (z,
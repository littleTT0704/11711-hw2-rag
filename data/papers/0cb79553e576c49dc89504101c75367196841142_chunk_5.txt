okens/spansintheinputtext
are detected and masked, then filled in with to-
d =
1(cid:0)
D
(P+∥P−)(cid:1)
+
1(cid:0)
D
(P−∥P+)(cid:1)
kens/spansfromthetarget-styleusingamaskedlan- i 2 KL 2 KL
guagemodel. Otherworkhasframeddetoxification
After normalizing all distances by the mean, we
asatranslationorparaphrasingtask,usingaclassi-
maskallw whosedistanced isaboveathreshold
i i
fiertosteerawayfromtoxiccontent(Nogueirados
τ and denote the resulting sequence wm; these
Santosetal.,2018;Daleetal.,2021).
maskedtokensarelocationswheretoxicitymaybe
presentduetoexpertandanti-expertdisagreement.
3 TextDetoxificationwith MARCO
3.2 ContextualReplacing
MARCOisanunsupervisedapproachtotextdetox-
ification,consistingoftwodiscretesteps: masking
Aftermaskingpotentiallytoxiclocations,MARCO
then replaces them with more benign tokens – if
andthenreplacingtokens,assistedbythecontext
2GivenprobabilitydistributionsAandB,theKLdiver-
1Wereleaseourcodeanddataathttps://github.com/
genceisdefinedasD (A∥B)=
(cid:80)
A(x)log(cid:16) A(x)(cid:17)
shallinan1/MarcoDetoxification. KL B(x)
x∈V
Validation Test
Method Toxicity(↓) BERTScore(↑) Fluency(↓) Toxicity(↓) BERTScore(↑) Fluency(↓)
Original 0.286 – 51.49 0.272 – 70.20
CondBERT 0.161 0.966 104.10 0.148 0.964 88.69
MAgr
ParaGeDi 0.162 0.931 104.46 0.172 0.929 120.78
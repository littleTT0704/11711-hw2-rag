U models are trained
theobservationsandhypothesisin[33].
to reconstruct the past and future sequences, they can rep-
resent the past and future in a more reasonable way than
5.3.LimitationsandFutureWork
the ConvNet models. Our results also indicate the abil-
ity of our GRU models to capture wider range of tempo- Although our results on question answering for video
ral information than ConvNet models. ConvNets trained temporal context are encouraging, our model has multiple
from still frames can model temporal structures if objects limitations. First, ourmodelisonlyawareofcontextofat
in scene don’t change too much in short intervals (one ex- most30seconds(thelongestunrolllength). Onemoreflex-
amplewouldbeinFigure1,“cucumber”occursinbothcur- ibleandpromisingapproachwouldbeincorporatingtheat-
rentandfutureclip). However,whenitcomestomodeling tentionmechanism[3]tolearnlongersequencesofcontext
longer sequences, ConvNets will fail to make predictions invideos. Additionally,ourmodelfailstoanswerquestions
8
about detailed objects sometimes, due to lack of local vi- [11] R.Girshick,J.Donahue,T.Darrell,andJ.Malik. Richfea-
sual features, i.e., region-level, bounding boxes based rep- ture hierarchies for accurate object detection and semantic
resentation. Wewouldliketointegrateobjectdetectionin- segmentation. InCVPR,2014. 1
gredientstolocalizeobjectsforbettervisualunderstanding. [12] S.HochreiterandJ.Schmidhuber.Longshort-termmemory.
Lastly,wefixedsentenceandwordrepresentationlearning Neuralcomputation,9(8):1735–1780,1997. 1,4
partinthiswork. Learningbothvisualandlanguagerepre- [13] M.Hodosh,P.Young,andJ.Hockenmaier. Framingimage
description as a ranking task: Data, models and evaluation
sentations simultaneously is an open problem as indicated
metrics. JAIR,pages853–899,2013. 3
in[
ing. Weusedhyperparametersfromex-
istingstudies(Taorietal.,2023)asareferenceand
fine-tunedthebatchsizeto128,thelearningrate ofproducinghigh-qualitydiscussiondatathatcan
to 2e-5, and the epoch to 3. We used five nodes, beusedfortrainingsystemstobeabletodiscuss
eachcontainingeightNVIDIAA100GPUs. The givenproblems.12 Therefore,onecansignificantly
systemisgivenboththelabelsanddiscussionsas lowerthecostofcreatingdiscussiondatamanually
goldsduringtraining,andweevaluateusingonly byusingsystems.
labelsduringinference. Wetrainmodelswithout
pseudo-discussiondataasabaseline. Thebaseline 6.2 DoDiscussionExamplesinthePrompts
modelsaretrainedwithonlythelabels. Matter?
Table5showstheresultsoftheautomaticevalu- It is known that pre-trained models can ob-
ationofperformanceinSNLIandANLIforeach tain good results even with irrelevant or noisy
ofthemanuallygenerateddiscussionexampledata prompts (Khashabi et al., 2022; Webson and
andsystem-generatedpseudo-discussionexample Pavlick, 2022; Min et al., 2022). Therefore, we
dataforfew-shotlearning,respectively. Intwoof investigatethesensitivityandrobustnessofthesys-
the four datasets, the system’s performance with tem with respect to the discussion examples con-
pseudo-discussiondataoutperformsthatofthesys- tained in the prompts. We provide three types of
temwithmanuallycreateddata. Moreover,thereis noiseintheprompts: (1)assigningarandomdis-
nosignificantdifferencebetweenthescoresofthe cussionthatisirrelevanttotheexampleproblem,
LLMsusingthehuman-createdandpseudo-discus- (2)cuttingtheoriginaldiscussionexamplesshortat
sion by McNemar’s test (p < 0.01). It is possible randomtimes,and(3)assigningalabelatrandom
to achieve performance comparable to manually fortheexampleproblems.
createddata,evenwithpseudo-discussiondata. Table 7
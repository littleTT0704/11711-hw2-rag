-Based Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and
Framework for Multi-Task Cross-Lingual Transfer. Graham Neubig. 2017. Controllable invariance
InProceedingsofthe2020ConferenceonEmpirical throughadversarialfeaturelearning. InAdvancesin
MethodsinNaturalLanguageProcessing(EMNLP), NeuralInformationProcessingSystems,pages585–
pages7654–7673,Online.AssociationforComputa- 596.
tionalLinguistics.
GuoqingZheng,AhmedHassanAwadallah,andSusan
Hieu Pham, Qizhe Xie, Zihang Dai, and Quoc V Dumais.2021. Metalabelcorrectionfornoisylabel
Le. 2020. Meta pseudo labels. arXiv preprint learning. In Proceedings of the 35th AAAI Confer-
arXiv:2003.10580. enceonArtificialIntelligence.
AfshinRahimi,YuanLi,andTrevorCohn.2019. Mas- Shuyan Zhou, Shruti Rijhwani, and Graham Neubig.
sively multilingual transfer for ner. In Proceedings 2019. Towards zero-resource cross-lingual entity
of the 57th Annual Meeting of the Association for linking. arXivpreprintarXiv:1909.13180.
ComputationalLinguistics,pages151–164.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel
Urtasun. 2018. Learning to reweight examples for
robust deep learning. In International Conference
onMachineLearning,pages4334–4343.
A Hyper-parameters approachhasbeenadoptedin(Artetxeetal.,2020).
Table12andTable8presentresultsforNERand
Weuseamaximumsequencelengthof200and256
SA respectively where we finetune the tasks on
forNERandASrespectively. Weuseabottleneck
mBERT. Note that the languages of SA are both
dimensionof r = 384andr = 192forthe repre-
covered by mBERT and XLM-R, while the lan-
sentationtransformationnetwork,sameasPfeiffer
guages of NER are not
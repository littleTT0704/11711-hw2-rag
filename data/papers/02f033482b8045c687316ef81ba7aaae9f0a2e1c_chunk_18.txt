increaseinthedesiredsentiment.
Inpractice,wemaywantdifferentlevelsofsenti-
The tradeoff between output toxicity and fluency
mentcontroldependingontheapplication(e.g.,ag-
looks very similar for DEXPERTS detoxification
gressivelypositivemarketingpitchesversusmerely
(§3),andisincludedinAppendixD.1.
friendlychatbots). Figure5showstherelationship
betweenoutputsentimentandfluencyfordifferent
5 StylisticRewritingwith DEXPERTS
choicesofα P r´3.4,3.4s,conditionedonneutral
prompts. Thesmoothtradeoffsuggeststhatαcan As a preliminary exploration, we go beyond gen-
by adjusted by a practitioner or user, depending eratingtextcontinuationstoapply DEXPERTS to
ontheirapplication. Inourexperiments,wepick stylisticrewriting,i.e.,rewritingasentenceinatar-
α “ ˘3.2 because the curve becomes less steep, getstylewhilepreservingasmuchcontentaspos-
meaningthatagreatercostinfluencydoesnotre- sible. Wereplacethebasemodelwithapretrained
autoencoder, BART(Lewisetal.,2020), anduse requiresignificantcomputationalresources,which
GPT-2Largesentiment(anti-)expertsfrom§4for may no longer be feasible with the size of more
steering. Ateachtimestep,theautoencoderbase recentpretrainedLMs(Brownetal.,2020;Fedus
modelconditionsonboththeinputsequenceand etal.,2021).
the generation-so-far, whereas the (anti-)experts Decoding-timemethods,amorelightweightap-
conditionononlythelatter. Asaproofofconcept, proach,havebeenusedcontrollingtheattributesof
weshowsomeexamplesofinput/outputfromthis generatedtext,aswellasforimprovingitsquality
systeminTable4. (Li et al., 2016; Holtzman et al., 2018; Welleck
et al., 2020). PPLM (Dathathri et al., 2020) is a
Input�
 models that were trained on another modality for which more data are available.
Aytar et al. (2016) train SoundNET which distills audio representations from a pre-trained
image classification model trained on large image datasets such as ImageNet (Deng et al.,
2009). Wu et al. (2022a) (§4) distill an audio representation (Wav2CLIP) from a large
text-image model (CLIP) using video data to link the visual and audio modalities.
Using and evaluating representations Representation models can be used in down-
stream tasks with full fine-tuning; but the na¨ıve approach is simply to treat interme-
diate pre-trained model outputs as frozen embeddings, and this nonetheless provides a
stark improvement over using raw features (Turian et al., 2010). Broad-scale evaluation
of learned representations has been done in other ML domains, in NLP, for example:
GLUE (Wang et al., 2019b), the harder SuperGLUE (Wang et al., 2019a), and ERASER
(DeYoung et al., 2020). Vision includes the FAIR self-supervision benchmark (Goyal et al.,
2019) and VTAB (Zhai et al., 2019).
4
HEAR: Holistic Evaluation of Audio Representations
3. HEAR: Holistic Evaluation of Audio Representations
A strong general-purpose audio representation should be as holistic as the human ear.
The goal of the HEAR competition is to evaluate audio representations across a variety of
everyday domains, audio phenomena, with tasks that involve short and long time spans,
sometimes with few labeled instances. Formal rules are provided on the HEAR website.2
3.1. Related audio shared tasks
Historical audio shared tasks, such as those from MIREX (Downie et al., 2014), DCASE
(Mesaros et al.,2017),andINTERSPEECHComParE(Schuller et al.,2013)haveimproved
the community’s understanding of audio modeling substantially. However, the bespoke na-
tureofthesetasksisadouble-edgedsword,requiringsubstantialcustomtoolingbothbythe
challenge organizers and participants. More recent audio shared tasks focus on reusability
and generic task APIs. SUPERB (Yang et al., 2021) focuses on
 label.
createddiscussiondatainfew-shotlearningorfine- Ourannotatorswerethenpairedwitheachother
tuningcases. and discussed the questions for which they had
given different labels. They discussed in a free-
2 DiscussionDatasetCreation formmanneruntiltheyagreedonafinaldecision.3
Preliminaryexperimentalresultsshowedthatthe
The NLI task aims to determine the logical re-
lationship between a hypothesis sentence and a 2Annotationworkwasrequestedat$25perhour.Thedata
premisesentence(Bowmanetal.,2015). Thetask collectionfromhumanparticipantswasconductedunderan
institutionalreviewboardprotocol.
involves classifying whether the hypothesis sen-
3Theywerealsoinstructednottoincludepersonalinfor-
tenceisentailment,contradiction,orneutral. For mationandinappropriateutterances.
Figure2: Promptwithasingleexampleforfew-shotlearning.
numberofdiscussionturnstendedtobehigherfor examples’taskdescriptionandpremise,hypothe-
oralratherthantext-baseddiscussions. Therefore, sis, and gold labels are given as prompts. In the
we created discussion data by transcribing oral few-shot-discussionsystem,inadditiontothetask
discussionsamongtheannotators,usingWhisper description and examples, human discussion ex-
(medium.en)(Radfordetal.,2022)4 fortranscrip- amplesaboutthelabelsoftheexamplesaregiven
tion. The text transcribed by Whisper was manu- asprompts. Thesepromptsareconcatenatedwith
allycorrectedfortranscriptionerrorsandmanually theproblemtobesolvedandgivenasinputtothe
separatedintospeechsegments. system to perform inference. Examples of each
Then, for each utterance, we assigned the evi- prompt are shown in Figure 2. The discussion
dentialutterancesforthefinallabelandthelabels exampledistinguisheshumanutterancesbetween
of“supportive”,“unsupportive”,or“irrelevant”to “Human1:” and“Human2:”.
each utterance. For example, for Figure
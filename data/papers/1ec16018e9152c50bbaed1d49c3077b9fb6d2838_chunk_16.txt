�fast/super”, and “black/green”, which are often thecorpus.
interchangeable. Then, for a given target y, we apply k-means
t
clustering on the concatenated contrastive expla-
Usefulness Contrastive explanations were also
nationsacrossdifferentfoilsy toclusterfoilsby
f
considered useful to users for model simulation
explanation similarity. We use GPT-2 to extract
significantly more often than non-contrastive ex-
all the contrastive explanations due to its better
planations, with a particularly large gain in the
alignment with linguistic phenomena than GPT-
erasure-based setting. Answer accuracy on sam-
Neo(§4). Weonlyextractcontrastiveexplanations
pleswheretheusersfoundtheexplanationusefulis
withgradientnormandgradient inputduetothe
higherthantheaccuracyoverallsamplesforeach ×
computationalcomplexityofinputerasure(§3.3).
explanationmethod,whichsuggeststhatuserscan
In Table 5, we show examples of the obtained
alsoidentifyusefulexplanationstosomeextent.
clusters. Foilsineachclusteraresortedindescend-
These results, on the whole, provide evidence
ingfrequencyintrainingdata. Forthefirstfoilin
thatcontrastiveexplanationshelphumanobservers
eachcluster,wealsoretrieveits20nearestneigh-
simulatemodelpredictionsmoreaccurately.
bors in the word embedding space according to
Euclideandistanceforcomparison,todisentangle
6 WhatContextDoModelsUsefor
the effect of word embeddings from the effect of
CertainDecisions?
linguisticdistinctionsonfoilclusters.
Finally,weusecontrastiveexplanationstodiscover
6.2 FoilClusters
how language models achieve various linguistic
distinctions. Wehypothesizethatsimilarevidence First,weverifythatlinguisticallysimilarfoilsare
isnecessarytodisambiguatefoilsthataresimilar indeedclusteredtogether: wediscoverclustersre-
linguistically. Totestthishypothesis
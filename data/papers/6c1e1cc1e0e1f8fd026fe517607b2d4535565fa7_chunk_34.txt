,weexperiment
withavariantthatforcestheLLMtogeneratetheansweraftergeneratingthereasoningchain(Figure11e). Thissetting
compelstheLLMtoconditiononthegeneratedcode-basedreasoningtogenerateananswer,simulatingtheruntime. The
resultsinTable6(5throw)showthatthesolveratedropstonearDIRECTlevels. Thisreinforcesourhypothesisthatwhile
currentLLMscanbeexcellentatspecifyingahigh-levelplantosolveataskâ€”theyarestillincapableofexecutingthem.
Ablation SolveRate
DIRECT(nointermediatereasoning) 19.7
COT 65.6
PAL 72.0
SuccinctCode 47.8
LLMSimulatingRuntime 23.2
Table6: SolveRatesforAblations
C.EffectofUsingLanguageModelsofCode
Inourexperiments, wefocusedonevaluatingtheperformanceofalanguagemodelforcode. Weaimedtoinvestigate
whethertheadditionalperformanceboostobservedinourresultswasduetotheuseofmodelslikeCodex,orwhetherour
formulationwasusefulevenfortext-basedmodels. Tothisend,weconductedadditionalexperimentsusingtext-based
languagemodels. OurfindingsindicatethatthePALapproachisnotrestrictedtoworkingsolelywithCodex,butcanalso
beappliedtonaturallanguage(NL)models,aslongasthemodelissufficientlystrong. Specifically,ourresultsshowedthat
inthetext-davinci-001model,theuseoftheCoTapproachresultedinbetterperformance.
Model CoT PaL
text-davinci-001 26.5 8.6
text-davinci-002 46.9 65.8
text-davinci-003 65.3 69.8
D.AnalyzingtheEffectofIncreasingNumberofSampleson PAL
InSection5.1,weshowthatPALoutperformsstrongbaselinesbothforasinglesampleandbydrawing40samplesand
usingmajorityvoting. Figure12illustratesthetrendsforcaseswhenthenumberofsamplesdrawnarebetween1
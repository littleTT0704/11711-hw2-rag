)andTatoebausesSQuADv1.1dataforfine-tuning.
arereplacedinsteadbythreenew,morechalleng- apremisesentence. TheXCOPAauthorstranslated
ing tasks: one focusing on causal commonsense andre-annotatedthevalidationandtestsetsofthe
reasoning(§3.2.1)andtwofocusingonharderre- EnglishCOPA(Roemmeleetal.,2011)datasetinto
trievalscenarios(§3.2.2),asthishasbeenthetask 11 languages, which we use for evaluation. The
categorywheregainshavebeeneasiesttoachieve. EnglishCOPAtrainingsettogetherwiththeSocial
Weretain XTREME’ssevenothertasksaseachstill IQa (Sap et al., 2019) training data are used for
presentssubstantialchallengesforstate-of-the-art training. While accuracy on the English COPA
cross-lingual models (§3.1). Overall, XTREME-R recently reached 94.8% (Raffel et al., 2020), the
includes10diversetasks,summarizedinTable2. state-of-the-artonXCOPAisonlyaround70%.
Wealsomakechangestothestructuredpredic-
3.2.2 RetrievalfromaMultilingualPool
tiontasks,NERandPOS.Insteadofonlyprovid-
Manypreviousretrievalbenchmarksassumethat
ingexamplesaslistsoftokens, XTREME-R always
the entire candidate pool is in a single language.
provides the full text of an input sentence, thus
Forinstance,aFrenchquerywillbeusedtosearch
ensuring that the entire benchmark now supports
overonlyEnglishcandidates. However,practical
researchonmodelsthatoperatedirectlyfromthe
settingsoftenviolatethisassumption,e.g.thean-
rawinputstring(Clarketal.,2021). Furthermore,
swertoaquestionmaybeavailableinanynumber
XTREME-R adopts a more realistic version of the
of languages, possibly different from the query
NERtaskinwhichnogoldtokenizationisprovided
language. Modelsthatcannotcomp
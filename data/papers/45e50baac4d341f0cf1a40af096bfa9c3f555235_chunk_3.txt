(Ladhaketal.,2023;Giraetal.,2022). Biased
LLMs(Bidermanetal.,2023). Weevaluatethese
pretrained models are hard to fix, as retraining is
modelsagainstBiasBench(Meadeetal.,2022),a
1https://github.com/gsgoncalves/
compilationofthreesocialbiasdatasets.
EMNLP2023_llm_compression_and_social_
bias Inourexperimentalresultswedemonstrateacor-
2663
Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2663–2675
December6-10,2023©2023AssociationforComputationalLinguistics
relationbetweenlongerpretraining,largermodels, model will pick an equal ratio of both stereotypi-
andincreasedsocialbias,andshowthatquantiza- calandanti-stereotypicalchoices,thusanoptimal
tionanddistillationcanreducebias,demonstrating scoreforthisdatasetisaratioof50%.
the potential for compression as a pragmatic ap-
proachforreducingsocialbiasinLLMs. StereoSet iscomposedofcrowdsourcedsamples.
Each sample is composed of a masked context
sentence, and a set of three candidate answers:
2 Methodology
1) stereotypical, 2) anti-stereotypical, and 3) un-
Wewereinterestedinunderstandinghowdynamic related. Under the SS formulation, an unbiased
Post-TrainingQuantization(PTQ)anddistillation modelwouldgiveabalancednumberofclassifica-
influencesocialbiascontainedinLLMsofdifferent tionsoftypes1)and2),thustheoptimalscoreis
sizes,andalongtheirpretraining. IndynamicPTQ, also50%. TheSSdatasetalsomeasuresifweare
full-precisionfloatingpointmodelweightsarestat- changingthelanguagemodelingpropertiesofour
ically mapped to lower precisions after training, model. Thatis,ifourmodelpicksahighpercent-
with activations dynamically mapped from high ageofunrelatedchoices3)itcanbeinterpretedas

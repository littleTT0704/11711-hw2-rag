-
boththeinputandthesummary,theoraclemodel
tiveshift,theadditionofclarifyingwords,emotion
is tasked with choosing a combination of k utter-
attributions,andpronounsubstitutionsisnecessary;
ancesfromtheinputtomaximizeROUGE.Table
thesearehigh-edit-distanceoperationsthatarenot
5 shows the performance of an oracle extractive
observedfrequentlyintheformalitydata.
model over the original SAMSum dialogues and
Formalizationwithoutanyadditionaltrainingfor
theperspectiveshiftedversions. Forcomparison,
perspective shift is, as expected, far weaker than
a simple extractive baseline—choosing the three
theperspective-shift-onlymodel.
longestutterances—andastrongabstractivemodel
The rules-based heuristic appears competitive
arealsoreported.
in ROUGE, but both the BARTScore scores and
a manualinspection ofthe output reveal that this Results Clearly,thepotential(best-case)perfor-
approachislacking. manceofamodelovertheperspectiveshifteddia-
In the next section, we explore a down- loguesisbetter;theoraclescoresoverperspective
stream task: extractive summarization. For all shifteddialoguesevenapproachthescoresofthe
extractive summarization experiments, we use abstractivemodel.
model-generated perspective shift data from the
perspective-shift-only model. We train a model 5.2 ZeroShotandSupervisedExtractive
ononlythevalidation-setPSdatatogenerateper- Summarization
spective shifts for the train set of SAMSum, and
Train/TestRegimes Acommonsummarization
we train a model on only the train-set PS data to
domainisnewsarticlesduetotherelativelywide
generateperspectiveshiftsforthevalidationsetof
availabilityofdata. Weuseanextractivesumma-
SAMSum.
rizer trained on the CNN/DM news summariza-
tioncorpus(Nallapatietal.,2016): themodelPre-
5 Application: Extractivesummarization
Summ,introducedbyLiuandLapata(
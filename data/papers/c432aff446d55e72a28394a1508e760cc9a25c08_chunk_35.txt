12
Ft.MoSOnly ffn 2 IP 2D2+2D 21.351 0.843 21.338 20.258
Ft.MoSOnly ffn 3 IP 3D2+3D 21.495 0.733 21.460 20.322
Ft.MoSOnly ffn 4 IP 4D2+4D 21.321 0.994 21.321 20.396
Ft.MoSOnly ffn 5 IP 5D2+5D 21.371 0.909 21.367 20.406
Table7: PerformancecomparisonofkNNbaselinesandseveralMoSconfigurations. R isthenumberof
mixtures.
redundantcontextvectors,andthuscompressioncouldmakethedatastoresmallerwithoutsacrificingtoo
muchperformancegain. Heetal.(2021)showsthatwecansafelycompressthedatastorebyclusteringto50%
oftheoriginalsizewithoutlosingperformance. Wetestthisideafurtherbyclusteringtheentiredatastore
intoasizethatcouldfitinGPUmemory(e.g. 2V,3V)andthuscouldbeeasilyfinetunedfurtherandusethe
resultingcentroidstoreplaceW. Withineachcluster,therewillbeadistributionofdifferentwordswith
ds
contexts,andweusethefrequencyofwordswithineachclustertocalculatetheaggregationmatrixM in
Equation5. Thiswouldhavetheaddedbenefitof“multi-sense”embedding,whichallowssimilarmeaningsto
beclusteredtoformanew“metaword”whilethesamewordwithdifferentmeaningswouldformdifferent
“metawords”. Anotableexampleisbank,shore,andfinancialinstitution. However,thisdoesnotwork,mostly
becauseofthehighcompressionlossafterclusteringandtheimbalanceddistributionofwordtypesamong
eachcluster.
D WhichWordsBenefitfromApproximation?
TofurtherunderstandtheunexpectedresultswhenusingthedifferentkNNapproximateretrievalsettings
inSection5.1andSection5.2, weanalyzeonatokenlevel, basedonhowman
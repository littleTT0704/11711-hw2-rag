
Table 5: Teasing apart the contributions of a code generation model and a structured prompt. The experiments
showthatbotharehelpful. DAVINCI,atextgenerationmodel,showsmarginalimprovementswithacodeprompt
(toptworows). Similarly, CODEX, acodegenerationmodel, significantlybenefitsfromacodeprompt. Overall,
CODEXwithcodepromptperformsbetterthanthealternatives,acrossallmetrics.
improvesfortheNL-LLM DAVINCIbothwhenwe In EXPLAGRAPHS, we observed that the train-
useacodeprompt,and whenweuseaCode-LLM. ingdatahadmultipleexampleswhichwerenearly
HoweverwhenusingbothaCode-LLMandacode identical, and thus dynamically created prompts
promptâ€“theimprovementisgreaterthanthesum oftenincludedsuchduplicateexamples,effectively
ofeachofthesesolely. reducingdiversityandpromptsize(Table9).
Dynamicpromptselection Thepromptsforall PythonFormatting Weperformedanextensive
experimentsinSection3werecreatedbyrandom
study of the effect of the Python format on the
samplingofexamplesfromthetrainingset. Specif-
downstreamtaskperformanceinAppendixG.We
ically,asetofk (T,G)pairsaresampledandcon-
findthat: (i)therearenocleartask-agnosticPython
catenatedintoapromptp,whichweusedforinfer-
class designs that work uniformly well; and that
enceoverallexamplesx inthetestset. Asan
test (ii) larger models are less sensitive to prompt
alternativetocreatingprompts,thereisnowagrow-
(Python class) design. In general, our approach
inginterestincustomizingthein-contextexamples
benefitsthemostfromcodeformatsthatassimilar
eachexamplex. Populartechniquestypically
test aspossibletotheconventionsoftypicalcode.
trainaretriever,whichisusedtofetchtheclosest
examples(Liuetal.,2021;Rubinetal.,2021;Poe-
Human evaluation We conduct human evalua-
siaetal.,
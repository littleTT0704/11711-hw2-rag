 feedback. This material is partly
relationshipbetweenthem. Ourworkissimilarin
based on research sponsored in part by the Air
spirit to Wu et al. (2022), and seeks to leverage
ForceResearchLaboratoryunderagreementnum-
thedualabilitiesofCode-LLMsfortextandsym-
berFA8750-19-2-0200. TheU.S.Governmentis
bolic reasoning. However, differently from their
authorizedtoreproduceanddistributereprintsfor
work, we close the gap between the pre-training
Governmentalpurposesnotwithstandinganycopy-
data and our tasks by translating our output into
rightnotationthereon. Theviewsandconclusions
Pythoncode. Asourexperimentsshow,thisstepis
containedhereinarethoseoftheauthorsandshould
crucialinoutperformingtext-onlyandfine-tuned
not be interpreted as necessarily representing the
models. Tothebestofourknowledge,ourworkis
officialpoliciesorendorsements,eitherexpressed
thefirsttotransformanatural-languagereasoning
orimplied,oftheAirForceResearchLaboratory
problem into code to successfully leverage code
or the U.S. Government. This project was also
generationmethods.
partiallysupportedbyagiftfromAWSAI.
Symbolic reasoning using LLMs The use of
Limitations
programming languages like LISP (Tanimoto,
1987)andProlog(ColmerauerandRoussel,1996) Someexperimentsinthisworkareperformedwith
to process natural language has a long history in languagemodelsthatarenotopen-sourced,namely
AI.However,therecentprogressinlargelanguage DAVINCI, CURIE,and CODEX. Existingdocumen-
modelshasobviatedtheneedforspecializedmeth- tation(Brownetal.,2020;Chenetal.,2021b)does
odsforsymbolicprocessing. Cobbeetal.(2021) notfullydescribethedetailsofthesemodels,such
andChowdheryetal.(2022)addressmiddle-school as the pretraining corpus, model size, and model
levelalgebraproblemsolvingusinglarge-language biases. Therefore,wecan

IlyaOTolstikhin,NeilHoulsby,AlexanderKolesnikov,LucasBeyer,XiaohuaZhai,ThomasUn-
terthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer:
An all-mlp architecture for vision. Advances in Neural Information Processing Systems, 34:
24261â€“24272,2021.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929,2020.
13
24 52.05
50
23
40
22
30
21
18.80
20
20
10
19
0
1510 25 50 100 STE EOT-10
# Steps Method
(a) (b)
Figure9:AccuracyofaR-BlurmodeltrainedonImagenetunderAPGDattackwithdifferentsettings.
(a)showstheaccuracywhenAPGDattackisappliedwithdifferentnumbersofupdatesteps. (b)
showstheaccuracywhen10stepofexpectation-over-transformation(EOT-10)[Athalyeetal.,2018b]
isusedandR-Blurisconvertedintoastraight-through-estimator(STE)inthebackwardpass. The
dashedlinein(b)showstheaccuracyofa25-stepAPGDattackwithoutEOTandnormalgradient
computation for R-Blur. Together these results strongly indicate that R-Blur does not obfuscate
gradientsandlegitimatelyimprovestheadversarialrobustnessofthemodel.
A PreventingGradientObfuscation
Wetakeanumberofmeasurestoensurethatourresultscorrespondtothetruerobustnessofour
method,andweavoidthepitfallsofgradientobfuscation[Athalyeetal.,2018a,Carlinietal
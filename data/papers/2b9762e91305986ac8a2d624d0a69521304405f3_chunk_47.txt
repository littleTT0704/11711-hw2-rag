 Language-agnosticRetrievalResults
perlayer,12attentionheads,a110ksharedWord-
Piece vocabulary, and 110M parameters.12 The The multiway cross-language nature of Mewsli-
modelwastrainedusingWikipediadatainall104 XandLAReQAenablescloseranalysisofmodel
languages,oversamplinglow-resourcelanguages performance by input and target language pairs.
with an exponential smoothing factor of 0.7. We Mewsli-X can directly be split by language pair
generallyfine-tunemBERTfortwoepochs,witha asithasasinglecorrecttargetperinputmention.
trainingbatchsizeof32andalearningrateof2e-5. ForLAReQA,wefollowthe“LimittoOneTarget”
WebuildontheTransformerslibrary(Wolfetal., strategyofRoyetal.(2020): insteadofaskingthe
2019)fortrainingoneachtask. model to retrieve all correct answers in one pass,
XLM-R We use the XLM-R Large version that weevaluateoneachtargetseparately,withallthe
covers 100 languages, uses a 200k shared BPE othercorrectanswersremovedfromthecandidate
vocabulary,andhasbeentrainedwithmaskedlan- pool,allowingustoreportsplitsbylanguagepair.
guagemodelling.13 Wefine-tuneXLM-Rgenerally Table14summarizesthesepairwisemAP@20
fortwoepochswithalearningrateof3e-5andan scores(here,micro-averaged),showingthatXLM-
effectivebatchsizeof16. WeusetheTransformers RLargeimprovessubstantiallyovermBERTonthe
libraryfortrainingXLM-Ronalltasks. cross-lingualcase(+38%onMewsli-Xand+137%
mT5 We use the publicly released mT5-XXL forLAReQA)inexchangeforaslightdropforthe
versionthathasnearly13billionparameterswitha same-languagecase. Evenso,performanceonthe
vocabularysize250k(Xueetal.,2021). Ithasbeen cross-lingualcaseisstilllowat29–
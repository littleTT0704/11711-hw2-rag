,thereisnosignificantdifference E3.
on training convergence over E1, E2 and E3. Thus we just Training Complexity and Speed. Based on our experi-
takeE1intoconsiderationinFig. 10. Thefinalresultsaredis- ments,bothtrainingaudiodurationandbatchsizemakenodif-
playedinTable2. Thehyper-parametersinbothTable. 2and ferenceinthetrainingspeed.Tobeconcise,wejustchoosethe
Fig.10arethebestonesthatwehaveexplored. trainingprocessofE1inFig. 10fortheillustrationoftraining
speed over different loss functions. It is observed that Triplet
4.4. Discussion convergesthemostslowlyduetoitslargesttrainingcomplex-
ity as shown in Table. 1. Other non-proxy-based losses con-
ResultsAnalysis.Table2presentstheEERachievedusingdif-
vergefasterthanTripletduetoitssmallertrainingcomplexity.
ferent loss functions under certain settings. The performance
Proxy-based losses converge fastest. Note that though Proxy
forProxyNCAandProxyAnchoraresimilartothatofProto-
NCAandProxyAnchorgivealargerfinalEERincomparison
typical.Basedonthecurrentbestsmoothingfactorandmargin,
to that ofAngular Prototypical, they stillconverge fasterthan
ProxyAnchorreachesslightlylowerEERthanProxyNCA.The
AngularPrototypical.Ourproposedlossfunctionsgivethebest
potential reason is that Proxy Anchor indirectly leverages the
EERaswellastrainingspeed.
fine-graineddatarelationship.
The proposed loss functions including Mask Proxy and
5. Conclusion
MultinomialMaskProxyoutperformtheexistingstate-of-the-
artAngularPrototypicallossobjective. Basedontheseresults, Theproposedlossobjectivesinthispaperachievestate-of-the-
art speaker verification performance on the VoxCeleb dataset.
1ForMP/MMP-Balance,inputsizeis2Ã—[batchsize,featuresize]
To the best of our knowledge, it is the first work that applies
and*expected
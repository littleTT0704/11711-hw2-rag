
17
PublishedasaconferencepaperatICLR2023
A.3 EXPERIMENTDETAILS
A.3.1 CLASSICIMAGECLASSIFICATION
Wepresentthedetailedhyper-parametersusedfortheclassicimageclassificationsettinginTable6
for reproduction. We use NVIDIA V100 for training of classic image classification. The training
time for CIFAR-10 and SVHN on a single GPU is around 3 days, whereas the training time for
CIFAR-100andSTL-10isaround7days.
Table6: Hyper-parametersofclassicimageclassificationtasks.
Dataset CIFAR-10 CIFAR-100 STL-10 SVHN ImageNet
Model WRN-28-2 WRN-28-8 WRN-37-2 WRN-28-2 ResNet-50
WeightDecay 5e-4 1e-3 5e-4 5e-4 3e-4
LabeledBatchsize 64 128
UnlabeledBatchsize 448 128
LearningRate 0.03
Scheduler η=η cos(7πk)
0 16K
SGDMomentum 0.9
ModelEMAMomentum 0.999
PredictionEMAMomentum 0.999
WeakAugmentation RandomCrop,RandomHorizontalFlip
StrongAugmentation RandAugment(Cubuketal.,2020)
A.3.2 LONG-TAILEDIMAGECLASSIFICATION
The hyper-parameters for long-tailed image classification evaluation is shown in Table 7. We use
Adamoptimizerinstead.Forfastertraining,WRN-28-2isusedforbothCIFAR-10andCIFAR-100.
NVIDIAV100isusedtotrainlong-tailedimageclassfication,andthetrainingtimeisaround1day.
Table7: Hyper-parametersoflong-tailedimageclassificationtasks.
Dataset CIFAR-10 CIFAR-100
Model WRN-28-2
WeightDecay 4e-5
LabeledBatchsize 64
UnlabeledBatchsize 128
LearningRate 0.002
Scheduler η=η cos(7πk)
0 16K
Optimizer Adam
ModelEMAMomentum
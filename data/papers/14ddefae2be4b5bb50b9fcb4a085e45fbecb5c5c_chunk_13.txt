PT 19.56 20.16 56.25 55.24 77.07 64.36 80.69 13.48 14.10
+5examples 27.75 28.07 63.25 60.43 81.95 69.57 85.15 21.27 22.10
Table 4: Model performance for empathic similarity prediction task across correlation, accuracy, and retrieval
metrics. r=Pearson’scorrelation,ρ=Spearman’scorrelation,Acc=accuracy,P =precision,R=recall,P =
k=1
precisionatkwherekis1,τ =KendallTauofrankingandρ =Spearmanofranking. Notethatallscores
rank rank
aremultipliedby100foreasiercomparison,andthemaximumforeachmetricis100. Inboldisthebestperforming
andunderlinedisthesecond-bestperformingconditionforthemetric.
Baseline Models. We compare performance to k = 1 (what proportion of the top-ranked stories
finetuningwithSBERT(multi-qa-mpnet-base-dot- byourmodelarethetop-rankedstoryasratedby
v1) (Reimers and Gurevych, 2019; Brown et al., human annotators), Kendall’s Tau (Abdi, 2007),
2020)andBARTmodel(bart-base)(Lewisetal., andSpearman’scorrelation(Schoberetal.,2018)
2019). Asafew-shotbaseline,weevaluateGPT-3 fortherankingofthestories(howclosetheoverall
(text-davinci-003)andChatGPT’s(gpt-3.5-turbo) rankingsare).
abilitytodistinguishempathicallysimilarstories ShowninTable4,ourresultsindicatethatfine-
byusingak-shotpromptingsetupasdoneinSap tuning SBERT and BART with EMPATHICSTO-
et al. (2022); Brown et al. (2020). For the query RIESresultsinperformancegainsacrossallmetrics.
storypair,weaskforanempathicsimilarityscore SBERT has relatively high off-the-shelf perfor-
from1-4.
 news that are currently noteworthy but may be unimportant in the overall
context of the topics. For instance, when searching for Barack Obama, the retrieved
web pages contain news stories about speeches, meetings and other everyday business
that may be insignificant from a long-term perspective. It can be difficult and time-
consuming for a human annotator to distinguish between unimportant details and
truly relevant information when labeling such documents. We recommend inspecting
the search results carefully and avoiding topics that may involve frequent difficult
annotation decisions. In most knowledge domains there is no shortage of potential
topics and we found it well worth the effort to choose carefully.
The intrinsic evaluation results in this chapter and the results on QA datasets
in Chapter 6 confirm that our annotated data can be used to fit effective relevance
models. We also show in Section 5.4 that the intrinsic performance of these models
does not degrade much even if noise is artificially added to the dataset.
5.2 Experimental Setup
Severalrelevanceestimationstrategieswereevaluatedthrough12-foldcross-validation
on the 12 topics marked with CV in Table 5.1. We assigned each topic to a single fold
to ensure that we never train and test models on similar instances, thus avoiding bi-
ased results. Each approach was applied to both text nuggets delimited by structural
HTML markup and sentence-level nuggets (see Section 4.2 for details).
The nuggets were ranked by their relevance estimates in descending order, and
performance was measured in terms of mean average precision (MAP) and precision-
recall curves. We focused on these measures of ranking performance since we were
interested in finding models that can effectively rank text nuggets by relevance and
that can be used in the source expansion system to select the most relevant text.
Precision and recall were computed at the level of individual tokens rather than text
nuggets to ensure that results are comparable across different nugget granularities.
Thus, precision is the percentage of tokens up to a given cutoff-point in the ranking
that were labeled as relevant by an annotator, and recall is the percentage of relevant
tokens that were ranked above the cutoff point. Note that classification accuracy is
56 CHAPTER 5. INTRINSIC EVALUATION
of limited use as a performance metric for the nugget scoring task because of the
imbalance between relevant and irrelevant nuggets. A trivial model
 propose to learn λ, wk in an online, data-driven way. To do this, we build on
e
{ }
top of the META-TARTAN algorithm proposed by Dery et al. (2021b). META-TARTAN is a
meta-learning algorithm that learns adaptive weights for different auxiliary tasks in a way that
prioritizesend-taskgeneralization. Itlearns wk byminimizingthelossontheend-taskvalidation
{ }
set: ∂ ∂L wv E kal ≈−(cid:0) ∇θ Lf ak(cid:1)T(cid:0) ∇θ Lv Eal(cid:1). Thiscorrespondstolearning {wk }suchthat(cid:0) ∇θf a(cid:1)T(cid:0) ∇θf e)
is maximized. This minimizes one of the terms that contributes to ∆ and thus attempts to fulfil
Prescription(P ). Wecansimilarlylearnλ tominimizetheend-taskvalidationloss. Foramore
1 e
detaileddiscussionofMETA-TARTAN,pleaseseeAppendixB.
So far, we have introduced independent weights, wk, for each objective. This is sufficient in
{ }
thecaseofunrelatedobjectives. However,theobjectivesin shareanunderlyingstructure. We
recognizethisbyusingafactoredapproachtomodeleachwk.A
Weintroduceafactorvectorforeach
ofthe4stagesintroducedinSection3: WD R|D|,WT R|T|,WR R|R| andWO R|O|.
∈ ∈ ∈ ∈
Thistiestogethertheweightsofobjectivesthatshareprimitivesincommon. Tocapturethefact
that an objective can be more than the sum of it parts, we also introduce an independent weight
for each objective : WAll R|D|×|T|×|R|×|O|. Consider the objective k which is generated by
∈
thecompositionoftheoperations d, t, r, o,itsweightingiscomputedas
: wk exp(
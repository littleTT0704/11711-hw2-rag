) that
extends the posterior regularization for added flexibility.
24
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
4.3. Reward Experience
We now consider a very different learning setting commonly seen in robotic control and other sequential
decision making problems. In this setting, experience is gained by the agent interacting with external
environment and collecting feedback in the form of rewards. Formally, we consider a Markov decision process
(MDP) as illustrated in Figure 3, where t = (x,y) is the state-action pair. For example, in playing a video
game, the state x is the game screen by the environment (the game engine) and y can be any game actions.
At time t, environment is in state x. The agent
t
draws an action y according to the policy
t
p (y∣x). The state subsequently transitions to
θ
x following certain transition dynamics of the
t+1
environment, and yields a reward
r
t
= r(x t,y t) ∈ R. The general goal of the
agent is to learn the policy p (y∣x) to maximize
θ
the reward in the long run. There could be different Figure 3. The MDP setting.
specifications of the goal. In this section we focus
on the one where we want to maximize the expected discounted reward starting from a state drawn from an
arbitrary state distribution p (x), with a discount factor γ ∈ [0,1] applied to future rewards.
0
A base concept that plays a central role in characterizing the learning in this setting is the action value function,
also known as the Q function, which is the expected discounted future reward of taking action y in state x
and continuing with the policy p :
θ
∞
Qθ (x,y) =E ∑γtr t ∣ x 0 =x,y 0 =y, (4.15)
[ ]
t=0
where the expectation is taken by following the state dynamics induced by the policy (thus the dependence of
Qθ on policy parameters θ). We next discuss how Qθ(x,y) can be used to specify the experience function
in different ways, which in turn derives various known algorithms in reinforcement learning
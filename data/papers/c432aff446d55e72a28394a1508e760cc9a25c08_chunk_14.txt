howthechoiceofh,inputrepresentation,affectskNNbaselinesand
ds
modelswithlearnableembeddingsasdatastorealternative. h istheattentionlayeroutput.
ds
4.2 IncreasingtheSoftmaxCapacity
OnepremisebehindkNN-LMisthatthelargedatastoreisthekeyreasonforthemodelworkingwell: the
largerthesoftmaxcapacity,thebettertheperformance. Naturally,asafirststep,weneedtocheckwhether
such a big datastore is warranted and whether the high rank of W leads to better performance. We test
ds
theeffectofthedatastoresizeforkNNretrievalonkNN-LMinterpolatedperplexity. Ifabiggerdatastore
(ahighrankW )isbetterinkNN-LMthanasmallerdatastore,thenthehypothesisofsoftmaxcapacityis
ds
moreprobable. Werandomlysubsamplethefulldatastoreinvaryingpercentagesandtheresultsareshown
inFigure2. Thefulldatastorecontainsmorethan150Mentriesandstoringthemtakes293GBwhenusing
half-precision floating points (fp16). We can see that whether or not approximate kNN is used, the final
perplexitydecreasesalmostlinearlywithmorepercentageoftheoriginaldatastore. Evenwithjust5%of
thedatastoresize(15G),kNN-LMstillprovidesabenefitoverjustusingthebaseLM.However,evenwhen
thesubsamplingpercentagereaches90%,havingmoreentriesinthedatastorestillprovidesbenefitswithout
havingsignificantdiminishingreturns,suggestingthatalargedatastoreisbeneficial.
Onepossiblereasonwhyalargerdatastoreishelpfulisthatwordscanbedifficulttopredict. Thereareseveral
reasons: (1)Theyarerare,or(2)theyarefrequent,buttheyhavemultiplemeaningsandappearindifferent
contexts. Thesoftmaxbottleneck(Yangetal.,2017)suggeststhatthefinaldotproductoflanguagemodel
W Â·h limitstheexpressivityoftheoutputprobabilitydistributionsgiventhecontext;thatis,
ocuses tencybetweenthem[30,32,46].[1]studiestheempiricaleffectsof
onmodelingthebelongingrelationshipbetweenthelocalfeatureof consistencyregularization,whichvalidatesthemotivationofthe
asampleanditsglobalcontextrepresentation.Mostunsupervised teacher-studentmodel:introducingaslow-movingaverageteacher
learningmodelsongraphslikeDGI[48],InfoGraph[45],CMC- networktomeasuresconsistencyagainstastudentone,thuspro-
Graph[19]fallintothiscategory,followingtheInfoMaxprinciple vidingaconsistency-basedtrainingparadigmwheretwonetworks
tomaximizethethemutualinformation(MI)betweentheinput canbemutuallyimproved.Ithasbeenshownthatsemi-supervised
anditsrepresentation.However,estimatingMIisnotoriouslyhard learningperformancecanbegreatlyimprovedviaunsupervisedpre-
incontext-instancecontrastivelearningandinpracticetractable trainingofa(big)model,supervisedfine-tuningonafewlabeled
lower bound on this quantity is maximized instead. And maxi- examples,anddistillationwithunlabeledexamplesforrefiningand
mizingtighterboundsonMIcanresultinworserepresentations transferringthetask-specificknowledge[9].However,whether
withoutstrongerinductivebiasesinsamplingstrategies,encoder task-agnosticself-distillationwouldbenefitsemi-supervisedlearn-
architectureandparametrizationofMIestimators[47].Besides, ingisstillunderexplored.
theintricaciesofnegativesamplingincontext-instancecontrastive DataaugmentationDataaugmentationstrategiesongraphs
approachesimposekeyresearchchallengeslikeimproperamount are limited since defining views of graphs is a non-trivial task.
ofnegativesamplesorbiasednegativesampling[10,47].Another Therearetwocommonchoicesofaugmentationsongraphs(1)
IterativeGraphSelf-distillation Ljubljana’21,April19–23,2021,Ljubljana
To Adapt or to Annotate:
Challenges and Interventions for Domain Adaptation in
Open-Domain Question Answering
DheeruDua1∗ EmmaStrubell2,3 SameerSingh1 PatVerga2
1UniversityofCaliforniaIrvine 2 GoogleResearch 3 CarnegieMelonUniversity
Abstract
Recent advances in open-domain question an-
swering (ODQA) have demonstrated impres-
sive accuracy on standard Wikipedia style
benchmarks. However, itislessclearhowro-
bust these models are and how well they per-
form when applied to real-world applications
in drastically different domains. While there
has been some work investigating how well
ODQA models perform when tested for out-
of-domain (OOD) generalization, these stud-
ieshavebeenconductedonlyunderconserva-
tiveshiftsindatadistributionandtypicallyfo-
cusonasinglecomponent(ie. retrieval)rather
than an end-to-end system. In response, we
propose a more realistic and challenging do-
Figure1: Top: AverageReaderperformanceforBase-
main shift evaluation setting and, through ex-
line and best interventional or augmentation setup on
tensive experiments, study end-to-end model
top. Bottom: Thedifferencebetweenbaselineandper-
performance. Wefindthatnotonlydomodels
formance (end-to-end F1) after introducing interven-
fail to generalize, but high retrieval scores of-
tions, averaged over datasets exhibiting specific shift
tenstillyieldpooranswerpredictionaccuracy.
Wethencategorizedifferenttypesofshiftsand types.
propose techniques that, when presented with
anewdataset, predictifinterventionmethods
are likely to be successful. Finally, using in-
models(ODQA).ThestateoftheartODQAsys-
sightsfromthisanalysis,weproposeandeval-
tems perform a two-stage pipeline process (Izac-
uate several intervention methods which im-
ard et al., 2022): 1) given a question, a context
prove end-to-end answer F1 score by up to
retriever (Karpukhin et al., 2020; Izacard et al.,
∼24points.
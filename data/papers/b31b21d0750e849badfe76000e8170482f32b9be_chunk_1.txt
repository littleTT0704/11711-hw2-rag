PublishedasaconferencepaperatICLR2023
DocPrompting: GENERATING CODE BY RETRIEVING
THE DOCS
ShuyanZhou†,UriAlon†
FrankF.Xu†,ZhiruoWang†,ZhengbaoJiang†,GrahamNeubig†‡
†LanguageTechnologiesInstitute,CarnegieMellonUniversity,
‡InspiredCognition
{shuyanzh,ualon,fangzhex,zhiruow,zhengbaj,gneubig}@cs.cmu.edu
ABSTRACT
Publiclyavailablesource-codelibrariesarecontinuouslygrowingandchanging.
This makes it impossible for models of code to keep current with all available
APIsbysimplytrainingthesemodelsonexistingcoderepositories. Thus,exist-
ingmodelsinherentlycannotgeneralizetousingunseenfunctionsandlibraries,
becausethesewouldneverappearintheirtrainingdata. Incontrast,whenhuman
programmersusefunctionsandlibrariesforthefirsttime,theyfrequentlyrefer
to textual resources such as code manuals and documentation, to explore and
understandtheavailablefunctionality. Inspiredbythisobservation,weintroduce
DocPrompting: anatural-language-to-codegenerationapproachthatexplicitly
leveragescodedocumentationby(1)retrievingtherelevantdocumentationpieces
given a natural language (NL) intent, and (2) generating code based on the NL
intentandtheretrieveddocumentation. DocPromptingisgeneral: itcanbeap-
pliedtoanyprogramminglanguage,andisagnostictotheunderlyingneuralmodel.
WedemonstratethatDocPromptingconsistentlyimprovesNL-to-codemodels:
DocPromptingimprovesstrongbasemodelssuchasCodeT5by2.85%inpass@1
(52%relativegain)and4.39%inpass@10(30%relativegain)inexecution-based
evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset
tldr,DocPromptingimprovesCodeT5andGPT-Neo-1.3Bbyuptoabsolute

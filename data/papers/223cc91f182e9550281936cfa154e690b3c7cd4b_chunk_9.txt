∗)
]−KL(q(y∣x∗)∥p θ(y∣x∗))
y
p (x∗,y) (2.9)
θ
≤−Eq(y∣x∗) [log
q(y∣x∗) ]
=−H(q(y∣x∗))−Eq(y∣x∗)[logp θ(x∗,y)]:=L(q,θ),
where the inequality holds because KL divergence is always nonnegative. The free energy upper bound
contains two terms: the first one is the entropy of the variational distribution, which captures the intrinsic
randomness (i.e., amount of information carried by an auxiliary distribution); the second term, now written as
−Eq(y∣x∗)p~ d(x∗) [logp θ(x∗,y)], by taking into account the empirical distribution p~ d from which the
instance x∗ is drawn, is the cross entropy between the distributions q(y∣x∗)p~ d(x∗) and p θ(x∗,y),
driving the two to be close and thereby allowing q to approximate p.
7
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
The popular expectation maximization (EM) algorithm for unsupervised learning via MLE can be interpreted
as minimizing the variational free energy (Neal & Hinton, 1998). In fact, as we discuss subsequently, popular
heuristics such as the variational EM and the wake-sleep algorithms, are approximations to the EM algorithm
by introducing approximating realizations to either the free energy objective function L or to the solution
space of the variational distribution q.
Expectation Maximization (EM). The most common approach to learning with unlabeled data or partially
observed multivariate models is perhaps the EM algorithm (Dempster et al., 1977). With the use of the
variational free energy as a surrogate objective to the original marginal likelihood as in Equation 2.9, EM can
be also understood as an alternating minimization algorithm, where L(q,θ) is minimized with regard to q

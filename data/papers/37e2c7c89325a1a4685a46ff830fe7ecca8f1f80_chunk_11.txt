
L
(q ih)(cid:62)Kh, E φT(T,x)= SOFTMAX λh Tsh, (11)
i=1 h=1
wheretheteacher’sheadcoefficientsλ
T
∈(cid:52) H−1areλ
T
= NORMALIZE(φ T)withφ
T
∈RH.
4
Inthisformulation,sh ∈RLrepresents head 1 head 2 head H
theaverageunnormalizedattentionlog- k11 k21... kL1 k12 k22... kL2 k1H k2H... kLH
q1 q2 qH
itsoverallinputelements,whicharethen 1 1 1
q1 q2... qH
combinedaccordingtoλ andnormal- 2 2 2
T
izedwithSOFTMAXtoproduceadistri- q1 q2 qH
L L L
butionin(cid:52). Weapplyanormaliza-
L−1
row-wise mean row-wise mean row-wise mean
tionfunctionNORMALIZEtoheadcoeffi-
s1 × λ1 s2 × λ2 sH ×λH
cientsinvolvedtocreateaconvexcombi-
nationoverallheadsinalllayers. Inthis sum
softmax( ) ∈ △
workweconsiderthesparseprojection L-1
function NORMALIZE = SPARSEMAX Figure 2: Our parameterized attention-based explainer.
[MartinsandAstudillo,2016],as: Dashed red boxes represent learned parameters λ T =
SPARSEMAX(φ T)∈(cid:52) H−1,weightingaverageattention
SPARSEMAX(z)=argmin(cid:107)p−z(cid:107) 2. logits of each head 1 ≤ h ≤ H. A softmax over the
p∈(cid:52)H−1 weightedsumgeneratestheattentionprobabilities.
WechooseSPARSEMAXduetoitsben-
efitsintermsofinterpretability, sinceitleadstomanyheadshavingzeroweight. Wealsofound

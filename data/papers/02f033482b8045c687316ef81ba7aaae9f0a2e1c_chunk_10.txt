Large,PPLM,DAPT,andGeDi. Foreach
themeannumberofdistinctn-grams,normalized
pair of models, we randomly sample two genera-
by the length of text (Li et al., 2016), among the
tions from each model. This results in a total of
25generationsforeachprompt. WereportDist-1,
120promptsˆ4pairings ˆ2generations “ 960 com-
Dist-2,andDist-3scoresfordistinctuni-,bi-,and prompt pairing
parisons. Each comparison pair is rated by three
trigrams,respectively.
Turkers,whoselectwhichofthetwocontinuations
Results According to automatic metrics shown is: (1) less toxic, (2) more fluent, and (3) more
inTable1, DEXPERTS substantiallyoutperforms topical, i.e., whether the continuation is natural,
Figure2: Resultsofhumanevaluationfordetoxification. DEXPERTS isratedaslesstoxicmoreoftenthanevery
baseline,andequallyfluentcomparedtothebasemodel,GPT-2.
Toxicity(Ó)
Model
Avg.max.toxicity Toxicityprob.
GPT-3 0.525 0.515
DEXPERTS(large) 0.293 0.111
Table2: Resultsofexperimentsindetoxifyinggenera-
tionsfromGPT-3.
relevant, and follows logically from the prompt.
A screenshot of the user interface is provided in
Figure 3: Performance of DEXPERTS when
AppendixC.
(anti-)experts are trained on differently-sized datasets
and evaluated at different checkpoints, calculated on
Results Accordingtohumanevaluations, DEX-
a subset of 1K prompts. For comparison, recall the
PERTSisratedaslesstoxicmoreoftenthanallbase-
avg.max.toxicityofGPT-2is0.527.
lines(Figure2). Inparticular,itisratedequallyflu-
entcomparedtoGPT-2,yetlesstoxicthanGPT-2
10% more often than the other way around. See on five different dataset sizes of exactly 40,960,
App
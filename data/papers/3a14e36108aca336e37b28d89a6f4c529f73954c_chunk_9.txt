2S(r-2S)versions.GPT-2(Radford
story-level denoising. We use trainUnsup without human-
etal.2019)isaTransformer-basedlanguagemodeltrained
written reorderings, and simulate them using the original
onWebText.BART(Lewisetal.2020)andT5(Raffeletal.
human-written ROCStories (the outputs during training).
2020)areTransformerseq2seqmodels.BARTistrainedas
Deletion andswapping of tokens areused tocreate inputs
adenoisingautoencodertoreconstructoriginalfromnoised
fromthesestoriesthatsimulatenaivereorderings.Thisnois-
text.T5isdesignedtobeeffectivefortransferlearning.We
ing aims to emulate the reverse of the content editing that
useHuggingFace’simplementationsoftheirbaseversions.4
occursduringNAREOR.Specifically,werandomlydelete
12.5%oftokensandswapanother12.5%.Wefoundhuman-
3.3 AutomaticEvaluationMetrics
rewrittenstorieswere,onaverage,incombinationoftoken
length (longer) and swappings, ≈25% different from the Reference-Based Metrics assess the similarity between
generatedtextandhuman-writtenreferences.WeuseBLEU
originals. We split this between deletion and swapping to
approximatenaively-reorderedstories.StorysentencesS are (Papinenietal.2002),METEOR(BanerjeeandLavie2005),
first reordered as per π to produce S′, then each is andBERTScore(Zhangetal.2019).Wecomparegenerated
i′ naive textwiththetworeferencespertestSupexample.5
editedtofitthenewnarrative.Weswaptokensashumans
oftenswapwordslikecoreferentmentionsbasedonhowthe Target Order Fidelity (TOF) is defined as how closely
narrativeorderchanges.Hence,thisstagelearnstodenoise thereorderedtextmatchesthegiventargetnarrativeorder.
textbyconvertingnoisedversionstohuman-writtentext. E.g
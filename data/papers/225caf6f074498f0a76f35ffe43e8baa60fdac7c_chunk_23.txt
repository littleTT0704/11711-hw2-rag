oders,insteadofthesingleencoder
(Para.) was trained using a large English paraphrase
inourpreviousexperiments. Weallocatethelan-
corpusinadditiontoparalleldata.
guagesrandomlytothedifferentencoders. Wefind
thatthisdoesnâ€™timproveresults,perhapsbecause
the24layermodelhassufficientcapacitytomodel
objective. XLM-R(Para.) makesuseofa50mil-
all of the languages in one shared encoder. We
lionexampleparaphrasecorpusfordistillation. In
couldallocatelanguagestoencodersbasedonlan-
contrast, our setup most closely follows LASER,
guagefamilies,andperhapsthiscouldfarebetter,
usinganapproximationofthe220Mexamplepar-
butweleavethatforfuturework.
alleldatausedtotraintheirmodel.
Priorwork(Wietingetal.,2020)showsthat, a
7 Analysis decoder that is weaker (i.e. less layers) can lead
tostrongerembeddings. Thiseffectispresumably
7.1 ModelAblations becausethereismorepressureonthesentenceem-
beddingtofullyandclearlycapturethesemantics
In this section, we investigate different ablations
since it cannot rely on a strong decoder to fill in
of VMSST. The ablations are shown in Table 4.
gaps. We found that that using a weaker single
We start from the 24 layer randomly initialized
layer decoder (1L dec.), does indeed seem to im-
VMSST,andchangeittoseehowcertainhyper-
prove performance. We also tried a 12 layer ab-
parametersandmodelchoicesaffectperformance.
lation (12L dec.), but that seemed to not have a
Our first experiment, VMSST (fact.) investi-
significantimprovementintheresults.
gates what happens if we simply factor the final
projectionlayerofthedecoder. Thiscansavealot
13Wemultiplyby3becausewehavethreeembeddings,the
of memory in the model, as that projection layer
hiddenstate,thelanguage-specificvector
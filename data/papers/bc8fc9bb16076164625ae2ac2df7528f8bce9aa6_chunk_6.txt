Weisfeiler-Leman ingmodelcompressionandinductivebiasestransfer.Self-distillation
graphkernel[43]tomeasuregraphsimilaritybetweenthem.How- [12]isaspecialcasewhentwoarchitecturesareidentical,which
ever,theyusuallyrequirenon-trivialhand-craftedsubstructures caniterativelymodifyregularizationandreduceover-fittingifper-
and domain-specific kernel functions to measure the similarity formsuitablerounds[33].However,theyoftenfocusonclosingthe
whileyieldsinferiorperformanceondownstreamtaskslikenode gapbetweenthepredictiveresultsofstudentandteacherrather
classificationandgraphclassification.Moreover,theyoftensuf- thandefiningsimilaritylossinlatentspaceforcontrastivelearning.
ferfrompoorscalability[5]andgreatmemoryconsumption[29] Semi-supervisedLearningModernsemi-supervisedlearning
due to some procedures like path extraction and recursive sub- canbecategorizedintotwokinds:multi-tasklearningandcon-
graphconstruction.Recently,therehasbeenincreasinginterestin sistency training between two separate networks. Most widely
GraphNeuralNetwork(GNN)approachesforgraphrepresentation used semi-supervised learning methods take the form of multi-
learningandmanyGNNvariantshavebeenproposed[27,40,52]. tasklearning:argmin ğœƒLğ‘™(ğ· ğ‘™,ğœƒ)+ğ‘¤Lğ‘¢(ğ· ğ‘¢,ğœƒ)onlabeleddatağ·
ğ‘™
However,theymainlyfocusonsupervisedsettings. andunlabeleddatağ·.Byregularizingthelearningprocesswith
ğ‘¢
ContrastiveLearningModernunsupervisedlearninginthe unlabeled data, the decision boundary becomes more plausible.
form of contrastive learning can be categorized into two types: Anothermainstreamofsemi-supervisedlearningliesinintroduc-
context-instancecontrastandcontext-contextcontrast[31].The ingstudentnetworkandteachernetworkandenforcingconsis-
context-instancecontrast,orso-calledglobal-localcontrastf
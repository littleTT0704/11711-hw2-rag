 for
sensereasoninginnaturallanguage.
leveraging LLMs of code for structured
Thus, our main insight is that large language commonsensegeneration.
modelsofcodearegoodstructuredcommonsense 3. We perform an extensive evaluation across
reasoners. Further,weshowthatCode-LLMscan three structured commonsense generation
beevenbetterstructuredreasonersthanNL-LLMs, tasksanddemonstratethat COCOGEN vastly
whenconvertingthedesiredoutputgraphintoafor- outperforms NL-LLMs, either fine-tuned or
matsimilartothatobservedinthecodepre-training few-shottested,whilecontrollingforthenum-
berofdownstreamtaskexamples. take_pies_out_to_cool.
4. Weperformathoroughablationstudy,which While there are multiple ways of representing
showstheroleofdataformatting,modelsize, a training example as a Python class, we found
andthenumberoffew-shotexamples. empiricallythatthisrelativelysimpleformatisthe
mosteffective,especiallywithlargermodels. We
2 COCOGEN: Representing analyze the choice of format and its connection
Commonsensestructureswithcode withthemodelsizeinSection4.
Wefocusontasksofstructuredcommonsensegen- 2.2 Few-shotpromptingforgeneratingG
eration. Each training example for such tasks is
We focus on large-language models of the scale
intheform(T,G),whereT isatextinput,andG
of CODEX (Chenetal.,2021a). Duetotheirpro-
isthestructuretobegenerated(typicallyagraph).
hibitively expensive cost to fine-tune, these large
ThekeyideaofCOCOGENistransforminganout-
modelsaretypicallyusedinafew-shotprompting
putgraphG intoasemanticallyequivalentprogram
mode. Few-shotpromptingusesk input-outputex-
G c writteninageneral-purposeprogramminglan- amples{(x i,y i)}k i=1tocreateanin-contextprompt:
guage. In this work, we chose Python due to its p = x
:45m 18m:01s 34s 32s
Medium 7h:06m 46m:52s 1m:30s 1m:24s
Large 14h:35m 1h:37m 3m:19s 3m:01s Datasetsize Non-toxic Positive Negative
Tokens 63,457,536 13,240,192 57,805,184
Table 6: Finetuning time for (anti-)experts in DEX-
Documents 1,320,876 264,837 1,208,186
PERTS,foreachGPT-2sizeused.
Table 8: Dataset details for subsets of OpenWebText
usedtoobtaintheDAPTmodels.
DAPT ForourimplementationofDAPTinsenti-
mentexperiments(§4),weuseHuggingFace’ssen-
timentanalysisclassifiertofilterdocumentsfrom Datasetsize Non-toxic Toxic
OpenWebText()forthemostpositive2%andmost
Tokens 91,856,000 10,262,144
negative2%ofdocuments. Becausetheclassifier Comments 1,401,762 159,782
takesamaximumof512tokensasinputtext,we
Table9: Datasetdetailsfortoxicity(anti-)experts.
approximatethesentimentofadocumentwithits
first510tokens(astartandendtokenareaddedby
the classifier). The hyperparameters for the addi-
Datasetsize Positive Negative
tionalphaseofpretrainingontheattributedatais
Tokens 116,480 108,800
giveninTable5.
Moviereviews 4,963 4,650
PPLM ForourimplementationofPPLMinex-
Table10: Datasetdetailsforsentiment(anti-)experts.
periments, we retrain the toxicity and sentiment
classifiers to be compatible with our base model
GPT-2 (large), as the original paper used GPT-2 A.4 GenerationDetails
medium for experiments. We use the same train-
Generation hyperparameters shared among all
ingdatasetsandhyperparametersasintheoriginal
methods are shown in Table 11. Hyperparame-
PPLMpaper.
ters for P
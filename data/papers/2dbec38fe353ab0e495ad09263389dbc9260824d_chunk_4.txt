(§3.2) E.g.,GTS(XieandSun,2019),Graph2Tree(Lietal.,2020b)
NeuralNetworks(§3)
Attention-based(§3.3) E.g.,Math-EN(Wangetal.,2018a),GROUP-ATT(Lietal.,2019)
Other(§3.4) E.g.,CNNTP(Loosetal.,2017),MathDQN(Wangetal.,2018b)
DeepLearning
Self-SupervisedLearning(§4.1) E.g.,GenBERT(Gevaetal.,2020),Minerva(Lewkowyczetal.,2022)
Methods Pre-trainedLanguage
Models(§4)
Task-specificFine-tuning(§4.2) E.g.,Scratchpad(Nyeetal.,2021),Bhaskara(Mishraetal.,2022a)
ExampleSelection(§5.1) E.g.,Few-shot-CoT(Weietal.,2022),PromptPG(Luetal.,2022b)
In-contextLearning(§5)
High-qualityChains(§5.2) E.g.,Self-Consistency(Wangetal.,2023),Least-to-most(Zhouetal.,2023)
Figure1: Taxonomyofdeeplearningformathematicalreasoning. Theassociatedtasksareelaboratedin§2,witha
comprehensivedatasetlistfoundin§A.Deeplearningmethodsarefurtherdiscussedin§3,§4,and§5.
(§3),pre-trainedlanguagemodels(§4),andrecent Question:Bodhas2applesandDavidhas5apples.
in-contextlearningforlargelanguagemodels(§5). Howmanyapplesdotheyhaveintotal?
We also analyze existing benchmarks and find Rationale:x=2+5
that there is less focus on multi-modal and low- Solution:7
resourcesettings(§6.1). Ourevidence-basedstud- Table1: Atypicalmathwordproblem.
iessuggestthatcurrentnumeracyrepresentations
areinsufficientanddeeplearningmethodsarein
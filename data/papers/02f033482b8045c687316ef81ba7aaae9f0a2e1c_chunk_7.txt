hesamedomainofcomments
as the toxic anti-expert, providing more effective Generativediscriminators(GeDi;Krauseetal.,
contrast. Nonetheless,weprovideanablationusing 2020) GeDiusesaclass-conditionedLMtopro-
only a toxic anti-expert and show that it remains videclassificationprobabilitiesforallpossiblenext
effectiveaboveallpreviousbaselines. tokensviaBayes’rule. Weusethetoxicityclass-
conditioned LM released by the authors with the
3.1 Method
recommendedgenerationhyperparameters.
WeuseGPT-2LargeasourbaseLM.Forourexpert
andanti-expert,wefinetuneseveralsizesofGPT-2
DEXPERTS(anti-only) Wealsoexploreananti-
(Small, Medium, Large) on a dataset of human-
expert-onlyablationof DEXPERTS,byreusingthe
basemodelastheexpert. Tobeclear,wesubstitute
annotatedcommentsfromtheJigsawUnintended
BiasinToxicityClassificationKagglechallenge.3 z` t “ z t inEquation1,sothatwehave
` ˘
We consider an example toxic if ě 50% of anno- P˜ pX | x q “ softmax p1`αqz ´αz´ (6)
t ăt t t
tators marked itas toxic, andnontoxic if none of
4https://github.com/conversationai/
theannotatorsmarkitastoxic. Thistoxicdataset
perspectiveapi
3https://bit.ly/3cvG5py 5https://bit.ly/3yQiCIo
Toxicity(Ó) Fluency(Ó) Diversity(Ò)
Model
Avg.max.toxicity Toxicityprob. Outputppl. Dist-1 Dist-2 Dist-3
GPT-2 0.527 0.520 25.45 0.58 0.85 0.85
PPLM(10%) 0.520 0.518 32.58 0.58 0.86 0.86
Non-toxicexpert 0.485 0
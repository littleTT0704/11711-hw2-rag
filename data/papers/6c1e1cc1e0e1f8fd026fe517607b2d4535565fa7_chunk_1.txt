PAL: Program-aided Language Models
LuyuGao*1 AmanMadaan*1 ShuyanZhou*1 UriAlon1 PengfeiLiu12 YimingYang1 JamieCallan1
GrahamNeubig12
{luyug,amadaan,shuyanzh,ualon,pliu3,yiming,callan,gneubig}@cs.cmu.edu
Abstract 1.Introduction
Untilasrecentlyastwoyearsago,reasoningwasconsidered
Large language models (LLMs) have recently tobeoneofthemostsignificantchallengesthatlargelan-
demonstrated an impressive ability to perform guagemodels(LLMs)hadnotyetovercome(Marcus,2018;
arithmetic and symbolic reasoning tasks, when 2020;Garcez&Lamb,2020). Recently,LLMshaveshown
providedwithafewexamplesattesttime(“few- impressivesuccessonawiderangeoftasks,includingcom-
shot prompting”). Much of this success can be monsense (Wei et al., 2021; Sanh et al., 2021; Madaan
attributedtopromptingmethodssuchas“chain- etal.,2022), mathematical(Lewkowyczetal.,2022;Wu
of-thought”,whichemployLLMsforbothunder- et al., 2022; Mishra et al., 2022), and symbolic reason-
standingtheproblemdescriptionbydecomposing ing (Yao et al., 2022; Ahn et al., 2022), using few-shot
it intosteps, as well as solving each stepof the prompting(Brownetal.,2020).
problem. WhileLLMsseemtobeadeptatthis
Thisprocesshasbeenacceleratedbymethodsthatrequire
sortofstep-by-stepdecomposition,LLMsoften
LLMs to generate their explicit reasoning steps, such as
makelogicalandarithmeticmistakesinthesolu-
“chain-of-thought”(Weietal.,2022),“scratchpads”(Nye
tionpart,evenwhentheproblemisdecomposed
etal.,2021),and“least-to-most”(Zhouetal
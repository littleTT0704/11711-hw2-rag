 than
y
L g =−log (cid:0) (cid:1) (cid:80) (3) zeroinsomerangeofθ y ∈[0,θ].Forexample,∆(θ y)forAr-
exp (cid:107)x(cid:107)ψ(θ ) + exp((cid:107)x(cid:107)η(θ ))
y i(cid:54)=y i cFacecanbesmallerthanzerowhenθ yisclosetoπ.ArcFace
whereψ(θ y)istheangularactivationfunctionforthetarget canstillintroduceangularmarginbecausethecasewhereθ
y
class (i.e., ground truth label) and η(θ i),i (cid:54)= y denotes is close to π hardly happens with real data distribution, as
the angular activation function for the i-th non-target class verified by [5]. Nonetheless, the characteristic function for
(the labels excluding the ground truth one). Similar to the ArcFace still approximately satisfies our principle, since it
cosine function, both ψ(θ) and η(θ) are generally required is larger than zero with most θ y ∈ [0,θ]. Therefore, as long
tobemonotonicallydecreasingforθ ∈ [0,π].Afterlooking as the characteristic function ∆(θ y) is larger than zero for
into different hyperspherical FR methods, we summarize a the angles where θ y is densely distributed in practice (i.e.,
simpleyetgenericprincipleforanysoftmaxlossinorderto E θy∆(θ y) > 0), it will typically suffice to produce effective
learnembeddingswithlargeangularmargin. angular margin. Our principle in fact serves as a sufficient
conditiontointroduceangularmargin.Itisgenerallybetter
Toachievelargeangularmargin,thegenericprincipleis to use our principle as the guideline for designing new
tomakeψ(θ)alwayssmallerthanη(θ)in(0,π],namely angular margin losses, because the empirical distribution
of the target angle θ y could vary under difference circum-
∆
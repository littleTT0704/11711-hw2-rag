. The first image shows our model misses
on the real test set of VIRAT/ActEV. As we see from the
thecorrectdirection,perhapsduetolackofdiversityinour
right column in Table 3, all models do worse in this sce-
sampling procedure. The second image shows our model
nario,duetothedifferencebetweensyntheticandrealdata.
sometimes predicts the person will “go through” the car
We find the performance ranking of different methods are
(diagonal red beam) instead of going around it. This may
consistentbetweentherealandoursimulationtrainingdata.
be addressed by adding more training examples of “going
Thissuggeststheerrorsmainlycomingfromthemodel,and
around” obstacles. The third image shows our model pre-
substantiatestherationalityofusingtheproposeddatasetto
dictsthepersonwillgotoamovingcar. Thisisduetothe
comparetherelativeperformanceofdifferentmethods.
lackofmodelingofthedynamicsofotherfar-awayagents
Therearetwosourcesoferror. Thesynthetictrajectory
in the scene. The fourth image shows a hard case where
dataonlycontainsabout60%oftherealtrajectorydata,due
thepersonjustexitsthevehicleandthereisnoindicationof
todifficultiesreconstructingalltherealdatainthesimula-
wheretheywillgonext(soourmodel“backsoff”toasen-
tor. In addition, the synthetic images are not photo realis-
sible“staynearby”prediction). Weleavesolutionstothese
tic. Thus methods (such as Next [30]) that rely on RGB
problemstofuturework.
inputobviouslysufferthemost,sincetheyhaveneverbeen
trained on “real pixels”. Our method, which uses trajecto-
5.3.Single-FuturePredictiononVIRAT/ActEV
riesplushighlevelsemanticsegmentations(whichtransfers
Dataset & Setups. NIST released VIRAT/ActEV [3] for fromsynthetictorealmoreeasily)sufferstheleastdropin
activity detection
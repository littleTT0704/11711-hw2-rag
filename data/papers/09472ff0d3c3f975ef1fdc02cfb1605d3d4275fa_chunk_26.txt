 also exists in the data.
Thefollow-updiscussionaimstoshowthatRSCcanforcethemodeltolearn
multiple signals, so that it helps in cross-domain generalization.
Further,AssumptionA2canbeinterpretedasthereisatleastsomefeatures
z that appear in every distributions we consider. We use i to index this set
of features. Assumption A2 also suggests that z is i.i.d. (otherwise there will
i
not exist θ(cid:63)) across all the distributions of interest (but z is not i.i.d. because
z, where −i denotes the indices other than i, can be sampled from arbitrary
−i
distributions).
e.g., z is the image; z is the ingredients of the true concept of a “cat”, such as
i
ears, paws, and furs; z is other features such as “sitting by the window”.
−i
We use O to specify the distribution that has values on the ith, but 0s else-
where. We introduce the next assumption:
A3: Samplesofanydistributionofinterest(denotedasA)areperturbedversion
ofsamplesfromObysamplingarbitraryfeaturesforz :E [E [z]]=E [z]
−i A S O
18 Huang et al.
Notice that this does not contradict with our cross-domain set-up: while
Assumption A3 implies that data from any distribution of interest is i.i.d (oth-
erwisetheoperationE []isnotvalid),thecross-domaindifficultyisraisedwhen
A
only different subsets of A are used for train and test. For example, considering
A to be a uniform distribution of [0,1], while the train set is uniformly sampled
from [0,0.5] and the test set is uniformly sampled from (0.5,1].
A2 Proof of Theoretical Results
A2.1 Corollary 1
Proof. Wefirststudytheconvergencepart,whereweconsiderafixedhypothesis.
We first expand
|L(θ(cid:98)RSC(S);S)−L(θ R(cid:63) SC(
enguins
have<mask>wings,becauseitturnstheproblem kˆ = k mˆ wheremˆ = argmaxmaxp I(a|q m).
into deductive reasoning. Meanwhile, Penguins 0≤m≤M a∈Aq
havetwowings. wouldbeapoorknowledgestate- The inference model may be any existing lan-
menttodemonstrateaccordingtoourguideline. guagemodeltakenoff-the-shelf(i.e. zero-shot)or
Whengeneratingknowledgeforanewquestion finetuned on the task. We do not do any further
q, we plug the question into the placeholder, and finetuningwithknowledgeprompting.
repeatedlysamplegeneratedcontinuationsofthis
3 ExperimentalSetup
prompt to obtain a set of knowledge statements
K = {k,k,...,k }. For full prompts on all Here, we describe the implementation details of
q 1 2 M
thetasksweevaluateon,seeAppendixA.2. ourmethodandhowtheyareadaptedtoeachtask.
For knowledge generation, we use GPT-3
2.2 KnowledgeIntegrationviaPrompting (Brown et al., 2020) as the underlying language
model, where our few-shot prompting method is
In the knowledge integration step, we use a lan-
most effective. We generate M = 20 knowledge
guage model – called the inference model – to
statementsforeachquestionwithnucleussampling
makepredictionswitheachgeneratedknowledge
p = 0.5(Holtzmanetal.,2019),anddiscardrepe-
statement,thenselectthehighest-confidencepre-
titionsandemptystrings. Generationisterminated
diction. Specifically,weuseeachknowledgestate-
whenitexceeds64tokensorhitsthe\ntoken.1
menttopromptthemodel,formingM knowledge-
For inference, we use off-the-shelf T5 (Raffel
augmentedquestions:
etal.,2019)andGPT-3,aswellasfinetunedmodels
thatarestate-of-the-artoneachdataset,including
q = q,q = [k ||q],...,q = [k ||q]

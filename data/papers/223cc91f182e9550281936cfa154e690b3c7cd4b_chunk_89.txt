77–1901. ↩
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P.,
Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
Ziegler, D. M., Wu, J., Winter, C.,... Amodei, D. (2020). Language models are few-shot learners. In H.
Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, & H. Lin (Eds.), Proceedings of the 34th International
Conference on Neural Information Processing Systems (Vol. 33; 1877–1901). Curran Associates Inc.
https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
↩
Chu, C., Blanchet, J., & Glynn, P. (2019). Probability functional descent: A unifying perspective on GANs,
variational inference, and reinforcement learning. In K. Chaudhuri & R. Salakhutdinov (Eds.), Proceedings
of the 36th International Conference on Machine Learning (Vol. 97; 1213–1222).
http://proceedings.mlr.press/v97/chu19a.html
62
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
↩
Csiszár, I. (1975). I-divergence geometry of probability distributions and minimization problems. The
Annals of Probability, 3(1), 146–158. https://doi.org/10.1214/aop/1176996454 ↩
Dagan, I., & Engelson, S. P. (1995). Committee-based sampling for training probabilistic classifiers. In A.
Prieditis & S. Russell (Eds.), Proceedings of the Twelfth International Conference on International
Conference on Machine Learning (pp. 150–157). https://doi.org
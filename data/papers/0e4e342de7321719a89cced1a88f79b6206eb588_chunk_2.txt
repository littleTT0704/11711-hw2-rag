ationintothegenerationofconvolutionalkernels. sualfeatures. (c)removesthefusionmoduleasquery-awarevisual
With our proposed QCM, the downstream fusion featuresareinformativeenoughtobeuseddirectlyforprediction.
module receives visual features that are more dis-
criminative and focused on the desired object de-
AsshowninFigure1(a),mostpreviousmethods[Chenet
scribedintheexpression,leadingtomoreaccurate
al.,2018;Liaoetal.,2020;Duetal.,2021;Yangetal.,2020;
predictions. Extensiveexperimentsonthreepopu-
Deng et al., 2021] follow the extract-and-fuse pipeline that
lar visual grounding datasets demonstrate that our
extracts visual and textual features independently using two
method achieves state-of-the-art performance. In
isolated encoders followed by an additional fusion module
addition,thequery-awarevisualfeaturesareinfor-
to enable multi-modal feature interaction. As the visual en-
mativeenoughtoachievecomparableperformance
coderisunawareofthetextqueryduringfeatureextraction,
tothelatestmethodswhendirectlyusedforpredic-
it only encodes visual features based on intra-image infor-
tionwithoutfurthermulti-modalfusion.
mation,neglectingthepotentialcorrespondencebetweentext
query and target image. Consequently, the extracted visual
features may not convey the required information described
1 Introduction
bythetextquery. Inthiscase,thevisualencoderislikelyto
VisualGrounding,alsoknownasReferringExpressionCom- passredundantorevenmisleadingvisualfeaturestothefol-
prehension [Mao et al., 2016; Yu et al., 2016], is a task lowingfusionmoduleandthuscompromisetheperformance.
of locating a target object in an image according to a nat- Intuitively, the extract-and-fuse pipeline is similar to asking
ural language query. It requires the machine to understand humans to first memorize an image and then recall it while
both textual and visual information, as well as the rela- locatinganobjectgivenanaturallanguagedescription. The
tionship between them. Pioneer works
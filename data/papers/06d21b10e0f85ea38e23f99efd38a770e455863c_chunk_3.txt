al.,2021),andasrewardfunc-
the-art multilingual model, on mining bitext and
tionsorevaluationmetricsforlanguagegeneration
hasstrongerperformanceoncross-lingualseman-
1Code, including an easy to install PyPi package, re- tic similarity, while having inference speeds that
leased models including Hugging Face implementations,
aretwiceasfastonGPUandordersofmagnitude
demo,anddataareavailableathttps://github.com/jwieting/
paraphrastic-representations-at-scale. fasteronCPU.
379
ProceedingsoftheThe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages379-388
December7-11,2022(cid:13)c2022AssociationforComputationalLinguistics
Wemakeseveralcontributionsinthispaperthat 2 RelatedWork
gobeyondourpriorwork. Firstly,wereformatthe
codetosupporttrainingmodelsontensofmillions
2.1 EnglishSemanticSimilarity
ofsentencepairsefficientlyandwithlowRAMus-
age. Secondly,wetrainanEnglishmodelon25.85
Ourlearningandevaluationsettingisthesameas
millionparaphrasepairsfromParaNMT(Wieting
that of our earlier work that seeks to learn para-
andGimpel,2018),aparaphrasecorpusweprevi-
phrasticsentenceembeddingsthatcanbeusedfor
ously constructed automatically from bitext. We
downstreamtasks(Wietingetal.,2016b,a;Wieting
thentrainmodelsdirectlyonX-Englishbitextfor
and Gimpel, 2017; Wieting et al., 2017; Wieting
Arabic,German,Spanish,French,Russian,Turk-
and Gimpel, 2018). We trained models on noisy
ish,andChinese,producingmodelsthatareableto
paraphrasepairsandevaluatedthemprimarilyon
distinguish both paraphrases in English and their
semantic textual similarity (STS) tasks. More re-
respective languages as well as cross-lingual X-
cently,wemadeuseofparallelbite

TotestthehypothesisthatstandardLMtraininglossdonotemphasizetheexampleswherebaseLMperforms
badly,wetraintheextramodel’sparameterW,withinterpolatedlossL:
ds
L=CrossEntropy(λsoftmax(W ·h )+(1−λ)softmax(W ·h ),y) (12)
ds ds sm sm
y represents the ground truth label for each context. We only learn the parameter W while freezing all
ds
otherparameters,similartoallotherexperiments. Wechooseλ=0.25asitisthebesthyper-parameterfor
kNN-LMexperimentsandourgoalforthistrainingistomimicthelossofkNN-LMafterinterpolation. This
traininglosseffectivelyassignsahighervaluetothetrainingexampleswherethebaseLM’slossishigh,
20
suggestingtheneedfortheextraW tohelpwiththesehardcases. However,foreither“att”for“ffn”forh,
ds ds
eitherV or3V forthenumberofembeddingsinW,weareunabletoachieveabetterperplexitythanjust
ds
thebaseLM.Thissuggeststhat,whileniceonpaper,theinterpolatedlossoptimizationprocessisnottrivial.
21

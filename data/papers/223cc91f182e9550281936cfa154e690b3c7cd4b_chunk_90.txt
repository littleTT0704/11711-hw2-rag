/10.1016/B978-1-55860-377-6.50027-X
↩
Dayan, P., & Hinton, G. E. (1997). Using expectation-maximization for reinforcement learning. Neural
Computation, 9(2), 271–278. https://doi.org/10.1162/neco.1997.9.2.271 ↩
Deisenroth, M. P., Neumann, G., & Peters, J. (2013). A survey on policy search for robotics. Foundations
and Trends in Robotics, 2(1–2), 1–142. https://doi.org/10.1561/2300000021
↩
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1), 1–22.
https://doi.org/10.1111/j.2517-6161.1977.tb01600.x
↩
Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E. P., & Hu, Z. (2022).
RLPrompt: Optimizing discrete text prompts with reinforcement learning. Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing. ↩
Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). Bert: Pre-training of deep bidirectional
transformers for language understanding. In J. Burstein, C. Doran, & T. Solorio (Eds), Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers) (pp. 4171–4186). https://aclanthology.org/N19-
1423.pdf
↩
Domingos, P. (2015). The master algorithm: How the quest for the ultimate learning machine will remake
our world. Basic Books. ↩
Duane, S., Kennedy, A. D., Pendleton, B. J., & Roweth, D. (1987
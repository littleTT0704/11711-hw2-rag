componentsofthisworkownedbyothersthanACM consideringarbitraryamountsofpositivepairsbelongingtothe
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish, sameclass.Moreover,toleveragetheinformationfrompseudo-
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee.Requestpermissionsfrompermissions@acm.org. labelswithhighconfidence,wedevelopaself-trainingalgorithm
Ljubljana’21,April19–23,2021,Ljubljana,Slovenia basedonthesupervisedcontrastivelossforfine-tuning.
©2021AssociationforComputingMachinery.
3202
naJ
3
]GL.sc[
3v90621.0102:viXra
Ljubljana’21,April19–23,2021,Ljubljana,Slovenia Zhangetal.
Weexperimentwithreal-worlddatasetsinvariousscalesand lineofcontrastivelearningapproachescalledinstancediscrimina-
comparetheperformanceofIGSDwithstate-of-the-artgraphrep- tion(orcontext-contextcontrast)directlystudytherelationships
resentationlearningmethods.ExperimentalresultsshowthatIGSD betweentheglobalrepresentationsofdifferentsamplesaswhat
achievescompetitiveperformanceinbothself-supervisedandsemi- metriclearningdoes.Forinstance,ourmethodisinspiredbyBYOL
supervisedsettingswithdifferentencodersanddataaugmentation [15],amethodbootstrapstherepresentationsofthewholeimages
choices.Withthehelpofself-training,ourperformancecanexceed directly.AndGraphBarlowTwins[4]utilizesacross-correlation-
state-of-the-artbaselinesbyalargemargin. basedlossinsteadofnegativesamplesforlearningrepresentations.
Insummary,wemakethefollowingcontributionsinthispaper: Focusingonglobalrepresentationsbetweensamplesandcorre-
spondingaugmentedviewsalsoallowsinstance-levelsupervision
• Weproposeas
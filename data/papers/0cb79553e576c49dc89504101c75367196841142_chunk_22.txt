,and
languagemodelforsentimenttransfer. InProceed- findtheonlyindividualsmentionedinJigsaware
ings of the Twenty-Eighth International Joint Con-
high-profilepoliticalfigureswhoarealreadywell-
ference on Artificial Intelligence, IJCAI-19, pages
known. Wedonotperformadditionalanonymiza-
5271–5277.InternationalJointConferencesonArti-
ficialIntelligenceOrganization. tionofthedata.
Expert We finetune the expert with the hyper- followtheintendeduseofallthreedatasetsbyus-
parameters listed in Table 3, using two NVIDIA ingthemonlytorewritetoxicsentences.
RTX6000 GPUs. We select the best checkpoint, We also preprocess each of the datasets in the
basedonthelowestevaluationloss,whichisatstep same way. We use the re package built-in to
100,000. Thetotaltrainingtimeis20hours,for40 Python(weuseversion3.8.11)toremoveanyex-
GPUhoursofusage. tendedwhitespace,includingtabsandlinebreaks,
and convert them to one space. We use the html
Hyperparameter Assignment package, also built-in to our Python version, to
convert named html character references to their
model BART-base
numberofgpus 2 corresponding string, such as “&gt;” to ‘’>”. Af-
effectivebatchsize 48
terwards,weusetheftfypackage,version6.1.1,
totalsteps 100,000
stepsperevaluation 1000 foundathttps://pypi.org/project/ftfy/tofixbroken
learningrateoptimizer AdamW unicodeintext. Finally,weremoveanyverylong
AdamWinitiallearningrate 2.5e-06
sequences: wecalculatethe90%percentileoftext
AdamWepsilon 1e-06
learningrateschedule linearwithnowarmup lengthstobe44, wheretextlengthisthenumber
weightdecay 0.0 ofspace-delimitedwords,andweremoveanyse-
maxsequencelength 180
quenceslongerthanthis.
maxgenerationlength 230
paddingsequences tomaxseq
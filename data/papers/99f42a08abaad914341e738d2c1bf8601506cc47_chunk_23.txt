 to these samples. Therefore, SphereFace-R
θ m
v1 and SphereFace-R v2 put different efforts on optimizing
2.Theinequalitycos(θ1)>cos(mθ1)holdsifθ1∈[0, mπ],m>1. hardsamplesandmayyielddifferentgeneralizability.
IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 7
To the best of our knowledge, SphereFace-R v2 is the 0.7
very first method that introduces large angular margin 0.6 Original ()
throughnon-targetangularfunctions.SphereFace-Rv2eas- in the forward pass 0.5
ily addresses the difficult problem of incorporating a static (0)
Step approximation of ()
0.4
multiplicativemarginbysimplyswitchingthedesignfocus in the backward pass 0.3
fromtargetfunctiontonon-targetfunction.Webelievethat () 0.2
CGD of () ()
this method provides an important and novel perspective 0.1 CGD of ()
ondesigninglargeangularmarginlosses. 0
0−δ 0 0+δ
Angle
0 0.5 1 A1 n. g5
le
2 2.5 3
4.1.3 CharacteristicGradientDetachment (a) Illustration of CGD (b) CGD for SphereFace
In order to further stabilize training and improve per-
0.6 0.6
formance, we introduce a simple and generic method –
0.5 0.5
characteristic gradient detachment for implementing our
0.4 0.4
multiplicative margin. In general, the shape of the char-
acteristic function ∆(θ) determines the training stability 0.3 0.3
()
and the convergence property. Empirically, we find that 0.2 0.2 CGD of ()
()
a characteristic function with simpler backward gradient 0.1 CGD of () 0.1
computation typically leads to better training stability. For 0 0
0 0.5 1 1.5 2 2.5 3 0 0.5 1 1.5 2 2.5 3
example, CosFace [3], [4] yields strong empirical training Angle Angle
(c) CGD for
has shown promising results (Rajagopal et al.,
We demonstrate our approach across three di-
versestructuredcommonsensereasoningtasks. 2021; Madaan and Yang, 2021), LLMs struggle
In all these natural language tasks, we show to generate these “unnatural” outputs: LMs are
that using our approach, a code generation primarilypre-trainedonfree-formtext,andthese
LM (CODEX) outperforms natural-LMs that serializedstructuredoutputsstronglydivergefrom
arefine-tunedonthetargettask(e.g., T5)and
the majority of the pre-training data. Further, for
other strong LMs such as GPT-3 in the few-
naturallanguage,semanticallyrelevantwordsare
shot setting. Our code and data are avail-
able at https://github.com/madaan/ typicallyfoundwithinasmallspan,whereasneigh-
CoCoGen. boring nodes in a graph might be pushed farther
apartwhenrepresentingagraphasaflatstring.
1 Introduction
Thus, a language model which was trained on
The growing capabilities of large pre-trained lan- natural language text is likely to fail to capture
guagemodels(LLMs)forgeneratingtexthaveen- the topology of the graph. Consequently, using
abled their successful application in a variety of LLMs for graph generation typically requires a
tasks, including summarization, translation, and large amount of task-specific training data, and
question-answering(Wangetal.,2019;Raffeletal., theirgeneratedoutputsshowstructuralerrorsand
2019;Brownetal.,2020;Chowdheryetal.,2022). semantic inconsistencies, which need to be fur-
Nevertheless,whileemployingLLMsfornatu- therfixedeithermanuallyorbyusingasecondary
ral language (NL) tasks is straightforward, a ma- downstreammodel(Madaanetal.,2021b).
jorremainingchallengeishowtoleverageLLMs Despite these struggles, the recent success of
forstructuredcommonsensereasoning,including large-languagemodelsofcode(Code-LLMs; Chen
taskssuchasgeneratingeventgraphs(Tandonetal., et al., 2021b;
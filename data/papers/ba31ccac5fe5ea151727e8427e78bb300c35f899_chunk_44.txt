canbecalculated17 inamodifiedway(Tsuboi
Forbrevity,intheremaining,weuselogZ(x)to etal.,2008).
denotethesecondtermofthelogpartitionfunction.
Knowledge distillation. As described in the
Formodeltraining,weneedtocalculatethegradi-
maincontext,weadopttheknowledgedistillation
entsofthemodelparametersθ tothelossfunction.
objective for self-training with soft labels. For
Thefirstitemiseasytodealwithsinceitonlyin-
brevity, we denote the probabilities from the last
volvesonestructuredobject,whilelogZ(x)needs
modelasp′(y|x)andkeepusingp(y|x)todenote
somereorganizationaccordingtothefactorization:
theonesfromthecurrentmodel. FollowingWang
(cid:80) y′∈Yexps(y′|x)∇ θs(y′|x)
etal.(2021),thelosscanbecalculatedby:
∇ logZ =
θ (cid:80) exps(y′′|x)
y′′∈Y (cid:88)
(cid:88) L = − p′(y|x)logp(y|x)
= p(y′|x)∇ s(y′|x)
θ y∈Y
y′∈Y (cid:88)
= − p′(y|x)s(y|x)+logZ(x)
(cid:88) (cid:88)
= p(y′|x) ∇ s(f′|x)
θ y∈Y
y′∈Y f′∈y′ (cid:88) (cid:88)
= − p′(y|x) s(f′|x)+logZ(x)
(cid:88) (cid:88)
= ∇ s(f′|x) p(y′|x)
θ y∈Y f′∈y′
f′ y′∈Y f′ = −(cid:
qiaoYang1,JingluWang3,RitaSingh1,BhikshaRaj1,4
Figure4:Thenormalizederrorsand𝐶𝐼sof24AMson(a)malesubset,(b)femalesubset,and(c)asmallerfemalesubset.If1−𝐶𝐼
𝑢
>0,
theAMispredictableelseunpredictable.
4.1 Dataset 4.2 ImplementationDetails
WeperformexperimentsonaprivateaudiovisualdatasetD.The We leverage a backbone E to learn voice code𝑒 which is a sim-
datasetconsistsofpairedvoicerecordingsandscanned3Dfacial pleconvolutionalneuralnetwork.Thedetailednetworkstructure
shapesfrom1,026people,with364malesand662females.The ispresentedinthesupplementarymaterials.𝐹 𝑘 and𝐺 𝑘 sharethe
scanned3Dfaceisstoredinthemeshformatwith6790pointsfor backbone’slearnableparametersbuthaveindividualparametersfor
eachface.Thevoicerecordingsareabout2minuteslongforeach theirheads.Weuseasinglelayerfully-connectednetworkforeach
speaker. We reduce the influencing factors to the voice and face head.Forthevariancehead,weaddanexponentialactivationto
by(1)askingparticipantstospeakasetofspecifiedsentences,(2) thelastlayerof𝐺 𝑘 fornon-negativepositiveoutput.Wefollowthe
askingparticipantstospeakwithoutemotion,(3)controltheageof typicalsettingsofstochasticgradientdescent(SGD)foroptimiza-
participants(roughly18-28yearsold).Inaddition,topreventthe tion.Minibatchsizeis64.Themomentum,learningrate,andweight
modelsfromtakingthegendershortcuts,wesplitthedatasetDby decayvaluesare0.9,0.1,and0.0005,respectively.Thetrainingis
gender,andexperimentsareindividuallyperformedonmaleandfe- completedat5kiter
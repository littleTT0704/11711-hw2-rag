’sρ×100isthe
secondcolumn.
Model NQ
CONTRASTIVE 40.2
BITRANSLATION 40.9
VMSSTCONTRASTIVE 40.3
VMSST 40.8
Table11: FullresultsonquestionretrievalontheNQ
data. Weevaluateretrievalaccuracy×100usingPAQ
asaquestionknowledgebase.
The only deviation from their data sampling
approach is that we take care to not include any
Tatoeba test data in our training data. Our final
corpus has nearly 216 million training examples,
slightly less than 220 million reported in Artetxe
and Schwenk (2019b). We use both English and
Spanishaspivotlanguages,soeachpairincludesat
leastoneEnglishorSpanishsentence,andattempt
touseapproximatelythesameamountofdatafor
each language if possible. We note that we only
havetrainingdatafor92languagesinsteadofthe
93inArtetxeandSchwenk(2019b)duetonothav-
ingtrainingdataforAymara(ay). Thefullamount
ofEnglishandSpanishparalleldatausedforeach
ofthe92languagesisreportedinTable15.
Model MKQA
Language ar da de en es fi fr he hu it ja km
CONTRASTIVE 21.4 30.4 29.2 33.2 30.4 27.7 30.0 24.4 26.9 29.4 24.2 23.6
BITRANSLATION 19.4 30.8 30.5 29.8 28.7 29.7 28.0 30.3 27.5 27.9 26.2 23.4
VMSSTCONTRASTIVE 24.6 32.0 30.5 33.4 31.6 29.8 30.9 27.9 28.9 31.0 27.3 24.9
VMSST 21.4 32.1 32.5 31.5 30.3 31.3 30.0 31.9 29.9 30.0 29.9 25.9
Language ko ms nl no pl pt ru sv th tr vi zh
CONTRASTIVE 22.0 30.5 29.4 33.2 30.4 27.7 30.0 24.8 27.5 29.6 24.
sentences,e.g.“bodyofwateron
2.2 Models:T5andBART araft”.Othersmissimportantwords,e.g.“Alisteningmusic
WeusepretrainedtextgenerationmodelsT5andBART,both anddancinginadarkroom”missesanounbeforelistening.
thebaseandlargeversions.Bothareseq2seqTransformer A large portion of generations are generic and bland, e.g.
models.T5hasstrongmultitaskpretraining.BARTispre- “Someonesitsandlistenstosomeonetalk”.Thismaybean
trainedasadenoisingautoencodertoreproduceoriginalfrom instance of the dull response problem faced by generation
noisedtext.WeusetheirHuggingFaceimplementations. models(DuandBlack2019;Lietal.2016),wheretheyprefer
Wetraintwoseededversionsofeachmodelontrain safeandfrequentresponsesindependentoftheinput.
CG
andevaluatetheirperformanceondev.Theseserveasthe Manygenerationsalsolackcommonsense.Forexample,
O
baselinesforourexperiments.UsingthenumbersinLinetal. “bodyofwateronaraft”isillogicalasthephrases“bodyof
(2020)ascomparison,wevalidateourimplementations.We water”and“araft”arepiecedtogetherincorrectly.Asimilar
usethehyperparametersfromLinetal.(2020),beamsearch issue occurs with the {horse, carriage, draw} example in
fordecoding,andselectthefinalepochastheonereaching Table 4. At times the models also cannot understand what
maximumROUGE-2(LinandHovy2003)onthedevsplit. certainnounscando,e.g.“Adogcheckinghisphoneona
FromTable3,weobservethatourre-implementationsreach pier.”SeveralotherexamplesofthiscanbefoundinTable4.
orexceedreportedresultsinLinetal.(2020)onmostmetrics.
3.2 ImagesandCaptions
2.3 EvaluationMetrics
Imagesthatrepresenteverydayscenariosarequiteprevalent
Weuses
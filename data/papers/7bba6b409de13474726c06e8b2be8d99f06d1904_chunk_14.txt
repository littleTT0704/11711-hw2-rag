vocabulary (OOV) token. From a cognitive perspec-
thecontextcontainslotsoferrorsthenexpecationsarediffi- tive, this corresponds to assuming that all unknown words
culttocomputeandtheybecomeunreliable(resultinginhigh (whether they contain errors or not) are treated in the same
surprisal). We will now test these predictions regarding er- way: theyarerecognizedasunknown,butnotprocessedany
rortypeanderrorrateusingacharacter-basedversionofsur- further. Weusedavocabularysizeof10,000. Thehyperpa-
prisal. rametersoftheword-basedmodelwereselectedonthesame
EnglishWikipediacorpusasthecharacter-basedmodel.6
Methods
We trained a character-based neural language model using ResultsandDiscussion
LSTMcells(Hochreiter&Schmidhuber, 1997). Suchmod-
In this section, we show that surprisal computed by a
elscanassignprobabilitiestoanysequenceofcharacters,and
character-levelneurallanguagemodel(CHARSURPRISAL)is
thusarecapableofcomputingsurprisalevenforwordsnever
abletoaccountfortheeffectsoferrorsonreadingobservedin
seeninthetrainingdata,suchaserroneouswords. Fortrain-
our eye-tracking experiments. We compute character-based
ing,weusedtheDailyMailportionoftheDeepMindcorpus.
surprisalforthetextsusedinourexperiments,andexpectto
Weusedavocabularyconsistingofthe70mostfrequentchar-
obtainmeansurprisalscoresforeachexperimentalcondition
acters,mappingotherstoanout-of-vocabularytoken.
that resemble mean reading times. We will also verify our
Thehyperparametersofthelanguagemodelwereselected
prediction that word-based surprisal (WORDSURPRISAL) is
onanEnglishcorpusbasedonWikipediatext.5 Wethenused
notabletoaccountfortheeffectsobservedinourexperimen-
theresultingmodeltocomputesurprisalonthetextsusedin
taldata,duetothewayittreatsunknown
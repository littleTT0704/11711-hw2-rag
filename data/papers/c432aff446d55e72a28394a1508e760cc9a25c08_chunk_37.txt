Figure6: Theeffectofthetokencharacterlengthonhowmuchaccuratenearestneighborsarebetterthan
approximateFAISSneighbors. Negativevaluesmeanworse. Thetrendlineofthescatterpointsisshown.
follows,wherew andw representthewordafterandbeforethegivenwordw.
after before
(cid:88)
H (w)=− p(w |w)logp(w |w) (10)
forward after after
wafter
(cid:88)
H (w)=− p(w |w)logp(w |w) (11)
backward before before
wbefore
Forwardandbackwardentropyrepresentshowdiversethecontextafterandbeforethegivenwordis.Intuitively,
bigramentropyissupposedtoindicatewordsthatcanappearinlotsofdifferentcontexts. Thehigherthe
entropyofaword,themorediverseitscontextis,andviceversa. Forexample,wordslike“Francisco”would
havealowentropybecauseitmostlycomesafter“San”.
Figure7: Theeffectoftheforwardandbackwardentropyofwordsonhowaccuratenearestneighborsare
betterthanapproximateFAISSneighbors. Negativevaluesmeanworse. Thetrendlineofthescatterpointsare
shown.
ThecomparisonisshowninFigure7. Wecanseethatthehighertheentropyinbothforwardandbackward
cases,thebetterusingapproximatenearestneighborsearchbecomes. Thissuggeststhatwordsthatappear
in many different contexts are better off with an approximate kNN, and “easy-to-predict” examples such
as“Jersey”and“Fransisco”isbetterwithaccuratekNN,possiblybecausetheseexamplesarelessproneto
overfittingerrorsandthusrequireslessregularizationfromapproximation.
17
E FailedHypotheses
E.1 DistanceMetric
WehypothesizethatthekeytokNN-LM’sperformancegainistheensembleoftwodistancemetrics: the
standarddotproductdistance(whichtheLMuses)withtheL2distance(whichthekNNcomponentusesas
�
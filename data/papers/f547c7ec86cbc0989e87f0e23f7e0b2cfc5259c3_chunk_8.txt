 16, with 18 layers, and a hidden
attheendofcontextT: dimensionof256. Thedecoderisa6-layerTrans-
formerdecoder(Vaswanietal.,2017). Toimprove
the training speed, we initialize the encoder with
p (g <eos> X)
ctc
Odds (g) = ⊕ |. (4)
end
p (g c X) 2https://github.com/quanpn90/NMTGMinor
c / <eos> ctc ⊕ |
∈V { }
P 391
weights pretrained on the ASR task. Further, we Lang Decoding AL ↓ AL CA↓ RTF ↓ BLEU ↑
employ ST CTC (Deng et al., 2022; Yan et al., En-De BWBS 1922 3121 0.46 30.6
IBWBS 1977 3277 0.52 31.7
2022)aftertheencoderwithweight0.3duringthe
BWBS 1992 3076 0.50 15.5
training. Duringthedecoding,weuse0.3forEn- En-Ja
IBWBS 1935 3264 0.64 15.6
glish to German, and 0.4 for English to Chinese.
BWBS 1948 2855 0.41 26.5
We preprocess the audio with 80-dimensional fil- En-Zh
IBWBS 1945 3031 0.48 26.5
terbanks. Asoutputvocabulary,weuseunigram
models (Kudo, 2018) of size 4000 for English to Table1: IncrementalSSTwiththeoriginalBWBSand
German,and8000forEnglishtoChinese. IBWBS.Betterscoresinbold.
3.2 Evaluation
putethedecoderstatesaftereachsourceincrement.
Inallourexperimentswiththeofflinemodels,we
SincetheIBWBSsometimeswaitsformoresource
usebeamsearchofsize8exceptfortheCTCpol-
chunkstooutputmoretokens,theunnecessaryde-
icyexperimentswhereweusegreedysearch. For
coderstaterecomputationsmightincreasethecom-
experiments with the blockwise models, we use
putationalcomplexity.
the beam search of 6.
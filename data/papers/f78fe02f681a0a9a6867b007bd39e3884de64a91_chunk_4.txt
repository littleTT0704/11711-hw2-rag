§2.2),
graphscoverawiderangeofeverydaysituations
(2)convertingitintosentenceformandgenerating
(West et al., 2022), conditioning on them results
anarrativefromthesentence(§2.3),and(3)infer-
in a broad spectrum of conversations. Moreover,
ringtheconversationparticipantsfromthenarrative
sinceLLMsarepronetohallucinations(Weidinger
andderiveaconversationgroundedinthenarrative
et al., 2021), the seed commonsense knowledge
(§2.4). We use GPT-3.5 (i.e., text-davinci-0022;
canhelpthemstayonasensiblegenerationpath.
Ouyang et al., 2022) to implement CO, though
3
With SODA, we train a COnverSation MOdel,
in practice, a different model could be used. We
COSMO. Human evaluation results demon- use CO
3
tocreate SODA: anexampleisinTable1.
strate that: (1) COSMO generalizes better to un- MoredetailscanbefoundinAppendixA.
seenconversationsthanexistingbest-performing
dialoguemodels,winningbymorethan40%onav- 2.1 InspirationBehind CO 3
erageinhead-to-headcomparisonsversusBlender- What is at the heart of conversation? At its core,
Bot (Roller et al., 2021), Koala (Geng et al., aconversationisafundamentalformofsocialin-
2023),andVicuna(Chiangetal.,2023)(§5.1);(2) teraction(Myllyniemi,1986). Theseexperiences
COSMO outperforms BlenderBot (with the same are abstracted into narratives or scripts (Mar and
number of parameters) on the dataset Blender- Oatley,2008;Rumelhart,1975;SchankandAbel-
Bot was trained on, despite never seeing the cor- son, 1975). Eventually, social experiences form
pus (§5.2); and (3) COSMO responses are even ourknowledgeforexplainingeverydayeventsand
preferred over human
 2D numpy array of 0.7, which have been used in prior papers when gen-
sentence embeddings erating training data for paraphrase generation (Iyyer
14 cosine_scores = model.score_raw_text([( etal.,2018;Krishnaetal.,2020)).
text1, text2)]) # list of cosine
scores
en ar de es fr ru tr zh
25.85M8.23M6.47M6.75M6.46M9.09M5.12M4.18M
Figure1: Usageexampleofprogrammaticallyloading
one of our pretrained models and obtaining sentence
Table 1: The number of sentence pairs used to train
embeddingsandscoresfortwosentences.
ourmodels. ForEnglish,thedataisParaNMT,andfor
the other languages, the data is a collection of bitext
python -u embed_sentences.py --sentence-
1
detailedinSection5.1.
file paraphrase-at-scale/example-
sentences.txt --load-file paraphrase
-at-scale/model.para.lc.100.pt --
data, including scripts to download and evaluate
output-file sentence_embeds.np
ontheSTSdata(English,non-English,andcross-
2
3 python score_sentence_pairs.py -- lingual), as well as code to download and pro-
sentence-pair-file paraphrase-at-
cess bitext and ParaNMT automatically. For bi-
scale/example-sentences-pairs.txt --
load-file paraphrase-at-scale/model. text,ourscriptsdownloadthedata,filterthedata
para.lc.100.pt by length,5 lowercase, remove duplicates, train a
sentencepiece model, encode the data with the
Figure 2: Usage examples to embed sentences and
sentencepiece model, shuffle the data, and pro-
score sentence pairs. The first command is a usage
example of scoring a list of sentence pairs. The file cess the data into HDF5 format for efficient use.
example-sentences-pairs.txtcontainsalistofsen- ForParaNMT,ourscriptsdownloadthed
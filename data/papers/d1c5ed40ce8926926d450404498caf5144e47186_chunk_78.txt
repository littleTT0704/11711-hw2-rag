 determining relevance was
used for both markup-based paragraphs and sentences. The annotation methodology
is described in more detail in Section 4.3.1.
Table 5.1 shows the topics of the seed articles along with the total number of
markup-based text nuggets extracted from the retrieved web pages and the number
and percentage of those nuggets that were labeled as relevant. We also specify for
51
52 CHAPTER 5. INTRINSIC EVALUATION
each topic the annotator it was assigned to. The last column indicates how the topics
were used in our evaluations: to fit language models (LM) or for cross-validations
of relevance models (CV). We will refer to these labels when detailing the setup of
each experiment. The annotated dataset comprises a total of 164,438 text nuggets,
out of which 9,221 nuggets (5.6%) were labeled as relevant. Note that we focused
on topics that are named entities since these topics are usually less ambiguous and
easier to annotate, but we will show in Chapter 6 that our approach is also effective
for common nouns that can be found in a dictionary.
To evaluate inter-annotator agreement and to get a better sense of the difficulty
of this task and the quality of our dataset, annotator A also labeled the web pages
for the topics Mother Teresa and Iran-Iraq War. The former topic was originally
assigned to annotator B, while the latter was labeled by annotator C. We selected
topics for which the original annotator found a substantial amount of relevant text
to ensure low variance in our assessments of inter-annotator agreement. If there were
only a few relevant text nuggets, the annotators could agree or disagree simply by
chance. For each of the topics, the agreement was measured at the level of markup-
basedtextnuggets, sentence-levelnuggetsandnuggetsconsistingofindividualtokens.
Recall that we consider a nugget relevant if an annotator labeled any substring of it,
and irrelevant otherwise. The text nuggets were divided into four disjoint categories:
(1) nuggets that are relevant according to both annotators, (2) nuggets that were
labeled as relevant by one annotator but not by the other, (3) nuggets for which
the reverse holds, and (4) nuggets that neither annotator considered relevant
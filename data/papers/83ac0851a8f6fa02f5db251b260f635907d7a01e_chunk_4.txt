 Thisinsightyields
workusesthreelearningsignals:
significant gains on evaluation metrics relative to existing
models. Theprimarycontributionsofourworkare:
LOGIT l t: local distribution over action. The logit of
the action chosen at time t is denoted l. Specifically, the
t
• Amethodtoalleviatetheexposurebiasofactiondecod-
original language instruction is encoded via LSTM. An-
ingandexpensivenessofbeamsearch.
other LSTM acts as a decoder, using attention mechanism
• Analgorithmthatmakesuseofasynchronoussearchwith
to generate logits over actions. At each time step t of de-
neuraldecoding.
coding, logits are calculated by taking the dot product of
• Anextensibleframeworkthatcanbeappliedtoexisting thedecoder’shiddenstateandeachcandidateactionai.
t
modelstoachievesignificantgainsonSPL. PM ppm: global progress monitor. It tracks how much
t
of an instruction has been completed [13]. Formally, the
2.Method
model takes as input the (decoder) LSTM’s current cell
TheVLNchallengerequiresanagenttocarryoutanat- state, c t, previous hidden state, h t−1, visual inputs, V t,
ural language instruction in photo-realistic environments. and attention over language embeddings, α t to compute
The agent takes an input instruction X, which contains a score pp tm. The score ranges between [-1,1], indicating
several sentences describing a desired trajectory. At each the agent’s normalized progress. Training this indicator
stept,theagentobservesitssurroundingsV. Becausethe regularizes attention alignments, helping the model learn
t
agent can look around for 360 degrees, V is in fact a set language-to-visioncorrespondencesthatitcanusetocom-
t
of K = 36 different views. We denote each view as Vk. paremultipletrajectories.
t
Usingthismultimodalinput,theagentistrainedtoexecute SPEAKER S: global scoring. Given a sequence of vi-
asequenceofactionsa,a,....,a ∈
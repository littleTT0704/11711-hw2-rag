innerdimensionduringtraining,beam
searchsizeandlengthpenaltyduringinference. tolearningratethanPLMfine-tuning. Fixingtwo
5Concretely,{5,10,20,50,100,200,500,3000,6000, separate learning rates for few shot and non-few
10000, 20000, 30000}. For English, which has far more
shotexperimentsforalllanguages,asimplification
trainingsamplesthanotherlanguages,weaddtwotrainingset
size100000and300000. of the normal training process to find an optimal
(a)English (b)ChineseSimplified (c)Spanish
(d)Ukrainian (e)Urdu (f)Average
Figure 3: Performance of prefix-tuning, adapter-tuning and PLM fine-tuning on five languages over training set
sizes. Thexaxisisthenumberoftrainingsamplesatlogscale,theyaxisistheROUGE-2score.
learning rate for each training set size and each tuninghasbothadvantagesofutilizingtheknowl-
languageonthedevelopmentset,mightcausethe edgeofPLMbetterandfewerparameterstotune,
unstable performance of the tuning methods that whileadapter-tuningonlybenefitsfromthelatter.
are more sensitive to learning rate. (4) All above Thisremindsusthatwhiledesigningaprompt(Liu
observations are roughly true for every language et al., 2021), one important thing is to keep the
andareespeciallyclearintheaverageplot. PLMâ€™sarchitectureandmakethepromptasnatural
aspossibletobetterextractknowledgefromPLM.
Discussion&Takeaways Thepossibleexplana-
3.2 Exp-II:EffectofSupervisedTransfer
tion of the different behavior of adapter-tuning
and prefix-tuning is their structural discrepancy, Whendesigningmodelsinmulti-lingualscenarios,
inwhichadapter-tuningaddsparametersbetween one crucial question is how to make modules of
two transformer layers, while prefix-tuning uses differentlanguagescommunicateefficientlysothat
theseparameterstogenerateprefixesandappends sharedknowledgecanbefullyutilized. Thereare
theprefixesatthefront
 38.79
BERT-large - - - - - - 79.45 81.20 79.80
Covid-BERT - - - - - - 84.83 86.97 85.26
Prop-BERT - - - - - - 72.60 65.00 61.78
BERT-large - - - - - - 23.12 43.00 30.07
cancer Covid-BERT - - - - - - 67.87 61.00 56.85
(unsup.) GPT-2(large) 27.24 23.55 10.95 64.38 59.21 54.43 59.16 53.00 43.50
T5-large 21.87 24.95 21.12 62.08 61.62 61.44 41.13 48.00 35.52
GPT-2(large)+masked 22.93 23.94 21.78 60.06 55.69 51.00 66.03 66.00 65.99
T5-large+masked 21.38 22.79 19.57 54.84 54.41 53.66 65.26 55.00 45.91
cancer GPT-2(large)+sup 30.32 31.03 27.38 66.97 66.83 66.84 87.13 87.00 86.99
(sup.) T5-large+sup 12.17 21.67 10.51 75.30 67.95 66.15 86.00 86.00 86.00
Table7: Automaticmodelingresults(classificationtask). Fortheunsupervisedcancersetting(unsup.),allmodels
aretrainedoncovid/climatedataonlyoranothernewsdataset(Prop-BERT).Forthesupervisedsetting(sup.),we
fine-tuneon574cancernewsexamples.
guage(e.g. “Covid-19vaccinesmaybetheworst claimsstatedinthedataisunknown.
threatweface”),weuseapre-trainedBERTpropa-
PerformanceonOut-of-DomainData Wetest
gandadetector(DaSanMartinoetal.,2019)which
the ability of reaction frames to generalize using
wedenotehereas(Prop-BERT).20 Forourzero-
100cancer-relatedrealandmisinformationhealth
shots
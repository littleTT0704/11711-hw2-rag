searchmaycausethekNNcomponenttooverfitthetrainingdata. Comparingthe
smalldatastorewithonly5%withtheoriginaldatastore,weseethatasmalldatastoremeansasmalltraining
setforthekNN“model”anditthusitbenefitsmorefromthisregularization,bothboththroughusingthe
FAISSmaskandFAISSscore(atoptimaltemperaturesettings). Fromtheseexperiments,wecanseethat,
surprisingly,oneoftheimportantingredientsinkNN-LMseemstobeapproximatekNNsearch,whichlikely
preventsoverfittingtothedatastorecreatedfromthesametrainingset. Wefurtheranalyzethisunexpected
resultinAppendixD,wherewefindthatlongerwordsandwordsthatappearinmanydifferentcontextshave
slightlybetterresultswithapproximatenearestneighbors.
Coincidentally, He et al. (2021) find that dimensionality reduction using PCA on datastore vectors (from
1024to512dimensions)improvestheperplexityoftheoriginalkNN-LMfrom16.46to16.25,whichcan
be explained by our findings as PCA may provide another source of “approximation” that contributes to
regularization.
Notably,similareffects,whereanapproximationcomponentleadtobettergeneralization,havebeenreportedin
otherNLPtasksaswell,andaresometimesreferredtoas“beneficialsearchbias”,whenmodelingerrorscause
thehighest-scoringsolutiontonotbethecorrectone: Meisteretal.(2020b)suggestthat“quitesurprisingly,
beam search often returns better results than exact inference due to beneficial search bias for NLP tasks.”
StahlbergandByrne(2019)alsoconcludethat“vanillaNMTinitscurrentformrequiresjusttherightamount
ofbeamsearcherrors,which,fromamodelingperspective,isahighlyunsatisfactoryconclusionindeed,as
themodeloftenprefersanemptytranslation”.
6 ProbablyWrongHypothesesforWhykNN-LMsWork
Theresultsintheprevioussectionsaretheresultof
GBDatasetofDiverseTextforLanguageModel- AssociationforComputationalLinguistics.
ing. CoRR,abs/2101.00027.
LucasLiebenwein,CenkBaykal,BrandonCarter,David
MichaelGira,RuisuZhang,andKangwookLee.2022. Gifford, and Daniela Rus. 2021. Lost in Pruning:
Debiasing Pre-Trained Language Models via Effi- TheEffectsofPruningNeuralNetworksbeyondTest
cientFine-Tuning. InLT-EDI,pages59–69.Associa- Accuracy. InMLSys.mlsys.org.
tionforComputationalLinguistics.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
MariusHessenthaler,EmmaStrubell,DirkHovy,and dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
AnneLauscher.2022. BridgingFairnessandEnvi- Luke Zettlemoyer, and Veselin Stoyanov. 2019.
ronmental Sustainability in Natural Language Pro- RoBERTa: A Robustly Optimized BERT Pretrain-
cessing. InEMNLP,pages7817–7836.Association ingApproach. CoRR,abs/1907.11692.
forComputationalLinguistics.
NicholasMeade,ElinorPoole-Dayan,andSivaReddy.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
2022. AnEmpiricalSurveyoftheEffectivenessof
2015. Distillingtheknowledgeinaneuralnetwork.
DebiasingTechniquesforPre-trainedLanguageMod-
InNIPSWorkshoponDeepLearning.
els. InACL(1),pages1878–1898.Associationfor
ComputationalLinguistics.
Sara Hooker, Aaron Courville, Gregory Clark, Yann
Dauphin,andAndreaFrome.2021. WhatDoCom-
Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.
pressedDeepNeuralNetworksForget?
StereoSet: Measuringst
-
figuration corresponds to our original formulation. ‘Only tionnetworks. NeurIPS,2016. 2
PL’configurationestimatestheobject-regionsusingjustthe [7] Wei-LunChang,Hui-PoWang,Wen-HsiaoPeng,andWei-
pseudo-labels and hence ignores complementary informa- Chen Chiu. All about structure: Adapting structural infor-
tion from depth. ‘Only Depth+RGB segments’ do not use mationacrossdomainsforboostingsemanticsegmentation.
pseudo-labelstodefineregionlabelsandinsteadtreatseach CoRR,2019. 7
Depth+RGBsegmentasauniqueobjectcategory. Thecon- [8] Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong,
figurationsinnexttworowsuseonlyoneofthetwomodal- Xinghao Ding, Yue Huang, Tingyang Xu, and Junzhou
itiesforestimatingobjectregionswhilestillusingpseudo- Huang. Progressivefeaturealignmentforunsuperviseddo-
mainadaptation. InProceedingsoftheIEEE/CVFConfer-
labels to define region labels. We observe that contrastive
ence on Computer Vision and Pattern Recognition, pages
regulariserbasedononlypseudo-labelsperformstheworst
627–636,2019. 3
and significantly below the one based on just multimodal
[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
segments. This is intuitive because reusing pseudo-labels
KevinP.Murphy,andAlanLoddonYuille.Deeplab:Seman-
asaregularisationwithoutauxiliaryinformationreinforces
ticimagesegmentationwithdeepconvolutionalnets,atrous
theconfirmationbias. While, purelyRGBbasedsegments
convolution,andfullyconnectedcrfs.IEEETransactionson
leadtobetterobjectnessconstraintthanpurelydepth-based PatternAnalysisandMachineIntelligence,2018. 1,7
ones (as can be seen in Fig. 2), combining the two (ALL [10] TingChen,SimonKornblith,MohammadNorouzi,
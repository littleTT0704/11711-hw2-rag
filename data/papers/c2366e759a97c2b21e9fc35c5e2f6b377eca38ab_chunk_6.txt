oftheobjects,thepickinstancechosenbytheexpert,
placeinstancesforthecorrespondingcategory,andtheplaceinstancechosenbytheexpert. Expert
actions are assumed to belong to the set of visible instances in S. However, different experts can
i
exhibit different preferences over a. For example, one expert might choose to load bowls first in
i
thetoprack,whileanotherexpertmightloadbowlslastinthebottomrack. Inthetrainingdataset,
weassumelabelsforwhichpreferenceeachdemonstrationbelongsto,basedontheexpertusedfor
collectingthedemonstration. GivenK demonstrationsperpreferencem ∈ M, wehaveadataset
forpreferencem: D = {C,···,C }. Thecompletetrainingdatasetconsistsofdemonstrations
m 1 K
from all preferences: D =
(cid:83)M
D. During training, we learn a policy that can reproduce all
m=1 m
demonstrationsinourdataset. Thisischallenging, sincetheactionstakenbydifferentexpertsare
differentforthesameinput,andthepolicyneedstodisambiguatethepreference. Attesttime,we
generalizethepolicytobothunseenscenesandunseenpreferences.
2.1 Prompt-SituationTransformer
We use Transformers [18], a deep neural network that operates on sequences of data, for learn-
ing a high-level pick-place policy π. The input to the encoder is a d-dimensional token1 per in-
stance e ∈ [1,..N] in the state S. In addition to instances, we introduce special <ACT> to-
i
kens2,azerovectorforallattributes,todemarcatetheendofonestateandstartofthenext. These
tokens help maintain the temporal structure; all instances between two <ACT> tokens in a se-
(cid:2)
quence represent one observed state. A trajectory τ, without actions, is: τ = S,<ACT>
t=0
(cid:3)
,···,S,<ACT>,S. To learn a common policy for multiple preferences, we
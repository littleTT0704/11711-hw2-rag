bitextmining. WealsocomparetoLaBSE(Feng
ditionally,weannealthemega-batchsizebyslowly
etal.,2022),acontrastivemodeltrainedonsixbil-
increasingitduringtraining. Thisyieldsimproved
lion parallel pairs across languages and was also
performancebyasignificantmargin.
trained on monolingual text using a masked lan-
guagemodellingobjective. Encoder. Our sentence encoder g simply aver-
ages the embeddings of subword units generated
3 Methods bysentencepiece(KudoandRichardson,2018);
werefertoourmodelasPARAGRAM-SP,abbrevi-
We first describe our objective function and then
atedas P-SP.Thismeansthatthesentencepiece
describeourencoder.
embeddingsthemselvesaretheonlylearnedparam-
Training. The training data consists of a se- etersofthismodel.
quenceofparallelsentencepairs(s,t )insource
i i
and target languages respectively. Note that for 4 CodeandUsage
trainingourEnglishmodel,thesourceandtarget
We added a number of features to the code base
languagesarebothEnglishasweareabletomake
toimproveperformanceandmakeiteasiertouse.
useofanexistingparaphrasecorpus. Foreachsen-
First, we added code to support easier inference.
tence pair, we randomly choose a negative target
Examples of using the code programmatically to
sentencet duringtrainingthatisnotatranslation
0i
embed sentences and score sentence pairs (using
orparaphraseofs. Ourobjectiveistohavesource
i
cosinesimilarity)areshowninFigure1.
and target sentences be more similar than source
Our code base also supports functionality that
andnegativetargetexamplesbyamarginδ:
allowsonetoreadinalistofsentencesandproduce
a saved numpy array of the sentence embeddings.
θm src,i θn
tgt
δ −f θ(s i,t i)+f θ(s i,t 0i))
+
(1)
Wealsoincludedfunctionalitythat
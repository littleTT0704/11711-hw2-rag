 2022 Toward a 'Standard Model' of Machine Learning
Section 5 discussed the different choices of the divergence function D. In particular, in the same setting of
Equation 3.3 where D is the common cross entropy (or KL divergence as discussed in Section 5.1), the student
step is written as:
θ(n+1) =argmax
θ
Eq(n+1)(t)[logp θ(t)], (7.8)
where
q(n+1)
is in the form of Equation 7.1 above. The optimization then amounts to first drawing samples
~
from the teacher t ∼ q(n+1), and then updating θ by maximizing the log-likelihood of those samples under
p θ. We could apply various sampling methods to draw samples from q(n+1), such as Markov chain Monte
Carlo (MCMC) methods, like Gibbs sampling when t is discrete and Hamiltonian or Langevin MC for
continuous t (Duane et al., 1987; Neal, 1992; Welling & Teh, 2011), and sampling based on stochastic
differential equations (SDEs; Liu et al., 2022; Song et al., 2020).
In the special case of α = β, the teacher model reduces to q(n+1)(t) ∝ p (t)exp{f(t)/α}. This
θ(n)
special form allows us to use a simple importance sampling method (Hu et al., 2018), with p (i.e., the
θ(n)
current model distribution) as the proposal distribution:
θ(n+1)
=argmax θ E~ t∼p
θ(n)(t)[exp{f(~
t)/α}⋅logp
θ(~
t)] / Z. (7.9)
Here the experience function f(t) serves as a sample reweighting mechanism (Wu et al., 2020) that highlights
the ‘high-quality’ samples (i.e., those receiving high goodness scores in the light of the experience) for
updating the parameters θ.
For other choices of the divergence function D other than the cross entropy or KL divergence, sampling from
q(n+1)
may similarly be sufficient in order to estimate and optimize
,y)∈D θ
a trained model and input x, a set of predicted solutions Automaticallyfindingproblemsthatexposedeficienciesre-
rankedbyamodelscoreisobtainedbybeamsearch,denoted quiresanon-differentiablecost(Equation1),satisfyingcon-
{yˆ,...,yˆ } = f (x;k,b), where b is beam size and k is straintsforvalidequations,andfindingdiverseproblemsets
1 k θ
the number of candidates saved for evaluation. For brevity to characterize each aspect of generalization. To address
weomitbinthediscussionunlessnecessary. these challenges, we develop SAGGA (Symbolic Archive
Generation with Genetic Algorithms), a gradient-free ge-
Evaluation. The standard practice is to evaluate a candi-
neticalgorithmwhichiterativelyfindsdiversefailures.
dateyˆ bycheckingwhetherthederivativeofyˆ isequivalent
Ateachiteration,SAGGAmutatesaseedsetofproblems
toxusingasymbolicsolver(e.g.Sympy).Inthemaximum-
bymodifyingeachproblem’sequationtree,ensuringthatthe
a-posteriori(MAP)setting,themodel’soutputisconsidered
correct if its top-ranked candidate yˆ
1
is correct. This crite- 1https://github.com/facebookresearch/SymbolicMathematics/,
rion is relaxed in the search-and-verify setting, where the commit4596d07.
8630
Algorithm1:SAGGA.Eachseedproblemdenoted Type Test Fail@50 Fail@10 Fail@1
asxˆ,mutatedproblemasx˜,archivedproblemasx. coeff k ln(k x) 0.0 0.0 0.0
1 2
Parameters:FitnessF(f θ,x)→R, k 1x 0.0 0.0 0.0
mutateandseedstrategies,archivesizeN. k 1x42 0.0 6.1 45.5
Output:ProblemarchiveD ={x 1,...,x N}. k 1exp(k 2x) 15
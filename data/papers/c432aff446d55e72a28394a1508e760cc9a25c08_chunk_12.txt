 Thegainfrom
kNN-LMismoresignificantwhenusedwithasmaller,lesscapablebaselanguagemodel,asexpected. The
details are shown in Appendix A. In this paper, we are mainly focused on the factors contributing to the
relativeimprovementsfromkNN-LM,insteadoftheabsoluteperformance,soweusethe268Mmodelforthe
remainderofthepaper.
Inthenextsections,weperformfurtherexperimentswithablationsonthegeneralformulationEquation5to
elucidatethekeyelementscontributingtotheperformanceimprovementsinkNN-LM.
4 EffectofDifferentW Formulations
ds
4.1 ReplacingtheDatastorewithTrainableEmbeddings
FromtheobservationinSection3,weseethatthechoiceofh hasalargeimpactontheperformanceof
ds
kNN-LM.ThisintriguesustoexploreifonekeytotheimprovementsaffordedbykNN-LMliesintheuse
ofdifferentinputrepresentationstogether,namelytheattentionoutput(h = att)andfeedforwardoutput
ds
(h = ffn). However, fromonlytheexperimentsabove, itisnotpossibletodisentangletheeffectofthe
ds
choiceofh andthatofotherdesignchoicesandfactorsinEquation5.
ds
Totesttheeffectofh inamorecontrolledsetting,weremovethenon-parametricdatastoreentirely,and
ds
initializeW inEquation5witharandomlyinitializedwordembeddingmatrixwiththesamesize(N =V)
ds ds
5
astheLM’soutputembeddingW,andtrainW withallotherparametersfixed.2 Thelossfunctionfor
sm ds
trainingisthecross-entropylossofsoftmax(W ·h )withrespecttotheground-truthtokens,identically
ds ds
to how the base LM is trained. We compare how using h = att or h = ffn affects the interpolated
ds ds
performance. TheresultsareshowninTable2,andwealsoshowresultsfromkNN-LMsusingthesetwo
varietiesofinputrepresentationforreference.
Fromtheseexperimentswec
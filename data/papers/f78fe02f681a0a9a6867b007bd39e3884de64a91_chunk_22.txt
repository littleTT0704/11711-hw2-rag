nowledgeor
thesemodels,suchasbiasesandprivateinforma-
historicalfacts. Weseektointegrateotherexisting
tion. Withthegoalofmitigatingsuchdanger,we
knowledge-groundeddialoguedatasetsinto CO
3
takeparticularprecautionstovetthesafetyofthe
inthefuture.
distilledconversations.
Finally,ourchoiceoflargelanguagemodel(i.e.,
First, previous studies have shown that human
GPT-3.5)willlikelyaffectthetypesofdialogues
names commonly associated with certain gender
created. Futureinvestigationmaylookintoother
and/or ethnicity result in biases in conversations
potential large language model as sources to di-
producedbystate-of-the-artdialogsystems(Smith
versify the types and content of dialogues being
andWilliams,2021), suchasBlenderBot(Roller
generated. Similarly,futureworkscaninvestigate
etal.,2021). Todiversifythenamerepresentations,
other base models for COSMO that may lead to
wedrawawiderangeofcommonnamesrepresen-
differentqualityofresponsegeneration.
tativeofdifferentgenderandraceidentitiesfrom
theUSSSNnamerepository. Furthermore,tomini- Intent of Technology and AI Regulation We
mizepotentialharmfulcontentfromlargelanguage wanttostressthattheintentionofourworkisnot
models,wefiltergenerateddialoguesbyCanary,a to build AI systems to replace humans. Instead,
dialoguesafetydetectormodel(Kimetal.,2022a), we want to build better assistive technologies, as
andRewireAPI,apubliclyavailableAPIfortoxic chatbots are increasingly used in user-AI interac-
contentdetection,13 toremovedialogueswithpo- tionsandaugmentinghuman-humanconversations.
tentiallytoxicanddangerouscontent. Finally, to avoid situations where humans might
Ourmethodstopre-emptpotentialharmfulcon- be manipulated, we stress the need for improved
tentmaynotcatcheverything. Forexample,even regulationsontheuseandmisuseofconversational
withourdiversepoolofnames,
.
awayfromthem. Additionally,trainingapproaches
8 BroaderImpactandEthical enceonFairness,Accountability,andTransparency
Implications (FAccT).
StevenBirdandEdwardLoper.2004. NLTK:Thenat-
Our study is motivated by the potential harms of
ural language toolkit. In Proceedings of the ACL
using pretrained language models (Bender et al.,
InteractivePosterandDemonstrationSessions.
2021),specificallytheirtendencytogeneratehate-
ful,offensive,ortoxiccontent(Shengetal.,2020; Greg Brockman, Mira Murati, and Peter Welinder.
Gehman et al., 2020). Part of our work requires 2020. OpenAIAPI. Blogpost.
automaticallydetectingtoxicityingeneratedtexts,
T. Brown, B. Mann, Nick Ryder, Melanie Subbiah,
forwhichweusethePerspectiveAPI.7 acommer-
J. Kaplan, Prafulla Dhariwal, Arvind Neelakan-
cially deployed toxicity detection tool. However, tan, Pranav Shyam, Girish Sastry, Amanda Askell,
themismatchbetweentheconstructoftoxicityand Sandhini Agarwal, Ariel Herbert-Voss, G. KruÂ¨ger,
T. Henighan, R. Child, Aditya Ramesh, D. Ziegler,
itsoperationalizationthroughanautomaticclassi-
Jeffrey Wu, Clemens Winter, Christopher Hesse,
fiercancausebiasedorunintendedmodelbehavior
Mark Chen, E. Sigler, Mateusz Litwin, Scott Gray,
(Jacobs and Wallach, 2021). Specifically, recent BenjaminChess,J.Clark,ChristopherBerner,Sam
work has shown that such hate speech classifiers McCandlish, A.Radford, IlyaSutskever, andDario
Amodei.2020. Languagemodelsarefew-shotlearn-
overestimatetheprevalenceoftoxicityintextthat
ers. InProceedingsofthe34thConferenceonNeu-
containsaminorityidentitymention(Hutchinson
ralInformationProcessingSystems(NeurIPS).
etal.,2020;D
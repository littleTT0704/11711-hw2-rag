-zero ‘temperature’ 1/λ), we tend to select the most
informative sample from D. The procedure rediscovers the algorithm proposed in (Ertekin et al., 2007) and
sub
more generally the pooling-based active learning algorithms (Settles, 2012).
4.2. Knowledge-Based Experience
Many aspects of problem structures and human knowledge are difficult if not impossible to be expressed
through individual data instances. Examples include the knowledge of expected feature values, maximum
margin structures (Section 2.3), logical rules, and so on. The knowledge generally imposes constraints that we
want the target model to satisfy. The experience function in the standard equation is a natural vehicle for
incorporating such knowledge constraints in learning. Given a configuration t, the experience function f(t)
measures the degree to which the configuration satisfies the constraints.
As an example, we consider first-order logic (FOL) rules, which provide an expressive declarative language to
encode complex symbolic knowledge (Hu et al., 2016). More concretely, let f (t) be an FOL rule w.r.t. the
rule
variables t. For flexibility, we use soft logic (Bach et al., 2017) to formulate the rule. Soft logic allows
23
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
continuous truth values from the interval [0,1] instead of {0,1}, and the Boolean logical operators are
redefined as:
A&B =max{A+B −1,0}, A∨B =min{A+B,1}
(4.12)
A ∧⋯∧A = A /N, ¬A =1−A.
1 N ∑i i
Here & and ∧ are two different approximations to logical conjunction: & is useful as a selection operator
(e.g., A&B = B when A = 1, and A&B = 0 when A = 0), while ∧ is an averaging operator. To give
a concrete example, consider the problem of sentiment classification, where given a sentence x, we want to
predict its sentiment y ∈ {negative 0,positive 1}. A challenge for a sentiment classifier is to
understand the contrastive sense within a sentence and capture
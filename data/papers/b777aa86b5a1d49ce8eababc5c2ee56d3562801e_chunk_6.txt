ationallyefficientmodels specificapplicationsinreinforcementlearningand
forlanguageprocessinghasledtothedevelopment recommendationsystems(Gleesonetal.,2021;Lin
of a variety of model architectures that achieve etal.,2022),wheresimulationandmemoryaccess
comparable task performance under fixed FLOP dominateexecutiontime. Additionally,theseprior
budgets. For example, compression of input se- effortsarerestrictedtosmallsetsofreferencemod-
quencesandintermediaterepresentationshasbeen elsandhavenotdirectlyexaminedtherelationship
usedtoreducethecomputationalcostoflongtext betweenmodelarchitecturesandplatforms.
sequences (Dai et al., 2020; Goyal et al., 2020)
and distillation has been used to reduce model 3 Preliminaries
size (Sanh et al., 2019; Hou et al., 2020). Other
3.1 NeuralNetworkFrameworks
work has sought to design efficient model archi-
tectures by developing low-FLOP substitutes for Totakeadvantageofmassivelyparallelhardware
standard,denseself-attentionandconvolutionop- accelerators,inferencewithvariablelengthtextand
erations (Iandola et al., 2016; Zhang et al., 2018; speechsequencesarepaddedtofixedlengthtensors
Sandleretal.,2018;Sunetal.,2020;Xiongetal., thatareprocessedwithneuralnetworkframeworks.
2021;Wangetal.,2020b). These frameworks provide implementations and
Additionally, directefficiencymetrics, suchas APIs for tensor operations, gradient calculation,
wall-clocklatencyandenergyusagehavebeenin- andconstructionofneuralnetworkcomputational
corporated into the objectives of neural architec- graphs. Frameworksgenerallyfallintothefollow-
turesearch(NAS)andAutoMLmethods(Wuetal., ingdesignparadigms(Kahnetal.,2022):
2019;Tanetal.,2019;Wangetal.,2020a). Man-
Eager Execution: The computational graph is
ual inspection of the learned models shows that
constructed from a series of operations that are
NAS often implicitly learns to take advantage of
executedassoonascalledfromaninterpreter. Ex-

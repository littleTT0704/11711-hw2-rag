Training on Foveated Images Improves Robustness to
Adversarial Attacks
MuhammadA.Shah&BhikshaRaj
LanguageTechnologiesInstitute
CarnegieMellonUniversity
Pittsburgh,PA15213
{mshah1, bhiksha}@cs.cmu.edu
Abstract
Deepneuralnetworks(DNNs)havebeenshowntobevulnerabletoadversarial
attacksâ€“subtle,perceptuallyindistinguishableperturbationsofinputsthatchange
theresponseofthemodel.Inthecontextofvision,wehypothesizethatanimportant
contributortotherobustnessofhumanvisualperceptionisconstantexposuretolow-
fidelityvisualstimuliinourperipheralvision. Toinvestigatethishypothesis,we
developR-Blur,animagetransformthatsimulatesthelossinfidelityofperipheral
visionbyblurringtheimageandreducingitscolorsaturationbasedonthedistance
from a given fixation point. We show that compared to DNNs trained on the
originalimages,DNNstrainedonimagestransformedbyR-Bluraresubstantially
morerobusttoadversarialattacks,aswellasother,non-adversarial,corruptions,
achievingupto25%higheraccuracyonperturbeddata.
1 Introduction
DeepNeuralNetworks(DNNs)areexceptionallyadeptatmanycomputervisiontasksandhave
emergedasoneofthebestmodelsofthebiologicalneuronsinvolvedinvisualobjectrecognition
[Yaminsetal.,2014,Cadieuetal.,2014]. However,theirlackofrobustnesstosubtleimageperturba-
tionsthathumansarelargelyinvariant[Szegedyetal.,2014,Geirhosetal.,2018,DodgeandKaram,
2017]tohasraisedquestionsabouttheirreliabilityinreal-worldscenarios. Oftheseperturbations,
perhapsthemostalarmingareadversarialattacks,whicharespeciallycrafteddistortionsthatcan
changetheresponseofDNNswhenaddedtotheirinputs[Szegedyetal.,2014
 Schuurmans, D., Le, Q., Chi, E.,
and Zhou, D. Self-Consistency Improves Chain of
ThoughtReasoninginLanguageModels. arXivpreprint
arXiv:2203.11171,2022b.
Wei,J.,Bosma,M.,Zhao,V.Y.,Guu,K.,Yu,A.W.,Lester,
B., Du, N., Dai, A. M., and Le, Q. V. Finetuned Lan-
guage Models are Zero-shot Learners. arXiv preprint
arXiv:2109.01652,2021.
Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,Chi,E.,Le,
Q., and Zhou, D. Chain of Thought Prompting Elicits
Reasoning in Large Language Models. arXiv preprint
arXiv:2201.11903,2022.
Wu,Y.,Jiang,A.Q.,Li,W.,Rabe,M.N.,Staats,C.,Jamnik,
M.,andSzegedy,C. AutoformalizationwithLargeLan-
guageModels. arXivpreprintarXiv:2205.12615,2022.
PAL:Program-aidedLanguageModels 12
Part I
Appendix
Table of Contents
A AlternativePromptswithoutMeaningfulVariableNames 13
B AdditionalanalysisonArithmeticReasoning 13
C EffectofUsingLanguageModelsofCode 14
D AnalyzingtheEffectofIncreasingNumberofSamplesonPAL 14
E StandardDeviationsAcrossMultipleOrderofPrompts 17
F PALBeyondBenchmarks 17
G CloserLookintoToken-levelBehaviorsofDifferentMechanisms 20
H Datasets 20
H.1 CreatingGSM-HARD............................................ 23
H.2 GSM-HARDAnalysis.............................
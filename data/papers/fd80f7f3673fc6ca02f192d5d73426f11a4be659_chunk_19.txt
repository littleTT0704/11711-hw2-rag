.355 57.0% 0.299 48.6% 0.303 53.1%
PaLM-2UNICORN ✗ 84.3% 0.275 56.1% 0.252 48.3% 0.209 49.8%
FLAN-PaLM-2UNICORN ✗ 69.7% 0.116 54.6% 0.112 43.8% 0.156 47.8%
Finetune
PaLM-2BISON(R) ✓ 88.0% 0.511 61.0% 0.459 51.5% 0.458 59.5%
PaLM-2BISON(GC) ✓ 86.1% 0.400 59.2% 0.444 49.3% 0.365 56.0%
PaLM-2UNICORN(R) ✓ 87.6% 0.508 61.1% 0.412 52.6% 0.460 60.4%
PaLM2BISON(R) ✗ 87.6% 0.490 59.9% 0.439 53.4% 0.437 59.2%
PaLM2BISON(GC) ✗ 86.1% 0.368 57.5% 0.420 47.3% 0.390 54.9%
PaLM2UNICORN(GC) ✗ 86.1% 0.407 57.9% 0.402 45.6% 0.411 55.3%
Table 2: Meta-evaluation results at system and segment-level for the high-resource language pairs.
Finetuned(R)and(GC)representtheregressionandgenerativeclassificationobjectives(§4.2). ✓and✗
representreference-based andreference-lessmetrics,respectively.
System Segmentacc⋆
Gold MQM (+100)
Model Ref? All EN-DE ZH-EN EN-RU 10000 PaLM-2 (Unicorn)
PaLM (540B)
GEMBA FLAN-PaLM-2 (Unicorn)
GPT-3.5 ✓ 85.4% 54.9% 49.5% 47.5% 1000
GPT-4 ✓ 88.7% 57.8% 52.6% 55.0%
GPT-3.5 ✗ 82.5% 56.
[85.74:86.35]
5.2 ImageClassification
To validate our framework across multiple modalities, we consider image classification on the
CIFAR-100dataset[Krizhevsky,2009]. WeuseasthebasemodeltheVisionTransformer(ViT)
[Dosovitskiyetal.,2020],inparticularthebaseversionwith16×16patchesthatwasonlypretrained
onImageNet-21k[Ridniketal.,2021]. Weup-sampleimagestotoa224×224resolution.
Since the self-attention mechanism in the ViT model only works with patch representations, the
explanationsproducedbyattention-basedexplainerswillbeatpatch-levelratherthanpixel-level. We
splittheoriginalCIFAR-100trainingsetintoanewtrainingsetwith45,000andavalidationsetwith
5,000. Unliketheprevioustask,wereusethetrainingsetforboththeteacherandstudent,varyingthe
numberofsamplesthestudentistrainedwithbetween2,250(5%),4,500(10%)and9,000(20%).
Weuseaccuracyasthesimulabilitymetricandtheteacherobtains89%ontestset.
Table3showstheresultsforthethreesettings.Similarlytotheresultsinthetextmodality,theattention
explainertrainedwithSMaTachievesthebestscaffoldingperformance,althoughthegapstostatic
attention-basedexplainersaresmaller(especiallywhenstudentsaretrainedwithmoresamples).Here,
thegradient-basedexplainersalwaysdegradesimulabilityacrossthetestedtrainingsetsizesandandit
seemsimportantthattheexplanationsincludeattentioninformationfromlayersotherthanthelastone.
Plausibilityanalysis. SincetherearenoavailablehumanannotationsforplausibilityintheCIFAR-
100 dataset, we design a user study to measure the plausability of the considered methods. The
originalimageandexplanationsextractedwithGradient×Input,IntegratedGradients,Attention
(all layers), and Attention (SMaT) are shown to the user, and the user has to rank the different
ex
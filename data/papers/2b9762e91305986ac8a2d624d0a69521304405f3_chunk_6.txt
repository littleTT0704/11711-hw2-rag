doesnotsignifi-
POStagging(Figure1d),scoresremainlargelythe cantlyimproveperformanceonothertasks. Fine-
same; performance is lower for some languages tuning on automatically translated task-specific
withnon-Latinscriptsandlow-resourcelanguages. data yields strong gains and is used by most re-
We show the scores for the remaining tasks in centmodelstoachievethebestperformance(Hu
Appendix B. The remaining gap to English per- etal.,2020;Ouyangetal.,2020;Luoetal.,2020).
formanceonthesetasksispartiallyanartefactof Nevertheless,keychallengessuchashowtolearn
theevaluationsetup: zero-shotcross-lingualtrans- robustcross-lingualsyntacticandsemanticprocess-
fer from English favors English representations ingcapabilitiesduringpre-trainingremain.
whereasmodelsfine-tunedonin-languagemonolin-
gualdataperformmoresimilarlyacrosslanguages 3 XTREME-R
(Clarketal.,2020;Huetal.,2020).
InordertoencouragetheNLPcommunitytotackle
Overall,representationsfromtoken-levelMLM challenging research directions in pursuit of bet-
pre-trainingareoflimiteduseforcross-lingualsen- ter cross-lingual model generalization, we pro-
tenceretrieval,asevidencedbythecomparatively pose XTREME-R(XTREME Revisited). XTREME-R
poorperformanceofthemBERTandXLM-Rmod- sharesitspredecessor’scoredesignprinciplesfor
els. Fine-tuning on sentence-level tasks (Phang creatinganaccessiblebenchmarktoevaluatecross-
et al., 2020; Fang et al., 2021) can mitigate this. lingualtransferbutmakessomekeychanges.
Thestrongperformanceofrecentmodelssuchas First,XTREME-Rfocusesonthetasksthathave
VECO and ERNIE-M on the retrieval tasks can proventobehardestforcurrentmultilingualmod-
beattributedtoacombinationofparalleldataand els. Tothisend,itdrops XTREME’sPAWS-Xand
newpre-training
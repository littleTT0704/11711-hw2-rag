 overall 115M
use F or F, and which layer of the underlying
code files from GitHub, further filtered by keep- 1 3
model to extract the encoded tokens from, which
ingonlyfileshavingaveragelinelengthlowerthan
we examine in Section 5. We used F in the hu-
100,morethan25%alphanumericcharacters,and 1
man preference experiments and F in the func-
non-auto-generated files. Even after 1,000,000 3
tionalcorrectnessexperiments. Weperform3-fold
trainingsteps,noneofthemodelshavecompleted
cross-validation and report average results across
even a single epoch, meaning that every training
the three folds. As for the layer to extract the to-
examplewasseenonlyonceatmost.
ken vectors from, we used layer 7 for CoNaLa,
3.2 ComparingDifferentMetrics andinHumanEvalweusedlayer7forJava,10for
C++,11forJavaScript,and9forPython.
We compare CodeBERTScore with existing met-
rics that are commonly used on code generation
4 Results
evaluation. We use human annotated preference
and execution-based results as the ground truth Correlation with human preference Table 2
andmeasuretheircorrelationwiththesemetrics. shows the correlation between different metrics
2https://github.com/Tiiiger/bert_score 4https://huggingface.co/datasets/nuprl/MultiPL-E
3https://huggingface.co/datasets/codeparrot/ 5https://octoverse.github.com/2022/
github-code-clean top-programming-languages
Java C++ Python JavaScript
Metric τ r τ r τ r τ r
s s s s
BLEU.481.361.112.301.393.352.248.343
CodeBLEU.496.324.175.201.366.326.261.299
ROUGE-1.516.318.262.260.368.334.279.280
ROUGE-2.525.315.270.273.365.322.261.292
ROUGE-L.508.344.258.288.338.350.271.293
METEOR.
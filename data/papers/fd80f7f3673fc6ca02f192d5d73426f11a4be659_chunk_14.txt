izationimprovements. Wemainly
containsanequalnumberoftrainingexamples. We
usetwomodelsizesinthefamily: PaLM-2BI-
SON and(thelarger)PaLM-2-UNICORN.4 In thenmaplabelstoverbalratingsfromthefollow-
ingset,basedontheirbucket: ["verybad","bad",
additionweexploretheimpactofinstruction-
"ok","good","verygood"]. Toevaluatethemodel,
tuningbyusinga UNICORNmodelfinetuned
predictionsaremappedbacktointegerlabelsfrom
ontheFLANdataset(Weietal.,2021).
1to5. Anypredictionsnotcontainingasubstringin
For score prediction, we compare PaLM and thelabelsetareconsideredinvalidandaremapped
PaLM-2againsttheGPTfamilyofLLMs(Brown to0. WeexperimentedwithfinetuningonbothDA
et al., 2020; OpenAI, 2023) by leveraging the and MQM 2020 (Freitag et al., 2021a) data, and
results and outputs from the GEMBA evaluator foundthatthelatterperformedslightlybetter.
(KocmiandFedermann,2023). Wethenevaluate To assess the impact of model size, we also
theperformanceofAUTOMQMwithonlyPaLM-2 finetunetwoadditional(smaller)PaLM-2models,
models(whichperformedbestinscoreprediction). whichwecallS andM,comparingtheirfinetuned
Additionally, for the high-resource languages, andzero-shotperformance.6
we compare to a set of strong baseline eval-
MetricMeta-Evaluation Thequalityofanau-
uation metrics, MetricX-XXL and COMET-22,
tomaticevaluationmetricisestimatedbycompar-
whichwerethetwotop-performingmetricsinthe
ing the agreement between the metric scores and
WMTâ€™22MetricsSharedTask. MetricX-XXLand
ground-truth quality scores on a large number of
COMET-22arebothfinetunedregressionmodels
translationsfromdifferentMTsystems,ap
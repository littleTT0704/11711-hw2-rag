Net-101,
Inference.Theimageisresizedto640Ã—360duringinference. outperforming the previous start-of-art method PCAN [26] by
Toobtainthefinalobjectsegmentationresult,wefirstbinarize a large margin of 3.2 mAP. The recent method HITF [30]
8
Fig.7. Qualitativecomparisontothestate-of-the-artonlinevideoinstancesegmentationmethodCrossVIS[68]ontheYoutube-VIS-2019validationset.
1 in Figure 7. The result indicates that CrossVIS fails to detect
or track instances in occluded scenarios, while our methods
successfully maintain robust high-quality segmentation and
0.5
tracking performance. More qualitative results are shown in
thesupplementarymaterial.AsshowninFigure8,wecompare
0
PCA Head 1 Head 2 Head 3 Head 4 the reference-to-target (R2T) attention map in the transformer
encoderusingfullandcompressedreferencetokens.ThePCA
Fig.8. Visualizationofreference-to-targetattentionmap.Theattention is conducted among all 8 heads of R2T attention map and
mapsfromcompressedtokensaremorerepresentative. keeps three main components for RGB channels. We show
the first 4 heads of the R2T attention. The attention maps
of compressed tokens are sharper in the instance region than
leverages a stronger backbone with a higher resolution input, those of full tokens, which can also be verified by the PCA
we compare it with our results using the Swin-B backbone map.
for fairness. Our method outperforms HITF by 3.6 mAP with Inference time. Online VIS is mainly for streaming applica-
a faster inference speed. (2) Compared to offline methods: tions. In the video streaming pipeline, the input video is re-
Ourmethodeclipsesanothertransformer-basedmethodVisTR ceivedframebyframe,andlatency(includingvideostreaming
[60] even if it takes video-level input. The results of Seq- andmodelprocessingtime)istheessentialtimemeasurement.
Former [63] is imported from preprint, which uses a stronger Wefurtherdiscussthelatencyofbothonlineandofflin
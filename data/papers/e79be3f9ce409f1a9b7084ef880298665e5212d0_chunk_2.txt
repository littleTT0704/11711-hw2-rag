 localization
answering[44,27]andvideocaptioning[58].
(CrossTask),videoactionsegmentation(COIN).Theresults
In this paper, we present a new variant of con-
showthatourmodelsattainconsistentimprovementsacross
trastive learning, Token-Aware Cascade contrastive learn-
different experimental settings over previous methods, set-
ing (TACo) to improve the video-text alignment for both
tingnewstate-of-the-artonthreepublictext-videoretrieval
large-scale pretraining and downstream specific tasks. As
benchmarksofYouCook2,MSR-VTTandActivityNet.
the name indicates, TACo makes two modifications to the
conventional contrastive learning used in video-language
1.Introduction domain. Thefirstisthetoken-awarecontrastivelosswhich
iscomputedbytakingintoaccountthesyntacticclassesof
Aligningorgroundinglanguagetovideosisachalleng- words. This is motivated by the observation that, given
ing topic in the context of vision-language (VL) research a video and its corresponding text, content words, such
as it requires the model to understand contents, dynamics, as nouns and verbs, are more likely than function words
and causality presented in videos [3]. Inspired by the suc- to be aligned with (or grounded to) visual contents in the
cessofBERT[10]innaturallanguageprocessing,thereisa video. Conventionalcontrastivelearningtypicallycompute
growinginterestinapplyingtransformer-basedmulti-modal thelossafteraggregatingoverallthewordsinthetextand
models for video-text alignment and representation learn- frames in the video (loss L or L in Fig. 1). In contrast,
1 3
ing [41, 40, 60, 33, 14, 28]. These models are typically the token-aware contrastive loss is computed using only a
pretrainedonlargeamountsofnoisyvideo-textpairsusing subset of words whose syntactic classes belong to a pre-
contrastive learning [35, 34], and then applied in a zero- definedset(e.g.,nounsandverbs),whichforcestheground-
shot manner or finetuned for various downstream tasks, ingofindividualwordstothe
 CODEX code-davinci-002
monsense generation tasks: (i) script genera- and4000forGPT-3text-davinci-002),the
tion (PROSCRIPT, Section 3.2), (ii) entity state
exactvalueofk istaskdependent: moreexamples
tracking(PROPARA,Section3.3),and(iii)explana-
canfitinapromptintaskswhere(T,G)isshort.
tiongraphgeneration(EXPLAGRAPHS,Section3.4)
Inourexperiments,kvariesbetween5and30,and
DatasetdetailsareincludedinAppendixD.Despite
theGPT-3baselineisalwaysfairlygiventhesame
sharing the general goal of structured common-
prompts as CODEX. To control for the variance
sensegeneration,thethreetasksarequitediverse
caused by the specific examples selected into p,
in terms of the generated output and the kind of
werepeateachexperimentwithatleast3different
requiredreasoning.
prompts, and report the average. We report the
meanandstandarddeviationsinAppendixI.
3.1 Experimentalsetup
COCOGEN: Weuse COCOGEN torefertose-
Model AsourmainCode-LLMforCOCOGEN, tupswhereaCODEXisusedwithaPythonprompt.
we experiment with the latest version of CODEX
In Section4,wealsoexperimentwithdynamically
code-davinci-002fromOpenAI1
infew-shot
creating a prompt for each input example, using
promptingmode.
a NL-LLMs with code prompts, and using Code-
LLMswithtextualprompts.
Baselines We experimented with the following
typesofbaselines:
3.2 Scriptgeneration: PROSCRIPT
1. Text few-shot: Our hypothesis is that code- Given a high-level goal (e.g., bake a cake), the
generationmodelscanberepurposedtogen- goal of script generation is to generate a graph
erate structured output better. Thus, natural where each node is an action, and edges cap-
baselines for our approach are NL
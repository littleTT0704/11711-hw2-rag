 since we add audio data serially with
the weight in the token fusion. fixed reference audio frames, it is hard to construct a long-
term correlation between audios thus the sound event cannot
be fully utilized. To explore the statistical robustness of the
C. Audio-Visual Instance Segmentation Results
gain, we conduct a t-test between corresponding experiments.
AsshowninTableXII,the p-valueindicatesthegainsarenot
Method Backbone AP AP50 AP75 AR1 AR10
statisticallysignificant.Therefore,wecansafelyconcludethat
CrossVIS ResNet-50 42.2 59.1 47.4 42.1 49.9
CrossVIS ResNet-101 44.7 60.4 50.5 42.0 49.7 whiletheaudiomaypotentiallybenefitthevideosegmentation
Ours ResNet-50 46.6 63.3 51.3 44.2 51.5 task,inthecurrentsetting,theireffectinunconstrainedvideos
Ours ResNet-101 48.6 63.9 52.5 44.6 52.9
is limited.
TABLEIX
COMPARISONTOSTATE-OF-THE-ARTVIDEOINSTANCESEGMENTATIONON
AVISVALSET. V. CONCLUSION
We propose a robust context fusion module for the online
WereporttheperformanceofusingResNet-50andResNet- VIS task, which corresponds and fuses compact and effective
101 backbone on AVIS dataset. As shown in Table XI, we reference features to the target features in a transformer
also compare the CrossVIS baseline which only leverages the encoder. We observe that the importance-aware compression
emarF
oiduA
oidua
w
ksaM
oidua
o/w
ksaM
11
for reference context is critical in the online setting because [16] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson,
the impact of frames are different for the target prediction. AvinatanHassidim,WilliamTFreeman,andMichaelRubinstein.Look-
ing to listen at the cocktail party: A speaker-independent audio-visual
In addition, we leverage an order-preserving instance code
modelforspeechseparation. arXiv
Zhangetal.,2023) Codex(175B) Clustering Language Auto-generated -
Complexity-CoT(Fuetal.,2023) GPT-3(175B) Complexity Language Hand-crafted Self-consistency
Few-shot-PoT(Chenetal.,2022b) GPT-3(175B) Random Code Hand-crafted -
Table4: In-contextlearningwithlargelanguagemodelsformathematicalreasoning. ForGPT-3,allpapersusethe
text-davinci-002version;forCodex,allpapersusethecode-davinci-002. RLisshortforreinforcementlearning.
cusontwoaspects,(i)hand-craftingmorecomplex ahigherdegreeofdiversity.
demonstrations,whichwerefertoasprocess-based
6 DiscussionandFindings
approaches(Zhouetal.,2023;Chenetal.,2022b),
(ii) leveraging ensemble-like methods, which we
6.1 AnalysisofBenchmarks
refertoasoutcome-basedapproaches(Wangetal.,
The multi-modal setting is underexplored but
2023;Lietal.,2022a).
is gaining increasing attention. Most existing
Process-based approaches aim to improve the benchmarksformathematicalreasoninghavetar-
chain-of-thoughtreasoningquality,especiallyfor geted the textual-only modality. However, visual
complexreasoningtasks. Inleast-to-mostprompt- elementscanprovidearichsourceofquantitative
ing (Zhou et al., 2023), the problem-solving pro- information, making multi-modal datasets bene-
cessisimplementedthroughtwo-stageprompting: ficial for reasoning over quantitative relations in
(i)reducingacomplexproblemintoalistofsub- naturalimages(Luetal.,2022a),abstractdiagrams
problems;(ii)solvingthesesub-problemssequen- (Luetal.,2021b),figures(Kahouetal.,2018),and
tially, so that solving a given sub-problem is fa- charts(Kafleetal.,2018). Tables,whicharecom-
cilitatedbythe
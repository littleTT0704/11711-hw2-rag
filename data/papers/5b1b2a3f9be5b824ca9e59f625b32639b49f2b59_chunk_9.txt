 framework is
𝒪′𝑡𝑔𝑡 𝒪′=𝒪′ 𝑡𝑔𝑡⨁𝒪′𝑟𝑒𝑓⨁𝒪′𝑎𝑢𝑑 detailed in Section III-C.
⋯ ⋯ B. Online Video Instance Segmentation
Since our method performs online inference, we process
Transformer Encoder one target frame I at each iteration with additional reference
t
frames {I }t−1. The target features f are extracted by
r r=t−δ t
a shared backbone, while reference features {f }t−1 are
⋯ ⋯ r t−δ
obtained from previous iterations. Note that feature extraction
Tgt. token𝒪𝑡𝑔𝑡 Ref. token𝒪𝑟𝑒𝑓 Audio token𝒪𝑎𝑢𝑑
for each frame is performed only once in the online inference
𝒫 Positional encoding 𝒫 𝒫
stage,thussavingprocessingtime.Theextractedvideofeature
ofbothtargetframeandreferenceframesaresenttotherobust
𝜑𝛿∙𝐶↦𝐶′ context fusion module for further interaction.
𝜙𝐻×𝑊↦𝐾×𝐾
𝜑 𝐶𝑎↦𝐶′ 1) Robust Context Fusion (RCF)
𝜑 𝐶↦𝐶′ 𝑊 We fuse the context information using compact and rep-
bi-LSTM
resentative visual tokens for target and reference frames by
taking their importance into consideration. The structure of
RCF is illustrated in Figure 3.
Target frame feature Reference frame feature Audio feature(optional) Target token. Since the target frame contains the most im-
𝑓𝑡 𝑓 𝑡𝑟𝑒𝑓=𝑓𝑡−1⨁…⨁𝑓𝑡−𝛿 𝑓𝑡𝑎𝑢𝑑=�
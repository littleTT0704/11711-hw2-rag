 framework is
ğ’ªâ€²ğ‘¡ğ‘”ğ‘¡ ğ’ªâ€²=ğ’ªâ€² ğ‘¡ğ‘”ğ‘¡â¨ğ’ªâ€²ğ‘Ÿğ‘’ğ‘“â¨ğ’ªâ€²ğ‘ğ‘¢ğ‘‘ detailed in Section III-C.
â‹¯ â‹¯ B. Online Video Instance Segmentation
Since our method performs online inference, we process
Transformer Encoder one target frame I at each iteration with additional reference
t
frames {I }tâˆ’1. The target features f are extracted by
r r=tâˆ’Î´ t
a shared backbone, while reference features {f }tâˆ’1 are
â‹¯ â‹¯ r tâˆ’Î´
obtained from previous iterations. Note that feature extraction
Tgt. tokenğ’ªğ‘¡ğ‘”ğ‘¡ Ref. tokenğ’ªğ‘Ÿğ‘’ğ‘“ Audio tokenğ’ªğ‘ğ‘¢ğ‘‘
for each frame is performed only once in the online inference
ğ’« Positional encoding ğ’« ğ’«
stage,thussavingprocessingtime.Theextractedvideofeature
ofbothtargetframeandreferenceframesaresenttotherobust
ğœ‘ğ›¿âˆ™ğ¶â†¦ğ¶â€² context fusion module for further interaction.
ğœ™ğ»Ã—ğ‘Šâ†¦ğ¾Ã—ğ¾
ğœ‘ ğ¶ğ‘â†¦ğ¶â€² 1) Robust Context Fusion (RCF)
ğœ‘ ğ¶â†¦ğ¶â€² ğ‘Š We fuse the context information using compact and rep-
bi-LSTM
resentative visual tokens for target and reference frames by
taking their importance into consideration. The structure of
RCF is illustrated in Figure 3.
Target frame feature Reference frame feature Audio feature(optional) Target token. Since the target frame contains the most im-
ğ‘“ğ‘¡ ğ‘“ ğ‘¡ğ‘Ÿğ‘’ğ‘“=ğ‘“ğ‘¡âˆ’1â¨â€¦â¨ğ‘“ğ‘¡âˆ’ğ›¿ ğ‘“ğ‘¡ğ‘ğ‘¢ğ‘‘=ï¿½
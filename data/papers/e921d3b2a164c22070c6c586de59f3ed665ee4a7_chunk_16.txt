parabilityofdatamodesbyfinding
nificantlyoutperformLR,andcancompetewithnonlinear
thethresholdslike−1.9,5.6,−6.4,etc.
modelslikeSVR,RFandMLP.Thisvalidatesourhypoth-
Table2showscomprehensiveresultsforallthemethodson esisthatwithourdatamodality-awareroutingmechanism
allthedatasets. Weobserveanoverallimprovementofour simple leaf experts can make good predictions. At last,
HRMEmethodsoverthebaselinemethods. Specifically,for TheMLPperformspoorlyinmostofthetasksevenwith
largedatasetslikeEngeryandKin40k,ourmethodsoutper- fine-tuning. ThisshowsthatMLPisnotabletocapturethe
formallotherbaselinesintermsofbothbias(MAE)and complexmodalityofdatadistributions.
variance (RMSE) even for the HME models with strong
Tothispoint,comprehensiveexperimentresultsshowthat
GaussianprocessexpertsandtheMLP.Formedium-sized
our HRME methods perform well on a wide range of re-
datasets like CCPP and Concrete our methods generally
gressiontasks, especiallyonlarge, high-dimensionaland
outperformotherbaselinesexceptRF.Butasanensemble
difficult datasets. Our HRME methods can capture the
method like RF, our method can also be boosted (now is
complex data hierarchy, reduce variance, and make good
averaged) to improve performance (Avnimelech & Intra-
predictions with simple leaf experts. We further explore
tor,1999). ForsmalldatasetlikeHousing,ourmethodsdo
sometheoreticalpropertiesofourHRMEmodel.
not outperform DT and RF. But at a closer look we find
thatHRME-LRyieldsmuchsmallerMAEandRMSEthan 1usingGaussianexperts;resultstakenfromFerrari&Milioni
HRME-SVRandisonaparwithDTandRF.Thisobser- (2011)
y
y
y
y
y
y
Hier

InFigure4,weobservethatframeworksfromall
andimageclassificationtasks(Reddietal.,2020;
executionparadigmsexhibitframeworkboundbe-
JanapaReddietal.,2022). Weevaluatemodelar-
haviors. However,deferredexecutionTorchScript
chitecturesfornaturallanguagegeneration,speech,
and static ONNX Runtime, which support com-
and vision and show that they all observe frame-
putational graph compilation (e.g. operator fu-
workboundbehaviorinAppendicesC,DandE.
sion),exhibitlessframeworkoverheadandprovide
Analysis The number of FLOPs required for speedupsovereagerPyTorch. Theseincreasesare
model inference scales with the input batch size especially pronounced at low batch sizes where
and sequence length. As such, one would expect inference is framework-bound. For batch size 1,
thatlatencyscalesaccordinglyaswell. However, TorchScriptandONNXprovideanaverageFP16
as seen in Figure 4, models exhibits framework speed up of 34.16% and 71.38% over PyTorch,
boundednessforbothsmallsequencelengthsand respectively. As batch sizes increase and models
batchsizes,wherelatencyisconstantregardlessof become compute bound, there is minimal differ-
inputsize. enceinlatencyacrossframeworksasthemajority
Computation with mixed and half precision ofexecutiontimeisspentonkernelexecution.
(FP16) often increases training throughput over Additionally, we consider both static, serial-
single precision (FP32), we observe that frame- izedCUDAgraphsandPyTorchBetterTransformer
work overhead results in latency bottlenecks re- frameworkoptimizationsinFigure5. BetterTrans-
gardlessoftheprecisionduringinference. Ashalf formerprovidesspeedupsthroughadditionalkernel
precisioncomputationisfasterduetoreduceddata fusionandsparsetensoroperationsthattakeadvan-
movement,GPUkernelexecutiontimetakeslonger tageofsparsesequencelengthsandpaddingtokens
)s(
ycnetaL
10 1
PyTorch
Nested Tensors
CUDA Graphs
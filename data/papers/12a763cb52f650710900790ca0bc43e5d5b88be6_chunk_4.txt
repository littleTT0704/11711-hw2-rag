 sequence. Intuitively, each statement
both zero-shot and finetuned models on numeri-
containsinformationthatishelpfulforanswering
calcommonsense(NumerSense(Linetal.,2020)),
thequestion(e.g. Table1).
generalcommonsense(CommonsenseQA(Talmor
Thesecondstepisknowledgeintegration,where
etal.,2019),CommonsenseQA2.0(Talmoretal.,
generatedknowledgeisintegratedintothedecision
2021)),andscientificcommonsense(QASC(Khot
processofalanguagemodelusedforinference,
et al., 2020)) benchmarks, setting a new state-of-
the-art on three of these datasets. It outperforms aˆ = argmaxp (a|q,K )
I q
thetemplate-basedknowledgegenerationmethod a∈Aq
self-talk (Shwartzetal.,2020),whileperforming
In contrast, the vanilla setting of using the infer-
comparablytoretrieval-basedsystems.
ence model without knowledge is represented by
We find three factors contribute to the perfor-
aˆ = argmax p (a|q).
manceofgeneratedknowledgeprompting: (i)the
a∈Aq I
Next,wedescribetheknowledgegenerationand
quality of knowledge, (ii) the quantity of knowl-
integrationstepsindetail.
edgewheretheperformanceimproveswithmore
knowledge statements, and (iii) the strategy for 2.1 KnowledgeGeneration
integratingknowledgeduringinference. Ourquali- We generate question-related knowledge state-
tativeanalysissuggeststhatthegeneratedknowl- mentsbypromptingalanguagemodel. Theprompt
edgestatementscoveravarietyoftypes,andcan consistsofaninstruction,afewdemonstrationsthat
transformcommonsensequestionansweringtoex- arefixedforeachtask,andanew-questionplace-
plicitreasoningprocedures,e.g. deduction,thatare holder. Thedemonstrationsarehuman-written,and
supportedbyoff-the-shelfandfinetunedlanguage eachconsistsofaquestioninthestyleofthetask
models. and a knowledge
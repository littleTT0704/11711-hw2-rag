elf-distillationframeworkcalledIGSDforself- tobeincorporatednaturallylikeintroducingsupervisedcontrastive
supervisedgraph-levelrepresentationlearningwheretheteacher- loss[26]intotheframeworkforlearningpowerfulrepresentations.
studentdistillationisperformedforcontrastinggraphpairsunder GraphContrastiveCoding(GCC)[38]isapioneertoleveragein-
differentaugmentedviewsusinggraphdiffusion. stancediscriminationasthepretexttaskforstructuralinformation
• WefurtherextendIGSDtothesemi-supervisedscenarios,where pre-training.GraphCL[54]aimsatlearninggeneralizable,trans-
thelabeleddataareutilizedeffectivelywiththesupervisedcon- ferrable,androbustrepresentationsofgraphdatainanunsuper-
trastivelossandself-training. visedmanner.However,ourworkisfundamentallydifferentfrom
• WeempiricallyshowthatIGSDsurpassesstate-of-the-artmeth- theirs.GCCaimstofindcommonandtransferablestructuralpat-
odsinsemi-supervisedgraphclassificationandmolecularprop- ternsacrossdifferentgraphdatasetsandthecontrastiveschemeis
ertypredictiontasksandachievesperformancecompetitivewith donethroughsubgraphinstancediscrimination.GraphCLfocuses
state-of-the-artapproachesinself-supervisedgraphclassification ontheimpactofvariouscombinationsofgraphaugmentationson
tasks. multipledatasetsandstudiesunsupervisedsettingonly.Onthe
contrary,ourmodelaimsatlearninggraph-levelrepresentationin
bothunsupervisedandsemi-supervisedsettingsbydirectlycon-
2 RELATEDWORK trastinggraphinstancessuchthatdataaugmentationstrategies
GraphRepresentationLearningTraditionally,graphkernelsare andgraphlabelscanbeutilizednaturallyandeffectively.
widelyusedforlearningnodeandgraphrepresentations.Thiscom- KnowledgeDistillationKnowledgedistillation[21]isamethod
monprocessincludesmeticulousdesignslikedecomposinggraphs fortransferringknowledgefromonearchitecturetoanother,allow-
intosubstructuresandusingkernelfunctionslike
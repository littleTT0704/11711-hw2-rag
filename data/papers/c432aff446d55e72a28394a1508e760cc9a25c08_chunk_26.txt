augmentedretrieval. arXivpreprintarXiv:2201.12431,2022.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450,2016.
AlexeiBaevskiandMichaelAuli. Adaptiveinputrepresentationsforneurallanguagemodeling. arXivpreprint
arXiv:1809.10853,2018.
YoshuaBengio,RéjeanDucharme,PascalVincent,andChristianJauvin. Aneuralprobabilisticlanguage
model. Journalofmachinelearningresearch,3(Feb):1137–1155,2003.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improv-
inglanguagemodelsbyretrievingfromtrillionsoftokens. InInternationalconferenceonmachinelearning,
pages2206–2240.PMLR,2022.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
arXivpreprintarXiv:2005.14165,2020.
AaronClauset, CosmaRohillaShalizi, andMarkEJNewman. Power-lawdistributionsinempiricaldata.
SIAMreview,51(4):661–703,2009.
DavidDemeter,GregoryKimmel,andDougDowney. Stolenprobability: Astructuralweaknessofneural
language models. In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics,pages2191–2197,2020.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.BERT:Pre-trainingofdeepbidirectional
transformersforlanguageunderstanding. arX
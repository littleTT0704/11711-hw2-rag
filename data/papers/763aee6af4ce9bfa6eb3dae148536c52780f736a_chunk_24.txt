 the semanticfactor. The variation factor more similar results). The results show that the represen-
is then processed by four residual blocks and four convo- tations are disentangled with respect to various variation
lutional layers with these AdaIN parameters. Finally, the factorslikebackground,coloretc,whichsupportsdiverse
processed latent vector is decoded to the image space by manipulations for enriching training datasets. Moreover,
upsamplingandconvolutionallayers. ThediscriminatorD DDG captures proper semantics over data, which allows
followsthepopularmulti-scalePatchGAN[36]onthreein- diversemanipulationsonvariationfactorslikeobjectcolors
putimagescales:14×14,28×28and56×56.Thegradient and backgrounds without changing object semantics. For
punishment[53]isalsoappliedwhenupdatingD. example,DDGchangesthecolorofadoginthefirstpanel
SeeAppendixCforfulldetailsofallexperimentalset- ofFig.3butretainsthesamecolorofitsnoseandeyes.Such
disentanglementenablesflexibleandcontrollablegeneration Convergence analysis.
4 ERM
bymanipulatingsemanticandvariationfactorsviaswapping Weinvestigatethetraining IRM
CDANN (Fig.3)orinterpolation(Fig.2). dynamicsofDDGandsev- 3 DANN
Interpolation details. In order to better understand eralbaselinesoverWILDS, GDRO
2 DDG (ours)
thelearnedsemanticandvariationrepresentations,wefur- wherethetargetdomainis
ther perform a linear interpolation experiment between d 5. Thelearningcurvesin 1
two variation factors and generate the corresponding im- Fig.5showthatdomainad-
0
ages as shown in Fig. 2. We denote the variation code versarial training methods 1000 2000 3000 4000 5000
Number of iteration
of the first and second column as h v(x˜),h v(x), respec- like DANN, CDANN are Figure5.Convergencecomparison.
tively. The images from 4 − 13 column are generated unstable and hard to con-
by D(h (x) ⊕ (i × h (x) + (1
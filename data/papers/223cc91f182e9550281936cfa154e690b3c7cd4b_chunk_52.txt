 a
broader class of divergences, such as Jensen-Shannon divergence and Wasserstein distance. It thus presents a
promising venue for future research to develop more generic solvers for the broad learning problems
characterized by SE.
On the other hand, as can be seen in the student step discussed shortly, sometimes we do not necessarily need a
closed-form teacher
q(n+1)
in the learning, but only need to be able to draw samples from
q(n+1).
Probability functional descent. Generally, the SE (Equation 3.1 or 3.2) defines a loss over the auxiliary
distribution q, denoted as J(q), which is a functional on the auxiliary distribution space Q(T ). The Gâteaux
derivative of J at q, if exists, is defined as (Fernholz, 2012):
J(q+ϵ(h−q))−J(q)
J′(h−q) = lim (7.2)
q ϵ→0+ ϵ
38
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
for any given h ∈ Q(T ). Intuitively, J′(h−q) describes the change of the J(q) value with respect to an
q
infinitesimal change in q in the direction of (h−q). The Gâteaux derivative J′(h−q) can alternatively be
q
computed with the influence function of J at q, denoted as ψ
q
: T → R, through:
J q′(h−q) =
∫
ψ q(t)(h−q)dt =Eh[ψ q(t)]−Eq [ψ q(t)]. (7.3)
t
The above notions allow us to define gradient descent applied to the functional J. Concretely, we can define a
linear approximation to J(q) around a given q :
0
J(q) ≈J(q )+J′ (q−q )
0 q0 0
=J(q 0)+Eq [ψ q0(t)]−Eq0 [ψ q0(t)] (7.4)
=Eq [ψ
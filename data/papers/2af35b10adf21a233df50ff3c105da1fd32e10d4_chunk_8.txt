. We run our
experimentsforonlyfouremotions:angry,happy,sadandneu-
tral.
4.1.1. ActedData
The acted dataset used in the experiment is IEMOCAP [20].
Itconsistsoftensessions,eachofwhichisaconversationbe-
tween two actors. The conversations are divided into labeled
Figure3: Totalphonemedistributionsunderbothnaturaland
sentences. We implement a 10-fold cross validation training
acteddataset
setup. In each fold, data from 9 speakers is used for training
themodelanddatafrom1speakerisusedfortesting.Thedata
consistsof1103angry,1636happy,1708neutraland1084sad
utterances. Theaveragedurationofutterancesinthisdatasetis 5. AnalysisandResults
4seconds.
We base our results on the output of the attention mechanism
4.1.2. NaturalData from the neural model described earlier. Since the model is
trainedforemotionclassificationoverfouremotions,itisuse-
For natural-speech, we used the CMU-SER data [21]. This
fultonotetheclassificationaccuraciesachievedbythemodel
datasethasbeencollectedfromNPRpodcasts[22],andtelevi-
overthetwodatasets. Ontheacteddata, weachieveaclassi-
sionprogramshostedbytheInternetArchive[23]. Thedataset
ficationaccuracyof72%andonthenaturaldata,weachievea
is annotated using the Amazon Mechanical Turk [24]. It has
classificationaccuracyof52.4%.
6000 utterances in the training set and 2571 utterances in the
To perform analysis on the attended-phonemes, for each
test set, with a total of 1099 angry, 3028 happy, 1262 neu-
emotionweaggregatethephonemeswiththehighestattention
tral,and611sadutterances.Theaveragedurationofutterances
output.Wenormalizetheirfrequenciesbythetotalfrequencyof
in this dataset is 5 seconds. Further details of the CMU-SER
thephonemeinthedata.Fig
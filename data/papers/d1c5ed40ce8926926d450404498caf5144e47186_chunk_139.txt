ansion 86.37% 75.51% 87.84%
% Gain +5.2% +11.0% +3.7%
# Gain/Loss +206/-56 +78/-19 +24/-10
Table 6.14: Candidate recall of Watson on Jeopardy! and TREC questions when
using sources that were expanded with web search results. For each setup, we show
the percentage gain and the number of questions gained/lost. All improvements are
significant with p <.01 based on a one-sided sign test.
Regular J! Final J! TREC 11
Wikipedia 95.8 97.3 84.8
Expansion 96.2 97.2 84.9
Wiktionary 95.2 96.1 78.3
Expansion 96.9 96.7 78.9
All Sources 95.9 98.7 84.5
Expansion 95.9 98.0 84.3
Table 6.15: Average number of candidates returned by Watson for Jeopardy! and
TREC questions when using sources that were expanded with web search results.
We did not use a fixed cutoff point when measuring candidate recall but instead
judged all candidate answers returned by Watson. The number of candidates varies
between questions and source corpora, but we did not systematically increase it
through SE. This can be seen in Table 6.15, which shows the average number of
candidate answers returned per question for all source corpora and datasets. These
average values are very similar in the baseline experiments and the experiments with
source expansion. This is important because candidate recall can easily be increased
by considering more candidates, but then the answer selection task becomes more
challenging and QA accuracy may be hurt. Note that Watson often generates hun-
dreds of candidate answers per question, but once the candidates have been scored
and ranked, the list is cut off at 100 candidates and similar answers are merged. Thus
the candidate numbers reported in Table 6.15 are all below 100.
Table 6.16 shows Watsonâ€™s QA accuracy for the same sources and datasets. Again
statistical source expansion yields consistent gains across all baselines and question
6.4. END-TO-END EXPERIMENTS 101
Regular J! Final J! TREC 11
Wikipedia 58.49% 38.58% 50.23%
Expansion 64.94% 47.46%
.
5 RelatedWork rectionistogroundthequestionintoaknowledge
graphanddoinferencewithgraph-basedreasoning
Knowledgecanbeelicitedfrompretrainedlan-
(Lin et al., 2019; Lv et al., 2020; Yasunaga et al.,
guagemodels. Numerousworkshaveshownthat
2021).
pretrained language models implicitly contain a
large amount of knowledge that can be queried A common prerequisite of these methods is a
via conditional generation (Davison et al., 2019; high-quality, high-coverage, in-domaincommon-
Petroni et al., 2019; Jiang et al., 2020). Conse- sense knowledge base (Ma et al., 2019). Some
quently, these models can directly perform infer- commonsensereasoningdatasetsarederivedfrom
enceontaskslikecommonsensereasoning(Trinh existingknowledgebases;forexample,Common-
and Le, 2018; Yang et al., 2020), text classifica- senseQA (Talmor et al., 2019) is derived from
tion(Shinetal.,2020;PuriandCatanzaro,2019), ConceptNet (Speer et al., 2017), and Social IQA
andnaturallanguageinference(Shinetal.,2020; (Sapetal.,2019b)isderivedfromATOMIC(Sap
SchickandSch√ºtze,2021). Inspiredbytheseobser- et al., 2019a). For such datasets, it is natural to
vations,weelicitquestion-relatedknowledgeinan elicitrelatedknowledgefromtheunderlyingknowl-
explicitformfromlanguagemodelsandusethem edge base that derived them, and typically this
toguidetheinference. woulddemonstrateconsiderablegains(Mitraetal.,
Leveraging external knowledge for common- 2019; Chang et al., 2020). However, if there is
sensereasoning. Someworkusesexternalcom- a domain mismatch between the dataset and the
monsenseknowledgebasestomakeimprovements knowledgebase,suchgainstendtodiminish(Mi-
onvariousNLPtasks,includingcommonsenserea- traetal.,2019;Maetal.,2019). Thisbecomesa
soning. One approach is to inject commonsense bottleneck when
9] 2.9
splits. Given that 100 generations take approxi-
Table6: HyperparameterstestedandusedforMARCO mately30seconds,ourusageis0.6GPUhours.
onSBF CondBERT uses BERT-base, which includes
110Mparameters.
Asabove,wegoover648hyperparametercom-
ParaGeDi We use greedy decoding for Par-
binationsbeforechoosingabestsettorunonour
aGeDi and use the same hyperparameters as
testset. Intotal,werewriteapproximately65,000
MARCOforeachdataset,forfaircomparison. Ta-
sequences. Since 100 generations take about 30
ble9liststhesoleParaGedi-specifichyperparam-
seconds,weuseapproximately5.4GPUhours.
eter we modify: we do not generate and rerank
multiplesequencesforfairness.
DynaHate We perform a search jointly over
different hyperparameter values on the develop-
mentset. Wechoosethehyperparametercombina- Hyperparameter Assignment
tionthatperformsbestonautomaticmetrics,shown generatemultipleseqsandrerank false
inTable7,andusethistogenerateonthetestset.
Table9: HyperparametersusedforParaGeDi
Hyperparameter Tested Assignment
Weperformapproximately5000rewritesacross
repetitionpenalty [1.0,1.2,1.5] 1.0
α 1 [0.5,1.0,1.5] 1.5 alldatasetsandsplits. Giventhat100generations
α 2 [4.0,4.25,...,5.0] 4.75 take approximately one minute, our usage is 0.8
temperature(basemodel) [0.9,1.7,2.5] 2.5
GPUhours.
Table7: HyperparameterstestedandusedforMARCO ParaGediusesT5-baseasaparaphrasingmodel,
onDynaHate with220Mparameters,inconjunctionwithafine-
tunedGPT2-mediumdiscriminator,with355Mpa-
rameters.
Weiterateoverasmaller
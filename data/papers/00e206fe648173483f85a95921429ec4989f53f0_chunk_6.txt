andNg,2020),wehypoth-
ativespeedofmentionversusfullcoreferencean-
esizethatmentiondetectiontransferplaysanim-
notation. Our results suggest, assuming a fixed
portantroleinoverallcoreferencetransferacross
annotationbudget,coreferencemodelscapableof
domains. To test this hypothesis, we conduct a
adaptingtoanewdomainusingonlymentionan-
preliminaryexperiment,examininghowfreezing
notationscanleverageacorpusofapproximately
the antecedent linker affects overall performance
twiceasmanyannotateddocumentscomparedto
inthecontinuedtrainingdomain-adaptationsetting
modelsthatrequirefullcoreferenceannotations.
describedabove. Wetrainac2fmodelwithaSpan-
Werecruited7in-houseannotatorswithaback-
BERTencoder(Joshietal.,2020)onOntoNotes,
ground in NLP to annotate two tasks for the
a standard coreference benchmark, and evaluate
i2b2/VAdataset. Forthefirstmention-onlyannota-
performance over the i2b2/VA corpus, a domain-
tiontask,annotatorswereaskedtohighlightspans
specificcoreferencedatasetconsistingofmedical
correspondingtomentionsdefinedinthei2b2/VA
notes(seeÂ§5.2fordetails). Weadditionallyusethe
annotation guidelines. For the second full coref-
training set of i2b2/VA for continued in-domain
erence task, annotators were asked to both high-
training,andweisolatetheimpactofmentionde-
light spans and additionally draw links between
tection by training with and without freezing the
mentionpairsifcoreferent. AllannotatorsusedIN-
antecedentlinker.
CEpTION(Klieetal.,2018)andunderwenta45
ResultsaregiveninTable1. Continuedtraining
minutetrainingsessiontolearnandpracticeusing
ofjusttheencoderandmentiondetectorresultsina
theinterfacebeforebeginningtimedexperiments.2
largeimprovementof17pointsoverthesourcedo-
In order to measure the effect of document
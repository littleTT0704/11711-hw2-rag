 highly complex problem of directly
optimizing θ against data, to an alternating optimization problem over q and θ, which is algorithmically easier
to solve since q often acts as an easy-to-optimize proxy to the target model. The auxiliary q can also be more
flexibly updated to absorb influence from data or constraints, offering a teacher-student–style iterative
mechanism to incrementally update θ as we will see in the sequel.
12
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
By reformulating learning as a constrained optimization problem, the maximum entropy point of view also
offers a great source of flexibility for applying many powerful tools for efficient approximation and enhanced
learning, such as variational approximation (e.g., by relaxing Q to be easy-to-inference family of q such as the
mean field family, Jordan et al., 1999, and Xing et al., 2002), convex duality (e.g., facilitating dual sparsity of
support vectors via the complementary slackness in the KKT conditions), and kernel methods as used in
(Taskar et al., 2004; Zhu & Xing, 2009).
It is intriguing that, in the dual point of view on the problem of (supervised) MLE, data instances are encoded
as constraints (Equation 2.4), much like the structured constraints in posterior regularization. In the following
sections, we present the standardized formalism of machine learning algorithms and show that indeed a myriad
types of experience besides data instances and constraints can all be encoded in the same generic form and be
used in learning.
3. A Standard Model for Objective Function
Generalizing from Equation 2.16, we present the following general formulation for learning a target model via
a constrained loss minimization program. We would refer to the formulation as the ‘Standard Equation’
because it presents a general space of learning objectives that encompasses many specific formalisms used in
different machine learning paradigms.
Without loss of generality, let t ∈ T be the variable of interest, for example, the input-output pair
t = (x,y) in a prediction task, or the target variable t = x in generative modeling. Let p θ(t) be the target
model with parameters θ to be learned. Generally, the SE is agnostic to the specific
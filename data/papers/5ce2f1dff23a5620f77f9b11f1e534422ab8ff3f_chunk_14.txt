 2021), and the 530B parameter
and fine-tuned GPT (Micheli & Fleuret, 2021) in Ta- MT-NLG (Smith et al., 2022) models. Table 2 reports
ble1. Forhumangoalspecifications, PEToutperforms the generation accuracy and the RoBERTa (Liu et al.,
SOTA (GPT) by 25% on seen and 5% on the unseen 2019)embeddingcosinesimilarityagainstground-truth
split. sub-tasks. We observe that all LLMs achieve high
accuracy on template goal specifications, where there
Although PET under-performs GPT on Template goal
is no variation in sentence structures. For human goal
specifications, GPT requires fine-tuning on fully text-
specification, MT-NLG generates subtasks similar to
based expert trajectory and thus loses adaptability to
ground truth in terms of embedding similarity, while
differentenvironmentsettings. Qualitatively,onhuman
the other smaller models perform significantly worse.
goal specification tasks, where the goal specifications
areout-of-distribution, GPToftengetsstuckrepeating
Eliminate module Weevaluatethezero-shotrecep-
the same action after producing a single wrong move.
tacle/object masking performance of Macaw on the
On the other hand, since the Plan module of PET is
three splits of AlfWorld. In Fig 6, we illustrate the
not trained on the task, it generalizes to the variations
AUCcurveoftherelevancescorethatthemodelassigns
for human goal specifications as shown in Section 4.5.
to the objects v.s. objects that the rule-based expert
Quantitatively, GPTsuffersfromarelative50%perfor-
interacted with when completing each task. Since the
mance drop transferring from template to human-goal
Macaw QA model is queried in a zero-shot manner, it
specifications, whereas PET incurs only a 15 âˆ¼ 25%
demonstrates consistent masking performance on all
drop.
three splits of the environment, even on the unseen
The setting closest to PET is BUTLER with behavior split. In addition, we note that object receptacle accu-
cloning (BUTLER + BC). Since BUTLER + BC per- racy
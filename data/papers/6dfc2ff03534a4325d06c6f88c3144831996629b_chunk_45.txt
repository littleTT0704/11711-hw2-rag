 are remapped 17Weusethehttps://github.com/gatagat/lapimplementation.
uniquely for each query q i, with some probability, there 18WenotethatduringtheAdversarialMatchingprocess,foreitherQues-
should be at least some remappings of r that make sense, tionAnsweringorAnswerJustification,thedatasetisbrokenupinto11
i
andthequestionrelevancemodelP shouldselectthem. folds.Foreachfold,BERTisfine-tunedontheotherfolds,notonthefinal
rel datasetsplits.
Semantic categories Recall that we use 11 folds for 19Also,BERT-Largerequiresmuchmorememory,enoughsothatit’s
the dataset of around 290k questions, answers, and ratio- hardertofine-tuneduetothesmallerfeasiblebatchsize.
13
Q A
larityfortheentiredataset,overbucketsof3000examples. 1.0
Thus,weoptedtousealargerbucketsizeratherthanamore
expensivemodel. 0.8
Similaritymodeldetails Whilewewanttheresponses
to be highly relevant to the query, we also want to avoid 0.6
cases where two responses might be conflated by humans
- particularly when one is the correct response. This con- 0.4
flation might occur for several reasons: possibly, two re-
Relevance Model
sponsesareparaphrasesofoneanother,oroneresponseen- 0.2
Worker
tailsanother. Welumpbothunderthe‘similarity’umbrella
as mentioned in the paper and introduce a model, P, to 0.0
sim
2 1 0
predicttheprobabilityofthisoccurring-broadlyspeaking, 10 10 10
thattworesponsesr andr havethesamemeaning.
i j
We used ESIM+ELMo for this task [10, 57], as it still QA R
doesquitewellontwo-sentencenaturallanguageinference 1.0
tasks (although not as well as BERT), and can be made
muchmoreefficient.Attesttime,themodelmakesthesim- 0.8
ilaritypredictionwhengiventwot
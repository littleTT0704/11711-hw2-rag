
0 C 0
µˆ andvarianceσˆ2arepluggedbackintoEq.(5)tocomputesampleweights.
t t
EstimatingtheGaussianparametersadaptivelyfromtheconfidencedistributionduringtrainingnot
only improves the generalization but also better resolves the quantity-quality trade-off. We can
verifythisbycomputingthequantityandqualityofpseudo-labelsasshowninTable1. Thederived
quantityf(p)isboundedby[λm 2ax(1+exp( −( C1 2− σˆtµˆ 2t)2 )),λ max], indicatingSoftMatchguarantees
atleastλ /2ofquantityduringtraining. Asthemodellearnsbetterandbecomesmoreconfident,
max
i.e., µˆ increases and σˆ decreases, the lower tail of the quantity becomes much tighter. While
t t
quantity maintains high, the quality of pseudo-labels also improves. As the tail of the Gaussian
exponentiallygrowstighterduringtraining,theerroneouspseudo-labelswherethemodelishighly
unconfident are assigned with lower weights, and those whose confidence are around µˆ are more
t
efficiently utilized. The truncated Gaussian weighting function generally behaves as a soft and
adaptiveversionofconfidencethresholding,thuswetermtheproposedmethodasSoftMatch.
3.2 UNIFORMALIGNMENTFORFAIRQUANTITY
Asdifferentclassesexhibitdifferentlearningdifficulties, generatedpseudo-labelscanhavepoten-
tially imbalanced distribution, which may limit the generalization of the PMF assumption (Oliver
etal.,2018;Zhangetal.,2021). Toovercomethisproblem,weproposeUniformAlignment(UA),
encouragingmoreuniformpseudo-labelsofdifferentclasses.Specifically,wedefinethedistribution
inpseudo-labelsastheexpectationofthemodelpredictionsonunlabeleddata:E [p(yxu)].Dur-
ingtraining,itisestimatedasEˆ [p(yxu)]usingtheEMAofbatchpredictionsD oU nunla| beleddata.
We use the
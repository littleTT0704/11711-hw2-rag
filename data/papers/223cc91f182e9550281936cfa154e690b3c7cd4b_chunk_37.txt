 intrinsically motivated variant of the policy gradient algorithm (and other RL
algorithms discussed below), by replacing the standard extrinsic-only
Qθ(x,y)
in the experience function
Equation 4.16 with the combined Qθ(x,y) +Qin,θ(x,y). Let fθ (x,y) denote the resulting
reward,ex+in
experience function that incorporates both the extrinsic and the additive intrinsic rewards.
We can notice some sort of symmetry between fθ (x,y) and the actively supervised data
reward,ex+in
experience f in Equation 4.10, which augments the standard supervised data experience with the additive
active
informativeness measure u(x). The resemblance could naturally inspire mutual exchange between the
research areas of intrinsic reward and active learning, for example, using the active learning informativeness
measure as the intrinsic reward
rin
, as was studied in earlier work (Li et al., 2011; Pathak et al., 2019;
Schmidhuber, 2010).
4.4. Model-Based Experience
A model may also learn from other models of the same or related tasks. For example, one can learn a small-
size target model by mimicking the outputs of a larger pretrained model, or an ensemble of multiple models,
that is more powerful but often too expensive to deploy. Thus, the large model serves as the experience, or the
source of information about the task at hand. By seeing that the large source model is effectively providing
27
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
‘pseudo-labels’ on the observed inputs D = {x∗}, we can readily write down the corresponding experience
function, as a variant of the standard supervised data experience function in Equation 4.2:
f :=f mm oim deic lking (x,y;D) =logEx∗∼D,y~∼p model′(y∣x∗)[I(x∗,y~)(x,y)], (4.23)
where y~ ∼ p model′(y∣x∗) denotes that we draw label samples from the output distribution of the large
source model.
Another way of model-
required
theannotatortoundertakethefullexecutionandscrutinizetheintermediatestates.
HumanPerformance Wesampleonetaskfromeachofthe170tem-
Avg.Time 110s
platesandaskfivecomputersciencegraduatestudentstoperformthese
SuccessRate 74.68%
tasks. The human performance is on the right. Overall, the human info
SuccessRate 81.32%
annotators complete 78.24% of the tasks, with lower performance on others
SuccessRate 78.24%
all
information-seekingtasks. Throughexaminingtherecordedtrajectories,
wefoundthat50%ofthefailuresareduetomisinterpretingtheintent(e.g.,providingtraveldistance
whenaskedfortraveltime),incompleteanswers(e.g.,providingonlynamewhenaskedfornameand
email),andincompleteexecutions(e.g.,partiallyfillingtheproductinformation),whiletheremaining
instanceshavemoreseverefailures,wheretheexecutionsareoff-target.
4 BASELINE WEB AGENTS
We experiment with three LLMs using two prompting strategies, both with two examples in the
context. In the first setting, we ask the LLM to directly predict the next action given the current
observation,theintentandthepreviouslyperformedaction. Inthesecondsetting,withthesame
information,themodelfirstperformschain-of-thoughtreasoningstepsinthetextbeforetheaction
prediction(CoT,Weietal.(2022);Yaoetal.(2022b)). Beforetheexamples,weprovideadetailed
7
Underreview
overviewofthebrowserenvironment,theallowedactions,andmanyrules. Tomakethemodelaware
oftheunachievabletasks,theinstructionexplicitlyaskstheagenttostopifitbelievesthetaskis
impossibletoperform. WerefertothisdirectiveasUnachievablehint,orUAhint. Thisintroduction
islargelyidenticaltotheguidelineswepresentedtohumanannotatorstoensureafaircomparison.
WeuseanaccessibilitytreewithelementIDsast
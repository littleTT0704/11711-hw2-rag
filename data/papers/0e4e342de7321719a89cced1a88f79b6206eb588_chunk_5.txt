olutionmod- fusepipeline.RCCF[Liaoetal.,2020]usesacross-modality
ule(QCM)thatextractsquery-awarevisualfeaturesand
correlationfilteringonvisualandtextualfeatures,andReSC
canbeflexiblyintegratedintovisualencoders. Wealso [Yangetal.,2020]proposesarecursivesub-queryconstruc-
introduceavisualgroundingpipelineVGQCwhichuti-
tionframeworktohandlecomplexqueries. Withthesuccess
lizes QCM to address the problem in previous extract- andpopularityoftransformers[Vaswanietal.,2017],recent
and-fusemethods.
one-stage methods have adopted transformer-based fusion
2. Our experiments show that VGQC w/ fusion achieves modulesandhaveachievedstate-of-the-artperformance[Du
state-of-the-art performance on RefCOCO [Yu et al., etal.,2021;Dengetal.,2021].
2016], RefCOCO+ [Yu et al., 2016], and RefCOCOg
2.2 Multi-modalFeatureFusion
[Maoetal.,2016],andVGQCw/ofusionachievescom-
parable performance with faster inference speed and a We also review several multi-modal feature fusion methods.
simplermodelstructure. [Landi et al., 2019] constructs dynamic convolution filters
fromtextandpost-processestheoutputfeaturemapswiththe
3. We conduct extensive analysis with visualizations to
filters. Comparedtodynamicconvolution[Chenetal.,2020]
demonstrate the diversity of candidate kernels as well
which composes convolution kernels with attention weights
as the effectiveness of encoding query information into
derived from the previous feature map, it directly generates
QCMâ€™sattentionweightsandaggregatedkernels,which
convolution kernels from text. Another convolution-based
enable such kernels to extract query-aware visual fea-
method MMTM [Joze et al., 2020] learns a joint represen-
tures.
tation and generates an excitation signal for each modality.
In transformer-based fusion methods, [Nagrani et al., 2021]
2
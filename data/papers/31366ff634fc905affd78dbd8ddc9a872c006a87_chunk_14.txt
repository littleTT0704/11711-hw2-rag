onthedevelopmentsetofdifferentdatasetswiththelanguage-specific
pretrainedmodel(Lang-specific)andwiththebaseCodeBERT(Basemodel).
0.45 tion, while higher layers encode deeper semantic
meaninginnaturallanguage.
Does encoding natural language context help?
0.4
One major difference between CodeBERTScore
and BERTScore is that CodeBERTScore lever-
0.35 ages the context for the generated code, such as
the natural language instruction or intent that was
given as input for generation. We find that us-
Java
0.3 C++ ingcontextincreasesthecorrelation,forexample,
JavaScript the Kendall-Tauof CodeBERTScore from0.50 to
Python 0.52. While this paper mainly focuses on natu-
0.25 ral language instructions, we believe that Code-
0 2 4 6 8 10 12
BERTScore can thus benefit other programming
Layer
scenarios as well, for example when generating
code given the human-written comments, or gen-
Figure 5: The average of Kendall-Tau and Spearman
onthedevelopmentsetofHumanEvalwhenusingthe eratingcodegiventheprecedingcodecontext.
embeddingsfromdifferentlayers.
CodeBERTScore allows soft matching of
tokens The heatmaps in Figure 6 show the sim-
ilarityscoresbetweentokensinCodeBERTScore.
Which transformer layer should we use? We For example, both shutil.rmtree and
further investigate the impact of using hidden os.rmdir in Figure 6(a) delete a folder;
states from different layers of the model — the CodeBERTScorealignseachtokentoarespective
layerwhichthevectorsinEquation(2)comefrom, tokenintheotherexpression,eventhoughthetwo
in the computation of CodeBERTScore. The re- spansdonotsharemanyidenticaltokens.
sults are shown in Figure 5: generally, the deeper In Figure 6(b), both code snippets calculate a
the layer – the higher the average correlation be- squareroot,whereoneusesmath.sqrt(x)and
tween CodeBERTScore and functional correct- the other uses x ** 0
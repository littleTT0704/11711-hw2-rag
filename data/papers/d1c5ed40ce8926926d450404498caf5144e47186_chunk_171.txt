 plot, the performance of supervised learning using all instances in
the training folds is indicated by a solid horizontal line.
The Random query selection strategy performs reasonably well if the feature set
is small and even catches up with uncertainty sampling after 1,000 steps when using
logistic regression models. This is because not much training data is needed to fit a
model with only 19 relevance features. If a larger feature set is used, more training
data is required to avoid overfitting, and random sampling converges more slowly.
Maximum Score is usually more effective in early iterations because it selects a more
evennumberofpositiveandnegativeinstancesthanrandomsampling. However, after
this initial phase it performs poorly since it almost only selects positive examples,
and it is outperformed by the random strategy after 1,000 iterations in most settings.
Query selection based on the Diversity criterion is initially highly effective because
it quickly explores the feature space and yields a relatively accurate model after few
iterations. This method also consistently performed well across all test folds in the
cross-validation and all random restarts. However, the slope of the learning curves
decreasesquicklybecausethereisnotmuchmeritinselectingadditionalqueriesbased
on this criterion once a diverse sample of the training data has been labeled.
Uncertainty sampling is clearly more effective for SVMs than for LR models. We
observed that this method only performs well with logistic regression if a relatively
accurate model is found at an early stage. If, on the other hand, queries are selected
based on ineffective models, this approach continues to make poor choices and the
learningratecanbeverylow. Whenusingthelargerfeatureset, uncertaintysampling
with LR models performs worse than random query selection for over 250 iterations.
Sometimes this method cannot recover if the models are too far from the optimum
and yields an extremely poor final model. These occasional failures have a noticeable
impact on the learning curves obtained by averaging over multiple folds and random
restarts. However, when performing live active learning one could start over if the
algorithm is caught in a local optimum. This is not difficult to detect even without
labeled test data because most of the selected queries would be negative examples.
When using SVMs, there is a strong theoretical motivation for uncertainty sampling
that explains why this method is very effective. It can be shown that if the data are
linearly
ly compare the impact of different strategies. The results are summarized in
Table 6.8 for Wikipedia and in Table 6.9 for Wiktionary.
In general, relevance estimation methods that were more effective for ranking text
nuggets in Chapter 5 also have higher search recall. Thus we can select an effective
method based on intrinsic evaluation results and, with few exceptions, can expect it
to perform well when applied to the question answering task. It can also be seen that
search recall improves even if the seed documents are expanded with content selected
randomly from the retrieved web pages (Random baseline). This is most apparent if a
small seed corpus is used that does not contain the answers to many of the Jeopardy!
and TREC questions, such as Wiktionary. However, one should keep in mind that
these are not arbitrary web pages but that they were selected by the Yahoo! search
6.3. SEARCH EXPERIMENTS 87
engine among billions of candidates, and they are related to the topics of the seeds
that are most popular and useful for our QA tasks. Furthermore, the gains in search
recall are clearly larger if relevant text is selected based on a more informed relevance
estimation approach.
The performance of the Round Robin baseline is relatively low for Wiktionary
because the retrieved web pages are mostly entries in online dictionaries that often
start with irrelevant text such as headings, menu bars and advertisements. Because
Wiktionary seeds are usually very short, the threshold that restricts the size of the
pseudo-documents relative to the seed length can be exceeded before the relevant
sectionsofthewebpagesarereached. Ontheotherhand,thebaselinethatusessearch
engine rankings (Search Rank) is relatively ineffective when expanding Wikipedia
because the web pages retrieved by Yahoo! for Wikipedia seeds can be very long, and
sometimes the search results at the highest ranks do not contain useful content. In
that case, the allotted length for the expanded document is mostly taken up by noise
that is not relevant for QA. The Cosine Sim baseline is less effective for Wiktionary
than for Wikipedia because it exclusively relies on the seed documents for estimating
therelevanceoftextnuggets. Wiktionaryentriesarenotonlymuchshorteronaverage
than Wikipedia articles, but they also contain more text that is not directly relevant,
such as examples of how a dictionary term is used in a sentence,
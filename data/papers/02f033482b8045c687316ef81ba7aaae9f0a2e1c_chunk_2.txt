canbeleveragedforeffec-
offensivesentencecompletionorfriendlyconversa-
tiveattributecontrol. Thedifferenceinlogitsz`´z´
tiongeneration(Seeetal.,2019;Shengetal.,2020;
outputbytheexpertandanti-expertrepresentstheper-
Gehmanetal.,2020). Forexample,asafecomple-
turbations to make to the logits z of the pretrained
tiontotheprompt“Whensherejectedhisadvance,
“base”LM.
hegrabbed...” requiresavoidingwordchoicesthat
couldleadtocontinuationswithgender-basedvio-
lence(e.g.,“her”;Figure1). 2021),finetuningorre-trainingapproachesarebe-
Without such steering, these language models comingincreasinglycomputationallyinfeasiblefor
risk generating mindless and offensive content mostresearchers.
(Shengetal.,2019;Holtzmanetal.,2020)which We propose DEXPERTS,1 a decoding-time
hinders their safe deployment (Brockman et al., method for controlled text generation based on a
2020; Bender et al., 2021). Importantly, as the
1DEXPERTS stands for Decoding-time Experts.
scaleofpretrainedLMsincreases(e.g.,175Band
Our code is available at https://github.com/
1.6Tparameters;Brownetal.,2020;Fedusetal., alisawuffles/DExperts.
product of experts (Hinton, 2002). Our method 2 ExpertsandAnti-Expertsfor
combines an out-of-the-box pretrained (“base”) ControlledGeneration
LMwith“expert”LMsand/or“anti-expert”LMs,
Giveninputtextasaprompt,thetaskofcontrolled
whichmodeltextwithdesirableandundesirableat-
textgenerationistogenerateacontinuationthat
tributes, respectively. By generatively modeling
flowsnaturallyfromthepromptwhilehavingthe
text with particular attributes and directly com-
desiredattribute(e.g.,positivesentiment)butnot

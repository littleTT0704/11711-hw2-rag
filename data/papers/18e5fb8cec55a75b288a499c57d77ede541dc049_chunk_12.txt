asubgraphfromConceptNet.
ate MLMs, we mask out one token at a time and compute
TheanswercandidatescombineConceptNetnodeswith
itsloss(Zhouetal.2020).Werepeatthisprocessforevery additionalcrowdsourceddistractors.
tokeninthesequence.ThefinalMLMscoreis: 3.PhysicalIQA(PIQA) (Bisketal.2020)isatwo-choice
question answering dataset which focuses on physical rea-
n
1 X soning.Givenaquestion,thesystem(orhuman)isaskedto
S (T)=(cid:0) logP (t j:::t ;t :::) (2)
MLM n i i(cid:0)1 t+1 pickthemoreplausibleoutoftwopossiblecontinuations.
i=1
4. SocialIQA (SIQA) (Sap et al. 2019b) is a question-
Thepredictedoptionistheonewiththelowestscore.
answeringdatasetwhichrequiresreasoningaboutsocialin-
LMFinetuning Inthetypicalmodelarchitectureforfine- teractions. Each entry contains a context, a question, and 3
tuningLMformultiple-choicetasks,alinearlayerisadded answercandidates.ThecontextisderivedfromtheATOMIC
ontopoftheLMencodertopredicttheanswer.Themodel knowledgegraph,thequestionsaregeneratedbasedonnine
inputs are separated by a model-specific delimiter. How- templates (corresponding to the relations in ATOMIC), and
ever,asthisarchitectureintroducesrandomlyinitializedpa- theanswersarecrowdsourced.
rameters, it may not be able to fully utilize the pre-trained 5. WinoGrande (WG) (Sakaguchi et al. 2019) contains
weights (Tamborrino et al. 2020). Instead, we re-use the 44 thousand pronoun resolution problems. Each entry con-
GPT-2andRoBERTawithLMheadforfinetuning.Bykeep- sists of a context description with an emphasized pronoun,
ingthemodelintact,wecanreusethesameconvertingtem- andtwooptionsareoffered
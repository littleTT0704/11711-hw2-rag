sexposedbyourgeneralizedformulation. Basedonthesehypotheses,weperformablation
experimentsandanalyzethenuancesbetweendifferentimplementationsofthegeneralizedversionofP.
kNN
Astheanswertoourquestion,“whykNN-LMswork”,weeventuallyshowthatthemostprobablereasonsare
threefold:
1. Ensemblingtheoutputofsoftmaxusingtworepresentationsfromdifferentlayersofthetransformer
isimportant;inourexperiments,thisaccountsfor55%oftheperformancegainofkNN-LM,or6.5%
relativeperplexityimprovementcomparedtothebaseLM.
2. kNN-LMusesapproximatenearestneighborsearchtohandlethelargenumberofcandidates,and
thelackofthisprecisenessinthisalgorithmactuallyhelpskNN-LMtogeneralizebetterthanusing
exactnearestneighborsearchanddistancecalculation,possiblyduetoaregularizationeffect. The
relativeperplexityimprovementfromthisfactorisabout2.6%.
3. Depending on the design decisions that are chosen for modeling, adding a temperature term to
the kNN non-parametric component can become crucial to the success of modeling (although
coincidentally,intheoriginalsettingsofKhandelwaletal.(2020b),atemperatureof1.0iscloseto
optimal,whichhidtheimportanceofthisterm). Insomesettings,therelativeperplexitygapbetween
thedefaultandoptimaltemperaturecanbeashighas3.7%.
Finally,onesignificantdrawbacktothecurrentkNN-LMistheinefficiencyofkNNsearchperformedateach
step(Heetal.,2021;Borgeaudetal.,2022;Alonetal.,2022;Wangetal.,2022). Becauseofthesimilarity
betweenkNN-LMandtheparametricLM’slastlayersandthemanydesignchoices,wealsodemonstratethat
weareabletomakekNN-LMmoreefficientbysubstitutingthekNNsearchwithanothermatrixoperation
thatcanfitinacceleratormemorywhilemaintainingmorethan
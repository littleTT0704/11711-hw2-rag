Vaduguru.
hardware platforms. We show that inference per-
formed with these large neural networks, which
was previously assumed to be compute bounded,
References
is in fact limited by overhead incurred by deep
Rami Al-Rfou, Guillaume Alain, Amjad Almahairi,
learningframeworks. Whilewidertransformerar-
ChristofAngermueller,DzmitryBahdanau,Nicolas
chitectures(e.g. BERT-Base)exhibitlessbounded-
Ballas,FrédéricBastien,JustinBayer,AnatolyBe-
ness behaviors than narrower, deeper CNNs (e.g. likov,AlexanderBelopolsky,etal.2016. Theano: A
pythonframeworkforfastcomputationofmathemat- MostafaDehghani,AnuragArnab,LucasBeyer,Ashish
icalexpressions. arXive-prints,pagesarXiv–1605. Vaswani,andYiTay.2021. Theefficiencymisnomer.
arXivpreprintarXiv:2110.12894.
Ibrahim M Alabdulmohsin, Behnam Neyshabur, and
XiaohuaZhai.2022. Revisitingneuralscalinglaws Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
inlanguageandvision. AdvancesinNeuralInforma- KristinaToutanova.2018. Bert: Pre-trainingofdeep
tionProcessingSystems,35:22300–22312. bidirectionaltransformersforlanguageunderstand-
ing. arXivpreprintarXiv:1810.04805.
RezaYazdaniAminabadi,SamyamRajbhandari,Am-
mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, JamesGleeson,MosheGabel,GennadyPekhimenko,
OlatunjiRuwase,ShadenSmith,MinjiaZhang,Jeff Eyal de Lara, Srivatsan Krishnan, and Vijay
Rasley,etal.2022. Deepspeed-inference: enabling JanapaReddi.2021. Rl-scope: Cross-
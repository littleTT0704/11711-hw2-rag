
forComputationalLinguistics: HumanLanguageTechnologies,pages5546–5556,Seattle,UnitedStates,
July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.406. URL
https://aclanthology.org/2022.naacl-main.406.
13
A kNN-LMGeneralizationtoOtherLMs
#params BaseLMPPL kNN-LMPPL AbsolutePPLGain
Ours 268M 21.75 19.17 2.58
Distilled-GPT2 82M 18.25 14.84 3.41
GPT2-small 117M 14.84 12.55 2.29
GPT2-medium 345M 11.55 10.37 1.18
GPT2-large 774M 10.56 9.76 0.80
Table4: PerformanceofkNN-LMappliedtootherpretrainedlanguagemodelsofdifferentsizes.
TotestthegeneralizabilityofkNN-LM,wefollowthesameexperimentalsetupasusedinSection3. We
selectseveralpretrainedmodelsfromtheGPT2family(Radfordetal.,2019)ofvariousparametercounts,
plusadistilledversionofGPT2,DistillGPT2.(Sanhetal.,2019)Wetakethepretrainedmodelcheckpoint,
buildthedatastoreandevaluateontheWikitext-103datasetsplits. TheresultsareshowninTable4. Wecan
seethatkNN-LMshasgoodgeneralizabilityonothermodels. ItimprovestheperplexityofallthebaseLMs
tested. However,thelargerthemodelis,andusuallythebetterthebaseLM’sperplexityis,thelessgaincan
beachievedfromaddingkNN.NotethatourmodelistrainedfromscratchonWikitext-103datasetandthus
evenwitharelativelylargemodelsize,theperplexityandperplexitygainfromaddingkNNisstilllessthan
modelswithpretraining. Withoutlossofgeneralizability,wewilluseourowntrained-from-scratchmodelas
thebaseLMinthefollowingsectionsforablation
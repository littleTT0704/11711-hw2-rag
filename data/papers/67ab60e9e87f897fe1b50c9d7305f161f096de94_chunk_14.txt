 models and tasks against each other, under the assumption each task
is equally weighted. The normalization procedure is as follows: 1) For each task, we stan-
dardize the scores to zero mean and unit variance. Unlike transforming tasks to ranks, we
assumethatthescale of intra-task scores is important. 2) Thestandardized scores are Win-
sorized (clamped) tohave variance within[-1, +1]. By limiting theimportanceof extremely
high or low scores on a single task, this approach allows for better inter-task comparison.
In the following paragraphs, we describe a few interesting patterns and trends in the
submitted models. Many evaluted models use the last layer as the representation. It is
known that non-final layers and/or fusing various layers might capture more information
(Shor et al., 2020; Baevski, 2020; Yang et al., 2021). Intermediate layers often model audio
phenomenathat arenot necessary for thefinalloss. NTU-GURA’s ablation studies support
that, as evidence by the relative performance of their different models. For conciseness, we
use the term “strong speech models” to refer to NTU-GURA’s fused models that include
pretrained speech models.
Pitch tasks NSynthpitchandMaestrotaskshavesimilarresults,andmodelsthatinclude
CREPE embeddings (Kim et al., 2018b) perform best. This makes sense as these tasks
require modeling of pitch, which CREPE was specifically trained for, while many other
representations focus on discriminating between semantic objects (e.g., cat vs dog or guitar
vs piano) but are pitch agnostic. Interestingly, models trained for semantic discrimination
(e.g., via AudioSet) and speech models do nonetheless represent pitch to some degree, as
evidenced by the decent performance of OpenL3 and wav2vec2 on these tasks.
Broad Domain Semantic-Object Tagging FSD50K and ESC-50semantic-object tag-
ging results are strongly correlated, as well as—perhaps surprisingly—GTZAN genre tag-
ging. The models that perform the best on this group are the ones pretrained on the
AudioSet semantic-object tagging task. What we glean from this large-scale survey of di-
verse models
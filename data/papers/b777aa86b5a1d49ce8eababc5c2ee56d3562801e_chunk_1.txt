The Framework Tax:
Disparities Between Inference Efficiency in NLP Research and Deployment
JaredFernandez1 JacobKahn3 ClaraNa1 YonatanBisk1 EmmaStrubell1,2
1LanguageTechnologiesInstitute,CarnegieMellonUniversity
2 AllenInstituteforArtificialIntelligence 3FAIR
{jaredfern, strubell}@cmu.edu, {csna, ybisk}@cs.cmu.edu, jacobkahn@fb.com
Abstract
Increasedfocusonthecomputationalefficiency
ofNLPsystemshasmotivatedthedesignofef-
ficientmodelarchitecturesandimprovements
to underlying hardware accelerators. How-
MACS are not
ever,theresultingincreasesincomputational proportional to
throughputandreductionsinfloatingpointop- Latency
erationshavenotdirectlytranslatedtoimprove-
ments in wall-clock inference latency. We
demonstrate that these discrepancies can be
largelyattributedtobottlenecksintroducedby
deep learning frameworks. We denote this
phenomenon as the framework tax, and ob-
serve that the disparity is growing as hard-
Figure1:Latencyasafunctionofamodelâ€™smultiplyac-
warespeedincreasesovertime. Inthiswork,
cumulateoperations(MACs)onanNvidia2080tiGPU.
we examine this phenomenon through a se-
Expectedrelationshipsdonothold;modelcomplexity
ries of case studies analyzing the effects of
andhardwarecapabilitiesfailtopredictlatencydueto
modeldesigndecisions,frameworkparadigms,
framework-boundedness.
andhardwareplatformsontotalmodellatency.
Code is available at https://github.com/
JaredFern/Framework-Tax.
to make up 80 to 90% of ML cloud computing
1 Introduction demand(Barr,2019;Leopold,2019). Intheseset-
tings,metricsofmodelspeedsuchaslatencyand
Natural language processing systems have bene-
throughput are essential for inference workloads
fited from improvements in performance driven
thataresubjecttorealwall-clocktimeconstraints
by
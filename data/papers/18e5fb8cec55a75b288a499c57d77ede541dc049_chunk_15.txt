Gs:Self-talk, beeninvestigatedindetail.
COMET-DynaGen,andSMLM.Toindicatetheupperbound
H5: When selecting negative samples for a question, it
of this work, we include results of a supervised fine-tuned
helpstouseanadversarialstrategythatensurestheques-
RoBERTasystemandofhumanevaluation.
tion is not trivial for a language model. H5 is inspired
by adversarial filtering, which has not been investigated
Implementation
indetailforautomatically-generatedquestionsandacross
FortheLMbaselines,wedirectlyloadtheweightsfromthe KGs.
Transformerslibrary(Wolfetal.2019)andevaluateonthe H6: Preserving the task structure when generating syn-
downstreamtasks.ThefinetunedLMsaretrainedforasin- thetic data leads to better accuracy. This is implicitly
gle epoch on our synthetic QA set. For Adv-filter, we train assumed in prior data augmentation work (Kocijan et al.
themodelsfor5epochstocompensateforlesstrainingdata. 2019).
We use our synthetic dev set to select the best model. We
H7:Theautomaticallycreatedquestionsarenotablyeasier
describeotherhyper-parametersusedandcomputinginfras-
forhumansthantheyareformachines-ageneralassump-
tructureintheappendix.
tion made by commonsense task creators and typically
correctforanyexisting,human-generatedbenchmark.
Hypotheses
Basedonindividualpriorfindingsandunderstandingofdif- Results
ferent components of our framework, we put forward a set
We evaluate various combinations of: knowledge sources,
ofhypotheseswhichwillbevalidatedinourexperiments:
question generation strategies, LMs, training regimes, and
H1:RoBERTawouldhavebetterperformancethanGPT-2. tasks. We use accuracy as a metric. All our experiments
This is in line with prior findings that RoBERTa has the areperformedinazero-shotsetting,i.e.,themodelsdonot
advantageofbi-directionalcontext(Zhouetal.2020). leverage the official training data of the task
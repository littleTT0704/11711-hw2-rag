 the findings of Puri et al. (2022) on instruction-example
equivalence.
Few-shotGPT-3answerpredictionunderperformsBha¯skara. While
prompt-based models outperform our fine-tuned models in general when compar-
ing within direct-answering and program-synthesis, when comparing Bha¯skara
program-synthesis to GPT-3 direct answering we find that the much smaller
Bha¯skara consistently outperforms GPT-3.
7NotethatthetrainingsetforCodexisnotknown.
13
1F
egarevA
Few-shot Codex performance is relatively strong. Relative to the 2.7B
trained models, Codex demonstrates strong few-shot IID and OOD performance.
Somenotableexceptionstothispatternarethestatistics,linearalgebra,multiple-
choice question answering, and NLI tasks. Generally, OOD few-shot performs
much better than OOD for the fine-tuned models.
Few-shot Codex fails on some tasks. Despite strong performance relative
to Bha¯skara, Codex obtains less that 0.5 F1 on several tasks, with especially
poor performance on geometry, number theory, advanced math, complex lan-
guage, computer science problems, science formulas, and real world knowledge.
6 Conclusion
Inthiswork,weintroduceL¯ila,aunifiedmathematicalreasoningbenchmarkfor
a holistic evaluation of AI agents. L¯ila consists of 23 tasks across 4 dimensions
(i) mathematical abilities, (ii) language format, (iii) language complexity, (iv)
external knowledge. It builds on 20 existing mathematical reasoning datasets to
collect instructions and Python programs. Further, it also supports measuring
out-of-distribution performance and robustness to language perturbations via
L¯ila-OOD and L¯ila-Robust respectively. We also introduce Bha¯skara, a
2.7B-parameterfine-tunedmulti-taskmodel. Wefindthatmulti-taskingimproves
over single-task performance by 21.83% F1 score on average, and that our model
is a strong starting point for further fine-tuning on new math reasoning tasks.
The best performing model we evaluate achieves only 60.40% F1
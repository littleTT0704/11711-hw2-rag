 of up to 5 patches of mel-spectrogram input, shuffled in time.
Soundsensing YAMNet 2-D MobileNet (Howard et al., 2017). Pretrained to tag Au-
dioSet.
Stellenbosch LSL Audio DBERT 1-D CNNencoder and modifiedBERT transformer.
Pretrained as the discriminator with a GAN objective, using the clustering model as the
generator, on 960 hours of Librispeech (Panayotov et al., 2015). Embeddings are taken
from layer 16 of 24 by default.
5. Results and Discussion
In Figure 1 we present the primary score of submitted models on each HEAR task. By
default, evaluation uses a deterministic seed, for reproducibility. Nonetheless, scores are
stableacrossourevaluation, withamedian95%confidenceintervalof2.5e-3whenseedingof
model weights and hyperparameter grid points is selected non-determinisically. Shor et al.
(2020); Wu et al. (2022a) present scores for some of the the same models and tasks. HEAR
reported scores are similar but not identical, due to downstream training differences.
To display model similarity at a glance, we present t-SNE visualizations of normalized
scoresbytask(Figure2(a))andbymodel(Figure2(b)). Wealsoshowcorrelation tablesfor
tasks(Figure3)andmodels(Figure4)togive greaterinsightintomodelandtasksimilarity,
in similar spirit to the confusion matrices of Wu et al. (2022a). Zhai et al. (2019) compare
a variety of aggregation techniques for evaluating cross-task model performance, and find
8
HEAR: Holistic Evaluation of Audio Representations
thattheyareallhighlycorrelated, settlinguponsimplemeantop-1. Gosiewska et al.(2020)
proposesanELO-likemeta-scoreforcross-taskmodelperformance,similartoachessrating.
Although it is tempting to give a single score for every model, we believe that would strip
out important nuances shown in the full score table (DeYoung et al., 2020).
For these summary figures, we normalize each model/task score. Normalized scores
allow us to compare
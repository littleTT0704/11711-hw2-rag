arietyofwaystodefine frozen. Someexistingworks(Bapnaetal.,2019)
“adapter”,weadoptthedefinitionof(Bapnaetal., followthisframeworkbutmainlyfocusontheuse
2019). Specifically, the adapter block consists of ofadapters.
(1)alayernormalizationLN(·)fortheinputofthe
MultilingualPrivate-sharedTuning(MPS) In
adapters, (2) an autoencoder whose inner dimen-
the above method, although systems of different
sion can be adjusted according to the complexity
languages share one pre-training model, their pa-
ofthetargettaskwithadownprojectionlayer,an
rameterscannotbemodified,whichresultsinthe
upprojectionlayer,andanonlinearactivationfunc-
lack of information interaction across languages
tionbetweenthem,and(3)aresidualconnection.
andthedifficultyinminingthesharedknowledge.
Formally,givenh ∈ Rd betheoutputofi-thlayer,
i
Inthisframework,parametersfrombothadditional
theadapterisformulated:
modulesandpre-trainedmodelscanbeupdated.
ADAPTER(h i) = (ReLU(LN(h i)W idb))W ibd+h i,
3 Experiments
wherebistheinnerdimension,Wdb ∈ Rd×bisthe
i Dataset As our evaluation testbed, we use the
weightofdownprojectionlayerandWbd ∈ Rb×d
i XL-Sum corpus (Hasan et al., 2021),2 which
istheweightofupprojectionlayer.
is a news dataset containing 1.1 million article-
Prefix-tuning Prefix-tuning(LiandLiang,2021) summary pairs in 45 languages. The dataset is
prepends a prefix for every layer of a LM. Let collected from the British Broadcasting Corpora-
HLM ∈ Rt×d, where d is the hidden dimension tion(BBC)website,usingaboldparagraphatthe
i
of LM, t is the input sequence length, denote beginningofeacharticleasthesummaryandthe
the hidden representation of the i-th layer
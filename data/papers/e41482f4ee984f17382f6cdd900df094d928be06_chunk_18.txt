modeltostopwhen
particularlyinrealisticenvironmentssuchas encounteringunachievablequestions.
WebArena.
5.2 ANALYSIS
Domodelsknowwhentostop? Inourerroranalysisoftheexecutiontrajectories,weobserve
a prevalent error pattern of early stopping due to the modelâ€™s conclusion of unachievability. For
instance,GPT-4erroneouslyidentifies54.9%offeasibletasksasimpossible. Thisissueprimarily
stemsfromtheUAhintintheinstruction,whilethishintallowsmodelstoidentifyunachievable
tasks,italsohindersperformanceonachievabletasks. Toaddressthis,weconductanablationstudy
whereweremovethishint. Wethenbreakdownthesuccessrateforbothachievableandunachievable
tasks. AsshowninTable2,eliminatingthisinstructionledtoaperformanceboostinachievable
tasks,enhancingtheoveralltasksuccessrateof GPT-4 to14.41%. Despiteanoveralldeclinein
identifyingunachievabletasks,GPT-4retainsthecapacitytorecognize44.44%ofsuchtasks. Itdoes
sobygeneratingreasonsofnon-achievability,evenwithoutexplicitinstructions. Ontheotherhand,
GPT-3.5rarelyexhibitsthislevelofreasoning. Instead,ittendstofollowproblematicpatternssuch
ashallucinatingincorrectanswers,repeatinginvalidactions,orexceedingthesteplimits. Thisresult
suggeststhatevensubtledifferencesininstructiondesigncansignificantlyinfluencethebehaviorof
amodelinperforminginteractivetasksincomplexenvironments.
Can a model maintain consistent performance across similar
25
tasks? Tasksthatoriginatefromthesametemplateusuallyfollow gpt-3.5-direct
similarreasoningandplanningprocesses,eventhoughtheirobserva- 20 gpt-3.5-cot
gpt-4-cot
tionsandexecutionswilldiffer. Weplotahistogramofper-template 15
successratesforourmodelsinTable
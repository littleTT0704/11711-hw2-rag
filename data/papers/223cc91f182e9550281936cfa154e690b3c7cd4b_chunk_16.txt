∗) ≤ϵ +ξ
⎧ [ ]
(2.18)
⎩⎨ [4pt]−y∗ +Eq [θ⊤T(x∗)
]
≤ϵ +ξ′,
for all instances (x∗,y∗) ∈ D.
Alternating optimization for posterior regularization. Having seen EM-style alternating minimization
algorithms being applied as a general solver for a number of optimization-theoretic frameworks described
above, it is not surprising that the posterior regularization framework can also be solved with an alternating
minimization procedure. For example, consider the simple case of linear constraint in Equation 2.17, penalty
function U(ξ) = ∥ξ∥, and q factorizing across θ = {θ }. At each iteration n, the solution of q(θ ) is
1 c c
given as (Ganchev et al., 2010):
q(n+1)(θ c) =exp {Eq(n)(θ \c)∑logp(x∗∣θ)π(θ)+T(x∗;θ) }/Z, (2.19)
x∗
where θ denotes all components of θ except θ, and Z is the normalization factor. Intuitively, a
\c c
configuration of θ
c
with a higher expected constraint value E\cT(x∗;θ) will receive a higher probability
under q(n+1)(θ ). The optimization procedure iterates over all components c of θ.
c
2.4. Summary
In this section, we have seen that the maximum entropy formalism provides an alternative insight into the
classical learning frameworks of MLE, Bayesian inference, and posterior regularization. It provides a general
expression of these three paradigms as a constrained optimization problem, with a paradigm-specific loss on
the model parameters θ and an auxiliary distribution q, over a properly designed constraint space Q where q
must reside:
min L(q,θ)
q,θ
(2.20)
s.t. q ∈ Q.
In particular, the use of the auxiliary distribution q converts the originally
 may all contain subheadings
such as “Pronunciation” or “Translations”. This could bias the selection of relevant
content through topicality features that use the word distribution in the seeds, and it
may hurt performance more than random noise. However, the worst case impact on
the LR model is limited: even if no usable seed content is available at all, the model
still performs at 67% MAP.
66 CHAPTER 5. INTRINSIC EVALUATION
85%
LR Adjacent
80%
(Markup)
75%
Cosine Sim
70%
(Markup)
P
65%
A
M 60%
55%
50%
45%
40%
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Probability of dropping seed tokens
Figure 5.6: Effect of the seed document length on the ranking performance of logistic
regression models and the cosine similarity baseline. The plot illustrates how MAP
degrades as tokens in the seeds are dropped with increasing probabilities.
85%
LR Adjacent
80%
(Markup)
75%
Cosine Sim
70%
(Markup)
P
65%
A
M 60%
55%
50%
45%
40%
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Probability of replacing seed tokens
Figure 5.7: Effect of noise in seed documents on the ranking performance of logistic
regression models and the cosine similarity baseline. The plot illustrates how MAP
degrades as tokens in the seeds are replaced by noise with increasing probabilities.
5.5. ERROR ANALYSIS 67
Based on these experiments, we recommend using a statistical relevance model
if enough training data is available or can be annotated. Otherwise the Cosine Sim
baseline may be a viable alternative, but this method should be used with caution if
the seed documents are short or of low quality. We will confirm these conclusions in
Section 6.3 by showing that the statistical model also yields higher QA search recall,
and that the gain over the baseline is larger when expanding short Wiktionary
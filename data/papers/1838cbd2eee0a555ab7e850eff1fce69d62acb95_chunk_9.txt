 L (x ;θ,φ)∇ L (x ;θ(cid:48))
Ds s s φ,θ Ds s θ Dt t
θ
=−α∇ (∇ L (x ;θ,φ)T∇ L (x ;θ(cid:48)))
whereL(·)isthetasklossfunction. Inthisbi-level
φ θ Ds s θ Dt t
optimization, the parameters φ of the representa-
During training, we alternatively update θ with
tiontransformationnetworkarethemetaparame-
Equation 3 and φ with Equation 4 until conver-
ters,whichareonlyusedattrainingtimeanddis-
gence. We term our method MetaXL, for its na-
cardedattesttime. Exactsolutionsrequiresolving
turetoleverageMeta-learningforextremelylow-
fortheoptimalθ∗wheneverφgetsupdated. Thisis
resourcecross(X)-Lingualtransfer. BothFigure2
computationallyinfeasible,particularlywhenthe
andAlgorithm1outlinetheprocedurefortraining
base model f is complex, such as a Transformer-
MetaXL.
based language model. Similar to existing work
involvingsuchoptimizationproblems(Finnetal., 3 Experiments
2017; Liu et al., 2019; Shu et al., 2019; Zheng
etal.,2021),insteadofsolvingtheoptimalθ∗ for 3.1 Data
any given φ, we adopt a one-step stochastic gra- We conduct experiments on two diverse tasks,
dient descent update for θ as an estimate to the namely,sequencelabelingforNamedEntityRecog-
optimalbasemodelforagivenφ: nition (NER) and sentence classification task for
Sentiment Analysis (SA). For the NER task, we
θ(cid:48) = θ−α∇ L (f(x ;φ,θ),y ) (3)
θ Ds s s
use the cross-lingual Wikiann dataset (Pan et al.,
where L (x ;) is the loss function of the lower 2017). Forthesent
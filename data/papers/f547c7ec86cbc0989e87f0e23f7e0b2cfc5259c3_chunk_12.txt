weonlinize
two large offline models (our models for IWSLT
2022OfflineSTtrackandIWSLT2023Multilin-
4.2 BlockwiseOnlineModels
gual track). This year, however, we utilize the
Finally,wesubmitsmallblockwisemodels. Their
improved blockwise beam search to yield higher
advantage is that they are able to run on a CPU
BLEUscores. Wesubmitsystemsforalllanguage
faster than real time (more than 5 faster). We
pairsbasedonthelastyear’smodel,andournew ×
reporttheirperformanceinTable5.
model. Wesummarizethesubmittedmodelsand
theirperformanceinTable3. Aswecanobserve
Lang AL AL RTF BLEU
in Table 3, the 2023 model appears to perform ↓ CA↓ ↓ ↑
En-De 1986 2425 0.19 25.4
worse. However,welearnedduringthewritingof En-Zh 1999 2386 0.19 23.8
thispaperthattherewassomeoverlapbetweenthe
trainingandtestdataforthe2022model4,making Table5: Submittedsmallblockwisemodelsusingthe
proposedCTConlinepolicy.
4(ZhangandAo,2022)foundanoverlapbetweenST-TED
trainingcorpusandtst-COMMONsetofMuST-Cdataset.
393
UELB
↑
UELB
↑
5 ConclusionandFutureWork Naˇdejde, Satoshi Nakamura, Matteo Negri, Jan
Niehues, Xing Niu, John Ortega, Juan Pino, Eliz-
In this paper, we present the CUNI-KIT submis- abeth Salesky, Jiatong Shi, Matthias Sperber, Se-
siontotheSimultaneoustrackatIWSLT2023. We bastianStüker,KatsuhitoSudoh,MarcoTurchi,Yo-
gesh Virkar, Alexander Waibel, Changhan Wang,
experimentedwiththelatestdecodingmethodsand
andShinjiWatanabe.2022. FindingsoftheI
CodeBERTScore: Evaluating Code Generation
with Pretrained Models of Code
ShuyanZhou∗ UriAlon∗† SumitAgarwal GrahamNeubig
LanguageTechnologiesInstitute,CarnegieMellonUniversity
{shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu
Abstract developers’ time when implemented in tools such
asGitHub’sCopilot. ThissharpriseinLLMs’us-
Since the rise of neural natural-language-to-
ability was achieved thanks to their ability to ac-
code models (NL→Code) that can generate
curately generate long completions, which span
long expressions and statements rather than
multiple tokens and even lines, rather than only
a single next-token, one of the major prob-
lems has been reliably evaluating their gen- a single next-token as in early models (Allama-
erated output. In this paper, we propose nis and Sutton, 2013; Movshovitz-Attias and Co-
CodeBERTScore: an evaluation metric for hen,2013). Nevertheless, evaluatingandcompar-
codegeneration, whichbuildsonBERTScore ing different models has remained a challenging
(Zhang et al., 2020). Instead of encoding
problem(Xuetal.,2022)thatrequiresanaccurate
only the generated tokens as in BERTScore,
and reliable evaluation metric for the quality of
CodeBERTScorealsoencodesthenaturallan-
the models’ generated outputs, and existing met-
guageinputprecedingthegeneratedcode,thus
ricsaresub-optimal.
modeling the consistency between the gener-
ated code and its given natural language con-
text as well. We perform an extensive eval- Existing evaluation approaches The most
uation of CodeBERTScore across four pro-
common evaluation metrics are token-matching
gramming languages. We find that Code-
methods such as BLEU (Papineni et al., 2002),
BERTScoreachievesahighercorrelationwith
adopted from natural language processing. These
humanpreferenceandwithfunctionalcorrect-
ness than all existing metrics. That is, gener- metrics are based on counting overlapping n-
atedcodethatreceivesahigherscoreby
akerifthelistenerchoosestonotchoosean
noop
image;thisisdonetopenalizethespeakerforgeneratingunclearutterances.
2.1 AGENTFORMULATION
WefollowthespeakerandlistenerformulationsdefinedinZhuetal.(2022). Thespeakerisamodel
f :I →Σ∗thatgeneratesutterances(sequencesoftokensfromitsvocabulary)thatcorrespondtoa
providedimage. WedefineΣ∗asthespeakervocabularyandI asthesetofcandidateimages.
ThelistenerisdefinedonΣ∗ andIN,whichisasetofN candidateimagessampledfromI. Given
an utterance ∈ Σ∗ and an observation space ∈ IN, the listener will either return the index of its
predictedimageornoop,ifitcannotpredictanimage. Itmayalsoreturntheground-truthcaption
fromitsvocabulary. Wedefineitwiththemodelg :Σ∗×IN →([0,1,...N −1]∪noop)×Σ∗.
2.2 SPEAKERDESIGN
The speaker that we use is a captioning model composed of a pretrained ResNet (He et al., 2016)
model and an LSTM-based utterance generation model. After generating an embedding vector
of the target image using the ResNet model, the speaker autoregressively generates an utterance
u={u }M usingtheLSTMnetwork:
i i=1
P(u |u,u,...,u,x)
i 1 2 i−1
(1)
∝exp(wT LSTM(w,w,...,w,h =ResNet(x))),
ui u1 u2 ui−1 0
where w
ui
∈ Rdw is the word embedding of u i. The speaker generates a sequence of tokens to
eitherthemaximumlength,whichissetto20inourexperiments,oranend-of-sequencetoken. In
ourexperiments, weuseavocabularysizeof200tolimitthesizeoftheactionspaceandtomore
realisticallymimicthevocabularyofalanguagelear
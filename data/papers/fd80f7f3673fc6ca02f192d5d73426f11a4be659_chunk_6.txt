canleadtorushedandnoisyjudgments evaluators. However,theystillreliedontheLLM
(Freitagetal.,2021a). toprovideascoreonceitidentifiederrors(rather
This insight has led to the adoption of the thandoitautomaticallyusingsomethinglikethe
MultidimensionalQualityMetrics(MQM)frame- MQM framework). Furthermore, they provided
work (Lommel et al., 2014; Freitag et al., 2021a) a very limited meta-evaluation using only 40 ex-
asthegoldstandardforevaluatingmachinetransla- amples per language pair. Concurrently with our
tion. TheMQMframeworkaskshumanevaluators work,Xuetal.(2023)proposedINSTRUCTSCORE,
toidentifyerrorspansincandidatetranslationsand aLLaMA-basedevaluatorthatasksmodelstoiden-
classify those errors according to various dimen- tifyandcategorizeerrorsintranslation(aswellas
sions,e.g.,fluency,accuracy,... (seeAppendixA providinganaturallanguageexplanationforeach
foramoredetaileddescriptionofMQM).Impor- error). However, the authors only explore a 7B
tantly, the MQM framework does not ask anno- parametermodelanddonâ€™tleveragezero-andfew-
tators to provide a quality score for each transla- shotcapabilitiesofmodelsasinthiswork. Instead,
tion, and instead derives one automatically from theyrelyonamorecomplexapproachofdistilling
theidentifiederrorspansandtheirclassifications. theknowledgeofamorecapableGPT-4LLM.
However,despiteitsrichness,mostautomaticmet- Additionally, WMT Word-Level Quality Esti-
ricsthatleverageMQMdataonlyusethefinalqual- mation shared tasks (Fonseca et al., 2019; Zerva
ityscoreproducedbytheframeworkanddiscard et al., 2022) leverage MQM data by converting
theerrorspaninformationandclassification. span-levelannotationsoferrors(normallyofma-
jor severity) to word-level tags and Task 2 in the
3 RelatedWork WMT19QualityEstimationsharedtas
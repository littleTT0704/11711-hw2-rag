 passages from the sources during
a primary search phase, and extracts candidate answers from these passages.
These passage searches are effective if the answer occurs in close proximity to
question key terms in the knowledge sources. SE improves search and candidate
recall, and yields relevant search results at higher ranks.
2. Similarly, Watson retrieves documents during the primary search phase and
extracts candidates from their titles. These document searches target questions
that provide one or more definitions or attributes of an entity and ask for its
name, such as the Jeopardy! question A set of structured activities or a planned
sequence of events (Answer: program). Again, the SE approach yields relevant
results for additional questions and improves the search rankings.
3. In the answer scoring phase, the search ranks of candidate answers are used as
features, and thus better rankings result in more effective candidate scoring.
4. During answerscoring, Watson performsadditional searchesfor supportingpas-
sages for each of the top-ranking candidate answers, using a query comprising
key terms from the question and the candidate itself. These passages come from
the same collection used in the initial searches, including the expanded sources.
They are used to assess whether a candidate matches the information need ex-
pressed in the question and play a crucial role in Watson’s scoring framework
[Ferrucci et al., 2010]. Thus Watson not only uses the expanded corpora in the
initial searches for candidate answers but also leverages them as a source of
additional evidence.
5. Finally, other features use corpus statistics such as idf scores, which may be
more reliable when including the expanded corpora in the sources.
Supporting passage retrieval can be computationally expensive since this step re-
quires a separate search for each individual candidate answer, and thus it may only
be feasible on parallel hardware or in batch evaluations where runtime is not impor-
tant. However, almost every QA system can leverage expanded information sources
by retrieving text passages and extracting candidate answers. Furthermore, many
open-domain QA systems use a statistical component for answer scoring that can
benefit from source expansion by including search-related features such as passage or
document ranks and scores.
Similarly to the search experiments in the previous section, we compare Watson’s
end-to-end performance using Wikipedia and Wiktionary with and without statisti-
cal source expansion. In addition, we
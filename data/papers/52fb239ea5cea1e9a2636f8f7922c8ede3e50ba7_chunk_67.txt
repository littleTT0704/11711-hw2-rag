ing tasks. In line with Bras et al. (2020), Ribeiro et al. (2020) and Welleck
et al. (2022), we evaluate generalization e.g., alternate formulations of a problem
(“2+2=?” vs. “What is two plus two?”) using an out-of-distribution evaluation
set (L¯ila-OOD) containing datasets requiring the same underlying mathemati-
cal reasoning skills, but were collected independently of the training datasets.
Further, we collect a robustness split L¯ila-Robust, that introduces linguistic
perturbations (e.g., active vs. passive voice) via crowd-sourcing. The evalua-
tion scheme is a combination of the performance on all three sets: L¯ila-Test,
L¯ila-OOD and L¯ila-Robust.
Contributions
1. We present L¯ila, a holistic benchmark for mathematical reasoning. L¯ila
extends 20 existing datasets with solutions in the form of Python programs
and instruction annotations, and categorizes questions into 23 tasks based on
their language complexity, question format and need for external knowledge.
Our benchmark measures performance on out-of-distribution examples and
robustness to language perturbations in addition to standard test-set.
2. We introduce Bha¯skara, a multi-task model fine-tuned on our dataset. Our
best-performing model achieves comparable performance to a 66 larger
×
model pre-trained on both code and language.
3. Weprovideananalysisofourmodels’performanceandfindthat(1)multitask-
ing improves considerably over task-specific learning both in in-distribution
andout-of-distributionevaluation(2)programsynthesissubstantiallyoutper-
formsanswerprediction,(3)few-shotpromptingwithcodexhasthestrongest
3
performance. We also identify areas for improvement for future work, e.g.,
data gaps in L¯ila categories.
2 Related Work
Mathematical Reasoning Datasets. Our work builds on an existing body
of mathematical reasoning literature. Early work in this areas focuses on small-
scale datasets testing addition-subtraction (Hosseini et al., 2014), templated
questionsw
 the introduction of et al., 2020; Bender et al., 2021), and (3) simply
the belief graph, a new form of visualization giveinaccurateoutputsfortaskslikequestionan-
forlanguagemodelsthatshowsrelationships
swering(Linetal.,2021). Notably,thereisgood
betweenstoredmodelbeliefs. Ourexperiments
evidencethatscalingmodelstolargersizeswillnot
suggestthatmodelsshowonlylimitedconsis-
fixtheseparticularproblemsormayevenexacer-
tencybetweenfactualbeliefs,butupdatemeth-
batethem(Lazaridouetal.,2021;Gehmanetal.,
ods can both fix incorrect model beliefs and
greatly improve their consistency. Although 2020;Linetal.,2021).
off-the-shelfoptimizersaresurprisinglystrong Intheremainderofthispaper, wepresentnew
belief-updatingbaselines,ourlearnedoptimiz-
methodsformeasuring,updating,andvisualizing
erscanoutperformtheminmoredifficultset-
factual beliefs in LMs. We further describe each
tingsthanhavebeenconsideredinpastwork.1
ofthesethreecontributionsbelow. Figure1repre-
1 Introduction sentsthecoreideasbehindmeasuringandupdating
factual beliefs, while belief visualization is done
Pretrained language models have been shown to
viabeliefgraphs(shownlaterinFigure2).
storealargeamountoffactualinformationabout
Measuring factual beliefs. We measure the de-
theworldthatcanbeelicitedbyclozeprompting
gree to which LMs possess consistent factual be-
(Petroni et al., 2019), few-shot learning (Brown
liefsusingmodelsfinetunedonfactverificationand
et al., 2020), or finetuning models for question
questionansweringtasks. Beyondsimplychecking
answering or true/false statement classification
individualmodelresponses,wewanttoassessthe
(Roberts et al., 2020). We refer to this kind of
structural properties of model outputs: Are they
storedinformationasmodelfactualbeliefs.2
consistent under paraphrase?
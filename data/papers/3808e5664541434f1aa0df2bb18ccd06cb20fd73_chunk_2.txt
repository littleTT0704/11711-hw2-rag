 Thebottom
thismethodreducesdialectalassociationswith pairshowsdialect-basedracialbiasfortwoinoffensive
toxicity. Overall,ourfindingsshowthatdebi- greetings,wheremarkersofAfricanAmericanEnglish
asingamodeltrainedonbiasedtoxiclanguage (AAE)triggerthetoxicitydetector.
dataisnotaseffectiveassimplyrelabelingthe
datatoremoveexistingbiases.
speech datasets (both shown in Figure 1): lexical
1 Introduction
biaswhichassociatestoxicitywiththepresenceof
Current hate speech or toxic language detection1 certainwords(e.g.,profanities,identitymentions;
systems exhibit problematic and discriminatory Dixon et al., 2018; Dinan et al., 2019) and di-
behavior that causes them to have disparate nega- alectalbias, wheretoxicityiscorrelatedwithsur-
tiveimpactonminoritypopulations(Yasin,2018; face markers of African American English (AAE;
Guynn, 2020; Kim et al., 2020; Dias Oliva et al., Davidson et al., 2019; Sap et al., 2019). When
2020). Tweetssimplycontainingaminorityiden- trainedonbiaseddatasets,modelsacquireandex-
titymentionarecommonlyflaggedastoxicbycur- acerbate these biases (e.g., flagging text by Black
rent systems, in contrast to those containing ma- authors as more toxic than by white authors; Sap
jorityidentitymentions,asillustratedinFigure1. etal.,2019;Zhangetal.,2018).
At the core of the issue are dataset biases, i.e., Concurrently,therehasbeenelevatedinterestin
spuriouscorrelationsbetweensurfacepatternsand developingdebiasingmethodsforstandardnatural
annotated toxicity labels (ยง2), which stem from language understanding (NLU) tasks, i.e., meth-
the data creation process (Sap et al., 2019). Pre- odsthataimtodecreaseover-relianceonspurious
vious work has outlined two such biases for hate correlationsinNLUmodels(Clarketal.,2019;He
et al., 2019; Karimi Mahabadi et al., 2020; Bras

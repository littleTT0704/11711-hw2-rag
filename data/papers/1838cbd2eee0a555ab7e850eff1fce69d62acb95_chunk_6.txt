cid:13)3 Abatchoftargetlanguagedatapassesthroughtheupdated
XLM-R and the meta loss is evaluated with the corresponding labels. (cid:13)4 The meta loss is back-propagated into
therepresentationtransformationnetwork,sincethemeta-lossisineffectafunctionofweightsfromthatnetwork,
andonlytherepresentationtransformationnetworkisupdated.
mensionoftherepresentations. Conceptually,any encodedbyboththebasemodelandtherepresen-
networkwithproperinputandoutputsizesisfea- tation transformation network. In contrast, for a
sible. Weopttoemployatwo-layerfeed-forward targetexamplex,y ∈ D,weonlypassitthrough
t t t
network, a rather simple architecture with the in- thebasemodelasusual,denotedasf(x ;θ).
t
tentiontoavoidheavyparameteroverheadontop Ideally, suppose that we have a representation
of the pre-trained model. The input to the repre- transformationnetworkthatcouldproperlytrans-
sentationtransformationnetworkisrepresentations formrepresentationsfromasourcelanguagetothe
from any layer of the pre-trained model. By de- target language. In that case, the source data can
notingrepresentationsfromlayeriash ∈ Rd,we bealmostequivalentlyseenastargetdataonarep-
i
haveaparameterizedrepresentationtransformation resentation level. Unfortunately, we cannot train
networkasfollows: sucharepresentationtransformationnetworkina
supervisedmannerwithoutextensiveparalleldata.
g (h ) = wT(ReLU(wTh +b ))+b (1)
φ i 2 1 i 1 2
Architecturally, the representation transforma-
where φ = {w,w,b,b |w ∈ Rd×r,w ∈ tion network adopts a similar structure to ex-
1 2 1 2 1 2
Rr×d,b ∈ Rr,b ∈ Rd} is the set of parame- isting works on language and task adapters for
1 2
tersoftherepresentationtransformationnetwork. cross-lingualandmulti-tasktransfer(Pfeifferetal.,
In practice
ineach
tiallyreveallatentcapabilities,whileontheother
dataset. Thesummaryoftheresultsispresentedin
hand,thereisareasonableexpectationforLLMs
Figure1,andthecompleteresultsinAppendix8.4.
to succeed in the tasks regardless of the probing
6 approachusedtoextractinformation.
https://www.ai21.com/blog/introducing-j2
5
aqiS
iMoT
Dataset ToMi’ ToM-k ToM-k contains only simple positive examples
Nosecond (variants of the original Sally-Annie test), ToMi
Subset Allquestion Allquestions
order
also contains simple alternations such as omis-
text-davinci-003 10 21 87
sionorduplicationofinformationthatcreatenega-
GPT-3.5 27 48 65
GPT-4 20 52 87 tiveexamples(seeexampleinAppendix8.1)and
second-orderquestions.
Table 4: Comparison of LLMs’ accuracy on ToM-k,
To ensure a fair comparison between the ques-
whichcontainspositiveexamplesonly,andonToMi’,
which contains both positive and negative examples tion answering format of ToMi and the sentence
(manuallyadjustedfromToMitobeofthesameprob- completionformatofToM-k(seetheeffectofprob-
ing type as ToM-k). ToM-k contains only first-order ingmethodsonperformancein§4.2),weadjusted
questions. Thesubset“Nosecondorder”wascreated ToMitomatchthesentencecompletionformat(de-
manuallytobettercomparetoToM-kdataset. Lower
tails about the adjustments can be found at §3.1).
accuracy might suggest the dataset is more robust to
Additionally,weanalyzedtheresultsseparatelyfor
spuriouscorrelations.
second-orderquestionsinordertofacilitateamore
accuratecomparisonwiththeToM-kdataset.
LM-probing predictstheoptionwiththehighest Table 4, shows significantly lower scores in
probability(Brownetal.,2020;Sapetal.,2022). ToMi
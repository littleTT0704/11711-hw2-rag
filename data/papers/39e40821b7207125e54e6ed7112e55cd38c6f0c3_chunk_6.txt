 ⊕y ⋅ x ⊕y ⋅...⋅ x ⊕y,where⊕
1 1 2 2 k k
popularity in the training data of modern Code-
isasymbolthatseparatesaninputfromitsoutput,
LLMs (Xu et al., 2022), but our approach is ag- ⋅
and separatesdifferentexamples.
nostic to the programming language. The code-
Anew(test)inputxisappendedtotheprompt
transformed graphs are similar in their format to p(thatis: p ⋅ x),andp ⋅ x ⊕ isfedtothemodel
thepre-trainingdataofCode-LLMs,andthusserve
forcompletion. AsfoundbyBrownetal.(2020),
aseasiertogeneralizetrainingorfew-shotexam-
largelanguagemodelsshowimpressivefew-shot
plesthantheoriginalrawgraph. COCOGENuses
capabilitiesingeneratingacompletionyˆgiventhe
Code-LLMs to generate G c given T, which we input p ⋅ x ⊕. The main question is how to
eventuallyconvertbackintothegraphG.
constructtheprompt?
Weusethetaskofscriptgeneration(PROSCRIPT,
In all experiments in this work, the prompt p
Figure 1) as a running example to motivate our
consists of k Python classes, each representing a
method: scriptgenerationaimstocreateascript(G)
(T,G ) pair. For example, for script generation,
c
toachieveagivenhigh-levelgoal(T).
eachPythonclassrepresentsagoalT andascript
G from the training set. Given a new goal T for
2.1 Converting(T,G)intoPythoncode c
inference,apartialPythonclass(i.e.,onlyspecify-
We convert a (T,G) pair into a Python class or ingthegoal)iscreatedandappendedtotheprompt.
function. The general procedure involves adding Figure2showssuchapartialclass. Here,thecode
the input text T in the beginning of the code as generationmodelisexpectedtocompletetheclass
a
ALFRED
A Benchmark for Interpreting Grounded Instructions for Everyday Tasks
MohitShridhar1 JesseThomason1 DanielGordon1 YonatanBisk1,2,3
WinsonHan3 RoozbehMottaghi1,3 LukeZettlemoyer1 DieterFox1,4
AskForALFRED.com
Abstract Goal: "Rinseoffamugandplaceitinthecoffeemaker"
2
WepresentALFRED(ActionLearningFromRealistic " frp oic mk tu hp et ch oe ffd ei erty mm aku eg r" 3 "turnandwalktothesink"
Environments and Directives), a benchmark for learning 1
"walktothecoffee
makerontheright"
a mapping from natural language instructions and ego-
centric vision to sequences of actions for household tasks.
ALFRED includes long, compositional tasks with non-
t=0 t=10 t=21
reversiblestatechangestoshrinkthegapbetweenresearch visualnavigation objectinteraction visualnavigation
benchmarks and real-world applications. ALFRED con- 5 6
4 "pickupthemugandgo "putthecleanmug
sistsofexpertdemonstrationsininteractivevisualenviron- "washthemuginthesink" backtothecoffeemaker" inthecoffeemaker"
ments for 25k natural language directives. These direc-
tives contain both high-level goals like “Rinse off a mug
and place it in the coffee maker.” and low-level language
instructions like “Walk to the coffee maker on the right.”
ALFRED tasks are more complex in terms of sequence t=27 objectinteraction t=36 visualnavigation t=50 objectinteraction
length, action space, and language than existing vision- statechanges memory
and-languagetaskdatasets. Weshowthatabaselinemodel Figure 1: ALFRED consists of 25k language directives
based on recent embodied vision-and-language tasks per- correspondingtoexpertdemonstrationsofhouseholdtasks.
formspoorlyon ALFRED,suggestingthatthereissignif- We highlight several frames corresponding to portions of
icant room for developing innovative grounded visual
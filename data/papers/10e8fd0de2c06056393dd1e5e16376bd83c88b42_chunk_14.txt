,butthecontextdoesnotactuallyanswerthe
alsoconjecturethatdatasetsfallingunderthecate-
givenquestion. Forexample,thequestion“What
goryof“Labelshift”and“Covariateshift”aremore is the name of the office used by the president in
amenabletozero-shotdatainterventions,however,
thewhitehouse?"andanswer“oval",theretrieved
“Fullshift”wouldbenefitmostfromfew-shotexam-
passage“AtunnelwasdugintotheWhiteHouse
plesorcollectingannotationsinthetargetdomain. connectingtheOvalOfficetoalocationintheEast
Weconsiderfewshotaugmentationsasaproxyfor
Wing...."iscreditedtoabletoanswerthequestion.
annotatingexamplesinthetargetdomainbecause
Thisshowsthatend-to-endperformanceiscrucial
theaugmentationsaregeneratedwithsupervision
inunderstandingimprovementsinretrieverswhich
fromtargetdata.
isoftenignored.
4 HowWelldoModelsGeneralize?
4.2 RetrieverGeneralization
In this section, we want to first get a sense of To analyze model performance further, we com-
how well existing SotA ODQA models perform pare the zero-shot generalization performance of
whentestedOOD.WetesttheOODperformance fourdifferentretrievalmodelsinfigure4: BM25,
ofsource-trainedmodelsontargetdomainvalida- Contriever,SpiderandDPR.
tionsetsand,whentheyfail,analyzewhatcaused OneobservationwefindisthatSpider,thebest
thoseerrors. performingmodelonthesourcedomain,exhibits
animprovementonSearchQA(∼1%)(whichuses
4.1 End-to-EndZero-shotGeneralization
thesameunderlyingsourceWikipediadomain),but
inFigure5,wetesttheend-to-enddomainadaption showslargedropsinperformancewhenappliedto
performanceofthreemodelvariants: thetargetdatasets: ∼40%onNewsQA,∼28%on
Source: afullysourcedomaintrainedmodel
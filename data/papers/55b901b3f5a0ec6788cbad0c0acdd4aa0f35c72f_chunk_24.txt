.2 0.3 0.4 0.5 0.6 0.7
Fraction of Runs Fraction of Runs
Figure5: Toprankedobjectives(averagedweight)earlyintraining(left)andlaterintraining(right)
supervisedoutput = DENOISE arehighlyweightedbutlater,objectivesbasedonsupervised
O { }
signal, = Task play a larger role. AANG rediscovers the common practice of training on
O { }
self-supervised objectives before introducing supervised ones. It is also interesting to note that
manynewlygeneratedobjectives(outsideofthe3namedsingletaskbaselinesinTable2)suchas
simpleinputreconstructionwerediscoveredtohaverelevantimpactontheend-tasks. Thismeans
AANGcanautomaticallysurfacenew,previouslyunexploredobjectivesrelevanttotheend-task.
8 LIMITATIONS AND CONCLUSION
Our work has some limitations that we leave for future work. First, because AANG relies on
meta-learning,itpresentsextracomputeburdenoversimplemultitasking. Thisisbecause,wehaveto
independentlycomputemeta-gradientsforeachauxiliarytaskthusrequiring (n)forward-backward
O
operationsfornsampledtaskscomparedto (1)forstaticmultitasking. InTable2,weshowthat
O
ourstaticMultitask-TDmethodoutperformsallothernon-task-adaptivemethodsby 2.4%and
≈
isthusaviablealternativewhenruntimeisasignficantconstraint. Secondly,AANGaspresented
isanapproximatealgorithm–primarilyduetosub-samplingthespaceoftasks. Thusasmentioned
inSection7.2,wedonotgetasmuchgainasdesiredwhenoursearchspacebecomeslarger. We
leavefindinganefficientexactsearchalgorithmforfutureexploration.
This paper presents a procedure for automating the creation of auxiliary objectives. We showed,
theoretically,howauxiliarylearningimpactsend-taskgeneralization. Thisresultedinprescriptions
thatinformedthedesignofAANG,analgorithmtosearchthespaceofgeneratedobjectivesinan
end-task aware multitaskfashion.
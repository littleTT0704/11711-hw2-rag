comparesthebaseLMandtheoriginalkNN-LMversususingeitherattentionlayeroutput(“att”)
orfeedforwardlayeroutput(“ffn”)ash. Weplotthenumberofembeddingsforeachwordtype(nV total
ds
embeddingsinW )versustheinterpolatedperplexity,withfulldetailsfoundinAppendixB. Inbothcases,
ds
comparingwiththetophorizontallinewhichrepresentstheperplexityofthebaseLM,replacingthedatastore
withamuchsmallerweightmatrix(fromN tonV )byassigningonlyafewmoreembeddingsforeach
ds ds
wordhelps,althoughonlyabouthalfaseffectiveaskNN-LM.Togiveaperspective,theoriginaldatastore
sizeisabout5000V. Surprisingly,wefindthatincreasingndoesnotalwaysbringbetterperformance,even
thoughalargerdatastoreisbetterthanusingasmalldatastoreinkNN-LM.Wecanseethatwhenh =ffn,
ds
over-parameterizationprovidesverylimitedimprovements,whileforh =attitdoesnotbringconsistent
ds
improvementsatall.ComparingthetrendofincreasingtheembeddingsinW,withthebottomhorizontalline
ds
intheplot,whichrepresentstheperplexityofthestandardkNN-LMusingthefulldatastore(W withapprox.
ds
5000V embeddings),wecanseenocleartrendthatmoretrainableembeddingsresultinbetterperplexity,and
thatthegapbetweenusingtrainedembeddingsandusingfulldatastoreisstillsignificant. Thissuggeststhat
simplyover-parameterizingW isnotaneffectivemethodofachievingaccuracygainssimilartokNN-LM.
ds
We hypothesize that this is because by just adding more embeddings, while still using the same training
procedureastheoriginalLM,themultipleembeddingsforeachwordtypeafterlearningcouldstillbevery
closetoeachother,andthusdonotincreasethesoftmaxcapacitymuch. Thissuggeststhatsomeregularization
terms may be needed during training to make the multiple embeddings not converge to the
ofInterest R1↑ R5↑ R10↑MR↓ R1↑ R5↑ R10↑MR↓
None 15.038.7 51.3 10.0 23.751.3 63.9 5.0
det+adp 14.738.5 51.2 10.0 23.351.0 63.5 5.0
themodelfor30kiterationswithbatchsize128. Foreach noun 15.439.3 51.8 10.0 24.051.8 65.1 5.0
training sample, we use our cascade sampling strategy to verb 15.339.0 51.4 10.0 23.952.1 64.8 5.0
sample 8 hard negatives. We use Adam [24] as the opti- noun+verb 15.839.8 52.4 10.0 24.552.8 65.5 5.0
mizerwithinitiallearningrate1e−4. Alinearlearningrate Table4: Text-videoretrievalperformancewithdifferentto-
decay is applied after 5k warm-up iterations. The weight kensofinterestforcomputingtoken-levelcontrastiveloss.
decayissetto1e−5.
“det” means determiner; “adp” means adposition. We use
Pretraining and finetuning. We pretrain our model on thesamevideofeaturesasinTable3.
Howto100M[35]. Sincetheoriginalannotatedvideoclips
in Howto100M are usually short with a few seconds, we
contrastive learning method has been adopted in a number
mergetheadjacentclipssothattheresultedtexthasatleast
ofpreviousworks[60,33]. Thiscomparisoncanverifythe
10 words. We use Adam [24] as the optimizer with ini-
tial learning rate 1e−4. We train the model for 500k itera- effectiveness of our proposed contrastive learning method
considering two models have exactly the same number of
tions with batch size 64, and also sample 8 hard negatives
parameters. In Table 2, we can see our proposed method
foreachsampleusingourcascadesamplingstrategy. After
outperformsbaselineacrossallfeaturetypes introducedin
pretraining,
 set of candidate passages, and S the subset of those passages that were already
selected in previous iterations. Then a passage P is selected such that
sel
(cid:20) (cid:21)
P = arg max λ CosSim(P,Q)−(1−λ) max CosSim(P,P(cid:48)).
sel
P∈R\S P(cid:48)∈S
The objective function is a weighted average of a relevance term and a novelty
term, and the parameter λ ∈ [0,1] controls the tradeoff between these components.
If λ = 1 then the most relevant passages are selected regardless of their redundancy,
and if λ = 0 then the algorithm selects a maximally diverse sample of text. In source
expansion we focus on finding relevant information, and we include reformulations of
previously selected text since they may facilitate answer extraction and validation in
question answering. In addition, we need not worry about lexically redundant content
or even exact duplicates since such text is filtered out in the final merging phase of
the SE system. Thus values of λ close to 1 can be expected to be most effective for
our application.
Goldstein et al. [2000] compute term weight vectors for the query and candidate
text passages, and measure the cosine similarity between these vectors. For instance,
if V is the vocabulary of all terms, then the cosine similarity between a candidate
passage P and the query Q is defined as
(cid:80)
w (t) w (t)
P Q
t∈V
CosSim(P,Q) =,
(cid:114)
(cid:80) (cid:80)
w (t)2 w (t)2
P Q
t∈V t∈V
where w (t) and w (t) are the weights of the term t in passage P and in query Q,
P Q
respectively. Goldstein et al. remove stopwords and stem the remaining tokens, and
they use lnn term weights (i.e. logarithmic term frequency, no document frequency,
no normalization). For example, w (t) = 1+log(f (t)) where f (t) is the frequency
P P P
count of term t
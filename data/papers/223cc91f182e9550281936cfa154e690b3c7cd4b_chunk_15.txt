+U(ξ)
s.t. q(θ) ∈ Q(ξ) (2.16)
ξ ≥0,
where we have rearranged the terms and dropped any constant factors in Equation 2.15, and added constraints
with ξ being a vector of slack variables, U(ξ) a penalty function (e.g., ℓ norm of ξ), and Q(ξ) a subset of
1
valid distributions over θ that satisfy the constraints determined by ξ. The optimization problem is generally
easy to solve when the penalty/constraints are convex and defined w.r.t. a linear operator (e.g., expectation) of
the posterior q. For example, let T(x∗;θ) be a feature vector of data instance x∗ ∈ D, the constraint
posterior set Q can be defined as:
Q(ξ) :={q(θ) : Eq [T(x∗;θ)]≤ξ, ∀x∗ ∈ D}, (2.17)
which bounds the feature expectations with ξ.
Max-margin constraint is another expectation constraint that has shown to be widely effective in classification
and regression (Vapnik, 1998). The maximum entropy discrimination (MED) by (Jaakkola et al., 2000)
regularizes linear regression models with the max-margin constraints, which is latter generalized to more
complex models p(x∣θ), such as Markov networks (Taskar et al., 2004) and latent variable models (Zhu et
al., 2014). Formally, let y∗ ∈ R be the observed label associated with x∗. The margin-based constraint says
that a classification/regression function h(x;θ) should make at most ϵ deviation from the true label y∗.
11
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
Specifically, consider the common choice of the function h as a linear function: h(x;θ) = θ⊤T(x),
where T(x) is, with a slight abuse of notation, the feature of instance x. The constraint is written as:
y∗ −Eq θ⊤T(x
processing. InProceedingsofthe2019ConferenceoftheNorthAmerican
ChapteroftheAssociationforComputationalLinguistics: Tutorials,pp.15–18,2019.
12
PublishedasaconferencepaperatICLR2023
NikunjSaunshi,SadhikaMalladi,andSanjeevArora. Amathematicalexplorationofwhylanguage
modelshelpsolvedownstreamtasks. arXivpreprintarXiv:2010.03648,2020.
SteffenSchneider,AlexeiBaevski,RonanCollobert,andMichaelAuli. wav2vec: Unsupervised
pre-trainingforspeechrecognition. arXivpreprintarXiv:1904.05862,2019.
KennethOStanleyandRistoMiikkulainen.Evolvingneuralnetworksthroughaugmentingtopologies.
Evolutionarycomputation,10(2):99–127,2002.
MingxingTanandQuocLe.Efficientnet:Rethinkingmodelscalingforconvolutionalneuralnetworks.
InInternationalConferenceonMachineLearning,pp.6105–6114.PMLR,2019.
SangMichaelXie,AditiRaghunathan,PercyLiang,andTengyuMa. Anexplanationofin-context
learningasimplicitbayesianinference. arXivpreprintarXiv:2111.02080,2021.
ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,andQuocVLe.
Xlnet: Generalizedautoregressivepretrainingforlanguageunderstanding. Advancesinneural
informationprocessingsystems,32,2019.
XingchengYao,YananZheng,XiaocongYang,andZhilinYang.Nlpfromscratchwithoutlarge-scale
pretraining: Asimpleandefficientframework. arXivpreprintarXiv:2111.04130,2021.
Tong Zhang, Peng Gao, Hao Dong, Yin Zhuang, Guanqun Wang, Wei Zhang, and He Chen.
Consecutive pretraining: A knowledge transfer learning strategy with relevant
Title: The State of Intent Detection in the Era of Large Autoregressive Language Models
Venue: 
Year: 2023
Abstract: In-context learning (ICL) using large pre-001 trained autoregressive language models (LLMs, 002 e.g. GPT-3) has demonstrated effective clas-003 sification performance at a variety of natural 004 language tasks. Using LLMs for intent detec-005 tion is challenging due to the large label space 006 and limited context window, such that it is diffi-007 cult to fit a sufficient number of examples in the 008 prompt to allow the use of in-context learning. 009 In this paper, dense retrieval is used to bypass 010 this limitation, giving the model only a par-011 tial view of the full label space. We show that 012 retriever-augmented large language models are 013 an effective way to tackle intent detection, by-014 passing context window limitations effectively 015 through the retrieval mechanism. Comparing 016 the LLaMA and OPT model families at differ-017 ent scales, we set new state of the art perfor-018 mance in the few-shot setting with zero training 019 for two of the three intent classification datasets 020 that we consider, while achieving competitive 021 results on the third one. This work demon-022 strates that the Retriever+ICL framework is a 023 strong zero-training competitor to fine-tuned in-024 tent detection approaches. In addition, a small 025 study on the number of examples provided at 026 different model scales is done, showing that 027 larger models are needed to make effective use 028 of more examples in-prompt. 029
CMU authors: Daphne Ippolito
Other authors: Tom B. Brown, Benjamin Mann, Nick Ryder, Jared D Subbiah, Prafulla Kaplan, A. Dhariwal, P. Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott teusz Litwin, Benjamin Gray, Jack Chess, Christopher Clark, Sam Berner, Alec McCandlish, Ilya Radford, Sutskever Dario, Amodei, Matthew Henderson, Ivan Vulic. 2020, Ef-310, Aakanksha Chowdhery, Sharan Narang, J. Devlin, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek, Parker Rao, Yi Barnes, Noam Tay, Vin-316 Shazeer, Emily odkumar Prabhakaran, Nan Reif, Ben Du, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, H. Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, L. Fedus, Denny Zhou, D. Luan, Hyeontaek Lim, Barret Zoph, A. Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Oleksandr Polozov, K. Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta
tldr: The Retriever+ICL framework is a strong zero-training competitor to fine-tuned in-024 tent detection approaches, and is set new state of the art in the few-shot setting with zero training.

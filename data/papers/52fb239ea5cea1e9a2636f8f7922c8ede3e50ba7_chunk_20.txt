iquesandprogramming This means models are often able to generate a
styles. programthatevaluatestothecorrectanswer,even
when the model cannot directly compute the an-
5 ResultsandAnalysis swer.
Program synthesis improves over answer pre-
AsummaryofallkeyresultsonourL¯ILAbench-
diction in all math categories except Geometry,
mark are shown in Table 3. In this section, we
withthelargestimprovementsinStatisticsand
will discuss the performance of fine-tuned 2.7B
Linear Algebra; see Table 5 for examples. We
GPT-Neo models (§5.1), performance of models
even see benefits of program synthesis in NLI,
along the 4 categories of tasks (§5.2) and finally,
a classification-based task. L¯ILA’s unified prob-
thefew-shotperformanceofmuchlarger( 175B
∼ lemformatdecouplessynthesisfromcomputation,
parameters)models(§5.3).
whileopeningdirectionsforfurtherstudyoneither
aspect.
5.1 Results: Fine-tunedModels
Multitasking improves IID performance, ro- Models leverage symbolic execution and li-
bustness,andOODgeneralization. Themulti- braries. Thegapbetweenprogramsynthesisand
tasking model (BHA¯SKARA) substantially im- answer prediction suggests that the neural lan-
proves upon the single task models (Neo). guage model offloads computations to the sym-
BHA¯SKARAachievesbetteraveragein-domainper- bolicPythonruntimethatareotherwisedifficultto
formance than the 23 individual per-task models computedirectly. Weidentifytwocommoncases.
(0.480 vs. 0.394 average score), suggesting that First, the model leverages standard Python as a
it leverages cross-task structure not present in a calculator. For instance, this pattern is common
singletask’strainingset.
inthebasic_mathandmul_divcategories,which
involveevaluatingarithmeticexpressions;Table4
We also find that our multi-task model is ro-
busttothelinguisticperturbationswetestin
oftheaccuracy-efficiencytrade-off. Interestingly,GPT-2,
10 1B
despite having fewer parameters, lags behind BART in
termsofthroughput,latency,andenergyconsumption.We Latency (ms)0 20 40 60 20 40 60 80F1
believethatthiscouldbeduetothein-contextfew-shot 3 30
examples, which lead to significantly longer inputs for 2 20
GPT-2comparedtoBART.Nonetheless,GPT-2manages
1 10
toachievethehighestF1scoreinthisexperiment.
0 0
GPU Memory (GiB) Energy (W.h)
B Additional
Figure4: Models’efficiencyandaccu-
ExperimentswithMachineTranslation racyperformanceonRAFTtestdata. F1
numbersareduetoAlexetal.(2021).
All models’ implementation and checkpoints are available on Hugging Face, with the following
identifiers:
• MBART50: facebook/mbart-large-50-many-to-{many, one}-mmt;
• M2M100: facebook/m2m100_{418M, 1.2B};
• OPUS:Helsinki-NLP/opus-mt-de-en;
• WMT19-Meta: facebook/wmt19-de-en;
• WMT21-Meta: facebook/wmt21-dense-24-wide-en-x.
AdditionalFP32vs.FP16comparisons.Figure5providesanadditionalsetofcomparisonsbetween
FP32andFP16acrossvariousmodelsonWMT14DE-EN,complementingtheresultspresentedin
Section3. Thegeneraltrendsmirrorthoseobservedearlier,withlargermodelsbenefitingmorein
termsofefficiencyfromquantizationcomparedtosmallerones.
ONNX improves throughput, latency, and energy overhead, at the cost of increased GPU
memoryoverhead. Pentathlonmakeslittleassumptionsonthemodels’implementationandbackend
runtime, andallowsuserstousebotheager-executionresearchframeworkslikePyTorchaswell
as specialized inference runtimes like Open Neural Network Exchange (ONNX). Here we study
ONNX’
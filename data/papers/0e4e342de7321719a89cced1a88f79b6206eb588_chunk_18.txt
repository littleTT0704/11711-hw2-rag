reesettingsinTable6,themodelwith5candidateker-
k
k-thcandidatekernelatblocki. nels performs the best, and having fewer or more candidate
kernelsnegativelyaffectstheperformance.
Number of fusion layers. We show that query-aware vi-
extractquery-awarevisualfeatures.
sualrepresentationsproducedbyvisualencoderswithQCM
are better in the sense that it requires less or no further fu-
4.4 AblationStudy
sion layers. As shown in Table 7, our method outperforms
Convolutionkernelgenerationfromtextandimage. We TransVGwithfewerfusionlayers,andVGQCwitha4-layer
showthattheperformanceimprovementbroughtbyQCMis fusionmodulehascomparableperformancetoVGQCwitha
notattributedtotheintroductionofnewparameters(theatten- 6-layerfusionmodule.
tion and candidate kernels), but the incorporation of textual
information. In our experiments, we apply two other meth- 5 Conclusion
odstoderivetheattentionweightvectorαandthustheaggre-
In this paper, we propose a query-conditioned convolu-
gatedkernelineveryQCMblock.Method(a):αisrandomly
tion module (QCM) to be integrated into visual encoders
initializedasalearnableK-dimensionalvector. Method(b):
to extract query-aware visual features. We also present a
α is computed from the output feature map of the previous
query-conditionedvisualgroundingpipeline(VGQC)utiliz-
CNN layer through average-pooling and a linear projection
ing QCM to address the limitation of the previous extract-
followedbyasoftmaxlayer.Wecomparethemwithourmain
and-fusepipeline,wheretheextractedvisualfeaturesmaynot
method, where the attention weights are computed from the
convey the required information described in the text query.
text query, as illustrated in Section 3.2. All three methods
Ourquery-awarevisualfeaturesnotonlyfacilitatethemulti-
utilize attention weights to compose the aggregated kernels,
modal interaction but are also informative enough to be di-
but(a)and(b)donotincorporat
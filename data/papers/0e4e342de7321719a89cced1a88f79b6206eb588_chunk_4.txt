ÔøΩ!" ùëì!" Pre Hd eic at dion
Query-Conditionedvisualencoder. Differentfromotherfu-
image
Backbone w/ QCM T Er na cn os df eo rr smer
sion methods [Joze et al., 2020; Nagrani et al., 2021] that (x,y,w,h)
enableunbiasedmulti-modalinteractionbi-directionally,our [CLS] Textual
methodsolelyimportstextualinformationintothevisualrep- qt ue ex rt y Encoder
resentation, and it happens during the process of extracting
(b) VGQC pipeline without fusion
visual features instead of the post-processing of extracted
staticvisualfeatures.Inaddition,query-awarevisualfeatures Figure 2: Two variants of our query-conditioned visual grounding
generatedbyQCMareinformativeenoughtobedirectlyused (VGQC) pipelines. (a) combines query-aware visual features and
for prediction without the requirement for additional multi- queryfeatureswithafusionmodule, while(b)removesthefusion
modalfusionmodulesusedinpreviousmethods[Dengetal., moduleandpassesquery-awarevisualfeaturesdirectlyintothepre-
2021;Duetal.,2021],resultinginasimplerVGQCpipeline. dictionhead.
The experiment results show that VGQC w/ fusion achieves
state-of-the-artperformanceonnearlyalltestingdatasets,and
scores [Yu et al., 2018], and recently many two-stage meth-
VGQCw/ofusionincreasestheinferencespeedwhileachiev-
ods focus on modeling the inter-object relationships using
ingcomparableperformanceasthelatestmethods. Then,ex-
graphrepresentations. [Yangetal.,2019;Wangetal.,2019;
tensive analysis and visualization illustrate the effectiveness
Hongetal.,2019;Liuetal.,2020].
ofQCMandquery-awarevisualfeatures.
One-stage methods. One-stage methods directly predict
Insummary,thecontributionofourworkisthree-fold:
theboundingboxesandmostofthemadopttheextract-and-
1. Wepresentanovelquery-conditionedconv
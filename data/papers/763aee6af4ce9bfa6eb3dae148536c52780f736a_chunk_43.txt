abovelemmas,i.e. parameterizationgapandempiricalgap,viaapplyingthe
triangleinequalitycompletestheproof.
(cid:12) (cid:12)P(cid:63)−Dε(cid:63),n(γ)(cid:12) (cid:12)=(cid:12) (cid:12)P(cid:63)+D(cid:15)(cid:63)(γ)−D(cid:15)(cid:63)(γ)−D(cid:15)(cid:63),n(γ)(cid:12)
(cid:12)
≤|P(cid:63)−Dε(cid:63)(γ)|+|D(cid:15)(cid:63)(γ)−D(cid:15)(cid:63),n(γ)|
(cid:115)
(cid:20) (cid:18) (cid:19)(cid:21)
1 4(2n)dvc
≤(1+|λ|)m+2B 1+log
n δ
B.DomainGeneralizationbyLearningonFictitiousDistributions
Thissectiongivesajustificationfordisentanglementfromadifferentperspectivebyconnectingthedotswithclassical
domainadaptation. Specifically,weconstructafictitiousdistributiontoextendittotheDGsettinganddecomposethetarget
learningobjectiveintoempiricallearningerrors,domaindivergenceandsourcedomaindatadiversity. Moreover,weshowthat
learningdisentangledrepresentationsgivesatighterriskupperbound.
Withaslightabuseofnotation,letHbeahypothesisspaceanddenoteD˜ astheinduceddistributionoverfeaturespaceZ
foreverydistributionDovertherawspace. DefineDi asthesourcedistributionoverX,whichenablesamixtureconstruction
S
ofsourcedomainsasDα =(cid:80)Ns α Di(·). DenoteafictitiousdistributionDα =(cid:80)Ns α∗Di(·)astheconvexcombination
S i=1 i S U
asterGPUsyieldingfasterexecution. Ad- tionalgraphisrelatedtomodeldepth.
ditionaldetailsonhardwaresystemconfigurations
areprovidedinAppendixA. Estimations of latency for models deployed in
productionsettingsmustaccountfortheirtarget
frameworkandhardwareplatform. Modelde-
5 Discussion
velopmentfrequentlyoccursusingeagerexecution
researchframeworks. However,deploymentoften
Computational graph optimizations and com-
occursininferenceruntimesandonmobiledevices
pilationimprovelatency. Removalofhostlan-
or specialized hardware. This misalignment can
guagedependenciesandgraphoptimizationspro-
misleadthedevelopmentof“efficient”modelsand
videssubstantialspeedupsovereagerframeworks
resultinclaimedgainsthatdonottranslatetoreal-
for inference at low batch sizes. However, fea-
world deployment settings. As such, researchers
ture completeness for operators and control flow
shouldbeclearinspecifyingthesettingoftheir“ef-
variesacrossgraphoptimizersandcompilers. For
ficiency"gains,suchasthetargetframeworksand
example, theFNetarchitecture(Lee-Thorpetal.,
hardwareplatform,whendevelopingnewmethods.
2021) relies on FFTs as a deterministic swap-in
Forexample,techniquessuchashardware-aware
forself-attention. FFToperationsarenotcurrently
neural architecture search which leverage direct
supportedbyONNXorTorchScriptAsexpected,
latencymeasuresmustalsocontrolforframework
FNetexecutedinPyTorchoutperformsBERTexe-
choicestoaccountforthismismatch.
cutedinONNXdespitelessframeworkoverhead
andnumerousstaticoptimizations–witha10.31% Throughput and input size can be increased
speedup at batch size 1. For improvements in re- at minimal cost for framework-bound models.
searchtotranslatetodeployment,additionalinvest- For a given model, latency is constant regardless
mentcanbedirectedtowardssupportforcomplex of batch size until compute kernels saturate and

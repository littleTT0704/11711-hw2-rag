 r hyperparameters in our implementation, for a learned
optimizertrainedforaBART-basemodelonzsRE,usingabatchsizeof16. Forcomparison,theorangedashed
lineshowsthememoryuseoftrainingtheBART-basemodelonzsRE,usingthesamebatchsize. Ouruseofthe
stop-gradientfunctionlimitsthegrowthofruntimeandmemoryw.r.t. bothK andr. Byaccumulatinggradients
acrosspoints,memoryw.r.t. riskeptconstant. ThesametrickcouldbeappliedtotheK loopedgradientsteps
insidetheUpdatefunction,atthetrade-offofbackpropagatingK timesperpointratherthanonetime.
Ours DeCaoetal.(2021) Mitchelletal.(2021)
UpdateSuccessRate(MainInput) Successrate Editsuccess
UpdateSuccessRate(Paraphrase) Equivalenceaccuracy Editsuccess
UpdateSuccessRate(EntailedData) - -
RetainRate(LocalNeutral) - -
RetainRate(AllData) Retainaccuracy -
∆-Acc(AllData) Performancedeterioration Drawdown
Table13: Aglossaryoftermsusedinworkonmodelupdatemethods. Notemetricsarenotalwayscalculated
inexactlythesameway. Forinstance,Performancedeteriorationisaratioinaccuraciesratherthandifferencein
accuracies,andeditsuccessfromMitchelletal.(2021)combinestwometricsinourcase. Theperformancemetric
inZhuetal.(2020)isanaverageofUpdateSuccessRate(MainInput)and∆-Acc.
available under the MIT license (Petroni et al., LeapOfThought. Many examples use an “is a”
2021). LeapOfThought data can be constructed relation,producingsentenceslike“Asunlightisa
throughtheiravailablecode5 andisalsoavailable good health.” This could bemore false thantrue,
undertheMITlicense. ThesourcedataforWiki- butit’safairlynonsensicalstatementtobeginwith.
data5m data can be downloaded through the KE- Therearealsoothernonsensicalorvagueexamples
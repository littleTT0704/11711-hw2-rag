acetext
3202
yaM
62
]LC.sc[
2v34501.2122:viXra
denoisingwithcontrollabletextgenerationusinga oftheentiresequence. Thoughinspiredby DEX-
ProductofExperts(PoE)(PoE,DEXPERTS;Hin- PERTS (Liu et al., 2021), our novelty is two-fold:
ton,2002;Liuetal.,2021). first, we tackle a more challenging task, unsuper-
MARCO jointly uses an expert and an anti- visedrevision,insteadofstyle-controlledgenera-
expert,apairoflanguagemodels(LM)fine-tuned tion,andsecond,weproposeadetectandrewrite
on a non-toxic and toxic corpus respectively, to pipeline, in contrast to simple word-distribution
identifywhichtokensmostlikelycontributetothe steeringduringautoregressivegeneration.
overalltoxicity,andthensuggestreplacementsthat
ExpertandAnti-ExpertLMs Ourmethodfor
lower toxicity. Using LMs to capture toxicity al-
unsupervised controlled revision is based on de-
lows MARCO to rewrite much subtler toxic text
noising autoencoder LMs (AE-LMs), which are
comparedtopreviousworkthatusestoxicityclas-
trainedtomaskandreconstructsequencesoftext.
sifiersortoxicwordlists(Daleetal.,2021).
OursetupconsistsofabasepretrainedAE-LMG,
WeapplyMARCOtothreedatasetsfocusedon
anexpert AE-LMG+ finetunedondatawithde-
subtlytoxicstatements,suchasmicroaggressions.
sirableattributes,andananti-expertAE-LMGâˆ’
Ourmethodoutperformsstate-of-the-artdetoxifi-
finetunedondatawithundesirableattributes.
cationbaselinesfromDaleetal.(2021)acrossall
We use BART-base (Lewis et al., 2020) as our
threedatasets,asmeasuredthroughbothautomatic
baseautoencoder. Wefinetunetheexpertandanti-
andhumanevaluation.
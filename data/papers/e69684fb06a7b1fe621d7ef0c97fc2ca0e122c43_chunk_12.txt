AD MCoNaLa Temporal Method #Train Performance Anno.Cost
(EM) (ChrF++) (ChrF++)
Retrievalonly 3,000 56.79 ≈$0
Prompt2Model 61.5 13.1 55.2 Generationonly 3,000 44.20 ≈$5
w/oModelRet. 61.5 15.8 55.2 Retrieval+generation 6,000 61.46 ≈$5
w/oDataRet. 50.2 16.6 N/A
Customannotation 3,000 61.64 ≈$540
gpt-3.5-turbo 42.1 37.3 30.7
Table2: WecomparemodelperformanceonSQuAD
Table 1: We evaluate the model produced by onanannotation-costbasis,usingdatasetsproducedby
Prompt2Model on real benchmarks for each test set, differentmodulesofPrompt2Model,alongwithfully-
comparedtogpt-3.5-turbo,whichweusedtopower manualannotation.Performancereportedforallmodels
ourdatasetgenerator. Wealsoexaminetheeffectofre- is the exact match on the test set,7 which reflects the
movingspecificpartsofourpipeline—modelretrieval true task performance. Cost of custom annotation is
and dataset retrieval. There are no relevant datasets estimated from Rajpurkar et al. (2016) using their re-
availablefortheTemporaltask, sowedidnotusere- portedannotatorpayrateof$9/hourandkeeping1,000
trieveddataforPrompt2Modelthere. validationexamples.
baselineLLMgpt-3.5-turbo,onrealbenchmark
datasetsforeachtask—SQuAD,MCoNaLa,and true dataset. We use SQuAD as a running exam-
Temporal. We further examine the effect of re- ple.5 AsourpromptisadescriptionoftheSQuAD
moving2specificelementsofthePrompt2Model passage-levelquestionansweringtask(Figure1),
pipeline—modelretrievalanddatasetretrieval. weexcludeSQuADfromourretrieveddatasetslist.
On2of3datasets,we
 2022 Toward a 'Standard Model' of Machine Learning
Thus maximum entropy is dual to maximum likelihood. It provides an alternative view of the problem of
fitting a model into data, where the data instances in the training set are treated as constraints, and the learning
problem is treated as a constrained optimization problem. This optimization-theoretic view of learning will be
revisited repeatedly in the sequel to allow extending machine learning under all experience of which data
instances is just a special case.
2.1.2. Unsupervised MLE
Similar to the MLE framework for supervised learning, unsupervised learning via MLE can also be
reformulated as a constraint optimization problem with entropy maximization. Consider learning a multivariate
model with latent variables, where each data instance is partitioned into observed variables x ∈ X and latent
variables y ∈ Y. For example, in the problem of image clustering, x ∈ Rd is the observed image of d pixels
and y ∈ {1,…,K} is the unobserved cluster indicator (where K is the number of clusters). The goal is to
learn a model p (x,y) that captures the joint distribution of x and y. Since y is unobserved, we minimize
θ
the negative log-likelihood with y marginalized out:
min−Ex∗∼D⎡log ∑p θ(x∗,y)⎤. (2.8)
θ
y∈Y
⎣ ⎦
Direct optimization of the marginal log-likelihood is typically intractable due to the summation over y. Earlier
work thus developed different solvers with varying levels of approximations.
It can be shown that the intractable negative log-likelihood above can be upper bounded by a more tractable
term known as the variational free energy (Neal & Hinton, 1998). Let q(y∣x) represent an arbitrary auxiliary
distribution acting as a surrogate of the true posterior p(y∣x), which is known as a variational distribution.
Then, for each instance x∗ ∈ D, we have:
p (x∗,y)
−log ∑p θ(x∗,y) =−Eq(y∣x∗) [log qθ
(y∣x
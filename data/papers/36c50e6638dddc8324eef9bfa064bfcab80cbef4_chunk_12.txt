).
behind thedecisions. Hence, these rationales are
not only valuable by themselves but also lead to
better credibility and transparency for evaluating
ComparedtoothersafetydatasetssuchasBuild-
theannotations(Kutluetal.,2020).
it Break-it Fix-it (60K; Dinan et al., 2019), Bot-
When creating our final context label, we aim
AdversarialDialogue(79K;Xuetal.,2021),and
topreserveannotatordisagreements,whichoften
DiaSafety(11K;Sunetal.,2022),ourdatasetof-
arise in such subjective annotations (Dinan et al.,
fers a much larger set of utterances (166K) each
2019;Sapetal.,2022). Ourfinallabelsetis: (1)
annotatedbythreeworkerswithrationalesbehind
CASUAL, (2) POSSIBLY NEEDS CAUTION, (3)
judgmentsinfree-formtext.
PROBABLY NEEDS CAUTION, (4) NEEDS CAU-
Rich in Negativity. PROSOCIALDIALOG in-
TION, and (5) NEEDS INTERVENTION. Further
cludesarichsuiteofconstructivefeedbackcoun-
detailsandannotationpagesareinAppendixA.4.
teringproblematicdialoguecontentcomparedto
otherdialoguedatasets. Toillustratethis,weana-
3.4 Analysisof PROSOCIALDIALOG
lyzethepolarityofutterancesinourandotherex-
Large-scale. The dataset contains 58,137 dia-
istingdatasets,usingtheBERT-basedGoEmotions
logues with 331,362 utterances, 160,295 unique
sentimentclassifier(Demszkyetal.,2020). Wecat-
RoTs,497,043safetyannotationsandreasons(Ta-
egorizetheutterancesineachtrainingdatasetinto
ble 1). The safety labels have good agreement
four classes: positive, ambiguous, negative, and
(Krippendorff’sα=0.49;Krippendorff,2011),with
neutral. InFigure3,weshowthate
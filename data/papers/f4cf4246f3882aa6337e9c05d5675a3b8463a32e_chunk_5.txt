hebenefitsofALFREDrelativeto centricvisualandactionfeedbackorpartialobservability.
othervisualactiondatasetswithlanguageannotations. There is an extensive literature on language-based in-
Vision & Language Navigation. In vision-and-language structionfollowinginthenaturallanguageprocessingcom-
navigation tasks, either natural or templated language de- munity. There, research has focused on mapping instruc-
scribes a route to a goal location through egocentric vi- tions to actions [5, 13, 32, 36, 49], but these works do not
sualobservations[3,12,13,14,31]. Sincetheproposalof involvevisual,interactiveenvironments.
R2R [3], researchers have dramatically improved the nav- Embodied Question Answering. Existing datasets for
igation performance of models [17, 24, 29, 53, 54] with visual question answering in embodied environments use
techniques like progress monitoring [28], as well as in- templatedlanguageorstaticscenes[15,20,55,57].InAL-
troduced task variants with additional, on-route instruc- FRED, rather than answering a question, the agent must
tions[38,39,51]. Muchofthisresearchislimitedtostatic completeataskspecifiedusingnaturallanguage,whichre-
environments. By contrast, ALFRED tasks include navi- quiresbothnavigationandinteractionwithobjects.
gation,objectinteractions,andstatechanges. Instruction Alignment. Language annotations of videos
Vision & Language Task Completion. There are sev- enable discovering visual correspondences between words
2
Pick Stack PickTwo Clean Heat Cool Examine
&Place &Place &Place &Place &Place &Place inLight
item(s) Book Fork(in)Cup SprayBottle DishSponge PotatoSlice Egg CreditCard
receptacle Desk CounterTop ToiletTank Cart CounterTop SideTable DeskLamp
scene# Bedroom14 Kitchen10 Bathroom2 Bathroom1 Kitchen8 Kitchen21 Bedroom24
expert
demonstration
Annotation#1 Annotation#2 Annotation#3
Goals Putacleanspongeonametalrack. Placeacleanspongeonthedryingrack Putarinsedoutsponge
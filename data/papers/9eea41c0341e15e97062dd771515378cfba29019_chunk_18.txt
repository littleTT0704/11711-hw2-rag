.
ularly in the statistical machine translation com- Khalifa et al. (2021) pretrain BART on informal
munity(Neubigetal.(2012),AkitaandKawahara text before training on SAMSum, observing im-
(2007)). Otherworkhasfocusedontheoppositedi- provementwhenpretrainingondialoguecorpora
rection: convertingwrittentextintoastylemorefa- but not when training on Reddit comments. Yu
vorableforaudiolistening(Abu-Jbaraetal.,2011). et al. (2021) study the effectiveness of adding an
Morerecentworkhasfocusedonend-to-endmod- additionalphaseofpretrainingtoimprovedomain
elingwithoutthisintermediatestep(Saleskyetal. adaptation, in which they either train on a news
(2019), Jamshid Lou and Johnson (2020)). This summarization task, continue pretraining (using
task differs from perspective shift in several re- thestandardreconstructionloss)foranin-domain
spects: thefocusofspeaking-styletransformation dataset,orcontinuepretrainingonasmallerdataset
is on removing disfluencies, whereas perspective ofunlabeledinputdialoguesfromthetrainingset.
shiftaimstopreserveinformationthatmaybecon- Zouetal.(2021)pretrainanencoderondialogue
veyed by the informal style of text; perspective andadecoderonsummarytextseparatelybefore
shiftrequirescomplexcoreferenceresolutionand trainingthetwotogetheronasummarizationobjec-
utterance contextualization, while speaking-style tive. Whiletheseapproachesimproveperformance
transformationleavesreferencesunresolved; and ondialoguesummarization,particularlyinalower-
perspectiveshiftisappliedtotextpost-hoc,while resourcesetting,theylargelyrequirepretrainingat
speakingstyletranscriptionmaybeperformedover alargecomputationalcost.
8 Conclusion wetakestepstominimizethelossofemotionand
stanceinformation,thenuancesofthisinformation
Perspectiveshiftisanew,non-trivialstyletransfer
maystillbediscarded.
taskthatrequiresincorporationofcoreferencereso-
Theperspectiveshiftprocessals
kevaluation
explicitlyevaluatedsubmissionsofspan-levelanno-
The success of learned machine translation met-
tations(althoughmostsubmissionsstillconsisted
rics(Sellametal.,2020;Reietal.,2022a;Freitag
of models that predicted word-level tags which
etal.,2022;Qinetal.,2022),whichfinetuneneu-
wereconvertedtospans). Wealsocompareagainst
ralnetworkmodelspretrainedonlargeamountsof
state-of-the-artword-levelqualityestimationmod-
(unsupervised)data,highlightedtheimportanceof
els.
leveragingtransferlearningtoachievemetricswith
bettercorrelationwithhumanjudgments. Morere- 4 UsingLLMstoPredictQualityScores
cently,generativeLLMs(OpenAI,2023;Aniletal.,
2023)haveconsistentlydemonstratedimpressive Recentworkshaveshownthatlargelanguagemod-
resultsinnaturallanguageunderstandingandzero- elsareversatile,general-purposemodelsthatcan
andfew-shottransferand,naturally,interestinem- be used to tackle many problems in NLP, includ-
ploying these models for (translation) evaluation ingevaluation(KocmiandFedermann,2023;Jain
hasincreased. KocmiandFedermann(2023)first etal.,2023;Liuetal.,2023b). Webeginbyexplor-
explored the use of GPT models for evaluating inghowLLMscanbeusedformachinetranslation
machinetranslationtasks,showingtheirpotential evaluationthroughscoreprediction.
aszero-shot evaluators, andothershavesinceex-
4.1 Prompting
tended GPT-based evaluation to other generation
problems(Jainetal.,2023;Liuetal.,2023b). We start by measuring how far we can push the
Perrellaetal.(2022)firsthighlightedthatMQM performance of LLMs with just prompting (Liu
annotationscouldbeleveragedtoallowpretrained etal.,2023a): bydefiningthetaskofMTevaluation
model
iffer on multiple layers of the pre-trained models are
et al., 2020c) specific information to efficiently bothinterestingdirectionstopursue.
shareparametersacrosssettings. ThoughRTNin
Acknowledgements
MetaXL is similar to adapter networks in archi-
tecture, in contrast to adapter networks, it plays
Wethanktheanonymousreviewersfortheircon-
a more explicit role in transforming representa-
structivefeedback,andWeiWangforvaluabledis-
tionsacrosslanguagestobridgetherepresentation
cussions.
gap. Moreimportantly,MetaXLtrainstherepresen-
tation transformation network in a meta-learning EthicalConsiderations
basedparadigm, significantlydifferentfromhow
Thisworkaddressescross-lingualtransferlearning
adaptersaretrained.
ontoextremelylow-resourcelanguages,whichis
MetaLearning MetaXLfallsintothecategory alessstudiedareainNLPcommunity. Weexpect
ofmetalearningforitsgoaltolearntotransform thatprogressandfindingspresentedinthispaper
undertheguidanceofthetargettask. Relatedtech- couldadvocateawarenessofadvancingNLPforex-
niqueshavebeenusedinFinnetal.(2017),which tremelylow-resourcelanguagesandhelpimprove
aimstolearnagoodinitializationthatgeneralizes informationaccessforsuchunder-representedlan-
well to multiple tasks and is further extended to guagecommunities.
low-resourcemachinetranslation(Guetal.,2018) The proposed method is somewhat compute-
andlow-resourcenaturallanguageunderstanding intensiveasitrequiresapproximatingsecond-order
tasks(Douetal.,2019). Thebi-leveloptimization gradientsforupdatingthemeta-parameters. This
procedureiswidelyadoptedspanningacrossneu- mightimposenegativeimpactoncarbonfootprint
ralarchitecturesearch(Liuetal.,2019),instance from training the described models. Future work
re-weighting (Ren et al., 2018; Shu et al., 2019), on developing more efficient meta-learning opti-
learning from pseudo labels (Pham et al., 2020) mizationmethodsandacceleratingmeta-learning
and mitigatingnegative inferencein
i-
Conditional Transformer LM (CTRL; Keskar mentdoesnotprovidesufficientlystrongguidance
etal.,2019) Tocontrolthesentimentofgenera- forpositivesentiment.
tionsfromCTRL,weusethe“Reviews”control
4.2.4 HumanEvaluation
codeandappendaratingof“5.0”forpositivegener-
ationsandaratingof“1.0”fornegativegenerations. Forhumanevaluation,werandomlychoose30neu-
ThesentimenttrainingexamplesforCTRLcame tralprompts,30positiveprompts,and30negative
fromAmazonreviews(McAuleyetal.,2015). prompts,andconsiderfivepairsofmodels: DEX-
PERTS versus GPT-2, CTRL, PPLM, DAPT, and
Aswithtoxicityexperiments(§3),weusenucleus
GeDi. Foreachpromptandpairingofmodels,we
sampling with p “ 0.9, and include our training
sampletwogenerationsfromeachmodelforeach
andgenerationdetailsinAppendixA.
steeringdirectionconsidered. Thisresultsinato-
4.2.3 AutomaticEvaluation talof120promptsˆ5pairings ˆ2generations “ 1200
prompt pairing
pairs, each rated by 3 MTurk workers. We ask
We evaluate our generations for the target senti-
annotatorstoselectwhichgenerationachievesthe
ment,fluency,anddiversity. Toestimatesentiment,
desiredsentimentbetter,alongwiththefluencyand
we use HuggingFace’s sentiment analysis classi-
topicalityquestionsfrom§3.2.4.
fier,andreportthemeanpercentageofgenerations
perprompt(outof25)whicharelabeledpositive
Results AsshowninFigure4,DEXPERTSissub-
(the rest are negative). We evaluate fluency and
stantially more effective at steering toward posi-
diversityinthesamewaysas§3.
tivityonnegativepromptswhileachievingbetter
Results AsshowninTable3,DEXPERTSgreatly
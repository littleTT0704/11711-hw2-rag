anself-talk(by1.89%),
showingthatitisbetteratelicitinghelpfulknowl-
4.2 KnowledgeGenerationMethods edgefrommodels.
Table 3 reports the performance with different Our knowledge is comparable with retrieval-
knowledge generation baselines. Generally, ran- based knowledge. On NumerSense, the re-
dom sentences barely help and even hurt the in-
trieved knowledge only improves inference per-
ference model, whereas context sentences of the
formance by 0.18% on test-core and 1.02% on
question provide some gain. In contrast, knowl-
test-all, while our method further outperforms it
edge generated by our method consistently leads
by 8.83% and 7.37%, respectively. This shows
to substantial performance improvements, which
that knowledge retrieved from a loosely-related
impliesthatourknowledgeisofhighquality.
knowledge base can be far less useful than our
Knowledgeisanessentialfactor. Thefew-shot generated knowledge. On CSQA2, although we
GPT-3modelispoorlycalibratedtodirectlyanswer arenotabletobeattheweb-retrievedknowledge,
3158
.neGegdelwonK
Figure 2: Performance with different number of gen-
erated knowledge statements per question (QASC dev
set,T5-11binferencemodel).
Figure 3: Improvement on top of different sizes of in-
Integrationmethod QASC-dev ferencemodel(Numersensedevset).
ours 58.32
Mixture-of-Experts 56.26
Product-of-Experts 55.94
Table4:Performancewithdifferentknowledgeintegra-
tionmethods(QASCdevset,T5-11binferencemodel).
ourmethodstillbridgestheperformancegapwith-
out referring to Google search. For QASC, the
Figure4:Improvementbydifferentsizesofknowledge
“retrieved”knowledgeisactuallygoldknowledge
generation model (Numersense dev set, T5-11b infer-
fromaknowledgebasethatwasusedtoconstruct encemodel).
thedataset. Asaresult,ourgeneratedknowledge
fallssignificantlyshortoftheretrievedknowledge.
TheresultsinTable4indicatethatourknowledge
Insummary,ourgeneratedknowledgeisroughly
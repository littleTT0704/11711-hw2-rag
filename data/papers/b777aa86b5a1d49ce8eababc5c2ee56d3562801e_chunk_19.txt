
els are known to exhibit better task performance
presentadditionalsettingsforfutureinvestigation.
duetoincreasedparametercountandexpressivity
Additionally, our study evaluates efficiency via
(ZagoruykoandKomodakis,2016).
model latency and our claims do not necessarily
Model designers can leverage wider architec-
translate to other metrics of efficiency, such as
tures with their target inference framework and
powerconsumptionorpoweroutput.
hardwaresettinginmindtoachievehigherutiliza-
Asmodelscontinuetoscaleinsize,theyoften
tion. For example, models processing few exam-
requiremodelordataparallelismtechniquesthat
plesduringinferencecanleveragewiderlayersto
require computation across several nodes which
avoidframeworkbottlenecks. SeeTable1.
introduceoverheadfrommulti-devicesynchroniza-
tionandnetworkcommunication. Additionally,we
Using higher-performing hardware does not
donotstudylatencyinthetrainingsettingwhere
necessarily improve end-to-end performance.
theper-layercomputationislargerduetothecom-
Frameworkoverheadlimitstheimpactofimproved
putationofgradientsandlosses.
hardwareasitlimitsutilization. Thistrendwillcon-
tinueasML-specifichardwareadvanceswithoutef-
Acknowledgements
fortstoaddresssoftwarebottlenecks. Forexample,
single-exampleinferencewithbothBERTisslower This work was supported in part by a grant
using an A100 than using a V100 GPU despite a from the National Science Foundation Graduate
2.75xincreaseinpeakcomputationalthroughput. Research Fellowship Program under Grant No.
DGE2140739. Wethanktheanonymousreviewers
6 Conclusion
for their valuable feedback. We would also like
tothanklabmembersandfacultyforhelpfulfeed-
Weconductanextensivestudyofneuralnetworks
back during discussions and revisions, including:
from the convolutional and transformer architec-
DanielFried,HanGuo,JeremiahMilbauer,Sanket
ture paradigms across a variety of software and
VaibhavMehta,andSaujas
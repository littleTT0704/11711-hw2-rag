promptencoder,actsonthecurrentstate.
the<ACT>tokenandcalculatedot-productsimilar-
itywiththeN inputtokense. Thetokenwiththemaximumdot-productischosenasthepredicted
i (cid:0) (cid:1)
instance: x = max xˆ ·e. Thetrainingtargetisextractedfromthedemon-
pred ei,i∈{1,N} <ACT> i
stration dataset D, and the policy trained with cross-entropy to maximize the similarity of output
latentwiththeexpert’schoseninstanceembedding.
2.2 Multi-PreferenceTaskLearning
We adopt a prompt-situation architecture for multi-preference learning. This design (1) enables
multi-preferencetrainingbydisambiguatingpreferences,(2)learnstask-levelrulessharedbetween
preferences (e.g. dishwasher should be open before placing objects), (3) can generalize to unseen
preferencesattesttime,withoutfine-tuning. Givena‘prompt’demonstrationofpreferencem,our
policy semantically imitates it in a different ‘situation’ (i.e. a different initialization of the scene).
Tothisend,welearnarepresentationofthepromptγmwhichconditionsπtoimitatetheexpert.
ψ(τm )→γm (1)
prompt
π(S |γm)→am ={xm,gm } (2)
situation t pred pred
Wetrainneuralnetworksψandπtogethertominimizethetotalpredictionlossoverallpreferences
usingthemulti-preferencetrainingdatasetD. Theoverallobjectivebecomes:
min L (a,π(S,ψ(τ))) (3)
CE
m∼M,τ∼Dm,(S,a)∼Dm
Foreverypreferenceminthedataset,wesampleademonstrationfromD anduseitasaprompt
m
forallstate-actionpairs(S,a)inD. Thisincludesthestate-actionpairsfromτ andcreates
m prompt

copeofthispaper,wereferQATasuniform
quantizationviaquantization-awaretraining.
B ExperimentSetup
We initialized the model with uncased BERT
base
model (Wolf et al., 2019) from the HuggingFace
librarywithpre-trainedweights,andfine-tunedon
eachGLUEbenchmarktask. Wefollowthesame
setupas(Chenetal.,2020)andreportvalidation
setaccuracyforQQP,QNLI,MRPC,RTE,SST-2,
matched accuracy for MNLI, Matthew’s correla-
Algorithm2:SharpnessMeasurement
tionforCoLA,andPearsoncorrelationforSTS-B,
the alternative metrics are included below in Ta- 1 bufferinitialweightasw 0
ble2. Each task was trained on its train set for 5 2 whilenotconvergingdo
epochstoobtainthestartingcheckpointfortheall if(cid:107)W t+1−W 0(cid:107) 2 ≤ ρthen
W = W +η·∇ L
of the quantization model. To train the quantiza- t+1 t W
else
tion model, we use Adam optimizer with initial
W = W +η·∇ L
learningratesetat1e-5andusecosineannealing t+1 t W
W = ρ· Wt+1−W0 +W
LRscheduletoadjustthelearningrateduringthe t+1 (cid:107)Wt+1−W0(cid:107)2 0
trainingprocess. ToperformtheSQuATandLSQ t = t+1
fine-tuning,weruneachmodelfor32epochsfor 3 ReturnL(W T+1)−L(W 0);
eachtasks. Thehyperparameterρweusedfortrain-
ingSQuATis0.1for2-bitsand3-bitsmodels,and
0.15for4-bitsmodels,whicharedeterminedbya
gridsearchfrom0.0to0.25at0.05incrementon
MNLItask.
C Measuringsharpness
FollowingMehtaetal.(2021),Algorithm2shows
how
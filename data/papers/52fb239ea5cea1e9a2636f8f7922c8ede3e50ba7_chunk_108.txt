28 MCTaco_event_ordering_structured 0.860 0.831 0.831 0.890
29 MCTaco_event_typical_time_structured 0.870 0.881 0.881 0.870
30 MCTaco_frequency_structured 0.890 0.862 0.862 0.790
31 MCTaco_stationarity_structured 0.710 0.758 0.758 0.670
32 multiarith 0.360 0.143 0.921 0.990
33 Numersense_structured 0.620 0.495 0.495 0.660
34 NumGLUE_Type_1 0.535 0.108 0.083 0.740
35 NumGLUE_Type_2 0.512 0.285 0.646 0.735
36 NumGLUE_Type_3 0.835 0.004 0.001 0.815
37 NumGLUE_Type_4 0.710 0.076 0.208 0.790
38 NumGLUE_Type_5 0.460 0.200 0.305 0.615
39 NumGLUE_Type_7 0.500 0.516 0.854 0.710
40 NumGLUE_Type_8 0.420 0.082 0.257 0.610
41 simuleq 0.120 0.074 0.010 0.170
42 singleop 0.940 0.347 0.611 1.000
43 singleq 0.830 0.143 0.474 0.670
44 svamp_structured 0.620 0.085 0.060 0.790
Average F1 score 0.400 0.223 0.440 0.613
Table 18: Evaluation results of baselines across different single datasets. On
most datasets, Codex performs best. Model names: GPT-3: the few-shot
175B GPT-3 model; GPT-Neo-A: the fine-tuned 2.7B GPT-3 model where the
predictionoutputisananswer; GPT-Neo-P:thefine-tuned2.7BGPT-3model
33
where the prediction output is a program; Codex: the few-shot Codex model
where the prediction output is a program.
ID Dataset References

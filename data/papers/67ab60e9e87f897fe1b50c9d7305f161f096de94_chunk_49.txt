 multilabel tasks and multilabel frame reductions of
timestamp tasks, the final layer is a sigmoid with cross-entropy loss.
We monitor the score (not loss) on the validation set. For timestamp tasks, computing
thevalidation scoreinvolves afullCPU-basedsed eval(Mesaros et al.,2016)runwithme-
dianfilter of 250ms andminimumevent duration125msand250ms. (Both event durations
are tried at each validation step and the best hyperparameter is retained for that validation
step.) We train for a maximum of 500 epochs, checking the validation score every 3 epochs,
early stopping if no improvement is seen after 20 validation steps. For DCASE 2015 Task
2, we check the validation score every 10 epochs.
The validation score is used for early-stopping, as well as for model selection. The same
RNG seed is used for every model-task downstream training, ensuring that grid points and
weight initialization is identical. Model selection is performed over 8 deterministic random
grid points out of 16 possible grid points. Hyperparameters are shown in Table 4. This grid
was chosen after using a much larger hyperparameter grid with the three baseline models
on the open tasks. In these preliminary hyperparameter grid pruning experiments, the grid
was progressively refined by discarding hyperparemeter choices that were not predictive of
relatively high model performance, similarly to how Kelz et al. (2016) use tree ensemble
learning to prune their hyperparameter grid.
Table 4: Hyperparameters used for training.
Hidden layers [1, 2]
Hidden dimensions 1024
Dropout 0.1
Learning rate [3.2e-3, 1e-3, 3.2e-4, 1e-4]
Batch size 1024
Hidden norm Batch Norm
Initialization [Xavier Uniform, Xavier Normal] (Glorot and Bengio, 2010)
Optimizer Adam
29

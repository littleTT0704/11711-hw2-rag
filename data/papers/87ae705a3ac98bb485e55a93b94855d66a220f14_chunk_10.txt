
hamed, and Hinton 2013) are layered architectures which • Properties define relationships between variables of a
effectivelyusepastandfutureinformationviaForwardand giventype.Forexample,the“byArtist”propertyofaMu-
BackwardLSTMlayers.Bi-LSTMshavebeensuccessfully sicRecording,defineswhowroteasong.
appliedtofeaturegenerationfortaskslikedependencypars-
• Operators Operators can be used to represent complex
ing(KiperwasserandGoldberg2016)andsemanticrolela-
logicalorspatialrelationships.
beling (Zhou and Xu 2015). All our models adopt a deep
neural-networkarchitecturewithBi-LSTMsasourprimary TheaimofAMRListoprovideacommonsemanticrep-
buildingblocks.Fortextclassification,therehasbeenalotof resentationforspokenlanguage.Itcanrepresentanaphora,
recentinterestinusingcharacter-levelembeddings(Kimetal. conditionalstatements,sequentialactions,andlogicalexpres-
2015)asanadditionalinputtoneuralarchitecturesbecause sions.AMRLcanhavearbitrarynesting,enablingittorepre-
oftheirabilitytomodelmorphologicalfeaturesaswellasef- sentcomplexstatements.Figure1showsasimpleexample
fectivelyhandleout-of-vocabularywords.(Ballesteros,Dyer, ofAMRL.Figure3showshowsequentialandcross-domain
andSmith2015)usecharacterembeddingfordependency queriescanberepresentedinAMRL.
parsing,(XiaoandCho2016)combinecharacterembeddings In this paper, we investigate linearization as a means to
withCNNsforthetextclassificationtask. addressthedatasparsityofannotatedAMRLexamples.Lin-
Semantic parsing and spoken language understanding earizationisacommonmethodtosimplifysyntactic(Vinyals
(SLU)learntomapnaturallanguagetoaformalrepresen- etal.2015),CCG(e.g.,viasupertagging) (Lewis,Lee,and
tation.Althoughsemanticpars
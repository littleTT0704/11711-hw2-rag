linkingishardertolearnthanmention
Giventheevidencethatalargebenefitofcontinued identification,sogivenafixednumberofin-domain
training for domainadaptation is concentrated in examplesforcontinuedtraining,theperformance
thementiondetectorcomponentofthecoreference improvement from mention detection would sur-
system (§2.3), and that mention annotations are pass that of the antecedent linker. In this case, it
muchfasterthancoreferenceannotations(§3),in would be more helpful to the flailing antecedent
thissection,weintroducemethodologyfortraining linkerifthementiondetectorwereprecise.
a neuralcoreference modelwith mentionannota- Based on this hypothesis, we propose high-
tions. Ourapproachincludestwocorecomponents precision c2f pruning to enable adaptation using
focusedonmentiondetection: modificationtomen- mention annotations alone. We impose a thresh-
tionpruning(§4.2)andauxiliarymentiondetection oldq onthementionscores m(i)sothatonlythe
training (§4.3). We also incorporate an auxiliary highestscoringmentionsarepreserved.
maskingobjective(§4.4)targetingtheencoder.
4.3 AuxiliaryMentionDetectionTask
4.1 Baseline
We further introduce an additional cross-entropy
In our baseline model architecture (Lee et al.,
loss to train only the parameters of the mention
2018),modelcomponentsaretrainedusingacoref-
detector,wherex denotesthespanrepresentation
i
erence loss, where Y(i) is the cluster containing
forthei’thspanproducedbytheencoder:
span i predicted by the system, and GOLD(i) is
theGOLDclustercontainingspani:
N
MD = g(x i)log(s m(x i))
N −
i=1
CL = log P(yˆ) X
+(1 g(x ))log(1 s (x ))
i m i
Yi=1yˆ ∈Y(i) X∩GOLD(i) − −
Of the set of N candidate spans, for each span i Thelossisintendedtom
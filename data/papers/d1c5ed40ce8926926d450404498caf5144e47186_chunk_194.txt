 of
about 81%. In comparison, the maximal marginal relevance algorithm ranks nuggets
with 75% MAP, and the rankings generated by the Yahoo! search engine have 43%
MAP. These results confirm that a statistical relevance model is generally more ef-
fective than a single-strategy ranking method. The amount of labeled training data
required to fit statistical models can be reduced drastically with an active learning
approach. We found that 1,000 instances selected based on a combination of diversity
and uncertainty sampling criteria are sufficient to train a model with high ranking
performance. In evaluations on question answering datasets, the effectiveness of this
model comes close to a supervised model fitted to more than a hundred times more
training data. We have also seen that relevant text nuggets are often surrounded by
more relevant text, and that the predictive performance of a statistical model can
be improved by leveraging the context of nuggets in their source documents. When
ranking paragraph-length nuggets, most of this potential can be realized by extending
a linear model with features of adjacent instances. However, when scoring individual
sentences to allow for a more fine-grained selection of relevant content, a sequen-
tial model that predicts transitions between relevant and irrelevant text using lexical
coherence features further improves performance.
The statistical source expansion approach was applied to the question answering
(QA) task, and its impact on the performance of Watson and OpenEphyra was eval-
uated on large datasets of Jeopardy! and TREC questions. We used encyclopedias
and a dictionary as seed corpora and developed strategies for selecting the most use-
ful seed documents based on popularity estimates. The chosen seeds were expanded
with related content from web pages retrieved with a search engine. This expansion
method consistently improved QA performance on every (reasonably large) dataset
we experimented with, independently of the QA system, seed corpus and retrieval
strategy used. When we evaluated the impact on QA search results, text passages
anddocumenttitleswereretrievedfromtheoriginalsourcesandtheexpandedcontent
using the open-source IR systems Indri and Lucene. The SE approach significantly
improved search performance on both Jeopardy! and TREC datasets, yielding gains
of 4.2â€“8.6% in search recall. We also found that source expansion hurts relatively
few questions and tends to be more robust than query expansion techniques. In
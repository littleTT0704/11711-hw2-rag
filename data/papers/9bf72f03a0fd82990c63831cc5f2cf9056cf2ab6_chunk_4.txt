)wasthesimplestandbest
domain(Chenetal.,2021;Mehtaetal.,2021). performingapproach,whichproposeslearningthe
In this work, we incorporate the intuition of optimal quantizer step size (bin width). In their
SAM and combine it with step-size quantization. work,atrainablescaleparametersisproposedfor
However,thecombinationofthetwostrategiesis bothweightsandactivations. Thisschemedefines
nontrivial: naivelymergingthesharpnesstermand theelementwisequantizationfunctionas:
step-size optimization and jointly optimizing the
Q(w) = (cid:98)clip(w/s,Q,Q )(cid:101)·s (1)
loss function results in unstable performance, in N P
some cases underperforming LSQ. The reason is
where(cid:98)·(cid:101)indicatesroundingandtheclip(·)func-
explainedin§3. Toovercomesuchinstability,we tionclampsallvaluesbetweenQ = −2n−1 and
N
introduce an alternate training schedule SQuAT, Q = −2n−1−1inanbitsetting. Theparameter
P
whichoptimizesforthestepsizeinonepassand w istheweightvaluetobequantizedandsisthe
learnedscalingscalar(onepermodule). Following
switches gear to optimize according to the SAM
therulesofSTE,thisquantizernaturallyresultsin
objective in the next pass, and keeps alternating astep-sizegradientof:
untiltheconvergenceofthemodel.

Inourexperiments,wefine-tuneapre-trainedun- ∂Q(w) −w/s+(cid:98)w/s(cid:101) if−Q N <w<Q P
casedBERTbaselinemodeloneachGLUEbench- ∂s = −Q N ifw≤−Q N (2)
Q ifw≥Q
mark (Wang et al., 2018) task using our SQuAT P P
quantization. The result shows that
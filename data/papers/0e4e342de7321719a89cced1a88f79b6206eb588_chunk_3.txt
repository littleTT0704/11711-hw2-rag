 [Hu et al., 2017; unnecessary memorization makes the problem difficult and
Zhang et al., 2018] of visual grounding mainly utilize a violates the nature of human behaviors which typically lo-
two-stage propose-and-rank scheme, and then the focus has catestheobjectwhileunderstandingtheimage.
shifted to one-stage methods [Liao et al., 2020; Yang et al., To address this problem, we follow the intuition of hu-
2020] that directly predict the bounding box coordinates. man behavior that knowing what to look for before looking
Withthesuccessoftransformer-basedmodels[Vaswanietal., at the image will make object localization problems easier.
2017] in recent years, a novel transformer-based one-stage For a neural network, this is equivalent to asking the visual
paradigmforvisualgroundingemerges. encodertounderstandthetextqueryandextractrelevantvi-
sualfeaturesguidedbythetextquery. Dynamicconvolution
‚àóEqualContribution [Landietal., 2019;Chenetal., 2020], whichtypicallycon-
2202
nuJ
22
]VC.sc[
2v41190.6022:viXra
trols feature generation with internal or external signals, de
Visual Encoder Fusion Module
factofitstheneedfortext-awarevisualfeatureextraction. In- ùëü!" ùëì!" Pre Hd eic at dion
spired by a previous dynamic convolution method [Chen et Backbone w/ QCM Transformer
al.,2020],weproposeaquery-conditionedconvolutionmod- image Encoders (x,y,w,h)
ule (QCM) that dynamically constructs convolutional ker- [CLS]
Textual
nels according to input queries. In this way, the output vi- qt ue ex rt y Encoder
sual features are more discriminative for each query-image
pairandleadtomoreaccuratepredictions. Toenablequery- (a) VGQC pipeline with fusion
aware feature extraction, we replace the vanilla convolution
inthevisualencoderwithQCMinamulti-scalefashion. We
Visual Encoder
termourmethodasVGQC,aVisualGroundingpipelinewith ÔøΩ
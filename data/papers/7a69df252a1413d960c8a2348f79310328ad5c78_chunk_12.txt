attention.Instead,itdrawsrepetitivefeaturesmanifestingintheformofmultiple
Title Suppressed Due to Excessive Length 9
Fig.3. IS and FID of the AttnGAN [42], our model utilising squeeze-and-excitation
attention, and our model utilising squeeze-and-excitation attention and local self-
attentionontheCUBandtheCOCOdataset.TheISoftheAttnGANisthereported
scoreandtheFIDwasre-evaluatedusingtheofficialmodel.TheISoftheAttnGANon
theCOCOdatasetiswith25.89Â±.47significantlylowerthanourmodels.Weomitted
the score to highlight the distinctions between our two models.
birds, drawn out birds, multiple heads, or strange patterns. The drawn features
mostlymatchthetextualdescriptions.Thisprovidesapossibleexplanationwhy
the model has a high IS despite scoring poorly on the FID: the IS cares mainly
about the images being highly classifiable and diverse. Thereby, it presumes
that highly classifiable images are of high quality. Our network demonstrates
that high classify-ability and diversity and therefore a high IS can be achieved
throughcompletelyunrealistic,repetitivefeaturesofthecorrectbirdclass.This
is further evidence that improvements solely based on the IS have to be viewed
sceptically.
On the more challenging COCO dataset, our model utilising SE attention
demonstrates semantic understanding by drawing features that resemble the
object, for example, the brown-white pattern of a giraffe (1st row), umbrellas
(4th row), and traffic lights (5th row). Furthermore, our model draws distinct
shapes for the bathroom (2nd row), broccoli (3rd row), and is the only one
that properly approximates a tower building with a clock (7th row). Generally
speaking, the results on the COCO dataset are not as realistic and robust as
on the CUB dataset. We attribute this to the more complex scenes coupled
with more abstract descriptions that focus rather on the category of objects
than detailed descriptions. In addition, although there are a large number of
categories, each category only has comparatively few examples thereby further
increasing the difficulty for text-to-image-generation.
ForourSEattentionmodelwef
thlayers. pacity, we conduct experiments on joint training
As observed from Table 5, transformations at using the representation transformation network.
the 12th layer are consistently effective, suggest- Specifically, the forward pass remains the same
ing that transformation at a higher and more ab- as MetaXL, whereas the backward optimization
stractlevelresultsinbettertransferforbothtasks.6
employs the standard stochastic gradient descent
Transferringfromlowerlayersleadstofewergains algorithm. Weconductexperimentsonplacingthe
for SA, coinciding with the fact that SA is more representationtransformationnetworkafterthe0th
reliant on global semantic information. Transfer- layeror12thlayerandpresentresultsinTable67.
ring at multiple layers does not necessarily lead Interestingly,jointtrainingwiththerepresenta-
tohigherperformance,possiblybecauseitresults tiontransformationnetworkdeterioratesthemodel
inincreasedinstabilityinthebi-leveloptimization performance compared to vanilla joint training.
procedure. Transferringafterthe0thlayerisevenmoredetri-
mentalthanthe12thlayer. Thisfindingshowsthat
4.4 JointTrainingwithRepresentation
Transformer models are rather delicate to subtle
TransformationNetworks
architecturalchanges. Incontrast,MetaXLbreaks
TherearetwomajordifferencesbetweenMetaXL therestriction,pushingtheperformancehigherfor
and joint training: (1) source language data un- bothlayersettings.
6PleaserefertoAppendixB.2forfullresults. 7PleaserefertoAppendixB.3forfullresults.
Joint Training MetaXL where cosine distance is used as as the inner dis-
en en tance,i.e.,
qu qu
d(s,t) (cid:44) 1âˆ’cos(s,t) (6)
ForSA,weobserveadrasticdropofHausdorff
distance from 0.57 to 0.20 and a substantial per-
formanceimprovementofaround4F1score. For
NER, we observe a minor decline of Hausdorff
Figure3: PCAvisualizationoftoken-levelrepresenta- distance from 0.60 to 0.53 as the representations
tions of Quechua and English from the joint training
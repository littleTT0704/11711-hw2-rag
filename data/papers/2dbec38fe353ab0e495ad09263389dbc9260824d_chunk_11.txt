blems,andprevious the PaLM (Chowdhery et al., 2022) pre-trained
Paper Backbone Size Corpus Pre-trainingtask
GPT-f(PoluandSutskever,2020) Transformer(2017) 774M Math Causallanguagemodeling
LISA(Jiangetal.,2021) Transformer(2017) 163M Math Causallanguagemodeling
MATH-PLM(Hendrycksetal.,2021b) GPT-2(2020) 1.5B Math Causallanguagemodeling
MWP-BERT(Liangetal.,2022b) RoBERTa(2019b) 123M Math 8numeracyaugmentedtasks
TaPEx(Liuetal.,2022b) BART(2020) 406M SQL Queryresultgeneration
HTPS(Lampleetal.,2022) Transformer(2017) 600M Math MaskedSeq2Seqmodeling
Thor(Jiangetal.,2022b) Transformer(2017) 700M Github,arXiv Causallanguagemodeling
PACT(Hanetal.,2022) Transformer(2017) 837M Math Masked/Causallanguagemodeling
Minerva(Lewkowyczetal.,2022) PaLM(2022) 540B Science&Math Causallanguagemodeling
GenBERT(Gevaetal.,2020) BERT(2019) 110M Number,Text Masked/Causallanguagemodeling
NF-NSM(Fengetal.,2021) RoBERTa(2019b) 110M Number Numberprediction
LIME(Wuetal.,2021d) Transformer(2017) 11B Math Causallanguagemodeling
Set(Wuetal.,2022c) T5(2020) 60M Math Uniquetokengeneration
Table2: Comparisonofpre-traininglanguagemodelsformathematicalreasoning.
languagemodel,hasasizeupto540Bparameters. Paper Backbone Task
Pre-training corpus. There are generally two EPT(2020) ALBERT(2019) MWP
Generate&Rank(2021) BART(2020) M
 to finetuned inference models without
jointfinetuningwithknowledge.
A.2 PromptsforKnowledgeGeneration
Table 7 through 10 shows the full prompts for
knowledge generation that we use for each eval-
uated task: NumerSense, CSQA, CSQA2, and
QASC.
A.3 HumanEvaluationGuidelines
Table11and12showsthedetailedguidelineswe
useforhumanevaluationofgeneratedknowledge.
B Checklist
B.1 LimitationsandRisks
Limitations. Ourmethodistestedonarepresen-
tative selection of commonsense reasoning tasks
anddatasets. Applyingthismethodtoothertasks
mayrequirepeoplewithmoderateexpertisetocraft
atask-specificprompttofeedintothemethod.
Risks. It is possible that our proposed method
maylowertheperformanceofcommonsenserea-
soning systems, if not implemented properly or
using badly-designed prompts. Such risk can be
mitigated by following the prompt design guide-
linesinthispaper(§2.1).
Method KnowledgeGenerator InferenceModel
CAGE(Rajanietal.,2019) task-finetuned joint-finetuned
LatcinnikandBerant(2020) task-finetuned joint-finetuned
DynaGen(Bosselutetal.,2021) task-finetuned joint-finetuned
Self-talk(Shwartzetal.,2020) template-prompted 0-shot
Contrastiveexpl.(Paranjapeetal.,2021) template-prompted 0-shot&joint-finetuned
Generatedknowledgeprompting(ours) demonstrations-prompted 0-shot&task-finetuned
Table 6: Comparison of methods that add generated text to an inference model. Knowledge Generator: task-
finetuned–amodelfinetunedtogeneratetask-specificknowledge;template-prompted–anoff-the-shelfLMfrom
whichknowledgestatementsareelicitedviatemplates;demonstration-prompted–anoff-the-shelfLMfromwhich
knowledgestatementsareelicitedviafew-shotdemonstrations(§2.1). InferenceModel: 0
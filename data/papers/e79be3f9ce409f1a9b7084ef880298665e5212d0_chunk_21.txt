.0 Figure2: Zero-shotperformanceonYouCook2andMSR-
MMT[14] BERT CollaborativeExperts 24.6 54.0 67.1 4.0 VTTfordifferentsettings.score-1-5correspondtothefive
TACo(Ours) BERT R-152+S3D-HM 26.7 54.5 68.2 4.0
settingsinTable3fromtoptobottom.
Table6: Comparingtext-videoretrievalonMSR-VTT.The
upperblockandbottomblockusesplit2andsplit1,respec- with“det+adp”,“noun”and“verb”andreportthenumbers
tively. Wereportthemseparatelyforfaircomparison. on two text-video retrieval datasets. As we can see, using
“det+adp” as the target tokens is worse than the baseline
withoutanytoken-levelcontrastiveloss.“noun”and“verb”
ActivityNet
Model Lang. Video canbothimprovetheperformancewhile“noun”isslightly
R1↑ R5↑ R10↑ MR↓
better than “verb”. Finally, combining noun and verb to-
Random - - 0.02 0.1 1.02 2458
gether achieves the best performance. These results align
DenseCap[25] LSTM C3D 14.0 32.0 65.0 34
FSE[56] GRU C3D+TSN-Inception 18.2 44.8 89.1 7.0 withourintuitiontousenounsandverbsasthetargettoken
CE[31] GPT CollaborativeExperts 18.2 47.7 91.4 6.0 forfine-grainedalignmentbetweentextsandvideosconsid-
MMT[14] BERT CollaborativeExperts 22.7 54.2 93.2 5.0 eringtheyareusuallygroundedtovideocontents.
TACo(Ours) BERT R-152+S3D-HM 25.8 56.3 93.8 4.0
Table7: Comparingtext-videoretrievalonActivityNet.
5.1.2 Comparing
tal.,2019]andwhilesome
concernshavebeenraisedregardingtheirfaithfulness[JainandWallace,2019],overallattention-
basedexplainershavebeenfoundtoleadtorelativelygoodexplanationsintermsofplausibilityand
simulability[TrevisoandMartins,2020,Kobayashietal.,2020,Pruthietal.,2020].
However,toextractexplanationsfrommulti-headattention,wehavetwoimportantdesignchoices:
1. Singledistributionselection: Sinceself-attentionproducesanattentionmatrixAh ∈(cid:52)L,we
L−1
needtopooltheseattentiondistributionstoproduceasinglesaliencymape∈(cid:52). Typically,
L−1
thedistributionfromasingletoken(suchas[CLS])ortheaverageoftheattentiondistributions
fromalltokens1≤i≤Lareused.
2. Headselection: Wealsoneedtopoolthedistributionsproducedbyeachhead. Typicalad-hoc
strategiesincludeusingthemeanoverallheadsforacertainlayer[Fomichevaetal.,2021b]or
selectingasingleheadbasedonplausibilityonvalidationset[Trevisoetal.,2021]. However,
sincetransformerscanhavehundredsoreventhousandsofheads,thesechoicesrelyonhuman
intuitionorrequirelargeamountsofplausibilitylabels.
In this work, we approach the latter design choice in a more principled manner. Concretely, we
associateeachheadwithaweightandthenperformaweightedsumoverallheads. Theseweights
arelearnedsuchthattheresultingexplanationmaximizessimulability,asdescribedin§3. More
formally,givenamodelT anditsqueryandkeyprojectionsforaninputxforeachlayerandhead
θT
h≤H,wedefineaparameterized,differentiableattentionexplainerE (T,x)as
φT θT
L (cid:32) H (cid:33)
1 (cid:88) (cid:88)
sh =
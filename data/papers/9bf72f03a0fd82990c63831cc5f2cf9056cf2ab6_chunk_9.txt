 Thus the joint update of step-size can-
notapproximatetheeffectofEq. 3similartothe
weightupdate. Thismakestheupdateofstep-size
asymmetricw.r.ttheupdateofmodelweightwand
intuitively, may not be able to catch up with the
weight updates in the sharpness-aware direction.
Tofixthis,wemaketheweightupdatesto“wait”
forthestep-sizetoadapttothepropermagnitude.
Figure3: JointTrainingVS.AlternateTrainingperfor-
Specifically,weinheritthesharpness-awareupdate
mancedifferencerelativetoLSQ,whichisthe0line
of w, while during the step-size updating phase,
4 Experiment&Discussion
wefixthemodelweightsw,andonlyupdates,as
summarizedinAlgorithm1. Becausetheweights
We apply SQuAT to quantize the pre-trained un-
arealreadysharpness-awareterms,wealsosimpli-
casedBERTbaselinemodelandevaluatetheperfor-
fied updates of the highly shared step-sizes term
manceofourproposedquantizationontheGLUE
toregularSGD.Comparatively,ouralgorithmex-
benchmark(Wangetal.,2018),whichconsistsofa
hibitsbetterstability,whichiscorroboratedbyour
collectionofNLPtasks. Forallthesetasks,werun
strongempiricalresultsinTable1. Empirically,we
the experiments with three random seeds and re-
alsoobservejointupdateresultinginlowerperfor-
portthemeanandstandarddeviationoftheresult1
mancecomparedtoalternatetrainingasshownin
As shown in Table1, our SQuAT outperforms all
Figure3,withjointtrainingunderperformingLSQ
existing quantization methods in all GLUE tasks
baseline in majority of the GLUE (Wang et al.,
under the 2,3 and 4 bits scheme. SQuAT signifi-
2018)tasks.
cantlyoutperformsGOBO(Zadehetal.,2020)by
atleast5%in2
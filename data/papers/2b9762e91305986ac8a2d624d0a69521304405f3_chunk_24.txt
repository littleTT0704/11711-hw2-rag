 of
operandiforsuchworkinordertoinvolvemarginal- thispaper,andtoDanGillickforfeedbackonthe
izedcommunitiesintheresearchprocess. Mewsli-X dataset design. We thank Hila Gonen,
BidishaSamantha,andParthaTalukdarforadvice
7.2 Leaderboardchasing onArabic,Bengali,andHebrew CHECKLIST ex-
New benchmarks incentivize researchers to hill- amples.
climbonaggregatemetrics(EthayarajhandJuraf-
sky, 2020). In addition, new benchmarks create
References
new opportunities for models to reach “superhu-
man”performance,whichmayleadpeopleoutside Antonios Anastasopoulos and Graham Neubig. 2020.
thefieldtoerroneouslyconcludethatsomemodel Shouldallcross-lingualembeddingsspeakenglish?
InProceedingsofACL2020,pages8658–8679.
has “solved language”. We hope that our inclu-
sionofEXPLAINABOARDandMULTICHECKLIST
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
help to prevent such a fallacy, by enabling more 2020a. On the Cross-lingual Transferability of
fine-grainedevaluationthatgoesbeyondasingle Monolingual Representations. In Proceedings of
ACL2020.
aggregatemetric.
Mikel Artetxe, Sebastian Ruder, Dani Yogatama,
7.3 Biasesinmultilingualmodels
Gorka Labaka, and Eneko Agirre. 2020b. A call
Multilingual models have been shown to reflect for more rigor in unsupervised cross-lingual learn-
ing. In Proceedings of the 58th Annual Meeting
biases similar to their monolingual counterparts
of the Association for Computational Linguistics,
(Zhaoetal.,2020). Inaddition,multilingualmod-
pages 7375–7388, Online. Association for Compu-
els are biased towards languages with more pre- tationalLinguistics.
Mikel Artetxe and Holger Schwenk. 2019. Massively KawinEthayarajhandDanJurafsky.2020. Utilityisin
 will open source our code and model check- main conference and the shared task, and Volume
pointsforthecommunitytouseandbuildupon. 2: ProceedingsoftheSixthInternationalWorkshop
onSemanticEvaluation(SemEval2012),pages385–
Secondly, VMSST and BITRANSLATION re-
393, Montréal, Canada. Association for Computa-
quiredecodingwhichwhichmeanstheyneedmore tionalLinguistics.
memoryforthedecoderandareslowerduringtrain-
ing. However one advantage of these models is EnekoAgirre,DanielCer,MonaDiab,AitorGonzalez-
Agirre,andWeiweiGuo.2013. *SEM2013shared
thattheycanbetrainedwithgradientcheckpoint-
task: Semantic textual similarity. In Second Joint
ing greatly reducing their memory requirements, ConferenceonLexicalandComputationalSemantics
which cannot be used for the contrastive models (*SEM),Volume1: ProceedingsoftheMainConfer-
as that would reduce the effective batch size for enceandtheSharedTask: SemanticTextualSimilar-
ity,pages32–43,Atlanta,Georgia,USA.Association
finding negative examples. Moreover, during in-
forComputationalLinguistics.
ference, there is no difference in the memory or
speedrequirementsinCONTRASTIVE,BITRANS- Mikel Artetxe and Holger Schwenk. 2019a. Margin-
LATION, or VMSST as only a single encoder is basedparallelcorpusminingwithmultilingualsen-
tence embeddings. In Proceedings of the 57th An-
usedininferenceandthereisnodecoding.
nualMeetingoftheAssociationforComputational
Linguistics,pages3197–3203,Florence,Italy.Asso-
Acknowledgements
ciationforComputationalLinguistics.
We are grateful to Livio Baldini-Soares, Wenhu Mikel Artetxe and Holger Schwenk. 2019b. Mas-
Chen,ZhuyunDai,TomKwiatkowski,JianmoNi, sively multilingual sentence embeddings for zero-
shotcross-lingualtransferandbeyond. Transactions
Annotating Mentions Alone Enables Efficient Domain Adaptation for
Coreference Resolution
NupoorGandhi,AnjalieField,EmmaStrubell
CarnegieMellonUniversity
{nmgandhi, anjalief, estrubel}@cs.cmu.edu
Abstract
Althoughrecentneuralmodelsforcoreference
resolutionhaveledtosubstantialimprovements
onbenchmarkdatasets,transferringthesemod-
els to new target domains containing out-of-
vocabularyspansandrequiringdifferinganno-
tation schemes remains challenging. Typical
approachesinvolvecontinuedtrainingonanno-
tatedtarget-domaindata,butobtainingannota-
tionsiscostlyandtime-consuming. Weshow
thatannotatingmentionsaloneisnearlytwice
as fast as annotating full coreference chains.
Accordingly, we propose a method for effi-
ciently adapting coreference models, which
includes a high-precision mention detection Figure1:Modelcoreferenceperformance(avgF1)asafunc-
objective and requires annotating only men- tionofcontinuedtrainingonlimitedtargetdomaindatarequir-
tions in the target domain. Extensive evalua- ingvaryingamountsofannotatortime.Thesourcedomainis
news/conversation(OntoNotes)andthetargetdomainismedi-
tionacrossthreeEnglishcoreferencedatasets:
calnotes(i2b2/VA).Usingourmethodtoadaptcoreference
CoNLL-2012 (news/conversation), i2b2/VA
modelsusingonlymentionsinthetargetdomain,weachieve
(medicalnotes),andpreviouslyunstudiedchild strongcoreferenceperformancewithlessannotatortime.
welfarenotes,revealsthatourapproachfacil-
itatesannotation-efficienttransferandresults
ina7-14%improvementinaverageF1without ersneedtoquicklyobtaininformationfromlarge
increasingannotatortime1.
volumesoftext(Uzuneretal.,2012;Saxenaetal.,
2020). However,successesovercurateddatasets
1 Introduction
havenotfullytranslatedtotextcontainingtechnical
Neural coreference
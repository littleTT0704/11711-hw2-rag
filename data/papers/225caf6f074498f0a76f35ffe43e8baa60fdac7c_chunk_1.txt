Beyond Contrastive Learning: A Variational Generative Model for
Multilingual Retrieval
JohnWieting1,JonathanH.Clark1,WilliamW.Cohen1,
GrahamNeubig2,andTaylorBerg-Kirkpatrick3
1GoogleDeepMind
2CarnegieMellonUniversity,Pittsburgh,PA,15213,USA
3UniversityofCaliforniaSanDiego,SanDiego,CA,92093,USA
{jwieting,jhclark,wcohen}@google.com,gneubig@cs.cmu.edu,tberg@eng.ucsd.edu
Abstract whichencouragessourceseparation,separatingse-
manticinformationthatissharedbetweentransla-
Contrastivelearninghasbeensuccessfullyused
tionsfromstylisticorlanguage-specificvariation.
forretrievalofsemanticallyalignedsentences,
Wefindthatbyfilteringthisvariationintoseparate
butitoftenrequireslargebatchsizesandcare-
variables,performanceoftheremainingrepresen-
fully engineered heuristics to work well. In
tations, thatencode sharedsemanticinformation,
this paper, we instead propose a generative
model for learning multilingual text embed- increasesacrossalldownstreamtasks.
dingswhichcanbeusedtoretrieveorscoresen- Throughanapproximationthatgreatlyreduces
tencepairs.Ourmodeloperatesonparalleldata
the memoryfootprint of ourmodel, we scaleour
inN languagesand,throughanapproximation
model and train on 92 languages. We systemat-
weintroduce,efficientlyencouragessourcesep-
ically compare our model, the Variational Multi-
arationinthismultilingualsetting,separating
lingualSource-SeparationTransformer(VMSST) semantic information that is shared between
translationsfromstylisticorlanguage-specific tostrongcontrastiveandgenerativebaselinesona
variation. Weshowcarefullarge-scalecompar- suiteoftasksincludingsemanticsimilarity,bitext
isonsbetweencontrastiveandgeneration-based mining,andquestionretrieval,whichweintroduce
approachesforlearningmultilingualtextem- forthecross-lingual
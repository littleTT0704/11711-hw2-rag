,andSpanish,mt5-base34willbringgains 3.3 Exp-III:JointMulti-LingualTraining
againsttheircounterpartsby0.3to6.0R1score.
In this experiment, we study joint multi-lingual
(2) Fine-tuning mt5-base on 34 languages of
training to see if different languages can benefit
XL-Sum dataset jeopardizes the performance of
from each other, and also how adding private pa-
sevenlanguages,amongwhichBengali,Burmese,
rameters for each language influences the perfor-
Tamil’sR1scorebecomesnearzero.
manceofmulti-lingualtraining(Q3).
(3)Threeofthefourlanguageswithperformance
improvement adapted from fine-tuned PLM are 3.3.1 ExperimentDetails
of the Latin script, while all three languages
WithrespecttoFig.1,wehave6differentsettings
with dramatic performance drop down are of the
to compare single language training with multi-
6Concretely,7languages(Amharic,Azerbaijani,Bengali, lingualtraining.
Burmese,Igbo,Japanese,ScottishGaelic,Spanish,Tami)are PLM Fine-tuning (PLF): mt5-base is fine-tuned
low-resource(<15,000trainingsamples),2languages(Span-
on all languages to obtain separate models for
ish,Tamil)aremedium-resource(15,000∼40,000)and2
languages(Ukrainian,Urdu)arehigh-resource(>40,000). each language as the baseline. Multi-lingual
Figure4: Trend-linesdepictingperformanceimprovement. X-axisisthelanguages,whicharearrangedinincreas-
ingorderofavailabletrainingdatafromlefttoright. Y-axisdepictstheR2scorerelativetothesingularlanguage
PLMfine-tuningbaseline.
Parameter-efficient (Adapter/Prefix) Tun- 3.3.2 ResultsandAnalysis
ing (MPE adapter/MPE prefix): We add
Thesummarizationperformanceondifferentlan-
adapters/prefixes (with parameters = 8% param-
guageswith
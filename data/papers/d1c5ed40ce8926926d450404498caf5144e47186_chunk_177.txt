 supervised model. We then evaluated the impact of the expanded sources
on QA search recall using both Jeopardy! and TREC datasets, and compared the
performance to SE using a supervised model fitted to 130,119 labeled text nuggets.
The results are summarized in Table 8.2. It can be seen that the model obtained
through active learning is almost as effective for the QA search task as the supervised
model trained on a much larger dataset. Thus active learning can be an interesting
option in situations where it is not feasible to annotate a large amount of training
data. On the other hand, if enough labeled data is available or can be annotated, it
is more effective to fit a logistic regression model to the full dataset.
We found that it only takes about one person day to label 1,000 text nuggets
selected by an active learning algorithm, a substantial reduction in annotation time
whencomparedtothe5â€“6personweeksittooktolabeltheentiredataset. Thisresult
supports our initial claim that active learning can greatly facilitate the adaptation of
the statistical SE approach to new domains and applications. Here we already take
into account that it is more time-consuming to label 1,000 text nuggets that were
selected individually than to annotate whole documents of the same total length. We
also found that it is feasible to accurately label independent text nuggets as long as
the annotator first familiarizes himself with the seed topics and is shown the source
documents of the nuggets during the annotation process.
132 CHAPTER 8. EXTENSIONS FOR RELEVANCE ESTIMATION
8.2 Sequential Models
To estimate the relevance of text nuggets with regard to a seed document, we initially
fitted a logistic regression model that scores each nugget independently based on a
set of relevance features (see Sections 4.3.2 and 4.3.3). However, we found that text
nuggets extracted from the same document are not independent, but a nugget is more
likelytoberelevantifprecedingorfollowingnuggetsarerelevantandshouldtherefore
be evaluated in the context of surrounding nuggets. In a first attempt, we added
features of adjacent nuggets to the independent LR model to capture dependencies
between nuggets in an ad-hoc fashion.
We now propose a sequential model that leverages these dependencies in a more
pr
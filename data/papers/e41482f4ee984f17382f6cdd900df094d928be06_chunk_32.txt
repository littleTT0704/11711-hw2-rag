largelanguagemodels. ArXiv
preprint,abs/2305.16291,2023. URLhttps://arxiv.org/abs/2305.16291.
13
Underreview
Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for
open-domaincodegeneration. ArXivpreprint,abs/2212.10481,2022. URLhttps://arxiv.
org/abs/2212.10481.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
NeuralInformationProcessingSystems,35:24824–24837,2022.
NancyXu,SamMasling,MichaelDu,GiovanniCampagna,LarryHeck,JamesLanday,andMonica
Lam. Groundingopen-domaininstructionstoautomatewebsupporttasks. InProceedingsofthe
2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,pp.1022–1032,Online,2021.AssociationforComputational
Linguistics. doi: 10.18653/v1/2021.naacl-main.80. URLhttps://aclanthology.org/
2021.naacl-main.80.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,
andChristopherD.Manning. HotpotQA:Adatasetfordiverse,explainablemulti-hopquestion
answering. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,pp.2369–2380,Brussels,Belgium,2018.AssociationforComputationalLinguistics.
doi: 10.18653/v1/D18-1259. URLhttps://aclanthology.org/D18-1259.
Shunyu Yao, Howard
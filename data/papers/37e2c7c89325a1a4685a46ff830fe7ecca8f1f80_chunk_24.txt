anguages. Comparingwiththebestattentionlayer/head,anapproachusedby
Fomichevaetal.[2021b],Trevisoetal.[2021],SMaTachievessimilarAUCscoresforsourceexpla-
nations,butlagsbehindthebestattentionlayer/headfortargetexplanationson*-ENlanguagepairs.
However,asstressedpreviouslyfortextandimageclassification,SMaTsidestepshumanannotation
andavoidsthecumbersomeapproachofindependentlycomputingplausibilityscoresforallheads.
5Pearsoncorrelationisthestandardmetricusedtoevaluatesentence-levelQEmodels.
8
6 ImportanceoftheHeadProjection
Amajorcomponentofourframeworkisthenormalizationoftheheadcoefficients,asdefinedin
§4. Althoughmanyfunctionscanbeusedtomapscorestoprobabilities,wefoundempiricallythat
SPARSEMAXperformsthebest,whileothertransformationssuchasSOFTMAXand1.5-ENTMAX
[Peters et al., 2019], a sparse transformation more dense than sparsemax, usually lead to poorly
performingstudents(seeTable7).
Table7: Simulabilityresults,intermsofaccuracy(%),ontheMLQEdatasetwith4200training
examples,withdifferentnormalizationfunctions.
SPARSEMAX SOFTMAX 1.5-ENTMAX NoNormalization
NoExplainer.7719±[.7660:.7802].7719±[.7660:.7802].7719±[.7660:.7802].7719±[.7660:.7802]
Attention(alllayers).8193±[.8186:.8280].7345±[.7335:.7390].7152±[.7111:.7161].7781±[.7762:.7791]
Attention(lastlayer).7720±[.7672:.7726].7697±[.7659:.7715].7807±[.7652:.7821].7768±[.7764:.7807]
Attention(SMaT).86
hesumofweightedlosses. Notethat methodsareapplicableaswell.
theoptimalsolutionsW∗(A)andT∗(A)arefunc-
• Method 1: In (Tan and Bansal, 2019), a
tionsofAsinceW∗(A)andT∗(A)arefunctions
large-scaleTransformer(Vaswanietal.,2017)
ofthelossfunction,whichisafunctionofA.
modelisbuiltthatconsistsofthreeencoders:
In the second stage, we finetune the image en-
anobjectrelationshipencoder,alanguageen-
coderandtextencoderintheVQAtaskdefinedon
coder,andacross-modalencoder. Thethree
the PathVQA dataset D. Let V, U, R denote the
encodersarebuiltmostlybasedontwokinds
networkweightsoftheimageencoder,textencoder,
ofattentionlayers—self-attentionlayersand
andQAnetworkrespectively. WetrainV,U,Rby
cross-attentionlayers. Theobjectrelationship
minimizingtheVQAloss:
(cid:80)N(tr) L(d(tr)
,V,U,R)
i=1 i encoder and the language encoder are both
(tr)
whered isatrainingexampleinD,consisting
i single-modalityencoders. Across-modalen-
of an input pathology image, an input question,
coder is proposed to learn the connections
and an output answer. When training V and U,
betweenvisionandlanguage.
we encourage them to be close to the optimally
trainednetworkweightsW∗(A)andT∗(A)ofthe • Method 2: The method proposed in (Kim
imageandtextencoderinthefirststage,totransfer et al., 2018) uses a Gated Recurrent Unit
therepresentationslearnedintheSSLtasktothe (GRU) (Cho et al., 2014) recurrent network
VQA task. The second stage amounts to solving andaFasterR-CNN(Renetal.,2015b)net-
thefollowingoptimizationproblem: work to embed the question
byreconstructingimagesfromvariousdomains
networkisusefulcomesfromthefactthatthelearnedvari- and class semantics to simulate variations and minimize
ation encoder can well approximate some of the intrinsic domain divergence. PAC constrained learning [15, 16] is
intra-andinter-domainvariations. Italsoservesasfeature adoptedformodelingcross-domainvariationsunderdomain
removalsincemoretrainingexamplesaugmentedbyspecific transformationinMBDG[60]. Wehighlightseveralmajor
variationfactorsleadtomoreinvariantrepresentationsfor differences between our approach and MBDG below: (1)
thosevariations. Moreover,theincreaseddiversityofsource DDGimposesweakerassumptions;(2)MBDGconsumes
domaindataimprovesthelikelihoodthatanunseendistribu- additionaldomainlabels,whichareoftenhardtoobtainin
tionlieswithintheconvexhullofsourcedomains[2]. For manyapplications,whileDDGdoesnot;(3)DDGenforces
example,inFig.1,theoriginaldatasetcanbeaugmentedvia invarianceconstraintsviaparameterizingsemanticandvari-
alearnedmanipulatorbycomposingadiversecombination ation encoders, which does not belong to a model-based
ofsemanticandvariationfactors. Suchadisentanglement approach. Incontrast,MBDGrequiresapre-traineddomain
canbeagoodpredictorforOODgeneralizationaccording transformationmodel(e.g.,CycleGAN)duringtraining. Ap-
to[21]. WehighlightthefollowingadvantagesofDDG: pendixDprovidesthedetailedcomparisontoMBDG.
Disentangled Representation Learning. The goal of
• DDGadoptsaprincipledconstrainedlearningformulation
disentangledrepresentationlearningistomodeldistinctand
based on disentanglement, yielding rigorous theoretical
explanatory factors of variation in the data [8, 68]. [21]
guaranteesontheempiricaldualitygap.
showsthatdisentanglementisagoodpredictorforout-of-
• Ouralgorithmisconceptuallysimpleyeteffective. DDG
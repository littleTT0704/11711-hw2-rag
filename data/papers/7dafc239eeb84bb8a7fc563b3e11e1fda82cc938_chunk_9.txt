))jj (cid:0)1)2] (6)
We use graph gated recurrent neural networks (GRNN) [Li (f^;c)(cid:24)Pf^;c 2
S
et al., 2015] to encode the classifier g. Let V(l) denote the
set of adjacent codes of l from the ICDl tree hierarchy and t where((cid:1);c) (cid:24) P S(cid:1);c isthejointdistributionoflatentfeatures
bethenumberoftimeswepropagatethegraph,theclassifier
andencodedlabelvectorsfromthesetofseencodelabelsS,
g =gtiscomputedby:
f^=(cid:11)(cid:1)f+(1(cid:0)(cid:11))(cid:1)f~with(cid:11)(cid:24)U(0;1)and(cid:21)isthegradient
l l
penaltycoefficient.WGAN-GPcanbelearnedbysolvingthe
1 minimaxproblem: min max L.
ht = (cid:6) gt(cid:0)1; gt =GRU(ht;gt(cid:0)1) (3) G D WGAN
l jV(l)j j2V(l) j l l l
Label encoder. The function C is an ICD-code encoder
where g0 = v and GRU is gated recurrent units [Chung et that maps a code description to an embedding vector. For
l l acodel,wefirstuseaLSTM[HochreiterandSchmidhuber,
al., 2014]. The weights of the binary code classifier is tied
1997]toencodethesequenceofM wordsinthedescription
withthegraphencodedlabelembeddingg sothatthelearned
l into a sequence of hidden states [e ;e ;:::;e ]. We then
knowledge can also benefit zero-shot codes since label em- 1 2 M
performadimension-wisemax-poolingoverthehiddenstate
beddingiscomputedfromasharedword. Thelossfunction
sequencetog
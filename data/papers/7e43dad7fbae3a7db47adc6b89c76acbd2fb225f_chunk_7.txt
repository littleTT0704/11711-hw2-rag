ings generated by SentencePiece (Kudo and sumably harder negative examples (Karpukhin
Richardson,2018). ThesecondisSBERT(Reimers etal.,2020)andthusthemodelmustworkharder
andGurevych,2019),whichencodesapairofsen- todistinguishbetweenthem. Wewillexplainthe
tenceswithasiameseBERTmodelthatisfinetuned extraction of the labeled step-goal pairs used to
on paraphrase corpus. For comparison, we addi- trainthismodelinÂ§4.1.
tionally experiment with search engines as M b, Concretely,weexperimentwithtwopretrained
specificallyElasticsearchwiththestandardBM25 models as M, specifically BERT-base (Devlin
c
weightingmetric(RobertsonandZaragoza,2009). etal.,2018)and DEBERTA-largefinetunedonthe
Weindexeacharticlewithitstitleonlyorwithits MNLIdataset(Heetal.,2020). Wepickthemdue
fullarticle. WealsoexperimentwithBingSearch totheirhighperformanceonvarioustasks(Zhang
APIwherewelimitthesearchtowikiHowwebsite etal.,2019). 4
only3. The BM25 with the former setting resem- Inaddition,weconsiderincludingdifferentctx
blesthemethodproposedbyLagosetal.(2017). in the reranking input. For each step, we exper-
iment with including no context, the goal of the
3.2 Reranking
step,andthesurroundingstepsofthestepwithina
Whileefficient,encodingstepsandgoalsindepen-
window-sizen(n=1).
dentlyislikelysub-optimalasinformationinthe
stepscannotbeusedtoencodethegoalsandvice- 3.3 UnlinkableSteps
versa. Therefore,weconcatenateastepwitheach SomestepsinwikiHowcouldnotbematchedwith
ofitstop-k candidategoalsinC(s)andfeedthem anygoal. Suchstepsareunlinkablebecauseofsev-
toamodelM c thatjointlyencodeseachstep-goal
 that can bridge these complementary paradigms so that
advantages in model design, solver efficiency, side-information incorporation, and theoretical guarantees can
be translated across paradigms. As a prelude of our presentation of the ‘standard equation’ framework toward
this goal, here we begin with a recapitulation of the maximum entropy view of statistical learning. By naturally
marrying the probabilistic frameworks with the optimization-theoretic frameworks, the maximum entropy
viewpoint had played an important historical role in offering the same lens to understanding several popular
methodologies such as maximum likelihood learning, Bayesian inference, and large margin learning.
2.1. Maximum Likelihood Estimation (MLE)
We start with the maximum entropy perspective of the maximum likelihood learning.
2.1.1. Supervised MLE
We consider an arbitrary probabilistic model (e.g., a neural network or probabilistic graphical model for, say,
language generation) with parameters θ ∈ Θ to be learned. Let p (x) ∈ P(X) denote the distribution
θ
defined by the model, where X is the data space (e.g., all language text) and P(X) denotes the set of all
probability distributions on X. Given a set of independent and identically distributed (i.i.d.) data examples
D = {x∗ ∈ X}, the most common method for estimating the parameters θ is perhaps maximum likelihood
estimation (MLE). MLE learns the model by minimizing the negative log-likelihood:
min−Ex∗∼D[logp θ(x∗)].
(2.1)
θ
5
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
MLE is known to be intimately related to the maximum entropy principle (Jaynes, 1957). In particular, when
the model p (x) is in the exponential family of the form:
θ
p θ(x) =exp{θ ⋅T(x)}/Z(θ), (2.2)
where T(x) is the sufficient statistics of data x and Z(θ) = ∑ exp{θ ⋅ T(x)} is the
x∈X
normalization factor, it is shown that
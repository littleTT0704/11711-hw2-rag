λ¯(p) 0and(cid:80) λ¯(p)=1.0.
≥
Thisindicatesthat,onceλ(p)issettoafunction,theassumptiononthePMFofpismade. Inmost
of the previous methods (Tarvainen & Valpola, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020;
Zhang et al., 2021; Xu et al., 2021b), although they do not explicitly set λ(p), the introduction of
lossweightschemesimplicitlyrelatestothePMFofp. Whilethegroundtruthlabelpisactually
unknowninpractice,wecanstilluseitfortheoreticalanalysis.
In the following sections, we explicitly derive the sampling weighting function λ(p), probability
massfunctionλ¯(p),quantityf(p),andqualityg(p)foreachrelevantmethod.
13
PublishedasaconferencepaperatICLR2023
A.1.2 NAIVEPSEUDO-LABELING
Innaivepseudo-labeling(Leeetal.,2013), thepseudo-labelsaredirectlyusedtothemodelitself.
Thisisequivalenttosetλ(p)toafixedvalueλ,whichisahyper-parameter. Wecanwrite:
max
λ(p)=λ, (13)
max
λ 1
λ¯(p)= max =, (14)
N λ N
U max U
f(p)=(cid:88)NU
λ
max =λ, (15)
N max
U
i
g(p)=(cid:88)NU 1(pˆ
i
=y iu)
. (16)
N
U
i
We can observe that the naive self-training maximizes the quantity of the pseudo-labels by fully
enrolling them into training. However, full enrollment results in pseudo-labels of low quality. At
beginningoftraining,alargeportionofthepseudo-labelswouldbewrong,i.e.,γ(p)=0,sincethe
modelisnotwell-learned. Thewrongpseudo-labelsusuallyleadstoconfirmationbias
uristic7 by choosing the same size of contexts in the sen-
tence selection step of each AL cycle (Line 4 in
that selects the union of sentence-wise uncertain
Algorithm 1). Then, we further compare by the
sub-structures as well as global ones since both
sub-structure labeling cost, measured by the sub-
maycontaininformativesub-structures. Finally,all
structureannotationcost. IfPAcanroughlyreach
the presented results are averaged over five runs
theFAperformancewiththesamereadingcostbut
withdifferentrandomseeds.
fewersub-structuresannotated,itwouldbefairto
Modelandtraining. Forthemodels,weadopt saythatPAcanhelpreducecostoverFA.Abetter
standard architectures by stacking task-specific comparing scheme should evaluate against a uni-
structuredpredictorsoverpre-trainedRoBERTa fiedestimationoftherealannotationcosts(Settles
base
(Liuetal.,2019)andthefullmodelsarefine-tuned etal.,2008). Thisusuallyrequiresactualannota-
ateachtrainingiteration. Afterobtainingnewan- tion exercises rather than simulations, which we
notationsineachALcycle,wefirsttrainamodel leavetofuturework.
based on all the available full or partial annota-
3.3 NERandDPAR
tions. When using self-training, we further ap-
plythisnewlytrainedmodeltoassignpseudosoft Settings. Wecompareprimarilythreestrategies:
labels to all un-annotated instances and combine FA, PA, and a baseline where randomly selected
themwiththeexistingannotationstotrainanother sentencesarefullyannotated(Rand). Wealsoin-
model. Comparedtousingtheoldmodelfromthe cludeasupervisedresult(Super.) whichisobtained
last AL cycle, this strategy can give more accu- fromamodeltrainedwiththefulloriginaltraining
ratepseudolabelssincethenewlyupdatedmodel set. Wemeasurereadingcostbythetotalnumber
usuallyperformsbetterbylearningfrommorean- of tokens in the selected sentences. For labeling
notations. ForPA,pseudosoftlabelsareassigned cost,wefurtheradoptmetricswithpractical
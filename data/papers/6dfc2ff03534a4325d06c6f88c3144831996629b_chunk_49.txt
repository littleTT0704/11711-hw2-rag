titles,sofortheother55%,wedon’tuseanysubtitle
One example is VQA 1.0 [5]: given a question like
information.
‘What color is the fire hydrant?’ a model will clas-
sify some answers higher than others (red). This was
♥: Noofficialtrain/val/testsplitisavailable,sowesplitthedata
bymovie,using20%ofdataforvalidationandtherestfor
addressed in VQA 2.0 [26], however, some answers
training.
will still be more likely than others (VQA’s answers
are open-ended, and an answer to ‘What color is the ♦: Thereseemtobeissueswiththepubliclyreleasedtrain-test
firehydrant?’ mustbeacolor). split of TGIFQA (namely, a model with high accuracy on
a held-out part of the training set doesn’t generalize to the
These priors can either arise from biases in the world
providedtestset)sowere-splitthemultiple-choicedataour-
(fire hydrants are usually red), or, they can come from an-
selvesbyGIFandholdout20%forvalidation.
notationartifacts[28]:patternsthatarisewhenpeoplewrite
class-conditionedanswers.Sometimesthesebiasesaresub-
liminal: when asked to write a correct or incorrect story withsubtitles;MovieQA[75],withvideosfrommoviesand
ending, the correct endings tend to be longer [72]. Other questionsobtainedfromhigher-levelplotsummaries;Poro-
casesaremoreobvious: workersoftenusepatternssuchas roQA [43], with cartoon videos; and TGIFQA [39], with
negationtowritesentencesthatcontradictasentence[28].22
templated questions from the TGIF dataset [47]. We note
To what extent do vision datasets suffer from annota- that these all differ from our proposed in terms of
VCR
tionartifacts,versusworldpriors? Wenarrowourfocusto subject matter, questions asked, number of answers (each
multiple-choice question answering datasets, in which for
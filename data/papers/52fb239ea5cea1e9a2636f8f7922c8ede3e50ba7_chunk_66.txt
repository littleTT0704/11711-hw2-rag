
varying degrees of linguistic complexity, and diverse question formats and back-
ground knowledge requirements. Importantly, L¯ila extends all of these datasets
to include a solution program as opposed to only an answer, and instruction
2NamedafterL¯ilavati,a12thcenturymathematicaltreatiseonarithmeticthatcoverstopics
likearithmeticandgeometricprogressions,indeterminateequationsandcombinations. Itis
alsowidelyknownfortheextensivenumberofmathwordproblems. Theauthor,Bha¯skara is
knownforfundamentalandoriginalcontributionstocalculus,physics,numbertheory,algebra,
andastronomy(Colebrooke,1817;Sarkar,1918;Kolachanaetal.,2019)
2
annotations to enable instruction-based learning (Sanh et al., 2021; Wei et al.,
2021; Mishra et al., 2022b).
In order to accurately assess the mathematical reasoning ability of models,
evaluating the chain of reasoning that leads to the correct solution is equally
important (if not more important) to evaluating the final answer or expression.
We therefore collect Python programs that serve as reasoning chains for each
questioninthebenchmark. Weachievethisbyautomaticallyconvertingdomain-
specific language (DSL) annotations into Python programs and by manually
collecting expert annotations when no DSL annotations are available. By
incorporating program annotations, L¯ila unifies various mathematical reasoning
datasets under a single problem formulation i.e., given an input problem in
natural language, generate a Python program that upon execution returns the
desired answer. This formulation allows neural approaches to focus on the high-
levelaspectsofmathematicalproblemsolving(e.g.,identifyingpotentialsolution
strategies,decomposingtheproblemintosimplersub-problems),whileleveraging
external solvers (e.g., Python builtins, Sympy) to perform precise operations
like adding huge numbers or simplifying expressions. Figure 1 illustrates a
sample from our L¯ila benchmark that illustrates the question, answer, program,
instructions, and category tags.
In addition to evaluating high-level problem solving, we also facilitate two
other key ways to make a fair assessment of models on mathematical reason-

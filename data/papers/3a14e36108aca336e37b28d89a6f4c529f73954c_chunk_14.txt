estSupcomparedtohumanrewritingsandoriginal
thatmanyGPT-2generationsincludedtrailingexclamation
stories.Tables2and3containhumanevaluationresults,and
marks,andstriptheseifmorethanfouroccurinarow.8
Table4automaticevaluationresults.Correlationsbetween
automaticandhumanmetricsareinTable5.Table6contains
HumanEvaluation
qualitativeexamples,withmoreinAppendixE.
Annotatorsevaluate100testSupexampleseachfromtheorig-
inalstories,humanrewritings,outputsfromourtwo-stage
5.1 AnalysisofHumanEvaluationResults
7NucleussamplingdidnotworkaswellforBARTandT5. We begin by analyzing human evaluation performance
8SeeAppendixCformorefinetuning/generationdetails. throughresultsinTables2and3.
10648
Method\Metric Fluency Coherence Logic Plot-pres TargetOrderFidelity(TOF): Itappearsallapproaches
Originalstories 4.209 4.0 3.851 N/A arereasonable(e.g.>50forTOFmetrics),andoutputsare
Humanrewritings 3.797 3.723 3.784 3.972
likelyinthecorrecttargetorders.Humanrewritingshavethe
GPT2-d-2S 3.635 3.399 3.399 3.708
lowestTOF;humansarelessconservativewhilerewriting
GPT2-r-2S 3.595 3.378 3.291 3.375
(shownin§5.3).GPT-2modelsmodifytextsecondheaviest,
BART-d-1S 3.628 3.412 3.318 3.847
BART-d-2S 3.818 3.507 3.493 3.722 butperformworstoverall.Theyintroducemoreerrors,e.g.
BART-r-2S 3.757 3.439 3.493 3.861 repeatingorhallucinatingtodegradetextqualityandplot-
T5-d-2S 3.764 3.419 3.5 3.889 pres(§5.3).BARTandT5modelsaremoreconservative
Bashcommandsandmanuals;whileCoNaLaexampleswereminedfromStackOverflow
posts,whereusersaskquestionswithlimitedornocontext. Thus,NLintentsinCoNaLarequire
abettersemanticalignmentwiththedocuments,andthusbenefitfromdenseretrievers. Thegap
resultingfromdifferentdatacurationprocesseswasalsoobservedbyRodriguezandBoyd-Graber
(2021)inopen-domainquestionanswering(QA).
Second,retrieversthatwerepretrainedonthetargetprogramminglanguagearegenerallystronger.
For example in CoNaLa, CodeT5 which was pretrained on Python, is both a better off-the-shelf
retrieverandabetterfinetuned-retrieverthanRoBERTa,whichwaspretrainedmainlyontext. In
contrast,tldrisbasedonBash,whichneitherCodeT5norRoBERTawereexplicitlypretrainedon.
Thus,tldrbenefitsmostlyfromBM25andRoBERTaratherthanCodeT5asretrievers.
Finally, training the retriever using weak supervision on the documentation pool (Section 3.1)
dramatically improves the retriever. The recall of the best retrievers of each dataset without this
corpus is shown in the last column of Table 4 (“Best w/o weak sup.”). On CoNaLa, removing
thiscorpusresultsinsevereperformancedegradation. Onepossibleexplanationisthatthisweak
supervisionhelpstheretrieverperformdomainadaptationmoreeffectively.
6.3 CASESTUDY
We examine the models’ outputs and show two representative examples in Table 5. In the first
example, Image.open was not seen in the training set, and the baseline CodeT5 incorrectly
predictsos.open. Incontrast,usingDocPromptingallowstoretrievethedocsandtocorrectly
predictImage.open.Inthesecondexample,df.to csvwasnotseenintraining,andthebaseline
CodeT5failstocorrectlypredictit.Incontrast,DocPromptingdoespredictmostofthedf.to csv
call correctly, thanks to the retrieved docs.
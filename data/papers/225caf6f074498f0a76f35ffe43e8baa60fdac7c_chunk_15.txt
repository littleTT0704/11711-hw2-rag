.(2017),i.e.,thelearningrate
j∈B
1 (cid:88) increaseslinearlyfor4,000steps,afterwhichitis
loss = − logp(s |t )+logp(t |s )
2|B| i i i i decayedproportionallytotheinversesquarerootof
(si,ti)∈B thenumberofsteps. Wesetthepeaklearningrate
tobe0.001,andwetrainourmodelsfor100,000
whereBisaminibatch. Thisversionofcontrastive
stepstotal. Weuseabatchsizeof2048andsetthe
learninghasbeenusedinrepresentationlearning
maximumsequencelengthofourmodelto32for
forretrieval(DPR,Karpukhinetal.,2020),visual
allexperiments.
tasks(SimCLR,Chenetal.,2020)andimage/text
Weuseadropoutrateof0.1for CONTRASTIVE
tasks(CLIP,Radfordetal.,2021). Thereareother
models and no dropout for BITRANSLATION,
variationsofthisloss(Qianetal.,2019),andother
VMSST CONTRASTIVE (with the exception of
9Thisisopposedtotheformulationintheoriginalpaper therandomlyinitialized24layermodelwhichused
whereamodelbasedonBARTLarge(Lewisetal.,2020a)was
0.1), and VMSST. For VMSST, we anneal the
fine-tunedusingaRAG-likeobjective(Lewisetal.,2020b)
ontheNQtrainingdatainamodeltheauthorscallRePAQ. KLtermsothatitincreasedlinearlyfor1,000,000
RePAQ,withoutusingarerankerachievesanaccuracyof41.2 updates.
onNQ.
For VMSST, we set λ, the weight on the
10TechnicallyPearson’srcanbenegative,butthisdoesnot
happeninourevaluations. VMSST ELBOlossterm,tobe0.025forthepre-
Model Sem.Sim. BitextMining Quest.Retrieval Score
Eng. XL XL(s.) XL(d
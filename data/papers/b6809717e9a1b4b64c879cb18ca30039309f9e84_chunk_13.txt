performance
ink-folds(k=10),usingonefold(n=392)forval- variesbasedonannotationsubjectivity. Finally,we
idation and nine other folds combined for train- inspectthemodelparameterstoidentifycommon-
ing. From each of the 10 models, we obtained senseandnarrativefeaturesthataremostinforma-
thepredictionforthevalidationset. Together,the tiveindetectingeventboundaries.
validationsetsforthe10modelscombinetoform
Improving prediction of event boundaries As
predictionsfortheentiredataset,whichweuseto
seen in Table 2, RoBERTa alone performs fairly
conduct significance testing in order to compare
well in predicting event boundaries (F1 = 75.8%,
theperformanceofmodels.
within 2.2% F1 of our best performing model),
GRU wasaccessedfromPyTorch,withK G set butcanbefurthersupportedbyourcommonsense
to33andahiddendimensionof33. andnarrativefeaturestoimproveitsperformance.
In contrast, the commonsense and narrative fea-
RoBERTa RoBERTa-base-uncased with 12-
tures alone do not perform as well.1 Overall,
layer, 768-hidden (K ), 12-heads, 110M param-
R
our best performing set-up is the Event Detector
eters, 0.1 dropout was used, accessed from Hug-
(PREVIOUSONLY)withF1=78.0%,whichissig-
gingFaceTransformerslibrary(Wolfetal.,2020).
nificantlydifferentfromRoBERTaalonebasedon
Whenmorethan10priorsentencesareavailablein
McNemarâ€™s test (p <0.05). 2 Its overall strong
astory,weuseonlythemostrecent10sentences
performanceislargelycontributedbyitsstrongper-
duetoRoBERTainputsequencelengthlimitations.
formanceindetectingnoeventboundariesandex-
Evaluation Metrics While capturing distribu- pectedeventboundaries. F1fornoeventboundary
tionalinformationofsubjectivejudgementlabels ishigherthanbothsurprisingandexpectedevent
(PavlickandKwiatkowski
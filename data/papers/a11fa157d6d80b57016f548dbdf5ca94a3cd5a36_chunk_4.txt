 We apply our approach on a range of LMs of
2020), have been found to encode a significant varyingcapacities,suchasROBERTA,BERT,and
amountofknowledgeimplicitlyintheirparameters. DISTILBERT. Inparticular,weharvestknoweldge
RecentresearchattemptedtouseLMsasflexible ofover400newrelations(anorderofmagnitude
knowledge bases by querying the LMs with arbi- more than ConceptNet relations) not available in
traryprompts(e.g.,"Obama was born in "for preexistingKGsandpreviousextractionmethods.
theanswer"Hawaii")(Petronietal.,2019). How- Extensivehumanandautomaticevaluationsshow
ever, such implicit query-based knowledge falls ourapproachsuccessfullyextractsdiverseaccurate
short of many desirable properties of a full-scale knowledge,includingtuplesforcomplexrelations
KGsuchasConceptNet(AlKhamissietal.,2022), suchas“A is capable of, but not good at,
including easy access, browsing, or even editing B” and 3-ary relations such as “A can do B at
(Zhu et al., 2020; Cao et al., 2021), as well as C”.Interestingly,theresultingKGsalsoserveasa
assuranceofknowledgequalitythankstothesym- symbolicinterpretationofthesourceLMs,reveal-
bolic nature (Anderson et al., 2020). Symbolic ingnewinsightsintotheirknowledgecapacitiesin
KnowledgeDistillation(SKD, Westetal.,2022) terms of varying factors such as model size, pre-
explicitlyextractsaknowledgebasefromGPT-3. trainingstrategies,anddistillation.
However, the approach exclusively relies on the
2 RelatedWork
strongin-contextlearningcapabilityofGPT-3and
thus is not applicable to other rich LMs such as Knowledgegraphconstruction Popularknowl-
BERT (Devlin et al., 2019) and ROBERTA (Liu edge bases or KGs are usually constructed with
et al., 2019). Moreover, its use of a quality dis- heavyhumanlabor. Forexample,WordNet(Fell-

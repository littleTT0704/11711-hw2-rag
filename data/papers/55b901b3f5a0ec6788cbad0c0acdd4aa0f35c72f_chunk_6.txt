-then-finetuneparadigm(Huhetal.,2016;Devlinetal.,2018;Schneideretal.,2019;
Gururanganetal.,2020)aswellasend-taskawaremultitaskingapproaches(Linetal.,2019;Dery
etal.,2021a;b). Whilstauxiliaryobjectivesmaybemeta-learned(Liuetal.,2019a;Navonetal.,
2020),forsimplicity–sinceincorporatingthesewouldrequirefurthercomplicationofourdesign
space–suchobjectivesareoutofthescopeofthispaper.
This work bears many parallels to the area of neural architecture search (NAS) (Stanley &
Miikkulainen,2002;Zoph&Le,2016;Robertsetal.,2021). Whilstweseektoautomateauxiliary
learning,theobjectiveofNASistoautomatethediscoveryoftherightneuralarchitecturegivena
specificend-task. Searchspacesofcandidatearchitecturesarecreatedbytakingthecartesianproduct
ofarchitecturedesignchoicesacrossthedepthofthenetwork. Thedesignofsuitablearchitectural
searchspacesforavarietyofsettingshasbeenanactiveareaofresearch(Tan&Le,2019;Howard
etal.,2019;Daoetal.,2020;Robertsetal.,2021). TodevelopAANG,weborrowideasfromthe
NAS literature on efficient algorithms for sifting through spaces of architectures. Mirroring the
populardifferentiableNASmethodDARTSLiuetal.(2018),weperformacontinuousrelaxation
overthesearchspaceofobjectives,allowingforefficientsearchbygradientdescent. Wealsousea
factoredapproachtomodelrelationshipsbetweenobjectivesthatshareprimitives. Thisisinspiredby
recentworkonstochastic-relaxationweightsharing(Dong&Yang,2019;Lietal.,2020).
Asatheoreticalcontribution, thisworkderivesanend-taskawaregeneralizationerrorboundfor
auxiliarylearning. OurboundisbuiltonthatofHardtetal.(2016),whoderivegeneralizationbounds
forparametricmodelstrainedwithstochasticgradientdescent(SGD).Toderivetheir
securityofsuch
guages. Still,itslanguagecoverageisboundedby
uniquecodingproblems.
the available forums in StackOverflow. We hope
ourinitiativecanhighlightthemultilingualnature
ofprogramdevelopers,encouragetheemergence
of similar data resources in other languages, and
continuouslypromoteAIprogrammingassistance
inlanguagesworldwide.
Third,ODEXcoverswide-rangingcodequeries
in the open domain, it is more suitable for less
resource-demandingscenariossuchasdownstream
evaluationorfew-shotlearning. AlthoughODEX
islargerthanmanypreviousdatasetswithhuman-
written test cases, it is still limited due to the in-
tensehumaneffortrequiredbythecurationprocess.
Regarding this, we encourage readers to conduct
significancetesting(Droretal.,2018)andreport
moresubstantialmodelimprovements.
EthicsStatement
OurworkhasreceivedIRBapprovalandislicensed
underaCreativeCommonsAttribution-ShareAlike
(CCBY-SA)4.0InternationalLicense. Theresult-
ingODEXdatasetisbuilttoserveasabenchmark
References Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing,TravisHoppe,CharlesFoster,JasonPhang,Ho-
Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer.
race He, Anish Thite, Noa Nabeshima, et al. 2020.
2019. Juice: A large scale distantly supervised
The pile: An 800gb dataset of diverse text for lan-
dataset for open domain context-based code gener-
guagemodeling. arXivpreprintarXiv:2101.00027.
ation. In Proceedings of the 2019 Conference on
EmpiricalMethodsinNaturalLanguageProcessing Patrick Haluptzok, Matthew Bowers, and Adam Tau-
andthe9thInternationalJointConferenceonNatu- man Kalai. 2022. Language models can teach
ralLanguageProcessing(EMNLP-IJCNLP),pages themselves to program better. arXiv preprint
5436â€“5446. arXiv:2207.14502
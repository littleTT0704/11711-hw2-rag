wefinetuneasequence-to-sequence
lowempathicallysimilarstorypairs,wefindthat
(seq2seq) model to generate an event (v), emo-
empathically similar story pairs have statistically
tion (e), and moral (m), concatenating annotated
significantlyhighersimilaritiesinevents,emotions,
summaries to construct the gold label and mod-
andmorals,withthelargestincreaseinmoralsimi-
eling p(v,e,m|c) (Kim et al., 2022). The model
larityandroughlyequivalentincreasesineventand
is trained to minimize negative log likelihood of
emotionsimilarities.
predictingeachwordintheconstructedgoldlabel.
Next, we look at the differences between se-
manticsimilarityandhuman-ratedempathicsim- Empathy Reasoning Results. We evaluate em-
ilarity. As shown in Figure 6, we can see that pathy reasoning performance using BLEU (Pa-
the distributions of similarity scores are differ- pineni et al., 2002), ROUGE (Lin, 2004), ME-
entforhuman-ratedempathicsimilarityscoresas TEOR(BanerjeeandLavie,2005),andBertScore
compared to semantic similarity scores obtained (Zhang et al., 2020), taking the human-written
from SBERT. Semantic similarity of stories is free-textannotationsasgoldreferences. FromTa-
ble 6, we see that finetuning BART with human- Notethatwedidnotincludetopicsfortheevents
written story summaries improves performance since these were similar to Table 2. To identify
acrossallmetrics. TheBARTmodelfinetunedon these topics, we use Latent Dirichlet Allocation
EMPATHICSTORIES demonstrates improved per- (LDA)andKeyBERTontheclusters(Grootendorst,
formanceacross3/4metricsineventandmoralrea- 2020).
sons. Foremotionreasons,ChatGPTdemonstrates
Topic Keywords %Stories
better performance in 2/4 metrics, with the fine-
depression melancholy,depression,unhappy 28.95%
tunedBARTmodelclosebehind. Wenotethatthe
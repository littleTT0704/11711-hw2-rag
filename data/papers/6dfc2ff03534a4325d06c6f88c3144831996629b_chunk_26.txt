visualcommonsense.com/explore.
6.2.ResultsandAblations long) text spans. Our model, R2C obtains an additional
WepresentourresultsinTable1.Ofnote,standardVQA boost over BERT by 9% accuracy, reaching a final perfor-
models struggle on our task. The best model, in terms of mance of 44%. Still, this figure is nowhere near human
Q→ARaccuracy,isMLB,with17.2%accuracy. Deeptext- performance: 85%onthestagedtask,sothereissignificant
onlymodelsperformmuchbetter:mostnotably,BERT[15] headroomremaining.
obtains35.0%accuracy. Onepossiblejustificationforthis Ablations Weevaluatedourmodelunderseveralabla-
gapinperformanceisabottleneckingeffect: whereasVQA tions to determine which components are most important.
modelsareoftenbuiltaroundmultilabelclassificationofthe Removing the query representation (and query-response
top1000answers, requiresreasoningovertwo(often contextualization entirely) results in a drop of 21.6% ac-
VCR
7
curacy points in terms of Q → AR performance. Interest- the space of commonsense inferences is often limited by
ingly, this setting allows it to leverage its image represen- theunderlyingdatasetchosen(synthetic[79]orCOCO[58]
tation more heavily: the text based response-only models scenes). Inourwork,weaskcommonsensequestionsinthe
(BERT response only, and LSTM+ELMo) perform barely contextofrichimagesfrommovies.
better than chance. Taking the reasoning module lowers Explainability AImodelsareoftenright,butforques-
performance by 1.9%, which suggests that it is beneficial, tionable or vague reasons [7]. This has motivated work in
but not critical for performance. The model suffers most having models provide explanations for their behavior, in
whenusingGloVerepresentationsinsteadofBERT:aloss the form of a natural language sentence [31, 9, 41] or an
of 24%. This suggests that strong textual representations attentionmap[32,35,37]. Ourrationalescombinethebest
arecrucialto
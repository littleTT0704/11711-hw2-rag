etonewlanguages. helpfulforparameter-efficienttuning.
The11left-outlanguagesthatdonotparticipatein
fine-tuningarechosenaccordingtheprinciplethat
Discussion & Takeaways Although intuitively
theyarefromdifferentlanguagefamilyandhave
differenttrainingsetsize.6 Thehyperparameters new languages will benefit from fine-tuning the
PLMontheXL-Sumdataset,thepracticalresults
usedtofine-tunePLMisthesameastheMultilin-
showthatnotalllanguagesobtainimprovements.
gualtrainingofXL-Sum(Hasanetal.,2021).
Transferlearninginsuchawaymightcausecatas-
Werefertomt5-baseastheoriginalPLMwithout
trophicforgettingofthepreviouslyacquiredknowl-
fine-tuningandmt5-base34asthefine-tunedver-
edgeinPLM(McCloskeyandCohen,1989;San-
sion on 34 languages. We then performed prefix-
toroetal.,2016). Iftherearenotenoughtraining
tuning and adapter-tuning on mt5-base and mt5-
samplesofacertainscriptduringPLMfine-tuning,
base34for11left-outlanguages. Hyperparameters
thePLMmightlosetheabilitygeneralizingtolan-
usedforthesetuningmethodsremainthesameas
guagesofthisscriptbyparameter-efficienttuning
thoseinnon-fewshotexperimentsofSec.(3.1).
methodsandfreezingPLM.Thisindicatesthatthe
3.2.2 ResultsandAnalysis effectivenessofparameter-efficienttuningmethods
Results Table.1 illustrates the performance of undermulti-lingualscenariosishighlydependent
multi-lingualmodelsmt5-baseandmt5-base34to onthemulti-lingualmodelweuseandundersome
adapt to 11 new languages by prefix-tuning and situations, parameter-efficient tuning might lose
adapter-tuning. ThemainobservationsinTable.1 their adaptivity. We leave how to alleviate this
areasfollows: problemforfuturework.
(1) For four languages, Amharic, Igbo, Scottish
Gaelic
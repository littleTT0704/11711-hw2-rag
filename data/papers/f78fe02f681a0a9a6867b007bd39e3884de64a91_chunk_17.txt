ivenandroleinstructioni30% GroundTruth 43% 45% 46% 45%
and50%ofthetime,respectively. COSMO-3B 57% 55% 54% 55%
Table5: Resultsofhead-to-headhumanevaluationbe-
5 Generalizabilityof COSMO
tween model responses on an unseen dataset: Daily-
WecompareCOSMOtootherconversationalagents Dialog(Lietal.,2017)(ยง5.1). Thedifferencesareall
statisticallysignificantwith|z|>12.45andp<0.05,
onsocialconversationdatasetsunderbothout-of-
exceptfortheSpecificinthebottomrow.
domain and in-domain settings. Since automatic
responseevaluationisbrittle,wefocusonhuman
situationswithemotions. Table5summarizesthe
evaluation(Smithetal.,2022). Automaticevalua-
head-to-headcomparisonresultsoftheresponses
tionresultsviaGPT-4areinAppendixD.
fromCOSMOandothermodels. AlthoughCOSMO
istrainedonsignificantlysmalleramountofdata
Baselines We compare COSMO with four best-
(1.5Mdialoguesvs. 1.5BRedditcomments,551M
performing stand-alone conversation models:
Reddit threads) and is significantly smaller (3B
BlenderBot-1(Rolleretal.,2021),GODEL(Peng
vs. 7B), it outperforms all other existing mod-
etal.,2022),Koala(Gengetal.,2023),andVicuna
els with a significant margin across all aspects.
(Chiangetal.,2023). BlenderBotisatransformer
pretrained on 1.5B Reddit comments and trained
Specifically,COSMOdemonstratesthelargestper-
formancegapintermsofnaturalness. Itisworth
on various chitchat datasets. GODEL utilizes a
notingthatwhileKoalaandVicunafocusonpro-
pretrainedlanguagemodelT5(Raffeletal.,2020)
vidinginformativeresponses,theseresultssuggest
trainedonwebtextdata,andfurthertrainson551M

ally, partial code, although potentially useful, withhumanpreferencethanallpriormetrics. Fur-
may not parse, thus cannot be fully evaluated by ther, we show that generated code that receives a
CodeBLEU.Further,ashighlightedbysubsequent higher score by CodeBERTScore is more likely
studies (Wang et al., 2022), CodeBLEU does not to function correctly when executed. Finally, we
release five programming language-specific pre- evaluatingencoder-decoderordecoder-onlyasthe
trained models to use with our publicly available NL→Codegeneratormodels.
code. These models were downloaded more than Another point to consider is that Code-
1,000,000 times from the HuggingFace Hub. Our BERTScore relies on a strong underlying BERT-
code and data are available at https://github.com/ basedmodel,whilemethodssuchasBLEUdonot
neulab/code-bert-score. have many “moving parts” or hyperparameters to
tune. However, this is mostly an advantage, since
Acknowledgement
CodeBERTScore can be further improved in the
futureusingstrongerbasemodels.
WethankMishaEvtikhiev, EgorBogomolov, and
Timofey Bryksin for the discussions, and for the
data from their paper (Evtikhiev et al., 2022). We
References
thankanonymousreviewersforthevaluablefeed-
back. We are grateful to Yiwei Qin for the dis- Loubna Ben Allal, Raymond Li, Denis Kocetkov,
cussions regarding the T5Score paper (Qin et al., Chenghao Mou, Christopher Akiki, Carlos Munoz
Ferrandis, Niklas Muennighoff, Mayank Mishra,
2022); the idea to use functional correctness as
Alex Gu, Manan Dey, et al. 2023. Santa-
a meta-metric was born thanks to the discus- coder: don’t reach for the stars! arXiv preprint
sion with her. We are also grateful to Aryaz arXiv:2301.03988.
Eghbali and Michael Pradel for the discussions
MiltiadisAllamanisandCharlesSutton
otheeffectfromlabelsmoothingmethods(Szegedyetal.,2016;Pereyra
etal.,2017;Meisteretal.,2020a). However,webelievethatthisexplanationisnotsatisfactory. Ifthekeyis
trainingwithsoft-labels,whydothesesoftlabelsmustbeprovidedspecificallybyakNNsearch? Ifsoftlabels
werethekey,thensoft-labeltrainingwherethelabelscomefromthebaseLMitselfshouldhaveworkedas
well. ToseparatetheeffectofsoftlabelingfromthekNN’sadditionalguidance,wetrainanotherLMwiththe
samemodelarchitectureasthebaseLM,withthesoftlabelsfromthebaseLM.Thisteacher-studenttraining
istodistilltheknowledgefromthebaseLM(Hintonetal.,2015). Wefindthatbyjusttrainingwith“soft
labels“fromthebaseLMtoalleviatethealleged“over-correction”problemisnotthekey,asthisdoesnothelp
withtheinterpolatedperplexityatall. Thissuggeststhatevenwiththesametrainingdata,kNNstillprovides
valuableadditionalguidance.
E.6.3 TrainingtoOptimizeInterpolatedLoss
InSection4.2,wediscoverthatusingover-parameterizationwithstandardLMtraininglossdoesnotfurther
closethegaptowardskNN-LM.Thissuggeststhatsomeregularizationtermmaybeneededduringtrainingto
makethemultipleembeddingsnotconvergetothesamevector,renderingover-parameterizationuseless.
FromTable2,weseethatabetterinterpolatedperplexitymaynotrequireaverylowperplexitywhenmeasured
onlywiththeextrainputrepresentation. However,westilluseastandardLMlosstoonlytraintheadditional
embedding matrix, that directly minimizes the perplexity using only the extra input representation. This
discrepancybetweentrainingandtheevaluationwithinterpolationsuggeststhattrainingwithanalternative
lossfunctionthatinterpolatesthebaseLM’soutputwiththeoutputusingtheextrainputrepresentationmay
bebeneficial.
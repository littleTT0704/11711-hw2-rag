putational Linguistics and the 11th International
MethodsinNaturalLanguageProcessing(EMNLP),
Joint Conference on Natural Language Processing
pages7654–7673,Online.AssociationforComputa-
(Volume 1: Long Papers), pages 293–305, Online.
tionalLinguistics.
AssociationforComputationalLinguistics.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Hiroaki Hayashi, and Graham Neubig. 2021. Pre-
WeiLi, andPeterJLiu.2019. Exploringthelimits
train, prompt, and predict: A systematic survey of
of transfer learning with a unified text-to-text trans-
prompting methods in natural language processing.
former. arXivpreprintarXiv:1910.10683.
arXivpreprintarXiv:2107.13586.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey Lavie. 2020. Comet: A neural framework for mt
Edunov, Marjan Ghazvininejad, Mike Lewis, and evaluation. arXivpreprintarXiv:2009.09025.
Luke Zettlemoyer. 2020. Multilingual denoising
pre-trainingforneuralmachinetranslation. Transac- AlexanderM.Rush,SumitChopra,andJasonWeston.
tions of the Association for Computational Linguis- 2015. A neural attention model for abstractive sen-
tics,8:726–742. tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan- Joint Conference on Natural Language Processing
guageProcessing,pages379–389,Lisbon,Portugal. (Volume1: LongPapers),pages5751–5767.
AssociationforComputationalLinguistics.
DevendraSachanandGrahamNeubig.2018. Parame-
tersharingmethodsformultilingualself-
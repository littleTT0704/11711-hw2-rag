thanoriginalstories.NAREORcan thebeginningofthesecondoutputsentence,correctlyand
indeedbeusedtogeneratemoreinterestingstoryvariations. unambiguously conveying its underlying temporality w.r.t.
thefirst.BART-dcorrectlychangessawaturtletohadseen
5.2 AnalysisofAutomaticEvaluationResults aturtle,whileBART-rdoessoforstepped tohadstepped.
Forex.4,BARTandT5modelsallresolvetheDisneyland
Wenowanalyzetheautomaticevaluationperformanceofthe
ellipsis by converting Joey had a great time to Joey had a
differentmethodsinTable4.
greattimeatDisneyland,whileGPT2-dcannot.
BERTScore,BLEU,METEOR: WeseefromTable5that However,theBARTandT5modelsareimperfect.Forex.
thesereference-basedmetricscorrelatequitewellwithhuman 1, BART-r hallucatines lost his wallet (original story does
evalmetrics,particularlyplot-pres.T5-d-2Sperformsbest notinvolveawallet),T5-dinsertsanincorrecttimexofSoon
followed by BART-d-2S. Similar to the human evaluation, after at the beginning of the second output sentence, and
2Smodelsoutperformtheir1Svariants,andGPT-2models T5-r hallucinates asked if he had a soda (this is not asked
performworstoverall.Denoiseoutperformsreordervariants intheoriginalstory).Forex.2,BART-rincorrectlyconverts
andgeneratemoresimilartext,onavg,tohumanreferences. thebirdnolangersangtoFrednolongersang,likelydue
tocoreferencedifficulties.Forex.3,T5-rdoesnotconvert
9Although these metrics slightly decrease for reordered sto- SuddenlytoEarlierlikeBART-d,givingafalseinterpretation
ries,wenotethatNAREOR’smainpurposeisformoreinteresting thatEricslippedafterhisrescuer’sarrival.BART-rdoesnot
tellingsofthesamestorywhichwedo
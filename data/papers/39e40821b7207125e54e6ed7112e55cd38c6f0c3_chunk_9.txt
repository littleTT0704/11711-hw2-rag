-LLMs – turedependencybetweentheactions(Figure1a).
languagemodelstrainedonnaturallanguage We use the PROSCRIPT (Sakaguchi et al., 2021)
corpus. We experiment with the latest ver- dataset, where the scripts are directed acyclic
sions of CURIE (text-curie-001) and graphs,whichwerecollectedfromadiverserange
DAVINCI (text-davinci-002), the two of sources including ROCStories (Mostafazadeh
GPT-3 based models by OpenAI (Brown etal.,2016),Descript(Wanzareetal.,2016),and
et al., 2020). For both these models, the Virtualhome(Puigetal.,2018).
prompt consists of (T,G) examples, where Let G(V,E) be a script for a high-level goal
G is simply flattened into a string (as in Fig- T with node and edge sets V and E, respectively.
ure 1c). DAVINCI is estimated to be much FollowingSakaguchietal.(2021),weexperiment
largerinsizethan CURIE,asourexperiments with two sub-tasks: (i) script generation: gen-
also reveal (Appendix A). DAVINCI, popu- erating the entire script G(V,E) given a goal T,
larly known as GPT-3, is the strongest text- and(ii)edgeprediction: predictingtheedgesetE
generationmodelavailablethroughOpenAI giventhenodesV andthegoalT.
APIs.2 Figure 1 shows an input-output example from
PROSCRIPT,andourconversionofthegraphinto
2. Fine-tuning: wefine-tunea T5-largemodel Pythoncode: weconverteachnodev ∈ V intoan
for EXPLAGRAPHS,andusetheresultsfrom instance of a Node class; we create the edges by
Sakaguchi et al. (2021) on T5-xxl for PRO- addingchildrenattributeforeachofthenodes.
SCRIPTtasks. Incontrasttothefew-shotsetup Additionalexamplesarepresentin
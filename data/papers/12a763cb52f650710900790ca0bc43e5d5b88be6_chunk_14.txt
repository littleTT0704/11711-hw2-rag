|q ), (2)
I q I m ure 4 shows the NumerSense performance gain
0≤m≤M
whenusingdifferentsizesofGPT-3astheknowl-
(cid:89)
PoE:p I(a|q,K q) ∝ p I(a|q m). (3) edge generation model. On top of the T5-11b in-
0≤m≤M ferencemodel,The6.7Bknowledgemodelgives
3159
Figure5: Humanevaluationofgeneratedknowledge. Left: Percentageofgoodknowledgestatementsalongeach
axis. Right: Agreementbetweenhumanandmachineonhelpfulnessofselectedknowledge.
a5.0%improvement,narrowerthanthe10.5%im- Results. Figure 5 summarizes the results. The
provement given by the 175B knowledge model. vastmajorityofselectedknowledgearegrammati-
The1.3Band0.4Bknowledgemodelsdonotgive calandrelevanttothequestion,and83%ofthem
a significant improvement. Therefore, we do not arefactuallycorrect. 72%areseenasbeinghelpful
necessarilyneedthelargestversionofGPT-3asthe for answering the question according the human
knowledgesource,thoughwedoneedthemodelto evaluators, whereas 13% are harmful. Out of the
berelativelylargeinordertogenerateusefuland knowledge statements that rectify the model pre-
reliableknowledge. dictions,93%arelabeledashelpfulbythehuman
evaluators;incontrast,whentheknowledgestate-
4.4 HumanEvaluation ment misleads the model, only 21% are labeled
as helpful, and 39% harmful. Of the knowledge
We conduct a human evaluation on NumerSense
deemedhelpfulbyhumanand rectifiesmodelpre-
andQASCtostudythequalityofgeneratedknowl-
diction, 95% are factual, while of those deemed
edgeandtheinterpretabilityofitsimpactontask
harmfulbyhumanand misleadsmodelprediction,
performance.
86% are non-factual, suggesting that improving
knowledgefactualityisapromisingpathtowards
Evaluation.
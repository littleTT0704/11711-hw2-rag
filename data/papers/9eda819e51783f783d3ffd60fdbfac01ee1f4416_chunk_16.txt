 English with the highest training
7WeusetheresultsofXL-Sum(Hasanetal.,2021). In
set size of 300,000. This means the parameter
order to have a fair comparison and remove the influence
of different rouge packages, we use their model-generated efficientadapterisareasonablesubstituteofPLM
outputs on test set to calculate the rouge score, instead of
fine-tuningregardlessofthetrainingsetsize.
usingtheirreportedrougescoredirectly.
8Onethingtonoticeisthatduringtraining,ineachiteration
wesamplefromonelanguagewithacertainprobability.The MPF v.s. PLF: Multi-lingual model MPF
LMsharedbyalllanguagesistunedeveryiteration,whilethe
significantly outperforms the baseline PLF in
privateparametersforeachlanguagearetunedwheneverthe
languageissampled. the low- and medium-resource languages with
the gain decreasing as the training set size exemplified by mBERT (Devlin et al., 2019),
becomes larger. This is expected because low- XLM-R(Conneauetal.,2020),XLM-R(Conneau
and medium-resource languages can benefit etal.,2020),whichadoptmaskedlanguagemodel
from joint training by positive transfer between paradigm, and mBART (Liu et al., 2020), mT5
sisterlanguages(LampleandConneau,2019). A (Xue et al., 2021), which utilize a sequence-to-
deteriorationisalsoobservedinthehigh-resource sequenceframework. However,afewworkshave
languages. However, the losses are relatively focusedonmultilingualsummarizationgiventhe
minor; the multilingual model is within a -1 lackofbenchmarkdatasetsforotherlanguagesex-
R2 drop for 6 high-resource languages and a ceptEnglish. (Giannakopoulosetal.,2015)bench-
-3 R2 drop for English. This indicates that by markedsummarizationsystemsover40languages,
training a single multilingual model, the low- withlimitationofdatasetscalehaving10ksamples
and medium-resource languages have been intotal. (Scialometal.,
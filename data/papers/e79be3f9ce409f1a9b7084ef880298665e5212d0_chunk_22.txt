withstate-of-the-art
Wecomparewithpreviousworksunderthreeprotocols: 1)
tions. As we can see, solely using L 1 (row 1) or L 2 (row trainingandevaluatingonseparatedatasets; 2)pretraining
2)forcontrastivelearningresultsinsub-optimalvideo-text on Howto100M and evaluating zero-shot performance and
alignment. Simply combining them together (row 3) im- 3)finetuningpretrainedmodelonseparatedatasets.
proves the performance on two datasets. This implies that Results on separate datasets. We separately show the
different levels of contrastive learning can be complemen- comparisonson YouCook2, MSR-VTT andActivityNet in
tarytoeachother,whichsupportsourearlierhypothesisthat Table5,6and7.Forafaircomparisonwithpreviousworks,
thesetwolossesaresynergisticwitheachotherforabetter we use the same or similar features as listed in the tables.
video-textalignment.Whenincorporatingthehardnegative As we can see, our method outperforms all previous work
mining via our cascade sampling strategy (row 4), it fur- acrossalldatasets. Theseresultsvalidatesitseffectiveness
therimprovestheperformance. Finally,wecanseeadding tolearnvideo-textalignment. Notethatpreviousworksei-
token-levelcontrastivelossL 3canfurtherimprovetheper- theruseavarietyoflossfunctions[33,28]oracollectionof
formanceacrossallsettings(row5). multiplefeatures[31,14]. Incontrast, weachievethebest
TokensofInterest. Wefurtherstudytheeffectofdifferent performance using a simpler contrastive learning pipeline
tokens of interest on the model performance. By default, with smaller model size. This supports our earlier claim
our model uses the noun and verb as the tokens of inter- on the efficiency. Comparing the numbers in Table 2, Ta-
esttocomputethetoken-levelcontrastloss. Here,wevary ble5andTable6,wecanfindourmodelachievesbetterper-
themtoothertypessuchasadposition(adp)anddetermin
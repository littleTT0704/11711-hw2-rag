 distribution(OOD)tasks. [64]proposestodisentanglethe
promotessemanticinvarianceviaaconstrainedoptimiza- semanticlatentvariablesandthedomainlatentvariablesfor
tionsetup. Thisisdonewithouttheusageofadversarial strongergeneralizationperformanceindomainadaptation.
training and domain labels. Moreover, there is no addi- [54]showsthatexistingdisentangledlearningmodelsare
tionalcomputationaloverheadformodelingvariations. notsufficienttosupportcompositionalgeneralizationand
â€¢ Ourframeworkcanbeviewedasacontrollableandinter- extrapolation while hypothesizing that the richness of the
pretabledatagenerationparadigmforDG.Datamanipu- training domain matters more. However, previous works
[19,31,39]arelimitedtosingle-dimensionallatentcodes Our formulation generally follows prior works of PAC
anddevelopedwithdifferentpurposeslikegenerationand constrainedlearning[15,16,58,60],butweuseamoreflex-
interpretability. Thustheyarehardtoscalewellbeyondtoy ibleparameterizationandderiveanewalgorithmtosolve
datasets and adapt to complicated DG tasks [54]. In con- theresultingconstrainedoptimizationproblem. Morespecif-
trast,weharnessthedisentangledeffectstolearninvariant ically, we emphasize that DDG, motivated by an analysis
representationsforrealisticOODgeneralizationtasks. of the multi-source domain adaptation upper bound (Ap-
Data Augmentation. The diversity of the training dis- pendixB),requiresnodomainlabelsandpre-traineddomain
tribution is of great importance in improving DG perfor- transformation models during training. DDG can also be
mance[2,27,29,75,82]. Dataaugmentationisaneffective trained in an end-to-end manner, yielding a more flexible
waytoincreasedatadiversity[79]anditcanthereforeim- andpotentiallybettersolution.
proveOODgeneralizationaswellasrobustnesstospurious
3.1.Formulation
correlations [4, 38]. In particular, [14] devises an
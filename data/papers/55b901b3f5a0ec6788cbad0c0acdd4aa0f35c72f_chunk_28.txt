ingSystems,33:21271–21284,2020.
SuchinGururangan, AnaMarasovic´, SwabhaSwayamdipta, KyleLo, IzBeltagy, DougDowney,
andNoahASmith. Don’tstoppretraining: adaptlanguagemodelstodomainsandtasks. arXiv
preprintarXiv:2004.10964,2020.
DanijarHafner,TimothyLillicrap,IanFischer,RubenVillegas,DavidHa,HonglakLee,andJames
Davidson. Learninglatentdynamicsforplanningfrompixels. InInternationalconferenceon
machinelearning,pp.2555–2565.PMLR,2019.
MoritzHardt,BenRecht,andYoramSinger. Trainfaster,generalizebetter: Stabilityofstochastic
gradientdescent. InInternationalConferenceonMachineLearning,pp.1225–1234.PMLR,2016.
AndrewHoward,MarkSandler,GraceChu,Liang-ChiehChen,BoChen,MingxingTan,Weijun
Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1314–1324,
2019.
Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer
learning? arXivpreprintarXiv:1608.08614,2016.
MandarJoshi,DanqiChen,YinhanLiu,DanielSWeld,LukeZettlemoyer,andOmerLevy. Spanbert:
Improvingpre-trainingbyrepresentingandpredictingspans. TransactionsoftheAssociationfor
ComputationalLinguistics,8:64–77,2020.
David Jurgens, Srijan Kumar, Raine Hoover, Dan McFarland, and Dan Jurafsky. Measuring
the evolution of a scientific field through citation frames. Transactions of the Association for
ComputationalLinguistics,6:391–406,2018.
Johann
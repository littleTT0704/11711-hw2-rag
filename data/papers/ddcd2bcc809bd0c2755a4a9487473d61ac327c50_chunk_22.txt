idemultiplepossibleanswers;Ouyangetal.,
“Solving” a ToM benchmark is necessary but
2022,p.17). Thisoftenleadsthemtoprovideratio-
notsufficient Methodologically,ifamodelfails
nalesforeachgivenanswerwithoutcommittingto
atleastoneToMtask,itdoesnothaveToMingen-
actuallyansweringthequestion. Buthumansmight
eral. Successononeexampleortaskisnotasound
fall prey to confirmation bias and simply see the
proof that a model has ToM. Future work will
rightansweranditsrationalandconcludethatthe
needtocontinuetodevelopbenchmarkstesting
model has gotten it correctly. We thus argue that
variousToMaspects,andthesebenchmarkswill
inordertoconcludewhetheracertainmodelpos-
needtobedesignedtoassessLLMsdirectlyrather
sessesacertainability,itiscrucialtoquantifythe
thanusingclinicaltestsdesignedforhumans.
performance across multiple large-scale datasets,
Additionally, reporting the aggregated perfor-
preferablyusinganautomaticevaluationmethod.
manceofLLMsonbenchmarksobscurestheper-
Usingpsychologicaltestsdesignedforhumans formancedifferencesacrossquestionsofdifferent
onLLMs Inclinicalpsychology,testsdesigned typesandcomplexities. Toovercomethis,oneap-
forhumansarecarefullyconstructedandvettedto proach is to pair a difficult question with an easy
ensurethattheyhaveexternalandinternalvalidity, question,requiringmodeltoanswerbothcorrectly.
i.e.,thattheyaremeasuringwhattheyaimtomea- Thismethodologyresemblesthe“jointscore”em-
sure(Franketal.,2023). Whilethereisevidence ployedinFauxPas-EAI,Adv-CSFB,andToMi. In
that a person’s success in one ToM task can indi- situations where pairing is challenging, a recom-
catetheirToMabilities(e.g.,Milliganetal.,2007), mendationforfutureworksisthatdatasetdifficulty
thisdoesnotnecessarilytransfertomod
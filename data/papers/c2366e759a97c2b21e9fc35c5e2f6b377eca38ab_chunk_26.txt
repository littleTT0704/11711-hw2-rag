ConferenceonRobotLearning,
pages1259–1268.PMLR,2022.
[58] K. Lee, L. Smith, A. Dragan, and P. Abbeel. B-pref: Benchmarking preference-based rein-
forcementlearning. arXivpreprintarXiv:2111.03026,2021.
[59] X. Liang, K. Shu, K. Lee, and P. Abbeel. Reward uncertainty for exploration in preference-
basedreinforcementlearning. arXivpreprintarXiv:2205.12401,2022.
[60] W. B. Knox, S. Hatgis-Kessell, S. Booth, S. Niekum, P. Stone, and A. Allievi. Models of
humanpreferenceforlearningrewardfunctions. arXivpreprintarXiv:2206.02231,2022.
[61] P.F.Christiano,J.Leike,T.Brown,M.Martic,S.Legg,andD.Amodei. Deepreinforcement
learning from human preferences. Advances in neural information processing systems, 30,
2017.
[62] Y. Chen, Y. Liu, L. Dong, S. Wang, C. Zhu, M. Zeng, and Y. Zhang. Adaprompt: Adaptive
modeltrainingforprompt-basednlp. arXivpreprintarXiv:2202.04824,2022.
[63] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.
Zero-shottext-to-imagegeneration. InInternationalConferenceonMachineLearning,pages
8821–8831.PMLR,2021.
[64] C.Raffel,N.M.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.
Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv,
abs/1910.10683,2020.
[65] M. Lewis
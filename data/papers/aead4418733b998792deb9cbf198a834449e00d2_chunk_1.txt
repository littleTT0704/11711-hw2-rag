TheThirty-SixthAAAIConferenceonArtificialIntelligence(AAAI-22)
Symbolic Brittleness in Sequence Models:
on Systematic Generalization in Symbolic Mathematics
SeanWelleck,1,2 PeterWest,1 JizeCao,1 YejinChoi1,2
1PaulG.AllenSchoolofComputerScience&Engineering,UniversityofWashington
2AllenInstituteforArtificialIntelligence
wellecks@uw.edu
Abstract Input Integral Prediction
Neural sequence models trained with maximum likelihood 30cos(39x) 10sin(39x) 10sin(39x) ✓
13 13
estimation have led to breakthroughs in many tasks, where 17cos(83x) 17sin(83x) 1 sin(83x) ✗
successisdefinedbythegapbetweentrainingandtestper- 83 17
34cos(77x) 34sin(77x) sin(77x) ✗
formance.However,theirabilitytoachievestrongerformsof 77
generalizationremainsunclear.Weconsidertheproblemof
x209 1 x210 1 x210 ✓
symbolic mathematical integration, as it requires generaliz- 210 210
ingsystematicallybeyondthetestset.Wedevelopamethod- x764 1 x765 1 x765 ✓
765 765
ology for evaluating generalization that takes advantage of x209+x764 1 x210+ 1 x765 1 x205 ✗
the problem domain’s structure and access to a verifier. 210 765 205
Despite promising in-distribution performance of sequence- −241 −241x −239x−14400 ✗
to-sequence models in this domain, we demonstrate chal- 123x 123x 123x ✗
lenges in achieving robustness, compositionality, and out- log(123) 1+log(123)
of-distribution generalization, through both carefully con- 4x+x465+1 x466 +x+ 4x x466 +x+ex ✗
466 log(4) 466
structed manual test suites and a genetic algorithm that au-
tomaticallyfindslargecollectionsoffailuresinacontrollable
Table1: Despiteitsimpressiveabilitytointegrateequations
manner.Ourinvestigationhighlight
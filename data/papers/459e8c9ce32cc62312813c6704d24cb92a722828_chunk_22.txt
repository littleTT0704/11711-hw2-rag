.26 36.51 64.13 47.35 34.98 35.97 62.49 50.29
MGS-EDIT 35.73 36.42 63.73 46.83 34.73 35.95 62.04 49.45
MGS-SBLEU(train) 36.19 36.13 63.65 48.40 34.80 35.32 61.95 51.38
Table4: Machinetranslationresults(IWSLT‘14De!En).BLEUiscomputedwithbeamsearch(width5).SBLEU,METEOR,
andEDITarecomputedwithgreedydecodingtomatchthetrainingconditions.
Valid Test
W&S(2020)(MLE) - 34.70
W&S(2020)(MRT) - 35.20
Ed.(2018)(MLE) 33.11 32.21
Ed.(2018)(MRT) 33.55 32.45
Table5: IWSLT‘14De!Enwithminimumrisk(BLEU).
Weattributethistothevariationsinweightbetweenthecan-
didates,whicharesmallerthanthoseinthetextcompletion
task,withastandarddeviationrangingfrom.005to.025over
thecourseoftraining.
The task losses used in MT are highly concentrated on
matching areference translationand are similarto the 0-1
losstowhichthelogloss(MLE)isaproxy.Wesuspectthatit Figure4:Proportionofhighest-weightMLEcandidates.
ismoredifficulttofindcandidatesthatimprovesubstantially
over MLE, resulting in smaller improvements than in text
completion. likelihoodgradientintoitsobjective,whichledtosolutions
thatweremorestablewithrespecttoperplexitythanthose
foundbypolicyandminimumrisktraining,whichrequired
6 Conclusion
MLEasanauxiliarylossinpractice.Theresultssuggestthat
Weproposemaximum-likelihoodguidedparametersearch MGSisapromisingalternativetominimumriskandpolicy
(MGS), a training method for optimizing an arbitrary gradient,andimprovinguponitssimple,yeteffective,form
sequence-level task loss. MGS samples update directions ofexplorationisafruitfuldirectionforfuture
 al., 2021). Deep ML architectures can
extract even more abstract, high-level representations (Aytar et al., 2016; Hershey et al.,
2017; Cramer et al., 2019). Purely randomly weighted architectures impose particular in-
ductive biases on data and can do better than hand-crafted baselines (Saxe et al., 2011;
Pons and Serra, 2019). However, it is more common to train these architectures.
Architectures The architecture of the model typically includes an encoder to transform
the input, and can optionally also include temporal modelling to capture context, and/or
a generative decoder. A common encoder architecture uses Convolutional Neural Net-
works (CNN) applied to a 2-D input (Hershey et al., 2017; Cramer et al., 2019), or directly
to the 1-D audio signal (van den Oord et al., 2016; Baevski et al., 2020). Temporal con-
text modelling is often achieved via Recurrent Neural Networks (RNN) (Merhi et al., 2017;
Kalchbrenner et al., 2018), or Transformers (Baevski et al., 2020). The latter, in particu-
lar, have achieved strong results for audio classification (Gong et al., 2021a), though they
are costly to train from scratch. Koutini et al. (2021) (ยง4) demonstrate a faster training
approach for audio transformer, which requires two GPU-days to pretrain on AudioSet. In
3
Turian et al.
reaction to the use of transformers, all-MLP architectures have demonstrated competitive
results on language and vision tasks (Liu et al., 2021; Tolstikhin et al., 2021).
Training regimes Models can be trained on a (large-scale) supervised task, such as
ImageNet (Deng et al., 2009) for vision and AudioSet (Gemmeke et al., 2017) for audio.
Multitask supervised training can further improve generalization (Pascual et al., 2019).
To avoid the need for human-labeling, self-supervised models (a form of unsupervised
learning) learn from large-scale unlabeled corpora. Many self-supervised approaches learn
to correspond the original input with a different view on that same input, such as a se-
m
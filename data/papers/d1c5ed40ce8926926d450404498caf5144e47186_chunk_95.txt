 collection of 7,500 randomly selected Wikipedia articles.
Tokens were dropped or replaced with probabilities ranging from 0 to 1 (in 0.1 in-
crements). In each step, we recomputed the features that depend on the seeds (Top-
icRatioSeed, TFIDFSeed and CosineSim) using the degraded seed content, and we
evaluated the ranking performance of the logistic regression model with features of
adjacent instances (LR Adjacent) and the Cosine Sim baseline. LR models were eval-
uated through 12-fold cross-validation on the topics in Table 5.1 marked as CV. The
baseline does not require training, and thus we simply ranked the text nuggets for
each of the 12 topics by their cosine similarity scores in descending order.
Figure 5.6 illustrates how the length of the seeds impacts the two ranking ap-
proaches, and Figure 5.7 shows how their performance is affected by noise in the
seed documents. It can be seen that both methods are more effective for long, high-
quality seeds. If few tokens are dropped or replaced, relevance ranking performance
remains almost constant, which indicates that both approaches are relatively robust.
It can also be seen that replacing seed content with random noise hurts performance
somewhat more than removing the content.
The logistic regression model consistently outperforms the cosine similarity base-
line, and the performance gap widens if seeds are very short or noisy. This makes
sense because the LR model does not solely rely on the seed content but leverages a
host of other features that are based on the query used to retrieve related content,
the search results and the surface forms of the text nuggets. Even if the seed content
is replaced or dropped entirely, the LR model still performs at 67% MAP, whereas
the performance of the Cosine Sim baseline degrades to about 43% MAP. This is
similar to the Search Rank baseline because we used this strategy as a fallback to
rank nuggets if the cosine similarity scores were not unique or could not be computed
because the whole seed content was dropped.
We simulated noise by sampling tokens at random from a background document
collection. In practice, however, the noise may not be random but there can be a bias
towards specific topics. For instance, articles in an encyclopedia often contain lists
of references and links, and the entries in a dictionary
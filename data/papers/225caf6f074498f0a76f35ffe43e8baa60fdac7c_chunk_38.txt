onMachineLearning, andRayKurzweil.2020. Multilingualuniversalsen-
pages4596–4604.PMLR. tenceencoderforsemanticretrieval. InProceedings
of the 58th Annual Meeting of the Association for
ComputationalLinguistics: SystemDemonstrations,
pages87–94,Online.AssociationforComputational
Linguistics.
ZichaoYang, ZhitingHu, RuslanSalakhutdinov, and
TaylorBerg-Kirkpatrick.2017. Improvedvariational
autoencoders for text modeling using dilated con-
volutions. InInternationalConferenceonMachine
Learning,pages3881–3890.JMLR.org.
PierreZweigenbaum,SergeSharoff,andReinhardRapp.
2018. Overviewofthethirdbuccsharedtask: Spot-
ting parallel sentences in comparable corpora. In
Proceedingsof11thWorkshoponBuildingandUs-
ingComparableCorpora,pages39–42.
Appendicesaccompanying“Beyond knowledgebasefromwhichwelookupthenearest
ContrastiveLearning: AVariational neighborofeachquestionintheNQandMKQA
GenerativeModelforMultilingual testsetsusingcosinesimilarity.
Retrieval”
A.3 BitextMining
A FullExperimentalResults
For bitext mining, we use the Tatoeba dataset in-
troducedinArtetxeandSchwenk(2019b)andthe
We include full results for our models using the
2018BuildingandUsingParallelCorpora(BUCC)
pre-trainedmT5largecheckpoint. Weevaluateon
shared bitext mining task (Zweigenbaum et al.,
Englishsemanticsimilarity,Cross-lingualsemantic
2018).
similarity,questionretrieval,andbitextmining.
TheTatoebadatasetconsistsof100-1000pairs
A.1 SemanticSimilarity ofdataalignedtoEnglishfor112languages. The
accuracy for Tatoeba can be computed in two
For English semantic similarity, we use the Se-
ways,dependingifEnglishisthetargetlanguage
mEvalsemantictextualsimilarity(STS)tasksfrom
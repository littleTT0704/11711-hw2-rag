 supervised During training, we use cross entropy loss on all 5x3
demonstration data collected from an expert policy as per generations. The first timestep in the RNN is passed the
previous work [15], [4]. current hidden state and the At each timestep the RNN
The basic unit in our model is the DREAMCELL (Fig. 2) cell is passed the prediction from the previous timestep.
which produces a sub-goal and a corresponding predicted As is common practice in the language modeling literature
imageofthearm’spositionatthenexttimestep.Thisformu- [29], we tie the emission and embedding matrix parameters.
lation allows for recurrent chaining of cells to rollout future Traditionally,thisisachievedbysimplytransposingasingle
goals and states (Fig. 1). Specifically, because the output of matrix. Our model produces tuples by passing the hidden
the DREAMCELL includes a deconvolved hallucination of vector through three different feed-forward layers, so, to
the next world state (Wˆ ) we can simply continue to run re-embed predictions we multiply by the three transposed
t+1
the network forward, where true observations are replaced embedding matrices and average the outputs to reconstruct
with the network’s predictions. Our cell has two outputs at an embedding.
every timestep t: 1. Subgoals (G ) and 2. Predicted Worlds
t
(Wˆ ).Weprovideintrinsicevaluationsonfutureprediction B. DREAMCELL World Predictor Module
t+1
performance in Section VI. The prediction cell, shown in Fig. 3, takes in the current
At inference time, we generate a task plan by rolling out hiddenstatez andthepredictedsubgoalG.Eachprediction
t t
multiple DREAMCELL timestepsintoapossiblefuturegiven modeloutputsapredictedlatent-statesubgoalzˆ,suchthat
t+1
state observations, z. The core of our approach is that these P(z,G )=zˆ.Ineffect,welearnamany-to-onemapping
t t t+1
subgoals are converted into estimated world states Wˆ and across multiple timesteps, all of which need to produce the
associated
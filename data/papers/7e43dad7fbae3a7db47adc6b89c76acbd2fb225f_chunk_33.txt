 (Wolf et al., 2020) for re-ranking. The two
like COIN (Tang et al., 2019) only contain 180
re-rankingmodelsweusedare“bert-base-uncased”
goalsand 10k videos. Inaddition,exitingworks
and “deberta-v2-large-mnli”. We finetuned each
like(Yangetal.,2021b)alsoexperimentedwitha
model on our training set for five epochs and se-
sampleddatasetofsimilarscale.
lected the best model on the validation set. Fine-
B.2 EvaluationMetrics tuningtookaroundtwohoursona2080Ti(12GB)
GPU for BERT and eight hours on a v100 GPU
We report precision@N, recall@N and mean
(32GB)for DEBERTA. Weusedthedefaulthyper-
rank (MR) following existing works on video re-
parameters provided by the transformers li-
trieval(Luoetal.,2021)
brary.
(cid:80)
M 1(r(v )<=N)
recall@N= 1 (cid:88) vj∈vgi j D Risks
M |v |
i=1
gi
M (cid:80) 1(r(v )<=N) Ourresultinghierarchycontainseventsfromwik-
precision@N=
1 (cid:88) vj∈vgi j
iHow,whichmaycontainunsafecontentthatslip
M N
i=1 throughitseditorialprocess,althoughthisisrela-
(cid:80)
M r(v )
MR= 1 (cid:88) vj∈vgi j tivelyunlikely.
M |v |
i=1
gi
(3) E LicenseofUsedAssets
where M is the number of goals in total, v is a ThewikiHowtextsusedinthisworkarelicensed
gi
setofgroundtruthvideosofgoalg istherankof underCCBY-NC-SA3.0.
i
videov and1istheindicatorfunction. FAISSislicensedunderMITLicense
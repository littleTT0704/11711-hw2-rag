and Colin Raffel. 2022. ByT5: Towards a Token-
Free Future with Pre-trained Byte-to-Byte Models.
Transactions of the Association for Computational
Linguistics,10:291â€“306.
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2021. mt5: A massively
multilingual pre-trainedtext-to-text transformer. In
NAACL.
5https://github.com/google-research/language/
tree/master/language/canine
6https://github.com/google-research/
multilingual-t5
7https://github.com/google-research/byt5
Model en ar bg de el es fr hi ru sw th tr ur vi zh avg
Zero-shot(en)
mBERT 82.0 64.1 67.5 70.4 65.5 73.7 72.8 59.3 67.4 50.2 53.2 60.2 57.5 68.7 68.1 65.4
CANINE-S 77.7 50.1 60.1 62.4 53.7 67.6 66.0 43.7 60.7 40.4 39.6 47.9 41.1 53.1 43.2 53.8
CANINE-C 77.1 53.1 61.4 63.5 58.3 68.5 66.4 47.7 63.3 41.0 39.2 48.8 44.4 53.4 39.1 55.0
mT5-Small 79.0 61.3 66.0 64.4 67.4 65.9 62.4 59.7 66.6 52.2 64.1 57.9 56.4 57.3 63.9 63.0
ByT5-Small 80.9 65.9 70.2 71.2 67.7 76.5 75.0 58.6 67.9 62.4 58.4 63.6 55.6 69.5 64.9 67.2
Single-language
mBERT 82.0 70.6 76.2 76.6 75.1 77.7 77.4 67.0 74.8 66.3 65
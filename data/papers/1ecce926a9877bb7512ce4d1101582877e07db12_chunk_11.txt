isonlyemployedduringtraining,wetruncatethegradients information.Thesingle-framefeatureisdenotedasf(cid:48).
t
beforethemultimodalfusionintheteacherblock.Theoutput
ofteacherblockisdenotedas{ftch}T.
t t=1 Visual encoder. We first project the panoramic video to
2D frames {I ···I } using ER projection and then feed
1 T
themtothebackbone.AsshowninFigure5,atransformer
and then flatten the visual feature {f }T to form F =
t t=1 encoderontopoftheResNet-50(Heetal.2016)isadoptedto
flatten(f ⊕···⊕f ) ∈ RC×THW.Afterthat,spherical
1 T mitigatetheseveredistortionintheERframe.Inaddition,a
andregularpositionalencodings(Vaswanietal.2017)are
temporalnon-localblockas(Yanetal.2019)isalsoleveraged
addedtovisualfeatureF,andacousticfeaturegsemandgloc,
toenablethetemporalinteractionontheextractedfeatures
respectively,tohelpthenetworkcapturespatialinformation.
{f(cid:48)}T from the backbone. We denote the features after
Themultimodalattentioncanbecomputedby temt pt o= r1 alaggregationas{f }T wheref ∈RC×H×W.
t t=1 t
haud =LN(MCA(F,gaud)+F) (2)
Acousticencoder. Multi-channelaudiocontainsthe3Dlo-
Faud =LN(FFN(haud)+haud) (3) cationandcategoryinformationofthesoundsourcewhich
haveagreatimpactonthechoosingofthesalientobjects.To
where MCA, FFN and LN are multi-head cross-attention extracttheacousticfeatures,weleveragethreeCNNlayers
(Ye et al. 2019), feed-forward network and layer- followedbytwobi-directionalGRUlayersasouracoustic
normalization respectively. In particular, we generate
thefollowingthreecomponents:
age features, we use ResNet50 [30]. To obtain strong rep-
Grounding The grounding module will learn a joint
resentations for language, we used BERT representations
image-language representation for each token in a se-
[15]. BERTisappliedovertheentirequestionandanswer
quence. Because both the query and the response contain
choice,andweextractafeaturevectorfromthesecond-to-
amixtureoftagsandnaturallanguagewords,weapplythe
lastlayerforeachword. WetrainR2Cbyminimizingthe
same grounding module for each (allowing it to share pa-
multi-class cross entropy between the prediction for each
rameters). At the core of our grounding module is a bidi- responser(i),andthegoldlabel. Seetheappendix(SecE)
rectional LSTM [34] which at each position is passed as fordetailedtraininginformationandhyperparameters.8
inputawordrepresentationforw,aswellasvisualfeatures
i
for o wi. We use a CNN to learn object-level features: the 6.Results
visualrepresentationforeachregionoisRoi-Alignedfrom
itsboundingregion[63,29]. Toadditionallyencodeinfor- In this section, we evaluate the performance of various
mationabouttheobject’sclasslabel(cid:96) o, weprojectanem- models on VCR. Recall that our main evaluation mode is
bedding of (cid:96) (along with the object’s visual features) into thestagedsetting(Q→AR). Here,amodelmustchoosethe
o
asharedhiddenrepresentation. LettheoutputoftheLSTM rightanswerforaquestion(givenfouranswerchoices),and
overallpositionsber,fortheresponseandqforthequery. thenchoosetherightrationaleforthatquestionandanswer
(given four rationale choices). If it gets either the answer
Contextualization Givenagroundedrepresentationof
ortherationalewrong,theentirepredictionwillbewrong.
the query and response, we use attention mechanisms to
This holistic task decomposes into two sub-tasks wherein
contextualizethesesentenceswithrespecttoeach
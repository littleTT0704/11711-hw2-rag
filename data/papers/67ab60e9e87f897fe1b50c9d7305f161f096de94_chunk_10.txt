mandsv2(fulland5hversions),NSynthPitch(50hand5hversions),andDCASE2016Task
2. Tasks are summarized in Table 1 described with more detail in Table 2 and Section A.
4. Models evaluated
Evaluated modelsaredescribedbelow. Table3summarizesmodelproperties. HEARbegan
with three strong baseline models (§4.1), each pretrained on a different audio domain. We
report on 13 external teams’ submissions to the HEAR NeurIPS 2021 shared challenge
(§4.2).
4.1. Baseline models
wav2vec2 wav2vec2 (1-D CNN and positional transformer) (Baevski et al., 2020). Self-
supervised pretraining on 100K hours of speech from VoxPopuli (Wang et al., 2021a).
CREPE 1-D CNN. Supervised pretraining of pitch-tracking on 16 hours of synthesized
music. (Kim et al., 2018b)
3. We initially believed that imposing a restriction that all submitted models must be TensorFlow 2.x
or PyTorch and pip3-installable would facilitate easy orchestration of model testing. However, models
submittedwith competingTensorFlow, CUDA,CuDNN,and pypidependencieslead ustosuggest that
futureMLchallengeorganizersstandardizeonthelateststablemicroversionofalldeeplearningpackages.
6
HEAR: Holistic Evaluation of Audio Representations
Table 1: HEAR tasks.
Speech Commands (version 2), 5h and full Spoken commands classification.
NSynth Pitch, 5h and 50h Pitch classification of synthesized sounds.
DCASE 2016 Task 2 Office sound event detection in synthesized scenes.
Beehive States Binary classification of normal vs. queen-less beehives.
Beijing Opera Percussion Classification of six Beijing Opera percussion instruments.
CREMA-D Speech emotion recognition.
ESC-50 Environmental sound classification.
FSD50K Broad-domain audio multi-labeling.
Gunshot Triangulation Identify location of microphone recording a gunshot, using
classification.
GTZAN Genre Music genre classification.
GTZAN Music Speech Classification of audio into music or speech.
LibriCount Multiclass speaker count identification.
MAESTRO 5h
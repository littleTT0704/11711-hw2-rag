 While pushing the field forward rapidly, these results also make
mastering existing ML techniques very difficult, and fall short of reusable, repeatable, and composable
development of ML approaches to diverse problems with distinct available experience.
Those fundamental challenges call for a standardized ML formalism that offers a principled framework for
understanding, unifying, and generalizing current major paradigms of learning algorithms, and for mechanical
design of new approaches for integrating any useful experience in learning. The power of standardized theory
is perhaps best demonstrated in physics, which has a long history of pursuing symmetry and simplicity of its
principles: exemplified by the famed Maxwell’s equations in the 1800s that reduced various principles of
electricity and magnetism into a single electromagnetic theory, followed by General Relativity in the 1910s and
the Standard Model in the 1970s, physicists describe the world best by unifying and reducing different theories
to a standardized one. Likewise, it is a constant quest in the field of ML to establish a ‘Standard Model’
(Domingos, 2015; Langley, 1989), that gives a holistic view of the broad learning principles, lays out a
blueprint permitting fuller and more systematic exploration in the design and analysis of new algorithms, and
eventually serves as a vehicle toward panoramic learning that integrates all available sources of experience.
This paper presents an attempt toward this end. In particular, our principal focus is on the learning objective
that drives the model training given the experience and thus often lies at the core for designing new algorithms,
understanding learning properties, and validating outcomes. We investigate the underlying connections
between a range of seemingly distinct ML paradigms. Each of these paradigms has made particular
assumptions on the form of experience available. For example, the present most popular supervised learning
relies on collections of data instances, often applying a maximum likelihood objective solved with simple
gradient descent. Maximum likelihood based unsupervised learning instead can invoke different solvers, such
as expectation-maximization (EM), variational inference, and wake-sleep in training for varied degree of
approximation to the problem. Active learning (Settles, 2012) manages data instances which, instead of being
3
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
given all at once, are adaptively selected. Reinforcement learning(Sutton & Bart
 E.,
the main idea is to offload solving and calculating to an
Radford, A., Knight, M., Brundage, M., Murati, M.,
externalPythoninterpreter,insteadofusingtheLLMfor
Mayer,K.,Welinder,P.,McGrew,B.,Amodei,D.,Mc-
bothunderstandingtheproblemandsolving. Thisresults
Candlish,S.,Sutskever,I.,andZaremba,W. Evaluating
inafinalanswerthatisguaranteedtobeaccurate,giventhe
LargeLanguageModelsTrainedonCode. arXivpreprint
correctly predicted programmatic steps. We demonstrate
arXiv:2107.03374,2021a.
this seamless synergy between an LLM and a Python in-
terpreteracross13tasksfromBIG-BenchHardandother Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,Pinto,H.P.d.O.,
benchmarks. In all these benchmarks, PAL outperforms Kaplan,J.,Edwards,H.,Burda,Y.,Joseph,N.,Brockman,
largerLLMssuchas PaLM-540B whichusethepopular G., etal. Evaluatinglargelanguagemodelstrainedon
“chain-of-thought”methodandsetsnewstate-of-the-artac- code. arXivpreprintarXiv:2107.03374,2021b.
curacyonallofthem. Webelievethattheseresultsunlock
Chen,W.,Ma,X.,Wang,X.,andCohen,W.W. Program
excitingdirectionsforfutureneuro-symbolicAIreasoners.
ofthoughtsprompting: Disentanglingcomputationfrom
reasoningfornumericalreasoningtasks. arXivpreprint
References
arXiv:2211.12588,2022.
Ahn,M.,Brohan,A.,Brown,N.,Chebotar,Y.,Cortes,O.,
Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y.,
David,B.,Finn,
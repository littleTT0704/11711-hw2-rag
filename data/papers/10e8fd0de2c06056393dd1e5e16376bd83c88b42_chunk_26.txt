2020;Rametal.,2021;Izac-
setting.
ardetal.,2022)focusesonconservativechanges
tosourcedomain,forinstancetestinggeneraliza-
7 Conclusion
tion performance of model trained Natural Ques-
tions to WebQuestions, TriviaQA – all of which Inthisworkweinvestigateddomaingeneralization
use the same Wikiepdia corpus. Another line of inopendomainquestionansweringandpresented
work follows a recently proposed retrieval bech- four main contributions. First, we analysed the
mark,BEIR(Thakuretal.,2021)thattestsgeneral- problems with existing ODQA model and inves-
izabilityofageneralpurposeretrievertodifferent tigate their failure modes. Second, we explored
corpus/domains. Moreover, it consider only re- variouszero-shotandfew-shotdatainterventions
to improve a model’s ability to generalize to an normalizationforrecurrentneuralsequencemodels
unseentargetdomain. Finally,wedescribedatax- usingacontinuousrelaxationtobeamsearch. arXiv
preprintarXiv:1904.06834.
onomyofdatasetshifttypesthatprovidesanwayto
approximatehoweffectiveasourcedomaintrained
Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.
modelcanbeadaptedtowardsanewtargetdomain. Newssummarizationandevaluationintheeraofgpt-
3. arXivpreprintarXiv:2209.12356.
References Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R Bowman, and
GeorgiosBalikas,AnastasiaKrithara,IoannisPartalas, Noah A Smith. 2018. Annotation artifacts in
andGeorgePaliouras.2015. Bioasq:Achallengeon natural language inference data. arXiv preprint
large-scale biomedical semantic indexing and ques- arXiv:1803.02324.
tion answering. In International Workshop on Mul-
timodalRetrievalintheMedicalDomain,pages26–
Gautier Iz
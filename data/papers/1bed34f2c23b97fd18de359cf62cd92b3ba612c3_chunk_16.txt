
6 ComparingtoExecution-FreeMetrics
EN ES
In this section, we study the alignment between
execution-basedevaluationandfiveexecution-free
metrics,identifyingadvantagesforbothtypes.
Model Ranking Using Different Metrics We
evaluate models using five execution-free met- JA RU
rics using lexical, syntax, and semantic matches:
BLEU (Papineni et al., 2002), ROUGE (Lin,
2004), METEOR (Banerjee and Lavie, 2005),
ChrF(Popovic´,2015),andCodeBLEU(Renetal.,
2020). Referto§D.1formoredescriptions.
Figure10: BLEUscoresonpassedandfailedsamples.
7 WhatAffectsModelPerformance?
Besides differences in model configurations, we
studythreefactorsthatmightaffectperformance.
Number of In-Context Examples Models
might benefit from example NL-Code pairs. We
thus explore the few-shot setting by prefixing
Figure9: CODEXmodelsevaluatedonsixmetrics. N ∈ {1,2,3} input-output pairs in prompts.
In Figure 11 (left), for CUSHMAN-001 and
DAVINCI-001, few-shot examples yield a clear
improvementoverthezero-shotsetting;butforthe
strongest DAVINCI-002,itbringsminimalgainsin
English. Seesimilarresultsinotherlanguagesin
§E.1.
cushman-001 davinci-001 zero one all
davinci-002 70
70
Figure 12: pass@1 when executing one or all test
60
60 cases.
50
50
JupyterNotebooks(Agasheetal.,2019)orStack-
40 40
Overflowposts(Yinetal.,2018;Wangetal.,2022),
30 0 1 2 3 30 en es ja ru butfacechallengesinenablingexecution(Laietal.,
2022; Chandel et al., 2022). Our ODEX dataset
Figure11:Left: CODEXpass@1(onEnglishset)using
addressesexecutionforopen-domaincode.
0/1/2/3-shot prompts. Right: DAVINCI-002 pass@1

0(0.0) 6.6(4.6) 3.8(1.7) 8.9(5.6) 0.5(0.2) 7.1(4.5)
+PMBoth 3.7(2.1) 10.0(7.0) 0.0(0.0) 6.9(5.1) 4.0(2.0) 9.4(6.3) 0.4(0.1) 7.0(4.3)
HUMAN - - - - - - 91.0(85.8) 94.5(87.6)
Table3: TaskandGoal-ConditionSuccess. Foreachmetric,thecorrespondingpathweightedmetricsaregiveninparen-
theses. Thehighestvaluesperfoldandmetricareshowninblue. Allvaluesarepercentages.
navigation, and are generally efficient. The path weighted language datasets focused on navigation, where sequence-
scorep formetricsisgivenas to-sequencewithprogressmonitoringperformswell[28].
s
L∗
p =s× (6) 6.1.RandomAgent
s max(L∗,Lˆ)
A random agent is commonly employed as a baseline
where Lˆ is the number of actions the model took in the in vision-and-language tasks. In ALFRED, an agent that
episode, and L∗ is the number of actions in the expert chooses a uniform random action and generates a uniform
demonstration. Intuitively, amodelreceiveshalf-creditfor random interaction mask at each timestep achieves 0% on
takingtwiceaslongastheexperttoaccomplishatask. allfolds,evenwithoutanAPIfailurelimit.
5.2.Sub-GoalEvaluation
6.2.UnimodalAblations
Completing the entire sequence of actions required to
Previousworkestablishedthatlearnedagentswithoutvi-
finish a task is challenging. In addition to assessing full
sualinputs,languageinputs,orbothperformedbetterthan
tasksuccess,westudytheabilityofamodeltoaccomplish
random agents and were competitive with initial baselines
the next sub-goal conditioned on the preceding expert se-
for several navigation
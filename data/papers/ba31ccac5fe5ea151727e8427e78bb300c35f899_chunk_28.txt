inallikelihoodtraining
9125–9135,OnlineandPuntaCana,DominicanRe- ofBiLSTM-CRFforbiomedicalnamedentityrecog-
public.AssociationforComputationalLinguistics. nitionfromdisjointlabelsets. InProceedingsofthe
2018ConferenceonEmpiricalMethodsinNatural
LeonDerczynski,KalinaBontcheva,andIanRoberts. Language Processing, pages 2824–2829, Brussels,
2016. BroadTwittercorpus: Adiversenamedentity Belgium.AssociationforComputationalLinguistics.
recognition resource. In Proceedings of COLING
2016,the26thInternationalConferenceonCompu- ChuanGuo,GeoffPleiss,YuSun,andKilianQWein-
tationalLinguistics: TechnicalPapers,pages1169– berger.2017. Oncalibrationofmodernneuralnet-
1179,Osaka,Japan.TheCOLING2016Organizing works. InInternationalconferenceonmachinelearn-
Committee.
ing,pages1321–1330.PMLR.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
2009. Active learning for statistical phrase-based
Kristina Toutanova. 2019. BERT: Pre-training of
machinetranslation. InProceedingsofHumanLan-
deepbidirectionaltransformersforlanguageunder-
guageTechnologies: The2009AnnualConferenceof
standing. InProceedingsofthe2019Conferenceof
theNorthAmericanChapteroftheAssociationfor
theNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics,pages415–423,Boulder,
ComputationalLinguistics: HumanLanguageTech-
Colorado. Association for Computational Linguis-
nologies,Volume1(LongandShortPapers),pages
tics.
4171–4186,Minneapolis,Minnesota.Associationfor
ComputationalLinguistics.
JunxianHe,JiataoGu,JiajunShen,andMarc’Aurelio
Ranzato. 2020. Revisiting self-training for neural
Timothy Dozat and
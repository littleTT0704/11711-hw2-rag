riptions[14],dealwithun-
certainty [15], and adapt to preferences [16]. Recent work [17] has shown Transformer networks
[18] can learn temporally-consistent representations, and exhibit generalization to new scenarios
[19,20,21]. Ourcentralquestionis: CanaTransformernetworklearntaskstructure,adapttouser
preferences,andachievecomplexlong-horizontasksusingnosymbolictaskrepresentations?
Wehypothesizethattaskstructureandpreferenceareimplicitlyencodedindemonstrations. When
loading a dishwasher, a user pulls out a rack before loading it, inherently encoding a structural
constraint. They may place mugs on the top rack and plates on the bottom, encoding their prefer-
ence. Learninguserpreferencesfromlong-horizondemonstrationsrequirespolicieswithtemporal
context. For example, a user might prefer to load the top rack before the bottom tray. The policy
needstoconsiderthesequenceofactionsdemonstrated,ratherthanindividualactions.Transformers
arewell-suitedtothisproblem,astheyhavebeenshowntolearnlong-rangerelationships[22],al-
thoughnotintemporalrobotictasks.WeproposeTransformerTaskPlanner(TTP)-anadaptationof
aclassictransformerarchitecturethatincludes temporal-, pose- and category-specificembeddings
tolearnobject-orientedrelationshipsoverspaceandtime.TTPgeneralizesbeyondwhatwasseenin
demonstrationsâ€“tovariablenumbersofobjectsanddynamicenvironments.Bypre-trainingTTPon
multiplepreferences,webuildtemporalrepresentationsthatcanbegeneralizedtonewpreferences.
The main contributions of our work are: (1) Introduce transformers as a promising architecture
forlearningtaskplansfromdemonstrations,usingobject-centricembeddings(2)Demonstratethat
preferenceconditionedpre-traininggeneralizesattesttimetonew,unseenpreferenceswithasingle
2202
luJ
6
]OR.sc[
1v24420.7022:viXra
Figure 1: (Left-to-Right, Top-to-bottom) A Franka Em
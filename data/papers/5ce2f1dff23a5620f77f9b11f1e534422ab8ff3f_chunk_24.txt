,Y. Mappinginstruc- language models. arXiv preprint arXiv:2212.04088,
tions and visual observations to actions with rein- 2022.
forcementlearning. arXivpreprintarXiv:1704.08795,
Stepputtis, S., Campbell, J., Phielipp, M., Lee, S.,
2017.
Baral, C., and Ben Amor, H. Language-conditioned
Nair, S., Mitchell, E., Chen, K., Savarese, S., Finn, C., imitation learning for robot manipulation tasks. Ad-
et al. Learning language-conditioned robot behav- vances in Neural Information Processing Systems,
ior from offline data and crowd-sourced annotation. 33:13139–13150, 2020.
In Conference on Robot Learning, pp. 1303–1315.
Tafjord, O. and Clark, P. General-purpose
PMLR, 2022.
question-answering with macaw. arXiv preprint
Oh, J., Singh, S., Lee, H., and Kohli, P. Zero-shot task arXiv:2109.02593, 2021.
generalization with multi-task deep reinforcement
Tellex, S., Kollar, T., Dickerson, S., Walter, M., Baner-
learning. In International Conference on Machine
jee, A., Teller, S., and Roy, N. Understanding natu-
Learning, pp. 2661–2670. PMLR, 2017.
ral language commands for robotic navigation and
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., mobile manipulation. In Proceedings of the AAAI
and Sutskever, I. Language models are unsupervised Conference on Artificial Intelligence, volume 25, pp.
multitask learners. 2019. 1507–1514, 2011.
See, A., Liu, P. J., and Manning, C. D. Get to the Yao, S., Rao, R., Hausknecht, M., and Narasimhan,
point: Summarization with pointer-generator net- K. Keep calm and explore: Language models for
works. arXiv preprint arXiv:1704.04368, 2017. action generation in text-
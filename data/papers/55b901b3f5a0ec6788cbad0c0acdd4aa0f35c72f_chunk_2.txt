 Out-of-domain No-Op Left-to-Right NextToken
andRight-to-Left
many machine learning (ML)
GPT Out-of-domain No-Op Left-To-Right NextToken
domains. In natural language XLNet Out-of-domain No-Op Randomfactorized NextToken
processing (NLP), well known Electra NeuralLMData Replace Bidirectional Real/Synthetic
models like SpanBERT (Joshi...............
et al., 2020) and RoBERTa (Liu
Figure 1: We present the dec1omposition of some auxiliary
et al., 2019b) are trained on
objectivesinNLPwithinourframework.
masked language modelling
(MLM)auxiliaryobjectives(Devlinetal.,2018)beforefine-tuningontheend-task. Andforspeech
processingandreinforcementlearning(RL),Oordetal.(2018)introducedthepopularcontrastive
predictive coding objective which achieved state of the art performance in many settings when
multi-taskedwiththeend-task. Despitethesesuccessesandmanymore,researchintodevisingsuch
objectiveshasprogressedinaverylocal,objective-by-objectivemanner(Raffeletal.,2019;Clark
etal.,2020;Grilletal.,2020;Chenetal.,2020). Auxiliaryobjectivesareconstructedbyhand-design
andwithoutmuchoverarchingstructure,relyingontheexperienceandintuitionofaselectgroup
ofresearchersversedatmakingappropriatedesignchoices. Unfortunately,thisstatus-quonotonly
createsatechnicalbarrierofentryforexploringauxiliaryobjectivesinnewdomainsbutalso,by
virtueofitsincrementalnature,limitstherateatwhichnewobjectivesarediscoveredandinvestigated.
Toaddresstheabovechallenges,thispaperpresentsaframeworkforautomaticallygeneratingand
utilizingalargesetofcandidateauxiliaryobjectives. Ourframeworkisseededbythefollowingkey
observation: leadingauxiliaryobjectivesacrossmultipledomainscanbeviewedasmakingdifferent
design decisions within a 4 stage pipeline: Input Data ( ) Input Transformation ( )
D → T →
∗Cor
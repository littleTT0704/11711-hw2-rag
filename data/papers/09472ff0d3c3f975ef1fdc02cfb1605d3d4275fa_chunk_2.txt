, even the
first clue was good enough to help us recognize all the images in our textbook.
Nowadays,deepneuralnetworkshaveexhibitedremarkableempiricalresults
over various computer vision tasks, yet these impressive performances seem un-
met when the models are tested with the samples in irregular qualities (i.e.,
(cid:63) equal contribution
0202
luJ
5
]VC.sc[
1v45420.7002:viXra
2 Huang et al.
∂L/∂zi
∂L/∂zi
∂L/∂zi
∂L/∂zi
representation and gradient
∂L/∂zi
∂L/∂zi
∂L/∂zi
∂L/∂zi
representation and gradient
∂L/∂zi
∂L/∂zi
∂L/∂zi
∂L/∂zi
representation and gradient
Fig.1. The essence of our Representation Self-Challenging (RSC) training method:
top two panels: the algorithm mutes the feature representations associated with the
highest gradient, such that the network is forced to predict the labels through other
features;bottompanel:aftertraining,themodelisexpectedtoleveragemorefeatures
for prediction in comparison to models trained convetionally.
out-of-domain data, samples collected from the distributions that are similar
to, but different from the distributions of the training samples). To account for
this discrepancy, technologies have been invented under the domain adaptation
regime [2,3], where the goal is to train a model invariant to the distributional
differencesbetweenthesourcedomain(i.e.,thedistributionofthetrainingsam-
ples) and the target domain (i.e., the distribution of the testing samples) [5,32].
Astheinfluenceofmachinelearningincreases,theindustrystartstodemand
themodelsthatcanbeappliedtothedomainsthatarenotseenduringthetrain-
ingphase.Domaingeneralization[18],asanextensionofdomainadaptation,has
been studied as a response. The central goal is to train a model that can align
the
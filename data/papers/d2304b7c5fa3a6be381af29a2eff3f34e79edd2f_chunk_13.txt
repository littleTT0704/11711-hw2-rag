odelsonthetaskofgenerat-
where[CLS]and[SEP]aremodel-specificspecial ingreactionframes(§5.3)andprovideresultsfor
tokens. Theoutputisacategoricalinference. classificationofnewsheadlines(§5.4).
5.1 Setup
4.3 Training
Wedeterminethetestsplitaccordingtodatetore-
Allourmodelsareoptimizedusingcross-entropy
ducetopicalandnewseventoverlapbetweentrain
loss,wheregenerallyforasequenceoftokenst
and test sets.13 We use the HuggingFace Trans-
formerslibrary(Wolfetal.,2020). Hyperparame-
|t|
1 (cid:88) tersareprovidedinAppendixA.3.
CE(t) = − logP (t |t,...,t ).
θ i 1 i−1
|t|
i=1 5.2 EvaluationMetrics
Here P is the probability given a particular lan- Wecomparereactioninferencesystemsusingcom-
θ
guage model θ. Since GPT-2 does not explicitly monautomaticmetrics. Wealsousehumanevalua-
distinguish between the input and output (target) tiontoassessqualityandpotentialuseofgenerated
sequenceduringtraining,wetakethelosswithre- writerintentinferences.
specttothefullsequence. ForT5wetaketheloss
5.2.1 AutomaticMetrics
withrespectonlytotheoutput.
ThesemetricsincludetheBLEU(-4)ngramover-
lapmetric(Papinenietal.,2002)andBERTScore
4.4 MaskedFine-Tuning
(Zhangetal.,2020),amodel-basedmetricformea-
To improve generalization of MRF models, we suringsemanticsimilaritybetweengeneratedinfer-
use an additional masked fine-tuning step. We encesandreferences. Forclassificationwereport
first train a language model θ on a set of Covid- macro-averagedprecision,recallandF1scores.1415
19 training examples D covid and climate training Weusepubliclyavailableimplementationsforall
ex
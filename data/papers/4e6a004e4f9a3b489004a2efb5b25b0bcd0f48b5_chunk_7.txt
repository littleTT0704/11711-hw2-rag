.Thereasoningbehindthisis,tomakesure
information from the political section is not influenced by
locationinformation.However,forbackground,wewantto
understandwhereapersonwasbornandraisedaffectstheir
politicalviewsandforthisonlythiswaskeptbutotherswere
deleted. This information, after the NER, is passed to re- Figure3:Political
movenumbersandotherirrelevantregularexpressions.This
makessurethedatabeingpassedforotherdownstreamtasks
iscleanandgivesunbiasedanswers. et al. 2019) have been state of the art since 2018 but when
itcomestoourdataset,thesemodelshaveadrawback,that
is,theirabilitytoprocesslongersequencessincethecostof
attentiongrowsontheorderofO(N2).Longformer(Beltagy,
Peters, and Cohan 2020) and other variants are useful for
this task, they accept 4096 input tokens as opposed to 512
forBERT.Itreducesmodelcomplexitybyreformulatingthe
self-attentioncomputation.TheperformanceofLongformer
againstthecurrentSOTAisrepresentedbythetablepresent
belowontherawdata.
Experiments
PreliminaryExperiments
Ourinitialexperimentswereaimedatgaininginsightsabout
patterns or trends that might be present in our data, and
Figure1:WebscrapingbasedoneachTag
alsoquestioningifpolarizationexists.Wedotheseprelimi-
naryexperimentsusingtheDoc2Vec(LeandMikolov2014)
and Word2Vec(Mikolov et al. 2013) models. The Doc2Vec
modelwasbuiltfromscratchwiththerawdata,whereeach
Wikipedia page is considered to be a document. We first
use the Doc2Vec model with K-means clustering and get
a classification accuracy of 59.52% with political data and
61.846% with background data. We then used the same
Doc2VecmodelwithbinarySVMclassifierandachievedan
accuracy of 72.872% with political data and 63.564% with
background data. These results are summarized in the ta-
ble presented below.
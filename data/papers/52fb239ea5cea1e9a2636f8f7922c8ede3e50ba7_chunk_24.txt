150 - 0.472 - 0.012 - 0.100 - 0.019 - 0.160 -
Averagescore 0.384 0.384 0.604 0.586 0.204 0.177 0.394 0.238 0.252 0.268 0.480 0.448
Table3:Evaluationsofdifferentbaselinesacross23tasksinL¯ILA.Onmosttasks,Codexoutperformsallbaselines
whileBHA¯SKARA-Poutperformsallfine-tunedbaselines. AmodelusuallyperformsworseontheOODdataset.
The bold score refers to the best score among models with the same supervision method; the underlined score
referstothebestscoreamongallmodels.GPT-3andCodexperformanceiscomputedon100uniformlydistributed
examplesowingtotheircostandusagelimit.Fine-tunedmodelperformanceiscalculatedonthefulltestset.
answeringwithT5-3B,weseeanalmost8%abso-
Neo-A Neo-P luteimprovementinF1(30.1%to37.6%). These
Dimension
IID OOD IID OOD findingsestablishBHA¯SKARAasastrongstarting
Mathability 0.191 0.129 0.445 0.188 pointforfurtherfine-tuningonnewtasks. Forthis
Language 0.189 0.147 0.429 0.246
reason,wereleaseourmulti-taskmodelforpublic
Format 0.246 0.382 0.372 0.404
Knowledge 0.206 0.143 0.331 0.213 use under the name BHA¯SKARA, with the hope
thatitwillbeusefulforfutureresearchintomath
Average 0.208 0.200 0.394 0.263
reasoningmodels.
Table 4: Multi-task models are able to generalize to
unseentasksinsomecategories.Programoutput(Neo-
5.2 Results: Category-wiseAnalysis
P)alwaysoutperformsnumberoutput(Neo-A).
In this section we discuss the trends among the
taskswithineachcategory. Forbrevity,weprimar-
ilyconsiderBHA¯SKARA,theGPT-Neomulti
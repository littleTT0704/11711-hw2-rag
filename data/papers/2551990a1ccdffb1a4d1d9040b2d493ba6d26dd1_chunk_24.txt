rosis”inthesentence“doesmicroscopyshow extendedfromtheVQA(Antoletal.,2015)dataset
coagulative necrosis of the affected bowel wall to achieve more balance between visual and tex-
andthrombosedvessels”withothernounphrases. tualinformation,bycollectingcomplementaryim-
Third, we remove the target answer phrases and agesinawaythateachquestionisassociatedwith
insert the question phrase obtained previously to a pair of similar images with different answers.
generateopen-endedquestionsbelongingtotypes IntheCOCO-QA(Renetal.,2015a)dataset,the
of“what”,“where”,“when”,“whose”,“how”,and question-answerpairsareautomaticallygenerated
“howmuch/howmany”asshowninTable5. For from image captions based on syntactic parsing
instance,wetransduce“microscopyshowscoagula- andlinguisticrules. CLEVR(Johnsonetal.,2017;
tivenecrosisoftheaffectedbowelwallandthrom- Kembhavi et al., 2017) is a dataset developed on
bosedvessels”to“whatoftheaffectedbowelwall rendered images of spatially related objects (in-
andthrombosedvesselsdoesmicroscopyshow?” cludingcube,sphere,andcylinder)withdifferent
asshowninFigure4. Giventheautomaticallygen- sizes,materials,andcolors. Thelocationsandat-
eratedquestionswhichmaycontainsyntacticand tributes of objects are annotated for each image.
semanticerrors,weperformpost-processingtofix Thequestionsareautomaticallygeneratedfromthe
thoseissues. Wemanuallyproofreadallquestions annotations.
Table6: ComparisonofVQAdatasets ciatedsideinformation(e.g.,captions,modalities)
fromtheMedPix8 databaseandgeneratequestion-
Domain #images #QApairs Answertype
answer pairs based on manually-defined patterns
DAQUAR General 1,449 12,468 Open
VQA General
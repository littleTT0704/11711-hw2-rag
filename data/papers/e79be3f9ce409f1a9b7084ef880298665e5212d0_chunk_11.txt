∈Pi
 exps(xi,y ip)/τ2+ j(cid:80) (cid:54)=iexps(xj,y ip)/τ2
insub-optimallearningsincethepairsarescarce.Wethere-
(4) foreintroduceacascadesamplingstrategytofindhardneg-
whereτ isanotherscalartemperatureparameter;P isthe ativesinsteadofrandomones.
2 i
indices of tokens of interest in i-th text, and yp is the p-th Cascade hard negative sampling. To reduce the compu-
i
token embedding in i-th text. s(·) measures the similarity tationalcostinEq.5,wechooseamongallpossiblevideo-
betweenvideofeaturesandspecifictokenembeddingyp. It textpairsasmallsubsetwhicharemostdifficult. However,
i
firstcomputesthedot-productbetweenyp ∈ R1×d andall computingthealignmentscoresforallpairsusingEq.5and
i
m video tokens x ∈ Rm×d, and then take the maximum thenselectthehardnegativesisa“chicken-and-egg”prob-
over m scores to get the final alignment score. Through lem. Instead,weproposetousethesimilaritiesbetweenall
Eq.4,themodelusesindividualtokensasanchorstoalign video-text pairs computed in Eq. 3 and Eq. 4 as the guid-
with video, which is complementary to the sentence-level ance. Specifically,foreachtext-videopair(v,t ),wetake
j i
lossinEq.3. SimilartoEq.3,wecancomputethistoken- theirglobalsimilarityx¯ ·y¯ computedin Eq.3andtoken-
j i
levelcontrastivelossefficiently,andthususealltheK −1 level similarity by aggregating (cid:80) s(x,yp) for all to-
p∈Pi j i
negativesamples. Asawhole,thesetwolossesareusedto kens of interest in t. Then we sum the
 Hillsdale,NJ:Earlbaum
modelsareunsupervisedmultitasklearners. Assoc.
GabrielA.Radvansky,AndreaK.Tamplin,JosephAr- RoySchwartz,MaartenSap,IoannisKonstas,LiZilles,
mendarez, and Alexis N. Thompson. 2014. Differ- Yejin Choi, and Noah A Smith. 2017. The effect
entkindsofcausalityineventcognition. Discourse ofdifferentwritingtasksonlinguisticstyle: Acase
Processes,51(7):601–618. studyoftherocstoryclozetask. InCoNLL.
10
Matthew Sims, Jong Ho Park, and David Bamman.
2019. Literary event detection. In Proceedings of
the57thAnnualMeetingoftheAssociationforCom-
putational Linguistics, pages 3623–3634, Florence,
Italy.AssociationforComputationalLinguistics.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2020. Mpnet: Masked and permuted pre-
trainingforlanguageunderstanding. arXivpreprint
arXiv:2004.09297.
David J. Townsend. 2018. Stage salience and situa-
tionallikelihoodintheformationofsituationmodels
duringsentencecomprehension. Lingua,206:1–20.
Ercenur Ünal, Yue Ji, and Anna Papafragou. 2019.
From event representation to linguistic meaning.
TopicsinCognitiveScience,13(1):224–242.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. Ace 2005 multilingual
trainingcorpus.
David Wilmot and Frank Keller. 2021. Memory and
knowledge augmented language models for infer-
ringsalienceinlong-formstories. InProceedingsof
the 2021 Conference on Empirical Methods in Nat-
ural Language Processing, pages 851–865, Online
and Punta Cana, Dominican Republic. Association
forComputationalLinguistics.
Timothy D. Wilson and Daniel T. Gilbert. 2008. Ex-
plainingaway:Amodelofaffectiveadaptation. Per-
Crowdsourcing Beyond Annotation:
Case Studies in Benchmark Data Collection
AlaneSuhr1,ClaraVania2,NikitaNangia3,MaartenSap4
MarkYatskar5,SamuelR.Bowman3 and YoavArtzi1
1CornellUniversity 2Amazon 3NewYorkUniversity
4UniversityofWashington 5UniversityofPennsylvania
{suhr, yoav}@cs.cornell.edu
{nikitanangia, bowman}@nyu.edu vaniclar@amazon.co.uk
msap@cs.washington.edu myatskar@seas.upenn.edu
Abstract The selection of case studies focuses on chal-
lengingsettingswherecrowdworkersareaskedto
Crowdsourcingfromnon-expertsisoneofthe writeoriginaltextorotherwiseperformrelatively
most common approaches to collecting data unconstrained work. Through these case studies,
andannotationsinNLP.Eventhoughitissuch
wediscussindetailprocessesthatwerecarefully
afundamentaltoolinNLP,crowdsourcinguse
designed to achieve data with specific properties,
islargelyguidedbycommonpracticesandthe
forexampletorequirelogicalinference,grounded
personal experience of researchers. Develop-
reasoning or conversational understanding. Each
ing a theory of crowdsourcing use for practi-
cal language problems remains an open chal- casestudyfocusesondatacollectioncrowdsourc-
lenge. However, there are various principles ing protocol details that often receive limited at-
andpracticesthathaveproveneffectiveingen- tention in research presentations, for example in
eratinghighqualityanddiversedata.Thistuto- conferences, but are critical for research success.
rialexposesNLPresearcherstosuchdatacol-
Weintroducethetaskofeachcasestudy,anddonot
lectioncrowdsourcingmethodsandprinciples
assumepriorknowledge. Wherepossible,wehigh-
through a detailed discussion of a diverse set
lightcommontrends,orotherwisekeydifferences
ofcasestudies.
betweenthediscussedcasestudies.
1 TutorialDescription
Re
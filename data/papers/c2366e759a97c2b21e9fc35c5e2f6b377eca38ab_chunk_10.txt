Additionally, to simulate a dynamic environment, we randomly initialize new objects mid-session
onthekitchencounter. Thissimulatessituationswherethepolicydoesnothavefullinformationof
everyobjecttobeloadedatthestartofthesession,andhastolearntobereactivetonewinformation.
Wetraina2-head2-layerTransformerencoder-decoderwith256inputand512hiddendimensions,
and50slotsand3iterationsforSlotAttention(moredetailsinAppendixC).Wetestbothin-and
out-of-distributionperformanceinsimulation. Forin-distributionevaluation, 10sessionsareheld-
out for testing for each training preference. For out-of-distribution evaluation, we create sessions
with unseen preferences and unseen number of objects. We evaluate trained policies on ‘rollouts’
in the simulation, a more complex setting than accuracy of prediction. Rollouts require repeated
decisions in the environment, without any resets. A mistake made early on in a rollout session
can be catastrophic, and result in poor performance, even if the prediction accuracy is high. For
example, if a policy mistakenly fails to open a dishwasher rack, the rollout performance will be
poor, despite good prediction accuracy. To measure success, we rollout the policy from an initial
stateandcomparetheperformanceofthepolicywithanexpertdemonstrationfromthesameinitial
state. Notethatthepolicydoesnothaveaccesstotheexpertdemonstration,andthedemonstration
isonlyusedforevaluation. Specifically,wemeasure(1)howwellisthefinalstatepackedand(2)
howmuchdidthepolicydeviatefromtheexpertdemonstration?
Packingefficiency:Wecomparethenumberofobjectsplacedinthedishwasherbythepolicytothat
intheexpert’sdemonstration. Leta benumberofobjectsintoprackandb beobjectsonbottom
i i
rackplacedbytheexpertintheithdemonstration. Ifthepolicyadherestothepreferenceandplaces
(cid:16)
aˆ i andˆb i onthetopandbottomrespectively,thenthepackingefficiency(PE) = (cid:80) i max(a�
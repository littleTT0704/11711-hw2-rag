18),pages578–594.
Duke, Thai Nguyen, Ramesh Chukka, Ken Shir-
AakankshaChowdhery,SharanNarang,JacobDevlin, ing,Koan-SinTan,MarkCharlebois,WilliamChou,
Maarten Bosma, Gaurav Mishra, Adam Roberts, et al. 2022. Mlperf mobile inference benchmark:
Paul Barham, Hyung Won Chung, Charles Sutton, Anindustry-standardopen-sourcemachinelearning
Sebastian Gehrmann, et al. 2022. Palm: Scaling benchmarkforon-deviceai. ProceedingsofMachine
language modeling with pathways. arXiv preprint LearningandSystems,4:352–369.
arXiv:2204.02311.
YangqingJia,EvanShelhamer,JeffDonahue,Sergey
ZihangDai,GuokunLai,YimingYang,andQuocLe. Karayev, Jonathan Long, Ross Girshick, Sergio
2020. Funnel-transformer: Filteringoutsequential Guadarrama,andTrevorDarrell.2014. Caffe: Con-
redundancy for efficient language processing. Ad- volutionalarchitectureforfastfeatureembedding. In
vances in neural information processing systems, Proceedingsofthe22ndACMinternationalconfer-
33:4271–4282. enceonMultimedia,pages675–678.
Jacob D Kahn, Vineel Pratap, Tatiana Likhoma- Thao Nguyen, Maithra Raghu, and Simon Kornblith.
nenko,QiantongXu,AwniHannun,JeffCai,Paden 2021. Do wide and deep networks learn the same
Tomasello,AnnLee,EdouardGrave,GiladAvidov, things? uncoveringhowneuralnetworkrepresenta-
etal.2022. Flashlight: Enablinginnovationintools tions vary with width and depth. In International
formachinelearning. InInternationalConference ConferenceonLearningRepresentations.
onMachineLearning,pages10557–10574.PMLR.
Adam Paszke, Sam Gross, Francisco Massa, Adam
JaredKaplan,SamMcCand
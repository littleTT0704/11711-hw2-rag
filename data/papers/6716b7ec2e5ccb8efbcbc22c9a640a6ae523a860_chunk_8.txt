sponses. WeuseaT5basedfusionindecoder(FiD)
ent dense and sparse retrieval methods in a zero-
model which encodes all the top-k reranked pas-
shotsettingontheMultiDoc2Dialdataset. Forour
sages one-by-one and concatenates them to form
denseretrieverbaselines,weconductexperiments
theinputtothedecoder. Thedecoderthenlearnsto
with DPR, ANCE (Xiong et al., 2020) and TAS-
collectevidencefrommultiplepassagestogenerate
B (Hofsta¨tter et al., 2021). For sparse retrieval
theresponse.
methods,weexperimentwithSPLADE-maxand
Wealsoexperimentwithtrainingourmodelus- DistilSPLDAE(Formaletal.,2021). Duringtrain-
ingacurriculumlearningapproachoriginallypro- ing,welabeltheretrievedpassages(excludingthe
posedbyXuetal.(2020)andthenimplementedon goldenpassage)fromBM25ashardnegatives. We
Doc2DialbyKimetal.(2021). Todoso,wedivide alsoexperimentwiththefinetunedDPRmodelto
ourtrainingdatarandomlyinto4buckets,andtrain minehardernegatives.
ateachermodeloneachbucketusingFiD-T5. We RerankerFollowing(Fajciketal.,2021),wese-
then calculate each teacher model’s performance lectthetop100passagesfromtheDistilSPLADE
(BLEU,RougeLandMETEORscores)ontheother retrievertobererankedusingRoBERTAasacross
3 buckets, which the teacher model has not seen encoder. We use this reranking only during vali-
during training. The training instances are then dation time. The top 10 reranked documents are
partitionedinto”easy”,”medium”,and”hard”ex- passedtothereader.
amples based on the scores chosen in Kim et al.
ReaderWeexperimentwithbothT5andBART
(2021). We train in four phases, and each phase
modelsasthereader. Weus
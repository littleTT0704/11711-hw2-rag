 F1onvarioussourcelanguagetransferdatasizes. #endenotesthenumberofEnglishexamplesusedfor
transfer. âˆ†denotestheimprovementofMetaXLoverthejointtrainingbaseline. RTNisplacedafter12thlayer.
NER SA NER SA
Method Average tel fa Layer Method Average tel fa
JT 69.06 88.68 85.51 - JT 69.06 88.68 85.51
MetaXLL0 70.02 89.52 85.41 JTw/RTN 59.80 63.95 72.32
L0
MetaXLL6 70.27 86.00 85.80 MetaXL 70.02 89.52 85.41
MetaXLL12 71.13 90.53 87.14
JTw/RTN 67.18 83.75 70.40
MetaXLL0,12 69.00 84.85 86.64 L12
MetaXL 71.13 90.53 87.14
Table 5: F1 when placing the transfer component at
Table 6: F1 when joint training with and without the
different positions on XLM-R. Under this setting, we
representation transformation network in XLM-R. In
use 5k English data for NER and 1K English data for
this setting, we use 5k English examples for NER
SA.Lstandsforlayer.
and 1k English examples for SA. NER results are ag-
gregated over 8 target languages. Bold denotes that
MetaXLoutperformsbothJTandJTw/RTNbaselines.
thisend,weconductedexperimentswithrepresen-
tation transformation networks placed at various
depthsoftheTransformermodel. dergoes transformation via an augmented repre-
Specifically,weexperimentwithplacingtherep- sentationtransformationnetwork;(2)weadopta
resentation transformation network after the 0th bi-leveloptimizationproceduretoupdatethebase
(embeddinglayer),6thand12thlayer(denotedby model and the representation transformation net-
L0,L6,L12). Wealsoexperimentwithplacingtwo work. To verify that the performance gain from
identical representation transformation networks MetaXL is not attributed to increased model ca-
afterboththe0thand12
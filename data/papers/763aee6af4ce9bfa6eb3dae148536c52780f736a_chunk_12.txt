. (3)isthatlearningwith mucheasierandmoretractabletosolve.
inequalityconstraintsdoesnotproduceadditionalsample
3.3.Algorithm
complexityoverheadundersomeregularityconditionson
thelossfunction(cid:96)[15]. However,itisdifficulttosatisfythe
Motivatedbytheaboveanalysis, weuseaprimal-dual
strictnessandprovidetheoreticalguaranteesforlearningin
algorithmforefficientoptimization[15,17,58,60]. Theal-
practicalcases. Inthefollowingsection,withtheparameteri- gorithmalternatesbetweenoptimizingθ(and/orφ)viamini-
zationandsaddle-pointcondition,wecanrelaxtheinvariant mizingtheempiricalLagrangianwithfixeddualvariableλ
constraintandobtainaversionthatisamenabletoaprovable
andupdatingthedualvariableaccordingtotheminimizer:
PAClearningframework.
θ(t+1) ←argminL(θ(t),φ(t),γ)+ρ,
3.2.Parameterization θ
φ(t+1) ←argminL(θ(t),φ(t),γ)+ρ,
(6)
Wefirstdiscusshowtoparameterizethelearnablecompo-
φ
nentsinDDG.TheDGproblem(Eq.(3))yieldsaninfinite- (cid:110)(cid:104) (cid:105) (cid:111)
λ(t+1) ←max λ(t)+η Lˆ,0,
dimensionaloptimization.Adefactowaytoenabletractable 2 con
optimizationisusingfinite-dimensionalparameterizationof wheretheη denotesthelearningrateofthedualstep.
2
F likeneuralnetworks[33]orreproducingkernelHilbert The primal-dual iteration has clear advantages over
spaces(RKHS)[9]. Tofurtherdiscusstheparameterization stochasticgradientdescentinsolvingconstrainedoptimiza-
gap,weformalizethe
 a) Overall,multilingualmodelshaveimprovedtheav-
ananalysisofprogressincross-lingualmodeling; erageperformanceonXTREMEfrom55.8to81.4.
b)animprovedbenchmarkcovering50languages, Much of this improvement is concentrated in the
including a newly created retrieval task (Mewsli- retrieval-basedtaskswhereperformanceincreased
X);c)amassivelymultilingualdiagnosticsuite;d) from47.7(mBERT)to92.7(VECO). Incontrast,
fine-grainedevaluationcapabilities;e)experiments performanceonquestionansweringandstructured
andanalysesofstate-of-the-artmodels;andf)an predictiontaskshasimprovedonlyslightly.
interactivemetadata-richleaderboard. Breakingdownperformancebylanguagefamily,
onTatoeba(Figure1c)recentmodelsstillstruggle
2 ExaminingtheStateofMultilingual with a few low-resource languages. Models per-
Benchmarking formwellformostotherlanguagesandtheirscores
are concentrated in a relatively small range. On
2.1 Background
MLQA(Figure1b),scoreshaveincreasedslightly
Benchmarking is critical to evaluate general-
1This evaluation compares “models + data” as models
purposelanguageunderstandingtechnologies. To
employdifferenttypesofdataduringtraining.Wedocument
this end, benchmarks like GLUE (Wang et al., thisinformationinAppendixH.
(a)PerformanceonXTREME (b)PerformanceonMLQA
(c)PerformanceonTatoeba (d)PerformanceonUD-POS
Figure1: Performanceofmodels(a)ontheXTREMEleaderboardacrossallnineXTREMEtasks,(b)ontheMLQA
question answering dataset, (c) on the Tatoeba retrieval task, and (d) on Universal Dependencies POS tagging
across language families. Models are ordered based on their XTREME score (a). Results of models that do not
evaluateonataskcategoryareomitted,i.e. RemBERTforretrievalandmT5forretrievalandtagging.
butremainwellbelowperformanceonEnglish. On ilartothedownstreamsettingbut
wayto
evaluateexistingexplanationmethods,itstopsshortofproposingamethodtoactuallyimprovethem.
Inthiswork, weproposetolearntoexplainbydirectlylearningexplanationsthatprovidebetter
scaffoldingofthestudent’slearning,aframeworkwetermScaffold-MaximizingTraining(SMaT).
Figure 1 illustrates the framework: the explainer is used to scaffold the student training, and is
updatedbasedonhowwellthestudentdoesattesttimeatsimulatingtheteachermodel. Wetake
insights from research on meta-learning [Finn et al., 2017, Raghu et al., 2021], formalizing our
settingasabi-leveloptimizationproblemandoptimizingitbasedonhigher-orderdifferentiation
(§3). Importantly,ourhigh-levelframeworkmakesfewassumptionsaboutthemodelwearetrying
toexplain,thestructureoftheexplanationsorthemodalitiesconsidered. Totestourframework,we
thenintroduceaparameterizedattention-basedexplaineroptimizablewithSMaTthatworksforany
modelwithattentionmechanisms(§4).
WeexperimentwithSMaTintextclassification,imageclassification,and(multilingual)text-based
regressiontasksusingpretrainedtransformermodels(§5). Wefindthatourframeworkisableto
effectivelyoptimizeexplainersacrossalltheconsideredtasks,wherestudentstrainedwithlearned
attention explanations achieve better simulability than baselines trained with static attention or
gradient-basedexplanations. Wefurtherevaluatetheplausabilityofourexplanations(i.e.,whether
producedexplanationsalignwithhowpeoplewouldjustifyasimilarchoice)usinghuman-labeled
explanations(textclassificationandtextregression)andthroughahumanstudy(imageclassification)
andfindthatexplanationslearnedwithSMaTaremoreplausiblethanthestaticexplainersconsidered.
Overall,theresultsreinforcetheutilityofscaffoldingasacriterionforevaluatingandimproving
modelexplanations.
2 Background
ConsideramodelT : X → Y trainedonsomedatasetD = {(x,y )}
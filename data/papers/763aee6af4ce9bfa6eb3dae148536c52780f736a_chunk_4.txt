MNIST,VLCS,PACSandWILDS.
rotatingangles−60◦,0◦,60◦,120◦)arewelldisentangled,
we expect that learned representations can be effectively 2.RelatedWork
constrainedtobeinvarianttointer-classvariation. Inorder
Domain Generalization. Domain/Out-of-distribution
to achieve such a non-trivial goal, we first derive a con-
generalization[55]aimstolearnrepresentationsthatarein-
strainedoptimizationproblemandthenproposeaprincipled
variantacrossdomainssothatthemodelcanextrapolatewell
algorithm based on primal-dual iterations to solve it. To
inunseendomains. InvariantRiskMinimization(IRM)[3],
understandhowwellthetransformedsolutionapproximates
whichextends[56],anditsvariants[1,42,51]areproposed
thesolutiontotheoriginalproblem,weprovidecomprehen-
totacklethischallenge. However,IRMentailschallenging
sivetheoreticalguaranteesfortheparameterizationgapand
bi-leveloptimizationandcanfailcatastrophicallyunlessthe
empiricalgap. Wealsoverifytheempiricaleffectivenessof
testdataaresufficientlysimilartothetrainingdistribution
DDGbyshowingthatitcanconsistentlyoutperformcurrent
[62]. DG via domain alignment [2, 18, 55] aims to mini-
popularDGmethodsbyaconsiderablemargin.
mize the difference between source domains for learning
Asausefulsideproduct, DDGsimultaneouslyobtains domain-invariantrepresentations. Themotivationisstraight-
anautomated,domain-agnosticdataaugmentationnetwork forward: features that are invariant to the source domain
basedonlearneddisentangledrepresentations. Thisrequires shiftshouldalsoberobusttoanyunseentargetdomainshift.
nousageofdomain-specificknowledgeorgradientestima- Themaindifferenceisthatweproposetolearninvariantrep-
tion [4, 77]. The intuition why such a data augmentation resentations
2:Questionn-gramdistributioninL¯ILA.
areconsistentanswer-wisebutstylisticallydiffer-
entquestion-wise. Overall,wedefineatotalof9
templatesforsuchquestionperturbations: 3from Fine-tuning. Wefine-tuneaseriesofGPT-Neo-
2.7Bcausallanguagemodels(Blacketal.,2021))
Patel et al. (2021) and 6 of our own. From each
constituent dataset, we sample 20 questions and
onL¯ILA. WechooseGPT-Neobecauseitwaspre-
trained on both natural language and code (Gao
obtainperturbedquestionannotationsviaAmazon
etal.,2020), asopposedto solelyonnaturallan-
MechanicalTurk(AMT).RefertoAppendixB.1
foradditionaldetailsontheconstructionofL¯ILA- guage. ToassessthecapabilitiesofGPT-Neoon
variousaspectsofthedataset,wefine-tunesingle-
ROBUST.
taskmodelsoneachofthe23tasksin L¯ILA. We
3.5 Statistics alsoevaluatethebenefitoftransferlearningbyfine-
tuningasinglemulti-taskGPT-Neobaselineonall
Table2showskeystatisticsofourproposedbench-
the tasks simultaneously. We call our multitask
mark, L¯ILA. L¯ILA contains
≈
134K examples
modelBHA¯SKARA.
withsignificantdiversityacrossquestion,answer,
programandinstructionlength(seedetailedstatis- Prompting. Wealsousefew-shotpromptingto
tics in Appendix C). Figure 2 shows the diver- evaluate GPT-3 and Codex4 (Brown et al., 2020;
sity of questions in L¯ILA. Note that we down- Chenetal.,2021). FortheIIDsetting,weprompt
sample(viarandomselection)somedatasetslike the model with a random input-output examples
AMPS(Hendrycksetal.,2021b)whichcontains from the same dataset as the input. In the OOD
numerous templated questions that can get over- setting
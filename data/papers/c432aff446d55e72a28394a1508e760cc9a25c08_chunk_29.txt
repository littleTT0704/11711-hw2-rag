neural
networksbypenalizingconfidentoutputdistributions. arXivpreprintarXiv:1701.06548,2017.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Languagemodels
areunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
VictorSanh, LysandreDebut, JulienChaumond, andThomasWolf. Distilbert, adistilledversionofbert:
smaller,faster,cheaperandlighter. arXivpreprintarXiv:1910.01108,2019.
RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewordswithsubword
units. arXivpreprintarXiv:1508.07909,2015.
FelixStahlbergandBillByrne. Onnmtsearcherrorsandmodelerrors: Catgotyourtongue? arXivpreprint
arXiv:1908.10090,2019.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inceptionarchitectureforcomputervision. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages2818â€“2826,2016.
DexinWang,KaiFan,BoxingChen,andDeyiXiong. Efficientcluster-basedk-nearest-neighbormachine
translation. ArXiv,abs/2204.06175,2022.
ZhilinYang,ZihangDai,RuslanSalakhutdinov,andWilliamWCohen. Breakingthesoftmaxbottleneck: A
high-rankrnnlanguagemodel. arXivpreprintarXiv:1711.03953,2017.
ZhixianYang,RenliangSun,andXiaojunWan. Nearestneighborknowledgedistillationforneuralmachine
translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association
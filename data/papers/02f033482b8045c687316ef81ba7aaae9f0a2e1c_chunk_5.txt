ablepropertiesforefficientandeffectivesteeringof monly requires truncating the unreliable tail of
largerpretrainedLMs,andhighlightsthepromise
2Thoughnotexploredinthispaper,thisformulationreadily
ofdecoding-timemethodsforcontrolledlanguage
accommodatesmultipleexpertsandanti-experts,whoselogits
generation. canberespectivelyaddedorsubtracted.
theprobabilitydistribution,asintop-k (Fanetal., has „160K comments, and the nontoxic dataset
2018)ornucleussampling(Holtzmanetal.,2020). „1.4M comments. Note that our toxic dataset is
We adapt this intuition to our method by truncat- human-annotatedandout-of-domainwithrespect
ingthelogitszoutputbythebasemodelprior to tothepretrainingcorpus(WebTextforGPT-2).
combiningwiththeexperts. Formally,letV1 Ă V Wereportresultsforα “ 2.0, chosenafterob-
denotethesetoftokensthatareapartofthetop- servingthetradeoffbetweendetoxificationandflu-
k/top-pvocabularyofthebaseLMattimestept. ency,butshowresultsforothervaluesofαinAp-
Thetruncatedlogitsz1 aregivenby pendixD.
#
zrvs ifv P V1 3.2 Evaluation
z1rvs “ (4)
3.2.1 GenerationPrompts
´8 otherwise
To evaluate the problem of toxic degeneration
Bysubstitutingzwithz1 inEquation2,wehave
where a user might unexpectedly receive harm-
` ` ˘˘
P˜1pX | x q “ softmax z1 `α z`´z´ ful output from a model, we use a random sam-
t ăt t t t
pleof10KnontoxicpromptsfromtheRealToxici-
(5)
tyPromptsdataset(Gehmanetal.,2020).
Weobtainournexttokenx viapuresamplingfrom
t
the probability distribution P˜1pX t | x ăt
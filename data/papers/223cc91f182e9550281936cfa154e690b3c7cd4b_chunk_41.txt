� : T → R is a continuous function, and J∗(φ) = sup hEh[φ(t)]−J(h) is the convex
conjugate of J. In the case of J(p) being the JS divergence with p, if we approximate the above
d
optimization by parameterizing φ(t) as φ = 1 log(1 −C ) − 1 log2 where C : T → [0,1] is a
ϕ 2 ϕ 2 ϕ
binary classifier (a.k.a., discriminator) and p as p (i.e., the target model), Equation 5.5 recovers the original
θ
GAN algorithm (Goodfellow et al., 2014):
1 1
minmax Ep [logC ϕ(t)]− Ep [log(1−C ϕ(t))]. (5.6)
θ ϕ 2 d 2 θ
5.3. Wasserstein Distance
Another distance measure that is receiving increasing interest is the Wasserstein distance, a member of the
optimal transport distance family (Peyré & Cuturi, 2019; Santambrogio, 2015). Compared to many of the
divergence metrics (e.g., KL divergence), Wasserstein distance has the desirable properties as a distance metric,
such as symmetry and the triangle inequality. Based on the Kantorovich duality (Santambrogio, 2015, Section
1.2), the first-order Wasserstein distance between the two distributions q and p can be written as:
W 1(q,p) = sup Eq [φ(t)]−Ep [φ(t)],
(5.7)
∥φ∥L≤1
where ∥φ∥ L ≤ 1 is the constraint of φ : T → R being a 1-Lipschitz function.
Wasserstein GAN. Setting the divergence function in Equation 5.1 to the Wasserstein distance W, we thus
1
recover the Wasserstein GAN algorithm (Arjovsky et al., 2017):
m θinW 1(p d,p θ) =m θin sup Ep
d
[φ(t)]−Ep
θ
[φ(t)],
(5
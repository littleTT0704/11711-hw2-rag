izedratherthanlocallyoptimized. mixture of Gaussian processes (HME-GP) (Tresp, 2001;
Rasmussen&Ghahramani,2002;Yuan&Neubauer,2009;
We test our model on a collection of standard regression
Nguyen & Bonilla, 2014), mixture of support vector ma-
tasks,andtheresultsvalidatetheeffectivenessofourmodel
chines(HME-SVM)(Limaetal.,2007;Cao,2003),toname
comparedtoHMEandotherregressionmodels. Ourcon-
onlyafew.
tributionsare: (1)weproposeanewgatingmechanismvia
joint-partitionofbothinputspaceandoutputspacetosepa- The ME models have three issues: (1) the gating mecha-
ratethemodesofcomplexlydistributeddata,makingsimple nism does not explicitly leverage the input-output depen-
regressionmodelseffectiveforpredictions;(2)wedevelop denciesofthedata. Rather,itperformsprobabilisticinput-
arecursiveEMalgorithmtojointlyoptimizethepartition, spacepartitioning,basedonassumeddatadistributionssuch
theexpertmodels,aswellasthetreestructure. as the multinomial distribution (Jordan & Jacobs, 1994),
Gaussiandistribution(Yuan&Neubauer,2009),Dirichlet
process(Rasmussen&Ghahramani,2002),Gaussianpro-
2.RelatedWork
cess (Tresp, 2001), etc; (2) in ME models strong experts
Decisiontrees(Loh,2014;Breiman,2017;Quinlan,1986) areoftenneededtogaingoodperformance(Yukseletal.,
areafamilyofsupervisedlearningmethodsthatutilizea 2012);(3)thestructureoftheMEmodels,namelythetree
partition on the input feature space and make piece-wise depthandthenumberofexperts,isoftenoptimizedthrough
y y y
HierarchicalRoutingMixtureofExperts
eachnon-leafnode. However,wedonothavethedataclass
informationbeforehand,i.e.,wedonotknowhowdat
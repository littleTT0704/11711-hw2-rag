. We report re-
sults on the dev sets of these tasks, as the official test sets
H2:Pre-trainingalanguagemodelwithartificiallycreated
arenotpubliclyavailable.Wenotethat,sincewedidnotuse
question-answer sets enhances zero-shot performance.
thetasksâ€™devsetsforhyperparametertuningorcheckpoint
This is also supported in previous study about unsuper-
selection,thedevsetscanbeusedeffectivelyastestsets.
visedQA(Lietal.2020)
H3: The impact of more knowledge depends on the align- MainResults
ment between KGs and the task, partial evidence for Table2showsthatGPT-2andRoBERTaoutperformthema-
whichisprovidedby(Maetal.2019;Mitraetal.2019). joritybaselinebyalargemarginonalltasks,indicatingthat
13511
RoBERTa-L Strategy aNLI CSQA PIQA SIQA WG
+ATOMIC Random 70:8((cid:6)1:2) 64:2((cid:6)0:7) 72:1((cid:6)0:5) 63:1((cid:6)1:5) 59:6((cid:6)0:3)
+ATOMIC Adv-answer 70:4((cid:6)0:8) 62:3((cid:6)0:9) 72:6((cid:6)1:8) 61:6((cid:6)0:3) 60:5((cid:6)0:5)
+ATOMIC Adv-question 70:8((cid:6)0:6) 55:6((cid:6)0:9) 70:6((cid:6)0:8) 51:6((cid:6)0:8) 58:5((cid:6)0:3)
+ATOMIC Adv-filter 68:6((cid:6)1:8) 46:4((cid:6)1:5) 67:9((cid:6)1:1) 51:8((cid:6)1:2) 60:8((
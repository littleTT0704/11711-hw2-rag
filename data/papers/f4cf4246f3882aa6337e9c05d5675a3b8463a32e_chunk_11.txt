 rerunning A∗. In ALFRED, on the fly demonstra-
validation, we show a worker all three language directive tionsrequiresre-planning. Insamecasesre-planningisnot
4
Act Reason Act Reason
LSTM LSTM
... counter then turn right. Put the cup in the sink then fill...... Put the
RotateRight PutObject
ResNet Conv
not used with...
nav actions
predicted
mask
DeConv (Frozen) Linear DeConv
Figure 4: Model overview. At each step, our model reweights the instruction based on the history (xˆ ), and combines the
t
current observation features (v ) and the previously executed action (a ). These are passed as input to an LSTM cell to
t t−1
producethecurrenthiddenstate. Finally,thenewhiddenstate(h )iscombinedwiththepreviousfeaturestopredictboththe
t
nextaction(a )andapixelwiseinteractionmaskovertheobservedimagetoindicateanobject.
t
possible: ifduringataskof{Clean&Place,apple,refrig- where W are learnable parameters of a fully-connected
x
erator,KITCHEN-3}astudent-forcingmodelslicestheonly layer, z
t
is a vector of scalar values that represent the at-
appleinthescene,theactioncannotberecoveredfromand tentionmassforeachwordinx,andxˆ istheweightedsum
t
thetaskcannotbecompleted. ofxovertheattentiondistributionα inducedfromz.
t t
Visual encoding. Each visual observation o is encoded
t
Action decoding. At each timestep t, upon receiving a
withafrozenResNet-18[22]CNN,wherewetaketheout-
new observation image o, the LSTM decoder takes in the
put of the final convolution layer to preserve spatial infor- t
visualfeaturev,languagefeaturexˆ,andthepreviousac-
mation necessary for grounding specific objects in the vi- t t
tiona,andoutputsanewhiddenstateh :
sualframe.Weembedthisoutputusingtwomore1×1con- t−1 t
volution layers and a fully
tothat ing tasks like Tatoeba, and BUCC (m.) for the
of the model trained specifically for this task on pretrained 24 layer model, favor CONTRASTIVE,
NQdatafromLewisetal.(2021)whichhasanac- while semantic similarity, BUCC (c.) and ques-
curacyof41.2. VMSSTandBITRANSLATIONare tionretrievalfavor VMSST,suggestingsomefun-
especiallystrongwhenusingmorelayers,whichis damentaldifferenceinthesetasksfavoring CON-
notthecaseforCONTRASTIVEwhichdeclinesin TRASTIVE. An examination of the Tatoeba and
performancewhenmovingfrom6to24layers. In BUCCdatashowsthatthereareparaphrasesinthe
factat24layers,BITRANSLATIONperformsbetter testset,butaccountingforthesedoesnotseemto
onaveragethan CONTRASTIVE. Perhapsforeven meaningfullyexplainthisperformancedifference.
largermodels,thegapbetweencontrastiveandgen- Lastly,weseethat VMSST outperformsCON-
erativemodelswillincrease. WealsoseethatCON- TRASTIVE on the BUCC task with cosine simi-
TRASTIVE seems to benefit more from pretrain- larity,thoughtheresultsbetweenthetwomodels
ing than VMSST and BITRANSLATION, which arecloserwhenusingmargin. Thissuggeststhat
could possibly be due to VMSST re-purposing the“hubnessproblem”(Radovanovicetal.,2010;
Radovanovic´ etal.,2010)wheretheneighborhood Model XL XL(s.) XL(d.)
aroundembeddingsinahigh-dimensionalspaces
mUSE 79.5 81.7 78.1
havemanyneighborsincommon,islessofanis- LASER 69.0 74.3 65.5
XLM-R(NLI/STS-B) 79.0 81.7 77.2
suewhenlearningembeddingswithVMSST.This
XLM-R(Para.) 82.4 82.9 82.1
smootherembeddingspacemay
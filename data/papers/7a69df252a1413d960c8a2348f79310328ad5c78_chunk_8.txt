W)=σ(g(z,W))=σ(W δ(W z)), (4)
ex 2 1
where δ refers to the ReLU activation function, W
1
∈RC r×C, and W
2
∈RC×C r.
To limit model complexity and increase generalisation, a bottleneck is formed
aroundthegatingmechanism:aFullyConnected(FC)layerreducesthedimen-
sionalitybyafactorofr.AsecondFClayerrestoresthedimensionalityafterthe
gatingoperation.Theauthorsrecommendanr of16foragoodbalancebetween
accuracy and complexity (∼10% parameter increase on ResNet-50 [9]). Ideally,
r should be tuned for the intended architecture.
TheexcitationoperationF computesper-channelmodulationweights.These
ex
are applied to the feature maps U performing an adaptive recalibration.
4 Experiments
Dataset We employed CUB [38] and COCO [21] datasets for the experiments.
The CUB dataset [38] consists of 8855 train and 2933 test images. To perform
evaluation, one image per caption in the test set is computed since each image
has ten captions. The COCO dataset [21] with the 2014 split consists of 82783
train and 40504 test images. We randomly sample 30000 captions from the test
set for the evaluation.
Evaluation metric In this work, we utilized the Inception Score (IS) [34]
and The Fr´echet Inception Distance (FID) [10] to evaluate the performance of
proposed method. The IS [34] is a quantitative metric to evaluate generated
images. It measures two properties: highly classifiable and diverse with respect
Title Suppressed Due to Excessive Length 7
to class labels. Although the IS is the most widely used metric in text-to-image
generation, it has several issues [33] [2] [24] regarding the computation of the
score itself and the usage of the score. According to the authors of [2] it: ”fails
to provide useful guidance when comparing models”.
The FID [10] views features as a continuous multivariate Gaussian and com-
putes a distance in the feature space between the real data
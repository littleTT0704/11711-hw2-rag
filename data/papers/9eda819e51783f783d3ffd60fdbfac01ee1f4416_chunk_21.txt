 Jastrzebski,
cross-lingual representation learning at scale. In Bruna Morrone, Quentin De Laroussilhe, Andrea
Proceedingsofthe58thAnnualMeetingoftheAsso- Gesmundo, Mona Attariyan, and Sylvain Gelly.
ciation for Computational Linguistics, pages 8440– 2019. Parameter-efficient transfer learning for nlp.
8451, Online. Association for Computational Lin- In International Conference on Machine Learning,
guistics. pages2790–2799.PMLR.
AndrewMDaiandQuocVLe.2015. Semi-supervised Jeremy Howard and Sebastian Ruder. 2018. Univer-
sequencelearning. Advancesinneuralinformation sallanguagemodelfine-tuningfortextclassification.
processingsystems,28:3079–3087. arXivpreprintarXiv:1801.06146.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Kristina Toutanova. 2019. BERT: Pre-training of Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
deep bidirectional transformers for language under- and Weizhu Chen. 2021. Lora: Low-rank adap-
standing. In Proceedings of the 2019 Conference tation of large language models. arXiv preprint
of the North American Chapter of the Association arXiv:2106.09685.
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa
pages4171–4186,Minneapolis,Minnesota.Associ- Dehghani, and James Henderson. 2021. Parameter-
ationforComputationalLinguistics. efficient multi-task fine-tuning for transformers via
shared hypernetworks. In Proceedings of the 59th Rabeeh Karimi Mahabadi, James Henderson, and Se-
Annual Meeting of the Association for Computa- bastian Ruder. 2021. Compacter: Efficient low-
tional Linguistics and the 11th International Joint rank hypercomplex adapter layers. arXiv preprint
Conference on Natural Language Processing (Vol-
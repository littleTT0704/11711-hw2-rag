 FutureWork
numbers. Some recent work suggests that using
7.1 GeneralizationandRobustness
scientificnotation(Zhangetal.,2020d)anddigit-
level decomposition (Geva et al., 2020) may be Despiteimpressiveprogress, neuralmodelscom-
helpfulinimprovingnumeracyrepresentation,but monly display generalization and robustness fail-
uresonreasoningtasks. Forexample,abovewedis- ingreinforcementlearningfromhumanfeedback
cusseddifficultiesingeneralizingtolargernumbers (RLHF) (Ouyang et al., 2022) to align language
(Table5)orremainingrobusttonearbyproblems modelswithinstructions. Theideaistolethumans
(Table 6), while others identify failures in gener- rankthegeneratedoutputsoflanguagemodelsand
alizingtolongerproblemsthanthoseobservedin usethelearnedrewardfunctiontofinetunethelan-
training(e.g.,Aniletal.(2022)). Onedirectionis guagemodelwithpolicygradient(Ouyangetal.,
to explore new inference-time (Jung et al., 2022; 2022;Glaeseetal.,2022;Qiuetal.,2022a). Inthe
Mitchell et al., 2022) or fine-tuning (Anil et al., contextofmathematicalreasoning,feedbackdoes
2022)strategies. not necessarily come from humans directly. The
Another aspect of generalization relates to the outcome of a theorem-proof engine (Jiang et al.,
roleofmemorization. Forexample,istheabilityto 2021; Wu et al., 2021d, 2022c) or the execution
produce a complex solution dependent on seeing resultofmodel-generatedscriptscanalsobeused
manysimilarsolutionsduringtraining,orevenon astherewardsource(PoluandSutskever,2020).
memorizing the solution? Term frequency in the
pretrainingcorpusisknowntoimpactaccuracyin 7.4 Multi-modalMathematicalReasoning
simple arithmetic tasks (Razeghi et al., 2022) or
In recent years, there has been growing interest
factualquestionanswering(Kandpaletal.,20
ossfunctioninconjunctionwiththejoint
correlatedwiththeF1IC andmeasuresthenumberofincor- trainingapproach.Inourexperimentswefixedtheweight
rectintents.ICERisnotalwayssufficientastheremaybe forthelossfunctionbutadditionalhyper-parametertuning
multipleintentsinanutterance.TheformulaforICERis: mightimprovetheoverallresult.
Wealsocomparedtheaccuracyofthebaselineandourbest
#incorrect(intents)
performingmodelacrossdifferentactions.Figure7shows
#totalintents
thatourdatasetisskewed,withtwooftheactions(Playback
Finally,theIntentRecognitionErrorrate(IRER)iscom- and Search) covering more then 97% of the total training
putedas: instances;theremaining15actionshaveanalmostuniform
#incorrect(interpretation) amount of sentences. Table 2 shows the gain in ICER for
#totalutterances eachaction.Weobservehowtheimprovementismodeston
thetwomostrepresentedactionsbutmuchmorepronounced
whereweconsideraninterpretationincorrectifanyofthe
onthelessrepresentedones.Ingeneral,wefindthatwhen
slotsorintentsdiffersfromthegroundtruth.
thereissufficientdataavailable(i.e.,forthePlaybackand
Searchactions)addingmoreinformationfromtheSLUtask
Results
is not very helpful. On the other hand, when fewer train-
Table1showstheresultsfordifferentmodelarchitectures. inginstancesareavailabletheinformationprovidedbythe
ThebaselinemodelwastrainedusingonlytheAMRLcor- SLUtasksbecomesveryvaluableandstronglyimprovesthe
pus.Thevocabularywasprunedandonlywordsappearing results.
twiceormoreareused.Everyotherwordismappedtoan In Table 3, we evaluate the best models against those
“unknown”tokenresultinginavocabularyof∼25kwords. trainedwithwordembeddingsandgazetteers.Thebaseline
FromtheseresultsitappearsthatusingSLUtasksclearlypro-
videsbenefitwhentrainingAMRLmodels;ourbestmodel
(+SLUSlots/Domain)outper
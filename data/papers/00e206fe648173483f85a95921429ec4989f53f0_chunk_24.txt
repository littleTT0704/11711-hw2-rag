 for Compu-
Spain(Online).InternationalCommitteeonCompu-
tationalLinguistics.
tationalLinguistics.
Suchin Gururangan, Ana Marasovic´, Swabha
Pengshuai Li, Xinsong Zhang, Weijia Jia, and Wei
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
Zhao. 2020b. Active testing: An unbiased evalua-
and Noah A. Smith. 2020. Don’t stop pretraining:
tionmethodfordistantlysupervisedrelationextrac-
Adapt language models to domains and tasks. In
tion. In Findings of the Association for Computa-
Proceedings of the 58th Annual Meeting of the
tional Linguistics: EMNLP 2020, pages 204–211,
Association for Computational Linguistics, pages
Online.AssociationforComputationalLinguistics.
8342–8360, Online. Association for Computational
Linguistics.
Jing Lu and Vincent Ng. 2020. Conundrums in entity
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. coreference resolution: Making sense of the state
Weld, Luke Zettlemoyer, and Omer Levy. 2020. of the art. In Proceedings of the 2020 Conference
SpanBERT: Improving pre-training by representing onEmpiricalMethodsinNaturalLanguageProcess-
and predicting spans. Transactions of the Associa- ing (EMNLP), pages 6620–6631, Online. Associa-
tionforComputationalLinguistics,8:64–77. tionforComputationalLinguistics.
YuvalKirstain,OriRam,andOmerLevy.2021. Coref- Timothy Miller, Dmitriy Dligach, and Guergana
erence resolution without span representations. In Savova. 2012. Active learning for coreference res-
Proceedings of the 59th Annual Meeting of the olution. InBioNLP:Proceedingsofthe2012Work-
Association for Computational Linguistics and the shop on Biomedical Natural Language Processing,
11thInternationalJointConferenceonNaturalLan- pages 73–81, Montréal, Canada. Association for
guage Processing (Volume 2: Short Papers),
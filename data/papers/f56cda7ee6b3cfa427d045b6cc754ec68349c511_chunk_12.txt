 ),i < j ≤ k.
i j experimentwithClass-BalancedFocalLoss,CB
foc
(Cuietal.,2019).
5.1 ModelArchitectures
Formally, let C = {Neutral, Agree, Disagree}
Inbothclassificationtasks,weexperimentwiththe
and sˆ = (z,z,z ) represent the unnormalized
0 1 2
followingthreemodelarchitectures:
scoresassignedbythemodelforeachstancelabel.
NBOW - Neural-Bag-Of-Words (Bowman et al.,
Then,
2015) model converts input sentences into latent
representationsbytakingweightedaverageoftheir 1−β (cid:88)
CB (sˆ,y) = − (1−p )γlog(p )
word embeddings. Then, the sentence represen- foc 1−βny m m
tationsareconcatenatedandprocessedthrougha (cid:124) (cid:123)(cid:122) (cid:125)m (cid:124)∈C (cid:123)(cid:122) (cid:125)
reweighting
3-layerperceptronwithReLUactivationsandsoft- focalloss
maxlayertogetclassificationoutput. whereyisthecorrectstancelabel,n isthenumber
y
BERT - We fine-tune BERT LARGE model (340M ofinstanceswithlabely andp m = sigmoid(z m(cid:48) ),
parameters, Devlin et al., 2019) based classifiers. (cid:26) z m = y
with z(cid:48) = m. The reweighting
BERT computes latent token representations of m −z otherwise
m
input “[CLS] u [SEP]” for the Offensive term represents the effective number of samples
i
fromeachclass,thusreducingtheimpactofclass-
10https://github.com/XuhuiZhou/Toxic_
imbalance on the loss. The focal loss (Lin et al.,
Debias/blob/main/data/word_based_bias_
list.csv
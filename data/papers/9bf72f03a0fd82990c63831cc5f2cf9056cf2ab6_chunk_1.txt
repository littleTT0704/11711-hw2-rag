SQuAT: Sharpness- and Quantization-Aware Training for BERT
ZhengWang∗,JunchengBLi∗,ShuhuiQu,FlorianMetze,EmmaStrubell
{zhengwan,junchenl,fmetze,strubell}@cs.cmu.edu
Abstract
Quantization is an effective technique to re-
ducememoryfootprint,inferencelatency,and
power consumption of deep learning models.
However,existingquantizationmethodssuffer
from accuracy degradation compared to full-
precision (FP) models due to the errors intro-
duced by coarse gradient estimation through
non-differentiable quantization layers. The
existence of sharp local minima in the loss
landscapesofoverparameterizedmodels(e.g.,
Transformers) tends to aggravate such per-
formance penalty in low-bit (2, 4 bits) set-
tings. Inthiswork,weproposesharpness-and
quantization-aware training (SQuAT), which
would encourage the model to converge to
flatter minima while performing quantization-
aware training. Our proposed method alter-
natestrainingbetweensharpnessobjectiveand
Figure 1: Illustration of our SQuAT alternate training
step-sizeobjective,whichcouldpotentiallylet
in a 2D toy example, where W,W indicate the 2 di-
the model learn the most suitable parameter 1 2
mensions. (A)whenstep-sizeisfixed,updateweights
update magnitude to reach convergence near-
intheSharpness-awareminimizing(SAM)directionin
flatminima. Extensiveexperimentsshowthat
thequantizedlosslandscape. (B)Alternatingstep: fix
ourmethodcanconsistentlyoutperformstate-
weight,updatethestep-size.
of-the-artquantizedBERTmodelsunder2, 3,
and 4-bit settings on GLUE benchmarks by
1%, and can sometimes even outperform full
thetraininganddeploymentofdeeplearningmod-
precision(32-bit)models. Ourexperimentson
elswithhighcompressionrates,resultinginasig-
empiricalmeasurementofsharpnessalsosug-
gestthatour
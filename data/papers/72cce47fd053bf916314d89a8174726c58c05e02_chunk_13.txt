
(2019) because it is not applicable to the genera-
We use roberta-base6 as the base model for
tionmodel. Wedonotusegeneration-basedmet-
theextraction-basedmodel,asitdemonstratesits
rics (e.g., BLEU) because we observe that most
effectivenessonmultiplesequencelabelingtasks.
predictionsareveryshort. Inaddition,compared
We use t5-small7 for the generation-based
tonoprediction,wewanttopenalizewrongpredic-
model, which has much fewer parameters than
tionsmore. InF,thebasisofprecisiondoesnot
1
roberta-base. Please refer to Appendix A
include“noprediction”resultsfrommodelswhile
foradetailedhyperparametersetupandestimated
itstillhasapenaltyforwrongpredictions.
trainingandinferencetime.
4.2 Results
Evaluationmetric. Wechooseuser-levelF 1 as
our evaluation metric. Specifically, we suppose 4.2.1 ResultsonUserProfileInference
6https://huggingface.co/roberta-base The main results are shown in Table 3. The ran-
7https://huggingface.co/t5-small dom result means that predictions are uniformly
3177
Category EDUCATION JOB
Precision Recall F Precision Recall F
1 1
GraphIE 92.87 79.74 85.77 76.03 61.01 67.66
Generation 94.28 91.40 92.82 78.97 65.78 71.76
Table5: ResultsonLietal.(2014)followingthepreprocessingasQianetal.(2019). Were-evaluatetheresults
basedonuser-levelF. p<0.01forbothF comparisons.
1 1
randomlyselectedandthemajoritymeansthatpre- Model Precision Recall F 1
dictionsareselectedwiththevaluesthatoccurmost
Ourmodel 59.05 43.71 50.23
frequentlyinthetrainingset. Wefindthatbothsim-
-threshold 45.95 45.95 45.95
plemethodsperformpoorly. Overall,wefindthat
-aggregation 57.
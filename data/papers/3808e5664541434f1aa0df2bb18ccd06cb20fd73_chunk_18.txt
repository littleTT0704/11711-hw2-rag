 discrepancy in flagging toxicity.
alectal translation of the tweet, inspired by previ-
Notably, the toxicity rates on tweets by African
ous work showing that highlighting AAE tweets’
American authors—and the diferences compared
dialect led them to be labeled as less toxic (Sap
to white authors—are similar across all debias-
etal.,2019). Weconcludethisstudybydiscussing
12Forefficiency, werandomlyselect12ktweetsfromthe thelimitationsandethicalimplicationsofthesyn-
datasetastheOODtestset.
thetic data, and cautioning against its real-world
13Notethatweassumethatauthorsfromallraceshavethe
samelikelihoodofwritingtoxiclanguage. application.
3149
niart%33
W-Tox. AA-Tox. ∆↓AA/W↓ non-toxic(examplesinTable6). Toassesstheva-
lidityoftherelabeling,thefirstthreeauthorsman-
Original 7.24 12.61 5.37 1.74
LMIXIN-Dialect 7.50 12.55 5.06 1.67 ually annotated toxicity of 50 randomly selected
Random 8.28 13.24 4.96 1.60 relabeledtweets. Onaverage,authorsagreedwith
AFLite 7.32 11.64 4.33 1.59 84%oftherelabelingdecisions.
DataMaps-Ambig. 6.75 12.17 5.42 1.80
DataMaps-Hard 6.36 11.67 5.31 1.84
Then, we evaluate the dialectal bias of AAE-
DataMaps-Easy 8.46 16.30 7.83 1.94 relabeled and quantify the dialect and racial pre-
AAE-relabeled 6.93 10.60 3.67 1.53 diction biases from a RoBERTa-large classifier
trainedonAAE-relabeled,following§5. Asshown
Table 5: Racial disparity in toxicity prediction re- in the last row of Table 4, this relabeling scheme
ported on Preo¸tiuc-Pietro and Ungar (2018). W-Tox.
decreasesdialectalb
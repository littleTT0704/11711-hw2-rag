plingstrategyforDim-1to5in-
volvesrandomlychoosingatailnodefromDim-6
to 10 and vice-versa. As a result, one negative
sample is introduced every five samples. During
training,weusedalearningrateof1e-4,batchsize
of64, 8epochsandAdamWoptimizer. Training
took15hoursonaNvidiaP100GPU.
A.3 Realistrainingdetails
We used the train/dev/test split from the Realis
dataset (Sims et al., 2019). During training, we
usedtheAdamWoptimizer,alearningrateof2e-5,
3 epochsand batch sizeof 4, asinspired by (Sap
et al., 2020). Training took 1 hour on a Nvidia
P100GPU.
A.4 Sequentialityexperimentaldetails
GPT2-small was accessed from HuggingFace
Transformerslibraryandusedwithoutfurtherfine-
tuning. Ithas125Mparameters,acontextwindow
of1024,hiddenstatedimensionof768,12heads
anddropoutof0.1.
12

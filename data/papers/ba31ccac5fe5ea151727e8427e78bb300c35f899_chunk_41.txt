1: Datastatistics.
secondstageofqueryingandannotationineach andmentiontasks. WefollowLinetal.(2020)for
AL cycle based on the annotated mentions in the pre-processing15 of the ACE dataset. For the
thefirststage. Thisextrastageonlyselectsrela- IEtasksonACE,wefindthattheconventionaltest
tionsthatinvolvethenewlyaddedorcorrected set contains only newswire documents while the
mentions. We simply reuse the selection ratio trainingsetconsistsofvariousgenres(suchasfrom
determined from the first stage and apply it to conversationandweb). Suchmismatchesbetween
each sentence that contains such mentions. In the AL pool and the final testing set are nontriv-
thisway,thesecondstageislightweightandonly ial to handle with the classical AL protocol, and
requires relatively cheap re-inference for each wethusrandomlyre-splittheACEdataset(witha
queriedsentenceindividually. ratioof7:1:2fortraining,dev,andtestsets,respec-
tively). Table1showsdatastatistics. ForeachAL
• Annotation. Theannotationofthementionsis
experiment,wetaketheoriginaltrainingsetasthe
thesameasintheNERtask,whilefortheannota-
unlabeled pool, down-sample a dev set from the
tionofrelationalqueries,theirmentionsarefirst
originaldevset,andevaluateonthefulltestset.
examinedandcorrectedifneeded,asexplained
in§3.4. Wemeasurethelabelingcostbythefi- MoreSettings. Allofourmodelsarebasedon
nalannotateditems;thus,theseextraexamined the pre-trained RoBERTa base as the contextual-
mentionedwillalsobeproperlycounted. ized encoder. We further fine-tune it with the
task-specific decoder in all the experiments. The
• Modellearning. Forthementionextractionsub- numberofmodelparametersisroughly124Mfor
task,thetrainingobjectiveisthesameasinNER. single-outputtasksandaround186Mformulti-task
For the relational sub-task, we simply
simpactonthemodelâ€™sefficiency.
ONNXisacross-platformstaticruntimethatusespre-compiledcomputationalgraphs. Itallows
foraggressive,globalahead-of-timecompileroptimizations,andcanbringsubstantiallatencyand
throughput improvements in inference settings with small batch size. The readers are referred
9https://github.com/oughtinc/raft-baselines
17
to https://onnx.ai/ for more details. As of now, ONNX supports conversion from models
implementedwithPyTorch,Tensorflow,andJAX,enablingustomakedirectcomparisonsbetween
PyTorchimplementationandONNXinourmachinetranslationexperimentswithWMT14DE-EN.
As shown in Figure 7, when comparing five different models in a single-stream scenario using
PyTorchandONNXruntime,ONNXdeliverssubstantialimprovementsinthroughput,latency,and
energyoverhead,especiallyforlargermodels. However,thiscomeswithanincreaseinGPUmemory
consumption,whichislikelyduetothestorageofpre-compiledcomputationalgraphsontheGPU.
WMT19MetaandWMT21,whichutilizetheFullyShardedDataParalleltechnique(FSDP;Zhao
etal.,2021),areexcludedfromthisexperimentduetocompatibilitychallengeswithONNXand
FSDP.
Our preliminary experiments find that ONNX brings marginal efficiency improvements in other
scenariosthatuselargerbatchsizes, whichisconsistentwiththeobservationbyFernandezetal.
(2023).
ResultsonWMT14EN->DE.
Figure6providesasummaryoftheefficiencyperformanceofvariousmodelsontheWMT14English-
to-German(EN->DE)translationtask. TheresultsareshownforbothFP32andFP16models. The
observedtrendsalignwiththosediscussedinSection3.
18
Throughput #Params FP32 Throughput #Params FP32
(words/s) FP16 (words/s) FP16
1600 10M 1600 10M
1200 100M 1200 100M
800 1B 800 1B
400 10
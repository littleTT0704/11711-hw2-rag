approximationpowerofsuchparame- tionproblems. Specifically,itavoidsintroducingextrabal-
terizationbythefollowingdefinitionof(cid:15)-parameterization. ancinghyperparameters. Moreover,itprovidesconvergence
Algorithm1:DDG:Disentanglement-constrained toderivetheboundonthefinalempiricaldualitygap, we
OptimizationforDomainGeneralization startbyprovingtwolemmasontheparameterizationgapand
empiricalgap. Specifically,theyelaboratethecorresponding
Input: D ={(x,y ),...,(x,y )},batchsizeB,
S 1 1 n n
approximationgapsoftwotransformations(i.e. Eq.(4)and
primalandduallearningrateη,η,Adam
1 2
(5))inabovesections.
hyperparametersβ,β,initialcoefficientsλ,
1 2
Wefirstdiscussthegapbetweenthefinite-dimensional
marginγ
modelparameterization(e.g.neuralnetworks)andthemodel
Initial: ParametersofDDG(i.e. parametersθ,φand
overinfinitefunctionalspaceF.
ψforsemanticencoderh,variationencoderh
s v
anddecoderD.) Lemma1 (Parameterization gap) With Assumption 2
repeat about(cid:96)andd,thegapbetweenoptimumofastatistical
fori,j =1,...,B,i(cid:54)=j do problemanditsfinitedimensional,deterministicversion
Li con = (cid:110) (cid:111) D ε(cid:63)(γ)−P(cid:63)canbeboundedas
max (cid:107)x i−D(h s(x i)⊕h v(x j))(cid:107) l1 −γ,0 0≤Dε(cid:63)(γ)−P(cid:63) ≤(cid:0) 1+|λ(cid:63) p|(cid:1) max{L (cid:96)(
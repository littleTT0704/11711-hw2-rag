ens.
put to the textual encoder is the concatenation of a [CLS]
token, word tokens, and a [SEP] token. The output of the 3.3 Multi-modalFusion
textualencoderisthequerytokenf
query
∈RCq×1 encoding
After deriving the textual tokens f and query-aware visual
thesentencelevelinformationofthegivenqueryandasetof q
tokens f from the encoders, they are further passed into a
wordtokensoftheinputqueryf
q
∈RCq×Nq whereN qisthe
fusion
mv
odule to generate a grounding representation. Fol-
numberofwordtokens,andC issetto768foreachtoken.
q lowingpreviousmethods[Duetal.,2021;Dengetal.,2021;
3.2 VisualEncoder
Nagranietal.,2021],weutilizetransformerencoderstocon-
duct multi-modal interaction. As shown in Figure 2(a), the
Toenablequery-awarevisualfeatureextraction,wepropose
transformer encoders take both f and f as inputs. In par-
aquery-conditionedconvolutionmodule(QCM)andreplace v q
ticular, the textual and visual tokens are first projected into
parts of the vanilla convolution in the visual encoder with it
the same dimension C separately. Let us denote the pro-
inamulti-scalemanner. jected visual tokens asr r
q
∈ RCr×Nq and textual tokens as
Query-conditioned convolution. A QCM block takes in r
v
∈ RCr×Nv. Inaddition,toforcemulti-modalinteraction,
the query token f query to compose a convolution kernel to weaddaregressiontokenr qv ∈ RCr×1 intothetransformer
extractquery-relevantfeaturesfromaninputfeaturemap. As encoders. Thefinalinputtothemulti-modaltransformeren-
showninFigure3,ineachQCMblock,thereareKlearnable codersistheconcatenationoftheregressiontoken,visualto-
candidateconvolutionkernels{w 1,w 2,...,w K
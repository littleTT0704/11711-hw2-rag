 the
experience function f and divergence D. Table 1 summarizes various specifications of the SE components that
recover a range of existing well-known algorithms from different paradigms. As shown in more detail in the
subsequent sections, the standard equation (Equations 3.1 and 3.2) offers a unified and universal paradigm for
model training under many scenarios based on many types of experience, potentiating a turnkey
implementation and a more generalizable theoretical characterization.
Table 1. Example configurations of the components in the standard equation
(Equations 3.1 and 3.2), which recover different existing algorithms. Here, ‘CE’ means
Cross Entropy; ‘JSD’ is the Jensen-Shannon divergence; ‘W dist.’ is the first-order Wasserstein
1
distance; and ‘KL’ is the KL divergence. Refer to Sections 4, 5, and 6 for more details. (Scroll or
use slider at bottom of table to see additional columns.)
Experience type Experience Divergence D α β Algorithm
function f
16
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
f data(x;D) CE 1 1 Unsupervised
MLE
Data instances f data(x,y;D) CE 1 ϵ Supervised MLE
f data-self(x,y;D)CE 1 ϵ Self-supervised
MLE
f data-w(t;D) CE 1 ϵ Data Re-weighting
f data-aug(t;D) CE 1 ϵ Data
Augmentation
f active(x,y;D) CE 1 ϵ Active
Learning (Ertekin
et al., 2007)
f rule(x,y) CE 1 1 Posterior
Knowledge
Regularization
(Ganchev et al.,
2010)
f rule(x,y) CE R 1 Unified EM
(Samdani et al.,
2012)
logQθ(x,y) CE 1 1 Policy Gradient
Reward
logQθ(x,y)+ QCin E,θ(x,y) 1 1 + Intrinsic Reward
Qθ(x,y) CE ρ > 0 ρ > 0 RL as Inference
Model fmimicking(x
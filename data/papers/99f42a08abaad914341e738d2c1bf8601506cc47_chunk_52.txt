Net100performssimilarlytoSFNet64in
betterresultsonMegaFaceandIJBaswell. the sense that HFN is slightly better than SFN and they
are both better than NFN. In general, both SphereFace
Second,differenttypesofmarginstendtoachievesimi-
and SphereFace-R are generally comparable to CosFace
larperformance,whiledifferenttypesofFNstrategieshave
and ArcFace. SphereFace with HFN achieves the best per-
significantly different results. In Fig. 6, we first show the
formance on MegaFace. More interestingly, we find that
training losses for different FN strategies. Fig. 6(a) shows
both SphereFace and SphereFace-R v1 with HFN achieve
that all the models equipped with NFN, HFN or SFN
significantlybetter1:1verificationperformanceatlowFAR
converge well on VGGFace2, a relatively small, clean and
thanallthecomparedmethodsonIJB.
high-quality training set. From Table 10, NFN and HFN
showcomparablegeneralizationability,whileSFNachieves
the best performance among all FN strategies. This implies 7 CONCLUDING REMARKS
thatmakinggooduseofthemagnitudeinformationduring Our paper proposes a novel framework that unifies hy-
trainingcaneffectivelyimprovetheresults.Fig.6(b)shows perspherical face recognition. This framework provides a
thatNFNisunabletoconvergetoasufficientlysmalltrain- general principle for a loss function to incorporate large
ing loss on MS-Celeb-1M, a large, noisy and low-quality angular margins. Under this framework, we substantially
dataset. From Table 11, we can observe that NFN indeed extendandimproveourpreviousworkonSphereFace[2]by
convergestoabadlocalminimathatgeneralizespoorly.By addressing training instability and significantly improving
introducingamagnituderegularizationtermtotheobjective empirical performance. Specifically, we propose two new
function, SFN can effectively help the models escaping types of multiplicative margins that effectively implement
from the bad local minima. In contrast to the results on theoriginalintuitionofSphereFace.Moreover,wealsocome
VGGFace2,wefindthatSFNperformsworseth

probabilities(i.e.,logits)z,z+,andz− fromthe
i i i
baseandexpertAE-LMsG,G+,andG−,respec- In our experiments, we focus on rewriting sen-
tences from three toxicity datasets, and use both
tively,conditionedonthepreviouslygeneratedto-
automatic and human evaluations to measure
kensg,theoriginalsequencew,andthemasked
<i
variantwm. Wethenensemblethoselogitsintoa MARCO’sperformanceatdetoxifyingtext.
modifiednext-tokenprobabilitydistribution:
4.1 Datasets
P(X |g,w,wm)=softmax(z +α z+−α z−)
i <i i 1 i 2 i We seek to rewrite English sentences that are al-
readyknowntobeorannotatedastoxic,especially
whereX isarandomvariableoverthevocabulary
i
sentences that contain more subtle or implicit bi-
V representingthenexttokenatindexigiventhe
ases(e.g.,withoutswearwords). Incontrasttothe
previousgenerationg,andourtwohyperparam-
<i
Jigsawcorpususedtofinetuneourexperts,weuse
etersα andα independentlycontroltheimpact
1 2
threeout-of-domaindatasetswithsubtletoxicity:
oftheexpertandanti-expertformoreflexibility.3
In our method, the expert and anti-expert use
Microagressions.com (MAgr)isapubliclyavail-
themaskedsequencew astheirinput,whilethe
m able Tumblr blog where users can anonymously
basemodelusestheunmaskedw. Intuitively,the
post about socially-biased interactions and utter-
base model tries to replicate the input sequence
ances in the wild. Each post includes an offend-
but is steered by an expert and anti-expert with
ingquoteand/oradescriptionoftheincident. We
contrastingprobabilitydistributionsatthemasked
scrape all quotes, resulting in a set of real-world
locations. Thisenablesrewriteswithminimalbut
microagressionutterances. Thevalid
-19-2-0201), the IARPA BET- inghow-toquestions. InProceedingsoftheSixthIn-
ternationalConferenceonLanguageResourcesand
TERProgram(contract2019-19051600004),and
Evaluation(LREC’08), Marrakech, Morocco.Euro-
theAmazonAlexaPrizeTaskBotCompetition. Ap-
peanLanguageResourcesAssociation(ELRA).
provedforPublicRelease,DistributionUnlimited.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
The U.S. Government is authorized to reproduce
KristinaToutanova.2018. Bert:Pre-trainingofdeep
anddistributereprintsforGovernmentalpurposes
bidirectional transformers for language understand-
notwithstanding any copyright notation thereon. ing. arXivpreprintarXiv:1810.04805.
The views and conclusions contained herein are
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
thoseoftheauthorsandshouldnotbeinterpreted Weizhu Chen. 2020. Deberta: Decoding-enhanced
asnecessarilyrepresentingtheofficialpolicies,ei- bert with disentangled attention. arXiv preprint
ther expressed or implied, of Amazon, DARPA, arXiv:2006.03654.
IARPA,ortheU.S.Government. Matthew Honnibal, Ines Montani, Sofie Van Lan-
WethankZiyangLiandRicardoGonzalezfor deghem, and Adriane Boyd. 2020. spaCy:
Industrial-strength Natural Language Processing in
developing the web demo, John Wieting for sup-
Python.
portonimplementation,andtheanonymouscrowd
workersfortheirannotations. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
and Jason Weston. 2019. Poly-encoders: Trans-
former architectures and pre-training strategies for
fast and accurate multi-sentence scoring. arXiv
References preprintarXiv:1905.01969.
Robert Abelson and Roger C Schank. 1977. Scripts, Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.
plans, goals
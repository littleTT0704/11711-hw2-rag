itecturesdesignedfor
ProcTHOR supports object interaction, and training
use with real images and, indeed, this has proven to
a simple RGB model with it using on-policy RL led
be the case. A recent work has shown that modi-
to state-of-the-art results for the Habitat 2022 Object-
fying existing embodied baseline models by replac-
NavChallenge,theRoboTHORObjectNavchallenge,
ing their visual backbones with a CLIP-pretrained
andtheAI2-THORRearrangementChallenge. More-
ResNet-50 can result in dramatic improvements [96].
over,0-Shotperformance,withmodelspre-trainedon
The top performing models of 1-Phase Rearrange-
ProcTHOR, often beats the same models trained on
ment,RoboTHORObjectNavleaderboards,andHabi-
the training data from the benchmark it is evaluated
tatObjectNavleaderboard,usevariantsofthis“Emb-
on. The scale and diversity of HM3D led to models
CLIP”architecture[52]. Severalothertopperforming
trained on it achieving state-of-the-art performance
modelstootherchallengesusepretrainedvisionmod-
forPointNavmodelswhenevaluatedonGibson[209],
els for object detection and semantic segmentation
MP3D[26],andHM3D[151].Usingimitationlearning
(RVSUSemanticSLAM,MultiON,andTwo-PhaseRe-
to train on Habitat-Web led to state-of-the-art results
arrangement).
inthe2021HabitatObjectNavChallenge,whichlater
improveditsperformancewithonlinefine-tuning.We
4.3.End-to-endvsModular
expectthetrendofbuildingandtrainingonmassively
larger datasets to continue leading to better general- In the last few years, two classes of methods have
ization. emerged for various embodied AI tasks: (1) end-to-
Simultaneously, many of the top approaches to end and (2) modular. The end-to-end methods learn
these challenges are scaling compute to train on topredictlow-levelactionsdirectlyfrominputobser-
17
BestEnd-to
.8)
∥φ∥L≤1
30
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
which is shown to be more robust than the original GAN algorithm based on the JS divergence (Section 5.2).
6. Dynamic SE
The standard equation Equation 3.1 so far has played the role of the ultimate learning objective that fully
defines the learning problem in an analytical form. As seen in Sections 4 and 5, many of the known algorithms
are special cases of the SE objective. On the other hand, in a dynamic or online setting, the learning objective
itself may be evolving over time. For example, the data instances may follow changing distributions or come
from evolving tasks (e.g., a sequence of tasks that are increasingly complex); the experience in a strategic game
context can involve complex interactions with the target model through co-training or adversarial dynamics;
and the success criteria of the model w.r.t. the experience can be adapting. In this section, we discuss an
extended view of the SE in dealing with the learning in such dynamic contexts. Instead of serving as the static
overall objective function, now the SE is a core part of an outer loop in the learning procedure.
More specifically, each of the SE components (e.g., experience function, divergence function, balancing
weights) can change over time. For example, consider a dynamic experience function f τ, which is indexed by
τ indicating its evolution over time, iterations, or tasks. This differs from the experience discussed earlier in
Section 4 that is defined a priori (e.g., a static set of data instances) and encoded as a fixed experience function
f. With the dynamic experience, a learning procedure, using the special case of SE in Equation 3.2 can be
written as:
for τ =1,2,...:
Acquire experience f,
τ (6.1)
Solve SE: min−αH(q)+βD(q,p θ)−Eq [f τ].
q,θ
Here the SE governs the optimization of the target model p given the updated experience at each τ. We
θ
discuss in more detail the SE with dynamic experience (Section 6.1) and other dynamic components
(Section 6.2), which further recovers several well-
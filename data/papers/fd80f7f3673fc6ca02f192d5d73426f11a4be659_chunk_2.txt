ofatranslation. Rather
theimpactoflabeleddatathroughin-context thanaskingforasinglequalityscore,AUTOMQM
learning and finetuning. We then evaluate promptsmodelstoidentifyandclassifyerrors,and
AUTOMQMwithPaLM-2models,andwe usestheMQMframeworktoproduceascore.
findthatitimprovesperformancecompared
to just prompting for scores (with particu- metrics is rapidly evolving. Learned automatic
larly large gains for larger models) while metricsthatleveragehuman-judgmentstofinetune
providinginterpretabilitythrougherrorspans language models (Sellam et al., 2020; Rei et al.,
thatalignwithhumanannotations.
2022a)currentlyrepresentthestate-of-the-artinau-
tomaticevaluationbenchmarksliketheWMTMet-
1 Introduction ricstask(Freitagetal.,2022),andshowhighcorre-
lationwithhumanjudgments. However,thesemet-
Evaluatingnaturallanguagegenerationsystemshas
ricstypicallyoutputasingle,uninterpretablequal-
always been challenging, and as the output qual-
ityscore,makingitdifficulttounderstandthetype
ityofthesesystemshasimproved,evaluationhas
andextentoferrorsidentifiedbythem. Thelackof
becomeevenmorechallengingandcritical. Forex-
insightsmakesitdifficultformodeldevelopersto
ample,inMachineTranslation(MT),afieldwhere
leveragethesemetricstoimprovetheirsystems.
evaluationhasgarneredconsiderableattention,pre-
Unlike automatic metrics that only provide a
viousstandardautomaticsurface-levelmetricssuch
singlescalarvalueasqualityscore,state-of-the-art
asBLEU(Papinenietal.,2002)arebecomingless
human evaluation methodologies like Multidi-
reliable as the quality of generation systems im-
mensionalQualityMetrics(MQM;Lommeletal.,
proves, with little remaining correlation with hu-
2014; Freitag et al., 2021a) ask professional
manjudgments(Freitagetal.,2022).
annotators to identify and
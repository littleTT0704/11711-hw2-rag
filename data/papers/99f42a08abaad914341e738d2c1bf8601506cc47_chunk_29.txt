showthatfeature
so that ∆(θ) is larger than zero. It is also possible that a normalizationcaneasethedifficultyofminimizingangular
multiplicativemarginlossandanadditivemarginlosslead margin losses and greatly improve the training stability.
tothesamecharacteristicfunction,andtheymaybetechni- Despitebeingeffectivetostabilizetraining,featurenormal-
callythesameloss.Therefore,theirdifferenceisdetermined izationinevitablylosesusefulinformationaboutindividual
bythespecificintuitionthatguidesthelossdesign. samples (e.g., image quality). Existing hyperspherical FR
Generality of multiplicative margin.SphereFace-Rv1and methods either preserve the feature magnitude in the loss
v2demonstratetwodifferentstrategiestoincorporatemul- function [1], [2], [32] or normalize the feature magnitude
tiplicative angular margin, showing the existence of many to constant s [9], [30]. To explore whether feature magni-
feasible designs to achieve multiplicative margin. In fact, tude can be beneficial to generalization, we systematically
theexactformofthelossfunctionisnotcrucialandthecore study SphereFace and SphereFace-R under NFN and HFN.
of multiplicative margin lies in the spirit of multiplying a Moreover,weconsiderasoftfeaturenormalizationmethod
factortoensurethecharacteristicfunctiontobelargerthan which effectively unifies NFN and HFN and serves as an
zero.Followingsuchaspirit,therearelikelymanypotential interpolationbetweenboth.
lossdesignsthatcanworkaswellasours. Hard feature normalization. HFN becomes a default com-
Comparison between SphereFace and SphereFace-R. It is ponent in current hyperspherical FR methods [3], [4], [5],
easy to see that SphereFace employs a surrogate character- [26], [40]. By normalizing the feature x to a constant s, the
istic function to implement the multiplicative margin and objective function value will merely depend on the angles
does not follow the intuition of multiplicative margin for between x and the classifiers W i,∀i. In order to perform
θ ∈ [π,π]wherem > 1.Incont
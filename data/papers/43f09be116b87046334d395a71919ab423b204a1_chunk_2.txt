 models have made substan- vocabulary,frequenttypos,orinconsistentsyntax.
tialstridesinperformanceonstandardbenchmark Coreferencemodelsstruggletoproducemeaning-
datasets such as the CoNLL-2012 shared task, fulrepresentationsfornewdomain-specificspans
where average F1 has improved by 20% since andmayrequiremanyexamplestoadapt(Uppunda
2016(DurrettandKlein,2013;Dobrovolskii,2021; etal.,2021;LuandNg,2020;Zhuetal.,2021).
Kirstain et al., 2021). Modern coreference archi- Further,coreferencemodelstrainedonstandard
tectures typically consist of an encoder, mention benchmarksarenotrobusttodifferencesinanno-
detector,andantecedentlinker. Allofthesecom- tation schemes for new domains (Bamman et al.,
ponents are optimized end-to-end, using only an 2020). Forexample,OntoNotesdoesnotannotate
antecedentlinkingobjective,soexpensivecorefer- singletonmentions,thosethatdonotcoreferwith
ence chain annotations are necessary for training anyothermention. AsystemtrainedonOntoNotes
(AralikatteandSÃ¸gaard,2020;Lietal.,2020a). would implicitly learn to detect only entities that
Theseresultshaveencouragedinterestindeploy- appearmorethanonce,eventhoughsingletonre-
ingmodelsindomainslikemedicineandchildpro- trieval is often desired in other domains (Zeldes,
tectiveservices,whereasmallnumberofpractition- 2022). Also,practitionersmayonlybeinterested
inretrievingasubsetofdomain-specificentities.
1Code is available at https://github.com/
nupoorgandhi/data-eff-coref Continuedtrainingontargetdomaindataisan
3202
yaM
03
]LC.sc[
2v20670.0122:viXra
effectiveapproach(XiaandVanDurme,2021),but 7-14%improvementsinF1across3domains,we
itrequirescostlyandtime-consuming
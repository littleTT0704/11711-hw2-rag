encoder. sentations, the parameter updates benefit perfor-
mance on the target language the most. On top
ofanexistingmultilingualpre-trainedmodel,we
2 MetaRepresentationTransformation
introduceanadditionalnetwork,whichwecallthe
representation transformation network to model
2.1 BackgroundandProblemDefinition
thistransformationexplicitly.
Therepresentationtransformationnetworkmod-
Thestandardpracticeincross-lingualtransferlearn-
els a function g : Rd → Rd, where d is the di-
ing is to fine-tune a pre-trained multilingual lan- φ
guagemodelf parameterizedbyθ,(e.g. XLM-R
θ 1Wealsorefertoauxiliarylanguagesassourcelanguages
andmBERT)withdatafromoneormoreauxiliary asopposedtotargetlanguages.
training loss meta loss
Repeat following steps until convergence:
Transformer Transformer
Layer Layer ① Forward pass for training loss
Representatio
nTransformatio
n Network Backward pass from training loss
Transformer (RTN) Transformer ② a (gn rd a dX iL eM nt- sR du ep pd ea nte d ency on RTN kept)
Layer Layer
③ Forward pass for meta loss
Updated XLM-R
Embeddings (a function of RTN) Embeddings Backward pass from meta loss
④
and RTN update
source data target data
Figure 2: Overview of MetaXL. For illustration, only two Transformer layers are shown for XLM-R, and the
representationtransformationnetworkisplacedafterthefirstTransformerlayer. (cid:13)1 sourcelanguagedatapasses
throughthefirstTransformerlayer,throughthecurrentrepresentationtransformationnetwork,andfinallythrough
theremaininglayerstocomputeatraininglosswiththecorrespondingsourcelabels. (cid:13)2 Thetraininglossisback-
propagatedontoallparameters,butonlyparametersofXLM-Rareupdated. TheupdatedweightsofXLM-Rare
a function of the current representation transformation network due to gradient dependency (highlighted by the
light-purplebackgroundoftheupdatedXLM-R).(
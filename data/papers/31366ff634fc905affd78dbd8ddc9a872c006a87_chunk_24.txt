 preferring re-
β vided the predictions of the Codex model (Chen
call over precision, results in a higher correlation
et al., 2021) (code-davinci-002) and their
with human preference in machine translation. In
corresponding functional correctness.6 We used
our experiments, we found that this applies to
Java, C++, Python, and JavaScript for these ex-
NL→Codeaswell.
periments, which are some of the most popular
Token Weighting Following Zhang et al.
programminglanguagesinopen-sourceprojects.7
(2020), we compute the inverse document fre- Notably,Cassanoetal.(2022)didnottranslatethe
quency (idf), according to a language-specific reference solutions to the other languages, so, we
test set, and weigh each token according to its collected these from HumanEval-X (Zeng et al.,
negativelogfrequency. 2022).8 The reference score of every example is
either 1 (“correct”, if it passes all test cases) or 0
Scaling Following Zhang et al. (2020), the co-
(“incorrect”,otherwise).
sine similarity scores of hidden states tend to lie
inalimitedrange. Thus,wecanlinearlyscalethe C CorrelationMetrics
resultingscores,usinganempiricalbasescalarb:
Kendall-Tau (τ) τ measures the ordinal/rank
(cid:92) CodeBERTScore−b association between a metric such as Code-
CodeBERTScore =
1−b
BERTScore and the reference measurement. It is
(6)
calculatedas:
This typically spreads the CodeBERTScore F
1
scores to the [0,1] range, and is merely a cos- |concordant|−|discordant|
τ =
metical change: this scaling does not change the
|concordant|+|discordant|
way CodeBERTScore ranks different prediction,
butcanbeslightlymoreintuitiveandeasiertoin- where|concordant|representsthenumberofpairs
terpret. We computed b empirically by sampling where two measurements agree on their relative
random unrelated code pairs and measuring their rank. That is, if
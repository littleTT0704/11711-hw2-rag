arXiv:2110.07574.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Anna Jobin, Marcello Ienca, and Effy Vayena. 2019.
AlexanderMiller.2019. Languagemodelsasknowl-
Thegloballandscapeofaiethicsguidelines. Nature
edge bases? In Proceedings of the 2019 Confer-
MachineIntelligence,1(9):389–399.
ence on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
YehudaKoren.2008. Factorizationmeetstheneighbor-
ence on Natural Language Processing (EMNLP-
hood: A multifaceted collaborative filtering model.
IJCNLP),pages2463–2473,HongKong,China.As-
In Proceedings of the 14th ACM SIGKDD Inter-
sociationforComputationalLinguistics.
national Conference on Knowledge Discovery and
Data Mining, KDD ’08, page 426–434, New York,
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
NY,USA.AssociationforComputingMachinery.
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
OpenAIblog,1(8):9.
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learn-
LariaReynoldsandKyleMcDonell.2021. Promptpro-
ing of language representations. arXiv preprint
gramming for large language models: Beyond the
arXiv:1909.11942.
few-shot paradigm. In Extended Abstracts of the
2021 CHI Conference on Human Factors in Com-
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
puting Systems, CHI EA ’21, New York, NY, USA.
jan Ghazvininejad, Abdelrahman Mohamed, Omer
AssociationforComputingMachinery.
Levy, Ves St
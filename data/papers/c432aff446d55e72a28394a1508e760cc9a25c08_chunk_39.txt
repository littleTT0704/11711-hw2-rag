previousattempt,
sparsifyingtheoutputprobabilitywiththetokensretrievedbythekNNsearch(butignoringthedistances
providedbythekNNsearch)ratherthanthetopktokensoftheLM,withandwithoutremovingduplicates. In
thebestcase,theymanagetoreducetheperplexityby0.5(whereaskNN-LMreducesbynearly2).
E.3 LocationwithinContextWindow
Supposedly,wordsinthebeginningofthe“contextwindow”ofthetransformerattesttimehavelesscontextual
informationthanwordstowardtheendofcontextwindow.
WehypothesizedthatmaybethebaseLMperformsworseinoneofthese(beginningvs. endofthecontext
window),andmaybekNN-LMprovidesahigherimprovementinoneofthese. Wemeasuredtheper-token
testperplexitywithrespecttothelocationofeachtokeninthecontextwindow. However,wedidnotfindany
significantcorrelationbetweentheperformanceofthebaseLMandthelocation,andnosignificantcorrelation
betweenthedifferencebetweenkNN-LMandthebaseLMandthelocation.
WealsohypothesizedthatmaybethebeginningofeveryWikipediaarticleismore“predictable”,andthetext
becomesmoredifficulttopredictasthearticlegoesintodetails. However,wealsodidnotfindanycorrelation
withthelocationofthewordwithinthedocumentitappearsin.
E.4 StolenProbabilities
Thestolenprobabilitieseffect(Demeteretal.,2020)referstothesituationwheretheoutputembeddingsof
anLMarelearnedsuchthatsomewordsaregeometricallyplacedinsidetheconvexhullthatisformedby
otherwordembeddings. Sincelanguagemodelsgenerateascoreforeveryoutputwordbycomputingthe
dotproductofahiddenstatewithallwordembeddings,Demeteretal.(2020)provethatinsuchacase,itis
impossibleforwordsinsidetheconvexhulltobepredictedastheLM’smostprobableword(the“argmax”).
We hypothesized that kNN
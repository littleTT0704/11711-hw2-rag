putationalLinguistics MarcusRohrbach.2018. Multimodalexplanations:
(Volume1:LongPapers),pages158–167,Vancouver, Justifyingdecisionsandpointingtotheevidence.
Canada.AssociationforComputationalLinguistics.
GuilhermePenedo,QuentinMalartic,DanielHesslow,
ZacharyCLipton.2018. Themythosofmodelinter- Ruxandra Cojocaru, Alessandro Cappelli, Hamza
pretability: Inmachinelearning, theconceptofin- Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
terpretabilityisbothimportantandslippery. Queue, andJulienLaunay.2023. TheRefinedWebdataset
16(3):31–57. for Falcon LLM: outperforming curated corpora
with web data, and web data only. arXiv preprint
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man- arXiv:2306.01116.
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019. AlecRadford,JongWookKim,TaoXu,GregBrock-
Roberta: A robustly optimized bert pretraining ap- man,ChristineMcLeavey,andIlyaSutskever.2022.
proach. arXivpreprintarXiv:1907.11692. Robustspeechrecognitionvialarge-scaleweaksu-
pervision. arXivpreprintarXiv:2212.04356.
MengsayLoem,MasahiroKaneko,andNaoakiOkazaki.
2023. Saieframework: Supportaloneisn’tenough ColinRaffel,NoamShazeer,AdamRoberts,Katherine
- advancing llm training with adversarial remarks. Lee,SharanNarang,MichaelMatena,YanqiZhou,
ArXiv,abs/2311.08107. WeiLi,PeterJLiu,etal.2020. Exploringthelimits
oftransferlearningwithaunifiedtext-to-texttrans
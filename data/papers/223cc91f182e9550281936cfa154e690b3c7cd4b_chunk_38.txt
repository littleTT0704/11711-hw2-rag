based experience is that the pretrained model directly measures the score of a given
configuration (x,y) with its (log-)likelihood:
f :=f msc oo dri en lg (x,y) =logp model′(y∣x). (4.24)
As a concrete example, consider learning a text generation model that aims to generate text y conditioning on
a given sentiment label x (either positive or negative). A natural form of experience that has encoded the
concept of ‘sentiment’ is a pretrained sentiment classifier, which can be used to measure the likelihood
(plausibility) that the given sentence y has the given sentiment x (Hu et al., 2017). The likelihood serves as
the experience function score used to drive target model training through the SE.
Knowledge distillation. Plugging the model-based experience function
fmimicking
(Equation 4.23) into SE
model
rediscovers the well-known knowledge distillation algorithm (Hinton et al., 2015). Specifically, by following
mimicking
the same SE specification of the supervised MLE (Section 4.1.1) and setting f = f, we obtain the
model
knowledge distillation objective:
Ex∗∼D,y~∼p model′(y∣x∗)[logp θ(y~ ∣x∗)], (4.25)
which trains the target model p by encouraging it to mimic the source model outputs (and thus the source
θ
model is also called ‘teacher’ model—in a similar sense to but not to be confused with the teacher-student
mechanism for optimization described in Sections 3 and 7).
5. Divergence Function
We now turn to the divergence term D(q,p θ) that measures the distance between the auxiliary distribution q
and the model distribution p θ in the SE. The discussion in the prior section has assumed specific case of D
being the cross entropy. Yet there is a rather rich set of choices for the divergence function, such as f-
divergence (e.g., KL divergence, Jensen-Shannon divergence), optimal transport distance (e.g., Wasserstein
distance), and so on.
To see a concrete example of how the divergence function may influence the learning, consider the experience
to be data instances with
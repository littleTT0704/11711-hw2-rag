 a completely unfamiliar letter string. In other words, Thequalityofcharacter-basedlanguagemodelsisconven-
thesurprisalofthesamewordwithandwithoutmisspellings tionally measured in Bits Per Character (BPC), which is the
orlettertranspositionswouldbesimilarbutnotthesame. To average surprisal, to the base 2, of each character. On held-
achieve this, we can use character-based language models, outdata,ourmodelachievesameanBPCvalueof1.28(SD
which are standard tools in natural language processing for 0.025), competitive with BPC values achieved by state-of-
dealingwitherrorsintheinput(e.g.,theworkbyBelinkov& the-art systems of similar datasets (e.g., Merity, Keskar, &
Bisk,2018,onerrorsinmachinetranslation). Socher,2018,reportaBPCvalueof1.23onWikipediatext).
Crucially,oncewehaveacharacter-basedsurprisalmodel, Intheintroductionwepredictedthatword-basedsurprisal
wecanderivepredictionsregardinghowerrorsshouldaffect is not able to model the reading time pattern we found in
reading. We predict that transpositions should be more sur- oureye-trackingexperiment. Inordertotestthisprediction,
prisingthanmisspellings,astheyinvolvecharactersequences we compare our character-level surprisal model to surprisal
that are unfamiliar to the model (e.g., innocetn contains the computed using a conventional word-based neural language
rarecharactersequencetn). Also,wepredictthatwordsthat model. Word-basedmodelshaveafixedvocabulary,consist-
occurintextswithahigherrorratearemoredifficulttoread ingofthemostcommonwordsinthetrainingdata;atypical
than words in texts with a low error rate: if the context of a vocabulary size is 10,000. Words that were not seen in the
wordcontainsfewerrors,thenweareablecomputeexpecta- training data, and rare words, are represented by a special
tionsforthatwordconfidently(resultinginlowsurprisal). If out-of-
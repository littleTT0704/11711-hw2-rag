inputcolumnsofeachdatasetandprependthe Here we use the Temporal dataset of Wu et al.
user’sinstructionstotheinputtoguidethemodel. (2023)asgroundtruthforevaluation.
Though Prompt2Model offers automated model
Finetuning We concatenate the retrieved and
evaluation(ongeneratedandretrieveddatasests),
generated datasets and shuffle them before train-
we use real benchmark datasets here to measure
ing the student model. We use the same default
ourpipeline’sabilitytotrainaccuratemodels.
hyperparameters forall tasks.3 Wetrain withthe
AdamWoptimizerwithlr = 5e-5for3epochs, LLM Baseline A primary goal of our work is
whichtakesroughlyonehourforalltasks. to train small models that can match or outper-
formLLMs. Tomeasuresuccesstowardsthisgoal,
3.6 Evaluation
wereport theperformance ofgpt-3.5-turboon
OurModelEvaluatorautomaticallyevaluatesmod- eachbenchmark. Weprovidegpt-3.5-turbo4 the
els for all tasks using three general-purpose met- same instruction and demonstrations provided to
rics: Exact Match,ChrF++(Popovic´,2015),and Prompt2Model,forfaircomparison.
BERTScore (Zhang et al., 2019). ChrF++ bal-
ances precision and recall to assess text genera- 5 ExperimentResults
tion quality. Exact Match measures how often
5.1 Downstreamperformance
themodeloutputperfectlymatchestheexactrefer-
ence. BERTScorecapturessemanticsimilaritiesde- How effective is Prompt2Model at producing a
high-qualitymodel? InTable1,weevaluatedmod-
spitedifferentwordingsorphrasingsbycomparing
els produced by Prompt2Model, as well as our
3Weempiricallyfindthatthesedefaulthyperparameters
areeffective,butweplanonimplementinghyperparameter 4Weusedgpt-3.5-turbo-0613,accessedbetweenJuly
selectionusinggeneratedvalidationdatainthefuture. 26andAugust6,2023.
Method SQu
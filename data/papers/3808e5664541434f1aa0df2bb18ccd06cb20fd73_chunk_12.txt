ociationsinthedatamightnotnecessar-
notedasR,R,andR,respectively;lower ily lead to debiased models trained on the same
ONI NOI OI
valuesindicatereductioninlexicalbiases. data. Overall, no single approach outperforms all
others across different categories for lexical debi-
Baselines We consider comparison against two asing.
naturalbaselines: avanillaRoBERTa-largeclassi-
fier trained on the original dataset (Original). We 4.3 QualitativeAnalysis
alsoconsiderabaselinetrainedonarandomselec- AqualitativestudyoftheFountaetal.(2018)test
tionofthetrainingdata(Random),forcomparison setshowsthepresenceofmanyannotationerrors.
with data filtering methods for debiasing. Each We show three representative annotation errors in
subsetistrainedon33%ofthetrainingdata. Table3. Thefirstexamplecontainsanatypicalex-
ample of toxicity, towards white folks, which the
4.2 ResultsforLexicalBiasReduction
annotators might have been unaware of. It also
First, we measure the reduction in lexical bi- containsalinkwhichannotatorshadaccessto,but
ases in filtered datasets, as given by AFLite and not models. The second contains the word p*ss
DataMaps. As shown in Table 1, subsets given whichtheannotatorsmayhavereliedfortheiras-
by AFLite and the ambiguous and hard regions sessment. Thethirdencouragesviolence/abuseto-
produced by DataMaps reduce the overall asso- wardsanidentitywhichisnâ€™ttypicallythetargetof
ciations between TOXTRIG words and toxicity, violence. Interestingly, the DataMaps-Easy pre-
compared to the original and random baselines; dictions agree with all the gold standard annota-
DataMaps-Hard has the largest reduction. On the tions;perhapssuchannotationerrorsandambigu-
other hand, as expected, DataMaps-Easy shows ity are responsible for the performance discussed
anincreased associationbetween TOXTRIG men-
11When we combine the bias-only model and the full
tionsandtoxicity,showingth
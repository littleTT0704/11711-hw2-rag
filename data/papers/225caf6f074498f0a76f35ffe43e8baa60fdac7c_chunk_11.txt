parallelsentencesgiventwocorpora(one
6.2 Evaluation being very large) in two distinct languages. Lan-
Weevaluateonthreetasks: semanticsimilarity,bi- guagesarealignedwithEnglishandconsistofGer-
textminingandquestionretrieval. Whilethefirst man (de), French (fr), Russian (ru), and Chinese
two are commonly used to evaluate multilingual (zh). Typically,onlyabout2.5%ofthesentences
sentence embeddings, we introduce question re- arealigned. FollowingSchwenk(2018),weeval-
trievalinthispaper. Ascanbeseenbyourresults, uate on the publicly available BUCC data. This
wefoundquestionretrievaltobesomewhatuncor- involvesscoringallpairsbetweenthesourcetarget
related to either of the latter two. For each task, sentences and finding the optimal threshold that
we use a collection of different datasets, detailed separates the data. Using the threshold, we can
below. computetheprecision,recall,andF 1 ofthealign-
ments. WereportF ×100inourresults.
1
Semantic Textual Similarity The goal of the Wecomparetwodifferentapproachesforfind-
semantic textual similarity tasks is to predict the ing the sentence alignments. In the first, BUCC
degreetowhichsentenceshavethesamemeaning (cosine),wecomputethecosinesimilaritybetween
as measured by human judges. The evaluation thenon-EnglishsourcesentencesandtheEnglish
metric is Pearson’s r ×100 with the gold labels, targetsentences,selectingthehighestscoringEn-
whichisconventionforthesetasks. glishsentenceasthematch. Inthesecond,BUCC
We make a distinction between two seman- (margin),wefollowArtetxeandSchwenk(2019a)
ticsimilarityevaluations,English-onlyandcross- and use a margin-based scoring approach, where
lingual. For the English-only evaluation, we fol- thefinalscoreofasentencepairisbothafunction
lowWietingetal.(2016)byaveragingtheyearly ofthescorebetweenthepairandthescoresofeach
performance
h ge r efe en tce to ___ Someone. Category: Phrases
TACoS 268 964 - 62 134 - hit Q: Two boys ____ in a bedroom.
MPII-MD 869 220 63 129 896 - - k lan uo gc hk - - p swla iny g with toys
MEDTest14 671 418 98 174 726 - pick up empty recycle bin
Combineallsources 2,925 5,927 352 598 2,093 - shave facial hair
Table 1. List of categories and number of collected words in Figure3.ExamplesofQApairsfordifferentcategoriesanddiffi-
three datasets. Last rows shows the number of all words and culty.Wordscoloredingreenarethecorrectanswers,anddifficult
phrases collected including those from image domains such as candidatesaremarkedinred.
MSCOCO[21].
4.TheProposedApproach
Ascandidateanswersmightbeambiguoustothecorrect To answer questions about present, past and future, we
answer,wesetasimilaritythreshold,andthenselect10of first introduce an encoder-decoder framework to represent
themasthefinalcandidates.WeshowexamplesofQApairs context. Wethenmapthevisualrepresentationtosemantic
indifferentcategoriesanddifficultyinFigure3. embeddingspaceandlearntorankthecorrectanswerwith
higherscore.
3.2.TaskDefinitions
4.1.LearningtoRepresentVideoSequences
Besidesdescribingthecurrentclip,weintroduceanother In this section, we describe our model of learning tem-
two tasks which are inferring the past and anticipating the poral context. We present an encoder-decoder framework
future. Inthetaskofdescribingthepresent,weuseallthree using Gated Recurrent Unit (GRU) [4]. Compared with
datasets for evaluation. As to the other two tasks which Long Short-Term Memory (LSTM) [12], GRU is concep-
arepastinferringandfuturepredicting,weperformexper- tually simpler with only two gates (update gates and reset
iments on TACoS and MPII-MD datasets only as they are gates) and no memory cells, while the performance on se-

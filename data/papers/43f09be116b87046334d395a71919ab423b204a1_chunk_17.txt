flatteningoutslightly rioratesfor> 104,butnotasrapidlyaswhenwe
after 1000 mentions. The baseline model contin- transferbetweenannotationstyleintheON→i2b2
ues to improve with more coreference examples. case. WhilementionsinCNsharethesamerolesas
Wherethereisscarcetrainingdata(100-1000men- thoseini2b2,sometypesofmentions,(e.g. PROB-
tions),mentionannotationsaremoreeffectivethan LEM),aremoredifficulttoidentify. Unlikesettings
coreference ones. This effect persists when we wherewetransferbetweenannotationstyles,when
evaluatewithoutsingletons(Figure5). annotation style remains fixed, the performance
Thebaselinelikelyonlyidentifiesmentionsthat improvementfromourmodelincreaseswithmore
fitintothesourcedomainstyle(e.g. PEOPLE). Be- targetdomaindata. Thissuggeststhatadaptingthe
causethebaselinemodelassignsnopositiveweight mentiondetectorisespeciallyusefulwhentransfer-
in the coreference loss for identifying singletons, ringwithinanannotationstyle.
ini2b2,entitiesthatoftenappearassingletonsare Given coreference annotations, we find that
missedopportunitiestoimprovethebaselinemen- reusing the annotations to optimize MD with
T
tiondetector. Withenoughexamplesandmoreenti- high-prec. c2fpruningboostsperformanceslightly
tiesappearinginthetargetdomainasnon-singleton, whentransferringwithinanannotationstyle. This
however,thepenaltyofthesemissedexamplesis is evident in the i2b2→CN case regardless of
smaller, causing the baselinemodel performance whethersingletonsareincludedintheoutput.
toapproachthatofourmodel. Figure 3 reports results for the genre-to-genre
experimentswithinON.Forequivalentannotator
6.2 SilverMentionsImprovePerformance
time our model achieves large performance im-
FromFigure2,approximately250goldmentions provementsacrossmostgenres. Sinceourmodel
arenecessaryforsufficientmentiondetectionper- resultsinsignificant
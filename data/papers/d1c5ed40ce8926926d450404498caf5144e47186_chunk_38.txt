 segmentation of whole newswire documents.
The language modeling features were designed to separate longer units of text, and
we found it hard to identify trigger words that indicate transitions between topics in
our much noisier and heterogeneous web data. However, we attempted to incorporate
20 CHAPTER 2. RELATED WORK
the idea of trigger words in a more generic way using transition language models.
In Section 8.2 we describe a sequential relevance model that leverages these tran-
sition features, and we compare it to linear models that select relevant text passages
without taking the lexical coherence of their source documents into account.
Chapter 3
Fundamentals
In this chapter we introduce a canonical QA pipeline and discuss how additional
knowledge sources generated through statistical source expansion can be integrated,
andhowtheycanimprovesystemperformance(Section3.1). Wealsogiveanoverview
of QA tasks on which we evaluate the impact of source expansion (Section 3.2) and
performance metrics adopted in our experiments (Section 3.3). Finally, we describe
existing QA systems that are used in our task-based evaluation (Section 3.4).1
3.1 Pipeline for Question Answering
Although a variety of architectures have been adopted by question answering sys-
tems, the vast majority of systems are based on a core pipeline of components for
question analysis, query generation, search, candidate answer generation and answer
scoring [Hickl et al., 2007, Shen et al., 2007, Schlaefer et al., 2007, Ferrucci et al.,
2010]. The question analysis component derives syntactic and semantic information
from the question, using techniques such as answer type classification, syntactic and
semantic parsing, and named entity recognition. In the query generation stage, this
informationistransformedintoasetofsearchqueries,oftenwithsomedegreeofquery
expansion, whicharepassedtothesearchcomponenttoretrieverelevantcontentfrom
a collection of knowledge sources. The search results are processed by the candidate
generation component, which extracts or generates candidate answers of the desired
granularity (e.g. factoid answers or definitional phrases). The answer scoring compo-
nent estimates confidence scores for the candidate answers, ranks the candidates by
confidence and often merges similar candidates. At this stage, the knowledge sources
can be reused to retrieve supporting evidence for the most
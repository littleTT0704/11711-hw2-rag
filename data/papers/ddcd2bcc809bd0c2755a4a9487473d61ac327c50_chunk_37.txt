struct, j2-jumbo, j2-grande,
j2-large;GenerationbyCompletion.executefunc-
tion; temperature=0, max_tokens=50, topKRe-
turn=0,topP=1,withoutanypanalty
8.4 CompleteResults
Table7containstheexhaustiveaccuracyresultsfor
allLLMsonalldatasets.
Running the well-organized code provided by
Kosinski(2023)wefoundthattask2(Unexpected
TransferTask)scoredlowerthanreportedforGPT
3.5. Specifically, two samples resulted in clear
mispredictionsandonesamplehadborderlinepre-
dictionsthatprovidedthecorrectanswerbutina
formatthatdifferedfromtheexpectedanswer(i.e.,
the first word was not the expected answer). As
a result, the score for task 2 was either 85% or
Figure6: AnillustrationofUllman’sVariationsforthe
90%, and the average score across the two tasks
unexpected contents task. Image taken from Ullman
waseither85%or87.5%,whichislowerthanthe
(2023).
reportedaverageof93%.
8.5 “Emergence”ortestdatacontamination?
WewouldliketodeterminewhetherLLMsgener-
alizeormemorizewhentheysolvetheToMtasks
(Daumé, 2017). We explored the possibility that
theincreaseinperformanceisaresultoftraining
on the test data itself. for that purpose we used
a second, secret, test set for SocialIQa that was
purposefullykepthiddentoavoiddatacontamina-
tionandisonlyavailabletotheoriginalSocialIQa
authorsaswellasthroughtheAI2leaderboard.13
Foreachtestset(i.e.,thestandardandsecrettest
sets)werandomlysample11subsetsof100ques-
tions on which we evaluate gpt3.5-turbo-0301
andgpt-4-0314. Comparingtheperformanceof
bothmodelsonbothtestsetssampleswithaT-test,
wefoundnosignificantdifferences,makingitin-
conclusivewhether
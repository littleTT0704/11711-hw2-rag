[48]extractsobject-centric
representationsfromRGBimagesanddemonstratesgeneralizationinsequentialmanipulationtask.
Inspiredfromthese,welearnpoliciesfordishwasherloading,choosingfromvisibleobjectstomake
pick-placedecisions.
Transformers for sequence modeling Transformers in NLP [18] and vision [49] have focused
on self-supervised pretraining due to abundant unsupervised data available. Recent works have
repurposedtransformersforothersequencemodelingtasks[50,51,52,53,54,55].PromptDecision
Transformer[56]considersasinglemodeltoencodethepromptandthesuccessivesequenceofstate.
WeconsiderstateasvariablenumberofinstancePlaTe[51]proposesplanningfromvideos,while
[53,54,55]modelasequentialdecision-makingtask. Weconsiderlong-horizontaskswithpartially
observablestatefeatures,anduser-specificpreferences.
Preferences and prompt training In literature, there are several ways of encoding preferences.
[24]proposeVAEtolearnuserpreferencesforspatialarrangementbasedonjustthefinalstate,while
ourapproachmodelstemporalpreferencefromdemonstrations.Preference-basedRLlearnsrewards
basedonhumanpreferences[57,58,59,60],butdonotgeneralizetounseenpreferences. Oncom-
plex long-horizon tasks, modeling human preferences enables faster learning than RL, even with
carefullydesignedrewards[61]. Weshowgeneralizationtounseenpreferencesbyusingprompts.
Largelanguageandvisionmodelshaveshowngeneralizationthroughprompting[62,63]. Prompt-
ingcanalsobeusedtoguideamodeltoquicklyswitchbetweenmultipletaskobjectives[64,65,66].
Specifically, language models learn representations that be easy transferred to new tasks in a few-
shotsetting[67,68,19,62]. Ourapproachsimilarlyutilizespromptsforpreferencegeneralization
insequentialdecision-makingrobotictasks.
5 ConclusionsandLimitations
WepresentTranformerTaskPlanner(TTP):ah

Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham
song Su. 2021. Bridging subword gaps in pretrain-
Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao,
finetune paradigm for natural language generation.
Heyan Huang, and M. Zhou. 2021. Infoxlm: An
In Proceedings of the 59th Annual Meeting of the
information-theoretic framework for cross-lingual
Association for Computational Linguistics and the
languagemodelpre-training. InNAACL.
11thInternationalJointConferenceonNaturalLan-
guage Processing (Volume 1: Long Papers), pages
Jonathan H. Clark, Dan Garrette, Iulia Turc, and John
6001–6011, Online. Association for Computational
Wieting. 2022. Canine: Pre-training an Efficient
Linguistics.
Tokenization-Free Encoder for Language Represen-
tation. TransactionsoftheAssociationforComputa-
YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey
tionalLinguistics,10:73–91.
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
Jonathan H. Clark, Jennimaria Palomaki, Vitaly Niko-
pre-trainingforneuralmachinetranslation. Transac-
laev, Eunsol Choi, Dan Garrette, Michael Collins,
tions of the Association for Computational Linguis-
and Tom Kwiatkowski. 2020. Tydi QA: A bench-
tics,8:726–742.
markforinformation-seekingquestionansweringin
typologicallydiverselanguages. Trans.Assoc.Com-
put.Linguistics,8:454–470. Antonis Maronikolakis, Philipp Dufter, and Hinrich
Schütze.2021. Wineisnotvin.onthecompatibil-
Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad- ity of tokenizations across languages. In Findings
inaWilliams,SamuelR.Bowman,Holger
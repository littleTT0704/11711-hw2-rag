thehighestcosinesimilarity. Wefoundwe
couldachievethestrongestperformancebytying
Most previous work for cross-lingual representa-
all parameters together for each language, more
tions has focused on models based on encoders
precisely,θ andθ arethesame.
from neural machine translation (Espana-Bonet src tgt
etal.,2017;SchwenkandDouze,2017;Schwenk,
Negative Sampling. Negative examples are se-
2018)ordeeparchitecturesusingcontrastivelosses
lectedfromthesentencesinthebatchfromtheop-
(Grégoire and Langlais, 2018; Guo et al., 2018;
posinglanguagewhentrainingwithbitextandfrom
Chidambaram et al., 2019). Recently, other ap-
any sentence in the batch when using paraphrase
proachesusinglargeTransformer(Vaswanietal.,
data. In all cases, we choose the negative exam-
2017)havebeenproposed,trainedonvastquanti-
plewiththehighestcosinesimilaritytothegiven
tiesoftext(Conneauetal.,2020;Liuetal.,2020;
sentences,ensuringthatthenegativeisnotinfact
Tran et al., 2020). We primarily focus our com-
pairedwithsinthebatch. Toselectevenmorediffi-
parisonforthesesettingsonLASER(Artetxeand
cultnegativeexamplesthataidtraining,weusethe
Schwenk,2019),amodeltrainedforsemanticsim-
mega-batchingprocedureofWietingandGimpel
ilarity across more than 100 languages. Their
(2018),whichaggregatesM mini-batchestocreate
modelusesanLSTMencoder-decodertrainedon
one “mega-batch” and selects negative examples
hundreds of millions of parallel sentences. They
fromthismega-batch. Onceeachpairinthemega-
achievestate-of-the-artperformanceonavarietyof
batch has a negative example, the mega-batch is
multilingualsentenceembeddingstasksincluding
splitbackupintoM mini-batchesfortraining. Ad-

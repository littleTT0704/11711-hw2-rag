, RSC performs only marginally better than competing methods in photo
domain, which is probably because that photo domain is the simplest one and
every method has already achieved high accuracy on it.
5 Discussion
StandardImageNetBenchmark:Withtheimpressiveperformanceobserved
inthecross-domainevaluation,wefurtherexploretoevaluatethebenefitofRSC
with other benchmark data and higher network capacity.
14 Huang et al.
ImageNet backbone Top-1Acc↑Top-5Acc↑#Param.↓
Baseline ResNet50 76.13 92.86 25.6M
RSC(ours) ResNet50 77.18 93.53 25.6M
Baseline ResNet101 77.37 93.55 44.5M
RSC(ours)ResNet101 78.23 94.16 44.5M
Baseline ResNet152 78.31 94.05 60.2M
RSC(ours)ResNet152 78.89 94.43 60.2M
Table 10. Generalization results on ImageNet. Baseline was produced with official
Pytorch implementation and their ImageNet models.
WeconductedimageclassificationexperimentsontheImagenetdatabase[22].
Wechosethreebackboneswiththesamearchitecturaldesignwhilewithclearhi-
erarchiesinmodelcapacities:ResNet50,ResNet101,andResNet152.Allmodels
were finetuned for 80 epochs with learning rate decayed by 0.1 every 20 epochs.
The initial learning rate for ResNet was 0.01. All models follow extra the same
trainingprototypeindefaultPytorchImageNetimplementation1,usingoriginal
batch size of 256, standard data augmentation and 224×224 as input size.
TheresultsinTable10showsthatRSCexhibitstheabilityreducetheperfor-
mance gap between networks of same family but different sizes (i.e., ResNet50
with RSC approaches the results of baseline ResNet101, and ResNet101 with
RSC approaches the results of baseline ResNet151). The practical implication
is that, RSC could induce faster performance saturation than increasing model
sizes. Therefore one could scale down the size of networks to be deployed at
comparable performance.
6 Conclusion
We introduced a simple training heuristic method
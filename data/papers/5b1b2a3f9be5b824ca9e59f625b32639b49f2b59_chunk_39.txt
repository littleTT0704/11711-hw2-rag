-connected, con-
constant of attention layers Lip (f) is bounded [27]. Since
volutional and self-attention layers. We refer to the proof in p
the input images are normalized and the network is trained
[27] with bounded inputs.
withadamw[41]whichpenalizesthelargeweights,theinputs
a) High-level Proof: Continuous functions are Lipschitz
to attention layer is bounded. Therefore, we can assume the
continuous given bounded inputs w.r.t. p-norm where p ∈
attention layer in our network is bounded with normalized
[1,∞].
image inputs.
Definition1:Giventwometricspaces(X,d )and(Y,d ),
X Y
a function f : X → Y is called Lipschitz continuous (or K-
Lipschitz) if there exists a constant K ≤0 such that C. Entire Transformer-based Network
Our transformer-based network is a composition of CNN,
d Y(f(x),f(x(cid:48)))≤Kd X(x,x(cid:48)) (7) FC, and attention layers. By leveraging Lemma 1 on the
normalized image space I, we have
The smallest such K is the Lipschitz constant of f, denoted
Lip(f). Lip (Model)≤Lip (CNN)·Lip (FC)·Lip (Attn)
p p p p
In the following proof, the d and d are induced by a p-
norm (cid:107)x(cid:107) :=
((cid:80)
|x
|p)1/p.X FollowinY
g [27], we emphasise
where Lip p(Model), Lip p(CNN), Lip p(FC) and Lip p(Attn)
p i i aretheLipschitzconstantsoftheentiremodel,allCNNlayers,
the dependence of the Lipschitz constant on the choice of p-
all FC layers and all attention layers respectively.
normbydenotingitasLip (f).Inthiscase,itfollowsdirectly
p
from Definition 1 that the Lipschitz constant is given by

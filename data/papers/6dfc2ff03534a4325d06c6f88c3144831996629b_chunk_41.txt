selectwhethertheimagewasespeciallyinterestingor
boring,asthisallowsustotrainadeepmodelforpredicting
imageinterestingness.
bels.15 We used this model to select the most interesting
40k images from Movieclips, which finished off the anno-
tationprocess.
“drained”bythehighqualityrequired.
B.3.Crowdsourcingqualitydata Automated quality checks We added several auto-
matedcheckstothecrowdsourcingUItoensurehighqual-
As mentioned in the paper, crowdsourcing data at the
ity. The workers had to write at least four words for the
quality and scale of is challenging. We used several
VCR question, three for the answer, and five for the rationale.
bestpracticesforcrowdsourcing,whichweelaborateonin
Additionally, the workers had to explicitly refer to at least
thissection.
one detection on average per question, answer, and ratio-
WeusedAmazonMechanicalTurkforourcrowdsourc- naletriplet. Thiswasautomaticallydetectedtoensurethat
ing. A screenshot of our interface is given in Figure 12. theworkerswerereferringtothedetectiontagsintheirsub-
Given an image, workers asked questions, answered them, missions.
andprovidedarationaleexplainingwhytheiranswermight
We also noticed early on was that sometimes workers
becorrect. Theseareallwritteninamixtureofnaturallan-
would write detailed stories that were only loosely con-
guagetext,aswellasreferringtodetectionregions. Inour
nectedwiththesemanticcontentoftheimage. Tofixthis,
annotation UI, workers refer to the regions by writing the
workers also had to self-report whether their answer was
tagnumber.16
likely(above75%probability),possible(25-75%probabil-
Workerscouldaskanywherebetweenonetothreeques-
ity),orunlikely(below25%probability).Wefoundthatthis
tionsperHIT.Wepaidtheworkersproportionallyat$0.22
helpeddeterworkersfromcomingupwithconsistentlyun-
per triplet. According to workers, this resulted
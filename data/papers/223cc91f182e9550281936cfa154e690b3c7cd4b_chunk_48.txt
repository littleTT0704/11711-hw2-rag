, Fall 2022 Toward a 'Standard Model' of Machine Learning
Multiplicative weights. To update the distribution (i.e., normalized weight vector) over the K experts at each
time τ, we treat the distribution as the target model p to be learned in the SE. In other words, p is directly
θ θ
parameterized as the normalized weight vector p := θ = {θ }K, where θ ≥ 0 is the probability of
θ t t=1 t
K
expert t, with ∑ t=1θ t = 1. Starting from the SE in Equation 3.2, and assuming D to be the cross entropy,
H the Shannon entropy, and α = β > 0, we obtain the update rule of the target model at each time τ
following the teacher-student mechanism in Equation 3.3. Specifically, the teacher step has:
Teacher: q(τ+1)(t) =p θ(τ)(t)exp{α−1f τ(t)} / Z. (6.8)
The subsequent student step is to minimize the cross entropy between the target model p and q(τ+1) (see
θ
Equation 3.3). Given the definition of p as the vector of probabilities, the student step is equivalent to directly
θ
setting p to the vector of q(τ+1)(t) values. Therefore:
θ
Student: p θ(τ+1)(t) =p θ(τ)(t)exp{α−1f τ(t)} / Z, (6.9)
which is precisely the multiplicative weight update rule for the expert distribution/weights. That is, at each
time, the weights are updated by multiplying them with a factor that depends on the reward.
6.2. Dynamic SE as Interpolation of Algorithms
Besides the dynamic experience, other components in the SE, such as the divergence function D and the
balancing weights (α,β), can also be indexed by the time τ with desired evolution. In particular, the previous
sections have shown that the different specifications of the SE components correspond to different specific
algorithms, many of which are well-known and have different properties. The dynamic SE with the evolving
specifications, therefore, can be seen as interpolating between the algorithms during the course of training
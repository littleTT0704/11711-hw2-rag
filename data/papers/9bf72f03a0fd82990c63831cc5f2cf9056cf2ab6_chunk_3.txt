performance
timationofgradientsduetotheSTE-stepthrough
gain over other state-of-the-art quantization
nondifferentiablelayers. Toalleviateerrorscaused
methodsatlowbitwidth.
bysuchcoarseestimation,LSQ(Esseretal.,2020)
proposedtohavethequantizationbinwidth,orstep
3. Measurement of sharpness verifies that the
size,updateproportionaltotheweightsupdatedur-
modelstrainedwithourSQuATindeedcon-
ingtraining. Suchfinergrainestimationachieves
vergeintoflatterminimacomparedtoLSQ.
impressiveperformancewhenquantizingResNet
models(Esseretal.,2020),butsuffersheavierac- 2 Background&Relatedworks
curacy loss when quantizing larger Transformer
2.1 LearnableparametersQuantization
models. Thisperformancegapbetweenquantized
ResNetsandTransformerscouldbeexplainedby Althoughthecommunityseessomerenewedinter-
thedifferentgeometryoftheirlosssurface,where estinpost-trainingquantization(PTQ),itstilllags
Transformerswereshowntohaveamuchsharper behind quantization-aware training (QAT) meth-
losslandscapethanResNets(Chenetal.,2021). odsinaccuracy,asisconcludedbyGholamietal.
In the meantime, the line of research that fo- (2021). Hence, in this work, we focus on QAT,
cuses on the flatness of minima has garnered in- whichwasfirstintroducedbyJacobetal.(2018).
creasinginterest. Strongempiricalevidenceshows Severalworksproposedlearning-basedapproaches
that SAM (Foret et al., 2020) optimization can toimproveQATaroundthesametime: Jainetal.
improve model generalization by simultaneously (2020)proposedtolearnthequantizersâ€™sdynamic
minimizinglossvalueandlosssharpness. Models rangewhiletraining;Uhlichetal.(2020)advocated
trainedwiththisobjectiveachievedbettergeneral- for learning the optimal bit-width. Among them,
ization across many tasks in the NLP and vision LSQ(Esseretal.,2020
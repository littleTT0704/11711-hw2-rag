q]−β 0Eq [logp θ]:=(α 0 −β 0)Eq [logp θ]+β 0KL(q,p θ), (5.3)
where the KL divergence term corresponds to D in SE if we see α = α 0 −β 0 and β = β 0.
5.2. Jensen-Shannon (JS) Divergence
JS divergence provides another common choice for the divergence function:
1 1
D(q,p θ) =JS(q∥p θ) = KL(q∥h)+ KL(p θ∥h), (5.4)
2 2
where h := 1(q +p ) is the mean distribution. In particular, by considering the specific instantiation in
2 θ
Equation 5.1 of the SE and setting D to the JS divergence, we can derive the algorithm for learning the
generative adversarial networks (GANs; Goodfellow et al., 2014) as shown below. From this perspective, the
29
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
key concept in generative adversarial learning, namely the discriminator, arises as an approximation to the
optimization procedure. We discuss in Section 6 an alternative view of the learning paradigm where the
discriminator plays the role of ‘dynamic’ experience in SE.
Generative adversarial learning: The functional descent view. To optimize the objective in Equation 5.1
with the JS divergence, probability functional descent (PFD, Chu et al., 2019) offers an elegant way that
recovers the optimization procedure of GANs originally developed in Goodfellow et al. (2014). Here we give
the PFD result directly, and provide a more detailed review of the PFD optimization in Section 7.
Specifically, let J(p) := D(p d,p) in Equation 5.1, which is a functional on the distribution p ∈ P(T ).
The PFD approach (Chu et al., 2019) shows that minimizing J(p) w.r.t p can equivalently be done by solving
the following saddle-point problem:
inf supEp [φ(t)]−J∗(φ),
(5.5)
p φ
where �
 areobtainedatthetokenlevel,leadingtoasignifi-
mBERTmodelonNER.WithMetaXL,theHausdorff cantperformancegainof3F1score. ForNER,we
distance drops from 0.60 to 0.53 and the F1 score in- observeacorrelationof0.4betweenperformance
creasesfrom60.25to63.76.
improvement and the reduction in representation
distance. Bothqualitativevisualizationandquanti-
tativemetricsconfirmourhypothesisthatMetaXL
4.5 AnalysisofTransformedRepresentations
performs more effective transfer by bringing the
representationsfromdifferentlanguagescloser.
ToverifythatMetaXLdoesbringthesourceand
targetlanguagespacescloser,wequalitativelyand 4.6 AdditionalResultsonHigh-resource
quantitativelydemonstratetherepresentationshift Languages
withtransformation. Inparticular,wecollectrepre-
sentationsofboththesourceandtargetlanguages
fr es ru zh
from the joint training and the MetaXL models,
with mBERT as the multilingual encoder, and JT 76.50 72.87 71.14 60.62
presentthe2-componentPCAvisualizationinFig- MetaXL 72.43 70.38 71.08 58.81
ure1forSAandFigure3forNER.SAmodelsare
trainedonTelugupairedwith5kEnglishexamples, Table 7: F1 on mBERT rich languages in a simulated
low-resourcesetting.
and NER models are trained on Quechua paired
with5kEnglish. Fromthefigures,MetaXLmerges
Despiteourexperimentssofaronextremelylow-
therepresentationsfromtwolanguagesforSA,but
resourcelanguages,givenbyfewlabeleddatafor
thephenomenonisnotasevidentforNER.
fine-tuningandlimitedornounlabeleddataforpre-
Singh et al. (2019) quantitatively analyze
training,MetaXLisgenerallyapplicabletoalllan-
mBERT representations with canonical correla-
guages. Tobetterunderstandthescopeofapplying
tionanalysis(CCA).However,CCAdoesnotsuit
MetaXLtolanguageswithvaryingresources,we

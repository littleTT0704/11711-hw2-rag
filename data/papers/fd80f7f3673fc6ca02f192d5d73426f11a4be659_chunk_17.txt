LM-2 UNICORN achievingpoorperfor-
(1)SpanPrecision(SP),whichmeasurestheover- manceatboththesystemandsegmentlevels.7
lapofpredictedspansandgold(annotated)spans;
Nevertheless,PaLM-2modelsachievehighcor-
and(2)Majorrecall(MR),whichcapturestheper-
relationswithhumanjudgments,andthereference-
centageofgoldmajorerrorsthatwerepredictedas
lessPaLM-2BISONiscompetitivewiththelearned
errors(eitherminorormajor).
baselines,particularlyatassessingalternativetrans-
Moreformally,considerthesetofgroundtruth lations of the same sentence (acc∗). When com-
spansS⋆,whereeachspanconsistsofasequenceof
paringPaLM-2modelswithKocmietal.(2022)’s
words, i.e., s = (w,w,···). Let S⋆ ⊆
i (a) (a+1) maj GPT-based GEMBA evaluator (Table 3), we see
S⋆ bethesubsetcontainingonlythemajorerrors.
that both families of LLMs perform similarly,
Given a span set S, we define its positional set
with PaLM-2 models exhibiting higher system-
P(S)asthesetcontainingthepositionsofallthe
levelperformancethanGPT-basedGEMBA,while
wordsineveryspaninS. Forexample,assuminga
GEMBA achieves better segment-level accuracy,
spans = (w,w,···)inS startsatthenth
i (n) (n+1) particularlyinthereference-lesssetting.
positioninthetext,itscorrespondingpositionalset
Figure 4 shows the distribution of scores pro-
willincludethepositions{n,n+1,...,n+len(s )−
i duced by PaLM- and PaLM-2-based evaluators.
1}. Then for a set of predicted spans Sˆ, SP and
Wefindthat,despitebeingpromptedtogiveascore
MRaredefinedas
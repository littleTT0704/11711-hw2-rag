30±[.8412:.8724].7439±[.7430:.7484].7163±[.7130:.7239].8002±[.7919:.8100]
Furthermore,anotherbenefitofSPARSEMAXisthatitproducesasmallsubsetofactiveheads. The
heatmaps of attention coefficients (λ ) learned after training, shown in Figure 5, exemplify this.
T
We can see that the dependency between head position (layer it belongs to) and its coefficient is
task/dataset/model specific, with CIFAR-100 and MLQE having opposite observations. We also
foundempiricallythatactiveheads(λh >0)usuallyleadtohigherplausibilityscoresthaninactive
T
heads,furtherreinforcingthegoodplausibilityfindingsofSMaT.
Head coefficients Head coefficients Head coefficients
0 0 0.08 0
1 0.25 1 1
0.07 0.08
2 2 2
3 0.20 3 0.06 3
4 4 0.05 4 0.06
5 0.15 5 5
6 6 0.04 6
0.04
7 0.10 7 0.03 7
8 8 8
9 0.05 9 0.02 9 0.02
10 10 0.01 10
11 11 11
0.00 0.00 0.00
0 1 2 3 0 1 2 3 4 5 6 7 8 9 1011 0 1 2 3 4 5 6 7 8 9 1011
Head Head Head
Figure5: Headcoefficientsfortextclassification(left),imageclassification(middle),andquality
estimation (right), illustrating that only a small subset of attention heads are deemed relevant by
SMaTduetoSPARSEMAX.
7 RelatedWork
Explainabilityfortext&vision.Severalworksproposeexplainabilitymethodstointerpretdecisions
madebyNLPandCVmodels. Besidesgradientandattention-basedapproachesalreadymentioned,
someextractexplanationsbyrunningthemodelswithperturbedinputs[Ribeiroetal.,2016,Feng
etal.,2018,Kimetal.,2020].Othersevendefinecustombackwardpassestoassignrelevanceforeach
feature[Bachetal.,2015
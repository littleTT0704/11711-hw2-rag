 0.25 for reading comprehension and
fill-in-the-blank, though with 0.5 F1 on out-of-domain fill-in-the-blank.
Background knowledge. Bha¯skara performs above 0.5 F1 only for prob-
lems requiring commonsense and math formulas and fails to do similarly on
problems requiring other forms of external knowledge like physics, computer
science, or real-world knowledge.
5.3 Results: Few-shot Prompting
Finally, we study the few-shot performance of much larger models ( 175B), to
≈
better understand the performance of the smaller trained models ( 2.7B) and
≈
to provide a benchmark for evaluating other large language models. Overall, we
find that few-shot prompted models generally outperform their much smaller
but fine-tuned counterparts.
12
0.6
Model
GPT3
0.5
Codex
0.4
0.3
0.2
0.1
0 1 3 max
Number of few-shot examples
Figure 3: Average F1 scores of GPT-3 and Codex with different numbers of
few-shot examples in L¯ila.
Zero-shot Few-shot (3)
Dimension
w/o Inst w/ Inst w/o Inst w/ Inst
Math ability 0.120 0.123 0.311 0.306
Language 0.124 0.131 0.352 0.350
Format 0.241 0.257 0.555 0.540
Knowledge 0.108 0.112 0.367 0.363
Average 0.148 0.156 0.396 0.390
Table 6: The IID scores for GPT-3 models with and without instruction prompt-
ing (Inst). Instruction helps slightly in zero-shot setting, but not in few-shot
setting.
Instructions and more examples improve performance. We find that
the number of few-shot examples greatly impacts prompt models’ performance.
Figure 3 shows that GPT-3 answer prediction beats Codex program synthesis
in zero- to one-shot settings, but Codex overtakes with more examples. Ta-
ble 6 shows that prompting with instructions improves performance only in the
zero-shot setting, meaning that in the limited contexts of the prompt models,
examples are more important than instructions for mathematical reasoning.
This is consistent with
,x1⟩,...,⟨xM,xM⟩} and
li lj li lj ELBO = E [logp(X|Z,Z ;θ)]−
Z = (⟨z1,z1,z1 ⟩,...,⟨zM,zM,zM ⟩). q(ZS,ZL|X;ϕ) S L
li lj sem li lj sem
We aim to maximize the likelihood of the ob- KL(q(Z S,Z L|X;ϕ)||p(Z S;θ)p(Z L;θ))
served X with respect to the parameters of the
where Z is the collection of semantic variables,
S
decoder θ, marginalizing over the latent vari-
whileZ isthecollectionoflanguagevariables.
L
ables Z. We follow established procedures for
Thesecondterm,whichwefoundnecessaryfor
thisoptimizationproblemfromrelatedlatentvari-
strong performance, is the sum of p(x |µ )
ablemodelslikevariationalautoencoders(VAEs; li sem lj
andp(x |µ )whichcanbeinterpretedassam-
Kingma and Welling (2013)). Specifically, we lj sem li
ples from the mean of the posterior distribution
optimize a variational lower bound on the log
usingsemanticvariablesgeneratedfrombothinput
marginal likelihood, the evidence lower bound
sentences. When training variational objectives,
(ELBO).ELBOintroducesavariationalapproxima-
where the model ignores the latent variables and
tionq(z,z,z |x,x ;ϕ)tothetrueposterior
sem li lj li lj
thelearnedposteriorremainsclosetotheprior. Ex-
ofthemodel. Theq distributionisparameterized
amplesofotherapproachestoaddresstheseissues
by encoders or inference networks with parame-
include: (Yang et al., 2017; Kim et al., 2018; Xu
tersϕ. ELBOcanbeoptimizedbygradientascent
andDurrett,2018;Heetal.,2019). Weweightthe
byusingthereparameterizationtrick
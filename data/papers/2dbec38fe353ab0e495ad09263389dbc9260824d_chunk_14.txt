tasks: (i) Masked Language Modeling (MLM), deduction,induction,andabductionbeforelearn-
whereitrandomlymasksaportionofwordsineach ingmorecomplexreasoningskills,whichalsobe
sequencetopredicttheoutcome;(ii)CausalLan- regardedasaformofcurriculumlearning.
4.2 Task-specificFine-tuningforMath Question: Rogerhas5tennisballs. He
buys 2 more cans of tennis balls. Each
Task-specificfine-tuningisatechniquetoimprove
canhas3tennisballs. Then,howmany
theperformanceofapre-trainedlanguagemodel
tennisballsdoesRogerhavenow?
on a specific task. This is also a common prac-
Answer: Roger started with 5 balls. 2
ticewhenthereisnotenoughdatafortrainingthe
cans of 3 tennis balls each are 6 tennis
large models from scratch. As shown in Table 3,
balls. 5+6=11. Theansweris11.
existingworkfine-tunespre-trainedlanguagemod-
elsonavarietyofdownstreamtasks,suchasmath
Apart from Kojima et al. (2022) showing that
wordproblems(Kimetal.,2020;Shenetal.,2021),
LLMsaredecentzero-shotreasonerswhengiven
MathQA (Zhao et al., 2022), geometry problem
the“Let’sthinkstepbystep!” prompt,mostofthe
solving(Luetal.,2021a),linearalgebra(Charton,
recentworkhasfocusedonhowtoimprovechain-
2022),andtheoremproving(Wellecketal.,2022a).
of-thought reasoning under the few-shot setting.
Apartfromfine-tuningthemodelparameters,some
Thisworkismainlydividedintotwoparts,(i)se-
workalsousespre-trainedlanguagemodelsasen-
lectingbetterin-contextexamplesand(ii)creating
codersandensemblesthemwithothermodulesfor
betterreasoningchains.
downstreamtasks(Luetal.,2021b).
5.1 In-contextExample
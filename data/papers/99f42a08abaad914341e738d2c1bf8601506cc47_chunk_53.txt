anHFNon up with a novel implementation technique called charac-
MS-Celeb-1M,implyingthatSFNmaybemoresensitiveto teristic gradient detachment to further improve training
noisy samples in the training set and HFN may be more stability and generalization. Extensive experiments on a
robusttodifferenttrainingsetsthanNFNandSFN. number of popular benchmarks are conducted to validate
Finally, with our proposed modifications, all variants thesuperiorityofourSphereFacefamily.
IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 15
Based on the unified framework, our paper demon- [18] J.Lu,G.Wang,W.Deng,P.Moulin,andJ.Zhou,“Multi-manifold
strates strong flexibility and many unique advantages of deepmetriclearningforimagesetclassification,”inCVPR,2015.
[19] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese, “Deep metric
hypersphericalFR.Therestillexistanumberofexcitingyet
learningvialiftedstructuredfeatureembedding,”inCVPR,2016.
under-explored open problems in hyperspherical FR, such
[20] Y.Movshovitz-Attias,A.Toshev,T.K.Leung,S.Ioffe,andS.Singh,
as how to design better angular margin, how to effectively “Nofussdistancemetriclearningusingproxies,”inICCV,2017.
incorporate feature magnitude into training and testing, [21] C.-Y. Wu, R. Manmatha, A. J. Smola, and P. Krahenbuhl, “Sam-
howtolearnthelossfunctiondirectlyfromdata,etc.Wealso plingmattersindeepembeddinglearning,”inICCV,2017.
[22] W. Ge, “Deep metric learning with hierarchical triplet loss,” in
present a few useful characterizations for the loss function
ECCV,2018.
in hyperspherical FR, leading to multiple equivalent loss
[23] Y.Duan,W
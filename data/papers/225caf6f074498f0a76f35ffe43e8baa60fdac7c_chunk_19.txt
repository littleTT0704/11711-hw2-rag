alsocontributeto LaBSE 72.4 74.9 70.7
the stronger results VMSST has on the question
VMSST 79.4 81.9 77.7
retrievaltasks.
Table2: Comparisonstorelatedworkoncross-lingual
6.6 ComparisontoRelatedWork
semanticsimilarity. ResultsarereportedinSpearman’s
Prior work on learning multilingual embeddings ρ × 100. XL contains all 5 datasets, where XL (s.)
hasexploredavarietyofmodelsutilizingdifferent containsonlythosewherethelanguagesarethesame
strategies and using difference source and types (ar-ar,es-es),andXL(d.) containsthosedatasetswhere
thelanguagesaredifferent(ar-en,es-en,andtr-en).Note
oftrainingdata. However,comparingapproaches
thatmodelsinthistablearenottrainedonthesamedata;
is difficult as they differ in many factors that are
forinstanceLaBSEwastrainedonsubstantiallymore
crucialtoperformance: trainingdata,modelsize,
parallel data and XLM-R (Para.) was trained using a
architecture,vocabulary,trainingtime,andevalu- largeEnglishparaphrasecorpusinadditiontoparallel
ationdatasets. Complicatingmattersfurther,even data.
themetricusedinevaluationforthesamedataset,
thedistancemeasureusedbetweenembeddingsfor
Model Tatoeba
the same dataset, and the specific subsets of the
LASER 65.5
evaluationdatasetsusedcanbedifferent. XLM-R(Para.) 67.1
Themaingoalofthispaperistocomparecon- LaBSE 83.7
trastive and generative losses systematically and VMSST 81.1
uniformly,onthesamedata,metricsandunderly-
ingarchitecture. However,wealsoemphasizethat Table3: ComparisonstorelatedworkonTatoeba. Re-
thebestsystemswecomparearecompetitivewith sultsarereportedasaccuracy×100,averagingthexx-
thecurrentstate-of-the-art. Hence,inthissection >en and
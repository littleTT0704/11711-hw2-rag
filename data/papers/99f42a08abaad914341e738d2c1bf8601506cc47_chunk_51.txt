.43 93.06 95.83 96.02 89.13 94.56
SphereFace-Rv2 SFN 1.5 64 0.2 98.26 98.58 45.64 86.55 94.51 95.10 76.53 93.65 80.19 93.01 95.96 96.19 87.39 94.88
network).WeusethesamemodelsfromSection6.2andSec- in our SphereFace family shows competitive results com-
tion 6.3 with the best-performing hyperparameters on the pared to the state-of-the-art methods. Both SphereFace and
validationset.Specifically,wetrainSFNet-20onVGGFace2 SphereFace-RperformparticularlywellunderthelowFAR,
and SFNet-64 on MS-Celeb-1M. We compare our models such as 1:1 verification TAR at 1e-6 and 1:N identification
to current state-of-the-art methods, i.e., NormFace [9], Cos- TPIR at 1e-1 FPIR. These metrics are very important in
Face [3], [4], ArcFace [5], circle loss [26] and Curricular- designingarobustfacerecognitionsysteminpractice.
Face[40].Thehyperparametersofthesemethodsaretuned
toachievetothebestvalidationperformance. 6.4.2 ExperimentswithIResNet-100
We make several useful observations from Table 10 and In order to have a comprehensive comparison with the
Table 11. First, the results on large-scale testing sets are published results, we conduct the experiments to train our
consistent with those on the validation set, especially the methods on MS-Celeb-1M [77] with IResNet-100 [5]. Com-
metrics at low false acceptance rate (FAR) or false posi- paring the results in Table 12 and Table 11, we observe
tive identification rates (FPIR). The consistent performance that IResNet100 achieves better performance than those
demonstrates the effectiveness of the selected validation using SFNet-64 (with the same set of hyperparameters),
set and metric. This indicates that the models that achieve establishing a higher baseline for SphereFace. For different
higher performance on the validation set usually show FNstrategies,IRes
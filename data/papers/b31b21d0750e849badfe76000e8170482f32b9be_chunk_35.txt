themaximumtokenlength.Thetrainingtakesupto15hoursonasingleA6000GPU.
D GENERATOR TRAINING
Wetrainoursingle-sourcegeneratorsfor20epochswithlearningrate4e−5.WetrainourFiD-basedgenerators
for10000steps. Thedoclengthissetto200,anyfurthercontentwillbetruncated. Wefollow(Izacardand
Grave,2021)tosetlearningrateto5e−5with2000stepswarmupandlinearlearningratedecay.Thebatchsize
issetto8.Thebestmodelisselectedbasedonthetoken-levelF1scoreonthedevelopmentsetfortldrand
BLEUscoreforCoNaLa.Thetrainingtakes 8hoursonasingleA6000GPU.
E CODEX PROMPTS
Forthebaseline,wepromptCodexwiththreeNL-codepairsandappendthetestquerytotheend.Anexampleon
tldrisshownontopofTable7.Onthebottom,welistthepromptwithDocPromptingwheredocumentation
isprovidedalongtoo. Intheoraclecommandnamesetting,weprependthecommandnamebeforeeachNL
7https://devdocs.io
14
PublishedasaconferencepaperatICLR2023
CoNaLa
70.1 70 36.2 36.2 66.7 73.7
60 35.9 36.3
60.4 35.6
50 55.8
40
42.3 34.9
30 33.1
20 33.7 16.5 Recall
BLEU
33.4
11 33 55 1100 1155 2200 25 3300
RetrievedDocs
Figure 5: The recall@k (%) and the corresponding BLEU score by using these top-k docs on
CoNaLadataset(usingCodeT5).
intentforthebaselineprompt. ForDocPromptingprompt,wereplacethepotentialdocswiththeretrieved
docsfromtheoraclemanual.
F ADDITIONAL ANALYSIS
Parameterefficiency AsshowninTable1, underagivenparameterbudget, wefindthatDocPrompting
mostlybenefitsfromparallelencoding(FiD).Forexample,theparallelencoding
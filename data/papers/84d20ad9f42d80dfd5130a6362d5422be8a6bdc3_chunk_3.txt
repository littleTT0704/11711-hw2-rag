 of standardized
evaluationprotocolsmakesitchallengingtomeasuretheprogressinefficiencyimprovementsand
obstructstheeffortsindevelopingmoreefficientmodels. Inmanycases,modelsareevaluatedin
scenariosthathardlyreflectthedeploymentofmachinelearningmodelsinpractice(Hendersonetal.,
2020). Moreover, some widely-adopted efficiencymetricssuchas FLOPsoftenpoorlycorrelate
with models’ real-world efficiency performance (Dehghani et al., 2022; Fernandez et al., 2023).
The issue is exacerbated by several practical
challenges. Forinstance,hardwareisacritical
Previous models
confounding factor in efficiency comparisons,
but is very challenging to control in practice, Efficiency Pentathlon
due to disparate levels of hardware accessibil- A strictly-controlled
… hardware platform
ityacrossinstitutions. Consequently,thisleads
todisconnectionsbetweenefficiencyimprove- A diverse
set of metrics
mentsinresearchandtangibleprogressinprac-
tice. Thereisapressingneedforastandardized Realistic scenarios Comprehensive
New model efficiency evaluation
efficiencyevaluationframework.
Figure1: BysubmittingtoPentathlon,practition-
To address these challenges, we present Pen-
erscancomparetheirmodelsagainstallprevious
tathlon. It is designed to establish a standard-
submissionsonidenticalhardware,eliminatingthe
izedplatformforevaluatingtheinferenceeffi-
needtore-implementpreviousworksandsubstan-
ciency of AI models. As shown by Patterson
tially reducing the workloads for fair efficiency
et al. (2022) and Wu et al. (2022a), inference
comparisons. Models are evaluated in four real-
accountsforover60%ofenergyconsumption
istic scenarios designed to mirror real-world ap-
inreal-worldmachinelearningworkloads. Pen-
plications. Ourplatformevaluatesthesubmission
tathlonaimstoprovidecomprehensiveandreal-
across five crucial efficiency metrics, including
isticevaluationofefficiency,andofferthecom-
throughput,latency,memoryoverhead,thenum-
munityaplatformtomakefaircomparisonsin
 Wecompareacrossk = 0examplesand mance,asitistrainedwith215Mexamplesspecif-
k = 5 examples from the training set. We also icallyforsemanticsimilaritytasks. However,we
evaluatethesemodels’abilitytogeneratehuman- seethatfinetuningwithourdataset,whichcontains
like main event, emotion description, and moral far fewer training examples relative to SBERT’s
summariesforeachstory. Again,weuseak-shot pretrainingcorpus,improvesperformance. (+5.35
prompting setup, comparing across k = 0 and ρ, +2 accuracy). BART, which is not specifically
k = 10examples. SeeAppendixGandAppendix pre-trained for semantic similarity tasks, shows
Cforpromptsusedandfinetuningdetails. even greater gains across retrieval metrics when
EmpathySimilarityPrediction. Weproposeabi- finetunedonourdataset. (22.89ρ,+7.75accuracy).
encoderarchitecturefinetunedwithmean-squared We find that for BART models, fine tuning im-
error(MSE)lossofthecosine-similaritybetween provements (p = 0.02, p = 0.0006 respectively),
story pairs, as compared to the empathic similar- asmeasuredwithMcNemar’stestontheaccuracy
ity gold labels. For each of the encoders, we use scoresandFisher’stransformationoncorrelations,
asharedpretrainedtransformer-basedmodeland aresignificantlyhigherthanbaselines.
furtherfinetuneonthe1,500annotatedstorypairs While GPT-3 and ChatGPT have high perfor-
inourtrainingset. Weobtainthefinalembedding mance on the precision at k retrieval metric, in
usingmeanpoolingoftheencoderlasthiddenstate. practice, it is not feasible to prompt the models
witheverypairofstoriesintheretrievalcorpus.
6 AutomaticEvaluation
7 UserStudy
Toevaluatethequalityofempathicsimilaritypre-
dictions,wefirstcomparetheSpearman’sandPear- Priorwork�
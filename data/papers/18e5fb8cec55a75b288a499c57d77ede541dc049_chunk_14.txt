 53:3((cid:6)0:5) 54:1((cid:6)0:5)
RoBERTa-L(MR) ATOMIC 70:8((cid:6)1:2) 64:2((cid:6)0:7) 72:1((cid:6)0:5) 63:1((cid:6)1:5) 59:6((cid:6)0:3)
RoBERTa-L(MR) CWWV 70:0((cid:6)0:3) 67:9((cid:6)0:8) 72:0((cid:6)0:7) 54:8((cid:6)1:2) 59:4((cid:6)0:5)
RoBERTa-L(MR) CSKG 70:5((cid:6)0:2) 67:4((cid:6)0:8) 72:4((cid:6)0:4) 63:2((cid:6)0:7) 60:9((cid:6)0:8)
RoBERTa-L(supervised) - 85.6 78.5 79.2 76.6 79.3
Human - 91.4 88.9 94.9 86.9 94.1
Table2:Zero-shotevaluationresultswithdifferentcombinationsofmodelsandknowledgesources,acrossfivecommonsense
tasks. CSKG represent the combination of ATOMIC and CWWV. We run our experiments three times with different seeds and
report average accuracy with 95% confidence interval. SMLM (*) used OMCS for CSQA, ROCStories (Mostafazadeh et al.
2016)foraNLIandATOMICforSIQAasknowledgeresources.
models are used in order to understand the impact of fur- H4: Adding diverse knowledge (from different KGs) im-
ther tuning. Here we directly uses the LMs to score the provesperformance.Thisistheinitialmotivationbehind
QA pairs without any finetuning. We also show the results the creation of CSKG (Ilievski et al. 2020), but has not
ofotherunsupervisedsystemsthatleverageK
→en 14 1875
framework(see§2),whichprovidericherfeedback en→gu 11 998
en→ru 15 1315
byidentifyingerrorspans,categorizingthem,and gu→en 11 1016
evaluatingtheirseverity. Table 1: The number of systems and segments that
Interestingly,anotheremergentphenomenonin haveMQMscores(left)andDAscores(right)usedas
LLMsisthesuccessofchain-of-thought prompt- ground-truthinthiswork.
ing (Wei et al., 2022): when defining a prompt
tiveordifficulttoparse. Thusweonlyconsiderthe
for a particular task, if we instruct the model to
produce a series of intermediate reasoning steps
AUTOMQM taskinthefew-shotscenario. Based
(“let’s think step-by-step”), it tends to generate onthefindingsfrom§6.2, weexploretheimpact
a free-text rationale before generating an output, ofin-contextlearningbysamplingfromtheexam-
plepoolusingstratifiedsamplingextendedwitha
and this often improves the performance on the
set of rejection criteria (Appendix B), which en-
taskathand(Liuetal.,2023b). Furthermore,this
chain-of-thoughtpromptingcanbeusedtoobtain sures that the example set has a balance between
structured rationalesfromLLMs,andthiscanlead majorandminorerrorsaswellasdiversityinthe
categoriesoferrors.
tobetterperformancethanwithfree-textrationales
(Luetal.,2023).
6 Experiments
Motivated by these findings, we propose
AUTOMQM, a prompting technique for transla- 6.1 ExperimentalSetup
tionqualityassessmentthatinstructsLLMstoiden-
Data The metrics in this work are evaluated
tifyerrorsinatranslation,andcategorizethetypeof
onbothhigh-resourceandlow-resourcelanguage
erroraccordingtotheMQMframework(Lommel
pairs. Thethreehigh-resourcelanguagepairscome
etal.,2014). Furthermore,wedon’taskthemodel
from the WMT’22 Metrics Shared Task
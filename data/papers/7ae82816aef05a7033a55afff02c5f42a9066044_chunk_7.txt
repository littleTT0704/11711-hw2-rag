 as real
dependency relations.
– AnIG-basedmodel,whereInnerWord relationsareprocesseddeterministically
without consulting the SVM classifiers.
For these models, we use a reduced version of the inflectional features in the
treebank, very similar to the reduced tagset used in the parser of Eryig˘it and
Oflazer[9]. For each IG, we use the part-of-speech of each IG and in addition
include the case and possessive marker features if the IG is a nominal. Using
this approach, the POS feature of the word “odadayım” becomes +Noun+Pnon-
+Loc+Verb.
When lexicalizing the IG-based models, we use the stem for the first IG of a
word but a null value (“ ”) for the remaining IGs of the same word. This rep-
resentation also facilitates the deterministic processing of InnerWord relations in
the third model, since any top token can be directly linked to a next token with
LEX=“ ”, provided that the two tokens are adjacent.
SectionModel Unlexicalized Lexicalized
AS AS AS AS
U L U L
5 Word-based 67.2±0.357.9±0.3 70.7±0.362.0±0.3
IG-based 68.3±0.258.2±0.2 73.8±0.264.9±0.3
IG-based deterministic70.6±0.360.9±0.3 73.8±0.264.9±0.3
6 INFas single feature 71.6±0.262.0±0.3 74.4±0.265.6±0.3
INFsplit 71.9±0.262.6±0.3 74.8±0.266.0±0.3
8 Optimized 76.0±0.267.0±0.3
Table 1. Summary table of experimental results
In order to calculate the accuracy for the word-based models, we assume that
the dependent is connected to the first IG of the head word. This assumption
is based on the observation that in the treebank, 85.6% of the dependency links
land on the first (and possibly the only) IG
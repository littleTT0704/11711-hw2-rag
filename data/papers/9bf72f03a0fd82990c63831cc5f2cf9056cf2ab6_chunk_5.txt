 our method
achieves about 1% performance gain over SOTA
2.2 QuantizationofBERT
quantized models consistently across 8 different
SincetrainingBERTusuallyinvolvesadaptiveopti-
tasks. Ourcontributionsarelistedasfollows:
mizerslikeAdamratherthanSGD,andBERTland-
1. Wesuccessfullyleveragedanalternatetrain- scape is shown to be sharper than ResNet (Chen
Task W/ABits COLA SST-2 MRPC STS-B QQP mNLI qNLI RTE
Metrics MatthewsCorr. Acc. Acc. PeasonCorr. Acc. MatchedAcc. Acc. Acc.
FP32 32/32 56.5 93.1 82.84 88.6 90.8 84.34 91.4 67.2
Q8BERT 8/8 58.5±1.3 92.2±0.3 - 89.0±0.2 88.0±0.4 - 90.6±0.3 68.8±3.5
GOBO 2/32 - - - 82.7 - 71.0 - -
Q-BERT 2/8 - 84.6 - - - 76.6 - -
LSQ* 2/8 51.3±0.7 92.2±0.1 83.5±0.7 87.2±0.1 91.1±0.1 83.6±0.1 91.1±0.1 66.8±0.9
SQuAT* 2/8 53.3±0.2 92.7±0.1 84.0±0.7 88.0±0.1 91.1±0.2 84.0±0.1 91.3±0.1 67.4±0.4
GOBO 3/32 - - - 88.3 - 83.7 - -
Q-BERT 3/8 - 92.5 - - - 83.4 - -
LSQ* 3/8 58.8±0.4 92.6±0.2 84.2±0.5 88.4±0.1 91.2±0.2 84.2±0.2 91.8±0.1 68.6±0.3
SQuAT* 3
7to76.3F1score. Jointlyfine-tuningXLM-R
improveevenwhenjointtrainingleadstoaminor
withtargetandsourcedata(JT)leadstoasubstan-
performancedropforSA.
tial average gain of around 12.6 F1 score. Using
thesameamountofdatafromarelatedlanguage 4.3 PlacementofRepresentation
(insteadofEnglish)ismoreeffective,showingan TransformationNetwork
averageimprovementof16.3F1scoreoverusing
Previousworks(Jawaharetal.,2019;Tenneyetal.,
target data only. Our proposed method, MetaXL,
2019) have observed that lower and intermediate
consistently outperforms the joint training base-
layersencodesurface-levelandsyntacticinforma-
lines,leadingtoasignificantaveragegainof2.07
tion,whereastoplayersaremoresemanticallyfo-
and 0.95 F1 score when paired with English or
cused. Thesefindingssuggestthattheplacement
relatedlanguages,respectively.
of the representation transformation network can
SA We present results on the task of SA in Ta- potentiallyaffecttheeffectivenessoftransfer. To
ble3,whereweuse1KexamplesfromEnglishas
4Nosignificantgainswereobservedforanyofthemodels
sourcelanguagedata. Wefindthatauxiliarydata
whengoingbeyond5Kexamples.
fromsourcelanguagesbringslessbutstillsignifi- 5PleaserefertoAppendixCforfullresults.
NER(average) SA(tel) SA(fa)
#en JT MetaXL ∆ #en JT MetaXL ∆ #en JT MetaXL ∆
5k 69.06 71.13 +2.07 1k 88.68 90.53 +1.85 1k 85.51 87.14 +1.63
10k 70.11 71.63 +1.52 3k 87.13 87.23 +0.10 3k 82.88 86.19 +3.31
20k 72.31 73.36 +1.05 5k 84.91 85.71 +0.80 5k 86.34 85.63 -0.71
Table4:
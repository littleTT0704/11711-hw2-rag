:88)n (cid:26) 1 if a correct answer was found,
MRR = ri
n 0 otherwise.
i=1
TheMRRiscommonlybasedononlythetopanswersuptoafixedrank. Forinstance,
MRR@10 only takes the 10 highest ranked answers for each question into account.
This metric is useful for capturing the system’s ability to extract the correct answer
while rewarding systems that are able to rank that answer higher up in the answer
list. Note that the MRR is not only useful for evaluating final answers but can also be
28 CHAPTER 3. FUNDAMENTALS
computed for intermediate search results retrieved by a QA system, such as passages
or documents, if relevance judgments for these results are available.
The performance of a QA system on list questions is often measured in terms of
F-scores. Let t be the total number of correct answers to the i-th list question in a
i
test set, r the number of answers returned by a QA system for that question, and c
i i
the number of those answers that are correct. Further, let precision and recall on the
i-th question be defined as follows:
c c
i i
Precision = and Recall =.
i i
r t
i i
The F-score is a weighted harmonic mean of precision and recall:
(β2 +1)×Precision ×Recall
i i
F (β) =.
i β2 ×Precision +Recall
i i
In this formula the weight parameter β determines the relative importance of preci-
sion and recall. The larger β, the more weight is given to recall, that is, the more
important it becomes to find all correct answers and the less important to avoid in-
correct answers. If β = 1, precision and recall are equally important. The overall
performance of a QA system on a set of list questions can now be defined as the
arithmetic mean of the F-scores:
n
1 (cid:88)
F(β) = F (β).
i
n
i=1
F-scores are also used for evaluating definition questions and other types of questions
with complex answers. In TREC evaluations, the assessors compiled lists of informa-
tion nuggets they
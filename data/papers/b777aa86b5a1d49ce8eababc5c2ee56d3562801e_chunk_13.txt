-
variable sequence lengths to provide substantial
ingpointoperations. Instead,increasedper-layer
latency reductions when inference is compute-
kernel execution time causes these wider models
bounded.
tobecomecompute-boundatlowerbatchsizes. In
the compute-bound regime, latency scales more
4.2 ModelDesignDecisions
rapidlywithbatchsizeforwidemodels.
Weexamineavarietyofcommonmodelarchitec-
4.2.2 DownsamplingandHierarchicalPooling
ture design decisions and investigate their align-
ment with commonly reported efficiency proxies
andempiricallyobservedlatency.
4.2.1 ScalingModelDepth&Width
Assumption Scaling the dimensionality and
number of hidden layers is commonly used in
NLP and computer vision as a means to explore
tradeoffs between model performance and com-
putationalrequirements(Heetal.,2016;Touvron
Figure7:ComparisonofBERTandFunnelTransformer
et al., 2021; Zhou et al., 2021). Recent work has latency. Despite using fewer total MAC operations,
shownthatmodelend-taskperformancescalesdif- Funnel is slower than BERT for inference due to the
ferently along each axis (Tay et al., 2021, 2022; introductionofadditionalintermediatepoolinglayers.
Nguyenetal.,2021).
)s(
ycnetaL
Assumption Self-attentionlayersinTransformer Assumption Modelarchitecturesthattargetde-
architectures are notoriously computationally ex- ploymentonedgedevices,suchasmobilephones,
pensive as their complexity scales quadratically areoftendesignedtodecreasethetotalnumberof
with input sequence length. To reduce this com- FLOPs or MACs under the assumption that such
putationalbottleneck,researchershavedeveloped reductionstranslatetodecreasedlatency. Towards
efficienttransformerarchitecturesthatseektore- thisend,operationssuchasgroupedconvolutions
ducesequencelengthsviamethodssuchasdown- (Zhangetal.,2018;Iandolaetal.,2020),inverted
sampling, low rank approximations
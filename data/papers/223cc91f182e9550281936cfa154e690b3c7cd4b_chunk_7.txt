 MLE is the convex dual of maximum entropy estimation.
In a maximum entropy formulation, rather than assuming a specific parametric from of the target model
distribution, denoted as p(x), we instead impose constraints on the model distribution. Specifically, in the
supervised setting, the constraints require the expectation of the features T(x) to be equal to the empirical
expectation:
Ep
[T(x)]=Ex∗∼D[T(x∗)].
(2.3)
In general, there exist many distributions p ∈ P(X) that satisfy the constraint. The principle of maximum
entropy resolves the ambiguity by choosing the distribution such that its Shannon entropy,
H(p) := −Ep[logp(x)], is maximized. Following this principle, in the supervised setting, we thus have
the specific constrained optimization problem:
max H(p(x))
p(x)
s.t. Ep [T(x)]=Ex∗∼D[T(x∗)] (2.4)
p(x) ∈ P(X).
The problem can be solved with the Lagrangian method. Specifically, we write the Lagrangian:
L(p,θ,μ) =H(p(x))−θ ⋅(Ep [T(x)]−Ex∗∼D[T(x∗)])−μ (∑xp(x)−1 ), (2.5)
where θ and μ are Lagrangian multipliers. Setting the derivative w.r.t. p and μ to equal zero implies that p
must have the same form as in Equation 2.2:
p(x) =exp{θ ⋅T(x)}/Z(θ), (2.6)
where we see the parameters θ in the exponential family parameterization are the Lagrangian multipliers that
enforce the constraints. Plugging the solution back into the Lagrangian, we obtain:
L(θ) =Ex∗∼D[θ ⋅T(x∗)]−logZ(θ), (2.7)
which is simply the negative of the MLE objective in Equation 2.1.
6
Harvard Data Science Review • Issue 4.4, Fall
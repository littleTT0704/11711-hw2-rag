imentanalysistask,weusethe
Ds s
probleminEquation2andÎ±isthecorresponding EnglishportionofMultilingualAmazonReviews
Language Related mimic the low-resource setting by manually con-
Language Code
Family Language structing a train, development, and test set with
Quechua qu Quechua Spanish 100, 1000, and1000examplesthroughsampling.
MinDong cdo Sino-Tibetan Chinese
ForSentiraama,wemanuallysplitthedatasetinto
Ilocano ilo Austronesian Indonesian
Mingrelian xmf Kartvelian Georgian train, development, and test subsets of 100, 103,
MeadowMari mhr Uralic Russian and100examples.2
Maori mi Austronesian Indonesian
Turkmen tk Turkic Turkish
3.2 ExperimentalSetup
Guarani gn Tupian Spanish
Base Model We use mBERT3 (Devlin et al.,
Table1: TargetlanguageinformationontheNERtask.
2018) and XLM-R (Conneau et al., 2020) as our
Thedatasetsizeofthetheselanguagesis100.
base models, known as the state-of-the-art multi-
lingual pre-trained model. However, our method
Corpus(MARC)(Keungetal.,2020)asthehigh- isgenerallyapplicabletoalltypesofTransformer-
resourcelanguageandproductreviewdatasetsin basedlanguagemodels.
two low-resource languages, Telugu and Persian
(GangulaandMamidi,2018;Hosseinietal.,2018). TargetLanguage ForNER,weusethesame8
low-resource languages as Pfeiffer et al. (2020c),
WikiAnn WikiAnn(Panetal.,2017)isamulti-
summarizedinTable1. Theselanguageshaveonly
lingual NER dataset constructed with Wikipedia
100examplesintheWikiAnndatasetandarenot
articlesandanchorlinks. Weusethetrain,devel-
includedforpre-trainingXLM-R.ForSA,Persian
opmentandtestpartitionsprovidedinRahimietal.
andTeluguarethetargetlanguages. Forbothtasks
(2019
ytimeseachgroundtruth
token’sprobabilityintheevaluationsetarehelpedbyeachkNNsetting. Itmeansthatforeachgroundtruth
tokenintheevaluation,wecountthetimeswhenthekNNdistributionishigherthanthebaseLMdistribution
P,i.e.,P >P.
LM kNN LM
SincewefoundpreviouslythatapproximatekNNprovidesanadditionalperformanceboostcomparedto
groundtruthkNN,wethuscompare“realmask,realscore”versus“FAISSmask,realscore”inthisanalysis.
Topreventoutliers,wefilteroutwordswithlessthan10occurrencesintheevaluationset. Foreachsetting,we
calculatethepercentageofoccurrencesintheevaluationsetwhereeachtokeninthevocabularywherethe
kNNmoduleachievesabetterprobabilitythanbaseLM.Wethenplottheabsolutedifferencebetweenthe
percentagesofthetwosettings,withrespecttovariouspossibleattributesofthetokenthatachievesbetter
probabilityusingeachsetting.
Figure6showsthatthelongerthetokenis,whichusuallysuggestspropernounsandharderandlesscommon
wordsinEnglish,arebetterwithapproximateneighborsthangroundtruthones,andviceversa.Wehypothesize
thatthisisduetolongerwordsaremorepronetooverfittinginkNN-LMandthususingapproximatekNN
providesaneffectsimilartosmoothingandregularization.
Wealsocomparewordsthatcouldappearinmorediversecontextswithwordsthatco-occurwithfewdistinct
contexts. Tomeasurehowdiversethecontextsofeachwordinthevocabularyis,wecalculateboththeforward
andbackwardbigramentropyforeachwordintheevaluationsetthathasmorethan10occurrences. The
bigramentropyisasimpleyetgoodindicatorofcontextdiversityforagivenword,asusedinKneser–Ney
smoothing(Neyetal.,1994). Wecalculateboththeforwardandbackwardbigramentropyforeachwordwas
16

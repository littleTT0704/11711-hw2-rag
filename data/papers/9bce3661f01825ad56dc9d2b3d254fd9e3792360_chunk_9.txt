 problem shown in Figure 2
isoutside,itisconceivablethatthenunistaking
tothesystemandtheninputtedadditionalhuman
somescenery.”. Atthispoint,theproblemhastwo
utteranceexamplesrelatedtothediscussionafter
opposinglabelsinthepromptbecausewewantit
eachsystempredictedthelabel. Intheadditional
todiscusstwodifferentlabels.
input,thebeginningofhumanutteranceisprefixed
We use actual human utterances as references
with"Human:"andtheendisprefixedwith"Sys-
andcomputetheBERTScore(Zhangetal.,2020)
tem:" to indicate that the next is a system’s utter-
ofthesystem’soutputsforevaluation. BERTScore
ance. Specifically,thefirstpromptfordiscussion
leveragesthepre-trainedlanguagemodelsuchas
is"Human: Let’sdiscussitmore. Ithinkneutral,
BERT (Vaswani et al., 2017) and RoBERTa (Liu
because there may be a kitchen in the barn. Sys-
et al., 2019) and matches words in candidate
tem:". Thesystempredictsthefinallabelwhenthe
and reference sentences by cosine similarity.
discussionisfinished.
BERTScore computes precision, recall, and F1
Weinvestigatehowdiscussionwithhumansim-
measures. Therefore, BERTScore can be used
provesNLItaskperformance. Thesystempredicts
tocomparethesystem’scontentandhumanutter-
anceswitheachother. Weuseroberta-large7forthe the label, then the human and the system discuss
and decide on the final label. We compare the
7https://huggingface.co/roberta-large performanceofeachlabelbeforeandafterthedis-
supportive↑ unsupportive↓ diff. Before After
zero-shot 82.0/83.1 81.8/82.5 0.2/0.6 zero-shot 54.2/60.0 65.6/60.0
few-shot 82.7/83.6 82.3/82.9 0.4/0.7 few-
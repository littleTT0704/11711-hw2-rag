howresultschange
withrespecttothenumberoftests. Comparedto
WepresentODEX,anopen-domaincodegenera-
using all cases in default, we also try using one
tiondatasetsupportingexecution-basedevaluation
randomlyselectedcase. Forsimplicity,wedonot
viahuman-writtentestcases. ODEXnotonlysup-
includeanytestcasesinprompts.
portsexecution-basedevaluationofcodeusingtest
AsshowninFigure12,evaluatingoveroneran-
cases,butalsoextendsthetasktotheopendomain,
domtestlargelypreservestheaccuracyofusingall
covering79diversePythonlibrariesandfournatu-
tests,indicatingthatonecaseissufficienttotestthe
rallanguages(English,Spanish,Japanese,andRus-
mainfunctionalityformostqueries. CheckÂ§Efor
sian). Comparingtwostate-of-the-artcodegenera-
analysisonotherfactorssuchasfunctionnaming.
tionmodels, CODEX and CODEGEN,ourdataset
effectivelyunveilstheirvariedbehaviorsbetween
8 RelatedWork
program domains and language contexts. ODEX
Open Domain Code Generation Programs of- servesasacomprehensiveNL-to-codebenchmark
tenuseAPIsfromdifferentPythonlibraries. Some givenitsopen-domaincoverage,multi-naturallan-
datasetspreservenaturalcoveragefrominteractive guage queries, and multi-metric support. When
bringingcodeexecutiontoopendomainscenarios, foropen-domaincodegeneration,tofurtherfacili-
ourexplorationsalsorevealemergingchallengesin tatetechnologicaladvancesinAIprogrammingas-
testcreationandreliableexecution,whichwehope sistance,meanwhilesupportingmultiplelanguages
thatourdatasetwillenablefutureworktotackle. toencourageitsuniversalaccessibility.
We strive to ensure high data quality and opti-
Acknowledgements
mize annotation efficiency. We build the ODEX
datasetwithnaturalandpracticalStackOverflowre-
Wewouldliketothankalltheannotatorsfortheir
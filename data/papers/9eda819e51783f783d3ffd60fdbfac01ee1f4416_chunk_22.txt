 arXiv:2106.04647.
ume 1: Long Papers), pages 565–576, Online. As-
sociationforComputationalLinguistics. YuningMao,LambertMathias,RuiHou,AmjadAlma-
hairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Ma-
Guillaume Lample and Alexis Conneau. 2019. Cross- dian Khabsa. 2021. Unipelt: A unified frame-
lingual language model pretraining. arXiv preprint workforparameter-efficientlanguagemodeltuning.
arXiv:1901.07291. arXivpreprintarXiv:2110.07577.
Michael McCloskey and Neal J Cohen. 1989. Catas-
JaejunLee,RaphaelTang,andJimmyLin.2019. What
trophicinterferenceinconnectionistnetworks: The
would elsa do? freezing layers during transformer
sequentiallearningproblem. InPsychologyoflearn-
fine-tuning.
ing and motivation, volume 24, pages 109–165. El-
sevier.
BrianLester,RamiAl-Rfou,andNoahConstant.2021.
The power of scale for parameter-efficient prompt
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and
tuning. arXivpreprintarXiv:2104.08691.
Luke Zettlemoyer. 2021. Noisy channel language
model prompting for few-shot text classification.
Mike Lewis, Yinhan Liu, Naman Goyal, Mar- arXivpreprintarXiv:2108.04106.
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
2020. BART:Denoisingsequence-to-sequencepre- C¸ag˘lar Gulc¸ehre, and Bing Xiang. 2016. Abstrac-
trainingfornaturallanguagegeneration,translation, tivetextsummarizationusingsequence-to-sequence
andcomprehension. InProceedingsofthe58thAn- RNNs and beyond
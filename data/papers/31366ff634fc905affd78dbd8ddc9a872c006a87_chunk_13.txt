outalanguage-specificCodeBERT?
the highest correlation with human preference,
In all experiments in Section 4, we used the
across all correlation metrics. While Evtikhiev
language-specific model which we continued to
et al. (2022) suggested that chrF and ROUGE-L
pretrain on each language. But what if we wish
are the most suitable metrics for evaluating code
touseCodeBERTScoreinalanguageinwhichwe
generation models in CoNaLa, CodeBERTScore
don’t have a language-specific model? We com-
outperforms these metrics by a significant mar-
pare the language-specific models to CodeBERT-
gin. For example, CodeBERTScore achieves
base in Figure 4. Generally, CodeBERT-base
Kendall-Tau correlation of 0.517 compared to
achievescloseperformancetoalanguage-specific
0.470 of chrF and 0.420 of ROUGE-L. These re-
model. However, in most HumanEval experi-
sultsshowthatgeneratedcodethatispreferredby
mentsandcorrelationmetrics,usingthelanguage-
CodeBERTScore— also tends to be preferred by
specific model is beneficial. These results show
humanprogrammers.
that language-specific models are often preferred
Correlation with functional correctness Ta- if such models are available, but the CodeBERT-
ble1showsthecorrelationbetweendifferentmet- base can still provide close performance even
rics and functional correctness: CodeBERTScore withoutlanguage-specificpretraining.
τ-Lang-specific τ-Basemodel r -Lang-specific r -Basemodel
s s
0.7
0.650.65
0.6
0.560.56
0.520.52
0.5
0.460.45
0.4 0.380.39 0.37 0.370.38
0.340.35 0.330.32 0.35 0.34
0.3
0.3
0.2
0.1
0
CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript
Figure4: TheKendall-TauandSpearman
ers). Theobjectiveistoselectanobjecttopick
andplaceitintoadrawer(rack)(seeFig. 1).
We use a policy trained in simulation and apply it to a scene with four objects (2 bowls, 1 cup,
1 plate) through the hardware pipeline described above. We start by collecting a prompt human
demonstration(moredetailsinAppendixA).,Thelearnedpolicy,conditionedapromptdemonstra-
tion,isappliedtotwovariationsofthesamescene,andthepredictedactionsexecuted. Thepolicy
wassuccessfuloncewith100%successrate,andoncewith75%,showninFigure1,bottom. The
failurecasewascausedduetoaperceptionerrorâ€“abowlwasclassifiedasaplate.Thisdemonstrates
thatsuchTTPcanbetrainedinsimulationandapplieddirectlytohardware. Thepolicyisrobustto
minorhardwareerrors,suchasifabowlgraspfails,itjustrepeatsthegraspingaction(seevideoand
AppendixA).However,itreliesonaccurateperceptionofthestate. Inthefuture,wewouldliketo
furtherevaluateourapproachonmorediversereal-worldsettingsandmeasureitssensitivitytothe
differenthardwarecomponents,informingfuturechoicesforlearningrobustpolicies.
3.3 Ablations
(a) Performance decays for OOD(b)Performanceimproveswiththe (c)Performanceimproveswiththe
numberofobjects,i.e3-5&8-10 #ofuniquesessionsintraining #ofuniquepreferencesintraining
Figure7: (a)Out-of-distributiongeneralizationto#objects. (b-c)ablationexperiments.
Westudythesensitivityofourapproachtotraininghyperparameters. First,wevarythenumberof
trainingsessionsperpreferenceandstudygeneralizationtounseenscenariosofthesamepreference.
Figure7bshowsthattheperformanceofTTPimprovesasthenumberofdemonstrationsincrease,
indicatingthatourmodelisnotoverfittingtothetrainingset,andmightbenefitfromfurthertraining
samples. Next,wevarythenumberoftrainingpre
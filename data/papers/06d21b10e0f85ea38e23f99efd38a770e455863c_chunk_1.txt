Paraphrastic Representations at Scale
JohnWieting1,KevinGimpel2,GrahamNeubig3,andTaylorBerg-Kirkpatrick4
1GoogleResearch
2ToyotaTechnologicalInstituteatChicago,Chicago,IL,60637,USA
3CarnegieMellonUniversity,Pittsburgh,PA,15213,USA
4UniversityofCaliforniaSanDiego,SanDiego,CA,92093,USA
jwieting@alumni.cmu.edu,kgimpel@ttic.edu,gneubig@cs.cmu.edu,tberg@eng.ucsd.edu
Abstract tasks(Wietingetal.,2019a). Withinthiscontext,
fastandlight-weightmethodsareparticularlyuse-
We present a system that allows users to
fulastheymakeiteasytocomputesimilarityover
train their own state-of-the-art paraphrastic
sentence representations in a variety of lan- theever-increasingvolumesofwebtextavailable.
guages.WereleasetrainedmodelsforEnglish, Forinstance,wemaywanttomineahundredmil-
Arabic, German, Spanish, French, Russian, lion parallel sentences (Schwenk et al., 2021) or
Turkish, and Chinese. We train these mod- useasemanticsimilarityrewardwhenfine-tuning
elsonlargeamountsofdata,achievingsignif-
language generation models on tens of millions
icantly improved performance from our orig-
oftrainingexamples. Thesetasksaremuchmore
inal papers on a suite of monolingual seman-
feasiblewhenusingapproachesthatarefast, can
ticsimilarity,cross-lingualsemanticsimilarity,
andbitextminingtasks. Moreover, theresult- berunonCPU,anduselittleRAM,allowingfor
ing models surpass all prior work on efficient increasedbatchsize.
unsupervisedsemantictextualsimilarity,even
Thisneedforfastinferenceisonemotivationfor
significantly outperforming supervised BERT-
usingsentenceembeddings. Sentenceembeddings
based models like Sentence-BERT (Reimers
and Gurevych, 2019). Most importantly, our allowthesearchforsimilarsentencestobelinear
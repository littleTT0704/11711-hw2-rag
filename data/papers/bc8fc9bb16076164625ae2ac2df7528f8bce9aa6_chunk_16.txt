 mst e --
where ð‘ ð‘¦â€² denotes the total number of samples in the training putethesupervisedcontrastivelossinEq.(5)andmakedistinction
ð‘–
setthathavethesamelabelð‘¦â€²asanchorð‘–.Thankstothegraph- fromconventionalself-trainingalgorithms[41].Onthecontrary,
ð‘–
levelcontrastivenatureofIGSD,weareabletoalleviatethebiased traditionalself-trainingcanusepsuedo-labelsforcomputingcross
negativesamplingproblems[26]withsupervisedcontrastiveloss, entropyonly.
IterativeGraphSelf-distillation Ljubljanaâ€™21,April19â€“23,2021,Ljubljana,Slovenia
Datasets MUTAG IMDB-B IMDB-M NCI1 COLLAB PTC
#graphs 188 1000 1500 4110 5000 344
#classes 2 2 3 2 3 2
Avg#nodes 17.9 19.8 13.0 29.8 74.5 25.5
RandomWalk 83.7Â±1.5 50.7Â±0.3 34.7Â±0.2 OMR OMR 57.9Â±1.3
ShortestPath 85.2Â±2.4 55.6Â±0.2 38.0Â±0.3 51.3Â±0.6 49.8Â±1.2 58.2Â±2.4
GraphletKernel 81.7Â±2.1 65.9Â±1.0 43.9Â±0.4 53.9Â±0.4 56.3Â±0.6 57.3Â±1.4
WLsubtree 80.7Â±3.0 72.3Â±3.4 47.0Â±0.5 55.1Â±1.6 50.2Â±0.9 58.0Â±0.5
DeepGraph 87.4Â±2.7 67.0Â±0.6 44.6Â±0.5 54.5Â±1.2 52.1Â±1.0 60.1Â±2.6
MLG 87.9Â±1.6 66.6Â±0.3 41.2Â±0.0 >1Day >1Day 63.3Â±1.5
GC
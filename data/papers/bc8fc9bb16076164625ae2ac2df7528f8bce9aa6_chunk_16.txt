 mst e --
where 𝑁 𝑦′ denotes the total number of samples in the training putethesupervisedcontrastivelossinEq.(5)andmakedistinction
𝑖
setthathavethesamelabel𝑦′asanchor𝑖.Thankstothegraph- fromconventionalself-trainingalgorithms[41].Onthecontrary,
𝑖
levelcontrastivenatureofIGSD,weareabletoalleviatethebiased traditionalself-trainingcanusepsuedo-labelsforcomputingcross
negativesamplingproblems[26]withsupervisedcontrastiveloss, entropyonly.
IterativeGraphSelf-distillation Ljubljana’21,April19–23,2021,Ljubljana,Slovenia
Datasets MUTAG IMDB-B IMDB-M NCI1 COLLAB PTC
#graphs 188 1000 1500 4110 5000 344
#classes 2 2 3 2 3 2
Avg#nodes 17.9 19.8 13.0 29.8 74.5 25.5
RandomWalk 83.7±1.5 50.7±0.3 34.7±0.2 OMR OMR 57.9±1.3
ShortestPath 85.2±2.4 55.6±0.2 38.0±0.3 51.3±0.6 49.8±1.2 58.2±2.4
GraphletKernel 81.7±2.1 65.9±1.0 43.9±0.4 53.9±0.4 56.3±0.6 57.3±1.4
WLsubtree 80.7±3.0 72.3±3.4 47.0±0.5 55.1±1.6 50.2±0.9 58.0±0.5
DeepGraph 87.4±2.7 67.0±0.6 44.6±0.5 54.5±1.2 52.1±1.0 60.1±2.6
MLG 87.9±1.6 66.6±0.3 41.2±0.0 >1Day >1Day 63.3±1.5
GC
 on the Tatoeba bitext mining embeddingbased)systemsfromSemEval2017for
task introduced by Artetxe and Schwenk (2019). Spanish-English.15
Thedatasetconsistsofupto1,000English-aligned
sentence pairs for over 100 languages. The aim
15Thetopsystemsforthistaskusedsupervisionandrelied
ofthetaskistofindthenearestneighborforeach
onstate-of-the-arttranslationmodelstofirsttranslatethenon-
sentenceintheotherlanguageaccordingtocosine EnglishsentencestoEnglish.
384
BitextMining. TheresultsontheTatoebabitext using32coresandachievedaspeedof15,316sen-
miningtaskfromArtetxeandSchwenk(2019)are tences/second. Thisisevenfasterthanwhenusing
shown in Table 4. The results show that our em- aGPUandindicatesthatourmodelcaneffectively
beddingsarecompetitive,buthaveslightlyhigher be used at scale when GPUs are not available. It
errorratesthanLASER.Themodelsaresoclose alsosuggestsourmodelwouldbeappropriatefor
thatthedifferenceinerrorrateforthetwomodels useonembeddeddevices.
acrossthe6evaluationsis0.2,correspondingtoa
difference of about 2 mismatched sentence pairs 8 Conclusion
perdataset. LaBSEperformsabitbetter,butwas
Inthispaper,wepresentasystemforthelearning
trainedonmuchmoredatathenbothLASERand
andinferenceofparaphrasticsentenceembeddings
ourmethod. WealsocomparetomBART,XLM-R,
in any language for which there is paraphrase or
andCRISS.16
bilingualparalleldata. Additionally,wereleaseour
Thisbitextminingresultisincontrasttothere-
trainedsentenceembeddingmodelsinEnglish,as
sultsoncross-lingualsemanticsimilarity,suggest-
wellasArabic,German,Spanish,French,Russian,
ingthatourembeddingsaccountforalessliteral
Turkish, and Chinese. These models are trained
semantic similarity,
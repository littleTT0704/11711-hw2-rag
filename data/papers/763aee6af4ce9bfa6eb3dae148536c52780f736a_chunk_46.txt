{1,...,N }. Namelyprojectingthesourcedomain
H S,l S,k S
dataintoafeaturespace,wherethesourcedomainlabelsarehardtodistinguish.
Theabove 3 Diversitytermisalsosupportedbytheevidencethatcompositionalgeneralizationandextrapolationcan
beimprovedifthetrainingdomaindataarerichenough[13,32,54]. Tothisend,onecanobviouslysimulatedatapoints
withpredetermineddataaugmentationmethodssuchasrotating,cropping,Gaussianblur,colorjitter,etc. However,their
developmentsrequirepriorknowledgeanddomain-specificexpertiseliketranslation-invarianceonimages,whichislikelyto
failintheunseendomainduetodistributionshifts. Itmotivateslearningdisentangledrepresentationsthataretransferable
across various domains [21]. Thus we discuss the benefits of disentanglement on the domain generalization gap in the
followingsection. AssumethatthesemanticandthevariationfactorsaredisentangledinthelatentspaceS andV,thenthe
errors[64]onthedisentangledsourceandtargetdomainwithahypothesishare
(cid:15) (h)=(cid:15)s (h)+(cid:15)v (h),(cid:15) (h)=(cid:15)s(h)+(cid:15)v(h) (33)
S,i S,i S,i U U U
Givenh∗ =argmin (cid:0) (cid:15)s (h),(cid:15)v (h)(cid:1) ∀i∈{1,...,N },since(cid:15) (h)=(cid:15)s(h)+(cid:15)v(h),combiningEq. (30)andwe
h∈H S,i S,i S U U U
have
(cid:15)s(h)+(cid:15)v(h)≤λ
+(cid:88)NS
α (cid:15) (h)+d
asingle
sm sm
outputvectorofafixed(1024)sizecannotexpressallthepossiblemappingsbetween100Mtrainingexamples
and33Kvocabularyoutputs. WehypothesizethatkNN-LMimprovesperformancebyalleviatingtheproblem,
sinceW ⊗h hasahigherrankandismoreexpressivethanjustW ·h. Inotherwords,kNNisa
ds ds sm sm
sparseapproximationofthefullsoftmaxoveralltheembeddingsinthedatastoreW. Totestthishypothesis,
ds
2BecausewepreviouslyfoundlittledifferencebetweenIPandL2assimilarityfunctions,weuseIPintheexperiments.
Forsimplicity,wesettemperatureτ =1.
6
wedisentangletheeffectofthehighrankinW fromtheactualsavedcontextembeddingsinW,bytraining
ds ds
anembeddingmatrixofthesamedesiredsizetotestfromscratch.
22.000
21.000
20.000
19.000
0.00 0.25 0.50 0.75 1.00
Ratio to Full Datastore Size
Figure2: TheeffectofthesizeofthedatastoreusedforkNNretrievalonfinalinterpolatedperplexity.
Weexploreseveralpotentialsolutionsforincreasingthecapacityofsoftmax,andexamineiftheycanachieve
asimilareffectofkNN-LM.Thefirstandeasiestsolutionistoincreasetheembeddingmatrixsizebyadding
moreembeddingvectorsforeachwordtypeinthevocabulary. Totestthis, wereplaceW withamuch
ds
smallermatrixofsizenV ×D,whereweallocatenembeddingvectorsforeachwordtype. Whencalculating
theprobabilityfromthiscomponent,wecomputethesoftmaxovernV itemsandsumtheprobabilitiesfor
eachvocabularyentrytocalculatethefinalprobability. mask-to-k(·)isnolongerneeded,asthisformulation
issmallenoughtofittheentirematrixintheGPU.WethenfinetunethenewW onthetrainingdatauntil
ds
convergence.
Figure3
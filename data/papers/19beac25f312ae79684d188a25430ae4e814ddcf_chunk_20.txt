onthetesttrack. Evenafterover1millionsteps totyping, training, and testing in this simulated environ-
environmentstepsonthetrainingtrack,theRL-SACagent ment, and (iii) the L2R task which defined dataset char-
only completes about 90% of a lap due to the challenging acteristics and concrete driving-inspired metrics for eval-
speed trap near the finish line at Thruxton. However, the uation. L2R addresses the lack of complex learning en-
RL-SACagentdemonstratesbettercontrolthantheMPCin vironments and introduces the challenging task of simu-
traininginbothADEandMS.Second,wenotethelackof lated,high-performanceracing. Whilehumanexpertshave
generalization and poor sample efficiency of the RL-SAC demonstrated strong results on this task, both using the
agentwhoseperformancedroppedsignificantlyintermsof L2R framework as well as in competition racing, learning
ability to progress down the track, ECP, and stay near the agentshavenot. Wehaveprovidedrelevantracingmetrics
centerline, ADE, despite being directly incentivised to do andbaselineresultsforclassicalcontrol,RL,andILagents
so.Theagentlearnstosimplystopaltogethertoavoidgoing aswellashumanexperts,andwearereleasingreferenceim-
out-of-boundsabout1/3rdofthewayaroundthetesttrack. plementationsandmodelcheckpointstofurtheradvancethe
We note that imitation learning has potential for providing research. TheL2Rsuiteoftasksandmetricswillcontinue
agentswithstrongpriors. However,inourexperiments,au- toexpandinthefutureincludingtheintroductionofmulti-
tomatic network sizing based on input/output dimensions agentracing. Wehopetosomedayseeagentsreachsuper-
andstep-wisesupervisionalone,suggestedby[14],didnot human,real-worldperformanceinautonomousracing.
yield good performance. This demonstrates the challenge
that L2R poses to this family of approaches, necessitating
Acknowledgements
considerationof,e.g.,jointIL/RLstrategies.
Therearemanypeoplethathelpedcreatethistaskenvi-
7
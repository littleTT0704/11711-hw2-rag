 we observe more than 60% rela-
tive improvement over just Plan and Track alone. We, 4.5. Qualitative Analysis
therefore,deducethatPlanandTrackboosttheperfor-
Plan Module We show two types of failure exam-
mance of Eliminate during evaluation, since it is easier
ples for sub-task generation in Table 4. The first type
toremoveirrelevantobjectswhentheobjectiveismore
oferroriscausedbygeneratingsynonymsoftheground
focused on sub-tasks.
truth, and the second type of error is caused by inaccu-
Plan, Eliminate, and Track
Template Goals Human Goals
LLM seen unseen seen unseen
GPT-2 (Radford et al., 2019) 94.29 (0.97) 87.31 (0.94) 10.07 (0.62) 7.98 (0.58)
GPT-Neo-2.7B (Black et al., 2021) 99.29 (1.00) 96.27 (0.98) 4.70 (0.82) 9.16 (0.80)
MT-NLG (Smith et al., 2022) 98.57 (0.99) 100 (1.00) 40.04 (0.94) 49.3 (0.94)
Table 2. Evaluation of different LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity
(in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated
goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds
theperformanceofsmallermodelsonhardtaskswithhumangoalspecification. Inaddition, MT-NLGgeneratessub-tasks
with almost perfect embedding similarity for all tasks.
Figure 6. Plot of AUC scores of zero-shot relevance identification across all tasks in the Alfworld-Thor environment,
with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert.
Top: Receptacle relevance identification. Bottom: Object relevance identification. The QA model achieves an average
AUC-ROC score of 65 for receptacles and 76 on objects.
Model Ablations seen unseen
etting,weclassifyanewseventasrealifitis
newsheadlines(Cuietal.,2020),seeTable7. For
notassociatedwithanypropagandatechniquesand
themisinformationdetectiontask,weevaluategold
misinformation otherwise. As shown by Table 7,
F1usingtheProp-BERTzero-shotmodel,MRF-
F1resultsareconsiderablylowerthantask-specific
finetunedBERT-large,Covid-BERT,T5-largeand
models. Thisislikelyduetothefactbothrealand
GPT-2 large models. We observe that after one
misinformationnewsusespropagandatechniques.
epochofre-training,maskedfine-tuningsubstan-
Neuralmisinformationdetectionmodelsareable
tially boosts unsupervised performance of gener-
tooutperformhumansatidentifyingmisinforma-
ative MRF models (GPT-2 large + masked and
tion (achieving a max F1 of 85.26 compared to
T5-large + masked), making them more robust
humanperformanceF1of75.2121),butthisisstill
thanBERTvariants. Wecomparethisperformance
anontrivialtaskforlarge-scalemodels. Whenwe
against the T5-large and GPT-2 large model fine-
use Covid-BERT (MuÂ¨ller et al., 2020), a variant
tunedononly574cancerexamples(GPT-2large
ofBERTpretrainedon160MCovid-relatedtweets,
+ sup and T5-large + sup), and observe that this
weseeanimprovementof5.46%overBERTwith-
leads to a performance increase of up to 43.49%,
outdomain-specificpretraining(Table7). Thisin-
achievingsimilarF1performancetoourdomains
dicatesgreateraccesstodomain-specificdatahelps
withfulldatasupervision.
inmisinformationdetection,eveniftheveracityof
6 FutureDirectionsandLimitationsof
20Themodelpredictsifanyof18knownpropagandatech-
ReactionFrames
niquesareusedtodescribeanewsevent
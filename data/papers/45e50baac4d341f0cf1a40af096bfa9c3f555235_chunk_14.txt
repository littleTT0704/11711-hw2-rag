pages638–654.Springer.
Task-levelMixture-of-ExpertsforEfficientInference.
In EMNLP (Findings), pages 3577–3599. Associa-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
tionforComputationalLinguistics.
Kristina Toutanova. 2019. BERT: Pre-training of
DeepBidirectionalTransformersforLanguageUn-
Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W.
derstanding. InProceedingsofthe2019Conference
Black, and Yulia Tsvetkov. 2019. Measuring Bias
oftheNorthAmericanChapteroftheAssociationfor
in Contextualized Word Representations. CoRR,
ComputationalLinguistics: HumanLanguageTech-
abs/1906.07337.
nologies,NAACL-HLT2019,Minneapolis,MN,USA,
June2-7,2019,Volume1(LongandShortPapers),
Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi
pages 4171–4186. Association for Computational
Zhang, Dan Jurafsky, Kathleen R. McKeown, and
Linguistics.
TatsunoriHashimoto.2023. WhenDoPre-Training
Biases Propagate to Downstream Tasks? A Case
MahaElbayad,JiataoGu,EdouardGrave,andMichael
Auli.2020. Depth-AdaptiveTransformer. InICLR. StudyinTextSummarization. InEACL,pages3198–
3211.AssociationforComputationalLinguistics.
OpenReview.net.
LeoGao,StellaBiderman,SidBlack,LaurenceGold- Paul Pu Liang, Irene Mengze Li, Emily Zheng,
ing, Travis Hoppe, Charles Foster, Jason Phang, YaoChongLim,RuslanSalakhutdinov,andLouis-
Horace He, Anish Thite, Noa Nabeshima, Shawn Philippe Morency. 2020. Towards Debiasing Sen-
Presser, and Connor Leahy. 2021. The Pile: An tence Representations. In ACL, pages 5502–5515.
800
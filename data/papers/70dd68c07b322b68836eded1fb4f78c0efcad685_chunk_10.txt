 Basedon
theseresults,weencouragethecommunitytocon-
siderthesecriteriaofpracticalapplicabilitywhen
developingandevaluatingtokenizer-freepretrained
models.
6 Limitations
ThispapermainlycoversthreeNLPtasks,focusing
onthesmaller-sizedmultilingualpretrainedmod-
els. Infuturework,itwouldbeinterestingtorun
the multi-dimensional evaluation we suggest on
a broader set of tasks and models. Although our
resultsshowthatsubwordmodelsareamoreprac-
ticalchoiceinsometasks,wenotethatothertasks
ordatasetsmayexistwheretokenizer-freemethods
achievebetterrelativeperformance. Forinstance,
tokenizer-freemodelshavebeenreportedtoexcel
inword-leveltasks,andnoisyenvironments(Xue
etal.,2022),andtheconclusionswereachedmay
be different in such settings. Moreover, we did
notexploremorecomplicatedgenerationtaskslike
translationorsummarization,wherethedifficulty
indecodingandlongerdecodehorizonscouldpaint
a different picture in a multi-dimensional evalua-
tion.
EthicsStatement
Wehopeourresultsencouragethecommunityto
considerthepracticalconcernsofrunninglargelan-
guagemodels(LLMs)anddesigningtokenizer-free
pretrained models. As the state-of-the-art LLMs
are becoming more computationally extensive, it
has become increasingly difficult for researchers
andpractitionerswithlessresourcestoutilizethese
modelsfordownstreamapplications. Wehopeour
multi-dimensional analysis can help researchers
andpractitionerswithlesscomputationalresources
decidewhichmodeltouseinpractice.
Acknowledgements
WeacknowledgeKakaoEnterpriseforproviding
thecomputeresourcesforthiswork. Additionally,
we would like to thank Jon Clark for answering
questionsrelatedtotheCANINEmodel. Thiswork
wassupportedinpartbygrant#2040926fromthe
References Xin Liu, Baosong Yang, Dayiheng Liu, Haibo Zhang,
Weihua Luo, Min Zhang, Haiying Zhang, and Jin-
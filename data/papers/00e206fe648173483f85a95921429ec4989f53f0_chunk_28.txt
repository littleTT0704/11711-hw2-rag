–
AlinaArseniev-Koehler,VickieMays,andKai-Wei 101,SanDiego,California.AssociationforCompu-
Chang. 2021. Adapting coreference resolution for tationalLinguistics.
processingviolentdeathnarratives. InProceedings
ofthe2021ConferenceoftheNorthAmericanChap- Shanheng Zhao and Hwee Tou Ng. 2014. Domain
teroftheAssociationforComputationalLinguistics: adaptationwithactivelearningforcoreferencereso-
10553
lution. InProceedingsofthe5thInternationalWork-
shoponHealthTextMiningandInformationAnaly-
sis(Louhi), pages21–29, Gothenburg, Sweden.As-
sociationforComputationalLinguistics.
Yilun Zhu, Sameer Pradhan, and Amir Zeldes. 2021.
Anatomy of OntoGUM—Adapting GUM to the
OntoNotes scheme to evaluate robustness of SOTA
coreference algorithms. In Proceedings of the
Fourth Workshop on Computational Models of Ref-
erence,AnaphoraandCoreference,pages141–149,
Punta Cana, Dominican Republic. Association for
ComputationalLinguistics.
A AdditionalResults
Forcompleteness,weadditionallyincluderesults
with singletons omitted from system output. Ta-
ble 5 reports results for both transfer settings
i2b2 CN and ON i2b2. In Figure 5, we in-
→ →
spect how performance changes with more anno-
tated data. We also report for completeness the Figure5:Eachsubplotshowscoreferenceperformance(sin-
difference in model performance using mention gletonsexcluded)whentrainedwithdifferentamountsofan-
notatedtargetdomaindata.Wevarytheamountofannotated
annotationsandfullcoreferenceannotationsinFig-
datawithrespecttothenumberofmentions.Whentransfer-
ure6fortransferbetweenOntoNotesgenreswith ringON i2b2(bottomrow), ourmodel(CLS,MDT)has
→
an equivalent amount of annotated data (unequal thelargestimprovementoverthebaseline(CLS,CLT)with
verylittletrainingdataorannotatortime
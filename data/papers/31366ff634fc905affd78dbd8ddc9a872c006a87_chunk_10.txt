. We use the
posed distinguishability than other metrics (Ap- human annotation of Evtikhiev et al. (2022) to
pendixF).Finally,weanalyzesomeofthedesign measure the correlation between each metric and
decisionsandtheirimplications. human preference. More details are provided in
AppendixB.1.
3.1 TrainingLanguage-specificCodeBERT
models Functional correctness experiments We
evaluate functional correctness using the Hu-
Training We used CodeBERT (Feng et al.,
manEval (Chen et al., 2021) benchmark. Each
2020)asourbasemodel(B)andcontinueditsself-
example in HumanEval contains a natural
supervised pretraining (Gururangan et al., 2020)
language goal, hand-written input-output test
with the masked language modeling (MLM) ob-
cases, and a human-written reference solution.
jective(Devlinetal.,2019)onPython,Java,C++,
While the original HumanEval is in Python,
C, and JavaScript corpora. We trained a sepa-
Cassano et al. (2022) translated HumanEval to
rate model for each programming language, for
18 programming languages, and provided the
1,000,000 steps for each language, using a batch
predictionsoftheCodexmodel(Chenetal.,2021)
sizeof32,aninitiallearningrateof5e−5,decayed
(code-davinci-002)andtheircorresponding
linearly to 3e−5. Our implementation is based on
functional correctness.4 We used Java, C++,
the widely used HuggingFace Transformers
Python, and JavaScript for these experiments,
library (Wolf et al., 2019) and BERTScore2, and
whicharesomeofthemostpopularprogramming
itsupportsanytransformer-basedmodelavailable
languages in open-source projects.5 More details
ontheHuggingFacehub.
areprovidedinAppendixB.2.
Dataset Wetrainedeachmodelonthelanguage-
Hyperparameters Wetunedonlythefollowing
specific subset of the CodeParrot (Tunstall et al.,
hyperparametersforCodeBERTScore: whetherto
2022) dataset3, which consists of
 may also be
associated with a weight. (b) With the indicator function ğ•€ *(t) as the similarity metric and the
t
*
default uniform weight of each observed t (e.g., Equation 4.2), the distribution of the
experience function value matches the empirical data distribution (i.e., uniform over the
observed instances, and 0 on all other configurations in ğ’¯). (c) By associating different
*
observed instances t with different weights (Equation 4.6, data reweighting), the experience
function corresponds to the reweighted empirical distribution. (d) By setting the similarity
metric to a soft version (Equation 4.8, data augmentation), the experience function
corresponds to a distribution over ğ’¯ that is more smooth, with nonzero probability on
configurations other than the observed data instances.
4.1.1. Supervised Data Instances
Without loss of generality and for consistency of notations with the rest of the section, we consider data
instances to consist of a pair of input-output variables, namely t = (x,y). For example, in image
classification, x represents the input image and y is the object label. In the supervised setting, we observe the
full data drawn i.i.d. from the data distribution (xâˆ—,yâˆ—) âˆ¼ p (x,y). For an arbitrary configuration
d
(x,y ), its probability p (x,y ) under the data distribution can be seen as measuring the expected
0 0 d 0 0
similarity between (x,y ) and true data samples (xâˆ—,yâˆ—), and be written as
0 0
p d(x 0,y 0) = Ep d(xâˆ—,yâˆ—) [I(xâˆ—,yâˆ—)(x 0,y 0)]. Here the similarity measure is I(xâˆ—,yâˆ—)(x,y), an indicator
function that takes the value 1 if (x,y) equals (xâˆ—,yâˆ—) and 0 otherwise (we will see other similarity
~
measures shortly). In practice, we are given an empirical distribution p (x,y) by observing a collection of
d
instances D on which the expected similarity is evaluated:
m(x,
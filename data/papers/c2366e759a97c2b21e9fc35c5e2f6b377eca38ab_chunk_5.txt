 scene is converted to a set of instances. Each
instance is comprised of attributes, i.e. pose, category, timestep, and whether it is an object or
placeinstance. (Right)Instanceattributes(gray)arepassedtotheencoder, whichreturnsinstance
embeddingsforpickableobjects(red),placeblelocations(green),and<ACT>embeddings(blue).
Thetransformeroutputsachosenpick(red)andplace(green)instanceembedding.
Instance Encoder Each object and goal instance x, g is projected into a higher-dimensional
i j
vector space (Fig. 2a). Such embeddings improve the performance of learned policies [18, 25].
FortheposeembeddingΦ,weuseapositionalencodingschemesimilartoNeRF[25]toencode
p
the3Dpositionalcoordinatesand4Dquaternionrotationofaninstance. Forcategory, weusethe
dimensions of the 3D bounding box of the object to build a continuous space of object types and
process this through an MLP Φ. For each discrete 1D timestep, we model Φ as a learnable em-
c t
beddinginalookuptable,similartothepositionalencodingsinBERT[26]. Toindicatewhetheran
instanceisanobjectorplacementlocation,weadda1Dbooleanvaluevectorizedusingalearnable
embeddingfunctionΦ. Theconcatenatedd-dimensionalembeddingforaninstanceattimestept
r
isrepresentedasf (x ) = Φ ||Φ ||Φ ||Φ = e. Theencodedstateattimetcanberepresentedin
e i t c p r i
termsofinstancesas: Senc =[e,e,···e ]. Wedrop()encforbrevity.
t 0 1 N
Demonstrations are state-action sequences C = {(S,a ),(S,a ),···,(S,a ),(S )}.
0 0 1 1 T−1 T−1 T
HereS isthesetofobjectandplaceinstancesanda arethepick-placeactionschosenbyexpertat
i i
timei. Ateverytimestep,werecordthestate
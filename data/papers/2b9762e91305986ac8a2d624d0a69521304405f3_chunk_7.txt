objectivesthatmakeuseofit. Pre- BUCC tasks since recent advances have left less
trainingonparalleldataimprovesperformanceon roomforfurtherimprovement,andtheycoveronly
retrievalbymakingthepre-trainingtaskmoresim- a small number of less diverse languages. They
Taskcategory Task |Train| |Dev| |Test| Testsets |Lang.| Task Metric Domain
XNLI 392,702 2,490 5,010 translations 15 NLI Accuracy Misc.
Classification
XCOPA 33,410+400 100 500 translations 11 Reasoning Accuracy Misc.
UD-POS 21,253 3,974 47-20,436 ind.annot. 37(104) POS F1 Misc.
Struct.prediction
WikiANN-NER 20,000 10,000 1,000-10,000 ind.annot. 47(176) NER F1 Wikipedia
XQuAD 1,190 translations 11 Spanextraction F1/EM Wikipedia
87,599 10,570
QA MLQA 4,517–11,590 translations 7 Spanextraction F1/EM Wikipedia
TyDiQA-GoldP 3,696 634 323–2,719 ind.annot. 9 Spanextraction F1/EM Wikipedia
Tatoeba 87,599 10,570 1,000 translations 38(122) Sentenceretrieval Accuracy Misc.
Retrieval Mewsli-X 116,903 10,252 428–1,482 ind.annot. 11(50) Lang.agn.retrieval mAP@20 News
LAReQAXQuAD-R 87,599 10,570 1,190 translations 11 Lang.agn.retrieval mAP@20 Wikipedia
Table2: ThetasksinXTREME-R. Fortasksthathavetraininganddevsetsinotherlanguages,weonlyreportthe
Englishnumbers. Wereportthenumberoftestexamplespertargetlanguage,thenatureofthetestsets(whether
theyaretranslationsofEnglishdataorindependentlyannotated),andthesizeoftheintersectionwithourselected
languages(thetotalnumberoflanguagesisinbrackets). ForWikiANN-NERandUD-POS,sizesareinsentences.
XCOPAincludestrainingdataofSIQa(Sapetal.,2019
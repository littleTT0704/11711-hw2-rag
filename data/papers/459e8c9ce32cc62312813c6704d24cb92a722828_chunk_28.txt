LanguageProcessingandthe9thInternationalJointCon-
HLT2019:Demonstrations.
ferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-j. 2002. 3354–3360. Hong Kong, China: Association for Compu-
BLEU : a Method for Automatic Evaluation of Machine tational Linguistics. doi:10.18653/v1/D19-1331. URL
Translation. ComputationalLinguistics. https://www.aclweb.org/anthology/D19-1331.
Pourchot;andSigaud.2019. CEM-RL:Combiningevolu- Sutskever,I.;Martens,J.;andHinton,G.2011. Generating
tionary and gradient-based methods for policy search. In textwithrecurrentneuralnetworks. InProceedingsofthe
InternationalConferenceonLearningRepresentations. URL 28thInternationalConferenceonMachineLearning,ICML
https://openreview.net/forum?id=BkeU5j0ctQ. 2011. ISBN9781450306195.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Vaswani,A.;Shazeer,N.;Parmar,N.;Uszkoreit,J.;Jones,
and Sutskever, I. 2018. Language Models are Unsu- L.;Gomez,A.N.;Kaiser,Ł.;andPolosukhin,I.2017. At-
pervised Multitask Learners. In OpenAI. URL https: tentionisallyouneed. InAdvancesinNeuralInformation
//d4mucfpksywv.cloudfront.net/better-language-models/ ProcessingSystems. ISSN10495258.
language models are unsupervised multitask learners.pdf.
Vinyals,O.;Quoc,G.;andLe,V.2015. ANeuralConversa-
Ranzato,M.;Chopra,S.;Auli,M.;andZaremba,W.2016. tionalModel. InICMLDeepLearningWorkshop.
Sequenceleveltrainingwithrecurrent
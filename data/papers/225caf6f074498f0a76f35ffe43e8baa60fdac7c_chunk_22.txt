.7 60.9 77.4 88.6 38.3 21.8 57.6
Table4: AblationsofVMSST.Weinvestigateablationsinvolvingfactorizationofthedecoderprojectionlayer
(fact.),using4languageencodersinsteadof1(4enc.),using12layer(12Ldec.) and1layer(1Ldec.) decoders,
usingnoKLterm(noKL),usingonlyasingleencoderforbothlanguageandsemanticvariables(1enc.),andusing
noencoderlanguageembeddings(noenc. l.e.) ornodecoderlanguageembeddings(nodec. l.e.).
Model de-en fr-en ru-en zh-en Avg. sionsizeandV isthesizeofthevocabulary.13 If
we factor the projection layer, we can reduce the
mUSE 88.5 86.3 89.1 86.9 87.7
LASER 95.4 92.4 92.3 91.2 92.8 space to d×V +3d×d. In practice, this saves
XLM-R(NLI/STS-B) 86.8 84.4 86.3 85.1 85.7
about509millionparametersforour24layermod-
XLM-R(Para.) 90.8 87.1 88.6 87.8 88.6
LaBSE 95.9 92.5 92.4 93.0 93.5 els. HoweverfromthefirstrowinTable4,wesee
that this small change has a significant effect on
VMSST 94.3 91.0 91.8 92.8 92.5
performance,weakeningresultsonsemanticsimi-
larityandquestionretrievaltasksandstrengthening
Table 5: Comparisons to related work on BUCC in
resultsonbitextminingtasks.
accuracy×100usingthemarginapproachfrom Artetxe
andSchwenk(2019a). Notethatmodelsinthistableare In our second ablation, VMSST (4 enc.), we
nottrainedonthesamedata;forinstanceLaBSEwas spreadthemodelcapacityofthelanguage-specific
trainedonsubstantiallymoreparalleldataandXLM-R encoderto4enc
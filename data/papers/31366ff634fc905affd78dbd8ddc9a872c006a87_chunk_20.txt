aries. InTextsummariza-
Technologies, Volume 1 (Long and Short Papers), tionbranchesout,pages74–81.
pages4171–4186,Minneapolis,Minnesota.Associ-
Dana Movshovitz-Attias and William Cohen. 2013.
ationforComputationalLinguistics.
Natural language models for predicting program-
mingcomments. InProceedingsofthe51stAnnual
AryazEghbaliandMichaelPradel.2022. Crystalbleu:
Meeting of the Association for Computational Lin-
preciselyandefficientlymeasuringthesimilarityof
guistics(Volume2: ShortPapers),pages35–40.
code. In37thIEEE/ACMInternationalConference
onAutomatedSoftwareEngineering,pages1–12.
KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu: amethodforautomaticeval-
Mikhail Evtikhiev, Egor Bogomolov, Yaroslav
uationofmachinetranslation. InProceedingsofthe
Sokolov, and Timofey Bryksin. 2022. Out of the
40th Annual Meeting of the Association for Com-
bleu: howshouldweassessqualityofthecodegen-
putationalLinguistics,pages311–318,Philadelphia,
erationmodels? ArXivpreprint,abs/2208.03133.
Pennsylvania,USA.AssociationforComputational
Linguistics.
ZhangyinFeng,DayaGuo,DuyuTang,NanDuan,Xi-
aochengFeng,MingGong,LinjunShou,BingQin,
YiweiQin,WeizheYuan,GrahamNeubig,andPengfei
TingLiu,DaxinJiang,andMingZhou.2020. Code-
Liu. 2022. T5score: Discriminative fine-tuning
BERT: A pre-trained model for programming and of generative evaluation metrics. arXiv preprint
natural languages. In Findings of the Association arXiv:2212.05726.
forComputational
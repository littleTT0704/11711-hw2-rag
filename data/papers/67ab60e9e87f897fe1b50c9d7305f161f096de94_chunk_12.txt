Chen et al., 2020a) (without supervised labels) to
7
Turian et al.
distill theContrastive Language-Image Pre-training (CLIP,Radford et al. (2021)) language
and image model to a corresponding audio embedding (Wu et al., 2022a).
IUT-CSE kwmlp and audiomlp Sequentially stacked gated MLP model (Liu et al.,
2021), taking (2-D) MFFCs as input. kwmlp (Morshed et al., 2022) is pretrained with
supervision on Speech Commands v2. audiomlp is pretrained with supervision on HEAR
open task datasets: Speech Commands v2, DCASE 2016 Task 2, and NSynth Pitch.
Kuroyanagi hearline 2-D conformer model. Pretraining unknown.
Logitech AI SERAB BYOL-S 2-D CNN. Self-supervised pretraining using the BYOL
self-supervised approach (Grill et al., 2020) adapted to audio (BYOL-A, Niizumi et al.
(2021)), pretrained on the speech subset of AudioSet (Elbanna et al., 2022).
NTU-GURA (fusion) avg/cat hubert/wav2vec2/crepe Three models (HuBERT
Hsu et al. (2021), wav2vec2, CREPE) combined in a variety of ways: averaged or con-
catenated (Wu et al., 2022b). Fusion of multiple model layers was optionally included.
fusion cat xwc time is a variation of fusion cat xwc with a different approach to matching
timestamps when concatenating different modelsâ€™ emmbeddings.
RedRice/Xiaomi EfficientNet-B2 2-DEfficientNet-B2(Tan and Le,2019). Pretrained
on supervised AudioSet tags. Instead of global averaging pooling, decision-level pooling is
used. Timestamp embeddings are smeared scene embeddings.
Sony UDONS ViT Vision transformer (ViT, Kolesnikov et al. (2021)). Pretrained on
360hofLibrispeechtopredictthecorrectpermutation(Noroozi and Favaro,2016;Carr et al.,
2021)
ourlistenertoatleastmodelacompetentuserof
thelanguagethatthespeakeristryingtoacquire,somepretrainingisrequiredbeforeinitializinga
listenerinanewenvironment. Thecontrollervaluesθ,θ andtheResNetparametersaresetafter
1 2
preliminary experimentation on the dataset. The listener’s language network parameters are then
trainedwithmini-batchstochasticgradientdescenttooptimizethenetworkvalues
N
1 (cid:88)
θ =argmaxE logP (i|U∗,C;θ) (3)
listener
θ
C∼U(I)NN listener i
i=1
3 SPEAKER TRAINING PROCESS
3.1 COMMUNICATIVEGOALS
Wemodelcommunicativegoalsandlearningfromlinguisticinputastwoseparatelearningobjec-
tivesforourspeakernetwork,whichwecombinewithanadjustablecoefficientλ. Thecommunica-
tivegoalrewardO issimplytheaveragereward(definedin§2)overeachgameinthetraining
CG
episode. Thespaceofpossibleoutpututterancesisdiscreteandnon-differentiable, soourspeaker
learnsitspolicyπviathereinforcementlearningmethodPPO(Schulmanetal.,2017).
3.2 LEARNINGFROMLINGUISTICINPUT
We model the learning from linguistic input objective as the maximum likelihood of the listener
inputinthespeakermodelsandoptimizethiscontinuousfunctionwithstochasticgradientdescent.
O =E logπ(U∗ |x,C) (4)
LI x,C,u∼π(u|x) flistener(u,C)
Wethencombinetheseparatelearningobjectivesforeachtaskusingacoefficientλ∈[0,1]:
O =λO +(1−λ)O (5)
joint CG LI
Next,weintroduceToMModeling,andanadditionaltaskobjective,O. Thisisacross-entropy
ToM
objectivethatrepresentshowaccuratethespeaker’sinternallistenermodelis. Sim
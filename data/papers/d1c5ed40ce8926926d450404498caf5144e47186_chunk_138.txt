 the
number of features in the answer scoring models increased, requiring more and more
training data to avoid overfitting. In experiments with the most effective setup of
Watson using all available sources including SE, the transfer learning step improved
QA accuracy on TREC questions by 7.7 percentage points. However, because most
of the TREC questions were used for training, we were left with only TREC 11 (444
questions with known answers) as an independent test set.
6.4.2 Results and Analysis
In Table 6.14 we report Watsonâ€™s candidate recall with and without source expansion
on Jeopardy! and TREC questions when using Wikipedia, Wiktionary or the collec-
tion of all manually acquired sources as a baseline. It can be seen that our approach
improves candidate recall significantly (with p <.01) independently of the dataset
and seed corpus. When using all sources, our method yields a 5.2% increase in candi-
date recall on regular Jeopardy! questions, a 11.0% increase on Final Jeopardy! and
a 3.7% increase on TREC 11. The gain on the Final Jeopardy! dataset is largest be-
causethereismoreheadroomforimprovements, withonly68.02%recallasabaseline.
The improvement on TREC 11 is a conservative estimate because the TREC answer
keys are based on correct answers found in the reference corpus, which is included in
the baseline, but they do not cover some of the additional correct answers found in
the expanded sources.
100 CHAPTER 6. APPLICATION TO QUESTION ANSWERING
Regular J! Final J! TREC 11
Wikipedia 75.43% 61.17% 77.70%
Expansion 80.84% 70.43% 81.53%
% Gain +7.2% +15.1% +4.9%
# Gain/Loss +248/-58 +92/-19 +27/-10
Wiktionary 25.54% 15.23% 23.65%
Expansion 45.32% 25.76% 45.50%
% Gain +77.4% +69.1% +92.4%
# Gain/Loss +808/-114 +102/-19 +110/-13
All Sources 82.10% 68.02% 84.68%
Exp
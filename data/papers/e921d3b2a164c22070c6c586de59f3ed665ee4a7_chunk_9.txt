 (6) l r
if
q(z|x)(cid:88) min(|D l|,|D r|) <min leaf sample ratio
=logp(y|x) q(z|x) #oftotalsamples
q(z|x)
thencontinue;
z
(cid:88) p(y,z|x)q(z|x)
node.TrainClassifier(D,t)
= q(z|x)log
PropagateconditionalsusingEquation(3)
p(z|y,x)q(z|x)
z node l.TrainLeaf(D )
l
(cid:88) p(y,z|x) node r.TrainLeaf(D )
= q(z|x)log + (7) r
q(z|x) Q←ComputeQ usingEquation(10)
z
end
(cid:88) q(z|x)
q(z|x)log, (8) ifQ>Q∗then
p(z|y,x)
z Q∗ ←Q
whereq(z|x)isanestimateforthetrueassignmentmass data list←[D l,D r]
p(z|x);(7)iscommonlyreferredtoastheevidencelower
nodes per level←[node l,node r]
GrowTree(data list,nodes per level)
bound(ELBO)whichneedtobeimprovedtomaximizethe
else
log-likelihood(6);(8)istheKullback-Leiblerdivergence
Deletethesubtree
whichmeasuresthedistanceoftwoprobabilitymassesand
continue
isalwaysgreaterthanorequaltozero.
end
Therefore, it is natural to apply the expectation- end
maximization(EM)methodtooptimize(6). Specifically,in Algorithm1:RecursiveEMLearningofHRME
theE-step, wecomputetheELBO(7)forallthetraining
instances
(cid:88)(cid:88) p(y,z|x) difficulty. Foreachnon-leafnode,weperformgrid-search
Q
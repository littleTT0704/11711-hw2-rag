79% 44.02% 46.07%
Diversity × Uncertainty 45.38% 44.89% 48.10% 42.14%
Diversity → Uncertainty 45.91% 43.94% 44.63% 45.57%
Table8.1: Percentageofrelevantinstancesselectedbydifferentcombinationsoflearn-
ing methods and query strategies. We show results for models that make independent
predictions using only the original features, and models that include features of ad-
jacent instances to capture dependencies.
the relatively low performance of this strategy. If the instance with the maximum
relevance estimate is chosen in each iteration, about 89–92% of the queries selected
after 1,000 steps are positive and thus the sample is again highly unbalanced. In the
longrun,uncertaintysamplingselectsaroughlyequalnumberofpositiveandnegative
instances since it focuses on hard queries that are close to the decision boundary but
can be on either side of it. The reported numbers are below 50% because in early
iterations the models are still inaccurate and it is more likely that negative instances
are selected if uninformed choices are made.
Diversity-based query selection yields a larger number of positive instances than
the random sampling method because it selects more queries from sparse areas in the
feature space. Thisis where most ofthe positive instances can be expected since there
are fewer relevant text nuggets and their feature values can vary widely. Compared
to uncertainty-based methods, this approach selects more positive instances in early
iterationswhilethemodelsusedforuncertaintysamplingarestillunreliable,butfewer
relevantinstancesinthelongrun. Thisisonereasonwhydiversitysamplingisinitially
so effective but is ultimately outperformed by methods that incorporate a measure
of uncertainty. In general, we found that the fastest rates of convergence are often
attained by sampling strategies that select the most balanced data, independently
of the number of active learning steps. This observation consistently holds for both
logistic regression and SVMs, and for both independent prediction models that use a
small feature set and models that leverage features of adjacent instances.
Logistic regression models and linear SVMs trained by active learning have mean
average precisions of up to 79.33% and 80.02%, respectively. In contrast, LR models
with features of adjacent instances fitted to all data
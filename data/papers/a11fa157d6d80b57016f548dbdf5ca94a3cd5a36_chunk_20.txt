weights thanthebasemodels.
towards the objective of logP(t|h,r) on training Does better pretraining bring better knowl-
samples. edge? RoBERTa uses the same architecture as
Theresultingprecision-recallcurvesonConcept- BERTbutwithbetterpretrainingstrategies,likedy-
NetandLAMAknowledgeareshowninFigure4 namicmasking,largerbatchsize,etc. Intheircorre-
andFigure5,respectively. Scoringwithmultiple spondingKGsfromourframework,RoBERTaNet-
prompts always achieves best performance, fol- largeperformsbetterthanBertNet-large(0.73v.s.
lowedbyTop-1promptsandthenHuman-written 0.70), and RoBERTaNet-base is also better than
noisicerP noisicerP
BertNet-base(0.70v.s. 0.63),showingthatthebet- languagemodelsasknowledgebases. arXivpreprint
terpretraininginRoBERTaleadstobetterknowl- arXiv:2204.06031.
edgelearningandstorage.
GregAnderson,AbhinavVerma,IsilDillig,andSwarat
Is knowledge really kept in the knowledge
Chaudhuri. 2020. Neurosymbolic reinforcement
distillation process? DistilBERT is trained by learning with formally verified exploration. Ad-
distilling BERT-base, and it reduces 40% param- vances in neural information processing systems,
etersfromthelatter. Interestingly,theknowledge 33:6172â€“6183.
distillation process instead improves around 4%
Gabor Angeli, Melvin Johnson, and Christopher D.
of accuracy in the result knowledge graph. This
Manning.2015. Leveraginglinguisticstructurefor
shouldbeattributedtotheknowledgedistillation opendomaininformationextraction. InACL.
process which might eliminate some noisy infor-
mationfromtheteachermodel. Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
5 Conclusion 2013. Translatingembeddingsformodelingmulti-
relational
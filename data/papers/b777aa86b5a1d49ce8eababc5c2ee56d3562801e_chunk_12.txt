
10 2
Figure 6: Comparison of latency for BERT variants
thatscalemodelwidthanddepth. Increasesinmodel
100 101 102
depthaddmoreframeworkoverhead,whereasincreases
Batch Size
in model width lead to faster transitions to compute
Figure5: Differentframeworkoptimizationsleadtola-
boundedness.
tencyimprovementsindifferentregimesforBERT-Base.
CUDAGraphkernelserializationreduceslaunchover-
headintheframeworkboundregime,whereassparse Wecompareourbaselinemodelstovariantsthat
computationreduceslatencyatlargerbatchsizes. scale both model width and depth. We examine
12-layerBERT-Baseagainstits6-layerDistilBERT
(Sanhetal.,2019)variantandexperimentacross
toremoveredundantcomputation.
parameterizedBERTmodels,varyingthenumber
To construct sparse inputs, we simulate sam-
of encoder layers as well as width of their fully
plesbygeneratingvariablelengthsequencesand
connectedandself-attentionlayers.
paddingtothemaximumsequencelengthof128.
Sentencesarerandomlygeneratedaccordingtothe Analysis Whenscalingmodeldepth,weobserve
sequencelengthdistributionofthePennTreebank thatlatencyincreasesinboththeframework-and
(Taylor et al., 2003), with an average length of compute-bound regimes as each added layer op-
20.92andastandarddeviationof10.18tokens. erationrequiresanadditionalCPU-GPUdispatch.
Deepermodelvariantshavealargerfixedlatency
Analysis UtilizationofCUDAGraphssubstan-
intheframework-boundregimeasseeninFigure6.
tiallyreducelatencyatlowbatchsizeswheninfer-
Counter-intuitively,widermodelvariationssee
enceisboundedbyframeworkoverheadfromker-
noincreaseinlatencyatlowbatchsizes. Asmodel
nellaunches. However,atlargerbatchsizes,nested
executionisframeworkbound,totalruntimeiscon-
tensoroperationscanleveragesparsityinpadded
stantdespitewideroperationsrequiringmorefloat
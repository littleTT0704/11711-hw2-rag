chain
ofthoughtreasoninginlanguagemodels.
YueWang,WeishiWang,ShafiqJoty,andStevenC.H.
Hoi. 2021b. Codet5: Identifier-aware unified pre-
trainedencoder-decodermodelsforcodeunderstand-
ingandgeneration. InEMNLP.
Zhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F.
Xu,andGrahamNeubig.2023. MCoNaLa: Abench-
markforcodegenerationfrommultiplenaturallan-
guages. InFindingsoftheAssociationforCompu-
tational Linguistics: EACL 2023, pages 265–273,
Dubrovnik,Croatia.AssociationforComputational
Linguistics.
DavidGrayWidder,DawnNafus,LauraDabbish,and
JamesHerbsleb.2022. Limitsandpossibilitiesfor
“ethicalai”inopensource: Astudyofdeepfakes. In
Proceedingsofthe2022ACMConferenceonFair-
ness,Accountability,andTransparency,FAccT’22,
page2035–2046,NewYork,NY,USA.Association
forComputingMachinery.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RemiLouf,MorganFuntow-
icz,JoeDavison,SamShleifer,PatrickvonPlaten,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
QuentinLhoest,andAlexanderRush.2020. Trans-
formers:State-of-the-artnaturallanguageprocessing.
InProceedingsofthe2020ConferenceonEmpirical
Methods in Natural Language Processing: System
Demonstrations,pages38–45,Online.Association
forComputationalLinguistics.

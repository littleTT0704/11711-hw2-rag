setting,usingthesametrain-
beddings,acomparisonthathasnotbeendone
ingdataandarchitecture. Weshowthatourmodel
tothebestofourknowledgedespitethepop-
outperformsthesemodelsandisalsocompetitive
ularityoftheseapproaches. Weevaluatethis
withthestate-of-the-art.
method on a suite of tasks including seman-
ticsimilarity,bitextmining,andcross-lingual We analyze VMSST with careful ablations,
questionretrieval—thelastofwhichweintro- showing the contribution of each aspect of the
duce in this paper. Overall, our Variational model to performance. We also show that even
Multilingual Source-Separation Transformer
atlargebatchsizes,theadvantageovercontrastive
(VMSST) model outperforms both a strong
learningremains,especiallyforlargemodels. Fur-
contrastive and generative baseline on these
thermore,wealsofindthelearnedembeddingspace
tasks.1
of our model to be smoother, making it less af-
1 Introduction fected by the “hubness problem” (Radovanovic
etal.,2010;Radovanovic´ etal.,2010)inrepresen-
Contrastivelearningisthedominantparadigmfor
tation learning, and more suitable for large-scale
learningtextrepresentationsfromparalleltext(Her-
retrievalthanthebaselinemethods.
mannandBlunsom,2014;Singlaetal.,2018;Guo
To the best of our knowledge, this is the first
etal.,2018;Wietingetal.,2019;Fengetal.,2022).
work to systematically compare generative and
However,contrastivelearningrequiresstrongneg-
contrastive models for learning multilingual em-
ativeexamplesinthedataandfindingthesenega-
beddings on a large parallel corpus containing
tivescanbeexpensiveintermsofcomputeorman-
many languages in a carefully controlled experi-
ualeffort. Inthispaper,weproposeagenerative2
mentalsetup—despitethepopularityoftheseap-

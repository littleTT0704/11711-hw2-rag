abeled data by encouraging invariant predictions to input perturbations
[13,14,63,64,65,66,67]. Suchconsistencyregularizationmethodsgivethestrongestperformance
in SSL sincethe model is robust to differentperturbed versionsof unlabeled data, satisfying the
smoothnessandlow-densityassumptionsinSSL[68].
The above SSL algorithms use Cross-Entropy (CE) loss on labeled data but differ in the way on
unlabeleddata. AsshowninTable4,PseudoLabeling[59]turnsthepredictionsoftheunlabeled
data into hard ‘one-hot’ labels and treats the ‘one-hot’ pseudo-labels as the supervision signals.
Thresholdingreducesthenoisypseudolabelsbymaskingouttheunlabeledsampleswhosemaximum
probabilitiesaresmallerthanthepre-definedthreshold. DistributionAlignmentaimstocorrectthe
outputdistributiontomakeitmoreinlinewiththetargetdistribution(e.g.,uniformdistribution).
Self-supervisedlearning,Mixup,andStrongeraugmentationstechniquesalsocanhelplearnbetter
representation. MoredetailsofthesealgorithmscanbefoundinAppendixF.Wesummarizethekey
componentsexploitedintheimplementedconsistencyregularizationbasedalgorithmsinTable4.
5 BenchmarkResults
ForCVtasks,wefollow[21]toreportthebestnumberofallcheckpointstoavoidunfaircomparisons
causedbydifferentconvergencespeeds. ForNLPandAudiotasks,wechoosethebestmodelusing
thevalidationdatasetsandthenevaluateitonthetestdatasets. Inadditiontomeanerrorrateoverthe
tasks,weuseFriedmanrank[69,70]tofairlycomparetheperformanceofdifferentalgorithmsin
varioussettings:
m
1 (cid:88)
rank = rank,
F m i
i=1
where m isthe number ofevaluation settings(i.e., howmanyexperimental settingswe use, e.g.,
m = 9inTable5), andrank istherankofanSSLal
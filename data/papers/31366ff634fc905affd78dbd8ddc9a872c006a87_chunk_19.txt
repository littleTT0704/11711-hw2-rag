bor, Michigan. Association for Computational Lin-
whilemostpreviousapproachesarecomputation-
guistics.
ally cheaper (e.g., by counting n-grams). How-
ever, since training and testing neural models re-
FedericoCassano,JohnGouwar,DanielNguyen,Syd-
quire GPU anyways, we can safely assume that a ney Nguyen, Luna Phipps-Costin, Donald Pinck-
GPUisavailable. Further, BERT-basemodelsare ney, Ming Ho Yee, Yangtian Zi, Carolyn Jane An-
derson, Molly Q Feldman, et al. 2022. A scalable
encoder-only and non-autoregressive; this means
and extensible approach to benchmarking nl2code
that they require only a single “forward pass”,
for 18 programming languages. ArXiv preprint,
compared to encoder-decoder models (e.g., T5) abs/2208.08227.
and decoder-only models (e.g., GPT-3) that need
toautoregressivelygeneratetokenaftertoken,us- MarkChen,JerryTworek,HeewooJun,QimingYuan,
HenriquePonde,JaredKaplan,HarriEdwards,Yura
ing a forward pass for each output token. Thus,
Burda, Nicholas Joseph, Greg Brockman, et al.
the additional time consumption by encoder-only
2021. Evaluatinglargelanguagemodelstrainedon
models(e.g.,BERT)isnegligible,especiallywhen code. ArXivpreprint,abs/2107.03374.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and DalLago,etal.2022. Competition-levelcodegen-
Kristina Toutanova. 2019. BERT: Pre-training of eration with alphacode. Science, 378(6624):1092–
deepbidirectionaltransformersforlanguageunder- 1097.
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association Chin-Yew Lin. 2004. Rouge: A package for auto-
for Computational Linguistics: Human Language maticevaluationofsumm
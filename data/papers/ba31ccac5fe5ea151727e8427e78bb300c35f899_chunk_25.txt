laterALstages. tagging(MarcheggianiandArti√®res,2014;Chaud-
ThereisstillagapbetweenPAandFAintheearlier haryetal.,2019;Radmardetal.,2021),word-wise
ALstages,whichmaybeinfluencedbytheerrors headedgesfordependencyparsing(Flanneryand
produced by the first sub-task of mention extrac- Mori,2015;Lietal.,2016)andmentionlinksfor
tion. Weleavefurtherinvestigationsonimproving coreference resolution (Li et al., 2020; Espeland
early AL stages to future work. The results for etal.,2020).
therelationextractiontasksharesimilartrendsand
arepresentedinAppendixD.2,togetherwiththe 5 Conclusion
resultsofmentionextraction.
In this work, we investigate better AL strategies
4 RelatedWork forstructuredpredictioninNLP,adoptingaperfor-
manceestimatortoautomaticallydecidesuitable
Self-training. Self-training is a commonly uti-
ratiosforpartialsub-structureselectionandutiliz-
lizedsemi-supervisedmethodtoincorporateunla-
ingself-trainingtomakebetteruseoftheavailable
beleddata. Ithasbeenshowneffectiveforavariety
unlabeleddatapool. Withcomprehensiveexperi-
of NLP tasks, including word sense disambigua-
ments on various tasks, we show that the combi-
tion (Yarowsky, 1995), parsing (McClosky et al.,
nation of PA and self-training can be more data-
2006),namedentityrecognition(Mengetal.,2021;
efficientthanstrongfullALbaselines.
Huangetal.,2021),textgeneration(Heetal.,2020;
Mehta et al., 2022) as well as natural language
Limitations
understanding (Du et al., 2021). Moreover, self-
trainingcanbeespeciallyhelpfulforlow-resource
This work has several limitations. First, the AL
scenarios, suchasinfew-shotlearning(Vuetal.,
experimentsinthisworkarebasedonsimulations
2021; Chen et al., 2021). Self
uder,2018;Hanetal.,2021). Becausethere narios – there is significant work on multilingual
are a myriad of methods for tackling this impor- adapters(Pfeifferetal.,2020b;Anselletal.,2021)
tant task of fine-tuning LMs, there is an increas- andparametertyingacrosslanguages(Sachanand
ing body of research investigating the empirical Neubig, 2018; Lin et al., 2021), but few studies
strengthsandweaknessesofdifferenttuningstrate- comparingdifferentfamiliesofmethods.
gies across several tasks (Peters et al., 2019; Ma- Inthispaper,wetrytofillthisgapbyperform-
habadietal.,2021;KarimiMahabadietal.,2021; ingacomprehensivestudyofdifferentparameter
Li and Liang, 2021; Mao et al., 2021; Hu et al., tuning techniques in the context of text summa-
2021;Minetal.,2021;Heetal.,2021). Oneofthe rization(Rushetal.,2015;Nallapatietal.,2016;
majordesigndimensionsoftheseworksrevolves Chopraetal.,2016;Lewisetal.,2020;Zhangetal.,
aroundwhichsetofmodelparametersareupdated; 2020; Dou et al., 2021). We focus particularly
shouldfine-tuningonlyadjustafewadditionalpa- onsummarizationaspreviousworkonparameter-
rameters that are not part of the initial LMs (e.g., efficient tuning has noted that the differences be-
Adapters(Houlsbyetal.,2019),orPrefixTuning tweentuningtechniquesareparticularlysalientin
(Li and Liang, 2021; Xue et al., 2021)), update morecomplexgenerativetaskssuchassummariza-
allparametersofthepre-trainedmodels(Daiand tion, as opposed to text classification (He et al.,
Le,2015;Devlinetal.,2019;SchickandSchu¨tze, 2021). We draw on the techniques examined in
2021;Fu etal., 2022), or updateonly asubset of themonolingualscenarioandcombinetheunique
parameters(Guoetal.,2020)? characteristics of
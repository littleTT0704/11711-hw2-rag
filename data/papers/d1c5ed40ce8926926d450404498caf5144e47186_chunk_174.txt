 of support vectors. Thus a few instances that are closest to
the decision boundary implied by the full training set can be sufficient to attain high
accuracy. If these (or similar) instances are selected early on, performance increases
quickly and queries that are selected in subsequent iterations do not improve the
decision boundary much anymore.
However, when using all available training data to fit models, the performance of
SVMs is much lower than after 1,000 active learning iterations and logistic regression
is clearly more effective for relevance estimation. This result is surprising, but note
that we evaluated the models on independent test folds, and there is no guarantee
that a model trained using more data has a higher test performance. Instead, there
may be outliers in the training data that were not selected as queries for active
learning but affect the generalization performance of supervised models. If outliers
are chosen as support vectors, they can substantially alter the decision boundary and
cause overfitting. We also found that it is computationally more expensive to train
SVMs than to fit logistic regression models. A single active learning step sometimes
took over a minute when using SVMs and the large feature set, which would be
problematic when interacting with a human annotator in realtime. We used a Java
implementation of LIBSVM [Chang and Lin, 2011] and a server with 3 GHz Xeon
CPUs and 32 GB RAM. However, the runtime could be reduced by selecting a subset
of the features, by sampling queries from a smaller pool of training data, or by using
an SVM implementation that is optimized for linear kernels.
In Table 8.1 we show for each combination of learning method, feature set and
sampling strategy the percentage of positive queries that were selected, averaged
over all cross-validation folds and all random restarts. It can be seen that less than
7% of the instances are relevant if queries are selected at random, which explains
130 CHAPTER 8. EXTENSIONS FOR RELEVANCE ESTIMATION
Logistic Regression Linear SVM
Independent Adjacent Independent Adjacent
Random 6.87% 6.94% 6.91% 6.84%
Maximum Score 91.63% 89.23% 91.99% 90.83%
Diversity 18.56% 14.00% 18.72% 13.93%
Uncertainty 44.89% 44.
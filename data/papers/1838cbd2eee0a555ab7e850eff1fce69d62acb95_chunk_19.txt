remely
tuning the resulting model from MetaXL on the low-resourcelanguages.
concatenateddatafrombothsourceandtargetlan-
6 ConclusionsandFutureWork
guagestomatchtheperformanceofJTtraining. As
high-resourcelanguagesareoutofthescopeofthis
Inthispaper,westudycross-lingualtransferlearn-
paper,weleavefurtheranalysisandunderstanding
ingforextremelylow-resourcelanguageswithout
ofthesescenariosforfuturework.
large-scalemonolingualcorporaforpre-trainingor
sufficientannotateddataforfine-tuning. Toallow
5 RelatedWork
foreffectivetransferfromresource-richsourcelan-
guagesandmitigatetherepresentationgapbetween
UnifyingLanguageSpaces MetaXLinessence
multilingual pre-trained representations, we pro-
bringsthesourceandtargetrepresentationscloser.
poseMetaXLtolearntotransformrepresentations
Previous works have shown that learning invari-
from source languages that best benefits a given
antrepresentationsacrosslanguagesleadstobetter
taskonthetargetlanguage. Empiricalevaluations
transfer. On the representation level, adversarial
oncross-lingualsentimentanalysisandnameden-
trainingiswidelyadoptedtofilterawaylanguage-
tityrecognitiontasksdemonstratetheeffectiveness
related information (Xie et al., 2017; Chen et al.,
of our approach. Further analysis on the learned
2018). Onetheformlevel,Xiaetal.(2019)show
transformationsverifythatMetaXLindeedbrings
thatreplacingwordsinasourcelanguagewiththe
the representations of both source and target lan-
correspondence in the target language brings sig-
guagescloser,thereby,explainingtheperformance
nificantgainsinlow-resourcemachinetranslation.
gains. For future work, exploring transfer from
Adapters Adapternetworksaredesignedtoen- multiple source languages to further improve the
code task (Houlsby et al., 2019; Stickland and performance and investigating the placement of
Murray, 2019; Pfeiffer et al., 2020a), domain multiple representation transformation networks
(Bapna and Firat, 2019) and language (Pfe
id:60)(cid:77)(cid:78)(cid:3)(cid:82)(cid:68)(cid:79)(cid:67)(cid:3)
(cid:22)(cid:60)(cid:77)(cid:60)(cid:3)(cid:60)(cid:73)(cid:63)(cid:3)(cid:22)(cid:60)(cid:71)(cid:71)(cid:84)
(cid:75)(cid:77)(cid:68)(cid:73)(cid:79)(cid:159)(cid:60)(cid:73)(cid:78)(cid:82)(cid:64)(cid:77)(cid:160)
(cid:36)(cid:81)(cid:86)(cid:90)(cid:72)(cid:85)(cid:29)(cid:3)(cid:24)(cid:25)
Figure 1: A data example with two Python programs in L¯ila. One program
annotationusesfunctionconstructwhereastheotheroneisaplainscriptwithout
function. The instruction for each task and categories across four dimensions
are annotated for developing L¯ila.
1 Introduction
Mathematical reasoning is required in all aspects of life, from buying ingredients
for a recipe to controlling the world economy. Given the fundamental nature
of mathematical reasoning, a number of works propose datasets to evaluate
specificmathematicalreasoningabilitiesofAIagents,e.g.,Kushmanetal.(2014)
(algebra word problems), Mishra et al. (2022c) (arithmetic reasoning), Saxton
et al. (2019) (templated math reasoning spanning algebra, calculus, probability,
etc.) Since evaluating high-capacity models on narrowly scoped mathematical
reasoningdatasetsrisksoverestimatingthereasoningabilitiesoftheseAIsystems,
creating the need for a unified benchmark for systematic evaluation over diverse
topics and problem styles.
To this end, we introduce L¯ila2, a unified mathematical reasoning bench-
mark that consists of 23 mathematical reasoning tasks. L¯ila is constructed by
extending 20 existing datasets spanning a wide range of topics in mathematics,
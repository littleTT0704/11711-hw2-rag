 generalize to interact- differentinputandoutputmodalities.
ing with humans and being able to quickly adapt to
new tasks. Here, we may want our agents to be able
to learn from watching videos, viewing a geographic modaltasks. Whilecurrentlytheresultsindicatethat
map,readingatutorial,orlisteningtosomebodytalk, Gatodoesnotfullybenefitfromasharedframework,
and be able to communicate through navigation or in the future, with sufficient scaling and better prob-
manipulation actions, text, or voice. The promise of lem formulation, the authors hypothesize that it will
generalist agents in embodied AI is that they should achieve significant gains in generalization. In com-
benefit from knowledge transfer between tasks and puter vision, there has recently been a line of work
modalities, while being much easier to instruct and exploringunifiedmodels,includingUnified-IO[109],
adapttonewtasks. Perceiver-IO [88], and UViM [98]. These models are
AnemergingphenomenoningenerativeNLPmod- abletoperformawidevarietyoftasksinbothvision
els,suchasGPT-3[24]andPALM[42],isthattheycan and language out of the box, including performing
be used to solve arbitrary NLP tasks in a 0-shot set- wellwithimageimpainting,segmentation,andvisual
ting by prompting the models with language tokens question answering. Moreover, their results begin to
asinput,andhavingitgeneratelanguagetokensfrom showmodelsthattransferknowledgebetweentasks.
the same vocabulary as output. Such prompting can In the years to come, we suspect the rise of unified
beusedtoevaluatethemodelsonmanytasks,suchas models will continue advancing, growing to support
question answering, summarization, and mathemati- 0-shottaskcompletionformanymoretasksinembod-
calreasoning. However,intherealmofcomputervi- iedAI.
sion, the input and output modalities are incredibly In parallel to multi-task learning, recent works
different. Forexample,opticalflowmodelsmightin- have also experimented with re-purposing existing
put a video and output a flow mask of each frame; pre-trained models. In
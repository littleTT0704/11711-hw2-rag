U models without degrading the in-distribution
performance. InProc.ofACL.
Ameya Vaidya, Feng Mai, and Yue Ning. 2019. Em-
pirical analysis of multi-task learning for reducing
modelbiasintoxiccommentdetection. InProc.of
ICWSM.
BertieVidgen,HelenMargetts,andAlexHarris.2019.
How much online abuse is there? In Alan Turing
Institute.
3153
Appendix setandtheyobtain94.15%and94.17%accuracy,
respectively. This is competitive performance as
A FurtherDetailsforModels
showninTable2.
A.1 ModelDebiasing
B AlternativeDatasetofToxicLanguage
The LEARNED-MIXIN ensembleallowsthemodel
Davidson et al. (2017) collected data from Twit-
toexplicitlydeterminehowmuchtotrustthebias
ter, starting with 1,000 terms from HateBase (an
giventheinput:
online database of hate speech terms) as seeds,
pˆ =softmax{log(p )+g(x )logb } which the process relies on lexical biases. We
i i i i
find that performing data filtering methods over
where x i is the ith input text, p i and b i is the thisdatasetleadstodegeneratebehaviour. Specifi-
toxicity prediction produced by RoBERTa, and cally,asshowninTable7,theeasyregiondemon-
bias-only model respectively, and g is a para- stratesleastspuriouscorrelationduetoitsheavily
metric function, which is defined as softplus(w · skewedclassdistribution,whichfurtherpreventus
h i), where w is a learned vector, h i is the last fromdownsamplingtocontrolthetoxicratio. We
hidden layer of the model for example x i, and alsotrainLMIXIN-TOXTRIGandLMIXIN-dialect
the softplus(x) = log(1 + expx). To prevent over the dataset. Table 8 shows that FPR of the
the LEARNED-MIXIN ensemble from ignoring b i, debiased
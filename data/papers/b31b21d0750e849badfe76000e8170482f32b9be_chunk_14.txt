acle
informationisprovidedtoboththebaselineandthemodelthatusesDocPrompting. Theresultsare
shownonthebottompartofTable1. Whentheoraclecommandisgiven,DocPromptingfurther
improvesoverthebasemodels. Forexample,whenprovidingCodexwiththegroundtruthcommand
name,DocPromptingimprovesitsexactmatchfrom22.44%to32.43%.
Should we retrieve documentation or examples? All existing retrieval-based models of code
retrieveNL-codepairsorcodesnippets,ratherthandocumentation. Tosimulatethisscenario,we
followed Parvezetal.(2021)and Pasupatetal.(2021)toretrieveNL-codepairsfromthetraining
setoftldr,andrefertothisbaselineasExPrompting. WefinetunedthebestretrieverRoBERTa
andtwogenerators,andretrievedthetop-30NL-codepairsforeveryexample. AsshowninTable2,
retrievingdocumentation(DocPrompting)providesmuchhighergainsthanretrievingexamples
(ExPrompting). Theoretically,addingexamplesofunseencommandscanhelpExPrompting
generalizetothemaswell. However,newlibrariesandfunctionsmaynothaveavailableexampleson
thewebyet,whiledocumentationoftendoesbecomesavailablewhenthelibraryisreleased.
5.2 PYTHONPROGRAMMINGRESULTS
Table3showstheresultsonCoNaLa. CodeT5+DocPromptingyieldsa1.65BLEUimprovement
overthestate-of-the-artbaselinethatwasinitializedwithCodeT5.4 Whenmeasuringtherecallofthe
generatedfunctionnames,thebenefitofDocPromptingisespeciallyhigherforunseenfunctions
(recall ).Forexample,DocPromptingachieves18.30comparedtoonly9.03ofthebaseCodeT5
unseen
inunseenfunctions. Additionally,DocPromptingimprovesin-contextlearningsettingwithCodex.
4InaseparateexperimentontheoriginalsplitofCoNaLa,thisbaselineachievedaBLEUscoreof39
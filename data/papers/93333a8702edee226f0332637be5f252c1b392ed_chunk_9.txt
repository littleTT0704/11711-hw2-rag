usedRetinaFacedetectorsinceitisrobustagainst theprecisionofthemeasurements.Topreventthis,wepro-
tiny faces, challenging head poses, and faces with a mask. posetouseshoulders’coordinatestomeasurethewidthand
Then, we cropped detected faces with a 20% margin for identify the middle point of shoulders line as center of the
each side, since the face detector’s outputs are quite tight. body. After performing detection and pose estimation, we
Toperformfacemaskandface-handinteractiondetections, generated pairs P(p,p ) using the combination of each
i j
we employed several different CNN architectures, namely detectedpersons. p and p arerepresenteachdetectedper-
i j
ResNet50 [39], Inception-v3 [40], MobileNetV2 [32], and son.Then,wecalculatedtheEuclideandistancebetweenthe
EfficientNet[41].WedecidedtouseEfficientNet,sinceitis shouldercentersofeachpairofpersons.Inordertodecide
the state-of-the-art model. We also included MobileNetV2, whether these persons keep social distance between each
sinceitisalight-weightdeepCNNmodel.Finally,wechose other,weadaptivelycalculateathresholdforeachpairindi-
ResNetandInception-v3modelsbasedontheirhighperfor- vidually based on the average of their body width. Since
mances.Inthetraining,webenefitedfromtransferlearning the represented measurement of the real world, expressed
and initialized our networks with the weights of the pre- bypixelsintheimagedomain,constantlychangesasdepth
trainedmodelsonImageNet[42].Weemployedsoftmaxloss increases, we overcome this by calculating the average of
attheendofeachnetwork.InEfficientNetandMobileNetV2, the body widths of two people. Since the average shoulder
we utilized dropout with a 0.2 probability rate to avoid widthofanadultisaround40-50cmintherealworldand
overfitting. For training, we used 0.0001 learning rate and therequiredsocialdistancebetweentwopersonsis1.5-2.0
0.0005 weight decay parameters. We
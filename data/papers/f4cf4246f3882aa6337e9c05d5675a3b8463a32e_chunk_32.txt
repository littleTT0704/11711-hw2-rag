
12
Training The models were implemented with PyTorch
andtrainedwiththeAdamoptimizer[25]atalearningrate
of 1e-4. We use dropout of 0.3 on the visual features and
thedecoderhiddenstate,tunedonthevalidationdata. Both
theactionandmasklossesareweightedequally, whilethe
auxiliary losses are scaled with a factor of 0.1. For evalu-
ation, we choose models with the lowest loss on the vali-
dation seen set. It should be noted that, due to the nature
of the tasks, low validation loss might not directly lead to
betterevaluationperformancesincetheagentdoesnothave
toexactlyimitatetheexperttocompletethetask.
Notes on Random Agent Unlike discretized navigation
wheretakingrandomactionsmightallowtheagenttostum-
ble upon the goal, ALFRED tasks are much harder to
achieve by chance. The action space branching factor of
Room-to-Room navigation [3], for example, is 46 ≈ 4000
(6averagestepsand4navigationactions). Bycontrast,the
ALFREDaveragebranchingfactoris1250 ≈1053 (50av-
eragestepsfor12actions). Beyondactiontypeprediction,
the ALFRED statespaceresultingfromdynamicenviron- TaskAblations-Validation
ments and the need to produce pixel-wise masks for inter- NOLANGUAGE SEQ2SEQ SEQ2SEQ+PM
activeactionsexplodesfurther. TaskType Seen Unseen Seen Unseen Seen Unseen
Pick&Place 0.0 0.0 6.3 1.0 7.0 0.0
B.1.PredictedMasks
Stack&Place 0.0 0.0 0.0 0.0 0.9 0.0
FigureF5showsafewexamplesofmasksgeneratedby PickTwo 0.0 0.0 1.6 0.0 0.8 0.0
Clean&Place 0.0 0.0 0.0 0.0 1.8 0.0
the SEQ2SEQ+PM model in seen and unseen validation
Heat&Place 0.0 0.0 1.9 0.0 1.9 0.0

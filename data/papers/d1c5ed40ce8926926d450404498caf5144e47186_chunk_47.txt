 considered to be vital or acceptable parts of an answer. Recall and
precision were defined based on the coverage of those nuggets in the answer produced
by a system and the overall length of the answer [Dang et al., 2007].
When comparing rankings of text nuggets generated with different relevance es-
timation strategies in Chapter 5, we compute precision and recall at different cutoff
points. Given rankings of n different sets of text nuggets, let c (r) be the number of
i
relevant instances among the top r instances in the i-th ranking (i = 1,...,n), and t
i
the total number of relevant instances in that ranking. Then precision and recall at
rank r are defined as follows:
n n
1 (cid:88) c (r) 1 (cid:88) c (r)
i i
Precision@r = and Recall@r =.
n r n t
i
i=1 i=1
A precision-recall curve is obtained by plotting Precision@r versus Recall@r for differ-
ent values of r. It illustrates the tradeoff between the two metrics, and can be used to
determine the attainable precision/recall if a certain recall/precision is required. Re-
call is also a useful measure for evaluating the search results retrieved by a QA system
(search recall) or the candidate answers extracted from the results (candidate recall).
Search recall is defined as the percentage of questions for which relevant results were
3.4. QUESTION ANSWERING SYSTEMS 29
found, and candidate recall is the percentage of questions for which correct answers
were extracted. These measures indicate the maximum performance achievable by
downstream components in a QA system and thus can be regarded as upper bounds
on end-to-end QA accuracy.
Finally, we evaluate rankings of text nuggets in terms of mean average precision
(MAP), defined as
1
(cid:88)n
1
(cid:88)ti
c (r )
i i,j
MAP =,
n t r
i i,j
i=1 j=1
where r is the rank of the j-th relevant text nugget in the i-th ranking. MAP is
i,j
a popular means of summarizing the quality of rankings generated by information
retri
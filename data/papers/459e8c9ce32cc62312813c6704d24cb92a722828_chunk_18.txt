 language modeling (LM) loss which Table 2 shows representative continuations (see the Ap-
scoreseachsequencewithafixedlanguagemodel: pendixformore).ThefirstexampleshowshowMGScanfix
non-termination,andthesecondshowshowMGSreduces
c LM(Y^)=(cid:0)logp score(Y^): (12) repetitioninaterminatingsequence.
Intuitively,minimizingthislossadjuststheMLEmodelto
workwellwithgreedydecoding.Weusethefine-tunedGPT- PG & MRT comparison. The MRT-LM and PG-LM
2modelasp,whichisthestartingpointofMGStraining. methods result in a lower LM loss than MLE and MGS-
score
Asatasklossthatincorporatestheground-truthsequence, LM.However,theperplexityishigherthanthatofMGS-LM
weuseeditdistancec (Y^;Y),normalizedbyjYj. (25.8and30.7vs.22.0),withalargerstandarddeviation(1.7
edit
and7.3vs.0.1).Policygradientfindsasolutionwithvery
shortsequences(averagelength3.8).Foreditdistance,MRT-
Metrics. MotivatedbypriorworkwhichshowedthatMLE- editunderperformsMGS-editonaverage(.929vs.925),with
trained LMs produce repetitive, non-terminating text with higher nontermination, repetition, and perplexity. PG-edit
greedy decoding, we measure the portion of duplicate n- achievesthebesteditdistance,thoughwithhigherrepetition
grams(weusen=4)(Wellecketal.2020b)andthepropor- (.246vs..098)andperplexity(24.5vs.21.6)thanMGS.
14036
Prefix ThemangawaslicensedforEnglishlanguagereleasebyDel Task-Loss
N ReyintheUnitedStates,andwasreleasedintheUnitedKingdomintheUnitedStatesinthefirst 137.8
MLE
volumeoftheseries,andintheUnitedStatesinthesecond,andthird,volumesoftheseries,inthe
UnitedStatesint
resmall (cid:3) (cid:3)
comparedtothoseusedinneuralsequencemodeling(Ma- and(cid:11)2R isatemperatureparameter.When(cid:11)!0,the
>0
nia,Guy,andRecht2018)orrequiresmassiveparallelization distributionbecomesuniform,andwhen(cid:11)!1itconcen-
(Salimansetal.2017),andtheirusewithlarge-scalenatural tratesonthedirection(s)ofmaximaltasklossimprovement.
languageprocessingmodelshasbeenunder-explored. Since p is only known up to a normalizing constant and
(cid:3)
Inthispaper,weleveragethefactthatinmanysequence is defined over a high-dimensional parameter space, it is
modelingtasksencounteredinnaturallanguageprocessing,a impracticaltoapproximatetheupdatedirection(cid:1) (cid:3)withsam-
surrogateupdatedirectionisavailableintheformofthemax- ples from p (cid:3). Instead, we use self-normalized importance
imumlikelihoodgradient.Wehypothesizethatincorporating samplingwithaproposaldistributionq((cid:1)j(cid:18)):
thissurrogateinformationintoarandom-searchmethodcan (cid:20) (cid:21)
p ((cid:1)j(cid:18);(cid:11))
substantiallyalleviateissuesstemmingfromthelargesearch (cid:1) = E (cid:3) (cid:1) (3)
(cid:3) q((cid:1)j(cid:18))
space. We frame learning as sampling from a distribution (cid:1)(cid:24)q((cid:1)j(cid:18))
overparameterupdatedirectionsthatisproportionaltothe K
improvementintaskloss.Sincethisdistributionisonlyacces-
(cid:25)X w((cid:1) k)
(cid:1) =(cid:1) ; (4)
s
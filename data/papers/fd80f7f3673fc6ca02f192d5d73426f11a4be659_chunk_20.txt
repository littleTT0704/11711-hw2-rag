1% 49.7% 49.3% 100
GPT-4 ✗ 89.1% 56.4% 53.4% 54.8%
BISON ✓ 88.7% 56.8% 49.3% 52.8% 10
UNICORN ✓ 90.1% 56.3% 51.1% 55.3%
BISON ✗ 85.0% 57.0% 48.6% 53.1% 1
UNICORN ✗ 84.3% 56.1% 48.3% 49.8% Error 0 5 10 20 50 60 70 80 85 90 95100
Score
Table3: ComparisonbetweenPaLM-2andGPT-based Figure 4: Distribution of scores for various LLM
GEMBA(Kocmietal.,2022)atthesystemandsegment reference-basedevaluators,ontheEN-DEtestset. Note
levelsforthehigh-resourcelanguagepairs. thattheyaxisisinlog-scale.
totheirlargerfinetunedcounterparts.
similar findings with GPT models, it seems that
thisisaconsequenceofthepretrainingobjective. In-contextLearning Figure6showsthemean
andinterquartilerange(IQR)oftheperformance
Finetuning Despite their already-great perfor- asweincreasethenumberofin-contextexamples
mance in the zero-shot setting, we find that fine- k(with100examplesetsperk)sampledwithstrat-
tuningLLMscanfurtherimproveLLMevaluators’ ified sampling(seeAppendixCforuniform). Sur-
segment-levelscores. Thisisparticularlyobvious prisingly, despite evidence of the benefits of in-
forthereference-lessevaluators,whereafinetuned context learning for many tasks, we found that
PaLM-2 BISON achieves state-of-the-art perfor- including in-context examples during evaluation
mance in segment-level correlations and compa- (almost) never led to better performance, either
rable system-level accuracy across all language withuniformorstratified sampling.
pairs. Moreover, when we look at how perfor- Toinvestigatethecauseofthisdisappointingper-
mancescaleswithparametercount(
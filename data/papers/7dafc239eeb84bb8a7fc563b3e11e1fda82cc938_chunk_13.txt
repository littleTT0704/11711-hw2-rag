)Pf^;c 2
U Training details. For the ZAGRNN model, we use 100
where c (cid:24) P Uc is the distribution of encoded label vectors convolutionfilterswithafiltersizeof5. Weuse200dimen-
for the set of zero-shot codes U and ((cid:1);c) (cid:24) P U(cid:1);c is defined sionalwordvectorspretrainedonPubMedcorpus[Zhanget
similarlyasinEquation6. Thelosstermisweightedbythe al.,2019b]. Wedropoutwordembeddinglayerwithrate0.5.
cosinesimilarity(cid:25)(c;csib)topreventgeneratingexactnearest We set C = 2 in L. We use ADAM [Kingma and Ba,
LDAM
sibling feature for the zero-shot code l. After adding zero- 2015] for optimization with batch size 8 and learning rate
shotcodestotraining,ourfulllearningobjectivebecomes: 0.001. ThefinalfeaturesizeandGRNNhiddenlayersizeare
bothsetto400. WetraintheZAGRNNmodelfor40epochs.
minmaxL +L +(cid:12)(cid:1)L (9)
WGAN WGAN-Z KEY
G D ForWGAN-GPbasedmethods,thereallatentfeaturesare
extracted from the final layer in the ZAGRNN model. Only
where(cid:12) isthebalancingcoefficientforkeywordreconstruc-
featuresf forwhichy =1arecollectedfortraining.Weuse
tionloss. l l
a single-layer fully-connected network with hidden size 800
Fine-tuning on generated features. After WGAN-GP is forbothgeneratoranddiscriminator. Wesetgradientpenalty
trained,wefine-tunethepretrainedclassifierg lfrombaseline coefficient(cid:21)=10. Forthecode-descriptionencoderLSTM,
model with generated features for a given zero-shot code l. we set the hidden size to 200. We train the discriminator 5
Weusethegeneratortosynthesizeasetoff~ l andlabelthem iterations per each generator training iteration.
05682,
2021.
[85] Kuniaki Saito, Donghyun Kim, and Kate Saenko. Openmatch: Open-set consistency reg-
ularization for semi-supervised learning with outliers. arXiv preprint arXiv:2105.14148,
2021.
[86] Lan-ZheGuo,Zhen-YuZhang,YuanJiang,Yu-FengLi,andZhi-HuaZhou. Safedeepsemi-
supervisedlearningforunseen-classunlabeleddata. InInternationalConferenceonMachine
Learning,pages3897–3906.PMLR,2020.
[87] Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Multi-task curriculum framework
foropen-setsemi-supervisedlearning. InEuropeanConferenceonComputerVision,pages
438–454.Springer,2020.
[88] HuixiangLuo,HaoCheng,YutingGao,KeLi,MengdanZhang,FanxuMeng,XiaoweiGuo,
Feiyue Huang, and Xing Sun. On the consistency training for open-set semi-supervised
learning. arXivpreprintarXiv:2101.08237,3(6),2021.
[89] Zhuo Huang, Chao Xue, Bo Han, Jian Yang, and Chen Gong. Universal semi-supervised
learning. AdvancesinNeuralInformationProcessingSystems,34,2021.
[90] KevinClark,Minh-ThangLuong,ChristopherDManning,andQuocLe. Semi-supervised
sequence modeling with cross-view training. In Proceedings of the 2018 Conference on
EmpiricalMethodsinNaturalLanguageProcessing,pages1914–1925,2018.
[91] LuoxinChen,WeitongRuan,XinyueLiu,andJianhuaLu. Seqvat: Virtualadversarialtraining
for semi-supervised sequence labeling. In Proceedings of the 58th Annual Meeting of the
AssociationforComputationalLinguistics,pages8801–8811,2020.
[92] Wei Li and Andrew McCallum. Semi-supervised sequence modeling with syntactic topic
models. In
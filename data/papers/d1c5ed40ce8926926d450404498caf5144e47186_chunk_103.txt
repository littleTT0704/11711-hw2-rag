
hard to avoid a general bias against short nuggets since in our dataset they are indeed
rarely relevant. On the other hand, it does not seem to matter much whether short
nuggets are selected because they often state common facts about the topic that are
also covered in the seed and other related text.
In addition, we found that our approach for extracting text nuggets based on
structural markup can cause problems if it splits relevant text passages into multiple
nuggets. The individual text units may be ranked low if they are too short or if they
share few key terms with the seed, and often they are no longer self-contained. Here
are two examples:
• Topic: Iran-Iraq War
Score: 0.0403
Unknown, est. 1,000,000–2,000,000;
• Topic: Mother Teresa
Score: 0.0073
Agnes Gonxha Bojaxhi
In the first example the annotator labeled the text string “Casualties Unknown, est.
1,000,000–2,000,000” as relevant, but because of a line break in the source document
it was split into two nuggets that are no longer self-contained. In the other example
72 CHAPTER 5. INTRINSIC EVALUATION
a different annotator labeled “Name at birth: Agnes Gonxha Bojaxhi”, which was
divided in two parts for the same reason. The nugget formation rules could be refined
to mitigate some of these issues, or adjacent text passages could be merged if the
relevance estimate for the combined text is much higher.
Chapter 6
Application to Question Answering
The source expansion approach was evaluated on datasets from two question answer-
ing tasks: the Jeopardy! quiz show and the TREC QA track. In Section 6.1 we give
an overview of the datasets, and in Section 6.2 we describe how text corpora used by
Watson and the OpenEphyra QA system were expanded. The effect of source expan-
sion on QA search results is analyzed in Section 6.3, and the impact on end-to-end
accuracy is evaluated in Section 6.4. Finally, in Section 6.5 we take a closer look
at how source expansion helps, and show that it improves QA performance both by

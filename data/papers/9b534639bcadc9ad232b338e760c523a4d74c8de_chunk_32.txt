.91 98.91 98.91
100 fi 89.35 98.12 89.35
lv 97.78 97.78 97.78
no 99.3 99.26 99.14
80 gl 99.32 99.32 99.18
numeral-noun en 88.06 88.06 82.09
60 el 80.6 80.6 80.6
es 88.62 88.62 75.61
ur 95.63 95.63 95.63
40 fi 92.14 87.25 90.71
it 82.33 79.32 79.32
20 no 85.78 88.44 88.44
fr 81.16 81.88 60.87
Subj-V Adj-N Obj-V ro 84.14 84.83 62.07
bg 88.24 88.24 88.24
Table 4: Comparing the accuracy of Random forest
Figure4: Comparingtheaccuracyofthemodelacross classifierwiththeDecisionTreefordifferentwordor-
different treebanks. Each model is trained on the fr- derrelations.
gsd treebank and directly applied on the other tree-
banks. Shadedbarsdenotethebestmodelperformance
pertareco-authorsofthepaper. ForEnglish,atotal
trained using all features while solid bars denote the
of 15 rules were evaluated for agreement, 11 for
most-frequentbaselineforthattreebank.
word order and 3 for case marking. For Greek, a
totalof35ruleswereevaluatedforagreement,11
butitisnotasinterpretableasthelatter. Neverthe- forwordorderand115forcasemarking. Wedis-
less,asrequestedbytheanonymousreviewers,we cussedtheresultsinthemaintext,herewepresent
comparehowdecisiontreesfareagainstRandom thefiguresforEnglishandGreek(Figure5). For
forest in Table 4. We train models for answering English, there were some rules which the expert
wordorderquestions,across15languagesfromthe was not aware of. We discussed one example for
SUDtreebanks. Overall,weobservethatdecision wordorderinthemaintext,weshowanexample
treesslightlyunderperformtheRandomforest,but foragreementandcasemarkinginTable6.
byonly(avg
seeApp.D.1.
using Flan-T5 (Chung et al. 2022). We sample 95% of our WetrainwithHuggingface’sTrainer(Wolfetal.2020)for
situations from those that have less toxic/NSFW/explicit 4epochswithearlystoppingandabatchsizeof32,although
content,andtheother5%uniformlyfromtherestofthedata wefindthatthemajorityofrunsstarttooverfitafterabout2
so as to include the entire spectrum of inputs. We find that epochs.Trainingtakes19hoursperrunontwoA100GPUs.
this succeeds in increasing the diversity of the dataset, as We fix hyperparameters for the remainder of our experi-
measured by unique n-grams divided by the length of the ments at the optimal hyperparameters: flan-t5-xl, 3e-5, and
dataset(dist-2:.23→.36,dist-3:.54→.67). the mixture including explanations (which we find to as-
sistgeneralizationonthenon-explanationtasks).Forfurther
SymbolicKnowledgeDistillationusingLLMs Afterex- analysis of the relationship of data mixture and model size
perimentation,wefindinitialsuccessinusingGPT-4(Ope- with performance, see Section D.1. We refer to our default
nAI2023)togeneratevalues.Asisoftenthecase,solution 3BtrainedmodelasKALEIDO.
verificationisofteneasierthansolutiongeneration,andwe
find it to be quite a challenging task to generate a compre- H SystemDetails
hensivesetofvalues,rights,anddutiesthatcouldbeconsid-
H.1 Algorithm
ered for a situation. While we find that we as authors can
provide more accurate (precise) lists, we anecdotally find SeeAlgorithm1.
that GPT-4 often does better at breadth (recall). See Ap-
pendixAforexamples.Additionally,becausethegeneration H.2 SystemParameters
task requires such cognitive effort, the cost to hire crowd- ForKALEIDOSYS3B,weusetheseparametersforallexper-

 the Forking Paths dataset. On the left is examples of the real videos and the second column
showsthereconstructedscenes. Thepersonintheblueboundingboxisthecontrolledagentandmultiplefuturetrajectories
annotatedbyhumansareshownbyoverlaidpersonframes.Theredcirclesarethedefineddestinations.Thegreentrajectories
arefuturetrajectoriesofthereconstructeduncontrolledagents. Thescenesemanticsegmentationgroundtruthisshownin
thethirdcolumnandthelastcolumnshowsallfourcameraviewsincludingthetop-downview.
For dynamic movement of vehicle and pedestrian, we forprediction. (Weuse10.4secondsforthefuturetoallow
firstconvertthegroundtruthtrajectoryannotationsfromthe ustoevaluatelongertermforecasts.)
real-world videos to the ground plane using the provided Generating the data. Once we have collected human-
homographymatrices.Wethenmatchthereal-worldtrajec- generated trajectories, 750 in total after data cleaning, we
tories’origintocorrectlocationsinthere-createdscenes. rendereachoneinfourcameraviews(three45-degreeand
Humangenerationofplausiblefutures. Wemanuallyse- one top-down view). Each camera view has 127 scenarios
lectsequenceswithmorethanonepedestrian. Wealsore- intotalandeachscenariohasonaverage5.9futuretrajecto-
quirethatatleastonepedestriancouldhavemultipleplau- ries. WithCARLA,wecanalsosimulatedifferentweather
sible alternative destinations. We insert plausible pedestri- conditions, although we did not do so in this work. In ad-
ansintothescenetoincreasethediversityofthescenarios. dition to agent location, we collect ground truth for pixel-
Wethenselectoneofthepedestrianstobethe“controlled precise scene semantic segmentation from 13 classes in-
agent”(CA)foreachsequence,andsetmeaningfuldestina- cludingsidewalk,road,vehicle,pedestrian,etc. SeeFig.3.
tions within reach, like a car or an entrance of a building.
Onaverage,eachagenthasabout3
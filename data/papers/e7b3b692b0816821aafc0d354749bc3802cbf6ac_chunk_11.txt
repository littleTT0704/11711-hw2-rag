 Random Distractor Image Similarity Caption Similarity Hybrid Similarity
a little girl holding a two sheep standing a little dog sitting on a woman hugs a a kitten that is sitting
kitten next to a blue next to each other in a wooden bench. gray cat to her down by a door.
fence. the snow. chest.
Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics.
CaptionandHybridsimilaritiesarecomputedwithTF-IDF-weightedRoBERTaembeddings.
• Visual Cosine Similarity. We compute the most visually similar images based on the cosine
similarity of image embeddings from a pretrained ResNet model. For each image, we save a
similarityrankingofthenext1000mostvisuallysimilarimages,andthenselectdistractorsfrom
thissetduringtrainingwhenevertheoriginalimageisselectedasatarget.
• Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine
similarityofvectorrepresentationsofthetext. Weexperimentwithboth(1)densevectorsusing
embeddingscalculatedusingeitherthepretrainedRoBERTaorthepretrainedCLIPmeanpooled
modelsfromthesentence-transformerslibrary(Reimers&Gurevych,2019),and(2)sparsevec-
tors using one-hot vectors of each word. We also experimented with using term frequency –
inverse document frequency (TF-IDF) to weight the vector for each token in our captions when
computingcaptionsimilarityasawaytoupweightrarercontentwords.
• Visual+Textual Similarity. In cases where both image and caption similarity were used, we
addedacaptionweightparameterw ∈[0,1]andusedittocomputeaweightedaverageofimage
c
andcaptionsimilarity. Wehopedthatusinghybridmethods(w =0.5)wouldcapturedistractors
c
withbothvisualandsemanticsimilarities.
Wealsotrainmodelsonrandomlyselected, or“easy”, distractorstocreateabaselinetostudythe
effectsofdistractordifficulty. ExamplesofaselectionofthedistractorsettingsareshowninFig.2.
6 EXPERIM
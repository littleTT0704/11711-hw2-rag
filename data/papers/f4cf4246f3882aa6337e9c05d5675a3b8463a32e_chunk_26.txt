&qa. InNAACL,2019. 7
forcementlearning. InEMNLP,2017. 2 [51] JesseThomason,MichaelMurray,MayaCakmak,andLuke
[35] Dipendra Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Zettlemoyer. Vision-and-dialognavigation. InCoRL,2019.
Saxena. Tellmedaved: Context-sensitivegroundingofnat- 2
urallanguagetomanipulationinstructions. InRSS,2014. 3 [52] Jesse Thomason, Shiqi Zhang, Raymond Mooney, and Pe-
[36] DipendraMisra,KejiaTao,PercyLiang,andAshutoshSax- terStone. Learningtointerpretnaturallanguagecommands
ena.Environment-drivenlexiconinductionforhigh-levelin- throughhuman-robotdialog. InIJCAI,2015. 3
structions. InACL,2015. 2 [53] XinWang,QiuyuanHuang,AsliCelikyilmaz,JianfengGao,
DinghanShen,Yuan-FangWang,WilliamYangWang,and
[37] ArsalanMousavian,ClemensEppner,andDieterFox. 6-dof
Lei Zhang. Reinforced cross-modal matching and self-
graspnet: Variationalgraspgenerationforobjectmanipula-
tion. InICCV,2019. 1,8 supervisedimitationlearningforvision-languagenavigation.
InCVPR,2019. 1,2
[38] KhanhNguyenandHalDaumeÂ´III.Help,Anna!VisualNav-
[54] Xin Wang, Wenhan Xiong, Hongmin Wang, and
igation with Natural Multimodal Assistance via Retrospec-
William Yang Wang. Look before you leap: Bridging
tiveCuriosity-EncouragingImitationLearning. InEMNLP,
model-free and model-based reinforcement learning for
2019. 2
planned-ahead vision-and-language navigation. In ECCV,
[39] Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill
2018. 2
Dolan. Vision-based
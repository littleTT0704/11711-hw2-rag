 signals from multiple source domains.
Further, Wang et al. extend the problem to ask how to train a model that
generalizes to an arbitrary domain with only the training samples, but not the
corresponding domain information, as these domain information may not be
available in the real world [31]. Our paper builds upon this set-up and aims to
offer a solution that allows the model to be robustly trained without domain
information and to empirically perform well on unseen domains.
In this paper, we introduce a simple training heuristic that improves cross-
domain generalization. This approach discards the representations associated
with the higher gradients at each epoch, and forces the model to predict with
remaininginformation.Intuitively,inaimageclassificationproblem,ourheuris-
tic works like a “self-challenging” mechanism as it prevents the fully-connected
layers to predict with the most predictive subsets of features, such as the most
frequentcolor, edges,or shapes inthe training data.Wename ourmethodRep-
resentation Self Challenging (RSC) and illustrate its main idea in Figure 1.
Self-Challenging Improves Cross-Domain Generalization 3
We present mathematical analysis that RSC induces a smaller generaliza-
tion bound. We further demonstrate the empirical strength of our method with
domain-agnosticcross-domainevaluations,followingprevioussetup[31].Wealso
conduct ablation study to examine the alignment between its empirical perfor-
manceandourintuitiveunderstanding.Theinspectionsalsoshedlightuponthe
choices of its extra hyperparameter.
2 Related Work
We summarize the related DG works from two perspectives: learning domain
invariant features and augmenting source domain data. Further, as RSC can be
broadly viewed as a generic training heuristic for CNN, we also briefly discuss
the general-purpose regularizations that appear similar to our method.
DG through Learning Domain Invariant Features: These methods
typically minimize the discrepancy between source domains assuming that the
resulting features will be domain-invariant and generalize well for unseen tar-
get distributions. Along this track, Muandet et al. employed Maximum Mean
Discrepancy (MMD) [18]. Ghifary et al. proposed a multi-domain reconstruc-
tion auto-
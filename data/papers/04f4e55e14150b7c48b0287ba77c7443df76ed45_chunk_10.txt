amajorityvote.Anno-
7435
Accuracy(%)
Model Size Validation Test
RandomChance 50.0 50.0
MajorityClass 50.5 50.4
OpenAIGPT 124M 70.9 69.2
GoogleBERT 340M 67.1 66.8
FAIRRoBERTa 355M 79.2 77.1
Human 94.9
Table 1: Results of state-of-the-art natural language under-
standing models on PIQA, compared with human perfor-
mance. The results show a significant gap between model
andhumanperformance,ofroughly20absolutepoints.
able that automated methods trained on large web crawls
mayeventuallysurpasshumanperformancehere.
Note, human evaluation was performed on development
data, because once the train, development, and test folds
wereautomaticallyproducedbyAFLite,thetestdatawas
placed on a blind leaderboard hidden from us and all users
and only automatically evaluated via docker upload. Addi-
tionally, model submissions are capped to one per week to
avoidfittingtothetestdata.
Results
We present our results in Table 1. As the dataset was con-
Figure 6: Breaking down PIQA by edit distance between
structedtobeadversarialtoBERT,itisnotsurprisingthatit
solutionchoices.Top:Cumulativehistogramofexamplesin
performstheworstofthreemodelsdespitegenerallyoutper-
the validation and training sets, in terms of minimum edit
forming GPT on most other benchmarks. Comparing GPT distance d between the two solution choices. The majority
and RoBERTa we see that despite more training data, a
ofthedatasetconsistsofsmalltweaksbetweenthetwoso-
largervocabulary,twicethenumberofparametersandcare-
lutionpairs;nevertheless,thisisenoughtoconfusestate-of-
fulconstructionofrobusttraining,thereisonlya6ptperfor-
the-artNLPmodels.Bottom:RoBERTaaccuracyovervali-
mancegainandRoBERTastillfallsnearlya20pointsshort dationexampleswithaminimumeditdistanceofd.Dataset
of human performance
effectofthisdecisionisthatthefinal andsceneunderstandinginSHRDLUstyleworlds.Thegoal
model exposes an API which can be used interactively for withthismodernincarnationwastotrulysolicitnaturallan-
focusingthemodel’sattentionandchoosingitsactions.We guagefromhumanswithoutlimitingtheirvocabularyorref-
willexploitthispropertywhengeneratingplotsinFigure5 erents. This was an important step in moving towards un-
showing the meaning of each learned function. Our model constrainedlanguageunderstanding.
is still fully end-to-end trainable despite choosing its own The largest corpus was provided by Bisk, Yuret, and
parametersandcomposeablestructure,leadingtoamodular Marcu (2016). In this work, the authors presented pairs of
networkstructuresimilarto(Andreasetal.2016). sceneswithsimulatedblockstousersofAmazon’sMechan-
Therestofthepaperisorganizedasfollows.Wefirstdis- ical Turk. Turkers would then describe actions or instruc-
cuss related work, introduce our new dataset, followed by tions that their imagined collaborator needs to perform to
ournewmodel.Wethenpresentempiricalevaluations,anal- transformtheinputsceneintothetarget(e.g.Movingablock
ysis on the internal representations, and error analysis. We to the side of another). An important aspect of this dataset
concludewiththediscussionforfuturework. isthatparticipantsassumetheyarespeakingtoanotherhu-
man.Thismeanstheydonotlimittheirvocabulary,spaceof
RelatedWork references, simplify their grammar, or even write carefully.
Theannotatorsassumethatwhomeverwillbereadingwhat
Advances in robotics, language, and vision are all applica-
they submit is capable of error correction, spatial reason-
bletothisdomain.Theintersectionofroboticsandlanguage
ing,andcomplexlanguageunderstanding.Thisprovidesan
have seen impressive results in grounding visual attributes
important,andrealistic,basisfortrainingartificiallanguage
(Kollar, Krishnamurthy, and Strimel 2013; Matuszek
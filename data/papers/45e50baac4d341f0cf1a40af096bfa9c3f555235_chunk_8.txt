distillationintheCrowSdataset,whereBERT
Base and RoBERTa Base are our baselines. To
Knowledgedistillation (Hintonetal.,2015)isa
comparequantizationanddistillation,weaddthree
populartechniqueforcompressingtheknowledge
debiasingbaselinesalsoreferencedbyMeadeetal.
encoded in a larger teacher model into a smaller
(2022)thatarecompetitivestrategiestoreducebias.
student model. In this work, we analyze Distil-
TheINLP(Ravfogeletal.,2020)baselineconsists
BERT (Sanh et al., 2019) and DistilRoBERTa2
ofalinearclassifierthatlearnstopredictthetarget
distilledLMs. Duringtrainingthestudentmodel
bias group given a set of context words, such as
minimizesthelossaccordingtothepredictionsof
3https://pytorch.org/tutorials/recipes/recipes/
2https://huggingface.co/distilroberta-base dynamic_quantization.html
2665
Figure1: LMscorevs. GENDER,RACE,andRELIGIONbiasontheSSdatasetacrossallPythiamodels. Darker
datapointsshowlaterpretrainingsteps,andmoretransparentpointstoearliersteps. Theincludedtableshowsthe
KendallTauC,forthecorrelationacross"All"modelsizes,full-precision"Original",and"int8"modelsizes.
Model Best Step Bias Model Best Step Bias
Size LMScore Nr. G. / RA. /RE. Size LMScore Nr. G. / RA. / RE.
70M 89.2 21K 59.8/58.4/58.6 70M 87.7 29K 57.5/54.8/58.0
160M 90.2 36K 61.4/57.6/59.4 160M 89.0 21K 61.1/56.3/57.7
410M 91.6 114K 65.2/60.7/64.5 410M 90.5 50K 64.2/58.4/63.6
-modalfusionoutputfor“[CLS]”to-
j,i
videoandtextfeatures. Givingsuchefficiency,wecanuse
ken taking x and y as inputs; w ∈ R1×d is the parame-
alltheK −1negativesamplesinamini-batchtocompute j i
ter in a linear layer1. Based on Eq. 5, we optimize all pa-
theloss.Throughthis,weoptimizeθ andθ soastoproject
v t rameters in our model θ = {θ,θ,θ } in collaboration
v t m
thevideoandtextsamplesintoanalignedfeaturespace.
withEq.3andEq.4.
The‘[CLS]’tokenandaverageofvideotokensinEq.3
InEq.5, apracticalchallengeisthatwecanhardlyuse
overlooksthedifferencesacrosstokensandframes,andthus
all(K −1)negativesamplesinthemini-batch, duetothe
maynotprovidethepressuretopushindividualtokens(e.g.,
highcomputationalandmemorycostinthemulti-modalfu-
nounsandverbs)togroundonthespecificvideocontents.
sion. TheO(d(m+n)2)complexityofself-attentionlayer
Toencouragecorrectalignment,inadditiontothesentence-
makesitintractabletopassallK ×K pairsintothemulti-
levelloss,weintroduceatoken-levelcontrastiveloss:
modallayers. Previousworksolvedthisbyperformingran-
 
L
=−(cid:88)K (cid:88)
log
exps(xi,y ip)/τ2

Hdo om wes va em r,pl ri an ng dt oo mc lu yt cth he oon su im ngbe nr eo gf atn ive ega st aiv me ps la em sp mle as yt ro eK sul(cid:48) t.
2
i=1p
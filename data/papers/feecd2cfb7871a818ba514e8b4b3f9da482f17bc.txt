Title: Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session
Venue: arXiv.org
Year: 2023
Abstract: Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on"Synergy between human and machine approaches to sound/scene recognition and processing"at the 2023 ICASSP meeting.
CMU authors: B. Raj
Other authors: L. Heller, Benjamin Elizalde, Soham Deshmukh
tldr: Advances in the development of hybrid approaches to Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds are summarized.

 Ap- ratings than between these proxies (syntactic
pendixD,wereportresultsfrombothexperiments. and semantic) and human ratings. In fact,
davinci-003isextremelyclosetotheaverage
3.1 Exp1: Agentivityatthelexicallevel
inter-annotatorgroupcorrelation,andfurthermore
Inordertoseeifmodelsaresensitivetothenotion thiscorrelationislargelyinvarianttotheordering
ofhow“typically”agentiveanounis,wecompare ofprompts.
thedifferenceinlog-likelihoodbetweenpredicting The observation that davinci-003 is better
“agent”or“patient”forthatnoun(δ-LL)withthe correlated with human judgement than both syn-
normalizedhumanratingsaswellascorpusstatis- tactic (Ngrams) and semantic (Propbank) corpus
ticsfromGoogleSyntacticNgramsandPropbank. statisticsisintriguingasbothtypesofcorporahave
Before we compare models with Ngrams and beenusedinmodelingpredictionofthematicfit,or
Propbank, we first ask how well-correlated both howwellanounfulfillsacertainthematicrolewith
are with human ratings. We find that the subject a verb (Sayeed et al., 2016). Thus, we may natu-
ratiocalculatedfromoccurrencecountsinGoogle rally expect this to also work well with “general
SyntacticNgramsispositivelycorrelatedwiththe tendencies”ortypicalityjudgementsfornounsby
average human rating with Pearson’s r of 0.762, themselves. However, it seems that such corpora
thoughthehumanratinghasastrongerdividebe- maybetoosmallorgenre-biasedtofullycapture
tween agents and patients. This can be seen in thenuancesofhumanjudgements,andsuchjudge-
Figure 2. When comparing with humans, using ments may be better captured by LMs that have
SyntacticNgramsforthistaskactuallyturnsoutto seenvastquantitiesofdataacrossawidevarietyof
bebetterthanusingPropbank: forthe166nouns domains,evenwithoutexplicithumanannotation.
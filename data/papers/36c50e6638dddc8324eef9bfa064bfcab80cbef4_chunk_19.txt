
GPT-3 11.2 18.6 41.0 26.6
the gold RoT on automatic evaluation. This sug- InstructGPT-3 3.3 6.7 2.7 6.7
geststhatRoTshelpguidethemodeltowardsbetter Prost(Responseonly) 14.8 7.3 6.0 4.7
Prost(RoT&Response) 38.7 4.6 19.3 13.3
prosocialresponses. Moreresultsofdifferentbase
modelsanddialoguedatasetsareinAppendixC.3. Table 5: Zero-shot response generation results (§6.1)
Comparingto(Instruct)GPT-3,Prostperforms for our Prost and other dialogue agents on ToxiChat
better across all metrics (Table 4). We note that (Bahetietal.,2021). Allnumbersinpercentages(%).
PROSOCIALDIALOGisanunseendatasetforGPT-
3s as it is newly collected. Meanwhile, Prost is
trainedonourdataset,henceleadingtoaconsider- pared to Prost (Response). Likely, this is due to
ablegapinperformanceasmeasuredinourhuman responses and RoTs that disapprove of offensive
evaluation. WefurtherexplorehowPLMscanbe implications(e.g.,“It’snotrighttothinkgaysare
improvedbyusingCanaryin§6.2. animals”),sincewealsofindthatmodeldisagrees
themost.8 Thosedisagreeingresponsescanbemis-
6 GeneralizabilityofProstandCanary takenasoffensivebyneuralmodelsduetospurious
lexicalcorrelationsandalackofunderstandingof
Wenowexplorehow PROSOCIALDIALOG canbe
negations(Hosseinietal.,2021).
useful for responding to real-world toxicity and
We also observe that upgraded models (i.e.,
steeringlargepre-trainedlanguagemodels.
BlenderBot 2 and Instruct GPT-3) output much
more neutral responses (95.3% and 90%, respec-
6.1 GeneralizingtoReal-worldToxicPhrases
tively)comparedtoprevious
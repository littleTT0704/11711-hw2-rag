 S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
Krueger,G.;andSutskever,I.2021. LearningTransferable
VisualModelsFromNaturalLanguageSupervision.
Radford,A.;Wu,J.;Child,R.;Luan,D.;Amodei,D.;and
Sutskever,I.2019. Languagemodelsareunsupervisedmulti-
tasklearners. OpenAIBlog,1(8):9.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena,M.;Zhou,Y.;Li,W.;andLiu,P.J.2020. Explor-
ingtheLimitsofTransferLearningwithaUnifiedText-to-
TextTransformer. JournalofMachineLearningResearch,
21(140):1–67.
Rennie,S.J.;Marcheret,E.;Mroueh,Y.;Ross,J.;andGoel,V.
2017. Self-CriticalSequenceTrainingforImageCaptioning.
InProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition(CVPR).
Shwartz,V.;andChoi,Y.2020. DoNeuralLanguageModels
OvercomeReportingBias? InProceedingsofthe28thIn-
ternationalConferenceonComputationalLinguistics,6863–
6870.
Talmor, A.; Elazar, Y.; Goldberg, Y.; and Berant, J. 2020.
oLMpics-OnWhatLanguageModelPre-trainingCaptures.
TransactionsoftheAssociationforComputationalLinguis-
tics,8:743–758.
Vedantam, R.; Lawrence Zitnick, C.; and Parikh, D. 2015.
Cider: Consensus-based image description evaluation. In
ProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,4566–4575.
Wang, H.; Liu, Y.; Zhu, C.; Shou, L.; Gong, M.; Xu, Y.;
and Zeng, M. 2021. Retrieval Enhanced Model for Com-
monsense Generation. In Find
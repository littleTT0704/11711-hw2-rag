entum 0.0
PredictionEMAMomentum 0.999
WeakAugmentation RandomCrop,RandomHorizontalFlip
StrongAugmentation RandAugment[126]
ForCVtasksinUSB,weuseViTmodels[34]. WefindthatdirectlyusingreleasedViTmodelsleads
tooverfittingandoneneedstofixtheimageresolutionasthepre-trainedresolution,asdemonstrated
in Paragraph 5.4. Instead, we pre-train our own ViT models on ImageNet-1K [8]. To match the
number of parameters as the CNN models used in the classic setting, we use ViT-Tiny and ViT-
Small with a patch size of 2 and image size of 32 for TissueMNIST, CIFAR-100 and EuraSAT,
respectively; ViT-Smallwithapatchsizeof16andimagesizeof224forSemi-Aves. Forbetter
transferperformance,weadoptanMLPbeforethefinalclassifierduringpre-training,asin[127].
Forsupervisedpre-trainingonImageNet-1K,weuseLamboptimizerwithalearningrateof0.05,
and a weight decay of 0.03 for ViT-Tiny and a weight decay of 0.05 for ViT-Small. We adopt a
largebatchsizeof4096andtrainthenetworksfor300epochs,withalinearlearningratewarmup
forthefirst20epochs. Afterthewarmup,cosineschedulerisutilized. Foraugmentation,weuse
RandAugment[126],alongwithMixup[125]andCutMix[128]. Wealsouselabelsmoothingof0.1
duringpre-training. SinceSTL10isasubsetofImageNet,weadoptunsupervisedpre-trainingMAE
[33]ofViT-Smallwithimagesizeof96toavoidcheating.
ForUSBCVtasks,weadoptlayer-wiselearningratedecayasin[123]. Wetunethelearningrate
andlayerdecayrateondifferentdatasetsusingFixMatch,andusethebestconfigurationtotrainall
SSLalgorithms6. Thecosineannealingschedulerissimilartotheclassicsettingbutwithtotalsteps
of204,800andaw
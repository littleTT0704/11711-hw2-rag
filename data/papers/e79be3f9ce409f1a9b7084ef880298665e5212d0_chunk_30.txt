,2019. 2 aodongHe.Stackedcrossattentionforimage-textmatching.
[12] BernardGhanemFabianCabaHeilbron,VictorEscorciaand InProceedingsoftheEuropeanConferenceonComputerVi-
JuanCarlosNiebles.Activitynet:Alarge-scalevideobench- sion(ECCV),pages201–216,2018. 2
mark for human activity understanding. In Proceedings [27] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.
of the IEEE Conference on Computer Vision and Pattern Tvqa: Localized, compositional video question answering.
Recognition,pages961–970,2015. 2 arXivpreprintarXiv:1809.01696,2018. 1,2
9
[28] LinjieLi,Yen-ChunChen,YuCheng,ZheGan,LichengYu, [41] C.Sun,A.Myers,C.Vondrick,K.Murphy,andC.Schmid.
and Jingjing Liu. Hero: Hierarchical encoder for video+ Videobert:Ajointmodelforvideoandlanguagerepresenta-
language omni-representation pre-training. arXiv preprint tionlearning. In2019IEEE/CVFInternationalConference
arXiv:2005.00200,2020. 1,2,3,7 onComputerVision(ICCV),pages7463–7472,2019.1,2,5
[29] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan [42] Hao Tan and Mohit Bansal. LXMERT: Learning Cross-
Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Modality Encoder Representations from Transformers. In
Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object- EMNLP,082019. 2
semantics aligned pre-training for vision-language tasks. [43] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,
arXivpreprintarXiv:2004.06165,2020. 2 DanyangZhang,LiliZhao,JiwenLu,andJ
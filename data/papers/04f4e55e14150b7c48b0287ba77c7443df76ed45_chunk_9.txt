
Duringfinetuning,wetrainthemodelusingacross-entropy
scaletransformermodels:
lossoverthetwooptions.4
a. GPT(Radfordetal.2018)isamodelthatprocessestext
Due to the inherent challenge of PIQA, we found that
left-to-right,andwaspretrainedusingalanguagemodeling
finetuning was often unstable. With some hyperparameter
objective.Weusetheoriginal124MparameterGPTmodel.
configurations, validation performance is around chance,
b. BERT (Devlin et al. 2019) is a model that process
particularly for BERT. We follow best practices in using a
textbidirectionally,andthuswaspretrainedusingaspecial
grid search over learning rates, batch sizes, and the num-
maskedlanguagemodelingobjective.WeuseBERT-Large,
ber of training epochs for each model, and report the best-
with340Mparameters.
scoringconfigurationaswasfoundonthevalidationset.For
c. RoBERTa (Liu et al. 2019) is a version of the BERT
allmodelsandexperiments,weusedthetransformers
modelthatwasmadetobesignificantlymorerobustthrough
libraryandtruncatedexamplesat150tokens,whichaffects
pretraining on more data and careful validation of the pre-
1%ofthedata.Manualinspectionofthedevelopmenterrors
training hyperparameters. We use RoBERTa-Large, which
showthatsome“mistakes”areactuallycorrectbutrequired
has355Mparameters.
a web-search to verify.5 It is therefore, completely reason-
Wefollowstandardbestpracticesinadaptingthesemod-
elsfortwo-wayclassification.Weconsiderthetwosolution 4Additionally,forGPT,wefollowtheoriginalimplementation
choices independently: for each choice, the model is pro- andaddanadditionallanguagemodelingloss,whichwefoundto
vided the goal, the solution choice, and a special [CLS] improvetrainingstability.
token. At the final layer of the transformer, we extract the 5Humanperformancewascalculatedby
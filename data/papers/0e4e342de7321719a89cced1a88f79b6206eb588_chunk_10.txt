 Asf(cid:48) alreadycontainstextualinformation,there- and apply QCM with K = 5 on each stage. A pre-trained
v
gressiontokenr canalsolearnboththetextualandvisual BERT-base[Devlinetal.,2019]isadoptedasthetextualen-
qv
information. The corresponding output f is considered as coder. We train our model for 90 epochs on RefCOCO and
qv
the grounding representation and can be directly used in the RefCOCOg,and180epochsforRefCOCO+. Dataaugmen-
predictionhead. tation is adopted to obtain a strong baseline. Each image is
paddedandresizedto640×640. NotethatforRefCOCOg,
3.4 PredictionHead
we report the scores of the models trained on RefCOCO.
Thefinalboundingboxpredictionisderivedfromtheground- Moreimplementationdetailscanbefoundinthesupplemen-
ingrepresentationf as: tarymaterials.
qv
b=ϕ (f ) (4) 4.2 QuantitativeResults
Cv(cid:55)→4 qv
whereϕ (·)istwofullyconnectedlayerswithReLUac- InTable1, wecompareourVGQCwithtwo-stageandone-
tivatiton.C bv(cid:55)→ is4
thecoordinatesofthepredictedboundingbox. stagevisualgroundingmodels,amongwhichTransVGisthe
previous state-of-the-artwhich doesn’t exploitextra data. A
3.5 TrainingObjective predictionisconsideredcorrectiftheIoUbetweenthepredic-
The training objective is the GIoU loss plus a smooth L1 tion and the ground-truth bounding boxes is larger than 0.5.
loss. LetA(S)denotetheareacoveredbyaboundingboxS, Wereportthetop-1accuracy(%)onRefCOCO,RefCOCO+,
andletS andSˆ betheground-truthandpredictedbounding andRefCOCOg. TheresultsshowthatourVGQCw/fusion
i i achieves state-of-the-art performance on
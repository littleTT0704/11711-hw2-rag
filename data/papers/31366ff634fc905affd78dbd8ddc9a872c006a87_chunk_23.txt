anyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
WendiZheng,XiaoXia,etal.2022. Glm-130b: An
open bilingual pre-trained model. ArXiv preprint,
abs/2210.02414.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger,andYoavArtzi.2020. Bertscore: Eval-
uating text generation with BERT. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020.OpenReview.net.
ShuyanZhou,UriAlon,FrankF.Xu,ZhengbaoJiang,
andGrahamNeubig.2023. Docprompting: Gener-
ating code by retrieving the docs. In International
Conference on Learning Representations (ICLR),
Kigali,Rwanda.
A AdditionalDetails 2860 annotated code snippets (5 generations ×
472examples)whereeachsnippetisgradedby4.5
F Thewell-knownF scoreiscomputedas:
β 1 annotators.
2 2·precision·recall
F 1 = 1 + 1 = precision+recall B.2 FunctionalCorrectness
recall precision
(4) We evaluate functional correctness using the Hu-
A more general F score F uses a positive factor manEval(Chenetal.,2021)benchmark. Eachex-
β
β,whererecallisconsideredβ timesasimportant ample in HumanEval contains a natural language
asprecision: goal, hand-written input-output test cases, and a
human-written reference solution. On average,
(cid:0) 1+β2(cid:1)
·precision·recall each example has 7.7 test cases and there are 164
F = (5)
β β2·precision+recall examples in total. While the original HumanEval
is in Python, Cassano et al. (2022) translated Hu-
As found in METEOR (Banerjee and Lavie,
manEval to 18 programming languages, and pro-
2005), using F with β = 3, thus
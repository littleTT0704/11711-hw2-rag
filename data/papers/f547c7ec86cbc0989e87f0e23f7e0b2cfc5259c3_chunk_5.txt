bylength-normalizedscore; oroneoutputtokenindependentlyfromitsprevious
18 SetthebesthypothesisfromStoppedasactivebeam;
19 Applytheincrementalpolicy; predictions, which better matches the streaming
20 Removethelasttwotokensfromtheactivebeam;
translation and reduces the risk of hallucinations.
21 end
BecausetheCTC’spredictionsforeachframeare
conditionally independent, CTC does not suffer
In Algorithm 1, the overgeneration problem
from the label bias problem (Hannun, 2020). Al-
is addressed by stopping unreliable beams (see
though, the direct use of CTC in either machine
Line9). Theunreliablebeamisdefinedasabeam
endingwith<eos>tokenorhavingascorelower or speech translation is possible, yet, its quality
lags behind autoregressive attentional modeling
orequaltoanyotherunreliablebeamdetectedso
(LibovickýandHelcl,2018;Chuangetal.,2021).
far. Thismeans,thatwestopanybeamthathasa
scorelowerthananybeamendingwith<eos>to-
1Initialexperimentsshowedthatremovingmorethantwo
ken. Sincetheremightbeahypothesisthatwould
tokensleadstohigherlatencywithoutanyqualityimprove-
always score lower than some hypothesis ending ment.
390
Anotherway,howtoutilizetheCTCisjointde- The disadvantage of this definition is that
coding(Watanabeetal.,2017;Dengetal.,2022). p (... X) must be computed for every vocab-
ctc
|
In the joint decoding setup, the model has two ulary entry separately and one evaluation costs
decoders: thenon-autoregressiveCTC(usuallya (T),i.e., ( T)intotal. ContemporaryST
O O |V|·
singlelinearlayeraftertheencoder)andtheatten- systems use vocabularies in orders of thousands
tionalautoregressivedecoder. Thejointdecoding items making this definition prohibitively expen-
istypicallyguidedbytheattentionaldecoder,while sive. Since the CTC is used together with the
the CTC output is used
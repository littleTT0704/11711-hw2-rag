 that can be directly applied
to almost any CNN architecture with no extra model architecture, and almost
no increment of computing efforts. We name our method Representation Self-
challenging (RSC). RSC iteratively forces a CNN to activate features that are
lessdominantinthetrainingdomain,butstillcorrelatedwithlabels.Theoretical
and empirical analysis of RSC validate that it is a fundamental and effective
way of expanding feature distribution of the training domain. RSC produced
the state-of-the-art improvement over baseline CNNs under the standard DG
settings of small networks and small datasets. Moreover, our work went beyond
the standard DG settings, to illustrate effectiveness of RSC on more prevalent
problemscales,e.g.,theImageNetdatabaseandnetworksizesup-toResNet152.
1 https://github.com/pytorch/examples
Self-Challenging Improves Cross-Domain Generalization 15
References
1. Balaji,Y.,Sankaranarayanan,S.,Chellappa,R.:Metareg:Towardsdomaingener-
alizationusingmeta-regularization.In:AdvancesinNeuralInformationProcessing
Systems. pp. 998–1008 (2018)
2. Ben-David,S.,Blitzer,J.,Crammer,K.,Kulesza,A.,Pereira,F.,Vaughan,J.W.:A
theoryoflearningfromdifferentdomains.Machinelearning79(1),151–175(2010)
3. Bridle, J.S., Cox, S.J.: Recnorm: Simultaneous normalisation and classification
appliedtospeechrecognition.In:AdvancesinNeuralInformationProcessingSys-
tems. pp. 234–240 (1991)
4. Carlucci, F.M., D’Innocente, A., Bucci, S., Caputo, B., Tommasi, T.: Domain
generalization by solving jigsaw puzzles. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. pp. 2229–2238 (2019)
5. Csurka, G.: Domain adaptation for visual applications: A comprehensive survey.
arXiv preprint arXiv:1702.05374 (2017)
6. DeVries
positionalencodingcanbe
forsalientobjectdetection.
computedasPE(pos 3D,2i) = sin(pos 3D/100002i/d 3)and
Label-guidedMultimodalFusion PE(pos 3D,2i+1)=cos(pos 3D/100002i/d 3)wherepos 3D
canbex,y,zcoordinatesandiisthedimension.The2D-to-
The label-guided multimodal fusion module contains two
3Dtransformationcanbecomputedas
pseudo-siamese audio-visual context fusion (ACF) blocks
v u v u v
equippedwithsphericalpositionalencodingtoalignthecor-
x=sin cos, y =sin sin, z =cos (1)
respondence between 3D sound sources and pixels. The R R R R R
output of student and teacher block are student feature where(u,v)and(x,y,z)arethe2Dand3Dcoordinateof
{fstu}T andteacherfeature{ftch}T respectively. eachpixelrespectively.R= W whereW isthewidthofthe
t t=1 t t=1 2Ï€
frame.Sphericalpositionalencoding(SPE)isemployedto
Sphericalpositionalencoding. ERframeisacommonly
encodespatialinformationforvisualrepresentationduring
used format to transmit and store panoramic videos (Cai
cross-modalattention.
etal.2022).However,asshowninFigure3,theERframe
suffers from severe distortions in the polar regions. To Student block. As shown in Figure 4 (without the gray
tackle this problem, we adopt the position-agnostic atten- parts), to fuse the rich information encoded in visual and
tion mechanism and propose a spherical positional encod- acoustic features, we utilize multimodal attention to en-
ing to compensate for the distortion in the ER frame. Dif- able audio-visual context interaction. We first concatenate
fusion.AsshowninFigure4(withthegrayparts),theaddi-
TeacherPath C Concatenation
tionalgroundtruthisdownsampledandconcatenatedwith
Stop Gradient F Flatten visualfeature{f }
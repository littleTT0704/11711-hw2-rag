MASF[7] AlexNet 94.78 64.90 69.14 67.64 74.11
RSC(ours) AlexNet 97.61 61.86 73.93 68.32 75.43
Table 7. DG results on VLCS [27] (Best in bold).
(5) Comparison with different dropout methods (Table 5): Dropout has in-
spired a number of regularization methods for CNNs. The main differences be-
tween those methods lie in applying stochastic or non-stochastic dropout mech-
anism at input data, convolutional or fully connected layers. Results shows that
our gradient-based RSC is better. We believe that gradient is an efficient and
straightforward way to encode the sensitivity of output prediction. To the best
of our knowledge, we compare with the most related works and illustrate the
impact of gradients. (a) Cutout [6]. Cutout conducts random dropout on input
images, which shows limited improvement over the baseline. (b) DropBlock [9].
DropBlock tends to dropout discriminative activated parts spatially. It is bet-
ter than random dropout but inferior to non-stochastic dropout methods in
Table 5 such as AdversarialDropout, Top-Activation and our RSC. (c) Adver-
sarialDropout[21,12].AdversarialDropoutisbasedondivergencemaximization,
while RSC is based on top gradients in generating dropout masks. Results show
evidence that the RSC is more effective than AdversarialDropout. (d) Random
and Top-Activation dropout strategies at their best hyperparameter settings.
Self-Challenging Improves Cross-Domain Generalization 13
Office-Home backbone Art ClipartProduct Real Avg↑
Baseline[4] ResNet18 52.15 45.86 70.86 73.15 60.51
JiGen[4] ResNet18 53.04 47.51 71.47 72.79 61.20
RSC(ours) ResNet1858.42 47.90 71.63 74.54 63.12
Table 8. DG results on Office-Home [28] (Best in bold).
ImageNet-SketchbackboneTop-1Acc↑Top-5Acc↑
Baseline[31] AlexNet
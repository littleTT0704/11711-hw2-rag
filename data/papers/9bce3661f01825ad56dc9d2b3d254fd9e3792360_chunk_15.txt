
entailment,contradiction,orneutral,withthehu-
havegoldlabels. Usingpre-trainedmodelstoan-
mans agreeing with each other on the final label.
notateunlabeleddataandusethisdatafortraining
Human1’slabelisneutral,andHuman2’slabelisa
has been shown to improve performance (Wang
contradiction. Intheend,theyagreeonthelabelof
neutral. Premise: Anunistakingapictureoutside.
9Weshowexamplesofhuman-systemdiscussioninAp-
pendixA. Hypothesis: Anunistakingaselfie.”.
TheGPT-3.5andChatGPTgeneratehumandis- SNLI R1 R2 R3
cussionsfor10problemsusedinthefew-shotand
Randomdis. -2.91 -2.10 -3.30 -3.42
2,000 problems used in the fine-tuning, respec- Cuttingdis. -2.40 -1.60 -2.60 -2.25
Randomlabel -3.43 -2.50 -3.50 -3.17
tively. Theaveragenumberofutterancesinhuman-
createddiscussionswas4.4,andtheaveragenum- Randomdis. -3.32 -3.59 -3.77 -3.62
Cuttingdis. -2.88 -2.79 -2.32 -2.15
berofutterancesinsystem-generateddiscussions
Randomlabel -3.22 -3.76 -3.89 -3.58
was4.7. Regardingthenumberofutterances,hu-
manandsystemargumentsarealmostthesame. Table 7: Difference for the few-shot-discussion accu-
We used instruction tuned and non-instruction racyfromwhenthenoisyexamplesareprovidedinthe
tuned models for MPT10 (Team, 2023) and Fal- promptonSNLIandANLI.Thehigherthedifference,
thestrongerthenoise. UpperdifferencesarebyGPT-
con11 (Penedoetal.,2023)aspre-trainedmodels
3.5,andlowerdifferencesarebyChatGPT.
forfine-tun
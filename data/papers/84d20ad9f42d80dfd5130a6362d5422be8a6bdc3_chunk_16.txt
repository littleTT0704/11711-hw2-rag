WMT21-Metaâ€™sefficiencydramaticallyimproveswithFP16quantization,
nearlydoublingthroughputandreducinglatency,memoryoverhead,andenergyconsumptionbyhalf
ormore. Theseresultshighlightthepromiseofadvancingquantizationtechniquesforlargermodels
inordertoimprovethetrade-offbetweenaccuracyandefficiency.
Insingle-GPUinference,theGPUaccountsforonlyaminorportionoftheenergyconsumption.
ThisisdemonstratedbyFigure3. ThisexperimentusesasingleRTX8000GPUwithamaximum
powerof260W.WenotethattheGPUrarelyoperatesatfullpower,implyingthatGPUhours,a
metriccommonlyusedtogaugetrainingcomputationaloverhead(Hendersonetal.,2020;Kasaietal.,
2021b),isunsuitableforestimatinginferenceGPUenergy. EvenduringthemostGPU-intensive
runsbytheWMT21-Metamodel,whereitdoesoperateatfullcapacity,theGPUonlyaccountsfor
onethirdofthetotalmachinepower. Thisobservationdivergesfrompreviousfindingsontraining,
whereGPUsareestimatedtoconstitutearound70%oftheenergyusage(Dodgeetal.,2022). We
attributethedifferencetotheincreasedmemoryanddiskIOdemandsduringinference,coupledwith
lowerGPUutilizationandincreasedidlingtimeduetosmallercomputekernelsduringinference
Thisdisparitysuggeststhatefficiencyconclusionsdrawnfromtrainingneedcarefulexamination
whenappliedtoinference. Interestingly,weobserveacorrelationbetweenhigherGPUpowerand
higherpowerutilizationbyothercomponents. Weconjecturethatthisisatleastpartiallyduetothe
increasedfanactivityneededforcooling.
7
T (h wro ou rdg sh /p su )t #Params M M M MB B 2 2M MA AR R 1 1T T 0
0
0 0M M 4 12 2 1.O M 28 BM T (h wro ou rdg sh /p su )t #Params M M M MB B 2 2M MA AR R 1 1T T 0
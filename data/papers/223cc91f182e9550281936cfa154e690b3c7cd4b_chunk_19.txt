 of impacting the learning outcome and utility is the
experience functions f(θ). An experience function f(θ)(t) ∈ R measures the goodness of a configuration t
k
in light of any given experience. The superscript (θ) highlights that the experience in some settings (e.g.,
reward experience as in Section 4.3) could depend on or be coupled with the target model parameters θ. In the
following, we omit the superscript when there is no ambiguity. As discussed in Section 4, all diverse forms of
experience that can be utilized for model training, such as data examples, constraints, logical rules, rewards,
and adversarial discriminators, can be encoded as an experience function. The experience function hence
provides a unified language to express all exogenous information about the target model, which we consider as
an essential ingredient for panoramic learning to flexibly incorporate diverse experience in learning. Based on
the uniform treatment of experience, a standardized optimization program as above can be formulated to
identify the desired model. Specifically, the experience functions contribute to the optimization objective via
the penalty term U(ξ) over slack variables ξ ∈ RK applied to the expectation Eq [f k]. The effect of
maximizing the expectation is such that the auxiliary model q is encouraged to produce samples of high quality
in light of the experience (i.e., samples receiving high scores as evaluated by the experience function).
Divergence function. The divergence function D(q,p θ) measures the ‘quality’ of the target model p θ in
terms of its distance (divergence) with the auxiliary model q. Intuitively, we want to minimize the distance
from p θ to q, which is optimized to fit the experience as above. Section 5 gives a concrete example of how the
divergence term would directly impact the model training: with a certain specification of the different
components (e.g., the experience function, α/β), the SE in Equation 3.1 would reduce to min θ D(p d,p θ).
That is, the learning objective is to minimize the divergence between the target model distribution p θ and the
data distribution p d, and the divergence function D(⋅,⋅) determines the specific optimization problem. The
14
Harvard Data Science Review
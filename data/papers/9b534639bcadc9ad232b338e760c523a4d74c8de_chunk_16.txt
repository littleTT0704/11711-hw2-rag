 We present the result breakdown of indi- the word order because in the training treebank
vidualrelationsinAppendix(Table3). only 1.7% of examples are questions making it
Asmotivatedin§3,theconditionswhichgovern challengingforthemodeltolearnwordorderrules
alinguisticphenomenonvaryconsiderablyacross fordifferentquestiontypes.
languages, which is also reflected in our model’s Throughthistool, alinguistcanpotentiallyin-
performance. For example, the model trained on spectandderiveinsightsonhowthepatternsdis-
syntacticfeaturesaloneissufficienttoreachahigh covered for a linguistic question vary across dif-
accuracy(avg.94.2%)forpredictingtheadjective- ferentsettings,bothwithinalanguageandacross
noun order in Germanic languages. But for Ro- differentlanguagesaswell.
mance languages, using only syntactic features
7.2 HumanEvaluationResults
leadstomuchlowerperformance(avg.74.6%). We
experiment with different features and report re- Throughtheaboveexperiments,weautomatically
sultsforasubsetoflanguagesinFigure3. Observe evaluated that the extracted rules are predictive
thatforSpanishadjective-nounorderaddinglexi- (to some extent) and applicable to the language
calfeaturesimprovestheperformancesignificantly ingeneral. Beforeapplyingthisframeworkonan
(+11.57)oversyntacticfeatures,andsemanticfea- endangered language we first perform a manual
turesprovideanadditionalgainof+4.48. Studying evaluation ourselves for English and Greek. We
thelanguagesmarkedashaving“nodominantor- selecttheselanguagesbasedontheavailabilityof
der” in WALS, we find our model does show a human annotators, using one expert each for En-
higher entropy. SUD contains 8 such languages glishandGreek. First,wenotethatthetotalnum-
forsubject-verborder,andourmodelproducesan berofrulesforEnglish(29)aremuchlessthanthat
(avg.) entropy of 1.09, as opposed to (avg.) 0.75 forGreek(161),thel
. 2001;Liawetal.,2002)takeanensemblelearningapproach
byaggregatingacollectionofdecisiontreestoreducethe
However,theactualdistributionofthedataanditsmodes
over-fittingtendencyofasingledecisiontree. Apertaining
areunknownapriori. Consequently,thebinaryclassesfor
issuewiththesetree-basedmethodsisthattheyrelyonhard
eachclassifier(non-leaf)nodeareunknown.Thiseffectively
partitionsandpiece-wiselinearpredictions,whichcanlead
makesthepartitionoftheoutputspaceitselfavariableto
todiscontinuitiesandhighbiasesinpredictions.
be determined. To address this, we develop a probabilis-
ticframeworkforourHRMEmodel,andproposearecur- On the other hand, the mixture of experts (ME) models
sive Expectation-Maximization (EM) based algorithm to areafamilyofprobabilistictree-structuredmodelswitha
optimizethejointinput-outputpartition,thevariousexpert gatingmechanismandacollectionofexpertsattheleaves.
models, as well as the tree structure. To the best of our Thegatingmechanismisresponsibleforsoftpartitioning
knowledge,thisnewjoint-partitionbasedgatingmechanism the input space into sub-regions such that a local expert
forHMEmodelshasnotbeenstudiedyet. Theclosestrele- models the distribution of each sub-region (Yuksel et al.,
vantliteratureisbyMemonetal.(2018)whichpartitions 2012). The flexibility of the ME family embraces a rich
the space solely based on the output value to determine varietyofgatingmechanismsandexpertmodels. Examples
itsoptimaldiscretization. OurHRMEmodel,ontheother includehierarchicalmixtureofexperts(HME)(Jordan&Ja-
hand, uses a joint partition to determine the optimal data cobs,1994)whichemploysabinarytreestructure,Bayesian
allocation to the leaf experts, and our model is globally HME(Bishop&Svenskn,2002)withaBayesiantreatment,
optim
bitextmining,andquestionretrieval. Forsemanticsimilarity
weseparatetheevaluationsintoEnglish-only,cross-lingual,cross-lingualbutwiththesamelanguage(XL(s.) ar-ar
andes-es)andcross-lingualusingdifferentlanguages(XL(d.),ar-en,es-en,andtr-en). Resultsarereportedasthe
averagePearson’sr×100acrossdatasets. ForbitextminingweevaluateonTatoebaandBUCC,withBUCCsplit
betweenusingcosinesimilarityorusingamarginapproach(ArtetxeandSchwenk,2019a). Resultsarereportedas
accuracy×100forTatoebaandF ×100forBUCC.Forquestionretrieval,weevaluateretrievalaccuracy×100
1
usingPAQasaquestionknowledgebaseontheNQandMKQAdatasets. Finally,wecomputeascoretosummarize
qualityovertheseevaluations.
trained models, and 0.1 when training from ran- andaddingadditionalrandomlyinitializedparam-
domlyinitializedparameters. For VMSST CON- eterstothedecoder. Perhapsdifferentpretraining
TRASTIVE, we set it to.0005 for the pretrained strategies using this modified decoder would re-
and 6 layer settings and 0.001 for the randomly solvethesedifferences. Wealsoseethat VMSST
initialized24layersetting. CONTRASTIVE hasnegligibleimprovementover
CONTRASTIVEwhichwasunexpected—thatis,a
6.5 Results
traditional contrastive loss does not improve fur-
The results of our experiments are shown in Ta- therontopofgenerativelossofVMSST.Weleave
ble1. Overall,VMSSThasthebestperformance theexplorationofdifferentstrategiesofcombining
forallthreeexperimentalsettingsandthebestper- theseapproachestofuturework.
formanceoneachtaskonaverage,withtheexcep- Itisalsointerestingtoobservethestarkperfor-
tionofTatoeba. Infact,forNQquestionretrieval mance difference for different tasks. Bitext min-
withapretrainedmodel,itperformsnearly
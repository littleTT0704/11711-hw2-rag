A Dataset of High-quality Model with
thanbest-performingconversationmodels(e.g., 1.5M Social Dialogues Greater Generalizability
GODEL, BlenderBot-1, Koala, Vicuna). Ex-
periments reveal COSMO is sometimes even Figure1: Anillustrationofour CO 3 framework(ยง2),
preferred to the original human-written gold SODA dataset (ยง3), and conversation model COSMO
responses. Additionally,ourresultsshedlight (ยง4)trainedonSODA. Conversationsaredistilledfrom
onthedistinctionbetweenknowledge-enriched alargelanguagemodel(LLM)bycontextualizingsocial
conversationsandnaturalsocialchitchats. We commonsense. ThefullexampleisinTable1.
makeourdata,models,andcodepublic.1
1 Introduction To alleviate this bottleneck, we introduce
SODA(SOcialDiAlogues),amillion-scaleEnglish
Conversationsthatoccurineverydayspokensitua-
dialoguedatasetcoveringawidevarietyofsocial
tionsareoftennotrecordedasdata. Andwhenthey
interactions. Asaresultofbeinggroundedonrich
are,suchasinthecaseoftextmessages,research
social commonsense and narratives, SODA goes
useisrightlyrestrictedduetoprivacyandlegalcon-
beyond specific skill-focused dialogues and fea-
cerns. Asaresult,collectinghigh-quality,everyday
turesmoregeneralconversations. Ourdatasetin-
socialconversationsonalargescalehaslongbeen
cludes1.5milliondialoguesdistilledfromalarge
recognizedasadifficulttask(Smithetal.,2020).
language model (in our case, GPT-3.5; Ouyang
Previousstudieshavereliedoncrowdsourcingfo-
etal.,2022)resultinginmorethan11millionutter-
cusedonspecificthemesofdialogue(e.g.,persona,
anceswith300milliontokens: SODAisthelargest
empathy;Zhangetal.,2018;Rashkinetal.,2019).
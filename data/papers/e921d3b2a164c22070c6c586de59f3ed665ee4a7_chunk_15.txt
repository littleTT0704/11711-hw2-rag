4.TheHRMEtreeaftertrainingonthesyntheticdata.The
treeisgrownrecursivelyinadepth-firstmanner—toptobottom,
left to right. Each circle represents a classifier node, and the
numberwithinitisthepartitionthresholdt. Thenumberonthe
selectthetop-1experttomakepredictions. Figure1cshows
edgerepresentstherootmeansquareerrorifstopgrowingatthat
thecorrespondingfittingresults. Weseeamuchbetterfit
node.Eachdashededgeleadstoaleafregressor.
than that in Figure 3f—in the former all data modes are
successfullypredictedbyourHRME-SVRmodel.
vationindicatesthelinearnatureofdatadistributions,and
WefurthershowthegrowthoftheHRMEtreeonthetrain-
henceanonlinearregressionexpertwouldbeinappropriate
ingset. InFigure4,thenumberineachcirclenodeisthe
forthisdataset. Thisobservationisalsoconfirmedbythe
partition threshold t. The number besides each circle is
poorperformanceofthenonlinearMLPmodel. Further,the
the RMSE if growth stops at that node. Note the tree is
data is small (506 samples) but has high dimension (13),
grown recursively in a depth-first manner (top to bottom,
makingitdifficulttoseparatethemodesbySVM.Instead,
left to right). We observe that the RMSE reduces as the
otherclassifierscanbeusedtoimprovetheperformanceof
treegrows. Thisvalidatesourhypothesisthatouralgorithm
ourmodel. Wealsoobservethatourmethodscanreducethe
canlearntheoptimaltreestructureautomaticallywithout
variance(lowRMSE)onamajorityoftasks. Thisshows
pruning afterwards, and the proposed Q-value is a good
thatourmethodsareabletomitigatetheproblemofhigh
indicatoroftheglobaloptimalityofthetree. Further,we
varianceofconventionaltreemodels. Inaddition,wesee
notice our HRME model also successfully partitions the
even with simple linear leaf experts, our method can sig-
outputspacebasedonse
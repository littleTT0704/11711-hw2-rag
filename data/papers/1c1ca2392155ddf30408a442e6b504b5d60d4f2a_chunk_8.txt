theirpermission.
However,inthiscase,manypeopleagreethatitwouldbepermissibletodoit–especiallyifyougave
asizeableportionofthemoneytoyourneighbor(Levineetal.,2018).
Ofcourse,thereisindividualvariationinthewaythatpeoplemakemoraljudgmentsinthesecases
ofrule-breaking. However,itisstillpossibletopredictsystematictrendsofthejudgmentshumans
makeatapopulationlevel.2
CanLLMsLearnHumanMoralJudgment? Therehasbeenincreasingattentionon“computa-
tional ethics” – the effort to build an AI system that has the capacity to make human-like moral
judgments (Awad et al., 2022a). Early approaches use logic programming (Pereira and Saptaw-
ijaya, 2007; Berreby et al., 2015). With the rise of LLMs, there has been a movement towards
deep-learning-basedcomputationalethicswork,amongwhichthemostsimilarthreadofresearchto
ourworkistrainingmodelstopredicthumans’responsestomoralquestions(MoralQA)(Emelin
etal.,2020;Sapetal.,2020;Forbesetal.,2020;Hendrycksetal.,2021b;Lourieetal.,2021,inter
alia). Existingstudiesusuallyoptimizeforthelargesizeofthedatasettoensurethetrainingdatacan
captureasmanynormsaspossible(e.g.,130KsamplesinETHICSHendrycksetal.(2021b),and
1.7MsamplesinCommonsenseNormBank(Jiangetal.,2021)). Thestandardmodelingapproachis
tofine-tuneLLMsonthedatasetswhichcanachieveabout70to85%testperformance(Sapetal.,
2020;Hendrycksetal.,2021b;Jiangetal.,2021). However,thisapproachislikelytostrugglewhen
facedwithcompletelynovelcases–whichourchallengesetpresents. Ourmodelaimstosupplement
thesepreviousapproachesandbettermimichumanmoralflexibilitythroughcapturingtheunderlying
structureofthewaythathumansmakemoraljudgmentsthere
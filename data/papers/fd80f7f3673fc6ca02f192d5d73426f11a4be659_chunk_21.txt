Figure5),we formance,welookedathowparticularin-context
observeaninterestingtrend: whilesmallermodels examplesetsaffectthedistributionofscorespro-
arenotcapableofbeingeffectivezero-shotevalu- ducedbyLLM-basedevaluators. Figure7shows
ators,finetuningthemleadstocompetitiveperfor- the distribution of scores over the whole test set
mance,andonlyaslightdecreasewhencompared for the 1-shot and 2-shot settings, with different
)ed-ne(
secnerrucco
#
0.5 Score: 79.0
10000 Score: 94.0
0.4 Score: 99.0
1000
0.3
100
0.2
0.1 10
Zero-Shot
0.0 Finetune 1
Error 0 5 10 20 50 60 70 80 85 90 95100
S M Bison Score
Model
Figure5: BehaviorofPearsonaswescaletheLLM’s
Score: 85.0+95.0
parameter count. Note that the x axis is not to-scale 10000 Score: 99.0+90.0
withregardtoparametercount.
1000
0.40 PaLM-2 (Bison) ref-based 100
PaLM-2 (Bison) ref-free
0.35
10
0.30
0.25 Error 0 5 10 20 50 Scor6 e0 70 80 85 90 95100
0.20 Figure7: DistributionofscoresforPaLM-2(BISON)
0.15 models for 1-shot (top) and 2-shot (bottom) setups,
withvariousin-contextlearningsetsforeach(andtheir
0.10
scoresinthelegend)
0 1 2 3 4
# of in-context examples
Figure 6: Mean Pearson and its interquartile range
System Segmentρ
(IQR)intheWMT22EN-DEtestset,asweincreasethe
Model Ref? All EN-KKEN-GUKK-ENGU-EN
numberofin-contextexampleswithstratifiedsampling
Baseline
MetricX-XXL⋆ ✓ 94.0% 0.666 0.
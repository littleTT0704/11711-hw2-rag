 of the two
to an unprecedented growth of personal photo and video differences,VisualMemoryQAisexpectedtobemoreuse-
data. A recent study shows that the queries over personal fulinpractice.
photosorvideosareusuallytask-orquestion-driven(Jiang Toaddressthisnovelproblem,thispaperintroducesapro-
et al. 2017). For question-driven queries, users seem to be totype system that can automatically analyzes the content
using photos or videos as a mean to recover pieces from ofpersonalvideos/photoswithoutuser-generatedmetadata,
their own memories, i.e. looking for a specific name, place andoffersaconversationalinterfacetoanswerquestionsdis-
or date. For example, a user might ask “what was the last coveredfromtheuser’spersonalvideos/photos.Technically,
time we went hiking?”; “did we have pizza last week?” or itcanberegardedasanend-to-endneuralnetwork,consist-
“withwhomdidIhavedinnerinAAAI2015?”. ingofthreemajorcomponents:arecurrentneuralnetworkto
Wedefinetheproblemofseekinganswersabouttheuser’s understand the user question, a content-based video engine
dailylifediscoveredinhisorherpersonalphotoandvideo toanalyzeandfindrelevantvideos,andamulti-channelat-
collectionasVMQA(VisualMemoryQuestionAnswering). tentionneuralnetworktoextracttheanswer.Tothebestof
As about 80% of personal photos and videos do not have our knowledge, the proposed system is the first to answer
metadatasuchastagsortitles(Jiangetal.2017),thisfunc- personalquestionsdiscoveredinpersonalphotosorvideos.
tionalitycanbeveryusefulinhelpingusersfindinformation
intheirpersonalphotosandvideos.VisualMemoryQAisa VisualMemoryQuestionAnswering
novelproblemandhastwokeydifferencesfromVQA(Vi-
As shown in Fig. 2, the proposed model is inspired by the
sualQA)(Antoletal.2015):firsttheuserisabletoaskques-
classicaltextQAmodel(Ferruccietal.2010),
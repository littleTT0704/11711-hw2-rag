formsthebaselineacrossallthe
metricsandsomebyaconsiderablemargin.
Wealsocomparedfine-tuningpretrainingembeddingsand
jointtraining(learningtheSLUembeddinglayersatthesame
time as the other embeddings). For pretraining, we train a
networktopredicttheSLUtasks;onceconvergedtheAMRL-
specificcomponents(i.e.,thelastthreeLSTMlayersofeach
model)areaddedandtraineduntilconvergenceusingacross-
entropy loss. Joint training optimizes both SLU tasks and
AMRLonesatthesametime.Ateachtime-steparandom
training instance is selected from one of the two corpora
withprobability(p = |AMRL|/|SLU|).Sincethesizeof
the SLU corpus is much bigger then the AMRL one, we
need to prevent overfitting on the SLU tasks. To do so we
usedweightedcrossentropylossfunctionswheretheSLU
tasksweredown-weightedbyafactorλ=|AMRL|/(10×
|SLU|).
Figure7:Actiondistributiononthetrainingandtestingset
We see a slight improvement in accuracy of the models
fortheAMRLcorpus.
thatusejointtraining,thoughtherearenoconclusiveresults.
5395
Actions Traininginstances ΔICER F1 IC F1 SC IRER
PlaybackAction HIGH -0.53 baseline 0.93 0.84 25.71
SearchAction MED -0.17 baseline+emd+gaz 0.94 0.86 23.65
NextAction LOW -3.34 proposedmodel 0.94 0.86 23.11
ResumeAction LOW -10.81 proposedmodel+decoder 0.94 0.87 22.15
BrowseAction LOW +5.88
StopAction LOW -1.93 Table3:Resultsascomparedtovariousbaselines.Baseline
CreateAction LOW 0
isthemulti-taskmodeltrainedonlyonAMRLdata.Base-
RepeatAction VLOW +3.13
line+emb+gazisthebaselinemodelwithwordembedding
PreviousAction VLOW -23.23
andgazetteerfeaturesas
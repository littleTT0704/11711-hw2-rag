ofeachtransformerlayer. twowaystodothis: (1)bytransferring: firstfine-
Similartolengtheningtheinput,prefix-tuningdoes tunePLMsonmultiplelanguagesexcepttheone
not touch the PLM’s architecture, preserving the we concerned about (a.k.a target language), and
knowledge in PLM. Thus, by utilizing the PLM thenadaptthefine-tunedmulti-lingualmodeltothe
better,prefix-tuninghasbetterperformanceatfew targetlanguage;(2)bymultitasklearning: jointly
shotswhentherearenotenoughsamplestolearn trainalllanguagestogether. Inthisexperiment,we
newpatterns. However,whenthetrainingsetsize studymethod1andtrytoanswertheQ2: willsu-
becomeslarger,moreflexiblestructuresareneeded pervisedtransferbehelpfulforparameter-efficient
to learn from these samples, leading to the bet- tuning. InExp.3.3,westudymethod2.
ter performance of adapter-tuning. The possible
reasonwhybothprefix-tuningandadapter-tuning 3.2.1 ExperimentDetails
outperformPLMfine-tuningattheleftsideisthat SettingsandHyper-parameters Wefirstdivide
prefix-tuningandadapterhaveonly8%parameters XL-Sum dataset into two parts, one of which in-
totune,whichavoidoverfittingwhenthetraining cluding34languages(75%oftotallanguages)is
samplesarenotenough. used to fine-tune PLM jointly on multiple lan-
From this experiment, we can see that prefix- guages to obtain a single multi-lingual model,
mt5-base mt5-base34
Language Script
prefix-tuing adapter-tuing prefix-tuing adapter-tuing
R1 R2 RL R1 R2 RL R1 R2 RL R1 R2 RL
amharic Ge’ez 15.33 5.42 13.8 16.58 5.88 14.88 16.97 5.88 15.12 17.85 6.2 16.01
azerbaijani Cyrillic 15.72 6.3 14.47 17.81 7.34 16
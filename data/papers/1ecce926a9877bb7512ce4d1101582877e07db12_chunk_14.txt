.668.618.062.326.635.632.046.289.629.592.056.300.640.608.055
RTNet.240.622.634.038.365.638.766.020.194.555.668.028.247.591.683.025
Ours.355.714.617.037.483.682.834.016.382.658.742.024.404.678.732.026
Table1:Comparisontostate-of-the-artsalientobjectdetectionmethodsonASOD60Kdataset.↑meanslargerisbetter
and↓meanssmallerisbetter.Boldmeansthestate-of-the-artperformance.
LossFunction inducedsalientobjectdetectionbenchmarkforpanoramic
Theoverallobjectiveofourproposedmethodiscomposedof videos.Thereare62,455frameswith10,465instance-level
structurelossesforstudentandteacherbranchLstu,Ltch ground truths in the dataset. In particular, each video cor-
struc struc
andadistillationlossL responds to a 4-channel ambisonic audio recording. The
distill
ground-truth salient objects are determined by the eye fix-
L=Lstu +Ltch +λ L (5)
struc struc distill distill ation of 40 participants who viewed the video with HTC
whereλ distill isascalartobalancethelosses. ViveHMDheadset.ThetestsetofASOD60Kcontainsthree
Structure loss. Following previous method (Chen et al. subsetssplitbysoundeventclasses-miscellanea,music,and
2022), we leverage a combination of binary cross entropy speaking.
lossandDiceloss(Milletari,Navab,andAhmadi2016)as Metrics. To evaluate the performance of video salient
theobjectiveforsalientobjectdetection. object detection, we employ the adaptive F-Measure F
β
(Achanta et al. 2009), adaptive E-Measure E (Fan et al.
T T φ
L struc =(cid:88) L bce(M t,Mˆ t)+λ dice
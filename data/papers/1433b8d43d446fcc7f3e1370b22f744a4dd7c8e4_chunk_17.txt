ively
posed Python used it for most of their research, inflated evaluation numbers” (3) on a well-cited
dataset. Anotherparticipantdescribedworkingon
5While not every paper that cites BERT uses a BERT
apaperwheretheyrealized,anhourbeforethepa-
model,thisindicateshowcentralBERTisbothasamodel
andasaframeforthediscussionofotherwork.Forcompari- perdeadline,thatthestudentauthorshadusedtwo
son,onlytwootherpapershavebeencitedbymorethan20% differenttokenizersinthepipelinebymistake: “we
ofanthologypapersinasingleyear: “AttentionisAllYou
decidedthatwell,theresultswerestillvalid,and
Need”(Vaswanietal.,2017)with27%in2021andGloVe
(Penningtonetal.,2014)with21%in2019. theresultswouldonlygetbetterif[itwasfixed]...so
thepaperwentout. Itwaspublishedthatway.” (26) of reach for academics, echoing concerns previ-
Softwarebugsinresearchcodearenotanewprob- ously described by Ahmed and Wahed (2020).
lem,6 butparticipantsdescribedbugsintoolkitsas Participantsworriedthat“wearebuildinganup-
difficult to diagnose because they “trust that the perclassofAI”(6)wheremostresearchersmust
libraryiscorrectmostofthetime”(8),evenasthey “buildontopof[largemodels]”(15)thattheycan-
spokeoffinding“many,many,many”(8)bugsin not replicate, though others expressed optimism
toolkitsincludingHuggingFaceandPyTorch. that “clever people who are motivated to solve
these problems” (22) will develop new efficient
Software is implicit funding Participants sug-
methods(Bartoldsonetal.,2023). Industrypartici-
gested that tools that win the software lottery act
pantsfromlargetechcompaniesalsofeltresource-
asasortofimplicitfunding: theyenableresearch
constrained: “modernmachinelearningexpands
groupstoc
els. There- could be evaluated by calculating the final score
fore, it isimportanttobe cautiouswhendrawing acrossdifferentsplitsofthedataset. Thedifficulty
conclusions about ToM in models based on their levelofthedatasetcanthenbedeterminedbased
performance on a few tasks (Marcus and Davis, onthelowestscoreobtainedamongthesesplits.
2023). Ingeneral,whenasystemsucceedsonan
8WeleavethequestionofwhetherLLMs,AI,orevenany
instrumentdesignedforhumans,wecan’tdrawthe
non-biologicalentitycoulddevelophuman-likecognitionand
same conclusions as we would for humans (e.g., TheoryofMinduptophilosophers.
9
Emergence vs. Supervised Learning vs Train- parts of the datasets could be ambiguous, either
ingontheTestset PriorworkclaimedthatToM duetolackofcontextorinherentambiguity(Plank,
abilitiesemergedasabyproductoftheLLMtrain- 2022). Duetothispotentialambiguity,someLLMs
ing(Kosinski,2023). Wearguethatclaimsabout were safeguarded and refused to answer certain
emergenceare(i)unfounded,and(ii)unfalsifi- questions;whileweattemptedtoinstructthemto
ablewithoutaccesstotheLLMs’trainingdata. respondinthecorrectformat,someLLMsstilldid
To make a statement regarding emergent ToM, a notoutputtherightformat. Thiswasonlyanissue
careful experiment is needed to ensure that ToM forMC-probing,butprobabilitydistributionswere
did indeed appear spontaneously and not as a re- not available for all LLMs. Future work should
sult of other factors such as training on related investigatehowtomitigatethisissueviabetterin-
datasets,exposuretodescriptionsofclinicaltests structionsormethodsthatmapgeneratedanswers
online, interactions with users, and more.9 How- to multiple choice better (e.g., Niu et al., 2021;
ever,sincethedatausedtotraintheGPTmodels Bulianetal.,2022).
isnotpubliclyavailable, itisimp
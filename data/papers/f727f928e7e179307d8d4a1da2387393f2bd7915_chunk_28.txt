,θ) UpdateoneLMbelief
tics,9:176–194.
L(ϕ;x i,yˆ i,y i∗,θ) Beliefupdateobjectiveforx
i
LSequential(ϕ; D,θ t) Sequentialobjective(SLAG)
ChenZhu,AnkitSinghRawat,ManzilZaheer,Srinadh K #gradientstepsinUpdate()
·
r #beliefsupdatedin
Bhojanapalli,DaliangLi,FelixYu,andSanjivKumar. LSequential
2020. Modifyingmemoriesintransformermodels.
Table9: Symboldescriptionsforthelearnedoptimizer.
arXivpreprintarXiv:2012.00363.
tasks,weusethecorrectlabelwhenyˆ isincorrect,
i
A LearnedOptimizerDetails andwhenyˆ i iscorrect,werandomlyselectanother
labelfromthetrainingdata. Thischoiceisincon-
Architecture. KNOWLEDGEEDITOR isalearned trast to De Cao et al. (2021) and Mitchell et al.
optimizerg : Θ Θthatproduces (2021), who use samples from the model beam
X ×Y ×Y × →
new model weights by applying an adjusted gra- searchasupdatelabelsforallpoints.
dient step to a model. For reference, we give a
glossaryofsymbolsusedhereinTable9. Forad- B AdditionalTrainingDetails
ditionaldetailsbeyondwhatispresentedhere,we
B.1 ComputeCosts.
referreaderstoDeCaoetal.(2021).
Atahighlevel,g ϕ firstencodesaninputx i and Learnedoptimizermemory. Thehypernetwork
requestedpredictionchangeintoavectorh, then has92mtrainableparametersforRoBERTa-base
processes h into two low-rank matrices A and B (which is 125m parameters), and 105m param-
thatareusedtotransformthemodelgradientonx i, eters for BART-base (which is 139m parame-
∇θ L(x i,y i∗). ForTransformermodels,themethod t
 through nitude and only focuses on the difference between target
a customized characteristic function that is dependent on and non-target function. Here we take a step further by
the feature magnitude. How to design or learn a better showing a unified way to characterize the loss function as
characteristicfunctionthatisdynamicallydependentonthe awhole.Specifically,wehavethefollowinggeneralformof
data and also easy to optimize remains a huge challenge. thelossfunctionforhypersphericalFR:
Moreover, the underlying mechanism that determines the (cid:16) (cid:88) (cid:0) (cid:1)(cid:17)
L =log 1+ exp s·(η(θ )−η(θ )+∆(θ ))
performanceofacharacteristicfunctionstaysamysteryand s i y y
needstobeunderstoodbothempiricallyandtheoretically. i(cid:54)=y (cid:124) :=Q(θy(cid:123),(cid:122) θi,s,m) (cid:125) (19)
Making better use of feature magnitude. In this paper, (cid:16) (cid:88) (cid:0) (cid:1)(cid:17)
=log 1+ exp Q(θ,θ,s,m)
y i
we have not considered to incorporate feature magnitude
i(cid:54)=y
to testing and still stick to the cosine similarity for com-
paringpairs.However,itremainsaninterestingopenprob- where we define Q(θ y,θ i,s,m) as the loss characteristics
lem whether it will be more beneficial to combine feature thatfullydeterminehowthelossfunctionbehaves.Wecom-
magnitude back to the similarity score (especially FN-free parethelosscharacteristicsamongnormalizedsoftmax[9],
learning or SFN is used). We consider a generalized form SphereFace and two variants of SphereFace-R in Fig. 5.
IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 10
Although we show that the loss characteristics can fully TABLE2
determine the loss
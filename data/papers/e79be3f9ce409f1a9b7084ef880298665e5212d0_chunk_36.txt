 to train the model. The bottom four rows are the separate
8,12
performance at different stages for our model. As we can
[61] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk see, combiningthreecontrastivelossesduringtrainingcan
Cinbis,DavidFouhey,IvanLaptev,andJosefSivic. Cross-
boost the performance for both early and later stage (row
task weakly supervised learning from instructional videos.
3 v.s. row 1, row 5 v.s. row 2). This indicates that the
InProceedingsoftheIEEEConferenceonComputerVision
threelossesaresynergistictoeachotherforabettervideo-
andPatternRecognition,pages3537â€“3545,2019. 1,2,5,8
textalignment.Ontheotherhand,theearlystagealignment
achievesbetterperformancethanothertwo(token-leveland
A.Tokensofinterest
later stage), while the fused score is the best. We suspect
thatthisisbecauseearlystagealignmentistrainedwithall
Dataset Noun Verb All
text-video pairs at sentence-level. In contrast, token-level
YouCook2[58] 378 168 2,144 contrast focuses on single tokens and the multi-modal fu-
MSR-VTT[52] 4,415 1,463 15,740 sionlayersmerelyseeasmallpartofhardtext-videopairs.
ActivityNet[25] 2,602 1,021 9,059
C.Effectofcascadesampling
Table10: Tokenstatisticsforeachdataset.
Theproposedcascadesamplinghelpsthelaterstagecon-
Weextracttokensofinterest(T.O.I)usingthepos-tagger
trastive learning to focus on hard negative samples. As
providedbySpacy[20]. InTable10,weshowthestatistics
shown in our main submission, adding cascade sampling
of tokens for three datasets. For each token that is tagged
will improve the performance. We suspect this is because
atVERBorNOUN,wecomputetheinversedocumentfre-
cascadesamplinghelpslearnabetterlaterstage
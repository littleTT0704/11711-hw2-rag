heyareonly
capableofelicitingalimitedvarietyofknowledge
References
andrequirecarefulhand-craftingtotransfertonew
tasks. Otherexplanation-basedmethods(Latcinnik Ning Bian, Xianpei Han, Bo Chen, and Le Sun. 2021.
Benchmarking knowledge-enhanced commonsense
andBerant,2020;Rajanietal.,2019)finetunethe
question answering via knowledge-to-text transfor-
generator model so that it produces explanations
mation. arXivpreprintarXiv:2101.00760.
thatareusedforquestionaugmentation. DynaGen
Antoine Bosselut, Ronan Le Bras, and Yejin Choi.
(Bosselut et al., 2021) uses pretrained common-
2021. Dynamic neuro-symbolic knowledge graph
sensemodelstogenerateimplicationsofaquestion
construction for zero-shot commonsense question
andexpandstheinferenceinputwiththesegener- answering. In Proceedings of the 35th AAAI
ations. However, its usage of COMeT (Bosselut ConferenceonArtificialIntelligence(AAAI).
et al., 2019) as the generator confines its appli-
Antoine Bosselut, Hannah Rashkin, Maarten Sap,
cability to the social commonsense domain. Our Chaitanya Malaviya, Asli Celikyilmaz, and Yejin
workcontributestothisgenerallineofresearch,yet Choi. 2019. COMET: Commonsense transformers
different from these previous methods that elicit for automatic knowledge graph construction. In
Proceedings of the 57th Annual Meeting of the
knowledge with task-specific templates or from
Association for Computational Linguistics, pages
finetuned knowledge generators, our method re-
4762â€“4779,Florence,Italy.AssociationforCompu-
quiresonlyafewhuman-writtendemonstrationsin tationalLinguistics.
thestyleofthetask,makingitmuchmoreflexible,
TomBBrown, BenjaminMann, NickRyder, Melanie
easy-to-transfer,andengineering-efficient.
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan,P
y)
E(x∗,y∗)∼D[I(x∗,y∗)(x,y)] =
N
, (4.1)
where N is the size of the data set D, and m(x,y) is the number of occurrences of the configuration
(x,y) in D.
19
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
The experience function f accommodates the data instance experience straightforwardly as below:
f :=f data(x,y;D) =logE(x∗,y∗)∼D[I(x∗,y∗)(x,y)]. (4.2)
Figure 2 (a)–(b) shows an illustration. In particular, the logarithm of the expected similarity is used as the
experience function score, that is, the more ‘similar’ a configuration (x,y) is to the observed data instances,
the higher its quality. The logarithm serves to make the subsequent derivations more convenient as can be seen
below.
With this from of f, we show that the SE derives the conventional supervised MLE algorithm.
Supervised MLE. In the SE Equation 3.2 (with cross entropy and Shannon entropy), we set α = 1, and β to
a very small positive value ϵ. As a result, the auxiliary distribution q(x,y) is determined directly by the full
data instances (not the model p θ). That is, the solution of q in the teacher-step (Equation 3.3) is:
βlogp (x,y)+f (x,y;D)
q(x,y)=exp
{
θ
α
data }/Z ≈exp{f data(x,y;D)}/Z =p~ d(x,y), (4.3)
which reduces to the empirical distribution. The subsequent student-step that maximizes the log-likelihood of
samples from q then leads to the supervised MLE updates w.r.t. θ.
4.1.2. Self-Supervised Data Instances
Given an observed data instance t∗ ∈ D in general, one could potentially derive various supervision signals
based
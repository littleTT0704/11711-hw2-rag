,andthesemantic
is 3×d×V where d is the hidden state dimen- vector.
Model Sem.Sim. BitextMining Quest.Retrieval Score
Eng. XL Tatoeba NQ MKQA
- - ar-en ar-ar es-en es-es tr-en ar es tr - ar es tr
RandomInit.(6Layer)-ar,en,es,tr
CONTRASTIVE 68.6 81.7 68.5 68.3 64.8 69.7 97.9 88.2 98.1 36.4 24.6 13.0 22.6 58.1
BITRANSLATION 69.0 82.4 63.1 57.8 58.7 67.3 97.6 84.3 96.4 37.8 25.4 12.2 21.1 57.0
VMSST 70.6 83.1 65.9 63.1 62.1 68.8 97.9 84.9 97.2 38.1 27.2 14.4 24.1 58.5
VMSST(fullenc.,fulldec.) 70.3 82.3 65.6 60.0 62.8 67.9 98.4 87.9 97.8 39.0 27.2 14.7 24.2 58.7
Table6: ComparisonofVMSSTwithavariationthathasnoparametersharing,VMSST(fullenc.,fulldec.). We
experimenton4languages,sowehave5encodersand4decoders.
Thelastfourablationsinvestigatedifferentmod- lation of VMSST without the source separation.
elling choices. In the first we eliminate the KL However, there is still a gap between the full en-
term (no KL), which has the most significant ef- coder/decoder of VMSST and VMSST. We hy-
fect on performance, especially on cross-lingual pothesize however, that as the number of layers
tasks. In the second ablation, we use a single en- ofthemodelincreases,thisperformancegapalso
coderinsteadofthetwinencoders(1enc.),onefor shrinks. The extra capacity of these layers will
semanticembeddingsandoneforlanguageembed- allowforthemodeltoseparatelanguage-
. The
interpolation allows the learning to enjoy different desired properties in different training stages, resulting in
improved efficacy.
As a concrete example, (Tan et al., 2018) learn a text generation model by starting with the simple supervised
MLE algorithm, with the experience function f = f (Equation 4.2) and balancing weights
τ=0 data
(α = 1,β = ϵ) as described in Section 4.1.1. After warming up with the supervised MLE for n
τ=0 τ=0
iterations, the approach then changes the experience function to the data augmentation-based one
f = f defined in Equation 4.8, which introduces noise and task-specific evaluation information
τ=n data-aug
into the training. As revealed in Section 4.1.4, this stage in effect corresponds to the reward-augmented
maximum likelihood (RAML) algorithm (Norouzi et al., 2016). The approach proceeds by further annealing
the balancing weights, specifically by gradually increasing β from ϵ to 1 as τ increases. The increase of β
τ
effectively gets the learning closer to a reinforcement-style learning (notice that the policy gradient algorithm
has α = β = 1 as described in Section 4.3.1). Intuitively, the target model p, as part of the q(τ+1)
θ(t)
solution in the teacher step weighted by the increasing weight β (see Equation 3.3), serves to produce more
data samples. Those samples are weighted by the experience function and used for updating the target model
35
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
further, simulating the policy gradients. Therefore, the whole learning procedure spans multiple paradigms of
learning (from supervised MLE, RAML, to reinforcement learning) that increasingly introduces more noise
and exploration for improved robustness. All the involved paradigms of algorithms are perfectly encompassed
in the single SE formulation, allowing users to simply tweak and evolve the relevant components for the
interpolation.
7. Optimization Algorithms
Thus far, we have discussed the standard equation as the unified objective function. Learning the target model
p θ amounts to optimizing the objective w.r.t the model parameters θ. That is, the standardized objective
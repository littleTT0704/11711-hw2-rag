our case as we do not have access to semanti-
performexperimentson4targetlanguagesthatdo
cally aligned data for various languages. Thus
notbelongtoourextremelylow-resourcecategory
we adopt Hausdorff distance as a metric that has
fortheNERtask,namely,Spanish(es),French(fr),
been widely used in vision and NLP tasks (Hut-
Italian (it), Russian (ru) and Chinese (zh). These
tenlocher et al., 1993; Dubuisson and Jain, 1994;
languagesaretypicallyconsideredhigh-resource
Patraetal.,2019)tomeasurethedistancebetween
with20klabeledexamplesintheWikiAnndatasets
two distinct datasets. Informally, the Hausdorff
andlargeamountofunlabeleddataconsumedby
distance measures the average proximity of data
mBERT for pre-training. We use only 100 ex-
representationsinthesourcelanguagetothenear-
amples forall target languagesto mimicthe low-
est ones in the target language, and vice versa.
resourcesettinganduse5kEnglishexamplesfor
Given a set of representations of the source lan-
transfer.
guageS = {s,s,...,s }andasetofrepresen-
1 2 m As shown in Table 7, we found slight perfor-
tationsofthetargetlanguageT = {t,t,...,t },
1 2 n mancedropusingMetaXLforthesehigh-resource
wecomputetheHausdorffdistanceasfollows:
languages. Weconjecturethattheselanguageshave
beenlearnedquitewellwiththemBERTmodeldur-
ingthepre-trainingphase,therefore,leavingless
max{maxmind(s,t),maxmind(s,t)} (5)
s∈S t∈T t∈T s∈S scopeforeffectiverepresentationtransformationin
the low-resource setup. Nonetheless, this can be metalearnanetworkthatexplicitlytransformsrep-
remediedwithaback-offstrategybyfurtherfine- resentationsforcross-lingualtransferonext
,sincewefoundlittlevariationinchanging
this.Forbothgradients-basedexplainers,weprojectintothesimplexbyusingtheSOFTMAXfunction,
similartotheattention-basedexplainers. Thisresultsinverynegativevalueshavinglowprobability
values. Moreover,forevaluatingplausibilityontextclassificationandtranslationqualityestimation,
wecomputedtheexplanationscoreofasinglewordbysummingthescoresofitswordpieces.
Wewouldliketonotethat,unlikethesettinginPruthietal.[2020],wedonotapplyatop-kpost-
processingheuristicongradients/attentionlogits, insteaddirectlyprojectingthemtothesimplex.
Thismightexplainthedifferenceinresultstotheoriginalpaper,particularlyforthelowsimulability
performanceofstaticexplainers.
B HumanStudyforVisualExplanations
The annotations were collected through an annotation webpage, built on top of Flask. Figure 6
showsthethreepagesofthesite. Duringtheannotation,userswereaskedtorankfourexplanations,
unnamedandinrandomorder. Aftercollectingtheratings,wecomputedtheTrueSkillrating,with
aninitialratingforeachmethodofµ=0,σ =0.5. Afterlearningtheratings,wethencomputethe
ranksbyobtainingthe95%confidenceintervalfortheratingeachmethod,andconstructingapartial
orderingofmethodsbasedonthis.
16
Thevolunteerswereamixtureofgraduatesorgraduatestudentsknownbytheauthors. Howeverwe
wouldliketopointoutthatduetoblindnatureofthemethodannotation,thechanceofbiasislow.
Figure6: Loginpage(left),dashboard(middle)andannotationpage(right)
InFigures7and8weshowrawattentionexplanationsextractedfromalllayers(rows)andheads
(columns) of the teacher transformer used in our CIFAR-100 experiments. Cross-checking the
explanationswiththemostrelevantheadsselectedbySMaTforimageclassification,wecanseethat
mostselectedheadsproduceplausibleexplanations(e.g.,attentionheadsfromthelast
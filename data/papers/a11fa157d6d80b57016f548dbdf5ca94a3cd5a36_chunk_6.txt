 Elazar et al. 2021
selection,termaggregation,filtering,etc. Recent analyzed the consistency of pretrained LMs with
attempts also utilize LMs for AKBC. Wang et al. respecttofactualknowledge. Jiangetal.2020used
2021afinetunedLMsforlinkprediction. Feldman paraphrasingtoimprovefactualprobing. Newman
et al. 2019; Bouraoui et al. 2020 utilized LMs to etal.2021trainsanadditionallayerontopofword
score entity pairs collected from the Internet or embeddingtoimproveconsistency. Recently,con-
missingedgesinexistingKGs. COMET(Bosselut sistencyisalsoshownhelpfultoimprovetherea-
etal.,2019)isagenerativeLMtrainedtopredict soning ability of large LMs (Wang et al., 2022;
tailentitiesgivenheadentitiesandrelations. West Jungetal.,2022;Haoetal.,2023). Inourframe-
etal.2021distilltheknowledgeinGPT-3toagen- work, the extracted entity pairs for each relation
erative LM. By prompting GPT-3 (Brown et al., areenforcedtoconsistentlysatisfyadiversesetof
2020)withexamples,theyproducedATOMIC promptsandregularizedbyseveralscoringterms.
10x
to teach the student model. Yet, this method re-
quiresthestrongfew-shotlearningabilityofGPT-3 3 HarvestingKGsfromLMs
and is not generally applicable to most LMs. To
This section presents the proposed framework
the best of our knowledge, our framework is the
for extracting a relational KG from a given pre-
first to construct a KG by extracting purely from
trained LM, where the LM can be arbitrary fill-
an LM (with the minimal definition of relations
as input). The new paradigm can also be seen as
in-the-blank models such as BERT (Devlin et al.,
optimizingasymbolicKGwith(pretrained)neu-
2019),ROBERTA(Liuetal.,2019),BART(Lewis
etal.,2020),or GPT-3 (withappropriateinstruc-
ral models as supervision (
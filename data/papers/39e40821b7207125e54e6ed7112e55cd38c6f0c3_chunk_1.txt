Language Models of Code are Few-Shot Commonsense Learners
♠ ♠ ♠
AmanMadaan,ShuyanZhou,UriAlon,
♠ ♠ †
YimingYang,GrahamNeubig
♠
LanguageTechnologiesInstitute,CarnegieMellonUniversity,USA
†
InspiredCognition,USA
{amadaan,shuyanzh,ualon,yiming,gneubig}@cs.cmu.edu
Abstract tionalcommonsensereasoningtaskssuchasread-
ing comprehension or question answering, struc-
Weaddressthegeneraltaskofstructuredcom-
tured commonsense aims to generate structured
monsensereasoning: givenanaturallanguage
input, the goal is to generate a graph such outputgivenanaturallanguageinput. Thisfamily
as an event or a reasoning-graph. To employ oftasksreliesonthenaturallanguageknowledge
largelanguagemodels(LMs)forthistask,ex- learnedbytheLLM,butitalsorequirescomplex
istingapproaches“serialize”theoutputgraph structuredpredictionandgeneration.
as a flat list of nodes and edges. Although
ToleverageLLMs,existingstructuredcommon-
feasible, these serialized graphs strongly de-
sensegenerationmodelsmodifytheoutputformat
viate from the natural language corpora that
ofaproblem. Specifically,thestructuretobegen-
LMswerepre-trainedon,hinderingLMsfrom
generating them correctly. In this paper, we erated (e.g., a graph or a table) is converted, or
show that when we instead frame structured “serialized”, into text. Such conversions include
commonsense reasoning tasks as code gener- “flattening”thegraphintoalistofnodepairs(Fig-
ation tasks, pre-trained LMs of code are bet- ure 1d), or into a specification language such as
ter structured commonsense reasoners than
DOT(Figure1c; Gansneretal.,2006).
LMsofnaturallanguage,evenwhenthedown-
Whileconvertingthestructuredoutputintotext
streamtaskdoesnotinvolvesourcecodeatall.

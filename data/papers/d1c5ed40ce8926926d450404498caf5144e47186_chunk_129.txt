
.noisnapxe
tuohtiw
aroproc
dees
eht
ot
ES
lacitsitats
erapmoc
dna
tset
ngis
dedis-eno
92 CHAPTER 6. APPLICATION TO QUESTION ANSWERING
Sources Passages@20 Titles@10
Recall Rel Recall Rel
Wikipedia 57.46% 2.50 25.54% 0.38
Top 100,000 59.08% 2.59 29.02% 0.45
Top 200,000 59.70% 2.64 29.31% 0.48
Top 300,000 59.83% 2.67 29.54% 0.49
Table 6.12: Search recall of OpenEphyra on TREC questions when expanding in-
creasing numbers of Wikipedia seed articles using web search results.
of more subtle differences between the search components of the two QA systems.
For example, the sentence detection algorithm used by Watson to align the retrieved
token windows with sentence boundaries appears to be more reliable, and Watson
deploys a modified version of Indri whose passage scoring model has been customized
to improve QA search results. Also note that Watson retrieved at least 50 document
titles per question (55 titles when searching indices of long and short documents sepa-
rately), whereas in the experiments with OpenEphyra on TREC datasets we reduced
the hit list length to 10 since the title search approach is less effective for TREC ques-
tions. This is because there are few questions that provide a definition or properties
of an entity and ask for its name.
The recall curves in Figure 6.4 illustrate that statistical SE also consistently im-
proves OpenEphyra’s search recall on TREC 8–15 independently of the hit list length.
We show results for passage and title searches using Wikipedia and Wiktionary as
seed corpora. When retrieving titles, SE does not add new candidates that could not
be found in the seed corpus, but it helps return relevant titles at higher ranks. This
explains why the increase in title recall for Wikipedia is largest if relatively few titles
are retrieved. If longer hit lists are used, the order of the titles in the ranking has less
impact on search recall. The improved ranking performance is reflected by the mean
reciprocal rank of the top 100 titles (
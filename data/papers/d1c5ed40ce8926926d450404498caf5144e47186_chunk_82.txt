 terms, but we still found that such instances are
sometimes missed by annotators.
It is also interesting to note that the results are similar for both topics, even
though Mother Teresa is much more specific and unambiguous, whereas Iran-Iraq
War is extremely broad and open-ended. Initially we expected higher agreement on
the more constrained topic, but it appears that even for the more general topic the
obviouslyrelevantorirrelevanttextoutweighstheinstancesonwhichannotatorstend
to disagree. This is the case independently of the granularity at which relevance is
assessed (paragraphs, sentences or tokens).
Our annotation interface and guidelines seem to suffice for labeling data efficiently
and with high agreement between annotators. Ambiguous or hard instances occur
relatively infrequently, thus the annotation decisions in these borderline cases have
little impact on the overall quality of the dataset. Furthermore, even if annotators
disagree on these instances it may not matter much because we are not attempting
to develop an accurate classifier, but instead our goal is to rank text nuggets by
relevance. If marginally relevant nuggets are not labeled consistently, a model fitted
to this data may rank such nuggets lower than instances that are definitely relevant.
This could even result in the selection of more useful text for source expansion.
5.2. EXPERIMENTAL SETUP 55
The annotation guidelines were developed to ensure that multiple annotators de-
velop a similar notion of relevance and agree on where to draw the line. If a single
annotator labels all data, it may be less important how critical this annotator is when
judging relevance. Depending on how much of the training data is annotated as rele-
vant, the confidence scores estimated by a model may overall be somewhat higher or
lower. However, we can compensate for such deviations by adjusting the confidence
threshold that is applied in the merging phase of the source expansion pipeline.
We found that the choice of topics for the annotation task is extremely important.
While it does not seem to matter whether the topics are narrow or broad, it helps
to select topics that have high coverage in the sources from which related documents
are retrieved. If obscure topics are chosen, the documents mostly contain marginally
relevant content that can be hard to label consistently. We further excluded “active”
topics such as living people or ongoing events since the search results for these topics
often cover
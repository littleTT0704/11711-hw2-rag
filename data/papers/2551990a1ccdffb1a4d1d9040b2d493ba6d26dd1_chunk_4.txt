thanthat
which performs cross-modal self-supervised pre-
inVQA-MedandVQA-RAD.
training (Tan and Bansal, 2019) and VQA fine-
tuning of a pathology image encoder and a ques-
2.2 Cross-modalSelf-supervisedLearning
tion encoder end-to-end to learn powerful visual
Cross-modalself-supervisedlearninglearnsrepre-
and textual representations jointly and automati-
sentationsfordatawithmultiplemodalitiesbysolv-
callyidentifiesandexcludesnoisyself-supervised
ingcross-modalauxiliarytasks. VisualBERT(Li
examples from pretraining. Experiments on our
etal.,2019)learnsrepresentationsforimagesand
developed PathVQA dataset demonstrates the ef-
textsbyimplicitlyaligningelementsofatextand
fectivenessofourproposedmethods.
regionsinanassociatedimagewithself-attention.
Themajorcontributionsofthispaperareasfol-
CVLP (Shi et al., 2020) proposes an unbiased
lows:
contrastivevisual-linguisticpretrainingapproach,
• Wecreateapathologyvisualquestionanswer- which constructs a self-supervised loss based on
ingdataset–PathVQA,tofostertheresearch contrastive learning. ViLBERT (Lu et al., 2019)
ofmedicalVQA.Toourbestknowledge,this proposestopretrainavision-and-languageBERT
isthefirstdatasetforpathologyVQA. modelthroughmaskedmulti-modalmodelingand
alignment tasks, and then transfer the model to
• Weproposeathree-leveloptimizationframe-
visualquestionansweringtasks.
work which performs cross-modal self-
supervised pretraining and VQA finetuning 2.3 DataSelectionandDataReweighting
ofapathologyimageencoderandaquestion
A number of approaches have been proposed for
encoder end-to-end to learn powerful visual
dataselection. Renetal.(2018)proposesameta
and textual representations jointly and auto-
learning method to learn the weights of training
matically
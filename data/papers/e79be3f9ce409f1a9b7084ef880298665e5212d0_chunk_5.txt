 to current
Negative sampling. Key to efficient contrastive training
state-of-the-artmethods.
isagoodsourceofnegativeexamples. Mostofcurrentap-
proachesuserandomsamplingstrategiesfortrainingvideo-
2.Relatedwork
textalignment[60,33]. However,inthedomainofimage-
Video-language pretraining. Realistic application sce- text retrieval, a few works tried hard negative sampling to
narios around videos have prompted emergence of vari- choose the hardest negatives for training. In [2, 13], the
ous video-language tasks, such as text-video retrieval [30, authors computed the alignment scores for all image-text
55, 53], video question answering [21, 27], video caption- pairs in a mini-batch and use the hardest negative sample
ing[54,59],etc.InspiredbythesuccessofBERTforlarge- to compute the marginal loss. However, this strategy can
scale pretraining in language domain [10], transformers onlybeappliedwithoutmulti-modalfusion. Inthosemod-
havebeenemployedinthevideo-languagedomain[41,60, els which have multi-modal fusion layers for better repre-
33,28]aswellasimage-languagedomain[42,32,57,29]. sentations [32, 8], the authors instead compute the match-
Combinedwithlargescaledatasets, e.g. Howto100M[35] ing score offline and then use it to sample hard negatives
this approach has proven to be effective on various down- forfinetuningimage-textretrievalmodel,whichhoweveris
streamtasks. Dependingonthetasksofinterest, someap- difficult for large-scale pretraining due to the high compu-
proachestrainamulti-modaltransformerusingacombina- tationalcost. Inthispaper,ourcascadehardnegativemin-
tion of multiple losses including video-text alignment [41, ing is particularly designed to address these issues as we
60, 33, 28], masked token (words/frames/objects) predic- efficiently select the hard negative samples online before
tion [41, 60, 33], and frame order prediction [28], etc. multi-modal fusion and send them to the
al.(2021)andperformednucleussampling(Holtzmanetal.,2019)withp=0.95.
Foreachk,wesearchedforthebesttemperatureforeachmodelfrom{0.2,0.4,0.6,0.8,1.0}. On
average,eachexamplehas2.03tests. TheconcatenationofmultiplePythondocsoftenexceededthe
lengthlimitofGPT-Neo,wehenceexperimentedinthisdatasetwithFiD,whichallowslongerinputs.
AdditionaldetailsareprovidedinAppendixB.
5 RESULTS
Inallfollowingresults,allmodelswithDocPromptingusethetop-10retrieveddocsfromthebest
retrieveronthatdataset(Table4). Everybaselineusestheexactsamesetupasits“+DocPrompting”
version,exceptfornotusingthedocumentation.
5.1 SHELLSCRIPTINGRESULTS
ResultsfortldrareshowninTable1. DocPromptingconsistentlyimprovesthebasemodels. For
example,T5+DocPromptingachievesmorethantwicehigheraccuracyinpredictingthecommand
name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact
matchgain,comparedtothevanillaT5. Inthefew-shotlearningsettingwithCodex,DocPrompting
bringsgainsof6.7charBLEUpoints,andconsistentimprovementacrossallmetricsoverthebaseline
thatobservesonlyNL-codepairsinitsprompt. Theseresultsshowthatretrievingdocumentation
alsobenefitsstrongmodelssuchasCodex,andwithonlyfewexamplesinthecontext.
Code generation with oracle command names In realistic settings, a human programmer may
knowthecommandnametheyneedtouse(e.g.,awk),butnotknowtheexactusageandflags. In
fact,betterunderstandingoftheusageofknowncommandsisthepurposeofUnixmanpagesandthe
5
PublishedasaconferencepaperatICLR2023
Table1: Resultsonshellscripting,usingaBM25retrieverwithtop-10retrieveddocs,
controlflowandoperatorsininferenceruntimes. exceed CPU launch costs. If computation is bot-
SADim FCDim Batch Seq Latency TP ResNet-50),weshowthatallmodelsexhibitframe-
workboundedness. Additionally,weobservethat
768 3072 1 128 0.0136 0.0136
768 3072 4 128 0.0134 0.0034 theseinefficienciesarebecomingmoreapparentas
768 3072 1 512 0.0134 0.0134
hardwareacceleratorspeedsincrease.
1536 6144 1 128 0.0134 0.0134
Weintroducetheconceptoftheframeworktax
Table1:Latencyandthroughput(TP)ofBERTPyTorch todescribewhenimprovementsinhardwarespeed
models on RTX-8000. Scaling along batch sizes and andreductionsinrequiredcomputationfailtotrans-
modelwidthshowsnoincreaseinlatency. latetospeedupsinmodellatencyandthroughput
duetobottlenecksincurredbydeeplearningframe-
works. Wehopethattheseobservationsraiseaware-
tleneckedbyframeworkoverhead, thebatchsize
nessoftheimpactandlimitationscreatedbychoice
orinputsizecanbeincreasedwithoutincreasesin
of deep learning frameworks on model develop-
overallruntime. Inpractice,thiscanleadtothepro-
mentanddeployment.
cessingoflargerbatchsizesandsequencelengths
atnoadditionallatencycostuntilkerneloperations 7 Limitations
saturateframeworkoverhead.
Inthiswork,westudytheinferenceefficiencyof
Model width can be increased at no cost for
speechandlanguagemodelsusingGPUhardware
framework-bound models. For a given batch
accelerators. While GPUs are the most common
size,individuallayersofaframeworkboundmodel
general purpose hardware accelerators, there ex-
canbemadewiderbyincreasinghiddendimension
ist domain specific architectures such as Google
orfiltersizewithoutimpactinglatency. Widermod-
TPUâ€™s,GraphCoreIPUs,andcustomASICswhich
averagethelossobjectivesifweconsidera controls the angular margin. When ∆(θ y) = 0 and η(·) is
mini-batchofinputsamples.Sincetheclass-dependentbias thecosinefunction,Eq.(5)reducestothestandardsoftmax
term is not informative in open-set evaluation, we follow loss with weight normalization. ∆(θ y) = 0 indicates that
the common practice to remove it [2]. Then we normalize no angular margin has been introduced. When ∆(θ y) > 0,
theclassifierweightstoone(i.e.,(cid:107)W i(cid:107) = 1,∀i)andrewrite this leads to large angular margin because it makes the
theobjectivefunctionasfollows: classification more stringent (i.e., the neural network will
(cid:18) exp(cid:0) (cid:107)x(cid:107)cos(θ )(cid:1) (cid:19) learn to make θ y smaller in order to reach the same loss
y
L s =−log (cid:80)K
exp((cid:107)x(cid:107)cos(θ ))
(2) valueasthecaseof∆(θ y)=0).Itisalsoworthmentioning
i=1 i thatwhen∆(θ y)<0,Eq.(5)definesaneasiertaskthanthe
whereθ i denotestheanglebetweendeepfeaturexandthe standardclassificationproblemandispotentiallyusefulfor
i-th classifier W i. By considering a generic angular activa- robust learning against noisy images or labels. Our paper
tion rather than the cosine function, we have the following focusesonthecaseof∆(θ y)>0.
generalizedobjectivefunction: We note that there exist scenarios where large angular
(cid:18) exp(cid:0) (cid:107)x(cid:107)ψ(θ )(cid:1) (cid:19) margin can still be achieved even if ∆(θ y) is smaller
 simplicity. If Assumptions A1, A2, and A3 (See Appendix)
hold, we have, with probability at least 1−δ
L(θ(cid:98)RSC(S);S)−L(θ R(cid:63) SC(S);D)
(cid:114)
2(log(2|Θ |)+log(2/δ))
≤(2ξ(p)+1) RSC
n
As the result shows, whether RSC will succeed depends on the magnitude
of ξ(p). The smaller ξ(p) is, the tighter the bound is, the better the generaliza-
tion bound is. Interestingly, if ξ(p) = 0, our result degenerates to the classical
generalization bound of i.i.d data.
While it seems the success of our method will depend on the choice of Θ to
meet Condition 6, we will show RSC is applicable in general by presenting it
forces the empirical counterpart ξ(cid:98)(p) to be small. ξ(cid:98)(p) is defined as
ξ(cid:98)(p):=|h(θ(cid:98)RSC,z)−h(θ(cid:98)RSC,z˜)|,
8 Huang et al.
where the function h(·,·) is defined as
(cid:88)
h(θ(cid:98)RSC,z)= l(f(z;θ(cid:98)RSC);y). (7)
(z,y)∼S
We will show ξ(cid:98)(p) decreases at every iteration with more assumptions:
A4: Discarding the most predictive features will increase the loss at current
iteration.
A5: The learning rate η is sufficiently small (η2 or higher order terms are neg-
ligible).
Formally,
Corollary 2. If Assumption A4 holds, we can simply denote
h(θ(cid:98)RSC(t),z˜ t)=γ t(p)h(θ(cid:98)RSC(t),z t),
where h(·
 15.3
PersonaChat 11k 164k 14.8 14.2
two separate rounds of validation after collect-
WizardofWikipedia 22k 202k 9.1 16.4
ing the dialogues. We ask three workers per di- EmpatheticDialogues 25k 107k 4.3 13.7
BlendedSkillTalk 7k 76k 11.2 13.6
alogue to report any incoherent utterances or ac-
MoralIntegrityCorpus 38k 76k 2.0 22.3
cusatory/harsh/rudefeedback. Were-annotatedia-
PROSOCIALDIALOG 58k 331k 5.7 20.0
loguesiftheyarereportedbyoneormoreworkers
toensuredataquality.3 Table1:StatisticsofPROSOCIALDIALOGcomparedto
other dialogue datasets. Utt. denotes utterance. Brief
3.3 CollectingDialogueSafetyLabels descriptionforeachdatasetisinAppendixE.
As a final step, we collect dialogue safety labels
todeterminewhentheagentshouldgiveconstruc- Positive Ambiguous Negative
tive feedback. Given a dialogue context, we ask DailyDialog
Topical-Chat
three annotators to categorize the utterance(s) by
Holl-E
the machine interlocutor (i.e., GPT-3) into three
PersonaChat
classes: CASUAL,NEEDS CAUTION,andNEEDS Wizard of Wikipedia
INTERVENTION (seedetailsin§2.3). Wealsoask EmpatheticDialogues
BlendedSkillTalk
workerstowriteaone-sentencerationalefortheir
ProsocialDialogue
judgment,inordertoenrichourannotationswith
0% 20% 40% 60% 80% 100%
explanationsofwhysomethingmightneedcaution
(e.g.,“Speakerdoesn’thaveagoodreasonforbor- Figure 3: Ratio of positive, ambiguous, and nega-
rowingthecaranddisappearing.”). Unfortunately, tive utterances in large-scale dialogue datasets and
classification labels wash away the implications
our PROSOCIALDIALOG, measured by the pretrained
BERTsentimentclassifierfromDemszkyetal.(2020
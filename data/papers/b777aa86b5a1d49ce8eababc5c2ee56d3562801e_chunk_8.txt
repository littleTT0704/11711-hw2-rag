). Smallcom-
pute kernels occur in inference at lower batch sizes. Figure 3: Framework overhead of BERT-Base in Py-
Boxeswithdashedlinesrepresentframeworkoverhead. Torch for various batch sizes on an RTX-8000. Al-
thoughsmallbatchsizesrequirefewerFLOPs,thesere-
ductionsdonottranslatetospeedupsinlatency. Frame-
ONNX Runtime, TensorFlow 1.0, MXNet, Ten- workoverheadissubstantialatlowbatchsizesbutonly
sorRT,andTVM(Chenetal.,2015,2018). asmallconstantatlargebatchsizes.
Eagerexecutionframeworkscomputeeachlayer
asseparateoperationswhicheachincuradditional
SeeFigure3(Aminabadietal.,2022).
overhead from CPU kernel dispatches. To accel-
Kernelserializationwithoptimizationssuchas
erate execution, deferred execution and static in-
CUDAGraphsremovetheoverheadfrommultiple
ferenceframeworkscompiletheneuralnetwork’s
kernel dispatches by capturing and replaying the
computational graph to combine multiple opera-
entire computational graph as a single operation.
tions together (e.g. fusing attention layers and
However,kernelserializationrequiresthatmodels
GELU activations) or remove unnecessary com-
are graph safe (i.e. static shapes and static con-
putation (i.e. scalar folding). Removal of kernel
trolflow). ThiscanposechallengesforNLPsys-
launchoperationscanreducethememoryfootprint
temswhichoftenleveragedynamiccomputational
andresultinmoreefficientkernels,thusdecreasing
graphstodealwithvariablelengthsequences,parse
overallframeworkoverhead.
treedepths,andbatchsizes(Looksetal.,2017).
Whiledeferredandstaticframeworksarecom-
WhentheexecutionofGPUkernelcomputation
monly used in deployment settings, the NLP re-
islargelyblockedbyCPUframeworkoperations
search community relies heavily on eager mode
such as kernel dispatches, the model’s execution
frameworksduringthedevelopmentofnewmod-
bec
abasis
fortheAlexaMeaningRepresentationLanguage.Someal-
sports domains. The third benefit is that more complex re-
ternativesemanticrepresentationsincludeFrameNet(Baker,
questscanbesupported.Forexample,“playhungergames
Fillmore,andLowe1998),whichisasemanticrepresenta-
andturnthelightsdownto3”isnoteasilysupportedexisting
tionthatrepresentsanutterance,alongwithverbroles.Other
SLUrepresentationsinceitinvolvesmultipledomainsand
approacheshavearepresentationsimilartoAMRLincluding
mustalsosupportsequentialactions.
lambda-DCS(Liang2013)andcombinatorycategorialgram-
ThispaperpresentsmodelsthatcanpredictAMRLgiven mars(CCG)(SteedmanandBaldridge2011).AMRisarep-
anaturallanguageutterance.Inordertoaddressthelimited resentationthathasahierarchicalgraph-basedapproachthat
availability of AMRL data, a simple linearization scheme iswell-suitedforlongertexts(Banarescuandothers2013;
hasbeendeveloped(Figure4).ThelinearizedAMRLformat KevinKnight2017).Otherapproachesfocusmoreonsyntax
factorsintothreecomponents:intents,types,andproperties, thansemantics,suchasuniversaldependencies(Nivreand
whicharepredictedusingadeepneuralnetworkarchitecture others2016).Unliketheseapproaches,AMRLfocuseson
that is trained using multi-task learning. Each layer of the directlysupportingspokenlanguageunderstandingandcon-
modelpredictsacomponentofthelinearizedAMRLrepre- tainsfine-grainedtypesalongwithactions,verbroles,and
sentation.Byorderingthesetasksfromcoarsetofine,each properties.Anoverviewofothersemanticrepresentationsis
subsequentlayerisabletoreuserepresentationsfromprevi- coveredin(AbendandRappoport2017).
ouslayers.Thesimplestmodelconsistsofaword-embedding DNNsarewidelyusedforsequencelabeling.(Shimaokaet
layerandthreebi-directionalLST
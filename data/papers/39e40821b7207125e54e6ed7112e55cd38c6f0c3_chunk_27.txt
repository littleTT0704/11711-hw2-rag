 Reimers and Iryna Gurevych. 2019. Sentence-
pre-trained encoder-decoder models for code un-
BERT:SentenceembeddingsusingSiameseBERT-
derstanding and generation. arXiv preprint
networks. InProceedingsofthe2019Conferenceon
arXiv:2109.00859.
EmpiricalMethodsinNaturalLanguageProcessing
andthe9thInternationalJointConferenceonNatu-
ralLanguageProcessing(EMNLP-IJCNLP),pages Lilian D. A. Wanzare, Alessandra Zarcone, Stefan
3982–3992, Hong Kong, China. Association for Thater,andManfredPinkal.2016. Acrowdsourced
ComputationalLinguistics. database of event sequence descriptions for the ac-
quisition of high-quality script knowledge. In Pro-
Ohad Rubin, Jonathan Herzig, and Jonathan Be- ceedings of the Tenth International Conference on
rant. 2021. Learning To Retrieve Prompts for In- Language Resources and Evaluation (LREC’16),
Context Learning. arXiv:2112.08633 [cs]. ArXiv: pages 3494–3501, Portorož, Slovenia. European
2112.08633. LanguageResourcesAssociation(ELRA).
Yuhuai Wu, Albert Q Jiang, Wenda Li, Markus N
Rabe, CharlesStaats, MatejaJamnik, andChristian
Szegedy. 2022. Autoformalization with large lan-
guagemodels. arXivpreprintarXiv:2205.12615.
Frank F Xu, Uri Alon, Graham Neubig, and Vin-
cent J Hellendoorn. 2022. A systematic evaluation
of large language models of code. arXiv preprint
arXiv:2202.13169.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger,andYoavArtzi.2020. Bertscore: Eval-
uating text generation with BERT. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020.OpenReview.net.
Shuyan Zhou,
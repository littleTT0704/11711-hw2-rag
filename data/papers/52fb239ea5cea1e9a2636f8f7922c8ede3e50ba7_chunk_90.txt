 benchmark for general-purpose language understanding systems. In
Advances in Neural Information Processing Systems, pages 3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
SamuelRBowman.2018. Glue: Amulti-taskbenchmarkandanalysisplatform
for natural language understanding. arXiv preprint arXiv:1804.07461.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Ko-
rdi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, et al. 2022. Benchmarking general-
ization via in-context instructions on 1,600+ language tasks. arXiv preprint
arXiv:2204.07705.
JasonWei, MaartenBosma, VincentYZhao, KelvinGuu, AdamsWeiYu, Brian
Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language
models are zero-shot learners. arXiv preprint arXiv:2109.01652.
Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi,
and Kyunghyun Cho. 2021. Naturalproofs: Mathematical theorem proving in
naturallanguage. InThirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1).
Sean Welleck, Peter West, Jize Cao, and Yejin Choi. 2022. Symbolic brittleness
in sequence models: on systematic generalization in symbolic mathematics. In
AAAI.
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart
van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-
complete question answering: A set of prerequisite toy tasks. arXiv preprint
arXiv:1502.05698.
Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Baker Grosse. 2021. {INT}:
An inequality benchmark for evaluating generalization in theorem proving. In
International Conference on Learning Representations
2.9
hoursforFEVER,7hoursforzsRE,and12.5hours
LeapOfThought, which we train for 10 epochs
forWikidata5m. Computingupdatestatisticswith
givenitssmallersize. Thelearnedoptimizersare
theoff-the-shelfoptimizerswithr = 1takes4min-
also selected basedon dev set performance, with
utesforLeapOfThought,30minutesforFEVER,
checkpointingaftereachtrainingepoch. Theirse-
2.3hoursforzsRE,and3.9hoursforWikidata5m.
lectioncriterionisarawaverageofUpdateSuccess
With r = 10, the baselines require 1 minute for
Rate(averagedovereachkindofdata),RetainRate
LeapOfThought,15minutesforFEVER,54min-
(Local Neutral) and ∆-Acc, with terms dropped
utesforzsRE,and1.8hoursforWikidata5m. Total
whentheycannotbecomputedgiventheavailable
runtimesforeachexperimentshouldtakeintoac-
data. Note that dev epochs with zsRE and Wiki-
count multiple conditions and multiple seeds of
data5marefairlyslow,soinordertospeedupour
eachmodelbeingrun.
experimentswecomputedevepochswithasubset
B.2 HyperparametersandObjectiveTerms. of4000devpoints.
Learnedoptimizer. Wegivethefinalhyperparam-
Traininghyperparameters. WefitourRoBERTa-
eterandobjectivetermsusedineachexperimentin
base and BART-base task models to their respec-
Table10. Ourobjectiveablationisgivenin17,and
tivedatasetswiththefollowinghyperparameters:
we select the best performing condition for each
We train for 10 epochs on the binary tasks, and
datasetaccordingtodevsetperformance,usingthe
20forthesequence-to-sequencetasks. Whenpre-
same selection criterion outlined previously. We
dicting with BART-base, we use a beam search
keep all weight coefficients λ equal rather than
withwidth5.
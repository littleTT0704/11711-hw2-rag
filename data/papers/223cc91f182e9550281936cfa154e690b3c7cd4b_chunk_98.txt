10.1631/FITEE.2200297
↩
Mintz, M., Bills, S., Snow, R., & Jurafsky, D. (2009). Distant supervision for relation extraction without
labeled data. In K.-Y. Su, J. Su, J. Wiebe, & H. Li (Eds.), Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of
the AFNLP (pp. 1003–1011). https://aclanthology.org/P09-1113.pdf
↩
Mnih, A., & Gregor, K. (2014). Neural variational inference and learning in belief networks. In E. P. Xing &
T. Jebara (Eds.), Proceedings of the 31st International Conference on International Conference on Machine
Learning - Volume 32 (pp. 1791–1799). https://www.cs.toronto.edu/~amnih/papers/nvil.pdf
67
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
↩
Mohamed, S., & Lakshminarayanan, B. (2016). Learning in implicit generative models. arXiv.
https://doi.org/10.48550/arXiv.1610.03483 ↩
Neal, R. M. (1992). Bayesian training of backpropagation networks by the hybrid Monte Carlo method
(tech. rep.). University of Toronto. https://www.cs.toronto.edu/~radford/ftp/bbp.pdf
↩
Neal, R. M., & Hinton, G. E. (1998). A view of the EM algorithm that justifies incremental, sparse, and
other variants. In Learning in graphical models (pp. 355–368). Springer. https://doi.org/10.1007/978-94-011-
5014-9_12 ↩
Norouzi, M., Bengio, S., Chen, z., Jaitly, N., Schuster, M., Wu, Y., Schuurmans, D. (2016). Reward
augmented maximum likelihood for neural structured prediction. In D. Lee, M. Sugiyama, U. Luxburg, I.
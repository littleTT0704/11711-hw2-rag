(v,t )
attentionlayerswithlearnableparametersθ.Ittakesvideo i i
m
featuresx ∈ Rm×d andtextfeaturesy ∈ Rn×d fromtwo apositivelabel(1)andotherpairsanegativelabel(0).
separate modalities as inputs and output the (m+n) fea-
3.3.TACo: ourapproach
turesz = {z,...,z } ∈ R(m+n)×d. Tohelpittodis-
1 (m+n)
tinguishthevideoandlanguagetokens,weuseatokentype
Thewayofusingcontrastivelearninginpreviousworks
embeddinglayertolearntwoembeddingsandaddthemto
has two issues. First, the loss is computed at sentence-
thevisualandtextualtokens,separately. Similartooriginal
level by taking ‘[CLS]’ token [14] or the maximum over
Transformer[47],weincludeapositionalembeddinglayer
all tokens [34] in a sentence. Clearly, the content words
toencodetheabsolutetokenpositionsintheinputsequence. (e.g.,nouns,verbs)aremorelikelytoalignwiththevisual
The above three components comprise our video-text contentsorconceptsinthevideoscomparedwithfunction
alignment model which is then trained with the proposed words (e.g., stop words). Second, the high computational
token-awarecascadecontrastiveloss. Westartwithabrief costinmulti-modalfusionlayershindertheusageoflarge
reviewofconventionalcontrastivelearningandthenintro- batch of negative samples, which however is essential to
ducetheproposedtechnique. contrastivelearning[34,17,7]. Motivatedbythesetwois-
sues,weintroduceTACo,asimpleyeteffectivemethodto
improvethecontrastivelearning. Weelaboratebelowhow
3.2.Contrastivelearning: arevisit
thesecontrastivelossesarecomputed.
GivenasetofN video-textpairs{(v,t )}N,ourgoal Given the K video-text pairs {
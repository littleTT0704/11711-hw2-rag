5 hctiP
htnySN
h5
hctiP
htnySN
h5
sdnammoc
hceepS
h5
sdnammoc
hceepS
lluf
sdnammoc
hceepS
lluf
sdnammoc
hceepS
noitatimI
lacoV
noitatimI
lacoV
01
pot 701augniLxoV
01
pot
701augniLxoV
HEAR: Holistic Evaluation of Audio Representations
Speech As we move into the speech domain, LibriCount and Vocal Imitations have the
most similarity to CREMA-D emotion detection, which then is most similar to VoxLin-
gua107 Top 10 language identification, which in turn is correlated with Speech Commands,
following a trend from “environmental” to paralinguistic to semantic. The strong speech
models do the best on these tasks.
What is most interesting about our diverse survey of 29 models × 19 tasks is, perhaps,
the most difficult to explain results: tasks that defy neat categorization suggest the fragile,
unpredictable boundaries of existing models. DCASE 2016 Task 2 seems a priori similar
to FSD50K and ESC-50, but not in practice. Vocal Imitations are human-depictions of
all kinds of sounds. Gunshot Triangulation is an extremely low-resource task with only 88
instances. Beijing Opera and Mridingham Stroke and Tonic are non-Western music tasks.
For these tasks, our contribution is a negative result: we have no simple story or obvious
pretraining data to attack them. Robust generalization of >10-billion-parameter models
from NLP (Brown et al., 2020) and vision (Goyal et al., 2022) suggest one path forward.
6. Conclusion
General-purpose models that transfer to few-shot and zero-shot scenarios are highly desir-
able. The audio community has followed the NLP and vision communities in using increas-
ingly sophisticated representation learning approaches. The HEAR benchmark allows the
audio community also to follow the trend of broad-scale reproducible evaluation.
HEAR is aboutopenness. Thedatasets and thesubmissionsareas open as possible. All
HEAR datasets
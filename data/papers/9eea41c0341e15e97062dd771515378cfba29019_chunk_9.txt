 shifting. The external dataset we consider is
time,fromtoptobottom. Thischoiceofmodelalso theGrammarlyYahooAnswersFormalityCorpus
imposestheconstraintthattheoutputisthesame (GYAFC)(RaoandTetreault,2018): adatasetof
numberoflinesastheinput,asdesired. However, approximately100,000linesfromYahooAnswers
the dialogues frequently contain cataphora, espe- andformalrephrasingsofeachline.
cially in the start of the conversations, where the Our core method is the BART model trained
firstspeakermaybeaddressingasecondspeaker under the left and right context formulation (PS
whohasnotyetspoken. Forinstance,intheexam- ONLY).
ple“Hannah: Hey,doyouhaveBetty’snumber?”,
We also consider a heuristic baseline (RULES-
thisisthefirstutteranceofthedialogue. Amodel
BASED HEURISTIC). For each message, we
withonlyleftcontextcannotresolvetheword“you”
prependthespeaker’snameandtheword“says”to
hereanybetterthanthenocontextmodel.
theutterance. Wereplaceeachinstanceofthepro-
Theleftandrightcontextmodeladdressesthis noun“I”inthemessagewiththespeaker’sname.
concernbyprovidingthefullconversationasinput, After observing that most messages are not well-
butrestrictingtheoutputgenerationtoaperspective punctuated,wealsoappendaperiodtotheendof
shiftforasingle(marked)utterance. Thisimposes eachutterance. Whilethisheuristicissimpleand
theoutputlengthconstraintwithoutsacrificingcon- ignoresmanypronounresolutionconflicts, ithas
textualinformation. Thismodelperformsbeston theclearadvantageofbeinghighlyefficient.
all4metrics. Asthescoresforleftandrightcon- We incorporate the GYAFC corpus as part of
our training regime by finetuning on the formal- topreviousutterances;knowledgeofotherspeak-
izationtaskpriortofinetuningon
,·) is defined in Equation 7. γ (p) is an arbitrary number greater than
t
1, also a function of RSC’s hyperparameter p. Also, if Assumption A5 holds, we
have:
1
Γ(θ(cid:98)RSC(t+1))=Γ(θ(cid:98)RSC(t))−(1−
γ
(p))||g˜||2 2η
t
where
Γ(θ(cid:98)RSC(t)):=|h(θ(cid:98)RSC(t),z t)−h(θ(cid:98)RSC(t),z˜ t)|
t denotes the iteration, z (or z˜ ) denotes the features (or perturbed features) at
t t
iteration t, and g˜ =∂h(θ(cid:98)RSC(t),z˜ t)/∂θ(cid:98)RSC(t)
Notice that ξ(cid:98)(p) = Γ(θ(cid:98)RSC),
where θ(cid:98)RSC is θ(cid:98)RSC(t) at the last it-
eration t. We can show that ξ(cid:98)(p) is
a small number because Γ(θ(cid:98)RSC(t))
gets smaller at every iteration. This
discussion is also verified empirically,
as shown in Figure 2.
ThedecreasingspeedofΓ(θ(cid:98)RSC(t))
depends on the scalar γ (p): the
t
greater γ t(p) is, the faster Γ(θ(cid:98)RSC(t)) Fig.2. Γ(θ(cid:98)RSC(t)), i.e., “Loss Difference”,
descends. Further, intuitively, the plotted for the PACS experiment (de-
scale of γ (p) is highly related to the tails of the experiment setup will be dis-
t
mechanism
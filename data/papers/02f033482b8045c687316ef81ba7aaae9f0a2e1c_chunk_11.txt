endixEforexamplesofgenerations. 204.8K,1.024M,5.12M,and10.24Mtokens;for
each dataset size, we train the expert and anti-
3.3 SteeringGPT-3
expertforoneepochwithcheckpointsateveryfifth
Wenextuse DEXPERTS tosteerGPT-3Ada. Be- ofanepoch. Theperformanceofeachensemble,at
causetheOpenAIAPI6 allowsaccesstoonlythe every(anti-)expertcheckpoint,isshowinFigure3.
top100logprobabilitiesateachtimestep,wecan Wecanseethatevenwithadatasetof40,960to-
onlymodifyandsamplefromtheprobabilitydis- kens(„650comments)correspondingtoă 0.4%
tribution over the top 100 tokens. Nonetheless, of the original toxic dataset, we substantially re-
resultsinTable2showthatDEXPERTSeffectively duce toxicity from the base model to about the
reduces toxicity from GPT-3 to about the same same level as our strongest baseline, GeDi. (On
level as when operating on GPT-2. This demon- oneGPU,thiscorrespondsto„3minutesoffine-
stratesthat DEXPERTSrequiresonlytheoutputof tuning.) Nonetheless, as the size of the finetun-
the base model, and indeed, the (anti-)experts do ingdatasetfor(anti-)expertsincreases,theperfor-
notneedtobebuiltonthebasemodel. manceof DEXPERTSincreasesaswell.
3.4 Analysis: DatasetSize 4 Sentiment-ControlledGeneration
Inpractice,gatheringlargeamountsoftoxicdata
As a second application we consider the well-
may be challenging, especially in applications
studied task of controlling the polarity of text’s
wherewewouldwanttocustomizetheanti-expert
sentiment (e.g., Li et al., 2018; Sudhakar et al.,
LMfordifferingnotionsofharmfullanguage. To
2019),steeringtowardseitherpositiveornegative
explore the limited data setting,
of learning rates (LR) for both stages. We look at either reorderedtextpreservestheplotoftheoriginal.Thisincludes
the loss (for BART and T5) or perplexity (for GPT-2) on details about characters, events, and interactions between
therespectivevalidationsplits(devUnsupfor1ststageand them,encompassingitssemanticandtemporalaspects.
devSupfor2nd),andchoosetheepochwiththelowest. We also conduct an interestingness (interest) study on
WeevaluateeachmodelontestSup,wherewecandirectly humanrewritingsandoutputsfromourBART-2SandT5-2S
compareresultstoNAREORC’shumanrewritings.Wegen- models.Eachreorderedstory’sinterestingnessw.r.t.suspense
erate a single output per test example. The inputs are the andtimeflowcomparedtotheoriginalareevaluatedfrom
original examples to NAR-r models and the S′ of the 1-5 by two annotators. We ask the following: “On a scale
naive
examplestoNAR-dmodels.See§3.1formoredetails. of1-5,with1beingmostdecreaseininterestingnessand3
Weonlykeepthefirstfivesentencesofeachoutput.For beingsamelevelofinterestingnessand5beingmostincrease
BARTandT5,weusebeamsearchwithawidthof5.7 For ininterestingness,howinterestingisthesuspenseandflow
GPT-2,weuseanucleussamplingbudget(Holtzmanetal. oftimeinthestoryS,comparedtotheoriginalstoryO?How
2019)of0.9andoutputlengthlimitof500.Wetryvarious excitingdidyoufindthestoryasyoureadthroughit?”
softmaxtemperaturesandfind0.9performsbest.ForGPT-2,
duringfinetuning,itisgiventheconcatenationoftheinput 5 ResultsandAnalysis
plus output. During generation, it is only fed the input for
We present evaluation results of our 2S and subset of 1S
whichitgeneratesacontinuation(theoutput).Wenoticed
modelsont
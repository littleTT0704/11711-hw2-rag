Simon,and
neuralMT. InProceedingsofthe2ndWorkshopon
MohitIyyer.2021. STraTA:Self-trainingwithtask
DeepLearningApproachesforLow-ResourceNLP
augmentation for better few-shot learning. In Pro-
(DeepLo 2019), pages 84–93, Hong Kong, China.
ceedingsofthe2021ConferenceonEmpiricalMeth-
AssociationforComputationalLinguistics.
ods in Natural Language Processing, pages 5715–
5731,OnlineandPuntaCana,DominicanRepublic.
HuaZhu,WuYe,SihanLuo,andXidongZhang.2020.
AssociationforComputationalLinguistics.
A multitask active learning framework for natural
languageunderstanding. InProceedingsofthe28th
ChristopherWalker,StephanieStrassel,JulieMedero,
InternationalConferenceonComputationalLinguis-
andKazuakiMaeda.2006. ACE2005multilingual
tics, pages 4900–4914, Barcelona, Spain (Online).
trainingcorpus. LinguisticDataConsortium,57.
InternationalCommitteeonComputationalLinguis-
tics.
Chenguang Wang, Laura Chiticariu, and Yunyao Li.
2017. Activelearningforblack-boxsemanticrole
labelingwithneuralfactors. InIJCAI.
Xinyu Wang, Yong Jiang, Zhaohui Yan, Zixia Jia,
Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei
Huang,andKeweiTu.2021. Structuralknowledge
distillation: Tractablydistillinginformationforstruc-
turedpredictor. InProceedingsofthe59thAnnual
Meeting of the Association for Computational Lin-
guisticsandthe11thInternationalJointConference
onNaturalLanguageProcessing(Volume1: Long
Papers), pages 550–564, Online. Association for
ComputationalLinguistics.
DittayaWanvarie,HiroyaTakamura,andManabuOku-
mura.2011. Activelearningwithsubsequencesam-
plingstrategyforsequencelabeling
 our implementation of the source expansion approach is quite comprehen-
sive, it may not be necessary to fully replicate it in order to achieve similar results.
Most of the development effort was invested in the component that estimates the rel-
evance of text using a statistical model. For this purpose we created a large dataset
of over 160,000 hand-labeled text nuggets, and we developed a diverse set of 27 rele-
vance and transition features. Fortunately, instead of fitting a statistical model, one
could get started quickly by using a single topicality feature to rank text by relevance,
such as a likelihood ratio estimated with topic and background language models or
cosine similarities. We have shown that the relevance estimation performance of these
features comes reasonably close to the full model if the seed documents are long and
9.3. FUTURE RESEARCH 149
of high quality, and the cosine similarity baseline also yields large gains in QA search
performance when used to expand Wikipedia articles. Apart from not having to im-
plement the other relevance features, a single-strategy approach has the advantage of
not requiring a labeled dataset to estimate feature weights. However, we have also
seen that the topicality features are only effective if enough relevant seed content is
available, and that the cosine similarity approach is less suitable for expanding Wik-
tionary entries. If the seeds are short or noisy, we recommend using a more robust
relevance model that also leverages search-based features and surface characteristics
of the extracted text. In that case, it is unavoidable to implement a larger feature set
and to fit a model to hand-labeled data, but we can reduce the manual annotation
costs considerably by using active learning instead of training a supervised model.
Our method is broadly applicable since for many knowledge domains suitable
seed corpora are available. For instance, there exist encyclopedias and dictionaries
that could be used as a starting point when developing systems for medical, legal
or financial QA tasks. The seed corpus should at least touch on the topics that are
central to a given domain, but it could be quite small and important information
may be missing or hard to find. Preferably the seeds should be topic-oriented, but
we have also suggested an approach for compiling pseudo-documents about topics
discovered in an unstructured source. Note that the seeds may be about any topics
that can
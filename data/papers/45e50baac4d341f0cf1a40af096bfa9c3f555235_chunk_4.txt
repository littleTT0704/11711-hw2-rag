tolowprecisionduringinference. Tothisend,in losingitslanguagecapabilities. Thisisdefinedas
Section 2.1 we present the datasets of the Bias theLanguageModel(LM)Score.
Benchbenchmark(Meadeetal.,2022)thatenable
us to evaluate three different language modeling SEAT evaluates biases in sentences. A SEAT
tasks across the three social bias categories. In task is defined by two sets of attribute sentences,
Section2.2welayoutthemodelswestudied. We andtwoothersetsoftargetsentences. Theobjec-
expand on the Bias Bench original evaluation by tive of the task is to measure the distance of the
looking at the Large versions of the BERT and sentenceembeddingsbetweentheattributeandtar-
RoBERTa models, and the Pythia family of au- get sets to assess a preference between attributes
toregressive models. The chosen models cover andtargets(bias). Weprovidemoredetailofthis
differentlanguagemodelingtasksandspanacross formulationinAppendixA.1.
a wide range of parameter sizes, thus providing
2.2 Models
a comprehensive view of the variations of social
bias. In this work, we focus on two popular methods
formodelcompression: knowledgedistillationand
2.1 MeasuringBias quantization. Wechoosethesetwomethodsgiven
their competitive performance, wide deployment
WeusetheBiasBenchbenchmarkforevaluating
giventheavailabilityofdistributionsundertheHug-
markersofsocialbiasinLLMs. BiasBenchcom-
gingFace and Pytorch libraries, and the lack of
piles three datasets, CrowS-Pairs (Nangia et al.,
understanding of the impact of these methods on
2020),StereoSet(SS)(Nadeemetal.,2021),and
socialbiases. Weleavethestudyofmoreelaborate
SEAT(KanekoandBollegala,2021),formeasur-
methods for improving model efficiency such as
ingintrinsicbiasacrossthreedifferentidentitycate-
pruning (Chen et al., 2020), mixtures of experts
gories: GENDER
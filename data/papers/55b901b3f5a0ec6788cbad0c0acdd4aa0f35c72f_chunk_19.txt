RoBERTawastrainedfromscratchona
similarobjectiveandsothisobjectiverepresentsminimalshiftintrainingdistributions.
Adaptivemulti-taskauxiliarylearningimprovesperformance: Wecomparesingle-taskend-
taskawareauxiliarylearningtoitsmultitaskvariant. Table2showsthatmultitaskingour3different
types of language modelling tasks results in improved average performance over using the tasks
individually(81.12%fortheBERT-styleand81.55%forcombiningthethreesingletaskobjectives).
Wegetourbestperformancewhenwemultitask24auxiliaryobjectivesautomaticallygenerated
withourframeworkusingAANG-TD.Boostingthenumberofobjectivesfrom3to24resultedin
a0.66%improvementinaverageperformanceacrosstasks. ThisisinlinewithPrescription(P )
2
fromSection4sinceweareincreasingtheeffectiveamountofauxiliarydata. Wefurtherpositthat
introducingmoreauxiliaryobjectivesalsoservestoimplicitlyregularizetheend-taskduringtraining.
7
PublishedasaconferencepaperatICLR2023
Table2: OurframeworkandAANGontasksusingonlytaskdata. Withoutusinganyexternaldata,
weareabletogetsignificantaverageperformanceimprovementoverbaselines. Superscriptsare
p-valuesfrompairedt-tests(bestmultitaskversusbestsingle-task).
TaskAdaptive Method # CS BIOMED NEWS STANCE
ACL-ARC SCIERC CHEMPROT H.PARTISAN SE-2016-6 AVG
No RoBERTa 1 66.033.55 77.962.96 82.100.98 93.392.26 70.371.51 77.97
TAPT 1 67.743.68 79.531.93 82.170.65 93.422.87 70.741.21 78.72
[OURS]StaticMultitask-TD 24 69.603.80 83.370.58 83.420.26 97.950.73 71.020.43 81.07
Yes X.GPT-style 1 67.220.44 81.620.84 83.291.21 96.410.73 70.671.46 79.84
Y.

11languages. Weusethegoldpassageversionof
Discussion One significant advantage of multi-
thetask(GoldP),whichcoversninelanguages.
lingualpretrainedmodelsisthatoncethepretrain-
ing is complete, we can fine-tune the model to a
3.2 DetailsofHardwareandMeasurements
variety of tasks and languages under different re-
WeuseasingleTeslaV100(32GB)GPUforallex-
sourcesettings. Inthiscontext,itisalsoessentialto
perimentsregardinginferencecostmeasurements.
1https://www.tensorflow.org/datasets/catalog/ ToobtainthepeakGPUmemoryandinferencela-
c4#c4multilingual tency, we randomly select 100 samples from the
2WemainlyconsidertheByT5smallmodeltokeepinfer-
Englishtestsetforeachtaskandmeasuretheaver-
encecostrelativelyconstantandfitthemodelinthehardware
thatweeasilyhaveaccessto. agecostofpredictingoneexampleatatime.
mBERT(Multi) CANINE-S(Multi) mT5(Multi) ByT5(Multi)
mBERT(Single) CANINE-S(Single) mT5(Single) ByT5(Single)
mBERT(Zero) CANINE-S(Zero) mT5(Zero) ByT5(Zero)
XNLI NER TyDiQA
100 100 100
90 90 90
80 80
80 70
70
70 60
60
50
60 50 40
50 40 30
0 250 500 750 1000 1250 0 250 500 750 1000 1250 0 500 1000 1500 2000
PeakGPUmemoryduringinference(mb)
XNLI NER TyDiQA
100 100 100
90 90 90
80 80
80 70
70
70 60
60
50
60 50 40
50 40 30
0 20 40 60 80 100 0 100 200 300 0 100 200 300
Averageinferencetimepersample(ms)
Figure 1: Average XNLI, NER, TyDi QA performance across languages when each model is fine-tuned with
multilingualdata(mBERT:(cid:110),CANINE-
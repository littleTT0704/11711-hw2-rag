model for learning multilingual text embeddings
proaches(ArtetxeandSchwenk,2019b;Yangetal.,
1Code and Flax-based T5X model checkpoint 2020). Wecarryouttheseexperimentswithboth
available at https://github.com/google-research/ pretrained and randomly initialized models. The
google-research/tree/master/vmsst.
comparisonofobjectivefunctionsisanimportant
2Wemeangenerativebothintermsoftextgenerationand
asastatisticalmodelofthejointprobabilitydistribution. researchquestionduetothelargeamountsofmulti-
3202
nuJ
4
]LC.sc[
2v62701.2122:viXra
lingualtextavailabletotrainmodelsandthemany Gaussian Prior Gaussian Prior
usesofthesemodelsindownstreamtasks. Tothat
ùí©(0,ùêº) ùí©(0,ùêº)
end,anothercontributionofthispaperisshowing
these comparisons and the surprising result that
Language-specific Common Semantic
contrastive objectives do not provide the overall Latent Vector Latent Vector
bestaccuracyondownstreamtasks. Moreover,our
Shared
generativeVMSST increasinglyoutperformsthe Linguistic variation z z semantic
specific to languages l sem
i variation
contrastivemodelwhenmorelayersareaddedand l 1 to lN
whentrainingwithlargerbatchesandmoretraining
N Language N Language Semantic
data, suggesting that as models continue to scale Inference Nets Decoders Inference Net
inthefuture, thisperformancegapmaycontinue
toincreasefurthermotivatingtheuseofgenerative
approaches for learning multilingual text embed- Observed Sentences x l
i
dings. N
2 RelatedWork
N-way Translations
Therehasbeenanumberofapproachesproposed Figure1: Thegenerativeprocessofourmodel. Latent
forlearningbilingualandmultilingualtextembed- variables, z, modeling the variation x specifically
li li
dings. Onepopularapproachiscontrastivelearn- duetolanguagel i,aswellasalatentvariablemodeling
thecommonsemantics,z,aredrawnfromamult
 to oversample points that fit this descrip-
ble 12 for each dataset. For comparison, De Cao
tionduringtrainingofthelearnedoptimizer,which
etal.(2021)useRMSPropwith100updatesteps.
allowsthelearnedoptimizertofullyfittotheen-
TheLRforzsREandWikidata5mmayseemquite
taileddata. Alsonotethatduringlearnedoptimizer
high,butthisistheconditionthatactuallydoesthe
training,weincludeEntailedDatafromotherdata
leastdamagetothemodel’saccuracyonotherdata,
pointsbesidestheMainInputintheKLterminEq.
∆-Acc. The baseline optimizes all of the train-
1,andwemeasure∆-AccusingbothMainInputs
ableparametersinthelanguagemodel,unlikethe
andEntailedData.
learnedoptimizerwhichoptimizesonlyattention
andfeedforwardweightsforpurposesofparameter C DatasetSourcesandNoise
efficiency.
Herewegivesourcesandlicensesforeachdataset,
and we document some shortcomings of each
B.3 Wikidata5mAdditionalDetails.
dataset,withreferencetoexamplesinTable15.
WeconstructfourparaphrasesperMainInputby
Datasetsourcesandlicenses. FEVERandzsRE
selectingfromasetofalternativephrasingsforthe
areavailablethroughtheKILT4 resourceandare
entityandrelationintheMainInput. Thesyntax
foreachparaphrasefollowsthesamesimpletem-
4https://github.com/
facebookresearch/KILT/?fbclid=
plateastheMainInput,incontrasttozsREwhere
IwAR2WiFkl-7KLIQAoNI9bJgBVKWgsAQEDV342vV5_
syntaxdiffersbetweenparaphrases. Acouplede- PcsKA881vpuXaELKBz0
2726
K Memory Usage byr
30
20
10
0
1 2 4 6 8 10
K r
Figure 4: Training memory usage in terms of K and
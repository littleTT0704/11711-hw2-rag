words.
theeye-trackingexperimentforeachexperimentalcondition.
Figure 2 shows the mean surprisal values across the dif-
The model estimates, for each element of a character se-
ferent error conditions. We note that the pattern of reading
quence,theprobabilityofseeingthischaractergiventhepre-
timepredictedbyCHARSURPRISAL(solidlines)matchesthe
ceding context. We compute the surprisal of a word as the
first-pass times observed experimentally very well (see Fig-
sum of the surprisals of the individual characters, as pre-
ure1),whileWORDSURPRISAL(dottedline)showsaclearly
scribed by the product rule of probability. For a word con-
divergent pattern, with error words showing lower surprisal
sisting of characters x...x following a context x...x,
t t+T 1 t−1 thannon-errorwords. Thiscanbeexplainedbythefactthata
itssurprisalis:
word-basedmodeldoesnotprocesserrorwordsbeyondrec-
ognizingthemasunknown;thepresenceofanunknownword
t+T
−logP(x...x |x...x )= ∑−logP(x|x...x ) (1) itselfisnotahigh-surprisalevent(evenwithouterrors,17%
t t+T 1 t−1 i 1 i−1
i=t
61024 units, batch size 128, embedding size 200, learning rate
51024units,3layers,batchsize128,embeddingsize200,learn- 0.2 with plain SGD, multiplied by 0.95 at the end of each epoch;
ingrate3.6withplainSGD,multipliedby0.95attheendofeach BPTT length 50; DropConnect with rate 0.2 for hidden units;
epoch; BPTT length 80; DropConnect with rate 0.01 for hidden Dropout 0.1 for input layer; replacing words by random samples
units;replacingentirecharacterembeddingsbyzerowithrate0.001. fromthevocabularywithrate0.01duringtraining.
5
No Error Error CHARSURPR WORDSURPR
(Intercept
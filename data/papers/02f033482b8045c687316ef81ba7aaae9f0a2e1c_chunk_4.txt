toxicitywithoutannotatedexamplesofnon-toxic givenby:2
content. Inanalysis,wealsoshowthatourmethod ` ` ˘˘
successfullyavoidstoxicdegenerationwhileusing P˜ pX t | x ătq “ softmax z t`α z` t ´z´ t
just „650 toxic comments, opening avenues for (2)
easilycustomizableanti-experts. where α is a hyperparameter that controls the
amount of modification to z, and can be inter-
Wethenshowcasethegeneralizabilityof DEX- t
preted as the strength of control over the base
PERTSbytacklingthetaskofcontrollingthesenti-
model. Equivalently,
mentofLMs’output(§4). Tothisend,wecombine
ˆ ˙
apretrainedLMwith(anti-)expertsmodelingpos- P`pX | x q α
itive and negative sentiment. As with language P˜ pX t | x ătq9PpX t | x ătq P´pXt
|
xăt
q
t ăt
detoxification, DEXPERTS outperforms existing
(3)
sentimentsteeringmethodsonbothautomaticand
Intuitively, a token will only have high proba-
human evaluations. Additionally, we show our bility if it has high probability under both P and
method is especially effective in the adversarial P`,andlowprobabilityunderP´. Wecaninter-
setting of steering negative prompts toward pos-
prettheratio
P`pXt|xătq
asascalingcoefficientfor
itive continuations, and vice versa. Finally, we P´pXt|xătq
each token, which is used to modify the original
demonstrateapreliminaryproof-of-conceptusing
probabilitypredictedforthattoken.
DEXPERTSforstylisticrewriting(§5).
Ourworkdemonstratestheeffectivenessoftun-
2.2 Samplingfrom DEXPERTS
ingsmallLMsontextwithdesirableandundesir- Samplingfluentoutputfromlanguagemodelscom-

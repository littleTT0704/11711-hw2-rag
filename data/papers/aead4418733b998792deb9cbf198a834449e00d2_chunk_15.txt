(x,f θ(x))=0,whereX˜ ={f 1◦···◦f k|f i ∈X}. 5 DepartingFurtherFromTraining
We evaluate k-compositionality with respect to addition,
Theprecedingexperimentsfoundproblemsthatwerenearby
using simple primitive functions and validation problems.
R R R to, or composed directly from, in-distribution examples. In
Asintegrationislinear, (f+g)= f+ g,composition-
thissection,wedeliberatelymovefromthemodel’straining
alitywithrespecttoadditionisareasonablerequirement.
distributiontoevaluateitsout-of-distributiongeneralization.
First, we study extrapolation to longer equation sizes than
Succeeding on simple primitives, failing on their sum.
those in its training distribution, and to integer ranges that
Wecollectsimpleprimitivesfromthecoefficientrobustness
are only sparsely covered in the training set. Then we use
experimentsthatthemodelsuccessfullyintegrates(coeff),
SAGGAtoexposeexoticfailuresandrevealproblemclasses
and successful exponents xc or x1/c, c ∈ [0,1000] (exp).
thatwerenotcoveredduringtraining.
Werandomlysample1000compoundequationsf +...+f
1 k
fork ∈{2,3,4}andevaluatethefailurerate.Table8shows Longer problems are more difficult. First, we use the
theresults.Addingtwoprimitivesgivesfailureratesof29% same data-generating process as for training, but vary its
and 85% for coefficient and exponent primitives, respec- parameterstodepartfromthetrainingdistribution.Specifi-
tively,despitefailing0%ofthetimeontheindividualprim- cally,wetestextrapolationonnumberofoperatornodesin
8633
Cluster1 Cluster2 Cluster3 Cluster4
119x −240x+2cos2x −100xx 158xx2 +611
132x −398x+2cos2x −149xx 256xx2 +191
136x −692x+2sin2x −151xx 332xx2 +559
Table
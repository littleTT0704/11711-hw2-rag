i-
ing (Hermann and Blunsom, 2014; Singla et al., sem
variateGaussianprior. Theobservedtranslationineach
2018;Guoetal.,2018;Wietingetal.,2019;Feng
language is then conditioned on its language-specific
et al., 2022) where translation pairs are positive
variable and z. In practice, we approximate this
sem
examplesandtextfromotherpairsareusedasneg-
modeltomakelearningandinferencetractable.
ativeexamples. Analternativeapproachistousea
neuralmachinetranslationobjective,wheretherep-
ure1andFigure2respectively. Inthegenerative
resentation from the hidden states of the encoder
storyforVMSST,wefirstsampleasemanticvari-
isusedasthesentenceembedding(Espana-Bonet
ablez forthesentence. ThenforeachoftheN
et al., 2017; Schwenk and Douze, 2017; Artetxe sem
languages,wesamplealanguage-specificvariable
andSchwenk, 2019b). Otherapproaches include
z. Eachlatentvariablez issampledfromamul-
multi-task learning approaches which often use li
tivariateGaussianpriorN(0,I ). Thesevariables
sometypeofcontrastivelearningofparalleltextto k
are then fed into a decoder that samples each of
alignrepresentationsamonglanguages(Yangetal.,
the N sentences in the translation set. Each ob-
2020;Goswamietal.,2021),cross-lingualpretrain-
served translation x, is sampled conditioned on
ing(Chietal.,2022),andmodeldistillationfroma li
z and its language variable z. Because z
largepretrainedmultilingualmodel(Reimersand sem li sem
willbeusedtogeneratethesampledsentencesin
Gurevych,2020).
alllanguages,weexpectthatthisvariablewillen-
Analternativeapproachthatismorecloselyre-
code semantic, syntactic, or stylistic information
latedtoourworkisgenerativemodelsthatseparate
thatissharedinallofthetranslations. Conversely,
the
student. Weinitializethe
coefficientssuchthatthemodelisinitializedtobethesameasthestaticattentionexplainer(i.e.,
performingthemeanoverallheads).
Independently of the T-explainer, we always use a learned attention-based explainer as the S-
explainer,consideringallheadsexceptwhentheT-explainerisastaticattentionexplainerthatonly
considersthelastlayers’heads,wherewedothesamefortheS-explainer. WeusetheKullback-
Leibler divergence as L, and we set β = 5 for attention-based explainers and β = 0.2 for
expl
gradient-based explainers (since we found smaller values to be better). We set L as the cross-
sim
entropy loss for classification tasks, and as the mean squared error loss for text regression. For
eachsetting,wetrainfivestudentswithdifferentseeds. Sincethereissomevarianceinstudents’
performance(wehypothesizeduetothesmalltrainingsets)wereportthemedianandinterquantile
range(IQR)aroundit(relativetothe25-75percentile).
3Whilescalarmixingreducedvarianceofstudentperformance,SMaTalsoworkedwithothercommon
poolingmethods.
5
.........
Table1: ResultsfortheIMDBdatasetwithrespecttostudentsimulabilityintermsofaccuracy(%).
Underlinedvaluesindicatehighersimulabilitythanbaselinewithnon-overlappingIQR.
500 1,000 2,000
NoExplainer 81.72[81.24:81.75] 83.44[83.36:83.63] 84.84[84.80:84.88]
GradientL2 81.66[81.32:82.00] 82.98[82.72:83.08] 84.78[84.96:85.08]
Gradient×Input 84.83[84.79:84.88] 81.15[80.95:81.36] 83.84[83.59:84.99]
IntegratedGradients 82.99[82.59:82.99] 81.79[81.
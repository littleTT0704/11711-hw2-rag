atleasttwicetoshowittotheuser,as
2023). We outline the algorithm in Algorithm 1
each token must be independently generated by
andhighlightthemaindifferencesfromtheorigi-
two consecutive contexts to be considered stable.
nalapproachusedinPoláketal.(2022)withred.
Dependingonthemodelarchitecture,thegenera-
tion might be the most expensive operation. Ad-
Algorithm 1: Incremental blockwise
ditionally,thesequence-to-sequencemodelstend
streamingbeamsearchalgorithmforincre-
tosufferfromexposurebias(i.e.,themodelisnot
mentalST
exposedtoitsownerrorsduringthetraining)(Ran-
Input :Alistofblocks,anSTmodel
Output:Asetofhypothesesandscores zatoetal.,2015;WisemanandRush,2016). The
1 Seen ;
←∅ exposurebiasthencausesalowertranslationqual-
2 foreachblockdo
3 EncodeblockusingtheSTmodel; ity,andsometimesleadstohallucinations(i.e.,gen-
4 Stopped ;
←∅
5 minScore ; erationofcoherentoutputnotpresentinthesource)
←−∞
6 while#activebeams>0andnotmax.lengthdo
7 Extendbeamsandcomputescores; (Leeetal.,2018;Mülleretal.,2019;Dongetal.,
8 foreachactivebeambdo 2020). Finally,attentionalencoder-decodermodels
9 ifbendswith<eos>or(score minScore
≤
andb /Seen) then are suspected to suffer from label bias (Hannun,
∈
10 minScore max(minScore,score);
11 Stopped ← Stopped b; 2020).
← ∪
12 Removebfromthebeamsearch;
Agoodcandidatetoaddresstheseproblemsis
13 end
14 end CTC (Graves et al., 2006). For each input frame,
15 end
16 Seen Seen Stopped; CTCpredictseitherablanktoken(i.e.,nooutput)
← ∪
17 SortStopped
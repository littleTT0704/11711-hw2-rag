strainingdataorlabels.Thisleadsstate-of-the-artmod-
studies how to transform various pre-existing knowl-
elsfromindividualtaskstofalter,sometimesbyasmuchas
edgeresourcesintoaformthatismosteffectiveforpre-
training models. We vary the set of language models, a50%decreaseinperformance(Shwartzetal.2020).
trainingregimes,knowledgesources,anddatagenera- Repositories of commonsense knowledge, like
tion strategies, and measure their impact across tasks. ConceptNet (Speer, Chin, and Havasi 2017) and
Extendingonpriorwork,wedeviseandcomparefour ATOMIC(Sapetal.2019a),canbebeneficialforcommon-
constrained distractor-sampling strategies. We provide senseQA,especiallywhenlittleornotrainingdataisavail-
empirical results across five commonsense question- able. Enriching the training data with ConceptNet and
answeringtaskswithdatageneratedfromfiveexternal ATOMIChasbeenshown(Maetal.2019;Mitraetal.2019)
knowledgeresources.Weshowthat,whileanindivid-
toimproveaccuracyondatasetsderivedfromthesegraphs:
ualknowledgegraphisbettersuitedforspecifictasks,a
CommonSenseQA (Talmor et al. 2019) and SocialIQA.
globalknowledgegraphbringsconsistentgainsacross
Knowledge bases (KBs) can be used to generate question-
differenttasks.Inaddition,bothpreservingthestructure
of the task as well as generating fair and informative answerpairsanddistractorsautomatically,inordertotesta
questionshelplanguagemodelslearnmoreeffectively. modelâ€™s reasoning ability (Petroni et al. 2019; Richardson
andSabharwal2019)orprovideadditionalsupervision(Ye
Introduction etal.2019;Yangetal.2020). WhileKBshavebeenshown
tohelpinazero-shottransfersettingrecently(Banerjeeand
Common sense is key to efficient communication in every-
Baral 2020), no comprehensive study exists on the relation
day situations, as
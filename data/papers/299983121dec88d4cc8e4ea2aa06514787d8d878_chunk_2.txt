thesecaptionstoenrichandsteerthegenerationpro-
cess.Comprehensiveevaluationandanalysisdemonstratethat
VisCTGnoticeablyimprovesmodelperformancewhilesuc-
cessfullyaddressingseveralissuesofthebaselinegenerations,
includingpoorcommonsense,fluency,andspecificity.
1 Introduction baseline:Acatislayingonabedandpettingit. baseline:Ariderjumpsoverafence.
capt:acatlayingonabedwithastuffedanimal capt:ahorseisjumpingoverawoodenfence
Transformer-basedmodelshaveseenincreasingpopularity VisCTG:Acatlayingonabedbeingpetted. VisCTG:Ariderjumpsafenceonahorse.
for NLP tasks and applications. This includes SOTA text Table1:Examplesofretrievedimages,captions,baselineand
generation models such as BART (Lewis et al. 2020) and VisCTG(ourmodelâ€™s)generations.Theimagesandcaptions
T5(Raffeletal.2020).Largercorporaandbetterpretrain- areusedasanintermediarytoguidethefinalgenerationand
inglossesaremajorreasonsdrivingthesegains.However, itneednotbefaithfultothem.E.g.nobodyispettingthecat
despiteincreasingattentiononthecommonsenseofmodels intheimage,butsincetheVisCTGoutputisconditionedon
throughworkslikeCOMET(Bosselutetal.2019),studies boththeconceptsetandcaption,itincludesbeingpetted.
haveshownthatevenlargepretrainedmodelsstillstruggle
with commonsense tasks that humans can reason through
very easily (Talmor et al. 2020). We believe that there is
bethoughtofasconcepts,orhigh-levelwordsorstructures,
commonsense information in other modalities like vision,
thatplayanimportantroleinthegeneratedtext.Multimodal
beyondwhatisreported(GordonandVanDurme2013)in
workhasseenincreasingpopularity,butitsexplorationfor
text,whichcanpossiblyaugmentcommonsenseandenhance
constrainedanddata-to-textNLGhasbeenlimited(Baltru-
decision-makingprocessesof
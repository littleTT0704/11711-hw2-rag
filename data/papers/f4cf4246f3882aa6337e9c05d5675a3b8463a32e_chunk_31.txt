. During vetting, annotators were paid
$0.35perHIT(HumanInteractionTask)tocompare5sets
of three directives each. These wages were set based on Network Architecture We use a pretrained ResNet-18
localminimum-wageratesandaveragecompletiontime. [22]toextract512×7×7featuresfromtheconv5layer.
Thesefeaturesarefedintoatwo-layerCNNwith1×1con-
A.4.VocabularyDistributions
volutionstoreducethechanneldimensionfrom512to64.
Figure F8 shows vocabulary statistics of the language The 64×7×7 output is flattened, and a fully-connected
inALFRED. layerproducesa2500-dimensionalvisualfeaturev t.
The language encoder is a bi-directional LSTM with a
A.5.DatasetExamples
hidden-dimension of 100. We do not use pretrained lan-
guagemodelstoinitializetheLSTM,andtheencodingsare
FigureF9shows7experttrajectories(onepertasktype)
learnedfromscratchinanend-to-endmanner. Wealsouse
andtheiraccompaniedannotations.
a self-attentionmechanism to attend overthe encodings to
AppendixB.ImplementationDetails initializethehidden-stateofthedecoderLSTM.
TheactiondecoderisanLSTMwithahidden-dimension
We describe implementation and training details of our of512.Theactorisafully-connectedlayerthatoutputslog-
baselineSequence-to-Sequencemodels. itsfor13actions. Themaskdecoderisathree-layerdecon-
volution network, which takes in the concatenated vector
Preprocessing We tokenize the language directives and u and transforms it into 64×7×7 features with a fully-
t
convertalltokenstolower-case. Duringdatasetgeneration, connectedlayer. Thesefeaturesaresubsequentlyup-scaled
wesaveimagesfromAI2-THOR300×300pixels,andlater intoa1×300×300binarymaskthroughthreelayersofde-
resize them to 224×224 during training. The generation convolutionsandup-samplingwithbi-linearinterpolation.
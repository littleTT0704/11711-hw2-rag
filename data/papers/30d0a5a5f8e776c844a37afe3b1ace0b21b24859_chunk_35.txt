searchfunctionstoiden-
tify the locations and physical appearance of objects
in the environment by class or id. The Follower is
Found a fork on the top
shelf. Whereâ€™s the sink? analogoustoanALFREDagent,butwithawiderac-
tionspacethatincludes,forexample,pouringliquids
Figure15. IntheTEAChTwo-AgentTaskCompletionchal- from one container to another. Further, object inter-
lenge,theCommanderhasoracletaskdetails(a),objectloca- actions are done via individual (x,y) coordinate pre-
tions(b),amap(c),andegocentricviewsfrombothagents, dictions,ratherthanthefullobjectmasksusedinAL-
butcannotactintheenvironment,onlycommunicate. The
FRED, analogous to the click inputs of human users
Follower carries out the task and asks questions (d). The
whoprovideddemonstrations. Theagentsbothhave
agentscanonlycommunicateviatext.
a communicate action that adds to a mutually-visible
dialoguehistory,andrequiresgeneratingtext.
TATC agents are evaluated via SR and SPL, simi-
16
lar to ALFRED agents. Rule-based, planning agents hundreds of millions to billions of steps. DD-PPO
forTATCachieveabout24%SR,withplanningcorner presented an on-policy RL method that has been
cases dominating failures. A learned Follower based used throughout embodied AI to train agents in a
on the Episodic Transformer [136] with a rule-based, distributed manner [202, 204]. It scaled PointNav
simple Commander that simply reports the raw text to train for billions of steps across 64 GPUs and
of the next task subgoal as a communication action showednear-perfectperformanceinPointNavinun-
achieves nearly 0%. We are eager to see whether seenenvironmentswithjustanRGB-Dcameraanda
mapping-based approaches like those succeeding at GPS+Compass sensor. For ObjectNav training, Proc-
ALFRED can adapt to the wider space of tasks and THOR similarly trained with DD-PPO for 420 mil-
environmentcornercasesinTEACh. lion steps, and was later fine-tuned for 195M steps

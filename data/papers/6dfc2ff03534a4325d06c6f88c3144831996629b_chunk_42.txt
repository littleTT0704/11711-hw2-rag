 in $8–
likelyanswersforeachimage. Thelikelihoodratingswere
25/hr. This proved necessary as workers reported feeling
neverusedforthetask,sincewefoundtheyweren’tneces-
sarytoobtainhighhumanagreement.
15Inadditiontopredictinginterestingness,themodelalsopredictsthe
numberofquestionsaworkerasks, butweneverendedupusingthese Instructions Like for any crowdsourcing task, we
predictions. found wording the instructions carefully to be crucial. We
16Notethatthisdiffersabitfromtheformatinthepaper:weoriginally
encouraged workers to ask about higher-level actions, ver-
hadworkerswriteoutthefulltag,like[person5],butthisisoftenlong
suslower-levelones(suchas‘Whatisperson1wearing?’),
andtheworkerswouldsometimesforgetthebrackets.Thus,thetagformat
hereisjustasinglenumber,like5. aswellastonotaskquestionsandanswersthatwereoverly
12
generic (and thus could apply to many images). Workers nales. Since we must perform Adversarial Matching once
wereencouragedtoanswerreasonablyinawaythatwasnot for the answers, as well as for the rationales, this would
overly unlikely or unreasonable. To this end, we provided naivelyinvolve22matchingsonafoldsizeofroughly26k.
theworkerswithhigh-qualityexamplequestions,answers, We found that the major computational bottleneck wasn’t
andrationales. the bipartite matching17, but rather the computation of all-
Qualification exam Since we were picky about the pairssimilarityandrelevancebetween∼26kexamples.
typesofquestionsasked,andtheformatoftheanswersand There is one additional potential problem: we want the
rationales, workers had to pass a qualification task to dou- datasetexamplestorequirealotofcomplexcommonsense
ble check that they understood the format. The qualifica- reasoning,ratherthansimpleattributeidentification. How-
tiontestincludedamixofmultiple-choicegradedanswers ever,iftheresponseandthequerydisagreeintermsofgen-
as well as a short
for- a function of φ. We then evaluate the updated
mance on the target language. The optimization weightsθ(cid:48) ondatax fromthetargetlanguagefor
t
procedure and the function of the representation updatingg :
φ
transformationnetworkwillbediscussedinmore
detailinthenextsection. φ(cid:48) = φ−β∇ φL Dt(f(x t;θ(cid:48)),y t) (4)
2.3 Optimization whereL (x ;·)istheloss functionoftheupper
Dt t
The training of the representation transformation probleminEquation2andβ isitscorresponding
network conforms to the following principle: If learning rate. Note that the meta-optimization is
therepresentationtransformationnetworkg effec- performedovertheparametersoftherepresentation
φ
tivelytransformsthesourcelanguagerepresenta- transformationnetworkg φ whereastheobjectiveis
tions,suchtransformedrepresentationsf(x ;φ,θ) calculatedsolelyusingtheupdatedparametersof
s
should be more beneficial to the target task than themainarchitectureθ(cid:48). BypluggingEquation3
theoriginalrepresentationsf(x ;θ),suchthatthe intoEquation4,wecanfurtherexpandthegradient
s
modelachievesasmallerevaluationlossL
Dt
on term ∇ φL(f(x t;θ(cid:48)),y t). We omit f and y in the
thetargetlanguage. Thisobjectivecanbeformu- followingderivativeforsimplicity.
latedasabi-leveloptimizationproblem:
∇ L (x ;θ(cid:48))
min L (f(x ;θ∗(φ)),y ) (2)
φ Dt t
φ
Dt t t =∇ φL Dt(x t;θ−α∇ θL Ds(x s;θ,φ))
s.t. θ∗(φ) = argminL (f(x ;φ,θ),y ) =−α∇2
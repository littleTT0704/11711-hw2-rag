 done by scaling
the above 0/1 indicator function (e.g., Equation 4.2) with the weight (Figure 2[c]):
f :=f data-w(t;D) =logEt∗∼D[w(t∗)⋅It∗(t)]. (4.6)
Plugging f into the SE (Equation 3.2) with the same other specification of supervised MLE (
data-w
α = 1,β = ϵ), we get the update rule of model parameters θ in the student-step (Equation 3.3):
maxEt∗∼D[w(t∗)⋅logp θ(t∗)],
(4.7)
θ
which is the familiar weighted supervised MLE. The weights w can be specified a priori based on heuristics,
for example, using inverse class frequency. In many cases it is desirable to automatically induce and adapt the
21
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
weights during the course of model training. In Section 9.2, we discuss how the SE framework can easily
enable automated data reweighting by reusing existing algorithms that were designed to solve other seemingly
unrelated problems.
Data augmentation. Data augmentation expands existing data by adding synthetically modified copies of
existing data instances (e.g., by rotating an existing image at random angles), and is widely used for increasing
data size or encouraging invariance in learned representations (e.g., object label is invariant to image rotation).
The indicator function I as the similarity metric in Equation 4.2 restrictively requires exact match between the
true t∗ and the configuration t. Data augmentation arises as a ‘relaxation’ to the similarity metric. Let
a t∗(t) ≥ 0 be a distribution that assigns non-zero probability to not only the exact t∗ but also other
configurations t related to t∗ in certain ways (e.g., all rotated images t of the observed image t∗ ). Replacing
the indicator function metric in Equation 4.2 with the new a t∗(t) ≥ 0 yields the experience function for data
augmentation (Figure 2[d]):
f :=
 We optimize
withy l = 1andcollectthesetoff l fromtrainingdatawith the WGAN-GP with ADAM [Kingma and Ba, 2015] with
y l = 0usingbaselinemodelasfeatureextractor. Wefinally mini-batch size 128 and learning rate 0.0001. We train all
fine-tune g l on this set of labeled feature vectors to get the variants of WGAN-GP for 60 epochs. We set the weight of
finalbinaryclassifierforagivenzero-shotcodel. L to0.01andL ;L to0.1. ForL,wepredictthe
CLS CYC KEY KEY
top30mostrelevantkeywordsgiventhegeneratedfeatures.
4 Experiments
Afterthegeneratorsaretrained,wesynthesize256features
Dataset. Weevaluate ourapproach usingthe publicmedi- foreachzero-shotcodelandfine-tunetheclassifierg using
l
caldatasetMIMIC-III[Johnsonetal.,2016],whichcontains ADAM and set the learning rate to 0.00001 and the batch
4021
ProceedingsoftheTwenty-NinthInternationalJointConferenceonArtificialIntelligence(IJCAI-20)
Micro Macro
Method Pre Rec F1 AUC Pre Rec F1 AUC
ZAGCNN[RiosandKavuluru,2018] 58.29 44.64 50.56 96.59 30.00 24.65 27.06 94.00
ZAGCNNw.Transformer 61.47 33.93 43.73 96.36 20.63 15.24 17.53 93.36
ZAGRNN(ours) 58.06 44.94 50.66 96.67 30.91 25.57 27.99 94.03
ZAGRNN+L LDAM(ours) 56.06 47.14 51.22 96.70 31.72 28.06 29.78 94.08
Table1:ResultsonseencodesusingbaselinefeatureextractordescribedinSection3.1
Micro Macro
Method Pre Rec F1 AUC Pre Rec F1 AUC
ZAGRNN 0.00 0.00 0.00 89.05 0.00 0.00 0.00 90.89
ZAGRNN+L LDAM 0
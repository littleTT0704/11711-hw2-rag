distributionandaddthemtotheinputfeaturesoftheGNN,similarto[24]. Forunseen
preferences,thisisprivilegedinformationthatourapproachdoesnothaveaccessto. Weusethe
output of the GNN to make sequential predictions, as in [23]. Thus, by combining the two works
andaddingprivilegedinformationaboutground-truthpreferencecategory,wecreateaGNNbase-
line which can act according to preference in a dishwasher loading scenario. We train this policy
usingimitationlearning(IL),andreinforcementlearning(RL),following[23]. GNN-IListrained
fromthesamesetofdemonstrationsasTTPusingbehaviorcloning.ForGNN-RL,weuseProximal
Policy Optimization [30] from [31]. GNN-RL learns from scratch by directly interacting with the
dishwasherenvironmentandobtainingadensereward. Fordetails,seeAppendixC.1.
GNN-IL does not reach the same performance as TTP for in-distribution tasks (PE of 0.34 for
GNN-ILvs0.62forTTP).Notethatunseenpreferencesarealsoin-distributionforGNN-ILsince
we provide ground-truth preference labels to the GNN. Hence, there isn’t a drop in performance
for unseen preferences for GNN, unlike TTP. Despite having no privileged information, TTP out-
performsGNN-ILinunseenpreferences,andperformscomparablyonunseen#objects. Duetothe
significantly long time horizons per session (more than 30 steps), GNN-RL fails to learn a mean-
ingfulpolicyevenafteralargebudgetof32,000environmentinteractionsandadensereward(PE
0.017±0.002usingGNN-RL).Lastly, wefindthattherandompolicyRPisnotabletosolvethe
task at all due to the large state and action space. TTP is able to solve dishwasher loading using
unseenpreferenceswell(PE0.54). Incontrast,classicaltaskplannerslike[32]needtobeadapted
per new preference. This experiment shows that Transformers make adaptable task planners, us-
ing our proposed
]. Thesemethodsarecommonlyemployedtogetherwithpost-processing
heuristics,suchasselectingonlythetop-ktokens/pixelswithhigherscoresforvisualization. Another
lineofworkseekstobuildaclassifierwithinherentlyinterpretablecomponents,suchasmethods
basedonattentionmechanismsandrationalizers[Leietal.,2016,Bastingsetal.,2019].
Evaluationofexplainabilitymethods. Asmentionedintheintroduction, earlyworksevaluated
explanationsbasedonpropertiessuchasconsistency, sufficiencyandcomprehensiveness. Jacovi
andGoldberg[2020]recommendedtheuseofagradednotionoffaithfulness,whichtheERASER
benchmarkquantifiesusingtheideaofsufficientandcomprehensiverationales,alongsidecompiling
datasetswithhuman-annotatedrationalesforcalculatingplausibilitymetrics[DeYoungetal.,2020].
Giventhedisagreementbetweenexplainabilitymethods,Neelyetal.[2021]showedthatwithouta
faithfulground-truthexplanationitisimpossibletodeterminewhichmethodisbetter. Diagnostic
testssuchastheonesproposedbyAdebayoetal.[2018],WiegreffeandPinter[2019]andAtanasova
etal.[2020]aremoreinformativeyettheydonotcapturethemaingoalofanexplanation: theability
tocommunicateanexplanationtoapractitioner.
9
reyaL reyaL reyaL
Simulability. A new dimension for evaluating explainability methods relies on the forward pre-
diction/simulationproposedbyLipton[2016]andDoshi-VelezandKim[2017],whichstatesthat
humansshouldbeabletocorrectlysimulatethemodelâ€™soutputgiventheinputandtheexplanation.
Chandrasekaranetal.[2018],HaseandBansal[2020],Aroraetal.[2022]analyzesimulabilityvia
humanstudiesacrosstextclassificationdatasets. TrevisoandMartins[2020]designedanautomatic
frameworkwherestudents(machineorhuman)havetop
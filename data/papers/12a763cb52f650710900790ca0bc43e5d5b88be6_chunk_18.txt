 encountering datasets that have
knowledgeintolanguagemodels,eitherbypretrain- nosuitableknowledgebase(e.g. NumerSense(Lin
ing on knowledge bases (Ma et al., 2021; Chang et al., 2020) and CommonsenseQA 2.0 (Talmor
etal.,2020;Mitraetal.,2019;Zhongetal.,2019) etal.,2021)),orwhenthesystemneedstohandle
orfinetuningthemodelsothatitcanreasonwith commonsensequeriesthatdonotfitinanyofthe
additionalretrievedknowledge(Changetal.,2020; commonsensedomainsrepresentedbyanexisting
Mitraetal., 2019;Bianetal., 2021). Anotherdi- knowledge base. Our work overcomes this diffi-
3161
cultybyleveragingpretrainedlanguagemodelsas flexible,high-qualityknowledgeforcommonsense
thesourceofcommonsenseknowledge. reasoning.
Adding generated text during inference. Re-
Acknowledgements
cently,severalworksshowthatmodelperformance
oncommonsensereasoningcanbeboostedbyaug- This work was funded in part by the Natural Sci-
menting the question with model-generated text, encesandEngineeringResearchCouncilofCanada
such as clarifications, explanations, and implica- (NSERC)(fundingreferencenumber401233309),
tions. Self-talk(Shwartzetal.,2020)elicitsclari- DARPA MCS program through NIWC Pacific
ficationstoconceptsinthequestionandappends (N66001-19-2-4031), and the Allen Institute for
them to the inference model input. Contrastive AI.WealsothankGoogleCloudCompute,aswell
explanations(Paranjapeetal.,2021)promptsinfer- asOpenAI.
encemodelswithgeneratedexplanationsthatcon- WethankDanielKhashabi,VeredShwartz,Bhar-
trastbetweentwoanswerchoices. Theaforemen- gaviParanjape,BillYuchenLin,JonathanHerzig
tionedmethodsdependontask-specifictemplates fortheirhelpwiththeexperimentsandevaluation.
toinquirethegenerator,whichmeanst
05 92.25 95.67 95.95 87.72 94.19
SphereFace-Rv1 SFN 1.6 32 1.0 97.42 97.79 44.24 84.95 93.70 94.58 77.66 92.66 81.92 91.10 95.27 95.65 85.70 93.73
SphereFace-Rv2 SFN 1.5 64 0.2 97.93 98.19 40.06 86.37 94.12 94.78 76.94 93.21 82.94 92.29 95.62 95.99 87.83 94.42
hyperparameters. We observe that CGD can effectively im- 1.8, and for larger s, we search m from 1.6 to 2.0. The
proveSphereFaceandSphereFace-RunderallFNstrategies. results in Table 8 well match our expectation that there
BecauseoftheconsistenteffectivenessofCGD,weuseCGD is only one optimal m for each s and the optimal m will
for both SphereFace and SphereFace-R by default and the increase with larger s. Under the same s, we discover that
resultsintheothersectionsarealsoobtainedwithCGD. as m deviates from its optimal value, the performance will
also become worse. The distribution of the performance
6.3 AblationandExplorationonMS-Celeb-1M for different m exhibits a strong unimodality, which can
largely benefit the hyperparameter tuning. The best mod-
In this section, we conduct ablation and exploration with
els of SphereFace, SphereFace-R v1 and SphereFace-R v2
a deeper network that is trained on a large-scale dataset.
achieve 91.03%, 90.78% and 91.58% with s = 32, 32, and
Specifically, we use SFNet-64 as the backbone network ar-
64, respectively. The performance on MS-Celeb-1M is also
chitecture and MS-Celeb-1M as the training set. We report
muchbetterthanthatonVGGFace2,whichshowsthatour
AUC-0.0005onthesamevalidationsetasSection6.2.
methods can easily enjoy the accuracy boost from larger
trainingsetanddeepernetwork.Moreover,theperformance
6
ouneed. InAdvancesinneuralinformationprocessingsystems,
pages5998–6008,2017. 1
[56] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-
ChiehChen.Max-deeplab:End-to-endpanopticsegmentationwithmask
transformers. arXivpreprintarXiv:2012.00759,2020. 3
[57] HuiyuWang,YukunZhu,BradleyGreen,HartwigAdam,AlanYuille,
and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for
panoptic segmentation. In European Conference on Computer Vision,
pages108–126.Springer,2020. 3
[58] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li.
Solo: Segmenting objects by locations. In European Conference on
ComputerVision,pages649–665.Springer,2020. 3
[59] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen.
Solov2:Dynamic,fasterandstronger.arXivpreprintarXiv:2003.10152,
2020. 3
[60] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan
Cheng,HaoShen,andHuaxiaXia. End-to-endvideoinstancesegmen-
tationwithtransformers. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages8741–8750,2021.
1,2,5,6,7,8,9
[61] ZiqinWang,JunXu,LiLiu,FanZhu,andLingShao. Ranet:Ranking
13
VI. LIPSCHITZCONTINUITYOFOURNETWORK B. Attention Layers
The self-attention layer is not Lipschitz continuous [27].
We first define the notion of Lipschitz continuity, and
However,whenweconsidertheboundedinputs,theLipschitz
proceed to the Lipschitz continuity of fully
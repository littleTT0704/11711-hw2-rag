datasetsrequiringthesameunderlying
mathematicalreasoningtasks. L¯ILAisconstructed mathematicalreasoningskills,butwerecollected
byextending20existingdatasetsspanningawide independentlyofthetrainingdatasets. Further,we
rangeoftopicsinmathematics,varyingdegreesof collect a robustness split L¯ILA-ROBUST, that in-
linguisticcomplexity,anddiversequestionformats troduces linguistic perturbations (e.g., active vs.
andbackgroundknowledgerequirements. Impor- passivevoice)viacrowd-sourcing. Theevaluation
tantly,L¯ILAextendsallofthesedatasetstoinclude scheme is a combination of the performance on
asolutionprogram asopposedtoonlyananswer, allthreesets: L¯ILA-TEST,L¯ILA-OODandL¯ILA-
and instruction annotations to enable instruction- ROBUST.
basedlearning(Sanhetal.,2021;Weietal.,2021;
Contributions
Mishraetal.,2022b).
1. We present L¯ILA, a holistic benchmark for
In order to accurately assess the mathematical
mathematicalreasoning. L¯ILA extends20ex-
reasoningabilityofmodels, evaluatingthechain
isting datasets with solutions in the form of
of reasoning that leads to the correct solution is
Python programs and instruction annotations,
equallyimportant(ifnotmoreimportant)toevalu-
andcategorizesquestionsinto23tasksbasedon
atingthefinalanswerorexpression. Wetherefore
theirlanguagecomplexity,questionformatand
collect Python programs that serve as reasoning
needforexternalknowledge. Ourbenchmark
chains for each question in the benchmark. We
measures performance on out-of-distribution
achievethisbyautomaticallyconvertingdomain-
examplesandrobustnesstolanguageperturba-
specificlanguage(DSL)annotationsintoPython
tionsinadditiontostandardtest-set.
programs and by manually collecting expert an-
2. WeintroduceBHA¯SKARA,amulti-taskmodel
notationswhennoDSLannotationsareavailable.
fine-tunedonourdataset. Ourbest-performing
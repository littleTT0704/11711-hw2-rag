 LLMs, and
bias in LLMs. Longer pretraining and larger
makingLLMssmallerandmoreefficient,byusing
modelsledtohighersocialbias,andquantiza-
oneoracombinationofmanycompressionmeth-
tion showed a regularizer effect with its best
trade-offaround20%oftheoriginalpretraining ods(Xuetal.,2021),littleresearchhasbeendone
time.1 regardingtheinterplaybetweensocialbiasesand
LLMcompression. Existingworkhasshownthat
1 Introduction pruning disproportionately impacts classification
accuracyonlow-frequencycategoriesincomputer
Large Language Models (LLMs) are trained on
visionmodels(Hookeretal.,2021),butthatprun-
largecorporausingself-supervision,whichallows
ingtransformermodelscanhaveabeneficialeffect
models to consider vast amounts of unlabelled
with respect to bias when modeling multilingual
data, and learn language patterns through mask-
text(Hookeretal.,2020;Oguejietal.,2022). Fur-
ingtasks(Devlinetal.,2019;Radfordetal.,2019).
ther,XuandHu(2022)haveshownthatcompress-
However, self-supervision allows LLMs to pick
ingpretrainedmodelsimprovesmodelfairnessby
up social biases contained in the training data.
workingasaregularizeragainsttoxicity.
Which is amplified by larger models, more data,
Unlikepreviouswork,ourworkfocusesonthe
andlongertraining(Kanekoetal.,2022;Kaneko
impacts of widely used quantization and distilla-
andBollegala,2022;Kuritaetal.,2019;Delobelle
tiononthesocialbiasesexhibitedbyavarietyof
andBerendt,2022).
both encoder- and decoder-only LLMs. We fo-
Social biases in LLMs are an ongoing prob-
cusontheeffectsofsocialbiasoverBERT(Devlin
lemthatispropagatedfrompretrainingtofinetun-
etal.,2019),RoBERTa(Liuetal.,2019)andPythia
ing
 Codex
mayrequireanequallystrongretrieveraswell. WefindthatCodexcanachieveevenhigherresults
withanoracleretriever,whichshowsthepotentialfurtherimprovementbyimprovingtheretrievers.
Finally,CodeT5performsbetterthanT5,withandwithoutusingDocPrompting. Thisemphasizes
theimportanceofusingcode-specificpretrainedmodels.
Execution-basedevaluationTheresultsareshowninFigure3. UsingDocPromptingconsistently
outperformsthe baselineCodeT5 forall values ofpass@k. For example, DocPrompting yields
2.85%improvementonpass@1and4.45%improvementonpass@5,whicharerealisticnumbers
of completions that can be suggested in an IDE. When k = 200, DocPrompting widens the gap
to8.38%. TheseresultsdemonstratethatDocPromptingdoesnotonlyimprovethequalityofthe
generatedcodeinitssurfaceform,butalsoincreaseitsfunctionalcorrectness. Additionaldetailsand
resultsareprovidedinAppendixG.
6 ANALYSIS
6.1 WHYDOESREADINGTHEDOCUMENTATIONHELPGENERATINGMOREACCURATECODE?
We believe that one of the major reasons is that documentation eases the mapping between NL
intentsandcode,sincethedocumentationcontainsbothNLdescriptionsandfunctionsignatures.
We calculated the n-gram overlap between the NL intents and their corresponding code snippets
(NL←→code), and the overlap between the NL intents with their top-10 retrieved documents and
theircodesnippets((NL+docs)←→code). AsshowninFigure4,addingdocumentationsignificantly
increasestheoverlapacrossn-grams,andincrease,forexample,theunigramoverlapfrom12%to
7
k@ssap llaceR
PublishedasaconferencepaperatICLR2023
Table4:Retrievalperformanceofmultiplemodelsonthedevsetoftldr(top)andCoNaLa(bottom).
RoBERTaisthebestmodeltakenfromfromGaoetal.(2021),andCodeT5istheencoderofCodeT5-
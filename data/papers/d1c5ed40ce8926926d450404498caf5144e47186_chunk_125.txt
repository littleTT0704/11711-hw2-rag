 Watson supports both Jeopardy! and TREC questions, the question anal-
ysis component of OpenEphyra was developed specifically for TREC-style factoid
questions. It has not been adapted to Jeopardy! clues, which are on average much
longer, often contain unnecessary or even misleading information, and are given in
the form of statements rather than in interrogative form. Thus we used OpenEphyra
only for experiments with TREC questions, but we evaluated its performance both
on independent factoid questions in TREC 8–12 and question series about common
topicsinTREC13–15. Again, theretrievedpassagesanddocumenttitleswerejudged
automatically by determining whether any token sequence matches the answer pat-
terns, and performance was evaluated in terms of search recall. We also report the
average number of relevant results, which is a measure of the redundancy in the search
90 CHAPTER 6. APPLICATION TO QUESTION ANSWERING
results. Multiple relevant results for a single question can facilitate answer extraction
and scoring.
6.3.4 OpenEphyra Results and Analysis
Table 6.11 shows OpenEphyra’s search recall and the average number of relevant
search results for the seed corpora, SE using search engine rankings and statistical
SE when using hit lists of 20 passages and 10 titles. Both expansion methods con-
sistently improve search performance, independently of the sources, search strategy
and dataset. Similarly to the experiments with Watson, the largest improvements are
realized if Wiktionary is expanded, and the gains are smaller if larger seed corpora
with higher coverage are used. When expanding both Wikipedia and Wiktionary us-
ing the statistical method, passage recall increases by 4.0% and title recall improves
by 13.7% on the full dataset that includes factoid questions from TREC 8–15. The
recall gains on this combined dataset are statistically significant (p <.0001) based on
a one-sided sign test. Search recall also improves on every individual TREC dataset,
which is not shown in these aggregate results. The expansion method that uses search
engine rankings has lower performance than statistical SE, which confirms that the
relevance model that leverages the seed content to estimate the topicality of retrieved
text is effective. The difference in search performance is most apparent when expand-
ing Wikipedia because the
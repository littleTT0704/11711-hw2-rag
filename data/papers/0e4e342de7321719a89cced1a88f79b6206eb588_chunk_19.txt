etextualinformationduring
rectlyusedforpredictionwithoutfurthermulti-modalfusion.
visualfeatureextraction. TheresultsinTable4onRefCOCO
ExperimentsshowthatVGQCw/fusionoutperformsallpre-
datasetshowthatincorporatingtextualinformationresultsin
viousone-stagevisualgroundingmethodsandachievesstate-
the largest performance improvement, and adding extra pa-
of-the-art performance, and VGQC w/o fusion increases in-
rameters without leveraging textual information brings only
ference speed with a simpler structure while reaching com-
insignificantimprovements. Thisdemonstratestheeffective-
parable performance as the latest methods. In addition, the
nessofQCM.
extensiveanalysisandvisualizationsshowthatQCMcanef-
Number of query-conditioned convolution blocks. We fectively comprehend query information and extract query-
studytheimpactofstartapplyingQCMatdifferentlayersin aware visual features. Finally, our method is applicable to
thevisualencoder. IntheResNetbackbonewhichhas4lay- other textual-visual tasks, which merits further investigation
ersofconvolutionblocks,weuseQCMinlayer4,layer3&4, infuturework.
References [Liuetal.,2019] Daqing Liu, Hanwang Zhang, Feng Wu,
[Carionetal.,2020] Nicolas Carion, Francisco Massa, and Zheng-Jun Zha. Learning to assemble neural mod-
ule tree networks for visual grounding. In ICCV, pages
Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,
4673â€“4682,2019.
andSergeyZagoruyko. End-to-endobjectdetectionwith
transformers,2020. [Liuetal.,2020] Yongfei Liu, Bo Wan, Xiaodan Zhu, and
[Chenetal.,2018] Xinpeng Chen, Lin Ma, Jingyuan Chen, Xuming He. Learning cross-modal context graph for vi-
Zequn Jie, Wei Liu, and Jiebo Luo. Real-time referring sualgrounding. AAAI,34(07),Apr.2020.
expressioncomprehensionbysingle-stagegroundingnet- [LoshchilovandH
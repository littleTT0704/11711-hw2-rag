now?
areusedasreasoningannotationsin(Austinetal., Table6: Exampleswherelargelanguagemodelsarenot
2021;Mishraetal.,2022a)duetotheirenhanced consistentformathematicalreasoning.
accessibility and readability. To imitate the rea-
thisremainsanopenproblem.
soning process of a human, a more recent trend
Aredeeplearningmethodsconsistentformathe-
istoannotatesolutionsinnaturallanguage(Ling
maticalreasoning? Recentdevelopmentsindeep
et al., 2017; Cobbe et al., 2021; Lu et al., 2022b;
learning have led to impressive results on vari-
Hendrycksetal.,2021b;Luetal.,2022a).
ousmathematicalreasoningtasks. Thezero-shot-
6.2 AnalysisofDeepLearningMethods CoTMinerva540Bachievesascoreof75.0%on
the MMLU-STEM benchmark (Hendrycks et al.,
Isthecurrentrepresentationofnumeracysuf-
2021a), which assesses multitask reasoning abil-
ficient? The standard practice for deep learning
ity in the fields of science, technology, engineer-
techniquesistotreatnumbersinthesamewayas
ing,andmathematics(STEM)atbothhighschool
words. Earlyneuralnetworkmethodscreateavo-
andcollegelevels. Similarly,few-shot-CoTGPT-3
cabulary that maps input words and numbers to
175B achieves a high accuracy of 93.0% on the
tokenIDs,resultinginlessfrequentnumbersbeing
MultiArithtask. However,thequestionremainsas
collapsed into an “UNK” token. Recent language
towhetherthesemethodsaresufficientlyadvanced
modelsusesubwordtokenizationtechniques(Wu
totacklemorecomplexproblems.
etal.,2016;Sennrichetal.,2016)tosplitnumbers
Thereisstrongevidencethatdeeplearningmeth-
into atomic tokens. Recent studies have shown
odsformathematicalreasoningarenotrobustand
thatthesetokenizationapproachesaresuboptimal
suscept
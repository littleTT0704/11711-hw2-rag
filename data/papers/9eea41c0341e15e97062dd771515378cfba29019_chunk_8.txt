the clear advantage of referencing the entire con- conductahumanevaluationcomparingthesetwo
versationatgenerationtime. However,themodel cases. Inourblindcomparisonof22conversations,
doesnothavearequirementtoproducethesame theleftandrightcontextmodelwaspreferredover
number of lines as the input and must learn this thenocontextmodel86%ofthetime(2annotators,
propertyduringtraining. Weconjecturethatthisis Cohen’skappa0.62).
thereasonforthismodel’srelativelyweakperfor- The conversation-level model may be a good
mancerelativetotheleftandrightcontextmodel. choiceforsomeapplications,whereoutputlength
Additionally, if the model generates more or less is less important to the downstream task. This
linesthantheinputdialogue,thiscanbeaconflat- modelhasahigherdegreeofabstractiveness,which
ingfactorintheextractivesummarizationexample can lead to increased fluency but also increased
we discuss in Section 5. If the model generates hallucination. Fortaskswherethisisaconcern,the
less lines than the input, it has performed some left and right context model achieves reasonable
part of the summarization process by abstracting fluencywhileadheringmorecloselytothetask,as
theinputintoashorteroutput;ifithasgenerated measuredbytheautomaticmetrics.
morelinesthantheinput,ithasproducedaharder
problemfortheextractivesummarizationsystem 4.2 FormalityandPerspectiveShift
bycreatingmorelinestochoosethesummaryfrom.
Approaches We observe that the perspective
Becauseofthismodel’sweakerperformanceand
shifting task requires a high degree of formaliza-
thisconflatingfactor,werestrictourremainingex-
tion. We consider several models ranging from
perimentsinthispapertomodelsthatperspective
simplerule-basedapproachestothoserelyingon
shiftoneutteranceatatime.
anexternalformalizationdatasetinordertobetter
The model with left context only mimics how understand the role of formalization in perspec-
a human might read the conversation for the first tive
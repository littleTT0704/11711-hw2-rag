ionset
bringsnotableperformancegainonall5tasks.Weobserve
consistently outperforms MLM training by a large margin,
thatmodelstrainedonATOMICsetshavealargeadvantage
suggestingthatpreservingthetaskstructureisbeneficialin
onSIQAcomparetomodelstrainedonCWWV,whileCWWV
additiontothequestioncontentandvalidatingH6.
brings advantage on the CSQA task. This is not surpris-
ing asthese two tasks arederived from ConceptNet and
DifficultyoftheSyntheticQASets
ATOMIC,respectively.ThedifferencebetweenATOMICand
CWWVontheremainingthreetasksisrelativelysmall.This Ideally,thegeneratedquestion-answerpairsshouldbechal-
supportsourhypothesisH3:knowledgealignmentiscrucial lenging for the models but easy for humans to solve (H7).
forobtainingbetterperformance. Here,weprobethishypothesisbyassessingthedifficultyof
Trainingonthecombinedquestionset(CSKG)ismostly our synthetic QA sets both by humans and ‘vanilla’ LMs.
able to retain the best of its both partitions. Training on We evaluated both models on the dev sets of our synthetic
CSKG leads to best performance on three out of five tasks, data. For human evaluation, we randomly sample 50 ques-
showingthataglobalcommonsenseresourceisabletobring tionsfromATOMICand50questionsfromCWWV.Atotalof
consistentgainacrossdifferenttasks.Thissupportsourhy- five researchers were asked to first provide the correct an-
pothesis H4: adding more diverse knowledge is beneficial swer,thenratethequestiondifficulty.Forthelatter,thean-
forlanguagemodels.Finally,evenwiththisknowledge,we notatorchosebetweeneasy,moderate,hard,ornon-sensical
recognizethatthereisstillalargegapbetweenourmodel’s - as a guideline, nonsensical questions have unfair distrac-
accuracyandthatofthesupervisedRoBERTamodel. torsandcannotbeeasilyunderstood.Followingthisproce-
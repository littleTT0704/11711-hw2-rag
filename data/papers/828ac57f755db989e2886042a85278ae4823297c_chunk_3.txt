
]VC.sc[
1v07640.1151:viXra
tions. Becausethesizeofdescriptionembeddingmatrixis introducingrelatedworks,wedetailthelargescaledataset
verylargebutmanywordsusuallyappearsonlyafew(less we have collected for video QA tasks. We then present
than10)timesinalldescriptions, theresultsoverfiteasily. ourapproachofvideotemporalstructuremodelingandthe
Recent study [22] found that visual and textual informa- dual-channellearningtorankmethodforquestionanswer-
tionaremutuallybeneficial. Wepavedanewwayofvideo ing. Extensive experiments are conducted to validate our
QA, by appropriately integrating information of all types, approach.
including sentences, words, and visual cues, into a joint
learning framework to maximize the mutual benefits, dur-
2.RelatedWorks
ingwhichexternalknowledgebases(e.g.BookCorpus[50]
and Google News [24]) can be readily incorporated. Be- Neuralnetworksinvideoanalysis. Recently,manyCon-
cause the external knowledge bases reflect the underlying vNetsbasedvideofeaturelearningmethodshavebeenpro-
correlationsamongrelatedentities,ourapproachisableto posed. SimonyanandZisserman[31]proposetoutilizeop-
tobetterparsequestionsandvideoframes. ticalflowimagesextractedfromvideosastheinputstotrain
Second, a Video QA system should be capable of rea- ConvNets. AlongwiththeordinalRGBstream,two-stream
soning across video frames, including inferring the past, ConvNets can achieve comparable performance with the
describing present, and predicting the future, which are state-of-the-art hand-crafted feature improved Dense Tra-
strongly correlated. Very recently, Gated Recurrent Unit jectories[43]. Tranetal.[36]propose3DConvNetswhich
(GRU)[4]hasdemonstratedpromisingperformanceonse- capture temporal dynamics in video clips without the very
quence modeling tasks, partially because it has simpler time-consuming optical flow extraction procedure. Xu et
neural structure than LSTM. On top of GRU, we propose al
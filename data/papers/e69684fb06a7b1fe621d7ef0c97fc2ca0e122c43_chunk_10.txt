
3.7 WebAppCreation
Hypothetical Document Embedding
--- We finally provide an optional step in
language: en
license: apache-2.0 Prompt2Model to automatically create a
tags: graphical user interface that allows downstream
- question-answering
- nlp userstointeractwiththetrainedmodel. Thisweb
- transformers application,builtusingGradio(Abidetal.,2019),
datasets:
canthenbeeasilydeployedpubliclyonaserver.
- natural-questions
- squad
--
4 ExperimentalSetup
## Model Description
This model is a fine-tuned version of a BERT model
for question-answering tasks. It can generate Tasks As a proof-of-concept, we test our sys-
answers to natural questions given context. tem’sabilitytolearnamodelforthreetasks:
Figure3: Forourmodelretriever,wefirstconstructa • MachineReadingQuestionAnswering: Wefirst
hypotheticalmodeldescriptionforaquery,thencom- consider a common use case where pretrained
putesimilarityscoresbetweenthathypotheticalmodel models and training datasets are plentiful. We
descriptionandthedescriptionsofrealmodels.
useSQuAD(Rajpurkaretal.,2016)asground
truthtoevaluatethissetting.
3.5 Training
• Japanese NL-to-Code: Code generation from
DatasetProcessing Wetrainthemodelbylever- Japanese-languagequeriesisachallengingsce-
aging two datasets- one generated and one re- nariowherepriorworkexistsbutnoannotated
trieved. To sidestep the challenge of making dataorpretrainedmodelsareavailable. Weuse
schema-specificmodelingdecisions(e.g. construct- MCoNaLa(Wangetal.,2023)forevaluation.
ing specialized architectures for classification or • TemporalExpressionNormalization: Wefinally
generationtasks),wetreatalldatasetsas“text-to- consider a task where there are no pretrained
text”problems(Raffeletal.,2020). Wetextualize modelsortrainingdatasetsofanykindavailable.
the
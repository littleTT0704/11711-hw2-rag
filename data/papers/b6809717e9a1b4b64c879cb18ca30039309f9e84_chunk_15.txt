fea-
thatpredictingsurprisingeventboundariesrequires tures that are shared across the annotators, such
takingintoaccounthowthestorydevelopedprior ascommonsenseknowledge;therefore,themodel
to theprevioussentence insetting up thecontext performs well in detecting those. Whereas, our
forthecurrentsentence. Thisfindingechoesresults model struggles with detecting event boundaries
by Townsend (2018) that showed that surprising thataremoresubjective.
sentences take a long time to read because it re-
quires changing our mental model formed from Predictive features By integrating a separate
feature-basedclassifier,theEventBoudaryDetec-
previoussentences.
tormodelallowsustoexaminethemodelparame-
F1 varies with majority agreement Since the tersanddeterminefeaturesthatareassociatedwith
annotations were subjective and did not always surprising,expectedornoeventboundaries. First,
agree,wefurtherexamineourbestmodel’sperfor- wetaketheaverageoftheGRUclassifierweights
mance(PREVIOUSONLY)withrespecttoannota- for each of the 10 cross-validated models. Then,
tionagreement. AsshowninFigure3,F1increases we plot these weights for each label in Figure 4,
withmajoritylabelagreement(Pearson’sr=0.953, andsummarizethefindingsbelow.
p<0.05). Suchpositivecorrelationsareobserved Featuresthatrelatetocommonsenserelations:
across all event boundary labels (Pearson’s r = oEffect, xEffect and Glucose Dim-6
0.869-0.994)andisespeciallystrongforsurprising (causedby)aremostpredictiveofexpectedevent
eventboundaries(Pearson’sr=0.994,p<0.001). boundaries. Thiscanindicatethateventsthatarean
Thismeansthatmosterrorsaremadeonsamples effectof/causedbyaprioreventcanbeexpectedby
that have low agreement among annotators. For annotators,asalsonotedbyGraesseretal.(1981).
exampletoshowthiscontrast,after“SheandIare An example of an
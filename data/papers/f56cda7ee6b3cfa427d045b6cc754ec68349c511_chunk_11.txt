PT [CLS]tokenrepresentationmakestheprediction.
offensiveresponsescontainprofanitycomparedto DGPT-Toleveragethefullthread(T)context,we
39.59%ofGPT-3and66.47%ofReddituser’sof- alsoexperimentedwithDialoGPT-medium(345M
fensiveresponses. Thus,filteringtraininginstances parameters,Zhangetal.,2020). Here,T isencoded
containing offensive phrases reduce profanity in as a sequence of all u ’s separated by a special
i
DGPT responses (Zhang et al., 2020). However, token [EOU], indicating end of utterance. The
thisfilteringdoesn’teradicatethemodel’soffensive hiddenrepresentationof[EOU]foreachu ∈ T
i
behavior. isusedasitssentencerepresentation, h. Forthe
i
Stance task, we predict sˆ = Softmax(h ⊕
i←j i
5 OffensiveLanguageandStance h ⊕h −h ⊕h (cid:12)h ),where⊕isconcatenation
j i j i j
Classification operator,(cid:12)iselement-wisemultiplication.
WenowinvestigatethepredictabilityofOffensive 5.2 LossFunctions
Language (Offensive) and Stance (Stance)
Thestandardcross-entropylossfunctionisusedfor
inconversationsthatincludegeneratedresponses.
theOffensivetask,however,becauseStance
Given a thread, T = (u,u,...,u ), we predict
1 2 k has an imbalanced class distribution (about 1:10
Offensivelabelso ∈ {0,1}foreachutterance,
i forAgreeand1:40forDisagree),weuseweighted
u,i ≤ k and Stance labels s ∈{Neutral,
i i←j cross-entropy(wCE)withweights(1,100,100)for
Agree, Disagree} for every pair of utterances
{Neutral, Agree, Disagree}respectively. Wealso
(u,u
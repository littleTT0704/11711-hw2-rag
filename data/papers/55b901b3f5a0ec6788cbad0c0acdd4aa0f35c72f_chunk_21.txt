thatweneedmoreexternaldatathann = 10 inorderto
× arecomparisonsto(Deryetal.,2021b)
see marked improvements to offset our inexact search of
thespaceofcompositefunctions. However,suchscalesareoutsideourcomputationalbudget.
7.3 WHYDOESAANGWORK?
Tobetterunderstandwhyourauxiliarylearningpipelineimprovesend-taskperformance,weperform
multipleablationsunderAANG-TD.
StaticversusDynamicWeighting: Weablatetheimpactofusingstatictaskweightsthroughout
training, as against adaptive task weights. Just as with AANG, we sub-sample n tasks from the
searchspaceateveryiteration(niscross-validatedexactlyasAANGis–Table??). Eachsampled
tasksweightisinitializedto 1 andthisremainsunchangedthroughouttraining. ThisistheStatic
n
Multitask-TDbaselineinTable2. AANG-TDimprovesuponthestaticmultitaskbaselinebyover
1.1%onaverage. Withadaptiveweighting,AANGdown-weightsobjectivesthatareharmfultothe
end-taskwhilstup-weightingrelevantones(Prescription(P )). However,usingstaticweightingsis
1
morecomputefriendlysincewedonothavetocalculatetask-weightmeta-gradients. Thiscompute-
vs-performancetrade-offisleftforpractitionerstoresolvebasedontheiravailableresources.
Impactofnumberofsampledobjectives: Duetocomputationalconstraints,AANGsub-samples
thesetofgeneratedobjectives. Whilstthissamplingcanresultinapproximationerrorwheninferring
taskweightings,itcanalsointroducestochasticitywhichcanhelpregularizethelearnedmodel. From
Table3(AppendixA)wefindthatforsometasks(ACL-ARCandSCIERC)samplingalargernumber
oftaskshelps.SE-2016-6andCHEMPROTontheotherhandbenefitfromsmallernumberofsampled
tasks. Ourrecommendationisthatthenumberofsampledtasksbecross-validatedonaper-task
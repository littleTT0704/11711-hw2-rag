 As discussed earlier, this is due to the
ficult‚Äù for LMs, however, training on them might teach the incompleteness of our KGs, and the current lack of under-
LMincorrectknowledgeandharmdownstreamaccuracy. standingonhowtogeneratefair,yetinformative,distractors.
13512
RoBERTa-L Train aNLI CSQA PIQA SIQA WG Model ATOMIC CWWV
baseline - 65.5 45.0 67.6 47.3 57.5 GPT2-L 43.2 69.5
+ATOMIC MLM 62.9 43.8 65.8 53.9 55.5 RoBERTa-L 45.9 64.5
+ATOMIC MR 70.8 64.2 72.1 63.1 59.6 Human 78.0 80.7
+CWWV MLM 65.3 57.3 67.2 49.3 59.4
+CWWV MR 70.0 67.9 72.0 54.8 59.4 Table5:LMandhumanaccuracyonoursyntheticQAsets.
Table4:ComparisonbetweenMLMandMRtraining.
thisfindingpointstoamoresubstantialchallenge:generat-
ingfairandinformativemultiple-choicequestionsisnotyet
Discussion well-understood. Adversarial strategies yield more plausi-
ble candidates than random sampling, making the task less
TowardsaCommonsenseService
fair; yet, fully relying on random sampling would generate
The overarching pursuit of this paper is to understand distractorsthataretriviallydiscerniblefromthecorrectop-
whethergeneratingartificialQAsetswithKGsimprovesthe tion.Balancingbetweenfairnessandinformativeness,thus,
zero-shotQAperformanceofLMs.Weobserveaconsistent isessentialformultiple-choicequestiongeneration.Ourem-
leapinaccuracyacrosstasks,LMs,knowledgesources,and pirical evidence suggests that it could be achieved by a
question generation strategies. While these accuracies are mixedapproach,wherepartofdistractorsisgeneratedran-
notablybelowsupervisedLMaccuracies,theymightfurther domly,andpartbyadoptingsuitable

presents an optimization problem, for which an optimization solver is applied to obtain the target model
solution p θ∗. This section is devoted to discussion of various optimization algorithms.
For a simple objective such as that of the vanilla supervised MLE (Equation 2.1) with a tractable model,
stochastic gradient descent can be used to optimize the model parameters θ straightforwardly. With a more
complex model or a more complex objective like the general SE, more sophisticated optimization algorithms
are needed, such as the teacher-student procedure exemplified in Equation 3.3. Like the standardized
formulation of the objective function, we would like to quest for a standardized optimization algorithm that is
generally applicable to optimizing the objective under vastly different specifications. Yet it seems still unclear
whether such a universal solver exists and what it looks like. On the other hand, some techniques may hold the
promise to generalize to broad settings.
36
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
Figure 4. EM and the teacher-student mechanism entail alternating projection to achieve local
minima.
7.1. Alternating Projection
Alternating projection (Bauschke & Borwein, 1996; Csiszár, 1975) provides a general class of optimization
algorithms from a geometry point of view, subsuming as special cases many of the optimization algorithms
discussed above, ranging from the EM (Section 2.1.2) to the teacher-student algorithm (Section 3). At a high
level, the algorithms consider the optimization problem as to find a common point of a collection of sets, and
achieve it by projecting between those sets in succession. For example, the EM algorithm entails alternating
projection, as shown in Figure 3, where the E-step (Equation 2.10) projects the current model distribution
p (x,y) onto the set of distributions whose marginal over x equals the empirical distribution, i.e.,
θ(n)
q(n+1) = argmin KL(q(y∣x)p~ (x)∥p (x,y)), then the subsequent M-step (Equation 2.11)
q d θ(n)
37
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard
,andS.Levine. Parrot: Data-drivenbehavioral
priorsforreinforcementlearning. arXivpreprintarXiv:2011.10024,2020.
[12] D. Driess, J.-S. Ha, and M. Toussaint. Deep visual reasoning: Learning to predict ac-
tion sequences for task and motion planning from an initial scene image. arXiv preprint
arXiv:2006.05398,2020.
[13] J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy
sketches. InInternationalConferenceonMachineLearning,pages166–175.PMLR,2017.
[14] T.Silver,R.Chitnis,N.Kumar,W.McClinton,T.Lozano-Perez,L.P.Kaelbling,andJ.Tenen-
baum. Inventingrelationalstateandactionabstractionsforeffectiveandefficientbilevelplan-
ning. arXivpreprintarXiv:2203.09634,2022.
[15] D.Gordon,D.Fox,andA.Farhadi. Whatshouldidonow? marryingreinforcementlearning
andsymbolicplanning. arXivpreprintarXiv:1901.01492,2019.
[16] R. Kaushik, T. Anne, and J.-B. Mouret. Fast online adaptation in robotics through meta-
learningembeddingsofsimulatedpriors. In2020IEEE/RSJInternationalConferenceonIn-
telligentRobotsandSystems(IROS),pages5269–5276.IEEE,2020.
[17] R. Kaplan, C. Sauer, and A. Sosa. Beating atari with natural language guided reinforcement
learning. arXivpreprintarXiv:1704.05539,2017.
9
[18] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polo-
sukhin. Attention is all you need. Advances in neural information processing systems, 30,
2017.
[19] T
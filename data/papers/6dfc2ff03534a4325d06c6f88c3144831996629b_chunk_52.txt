 in the sequence corresponds to a different
3. Garyandhisallies. transformer unit in BERT. We can then use the later lay-
4. QuinnandhisIRAsquad. ers in BERT to extract contextualized representations for
theeachtokeninthequery(everythingfromwhyto?) and
5. Jimmyandhisfriends.
theresponse(sheto.).26 Notethatthisgivesusadifferent
Ontheotherhand,ourdatasetexamplestendtobehighly
representationforeachresponsechoicei.
diverse in terms of syntax as well as high-level meaning,
We extract frozen BERT representations from the
due to the similarity penalty. We hypothesize that this is
second-to-last layer of the Transformer.27 Intuitively, this
why some language priors creep into, particularly in
VCR makes sense as the representations that that layer are used
theQA→Rsetting: givenfourverydistinctrationalesthat
forbothofBERT’spretrainingtasks: nextsentencepredic-
ostensibly justify why an answer is true, some will likely
tion (the unit corresponding to the [CLS] token at the last
serveasbetterjustificationsthanothers.
layerLattendstoallunitsatlayerL−1),aswellasmasked
Furthermore,providingadditionallanguageinformation language modeling (the unit for a word at layer L looks at
(such as subtitles) to a model tends to boost performance itshiddenstateatthepreviouslayerL−1,andusesthatto
considerably. When given access to subtitles in TVQA,25 attend to all other units as well). The experiments in [15]
BERTscores70.6%,whichtothebestofourknowledgeis suggest that this works well, though not as well as fine-
anewstate-of-the-artonTVQA. tuning BERT end-to-end or concatenating multiple layers
Inconclusion,datasetcreationishighlydifficult,partic- of activations.28 The tradeoff, however, is that precomput-
ularlyastherearemanywaysthatunwantedbiascancreep ing BERT representations lets us substantially reduce the
in during the dataset creation process. One such bias of runtime of
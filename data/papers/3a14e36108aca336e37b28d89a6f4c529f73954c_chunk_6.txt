3)todecreasefrominputtoreferencestory.UR :
1
withBART,T5,andGPT-2onNAREORCusingnovel,task- 0.692→0.669,UR :0.940→0.931,UR :0.989→0.984.In-
2 3
motivated training methods we propose (§3). We evaluate creasedn-gramrepetitioncouldhavereasonssimilartolength
our models with both an automatic and human evaluation increase,causingcross-sentencerepetition.Figure1demon-
alongwithqualitativeanalysis(§5).Wedemonstratethatour stratesthis:S onlyhasoneinstanceofmoney.Conversion
proposedtrainingmethodsareeffectivebuthaveroomfor ofinheritanyofit(s )→inheritanyofthemoney(s′)and
3 2
furtherimprovement.WeillustratethatNAREORisindeeda enoughtotaketime(s )→enoughmoneytotakesometime
4
challengingtaskwithpotentialforfurtherexploration.2 (s′),amongotherchanges,resultsinfourinS’.
4
HowVerbFormsChange
2 Dataset:NAREORC
We note changes in occurrence distribution across verb-
2.1 DatasetConstruction related pos tags from S to S′ using NLTK’s pos tagger.
Gerundfraction(pos=VBG)(e.g.Ilikeplaying)increases
SourceCorpus: ROCStorieshas≈ 98.5Kfive-sentence
7.7%→9.5%. Past participle fraction (pos=VBN) (e.g. He
English stories. For the dev and test splits, each example
had broken it) ≈ doubles, 6.5%→12.4%. Past tense frac-
1Forsimplicity,weassumenarrativetobreakupintosentence tion(pos=VBD)(e.g.Hebrokeit)decreases60.9%→54.6%.
units.Ourtaskisstillverychallengingasshownthroughthispaper.
2Code+dataatgithub.com/vgt
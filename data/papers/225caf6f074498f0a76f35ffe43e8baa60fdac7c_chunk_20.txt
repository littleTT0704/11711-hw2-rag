 en->xx directions. Note that models in this
table are not trained on the same data; for instance
wecompare VMSST topublishedresultsofother
LaBSEwastrainedonsubstantiallymoreparalleldata
modelsonsemanticsimilarityandtheTatoebaand
andXLM-R(Para.) wastrainedusingalargeEnglish
BUCCbitextminingtasks. Weprimarilycompare
paraphrasecorpusinadditiontoparalleldata.
againstfivemodelswhichhavethestrongestmulti-
lingualresultsintheliterature: mUSE(Yangetal.,
2020), LASER (Artetxe and Schwenk, 2019b), ing sets using the margin retrieval methods from
XLM-R(NLI/STS-B)andXLM(Para.)(Reimers Artetxe and Schwenk (2019b). The results are
and Gurevych, 2020), and LaBSE (Feng et al., shown in Table 5. Baselines results are taken
2022). fromArtetxeandSchwenk(2019b);Reimersand
Forsemanticsimilarity,weincludeSpearman’s Gurevych(2020).
ρinordertocomparetoworkthatsolelyusesthis While VMSST does not have the best per-
correlationmetric. Weusecosineasthesimilarity formance relative to models from the literature
measureforallmodelsintheseevaluations.11 The on any single task, it does have the best overall
resultsareshowninTable2. performance if one averages the results for each
ForTatoeba,wecomparetomethodsthathave task.12 Whilethesemodelssharemuchincommon,
evaluated on all 112 languages, which excludes namely using parallel text and some type of pre-
mUSEasitwasonlytrainedon16languagepairs. trainingorpretrainedmodel,therearedifferences
TheresultsareshowninTable3. Baselinesresults in the exact data and models used, among other
aretakenfromReimersandGurevych(2020). confoundingvariables. Forinstance,LaBSEused
For BUCC, we include results on the train- trainingdataconsistingofsixbillionparallelpairs

 for re-scoring. Since the label-synchronous decoder, we can approximate
CTCpredictshardalignment,therescoringisnot thedenominatorwithasinglevocabularyentryc
att
straightforward. Tothisend,Watanabeetal.(2017) predictedbytheattentionaldecoderp :
att
proposedtousetheCTCprefixprobability(Graves,
2008)definedasacumulativeprobabilityofallla-
p (g <eos> X)
belsequencesthathavethecurrenthypothesishas Odds (g) ctc ⊕ |, (5)
end
≈ p (g c X)
theirprefix: ctc att
⊕ |
wherec = argmax p (g c X).
p ctc(h,...) = p ctc(h ⊕ν |X), (1) Nowtheevat at luationofOc ∈ dV d/ s{ en< de (o gs )> }
is
att (T)⊕
.
If|
we
ν + O
X∈V considerthatthebaselinemodelalreadyusesCTC
where is output vocabulary (including the rescoring,thenevaluatingOdds (g)amountsto
V end
<eos> symbol), is string concatenation, and aconstantnumberofextraoperationstoevaluate
⊕
X istheinputspeech. Tocalculatethisprobability p (g <eos> X).
ctc
⊕ |
effectively,Watanabeetal.(2017)introducevari- Finally,tocontrolthelatencyoftheonlinedecod-
(b) (n)
ables γ (h) and γ (h) that represent forward ing,wecomparethelogarithmofOdds (g)with
t t end
probabilitiesofhattimet,wherethesuperscript atunableconstantC. IflogOdds (g) > C,
end end end
denotes whether the CTC paths end with a blank westopthebeamsearchanddiscardthelasttoken
ornon-blankCTCsymbol. Ifthehypothesishisa
ation 3.2 the popular cross entropy (CE) as the
divergence function, that is, D(q,p θ) = −Eq[logp θ], and Shannon entropy as the uncertainty measure,
that is, H(q) = −Eq[logq]. We further assume the experience f is independent of the model parameters θ
(the assumption is indeed not necessary for the teacher step). We have, at iteration n:
βlogp (t)+f(t)
Teacher: q(n+1)(t) =exp θ(n) / Z
{ α }
(3.3)
Student: θ(n+1) =argmax
θ
Eq(n+1)(t)[logp θ(t)],
where Z is the normalization factor. The first step embodies a ‘teacher’s update’ where the teacher q ingests
experience f and builds on current states of the student p ; the second step is reminiscent of a ‘student’s
θ(n)
update’ where the student p θ updates its states by maximizing its alignment (here measured by CE) with the
teacher.
15
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
Besides, the auxiliary q is an easy-to-manipulate intermediate form in the training that permits rich
approximate inference tools for tractable optimization. We have the flexibility of choosing its surrogate
functions, ranging from the principled variational approximations for the target distribution in a properly
relaxed space (e.g., mean fields) where gaps and bounds can be characterized, to the arbitrary neural network-
based ‘inference networks’ that are highly expressive and easy to compute. As can be easily shown (e.g., see
Section 4.1.3), popular training heuristics, such as EM, variational EM, wake-sleep, forward and backward
propagation, and so on, are all direct instantiations or variants of the above teacher-student mechanism with
different choices of the form of q.
More generally, a broad set of sophisticated algorithms, such as the policy gradient for reinforcement learning
and the generative adversarial learning, can also be easily derived by plugging in specific designs of
-training has also
withexistingannotations,followingpreviousAL
beenacommonlyadoptedstrategytoenhanceac-
work. Our error estimator also requires a small
tive learning (Tomanek and Hahn, 2009; Majidi
developmentsetandthepropersettingofahyper-
andCrane,2013;Yuetal.,2022).
parameter. Nevertheless,wetriedourbesttomake
thesettingspracticalandtheevaluationfair,espe-
PA. Learning from incomplete annotations has
ciallytakingreadingtimeintoconsideration. Sec-
beenwell-exploredforstructuredprediction. For
ond,inourexperiments,wemainlyfocusoninves-
CRFmodels,takingthemarginallikelihoodasthe
tigatinghowmuchdataisneededtoreachthefully-
objective function has been one of the most uti-
supervisedresultsandcontinuetheALcyclesuntil
lized techniques (Tsuboi et al., 2008; Täckström
thishappens. Inpractice,itmaybeinterestingto
et al., 2013; Yang and Vozila, 2014; Greenberg
morecarefullyexaminetheearlyALstages,where
et al., 2018). There are also other methods to
mostoftheperformanceimprovementshappen. Fi-
deal with incomplete annotations, such as adopt-
nally, for the IE tasks with multiple output types,
ing local models (Neubig and Mori, 2010; Flan-
wemainlyfocusonthesecondrelationalsub-task
neryetal.,2011),max-marginobjective(Fernan-
and adopt a simple weighting setting to combine
desandBrefeld,2011), learningwithconstraints
theuncertaintiesofthetwosub-tasks. Moreexplo-
(Ningetal.,2018,2019;Mayhewetal.,2019)and
rations on the dynamic balancing of the two sub-
negativesampling(Lietal.,2022).
tasksinpipelinedmodels(RothandSmall,2008)
ALforstructuredprediction. ALhasbeenin- wouldbeaninterestingdirectionforfuturework.
vestigatedforvariousstructuredpredictiontasksin
References Eraldo R Fernand
 1 1 (cid:107) t − t(cid:107) 2 2 (cid:107) t − t(cid:107)
(cid:20) (cid:21) (cid:20) (cid:21)
=δ +αλ β E w w(cid:48) +αλ β E w w(cid:48)
t 1 1 f1...ft∼Pλ (cid:107) t − t(cid:107) 2 2 f1...ft∼Pλ (cid:107) t − t(cid:107)
=δ +α(λ β +λ β )δ
t 1 1 2 2 t
=(1+α(λ β +λ β ))δ
1 1 2 2 t
E.4 STABILITYOFDYNAMICSAMPLING
WerepeatthedescriptionofourAuxiliaryLearningwithDynamicSamplingSettinghereforeaseof
access.
18
PublishedasaconferencepaperatICLR2023
Setting: Wearegivenanauxiliaryobjectivef (;z) [0,1]withN samplesS =(z,...,z )
a
· ∈
a a 1 Na
fromthedistribution. AtanyiterationofSGD,wesampleachoiceofeithertheend-taskfunction
a
D
f or the auxiliary objective f according to the probabilities λ, λ λ + λ = 1. Given the
e a e a e a
|
chosenobjective,wesampleadata-pointandperformstochasticgradientdescent(SGD)basedon
thesampleddata-point.
AnequivalentwaytoinstantiatethisproceduretocreateS bydrawingN(cid:48) =N +N totalsamples
A e a
fromtheend-taskandauxiliarytaskaccordingto. S(cid:48) isthencreatedbyreplacing1end-task
sampleinS. Ateachstep,asampleisdrawnfromP aλ distrA ibution: z,z(cid:48) P,P andagradient
A i i ∼ SA S A(cid:48)
stepistakenonthefunctioncor
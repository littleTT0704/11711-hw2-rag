riever Besidestrainingdata,wemust Prompt2Modelisdesignedmodularlytosupport
identifyanappropriatemodeltofinetune. Wecast customizationofeachcomponentinourframework
thisasaretrievalproblem,whereeachmodelisrep- (describedin§2),butwehaveprovidedareference
resentedbyuser-generateddescriptionsandmeta- implementationtoenableimmediateadoption.
data such as popularity or tasks supported. The
3.1 PromptParser
referenceimplementationofourModelRetriever,
describedin§3.4,searchesagainstpretrainedmod- We parse the prompt into instruction and
els on Hugging Face (Wolf et al., 2020), but this demonstrationsfields(showninFigure2),where
theinstructionrepresentstheprimarytaskorobjec- Self-Consistency Decoding Given that LLM
tiveandthedemonstrationsexemplifythedesired maygeneratenon-uniqueorincorrectoutputsfor
behavior. Toachievethis,weutilizeanLLMwith the same inputs, we use self-consistency filtering
in-contextlearningtosegmentuserprompts, em- (Wangetal.,2022)toselectpseudo-labels. Specifi-
ployingtheOpenAIgpt-3.5-turbo-0613inour cally,wecreateaconsensusoutputforeachunique
experiments. If the instruction provided is iden- input by selecting the most frequent answer; in
tified to be in a language other than English, we thecaseofties,weheuristicallyselecttheshortest
translateittoEnglishusingtheDeepLAPI.2 answer. Thispromotesaccuracyofthegenerated
datasetwhileensuringuniqueexamples.
3.2 DatasetRetriever
Asynchronous Batching API requests are par-
To retrieve datasets for a prompt, we adapt the allelizedusingzeno-build (NeubigandHe,2023).
DataFinder system introduced by Viswanathan
We use additional mechanisms, such as dynamic
et al. (2023). By extracting user-generated
batchsizeandthrottling,tooptimizeAPIusage.
dataset descriptions for each dataset in Hugging
Face Datasets (Lhoest et
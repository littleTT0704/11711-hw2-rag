�feelindifferent’
somethingelse’
Table8: Processforhandlingmissingreaderannotations.
LabelType Misinfo↓ Real↑ Effectsize ganda detection models are trained using the set-
Pred 2.040 3.240 0.764 tingsgivenin(DaSanMartinoetal.,2019). BERT
Gold 2.531 3.213 1.380
modelshave345Mparameters.
Table9: Likelihoodofnewseventsspreading,i.e. the A.3.2 GenerativeModels
annotators’ratingforhowlikelyitistheywouldshare
For GPT-2, models are finetuned with a learning
orreadthearticlebasedontheshownnewsevent. For
rateof2e-5. Weusealearningrateof5e-5forT5.
“Pred”,weignoreheadlineswhereannotatorswereun-
sureaboutthelabel. Forthisandthefollowingtables, ForallmodelsexceptGPT-2largeweuseabatch
arrowsindicatethedesireddirectionofthescore. We sizeof16. ForGPT-2largeweuseabatchsizeof
useCohen’sdtocomputeeffectsize. 4. We use beam search with a beam size of 3 for
thegenerationtask. Generationmodelsaretrained
foramaximumof10epochsusingearlystopping
A.3 ExperimentalSetupandModel
basedondev. loss(inthecaseoftheGPT-2model
Hyperparameters
finetuned on cancer data we finetune for a single
All models are trained on either a single Quadro epoch). We optimize using AdamW (Loshchilov
RTX 8000 or TITAN Xp GPU. Average training andHutter,2019)andlinearwarmup. Modelsizes
timeforgenerativemodelsrangesfromapprox. 1 range from 124M parameters for GPT-2 small to
hour per epoch for T5-base to 4 hours for GPT-2 774MparametersforGPT-2large.
large. Inference time for models ranges from ap-
A.4 EffectofReaderPerceptiononArticle
prox. 10-20 minutes. Average training time for
SharingorReading
BERT models is approx. 30 minutes
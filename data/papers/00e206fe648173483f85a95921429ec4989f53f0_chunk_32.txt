definitionsforlabelsusedbyannotatorsin
Krippendorf’salpha 0.371 - theCNdataset,ascomparedtothedescriptionsin
AveragePrecision 0.275 - thei2b2/VAdatasetafterwhichtheyweremodeled.
AverageRecall 0.511 -
Table7reportsmeasuresforinter-annotatoragree-
AverageF1 0.342 -
IAA 0.368 0.73 ment for the CN dataset, compared to agreement
Table 6: Annotation agreement metrics for timed experi- reportedforcoreferenceannotationsinOntoNotes.
mentsofmentiondetectionandcoreferenceresolution.Inter-
Annotator Agreement (IAA) refers to a metric defined in
(Uzuneretal.,2012).Forcoreference,precision,recall,and
F1areaveragedoverstandardmetricsdefinedin§B. CNAnnotationAgreement
AgreementMetric Non-expertAnnotators OntoNotes
MUC 72.0 68.4
CEAFφ4 40.5 64.4
CEAFm 63.4 48.0
For a given number of mentions m, we gener- B3 57.8 75.0
ated models for min(max(6,15000/m),15) ran- Krippendorf’sMDalpha 60.5 61.9
domseeds. Theseboundswereselectedbasedon Krippendorf’sref.alpha 70.5 -
Table7: AnnotationagreementmetricsfortheCNdataset
preliminaryexperimentsassessingdeviation.
computedoverarandomsampleof20documents.Weachieve
agreementonparwithOntoNotes(Pradhanetal.,2012).
We use a learning rate of 2 10 5 for the en-
−
×
coder and 1 10 4 for all other parameters. We
−
×
train on the source domain for 20 epochs and on
the target domain for 20 epochs or until corefer-
enceperformanceoverthedevsetdegradesfortwo
consecutiveiterations. Trainingtimeforallmodels
rangesbetween80-120minutes,dependingonsize
ofdataset. WeusedV100,RTX8000,andRTX600
GP
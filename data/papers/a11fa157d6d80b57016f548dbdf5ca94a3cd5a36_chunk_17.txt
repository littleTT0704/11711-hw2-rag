andEisner,2021)be-
textminingbasedonConceptNetasshowninthe causetheyeitheraredifficulttofitinourproblem
table. The accuracy of 65% is at a comparable orrequiremoretrainingdataandfailwithonlythe
levelwiththatofCOMET(230K)andTransOMCS fewshotofexampleentitypairsinoursetting.
(18.4M),whichisreasonableespeciallyconsider-
We harvest 1000 tuples for each Human rela-
ing our method solely uses an LM as the source
tion, and evaluate them with human annotation.
of knowledge without any external training data,
The annotation results are presented in Table 3
bringingflexibilitytodynamicallyincorporatenew
(We also list the detailed results per relation in
relations. Besides,forourRobertaNetonConcept-
Table5forreference)Our TOP-1 PROMPT signif-
Net relations, although the numbers listed in the
icantly improves the accuracy up to 9% over the
tablearenotsimplycomparable,wecanstillfind
HUMAN PROMPT,demonstratingtheeffectiveness
thatRobertaNetachievessimilaraccuracyandab-
of our prompt searching algorithm in generating
solutelyhighernoveltycomparingwiththeknowl-
high-quality prompts. MULTI-PROMPTS further
edge from COMET, which is already finetuned
improvestheaccuracybyanadditional4%,indicat-
usinglargenumberofknowledgetermsunderthe
ingthatthecombinationofdiversepromptsbetter
samesetofConceptNetrelations. Further,ourre-
capturesthesemanticsofarelation. However,the
sultsonthe"human"relationsetdemonstratethat
methodutilizingtheoptimizedpromptby AUTO-
ourRobertaNetkeepsworkingcomfortablyonour
PROMPTresultsinloweraccuracythantheuseof
highly realistic relations of user interests, includ-
humanorsearchedprompts. Thiscanbeattributed
ingthecomplexonesasdescribedinsectionÂ§3.2.
totheinsufficientnumberofexampleknowledge
Weshowcaseknowledgesamplesharvestedfrom
tuples used to learn effective prompts for the de-
D
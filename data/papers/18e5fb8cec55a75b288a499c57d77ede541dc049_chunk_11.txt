bothintermsoftheir
language models and masked language models (MLM). format(questionanswering,pronounresolution,naturallan-
Specifically,weuseGPT-2andRoBERTatoselectthebest guage inference), as well as their type of knowledge (e.g.,
answer candidate. Given a context C, a question Q, and a social or physical knowledge). Secondly, we prefer larger
listofansweroptions(A ;A :::),weconcatenateC andQ taskdatasetsthataremanuallyconstructed.Forthisreason,
1 2
witheachansweroptiontobuildinputsequences(T ;T :::). we do not include datasets like COPA (Gordon, Kozareva,
1 2
WealsousetemplatestoconvertasequenceT intoanatu- and Roemmele 2012), or HellaSwag (Zellers et al. 2019).
ral language sentence following (Shwartz et al. 2020). For Weoptforthefollowingfivetaskdatasets:
example,wetransformthesequence:[C]WhatwillXwant
1. Abductive NLI (aNLI) (Bhagavatula et al. 2019) is
todonext?[A ]into:[C],asaresult,Xwantto[A ].The
i i posedasanaturallanguageinferencetask.Giventhebegin-
score S for the resulting sequence using an auto-regressive
ningandtheendingofastory,thetaskistochoosethemore
LMiscomputedasfollows:
plausiblehypothesisoutoftwooptions.Thedatasetconsists
n ofnearly170kentries.
1 X
S (T)=(cid:0) logP (t jt :::t ) (1)
LM n i 1 i(cid:0)1 2.CommonsenseQA(CSQA) (Talmoretal.2019)eval-
i=1 uates a broad range of common sense aspects. Each entry
where n is the number of tokens in the sequence and P is containsaquestionand5answercandidates.Thequestions
the conditional probability provided by the LM. To evalu-
arecrowdsourcedbasedon
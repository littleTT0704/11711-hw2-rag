icalapproachestothisprobleminnaturallanguage
search around the current parameters and around the max- processing are based around the policy gradient estimator
imumlikelihoodgradient,witheachdirectionweightedby (Williams1992),suchasShenetal.(2016);Ranzatoetal.
itsimprovementinthetaskloss.MGSshiftssamplingtothe (2016);Bahdanauetal.(2017);Yuetal.(2017).Thisestima-
parameterspace,andscorescandidatesusinglossesthatare
torisusedtooptimizeanarbitrarytasklossbyintroducing
pooledfrommultiplesequences.Ourexperimentsshowthat
stochasticityviaautoregressivesamplingintheactionspace,
MGS is capable of optimizing sequence-level losses, with
whichisacriticaldownsideinNLP,wheretheactionspace
substantial reductions in repetition and non-termination in
(vocabulary)islargeandthesequence-levelrewardissparse.
sequencecompletion,andsimilarimprovementstothoseof
minimumrisktraininginmachinetranslation. Theestimatorâ€™svariancegrowswiththesequencelength,ne-
cessitatingaparameterizedbaselineoraheuristicsampling
scheduleinpractice,whilerequiringinitializationfromapre-
1 Introduction
trainedmodel.Recently,theeffectivenessofthesemethods
Neuralautoregressivesequencemodelsareusedinavariety in NLP has been called into question (Caccia et al. 2020;
ofnaturallanguageprocessing(NLP)tasks,suchasmachine Choshenetal.2020).
translation(Bahdanau,Cho,andBengio2015),summariza-
An alternative class of methods optimize a black-box
tion(Rush,Chopra,andWeston2015),dialoguemodeling
function without requiring gradient information. Of these,
(Vinyals,Quoc,andLe2015),andtextcompletion(Sutskever,
estimation-of-distribution algorithms, including the cross-
Martens,andHinton2011;Radfordetal.2018;Holtzman
entropymethod(Rubinstein1999),evolutionarystrategies
etal.2019;Wellecketal
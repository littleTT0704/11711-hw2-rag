 indicating the
potential for improvement on the proposed benchmark.
6.1 Limitations
One drawback of our unified format is the difficulty of evaluating models. In
our work we use F1 for lack of a better alternative. F1 likely over-estimates
performance, e.g., given the gold answer “2 apples”, the predicted answers “2”
and “apples” receive the same score, though the former is better.
L¯ila contains 23 tasks which are created from 20 datasets and 44 sub-
datasets. There is scope to add more mathematical reasoning datasets (e.g.,
theoremproving.) Theflexibleunifiedformatof L¯ilaallowsforfutureextensions.
Additionally, our categorization provides a way to identify areas for extension.
For instance, we only have 1 dataset for linear algebra, which happens to not
use natural language, and takes the form of generative QA. Our benchmark
will benefit from future linear algebra additions, perhaps with word problems
formatted as fill-in-the-blank questions.
14
References
GillesAdda,BenoîtSagot,KarënFort,andJosephMariani.2011. Crowdsourcing
for language resource development: Critical analysis of amazon mechanical
turk overpowering use. In 5th Language and Technology Conference.
ArmenAghajanyan,AnchitGupta,AkshatShrivastava,XilunChen,LukeZettle-
moyer, and Sonal Gupta. 2021. Muppet: Massive multi-task representations
with pre-finetuning. arXiv preprint arXiv:2101.11038.
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and
HannanehHajishirzi.2019.Mathqa: Towardsinterpretablemathwordproblem
solving with operation-based formalisms. arXiv preprint arXiv:1905.13319.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
Le, et al. 2021. Program synthesis with large language models. arXiv preprint
arXiv:2108.
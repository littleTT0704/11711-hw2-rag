ausesthespeakermodeltodevelop
longer,morefluent,andmorepreciseutterancestodistinguishbetweenpotentialreferents,although
thesedonotalwaystranslatetogainsinreferentialgameperformance. Theseresultssuggestcon-
tributions to language acquisition from both ToM and environmental pressures in this setting. We
stillfindsignificantgapsbetweenthelanguagethatourspeakermodelacquiresandhumancaptions.
Additionally,werestrictboththevocabularyandthemaximumlengthofourspeakers’utterances.
These both suggest that there is still room for improvement in this class of models. However, we
hopeourresultsstillhintatbothbettertrainingmethodsforpragmaticlanguagemodelsandadeeper
computationalunderstandingofhumanlanguageacquisition.
2 IMAGE REFERENTIAL GAME ENVIRONMENT
Following Lazaridou et al. (2016); Lowe et al. (2019); Zhu et al. (2022), we consider image ref-
erentialgameswithrealworldimages, whereaspeakerandalistenercollaborateonidentifyinga
targetimagex ∈ C amongdistractorsrandomlysampledfromasetofcandidatesC. Theidentity
ofthetargetisknownonlytothespeaker. ThespeakergeneratesanEnglish-languageutteranceu
2
PublishedasaconferencepaperatICLR2023
thatdescribesthetargetimage,whichisthenpassedtothelistener. Thelistenerwilleitherselectan
imagexˆ, inthecasewheretheyunderstandtheutterancewithahighenoughprobabilityorrefuse
to act, in the case where they do not. The listener can also determine whether or not to provide
feedback(linguisticinputintheformoftheground-truthcaptionofthetargetimage)tothespeaker.
Speakertrainingismotivatedbyarewardfunctionthatseekstomodelthecommunicativegoalof
directingthelistenertothecorrectreferent. Thespeakergetsapositiverewardof1ifthelistener
chooses the target image and a penalty of −1 if the listener chooses the wrong target image. Ad-
ditionally,asmallerpenaltyw isappliedtothespe
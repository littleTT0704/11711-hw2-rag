 we predict whether
datagenerationrequiringonlyahandfulofexam-
existing intervention schemes would be use-
plesfromthetargetdomain. Whilemanyexisting
fulinadaptingfromagivensourcemodelor
workshaveleveragedquestiongenerationmodels
would it better to collect annotations in the
forcreatingadditionaltrainingdata,thesemodels
targetdomain?
aretypicallytrainedonthesourcedomaindataand
3. What interventions or adaptation strategies suffer the same generalization shortcomings. In-
can we perform to improve ODQA perfor- stead,inspiredbythestrongperformanceoflarge
manceinOODtesting? language models (LLM) for summarization, we
generatesentencesbypromptingtheLLMwitha
Followingtheaboveresearchquestionswemake handfulofexamplesfromthetargetdomain. We
fourprimarycontributioninthiswork. convert the generated sentences into cloze style
First, in Section 2 we aggregate a set of seven QApairsandshowthatthistechniqueisespecially
ODQA datasets, spanning five different domains effective when zero shot adaptation methods fail
forevaluatingdomaingeneralizationofanODQA tocapturethetargetdomaindistribution,yielding
modeltrainedongeneralpurpose. InSection4,we improvementsofupto24%inF1.
use this test-bed to show that most SotA ODQA
modelsfailtogeneralize,andgoontoanalyzethe 2 BackgroundandSetup
failuremodesforOODgeneralization. Forexam-
An open-domain (ODQA) model learns interac-
ple, we observe that the retriever model’s perfor-
tions among three random variables: question
manceisquitesensitivetothetypeofentitiesand
(Q), answer (A) and context (C). For a given
lengthofpassagesseenattraining,andadditionally,
q ∈ Q, first the retriever R returns a set of pas-
in∼ 65%ofcaseswheretheanswerstringappears
sages,c = R(q,C). Thesepassagesarethensent
intheretrievallist(oneofthemostcommonlyused q
toanansweringmodelM(alsoknownasreader)
metricsforretrievalaccuracy
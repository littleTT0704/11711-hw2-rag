,51],angularalignmentregularization[18,48,49],dis-
representationsofsemanticconceptsthatareinvariantto tributionallyrobustoptimization[7]anddataaugmentation
nuisancefactorsandgeneralizableacrossdomains.Compre- [4,77,82]. However,havingnoaccesstotargetdomaindata
hensiveexperimentsonpopularbenchmarksshowthatDDG posesgreatchallenges. Twomainlinesofresearchseekto
can achieve competitive OOD performance and uncover addressthis. First,trainingdomainlabelsareassumedtobe
interpretablesalientstructureswithindata. availablein[3,23,47,60,66]suchthatthedivergenceto
differentdomainscanbeminimized.However,thesedomain
labels are often impractical or prohibitively expensive to
1.Introduction obtain[28]. Moreover,itisnon-trivialtominimizedomain
divergencewithdomainadversarialtrainingwhichisnotori-
Learning representations that can reflect intrinsic class
ouslyhardtoconverge[63]. Thesecondlineofworkstries
semanticsandalsorenderstronginvariancetocross-domain
tomodelthecross-domaindistributionshiftsandcapturethe
variationisofgreatsignificancetorobustnessandgeneral-
semanticinvariance[37,62,78]. However,ithasbeenfound
izationindeeplearning. Despitebeingempiricallyeffective
in[27]thatthisgoalcanbeverydifficulttoachieve. What
onmanyvisualrecognitionbenchmarks[65],modernneu-
makestheproblemevenmorechallengingistheinconsis-
ralnetworksarestillpronetolearningshortcutsthatstem
tencyoftheevaluationprotocol. Surprisingly,[27]shows
from spurious correlations [24], resulting in poor out-of-
that even the standard empirical risk minimization could
distribution(OOD)generalization. Totacklethischallenge,
outperformmanyrecentlyproposedmodelsundercertain
domaingeneralization(DG)hasemergedasanincreasingly
conditions. Motivatedbythesechallenges,weaimtodisen-
important task where the goal is to
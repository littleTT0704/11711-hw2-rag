Towards Principled Disentanglement for Domain Generalization
HanlinZhang1,* Yi-FanZhang2,* WeiyangLiu3,4 AdrianWeller3,5 BernhardScho¨lkopf4 EricP.Xing1,6
1CarnegieMellonUniversity 2ChineseAcademyofScience 3UniversityofCambridge
4MaxPlanckInstituteforIntelligentSystems,Tu¨bingen 5AlanTuringInstitute 6MBZUAI *EqualContribution
Abstract
...
A fundamental challenge for machine learning models
is generalizing to out-of-distribution (OOD) data, in part
duetospuriouscorrelations. Totacklethischallenge, we Variation Encoder v 1 v n Generator
first formalize the OOD generalization problem as con-
v
strainedoptimization,calledDisentanglement-constrained
... Domain Generalization (DDG). We relax this non-trivial
constrainedoptimizationproblemtoatractableformwith s
finite-dimensionalparameterizationandempiricalapproxi- Semantic Encoder s 1 s n Minimize semantic gap
mation.Thenatheoreticalanalysisoftheextenttowhichthe
Figure1.AnillustrationofDDGbasedondisentanglementofdigitlabels
abovetransformationsdeviatesfromtheoriginalproblemis (semantics)androtatedangles(variationacrossdomains).DDGseeksto
provided. Basedonthetransformation,weproposeaprimal- minimizethesemanticdifferenceofthegeneratedsamplesfromthesame
classwhilediversifyingthevariationacrosssourcedomains.
dualalgorithmforjointrepresentationdisentanglementand
domaingeneralization.Incontrasttotraditionalapproaches
based on domain adversarial training and domain labels, beenmadefromadiversesetofdirections,suchasdomain
DDG jointly learns semantic and variation encoders for adaptation[5,6,23,84],self-supervisedlearning[12,29],
disentanglement, enabling flexible manipulation and aug- causalinference[56,61,67,81],invariantriskregularization
mentation on training data. DDG aims to learn intrinsic [3,42
etal.,2019)
tooltoobtaindiversediagnosticsdataforotherwise hasbeenpretrainedontheWikipediasof104lan-
resource-starvedlanguages. Weviewparticipatory guagesusingMLM.
research(∀etal.,2020)withnativespeakerstocre- XLM-R XLM-R Large (Conneau et al., 2020)
atetemplate-basedtestcasestestingforlanguage- usesthesameMLMobjectivewithalargermodel,
specificbehaviourasparticularlypromising. and was trained on a magnitude more web data
Multilingual EXPLAINABOARD The standard from100languages.
practiceinleaderboardsistoaverageperformance mT5 Multilingual T5 (Xue et al., 2021) is an
across different settings (Wang et al., 2019b,a). encoder-decodertransformerthatframesNLPtasks
While this provides discriminative power, it has ina“text-to-text”format. Itwaspre-trainedwith
limited utility for examining the relative advan- MLMonalargemultilingualwebcorpuscovering
tages of systems, the characteristics of different 101languages. WeemploythelargestmT5-XXL
datasetsandlanguages,andhowthesefactorsrelate variantwith13Bparameters.
toeachother. Toprovidemoregranularevaluation Translate-train To evaluate the impact of MT,
capabilities, weextendFuetal.(2020);Liuetal. we fine-tune mBERT on translations of English
(2021)’sEXPLAINABOARDtothetaskcategories trainingdatafromHuetal.(2020). Wecreatenew
andlanguagesinXTREME-R. EXPLAINABOARD translations for the XCOPA and SIQa data using
providesamorenuancedimpressionofamodel’s anin-houseMTsystem.5
performanceonataskbydefiningtask-specificat- Translate-trainmultilingual Inaddition,wefine-
tributes (e.g. entity length for NER). The test set tunebothmBERTandmT5onthecombinedtrans-
is partitioned into different buckets based on the l
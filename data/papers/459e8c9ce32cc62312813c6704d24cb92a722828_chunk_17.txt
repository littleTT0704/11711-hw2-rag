odinglength
of1.3timestheground-truthlength.Modelsareevaluated
Wealsoreportthetaskloss,averagelengthofthegenerated
withamaxdecodinglengthof500tokens.SeetheAppendix
continuations,andtheperplexity.
formoredetails.
FortheMRTandPGbaselineswefinetuneusing4samples.
Forpolicygradientweusedanexponentialmovingaverage Effectonsequence-leveltaskloss. Table1showsthetask
baseline. Each method is stochastically mixed with MLE lossesandmetricsforthebaselinefine-tunedmodel(MLE)
accordingtoahyper-parameter(cid:11) 2 [0;1]:givenatraining and each model trained with MGS to optimize the indi-
batch,wedrawz (cid:24) Bernoulli((cid:11))anduseMLEwhenz is cated task loss (MGS-loss). The baseline has the highest
zero.Weperformedagridsearchusing(cid:11)2f0:1;0:3;0:5g, tasklosses,andahighdegreeofnon-termination(.387)and
selecting(cid:11)basedonthevalidationtasklossthatthemodelis repetition(.538).MGS-LMsubstantiallyreducestheLMtask
optimizing.IntheAppendixwealsoreportresultsforMRT loss(59.1),alongwithnon-termination(.012)andrepetition
andPGwithoutstochasticallymixingMLEandanablation (.035).
ofthechoiceofMRTcandidates. MGS-editachieveslowereditdistance(.928)thanMGS-
Ourmainresultsarereportedwithgreedydecoding;refer LM, while also substantially reducing LM task loss, non-
totheAppendixforresultswithancestralsampling. termination,andrepetition.BothMGSvariantsresultinshort
sequences,especiallyMGS-LM,whichisexpectedduetothe
biastowardsshortsequencesinMLE-trainedLMs(Stahlberg
Tasklosses. Weexperimentwithtwosequence-leveltask andByrne2019).
losses. We define a
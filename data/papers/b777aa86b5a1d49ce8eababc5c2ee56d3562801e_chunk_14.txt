, and locality residual bottlenecks (MBConvs) (Sandler et al.,
sensitive hashing (Dai et al., 2020; Kitaev et al., 2018;Sunetal.,2020),andsqueezeandexcitation
2020;Wangetal.,2020b;Xiongetal.,2021). layers(Iandolaetal.,2016)haveallbeenproposed
We examine the performance of the Funnel assubstitutesfordenseconvolution,self-attention,
Transformerwhichappliesaveragepoolingtoper- andlinearoperations. However,inpracticethese
formsequencelengthreductionevery4layers. The operationsoftenlackthehighlyoptimizedframe-
modelachievescomparableaccuracytoBERTon work and hardware support developed for more
downstreamtaskswhilerequiring42%fewertotal standardoperationsandasaresultexhibithigher
MAC operations through sequence length reduc- per-floplatencyandpoormemoryutilization.
tion. Thismodelachievessimilardownstreamtask We examine this assumption with models that
performancetoBERT-Baseandtrains33%faster use grouped convolutions in SqueezeBERT (Ian-
basedonwall-clocktime. dola et al., 2020) inverted bottleneck layers and
MobileBERT(Sunetal.,2020).
Analysis WhileFunnelTransformerreducesto-
Analysis To achieve comparable accuracy on
talFLOPsandobtainssubstantialspeedupsinlarge-
downstreamlanguagetaskswithlow-FLOPopera-
scale training, this speedup does not translate to
tions,efficientBERTvariantsrequiremuchdeeper
increasedspeedofinferenceasseeninFigure7. In
modelarchitectureswhichresultsinmuchhigher
practice,theaveragepoolinglayersusedtoperform
fixedframeworkoverheadasseeninFigure8. Ad-
sequence length reductions add additional opera-
ditionally, these models exhibit worse FLOP per
tions to the computation graph and increase the
secondduetopoormemoryutilizationcompared
modelâ€™sframeworkoverhead. Atlowbatchsizes,
to conventional dense linear and convolution op-
FunnelTransformerisframeworkboundatamuch
erations. Theseoperationscanleadt
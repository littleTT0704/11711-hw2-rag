,pp.1123–1128.
means that the robot can readily justify its decisions even [9] R. Paul, J. Arkin, N. Roy, and T. Howard, “Efficient grounding of
abstract spatial concepts for natural language interaction with robot
when it does make a mistake, facilitating a human user
manipulators,” in Proceedings of the 2016 Robotics: Science and
providing a new instruction that considers this mistake. SystemsConference,June2016.
Overall, these results show that we can learn representations [10] D.Arumugam,S.Karamcheti,N.Gopalan,L.L.Wong,andS.Tellex,
“Accurately and efficiently interpreting human-robot instructions of
forataskthataresufficientforplanningandexecutionpurely
varying granularities,” in Proceedings of the 2017 Robotics: Science
from language and raw sensory data. andSystemsConference,2017.
We performed an ablation analysis on the best prediction [11] A. Srinivas, A. Jabri, P. Abbeel, and S. Levine, “Universal planning
networks,”inProceedingsoftheInternationalConferenceinMachine
models to determine how much they use information from
Learning(ICML),2018.
different layers. In particular, we see similar performance [12] M. Garnelo, K. Arulkumaran, and M. Shanahan, “Towards deep
when training without the image loss (89.5% successful symbolic reinforcement learning,” in Deep Reinforcement Learning
WorkshopatNIPS,2016.
on held out test data) and without the image and object
[13] O.Nachum,S.Gu,H.Lee,andS.Levine,“Data-efficienthierarchical
losses (88.6% successful). This suggests that our image reinforcementlearning,”2018.
reconstruction loss may help, and certainly does not have [14] J. Tremblay, T. To, A. Molchanov, S. Tyree, J. Kautz, and S. Birch-
field, “Synthetically trained networks for learning human-readable
a negative impact.
plans from real-world demonstrations,” International Conference on
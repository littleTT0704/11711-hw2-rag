 deep models
forsemanticcompositionalityoverasentimenttree-
bank. In Proceedings of the 2013 Conference on Hyperparameter Assignment
EmpiricalMethodsinNaturalLanguageProcessing model GPT-2(S/M/L)
(EMNLP). numberofparameters 124M/355M/774M
numberofsteps 1-3epochs
AkhileshSudhakar,BhargavUpadhyay,andArjunMa- effectivebatchsize 512
heswaran. 2019. “Transforming” delete, retrieve, blocksize 128
generate approach for controlled text style transfer. learningrateoptimizer Adam
In Proceedings of the 2019 Conference on Empiri- Adamepsilon 1e-8
Adaminitiallearningrate 5e-5
cal Methods in Natural Language Processing and
learningratescheduler linearwithnowarmup
the 9th International Joint Conference on Natural
weightdecay 0
LanguageProcessing(EMNLP-IJCNLP).
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di- Table5: Hyperparametersforfinetuning(anti-)experts
nan,KyunghyunCho,andJasonWeston.2020. Neu- for DEXPERTS and continued pretraining in domain-
ral text generation with unlikelihood training. In adaptive pretraining (DAPT). We finetune the senti-
ProceedingsoftheEighthInternationalConference ment(anti-)expertsandallDAPTmodelsfor3epochs,
onLearningRepresentations(ICLR). andthetoxicity(anti-)expertsforoneepoch.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, ClementDelangue, AnthonyMoi, Pier- Thefinetuningtimeforeachmodelsizeisshown
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- inTable6.
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, 8https://openai.com/api/
Size Non-toxic Toxic Positive Negative forfinetuningourexpertsandanti-expertsaregiven
inTable9andTable10.
Small 2h
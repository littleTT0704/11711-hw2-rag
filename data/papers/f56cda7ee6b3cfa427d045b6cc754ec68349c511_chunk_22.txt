
jectivetodiscouragethedialoguemodelfromgen-
workerswas$12.26. Thein-houseannotatorswere
eratinginappropriateresponses. Toenhancesafety,
paid$13perhour. Finally,wenotethatclassifiers
Xu et al. (2020) train chatbots to avoid sensitive
trained on our dataset are fallible and should be
discussionsbychangingthetopicoftheconversa-
usedwithcarefulconsideration(Sapetal.,2019;
tion. In contrast, we tackle contextual offensive
Dixonetal.,2018).
languagebyfine-tuningmodelstogenerateneutral
andsaferesponsesinoffensivecontexts. Acknowledgments
8 Conclusion Wewouldliketothanktheanonymousreviewers
forprovidingvaluablefeedbackonanearlierdraft
To better understand the contextual nature of of- of this paper. This material is based in part on
fensive language, we study the stance of human researchsponsoredbytheNSF(IIS-1845670)and
and model responses in offensive conversations. DARPAviatheARO(W911NF-17-C-0095). The
We create TOXICHAT, a corpus of 2,000 Reddit viewsandconclusionscontainedhereinarethose
conversationsaugmentedwithresponsesgenerated of the authors and should not be interpreted as
bytwodialoguemodelsandcrowd-annotatedwith necessarilyrepresentingtheofficialpolicies,either
targeted-offensive language and stance attributes. expressedorimplied,ofNSF,ARO,DARPAorthe
Classifiers trained on our corpus are capable of U.S.Government.
automatically evaluating conversations with con-
textuallyoffensivelanguage.
References
OuranalysesconsistentlyfindthatRedditusers
agree much more with offensive contexts. This
Soumya Barikeri, Anne Lauscher, Ivan Vulic´, and
trendcouldbeexplainedbythetendencyofsocial- Goran Glavaš. 2021. RedditBias: A real-world re-
source for bias evaluation and debiasing of conver- IEEE/CVFConferenceonComputerVisionandPat-
sational language models. In Proceedings of the ternRecognition,pages9
 we investigate
sentiment.
the relationship between the dataset size used to
trainthe(anti-)expertsanditseffectivenessatsteer- 4.1 Method
ing the base model. We finetune GPT-2 Large
Weusethesamepretrainedmodelfrom§3,GPT-2
6https://openai.com/api/ Large,asourbaseLM.WefinetuneGPT-2(Small,
%PositiveSentiment Fluency(Ó) Diversity(Ò)
Target
Model Positive Neutral Negative
Sentiment Outputppl. Dist-1 Dist-2 Dist-3
prompts prompts prompts
DEXPERTS(large) 94.46 36.42 45.83 0.56 0.83 0.83
DEXPERTS(medium) 94.31 33.20 43.19 0.56 0.83 0.83
DEXPERTS(small) 94.57 31.64 42.08 0.56 0.83 0.84
Positive GeDi 86.01 26.80 58.41 0.57 0.80 0.79
Positiveexpert 79.83 43.80 64.32 0.59 0.86 0.85
DAPT 77.24 14.17 30.52 0.56 0.83 0.84
DEXPERTS(anti-only) 60.72 4.43 46.00 0.65 0.80 0.78
CTRL 61.81 18.88 43.79 0.51 0.83 0.86
PPLM(10%) 52.68 8.72 142.11 0.62 0.86 0.85
GPT-2 99.08 50.02 0.00 29.28 0.58 0.84 0.84
PPLM(10%) 89.74 39.05 181.78 0.63 0.87 0.86
CTRL 79.05 37.63 35.94 0.50 0.83 0.86
DEXPERTS(anti-only) 93.75 34.05 44.23 0.65 0.81 0.78
DAPT 87.43 33.28 32.86 0.58 0.85 0.84
Negative
Negativeexpert 61.67 24.32 65.11 0.60 0.86 0.85

ihalcea,GermanRigau,LarraitzUria,andJanyce
Wiebe. 2015. SemEval-2015 task 2: Semantic tex-
duce padding and extra computation. We use a
tual similarity, English, Spanish and pilot on inter-
batch size of 64 for each model. The number of pretability. In Proceedings of the 9th International
sentences embedded per second is shown in Ta- Workshop on Semantic Evaluation (SemEval 2015),
ble5. pages 252–263, Denver, Colorado. Association for
ComputationalLinguistics.
From the results, we see that our model is eas-
ilythefastestonGPU,sometimesbyanorderof Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
magnitude. Interestingly, using a single core of Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
CPU, we achieve similar speeds to inference on
Wiebe. 2014. SemEval-2014 task 10: Multilingual
GPU, which is not the case for any other model.
semantic textual similarity. In Proceedings of the
Moreover, we repeated the experiment, this time 8thInternationalWorkshoponSemanticEvaluation
(SemEval 2014), pages 81–91, Dublin, Ireland. As-
16Resultsarecopiedfrom(Tranetal.,2020). sociationforComputationalLinguistics.
385
EnekoAgirre,CarmenBanea,DanielCer,MonaDiab, Muthu Chidambaram, Yinfei Yang, Daniel Cer, Steve
Aitor Gonzalez-Agirre, Rada Mihalcea, German Yuan, Yunhsuan Sung, Brian Strope, and Ray
Rigau, and Janyce Wiebe. 2016. SemEval-2016 Kurzweil. 2019. Learning cross-lingual sentence
task 1: Semantic textual similarity, monolingual representationsviaamulti-taskdual-encodermodel.
and cross-lingual evaluation. In Proceedings of the In Proceedings of the 4th Workshop on Represen-
10th International Workshop on Semantic Evalua- tation Learning for NLP (Rep
To train CTG models, we need conversations
isoffensive. Usingthistestset, ourCTGmodels
with their last response labeled with control at-
arecomparedagainstDGPT-medium,GPT-3,and
tributes. Therefore,weextract5millioncomment
Blenderinbothautomaticandhumanevaluations.
threads, similar to §3, and retrieve offensiveness
andstancepredictionsusingourbestDGPTmodel- 6.2 AutomaticEvaluation
basedOffensiveandStanceclassifiers(§5.4).
Anidealdialoguemodelshouldhavediverse,en-
Tominimizeclassificationerrors,weusehighpreci-
gagingandsaferesponses. Thus,weevaluatethe
sionpredictionsbyselectingappropriatethresholds
responsesgeneratedbyallthecandidateconversa-
fordifferentclassificationprobabilities.12 Foreach
tionmodelsusingthefollowingautomaticmetrics,
thread, we retain Offensive prediction of the
Distinct-1,2 is the ratio of unique unigrams and
lastutteranceandStancepredictionbetweenthe
bigramstothetotal.
lasttwoutterances.
%Badispercentageofgeneratedresponsescon-
For all 3 proposed control experiments, we tainingprofaneword/phrasesidentifiedbyToxicity
first create samples of L ≈ 250,000 high- Triggers(Zhouetal.,2021,similarto§4).
precision classifier labeled threads in the format %Offispercentageofresponsespredictedoffen-
{(x i,ct i,y i)}L i=1 (label-controlled data). Herex i sivebytheDGPT+Offensiveclassifier.
is the thread without the last utterance, ct i is the %Agree,%Neutralarepercentagesofgenerated
classifier labeled control token and y i is the last responses predicted agree or neutral respectively
utteranceorresponsetox i. Wediscard‘Disagree’ bytheDGPT(CB foc)Stanceclassifier.13
stanceresponses,asweonlyfoundabout10,000 Table3containstheresultsfromourautomatic
high-precision disagreeing responses.
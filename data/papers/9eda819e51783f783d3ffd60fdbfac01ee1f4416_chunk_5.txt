-tuningoutperformsadapter-
addedbydifferenttuningstrategies.
tuningwithextremelyfewsamplesoverdifferent
Basedonwhetherandwhenparametersθ and
plm
languages §3.1. (2) Parameter-efficient tuning is
θ will be tuned, different tuning strategies as
add
possible to fail in the supervised transfer setting
illustratedinFig.2havebeenexplored,whichwe
(§3.2),wherepre-trainedlanguagemodelsarefine-
willdetailbelowforthebetterintroductionofmulti-
tuned on the source languages whose scripts are
lingualtuningstrategies.
distantfromthetargetlanguage’s. (3)Addinglan-
guagespecificadaptersorprefixeswhileaddition- PLMFine-tuning Thisisoneofthemostcom-
ally tuning the PLM’s parameters, can maintain montuningstrategiesthataimtotuneallofthepa-
multi-lingualPLMfine-tuning’sadvantageofshar- rametersθ. WhilePLMfine-tuninghasachieved
plm
inginformationamonglanguages,aswellaspre- strong performance on many benchmarks, one
serving private parameters for each language to major limitation lies in the requirement of large
trainingsamples,whichisnotfeasibleinthelow- trainingsamplesfromdifferentlanguagesarepro-
resourcescenario. vided. Summarizationsystemsshareonemultilin-
To alleviate this issue, parameter-efficient tun- gualpre-trainedlanguagemodelwhoseparameters
inghasbeenextensivelyexploredrecently,among canbeupdatedbyanysystem.
whichweselecttworepresentativemethodswhich
MultilingualParameter-efficientTuning(MPE)
areinitiallydesignedforgenerationtasks,consis-
Inthisframework,additionalprivateparameterized
tentwithourgoal.
modulessuchasprefixoradapterareintroducedfor
Adapter-tuning Adapter-tuningaddsadditional each system besides one shared multilingual pre-
lightweightlayersbetweenthelayersofanexisting trained language model, whose parameters keep
PLM.Althoughthereisav
.3and
of OOD generalization, and bad worst-case performance Fig.10binAppendixDshowthatDDGproducessamples
is tightly connected to issues like disparity amplification withdiversestyles. Incontrast,Fig.8inAppendixDshows
[28]. The performance gain of DDG is larger under the that it is much more difficult for heuristic-based methods
variation-richdatasetPACS.Thismakesintuitivesensesbe- suchasAugMixtogeneratesampleswithdiversestylesfor
causeDDGisabletobettercaptureinter-domainvariations training. Thequalitativeresultsvalidatetheeffectivenessof
forimprovingOODgeneralization. DDGasanautomaticdataaugmentationmethod.
4.3.EmpiricalAnalysesandAblations 5.ConcludingRemarks
Effectofdataaugmen- 100 Weproposeanoveldisentangledlearningframeworkfor
tation.Wefirstevaluatethe Baseline domaingeneralization, withboththeoreticalanalysesand
95 w/ Augmix
effectofdataaugmentation w/ ours practicalalgorithmicimplementation. Byseparatingseman-
by comparing our learned 90 tic and variation representations into different subspaces
dataaugmentationnetwork 85 whileenforcinginvarianceconstraints,DDGyieldssuperior
withaheuristic-basedaug- OOD performance with improved empirical convergence
80
mentationmethodAugMix andalsoyieldsinterpretableandcontrollablegenerativere-
[30]. Fig. 4 shows that 75 A C P S Avg sults. Inthiswork,weonlyconsiderthedisentangledeffects
theconstraintsoptimization Figure4.DataaugmentationonPACS between semantic and variation factors since it is hard to
brings great performance withdifferenttargetdomain. provideknowngenerativevariationfactorsthatmanifestthe
gainovervanillaERMand distribution shifts precisely. It remains an open problem
adataaugmentationheuristicsAugmix,especiallytheworst- toimprovethedisentanglementbetweendifferentvariation
case (i.e. the S domain in PACS) performance. The ef- factorswithlimitedsupervisionandevaluatethetreatment
fectivenessofthedataaugmentationprocedureinDDGis effect
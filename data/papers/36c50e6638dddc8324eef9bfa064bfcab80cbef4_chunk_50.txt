 al., 2019) on PROSOCIAL- guageandstance.
DIALOG without pre-training on other datasets. Descriptionsforbaselinemodels. BlenderBot
TheNormTransformerisaGPT-2-XLmodelpre- 2 (Komeili et al., 2021) is a dialogue agent fea-
trained on the Social Chemistry dataset (Forbes turing long-term memory and Internet searching
etal.,2020). DialoGPT(Zhangetal.,2020)isalso capability. Instruct GPT-3 (Ouyang et al., 2022)
a GPT-2 dialogue model pre-trained on a Reddit isalarge-scalepre-trainedlanguagemodelexplic-
corpus. T5isasequence-to-sequenceTransformer itlytrainedtofollownaturallanguageinstructions
modelthatshowsgreatperformanceinvariousgen- better. Itisalsoreportedlyknowntobemuchless
erativetasks. toxic and biased than the GPT-3 (Ouyang et al.,
2022).
C.3 ResponseGeneration
D.2 ImprovingProsocialityofPre-trained
Detailsofhumanevaluation.
LanguageModelswithCanary
1. Prosociality: “Whichresponsebetterimplies Method. To obtain vanilla outputs from a PLM,
that the other speaker should behave proso- weconstructabasicpromptP withdialoguecon-
0
cially,ethically,andfollowsocialnorms?” textcasfollows: “Thefollowingisaconversation
PROSOCIAL Wizardof Empathetic Blended
DailyDialog TopicalChat PersonaChat
Model DIALOG Wikipedia Dialogues SkillTalk
PPL F1 PPL F1 PPL F1 PPL F1 PPL F1 PPL F1 PPL F1
GPT-2 8.30 29.38 11.33 14.46 13.54 17.81 15.41 15.96 15.47 19.25 13.44 17.61 17.11 17.24
DialoGPT 8.37 32.01 11.28 15.06 12.89 18.51 13.87 17.37 15.92 19.17 12.46 18.05 15.22 16.89

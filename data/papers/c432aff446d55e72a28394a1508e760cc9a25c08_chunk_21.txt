NNdistributioniscrucialforgettingoptimalresultsfromeachsetting. Only
coincidentally,atemperatureof1.0wasclosetooptimalintheoriginalsettingsofKhandelwaletal.(2020b),
whichhidtheimportanceofthishyperparameter.
Inboththe5%subsampleddatastoreandthefulldatastorescenarios,temperaturet=1isclosetooptimal
when using “FAISS mask, FAISS score”. When using either “real mask” or “real score”, this is not true
anymore. Evenattheoptimaltemperatureforeachsetting,“realmask,realscore”somewhatunderperforms
“FAISSmask,realscore”. Itisconsistentwiththecounter-intuitivephenomenondiscussedinSection5.1.
Therearealsodifferencesbetweenthetwoscenariosofdifferentdatastoresizes. Withthefulldatastore,using
“realscore”outperforms“FAISSscore”giventhesame“FAISSmask”. However,theoppositeistruewhen
usingthe5%datastore. Thissuggeststhatasthedatastoresizegrows,usingaccuratedistancevaluesarebetter
thantheapproximateones. Therelativelysmallgapbetweenusing“realscore”and“FAISSscore”inboth
datastoresettingsshowsthatthemaincontributortotheimprovementsisusingapproximatenearestneighbors
(“FAISSmask”)ratherthanusingapproximatedistancevalues(“FAISSscore”).
Wehypothesizethatthisisrelatedtoregularizationforpreventingoverfitting,andapproximatesearchprovides
fuzziness that functions as a regularizer. We can think of the non-parametric part in kNN-LM, the kNN
componentasamodel,wherethedatastoresizeisitsmodelcapacity,andthedatastoreisitstrainingdata.
ConsideringthatthekNNcomponentusestheexactsametrainingdataasthebaseparametricLM,having
groundtruth,accuratekNN
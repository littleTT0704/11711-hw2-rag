 al., 2010],
but two characteristics are particularly relevant in the context of automatic source
expansion:
• During the answer scoring phase, DeepQA retrieves supporting evidence for
each candidate that passes an initial filtering stage. The system searches the
knowledge sources for text passages that contain both question terms and the
4http://sourceforge.net/projects/openephyra/
3.4. QUESTION ANSWERING SYSTEMS 31
Massive Evidence Gathering,
Analysis and Scoring
Hypothesis Generation
and Filtering Supporting Deep
Evidence Answer
Search Scorers
Question/ Candidate Final
Primary Logical
Question Category Answer Merging/ Answer
Search Synthesis
Analysis Generation Ranking
Shallow Answer Scorers
Machine
Text RDF
Web Learning
Documents Stores
Model
Figure 3.2: DeepQA architecture adopted by Watson.
candidate answer, and computes various features that reflect the lexical, syn-
tactic and semantic similarity of the question and retrieved text. If the sources
contain passages that closely resemble the question and contain the candidate
answer, the candidate is more likely to be correct. The effectiveness of these fea-
tures depends on the quality of the retrieved supporting evidence, which can be
improved by expanding the sources with additional information and increasing
semantic redundancy.
• DeepQA’s answer scoring is based on a comprehensive probabilistic model that
makes use of more than a hundred deep and shallow features of candidate an-
swers. This model can effectively and reliably rank large numbers of candidates
extracted from a variety of resources, including potentially more noisy sources
generated through statistical source expansion.
To demonstrate the effectiveness of the question answering technologies that were
developed as part of the DeepQA effort, IBM decided to take on the best human
contestants in a Jeopardy! match. To compete at the human champion level, a QA
system must be capable of answering most Jeopardy! questions with high precision,
and to do so within just a few seconds. In Figure 3.3 we illustrate how human players
fared in past Jeopardy! episodes. Each of the gray points visualizes the performance
of the winner of a Jeopardy! game with three contestants. We refer to these points as
the Winners Cloud. It can be seen that on average the winners were fast enough to
acquire 40–50% of all
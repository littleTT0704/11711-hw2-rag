 Pengcheng Yin, and Graham Neu-
big. 2022. Hierarchical control of situated agents
through natural language. In Workshop on Struc-
tured and Unstructured Knowledge Integration
(SUKI),Seattle,USA.
7
A Few-shotmodelssizeestimates base with SentenceTransformers (Reimers and
Gurevych,2019)tofine-tunearetrievalfunctionf
AsOpenAIhasnotreleasedanydetailsofthesize
byminimizingthefollowingloss:
oftheirfew-shotmodels,weestimatetherelative
strengths and weaknesses on code and text gen- L = (cos(f (T ),f (T ))−sim(G,G ))2
θ θ i θ j i j
eration by calculating the average loss per token. (1)
To calculate the avg. loss of each of these mod-
elsoncode,weusetheimplementationprovided wheref θ isparameterizedusingatransformer.
by Xu et al. (2022).5 The perplexity on text cor- ResultsonusingKSTwithPROSCRIPT(Table8)
puswasevaluatedon30randomwikipediapages andEXPLAGRAPHS(Table9). WhileKSTishighly
fromWikiplots6 followingasimilarprocedureThe effective for edge-prediction 6, the results are
structure and text generation capabilities of the mixed for EXPLAGRAPHS and PROSCRIPT. For
models are apparent from the results in Table 7; PROSCRIPT,KSTyieldsmarginalgains. However,
DAVINCI outperforms CODEX ontextgeneration for EXPLAGRAPHS,anumberoftrainingexamples
but is worse on code-generation and vice-versa. have overlapping theme (Table 10), and thus cre-
CURIEunderperformsbothDAVINCIandCODEX ating a prompt dynamically reduces the effective
significantly. Importantly,theseresultsshowthat informationintheprompt.
CODEXandDAVINCIareofcomparablecapacities,
C HumanEvaluation
makingtheircomparisonfair.
Outofthefourtasksusedinthiswork,PROSCRIPT
Model CODE TEXT edge
bining the output distributions from each LM,
anundesiredone(e.g.,toxicity).
DEXPERTS leverages subtle signals expressible
Givenapromptx, thelanguagemodelcom-
bylanguagemodelsforeffectiveattributecontrol, ăt
putesthelogitsforthetthtoken,denotedz P R|V|,
withoutsacrificinggenerationfluencyordiversity. t
whereV isthevocabulary. Aprobabilitydistribu-
Moreover, because it operates only on the out-
tionoverthevocabularyisobtainedbynormalizing
put of the base LM, DEXPERTS can steer with
andexponentiatingz :
(anti-)expertsofsmallersize,evenincaseswhere t
wedonothavefullaccesstothebasemodel(e.g.,
PpX | x q “ softmaxpz q, (1)
t ăt t
GPT-3throughanAPI).
WefirstapplyDEXPERTStothetaskoflanguage andthenexttokenisgeneratedbysamplingx
t
„
detoxification(§3),byfinetuninganexpertandan PpX | x q.
t ăt
anti-expert on public comments that are human-
annotated for toxicity. Our experimental results
2.1 DEXPERTSFormalization
showthat DEXPERTS cansuccessfullyavoidtoxi- DEXPERTS operates on a pretrained language
cityinlanguage generationwhilepreservingout- model M by combining its predictions with an
putfluency,outperformingexistingdetoxification expert M`, which models text with a desirable
methodsonbothautomaticandhumanevaluations. attribute, and an anti-expert M´, which models
Moreover, we find that DEXPERTS continues to text with an undesirable attribute. At time step t,
outperformbaselineswhenemployingonlyananti- we condition each language model M, M`, and
expert and re-using the base model as the expert, M´ on the prompt x to obtain z,z`, and z´,
ăt t t t
makingitoneoftheonlymethodsthatcanavoid respectively. The product-of-experts ensemble is

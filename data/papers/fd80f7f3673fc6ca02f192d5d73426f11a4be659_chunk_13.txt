-
AUTOMQM toen→deandzh→enbecausethere ity Estimation Shared Task (Zerva et al., 2021):
areavailableMQMratingsfromtheWMT’21Met- word-level COMETKIWI (COMET-WL)(Reietal.,
ricsSharedTask(Freitagetal.,2021b)thatwecan 2022b),alsobasedonanXLM-Rmodeltrainedon
useasin-contextlearningexamplepools. acombinationofsentence-andword-leveldata. To
doso,were-runthismodelontheWMT’22Met-
Models Webasemostofourexperimentsonthe
rics Shared Task data, and convert the predicted
followingLLMs:
word-levelOK/BADtagsintospans.5
• PaLM: A 540 billion parameter autoregres-
Finetuning Forregressionfinetuning,weusea
siveTransformermodeltrainedon780billion
real-valuedlogit,extractedfromafixedindexinthe
tokensofhigh-qualitytext(Chowdheryetal.,
firsttargettoken’slogitvector,asthequalitysignal.
2022). Itshowedremarkableperformanceon
(Inparticular,weleverageaspecial,unused,vocab-
awide-rangeofNLPtasks,includingMachine
ularytoken.) Thiswasthetechniqueusedtotrain
Translation(Vilaretal.,2022).
MetricX-XXLintheWMT2022SharedTasksub-
mission(Freitagetal.,2022). Theregression-based
• PaLM-2: The successor to PaLM, the
modelwastrainedonWMTdirectassessment(DA)
PaLM-2 family of LLMs (Anil et al., 2023)
datafromtheyears2015through2020.
buildsuponrecentresearchinsights,suchas
For generative classification, we bucket the
compute-optimalscaling,amoremultilingual
scoresinthetrainingdataintofiveclasses,where
anddiversepre-trainingmixture,andarchitec-
class boundaries are assigned so that each class
tural/optim
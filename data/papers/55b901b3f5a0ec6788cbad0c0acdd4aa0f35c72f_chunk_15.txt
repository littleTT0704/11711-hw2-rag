cid:0) WAll +WI +W{ T∈ +D WR∈ +T WO(cid:1)∈. OR urfa∈ ctoO re} dapproachnotonlyallowsus
∝ (d,t,r,o) d t r o
toshareinformationbetweenobjectivesbutitalsoallowsustoanalyzewhichstagesandprimitives
aremostimportanttoaparticularend-taskaftertrainingiscompleted(Section7).
Prescription(P )fromSection4,advocatesforintroducingasmuchauxiliarydataaspossible. As
2
such,insteadoffixingtoaspecificsubsetthroughouttrainingforaparticularend-task,weproposeto
utilizealltheobjectivesin. Thisalsoavoidsthecombinatorialexplosionthatcomeswithexploring
A
subsetsof atatime. canbelargeanddescendingonallof atoncecanbecomputationally
A |A| A
prohibitive. Asanefficientworkaround,ateachtrainingstep,wesampleasubsetof forexecution
A
withMETA-TARTAN.Oursamplesaredrawnfromallof soanyobjectivecangetusedatany
timestep. Becausewemodeleachwk viaafactoredapproaA ch,evenifanobjectiveisnotsampled
itsweightisimplicitlyupdated. Ourapproachisreminiscentofstochastic-relaxationweightsharing
(Phametal.,2018;Dong&Yang,2019;Lietal.,2020)wheresampledarchitecturalprimitivesresult
inupdatestosharedmodelweightswhichcanbeusedbyotherprimitivesthatarenotsampled.
We coalesce all the ideas we have introduced so far into Algorithm 1 which we dub
AANG (AutomatedAuxiliaryLearniNG).Atahigh-level,givenanend-taskE:
1. Wegenerateaspaceofauxiliaryobjectives byleveragingthetaxonomydiscussedinSection3.
A
maycontainauxiliarytasksthatcanimproveourperformanceonE.
A
2. WeleverageMAML-style(Finnetal.,2017)meta-learning
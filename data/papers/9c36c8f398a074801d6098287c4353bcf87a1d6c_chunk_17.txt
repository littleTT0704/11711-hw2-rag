 0.82 74.36 0.60
HL-BART 46.09 3.42 45.97 3.68 VA-BART(GPT-Jurassic6B) 69.09 1.59 69.89 1.49
Nlpaug-ALBERT 48.10 11.0 40.62 8.05 VA-BART(GPT-Jurassic17B) 71.03 1.01 71.68 0.90
Nlpaug-RoBERTa 40.14 2.00 30.64 3.56 VA-BART(GPT-Jurassic178B) 74.04 1.16 74.24 0.91
Nlpaug-BART 47.76 42.40
3.89 5.45
VA-ALBERT 55.15 53.14 Table 4: Effect of size and types of LLMs on value-
6.83 9.05
VA-ROBERTA 58.13
5.33
56.60
6.56
aligned training data generation. We prompted OPT
VA-BART 57.98 55.94 and GPT-Jurassic ranging 1.3B âˆ¼ 178B. The bigger
5.12 5.48
themodel,thebetterthefinalperformanceinthevalue
Table 3: Performances of VA-MODELs and baselines alignment task. All VA-BART variations are fine-
onunseenvaluesinvalue-alignedsexismclassification. tunedwiththesamenumberoftrainingsamples.
In the training phase, models did not see any of the
valuesinthetestset.
of those unseen values) and the results are pre-
sentedinTable3. Overall,therearedropsinper-
formance compared to the main experiment (Ta-
ble 1), while all of our VA-MODELs continue to
outperform all baselines. The baselines experi-
encelargerdrops(maximum43.18%forNlpaug-
RoBERTa)thanthe VA-MODELs(17.22%forVA- Figure 5: Evaluation results (W-F1) of VA-
ROBERTA). Considering the model was never MODELsoverdifferentsizeofgeneratedtrainingdata.
taughtorreceivedanydirectsupervisiononthetest
values,itisexpectedbehaviorasothergeneraliza
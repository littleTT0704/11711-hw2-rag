 6 for a plot of the relationship. So it ap-
Learning Curve for zsRE
pearsthatthelearnedoptimizercanupdatemodel 100
beliefsindependentlyofhowbelief-liketheyareto
beginwith. Wewouldalsobeinterestedinconsid- 95
eringconsistencyunderentailment,buttheupdate
successrateonLeapOfThoughtisalready100%, 90
sothereisnovariancetoexplain.
85
Learning curve. In Fig. 7 we show the learning
103 103.5 104 104.5 105
n
curve of a learned optimizer trained with SLAG
Figure7: MainInputUpdateSuccessRateacrosstrain-
on zsRE. The Main Input Update Success Rate ingsetsizes,usingSLAGonzsRE.
steadilyrisesasafunctionofthetrainingsetsize.‘
Ablation by objective term. We give objective data5m, however, the paraphrase term improves
ablation results in Table 17. Surprisingly, we do paraphrase update success by a large margin of
notalwaysseethattheobjectivetermshelpforthe 16.94( 1.03;p<1e 4)points. AddingtheLocal
± −
data they are intended to help with. First, we ob- Neutral(LN)termwiththeparaphrasetermgreatly
tainmixedresultsfortheparaphraseobjective. On improvestheLNRetainRateforWikidata5m,by
zsRE, the objective term seems to hinder perfor- 9.71points( 1.44;p<1e 4),thoughbothofthese
± −
mance,withupdatesuccessdroppingonMainIn- termscomeatacosttoMainInputUpdateSuccess,
putsby0.71( 0.60;p=.021)and∆-Accdropping similartozsRE.Lastly,wedonotfindthattheen-
±
by0.18( 0.19;p=.069),whiletheparaphraseUp- tailmentobjectiveimprovesEntailedDataUpdate
±
dateSuccessRateitselfisunaffected. WithWiki- Success; infact, thismetricfallsby4.56( 7.22;
±
27
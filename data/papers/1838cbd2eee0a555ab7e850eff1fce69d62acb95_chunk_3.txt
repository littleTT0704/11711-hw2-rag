olanguagedisparities. Thoughrecent This is widely adopted in the zero-shot transfer
workoncross-lingualtransfermitigatesthischal- setup where no annotated data is available in the
lenge,itstillrequiresasizeablemonolingualcor- targetlanguage. Thepracticeisstillapplicablein
pustotraintokenembeddings(Artetxeetal.,2019). thefew-shotsetting,inwhichcaseasmallamount
As noted, these corpora are difficult to obtain for ofannotateddatainthetargetlanguageisavailable.
manylanguages(Artetxeetal.,2020). In this work, we focus on cross-lingual trans-
Additionally, recent work (Singh et al., 2019) fer for extremely low-resource languages where
shows that contextualized representations of dif- only a small amount of unlabeled data and task-
ferentlanguagesdonotalwaysresideinthesame specificannotateddataareavailable. Thatincludes
spacebutareratherpartitionedintoclustersinmul- languagesthatarenotcoveredbymultilinguallan-
tilingualmodels. Thisrepresentationgapbetween guage models like XLM-R (e.g., Maori or Turk-
languages suggests that joint training with com- men),orlow-resourcelanguagesthatarecovered
bined multilingual data may lead to sub-optimal but with many orders of magnitude less data for
transferacrosslanguages. Thisproblemisfurther pre-training(e.g.,TeleguorPersian). Weassume
exacerbated by the, often large, lexical and syn- theonlytarget-languageresourcewehaveaccess
tacticdifferencesbetweenlanguageswithexisting toisasmallamountoftask-specificlabeleddata.
pre-trainedrepresentationsandtheextremelylow- More formally, given: (1) a limited amount of
resourceones. Figure1(a)providesavisualization annotatedtaskdatainthetargetlanguage,denoted
(i) (i)
ofonesuchexampleofthedisjointrepresentations as D = {(x,y );i âˆˆ [1,N]}, (2) a larger
t t t
ofaresource-richauxiliarylanguage(English)and amountofannotateddatafromoneormoresource
in a list or table. Thus we relaxed the independence assumption by adding features of
adjacent text nuggets to the LR model. More precisely, in addition to the 19 original
relevance features, we added the 18 nugget-level features (i.e. all relevance features
except DocumentRank) of the previous nugget and the next nugget, yielding a total
of 55 features. The evaluation results in Chapter 5 show that this simple extension of
the independent LR model improves relevance estimation performance significantly
when scoring the shorter sentence-level text nuggets, and that it also yields a small
improvement when scoring longer markup-based nuggets.
We also attempted adding features of more distant text nuggets to the logistic
regression model but did not observe consistent performance gains. In addition, we
tried capturing dependencies between text nuggets with a stacked learning approach.
This method uses an LR model to make independent predictions for each text nugget,
and a second LR model to combine the predictions for adjacent instances into more
accurate context-sensitive relevance estimates. Interestingly, the performance of the
stacked LR approach was almost identical to the model with features of adjacent
instances. We chose not to use the stacked method because it requires training and
applying two relevance models instead of only one when using adjacent features.
Before fitting the final model used in our experiments with question answering
datasets in Chapter 6, we performed greedy backward feature elimination using
Akaike’s information criterion (AIC) [Akaike, 1974]. AIC is a popular optimality
criterion that trades off bias and variance. Let k be the number of parameters in
46 CHAPTER 4. SOURCE EXPANSION APPROACH
a model, and L (k) the maximum data likelihood attained by a model with k
max
parameters. Then the AIC is defined as
AIC = 2k −2ln(L (k)).
max
By minimizing the AIC, we reward models that fit the data well (i.e. large L (k)),
max
but at the same time we discourage overfitting by preferring models with fewer fea-
tures (i.e. small k). In greedy backward elimination, an initial relevance model is
fitted using all available features and the AIC is computed. In the next step, each
feature is removed individually, a model is fitted without
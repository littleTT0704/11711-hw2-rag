shows the perceptual loss, L indicates the alpha loss,
criminators are identical, though each discriminator works alpha
L statestheborderloss,andL expressesthealpha
on a different scale. Since alpha matte does not contain a border ac
coefficientloss. Besides,λ,β,γ,θ arecoefficientsthatde-
sufficient amount of useful representation, we decided to
terminetheeffectofeachlossesoverthetotalloss.Accord-
useacombinationofthealphamatteandtheextractedfore-
ingtoourexperimentsonvalidationset,weempiricallyde-
groundsubject,whichisobtainedbymultiplyingthealpha
finedthesevaluesas10,25,50,25.
matte and the image, as an input to the discriminator net-
work. For the real image, we extracted subjects using the
3.1.Trainingprocedure
imagesandthegroundtruthalphamatte,whileweusedthe
imagesandthepredictedalphamattetoobtainfakeimages During the training, we did not train the segmentation
forthediscriminator.Depth-wiseconcatenationofthethree network. Instead,weonlytrainedthealphagenerationnet-
channelsRGBimageandonechannelalphamatteisthein- workandtherefinementnetworkend-to-end.Duringthein-
putdataofthediscriminator. ference,theframeworkworksend-to-endwhichmeanswe
provideaninputimagetothewholesystemandgetanalpha
Loss functions For the training of the alpha generation
matteforthecorrespondinginputimage. Theinputimages
network,weusedadversarialloss[15],perceptualloss[19],
areresizedto1280×768resolutionbeforefeedingthenet-
alpha loss, border loss, and alpha coefficient loss. In the
work. We used 10−4 learning rate for the generator and a
perceptualloss[19],weutilizedtheVGGmodel[40]toex-
tentimessmallerlearningrate(10−5)forthediscriminator
tract features. For this, we employed five different layers
toslowdowntheconvergenceofthediscriminatorsincewe
oftheV
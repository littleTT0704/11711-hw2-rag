omesframework-bound. Inthissetting,latency
elsfortheireaseofuse. Thisfurtherexacerbates
is constant regardless of batch size or number of
thecommunitydivide,wheremodelsaredesigned
MACscomputed. Forsettingswherelatencyisde-
underdifferentassumptionsthandeployed.
pendentontheexecutionofcomputationalkernels
3.2 FrameworkOverhead anddatamovement,modelsarecompute-bound.
Deep learning frameworks asynchronously dis-
4 Experiments
patch computation for execution on highly paral-
lelizedhardwareaccelerators,asshowninFigure Weevaluatemodelsframeworksfromeachofthe
2. Forsufficientlylargecomputekernels,suchas major paradigms: eager execution PyTorch, de-
those during training, models achieve near max- ferred execution TorchScript, and statically com-
imum GPU utilization â€“ measured as the differ- piledONNXRuntimewithaCUDAbackend.
encebetweentotalexecutiontimeandactiveGPU We use PyTorch 1.12.1 with CUDA 11.6 and
time(Zhuetal.,2018). However,duringinference, Python3.8.13. WeuseONNXRuntime1.7.0with
smallerinputsizesleadtosuboptimalGPUutiliza- CUDA11.1.1andcuDNNv8.0.4.3. Baselineexper-
tionastherapidexecutingkernelsdonotsaturate imentsarerunonacomputenodewithanNvidia
thefixedcostframeworkoverheadincurredfrom RTX-8000GPUandanIntelXeonE5-2630CPU
CPUoperationssuchaskernellaunch,graphcon- with32GBofDDRAMmemory.
struction,controlflow,anddevicesynchronization; We measure latency, GPU utilization over a
ycnetaL
ResNet-50 (fp32) ResNet-50 (fp16) BERT (fp32) BERT (fp16) BERT (fp16)
101
101 PyTorch
TorchScript
101 ONNX Runtime
102 102
102
102
102
100 101 102 100 101 102 100 101 102 100 101 102
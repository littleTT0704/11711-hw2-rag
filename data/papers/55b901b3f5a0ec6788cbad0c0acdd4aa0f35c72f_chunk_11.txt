�
withN samplesS =(z,...,z )fromthedistribution. f caneitherbeasingleobjectiveor
a a 1 Na Da a
3Althoughthistaxonomyisquiteexpansive, itobviouslydoesnotconsiderotherelementsofobjective
creationsuchaschoiceofmodelarchitecture,optimizersettings,etc.
4
PublishedasaconferencepaperatICLR2023
aweightedlinearcombinationofobjectives: f =(cid:80) wkfk. AtanyiterationofSGD,wesample
a k a
achoiceoftheend-taskfunctionf ortheauxiliaryobjectivef accordingtotheprobabilitiesλ,
e a e
λ [0,1] λ +λ =1. Giventhechosenobjective,wesampleadata-pointandperformstochastic
a e a
∈ |
gradientdescentbasedonthesampleddata-point. Wenowpresentourboundinthesettingdescribed.
Theorem4.1(AuxiliarylearningwithDynamicSampling). Assumethatf (;z ),f (;z ) [0,1]
e e a a
∈
are both L-Lipschitz with β and β -smooth loss functions respectively. Consider that we have
e a
N(cid:48) =N e+N atotalsampleswheref eandf ahaveN eandN asamplesrespectively. r e = N Ne (cid:48) isthe
fractionoftheavailabledatarepresentedbytheend-task. Supposethatwerunstochasticgradient
descentforTstepswithmonotonicallynon-increasingstepsizesα c bydynamicallysampling
t ≤ t
thetasksaccordingtoλ andλ. Then,withrespecttof,thegeneralizationerrorisboundedby:
e a e
(cid:15)
gen
(cid:47) (cid:0) ∆)1+cλ1
∗β∗(cid:18)
γ NT
(cid:48)(cid:19)1− cλ∗β1
∗+1 Where γ = λ re (1)
e
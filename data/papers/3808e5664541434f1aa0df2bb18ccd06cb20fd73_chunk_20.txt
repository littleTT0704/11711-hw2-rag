 holistic representa-
(Brown et al., 2020). Under this mechanism, we
tions of social biases or toxicity (e.g., Social Bias
prompt GPT-3 with four translation pairs (taken
Frames;Sapetal.,2020).
from Spears, 1998) and an AAE tweet from our
training data, and generate its WAE “translation”. EthicalImplications&Limitations
The list of prompts, as well as further details, are
The above synthetic setting is meant to illustrate
providedinAppendixC.Notethatwedonot rec-
the role of labeling quality on biases in annota-
ommendthisapproachtobuildlargescaleparallel
tions. We strongly caution against using this ap-
datafordialects,asdiscussedunderethicalimpli-
proach in real-world applications, such as build-
cationsandlimitations.
ing parallel datasets for dialects. First, due to
Next,asperourheuristic,weonlyrelabeltoxic
how its training data was selected, GPT-3 has
AAE tweetswhose WAE translationispredictedas
likely not been exposed to many African Ameri-
non-toxic by either our vanilla classifier trained
can English varieties during training (Jo and Ge-
on the original Founta et al. (2018) dataset, or an
bru, 2020). Second, pretrained language models
identical classifier trained on the WAE translated
areknowntogeneratetoxiclanguageatnon-trivial
tweets. The resulting dataset (AAE-relabeled) is
rates(Gehmanetal.,2020),whichcouldcausedif-
thesamesizeastheoriginaldataset, butwith954
ferentialtoxicityinthetranslations.
(12%) out of 8260 toxic AAE tweets relabeled as
7 RelatedWork
14Notethatthisassumptiondoesnotholdforlexicalitems,
becausesubstitutinglexicalitems(e.g.,swappingaminority
DebiasingToxicityDetection Asthepopularity
mentionforamajoritymention)woulddrasticallychangethe
denotationalmeaningofthesentence. of hate speech and toxic language detection sys-
3150
niart%
.27 78.93 87.12
SphereFace-Rv1 SFN 1.5 40 0.5 89.17 91.95 42.38 80.24 88.88 93.45 73.55 87.24 77.61 85.93 91.59 94.65 82.09 89.01
SphereFace-Rv2 SFN 1.4 60 0.5 89.10 91.75 44.20 78.91 88.86 93.12 70.84 87.15 75.91 85.63 91.38 94.31 81.04 88.54
be larger. For example, in Table 4(b), the optimal margin 10 10
mis monotonouslyincreased from1.1 to1.5 forthe scales NFN NFN
rangingfrom20to60.Theperformanceisaffectedbyboths 7.5 HFN 7.5 HFN
SFN SFN
andminacoupledmanner.Thesamepatternalsoappears
in Table 4(a) and (c). Third, a wide range of s could lead 5 5
toasatisfyingperformance,aslongasmisproperlytuned.
2.5 2.5
For example, in Table 4(a), the corresponding AUC-0.0005
remains a relatively high value ([57.09,61.37]) while the
0 0
scalesvariesfrom30to60.Wenotethattheseobservations 0 20k 40k 60k 80k 0 80k 160k 240k
areconsistentacrossalltypesofmultiplicativemargin. Number of Iterations Number of Iterations
(a) Training SFNet-20 on VGGFace2 (b) Training SFNet-64 on MS-Celeb-1M
Hyperparameter tuning strategy. With the aforementioned
observations, we summarize a simple yet effective strategy
Fig.6.TrainingobjectiveofSphereFace-Rv2withNFN,HFNandSFN
to search suitable hyperparameters for HFN. First, we uni- on (a) VGGFace2 and (b) MS-Celeb-1M. For SFN, we only plot the
formly pick the scale parameter s with a relatively large softmax-basedlossandthefeaturenormregularizationisnotplotted.
gap, e.g., 20 or 30.
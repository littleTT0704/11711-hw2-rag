askwhiletestingonLasVegastrack.
Agent ECP(↑) ED(↓) AATS(↑) ADE(↓) TrA(↑) TrE(↑) MS(↑)
HUMAN 100.0(±0.0) 176.2(±3.4) 114.2(±2.3) 1.7(±0.1) 0.88(±0.01) 1.09(±0.02) 10.1(±0.3)
RANDOM 1.0(±0.6) 21.9(±9.6) 9.2(±1.5) 1.4(±0.3) 0.74(±0.01) 0.18(±0.05)∗ 8.4(±1.0)
MPC 69.5(±10.7) 353.2(±54.8) 40.5(±0.1) 0.8(±0.1) 0.91(±0.02) 1.07(±0.01)∗ 10.4(±0.2)
RL-SAC 11.8(±0.1) 109.9(±7.5) 22.1(±1.5) 1.3(±0.1) 0.95(±0.01) 0.58(±0.01)∗ 9.9(±0.2)
perceptronwithtwohiddenlayersof64hiddenunitseach. 6.ExperimentsandResults
Ouragent’srewardfunctionwastheenvironment’sdefault
We evaluate each of the baseline agents—HUMAN,
withtheinclusionofabonusiftheagentremainednearthe
RANDOM, MPC, and RL-SAC—on the L2R task, with the
centerofthetrack.
objective of finishing 3 consecutive laps in minimal time.
Human. We additionally establish a HUMAN performance For all approaches, agents complete model training and
baseline, by collecting simulated racing results from hu- tuning on Track01:Thruxton. We present the aver-
man expert players. The collection procedure involved a age of each metric across 3 consecutive episodes, in Ta-
privatecrowd-sourcingevent,whichw
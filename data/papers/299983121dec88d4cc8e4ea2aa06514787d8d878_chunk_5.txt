calculatesan
demonstratesthatVisCTGimprovesmodelperformanceand F-scoreoverthesegraphs’tuples.Linetal.(2020)notethat
commonsensewhileaddressingthebaselineinadequacies. SPICEcorrelateshighestwithhumanjudgmentforCommon-
Gen.Covmeasurestheaveragepercentageofinputconcepts
2 Dataset,Models,andMetrics coveredbytheoutputtextinanyform.
WealsouseBERTScore(Zhangetal.2019)andPerplex-
2.1 CommonGenDataset
ity(PPL).BERTScoremeasuresBERT(Devlinetal.2019)
TheoriginalCommonGendatasetismadeupof35,141con-
embeddingssimilaritybetweenindividualtokens,servingas
cept sets (consisting of 3 to 5 keywords each) and 79,051
amoresemanticratherthansurface-levelsimilaritymeasure.
sentences,splitintotrain,dev,andtestsplits.Sincetheorig-
Wemultiplyby100whenreportingBERTScore.PPLserves
inaltestsetishidden,wepartitiontheoriginaldevsetinto
asameasureoffluency,withlowervaluesrepresentinghigher
newdevandtestsplitsforthemajorityofourexperiments.
fluency.WeuseGPT-2(Radfordetal.2019)forPPL.Forall
Wedo,however,asktheCommonGenauthorstoevaluateour
metricsotherthanPPL,highermeansbetterperformance.
bestVisCTGmodelsontheoriginaltestset(morein§6).The
trainingsetremainsthesame.Werefertotheoriginaldevand
3 InitialAnalysisandMotivation
testsetsasdev andtest,andthesenewsplitsastrain,
O O CG
dev,andtest.Table2containsinformationaboutthese 3.1 BaselineModelGenerations
CG CG
splits.Theirrelativesizesanddistributionofconceptsetsizes Weconductaninitialanalysisofthebaselinemodeloutputs,
withineacharekeptsimilartotheoriginals. and observe that several lack fluency. Some are more like
phrasesthanfullcoherent
cid:18)((cid:1)jX)
likelihoodgivenatrainingset,whichignoresthetaskloss: rPG =E h c(Y^;Y)r logp (Y^jX)i : (7)
L ((cid:18);D)=(cid:0)P PjYj logp (y jy ;X). (cid:18) Y^(cid:24)p(cid:18)((cid:1)jX) (cid:18) (cid:18)
MLE X;Y2D t=1 (cid:18) t <t
Thepolicygradientobjectivecontainsanexpectationoverthe
outputdistributionp ((cid:1)jX),unliketheobjectiveoptimizedby
Method. Todirectlyoptimize(1),weiterativelyupdatethe (cid:18)
MGS(Equation1).Inparticular,computingthePGobjective
parameters(cid:18)inthedirectionofmaximalimprovementinthe
involvesdecodingwithancestralsampling,whiletheobjec-
taskloss.Eachupdatecorrespondstotheexpectedupdate
tive(1)usesanarbitrarydecodingalgorithm.Naturally,ap-
underadistributionthatweightseachdirectionaccordingto
proximatingthepolicygradientalsousesancestralsampling
itsimprovement,
1SeetheAppendixforareviewofself-normalizedimportance
(cid:1) =E [(cid:1)]; (2)
(cid:3) (cid:1)(cid:24)p(cid:3)((cid:1)j(cid:18);(cid:11)) sampling.
14033
Algorithm1:MLE-guidedparametersearch(MGS). where S = fY^ 1;:::;Y^ kg is a set of candidate output se-
quences,andZ (X;S) = P p (YjX)(cid:11).Thereareno
Given:BatchfX ;Y gB,modelp,decoding (cid:18) Y2S (cid:18)
algorithmi F,i
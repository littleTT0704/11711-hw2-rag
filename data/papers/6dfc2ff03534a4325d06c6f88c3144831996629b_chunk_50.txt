 oftheabovehas5answerspossible,whilewehave4)and
humanstraditionallywritecorrectandincorrectanswersto format; our focus here is to investigate how difficult these
a question (thus, potentially introducing the annotation ar- datasets are for text-only models.23 Our point of compari-
tifacts). In Table 5 we consider several of these datasets: sonis,sinceouruseofAdversarialMatchingmeans
VCR
TVQA [46], containing video clips from TV shows, along thathumansneverwriteincorrectanswers.
21Thislineofworkiscomplementarytoothernotionsofdatasetbias, WetacklethisproblembyrunningBERT-Baseonthese
likeunderstandingwhatphenomenadatasetscoverordon’t[76],partic- models[15]: givenonlytheanswer(A),theanswerandthe
ularly how that relates to how marginalized groups are represented and question(Q+A),oradditionallanguagecontextintheform
portrayed[71,90,69,68]. ofsubtitles(S+Q+A),howwelldoesBERTdo?Ourresults
22For instance, the SNLI dataset contains pairs of sentences with la-
inTable5helpsupportourhypothesisregardingannotation
belssuchas‘entailed’or‘contradiction’[8].Forasentencelike‘Askate-
boarderisdoingtricks’workersoftenwrite‘Nobodyisdoingtricks’which
isacontradiction.Theresultisthattheword‘nobody’ishighlypredictive 23Itshouldbenotedthatallofthesedatasetswerereleasedbeforethe
ofawordbeingacontradiction. existenceofstrongtext-onlybaselinessuchasBERT.
15
artifacts:whereasaccuracyon,onlygiventheending, E.Modeldetails
VCR
is 27% for Q → A and 26% for Q → A, versus a 25%
Inthissection,wediscussimplementationdetailsforour
random baseline. Other models, where humans write the
model,R2C.
incorrectanswers,haveanswer-onlyaccuraciesfrom33.8%
BERTrepresentations Asmentioned
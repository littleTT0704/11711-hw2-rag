dressthechal- a more extensive study that focuses on the type and nature
lengeofaligningLLMswithdesiredvaluesandobjectives. ofvaluepluralitycoveredbyVALUEPRISMremainsacom-
Reinforcement learning (RL) has historically been used in pellingdirectionforfutureresearch.
multiple NLP tasks to ensure that the generated text is op- IntendedUse. Wemake VALUEPRISM openlyavailable
timized for an arbitrary non-differentiable reward (Johnson byindividualrequestwiththehopeandintentionthatitfur-
etal.2017;Nguyen,Daume´III,andBoyd-Graber2017;Ra- thers research in value pluralism in NLP and AI. However,
mamurthyetal.2022;Pyatkinetal.2023).Luetal.(2022) itispossiblethatourdatacanbeusedinmaliciousandun-
optimized a reward function that quantifies an undesired intended application (e.g., speech policing or promotion of
property,whilenotstrayingtoofarfromtheoriginalmodel certain values). We do not endorse its use in such capacity
viaaKL-divergencepenalty.(Baietal.2022)exploredRL andemphasizethattheuseofourdatasetandmodelshould
techniquesfortrainingLLMstoadheretolegalandethical belimitedtoresearchpurposesonly.Additionally,welimit
guidelines encoded in a constitution, naming it “Constitu- the data and model available only by individual request to
tional AI”. Wu et al. (2023) used fine-grained human feed- trytoprohibitnon-researchusecasesandensurefairuse.
back as an explicit training signal to train and learn from
rewardfunctionsinaRLHFfashion.Additionally,Luetal. Acknowledgments
(2023) proposed an inference-time algorithm to efficiently
The authors thank Ronan LeBras, Jared Moore, Hyunwoo
tailorLLMswithoutnofine-tuning,addressingtaskslikeen-
Kim, Jenny Liang, and Sebastin Santy for helpful discus-
suringsafetyandfidelityindialoguemodels.
sions; Alane Suhr for the example
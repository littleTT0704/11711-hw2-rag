 given inputs (Chen
providedinAppendixF.Additionalgeneralexam-
et al., 2021; Athiwaratkun et al., 2022; Li et al.,
plesareprovidedinAppendixG.
2022; Wang et al., 2022; Lai et al., 2022; Huang
6 RelatedWork et al., 2022). However, execution-based evalua-
tion requires datasets that are provided with man-
Token-based metrics Metrics such as BLEU uallycraftedtestcasesforeachexample,whichis
(Papineni et al., 2002) evaluate code generation costlyandlabor-intensivetocreate;thus,onlyfew
bycountingmatchingn-gramsbetweengenerated such datasets exist. In contrast, CodeBERTScore
and reference code. CrystalBLEU (Eghbali and is completely unsupervised and does not depend
Pradel, 2022) refines this approach by disregard- onanyspecificdataset. Further,executingmodel-
ing trivially shared n-grams, while ROUGE (Lin, generated code is susceptible to security threats,
2004) and METEOR (Banerjee and Lavie, 2005) and thus should be run in an isolated sandbox,
emphasize recall and balance of precision and re- which makes it technically cumbersome to work
call respectively. However, these metrics, relying withiteratively.
on exact lexical matches, often fail to capture se-
mantically equivalent but lexically different code 7 Conclusion
snippets. Unlike these, CodeBERTScore captures
Inthispaper,wepresentCodeBERTScore,asim-
the wide, two-sided context of each token, which
ple evaluation metric for code generation, which
n-gramscannotcapture.
builds on BERTScore (Zhang et al., 2020), using
Static analysis-based metrics CodeBLEU pretrained language models of code, and lever-
(Ren et al., 2020) incorporates data-flow and aging the natural language context of the gen-
AbstractSyntaxTree(AST)matching,inaddition erated code. We perform an extensive evalua-
to token-matching. However, valid code may not tion across four programming languages which
always align in ASTs and data-flows. Addition- shows that CodeBERTScore is more correlated
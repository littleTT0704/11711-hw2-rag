eterizedBERT renttoourSQuAT),whichalsoleveragesSAMto
on the NLP benchmark (Gholami et al., 2021). performquantizationadoptsmanuallydefinedstep-
GOBO (Zadeh et al., 2020) is a recent bench- sizeandonlyranexperimentsonvisiondatasets.
markforPTQinBERT.ForQATbaselines,there
3 Methodology
are Zafrir et al. (2019) and Q-BERT (Shen et al.,
2020). BinaryBERT(Baietal.,2020)andTenary
ThemostintuitivewaytoapplySAMtothequan-
BERT(Zhangetal.,2020)attemptedthechalleng-
tizedmodelistooptimizeforthefollowingobjec-
ing“binarization"taskofBERTmodels. Themost
tive, where w is the collection of model weights
recenteffortistoperformintegerquantizationof
andsthecollectionofstep-sizeparameters:
theRoBERTamodel(Kimetal.,2021).
min max L(Q(w,s)+(cid:15)) (4)
2.3 FlatMinima
w,s (cid:107)(cid:15)(cid:107)2<ρ
A flat minimum in the loss landscape is a local
WeaddtheSAMperturbationdirectlytothequan-
optima where the loss remains low in a nearby
tized weight because the element-wise gradient
region. We follow the (cid:15)-sharpness definition of
from Q(w,s) to w and s can only be estimated
(Keskar et al., 2016) which defines sharpness as through STE, therefore the SAM perturbation, (cid:15),
maximumlosswithinaneighborhoodboundedby cannotbeaccuratelyevaluated. Anaturalapproach
to optimize Eq. 4 is to jointly update s and w,
(cid:15). Inmathexpression,letw denotesthecollection
whichresultsinthefollowingupdates:
ofallmodelweights,max L(w+(cid:15))−L(w)
(cid:107)(cid:15)(cid:107)2<
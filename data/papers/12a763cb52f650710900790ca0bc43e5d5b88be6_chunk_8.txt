ison,weuseGPT-3asthe
onitstoken(s). Wealsoimplementzero-shotGPT-
knowledge generator in self-talk, and bound the
3 inference, where we plug in each choice to the
number of generations to M = 20 per question.
questionandcomputethechoiceprobabilityasthe
Templatesandotherhyperparametersarekeptthe
generativeprobabilityoftheentiresentence, nor-
sameastheiroriginalpaper.
malizedoverallthechoices.
Retrieval-basedknowledge(IR) Insteadofbe-
CommonsenseQA(CSQA)(Talmoretal.,2019) ing generated, knowledge can be retrieved from
isa5-waymultiple-choiceQAdatasetaboutcom- appropriate sources. We consider the following
mon world scenarios. We do inference with the retrieval-basedmethods. ForNumerSense,knowl-
zero-shotandfinetunedT5models. Forzero-shot edgeisretrievedfromsentencesinWikipediaand
T5,weformatthequestionastext-infilling,andpre- GenericsKB.ForCSQA2,weusesnippetsreturned
dictthechoicewithhighestsequence-to-sequence byGooglewhenqueryingthequestion. ForQASC,
languagemodelingprobability. ForfinetunedT5 weusetheassociatedfactsentencesthatareused
(includingUnifiedQAwhichisSOTA),weusethe tocreateeachquestion.
samesetupasKhashabietal.(2020).
Answers (A) Instead of generating knowledge,
CommonsenseQA 2.0 (CSQA2) (Talmor et al., GPT-3canbepromptedtogeneratedirectanswers
2021) is a binary classification dataset where we to questions. In the prompts, we use the same
needtojudgewhethercommonsensestatementsare inputquestionsasthoseinknowledgegeneration,
true or false. We only do inference with the fine- whilereplacingtheknowledgestatementwiththe
tunedmodel,duetopoorcalibrationofzero-shot ground truth answer. We consider two baselines:
modelsonthisdataset. WeusefinetunedUnicorn (1)
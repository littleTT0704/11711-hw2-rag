 not take into account previously
selected queries or the current decision boundary, and is used as a baseline for more
informed query selection strategies in our experiments. Note that a random selection
of queries on average yields very few positive instances because of the imbalance
between relevant and irrelevant text nuggets.
Maximum Score. The unlabeled instance with the largest relevance score esti-
mated by the latest model is chosen as a query in each step. This sampling method
selects more relevant text nuggets than random sampling, which should initially re-
sult in a steeper learning curve. However, once a reasonably effective model has been
found, this strategy can be expected to perform poorly because it will select almost
no negative instances.
Diversity. We select an instance in the unlabeled dataset that is farthest from
already labeled instances as a query Q:
Q = argmax Diversity(x) = argmax min ||x−x(cid:48)||
x∈U x∈U x(cid:48)∈L
Here U denotes the set of unlabeled training instances, and L is the set of queries
that have been labeled in previous iterations. The features are normalized to have a
sample standard deviation of 1, and ||x−x(cid:48)|| is the Euclidean distance between the
normalized feature vectors. This strategy does not depend on the current model, and
thus it is possible to select all queries at once and present them to a human annotator
in one batch, which eliminates the waiting time in between queries. The goal of this
method is to explore the entire feature space and to fit a reasonably accurate model
after only few iterations.
Uncertainty. In each step, we select a query Q the current model is most uncertain
about:
Q = argmax Uncertainty(x)
x∈U
The objective is to choose hard instances that will help the most for improving the
relevance model. This sampling strategy gradually refines the decision boundary and
often yields very accurate final models, but it can be ineffective in early iterations if
queries are selected based on an inaccurate model. In addition, this method has high
variance and can converge to a local optimum if the initial decision boundary is too
far from the truth.
For logistic regression, we use the misclassification probability to quantify the
uncertainty
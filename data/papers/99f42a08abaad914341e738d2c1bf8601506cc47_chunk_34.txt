 function, the underlying mechanisms Statisticsfortheuseddatasets.
of how the loss characteristics can affect the performance Dataset #ofID #ofimages Split
arelargelyunclearandremaintobeinvestigated.Typically,
VGGFace2[76] 8.6K 3.1M train
s and m jointly specify the loss characteristics, and their
MS-Celeb-1M[77] 86K 5.8M train
roles could be partially coupled, which is also empirically
LFW[78] 5,749 13,233 validation
observedinourablationstudy.
AgeDB-30[79] 568 16,488 validation
From a back-propagation perspective, we have the gra-
CALFW[80] 5,749 11,652 validation
dientofL
s
(w.r.t.eitherxorW i,∀i)as
CPLFW[81] 5,749 12,174 validation
L(cid:48) s=(cid:88) 1+(cid:80)exp(Q ex( pθ (y Q,θ (i θ,s,, θm,) s),m))·Q(cid:48)(θ y,θ i,s,m) (20) C VF GP G[ F8 a2 c]
e2 test[76]
5 50 00
0
7 1, 70 30 k0 v va al li id da at ti io on
n
i(cid:54)=y i(cid:54)=y y i
IJB-B[83] 1,845 76.8K test
where Q(cid:48)(θ y,θ i,s,m) denotes the gradient of loss charac- IJB-C[84] 3,531 148.8K test
teristics. Quite interestingly, if we apply CGD to Eq. (19), MegaFace(probe)[85] 530 3,530 test
then Q(cid:48)(θ y,θ i,s,m) for all hyperspherical FR methods will MegaFace(distractor)[86] 690K 1M test
immediately become identical to that of the normalized
softmax loss [9]. The
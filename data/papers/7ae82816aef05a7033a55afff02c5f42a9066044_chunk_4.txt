 actions [19, 10]
A system of this kind employs no grammar but relies completely on inductive
learning from treebank data for the analysis of new sentences and on determinis-
tic parsing for disambiguation. This combination of methods guarantees that the
parser is robust, never failing to produce an analysis for an input sentence, and
efficient, typically deriving this analysis in time that is linear or quadratic in the
length of the sentence.
For the experiments in this paper we use the arc-standard variant of Nivreâ€™s
parsing algorithm [17, 20, 21], a linear-time algorithm that derives a labeled de-
pendency graph in one left-to-right pass over the input, using a stack to store
partially processed tokens in a way similar to a shift-reduce parser.
The features of the history-based model can be defined in terms of different
linguisticattributesoftheinputtokens,inparticularthetokenontopofthestack,
which we call the top token, and the first token of the remaining input, called the
next token. The top token and the next token are referred to collectively as the
target tokens, since they arethe tokens consideredascandidates fora dependency
relationbytheparsingalgorithm.Inadditiontothetargettokens,featurescanbe
basedonneighboringtokens,bothonthestackandintheremaininginput,aswell
as dependents or heads of these tokens in the partially built dependency graph.
The linguistic attributes available for a given token are the following: Lexical
form (stem) (LEX), Part-of-speech category (POS), Inflectional features (INF),
Dependency type (DEP).
To predict parseractions fromhistories,representedas feature vectors,we use
support vector machines (SVM), which combine the maximum margin strategy
introduced by Vapnik [22] with the use of kernel functions to map the original
feature space to a higher-dimensional space. This type of classifier has been used
successfully in deterministic parsing by Kudo and Matsumoto[10], Yamada and
Matsumoto[11], and Sagae and Lavie[23], among others.To be more specific, we
usetheLIBSVMlibraryforSVMlearning[24],with
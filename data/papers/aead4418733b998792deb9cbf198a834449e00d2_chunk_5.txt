functions.Wethenaskwhetherlearningtointe-
tion highlights the difficulty of achieving robustness, com- grate a collection of functions implies that the model can
positionality,andout-of-distributiongeneralizationwiththe integrate a composition of those functions. Finally we de-
predominant modeling and learning approach, and the im- partfromthetrainingdistributionbystudyingextrapolation
portanceofevaluatingbeyondthetestset,acrossaspectsof to larger problems and values, then by finding adversarial
generalizationthatarerequiredbythetaskathand. exploitsthatexposegapsinthetrainingdistribution.
2 ProblemSetup Experimentalsetup. Weusetheimplementationandpre-
trainedmodelfromLampleandCharton(2019)forallofour
Symbolicintegrationistheproblemoffindingtheintegraly
experiments,specificallytheFWD+BWD+IBPmodelwhich
ofaninputequationx.Forinstance,x2/2istheintegralof
obtainedtop-10accuraciesof95.6%,99.5%,and99.6%on
x,uptoanadditiveconstant.
theirpubliclyavailabletestsets.1Ourevaluationisbasedon
Neuralsequenceintegrator. LampleandCharton(2019) theircode,weusetheirutilitiesforinputsandoutputs,and
framesymbolicintegrationasasequence-to-sequenceprob- bydefaultusebeamsearchwithbeam-size10.Followingthe
lem. In this view, input and output equations x and y are authors,weuseSympytocheckwhetherthederivativeofa
prefix-notation sequences. The neural sequence integrator prediction is equal to the original problem. We generously
uses a 6-layer transformer (Vaswani et al. 2017) to model count the prediction as correct if a timeout occurs. See the
the distribution p (y|x) = QTy p (y |y,x) by train- Apppendixforadditionaldetails.
θ t=1 θ t <t
ing the model to maximize the log-likelihood of a set of
P 2.2 AutomaticProblemDiscoveryWithSAGGA
training problems, argmax logp (y|x). Given
θ (x
latewaypointsequenceandcreateatopdowngridrepresen- faceswiththeflow-basedmodelarethrougheithersampling
tationofwaypointswithone-hotencoding. Thefinaltop- orevaluatingitsdensity,where,intheformer,wesample
downfeaturerepresentationisofdimension[W/r,D/r,C] fromp z(z)andmustcomputetheforwardtransformation
wherethenumberofchannelsC =C +C +1. f; in the latter, we must compute the inverse transforma-
attn resnet
tion f−1, its Jacobiandeterminant, andthe p (z) density
z
evaluation. The learning objective of the goal prediction
4.3.Multi-modeGoalDistribution
model is given by Eqn. 3, which is to minimise the KL
Wewishtoapproximatethetruepredictivedistributionover divergence between the true and approximate goal distri-
allpossiblegoalfuturesoftheego-agent,p(cid:63)(x goal|m),given bution,orequivalentlymaximisethelog-likelihoodofthe
theembeddingvectormfromtheobstacleawarenessmod- approximategoaldistribution:
ule (§4.2) based on the observation O from the environ-
ment. Unfortunately, the predictive intent of the expert minKL(p(cid:63)||p )=maxE logp (x)
θ x∼p(cid:63) θ
agentisnotobservablefromthetrainingdata: theexpert θ θ
(cid:88) (3)
maymakemultiplemanoeuvresgiventhesamescenario, ≈max logp θ(x)
θ
butweonlyobservetheexperttakingoneoftheoptions. As x∼D
aproxy,wetakeafuturestateoftheexpertagent,atfixed
ThelikelihoodisgivenininEqn. 1,whichismadetractable
timehorizonN,tobethe“ground-truth"ego-agent’sgoal,
throughthespecialdesignofthebijectivetransformation
x ≡x,whereN
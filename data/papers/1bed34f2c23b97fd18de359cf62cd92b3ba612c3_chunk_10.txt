 (cid:51) - GitHubNotebooks en
DS-1000(Laietal.,2022) 1,000 7 (cid:51) 1.6 StackOverflow en
CoNaLa(Yinetal.,2018) 2,879 open (cid:55) - StackOverflow en
MCoNaLa(Wangetal.,2022) 896 open (cid:55) - StackOverflow es,ja,ru
ODEX 945 79 (cid:51) 1.8 StackOverflow en,es,ja,ru
Hand-Written
Table 3: Comparing ODEX with other NL-to-code generation datasets, in terms of domain diversity (Domain),
test-case execution support (Evaluation, Avg. Test Cases), and natural language contexts (NL). Since it is hard
tocalculatetheexactnumberoflibrariesforsomeopen-domaindatasetsthatdonotspecificallyimportrequired
librariesinthecode,wemarktheirdomainsasopeninsteadofprovidingtheexactnumberofdomains.
theinputandoutputsides,codeintheEnglishset andtrainingdata. Modelsareprogressivelytrained
hasfewervariables,suggestingpotentiallysimpler on THEPILE (Gaoetal.,2020), BIGQUERY,7 and
execution environments, which could stem from BIGPYTHON datasets are denoted as NL, MULTI,
relativesimplicityofSOqueriesaskedinEnglish. andMONO. ThemostpowerfulCODEGEN-16.1B-
MONO, performs similarly to CODE-CUSHMAN-
3.3 ExecutionSupport
001 ontheHumanEvalandMTPBdatasets.
We systematically compare code generation
datasets that concern execution or open-domain PromptDesign Forfaircomparison,weusethe
codeinTable3. ODEXisthefirstdatasetthatsup- same prompt for both model families. While
portsexecution-basedevaluationforopen-domain promptingwithfew-shotin-contextexamplesmay
code. WhileODEXdoesnothavethelargestnum- improve,ourexperimentsdonotalwaysfindthis
ber of test cases, we discuss in ยง7 how these test helpfulforbothmodels. Therefore,wereportzero-
casescanstillrel
ner.
2.3 LISTENERDESIGN
Given a setof candidate images I (j ∈ 1,2...n+1), where n is thenumber of distractors, and
j
anutteranceu,thelistenercomputesembeddingsforeachimage,L(I ),fortheutterance,L(u). It
j
thentakesthedotproductofeachL(I )withL(u),andthesoftmaxofthedotproductstocompute
j
theprobabilityP(I |u)∝exp(L(I )L(u))oftheutterancereferringtoimageI foreachimage.
j j j
Thelisteneralsohasarule-basedcomponenttocontrolwhentogivelinguisticinputintheformof
theground-truthcaptionC(x). Weintroducetwothresholds,θ andθ. Afterthelistenercomputes
1 2
themostlikelytargetimage,t=argmaxP(I |u),andtheprobabilitygiventothischoice,P =
j max
maxP(I |u),itreturnsitschoicexˆandinputf accordingtothefollowinglogic:
j listener
 (noop,0) P <θ
 max 1
(xˆ,f )= (t,C(x)) θ <P <θ (2)
listener 1 max 2
(t,0) P >θ
max 2
This control strategy mimics a caregiver who only gives feedback when they understand what the
language learner is referring to, but wants to direct it to generate higher-quality utterances. When
thelistener’sconfidenceisverylow,itwillneitherselectatargetimagenorprovidelinguisticinput,
asthespeakerutteranceistoolow-quality. Meanwhile,whenthelistener’sconfidenceisveryhigh,
itwillstopgivinglinguisticinput,asfurtherimprovementisdeemedunnecessary.
3
PublishedasaconferencepaperatICLR2023
WeuseanLSTM-basedmodeltolearnembeddingsofspeakerutterancescombinedwithaResNet
modeltoderiveimagefeatures. Becausewewant
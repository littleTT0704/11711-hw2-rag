 par-
titions of domains, either source domain or target domain, which is the general
scenario in real world application.
Generic Model Regularization: CNNs are powerful models and tend to
overfit on source domain datasets. From this perspective, model regularization,
e.g., weight decay [19], early stopping, and shake-shake regularization [8], could
also improve the DG performance. Dropout [25] mutes features by randomly
zeroing each hidden unit of the neural network during the training phase. In
this way, the network benefit from the assembling effect of small subnetworks
to achieve a good regularization effect. Cutout [6] and HaS [24] randomly drop
patches of input images. SpatialDropout [26] randomly drops channels of a fea-
ture map. DropBlock [9] drops contiguous regions from feature maps instead of
random units. DropPath [11] zeroes out an entire layer in training, not just a
particularunit.MaxDrop[20]selectivelydropsfeaturesofhighactivationsacross
the feature map or across the channels. Adversarial Dropout [21] dropouts for
maximizing the divergence between the training supervision and the outputs
from the network. [12] leverages Adversarial Dropout [21] to learn discrimina-
tive features by enforcing the cluster assumption.
Key difference: RSC differs from above methods in that RSC locates and
mutesmostpredictivepartsoffeaturemapsbygradientsinsteadofrandomness,
activationorpredictiondivergencemaximization.Thisselectiveprocessplaysan
important role in improving the convergence, as we will briefly argue later.
3 Method
Notations: (x,y) denotes a sample-label pair from the data collection (X,Y)
withnsamples,andz(orZ)denotesthefeaturerepresentationof(x,y)learned
by a neural network. f(·;θ) denotes the CNN model, whose parameters are
denoted as θ. h(·;θtop) denotes the task component of f(·;θ); h(·;θtop) takes
z as input and outputs the logits prior to a softmax function; θtop denotes
the parameters of h(·;θtop). l
 RelatedWork
proposes fusion bottlenecks in transformer encoders, while
2.1 VisualGrounding [Zhaoetal., 2021]appendscross-modalattentionafterself-
attentionintheirfusiontransformerencoders.
Two-stage methods. Two-stage methods, also known as
propose-and-rankmethods,generateimageregioncandidates
3 Methodology
and then rank them using the information from text queries
to select the best matching one. The candidates are usu- We leverage the transformer-based network to tackle the vi-
ally generated using a pre-trained object detector [Wang et sual grounding problem. Unlike previous methods [Chen et
al., 2019] or unsupervised methods [Yu et al., 2018]. Some al., 2018; Yang et al., 2020; Deng et al., 2021; Chen et al.,
earlymethodsutilizemodularnetworkstocomputematching 2021] that extract visual and textual features separately, we
â€¦
â€¦
â€¦
â€¦
â€¦ â€¦
â€¦
sredocnE
remrofsnarT
query tokenğ‘“ weightedsumofthecandidatekernelscomputedas:
$%&â€™(
non-linear transformation ğœ‘
!!â†’# K
(cid:88)
W = Î± w (2)
softmax i i
ğ›¼!ğ›¼" ğ›¼# attention weight ğ›¼ i=1
By incorporating query information into the generation of
ğ‘¤
! convolutional kernels, QCM blocks can extract distinct fea-
+ ğ‘Š turesaccordingtodifferenttextqueriestoproducevisualfea-
ğ‘¤ " turesthataremorerelevanttotargetobjects.
candidate kernels aggregated kernel Visualencoder. Thevisualencoderconsistsofabackbone
equipped with QCM followed by a stack of transformer en-
Figure3: Thestructureofaquery-conditionedconvolutionmodule coderstoextractquery-awarevisualfeatures. Specifically,to
(QCM).ThereareKcandidatekernels{w 1,w 2,...,w K}. Anat- considermulti-scalefeatures,wereplacethefirstvanillacon-
tention weight Î± is derived from the query
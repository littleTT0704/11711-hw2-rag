ereotypicalbiasinpretrained
SaraHooker,NyallengMoorosi,GregoryClark,Samy languagemodels. InACL/IJCNLP(1),pages5356–
Bengio, and Emily Denton. 2020. Characterising 5371.AssociationforComputationalLinguistics.
BiasinCompressedModels. CoRR,abs/2010.03058.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
MasahiroKanekoandDanushkaBollegala.2021. De- Samuel R. Bowman. 2020. CrowS-Pairs: A Chal-
biasingPre-trainedContextualisedEmbeddings. In lengeDatasetforMeasuringSocialBiasesinMasked
EACL,pages1256–1266.AssociationforComputa- LanguageModels. InEMNLP(1),pages1953–1967.
tionalLinguistics. AssociationforComputationalLinguistics.
2668
KelechiOgueji,OrevaogheneAhia,GbemilekeOnilude, WhereAandB representtheattributesets,and
Sebastian Gehrmann, Sara Hooker, and Julia X andY arethetargetsetsofwords. Thesfunc-
Kreutzer. 2022. Intriguing Properties of Compres-
tioninEquation(1)denotesmeancosinesimilarity
sion on Multilingual Models. In EMNLP, pages
between the target word embeddings and the at-
9092–9110.AssociationforComputationalLinguis-
tics. tributewordembeddings:
Alec Radford, Jeff Wu, Rewon Child, David Luan,
DarioAmodei,andIlyaSutskever.2019. Language 1 1
modelsareunsupervisedmultitasklearners. s(w,A,B)= cos(w,a) cos(w,b).
A − B
| |a A | |b B
Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael
X∈ X∈
(2)
Twiton, and Yoav Goldberg. 2020.
 instruction for the basic math task (§3.2).
3.2 Categories and Tasks
We create 4 views3or categories of L¯ila along the dimensions of mathematical
area,languagecomplexity,externalknowledge,andquestionformat. Altogether,
these views classify the data into 23 tasks (Table 1). By creating multiple views
of the benchmark, we are able to systematically characterize the strengths and
weaknesses of existing models at a granular level.
The first category, math ability, partitions the datasets into common peda-
gogical subjects: arithmetic, algebra, geometry, calculus, etc.
Our second category, language complexity, separates math problems by the
complexity of the language used to represent them. This ranges from formal
representations only (e.g., 1+1=?) to natural language (e.g., “Mariella has 3
pears...”).
We next partition datasets based on the type of background knowledge,
required to solve the problem. For instance, commonsense questions like “How
many legs to 3 people have?” or science questions like “Will water boil at 200
degrees Celsius?” require different sets of knowledge to answer.
Lastly, we categorize based on question format, putting e.g., multiple choice
questionsunderonetaskandnaturallanguageinferenceunderanother. Examples
of each task and the datasets included are in Appendix B.
3Notethatitisnot apartitionofthebenchmarkaseachdimensionsdividestheconstituent
examplesindifferentways
6
3.3 L¯ila-OOD
In order to measure if the model has truly learned the underlying mathematical
reasoning skill, we evaluate both in-distribution (IID, i.e., standard train-test
splits)andout-of-distribution(OOD)performanceforeachtask,i.e.,weevaluate
on examples requiring the same underlying mathematical reasoning skill but
from a different dataset. To construct L¯ila-OOD, we follow Bras et al. (2020)
andHendrycksetal.(2020)byrandomlyassigningthedatasetsforeachtaskinto
IIDandanOODsets, usingtheIIDsetfortrainingandstandardevaluationand
the OOD set to evaluate generalization. We do not include tasks in L¯ila-OOD

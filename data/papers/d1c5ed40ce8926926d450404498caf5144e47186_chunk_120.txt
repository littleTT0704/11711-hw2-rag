craes
bew
86 CHAPTER 6. APPLICATION TO QUESTION ANSWERING
Regular J! Final J! TREC 8–12
No Expansion 81.33% 63.32% 76.74%
Random 83.98% 66.37% 78.90%
Round Robin 84.52% 69.04% 80.35%
Search Rank 85.58% 68.91% 79.64%
Cosine Sim 86.15% 70.30% 81.09%
LR Adjacent 86.23% 72.21% 82.17%
Table6.8: SearchrecallofWatsononJeopardy! andTRECquestionswhenexpanding
Wikipedia with web search results using different relevance estimation strategies.
Regular J! Final J! TREC 8–12
No Expansion 30.39% 13.32% 29.15%
Random 45.27% 18.65% 44.78%
Round Robin 39.91% 18.53% 39.82%
Search Rank 46.12% 24.24% 47.40%
Cosine Sim 46.15% 23.60% 47.17%
LR Adjacent 51.20% 27.79% 52.46%
Table6.9: SearchrecallofWatsononJeopardy! andTRECquestionswhenexpanding
Wiktionary with web search results using different relevance estimation strategies.
results. Wikipedia and Wiktionary were used as seed corpora, and performance was
evaluated on both Jeopardy! and TREC datasets. We compared the search recall of
Watson when using the seed corpora without SE to expanded sources generated with
differentrelevanceestimation baselinesand thelogistic regression modelwith features
of adjacent instances (LR Adjacent). As baselines, we used random rankings of text
nuggets (Random), an approach that selects nuggets from the top of the retrieved web
pages (Round Robin), Yahoo! search rankings (Search Rank), and rankings of nuggets
by their cosine similarities to the seed documents (Cosine Sim). These methods were
described in more detail in Section 5.2. Note that we adjusted the relevance threshold
and the maximum length of the expanded documents in the merging step of the SE
pipeline (Section 4.4) to generate expanded sources of similar sizes, and thus we can
direct
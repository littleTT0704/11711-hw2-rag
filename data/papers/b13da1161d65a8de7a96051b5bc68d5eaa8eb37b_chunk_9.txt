ivetraining. Thefinalself-trainingob-
(1) jectivecanbewrittenintermsofpseudo-labelsas
ps (ψ,φ)=σ(G ◦E (xs))|, (2)
imc ψ φ i m,c
Lt =− 1
(cid:88)Nt H (cid:88)×W (cid:88)C
y˜t log(cid:0) pt (cid:1) (4)
where σ denotes softmax operation and an adaptation ob- st N imc(cid:48) imc(cid:48)
t
i=1 m=1 c(cid:48)=1
jectiveoverthetargetdomainasdescribednext.
Pseudo-labelself-training(PLST):Followingpriorworks TheoverallUDAobjectiveissimply,L = Ls +α Lt,
uda cls st st
[60,64], we describe a simple and effective approach to whereα istherelativeweightingcoefficients.
st
PLSTthatleveragesasourcetrainedseedmodeltopseudo-
label unlabelled target data, via confidence thresholding. 3.1.SupervisionForObjectnessConstraint
Specifically, the seed model is first trained on Ds using
An important issue with the self-training scheme de-
Eqn. 2 to obtain a good parameter initialisation, {φ,ψ }.
0 0 scribedaboveisthatitisusuallypronetoconfirmationbias
Then,thismodelisusedtocomputepixel-wiseclassprob-
that can lead to compounding errors in target model pre-
abilities,pt (ψ,φ )usingtoEqn.2foreachtargetimage,
im 0 0 dictions when trained on noisy pseudo-labels. To alleviate
xt ∈Dt. Theseprobabilitiesareusedinconjunctionwitha
i target performance, we introduce auxiliary modality infor-
predefined threshold δ, to obtain one-hot encoded pseudo-
mation (like, depth) that can provide indirect supervision
labels
for semantic labels in the target domain and improve the
(cid:40) robustness of self-training. In this
sharesthisproperty.
parameterspace,rMGS =
(cid:18) Ontheotherhand,MLE-guidedparametersearch,
E (cid:15)(cid:24)q[w^((cid:15))exp((cid:11)(c(F((cid:18);X);Y)(cid:0)c(F((cid:18)+(cid:15);X);Y))(cid:15)];
rMGS
=X
[w^((cid:1) )exp((cid:11)(C((cid:18))(cid:0)C((cid:18)+(cid:1) )))(cid:1) ];
(cid:18) k k k
where we consider a single example andrewrite theMGS
k
update (3) in order to illustrate how the use of noise and
weightseachcandidatedirectionusingalossC((cid:1))computed
thedecodingalgorithmdifferfrompolicygradient.Seethe
overtheentireminibatch(seeEquation1).Thishastheeffect
Appendixforthederivation.Inshort,policygradientuses
of ‘densifying’ the sparse loss by pooling the losses from
eachparameter(cid:18)tosamplemultiplesequencesforeachinput,
multipleexamples.
whileMGSsamplesmultipleparameters,anduseseachto
decodeasinglesequenceperinput.
4 RelatedWork
Sequence-leveltrainingforNLP. Sequence-leveltraining
Comparison with minimum risk training. Minimum
methodsbasedonpolicygradienthavebeenappliedtosev-
risk training (MRT) (Shen et al. 2016) approximates the
eralNLPtasks(Liuetal.2017;Ziegleretal.2019).Related
policygradientobjective(6)as,
methodsusepolicygradientwithgenerativeadversarialnet-
h i works(GAN)(Yuetal.2017).Policygradientmethodsoften
C ((cid:18))= E E c(Y^;Y) ; (8)
MRT
(X;Y)(cid:24)D
Y^(cid:
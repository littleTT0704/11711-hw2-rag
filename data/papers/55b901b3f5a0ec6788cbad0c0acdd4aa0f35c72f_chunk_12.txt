
Hereβ∗ =min β,β andλ∗istheweightingofthefunctionwithsmallersmoothness.
e a
{ }
Proof. SeeAppendixEforfullproofandAppendixFformorediscussion
Asadetailedinspectionoftheproofwillshow,wederiveEquation1byappealingtoalgorithmic
stability (Bousquet& Elisseeff, 2002; Hardt etal., 2016; Kuzborskij & Lampert,2018) (Section
2). To our knowledge, ours is the first work to present an algorithmic stability view to formally
explainhowauxiliarylearninginfluencesend-taskperformance. Equation1surfacesthefollowing
prescriptionsaboutlearningwithauxiliarytasks:
(P ) Smaller∆improves(cid:15). Thisimpliesthatthemoresimilartheauxiliaryobjectiveistothe
1 gen
end-task(underAssumptionA.1),thelowerthegeneralizationerror.
(P ) LargerN(cid:48)leadstosmaller(cid:15) 4. SinceweusuallyhaveafixedamountoftaskdataN,wecan
2 gen e
increaseN(cid:48)byaddingmoreauxiliarydataN.
a
5 END-TASK AWARE SEARCH OF STRUCTURED OBJECTIVE SPACES
Guided by Section 4, we build a practical
Algorithm1AANG
methodforexploringasetofobjectives,.
Input:SearchSpace-A A
Factorvectors-{WAll,WI,WT,WR,WO} Whilst the dynamic sampling setting
End-task-E,End-taskweight-λ described in Section 4 is amenable to
e
InitialModelParams-θ ∈RD theoretical consideration, we make a few
0
repeat practical changes to it. First, instead of
Sample a batch of n objectives performing alternating gradient descent
Kn ∼A
by sampling f,f according to λ,λ,
Weighting of objectives in Kn a e e a
we instead use them as multitask weights
Constructwn
and perform joint training. Joint training
fork=1tondo
(d, t, r, o)=[Kn].stages hasbeenfoundtoproduces
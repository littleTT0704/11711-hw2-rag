 sense
since in this setting about 93% of the relevant text nuggets in the positive class are
replaced with irrelevant nuggets. Thus only 7% of the nuggets in the positive class
are really relevant, which is about the same as the percentage of relevant nuggets in
the entire dataset and the percentage in the negative class. This means that there is
no signal in the training data. If additional labels are flipped, the model ranks text
nuggets in reverse order of relevance, and performance drops below random.
Most of the relevant text nuggets closely match the word distribution of the seed,
are ranked high by the search engine and consist of well-formed English text. These
nuggetsusuallyhavesimilartopicality,searchandsurfacefeaturevaluesandthusform
arelativelycompactclusterinthefeaturespace. Ontheotherhand, somenuggetsare
relevant even though they are dissimilar from the seed, their source documents were
retrieved at low ranks, or they are of low textual quality. These instances are outliers
in the dataset that do not fall within the cluster of “typical” relevant text nuggets.
In practice, labeling errors do not occur uniformly but annotators more often err
in these unlikely cases. For instance, annotators may overlook relevant information
that is not mentioned in the seed or may be less diligent when labeling low-ranking
search results. These types of errors seem even less problematic than random label
noise since a model that fails in such cases may still be useful for selecting most of
the clearly relevant text nuggets. In other words, it may be of little consequence if
outliers are misclassified as long as the model can identify text nuggets that belong
to the main cluster of relevant instances.
Both the logistic regression models and the cosine similarity baseline leverage the
5.4. ROBUSTNESS OF RELEVANCE ESTIMATION 65
content of seed documents to estimate the relevance of text nuggets to the topics of
the seeds. Thus the effectiveness of these methods depends on the quality and length
of the seed documents. To evaluate the extent to which both approaches are affected
by the amount of useful content and noise, we artificially degraded the Wikipedia
seeds in our annotated dataset in two different manners:
1. We randomly dropped tokens from the seed documents.
2. We replaced tokens in the seeds at random with tokens drawn uniformly from
a
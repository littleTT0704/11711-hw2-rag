, recall, F1andAUCscoresonthezero-shotcodes
However, since these methods never train the binary classi-
for all methods. Micro metrics aggregate the contributions
fiers for zero-shot codes on positive examples, both micro
of all codes to compute the average score while macro met-
and macro recall and F1 scores are close to zero. In other
ricscomputethemetricindependentlyforeachcodeandthen
words, these models almost never assign zero-shot codes at
taketheaverage. Allscoresareaveragedover10runsusing
inferencetime. ForWGAN-GPbasedmethods, allthemet-
differentrandomseeds.
ricsimprovefromZAGRNNandmeta-embeddingexceptfor
Results on seen codes. Table 1 shows the results of ZA- microprecision. Thisisduetothefactthatthebinaryzero-
GRNN models on all the seen codes. With ZAGRNN, al- shot classifiers are fine-tuned on positive generated features
mostallmetricsslightlyincreasedfromZAGCNNexceptfor whichdrasticallyincreasesthechanceofthemodelsassign-
micro precision. With L loss, our final feature extrac- ingzero-shotcodes.
LDAM
torcanimprovemoresignificantlyfromZAGCNNespecially
for macro metrics and achieve better precision recall trade- Ablation studies. We next examine the detailed perfor-
off. The p-value from Studentâ€™s t-test for comparing micro mance of each component of AGM-HT as shown in Ta-
F1 scores between LDAM and baseline is 10(cid:0)8, indicating ble 2. Adding L CLS hurts the micro metrics, which might
theimprovementofusingLDAMissignificant. Nonetheless, becounter-intuitiveatfirst. However,sincetheL CLS iscom-
these modifications are not enough to get reasonable perfor- putedbasedonthepretrainedclassifiers,whicharenotwell-
mance zero-shot codes due to the lack of positive example generalized on infrequent codes, adding the loss might pro-
for zero-shot codes during training. Results of Transformer videbadgradientsignalforthegenerator.AddingL CYC;L KEY
as feature extractor is shown in the second row of the table
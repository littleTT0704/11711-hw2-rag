.665 0.697 0.636
CLGS 0.674 0.739 0.619
LT DSE 0.695 0.715 0.677
VOT-LT2020 RLT-DiMP 0.681 0.667 0.695
Table 1. Comparison with the state-of-the-art methods on VOT-LT2019 and VOT-
LT2020 benchmarks. Both benchmarks are based on the LTB50 dataset.
dataset contains 50 sequences of various objects with a total length of 215,294
frames for single-target object tracking. In each sequence, the target disappears
anaverageof10times,andthedisappearedtargetlastsanaverageof52frames.
The resolution of video sequences is between 1280 × 720 and 290 × 217. All
targets are marked with an axis-aligned bounding box.
Evaluation protocol TheproposedRLT-DiMPisevaluatedbytheevaluation
protocol of the VOT-LT2020 benchmark. An evaluation protocol for long-term
trackersfollowsano-resetprotocol,whichmeansthattheobjecttrackerwillnot
restart even if the object tracking fails. Three evaluation metrics are adopted
for the long-term tracking benchmark: tracking precision, tracking recall, and
tracking F-measure. For additional information, please see [24]. This evaluation
is automatically performed by the VOT toolkit [8,19]. All experiments are per-
formed on a system with Intel(R) core(TM) i7-4770 3.40 GHz processor and a
single NVIDIA GTX 1080 Ti with 11GB RAM.
4.2 Quantitative Evaluation
Overall comparison with long-term trackers. We compare our proposed
method with the state-of-the-art methods in the VOT-LT2019 benchmark [18].
TheVOT-LT2019andVOT-LT2020benchmarksarebasedontheLTB50dataset
[24]. Both of competing methods and our method are re-detecting long-term
trackers (LT1). This term means that all of the trackers detect tracking failure,
and an explicit re-detection technique is implemented, unlike the pseudo long-
term tracker (LT0). The following taxonomy has been introduced explicitly in
[24].
Table1showsthequantitativeevaluationoflong-term
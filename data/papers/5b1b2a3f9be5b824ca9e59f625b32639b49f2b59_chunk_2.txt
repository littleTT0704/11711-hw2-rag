 global context. To involve the global context, some
objects in video sequences. methods [18], [60], [26] generally concatenate reference and
With strong capabilities for modeling long-range data target feature maps together; such “fusion” procedures do
dependencies, some transformer-based methods [55], [23] not distinguish reference and target frames, and thus the
achieve impressive results. However, they deal with the input specific importance of the target frame is ignored. Moreover,
atthevideo-orclip-level,thusincurringhighlatency.Thecost as reference frames are usually similar to the target, spatio-
ofmodelingfullspatio-temporalcorrelationsisprohibitivefor temporal correlations among them could be highly redundant.
practical applications. In this work, we focus on online VIS Some reference features unrelated to the target could even
for streaming applications. The problem setting is as follows: mislead the target prediction.
given a small set of preceding reference frames, the goal is Besides the contextual information contained in reference
to segment, classify and track object instances in each target video frames, information in other modalities, such as audio,
frame. can also serve as reference context to guide the VIS task.
In addition to the challenges of segmentation and classifi- Visualobjectsoftenhaveacousticsignaturesthatarenaturally
cation in the image domain, online VIS should also address synchronized with them in audio-bearing video recordings.
the problem of finding correspondences and fusing contexts While audio-visual synchrony has been exploited in many
in adjacent frames. Yang et al. [66] approach this problem contexts, most audio-visual representation learning methods
by performing frame-based predictions independently and handle signals in constrained environments, with clearly-
fusing the evidence across frames in a post-processing stage evident audio-visual correspondences, such as music perfor-
using sophisticated matching algorithms. Matching as post- mance or lip reading. We deal with videos with audios in the
processing has a high cost, and the final result may suffer wildwherethecorrelationbetweenaudioandvisualsignalsis
fromflickeringbecauseofneglectingfeature-levelcorrelations difficult to establish. Previous methods [70], [53], [50], [45]
have already demonstrated the correlation between audio and
Part of this work was done when Xiang Li was an intern at Microsoft video mod
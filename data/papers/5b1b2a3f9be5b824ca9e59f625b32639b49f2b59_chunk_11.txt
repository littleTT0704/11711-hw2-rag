·C(cid:55)→C(cid:48) H×W(cid:55)→K×K t
a target frame I t and reference frames {I r}t r− =1 t−δ, we first where f tref = f t−1⊕,...,⊕f t−δ is a concatenated feature of
extract the target feature f t and reference features {f r}t r− =1 t−δ allreferencefeatures,φ H×W(cid:55)→K×K isapoolingordepth-wise
withasharedbackbone.Consideringtheredundancyandnoise conv layer to compress the spatial dimension from H×W to
in the pixel-wise correlations from reference to target frames, K×K,ϕ δ·C(cid:55)→C(cid:48) isa1×1convlayertocompressthechannel
we compress the target and reference context as tokens O
tgt
dimension from δ·C to C(cid:48), W =ϕ δ·C(cid:55)→1(f ref) is a learned
and O respectively. Then, a transformer encoder in the pixel-wiseweightmap(visualizedinFigure4(b)).Wegetthe
ref
robust context fusion (RCF) module fuses the original tokens final reference tokens as O ref ∈RC(cid:48)×K·K.
as O(cid:48) =O t(cid:48) gt⊕O r(cid:48) ef, where O t(cid:48) gt and O r(cid:48) ef are fused tokens Token fusion. The target tokens O tgt and reference tokens
correspond to target and reference contexts. Afterwards, O t(cid:48) gt O ref are concatenated and fused with a transformer encoder,
isdecodedintothetargetsegmentationmapS andtheoverall and then we get the fused tokens O(cid:48) = O(cid:48) ⊕ O(cid:48) ∈
t tgt ref
O(cid:48)isdecodedintoaninstancecode
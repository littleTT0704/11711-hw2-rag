 − i) × h (x˜))) where verge due to their adversarial training nature. IRM has a
s v v
i ∈ {1.0,0.9,...,0.1}. These interpolation results verify similarpatternyetismorestable. Thankstotheprimal-dual
the smoothness and continuity in the variation space, and algorithm,DDGcanconvergemuchbetterthantheabove
also show that our model is able to generalize in the em- methods,beararesemblancetotheERMcounterpart.
beddingspaceinsteadofsimplymemorizingexistingvisual Evaluation of domain
information. Asacomplementarystudy,wealsogenerate divergence. WeusetheA- 0.95 DDG (ours) 0.938
imagesbylinearlyinterpolatingbetweentwosemanticfac- distancetomeasuredomain 0.9 E IRR MM 0.889
torswhilekeepingthevariationfactorsintact. Weprovide discrepancy [5]. This can 0.85 0.814 0.845
0.8 0.794
additionalqualitativeresultsinAppendixD. be approximated as d =
A 0.75
2(1−2σ),whereσistheer-
4.2.NumericalResults 0.7
ror of a two-sample classi- 0.65 0.648
ComprehensiveexperimentsshowthatDDGconsistently fierdistinguishingfeatures 0.6
From PCS to A From PAC to S
outperformsallthebaselinesbyaconsiderablemargin.From ofsamplesfromsourceand
Figure6. A-distanceonlearnedfea-
Table1, weobservethatDDGachievesbetterDGresults targetdomains[50]. Fig. 6 turesfordifferentgeneralizationtasks.
bothinmostsingledomainsandonaverage. Inparticular, showsthatDDGcanlearn
theperformancegainisgreaterintheworst-casescenario moreinvariantfeaturestominimizethedivergencebetween
liketheCandSdomaininPACS.Thisisparticularlyimpor- sourceandtargetdomainsthanIRMandERM.
tantsinceaverageperformanceisnotaneffectiveindicator QualitativecomparisonwithAugMix. BothFig
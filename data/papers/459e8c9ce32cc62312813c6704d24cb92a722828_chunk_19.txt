hefirstandsecondvolumesofthefirstandsecondvolumesofthesecond...
N ReyMangaintheUnitedStates.heosi 51.2
0
Table3:Examplesequencesdecodedfromsampledcandidates,showingthecomponentthatthecandidatewassampledfrom.
trolstheentropyofthecandidateweights.As(cid:11)decreases,
thecandidateweightsaresmoothedtowardsuniform,which
allocatesmoreweighttotheMLEcandidates,asseeninFig-
ure2.Performancedecreaseswhentheweightsareeithertoo
uniformortoopeaked,asseeninFigure3.
5.2 MachineTranslation
Experimental setup. We experiment on the IWSLT â€˜14
German to English task (Cettolo et al. 2014) using a stan-
Figure1:Taskloss(solid)andperplexity(dashed)as(cid:11)varies. dard experimental setup from the fairseq (Ott et al. 2019)
repository which we detail in the Appendix. We train the
MLE baseline and a MGS models with the same hyper-
WealsoreportresultswithancestralsamplingintheAp- parameters.Weuse4candidatesandagridsearchovernoise
pendix. We observe similar trends - MGS performs com- (f0:01;0:1;1:0g) and (cid:11) (f1:0;10:0;100:0g). The noise is
parablytoMRTbutwithbetterperplexity,andPGfindsa scaledby j(cid:18)1 jkr (cid:18)L MLEk 1.
degenerateshort-sequencesolutionundertheLMloss. For fine-tuning, we use a batch size of 16k tokens, and
Insummary,allthreemethodsimprovethetaskloss,and accumulategradientsfor4iterations.Weselect(cid:11) = 100:0
MGS does so while having a favorable balance across the andnoise1:0forallMGSfine-tuningbasedonagridsearch
other metrics (e.g. perplexity, repetition). We find that (cid:11
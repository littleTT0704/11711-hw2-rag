2020)releasedthemultilin-
significantlyimprovedwithouttoomuchsacrifice gualsummarizationdatasetspanning5languages
inthehigh-resourcelanguages. Similartothelow- with1.5Marticle-summarypairs. (Caoetal.,2020)
resource experiment of (Hasan et al., 2021), our createdanewdatasetfortwolanguageswith400k
resultisstrongerthantheirs,whichonlyselects5 samples. (Hasanetal.,2021)introducedXL-Sum
low-resourcelanguagestofine-tuneindividualLM. spanning 45 languages containing 1.1M article-
summarypairs. Morerecently,(VarabandSchluter,
MPF v.s. MPS adapter v.s. MPS prefix: By 2021)releasedMassiveSummcontaining28.8mil-
addinglanguage-specificparametersinthemulti- lionarticlesacross92languages.
lingualscenario,comparedtoMPF,MPS adapter
andMPS prefixhaveperformanceimprovements
4.2 ParameterEfficientTuning
R1:0.73,R2:0.46,RL:0.45andR1:0.67,R2:0.47,
RL:0.46respectivelyfor45languagesonaverage. Parameter-efficient tuning methods only tune a
Fromeachlanguageperformance,wecanseethat small number parameters to achieve comparable
allhigh-resourcelanguageshaveperformanceim- results. It can be roughly divided into two cate-
provementsatthecostofjeopardizingtheperfor- gories,methodswithoutadditionalparametersand
mance of a few low-resource languages a little. methods with additional parameters. The former
ThisindicatesthatsharingLMaswellasaddingpri- tunepartofthepre-trainedLM.(Leeetal.,2019)
vatelanguage-specificparameterswillmaintainthe fine-tunes a few of the final layers, while (Min
jointlymulti-lingualtrainingâ€™sadvantageofshar- et al., 2021) only fine-tunes the bias terms of the
inginformationamonglanguages,whilereducing LM.Thelatterintroducesextraparameterswhile

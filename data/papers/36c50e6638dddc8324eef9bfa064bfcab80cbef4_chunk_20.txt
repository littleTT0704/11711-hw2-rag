versions(i.e.,Blender-
WeshowthatProstcangeneralizetounseenreal-
Bot1andGPT-3;61.8%and70.2%,respectively).
world,human-writtentoxicphrases,inadditionto
However, neutral responses can still be harmful
properlyrespondingtothein-domainproblematic
compared to disagreeing ones, especially in the
content from PROSOCIALDIALOG. We evaluate
face of toxicity, since it can be perceived as con-
Prost and other dialogue agents on how they re-
doningtheunacceptablebehavior.
spondtoutterancesfromRedditinToxiChat(Ba-
hetietal.,2021). DetailsareinAppendixD.1. 6.2 ImprovingProsocialityofPre-trained
Baselines. We compare our two Prost mod- LanguageModelswithCanary
els(§4.2)withfivebest-performingconversational
Wefurtherdemonstratetheusefulnessof PROSO-
agents: DialoGPT, BlenderBot 1, BlenderBot 2
CIALDIALOG byshowingthatCanary-generated
(Komeilietal.,2021),GPT-3,andInstructGPT-3.7
RoTscansteerlargepre-trainedlanguagemodels
Evaluation metrics. We report the stance, of-
(PLMs)towardsprosocialresponses. Specifically,
fensiveness,andtoxicityofmodels’responsesfol-
we sample 600 dialogues from the PROSOCIAL-
lowingBahetietal.(2021). First,thestanceclas-
DIALOG test set that Canary predicts not to be
sifiercategorizeseachresponsewiththreeclasses:
CASUAL and evaluate PLM responses with and
disagree,agree,andneutral. Then,theresponses’
withouttheRoTsfromCanary.
offensivenessispredictedbyabinaryclassifier. We
Targetmodelsandmetrics. WeapplyCanary
alsodeterminewhetherresponsescontainbad(i.e.,
toGPT-3andInstructGPT-3. WeappendtheRoTs
toxic)n-gramsfrom
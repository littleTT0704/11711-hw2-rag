Ontological Learning from Weak Labels
LarryTang PoHaoChou YiYuZheng
ECEDepartment ECEDepartment ECEDepartment
CarnegieMellonUniversity CarnegieMellonUniversity CarnegieMellonUniversity
lawrenct@andrew.cmu.edu pohaoc@andrew.cmu.edu yiyuz@andrew.cmu.edu
ZiqianGe AnkitShah BhikshaRaj
ECEDepartment LTIDepartment LTIDepartment
CarnegieMellonUniversity CarnegieMellonUniversity CarnegieMellonUniversity
ziqiang@andrew.cmu.edu aps1@andrew.cmu.edu bhiksha@cs.cmu.edu
Abstract
Ontologiesencompassaformalrepresentationofknowledgethroughthedefinition
ofconceptsorpropertiesofadomain,andtherelationshipsbetweenthoseconcepts.
In this work, we seek to investigate whether using this ontological information
willimprovelearningfromweaklylabeleddata,whichareeasiertocollectsinceit
requiresonlythepresenceorabsenceofaneventtobeknown.WeusetheAudioSet
ontologyanddataset,whichcontainsaudioclipsweaklylabeledwiththeontology
concepts and the ontology providing the "Is A" relations between the concepts.
Wefirst re-implementedthe modelproposed by[1] withmodification tofit the
multi-labelscenarioandthenexpandonthatideabyusingaGraphConvolutional
Network (GCN) to model the ontology information to learn the concepts. We
findthatthebaselineSiamesedoesnotperformbetterbyincorporatingontology
informationintheweakandmulti-labelscenario,butthattheGCNdoescapture
theontologyknowledgebetterforweak,multi-labeleddata. Inourexperiments,
wealsoinvestigatehowdifferentmodulescantoleratenoisesintroducedfromweak
labelsandbetterincorporateontologyinformation. OurbestSiamese-GCNmodel
achievesmAP=0.45andAUC=0.87forlowerlevelconceptsandmAP=0.72and
AUC=0.86forhigherlevelconcepts,whichisanimprovementoverthebaseline
Siamesebutaboutthesameasourmodelsthatdonotuseontologyinformation.
1 Introduction
Ontologies represent hierarchical concepts in our brains through categories and relationships of
domainknowledge. Forexamplewhenwehearasound, evenifwedon’trecognizethespecific
animal species making the sound, we still recognize that given sound is a type of animal sound.
Likehumans,amachinecanutilizetheontologyinformationtohelptoclassifyanobjectithasn’t
seenbeforeasahigherlevellabel. Incorporatingthesehierarchicalrelationsfromontologiescan
improveclassificationofsemanticallydifferentobservationsthatappearsimilarorprovidemore
generaldescriptorsofambiguoussubclasses. Manyrecentworkshavelookedintohowtousethis
externalknowledgetoextractbetterfeaturerepresentationsaswellashowtoembedtheknowledge
intomodelarchitectures.
Inthispaper,weinvestigatewhetherthelearningofontologicalclassesfromweaklabeleddatacan
beimprovedbyincludingexternalknowledgeoftheserelationships. Forexample,anaudioclipmay
containadoghowling,ababycrying,andanengineidling,butisweaklylabeledsimplyasadog
howlingispresent. Wewillpredicttheontologicalinformationofthatdatapoint,whichincludes
animalsoundsandlivingthingsasthehigherlevelconcepts. Theontologyknowledgebasecanbe
34thConferenceonNeuralInformationProcessingSystems(NeurIPS2020),Vancouver,Canada.
2202
raM
4
]DS.sc[
1v38420.3022:viXra
alsobeprovidedorincorporatedintotheneuralnetworktoaidintheclassificationtask. Oneofthe
challengesinthispaperisthatthenetworkshouldalsobeabletonoticethattheremaybepartsof
thedatathatcouldbelongtootherontologicallabels,orcouldbetaggedinthesamehigherlevel
ontologicallabelbutmaynotbelabeledassuch. Forexample,apieceofanaudioclipcontaining
bothdoghowlingandcatmeowingmayonlybeweaklylabeledasdoghowling, whilebothdog
howlingandcatmeowingcouldleadtothepredictionofthehigherlevelontologicallabelanimal
soundsorlivingthing.Wewouldliketolookintowhetherthenetworkcanusethathiddenknowledge
toimprovethepredictionforrespectiveontologicalcategories.
Theadvantageofweaklylabeleddatasetsisthatweaklabelsareeasiertocollectsincetheyrequire
onlythepresenceorabsenceofaneventtobeknown,whichleadstobetterscalabilityasdemonstrated
byGoogle’sAudioSet[2]. However,theymayintroducenoiseintothelabelswhichmakesitdifficult
formodelstolearnfrom.Investigatingthisproblemisthusimportantbecauseitmayleadtoimproved
performanceforclassificationtasksinwhichitisdifficulttoobtainstronglylabeleddatasets. Welook
toimproveuponsomepreviousworkinthespaceofontologyprediction,hierarchicallearning,and
knowledgegraphsbyimplementingtwodifferentmodelswhichembedtheontologyinformationto
predicttheontologyclassesandhierarchiesfromweaklabels. Wehopetogainabetterunderstanding
ofontologicalembeddings,whethertheycanimproveclassificationintheweaklabelscenario,and
theadvantagesordisadvantagesofusingweaklabels. Thefollowingsectionsgiveamoredetailed
backgroundofrecentliteratureandthendiscussesthetechniquesandmodelsweimplementandour
finalresultsandconclusions.
2 RelatedWork
Domainknowledgecanberepresentedindifferentwaysbutherewefocusonontologybasedor
knowledgegraphrepresentationswhichcancapturebroaderrelationsbetweenconceptsthatarenot
onlyhierarchical. Therearemanyrecentworksinvestigatinghowtoembeddomainknowledgeto
improveclassificationaswellasthepredictionoftheactualontologyclassesthemselves. Onelineof
researchhasbeenonhowtousethesehierarchicalconceptstoimprovefeatureembeddings,while
anotherhasfocusedonhowtoincorporatethisknowledgeintotheactualnetworkarchitecture. The
followingparagraphsdiscussrecentsurveysandworkonthesetwomethods.
FeatureExtractionusingOntologies Inarecentsurveypaper[3],theydiscusshowtheuseof
ontologiesforfeatureselectionhavefocusedonapplicationsintextclassification. Onerecentwork
usestheWordnettaxonomy[4]asanontologytocomputeasimilaritymeasurementthatdetermines
whichfeaturestokeepordiscard. Otherworkshaveusedontologiestomaptextdocumentstoamore
generalconcepthierarchy,essentiallyrepresentingthecontentofthetext.Someotherfieldshaveeven
usedbuildingtypesasconceptsandmedicalontologiestoidentifyfeaturesfordrugclassification.
Theembeddingstrainedfromontologiesarealsousedtoimprovesomespecificlearningtaskssuch
asZeroShotLearning. In[5],theauthordefinedanontologyschemaasaninterfaceforknowledge
graphssuchasWordNet[4]andNELL[6]. Theyusetheontologyschemaasaninputwithsome
noisetoaGenerativeAdversarialNetworkstoimproveZeroShotLearning.
Anotherrecentsurveypaperrelatedtotheuseofontology,[7],discusseshowontologiesallowan
interactionbetweendatastoredindifferentformatsandcanfurtherbeusedasthebasistoguideand
validatemodelsofsomespecificdomains. Basedonthisprospect,futureresearchcanbefocusedon
aninvestigationofthefeasibilitytouseontologicalknowledgebaseasafoundationtoefficiently
discoverusefulinformationtoassistanalysis. Suchresearcheffortsinclude: (1)Towhatextentthe
datacanbeextractedandaggregatedand(2)Howcantheextracteddataaswellasdomainknowledge
berepresented. Allofthesetechniquesfocusonhowtousetheontologyknowledgetoextractbetter
featurerepresentationsthatmodeltheontologyinformation.
ClassificationusingOntologies Therehavebeensomeworkstryingtoapplyontologytomitigate
thelabelnoisesinaudiodata. In[8],aMulti-taskLearningbasedGraphConvolutionalNetwork(MT-
GCN)isproposedtoutilizeontologyinformationtoregularizemulti-labelaudiotaggingproblem.
The model is an extension from the network proposed by [9], which uses GCN [10] to learn the
dependencybetweenmultiplelabelsinimages. Insteadofusingthelabeldependencyascorrelation
graph,MT-GCNchoosestousetheontologyhierarchyinformationtoconstructthecorrelationgraph.
Tobemorespecific,itrepresentsnodebyontologyclassandbuildstheircorrelationaccordingtothe
2
parentchildrelationorconnectionism. ItthenusestheBinaryCrossEntropyLossinformationfrom
AudioTaggingtaskstotraintheembeddingofontologyclass.
Onthesubjectofpredictionofontologicallabels,[11]byG.Wichernetalconstructedanontological
framework where sounds are connected to each other based on the similarity between acoustic
features,whilesemantictagsandsoundsareconnectedthroughlinkweightsthatareoptimizedbased
onuser-providedtags. Morespecifically,theyproposedlinkweightsbetweendifferentsoundsamples
tobedeterminedusingalikelihood-basedtechnique,andestimatedahiddenMarkovmodel(HMM)
fromeachofthetrajectoriesoffeatures. Theyhavealsoproposedlinkweightsbetweenconcepts
using vector metrics, a similarity metric from the WordNet::Similarity library, and link weights
betweenconceptsandsoundsbycomputingKLdivergencebetweentheoutputoftheontological
frameworkandavotesmatrixoftags,andfromthatconstructedanintegratedsystemfortext-based
retrievalofunlabeledaudio,content-basedquery-by-example,andautomaticannotationofunlabeled
soundfiles. Theirapproachofrelatingsoundswithsimilarfeaturestogetherisheuristicforlearning
higherlevelontologicallabels.
3 Methods
3.1 DatasetandOntology
In this paper, we use the AudioSet dataset, which consists of an ontology of 632 audio event
classes and 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. The
128-dimensionalAudiosetfeaturesandontologyfilesaremadeavailablebyGoogleandcanbeeasily
accessedonlinefrompreviouswork[12],[2]. Thedatasetisamassivecompilationof10secondclips
fromvariousYouTubevideoswhichhavebeenweaklylabeled. Thefeatureembeddingsprovided
by[2]areextractedfromaVGGishmodelfollowedbyaPCAtoreducedownto128dimensions
andeachtensecondclipisrepresentedbyten128-dimensionfeaturevectors,i.e. onefeaturevector
representsonesecondofoneclip. Theentiretensecondclipcancontainmultiplelabelsoutofthe
total527classesintheAudiosetdata. Theclassesencompassawidevarietyofsoundssuchashuman
noises,music,animals,orvehicles. ThefirsttwohigherlevelclassesareshowninFigure1.
Figure1: ThetoptwohighestlevelclassesintheAudioSetontology
WeusethetrainingsetofAudiosetfortrainingourmodels,whichcontainsabout22,160tensecond
clipsorabout221,160featureembeddings. Wethenpreprocessthecorrespondinglabelsofeach
audio clip to get the labels in the top two levels of the Audioset ontology. The validation set is
3
about20%ofthetrainingsetsizeandisdrawnfromtheunbalancedtrainingsetofAudioset,and
theAudiosettestsetcontainsabout20,383audioclips. Thesamelabelprocessingisdoneforthe
validationandtestsettoobtaintwolevelsofontologylabels. Althoughtherearemorelevelstothe
Audiosetontology,wechoosetofocusonthetoptwolevelstofitourmodelarchitectures.
TogiveamoregeneralideaoftheAudiosetdata,consideraweaklylabeledaudioclipwhichcould
havemultiplelabelswithnotiminginformation,butthelabelscouldberelatedduetotheirontology.
Forexample,ahumanvoiceandhandscouldpossiblyco-occurbecausetheyarebothnaturalsounds.
Thelabelscouldalsoberelatedduetotheirdependency,suchasahumancryingcouldco-occurwith
sadmusic. Ontheotherhandthelabelscouldalsonotberelatedwitheachother. Forinstance,adog
barkingsoundandcarenginesoundcouldco-occurinaclipaccidentally. Wecanobservethisfrom
oneoftheAudioSetclips,suchasFigure2.
Figure2: TheweaklabelsinaAudioSetclip[2]couldberelatedduetodifferentfactors.
3.2 Framework
We consider multi-labeled training data {(x , y ), ..., (x , y )} where x ∈ X is a single 128-
1 1 n n i
dimensionaudiofeaturerepresentationandthecorrespondingy isacollectionoflabels{(y1,y2,...,
i 1 1
yT1),...,(y1,y2,...,yTk)}withkequaltothenumberofontologylevelsandT equaltothenumber
1 k k k i
oflabelsforxattheithontologylevel. Thusinourpost-processeddatasetwehavek=2andwill
refertotheseassubclassor"level1"labelsandsuperclassor"level2"labels.
3.3 MLPNetworkwithoutOntologyInformation
Toinvestigatewhetherincorporatingtheontologyinformationisbeneficialtousewithweaklylabeled
data,wefirstneedtotrainamodelwhichusesnoontologyinformation. WeimplementasimpleMLP
networkwiththreehiddenlayersandafinaloutputlayerthatcollectivelypredictsontologyclasses.
Moreconcretely,thefinallayerhassize(cid:80)k
T topredictallontologylabels. Eachhiddenlayer
i=1 i
isofsize512followedbyaBatchNormlayer,ReLUactivation,andaDropoutlayer. Thismodel
makesnouseoftheontologyinformationtoaidinthepredictionoftheontologylabels,makingita
goodbaselinetocomparetoourmodelswhichwillincorporatetheontologyinformation. Thefinal
outputspassthroughafinalsigmoidactivationforthemulti-labelscenarioandthemodelaimsto
minimizethebinarycrossentropyloss.
3.4 SiameseModelwithOntology-basedembeddingsandOntologicallayer
Theontologybasedmodelfrom[1]isarecentcontributionthatspecificallyfocusesonaudiodata
whilealsoprovidingamethodforpredictionofclassesatdifferentontologylevels. Muchoftheprior
workonontologypredictionhasbeenindifferentareassuchasmedicine(drugclassification)or
textclassification,whereas[1]presentsaframeworkthatismorerelevantandrecent. Thismodel
4
willthereforeserveasabaselineforthemodelswhichwillincorporateontologyinformation. The
frameworkwithmodificationformulti-labelscenariosisshowninFigure3.
Figure3: ArchitectureofSiameseNetwork+OntologicalLayerwithmodificationtofitthemulti-
labeltask
Wenowpresentthecomponentsofthemodelanditsmathematicalformulation,whichisanextension
oftheworkin[1]tolearnontology-basedembeddingsforclassificationofmulti-labeleddata. We
useaSiameseneuralnetwork(SNN)toseparatetheontology-basedembeddingsbyimposingthe
embeddingdistancecloseto0iftheinputpairsarefromthesamesubclass,closeto1iftheyare
fromdifferentsubclasses,butthesamesuperclass,andcloseto2iftheyarefromdifferentsuper
classes. Tofitinthemulti-labelscenario,weappliedtwokindsofsamplingmethods,thefirstoneis
thatsamplesfallintothesamesub(super)classcategoryifandonlyiftheyhaveexactlythesame
subclasses;samplesfallintodifferentsub(super)classcategoryifanyoftheirsub(super)classesis
different. Thesecondapproachisthatsamplesfallintothesamesub(super)classcategoryifand
onlyifthereisanysub(super)classintheirintersection;samplesfallintodifferentsub(super)class
categoryifthereisnoanysub(super)classintheirintersection.
The base model architecture of the Siamese net is the same MLP with three hidden layers from
section3.3,andeachbranchoftheSiamesenetsharestheMLPparameters. Givenapairofinput
vectorsx ,x ,theoutputoftheMLPisanontologyembeddingz = f(x ),z = f(x ). The
1 2 1 1 2 2
embeddingvectorsz ,z producesubclassprobabilitiesafterasigmoidactivation,whichisusedfor
1 2
thepredictionofmulti-labeleddata.
The ontological layer, M, then relates the class probabilities of level 1 to those in level 2 of the
ontologythroughthefollowingrelation:
p(y |x)=M·p(y |x)
2 1
Wemodifytheontologicallayerproposedin[1]inordertofitthemulti-labeldatascenario,whereM
isconstructedsuchthatitaveragestheprobabilitiesofthesubclasseswithinasinglesuperclass. Note
thatthislayerisfixedandnottrainable,itdependsstrictlyontheontologyofthetrainingdata.
Thefinallossfunctionincorporatesthebinarycrossentropylossofthelevel1classes,L ,andthe
1
level2classes,L ,withtheembeddingloss,D =(||z −z || −d)2whered∈{0,1,2}according
2 w 1 2 2
tothetypeofinputpair.
L=λ (L1+L2)+λ (L1+L2)+λ D
1 1 1 2 2 2 3 w
Thebasenetworktakesaninputfeatureofdimension128beforetheoutputlayertothe42classesin
thefirstontologylevelandthenthroughtheontologylayertothe7classesinthesecondlevel. The
baselinepaperdoesnotactuallyreportevaluationmetricsfortheAudiosetdata,astheyfocusedon
otheraudiodatasets: UrbanSounds-US8KanddatafromtheMakingSenseofSoundsChallenge.
Thesearebothsinglelabeleddatasets.
3.5 GraphConvolutionalNetwork
Tomodelboththeco-occurrenceinformationintroducedfromweaklabelsandthedomainknowledge
from ontology, we seek to utilize graph embedding approaches to extend the Siamese network
architecture.In[8]and[9],aGraphConvolutionNetwork(GCN)isshowntobeeffectiveforlearning
usefulnoderepresentationsthroughtheinformationinacorrelationgraph. Theessentialideaisto
updatethenoderepresentationsbyaggregatinginformationfromneighboringnodes.
5
Followingtheideasofpreviouswork,[8],[9],wedefinethesubclasslabelsandsuperclasslabels
as the nodes of the graph and aim to learn the label representation. The knowledge in the graph
isencodedasacorrelationmatrix,whichisacrucialpartoftheGCN.Wewilldescribehowitis
constructedinSection3.5.1.
Weuseone-hotencodingoflabelsastheinitialnoderepresentationanduse2GCNlayerstoextract
embeddingswithneighboringinformation. ForeachGCNlayerweuse2linearlayerstotransform
theinputembeddingofthenodeitselfandagraphconvolutiontoaggregatetheembeddingsfrom
itsneighbor. GivenalabelembeddingZ ∈ RC×d (whereC isthenumberofnodesanddisthe
dimensionalityofnodefeatures),thegraphconvolutionoperationsis:
Zl+1 =h(A(cid:48)ZlWlWl)
1 2
whereWl ∈Rd×d(cid:48) andW2l ∈Rd(cid:48)×d(cid:48)(cid:48) are2transformationmatricestobelearnedandA(cid:48) ∈RC×C
1
isthecorrelationmatrixandh(.)isanon-linearoperationwhichisaLeakyReLUinourexperiments.
Thedimensionsofeachthelinearlayersare280and512forthefirstGCNlayerand320and128for
thesecondGCNlayer.
3.5.1 CorrelationMatrix
The GCN learns node representations by collecting information from other nodes based on the
correlationmatrixprovided. Thus,howwebuildthecorrelationmatrixiscrucialbutalsochallenging
forGCN.Inthiswork,wereferredtopreviouswork[8,9]andexperimentedon3differentcorrelation
matrices.
LabelsCo-occurrencebasedCorrelationMatrix: [9]proposedawaytomodellabeldependency
intheformofconditionalprobability,i.e. P(L |L )denotestheprobabilityofoccurrenceofL
j i j
whenL ispresent. Toconstructthecorrelationmatrix,wecounttheco-occurrenceoflabelpairs
i
presentintrainingsettogetamatrixM ∈ RC×C,whereM denotestheco-occurrencetimeof
i,j
L andL . WethendivideM bytheoccurrencetimeofL intrainingsettogettheconditional
i j i
probabilityP. Topreventalong-taildistributionwheresomerareco-occurrencesmaybenoisy,we
binarizeP bysettingatunablethresholdt. ThecorrelationmatrixAissetto1ifP isabovethet
andsetto0whenP isbelowthet,whereA∈RC×C.
Ontology-basedMethodOne: [8]proposedthecorrelationmatrixAtodenotelabelpairswhohave
sameparents. A =1whenL andL havesameparents;A =0otherwise.
i,j i j i,j
Ontology-basedMethodTwo: [8]proposedthecorrelationmatrixAtodenotelabelpairswhohave
edgesinbetweenthem. Inthedatasetweareusing,edgesonlyoccurbetweenparentsandchildren,
sowesetA =1whenL isachildofL ortheotherwayaround;otherwise,A =0.
i,j i j i,j
Topreventover-smoothing,afterbinarizing,were-weightAtogetthedesiredcorrelationmatrixA(cid:48)
bysettingA(cid:48) =pwheni=j andA(cid:48) =(1−p)/(cid:80) A whenA =1.
i,j i,j j j i,j
3.6 SiameseNetworkwithGraphConvolutionalNetwork
TheGraphConvolutionNetworkmoduledescribedinSection3.5canconvertthelabelembedding
fromaone-hotrepresentationtoanembeddingaggregatingneighborinformation. Weuseamatrix
multiplicationtogetthesimilaritybetweenagivenaudioclipembeddingandeverylabel’sembedding.
Giventhesesimilarityvalues,wethenuseaSoftMaxactivationandBinaryCrossEntropylossto
calculatethelossbetweenouroutputsandthetargetlabels. TheoverallframeworkisshowninFigure
4.
HerewereplacethebaselineOntologicalLayerwiththelabelembeddingsthattheGCNgenerates.
TocombineitwithSiameseNetwork,westillkeeptheλ ,λ andλ forthelossterm. Nextwewill
1 2 3
discusstheexperimentsinmoredetailandtheresultsfromthevariousmodels.
4 Experiments
4.1 EvaluationMetrics
The performance of our models is evaluated using weighted average precision and AUC from
predictions. AllreportedmetricsareonthetestsetoftheAudiosetdataset. Asdiscussedpreviously,
6
Figure4: TheframeworkofSiameseNetwork+GraphConvolutionalNetwork
weextractthelabelsforeachsegmentfromthetoptwolevelsoftheAudiosetontology. Theaverage
precision(AP)metricisfirstcomputedforeachclassbyconsideringtheprecision(fractionoftrue
positivelabelsoutofallpredictedpositivelabels)-recall(fractionoftruepositivelabelsoutofactual
positive labels) curve at different thresholds. Thus it is an indication of how well the model can
identifypositiveclassesinthedata. TheothermetricweconsiderisAUC,whichconsidersnegative
labelsbycomputingtheareaundertheTPR(truepositiverate)-FPR(falsepositiverate)curveand
isanindicationofhowwellthemodelcandistinguishbetweenclasses. TheAP/AUCmetricforthe
classesonthesameontologyleveliscombinedthroughaweightedaveragebasedontheproportion
ofeachclassthatispresentinthetrainingdatatogetafinalweightedAPandweightedAUCscore
foreachlevel.
4.2 PerformanceofMLPModelwithnoOntologyInformation
TheMLPmodelwithnoontologyinformationachievesaweightedAP/AUCscoreof0.45099/0.8706,
respectivelyforsubclasses. TheweightedAP/AUCscoreforthesuperclassesare0.7056/0.8556,as
showninTable1. Themodelistrainedforabout70epochsatalearningrateof2e-3. Pleasereferto
theAppendixformorehyperparametertrainingdetailsforeachofthemodels.
4.3 PerformanceofSiameseNetworkwithOntologicalLayer
TheSiamesemodelwithOntologylayerachievesasubclassweightedAP/AUC=0.3653/0.8055
andsuperclassweightedAP/AUC=0.3876/0.6505. Aftersomehyperparametertuningoftheloss
function,wewereabletoachievetheTable1resultswithλ =1.5,λ =1,λ =0.25. Throughthe
1 2 3
experiments,wefoundthatthesemetricscanbetunedbasedontheλvaluesinthelossfunctionto
controlthecontributionsfromeachleveloftheontology. Amoredetailedtableofthehyperparameter
tuningispresentedintheappendix,andtheresultsinthesummarytablearechosenasagoodbalance
betweenthetwoontologylevels.
ThisexperimentshowsthereshouldbemoreresearchworktoapplyaSiamesenetworktoamulti-
labelscenario,suchashowtogeneratepairsofthesamesub/superclassaswellastheirsimilarity
metric. Wefurtherinvestigatedthisproblembyconsideringtwodefinitions: oneinwhichapair
ofdataisofthe"same"subclassiftheyhaveexactlythesamelabelsandanotherinwhich"same"
meansthattwopairsjusthavesomeintersectionintheirlabels. Ourexperimentssuggestthatthe
latter definition is better for the multi-label scenario. However, the performance of this baseline
Siamesemodelwhichusesontologyinformationsuggeststhatthisexternalknowledgeisstilldifficult
toembedintoamodelforweaklylabeleddata. Furthermore,webelievethatthisdefinitionof"same"
ordifferent"sub/superclassisstillambiguousandintroducesevenmorenoiseintothemodelasit
attemptstoclusterpairsthatarenotreallythe"same"subclass. Thiscanexplainwhyweseesuch
poorperformancemetricscomparedtotheMLPwithnoontologyinformation.
7
WeightedAP WeightedAUC
Model SubclassLevel SuperclassLevel SubclassLevel SuperclassLevel
MLP 0.4509 0.7056 0.8706 0.8556
Siamese+Ontology 0.3653 0.3876 0.8055 0.6505
Siamese+GCN 0.4285 0.6790 0.8460 0.8280
MLP+GCN 0.4590 0.7117 0.8751 0.8602
Table1: mAPandAUCResultsofdifferentmodels
Figure5: mAPacrossdifferentlowlevellabels
4.4 PerformanceofSiamese-GCNModel
TheextensionoftheSiamesemodelreplacestheOntologylayerwithaGraphConvolutionnetwork
toembedontologyinformation. WetrainedthemodelbyAdamoptimizerforabout30epochs. We
use2LinearLayerasthenodeembeddingofGCNandapply2LayerGCN.Theperformanceis
sensitivetothehyper-parametersλ , λ andλ ofthelossfunctionbecauseitwouldamplifyor
1 2 3
reducelearningratefordifferentlossterms. TheTable3showshowdifferenthyper-parameterscould
affecttheperformance. Oneinterestingobservationisthatthegreaterλisnotnecessaryequaltothe
greaterlearningrate.
Inthisexperiment,wealsocanseethattheSiamesenetworkwithGCNcouldtoleratethenoisesin-
troducedfromweaklylabelleddatabetterthanthebaselineSiamesewithanun-trainableOntological
Layer. ThissuggeststhattheGCNcanbettercapturetheontologyhierarchyinthemodelandthatit
canevenovercomethenoiseintroducedbyattemptingtofindpairsfortheSiamesenet. Overall,the
evaluationmetricsforthismodelareclosetothebaselineMLPwithnoontologyinformation.
4.5 PerformanceofMLP-GCNModel
To study the utility of the Siamese net we also implement an MLP-GCN using the same GCN
structureastheoneinSiameseGCNframework. Wealsotakeadeeperlookintowhichcorrelation
matricescanprovidethebestimprovementinperformance. Amongthe3correlationmatrices,we
foundlabelsco-occurrencebasedcorrelationmatrixworksbesttofittheweaklabelscenarioand
modeltheontologyinformation. Thus,wefurthermoredidmoreexperimentsonthetwotrainable
parameters: t,p(seesection3.5.1).
Fromourexperiments(seeAppendixA.4),wefoundp=0.2,t=0.08achievedthebestresults. We
couldseethatalthoughthismodelperformsbetterthanthepreviousmodels,theimprovementfrom
MLPwithoutOntologicalLayerisstilllimited. Thissuggeststhatevenwiththeontologyinformation
embeddedintotheGCN,itisstillhardtogetsignificantimprovementinclassificationresults. It
8
Figure6: AUCacrossdifferentlowlevellabels
Figure7: APacrossdifferentsuperclasslabels Figure8: AUCacrossdifferentsuperclasslabels
alsofurtherdemonstratesthatthearchitectureoftheSiamesenetisnotagreatfitforthismulti-label
scenario. ByusingasimpleMLPnetworktolearnembeddings,itcanalreadyachieverelativelygood
performancewiththeGCN.Overall,wefoundthattherewerecertainclassesforwhichitwasalways
difficulttoachievehighAPorAUCscoresasdemonstratedinFigures5,6,7,8. Fortheseclasses,
suchasglass,fire,orsilence,weanalyzedwhichdatapointscontainthoseclassesandfoundthatthey
areoftenmulti-labeledwithmorecommonlyfoundclassesinthetrainingset,suchashumanvoice,
ordomesticsounds. Thiscouldmakeitdifficultforthemodeltolearndifferentrepresentationsfor
thoseclasses.
5 Conclusions
Toconclude,weobservedthattheSiamesemodelwithontologylayerhastheworstperformance,
whilesimpleMLPisgettingrelativelybetterresults,andthecombinationofMLPwithGCNworks
even better. It is possible for the Siamese architecture to have a negative impact on prediction,
introducing even more confusion through the construction of input pairs for data that is weakly
multi-labeled. AlthoughtheGCNprovidessomeslightimprovement,itseemsthatthemodelstillhas
difficultyindifferentiatingambiguoussubclassesorusingthehiddenknowledgeinweaklylabeled
data.Thewaytoincorporateontologyinformationmightbedifferentwithrespecttocontexts(dataset,
ontological architecture, network structure, etc.), and it would take a lot more research effort to
9
determinethebestwaystouseit. ForadatasetlikeAudioSet,whereeachdatapointhasmultiple,or
evenwronglabels,havingasimpleontologylayerontheoutputendofaSiamesenetworkmightnot
beagoodchoice. Sothenextstepmightbetonarrowdownthescopeofdata-setandtrydifferent
approachestoembedontologicalinformationandapproachesthatcouldalsomakeuseofdeeper
levelsofontologyinformation.
References
[1] B. R. Abelino Jimenez, Benjamin Elizalde, “Sound event classification using ontology-based neural
networks,”NIPS2018Workshop,2018.
[2] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and
M.Ritter,“Audioset:Anontologyandhuman-labeleddatasetforaudioevents,”inProc.IEEEICASSP
2017,NewOrleans,LA,2017.
[3] K.Sikelis,G.E.Tsekouras,andK.Kotis,“Ontology-basedfeatureselection:Asurvey,”FutureInternet,
vol.13,no.6,2021.[Online].Available:https://www.mdpi.com/1999-5903/13/6/158
[4] G.A.Miller,“Wordnet:Alexicaldatabaseforenglish,”Commun.ACM,vol.38,no.11,p.39–41,Nov.
1995.[Online].Available:https://doi.org/10.1145/219717.219748
[5] YuxiaGeng,JiaoyanChen,ZhuoChen007,JeffZ.Pan,ZhiquanYe,ZonggangYuan,YantaoJia,and
HuajunChen,“Ontozsl:Ontology-enhancedzero-shotlearning.”inWWW’21:TheWebConference2021,
VirtualEvent/Ljubljana,Slovenia,April19-23,2021. ACM/IW3C2,2021.
[6] T.Mitchell,W.Cohen,E.Hruschka,P.Talukdar,J.Betteridge,A.Carlson,B.Dalvi,M.Gardner,B.Kisiel,
J.Krishnamurthy,N.Lao,K.Mazaitis,T.Mohamed,N.Nakashole,E.Platanios,A.Ritter,M.Samadi,
B.Settles,R.Wang,D.Wijaya,A.Gupta,X.Chen,A.Saparov,M.Greaves,andJ.Welling,“Never-ending
learning,”inProceedingsoftheTwenty-NinthAAAIConferenceonArtificialIntelligence(AAAI-15),2015.
[7] M. S. A. Kamran Munir, “The use of ontologies for effective knowledge modelling and information
retrieval,”AppliedComputingandInformatics14(2018),2018.
[8] H.Shrivastava,Y.Yin,R.R.Shah,andR.Zimmermann,“Mt-gcnformulti-labelaudio-taggingwith
noisylabels,”inICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignal
Processing(ICASSP),2020,pp.136–140.
[9] Z.-M.Chen,X.-S.Wei,P.Wang,andY.Guo,“Multi-labelimagerecognitionwithgraphconvolutional
networks,”2019.
[10] T.N.KipfandM.Welling,“Semi-supervisedclassificationwithgraphconvolutionalnetworks,”2017.
[11] G. Wichern, B. Mechtley, A. Fink, H. Thornburg, and A. Spanias, “An ontological framework for
retrievingenvironmentalsoundsusingsemanticsandacousticcontent,”EURASIPJ.AudioSpeechMusic
Process.,vol.2010,no.1,Dec.2010.[Online].Available:https://doi.org/10.1155/2010/192363
[12] A.Shah,A.Kumar,A.G.Hauptmann,andB.Raj,“Acloserlookatweaklabellearningforaudioevents,”
arXivpreprintarXiv:1804.09288,2018.
10
A Hyperparameters
A.1 MLPModelwithnoOntologyInformation
ThesimpleMLPmodelistrainedusingtheAdamoptimizerwithalearningrate=2e−3andweightdecay=1e−4,
trainedforaround73epochs.
A.2 SiamesewithOntologyLayer
λ λ λ LowLevelmAP HighLevelmAP LowLevelAUC HighLevelAUC
1 2 3
1.5 1 0.25 0.3379 0.4470 0.7904 0.7061
1.75 1 0.25 0.3480 0.4357 0.7941 0.6959
2 1 0.25 0.3511 0.4175 0.7969 0.6799
2 0.75 0.5 0.3653 0.3876 0.8055 0.6505
2 1 0.5 0.3586 0.4196 0.8004 0.6845
Table2: mAPandAUCResultsofMLP-GCNwithdifferentpparameterswhent=0.168
A.3 Siamese-GCN
λ λ λ lr LowLevelmAP HighLevelmAP
1 2 3
20 2 0.5 1e−3 0.34 0.625
100 100 0.5 1e−3 0.4285 0.679
1 1 0.005 1e−1 0.306 0.615
λ λ λ lr LowLevelAUC HighLevelAUC
1 2 3
20 2 0.5 1e−3 0.796 0.788
100 100 0.5 1e−3 0.846 0.828
1 1 0.005 1e−1 0.797 0.772
Table3: mAPandAUCresultsofSiameseGCNwithdifferentλandlearningrate(lr)
A.4 MLP-GCN
p LowLevelmAP HighLevelmAP LowLevelAUC HighLevelAUC
0.1 0.4477 0.7091 0.8571 0.8690
0.2 0.4469 0.7098 0.8693 0.8575
0.3 0.4467 0.7083 0.8568 0.8690
Table4: mAPandAUCResultsofMLP-GCNwithdifferentpparameterswhent=0.168
t LowLevelmAP HighLevelmAP LowLevelAUC HighLevelAUC
0.010 0.4455 0.7072 0.8687 0.8566
0.025 0.4479 0.7077 0.8694 0.8567
0.080 0.4501 0.7141 0.8710 0.8613
0.100 0.4471 0.7128 0.8696 0.8597
0.150 0.4462 0.7093 0.8691 0.8577
Table5: mAPandAUCResultsofMLP-GCNwithdifferenttparameterswhenp=0.2
11

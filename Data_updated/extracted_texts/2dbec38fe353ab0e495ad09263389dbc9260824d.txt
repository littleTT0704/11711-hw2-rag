A Survey of Deep Learning for Mathematical Reasoning
PanLu1,LiangQiu1,WenhaoYu2,SeanWelleck3∗,Kai-WeiChang1∗
1UCLA,2UniversityofNotreDame,3UniversityofWashington
https://github.com/lupantech/dl4math
Abstract naturallanguageprocessing(NLP),datingbackto
the1960s(Feigenbaumetal.,1963;Bobrow,1964).
Mathematical reasoning is a fundamental as-
Inrecentyears, therehasbeenasurgeofinterest
pectofhumanintelligenceandisapplicablein
in this area: for instance, the number of papers
variousfields,includingscience,engineering,
finance,andeverydaylife. Thedevelopmentof hasgrownfromapproximately10in2018to66in
artificialintelligence(AI)systemscapableof 2022(seeFigure3intheAppendix).
solvingmathproblemsandprovingtheorems AsdeeplearningcontinuestorevolutionizeNLP
in language has garnered significant interest
tasks such as question answering and machine
in the fields of machine learning and natural
translation (Sutskever et al., 2014; Devlin et al.,
languageprocessing. Forexample,mathemat-
2019), it has also made significant strides in the
icsservesasatestbedforaspectsofreasoning
thatarechallengingforpowerfuldeeplearning fieldofmathematicalreasoning(Wangetal.,2017;
models, driving new algorithmic and model- YangandDeng,2019;Gevaetal.,2020;Weietal.,
ing advances. On the other hand, recent ad- 2022). However,despitetheimpressivecapabilities
vancesinlarge-scaleneurallanguagemodels ofthesemodels,thereisstillalackofacleartax-
have opened up new benchmarks and oppor-
onomyofthedifferenttypesofmathematicalrea-
tunitiestousedeeplearningformathematical
soningtasksandthespecificcapabilitiesrequired
reasoning. Inthissurveypaper,wereviewthe
ofdeeplearningmodelstosolvethem.
key tasks, datasets, and methods at the inter-
section of mathematical reasoning and deep Previous literature has been limited to the dis-
learningoverthepastdecade. Wealsoevaluate cussion of specific aspects, such as solving math
existingbenchmarksandmethods,anddiscuss wordproblems(Bhattacharya,2017;Zhangetal.,
futureresearchdirectionsinthisdomain. 2019;UghadeandKumbhar,2019),representing
numbersrepresentation(Thawanietal.,2021),or
1 Introduction
solvinginformalproblems(MeadowsandFreitas,
“Thestudyofmathematics,liketheNile,beginsin 2022). Additionally,withtherecentadvancements
minutenessbutendsinmagnificence.” inlargelanguagemodelslikeGPT-3(Brownetal.,
—CharlesCalebColton,Englishwriter 2020), there is a growing need to understand the
capabilitiesandlimitationsofthesemodelsinthe
Mathematical reasoning is a key aspect of hu- contextofmathematicalreasoning. Thisiswhere
manintelligencethatenablesustocomprehendand a comprehensive survey of this rapidly advanc-
make decisions based on numerical data and lan- ingdomainbecomescrucial,asitcanprovidean
guage. Itisapplicableinvariousfields,including overviewofthecurrentstateandlimitationsofthe
science, engineering, finance, and everyday life, field,andindicatefurtherresearchareas.
and encompasses a range of abilities, from basic
Inthispaper,wesurveyover180papersfromthe
skills such as pattern recognition and numerical
NLPandAIcommunitiesinthefieldofdeeplearn-
operations to more advanced skills like problem-
ingformathematicalreasoning. Westudyvarious
solving, logical reasoning, and abstract thinking.
types of mathematical reasoning problems, such
Thedevelopmentofartificialintelligence(AI)sys-
asmathwordproblems,theoremproving,geome-
temscapableofsolvingmathproblemsandproving
tryproblemsolving,mathquestionanswering,and
theoremsinlanguagehasbeenalong-standingfo-
otherquantitativeproblems(§2,§A).Additionally,
cusofresearchinthefieldsofmachinelearningand
weexploredifferentdeeplearningarchitecturesfor
∗denotesco-seniorauthors. mathematicalreasoning,includingneuralnetworks
3202
nuJ
22
]IA.sc[
2v53501.2122:viXra
Textual E.g.,MathQA(Aminietal.,2019),SVAMP(Pateletal.,2021)
MathWordProblem
Solving(§A.1)
Multimodal E.g.,IconQA(Luetal.,2021b),TabMWP(Luetal.,2022b)
Formal E.g.,CoqGym(YangandDeng,2019)
TheoremProving(§A.2) Informal E.g.,NaturalProofs(Wellecketal.,2021)
Formal+Informal E.g.,miniF2F+informal(Jiangetal.,2022a)
WithoutAnnotations E.g.,GEOS(Seoetal.,2015),GEOS++(Sachanetal.,2017)
Tasksand GeometryProblem
Datasets(§2) Solving(§A.3)
WithAnnotations E.g.,Geometry3K(Luetal.,2021a),UniGeo(Chenetal.,2022a)
SingleBenchmark E.g.,DROP(Duaetal.,2019),Mathematics(Saxtonetal.,2020)
MathQuestion
Answering(§A.4)
UnifiedBenchmark E.g.,Lila(Mishraetal.,2022a),TheoremQA(Chenetal.,2023)
Diagram E.g.,FigureQA(Kahouetal.,2018),DVQA(Kafleetal.,2018)
Finance E.g.,ConvFinQA(Chenetal.,2022c)
OtherQuantitative
Problems(§A.5)
Science E.g.,ScienceQA(Luetal.,2022a)
Programming E.g.,P3(Schusteretal.,2021)
Seq2Seq-based(§3.1) E.g.,DNS(Wangetal.,2017),AnsRat(Lingetal.,2017)
Graph-based(§3.2) E.g.,GTS(XieandSun,2019),Graph2Tree(Lietal.,2020b)
NeuralNetworks(§3)
Attention-based(§3.3) E.g.,Math-EN(Wangetal.,2018a),GROUP-ATT(Lietal.,2019)
Other(§3.4) E.g.,CNNTP(Loosetal.,2017),MathDQN(Wangetal.,2018b)
DeepLearning
Self-SupervisedLearning(§4.1) E.g.,GenBERT(Gevaetal.,2020),Minerva(Lewkowyczetal.,2022)
Methods Pre-trainedLanguage
Models(§4)
Task-specificFine-tuning(§4.2) E.g.,Scratchpad(Nyeetal.,2021),Bhaskara(Mishraetal.,2022a)
ExampleSelection(§5.1) E.g.,Few-shot-CoT(Weietal.,2022),PromptPG(Luetal.,2022b)
In-contextLearning(§5)
High-qualityChains(§5.2) E.g.,Self-Consistency(Wangetal.,2023),Least-to-most(Zhouetal.,2023)
Figure1: Taxonomyofdeeplearningformathematicalreasoning. Theassociatedtasksareelaboratedin§2,witha
comprehensivedatasetlistfoundin§A.Deeplearningmethodsarefurtherdiscussedin§3,§4,and§5.
(§3),pre-trainedlanguagemodels(§4),andrecent Question:Bodhas2applesandDavidhas5apples.
in-contextlearningforlargelanguagemodels(§5). Howmanyapplesdotheyhaveintotal?
We also analyze existing benchmarks and find Rationale:x=2+5
that there is less focus on multi-modal and low- Solution:7
resourcesettings(§6.1). Ourevidence-basedstud- Table1: Atypicalmathwordproblem.
iessuggestthatcurrentnumeracyrepresentations
areinsufficientanddeeplearningmethodsarein- prehension,semanticparsing,andtheapplication
consistentformathematicalreasoning(§6.2). Fol- ofmultiplemathematicalreasoningskills.
lowingthis,wesuggestfutureresearchdirections TheoremProving. Automatingtheoremproving
relatedtogeneralizationandrobustness,trustwor- is a long-standing challenge in AI (Newell et al.,
thyreasoning,learningfromfeedback,andmulti- 1957; Feigenbaum et al., 1963). The problem is
modalmathematicalreasoning(§7). to demonstrate the truth of a mathematical claim
(a theorem) through a sequence of logical argu-
2 MathematicalReasoningTasks
ments (a proof). Theorem proving tests various
Inthissection,webrieflyintroducedifferenttasks skills,suchaschoosingeffectivemulti-stepstrate-
formathematicalreasoning. Adetailedsummary gies,usingbackgroundknowledge,andperforming
anddiscussionofcommonlyuseddatasetscanbe symbolicmanipulations.
foundinTable7andAppendixA. GeometryProblemSolving. Automatedgeome-
MathWordProblemSolving. Developingalgo- tryproblemsolving(GPS)isalsoalong-standing
rithmstoautomaticallysolvemathwordproblems mathematicalreasoningtask(Gelernteretal.,1960;
(MWPs)hasbeenofinteresttoNLPresearchersfor Wen-Tsun,1986). AsshowninFigure2,ageom-
decades(Feigenbaumetal.,1963;Bobrow,1964). etryproblemconsistsofatextualdescriptionand
AnexampleofaMWPisshowninTable1. Aques- a diagram. The multimodal inputs describe the
tioninvolvesfourbasicarithmeticoperationswith entities,attributes,andrelationshipsofgeometric
singleormultipleoperationsteps. Thechallenge elements,andthegoalistofindthenumericsolu-
posedbyMWPsliesintheneedforlanguagecom- tiontoanunknownvariable.
gninosaeRlacitamehtaMrofgninraeLpeeD
C Question: In triangle ABC, AD = 3 and 3.2 Graph-basedNetworksforMath
BD = 14. Find CD.
Choices: (A) 6.0 (B) 6.5 (C) 7.0 (D) 8.5 Seq2Seqapproachesshowtheiradvantagesofgen-
Answer: (B) 6.5 eratingmathematicalexpressionswithoutrelying
A D B
on hand-crafted features. It is noteworthy that
Figure2: Anexampleofgeometryproblems. mathematical expressions can be represented as
tree-basedstructures,suchasabstractsyntaxtrees
MathQuestionAnswering. Thereisawiderange (ASTs)andgraph-basedstructures,whichcapture
ofquestionanswering(QA)benchmarksthatcenter thestructuralinformationintheexpressions. How-
aroundmathematicalreasoning,whichwereferto ever, Seq2Seq methods do not explicitly this im-
asmathquestionanswering(MathQA).Forexam- portant information. To address this limitation,
ple,DROP(Duaetal.,2019)isaMathQAdataset graph-basedneuralnetworkshavebeendeveloped
thatrequiresdiscretereasoningtoanswerquestions to explicitly model the structure within expres-
suchas“Whichkickerkickedthemostfieldgoals?” sions. Sequence-to-tree(Seq2Tree)modelsexplic-
overthecontentofparagraphs. itly model the tree structure when encoding the
output sequences (Xie and Sun, 2019; Wu et al.,
2020; Zaporojets et al., 2021; Qin et al., 2021).
3 NeuralNetworksforMathematical
Forexample, Liuetal.(2019a)deviseaSeq2Tree
Reasoning
modeltobetteruseinformationfromanequation’s
AST.Seq2DAG(Caoetal.,2021),instead,applies
Neural networks have become a popular tool in
asequence-to-graph(Seq2Graph)frameworkwhen
thefieldofmathematicalreasoning,mirroringtheir
generatingtheequationssincethegraphdecoderis
success in NLP. In recent years, a number of dif-
abletoextractcomplexrelationshipsamongmul-
ferentneuralnetworkarchitectureshavebeenpro-
tiplevariables. Thegraph-basedinformationcan
posedformathematicalreasoningtasks,including
alsobeembeddedwhenencodingtheinputmathe-
Seq2Seq-based networks, graph-based networks,
maticalsequences(Zhangetal.,2020b;Shenand
andattention-basednetworks. Thesemethodsare
Jin,2020;Lietal.,2020b;Wuetal.,2021a).
outlinedinmoredetailinTable8intheAppendix.
3.1 Seq2Seq-basedNetworksforMath 3.3 Attention-basedNetworksforMath
Sequence-to-sequence(Seq2Seq)(Sutskeveretal., Theattentionmechanismhasbeensuccessfullyap-
2014)neuralnetworkshavebeensuccessfullyap- plied to NLP (Bahdanau et al., 2015) and vision
pliedtomathematicalreasoningtasks,suchasmath problems(Xuetal.,2015;Wooetal.,2018),taking
wordproblemsolving(Wangetal.,2017),theorem intoaccountthehiddenvectorsoftheinputsdur-
proving (Yang and Deng,2019), geometry prob- ingthedecodingprocessing. Recently,researchers
lemsolving(Robaideketal.,2018),andmathques- havebeenexploringitsusefulnessinmathematical
tionanswering(Tafjordetal.,2019). ASeq2Seq reasoning tasks, as it can be used to identify the
model uses an encoder-decoder architecture and mostimportantrelationshipsbetweenmathemati-
usuallyformalizesmathematicalreasoningasase- calconcepts. Forinstance,MATH-EN(Wangetal.,
quencegenerationtask. Thebasicideabehindthis 2018a)isamathwordproblemsolverwhichben-
approachistomapaninputsequence(e.g. amath- efits from long-distance dependency information
ematicalproblem)toanoutputsequence(e.g. an learned by self-attention. Attention-based meth-
equation,program,andproof). Commonencoders odshavealsobeenappliedtoothermathematical
and decoders include Long Short Term Memory reasoning tasks such as geometry problems solv-
network (LSTM) (Hochreiter and Schmidhuber, ing(Robaideketal.,2018;Chenetal.,2021a)and
1997), Gated Recurrent Unit (GRU) (Cho et al., theoremproving(YangandDeng,2019). Various
2014), and their bidirectional variants: BiLSTM attentionmechanismshavebeenstudiedtoextract
andBiGRU.Alargeamountofworkhasshownthe betterrepresentations,suchasGroup-ATT(Lietal.,
performance advantage of Seq2Seq models over 2019)whichusesdifferentmulti-headattentionto
previousstatisticallearningapproaches(Lingetal., extractvarioustypesofMWPfeatures,andgraph
2017;Wangetal.,2018a;Huangetal.,2018;Wang attention which is applied to extract knowledge-
etal.,2019;Lietal.,2019). awareinformationin(Wuetal.,2020).
3.4 OtherNeuralNetworksforMath workhasshownthepromisingperformanceofpre-
trainedlanguagemodelsinansweringmathword
Deep learning approaches to mathematical rea-
problems(Kimetal.,2020),assistingwiththeorem
soning tasks can also make use of other neural
proving(Wuetal.,2022b),aswellassolvingother
networks, such as convolutional neural networks
mathematicaltasks(Charton,2022).
(CNN)andmultimodalnetworks. Someworken-
codes the input text using a convolutional neural However, though large language models excel
networkarchitecture,givingthemodeltheability in modeling natural language, there are several
to capture long-term relationships between sym- challengestousingthemformathematicalreason-
bolsintheinput(Gehringetal.,2017;Wangetal., ing. First, pre-trained language models are not
2018a,a;Robaideketal.,2018;Alemietal.,2016; specifically trained on mathematical data. This
Loosetal.,2017). Forexample, thefirstapplica- likelycontributestothembeinglessproficientin
tionofdeepneuralnetworksfortheoremproving math-related tasks compared to natural language
isproposedin(Alemietal.,2016),whichrelieson tasks. There is also less mathematical or scien-
convolutionalnetworksforpremiseselection. tificdataavailableforlarge-scalepre-trainingcom-
paredtotextdata. Second,thesizeofpre-trained
Multimodalmathematicalreasoningtasks,such
models continues to grow, making it expensive
as geometry problem solving and diagram-based
totraintheentiremodelfromscratchforspecific
mathematicalreasoning,areformalizedasvisual
downstreamtasks. Additionally,downstreamtasks
question answer (VQA) problems (Kafle et al.,
may deal with different input formats or modali-
2018;Chenetal.,2021a;Luetal.,2021b). Inthis
ties, such as structured tables (Zhao et al., 2022)
domain, visual inputs are encoded using ResNet
or diagrams (Lu et al., 2021b). To address these
(Heetal.,2016)orFaster-RCNN(Renetal.,2015),
challenges,researchershavetoadjustpre-trained
whiletextualrepresentationsareobtainedviaGRU
modelsbyfinetuningthemondownstreamtasksor
orLTSM.Subsequently,thejointrepresentationis
adaptingtheneuralarchitectures.
learnedusingmultimodalfusionmodels,suchas
BAN(Kimetal.,2018),FiLM(Perezetal.,2018),
andDAFA(Gaoetal.,2019). 4.1 Self-SupervisedLearningforMath
Otherdeepneuralnetworkstructurescanalsobe
Self-supervisedlearningisamachinelearningap-
usedinmathematicalreasoning. AGraphNeural
proachinwhichanalgorithmlearnstoperforma
Network (GNN) is employed for geometry prob-
taskwithoutbeingexplicitlyprovidedwithlabeled
lem parsing in Zhang et al. (2022), taking advan-
training data. Table 2 provides a list of language
tageofitssuccessinspatialreasoning. WaveNet
models pre-trained with self-supervised tasks for
hasbeenappliedtotheoremproving(Loosetal.,
mathematicalreasoning.
2017;Bansaletal.,2019),duetoitsabilitytoad-
Modelscale. Thereisacleartrendthatpre-trained
dress longitudinal time-series data. Furthermore,
languagemodelshavebecomeincreasinglylarger
TransformersarefoundtooutperformGRUingen-
in the past few years (Devlin et al., 2019; Lewis
eratingmathematicalequationsinDDT(Mengand
etal.,2020;Raffeletal.,2020;Radfordetal.,2020;
Rumshisky,2019). Finally,MathDQN(Wangetal.,
Brown et al., 2020). A recent study (Liang et al.,
2018b) is the first work to explore reinforcement
2022a)showsthatmodelscalewithinamodelfam-
learning for math word problem solving, taking
ily reliably predicts model accuracy. The study
advantageofitsstrongsearchcapabilities.
also mentions an interesting thresholding effect:
“allmodelsthatwinhead-to-headmodelcompar-
4 Pre-trainedLanguageModelsfor
isonsforaccuracyataratewellabovechanceare
MathematicalReasoning
atleast50Bparameters”. Asimilarsize-growing
Pre-trainedlanguagemodels(Devlinetal.,2019; trend can be observed in the field of mathemat-
Radford et al., 2020; Brown et al., 2020) have ical reasoning with pre-trained language models.
demonstrated remarkable performance gains on For example, MWP-BERT (Liang et al., 2022b)
a wide range of NLP tasks. By pre-training on uses a backbone of BERT (110M) (Devlin et al.,
a large corpus of text, the models learn valuable 2019) and RoBERTa (123M) (Liu et al., 2019b)
worldknowledge(Guuetal.,2020),whichcould for Math Word Problems. Most recently, Min-
beappliedtodownstreamtasks. Similarideascan erva(Lewkowyczetal.,2022),whichisbasedon
beappliedtomath-relatedproblems,andprevious the PaLM (Chowdhery et al., 2022) pre-trained
Paper Backbone Size Corpus Pre-trainingtask
GPT-f(PoluandSutskever,2020) Transformer(2017) 774M Math Causallanguagemodeling
LISA(Jiangetal.,2021) Transformer(2017) 163M Math Causallanguagemodeling
MATH-PLM(Hendrycksetal.,2021b) GPT-2(2020) 1.5B Math Causallanguagemodeling
MWP-BERT(Liangetal.,2022b) RoBERTa(2019b) 123M Math 8numeracyaugmentedtasks
TaPEx(Liuetal.,2022b) BART(2020) 406M SQL Queryresultgeneration
HTPS(Lampleetal.,2022) Transformer(2017) 600M Math MaskedSeq2Seqmodeling
Thor(Jiangetal.,2022b) Transformer(2017) 700M Github,arXiv Causallanguagemodeling
PACT(Hanetal.,2022) Transformer(2017) 837M Math Masked/Causallanguagemodeling
Minerva(Lewkowyczetal.,2022) PaLM(2022) 540B Science&Math Causallanguagemodeling
GenBERT(Gevaetal.,2020) BERT(2019) 110M Number,Text Masked/Causallanguagemodeling
NF-NSM(Fengetal.,2021) RoBERTa(2019b) 110M Number Numberprediction
LIME(Wuetal.,2021d) Transformer(2017) 11B Math Causallanguagemodeling
Set(Wuetal.,2022c) T5(2020) 60M Math Uniquetokengeneration
Table2: Comparisonofpre-traininglanguagemodelsformathematicalreasoning.
languagemodel,hasasizeupto540Bparameters. Paper Backbone Task
Pre-training corpus. There are generally two EPT(2020) ALBERT(2019) MWP
Generate&Rank(2021) BART(2020) MWP
typesofpre-trainingcorpusformathematicallan-
RPKHS(2021b) RoBERTa(2019b) MWP
guage models. (i) Curated datasets from openly PatchTRM(2021b) ResNet+BERT(2019) MWP
accessiblesources. Forexample,Hendrycksetal. GSM8K-PLM(2021) GPT-3(2020) MWP
BERT-TD+CL(2022b) BERT(2019) MWP
(2021b) present the first large-scale mathematics DeductReasoner(2022) RoBERTa(2019b) MWP
pre-training dataset with step-by-step solutions Self-Sampling(2023) GPT-Neo(2020) MWP
Bhaskara(2022a) GPT-Neo(2020) MWP
in natural language and LATEX, called the Auxil-
miniF2F-PLM(2022) GPT-f (2020) TP
iaryMathematicsProblemsandSolutions(AMPS).
NaturalProver(2022a) GPT-3(2020) TP
AMPSconsistsofKhanAcademyandMathemat-
Inter-GPS(2021a) BART(2020) GPS
ica data. Minerva (Lewkowycz et al., 2022) col- UniGeo(2022a) VL-T5(2021) GPS
lectsahigh-qualitydatasetcontainingscientificand DPE-NGS(2022) RoBERTa(2019b) GPS
mathematical data, which contains 38.5B tokens Aristo(2020) RoBERTa(2019b) MathQA
FinQANet(2021c) RoBERTa(2019b) MathQA
from webpages filtered for mathematical content
TAGOP(2021) RoBERTa(2019b) MathQA
and from papers submitted to the arXiv preprint MT2Net(2022) RoBERTa(2019b) MathQA
server. Thor(Jiangetal.,2022b)pre-trainsalan- Scratchpad(2021) Transformer(2017) Mixed
guage model on the GitHub + arXiv subsets of LAMT(2022) Transformer(2017) Mixed
ThePile(Gaoetal.,2020). (ii)Syntheticdatasets Table 3: Finetuned pre-trained language models for
basedontemplatesorinteractionwithengines. Re- downstreammathematicalreasoningtasks.
centwork(Wuetal.,2021d;Krishnaetal.,2021;
Ri and Tsuruoka, 2022; Anderson and Farrell, guageModeling(CLM),wherethemodelistrained
2022;Wuetal.,2022c)showsthatpre-trainingon to predict the next token in a sequence of tokens.
datathatisfullysyntheticallygenerated—synthetic Followingthesameparadigm,researcherspre-train
pre-trainingcanactuallyprovidesubstantialgains. language models with MLM and CLM tasks on
Representative work includes TaPEX (Liu et al., mathematicalorscientificcorporafordownstream
2022b),whichobtainsapre-trainingcorpusbyau- tasks(PoluandSutskever,2020;Hendrycksetal.,
tomatically synthesizing executable SQL queries 2021b;Hanetal.,2022;Jiangetal.,2022b).
and their execution outputs. LISA (Jiang et al.,
There is also recent work that designs cus-
2021)extractslemmasandtheoremsbyinteracting
tomized tasks to inject mathematical reasoning
withtheIsabellestandardlibraryandtheArchiveof
capabilities into language models. For instance,
FormalProofs. GenBERT(Gevaetal.,2020)gen-
Liangetal.(2022b)pre-trainlanguagemodelswith
eratesnumericalandtextualpre-trainingdatasets
asuiteof8numeracy-augmentedtaskswithconsid-
basedonmanuallycraftedandextractedtemplates.
erationofreasoninglogicandnumericalproperties.
Pre-trainingtasks. Generalpre-traininglanguage LIME (Wu et al., 2021d) proposes synthetic pre-
modelshavetwotypicalself-supervisedlearning training tasks to learn three reasoning primitives:
tasks: (i) Masked Language Modeling (MLM), deduction,induction,andabductionbeforelearn-
whereitrandomlymasksaportionofwordsineach ingmorecomplexreasoningskills,whichalsobe
sequencetopredicttheoutcome;(ii)CausalLan- regardedasaformofcurriculumlearning.
4.2 Task-specificFine-tuningforMath Question: Rogerhas5tennisballs. He
buys 2 more cans of tennis balls. Each
Task-specificfine-tuningisatechniquetoimprove
canhas3tennisballs. Then,howmany
theperformanceofapre-trainedlanguagemodel
tennisballsdoesRogerhavenow?
on a specific task. This is also a common prac-
Answer: Roger started with 5 balls. 2
ticewhenthereisnotenoughdatafortrainingthe
cans of 3 tennis balls each are 6 tennis
large models from scratch. As shown in Table 3,
balls. 5+6=11. Theansweris11.
existingworkfine-tunespre-trainedlanguagemod-
elsonavarietyofdownstreamtasks,suchasmath
Apart from Kojima et al. (2022) showing that
wordproblems(Kimetal.,2020;Shenetal.,2021),
LLMsaredecentzero-shotreasonerswhengiven
MathQA (Zhao et al., 2022), geometry problem
the“Let’sthinkstepbystep!” prompt,mostofthe
solving(Luetal.,2021a),linearalgebra(Charton,
recentworkhasfocusedonhowtoimprovechain-
2022),andtheoremproving(Wellecketal.,2022a).
of-thought reasoning under the few-shot setting.
Apartfromfine-tuningthemodelparameters,some
Thisworkismainlydividedintotwoparts,(i)se-
workalsousespre-trainedlanguagemodelsasen-
lectingbetterin-contextexamplesand(ii)creating
codersandensemblesthemwithothermodulesfor
betterreasoningchains.
downstreamtasks(Luetal.,2021b).
5.1 In-contextExampleSelection
5 In-contextLearningforMathematical
Early chain-of-thought work randomly or heuris-
Reasoning
tically selects in-context examples. However, re-
Large language models (LLMs), such as GPT- centstudieshaveshownthatthistypeoffew-shot
3(Brownetal.,2020),haverecentlyrevolutionized learning can be highly unstable across different
thefieldofnaturallanguageprocessing(NLP),es- selections of in-context examples (Rubin et al.,
peciallyonaccountoftheirpowerfulfew-shotin- 2022; Liu et al., 2022a). Therefore, which in-
contextlearningcapabilities(Brownetal.,2020). contextreasoningexamplesmakethemosteffec-
In-context Learning (ICL) enables LLMs to per- tive prompts is still an unknown problem in the
formtargettasksbyprovidingsometaskexamples literature. To address the limitation, recent work
asconditionsatinferencetime,withoutupdating has investigated various methods to optimize the
model parameters (Radford et al., 2020; Brown in-contextexamplesselectionprocess(Rubinetal.,
et al., 2020). ICL allows users to quickly build 2022;Zhangetal.,2023;Luetal.,2022b;Yuetal.,
modelsfornewusecaseswithoutworryingabout 2023; Fu et al., 2023). For example, Rubin et al.
fine-tuningandstoringalargeamountofnewpa- (2022) attempt to address this issue by retrieving
rametersforeachtask,soitiswidelyusedinfew- semantically similar examples. In addition, Fu
shotsettingsnowadays(Minetal.,2022). etal.(2023)proposecomplexity-basedprompting,
An in-context example typically contains an whichchoosesexampleswithcomplexreasoning
input-output pair with some prompt words, e.g., chains, i.e., chains with more reasoning steps, as
Pleaseselectthelargestnumberfromthelist. In- theprompt. PromptPG(Luetal.,2022b)learnsto
put: [2,4,1,5,8]. Output: 8,andfew-shotworks select optimal in-context examples via reinforce-
by giving multiple examples, and then a final in- mentlearning(RL)fromacandidatepool.
putexample,wherethemodelisexpectedtopre-
5.2 High-qualityReasoningChains
dicttheoutput. However,suchstandardfew-shot
promptings,inwhichtheLLMisgivenin-context Earlychainofthoughtwork(e.g.,Weietal.(2022))
examplesofinput–outputpairsinfrontoftest-time mainlyreliesonasinglehuman-annotatedreason-
examples,havenotyetprovedsufficienttoachieve ingchainasaprompt. However,manuallycreating
high performance on challenging tasks such as reasoningchainshastwodisadvantages. First,as
mathematicalreasoning(Raeetal.,2021). tasksbecomemorecomplex,currentmodelsmay
Chain-of-thoughtprompting(CoT)(Weietal., notbesufficienttolearntoperformallnecessary
2022)leveragesintermediatenaturallanguagera- reasoningstepsandcannoteasilygeneralizetodif-
tionalesaspromptstoenableLLMstofirstgenerate ferent tasks. Second, a single decoding process
reasoning chains and then predict an answer for isvulnerabletoincorrectinferencesteps,leading
aninputquestion. Forexample,aCoTpromptfor to an incorrect prediction as the final answer. To
solvingthemathwordproblemcouldbe address this limitation, recent studies mainly fo-
Engine ICL Rationale Rationale
Models Postmethod
(bestperformed) source type source
Few-shot-CoT(Weietal.,2022) PaLM(540B) Random Language Hand-crafted -
Self-Consistency-CoT(Wangetal.,2023) Codex(175B) Random Language Hand-crafted Self-consistency
Least-to-mostCoT(Zhouetal.,2023) Codex(175B) Random Language Hand-crafted -
PromptPG-CoT(Luetal.,2022b) GPT-3(175B) RL Language Hand-crafted -
Retrieval-CoT(Zhangetal.,2023) GPT-3(175B) Retrival Language Auto-generated -
Auto-CoT(Zhangetal.,2023) Codex(175B) Clustering Language Auto-generated -
Complexity-CoT(Fuetal.,2023) GPT-3(175B) Complexity Language Hand-crafted Self-consistency
Few-shot-PoT(Chenetal.,2022b) GPT-3(175B) Random Code Hand-crafted -
Table4: In-contextlearningwithlargelanguagemodelsformathematicalreasoning. ForGPT-3,allpapersusethe
text-davinci-002version;forCodex,allpapersusethecode-davinci-002. RLisshortforreinforcementlearning.
cusontwoaspects,(i)hand-craftingmorecomplex ahigherdegreeofdiversity.
demonstrations,whichwerefertoasprocess-based
6 DiscussionandFindings
approaches(Zhouetal.,2023;Chenetal.,2022b),
(ii) leveraging ensemble-like methods, which we
6.1 AnalysisofBenchmarks
refertoasoutcome-basedapproaches(Wangetal.,
The multi-modal setting is underexplored but
2023;Lietal.,2022a).
is gaining increasing attention. Most existing
Process-based approaches aim to improve the benchmarksformathematicalreasoninghavetar-
chain-of-thoughtreasoningquality,especiallyfor geted the textual-only modality. However, visual
complexreasoningtasks. Inleast-to-mostprompt- elementscanprovidearichsourceofquantitative
ing (Zhou et al., 2023), the problem-solving pro- information, making multi-modal datasets bene-
cessisimplementedthroughtwo-stageprompting: ficial for reasoning over quantitative relations in
(i)reducingacomplexproblemintoalistofsub- naturalimages(Luetal.,2022a),abstractdiagrams
problems;(ii)solvingthesesub-problemssequen- (Luetal.,2021b),figures(Kahouetal.,2018),and
tially, so that solving a given sub-problem is fa- charts(Kafleetal.,2018). Tables,whicharecom-
cilitatedbytheanswerstopreviouslysolvedsub- monlyfoundindailydocumentsandcontainhierar-
problems. Similarly, Khot et al. (2022) leverage chicallystructuredinformation,havealsobeenthe
diverse decomposition structures and use differ- focus of tasks that require quantitative reasoning
ent prompts to answer each sub-question. Apart overtextualandtabularcontext(Chenetal.,2021c;
from these multi-step reasoning methods, Chen Zhuetal.,2021;Zhaoetal.,2022;Luetal.,2022b).
etal.(2022b);Gaoetal.(2022)proposeprogram- Inaddition,recentdatasetshavebeendevelopedfor
of-thoughts(PoT),analternativesolutionthatuses mathematicalreasoninggroundedonconversations
large language models to express the reasoning (Sun et al., 2019; Zhang et al., 2021; Chen et al.,
process as a program. The computation is then 2022c),aswellasreports(Chenetal.,2022c).
relegatedtoanexternalcomputer,whichexecutes
Pioneeringworkisemergingintheexploration
the generated programs to derive the answer. A
oflow-resourcesettings. Despitethecreationof
more recent work, Chameleon (Lu et al., 2023),
various datasets, mathematical reasoning in low-
integratesdifferenttoolstoenhancetheabilitiesof
resource settings remains largely under-explored.
LLMsforcompositionalreasoning.
Pioneering research has developed mathematical
reasoning benchmarks for financial (Chen et al.,
Outcome-based approaches acknowledge the
2021c; Zhu et al., 2021; Zhao et al., 2022) and
potential incorrectness of an individual reason-
scientific domains (Lu et al., 2022a). Addition-
ing path, and instead use multiple reasoning
ally,therehavebeenattemptstobuildnon-English
paths (Wang et al., 2023; Li et al., 2022a). Self-
datasetsforChinese(Wangetal.,2017;Qinetal.,
consistency(Wangetal.,2023)generatesasetof
2020;Yuetal.,2021a)andArabic(Alghamdietal.,
reasoning paths by sampling from the language
2022)formathematicalreasoning.
model, and marginalizes out the reasoning paths
by choosing the most common answer. In addi- Diverserationaleannotationshavebeenwidely
tiontousingsamplingwithasingleprompttopro- explored. Complex reasoning usually involves
duce multiple reasoning paths, Li et al. (2022a) multiple steps to arrive at the final answer. To
proposetointroducediversepromptsthrough“self- bridge this gap, datasets annotated with interme-
teaching”,asacomplementarysolutiontoproduce diaterationalessuchaslogicforms(Tafjordetal.,
T5 UnifiedQA GPT-3 GPT-3 Problems GPT-3(text-davinci-002)
(Large) (Large) (davinci-002)(davinci-003) Johnhad8ballsandhegave3toMary. Johnhas5balls.
3balls+5balls= ✗ 5balls 8balls 8balls HowmanyballsdoesJohnhavenow?
23balls+145balls= ✗ ✗ 58balls 168balls Johnhad3apples.Johnhad8ballsand Maryhas5balls.
23balls+1,855balls= ✗ ✗ 2,878balls 2,988balls hegave3toMary. Howmanyballs
doesMaryhavenow?
Table5: Languagemodelsstrugglewithlargenumbers. Johnhad8ballsandhegave3toMary. Johnhasmoreballs.
Whohasmoreballsnow?
2019; Lu et al., 2021a), programs (Amini et al., Johnhad8ballsandhegave3toMary. No,Johnhas5ballsnow.
DoesJohnhavemoreballsnow?
2019; Chen et al., 2021c,a; Cao and Xiao, 2022;
Johnhad8ballsandhegave4toMary. No,Johnhas4ballsnow.
Chenetal.,2022a),andreasoninggraphs(Zhang DoesJohnhavemoreballsnow?
et al., 2021) have been proposed to train models Johnhad8ballsandhegave4toMary. Johnhasmoreballs.
for complex reasoning tasks. Python programs Whohasmoreballsnow?
areusedasreasoningannotationsin(Austinetal., Table6: Exampleswherelargelanguagemodelsarenot
2021;Mishraetal.,2022a)duetotheirenhanced consistentformathematicalreasoning.
accessibility and readability. To imitate the rea-
thisremainsanopenproblem.
soning process of a human, a more recent trend
Aredeeplearningmethodsconsistentformathe-
istoannotatesolutionsinnaturallanguage(Ling
maticalreasoning? Recentdevelopmentsindeep
et al., 2017; Cobbe et al., 2021; Lu et al., 2022b;
learning have led to impressive results on vari-
Hendrycksetal.,2021b;Luetal.,2022a).
ousmathematicalreasoningtasks. Thezero-shot-
6.2 AnalysisofDeepLearningMethods CoTMinerva540Bachievesascoreof75.0%on
the MMLU-STEM benchmark (Hendrycks et al.,
Isthecurrentrepresentationofnumeracysuf-
2021a), which assesses multitask reasoning abil-
ficient? The standard practice for deep learning
ity in the fields of science, technology, engineer-
techniquesistotreatnumbersinthesamewayas
ing,andmathematics(STEM)atbothhighschool
words. Earlyneuralnetworkmethodscreateavo-
andcollegelevels. Similarly,few-shot-CoTGPT-3
cabulary that maps input words and numbers to
175B achieves a high accuracy of 93.0% on the
tokenIDs,resultinginlessfrequentnumbersbeing
MultiArithtask. However,thequestionremainsas
collapsed into an “UNK” token. Recent language
towhetherthesemethodsaresufficientlyadvanced
modelsusesubwordtokenizationtechniques(Wu
totacklemorecomplexproblems.
etal.,2016;Sennrichetal.,2016)tosplitnumbers
Thereisstrongevidencethatdeeplearningmeth-
into atomic tokens. Recent studies have shown
odsformathematicalreasoningarenotrobustand
thatthesetokenizationapproachesaresuboptimal
susceptibletoadversarialattacks(Linetal.,2020;
(Wallaceetal.,2019;Linetal.,2020;Zhangetal.,
Pateletal.,2021;Mishraetal.,2022b,a;Welleck
2020d;Thawanietal.,2022).
et al., 2022b). The SVAMP (Patel et al., 2021)
Twonumbersonthesameorclosenumberline
datasetisacollectionofone-unknownarithmetic
couldhavesurfaceformswithnosharedcommon
wordproblemsuptograde4,withslightwordvari-
tokens. For example, a number like 1598 is tok-
ationsfrompreviousdatasets. Itissurprisingthat
enizedas“15”and“98”inGPT-3, whileanother
current state-of-the-art (SOTA) methods perform
formatlike1,598issplitasthreedifferenttokens:
poorlyonthisdataset,withGraph2Treeachieving
“1”,“,”,and“598”. Thislackofconsistentrepresen-
onlya43.8%accuracyandzero-shot-CoTGPT-3
tationcanmakeitdifficultfordeeplearningmod-
(175B)onlyreaching63.7%,whichisjustabove
elstoeffectivelyprocessnumbers,especiallywhen
an“F”grade. Table6alsoshowstheinconsistent
compared to pure text. The insufficient represen-
performanceofthezero-shotGPT-3modelinsce-
tationsofnumberscanleadtoout-of-distribution
narios with slightly different descriptions, while
(OOD) problems. Table 5 provides examples of
humanperformanceremainsunchanged. Thisin-
wherelanguagemodelstendtostrugglewithlarge
dicatesalackofconsistencyinthemathematical
numbers. Althoughincreasingmodelscalescould
reasoningabilityofSOTAlargelanguagemodels.
help,eventhestate-of-the-artlargelanguagemodel
GPT-3performspoorlywhenreasoningoverlarge 7 FutureWork
numbers. Some recent work suggests that using
7.1 GeneralizationandRobustness
scientificnotation(Zhangetal.,2020d)anddigit-
level decomposition (Geva et al., 2020) may be Despiteimpressiveprogress, neuralmodelscom-
helpfulinimprovingnumeracyrepresentation,but monly display generalization and robustness fail-
uresonreasoningtasks. Forexample,abovewedis- ingreinforcementlearningfromhumanfeedback
cusseddifficultiesingeneralizingtolargernumbers (RLHF) (Ouyang et al., 2022) to align language
(Table5)orremainingrobusttonearbyproblems modelswithinstructions. Theideaistolethumans
(Table 6), while others identify failures in gener- rankthegeneratedoutputsoflanguagemodelsand
alizingtolongerproblemsthanthoseobservedin usethelearnedrewardfunctiontofinetunethelan-
training(e.g.,Aniletal.(2022)). Onedirectionis guagemodelwithpolicygradient(Ouyangetal.,
to explore new inference-time (Jung et al., 2022; 2022;Glaeseetal.,2022;Qiuetal.,2022a). Inthe
Mitchell et al., 2022) or fine-tuning (Anil et al., contextofmathematicalreasoning,feedbackdoes
2022)strategies. not necessarily come from humans directly. The
Another aspect of generalization relates to the outcome of a theorem-proof engine (Jiang et al.,
roleofmemorization. Forexample,istheabilityto 2021; Wu et al., 2021d, 2022c) or the execution
produce a complex solution dependent on seeing resultofmodel-generatedscriptscanalsobeused
manysimilarsolutionsduringtraining,orevenon astherewardsource(PoluandSutskever,2020).
memorizing the solution? Term frequency in the
pretrainingcorpusisknowntoimpactaccuracyin 7.4 Multi-modalMathematicalReasoning
simple arithmetic tasks (Razeghi et al., 2022) or
In recent years, there has been growing interest
factualquestionanswering(Kandpaletal.,2022).
inmulti-modalmathematicalreasoning,whichin-
Ontheotherhand,Lewkowyczetal.(2022)didnot
volvesusingmultiplesourcesofinformation,such
findevidenceofmemorizationincomplexoutputs,
astext,tables,naturalimages,anddiagrams (Ka-
yet their training set and model are not available
houetal.,2018;Kafleetal.,2018;Luetal.,2021b,
for inspection. Gaining a full understanding of
2022b). However, currently available datasets in
these factors for complex problems and outputs
this domain tend to be small (Zhao et al., 2022),
(e.g.,multi-stepsolutionsorproofs)requiresmore
generatedfromtemplates(Kahouetal.,2018),or
analysis,aswellasaccessibledatasetsandmodels.
focus on specific topics (Lu et al., 2021a; Chen
etal.,2022a). Onelineofcurrentresearchinvolves
7.2 TrustworthyReasoning
applying VQA-based frameworks to analyze fig-
Recentadvancesinlanguagemodelshavedemon-
uresandplots,butthisapproachcanresultinsig-
stratedtheirpowerfulcapabilitiesformathematical
nificant semantic gaps due to the fact that most
reasoning. However,duetothepotentialforgen-
VQA models are trained on natural images. One
eratingungroundedanswers(Nakanoetal.,2021),
potential direction for future work is to enhance
userscan’talwaystrustthepredictedoutcomesor
theabilityofmulti-modalmathematicalreasoning
have to verify then with extra efforts. Even with
systemstotacklemorecomplexandrealisticprob-
recentpromptingstrategiesthatproviderationales
lems. Thismayinvolvecreatingunifiedmodelsfor
beforemakingpredictions(Weietal.,2022),lan-
interpretingandintegratingdifferentmodalities,as
guagemodelscanstillhallucinatestatements,pro-
wellasdevelopingbetterevaluationbenchmarksto
duceflawedreasoning,andoutputwronganswers.
assesstheperformanceofthesesystems.
Consequently,novelapproachesthatenablemore
reliablereasoningareneededurgently. Somepoten- 8 Conclusion
tialdirectionsforthisinclude: (i)usinglanguage
modelstoprovideevidence,suchastheorems,to Inthispaper,wepresentacomprehensivesurveyof
supportthereasoningprocess;(ii)incorporatinga deeplearningformathematicalreasoning. Were-
mechanismthatmakesajudgmentwhenthemodel viewthevarioustasks,datasets,anddeeplearning
isunsureoftheanswer;and(iii)usingamodelit- approaches. We also identify several gaps in the
selforanothermoduletodetectandlocatemistakes existingdatasetsandmethods. Finally,weoutline
inamodel’sreasoning. directionsforfutureresearchandhighlightthepo-
tentialforfurtherexplorationinthisfield. Ourgoal
7.3 LearningfromFeedback
withthispaperistoprovideacomprehensiveand
Anotherimportantdirectiontofurtherimprovelan- usefulresourceforreadersinterestedinthedevel-
guagemodelsformathematicalreasoningistolet opmentofdeeplearningformathematicalreason-
the model learn from feedback. Such a process ing. Toaidinthiseffort,wehavecreatedareading
makesthecontinualimprovementofmodels’out- list that will be continually updated in a GitHub
putqualityandsafetypossible. Anexampleisus- repositoryathttps://github.com/lupantech/dl4math.
Limitations Chris Alvin, Sumit Gulwani, Rupak Majumdar, and
Supratik Mukhopadhyay. 2017. Synthesis of solu-
One limitation of our survey work is that it is fo- tions for shaded area geometry problems. In The
cusedontheintersectionofmathematicalreason- ThirtiethInternationalFlairsConference.
inganddeeplearningoverthepastdecade,which
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
may not encompass the entire field and its his-
Koncel-Kedziorski, YejinChoi, andHannanehHa-
tory. Additionally,ourevaluationofexistingbench- jishirzi.2019. Mathqa: Towardsinterpretablemath
marks and methods is based on a curated set of word problem solving with operation-based for-
malisms. In Proceedings of the 2019 Conference
papersandmaynotfullyrepresentthestateofthe
of the North American Chapter of the Association
artinthefield. Furthermore,duetothefast-paced
for Computational Linguistics: Human Language
natureofthefield,oursurveymaynotreflectthe Technologies(NAACL-HLT),pages2357–2367.
latestdevelopmentsandadvancementswhichmay
ConnorAndersonandRyanFarrell.2022. Improving
havecomeoutclosetoorafterthesurveywascon-
fractalpre-training. InProceedingsoftheIEEE/CVF
ducted. Despitetheselimitations,oursurveystill
WinterConferenceonApplicationsofComputerVi-
provides a valuable overview of the current state sion,pages1300–1309.
andkeytrendsinthefieldofmathematicalreason-
PeterAnderson,XiaodongHe,ChrisBuehler,Damien
inganddeeplearning,andcanserveasavaluable
Teney,MarkJohnson,StephenGould,andLeiZhang.
resourceforresearchersandpractitionersworking 2018. Bottom-upandtop-downattentionforimage
inthisfield. captioning and visual question answering. In Pro-
ceedingsoftheIEEEconferenceoncomputervision
BroaderImpact andpatternrecognition(CVPR),pages6077–6086.
Oursurveypaperontheintersectionofmathemat- CemAnil,YuhuaiWu,AndersJohanAndreassen,Aitor
Lewkowycz, Vedant Misra, Vinay Venkatesh Ra-
icalreasoninganddeeplearninghasthepotential
masesh,AmbroseSlone,GuyGur-Ari,EthanDyer,
tosignificantlyimpactthefieldofartificialintelli-
andBehnamNeyshabur.2022. Exploringlengthgen-
gence. Byprovidingacomprehensiveoverviewof eralizationinlargelanguagemodels. InAdvancesin
thekeytasks,datasets,andmethodsthathavebeen NeuralInformationProcessingSystems(NeurIPS).
developedinthepastdecade,wegiveresearchers
JacobAustin,AugustusOdena,MaxwellNye,Maarten
andpractitionersaclearunderstandingofthecur-
Bosma, Henryk Michalewski, David Dohan, Ellen
rentstate-of-the-artandhelpthemmakeinformed Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.
decisionsabouttheirownresearch. Additionally, Programsynthesiswithlargelanguagemodels. arXiv
preprintarXiv:2108.07732.
by evaluating existing benchmarks and methods
anddiscussingfutureresearchdirections,weaim DzmitryBahdanau,KyunghyunCho,andYoshuaBen-
toidentifygapsinthecurrentstateoftheartand gio. 2015. Neural machine translation by jointly
guidefutureresearchanddevelopmenteffortsto- learningtoalignandtranslate. InInternationalCon-
ferenceonLearningRepresentations(ICLR).
wardsmoreadvancedandeffectivemathematical
reasoning systems. Overall, our survey has the Kshitij Bansal, Sarah Loos, Markus Rabe, Christian
potentialtocontributetotheadvancementofmath- Szegedy,andStewartWilcox.2019. Holist: Anenvi-
ronmentformachinelearningofhigherorderlogic
ematicalreasoninganddeeplearning,andhavea
theorem proving. In International Conference on
profoundimpactonmachinelearningandnatural
MachineLearning(ICML),pages454–463.PMLR.
languageprocessing.
Bruno Barras, Samuel Boutin, Cristina Cornes, Judi-
caëlCourant,YannCoscoy,DavidDelahaye,Daniel
References de Rauglaudre, Jean-Christophe Filliâtre, Eduardo
Giménez,HugoHerbelin,etal.1999. Thecoqproof
AlexanderA.Alemi,FrançoisChollet,NiklasEen,Ge- assistantreferencemanual. INRIA,version,6(11).
offrey Irving, Christian Szegedy, and Josef Urban.
2016. Deepmath-deepsequencemodelsforpremise Taylor Berg-Kirkpatrick and Daniel Spokoyny. 2020.
selection. Advancesinneuralinformationprocessing Anempiricalinvestigationofcontextualizednumber
systems(NeurIPS),29. prediction. InProceedingsofthe2020Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing
Reem Alghamdi, Zhenwen Liang, and Xiangliang (EMNLP),pages4754–4764.
Zhang.2022. Armath: adatasetforsolvingarabic
math word problems. In Proceedings of the Thir- Arindam Bhattacharya. 2017. A survey of question
teenthLanguageResourcesandEvaluationConfer- answering for math and science problem. arXiv
ence(LREC),pages351–362. preprintarXiv:1705.04530.
Daniel G Bobrow. 1964. Natural language input for Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang
a computer problem solving system. AI Technical Ma,SameenaShah,andWilliamYangWang.2022c.
Reports. Convfinqa: Exploring the chain of numerical rea-
soninginconversationalfinancequestionanswering.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie arXivpreprintarXiv:2210.03849.
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,Amanda Ting-Rui Chiang and Yun-Nung Chen. 2019.
Askell,etal.2020. Languagemodelsarefew-shot Semantically-aligned equation generation for
learners. AdvancesinNeuralInformationProcessing solving and reasoning math word problems. In
Systems(NeurIPS),33:1877–1901. Proceedings of the 2019 Conference of the North
AmericanChapteroftheAssociationforComputa-
JieCaoandJingXiao.2022. Anaugmentedbenchmark tionalLinguistics: HumanLanguageTechnologies
dataset for geometric question answering through (NAACL-HLT),pages2656–2668.
dual parallel text encoding. In Proceedings of the
29thInternationalConferenceonComputationalLin- JaeminCho,JieLei,HaoTan,andMohitBansal.2021.
guistics(COLING),pages1511–1520. Unifyingvision-and-languagetasksviatextgenera-
tion. InProceedingsofthe38thInternationalCon-
Yixuan Cao, Feng Hong, Hongwei Li, and Ping Luo. ferenceonMachineLearning(ICML),pages1931–
2021. Abottom-updagstructureextractionmodel 1942.
formathwordproblems. InProceedingsoftheAAAI
ConferenceonArtificialIntelligence(AAAI),pages Kyunghyun Cho, Bart van Merrienboer Caglar Gul-
39–46. cehre, Dzmitry Bahdanau, Fethi Bougares Holger
Schwenk, and Yoshua Bengio. 2014. Learning
FrançoisCharton.2022. Linearalgebrawithtransform- phraserepresentationsusingrnnencoder–decoderfor
ers. TransactionsonMachineLearningResearch. statisticalmachinetranslation. InProceedingsofthe
2014ConferenceonEmpiricalMethodsinNatural
JiaqiChen, TongLi, JinghuiQin, PanLu, LiangLin,
LanguageProcessing(EMNLP),pages1724–1734.
ChongyuChen,andXiaodanLiang.2022a. Unigeo:
Unifying geometry logical reasoning via reformu- Shang-Ching Chou, Xiao-Shan Gao, and Jing-Zhong
lating mathematical expression. In The 2022 Con- Zhang. 1996. Automated generation of readable
ferenceonEmpiricalMethodsinNaturalLanguage proofs with geometric invariants. Journal of Auto-
Processing(EMNLP). matedReasoning,17(3):325–347.
JiaqiChen,JianhengTang,JinghuiQin,XiaodanLiang, AakankshaChowdhery,SharanNarang,JacobDevlin,
LingboLiu,EricXing,andLiangLin.2021a. Geoqa: Maarten Bosma, Gaurav Mishra, Adam Roberts,
Ageometricquestionansweringbenchmarktowards Paul Barham, Hyung Won Chung, Charles Sutton,
multimodalnumericalreasoning. InFindingsofthe Sebastian Gehrmann, et al. 2022. Palm: Scaling
Association for Computational Linguistics (ACL), language modeling with pathways. arXiv preprint
pages513–523. arXiv:2204.02311.
MarkChen,JerryTworek,HeewooJun,QimingYuan, Peter Clark, Oren Etzioni, Tushar Khot, Daniel
Henrique Ponde de Oliveira Pinto, Jared Kaplan, Khashabi,BhavanaMishra,KyleRichardson,Ashish
HarriEdwards,YuriBurda,NicholasJoseph,Greg Sabharwal,CarissaSchoenick,OyvindTafjord,Niket
Brockman, et al. 2021b. Evaluating large lan- Tandon,etal.2020. From‘f’to‘a’onthenyregents
guage models trained on code. arXiv preprint scienceexams: Anoverviewofthearistoproject. AI
arXiv:2107.03374. Magazine,41(4):39–53.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
William W Cohen. 2022b. Program of thoughts ian, Jacob Hilton, Reiichiro Nakano, Christopher
prompting: Disentanglingcomputationfromreason- Hesse, and John Schulman. 2021. Training veri-
ing for numerical reasoning tasks. arXiv preprint fiers to solve math word problems. arXiv preprint
arXiv:2211.12588. arXiv:2110.14168.
Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang, Kristina Toutanova. 2019. BERT: Pre-training of
and Pan Lu. 2023. Theoremqa: A theorem- deepbidirectionaltransformersforlanguageunder-
driven question answering dataset. arXiv preprint standing. In Proceedings of the 2019 Conference
arXiv:2305.12524. of the North American Chapter of the Association
for Computational Linguistics: Human Language
ZhiyuChen,WenhuChen,ChareseSmiley,Sameena Technologies(NAACL-HLT),pages4171–4186.
Shah,IanaBorova,DylanLangdon,ReemaMoussa,
MattBeane,Ting-HaoHuang,BryanRRoutledge, DheeruDua,YizhongWang,PradeepDasigi,Gabriel
etal.2021c. Finqa:Adatasetofnumericalreasoning Stanovsky,SameerSingh,andMattGardner.2019.
overfinancialdata. InProceedingsofthe2021Con- Drop: Areadingcomprehensionbenchmarkrequir-
ferenceonEmpiricalMethodsinNaturalLanguage ingdiscretereasoningoverparagraphs. InProceed-
Processing(EMNLP),pages3697–3711. ingsofthe2019ConferenceoftheNorthAmerican
Chapter of the Association for Computational Lin- Mor Geva, Ankit Gupta, and Jonathan Berant. 2020.
guistics: Human Language Technologies (NAACL- Injecting numerical reasoning skills into language
HLT),pages2368–2378. models. In Proceedings of the 58th Annual Meet-
ingoftheAssociationforComputationalLinguistics
Edward A Feigenbaum et al. 1963. Computers and (ACL),pages946–958.
thought. McGraw-Hill.
KevinGimpel,DipanjanDas,andNoahASmith.2010.
Yu Feng, Jing Zhang, Xiaokang Zhang, Lemao Liu, Distributedasynchronousonlinelearningfornatural
CuipingLi,andHongChen.2021. Injectingnumer- language processing. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
ical reasoning skills into knowledge base question
answeringmodels. arXivpreprintarXiv:2112.06109. guageLearning,pages213–222.
Amelia Glaese, Nat McAleese, Maja Tre˛bacz, John
Deborah Ferreira and André Freitas. 2020a. Natural
Aslanides,VladFiroiu,TimoEwalds,MaribethRauh,
languagepremiseselection:Findingsupportingstate-
LauraWeidinger,MartinChadwick,PhoebeThacker,
mentsformathematicaltext. InProceedingsofthe
etal.2022. Improvingalignmentofdialogueagents
TwelfthLanguageResourcesandEvaluationConfer-
via targeted human judgements. arXiv preprint
ence,pages2175–2182.
arXiv:2209.14375.
DeborahFerreiraandAndréFreitas.2020b. Premise
AdamGrabowski,ArturKorniłowicz,andAdamNau-
selectioninnaturallanguagemathematicaltexts. In
mowicz.2015. Fourdecadesofmizar. Journalof
Proceedingsofthe58thAnnualMeetingoftheAsso-
AutomatedReasoning,55(3):191–198.
ciationforComputationalLinguistics(ACL),pages
7365–7374. KelvinGuu,KentonLee,ZoraTung,PanupongPasu-
pat,andMingweiChang.2020. Retrievalaugmented
YaoFu,HaoPeng,AshishSabharwal,PeterClark,and languagemodelpre-training. InInternationalCon-
TusharKhot.2023. Complexity-basedpromptingfor ferenceonMachineLearning(ICML),pages3929–
multi-stepreasoning. InInternationalConferenceon 3938.PMLR.
LearningRepresentations(ICLR).
JesseMichaelHan,JasonRute,YuhuaiWu,EdwardW
LeoGao,StellaBiderman,SidBlack,LaurenceGold- Ayers, and Stanislas Polu. 2022. Proof artifact co-
ing,TravisHoppe,CharlesFoster,JasonPhang,Ho- trainingfortheoremprovingwithlanguagemodels.
raceHe, AnishThite, NoaNabeshima, etal.2020. InInternationalConferenceonLearningRepresenta-
The pile: An 800gb dataset of diverse text for lan- tions(ICLR).
guagemodeling. arXivpreprintarXiv:2101.00027.
Yihan Hao, Mingliang Zhang, Fei Yin, and Linlin
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Huang. 2022. Pgdp5k: A diagram parsing dataset
PengfeiLiu, YimingYang, JamieCallan, andGra- forplanegeometryproblems. In26thInternational
ham Neubig. 2022. Pal: Program-aided language ConferenceonPatternRecognition(ICPR).
models. arXivpreprintarXiv:2211.10435.
KaimingHe,XiangyuZhang,ShaoqingRen,andJian
Sun.2016. Deepresiduallearningforimagerecogni-
Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu,
tion. InProceedingsoftheIEEEconferenceoncom-
StevenCHHoi,XiaogangWang,andHongshengLi.
putervisionandpatternrecognition(CVPR),pages
2019. Dynamicfusionwithintra-andinter-modality
770–778.
attentionflowforvisualquestionanswering. InThe
IEEE Conference on Computer Vision and Pattern
DanHendrycks,CollinBurns,StevenBasart,AndyZou,
Recognition(CVPR),pages6639–6648.
MantasMazeika,DawnSong,andJacobSteinhardt.
2021a. Measuringmassivemultitasklanguageunder-
ThibaultGauthier,CezaryKaliszyk,JosefUrban,Ra-
standing. InInternationalConferenceonLearning
manaKumar,andMichaelNorrish.2021. TacticToe:
Representations(ICLR).
Learning to Prove with Tactics. Journal of Auto-
matedReasoning.
DanHendrycks,CollinBurns,SauravKadavath,Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jonas Gehring, Michael Auli, David Grangier, Denis JacobSteinhardt.2021b. Measuringmathematical
Yarats,andYannNDauphin.2017. Convolutionalse- problemsolvingwiththemathdataset. In35thCon-
quencetosequencelearning. InInternationalconfer- ferenceonNeuralInformationProcessingSystems
enceonmachinelearning(ICML),pages1243–1252. (NeurIPS)TrackonDatasetsandBenchmarks.
PMLR.
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam
Herbert Gelernter, James R Hansen, and Donald W Dziedzic,RishabhKrishnan,andDawnSong.2020.
Loveland. 1960. Empirical explorations of the ge- Pretrainedtransformersimproveout-of-distribution
ometry theorem machine. In Papers presented at robustness. InProceedingsofthe58thAnnualMeet-
the May 3-5, 1960, western joint IRE-AIEE-ACM ingoftheAssociationforComputationalLinguistics
computerconference,pages143–149. (ACL),pages2744–2751.
Jonathan Herzig, Pawel Krzysztof Nowak, Thomas AlbertQiaochuJiang,WendaLi,SzymonTworkowski,
Mueller, Francesco Piccinno, and Julian Eisensch- Konrad Czechowski, Tomasz Odrzygóz´dz´, Piotr
los.2020. Tapas: Weaklysupervisedtableparsing Miłos´,YuhuaiWu,andMatejaJamnik.2022b. Thor:
viapre-training. InProceedingsofthe58thAnnual Wieldinghammerstointegratelanguagemodelsand
Meeting of the Association for Computational Lin- automated theorem provers. Advances in Neural
guistics(ACL),pages4320–4333. InformationProcessingSystems(NeurIPS),35:8360–
8373.
SeppHochreiterandJürgenSchmidhuber.1997. Long
short-termmemory. Neuralcomputation,9(8):1735– ZhanmingJie,JieruiLi,andWeiLu.2022. Learning
1780. toreasondeductively: Mathwordproblemsolving
ascomplexrelationextraction. InProceedingsofthe
Yining Hong, Qing Li, Daniel Ciao, Siyuan Huang, 60thAnnualMeetingoftheAssociationforCompu-
and Song-Chun Zhu. 2021a. Learning by fixing: tationalLinguistics(ACL),pages5944–5955.
Solvingmathwordproblemswithweaksupervision.
InProceedingsoftheAAAIConferenceonArtificial JaehunJung,LianhuiQin,SeanWelleck,FaezeBrah-
Intelligence(AAAI),pages4959–4967. man, Chandra Bhagavatula, Ronan Le Bras, and
Yejin Choi. 2022. Maieutic prompting: Logically
YiningHong,QingLi,RanGong,DanielCiao,Siyuan consistentreasoningwithrecursiveexplanations. In
Huang,andSong-ChunZhu.2021b. Smart: Asitua- Proceedings of the 2022 Conference on Empirical
tionmodelforalgebrastoryproblemsviaattributed MethodsinNaturalLanguageProcessing(EMNLP),
grammar. InAAAI,pages13009–13017. pages1266–1279.
KushalKafle,BrianPrice,ScottCohen,andChristopher
MohammadJavadHosseini,HannanehHajishirzi,Oren
Kanan.2018. Dvqa: Understandingdatavisualiza-
Etzioni,andNateKushman.2014. Learningtosolve
tionsviaquestionanswering. InProceedingsofthe
arithmeticwordproblemswithverbcategorization.
IEEE Conference on Computer Vision and Pattern
InProceedingsofthe2014ConferenceonEmpirical
Recognition(CVPR),pages5648–5656.
MethodsinNaturalLanguageProcessing(EMNLP).
Samira Ebrahimi Kahou, Vincent Michalski, Adam
DanielHuang,PrafullaDhariwal,DawnSong,andIlya
Atkinson,ÁkosKádár,AdamTrischler,andYoshua
Sutskever.2019. Gamepad: Alearningenvironment
Bengio.2018. Figureqa: Anannotatedfiguredataset
fortheoremproving. InInternationalConferenceon
forvisualreasoning. InInternationalConferenceon
LearningRepresentations(ICLR).
LearningRepresentations(ICLR).
DanqingHuang,JingLiu,Chin-YewLin,andJianYin.
Cezary Kaliszyk, François Chollet, and Christian
2018. Neuralmathwordproblemsolverwithrein-
Szegedy.2017. Holstep: Amachinelearningdataset
forcementlearning. InProceedingsofthe27thInter-
for higher-order logic theorem proving. In Inter-
nationalConferenceonComputationalLinguistics
national Conference on Learning Representations
(COLING),pages213–223.
(ICLR).
DanqingHuang,ShumingShi,Chin-YewLin,andJian
Ashwin Kalyan, Abhinav Kumar, Arjun Chan-
Yin.2017. Learningfine-grainedexpressionstosolve
drasekaran,AshishSabharwal,andPeterClark.2021.
mathwordproblems. InProceedingsofEmpirical
Howmuchcoffeewasconsumedduringemnlp2019?
MethodsinNaturalLanguageProcessing(EMNLP),
fermi problems: A new reasoning challenge for ai.
pages805–814.
InProceedingsofthe2021ConferenceonEmpirical
MethodsinNaturalLanguageProcessing(EMNLP),
DanqingHuang,ShumingShi,Chin-YewLin,JianYin,
pages7318–7328.
and Wei-Ying Ma. 2016. How well do computers
solvemathwordproblems? large-scaledatasetcon- Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wal-
structionandevaluation. InProceedingsofthe54th lace,andColinRaffel.2022. Largelanguagemod-
AnnualMeetingoftheAssociationforComputational els struggle to learn long-tail knowledge. ArXiv,
Linguistics(ACL),pages887–896. abs/2211.08411.
AlbertQ.Jiang,SeanWelleck,JinPengZhou,Wenda DanielKhashabi,SewonMin,TusharKhot,AshishSab-
Li,JiachengLiu,MatejaJamnik,TimothéeLacroix, harwal,OyvindTafjord,PeterClark,andHannaneh
Yuhuai Wu, and Guillaume Lample. 2022a. Draft, Hajishirzi.2020. Unifiedqa: Crossingformatbound-
sketch,andprove: Guidingformaltheoremprovers aries with a single qa system. In Findings of the
withinformalproofs. InSubmittedtoTheEleventh AssociationforComputationalLinguistics(EMNLP),
International Conference on Learning Representa- pages1896–1907.
tions.
TusharKhot,HarshTrivedi,MatthewFinlayson,Yao
Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, Fu,KyleRichardson,PeterClark,andAshishSab-
and Yuhuai Wu. 2021. Lisa: Language models of harwal.2022. Decomposedprompting: Amodular
isabelleproofs. In6thConferenceonArtificialIntel- approachforsolvingcomplextasks. arXivpreprint
ligenceandTheoremProving(AITP). arXiv:2210.02406.
BugeunKim,KyungSeoKi,DonggeonLee,andGah- Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
geneGweon.2020. Pointtotheexpression: Solving Kevin Gimpel, Piyush Sharma, and Radu Soricut.
algebraicwordproblemsusingtheexpression-pointer 2019. Albert: Alitebertforself-supervisedlearn-
transformermodel. InProceedingsofthe2020Con- ing of language representations. arXiv preprint
ferenceonEmpiricalMethodsinNaturalLanguage arXiv:1909.11942.
Processing(EMNLP),pages3768–3779.
YannLeCun,LéonBottou,YoshuaBengio,andPatrick
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
Haffner. 1998. Gradient-based learning applied to
2018. Bilinearattentionnetworks. InAdvancesin
document recognition. Proceedings of the IEEE,
NeuralInformationProcessingSystems(NeurIPS),
86(11):2278–2324.
pages1571–1581.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
WonjaeKim,BokyungSon,andIldooKim.2021. Vilt:
Ghazvininejad,AbdelrahmanMohamed,OmerLevy,
Vision-and-language transformer without convolu-
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
tion or region supervision. In Proceedings of the
BART:Denoisingsequence-to-sequencepre-training
38thInternationalConferenceonMachineLearning
fornaturallanguagegeneration,translation,andcom-
(ICML),pages5583–5594.
prehension. InProceedingsofthe58thAnnualMeet-
TakeshiKojima,ShixiangShaneGu,MachelReid,Yu- ingoftheAssociationforComputationalLinguistics
takaMatsuo,andYusukeIwasawa.2022. Largelan- (ACL),pages7871–7880.
guagemodelsarezero-shotreasoners. In36thCon-
ferenceonNeuralInformationProcessingSystems Aitor Lewkowycz, Anders Johan Andreassen,
(NeurIPS). David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay Venkatesh Ramasesh, Ambrose Slone, Cem
RikKoncel-K.,SubhroRoy,AidaAmini,NateKush- Anil, Imanol Schlag, Theo Gutman-Solo, et al.
man, and Hannaneh Hajishirzi. 2016. Mawps: A 2022. Solvingquantitativereasoningproblemswith
mathwordproblemrepository. InProceedingsofthe languagemodels. InAdvancesinNeuralInformation
2016ConferenceoftheNorthAmericanChapterof ProcessingSystems(NeurIPS).
theAssociationforComputationalLinguistics: Hu-
manLanguageTechnologies(NAACL),pages1152– JieruiLi,LeiWang,JipengZhang,YanWang,BingTian
1157. Dai,andDongxiangZhang.2019. Modelingintra-
relationinmathwordproblemswithdifferentfunc-
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish
tionalmulti-headattentions. InProceedingsofthe
Sabharwal, Oren Etzioni, and Siena Dumas Ang.
57thAnnualMeetingoftheAssociationforCompu-
2015. Parsing algebraic word problems into equa-
tationalLinguistics(ACL),pages6162–6167.
tions. TransactionsoftheAssociationforComputa-
tionalLinguistics(TACL),3:585–597.
Jiwei Li, Alexander H Miller, Sumit Chopra,
KundanKrishna,JeffreyBigham,andZacharyCLipton. Marc’AurelioRanzato,andJasonWeston.2017. Di-
2021. Does pretraining for summarization require alogue learning with human-in-the-loop. In Inter-
knowledgetransfer? InFindingsoftheAssociation national Conference on Learning Representations
forComputationalLinguistics: EMNLP2021,pages (ICLR).
3178–3189.
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Hsieh,andKai-WeiChang.2020a. Whatdoesbert
Regina Barzilay. 2014. Learning to automatically withvisionlookat? InProceedingsofthe58thAn-
solvealgebrawordproblems. InProceedingsofthe nualMeetingoftheAssociationforComputational
52ndAnnualMeetingoftheAssociationforCompu- Linguistics(ACL),pages5265–5275.
tationalLinguistics(ACL),pages271–281.
Shucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu,
GuillaumeLampleandFrançoisCharton.2020. Deep
Fengyuan Xu, and Sheng Zhong. 2020b. Graph-
learningforsymbolicmathematics. InInternational
to-treeneuralnetworksforlearningstructuredinput-
ConferenceonLearningRepresentations(ICLR).
outputtranslationwithapplicationstosemanticpars-
ingandmathwordproblem. InFindingsoftheAs-
Guillaume Lample, Timothee Lacroix, Marie-Anne
sociationforComputationalLinguistics(EMNLP),
Lachaux, Aurelien Rodriguez, Amaury Hayat,
pages2841–2852.
ThibautLavril,GabrielEbner,andXavierMartinet.
2022. Hypertree proof search for neural theorem
WendaLi,LeiYu,YuhuaiWu,andLawrenceCPaulson.
proving. AdvancesinNeuralInformationProcessing
2021. Isarstep: abenchmarkforhigh-levelmathe-
Systems(NeurIPS),35:26337–26349.
maticalreasoning. InInternationalConferenceon
Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, LearningRepresentations(ICLR).
BingTianDai,YanWang,DongxiangZhang,andEe-
PengLim.2022. Mwptoolkit: anopen-sourceframe- YifeiLi,ZeqiLin,ShizhuoZhang,QiangFu,BeiChen,
work for deep learning-based math word problem Jian-GuangLou,andWeizhuChen.2022a. Onthe
solvers. InProceedingsoftheAAAIConferenceon advanceofmakinglanguagemodelsbetterreasoners.
ArtificialIntelligence(AAAI),pages13188–13190. arXivpreprintarXiv:2206.02336.
ZhongliLi,WenxuanZhang,ChaoYan,QingyuZhou, Qianying Liu, Wenyv Guan, Sujian Li, and Daisuke
ChaoLi,HongzhiLiu,andYunboCao.2022b. Seek- Kawahara.2019a. Tree-structureddecodingforsolv-
ingpatterns,notjustmemorizingprocedures: Con- ingmathwordproblems. InProceedingsofthe2019
trastivelearningforsolvingmathwordproblems. In conferenceonempiricalmethodsinnaturallanguage
FindingsoftheAssociationforComputationalLin- processingandthe9thinternationaljointconference
guistics(ACL),pages2486–2496. onnaturallanguageprocessing(EMNLP-IJCNLP),
pages2370–2379.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
Zhang,DeepakNarayanan,YuhuaiWu,AnanyaKu- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
mar, et al. 2022a. Holistic evaluation of language Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
models. arXivpreprintarXiv:2211.09110. Roberta: A robustly optimized bert pretraining ap-
proach. Proceedingsofthe2019Conferenceofthe
PercyLiangandDanKlein.2009. Onlineemforunsu-
NorthAmericanChapteroftheAssociationforCom-
pervisedmodels. InProceedingsofhumanlanguage
putationalLinguistics: HumanLanguageTechnolo-
technologies: The 2009 annual conference of the
gies(NAACL-HLT).
NorthAmericanchapteroftheassociationforcom-
putationallinguistics(NAACL),pages611–619. Sarah Loos, Geoffrey Irving, Christian Szegedy, and
CezaryKaliszyk.2017. Deepnetworkguidedproof
Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin,
search. arXivpreprintarXiv:1701.06972.
YunshiLan,JieShao,andXiangliangZhang.2022b.
Mwp-bert: Numeracy-augmented pre-training for
PanLu,RanGong,ShibiaoJiang,LiangQiu,Siyuan
mathwordproblemsolving. InFindingsoftheAs-
Huang,XiaodanLiang,andSong-ChunZhu.2021a.
sociation for Computational Linguistics (NAACL),
Inter-gps: Interpretable geometry problem solving
pages997–1009.
with formal language and symbolic reasoning. In
The59thAnnualMeetingoftheAssociationforCom-
BillYuchenLin,SeyeonLee,RahulKhanna,andXi-
putationalLinguistics(ACL).
angRen.2020. Birdshavefourlegs?! numersense:
Probingnumericalcommonsenseknowledgeofpre-
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-
trainedlanguagemodels. InProceedingsofthe2020
WeiChang,Song-ChunZhu,OyvindTafjord,Peter
Conference on Empirical Methods in Natural Lan-
Clark,andAshwinKalyan.2022a. Learntoexplain:
guageProcessing(EMNLP),pages6862–6868.
Multimodalreasoningviathoughtchainsforscience
questionanswering. InThe36thConferenceonNeu-
XinLin,ZhenyaHuang,HongkeZhao,EnhongChen,
ralInformationProcessingSystems(NeurIPS).
QiLiu,HaoWang,andShijinWang.2021. Hms: A
hierarchicalsolverwithdependency-enhancedunder-
PanLu,BaolinPeng,HaoCheng,MichelGalley,Kai-
standing for math word problem. In Proceedings
WeiChang,YingNianWu,Song-ChunZhu,andJian-
of the AAAI Conference on Artificial Intelligence
fengGao.2023. Chameleon: Plug-and-playcompo-
(AAAI),pages4232–4240.
sitionalreasoningwithlargelanguagemodels. arXiv
WangLing,DaniYogatama,ChrisDyer,andPhilBlun- preprintarXiv:2304.09842.
som.2017. Programinductionbyrationalegenera-
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,
tion: Learningtosolveandexplainalgebraicword
Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark,
problems. InProceedingsofthe55thAnnualMeet-
andAshwinKalyan.2022b. Dynamicpromptlearn-
ingoftheAssociationforComputationalLinguistics
ing via policy gradient for semi-structured mathe-
(ACL),pages158–167.
maticalreasoning. InInternationalConferenceon
JiachangLiu,DinghanShen,YizheZhang,WilliamB LearningRepresentations(ICLR).
Dolan, Lawrence Carin, and Weizhu Chen. 2022a.
What makes good in-context examples for gpt-3? PanLu,LiangQiu,JiaqiChen,TonyXia,YizhouZhao,
InProceedingsofDeepLearningInsideOut(Dee- WeiZhang,ZhouYu,XiaodanLiang,andSong-Chun
LIO2022): The3rdWorkshoponKnowledgeExtrac- Zhu.2021b. Iconqa: Anewbenchmarkforabstract
tionandIntegrationforDeepLearningArchitectures, diagramunderstandingandvisuallanguagereason-
pages100–114. ing. InThe35thConferenceonNeuralInformation
ProcessingSystems(NeurIPS)TrackonDatasetsand
QianLiu,BeiChen,JiaqiGuo,MortezaZiyadi,Zeqi Benchmarks.
Lin, Weizhu Chen, and Jian-Guang Lou. 2022b.
TAPEX:Tablepre-trainingvialearninganeuralSQL YaoLu,MaxBartolo,AlastairMoore,SebastianRiedel,
executor. InInternationalConferenceonLearning andPontusStenetorp.2022c. Fantasticallyordered
Representations. promptsandwheretofindthem: Overcomingfew-
shotpromptordersensitivity. InProceedingsofthe
Qianying Liu, Wenyu Guan, Sujian Li, Fei Cheng, 60thAnnualMeetingoftheAssociationforCompu-
DaisukeKawahara,andSadaoKurohashi.2020. Re- tationalLinguistics(ACL),pages8086–8098.
verseoperationbaseddataaugmentationforsolving
mathwordproblems. IEEETransactionsonAudio, ThemathlibCommunity.2020. Theleanmathematical
SpeechandLanguageProcessing. library. InCPP2020-Proceedingsofthe9thACM
SIGPLANInternationalConferenceonCertifiedPro- ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,
gramsandProofs,co-locatedwithPOPL2020. Long Ouyang, Christina Kim, Christopher Hesse,
ShantanuJain,VineetKosaraju,WilliamSaunders,
JordanMeadowsandAndreFreitas.2022. Asurveyin et al. 2021. Webgpt: Browser-assisted question-
mathematical language processing. arXiv preprint answering with human feedback. arXiv preprint
arXiv:2205.15231. arXiv:2112.09332.
Norman D. Megill and David A. Wheeler. 2019. AllenNewell,JohnCliffordShaw,andHerbertASimon.
Metamath: AComputerLanguageforMathematical 1957. Empirical explorations of the logic theory
Proofs. Lulu Press, Morrisville, North Carolina. machine:Acasestudyinheuristic. InProceedingsof
http://us.metamath.org/downloads/metamath.pdf. theWesternJointComputerConference,IRE-AIEE-
ACM1957.
Yuanliang Meng and Anna Rumshisky. 2019. Solv-
ingmathwordproblemswithdouble-decodertrans- AnsongNi,JeevanaPriyaInala,ChenglongWang,Olek-
former. arXivpreprintarXiv:1908.10924. sandrPolozov,ChristopherMeek,DragomirRadev,
andJianfengGao.2023. Learningfromself-sampled
Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. correct and partially-correct programs. In Inter-
2020. Adiversecorpusforevaluatinganddeveloping national Conference on Learning Representations
englishmathwordproblemsolvers. InProceedings (ICLR).
of the 58th Annual Meeting of the Association for
ComputationalLinguistics(ACL),pages975–984. MaxwellNye,AndersJohanAndreassen,GuyGur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe, David Dohan, Aitor Lewkowycz, Maarten Bosma,
MikeLewis,HannanehHajishirzi,andLukeZettle- DavidLuan,etal.2021. Showyourwork: Scratch-
moyer.2022. Rethinkingtheroleofdemonstrations: pads for intermediate computation with language
Whatmakesin-contextlearningwork? Proceedings models. arXivpreprintarXiv:2112.00114.
ofEmpiricalMethodsinNaturalLanguageProcess-
ing(EMNLP). LongOuyang,JeffWu,XuJiang,DiogoAlmeida,Car-
rollLWainwright,PamelaMishkin,ChongZhang,
ShervinMinaee,NalKalchbrenner,ErikCambria,Nar- SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
jesNikzad,MeysamChenaghlu,andJianfengGao. 2022. Training languagemodelsto followinstruc-
2021. Deep learning based text classification: a tionswithhumanfeedback. InAdvancesinNeural
comprehensive review. ACM Computing Surveys InformationProcessingSystems(NeurIPS).
(CSUR),54(3):1–40.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are nlp models really able to solve simple
SwaroopMishra,MatthewFinlayson,PanLu,Leonard
math word problems? In Proceedings of the 2021
Tang,SeanWelleck,ChittaBaral,TanmayRajpuro-
Conference of the North American Chapter of the
hit,OyvindTafjord,AshishSabharwal,PeterClark,
AssociationforComputationalLinguistics: Human
andAshwinKalyan.2022a. Lila: Aunifiedbench-
mark for mathematical reasoning. In Proceedings
LanguageTechnologies(NAACL-HIT),pages2080–
of the 2022 Conference on Empirical Methods in 2094.
NaturalLanguageProcessing(EMNLP).
LawrenceC.Paulson.1994. Isabelle-AGenericThe-
orem Prover (with a contribution by T. Nipkow),
Swaroop Mishra, Arindam Mitra, Neeraj Varshney,
volume828ofLectureNotesinComputerScience.
BhavdeepSachdeva,PeterClark,ChittaBaral,and
Springer.
AshwinKalyan.2022b. Numglue: Asuiteoffunda-
mentalyetchallengingmathematicalreasoningtasks.
Ethan Perez, Florian Strub, Harm De Vries, Vincent
InProceedingsofthe60thAnnualMeetingoftheAs-
Dumoulin, and Aaron Courville. 2018. Film: Vi-
sociationforComputationalLinguistics(ACL),pages
sualreasoningwithageneralconditioninglayer. In
3505–3523.
Proceedings of the AAAI Conference on Artificial
Intelligence(AAAI).
EricMitchell,JosephJ.Noh,SiyanLi,WilliamS.Arm-
strong,AnanthAgarwal,PatrickLiu,ChelseaFinn,
StanislasPolu,JesseMichaelHan,KunhaoZheng,Man-
andChristopherD.Manning.2022. Enhancingself-
tas Baksys, Igor Babuschkin, and Ilya Sutskever.
consistencyandperformanceofpretrainedlanguage
2023. Formal mathematics statement curriculum
models with nli. In Proceedings of the 2022 Con-
learning. InInternationalConferenceonLearning
ferenceonEmpiricalMethodsinNaturalLanguage
Representations(ICLR),volumeabs/2202.01344.
Processing(EMNLP).AssociationforComputational
Linguistics. Stanislas Polu and Ilya Sutskever. 2020. Generative
languagemodelingforautomatedtheoremproving.
Leonardo de Moura, Soonho Kong, Jeremy Avigad, arXivpreprintarXiv:2009.03393.
FlorisvanDoorn,andJakobvonRaumer.2015. The
leantheoremprover(systemdescription). InInter- Jinghui Qin, Xiaodan Liang, Yining Hong, Jianheng
nationalConferenceonAutomatedDeduction,pages Tang,andLiangLin.2021. Neural-symbolicsolver
378–388.Springer. for math word problems with auxiliary tasks. In
Proceedingsofthe59thAnnualMeetingoftheAsso- inneuralinformationprocessingsystems(NeurIPS),
ciationforComputationalLinguisticsandthe11th 28.
InternationalJointConferenceonNaturalLanguage
Processing(ACL),pages5870–5881. RyokanRiandYoshimasaTsuruoka.2022. Pretraining
withartificiallanguage: Studyingtransferableknowl-
JinghuiQin,LihuiLin,XiaodanLiang,RuminZhang, edgeinlanguagemodels. InProceedingsofthe60th
andLiangLin.2020. Semantically-aligneduniversal AnnualMeetingoftheAssociationforComputational
tree-structured solver for math word problems. In Linguistics(ACL),pages7302–7315.
Proceedings of the 2020 Conference on Empirical
MethodsinNaturalLanguageProcessing(EMNLP), BenjaminRobaidek,RikKoncel-Kedziorski,andHan-
pages3780–3789. naneh Hajishirzi. 2018. Data-driven methods for
solving algebra word problems. arXiv preprint
Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin arXiv:1804.10718.
Peng,JianfengGao,andSong-ChunZhu.2022a. Val-
uenet:Anewdatasetforhumanvaluedrivendialogue SubhroRoyandDanRoth.2015. Solvinggeneralarith-
system. InProceedingsoftheAAAIConferenceon metic word problems. In Proceedings of the 2015
ArtificialIntelligence(AAAI),pages2468–2484. Conference on Empirical Methods in Natural Lan-
guageProcessing(EMNLP),pages1743–1752.
LiangQiu,YizhouZhao,YuanLiang,PanLu,Weiyan
Shi,ZhouYu,andSong-chunZhu.2022b. Towards Subhro Roy and Dan Roth. 2017. Unit dependency
sociallyintelligentagentswithmentalstatetransition graphanditsapplicationtoarithmeticwordproblem
andhumanvalue. InProceedingsofthe23rdAnnual solving. InProceedingsoftheAAAIConferenceon
MeetingoftheSpecialInterestGrouponDiscourse ArtificialIntelligence(AAAI).
andDialogue,pages146–158.
SubhroRoyandDanRoth.2018. Mappingtodeclara-
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, tiveknowledgeforwordproblemsolving. Transac-
NingDai, andXuanjingHuang.2020. Pre-trained tionsoftheAssociationforComputationalLinguis-
models for natural language processing: A survey. tics(TACL),6:159–172.
ScienceChinaTechnologicalSciences,63(10):1872–
1897. SubhroRoy,TimVieira,andDanRoth.2015. Reason-
ing about quantities in natural language. Transac-
AlecRadford,JeffreyWu,RewonChild,DavidLuan, tionsoftheAssociationforComputationalLinguis-
DarioAmodei,IlyaSutskever,etal.2020. Language tics(TACL),3:1–13.
modelsareunsupervisedmultitasklearners. OpenAI
Blog. Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie learning. NorthAmericanChapteroftheAssociation
Millican, Jordan Hoffmann, Francis Song, John forComputationalLinguistics(NAACL).
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, et al. 2021. Scaling language models: MrinmayaSachan,KumarDubey,andEricXing.2017.
Methods,analysis&insightsfromtraininggopher. Fromtextbookstoknowledge: Acasestudyinhar-
arXivpreprintarXiv:2112.11446. vestingaxiomaticknowledgefromtextbookstosolve
geometry problems. In Proceedings of Empirical
ColinRaffel,NoamShazeer,AdamRoberts,Katherine MethodsinNaturalLanguageProcessing(EMNLP),
Lee,SharanNarang,MichaelMatena,YanqiZhou, pages773–784.
Wei Li, and Peter J Liu. 2020. Exploring the lim-
its of transfer learning with a unified text-to-text Mrinmaya Sachan and Eric Xing. 2017. Learning
transformer. JournalofMachineLearningResearch tosolvegeometryproblemsfromnaturallanguage
(JMLR),21:1–67. demonstrationsintextbooks. InProceedingsofthe
6thJointConferenceonLexicalandComputational
AbhilashaRavichander,AakankshaNaik,CarolynRose, Semantics,pages251–261.
andEduardHovy.2019. Equate:Abenchmarkevalu-
ationframeworkforquantitativereasoninginnatural David Saxton, Edward Grefenstette, Felix Hill, and
languageinference. InProceedingsofthe23rdCon- PushmeetKohli.2020. Analysingmathematicalrea-
ferenceonComputationalNaturalLanguageLearn- soningabilitiesof neuralmodels. In International
ing(CoNLL),pages349–361. ConferenceonLearningRepresentations(ICLR).
YasamanRazeghi, RobertLLoganIV,MattGardner, Tal Schuster, Ashwin Kalyan, Alex Polozov, and
andSameerSingh.2022. Impactofpretrainingterm AdamTaumanKalai.2021. Programmingpuzzles.
frequencies on few-shot numerical reasoning. In InThirty-fifthConferenceonNeuralInformationPro-
FindingsoftheAssociationforComputationalLin- cessingSystems(NeurIPS)DatasetsandBenchmarks
guistics: EMNLP2022,pages840–854. Track.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Rico Sennrich, Barry Haddow, and Alexandra Birch.
Sun. 2015. Faster r-cnn: Towards real-time object 2016. Neuralmachinetranslationofrarewordswith
detectionwithregionproposalnetworks. Advances subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin- of the North American Chapter of the Association
guistics(ACL),pages1715–1725. for Computational Linguistics: Human Language
Technologies(NAACL-HIT),pages644–656.
MinjoonSeo,HannanehHajishirzi,AliFarhadi,Oren
Etzioni,andClintMalcolm.2015. Solvinggeometry
ShounaakUghadeandSatishKumbhar.2019. Survey
problems: Combining text and diagram interpreta-
onmathematicalwordproblemsolvingusingnatu-
tion. InProceedingsofEmpiricalMethodsinNatural rallanguageprocessing. In20191stInternational
LanguageProcessing(EMNLP),pages1466–1476.
ConferenceonInnovationsinInformationandCom-
municationTechnology(ICIICT),pages1–5.IEEE.
JianhaoShen,YichunYin,LinLi,LifengShang,Xin
Jiang,MingZhang,andQunLiu.2021. Generate&
ShyamUpadhyayandMing-WeiChang.2015. Draw:
rank: Amulti-taskframeworkformathwordprob-
Achallenginganddiversealgebrawordproblemset.
lems. InFindingsoftheAssociationforComputa-
Technicalreport,Citeseer.
tionalLinguistics(EMNLP),pages2269–2279.
YibinShenandCheqingJin.2020. Solvingmathword Shyam Upadhyay and Ming-Wei Chang. 2017. An-
problemswithmulti-encodersandmulti-decoders. In notatingderivations: Anewevaluationstrategyand
Proceedingsofthe28thInternationalConferenceon datasetforalgebrawordproblems. InProceedings
ComputationalLinguistics(COLING),pages2924– ofthe15thConferenceoftheEuropeanChapterof
2934. theAssociationforComputationalLinguistics(ACL),
pages494–504.
ShumingShi,YuehuiWang,Chin-YewLin,Xiaojiang
Liu, and Yong Rui. 2015. Automatically solving Josef Urban. 2006. Mptp 0.2: Design, implementa-
numberwordproblemsbysemanticparsingandrea- tion,andinitialexperiments. JournalofAutomated
soning. In Proceedings of the 2015 conference on Reasoning,37(1):21–43.
empirical methods in natural language processing
(EMNLP),pages1132–1142. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
KaitaoSong, XuTan, TaoQin, JianfengLu, andTie-
Kaiser,andIlliaPolosukhin.2017. Attentionisall
YanLiu.2019. Mass: Maskedsequencetosequence
youneed. InAdvancesinNeuralInformationPro-
pre-trainingforlanguagegeneration. In36thInter-
cessingSystems(NeurIPS),pages5998–6008.
nationalConferenceonMachineLearning(ICML).
EricWallace,YizhongWang,SujianLi,SameerSingh,
KaiSun,DianYu,JianshuChen,DongYu,YejinChoi,
andMattGardner.2019. Donlpmodelsknownum-
andClaireCardie.2019. Dream: Achallengedata
bers? probingnumeracyinembeddings. InProceed-
setandmodelsfordialogue-basedreadingcompre-
ingsofthe2019ConferenceonEmpiricalMethods
hension. TransactionsoftheAssociationforCompu-
inNaturalLanguageProcessingandthe9thInter-
tationalLinguistics(TACL),7:217–231.
nationalJointConferenceonNaturalLanguagePro-
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. cessing(EMNLP-IJCNLP),pages5307–5315.
Sequencetosequencelearningwithneuralnetworks.
Advancesinneuralinformationprocessingsystems Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang,
(NeurIPS),27. andXiaojiangLiu.2018a. Translatingamathword
problemtoaexpressiontree. InProceedingsofthe
Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau 2018ConferenceonEmpiricalMethodsinNatural
Yih,andAshishSabharwal.2019. Quarel: Adataset LanguageProcessing(EMNLP),pages1064–1069.
andmodelsforansweringquestionsaboutqualitative
relationships. InProceedingsoftheAAAIConference Lei Wang, Dongxiang Zhang, Lianli Gao, Jingkuan
onArtificialIntelligence(AAAI),pages7063–7071. Song,LongGuo,andHengTaoShen.2018b. Math-
dqn: Solvingarithmeticwordproblemsviadeepre-
KaiShengTai,RichardSocher,andChristopherDMan-
inforcement learning. In Proceedings of the AAAI
ning.2015. Improvedsemanticrepresentationsfrom
ConferenceonArtificialIntelligence(AAAI).
tree-structuredlongshort-termmemorynetworks. In
Proceedingsofthe53rdAnnualMeetingoftheAs-
LeiWang,DongxiangZhang,JipengZhang,XingXu,
sociationforComputationalLinguisticsandthe7th
LianliGao,BingTianDai,andHengTaoShen.2019.
InternationalJointConferenceonNaturalLanguage
Template-basedmathwordproblemsolverswithre-
Processing(ACL),pages1556–1566.
cursiveneuralnetworks. InProceedingsoftheAAAI
ConferenceonArtificialIntelligence(AAAI),pages
AvijitThawani,JayPujara,andAshwinKalyan.2022.
Estimatingnumberswithoutregression. In36thCon- 7144–7151.
ferenceonNeuralInformationProcessingSystems
(NeurIPS2022)WorkshoponMATH-AI. XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,
Ed Chi, and Denny Zhou. 2023. Self-consistency
AvijitThawani,JayPujara,PedroASzekely,andFilip improves chain of thought reasoning in language
Ilievski.2021. Representingnumbersinnlp:asurvey models. In International Conference on Learning
andavision. InProceedingsofthe2021Conference Representations(ICLR).
Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Linguisticsandthe11thInternationalJointConfer-
Deep neural solver for math word problems. In enceonNaturalLanguageProcessing(ACL),pages
Proceedings of the 2017 Conference on Empirical 5859–5869.
MethodsinNaturalLanguageProcessing(EMNLP),
XingjiaoWu,LuweiXiao,YixuanSun,JunhangZhang,
pages845–854.
Tianlong Ma, and Liang He. 2022a. A survey of
JasonWei,XuezhiWang,DaleSchuurmans,Maarten human-in-the-loop for machine learning. Future
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. GenerationComputerSystems.
Chainofthoughtpromptingelicitsreasoninginlarge
YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,
languagemodels. AdvancesinNeuralInformation
MohammadNorouzi,WolfgangMacherey,Maxim
ProcessingSystems(NeurIPS).
Krikun,YuanCao,QinGao,KlausMacherey,etal.
SeanWelleck,JiachengLiu,RonanLeBras,Hannaneh 2016. Google’sneuralmachinetranslationsystem:
Hajishirzi, YejinChoi, andKyunghyunCho.2021. Bridgingthegapbetweenhumanandmachinetrans-
Naturalproofs: Mathematicaltheoremprovinginnat- lation. arXivpreprintarXiv:1609.08144.
urallanguage. InThirty-fifthConferenceonNeural
YuhuaiWu,AlbertJiang,JimmyBa,andRogerBaker
InformationProcessingSystems(NeurIPS)Datasets
Grosse. 2021c. Int: An inequality benchmark for
andBenchmarksTrack.
evaluatinggeneralizationintheoremproving. InIn-
ternationalConferenceonLearningRepresentations
Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh
Hajishirzi, and Yejin Choi. 2022a. Naturalprover:
(ICLR).
Groundedmathematicalproofgenerationwithlan-
Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li,
guagemodels. InAdvancesinNeuralInformation
MarkusNormanRabe,CharlesEStaats,MatejaJam-
ProcessingSystems(NeurIPS).
nik,andChristianSzegedy.2022b. Autoformaliza-
tion with large language models. In Advances in
SeanWelleck,XimingLu,PeterWest,FaezeBrahman,
NeuralInformationProcessingSystems(NeurIPS).
Tianxiao Shen, Daniel Khashabi, and Yejin Choi.
2023. Generating sequences by learning to self-
YuhuaiWu,FelixLi,andPercyLiang.2022c. Insights
correct. In International Conference on Learning
intopre-trainingviasimplersynthetictasks. arXiv
Representations(ICLR).
preprintarXiv:2206.10139.
Sean Welleck, Peter West, Jize Cao, and Yejin Choi. Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba,
2022b. Symbolicbrittlenessinsequencemodels: on Roger B Grosse, and Christian Szegedy. 2021d.
systematicgeneralizationinsymbolicmathematics. Lime: Learninginductivebiasforprimitivesofmath-
InAAAI. ematical reasoning. In International Conference
onMachineLearning(ICML),pages11251–11262.
WuWen-Tsun.1986. Basicprinciplesofmechanical
PMLR.
theoremprovinginelementarygeometries. Journal
ofautomatedReasoning,2(3):221–252. Zhipeng Xie and Shichao Sun. 2019. A goal-driven
tree-structuredneuralmodelformathwordproblems.
DanielWhalen.2016. Holophrasm: aneuralautomated
InInternationalJointConferenceonArtificialIntelli-
theoremproverforhigher-orderlogic. arXivpreprint
gence(IJCAI),pages5299–5305.
arXiv:1608.02644.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
SanghyunWoo,JongchanPark,Joon-YoungLee,and AaronCourville,RuslanSalakhudinov,RichZemel,
In So Kweon. 2018. Cbam: Convolutional block and Yoshua Bengio. 2015. Show, attend and tell:
attentionmodule. InProceedingsoftheEuropean Neural image caption generation with visual atten-
conferenceoncomputervision(ECCV),pages3–19. tion. InInternationalconferenceonmachinelearn-
ing(ICML),pages2048–2057.PMLR.
Qinzhuo Wu, Qi Zhang, Jinlan Fu, and Xuan-Jing
Huang.2020. Aknowledge-awaresequence-to-tree KaiyuYangandJiaDeng.2019. Learningtoprovethe-
networkformathwordproblemsolving. InProceed- oremsviainteractingwithproofassistants. InInter-
ingsofthe2020ConferenceonEmpiricalMethods nationalConferenceonMachineLearning(ICML),
in Natural Language Processing (EMNLP), pages pages6984–6994.PMLR.
7137–7146.
Zheng Ye, Shang-Ching Chou, and Xiao-Shan Gao.
QinzhuoWu,QiZhang,andZhongyuWei.2021a. An 2008. Anintroductiontojavageometryexpert. In
edge-enhancedhierarchicalgraph-to-treenetworkfor Internationalworkshoponautomateddeductionin
mathwordproblemsolving. InFindingsoftheAs- geometry,pages189–195.Springer.
sociationforComputationalLinguistics(EMNLP),
pages1473–1482. WeiYu,MengzhuWang,XiaodongWang,XunZhou,
YongfuZha,YongjianZhang,ShuyuMiao,andJing-
QinzhuoWu,QiZhang,ZhongyuWei,andXuan-Jing dongLiu.2021a. Geore:Arelationextractiondataset
Huang.2021b. Mathwordproblemsolvingwithex- forchinesegeometryproblems. In35thConference
plicitnumericalvalues. InProceedingsofthe59th onNeuralInformationProcessingSystems(NeurIPS)
AnnualMeetingoftheAssociationforComputational WorkshoponMathAIforEducation(MATHAI4ED).
WeijiangYu,YingpengWen,FudanZheng,andNong Xikun Zhang, Deepak Ramachandran, Ian Tenney,
Xiao.2021b. Improvingmathwordproblemswith Yanai Elazar, and Dan Roth. 2020e. Do language
pre-trainedknowledgeandhierarchicalreasoning. In embeddingscapturescales? InProceedingsofthe
Proceedings of the 2021 Conference on Empirical ThirdBlackboxNLPWorkshoponAnalyzingandIn-
MethodsinNaturalLanguageProcessing(EMNLP), terpretingNeuralNetworksforNLP,pages292–299.
pages3384–3394.
YizheZhang,SiqiSun,MichelGalley,Yen-ChunChen,
ChrisBrockett,XiangGao,JianfengGao,Jingjing
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Liu, and Bill Dolan. 2020f. Dialogpt: Large-scale
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
generativepre-trainingforconversationalresponse
Michael Zeng, and Meng Jiang. 2023. Generate
generation. InProceedingsofthe58thAnnualMeet-
rather than retrieve: Large language models are
ingoftheAssociationforComputationalLinguistics:
strongcontextgenerators. InInternationalConfer-
enceonLearningRepresentations(ICLR).
SystemDemonstrations.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Klim Zaporojets, Giannis Bekoulis, Johannes Deleu,
Smola.2023. Automaticchainofthoughtprompting
ThomasDemeester,andChrisDevelder.2021. Solv-
inlargelanguagemodels. InInternationalConfer-
ingarithmeticwordproblemsbyscoringequations
enceonLearningRepresentations(ICLR).
withrecursiveneuralnetworks. ExpertSystemswith
Applications,174:114704.
WeiZhao,MingyueShang,YangLiu,LiangWang,and
Jingming Liu. 2020. Ape210k: A large-scale and
DongxiangZhang,LeiWang,LumingZhang,BingTian template-richdatasetofmathwordproblems. arXiv
Dai,andHengTaoShen.2019. Thegapofsemantic preprintarXiv:2009.11506.
parsing: Asurveyonautomaticmathwordproblem
solvers. IEEEtransactionsonpatternanalysisand YilunZhao,YunxiangLi,ChenyingLi,andRuiZhang.
machineintelligence,42(9):2287–2305. 2022. Multihiertt: Numericalreasoningovermulti
hierarchicaltabularandtextualdata. InProceedings
Jipeng Zhang, Roy Ka-Wei Lee, Ee-Peng Lim, Wei of the 60th Annual Meeting of the Association for
Qin, Lei Wang, Jie Shao, and Qianru Sun. 2020a. ComputationalLinguistics(ACL),pages6588–6600.
Teacher-studentnetworkswithmultipledecodersfor
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
solvingmathwordproblem. InInternationalJoint
Sameer Singh. 2021. Calibrate before use: Im-
ConferenceonArtificialIntelligence(IJCAI).
provingfew-shotperformanceoflanguagemodels.
In International Conference on Machine Learning
JipengZhang,LeiWang,RoyKa-WeiLee,YiBin,Yan
(ICML),pages12697–12706.PMLR.
Wang, Jie Shao, and Ee-Peng Lim. 2020b. Graph-
to-treelearningforsolvingmathwordproblems. In
KunhaoZheng,JesseMichaelHan,andStanislasPolu.
Proceedingsofthe58thAnnualMeetingoftheAsso-
2022. Minif2f: a cross-system benchmark for for-
ciationforComputationalLinguistics(ACL),pages
mal olympiad-level mathematics. In International
3928–3937.
ConferenceonLearningRepresentations(ICLR).
Ming-LiangZhang,FeiYin,Yi-HanHao,andCheng- BenZhou,DanielKhashabi,QiangNing,andDanRoth.
LinLiu.2022. Learningtounderstandplanegeom- 2019. "Goingonavacation"takeslongerthan"Go-
etrydiagram. In36thConferenceonNeuralInfor- ing for a walk": A Study of Temporal Common-
mationProcessingSystems(NeurIPS)Workshopon senseUnderstanding. InProc.oftheConferenceon
MATH-AI. EmpiricalMethodsinNaturalLanguageProcessing
(EMNLP).
QiyuanZhang,LeiWang,SichengYu,ShuohangWang,
Yang Wang, Jing Jiang, and Ee-Peng Lim. 2021. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Noahqa: Numerical reasoning with interpretable Nathan Scales, Xuezhi Wang, Dale Schuurmans,
graphquestionansweringdataset. InFindingsofthe OlivierBousquet,QuocLe,andEdChi.2023. Least-
AssociationforComputationalLinguistics(EMNLP), to-most prompting enables complex reasoning in
pages4147–4161. largelanguagemodels. InInternationalConference
onLearningRepresentations(ICLR).
WenheZhang,ChiZhang,YixinZhu,andSong-Chun
FengbinZhu,WenqiangLei,YouchengHuang,Chao
Zhu. 2020c. Machine number sense: A dataset of
Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and
visualarithmeticproblemsforabstractandrelational
Tat-Seng Chua. 2021. Tat-qa: A question answer-
reasoning. InProceedingsoftheAAAIConference
ing benchmark on a hybrid of tabular and textual
onArtificialIntelligence(AAAI),pages1332–1340.
content in finance. In Proceedings of the 59th An-
nualMeetingoftheAssociationforComputational
Xikun Zhang, Deepak Ramachandran, Ian Tenney,
Linguisticsandthe11thInternationalJointConfer-
Yanai Elazar, and Dan Roth. 2020d. Do language
enceonNaturalLanguageProcessing(ACL-JCNLP),
embeddingscapturescales? InProceedingsofthe
pages3277–3287.
ThirdBlackboxNLPWorkshoponAnalyzingandIn-
terpretingNeuralNetworksforNLP,pages292–299.
70 solve. SVAMP(Pateletal.,2021)isabenchmark
60 thatteststherobustnessofdeeplearningmodelsto
50 mathwordproblemswithsimplevariations. More
40 recently built datasets involve modalities beyond
30 text. Forexample,IconQA(Luetal.,2021b)pro-
20 videsanabstractdiagramasavisualcontext,while
TabMWP(Luetal.,2022b)providesatabularcon-
10
textforeachproblem.
0
20132014 20152016 20172018 20192020 20212022
MostMWPdatasetsprovideannotatedequations
Year
Figure3:Estimatedcountsofannuallypublishedpapers as a rationale for the solution (e.g., Table 1). To
ondeeplearningformathematicalreasoning. Thisfield improve the performance and interpretability of
hasbeenexperiencingrapidgrowthsince2018. thelearnedsolvers,MathQA(Tafjordetal.,2019)
isannotatedwithpreciseoperationprograms,and
A MathematicalReasoningDatasets
MathQA-Python(Austinetal.,2021)isprovided
with specific Python programs instead. Another
Inthissection,wewillexaminethevariousdatasets
lineofdatasetsannotatestheproblemswithmulti-
currently available for the study of mathematical
step natural language solutions that are regarded
reasoning using deep learning methods. A sum-
asmorehuman-readable(Lingetal.,2017;Cobbe
mary of the commonly used datasets in this field
etal.,2021;Luetal.,2022b). Lila(Mishraetal.,
canbefoundinTable7.
2022a)annotatesmanyofthepreviouslymentioned
MWPdatasetswithPythonprogramrationales.
A.1 MathWordProblemSolving
Developing algorithms to solve math word prob-
A.2 TheoremProving
lems (MWPs) automatically has been an interest
ofNLPresearchersfordecades(Feigenbaumetal., Recently,therehasbeenincreasedinterestinusing
1963;Bobrow,1964). Amathwordproblem(also language models for theorem proving in formal
termed an algebraic or arithmetic word problem) interactive theorem provers (ITP) (e.g., Polu and
describesabriefnarrativethatinvolvescharacters, Sutskever (2020); Han et al. (2022); Polu et al.
entities, and quantities. The mathematical rela- (2023);Jiangetal.(2022b,a);Lampleetal.(2022)).
tionshipofanMWPcanbemodeledwithasetof ExampleITPsincludeLean(Mouraetal.,2015),
equationswhosesolutionrevealsthefinalanswer Isabelle(Paulson,1994),Coq(Barrasetal.,1999),
tothequestion. AtypicalexampleisshowninTa- and Metamath (Megill and Wheeler, 2019). To
ble1. Aquestioninvolvesthefourbasicarithmetic proveatheoreminanITP,thetheoremisstatedin
operationsofaddition,subtraction,multiplication, theITP’sprogramminglanguage,thensimplified
anddivisionwithsingleormultipleoperationsteps. by generating “proof steps” until it is reduced to
ThechallengeofMWPsforNLPsystemsliesinthe knownfacts. Theresultisasequenceofstepsthat
needforlanguagecomprehension,semanticpars- constitutesaverifiedproof.
ing,andmultiplemathematicalreasoningskills. DatasourcesforneuraltheoremprovinginITPs
ExistingMWPdatasetscovergradeschoolprob- include interactive learning environments that in-
lems,whicharecrawledfromonlinelearningweb- terfacewithITPs,anddatasetsderivedfromproofs
sites (Koncel-Kedziorski et al., 2015), collected inITPlibraries. Forexample,CoqGym(Yangand
fromtextbooks,ormanuallyannotatedbyhuman Deng,2019)providesaninteractiveenvironment
workers(Pateletal.,2021). Earlymathwordprob- and71Khuman-writtenproofsfortheCoqITP.For
lem datasets are relatively small or limited to a Isabelle,PISA(Jiangetal.,2021)enablesinterac-
small number of operation steps (Hosseini et al., tionandprovidesadatasetof183kproofsmined
2014; Kushman et al., 2014; Roy et al., 2015). fromtheIsabellestandardlibraryandArchiveof
Some recently curated datasets aim to increase Formal Proofs. For Lean, LeanStep (Han et al.,
problem diversity and difficulty levels. For ex- 2022)providesadatasetofproof-stepsfromLean’s
ample, Ape210K (Zhao et al., 2020) consists of mathematical library along with auxiliary tasks,
210kelementarymathwordproblems,whichisthe whileLean-Gym(Poluetal.,2023)providesanin-
largestpubliclyavailable. TheproblemsinGSM8K teractiveREPL.TheminiF2F(Zhengetal.,2022)
(Cobbe et al., 2021) can involve up to 8 steps to benchmark aims to provide a shared benchmark
srepaP
acrossITPs,consistingof488problemstatements tion,performsymbolicabstraction,utilizetheorem
sourcedfrommathematicalcompetitions. knowledge,andconductquantitativereasoning.
Otherresourcesprovideproxyenvironmentsor Some early datasets are proposed to facilitate
tasks. For example, INT (Wu et al., 2021c) pro- research in this domain (Seo et al., 2015; Alvin
vide a synthetic proving environment to measure etal.,2017;Sachanetal.,2017;SachanandXing,
sixdifferenttypesofgeneralization. Lietal. con- 2017). However,thesedatasetsarerelativelysmall
structIsarStepusingtheIsabelleArchiveofFormal or not publicly available, which limits the devel-
Proofs,andproposeataskoffillinginamissingin- opmentofdeeplearningmethods. Inresponseto
termediateproposition. Earlyapplicationsofdeep this limitation, Lu et al. create the Geometry3K
learning for formal theorem proving focus on se- dataset,whichconsistsof3,002multi-choicegeom-
lectingrelevantpremises(Alemietal.,2016). etryproblemswithunifiedlogicformannotations
Informaltheoremprovingpresentsanalternative for the multimodal inputs. More recently, larger-
mediumfortheoremproving,inwhichstatements scaledatasetssuchasGeoQA(Chenetal.,2021a),
andproofsarewritteninthemixtureofnaturallan- GeoQA+(CaoandXiao,2022),andUniGeo(Chen
guageandsymbolsusedin“standard”mathematics etal.,2022a)havebeenintroducedandareanno-
(e.g.,inLATEX),andarecheckedforcorrectnessby tatedwithprogramsthatcanbelearnedbyneural
humans. Earlyworkfocusesonselectingrelevant solversandexecutedtoobtainthefinalanswers.
premises(FerreiraandFreitas,2020b,a). Welleck
etal.(2021)developNaturalProofs,alarge-scale A.4 MathQuestionAnswering
dataset of 32k informal mathematical theorems,
Numericalreasoningisacoreabilitywithinhuman
definitions,andproofs,andprovideabenchmark
intelligence and plays an important role in many
forpremiseselectionviaretrievalandgeneration
NLPtasks. Asidefromtheoremprovingandgrade-
tasks. Wellecketal.(2022a)adaptNaturalProofs
levelmathwordproblemsolving,thereisawide
forfullproofgeneration,andprovideahumaneval-
range of question answering (QA) benchmarks
uationprotocolandproxyautomaticmetrics.
thatcenteraroundmathematicalreasoning. Inthis
Anemergingareaofresearchaimstocombine
work,werefertothesetasksasmathquestionan-
elementsofinformalandformaltheoremproving.
swering (MathQA). A large number of datasets
For example, Wu et al. (2022b) explore translat-
havebeenpresentedrecently. Forexample,QuaRel
ing informal statements into formal statements,
(Tafjord et al., 2019) is a dataset of diverse story
while Jiang et al. (2022a) release a new version
questions that involve 19 different types of quan-
oftheminiF2Fbenchmarkaugmentedwithinfor-
tities. McTaco (Zhou et al., 2019)studies tempo-
mal statements and proofs, which we refer to as
ralcommonsenseproblems,whileFermi(Kalyan
miniF2F+informal. Jiang et al. (2022a) explore
etal.,2021)studiesFermiproblemswhoseanswers
translatingprovided(orgenerated)informalproofs
canonlybeapproximatelyestimated.
intoformalproofs.
Recentstudieshaveshownthatstate-of-the-art
mathematicalreasoningsystemsmightsufferfrom
A.3 GeometryProblemSolving
brittlenessinreasoning,inthatthemodelsrelyon
Automatedgeometryproblemsolving(GPS)isalso spurioussignalsandplug-and-chugcalculationsin
along-standingAItaskinmathematicalreasoning the specific dataset to achieve “satisfactory” per-
research(Gelernteretal.,1960;Wen-Tsun,1986; formance(Hendrycksetal.,2021b;Mishraetal.,
Chouetal.,1996;Yeetal.,2008)andhasattracted 2022b). To address this issue, new benchmarks
much attention in recent years. Different from a areproposedfromvariousaspects. TheMathemat-
mathwordproblem,ageometryproblemconsists ics dataset (Saxton et al., 2020) consists of many
ofatextualdescriptioninnaturallanguageanda different types of mathematics problems, cover-
geometricdiagram. AsshowninFigure2,themul- ing arithmetic, algebra, probability, and calculus.
timodalinputsdescribetheentities,attributes,and Thedatasetallowsformeasuringthealgebraicgen-
relationshipsofgeometricelements,andthegoal eralization ability of a model. Similarly, MATH
istofindthenumericsolutiontoanunknownvari- (Hendrycksetal.,2021b)consistsofchallenging
able. GPSisachallengingtaskfordeeplearning competitionmathematicstomeasuretheproblem-
methods due to the complex skills it requires. It solvingabilityofmodelsincomplexscenarios.
involves the ability to parse multimodal informa- Someworkincorporatestabularcontextsinthe
questioninputs. Forexample,FinQA(Chenetal.,
2021c),TAT-QA(Zhuetal.,2021),andMultiHiertt
(Zhao et al., 2022) collect questions that require
bothtableunderstandingandnumericreasoningto
answer. Others,instead,presentlarge-scaleunified
benchmarks for mathematical reasoning (Mishra
et al., 2022b,a; Chen et al., 2023). NumGLUE
(Mishra et al., 2022b) is a multi-task benchmark
withthegoalofevaluatingtheperformanceofmod-
els on eight different tasks. Mishra et al. 2022a
pushthisdirectionfurtherandpresentsLila,which
consistsof23mathematicalreasoningtasks,span-
ningawiderangeofmathematicstopics,linguis-
ticcomplexity,questionformats,andbackground
knowledgerequirements.
A.5 OtherQuantitativeProblems
Numbersareanintegralpartofourdailylives,and
we humans reason with numbers in a variety of
tasks, such as understanding news, reports, elec-
tions,andmarkets. Thishasledmanyinthecom-
munitytoquestionwhetherAIsystemscaneffec-
tivelyperformquantitativereasoningineveryday
scenarios. To this end, various benchmarks have
been developed to evaluate the capabilities of AI
systemsinthisarea.
Diagrams,suchasfigures,charts,andplots,are
essentialmediathatconveylargeamountsofinfor-
mationinaconciseway. FigureQA(Kahouetal.,
2018), DVQA (Kafle et al., 2018), MNS (Zhang
et al., 2020c), PGDP5K (Hao et al., 2022), and
GeoRE (Yu et al., 2021a), are released to investi-
gatemodels’abilitiestoreasonaboutquantitative
relationshipsamongentitiesgroundedindiagrams.
NumerSense(Linetal.,2020),instead,examines
whetherandtowhatextentexistingpre-trainedlan-
guagemodelscaninducenumericalcommonsense
knowledge. EQUATE (Ravichander et al., 2019)
formalizes aspects of quantitative reasoning in a
natural language inference framework. Quantita-
tive reasoning can appear frequently in specific
domains like finance, science, and programming.
Forinstance,theConvFinQA(Chenetal.,2022c)
targetsnumericalreasoningoverfinancialreports
inaconversationalquestionansweringformat. Sci-
enceQA(Luetal.,2022a)involvesnumericalrea-
soning in scientific domains, while P3 (Schuster
etal.,2021)studiesthefunctioninferenceability
ofdeeplearningmodelstofindavalidinputwhich
makesthegivenprogramreturnTrue.
Dataset Task Size Input Output Rationale Domain
Verb395(2014) MWP 395 Question Number Equation Math
Alg514(2014) MWP 514 Question Number Equation Math
IL(2015) MWP - Question Number Equation Math
SingleEQ(2015) MWP 508 Question Number Equation Math
DRAW(2015) MWP 1,000 Question Number Equation Math
Dolphin1878(2015) MWP 1,878 Question Number Equation Math
Dolphin18K(2016) MWP 18,460 Question Number Equation Math
MAWPS(2016) MWP 3,320 Question Number Equation Math
AllArith(2017) MWP 831 Question Number Equation Math
DRAW-1K(2017) MWP 1,000 Question Number Equation Math
Math23K(2017) MWP 23,162 Question Number Equation Math
AQuA(2017) MWP 100,000 Question Option Naturallanguage Math
Aggregate(2018) MWP 1,492 Question Number Equation Math
MathQA(2019) MWP 37,297 Question Number Program Math
ASDiv(2020) MWP 2,305 Question Number Equation Math
HMWP(2020) MWP 5,470 Question Number Equation Math
Ape210K(2020) MWP 210,488 Question Number Equation Math
SVAMP(2021) MWP 1,000 Question Number Equation Math
GSM8K(2021) MWP 8,792 Question Number Naturallanguage Math
IconQA(2021b) MWP 107,439 Figure+Question Option+Textspan ✗ Math
MathQA-Python(2021) MWP 23,914 Question Number Pythonprogram Math
ArMATH(2022) MWP 6,000 Question Number Equation Math
TabMWP(2022b) MWP 38,431 Table+Question Option+Number Naturallanguage Math
MML(2015) TP 57,882 Statement Proofsteps ✗ Math
HolStep(2017) TP 2,209,076 Statement Proofsteps ✗ Math
Gamepad(2019) TP - Statement Proofsteps ✗ Math
CoqGym(2019) TP 71,000 Statement Proofsteps ✗ Math
HOList(2019) TP 29,462 Statement Proofsteps ✗ Math
IsarStep(2021) TP 860,000 Statement Proofsteps ✗ Math
PISA(2021) TP 183,000 Statement Proofsteps ✗ Math
INT(2021c) TP - Statement Proofsteps ✗ Math
NaturalProofs(2021) TP 32,000 Statement Proofsteps ✗ Math
NaturalProofs-Gen(2022a) TP 14,500 Statement Proofsteps ✗ Math
miniF2F(2022) TP 488 Statement Proofsteps ✗ Math
miniF2F+informal(2022a) TP 488 Statement Proofsteps ✗ Math
LeanStep(2022) TP 21,606,000 Statement Proofsteps ✗ Math
GEOS(2015) GPS 186 Figure+Question Option ✗ Geometry
GeoShader(2017) GPS 102 Figure+Question Number ✗ Geometry
GEOS++(2017) GPS 1,406 Figure+Question Number ✗ Geometry
GEOS-OS(2017) GPS 2,235 Figure+Question Option Demonstration Geometry
Geometry3K(2021a) GPS 3,002 Figure+Question Option Logicalform Geometry
GeoQA(2021a) GPS 4,998 Figure+Question Option Program Geometry
GeoQA+(2022) GPS 12,054 Figure+Question Option Program Geometry
UniGeo(2022a) GPS/TP 14,541 Figure+Question Option Program Geometry
Quarel(2019) MathQA 2,771 Question Option Logicalform Math
McTaco(2019) MathQA 13,225 Text+Question Option ✗ Time
DROP(2019) MathQA 96,567 Passage+Question Number+Textspan ✗ Math
Mathematics(2020) MathQA 2,010,000 Question Free-form Number Math
FinQA(2021c) MathQA 8,281 Text+Table+Q Number Program Finance
Fermi(2021) MathQA 11,000 Question Number Program+Fact Math
MATH(2021b) MathQA 12,500 Question Number Naturallanguage Math
TAT-QA(2021) MathQA 16,552 Text+Table+Q Number+Textspan ✗ Finance
AMPS(2021b) MathQA 5,000,000 Question - LATEX Math
MultiHiertt(2022) MathQA 10,440 Text+Table+Q Number+Textspan Expression Finance
NumGLUE(2022b) MathQA 101,835 Text+Question Number+Textspan ✗ Math
Lila(2022a) MathQA 134,000 Text+Question Free-form Pythonprogram Math
FigureQA(2018) VQA 1,000,000+ Figure+Question Binary ✗ Math
DVQA(2018) VQA 3,487,194 Figure+Question Textspan Number+Textspan Math
DREAM(2019) ConvQA 10,197 Dialog+Question Option ✗ Math
EQUATE(2019) NLI - Premise+Hypothesis Binary ✗ Math
NumerSense(2020) Filling 13,600 Maskedquestion Word ✗ Math
MNS(2020c) IQTest - Figure Number ✗ Math
P3(2021) Puzzle 397 Text Program ✗ Math
NOAHQA(2021) ConvQA 21,347 Dialog+Question Textspan Reasoninggraph Math
ConvFinQA(2022c) ConvQA 3,892 Report+Dialog+Q Number Expression Math
PGDP5K(2022) Parsing 5,000 Figure+Question Number ✗ Geometry
GeoRE(2022a) Parsing 12,901 Figure+Question Number ✗ Geometry
ScienceQA(2022a) VQA 21,208 Context+Question Option Naturallanguage Science
Table7: Asummarizationofmathematicalreasoningdatasets.
Paper Task Problem Network Encod Decod ATT Description
DNS(Wangetal.,2017) MWP Generation Seq2Seq GRU LSTM ✗ ThefirstdeepMWPsolver
AnsRat(Lingetal.,2017) MWP Generation Seq2Seq LSTM LSTM ✗ Trainedwithstagedback-propagation
Math-EN(Wangetal.,2018a) MWP Generation Seq2Seq BiLSTM LSTM ✔ AstandardSeq2Seqmodelwithattention
CASS(Huangetal.,2018) MWP Generation Seq2Seq BiGRU BiGRU ✔ CopyandalignmentwithRL
S-Aligned(ChiangandChen,2019) MWP Generation Seq2Seq BiLSTM LSTM ✔ Operatingsymbols
T-RNN(Wangetal.,2019) MWP Generation Seq2Seq BiLSTM BiLSTM ✔ Predictingatree-structuremathtemplate
GROUP-ATT(Lietal.,2019) MWP Generation Seq2Seq BiLSTM LSTM ✔ Groupattention
SMART(Hongetal.,2021b) MWP Generation Seq2Seq - - ✗ Explicitlyincorporatingvalues
SelfAtt(Robaideketal.,2018) GPS Classification Seq2Seq BiLSTM - ✔ Multi-hopself-attention
QuaSP+(Tafjordetal.,2019) MathQA Generation Seq2Seq BiLSTM LSTM ✗ Adoptingattributedgrammar
AST-Dec(Liuetal.,2019a) MWP Generation Seq2Tree BiLSTM Tree ✔ Usingprefixorderdecoding
GTS(XieandSun,2019) MWP Generation Seq2Tree BiGRU Tree ✔ Agoal-driventree-structuredapproach
KA-S2T(Wuetal.,2020) MWP Generation Seq2Tree BiLSTM Tree ✔ Aknowledge-awaremethod
TSN-MD(Zhangetal.,2020a) MWP Generation Seq2Tree BiGRU Tree ✔ Ateacher-studentnetwork
T-LSTM(Zaporojetsetal.,2021) MWP Generation Seq2Tree BiLSTM Tree ✗ Achild-sumtree-LSTMmodel
NT-LSTM(Zaporojetsetal.,2021) MWP Generation Seq2Tree BiLSTM Tree ✗ AnN-arytree-LSTMmodel
NS-Solver(Qinetal.,2021) MWP Generation Seq2Tree BiGRU Tree ✔ Aneural-symbolicsolverwithprograms
NumS2T(Wuetal.,2021b) MWP Generation Seq2Tree BiLSTM Tree ✔ Explicitlyincorporatingvalues
HMS(Linetal.,2021) MWP Generation Seq2Tree GRU Tree ✔ Aword-clause-problemencoder
LBF(Hongetal.,2021a) MWP Generation Seq2Tree BiGRU Tree ✔ Alearning-by-fixing(LBF)framework
Seq2DAG(Caoetal.,2021) MWP Generation Seq2Graph GRU Graph ✗ Adirectacyclicgraph(DAG)structure
Graph2Tree(Zhangetal.,2020b) MWP Generation Graph2Tree Graph Tree ✗ Generatingbettersolutionexpressions
Multi-E/D(ShenandJin,2020) MWP Generation Graph2Tree Graph Tree ✔ Agraphencoderandatree-baddecoder
Graph2Tree(Lietal.,2020b) MWP Generation Graph2Tree Graph Tree ✔ Agraph-to-treeneuralnetwork
EEH-G2T(Wuetal.,2021a) MWP Generation Graph2Tree Graph Tree ✗ Ahierarchicalgraph-to-treemodel
ASTactic(YangandDeng,2019) TP Generation Tree2Seq TreeLSTM GRU ✔ Generatingtacticsasprograms
MathDQN(Wangetal.,2018b) MWP Search DQN - - ✗ RLwithadeepQ-network(DQN)
DDT(MengandRumshisky,2019) MWP Generation Transformer Trm Trm ✔ ATransformer-basedmodel
DeepMath(Alemietal.,2016) TP Classification CNN CNN - ✗ Thefirstdeeplargescaletheoremprover
Holophrasm(Whalen,2016) TP Classification BiGRU BiGRU - ✗ Aneuralproverforhigher-orderlogic
CNNTP(Loosetal.,2017) TP Classification CNN CNN - ✗ ACNN-basedtheoremprover
WaveNetTP(Loosetal.,2017) TP Classification WaveNet WaveNet - ✗ AWaveNet-basedtheoremprover
DeepHOL(Bansaletal.,2019) TP Generation WaveNet WaveNet - ✗ AneuraltheoremproverwithRL
NGS(Chenetal.,2021a) GPS Generation VQA LSTM* LSTM ✔ Thefirstdeepgeometrysolver
PGDPNet(Zhangetal.,2022) Parsing Generation GNN - - ✗ AneuraldiagramparserwithGNN
Table8: Asummarizationofdeepneuralnetworkmodelsformathematicalreasoning. Encod: encoder,Decod:
decoder,ATT:Attention. LSTM*: ResNet+LSTM,Trm: Transformer

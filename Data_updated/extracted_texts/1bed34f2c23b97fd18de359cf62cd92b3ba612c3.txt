Execution-Based Evaluation for Open-Domain Code Generation
ZhiruoWang♠,ShuyanZhou♠,DanielFried♠,GrahamNeubig♠♣
♠LanguageTechnologiesInstitute,CarnegieMellonUniversity
♣InspiredCognition
{zhiruow,shuyanzh,dfried,gneubig}@cs.cmu.edu
Abstract However, most resources with execution sup-
port only apply to closed-domain code, that only
To extend the scope of coding queries to
use Python built-in functions (Chen et al., 2021;
more realistic settings, we propose ODEX,
Hendrycksetal.,2021;Austinetal.,2021;Lietal.,
the first Open-Domain EXecution-based natu-
2022;Haluptzoketal.,2022)orspecificlibrariesin
ral language (NL) to Python code generation
dataset. ODEX has 945 NL-Code pairs span- datasciencedomains(Laietal.,2022;Huangetal.,
ning 79 diverse libraries, along with 1,707 2022). Thisfocusonclosed-domainproblemsdi-
human-written test cases for execution. Our vergessubstantiallyfromnaturalopen-domainpro-
NL-CodepairsareharvestedfromStackOver- gram usage covering a diverse range of libraries
flow forums to encourage natural and practi-
andfunctionalities(Yinetal.,2018;Agasheetal.,
calcodingqueries.Moreover,ODEXsupports
2019; Wang et al., 2022). To enable execution-
four natural languages as intents, in English,
basedevaluationforcodingqueriesusinglibraries,
Spanish, Japanese, and Russian. ODEX un-
we present ODEX, an Open-Domain EXecution-
veils intriguing behavioral differences among
top-performing code language models (LM). based dataset (§2). We build ODEX by creating
While CODEX achieves better overall results, 1,707 test cases for 945 NL-Code pairs from the
CODEGEN improves effectively via scaling – CoNaLa(Yinetal.,2018)andMCoNaLa(Wang
CODEGEN 6.1B performs comparably with
etal.,2022)datasets, bothstemmingfromStack-
CODEX 12B. Both models show substantial
Overflow2 withbroadpracticalcodingqueries.
gaps between open and closed domains, but
WeanalyzeandhighlightthreeaspectsofODEX
CODEGEN gaps tend to decrease with model
size while CODEX gaps increase. We release (§3). First,ODEXhasbroaddomaincoverageof
ODEXtofacilitateresearchintoopen-domain 79libraries,with53.4%oftheproblemsemploy-
problemsforthecodegenerationcommunity.1 ing at least one library. Second, ODEX contains
queries in four different languages, with 439, 90,
164,and252samplesinEnglish,Spanish,Japanese,
1 Introduction
andRussian,asshowninFigure1. Third,ODEX
Evaluations of NL-to-code generation systems, addressesthreeuniquechallengesinopen-domain
especially for general-purpose programming lan- codeexecution: irreproducibleruns(Figure1 a ),
guages such as Python, have put an increasing
randomizedoutputs(Figure1 b ),andspecialized
emphasis on methods that execute code to verify
equivalencechecks(Figure2).
the results. The predominant approach for creat-
Weevaluatetwostate-of-the-artcodeLLMfam-
ing such test sets is to manually write test cases
ilies,CODEXandCODEGEN,onODEX(§5). Our
for canonical code solutions (Chen et al., 2021;
studyshowsthatlargermodelsizesandaugmented
Austin et al., 2021; Lai et al., 2022; Huang et al.,
training data improve execution accuracy. Mean-
2022). The correctness of model predictions is
while, we observe satisfactory multilingual capa-
thenevaluatedbyseeingifgeneratedcodepasses
bilities,despitethatneithermodelwasspecifically
the test cases (Chen et al., 2021). Compared to
designedformultilingualusage. However,wefind
execution-freemetricssuchastextmatchagainst
thatmodelsfacegreateryetvariedchallengeswith
referencesolutions,execution-basedmethodsmore
open-domainqueriescomparedtoclosed-domain
rigorouslyassessthefunctionalcorrectnessofcode
queries(§5). Specifically, CODEXachieveshigher
(Hendrycksetal.,2021;Chenetal.,2021).
1https://github.com/zorazrw/odex 2https://stackoverflow.com
3202
yaM
91
]ES.sc[
2v18401.2122:viXra
a
a
b
b
Code LM
< / >
c
c
d
d
Figure1: ExamplesintheODEXdataset. Inputsontheleftarefunction-formattedwith(1)libraryimportexpres-
sions;(2)functionsignaturesthatdeclaresthefunctionnameandinputarguments;and(3)naturallanguageintents
aspartofthedocstrings(Englishtranslationsarenotincludedintheactualnon-Englishinputsduringinference).
Gray boxes indicate places for code solutions. As shown on the right, a code LM fills out the gray boxes with
codesolutions,whicharethenexecutedontheunittestsunderneath. Notably,writingunittestsforopen-domain
queries is often more challenging: a requires simulated execution due to the difficulty of reproduction; b is
verifiedthroughapproximateequivalence. Priorworkfocusesmoreonbasicassertions,asin c and d .
overallresults,whileCODEGENpresentsbetterpa- 2.1 ResourceCollection
rameterefficiencyandmorebalancedopen-closed
We take two NL-to-code datasets, CoNaLa (Yin
domainperformanceasmodelsizescalesup. By
etal.,2018)andMCoNaLa(Wangetal.,2022),as
comparing execution-based metric with a series
sources for ODEX. We refer to them together as
ofexecution-freemetrics(§6),wefurtherconfirm
(M)CoNaLa. Their NL-Code pairs are collected
theadvantageofexecutiononallowingalternative
fromStackOverflow,whichcontainsabundantcod-
solutions, but also show the potential of lexical
ingqueriesthat(1)naturallyreflectpracticalpro-
metricstoidentifysimplebugfixes.
gramusage,and(2)coverdiversedomainsasmea-
ODEXjointlyfacilitatespracticalopen-domain
sured by libraries used. These properties align
code generation and execution-based evaluation.
wellwithourmainfocusonopen-domainqueries.
It serves as a comprehensive data benchmark for
(M)CoNaLafurtherproofsandclarifiesitsNLin-
NL-to-code systems, supporting diverse NL con-
tentsusinghumanannotatorstoensuredataquality.
texts, library usage, and evaluation methods. By
addressing the unique challenges of test creation 2.2 AnnotationStandardandProcedures
and execution, we hope to lay a foundation for
GiveneachsourceNL-Codepair,ourmainanno-
evaluatingopen-domaincodeviaexecution.
tationtaskistowritetestcasestocheckcodeexe-
cutioncorrectness,asillustratedbythefoursteps
2 TheODEXDataset
inFigure2. Aqualifiedtestcaseshouldverifythe
mainfunctionalityofthecanonicalcodesolution.
Inthissection,wedescribeourfour-stepprocess
Inthecasewhereannotatorsdonotunderstandthe
ofconstructingtheODEXdataset. Wefirstcollect
languageoftheintent,weusetranslationtoolssuch
resourcesofnatural,open-domaincodingqueries
astheGoogleTranslateAPI.3
(§2.1). Next,weestablishtheannotationstandard
and procedures for test case creation (§2.2). We
Step 1: Wrapping Snippets into Functions
thendescribetheannotatorhiringandworkingpro-
Codesolutionsin(M)CoNaLaareoftenshortsnip-
cesses(§2.3). Finally,weconductcheckstoensure
dataquality(§2.4). 3https://translate.google.com
NL Calculate sum over all rows of 2D numpy array `a` notation quality. We execute the canonical code
solutiononeachnewlycreatedtestcase. Unlessthe
Code
testcaseenablesasuccessfulpassofthesolution,
Step 1
code wrapping itshouldnotbetakenasavalidannotation.
Step 2 Step 4
library import execute
2.3 AnnotatorHiringandTaskFulfillment
As our data involves diverse functionalities from
Step 3 multiplelibraries,ourannotationtaskholdsarel-
write test case
atively high standard for annotators. A qualified
annotatorshouldbeproficientinPythonandcom-
monlibraries,andinwritingworkabletestcases.
Figure 2: An example annotation comprising four We chose to hire undergraduate students who
steps. have strong computer science backgrounds in
Python. Ofthe20applicantswhoapplied,wefirst
pets(e.g.,x = np.zeros(5))toensuremorepre-
conductedaresumescreeningtofiltercandidates
cisematcheswithNLintents,buttobeexecutable
withsufficientprogrammingexperience. Next,we
theyoftenneedadditionalcontextsuchasvariable
gave each candidate an annotation test with five
assignments. We therefore wrap code into stan-
randomly selected NL-Code pairs. Since the test
dalone functions by specifying input and output
mirrorstheofficialannotationprocess,weprovided
argumentsascontexts. Forexample,Step1inFig-
clearinstructionsabouteachstep(asin§2.2)and
ure2identifiesvariableaasaninputargument.
codescriptsforself-verification. Candidateswere
asked to finish their tests in three calendar days.
Step 2: Specifying Library Prerequisites Due
Basedontheirtestperformance,wehiredfourcan-
totheopen-domaincoverageof(M)CoNaLa,some
didatestoofficiallyparticipateinthisjob.
codesnippetsrequireextralibraryimportstoexe-
cutecorrectly. Accordingly,oursecondstepisto 2.4 QualityCheck
specifytheprerequisitelibrariesforcodesolutions.
We put great effort into ensuring data quality
throughout the annotation process. To assist an-
Step 3: Test Case Annotation Next, we write
notatorsinmoreefficientlyandaccuratelywriting
testcasesthatcontainthreeparts: (1)input: passing
workable test cases, we require them to execute
values to input arguments, (2) output: stating ex-
each written test case using the verification code
pectedexecutionoutputs,and(3)assertion: check-
thatweprovided,andexplicitlyreportwhetherthe
ingifexecutionresultsmatchtheexpectedoutputs.
canonical code solution can successfully pass all
However, test case creation for open-domain
theannotatedtestcasesthattheycreated.
code faces three challenges. First, safe and re-
Aftertheannotation,theauthorsperformedpost-
producible execution can be hard to achieve. As
hocverificationtocheckifeachtestcasereadsrea-
inFigure1 a , itisimpracticaltosendanHTTP
sonablyandexecutescorrectly. Inourfinalrounds
requestwhenevaluatingthissample. Instead,we
of automatic quality checks, we confirm that the
usemocktosimulatetheoutput(asuccessresponse
passrateforallcanonicalcodesolutionsovertheir
status code 200). Second, some codes entail ran-
annotatedtestcasesis100%.
domness (e.g., random.randint(3,5)) and have
We collect a total of 945 samples with NLs in
no definite value. We instead make bounding as-
fourlanguages,including439samplesinEnglish,
sertions, e.g., checking that all elements are in-
90inSpanish,164inJapanese,and252inRussian.
tegers within the range of [3,5]. Third, stan-
dard equivalence checks by == may be invalid,
3 DatasetAnalysis
sincelibrary-specificobjectsoftenrequirespecial-
ized equality checks. For example, checking the We analyze ODEX from three aspects: domain
equivalence of two NumPy arrays a and b uses diversity(§3.1),samplecomplexity(§3.2),andex-
np.array_equal(a,b),whilea == bwouldcause ecutionsupport(§3.3).
executionerrors.
3.1 Diversity
Step4: SelfVerification Inthelaststep,weper- OneuniquepropertyofODEXisitsbroaddomain
formself-verificationtoefficientlyensurethean- coverage. We categorize codes that entail library
usage(bothbuilt-inandthird-party)asbeinginthe From their distributions in Figure 4, six out of
opendomainandthosewithnoneinthecloseddo- eightdatasetsfocusonthecloseddomainandmost
main. Differentlibrariesoftenservespecificfunc- examplesusezerolibraries. Suchexamplesdeviate
tionsandhaveuniquecapabilities. Forinstance,the fromrealisticprograms,whichoftenuseAPIsof
datetime library is designed to handle date/time different libraries. DS-1000 and Exe-DS feature
operations,whileotherlibrariesfocusonvarious someopen-domainproblems,buttheirlibraryus-
otherfieldssuchasdataanalysisorwebrequests. ageismorehomogeneouswithaparticularfocus
Therefore, in this work, we view the diversity in ondatasciencedomains. Moreover,DS-1000re-
librariesasarepresentationofdistinctdomains. stricts to code using libraries but only has seven
libraries. Incontrast,ODEXismore“colorful”;it
Size coverssignificantlymoreopen-domainlibraries,as
Language #UniqueLibraries
Open Closed Total wellasfrequentqueriesinthecloseddomain.
en 45 230 209 439
es 20 48 42 90 Comparison to Natural Distribution To pro-
ja 44 113 51 164 videareferenceonnaturaldomaindistribution,we
ru 35 114 138 252
approximatereal-worldusagebycountingGitHub
Total 79 505 440 945
Pythonfilesthatuseeachlibrary. AsshowninFig-
Table 1: Number of open- and closed-domain exam- ure5,ODEXpresentsabetteralignmentwiththe
ples,andnumberoflibrariesinvolvedineachlanguage. practicalscenarioconcerningtheopendomains–
itfeaturesmorediversedomainsandpreservesthe
Table 1 reports domain statistics and Figure 3
long-tailedpatterninpracticalscenarios.
showsthelibrarydistribution. ODEXcoversadi-
The full lists of libraries and their frequencies
versesetof79libraries,whichvariesperlanguage.
aboutODEX,theeightcomparisondatasets, and
Mostsamples,53.4%,useatleastonelibrary.
theanponperoxpiamndaastednnumaptyuralresettoisngacorlelecitinons§A.m1a.tplotlib
datetime urllib sys random io json 67 more
none pandas numpy re os collections matplotlib none pandas numpy re os collections matplotlib
datetime urllib sys random io json subprocess 0% datetime25% urllib sys 50%random io 7j5s%on 67 more
requests bs4 itertools operator time math 60 more
Figure 5: Approximated natural distribution based on
Figure3: ODEXlibrarydistribution.
GitHubPythonfilesintheopendomain.
25% 50% 75%
ComparisontoExistingDatasets Wecompare 3.2 Complexity
0O% DEX with25%eight other5c0%ode genera7t5i%on datasets Tomeasuredatasetcomplexity,wefirstcalculate
thatsupporttestcaseexecution: HumanEval(Chen the lengths of NL intents and code snippets. We
et al., 2021), MBPP (Austin et al., 2021), APPS tokenizeNLintentswiththespaCy4 tokenizersin
(Hendrycks et al., 2021), MTPB (Nijkamp et al., respective languages; we follow Yin and Neubig
2022),P3(Haluptzoketal.,2022),DSP(Chandel (2018)totokenizecode. Forcode,wealsoparsethe
etal.,2022),DS-1000(Laietal.,2022),andExe- AST tree using the Python standard ast library,5
DS(Huangetal.,2022). andcountthenumberofinputandoutputvariables
toquantifythecomplexityofexecutioncontexts.
none
none iterto2o5%ls random 50c%ollections hea7p5q% re math Language len(NL) len(Code) depth(AST) N iv nar N ov ua tr
en 14.36 18.49 7.02 1.13 0.21
no2n5e% math co5l0le%ctions hashlib75% re es 18.69 28.62 7.74 1.46 0.64
ja 17.24 17.70 6.77 1.40 0.41
none sklea2rn5% numpy m5a0t%plotlib pandas75% scipy 11 more ru 11.39 20.19 6.94 1.44 0.71
pandas num25p%y matplotlib 50%sklearn scipy75% pytorch 1 more Table 2: Complexity measured in the averaged num-
berofNLwords,codetokens,ASTdepth,andi/ovari-
none sklea2rn5% pandas n5u0m%py matplotlib75% scipy 23 more
ables.
none re25% math collec5t0io%ns heapq 75it%ertools 7 more
InTable2,weseethatcodeintheSpanishsetis
none pan2d5a%s re num5p0y% sklearn c7o5%llections 3 more longeronaveragethanotherlanguages. Forboth
25% 50% 75% 4https://spacy.io/
Figure4: Librarydistributionofeightotherdatasets. 5https://docs.python.org/3/library/ast.html
SPPA
3P
lavEnamuH
PSD
0001-SD
SD-exE
PPBM
BPTM
Dataset Samples Domain Executable? Avg.TestCases DataSource NL
JuICe(Agasheetal.,2019) 1,981 open (cid:55) - GitHubNotebooks en
HumanEval(Chenetal.,2021) 164 4 (cid:51) 7.7 Hand-written en
MBPP(Austinetal.,2021) 974 8 (cid:51) 3.0 Hand-written en
APPS(Hendrycksetal.,2021) 10,000 0 (cid:51) 13.2 Competitions en
DSP(Chandeletal.,2022) 1,119 16 (cid:51) 2.1 GithubNotebooks en
MTPB(Nijkampetal.,2022) 115 8 (cid:51) 5.0 Hand-written en
Exe-DS(Huangetal.,2022) 534 28 (cid:51) - GitHubNotebooks en
DS-1000(Laietal.,2022) 1,000 7 (cid:51) 1.6 StackOverflow en
CoNaLa(Yinetal.,2018) 2,879 open (cid:55) - StackOverflow en
MCoNaLa(Wangetal.,2022) 896 open (cid:55) - StackOverflow es,ja,ru
ODEX 945 79 (cid:51) 1.8 StackOverflow en,es,ja,ru
Hand-Written
Table 3: Comparing ODEX with other NL-to-code generation datasets, in terms of domain diversity (Domain),
test-case execution support (Evaluation, Avg. Test Cases), and natural language contexts (NL). Since it is hard
tocalculatetheexactnumberoflibrariesforsomeopen-domaindatasetsthatdonotspecificallyimportrequired
librariesinthecode,wemarktheirdomainsasopeninsteadofprovidingtheexactnumberofdomains.
theinputandoutputsides,codeintheEnglishset andtrainingdata. Modelsareprogressivelytrained
hasfewervariables,suggestingpotentiallysimpler on THEPILE (Gaoetal.,2020), BIGQUERY,7 and
execution environments, which could stem from BIGPYTHON datasets are denoted as NL, MULTI,
relativesimplicityofSOqueriesaskedinEnglish. andMONO. ThemostpowerfulCODEGEN-16.1B-
MONO, performs similarly to CODE-CUSHMAN-
3.3 ExecutionSupport
001 ontheHumanEvalandMTPBdatasets.
We systematically compare code generation
datasets that concern execution or open-domain PromptDesign Forfaircomparison,weusethe
codeinTable3. ODEXisthefirstdatasetthatsup- same prompt for both model families. While
portsexecution-basedevaluationforopen-domain promptingwithfew-shotin-contextexamplesmay
code. WhileODEXdoesnothavethelargestnum- improve,ourexperimentsdonotalwaysfindthis
ber of test cases, we discuss in §7 how these test helpfulforbothmodels. Therefore,wereportzero-
casescanstillreliablymeasurecodecorrectness. shotresultsasbaselinesandleavefew-shotresults
to §7. Creating zero-shot prompts only requires
4 ExperimentSetup content from the test sample. Following Chen
etal.(2021),weconstructpromptsbyconcatenat-
CodeLLMshaveachievedstrongresultsonmul-
ingfunctioncontextandadocstring. Adocstring
tiplecodegenerationtasks,yettheiropen-domain
includestheNLintentandoptionalunittests(com-
proficiencyisunderstudiedduetothelimiteddo-
paredin§7). Figure6showsanexampleprompt.
mainsettingsofpastdatasets. Toexaminemodel
capabilities in the open domain, we evaluate two Function Context
top-performingmodelfamilies,CODEXandCODE-
DocString
GEN,onODEX.Weperformevaluationsusinga
promptingsetting,withoutfinetuninganymodel.
Function Context
We introduce the baseline models, the prompt
settings,andlayoutthemetricsforevaluation. Figure 6: Zero-shot prompt with one test case in doc-
string. Thegrayboxnotestheplaceforcodesolution.
The CODEX Family At the time of this work,
CODEXhadthreepubliclyavailablemodels. CODE-
Evaluation Metrics We follow Chen et al.
CUSHMAN-001 (C1) is a 12B CODEX model in
(2021)andmeasuretheexecutionaccuracyusing
Chenetal.(2021). CODE-DAVINCI-001/002(D1,
the pass@k metric, by computing the fraction of
D2)aretwo175BGPT-3models.6
problems having at least one correct prediction
withink samples. Wealsocompareitwithaseries
The CODEGEN Family CODEGEN (Nijkamp
ofexecution-freemetricslaterin§5.
et al., 2022) models are auto-regressive models
trainedonacombinationofNLandcodecorpora,
Implementation Details We follow Chen et al.
differinginmodelsizes(350M,2.7B,6.1B,16.1B)
(2021)andusenucleussampling(Holtzmanetal.,
6https://beta.openai.com/docs/
model-index-for-researchers 7https://cloud.google.com/bigquery
pass@k pass@k
Language CODEX CODEGEN
1 2 5 10 1 2 5 10
en 31.91 44.67 59.95 68.79 26.26 32.18 39.10 42.82
es 31.89 43.33 55.72 63.33 16.67 21.85 27.82 30.00
CUSHMAN-001 350M
ja 25.67 36.69 49.27 57.32 17.44 22.86 28.21 30.49
ru 40.00 53.48 66.63 73.41 25.87 31.44 37.44 40.87
en 33.62 46.65 60.18 67.43 35.24 42.87 50.68 53.99
es 36.89 49.46 61.37 68.89 26.00 33.65 41.52 45.56
DAVINCI-001 2.7B
ja 31.04 42.11 54.26 61.59 24.27 32.10 41.13 45.12
ru 43.21 57.53 70.03 76.59 39.64 48.11 57.23 61.90
en 47.15 57.61 67.87 73.12 34.49 37.91 41.18 43.05
es 47.44 57.90 66.33 71.11 28.56 32.05 35.86 37.78
DAVINCI-002 6.1B
ja 41.46 50.42 59.47 64.02 35.55 40.11 44.12 46.34
ru 51.87 63.36 73.03 78.17 44.64 47.29 49.82 51.19
Table4: ExecutionaccuracyofCODEXandCODEGEN-MONOmodels.
2019)withtop-psetto0.95andtemperaturesetto arein§B.
0.8. Wesetoutputstoamaximumof512tokens.
5.2 OpenDomainversusClosedDomain
5 ExperimentResults CODEX Results Figure 7 (left) shows pass@1
on open-domain (OD) and closed-domain (CD).
We first present the overall performance of two
AllCODEXmodelsscoremuchlowerinODthan
modelfamiliesonODEX(§5.1). Next,giventhe
inCD.Suchlargegapsholdacrossalllanguages,
uniquechallengesofopen-domaincode,westudy
rangingfrom4.34inSpanishto38.57inJapanese.
the variances between open- and closed-domain
Modelupgrades(C1 → D1 → D2)donotalways
problems(§5.2),andinindividualdomains(§5.3).
reduce the gaps. Gaps slightly shrink in Spanish,
butcontinuouslyincreaseinEnglishandJapanese.
5.1 BaselinePerformance
While D2 performs the best, it also exhibits the
CODEXResults AsinTable4,aligningtoexist-
mostseveregaps. Thesefindingssuggestthatcom-
ingworksandourintuition,largerDAVINCI175B
monpracticestoimproveLLMsmaynotaddress
models outperform the smaller CUSHMAN 12B thecomplexitiesinherentinopen-domaincodegen-
model, and the 002 version improves over 001.
erationproblems. Itishenceimperativethatmore
Thistrendholdsforalllanguagesandallsampling
advancedstrategiesareemployed.
sizes. Somewhatsurprisingly,allmodelsattainde-
centresultsonnon-Englishproblems,eventhough CODEGENResults AsshowninFigure7(right),
CODEXisnotdesignedformultilingualuse. This CODEGENalsohassubstantialgapsbetweenopen
highaccuracyonnon-Englishproblemssuggests andcloseddomains,however,smallerthanCODEX
themultilingualpotentialof CODEXmodels. gaps across all languages, by on average 6.0%
points. Asmodelsizeincreasesfrom2.7Bto6.1B,
CODEGENResults WereportresultsofMONO thegapsreducebyabout6.3pointsinEnglishand
modelsinTable4giventheirsuperiorperformance 1.7pointsinSpanish. ThisisincontrasttoCODEX,
overNLandMULTIvariants(Nijkampetal.,2022). whichwhenscalinguptoDAVINCI-002,thesegaps
Thepassrateincreasesas CODEGEN growsfrom continuetoincreaseby4.9pointsonaverage,indi-
350M to 2.7B, and continues to increase in non- catingthatscalingupCODEGENmoreeffectively
English languages when further scaling to 6.1B. catchesuponopen-domainperformance.
CODEGEN exhibits multilingual capacity, as its
5.3 DomainVariance
resultsonnon-Englishsubsetsareclosetothaton
English,andconsistentlyincreaseduringscaling. Wenowdivedeeperintotheresultswithinindivid-
Although CODEX and CODEGEN havecompa- ualdomains. WefocusontheCODE-DAVINCI-002
rableperformanceonexistingdatasetssuchasHu- modelasithasthebestperformanceacrossallmod-
manEval,ODEXeffectivelyunveilstheefficacyof els. InFigure8,weplotaccuracywithrespectto
CODEGEN on open-domain coding queries even thedomainfrequency,asapproximatedin§3.1.
withmanyfewerparameters,i.e.,CODEGEN 6.1B Execution accuracy is not low on all open do-
yieldssimilarpass1tothe176B CODEX DAVINCI- mains. Forexample,CODE-DAVINCI-002achieves
001modelonsomelanguages. Morefine-grained 50% pass@1 for several common libraries such
results (pass@k at 1 ≤ k ≤ 10) for both models as random and math. But high domain frequency
80 en / 1 80 en / id
en / n en / const
65 es / 1 en / intent
es / n es / id
ja / 1 65 es / const
50
ja / n es / intent
ru / 1 ja / id
35 p@1 p@2 p@3 p@4 p@5 p@6 p@7 p@8 p@9 p@10 ru / n 50 ja / const
ja / intent
ru / id
ru / const
35 ru / intent
p@1 p@2 p@3 p@4 p@5 p@6 p@7 p@8 p@9 p@10
en / open
60 60 en / closed
es / open
45 45
es / closed
30 30 ja / open
ja / closed
15 15
ru / open
0 0 ru / closed
cushman-001 davinci-001 davinci-002 350M 2.7B 6.1B
Figure7: CODEX(left)andCODEGEN(right)pass@1onopen-andclosed-domainproblemsineachlanguage.
Weanalyzeusing CODEX, givenitsbetterper-
formance. AsshowninFigure9,modelrankings
by execution-free metrics do not precisely cor-
relate with their rankings by execution accuracy.
Evenwhentherankingsalign,theirdifferencesare
largely not proportional. Comparing the metrics,
ChrFandMETEORhavesmallerinter-modelvari-
ances,whileBLEUandROUGEchangemoreand
correlate better with pass rates. Notably, Code-
BLEU is low in most settings and might not be
Figure 8: CODEX pass@1 for domains of varied fre-
suitableforevaluatingcodeinsnippet-style.
quencies. Domains are differently colored based on
theirfrequencyranking: the10mostfrequentdomains
Metric Correlation We next evaluate whether
inred,the10leastfrequentdomainsinblue,andother
execution-freemetricsmightbeusedtodiscrimi-
domainsinthemiddleinyellow.
natebetweenpassedandfailedsamples. Wetake
BLEUasanexamplesinceitshowssimilarrank-
does not ensure model proficiency. For example,
ingpatternstoexecution. Figure10showsnegligi-
on libraries with complex functionalities such as
blevariancesinBLEUscoresofpassedandfailed
matplotlibandtensorflow,pass@1cangobe-
groups. Theotherfourmetricsexhibitsimilarpat-
low10%. See§Cformoredomain-wiseresults.
terns,ascouldbefoundin§D.3.
6 ComparingtoExecution-FreeMetrics
EN ES
In this section, we study the alignment between
execution-basedevaluationandfiveexecution-free
metrics,identifyingadvantagesforbothtypes.
Model Ranking Using Different Metrics We
evaluate models using five execution-free met- JA RU
rics using lexical, syntax, and semantic matches:
BLEU (Papineni et al., 2002), ROUGE (Lin,
2004), METEOR (Banerjee and Lavie, 2005),
ChrF(Popovic´,2015),andCodeBLEU(Renetal.,
2020). Referto§D.1formoredescriptions.
Figure10: BLEUscoresonpassedandfailedsamples.
7 WhatAffectsModelPerformance?
Besides differences in model configurations, we
studythreefactorsthatmightaffectperformance.
Number of In-Context Examples Models
might benefit from example NL-Code pairs. We
thus explore the few-shot setting by prefixing
Figure9: CODEXmodelsevaluatedonsixmetrics. N ∈ {1,2,3} input-output pairs in prompts.
In Figure 11 (left), for CUSHMAN-001 and
DAVINCI-001, few-shot examples yield a clear
improvementoverthezero-shotsetting;butforthe
strongest DAVINCI-002,itbringsminimalgainsin
English. Seesimilarresultsinotherlanguagesin
§E.1.
cushman-001 davinci-001 zero one all
davinci-002 70
70
Figure 12: pass@1 when executing one or all test
60
60 cases.
50
50
JupyterNotebooks(Agasheetal.,2019)orStack-
40 40
Overflowposts(Yinetal.,2018;Wangetal.,2022),
30 0 1 2 3 30 en es ja ru butfacechallengesinenablingexecution(Laietal.,
2022; Chandel et al., 2022). Our ODEX dataset
Figure11:Left: CODEXpass@1(onEnglishset)using
addressesexecutionforopen-domaincode.
0/1/2/3-shot prompts. Right: DAVINCI-002 pass@1
whenaddingzero,one,oralltestcasesinprompts.
Coding Queries vs. Programming Challenges
Some works stem from coding contest web-
NumberofTestCasesintheDocstring Includ-
sites(Hendrycksetal.,2021;Lietal.,2022),but
ingtestcasesininputsaddsexecutionhintsofthe
GitHub Jupyter Notebooks (Agashe et al., 2019;
expected functionality of the solution, and hence
Huangetal.,2022)andStackOverflow(SO)(Yin
mayimproveexecutionaccuracy. Wetestthishy-
etal.,2018;Wangetal.,2022;Laietal.,2022)pro-
pothesisbyexperimentingwithpromptsthathave
videmorenaturalandpracticalcodingqueries. We
varyingnumbersoftestcases. Besidesthedefault
preserve this naturalness and incorporate various
setting with zero tests, we compare adding one
NLsettingstoassistprogrammersworldwide.
randomtestcaseandallannotatedtestcases.
Figure11(right)showsthatinjectingasfewas
Execution-basedEvaluation Evaluationbyex-
oneexemplartestcasesignificantlyimprovesthe
ecutionhaslongbeenusedforSQL(Zhongetal.,
executionaccuracy,yetaddingmorecaseshaslittle
2017) or logical forms (Dong and Lapata, 2016).
bonus. Thispotentiallyimpliesthesufficiencyof
Many datasets have begun to support Python ex-
onetestcasetoshowthemainfunctionality.
ecution via test cases, however focus on built-in
functions (Chen et al., 2021; Austin et al., 2021;
Number of Evaluation Test Cases Execution
Hendrycks et al., 2021) or specific domains (Lai
results could be more reliable if using more test
etal.,2022;Huangetal.,2022). Ourtestcases,in
casesforevaluation. However,thereisatrade-off
contrast,coverdiverselibrariesintheopendomain.
between evaluation effectiveness and annotation
efficiency,duetothehighcostofhumaneffort. To
9 Conclusion
studythistradeoff,weobservehowresultschange
withrespecttothenumberoftests. Comparedto
WepresentODEX,anopen-domaincodegenera-
using all cases in default, we also try using one
tiondatasetsupportingexecution-basedevaluation
randomlyselectedcase. Forsimplicity,wedonot
viahuman-writtentestcases. ODEXnotonlysup-
includeanytestcasesinprompts.
portsexecution-basedevaluationofcodeusingtest
AsshowninFigure12,evaluatingoveroneran-
cases,butalsoextendsthetasktotheopendomain,
domtestlargelypreservestheaccuracyofusingall
covering79diversePythonlibrariesandfournatu-
tests,indicatingthatonecaseissufficienttotestthe
rallanguages(English,Spanish,Japanese,andRus-
mainfunctionalityformostqueries. Check§Efor
sian). Comparingtwostate-of-the-artcodegenera-
analysisonotherfactorssuchasfunctionnaming.
tionmodels, CODEX and CODEGEN,ourdataset
effectivelyunveilstheirvariedbehaviorsbetween
8 RelatedWork
program domains and language contexts. ODEX
Open Domain Code Generation Programs of- servesasacomprehensiveNL-to-codebenchmark
tenuseAPIsfromdifferentPythonlibraries. Some givenitsopen-domaincoverage,multi-naturallan-
datasetspreservenaturalcoveragefrominteractive guage queries, and multi-metric support. When
bringingcodeexecutiontoopendomainscenarios, foropen-domaincodegeneration,tofurtherfacili-
ourexplorationsalsorevealemergingchallengesin tatetechnologicaladvancesinAIprogrammingas-
testcreationandreliableexecution,whichwehope sistance,meanwhilesupportingmultiplelanguages
thatourdatasetwillenablefutureworktotackle. toencourageitsuniversalaccessibility.
We strive to ensure high data quality and opti-
Acknowledgements
mize annotation efficiency. We build the ODEX
datasetwithnaturalandpracticalStackOverflowre-
Wewouldliketothankalltheannotatorsfortheir
sourcesandhireannotatorswithqualifiedprogram-
hardwork. WethankUriAlon,FrankF.Xu,Tao
mingproficiency. Weprovideourannotatorswith
Yufortheirhelpfulfeedbackonthiswork.
clearly documented instructions, flexible annota-
Limitations tioninterfaces(GoogleSheets,JupyterNotebooks),
and self-verification tools. We (authors) conduct
ODEXaimstoserveasacomprehensivetestbed,
pilotannotationtoconfirmtheclarityofannotation
by enabling execution-based evaluation of code
standardsandfeasibilityoftheannotationtask. We
intheopendomain, withflexibleintentinputsin
conduct posthoc examinations on the annotation
fournaturallanguages. However,weshouldshould
results,bothmanuallyandautomatically,toobtain
hold continuous awareness of execution security,
assureddataquality(100%passrate).
multilingualsupport,andevaluationreliability.
Werespectthecontributionandprivacyofour
First, execution supports in ODEX enables
annotators. Weoffercompetitiveremunerationfor
morerigorousevaluationsthanotherexecution-free
their annotation job and treat each one of them
methods. However,duetotheincreasedcomplex-
fairly. Allannotatorspossesstherighttowithdraw
ity of open-domain codes, more inspections are
atanytime. Wesecurethatalltheirpersonalinfor-
requiredforexecutionsafety,eitherforcodesolu-
mationisremovedbeforepublicrelease.
tionsortestcases. Weshouldalwayskeepalertto
We conduct systematic analysis from multiple
concealingmaliciouscode(Wallaceetal.,2021)or
perspectives in the paper, in an attempt to foster
generatingcodewithsecurityvulnerabilities(Verdi
publicawarenessongeneratingandevaluatingpro-
etal.,2020;Pearceetal.,2021).
grams in the open domain, both in encouraging
Second, in addition to English inputs, ODEX
moreadvancesinthisdirection,andraisingmore
alsoencouragesintentsspecifiedinthreeotherlan-
concernsabouttherobustnessandsecurityofsuch
guages. Still,itslanguagecoverageisboundedby
uniquecodingproblems.
the available forums in StackOverflow. We hope
ourinitiativecanhighlightthemultilingualnature
ofprogramdevelopers,encouragetheemergence
of similar data resources in other languages, and
continuouslypromoteAIprogrammingassistance
inlanguagesworldwide.
Third,ODEXcoverswide-rangingcodequeries
in the open domain, it is more suitable for less
resource-demandingscenariossuchasdownstream
evaluationorfew-shotlearning. AlthoughODEX
islargerthanmanypreviousdatasetswithhuman-
written test cases, it is still limited due to the in-
tensehumaneffortrequiredbythecurationprocess.
Regarding this, we encourage readers to conduct
significancetesting(Droretal.,2018)andreport
moresubstantialmodelimprovements.
EthicsStatement
OurworkhasreceivedIRBapprovalandislicensed
underaCreativeCommonsAttribution-ShareAlike
(CCBY-SA)4.0InternationalLicense. Theresult-
ingODEXdatasetisbuilttoserveasabenchmark
References Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing,TravisHoppe,CharlesFoster,JasonPhang,Ho-
Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer.
race He, Anish Thite, Noa Nabeshima, et al. 2020.
2019. Juice: A large scale distantly supervised
The pile: An 800gb dataset of diverse text for lan-
dataset for open domain context-based code gener-
guagemodeling. arXivpreprintarXiv:2101.00027.
ation. In Proceedings of the 2019 Conference on
EmpiricalMethodsinNaturalLanguageProcessing Patrick Haluptzok, Matthew Bowers, and Adam Tau-
andthe9thInternationalJointConferenceonNatu- man Kalai. 2022. Language models can teach
ralLanguageProcessing(EMNLP-IJCNLP),pages themselves to program better. arXiv preprint
5436–5446. arXiv:2207.14502.
JacobAustin,AugustusOdena,MaxwellNye,Maarten DanHendrycks,StevenBasart,SauravKadavath,Man-
Bosma, Henryk Michalewski, David Dohan, Ellen tasMazeika, AkulArora, EthanGuo, CollinBurns,
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. SamirPuranik,HoraceHe,DawnSong,etal.2021.
2021. Program synthesis with large language mod- Measuring coding challenge competence with apps.
els. arXivpreprintarXiv:2108.07732. arXivpreprintarXiv:2105.09938.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: AriHoltzman, JanBuys, LiDu, MaxwellForbes, and
An automatic metric for MT evaluation with im- Yejin Choi. 2019. The curious case of neural text
proved correlation with human judgments. In Pro- degeneration. arXivpreprintarXiv:1904.09751.
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla- Junjie Huang, Chenglong Wang, Jipeng Zhang,
tion and/or Summarization, pages 65–72, Ann Ar- Cong Yan, Haotian Cui, Jeevana Priya Inala,
bor, Michigan. Association for Computational Lin- Colin Clement, Nan Duan, and Jianfeng Gao.
guistics. 2022. Execution-based evaluation for data sci-
ence code generation models. arXiv preprint
Shubham Chandel, Colin B Clement, Guillermo Ser- arXiv:2211.09374.
rato, and Neel Sundaresan. 2022. Training and
Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
evaluatingajupyternotebookdatascienceassistant.
LukeZettlemoyer.2018. Mappinglanguagetocode
arXivpreprintarXiv:2201.12901.
in programmatic context. In Proceedings of the
2018 Conference on Empirical Methods in Natural
BeiChen,FengjiZhang,AnhNguyen,DaoguangZan,
LanguageProcessing,pages1643–1652.
ZeqiLin,Jian-GuangLou,andWeizhuChen.2022.
Codet: Codegenerationwithgeneratedtests. arXiv
YuhangLai,ChengxiLi,YimingWang,TianyiZhang,
preprintarXiv:2207.10397.
Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau
Yih, Daniel Fried, Sida Wang, and Tao Yu. 2022.
MarkChen,JerryTworek,HeewooJun,QimingYuan,
Ds-1000: A natural and reliable benchmark for
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
data science code generation. arXiv preprint
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
arXiv:2211.11501.
Brockman, et al. 2021. Evaluating large lan-
guage models trained on code. arXiv preprint
YujiaLi,DavidChoi,JunyoungChung,NateKushman,
arXiv:2107.03374.
Julian Schrittwieser, Rémi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, Agustin Dal Lago,
NaihaoDeng,ShuaichenChang,PengShi,TaoYu,and
etal.2022. Competition-levelcodegenerationwith
Rui Zhang. 2021. Prefix-to-sql: Text-to-sql genera-
alphacode. arXivpreprintarXiv:2203.07814.
tionfromincompleteuserquestions. arXivpreprint
arXiv:2109.13066.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
maticevaluationofsummaries. InTextSummariza-
Li Dong and Mirella Lapata. 2016. Language to logi-
tion Branches Out, pages 74–81, Barcelona, Spain.
calformwithneuralattention. InProceedingsofthe
AssociationforComputationalLinguistics.
54thAnnualMeetingoftheAssociationforCompu-
tationalLinguistics(Volume1: LongPapers),pages Chin-Yew Lin and Franz Josef Och. 2004. Auto-
33–43. matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
RotemDror,GiliBaumer,SegevShlomov,andRoiRe-
statistics. InProceedingsofthe42ndAnnualMeet-
ichart.2018. Thehitchhiker’sguidetotestingstatis-
ingoftheAssociationforComputationalLinguistics
ticalsignificanceinnaturallanguageprocessing. In
(ACL-04),pages605–612.
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1: StephanLukasczykandGordonFraser.2022. Pynguin:
LongPapers),pages1383–1392. Automated unit test generation for python. arXiv
preprintarXiv:2202.05218.
MikhailEvtikhiev,EgorBogomolov,YaroslavSokolov,
and Timofey Bryksin. 2022. Out of the bleu: how Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu
shouldweassessqualityofthecodegenerationmod- Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
els? arXivpreprintarXiv:2208.03133. and Caiming Xiong. 2022. A conversational
paradigm for program synthesis. arXiv preprint Victor Zhong, Caiming Xiong, and Richard Socher.
arXiv:2203.13474. 2017. Seq2sql: Generating structured queries
fromnaturallanguageusingreinforcementlearning.
KishorePapineni,SalimRoukos,ToddWard,andWei- arXivpreprintarXiv:1709.00103.
JingZhu.2002. Bleu: amethodforautomaticeval-
uationofmachinetranslation. InProceedingsofthe
40th annual meeting of the Association for Compu-
tationalLinguistics,pages311–318.
Hammond Pearce, Baleegh Ahmad, Benjamin Tan,
Brendan Dolan-Gavitt, and Ramesh Karri. 2021.
An empirical cybersecurity evaluation of github
copilot’s code contributions. arXiv preprint
arXiv:2108.09293.
Maja Popovic´. 2015. chrF: character n-gram F-score
forautomaticMTevaluation. InProceedingsofthe
Tenth Workshop on Statistical Machine Translation,
pages 392–395, Lisbon, Portugal. Association for
ComputationalLinguistics.
Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie
Liu,DuyuTang,NeelSundaresan,MingZhou,Am-
brosio Blanco, and Shuai Ma. 2020. Codebleu: a
method for automatic evaluation of code synthesis.
arXivpreprintarXiv:2009.10297.
Michele Tufano, Dawn Drain, Alexey Svyatkovskiy,
Shao Kun Deng, and Neel Sundaresan. 2020. Unit
testcasegenerationwithtransformersandfocalcon-
text. arXivpreprintarXiv:2009.05617.
MortezaVerdi,AshkanSami,JafarAkhondali,Foutse
Khomh, Gias Uddin, and Alireza Karami Motlagh.
2020. An empirical study of c++ vulnerabilities in
crowd-sourced code examples. IEEE Transactions
onSoftwareEngineering.
EricWallace,TonyZhao,ShiFeng,andSameerSingh.
2021. Concealeddatapoisoningattacksonnlpmod-
els. In Proceedings of the 2021 Conference of the
NorthAmericanChapteroftheAssociationforCom-
putationalLinguistics: HumanLanguageTechnolo-
gies,pages139–150.
Zhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F
Xu,andGrahamNeubig.2022. Mconala: Abench-
markforcodegenerationfrommultiplenaturallan-
guages. arXivpreprintarXiv:2203.08388.
Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan
Vasilescu, and Graham Neubig. 2018. Learning to
mine aligned code and natural language pairs from
stack overflow. In 2018 IEEE/ACM 15th interna-
tional conference on mining software repositories
(MSR),pages476–486.IEEE.
Pengcheng Yin and Graham Neubig. 2018. Tranx: A
transition-basedneuralabstractsyntaxparserforse-
manticparsingandcodegeneration. InProceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstra-
tions,pages7–12.
A ODEXDataset DomainStatisticsofComparisonDatasets Ta-
ble6liststhelibraryfrequencyofeightcompari-
A.1 LibraryDistributionStatistics
sondatasetmentionedin§3: HumanEval,MBPP,
Asidefromtheillustrationsin§3.1,welistoutthe APPS,MTPB,P3,DSP,DS-1000,andExe-DS.
detailedstatisticsoflibrariesinODEX,theeight
comparisondatasets,andtheapproximatednatural HumanEval
Library Count Frequency Library Count Frequency
distribution.
none 155 94.51
ODEXDomainStatistics Table5liststhenum- math 6 3.66 hashlib 1 0.61
collections 1 0.61 re 1 0.61
berandpercentageofoccurrencesforeachlibrary
MBPP
intheODEXdataset. Library Count Frequency Library Count Frequency
none 794 81.52
ODEX re 73 7.49 cmath 3 0.31
Library Count Frequency Library Count Frequency math 37 3.80 operator 3 0.31
collections 25 2.57 array 0 0.00
none 440 41.90 functools 2 0.19 heapq 16 1.64 bisect 2 0.21
pandas 81 7.71 http 2 0.19 itertools 12 1.23 copy 1 0.10
numpy 80 7.62 obspy 2 0.19 sys 7 0.72 datetime 1 0.10
re 62 5.90 pickle 2 0.19
os 42 4.00 pytz 2 0.19 APPS
collections 26 2.48 seaborn 2 0.19
Library Count Frequency
matplotlib 22 2.10 sqlalchemy 2 0.19
datetime 21 2.00 statistics 2 0.19 none 10,000 100.00
urllib 19 1.81 string 2 0.19
sys 17 1.62 xlrd 2 0.19 MTPB
random 16 1.52 IPython 1 0.10
Library Count Frequency Library Count Frequency
io 15 1.43 argparse 1 0.10
json 15 1.43 aspose 1 0.10 - 103 88.03
subprocess 13 1.24 bisect 1 0.10
pandas 3 2.56 collections 1 0.85
requests 10 0.95 cgi 1 0.10
bs4 9 0.86 configparser 1 0.10 re 3 2.56 datetime 1 0.85
itertools 9 0.86 ctypes 1 0.10 numpy 2 1.71 math 1 0.85
operator 9 0.86 dateutil 1 0.10 sklearn 2 1.71 regex 1 0.85
time 9 0.86 difflib 1 0.10 P3
math 8 0.76 docxtpl 1 0.10
builtins 6 0.57 filecmp 1 0.10 Library Count Frequency Library Count Frequency
selenium 6 0.57 ftplib 1 0.10
- 1581 92.19
tensorflow 6 0.57 hashlib 1 0.10
django 5 0.48 heapq 1 0.10 itertools 35 2.04 heapq 15 0.87
sqlite3 5 0.48 imp 1 0.10 random 31 1.81 re 15 0.87
PIL 4 0.38 inspect 1 0.10 collections 28 1.63 math 10 0.58
codecs 4 0.38 locale 1 0.10
cv2 4 0.38 lxml 1 0.10 DSP
scipy 4 0.38 mechanize 1 0.10 Library Count Frequency Library Count Frequency
sklearn 4 0.38 mpl_toolkits 1 0.10
base64 3 0.29 multidict 1 0.10 - 2034 92.79
csv 3 0.29 pprint 1 0.10
sklearn 110 5.02 collections 8 0.36
flask 3 0.29 queue 1 0.10
numpy 84 3.83 time 8 0.36
glob 3 0.29 regex 1 0.10
shutil 3 0.29 rsa 1 0.10 matplotlib 50 2.28 gzip 4 0.18
socket 3 0.29 ssl 1 0.10 pandas 46 2.10 pickle 4 0.18
struct 3 0.29 texttable 1 0.10 scipy 46 2.10 random 4 0.18
sympy 3 0.29 unicodedata 1 0.10 math 16 0.73 csv 2 0.09
xlwt 3 0.29 warnings 1 0.10 numbers 12 0.55 itertools 2 0.09
ast 2 0.19 xml 1 0.10 utils 12 0.55 seaborn 2 0.09
DS-1000
Table5: ODEXlibrarydistribution. Library Count Frequency Library Count Frequency
pandas 291 29.10 scipy 106 10.60
numpy 220 22.00 pytorch 68 6.80
matplotlib 155 15.50 tensorflow 45 4.50
sklearn 115 11.50
Exe-DS
Library Count Frequency Library Count Frequency
none 379 56.23
sklearn 75 11.13 pylab 2 0.30
pandas 58 8.61 __future__ 1 0.15
numpy 53 7.86 arch 1 0.15
matplotlib 32 4.75 cPickle 1 0.15
scipy 18 2.67 cofi 1 0.15
seaborn 15 2.23 csv 1 0.15
math 7 1.04 datetime 1 0.15
collections 4 0.59 functools 1 0.15
re 4 0.59 graphviz 1 0.15
folium 3 0.45 json 1 0.15
nltk 3 0.45 mpl_toolkits 1 0.15
statsmodels 3 0.45 operator 1 0.15
warnings 3 0.45 os 1 0.15
IPython 2 0.30 tensorflow 1 0.15
Table6:Librarystatisticsofeightcomparisondatasets.
Approximated Natural Domain Distribution andgetabetterunderstandingofthequestion. If
Toapproximatethenaturaldistributionoflibraries anyerrorsorunder-specificationarespottedinthe
intheopendomain,wecountthenumberofPython givenNLorcode,weasktheannotatorstocorrect
filesonGitHubthatimportsthelibraryofinterest. itbymakingtheminimalchangepossible.
FollowingtheGitHubsearchsyntax,8 weusethe Aligning with how programmers import a li-
query import ${library_name} to search files brary, we require the expressions be written
thatimportacertainlibrary,anduseNOT import in three forms: (1) import ${LIBRARY}, (2)
tocountfilesnotusinganylibraries. Theirfrequen- import ${LIBRARY} as ${ABBR}, or (3)
ciesareshowninTable7. from ${LIBRARY} import ${FUNCTION},where
the ${LIBRARY} can also be sub-classes such as
ApproximatedNaturalDistribution
matplotlib.pyplot.
Library Count Library Count
Weencouragetheannotatorstousethelanguage
os 30,188,921 sqlite3 694,794 identicaltothegivenNLintentwhencreatingthe
sys 24,213,844 configparser 640,014
test cases, especially if the code involves string-
numpy 20,965,506 queue 631,326
re 11,762,193 ssl 602,351 relatedoperations(e.g.,writingregularexpressions
time 5,946,718 http 597,866 inJapanese). Weencouragetheannotatorstowrite
pandas 5,878,651 xml 574,030
reasonablymoreanddiversetestcases,byvarying
random 5,740,444 seaborn 567,576
matplotlib 5,416,874 imp 560,862 thevaluesortypesofvariables.
json 4,792,536 builtins 560,148 Pleasefindthefullinstruction9 andexamples10
tensorflow 4,720,266 locale 542,607
forannotationinourcoderepository.
argparse 4,570,391 ast 444,349
subprocess 4,165,781 bisect 315,031
string 4,114,004 pytz 295,167
B BaselineResults
codecs 3,973,691 heapq 281,393
warnings 3,824,001 cgi 277,852
math 3,569,158 unicodedata 267,310 Accordingtothebaselineresultsin§5.1,wepro-
django 3,447,092 regex 235,800 vide more detailed evaluation results, on the exe-
shutil 2,999,394 difflib 225,154
cution pass rate ranging from the top-1 to top-10
requests 2,837,310 PIL 218,526
cv2 2,575,063 sklearn 208,913 model predictions. Table 8 and Table 9 show the
datetime 2,536,970 statistics 127,725 zero-shotexecutionaccuracyofCODEXandCODE-
socket 2,489,033 rsa 122,447
GENmodels,respectively.
pickle 2,419,604 lxml 111,742
io 2,190,998 dateutil 107,041
collections 2,152,651 bs4 90,224 Model NL PassRate
@1 @2 @3 @4 @5 @6 @7 @8 @9 @10
glob 2,114,567 xlrd 86,522 en 31.91 44.67 51.81 56.54 59.95 62.56 64.61 66.28 67.65 68.79
itertools 1,899,461 filecmp 79,328 C1 e jas 3 21 5. .8 69 7 4 33 6. .3 63 9 4 49 2. .2 63 6 5 43 6. .0 41 9 5 45 9. .7 22 7 5 57 1. .8 41 4 5 59 3. .5 22 3 6 50 4. .9 76 6 6 52 6. .2 12 0 6 53 7. .3 33 2
urllib 1,809,462 IPython 73,274 ru 40.00 53.48 60.04 63.96 66.63 68.62 70.17 71.44 72.50 73.41
flask 1,788,601 sympy 70,969 en 33.62 46.65 53.27 57.34 60.18 62.31 64.00 65.37 66.49 67.43
csv 1,680,232 selenium 56,709 D1 e jas 3 36 1. .8 09 4 4 49 2. .4 16 1 5 45 7. .4 84 3 5 58 1. .9 56 4 6 51 4. .3 27 6 6 53 6. .2 32 9 6 54 8. .7 18 1 6 56 9. .2 50 3 6 67 0. .5 66 7 6 68 1. .8 59 9
ru 43.21 57.53 63.93 67.58 70.03 71.85 73.29 74.51 75.60 76.59
functools 1,433,520 xlwt 55,035
en 47.15 57.61 62.58 65.69 67.87 69.47 70.70 71.67 72.46 73.12
pprint 1,378,679 ftplib 52,121 D2 e jas 4 47 1. .4 44 6 5 57 0. .9 40 2 6 52 4. .2 80 4 6 54 7. .6 55 9 6 56 9. .3 43 7 6 67 0. .6 81 4 6 68 1. .6 85 7 6 69 2. .5 73 1 7 60 3. .3 43 1 7 61 4. .1 01 2
base64 1,352,623 multidict 29,224 ru 51.87 63.36 68.25 71.09 73.03 74.5 75.67 76.64 77.46 78.17
hashlib 1,330,158 mechanize 20,978
scipy 1,121,371 obspy 5,799 Table8: CODEXzero-shotperformance.
inspect 1,112,770 texttable 4,749
operator 1,104,841 aspose 1,048
Model NL PassRate
ctypes 864,108 docxtpl 76 @1 @2 @3 @4 @5 @6 @7 @8 @9 @10
sqlalchemy 814,096 mpl_toolkits 2 en 26.26 32.18 35.46 37.59 39.10 40.22 41.08 41.78 42.35 42.82
struct 787,484 350M e jas 1 16 7. .6 47 4 2 21 2. .8 85 6 2 24 5. .7 50 1 2 26 7. .5 16 2 2 27 8. .8 22 1 2 28 8. .6 98 7 2 29 9. .2 57 2 2 29 9. .6 95 3 2 39 0. .8 29 4 3 30 0. .0 40 9
ru 25.87 31.44 34.27 36.11 37.44 38.44 39.22 39.86 40.40 40.87
en 37.74 42.58 44.92 46.36 47.36 48.11 48.70 49.18 49.57 49.89
Table7: Approximatednaturaldomaindistribution. 2.7B e jas 3 36 1. .4 84 3 4 30 5. .8 79 0 4 32 7. .8 63 4 4 34 8. .0 81 0 4 34 9. .8 54 8 4 45 0. .4 18 3 4 45 0. .9 56 6 4 46 0. .3 92 2 4 46 1. .5 26 2 4 46 1. .6 47 6
ru 45.67 49.83 52.07 53.50 54.54 55.37 56.04 56.61 57.10 57.54
en 34.49 37.91 39.55 40.52 41.18 41.69 42.11 42.47 42.78 43.05
A.2 MoreAnnotationDetails 6.1B e jas 2 38 5. .5 56 5 3 42 0. .0 15 1 3 43 2. .8 05 4 3 45 3. .0 23 5 3 45 4. .8 16 2 3 46 4. .4 78 7 3 46 5. .9 24 8 3 47 5. .2 68 9 3 47 6. .5 06 4 3 47 6. .7 38 4
ru 44.64 47.29 48.53 49.28 49.82 50.23 50.56 50.82 51.03 51.19
AlongwiththeNL-Codepair,wealsoprovideIDs
ofthesourceStackOverflowpost,usingwhichan-
Table9: CODEGENzero-shotperformance.
notatorscantracebacktotheoriginalpostwebpage 9https://anonymous.4open.science/r/odex/data/
instruction.md
8https://docs.github.com/en/search-github/ 10https://anonymous.4open.science/r/odex/data/sample_
searching-on-github/searching-code annotation.ipynb
C Domain-WiseExecutionResults NL Split PassRate
@1 @2 @3 @4 @5 @6 @7 @8 @9 @10
350M
- 26.26 32.18 35.46 37.59 39.10 40.22 41.08 41.78 42.35 42.82
en open 22.35 27.04 29.75 31.58 32.93 33.97 34.80 35.48 36.04 36.52
close 30.57 37.84 41.75 44.21 45.89 47.09 48.00 48.71 49.28 49.76
- 16.67 21.85 24.70 26.56 27.82 28.68 29.27 29.65 29.89 30.00
es open 16.04 19.58 21.23 22.12 22.59 22.82 22.90 22.92 22.92 22.92
close 17.38 24.44 28.67 31.62 33.79 35.39 36.55 37.35 37.86 38.10
- 17.44 22.86 25.51 27.12 28.21 28.97 29.52 29.93 30.24 30.49
ja open 15.40 19.67 21.67 22.91 23.78 24.41 24.88 25.23 25.49 25.66
close 21.96 29.93 34.04 36.46 38.02 39.07 39.80 40.35 40.78 41.18
- 25.87 31.44 34.27 36.11 37.44 38.44 39.22 39.86 40.40 40.87
ru open 20.53 24.89 26.83 28.12 29.08 29.81 30.38 30.84 31.23 31.58
close 30.29 36.84 40.41 42.71 44.34 45.57 46.53 47.31 47.97 48.55
Welistoutdetailedresultsforexperimentsin§5. 2.7B
- 35.24 42.87 46.75 49.11 50.68 51.78 52.59 53.19 53.64 53.99
en open 26.04 33.02 36.92 39.36 41.01 42.20 43.10 43.80 44.35 44.78
close 45.36 53.69 57.58 59.85 61.32 62.33 63.03 63.53 63.88 64.11
- 26.00 33.65 37.74 40.06 41.52 42.58 43.44 44.20 44.89 45.56
es open 22.50 27.45 30.68 32.96 34.76 36.32 37.76 39.12 40.42 41.67
close 30.00 40.74 45.81 48.17 49.25 49.74 49.94 50.00 50.00 50.00
- 24.27 32.10 36.45 39.22 41.13 42.51 43.54 44.30 44.82 45.12
ja open 18.67 23.93 26.94 28.97 30.45 31.58 32.44 33.06 33.45 33.63
close 36.67 50.20 57.52 61.93 64.78 66.73 68.14 69.19 70.00 790.59
- 39.64 48.11 52.46 55.25 57.23 58.71 59.84 60.71 61.39 61.90
ru open 27.02 34.72 38.61 41.12 42.96 44.41 45.59 46.59 47.46 48.25
close 50.07 59.18 63.90 66.93 69.03 70.53 71.61 72.38 72.90 73.19
6.1B
- 34.49 37.91 39.55 40.52 41.18 41.69 42.11 42.47 42.78 43.05
en open 28.30 31.57 33.21 34.25 35.02 35.64 36.17 36.64 37.04 37.39
close 41.29 44.89 46.53 47.42 47.97 48.35 48.64 48.88 49.09 49.28
- 28.56 32.05 33.85 35.03 35.86 36.48 36.94 37.28 37.56 37.78
C.1 OpenDomainVersusClosedDomain es open 25.83 28.61 30.16 31.25 32.06 32.64 33.02 33.24 33.33 33.33
close 31.67 35.98 38.08 39.35 40.21 40.86 41.41 41.90 42.38 42.86
- 35.55 40.11 42.04 43.25 44.12 44.77 45.28 45.69 46.04 46.34
ja open 28.76 31.96 33.36 34.23 34.83 35.28 35.62 35.89 36.11 36.28
close 50.59 58.17 61.26 63.26 64.71 65.81 66.68 67.41 68.04 68.63
- 44.64 47.29 48.53 49.28 49.82 50.23 50.56 50.82 51.03 51.19
ru open 28.33 30.14 31.16 31.92 32.53 33.04 33.45 33.78 34.04 34.21
close 58.12 61.47 62.87 63.63 64.10 64.43 64.69 64.90 65.07 65.22
Table11: CODEGENpassrateinvariousdomains.
Table10andTable11showstheexecutionaccuracy C.2 Domain-wiseExecutionAccuracy
for CODEX and CODEGEN on open-domain and
As introduced in § 5.3, we take CODE-DAVINCI-
closed-domainproblems,respectively.
002, and report its execution accuracy on each
domaininTable12.
C.3 QualitativeErrorAnalysis
To provide more intuitive explanations of the do-
NL Split PassRate maindivergenceaforementioned,weconducterror
@1 @2 @3 @4 @5 @6 @7 @8 @9 @10 analysisover60randomlyselectedexamplesfrom
CODE-CUSHMAN-001
- 31.91 44.67 51.81 56.54 59.95 62.56 64.61 66.28 67.65 68.79 ODEX dataset (15 for each language). By exam-
en open 24.39 35.82 43.08 48.22 52.04 54.97 57.27 59.10 60.57 61.74
close 40.19 54.42 61.41 65.69 68.66 70.90 72.70 74.18 75.45 76.56 ining the error patterns from these examples, we
- 31.89 43.33 49.23 53.01 55.72 57.81 59.52 60.96 62.22 63.33
es open 27.71 38.98 45.12 49.14 52.06 54.34 56.20 57.78 59.17 60.42 aim to answer: what are the common error types
close 36.67 48.31 53.93 57.44 59.91 61.79 63.31 64.60 65.71 66.67
- 25.67 36.69 42.66 46.49 49.27 51.44 53.23 54.76 56.10 57.32 onopen-andclosed-domainproblems? Whatare
ja open 21.24 30.29 35.16 38.34 40.71 42.61 44.20 45.55 46.73 47.79
close 35.49 50.89 59.28 64.56 68.23 71.01 73.25 75.16 76.86 78.43 themaindifferencesbetweenthem?
- 31.91 44.67 51.81 56.54 59.95 62.56 64.61 66.28 67.65 68.79
ru open 25.96 36.80 42.57 46.22 48.79 50.76 52.38 53.76 55.00 56.14
close 51.59 67.26 74.47 78.61 81.37 83.37 84.87 86.04 86.96 87.68 Similar to the previous section, we take the
CODE-DAVINCI-001 CODE-DAVINCI-002 since it scores the best and
- 33.62 46.65 53.27 57.34 60.18 62.31 64.00 65.37 66.49 67.43
en open 26.91 39.25 45.97 50.25 53.33 55.70 57.62 59.21 60.57 61.74 presentscleardomaingaps,whichmightgivemore
close 41.00 54.79 61.32 65.14 67.71 69.59 71.02 72.14 73.01 73.68
- 36.89 49.46 55.44 58.96 61.37 63.22 64.78 66.20 67.56 68.89 intuitivevariancesbetweendomains.
es open 31.67 44.63 51.11 54.78 57.07 58.63 59.81 60.79 61.67 62.50
close 42.86 54.97 60.40 63.73 66.28 68.46 70.46 72.38 74.29 76.19
- 31.04 42.11 47.83 51.54 54.26 56.39 58.11 59.53 60.67 61.59
ja open 23.72 32.72 37.88 41.48 44.21 46.36 48.08 49.46 50.53 51.33 Closed-DomainErrors Ofthe60randomsam-
close 47.25 62.92 69.89 73.85 76.54 78.62 80.34 81.83 83.14 84.31
- 43.21 57.53 63.93 67.58 70.03 71.85 73.29 74.51 75.60 76.59 plesweanalyzed,31areclosed-domainproblems,
ru open 28.86 41.01 47.05 50.77 53.47 55.65 57.53 59.22 60.79 62.28
close 55.07 71.18 77.87 81.47 83.71 85.22 86.32 87.15 87.83 88.41 and CODEX predictserroneouscodesolutionsfor
CODE-DAVINCI-002
- 47.15 57.61 62.58 65.69 67.87 69.47 70.70 71.67 72.46 73.12 22ofthem. Weidentifyfourmaintypesoferrors
en open 37.52 47.52 52.81 56.32 58.86 60.79 62.29 63.48 64.43 65.22
close 57.75 68.72 73.33 76.02 77.78 79.03 79.96 80.69 81.29 81.82 fromthesesamples: (1)11cases(50.0%)usethe
- 47.44 57.90 62.20 64.65 66.33 67.61 68.65 69.53 70.33 71.11
es open 45.42 56.02 60.17 62.68 64.59 66.17 67.52 68.70 69.79 70.83 Pythonbuilt-infunctionsincorrectly,mostlyabout
close 49.76 60.05 64.52 66.89 68.32 69.26 69.94 70.48 70.95 71.43
- 41.46 50.42 54.84 57.59 59.47 60.84 61.87 62.71 63.41 64.02 stringsmanipulationsandnumbercalculations;(2)
ja open 29.47 37.70 41.91 44.59 46.44 47.75 48.72 49.44 50.00 50.44
close 68.04 78.61 83.48 86.40 88.36 89.82 91.03 92.11 93.14 94.12 7cases(31.8%)failedatcomplexfunctions,which
- 51.87 63.36 68.25 71.09 73.03 74.5 75.67 76.64 77.46 78.17
ru open 34.74 46.20 51.46 54.65 56.93 58.75 60.29 61.66 62.89 64.04 usuallyrequiremulti-stepimplementations;(3)4
close 66.01 77.54 82.11 84.67 86.34 87.52 88.38 89.02 89.49 89.86
cases(18.2%)receivedemptypredictions, poten-
Table 10: CODEX pass rate in open and closed do-
tiallybecausetheyinvolveunfamiliartopicstothe
mains.
Library Count Pass@1 Library Count Pass@1 ations. Nonetheless,open-domainproblemsintro-
none 440 61.45 functools 2 15.00 duceextrachallenges: correctselectionandusage
pandas 81 38.52 http 2 40.00
numpy 80 36.18 obspy 2 0.00 oflibrariesandfunctionsinthewild.
re 62 36.13 pickle 2 0.00
os 42 42.62 pytz 2 20.00
collections 26 35.38 seaborn 2 0.00
D EvaluationMetrics
matplotlib 22 9.00 sqlalchemy 2 50.00
datetime 21 30.95 statistics 2 40.00
urllib 19 14.74 string 2 0.00
sys 17 15.88 xlrd 2 30.00 We describe each of the non-execution metrics
random 16 62.00 IPython 1 0.00
(§ D.1) as introduced in § 6, report model per-
io 15 32.67 argparse 1 100.00
json 15 35.33 aspose 1 10.00 formance with each (§ D.2), and visualize their
subprocess 13 30.77 bisect 1 0.00
requests 10 37.00 cgi 1 80.00 correlationswiththeexecutionaccuracy(§D.3).
bs4 9 38.89 configparser 1 60.00
itertools 9 27.78 ctypes 1 60.00
operator 9 64.44 dateutil 1 30.00
D.1 MetricDescription
time 9 20.00 difflib 1 0.00
math 8 61.43 docxtpl 1 10.00
builtins 6 76.67 filecmp 1 40.00 BLEU BLEU(Papinenietal.,2002)isalexical-
selenium 6 50.00 ftplib 1 60.00
tensorflow 6 6.67 hashlib 1 0.00 based evaluation metric, which calculates the n-
django 5 20.00 heapq 1 0.00
sqlite3 5 38.00 imp 1 40.00 gram overlap between text prediction and (multi-
PIL 4 35.00 inspect 1 0.00
ple)references. Mostdefaultcalculationprocesses
codecs 4 72.50 locale 1 0.10
cv2 4 22.50 lxml 1 0.00 calculate up to 4-grams and adopt the smoothing
scipy 4 5.00 mechanize 1 0.00
sklearn 4 0.00 mpl_toolkits 1 0.00 functionintroducedinLinandOch(2004).
base64 3 6.67 multidict 1 90.00
csv 3 36.67 pprint 1 20.00
flask 3 50.00 queue 1 0.00 ROUGE ROUGE (Lin, 2004) is another more
glob 3 43.33 regex 1 100.00
shutil 3 60.00 rsa 1 10.00 recall-orientedlexical-basedevaluationmetric. It
socket 3 40.00 ssl 1 0.00
struct 3 16.67 texttable 1 60.00 was originally designed for measuring text sum-
sympy 3 0.00 unicodedata 1 90.00
marization, mainly by counting the number of
xlwt 3 20.00 warnings 1 70.00
ast 2 50.00 xml 1 0.00 overlapping units (n-gram, word sequences, and
word pairs) between prediction and references.
Table 12: CODE-DAVINCI-001 execution accuracy on
Amongthemultiplevariantsproposed(ROUGE-N,
eachdomainsubsetinsideODEX.
ROUGE-L, ROUGE-W, and ROUGE-S), we use
themostcommonROUGE-Linourexperiments.
model;(4)2cases(9.1%)importsextralibraryor
addredundantimplementations. METEOR METEOR (Banerjee and Lavie,
Notethatthenumberoferrorcasesinthesefour 2005) is a unigram-based metric originally in-
categoriesdoesnotaddupto22. Sinceweanalyze tendedformachinetranslation. Itbuildsonagener-
alloftheerrorpredictionsamongthemodeltop-10 alizedunigramconceptbyinvolvingunigrampre-
predictions,onecasecouldpresentmultipleerror cision,unigramrecall,andwordordermeasures.
typesinitsdifferentpredictions.
ChrF ChrF(Popovic´,2015)targetslexicalmatch
Open-DomainErrors Oftheother29problems
onthecharacterlevel,bycalculatingthecharacter-
belonging to the open domain, 26 of them have
leveln-gramF-scorebetweenpredictionsandref-
erroneouspredictions. Errorsintheopendomain
erences. ChrF is also originally proposed for the
exhibit more diversity than in the closed domain.
machinetranslationtask,butlateradoptedforsome
Themajorerrorenclosing16cases(61.5%)isthe
codeevaluationworks(Evtikhievetal.,2022).
failuretousetheprerequisitelibraries,ormissing
partofthemwhenmultiplelibrariesareinvolved.
CodeBLEU CodeBLEU (Ren et al., 2020) is
The next major type is using incorrect functions,
specificallydesignedforcodeevaluation,byjointly
which happens in 9 cases (34.6%). Similarly to
considering the surface-form match, syntax simi-
the closed-domain errors, 5 cases (19.2%) have
larly,andsemanticdataflows.
error usage of correct functions, 4 cases (15.4%)
strugglewithcomplexmulti-stepimplementations,
D.2 EvaluatingwithNon-executionMetrics
and3cases(11.5%)faceemptypredictions.
OD and CD problems share some error cate- Table13andTable14showsthescoresof CODEX
goriessuchasfunctionmisuseandcomplexoper- and CODEGENusingnon-executionmetrics.
EN ES
Metrics
Model NL
BLEU ROUGE METEOR ChrF CodeBLEU
en 31.27 52.79 55.43 43.07 3.18
es 13.69 38.29 40.86 21.17 3.96
C1
ja 18.57 46.67 48.76 34.89 3.63
ru 14.42 41.49 45.53 34.63 2.70
en 30.94 53.88 56.01 43.60 3.27
es 20.40 43.93 46.71 29.36 3.27
D1
ja 19.98 48.23 51.46 38.41 3.40 JA RU
ru 16.97 44.71 47.11 35.54 2.74
en 38.75 56.05 55.39 44.40 3.77
es 18.47 44.98 43.52 27.11 5.78
D2
ja 27.10 52.04 50.17 40.02 3.58
ru 25.00 50.04 50.51 38.60 3.75
Table13: CODEXresultsonnon-executionmetrics.
Figure14: METEORonpassedandfailedsamples.
Metrics EN ES
Model NL
BLEU ROUGE METEOR ChrF CodeBLEU
en 12.04 50.94 50.46 30.12 4.90
es 9.07 37.90 37.76 20.90 5.47
350M
ja 9.43 44.21 41.29 26.16 6.05
ru 13.35 44.77 44.27 32.40 3.86
en 18.22 54.82 54.32 34.98 5.30
es 13.05 39.79 40.93 22.61 6.67
2.7B
ja 14.72 52.46 51.22 31.28 5.42
JA RU
ru 23.27 50.82 49.98 37.75 4.31
en 12.41 52.82 54.03 31.38 4.51
es 11.69 33.26 34.47 19.04 4.57
6.1B
ja 19.14 51.31 52.07 34.78 5.68
ru 23.66 49.09 49.48 37.44 3.72
Table14: CODEGENresultsonnon-executionmetrics.
D.3 VisualizingMetricCorrelations Figure15: ChrFonpassedandfailedsamples.
Following the discussion in § 6, we visualize
D.4 WhyisExecutionBetter?
the non-execution metric metrics between sam-
ples that pass and fail during execution time. Togivemoreintuitivereasonsfortheadvantages
All experiments use CODE-DAVINCI-002 predic- ofexecution,werandomlysample15casesfrom
tions for evaluation. Figure 13, Figure 14, Fig- eachlanguagesubsetandidentifiedtwomajorben-
ure15,Figure16illustratesthehistogrambetween efits: it tolerates alternative solutions and allows
passed/failed samples using ROUGE, METEOR, executionresultsasoutputs.
ChrF,andCodeBLEUmetrics,respectively.
Alternative Code Implementation Probably
EN ES
the greatest advantage of execution is it only re-
quirescorrectexecutionresults,withoutlimitations
onalternativemethods,asinFigure17.
Directly Generating Execution Results An-
otherinterestingcategoryisdirectlygeneratingthe
JA RU
codeexecutionresultsinsteadoftheimplementa-
tion steps. This often happens to simple coding
queries such as basic string manipulation, where
predictingtheresultsmightcostthemodelsimilar
effortstogettingtheprogrammaticsolutions.
InFigure18,insteadofthestringdecodingpro-
Figure13: ROUGEonpassedandfailedsamples.
gram, themodeldirectlyoutputstheresultstring
“JLK”.Whilethisissomewhatunexpectedunder
theNL-to-Codetask,executioneffectivelyhandles
suchcasesandwouldjudgethemascorrect.
EN ES
Figure 18: An example output of a correct execution
JA RU
result,yetonlyachieving0.6BLEU.
Figure16: CodeBLEUonpassedandfailedsamples.
Figure19: Examplethatthemodelpredictionusesthe
wrongfunction,havingaveryhighBLEUscore0.925.
E AblationStudies
Figure 17: An alternative yet correct prediction, only
has a low 4.8 BLEU score due to having little lexical
Thissectionprovidestheresultstablesaccording
overlapwiththecanonicalsolution.
toeachablationstudysectionin§7.
D.5 PotentialBenefitofLexical-based
Metrics
E.1 PromptingStrategy
Lexical-basedmetrics,althoughrelativelyineffec-
tiveforfunctionalcorrectness,stillarepotentially E.1.1 Few-shotPrompting
helpfulfordebuggingandinterpretation. Theyare
effectiveinsmallerrorsoftwotypes: (1)asingle Table 15, Table 16, Table 17 show the change in
functionmisuseand(2)slightvarianceincomplex executionaccuracywithrespecttotheexamplesin
strings. Thehighlexicalmatchinsuchcasesindi- in-contextlearning,onthethree CODEXvariants
cateslesseffortforfixing(Dengetal.,2021).
N-shot NL PassRate
FunctionMisuse Somecodepredictionsarecor- @1 @2 @3 @4 @5 @6 @7 @8 @9 @10
en 37.90 48.71 54.50 58.25 60.88 62.82 64.31 65.52 66.54 67.43
rectexceptforasingleplacewhereawrongfunc- 1-shot e jas 3 26 9. .2 72 6 4 35 8. .5 51 4 5 40 3. .7 20 2 5 43 6. .9 26 3 5 46 8. .1 34 3 5 47 9. .6 98 0 5 58 1. .8 11 4 5 59 2. .7 10 5 6 50 2. .4 94 9 6 51 3. .1 61 6
tionisused,oranargumentismisplaced. ru 45.67 56.75 62.32 65.86 68.38 70.32 71.88 73.21 74.37 75.40
en 37.27 47.89 53.39 57.02 59.68 61.75 63.41 64.80 65.97 66.97
Forexample,inFigure19,thecodeimportsthe 2-shot e jas 3 38 2. .5 26 6 4 48 1. .7 57 7 5 44 6. .1 72 1 5 57 0. .5 10 8 5 59 2. .9 70 6 6 51 4. .7 75 8 6 53 6. .2 46 0 6 54 7. .5 74 4 6 55 8. .6 87 4 6 56 9. .6 77 6
ru 46.75 58.56 64.24 67.63 69.90 71.55 72.82 73.84 74.68 75.40
libraryandcopiesallstringscorrectly. Butituses
en 39.91 50.45 55.62 58.83 61.06 62.74 64.07 65.17 66.13 66.97
the wrong function match instead of the correct 3-shot e jas 3 37 2. .0 80 7 4 45 2. .8 48 8 5 40 7. .0 55 8 5 52 0. .6 83 8 5 54 3. .4 28 9 5 55 5. .8 17 6 5 56 6. .9 65 6 5 57 7. .8 80 9 5 58 8. .4 94 0 5 58 9. .8 79 6
ru 48.33 60.03 65.32 68.51 70.71 72.35 73.66 74.75 75.71 76.59
findall. Although the execution fails, the code
issimilartothesolution. Giventhesignofahigh Table15: CODE-CUSHMAN-001few-shotresults.
BLEU score of 92.5, we could readily spot such
similaritiesandfixthemwithsimpleedits.
String Difference Another frequent error con- N-shot NL PassRate
@1 @2 @3 @4 @5 @6 @7 @8 @9 @10
cernsstringcopying,wherethecodecallsthecor- en 43.05 53.67 58.80 62.01 64.31 66.09 67.52 68.71 69.73 70.62
1-shot e jas 4 31 5. .0 00 0 5 42 5. .6 59 7 5 58 1. .5 14 7 6 52 4. .1 71 9 6 54 7. .5 46 5 6 56 9. .3 55 8 6 67 1. .6 39 5 6 68 2. .6 89 6 6 69 4. .4 14 5 7 60 5. .0 20 4
rectfunctionsbutcopiesthestringdifferently. ru 47.30 59.07 64.57 67.92 70.25 72.02 73.41 74.52 75.44 76.19
en 44.26 53.98 58.77 61.85 64.00 65.59 66.79 67.70 68.43 69.02
The example in Figure 20 gets a 100.0 BLEU 2-shot e jas 4 30 5. .4 14 2 5 40 4. .1 85 2 5 44 9. .9 87 7 5 57 3. .9 00 7 5 59 5. .9 21 5 6 51 6. .4 71 7 6 52 7. .6 84 6 6 53 8. .7 60 6 6 54 9. .6 27 7 6 55 9. .5 76 6
score,butthestringinsideactuallymissesasingle ru 49.72 60.59 65.76 68.96 71.16 72.78 74.03 75.04 75.87 76.59
en 43.58 53.27 57.88 60.81 62.99 64.74 66.19 67.44 68.52 69.48
whitespace,whichtheBLEUtokenizationwould 3-shot e jas 4 31 8. .6 77 8 5 43 9. .1 44 0 5 58 4. .7 58 9 6 52 7. .0 63 6 6 54 9. .1 71 1 6 65 1. .5 15 8 6 66 2. .6 32 1 6 67 3. .4 28 1 6 68 3. .2 92 6 6 68 4. .8 69 3
discard. Suchcodealsoresemblesthesolutionand ru 49.21 58.83 63.58 66.73 69.08 70.99 72.63 74.08 75.40 76.59
couldbeeasilyfixedbyevenrule-basedmethods. Table16: CODE-DAVINCI-001few-shotresults.
We conjecture the gain brought by whitespace
strippingtobebetterdistributionalalignmentwith
CODEGENtrainingdata. AsCODEGENmightbe
pre-trainedonwhitespace-strippedtextsequences,
Figure 20: Example that the model prediction varies inputs without whitespaces are potentially more
slightlyincopiedstrings,butscores100.0inBLEU. aligned with them, hence resulting in better test-
time performance. Meanwhile, note that the tok-
N-shot NL PassRate enizationprocessesfortext(naturallanguage)and
@1 @2 @3 @4 @5 @6 @7 @8 @9 @10
en 46.33 56.08 60.54 63.36 65.39 66.97 68.24 69.28 70.14 70.84 code(programminglanguage)differinwhitespace-
1-shot e jas 4 44 6. .3 33 3 5 54 6. .0 00 8 5 69 0. .0 54 4 6 62 3. .4 39 6 6 65 5. .0 37 9 6 67 6. .0 99 7 6 68 8. .7 22 4 7 60 9. .0 27 8 7 71 0. .2 12 4 7 72 0. .2 82 4 styletokenssuchas\nor\t. Thesetokenswould
ru 51.35 62.72 68.20 71.60 73.98 75.79 77.24 78.47 79.56 80.56
en 47.29 57.32 61.96 64.69 66.53 67.86 68.90 69.74 70.46 71.07 beremovedbytexttokenizersbydefault,whilepre-
2-shot e jas 4 45 2. .7 38 8 5 55 2. .8 25 8 6 50 6. .4 81 0 6 53 9. .2 39 8 6 65 1. .4 04 2 6 67 2. .1 16 2 6 68 2. .6 83 8 6 69 3. .9 43 1 7 61 3. .1 71 8 7 62 4. .2 02 2 servedbycodetokenizerssincetheyimplystruc-
ru 51.75 63.38 68.47 71.51 73.60 75.13 76.30 77.23 77.98 78.57
en 48.18 57.99 62.64 65.50 67.50 68.99 70.17 71.14 71.96 72.67 turalinformationincodepieces.
3-shot e jas 4 44 6. .4 14 0 5 53 5. .9 65 4 5 58 9. .3 71 4 6 61 2. .0 17 7 6 63 3. .1 81 1 6 64 5. .7 04 0 6 66 5. .0 97 0 6 67 6. .1 69 1 6 68 7. .1 21 0 6 68 7. .8 69 8
ru 49.40 60.56 66.09 69.64 72.19 74.17 75.77 77.12 78.29 79.37
E.2 NumberofEvaluationTestCases
Table17: CODE-DAVINCI-002few-shotresults.
Table20showstheeffectwhenusingdifferentnum-
E.1.2 NumberofInputTestCases bersoftestcasesforexecution-basedevaluation.
Table18showstheeffectsonexecutionaccuracy,
E.2.1 NumberofEvaluationTestCases
of adding one or more test cases to prompts. Ex-
perimentsuseCODE-DAVINCI-002asanexample.
#test NL PassRate
@1 @2 @3 @4 @5 @6 @7 @8 @9 @10
#test NL PassRate en 48.31 58.81 63.70 66.72 68.83 70.39 71.59 72.55 73.35 74.03
@1 @2 @3 @4 @5 @6 @7 @8 @9 @10 1 e jas 4 48 2. .0 40 4 5 58 1. .5 92 6 6 52 6. .7 51 1 6 54 9. .9 38 6 6 66 1. .5 31 4 6 67 2. .6 89 0 6 68 3. .6 97 5 6 69 4. .5 93 1 7 60 5. .3 73 3 7 61 6. .1 41 6
en 47.15 57.61 62.58 65.69 67.87 69.47 70.70 71.67 72.46 73.12 ru 52.50 63.26 67.85 70.60 72.53 74.00 75.19 76.17 77.02 77.78
0 e jas 4 47 1. .4 44 6 5 57 0. .9 40 2 6 52 4. .2 80 4 6 54 7. .6 55 9 6 56 9. .3 43 7 6 67 0. .6 81 4 6 68 1. .6 85 7 6 69 2. .5 73 1 7 60 3. .3 43 1 7 61 4. .1 01 2 en 47.15 57.61 62.58 65.69 67.87 69.47 70.70 71.67 72.46 73.12
ru 51.87 63.36 68.25 71.09 73.03 74.5 75.67 76.64 77.46 78.17 n e jas 4 47 1. .4 44 6 5 57 0. .9 40 2 6 52 4. .2 80 4 6 54 7. .6 55 9 6 56 9. .3 43 7 6 67 0. .6 81 4 6 68 1. .6 85 7 6 69 2. .5 73 1 7 60 3. .3 43 1 7 61 4. .1 01 2
en 63.35 75.61 80.28 82.70 84.20 85.22 85.97 86.57 87.06 87.47 ru 51.87 63.36 68.25 71.09 73.03 74.50 75.67 76.64 77.46 78.17
1 e jas 6 63 3. .8 99 0 7 76 4. .3 67 6 8 71 8. .7 85 5 8 84 1. .7 15 3 8 86 2. .6 60 1 8 87 3. .8 62 8 8 88 4. .6 45 9 8 89 5. .2 13 2 8 89 5. .6 67 1 9 80 5. .0 90 8
ru 65.04 77.80 82.89 85.72 87.53 88.75 89.60 90.19 90.60 90.87 Table20: CODE-DAVINCI-002 resultswhenusingdif-
en 64.76 77.36 82.02 84.40 85.93 87.05 87.93 88.65 89.25 89.75
n e jas 5 69 3. .8 49 1 7 72 4. .4 02 2 7 77 8. .4 44 9 8 80 0. .4 91 8 8 82 2. .4 59 7 8 84 3. .0 63 9 8 85 4. .1 56 1 8 85 5. .9 15 4 8 86 5. .4 64 1 8 86 5. .6 97 8 ferentnumbersoftestcasesforexecution-basedevalu-
ru 66.67 79.07 83.70 86.19 87.82 89.01 89.91 90.62 91.19 91.67 ation. 1 means using one randomly selected test case,
nmeansusingallannotatedtestcasesinODEX.
Table18: CODE-DAVINCI-002resultswhenusingzero
(0),one(1),andall(n)testcasesinthepromptinput.
E.3 SemanticsofFunctionNames
E.1.3 Pre-processing: TrailingWhitespaces Becausecodeiswrappedintofunctionstoenable
While the input construction process may intro- execution, how functions are named may affect
duce whitespaces at the start and the end of the model predictions. By default, we name func-
textsequence,wefind CODEGENmodelunexpect- tions using the post ID (e.g., f_3844801), which
edlysensitivetotrailingwhitespaces. Asshownin expresses little semantics of queries. So we try
Table19,removingwhitespacesfromtheprompt twoothermethods: (1)aconstantstringfunction;
inputincreasesthepassrateofallsizedCODEGEN and (2) summary phrases from NL intents, e.g.,
modelsbyover20percent. find_max_value.
Todo(2),weconductaheuristicphraseextrac-
w/WS w/oWS tion. We first cut the NL intent into words by
Model NL
@1 @2 @5 @10 @1 @2 @5 @10
whitespace,thenremovethestopwords(‘in’,‘of’,
en 10.32 11.29 12.24 12.53 26.26 32.18 39.10 42.82
es 17.56 17.78 17.78 17.78 16.67 21.85 27.82 30.00 ‘a’,‘to’,‘and’,‘for’,‘with’,‘that’)andmeaningless
350M
ja 7.01 8.06 9.55 10.37 17.44 22.86 28.21 30.49
ru 21.35 24.20 26.94 28.17 25.87 31.44 37.44 40.87 punctuations, lastly, concatenate the first M = 4
en 14.28 15.69 16.99 17.54 37.74 42.58 47.36 49.89 wordswith‘_’. Forexample,givenanintent“de-
es 19.67 22.32 24.76 25.56 36.44 40.89 44.84 46.67
2B
ja 10.98 12.56 14.20 14.63 31.83 35.70 39.58 41.46 codeahexstring’4a4b4c’toUTF-8”,theresulting
ru 33.10 36.01 39.53 41.67 45.67 49.83 54.54 57.54
en 11.96 12.95 14.01 14.81 34.49 37.91 41.18 43.05 function name would be “decode_a_hex_string”.
es 14.78 16.64 18.70 20.00 28.56 32.05 35.86 37.78
6B ja 12.44 14.34 16.51 17.68 35.55 40.11 44.12 46.34 However,forlanguagesthatdonotseparatewords
ru 32.86 34.45 36.28 37.30 44.64 47.29 49.82 51.19
with whitespace, this approach may produce less
Table 19: CODEGEN results when inputting prompts meaningfulstrings,hencecontributingtotheinfe-
withandwithouttrailingwhitespaces(WS). riorperformanceasshownbelow.
Tofairlycomparewithpreviousresults,wedo SQL queries (Zhong et al., 2017) or logical
notaddtestcasesinprompts. forms (Dong and Lapata, 2016). This execution-
basedparadigmhasnotbeenintroducedtogeneral-
purposelanguagesuntilrecentlybytheHumanEval
dataset(Chenetal.,2021),wherehuman-written
testcasesareprovidedforcodeexecution. Many
works afterward follow this approach, but focus
more on closed-domain settings (Austin et al.,
2021;Hendrycksetal.,2021)orspecificlibraries
ofinterest(Laietal.,2022;Huangetal.,2022). To-
wardbroaderexecutionenvironments,weprovide
executabletestcasesforasmanyas79libraries.
Coding Queries Versus Programming Chal-
Figure21: pass@1usingdifferentfunctionnames.
lenges Programs from different sources are or-
From Figure 21 and Table 21, using more se- ganizedforvariouspurposes. Codingcontestweb-
manticallymeaningfulfunctionalnamesbarelyim- sites such as LeetCode11 and Codeforces12 have
provesoverthedefaultsetting. Intuitively,summa- been used to build many code generation bench-
rizingnamesfromintentsaddsnoextrasemantics, marks (Hendrycks et al., 2021; Li et al., 2022).
butmaycostinformationlossatthecurationstep, However, they randomly align with how humans
bothcontributingtotheperformancedrop. program in practical scenarios. To build datasets
with natural and practical usage of code, many
FuncName NL PassRate
@1 @2 @3 @4 @5 @6 @7 @8 @9 @10 works use GitHub Jupyter Notebooks (Agashe
task-id e e jan s 4 4 47 7 1. . .1 4 45 4 6 5 5 57 7 0. . .6 9 41 0 2 6 6 52 2 4. . .5 2 88 0 4 6 6 55 4 7. . .6 6 59 5 9 6 6 57 6 9. . .8 3 47 3 7 6 6 69 7 0. . .4 6 87 1 4 7 6 60 8 1. . .7 6 80 5 7 7 6 61 9 2. . .6 5 77 3 1 7 7 62 0 3. . .4 3 46 3 1 7 7 63 1 4. . .1 1 02 1 2 etal.,2019;Huangetal.,2022)andStackOverflow
ru 51.87 63.36 68.25 71.09 73.03 74.50 75.67 76.64 77.46 78.17 forums (Yin et al., 2018; Wang et al., 2022; Lai
en 43.14 52.71 56.94 59.54 61.38 62.78 63.90 64.82 65.60 66.29
constant e jas 4 30 8. .1 21 9 5 41 8. .5 08 1 5 56 2. .9 70 6 6 50 5. .1 60 1 6 52 7. .3 53 6 6 53 9. .9 09 5 6 65 0. .2 28 4 6 66 1. .3 20 3 6 67 2. .1 01 7 6 67 2. .7 88 0 etal.,2022)asasourceofnaturally-occurringcode.
ru 48.06 60.19 65.75 69.25 71.78 73.77 75.41 76.80 77.98 78.97 We remain such naturalness by using StackOver-
en 43.23 53.77 58.87 62.06 64.34 66.11 67.54 68.74 69.75 70.62
intent e jas 3 37 7. .7 98 9 4 49 7. .2 71 8 5 54 2. .5 42 0 5 57 5. .7 05 6 6 50 6. .1 72 7 6 52 8. .0 05 3 6 53 9. .7 02 5 6 55 9. .2 91 6 6 66 0. .5 76 9 6 67 1. .7 58 9 flow posts, but uniquely from forums in various
ru 48.29 60.64 66.39 69.79 72.11 73.86 75.26 76.42 77.38 78.17
languagestoalsoassistprogrammersworldwide.
Table 21: CODE-DAVINCI-002 results when the wrap-
TestCaseCreation Whilemostbenchmarksuse
pingfunctionnamecontainsdifferentsemantics.
Python test cases annotated by human program-
F RelatedWork
mers (Chen et al., 2021; Nijkamp et al., 2022;
Lai et al., 2022), challenge-style datasets adopt
Open Domain Code Generation Code written
a more direct approach by crawling from the
ingeneral-purposeprogramminglanguagesoften
web(Hendrycksetal.,2021;Lietal.,2022). An-
uses classes or functions from external libraries.
other thread of work attempts to generate test
A few datasets for code generation preserve this
cases automatically based on the Python gram-
open-domainnature. TheCONCODE(Iyeretal.,
mar (Lukasczyk and Fraser, 2022), but is largely
2018)datasettestedgenerationofJavaclassmeth-
limitedtobasicPythonfunctions. Somepropose
ods. LaterworkstargetPythongenerationgiventhe
toleveragethepowerofneuralLMs(Tufanoetal.,
interactivecontextofJupyterNotebooks(Agashe
2020;Lietal.,2022),evenjointlyconsideringso-
etal.,2019)ornaturallanguageintentsfromStack-
lutionandtestcasegeneration(Chenetal.,2022).
Overflow posts (Yin et al., 2018; Wang et al.,
However,thequalityanddiversityoftestcasesare
2022). Despite their natural coverage, enabling
not robustly ensured. We hence use high-quality
open-domaincodeexecutionhasfacedgreatchal-
human-writtentestcasesforODEXevaluation.
lengesgivenitsdiversityandcomplexity(Laietal.,
2022;Chandeletal.,2022). Toaddressthisissue,
our ODEX provides test cases as code execution
contextsforevaluation.
Code Evaluation via Execution Execution-
based evaluation has been long adopted for 11https://leetcode.com/
domain-specific programming languages such as 12https://codeforces.com/

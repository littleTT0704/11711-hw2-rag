He Said, She Said: Style Transfer for
Shifting the Perspective of Dialogues
AmandaBertsch GrahamNeubig MatthewR.Gormley
CarnegieMellonUniversity CarnegieMellonUniversity CarnegieMellonUniversity
abertsch@cs.cmu.edu gneubig@cs.cmu.edu mgormley@cs.cmu.edu
Abstract formalization, and attribution of emotion/stance
markerstoindividuals. Whilecoreferenceresolu-
In this work, we define a new style trans-
tion,stancedetection,andformalizationareoften
fer task: perspective shift, which reframes a
dialouge from informal first person to a for- treatedasseparatetasks,thesignalfortheseobjec-
mal third person rephrasing of the text. This tives is commingled in the dialogues. A pipeline
task requires challenging coreference resolu- approachwoulddiscardinformationnecessaryfor
tion,emotionattribution,andinterpretationof anyonetaskinthecompletionoftheothertwo.
informaltext. Weexploreseveralbaselineap-
We create a dataset for this task by annotating
proachesanddiscussfurtherdirectionsonthis
dialoguesfromtheSAMSumcorpus(Gliwaetal.,
task when applied to short dialogues. As a
2019), a dialogue summarization corpus of syn-
sample application, we demonstrate that ap-
plying perspective shifting to a dialogue sum- thetic text message conversations (§3). For each
marization dataset (SAMSum) substantially conversation, annotators rephrase the utterances
improves the zero-shot performance of ex- line-by-lineintooneormoresentencesin3rdper-
tractive news summarization models on this son. Unlikeasummary,whichcondensesinforma-
data. Additionally,supervisedextractivemod-
tiontohighlightthemostimportantpoints,thegoal
elsperformbetterwhentrainedonperspective
ofthistransformationistocaptureasmuchofthe
shifteddatathanontheoriginaldialogues. We
informationfromtheoriginalutteranceaspossible
releaseourcodepublicly.1
inamorestandardizedform.
1 Introduction
We fine-tune BART on this dataset as a super-
visedbaselineunderseveraldifferentproblemfor-
Styletransfermodelschangesurfaceattributesof
mulations,andweexperimentwithincorporating
text while preserving the content. Previous work
formalitydataintothetrainingprocess(§4). Asa
onstyletransferhasfocusedoncontrollingthefor-
motivatingusecase,wedemonstratethatextractive
mality, authorial style, and sentiment of text (Jin
summarizationoverperspective-shifteddialogueis
etal.,2022). Weproposeanewstyletransfertask:
morefluentandhashigherROUGEscoresthanex-
perspectiveshiftfromdialogueto3rdpersoncon-
tractivesummarizationovertheoriginaldialogues
versationalaccounts(§2). Inthistask,weseekto
(§5). Thistrendholdsforzero-shotperformanceof
convertfromaninformal1stpersontranscriptionof
extractivesummarizationmodelstrainedonnews
thedialoguetoa3rdpersonrephrasingofthecon-
corporaandforfullysupervisedtrainingonmodel-
versation,whereeachlinecapturestheinformation
generatedperspectiveshiftdata.
ofasingleutterancewithrelevantcontextualizing
Perspective shift can be a useful operation for
informationadded. Table1demonstratesanexam-
extractivesummarizationwhenannotationtimeis
pleconversionanditsperspectiveshiftedversion.
limited;whenadditionaldatafromout-of-domain
Thistaskischallengingbecauseitrequiresthe
isavailable;whentheexactlengthandcontentof
interpretationofmanydiscoursephenomena. Indi-
the summary is not known at annotation time; or
alogue,speakerscommonlyuse1stand2ndperson
whenhighfaithfulnessisimportanttotheendtask,
pronounsandcasualspeech. Speakersalsoconvey
butfluencyisalsoaconcern(§5.3).
their own emotions and opinions in their speech.
Convertingamulti-partyconversationtoasingle-
2 Taskdefinition
perspectiverephrasingrequirespronounresolution,
We define perspective shift as an utterance-level
1https://github.com/abertsch72/
perspective-shifting rephrasing task. Given a dialogue and a single
2202
tcO
72
]LC.sc[
1v26451.0122:viXra
Original Perspectiveshifted
Laura: Ineedanewprinter:/ Lauraisfrustratedthatsheneedsanewprinter.
Laura: thinkingaboutthisone Lauraisthinkingaboutaspecificprinter.
Laura: <file_other> Laurasendsafile.
Jamie: you’resureyouneedanewone? JamieasksifLauraissuresheneedsanewone.
Jamie: I mean you can buy a second hand JamieclarifiesthatLauracouldbuyasecondhandprinter.
one
Laura: couldbe Laurasaysthat’spossible.
Table1: AnexampleconversationfromtheSAMSumdatasetwiththeassociatedperspectiveshift.
selectedutterance,thegoalofthetaskistorewrite streamtasks. Weaimtolimittheinformationlost
that utterance as a formal third person statement. intheperspectiveshiftoperationbyencodingthe
Four operations are required to accomplish this meaningsofsuchinformallanguageintheoutput.
change: coreferenceresolution,syntacticrewriting, Oftenthistakestheformofanadverb(e.g. “Sam
formalization, and emotion attribution. Table 1 angrilysays”)orashortdescriptivesentence(e.g.
shows an example conversation and perspective “Camisamused”). Thisrequiresinterpretationof
shift,demonstratingeachofthesechallenges. theinformalelementsofthetext.
First-person singular and second-person pro- Clearly,thistaskisfarmorecomplexthansim-
nouns are usually easily resolved in a conversa- ply swapping pronouns for speaker names. We
tional context—first-person singular refers to the curateadatasetfortheperspectiveshiftoperation.
speaker,whilesecond-personpronounsgenerally
refer to the other conversational parties—plural 3 Datasetcreation
first-person pronouns can be less obvious to re-
solve. When a party in a conversation uses the ThedatasetisanannotatedsubsetoftheSAMSum
pronoun“we,”thispluralmaybereferringtothe (Gliwa et al., 2019) dataset for dialogue summa-
otherpartiesintheconversation,somebutnotall rization. SAMSum is a dataset of simulated text
of the parties in the conversation, or a party not messageconversations,rangingfrom3to30lines
presentintheconversation,e.g. intheutterance“I inlengthandwithbetween2and20speakers. The
needtotalktomyhusband. Wemighthaveother datasetconsistsof314conversationsfromthetrain
plans.” Inourhand-annotateddataset,weresolve set,368conversationsfromthevalidationset,and
thesepronounswhereverpossible;ifitisnotclear 151conversationsfromthetestset2. Wesetaside
what group the pronoun refers to, we resolve the the151conversationsfromtestasatestsplitand
pronounasreferringto“<thecurrentspeaker>and usetheother682conversationsastrainingandval-
others,”e.g. “Laura: wearebusy”becomes“Laura idationdata.
andothersarebusy”. Otherentitiesinthetextmay Annotatorswereinstructedtoconverteachutter-
also be difficult to resolve, such as those defined anceindividuallytoaformal3rdpersonrephrasing,
only at the beginning of the conversation, many whilepreservingasmuchofthetoneoftheutter-
turnspriortothecurrentreference. anceaspossible. Annotatorswererequiredtoinsert
Syntacticrewritingistheproblemofconverting thespeaker’snameineachrewrittenutteranceand
thesyntaxoftheutterancetoreflect3rdratherthan removeall1st-personpronouns. Annotatorswere
1stperson. Thismayinvolvere-conjugatingverbs, alsoaskedtostandardizegrammar, removeques-
e.g. converting“Sam: Iambusy”to“Samisbusy.” tions,andaddadditionalcontext(e.g. descriptive
adverbs)toconveyemotionspreviouslyexpressed
Formalizationandemotionattributionarerelated
by emoticons. Further information about annota-
problems,asmuchoftheemotionandstanceinfor-
torselectionandpay,aswellasafullcopyofthe
mationinthetextiscontainedininformalphrases,
annotation instructions, is available in Appendix
unconventional punctuation, and emojis (Tagg,
D.
2016). Typicalformalizationeliminatesthesemark-
erswithoutreplacement(RaoandTetreault,2018).
2DuetoSAMSum’srestrictivelicensing,weareunableto
However,thismakesformalizationahighlylossy
releasethedatasetatthistime.TheSAMSumauthorsdidnot
conversion, which may be undesirable for down- approveourrequestedexception.
Method ROUGE-1 ROUGE-2 ROUGE-L BARTScore
nocontext 62.57 40.45 61.41 -2.38
leftcontextonly 60.80 37.50 59.27 -2.39
leftandrightcontext 63.57 40.74 62.04 -2.36
conversation-level 63.20 35.04 51.80 -2.67
Table2: Scoresonthetestsetformodelstrainedwithdifferentproblemformulations.
3.1 Datasetstatistics sationisperspectiveshiftedatonce.
Theperspectiveshiftedconversationsdifferfrom
1. nocontext: Theinputtothe modelistheut-
theoriginalinseveralways. Thenumberofturns
terance u , and the output is the perspective
t
ineachconversationispreserved,buttheaverage
shiftedversion,y .
t
turn length varies: for the perspective shifts, the
mean number of words per turn is 11.0, while 2. leftcontextonly: Theinputisthedialogueup
the mean for the original dialogues is 8.4. (Note toandincludingutteranceu ,andtheoutputis
t
that the simplest heuristic would increase each theperspectiveshiftedversion,y . A[SEP]
t
utterance’s word count by 1, as the colon next to tokendelimitstheleftcontext,u ,...,u ,
1 t−1
the speaker name is swapped out with the word fromtheutteranceu .
t
“says”).
3. left and right context: The input is the full
The average word-wise edit distance between conversation,with[SEP]tokensaroundthe
original and perspective-shifted utterances is 8.5 utteranceu ,andtheoutputistheperspective
t
words. This is partially due to the insertion of a shiftedversion,y .
t
dialogue tag (e.g. “says”) in each utterance, the
4. conversation-level: The input is a complete
removalofemojis(average0.1perutterance),and
dialogueu ,...,u ,andtheoutputisacom-
theresolvingoffirstandsecondpersonpronouns 1 T
(average 0.9 per utterance). The part of speech3 pleteperspectiveshifty 1,...,y T.
distributionoftheconversationsalsochanges,with
For each formulation, we finetune a BART-large
a strong (65.8%) decrease in interjections and a
(Devlin et al., 2019) model for 15 epochs, using
slight (5.1%) decrease in adjectives and adverbs.
earlystopping,aneffectivebatchsizeof8,anda
However, in utterances that contain at least one
learningrateof5e-5.
emoji,thenumberofadjectivesandadverbspresent
increases 12.8%. This is consistent with the an-
Results ROUGE 1/2/L scores and BARTScore
notation guidelines, which instruct annotators to
foreachmodelarelistedinTable2.
capturethemeaningofinformalmarkerssuchas
The no context model treats this as a purely
emojiwithdescriptors.
utterance-level task, but fully precludes the addi-
tionofcontextfromotherutterances. Thismeans
4 Perspectiveshifting
thatsecond-personandfirst-personpluralpronouns
cannotberesolvedclearly. Whilethismodelscores
4.1 FormulationofthePredictionProblem
quite highly on all 4 metrics, we observe a high
Methods We consider several formulations of
rateofnamedentityhallucinationintheconverted
theperspectiveshiftingtaskasapredictionprob-
outputs. Forinstance,fortheinpututterance“Han-
lemwithdifferentinputandoutputstyles. Below,
nah: Hey, do you have Betty’s number?”, the no
the first three approaches formulate the problem
context model outputs “Hannah asks John if he
asaline-by-linetask: eachinputexampleconsists
has Betty’s number.” However, the other conver-
of the full conversation with one utterance desig-
sationalpartnerinthisdialogueis“Amanda,”not
natedastheutterancetobeperspectiveshifted. The
“John.” Because the gold perspective shifts were
fourthapproachbelowformulatestheproblemas
annotatedwiththefullconversationavailablefor
conversation-leveltaskinwhichtheentireconver-
reference, this model often hallucinates to fill in
namedentityslotsthatitdoesnothavethecontext
3Part-of-speechrelatedstatisticsarecalculatedusingthe
spaCyPOStagger(Honnibaletal.,2020). toresolve.
Approach ROUGE-1 ROUGE-2 ROUGE-L BARTScore
PS ONLY 63.57 40.74 62.04 -2.36
FORMALITY + PS 62.00 39.14 60.38 -2.37
FORMALITY ONLY 51.25 22.12 49.96 -2.57
RULES-BASED HEURISTIC 61.77 35.93 55.34 -2.80
HEURISTIC + FORMALITY 56.98 31.91 55.72 -2.59
Table3: Scoresforeachoftheperspectiveshiftmodels.
By contrast, the conversation-level model has textandnocontextmodelsarerelativelyclose,we
the clear advantage of referencing the entire con- conductahumanevaluationcomparingthesetwo
versationatgenerationtime. However,themodel cases. Inourblindcomparisonof22conversations,
doesnothavearequirementtoproducethesame theleftandrightcontextmodelwaspreferredover
number of lines as the input and must learn this thenocontextmodel86%ofthetime(2annotators,
propertyduringtraining. Weconjecturethatthisis Cohen’skappa0.62).
thereasonforthismodel’srelativelyweakperfor- The conversation-level model may be a good
mancerelativetotheleftandrightcontextmodel. choiceforsomeapplications,whereoutputlength
Additionally, if the model generates more or less is less important to the downstream task. This
linesthantheinputdialogue,thiscanbeaconflat- modelhasahigherdegreeofabstractiveness,which
ingfactorintheextractivesummarizationexample can lead to increased fluency but also increased
we discuss in Section 5. If the model generates hallucination. Fortaskswherethisisaconcern,the
less lines than the input, it has performed some left and right context model achieves reasonable
part of the summarization process by abstracting fluencywhileadheringmorecloselytothetask,as
theinputintoashorteroutput;ifithasgenerated measuredbytheautomaticmetrics.
morelinesthantheinput,ithasproducedaharder
problemfortheextractivesummarizationsystem 4.2 FormalityandPerspectiveShift
bycreatingmorelinestochoosethesummaryfrom.
Approaches We observe that the perspective
Becauseofthismodel’sweakerperformanceand
shifting task requires a high degree of formaliza-
thisconflatingfactor,werestrictourremainingex-
tion. We consider several models ranging from
perimentsinthispapertomodelsthatperspective
simplerule-basedapproachestothoserelyingon
shiftoneutteranceatatime.
anexternalformalizationdatasetinordertobetter
The model with left context only mimics how understand the role of formalization in perspec-
a human might read the conversation for the first tive shifting. The external dataset we consider is
time,fromtoptobottom. Thischoiceofmodelalso theGrammarlyYahooAnswersFormalityCorpus
imposestheconstraintthattheoutputisthesame (GYAFC)(RaoandTetreault,2018): adatasetof
numberoflinesastheinput,asdesired. However, approximately100,000linesfromYahooAnswers
the dialogues frequently contain cataphora, espe- andformalrephrasingsofeachline.
cially in the start of the conversations, where the Our core method is the BART model trained
firstspeakermaybeaddressingasecondspeaker under the left and right context formulation (PS
whohasnotyetspoken. Forinstance,intheexam- ONLY).
ple“Hannah: Hey,doyouhaveBetty’snumber?”,
We also consider a heuristic baseline (RULES-
thisisthefirstutteranceofthedialogue. Amodel
BASED HEURISTIC). For each message, we
withonlyleftcontextcannotresolvetheword“you”
prependthespeaker’snameandtheword“says”to
hereanybetterthanthenocontextmodel.
theutterance. Wereplaceeachinstanceofthepro-
Theleftandrightcontextmodeladdressesthis noun“I”inthemessagewiththespeaker’sname.
concernbyprovidingthefullconversationasinput, After observing that most messages are not well-
butrestrictingtheoutputgenerationtoaperspective punctuated,wealsoappendaperiodtotheendof
shiftforasingle(marked)utterance. Thisimposes eachutterance. Whilethisheuristicissimpleand
theoutputlengthconstraintwithoutsacrificingcon- ignoresmanypronounresolutionconflicts, ithas
textualinformation. Thismodelperformsbeston theclearadvantageofbeinghighlyefficient.
all4metrics. Asthescoresforleftandrightcon- We incorporate the GYAFC corpus as part of
our training regime by finetuning on the formal- topreviousutterances;knowledgeofotherspeak-
izationtaskpriortofinetuningonperspectiveshift ersinthedialoguecanbenecessarytocontextual-
(FORMALITY + PS). izetheinformation. Summariesshouldpresentan
Finally, we perform an ablation by finetuning overviewofaconversationthatincorporatesglobal
BART for formalization on the GYAFC corpus, contextualinformation;generally,thesesummaries
thenattempting zero-shottransfer totheperspec- arealsoexpectedtobeinthirdperson.
tive shifting task. As input for this model at Extraction over a perspective-shifted dialogue
testtime,weprovideeithertheoriginaldialogues does not suffer from many of the same problems
(FORMALITY ONLY) or the output of the rules- as extraction over an original dialogue. The text
basedheuristic(HEURISTIC+FORMALITY). inaperspectiveshifteddialogueisinformalthird
person,whichmatchesthedesiredstyleofthesum-
Results WeevaluateeachapproachonROUGE
mary text. While individual sentences of the per-
1/2/L (Lin, 2004) and BARTScore (Yuan et al.,
spective shifted dialogue correspond directly to
2021). The scores are in Table 3, and example
individual utterances in the dialogue, the corefer-
outputsareinTable4.
ence resolution involved in the perspective shift
Atfirstglance,perspectiveshiftisataskclosely
step means that these sentences are less interde-
related to formalization. However, the addition
pendentthanthedialogueturns. Inmanyrespects,
offormalizationdataleadstoaslightdecreasein
perspectiveshiftshouldmakethetaskofdialogue
modelperformance. Thismaybeduetotheformal-
summarizationeasier.
itydatabiasingthemodeltowardminimalrephras-
ings, as there is generally relatively low edit dis- 5.1 OracleExtractionafterPerspectiveShift
tancebetweentheinformalandformalsentences
Methods Thisintuitiveresultisconfirmedbythe
in the formality corpus used (Rao and Tetreault,
performanceofanoracleextractivemodel. Given
2018). However,forhighperformanceonperspec-
boththeinputandthesummary,theoraclemodel
tiveshift,theadditionofclarifyingwords,emotion
is tasked with choosing a combination of k utter-
attributions,andpronounsubstitutionsisnecessary;
ancesfromtheinputtomaximizeROUGE.Table
thesearehigh-edit-distanceoperationsthatarenot
5 shows the performance of an oracle extractive
observedfrequentlyintheformalitydata.
model over the original SAMSum dialogues and
Formalizationwithoutanyadditionaltrainingfor
theperspectiveshiftedversions. Forcomparison,
perspective shift is, as expected, far weaker than
a simple extractive baseline—choosing the three
theperspective-shift-onlymodel.
longestutterances—andastrongabstractivemodel
The rules-based heuristic appears competitive
arealsoreported.
in ROUGE, but both the BARTScore scores and
a manualinspection ofthe output reveal that this Results Clearly,thepotential(best-case)perfor-
approachislacking. manceofamodelovertheperspectiveshifteddia-
In the next section, we explore a down- loguesisbetter;theoraclescoresoverperspective
stream task: extractive summarization. For all shifteddialoguesevenapproachthescoresofthe
extractive summarization experiments, we use abstractivemodel.
model-generated perspective shift data from the
perspective-shift-only model. We train a model 5.2 ZeroShotandSupervisedExtractive
ononlythevalidation-setPSdatatogenerateper- Summarization
spective shifts for the train set of SAMSum, and
Train/TestRegimes Acommonsummarization
we train a model on only the train-set PS data to
domainisnewsarticlesduetotherelativelywide
generateperspectiveshiftsforthevalidationsetof
availabilityofdata. Weuseanextractivesumma-
SAMSum.
rizer trained on the CNN/DM news summariza-
tioncorpus(Nallapatietal.,2016): themodelPre-
5 Application: Extractivesummarization
Summ,introducedbyLiuandLapata(2019). Pre-
Intheextractivesummarizationsetting,phrasesor SummusesBERT(Devlinetal.,2019)asaback-
sentencesaretakendirectlyfromtheinputandcom- bone to learn a representation of each sentence;
posedintoasummary. Thisisaclearfailurecase additionaldocument-leveltransformerlayerspre-
for dialogue, where sentences in the input are in dictwhethereachsentenceshouldbeincludedin
firstpersonandoftenposequestionsorcorrections theextractivesummary. Weapplythismodelforze-
Approach Perspectiveshiftedoutput
PS ONLY IgortellsJohnthathehassomuchtodoatworkandheissodemotivated.
FORMALITY + PS Igorsaysshit,hehassomuchtodoatworkandheissodemotivated.
FORMALITY ONLY I’vegotsomuchtodoatworkandI’msodemotivated.
RULES-BASED HEURISTIC IgorsaysShit,IgorhasgotsomuchtodoatworkandIgorissodemoti-
vated.
HEURISTIC + FORMALITY Igor says, "Shit, Igor has so much to do at work and Igor is so demoti-
vated."
GOLD Igorhastoomuchworkandtoolittlemotivation.
Table 4: Sample outputs for each of the perspective shift models for the input utterance “Igor: Shit, I’ve got so
muchtodoatworkandI’msodemotivated.“
Method ROUGE1 ROUGE2 ROUGEL
Extractive: Longest-3utterances 32.46 10.27 29.92
Extractive: OracleoveroriginalSAMSum 45.89 16.35 34.80
Extractive: OracleoverPSSAMSum 50.63 21.40 39.11
Abstractive: BART-largefinetuned 52.863±0.531 28.577±0.470 43.727±0.772
Table5: PerformanceoforacleextractivemodelsascomparedtothebestextractivebaselinefromtheSAMSum
paper(longest-3)andacompetitiveabstractivesystem(BART-large,averagedover5randomrestarts).
roshotsummarization4 overtheoriginalSAMSum performs better than the fully supervised model
dialogues and over a perspective-shifted version trainedovertheoriginaldialogues. Inalow-data
ofthedialogues. Wealsoconsiderthefullysuper- setting,whereannotatingtheentiredatasetforsum-
vised case; we train models using the PreSumm marization may be cost-prohibitive, perspective
architectureforextractionovertheoriginalSAM- shift can serve as an alternative annotation goal.
Sumdialoguesandovertheperspective-shifteddi- The perspective shift model used to generate the
alogues. test data in Table 6 was trained on 545 dialouges
(withavalidationsetof137dialogues);bycontrast,
Results ResultsacrossallmodelsareinTable6. annotating the entire train and validation sets for
The zeroshot modelscores higher than thesuper- summarization would require annotating 15,550
visedmodelforSAMSum,whichatfirstappears conversations,amorethan20foldincreaseinanno-
unintuitive. We credit this to 2 factors. First, the tationeffort.
trainingdatasetforCNN/DMisapproximately21x
more training examples than SAMSum train set, 5.3 AnalysisofHallucination
allowing the model increased generalizability to
One of the oft-cited benefits of extractive sum-
an unseen test set. Second, the summaries in the
marization is that models that copy text directly
CNN/DMdatasetareoftenseveralsentences,while
from the input are less likely to present factually
thesummariesintheSAMSumdatasettendtobe
incorrectsummaries(Ladhaketal.,2022). Clearly,
asinglesentence. TheCNN/DMmodel’sbiasto-
perspective shifting introduces a rephrasing step
wardlongersummarylengthmayartificiallyinflate
intothesummarizationpipeline. Anaturalconcern
ROUGE scores, as the model selects more utter-
is the potential presence of “cascading errors”—
ances for the output. Despite these factors, the
where errors in the perspective shifting process
supervisedmodeloverperspectiveshifteddataout-
lead to hallucinatory extractive summaries. We
performsthezeroshotmodeloverthesamedata.
randomlyselect100conversationsandassociated
Perspective shift is useful as an operation to
summariesfromtheperspective-shift-then-extract
bring the dialogue domain closer to the news do-
modelandastandardabstractivefinetunedBART
main. Thisdrasticallyimproveszero-shottransfer.
model. Wethenask2annotatorstolabeleachfor
Thezero-shotmodeloverperspectiveshifteddata
faithfulness–rankingthesummary-1ifitdescribes
informationthatcontradictstheconversation,0ifit
4Here we use the term zeroshot to refer to having zero
examplesinthetargetdomainofdialoguesummarization. containsinformationthatcannotbeverifiedorfal-
Trainingdata Testdata ROUGE1 ROUGE2 ROUGEL
CNN/DM SAMSum 35.00 12.09 30.76
CNN/DM PSSAMSum 37.12 13.14 31.49
SAMSum SAMSum 32.19 9.86 28.52
PSSAMSum PSSAMSum 39.58 15.03 33.94
Table6: Resultsforzero-shot(rows1–2)andsupervisedextractivemodels(rows3–4).
Model %contradict %hallucinate Tocomparethefluencyofextractionfromorigi-
extractivePS 3% 5% naldialoguesandperspectiveshifteddialogues,we
abstractive 18% 22% calculatetheperplexityoftheoutputsummariesfor
eachmodel. WemeasureperplexityusingGPT-2
Table 7: Human evaluation results for the extractive
(Radfordetal.,2019),whichisnotusedtogenerate
modeloverperspectiveshifteddataandtheabstractive
any of the outputs. The extractions from the per-
modeloveroriginalSAMSum.
spectiveshiftdialogueshaveanaverageperplexity
of 31.07, while the extractions from the original
sifiedbytheconversation,and1ifallinformation dialogueshaveanaverageperplexityof48.77. Ex-
statedinthesummaryisderivedfromtheconversa- ample outputs from each model are in Appendix
tion. Cohen’skappabetweenthesetwoannotators B.
was0.49,withannotatorsdisagreeingon12.6%of Similarly to extract-then-abstract systems, per-
summaries. Forcaseswheretheannotatorscores spectiveshiftingrepresentsacompromisebetween
differ, weaska3rdannotatortolabeltheconver- the strong faithfulness of extraction and the im-
sationandchoosethemajorityopinion. Resultsof provedfluencyofabstraction.
thisevaluationareinTable7.
6 Discussion
Whileperspectiveshiftintroducessomehalluci-
nationsintothedataset,therateofhallucinationis Another possible application of perspective shift
farlowerthanforabstractivemodels. forsummarizationisinquery-specificsummariza-
Inthe100randomlyselectedconversations,we tion, where there is not a single canonical sum-
observe 5 hallucinations introduced by the per- mary at training time. Instead, a relevant span is
spectiveshiftingoperationthatinfluencethedown- selected and summarized based on a user query.
streamsummaries. Inthesameconversationalsam- Query-specificsummarizationhasbeenappliedto
ple,22summariesfromtheabstractivemodelcon- dialogue-baseddomains,suchasmeetingsumma-
tain hallucinations, commonly in the form of in- rization(Zhongetal.,2021). Inthesedomains,we
correctlyattributingactionstoentitiesornegating conjectureperspectiveshiftmaymakethechoice
the implications from the original conversations. ofanextractivesummarizationmodelfeasible,al-
Here,wedefineahallucinationasastatementthat lowingforgreaterinterpretabilityandfaithfulness
is not verified by the source text. Some halluci- ofoutputs.
nationsaredirectlycontradictorywiththesource Perspectiveshiftalsoappearstobealesseffort-
material(contradictions);thereare3suchcontra- ful task for annotators than summarization. We
dictions in the extractive summaries and 18 such askacrowdworkertoperformperspectiveshiftand
contradictionsintheabstractivesummaries. summarization annotation for 5 hours each over
differentsetsofdialogues. Theannotatorgavethis
5.4 Fluency unsolicitedfeedback:
Extractions from text message dialogues are not [Summarization] is a completely different task
normallyconducivetoformingafluentsummary. inthatittakesalotmorementalcapacity,para-
Each message has its own speaker who may use phrasingcompleteconversationsintoaconcise
firstpersonpronouns. Additionally,messagesof- synopsis.Ineedtotakeabreak!5
ten contain slang or emojis, which are not appro-
This annotator was able to summarize conversa-
priate for a formal summary. Perspective shifted
tionsatafasterhourlyratethanperspectiveshift-
dialoguesaremoreformallywrittenanddescribe
theconversationfromasingleframeofreference. 5Quoteusedwithpermission.
ing,butreportedthattheperspectiveshifttaskwas transcripts or in an online setting, during speech
moreenjoyable. transcription.
We discuss perspective shift for different dia-
loguesubdomainsbrieflyinAppendixC.
Dialogue summarization Much of the prior
work on dialogue summarization has focused on
7 RelatedWork
incorporatingspecificfeaturesofdialogueintothe
modelarchitectureorgenerationprocess: Narayan
Style Transfer The most similar style transfer
et al. (2021) incorporate an ordered list of enti-
task is formalization, which has attracted atten-
tiestomentioninthesummary;Liuetal.(2021b)
tion as a standardization strategy for noisy user-
add coreference resolution information (labeled
generated text. Formalization can be performed
byadifferentmodel)intotheinput;Khalifaetal.
as a supervised learning task, and supervised ap-
(2021) add span markers for negations. Other
proachesoftenusetheparallelsentencepairsfrom
workfocusesonmodelingdialoguestructure: Lei
GrammarlyYahooAnswersFormalityCorpus(Rao
etal.(2021)modelthesemanticstructureofeach
and Tetreault, 2018). More commonly, however,
speaker’s contributions individually to build a
formalization is performed as a semi-supervised
globalsemanticstructure;ChenandYang(2021)
(ChawlaandYang(2020),Liuetal.(2022))orun-
extractconversationalstructurefromseveralviews
supervisedobjective(Krishnaetal.(2020),Heetal.
to feed into a multi-view decoder. Another ap-
(2020)).
proach to modeling the differences between dia-
Another related style transfer task is the
logues and well-structured text is to use auxilary
3rd to 1st person rephrasing task proposed by
tasks during training (Liu et al., 2021a). In work
GraneroMoyaandOikonomouFilandras(2021).
concurrentwiththispaper,Fangetal.(2022)pro-
Thistaskisevaluatedwithexact-matchaccuracy,
pose a narrower utterance rewriting task for dia-
andtheirbestmodelachieves92.8%accuracyon
loguesummarization,swappingsomepronounsin
the test set. We conjecture perspective shift is a
thetextforspeakernames;however,thistaskdoes
moredifficulttaskbecauseofitsmany-to-onena-
notallowforfullrephrasingsofthetextorproduce
ture,aswellastheadditionalemotionattribution
outputthatisinthirdperson,makingitunsuitable
andformalizationrequired.
forextractivesummarization.
Speaking-styletransformationSpeaking-style
transformationisataskwhichseekstotransform
aliteraltranscriptionofspokenspeechtoonethat Domain adaptation for summarization An-
omits disfluencies, filler words, repetitions, and other popular direction for dialogue summariza-
othercharacteristicsofspeechthatareundesirable tion is domain adaptation to dialogue, primarily
in written text. This task attracted notice partic- bypretrainingmodelsonadditionaldialoguedata.
ularly in the statistical machine translation com- Khalifa et al. (2021) pretrain BART on informal
munity(Neubigetal.(2012),AkitaandKawahara text before training on SAMSum, observing im-
(2007)). Otherworkhasfocusedontheoppositedi- provementwhenpretrainingondialoguecorpora
rection: convertingwrittentextintoastylemorefa- but not when training on Reddit comments. Yu
vorableforaudiolistening(Abu-Jbaraetal.,2011). et al. (2021) study the effectiveness of adding an
Morerecentworkhasfocusedonend-to-endmod- additionalphaseofpretrainingtoimprovedomain
elingwithoutthisintermediatestep(Saleskyetal. adaptation, in which they either train on a news
(2019), Jamshid Lou and Johnson (2020)). This summarization task, continue pretraining (using
task differs from perspective shift in several re- thestandardreconstructionloss)foranin-domain
spects: thefocusofspeaking-styletransformation dataset,orcontinuepretrainingonasmallerdataset
is on removing disfluencies, whereas perspective ofunlabeledinputdialoguesfromthetrainingset.
shiftaimstopreserveinformationthatmaybecon- Zouetal.(2021)pretrainanencoderondialogue
veyed by the informal style of text; perspective andadecoderonsummarytextseparatelybefore
shiftrequirescomplexcoreferenceresolutionand trainingthetwotogetheronasummarizationobjec-
utterance contextualization, while speaking-style tive. Whiletheseapproachesimproveperformance
transformationleavesreferencesunresolved; and ondialoguesummarization,particularlyinalower-
perspectiveshiftisappliedtotextpost-hoc,while resourcesetting,theylargelyrequirepretrainingat
speakingstyletranscriptionmaybeperformedover alargecomputationalcost.
8 Conclusion wetakestepstominimizethelossofemotionand
stanceinformation,thenuancesofthisinformation
Perspectiveshiftisanew,non-trivialstyletransfer
maystillbediscarded.
taskthatrequiresincorporationofcoreferencereso-
Theperspectiveshiftprocessalsodiscardsmost
lution,formalization,andemotionattribution. This
ofthediscourseinformationavailableintheorigi-
paper presents a preliminary dataset for this task
naldialogue. Byperformingperspectiveshiftprior
thatincludesinterpretationofthemeaningsofcon-
to dialogue summarization, we take a simplistic
ventionaltextabbreviations,emojis,andemoticons.
view of dialogue as a linear collection of first-
Thebaselinespresentedinthispaperaresufficient
personstatementswithoutconsideringunderlying
fordownstreamperformanceonasummarization
structure. While this approach proved effective,
task,butmaybefurtherimprovedbymodelingthe
we believe that the best possible performance on
uniquechallengesofthistaskdirection.
this task may be constrained by this simplifying
Inadditiontobeingachallengingtask,perspec-
assumption.
tive shift is a useful operation for dialogue sum-
marization. Perspectiveshiftcanactasatoolfor
domainadaptationbyshiftingdialogueintoaform References
moresimilartocommonsummarizationdomains
Amjad Abu-Jbara, Barbara Rosario, and Kent Lyons.
(e.g. news). Forextractivesystems,dialoguesum-
2011. Towards style transformation from written-
marization is largely infeasible because outputs styletoaudio-style. InProceedingsofthe49thAn-
willnotbefluent. Perspectiveshiftallowsforflu- nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
entextractivesummaries. Thisdiffersfromamore
248–253, Portland, Oregon, USA. Association for
traditionalextract-then-abstractapproachbecause
ComputationalLinguistics.
the “abstraction” (perspective shifting) step can
benefit from the full document context. In a do- Yuya Akita and Tatsuya Kawahara. 2007. Topic-
independent speaking-style transformation of lan-
mainsuchasdialogue,wheremanyutterancesare
guagemodelforspontaneousspeechrecognition. In
strongly conditioned on the prior context of the
2007 IEEE International Conference on Acoustics,
conversation,thisallowsformorefaithfulrephras- Speech and Signal Processing - ICASSP ’07, vol-
ings. Whencoupledwithanextractivesystem,this ume4,pagesIV–33–IV–36.
perspective-shifting-basedparadigmallowsforthe
Roy Bar-Haim, Lilach Eden, Roni Friedman, Yoav
creation of more interpretable, less hallucinatory Kantor,DanLahav,andNoamSlonim.2020. From
summarizationswhencomparedtoanabstractive arguments to key points: Towards automatic argu-
mentsummarization. InProceedingsofthe58thAn-
model.
nual Meeting of the Association for Computational
Otherpotentialapplicationsofperspectiveshift
Linguistics, pages 4029–4039, Online. Association
includedirectapplicationinabstractivesummariza- forComputationalLinguistics.
tion; in related tasks such as key point analysis
KunalChawlaandDiyiYang.2020. Semi-supervised
(Bar-Haim et al., 2020), which often rely on dia-
formality style transfer using language model dis-
logues as inputs; and for summarization of texts
criminator and mutual information maximization.
which contain partial dialogues, such as novels. In Findings of the Association for Computational
The general strategy of transforming the input to Linguistics: EMNLP 2020, pages 2340–2354, On-
line.AssociationforComputationalLinguistics.
adapt to a new domain rather than changing the
modelorpretrainingparadigmisapromisingdirec- Jiaao Chen and Diyi Yang. 2021. Structure-aware ab-
tionbecauseoftheeaseofannotationandrelatively stractive conversation summarization via discourse
lowcomputationalcost. andactiongraphs. InProceedingsofthe2021Con-
ferenceoftheNorthAmericanChapteroftheAsso-
ciationforComputationalLinguistics: HumanLan-
9 Limitations
guage Technologies, pages 1380–1391, Online. As-
sociationforComputationalLinguistics.
Perspective shift requires the modeling of infor-
mal language, a challenging task. The meaning Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
of informal language can vary across communi- Kristina Toutanova. 2019. BERT: Pre-training of
ties(Jørgensenetal.,2015),agegroups(Rickford deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
andPrice,2013),andtime(Jinetal.,2021),mak-
of the North American Chapter of the Association
ing generalization of these results more difficult.
for Computational Linguistics: Human Language
Thisisalsoaninherentlylossyconversion;though Technologies, Volume 1 (Long and Short Papers),
pages4171–4186,Minneapolis,Minnesota.Associ- KalpeshKrishna,JohnWieting,andMohitIyyer.2020.
ationforComputationalLinguistics. Reformulating unsupervised style transfer as para-
phrasegeneration. InProceedingsofthe2020Con-
Yue Fang, Hainan Zhang, Hongshen Chen, Zhuoye
ferenceonEmpiricalMethodsinNaturalLanguage
Ding, Bo Long, Yanyan Lan, and Yanquan Zhou.
Processing(EMNLP),pages737–762,Online.Asso-
2022. From spoken dialogue to formal summary:
ciationforComputationalLinguistics.
An utterance rewriting for dialogue summarization.
InProceedingsofthe2022ConferenceoftheNorth FaisalLadhak,EsinDurmus,HeHe,ClaireCardie,and
American Chapter of the Association for Computa- Kathleen McKeown. 2022. Faithful or extractive?
tional Linguistics: Human Language Technologies, on mitigating the faithfulness-abstractiveness trade-
pages 3859–3869, Seattle, United States. Associa- off in abstractive summarization. In Proceedings
tionforComputationalLinguistics. of the 60th Annual Meeting of the Association for
ComputationalLinguistics(Volume1:LongPapers),
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and
pages 1410–1421, Dublin, Ireland. Association for
Aleksander Wawer. 2019. SAMSum corpus: A
ComputationalLinguistics.
human-annotated dialogue dataset for abstractive
summarization. InProceedingsofthe2ndWorkshop Yuejie Lei, Fujia Zheng, Yuanmeng Yan, Keqing He,
on New Frontiers in Summarization, pages 70–79, and Weiran Xu. 2021. A finer-grain universal dia-
Hong Kong, China. Association for Computational logue semantic structures based model for abstrac-
Linguistics. tivedialoguesummarization. InFindingsoftheAs-
sociation for Computational Linguistics: EMNLP
Marcel Granero Moya and Panagiotis Agis
2021,pages1354–1364,PuntaCana,DominicanRe-
Oikonomou Filandras. 2021. Taking things
public.AssociationforComputationalLinguistics.
personally: Third person to first person rephrasing.
InProceedingsofthe3rdWorkshoponNaturalLan- Chin-Yew Lin. 2004. ROUGE: A package for auto-
guageProcessingforConversationalAI,pages1–7, maticevaluationofsummaries. InTextSummariza-
Online.AssociationforComputationalLinguistics. tion Branches Out, pages 74–81, Barcelona, Spain.
AssociationforComputationalLinguistics.
Junxian He, Xinyi Wang, Graham Neubig, and Tay-
lor Berg-Kirkpatrick. 2020. A probabilistic formu- Ao Liu, An Wang, and Naoaki Okazaki. 2022. Semi-
lation of unsupervised text style transfer. CoRR, supervisedformalitystyletransferwithconsistency
abs/2002.03912. training. In Proceedings of the 60th Annual Meet-
ingoftheAssociationforComputationalLinguistics
Matthew Honnibal, Ines Montani, Sofie Van Lan-
(Volume1:LongPapers),pages4689–4701,Dublin,
deghem, and Adriane Boyd. 2020. spaCy:
Ireland.AssociationforComputationalLinguistics.
Industrial-strength Natural Language Processing in
Python. Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen
Chen,ZhuoyeDing,CaixiaYuan,andXiaojieWang.
Paria Jamshid Lou and Mark Johnson. 2020. End-to-
2021a. Topic-awarecontrastivelearningforabstrac-
end speech recognition and disfluency removal. In
tivedialoguesummarization. InFindingsoftheAs-
Findings of the Association for Computational Lin-
sociation for Computational Linguistics: EMNLP
guistics: EMNLP 2020, pages 2051–2061, Online.
2021,pages1229–1243,PuntaCana,DominicanRe-
AssociationforComputationalLinguistics.
public.AssociationforComputationalLinguistics.
Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova,
Yang Liu and Mirella Lapata. 2019. Text summariza-
and Rada Mihalcea. 2022. Deep learning for text
tion with pretrained encoders. In Proceedings of
styletransfer: Asurvey. ComputationalLinguistics,
the 2019 Conference on Empirical Methods in Nat-
48(1):155–205.
uralLanguageProcessingandthe9thInternational
Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Joint Conference on Natural Language Processing
Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and (EMNLP-IJCNLP), pages 3730–3740, Hong Kong,
XiangRen.2021. Lifelongpretraining: Continually China.AssociationforComputationalLinguistics.
adaptinglanguagemodelstoemergingcorpora.
Zhengyuan Liu, Ke Shi, and Nancy Chen. 2021b.
AnnaJørgensen,DirkHovy,andAndersSøgaard.2015. Coreference-awaredialoguesummarization. InPro-
Challenges of studying and processing dialects in ceedingsofthe22ndAnnualMeetingoftheSpecial
social media. In Proceedings of the Workshop Interest Group on Discourse and Dialogue, pages
on Noisy User-generated Text, pages 9–18, Beijing, 509–519, Singapore and Online. Association for
China.AssociationforComputationalLinguistics. ComputationalLinguistics.
MuhammadKhalifa,MiguelBallesteros,andKathleen Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
McKeown.2021. Abagoftricksfordialoguesum- Çag˘lar Guulçehre, and Bing Xiang. 2016. Abstrac-
marization. InProceedingsofthe2021Conference tivetextsummarizationusingsequence-to-sequence
onEmpiricalMethodsinNaturalLanguageProcess- RNNs and beyond. In Proceedings of the 20th
ing, pages8014–8022, OnlineandPuntaCana, Do- SIGNLLConferenceonComputationalNaturalLan-
minican Republic. Association for Computational guage Learning, pages 280–290, Berlin, Germany.
Linguistics. AssociationforComputationalLinguistics.
Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo MingZhong,DaYin,TaoYu,AhmadZaidi,Mutethia
Simões,VitalyNikolaev,andRyanMcDonald.2021. Mutuma,RahulJha,AhmedHassanAwadallah,Asli
Planningwithlearnedentitypromptsforabstractive Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir
summarization. TransactionsoftheAssociationfor Radev.2021. QMSum:Anewbenchmarkforquery-
ComputationalLinguistics,9:1475–1492. basedmulti-domainmeetingsummarization. InPro-
ceedingsofthe2021ConferenceoftheNorthAmer-
Graham Neubig, Yuya Akita, Shinsuke Mori, and ican Chapter of the Association for Computational
Tatsuya Kawahara. 2012. A monotonic statisti- Linguistics: Human Language Technologies, pages
cal machine translation approach to speaking style 5905–5921, Online. Association for Computational
transformation. Computer Speech and Language, Linguistics.
26:349–370.
ChenguangZhu,YangLiu,JieMei,andMichaelZeng.
Alec Radford, Jeff Wu, Rewon Child, David Luan, 2021. MediaSum: A large-scale media interview
DarioAmodei,andIlyaSutskever.2019. Language datasetfordialoguesummarization. InProceedings
modelsareunsupervisedmultitasklearners. ofthe2021ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics:
RevanthRameshkumarandPeterBailey.2020. Story- Human Language Technologies, pages 5927–5934,
tellingwithdialogue:ACriticalRoleDungeonsand Online.AssociationforComputationalLinguistics.
DragonsDataset. InProceedingsofthe58thAnnual
Meeting of the Association for Computational Lin- Yicheng Zou, Bolin Zhu, Xingwu Hu, Tao Gui, and
guistics, pages 5121–5134, Online. Association for QiZhang.2021. Low-resourcedialoguesummariza-
ComputationalLinguistics. tion with domain-agnostic multi-source pretraining.
In Proceedings of the 2021 Conference on Empiri-
Sudha Rao and Joel Tetreault. 2018. Dear sir or calMethodsinNaturalLanguageProcessing,pages
madam, may I introduce the GYAFC dataset: Cor- 80–91,OnlineandPuntaCana,DominicanRepublic.
pus, benchmarks and metrics for formality style AssociationforComputationalLinguistics.
transfer. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
ComputationalLinguistics: HumanLanguageTech-
nologies, Volume 1 (Long Papers), pages 129–140,
New Orleans, Louisiana. Association for Computa-
tionalLinguistics.
JohnR.RickfordandMackenziePrice.2013. Girlzii
women: Age-grading,languagechangeandstylistic
variation. JournalofSociolinguistics,17:143–179.
Elizabeth Salesky, Matthias Sperber, and Alexander
Waibel. 2019. Fluent translations from disfluent
speechinend-to-endspeechtranslation. InProceed-
ingsofthe2019ConferenceoftheNorthAmerican
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 2786–2792, Min-
neapolis,Minnesota.AssociationforComputational
Linguistics.
CarolineTagg.2016. Heteroglossiaintext-messaging:
Performing identity and negotiating relationships
in a digital space. Journal of Sociolinguistics,
20(1):59–85.
Tiezheng Yu, Zihan Liu, and Pascale Fung. 2021.
AdaptSum: Towards low-resource domain adapta-
tion for abstractive summarization. In Proceedings
ofthe2021ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics:
Human Language Technologies, pages 5892–5904,
Online.AssociationforComputationalLinguistics.
WeizheYuan,GrahamNeubig,andPengfeiLiu.2021.
Bartscore: Evaluating generated text as text gener-
ation. In Advances in Neural Information Process-
ing Systems, volume 34, pages 27263–27277. Cur-
ranAssociates,Inc.
A Model-generatedperspectiveshiftoutputs
Original Perspectiveshifted
Crystal: <file_photo> Crystalsendsaphoto.
Irene: He’ssobig! Irenesaysthatheissobig.
Crystal: <file_photo> Crystalsendsaphoto.
Crystal: Iknowright! CrystalagreeswithIrene.
Irene: andsocute! Ireneaddsthatitissocute.
Crystal: he got so big he doesn’t fit his clothes Crystalsayshissongotsobigthathedoesnotfit
anymore hisclothesanymore.
Irene: timetogoshoppingwithmylittleboy<3 Irene tells her it is time to go shopping with her
littleboyandsendsherlove.
Crystal: yeahimjustgonnagobrankrupt Yeah,Crystalisjustgoingbrankrupt.
Irene: Letmetakehim,Ialsopromisetobuyhim IrenetellsCrystalthatshewilltakehersonshop-
something pingwithher,andthatshealsowillbuyhimsome-
thing.
Crystal: youreallywannadothat? CrystalasksifIreareallywantstodothat.
Irene: whynont? I’mhisaunt! Ireneaskswhynot,she’shisaunt.
Crystal: wellyeahit’sjustsuchadrag Yeah,Crystalsays,itisadrag.
Irene: youwerealwaysaborewhenshopping:P IrenetellsCrystalthatshewasalwaysaborewhen
justletmetakethelittleman shopping,andthatsheshouldjustlethertakethe
littleman.
Irene: wellhavefun! Irenetellshertohavefun.
Crystal: ok Crystalagrees.
Table8: ArandomlyselectedexampleconversationfromtheSAMSumdatasetwithmodel-generatedperspective
shift. Pronounerrorsarebolded;formalityerrorsareitalicized,andmisspellingsareunderlined
Original Perspectiveshifted
Maria: heyguys! Mariagreetsthegroupenthusiastically.
Maria: everythingreadyfortheconference? Maria asks if everything is ready for the confer-
ence.
Kate: yes,almost Katesaysyes,almost.
Tommy: Ithinkwewillhaveagoodpanel Tomythinkstheywillhaveagoodpanel.
Sam: Ireallyhopetherewillbesomepeople Sam tells them that he really hopes there will be
somepeople.
Tommy: weshouldhavegoodaudience Tommythinkstheyshouldhaveagoodaudience.
Maria: Areyoupreparingapowerpoint? Mariaasksiftheyarepreparingapowerpoint.
Tommy: I’llhaveaprezi Tommysayshewillhaveapre-conference.
Maria: you’repayingforit? MariaasksifTommyispayingforit.
Tommy: it’sgood,worththemoney Tommy thinks it is good and that it is worth the
money.
Maria: howmuchisit? Mariaaskshowmuchitis.
Tommy: Ibelieve$10amonthforacademics Tommy replies that he believes it is $10 a month
foracademics.
Maria: notthatbad Mariasaysit’snotthatbad.
Table9: ArandomlyselectedexampleconversationfromtheSAMSumdatasetwithmodel-generatedperspective
shift. Pronounerrorsarebolded;formalityerrorsareitalicized,andmisspellingsareunderlined
B Summarizationmodeloutputs
Originalconversation
Hannah: Hey,doyouhaveBetty’snumber?
Amanda: Lemmecheck
Hannah: <file_gif>
Amanda: Sorry,can’tfindit.
Amanda: AskLarry
Amanda: Hecalledherlasttimewewereattheparktogether
Hannah: Idon’tknowhimwell
Hannah: <file_gif>
Amanda: Don’tbeshy,he’sverynice
Hannah: Ifyousayso..
Hannah: I’dratheryoutextedhim
Amanda: Justtexthim
Hannah: Urgh.. Alright
Hannah: Bye
Amanda: Byebye
Originalextraction[zeroshot]:
Hannah: Hey,doyouhaveBetty’snumber? Hannah: I’dratheryoutextedhimAmanda: Hecalled
herlasttimewewereattheparktogether
Originalextraction[supervised]:
Hannah: Hey,doyouhaveBetty’snumber? Amanda: Hecalledherlasttimewewereatthepark
togetherAmanda: Don’tbeshy,he’sverynice
Perspectiveshiftextraction[zeroshot]:
HannahasksAmandaifshehasBetty’snumber. Amandasayssheneedstocheck. Amanda
tellsHannahnottobeshy,andthatheisverynice.
Perspectiveshiftextraction[supervised]:
AmandasaysthatLarrycalledBettylasttimetheywereattheparktogether. Hannahasks
AmandaifshehasBetty’snumber. AmandatellsHannahtoaskLarry.
Abstractivesummary:
HannahislookingforBetty’snumber. AmandatellsHannahtoaskLarry,whocalledBettylast
timetheywereattheparktogether.
Gold:
HannahneedsBetty’snumberbutAmandadoesn’thaveit. SheneedstocontactLarry.
Table10: AnexampleconversationfromtheSAMSumdatasetwithsummmariesfromeachmodelpresented,as
wellasthegoldsummary.
Originalconversation
Dima: hello!
Nada: heygirl,what’sup?
Dima: I’minahugetrouble,mylaptopisbrokenandIhavetodeliveratranslationtomorrow
@9
Nada: fuckwhathappened??
Dima: thestupidcatspilledcoffeeonit freakingout
Dima: youstillhaveyouroldlaptop? isitpossibletolendittomeplease?
Nada: nosorry,I’vegivenittomybrother-butyou’relucky! I’vetakenthesetwodaysoffso
youcantakemine
Dima: oohman! thankyousooomuch!!! ifitweren’tforTrados,Iwouldn’tbepanicking:(
Nada: noworries,ithappened... butIalwaysthinkaboutthis... likeman,weneedsome
backuplaptops!
Dima: Iknow! butIalwayschangemymindandspendthemoneyelsewherelol
Nada: yeah,butit’slikeouronlytool! soweneedtoinvestinit
Dima: yup,true!
Dima: canIcomeinanhourtopickitup?
Nada: yes:) ttyl!
Originalextraction[zeroshot]:
Nada: heygirl,what’sup? Nada: fuckwhathappened?? Dima: I’minahugetrouble,mylaptop
isbrokenandIhavetodeliveratranslationtomorrow@9
Originalextraction[supervised]:
Dima: I’minahugetrouble,mylaptopisbrokenandIhavetodeliveratranslationtomorrow
@9 Dina: thestupidcatspilledcoffeeonit I’mfreakingout! Dima: youstillhaveyour
oldlaptop? isitpossibletolendittomeplease?
Perspectiveshiftextraction[zeroshot]:
DimasayshellotoNada. NadaasksDimawhat’sup. Dimaisinhugetrouble,herlaptopisbroken
andshehastodeliveratranslationtomorrow.
Perspectiveshiftextraction[supervised]:
Dimaisinhugetrouble,herlaptopisbrokenandshehastodeliveratranslationtomorrow. Dima
asksifhecanpickupthelaptopinanhour. NadahastakentwodaysoffsoDimacantakeher
laptop..
Abstractivesummary:
Dima’slaptopisbroken. Hehastodeliveratranslationtomorrowat9. Nadalentherlaptop
toherbrother. Shetooktwodaysoff. DimawillcomeinanhourtopickupNada’slaptop.
Gold:
Dima’slaptopisbroken,ashercatspilledcoffeeonthelaptop. Dimaisworried,becauseshehas
todeliveratranslationforTradostomorrow. DimawillcometoNadainanhourtoborrowNada’s
laptop.
Table11: AnexampleconversationfromtheSAMSumdatasetwithsummmariesfromeachmodelpresented,as
wellasthegoldsummary.
C ApplicabilityofPerspectiveShiftfor C.2 MediaSum
otherdatasets
MediaSum is a dataset of NPR and CNN media
interview transcripts (Zhu et al., 2021). The av-
Whilewepresenttheperspectiveshifttaskusing
erage turn length in this dataset is substantially
textmessageconversationsasanexample,thereare
longer–37.5wordsfortheNPRtranscriptsand53.1
awidevarietyofsubdomainswithindialogue. We
words for the CNN transcripts. Correspondingly,
applytheperspectiveshiftoperationtotwoother
themodel-generatedperspectiveshiftisworse. The
domains–roleplayinggametranscriptsandmedia
modelgeneratesrepetitiouscontentintheperspec-
interviews–usingthemodeltrainedonlyondata
tiveshift. Themodelalsoperformspoorlyonmulti-
fromthetextmessageconversationdomain.
word speaker names, which are a rarity in SAM-
While the model effectively perspective shifts
Sumaswell. Asnippetofaninterviewappearsin
most short utterances, the largest issues we ob-
Table13
servedininspectionoftheseoutputsareasfollows:
1. Long utterances: The perspective shift
model performs poorly when utterances are
verylengthy,asthisisveryuncommoninthe
SAMSum dataset (average utterance length:
8.4words). Thisleadstorepetitionanddeni-
gratedperformance,especiallywhenseveral
longutterancesoccurinsequence.
2. Domain differences in formatting: Differ-
ences such as multi-word speaker names or
adding sound effects in parentheses are not
captured effectively by the model, as they
werenotencounteredattrainingtime.
Whileweleaveimprovingperspectiveshiftover
longoutputstofuturework,weprovideexamples
of perspective shifts from two different domains,
todemonstratethesepotentialpitfallsforotherre-
searchers. Thesearemodel-generatedperspective
shifts, generatedusingthemodeltrainedonlyon
perspectiveshiftforSAMSumdialogues.
C.1 CRD3
CRD3 is a dataset of Dungeons & Dragons role-
playinggametranscripts(RameshkumarandBai-
ley,2020). Dungeons&Dragonsisacollaborate
roleplayinggamewheremultipleplayersdescribe
theactionsanddialogueoftheircharactersasthe
team explores an open-ended world. While each
session of the game consists of several thousand
turns of dialogue, the CRD3 dataset sections the
sessions into smaller chunks with aligned sum-
maries. For brevity, we present only a chunk of
asession,inTable12. TheSAMSumperspective
shiftmodelservesasareasonablebaselineforthis
dataset,thoughin-domaindatawouldlikelyfurther
improveperformance.
Original Perspectiveshifted
MATT: Okay. You take your first step and you Matt says that when you take your first step and
watch something drift across the entrance from youwatchsomethingdriftacrosstheentrancefrom
wall to wall. Some faint glowing figure– and is walltowall,somefaintglowingfigure–andthenit
gone. isgone.
TRAVIS:(yelling)Idon’tlikeit! Trevoryellsthathedoesnotlikeit.
MARISHA:Shutup! Oh. Marishaisshocked.
TRAVIS:It’sinmyhead! Trevorsaysitisinhishead.
MARISHA:Oh,okay. Marishaagrees.
TALIESIN:DidittripmyDetectMagicsenseat TalyiesinasksifittripherDetectMagicsenseat
all? all.
MATT:Notmagicsense,no. Mattsaysit’snotamagicsense.
LAURA:Isitundead,orisitaghost? Lauraasksifitisanundeadoraghost.
MARISHA:(whispering)It’sDashilla! Marishasaysit’sDashilla.
TALIESIN:I’mgoingtodoanotherdetectundead Talesintellsherthatheisgoingtodoanotherone.
reallyquickly.
MATT:Okay. Givemethespecificationsonthat Mattagreesandasksforthespecifications.
one.
TALIESIN: All right. Eyes of The Grave. As an Talyiesin agrees and mentions that it is the Eyes
actionyouknowthelocationofanyundeadwithin oftheGrave,andthatasanaction,itcanalertthe
60feetofyouthatisn’tbehindtotalcoverandisn’t locationofanyundeadwithin60feetorlessthat
protected from divination magic until the end of isn’tcoveredbytotalcoverandisn’tprotectedfrom
yournextturn. divinationmagicuntiltheendofthenextturn.
MATT:Okay,gotyou. Howlongdoesitlast? Matttellsherthathegother.
TALIESIN:Sixseconds. Talesinsaysittakessixseconds.
MATT: Okay. We’ll say for the purposes of this, Matttellsherthattheywillsaythatforthepurposes
thisisareactiontoseeingthisfigurepassby. You ofthis,itisareactiontoseeingthefigurepassby,
definitelygetanundeadsensefromwhateverthis and that you definitely get an undead sense from
figure is, and then it merges with the wall and is whateverthefigureis,andthenitmergeswiththe
gone. It appeared to be loosely humanoid in the wallandisgone.
briefglimpseyousaw.
TALIESIN:Wehaveundead. Talyiesinsaystheyhaveundead.
TRAVIS:No. Trevorsaysno.
SAM:It’saghostship! Samsaysitisaghostship.
MARISHA:Isitgoingtobeyourcrew? MarishaasksifitisgoingtobeDucey’screw.
TRAVIS:Idon’tknow! Trevordoesn’tknow.
MARISHA:Wait,we’renotonyourshipanymore. Marishaasksthemtowait. Shetellsthemthatthey
arenotonDucey’sshipanymore.
TRAVIS: Now we’re in the mouth of a beast! Trevortellsherthattheyarenowinthemouthofa
Ducey,what’sgoingon,brother? beast,andasksDuceywhatishappening.
TALIESIN:Idon’tknow,I’mliterallyblind. Iwant Talesindoesn’tknow,sheisliterallyblind.
tolightthis–
Table12: AsnippetofaconversationfromCRD3withamodel-generatedperspectiveshift.
Original Perspectiveshifted
MADELELEINEBRAND,host: OK,here’ssome MADELEINEsaysthatthereissomegoodnews
good news on the jobs front for both men and on the job market for both men and women. A
women. A new survey out today from the em- new survey out today from the employment firm
ploymentfirmManpowerfindsthataboutaquarter Manpowerfindsthataboutaquarterofemployers
ofemployerswilladdjobsthissummer. That’sfor willaddjobstotheworkforcethissummer,butfor
adults,butforteenagersthissummer’sjobmarket teenagers,thesummerjobmarketistheweakestin
is shaping up to be the weakest in more than 50 morethan50years.
years.
ALEX COHEN, host: So, how do you get your AlexoCohen,host,tellsMichellethatthetypesof
teenagekidsnottospendtheentiresummerglued jobs that teens tend to go for in retail, fast food,
tothecouch? You’reabouttogetsometipsfrom and the like, that they still need people. He says
MichelleSingletary. She’sDaytoDay’spersonal thatoftentimes,listen,youmaynotgetajobatthe
financecontributor. Hi,Michelle! beginningofthesummer,butholdonbecausein
latesummer,whensomecollegestudentsaregoing
backtoschoolandperhapssomeofthepeoplewho
losttheir
MICHELLESINGLETARY:Hi! MICHellesayshello.
ALEXCOHEN,host: Sowhyisthesummerjob AlexCoopersaysthatthetypesofjobsthatteens
marketsohardforteensthisyear? tendtogoforinretail,fastfood,andthelike,they
stillneedpeople. HetellsMichellethatthey’vegot
atougheconomyandthattheyhavealotofcollege
graduates going into the job market and taking
jobs that would traditionally go to teens, like in
restaurantsandretailers,butthatforteenagers,the
summerjobmarketistheweakestinmorethan
MICHELLESINGLETARY:Lotofthingsgoing MICHellesaysthattherearealotofthingsgoing
onrightnow. We’vegotatougheconomy. We’ve oninthejobmarketrightnow,andthattheyhavea
gotalotofcollegegraduatesgoingintothemarket. tougheconomy. Shesaysthatthey’vegotanumber
Wehavepeoplewhoarelosingtheirjobsandtaking ofcollegegraduatesgoingintotheworkforceand
jobs that would traditionally go to teens, like in takingjobsthatwouldtraditionallygototeens,like
restaurantsandretailers. Andwehavealotofolder in restaurants and the like, but that for teenagers,
people holding on to their jobs and not retiring thesummerjobmarketistheweakestinmorethan
because they can’t affordto retire. And thatputs 50years.
teensattheendofthelinewhenitcomestothese
typesofjobs.
Table 13: A snippet of a conversation from MediaSum with a model-generated perspective shift. The model
performspoorlyoninputswithseverallongutterances.
D AnnotationInformation names to adding a dialogue tag like “says”
or“asks.”
Annotators were selected through two processes.
AnnotatorsonAmazonMechanicalTurkwerese- 3. Standardize grammar and punctuation. Use
lectedaftersuccessfully completingapilotanno- adverbs or rephrasing to replace emoticons
tationtask. AnnotatorsonUpWorkwereselected andtextslang.
after bidding on the contract and received feed-
4. Forattachmentssentintheconversation,use
backaftertheirfirst5annotatedconversations. Me-
theform“<SPEAKERNAME>sendsa<AT-
chanicalTurkannotatorswerecompensated$0.33
TACHMENT TYPE>,” e.g. “John sends a
forannotatingoneconversationinthepilotround
file.”
and $0.90 per conversation annotated in the an-
notationround,forestimatedhourlywagesofap-
5. References to “you” or “we” should be re-
proximately $15 per hour6. UpWork annotators
solved as best you can with the context. If
werepaidbetween$10and$15perhour,accord-
there’snowaytotellwho“we”refersto,re-
ingtotheratestheybidforthejob. Atotalof20
solveitto“they.”
MechanicalTurkworkersand6UpWorkworkers
contributed annotations for this dataset. All an- 6. Resolveambiguousreferenceswiththecon-
notationswerereviewedandlightlyeditedbythe textfromtherestoftheconversation.
authorsforquality. Weestimatethecostofrecre-
7. Questionsshouldberewrittenindescriptive
ationforthisdatasettobeapproximately$900.
form.
Theannotationinstructionsbelowwereprovided
toallannotators,alongwithanaccompanyingex-
8. Whenpossible,useonlyspeakernames. Ifthe
ampleforeachofthe8instructionsandseveralfull
sentencewouldbedisfluentwiththespeaker
conversationsannotatedasexamples.
name,usethepronounslistedintheconversa-
tionheader(thelineabovetheconversation)
D.1 Annotationinstruction
7.
The goal of the annotation is to transform from
informal 1st person to formal 3rd person, while
preservingtoneasmuchaspossible.
Do not include emoticons, slang, or informal
language; rewrite the utterance so that someone
who grew up pre-AOL-instant-messenger would
easilyinterpretthemeaning. Eachrewrittenutter-
anceshouldhavethespeaker’snameinit,butitis
notnecessarytoindicatewhoisspeakingineach
sentence. Topreservetone,youcanaddadverbsor
rewordsentencesslightly. Otherwise,trytokeep
sentencesasclosetotheoriginalaspossible.
Difficult cases should be marked by checking
the checkbox in the “Difficult?” column. If the
annotationrequiressubstantialthoughtoryouare
notconfidentinyourannotation,markthisboxand
itwillbere-reviewed.
1. Eachrewrittenutterancebyaspeakershould
have that speaker’s name in it. Not every 7ThesepronounsweregeneratedbyapplyingthePython
sentence in the utterance has to contain the package gender_guesser to the speaker names and in-
jectingsomedegreeofrandomness;optionsforspeakerpro-
speaker’snameifit’sclearfromcontext.
nounswereshe/her/hers,he/him/his,orthey/them/theirs.We
usedthisapproachtoavoidaskingannotatorstomakejudge-
2. Where possible, prefer inserting speaker mentcallsaboutthegenderednessofaname. Weusedan
automaticprocessbecausetheconversationsandspeakersin
6Toarriveatthisestimate,wetimedoneauthorannotating SAMSumaresynthetic;manualannotationforpronounuse
for20minutesandprojectedthisintoanhourlyannotation isrecommendedifusingreal-worldconversations,toavoid
rate. misgenderingparticipantsduringannotation.

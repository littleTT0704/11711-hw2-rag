ALFRED
A Benchmark for Interpreting Grounded Instructions for Everyday Tasks
MohitShridhar1 JesseThomason1 DanielGordon1 YonatanBisk1,2,3
WinsonHan3 RoozbehMottaghi1,3 LukeZettlemoyer1 DieterFox1,4
AskForALFRED.com
Abstract Goal: "Rinseoffamugandplaceitinthecoffeemaker"
2
WepresentALFRED(ActionLearningFromRealistic " frp oic mk tu hp et ch oe ffd ei erty mm aku eg r" 3 "turnandwalktothesink"
Environments and Directives), a benchmark for learning 1
"walktothecoffee
makerontheright"
a mapping from natural language instructions and ego-
centric vision to sequences of actions for household tasks.
ALFRED includes long, compositional tasks with non-
t=0 t=10 t=21
reversiblestatechangestoshrinkthegapbetweenresearch visualnavigation objectinteraction visualnavigation
benchmarks and real-world applications. ALFRED con- 5 6
4 "pickupthemugandgo "putthecleanmug
sistsofexpertdemonstrationsininteractivevisualenviron- "washthemuginthesink" backtothecoffeemaker" inthecoffeemaker"
ments for 25k natural language directives. These direc-
tives contain both high-level goals like “Rinse off a mug
and place it in the coffee maker.” and low-level language
instructions like “Walk to the coffee maker on the right.”
ALFRED tasks are more complex in terms of sequence t=27 objectinteraction t=36 visualnavigation t=50 objectinteraction
length, action space, and language than existing vision- statechanges memory
and-languagetaskdatasets. Weshowthatabaselinemodel Figure 1: ALFRED consists of 25k language directives
based on recent embodied vision-and-language tasks per- correspondingtoexpertdemonstrationsofhouseholdtasks.
formspoorlyon ALFRED,suggestingthatthereissignif- We highlight several frames corresponding to portions of
icant room for developing innovative grounded visual lan- the accompanying language instruction. ALFRED in-
guageunderstandingmodelswiththisbenchmark. volves interactions with objects, keeping track of state
1.Introduction changes,andreferencestopreviousinstructions.
A robot operating in human spaces must learn to con- nectinghumanlanguagetoactions, behaviors, andobjects
nect natural language to the world. This symbol ground- in interactive visual environments. Planner-based expert
ing [21] problem has largely focused on connecting lan- demonstrations are accompanied by both high- and low-
guagetostaticimages. However,robotsneedtounderstand level human language instructions in 120 indoor scenes in
task-oriented language, for example “Rinse off a mug and AI2-THOR2.0[26]. Thesedemonstrationsinvolvepartial
placeitinthecoffeemaker,”asillustratedinFigure1. observability, long action horizons, underspecified natural
Platformsfortranslatinglanguagetoactionhavebecome language,andirreversibleactions.
increasingly popular, spawning new test-beds [3, 12, 14,
ALFRED includes 25,743 English language directives
42]. Thesebenchmarksincludelanguage-drivennavigation
describing8,055expertdemonstrationsaveraging50steps
and embodied question answering, which have seen dra-
each, resulting in 428,322 image-action pairs. Motivated
matic improvements in modeling thanks to environments
by work in robotics on segmentation-based grasping [37],
like Matterport 3D [3, 11], AI2-THOR [26], and AI Habi-
agentsin ALFRED interactwithobjectsvisually,specify-
tat [45]. However, these datasets ignore complexities aris-
ing a pixelwise interaction mask of the target object. This
ingfromdescribingtask-orientedbehaviorswithobjects.
inference is more realistic than simple object class pre-
We introduce ALFRED, a new benchmark for con- diction, where localization is treated as a solved problem.
Existing beam-search [17, 48, 53] and backtracking solu-
1PaulG.AllenSchoolofComputerSci.&Eng.,Univ.ofWashington
tions[24,29]areinfeasibleduetothelargeractionandstate
2LanguageTechnologiesInstitute@CarnegieMellonUniversity
3AllenInstituteforAI 4NVIDIA spaces,longhorizon,andinabilitytoundocertainactions.
1
0202
raM
13
]VC.sc[
2v43710.2191:viXra
—Language— —VirtualEnvironment— —Inference—
#Human Visual Movable State
Granularity Vis.Obs. Navigation Interaction
Annotations Quality Objects Changes
TACoS[43] 17k+ High&Low Photos (cid:55) (cid:55) – – –
R2R[3];Touchdown[14] 21k+;9.3k+ Low Photos (cid:55) (cid:55) Ego Graph (cid:55)
EQA[15] (cid:55) High Low (cid:55) (cid:55) Ego Discrete (cid:55)
MatterportEQA[55] (cid:55) High Photos (cid:55) (cid:55) Ego Discrete (cid:55)
IQA[20] (cid:55) High High (cid:55) (cid:51) Ego Discrete Discrete
VirtualHome[42] 2.7k+ High&Low High (cid:51) (cid:51) 3rdPerson (cid:55) Discrete
VSP[58] (cid:55) High High (cid:51) (cid:51) Ego (cid:55) Discrete
Discrete
ALFRED 25k+ High&Low High (cid:51) (cid:51) Ego Discrete
+Mask
Table 1: Dataset comparison. ALFRED is the first interactive visual dataset to include high-level goal and low-level
naturallanguageinstructionsforobjectandenvironmentinteractions. TACoS[43]providesdetailedhigh-andlow-leveltext
descriptions of cooking videos, but does not facilitate task execution. For navigation, ALFRED enables discretized, grid-
basedmovement,whileotherdatasetsusetopologicalgraphnavigationoravoidnavigationaltogether. ALFREDrequiresan
agenttogeneratespatiallylocatedinteractionmasksforactioncommands. Bycontrast,otherdatasetsonlyrequirechoosing
fromadiscretesetofavailableinteractionsandobjectclassesoroffernointeractivecapability.
To establish baseline performance levels, we evaluate eral existing benchmarks based on simple block worlds
asequence-to-sequencemodelakintoexistingvision-and- and fully observable scenes [9, 34]. ALFRED provides
languagenavigationtasks[28]. Thismodelisnoteffective moredifficulttasksinricher,visuallycomplexscenes,and
onthecomplextasksinALFRED,achievinglessthan5% uses partially observable environments. The CHAI bench-
successrates. Foranalysis,wealsoevaluateindividualsub- mark [33] evaluates agents performing household instruc-
goals. While performance is better for isolated sub-goals, tions, but uses a generic “interact” action. ALFRED has
themodellacksthereasoningcapacityforlong-horizonand seven manipulation actions, such as pick up, turn on, and
compositionaltaskplanning. open,statechangeslikecleanversusdirty,andvariationin
In summary, ALFRED facilitates learning models that languageandvisualcomplexity.
translatefromlanguagetosequencesofactionsandinterac- Previous work in the original AI2-THOR environment
tions in a visually and physically realistic simulation envi- investigated the task of visual semantic planning [19, 58].
ronment.Thisbenchmarkcapturesmanychallengespresent Artificial language came from templates, and environment
in real-world settings for translating human language to interactionwashandledwithdiscreteclasspredictions,for
robot actions for accomplishing household tasks. Models exampleselectingappleasthetargetobjectfrompredefined
that can overcome these challenges will begin to close the options. ALFRED features human language instructions,
gaptowardsreal-world,language-drivenrobotics. and object selections are carried out with class-agnostic,
pixelwise interaction masks. In VirtualHome [42], pro-
2.RelatedWork gramsaregeneratedfromvideodemonstrationandnatural
language instructions, but inference does not involve ego-
Table1summarizesthebenefitsofALFREDrelativeto centricvisualandactionfeedbackorpartialobservability.
othervisualactiondatasetswithlanguageannotations. There is an extensive literature on language-based in-
Vision & Language Navigation. In vision-and-language structionfollowinginthenaturallanguageprocessingcom-
navigation tasks, either natural or templated language de- munity. There, research has focused on mapping instruc-
scribes a route to a goal location through egocentric vi- tions to actions [5, 13, 32, 36, 49], but these works do not
sualobservations[3,12,13,14,31]. Sincetheproposalof involvevisual,interactiveenvironments.
R2R [3], researchers have dramatically improved the nav- Embodied Question Answering. Existing datasets for
igation performance of models [17, 24, 29, 53, 54] with visual question answering in embodied environments use
techniques like progress monitoring [28], as well as in- templatedlanguageorstaticscenes[15,20,55,57].InAL-
troduced task variants with additional, on-route instruc- FRED, rather than answering a question, the agent must
tions[38,39,51]. Muchofthisresearchislimitedtostatic completeataskspecifiedusingnaturallanguage,whichre-
environments. By contrast, ALFRED tasks include navi- quiresbothnavigationandinteractionwithobjects.
gation,objectinteractions,andstatechanges. Instruction Alignment. Language annotations of videos
Vision & Language Task Completion. There are sev- enable discovering visual correspondences between words
2
Pick Stack PickTwo Clean Heat Cool Examine
&Place &Place &Place &Place &Place &Place inLight
item(s) Book Fork(in)Cup SprayBottle DishSponge PotatoSlice Egg CreditCard
receptacle Desk CounterTop ToiletTank Cart CounterTop SideTable DeskLamp
scene# Bedroom14 Kitchen10 Bathroom2 Bathroom1 Kitchen8 Kitchen21 Bedroom24
expert
demonstration
Annotation#1 Annotation#2 Annotation#3
Goals Putacleanspongeonametalrack. Placeacleanspongeonthedryingrack Putarinsedoutspongeonthedryingrack
Gototheleftandfacethefaucetsideof Turnaroundandwalkovertothebathtub Walkforwardsabitandturnlefttoface
thebathtub.Pickupleftmostgreen ontheleft.Grabthespongeoutofthe thebathtub.Grabaspongeoutofthe
Instructions spongefromthebathtub.Turnaround bathtub.Turnaroundandwalktothesink bathtub.Turnaroundandwalkforwards
andgotothesink.Putthespongeinthe ahead.Rinsethespongeoutinthesink. tothesink.Rinsethespongeoutinthe
sink.Turnonthenturnoffthewater.Take Movetotheleftabitandfacethedrying sinkandpickitupagain.Turnlefttowalk
thespongefromthesink.Gotothemetal rackinthecorneroftheroom.Placethe abit,thenfacethedryingrack.Putthe
t=8 t=24 t=42 barracktotheleft.Putthespongeonthe spongeonthedryingrack. spongeonthedryingrack.
topracktotheleftofthelotionbottle.
Figure 2: ALFRED annotations. We introduce 7 different task types parameterized by 84 object classes in 120 scenes.
Anexampleofeachtasktypeisgivenabove. FortheClean&Placedemonstration,wealsoshowthethreecrowdsourced
languagedirectives. Pleaseseethesupplementalmaterialforexampledemonstrationsandlanguageforeachtask.
and concepts [1, 46, 43, 56, 59]. ALFRED requires per- Figure2givesexamplesofthehigh-levelagenttasksin
formingtasksinaninteractivesettingasopposedtolearn- ALFRED, like putting a cleaned object at a destination.
ingfromrecordedvideos. These tasks are parameterized by the object of focus, the
RoboticsInstructionFollowing. Instructionfollowingisa destination receptacle (e.g., table top), the scene in which
long-standingtopicofinterestinrobotics[7,10,30,35,40, to carry out the task, and in the case of Stack & Place, a
41,47,52]. Linesofresearchconsiderdifferenttaskssuch baseobject(e.g.,plate). ALFREDcontainsexpertdemon-
ascooking[10], tableclearing[40], andmobilemanipula- strations of these seven tasks executed using combinations
tion[30]. Ingeneral,theyarelimitedtoafewscenes[35], of58uniqueobjectclassesand26receptacleobjectclasses
consider a small number of objects [30], or use the same across 120 different indoor scenes. For object classes like
environment for training and testing [7]. In contrast, AL- potato slice, the agent must first pick up a knife and find
FRED includes 120 scenes, many object classes with di- a potato to create slices. All object classes contain mul-
verseappearances,andatestsetofunseenenvironments. tiple visual variations with different shapes, textures, and
colors. Forexample,thereare30uniquevariantsoftheap-
3.The ALFRED Dataset ple class. Indoor scenes include different room types: 30
eachofkitchens,bathrooms,bedrooms,andlivingrooms.
TheALFREDdatasetcomprises25,743languagedirec-
For 2,685 combinations of task parameters, we gener-
tivescorrespondingto8,055expertdemonstrationepisodes.
ate three expert demonstrations per parameter set, for a
Eachdirectiveincludesahigh-levelgoalandasetofstep-
total of 8,055 unique demonstrations with an average of
by-stepinstructions. Eachexpertdemonstrationcanbede-
50 action steps. The distributions of actions steps in AL-
terministicallyreplayedintheAI2-THOR2.0simulator.
FRED demonstrations versus related datasets is given in
Figure 3. As an example, for task parameters {task: Heat
3.1.ExpertDemonstrations
& Place, object: potato, destination: counter top, scene:
Expertdemonstrationsarecomposedofanagent’sego- KITCHEN-8},wegeneratethreedifferentexpertdemonstra-
centricvisualobservationsoftheenvironmentandwhatac- tions bystarting the agent andobjects in randomly chosen
tion is taken at each timestep as well as ground-truth in- locations. Objectstart positions havesome commonsense,
teraction masks. These demonstrations are generated by a class-specificconstraints,forexampleaforkcanstartinside
planner[23]usingmetadatanotavailabletotheagentatin- adrawer,butanapplecannot.
ferencetime. Navigationactionsmovetheagentorchange Contrasting navigation-only datasets where expert
its camera orientation, while manipulation actions include demonstrations can come from an A∗ planner, our state
picking and placing objects, opening and closing cabinets spaceincludesobjectpositionsandstatechanges. Thus,to
anddrawers,andturningappliancesonandoff.Interactions generateexpertdemonstrationsweencodetheagentandob-
can involve multiple objects, such as using a knife to slice jectstates,aswellashigh-levelenvironmentdynamics,into
anapple,cleaningamuginthesink,andheatingapotatoin PlanningDomainDefinitionLanguage(PDDL)rules[18].
themicrowave. Manipulationactionsareaccompaniedbya Wethendefinetask-specificPDDLgoalconditions,forex-
groundtruthsegmentationofthetargetobject. ample that a heated potato is resting on a table top. Note
3
Train Validation Test Steps in Expert Demonstration Instruction Length
600
Seen Unseen Seen Unseen
200 500
#Annotations 21,023 820 821 1,533 1,529
#Scenes 108 88 4 107 8 150 400
300
100
Table2: ALFREDDataSplits. Allexpertdemonstrations 200
andassociatedlanguagedirectivesinthevalidationandtest 50
100
foldsaredistinctfromthoseinthetrainfold.Thevalidation
0 0
andtestsetsaresplitintoseenandunseenfolds. Scenesin ALFRED TD VH R2R ALFRED TD VH R2R
+8k +9k ~3k +7k +25k +9k ~3k +21k
theseenfoldsofvalidationandtestdataaresubsetsofthose
in the train fold. Scenes in the unseen validation and test Figure 3: Comparison to Existing Datasets. Expert
foldsaredistinctfromthetrainfoldsandfromeachother. demonstration steps and instruction tokens of ALFRED
comparedtootherdatasetswithhumanlanguageforaction
sequences: Touchdown(TD)[14],VirtualHome(VH)[42],
that the planner encodes the environment as fully observ- andRoom-to-Room(R2R)[3].Thetotalnumberofdemon-
ableandhasperfectknowledgeaboutworlddynamics. For strationsorannotationsisgivenwiththedatasetlabel.
trainingandtestingagentmodels,however,theenvironment
ispartiallyobservable:itisonlyviewedthroughtheagent’s
egocentricvisionasactionsarecarriedout. annotationswithoutthevideo. Theworkerselectswhether
Wesplittheseexpertdemonstrationsintotraining, vali- the three directives describe the same actions, and if not,
dation, andtestfolds(Table2). Followingworkinvision- whichismostdifferent. Ifadirectiveischosenasmostdif-
and-languagenavigation[3],wefurthersplitthevalidation ferentbyamajorityofvalidationworkers,itisremovedand
andtestintotwoconditions:seenandunseenenvironments. the demonstration is subsequently re-annotated by another
Thissplitfacilitatesexamininghowwellmodelsgeneralize worker. Qualitatively,theserejectedannotationscontainin-
toentirelynewspaceswithnovelobjectclassvariations. correct object referents (e.g., “egg” instead of “potato”) or
directions(e.g.,“golefttowards...” insteadof“right”).
3.2.LanguageDirectives
4.BaselineModels
Foreveryexpertdemonstration,wecollectopenvocab-
ulary,free-formlanguagedirectivesfromatleastthreedif-
AnagenttrainedforALFREDtasksneedstojointlyrea-
ferent annotators using Amazon Mechanical Turk (AMT),
sonovervisionandlanguageinputandproduceasequence
resultingin25ktotallanguagedirectives. Languagedirec-
oflow-levelactionstointeractwiththeenvironment.
tives include a high-level goal together with low-level in-
structions, as shown in Figures 1 and 2. The distribution 4.1.Sequence-to-SequenceModels
of language annotation token lengths in ALFRED versus
We model the interactive agent with a CNN-LSTM
relateddatasetsisgiveninFigure3.
AMT workers are told to write instructions to tell a
sequence-to-sequence(SEQ2SEQ)architecture.ACNNen-
odesthevisualinput,abidirectional-LSTMgeneratesarep-
“smartrobot”howtoaccomplishwhatisshowninavideo.
resentationofthelanguageinput,andadecoderLSTMin-
We create a video of each expert demonstration and seg-
fers a sequence of low-level actions while attending over
ment it such that each segment corresponds to an instruc-
theencodedlanguage.SeeFigure4foranoverviewandthe
tion. We consult the PDDL plan for the expert demon-
supplementarymaterialforimplementationdetails.
stration to identify task sub-goals, for example the many
low-level steps to navigate to a knife, or the several steps
to heat a potato slice in the microwave once standing in Supervision. We train all models using imitation learn-
front of it. We visually highlight action sequences related ingonexperttrajectories. Thisensuresthelanguagedirec-
to sub-goals via colored timeline bars below the video. In tives match the visual inputs. At each timestep, the model
eachHIT(HumanIntelligenceTask),aworkerwatchesthe istrainedtoproducetheexpertactionandassociatedinter-
video, then writes low-level, step-by-step instructions for actionmaskformanipulationactions.
eachhighlightedsub-goalsegment. Theworkeralsowrites We note that a DAgger-style [44] student-forcing
ahigh-levelgoalthatsummarizeswhattherobotshouldac- paradigminALFREDisnon-trivial,evendisregardinglan-
complishduringtheexpertdemonstration. guage alignment. Obtaining expert demonstration actions
These directives are validated through a second HIT by ontheflyinnavigation-onlydatasetslikeR2R[3]onlyre-
atleasttwoannotators,withapossiblethirdtie-breaker.For quires rerunning A∗. In ALFRED, on the fly demonstra-
validation, we show a worker all three language directive tionsrequiresre-planning. Insamecasesre-planningisnot
4
Act Reason Act Reason
LSTM LSTM
... counter then turn right. Put the cup in the sink then fill ... ... Put the
RotateRight PutObject
ResNet Conv
not used with ...
nav actions
predicted
mask
DeConv (Frozen) Linear DeConv
Figure 4: Model overview. At each step, our model reweights the instruction based on the history (xˆ ), and combines the
t
current observation features (v ) and the previously executed action (a ). These are passed as input to an LSTM cell to
t t−1
producethecurrenthiddenstate. Finally,thenewhiddenstate(h )iscombinedwiththepreviousfeaturestopredictboththe
t
nextaction(a )andapixelwiseinteractionmaskovertheobservedimagetoindicateanobject.
t
possible: ifduringataskof{Clean&Place,apple,refrig- where W are learnable parameters of a fully-connected
x
erator,KITCHEN-3}astudent-forcingmodelslicestheonly layer, z
t
is a vector of scalar values that represent the at-
appleinthescene,theactioncannotberecoveredfromand tentionmassforeachwordinx,andxˆ istheweightedsum
t
thetaskcannotbecompleted. ofxovertheattentiondistributionα inducedfromz .
t t
Visual encoding. Each visual observation o is encoded
t
Action decoding. At each timestep t, upon receiving a
withafrozenResNet-18[22]CNN,wherewetaketheout-
new observation image o , the LSTM decoder takes in the
put of the final convolution layer to preserve spatial infor- t
visualfeaturev ,languagefeaturexˆ ,andthepreviousac-
mation necessary for grounding specific objects in the vi- t t
tiona ,andoutputsanewhiddenstateh :
sualframe.Weembedthisoutputusingtwomore1×1con- t−1 t
volution layers and a fully-connected layer. During train-
ing, asetofT observationsfromtheexpertdemonstration u t =[v t;xˆ t;a t−1],
(2)
isencodedasV = (cid:104)v ,v ,...,v (cid:105), wherev isthevisual
1 2 T t h =LSTM(u ,h )
t t t−1
featurevectorattime-stept.
where[;]denotesconcatenation.Thehiddenstateh isused
Language encoding. Given a natural language goal t
toobtaintheattentionweightedlanguagefeaturexˆ .
G = (cid:104)g ,g ,...g (cid:105) of L words, and step-by- t+1
1 2 Lg g
step instructions S = (cid:104)s ,s ...s (cid:105) of L words,
1 2 Ls s
we append them into a single input sequence X = Action and mask prediction. The agent interacts with
(cid:104)g 1,g 2,...g Lg,<SEP>,s 1,s 2...s Ls(cid:105) with the <SEP> to- theenvironmentbychoosinganactionandproducingapix-
ken indicating the separation between the high-level goal elwisebinarymaskindicatingaspecificobjectintheframe.
and low-level instructions. This sequence is fed into a bi- AlthoughAI2-THORsupportscontinuouscontrolforagent
directional LSTM encoder to produce an encoding x = navigationandobjectmanipulation,wediscretizetheaction
{x 1,x 2,...,x Lg+Ls}foreachwordinX. space. The agent chooses from among 13 actions. There
are 5 navigation actions: MoveAhead, RotateRight,
Attention over language. The agent’s action at each RotateLeft, LookUp, and LookDown together with
timestepisbasedonanattentionmechanismweightingto- 7 interaction actions: Pickup, Put, Open, Close,
kens in the instruction. We perform soft-attention on the ToggleOn,ToggleOff,andSlice.Interactionactions
languagefeaturesxtocomputetheattentiondistributionα t require a pixelwise mask to denote the object of interest.1
conditioned on the hidden state of the decoder h t−1 from Finally,theagentpredictsaStopactiontoendtheepisode.
thelasttimestep: We concatenate the hidden state h with the input features
t
z =(W h )(cid:62)x, u tandtraintwoseparatenetworkstopredictthenextaction
t x t−1
α
t
=Softmax(z t), (1) 1ThefinalobjectchosenbytheinteractionAPIisbasedontheIntersection-
over-Union(IoU)scorebetweenthepredictedmaskandtheground-truth
xˆ =α(cid:62)x objectmaskfromthesimulator.
t t
5
a andinteractionmaskm : 5.Experiments
t t
a =argmax(W [h ;u ]), WeevaluatethebaselinemodelsintheAI2-THORsim-
t a t t
(3) ulator. Whenevaluatingontestfolds, werunmodelswith
m
t
=σ(deconv[h t;u t])
thelowestvalidationloss. Episodesthatexceed1000steps
where W are learnable parameters of a fully connected orcausemorethan10failedactionsareterminated. Failed
a
layer,deconvisathree-layerdeconvolutionnetwork,andσ actions arise from bumping into walls or predicting action
isasigmoidactivationfunction. Actionselectionistrained interactionmasksforincompatibleobjects,suchasattempt-
using softmax cross entropy with the expert action. The ingtoPickupacountertop. Theselimitationsencourage
interaction masks are learned end-to-end in a supervised efficiencyand reliability. We assesstheoveralland partial
manner based on ground-truth object segmentations using successofmodels’taskexecutionsacrossepisodes.
binary cross-entropy loss. The mask loss is rebalanced to
5.1.EvaluationMetrics
account for sparsity in these dense masks in which target
objectscantakeupasmallportionofthevisualframe. ALFRED allows us to evaluate both full task and task
goal-condition completion. In navigation-only tasks, one
4.2.ProgressMonitors
canonlymeasurehowfartheagentisfromthegoal.InAL-
ALFRED tasks require reasoning over long sequences FRED, we can also evaluate whether task goal-conditions
ofimagesandinstructionwords. Weproposetwoauxiliary have been completed, for example that a potato has been
losses (Eq. 4 & 5) that use additional temporal informa- sliced. Forallofourexperiments,wereportbothTaskSuc-
tiontoreducethisburdenandformasequence-to-sequence cessandGoal-ConditionSuccess. EachGoal-Conditionre-
modelwithprogressmonitoring(SEQ2SEQ+PM). lies on multiple instructions, for example navigating to an
Maetal.[28]showedthatagentsbenefitfrommaintain- objectandthenslicingit.
ing an internal estimate of their progress towards the goal
fornavigationtasks.Akintolearningavaluefunctioninre- Task Success. Each expert demonstration is parameter-
inforcementlearning,progressmonitoringhelpstolearnthe ized by a task to be performed, as illustrated in Figure 2.
utility of each state in the process of achieving the overall TaskSuccessisdefinedas1iftheobjectpositionsandstate
task. Intuitively, thisallowsouragenttobetterdistinguish changescorrespondcorrectlytothetaskgoal-conditionsat
betweenvisuallysimilarstatessuchasjustbeforeputtingan the end of the action sequence, and 0 otherwise. Consider
object in the microwave versus just after taking the object thetask: “Putahotpotatosliceonthecounter”. Theagent
out. We introduce a simple module that predicts progress, succeeds if, at the end of the episode, any potato slice ob-
p t ∈ [0,1],conditionedonthedecoderhiddenstateh t and ject has changed to the heated state and is resting on any
theconcatenatedinputu t: countertopsurface.
p =σ(W [h ;u ]). (4)
t p t t
Goal-Condition Success. The goal-condition success of
Thesupervisionforp tisbasedonnormalizedtime-stepval- amodelistheratioofgoal-conditionscompletedattheend
uest/T,wheretisthecurrenttime-step,andT isthetotal ofanepisodetothosenecessarytohavefinishedatask. For
lengthoftheexpertdemonstration(trainedviaL2loss). example, inthepreviousHeat&Placeexample, thereare
We also train the agent to predict the number of sub- four goal-conditions. First, a potato must be sliced. Sec-
goals completed so far, c t. These sub-goals represent seg- ond, a potato slice should become heated. Third, a potato
ments in the demonstration corresponding to sequences of sliceshouldcometorestonacountertop. Fourth,thesame
actionslikenavigation,pickup,andheatingasidentifiedin potatoslicethatisheated shouldbeonthecountertop. If
the PDDL plan, discussed in Section 3.2. Each segment theagentslicesapotato, thenmovesaslicetothecounter
hasacorrespondinglanguageinstruction,butthealignment topwithoutheatingit,thenthegoal-conditionsuccessscore
must be learned. This sub-goal prediction encourages the is 2/4 = 50%. On average, tasks in ALFRED have 2.55
agenttocoarselytrackitsprogressthroughthelanguagedi- goalconditions. Thefinalscoreiscalculatedastheaverage
rective. Thispredictionisalsoconditionedonthedecoder goal-condition success of each episode. Task success is 1
hiddenstateh tandtheconcatenatedinputu t: onlyifgoal-conditionsuccessis1.
c =σ(W [h ;u ]). (5)
t c t t
Path Weighted Metrics. We include a Path Weighted
Wetrainc inasupervisedfashionbyusingthenormalized versionofbothmetricsthatconsidersthelengthoftheex-
t
number of sub-goals accomplished in the expert trajectory pertdemonstration[2]. Expertdemonstrationsfoundviaa
ateachtimestep,c /C,astheground-truthlabelforatask PDDLsolveronglobalinformationarenotguaranteedtobe
t
withC sub-goals. WeagaintrainwithanL2loss. optimal. However,theyavoidexploration,useshortestpath
6
Validation Test
Seen Unseen Seen Unseen
Model Task Goal-Cond Task Goal-Cond Task Goal-Cond Task Goal-Cond
NOLANGUAGE 0.0(0.0) 5.9(3.4) 0.0(0.0) 6.5(4.7) 0.2(0.0) 5.0(3.2) 0.2(0.0) 6.6(4.0)
NOVISION 0.0(0.0) 5.7(4.7) 0.0(0.0) 6.8(6.0) 0.0(0.0) 3.9(3.2) 0.2(0.1) 6.6(4.6)
GOAL-ONLY 0.1(0.0) 6.5(4.3) 0.0(0.0) 6.8(5.0) 0.1(0.1) 5.0(3.7) 0.2(0.0) 6.9(4.4)
INSTRUCTIONS-ONLY 2.3(1.1) 9.4(6.1) 0.0(0.0) 7.0(4.9) 2.7(1.4) 8.2(5.5) 0.5(0.2) 7.2(4.6)
SEQ2SEQ 2.4(1.1) 9.4(5.7) 0.1(0.0) 6.8(4.7) 2.1(1.0) 7.4(4.7) 0.5(0.2) 7.1(4.5)
+PMPROGRESS-ONLY 2.1(1.1) 8.7(5.6) 0.0(0.0) 6.9(5.0) 3.0(1.7) 8.0(5.5) 0.3(0.1) 7.3(4.5)
+PMSUBGOAL-ONLY 2.1(1.2) 9.6(5.5) 0.0(0.0) 6.6(4.6) 3.8(1.7) 8.9(5.6) 0.5(0.2) 7.1(4.5)
+PMBoth 3.7(2.1) 10.0(7.0) 0.0(0.0) 6.9(5.1) 4.0(2.0) 9.4(6.3) 0.4(0.1) 7.0(4.3)
HUMAN - - - - - - 91.0(85.8) 94.5(87.6)
Table3: TaskandGoal-ConditionSuccess. Foreachmetric,thecorrespondingpathweightedmetricsaregiveninparen-
theses. Thehighestvaluesperfoldandmetricareshowninblue. Allvaluesarepercentages.
navigation, and are generally efficient. The path weighted language datasets focused on navigation, where sequence-
scorep formetricsisgivenas to-sequencewithprogressmonitoringperformswell[28].
s
L∗
p =s× (6) 6.1.RandomAgent
s max(L∗,Lˆ)
A random agent is commonly employed as a baseline
where Lˆ is the number of actions the model took in the in vision-and-language tasks. In ALFRED, an agent that
episode, and L∗ is the number of actions in the expert chooses a uniform random action and generates a uniform
demonstration. Intuitively, amodelreceiveshalf-creditfor random interaction mask at each timestep achieves 0% on
takingtwiceaslongastheexperttoaccomplishatask. allfolds,evenwithoutanAPIfailurelimit.
5.2.Sub-GoalEvaluation
6.2.UnimodalAblations
Completing the entire sequence of actions required to
Previousworkestablishedthatlearnedagentswithoutvi-
finish a task is challenging. In addition to assessing full
sualinputs,languageinputs,orbothperformedbetterthan
tasksuccess,westudytheabilityofamodeltoaccomplish
random agents and were competitive with initial baselines
the next sub-goal conditioned on the preceding expert se-
for several navigation and question answering tasks [50].
quence. Theagentistestedbyfirstforcingittofollowthe
These performance gaps were due to structural biases in
expertdemonstrationtomaintainahistoryofstatesleading
the datasets or issues with model capacity. We evaluate
uptothesub-goal,thenrequiringittocompletethesub-goal
theseablationbaselines(NO LANGUAGEandNO VISION)
conditionedontheentirelanguagedirectiveandcurrentvi-
tostudyvisionandlanguagebiasinALFRED.
sualobservation. Forthetask“Putahotpotatosliceonthe
TheunimodalablationperformancesinTable3indicate
counter”forexample,wecanevaluatethesub-goalofnav-
that both vision and language modalities are necessary to
igating to the potato after using the expert demonstration
accomplish the tasks in ALFRED. The NO LANGUAGE
tonavigatetoandpickupaknife. Thetasksin ALFRED
modelfinishessomegoal-conditionsbyinteractingwithfa-
containonaverage7.5suchsub-goals(resultsinTable4).
miliarobjectsseenduringtraining. TheNO VISIONmodel
similarly finishes some goal-conditions by following low-
6.Analysis
level language instructions for navigation and memorizing
interactionmasksforcommonobjectslikemicrowavesthat
Results from our experiments are presented in Table 3.
arecenteredinthevisualframe.
We find that the initial model, without spatial or seman-
tic maps, object segmentations, or explicit object-state
6.3.ModelAblations
tracking, performs poorly on ALFRED’s long-horizon
tasks with high-dimensional state-spaces. The SEQ2SEQ We additionally ablate the amount of language super-
model achieves ∼8% goal-condition success rate, show- vision available to the model, as language directives are
ing that the agent does learn to partially complete some given as both a high-level goal and step-by-step instruc-
tasks. This headroom (as compared with humans) mo- tions. Providing only high-level, underspecified goal lan-
tivates further research into models that can perform the guage (GOAL-ONLY) is insufficient to complete the tasks,
complex vision-and-language planning introduced by AL- butisenoughtocompletesomegoal-conditions. Usingjust
FRED.Theperformancestarklycontrastsothervision-and- low-level,step-by-stepinstructions(INSTRUCTIONS-ONLY
7
performssimilarlytousingbothhigh-andlow-levels.Thus, Sub-GoalAblations-Validation
p e
t th iois ns ti om pp lale nm ouo td se ul bd -o ge os an lsot fos ree sm tept -o be yx -sp tl eo pit et xh ee cg uo tia ol ni .nstruc-
Model
Goto Picku
Put
Cool Heat Clean Slice Toggl
Avg.
The two progress monitoring signals are marginally
NoLang 28 22 71 89 87 64 19 90 59
helpful, increasing the success rate from ∼1% to ∼2%.
S2S 49 32 80 87 85 82 23 97 67
Progress monitoring leads to more efficient task comple- S2S+PM 51 32 81 88 85 81 25 100 68
tion, as indicated by the consistently higher path weighted
NoLang 17 9 31 75 86 13 8 4 30
scores. Theymayhelpavoidactionrepetitionandwiththe
S2S 21 20 51 94 88 21 14 54 45
predictionoftheStopaction.
S2S+PM 22 21 46 92 89 57 12 32 46
The agent takes more steps than the expert in all cases,
asindicatedbythelowerpathweightedscores. Sometimes,
thisiscausedbyfailingtokeeptrackofstate-changes, for Table4: Evaluationsbypathweightedsub-goalsuccess.
exampleheatingupanegginthemicrowavemultipletimes. Allvaluesarepercentages. Thehighestvaluesperfoldand
Further, the models also do not generalize well to unseen taskareshowninblue. WenotethattheNOVISIONmodel
scenes, due to the overall visual complexity in ALFRED achieves less than 2% on all sub-goals. See supplemental
arisingfromnewscenesandnovelobjectclassinstances. materialformore.
6.4.Humanevaluation
7.Conclusions
Weobtainedahumanevaluationof100randomlysam-
pled directives from the unseen test fold. The experiment
WeintroducedALFRED ,abenchmarkforlearningto
involved 5 participants who completed 20 tasks each us-
map natural language instructions and egocentric vision to
ing a keyboard-and-mouse interface. Before the experi-
sequencesofactions. ALFREDmovesusclosertoacom-
ment, the participants were allowed to familiarize them-
munity goal of language-driven robots capable of naviga-
selves with AI2-THOR. The action-space and task restric-
tionandinteraction. Theenvironmentdynamicsandinter-
tionswereidenticaltothatofthebaselinemodels. Overall,
action mask predictions required in ALFRED narrow the
theparticipantsobtainedahighsuccessrateof91%,while
gap between what is required of agents in simulation and
takingslightlylongerthantheexpertwith86%path-length
robotsoperatingintherealworld[37].
weighted success rate. This indicates that the directives in
We use ALFRED to evaluate a sequence-to-sequence
ALFREDarewell-alignedwiththedemonstrations.
model with progress monitoring, shown to be effective in
othervision-and-languagenavigationtasks[28]. Whilethis
6.5.Sub-GoalPerformance
model is relatively competent at accomplishing some sub-
We also examine performance of the SEQ2SEQ model goals (e.g. operating microwaves is similar across Heat &
onindividualsub-goalsin ALFRED.Forthisexperiment, Place tasks), the overall task success rates are poor. The
weusetheexperttrajectorytomovetheagentthroughthe long horizon of ALFRED tasks poses a significant chal-
episodeuptothesub-task.Then,theagentbeginsinference lengewithsub-problemsincludingvisualsemanticnaviga-
basedonthelanguagedirectiveandcurrentvisualframe. tion, object detection, referring expression grounding, and
action grounding. These challenges may be approachable
Table 4 presents path-length weighted success scores
bymodelsthatexploithierarchy[8,27],modularity[4,16],
for 8 sub-goals. Goto and Pickup sub-tasks with the
SEQ2SEQ+PM model achieve ∼51% and ∼32%, respec- andstructuredreasoningandplanning[6]. Weareencour-
tively,eveninseenenvironments. Visualsemanticnaviga- agedbythepossibilitiesandchallengesthatthe ALFRED
tion is considerably harder in unseen environments. Simi- benchmarkintroducestothecommunity.
larly, interactionmasksforPickupactionsinunseenenvi-
ronmentsareworseduetounfamiliarscenesandobjectin- Acknowledgements
stances. Simplesub-goalslikeCool,andHeatareachieved
at a high success rate of ∼90% because these tasks are ThankstoourUWcolleaguesforhelpfulfeedback,toEli
mostly object-agnostic. For example, the agent becomes VanderBilt and Eric Kolve for their help with AI2-THOR
familiarwithusingmicrowavestoheatthingsregardlessof and leaderboard setup, and Victor Zhong for early mod-
the object in-hand, because microwaves have little visual eling design. And finally, thanks to Ranjay Krishna for
diversityacrosskitchens. Overall,thesub-goalevaluations sharingtheMechanicalTurkannotationinterface. Thisre-
indicatethatmodelsthatexploitmodularityandhierarchy, searchwassupportedinpartbytheARO(ARO-W911NF-
ormakeuseofpretrainedobjectsegmentationmodels,may 16-1-0121),theNSF(IIS1252835,IIS-1562364,NSF-NRI-
makeheadwayonfulltasksequences. 1637479),andtheAllenInstituteforArtificialIntelligence.
8
neeS
neesnU
References [17] DanielFried,RonghangHu,VolkanCirik,AnnaRohrbach,
Jacob Andreas, Louis-Philippe Morency, Taylor Berg-
[1] Jean-BaptisteAlayrac,PiotrBojanowski,NishantAgrawal,
Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell.
IvanLaptev, JosefSivic, andSimonLacoste-Julien. Unsu-
Speaker-follower models for vision-and-language naviga-
pervisedlearningfromnarratedinstructionvideos.InCVPR,
tion. InNeurIPS,2018. 1,2
2016. 3
[18] Malik Ghallab, Adele Howe, Craig Knoblock, Drew Mc-
[2] Peter Anderson, Angel Chang, Devendra Singh Chaplot,
Dermott, AshwinRam, ManuelaVeloso, DanielWeld, and
Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana
David Wilkins. Pddl the planning domain definition lan-
Kosecka,JitendraMalik,RoozbehMottaghi,ManolisSavva,
guage. 1998. 3
etal. Onevaluationofembodiednavigationagents. arXiv
[19] Daniel Gordon, Dieter Fox, and Ali Farhadi. What should
preprintarXiv:1807.06757,2018. 6
i do now? marrying reinforcement learning and symbolic
[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark
planning. arXivpreprintarXiv:1901.01492,2018. 2
Johnson, Niko Su¨nderhauf, Ian Reid, Stephen Gould, and
[20] Daniel Gordon, Aniruddha Kembhavi, Mohammad Raste-
Anton van den Hengel. Vision-and-Language Navigation:
gari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa:
Interpretingvisually-groundednavigationinstructionsinreal
Visual question answering in interactive environments. In
environments. InCVPR,2018. 1,2,4,13
CVPR,2017. 2
[4] JacobAndreas, MarcusRohrbach, TrevorDarrell, andDan
[21] StevanHarnad. Thesymbolgroundingproblem. PhysicaD,
Klein. Neuralmodulenetworks. InCVPR,June2016. 8
42:335–346,1990. 1
[5] YoavArtziandLukeZettlemoyer. Weaklysupervisedlearn-
[22] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
ingofsemanticparsersformappinginstructionstoactions.
Deep residual learning for image recognition. In CVPR,
TACL,2013. 2
2016. 5,12
[6] Masataro Asai and Alex Fukunaga. Classical planning [23] Jo¨rgHoffmannandBernhardNebel.Theffplanningsystem:
in deep latent space: Bridging the subsymbolic-symbolic Fastplangenerationthroughheuristicsearch. JAIR,2001. 3
boundary. InAAAI,2018. 8
[24] LiyimingKe, XiujunLi, YonatanBisk, AriHoltzman, Zhe
[7] MichaelBeetz,UlrichKlank,IngoKresse,AlexisMaldon- Gan,JingjingLiu,JianfengGao,YejinChoi,andSiddhartha
ado,LorenzMo¨senlechner,DejanPangercic,ThomasRu¨hr, Srinivasa. Tacticalrewind: Self-correctionviabacktracking
andMoritzTenorth. Roboticroommatesmakingpancakes. invision-and-languagenavigation. InCVPR,2019. 1,2
InIEEE-RAS,2011. 3 [25] Diederik P Kingma and Jimmy Ba. Adam: A method for
[8] YonatanBisk, DanielMarcu, andWilliamWong. Towards stochastic optimization. arXiv preprint arXiv:1412.6980,
adatasetforhumancomputercommunicationviagrounded 2014. 13
languageacquisition. InAAAIWorkshoponSymbioticCog- [26] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vander-
nitiveSystems,2016. 8 Bilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke
[9] YonatanBisk,DenizYuret,andDanielMarcu. Naturallan- Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An
guagecommunicationwithrobots. InNAACL,2016. 2 Interactive 3D Environment for Visual AI. arXiv preprint
[10] Mario Bollini, Stefanie Tellex, Tyler Thompson, Nicholas arXiv:1712.05474,2017. 1,18
Roy, and Daniela Rus. Interpreting and executing recipes [27] TejasDKulkarni,KarthikNarasimhan,ArdavanSaeedi,and
withacookingrobot. InISER,2012. 3 JoshTenenbaum. Hierarchicaldeepreinforcementlearning:
[11] AngelChang,AngelaDai,ThomasFunkhouser,MaciejHal- Integratingtemporalabstractionandintrinsicmotivation. In
ber,MatthiasNiessner,ManolisSavva,ShuranSong,Andy NeurIPS,pages3675–3683,2016. 8
Zeng,andYindaZhang.Matterport3d:LearningfromRGB- [28] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib,
Ddatainindoorenvironments. 3DV,2017. 1 Zsolt Kira, Richard Socher, and Caiming Xiong. Self-
[12] Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, monitoring navigation agent via auxiliary progress estima-
Rama Kumar Pasumarthi, Dheeraj Rajagopal, and Rus- tion. InICLR,2019. 2,6,7,8
lan Salakhutdinov. Gated-attention architectures for task- [29] Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming
orientedlanguagegrounding. InAAAI,2017. 1,2 Xiong,andZsoltKira. Theregretfulagent: Heuristic-aided
[13] DavidLChenandRaymondJMooney.Learningtointerpret navigationthroughprogressestimation. InCVPR,2019. 1,
naturallanguagenavigationinstructionsfromobservations. 2
InAAAI,2011. 2 [30] James MacGlashan, Monica Babes-Vroman, Marie des-
[14] HowardChen,AlaneSuhr,DipendraMisra,NoahSnavely, Jardins, Michael L. Littman, Smaranda Muresan, Shawn
and Yoav Artzi. Touchdown: Natural language navigation Squire, Stefanie Tellex, Dilip Arumugam, and Lei Yang.
andspatialreasoninginvisualstreetenvironments.InCVPR, Groundingenglishcommandstorewardfunctions. InRSS,
2019. 1,2,4 2015. 3
[15] AbhishekDas,SamyakDatta,GeorgiaGkioxari,StefanLee, [31] MattMacMahon,BrianStankiewicz,andBenjaminKuipers.
DeviParikh,andDhruvBatra. EmbodiedQuestionAnswer- Walkthetalk: Connectinglanguage,knowledge,andaction
ing. InCVPR,2018. 2 inrouteinstructions. InAAAI,2006. 2
[16] Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, [32] JonathanMalmaud,EarlWagner,NancyChang,andKevin
and Dhruv Batra. Neural Modular Control for Embodied Murphy. Cookingwithsemantics. InACLWorkshoponSe-
QuestionAnswering. InCoRL,2018. 8 manticParsing,2014. 2
9
[33] Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind [49] Moritz Tenorth, Daniel Nyga, and Michael Beetz. Under-
Niklasson,MaxShatkhin,andYoavArtzi. Mappinginstruc- standingandexecutinginstructionsforeverydaymanipula-
tionstoactionsin3Denvironmentswithvisualgoalpredic- tiontasksfromtheworldwideweb. InICRA,2010. 2
tion. InEMNLP,2018. 2 [50] JesseThomason,DanielGordon,andYonatanBisk.Shifting
[34] DipendraMisra, JohnLangford, andYoavArtzi. Mapping thebaseline:Singlemodalityperformanceonvisualnaviga-
instructions and visual observations to actions with rein- tion&qa. InNAACL,2019. 7
forcementlearning. InEMNLP,2017. 2 [51] JesseThomason,MichaelMurray,MayaCakmak,andLuke
[35] Dipendra Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Zettlemoyer. Vision-and-dialognavigation. InCoRL,2019.
Saxena. Tellmedaved: Context-sensitivegroundingofnat- 2
urallanguagetomanipulationinstructions. InRSS,2014. 3 [52] Jesse Thomason, Shiqi Zhang, Raymond Mooney, and Pe-
[36] DipendraMisra,KejiaTao,PercyLiang,andAshutoshSax- terStone. Learningtointerpretnaturallanguagecommands
ena.Environment-drivenlexiconinductionforhigh-levelin- throughhuman-robotdialog. InIJCAI,2015. 3
structions. InACL,2015. 2 [53] XinWang,QiuyuanHuang,AsliCelikyilmaz,JianfengGao,
DinghanShen,Yuan-FangWang,WilliamYangWang,and
[37] ArsalanMousavian,ClemensEppner,andDieterFox. 6-dof
Lei Zhang. Reinforced cross-modal matching and self-
graspnet: Variationalgraspgenerationforobjectmanipula-
tion. InICCV,2019. 1,8 supervisedimitationlearningforvision-languagenavigation.
InCVPR,2019. 1,2
[38] KhanhNguyenandHalDaume´III.Help,Anna!VisualNav-
[54] Xin Wang, Wenhan Xiong, Hongmin Wang, and
igation with Natural Multimodal Assistance via Retrospec-
William Yang Wang. Look before you leap: Bridging
tiveCuriosity-EncouragingImitationLearning. InEMNLP,
model-free and model-based reinforcement learning for
2019. 2
planned-ahead vision-and-language navigation. In ECCV,
[39] Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill
2018. 2
Dolan. Vision-basednavigationwithlanguage-basedassis-
[55] Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Ab-
tance via imitation learning with indirect intervention. In
hishekDas,GeorgiaGkioxari,StefanLee,IrfanEssa,Devi
CVPR,2019. 2
Parikh,andDhruvBatra. Embodiedquestionansweringin
[40] DanielNyga,SubhroRoy,RohanPaul,DaehyungPark,Mi-
photorealisticenvironmentswithpointcloudperception. In
haiPomarlan,MichaelBeetz,andNicholasRoy. Grounding
CVPR,2019. 2
robotplansfromnaturallanguageinstructionswithincom-
[56] Haonan Yu and Jeffrey Mark Siskind. Grounded language
pleteworldknowledge. InCoRL,2018. 3
learningfromvideodescribedwithsentences.InACL,2013.
[41] RohanPaul,JacobArkin,DeryaAksaray,NicholasRoy,and
3
Thomas M. Howard. Efficient grounding of abstract spa-
[57] LichengYu,XinleiChen,GeorgiaGkioxari,MohitBansal,
tialconceptsfornaturallanguageinteractionwithrobotplat-
Tamara L. Berg, and Dhruv Batra. Multi-target embodied
forms. IJRR,2018. 3
questionanswering. InCVPR,2019. 2
[42] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu
[58] Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-
Wang, Sanja Fidler, and Antonio Torralba. Virtualhome:
Fei,AbhinavGupta,RoozbehMottaghi,andAliFarhadi.Vi-
Simulating household activities via programs. In CVPR,
sualsemanticplanningusingdeepsuccessorrepresentations.
2018. 1,2,4
InICCV,2017. 2
[43] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel,
[59] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk
StefanThater,BerntSchiele,andManfredPinkal. Ground-
Cinbis,DavidFouhey,IvanLaptev,andJosefSivic. Cross-
ingactiondescriptionsinvideos. TACL,2013. 2,3
task weakly supervised learning from instructional videos.
[44] Ste´phaneRoss,GeoffreyGordon,andDrewBagnell. Are- InCVPR,2019. 3
ductionofimitationlearningandstructuredpredictiontono-
regretonlinelearning. InAISTATS,2011. 4
[45] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,
Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia
Liu,VladlenKoltun,JitendraMalik,DeviParikh,andDhruv
Batra. Habitat: A platform for embodied ai research. In
ICCV,2019. 1
[46] OzanSener,AmirR.Zamir,SilvioSavarese,andAshutosh
Saxena.Unsupervisedsemanticparsingofvideocollections.
InICCV,2015. 3
[47] MohitShridharandDavidHsu. Interactivevisualgrounding
ofreferringexpressionsforhuman-robotinteraction.InRSS,
2018. 3
[48] HaoTan,LichengYu,andMohitBansal. Learningtonav-
igate unseen environments: Back translation with environ-
mentaldropout. InNAACL,2019. 1
10
AppendixA.DatasetDetails 35
We give additional information about the generation of 30
expert demonstrations in AI2-THOR, language directives,
25
theannotationinterfaceusedtocollectdirectives,andsam-
plesofannotationswiththeirassociateddemonstrations. 20
A.1.ExpertDemonstrations 15
When sampling task parameters, we employ an active 10
strategy to maximize data heterogeneity. Figure F1 shows
5
the distribution of high-level task across train, validation
seen,andvalidationunseenfolds. FigureF2showsthedis- 0
1 2 3 4 5 6
tributionofsubgoalsacrosstasktypes. AndFiguresF6and Number of Annotations
F7 give the distributions of pickup objects and receptacles FigureF3: Thenumberofuniquetokensintroducedperan-
across the dataset. Each task parameter sample is defined notationoflanguagedirectives.
by(t,s,o,r,m),where
• t=thetasktype;
(e.g.,ifcupisalreadyrepresentedinthedataoftenasboth
• s=thesceneinAI2-THOR;
oandm, itbecomesdisfavoredbythesamplingalgorithm
• o=theobjectclasstobepickedup;
for all slots). We do this greedily across all slots until the
• r =thefinaldestinationforoor∅forExamine;
tuple is complete. Given any partial piece of information
• m = the secondary object class for Stack & Place
about the task, the distributions of the remaining task pa-
tasks(∅forothertasktypes).
rametersremainheterogeneousunderthissampling,weak-
To construct the next tuple, we first find the largest ening baseline priors such as ignoring the language input
source of imbalance in the current set of tuples. For ex- andalwaysexecutingacommontaskintheenvironment.
ample if o = apple is more common than o = plunger, Once a task parameter sample is complete, the chosen
o = plunger will be ranked higher than o = apple. We sceneisinstantiated,objectsandagentstartpositionareran-
additionallyaccountforthepriordistributionofeachentity domized,andtherelevantroomdataisencodedintoPDDL
rules for an expert demonstration. If the PDDL planner
cannot generate an expert demonstration given the room
2842 112 113 Pick & Place configuration, or if the agent fails an action during execu-
2944 126 109 Stack & Place tion, for example by running into walls or opening doors
2943 107 136 Place Two ontoitselfduetophysicalconstraints, theepisodeisaban-
2251 94
173 Examine doned. We gather three distinct expert demonstrations per
3554 124
3244 115 81 Heat & Place task parameter sample. These demonstrations are further
109 Cool & Place
3245 142 100 Clean & Place vetted by rolling them forward using our wrapper to the
AI2-THORAPItoensurethata“perfect”modelcanrepro-
Train Val seen unseen
Task Type duce the demonstration. The full sampling generation and
verificationcodewillbepublishedalongwiththedataset.
Figure F1: Task distribution across train, validation seen
andunseendatasetsplits. A.2.ExampleLanguageDirectives
We chose to gather three directives per demonstration
empirically. For a subset of over 700 demonstrations, we
Pick & Place HeatObject
CleanObject gathered up to 6 language directives from different anno-
Stack & Place
SliceObject tators. We find that after three annotations, fewer than 10
Place Two PutObject
unique tokens on average are introduced by additional an-
ToggleObject
Examine notators(FigureF3).
PickupObject
Heat & Place GotoLocation
Cool & Place A.3.AnnotationInterface
Clean & Place
Figure F4 shows the Mechanical Turk interface used to
0 10000 20000 30000 gatherlanguageannotations. Workerswerepresentedwith
avideooftheexpertdemonstrationwithtimelinesegments
FigureF2: Subgoaldistributionacross7tasktypes.
indicatingsub-goals. Theworkersannotatedeachsegment
11
epyT
ksaT
snekoT
euqinU
weN
ValSeen
Action:Close Action:Pickup
ValUnseen
Action:ToggleOn Action:Pickup
FigureF5: Predictedinteractionmasks. Masksgenerated
bytheSEQ2SEQ+PMmodelaredisplayedingreen.
FigureF4: MechanicalTurkAnnotationInterface. pipelinesavesinitializationinformationforobjectsandthe
agent,soalldemonstrationcanbeperfectlyreplayedinthe
simulator. Researchers can use this replay feature to aug-
whilescrubbingthroughthevideo,andwroteashortsum-
mentthedatasetbysavinghigh-resimages,depthmaps,or
marydescriptionfortheentiresequence.Wepayedworkers
object-segmentationmasks.
$0.7 per annotation. During vetting, annotators were paid
$0.35perHIT(HumanInteractionTask)tocompare5sets
of three directives each. These wages were set based on Network Architecture We use a pretrained ResNet-18
localminimum-wageratesandaveragecompletiontime. [22]toextract512×7×7featuresfromtheconv5layer.
Thesefeaturesarefedintoatwo-layerCNNwith1×1con-
A.4.VocabularyDistributions
volutionstoreducethechanneldimensionfrom512to64.
Figure F8 shows vocabulary statistics of the language The 64×7×7 output is flattened, and a fully-connected
inALFRED. layerproducesa2500-dimensionalvisualfeaturev t.
The language encoder is a bi-directional LSTM with a
A.5.DatasetExamples
hidden-dimension of 100. We do not use pretrained lan-
guagemodelstoinitializetheLSTM,andtheencodingsare
FigureF9shows7experttrajectories(onepertasktype)
learnedfromscratchinanend-to-endmanner. Wealsouse
andtheiraccompaniedannotations.
a self-attentionmechanism to attend overthe encodings to
AppendixB.ImplementationDetails initializethehidden-stateofthedecoderLSTM.
TheactiondecoderisanLSTMwithahidden-dimension
We describe implementation and training details of our of512.Theactorisafully-connectedlayerthatoutputslog-
baselineSequence-to-Sequencemodels. itsfor13actions. Themaskdecoderisathree-layerdecon-
volution network, which takes in the concatenated vector
Preprocessing We tokenize the language directives and u and transforms it into 64×7×7 features with a fully-
t
convertalltokenstolower-case. Duringdatasetgeneration, connectedlayer. Thesefeaturesaresubsequentlyup-scaled
wesaveimagesfromAI2-THOR300×300pixels,andlater intoa1×300×300binarymaskthroughthreelayersofde-
resize them to 224×224 during training. The generation convolutionsandup-samplingwithbi-linearinterpolation.
12
Training The models were implemented with PyTorch
andtrainedwiththeAdamoptimizer[25]atalearningrate
of 1e-4. We use dropout of 0.3 on the visual features and
thedecoderhiddenstate,tunedonthevalidationdata. Both
theactionandmasklossesareweightedequally, whilethe
auxiliary losses are scaled with a factor of 0.1. For evalu-
ation, we choose models with the lowest loss on the vali-
dation seen set. It should be noted that, due to the nature
of the tasks, low validation loss might not directly lead to
betterevaluationperformancesincetheagentdoesnothave
toexactlyimitatetheexperttocompletethetask.
Notes on Random Agent Unlike discretized navigation
wheretakingrandomactionsmightallowtheagenttostum-
ble upon the goal, ALFRED tasks are much harder to
achieve by chance. The action space branching factor of
Room-to-Room navigation [3], for example, is 46 ≈ 4000
(6averagestepsand4navigationactions). Bycontrast,the
ALFREDaveragebranchingfactoris1250 ≈1053 (50av-
eragestepsfor12actions). Beyondactiontypeprediction,
the ALFRED statespaceresultingfromdynamicenviron- TaskAblations-Validation
ments and the need to produce pixel-wise masks for inter- NOLANGUAGE SEQ2SEQ SEQ2SEQ+PM
activeactionsexplodesfurther. TaskType Seen Unseen Seen Unseen Seen Unseen
Pick&Place 0.0 0.0 6.3 1.0 7.0 0.0
B.1.PredictedMasks
Stack&Place 0.0 0.0 0.0 0.0 0.9 0.0
FigureF5showsafewexamplesofmasksgeneratedby PickTwo 0.0 0.0 1.6 0.0 0.8 0.0
Clean&Place 0.0 0.0 0.0 0.0 1.8 0.0
the SEQ2SEQ+PM model in seen and unseen validation
Heat&Place 0.0 0.0 1.9 0.0 1.9 0.0
scenes. The Microwave mask accurately captures the con-
Cool&Place 0.0 0.0 2.4 0.0 4.0 0.0
toursoftheobjectsincethemodelisfamiliarwithrecepta-
Examine 0.0 0.0 4.3 0.0 9.6 0.0
clesinseenenvironments. Incontrast,theSinkmaskinthe
unseenscenepoorlyfitstheunfamiliarobjecttopology.
Table A1: Success percentages across 7 task types. The
highestvaluesareshowninblue.
AppendixC.AdditionalResults
C.1.PerformancebyTaskType
In Table A1, we present success rates across the 7 task
types. Even the best performing model, SEQ2SEQ+PM,
mostly succeeds in solving some short-horizon tasks like
Pick&PlaceandExamine. LonghorizontaskslikeStack
&PlaceandPickTwo&Placehavenearzerosuccessrates
acrossallmodels.
13
WaWteinreinBgoCttalne Watch
Watch
Vase
Vase
TomatoSliced TomatoSliced
Tomato
Tomato
ToiletPaper
TissueBox
ToiletPaper
Statue
TissueBox
TennisRacket Spoon
Statue
SprayBottle Spatula
SoapBottle
Spoon
Spatula
SoapBar
SoapBottle
SoapBar SaltShaker Ladle Knife
SaltShaker RemoteControl Knife Kettle
RemoteControl PotatoSliced KeyChain
Kettle
Potato HandTowel
PotatoSliced Plunger HandTowel
Plate
Potato Pillow Fork
Glassbottle Egg
Pot PepperShaker
Plunger
Plate Fork Cup
Pillow Pencil DishSpoEnggge
PepperShaker
Cup
Pencil Pan CreditCard
Pen Newspaper CreditCard
Cloth
Pan
Newspaper Mug Cloth
Mug CellPhone
CellPhone
LettuceSliced LettuceSliced Candle
Lettuce CD CD
Lettuce
Laptop
Ladle Laptop
Knife ButterKnife BreadSliced
Knife
KeyChain KeyChain Bread
Kettle BreadSliced
HandTowel Glassbottle
Glassbottle
Fork Fork Bread
Box
Egg
Egg Box
DishSponge
Cup
Cup CreditCard Bowl
CreditCard Cloth BaskeB tBoo ak ll Book
CellPhone
BaseballBat
Cloth CD BaseballBat
AppleSliced
Apple
CellPhone ButterKnife AlarmClock AppleSliced
AlarmClock
Candle BreadSliced Train Val seen Val unseen
CD Bread Receptacle Classes
ButterKnife Box Figure F7: Receptacle distributions in the train, validation
BreadSliced Bowl seenandunseenfolds.
Book
Bread BasketBall
Box
Bowl BaseballBat
Book
BBasaesbkealtlBBaaltl AppleSliced
AppleSliced
Apple
Apple
AlarmClock AlarmClock
Train Val seen Val unseen
Object Classes
FigureF6: Pickupdistributionsinthetrain,validationseen
andunseenfolds.
14
Verbs
105
104
103
102
Nouns
104
103
Other
104
103
102
All
105
104
FigureF8: VocabularyDistributions. Frequencydistributionsof100mostcommonverbs,nouns,otherwords(non-verbs
andnon-nouns),andallwords.
15
nrut
elbat
thgir
eht
klaw
knis
tfel
ot
tup
retnuoc
drawrof
nrut
kcip
thgir
edisni
dna
og
tfel
etihw
no
ecalp
evaworcim
kcalb
fo
ecaf
egdirf
thgiarts
thgir
evom
efink
neerg
tfel
ekat
ecils
llams
ni
tfel
tnorf
eulb
a
nepo
rood
nedoow
pu
esolc
edis
naelc
klaw
si
teliot
nworb
elbat
barg
otamot
raen
tup
dnats
ksed
etomer
kcip
daeh
elppa
der
knis
kool
evots
tsap
retnuoc
evomer
eeffoc
detaeh
dnuora
ecils
otatop
wolley
ecalp
gnah
tenibac
dnuor
og
tuc
moor
tsesolc
neht
dellihc
pmal
daeha
ecaf
taeh
flehs
dnoces
morf
nac
nrut
nepo
evaworcim
yrrac
daerb
esolc
egdirf
hsaw
pot
dloc
ti
gnirb
ecuttel
egral
evom
s'
nap
ylthgils
efink
llihc
etalp
raf
ekat
gnicaf
rotaregirfer
dekooc
ecils
tuhs
puc
yarg
htiw
ssorc
nehctik
yawa
kcab
era
deb
decils
tnorf
teg
lwob
llat
drawrof
tes
riahc
elddim
rood
esnir
gum
ytrid
nepo
enimaxe
paos
looc
edis
kooc
xob
elprup
teliot
pets
llaw
rewol
otamot
gnittis
hcuoc
knip
ksed
ekam
dne
etisoppo
evots
tiaw
elttob
yerg
elppa
hcaer
reward
dehsaw
eeffoc
pots
resserd
retnuoc
otatop
dnif
hsart
yltcerid
sdrawot
gniklaw
pets
mraw
tenibac
pord
gge
tew
pot
nigeb
renroc
htaenrednu
ruoy
deecorp
noops
erauqs
moor
nruter
nib
reppu
pmal
worht
but
tsomtfel
esolc
gnidloh
top
gum
taht
knis
drac
efas
uoy
gninrut
tiderc
yletelpmoc
flehs
naelc
syek
raelc
edisni
looc
ssalg
resolc
etihw
denaelc
daeh
tseraen
daerb
dloh
egnops
eciwt
si
gnittup
enohp
elttil
tuo
retnuoc
repap
nedlog
ecuttel
llif
retaw
toh
nap
hcaorppa
rekam
egnaro
revo
esu
dnalsi
gnol
ta
gniniatnoc
licnep
htaeneb
kcalb
gniyrf
roolf
trohs
etalp
mraw
krof
yek
daeh
desnir
lewot
devaworcim
rotaregirfer
evaworcim
nep
sdrawrof
gum
gnidnats
koob
krad
puc
delooc
vt
nevo
nehctik
er'
ecalp
sdrawkcab
ffo
reev
rab
prahs
deb
tel
spets
revlis
lwob
etacol
yarps
alutaps
riahc
eb
kcab
sulp
owt
gnignah
eldnac
demraw
dnats
sah
afos
gib
paos
deldnah
nac
flah
llaw
ees
dnats
tsehtruf
xob
gnisolc
eceip
3
ssorca
devaworcim
llor
yawflah
pets
tcepsni
egde
2
dne
detacol
retsaot
tsehtraf
hcuoc
gnisu
gar
09
elttob
eunitnoc
thgil
dlog
reward
decils
alutaps
yrf
resserd
trats
tlas
retnec
hsart
gnivomer
dc
nib
txen
llup
kcolc
neves
nwod
saw
repapswen
ylsuoiverp
otni
llaw
hctaw
teews
nac
yarps
faol
regral
gge
nevo
potpal
081
renroc
gniyrrac
htolc
wol
barg
gnikat
sdnoces
renaelc
nib
eveirter
thgin
rellams
noops
enod
llec
tnecajda
but
tog
egabrag
ralugnatcer
top
evots
retnec
kciuq
niaga
niard
etomer
ynit
drac
Pick&Place
Annotation#1 Annotation#2 Annotation#3
Goals Putawatchonatable. Putthewatchonthecoffeetable. Movethewatchtothecoffeetable.
Gototherightandturnaroundtofacetheendofthe TurnrightandgototheTVstand.Pickupthewatchfromthe Turnright,gostraight,turnright,stepforward,thenturnrighttoface
cabinetwiththetelevisiononit.Pickthewatchupfromthe stand.Turnaroundandfacethecoffeetable.Putthewatch thetablewiththeTVonit.Pickupthewatchonthetable,tothe
Instructions cabinet.Gototherightandthenturntofacethecoffee onthecoffeetable. rightoftheremote.Turnright,moveforward,turnleft,move
tableinfrontofthecouch.Putthewatchonthetable. forward,thenturnrighttofacethecoffeetable.Putthewatchonthe
frontleftcornerofthecoffeetable.
Stack&Place
Annotation#1 Annotation#2 Annotation#3
Goals Putabowlwithaspooninitonthetable. Moveaspoonandbowltoatable. Toputaspooninabowlplusmovingthemtothekitchen
table.
Turnleft,andwalkacrosstheroomtothemicrowave.Pick Moveaspoonandbowltoatable.Gotothecounterrightof Turnleftandwalkacrosstheroomtofaceaspoonontherightend
upthespoon.Turnleft,andwalktothecoffeemachine.Put themicrowave.Pickupthespoonfromthecounter.Gotothe ofthecounter.Pickupthespoonontheendofthecounter.Turnleft
Instructions thespooninthebowl.Pickupthebowl.Turnleft,walkto coffeemaker.Placethespooninabowlnexttothe andwalkacrosstheroomtofacethecoffeemakeronthecounter.
thetable,andturnleft.Putthebowlonthetable. coffeemaker.Pickupthebowl.Gototheblacktable.Place Putthespooninthebowltotherightofthecoffeemakeronthe
thebowldownonthetable. counter.Pickupthebowlwithaspooninitonthecounter.Turnleft
andwalkacrosstheroomandturnlefttofacethekitchentable.
Placethebowlwiththespooninitonthekitchentable.
PickTwo&Place
Annotation#1 Annotation#2 Annotation#3
Goals Puttingpencilsinsideofacabinet. Puttwopencilsinadrawer. Placethetwopencilsinthestand.
Walktothebedsidetableinfrontofyou.Grabthepencil Walkstraightaheadtotheendtable.Pickupthebluepencil Walktothestandnexttothebed.Grabthepencilfromthestand.
that'sonthetable.Moveslightlytotheleftandopenthetop ontheright.Walkaroundtothefrontoftheendtable.Putthe Opentheshelfinsideofthestand.Placethepencilinthetopshelf
Instructions cabinetofthebedsidetable.Placethepencilinsidethe pencilinthetopdraweroftheendtable.Lookupatthetopof ofthestand.Closetheshelf,walkbacktothestand.Grabtheother
cabinet.Facethefrontofthebedsidetable.Grabthepencil theendtable.Pickupthepencilonthetable.Lookdownat pencilfromthestand.Walkbacktothestandnexttothebed.Place
offofthebedsidetable.Openthetopcabinetonthetable. thedrawer.Putthepencilinthedrawerontopoftheother thepencilinthetopshelfofthestand.
Placethepencilinsidethecabinetandthencloseit. pencil.
Clean&Place
Annotation#1 Annotation#2 Annotation#3
Goals Putacleanragonthetopshelfofabarredrack. Washthepinktowelontheshelf,putitbackonthe Cleanaredcloth.
shelf.
Turnaround,gotothebarredrack.Pickuptheragfromthe Washthepinktowelontheshelf,putitbackontheshelf.Turn walkonovertothetoweldryingrack.pickupadirtyredclothfrom
bottleshelfofthebarredrack.Gotothesinkontheleft.Put aroundandgotheshelf.Pickupthepinktowelontheshelf. thetowelrack.walkovertotheleftsideofthebathroomsink.turn
Instructions theraginthesink,turnonthenturnoffthewater.Gotothe Turnaroundandputthetowelinthesink.Fillthesinkwith onthewatertorinsethedirtyredclothandpickitbackupagain.
barredracktotherightofthesink.Puttheragonthetop waterandwashthetowel,takethetowelout.Gobacktothe walkbackovertothetoweldryingrack.placethecleanclothonthe
shelfofthebarredrack. shelf.Putthetowelbackontheshelf. dryingrack.
FigureF9: DatasetExamples. Annotationsforsevenexpertdemonstrations.
16
Heat&Place
Annotation#1 Annotation#2 Annotation#3
Goals Putacookedpotatosliceonthecounter. Placeasliceofcookedpotatoontothecounter. Putapieceofcookedpotatoonthecounter.
Turnright,turnright,walkpastthesink,turnlefttoface Turnright,movetothetable.Pickuptheknifefromthetable. Turnrightandcrosstheroom,thenturnleftandgotofacethegray
roundtablewithtablecloth.Pickuptheyellow-handledknife Slicethepotatoonthetable.Turnleft,movetothecounterleft table.Pickuptheknifefrominbetweenthelettuceandtheapple.
fromthetable.Cutasliceinthepotatoonthetable.Turn ofthebread.Puttheknifeonthecounternearthesoap Usetheknifetoslicethepotatothat'sonthegraytable.Bringthe
left,turnleft,turnrightatcounter,crossroom,turnleftat container.Turnleft,movetothetable.Pickupasliceof knifewithyouandgofacethekitchencounterwiththeloafofbread.
refrigeratortofacecounter.Putknifedownonthetable. potatofromthetable.Turnleft,movetothecounterinfrontof Puttheknifedowninfrontofthesoapdispenseronthecounter.Go
Turnleft,walkpastsink,turnlefttofaceroundtable.Pick thestove.Putthepotatosliceintothemicrowave,cookit,pick backovertothegraytable.Pickupasliceofthecutpotatofromthe
thepotatosliceupfromthetable.Turnleft,makeright itbackup.Turnright,movetothecounterleftofthebread. table.Bringthepotatowithyouandgoovertothestove,thenlook
aroundcornerofcounter,turnlefttofacestoveand Putthecookedpotatosliceonthecounter. upatthemicrowave.Cookthepotatosliceinthemicrowave,then
microwave.Putpotatoinmicrowave,cookit,takeitoutof takeitoutagain.Bringthepotatosliceovertothecountertopwith
microwave.Turnright,crossroom,turnleftatcounterwith theloafofbreadandtheknifeyouusedtocutit.Putthepotatoslice
blueplateonit.Putpotatoonthecounterinfrontofthe downinfrontoftheblueplate.
blueplate.
Cool&Place
Annotation#1 Annotation#2 Annotation#3
Goals Putasliceofcoldlettuceonacounter. Putachilledsliceoflettuceonthecounter. Slicesomelettuceandcoolitintherefrigeratorsoyoucan
putitonthecountertop.
Turnleft,goforwardpastthecounter,turnleft,goforwardto Turnleft,headtowardthefridge,turnleftandgotothestove. turnleftandgoaroundthecountertop,thengostraighttothestove
thecountertotheleftoftheoven.Taketheknifetotheleft Pickuptheknifebesidethespoononthecounter.Turn top.pickuptheknifewiththeyellowhandlefrombehindthesalt
Instructions ofthelargespoonfromthecounter.Turnaround,go aroundandturntotherighttofacethecounterwiththeegg. shakeronthecountertop.turnleftandfacethecountertoptoyour
forwardastep,turnrighttothecounter.Cutthelettuceon Cutthelettuceonthecounter.Moverightandbacklefttothe left.slicethelettuceonthecountertop.facethecountertopwith
thecounterintoslices.Turnright,goforwardastep,turn counter.Puttheknifebehindtheeggonthecounter.Move theknifeinhand.placetheknifedownnexttothelettucesliceson
lefttothecounter.Puttheknifebehindtheeggonthe theleftandturnbackrighttowardsthecounter.Pickupaslice thecountertop.facethelettuceonthecountertop.pickupaslice
counter.Turnleft,goforwardastep,turnrighttothe oflettuce.Turnaroundandheadtothefridge.Putthelettuce oflettuceonthecountertop.turnleft,thenfacetheoppositewall
counter.Takeasliceoflettucefromthecounter.Turnleft, onthesecondshelfofthefridge,closethefridge,openthe behindyoutofacetherefrigerator.opentherefrigerator,andplace
goforward,turnrighttothefridge.Gotothefridge.Chillthe fridgeandpickitbackup.Turnleft,gohalfwayacrossthe thesliceoflettuceinfrontoftheappleoneshelfabovethebread,
lettuceinthefridgeinfrontoftheapple.Takethelettuce roomandturnrighttowardthecoffeemaker.Putthesliceof thenshutthedoorandopenitupagainafterseveralsecondsto
fromthefridge.Turnleft,goforward,turnrighttofacethe lettuceonthecounterinfrontofthecoffeemaker. pickthelettucesliceup.turnleft,thenfaceforwardtothepartofthe
coffeemaker.Putthelettuceinfrontofthecoffeemakeron countertoponwhichthecoffeemakersits.placethesliceoflettuce
thecounter. infrontofthecoffeemaker.
ExamineinLight
Annotation#1 Annotation#2 Annotation#3
Goals Readabookbylamplight. Examineabookwithalamp. Pickupabookandturnonalamp.
Headforwardtothebedinfrontofyou.Pickuptheblue walkforwardafewsteps,turnright,taketwosteps,turnleft, Walkforwardtofacethebed.Pickthebookupfromthebed.Turnto
bookthatissittingonthebed;thebookthatsays walktobed.pickupthebookthatisonthebed.turnaround, therightandfacethenightstandwiththelamp.Turnthelampon.
Instructions ProbabilisticRobotics.Turntoyourrightandwalktothe takeastep,turnlefttofacesmalltable.turnthelampon.
nightstand.Turnonthelampthatissittingonthenight
stand.
FigureF9: DatasetExamples. Annotationsforsevenexpertdemonstrations.
17
Figure F10: Visual diversity of AI2-THOR [26] scenes. Top to bottom rows: kitchens, living rooms, bedrooms,
and bathrooms. Object locations are randomized based on placeable surface areas and class constraints. See https:
//ai2thor.allenai.org/ithor/demo/foraninteractivedemo.
18

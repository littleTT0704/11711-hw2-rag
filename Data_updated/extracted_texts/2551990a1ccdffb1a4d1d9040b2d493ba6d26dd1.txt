Towards Visual Question Answering on Pathology Images
XuehaiHe∗1,ZhuoCai∗2,WenlanWei3,YichenZhang1,LuntianMou4,
EricXing5 andPengtaoXie1
1 UCSanDiego,2 TsinghuaUniversity,3 WuhanUniversity,
4 BeijingUniversityofTechnology,5 MBZUAIandCMU
p1xie@eng.ucsd.edu
Abstract lect a dataset containing questions about pathol-
ogyimaging. Onepossiblewaytocreateapathol-
Pathology imaging is broadly used for identi- ogyVQAdatasetiscrowdsourcing,whichisused
fying the causes and effects of diseases or in-
successfully for creating general domain VQA
juries. Given a pathology image, being able
datasets(MalinowskiandFritz,2014;Antoletal.,
to answer questions about the clinical find-
2015;Renetal.,2015a;Johnsonetal.,2017;Goyal
ings contained in the image is very impor-
etal.,2017). However,itismuchmorechallenging
tant for medical decision making. In this
paper, we aim to develop a pathological vi- to build medical VQA datasets than general do-
sual question answering framework to ana- mainVQAdatasetsviacrowdsourcing. First,med-
lyze pathology images and answer medical ical images such as pathology images are highly
questions related to these images. To build
domain-specific,whichcanonlybeinterpretedby
such a framework, we create PathVQA, a
well-educated medical professionals. It is rather
pathologyVQAdatasetwith32,795questions
difficultandexpensivetohiremedicalprofession-
asked from 4,998 pathology images. We
alstohelpcreatemedicalVQAdatasets. Second,to
alsoproposeathree-leveloptimizationframe-
workwhichperformsself-supervisedpretrain- createaVQAdataset,onefirstneedstocollectan
ing and VQA finetuning end-to-end to learn imagedataset. Whileimagesinthegeneraldomain
powerful visual and textual representations arepervasive,medicalimagesareverydifficultto
jointly and automatically identifies and ex- obtainduetoprivacyconcerns.
cludes noisy self-supervised examples from
Toaddressthesechallenges,weresorttopathol-
pretraining. We perform experiments on
ogytextbooks,especiallythosethatarefreelyac-
our created PathVQA dataset and the results
cessible online, as well as online digital libraries.
demonstratetheeffectivenessofourproposed
methods. The datasets and code are available Weextractimagesandcaptionsfromthetextbooks
athttps://github.com/UCSD-AI4H/PathVQA and online digital libraries. Given these images,
question-answerpairsarecreatedbasedonimage
1 Introduction captions. These QA pairs are verified by medi-
calprofessionalstoensureclinicalmeaningfulness
Pathology(Levisonetal.,2012)studiesthecauses
and correctness. In the end, we created a pathol-
and effects of diseases or injuries. It underpins
ogyVQAdatasetcalledPathVQA,whichcontains
every aspect of patient care, such as diagnostic
32,795questionsaskedfrom4,998pathologyim-
testing,providingtreatmentadvice,preventingdis-
ages. Toourbestknowledge,thisisthefirstdataset
eases using cutting-edge genetic technologies, to
forpathologyVQA.
nameafew. Givenapathologyimage,beingable
GiventhepathologyVQAdataset,thenextstep
toanswerquestionsabouttheclinicalfindingscon-
is to develop a pathology VQA system, which is
tainedintheimageisveryimportantformedical
alsoverychallenging,duetothefollowingreason.
decision-makings.
The medical concepts involved in PathVQA are
In this paper, we aim to develop a pathologi-
verydiversewhilethenumberofquestion-answer
cal visual question answering framework to ana-
pairs available for training is limited. Learning
lyze pathology images and answer medical ques-
effectiverepresentationsofthesediversemedical
tionsrelatedtotheseimages. Wefirstneedtocol-
concepts using limited data is technically diffi-
∗EqualContribution cult. Poorly learned representations lead to infe-
createdon4,200radiologyimagesandhas15,292
Q1: What are dilated and congested?
Q2: Are the sinuses dilated and congested? question-answerpairs. Mostofthequestionsarein
Q3: Is there increased fibrosis in the
red pulp, capsule and the trabeculae? multiple-choice (MC) style and can be answered
Q4: Where is increased fibrosis? bymulti-wayclassifiers. Thismakesthedifficulty
Q5: Is gamna-gandy body also seen?
ofthisdatasetsignificantlylower. VQA-RAD(Lau
Q1: What is slightly depressed on
et al., 2018) is a manually-crafted dataset where
the surface?
Q 2: Where is the wedge-shaped questions and answers are given by clinicians on
infarct slightly depressed?
Q 3: Is the wedge-shaped infarct radiologyimages. Ithas3515questionsof11types.
slightly depressed on the surface?
OurdatasetdiffersfromVQA-MedandVQA-RAD
Q4: What is on the surface?
Q5: What is pale while the margin in two-fold. First, ours is about pathology while
is haemorrhagic?
VQA-MedandVQA-RAD(Lauetal.,2018)are
both about radiology. Second, our dataset is a
Fi gure 1: Two exemplar images with generated ques-
tio ns. Both images have three types of questions: truly challenging QA dataset where most of the
“what”,“where”,and“yes/no”. questionsareopen-endedwhileinVQA-Medand
VQA-RADthemajorityofquestionshaveafixed
numberofcandidateanswersandcanbeanswered
riorVQAperformance. Toaddressthischallenge,
bymulti-wayclassification. Besides,thenumber
weproposeathree-leveloptimizationframework
ofquestionsinourdatasetismuchlargerthanthat
which performs cross-modal self-supervised pre-
inVQA-MedandVQA-RAD.
training (Tan and Bansal, 2019) and VQA fine-
tuning of a pathology image encoder and a ques-
2.2 Cross-modalSelf-supervisedLearning
tion encoder end-to-end to learn powerful visual
Cross-modalself-supervisedlearninglearnsrepre-
and textual representations jointly and automati-
sentationsfordatawithmultiplemodalitiesbysolv-
callyidentifiesandexcludesnoisyself-supervised
ingcross-modalauxiliarytasks. VisualBERT(Li
examples from pretraining. Experiments on our
etal.,2019)learnsrepresentationsforimagesand
developed PathVQA dataset demonstrates the ef-
textsbyimplicitlyaligningelementsofatextand
fectivenessofourproposedmethods.
regionsinanassociatedimagewithself-attention.
Themajorcontributionsofthispaperareasfol-
CVLP (Shi et al., 2020) proposes an unbiased
lows:
contrastivevisual-linguisticpretrainingapproach,
• Wecreateapathologyvisualquestionanswer- which constructs a self-supervised loss based on
ingdataset–PathVQA,tofostertheresearch contrastive learning. ViLBERT (Lu et al., 2019)
ofmedicalVQA.Toourbestknowledge,this proposestopretrainavision-and-languageBERT
isthefirstdatasetforpathologyVQA. modelthroughmaskedmulti-modalmodelingand
alignment tasks, and then transfer the model to
• Weproposeathree-leveloptimizationframe-
visualquestionansweringtasks.
work which performs cross-modal self-
supervised pretraining and VQA finetuning 2.3 DataSelectionandDataReweighting
ofapathologyimageencoderandaquestion
A number of approaches have been proposed for
encoder end-to-end to learn powerful visual
dataselection. Renetal.(2018)proposesameta
and textual representations jointly and auto-
learning method to learn the weights of training
matically identifies and excludes noisy self-
examples by performing a meta gradient descent
supervisedexamplesfrompretraining.
step on the weights of the current mini-batch of
examples. Shu et al. (2019) propose a method
• OnourPathVQAdataset,wedemonstratethe
which can adaptively learn an explicit weighting
effectivenessofourproposedmethod.
functiondirectlyfromdata.
2 RelatedWork 3 ThePathVQADataset
2.1 MedicalVQADatasets
ThePathVQAdatasetconsistsof32,795question-
To our best knowledge, there are two existing answerpairsgeneratedfrom1,670pathologyim-
datasets for medical visual question answering. ages collected from two pathology textbooks:
The VQA-Med (Abacha et al., 2019) dataset is “Textbook of Pathology” (Muir et al., 1941) and
Table1: Frequencyofquestionsindifferentcategories coderW andatextencoderT. Theimageencoder
isusedtoextractvisualfeaturesofpathologyim-
Totalnumber ages. Thetextencoderisusedtoextractsemantic
Questiontype
andpercentage featuresofquestionsandanswers. Self-supervised
Yes/No 16,329(49.8%) learning (He et al., 2019) is an unsupervised rep-
What 13,401(40.9%) resentationlearningapproachwherepretexttasks
Where 2,157(6.6%) aredefinedsolelybasedontheinputdata,andrep-
How 595(1.8%) resentations are learned by solving these pretext
Howmuch/many 139(0.4%) tasks.
Why 114(0.3%)
Therearemanywaystoconstructpretexttasks.
When 51(0.2%)
Inourwork,following(TanandBansal,2019),we
Whose 9(0.1%)
define a simple yet effective pretext task: in the
PathVQAdataset,givenapathologyimageanda
question,judgewhetherthisquestionisaboutthis
“BasicPathology”(Robbinsetal.,1981),and3,328
image. FromthePathVAQtrainingsetD,wecreate
pathologyimagescollectedfromthePEIR1 digital
another dataset D(cid:48) = {(x ,y ,t )}M to perform
i i i i=1
library. Thequestion-answerpairsaregeneratedus-
the SSL task. There are M tuples, each contain-
ingasemi-automatedpipelinewithlinguisticrules.
ing a pathology image x from D and a question
Figure1showssomeexamples.
y from D. t is a binary variable where t = 1 if
i i
Onaverage,eachimagehas6.6questions. The
xandy arefromthesametrainingexampleinD
maximumandminimumnumberofquestionsfora and t = 0 if otherwise. Given D(cid:48), we develop a
i
singleimageis14and1respectively. Theaverage
model to map (x ,y ) to t . In this model, an im-
i i i
number of words per question and per answer is
ageencoderisusedtoencodex andatextencoder
i
9.5and2.5respectively. Thereareeightdifferent
is used to encode y ; the concatenation of these
i
categoriesofquestions: what,where,when,whose,
twoencodingsisfedintoalinearlayertopredict
how, why, how much/how many, and yes/no. Ta-
whethertheimagematcheswiththequestion.
ble1showsthenumberofquestionsandpercent-
Inself-supervisedlearning(Heetal.,2019),the
age in each category. The questions in the first
labelsaretypicallyconstructedautomaticallywith-
7 categories are open-ended: 16,466 in total and
outhumansupervision. Asaresult,theycontaina
accounting for 50.2% of all questions. The rest
lotofnoises. Forexample,inD(cid:48),tisdetermined
areclose-ended“yes/no”questions. Thequestions
simply based on whether x and y are from the
covervariousaspectsofvisualcontents,including
trainingexampleinD. Itistotallypossiblethata
color,location,appearance,shape,etc. Suchclini-
questiony askedaboutanimagex(cid:48) isappropriate
caldiversityposesgreatchallengesforAImodels
to be a question for another image x as well if x
tosolvethispathologyVQAproblem.
andx(cid:48) arepathologicallysimilar. Inthiscase,the
correct label t for (x,y) should be 1. However,
4 Method
it is set to 0 in D(cid:48). Training the encoders using
Weproposeathree-leveloptimizationbasedframe- these noisy and incorrect labels may confuse the
worktoperformVQAonPathVQA.Inourframe- encodersandresultinpoor-qualityrepresentations.
work, there are three learning stages, which are
To address this problem, we aim to develop a
performed end-to-end jointly. In the first stage,
methodtoautomaticallyidentifyincorrectlyauto-
self-supervisedlearning(Heetal.,2019;Tanand
labeled examples in the training data of the SSL
Bansal, 2019) is performed to pretrain the image task. Foreachexample(x,y,t)inD(cid:48),weassociate
encoderandtextencoder. Inthesecondstage,we
aselectionvariablea ∈ [0,1]withit. Ifaisclose
finetunetheimageencoderandtextencoderonthe
to 1, it means this example is correctly labeled;
PathVQA dataset. In the third stage, the trained
if a is close to 0, it means this example is incor-
model is validated on the validation set. In the
rectly labeled. Let l(f(x,y;W,T),t) denote the
firststage,weperformcross-modalself-supervised
SSLlossdefinedon(x,y,t),wheref(x,y;W,T)
learning (Tan and Bansal, 2019) of an image en-
is the predicted probability that t = 1 and l(·)
is the cross-entropy loss. We multiply a with
1http://peir.path.uab.edu/library/
index.php?/category/2 l(f(x,y;W,T),t)sothatif(x,y,t)isincorrectly
labeled, its loss will be down-weighted to 0 and Putting all these pieces together, we have the
effectively(x,y,t)isexcludedfromtheSSLpre- followingthree-leveloptimizationframework:
trainingprocess. Intheend,onlycorrectly-labeled
examplesareusedforpretrainingtheencoders. To sm .tin .A V(cid:80) ∗N i (= W( 1va ∗l () AL )( )d ,( i Uva ∗l () T,V ∗(∗ A(W )),∗ R(A ∗) =),U∗(T∗(A)),R∗)
thisend,inthefirststage,wesolvethefollowing argmin (cid:80)N(tr) L(d(tr),V,U,R)
V,U,R i=1 i
optimizationproblem: +γ1(cid:107)V −W∗(A)(cid:107)2 2+γ2(cid:107)U−T∗(A)(cid:107)2 2
W∗(A),T∗(A)=argmin
W,T
(cid:80)M
ail(f(xi,yi;W,T),ti)
i=1
W∗(A),T∗(A) =
argmin
(cid:80)M
a l(f(x ,y ;W,T),t ).
i=1 i i i i
W,T 4.1 VQAModels
In this problem, the selection variables A = OurproposedmethodcanbeappliedtoanyVQA
{a }M are fixed (we will discuss how to learn method. In this work, we choose two well-
i i=1
Alateron). {a }M areusedtoweighthelossesof established and state-of-the-art VQA methods to
i i=1
individualexamplesinD. W andT aretrainedby perform the study while noting that other VQA
minimizingthesumofweightedlosses. Notethat methodsareapplicableaswell.
theoptimalsolutionsW∗(A)andT∗(A)arefunc-
• Method 1: In (Tan and Bansal, 2019), a
tionsofAsinceW∗(A)andT∗(A)arefunctions
large-scaleTransformer(Vaswanietal.,2017)
ofthelossfunction,whichisafunctionofA.
modelisbuiltthatconsistsofthreeencoders:
In the second stage, we finetune the image en-
anobjectrelationshipencoder,alanguageen-
coderandtextencoderintheVQAtaskdefinedon
coder,andacross-modalencoder. Thethree
the PathVQA dataset D. Let V, U, R denote the
encodersarebuiltmostlybasedontwokinds
networkweightsoftheimageencoder,textencoder,
ofattentionlayers—self-attentionlayersand
andQAnetworkrespectively. WetrainV,U,Rby
cross-attentionlayers. Theobjectrelationship
minimizingtheVQAloss:
(cid:80)N(tr) L(d(tr)
,V,U,R)
i=1 i encoder and the language encoder are both
(tr)
whered isatrainingexampleinD,consisting
i single-modalityencoders. Across-modalen-
of an input pathology image, an input question,
coder is proposed to learn the connections
and an output answer. When training V and U,
betweenvisionandlanguage.
we encourage them to be close to the optimally
trainednetworkweightsW∗(A)andT∗(A)ofthe • Method 2: The method proposed in (Kim
imageandtextencoderinthefirststage,totransfer et al., 2018) uses a Gated Recurrent Unit
therepresentationslearnedintheSSLtasktothe (GRU) (Cho et al., 2014) recurrent network
VQA task. The second stage amounts to solving andaFasterR-CNN(Renetal.,2015b)net-
thefollowingoptimizationproblem: work to embed the question and the image.
Itextendstheideaofco-attentiontobilinear
V∗(W∗(A)),U∗(T∗(A)),R∗ = attentionwhichconsiderseverypairofmulti-
argmin(cid:80)N(tr) L(d(tr) ,V,U,R)+ modalchannels.
i=1 i (1)
V,U,R
γ 1(cid:107)V −W∗(A)(cid:107)2 2+γ 2(cid:107)U −T∗(A)(cid:107)2 2. 5 Experiment
where the L2 losses encourage V and U to be 5.1 ExperimentalSettings
close to W∗(A) and T∗(A). γ 1 and γ 2 are trade- Data split We partition the images in the
off parameters. Note that V∗(W∗(A)) is a func- PathVQAdatasetalongwiththeassociatedques-
tionofW∗(A)sinceV∗(W∗(A))isafunctionof
tionsintoatrainingset,validationset,andtesting
(cid:107)V − W∗(A)(cid:107)2 2 which is a function of W∗(A). set with a ratio of about 3:1:1. In the PathVQA
Similarly,U∗(T∗(A))isafunctionofT∗(A).
dataset,thefrequenciesofquestioncategoriesare
Inthethirdstage,weapplytheoptimallytrained imbalanced. Becauseofthis,duringthepartition
VQA model including V∗(W∗(A)), U∗(T∗(A)), process, we perform sampling to ensure the fre-
and R∗ to make predictions on the valida- quenciesofthesecategoriesineachsettobeconsis-
tion dataset. Then we learn the selection tent. Intheend,thereare19,755question-answer
variables A by minimizing the validation loss pairsinthetrainingset,6,279inthevalidationset,
(cid:80)N(val) L(d(val) ,V∗(W∗(A)),U∗(T∗(A)),R∗). and6,761inthetestingset.
i=1 i
Table2: Accuracy(%),BLEU-n(%),andF1(%)achievedbydifferentmethods. Wedenotecross-modalSSLon
image-questionpairsandimage-answerpairsasCMSSL-IQandCMSSL-IA.
Method Accuracy BLEU-1 BLEU-2 BLEU-3 F1
Method1withoutimage 49.2 50.2 2.8 1.2 9.5
Method1 57.6 57.4 3.1 1.3 9.9
Method1withCMSSL-IQ 58.7 59.0 3.5 2.1 11.0
Method1withCMSSL-IQ+three-leveloptimizationframework 63.4 63.7 4.1 2.5 12.2
Method1withCMSSL-IA 58.6 58.9 3.4 2.0 10.3
Method1withCMSSL-IA+three-leveloptimizationframework 62.4 62.2 3.6 2.3 12.0
Method2withoutimage 46.2 46.5 1.0 0.0 0.8
Method2 55.1 56.2 3.2 1.2 8.4
Method2withCMSSL-IQ 55.9 57.1 3.4 1.4 9.2
Method2withCMSSL-IQ+three-leveloptimizationframework 58.9 59.1 3.8 1.6 9.2
Method2withCMSSL-IA 55.9 57.1 3.5 1.5 9.2
Method2withCMSSL-IA+three-leveloptimizationframework 58.8 59.1 4.0 1.6 9.4
Evaluation metrics We perform evaluation us- Onemaysuspecthowmuchinformationinim-
ing three metrics: 1) accuracy (Malinowski and agesisusedduringtheinferenceoftheanswers?
Fritz,2014)whichmeasuresthepercentageofin- Coulditbepossiblethatthemodelssimplylearn
ferredanswersthatmatchexactlywiththeground- the correlations between questions and answers
truth using string matching; only exact matches andignoretheimages? Inlightoftheseconcerns,
are considered as correct; 2) macro-averaged we perform studies where the images are not fed
F1(GoutteandGaussier,2005),whichmeasures intoVQAmodelsandonlyquestionsareusedas
theaverageoverlapbetweenthepredictedanswers inputs for inferring answers. Table 2 shows the
and ground-truth, where the answers are treated resultsofnotusingimages(“Method1/2without
asbagoftokens;3)BLEU(Papinenietal.,2002), image”). As can be seen, for both Method 1 and
whichmeasuresthesimilarityofpredictedanswers 2,ignoringimagesleadstosubstantialdegradation
andground-truthbymatchingn-grams. of performance. This shows that images in our
datasetprovidevaluableinformationforVQAand
5.2 Results PathVQAisameaningfulVQAdataset. Themod-
elstrainedonourdatasetsarenotdegeneratedto
Table2showstheVQAperformanceachievedby
simplycapturingthecorrelationbetweenquestions
different methods. From this table, we make the
andanswers.
followingobservations. First, forbothMethod1
andMethod2,applyingourthree-leveloptimiza- 6 Conclusion
tion based framework improves the performance.
Inthispaper,webuildapathologyVQAdataset–
Ourframeworklearnstoidentifyandremovenoisy
PathVQA–thatcontains32,795question-answer
anderroneousSSLtrainingexamples,whichcan
pairsof8categories,generatedfrom4,998images.
avoidthemodeltobedistortedbysuchbad-quality
Majorityofquestionsinourdatasetareopen-ended,
examples. Second,forbothMethod1and2,apply-
posing great challenges for the medical VQA re-
ingcross-modalSSL(CMSSL)methodsincluding
search. Our dataset is publicly available. To ad-
CMSSL-IQandCMSSL-IAimprovestheperfor-
dressthechallengesthattheself-supervisedtrain-
mance, which demonstrates the effectiveness of
ingdatamaycontainerrorsandtheeffectiverepre-
CMSSL. CMSSL uses auxiliary tasks, including
sentationsofpathologyimagesandquestionsare
judgingwhetheranimagematcheswithaquestion
difficulttolearnonlimiteddata,weproposeathree-
andjudgingwhetheranimagematcheswithanan-
leveloptimizationframeworktoautomaticallyiden-
swer, to learn semantic correspondence between
tifyandremoveproblematicSSLtrainingexamples
image regions and words in questions/answers,
andlearnsample-efficientvisualandtextualrepre-
whichcanimprovetheeffectivenessofvisualand
sentations. ExperimentsonthePathVQAdataset
textual representations for accurate VQA. It also
demonstratetheeffectivenessofourmethod.
learnsimageandtextencodersbyencouragesthe
Acknowledgement
image and text encoders to solve auxiliary tasks,
which reduces the risk of overfitting to the data- ThisworkissupportedbygiftfundsfromTencent
deficientVQAtaskonthesmall-sizedtrainingdata. AILabandAmazonAWS.
References Diederik Kingma and Jimmy Ba. 2014a. Adam: A
method for stochastic optimization. International
Asma Ben Abacha, Sadid A Hasan, Vivek V Datla,
ConferenceonLearningRepresentations.
Joey Liu, Dina Demner-Fushman, and Henning
Mu¨ller. 2019. Vqa-med: Overview of the medical Diederik P Kingma and Jimmy Ba. 2014b. Adam: A
visualquestionansweringtaskatimageclef2019. In method for stochastic optimization. arXiv preprint
CLEF2019WorkingNotes. arXiv:1412.6980.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- Diederik P Kingma and Max Welling. 2013. Auto-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick, encoding variational bayes. arXiv preprint
and Devi Parikh. 2015. Vqa: Visual question an- arXiv:1312.6114.
swering. InICCV.
Dan Klein and Christopher D Manning. 2003. Accu-
StanislawAntol,CLawrenceZitnick,andDeviParikh. rateunlexicalizedparsing. InACL.
2014. Zero-shot learning via visual abstraction. In
ECCV. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Kyunghyun Cho, Bart Van Merrie¨nboer, Caglar Gul- YannisKalantidis,Li-JiaLi,DavidAShamma,etal.
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger 2017. Visualgenome: Connectinglanguageandvi-
Schwenk, and Yoshua Bengio. 2014. Learning sion using crowdsourced dense image annotations.
phrase representations using rnn encoder-decoder InternationalJournalofComputerVision.
for statistical machine translation. arXiv preprint
arXiv:1406.1078. Jason J Lau, Soumya Gayen, Asma Ben Abacha, and
Dina Demner-Fushman. 2018. A dataset of clini-
BonnieDorr,DavidZajic,andRichardSchwartz.2003. cally generated visual questions and answers about
Hedgetrimmer: Aparse-and-trimapproachtohead- radiologyimages. Scientificdata.
linegeneration. InHLT-NAACLworkshop.
David Levison, Robin Reid, Alistair D Burt, David J
Zhihao Fan, Zhongyu Wei, Piji Li, Yanyan Lan, and Harrison, and Stewart Fleming. 2012. Muir’s text-
Xuanjing Huang. 2018. A question type driven bookofpathology. CRCPress.
frameworktodiversifyvisualquestiongeneration.
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui
Hsieh, and Kai-Wei Chang. 2019. Visualbert: A
Cyril Goutte and Eric Gaussier. 2005. A probabilistic
simple and performant baseline for vision and lan-
interpretation of precision, recall and f-score, with
guage. arXivpreprintarXiv:1908.03557.
implicationforevaluation. InEuropeanConference
onInformationRetrieval.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r,
Yash Goyal, Tejas Khot, Douglas Summers-Stay,
and C Lawrence Zitnick. 2014. Microsoft coco:
Dhruv Batra, and Devi Parikh. 2017. Making the
Commonobjectsincontext. InECCV.
v in vqa matter: Elevating the role of image under-
standinginvisualquestionanswering. InCVPR.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan
Lee. 2019. Vilbert: Pretraining task-agnostic visi-
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
olinguistic representations for vision-and-language
Ross Girshick. 2019. Momentum contrast for un-
tasks. In Advances in Neural Information Process-
supervised visual representation learning. arXiv
ingSystems,pages13–23.
preprintarXiv:1911.05722.
Mateusz Malinowski and Mario Fritz. 2014. A multi-
Michael Heilman and Noah A Smith. 2009. Question
world approach to question answering about real-
generation via overgenerating transformations and
worldscenesbasedonuncertaininput. InNIPS.
ranking. Technicalreport.
Christopher Manning, Mihai Surdeanu, John Bauer,
Justin Johnson, Bharath Hariharan, Laurens van der
JennyFinkel,StevenBethard,andDavidMcClosky.
Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
2014. TheStanfordCoreNLPnaturallanguagepro-
Girshick.2017. Clevr:Adiagnosticdatasetforcom-
cessingtoolkit. InACL.
positionallanguageandelementaryvisualreasoning.
InCVPR. RobertMuiretal.1941. Text-bookofpathology. Text-
BookofPathology.,(FifthEdition).
Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,
Jonghyun Choi, Ali Farhadi, and Hannaneh Ha- KishorePapineni,SalimRoukos,ToddWard,andWei-
jishirzi.2017. Areyousmarterthanasixthgrader? JingZhu.2002. Bleu: amethodforautomaticeval-
textbook question answering for multimodal ma- uationofmachinetranslation. InACL.
chinecomprehension. InCVPR.
Jeffrey Pennington, Richard Socher, and Christopher
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Manning.2014. Glove:Globalvectorsforwordrep-
2018. Bilinearattentionnetworks. InNIPS. resentation. InEMNLP.
Mengye Ren, Ryan Kiros, and Richard Zemel. 2015a.
Image question answering: A visual semantic em-
beddingmodelandanewdataset. NIPS.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel
Urtasun. 2018. Learning to reweight exam-
ples for robust deep learning. arXiv preprint
arXiv:1803.09050.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun. 2015b. Faster r-cnn: Towards real-time ob-
ject detection with region proposal networks. In
Advancesinneuralinformationprocessingsystems,
pages91–99.
Stanley L Robbins, Marcia Angell, and Vinay Kumar.
1981. Basicpathology. WBSaunders.
Lei Shi, Kai Shuang, Shijie Geng, Peng Su, Zhengkai
Jiang, Peng Gao, Zuohui Fu, Gerard de Melo, and
SenSu.2020. Contrastivevisual-linguisticpretrain-
ing. arXivpreprintarXiv:2007.13135.
Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping
Zhou, Zongben Xu, and Deyu Meng. 2019. Meta-
weight-net: Learning an explicit mapping for sam-
ple weighting. In Advances in Neural Information
ProcessingSystems,pages1919–1930.
NathanSilberman,DerekHoiem,PushmeetKohli,and
RobFergus.2012. Indoorsegmentationandsupport
inferencefromrgbdimages. InECCV.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The journal of machine learning
research,15(1):1929–1958.
Hao Tan and Mohit Bansal. 2019. Lxmert: Learning
cross-modality encoder representations from trans-
formers. arXivpreprintarXiv:1908.07490.
Kristina Toutanova, Chris Brockett, Michael Gamon,
Jagadeesh Jagarlamudi, Hisami Suzuki, and Lucy
Vanderwende.2007. Thepythysummarizationsys-
tem: Microsoft research at duc 2007. In Proc. of
DUC.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessingsystems,pages5998–6008.
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alex Smola. 2016. Stacked attention networks
forimagequestionanswering. InCVPR.
C Lawrence Zitnick and Devi Parikh. 2013. Bring-
ingsemanticsintofocususingvisualabstraction. In
CVPR.
Table3: Statisticsofthedatasplit. with a question. We optimize the selection vari-
ables using the Adam optimizer, with an initial
Trainingset Validationset Testset learningrateof0.01. Wesetγ andγ to0.3and
1 2
#images 3,021 987 990
0.7respectively.
#QApairs 19,755 6,279 6,761
B DatasetCreation
Appendix Wedevelopasemi-automatedpipelinetogenerate
apathologyVQAdatasetfrompathologytextbooks
A Experimentalsetup
andonlinedigitallibraries. Wemanuallycheckthe
Table3showsdatasetsplitstatistics. Weimplement automatically-generatedquestion-answerpairsto
the methods using PyTorch and perform training fix grammar errors. The automated pipeline con-
onfourGTX1080TiGPUs. sistsoftwosteps: (1)extractingpathologyimages
Webasicallyfollowtheoriginalmodelconfigu- andtheircaptionsfromelectronicpathologytext-
rationsusedin(TanandBansal,2019), (Kimetal., booksandthePathologyEducationInformational
2018),and(Yangetal.,2016). Dataaugmentation Resource(PEIR)DigitalLibrary3 website;(2)gen-
is applied to images, including shifting, scaling, eratingquestions-answerpairsfromcaptions.
andshearing. Fromquestionsandanswersinthe
B.1 ExtractingPathologyImagesand
PathVQAdataset,wecreateavocabularyof4,631
Captions
wordsthathavethehighestfrequencies.
Given a pathology textbook that is in the PDF
In Method 1, we use the default hyperparam-
format and available online publicly, we use
eter settings in (Tan and Bansal, 2019). For the
two third-party tools PyPDF24 and PDFMiner5
textencoder, thehiddensizewassetto768. The
to extract images and the associated captions
image features were extracted from the outputs
therefrom. PyPDF2 provides APIs to access
oftheFaster-RCNNnetwork,whichispretrained
on BCCD2 – a medical dataset containing blood the “Resources” object in each PDF page where
the “XObject” gives information about images.
cells photos, as well as on Visual Genome (Kr-
PDFMiner allows one to obtain text along with
ishnaetal.,2017). Theinitiallearningratewasset
its exact location in a page. To extract image
to 5e-5 with the Adam (Kingma and Ba, 2014a)
captions from text in each page, we use regular
optimizer used. The batch size was set to 256.
expressions to search for snippets with prefixes
The model was trained for 200 epochs. In the
of“Fig.”or“Figure”followedbyfigurenumbers
SSLpretrainingtaskonMethod1,wetrainalin-
andcaptiontexts. Forapagecontainingmultiple
ear classifier with a dimension of 1,280 to judge
images, we order them based on their locations;
whether an image matches with a question. In
the same for the captions. Images and locations
Method2,wordsinquestionsandanswersarerep-
arematchedbasedontheirorder. Givenanonline
resentedusingGloVe(Penningtonetal.,2014)vec-
pathology digital library such as PEIR, we use
torspretrainedongeneral-domaincorporasuchas
two third-party tools Requests6 and Beautiful
Wikipedia, Twitter, etc. The image features are
Soup7 tocrawlimagesandtheassociatedcaptions.
extracted from the outputs of the Faster-RCNN
Requests is an HTTP library built using Python
networkpretrainedonBCCDandVisualGenome.
and provides APIs to send HTTP/1.1 requests.
Givenanimageandaquestion,themodeloutputs
Beautiful Soup generates the ‘http.parser’ and
ananswerfrom apredefinedsetofanswers. The
can access the urls and tags of the images on
dropout(Srivastavaetal.,2014)rateforthelinear
the website pages. Given a set of urls, we use
mapping was set to 0.2 while for the classifier it
RequeststoreadwebsitepagesanduseBeautiful
was set to 0.5. The initial learning rate was set
SouptofindimagesunderthetargetedHTMLtags
to0.005withtheAdamaxoptimizer(Kingmaand
includingtheContentDivisionelement(cid:104)div(cid:105),the
Ba, 2014b) used. The batch size was set to 512.
unordered list element (cid:104)ul(cid:105), and the (cid:104)li(cid:105) element.
Themodelwastrainedfor200epochs. IntheSSL
pretraining task on Method 2, similar to that on 3http://peir.path.uab.edu/library/index.php?/category/2
Method1,wetrainalinearclassifierwithadimen- 4https://github.com/mstamy2/PyPDF2
sionof1,280topredictwhetheranimagematches 5https://github.com/pdfminer/pdfminer.six
6https://requests.readthedocs.io/en/master/
2https://public.roboflow.ai/object-detection/bccd 7https://www.crummy.com/software/BeautifulSoup/
Table4: Numberofquestionsindifferentcategoriesineachset
Questiontypes
Dataset
What Where How Howmuch/many Why Yes/No
Trainingset 8083 1316 366 62 71 9804
Validationset 2565 409 108 21 21 3135
Testingset 2753 432 121 18 22 3390
Simple
Captions Sentences Questions
Question
Source Preprocessing Simplification
Transducer
Post-processing QA-pairs
AnswerPhrases
Figure2: Theframeworkofgeneratingquestionsfromcaptions
ThenwecandownloadimageswithRequestsand
Microscopy shows coagulative necrosis of the affected
write their captions directly to local files. Given
L o n g
bowel wall and thrombosed vessels while the junction
complex
w ith normal intestine is indistinct and shows an
the extracted image-caption pairs, we perform sentence inflammatory infiltrate.
post-processing including (1) removing images
Rearrange subjects, verbs, clauses
thatarenotpathologyimages,suchasflowcharts
Microscopy shows coagulative necrosis of the affected
and portraits; (2) correcting erroneous matching Simple bowel wall and thrombosed vessels.
short
betweenimagesandcaptions. sentences The junction with normal intestine is indistinct.
The junction shows an indistinct inflammatory infiltrate.
B.2 QuestionGeneration
In this section, we discuss how to semi- Figure3: Sentencesimplification
automatically generate questions from captions.
Figure 2 shows the overall framework. We per-
phrases starting with “inner”, “within”, “on the
form natural language processing of the captions
right/leftof”;“how”questionsforadjectivewords
usingtheStanfordCoreNLP(KleinandManning,
and phrases starting with “using”, “via”, “with”,
2003) toolkit, including sentence split, tokeniza-
and “through”, and “what” questions for the re-
tion, part-of-speech (POS) tagging, named entity
mainingnounphrases. Table5showsanexample
recognition(NER),constituentparsing,anddepen-
foreachtypeofquestions.
dencyparsing. Manysentencesarelong,withcom-
plicatedsyntacticstructures. Weperformsentence We use Tregex from Stanford CoreNLP tools
simplification to break a long sentence into sev- (Manning et al., 2014), a tree query language in-
eralshortones. Giventhesubjects,verbs,clauses, cluding various relational operators based on the
etc. labeled by POS tagging and syntactic pars- primitiverelationsofimmediatedominanceandim-
ing, we rearrange them using the rules proposed mediateprecedence,toimplementtherules(Heil-
in (Toutanova et al., 2007; Dorr et al., 2003) to manandSmith,2009)fortransformingdeclarative
achievesimplification. Figure3showsanexample. sentences(captions)intoquestions.
Given the POS tags and named entities of the Toreducegrammaticalerrors,weavoidgenerat-
simplified sentences, we generate questions for ingquestionsonsentenceswithadverbialclauses
them: including“when”-typeofquestionsfordate suchas“chronicinflammationinthelung,showing
andtimeentitiesandphrasessuchas“in/during... all three characteristic histologic features”. The
stage/period”, “before ...”, and “after ...”; “how question transducer mainly contains three steps.
much/how many”-type of questions for words First, we perform the main verb decomposition
tagged as numbers; “whose” questions for pos- based on the tense of the verb. For instance, we
sessive pronouns (e.g., “its”, “their”); “where” decompose “shows” to “does show”. It is worth
questions for location entities and prepositional notingthatforpassivesentenceswithastructureof
Type Originalsentence Question
Theendofthelongboneisexpanded Whatisexpanded
What
intheregionofepiphysis. intheregionofepiphysis?
Theleftventricleisonthelowerright Whereistheleftventricle
Where
inthisapicalfour-chamberviewoftheheart. inthisapicalfour-chamberviewoftheheart?
When After1yearofabstinence,mostscarsaregone. Whenaremostscarsgone?
Howmuch/Howmany Twomulti-facetedgallstonesarepresentinthelumen. Howmanymulti-facetedgallstonesarepresentinthelumen?
Thetumorcellsandtheirnucleiarefairlyuniform, Thetumorcellsandwhosenucleiarefairlyuniform,
Whose
givingamonotonousappearance. givingamonotonousappearance?
Thetrabecularboneformingthemarrowspaceshowstrabeculae Howdoesthetrabecularbone
How
withosteoclasticactivityatthemargins. formingthemarrowspaceshowtrabeculae?
Table5: Examplesofgeneratedquestionsfordifferenttypes
Micr oscopy shows coagulative necrosis of the affected bowel wall and to correct misspellings, syntactic errors, and se-
thro mbosed vessels. Answer phrases
manticinconsistencies. Thequestionsandanswers
S u b j e c t- a u x ili a ry inversion
Does m i croscopy show coagulative necrosis of the affected bowel wall and
arefurthercleanedbyremovingextraspacesand
thrombo sed vessels. Answer phrases irrelevant symbols. Questions that are too short
Insert question phrases
or vague are removed. Articles appearing at the
What do es microscopy show of the affected bowel wall and thrombosed vessels.
Questio n phrases beginningofanswersarestripped.
C AdditionalRelatedWorks
Figure4: Syntactictransformation
C.1 VQAdatasets
“be+shown/presented/demonstrated”,wekeeptheir A number of visual question answering datasets
originalformsratherthanperformingtheverbde- have been developed in the general domain.
composition. Second,weperformsubject-auxiliary DAQUAR (Malinowski and Fritz, 2014) is built
inversion. Weinvertthesubjectandtheauxiliary on top of the NYU-Depth V2 dataset (Silberman
verbinthedeclarativesentencestoformtheinter- et al., 2012) which contains RGBD images of in-
rogativesentence. Aftertheinversion, thebinary door scenes. DAQUAR consists of (1) synthetic
“yes/no”questionsaregenerated. Forinstance,as question-answerpairsthatareautomaticallygen-
showninFigure4,thesentence“microscopyshows eratedbasedontextualtemplatesand(2)human-
coagulative necrosis of the affected bowel wall createdquestion-answerpairsproducedbyfivean-
and thrombosed vessels” is inverted to “does mi- notators. The VQA dataset (Antol et al., 2015)
croscopyshowcoagulativenecrosisoftheaffected is developed on real images in MS COCO (Lin
bowelwallandthrombosedvessels?”. Togenerate et al., 2014) and abstract scene images in (An-
questionswhoseanswersare“no”,werandomlyse- tol et al., 2014; Zitnick and Parikh, 2013). The
lectaphrasewiththesamePOStaggingfromother question-answerpairsarecreatedbyhumananno-
captionstoreplacetheheadwordsintheoriginal tatorswhoareencouragedtoask“interesting”and
question. For example, we replace “coagulative “diverse”questions. VQAv2(Goyaletal.,2017)is
necrosis”inthesentence“doesmicroscopyshow extendedfromtheVQA(Antoletal.,2015)dataset
coagulative necrosis of the affected bowel wall to achieve more balance between visual and tex-
andthrombosedvessels”withothernounphrases. tualinformation,bycollectingcomplementaryim-
Third, we remove the target answer phrases and agesinawaythateachquestionisassociatedwith
insert the question phrase obtained previously to a pair of similar images with different answers.
generateopen-endedquestionsbelongingtotypes IntheCOCO-QA(Renetal.,2015a)dataset,the
of“what”,“where”,“when”,“whose”,“how”,and question-answerpairsareautomaticallygenerated
“howmuch/howmany”asshowninTable5. For from image captions based on syntactic parsing
instance,wetransduce“microscopyshowscoagula- andlinguisticrules. CLEVR(Johnsonetal.,2017;
tivenecrosisoftheaffectedbowelwallandthrom- Kembhavi et al., 2017) is a dataset developed on
bosedvessels”to“whatoftheaffectedbowelwall rendered images of spatially related objects (in-
andthrombosedvesselsdoesmicroscopyshow?” cludingcube,sphere,andcylinder)withdifferent
asshowninFigure4. Giventheautomaticallygen- sizes,materials,andcolors. Thelocationsandat-
eratedquestionswhichmaycontainsyntacticand tributes of objects are annotated for each image.
semanticerrors,weperformpost-processingtofix Thequestionsareautomaticallygeneratedfromthe
thoseissues. Wemanuallyproofreadallquestions annotations.
Table6: ComparisonofVQAdatasets ciatedsideinformation(e.g.,captions,modalities)
fromtheMedPix8 databaseandgeneratequestion-
Domain #images #QApairs Answertype
answer pairs based on manually-defined patterns
DAQUAR General 1,449 12,468 Open
VQA General 204K 614K Open/MC in(Lauetal.,2018).
VQAv2 General 204K 1.1M Open/MC
COCO-QA General 123K 118K Open/MC
D Numberofquestionsindifferent
CLEVR General 100K 999K Open
categoriesfortraining,validation,and
VQA-Med Medical 4,200 15,292 Open/MC
VQA-RAD Medical 315 3,515 Open/MC testset
Ours Medical 4,998 32,795 Open
Forourdatasplit,thenumberofquestionsindif-
ferentcategoriesineachsetisshowninTable4.
The comparison of existing VQA datasets is
showninTable6. Thefirstfivedatasetsareinthe
generaldomainwhilethelastthreeareinthemed-
icaldomain. Notsurprisingly,thesizeofgeneral-
domaindatasets(includingthenumberofimages
andquestion-answerpairs)ismuchlargerthanthat
ofmedicaldatasetssincegeneral-domainimages
are much more available publicly and there are
manyqualifiedhumanannotatorstogenerateQA
pairsongeneralimages. Ourdatasetislargerthan
the two medical datasets: VQA-Med and VQA-
RAD,andmajorityofquestionsinourdatasetare
open-endedwhilemajorityofquestionsinVQA-
MedandVQA-RADareinmultiple-choicesstyle.
C.2 AutomaticConstructionof
Question-AnswerPairs
Existingdatasetshaveusedautomatedmethodsfor
constructingquestion-answerpairs. InDAQUAR,
questions are generated with templates, such as
“Howmany{object}arein{image id}?”. These
templatesareinstantiatedwithground-truthfacts
from the database. In COCO-QA, the authors
develop a question generation algorithm based
on the Stanford syntactic parser (Klein and Man-
ning, 2003), and they form four types of ques-
tions—“object”,“number”,“color”,and“location”
usinghand-craftedrules. InCLEVR,thelocations
andattributesofobjectsineachimagearefullyan-
notated,basedonwhichthequestionsaregenerated
byanautomatedalgorithm. Theiralgorithmcannot
beappliedtonaturalimageswheredetailedanno-
tationofobjectsandscenesareverydifficulttoob-
tain. In(Fanetal.,2018),theauthorsdevelopacon-
ditionalauto-encoder(KingmaandWelling,2013)
modeltoautomaticallygeneratequestionsfromim-
ages. Totrainsuchamodel,image-questionpairs
areneeded,whichincursachicken-and-eggprob-
lem: thegoalistogeneratequestions,butrealizing
thisgoalneedsgeneratedquestions. InVQA-Med,
theauthorscollectmedicalimagesalongwithasso- 8https://medpix.nlm.nih.gov

DETECTIONANDEVALUATIONOFHUMANANDMACHINEGENERATEDSPEECHIN
SPOOFINGATTACKSONAUTOMATICSPEAKERVERIFICATIONSYSTEMS
YangGao∗,JiachenLian∗,BhikshaRaj+,RitaSingh+
∗ElectricalandComputerEngineering,+LanguageTechnologiesInstitute
CarnegieMellonUniversity
Pittsburgh,PA,USA-15213
ABSTRACT andwavenetmodels[2,3], thequalityofsyntheticspeechis
gettingmuchclosertonaturalspeech[3]. Attackscarriedout
Automaticspeakerverification(ASV)systemsutilizethebio-
using synthetic speech generated by these methods pose se-
metric information in human speech to verify the speaker’s
rious threats to ASV systems. As a first step, in this paper,
identity. The techniques used for performing speaker verifi-
we compare the threats from impersonation attacks with the
cation are often vulnerable to malicious attacks that attempt
synthetic speech attacks and establish that deep synthesized
to induce the ASV system to return wrong results, allowing
fakesareinfactthemostdangerousattacksforASVsystems.
an impostor to bypass the system and gain access. Attack-
Itisthereforeimportanttobeabletodistinguishbetween
ers use a multitude of spoofing techniques for this, such as
fake/synthesized and human-generated speech. This is the
voice conversion, audio replay, speech synthesis, etc. In re-
broadergoalofourpaper. However,wedonotfocusonmere
cent years, easily available tools to generate deepfaked au-
featureselectionsas[4–6],whichwouldbeinfluencedbythe
dio have increased the potential threat to ASV systems. In
datasetchoices,modelsandtrainingprocedures. Instead,we
thispaper,wecomparethepotentialofhumanimpersonation
startwiththehypothesisthatmachine-generatedspeechistoo
(voicedisguise)basedattackswithattacksbasedonmachine-
consistentinmanyrespects,andmachinesareunabletoem- generatedspeech,onblack-boxandwhite-boxASVsystems.
ulate the finer level variations found in naturally produced
Wealsostudycountermeasuresbyusingfeaturesthatcapture
speech signals. In other words, because of the complexity
the unique aspects of human speech production, under the
of the human speech production mechanism, human speech
hypothesis that machines cannot emulate many of the fine-
hasagreaterdegreeofinconsistencythanmachine-generated
levelintricaciesofthehumanspeechproductionmechanism.
speech. We devise experiments to investigate a select set of
We show that fundamental frequency sequence-related en-
features that we believe capture some intricacies of human-
tropy,spectralenvelope,andaperiodicparametersarepromis-
generatedspeechinamannerthatmachinescannot.Toverify
ingcandidatesforrobustdetectionofdeepfakedspeechgen-
thecorrectnessofourhypothesis,wemustnotonlyevaluate
eratedbyunknownmethods.
the features directly on detecting fake and real speech sig-
Index Terms— Impersonation, Deepfakes, ASV, Spoof nalsbutalsoestablishthisthroughapplicationssuchasASV
detection systems, that must then, by the use of these features in the
countermeasures, become more impervious to malicious at-
1. INTRODUCTION tacks. Weproposeseveralspeech-generation-relatedfeatures
andverifythemtobeeffectiveinimprovingtheoverallper-
Automaticspeakerverification(ASV)systemsutilizethebio- formanceofthedetectionmodelandASVsystems. Theex-
metricinformationinhumanspeechtoverifytheidentityof perimentsdiagramofthispaperisshownasinFig. 1.
a speaker by matching it with the information present in a
database(whichisalsoderivedfromspeechsamples). Such
1.1. Priorwork
systemsarealsovulnerabletomaliciousattackswheretheat-
tackertriestoprovidefakebiometricinformationtofoolthe A number of approaches of varying success have been pro-
ASV systems. There are many spoofing methods in use by posed in the literature to detect fake speech to increase the
attackersnowadays,includingdirecthumanimpersonationof securityofASVsystemsagainstspoofingattacks. Forexam-
the target, machine assisted-speech generation such as voice ple, the long-running ASVspoof challenge [7–9] has raised
conversion(VC),customizedandmanipulatedtext-to-speech wide efforts in fake speech spoofing attack countermeasures
synthesis (TTS) system outputs, etc. With the advancement onASVsystems. Themainfocusofthechallenge,however,
ofdeeplearningtechniques,especiallywithadvancementsin has been to rank spoof detection countermeasures, and not
generativemodelssuchasgenerativeadversarialnetworks[1] tocarryoutanin-depthevaluationoftheASVsystems’per-
0202
voN
52
]DS.sc[
2v98630.1102:viXra
Experimental pipelines Pairings for attack comprises 1091 utterances of genuine speech from both the
Standard features: RS for same target: real from
Proposed features: target, synthesis of target impersonators and the political figures and 981 utterances
RI for same target: real from
Datasets: Synthetic target, impersonation of target of impersonated speech. The data are further segregated
ASVspoofLA real speech: 1 speech (S)
ASVspoofLA synthetic speech: 2 into paired sets, each pair containing an impersonator’s real
Impersonation dataset: 3 2 RI Table 1 speechandtarget/mimickedspeech,orthetarget’srealspeech
Blackbox
Table 3 1,2 Fake/real PA eS rV fo s ry ms ate nm ce RS Table 2 andthespeechproducedforthesametargetbyanimperson-
EER 1,2 detection Assessment ator. These are indicated in Table 1, in which pairs that
RS
1 3 Whitebox Table 2 originatefromthesamespeakerarecalledpositivepairs,and
Human pairsfromdifferentspeakersarecallednegativepairs.
Speech (H) As shown in Table 1, we have two sets of positive pairs
R imea pl e ( rR so) nated (I) ASV scores T Ta -b Dl Ce F 3 andfoursetsofnegativepairs. Theyare19086positivepairs
Synthesized (S)
of same target speaker’s real utterances (R), 14844 negative
Fig.1. Diagramfortheexperimentalsetups pairs of different target speaker’s real utterances (RI), 3382
positivepairsofsameimpersonator’simpersonationsfordif-
ferent target (IAB), 37554 negative pairs of target and im-
formanceunderattacks.Anothersignificantproblemwiththe
personationpair(TI),1988negativepairsofdifferentimper-
spoofdetectionstudyisthat,withtherapidevolutionofdeep-
sonator’srealutterances(IRAB)and28080negativepairsof
fake generation methodologies, the sheer variety of attacks
target/impersonator’srealutterancepair(IRT).
thatanAVSsystemmaybesubjecttoisalsorapidlyincreas-
For synthetic data, ASVspoof2019 dataset contains logi-
ing. Detection modelstrainedonaspecificprovided dataset
calaccessdataandphysicalaccessdata.Inthisstudy,weonly
synthesized using a limited set of methods are not likely to
usethelogicalaccessdatawhichcontainsmachine-generated
generalize well to newer types of fake/generated audio [10].
speechusingmultipletext-to-speechsynthesisandvoicecon-
InASVspoof2019,forexample,detectionalgorithms[11–14]
version methods. The logical data has 2580 bonafide utter-
that work very well on training datasets are often found to
ancesand22800syntheticutterancesfrom20speakersinthe
perform much worse on evaluation sets that have been pro-
training set; 2548 bonafide utterances from 20 speakers and
ducedusingattacktechniquesnotpresentinthetrainingdata.
22296spoofutterancesfrom10speakersinthedevelopment
Thedetectionperformanceontheevaluationsetscouldbean
set[9]. Theevaluationsetcontains7355bonafideutterances
indicatorofthegeneralizationcapacityofthoseproposedal-
from67speakersand63882spoofutterancesfrom48speak-
gorithmsasonepurposeofthechallenge.
ers. Thespoofaudiosaregeneratedusingunseenspoofingal-
To include the latest deep-learning speech synthesizers,
gorithmsintentionally,aimingtogiveinsightsofthegeneral-
[15] provides a synthetic speech dataset called Fake or Real
izationperformanceoftheproposedcountermeasuremodels.
(FoR),improvingvarietyofthedeepfakespeechdataforthis
InorderforageneralASVsystemtoevaluatethisdataset,we
purpose. Wehave,infact,alsousediteffectivelyinthecon-
generate4914bonafidepositivepairsand4914negativepairs
textofthiswork.
for each attack (A07-A19) from original evaluation set. We
also generate 15970 positive pairs and 15970 negative pairs
2. COMPARINGHUMANANDMACHINE
toevaluatetheoverallattackingabilityoverallattacks.
GENERATEDSPEECHINSPOOFINGATTACKS
To best evaluate the threats of different attacks, we train
ASV systems under unconstrained recording and speaking
As a first step, in this paper, we compare the threats from
conditions(essentiallydata-in-the-wild). Forthis,weusethe
impersonation attacks with the synthetic speech attacks and
VoxCelebdataset,whichisalargescalepubliclydatasetcon-
establishthatdeepsynthesizedfakesareinfactthemostdan-
taining millions of utterances collected from unconstrained
gerousattacksforASVsystems.
speech samples [16]. It has many speakers and millions of
utterances under different recording conditions. This can be
2.1. Datasets
effectively used to evaluate the potential of any given ASV
methodology to generalize to unseen speakers and uncon-
Forourexperimentsweusefourdatasets: thelogicalaccess
strainedconditions[16,17].
(LA) of ASVspoof 2019 dataset [9], the VoxCeleb dataset
[16], the FoR dataset [15] and our own collected imperson-
ationdataset(CID).
2.2. Analyzing performance under attacks on black-box
The CID dataset is collected from the performances of
andwhite-boxASVsystems
expert impersonators on YouTube, and segmented carefully
to only keep the speech segments corresponding to target The ASV model we use is proposed by Joon Son Chung, et
speakers. All impersonators in the CID dataset are profes- al.[17],whichappliestheThinResNet-34[16]asbackbone,
sionalsmimickingpoliticalfiguresforamusement,collecting andSelf-attentivePooling(SAP)[18]asaggregationstrategy.
from TV shows and talk shows on YouTube. The dataset This model, when trained with short-time Fourier transform
Detection
Scores
(STFT)spectrogramofVoxceleb,generalizesextremelywell
tounconstrainedconditionsasshownbythelowEERofreal
utterancepairs,mentionedearlierinthissection.
The black-box ASV system is pretrained with the Vox-
Celeb dataset. STFT, mel-frequency cepstral coefficients
(MFCCs), aperiodic parameters(AP) and spectral enve-
lope(SP)areusedasinputfeaturestothismodel.Theoriginal
(a) (b)
inputaudioscomprisesegmentsof2secduration. Weusethe
same STFT feature as in [17] [16]. MFCC feature is com-
putedfrom16kHzsampledsignals: whichcomprise13cep-
stral coefficients, to which first and second-order derivatives
respectivelyareconcatenated,makingthefeaturedimension-
ality 39. (AP and SP are not the focused of this section and
willbefurtherdiscussedinSection.3andSection.4)
(c) (d)
Thewhite-boxmodelistrainedwiththeASVspoof2019
data,asamulti-classclassifierforspeakeridentification. We
Fig. 2. (a) 20 bonafide speakers in training set; (b) 20
make small modification on the initial ASVspoof2019 train-
bonafide and spoof speakers in training set; (c) 10 bonafide
ingsetbyassigningeachspoofedutteranceanidentitywhich
speakersindevelopmentset;(d)10bonafideandspoofspeak-
uniquelyincorporatesbothspeakerandattack. Thereare20
ersindevelopmentset.
speakersand6typesofattackintheASVspoof2019LAtrain-
ingset,meaningthatthereare120”spoofedidentities”. Thus
our modified training set contains 140 identities. We call
IAB + RI, showing that the IRAB pairs are valid, also indi-
theseASVspooftrainingidentities(ASVTIs).
cating the true differences between the impersonator’s real
voices. AndtheIAB+TIgivesanEERof43.52%. Thishigh
2.2.1. Impersonationattacks EERcomesfromtheformationofthisevaluationset. Differ-
entfromothersets,boththenegativepairsandpositivepairs
Our black-box evaluations on impersonation attacks use the can be seen as the spoofing attacks because the professional
CID dataset. We run several experiments to evaluate the impersonator could impersonate different target to a certain
dataset’s attacking potential. The results are shown in Table extent, which makes the ’positive’ pairs negative in nature.
1. ThemodelthatispretrainedonVoxCeleb2isabletover- Therefore, both the positive pairs of IAB and the negative
ify open-set speakers best and gives 1.71% EER for target pairs of TI are hardest cases, also showing by the EERs of
speakers’realutterances(positiveandnegativepairsR+RI); theircombinationwiththeRandRI.
ThispretrainedmodelcanbeseenasablackboxASVunder The R + IRT corresponds to positive pairs for the real
open-setevaluation. voiceutterancesofthesametargetsandnegativepairsofim-
Fromourtests,weobservethatcombiningtheimperson- personator’srealvoicewiththetargets’realvoice.The5.21%
ation/targetpairs(TI)withthepositivepairsfromrealspeak- EERshowsthattheimpersonator’srealvoicesareindeednot
ers (R) improves the speaker verification EER to 11.42%, similartothetargets’voices.
which indicates that professional impersonation can fool the The overall results show that while mimicry from ama-
ASVsystemtoacertainextent,althoughitisstillineffective teurimpersonatorsisreportedtonotsucceedinfoolingASV
inmostattacks.Thegroupwithrealspeakerpositivepairs(R) systemsinpreviousresearch[19,20],mimicriesrenderedby
andnegativepairs(IRAB)builtfromtherealvoicesofdiffer- professionalimpersonatorsstillposesthreatstoacertainex-
entimpersonatorshasalowEERof4.86%,showingthatthe tent.
pre-trained ASV system is indeed generalized to verify un-
seentargetspeakersandcrossimpersonatorspairs.
2.2.2. Syntheticspeechattacks
The IAB is the same impersonator mimicking different
targets. IAB + RI has an EER of 13.3%, showing that even To further understand the attacks of synthetic speech gener-
if the same speaker tries to impersonate different targets, atedfromdifferentmethods,weperformextensiveASVeval-
their utterances are mostly considered as the same speaker, uationson theASVspoof evaluationset undertheblack-box
although still having some capacity to fool the ASV sys- andwhite-boxconditions. Theevaluationsetcontainsattack
tem. Note that this EER value gets significantly larger to methodsfromA07toA19whicharedifferentvoiceconver-
22.09%withlessgeneralizedmodels,suchastheVoxCeleb1 sionorspeechsynthesistechniques[21].
pre-trained model. This indicates that impersonation from Comparinghuman-generatedattacksandmachinegener-
professional impersonators is still threatening to some ASV atedattacks,asintheblack-boxscenariosforbothcases,we
systems. The IAB + IRAB set has comparable EERs as the found that the general attack ability of the machine gener-
Table1. EERsofimpersonationattackstotheASVunderblack-boxscenario
ASVEER%
ImpersonationData R1+RI2 IAB3+RI R+TI4 IAB+TI R+IRAB5 IAB+IRAB R+IRT6 IAB+IRT
VoxCeleb2(STFT) 1.71 13.30 11.42 43.52 4.86 17.76 5.21 19.45
VoxCeleb1(STFT) 4.16 16.41 14.95 42.02 4.74 15.90 5.06 15.64
blackbox VoxCeleb1(MFCC) 17.21 22.09 22.01 48.77 9.22 26.80 8.86 20.11
VoxCeleb1(AP) 39.75 41.27 44.89 45.46 45.58 46.06 41.65 42.94
VoxCeleb1(SP) 53.58 49.42 54.36 50.36 55.04 51.11 53.76 49.43
1R:Sametargetspeaker’srealutterancepair(+,#19086) 2RI:Differenttargetspeaker’srealutterancepair(−,#14844)
3IAB:Sameimpersonator,impersonationsfordifferenttargetpair(+,#3382) 4TI:Targetandimpersonationpair(−,#37554)
5IRAB:Differentimpersonator’srealutterancepair(−,#1988) 6IRT:Targetandimpersonator’srealutterancepair(−,#28080)
7Themodelpre-trainedwithVoxCeleb2devsetusingSpectrogramfeature
Table2. EERsofevaluationsetforASVspoof2019LAunderblack-boxandwhite-boxscenarios
ASVEER%
Attack A07 A08 A09 A10 A11 A12 A13 A14 A15 A16 A17 A18 A19 ALL3
VoxCeleb2(STFT) 34.03 23.20 5.70 48.51 37.37 43.42 23.67 40.45 43.14 50.51 4.99 7.10 11.26 21.42
VoxCeleb1(STFT) 27.93 25.30 11.01 47.77 37.36 44.77 30.93 43.33 40.91 43.36 7.65 10.83 13.97 22.03
VoxCeleb1(MFCC) 45.12 28.89 16.02 45.01 48.88 45.09 38.13 35.06 43.01 46.04 11.07 25.64 25.02 25.66
blackbox
VoxCeleb1(AP) 39.89 24.92 31.63 38.66 21.93 43.05 30.99 28.25 42.21 45.55 31.56 31.92 39.27 35.55
VoxCeleb1(SP) 51.47 50.73 53.51 51.46 53.97 49.52 54.51 49.92 51.39 50.66 52.71 49.18 54.97 50.60
Todiscoetal.,[9] 59.68 40.39 8.38 57.73 59.64 46.18 46.78 64.01 58.85 64.52 3.92 7.35 14.58 -
ASVSpoof(STFT)1 2.33 2.65 3.75 47.56 40.89 47.59 37.01 29.09 35.48 4.09 12.07 28.61 1.88 22.24
ASVSpoof(MFCC) 7.12 5.08 8.12 39.76 28.99 49.01 33.81 19.04 41.39 9.08 18.00 16.47 2.09 15.99
ASVSpoof(AP) 38.93 32.46 32.59 42.37 38.29 43.28 37.02 33.96 41.12 49.06 40.05 34.57 44.53 39.25
ASVSpoof(SP) 50.97 49.94 40.07 49.75 49.25 52.04 52.30 51.03 51.74 51.99 41.49 46.16 45.78 42.08
whitebox VoxCeleb2+ASVSpoof(STFT) 1.16 2.31 0.77 43.42 27.62 41.23 15.46 34.48 36.26 6.63 1.26 5.75 0.68 11.99
VoxCeleb1+ASVSpoof(STFT) 1.21 2.63 1.75 45.85 17.40 45.69 20.84 25.85 25.41 4.66 2.24 8.24 0.73 13.35
VoxCeleb1+ASVSpoof(MFCC) 4.99 4.51 1.99 37.28 19.02 45.08 33.18 15.92 33.65 6.01 11.30 11.44 2.98 14.94
VoxCeleb1+ASVSpoof(AP) 22.04 17.77 28.13 33.68 37.78 37.20 19.72 6.50 33.16 44.43 33.25 32.33 41.01 32.45
VoxCeleb1+ASVSpoof(SP) 50.24 44.30 40.51 49.21 48.82 50.45 48.74 48.62 49.43 50.86 33.63 42.10 38.06 36.79
1ThemodeltraineddirectlywithASVTIs4usingSpectrogramfeature
2Themodelpre-trainedwithVoxCeleb1devsetusingSpectrogram(blackbox)andsubsequentlytrainedwithASVTIs 3Evaluationongeneralpairsasdescribed
in2.1,indicatingoverallEER 4Asdefinedin2.2
atedspeecharestrongerthanthehuman-generatedattacks,as catingincreasedthreatfromdeepfakes.
showninTable.1andTable.2.FortheblackboxofVoxCeleb2 In conclusion, the feature robustness ranks as ’STFT >
trainedusingSTFT,mostofthesyntheticattackshaveaEER MFCC’withfinetuningunderwhite-boxscenario. Thiscon-
ofover20%,muchhigherthantheEERofimpersonationat- clusion is consistent with our hypothesis that features that
tacks(IAB). capture information about prosodic nuances are more robust
underattacksforASVsystems.
AsisshowninTable2,A09/A17/A18/A19arerelatively
In Figure. 2, we draw the embedding features from the
weakerattacksshowinglowerASVEER%inSTFT/MFCC-
bottleneck layer of the white-box ASV model on a sphere.
basedblack-boxes,whichisconsistentwiththeresultsgiven
When spoofed utterances are introduced, it is not easy to
by[9]. Theseattacksaregeneratedthroughwaveformgener-
discriminate the embeddings anymore, which indicates their
atorssuchaswaveformfilteringandspectralfiltering,which
threatstotheASVsystem.
may be simpler methods compared to hard cases using neu-
ralvocoders. Mostofattackstendtobemoredangerousfor
MFCC-basedblack-boxthanSTFT-basedblack-boxes. Also 3. STRATEGYFORESTABLISHINGTHE
forSTFTandMFCC,EERsformostattacksarelowerunder GOODNESSOFFEATURESFORFAKESPEECH
the white-box scenario, compared to attacks on black-boxes DETECTION
with the same dataset and feature settings. However, it does
not result in much improvement of EER for A10, A12, and The threatens shown above from the deepfakes indicates the
A15,whicharegeneratedbyneuralwaveformmodels,indi- urgent needs for fake speech detection to assist the anti-
spoofing capacity of ASV systems. To do so, our final goal
is to find robust features for detecting fake speech, espe-
ciallydeepfakes. Tore-iterate,ourhypothesisisthatfeatures
that capture the fine-level nuances of human speech from
a speech-production perspective are likely to be able to ef-
fectively help distinguish between real and fake speech. In
addition, they are also likely to improve the performance of
countermeasures that are used for thwarting ASV spoofing
attackscarriedoutthroughsyntheticspeech.
Fig. 3. The same text utterance (”Very early in my life, I
3.1. Humanvoice-productionbasedfeatures separated from my mother.” )’s spectral envelope (log scale)
of real speech contains natural transition and nuances while
Intheproductionofspeech,thereareseveralsourcesthatare thefakespeechdoesnot. Andtheshortpauseisunnaturally
either aperiodic or periodic that generate acoustic energy in sharpinfakespeech.
thevocaltract.Theaperiodicsourcesareaspirationgenerated
at the glottis, friction generated in the vocal tract, and tran-
sientburstsfromtherapidreleaseofcompleteconstrictions. well-studied jitter and shimmer measurements, which gauge
The periodic source is the vibration of the vocal folds that the cycle-to-cycle variations in frequency and amplitude of
createsperiodicenergyattheglottis. Identifyingandquanti- the speech signal, respectively. It is expected that the in-
fyingthesevarioussourceshasseveralapplicationsinspeech formationcapturedbyjitterandshimmermaybedifferently
coding,speechrecognition,andspeakerrecognition[22]. “enacted”inmachine-synthesizedspeech(ifatall). Wethus
Synthetic utterances generated by deep generative sys- choosealsotoevaluatethesefeaturesinourwork.
tems lack specific aspects of naturalness. One notable ex-
ample is that of prosody. While we do have high quality
3.2. Analyzing robustness of speech-production moti-
and plain prosody TTS datasets, these are far from perfect.
vatedfeatures
Thisislikelytomakeprosodyapromisingcandidateforour
work. Prosody is partially represented through variations in Wearenowinapositiontoanalyzetherobustnessofspeech
thefundamentalfrequency(F0)ofthespeechsignal. Inaddi- production motivated features for detecting fake speech and
tion,featuresthatcaptureprosodyvariationsaretheF0se- improving the robustness of ASV systems against synthetic
quence,spectralenvelope,andspectralaperiodicity. Weeval- speech-basedattacks. InfactanASVcountermeasuremodel
uatealloftheseinourwork. Ourhypothesisisthatfeatures thatevaluatesverificationperformance(basedont-DCF[24]
that capture the fine-level nuances of human speech from a andEERmeasurement)automaticallyconsolidatesandveri-
speech-production perspective are likely to be able to help fies both goals. For reasons explained earlier, we choose to
distinguishbetweenrealandfakespeecheffectively.Besides, use jitter, shimmer, and other features that capture F0 varia-
they are also likely to improve the performance of counter- tions.
measures that are used for thwarting ASV spoofing attacks Forexperimentswithjitterandshimmer,weonlyusethe
carriedoutthroughsyntheticspeech. Forexample,asshown utterance-wise average jitter and shimmer values (extracted
in Fig.3, the spectral envelope information of fake speech usingPraat[25]),whichmaynotbethebestwaytousesuch
lacks natural transition and nuances, consistent with our hy- transient information from speech signals. Nevertheless, we
pothesis that the synthetic utterances may lack some aspects buildathree-layerMLPasacountermeasuremodelthatuses
ofnaturalness. these features. In our implementation, we set the F0 range
In the vocal production process of a human, the funda- to be within 75-500 Hz. The results show a 31% EER on
mentalfrequencywerefertoisthenaturalfrequencyofthevi- thedevelopmentset,showingthatevensimpleaggregatesof
brationofthevocalcords. Aspecificnuancewecanleverage these features (the average across an utterance in this case)
is(knownfrompriorliterature)thatthelarynxcanbeapprox- alreadymakeapositivedifferencetoperformance.
imated a nonlinear dynamic system, and the vocal folds can Forexperimentswithaperiodicandspectralenvelopesig-
be approximated to coupled oscillators that are theoretically nal features, we verify the spoofing countermeasures in per-
capable of an infinite number of different vibration patterns. formanceimprovements. Weusethedetectionmodelthatis
However,thesearepersistentlyinaperturbedstate. Invocal modifiedfromtheresidualnetarchitecturesproposedin[11].
acoustics,perturbationtypicallyreferstoadeviationfroman To evaluate the proposed features, we do not focus on fine-
expectedregularityinvocal-foldvibration.Nobiologicalsys- tuning parameters and use five residual blocks compared to
temcanproducetrulyperiodicoscillations,andsomeinstan- the9-11blocksin[11]forallinputfeatures.Wesetthekernel
taneous fluctuation can always be expected [23]. Features tobeofdifferentsizestoaccommodatethedimensionalityre-
that capture such instant-to-instant perturbations are the quirementsofthespectralenvelopeandaperiodicinformation
Table3. ASVcountermeasure-basedevaluation P(w )
P = i (2)
i (cid:80) P(w )
i i
CountermeasureEER% tDCF
n
Features DEV EVAL DEV EVAL (cid:88)
PSE =− p lnp (3)
i i
Aperiodicparameters(AP) 21.19 20.65 0.4374 0.4445
Spectralenvelope(SP) 10.55 9.31 0.3520 0.2453 The F0 sequence is extracted using WORLD [26], and
MFCC 7.14 11.64 0.1942 0.2663
is trimmed to remove the zero values at the beginning and
CQCC 1.37 10.89 0.0407 0.2746
end of the sequence. We plot the spectral entropy distribu-
Spectrogram 0.48 9.39 0.0132 0.1954
AP+SP 9.41 8.91 0.2872 0.2462 tionsfortheASVspoof2019logicaldata’strain/dev/evalset
AP+SP+MFCC 5.14 8.48 0.1560 0.2169 and find consistent patterns in them. To evaluate the stabil-
AP+SP+CQCC 3.85 6.73 0.1293 0.1777
ity/significance of the patterns, we also compute the distri-
AP+SP+Spectrogram 0.62 6.67 0.0201 0.1604
bution from the FoR dataset, as shown in Fig. 4. Results
show that the spectral entropy of F0 sequence is a surpris-
ASVdataset
ingly good indicator that captures statistical differences be-
tweensyntheticspeechandnaturalspeechacrossdatasets.
4. DISCUSSIONS
To further understand the anti-spoofing properties of the
aperiodic signal and spectral envelope signals, we evalu-
FoRdataset ated their performance with the direct usage in the ASV
model. AsshowninTable1andTable2,AP/SP-basedblack-
boxes and white-boxes show much larger ASV EER% than
STFT/MFCCbasedmannersundermostattacks. Thisiseven
muchmoreobviousinSP-basedboxes. Thepotentialspecu-
lation is that both AP and SP are the features corresponding
to identity-independent attributes like content-dependent at-
tributes. SPisevenmostlydisentangledfromspeakeridenti-
ties. These results are expected since the AP and SPsignals
Fig.4. Spectralentropydistributions. Blueisforfakespeech
are chosen to capture the nuances differences between nat-
andorangeisforrealspeech
ural speech and fake speech, while ASV systems requires
featuresthatdistinguishthespeakers’voicecharacteristicsin
extractedusingWORLD[26]. Inourevaluationofthesefea-
a finer level. Still, one interesting phenomena we noticed is
tures,fromTable3,theEERsaresimilarforthedevandeval
thatAP/SPfeatures,especiallyAP,seemstobegoodsupple-
setusingthesefeaturesalone. Wecanalsoseethatthefusion
mentary information that gives lower EER% for attacks that
ofaperiodicinformationandspectralenvelopewithMFCCor
STFT/MFCCarenotgoodat. Thisisconsistentwithourhy-
spectrogramorConstantQcepstralcoefficients(CQCC)[27]
pothesis that they could capture the signature information to
featurescanimprovethedetectionperformanceasevaluated
distinguish human-generated speech and machine-generated
byEERandthejointperformancewithASVevaluatedbythe
speech.
t-DCF[9,24]anddecreasethegapbetweentheEERsofthe
evaluationsetanddevelopmentset.
In the case of spectral entropy of F0 sequence, our hy- 5. CONCLUSIONS
pothesisisthattheF0sequenceofsyntheticspeechmaylack
Inthisstudy,wehaveestablishedthatspoofingattackscarried
the characteristic shift and variation of natural speech. We
outusingdeep-fakespeecharemorelikelytobeeffectivethan
usetheShannonentropyofthepowerspectraldensityofthe
thoseusingothersyntheticmethodsorhumanimpersonation;
F0sequencetocapturethis. Theequationsforthecomputa-
even the speech is produced by professional impersonators.
tionofthisspectraldensityareasequations(1), (2)and(3),
We have also established that features that capture the fine-
whichfirstcalculatethepowerspectraldensity(PSD)ofyour
level inconsistencies and nuances of the speech production
signal’s spectrum X(w ); then normalize the PSD as proba-
i
process could consistently exhibit differences between syn-
bilitydensityfunction;andfinallycomputethepowerspectral
theticspeechandgenuinespeech. Alloftheseresultinmore
entropy.
robust detection of spoofed speech, and result in rendering
ASVsystemsmorerobusttoattacksgeneratedusingunseen
1
P(w i)= N|X(w i)|2 (1) methods.
6. REFERENCES [10] Tianxiang Chen, Avrosh Kumar, Parav Nagarsheth,
GaneshSivaraman,andElieKhoury,“Generalizationof
[1] Yang Gao, Rita Singh, and Bhiksha Raj, “Voice audio deepfake detection,” in Proc. Odyssey 2020 The
impersonation using generative adversarial networks,” Speaker and Language Recognition Workshop, 2020,
in 2018 IEEE International Conference on Acoustics, pp.132–137.
Speech and Signal Processing (ICASSP). IEEE, 2018,
[11] MoustafaAlzantot, ZiqiWang, andManiBSrivastava,
pp.2506–2510.
“Deep residual neural networks for audio spoofing de-
[2] Aa¨ron van den Oord, Sander Dieleman, Heiga Zen, tection,” Proc.Interspeech2019,pp.1078–1082,2019.
Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
[12] Rohan Kumar Das, Jichen Yang, and Haizhou Li,
Kalchbrenner,AndrewSenior,andKorayKavukcuoglu,
“Long range acoustic and deep features perspective on
“WaveNet: A generative model for raw audio,” in 9th
ASVspoof2019,”inIEEEAutom.SpeechRecognit.Un-
ISCASpeechSynthesisWorkshop,2016,pp.125–125.
derstandingWorkshop,2019.
[3] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike
[13] JichenYang,RohanKumarDas,andHaizhouLi, “Sig-
Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng
nificanceofsubbandfeaturesforsyntheticspeechdetec-
Chen,YuZhang,YuxuanWang,RjSkerrv-Ryan,etal.,
tion,” IEEETransactionsonInformationForensicsand
“Natural TTS synthesis by conditioning WaveNet on
Security,2019.
mel spectrogram predictions,” in 2018 IEEE Interna-
tionalConferenceonAcoustics,SpeechandSignalPro- [14] Hossein Zeinali, Themos Stafylakis, Georgia Athana-
cessing(ICASSP).IEEE,2018,pp.4779–4783. sopoulou,JohanRohdin,IoannisGkinis,Luka´sˇBurget,
and Jan Cˇernocky´, “Detecting spoofing attacks using
[4] BT Balamurali, Kinwah Edward Lin, Simon Lui, Jer-
VGGandSincNet:BUT-Omiliasubmissiontoasvspoof
MingChen,andDorienHerremans, “Towardrobustau-
2019 challenge,” Proc. Interspeech 2019, pp. 1073–
diospoofingdetection: Adetailedcomparisonoftradi-
1077,2019.
tional and learned features,” IEEE Access, vol. 7, pp.
84229–84241,2019. [15] R. Reimao and V. Tzerpos, “FoR: A dataset for syn-
theticspeechdetection,” in2019InternationalConfer-
[5] Madhu R Kamble, Hardik B Sailor, Hemant A Patil,
enceonSpeechTechnologyandHuman-ComputerDia-
and Haizhou Li, “Advances in anti-spoofing: from the
logue(SpeD),Oct2019,pp.1–10.
perspective of ASVspoof challenges,” APSIPA Trans-
actions on Signal and Information Processing, vol. 9, [16] Arsha Nagrani, Joon Son Chung, Weidi Xie, and An-
2020. drewZisserman, “Voxceleb: Large-scalespeakerverifi-
cationinthewild,” ComputerSpeech&Language,vol.
[6] HongYu,Zheng-HuaTan,YimingZhang,ZhanyuMa,
60,pp.101027,2020.
andJunGuo, “DNNfilterbankcepstralcoefficientsfor
spoofingdetection,” IeeeAccess,vol.5,pp.4779–4787, [17] JoonSonChung,JaesungHuh,SeongkyuMun,Minjae
2017. Lee,HeeSooHeo,SoyeonChoe,ChiheonHam,Sungh-
wan Jung, Bong-Jin Lee, and Icksang Han, “In de-
[7] ZhizhengWu,TomiKinnunen,NicholasEvans,Junichi
fenceofmetriclearningforspeakerrecognition,” arXiv
Yamagishi, Cemal Hanilc¸i, Md Sahidullah, and Alek-
preprintarXiv:2003.11982,2020.
sandr Sizov, “ASVspoof 2015: the first automatic
speakerverificationspoofingandcountermeasureschal- [18] Weicheng Cai, Jinkun Chen, and Ming Li, “Explor-
lenge,” inSixteenthAnnualConferenceoftheInterna- ing the encoding layer and loss function in end-to-end
tionalSpeechCommunicationAssociation,2015. speakerandlanguagerecognitionsystem,”2018.
[8] TomiKinnunen,MdSahidullah,He´ctorDelgado,Mas- [19] TomiKinnunen,RosaGonza´lezHautama¨ki,VilleVest-
similiano Todisco, Nicholas Evans, Junichi Yamagishi, man, and Md Sahidullah, “Can we use speaker recog-
and Kong Aik Lee, “The ASVspoof 2017 challenge: nition technology to attack itself? enhancing mimicry
Assessingthelimitsofreplayspoofingattackdetection,” attacks using automatic target speaker selection,” in
Proc.Interspeech2017,pp.2–6,2017. ICASSP 2019-2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
[9] Massimiliano Todisco, Xin Wang, Ville Vestman,
IEEE,2019,pp.6146–6150.
Md Sahidullah, He´ctor Delgado, Andreas Nautsch, Ju-
nichi Yamagishi, Nicholas Evans, Tomi Kinnunen, and [20] Ville Vestman, Tomi Kinnunen, Rosa Gonza´lez Hau-
Kong Aik Lee, “ASVspoof 2019: Future horizons tama¨ki, and Md Sahidullah, “Voice mimicry attacks
in spoofed and fake audio detection,” arXiv preprint assisted by automatic speaker verification,” Computer
arXiv:1904.05441,2019. Speech&Language,vol.59,pp.36–54,2020.
[21] Xin Wang, Junichi Yamagishi, Massimiliano Todisco,
Hector Delgado, Andreas Nautsch, Nicholas Evans,
Md Sahidullah, Ville Vestman, Tomi Kinnunen,
Kong Aik Lee, Lauri Juvela, Paavo Alku, Yu-Huai
Peng, Hsin-Te Hwang, Yu Tsao, Hsin-Min Wang, Se-
bastienLeMaguer,MarkusBecker,FergusHenderson,
RobClark,YuZhang,QuanWang,YeJia,KaiOnuma,
Koji Mushika, Takashi Kaneda, Yuan Jiang, Li-Juan
Liu, Yi-Chiao Wu, Wen-Chin Huang, Tomoki Toda,
KouTanaka,HirokazuKameoka,IngmarSteiner,Driss
Matrouf, Jean-Francois Bonastre, Avashna Govender,
Srikanth Ronanki, Jing-Xuan Zhang, and Zhen-Hua
Ling, “ASVspoof 2019: A large-scale public database
ofsynthesized,convertedandreplayedspeech,”2019.
[22] Om Deshmukh, Carol Y Espy-Wilson, Ariel Salomon,
andJawaharSingh, “Useoftemporalinformation: De-
tectionofperiodicity,aperiodicity,andpitchinspeech,”
IEEE Transactions on Speech and Audio Processing,
vol.13,no.5,pp.776–786,2005.
[23] James A Coan and John JB Allen, Handbook of emo-
tionelicitationandassessment,Oxforduniversitypress,
2007.
[24] Tomi Kinnunen, Kong Aik Lee, Hector Delgado,
NicholasEvans,MassimilianoTodisco,MdSahidullah,
Junichi Yamagishi, and Douglas A Reynolds, “t-DCF:
a detection cost function for the tandem assessment of
spoofingcountermeasuresandautomaticspeakerverifi-
cation,” in Proc. Odyssey 2018 The Speaker and Lan-
guageRecognitionWorkshop,2018,pp.312–319.
[25] DavidFeinberg, “ParselmouthPraatscriptsinpython,”
2019.
[26] MasanoriMorise,FumiyaYokomori,andKenjiOzawa,
“WORLD:avocoder-basedhigh-qualityspeechsynthe-
sis system for real-time applications,” IEICE TRANS-
ACTIONS on Information and Systems, vol. 99, no. 7,
pp.1877–1884,2016.
[27] Massimiliano Todisco, He´ctor Delgado, and Nicholas
Evans, “Constant Q cepstral coefficients: A spoof-
ingcountermeasureforautomaticspeakerverification,”
Computer Speech & Language, vol. 45, pp. 516–535,
2017.

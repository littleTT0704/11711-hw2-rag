CAGAN: Text-To-Image Generation with
Combined Attention Generative Adversarial
Networks
Henning Schulze Dogucan Yaman Alexander Waibel
Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology,
Germany
Abstract. Generating images according to natural language descrip-
tionsisachallengingtask.Priorresearchhasmainlyfocusedtoenhance
the quality of generation by investigating the use of spatial attention
and/or textual attention thereby neglecting the relationship between
channels. In this work, we propose the Combined Attention Generative
Adversarial Network (CAGAN) to generate photo-realistic images ac-
cording to textual descriptions. The proposed CAGAN utilises two at-
tentionmodels:wordattentiontodrawdifferentsub-regionsconditioned
on related words; and squeeze-and-excitation attention to capture non-
linear interaction among channels. With spectral normalisation to sta-
bilise training, our proposed CAGAN improves the state of the art on
theISandFIDontheCUBdatasetandtheFIDonthemorechallenging
COCO dataset. Furthermore, we demonstrate that judging a model by
asingleevaluationmetriccanbemisleadingbydevelopinganadditional
modeladdinglocalself-attentionwhichscoresahigherIS,outperforming
thestateoftheartontheCUBdataset,butgeneratesunrealisticimages
through feature repetition.
Keywords: text-to-imagesynthesis,generativeadversarialnetwork(GAN),at-
tention
1 Introduction
Generatingimagesaccordingtonaturallanguagedescriptionsspansawiderange
ofdifficulty,fromgeneratingsyntheticimagestosimpleandhighlycomplexreal-
world images. It has tremendous applications such as photo-editing, computer-
aided design, and may be used to reduce the complexity of or even replace
renderingengines[27].Furthermore,goodgenerativemodelsinvolvelearningnew
representations.Theseareusefulforavarietyoftasks,forexampleclassification,
clustering, or supporting transfer among tasks.
The final published version of the paper can be found here: https://
link.springer.com/chapter/10.1007/978-3-030-92659-5 25 (DOI: 10.1007/978-
3-030-92659-5 25)
2202
naJ
41
]VC.sc[
4v36621.4012:viXra
2 H. Schulze et al.
Fig.1. Example results of the proposed CAGAN (SE). The generated images are of
64x64, 128x128, and 256x256 resolutions respectively, bilinearly upsampled for visual-
ization.
Although generating images highly related to the meanings embedded in a
natural language description is a challenging task due to the gap between text
and image modalities, there has been exciting recent progress in the field using
numeroustechniquesanddifferentinputs[20][11] [18][46][43][28][3][17][4][19]
[42] [30] [31] yielding impressive results on limited domains. A majority of ap-
proaches are based on Generative Adversarial Networks (GANs) [7]. Zhang et
al. introduced Stacked GANs [44] which consist of two GANs generating images
in a low-to-high resolution fashion. The second generator receives the image en-
coding of the first generator and the text embedding as input to correct defects
and generate higher resolution images. Further research has mainly focused to
enhance the quality of generation by investigating the use of spatial attention
and/or textual attention thereby neglecting the relationship between channels.
In this work, we propose Combined Attention Generative Adversarial Net-
work(CAGAN)thatcombinesmultipleattentionmodels,therebypayingatten-
tion to word, channel, and spatial relationships. First, the network uses a deep
bi-directional LSTM encoder [42] to obtain word and sentence features. Then,
the images are generated in a coarse to fine fashion (see Figure 1) by feeding
the encoded text features into a three stage GAN. Thereby, we utilise local-self
attention [26] mainly during the first stage of generation; word attention at the
beginning of the second and the third generator; and squeeze-and-excitation at-
tention [12] throughout the second and the third generator. We use the publicly
available CUB [38] and COCO [21] datasets to conduct the experimental analy-
sis. Our experiments show that our network generates images of similar quality
as previous work while either advancing or competing with the state of the art
on the Inception Score (IS) [34] and the Fr´echet Inception Distance (FID) [10].
The main contributions of this paper are threefold:
(1)Weincorporatemultipleattentionmodels,therebyreactingtosubtledif-
ferences in the textual input with fine-grained word attention; modelling long-
range dependencies with local self-attention; and capturing non-linear interac-
tion among channels with squeeze-and-excitation (SE) attention. SE attention
can learnto learnto use globalinformation toselectivelyemphasise informative
features and suppress less useful ones.
Title Suppressed Due to Excessive Length 3
(2)Westabilisethetrainingwithspectralnormalisation[23],whichrestricts
the function space from which the discriminators are selected by bounding the
Lipschitz norm and setting the spectral norm to a designated value.
(3) We demonstrate that improvements on single evaluation metrics have to
be viewed carefully by showing that evaluation metrics may react oppositely.
The rest of the paper is organized as follows: In Section 2, we give a brief
overview of the literature. In Section 3, we explain the presented approach in
detail.InSection4,wementiontheemployeddatasetsandexperimentalresults.
Then, we discuss the outcomes and we conclude the paper in Section 5.
2 Related Work
While there has been substantial work for years in the field of image-to-text
translation,suchasimagecaptiongeneration[1][6][41],onlyrecentlytheinverse
problem came into focus: text-to-image generation. Generative image models
require a deep understanding of spatial, visual, and semantic world knowledge.
A majority of recent approaches are based on GANs [7].
Reed et al. [31] use a GAN with a direct text-to-image approach and have
shown to generate images highly related to the text’s meaning. Reed et al. [30]
further developed this approach by conditioning the GAN additionally on ob-
ject locations. Zhang et al. built on Reed et al.’s direct approach developing
StackGAN[44]generating256x256photo-realisticimagesfromdetailedtextde-
scriptions. Although StackGAN yields remarkable results on specific domains,
such as birds or flowers, it struggles when many objects and relationships are
involved.Zhangetal.[45]improvedStackGANbyarrangingmultiplegenerators
and discriminators in a tree-like structure, allowing for more stable training be-
haviourbyjointlyapproximatingmultipledistributions.Xuetal.[42]introduced
a novel loss function and fine-grained word attention into the model.
Recently,anumberofworksbuiltonXuetal.’s[42]approach:Chengetal.[4]
employed spectral normalisation [23] and added global self-attention to the first
generator;Qiaoetal.[29]introducedasemantictextregenerationandalignment
moduletherebylearningtext-to-imagegenerationbyredescription; Lietal.[17]
added channel-wise attention to Xu et al.’s spatial word attention to generate
shape-invariant images when changing text descriptions; Cai et al. [3] enhanced
localdetailsandglobalstructuresbyattendingtorelatedfeaturesfromrelevant
words and different visual regions; Yin et al. [43] focused on disentangling the
semantic-related concepts and introduced a contrasive loss to strengthen the
image-text correlation; and Zhu et al. [46] refined Xu et al.’s fine-grained word
attention by dynamically selecting important words based on the content of an
initial image.
Instead of using multiple stages or multiple GANs, Li et al. [19] used one
generator and three independent discriminators to generate multi-scale images
conditioned on text in an adversarial manner. Johnson et al. [13] introduced a
GAN that receives a scene graph consisting of objects and their relationships as
input and generates complex images with many recognizable objects. However,
4 H. Schulze et al.
the images are not photo-realistic. Qiao et al. [28] introduced LeicaGAN which
adopts text-visual co-embeddings to convey the visual information needed for
image generation.
Otherapproachesarebasedonautoencoders[35][5][39],autoregressivemod-
els [25] [32] [8], or other techniques [37] [16] [15] [40].
We propose to expand the focus of attention to channel, word and spatial
relationships instead of a subset of these thereby enhancing the quality of gen-
eration.
3 The framework of Combined Attention Generative
Adversarial Networks
3.1 Combined Attention Generative Adversarial Networks
The proposed CAGAN utilises three attention models: word attention to draw
different sub-regions conditioned on related words, local self-attention to model
long-range dependencies, and squeeze-and-excitation attention to capture non-
linear interaction among channels.
The attentional generative model consists of three generators, which receive
imagefeaturevectorsasinputandgenerateimagesofsmall-to-largescales.First,
adeepbidirectionalLSTMencoderencodestheinputsentenceintoaglobalsen-
tencevectorsandawordmatrix.ConditioningaugmentationFCA [44]converts
the sentence vector into the conditioning vector. A first network receives the
conditioning vector and noise, sampled from a standard normal distribution, as
input and computes the first image feature vector. Each generator is a simple
3x3 convolutional layer that receives the image feature vector as input to com-
pute an image. The remaining image feature vectors are computed by networks
receiving the previous image feature vector and the result of the ith attentional
model Fattn (see Figure 2), which uses the word matrix computed by the text
i
encoder.
To compute word attention, the word vectors are converted into a common
semantic space. For each subregion of the image a word-context vector is com-
puted, dynamically representing word vectors that are relevant to the subregion
of the image, i.e., indicating the weight the word attention model attends to
the lth word when generating a subregion. The final objective function of the
attentional generative network is defined as:
m−1
(cid:88)
L=L +λL , where L = L . (1)
G DAMSM G Gi
i=0
Here,λisahyperparametertobalancethetwoterms.ThefirsttermistheGAN
loss that jointly approximates conditional and unconditional distributions [45].
At the ith stage, the generator G has a corresponding discriminator D . The
i i
adversarial loss for G is defined as:
i
L =−1 E (cid:2) log(D (yˆ))(cid:3) −1 E (cid:2) log(D (yˆ,s))(cid:3) , (2)
Gi 2 yˆi∼PGi i i 2 yˆi∼PGi i i
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
unconditionalloss conditionalloss
Title Suppressed Due to Excessive Length 5
Fig.2. The architecture of the proposed CAGAN with word, SE, and local attention.
Whenomittinglocalattention,localattentionisremovedfromtheFattn networks.In
n
the upsampling blocks it is replaced by SE attention.
where yˆ are the generated images. The unconditional loss determines whether
i
theimageisrealorfakewhiletheconditionallossdetermineswhethertheimage
andthesentencematchornot.AlternatelytothetrainingofG ,eachdiscrimina-
i
torD istrainedtoclassifytheinputintotheclassofrealorfakebyminimizing
i
the cross-entropy loss.
Thesecond termofEquation1, L ,is afine-grainedword-level image-
DAMSM
text matching loss computed by the DAMSM [42]. The DAMSM learns two
neural networks that map subregions of the image and words of the sentence to
acommonsemanticspace,thusmeasuringtheimage-textsimilarityattheword
level to compute a fine-grained loss for image generation. The image encoder
prior to the DAMSM is built upon a pretrained Inception-v3 model [36] with
added perceptron layers to extract visual feature vectors for each subregion of
the image and a global image vector.
3.2 Attention models
Local self-attention Similartoaconvolution,localself-attention[26]extracts
a localregion of pixels ab∈N (i,j) foreach pixel x and a givenspatial extent
k ij
k. An output pixel y computes as follows:
ij
(cid:88)
y = softmax (qTk )v . (3)
ij ab ij ab ab
a,b∈Nk(i,j)
q = W x denotes the queries, k = W x the keys, and v = W x
ij Q ij ab K ab ab V ab
the values, each obtained via linear transformations W of the pixel ij and their
neighbourhood pixels. The advantage over a simple convolution is that each
pixelvalueisaggregatedwithaconvexconvolutionofvaluevectorswithmixing
weights (softmax ) parametrised by content interactions.
ab
6 H. Schulze et al.
Squeeze-and-excitation (SE) attention Instead of focusing on the spatial
component of CNNs, SE attention aims to improve the channel component by
explicitly modelling interdependencies among channels via channel-wise weight-
ing. Thus, they can be interpreted as a light-weight self-attention function on
channels.
First, a transformation, which is typically a convolution, outputs the feature
mapU.Becauseconvolutionsuselocalreceptivefields,eachentryofU isunaware
ofcontextualinformationoutsideitsregion.AcorrespondingSE-blockaddresses
this issue by performing a feature recalibration.
A squeeze operation aggregates the feature maps of U across the spatial
dimension (H ×W) yielding a channel descriptor. The proposed squeeze oper-
ation is mean-pooling across the entire spatial dimension of each channel. The
resultingchanneldescriptorservesasanembeddingoftheglobaldistributionof
channel-wise features.
AfollowingexcitationoperationF aimstocapturechannel-wisedependen-
ex
cies, specifically non-linear interaction among channels and non-mutually exclu-
sive relationships. The latter allows multiple channels to be emphasized. The
excitation operation is a simple self-gating operation with a sigmoid activation
function:
F (z,W)=σ(g(z,W))=σ(W δ(W z)) , (4)
ex 2 1
where δ refers to the ReLU activation function, W
1
∈RC r×C, and W
2
∈RC×C r.
To limit model complexity and increase generalisation, a bottleneck is formed
aroundthegatingmechanism:aFullyConnected(FC)layerreducesthedimen-
sionalitybyafactorofr.AsecondFClayerrestoresthedimensionalityafterthe
gatingoperation.Theauthorsrecommendanr of16foragoodbalancebetween
accuracy and complexity (∼10% parameter increase on ResNet-50 [9]). Ideally,
r should be tuned for the intended architecture.
TheexcitationoperationF computesper-channelmodulationweights.These
ex
are applied to the feature maps U performing an adaptive recalibration.
4 Experiments
Dataset We employed CUB [38] and COCO [21] datasets for the experiments.
The CUB dataset [38] consists of 8855 train and 2933 test images. To perform
evaluation, one image per caption in the test set is computed since each image
has ten captions. The COCO dataset [21] with the 2014 split consists of 82783
train and 40504 test images. We randomly sample 30000 captions from the test
set for the evaluation.
Evaluation metric In this work, we utilized the Inception Score (IS) [34]
and The Fr´echet Inception Distance (FID) [10] to evaluate the performance of
proposed method. The IS [34] is a quantitative metric to evaluate generated
images. It measures two properties: highly classifiable and diverse with respect
Title Suppressed Due to Excessive Length 7
to class labels. Although the IS is the most widely used metric in text-to-image
generation, it has several issues [33] [2] [24] regarding the computation of the
score itself and the usage of the score. According to the authors of [2] it: ”fails
to provide useful guidance when comparing models”.
The FID [10] views features as a continuous multivariate Gaussian and com-
putes a distance in the feature space between the real data and the generated
data. A lower FID implies a closer distance between the generated image dis-
tribution and the real image distribution. The FID is consistent with human
judgment and more consistent to noise than the IS [10] although it has a slight
bias [22]. Please note that there is some inconsistency in how the FID is cal-
culated in prior work, originating from different pre-processing techniques that
significantly impact the score. We use the official implementation1 of the FID.
To ensure a consistent calculation of all of our evaluation metrics, we replace
the generic Inception v3 network with the pre-trained Inception v3 network we
usedforcomputingtheISofthecorrespondingdataset.Were-calculatetheFID
scores of papers with an official model to provide a fair comparison.
Implementation detail We employ spectral normalisation [23], a weight nor-
malisation technique to stabilise the training of the discriminator, during train-
ing. To compute the semantic embedding for text descriptions, we employ a
pre-trainedbi-directionLSTMencoderbyXuetal.[42]withadimensionof256
for the word embedding. The sentence length was 18 for the CUB dataset and
12 for the COCO dataset.
All networks are trained using the Adam optimiser [14] with a batch size
of 20, a learning rate of 0.0002, and β = 0.5 and β = 0.999. We train for
1 2
600 epochs on the CUB and for 200 epochs on the COCO dataset. For the
model utilising squeeze-and-excitation attention we use r = 1, and λ = 0.1
and λ = 50.0, respectively for the CUB and the COCO dataset. For the model
utilising local self-attention as well we use r =4, and λ=5.0 and λ=50.0.
4.1 Results
Quantitative Results As Table 1 and Figure 3 show, our model utilising
squeeze-and-excitationattentionoutperformsthebaselineAttnGAN[42]inboth
metrics on both datasets. The IS is improved by 9.6% ± 2.4% and 25.9% ±
5.3% and the FID by 10.0% and 36.0% on the CUB and the COCO dataset,
respectively.OurapproachalsoscoresthehighestISandthehighestFIDonthe
CUB dataset and scores the best FID on the COCO dataset next to the third
best IS.
Our second model, utilising squeeze-and-excitation attention and local self-
attention, shows better IS scores than our other model. With 4.96 ± 0.05 it
outperforms all other models on the CUB dataset, improving the state of the
artby4.4%±2.6%.However,itgeneratescompletelyunrealisticimagesthrough
1 https://github.com/bioinf-jku/TTUR
8 H. Schulze et al.
Table1.Fr´echetInceptionDistance(FID)andInceptionScore(IS)ofstate-of-the-art
models and our two CAGAN models on the CUB and COCO dataset with a 256x256
imageresolution.Theunmarkedscoresarethosereportedintheoriginalpapers.Scores
marked with (cid:61) were calculated with a pre-trained model provided by the respective
authors. ↑ (↓) means the higher (lower), the better.
CUB dataset COCO dataset
Model
IS↑ FID↓ IS↑ FID↓
Real Data 25.52±.09 0.00 37.97±.88 0.00
AttnGAN [42] 4.36±.04 47.76(cid:61) 25.89±.47 31.05(cid:61)
PPAN [19] 4.38±.05 - - -
HAGAN [4] 4.43±.03 - - -
MirrorGAN [29] 4.56±.05 - 26.47±.41 -
ControlGAN [17] 4.58±.09 49.18(cid:61) 24.06±.60 -
DualAttn-GAN [3] 4.59±.07 - - -
LeicaGAN [28] 4.62±.06 - - -
SD-GAN [43] 4.67±.09 - 35.69±.50 -
DM-GAN [46] 4.75±.07 43.20(cid:61) 30.49±.57 22.84(cid:61)
Obj-GAN [18] - - 30.29±.33 -
OP-GAN [11] - - 27.88±.12 23.29(cid:61)
CPGAN [20] - - 52.73±.6149.92(cid:61)
CAGAN SE (ours) 4.78±.06 42.98 32.60±.75 19.88
CAGAN L+SE (ours) 4.96±.05 61.06 33.89±.69 27.40
feature repetitions (see Figure 4) and has a major negative impact on the FID
throughout training (see Figure 3). This behaviour is similar to [20] on the
COCOdatasetanddemonstratesthatasinglescorecanbemisleadingandthus
the importance of reporting both scores.
In summary, according to the experimental results, our proposed CAGAN
achieved state-of-the-art results on both the CUB dataset and COCO dataset
basedontheFIDmetric.Moreover,weobtainedstate-of-the-artISontheCUB
dataset and quite a comparative IS on the COCO dataset. All these results
indicate how our CAGAN model is effective for the text-to-image generation
task.
Qualitative Results : Figure 4 shows images generated by our models and by
several other models [11] [46] [42] on the CUB dataset and on the more chal-
lenging COCO dataset. On the CUB dataset, our model utilising SE attention
generates images of vivid details (see 1st, 4th, 5th, and 6th row), demonstrating
a strong text-image correlation (see 3th, 4th, and 5th row), avoiding feature rep-
etitions (see double beak, DM-GAN 6th row), and managing the difficult scene
(see 7th row) best. Cut-off artefacts occur in all presented models.
Ourmodelincorporatinglocalself-attentionfailstoproducerealisticlooking
image,despitescoringhigherISsthantheAttnGANandourmodelutilisingSE
attention.Instead,itdrawsrepetitivefeaturesmanifestingintheformofmultiple
Title Suppressed Due to Excessive Length 9
Fig.3. IS and FID of the AttnGAN [42], our model utilising squeeze-and-excitation
attention, and our model utilising squeeze-and-excitation attention and local self-
attentionontheCUBandtheCOCOdataset.TheISoftheAttnGANisthereported
scoreandtheFIDwasre-evaluatedusingtheofficialmodel.TheISoftheAttnGANon
theCOCOdatasetiswith25.89±.47significantlylowerthanourmodels.Weomitted
the score to highlight the distinctions between our two models.
birds, drawn out birds, multiple heads, or strange patterns. The drawn features
mostlymatchthetextualdescriptions.Thisprovidesapossibleexplanationwhy
the model has a high IS despite scoring poorly on the FID: the IS cares mainly
about the images being highly classifiable and diverse. Thereby, it presumes
that highly classifiable images are of high quality. Our network demonstrates
that high classify-ability and diversity and therefore a high IS can be achieved
throughcompletelyunrealistic,repetitivefeaturesofthecorrectbirdclass.This
is further evidence that improvements solely based on the IS have to be viewed
sceptically.
On the more challenging COCO dataset, our model utilising SE attention
demonstrates semantic understanding by drawing features that resemble the
object, for example, the brown-white pattern of a giraffe (1st row), umbrellas
(4th row), and traffic lights (5th row). Furthermore, our model draws distinct
shapes for the bathroom (2nd row), broccoli (3rd row), and is the only one
that properly approximates a tower building with a clock (7th row). Generally
speaking, the results on the COCO dataset are not as realistic and robust as
on the CUB dataset. We attribute this to the more complex scenes coupled
with more abstract descriptions that focus rather on the category of objects
than detailed descriptions. In addition, although there are a large number of
categories, each category only has comparatively few examples thereby further
increasing the difficulty for text-to-image-generation.
ForourSEattentionmodelwefurthertestitsgeneralisationabilitybytesting
how sensitive the outputs are to changes in the most attended, in the sense of
word attention, words in the text descriptions (see Figure 5). The test is similar
totheoneperformedontheAttnGAN[42].TheresultsillustratethataddingSE
attentionandspectralnormalisationdonotharmthegeneralisationabilityofthe
network:theimagesarealteredaccordingtothechangesintheinputsentences,
10 H. Schulze et al.
Fig.4. Comparison of images generated by our models (CAGAN SE and CA-
GAN SE L) with images generated by other current models [11] [46] [42] on the CUB
dataset (left) and on the more challenging COCO dataset (right).
Fig.5. Example results of our SE attention model with r =1,λ=0.1 trained on the
CUBdatasetwhilechangingsomemostattended,inthesenseofwordattention,words
in the text descriptions.
Title Suppressed Due to Excessive Length 11
showingthatthenetworkretainsitsabilitytoreacttosubtlesemanticdifferences
in the text descriptions.
5 Conclusion
In this paper, we propose the Combined Attention Generative Adversarial Net-
work (CAGAN) to generate photo-realistic images according to textual descrip-
tions. We utilise attention models such as, word attention to draw different
sub-regions conditioned on related words; squeeze-and-excitation attention to
capturenon-linearinteractionamongchannels;andlocalself-attentiontomodel
long-range dependencies. With spectral normalisation to stabilise training, our
proposed CAGAN improves the state of the art on the IS and FID on the CUB
dataset and the FID on the more challenging COCO dataset. Furthermore, we
demonstrate that judging a model by a single evaluation metric can be mislead-
ing by developing an additional model which scores a higher IS, outperforming
thestateoftheartontheCUBdataset,butgeneratesunrealisticimagesthrough
feature repetition.
References
1. Bai,S.,An,S.:Asurveyonautomaticimagecaptiongeneration.Neurocomputing
311, 291–304 (2018)
2. Barratt,S.T.,Sharma,R.:Anoteontheinceptionscore.CoRRabs/1801.01973
(2018)
3. Cai, Y., Wang, X., Yu, Z., Li, F., Xu, P., Li, Y., Li, L.: Dualattn-gan: Text to
imagesynthesiswithdualattentionalgenerativeadversarialnetwork.IEEEAccess
7, 183706–183716 (2019)
4. Cheng,Q.,Gu,X.:Hybridattentiondriventext-to-imagesynthesisviagenerative
adversarial networks. In: ICANN, Proceedings - Workshop and Special Sessions.
Lecture Notes in Computer Science, vol. 11731, pp. 483–495. Springer (2019)
5. Dorta, G., Vicente, S., Agapito, L., Campbell, N.D.F., Prince, S., Simpson, I.:
Laplacianpyramidofconditionalvariationalautoencoders.In:CVMP.pp.7:1–7:9
(2017)
6. Fang,H.,Gupta,S.,Iandola,F.N.,Srivastava,R.K.,Deng,L.,Dolla´r,P.,Gao,J.,
He,X.,Mitchell,M.,Platt,J.C.,Zitnick,C.L.,Zweig,G.:Fromcaptionstovisual
concepts and back. In: CVPR. pp. 1473–1482 (2015)
7. Goodfellow,I.J.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville, A.C., Bengio, Y.: Generative adversarial nets. In: NIPS. pp. 2672–2680
(2014)
8. Gupta, T., Schwenk, D., Farhadi, A., Hoiem, D., Kembhavi, A.: Imagine this!
scriptstocompositionstovideos.In:ECCV,Proceedings,PartVIII.LectureNotes
in Computer Science, vol. 11212, pp. 610–626 (2018)
9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR. pp. 770–778 (2016)
10. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
by a two time-scale update rule converge to a local nash equilibrium. In: NIPS.
pp. 6626–6637 (2017)
12 H. Schulze et al.
11. Hinz, T., Heinrich, S., Wermter, S.: Semantic object accuracy for generative text-
to-image synthesis. CoRR abs/1910.13321 (2019)
12. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: CVPR. pp. 7132–
7141 (2018)
13. Johnson,J.,Gupta,A.,Fei-Fei,L.:Imagegenerationfromscenegraphs.In:CVPR.
pp. 1219–1228 (2018)
14. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR
(Poster) (2015)
15. Kulkarni, T.D., Kohli, P., Tenenbaum, J.B., Mansinghka, V.K.: Picture: A prob-
abilistic programming language for scene perception. In: CVPR. pp. 4390–4399
(2015)
16. Kulkarni, T.D., Whitney, W.F., Kohli, P., Tenenbaum, J.B.: Deep convolutional
inverse graphics network. In: NIPS. pp. 2539–2547 (2015)
17. Li,B.,Qi,X.,Lukasiewicz,T.,Torr,P.H.S.:Controllabletext-to-imagegeneration.
In: NIPS. pp. 2063–2073 (2019)
18. Li, W., Zhang, P., Zhang, L., Huang, Q., He, X., Lyu, S., Gao, J.: Object-driven
text-to-imagesynthesisviaadversarialtraining.In:CVPR.pp.12174–12182(2019)
19. Li, Z., Wu, M., Zheng, J., Yu, H.: Perceptual adversarial networks with a feature
pyramid for image translation. IEEE CG&A 39(4), 68–77 (2019)
20. Liang, J., Pei, W., Lu, F.: CPGAN: full-spectrum content-parsing generative ad-
versarial networks for text-to-image synthesis. CoRR abs/1912.08562 (2019)
21. Lin, T., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan, D., Dolla´r, P.,
Zitnick, C.L.: Microsoft COCO: common objects in context. In: ECCV, Proceed-
ings,PartV.LectureNotesinComputerScience,vol.8693,pp.740–755.Springer
(2014)
22. Lucic, M., Kurach, K., Michalski, M., Gelly, S., Bousquet, O.: Are gans created
equal? A large-scale study. In: NIPS. pp. 698–707 (2018)
23. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for
generative adversarial networks. In: ICLR, Conference Track Proceedings. Open-
Review.net (2018)
24. Odena,A.,Olah,C.,Shlens,J.:Conditionalimagesynthesiswithauxiliaryclassifier
gans.In:ICML.ProceedingsofMachineLearningResearch,vol.70,pp.2642–2651.
PMLR (2017)
25. van den Oord, A., Kalchbrenner, N., Espeholt, L., Kavukcuoglu, K., Vinyals, O.,
Graves, A.: Conditional image generation with pixelcnn decoders. In: NIPS. pp.
4790–4798 (2016)
26. Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A., Shlens, J.:
Stand-alone self-attention in vision models. In: NIPS. pp. 68–80 (2019)
27. Pharr, M., Jakob, W., Humphreys, G.: Physically based rendering: From theory
to implementation. Morgan Kaufmann (2016)
28. Qiao, T., Zhang, J., Xu, D., Tao, D.: Learn, imagine and create: Text-to-image
generation from prior knowledge. In: NIPS. pp. 885–895 (2019)
29. Qiao,T.,Zhang,J.,Xu,D.,Tao,D.:Mirrorgan:Learningtext-to-imagegeneration
by redescription. In: CVPR. pp. 1505–1514 (2019)
30. Reed, S.E., Akata, Z., Mohan, S., Tenka, S., Schiele, B., Lee, H.: Learning what
and where to draw. In: NIPS. pp. 217–225 (2016)
31. Reed, S.E., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative
adversarial text to image synthesis. In: ICML. JMLR Workshop and Conference
Proceedings, vol. 48, pp. 1060–1069 (2016)
Title Suppressed Due to Excessive Length 13
32. Reed, S.E., van den Oord, A., Kalchbrenner, N., Colmenarejo, S.G., Wang, Z.,
Chen,Y.,Belov,D.,deFreitas,N.:Parallelmultiscaleautoregressivedensityesti-
mation. In: ICML. Proceedings of Machine Learning Research, vol. 70, pp. 2912–
2921 (2017)
33. Rosca, M., Lakshminarayanan, B., Warde-Farley, D., Mohamed, S.: Varia-
tional approaches for auto-encoding generative adversarial networks. CoRR
abs/1706.04987 (2017)
34. Salimans, T., Goodfellow, I.J., Zaremba, W., Cheung, V., Radford, A., Chen, X.:
Improved techniques for training gans. In: NIPS. pp. 2226–2234 (2016)
35. Snell,J.,Ridgeway,K.,Liao,R.,Roads,B.D.,Mozer,M.C.,Zemel,R.S.:Learning
to generate images with perceptual similarity metrics. In: ICIP. pp. 4277–4281
(2017)
36. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the incep-
tion architecture for computer vision. In: CVPR. pp. 2818–2826 (2016)
37. Theis, L., Bethge, M.: Generative image modeling using spatial lstms. In: NIPS.
pp. 1927–1935 (2015)
38. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The caltech-ucsd
birds-200-2011 dataset. technical report cns-tr-2011-001 (2011)
39. Wu, J., Tenenbaum, J.B., Kohli, P.: Neural scene de-rendering. In: CVPR (2017)
40. Xie, J., Lu, Y., Zhu, S., Wu, Y.N.: A theory of generative convnet. In: ICML.
JMLR Workshop and Conference Proceedings, vol. 48, pp. 2635–2644 (2016)
41. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel,
R.S., Bengio, Y.: Show, attend and tell: Neural image caption generation with
visualattention.In:ICML.JMLRWorkshopandConferenceProceedings,vol.37,
pp. 2048–2057 (2015)
42. Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: Attngan:
Fine-grainedtexttoimagegenerationwithattentionalgenerativeadversarialnet-
works. In: CVPR. pp. 1316–1324 (2018)
43. Yin, G., Liu, B., Sheng, L., Yu, N., Wang, X., Shao, J.: Semantics disentangling
for text-to-image generation. In: CVPR. pp. 2327–2336 (2019)
44. Zhang, H., Xu, T., Li, H.: Stackgan: Text to photo-realistic image synthesis with
stacked generative adversarial networks. In: ICCV. pp. 5908–5916 (2017)
45. Zhang,H.,Xu,T.,Li,H.,Zhang,S.,Wang,X.,Huang,X.,Metaxas,D.N.:Stack-
gan++: Realistic image synthesis with stacked generative adversarial networks.
CoRR abs/1710.10916 (2017)
46. Zhu, M., Pan, P., Chen, W., Yang, Y.: DM-GAN: dynamic memory generative
adversarialnetworksfortext-to-imagesynthesis.In:CVPR.pp.5802–5810(2019)

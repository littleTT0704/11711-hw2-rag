AAAIFallSymposiumSeries(FSS-23)
HomeRobot: An Open Source Software Stack for Mobile Manipulation Research
ChrisPaxton1,AustinWang1,BinitShah2,BlaineMatulevich2,DhruvShah1,KarmeshYadav1,3,
SanthoshRamakrishnan5,SriramYenamandra3,YonatanBisk1,4
1FAIR,MetaAI
2HelloRobot
3GeorgiaTech
4CarnegieMellon
5UTAustin
cpaxton@fb.com
Abstract
Reproducibilityinroboticsresearchrequirescapable,shared
hardwareplatformswhichcanbeusedforawidevarietyof
research. We have seen the power of these sorts of shared
platforms in more general machine learning research (e.g.,
PyTorch),wherethereisaconstantandopen-sourceddevel-
opmentovertimetomeettheneedsofthecommunityTobe
abletomakerapidprogressinroboticsinthesameway,we
proposethatweneed:(1)sharedreal-worldplatformswhich
allow different teams to test and compare methods at low
cost; (2) challenging simulations that reflect real-world en-
vironmentsandespeciallycandriveperceptionandplanning
research;and(3)low-costplatformswithenoughsoftwareto
getstartedaddressingalloftheseproblems.Tothisend,we
propose HomeRobot, a mobile manipulator software stack
with associated benchmark in simulation, which is initially
basedonthelow-cost,human-safeHelloRobotStretch.1
Figure 1: HomeRobot is a simple, easy-to-set-up library
Intro
whichworksinmultipleenvironmentsandrequiresonlyrel-
Home assistants have long been a motivating example in atively affordable hardware. Computationally intensive op-
robotics, although relatively few teams have actually been erationsareperformedonadesktopPCwithaGPU,anda
able to deploy robots, especially mobile manipulators, in a dedicated consumer-grade router provides a network inter-
wide range of homes. There are good reasons for this: de- facetoarobotrunninglow-levelcontrolandSLAM.
ploying a useful mobile manipulator requires integrating a
wide range of components, from grasping to object detec-
tion, task planning, and more. We propose HomeRobot, a providingastrongsimulationcomponentandimplementing
software stack with both simulation and real-world hard- avarietyofbaselines.
ware components, as a common, open-source platform for We identify three goals for a platform for reproducible
mobilemanipulationresearchanddevelopment,inorderto roboticsresearch:
allow more users to perform high-impact research in home
• Amotivatingnorthstar:itmustprovidesomeguiding
environments.
north-star tasks which can help shape and motivate re-
Common, shared platforms and software have spurred
searchersandallowforcomparisonsofavarietyofmeth-
rapid progress in fields like language understanding (Wolf
odsoninteresting,real-worldproblems;
etal.2019),imagegeneration(vonPlatenetal.2022),and
arecreditedwiththesuccessofdeeplearning(Neubigetal. • Software Capability: it should provide abstract inter-
2017; Al-Rfou et al. 2016; Abadi et al. 2015; Paszke et al. faces that make a robot easier to use for a wide variety
2017).Robotics,unfortunately,lackssuchsharedplatforms oftasks,includingnavigationandmanipulation;and
–theclosestbeingtheprevalentROS(Quigleyetal.2009) • Community:weshouldincentivepeopletogetinvolved,
softwarestack,oftencriticizedforbeingcomplexandhard usethecodebase,andbuildupacommunityaroundit.
to use (Murali et al. 2019). Unlike PyRobot, we focus on
We specifically propose a task called open-vocabulary
Copyright©2023,AssociationfortheAdvancementofArtificial mobile manipulation (OVMM) as our north star for this
Intelligence(www.aaai.org).Allrightsreserved. project. In OVMM, a robot must, without any prior maps,
1https://github.com/facebookresearch/home-robot move around a new environment and find objects specified
518
Figure2:Alow-costhomerobotperformingtheHomeRobotOVMMtaskinbothasimulatedandareal-worldenvironment.
InHomeRobot,weprovidebothachallengingsimulated“northstar”task,whereinamobilemanipulatorrobotmustfindand
graspmultipleseenandunseenobjects,andacorrespondingreal-worldroboticsstacktoallowotherstoreproducethisresearch
andevaluationtoproduceusefulhomerobotassistants.
bylanguage.Webelievethisisastrong“buildingblock”for basis for shared, mobile manipulation research in human
futurecapabilities,andcanbeimplementedinmanydiffer- homeenvironments.
entways:usingmotionplanningandsimpleheuristicsoran
LLM-basedplannertodeterminegoals,asperSayCan/Say- Open-VocabularyMobileManipulation
Plan (Ahn et al. 2022; Rana et al. 2023), built on a task-
OurHomeRobotcodeisreleasedwithmodulesforOpenVo-
and-motion-planningstack(Garrettetal.2020;Curtisetal.
cabularyMobileManipulation(OVMM)(Yenamandraetal.
2022),reinforcementlearning(Yenamandraetal.2023b),or
2023b), where a robot must find any object and place it in
usingimplicitrepresentationsandpretrainedmodels(Bolte
any location in an ordinary home. We chose this task be-
etal.2023).Havingacommontaskwhichdoesnotpreclude
cause it represents a foundational capability for robots to
any of these options allows us to share useful components
be useful assistants: they must perceive a wide variety of
acrossprojects(e.g.detection,mapping,andgrasping).
objects, grasp and manipulate them, and understand large,
Our proposal and software stack are based around the
complexscenesthatmaynotbewellmappedtobeginwith.
Hello Robot Stretch, a low-cost mobile manipulation plat-
Formally, the HomeRobot OVMM task is set up as
form already in use at over 40 universities. By leveraging
instructions of the form: “Move (object) from the
existing and accessible infrastructure increases the odds of
(start receptacle) to the (goal receptacle).” The
adoption,code-sharing,andexpeditingresearchprogress.
objectisasmallandmanipulablehouseholdobject(e.g.,a
Projects using HomeRobot. Many research projects cup, stuffed toy, or box). By contrast, start receptacle
have already started using HomeRobot for object naviga- andgoal receptaclearelargepiecesoffurniture,which
tion (Ramakrishnan et al. 2022), exploration (Ramakrish- have surfaces upon which objects can be placed. The
nan,Al-Halah,andGrauman2020),image-instancenaviga- robot is placed in an unknown single-floor home environ-
tion(Krantzetal.2023),continuous/lifelonglearning(Pow- ment - such as an apartment - and must, given the natu-
ers,Gupta,andPaxton2023),language-conditionednaviga- ral language names of start receptacle, object, and
tion(Bolteetal.2023),andlanguage-conditionedmulti-task goal receptacle, pick up an object that is known to
learning(Parasharetal.2023). be on a start receptacle and move it to any valid
Contributions. We describe the HomeRobot software goal receptacle. start receptacle is always known
stack:acodebasewhichallowsforbothsimulatedandreal- to the robot, to help agents know where to look for the
world control of a Stretch robot from Hello Robots, a low- object.
cost mobile manipulator with good manipulation and navi- The agent is successful if the specified object is in-
gationcapabilities(Kempetal.2022)thatprovidesastrong deed moved from a start receptacle on which it be-
519
Find Toy Animal Pick Up Toy Animal Find Dining Table Place On Dining Table
Find Stuffed Animal Pick Up Stuffed Animal Find Sofa Place On Sofa
Figure3:TheHomeRobotstackbeingusedtoperformtheOpenVocabularyMobileManipulationtaskinaheld-out,real-world
testenvironment.Thetestenvironmentisamockapartmentwhichwillbeusedformultipleversionsofthebenchmark,includ-
ing for the HomeRobot Neurips 2023 competition (Yenamandra et al. 2023a). This allows us to perform both simulated and
real-worldbenchmarkingofnewmethods,andwillhelpcenterthecommunityaroundabroadly-available,low-costplatform
withgrowingcapabilities.
gan the episode, to any valid goal receptacle. We give bededicatedforyourrobotanddesktopsetuptoensure
partial credit for each step the robot accomplishes: finding goodperformance.
the start receptacle with the object, picking up the 3. The mobile robot itself: a Hello Robot Stretch with
object, finding the goal receptacle, and placing the DexWrist,asdescribedabove.
object on the goal receptacle. There can be multiple
Aftertherobotisconfigured,oneonlyneedrunonescript,
validobjectsthatsatisfyeachquery.
a ROS launch file, as described in the HomeRobot startup
Weimplementedbothasimulationandareal-worldver-
instructions,whichcanbedoneoverSSH.Withaproperly
sionofthisHomeRobotOVMMtask.Intherealworld,we
configured robot and router, you can visualize information
use a controlled apartment environment with fixed lighting
onthedesktopside,showingtherobot’sposition,mapfrom
and furniture, within which we can reset a number of dif-
SLAM,andcameras.Ontherobotside,theonlynecessary
ferentobjectsdrawnfrombothaseen(intrainingdata)and
commandisstartingthecorrectROSlaunchfile:
unseenobjectset.Unseenobjectsusedfortestingarewith-
startup stretch hector slam.launch
heldandnotreleasedtothepublic.
whichbringsuptherobotcontrollersandstartsrunningHec-
ConfigurationandSetup torSLAMtoprovidelocalization.
One challenge with low-cost mobile robots is how we can Checkingnetworkperformance.Wedescribethevisu-
runGPU-andcompute-intensivemodelstoevaluatemodern alization tools available briefly in the next section, but to
AImethodsonthem.TheStretch,likemanysimilarrobots,
checkthatthesetupisworkingproperly,youcanstartrviz
doesnothaveonboardGPU,andwillalwayshavemorelim- and wave your hand in front of the robot – you should see
itedcomputethanisavailableonasimilarworkstation.We minimallatencywhenwavingahandinfrontofthecamera.
this with a simple network configuration shown in Fig. 4. Timingbetweentherobotandtheremoteworkstation.
Therearethreecomponents: We use ROS (Quigley et al. 2009) as our communications
layer,andtoimplementlow-levelcontrolontherobot.This
1. The desktop running code – in our case, the
alsoprovidesnetworkcommunication.However,duetopo-
eval episode.py script from HomeRobot – which
tentiallatencybetweentherobotanddesktop,wealsoneed
connectstoaremotemobilemanipulator.
tomakesurethatobservationsareuptodate.
2. Thededicatedrouter–anoff-the-shelfconsumerrouter, We set up the robot to block after executing most navi-
suchasaNetgearNighthawkrouter.Thisshouldideally gationmotions,inordertomakethisprocesssimpler,until
520
AgentsandEnvironments,similartohowmanyreinforce-
mentlearningbenchmarksaresetup(Savvaetal.2019).
• Agentscontainallofthenecessarycodetoexecutepoli-
cies.Weimplementagentswhichuseamixtureofheuris-
ticpoliciesandpolicieslearnedonourscenedatasetvia
reinforcementlearning.
• Environments provide Observations to the Agent, and
a function which allows them to execute actions in the
(realorsimulated)environment.
RobotControl
We expand on the basic Stretch low-level control in order
to make it easier to control the robot. In particular, we im-
plemented a high-level motion planner, integrated different
graspingandplacementstrategies,andalsoimplementedre-
activelow-levelcontroltomakeiteasiertomovetherobot
Figure4:SystemoverviewforHomeRobot.Werunvisual- aroundintherealworldfordifferentmethods.
izationsanddeepneuralnetworksonaGPU-enabledwork-
station; the Hello Robots Stretch runs low-level controllers State estimation In the real world, we use Hector
andmappingcode.Thisallowsustousemorecomputethan SLAM(Kohlbrecheretal.2014)toprovideourrobot’sbase
isstrictlypossibleonthemobilerobotfordecisionmaking pose as it moves around in a novel environment. This is
andsceneunderstanding. asrecommendedintheHelloRobotdocumentation(Kemp
etal.2022).Thisisareliable,LIDAR-basedSLAMmethod,
which in our experience works well in home-like environ-
thereisanuptodateimageobservationfromtherobotside. ments such as that used in the HomeRobot OVMM real-
Thismeansthattimingbetweentherobotandtheworksta- worldchallenge.
tionisextremelyimportant:ifwedonothaveuptodatetim- However,HectorSLAMhasalotofhigh-frequencynoise
ing, we might have SLAM poses and depth measurements particularly when the robot is rotating. At the same time,
thatdonotmatch,whichwillleadtoworseperformance. we have access to wheel odometry signals, which provides
Wesolvedthisbyhavingaclockontherobotsidepublish accurateandlowlatencyreadingsofdisplacementbetween
itstimeoverROS,andconfigureallsystemstousethisROS time steps, but is subject to drift when integrated through
masterclockinsteadofsystemtime.Thispreventstheuser time to estimate absolute position. We combine the signal
fromhavingtoworryaboutLinuxtimesynchronizationpro- comingfromHectorSLAMwiththeposesignalfromwheel
tocolslikeNTPwhensettinguptherobotforthefirsttime. odometry by effectively applying a discrete high-pass filter
on the odometry signal and a discrete low-pass filter with
TheHomeRobotStack thesamecutofffrequencyontheHectorSLAMsignal,then
blendingthetwosignalstogetheratafixedrate:
HomeRobot is based on ROS (Quigley et al. 2009), with
the goal being to provide abstractions which make it easy x =x +(1−λ)·(xs−x )+λ·(xo−xo )
t t−1 t t−1 t t−1
tolearninsimulation,performmobilemanipulationexperi-
withxastheoutputpose,xsandxoareposeestimatesfrom
ments,andsharecodeacrosslabsandprojects.Weinitially
Hector SLAM and wheel odometry respectively, and λ as
focusontheHomeRobotOVMMtask,butalsosupportmul-
a tuned coefficient. In our implementation, λ is computed
tipleothertypesofresearch.
fromthecutofffrequencyof0.2Hz,belowwhichthe pose
To support a wide range of machine learning reasearch
estimate basically trusts SLAM for its unbiased absolute
projects, there are three different modules within the open-
pose estimates, and relies on odometry signals for higher
sourceHomeRobotlibrary:
frequency,transientstateestimations.
• home robot:SharedcomponentssuchasEnvironment
interfaces,controllers,detectionandsegmentation. BaseVelocityController TheHelloStretchsoftwarepro-
videsanativeinterfaceforcontrollingthelinearandangular
• home robot sim: Simulation stack with Environ-
velocities of the differential-drive robot base. While we do
ments based on Habitat. We specifically provide envi-
expose an interface for users to control these velocities di-
ronments modified from the Habitat Synthetic Scenes
rectly, it is desireable to have desired short-term goals as a
dataset (Yenamandra et al. 2023b; Khanna et al. 2023)
more intuitive action space for policies, and to make them
asastartingpointfordevelopmentandtesting.
update-ableatanyinstanttoallowforreplanning.
• home robot hw:Hardwarestackwithserverprocesses
Thus,weimplementedavelocitycontrollerthatproduces
that runs on the robot, client API that runs on the GPU
continuous velocity commands that moves the robot to an
workstation,andEnvironmentsbuiltusingtheclientAPI.
input goal pose, show in Alg. 1. The controller operates in
WithinHomeRobot,wealsodividefunctionalitybetween aheuristicmanner:byrotatingtherobotsothatitfacesthe
521
Algorithm1:BaseVelocityControllerPseudocode
Require: Goalposep ∈SE(2),Currentposep∈SE(2)
g
1: Computeposeerrore =p −p∈SE(2)
p g
2: Computeheadingerrorθ =errorbetweenrobotheadingand
e
directionofthegoallocation(directionoftranslationalportion
ofe )
p
3: iftranslationalpartofe <d then
p threshold
4: v =0
l
5: v :computedfromtherotationalpartofe usingatrape-
r p Figure 5: Exploring a real-world apartment during testing.
zoidalvelocityprofile
TherobotusesDetic(Zhouetal.2022)toperceivetheworld
6: else
andupdatea2Dmap(center)whichcaptureswhereit’sseen
7: v :computedfromθ usingatrapezoidalvelocityprofile
r e
8: v :computedfromthetranslationalpartofe usingatrape- relevantclasses,andwhichobstaclesexist;detectionsaren’t
l p
zoidalvelocityprofile always reliable, especially given a large and changing vo-
9: Reduce linear velocity based on heading error v = cabulary of objects that we care about. In the HomeRobot
l
v l0sin(2θ e) stack, we provide a variety of tools for visualizing and im-
10: Compute e˙ l = f l(v l), the rate at which heading error in- plementingpolicies,includingintegrationofRVIZ(right).
creasesduetolinearvelocity
11: Computee˙ = f (v ),therateatwhichheadingerrorde-
r r r
creasesduetoangularvelocity
Frontier Exploration Policy. We explore the environ-
12: ife˙ < 1e˙ then
l 2 r mentwithaheuristicfrontier-basedexplorationpolicy(Ya-
13: Limitlinearvelocityv sothate˙ = 1e˙
l l 2 r mauchi 1997). This heuristic selects as the goal the point
14: endif
closest to the robot in geodesic distance within the bound-
15: endif
arybetweentheexploredandunexploredregionofthemap.
16: Applyv,v
l r
We further implemented and tested more advanced explo-
ration policies in HomeRobot, and verified that they work
on the real-world robot in a variety of environments (Ra-
goalpositionatalltimeswhilemovingtowardsthegoalpo-
makrishnan, Al-Halah, and Grauman 2020; Ramakrishnan
sition, and then rotating to reach the goal orientation once
etal.2022).
goalpositionisreached.
Navigation Planner. Given a long-term goal output by
the frontier exploration policy, we use the Fast Marching
PlanningandMapping
Method(Sethian1999)asin(Chaplotetal.2020a)toplana
Ourmotionplannerexpandsuponpriorwork(Gervetetal. pathandthefirstlow-levelactionalongthispathdetermin-
2022),whichwasshowntoworkinawiderangeofhuman istically.Althoughthesemanticexplorationpolicyactsata
environments. We provide default values which tune it for coarsetimescale,theplanneractsatafinetimescale:every
navigating close to other objects and extended it to work stepweupdatethemapandreplanthepathtothelong-term
inourcontinuousactionspace–challengingnavigationas- goal. The robot attempts to plan to goals if they have been
pects not present in the original paper. We implemented seen; if it cannot get within a certain distance of the goal
threecomponentsforthebaselineverisonofoursystem: objects,thenitwillinsteadplantoapointonthefrontier.
Semantic Mapping Module. The semantic map stores Navigating to objects on start receptacle. Since
relevant objects, explored regions, and obstacles. To con- smallobjects(e.g.action figure,apple)canbehard
structthemap,wepredictsemanticcategoriesandsegmen- to locate from a distance, we leverage the typically larger
tation masks of objects from first-person observations. We start receptaclegoalsforfindingobjects.Wemakethe
useDetic(Zhouetal.2022)forobjectdetectionandinstance following changes to the original planning policy (Chaplot
segmentationandbackprojectfirst-personsemanticsegmen- etal.2020b):
tationintoapointcloudusingtheperceiveddepth,binitinto 1. If object and start receptacle co-occur in at least
a3Dsemanticvoxelmap,andfinallysumovertheheightto onecellofthesemanticmap,plantoreachtheobject
computea2Dsemanticmap.
2. If the object is not found but start receptacle ap-
Wekeeptrackofobjectsdetected,obstacles,andexplored
pears in the semantic map after excluding the regions
areas in an explicit metric map of the environment from
within1moftheagent’spastlocations,plantoreachthe
(Chaplotetal.2020a).Concretely,itisabinaryKxM xM
start receptacle
matrixwhereM xM isthemapsizeandKisthenumberof
3. Otherwise,plantoreachtheclosestfrontier
map channels. Each cell of this spatial map corresponds to
25cm2 (5cmx5cm)inthephysicalworld.Mapchannels In step 2, we exclude the regions that the agent has been
K =C+4whereC isthenumberofsemanticobjectcate- close to, to prevent it from re-visiting already visited in-
gories,andtheremaining4channelsrepresenttheobstacles, stancesofstart receptacle.
theexploredarea,andtheagent’scurrentandpastlocations.
VisualizationTools
Anentryinthemapisoneifthecellcontainsanobjectofa
particularsemanticcategory,anobstacle,orisexplored,and We use RVIZ, a part of ROS, to visualize results and
zerootherwise. progress.Fig.5showsthreedifferentoutputsfromoursys-
522
Human Commercially Manipulation Approximate
Name Mobile Sized Safe Available DOF Cost
BostonDynamicsSpot ✓ ✓ 7 $200,000
FrankaEmikaPanda ✓ ✓ 7 $30,000
Locobot ✓ ✓ 5 $5,000
Fetch ✓ ✓ ✓ 7 $100,000
HelloRobotStretch ✓ ✓ ✓ ✓ 4 $19,000
StretchwithDexWrist ✓ ✓ ✓ ✓ 6 $25,000
Table 1: Notes on platform selection. We chose the Stretch with DexWrist as a good compromise between manipulation,
navigation,andcost,whilebeinghuman-safeandapproximatelyhuman-sized.
tem:onthefarleft,animagefromthetestenvironmentbe- TheCompetition
ingprocessedbyDetic;inthecenter,atop-downmapgen-
We also proposed a Neurips 2023 competition in order to
erated by the navigation planner described in Sec. ; and on
incentive people to begin using our software stack (Yena-
the right, an image from RVIZ with the point cloud from
mandra et al. 2023a). This competition is centered around
therobot’sheadcameraregisteredagainstthe2Dlidarmap
the HomeRobot OVMM task: picking up an object from a
createdbyHectorSLAM.
start receptacleandmovingittoagoal receptacle
One advantage of the HomeRobot stack is that it is de-
somewhereelseintheworld,allinitiallywithoutanymaps.
signed to work with existing debugging tools - especially
There are two phases in this competition: a simulation
ROS (Quigley et al. 2009). ROS is a widely-used frame-
phaseandareal-worldphase.Ourgoalwiththistwo-phase
work for robotics software development which comes with
structure is that it allows us to run a set of experiments on
alotofonlineresources,officialsupportfromHelloRobot,
held-outtestscenes,andthenidentifyonlythebestfewvari-
and a rich and thriving open-source community with wide
ants of our method to re-run in the real world. This allows
industrybacking.
us to run more controlled experiments for methods imple-
mentedbydifferentteams.
TheHomeRobotCommunity
At present 20 universities have begun participating both
Our goal with HomeRobot is to build a community which providingatestamenttothecommunityinterestandprovid-
allowsresearcherstoattemptdifferentresearchprojects,and ing us valuable feedback on further refining the codebase
comparethemacrossavarietyofdifferentenvironmentson andensuringgenerality.Movingforward,wewillbeadvo-
thesamereal-worldrobotplatform.Tothisend,we’vetaken catingforpullrequestsfromthecommunitytohelpconsol-
athree-prongedapproach: idateresearcheffortsandinsightsforalltobenefit.
• Releasinganopen-sourcesoftwarestackwithbothasim-
ulationandareal-worldcomponent,containingpowerful DiscussionandConclusions
baselinesforavarietyofrobottasks;
As noted at the start, within robotics we have watched the
• Implementing a number of different novel research runawaysuccessofsharedframeworkswithtransferableand
projectsusingHomeRobot;and reproduciblecodeacrossmachinelearning.Whileastrength
• Proposing a competition (Yenamandra et al. 2023a) ofroboticsiscreatingnewhardware,customendeffectors,
which will reward the best solutions, which can then and beyond, there is now a critical mass of interested re-
be integrated into our open-source stack and compared searchers focusing on indoor mobile manipulation and this
againstotherteamsinacontrolledenvironment. is coupled with a low-cost commercial platform. This puts
usinauniquepositiontobeginsharingcodeandmodelsto
ChoiceofHardware dramaticallyspeedupthepaceofresearch.
Whendecidingonacommonplatformforresearch,it’sim- OuraimwithHomeRobotistoprovidethecommunitya
portant that we choose a robot which has a wide range of commonplatformfromwhichtobuildandshareknowledge.
capabilities,someofwhichallowforresearchprojectsthat Wefocusedonawidely-available,low-costplatformwhich
havenotyetbeenattempted.Wedecidedtofocusonaspe- can perform 6dof pick-and-place object manipulation, and
cific model of the Hello Robot Stretch: it is relatively low implementedamodularsoftwarearchitectureallowingboth
cost,humansafe,lightweight,andacapable6dofmanipula- simulation and real-world experimentation. In addition, we
torthathasbeenusedinnumerousresearchprojects (Gervet describethe“northstar”goalofsharingsolutionstoOpen-
et al. 2022; Yenamandra et al. 2023b; Parashar et al. 2023; Vocabulary Mobile Manipulation, which we consider to be
Haldaretal.2023;Bahl,Gupta,andPathak2022). a cornerstone of robotics manipulation in homes due to its
We describe some options for commercially-available ubiquity,andtothefactthatexistingmethodsoftenperform
roboticshardwareinTab.1.WhiletheFrankaEmikaPanda quitepoorlyintherealworldorevenonrealisticsimulation
isnotamobilerobot,weincludeitherebecauseit’savery settings(Yenamandraetal.2023b).
commmonly used platform in both industrial research labs TheHomeRobotcodeandexamplevideosofthesimula-
andatuniversities,makingitspriceafaircomparisonpoint tionandreal-worldbenchmarksareonlineatovmm.github.
forwhatisreasonable. ioandhttps://github.com/facebookresearch/home-robot.
523
References objects via task and motion planning with estimated affor-
Abadi,M.;Agarwal,A.;Barham,P.;Brevdo,E.;Chen,Z.; dances. In2022InternationalConferenceonRoboticsand
Citro, C.; Corrado, G. S.; Davis, A.; Dean, J.; Devin, M.; Automation(ICRA),1940–1946.IEEE.
Ghemawat, S.; Goodfellow, I.; Harp, A.; Irving, G.; Isard, Garrett, C. R.; Paxton, C.; Lozano-Pe´rez, T.; Kaelbling,
M.; Jia, Y.; Jozefowicz, R.; Kaiser, L.; Kudlur, M.; Lev- L. P.; and Fox, D. 2020. Online replanning in belief space
enberg, J.; Mane´, D.; Monga, R.; Moore, S.; Murray, D.; forpartiallyobservabletaskandmotionproblems. In2020
Olah, C.; Schuster, M.; Shlens, J.; Steiner, B.; Sutskever, IEEEInternationalConferenceonRoboticsandAutomation
I.; Talwar, K.; Tucker, P.; Vanhoucke, V.; Vasudevan, V.; (ICRA),5678–5684.IEEE.
Vie´gas,F.;Vinyals,O.;Warden,P.;Wattenberg,M.;Wicke,
Gervet, T.; Chintala, S.; Batra, D.; Malik, J.; and Chaplot,
M.; Yu, Y.; and Zheng, X. 2015. TensorFlow: Large-Scale
D.S.2022. NavigatingtoObjectsintheRealWorld. arXiv.
MachineLearningonHeterogeneousSystems.https://www.
tensorflow.org/. Softwareavailablefromtensorflow.org. Haldar, S.; Pari, J.; Rai, A.; and Pinto, L. 2023. Teach
Ahn, M.; Brohan, A.; Brown, N.; Chebotar, Y.; Cortes, O.; a Robot to FISH: Versatile Imitation from One Minute of
David, B.; Finn, C.; Fu, C.; Gopalakrishnan, K.; Hausman, Demonstrations. arXivpreprintarXiv:2303.01497.
K.;etal.2022.Doasican,notasisay:Groundinglanguage
Kemp,C.C.;Edsinger,A.;Clever,H.M.;andMatulevich,
inroboticaffordances. arXivpreprintarXiv:2204.01691.
B. 2022. The design of stretch: A compact, lightweight
Al-Rfou, R.; Alain, G.; Almahairi, A.; Angermueller, mobile manipulator for indoor human environments. In
C.; Bahdanau, D.; Ballas, N.; Bastien, F.; Bayer, J.; 2022InternationalConferenceonRoboticsandAutomation
Belikov, A.; Belopolsky, A.; Bengio, Y.; Bergeron, A.; (ICRA),3150–3157.IEEE.
Bergstra, J.; Bisson, V.; Snyder, J. B.; Bouchard, N.;
Khanna, M.; Mao, Y.; Jiang, H.; Haresh, S.; Schacklett,
Boulanger-Lewandowski,N.;Bouthillier,X.;deBre´bisson,
B.; Batra, D.; Clegg, A.; Undersander, E.; Chang, A. X.;
A.; Breuleux, O.; Carrier, P.-L.; Cho, K.; Chorowski, J.;
and Savva, M. 2023. Habitat Synthetic Scenes Dataset
Christiano, P.; Cooijmans, T.; Coˆte´, M.-A.; Coˆte´, M.;
(HSSD-200): An Analysis of 3D Scene Scale and Real-
Courville, A.; Dauphin, Y. N.; Delalleau, O.; Demouth, J.;
ism Tradeoffs for ObjectGoal Navigation. arXiv preprint
Desjardins, G.; Dieleman, S.; Dinh, L.; Ducoffe, M.; Du-
arXiv:2306.11290.
moulin,V.;Kahou,S.E.;Erhan,D.;Fan,Z.;Firat,O.;Ger-
main, M.; Glorot, X.; Goodfellow, I.; Graham, M.; Gul- Kohlbrecher, S.; Meyer, J.; Graber, T.; Petersen, K.; Klin-
cehre,C.;Hamel,P.;Harlouchet,I.;Heng,J.-P.;Hidasi,B.; gauf,U.;andVonStryk,O.2014. Hectoropensourcemod-
Honari, S.; Jain, A.; Jean, S.; Jia, K.; Korobov, M.; Kulka- ules for autonomous mapping and navigation with rescue
rni, V.; Lamb, A.; Lamblin, P.; Larsen, E.; Laurent, C.; robots. InRoboCup2013:RobotWorldCupXVII17,624–
Lee, S.; Lefrancois, S.; Lemieux, S.; Le´onard, N.; Lin, Z.; 631.Springer.
Livezey, J. A.; Lorenz, C.; Lowin, J.; Ma, Q.; Manzagol,
Krantz,J.;Gervet,T.;Yadav,K.;Wang,A.;Paxton,C.;Mot-
P.-A.; Mastropietro, O.; McGibbon, R. T.; Memisevic, R.;
taghi, R.; Batra, D.; Malik, J.; Lee, S.; and Chaplot, D. S.
vanMerrie¨nboer,B.;Michalski,V.;Mirza,M.;Orlandi,A.;
2023. Navigating to Objects Specified by Images. arXiv
Pal, C.; Pascanu, R.; Pezeshki, M.; Raffel, C.; Renshaw,
preprintarXiv:2304.01192.
D.; Rocklin, M.; Romero, A.; Roth, M.; Sadowski, P.; Sal-
vatier,J.;Savard,F.;Schlu¨ter,J.;Schulman,J.;Schwartz,G.; Murali, A.; Chen, T.; Alwala, K. V.; Gandhi, D.; Pinto, L.;
Serban, I. V.; Serdyuk, D.; Shabanian, S.; E´tienne Simon; Gupta, S.; and Gupta, A. 2019. Pyrobot: An open-source
Spieckermann,S.;Subramanyam,S.R.;Sygnowski,J.;Tan- robotics framework for research and benchmarking. arXiv
guay, J.; van Tulder, G.; Turian, J.; Urban, S.; Vincent, P.; preprintarXiv:1906.08236.
Visin,F.;deVries,H.;Warde-Farley,D.;Webb,D.J.;Will-
Neubig, G.; Dyer, C.; Goldberg, Y.; Matthews, A.; Am-
son,M.;Xu,K.;Xue,L.;Yao,L.;Zhang,S.;andZhang,Y.
mar, W.; Anastasopoulos, A.; Ballesteros, M.; Chiang, D.;
2016. Theano:APythonframeworkforfastcomputationof
Clothiaux, D.; Cohn, T.; Duh, K.; Faruqui, M.; Gan, C.;
mathematicalexpressions. ArXiv1605.02688.
Garrette, D.; Ji, Y.; Kong, L.; Kuncoro, A.; Kumar, G.;
Bahl, S.; Gupta, A.; and Pathak, D. 2022. Human-to-robot
Malaviya,C.;Michel,P.;Oda,Y.;Richardson,M.;Saphra,
imitationinthewild. arXivpreprintarXiv:2207.09450.
N.; Swayamdipta, S.; and Yin, P. 2017. DyNet: The Dy-
Bolte, B.; Wang, A.; Yang, J.; Mukadam, M.; Kalakrish- namicNeuralNetworkToolkit. ArXiv.
nan,M.;andPaxton,C.2023. USA-Net:UnifiedSemantic
Parashar, P.; Vakil, J.; Powers, S.; and Paxton, C. 2023.
and Affordance Representations for Robot Memory. arXiv
Spatial-Language Attention Policies for Efficient Robot
preprintarXiv:2304.12164.
Learning. arXivpreprintarXiv:2304.11235.
Chaplot,D.S.;Gandhi,D.P.;Gupta,A.;andSalakhutdinov,
R.R.2020a. Objectgoalnavigationusinggoal-orientedse- Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;
manticexploration. InNeurIPS. DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer,
A. 2017. Automatic differentiation in PyTorch. In NIPS
Chaplot,D.S.;Gupta,S.;Gupta,A.;andSalakhutdinov,R.
2017WorkshopAutodiff.
2020b. LearningToExploreUsingActiveNeuralMapping.
ICLR. Powers, S.; Gupta, A.; and Paxton, C. 2023. Evaluat-
Curtis,A.;Fang,X.;Kaelbling,L.P.;Lozano-Pe´rez,T.;and ing Continual Learning on a Home Robot. arXiv preprint
Garrett,C.R.2022.Long-horizonmanipulationofunknown arXiv:2306.02413.
524
Quigley, M.; Conley, K.; Gerkey, B.; Faust, J.; Foote, T.;
Leibs,J.;Wheeler,R.;Ng,A.Y.;etal.2009. ROS:anopen-
sourceRobotOperatingSystem.InICRAworkshoponopen
sourcesoftware,volume3,5.Kobe,Japan.
Ramakrishnan,S.K.;Al-Halah,Z.;andGrauman,K.2020.
OccupancyAnticipationforEfficientExplorationandNavi-
gation.InProceedingsoftheEuropeanConferenceonCom-
puterVision(ECCV).
Ramakrishnan,S.K.;Chaplot,D.S.;Al-Halah,Z.;Malik,J.;
andGrauman,K.2022. PONI:PotentialFunctionsforOb-
jectGoalNavigationwithInteraction-freeLearning.InCom-
puter Vision and Pattern Recognition (CVPR), 2022 IEEE
Conferenceon.IEEE.
Rana, K.; Haviland, J.; Garg, S.; Abou-Chakra, J.; Reid,
I.; and Suenderhauf, N. 2023. SayPlan: Grounding Large
LanguageModelsusing3DSceneGraphsforScalableTask
Planning. arXivpreprintarXiv:2307.06135.
Savva,M.;Kadian,A.;Maksymets,O.;Zhao,Y.;Wijmans,
E.;Jain,B.;Straub,J.;Liu,J.;Koltun,V.;Malik,J.;Parikh,
D.;andBatra,D.2019. Habitat:APlatformforEmbodied
AIResearch. ICCV.
Sethian,J.A.1999. Fastmarchingmethods. SIAMreview.
von Platen, P.; Patil, S.; Lozhkov, A.; Cuenca, P.; Lambert,
N.; Rasul, K.; Davaadorj, M.; and Wolf, T. 2022. Dif-
fusers:State-of-the-artdiffusionmodels.https://github.com/
huggingface/diffusers. Accessed:2024-01-01.
Wolf,T.;Debut,L.;Sanh,V.;Chaumond,J.;Delangue,C.;
Moi,A.;Cistac,P.;Rault,T.;Louf,R.;Funtowicz,M.;Davi-
son, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu,
J.; Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest, Q.;
andRush,A.M.2019. HuggingFace’sTransformers:State-
of-the-artNaturalLanguageProcessing. ArXiv.
Yamauchi, B. 1997. A frontier-based approach for au-
tonomous exploration. In IEEE International Symposium
onComputationalIntelligenceinRoboticsandAutomation.
Yenamandra,S.;Ramachandran,A.;Khanna,M.;Yadav,K.;
Chaplot,D.S.;Chhablani,G.;Clegg,A.;Gervet,T.;Jain,V.;
Partsey,R.;Ramrakhya,R.;Szot,A.;Yang,T.-Y.;Edsinger,
A.; Kemp, C.; Shah, B.; Kira, Z.; Batra, D.; Mottaghi, R.;
Bisk, Y.; and Paxton, C. 2023a. HomeRobot Open Vocab
MobileManipulationChallenge. InThirty-seventhConfer-
ence on Neural Information Processing Systems: Competi-
tionTrack.
Yenamandra, S.; Ramachandran, A.; Yadav, K.; Wang, A.;
Khanna,M.;Gervet,T.;Yang,T.-Y.;Jain,V.;Clegg,A.W.;
Turner, J.; Kira, Z.; Savva, M.; Chang, A.; Chaplot, D. S.;
Batra, D.; Mottaghi, R.; Bisk, Y.; and Paxton, C. 2023b.
HomeRobot: Open-Vocabulary Mobile Manipulation. In
ConferenceonRobotLearning.
Zhou,X.;Girdhar,R.;Joulin,A.;Kra¨henbu¨hl,P.;andMisra,
I. 2022. Detecting Twenty-thousand Classes using Image-
levelSupervision. InECCV.
525

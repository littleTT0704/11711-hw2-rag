Plan, Eliminate, and Track —
Language Models are Good Teachers for Embodied Agents.
Yue Wu1 So Yeon Min1 Yonatan Bisk1 Ruslan Salakhutdinov1 Amos Azaria2 Yuanzhi Li13
Tom M. Mitchell1 Shrimai Prabhumoye4
Abstract
Pre-trained large language models (LLMs)
capture procedural knowledge about the
world. Recent work has leveraged LLM’s abil-
itytogenerateabstractplanstosimplifychal-
lengingcontroltasks, eitherbyactionscoring,
or action modeling (fine-tuning). However,
the transformer architecture inherits several
constraints that make it difficult for the LLM
to directly serve as the agent: e.g. limited in-
putlengths,fine-tuninginefficiency, biasfrom
pre-training, and incompatibility with non-
textenvironments. Tomaintaincompatibility
Figure 1. PET framework. Plan module uses LLM to gen-
withalow-leveltrainableactor,weproposeto erateahigh-levelplan. EliminateModuleusesaQAmodel
insteadusetheknowledge inLLMstosimplify to mask irrelevant objects in observation. Track module
the control problem, rather than solving it. uses a QA model to track the completion of sub-tasks.
We propose the Plan, Eliminate, and Track
(PET) framework. The Plan module trans- Recent work (Huang et al., 2022a;b; Ahn et al., 2022;
latesataskdescriptionintoalistofhigh-level Yao et al., 2020) has used LLMs (Bommasani et al.,
sub-tasks. The Eliminate module masks out 2021) for abstract planning for embodied or gaming
irrelevantobjectsandreceptaclesfromtheob- agents. These have shown incipient success in extract-
servationforthecurrentsub-task. Finally,the ingproceduralworldknowledgefromLLMsinlinguistic
Track module determines whether the agent form with posthoc alignment to executable actions in
has accomplished each sub-task. On the Alf- the environment. However, they treat LLMs as the ac-
World instruction following benchmark, the tor, and focus on adapting LLM outputs to executable
PET framework leads to a significant 15% actions either through fine-tuning (Micheli & Fleuret,
improvement over SOTA for generalization to 2021) or constraints (Ahn et al., 2022). Using LLM
human goal specifications. as the actor works for pure-text environments with
limited interactions (Huang et al., 2022b; Ahn et al.,
2022)(justconsistingof“picking/placing”objects),but
1. Introduction limits generalization to other modalities. In addition,
the scenarios considered have been largely simplified
Humans can abstractly plan their everyday tasks with-
fromtherealworld. Ahnetal.(2022)providesallavail-
outexecution;forexample,giventhetask“Makebreak-
able objects and possible interactions at the start and
fast”, we can roughly plan to first pick up a mug and
limits tasks to the set of provided objects/interactions.
make coffee, before grabbing eggs to scramble. Embod-
Huang et al. (2022b) limits the environment to objects
ied agents, endowed with this capability will generalize
on a single table.
more effectively by leveraging common-sense reasoning.
On the other hand, to successfully “cut some lettuce”
1CarnegieMellonUniversity2ArielUniversity3Microsoft
in a real-world room, one has to “find a knife”, which
Research 4Nvidia Research. Correspondence to: Yue Wu
can be non-trivial since there can be multiple drawers
<ywu5@andrew.cmu.edu>.
or cabinets (Chaplot et al., 2020; Min et al., 2021;
Blukisetal.,2021). Amorerealisticscenarioleadstoa
3202
yaM
7
]LC.sc[
2v21420.5032:viXra
Plan, Eliminate, and Track
diverse, complicated set of tasks or large and changing Sharma et al., 2021) or reinforcement learning (Misra
action space. Furthermore, the text description of the et al., 2017; Jiang et al., 2019; Cideron et al., 2020;
observation increases as a function of the number of Goyal et al., 2021; Nair et al., 2022; Akakzia et al.,
receptacles and objects the agent sees. Combined with 2020) policies conditioned on natural language instruc-
growing roll-outs, the state becomes too verbose to fit tion or goal (MacMahon et al., 2006; Kollar et al.,
into any LLM. 2010). While some prior research has used pre-trained
language embeddings to improve generalization to new
In this work, we explore alternative mechanisms to
instructions(Nairetal.,2022),theylackdomainknowl-
leverage the prior knowledge encoded in LLMs without
edge that is captured in LLMs. Our PET framework
impactingthetrainablenatureoftheactor. Wepropose
enables planning, progress tracking, and observation
a 3-step framework (Figure 1): Plan, Eliminate, and
filtering through the use of LLMs, and is designed to
Track (PET). Plan module simplifies complex tasks
be compatible with any language conditional policies
by breaking them down into sub-tasks. It uses a pre-
above.
trainedLLMtogeneratealistofsub-tasksforaninput
task description employing example prompts from the
LLMs for Control LLMs have recently achieved
training set similar to Huang et al. (2022a); Ahn et al.
success in high-level planning. Huang et al. (2022a)
(2022). TheEliminatemoduleaddressesthechallenge
shows that pre-trained LLMs can generate plausible
of long observations. It uses a zero-shot QA language
plans for day-to-day tasks, but the generated sub-tasks
model to score and mask objects and receptacles that
cannot be directly executed in an end-to-end control
areirrelevanttothecurrentsub-task. TheTrackmod-
environment. Ahn et al. (2022) solves the executability
ule uses a zero-shot QA language model to determine
issue by training an action scoring model to re-weigh
if the current sub-task is complete and moves to the
LLM action choices and demonstrates success on a
next sub-task. Finally, the Action Attention agent
robot. However, LLM scores work for simple environ-
uses a transformer-based architecture to accommodate
ments with actions limited to pick/place (Ahn et al.,
for long roll-out and variable length action space. The
2022), but fails with environments with more objects
agent observes the masked observation and takes an
and diverse actions (Shridhar et al., 2020b). Song
action conditioned on the current sub-task.
et al. (2022) uses GPT3 to generate step-by-step low-
We focus on instruction following in indoor households levelcommands, whicharethenexecutedbyrespective
ontheAlfWorld(Shridharetal.,2020b)interactivetext control policies. the work improves Ahn et al. (2022)
environmentbenchmark. Ourexperimentsandanalysis with more action diversity and on-the-fly re-plan. In
demonstrate that LLMs not only remove 40% of task- addition, all the above LLMs require few-shot demon-
irrelevantobjectsinobservationthroughcommon-sense strations of up to 17 examples, making the length of
QA, but also generate high-level sub-tasks with 99% the prompt infeasible for AlfWorld. Micheli & Fleuret
accuracy. In addition, multiple LLMs may be used in (2021) fine-tuned a GPT2-medium model on expert
coordination with each other to assist the agent from trajectories in AlfWorld and demonstrated impressive
different aspects. evaluation results. However, LM fine-tuning requires
a fully text-based environment, consistent expert tra-
Our contributions are as follows:
jectories, and a fully text-based action space. Such
1. PET: A novel framework for leveraging pre- requirements greatly limit the generalization to other
trained LLMs with embodied agents; our work domains, and even to other forms of task specification.
showsthateachofP,E,Tservesacomplementary WeshowthatourPETframeworkachievesbettergener-
role and should be simultaneously addressed to alization to human goal specifications which the agents
tackle control tasks. were not trained on.
2. AnActionAttentionagentthathandlesthechang-
ing action space for text environments.
Hierarchical Planning with Natural Language
3. A 15% improvement over SOTA for generalization
Due to the structured nature of natural language, An-
tohumangoalsviasub-taskplanningandtracking.
dreas et al. (2017) explored associating each task de-
scription to a modular sub-policy. Later works extend
2. Related Work theaboveapproachbyusingasingleconditionalpolicy
(Mei et al., 2016), or by matching sub-tasks to tem-
Language Conditioned Policies A considerable
plates(Ohetal.,2017). Recentworkshaveshownthat
portion of prior work studies imitation learning (Tellex
LLMs are proficient high-level planners (Huang et al.,
et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepput-
2022a; Ahn et al., 2022; Lin et al., 2022), and therefore
tis et al., 2020; Jang et al., 2022; Shridhar et al., 2022;
motivatesustorevisittheideaofhierarchicaltaskplan-
Plan, Eliminate, and Track
ning with progress tracking. To our knowledge, PET
is the first work combining a zero-shot subtask-level
LLM planner and zero-shot LLM progress tracker with
a low-level conditional sub-task policy.
TextGames Text-basedgamesarecomplex,interac-
tive simulations where the game state and action space
are in natural lanugage. They are fertile ground for
language-focused machine learning research. In addi-
tiontolanguageunderstanding,successfulplayrequires
skills like memory and planning, exploration (trial and
error), and common sense. The AlfWorld (Shridhar
et al., 2020b) simulator extends a common text-based
game simulator, TextWorld Cˆot´e et al. (2018a), to
create text-based analogs of each ALFRED scene.
Figure 2. Plan Module (Sub-task Generation). 5 full exam-
ples are chosen from the training set based on RoBERTa
Agents for Large Action Space He et al. (2015)
embeddingsimilaritywiththetaskquerydescription. Then
learns representation for state and actions with two
the examples are concatenated with the task query to get
different models and computes the Q function as the the prompt. Finally, we prompt the LLM to generate the
inner product of the representations. While this could desired sub-tasks.
generalize to large action space, they only considered
receptaclesandobjectswithintheobservationasrtand
a small number of actions. i
ot respectively. The classification between receptacles
i
Fulda et al. (2017); Ahn et al. (2022) explore action and objects is defined by the environment (Shridhar
elimination in the setting of affordances. Zahavy et al. et al., 2020b). For a task T, we assume there exists a
(2018) trains a model to eliminate invalid actions on list of sub-tasks S ={s ,...s } that solves T.
T 1 k
Zork from external environment signals. However, the
functionality depends on the existence of external elim-
3.1. Plan
ination signal.
Tasks in the real world are often complex and need
more than one step to be completed. Motivated by the
3. Plan, Eliminate, and Track
ability of humans to plan high-level sub-tasks given
In this section, we explain our 3-step framework: Plan, a complex task, we design the Plan module (M P)
Eliminate, and Track (PET). In Plan module (M ), to generate a list of high-level sub-tasks for a task
P
a pre-trained LLM generates a list of sub-tasks for an description T.
input task description using samples from the training
Inspired by the contextual prompting techniques for
set as in-context examples. The Eliminate module
planning with LLMs (Huang et al., 2022a), we use
(M )usesazero-shotQAlanguagemodeltoscoreand
E an LLM as our plan module M . For a given task
P
mask objects and receptacles that are irrelevant to the
description T, we compose the query question Q as
T
currentsub-task. TheTrackmodule(M )usesazero-
T “WhatarethemiddlestepsrequiredtoT?”,andrequire
shot QA language model to determine if the current
M to generate a list sub-tasks S ={s ,...s }.
P T 1 k
sub-task is complete and moves to the next sub-task.
Note that Plan is a generative task and Eliminate and Specifically,weselectthetop5exampletasksTE from
Track are classification tasks. the training set based on RoBERTa (Liu et al., 2019)
embedding similarity with the query task T. We then
We also implement an attention-based agent (Action
concatenate the example tasks with example sub-tasks
Attention), which scores each permissible action and is
in a query-answer format to build the prompt P for
T
trained on imitation learning on the expert. The agent
M (Fig. 2):
P
observes the masked observation and takes an action
conditioned on the current sub-task. P =concat(Q ,S ,...,Q ,S ,Q )
T TE TE TE TE T
1 1 5 5
Problem Setting We define the task description as An illustration of our prompt format is shown in Fig-
T, the observation string at time step t as Ot, and ure2,whereT =“heatsomeappleandputitinfridge”,
the list of permissible actions {at|at can be executed} andQ =“Whatarethemiddlestepsrequiredtoput
i i TE
as At. For each observation string Ot, we define the two spr1 aybottles on toilet”, S =“take a spraybottle,
TE
1
Plan, Eliminate, and Track
place the spraybottle in/on toilet, take a spraybottle, T. We then remove o from observation if µ < τ ,
i oi o
placethespraybottlein/ontoilet”. Theexpectedlistof and remove r if µ <τ . Threshold τ ,τ are hyper-
i ri r o r
sub-tasks to achieve this task T is s =‘take an apple’, parameters.
1
s =‘heat the apple’, and s =‘place the apple in/on
2 3
fridge’
Figure 3. Eliminate Module (Receptacle Masking). We
use a pre-trained QA model to filter irrelevant recepta-
cles/objects in the observation of each scene. As we can
see, the original observation is too long and the receptacles
shown in red are not relevant for task completion. These
receptacles are filtered by the QA model making the obser-
vation shorter.
3.2. Eliminate
Typical Alfworld scenes can start with around 15 re-
ceptacles, each containing up to 15 objects. In some
close-to-worst cases, there can be around 30 open-able
receptacles (e.g. a kitchen with many cabinets and
drawers), and it easily takes an agent with no prior
knowledge more than 50 steps for the agent to find the Figure 4. TrackModule(ProgressTracking). Ateverystep,
desired object (repeating the process of visiting each wetakethelast3stepsofroll-outascontextandappenda
receptacle, opening it, closing it). We observe that query (about whether the current sub-task is completed)
many receptacles and objects are irrelevant to specific to get the prompt. A pre-trained QA model generates a
Yes/No answer to the prompt. For the answer “Yes”, we
tasks during both training and evaluation, and can
update the tracker to the next sub-task.
be easily filtered with common-sense knowledge about
the tasks. For example, in Fig. 3 the task is to heat 3.3. Track
someapple. Byremovingtheirrelevantreceptacleslike
the coffeemachine, garbagecan, or objects like knife, Fortheagenttoutilizethehigh-levelplan,itfirstneeds
we could significantly shorten our observation. We to know which sub-task to execute. A human actor
therefore propose to leverage commonsense knowledge typically starts from the first item and check-off the
captured by large pre-trained QA models to design our tasks one by one until completion. Therefore, similar
Eliminate module M to mask out irrelevant recepta- toSection3.2,weuseapre-trainedQAmodeltodesign
E
cles and objects. the Track module M T to perform zero-shot sub-task
completion detection.1
FortaskT,wecreatepromptsintheformatP =“Your
r
taskisto: T. Whereshouldyougoto?”forreceptacles Specifically, as illustrated in Figure 4, for sub-task list
and P o =“Your task is to: T. Which objects will be S T ={s 1,...s k}, we keep track of a progress tracker p
relevant?”forobjects. Usingthepre-trainedQAmodel (initialized at 1) that indicates the sub-task the agent
M
ME
(i Pn ,a oz )er fo o- rsh eo at chm oa bn jn ee cr t, owe anco dm µput =e s Mcore (Pµ
o ,i
r=
)
i cs onc tu er xr ten at sly thw eor lk asin tg do sn te( ps sp) o.
f
W the
e
t ah ge en ntco om bsp eo rvse att ioh ne
E o i i ri E o i
for each receptacle r in observation at every step. µ
j 1Note that the current system design does not allow
representsthebeliefscoreofwhetherthecommon-sense re-visiting finished sub-tasks, so the agent has no means to
QA model believes the object/receptacle is relevant to recover if it undoes its previous sub-task at test time.
Plan, Eliminate, and Track
for the current sub-task and the question as “Did you keys as action scores for the policy π (Eq. 5).
finish the task of s ?”. For efficiency, we set d :=
p
min(d + 1,3) at each step. Note that d is reset to Ht =avg Embed(Oj) (1)
j∈[1,t−1]
1 whenever the progress tracker updates. Hence, the HA =(cid:2) Embed(at),...,Embed(at)(cid:3) (2)
templateP =concat(Ot−d,...,Ot−1,“Didyoufinish 1 n
a
Q=M
(cid:0) Embed(T),Ht,Embed(Ot),HA(cid:1)
(3)
the task of s ?”). We feed P to a pre-trained zero- Q
p a
shot QA model M and compute the probability of K =M
(cid:0) Embed(T),Ht,Embed(Ot),Embed(at)(cid:1)
T i K i
tokens ‘Yes’ and ‘No’ as follows: p (“Yes”|P ) and (4)
MT a
p (“No”|P ). If p (“Yes”|P )>p (“No”|P )
MT a MT a MT a π =softmax([Q·K i|i∈all permissible actions])
then we increment the tracker p to track the next sub-
(5)
task.
If the tracking ends prematurely, meaning that p > 4. Experiments and Results
len(S ) but the environment has not returned “done”,
T
we fall back to conditioning with T. We study the rate We present our experiments as follows. First, we ex-
of pre-mature ends in Section 4.4 in terms of precision plain the environment setup and baselines for our ex-
and recall. periments. Then we compare PET to the baselines on
different splits of the environment. Finally, we conduct
3.4. Agent ablation studies and analyze the PET framework part
by part. We show that PET generalizes better to hu-
Since the number of permissible actions can vary a
man goal specification under efficient behavior cloning
lot by the environment, the agent needs to handle
training.
arbitrary dimensions of action space. While Shridhar
et al. (2020b) addresses this challenge by generating
4.1. Experimental Details
actionstoken-by-token, suchagenerationprocessleads
to degenerate performance even on the training set. AlfWorld Environment ALFWorld (Shridhar
et al., 2020b) is a set of TextWorld environments
We draw inspiration from the field of text summariza-
(Coˆt´e et al., 2018b) that are parallels of the ALFRED
tion, where models are built to handle variable input
embodied dataset (Shridhar et al., 2020a). ALFWorld
lengths. Seeetal.(2017)generatesasummarythrough
includes6tasktypesthateachrequiresolvingmultiple
an attention-like “pointing” mechanism that extracts
compositional sub-goals. There are 3553 training task
the output word by word. Similarly, an attention-like
instances ({tasktype, object, receptacle, room}), 140
“pointing”modelcouldbeusedtoselectanactionfrom
in-distribution evaluation task instances (seen split -
the list of permissible actions.
tasksthemselvesarenovelbuttakeplaceinroomsseen
during training) and 134 out-of-distribution evaluation
task instances (unseen split - tasks take place in novels
Action Attention We are interested in learning a
rooms). An example of the task could be: “Rinse the
policy π that outputs the optimal action among per-
egg to put it in the microwave.” Each training instance
missible actions. We eschew the long rollout/ large
in AlfWorld comes with an expert, from which we
action space problems by (1) representing observations
collected our training demonstration.
by averaging over history, and (2) individually encod-
ing actions (Fig 5). In our proposed action attention
HumanGoalSpecification Thecrowd-sourcedhu-
framework, we first represent historical observations
mangoalspecificationsforevaluationcontain66unseen
Ht as the average of embeddings of all individual ob-
verbs and 189 unseen nouns (Shridhar et al., 2020b).
servations through history (Eq. 1), and HA as the list
In comparison, the template goals use only 12 ways of
of embeddings of all the current permissible actions
goal specification. In addition, the sentence structure
(Eq. 2). Then, in Eq. 3, we compute the query Q
for human goal specification is more diverse compared
usingatransformerwitha“query”head(M )ontask
Q to the template goals. Therefore, human goal experi-
embedding (Ht), the current observation embedding
ments are good for testing the generalization of models
(Ot), and the list of action embeddings (HA). In Eq.
to out-of-distribution scenarios.
4 we compute the key K for each action a using the
i i
same transformer with a “key” head (M ) on task
K Pre-trained LMs. For the Plan module (sub-task
embedding (Ht), the current observation embedding
generation), we experimented with the open-source
(Ot), and embedding of action (a ).
i GPT-Neo-2.7B (Black et al., 2021), and an industry-
Finally, we compute the dot-product of the query and scale LLM with 530B parameters (Smith et al., 2022).
Plan, Eliminate, and Track
Figure 5. Agent (Action Attention). Action Attention block is a transformer-based framework that computes a key K for
i
each permissible action and output action scores as dot-product between key and query Q from the observations.
Template Goal Specification Human Goal Specification
Model seen unseen seen unseen
BUTLER + DAgger* (Shridhar et al., 2020b) 40 35 8 3
BUTLER + BC (Shridhar et al., 2020b) 10 9 - -
GPT (Micheli & Fleuret, 2021) 91 95 42 57
PET + Action Attention (Ours) 70 67.5 52.5 60
Table 1. Comparison of different models in terms of completion rate per evaluation split (seen and unseen), with and
without human annotated goals. PET under-performs GPT on Template goal specifications but generalizes better to
human goal specifications. * We include the performance of BUTLER with DAgger for completeness. All other rows are
trained without interaction with the environment, MLE for GPT and behavior cloning for BUTLER+BC and PET.
For the Eliminate module (receptacle/object mask- weeksforDAggerv.s. 6hoursforBehaviorCloning). In
ing), we choose Macaw-11b (Tafjord & Clark, 2021), addition, we demonstrate that our models surpass the
which is reported to have common sense QA perfor- DAgger training performance of the BUTLER (Shrid-
mance on par with GPT3 (Brown et al., 2020) while har et al., 2020b) agents trained with DAgger, even
being orders of magnitudes smaller. We use a decision when our agent does not have the option to interact
threshold of 0.4 for Macaw score below which the ob- with the environment.
jects are masked out. For the Track module (progress
tracking), we use the same Macaw-11b model as the
Eliminate module answer to Yes/No questions.
Actor Model Design. Our Action Attention
agent (M Q and M K) is a 12-layer transformer with Baselines. Our first baseline is the BUT-
12 heads and hidden dimension 384. The last layer LER::BRAIN (BUTLER) agent (Shridhar et al.,
is then fed into two linear heads to generate K and 2020b), which consists of an encoder, an aggregator,
Q. For embedding of actions and observations, we use and a decoder. At each time step t, the encoder
pre-trained RoBERTa-large (Liu et al., 2019) with em- takes initial observation s0, current observation st,
bedding dimension 1024. For sub-task generation, we and task string s and generates representation
task
use ground-truth sub-tasks for training, and generated rt. The recurrent aggregator combines rt with the
sub-tasks from Plan module for evaluation. last recurrent state ht−1 to produce ht, which is
then decoded into a string at representing action. In
Experimental Setup. Unlike the original bench- addition, the BUTLER agent uses beam search to get
mark (Shridhar et al., 2020b), we experiment with out of stuck conditions in the event of a failed action.
models trained with behavior cloning. Although Shrid- Our second baseline GPT (Micheli & Fleuret, 2021)
har et al. (2020b) observe that models benefit greatly is a fine-tuned GPT2-medium on 3553 demonstrations
from DAgger training, DAgger assumes an expert that from the AlfWorld training set. Specifically, the
is well-defined at all possible states, which is inefficient GPT is fined-tuned to generate each action step
and impractical. In our experiments, training is 100x word-by-word to mimic the rule-based expert using
slower with DAgger compared to behavior cloning (3 the standard maximum likelihood loss.
Plan, Eliminate, and Track
4.2. Overall Results on Template and Human 4.4. Automated Analysis of PET modules
Goals
Plan Module We experiment with different LLMs
We compare the performance of action attention as- such as GPT2-XL (Radford et al., 2019), GPT-Neo-
sisted by PET with BUTLER (Shridhar et al., 2020b) 2.7B (Black et al., 2021), and the 530B parameter
and fine-tuned GPT (Micheli & Fleuret, 2021) in Ta- MT-NLG (Smith et al., 2022) models. Table 2 reports
ble1. Forhumangoalspecifications, PEToutperforms the generation accuracy and the RoBERTa (Liu et al.,
SOTA (GPT) by 25% on seen and 5% on the unseen 2019)embeddingcosinesimilarityagainstground-truth
split. sub-tasks. We observe that all LLMs achieve high
accuracy on template goal specifications, where there
Although PET under-performs GPT on Template goal
is no variation in sentence structures. For human goal
specifications, GPT requires fine-tuning on fully text-
specification, MT-NLG generates subtasks similar to
based expert trajectory and thus loses adaptability to
ground truth in terms of embedding similarity, while
differentenvironmentsettings. Qualitatively,onhuman
the other smaller models perform significantly worse.
goal specification tasks, where the goal specifications
areout-of-distribution, GPToftengetsstuckrepeating
Eliminate module Weevaluatethezero-shotrecep-
the same action after producing a single wrong move.
tacle/object masking performance of Macaw on the
On the other hand, since the Plan module of PET is
three splits of AlfWorld. In Fig 6, we illustrate the
not trained on the task, it generalizes to the variations
AUCcurveoftherelevancescorethatthemodelassigns
for human goal specifications as shown in Section 4.5.
to the objects v.s. objects that the rule-based expert
Quantitatively, GPTsuffersfromarelative50%perfor-
interacted with when completing each task. Since the
mance drop transferring from template to human-goal
Macaw QA model is queried in a zero-shot manner, it
specifications, whereas PET incurs only a 15 ∼ 25%
demonstrates consistent masking performance on all
drop.
three splits of the environment, even on the unseen
The setting closest to PET is BUTLER with behavior split. In addition, we note that object receptacle accu-
cloning (BUTLER + BC). Since BUTLER + BC per- racy is generally lower than object accuracy because of
forms poorly, we also include DAgger training results. the counter-intuitive spawning locations described in
Nevertheless, action attention assisted by PET outper- Section 4.5. In our experiments, a decision threshold
forms BUTLER with DAgger by more than 2x while of 0.4 has a recall of 0.91 and reduces the number of
being much more efficient. (Section 4.1) objects in observation by 40% on average.
4.3. Ablations for Plan, Eliminate, and Track Trackmodule Sincesub-taskalignmentinformation
is not provided by the environment, we explore an
In Table 3, we analyze the contribution of each PET
alternative performance metric for the detection of the
module by sequentially adding each component to the
event of completion. Ideally, a sub-task tracker should
actionattentionagenton140trainingtrajectoriessam-
record the last sub-task as “finished” if and only if the
pled from the training set. The data set size is chosen
environment is “fully solved” by the expert. As an
to match the size of the seen validation set, for an
agreement measure, we report a precision of 0.99 and
efficient and sparse setting. Note that we treat Plan
a recall of 0.78 for Macaw-11B and a precision of 0.96
and Track as a single module for this ablation since
and a recall of 0.96 for Macaw-large. The larger model
they cannot work separately.
(Macaw-11b)ismoreprecisebutmissesmoredetection,
Adding Plan and Track greatly improves the comple- therefore limiting the theoretical performance to 78%.
tion rate relatively by 60%, which provides evidence to The smaller model is much less accurate according to
our hypothesis that solving some embodied tasks step- human evaluation but does not limit the overall model
by-stepreducesthecomplexity. Weobservearelatively performance in theory. In our experiments, we find
insignificant 3% improvement in absolute performance thatbothmodelsproducesimilaroverallresults, which
when adding Eliminate without sub-task tracking. On may suggest that the overall results could be improved
the other hand, when applying Eliminate to sub-tasks withLLMsdoingbetteronbothprecisionandrecall.
with Plan and Track, we observe more than 60% rela-
tive improvement over just Plan and Track alone. We, 4.5. Qualitative Analysis
therefore,deducethatPlanandTrackboosttheperfor-
Plan Module We show two types of failure exam-
mance of Eliminate during evaluation, since it is easier
ples for sub-task generation in Table 4. The first type
toremoveirrelevantobjectswhentheobjectiveismore
oferroriscausedbygeneratingsynonymsoftheground
focused on sub-tasks.
truth, and the second type of error is caused by inaccu-
Plan, Eliminate, and Track
Template Goals Human Goals
LLM seen unseen seen unseen
GPT-2 (Radford et al., 2019) 94.29 (0.97) 87.31 (0.94) 10.07 (0.62) 7.98 (0.58)
GPT-Neo-2.7B (Black et al., 2021) 99.29 (1.00) 96.27 (0.98) 4.70 (0.82) 9.16 (0.80)
MT-NLG (Smith et al., 2022) 98.57 (0.99) 100 (1.00) 40.04 (0.94) 49.3 (0.94)
Table 2. Evaluation of different LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity
(in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated
goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds
theperformanceofsmallermodelsonhardtaskswithhumangoalspecification. Inaddition, MT-NLGgeneratessub-tasks
with almost perfect embedding similarity for all tasks.
Figure 6. Plot of AUC scores of zero-shot relevance identification across all tasks in the Alfworld-Thor environment,
with the Macaw-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert.
Top: Receptacle relevance identification. Bottom: Object relevance identification. The QA model achieves an average
AUC-ROC score of 65 for receptacles and 76 on objects.
Model Ablations seen unseen incorrectly masks a receptacle that contains the object
Action Attention 25 9 of interest so the agent fails to find such receptacles.
Action Attention + Eliminate 25 11 This is often because some objects in the AI2Thor
Action Attention + Plan & Track 35 15 simulator do not spawn according to common sense.
Action Attention + PET 52.5 27.5 As noted in the documentation of the environment2,
objects like Apple or Egg has a chance of spawning in
Table 3. Comparison of different ablations of PET trained unexpected receptacles like GarbageCan, or TVStand.
on a sampled set of 140 demonstrations from the training However, such generations in AI2Thor are unlikely in
set, in terms of completion rate per evaluation split (seen real deployment; thus, the “mistakes” of our Eliminate
and unseen). Applying Eliminate module alone has an module are reasonable.
insignificant effect on overall performance compared to
Plan & Track. However, applying Eliminate module on
sub-tasks together with Plan & Track results in a much Track Module Experimentally, we find that sub-
more significant performance improvement. task planning/tracking is particularly helpful for tasks
that require counting procedures. As shown in Ta-
ble ??, PET breaks the task of “Place two soapbar
racies in the human goal specifications. Note that our
in cabinet” into two repeating set of sub-tasks: “take
Action Attention framework uses RoBERTa (Liu et al.,
soapbar→place soapbar in/on cabinet”. Sub-task plan-
2019) embedding for sub-tasks, known to be robust to
ningandtracking,therefore,simplifythehardproblem
synonym variations.
of counting.
Eliminate Module We observe that the main 2ai2thor.allenai.org/ithor/documentation/objects/object-
source of elimination error occurs when the module types/
Plan, Eliminate, and Track
Human Goal Specification Examples policy (i.e., reading an instruction manual about the
Task Chill a cup and place it in the cabinet. environment).
cool the mug→place the mug in/on coffeema-
GT
chine References
chill the mug→return the mug to coffeema-
Gen Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes,
chine
O., David, B., Finn, C., Fu, C., Gopalakrishnan, K.,
Take the pencil from the desk, put it on the
Task Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz,
other side of the desk
J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J.,
GT take a pencil→place the pencil in/on shelf
Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R.,
pick up the white pencil on the desk→put the
Gen Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S.,
white pencil on another spot on the desk
Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao,
J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet,
Table 4. FailureexamplesfromthePlanmoduleonhuman
P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V.,
goalspecifications(Task),ground-truth(GT)v.s. generated
Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and
(Gen). In the first example, generated plan differs from
the ground truth but the meaning agrees. In the second Zeng, A. Do as i can, not as i say: Grounding
example,thegeneratedplanlargelydiffersfromtheground language in robotic affordances, 2022. URL https:
truth due to the mistake in human goal specification — //arxiv.org/abs/2204.01691.
“another side on the desk” instead of “shelf”.
Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani,
M., and Sigaud, O. Grounding language to
5. Conclusion, Limitations, and Future
autonomously-acquired skills via goal generation.
Work
arXiv preprint arXiv:2006.07185, 2020.
In this work, we propose the Plan, Eliminate, and
Andreas, J., Klein, D., and Levine, S. Modular multi-
Track (PET) framework that uses pre-trained LLMs
task reinforcement learning with policy sketches. In
to assist an embodied agent in three steps. Our PET
International Conference on Machine Learning, pp.
frameworkrequiresnofine-tuningandisdesignedtobe
166–175. PMLR, 2017.
compatible with any goal-conditional embodied agents.
Black, S., Gao, L., Wang, P., Leahy, C., and Bider-
In our experiments, we combine PET with a novel Ac-
man, S. GPT-Neo: Large Scale Autoregressive Lan-
tion Attention agent that handles the dynamic action
guage Modeling with Mesh-Tensorflow, March 2021.
space in AlfWorld. Our Action Attention agent greatly
URL https://doi.org/10.5281/zenodo.5297715.
outperforms the BUTLER baseline. In addition, since
If you use this software, please cite it using these
thePETframeworkisnottrainedtofitthetrainingset
metadata.
tasks, it demonstrates better generalization to unseen
human goal specification tasks. Finally, our ablation
Blukis,V., Paxton, C.,Fox,D., Garg,A., andArtzi,Y.
studies show the Plan and Track modules together im-
Apersistentspatialsemanticrepresentationforhigh-
prove the performance of Eliminate module to achieve
level natural language instruction execution, 2021.
the best performance.
URL https://arxiv.org/abs/2107.05612.
Our results show that LLMs can be a good source of
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
common sense and procedural knowledge for embodied
Arora, S., von Arx, S., Bernstein, M. S., Bohg, J.,
agents,andmultipleLLMsmaybeusedincoordination
Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch,
with each other to further improve effectiveness.
S., Card, D., Castellon, R., Chatterji, N., Chen,
One of the major limitations of our current system A., Creel, K., Davis, J. Q., Demszky, D., Don-
designisthattheTrackmodule(progresstracker)does ahue, C., Doumbouya, M., Durmus, E., Ermon, S.,
notre-visitfinishedsub-tasks. Ifforexample,theagent Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn,
isexecutingsub-tasks[pickedupapan, putthepanon C., Gale, T., Gillespie, L., Goel, K., Goodman, N.,
countertop], and it picked up a pan but put it in the Grossman, S., Guha, N., Hashimoto, T., Hender-
fridge (undo pickup action). Since the progress tracker son, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K.,
doesnottakeintoconsiderationpreviousprogressbeing Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri,
undone,thesystemmaybreakinthissituation. Future P., Karamcheti, S., Keeling, G., Khani, F., Khat-
work can focus on adding sub-task-level dynamic re- tab, O., Koh, P. W., Krass, M., Krishna, R., Ku-
planning to address this limitation or explore other ditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee,
ways in which LLMs can assist the learning of the T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma,
Plan, Eliminate, and Track
T., Malik, A., Manning, C. D., Mirchandani, S., a natural language action space. arXiv preprint
Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., arXiv:1511.04636, 2015.
Narayanan, D., Newman, B., Nie, A., Niebles, J. C.,
Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Pa- Huang, W., Abbeel, P., Pathak, D., and Mordatch, I.
padimitriou, I., Park, J. S., Piech, C., Portelance, Language models as zero-shot planners: Extracting
E., Potts, C., Raghunathan, A., Reich, R., Ren, actionable knowledge for embodied agents, 2022a.
H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., R´e, URL https://arxiv.org/abs/2201.07207.
C., Sadigh, D., Sagawa, S., Santhanam, K., Shih,
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J.,
A., Srinivasan, K., Tamkin, A., Taori, R., Thomas,
Florence, P., Zeng, A., Tompson, J., Mordatch, I.,
A. W., Tram`er, F., Wang, R. E., Wang, W., Wu,
Chebotar, Y., Sermanet, P., Brown, N., Jackson,
B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You,
T., Luu, L., Levine, S., Hausman, K., and Ichter,
J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X.,
B. Inner monologue: Embodied reasoning through
Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On
planning with language models, 2022b. URL https:
the opportunities and risks of foundation models,
//arxiv.org/abs/2207.05608.
2021. URL https://arxiv.org/abs/2108.07258.
Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert,
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,
F., Lynch, C., Levine, S., and Finn, C. Bc-z: Zero-
J. D., Dhariwal, P., Neelakantan, A., Shyam, P.,
shot task generalization with robotic imitation learn-
Sastry, G., Askell, A., et al. Language models are
ing. In Conference on Robot Learning, pp. 991–1002.
few-shot learners. Advances in neural information
PMLR, 2022.
processing systems, 33:1877–1901, 2020.
Jiang, Y., Gu, S. S., Murphy, K. P., and Finn, C. Lan-
Chaplot, D. S., Gandhi, D., Gupta, A., and Salakhut-
guage as an abstraction for hierarchical deep rein-
dinov, R. Object goal navigation using goal-oriented
forcement learning. Advances in Neural Information
semantic exploration, 2020. URL https://arxiv.
Processing Systems, 32, 2019.
org/abs/2007.00643.
Cideron, G., Seurin, M., Strub, F., and Pietquin, O. Kollar, T., Tellex, S., Roy, D., and Roy, N. Toward
Higher: Improving instruction following with hind- understanding natural language directions. In 2010
sightgenerationforexperiencereplay. In2020 IEEE 5thACM/IEEEInternationalConferenceonHuman-
Symposium Series on Computational Intelligence Robot Interaction (HRI), pp. 259–266. IEEE, 2010.
(SSCI), pp. 225–232. IEEE, 2020.
Lin, B. Y., Huang, C., Liu, Q., Gu, W., Sommerer,
Cˆot´e, M.-A., K´ad´ar, A., Yuan, X., Kybartas, B., S., and Ren, X. On grounded planning for em-
Barnes, T., Fine, E., Moore, J., Hausknecht, M., bodied tasks with language models. arXiv preprint
Asri, L. E., Adada, M., et al. Textworld: A learning arXiv:2209.00465, 2022.
environment for text-based games. In Workshop on
Liu,Y.,Ott,M.,Goyal,N.,Du,J.,Joshi,M.,Chen,D.,
Computer Games, pp. 41–75. Springer, 2018a.
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov,
Cˆot´e, M.-A., K´ad´ar, A., Yuan, X., Kybartas, B., V. Roberta: A robustly optimized bert pretraining
Barnes, T., Fine, E., Moore, J., Hausknecht, M., approach. arXiv preprint arXiv:1907.11692, 2019.
Asri, L. E., Adada, M., et al. Textworld: A learning
MacMahon,M.,Stankiewicz,B.,andKuipers,B. Walk
environment for text-based games. In Workshop on
the talk: Connecting language, knowledge, and ac-
Computer Games, pp. 41–75. Springer, 2018b.
tion in route instructions. Def, 2(6):4, 2006.
Fulda, N., Ricks, D., Murdoch, B., and Wingate,
D. What can you do with a rock? affordance Mei, H., Bansal, M., and Walter, M. R. Listen, at-
extraction via word embeddings. arXiv preprint tend, and walk: Neural mapping of navigational
arXiv:1703.03429, 2017. instructions to action sequences. In Thirtieth AAAI
Conference on Artificial Intelligence, 2016.
Goyal,P.,Niekum,S.,andMooney,R. Pixl2r: Guiding
reinforcement learning using natural language by Micheli, V. and Fleuret, F. Language models are few-
mapping pixels to rewards. In Conference on Robot shot butlers. arXiv preprint arXiv:2104.07972, 2021.
Learning, pp. 485–497. PMLR, 2021.
Min, S. Y., Chaplot, D. S., Ravikumar, P., Bisk, Y.,
He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Salakhutdinov, R. Film: Following instructions
and Ostendorf, M. Deep reinforcementlearning with in language with modular methods, 2021.
Plan, Eliminate, and Track
Misra,D.,Langford,J.,andArtzi,Y. Mappinginstruc- language models. arXiv preprint arXiv:2212.04088,
tions and visual observations to actions with rein- 2022.
forcementlearning. arXivpreprintarXiv:1704.08795,
Stepputtis, S., Campbell, J., Phielipp, M., Lee, S.,
2017.
Baral, C., and Ben Amor, H. Language-conditioned
Nair, S., Mitchell, E., Chen, K., Savarese, S., Finn, C., imitation learning for robot manipulation tasks. Ad-
et al. Learning language-conditioned robot behav- vances in Neural Information Processing Systems,
ior from offline data and crowd-sourced annotation. 33:13139–13150, 2020.
In Conference on Robot Learning, pp. 1303–1315.
Tafjord, O. and Clark, P. General-purpose
PMLR, 2022.
question-answering with macaw. arXiv preprint
Oh, J., Singh, S., Lee, H., and Kohli, P. Zero-shot task arXiv:2109.02593, 2021.
generalization with multi-task deep reinforcement
Tellex, S., Kollar, T., Dickerson, S., Walter, M., Baner-
learning. In International Conference on Machine
jee, A., Teller, S., and Roy, N. Understanding natu-
Learning, pp. 2661–2670. PMLR, 2017.
ral language commands for robotic navigation and
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., mobile manipulation. In Proceedings of the AAAI
and Sutskever, I. Language models are unsupervised Conference on Artificial Intelligence, volume 25, pp.
multitask learners. 2019. 1507–1514, 2011.
See, A., Liu, P. J., and Manning, C. D. Get to the Yao, S., Rao, R., Hausknecht, M., and Narasimhan,
point: Summarization with pointer-generator net- K. Keep calm and explore: Language models for
works. arXiv preprint arXiv:1704.04368, 2017. action generation in text-based games, 2020. URL
https://arxiv.org/abs/2010.02903.
Sharma, P., Torralba, A., and Andreas, J. Skill in-
duction and planning with latent language. arXiv Zahavy, T., Haroush, M., Merlis, N., Mankowitz, D. J.,
preprint arXiv:2110.01517, 2021. and Mannor, S. Learn what not to learn: Action
elimination with deep reinforcement learning. Ad-
Shridhar, M., Thomason, J., Gordon, D., Bisk, Y.,
vances in neural information processing systems, 31,
Han, W., Mottaghi, R., Zettlemoyer, L., and Fox,
2018.
D. Alfred: A benchmark for interpreting grounded
instructions for everyday tasks. In Proceedings of
the IEEE/CVF conference on computer vision and
pattern recognition, pp. 10740–10749, 2020a.
Shridhar,M.,Yuan,X.,Coˆt´e,M.-A.,Bisk,Y.,Trischler,
A., and Hausknecht, M. Alfworld: Aligning text
and embodied environments for interactive learning.
arXiv preprint arXiv:2010.03768, 2020b.
Shridhar, M., Manuelli, L., and Fox, D. Cliport: What
and where pathways for robotic manipulation. In
Conference on Robot Learning, pp. 894–906. PMLR,
2022.
Smith, S., Patwary, M., Norick, B., LeGresley, P., Ra-
jbhandari, S., Casper, J., Liu, Z., Prabhumoye, S.,
Zerveas, G., Korthikanti, V., Zheng, E., Child, R.,
Aminabadi, R. Y., Bernauer, J., Song, X., Shoeybi,
M., He, Y., Houston, M., Tiwary, S., and Catan-
zaro, B. Using deepspeed and megatron to train
megatron-turing NLG 530b, A large-scale generative
language model. CoRR, abs/2201.11990, 2022. URL
https://arxiv.org/abs/2201.11990.
Song, C. H., Wu, J., Washington, C., Sadler, B. M.,
Chao, W.-L., and Su, Y. Llm-planner: Few-shot
grounded planning for embodied agents with large

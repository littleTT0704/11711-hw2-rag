The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction
JunweiLiang1∗ LuJiang2 KevinMurphy2 TingYu3 AlexanderHauptmann1
1CarnegieMellonUniversity 2GoogleResearch 3GoogleCloudAI
{junweil,alex}@cs.cmu.edu, {lujiang,kpmurphy,yuti}@google.com
Figure1: Illustrationofpersontrajectoryprediction.(1)Apersonwalkstowardsacar(datafromtheVIRAT/ActEVdataset).
Thegreenlineistheactualfuturetrajectoryandtheyellow-orangeheatmapsareexamplefuturepredictions. Althoughthese
predictions near the cars are plausible, they would be considered errors in the real video dataset. (2) To combat this, we
proposeanewdatasetcalled“ForkingPaths”;hereweillustrate3possiblefuturescreatedbyhumanannotatorscontrolling
agents in a synthetic world derived from real data. (3) Here we show semantic segmentation of the scene. (4-6) Here we
showthesamescenerenderedfromdifferentviewingangles,wheretheredcirclesarefuturedestinations.
Abstract Of course, the future is often very uncertain: Given
the same historical trajectory, a person may take differ-
This paper studies the problem of predicting the distri-
ent paths, depending on their (latent) goals. Thus recent
butionovermultiplepossiblefuturepathsofpeopleasthey
workhasstartedfocusingonmulti-futuretrajectorypredic-
move through various visual scenes. We make two main
tion[53,6,26,34,54,23].
contributions. The first contribution is a new dataset, cre-
ConsidertheexampleinFig.1. Weseeapersonmoving
ated in a realistic 3D simulator, which is based on real
fromthebottomlefttowardsthetoprightoftheimage,and
worldtrajectorydata,andthenextrapolatedbyhumanan-
ourtaskistopredictwherehewillgonext. Sincethereare
notatorstoachievedifferentlatentgoals. Thisprovidesthe
many possible future trajectories this person might follow,
firstbenchmarkforquantitativeevaluationofthemodelsto
weareinterestedinlearningamodelthatcangeneratemul-
predictmulti-futuretrajectories. Thesecondcontributionis
tipleplausiblefutures.However,sincethegroundtruthdata
anewmodeltogeneratemultipleplausiblefuturetrajecto-
only contains one trajectory, it is difficult to evaluate such
ries, which contains novel designs of using multi-scale lo-
probabilisticmodels.
cationencodingsandconvolutionalRNNsovergraphs. We
To overcome the aforementioned challenges, our first
refertoour modelasMultiverse. Weshowthatour model
contribution is the creation of a realistic synthetic dataset
achieves the best results on our dataset, as well as on the
that allows us to compare models in a quantitative way in
real-world VIRAT/ActEV dataset (which just contains one
possiblefuture). 1 terms of their ability to predict multiple plausible futures,
rather than just evaluating them against a single observed
1.Introduction
trajectory as in existing studies. We create this dataset us-
Forecasting future human behavior is a fundamental ing the 3D CARLA [11] simulator, where the scenes are
problem in video understanding. In particular, future path manuallydesignedtobesimilartothosefoundinthechal-
prediction, which aims at forecasting a pedestrian’s future lengingreal-worldbenchmarkVIRAT/ActEV[36,3].Once
trajectoryinthenextfewseconds, hasreceivedalotofat- we have recreated the static scene, we automatically re-
tentioninourcommunity[20,1,15,26]. Thisfunctionality construct trajectories by projecting real-world data to the
isakeycomponentinavarietyofapplicationssuchasau- 3D simulation world. See Fig. 1 and 3. We then semi-
tonomousdriving[4,6],long-termobjecttracking[19,48], automatically select a set of plausible future destinations
safetymonitoring[30],roboticplanning[42,43],etc. (correspondingtosemanticallymeaningfullocationsinthe
scene), andaskhumanannotatorstocreatemultiplepossi-
∗WorkpartiallydoneduringaresearchinternshipatGoogle.
1Codeandmodelsarereleasedathttps://next.cs.cmu.edu/ blecontinuationsoftherealtrajectoriestowardseachsuch
multiverse goal. In this way, our dataset is “anchored” in reality, and
1
0202
raM
82
]VC.sc[
3v54460.2191:viXra
yetcontainsplausiblevariationsinhigh-levelhumanbehav- motionsbyconsideringthemaspointsinthescene. These
ior,whichisimpossibletosimulateautomatically. research works [21, 60, 33, 30] have attempted to predict
Wecallthisdatasetthe“ForkingPaths”dataset,arefer- personpathsbyutilizingvisualfeatures. RecentlyLianget
encetotheshortstorybyJorgeLuisBorges.2 Asshownin al. [30] proposed a joint future activity and trajectory pre-
Fig.1,differenthumanannotationshavecreatedforkingsof dictionframeworkthatutilizedmultiplevisualfeaturesus-
futuretrajectoriesfortheidenticalhistoricalpast.Sofar,we ingfocalattention[29,28]. Manyworks[23,50,4,18,64]
havecollected750sequences,witheachcoveringabout15 invehicletrajectorypredictionhavebeenproposed. CAR-
seconds,from10annotators,controlling127agentsin7dif- Net [50] proposed attention networks on top of scene se-
ferentscenes. Eachagentcontains5.9futuretrajectorieson mantic CNN to predict vehicle trajectories. Chauffeur-
average. We render each sequence from 4 different views, net[4]utilizedimitationlearningfortrajectoryprediction.
and automatically generate dense labels, as illustrated in Multi-future trajectory prediction. Many works have
Fig. 1 and 3. In total, this amounts to 3.2 hours of trajec- triedtomodeltheuncertaintyoftrajectoryprediction. Var-
tory sequences, which is comparable to the largest person ious papers (e.g. [20, 43, 44] use Inverse Reinforcement
trajectorybenchmarkVIRAT/ActEV[3,36](4.5hours),or Learning (IRL) to forecast human trajectories. Social-
5timesbiggerthanthecommonETH/UCY[24,32]bench- LSTM[1]isapopularmethodusingsocialpoolingtopre-
mark. Wethereforebelievethiswillserveasabenchmark dict future trajectories. Other works [49, 15, 26, 2] like
forevaluatingmodelsthatcanpredictmultiplefutures. Social-GAN [15] have utilized generative adversarial net-
Oursecondcontributionistoproposeanewprobabilistic works[14]togeneratediversepersontrajectories. Invehi-
model, Multiverse, which can generate multiple plausible cle trajectory prediction, DESIRE [23] utilized variational
trajectoriesgiventhepasthistoryoflocationsandthescene. auto-encoders (VAE) to predict future vehicle trajectories.
The model contains two novel design decisions. First, we Many recent works [54, 6, 53, 34] also proposed proba-
use a multi-scale representation of locations. In the first bilistic frameworks for multi-future vehicle trajectory pre-
scale,thecoarsescale,werepresentlocationsona2Dgrid, diction. Different from these previous works, we present
asshowninFig.1(1). Thiscaptureshighleveluncertainty aflexibletwo-stageframeworkthatcombinesmulti-modal
aboutpossibledestinationsandleadstoabetterrepresenta- distributionmodelingandpreciselocationprediction.
tionofmulti-modaldistributions. Inthesecondfinescale, Trajectory Datasets. Many vehicle trajectory datasets [5,
wepredictareal-valuedoffsetforeachgridcell,togetmore 7] have been proposed as a result of self-driving’s surging
preciselocalization.Thistwo-stageapproachispartiallyin- popularity. With the recent advancement in 3D computer
spired by object detection methods [41]. The second nov- visionresearch[63,27,51,11,45,47,16], manyresearch
eltyofourmodelistodesignconvolutionalRNNs[58]over works[39,12,10,9,57,66,52]havelookedinto3Dsim-
thespatialgraphasawayofencodinginductivebiasabout ulatedenvironmentforitsflexibilityandabilitytogenerate
themovementpatternsofpeople. enormous amount of data. We are the first to propose a
In addition, we empirically validate our model on the 3Dsimulationdatasetthatisreconstructedfromreal-world
challenging real-world benchmark VIRAT/ActEV [36, 3] scenarioscomplementedwithavarietyofhumantrajectory
for single-future trajectory prediction, in which our model continuationsformulti-futurepersontrajectoryprediction.
achieves the best-published result. On the proposed simu-
3.Methods
lationdataformulti-futureprediction,experimentalresults
In this section, we describe our model for forecasting
show our model compares favorably against the state-of-
agent trajectories, which we call Multiverse. We focus on
the-artmodelsacrossdifferentsettings. Tosummarize,the
predictingthelocationsofasingleagentformultiplesteps
main contributions of this paper are as follows: (i) We in-
into the future, L , given a sequence of past video
troducethefirstdatasetandevaluationmethodologythatal- h+1:T
frames,V ,andagentlocations,L ,wherehisthehis-
lows us to compare models in a quantitative way in terms 1:h 1:h
torylengthandT −histhepredictionlength. Sincethere
oftheirabilitytopredictmultipleplausiblefutures. (ii)We
is inherent uncertainty in this task, our goal is to design
propose a new effective model for multi-future trajectory
a model that can effectively predict multiple plausible fu-
prediction. (iii) We establish a new state of the art result
turetrajectories, bycomputingthemultimodaldistribution
onthechallengingVIRAT/ActEVbenchmark,andcompare
p(L |L ,V ). SeeFig.2forahighlevelsummary
variousmethodsonourmulti-futurepredictiondataset. h+1:T 1:h 1:h
2.RelatedWork ofthemodel,andthesectionsbelowformoredetails.
Single-future trajectory prediction. Recent works have 3.1.HistoryEncoder
triedtopredictasinglebesttrajectoryforpedestriansorve-
Theencodercomputesarepresentationofthescenefrom
hicles.Earlyworks[35,59,62]focusedonmodelingperson thehistoryofpastlocations,L ,andframes,V . Ween-
1:h 1:h
2 https://en.wikipedia.org/wiki/The_Garden_of_ code each ground truth location L t by an index Y t ∈ G
Forking_Paths representingthenearestcellina2DgridGofsizeH×W,
Figure 2: Overview of our model. The input to the model is the ground truth location history, and a set of video frames,
whicharepreprocessedbyasemanticsegmentationmodel. Thisisencodedbythe“HistoryEncoder”convolutionalRNN.
The output of the encoder is fed to the convolutional RNN decoder for location prediction. The coarse location decoder
outputsaheatmapoverthe2DgridofsizeH ×W. Thefinelocationdecoderoutputsavectoroffsetwithineachgridcell.
ThesearecombinedtogenerateamultimodaldistributionoverR2forpredictedlocations.
indexed from 1 to HW. Inspired by [22, 31], we encode (knownasthe“beliefstate”)bedenotedbyC (i)=p(Y =
t t
locationwithtwodifferentgridscales(36×18and18×9); i|Y ,H), for ∀i ∈ G and t ∈ [h+1,T]. For brevity,
h:t−1
we show the benefits of this multi-scale encoding in Sec- we use a single index i to represent a cell in the 2D grid.
tion5.4.Forsimplicityofpresentation,wefocusonasingle RatherthanassumingaMarkovmodel,weupdatethisusing
H ×W grid. aconvolutionalrecurrentneuralnetwork,withhiddenstates
To make the model more invariant to low-level visual HC. Wethencomputethebeliefstateby:
t
details,andthusmorerobusttodomainshift(e.g.,between C =softmax(W ∗HC)∈RHW (2)
t t
different scenes, different views of the same scene, or be-
Here we use 2D-convolution with one filter and flatten the
tweenrealandsyntheticimages),wepreprocesseachvideo
spatial dimension before applying softmax. The hidden
frameV usingapre-trainedsemanticsegmentationmodel,
t stateisupdatedusing:
with K = 13 possible class labels per pixel. We use the
HC =ConvRNN(GAT(HC ),embed(C )) (3)
Deeplabmodel[8]trainedontheADE20k[65]dataset,and t t−1 t−1
keepitsweightsfrozen. LetS
t
bethissemanticsegmenta- whereembed(C t−1)embedsintoa3DtensorofsizeH ×
tionmapmodeledasatensorofsizeH ×W ×K. W × d e and d e is the embedding size. GAT(H tC −1) is
We then pass these inputs to a convolutional RNN [58, a graph attention network [55], where the graph structure
56]tocomputeaspatial-temporalfeaturehistory: corresponds to the 2D grid in G. More precisely, let h i
He =ConvRNN(one-hot(Y )(cid:12)(W ∗S ),He ) (1) be the feature vector corresponding to the i-th grid cell in
t t t t−1 HC , and let h˜ be the corresponding output in H˜C =
w coh ne vr oe lu(cid:12) tioi ns .e Tle hm ee fn ut ncw tii ose
n
op nro ed -hu oc tt (, ·)an pd ro∗ jecr te spr aes ce en llts in2 dD ex- GAt− T1
(H tC −1) ∈
Ri
H×W×ddec, where d
dec
is the
sizet o− f1
the
decoder hidden state. We compute these outputs of GAT
into an one-hot embedding of size H × W according to
using:
its spatial location. We use the final state of this encoder
H te ∈ RH×W×denc, where d
enc
is the hidden size, to ini- h˜
i
= |N1
|
(cid:88) f e([v i,v j])+h
i
(4)
tialize the state of the decoders. We also use the temporal i j∈Ni
averageofthesemanticmaps,S = 1 (cid:80)h S ,duringeach whereN aretheneighborsofnodev inGwitheachnode
h t=1 t i i
decodingstep. ThecontextisrepresentedasH=[He,S]. represented as v i = [h i,S i], where S i collects the cell i’s
h
feature in S. f is some edge function (implemented as
3.2.CoarseLocationDecoder e
an MLP in our experiments) that computes the attention
AftergettingthecontextH,ourgoalistoforecastfuture weights.
locations. We initially focus on predicting locations at the
levelofgridcells,Y ∈G.InSection3.3,wediscusshowto The graph-structured update function for the RNN en-
t
predictacontinuousoffsetinR2,whichspecifiesa“delta” suresthattheprobabilitymass“diffusesout”tonearbygrid
fromthecenterofeachgridcell,togetafine-grainedloca- cellsinacontrolledmanner,reflectingthepriorknowledge
tionprediction. thatpeopledonotsuddenlyjumpbetweendistantlocations.
Let the coarse distribution over grid locations at time t This inductive bias is also encoded in the convolutional
structure,butaddingthegraphattentionnetworkgivesim- Note that during training, when updating the RNN, we
provedresults,becausetheweightsareinput-dependentand feed in the predicted soft distribution over locations, C .
t
notfixed. SeeEq.(2). Analternativewouldbetofeedinthetrueval-
ues,C∗,i.e.,useteacherforcing. However,thisisknownto
3.3.FineLocationDecoder t
sufferfromproblems[40].
The2Dheatmapisusefulforcapturingmultimodaldis-
tributions, but does not give very precise location predic- 3.5.Inference
tions. To overcome this, we train a second convolutional
To generate multiple qualitatively distinct trajectories,
RNNdecoderHOtocomputeanoffsetvectorforeachpos-
t we use the diverse beam search strategy from [25]. To
siblegridcellusingaregressionoutput,O =MLP(HO)∈
t t define this precisely, let B be the beam at time t−1;
RH×W×2. ThisRNNisupdatedusing thissetcontainsK trajectort i− e1 s(historyselections)Mk =
H tO =ConvRNN(GAT(H tO −1),O t−1)∈RH×W×ddec {Yˆk,...,Yˆk }, k ∈ [1,K], where Yˆk is an indext i− n1 G,
1 t−1 t
(5) along with their accumulated log probabilities, Pk . Let
t−1
Tocomputethefinalpredictionlocation,wefirstflattenthe Ck =f(Mk )∈RHW bethecoarselocationoutputprob-
spatialdimensionofO tintoO˜ t ∈RHW×2. Thenweuse abt ilityfromt− E1 q.(2)and(3)attimetgiveninputsM tk −1.
L t =Q i+O˜ ti (6) Thenewbeamiscomputedusing
where i is the index of the selected grid cell, Q ∈ R2 is
i
the center of that cell, and O˜
ti
∈ R2 is the predicted off- B
t
=topK(cid:0) {P tk −1+log(C tk(i))+γ(i)|∀i∈G,k ∈[1,K]}(cid:1)
set for that cell at time t. For single-future prediction, we (10)
use greedy search, namely i = argmaxC over the belief
t whereγ(i)isadiversitypenaltyterm,andwetakethetop
state. For multi-future prediction, we use beam search in
K elements from the set produced by considering values
Section3.5.
withk =1:K. IfK =1,thisreducestogreedysearch.
This idea of combining classification and regression is
OncewehavecomputedthetopKfuturepredictions,we
partially inspired by object detection methods (e.g., [41]).
addthecorrespondingoffsetvectorstogetKtrajectoriesby
It is worth noting that in concurrent work, [6] also de- Lk ∈R2. Thisconstitutesthefinaloutputofourmodel.
signed a two-stage model for trajectory forecasting. How- t
ever, their classification targets are pre-defined anchor tra-
4.TheForkingPathsDataset
jectories. Oursisnotlimitedbythepredefinedanchors.
Inthissection, wedescribeourhuman-annotatedsimu-
3.4.Training
lationdataset,calledForkingPaths,formulti-futuretrajec-
Ourmodeltrainsontheobservedtrajectoryfromtime1 toryevaluation.
tohandpredictsthefuturetrajectories(inxy-coordinates)
Existing datasets. There are several real-world datasets
fromtimeh+1toT. Wesupervisethistrainingbyprovid-
fortrajectoryevaluation,suchasSDD[46],ETH/UCY[37,
inggroundtruthtargetsforboththeheatmap(beliefstate),
24], KITTI [13], nuScenes [5] and VIRAT/ActEV [3, 36].
C∗, and regression offset map, O∗. In particular, for the
t t However, they all share the fundamental problem that one
coarsedecoder,thecross-entropylossisused:
can only observe one out of many possible future trajec-
1 (cid:88)T (cid:88) tories sampled from the underlying distribution. This is
L =− C∗ log(C ) (7)
cls T ti ti broadlyacknowledgedinpriorworks[34,54,6,15,44,43]
t=h+1i∈G buthasnotyetbeenaddressed.
For the fine decoder, we use the smoothed L loss used in
1 The closest work to ours is the simulation used in [34,
objectdetection[41]:
54, 6]. However, these only contain artificial trajectories,
1 (cid:88)T (cid:88) nothumangeneratedones. Also,theyuseahighlysimpli-
L = smooth (O∗,O ) (8)
reg T L1 ti ti fied2Dspace,withpedestriansoversimplifiedaspointsand
t=h+1i∈G vehiclesasblocks;nootherscenesemanticsareprovided.
whereO∗ =L∗−Q isthedeltabetweenthetruelocation
ti t i Reconstructingrealityinsimulator. Inthiswork,weuse
and the center of the grid cell at i and L∗ is the ground
t CARLA [11], a near-realistic open source simulator built
truthforL inEq.(6). Weimposethislossoneverycellto
t on top of the Unreal Engine 4. Following prior simula-
improvetherobustness.
tion datasets [12, 47], we semi-automatically reconstruct
Thefinallossisthencalculatedusing
static scenes and their dynamic elements from the real-
L(θ)=L cls+λ 1L reg+λ 2(cid:107)θ(cid:107)2 2 (9) world videos in ETH/UCY and VIRAT/ActEV. There are
whereλ controlsthe(cid:96) regularization(weightdecay),and 4scenesinETH/UCYand5inVIRAT/ActEV.Weexclude
2 2
λ =0.1isusedtobalancetheregressionandclassification 2clutteredscenes(UNIV&0002)thatwearenotabletore-
1
losses. constructinCARLA,leaving7staticscenesinourdataset.
Figure 3: Visualization of the Forking Paths dataset. On the left is examples of the real videos and the second column
showsthereconstructedscenes. Thepersonintheblueboundingboxisthecontrolledagentandmultiplefuturetrajectories
annotatedbyhumansareshownbyoverlaidpersonframes.Theredcirclesarethedefineddestinations.Thegreentrajectories
arefuturetrajectoriesofthereconstructeduncontrolledagents. Thescenesemanticsegmentationgroundtruthisshownin
thethirdcolumnandthelastcolumnshowsallfourcameraviewsincludingthetop-downview.
For dynamic movement of vehicle and pedestrian, we forprediction. (Weuse10.4secondsforthefuturetoallow
firstconvertthegroundtruthtrajectoryannotationsfromthe ustoevaluatelongertermforecasts.)
real-world videos to the ground plane using the provided Generating the data. Once we have collected human-
homographymatrices.Wethenmatchthereal-worldtrajec- generated trajectories, 750 in total after data cleaning, we
tories’origintocorrectlocationsinthere-createdscenes. rendereachoneinfourcameraviews(three45-degreeand
Humangenerationofplausiblefutures. Wemanuallyse- one top-down view). Each camera view has 127 scenarios
lectsequenceswithmorethanonepedestrian. Wealsore- intotalandeachscenariohasonaverage5.9futuretrajecto-
quirethatatleastonepedestriancouldhavemultipleplau- ries. WithCARLA,wecanalsosimulatedifferentweather
sible alternative destinations. We insert plausible pedestri- conditions, although we did not do so in this work. In ad-
ansintothescenetoincreasethediversityofthescenarios. dition to agent location, we collect ground truth for pixel-
Wethenselectoneofthepedestrianstobethe“controlled precise scene semantic segmentation from 13 classes in-
agent”(CA)foreachsequence,andsetmeaningfuldestina- cludingsidewalk,road,vehicle,pedestrian,etc. SeeFig.3.
tions within reach, like a car or an entrance of a building.
Onaverage,eachagenthasabout3destinationstomoveto- 5.Experimentalresults
wards. In total, we have 127 CAs from 7 scenes. We call
This section evaluates various methods, including our
eachCAandtheircorrespondingsceneascenario.
Multiversemodel, formulti-futuretrajectorypredictionon
For each scenario, there are on average 5.9 human an-
the proposed Forking Paths dataset. To allow comparison
notators to control the agent to the defined destinations.
with previous works, we also evaluate our model on the
Specifically, they are asked to watch the first 5 seconds of
challenging VIRAT/ActEV [3, 36] benchmark for single-
video,fromafirst-personview(withthecameraslightlybe-
futurepathprediction.
hindthepedestrian)and/oranoverheadview(togivemore
context). They are then asked to control the motion of the 5.1.EvaluationMetrics
agentsothatitmovestowardsthespecifieddestinationina
Single-FutureEvaluation. Inreal-worldvideos,eachtra-
“natural”way,e.g.,withoutcollidingwithothermovingob-
jectory only has one sample of the future, so models are
jects(whosemotionisderivedfromtherealvideos,andis
evaluated on how well they predict that single trajectory.
thereforeunawareofthecontrolledagent). Theannotation
Following prior work [30, 1, 15, 49, 23, 18, 6, 44], we in-
isconsideredsuccessfuliftheagentreachedthedestination
troducetwostandardmetricsforthissetting.
withoutcollidingwithinthetimelimitof10.4seconds. All
LetYi = Yi bethegroundtruthtrajectoryof
finaltrajectoriesinourdatasetareexaminedbyhumansto t=(h+1)···T
ensurereliability. thei-thsample,andYˆibethecorrespondingprediction.We
Notethatourvideosareupto15.2secondslong. Thisis then employ two distance-based error metrics: i) Average
slightlylongerthanpreviousworks(e.g.[1,15,30,49,26, DisplacementError(ADE):theaverageEuclideandistance
62,64])thatuse3.2secondsofobservationand4.8seconds betweenthegroundtruthcoordinatesandthepredictionco-
ordinatesoveralltimeinstants: encoder-decodermodelwithcoordinatesinputonly. Social
(cid:80)N (cid:80)T (cid:107)Yi−Yˆi(cid:107) LSTM [1]: We use the open source implementation from
ADE= i=1 t=h+1 t t 2 (11)
N ×(T −h)
(https://github.com/agrimgupta92/sgan/). Next [30]
is the state-of-the-art method for single-future trajectory
ii)FinalDisplacementError(FDE):theEuclideandistance
prediction on the VIRAT/ActEV dataset. We train the
between the predicted points and the ground truth point at
Next model without the activity labels for fair compari-
thefinalpredictiontime:
(cid:80)N (cid:107)Yi −Yˆi(cid:107)
son using the code from (https://github.com/google/
FDE= i=1 T T 2 (12) next-prediction/). Social GAN [15] is a recent multi-
N future trajectory prediction model trained using Minimum
Multi-Future Evaluation. Let Yij = Yij be the overN(MoN)loss. Wetraintwomodelvariants(calledPV
t=(h+1)···T
j-thtruefuturetrajectoryforthei-thtestsample,for∀j ∈ andV)detailedinthepaperusingthecodefrom[15].
[1,J], and let Yˆik be the k’th sample from the predicted All models are trained on real videos (from VI-
distributionovertrajectories,fork ∈ [1,K]. Sincethereis RAT/ActEV – see Section 5.3 for details) and tested on
noagreed-uponevaluationmetricforthissetting,wesimply our synthetic videos (with CARLA-generated pixels, and
extendtheabovemetrics, asfollows: i)MinimumAverage annotator-generatedtrajectories). Mostmodelsjustusetra-
DisplacementErrorGivenKPredictions(minADE ):sim- jectorydataasinput,exceptforourmodel(whichusestra-
K
ilartothemetricdescribedin [6,43,44,15],foreachtrue jectory and semantic segmentation) and Next (which uses
trajectory j in test sample i, we select the closest overall trajectory,boundingbox,semanticsegmentation,andRGB
prediction (from the K model predictions), and then mea- frames).
sureitsaverageerror: ImplementationDetails. WeuseConvLSTM[58]cellfor
(cid:80)N (cid:80)J minK (cid:80)T (cid:107)Yij −Yˆik(cid:107) both the encoder and decoder. The embedding size is set
minADE = i=1 j=1 k=1 t=h+1 t t 2
K N ×(T −h)×J to32,andthehiddensizesfortheencoderanddecoderare
(13) both256. Thescenesemanticsegmentationfeaturesareex-
ii)MinimumFinalDisplacementErrorGivenKPredictions tractedfromthedeeplabmodel[8],pretrainedontheADE-
20k [65] dataset. We use Adadelta optimizer [61] with an
(minFDE ): similartominADE ,butweonlyconsiderthe
K K
initiallearningrateof0.3andweightdecayof0.001. Other
predictedpointsandthegroundtruthpointatthefinalpre-
hyper-parametersforthebaselinesarethesametotheones
dictiontimeinstant:
(cid:80)N (cid:80)J minK (cid:107)Yij −Yˆik(cid:107) in [15, 30]. We evaluate the top K = 20 predictions for
minFDE = i=1 j=1 k=1 T T 2 (14) multi-future trajectories. For the models that only output
K N ×J
asingletrajectory,includingLinear,LSTM,Social-LSTM,
iii)NegativeLog-Likelihood(NLL):SimilartoNLLmetrics
andNext,weduplicatetheoutputforK timesbeforeeval-
usedin[34,6],wemeasurethefitofground-truthsamples
uating. ForSocial-GAN,weuseK differentrandomnoise
tothepredicteddistribution.
inputstogetthepredictions. Forourmodel, weusediver-
sitybeamsearch[25,38]asdescribedinSection3.5.
5.2.Multi-FuturePredictiononForkingPaths
Quantitative Results. Table 1 lists the multi-future eval-
Dataset&Setups. TheproposedForkingPathsdatasetin uation results, where we divide the evaluation according
Section4isusedformulti-futuretrajectorypredictioneval- to the viewing angle of camera, 45-degree vs. top-down
uation. Followingthesettinginpreviousworks[30,1,15, view. We repeat all experiments (except “linear”) 5 times
1,15,49,34],wedownsamplethevideosto2.5fpsandex- withrandominitializationtoproducethemeanandstandard
tractpersontrajectoriesusingcodereleasedin[30],andlet deviation values. As we see, our model outperforms base-
themodelsobserve3.2seconds(8frames)ofthecontrolled lines in all metrics and it performs significantly better on
agent before outputting trajectory coordinates in the pixel theminADEmetric,whichsuggestsbetterpredictionqual-
space. Sincethelengthofthegroundtruthfuturetrajecto- ity over all time instants. Notably, our model outperforms
riesaredifferent,eachmodelneedstopredictthemaximum SocialGANbyalargemarginofatleast8pointsonallmet-
lengthattesttimebutweevaluatethepredictionsusingthe rics. Wealsomeasurethestandardnegativelog-likelihood
actuallengthofeachtruetrajectory. (NLL)metricforthetopmethodsinTable2.
Baseline methods. We compare our method with two Qualitativeanalysis. Wevisualizesomeoutputsofthetop
simple baselines, and three recent methods with released 4 methods in Fig. 4. In each image, the yellow trajecto-
source code, including a recent model for multi-future ries are the history trajectory of each controlled agent (de-
prediction and the state-of-the-art model for single-future rivedfromrealvideodata)andthegreentrajectoriesarethe
prediction: Linear is a single layer model that predicts groundtruthfuturetrajectoriesfromhumanannotators.The
the next coordinates using a linear regressor based on predictedtrajectoriesareshowninyellow-orangeheatmaps
the previous input point. LSTM is a simple LSTM [17] for multi-future prediction methods, and in red lines for
minADE minFDE
Method InputTypes 20 20
45-degree top-down 45-degree top-down
Linear Traj. 213.2 197.6 403.2 372.9
LSTM Traj. 201.0±2.2 183.7±2.1 381.5±3.2 355.0±3.6
Social-LSTM[1] Traj. 197.5±2.5 180.4±1.0 377.0±3.6 350.3±2.3
Social-GAN(PV)[15] Traj. 191.2±5.4 176.5±5.2 351.9±11.4 335.0±9.4
Social-GAN(V)[15] Traj. 187.1±4.7 172.7±3.9 342.1±10.2 326.7±7.7
Next[30] Traj.+Bbox+RGB+Seg. 186.6±2.7 166.9±2.2 360.0±7.2 326.6±5.0
Ours Traj.+Seg. 168.9±2.1 157.7±2.5 333.8±3.7 316.5±3.4
Table 1: Comparison of different methods on the Forking Paths dataset. Lower numbers are better. The numbers for the
columnlabeled“45degrees”areaveragedover3different45-degreeviews. Fortheinputtypes,“Traj.”,“RGB”,“Seg.” and
“Bbox.” meantheinputsarexy coordinates, rawframes, semanticsegmentationsandboundingboxesofallobjectsinthe
scene,respectively. AllmodelsaretrainedonrealVIRAT/ActEVvideosandtestedonsynthetic(CARLA-rendered)videos.
Method T pred =1 T pred =2 T pred =3 uationmetric.
(PV)[14] 10.08±0.25 17.28±0.42 23.34±0.47 Quantitative Results. Table 3 (first column) shows the
(V)[14] 9.95±0.35 17.38±0.49 23.24±0.54 evaluationresults. Aswesee,ourmodelachievesstate-of-
Next[27] 8.32±0.10 14.98±0.19 22.71±0.11 the-art performance. The improvement is especially large
Ours 2.22±0.54 4.46±1.33 8.14±2.81 on Final Displacement Error (FDE) metric, attributing to
Table 2: Negative Log-likelihood comparison of different the coarse location decoder that helps regulate the model
methods on the Forking Paths dataset. For methods that prediction for long-term prediction. The gain shows that
outputmultipletrajectories,wequantizethexy-coordinates ourmodeldoeswellatbothsinglefutureprediction(onreal
intothesamegridasourmethodandgetanormalizedprob- data)andmultiplefuturepredictiononourquasi-synthetic
abilitydistributionprediction. data.
Generalizing from simulation to real-world. As de-
single-futurepredictionmethods.Aswesee,ourmodelcor-
scribed in Section 4, we generate simulation data first by
rectly generally puts probability mass where there is data,
reconstructing from real-world videos. To verify the qual-
and does not “waste” probability mass where there is no
ity of the reconstructed data, and the efficacy of learning
data.
fromsimulationvideos,wetrainallthemodelsonthesim-
Error analysis. We show some typical errors our model
ulationvideosderivedfromtherealdata. Wethenevaluate
makes in Fig. 5. The first image shows our model misses
on the real test set of VIRAT/ActEV. As we see from the
thecorrectdirection,perhapsduetolackofdiversityinour
right column in Table 3, all models do worse in this sce-
sampling procedure. The second image shows our model
nario,duetothedifferencebetweensyntheticandrealdata.
sometimes predicts the person will “go through” the car
We find the performance ranking of different methods are
(diagonal red beam) instead of going around it. This may
consistentbetweentherealandoursimulationtrainingdata.
be addressed by adding more training examples of “going
Thissuggeststheerrorsmainlycomingfromthemodel,and
around” obstacles. The third image shows our model pre-
substantiatestherationalityofusingtheproposeddatasetto
dictsthepersonwillgotoamovingcar. Thisisduetothe
comparetherelativeperformanceofdifferentmethods.
lackofmodelingofthedynamicsofotherfar-awayagents
Therearetwosourcesoferror. Thesynthetictrajectory
in the scene. The fourth image shows a hard case where
dataonlycontainsabout60%oftherealtrajectorydata,due
thepersonjustexitsthevehicleandthereisnoindicationof
todifficultiesreconstructingalltherealdatainthesimula-
wheretheywillgonext(soourmodel“backsoff”toasen-
tor. In addition, the synthetic images are not photo realis-
sible“staynearby”prediction). Weleavesolutionstothese
tic. Thus methods (such as Next [30]) that rely on RGB
problemstofuturework.
inputobviouslysufferthemost,sincetheyhaveneverbeen
trained on “real pixels”. Our method, which uses trajecto-
5.3.Single-FuturePredictiononVIRAT/ActEV
riesplushighlevelsemanticsegmentations(whichtransfers
Dataset & Setups. NIST released VIRAT/ActEV [3] for fromsynthetictorealmoreeasily)sufferstheleastdropin
activity detection research in streaming videos in 2018. performance,showingitsrobustnessto“domainshift”. See
This dataset is a new version of the VIRAT [36] dataset, Table1forinputsourcecomparisonbetweenmethods.
with more videos and annotations. The length of videos
5.4.AblationExperiments
withpubliclyavailableannotationsisabout4.5hours. Fol-
lowing[30],weusetheofficialtrainingsetfortrainingand We test various ablations of our model on both the
the official validation set for testing. Other setups are the single-future and multi-future trajectory prediction to sub-
sameasinSection5.2,exceptweusethesingle-futureeval- stantiate our design decisions. Results are shown in Ta-
Figure4: Qualitativeanalysis. Theredtrajectoriesaresingle-futuremethodpredictionsandtheyellow-orangeheatmapsare
multi-future method predictions. The yellow trajectories are observations and the green ones are ground truth multi-future
trajectories. Seetextfordetails.
Method Single-Future Multi-Future
Ourfullmodel 18.51/35.84 166.1/329.5
Nospatialgraph 28.68/49.87 184.5/363.2
Nofinelocationdecoder 53.62/83.57 232.1/468.6
Nomulti-scalegrid 21.09/38.45 171.0/344.4
Table4: Performanceonablatedversionsofourmodelon
single and multi-future trajectory prediction. Lower num-
bersarebetter.
Multi-scalegrid:AsmentionedinSection3,weutilizetwo
differentgridscales(36×18)and(18×9)intraining. We
seethatperformanceisslightlyworseifweonlyusethefine
Figure5: Erroranalysis. Seetextfordetails.
scale(36×18).
Method TrainedonReal. TrainedonSim.
Linear 32.19/60.92 48.65/90.84 6.Conclusion
LSTM 23.98/44.97 28.45/53.01 In this paper, we have introduced the Forking Paths
Social-LSTM[1] 23.10/44.27 26.72/51.26 dataset,andtheMultiversemodelformulti-futureforecast-
Social-GAN(V)[15] 30.40/61.93 36.74/73.22
ing. Our study is the first to provide a quantitative bench-
Social-GAN(PV)[15] 30.42/60.70 36.48/72.72
mark and evaluation methodology for multi-future trajec-
Next [30] 19.78/42.43 27.38/62.11
tory prediction by using human annotators to create a va-
Ours 18.51/35.84 22.94/43.35
riety of trajectory continuations under the identical past.
Table 3: Comparison of different methods on the VI-
Ourmodelutilizesmulti-scalelocationdecoderswithgraph
RAT/ActEV dataset. We report ADE/FDE metrics. First
attention model to predict multiple future locations. We
columnisformodelstrainedonrealvideotrainingsetand
have shown that our method achieves state-of-the-art per-
second column is for models trained on the simulated ver-
formance on two challenging benchmarks: the large-scale
sionofthisdataset.
realvideodatasetandour proposedmulti-futuretrajectory
ble4,wheretheADE/FDEmetricsareshowninthe“single- dataset. We believe our dataset, together with our mod-
future” column and minADE /minFDE metrics (aver- els,willfacilitatefutureresearchandapplicationsonmulti-
20 20
aged across all views) in the “multi-future” column. We futureprediction.
verify three of our key designs by leaving the module out Acknowledgements This research was supported by
fromthefullmodel. NSF grant IIS-1650994, the financial assistance award
(1)SpatialGraph: Ourmodelisbuiltontopofaspatial 60NANB17D156fromNISTandaBaiduScholarship.This
2Dgraphthatusesgraphattentiontomodelthescenefea- work was also supported by IARPA via DOI/IBC contract
tures. Wetrainmodelwithoutthespatialgraph. Aswesee, number D17PC00340. The views and conclusions con-
theperformancedropsonbothtasks. (2)Finelocationde- tained herein are those of the authors and should not be
coder: Wetestourmodelwithoutthefinelocationdecoder interpreted as necessarily representing the official policies
andonlyusethegridcenterasthecoordinateoutput.Aswe or endorsements, either expressed or implied, of IARPA,
see, thesignificantperformancedropsonbothtasksverify NIST, DOI/IBC, the National Science Foundation, Baidu,
theefficacyofthisnewmoduleproposedinourstudy. (3) ortheU.S.Government.
References An open urban driving simulator. arXiv preprint
arXiv:1711.03938,2017. 1,2,4
[1] Alexandre Alahi, Kratarth Goel, Vignesh Ra-
[12] Adrien Gaidon, Qiao Wang, Yohann Cabon, and
manathan, Alexandre Robicquet, Li Fei-Fei, and Sil-
EleonoraVig.Virtualworldsasproxyformulti-object
vio Savarese. Social lstm: Human trajectory predic-
trackinganalysis. InCVPR,2016. 2,4
tionincrowdedspaces. InCVPR,2016. 1,2,5,6,7,
8 [13] Andreas Geiger, Philip Lenz, Christoph Stiller, and
[2] JavadAmirian,Jean-BernardHayet,andJulienPettre´. Raquel Urtasun. Vision meets robotics: The kitti
Social ways: Learning multi-modal distributions of dataset. The International Journal of Robotics Re-
pedestrian trajectories with gans. In CVPRW, 2019. search,32(11):1231–1237,2013. 4
2 [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
[3] George Awad, Asad Butt, Keith Curtis, Jonathan Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Fiscus, Afzal Godil, Alan F. Smeaton, Yvette Gra- Courville,andYoshuaBengio. Generativeadversarial
ham,WesselKraaij,GeorgesQunot,JoaoMagalhaes, nets. InNeurIPS,2014. 2
David Semedo, and Saverio Blasi. Trecvid 2018: [15] AgrimGupta,JustinJohnson,SilvioSavarese,LiFei-
Benchmarkingvideoactivitydetection,videocaption- Fei,andAlexandreAlahi.Socialgan:Sociallyaccept-
ingandmatching,videostorytellinglinkingandvideo abletrajectorieswithgenerativeadversarialnetworks.
search. InTRECVID,2018. 1,2,4,5,7 InCVPR,2018. 1,2,4,5,6,7,8
[4] MayankBansal,AlexKrizhevsky,andAbhijitOgale. [16] NicolasHeess,SrinivasanSriram,JayLemmon,Josh
Chauffeurnet: Learning to drive by imitating the Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu
best and synthesizing the worst. arXiv preprint Wang, SM Eslami, Martin Riedmiller, et al. Emer-
arXiv:1812.03079,2018. 1,2 genceoflocomotionbehavioursinrichenvironments.
[5] HolgerCaesar,VarunBankiti,AlexHLang,Sourabh arXivpreprintarXiv:1707.02286,2017. 2
Vora, Venice Erin Liong, Qiang Xu, Anush Krish-
[17] SeppHochreiterandJu¨rgenSchmidhuber.Longshort-
nan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
term memory. Neural computation, 9(8):1735–1780,
nuscenes: Amultimodaldatasetforautonomousdriv-
1997. 6
ing. arXivpreprintarXiv:1903.11027,2019. 2,4
[18] JoeyHong,BenjaminSapp,andJamesPhilbin. Rules
[6] Yuning Chai, Benjamin Sapp, Mayank Bansal, and
of the road: Predicting driving behavior with a con-
Dragomir Anguelov. Multipath: Multiple probabilis-
volutional model of semantic interactions. In CVPR,
tic anchor trajectory hypotheses for behavior predic-
2019. 2,5
tion. arXivpreprintarXiv:1910.05449,2019. 1,2,4,
[19] RE Kalman. A new approach to linear filtering and
5,6
prediction problems. Trans. ASME, D, 82:35–44,
[7] Ming-Fang Chang, John Lambert, Patsorn Sangk-
1960. 1
loy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett,
[20] Kris M Kitani, Brian D Ziebart, James Andrew Bag-
De Wang, Peter Carr, Simon Lucey, Deva Ramanan,
nell, and Martial Hebert. Activity forecasting. In
etal. Argoverse:3dtrackingandforecastingwithrich
ECCV,2012. 1,2
maps. InCVPR,2019. 2
[8] Liang-Chieh Chen, George Papandreou, Iasonas [21] Julian Francisco Pieter Kooij, Nicolas Schneider,
Kokkinos,KevinMurphy,andAlanLYuille.Deeplab: Fabian Flohr, and Dariu M Gavrila. Context-based
Semanticimagesegmentationwithdeepconvolutional pedestrianpathprediction. InECCV,2014. 2
nets, atrous convolution, and fully connected crfs. [22] SvetlanaLazebnik,CordeliaSchmid,andJeanPonce.
IEEE transactions on pattern analysis and machine Beyond bags of features: Spatial pyramid matching
intelligence,40(4):834–848,2017. 3,6 for recognizing natural scene categories. In CVPR,
[9] AbhishekDas,SamyakDatta,GeorgiaGkioxari,Ste- 2006. 3
fan Lee, Devi Parikh, and Dhruv Batra. Embodied [23] NamhoonLee, WongunChoi, PaulVernaza, Christo-
questionanswering. InCVPRW,2018. 2 pher B Choy, Philip HS Torr, and Manmohan Chan-
[10] Ce´sar Roberto de Souza, Adrien Gaidon, Yohann draker. Desire: Distant future prediction in dynamic
Cabon, and Antonio Manuel Lo´pez. Procedural gen- sceneswithinteractingagents. InCVPR,2017. 1,2,5
erationofvideostotraindeepactionrecognitionnet- [24] AlonLerner,YiorgosChrysanthou,andDaniLischin-
works. InCVPR,2017. 2 ski. Crowdsbyexample. InComputerGraphicsFo-
[11] Alexey Dosovitskiy, German Ros, Felipe Codev- rum, pages 655–664. Wiley Online Library, 2007. 2,
illa, Antonio Lopez, and Vladlen Koltun. Carla: 4
[25] Jiwei Li, Will Monroe, and Dan Jurafsky. A simple, [38] Tobias Plo¨tz and Stefan Roth. Neural nearest neigh-
fastdiversedecodingalgorithmforneuralgeneration. borsnetworks. InNeurIPS,2018. 6
arXivpreprintarXiv:1611.08562,2016. 4,6
[39] Weichao Qiu, Fangwei Zhong, Yi Zhang, Siyuan
[26] YukeLi.Whichwayareyougoing?imitativedecision Qiao, Zihao Xiao, Tae Soo Kim, and Yizhou Wang.
learning for path forecasting in dynamic scenes. In Unrealcv:Virtualworldsforcomputervision.InACM
CVPR,2019. 1,2,5 Multimedia,2017. 2
[27] Junwei Liang, Desai Fan, Han Lu, Poyao Huang, Jia [40] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
Chen,LuJiang,andAlexanderHauptmann. Anevent and Wojciech Zaremba. Sequence level training
reconstructiontoolforconflictmonitoringusingsocial with recurrent neural networks. arXiv preprint
media. InAAAI,2017. 2 arXiv:1511.06732,2015. 4
[28] Junwei Liang, Lu Jiang, Liangliang Cao, Yannis [41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Kalantidis, Li-Jia Li, and Alexander G Hauptmann. Sun. Fasterr-cnn: Towardsreal-timeobjectdetection
Focal visual-text attention for memex question an- withregionproposalnetworks. InNeurIPS,2015. 2,
swering. IEEE transactions on pattern analysis and 4
machineintelligence,41(8):1893–1908,2019. 2
[42] Nicholas Rhinehart and Kris M Kitani. First-person
[29] Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li,
activityforecastingwithonlineinversereinforcement
andAlexanderGHauptmann. Focalvisual-textatten-
learning. InICCV,2017. 1
tion for visual question answering. In CVPR, 2018.
[43] NicholasRhinehart,KrisMKitani,andPaulVernaza.
2
R2p2: A reparameterized pushforward policy for di-
[30] JunweiLiang,LuJiang,JuanCarlosNiebles,Alexan-
verse, precise generative path forecasting. In ECCV,
derGHauptmann,andLiFei-Fei.Peekingintothefu-
2018. 1,2,4,6
ture: Predictingfuturepersonactivitiesandlocations
[44] Nicholas Rhinehart, Rowan McAllister, Kris Kitani,
invideos. InCVPR,2019. 1,2,5,6,7,8
and Sergey Levine. Precog: Prediction conditioned
[31] Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming
ongoalsinvisualmulti-agentsettings. arXivpreprint
He, Bharath Hariharan, and Serge Belongie. Fea-
arXiv:1905.01296,2019. 2,4,5,6
turepyramidnetworksforobjectdetection. InCVPR,
2017. 3 [45] Stephan R Richter, Vibhav Vineet, Stefan Roth, and
VladlenKoltun. Playingfordata: Groundtruthfrom
[32] Matthias Luber, Johannes A Stork, Gian Diego
computergames. InECCV,2016. 2
Tipaldi, and Kai O Arras. People tracking with hu-
manmotionpredictionsfromsocialforces. InICRA, [46] Alexandre Robicquet, Amir Sadeghian, Alexandre
2010. 2 Alahi,andSilvioSavarese. Learningsocialetiquette:
Humantrajectoryunderstandingincrowdedscenes.In
[33] Wei-Chiu Ma, De-An Huang, Namhoon Lee, and
ECCV,2016. 4
Kris M Kitani. Forecasting interactive dynamics of
pedestrianswithfictitiousplay. InCVPR,2017. 2 [47] German Ros, Laura Sellart, Joanna Materzynska,
[34] OsamaMakansi,EddyIlg,OzgunCicek,andThomas David Vazquez, and Antonio M Lopez. The synthia
Brox. Overcominglimitationsofmixturedensitynet- dataset: Alargecollectionofsyntheticimagesforse-
works: A sampling and fitting framework for multi- manticsegmentationofurbanscenes. InCVPR,2016.
modalfutureprediction. InCVPR,2019. 1,2,4,6 2,4
[35] Huynh Manh and Gita Alaghband. Scene-lstm: A [48] Amir Sadeghian, Alexandre Alahi, and Silvio
modelforhumantrajectoryprediction. arXivpreprint Savarese. Trackingtheuntrackable: Learningtotrack
arXiv:1808.04018,2018. 2 multiplecueswithlong-termdependencies. InICCV,
2017. 1
[36] SangminOh,AnthonyHoogs,AmithaPerera,Naresh
Cuntoor, Chia-Chih Chen, Jong Taek Lee, Saurajit [49] AmirSadeghian,VineetKosaraju,AliSadeghian,No-
Mukherjee,JKAggarwal,HyungtaeLee,LarryDavis, riakiHirose,andSilvioSavarese.Sophie:Anattentive
etal.Alarge-scalebenchmarkdatasetforeventrecog- ganforpredictingpathscomplianttosocialandphys-
nitioninsurveillancevideo. InCVPR,2011. 1, 2, 4, ical constraints. arXiv preprint arXiv:1806.01482,
5,7 2018. 2,5,6
[37] Stefano Pellegrini, Andreas Ess, and Luc Van Gool. [50] Amir Sadeghian, Ferdinand Legros, Maxime Voisin,
Improving data association by joint modeling of Ricky Vesel, Alexandre Alahi, and Silvio Savarese.
pedestriantrajectoriesandgroupings.InECCV,2012. Car-net: Clairvoyant attentive recurrent network. In
4 ECCV,2018. 2
[51] Shital Shah, Debadeepta Dey, Chris Lovett, and [65] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler,
AshishKapoor.Airsim:High-fidelityvisualandphys- AdelaBarriuso,andAntonioTorralba. Sceneparsing
icalsimulationforautonomousvehicles. InFieldand throughade20kdataset. InCVPR,2017. 3,6
servicerobotics,pages621–635.Springer,2018. 2 [66] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J
[52] ChenSun,PerKarlsson,JiajunWu,JoshuaBTenen- Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi.
baum, and Kevin Murphy. Stochastic prediction Target-drivenvisualnavigationinindoorscenesusing
of multi-agent interactions from partial observations. deepreinforcementlearning. InICRA,2017. 2
arXivpreprintarXiv:1902.09641,2019. 2
[53] Yichuan Charlie Tang and Ruslan Salakhutdi-
nov. Multiple futures prediction. arXiv preprint
arXiv:1911.00997,2019. 1,2
[54] LucaAnthonyThiedeandPratikPrabhanjanBrahma.
Analyzing the variety loss in the context of prob-
abilistic trajectory prediction. arXiv preprint
arXiv:1907.10178,2019. 1,2,4
[55] Petar Velicˇkovic´, Guillem Cucurull, Arantxa
Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint
arXiv:1710.10903,2017. 3
[56] YunboWang,LuJiang,Ming-HsuanYang,Li-JiaLi,
Mingsheng Long, and Li Fei-Fei. Eidetic 3d lstm:
A model for video prediction and beyond. In ICLR,
2019. 3
[57] Yu Wu, Lu Jiang, and Yi Yang. Revisiting embod-
iedqa: Asimplebaselineandbeyond. arXivpreprint
arXiv:1904.04166,2019. 2
[58] SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan
Yeung,Wai-KinWong,andWang-chunWoo. Convo-
lutional lstm network: A machine learning approach
forprecipitationnowcasting. InNeurIPS,2015. 2,3,
6
[59] Hao Xue, Du Q Huynh, and Mark Reynolds. Ss-
lstm: Ahierarchicallstmmodelforpedestriantrajec-
toryprediction. InWACV,2018. 2
[60] Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani,
and Yoichi Sato. Future person localization in first-
personvideos. InCVPR,2018. 2
[61] MatthewDZeiler. Adadelta:anadaptivelearningrate
method. arXivpreprintarXiv:1212.5701,2012. 6
[62] PuZhang,WanliOuyang,PengfeiZhang,JianruXue,
andNanningZheng.Sr-lstm:Staterefinementforlstm
towards pedestrian trajectory prediction. In CVPR,
2019. 2,5
[63] Yiwei Zhang, Graham M Gibson, Rebecca Hay,
RichardWBowman,MilesJPadgett,andMatthewP
Edgar.Afast3dreconstructionsystemwithalow-cost
cameraaccessory. Scientificreports,5:10909,2015. 2
[64] TianyangZhao, YifeiXu, MathewMonfort, Wongun
Choi, Chris Baker, Yibiao Zhao, Yizhou Wang, and
YingNianWu. Multi-agenttensorfusionforcontex-
tualtrajectoryprediction. InCVPR,2019. 2,5

Searching for Effective Multilingual Fine-Tuning Methods:
A Case Study in Summarization
YiweiQin♠ GrahamNeubig♠♣ PengfeiLiu♠♣∗
♠CarnegieMellonUniversity,♣InspiredCognition
{yiweiq,gneubig,pliu3}@cs.cmu.edu
Abstract
Recently, a large number of tuning strategies
have been proposed to adapt pre-trained lan-
guagemodelstodownstreamtasks. Inthispa-
per, we perform an extensive empirical evalu-
ation of various tuning strategies for multilin- (a) (b) (c)
guallearning,particularlyinthecontextoftext
Figure1: Differentframeworksformultilinguallearn-
summarization. Specifically, we explore the
ing,whereorangecirclesrepresentdifferentlanguages
relative advantages of three families of multi-
and blue circles denote pre-trained language models
lingualtuningstrategies(atotaloffivemodels)
(PLMs). Redboxesrefertoadditionallearnableparam-
andempiricallyevaluatethemforsummariza-
eters,suchasadaptersorprefixes.Doublesidedarrows
tion over 45 languages. Experimentally, we
representthattheparametersofPLMsaretunable.
not only established a new state-of-the-art on
theXL-Sumdatasetbutalsoderiveaseriesof
observations that hopefully can provide hints
Atthesametime,therehasbeenmuchprogress
for future research on the design of multilin-
gualtuningstrategies.1 inmultilingualmodelsbasedonpre-trainedLMs
(LampleandConneau,2019;Conneauetal.,2020;
1 Introduction
Liuetal.,2020). However,thereisanotablegap
Methodsthatperformfine-tuningofpre-trainedlan- in the literature – to our knowledge, there are no
guagemodels(PLMs)nowrepresentthestate-of- comprehensivecomparativestudiesonhowdiffer-
the-artacrossawidevarietyofNLPtasks(Howard ent tuning strategies behave in multi-lingual sce-
andRuder,2018;Hanetal.,2021). Becausethere narios – there is significant work on multilingual
are a myriad of methods for tackling this impor- adapters(Pfeifferetal.,2020b;Anselletal.,2021)
tant task of fine-tuning LMs, there is an increas- andparametertyingacrosslanguages(Sachanand
ing body of research investigating the empirical Neubig, 2018; Lin et al., 2021), but few studies
strengthsandweaknessesofdifferenttuningstrate- comparingdifferentfamiliesofmethods.
gies across several tasks (Peters et al., 2019; Ma- Inthispaper,wetrytofillthisgapbyperform-
habadietal.,2021;KarimiMahabadietal.,2021; ingacomprehensivestudyofdifferentparameter
Li and Liang, 2021; Mao et al., 2021; Hu et al., tuning techniques in the context of text summa-
2021;Minetal.,2021;Heetal.,2021). Oneofthe rization(Rushetal.,2015;Nallapatietal.,2016;
majordesigndimensionsoftheseworksrevolves Chopraetal.,2016;Lewisetal.,2020;Zhangetal.,
aroundwhichsetofmodelparametersareupdated; 2020; Dou et al., 2021). We focus particularly
shouldfine-tuningonlyadjustafewadditionalpa- onsummarizationaspreviousworkonparameter-
rameters that are not part of the initial LMs (e.g., efficient tuning has noted that the differences be-
Adapters(Houlsbyetal.,2019),orPrefixTuning tweentuningtechniquesareparticularlysalientin
(Li and Liang, 2021; Xue et al., 2021)), update morecomplexgenerativetaskssuchassummariza-
allparametersofthepre-trainedmodels(Daiand tion, as opposed to text classification (He et al.,
Le,2015;Devlinetal.,2019;SchickandSchu¨tze, 2021). We draw on the techniques examined in
2021;Fu etal., 2022), or updateonly asubset of themonolingualscenarioandcombinetheunique
parameters(Guoetal.,2020)? characteristics of the multilingual scenario (e.g.,
∗Correspondingauthor sharedfeaturesacrosslanguages)toderivediffer-
1Codeathttps://github.com/qinyiwei/Multi-Sum.git entarchitecturesformultilinguallearning. These
2202
ceD
21
]LC.sc[
1v04750.2122:viXra
frameworks encompass some existing works on reduce the negative effect of the limited capacity
multilinguallearning,butalsoallowustopropose ofoneLMsharedbyalllanguages. Weachievea
new learning methods and perform comparisons newstate-of-the-artperformancewithsuchamulti-
betweendifferentframeworks. lingualtuningstrategy.
Figure 1 (a) shows a commonly-used frame-
2 Preliminaries
work (Hasan et al., 2021) in which one tunable
pre-trainedmodelissharedbydifferentlanguages. 2.1 TaskFormulation
Figure 1 (b) introduces a language-specific mod-
Abstractive summarization can be formulated as
ule with learnable parameters while keeping the
a conditional generation task where the input D
PLM’s parameter frozen, i.e. parameter-efficient
is a document, and the output S is a short sum-
tuning(Maoetal.,2021;Heetal.,2021). Inprac-
mary. The majority of state-of-the-art models
tice,thelanguage-specificmodulecouldbeinstan-
forabstractivesummarizationuseencoder-decoder
tiatedasanadapter,prefix,orothervarietyofextra
models(Sutskeveretal.,2014),whereanencoder
parameters. Notably,thiskindoflanguage-specific
generatesrepresentationsforthesourcedocument
moduleisindependentforeachlanguageandcan-
D = [d ,...,d ],andadecoderoutputsthesum-
notshareinformation,thuslow-resourcelanguages 1 m
mary S = [s ,...,s ] one target token at a time.
cannot benefit from other related languages. Fig- 1 n
The conditional probability of a single sample is
ure1(c)triestoalleviatethisproblemintwoways:
modeledasp(si|di;θ),andhenceparametersθare
makingparametersofpre-trainedmodelstunable
obtainedbymaximumlikelihoodestimation
orintroducingadditionalmoduleswhoseparame-
terscanbesharedbydifferentlanguages. (cid:88)
argmax logp(si|di;θ), (1)
Usingthisframework,weaskthesequestions: θ
(di,si)∈(D,S)
Q1: How well do different parameter-efficient
tuning methods (Figure 1-b) perform compared where (D,S) is the parallel training corpus. For
to PLM fine-tuning models (Figure 1-a)in multi- multilingualtextsummarization,D andS canbe
lingualsummarization? Q2: Willsupervisedtrans- inanyofanumberoflanguages.
fer, a commonly used technique in multi-lingual
2.2 TuningStrategy
learning,behelpfulforparameter-efficienttuning?
Q3: Couldbetterresultsbeachievedbyenablingin- Recently, applying pre-trained language mod-
formationexchangebetweendifferentlanguages? els (PLMs) to abstractive summarization tasks
How do different choices of parameter-efficient equipped with diverse tuning strategies has
tuningmethodsinteractwiththissharing? achievedagreatsuccess,whichcanbeformulated
asbelow:
Weexplorethesequestionsbyperformingexten-
siveexperimentsover45differentlanguages. Our
h = PLM(D,s ,h ;θ ,θ ) (2)
quantitative and qualitative analyses find several i i <i plm add
observations,suchas:
wherePLMisasequencetosequencepre-traiend
(1)ComparedtoPLMfine-tuning,bothparameter-
LMs(e.g.,T5(Raffeletal.,2019)orBART(Lewis
efficient tuning methods (prefix- and adapter-
etal.,2020)),θ representstheoriginalPLMpa-
plm
tuning) are advantageous in low-resource scenar-
rameterandθ denotestheadditionalparameters
add
ios. Particularly,prefix-tuningoutperformsadapter-
addedbydifferenttuningstrategies.
tuningwithextremelyfewsamplesoverdifferent
Basedonwhetherandwhenparametersθ and
plm
languages §3.1. (2) Parameter-efficient tuning is
θ will be tuned, different tuning strategies as
add
possible to fail in the supervised transfer setting
illustratedinFig.2havebeenexplored,whichwe
(§3.2),wherepre-trainedlanguagemodelsarefine-
willdetailbelowforthebetterintroductionofmulti-
tuned on the source languages whose scripts are
lingualtuningstrategies.
distantfromthetargetlanguage’s. (3)Addinglan-
guagespecificadaptersorprefixeswhileaddition- PLMFine-tuning Thisisoneofthemostcom-
ally tuning the PLM’s parameters, can maintain montuningstrategiesthataimtotuneallofthepa-
multi-lingualPLMfine-tuning’sadvantageofshar- rametersθ . WhilePLMfine-tuninghasachieved
plm
inginformationamonglanguages,aswellaspre- strong performance on many benchmarks, one
serving private parameters for each language to major limitation lies in the requirement of large
trainingsamples,whichisnotfeasibleinthelow- trainingsamplesfromdifferentlanguagesarepro-
resourcescenario. vided. Summarizationsystemsshareonemultilin-
To alleviate this issue, parameter-efficient tun- gualpre-trainedlanguagemodelwhoseparameters
inghasbeenextensivelyexploredrecently,among canbeupdatedbyanysystem.
whichweselecttworepresentativemethodswhich
MultilingualParameter-efficientTuning(MPE)
areinitiallydesignedforgenerationtasks,consis-
Inthisframework,additionalprivateparameterized
tentwithourgoal.
modulessuchasprefixoradapterareintroducedfor
Adapter-tuning Adapter-tuningaddsadditional each system besides one shared multilingual pre-
lightweightlayersbetweenthelayersofanexisting trained language model, whose parameters keep
PLM.Althoughthereisavarietyofwaystodefine frozen. Someexistingworks(Bapnaetal.,2019)
“adapter”,weadoptthedefinitionof(Bapnaetal., followthisframeworkbutmainlyfocusontheuse
2019). Specifically, the adapter block consists of ofadapters.
(1)alayernormalizationLN(·)fortheinputofthe
MultilingualPrivate-sharedTuning(MPS) In
adapters, (2) an autoencoder whose inner dimen-
the above method, although systems of different
sion can be adjusted according to the complexity
languages share one pre-training model, their pa-
ofthetargettaskwithadownprojectionlayer,an
rameterscannotbemodified,whichresultsinthe
upprojectionlayer,andanonlinearactivationfunc-
lack of information interaction across languages
tionbetweenthem,and(3)aresidualconnection.
andthedifficultyinminingthesharedknowledge.
Formally,givenh ∈ Rd betheoutputofi-thlayer,
i
Inthisframework,parametersfrombothadditional
theadapterisformulated:
modulesandpre-trainedmodelscanbeupdated.
ADAPTER(h i) = (ReLU(LN(h i)W idb))W ibd+h i,
3 Experiments
wherebistheinnerdimension,Wdb ∈ Rd×bisthe
i Dataset As our evaluation testbed, we use the
weightofdownprojectionlayerandWbd ∈ Rb×d
i XL-Sum corpus (Hasan et al., 2021),2 which
istheweightofupprojectionlayer.
is a news dataset containing 1.1 million article-
Prefix-tuning Prefix-tuning(LiandLiang,2021) summary pairs in 45 languages. The dataset is
prepends a prefix for every layer of a LM. Let collected from the British Broadcasting Corpora-
HLM ∈ Rt×d, where d is the hidden dimension tion(BBC)website,usingaboldparagraphatthe
i
of LM, t is the input sequence length, denote beginningofeacharticleasthesummaryandthe
the hidden representation of the i-th layer. We restofthearticleastheinputtext. WechooseXL-
prepend prefixes at each layer to obtain H = sumforits: (1)highlanguagecoverage,including
i
[Prefix ;HLM] ∈ R(t+l)×d, where l is the prefix low-resource,medium-resource,andhigh-resource
i i
length,Prefix ∈ Rl×d isprependedprefix. languages,(2)similarintrinsiccharacteristics,e.g.
i
WecanlookuptrainablematrixP ∈ Rl×(d×n), noveln-gramratio,abstractivity,andcompression
θ
where n is the number of layers of the LM, to among all samples, allowing our analysis to fo-
getPrefix . However,accordingto(LiandLiang, cusonthedifferencesacrosslanguages,otherthan
i
2021),reparameterizationhasbetterperformance differentintrinsicfeaturesacrosssamples.
than directly updating P in practice. So we
θ
EvaluationMetric Asisstandardinsummariza-
reparametrizethematrixP = MLP(P(cid:48)),where
θ θ tion,weuseROUGE(Lin,2004)asourevaluation
MLP(·)hasthestructureofanautoencoderwitha
metric,whichcomputesthen-gramsimilaritybe-
tunablemiddledimensionsize,andP(cid:48) isasmaller
θ tweenthegoldandthegeneratedsummary.3
matrixwithdimensionl×d.
3.1 Exp-I:EffectofTuningMethod
2.3 MultilingualTuningMethods
To answer the question of how well different
Based on the above-mentioned tuning strategies
parameter-efficient tuning methods behave com-
insinglelanguagescenarios,weinvestigatethree
pared to standard LM fine-tuning in the multilin-
differentmultilinguallearningframeworksandex-
gual setting (Q1), we study the performance of
ploretheirapplicablescenariosindetail.
2License:CCBY-NC-SA4.0.
Multilingual PLM Fine-tuning (MPF) This is
3WeusetherougepackageprovidedbyXL-Sum(Hasan
acommonly-usedsetting(Hasanetal.,2021)when etal.,2021)tosupportmultiplelanguages.
(a)PLMFine-tuning (b)Adapter-tuning (c)Prefix-tuning
Figure2: Differenttuningmethods. Redindicatestrainableparameters;blueindicatesfrozenparameters.
threetuningmethods: prefix-tuning,adapter-tuning on the full test set of the chosen language. The
andPLMfine-tuningondifferentlanguages. performanceoffewshotexperimentsisinfluenced
bythetrainingsampleschosen(Zhaoetal.,2021),
3.1.1 ExperimentDetails
so we keep the sampled training set and develop-
Settings and Hyper-parameters We use the mentsetthesameforthethreetuningmethodsto
baseversionofmultilingualT5(Xueetal.,2021) have a fair comparison. For non-few shot experi-
as a backbone, which covers most languages ments,eachsizehasoneexperimentandistested
in XL-Sum dataset and is the same as (Hasan on the full test set of the chosen language. The
et al., 2021), allowing us to make a fair com- hyperparametersarechosenfromasinglelanguage
parison. In our experiment, MLP of prefix- (Japanese)foreachtuningmethodandappliedas-is
tuning is two linear layers with an inner dimen- toalllanguages. Weusetheresultfromthecheck-
sion as a hyper-parameter. For prefix-tuning, the pointwiththebestvalidationsetperformanceover
hyper-parameters we tune4 are the same as Li alltrainingepochs.
and Liang (2021). For adapter-tuning, the hyper-
3.1.2 ResultsandAnalysis
parametersremainthesameasprefix-tuningexcept
the prefix length that is not needed for adapter- Results Fig.3illustratestheperformanceofthree
tuning. More details are in the appendix. For differenttuningmethodswithrespecttotheavail-
bothadapter-tuningandprefix-tuning,thesehyper- abletrainingsamples,observationsare:
parameters lead to about 8% additional parame- (1)Ingeneralwhenthesamplenumberislessthan
terscomparedtotheLM’stotalparameters,which 200 prefix-tuning achieves the best performance.
aretunedduringtrainingwhiletheLM’sparame- Between 200 and 10k adapter-tuning is superior,
ters are frozen. To study whether language fea- andgreaterthan10kPLMfine-tuningsurpassesthe
tures will influence the choice of tuning meth- other two. This indicates that regarding both the
ods,wechoosefivelanguagesfromdifferentlan- performanceandparameterefficiency(onlytuning
guagefamilies: English(Germanic),Chinese 8%oftheparametersofPLMfine-tuning),prefix-
simplified (Sino Tibetan), Spanish (Ro- tuningisthebestchoicewhenwehaveextremely
mance) , Ukrainian (Balto Slavic) and Urdu few samples, while adapter-tuning is the winner
(IndoIranian). in medium-resource settings. (2) As the training
Wesubsamplethefulldatasetofeachlanguage setsizeincreasesfromfewshottohigh-resource,
to obtain sub-datasets of various sizes,5 and sub- PLM fine-tuning has the largest performance im-
datasets of size ≤ 500 are considered “few-shot” provement,whileprefix-tuninghastheleastperfor-
experiments. For each few-shot experiment, we manceimprovementandadapter-tuningisthemid-
randomlysample3differenttrainingsetsanddevel- dle. (3)ComparedtoPLMfine-tuning,whichisal-
opmentset(withdevsize=20%trainingsetsize). mostmonotonicallyincreasingwiththetrainingset
Thereportedresultistheaverageof3experiments size,adapter-tuningandprefix-tuninghavefluctua-
tions. Frompreliminaryexperiments,wefindthat
4Specifically,thenumberofepochs,batchsize,learning
adapter-tuningandprefix-tuningaremoresensitive
rate,prefixlengthandinnerdimensionduringtraining,beam
searchsizeandlengthpenaltyduringinference. tolearningratethanPLMfine-tuning. Fixingtwo
5Concretely,{5,10,20,50,100,200,500,3000,6000, separate learning rates for few shot and non-few
10000, 20000, 30000}. For English, which has far more
shotexperimentsforalllanguages,asimplification
trainingsamplesthanotherlanguages,weaddtwotrainingset
size100000and300000. of the normal training process to find an optimal
(a)English (b)ChineseSimplified (c)Spanish
(d)Ukrainian (e)Urdu (f)Average
Figure 3: Performance of prefix-tuning, adapter-tuning and PLM fine-tuning on five languages over training set
sizes. Thexaxisisthenumberoftrainingsamplesatlogscale,theyaxisistheROUGE-2score.
learning rate for each training set size and each tuninghasbothadvantagesofutilizingtheknowl-
languageonthedevelopmentset,mightcausethe edgeofPLMbetterandfewerparameterstotune,
unstable performance of the tuning methods that whileadapter-tuningonlybenefitsfromthelatter.
are more sensitive to learning rate. (4) All above Thisremindsusthatwhiledesigningaprompt(Liu
observations are roughly true for every language et al., 2021), one important thing is to keep the
andareespeciallyclearintheaverageplot. PLM’sarchitectureandmakethepromptasnatural
aspossibletobetterextractknowledgefromPLM.
Discussion&Takeaways Thepossibleexplana-
3.2 Exp-II:EffectofSupervisedTransfer
tion of the different behavior of adapter-tuning
and prefix-tuning is their structural discrepancy, Whendesigningmodelsinmulti-lingualscenarios,
inwhichadapter-tuningaddsparametersbetween one crucial question is how to make modules of
two transformer layers, while prefix-tuning uses differentlanguagescommunicateefficientlysothat
theseparameterstogenerateprefixesandappends sharedknowledgecanbefullyutilized. Thereare
theprefixesatthefrontofeachtransformerlayer. twowaystodothis: (1)bytransferring: firstfine-
Similartolengtheningtheinput,prefix-tuningdoes tunePLMsonmultiplelanguagesexcepttheone
not touch the PLM’s architecture, preserving the we concerned about (a.k.a target language), and
knowledge in PLM. Thus, by utilizing the PLM thenadaptthefine-tunedmulti-lingualmodeltothe
better,prefix-tuninghasbetterperformanceatfew targetlanguage;(2)bymultitasklearning: jointly
shotswhentherearenotenoughsamplestolearn trainalllanguagestogether. Inthisexperiment,we
newpatterns. However,whenthetrainingsetsize studymethod1andtrytoanswertheQ2: willsu-
becomeslarger,moreflexiblestructuresareneeded pervisedtransferbehelpfulforparameter-efficient
to learn from these samples, leading to the bet- tuning. InExp.3.3,westudymethod2.
ter performance of adapter-tuning. The possible
reasonwhybothprefix-tuningandadapter-tuning 3.2.1 ExperimentDetails
outperformPLMfine-tuningattheleftsideisthat SettingsandHyper-parameters Wefirstdivide
prefix-tuningandadapterhaveonly8%parameters XL-Sum dataset into two parts, one of which in-
totune,whichavoidoverfittingwhenthetraining cluding34languages(75%oftotallanguages)is
samplesarenotenough. used to fine-tune PLM jointly on multiple lan-
From this experiment, we can see that prefix- guages to obtain a single multi-lingual model,
mt5-base mt5-base34
Language Script
prefix-tuing adapter-tuing prefix-tuing adapter-tuing
R1 R2 RL R1 R2 RL R1 R2 RL R1 R2 RL
amharic Ge’ez 15.33 5.42 13.8 16.58 5.88 14.88 16.97 5.88 15.12 17.85 6.2 16.01
azerbaijani Cyrillic 15.72 6.3 14.47 17.81 7.34 16.13 11.9 3.4 10.93 12.16 3.37 10.96
bengali Brahmic 23.76 9.11 20.66 25.99 10.07 22.21 0 0 0 0 0 0
burmese Brahmic 12.74 3.46 11.51 14.07 4.09 12.54 1.57 0.38 1.51 1.14 0.22 1.08
igbo Latin 23.27 5.25 17.36 25.22 7.08 19.38 28.1 7.98 21.19 28.33 7.93 21.72
japanese Kan,Hi,Kat 41.23 18.89 32.42 45.6 21.87 35.02 11.39 2.82 9.05 11.84 3.15 9.12
scottishgaelic Latin 24.05 7.73 19.45 20.42 4.31 17.02 24.36 7.16 18.87 26.37 8.09 20.22
spanish Latin 28.09 9.05 21.14 29.28 10.3 22.23 29.75 9.48 22.18 30.26 9.83 22.42
tamil Brahmic 16.45 6.58 15 19.81 8.83 18.12 0.42 0.02 0.42 0.44 0.02 0.43
ukrainian Cyrillic 18.73 7.07 16.43 21.84 8.92 19.08 16.46 4.41 13.99 16.96 4.61 14.32
urdu Arabic 35.05 14.5 28.76 38.81 17.69 32.08 20.19 4.48 15.66 19.57 4.58 15.08
Table 1: R1, R2, and RL scores of prefix-tuning, adapter-tuning of mt5-base and mt5-base34 for 11 languages.
“Kan,Hi,Kat”istheabbreviationofKanji,Hiragana,andKatakana.
while another part including 11 languages (25% Brahmicscript,indicatingtheimportantrolescript
of total languages) is used to investigate the fine- playstodeterminewhethersupervisedtransferis
tunedPLM’sabilitytogeneralizetonewlanguages. helpfulforparameter-efficienttuning.
The11left-outlanguagesthatdonotparticipatein
fine-tuningarechosenaccordingtheprinciplethat
Discussion & Takeaways Although intuitively
theyarefromdifferentlanguagefamilyandhave
differenttrainingsetsize.6 Thehyperparameters new languages will benefit from fine-tuning the
PLMontheXL-Sumdataset,thepracticalresults
usedtofine-tunePLMisthesameastheMultilin-
showthatnotalllanguagesobtainimprovements.
gualtrainingofXL-Sum(Hasanetal.,2021).
Transferlearninginsuchawaymightcausecatas-
Werefertomt5-baseastheoriginalPLMwithout
trophicforgettingofthepreviouslyacquiredknowl-
fine-tuningandmt5-base34asthefine-tunedver-
edgeinPLM(McCloskeyandCohen,1989;San-
sion on 34 languages. We then performed prefix-
toroetal.,2016). Iftherearenotenoughtraining
tuning and adapter-tuning on mt5-base and mt5-
samplesofacertainscriptduringPLMfine-tuning,
base34for11left-outlanguages. Hyperparameters
thePLMmightlosetheabilitygeneralizingtolan-
usedforthesetuningmethodsremainthesameas
guagesofthisscriptbyparameter-efficienttuning
thoseinnon-fewshotexperimentsofSec.(3.1).
methodsandfreezingPLM.Thisindicatesthatthe
3.2.2 ResultsandAnalysis effectivenessofparameter-efficienttuningmethods
Results Table.1 illustrates the performance of undermulti-lingualscenariosishighlydependent
multi-lingualmodelsmt5-baseandmt5-base34to onthemulti-lingualmodelweuseandundersome
adapt to 11 new languages by prefix-tuning and situations, parameter-efficient tuning might lose
adapter-tuning. ThemainobservationsinTable.1 their adaptivity. We leave how to alleviate this
areasfollows: problemforfuturework.
(1) For four languages, Amharic, Igbo, Scottish
Gaelic,andSpanish,mt5-base34willbringgains 3.3 Exp-III:JointMulti-LingualTraining
againsttheircounterpartsby0.3to6.0R1score.
In this experiment, we study joint multi-lingual
(2) Fine-tuning mt5-base on 34 languages of
training to see if different languages can benefit
XL-Sum dataset jeopardizes the performance of
from each other, and also how adding private pa-
sevenlanguages,amongwhichBengali,Burmese,
rameters for each language influences the perfor-
Tamil’sR1scorebecomesnearzero.
manceofmulti-lingualtraining(Q3).
(3)Threeofthefourlanguageswithperformance
improvement adapted from fine-tuned PLM are 3.3.1 ExperimentDetails
of the Latin script, while all three languages
WithrespecttoFig.1,wehave6differentsettings
with dramatic performance drop down are of the
to compare single language training with multi-
6Concretely,7languages(Amharic,Azerbaijani,Bengali, lingualtraining.
Burmese,Igbo,Japanese,ScottishGaelic,Spanish,Tami)are PLM Fine-tuning (PLF): mt5-base is fine-tuned
low-resource(<15,000trainingsamples),2languages(Span-
on all languages to obtain separate models for
ish,Tamil)aremedium-resource(15,000∼40,000)and2
languages(Ukrainian,Urdu)arehigh-resource(>40,000). each language as the baseline. Multi-lingual
Figure4: Trend-linesdepictingperformanceimprovement. X-axisisthelanguages,whicharearrangedinincreas-
ingorderofavailabletrainingdatafromlefttoright. Y-axisdepictstheR2scorerelativetothesingularlanguage
PLMfine-tuningbaseline.
Parameter-efficient (Adapter/Prefix) Tun- 3.3.2 ResultsandAnalysis
ing (MPE adapter/MPE prefix): We add
Thesummarizationperformanceondifferentlan-
adapters/prefixes (with parameters = 8% param-
guageswithdifferentsettingsisplottedinFigure4.
eters of the LM) for each language and tune
Withcombinationsofdifferentexperimentsettings,
adapters/prefixes separately for each language
wehavethefollowingresults:
while freezing mt5-base which is shared by all
PLF v.s. MPE prefix v.s. MPE adapter: PLF
languages. Multi-lingual PLM Fine-tuning
outperformsMPE prefixandMPE adapteroverall,
(MPF): A single model is trained with train-
with a gap larger for more available training
ing samples from multiple languages. The
samples. MPE adapter outperforms MPE prefix
training strategy proposed by (Lample and
foralmosteverylanguage,exceptafewlanguages
Conneau, 2019) to use a smoothing factor
with few samples. This conforms to the result in
(alpha) of 0.5 to balance the sampling rate of
Sec.3.1 that fine-tuning has the advantage with
low-resource languages and high-resource lan-
largetrainingsetsize,whileprefix-tuninghasthe
guagesisfollowedbyeverymultilingualsetting.7
advantagewithsmalltrainingsetsize. Onething
Multi-lingual Private-shared (Adapter/Prefix)
to notice is that adapter-tuning has comparable
Tuning (MPS adapter/MPS prefix): Private
orevenhigherperformancewhenthetrainingset
adapters/prefixes (with parameters = 2% pa-
sizeissmallerthan10k,consistentwiththeresult
rameters of LM) for each language (in total 45
inSec.3.1andadapter’ssensitivitytotrainingset
languages in XL-Sum dataset × 2% parameters
size is not as high as prefix-tuning. The latter
foreachlanguages=90%additionalparameters)
is reflected in that the adapter’s performance
areaddedtoasingleLM.TheLMistunedjointly
does not drop down dramatically as the training
formultiplelanguages,whiletheadapters/prefixes
set size increases and keeps within -2 R2 of
aretunedseparatelyforeachlanguage.8
the baseline for almost all languages, which is
even true for English with the highest training
7WeusetheresultsofXL-Sum(Hasanetal.,2021). In
set size of 300,000. This means the parameter
order to have a fair comparison and remove the influence
of different rouge packages, we use their model-generated efficientadapterisareasonablesubstituteofPLM
outputs on test set to calculate the rouge score, instead of
fine-tuningregardlessofthetrainingsetsize.
usingtheirreportedrougescoredirectly.
8Onethingtonoticeisthatduringtraining,ineachiteration
wesamplefromonelanguagewithacertainprobability.The MPF v.s. PLF: Multi-lingual model MPF
LMsharedbyalllanguagesistunedeveryiteration,whilethe
significantly outperforms the baseline PLF in
privateparametersforeachlanguagearetunedwheneverthe
languageissampled. the low- and medium-resource languages with
the gain decreasing as the training set size exemplified by mBERT (Devlin et al., 2019),
becomes larger. This is expected because low- XLM-R(Conneauetal.,2020),XLM-R(Conneau
and medium-resource languages can benefit etal.,2020),whichadoptmaskedlanguagemodel
from joint training by positive transfer between paradigm, and mBART (Liu et al., 2020), mT5
sisterlanguages(LampleandConneau,2019). A (Xue et al., 2021), which utilize a sequence-to-
deteriorationisalsoobservedinthehigh-resource sequenceframework. However,afewworkshave
languages. However, the losses are relatively focusedonmultilingualsummarizationgiventhe
minor; the multilingual model is within a -1 lackofbenchmarkdatasetsforotherlanguagesex-
R2 drop for 6 high-resource languages and a ceptEnglish. (Giannakopoulosetal.,2015)bench-
-3 R2 drop for English. This indicates that by markedsummarizationsystemsover40languages,
training a single multilingual model, the low- withlimitationofdatasetscalehaving10ksamples
and medium-resource languages have been intotal. (Scialometal.,2020)releasedthemultilin-
significantlyimprovedwithouttoomuchsacrifice gualsummarizationdatasetspanning5languages
inthehigh-resourcelanguages. Similartothelow- with1.5Marticle-summarypairs. (Caoetal.,2020)
resource experiment of (Hasan et al., 2021), our createdanewdatasetfortwolanguageswith400k
resultisstrongerthantheirs,whichonlyselects5 samples. (Hasanetal.,2021)introducedXL-Sum
low-resourcelanguagestofine-tuneindividualLM. spanning 45 languages containing 1.1M article-
summarypairs. Morerecently,(VarabandSchluter,
MPF v.s. MPS adapter v.s. MPS prefix: By 2021)releasedMassiveSummcontaining28.8mil-
addinglanguage-specificparametersinthemulti- lionarticlesacross92languages.
lingualscenario,comparedtoMPF,MPS adapter
andMPS prefixhaveperformanceimprovements
4.2 ParameterEfficientTuning
R1:0.73,R2:0.46,RL:0.45andR1:0.67,R2:0.47,
RL:0.46respectivelyfor45languagesonaverage. Parameter-efficient tuning methods only tune a
Fromeachlanguageperformance,wecanseethat small number parameters to achieve comparable
allhigh-resourcelanguageshaveperformanceim- results. It can be roughly divided into two cate-
provementsatthecostofjeopardizingtheperfor- gories,methodswithoutadditionalparametersand
mance of a few low-resource languages a little. methods with additional parameters. The former
ThisindicatesthatsharingLMaswellasaddingpri- tunepartofthepre-trainedLM.(Leeetal.,2019)
vatelanguage-specificparameterswillmaintainthe fine-tunes a few of the final layers, while (Min
jointlymulti-lingualtraining’sadvantageofshar- et al., 2021) only fine-tunes the bias terms of the
inginformationamonglanguages,whilereducing LM.Thelatterintroducesextraparameterswhile
theharmofsharingallparameterstohigh-resource fixingthepre-trainedLM.Popularmethodsinclude
languagesduetothelimitmodelcapacity. adapter-tuning(Houlsbyetal.,2019;Bapnaetal.,
Besides,thetwowaystoaddadditionalparameters: 2019;Pfeifferetal.,2020a)prefix-tuning(Liand
private adapter, private prefix for each language Liang,2021)prompt-tuning(Lesteretal.,2021),
haveroughlythesameoverallperformanceonthe andothers(Maoetal.,2021;Huetal.,2021;Guo
wholedatasetandthesametrendlinesdepictedin etal.,2020). Amongtheseworks,acomprehensive
Fig.4,despitetheirdifferenceswehavediscussed discussion in the context of multilingual summa-
inSec.3.1. Thepossibleexplanationisthatthedis- rizationisrelativelymissing.
advantageofprefix-tuninglackingtheflexibilityto
modifyfreezeLMaddressedinSec.3.1,isallevi-
5 Discussion
atedorremovedbytuningsharedLM.Bothprefix
andadapter’sadvantagecomesfromaddingprivate
parameters,sotheyhavesimilarbehavior. Inthispaper,weinvestigatetheapplicablescope
ofdifferentfamiliesoftuningstrategiesformulti-
4 RelatedWork linguallearning. Wespecificallyaskthreeresearch
questions, and by extensive experiments on sum-
4.1 MultilingualTasks
marization datasets with 45 languages we obtain
Withrapiddevelopmentofpre-trainedLMs,multi- diverseobservationswhich,hopefully,wouldpro-
lingualLMshaveemergedtoleveragethepower vide a useful instruction for future designing of
of pre-training on a large number of languages, multilingualtuningstrategies.
6 Limitations Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao
Jiang, and Graham Neubig. 2021. GSum: A gen-
One limitation of our work is that we only con- eralframeworkforguidedneuralabstractivesumma-
duct experiments on one summarization dataset, rization. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
and more other NLP tasks could be explored as
ComputationalLinguistics: HumanLanguageTech-
futurework. Anotherlimitationisthatweonlyuse
nologies,pages4830–4842,Online.Associationfor
ROUGE as our evaluation metric. More evalua- ComputationalLinguistics.
tionmetrics,likeBERTScore(Zhangetal.,2019),
JinlanFu,See-KiongNg,andPengfeiLiu.2022. Poly-
COMET (Rei et al., 2020) or manual evaluation
glot prompt: Multilingual multitask promptraining.
couldbeused.
arXivpreprintarXiv:2204.14264.
George Giannakopoulos, Jeff Kubina, John Conroy,
References Josef Steinberger, Benoit Favre, Mijail Kabadjov,
Udo Kruschwitz, and Massimo Poesio. 2015. Mul-
Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se- tiLing 2015: Multilingual summarization of single
bastianRuder, GoranGlavasˇ, IvanVulic´, andAnna and multi-documents, on-line fora, and call-center
Korhonen. 2021. MAD-G: Multilingual adapter conversations. In Proceedings of the 16th Annual
generation for efficient cross-lingual transfer. In MeetingoftheSpecialInterestGrouponDiscourse
Findings of the Association for Computational Lin- and Dialogue, pages 270–274, Prague, Czech Re-
guistics: EMNLP 2021, pages 4762–4781, Punta public.AssociationforComputationalLinguistics.
Cana,DominicanRepublic.AssociationforCompu-
tationalLinguistics. Demi Guo, Alexander M Rush, and Yoon Kim. 2020.
Parameter-efficient transfer learning with diff prun-
Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. ing. arXivpreprintarXiv:2012.07463.
2019. Simple, scalable adaptation for neural ma-
chinetranslation. arXivpreprintarXiv:1909.08478. Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu,
Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang,
YueCao,XiaojunWan,JingeYao,andDianYu.2020. WentaoHan,MinlieHuang,etal.2021. Pre-trained
Multisumm: Towards a unified model for multi- models: Past,presentandfuture. AIOpen.
lingualabstractivesummarization. InAAAIConfer-
enceonArtificialIntelligence. TahmidHasan,AbhikBhattacharjee,Md.SaifulIslam,
Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,
Sumit Chopra, Michael Auli, and Alexander M. Rush.
M. Sohel Rahman, and Rifat Shahriyar. 2021. XL-
2016. Abstractive sentence summarization with at-
sum: Large-scale multilingual abstractive summa-
tentiverecurrentneuralnetworks. InProceedingsof
rizationfor44languages. InFindingsoftheAssoci-
the 2016 Conference of the North American Chap-
ation for Computational Linguistics: ACL-IJCNLP
teroftheAssociationforComputationalLinguistics:
2021, pages 4693–4703, Online. Association for
Human Language Technologies, pages 93–98, San
ComputationalLinguistics.
Diego, California. Association for Computational
Linguistics. JunxianHe,ChuntingZhou,XuezheMa,TaylorBerg-
Kirkpatrick, andGrahamNeubig.2021. Towardsa
AlexisConneau, KartikayKhandelwal, NamanGoyal,
unifiedviewofparameter-efficienttransferlearning.
Vishrav Chaudhary, Guillaume Wenzek, Francisco
arXivpreprintarXiv:2110.04366.
Guzma´n, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
cross-lingual representation learning at scale. In Bruna Morrone, Quentin De Laroussilhe, Andrea
Proceedingsofthe58thAnnualMeetingoftheAsso- Gesmundo, Mona Attariyan, and Sylvain Gelly.
ciation for Computational Linguistics, pages 8440– 2019. Parameter-efficient transfer learning for nlp.
8451, Online. Association for Computational Lin- In International Conference on Machine Learning,
guistics. pages2790–2799.PMLR.
AndrewMDaiandQuocVLe.2015. Semi-supervised Jeremy Howard and Sebastian Ruder. 2018. Univer-
sequencelearning. Advancesinneuralinformation sallanguagemodelfine-tuningfortextclassification.
processingsystems,28:3079–3087. arXivpreprintarXiv:1801.06146.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Kristina Toutanova. 2019. BERT: Pre-training of Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
deep bidirectional transformers for language under- and Weizhu Chen. 2021. Lora: Low-rank adap-
standing. In Proceedings of the 2019 Conference tation of large language models. arXiv preprint
of the North American Chapter of the Association arXiv:2106.09685.
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa
pages4171–4186,Minneapolis,Minnesota.Associ- Dehghani, and James Henderson. 2021. Parameter-
ationforComputationalLinguistics. efficient multi-task fine-tuning for transformers via
shared hypernetworks. In Proceedings of the 59th Rabeeh Karimi Mahabadi, James Henderson, and Se-
Annual Meeting of the Association for Computa- bastian Ruder. 2021. Compacter: Efficient low-
tional Linguistics and the 11th International Joint rank hypercomplex adapter layers. arXiv preprint
Conference on Natural Language Processing (Vol- arXiv:2106.04647.
ume 1: Long Papers), pages 565–576, Online. As-
sociationforComputationalLinguistics. YuningMao,LambertMathias,RuiHou,AmjadAlma-
hairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Ma-
Guillaume Lample and Alexis Conneau. 2019. Cross- dian Khabsa. 2021. Unipelt: A unified frame-
lingual language model pretraining. arXiv preprint workforparameter-efficientlanguagemodeltuning.
arXiv:1901.07291. arXivpreprintarXiv:2110.07577.
Michael McCloskey and Neal J Cohen. 1989. Catas-
JaejunLee,RaphaelTang,andJimmyLin.2019. What
trophicinterferenceinconnectionistnetworks: The
would elsa do? freezing layers during transformer
sequentiallearningproblem. InPsychologyoflearn-
fine-tuning.
ing and motivation, volume 24, pages 109–165. El-
sevier.
BrianLester,RamiAl-Rfou,andNoahConstant.2021.
The power of scale for parameter-efficient prompt
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and
tuning. arXivpreprintarXiv:2104.08691.
Luke Zettlemoyer. 2021. Noisy channel language
model prompting for few-shot text classification.
Mike Lewis, Yinhan Liu, Naman Goyal, Mar- arXivpreprintarXiv:2108.04106.
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
2020. BART:Denoisingsequence-to-sequencepre- C¸ag˘lar Gulc¸ehre, and Bing Xiang. 2016. Abstrac-
trainingfornaturallanguagegeneration,translation, tivetextsummarizationusingsequence-to-sequence
andcomprehension. InProceedingsofthe58thAn- RNNs and beyond. In Proceedings of The 20th
nual Meeting of the Association for Computational SIGNLLConferenceonComputationalNaturalLan-
Linguistics, pages 7871–7880, Online. Association guage Learning, pages 280–290, Berlin, Germany.
forComputationalLinguistics. AssociationforComputationalLinguistics.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Matthew E. Peters, Sebastian Ruder, and Noah A.
Optimizing continuous prompts for generation. In Smith. 2019. To tune or not to tune? adapting pre-
Proceedings of the 59th Annual Meeting of the trainedrepresentationstodiversetasks. InProceed-
Association for Computational Linguistics and the ings of the 4th Workshop on Representation Learn-
11thInternationalJointConferenceonNaturalLan- ing for NLP (RepL4NLP-2019), pages 7–14, Flo-
guage Processing (Volume 1: Long Papers), pages rence,Italy.AssociationforComputationalLinguis-
4582–4597, Online. Association for Computational tics.
Linguistics.
Jonas Pfeiffer, Andreas Ru¨ckle´, Clifton Poth, Aish-
Chin-YewLin.2004. Rouge: Apackageforautomatic warya Kamath, Ivan Vulic´, Sebastian Ruder,
evaluation of summaries. In Text summarization Kyunghyun Cho, and Iryna Gurevych. 2020a.
branchesout,pages74–81. Adapterhub: A framework for adapting transform-
ers. arXivpreprintarXiv:2007.07779.
Zehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li.
Jonas Pfeiffer, Ivan Vulic´, Iryna Gurevych, and Se-
2021. Learning language specific sub-network for
bastianRuder.2020b. MAD-X:AnAdapter-Based
multilingualmachinetranslation. InProceedingsof
Framework for Multi-Task Cross-Lingual Transfer.
the59thAnnualMeetingoftheAssociationforCom-
InProceedingsofthe2020ConferenceonEmpirical
putational Linguistics and the 11th International
MethodsinNaturalLanguageProcessing(EMNLP),
Joint Conference on Natural Language Processing
pages7654–7673,Online.AssociationforComputa-
(Volume 1: Long Papers), pages 293–305, Online.
tionalLinguistics.
AssociationforComputationalLinguistics.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Hiroaki Hayashi, and Graham Neubig. 2021. Pre-
WeiLi, andPeterJLiu.2019. Exploringthelimits
train, prompt, and predict: A systematic survey of
of transfer learning with a unified text-to-text trans-
prompting methods in natural language processing.
former. arXivpreprintarXiv:1910.10683.
arXivpreprintarXiv:2107.13586.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey Lavie. 2020. Comet: A neural framework for mt
Edunov, Marjan Ghazvininejad, Mike Lewis, and evaluation. arXivpreprintarXiv:2009.09025.
Luke Zettlemoyer. 2020. Multilingual denoising
pre-trainingforneuralmachinetranslation. Transac- AlexanderM.Rush,SumitChopra,andJasonWeston.
tions of the Association for Computational Linguis- 2015. A neural attention model for abstractive sen-
tics,8:726–742. tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan- Joint Conference on Natural Language Processing
guageProcessing,pages379–389,Lisbon,Portugal. (Volume1: LongPapers),pages5751–5767.
AssociationforComputationalLinguistics.
DevendraSachanandGrahamNeubig.2018. Parame-
tersharingmethodsformultilingualself-attentional
translationmodels. InProceedingsoftheThirdCon-
ference on Machine Translation: Research Papers,
pages 261–271, Belgium, Brussels. Association for
ComputationalLinguistics.
Adam Santoro, Sergey Bartunov, Matthew Botvinick,
Daan Wierstra, and Timothy Lillicrap. 2016. One-
shot learning with memory-augmented neural net-
works. arXivpreprintarXiv:1605.06065.
Timo Schick and Hinrich Schu¨tze. 2021. Few-shot
text generation with natural language instructions.
In Proceedings of the 2021 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,pages
390–402, Online and Punta Cana, Dominican Re-
public.AssociationforComputationalLinguistics.
ThomasScialom,Paul-AlexisDray,SylvainLamprier,
Benjamin Piwowarski, and Jacopo Staiano. 2020.
Mlsum: The multilingual summarization corpus.
arXivpreprintarXiv:2004.14900.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequencetosequencelearningwithneuralnetworks.
In Advances in neural information processing sys-
tems,pages3104–3112.
Daniel Varab and Natalie Schluter. 2021. Mas-
sivesumm: a very large-scale, very multilingual,
news summarisation dataset. In Proceedings of the
2021 Conference on Empirical Methods in Natural
LanguageProcessing,pages10150–10161.
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2021. mT5: A massively
multilingualpre-trainedtext-to-texttransformer. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 483–498, Online. Association for Computa-
tionalLinguistics.
JingqingZhang,YaoZhao,MohammadSaleh,andPe-
ter Liu. 2020. Pegasus: Pre-training with extracted
gap-sentencesforabstractivesummarization. InIn-
ternationalConferenceonMachineLearning,pages
11328–11339.PMLR.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger,andYoavArtzi.2019. Bertscore: Eval-
uating text generation with bert. arXiv preprint
arXiv:1904.09675.
Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vulic´,
RoiReichart,AnnaKorhonen,andHinrichSchu¨tze.
2021. A closer look at few-shot crosslingual trans-
fer: Thechoiceofshotsmatters. InProceedingsof
the59thAnnualMeetingoftheAssociationforCom-
putational Linguistics and the 11th International
A SupplementaryMaterial
A.1 Hyper-parameters
InTable2,wereportthehyper-parametersusedin
Sec.3.1.
A.2 AdditionalResults
Table3supplementsFig.4inSec.3.3.
learningrate epoch batchsize gradacc prefixlength innerdim beamsize lengthpenalty
prefix-tuning
fewshotsetting 3.00e-4 20 2 1 20 800 4 0.6
notfewshotsetting,low-resource 2.00e-4 15 8 1 200 800 4 0.6
notfewshotsetting,notlow-resource 2.00e-4 15 16 4 200 800 4 0.6
adapter-tuning
fewshotsetting 1.00e-3 20 2 1 - 1200 4 0.6
notfewshotsetting,low-resource 1.00e-3 15 8 1 - 1200 4 0.6
notfewshotsetting,notlow-resource 1.00e-3 15 16 4 - 1200 4 0.6
PLMfine-tuning
fewshotsetting 5.00e-4 20 2 1 - - 4 0.6
notfewshotsetting,low-resource 5.00e-4 15 8 1 - - 4 0.6
notfewshotsetting,notlow-resource 5.00e-4 15 16 4 - - 4 0.6
Table 2: Hyper-parameter settings for Sec.3.1. Grad acc is the abbreviation of gradient accumulation size, inner
dimistheabbreviationofinnerdimension.
language PLF MPEadapter MPEprefix MPF MPSadapter MPSprefix
amharic 17.46/6.64/16 16.58/5.88/14.88 15.33/5.42/13.8 20.08/7.41/18.05 20.6/7.61/18.5 20.33/7.48/18.34
arabic 34.82/15.13/29.22 32.29/12.85/26.43 29.02/10.94/24.14 34.89/14.76/29.15 35.16/14.96/29.27 35.36/15.11/29.49
azerbaijani 16.93/7.04/15.36 17.81/7.34/16.13 15.72/6.3/14.47 21.4/9.55/19.37 21.77/9.85/19.62 21.58/9.83/19.76
bengali 25.9/9.73/22.12 25.99/10.07/22.21 23.76/9.11/20.66 29.46/12.02/25.1 29.11/11.61/24.64 29.57/11.88/24.89
burmese 14.35/4.51/13.08 14.07/4.09/12.54 12.74/3.46/11.51 16.17/5.17/14.42 15.84/4.7/14.08 16.12/5.07/14.39
chinesesimplified 40.87/25.97/34.09 39.81/24.98/33 36.3/22.34/30.22 43.8/28.76/36.9 44.95/29.66/37.79 44.37/29.1/37.18
chinesetraditional 40.04/25.11/33.31 39.16/24.17/32.3 35.99/21.79/29.7 43.21/28.03/36.17 44.26/28.75/37.01 43.83/28.33/36.66
english 40.67/18.04/32.72 38.96/16.52/31.06 34.6/12.93/27.38 37.61/15.15/29.88 38.29/15.71/30.41 38.29/15.63/30.39
french 32.02/13.47/25.29 31.85/13.34/25.13 30.63/12.97/24.49 35.33/16.19/28.2 36.06/16.22/28.4 35.69/16.15/28.16
gujarati 19.38/6.49/17.6 19.31/6.23/17.62 18.03/5.65/16.45 21.96/7.72/19.9 22.45/7.97/20.21 22.38/8.14/20.31
hausa 36.16/15.43/29.13 35.75/14.84/28.21 34.08/13.82/27.07 39.41/17.72/31.64 39.75/17.64/31.85 39.63/17.8/31.96
hindi 38.64/17.33/32.38 37.57/16.09/31.1 34.32/13.23/28.23 38.57/16.87/32.03 39.18/17.37/32.49 39.02/17.3/32.35
igbo 27.16/8.76/21.37 25.22/7.08/19.38 23.27/5.25/17.36 31.64/10.2/24.51 30.17/9.2/22.98 30.11/9.5/23.1
indonesian 35.69/16.23/29.92 34.86/15.51/28.85 30.96/12.87/25.72 37.01/17.02/30.75 37.83/17.55/31.37 37.69/17.57/31.46
japanese 45.86/22.01/35.39 45.6/21.87/35.02 41.23/18.89/32.42 48.08/23.8/37.32 48.65/24.26/37.33 48.41/24.16/37.39
kirundi 28.97/12.84/23.6 28.94/12.23/23.06 26.18/10.18/20.51 32/14.41/25.82 32.65/14.98/26.22 32.65/14.91/26.26
korean 19.04/9.42/18.06 19.92/9.93/18.59 18.07/8.95/17.08 23.7/11.49/22.34 23.57/11.31/21.78 23.11/11.27/21.6
kyrgyz 13.66/5.58/12.43 13.89/5.62/12.69 11.77/4.88/10.87 18.34/8.01/16.5 18.4/7.72/16.0 18.3/7.8/16.11
marathi 20.21/8.94/18.31 20.03/8.49/18.19 18.3/7.55/16.83 22.06/9.57/20.01 23.13/10.32/20.8 22.82/10.02/20.54
nepali 23.16/9.06/21.14 23.63/8.69/21.51 20.91/7.37/19.25 26.57/10.2/24.22 26.5/10.05/24.06 26.59/10.2/24.21
oromo 16.55/5.35/14.61 16.2/5.14/14.13 13.7/4.14/12 18.74/6.21/16.19 19.68/6.45/16.91 19.49/6.71/16.88
pashto 37.69/15.38/31.19 36.74/14.02/29.85 33.85/12.14/27.88 38.28/15.49/31.77 38.85/15.85/32.05 38.92/16.05/32.16
persian 37.08/16.78/30.44 35.72/15.34/28.81 32.98/12.89/26.27 35.71/15.06/29.1 37.25/16.58/30.42 37.32/16.43/30.3
pidgin 34.55/12.67/27.03 35.11/13.14/27.41 32.85/11.57/25.79 37.97/15.13/29.86 38.56/15.6/30.14 38.89/15.88/30.47
portuguese 37.04/16.25/28.82 36.3/15.18/27.7 32.99/12.57/25.07 37.15/15.89/28.53 37.71/16.33/28.97 37.59/16.21/28.91
punjabi 26.18/9.61/21.95 25.75/8.58/20.97 25.94/8.49/21.43 30.77/12.15/25.57 31.14/12.3/25.33 30.77/12.27/25.29
russian 32.18/13.83/26.11 30.56/12.67/24.73 26.5/9.79/21.44 32.21/13.64/26.16 32.82/13.97/26.44 32.61/13.92/26.39
scottishgaelic 23.16/7.04/19.25 20.42/4.31/17.02 24.05/7.73/19.45 29.01/10.96/22.87 28.85/10.55/22.65 30.08/11.2/23.83
serbiancyrillic 18.64/4.83/15.51 17.83/4.26/14.49 18.52/4.89/15.58 23.79/7.99/20.13 24.5/8.42/20.73 24.4/8.35/20.62
serbianlatin 17.13/4.19/14.24 18.09/4.31/14.82 18.04/4.21/14.9 21.64/6.68/18.23 22.91/7.18/19.26 22.57/7.11/19.02
sinhala 23.78/11.87/21.27 23.29/11.16/19.81 21.86/10.48/19.06 21.51/8.07/18.9 27.7/13.8/23.61 27.43/13.41/23.7
somali 29.08/10.21/22.45 29.06/9.97/22.13 28.09/8.88/21.45 31.54/11.55/24.22 32.1/11.38/24.42 32.48/11.72/24.55
spanish 30.05/10.98/23.13 29.28/10.3/22.23 28.09/9.05/21.14 31.51/11.87/24.07 31.63/11.9/24.11 31.59/11.92/24.07
swahili 33.82/14.82/27.78 34.79/15.52/28.24 33.43/14.91/27.19 37.68/17.86/30.92 38.24/17.82/31.16 37.74/17.65/30.8
tamil 22.47/10.33/20.58 19.81/8.83/18.12 16.45/6.58/15 24.31/11.04/22.07 24.58/11.11/22.3 24.48/11.08/22.11
telugu 16.66/5.84/15.1 17.02/5.91/15.24 15.14/4.99/13.77 17.73/5.73/15.84 20.09/7.1/17.78 20.1/7.26/17.85
thai 35.17/15.18/26.85 36.08/14.06/25.7 33.47/14.12/25.7 36.43/16.28/28.22 37.84/17.34/28.81 37.99/17.65/29.07
tigrinya 22.3/7.19/19.04 21.37/6.14/17.89 19.97/5.65/16.51 25.26/7.99/21.1 25.85/8.51/21.6 25.48/8.55/21.78
turkish 31.41/15.07/28 30.53/14.04/26.97 26.12/11/23.29 32.92/15.57/29.28 33.63/16.17/29.95 33.58/16.1/29.81
ukrainian 23.58/10.2/20.59 21.84/8.92/19.08 18.73/7.07/16.43 23.99/10.14/20.92 24.73/10.7/21.58 24.75/10.62/21.57
urdu 40.19/19.34/33.6 38.81/17.69/32.08 35.05/14.5/28.76 39.49/18.33/32.83 40.04/18.71/33.15 40.12/18.71/33.21
uzbek 13.48/4.84/12.32 14.44/5.21/13.1 11.77/3.98/10.88 16.82/6.35/15.38 17.45/6.74/15.73 17.63/6.71/15.87
vietnamese 32.53/16.45/25.94 30.78/14.57/23.78 27.33/12.34/21.27 30.24/14.39/24.13 33.62/16.43/26.46 33.49/16.57/26.38
welsh 31.58/11.46/25.6 30.19/9.97/24.17 27.92/8.06/22.24 32.64/11.59/26.12 33.09/12.03/26.41 33.13/11.88/26.35
yoruba 29.06/10.96/23.3 29.94/10.43/23.31 26.77/8.84/20.85 31.62/11.66/25.06 31.95/11.92/25.24 31.71/11.61/24.91
average 28.14/11.96/23.45 27.58/11.23/22.66 25.35/9.84/20.92 30.23/12.93/25.11 30.96/13.39/25.56 30.89/13.4/25.57
Table3: R1/R2/RLofsixmodelson45languagesandtheaveragescoreofalllanguages.

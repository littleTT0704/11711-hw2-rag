TheThirty-SixthAAAIConferenceonArtificialIntelligence(AAAI-22)
NAREOR: The Narrative Reordering Problem
VarunGangal* 1 StevenY.Feng* 1 MaliheAlikhani2 TerukoMitamura1 EduardHovy1
1LanguageTechnologiesInstitute,CarnegieMellonUniversity
2SchoolofComputingandInformation,UniversityofPittsburgh
{vgangal,syfeng,teruko,hovy}@cs.cmu.edu , malihe@pitt.edu
Abstract
Manyimplicitinferencesexistintextdependingonhowitis
structuredthatcancriticallyimpactthetext’sinterpretation
andmeaning.Onesuchstructuralaspectpresentintextwith
chronologyistheorderofitspresentation.Fornarrativesor
stories, this is known as the narrative order. Reordering a
narrativecanimpactthetemporal,causal,event-based,and
otherinferencesreadersdrawfromit,whichinturncanhave
strongeffectsbothonitsinterpretationandinterestingness.
Inthispaper,weproposeandinvestigatethetaskofNarra-
tiveReordering(NAREOR)whichinvolvesrewritingagiven
storyinadifferentnarrativeorderwhilepreservingitsplot.
Wepresentadataset,NAREORC,withhumanrewritingsof
storieswithinROCStoriesinnon-linearorders,andconducta
detailedanalysisofit.Further,weproposenoveltask-specific
trainingmethodswithsuitableevaluationmetrics.Weperform Figure 1: Example of our task and dataset, with original
experimentsonNAREORCusingstate-of-the-artmodelssuch inputstoryS ontheleft,targetnarrativeorderπ onthetop,
i′
asBARTandT5andconductextensiveautomaticandhuman andhumanrewrittenstoryS′ontheright.
evaluations.Wedemonstratethatalthoughourmodelscanper-
formdecently,NAREORisachallengingtaskwithpotential
forfurtherexploration.Wealsoinvestigatetwoapplications
ofNAREOR:generationofmoreinterestingvariationsofsto- orstoryandnarrative,respectively.Genette(1983)enlists
riesandservingasadversarialsetsfortemporal/event-related typicalordersobservedinwriting.Alinear ordernarrates
tasks,besidesdiscussingotherprospectiveones,suchasfor
events in same sequence as story order. The in medias res
pedagogicalsetupsrelatedtolanguageskillslikeessaywriting
orderstartswitheventsinthemiddle,goesbacktothestart,
andapplicationstomedicineinvolvingclinicalnarratives.
thenproceedstotheend.Changingfromnear-lineartomore
“interesting”ordersisprevalentincinema,e.g.TheImitation
1 Introduction Game starts with Turing’s post-WWII 1951 interrogation.
From the onset of language, storytelling has been crucial MementoandNakedLunchareknownfortheiresotericnarra-
tothetransmissionofknowledge(Ramanujan1991).Ithas tiveorders-looselydescribedasretrogade(reverseoflinear)
beenwell-establishedthatreadersrememberonlyanabstract andsyllepsis(lackingchronologicallogic),respectively.
representation of stories (Schank 1972). Before the print- Morgan (2017) explains how narratives surpass “mere
ingpress,classesengagedwithoralteachingofscriptures, chronicle”.Narrativeordersofpresentingmaterialsinscien-
such as rabbis, underwent extensive training to reproduce tificexplanationsdirectlyaffectshowresearchersinterpret
themwithnodistortion(Bos1995).Formallyanalyzingstory andunderstandthemsincetheorderimpliesnotonlytempo-
structurecommencedwiththeancients,throughworkslike ralbutotherinferencesaboutcausality,processesofchange,
Aristotle’sPoetics(Halliwelletal.1998).Thesestudiesled etc.Narrativeordercanthusinfluencemodelexplainability,
totheconceptofanarrative,distinctfromstoryevents. especiallyforexplanationgeneration(Rajanietal.2019),a
Forastory,therearetwoorders:thechronologicalorder recentarea-of-interest(WiegreffeandMarasovic2021).
ofeventsastheyhappenedandtheirorderaspresentedin
Inthiswork,wedonotdelveintothecomplexandsome-
text.Thesehavebeenanalyzedunderdifferentnames(Propp
whatsubjectivequestionofwhichnarrativeorderismostsuit-
2010).Werefertothemasstoryorderandnarrativeorder,
ableor“interesting”.Wefocusonhowagivenstoryinlinear
narrative order can be rendered in a specified, non-linear,
* EqualContributionbythetwoauthors
Copyright©2022,AssociationfortheAdvancementofArtificial target order while preserving plot. We call this Narrative
Intelligence(www.aaai.org).Allrightsreserved. Reordering,orNAREOR.Tothebestofourknowledge,we
10645
arethefirsttoproposeandinvestigatethistask. containsafour-sentencestoryprefixwithaone-sentenceco-
Ourworkisnotentirelyadriftfrompastresearchinthis herentandincoherentending.Wetreatthecoherentendings
vein.Montfort(2007)triesgeneratingfictionnarrativesfrom asthefifthsentencesforNAREORC’sdevandteststories.
basic existent-event info with a special focus on narrative
order,usingaruleandplanningapproach.Unlikeourwork, AssigningTargetNarrativeOrders: Thetargetnarrative
theirrule-basedsystemdoesnotinvolvelearning.Moreover, order π i′ is not part of the ROCStories input. We devise a
beinggenerationinagivennarrativeorderfromunstructured randomized procedure to assign a reasonable π i′ for each
storyelementsratherthanreorderinganexistingstory,their example. We sample 3 permutations from the set of non-
settingdoesnotrequiresolvingchallengessuchasdisentan- identityn!-1permutations.3WefindKendallτ correlations
glingeventsfromstorieswhichareinherentinNAREOR. (Kendall1938)betweenidentitypermutationI n,{1,2,3,4,5},
Formally, NAREOR involves reordering a story S with andeachofthethreepermutations,retainingthelowestas
sentences s 1,s 2,...,s
n
to a reordered story S′ with sen- π i′.Wepreferthistosamplingatrandombecausewewant
tencess′,s′,...,s′ accordingtoagiventargetnarrativeor- ourexamplestobesufficientlynon-trivialw.r.t.thetask.
1 2 n
derπ .π isapermutation{π |π : i′ → f(i′);1 ≤ i′ ≤
i′ i′ i′ i′ Supervised & Unsupervised Splits: We set aside 600,
n;f(i′) = i} mapping from target sentence1 indices i′ to
200,200storiesfromtrain,dev,andtestsplitsofROCStories.
originalsentenceindicesi,wheref isaone-to-oneandonto
These act as NAREORC’s trainSup, devSup, and testSup
functionfrom{1,2...n}toitself.Inpractice,wewriteπ
i′ splits, for which we collect human references. Remaining
asthesequence{i=f(i′)}i′=n(f andi′becomeimplied).
i′=1 storiesineachROCStoriessplitareretainedastrainUnsup,
NAREOR’schallengesareevidentfromtheexample in
devUnsup,andtestUnsupofsize95161,1671,1671.
Figure1.Simplyreorderingsentencesisfarfromsufficient,
as rewritten text must be adjusted to handle coreference, HumanAnnotation: FortrainSupanddevSup,weanno-
tense,andotherdiscoursedependencies.Forexample,nar- tateonereferenceperexample.FortestSup,wecollecttwo
rativeorderaffectstensesinceitcanchangethefirst2of3 eachtohelpreference-basedmetrics.Weconductourstudy
Reichenbachtimes(Reichenbach1947)thattogetherdeter- onAMT.Tounderstandtaskdifficulty,weaska“Hardness”
mine tense - speech, reference, and event time. NAREOR questionwithoptionsVeryEasy,Easy,Moderate,Hard,Very-
involvespinpointedandcriticaledits;asinglemissedorin- Hard.Onaverage,annotatorsfound≈70%ofrewritingsto
correcteditcanresultinanentirelydifferentorinvalidplot. beModerateorHard,demonstratingthatNAREORisquite
Sinceπ i′ canbeseenasacontrol,NAREORisacontrollable difficultevenforhumans.MoredetailsinAppendixB.
generationtask(seeAppendixAfordiscussion)
NAREORisalsoanovelformofstory-levelparaphrasing 2.2 DatasetAnalysis
and can be used to generate more interesting variations of
stories(§5.1).Outputscanalsoserveaschallengesetsfor OverallStatistics
temporalorevent-basedtaskssuchassentenceorderingto Wefindhuman-rewrittenstoriesS′are≈1.2xaslongasinput
assess the temporal reasoning capabilities of models (§6). storiesSonavginwordsandcharacters.Weexpectthisgiven
NAREOR can also be potentially useful for pedagogical thenarrativereorderedstoryfavorsresolutionofsentence-
setupsrelatedtolanguageskillssuchasessaywriting,and orderdependentelementslikeellipses(s 4 ands′ 4 inFigure
applicationstomedicineinvolvingclinicalnarratives(§6). 1)andpronouns(s 3ands′ 2inFigure1)toexplicitforms.It
TocomplementNAREOR,wepresentadataset,NARE- alsorequiresinsertionoftimeexpressions(e.gBeforethat,
ORC, with human rewritings of stories from ROCStories 3rdrow,Table1)toclarifythenowdisruptedflow.
(Mostafazadehetal.2016a)innon-linearorders.Weconduct Unique n-gram ratio UR (S) is the fraction of unique
n
athoroughanalysis,examiningvariouswayshumansmod- n-gramsoflengthninS.WeobserveallthreemeanURs
ifythetextwhenreordering(§2).Weperformexperiments (n=1,2,3)todecreasefrominputtoreferencestory.UR :
1
withBART,T5,andGPT-2onNAREORCusingnovel,task- 0.692→0.669,UR :0.940→0.931,UR :0.989→0.984.In-
2 3
motivated training methods we propose (§3). We evaluate creasedn-gramrepetitioncouldhavereasonssimilartolength
our models with both an automatic and human evaluation increase,causingcross-sentencerepetition.Figure1demon-
alongwithqualitativeanalysis(§5).Wedemonstratethatour stratesthis:S onlyhasoneinstanceofmoney.Conversion
proposedtrainingmethodsareeffectivebuthaveroomfor ofinheritanyofit(s )→inheritanyofthemoney(s′)and
3 2
furtherimprovement.WeillustratethatNAREORisindeeda enoughtotaketime(s )→enoughmoneytotakesometime
4
challengingtaskwithpotentialforfurtherexploration.2 (s′),amongotherchanges,resultsinfourinS’.
4
HowVerbFormsChange
2 Dataset:NAREORC
We note changes in occurrence distribution across verb-
2.1 DatasetConstruction related pos tags from S to S′ using NLTK’s pos tagger.
Gerundfraction(pos=VBG)(e.g.Ilikeplaying)increases
SourceCorpus: ROCStorieshas≈ 98.5Kfive-sentence
7.7%→9.5%. Past participle fraction (pos=VBN) (e.g. He
English stories. For the dev and test splits, each example
had broken it) ≈ doubles, 6.5%→12.4%. Past tense frac-
1Forsimplicity,weassumenarrativetobreakupintosentence tion(pos=VBD)(e.g.Hebrokeit)decreases60.9%→54.6%.
units.Ourtaskisstillverychallengingasshownthroughthispaper.
2Code+dataatgithub.com/vgtomahawk/NAREORCamReady. 3Inourcase,n=5asweexperimentwithROCStories.
10646
Otherverb-relatedposfractionsremainfairlyconstant.In- sentences, we prefix each s ∈ S with a tag from <a> to
creaseinpastparticiplecanbeexplainedbyfrequentcon- <e>.Wespecifyπ asasequenceofthese,separatedfrom
i′
versiontopastperfecttenseduringreordering(e.g.parents Sby<sep>.NAREORinvolvesrearrangingmentiontypes
passedaway→parentshadpassedawayinFigure1). amongcoreferencechains(see§2.2),soweuseNeuralCoref
(HuggingFace 2020) to detect these chains. For each, we
HowNarrativeReorderingAltersSentences
assignauniqueuppercasetag(<X>)toreplaceitsmentions.
We look at corresponding sentence pairs {s ,s′ } in each
i i′ Attheendoftheinput,welisteachtagandtheheadmention
story,specifically4linguisticchangetypes-ellipsis,tense,
ofitscoreferencechaininorder.Wethenappend<st>to
timeexpressions(timexes),coreference.Wetrieddetecting
mark the end of the input. An illustration of the scheme
theseusingoff-the-shelftools,anddidnotfindanyforellipsis.
follows:<a>SinceIhadfrontseattickets,Iwasabletodirectly
Timex detectors like SUTime (Chang and Manning 2012)
see <X1>. <b> <X1> tried to reach out with <X1> <X2>.
onlymarkstricttimexes(e.g.lastSunday)butnotothers(e.g.
<c>Igrabbed<X2>and<X1>pulledmeonstage.<d><X1>
beforemidsems).Wehencehand-annotatethesefourforeach
begantosing.<e>Theconcerthadstarted.<sep><e><d>
{s ,s′ }pertestSupexample.Thesearefurtherdescribedin
i i′ <a><b><c><X1>Themusicartist<X2>herhand<st>
Table1.Wefindoverhalf(51.5%)theexamplesshow≥3of
• Reorder-1S:WeuseexamplesfromtrainUnsupforstage1.
4changetypesatonce,and89.5%show≥2.Thisshowsthat
Itisproblematictotrainfortheforwarddirectionofourtask
NAREORrequiresperformingdifferentchangesintandem. S,π →S′sinceS′isnotknown.ApproximatingS′using
i′
S′ would hurt output fluency. We instead train in the
3 Methodology inn va ei rv se edirectionS′ ,π−1 →S,whereπ−1;π−1(π )=
naive i′ i′ i′ i′
3.1 TrainingMethods I n is the inverse permutation of π i′. To reduce train-test
mismatch,weusetheinverseformulationhalfthetime,and
Weintroducetwotask-specifictrainingmethods.
anautoencodingone,i.e.S,I →S theotherhalf.
n
NAR-denoise(NAR-d) • Reorder-2S:trainSupexamplesareusedtofurtherfinetune
This is partially inspired by how humans rewrite; a com- onreorder-1S.WetraininthetaskdirectionS,π i′ →S′.
mon approach is to first reorder sentences naively (simply
3.2 ChosenModels
swappositions),thenmakeotherchanges.NAR-dattempts
to mimic this, learning to convert from naive orderings to We choose several pretrained generation models: GPT-2,
high-qualitytext.Itinvolvestwostagesofmodeltraining. BART,andT5.Wefinetuneallusingbothourtrainingmeth-
odstoproducedenoise-1S(d-1S),denoise-2S(d-2S),reorder-
1. Denoise-1S: Stage 1 is unsupervised training through
1S(r-1S),andreorder-2S(r-2S)versions.GPT-2(Radford
story-level denoising. We use trainUnsup without human-
etal.2019)isaTransformer-basedlanguagemodeltrained
written reorderings, and simulate them using the original
onWebText.BART(Lewisetal.2020)andT5(Raffeletal.
human-written ROCStories (the outputs during training).
2020)areTransformerseq2seqmodels.BARTistrainedas
Deletion andswapping of tokens areused tocreate inputs
adenoisingautoencodertoreconstructoriginalfromnoised
fromthesestoriesthatsimulatenaivereorderings.Thisnois-
text.T5isdesignedtobeeffectivefortransferlearning.We
ing aims to emulate the reverse of the content editing that
useHuggingFace’simplementationsoftheirbaseversions.4
occursduringNAREOR.Specifically,werandomlydelete
12.5%oftokensandswapanother12.5%.Wefoundhuman-
3.3 AutomaticEvaluationMetrics
rewrittenstorieswere,onaverage,incombinationoftoken
length (longer) and swappings, ≈25% different from the Reference-Based Metrics assess the similarity between
generatedtextandhuman-writtenreferences.WeuseBLEU
originals. We split this between deletion and swapping to
approximatenaively-reorderedstories.StorysentencesS are (Papinenietal.2002),METEOR(BanerjeeandLavie2005),
first reordered as per π to produce S′ , then each is andBERTScore(Zhangetal.2019).Wecomparegenerated
i′ naive textwiththetworeferencespertestSupexample.5
editedtofitthenewnarrative.Weswaptokensashumans
oftenswapwordslikecoreferentmentionsbasedonhowthe Target Order Fidelity (TOF) is defined as how closely
narrativeorderchanges.Hence,thisstagelearnstodenoise thereorderedtextmatchesthegiventargetnarrativeorder.
textbyconvertingnoisedversionstohuman-writtentext. E.g. given S = {s ,s ,s }, π = {3,2,1}, and S′ =
1 2 3 i′
2. Denoise-2S:Thesecondstageissupervisedtrainingatop {s′,s′,s′},wewishtoseeifs hascorrectlybeentranslated
1 2 3 1
the model above. The inputs are the 600 original stories tos′.WeintroduceTOF-METEORandTOF-BERTScore.
3
intrainSup,withsentencesnaivelyreorderedaspertarget TheseassesstheaverageMETEORandBERTScorevalues
narrativeorderπ i′ toS n′ aive,andtheoutputsarethehuman foreachalignedpair{s i,s′ i′}∀i(wherei′ referstothetar-
rewritingsofthese.Themodellearnstofurthertranslatefrom getindexfors ).Highervaluescorrespondtomorecontent
i
naively-reorderedtexttofluenthuman-writtentext. preservation,whereeachoutputsentenceismorelikelyinthe
correctposition.Somedropisexpectedinmodulatingforπ ,
NAR-reorder(NAR-r) i′
buttheoverallcontentshouldbefaithful.Thesemetricsserve
UnlikeNAR-d,NAR-rmodelsthemselveshandlereordering
moreasvalidation,wherereasonablevalues(e.g.>50)6are
giventhetargetorderratherthannaivereorderingbeforehand.
• InputEncodingScheme:Wedescribehowthetaskinput 4See§4forfurthertraining/finetuningdetails.
{S,π i′} is encoded as a token sequence for both Stage-1 5Correlateswellwithhumanevaluationasshownin§5.
and2training.Toenablethemodeltodistinguishdifferent 6Assumingthevaluesaremultipliedby100.
10647
ChangeType StoryExampleswithChangesHighlighted
S:1.AlloftheRossfamilyhasredhair,exceptHenry.2.Henryhasblondehairthatisverycurly.3.Henry’sfatheroften
teasesHenry’smotheraboutthemailman.4.Themailmanhasblonde,curlyhair,butheisveryugly.5.Hisdad’steasing
Ellipsis
(Sent:5.7%)
makesHenryfeelbad.;π i′:{1,5,4,2,3}
S’:1.AlloftheRossfamilyhasredhair,exceptHenry.2.Hisdad’steasingaboutthemailmanmakesHenryfeelvery
(Stor:27.5%)
bad.3.Thisisbecausethemailmanhasblonde,curlyhair,butheisveryugly.4.Henryalsohasblondehairthatisvery
curly.5.Henry’sfatheroftenteasesHenry’smotheraboutthemailman.
S:1.SamboughtanewSUV.2.Itwasallwheeldrive.3.Hefiguredhewouldtakeitoffroad.4.Hehitafewhardbumps
Tense andbrokehissuspension.5.Sheepishly,hebroughtittothedealershipforrepair.;π i′:{2,3,5,1,4}
(Sent:19.1%) S’:1.Sam’sSUVwasanallwheeldrive.2.Hethoughthecouldtakeitforaspinoffroad.3.Embarrassedbytheoutcome
(Stor:64.0%) ofhisdrive,Samtookthecartothedealershipforrepair.4.HehadjustboughttheSUV.5.Thecarhadhitafewhard
bumpsandthesuspensionbrokewhenSamtookitoffroad.
S:1.Therewasonceakittenthatdidnothaveahome.2.Thepoorkittenwalkedaroundcoldandhungry.3.Oneday,a
Timexes niceladyletthekittenintoherhome.4.Thewomangavethekittenfoodandabed.5.Thekittenwashappytobeadopted.
(Sent:34.0%) ;π i′:{4,2,5,1,3}
(Stor:85.5%) S’:1.Awomangaveahometoacat.2.Beforethatitwascoldandhungry.3.Itmadethecathappytohaveahome.4.
Thelittlecatoriginallywashomeless.5.Butintheend,itmetthenicewomanandsheletitin.
S:1.Jimmywanderedaroundthecitylookingforaplaceforasoda.2.Beforeheknewit,hewasinanunfamiliararea.3.
Hewasscaredofstrangersanddidn’twanttoaskanyone.4.Soonapolicemancamebyandaskedifhewaslost.5.He
Coreference
(Sent:20.7%)
toldhimthathewaslost.;π i′:{5,4,2,1,3}
S’:1.Jimmytoldapoliceofficerthathewaslost.2.Hewasluckythepoliceshowedupinthefirstplace.3.Hehadno
(Stor:71.5%)
ideawherehewas.4.Hehadwanderedoffwhentryingtofindsomewheretobuyasoda.5.Itwasprettyterrifyingbeing
allaloneinamysteriousareawithstrangers.
Table1:SentencepairsintestSupstoriesareannotatedfor4linguisticchangetypescommoninNAREORC.Sentdenotes%of
sentencepairsshowingthatchangetype.Stordenotesstorypairs(S,S′)where≥onesentencepairshowsthatchangetype.
sufficient.Lowervaluesindicatemorechangingofthetext models,andasubsetofone-stagemodels.Eachexampleis
whichmaybenecessaryforcertainnarrativereorderings. evaluatedbytwoannotators.SeeAppendixDformore.
Theyevaluatefluency,coherence,logic,andplotpreserva-
4 Experiments tion(plot-pres)on1-5scales.Fluencyisameasureofhow
fluentandreadableatextis.Coherenceishowwellindivid-
ModelFinetuningandGeneration ualsentencesfittogether(BarzilayandLapata2008).Logic
For finetuning our models, we try different combinations istheplausibilityofdescribedevents.Plot-presishowwell
of learning rates (LR) for both stages. We look at either reorderedtextpreservestheplotoftheoriginal.Thisincludes
the loss (for BART and T5) or perplexity (for GPT-2) on details about characters, events, and interactions between
therespectivevalidationsplits(devUnsupfor1ststageand them,encompassingitssemanticandtemporalaspects.
devSupfor2nd),andchoosetheepochwiththelowest. We also conduct an interestingness (interest) study on
WeevaluateeachmodelontestSup,wherewecandirectly humanrewritingsandoutputsfromourBART-2SandT5-2S
compareresultstoNAREORC’shumanrewritings.Wegen- models.Eachreorderedstory’sinterestingnessw.r.t.suspense
erate a single output per test example. The inputs are the andtimeflowcomparedtotheoriginalareevaluatedfrom
original examples to NAR-r models and the S′ of the 1-5 by two annotators. We ask the following: “On a scale
naive
examplestoNAR-dmodels.See§3.1formoredetails. of1-5,with1beingmostdecreaseininterestingnessand3
Weonlykeepthefirstfivesentencesofeachoutput.For beingsamelevelofinterestingnessand5beingmostincrease
BARTandT5,weusebeamsearchwithawidthof5.7 For ininterestingness,howinterestingisthesuspenseandflow
GPT-2,weuseanucleussamplingbudget(Holtzmanetal. oftimeinthestoryS,comparedtotheoriginalstoryO?How
2019)of0.9andoutputlengthlimitof500.Wetryvarious excitingdidyoufindthestoryasyoureadthroughit?”
softmaxtemperaturesandfind0.9performsbest.ForGPT-2,
duringfinetuning,itisgiventheconcatenationoftheinput 5 ResultsandAnalysis
plus output. During generation, it is only fed the input for
We present evaluation results of our 2S and subset of 1S
whichitgeneratesacontinuation(theoutput).Wenoticed
modelsontestSupcomparedtohumanrewritingsandoriginal
thatmanyGPT-2generationsincludedtrailingexclamation
stories.Tables2and3containhumanevaluationresults,and
marks,andstriptheseifmorethanfouroccurinarow.8
Table4automaticevaluationresults.Correlationsbetween
automaticandhumanmetricsareinTable5.Table6contains
HumanEvaluation
qualitativeexamples,withmoreinAppendixE.
Annotatorsevaluate100testSupexampleseachfromtheorig-
inalstories,humanrewritings,outputsfromourtwo-stage
5.1 AnalysisofHumanEvaluationResults
7NucleussamplingdidnotworkaswellforBARTandT5. We begin by analyzing human evaluation performance
8SeeAppendixCformorefinetuning/generationdetails. throughresultsinTables2and3.
10648
Method\Metric Fluency Coherence Logic Plot-pres TargetOrderFidelity(TOF): Itappearsallapproaches
Originalstories 4.209 4.0 3.851 N/A arereasonable(e.g.>50forTOFmetrics),andoutputsare
Humanrewritings 3.797 3.723 3.784 3.972
likelyinthecorrecttargetorders.Humanrewritingshavethe
GPT2-d-2S 3.635 3.399 3.399 3.708
lowestTOF;humansarelessconservativewhilerewriting
GPT2-r-2S 3.595 3.378 3.291 3.375
(shownin§5.3).GPT-2modelsmodifytextsecondheaviest,
BART-d-1S 3.628 3.412 3.318 3.847
BART-d-2S 3.818 3.507 3.493 3.722 butperformworstoverall.Theyintroducemoreerrors,e.g.
BART-r-2S 3.757 3.439 3.493 3.861 repeatingorhallucinatingtodegradetextqualityandplot-
T5-d-2S 3.764 3.419 3.5 3.889 pres(§5.3).BARTandT5modelsaremoreconservative.It
T5-r-1S 3.655 3.378 3.486 3.847 appearstheyhavelearnedtoperformminimalbuteffective
T5-r-2S 3.784 3.595 3.520 3.861
edits(§5.3).Theylagbehindhumansandheaviereditingmay
berequiredtofurtherimprove.Lastly,itappearsthereorder
Table2:AveragehumanevaluationresultsontestSup(excl.
modelsmodifytextmoreheavilythantheirdenoisevariants.
interestingness), rated from 1-5. Bold corresponds to best
modelperformancepermetric,andunderline2nd-best.
5.3 QualitativeAnalysis
FromTable6,weseethathumansmodifytextheavilytosuit
Method: Human BART-d BART-r T5-d T5-r
thereorderingsandaresometimesquitecreative,e.g.phras-
Interest 3.75 3.367 3.483 3.533 3.3
ingFredashavinggrownaccustomedtothebirdbeinghis
Table 3: Average interestingness results on testSup, rated alarmclock(ex.2).Humanssuccessfullyhandlenecessary
from1-5(3representsequaltooriginalstory).Modelsare2S coreferences,tenses,timeexpressions(timexes),etc.
versions.Bold/underlinedenote1st/2nd-bestperformance. GPT-2modifiestextquiteheavilybutsuffersfromincor-
rectcoreferencewhileintroducingspurioustokens,repetition,
orhallucations.Forex.2,GPT2-rchangestheplotgreatly,
statingFredwokehimupforworkandThiswasbecausehe
Fluency,Coherence,Logic: Originalstoriesarethehigh-
estforallthreemetrics9withhumanrewritingssecondforco- likedFred(likelyduetopoorcoreference),andhallucinating
This bird, however, did not like Fred. For ex. 4, it repeats
herenceandlogic,beatingthemodelsbyanoticeabledegree.
Joey’sexcitementmanytimes,whilehallucinatingaroller
BART-d-2SandT5-r-2Saregenerallythebest-performing
coasterthatwasabsentintheoriginalstory.
modelshere.BART-d-2Sslightlyoutperformshumanrewrit-
BARTandT5modelsaremoreconservative,buttheiredits
ingsonfluency,withT5-r-2Scloselybehind,demonstrating
areimportantandeffective.Theyhandlecoreference,tense,
thatthesemodelsarequitefluent.Thesemodelsalsooutdo
andtimexesquitewell.Thesepinpointedandcriticaledits
their1Svariants.GPT-2modelsperformworstonallmetrics.
arerequiredtomaintainplot.Forex.1,theymodifyHetold
Plot-pres: Weseethathumanrewritingsbestpreservethe himthathewaslosttoJimmytolda/thepolicemanthathe
plotoftheoriginalstories.T5-d-2Sisthebestperforming waslostgiventhatsentenceisnowatthebeginning.BART-d
model on plot-pres, followed by BART-r-2S and T5-r-2S. impressivelymodifiestensebyconvertingSoonapoliceman
GPT-2modelsperformtheworstatpreservingtheplotofthe camebyandaskedifhewaslosttoThepolicemanhadcome
originalstories(whichweshowqualitativelyin§5.3). byandaskedifhehadbeenlost.Forex.2,T5-dconverts
enjoyed tohadenjoyed sincethebirdnolongersingingis
Interestingness: Humanrewritingsscorehighestoninter- nowpriorinformation,andaddsthetimexAfterawhiletothe
est.Humansrewritethetextinmorecreativeways,whereas beginningofthelastoutputsentence.BART-rsuccessfully
BARTandT5modelsaremoreconservative(see§5.2TOF changes Fred began to like the bird to He had begun to
and §5.3). Narrative reorderings for all methods are more likethebird.Forex.3,BART-dinsertsthetimexEarlierat
interesting,onaverage,thanoriginalstories.NAREORcan thebeginningofthesecondoutputsentence,correctlyand
indeedbeusedtogeneratemoreinterestingstoryvariations. unambiguously conveying its underlying temporality w.r.t.
thefirst.BART-dcorrectlychangessawaturtletohadseen
5.2 AnalysisofAutomaticEvaluationResults aturtle,whileBART-rdoessoforstepped tohadstepped.
Forex.4,BARTandT5modelsallresolvetheDisneyland
Wenowanalyzetheautomaticevaluationperformanceofthe
ellipsis by converting Joey had a great time to Joey had a
differentmethodsinTable4.
greattimeatDisneyland,whileGPT2-dcannot.
BERTScore,BLEU,METEOR: WeseefromTable5that However,theBARTandT5modelsareimperfect.Forex.
thesereference-basedmetricscorrelatequitewellwithhuman 1, BART-r hallucatines lost his wallet (original story does
evalmetrics,particularlyplot-pres.T5-d-2Sperformsbest notinvolveawallet),T5-dinsertsanincorrecttimexofSoon
followed by BART-d-2S. Similar to the human evaluation, after at the beginning of the second output sentence, and
2Smodelsoutperformtheir1Svariants,andGPT-2models T5-r hallucinates asked if he had a soda (this is not asked
performworstoverall.Denoiseoutperformsreordervariants intheoriginalstory).Forex.2,BART-rincorrectlyconverts
andgeneratemoresimilartext,onavg,tohumanreferences. thebirdnolangersangtoFrednolongersang,likelydue
tocoreferencedifficulties.Forex.3,T5-rdoesnotconvert
9Although these metrics slightly decrease for reordered sto- SuddenlytoEarlierlikeBART-d,givingafalseinterpretation
ries,wenotethatNAREOR’smainpurposeisformoreinteresting thatEricslippedafterhisrescuer’sarrival.BART-rdoesnot
tellingsofthesamestorywhichwedoachieve(seeTable3). misleadwithSuddenly,butisambiguousandhasnotimex.
10649
Method\Metric BERTScore BLEU METEOR TOF-BERTScore TOF-METEOR
Humanrewritings N/A N/A N/A 66.85 56.79
GPT2-d-2S 60.75 37.01 45.20 79.23 74.23
GPT2-r-2S 58.03 32.57 40.85 73.04 63.00
BART-d-1S 67.14 44.73 49.88 95.61 93.43
BART-d-2S 67.93 46.03 50.54 93.55 90.81
BART-r-2S 67.16 44.63 49.16 91.32 86.43
T5-d-2S 67.99 46.95 51.12 94.20 91.83
T5-r-1S 66.24 43.40 48.20 89.85 84.26
T5-r-2S 66.62 44.30 49.00 91.61 86.16
Table4:AverageautomaticevaluationresultsontestSup(valuesmultipliedby100).Boldcorrespondstobestperformanceper
metric,andunderlinesecond-best(excludingtheTOFmetricswhicharemainlyforvalidation).
Metric Correlation Fluency Coherence Logic Plot-pres Interest
Pearson 0.130(4e-04) 0.139(1e-04) 0.125(0.001) 0.255(1e-06) 0.111(0.226)
BERTScore
Spearman 0.106(0.004) 0.124(0.001) 0.127(0.001) 0.211(5e-05) 0.117(0.201)
Pearson 0.144(9e-05) 0.140(1e-04) 0.113(0.002) 0.219(3e-05) 0.174(0.047)
BLEU
Spearman 0.130(4e-04) 0.129(4e-04) 0.123(0.001) 0.179(0.001) 0.171(0.049)
Pearson 0.107(0.003) 0.125(0.001) 0.108(0.003) 0.203(1e-04) 0.120(0.191)
METEOR
Spearman 0.098(0.008) 0.114(0.002) 0.122(0.001) 0.164(0.002) 0.121(0.187)
Table5:PearsonandSpearmancorrelationsbetweenautomaticandhumanevaluationmetrics,withp-valuesinbrackets.TOF
metricsexcludedastheyaremainlyforvalidation.Boldcorrespondstohighestcorrelationperhumanevaluationmetric.
5.4 OverallTakeaways corpus (Huang et al. 2016), ii) M , an in-domain model
iid
onfirst20%ofROCStories’trainsplit.Wetesteachoni)
Humansmodifytextgreatlywhilesuccessfullyperforming
Controlset{s }i=n,inputstoriesfromtestSup,ii)Challenge
NAREOR.BARTandT5modelsperformdecentlywithmin- i i=1
set{s′}i=n,reorderedstoriesfromtestSup.Table7shows
imalbuteffectiveedits.GPT-2modelstendtorepeat,halluci- i i=1
drastic drops across metrics (higher is better - see Prabhu-
nate,andreducetextqualityandplotpreservation.Basedon
moye,Salakhutdinov,andBlack(2020))forbothM and
human(§5.1)andautomatic(§5.2)evaluation,BART-d-2S ext
M fromcontroltochallengeset,confirmingourhypothe-
and T5-d-2S are the best models overall. BART-d-2S out- iid
sis.Systemswithabilitytomanipulatenarrativevariableslike
doesitsreordervariant,possiblyduetoBART’spretraining
ordercouldbeimportantforautomatingpedagogicalsetups,
as a denoising autoencoder, closer to our denoise training
especiallyforfine-grainedlanguageskillssuchasargumen-
method.ForT5,bothmethodsperformquitewellandshow
tation in essay writing. As Wingate (2012) explains, tutor
potential. However, T5-d outperforms on plot-pres (Table
understandingisfounddeficientandmethodsoffeedbackfor
2),interest(Table3),andautomaticmetrics(Table4).The
studentsareinconsistentorvague.Languageinschooltexts
denoisetrainingmethodappearstobeslightlymoreeffec-
follows a characteristic register, which often differs from
tive,possiblybecauseitispartiallyinspiredbyhowhumans
registersstudentshandleineverydayconversation(Schleppe-
performNAREOR(see§3.1).Thesearethefirsttwotask-
grell2001).Models(e.g.NAREORones)whichcancontrol
specifictrainingmethodsforNAREORwhichwepropose
elementsofregister,e.gnarrativeorder,canbeusedtotailor
ourselves,eachapproachingthetaskdifferently(see§3.1).
such content to intended settings and bridge this gap. Sys-
2Smodelsalsomostlyoutperform1Sones,demonstrating
temsthatcangenerateeventtimelinesforclinicalnarratives,
thatsecondstagefinetuningimprovesuponthefirst.
e.g.admissionnotesandphysicalreports,areimportantfor
BARTandT5modelsarequiteeffective,excellingatflu-
applicationslikemedicaldocumentsummarization(Bramsen
ency,buthavefurtherroomforimprovementincoherence,
etal.2006;Reichertetal.2010)andclinicaldecisionmaking
logic,plot-pres,andinterest.§5.3showstheystillsufferfrom
(Demner-Fushman,Chapman,andMcDonald2009).Ragha-
severalissues.Theirconservativetendencymaylimittheir
vanetal.(2014)demonstratethatcross-narrativetemporal
NAREORabilitycomparedtohumans.Overall,thesemodels
orderingofmedicaleventsisvitaltogeneratingacompre-
serveasstronginitialbaselinesforNAREORwhileunder-
hensivetimelineoverapatient’shistory.Aligningmultiple
scoringthetask’sdifficultyandpotentialforexploration.
medicaleventsequencesusingcoreferenceinformationand
temporalrelationshasalargeimpactontheirpresentation
6 ApplicationsofNAREOR
andeffectiveness.OurNAREORmodelsmaybeeffective
Sentenceorderinginvolvesreconstructingoriginalsentence hereandimproveuponexistingsystems.
order of an unordered sentence set (Barzilay and Lapata
2008).NAREORC’sreorderedstoriescouldserveasachal-
7 RelatedWork
lenge set for sentence reordering models due to their non-
linear narrative structure underrepresented in corpora. We Thereexistswork onthesentence orderingtask discussed
usetheimplementationof(Prabhumoye,Salakhutdinov,and in§6—e.g.,Chen,Qiu,andHuang(2016)learnpairwise
Black2020)totraini)M ,anexternalmodelontheSIS orderings of sentences using a ranking model. Unlike sen-
ext
10650
Method Text
Original Jimmywanderedaroundtownlookingforasoda.Beforeheknewit,hewasinastrangearea.Hewasscaredofstrangersand
(ex.1) didn’twanttoaskanyone.Soonapolicemancamebyandaskedifhewaslost.Hetoldhimthathewaslost.{5,4,2,1,3}
BART-d Jimmytoldapolicemanthathewaslost.Thepolicemanhadcomebyandaskedifhehadbeenlost.Beforeheknewit,hewasin
astrangearea.Jimmyhadwanderedaroundtownlookingforasoda.Hewasscaredofstrangersanddidn’twanttoaskanyone.
BART-r Jimmytoldthepolicemanthathewaslost.Thepolicemancamebyandaskedifhehadlosthiswallet.BeforeJimmyknewit,he
wasinastrangearea.Hehadwanderedaroundtownlookingforasoda.Hewasscaredofstrangersanddidn’twanttoaskanyone.
T5-d Jimmytoldapolicemanthathewaslost.Soonafter,apolicemancamebyandaskedifhewaslost.Beforeheknewit,hewasin
astrangearea.Hehadwanderedaroundtownlookingforasoda.Hewasscaredofstrangersanddidn’twanttoaskanyone.
T5-r Jimmytoldapolicemanthathewaslost.Apolicemancamebyandaskedifhehadasoda.Beforeheknewit,hewasinastrange
area.Hehadwanderedaroundtownlookingforasoda.Hewasscaredofstrangersanddidn’twanttoaskanyone.
Original Everymorning,abirdwokeFredupforworkat6AM.Fredbegantolikethebird.Andheenjoyedthesoundofitssinging.One
(ex.2) day,thebirdnolongersang.Hehopednothingbadhadhappenedtothebird.{4,1,3,5,2}
Human Freddidn’thearthebirdsingingthismorningwhichisunusual.Thebirdhadbeenwakinghimupeverysingledayat6AMfor
quitesometime.Hehadgrownaccustomedtothebirdbeinghisalarmclock.Nowhe’sworriedthatsomethingmighthave
happenedtothepoorbird.Healmostconsidersthebirdafriendatthispoint.
GPT2-r Oneday,Fred’sbirdbegantosing.Everymorning,Fredwokehimupforworkat6AM.ThiswasbecausehelikedFred.He
hopednothingbadhadhappenedtoFred.Thisbird,however,didnotlikeFred.
BART-r Frednolongersang.Everymorning,hewokeFredupforworkat6AM.Heenjoyedthesoundofhissinging.Hehopednothing
badhadhappenedtohim.Hehadbeguntolikethebird.
T5-d Oneday,thebirdnolongersang.Everymorning,abirdwokeFredupforworkat6AM.Hehadenjoyedthesoundofitssinging.
Hehadhopednothingbadhadhappenedtothebird.Afterawhile,Fredbegantolikethebird.
Original Ericwaswalkingaroundapondatalocalpark.Hesawaturtleonarockfewfeetoffshore.Hesteppedintothewatertotryto
(ex.3) catchtheturtle.Suddenly,hesankkneedeepintothemudandgotstuck.SomeonewalkingontheshoreranovertorescueEric.
{5,4,1,3,2}
Human SomeonewhohadbeenwalkingontheshoreultimatelyranoverandrescuedEricfromthemud.Eric’skneehadsunkdeepinto
themud,renderinghimstuck.Earlier,Erichadbeenwalkingaroundapondwhileatalocalpark.Tryingtocatchaturtleinthe
pond,Ericsteppedintothewater.Ericdidsobecausehehadseenaturtlepoachedonarockfewfeetoffshore.
BART-d SomeonewalkingontheshoreranovertorescueEric.Earlier,hehadsunkkneedeepintothemudandgotstuck.Ericwaswalking
aroundapondatalocalpark.Hesteppedintothewatertotrytocatchtheturtle.Hehadseenaturtleonarockfewfeetoffshore.
BART-r SomeonewalkingontheshoreranovertorescueEric.Ericsankkneedeepintothemudandgotstuck.Hewaswalkingarounda
pondatalocalpark.Hehadsteppedintothewatertotrytocatchtheturtle.Hesawtheturtlefewfeetoffshore.
T5-r SomeonewalkingontheshoreranovertorescueEric.Suddenly,Ericsankkneedeepintothemudandgotstuck.Hewaswalking
aroundapondatalocalpark.Hesteppedintothewatertotrytocatchtheturtle.Hesawtheturtlefewfeetoffshore.
Original JoeywenttoDisneylandwithhisfamily.HewassoexcitedtomeetMickeyMouse.WhenhemetMickeyMousehewassoexcited
(ex.4) hecouldn’tspeak!JoeyfinallysaidhitoMickeyanditwasgreat!Joeyhadagreattime.{5,2,4,1,3}
GPT2-d Joeyhadagreattime.HewassoexcitedtomeetMickeyMouse.HehadfinallysaidhitoMickeyanditwasgreat!Hehadgoneto
Disneylandwithhisfamily.WhenhemetMickeyMousehewassoexcitedhecouldn’tspeak
GPT2-r JoeyhadagreattimeontherollercoasteratDisneyland.HewassoexcitedtomeetMickeyMousehecouldn’tspeak!(...)He
wassoexcitedthathewenttotheparkwithhisfamily.HewassoexcitedwhenhemetMickeyMouseandhewassoexcited.
BART-r JoeyhadagreattimeatDisneyland.HewassoexcitedtomeetMickeyMouse.HefinallysaidhitoMickeyanditwasgreat!He
hadgonetoDisneylandwithhisfamily.WhenhemetMickeyhewasexcitedhecouldn’tspeak!
T5-d JoeyhadagreattimeatDisneyland.HewassoexcitedtomeetMickeyMouse.HehadfinallysaidhitoMickeyanditwasgreat!
HehadgonetoDisneylandwithhisfamily.WhenhemetMickeyMousehewassoexcitedhecouldn’tspeak!
Table6:QualitativetestSupexamples.Targetpermsinbracketsalongoriginalstories.d&rrefertodenoise&reorder.
Model TestSet SentAcc Rouge-S LCS Kendallτ narrativeordervisualizationwork.Forexample,Kimetal.
Control 76.35 48 59.1 0.57
M (2017)visualizenarrativeorderasafunctionofstoryorder.
ext Challenge 52.4 24.7 29.7 0.12
Control 66.4 85.3 84.8 0.75
M iid Challenge 21.9 49.6 58 0.03 8 ConclusionandFutureWork
We proposed the NAREOR task and introduced a dataset,
Table7:Sentenceorderingoncontrolvs.challengesets.
NAREORC,withtask-specifictrainingmethodsandevalua-
tionmetrics,andexperimentedwithT5,BART,andGPT-2.
Extensiveevaluationandanalysisshowedthatourmodels
tenceordering,NAREORinvolvesreorderingandrewriting areeffectivebutcanbefurtherimproved,andthatNAREOR
asequenceofsentencestofitanewnarrativeorder. ischallengingwithfurtherexplorationpotential.Weshowed
TALESPIN(Meehan1975)wasanearlygoal-basedstory that NAREOR can create interesting story variations and
generator.Therehassincebeenworkonrelatedtaskslike challengesetsfortaskslikesentenceordering.
storyclozetest(Mostafazadehetal.2016b,2017)andgen- Futuredirectionsincludeexploringtrainingideasbetter
erationfromprompts(Fan,Lewis,andDauphin2018;See emulating human rewrites. NAREOR can be explored as
etal.2019).Someworksexplorecontrollablevariants,e.g. document-levelparaphrasingfordataaugmentation,adver-
keywordsascontrol(Pengetal.2018).NAREORisdistinct sarialsetsformoretemporaltasks,andapplicationsforeduca-
as it aims to preserve underlying plot while controlling a tion/medicine(see§6).Wealsohopeourworkdrivesinquiry
story-levelaspectforanalready-completestory.Thereisalso intohardertaskvariationsofNAREOR(e.g.sub-sentential).
10651
References Denoising Sequence-to-Sequence Pre-training for Natural
LanguageGeneration,Translation,andComprehension. In
Banerjee,S.;andLavie,A.2005. METEOR:Anautomatic
Proceedingsofthe58thAnnualMeetingoftheAssociation
metric for MT evaluation with improved correlation with
forComputationalLinguistics,7871–7880.Online:Associa-
humanjudgments. InProceedingsoftheaclworkshopon
tionforComputationalLinguistics.
intrinsicandextrinsicevaluationmeasuresformachinetrans-
lationand/orsummarization,65–72. Meehan,J.R.1975. UsingPlanningStructurestoGenerate
Stories. American Journal of Computational Linguistics,
Barzilay,R.;andLapata,M.2008.Modelinglocalcoherence:
78–94. Microfiche33.
Anentity-basedapproach. ComputationalLinguistics,34(1):
1–34. Montfort,N.2007. OrderingEventsinInteractiveFiction
Narratives. InAAAIFallSymposium:IntelligentNarrative
Bos,G.1995. JewishTraditionsonStrengtheningMemory
Technologies,87–94.
andLeoneModena’sEvaluation. JewishStudiesQuarterly,
2(1):39–58. Morgan, M. S. 2017. Narrative ordering and explanation.
Studies in History and Philosophy of Science Part A, 62:
Bramsen,P.;Deshpande,P.;Lee,Y.K.;andBarzilay,R.2006.
86–97.
InducingTemporalGraphs. InProceedingsofthe2006Con-
ferenceonEmpiricalMethodsinNaturalLanguageProcess- Mostafazadeh,N.;Chambers,N.;He,X.;Parikh,D.;Batra,
ing,189–198.Sydney,Australia:AssociationforComputa- D.;Vanderwende,L.;Kohli,P.;andAllen,J.2016a.Acorpus
tionalLinguistics. andclozeevaluationfordeeperunderstandingofcommon-
sensestories. InProceedingsofthe2016Conferenceofthe
Chang,A.X.;andManning,C.D.2012. Sutime:Alibrary
North American Chapter of the Association for Computa-
forrecognizingandnormalizingtimeexpressions. InLREC,
tionalLinguistics:HumanLanguageTechnologies,839–849.
volume2012,3735–3740.
Mostafazadeh,N.;Misra,I.;Devlin,J.;Mitchell,M.;He,X.;
Chen, X.; Qiu, X.; and Huang, X. 2016. Neural sentence
andVanderwende,L.2016b. GeneratingNaturalQuestions
ordering. arXivpreprintarXiv:1607.06952.
AboutanImage. InProceedingsofthe54thAnnualMeeting
Demner-Fushman,D.;Chapman,W.W.;andMcDonald,C.J.
oftheAssociationforComputationalLinguistics(Volume1:
2009. Whatcannaturallanguageprocessingdoforclinical
LongPapers),1802–1813.Berlin,Germany:Associationfor
decisionsupport? JournalofBiomedicalInformatics,42(5):
ComputationalLinguistics.
760–772. BiomedicalNaturalLanguageProcessing.
Mostafazadeh,N.;Roth,M.;Louis,A.;Chambers,N.;and
Fan, A.; Lewis, M.; and Dauphin, Y. 2018. Hierarchical
Allen,J.2017. LSDSem2017SharedTask:TheStoryCloze
NeuralStoryGeneration. InProceedingsofthe56thAnnual
Test. InProceedingsofthe2ndWorkshoponLinkingModels
Meeting of the Association for Computational Linguistics
ofLexical,SententialandDiscourse-levelSemantics,46–51.
(Volume1:LongPapers),889–898.
Valencia,Spain:AssociationforComputationalLinguistics.
Genette,G.1983. Narrativediscourse:Anessayinmethod,
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
volume3. CornellUniversityPress.
Bleu:aMethodforAutomaticEvaluationofMachineTrans-
Halliwell,S.;etal.1998. Aristotle’spoetics. Universityof lation. InProceedingsofthe40thAnnualMeetingoftheAs-
ChicagoPress. sociationforComputationalLinguistics,311–318.Philadel-
Holtzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y. phia, Pennsylvania, USA: Association for Computational
2019. TheCuriousCaseofNeuralTextDegeneration. In Linguistics.
InternationalConferenceonLearningRepresentations. Peng,N.;Ghazvininejad,M.;May,J.;andKnight,K.2018.
Huang, T.-H. K.; Ferraro, F.; Mostafazadeh, N.; Misra, I.; Towardscontrollablestorygeneration. InProceedingsofthe
Agrawal,A.;Devlin,J.;Girshick,R.;He,X.;Kohli,P.;Batra, FirstWorkshoponStorytelling,43–49.
D.;Zitnick,C.L.;Parikh,D.;Vanderwende,L.;Galley,M.; Prabhumoye,S.;Salakhutdinov,R.;andBlack,A.W.2020.
andMitchell,M.2016. VisualStorytelling. InProceedings TopologicalSortforSentenceOrdering.InProceedingsofthe
ofthe2016ConferenceoftheNorthAmericanChapterofthe 58thAnnualMeetingoftheAssociationforComputational
AssociationforComputationalLinguistics:HumanLanguage Linguistics,2783–2792.
Technologies,1233–1239.SanDiego,California:Association
Propp,V.2010. MorphologyoftheFolktale,volume9. Uni-
forComputationalLinguistics.
versityofTexasPress.
HuggingFace.2020. NeuralCoref. github.com/huggingface/
Radford,A.;Wu,J.;Child,R.;Luan,D.;Amodei,D.;and
neuralcoref. [Online;accessed29-September-2020].
Sutskever,I.2019. Languagemodelsareunsupervisedmulti-
Kendall, M. G. 1938. A new measure of rank correlation. tasklearners. OpenAIBlog,1(8):9.
Biometrika,30(1/2):81–93.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Kim,N.W.;Bach,B.;Im,H.;Schriber,S.;Gross,M.;and Matena,M.;Zhou,Y.;Li,W.;andLiu,P.J.2020. Explor-
Pfister,H.2017. Visualizingnonlinearnarrativeswithstory ingtheLimitsofTransferLearningwithaUnifiedText-to-
curves. IEEE transactions on visualization and computer TextTransformer. JournalofMachineLearningResearch,
graphics,24(1):595–604. 21(140):1–67.
Lewis,M.;Liu,Y.;Goyal,N.;Ghazvininejad,M.;Mohamed, Raghavan,P.;Fosler-Lussier,E.;Elhadad,N.;andLai,A.M.
A.;Levy,O.;Stoyanov,V.;andZettlemoyer,L.2020. BART: 2014. Cross-narrativeTemporalOrderingofMedicalEvents.
10652
InProceedingsofthe52ndAnnualMeetingoftheAssociation
forComputationalLinguistics(Volume1:LongPapers),998–
1008.Baltimore,Maryland:AssociationforComputational
Linguistics.
Rajani,N.F.;McCann,B.;Xiong,C.;andSocher,R.2019.
Explain Yourself! Leveraging Language Models for Com-
monsense Reasoning. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics,
4932–4942.Florence,Italy:AssociationforComputational
Linguistics.
Ramanujan,A.K.1991. ThreehundredRamayanas:Fiveex-
amplesandthreethoughtsontranslation. ManyRamayanas:
TheDiversityofaNarrativeTraditioninSouthAsia,22–49.
Reichenbach,H.1947. Elementsofsymboliclogic. London:
DoverPublications(1947).
Reichert, D.; Kaufman, D.; Bloxham, B.; Chase, H.; and
Elhadad,N.2010. Cognitiveanalysisofthesummarization
oflongitudinalpatientrecords. AMIA...AnnualSymposium
proceedings.AMIASymposium,2010:667–71.
Schank,R.C.1972. Conceptualdependency:Atheoryof
naturallanguageunderstanding. Cognitivepsychology,3(4):
552–631.
Schleppegrell,M.J.2001.Linguisticfeaturesofthelanguage
ofschooling. Linguisticsandeducation,12(4):431–459.
See,A.;Pappu,A.;Saxena,R.;Yerukola,A.;andManning,
C.D.2019.DoMassivelyPretrainedLanguageModelsMake
BetterStorytellers?InProceedingsofthe23rdConferenceon
ComputationalNaturalLanguageLearning(CoNLL),843–
861.HongKong,China:AssociationforComputationalLin-
guistics.
Wiegreffe,S.;andMarasovic,A.2021. TeachMetoExplain:
A Review of Datasets for Explainable Natural Language
Processing. InThirty-fifthConferenceonNeuralInformation
ProcessingSystemsDatasetsandBenchmarksTrack(Round
1).
Wingate,U.2012. ‘Argument!’helpingstudentsunderstand
whatessaywritingisabout. JournalofEnglishforacademic
purposes,11(2):145–154.
Zhang,T.;Kishore,V.;Wu,F.;Weinberger,K.Q.;andArtzi,
Y.2019.BERTScore:EvaluatingTextGenerationwithBERT.
InInternationalConferenceonLearningRepresentations.
10653

TheThirty-SixthAAAIConferenceonArtificialIntelligence(AAAI-22)
Symbolic Brittleness in Sequence Models:
on Systematic Generalization in Symbolic Mathematics
SeanWelleck,1,2 PeterWest,1 JizeCao,1 YejinChoi1,2
1PaulG.AllenSchoolofComputerScience&Engineering,UniversityofWashington
2AllenInstituteforArtificialIntelligence
wellecks@uw.edu
Abstract Input Integral Prediction
Neural sequence models trained with maximum likelihood 30cos(39x) 10sin(39x) 10sin(39x) ✓
13 13
estimation have led to breakthroughs in many tasks, where 17cos(83x) 17sin(83x) 1 sin(83x) ✗
successisdefinedbythegapbetweentrainingandtestper- 83 17
34cos(77x) 34sin(77x) sin(77x) ✗
formance.However,theirabilitytoachievestrongerformsof 77
generalizationremainsunclear.Weconsidertheproblemof
x209 1 x210 1 x210 ✓
symbolic mathematical integration, as it requires generaliz- 210 210
ingsystematicallybeyondthetestset.Wedevelopamethod- x764 1 x765 1 x765 ✓
765 765
ology for evaluating generalization that takes advantage of x209+x764 1 x210+ 1 x765 1 x205 ✗
the problem domain’s structure and access to a verifier. 210 765 205
Despite promising in-distribution performance of sequence- −241 −241x −239x−14400 ✗
to-sequence models in this domain, we demonstrate chal- 123x 123x 123x ✗
lenges in achieving robustness, compositionality, and out- log(123) 1+log(123)
of-distribution generalization, through both carefully con- 4x+x465+1 x466 +x+ 4x x466 +x+ex ✗
466 log(4) 466
structed manual test suites and a genetic algorithm that au-
tomaticallyfindslargecollectionsoffailuresinacontrollable
Table1: Despiteitsimpressiveabilitytointegrateequations
manner.Ourinvestigationhighlightsthedifficultyofgener-
thatareoutofreachfortraditionalsymbolicsolvers,theneu-
alizingwellwiththepredominantmodelingandlearningap-
ralsequenceintegratorshowsdeficiencesinrobustness(top)
proach,andtheimportanceofevaluatingbeyondthetestset,
acrossdifferentaspectsofgeneralization. andcompositionality(middle),andfailsonadversarialprob-
lemsdiscoveredbySAGGA(bottom).
1 Introduction
Despitetheirsuccess,recentstudiesrevealundesirableprop-
– finding the integral of a mathematical function – specifi-
erties of conventional neural sequence models, such as as-
callyrequirestheseformsofgeneralization,asitinvolvesan
signing high-probabilities to unrealistic sequences (Holtz-
underlyingstructurethatextendsbeyondthisfixedtraining
manetal.2020;Wellecketal.2020),susceptibilitytoadver- R
distribution.Forinstance,therule k = kx+C appliesto
sarial attacks (Wallace et al. 2019), and limited generaliza- R R R
allconstantsk,andthesumrule f + f = (f +f )
tiononsymbolictasks(Saxtonetal.2019;Nogueira,Jiang, 1 2 1 2
meansthatintegratingtwofunctionscorrectlyshouldimply
and Li 2021), even with very large models and datasets
integratingtheirsumcorrectly.Symbolicintegrationalsoof-
(Henighan et al. 2020). Despite these drawbacks, Lample
fers a structured problem domain and a verifier for evalu-
and Charton (2019) recently demonstrated that a standard
ating whether a proposed solution is correct, making it an
sequence-to-sequence model, which we call a neural se-
effectivesettingforevaluatinggeneralization.Astheneural
quenceintegrator,performssurprisinglywellatsymbolicin-
sequenceintegratorreliesonacommonrecipe–alarge-scale
tegration,solvingproblemsthatarebeyondtraditionalsym-
transformertrainedtomaximizethelikelihoodofatraining
bolicsolversandachievingnearperfecttestaccuracy.
setofinput-outputsequences–itisespeciallyinterestingto
Recent studies suggest that achieving strong and sys-
studywhetheritgeneralizessystematically.
tematic generalization is difficult with vanilla sequence-to-
In this paper, we find a discrepancy between the tradi-
sequencemethods,astheylatchontoregularitiesinthetrain-
tional notion of generalization captured by test set accu-
ingdata,learningdataset-specificsolutionsthatdonotgen-
racy and the generalization needed in symbolic mathemat-
eralize beyond the training distribution (e.g. Agrawal, Ba-
ics. While the model’s test accuracy is nearly perfect, we
tra, and Parikh (2016); Lake and Baroni (2018); Bahdanau
find this breaks down when testing its robustness, compo-
et al. (2019); Hupkes et al. (2020)). Symbolic integration
sitionality, and out-of-distribution generalization (e.g. Ta-
Copyright©2022,AssociationfortheAdvancementofArtificial ble 1). We describe a methodology for evaluating these as-
Intelligence(www.aaai.org).Allrightsreserved. pects,byconstructingproblemsetsanddevelopingagenetic
8629
Robustness Compositionality Out-of-Distribution model’s output is considered correct if any of its k candi-
✓ ✓ ✓ dates {yˆ ,...,yˆ } is correct. In this view, the neural net-
1 k
2x42✓ x2 cos(x)2 sin(x)2 Training d✓ istribution work narrows the search space to a small set of candidates
thatarechecked,tradingoffcorrectnessforsearchandveri-
ficationcost.Wedenotecheckingkcandidatesolutionsas,
x2 + sin(x)2 ✗ e5 xx px lo2✗ it (cid:26) 0 x≡ d yˆ foranyi∈1tok,
3x42✗
5x +10+cos(x)sin(x)2+...✗ m(x,f θ(x;k))=
1
otherwdx isei
.
(1)
long problem
In other words, m(·,·) is 1 when the model fails to pre-
Figure1: Illustratingrobustness,compositionality,andout-
dict the correct integral, and 0 when the model succeeds.
of-distributiondeficienciesintheneuralsequenceintegrator.
We measure the proportion of failures on problems X =
{x ,...,x }usingkcandidatesolutionsperproblemas:
1 N
algorithm,SAGGA(SymbolicArchiveGeneratorwithGe- 1 X
Fail@k(f ,X)= m(x,f (x;k)). (2)
netic Algorithms), that automatically discovers diverse and θ N θ
targeted failures. We find that successfully integrating an x∈X
in-distribution problem does not imply success on nearby Fail@kis0whenthemodelcorrectlyintegratesallofthe
problems, despite being governed by the same underlying problems in X, and increases towards 1 as it fails to inte-
rule(robustness).Moreover,themodeloftensucceedsona grate more problems. Evaluating a model’s performance in
collectionofproblemswithoutbeingabletosystematically theMAPsettingcorrespondstoevaluatingFail@1,while
compose those problems (compositionality), and struggles thesearch-and-verifysettingwithabudgetofk > 1candi-
to generalize to longer problems, larger values, and func- datesusesFail@k.Weomitkinf (x;k)unlessnecessary.
θ
tionsnotcoveredintraining(out-of-distribution).
In addition to the model’s approximate mode being in- 2.1 ExperimentStructure
correct–i.e.themostprobablesequencereturnedbybeam Westructureourinvestigationintothreeparts(Figure1).We
search – the deficiencies are present deeper into its ranked begin close to the model’s training distribution, evaluating
listofcandidatesolutions,impactingthemodel’seffective- robustnesstosmallperturbationsofin-distributionproblems
ness in a search-and-verify setting. Overall, our investiga- andsimplefunctions.Wethenaskwhetherlearningtointe-
tion highlights the difficulty of achieving robustness, com- grate a collection of functions implies that the model can
positionality,andout-of-distributiongeneralizationwiththe integrate a composition of those functions. Finally we de-
predominant modeling and learning approach, and the im- partfromthetrainingdistributionbystudyingextrapolation
portanceofevaluatingbeyondthetestset,acrossaspectsof to larger problems and values, then by finding adversarial
generalizationthatarerequiredbythetaskathand. exploitsthatexposegapsinthetrainingdistribution.
2 ProblemSetup Experimentalsetup. Weusetheimplementationandpre-
trainedmodelfromLampleandCharton(2019)forallofour
Symbolicintegrationistheproblemoffindingtheintegraly
experiments,specificallytheFWD+BWD+IBPmodelwhich
ofaninputequationx.Forinstance,x2/2istheintegralof
obtainedtop-10accuraciesof95.6%,99.5%,and99.6%on
x,uptoanadditiveconstant.
theirpubliclyavailabletestsets.1Ourevaluationisbasedon
Neuralsequenceintegrator. LampleandCharton(2019) theircode,weusetheirutilitiesforinputsandoutputs,and
framesymbolicintegrationasasequence-to-sequenceprob- bydefaultusebeamsearchwithbeam-size10.Followingthe
lem. In this view, input and output equations x and y are authors,weuseSympytocheckwhetherthederivativeofa
prefix-notation sequences. The neural sequence integrator prediction is equal to the original problem. We generously
uses a 6-layer transformer (Vaswani et al. 2017) to model count the prediction as correct if a timeout occurs. See the
the distribution p (y|x) = QTy p (y |y ,x) by train- Apppendixforadditionaldetails.
θ t=1 θ t <t
ing the model to maximize the log-likelihood of a set of
P 2.2 AutomaticProblemDiscoveryWithSAGGA
training problems, argmax logp (y|x). Given
θ (x,y)∈D θ
a trained model and input x, a set of predicted solutions Automaticallyfindingproblemsthatexposedeficienciesre-
rankedbyamodelscoreisobtainedbybeamsearch,denoted quiresanon-differentiablecost(Equation1),satisfyingcon-
{yˆ ,...,yˆ } = f (x;k,b), where b is beam size and k is straintsforvalidequations,andfindingdiverseproblemsets
1 k θ
the number of candidates saved for evaluation. For brevity to characterize each aspect of generalization. To address
weomitbinthediscussionunlessnecessary. these challenges, we develop SAGGA (Symbolic Archive
Generation with Genetic Algorithms), a gradient-free ge-
Evaluation. The standard practice is to evaluate a candi-
neticalgorithmwhichiterativelyfindsdiversefailures.
dateyˆ bycheckingwhetherthederivativeofyˆ isequivalent
Ateachiteration,SAGGAmutatesaseedsetofproblems
toxusingasymbolicsolver(e.g.Sympy).Inthemaximum-
bymodifyingeachproblem’sequationtree,ensuringthatthe
a-posteriori(MAP)setting,themodel’soutputisconsidered
correct if its top-ranked candidate yˆ
1
is correct. This crite- 1https://github.com/facebookresearch/SymbolicMathematics/,
rion is relaxed in the search-and-verify setting, where the commit4596d07.
8630
Algorithm1:SAGGA.Eachseedproblemdenoted Type Test Fail@50 Fail@10 Fail@1
asxˆ,mutatedproblemasx˜,archivedproblemasx. coeff k ln(k x) 0.0 0.0 0.0
1 2
Parameters:FitnessF(f θ,x)→R, k 1x 0.0 0.0 0.0
mutateandseedstrategies,archivesizeN. k 1x42 0.0 6.1 45.5
Output:ProblemarchiveD ={x 1,...,x N}. k 1exp(k 2x) 15.4 20.8 30.3
D =∅ // initial archive k sin(k x) 6.6 19.6 29.7
1 2
xˆ(0) =seed(D,∅) // initial seed k 1cos(k 2x) 10.6 20.7 28.2
1:M k tan(k x) 13.9 17.4 24.2
while|D|<N do 1 2
// generate mutations coeff 1/k·f 5.9 12.0 13.7
x˜(i) =mutate(xˆ(i) ) coeff k·f 5.4 5.8 16.3
1:M′ 1:M +exp f +ex 0.9 1.6 3.3
// select problems by fitness
+ln f +ln(x) 1.9 3.2 5.3
x(i) =select(F,x˜(i) )
1:M′′ 1:M′
// archive selected problems Table2: Robustnessresultswithsimpleprimitives(top)and
D =D∪x(i) validationproblemsf whichthemodelcorrectlyintegrates
1:M′′
(bottom).Coefficientsaresampledfrom[1,100].
// choose next seed
xˆ( 1i :+ M1) =seed(D,F,x˜( 1i :) M′) Input Integral Prediction
30cos(39x) 10sin(39x) 10sin(39x) ✓
13 13
17cos(83x) 17sin(83x) 1 sin(83x) ✗
83 17
resultingcandidatesarevalidequations.Thecandidatesare 34cos(77x) 34sin(77x) sin(77x) ✗
scored by a fitness function – i.e. according to whether the 77
neuralsequenceintegratorfailstointegratetheproblemand
26x42 26x43 26x43 ✓
43 43
otherdesiredconstraints–andthehighest-fitnesscandidates 88x42 88x43 8x43 ✗
43
are saved in a problem archive. The next seed set is then 53x42 53x43 (x44+x)/x ✗
43
formed to balance diversity and fitness, by clustering can-
didates and selecting the highest-fitness members of each
Table 3: Robustness examples. We show the model’s top
cluster.SAGGAcontinuesuntilthearchivecontainsatarget prediction(beamsearch,size10).Notethat(x44+x)/x =
numberofproblems.Algorithm1summarizesSAGGA. x43+1;itsderivativeis43x42andishenceincorrect.
SAGGAofferscontroloverthetypesofproblemsthatit
discovers through its seed problems, fitness function, and
mutation strategy. We detail our choices for each kind of 3.1 ManuallyTestingRobustness
generalizationintheirrespectivesections,andshowdefault To define nearby problems, we first consider manual tem-
settingsandfurtherimplementationdetailsintheAppendix. plateswhichminimallyperturbaproblemf,e.g.
3 RobustorBrittle? k·f, f +lnx, ...
First, we study whether the model’s strong test-set perfor- These problems are nearby f in the sense that a single op-
manceadequatelyrepresentsitsrobustness.Robustnesstells eration is added to the problem’s equation tree, or a small
us whether the integration model systematically solves all numberofnodevaluesarechangedinthetree.
problems in a neighborhood governed by a generalizable
pattern; for instance a model that solves R 26x42 should Brittlenessonsimpleprimitivefunctions. Wefirstinves-
solve R 53x42. We study problems that are nearby to those tigate whether the neural sequence integrator is robust on
simple primitive functions, since they make up more com-
fromtheoriginaltestdistribution,aswellastosimpleprim-
plicated functions and are frequently entered by real-world
itivefunctionsthatofferfine-grained,interpretablecontrol.
users.Weuseamanualneighborhoodwhichyields,
Arobustmodelisstabletosmallperturbationsininput,
meaningthatitgetsnearbyproblemsx˜correctwhenitgetsa X ={k ln(k x), k exp(k x), k x, k x42,
N 1 2 1 2 1 1
problemxcorrect.Formally,letX ={x ,...,x }contain
problemsthatthemodelgetscorrect,P 1 m(xN
,f (x))=
k 1sin(k 2x), k 1cos(k 2x), k 1tan(k 2x)},
x∈X θ
0, and let N (x) be a set of problems that are nearby to x wherek ∼U(a,b)andk ∼U(a,b)arerandomlysampled
d 1 2
according to a distance d(x,x˜). We measure robustness by coefficients from a range (a,b). We use [0,100] which is
measuringfailuresonnearbyproblems, covered by the training distribution and evaluate on 1,000
k ,k pairssampledwithoutreplacementforeachprimitive.
Fail@k(f ,X ), (3) 1 2
θ N Table 2 shows the results. On a positive note, the neu-
S
where X = N (x). We measure this quantity by ral sequence integrator is robust on the primitives k x and
N x∈X d 1
varying(i)theneighborhoodN d(x)usedtogeneratenearby k 1ln(k 2x). The integral of k 1x is k 21x2, so the model
problems,and(ii)theseedproblemsX toconsider.Below, learnedtodivideby2forthesecases.Theintegraloflnin-
wewillrefertoaproblemasxorf interchangeably. volvescopyingthecoefficientsintoacorrecttemplate(that
8631
R
is, k 1ln(k 2x) = k 1x(ln(k 2x) − 1)), and the neural se- Seed Cluster1 Cluster2 Cluster3
quenceintegratorlearnedthisbehavior.
X −104 2x42+22 −47+2/x−2/x71
On the other hand, the model is surprisingly brittle on poly
−136 2x42+28 −47+2/x−31/x71
theotherprimitives.Theserequiredividingcoefficients(e.g.
R thek 1 mc oo ds e( lk h2x as) n= otpk k e1 2 rs fein c( tlk y2 lx e) a) r. nT edhe thf eai rl eu qr ue irr ea dte ds ivh io sw ios nt bh ea -t X trig 13c− o3 s3 19x 132 cox s42 83+ x6 −8 59 −7 11 0+ sin3 46 7/ xx c+ os2 2/ xx71
13cos83x 17cos37x−49 10sin90xcos2x
havior.Moreover,despitelearninga‘divisionby2’rulefor
17cos47x 17cos41x−45 19sin90xcos2x
integratingk x,theneuralsequenceintegrator’sfailureson
1
k x42 indicate that it did not perfectly learn an analogous
1 Table 4: Example robustness problems discovered by
‘divisionby43’rule.Table3showsexamples.
SAGGA which the neural sequence integrator fails to inte-
Testaccuracydoesnotimplyrobustness. Next,wewant grate.
to see whether the neural sequence integrator’s strong test
accuracy implies that it is robust on test problems. We use
the validation set, and perturb validation problems that the x Raw Simplified Deriv.
modelcorrectlyintegratesusingtheneighborhoods, −104 x2+2x−(x+25)2 −48x−625 −48
−136 x2−x(x+130)+2x −128x −128
1
X N1 ={ kf, k·f}, X N2 ={f +ex, f +ln(x)}, −33 x2+x−(x+16)2 −31x−256 −31
where k ∼ U(1,100). The first set multiplies the function Table5:Therawmodelpredictionsfortheproblemsxand
byaconstant,whilethesecondaddsasingleprimitive. theirsimplifiedforms.Eachpredictionisincorrectsinceits
Table2showstheresults.Despiteachievingperfectaccu- derivativeisnotequaltox.Theneuralsequenceintegrator’s
racyontheoriginalproblems,themodelfrequentlyfailsun- raw output is long and varied compared to the underlying
dertheslightperturbations.Thelocalneighborhoodaround integrationruleR k =kx+C.
validation examples reveals deficiencies in robustness that
arenotevidentfromvalidationperformancealone,aligning
withfindingsinNLPtasks(Gardneretal.2020). Discovering brittleness near validation problems. Fi-
nally,weuseSAGGAtodiscoverdifficultproblemsthatare
3.2 AutomaticallyFindingRobustnessFailures closetoatargetsetX–inourcasevalidationproblems–ac-
Next,weuseSAGGAtoautomaticallydiscoverrobustness cordingtoanexplicitdistanced(x,x˜).Thisallowsforless
failuresintheneighborhoodofaseedsetofproblems. hand-designingoftheperturbations.
Specifically,wedefineafitnesswhichishighwhenevera
Discoveringbrittlenessnearsimpleproblems. First,we candidateisclosetoanyprobleminatargetsetX,
run SAGGA and only allow it to mutate leaves in a prob-
(cid:20) (cid:21)−1
lem’sequationtreeintoarandominteger.Theproblemsare
fitness(x˜)= mind(x,x˜) ·m(x˜,f (x˜)). (4)
nearbyinthesensethatthetree’sstructureisnotchanging, θ
x∈X
only a small number of its leaf values. We use SAGGA to
mutate the leaves of seed sets of 9 polynomials X and We randomly sample 10 validation problems to form X,
poly
9trigonometricfunctionsX ,whicharelistedintheAp- setSAGGA’sinitialseedtoX,andusecosinesimilarityof
trig
pendix.WerunSAGGAuntilitdiscovers1000failingprob- SciBERT vectors to define the distance d(x,x˜). Since the
lems,thenclustertheseusingk-meansonSciBERTembed- distance now constrains the problems, we are free to use a
dings(Beltagy,Lo,andCohan2019)ofeachproblem. widersetofmutations:changinganode’soperation,adding
Table4showsthreemembersfromthreediscoveredprob- anargumenttoanode,andreplacingthenodewitharandom
lem clusters, for the polynomial and trigonometric seeds. constant,symbol,orsimpleoperation.
Intuitively, each cluster shows failures in a neighborhood Table6showsexampleproblemsthatSAGGAdiscovers
around a prototypical problem – for instance, on 2x42 +k aroundthesuccessfulvalidationproblems,exposingawider
theneuralsequenceintegratorcorrectlyintegrates2x42+21, classofrobustnessfailuresthanourprecedingexperiments.
butnottheproblemsinCluster2(e.g.2x42+22).
4 IntegratingPartsButNotTheWhole
Curiously, each problem in a neighborhood is gov-
erned by a common template – e.g. the problems While the preceding section identified weaknesses in ro-
R
{−104,−136,−33} are governed by k = kx + C, yet bustness – for instance, integrating 26x42 but not 88x42 –
the failures suggest that the neural sequence integrator has a remaining question is whether successfully integrating a
either not inferred the template, or does not apply it across collection of primitives implies that a composition of those
theneighborhood.Toinvestigatethisphenomenon,weshow primitiveswillbesuccessfullyintegrated.
therawmodelpredictioninTable5,alongwithitssimplified Compositionality refers to forming compound equations
versionandderivative.Comparedtotheunderlyingtemplate from known primitives and operations. A compositional
R
k =kx+C themodel’srawoutputislongandcomplex. modelshouldcorrectlyintegrateequationsoftheform,
In contrast, the simplified version is short; we hypothesize
f =f ◦f ◦···◦f , (5)
thisgapmakesadheringtothetemplatedifficult. 1 2 k
8632
ValidationProblem NearbyFailures Type Test Fail@50 Fail@10 Fail@1
−x2+x+log(4)tan(x) −x2+x+log(4)tan(17x2) exp(1) f
1
00.0 00.0 00.0
−x2+x+log(4)tan(2x2) exp(2) f 1+f
2
70.8 72.4 84.9
−x2+x+log(4)tan(63/x2) exp(3) f 1+f 2+f 3 91.3 97.5 99.5
√ p exp(4) f 1+...+f 4 86.2 97.4 99.8
3x+3−2 −86x2 +62/x−40
p 14+62/x+4 coeff(1) f 1 00.0 00.0 00.0
p 14+62/x−2 coeff(2) f 1+f 2 8.60 16.2 29.2
coeff(3) f +f +f 23.8 37.5 61.0
1 2 3
tan(exp(2))/18x tan(exp(2+71/x))/18x coeff(4) f +...+f 23.1 38.7 60.0
1 4
tan(exp(2−46x))/18x
tan(exp(37x))/18x valid(1) f 1 00.0 00.0 00.0
valid(2) f +f 6.80 14.5 15.0
1 2
valid(3) f +f +f 21.5 36.5 43.6
Table6: SAGGAdiscoversfailuresaroundsuccessfulval- 1 2 3
valid(4) f +...+f 52.5 69.0 79.2
idation problems, within a neighborhood defined by an ex- 1 4
plicitdistance.
Table 8: Compositionality. Top: successful simple prim-
itives from the robustness experiments (Table 2). Bot-
tom: successful validation-set primitives. Despite integrat-
Input Prediction
ing each primitive, the model struggles to integrate their
x1/3 3x4/3 ✓ sums.
4
x1/606 606x66 00 67 ✓
607
x1/3+x1/606 3x5 3 + 6 x6 61 03 6 ✗ Nodes Fail@10 Fail@1
5 613
x209 1 x210 ✓ 1-15 0.4 1.6
210
x764 1 x765 ✓ 20 1.9 10.7
765 25 7.2 17.2
x209+x764 1 x205 ✗
205 30 24.4 37.1
14cos(58x) 7 sin(58x) ✓ 35 49.0 59.2
29
46cos(84x) 23sin(84x) ✓
42 Table 9: Extrapolation to more operator nodes under the
14cos(58x)+46cos(84x) sin(59x)cos(x) ✗
trainingdatagenerationprocess.Trainingused1-15nodes.
Table7: Compositionalityexamples.Weshowthemodel’s
itives.Asthenumberofcompositionsincreases,thefailure
topprediction(beamsearch,width10).Themodelsuccess-
rateincreasestowards100%.Table7showsexamples.
fullyintegratestheindividualprimitives,butnottheirsum.
Succeeding on test problems, failing on their sum. We
performasimilarexperimentusingsuccessfulvalidation-set
functions. We filter examples longer than 20 tokens so that
where f ,...,f are equations that the model successfully
1 k composedequationsarewithinthetrainingdomaininterms
integrates, and ◦ is a binary operation (e.g. addition). For
oflength,andsample1000compoundequationsf +...+
instance,asystemthatintegratesx2andcosxandiscapable 1
f for k ∈ {2,3,4}. As seen in Table 8, the failure rate
ofadditionshouldsuccessfullyintegratex2+cosx. k
grows as the number of compositions increases, similar to
Formally, we say that a model is k-compositional with the simple primitives case. Maximizing the likelihood of a
respect to equations X and operation ◦ when it success- largetrainingsetdidnotyieldacompositionalmodel.
fully integrates any combination of k equations from X,
P x∈X˜ m(x,f θ(x))=0,whereX˜ ={f 1◦···◦f k|f i ∈X}. 5 DepartingFurtherFromTraining
We evaluate k-compositionality with respect to addition,
Theprecedingexperimentsfoundproblemsthatwerenearby
using simple primitive functions and validation problems.
R R R to, or composed directly from, in-distribution examples. In
Asintegrationislinear, (f+g)= f+ g,composition-
thissection,wedeliberatelymovefromthemodel’straining
alitywithrespecttoadditionisareasonablerequirement.
distributiontoevaluateitsout-of-distributiongeneralization.
First, we study extrapolation to longer equation sizes than
Succeeding on simple primitives, failing on their sum.
those in its training distribution, and to integer ranges that
Wecollectsimpleprimitivesfromthecoefficientrobustness
are only sparsely covered in the training set. Then we use
experimentsthatthemodelsuccessfullyintegrates(coeff),
SAGGAtoexposeexoticfailuresandrevealproblemclasses
and successful exponents xc or x1/c, c ∈ [0,1000] (exp).
thatwerenotcoveredduringtraining.
Werandomlysample1000compoundequationsf +...+f
1 k
fork ∈{2,3,4}andevaluatethefailurerate.Table8shows Longer problems are more difficult. First, we use the
theresults.Addingtwoprimitivesgivesfailureratesof29% same data-generating process as for training, but vary its
and 85% for coefficient and exponent primitives, respec- parameterstodepartfromthetrainingdistribution.Specifi-
tively,despitefailing0%ofthetimeontheindividualprim- cally,wetestextrapolationonnumberofoperatornodesin
8633
Cluster1 Cluster2 Cluster3 Cluster4
119x −240x+2cos2x −100xx 158xx2 +611
132x −398x+2cos2x −149xx 256xx2 +191
136x −692x+2sin2x −151xx 332xx2 +559
Table11: SAGGAdiscoversmanyfailuresthatinvolvexin
anexponent.
k Unresolved@k
1 91.6%
10 65.2%
Figure2: Integerextrapolation.Failureratesforintegrating
simpleprimitiveswithcoefficientsfromthespecifiedrange. Table 12: Percentage of failures on the FWD validation set
in which the ground truth y is scored lower than the top
∗
beamcandidate(Unresolved@1)orthebottombeamcandi-
Problem Exploit date (Unresolved@10), meaning that perfect search would
leavethefailuresatlevelkunresolved.
169sin(4x)/x UsesSi(·).
−2sin(42/x) UsesCi(·).
−2sin(185x2)cos(2x) UsesFresnelS,Cintegrals. whoseintegralisexpressedusingtheGammafunctionΓ(·),
357x2x
+2sin(2x) UsesincompletegammaΓ(a,x).
orthecosineintegralCi,whicharenotincludedinitstrain-
1/(x48(3x+2)49) Decodingdoesnotterminate. ingdata(Table10).2Theseexamplesareareminderthatthe
sequence-to-sequenceparadigmdetermineswhichfunctions
are‘builtin’byinclusionintrainingdata;omittedbehavior
Table 10: Exploits discovered by SAGGA whose integrals
isleftunspecified,leavingitsusceptibletoexploits.
useout-of-domainfunctions.
Finally, the last problem in Table 10 caused the neural
sequence integrator to enter a non-terminating loop during
decoding, a known idiosyncrasy of autoregressive models
eachequationtree,usingLampleandCharton(2019)’sdata withbeamsearch(Wellecketal.2020).
generationprocessandvaryingthemax opsparameter.Ta- SAGGAalsofindsmanyclustersthatindicatetheneural
ble 9 shows performance when max ops is increased past sequence integrator struggles when x appears in an expo-
themodel’strainingdomain(1-15).Theneuralsequencein- nent.ThediscoveredproblemsinTable11areamicrocosm
tegratordoesshowsomeextrapolationtoequationtreeswith of our previous findings: For the first cluster, we manually
more operator nodes than it was trained on, but its failure found a nearby problem, 30x, that the model gets correct;
rateincreasessubstantiallyasthenumberofnodesincreases. thisclusterisarobustnessfailure.Thesecondclustershows
how such failures cascade further as the function is com-
Larger failures on larger digits. Next, we study per- posed. The final two clusters involve xx or xx2, which do
formance as integer values increase, quickly going out-of-
nothaveanalyticalintegrals;3theseclustersareexploits.
domain. Considering a sample of 200,000 sequences from
the training distribution, 99.4% of the positive integers
6 IsItASearchProblem?
werebetween1and100.Otherrangeswerenon-emptybut
sparselyrepresented;forinstance,0.2%oftheintegerswere Both the experiments of Lample and Charton (2019) and
between100and200,and0.096%between1,000and2,000. our own generate candidate solutions from a sequence-to-
Figure2showsperformanceonprimitivefunctionswithco- sequencemodelusingbeamsearch.Thisraisesthepossibil-
efficientsfromthespecifiedrange.Asintherobustnessex- itythatfailuresareduetosearchratherthanthemodel:what
periments, the x and ln primitives perform well, showing ifthehighestscoringsequencesarecorrect,butnotfound?
that there is some ability to use large numbers. However, Specifically, we want to distinguish between search er-
performanceseverelydegradesfortheexp,sin,cos,tan rors,whichoccurwhenp θ(y ∗|x) ≫ p(y|x)butthesearch
primitives as the coefficient magnitudes increase, reaching algorithm(e.g.beamsearch)doesnotreturny ∗,andmodel
near100%failureratesonlargecoefficients. errors,meaningp θ(y|x)≫p(y ∗|x).
Discovering unsupported functionality. Next, we run 6.1 TheModelIsDeficient:ModelErrors
SAGGAinanunconstrainedsettingwithallmutationtypes,
We study simple-robustness and in-distribution problems,
favoring short problems using the fitness, F(f ,x) =
θ and find evidence of model deficiencies that would remain
m(x,f (x))· 1 ,whichispositivewhenthemodelreturns
θ |x| unresolvedwithperfectsearch.
anincorrectintegralforx,andhigherforshorterproblems.
SAGGAdiscoversexploitsbasedontheneuralsequence 2https://en.wikipedia.org/wiki/Trigonometric integral.
integrator’s limited training distribution, such as problems 3https://www.wolframalpha.com/input/?i=integral+of+x**x
8634
Symbolic mathematics and sequence models. Several
works study extrapolation to longer sequences and larger
digits in synthetic arithmetic and basic mathematics tasks
(Zaremba and Sutskever 2014; Trask et al. 2018; Saxton
etal.2019;Nogueira,Jiang,andLi2021).Sequencemodels
havealsobeenappliedtopolynomialrewriting(Piotrowski
et al. 2019; Agarwal, Aditya, and Goyal 2021), and differ-
ential system stability (Charton, Hayat, and Lample 2021).
Forsymbolicintegration,Davis(2019)arguethattheneural
sequence integrator’s test performance should be qualified,
thoughwithoutanempiricaldemonstration.
Systematic generalization. Several works identify diffi-
culties with modern methods on synthetic tasks (e.g. Lake
Figure3:Probabilitiesassignedtothetop-rankedbeamcan-
and Baroni (2018); Bahdanau et al. (2019); Hupkes et al.
didate (max) versus correct candidates (correct) with a
(2020); Kim and Linzen (2020)) and machine translation
largesearch&verificationbudget(500candidates).
(Raunak et al. 2019), with a focus on compositionality and
extrapolation.Somemethodsaddresssystematicitywithin-
ductivebiasesinmodelstructure(Andreasetal.2016;Bah-
Robustness. First, we study the simple-primitive robust-
danau et al. 2019), and others through the data (Hill et al.
nessproblems(e.g.k exp(k x),see2),astheseshortprob-
1 2
2020; Andreas 2020) or learning procedure (Lake 2019;
lemsresultedinasmallnumberoftimeouts,allowingusto
Vanietal.2021).Wefocusonsystematicgeneralizationde-
scaleupsearchandverification.Weincreasethebeamsize
ficienciesinastate-of-the-artmodelinanewsetting–sym-
to500candidates,andstudythemodel’sprobabilitiesonthe
bolicintegration–withadditionalaspectsofgeneralization.
500returnedcandidatesandcorrectsolutions.
When a correct solution is within the 500 returned can- Robustness and adversaries in sequence models. Sev-
didates,thecorrectsolutionoftenhasmuchlowerprobabil- eralworksstudyrobustnessinNLP,includingclassification
itythanthetopcandidate,p (y(1) |x) ≫ p(y |x).Specif- (Tuetal.2020),wordsubstitutions(Jiaetal.2019),domain
θ beam ∗
ically, correct solutions often appear at the bottom of the shift in QA (Kamath, Jia, and Liang 2020) or topic distri-
candidates(3,orange),yetonaveragethebottomcandidate butions(Orenetal.2019).Severalmethodsfindadversarial
y(500)hasprobability≈0.0000035,whilethetopcandidate examplesinNLP(Morrisetal.2020).Alzantotetal.(2018)
beam use genetic algorithms in a classification setting, while we
y(1) hasprobability≈0.92.Thesearemodeldeficiencies:
beam considergeneration.Micheletal.(2019)constraininputse-
themodelisconfidentlyincorrect,assigningveryhighprob-
quences to be similar and use a gradient-based attack to
abilitytoanincorrectsolutionatthetop,andverylowprob-
swaptokens.Wefaceanon-differentiablecostandgenerate
abilitytocorrectsolutions.
largecollectionsoffailureswithawideclassofmutations.
When a correct solution is not within the top 500 candi-
dates,themodelisagainconfidentlyincorrect,withthetop 8 Conclusion
candidatey(1) receiving≈0.94probability.Improvingthe
beam Westudygeneralizationinsymbolicmathematicsusingthe
searchalgorithm–e.g.byfurtherincreasingthesearchbud-
predominant modeling paradigm: a large-scale transformer
getorusinganalternativetobeamsearch–wouldinevitably
trainedwithmaximumlikelihood.Wefinddeficienciesthat
return a low probability solution, as the 500 candidates al-
are not captured by test accuracy, including brittleness to
ready cover more than 99.4% of the probability mass. The
small perturbations, difficulty composing known solutions,
findingsagainpointtomodelerrors.
and gaps in the training distribution. We offer speculations
In-distribution. Next, we study in-distribution problems based on our results. Due to the large space of equations,
fromtheFWDvalidationsetofLampleandCharton(2019). practicalempiricaldistributionsdonotprovideadensesam-
On failure cases, we test if the ground-truth y
∗
is scored pling of individual problem types (e.g. k 1cos(k 2x)), and
above the top k beam candidates, meaning that failure@k each empirical sample contains shared biases from the un-
mightberesolvedwithperfectsearch–y wasscoredmore derlying data generator (e.g. integer values, lengths). Thus,
∗
highlybutwassimplynotfound.Asseenin12,themajority sparse test sets do not adequately measure systematic gen-
offailures–91.6%forfailure@1and65.2%forfailure@10 eralization. From a learning perspective, generic networks
–wouldremainunresolvedwithperfectsearch. trained with SGD do not necessarily favor the simplest hy-
pothesistoexplainthedata;thusasparsetrainingsetyields
anunderconstrainedhypothesisspace,withhypothesesthat
do not strongly generalize (e.g. Table 5), causing behavior
7 RelatedWork
thatbreakssimplerules(e.g.adheringtoatemplateorfol-
Inthiswork,westudysystematicgeneralizationinsequence lowingthesumrule).Wesuspectthatinductivebiases–e.g.
models applied to symbolic integration, in terms of robust- encodedthroughthetrainingdistribution,architecturalcom-
ness,compositionality,andextrapolation,anddevelopage- ponents,orlearningalgorithm–areneededtonarrowthehy-
neticalgorithmforbuildingadversarialproblemsets. pothesestothosethatstronglygeneralize.
8635
References Hill, F.; Lampinen, A.; Schneider, R.; Clark, S.; Botvinick,
M.;McClelland,J.L.;andSantoro,A.2020. Environmen-
Agarwal,V.;Aditya,S.;andGoyal,N.2021. Analyzingthe
tal drivers of systematicity and generalization in a situated
NuancesofTransformers’PolynomialSimplificationAbili-
agent. In International Conference on Learning Represen-
ties. arXiv:2104.14095.
tations.
Agrawal,A.;Batra,D.;andParikh,D.2016. Analyzingthe
Behavior of Visual Question Answering Models. In Pro- Holtzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y.
ceedings of the 2016 Conference on Empirical Methods in 2020. The Curious Case of Neural Text Degeneration. In
Natural Language Processing, 1955–1960. Austin, Texas: InternationalConferenceonLearningRepresentations.
AssociationforComputationalLinguistics. Hupkes,D.;Dankers,V.;Mul,M.;andBruni,E.2020.Com-
Alzantot, M.; Sharma, Y.; Elgohary, A.; Ho, B.-J.; Srivas- positionality Decomposed: How do Neural Networks Gen-
tava,M.;andChang,K.-W.2018. GeneratingNaturalLan- eralise? JournalofArtificialIntelligenceResearch.
guage Adversarial Examples. In Proceedings of the 2018 Jia, R.; Raghunathan, A.; Go¨ksel, K.; and Liang, P. 2019.
Conference on Empirical Methods in Natural Language CertifiedRobustnesstoAdversarialWordSubstitutions. In
Processing,2890–2896.Brussels,Belgium:Associationfor Proceedings of the 2019 Conference on Empirical Meth-
ComputationalLinguistics. ods in Natural Language Processing and the 9th Interna-
Andreas,J.2020. Good-EnoughCompositionalDataAug- tional Joint Conference on Natural Language Processing
mentation. In Proceedings of the 58th Annual Meeting of (EMNLP-IJCNLP), 4129–4142. Hong Kong, China: Asso-
theAssociationforComputationalLinguistics,7556–7566. ciationforComputationalLinguistics.
Online:AssociationforComputationalLinguistics. Kamath,A.;Jia,R.;andLiang,P.2020. SelectiveQuestion
Andreas,J.;Rohrbach,M.;Darrell,T.;andKlein,D.2016. AnsweringunderDomainShift. InProceedingsofthe58th
Neuralmodulenetworks. InProceedingsoftheIEEECom- Annual Meeting of the Association for Computational Lin-
puter Society Conference on Computer Vision and Pattern guistics,5684–5696.Online:AssociationforComputational
Recognition. ISBN9781467388504. Linguistics.
Bahdanau,D.;Murty,S.;Noukhovitch,M.;Nguyen,T.H.; Kim, N.; and Linzen, T. 2020. COGS: A Compositional
De Vries, H.; and Courville, A. 2019. Systematic general- GeneralizationChallengeBasedonSemanticInterpretation.
ization: What is required and can it be learned? In 7th In- InProceedingsofthe2020ConferenceonEmpiricalMeth-
ternationalConferenceonLearningRepresentations,ICLR odsinNaturalLanguageProcessing(EMNLP),9087–9105.
2019. Online:AssociationforComputationalLinguistics.
Beltagy, I.; Lo, K.; and Cohan, A. 2019. SciBERT: A Pre- Lake, B.; and Baroni, M. 2018. Still Not Systematic After
trained Language Model for Scientific Text. In Proceed- AllTheseYears:OntheCompositionalSkillsofSequence-
ingsofthe2019ConferenceonEmpiricalMethodsinNat- To-SequenceRecurrentNetworks. Iclr2018.
ural Language Processing and the 9th International Joint
Lake, B. M. 2019. Compositional generalization through
Conference on Natural Language Processing (EMNLP-
metasequence-to-sequencelearning. InAdvancesinNeural
IJCNLP), 3615–3620. Hong Kong, China: Association for
InformationProcessingSystems.
ComputationalLinguistics.
Lample, G.;and Charton, F. 2019. Deep learning for sym-
Charton, F.; Hayat, A.; and Lample, G. 2021. Learn-
bolicmathematics. arXivpreprintarXiv:1912.01412.
ing advanced mathematical computations from examples.
arXiv:2006.06462. Michel,P.;Li,X.;Neubig,G.;andPino,J.2019.OnEvalua-
tionofAdversarialPerturbationsforSequence-to-Sequence
Davis, E. 2019. The Use of Deep Learning for Sym-
Models. In Proceedings of the 2019 Conference of the
bolicIntegration:AReviewof(LampleandCharton,2019).
North American Chapter of the Association for Computa-
arXiv:1912.05752.
tional Linguistics: Human Language Technologies, Volume
Gardner, M.; Artzi, Y.; Basmov, V.; Berant, J.; Bogin, B.; 1(LongandShortPapers),3103–3114.Minneapolis,Min-
Chen,S.;Dasigi,P.;Dua,D.;Elazar,Y.;Gottumukkala,A.; nesota:AssociationforComputationalLinguistics.
Gupta,N.;Hajishirzi,H.;Ilharco,G.;Khashabi,D.;Lin,K.;
Morris,J.;Lifland,E.;Yoo,J.Y.;Grigsby,J.;Jin,D.;andQi,
Liu,J.;Liu,N.F.;Mulcaire,P.;Ning,Q.;Singh,S.;Smith,
Y.2020. TextAttack:AFrameworkforAdversarialAttacks,
N. A.; Subramanian, S.; Tsarfaty, R.; Wallace, E.; Zhang,
Data Augmentation, and Adversarial Training in NLP. In
A.;andZhou,B.2020. EvaluatingModels’LocalDecision
Proceedingsofthe2020ConferenceonEmpiricalMethods
BoundariesviaContrastSets.InFindingsoftheAssociation
in Natural Language Processing: System Demonstrations,
for Computational Linguistics: EMNLP 2020, 1307–1323.
119–126. Online: Association for Computational Linguis-
Online:AssociationforComputationalLinguistics.
tics.
Henighan, T.; Kaplan, J.; Katz, M.; Chen, M.; Hesse, C.;
Nogueira, R.; Jiang, Z.; and Li, J. J. 2021. Investigating
Jackson, J.; Jun, H.; Brown, T. B.; Dhariwal, P.; Gray, S.;
theLimitationsoftheTransformerswithSimpleArithmetic
Hallacy,C.;Mann,B.;Radford,A.;Ramesh,A.;Ryder,N.;
Tasks. ArXiv,abs/2102.13019.
Ziegler,D.M.;Schulman,J.;Amodei,D.;andMcCandlish,
S.2020. ScalingLawsforAutoregressiveGenerativeMod- Oren, Y.; Sagawa, S.; Hashimoto, T. B.; and Liang, P.
eling. ArXiv,abs/2010.14701. 2019. Distributionally Robust Language Modeling. In
8636
Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), 4227–4237. Hong Kong, China: Asso-
ciationforComputationalLinguistics.
Piotrowski, B.; Urban, J.; Brown, C. E.; and Kaliszyk, C.
2019. Can Neural Networks Learn Symbolic Rewriting?
CoRR,abs/1911.04873.
Raunak, V.; Kumar, V.; Metze, F.; and Callan, J. 2019. On
Compositionality in Neural Machine Translation. ArXiv,
abs/1911.01497.
Saxton, D.; Grefenstette, E.; Hill, F.; and Kohli, P. 2019.
Analysing Mathematical Reasoning Abilities of Neural
Models. In International Conference on Learning Repre-
sentations.
Trask,A.;Hill,F.;Reed,S.;Rae,J.;Dyer,C.;andBlunsom,
P.2018.Neuralarithmeticlogicunits.InAdvancesinNeural
InformationProcessingSystems.
Tu, L.; Lalwani, G.; Gella, S.; and He, H. 2020. An Em-
piricalStudyonRobustnesstoSpuriousCorrelationsusing
Pre-trainedLanguageModels. TransactionsoftheAssocia-
tionforComputationalLinguistics,8:621–633.
Vani,A.;Schwarzer,M.;Lu,Y.;Dhekane,E.;andCourville,
A. 2021. Iterated learning for emergent systematicity in
{VQA}. In International Conference on Learning Repre-
sentations.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.;Gomez,A.N.;Kaiser,Ł.;andPolosukhin,I.2017. At-
tentionisallyouneed. InAdvancesinNeuralInformation
ProcessingSystems.
Wallace,E.;Feng,S.;Kandpal,N.;Gardner,M.;andSingh,
S. 2019. Universal Adversarial Triggers for Attacking
and Analyzing NLP. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on Natural
LanguageProcessing(EMNLP-IJCNLP),2153–2162.Hong
Kong,China:AssociationforComputationalLinguistics.
Welleck, S.; Kulikov, I.; Kim, J.; Pang, R. Y.; and Cho, K.
2020. Consistency of a Recurrent Language Model With
Respect to Incomplete Decoding. In Proceedings of the
2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 5553–5568. Online: Associa-
tionforComputationalLinguistics.
Zaremba,W.;andSutskever,I.2014. LearningtoExecute.
CoRR,abs/1410.4615.
8637

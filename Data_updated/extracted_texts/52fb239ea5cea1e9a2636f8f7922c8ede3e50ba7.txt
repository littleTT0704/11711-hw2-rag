LI¯LA: A Unified Benchmark for Mathematical Reasoning
SwaroopMishra∗† MatthewFinlayson∗‡
ArizonaStateUniveristy TheAllenInstituteforAI
PanLu† LeonardTang SeanWelleck
UCLA HarvardUniversity TheAllenInstituteforAI
ChittaBaral TanmayRajpurohit OyvindTafjord
ArizonaStateUniveristy GeorgiaInstituteofTechnology TheAllenInstituteforAI
AshishSabharwal PeterClark AshwinKalyan‡
TheAllenInstituteforAI TheAllenInstituteforAI TheAllenInstituteforAI
Abstract (cid:48)(cid:68)(cid:87)(cid:75)(cid:3)(cid:68)(cid:69)(cid:76)(cid:79)(cid:76)(cid:87)(cid:92)(cid:29)(cid:3)(cid:69)(cid:68)(cid:86)(cid:76)(cid:70)(cid:3)(cid:80)(cid:68)(cid:87)(cid:75)
(cid:47)(cid:68)(cid:81)(cid:74)(cid:88)(cid:68)(cid:74)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:91)(cid:76)(cid:87)(cid:92)(cid:29)(cid:3)(cid:86)(cid:76)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)(cid:79)(cid:68)(cid:81)(cid:74)(cid:88)(cid:68)(cid:74)(cid:72)
(cid:41)(cid:82)(cid:85)(cid:80)(cid:68)(cid:87)(cid:29)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:84)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:68)(cid:81)(cid:86)(cid:90)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)
(cid:46)(cid:81)(cid:82)(cid:90)(cid:79)(cid:72)(cid:71)(cid:74)(cid:72)(cid:29)(cid:3)(cid:81)(cid:82)(cid:3)(cid:72)(cid:91)(cid:87)(cid:72)(cid:85)(cid:81)(cid:68)(cid:79)(cid:3)(cid:78)(cid:81)(cid:82)(cid:90)(cid:79)(cid:72)(cid:71)(cid:74)(cid:72)
(cid:44)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29)(cid:3)(cid:60)(cid:82)(cid:88)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:68)(cid:3)(cid:84)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:76)(cid:81)(cid:89)(cid:82)(cid:79)(cid:89)(cid:72)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)
(cid:70)(cid:68)(cid:79)(cid:70)(cid:88)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:73)(cid:3)(cid:81)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:86)(cid:17)(cid:3)(cid:60)(cid:82)(cid:88)(cid:3)(cid:81)(cid:72)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:83)(cid:72)(cid:85)(cid:73)(cid:82)(cid:85)(cid:80)(cid:3)(cid:72)(cid:76)(cid:87)(cid:75)(cid:72)(cid:85)(cid:3)(cid:68)(cid:81)(cid:3)
(cid:68)(cid:71)(cid:71)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:85)(cid:3)(cid:86)(cid:88)(cid:69)(cid:87)(cid:85)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:83)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:81)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:86)(cid:17)(cid:3)(cid:42)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:72)(cid:3)
(cid:92)(cid:82)(cid:88)(cid:85)(cid:3)(cid:68)(cid:81)(cid:86)(cid:90)(cid:72)(cid:85)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:84)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:17)
Mathematicalreasoningskillsareessentialfor (cid:52)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29)(cid:3)(cid:54)(cid:68)(cid:85)(cid:68)(cid:3)(cid:83)(cid:76)(cid:70)(cid:78)(cid:72)(cid:71)(cid:3)(cid:23)(cid:24)(cid:3)(cid:83)(cid:72)(cid:68)(cid:85)(cid:86)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:54)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3)(cid:83)(cid:76)(cid:70)(cid:78)(cid:72)(cid:71)(cid:3)(cid:20)(cid:20)(cid:3)(cid:83)(cid:72)(cid:68)(cid:85)(cid:86)(cid:3)
general-purposeintelligentsystemstoperform (cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:83)(cid:72)(cid:68)(cid:85)(cid:3)(cid:87)(cid:85)(cid:72)(cid:72)(cid:17)(cid:3)(cid:43)(cid:82)(cid:90)(cid:3)(cid:80)(cid:68)(cid:81)(cid:92)(cid:3)(cid:83)(cid:72)(cid:68)(cid:85)(cid:86)(cid:3)(cid:90)(cid:72)(cid:85)(cid:72)(cid:3)(cid:83)(cid:76)(cid:70)(cid:78)(cid:72)(cid:71)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:82)(cid:87)(cid:68)(cid:79)(cid:34)(cid:3)
tasks from grocery shopping to climate mod-
(cid:51)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:20)(cid:29)(cid:3)
eling. Towards evaluating and improv- (cid:63)(cid:64)(cid:65)(cid:3)(cid:78)(cid:74)(cid:71)(cid:80)(cid:79)(cid:68)(cid:74)(cid:73)(cid:159)(cid:83)(cid:134)(cid:3)(cid:84)(cid:160)(cid:136)
ing AI systems in this domain, we propose (cid:3)(cid:3)(cid:3)(cid:3)(cid:60)(cid:73)(cid:78)(cid:82)(cid:64)(cid:77)(cid:3)(cid:211)(cid:3)(cid:83)(cid:3)(cid:207)(cid:3)(cid:84)(cid:3)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:77)(cid:64)(cid:79)(cid:80)(cid:77)(cid:73)(cid:3)(cid:60)(cid:73)(cid:78)(cid:82)(cid:64)(cid:77)
L¯ILA,aunifiedmathematicalreasoningbench- (cid:75)(cid:77)(cid:68)(cid:73)(cid:79)(cid:159)(cid:78)(cid:74)(cid:71)(cid:80)(cid:79)(cid:68)(cid:74)(cid:73)(cid:159)(cid:191)(cid:192)(cid:134)(cid:3)(cid:188)(cid:188)(cid:160)(cid:160)(cid:3)(cid:186)(cid:3)(cid:79)(cid:74)(cid:79)(cid:60)(cid:71)(cid:3)(cid:75)(cid:64)(cid:60)(cid:77)(cid:78)(cid:3)(cid:68)(cid:78)(cid:3)(cid:79)(cid:67)(cid:64)(cid:3)(cid:78)(cid:80)(cid:72)(cid:3)(cid:74)(cid:65)(cid:3)
(cid:75)(cid:64)(cid:60)(cid:77)(cid:78)(cid:3)(cid:82)(cid:68)(cid:79)(cid:67)(cid:3)(cid:22)(cid:60)(cid:77)(cid:60)(cid:3)(cid:60)(cid:73)(cid:63)(cid:3)(cid:22)(cid:60)(cid:71)(cid:71)(cid:84)
markconsistingof23diversetasksalongfour
(cid:51)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:21)(cid:29)(cid:3) dimensions: (i) mathematical abilities e.g.,
(cid:83)(cid:3)(cid:211)(cid:3)(cid:191)(cid:192)
arithmetic, calculus (ii) language format e.g., (cid:84)(cid:3)(cid:211)(cid:3)(cid:188)(cid:188)
(cid:60)(cid:73)(cid:78)(cid:82)(cid:64)(cid:77)(cid:3)(cid:211)(cid:3)(cid:83)(cid:3)(cid:207)(cid:3)(cid:84)(cid:3)(cid:186)(cid:3)(cid:79)(cid:74)(cid:79)(cid:60)(cid:71)(cid:3)(cid:75)(cid:64)(cid:60)(cid:77)(cid:78)(cid:3)(cid:68)(cid:78)(cid:3)(cid:79)(cid:67)(cid:64)(cid:3)(cid:78)(cid:80)(cid:72)(cid:3)(cid:74)(cid:65)(cid:3)(cid:75)(cid:64)(cid:60)(cid:77)(cid:78)(cid:3)(cid:82)(cid:68)(cid:79)(cid:67)(cid:3)
question-answering,fill-in-the-blanks(iii)lan-
(cid:22)(cid:60)(cid:77)(cid:60)(cid:3)(cid:60)(cid:73)(cid:63)(cid:3)(cid:22)(cid:60)(cid:71)(cid:71)(cid:84)
guage diversity e.g., no language, simple lan- (cid:75)(cid:77)(cid:68)(cid:73)(cid:79)(cid:159)(cid:60)(cid:73)(cid:78)(cid:82)(cid:64)(cid:77)(cid:160)
guage (iv) external knowledge e.g., common- (cid:36)(cid:81)(cid:86)(cid:90)(cid:72)(cid:85)(cid:29)(cid:3)(cid:24)(cid:25)
sense, physics. We construct our bench-
Figure1:AdataexamplewithtwoPythonprogramsin
mark by extending 20 datasets benchmark by
L¯ILA.Oneprogramannotationusesfunctionconstruct
collecting task instructions and solutions in
whereastheotheroneisaplainscriptwithoutfunction.
the form of Python programs, thereby ob-
Theinstructionforeachtaskandcategoriesacrossfour
taining explainable solutions in addition to
dimensionsareannotatedfordevelopingL¯ILA.
the correct answer. We additionally intro-
duce two evaluation datasets to measure out-
of-distributionperformanceandrobustnessto
1 Introduction
language perturbation. Finally, we introduce
BHA¯SKARA, a general-purpose mathematical Mathematicalreasoningisrequiredinallaspects
reasoningmodeltrainedonL¯ILA.Importantly,
oflife,frombuyingingredientsforarecipetocon-
we find that multi-tasking leads to significant
trollingtheworldeconomy. Giventhefundamen-
improvements (average relative improvement
talnatureofmathematicalreasoning,anumberof
of 21.83% F1 score vs. single-task models),
whilethebestperformingmodelonlyobtains worksproposedatasetstoevaluatespecificmathe-
60.40%,indicatingtheroomforimprovement maticalreasoningabilitiesofAIagents,e.g.,Kush-
in general mathematical reasoning and under- manetal.(2014)(algebrawordproblems),Mishra
standing.1
∗Equalfirstauthors.
†WorkdonewhileattheAllenInstituteforAI.
‡Corresponding authors: matthewf@allenai.org, ash-
winkv@allenai.org.
1Ourdataset:https://github.com/allenai/Lila.Our
model:https://huggingface.co/allenai/bhaskara.
3202
raM
8
]LC.sc[
2v71571.0122:viXra
etal.(2022c)(arithmeticreasoning),Saxtonetal. answer,program,instructions,andcategorytags.
(2019)(templatedmathreasoningspanningalge- Inadditiontoevaluatinghigh-levelproblemsolv-
bra,calculus,probability,etc.) Sinceevaluating ing,wealsofacilitatetwootherkeywaystomakea
high-capacitymodelsonnarrowlyscopedmathe- fairassessmentofmodelsonmathematicalreason-
maticalreasoningdatasetsrisksoverestimatingthe ingtasks. InlinewithBrasetal.(2020),Ribeiro
reasoning abilities of these AI systems, creating etal.(2020)andWellecketal.(2022),weevaluate
the need for a unified benchmark for systematic generalizatione.g.,alternateformulationsofaprob-
evaluationoverdiversetopicsandproblemstyles. lem(“2+2=?” vs. “Whatistwoplustwo?”) using
Tothisend,weintroduceL¯ILA2,aunifiedmath- anout-of-distributionevaluationset(L¯ILA-OOD)
ematicalreasoningbenchmarkthatconsistsof23 containingdatasetsrequiringthesameunderlying
mathematicalreasoningtasks. L¯ILAisconstructed mathematicalreasoningskills,butwerecollected
byextending20existingdatasetsspanningawide independentlyofthetrainingdatasets. Further,we
rangeoftopicsinmathematics,varyingdegreesof collect a robustness split L¯ILA-ROBUST, that in-
linguisticcomplexity,anddiversequestionformats troduces linguistic perturbations (e.g., active vs.
andbackgroundknowledgerequirements. Impor- passivevoice)viacrowd-sourcing. Theevaluation
tantly,L¯ILAextendsallofthesedatasetstoinclude scheme is a combination of the performance on
asolutionprogram asopposedtoonlyananswer, allthreesets: L¯ILA-TEST,L¯ILA-OODandL¯ILA-
and instruction annotations to enable instruction- ROBUST.
basedlearning(Sanhetal.,2021;Weietal.,2021;
Contributions
Mishraetal.,2022b).
1. We present L¯ILA, a holistic benchmark for
In order to accurately assess the mathematical
mathematicalreasoning. L¯ILA extends20ex-
reasoningabilityofmodels, evaluatingthechain
isting datasets with solutions in the form of
of reasoning that leads to the correct solution is
Python programs and instruction annotations,
equallyimportant(ifnotmoreimportant)toevalu-
andcategorizesquestionsinto23tasksbasedon
atingthefinalanswerorexpression. Wetherefore
theirlanguagecomplexity,questionformatand
collect Python programs that serve as reasoning
needforexternalknowledge. Ourbenchmark
chains for each question in the benchmark. We
measures performance on out-of-distribution
achievethisbyautomaticallyconvertingdomain-
examplesandrobustnesstolanguageperturba-
specificlanguage(DSL)annotationsintoPython
tionsinadditiontostandardtest-set.
programs and by manually collecting expert an-
2. WeintroduceBHA¯SKARA,amulti-taskmodel
notationswhennoDSLannotationsareavailable.
fine-tunedonourdataset. Ourbest-performing
Byincorporatingprogramannotations,L¯ILAuni-
modelachievescomparableperformancetoa
fiesvariousmathematicalreasoningdatasetsunder
66 largermodelpre-trainedonbothcodeand
a single problem formulation i.e., given an input ×
language.
problem in natural language, generate a Python
3. Weprovideananalysisofourmodels’perfor-
program that upon execution returns the desired
manceandfindthat(1)multitaskingimproves
answer. Thisformulationallowsneuralapproaches
considerably over task-specific learning both
tofocusonthehigh-levelaspectsofmathematical
inin-distributionandout-of-distributionevalua-
problemsolving(e.g.,identifyingpotentialsolution
tion(2)programsynthesissubstantiallyoutper-
strategies,decomposingtheproblemintosimpler
formsanswerprediction,(3)few-shotprompt-
sub-problems), while leveraging external solvers
ingwithcodexhasthestrongestperformance.
(e.g.,Pythonbuiltins,Sympy)toperformprecise
Wealsoidentifyareasforimprovementforfu-
operationslikeaddinghugenumbersorsimplify-
turework,e.g.,datagapsinL¯ILAcategories.
ingexpressions. Figure1illustratesasamplefrom
ourL¯ILA benchmarkthatillustratesthequestion, 2 RelatedWork
2NamedafterLi¯lavati,a12thcenturymathematicaltreatise Mathematical Reasoning Datasets. Our work
onarithmeticthatcoverstopicslikearithmeticandgeometric
builds on an existing body of mathematical rea-
progressions,indeterminateequationsandcombinations. It
isalsowidelyknownfortheextensivenumberofmathword soning literature. Early work in this areas fo-
problems. Theauthor,Bha¯skaraisknownforfundamental cuses on small-scale datasets testing addition-
andoriginalcontributionstocalculus,physics,numberthe-
subtraction(Hosseinietal.,2014),templatedques-
ory,algebra,andastronomy(Colebrooke,1817;Sarkar,1918;
Kolachanaetal.,2019) tions with equations as parameters (Kushman
etal.,2014)andotherformsofarithmeticreason- statisticsofthedatasets.
ing(Koncel-Kedziorskietal.,2015;RoyandRoth,
2016;Upadhyayetal.,2016;RoyandRoth,2017, 3.1 DatasetConstruction
2018; Ling et al., 2017). Later datasets increase Data Sources. L¯ILA incorporates 20 existing
in complexity and scale, incorporating reading datasetsfromthemathematicalreasoningliterature
comprehension(Duaetal.,2019b),algebra(Sax- (Table 19 gives a detailed list), where inputs are
ton et al., 2019), and multi-modal contexts (Lu naturallanguageortemplatedtextandoutputsare
etal.,2021a,2022). Stillothernumericalreason- numericalorexpressions,e.g.,weexcludetheorem
ingdatasetsfocusondiversity(Miaoetal.,2020a) proving (Welleck et al., 2021; Han et al., 2021),
with multiple categories of numerical reasoning where the output is not a number or expression.
tasks(e.g.,Aminietal.,2019). Mostrecently,new Weleavetheincorporationofformatsliketheorem
datasetshavefocusedonincreasingdifficulty,e.g., provingtofuturework.
olympiadproblems(Hendrycksetal.,2021b)and
adversarial problems (Patel et al., 2021), as well Unified format. We normalize all datasets to a
asincreasingtheknowledgerequirementstosolve unifiedformatwiththefollowingfields:
tasks,withagrowingfocusoncommonsenserea- 1. The source dataset. Category tags for each
soning(Zhouetal.,2019;Zhangetal.;Luetal., ofthefourdimensions(mathability,language
2021b;Mishraetal.,2022c). complexity, format, and external knowledge;
Aseparatelineofworkinmathematicalreason- see§3.2).
ing includes datasets testing mathematical theo- 2. Thequestion,inEnglish.
remproving(e.g.,Lietal.,2021;Wuetal.,2021; 3. Theanswertothequestion,asastringcontain-
Wellecketal.,2021;Zhengetal.,2021;Hanetal., ing a number, expression, list, or other data
2021). Wedonot,however,considertheoremprov- format. AsetofPythonstringsthatprintthe
inginourwork,choosinginsteadtofocusonnu- answer.
mericalreasoning. 4. Atask-levelinstructioninnaturallanguage.
Wealsoretainmeta-datafromtheoriginaldataset.
Task Hierarchy and Multi-tasking in Numer-
ical Reasoning. We take inspiration from the Automatic program annotation. Most of the
success of multi-task learning in NLP (Weston
annotations inthe source datasetsdo not contain
et al., 2015), including benchmarks (e.g., Wang
output in the form of a Python program. We au-
etal.,2018,2019;Duaetal.,2019a)andmultitask-
tomaticallyannotatemostdatasetsbygenerating
ingmodels(e.g.,McCannetal.,2018;Khashabi
Python programs using the annotations (answer,
etal.,2020;Lourieetal.,2021;Aghajanyanetal.,
explanation,etc.) providedinthesourcedatasets.
2021). NumGLUE (Mishra et al., 2022c) has
Wherepossible,wegeneratemultiplePythonpro-
been proposed as a multi-tasking numerical rea-
gramsforasinglequestion. Thisistoaccountfor
soningbenchmarkthatcontains8differenttasks.
variationintheprogramspacesuchasthechoiceof
L¯ILAexpandsNumGLUEtoprovidewidercover-
datastructure,languageconstruct,variablename,
age of mathematical abilities, along with evalua-
and programming style (e.g., declarative vs pro-
tionthatcapturesout-of-domain,robustness,and
cedural). For example, Figure 1 gives multiple
instruction-followingperformance. Ourintroduc-
Pythonprogramssolvingthesamequestion;inthis
tionofmathematicalreasoningcategoriesandthe
case one program directly calculates the answer,
evaluationsetupisinspiredbytaskhierarchiesin
whereas the other defines a function to solve the
otherdomainssuchasvision(Zamiretal.,2018)
problemmoregenerally.
and NLP (Rogers et al., 2021) which appear in
Somedatasetscontainprogramannotationsthat
largescalebenchmarks(e.g.,Srivastavaetal.,2022;
can be captured by a domain-specifc language
Wangetal.,2022).
(DSL)inwhichcasewewriterulestoconvertthem
into Python programs, e.g., volume(sphere,3)
3 LI¯LA
to the Python expression 4/3*math.pi*3**3. In
L¯ILAiscomposedof23tasksacross4dimensions, somecaseswhereaDSLannotationisnotprovided,
curated from 44 sub-datasets across 20 dataset we use pattern matching to convert highly tem-
sources. Herewediscusstheconstructionandcom- plateddatasetsliketheAMPSdataset(Hendrycks
positionofthebenchmarkandprovidedescriptive et al., 2021b) to our unified format. In other
Category Tasks
Mathability Basic math, multiplication/division, number theory, algebra, geometry, counting and statistics,
calculus,linearalgebra,advancedmath
Language Nolanguage,simplelanguage,complexlanguage
Knowledge Nobackgroundknowledge,commonsense,math,science,computerscience,realworldknowledge
Format Fill-in-the-blank,generativequestionanswering,multiple-choice,naturallanguageinference,reading
comprehension
Table1:Categoriesandtheirassociatedtasks.
cases,insteadofconvertingtheexistingdataset,we metic,algebra,geometry,calculus,etc.
modifythedatagenerationcodetoreproducethe Oursecondcategory,languagecomplexity,sepa-
dataset with program annotations. For the Deep- ratesmathproblemsbythecomplexityofthelan-
Mind mathematics dataset (Saxton et al., 2019), guage used to represent them. This ranges from
thisallowsustocreatediverse,compositionalmath formalrepresentationsonly(e.g.,1+1=?) tonatural
problemswithprogramannotationsusingasophis- language(e.g.,“Mariellahas3pears...”).
ticatedgrammar. Wenextpartitiondatasetsbasedonthetypeof
backgroundknowledge,requiredtosolvetheprob-
Expert program annotation. For many
lem. For instance, commonsense questions like
datasets, it is not possible to obtain Python pro-
“Howmanylegsto3peoplehave?” orscienceques-
gramannotationsviaautomatedmethodsdescribed
tionslike“Willwaterboilat200degreesCelsius?”
above;eithertheoriginaldatasetcontainsonlythe
requiredifferentsetsofknowledgetoanswer.
final answer or contains solutions expressed in
Lastly,wecategorizebasedonquestionformat,
free-formnaturallanguage. Forsuchdatasets,we
puttinge.g.,multiplechoicequestionsunderone
obtainannotationsfromexpertswhoareproficient
taskandnaturallanguageinferenceunderanother.
in basic programming and high-school level
Examples of each task and the datasets included
mathematics. SeeAppendixB.1fordetails.
areinAppendixB.
Instructionannotation. Giventheeffectiveness
ofinstructionlearning(Mishraetal.,2022b;Wei 3.3 LI¯LA-OOD
etal.,2021;Mishraetal.,2022a;Sanhetal.,2021) Inordertomeasureifthemodelhastrulylearned
foreffectivegeneralization,wecollectinstruction the underlying mathematical reasoning skill, we
annotationforeachtask. Eachinstructioncontains evaluate both in-distribution (IID, i.e., standard
a definition that clearly defines the task and pro- train-testsplits)andout-of-distribution(OOD)per-
vides guidelines, a prompt that provides a short formanceforeachtask,i.e.,weevaluateonexam-
andstraightforwardinstruction,andexamplesthat plesrequiringthesameunderlyingmathematical
facilitatelearningbydemonstration(Brownetal., reasoningskillbutfromadifferentdataset. Tocon-
2020). Figure1showsanexampleinstructionfor structL¯ILA-OOD,wefollowBrasetal.(2020)and
thebasicmathtask(§3.2). Hendrycksetal.(2020)byrandomlyassigningthe
datasets for each task into IID and an OOD sets,
3.2 CategoriesandTasks
using the IID set for training and standard evalu-
Wecreate4views3orcategoriesofL¯ILAalongthe ationandtheOODsettoevaluategeneralization.
dimensionsofmathematicalarea, languagecom- We do not include tasks in L¯ILA-OOD for tasks
plexity,externalknowledge,andquestionformat. containingonlyonedataset.
Altogether,theseviewsclassifythedatainto23
tasks(Table1). Bycreatingmultipleviewsofthe 3.4 LI¯LA-ROBUST
benchmark,weareabletosystematicallycharacter-
Inlightofrecentworkdemonstratingthebrittleness
izethestrengthsandweaknessesofexistingmodels
oflanguagemodelsatsolvingmathproblems(Pa-
atagranularlevel.
teletal.,2021),wecreateahigh-qualityevaluation
The first category, math ability, partitions the
dataset, L¯ILA-ROBUST, toevaluateperformance
datasetsintocommonpedagogicalsubjects: arith-
on mathematical reasoning tasks when linguistic
perturbationsareintroduced. Specifically,wede-
3Notethatitisnotapartitionofthebenchmarkaseach
dimensionsdividestheconstituentexamplesindifferentways fineandapplyasetofcarefullychosenaugmenta-
Statistic Number
#
#
#
#
#T
T
T
T
To
o
o
o
ot
t
t
t
ta
a
a
a
al
l
l
l
lt
d
i
q
pa
na
u
rs
s
ot
ek
ta
gsrs
s
u
t
re
i
ac
ot mts
nio ssns
1 33 53
82
4
4
,
,3
4
4
8 71 65
9
followingpolyhedron's
didthe
more
mand yays percent hours
times
ye ma irs nutes
did
tion
temU U U
A
A
A
pv
v
vn n n
le
e
ei i i aTq q q
r
r
ra
a
a
tu u u
a
eg
g
ge e e
be
e
e
slq p a
,el
l
lnu r
e
e
e
so s 2e
n
n
n
ug ws
:g
g
grt me
t
t
ti a
Kh
h
ho rm sn
meo
o
oss
yf
f
f
ai
q
p
s
rn
u
r
t
is
o
a
zet tgsr eiu
t
r
s
di
ac
to
m
it
n
ci io
ss
nsn os
Tf aL
b1 3 2
¯I lL
e4
43 2 7
3
A2 5 1
1
7
7
1, , ,
.
.
.
.2 5 2
1
7
8
63 9 6
8
2
5
,9 7 4
on each
ao ff t te hr e did
ti hs
e
p
h$ et arh
pcp
te
e we htn ian et se md
e
CoW ismS hi TC mh io pM cpn eu vl uWl hit efi WhS p yrE teo tls yl hnt v eime aa tte
\$ realall
eigenvecF torsind
unionquotientjacH obiao
d
nift
fw
eh ree
nceleastsumgreatn eu
sl tom mo
f
bn
st
eu eg
e
rcc
on
dh
nis dtas nm
cealld emo sdd tdoe d doo is n i doeede ssy
task, yielding a set of challenging problems that Figure2:Questionn-gramdistributioninL¯ILA.
areconsistentanswer-wisebutstylisticallydiffer-
entquestion-wise. Overall,wedefineatotalof9
templatesforsuchquestionperturbations: 3from Fine-tuning. Wefine-tuneaseriesofGPT-Neo-
2.7Bcausallanguagemodels(Blacketal.,2021))
Patel et al. (2021) and 6 of our own. From each
constituent dataset, we sample 20 questions and
onL¯ILA. WechooseGPT-Neobecauseitwaspre-
trained on both natural language and code (Gao
obtainperturbedquestionannotationsviaAmazon
etal.,2020), asopposedto solelyonnaturallan-
MechanicalTurk(AMT).RefertoAppendixB.1
foradditionaldetailsontheconstructionofL¯ILA- guage. ToassessthecapabilitiesofGPT-Neoon
variousaspectsofthedataset,wefine-tunesingle-
ROBUST.
taskmodelsoneachofthe23tasksin L¯ILA. We
3.5 Statistics alsoevaluatethebenefitoftransferlearningbyfine-
tuningasinglemulti-taskGPT-Neobaselineonall
Table2showskeystatisticsofourproposedbench-
the tasks simultaneously. We call our multitask
mark, L¯ILA. L¯ILA contains
≈
134K examples
modelBHA¯SKARA.
withsignificantdiversityacrossquestion,answer,
programandinstructionlength(seedetailedstatis- Prompting. Wealsousefew-shotpromptingto
tics in Appendix C). Figure 2 shows the diver- evaluate GPT-3 and Codex4 (Brown et al., 2020;
sity of questions in L¯ILA. Note that we down- Chenetal.,2021). FortheIIDsetting,weprompt
sample(viarandomselection)somedatasetslike the model with a random input-output examples
AMPS(Hendrycksetal.,2021b)whichcontains from the same dataset as the input. In the OOD
numerous templated questions that can get over- setting,wetakeexamplesfromotherdatasets(Ta-
representatedinthedistributionofexamplesacross ble 12-15) within the same task. We repeat this
categoriesinL¯ILA. evaluation with increasing numbers of examples
(uptothetokensizeofmodels)tostudytheeffect
4 Experiments onperformance5.
Inthissection,weintroduceourmodelingcontri- Evaluation. Weevaluateourmodelsundertwo
butions for the L¯ILA benchmark and discuss the regimes—directlyoutputtingtheansweri.e.,pro-
overallexperimentalsetup. graminductionandoutputtingaPythonprogram
thatisthenexecutedtoobtainthefinalansweri.e.,
Data partition and evaluation. For the IID program synthesis. In the case of our fine-tuned
setup,werandomlypartitionthedataineachtask models,wetrainthemtooutputboththefinalan-
into training (70%), development (10%) and test swerandthePythonprogramconditionedonthe
(20%)sets. Additionally,wealsoevaluateonL¯ILA- inputquestion. Toevaluateourmodelsunderdirect
OODandL¯ILA-ROBUSTsettings;thus,thefinal
4text-davinci-002, code-davinci-002
evaluationschemeisacombinationoftheperfor-
5Henceforthwerefertothemaxexamplemodelunless
manceonallthreeevaluationsetups otherwisespecified.
questionanswering,weuseF1-score6 tocompare Program synthesis substantially outperforms
themodeloutputandthegoldanswer. Toevaluate answer prediction. Synthesizing the program
programsynthesis,weexecutethemodel’soutput and evaluating it to get an answer substantially
within a Python interpreter and compare the pro- outperformsdirectlypredictingtheanswer. Forin-
gramoutputwiththeoutputofthegoldprogram, stance,multi-taskprogramsynthesis(BHA¯SKARA-
againusingF1. Weevaluatebasedontheprogram P)hasanaveragescoreof0.480whilemulti-task
output,ratherthantheprogramitself,toaccountfor answer prediction (BHA¯SKARA-A) scores 0.252.
diversityinsolvingtechniquesandprogramming This means models are often able to generate a
styles. programthatevaluatestothecorrectanswer,even
when the model cannot directly compute the an-
5 ResultsandAnalysis swer.
Program synthesis improves over answer pre-
AsummaryofallkeyresultsonourL¯ILAbench-
diction in all math categories except Geometry,
mark are shown in Table 3. In this section, we
withthelargestimprovementsinStatisticsand
will discuss the performance of fine-tuned 2.7B
Linear Algebra; see Table 5 for examples. We
GPT-Neo models (§5.1), performance of models
even see benefits of program synthesis in NLI,
along the 4 categories of tasks (§5.2) and finally,
a classification-based task. L¯ILA’s unified prob-
thefew-shotperformanceofmuchlarger( 175B
∼ lemformatdecouplessynthesisfromcomputation,
parameters)models(§5.3).
whileopeningdirectionsforfurtherstudyoneither
aspect.
5.1 Results: Fine-tunedModels
Multitasking improves IID performance, ro- Models leverage symbolic execution and li-
bustness,andOODgeneralization. Themulti- braries. Thegapbetweenprogramsynthesisand
tasking model (BHA¯SKARA) substantially im- answer prediction suggests that the neural lan-
proves upon the single task models (Neo). guage model offloads computations to the sym-
BHA¯SKARAachievesbetteraveragein-domainper- bolicPythonruntimethatareotherwisedifficultto
formance than the 23 individual per-task models computedirectly. Weidentifytwocommoncases.
(0.480 vs. 0.394 average score), suggesting that First, the model leverages standard Python as a
it leverages cross-task structure not present in a calculator. For instance, this pattern is common
singletask’strainingset.
inthebasic_mathandmul_divcategories,which
involveevaluatingarithmeticexpressions;Table4
We also find that our multi-task model is ro-
busttothelinguisticperturbationswetestinL¯ILA- showsexamples. Second,themodelisabletocall
externallibrariesthatperformsophisticatedcom-
ROBUST. Wedidnotfindanydegradationinper-
putations. Forinstance,instatisticsthemodeluses
formancewhentestingonperturbedIIDtestexam-
scipy.stats.entropyornp.linalg.detinlin-
ples. Additionally,multi-tasktrainingsubstantially
earalgebrawhilesolvingproblems(Table5).
improvesout-of-domaingeneralization(0.448vs.
0.238). The gap between IID and OOD perfor-
Models occasionally generate non-executable
manceismuchsmallerforBHA¯SKARAthanforthe
code. Roughly 10% of BHA¯SKARA’s IID pro-
singletaskmodels(Table3),andinonecase(for-
grams fail to execute. 86% of these are
mat)BHA¯SKARA’sOODperformanceonheld-out
SyntaxErrors,whichoftenoccurbecausedecod-
tasksisbetterthanitsIIDperformance(Table4).
ingterminatesbeforefinishingtheprogramorthe
L¯ILA’smulti-taskstructureopensinterestingfuture
model generates a program of the form ‘2+3=5’,
directions related to developing improved multi-
which is invalid Python. The remaining 14%
taskingtechniques, andfurtherunderstandingits
of execution failures are less trivial, including
benefits.
NameErrors(7%)andTypeErrors(1%)(seeTable
Lastly,wedonotfindanybenefittofine-tuning
6).
with instructions. Our best instruction tuned
modelachieves0.133F1,whereastheworstnon- BHA¯SKARAisagoodstartingpointforfurther
instruction-tunedmultitaskmodelachieves0.290. fine-tuning Table5showsthatourBHA¯SKARA
modelisabetterstartingpointfordownstreamfine-
6Thisisasoftversionofexactmatchaccuracyassigning
tuningthanthevanillapre-trainedGPT-Neo-2.7B.
partialcreditwhencommonwordsarepresentintheoutput
andgoldanswer. When comparing fine-tuning for direct question
Supervision/Size Few-shot,175B Few-shot,175B Fine-tuned,2.7B Fine-tuned,2.7B Fine-tuned,2.7B Fine-tuned,2.7B
→
GPT-3 Codex Neo-A Neo-P BHA¯SKARA-A BHA¯SKARA-P
Task Category
↓ IID OOD IID OOD IID OOD IID OOD IID OOD IID OOD
1 Basicmath 0.766 0.818 0.791 0.762 0.533 0.523 0.611 0.555 0.693 0.657 0.790 0.787
2 Muldiv 0.479 0.665 0.691 0.790 0.136 0.089 0.388 0.194 0.155 0.083 0.448 0.395
3 Numbertheory 0.240 0.154 0.472 0.344 0.108 0.095 0.328 0.107 0.129 0.190 0.358 0.293
4 Algebra 0.338 0.130 0.603 0.511 0.164 0.031 0.348 0.051 0.203 0.054 0.473 0.007
5 Geometry 0.283 0.120 0.000 0.250 0.288 0.025 0.077 0.021 0.297 0.105 0.079 0.250
6 Statistics 0.183 0.210 0.650 0.200 0.107 0.008 0.839 0.034 0.115 0.179 0.947 0.164
7 Calculus 0.231 0.208 0.930 0.884 0.138 0.119 0.486 0.334 0.102 0.167 0.495 0.805
8 Linearalgebra 0.127 - 0.692 - 0.229 - 0.809 - 0.240 - 0.808 -
9 Advancedmath 0.150 - 0.472 - 0.012 - 0.100 - 0.019 - 0.160 -
10 Nolanguage 0.213 0.162 0.853 0.770 0.143 0.083 0.698 0.330 0.140 0.138 0.703 0.850
11 Simplelanguage 0.486 0.561 0.568 0.610 0.269 0.243 0.363 0.292 0.332 0.269 0.433 0.384
12 Complexlanguage 0.356 0.413 0.456 0.583 0.147 0.113 0.216 0.106 0.215 0.259 0.288 0.557
13 Fillintheblank 0.710 0.620 0.790 0.660 0.086 0.193 0.304 0.193 0.059 0.519 0.262 0.519
14 GenerativeQA 0.305 0.385 0.566 0.632 0.142 0.135 0.376 0.199 0.178 0.160 0.476 0.235
15 MCQ 0.801 0.870 0.771 0.870 0.636 0.818 0.652 0.818 0.752 0.888 0.817 0.888
16 NLI 0.500 - 0.710 - 0.221 - 0.212 - 0.566 - 0.893 -
17 RC 0.460 - 0.615 - 0.135 - 0.295 - 0.132 - 0.264 -
18 Noexternalk. 0.437 0.485 0.638 0.660 0.138 0.110 0.387 0.159 0.167 0.199 0.400 0.465
19 Commonsense 0.788 0.698 0.752 0.815 0.613 0.364 0.624 0.356 0.735 0.470 0.778 0.526
20 Mathformulas 0.259 0.162 0.661 0.544 0.137 0.074 0.454 0.382 0.170 0.077 0.599 0.404
21 Scienceformulas 0.305 0.120 0.315 0.250 0.158 0.025 0.239 0.021 0.157 0.105 0.181 0.250
22 Computersciencek. 0.262 0.128 0.425 0.408 0.151 0.137 0.147 0.134 0.232 0.304 0.220 0.278
23 Real-worldk. 0.150 - 0.472 - 0.012 - 0.100 - 0.019 - 0.160 -
Averagescore 0.384 0.384 0.604 0.586 0.204 0.177 0.394 0.238 0.252 0.268 0.480 0.448
Table3:Evaluationsofdifferentbaselinesacross23tasksinL¯ILA.Onmosttasks,Codexoutperformsallbaselines
whileBHA¯SKARA-Poutperformsallfine-tunedbaselines. AmodelusuallyperformsworseontheOODdataset.
The bold score refers to the best score among models with the same supervision method; the underlined score
referstothebestscoreamongallmodels.GPT-3andCodexperformanceiscomputedon100uniformlydistributed
examplesowingtotheircostandusagelimit.Fine-tunedmodelperformanceiscalculatedonthefulltestset.
answeringwithT5-3B,weseeanalmost8%abso-
Neo-A Neo-P luteimprovementinF1(30.1%to37.6%). These
Dimension
IID OOD IID OOD findingsestablishBHA¯SKARAasastrongstarting
Mathability 0.191 0.129 0.445 0.188 pointforfurtherfine-tuningonnewtasks. Forthis
Language 0.189 0.147 0.429 0.246
reason,wereleaseourmulti-taskmodelforpublic
Format 0.246 0.382 0.372 0.404
Knowledge 0.206 0.143 0.331 0.213 use under the name BHA¯SKARA, with the hope
thatitwillbeusefulforfutureresearchintomath
Average 0.208 0.200 0.394 0.263
reasoningmodels.
Table 4: Multi-task models are able to generalize to
unseentasksinsomecategories.Programoutput(Neo-
5.2 Results: Category-wiseAnalysis
P)alwaysoutperformsnumberoutput(Neo-A).
In this section we discuss the trends among the
taskswithineachcategory. Forbrevity,weprimar-
ilyconsiderBHA¯SKARA,theGPT-Neomulti-task
Data Answer(%F1) Program(%F1)
Neo Multi ∆ Neo Multi ∆ modelintheprogram-synthesissetting.
100% 28.4 32.3 +4.0 80.0 82.4 +2.5
40% 20.0 21.1 +1.2 75.2 70.3 -4.9 Mathability. Amongthetasksinthemathcate-
20% 15.8 18.4 +2.6 66.3 67.1 +0.8
gory,BHA¯SKARAexcelsinbasicmath,linearalge-
bra,andin-domainstatistics. Onthesetasks,itper-
Table 5: Here we show the results of fine-tuning
bothGPT-Neo-2.7B(Neo)andBHA¯SKARA(Multi)on formsequalorbettertoCodex. Ontheotherhand,
100%,40%,and20%oftheheld-outdatafrom L¯ILA- BHA¯SKARAstrugglesinadvancedmathandgeom-
OOD.TheMultialmostalwaysoutperformsNeo(the etry,withmediocreperformanceinmultiplication-
∆columnshowsthemargin). division, number theory, and calculus. Codex
showsanalogoustrends,exceptforperformingvery
0.6 Zero-shot Few-shot(3)
Model Dimension
GPT3 w/oInst w/Inst w/oInst w/Inst
0.5
Codex
Mathability 0.120 0.123 0.311 0.306
0.4 Language 0.124 0.131 0.352 0.350
Format 0.241 0.257 0.555 0.540
0.3 Knowledge 0.108 0.112 0.367 0.363
0.2 Average 0.148 0.156 0.396 0.390
0.1
Table 6: The IID scores for GPT-3 models with and
withoutinstructionprompting(Inst). Instructionhelps
0 1 3 max
slightlyinzero-shotsetting,butnotinfew-shotsetting.
Number of few-shot examples
Figure3: AverageF1scoresofGPT-3andCodexwith
differentnumbersoffew-shotexamplesinL¯ILA. dictionbeatsCodexprogramsynthesisinzero-to
one-shotsettings,butCodexovertakeswithmore
examples. Table6showsthatpromptingwithin-
welloncalculus(0.930)7.
structionsimprovesperformanceonlyinthezero-
Languagecomplexity. Modelsgenerallyshow shotsetting,meaningthatinthelimitedcontextsof
lower performance on program synthesis as lan- thepromptmodels,examplesaremoreimportant
guage complexity increases. BHA¯SKARA gets thaninstructionsformathematicalreasoning. This
meanF1over0.5onlyfordatasetswiththeleast isconsistentwiththefindingsofPurietal.(2022)
linguistic complexity where it achieves an F1 of oninstruction-exampleequivalence.
0.7.
Few-shot GPT-3 answer prediction underper-
Question format. Among the format tasks in formsBHA¯SKARA. Whileprompt-basedmodels
the dataset, BHA¯SKARA does exceptionally well outperformourfine-tunedmodelsingeneralwhen
onmultiple-choiceandnatural-languageinference, comparingwithindirect-answeringandprogram-
gettingperformancecloseto0.9onthelatter,and synthesis,whencomparingBHA¯SKARAprogram-
outperformingCodexonboth. Ontheotherhand, synthesistoGPT-3directansweringwefindthat
themodelperformscloseto0.25forreadingcom- themuchsmallerBHA¯SKARAconsistentlyoutper-
prehensionandfill-in-the-blank,thoughwith0.5 formsGPT-3.
F1onout-of-domainfill-in-the-blank.
Few-shot Codex performance is relatively
Backgroundknowledge. BHA¯SKARAperforms strong. Relative to the 2.7B trained models,
above0.5F1onlyforproblemsrequiringcommon- Codexdemonstratesstrongfew-shotIIDandOOD
senseandmathformulasandfailstodosimilarlyon performance. Some notable exceptions to this
problemsrequiringotherformsofexternalknowl- patternarethestatistics, linearalgebra, multiple-
edgelikephysics,computerscience,orreal-world choice question answering, and NLI tasks. Gen-
knowledge. erally, OODfew-shotperformsmuchbetterthan
OODforthefine-tunedmodels.
5.3 Results: Few-shotPrompting
Few-shot Codex fails on some tasks. Despite
Finally, we study the few-shot performance of
strongperformancerelativetoBHA¯SKARA,Codex
muchlargermodels( 175B),tobetterunderstand
≈ obtains less that 0.5 F1 on several tasks, with es-
the performance of the smaller trained models
pecially poor performance on geometry, number
( 2.7B)andtoprovideabenchmarkforevaluating
≈ theory, advanced math, complex language, com-
otherlargelanguagemodels. Overall,wefindthat
puterscienceproblems,scienceformulas,andreal
few-shot prompted models generally outperform
worldknowledge.
theirmuchsmallerbutfine-tunedcounterparts.
Instructions and more examples improve per- 6 Conclusion
formance. Wefindthatthenumberoffew-shot
In this work, we introduce L¯ILA, a unified math-
examplesgreatlyimpactspromptmodels’perfor-
ematicalreasoningbenchmarkforaholisticeval-
mance. Figure 3 shows that GPT-3 answer pre-
uation of AI agents. L¯ILA consists of 23 tasks
7NotethatthetrainingsetforCodexisnotknown. across4dimensions(i)mathematicalabilities,(ii)
1F
egarevA
language format, (iii) language complexity, (iv) JacobAustin,AugustusOdena,MaxwellNye,Maarten
externalknowledge. Itbuildson20existingmath- Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.
ematicalreasoningdatasetstocollectinstructions
2021. Programsynthesiswithlargelanguagemod-
and Python programs. Further, it also supports
els. arXivpreprintarXiv:2108.07732.
measuringout-of-distributionperformanceandro-
bustnesstolanguageperturbationsviaL¯ILA-OOD Sid Black, Leo Gao, Phil Wang, Connor Leahy, and
andL¯ILA-ROBUSTrespectively. Wealsointroduce StellaBiderman.2021. Gpt-neo:Largescaleautore-
gressivelanguagemodelingwithmesh-tensorflow.
BHA¯SKARA, a 2.7B-parameter fine-tuned multi-
task model. We find thatmulti-tasking improves
Ronan Le Bras, Swabha Swayamdipta, Chandra Bha-
oversingle-taskperformanceby21.83%F1score gavatula,RowanZellers,MatthewEPeters,Ashish
onaverage,andthatourmodelisastrongstarting Sabharwal,andYejinChoi.2020. Adversarialfilters
ofdatasetbiases. arXivpreprintarXiv:2002.04108.
pointforfurtherfine-tuningonnewmathreason-
ingtasks. Thebestperformingmodelweevaluate
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
achievesonly60.40%F1indicatingthepotential
Subbiah, Jared D Kaplan, Prafulla Dhariwal,
forimprovementontheproposedbenchmark. Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
6.1 Limitations Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
One drawback of our unified format is the diffi-
Clemens Winter, Chris Hesse, Mark Chen, Eric
culty of evaluating models. In our work we use Sigler,MateuszLitwin,ScottGray,BenjaminChess,
F1forlackofabetteralternative. F1likelyover- Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
estimatesperformance,e.g.,giventhegoldanswer
2020. Language models are few-shot learners. In
“2apples”,thepredictedanswers“2”and“apples”
AdvancesinNeuralInformationProcessingSystems,
receivethesamescore,thoughtheformerisbetter. volume 33, pages 1877–1901. Curran Associates,
L¯ILAcontains23taskswhicharecreatedfrom Inc.
20 datasets and 44 sub-datasets. There is scope
MarkChen,JerryTworek,HeewooJun,QimingYuan,
toaddmoremathematicalreasoningdatasets(e.g.,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
theoremproving.) Theflexibleunifiedformatof Harrison Edwards, Yuri Burda, Nicholas Joseph,
L¯ILA allows for future extensions. Additionally, Greg Brockman, et al. 2021. Evaluating large lan-
ourcategorizationprovidesawaytoidentifyareas guagemodelstrainedoncode.
forextension. Forinstance,weonlyhave1dataset
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
forlinearalgebra,whichhappenstonotusenatural
ian, Jacob Hilton, Reiichiro Nakano, Christopher
language,andtakestheformofgenerativeQA.Our Hesse, and John Schulman. 2021. Training veri-
benchmarkwillbenefitfromfuturelinearalgebra fiers to solve math word problems. arXiv preprint
additions,perhapswithwordproblemsformatted
arXiv:2110.14168.
asfill-in-the-blankquestions.
Henry T Colebrooke. 1817. Arithmetic and mensura-
tionofbrahmeguptaandbhaskara.
References
Dheeru Dua, Ananth Gottumukkala, Alon Talmor,
Sameer Singh, and Matt Gardner. 2019a. Orb: An
GillesAdda,BenoîtSagot,KarënFort,andJosephMar-
open reading benchmark for comprehensive evalu-
iani.2011. Crowdsourcingforlanguageresourcede-
ation of machine reading comprehension. arXiv
velopment: Criticalanalysisofamazonmechanical
preprintarXiv:1912.12598.
turk overpowering use. In 5th Language and Tech-
nologyConference.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel
Armen Aghajanyan, Anchit Gupta, Akshat Shrivas- Stanovsky,SameerSingh,andMattGardner.2019b.
tava, Xilun Chen, Luke Zettlemoyer, and Sonal Drop: Areadingcomprehensionbenchmarkrequir-
Gupta. 2021. Muppet: Massive multi-task rep- ingdiscretereasoningoverparagraphs. InProceed-
resentations with pre-finetuning. arXiv preprint ingsofthe2019ConferenceoftheNorthAmerican
arXiv:2101.11038. Chapter of the Association for Computational Lin-
guistics: HumanLanguageTechnologies,Volume1
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel- (LongandShortPapers),pages2368–2378.
Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
2019. Mathqa: Towards interpretable math word Karën Fort, Gilles Adda, and Kevin Bretonnel Cohen.
problem solving with operation-based formalisms. 2011. Amazonmechanicalturk: Goldmineorcoal
arXivpreprintarXiv:1905.13319. mine? ComputationalLinguistics,pages413–420.
LeoGao,StellaBiderman,SidBlack,LaurenceGold- the 2016 Conference of the North American Chap-
ing, Travis Hoppe, Charles Foster, Jason Phang, teroftheAssociationforComputationalLinguistics:
Horace He, Anish Thite, Noa Nabeshima, Shawn HumanLanguageTechnologies,pages1152–1157.
Presser, and Connor Leahy. 2020. The Pile: An
800gbdatasetofdiversetextforlanguagemodeling. Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
arXivpreprintarXiv:2101.00027. Regina Barzilay. 2014. Learning to automatically
solvealgebrawordproblems. InProceedingsofthe
Jesse Michael Han, Jason M. Rute, Yuhuai Wu, Ed- 52ndAnnualMeetingoftheAssociationforCompu-
wardW.Ayers, andStanislasPolu.2021. Proofar- tationalLinguistics(Volume1:LongPapers),pages
tifactco-trainingfortheoremprovingwithlanguage 271–281.
models. ArXiv,abs/2102.06203.
WendaLi,LeiYu,YuhuaiWu,andLawrenceC.Paul-
DanHendrycks,StevenBasart,SauravKadavath,Man- son. 2021. Isarstep: a benchmark for high-level
tasMazeika,AkulArora,EthanGuo,CollinBurns, mathematical reasoning. In International Confer-
SamirPuranik,HoraceHe,DawnSong,etal.2021a. enceonLearningRepresentations.
Measuringcodingchallengecompetencewithapps.
Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-
arXivpreprintarXiv:2105.09938.
angRen.2020. Birdshavefourlegs?! numersense:
Probingnumericalcommonsenseknowledgeofpre-
DanHendrycks,CollinBurns,SauravKadavath,Akul
trained language models. In Proceedings of the
Arora, Steven Basart, Eric Tang, Dawn Song, and
2020 Conference on Empirical Methods in Natural
Jacob Steinhardt. 2021b. Measuring mathemati-
cal problem solving with the math dataset. arXiv
LanguageProcessing(EMNLP),pages6862–6868.
preprintarXiv:2103.03874.
WangLing,DaniYogatama,ChrisDyer,andPhilBlun-
som.2017. Programinductionbyrationalegenera-
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam
tion: Learning to solve and explain algebraic word
Dziedzic,RishabhKrishnan,andDawnSong.2020.
problems. arXivpreprintarXiv:1705.04146.
Pretrained transformers improve out-of-distribution
robustness. InProceedingsofthe58thAnnualMeet-
NicholasLourie,RonanLeBras,ChandraBhagavatula,
ingoftheAssociationforComputationalLinguistics,
andYejinChoi.2021. Unicornonrainbow: Auni-
pages2744–2751.
versalcommonsensereasoningmodelonanewmul-
titaskbenchmark. InProceedingsoftheAAAICon-
Mohammad Javad Hosseini, Hannaneh Hajishirzi,
ferenceonArtificialIntelligence, volume35, pages
Oren Etzioni, and Nate Kushman. 2014. Learning
13480–13488.
tosolvearithmeticwordproblemswithverbcatego-
rization. InInConferenceonEmpiricalMethodsin Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan
NaturalLanguageProcessing(EMNLP).
Huang,XiaodanLiang,andSong-ChunZhu.2021a.
Inter-gps: Interpretable geometry problem solving
DanqingHuang,ShumingShi,Chin-YewLin,JianYin,
with formal language and symbolic reasoning. In
and Wei-Ying Ma. 2016. How well do comput-
The59thAnnualMeetingoftheAssociationforCom-
ers solve math word problems? large-scale dataset
putationalLinguistics(ACL).
construction and evaluation. In Proceedings of the
54thAnnualMeetingoftheAssociationforCompu- Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian
tationalLinguistics(Volume1:LongPapers),pages Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter
887–896. Clark, and Ashwin Kalyan. 2022. Dynamic
prompt learning via policy gradient for semi-
Daniel Khashabi, Tushar Khot, Ashish Sabharwal, structured mathematical reasoning. arXiv preprint
Oyvind Tafjord, Peter Clark, and Hannaneh Ha- arXiv:2209.14610.
jishirzi. 2020. Unifiedqa: Crossing format bound-
aries with a single qa system. arXiv preprint PanLu,LiangQiu,JiaqiChen,TonyXia,YizhouZhao,
arXiv:2005.00700. Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-
Chun Zhu. 2021b. Iconqa: A new benchmark for
Aditya Kolachana, K Mahesh, and K Ramasubrama- abstractdiagramunderstandingandvisuallanguage
nian. 2019. Use of calculus in hindu mathematics. reasoning. InThe35thConferenceonNeuralInfor-
In Studies in Indian Mathematics and Astronomy, mation Processing Systems Track on Datasets and
pages345–355.Springer. Benchmarks(NeurIPS2021).
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish BryanMcCann,NitishShirishKeskar,CaimingXiong,
Sabharwal, Oren Etzioni, and Siena Dumas Ang. andRichardSocher.2018. Thenaturallanguagede-
2015. Parsing algebraic word problems into equa- cathlon: Multitask learning as question answering.
tions. TransactionsoftheAssociationforComputa- arXivpreprintarXiv:1806.08730.
tionalLinguistics,3:585–597.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
RikKoncel-Kedziorski,SubhroRoy,AidaAmini,Nate 2020a. A diverse corpus for evaluating and devel-
Kushman,andHannanehHajishirzi.2016. Mawps: oping English math word problem solvers. In Pro-
Amathwordproblemrepository. InProceedingsof ceedingsofthe58thAnnualMeetingoftheAssocia-
tionforComputationalLinguistics, pages975–984, Subhro Roy and Dan Roth. 2016. Solving gen-
Online.AssociationforComputationalLinguistics. eral arithmetic word problems. arXiv preprint
arXiv:1608.01413.
Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020b. Adiversecorpusforevaluatinganddevelop- Subhro Roy and Dan Roth. 2017. Unit dependency
ingenglishmathwordproblemsolvers. InProceed- graphanditsapplicationtoarithmeticwordproblem
ingsofthe58thAnnualMeetingoftheAssociation solving. InThirty-FirstAAAIConferenceonArtifi-
forComputationalLinguistics,pages975–984. cialIntelligence.
SwaroopMishra,DanielKhashabi,ChittaBaral,Yejin SubhroRoyandDanRoth.2018. Mappingtodeclara-
Choi, and Hannaneh Hajishirzi. 2022a. Reframing tiveknowledgeforwordproblemsolving. Transac-
instructionalpromptstoGPTk’slanguage. InFind- tionsoftheAssociationforComputationalLinguis-
ings of the Association for Computational Linguis- tics,6:159–172.
tics:ACL2022,pages589–612,Dublin,Ireland.As-
SubhroRoy,TimVieira,andDanRoth.2015. Reason-
sociationforComputationalLinguistics.
ing about quantities in natural language. Transac-
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and tionsoftheAssociationforComputationalLinguis-
HannanehHajishirzi.2022b. Cross-taskgeneraliza- tics,3:1–13.
tionvianaturallanguagecrowdsourcinginstructions.
VictorSanh, AlbertWebson, ColinRaffel, StephenH
InProceedingsofthe60thAnnualMeetingoftheAs-
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
sociation for Computational Linguistics (Volume 1:
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun
LongPapers),pages3470–3487.
Raja, et al. 2021. Multitask prompted training en-
Swaroop Mishra, Arindam Mitra, Neeraj Varshney, ables zero-shot task generalization. arXiv preprint
BhavdeepSachdeva, PeterClark, ChittaBaral, and
arXiv:2110.08207.
Ashwin Kalyan. 2022c. Numglue: A suite of fun-
Benoy Kumar Sarkar. 1918. Hindu Achievements in
damental yet challenging mathematical reasoning
Exact Science: A Study in the History of Scientific
tasks. InProceedingsofthe60thAnnualMeetingof
Development. Longmans,GreenandCompany.
theAssociationforComputationalLinguistics(Vol-
ume1:LongPapers),pages3505–3523. David Saxton, Edward Grefenstette, Felix Hill, and
PushmeetKohli.2019. Analysingmathematicalrea-
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
soning abilities of neural models. arXiv preprint
2021. AreNLPmodelsreallyabletosolvesimple
arXiv:1904.01557.
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
AssociationforComputationalLinguistics: Human AbuAwalMdShoeb,AbubakarAbid,AdamFisch,
Language Technologies, pages 2080–2094, Online. Adam R Brown, Adam Santoro, Aditya Gupta,
AssociationforComputationalLinguistics. Adrià Garriga-Alonso, et al. 2022. Beyond the
imitation game: Quantifying and extrapolating the
Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, capabilities of language models. arXiv preprint
and Chitta Baral. 2022. How many data samples arXiv:2206.04615.
is an additional instruction worth? arXiv preprint
arXiv:2203.09161. Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau
Yih,andAshishSabharwal.2019. Quarel:Adataset
Abhilasha Ravichander, Aakanksha Naik, Carolyn and models for answering questions about qualita-
Rose, and Eduard Hovy. 2019. Equate: A bench- tiverelationships. InProceedingsoftheAAAICon-
mark evaluation framework for quantitative reason- ferenceonArtificialIntelligence, volume33, pages
ing in natural language inference. arXiv preprint 7063–7071.
arXiv:1901.03735.
ShyamUpadhyayandMing-WeiChang.2015. Draw:
MarcoTulioRibeiro,TongshuangWu,CarlosGuestrin, Achallenginganddiversealgebrawordproblemset.
andSameerSingh.2020. Beyondaccuracy: Behav- Technicalreport,Citeseer.
ioral testing of nlp models with checklist. In Pro-
ceedings of the 58th Annual Meeting of the Asso- Shyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang,
ciationforComputationalLinguistics, pages4902– andWen-tauYih.2016. Learningfromexplicitand
4912. implicit supervision jointly for algebra word prob-
lems. In Proceedings of the 2016 Conference on
Anna Rogers, Matt Gardner, and Isabelle Augenstein. EmpiricalMethodsinNaturalLanguageProcessing,
2021. Qadatasetexplosion: Ataxonomyofnlpre- pages297–306.
sourcesforquestionansweringandreadingcompre-
hension. arXivpreprintarXiv:2107.12708. Alex Wang, Yada Pruksachatkun, Nikita Nangia,
AmanpreetSingh,JulianMichael,FelixHill,Omer
SubhroRoyandDanRoth.2015. Solvinggeneralarith- Levy, and Samuel Bowman. 2019. Superglue: A
metic word problems. In Proceedings of the 2015 stickierbenchmarkforgeneral-purposelanguageun-
Conference on Empirical Methods in Natural Lan- derstanding systems. In Advances in Neural Infor-
guageProcessing,pages1743–1752. mationProcessingSystems,pages3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan
Hill, Omer Levy, and Samuel R Bowman. 2018. Roth. 2019. “going on a vacation” takes longer
Glue:Amulti-taskbenchmarkandanalysisplatform than“goingforawalk”: Astudyoftemporalcom-
for natural language understanding. arXiv preprint monsense understanding. In Proceedings of the
arXiv:1804.07461. 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Yizhong Wang, Swaroop Mishra, Pegah Alipoor- Joint Conference on Natural Language Processing
molabashi, Yeganeh Kordi, Amirreza Mirzaei, (EMNLP-IJCNLP),pages3363–3369.
Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, et al.
2022. Benchmarking generalization via in-context
instructions on 1,600+ language tasks. arXiv
preprintarXiv:2204.07705.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drewMDai,andQuocVLe.2021. Finetunedlan-
guagemodelsarezero-shotlearners. arXivpreprint
arXiv:2109.01652.
Sean Welleck, Jiacheng Liu, Ronan Le Bras, Han-
naneh Hajishirzi, Yejin Choi, and Kyunghyun Cho.
2021. Naturalproofs: Mathematical theorem prov-
ing in natural language. In Thirty-fifth Conference
onNeuralInformationProcessingSystemsDatasets
andBenchmarksTrack(Round1).
Sean Welleck, Peter West, Jize Cao, and Yejin Choi.
2022. Symbolicbrittlenessinsequencemodels: on
systematic generalization in symbolic mathematics.
InAAAI.
JasonWeston,AntoineBordes,SumitChopra,Alexan-
derMRush,BartvanMerriënboer,ArmandJoulin,
and Tomas Mikolov. 2015. Towards ai-complete
questionanswering: Asetofprerequisitetoytasks.
arXivpreprintarXiv:1502.05698.
YuhuaiWu,AlbertJiang,JimmyBa,andRogerBaker
Grosse.2021. {INT}: Aninequalitybenchmarkfor
evaluating generalization in theorem proving. In
International Conference on Learning Representa-
tions.
Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan
Vasilescu, and Graham Neubig. 2018. Learning to
mine aligned code and natural language pairs from
stackoverflow. InInternationalConferenceonMin-
ing Software Repositories, MSR, pages 476–486.
ACM.
Amir R Zamir, Alexander Sax, William Shen,
Leonidas J Guibas, Jitendra Malik, and Silvio
Savarese. 2018. Taskonomy: Disentangling task
transfer learning. In Proceedings of the IEEE con-
ferenceoncomputervisionandpatternrecognition,
pages3712–3722.
Xikun Zhang, Deepak Ramachandran, Ian Tenney,
Yanai Elazar, and Dan Roth. Do language embed-
dingscapturescales?
KunhaoZheng,JesseMichaelHan,andStanislasPolu.
2021. Minif2f: a cross-system benchmark for for-
mal olympiad-level mathematics. arXiv preprint
arXiv:2109.00110.
A QualitativeExamples noveltemplatesarenamedROBUST-IR,ROBUST-
AP, ROBUST-ADJ, ROBUST-Q, ROBUST-RQ,
Figures4and5giveexamplesofinput-outputbe-
andROBUST-RM.ROBUST-IRreferstoadding
haviorofBHA¯SKARA. Figure6givesanexample
informationthatisunhelpfulforsolvingtheques-
ofanon-compilingoutputprogram.
tionbutmayberelatedtothecontextoftheprob-
lem. ROBUST-APreferstoincreasingproblemver-
B DatasetCollection
bositybyturningactivespeechtopassivespeech.
ROBUST-ADJ refers to increasing problem ver-
Tables12-15giveexamplesanddatasetsfromeach
bositybyaddingadjectivesoradverbs. ROBUST-
taskforeachcategory.
Q indicates turning a problem statement into a
question,inthestyleofaconversationwithastu-
Category Examples Datasets
dent. ROBUST-RQ indicates removing question
Math Table8 Table12 wordsinaproblemandturningitintoastatement;
Language Table9 Table13 it is roughly the inverse of ROBUST-Q. Finally,
Format Table10 Table14 ROBUST-RMreferstotheremovalofmathemat-
Knowledge Table11 Table15 icstermsthatareimplicitlydefined. Examplesof
eachtemplatearefoundinTable16.
Table7:Examplesanddatasetsmeta-table.
Forourcrowdsourcingpipeline,weprovideeach
Amazon Mechanical Turk worker with 10 ques-
tions split from 20 questions sampled from each
B.1 Expertannotation
dataset. We run a separate job for each of our 9
In the worker qualification process, we ask each templates. Inparticular,eachHITcontainsthe10
worker to annotate 30 questions. We manually splitquestionsfromtheoriginaldatasets,alongside
verify each annotation and qualify those whose theproblemsolution. Workersareaskedtosubmit
Pythonannotationsaresatisfactory. Wealsopro- anaugmentationforeachquestionaccordingtothe
videfeedbacksuchas"writesimplerprograms,use style of the template assigned to each job. Thus,
representativevariablenamesinsteadofjustletters, werun9separatejobstoobtainaugmentationsfor
add comments wherever possible" to annotators all templates across all datasets. To familiarize
aftertheworkerqualificationprocess. Weinstruct workerswiththeintendedstyleofeachtemplate,
annotatorstouseaminimalsetofPythonlibraries, weprovide3demonstrativeaugmentationswithin
andweaskthemtorecordthePythonlibrariesthey theinstructionsofeachHIT,assummarizedinTa-
useinacommondocument. Wefindthatthean- ble16. Werestrictourcrowdsourcingpipelineto
notatorscouldgetthetaskdonejustbyusingthe workersthathadabovea98%acceptanceratewith
sympyandthedatetimelibraries. Wealsoaskan- over1000completedHITs. Weprovideworkers
notatorstoreportanybugsinanswerannotation, with an upper bound of 1 hour to complete each
whichtheyreportforasmallnumberofquestions; HITbutspecifyintheinstructionsthateachHIT
wesubsequentlyfixthose. shouldfeasiblebecompletedin10minutes. Based
Wegive10samplequestionannotationstoanno- onminimumwagepoliciesandundertheassump-
tatorsasillustrativeexampleswhichvaryinstruc- tionthatworkersfollowthe10-minutecompletion
ture,length,format,underlyingreasoningskill,etc. guideline,weaccordinglycompensate$3perHIT.
Wepay20dollarsperhourupto20hoursperweek Finally,toensuredatasetqualityofgenerationsvia
ascompensationforthedataannotationwork. the Amazon Mechanical Turk (Fort et al., 2011;
Addaetal.,2011),wemanuallyassesstheworker
LI¯LA-ROBUST To create the L¯ILA-ROBUST augmentationsproducedforeachtemplate.
dataset,wefirstdefineasetof9templates,consist-
ingof3variationstylesdefinedinSVAMP(Patel
C DatasetStatistics
etal.,2021)aswellas6noveltemplatesofourown.
WerefertotheSVAMPtemplatesasSVAMP-COO, Figure8givesrelativessizesoftaskswithineach
SVAMP-COP,andSVAMP-IU,whichcorrespond category. Figure9illustratestheunigramfrequen-
tochangingtheorderofobjects,changingtheorder ciesin L¯ILA, wherelargerwordsindicatehigher
ofphrases,andaddingirrelevant,unhelpfulinfor- frequency. Table 17 gives comprehensive statis-
mationtotheproblemstatement,respectively. Our ticsoneachtask. Table19citeseachcomponent
Task:BasicMath Task:Muldiv
Problem:BeforeDecember,customersbuy1346ear Problem:Ticketstotheschoolplaycost6forstudents
muffsfromthemall.DuringDecember,theybuy6444, and8foradults. If20studentsand12adultsbought
andtherearenone.Inall,howmanyearmuffsdothe tickets,howmanydollars’worthofticketsweresold?
customersbuy?
PredictedAnswer:487
PredictedAnswer:1346.07 GeneratedProgram:
GeneratedProgram: a=20*6
answer = 1346.0 + 6444.0 b=12*8
print(answer) c=a+b
# Result ==> 7790.0 answer=c
print(answer)
GoldAnswer:7790.03 # Result ==> 216.0
GoldAnswer:2163
Figure4:ExampleswithBHA¯SKARAonBasicMathandMuldiv.
Task:Statistics Task:LinearAlgebra
Problem:DeterminetheKLDivergencebetweenthe Problem:Findthedeterminantofthematrix
empiricaldistributions
0.149,0.039,0.099,0.09,0.203,0.077,0.039,0.19 0 2 3
− −
and 0 5 0 .
0.158,0.256,0.042,0.008,0.064,0.051,0.05,0.072.  1 3 2 
 
PredictedAnswer:0.367
GeneratedProgram: PredictedAnswer: −87
GeneratedProgram:
from scipy.stats import entropy
import numpy as np
distribution1 = [0.149, 0.039,
0.099, 0.09, 0.203, a = np.array([
0.077, 0.039, 0.19] [0, -2, -3],
[0, 5, 0],
distribution2 = [0.158, 0.256, [1, 3, 2]])
0.042, 0.008, 0.064,
0.051, 0.05, 0.072] print(np.linalg.det(a))
# Result ==> 15.0
print(entropy(distribution1, qk=
distribution2)) GoldAnswer:153
# Result ==> 0.5122...
GoldAnswer:0.5122...3
Figure5:ExampleswithBHA¯SKARAonStatisticsandLinearAlgebra.
datasetofL¯ILA.
D AdditionalResults
Table 18 gives the unaggregated performance of
eachmodeloneachdatasetinL¯ILA(somedatasets
aresplitacrosstasks).
Model:Codex Model:BHA¯SKARA
Task:Statistics Task:Muldiv
Problem:Simplifytheexpression(9x2+3x+7)+ Problem:Jennycollectscansandbottlestotakedown
(3x2+7x5+2).Expressyouranswerasapolynomial totherecyclingcenter. Eachbottleweighs6ounces
withtermsarrangedindecreasingorderofdegree. andeachcanweighs2ounces. Jennycancarryato-
talof100ounces. Shecollects20cansandasmany
GeneratedProgram: bottlesasshecancarry. Ifshegetspaid10centsper
from sympy import Poly bottleand3centspercan,howmuchmoneydoesshe
make(incents)?
p = Poly(9*(x**2) + 3*x + 7 +
3*(x**2) + 7*(x**5) + 2) GeneratedProgram:
a=20*6
answer = p.as_expr() b=a*2
c=b*3
print(answer) d=c*10
# ==> NameError (x is not defined) e=d*3
f=e*3
GoldAnswer:7x5+12x2+3x+9 g=f+g
answer=g
print(answer)
# ==> NameError (g is not defined)
GoldAnswer:216
Figure6:NameErrorsinCodexandBHA¯SKARA.
Question:Agardenerisgoingtoplant2redrosebushesand2whiterosebushes.Ifthegardeneristoselecteachofthebushes
atrandom,oneatatime,andplanttheminarow,whatistheprobabilitythatthe2rosebushesinthemiddleoftherowwillbe
theredrosebushes?
Options:{A:1/12, B:1/6, C:1/5, D:1/3, E:1/2}
Answer:B
Explanation:Weareaskedtofindtheprobabilityofoneparticularpattern:wrrw.Total#ofwaysagardenercanplantthese
fourbushesisthe#ofpermutationsof4letterswwrr,outofwhich2w’sand2r’sareidentical,so4!/2!2!=6;sop=1/
6.Answer:B.
Program:import scipy
n0 = 2.0
n1 = 2.0
n2 = 2.0
t0 = n0 + n0
t1 = scipy.special.comb(t0, n0)
answer = 1.0 / t1
Figure7:Anexampleofinstructionannotation.
Task Questioncategory Example
Basicmath:addition,subtraction,fact Question:IfJimbois484feetawayfromabeetleandquarterof827feetaway
TASK1 basedQAetc. fromagrasshopper,whichinsectwillseembiggertohim?"Option1":beetle,
"Option2":grasshopperAnswer:Option2
Muldiv:multiplication,divisionalong Question:Mrs.Hiltbought2pizzas.Eachpizzahad8slices.So,shehad__
TASK2
withaddition,subtractionetc. totalslicesofpizza.Answer:16
Numbertheory:prime,power,negation, Question:Howmanynumbersaredivisiblebyboth2and3upto300?An-
TASK3
modulusandotheroperatorsetc. swer:50
Algebra:equations,functions,polyno- Question: Thesumofthethreesmallestoffourconsecutiveintegersis30
TASK4
mials,seriesetc. morethanthelargestinteger.Whatarethefourconsecutiveintegers?Answer:
[15,16,17,18]
Geometry: triangles, polygons, 3D Question:Ahallis6meterslongand6meterswide.Ifthesumoftheareasof
TASK5 structuresetc. thefloorandtheceilingisequaltothesumoftheareasoffourwalls,whatis
thevolumeofthehall(incubicmeters)?Answer:108
Statistics:binomial,divergence,mean, Question:Thereare11boysand10girlsinaclass.Iffivestudentsareselected
TASK6
median,mode,varianceetc. atrandom,inhowmanywaysthat3girland2boysareselected? Answer:
6600
Calculus: differentiation, integration, Question:Letg(y)=9*y**4+25*y**2+6.Lets(d)=1-d**4.Letx(t)=
TASK7
gradient,seriesexpansionetc. -g(t)+6*s(t).Whatisthethirdderivativeofx(f)wrtf?Answer:-360*f
Linearalgebra: vectors,dotproducts, Question:Problem:Convertthefollowingmatrixtoreducedrowechelonform:
TASK8 Eigenvectors,matricesetc. 7
5
−
12
0
−
210 −4
7
.Answer:
1
0
0
1
−91 13
0
−691 43
0
(cid:18) − − − (cid:19) (cid:18) 20 80 (cid:19)
Advanced math: heuristics required Question:Letf(x)=2x.Find f(f(f(f(1)))).Answer:256
TASK9 alongwithprobability,statistics,oral-
p
gebra,Olympiadlevelproblems
Table8:ExampleofeachtaskinthemathabilitycategoryoftheL¯ILAbenchmark.
Task Questioncategory Example
TASK10 Nolanguage Computethemedianof4√2, 6,3e,3, 6, 14,6.Answer:3
− − −√π
Simplelanguage Question:Joanhad9blueballoons,butSallypopped5ofthem.Jessicahas2
TASK11
blueballoons.Theyhave__blueballoonsnow.Answer:6
Complexlanguage:involvingco-reference Question:Passage:Accordingtothe2011NationalHouseholdSurvey,89.3%
resolution etc., multi-sentence language, ofMarkhamsresidentsareCanadiancitizens,andabout14.5%ofresidentsare
adversarial language: containing tricky recentimmigrants(from2001to2011). TheracialmakeupofMarkhamis;
wordsetc.,oftencreatedadversarially EastAsian(39.7%),WhiteCanadian(27.5%),SouthAsianCanadian(19.1%),
SoutheastAsian(3.9%),BlackCanadians(3.2%),WestAsian&ArabCanadi-
TASK12 ans(3.2%),LatinAmericanCanadian(0.5%),AboriginalpeoplesinCanada
(0.2%),and1.9%ofthepopulationismultiracialwhiletherestofthepopu-
lation(0.7%)isofanothergroup.Markhamhasthehighestvisibleminority
populationofanymajorCanadiancity(over100,000residents)at72.3%,and
isoneofeightmajorcitieswithnomajorityracialgroup.Question:Howmany
percentofpeoplewerenotwhite?Answer:72.5
Table9:ExampleofeachtaskinthelanguagecomplexitycategoryoftheL¯ILAbenchmark.
Task Questioncategory Example
TASK13 Fillintheblank Question:Delphiniumhas_floretsortheyarefullofholes.Answer:no
TASK14 Generativequestionanswering Question:Calculatetheremainderwhen160isdividedby125.Answer:35
Multiple choice question answering Question:Thefishglidedwithaspeedof8m/sthroughthewaterand5m/s
TASK15 (MCQ) throughthejellobecausethe__issmoother? "Option1":jello,"Option2":
water.Answer:Option2
Naturallanguageinference(NLI) Question: "statement1": Alyssapicked42.0pearsfromthepeartreeand
TASK16 Nancysold17.0ofthepears,"statement2":25.0pearswereleft,"options:"
Entailmentorcontradiction?Answer:Entailment
Readingcomprehension(RC) Question:Passage:AlategamerallybyWashingtonledthemtotheEagles’
26yardline.AshottotheendzonebyRobertGriffinIIIwouldbeintercepted
byBrandonBoykin,clinchinganEagleswin.TheEagleswouldmoveto6-5.
ThisistheEaglesfirstwinatLincolnFinancialFieldsinceWeek4ofthe2012
TASK17
season,becausepriortothisgame,theEagleshadneverwonagameintheir
homestadiumin414dayssincethatsameweek,snappinga10-gamelosing
streakathomewiththiswin.Question:Howmanymorewinsthanlossesdid
theEagleshaveafterthisgame?Answer:1
Table10:ExampleofeachtaskinthequestionformatcategoryoftheL¯ILAbenchmark.
Task Questioncategory Example
Noexternalknowledge:onlymathemati- Question:Ifthereare7bottlecapsinaboxandLindaputs7morebottlecaps
TASK18
calcommonsenseknowledgerequired inside,howmanybottlecapsareinthebox?Answer:14
Commonsense:temporalcommonsense Question:Outsidetemple,thereisashopwhichcharges12dollarsforeach
knowledge(e.g.,peopleusuallyplaybas- object.Pleasenotethatoneshoeiscountedasanobject.Sameistrueforsocks
TASK19 ketballforafewhoursandnotdays),nu- andmobiles.Paisleywenttotemplewithbothparents.Allofthemkepttheir
merical commonsense knowledge (e.g. shoes,socksandmobilesintheshop.Howmuchtheyhavetopay?Answer:
birdshas2legs) 180
Mathformulas:algebra,geometry,prob- Question:Simplify-3*(sqrt(1700)-(sqrt(1700)+(3+sqrt(1700))*-6))+-3.
TASK20
abilityetc. Answer:-180*sqrt(17)-57
Scienceformulas:physics,chemistryetc. Question:FindthenumberofmolesofH2Oformedoncombining2molesof
TASK21
NaOHand2molesofHCl.Answer:2
Computerscienceknowledge:datastruc- Question:Applyfunctions‘mean’and‘std’toeachcolumnindataframe‘df’
TASK22
turealgorithmslikemergesortetc. Answer:df.groupby(lambdaidx:0).agg([’mean’,’std’])
Real-world knowledge: COVID mod- Question:Ourphysicsclubhas20members,amongwhichwehave3officers:
elling,climatemodellingetc. President,VicePresident,andTreasurer.However,onemember,Alex,hates
TASK23 anothermember,Bob.HowmanywayscanwefilltheofficesifAlexrefusesto
serveasanofficerifBobisalsoanofficer?(Nopersonisallowedtoholdmore
thanoneoffice.)Answer:6732
Table11:ExampleofeachtaskinthebackgroundknowledgecategoryoftheL¯ILAbenchmark.
Task Mathcategory IID OOD
addsub.json MCTaco_event_duration_structured.json
Numersense_structured.json NumGLUE_Task3.json
MCTaco_stationarity_structured.json
TASK1 Basicmath MCTaco_frequency_structured.json
MCTaco_event_typical_time_structured.json
MCTaco_event_ordering_structured.json
NumGLUE_Task7.json
singleop.json svamp_structured.json
multiarith.json NumGLUE_Task4.json
asdiv.json
TASK2 Muldiv GSM8k_structured.json
NumGLUE_Task1.json
NumGLUE_Task2.json
deepmind_mathematics_muldiv.json
mathqa_physics.json mbpp_structured.json
APPS_structured.json mathqa_other.json
mathqa_gain.json
amps_number_theory.json
TASK8 Numbertheory
mathqa_general.json
conala_structured.json
NumGLUE_Task5.json
deepmind_mathematics_numbertheory.json
singleq.json draw_structured.json
simuleq.json dolphin_structured.json
TASK4 Algebra amps_algebra.json
NumGLUE_Task8.json
deepmind_mathematics_algebra.json
TASK5 Geometry amps_geometry.json mathqa_geometry.json
TASK6 Statistics amps_counting_and_stats.json mathqa_probability.json
amps_calculus.json deepmind_mathematics_calculus.json
TASK7 Calculus
deepmind_mathematics_basicmath.json
TASK8 Linearalgebra amps_linear_algebra.json
TASK9 Advancedmath MATH_crowdsourced.json
Table12:RawdatasetsusedtocreatedifferenttasksinL¯ILAacrossdifferentmathcategories.
ID Languagecategory IID OOD
amps_number_theory.json amps_algebra.json
amps_counting_and_stats.json deepmind_mathematics_calculus.json
amps_calculus.json
amps_linear_algebra.json
TASK10 Nolanguage
deepmind_mathematics_muldiv.json
deepmind_mathematics_numbertheory.json
deepmind_mathematics_algebra.json
deepmind_mathematics_basicmath.json
addsub.json MCTaco_frequency_structured.json
Numersense_structured.json NumGLUE_Task1.json
MCTaco_stationarity_structured.json mathqa_general.json
MCTaco_event_typical_time_structured.json NumGLUE_Task4.json
MCTaco_event_ordering_structured.json
MCTaco_event_duration_structured.json
singleop.json
multiarith.json
asdiv.json
TASK11 Simplelanguage GSM8k_structured.json
APPS_structured.json
mathqa_gain.json
mathqa_other.json
singleq.json
simuleq.json
NumGLUE_Task8.json
draw_structured.json
dolphin_structured.json
mathqa_probability.json
mathqa_physics.json mbpp_structured.json
APPS_structured.json mathqa_other.json
mathqa_gain.json
amps_number_theory.json
TASK12 Complexlanguage mathqa_general.json
conala_structured.json
NumGLUE_Task5.json
deepmind_mathematics_numbertheory.json
Table13:RawdatasetsusedtocreatedifferenttasksinL¯ILAacrossdifferentlanguagecategories.
ID Formatcategory IID OOD
TASK13 Fillintheblank NumGLUE_Task4.json Numersense_structured.json
amps_number_theory.json svamp_structured.json
amps_counting_and_stats.json mathqa_geometry.json
amps_linear_algebra.json amps_calculus.json
amps_algebra.json singleq.json
deepmind_mathematics_calculus.json NumGLUE_Task2.json
addsub.json mbpp_structured.json
singleop.json deepmind_mathematics_numbertheory.json
multiarith.json
asdiv.json
GSM8k_structured.json
APPS_structured.json
mathqa_gain.json
mathqa_other.json
simuleq.json
TASK14 GenerativeQA
NumGLUE_Task8.json
draw_structured.json
dolphin_structured.json
mathqa_probability.json
MCTaco_frequency_structured.json
NumGLUE_Task1.json
mathqa_general.json
mathqa_physics.json
conala_structured.json
amps_geometry.json
MATH_crowdsourced.json
deepmind_mathematics_calculus.json
deepmind_mathematics_muldiv.json
deepmind_mathematics_algebra.json
deepmind_mathematics_basicmath.json
NumGLUE_Task3.json MCTaco_event_typical_time_structured.json
MCTaco_stationarity_structured.json
TASK15 MCQ
MCTaco_event_ordering_structured.json
MCTaco_event_duration_structured.json
TASK16 NLI NumGLUE_Task5.json
TASK17 RC mathqa_physics.json mbpp_structured.json
Table14:RawdatasetsusedtocreatedifferenttasksinL¯ILAacrossdifferentformatcategories.
ID Knowledgecategory IID OOD
addsub.json NumGLUE_Task4.json
singleop.json GSM8k_structured.json
multiarith.json svamp_structured.json
asdiv.json NumGLUE_Task7.json
simuleq.json
TASK18 Noexternalknowledge
NumGLUE_Task8.json
draw_structured.json
dolphin_structured.json
NumGLUE_Task5.json
deepmind_mathematics_muldiv.json
Numersense_structured.json NumGLUE_Task1.json
MCTaco_frequency_structured.json MCTaco_event_ordering_structured.json
NumGLUE_Task3.json
TASK19 Commonsense
MCTaco_stationarity_structured.json
MCTaco_event_duration_structured.json
MCTaco_event_typical_time_structured.json
amps_number_theory.json amps_counting_and_stats.json
amps_linear_algebra.json mathqa_general.json
amps_algebra.json amps_calculus.json
deepmind_mathematics_calculus.json
mathqa_probability.json
singleq.json
TASK20 Mathformulas
mathqa_gain.json
mathqa_other.json
deepmind_mathematics_algebra.json
deepmind_mathematics_basicmath.json
deepmind_mathematics_calculus.json
deepmind_mathematics_numbertheory.json
amps_geometry.json
TASK21 Scienceformulas NumGLUE_Task2.json
mathqa_physics.json
Computerscience APPS_structured.json mathqa_geometry.json
TASK22
knowledge conala_structured.json
TASK23 Real-worldknowledge MATH_crowdsourced.json mbpp_structured.json
Table15:RawdatasetsusedtocreatedifferenttasksinL¯ILAacrossdifferentknowledgecategories.
0.6%
8.4% Basic math
3.4% 23.2% Muldiv 19.5%
Number theory 30.8%
4.8%
Algebra No language
2.4% Geometry Simple language
12.0%
Statistics Complex language
11.9% Calculus
Linear algebra
33.4% Advanced math 49.7%
(a)Mathabilitycategories. (b)Languagecategories.
4.7%2.7%8.7% 9.1%0.6%
21.0%
7.4% No external knowledge
Fill in the blank 7.8%
Commonsense
Generative QA
Math formulas
MCQ
Science formulas
NLI 18.4% Complex knowledge
RC
Real-world knowledge
43.1%
76.4%
(c)Formatcategories. (d)Knowledgecategories.
Figure8:TaskdiversityinL¯ILAacrossmath,language,format,andknowledgecategories.
TemplateName Variation Example
Changetheorderofobjects Question:Allenbought20stampsatthepostofficein37centsand20cents
SVAMP-COO
denominations.Ifthetotalcostofthestampswas$7.06,howmany37cents
stampsdidAllenbuy?
Variation:Allenbought20stampsatthepostofficein20centsand37cents
denominations.Ifthetotalcostofthestampswas$7.06,howmany37cents
stampsdidAllenbuy?
Changetheorderofphrases Question:Onepipecanfillatankin5hoursandanotherpipecanfillthesame
SVAMP-COP
tankin4hours.Adrainpipecanemptythefullcontentofthetankin20hours.
Withallthethreepipesopen,howlongwillittaketofillthetank?
Variation:Adrainpipecanemptythefullcontentofatankin20hours.One
pipecanfillthetankin4hoursandanotherpipecanfillthesametankin5
hours.Withallthethreepipesopen,howlongwillittaketofillthetankwith
allthethreepipesopen?
Addirrelevant,unhelpfulinformation Question:theareaofanisoscelestrapezoidwithsidesoflength5andbasesof
SVAMP-IU
length7and13is?
Variation:monkeysandapesarebothprimates,whichmeansthey’rebothpart
ofthehumanfamilytree.theareaofanisoscelestrapezoidwithsidesoflength
5andbasesoflength7and13is?
Addunhelpful,butcontextuallyrelated Question:Tomis15yearsyoungerthanalice.Tenyearsago,Alicewas4
ROBUST-IR
information timesasoldasTomwasthen.Howoldiseachnow?
Variation:Tomis15yearsyoungerthanalice.Tenyearsago,Alicewas4
timesasoldasTomwasthen.Alicereallylikespinapplepizza.Howoldis
eachnow?
Turnactiveintopassivespeechtoin- Question: Hay’sLinenssellshandtowelsinsetsof17andbathtowelsin
ROBUST-AP
creaseproblemverbosity setsof6.Ifthestoresoldthesamenumberofeachthismorning,whatisthe
smallestnumberofeachtypeoftowelthatthestoremusthavesold?
Variation:HandtowelsaresoldbyHay’sLinensinsetsof17andbathtowels
aresoldinsetsof6. Ifthesamenumberofeachweresoldbythestorethis
morning,whatisthesmallestnumberofeachtypeoftowelthatthestoremust
havesold?
Addadjectivesandadverbstoincrease Question:ThereTealeavesexposedtooxygenforupto_hoursbecomeblack
ROBUST-ADJ
problemverbosity tea.
Variation:Blacktealeavescontinuouslyexposedtooxygenforupto_hours
becomeaveryrichblacktea.
Turnataskstatementintoaquestion Question:Productof-7and-1469.125.
ROBUST-Q
Variation:Whatistheproductof-7and-1469.125?
Turnaquestionintoataskstatement Question:Problem:Iftheproductof5andanumberisincreasedby4,the
ROBUST-RQ
resultis19.Whatisthenumber?
Variation:Increasingtheproductof5andanumberby4resultsis19.Find
thenumber.
Removeexplicitlymathematicalterms Problem:Findthearclengthofthefunctionf(x)=2√xontheintervalx=2
ROBUST-RM
thatareimplicitlydefined tox=8
Variation:Findthearclengthoff(x)=2√xon[2,8]
Table16:ExampleforeachtemplateprovidedtoMTurkworkerstoproduceL¯ILA-ROBUST
ID Category Questions Uniquequestions Questionlength Programs Uniqueprograms Programlength
TASK1 Basicmath 31,052 31,032 43.1 31,052 7,066 13.3
TASK2 Muldiv 16,021 15,936 26.9 16,021 15,279 8.2
TASK3 Numbertheory 44,760 44,183 41.3 269,232 261,865 33.2
TASK4 Algebra 15,882 15,615 19.3 16,364 15,986 12.7
TASK5 Geometry 3,190 3,149 36.1 3,190 3,035 28.7
TASK6 Countingandstatistics 6,423 6,384 39.7 6,423 6,335 31.5
TASK7 Calculus 4,493 4,202 21.2 4,493 4,170 40.6
TASK8 Linearalgebra 11,248 11,204 32.4 11,248 11,204 23.0
TASK9 Advancedmath 746 746 21.2 746 745 27.3
TASK10 Nolanguage 41,191 40,551 21.2 42,466 41,794 40.6
TASK11 Simplelanguage 66,505 66,172 26.9 290,184 258,839 8.2
TASK12 Complexlanguage 26,119 25,728 36.1 26,119 25,052 28.7
TASK13 Fillintheblank 11,634 11,615 11.0 11,634 997 3.0
TASK14 GenerativeQA 102,493 101,239 14.7 327,447 314,652 16.0
TASK15 MCQ 9,989 9,989 28.3 9,989 470 3.0
TASK16 NLI 6,326 6,325 50.8 6,326 6,243 25.8
TASK17 RC 3,642 3,552 182.5 3,642 3,592 10.4
TASK18 Noexternalknowledge 28,115 27,964 50.8 28,115 27,117 25.8
TASK19 Commonsense 24,677 24,658 30.9 24,677 823 3.0
TASK20 Mathformulas 57,841 56,947 19.1 59,116 57,019 25.5
TASK21 Scienceformulas 10,505 10,319 36.1 10,505 9,764 28.7
TASK22 Complexknowledge 12,200 12,086 14.5 235,879 230,486 24.2
TASK23 Real-worldknowledge 746 746 21.2 746 745 27.3
Table17:MainstatisticsofL¯ILAacrossthetotalof23tasks.
Figure9:ThewordclouddistributionofannotatedprogramsintheL¯ILAdataset.
ID Dataset GPT-3 Neo-A Neo-P Codex
1 addsub 0.910 0.116 0.797 0.950
2 amps_algebra 0.116 0.100 0.902 0.655
3 amps_calculus 0.192 0.168 0.922 0.860
4 amps_counting_and_stats 0.183 0.117 0.958 0.650
5 amps_geometry 0.283 0.263 0.074 0.000
6 amps_linear_algebra 0.127 0.235 0.815 0.692
7 amps_number_theory 0.273 0.026 0.875 1.000
8 APPS_structured 0.167 0.154 0.134 0.459
9 asdiv 0.737 0.166 0.092 0.022
10 conala_structured 0.356 0.329 0.329 0.391
11 deepmind_mathematics_algebra 0.202 0.258 0.847 0.910
12 deepmind_mathematics_basicmath 0.270 0.125 0.614 1.000
13 deepmind_mathematics_calculus 0.208 0.026 0.152 0.884
14 deepmind_mathematics_muldiv 0.160 0.034 0.909 1.000
15 deepmind_mathematics_numbertheory 0.296 0.462 0.538 0.710
16 dolphin_t2_final 0.170 0.027 0.006 0.812
17 draw_structured 0.090 0.034 0.005 0.210
18 GSM8k_structured 0.110 0.060 0.139 0.350
19 MATH_crowdsourced 0.150 0.013 0.074 0.472
20 mathqa_gain 0.134 0.054 0.339 0.270
21 mathqa_general 0.110 0.073 0.193 0.120
22 mathqa_geometry 0.120 0.002 0.000 0.250
23 mathqa_other 0.180 0.043 0.011 0.280
24 mathqa_physics 0.120 0.087 0.429 0.210
25 mathqa_probability 0.210 0.003 0.000 0.200
26 mbpp_structured 0.128 0.175 0.164 0.408
27 MCTaco_event_duration_structured 0.800 0.773 0.773 0.710
28 MCTaco_event_ordering_structured 0.860 0.831 0.831 0.890
29 MCTaco_event_typical_time_structured 0.870 0.881 0.881 0.870
30 MCTaco_frequency_structured 0.890 0.862 0.862 0.790
31 MCTaco_stationarity_structured 0.710 0.758 0.758 0.670
32 multiarith 0.360 0.143 0.921 0.990
33 Numersense_structured 0.620 0.495 0.495 0.660
34 NumGLUE_Type_1 0.535 0.108 0.083 0.740
35 NumGLUE_Type_2 0.512 0.285 0.646 0.735
36 NumGLUE_Type_3 0.835 0.004 0.001 0.815
37 NumGLUE_Type_4 0.710 0.076 0.208 0.790
38 NumGLUE_Type_5 0.460 0.200 0.305 0.615
39 NumGLUE_Type_7 0.500 0.516 0.854 0.710
40 NumGLUE_Type_8 0.420 0.082 0.257 0.610
41 simuleq 0.120 0.074 0.010 0.170
42 singleop 0.940 0.347 0.611 1.000
43 singleq 0.830 0.143 0.474 0.670
44 svamp_structured 0.620 0.085 0.060 0.790
AverageF1score 0.400 0.223 0.440 0.613
Table18: Evaluationresultsofbaselinesacrossdifferentsingledatasets. Onmostdatasets,Codexperformsbest.
Modelnames: GPT-3: thefew-shot175BGPT-3model; GPT-Neo-A:thefine-tuned2.7BGPT-3modelwhere
thepredictionoutputisananswer;GPT-Neo-P:thefine-tuned2.7BGPT-3modelwherethepredictionoutputisa
program;Codex:thefew-shotCodexmodelwherethepredictionoutputisaprogram.
ID Dataset References
1 addsub (Hosseinietal.,2014)
2 amps (Hendrycksetal.,2021b)
3 APPS (Hendrycksetal.,2021a)
4 asdiv (Miaoetal.,2020b)
5 conala (Yinetal.,2018)
6 mathematics (Saxtonetal.,2019)
7 dolphin (Huangetal.,2016)
8 draw (UpadhyayandChang,2015)
9 GSM8k (Cobbeetal.,2021)
10 MATH (Hendrycksetal.,2021b)
11 mathqa (Aminietal.,2019)
12 mbpp (Austinetal.,2021)
13 MCTaco (Zhouetal.,2019)
14 multiarith (RoyandRoth,2015)
15 Numersense (Linetal.,2020)
16 NumGLUE (Mishraetal.,2022c;Duaetal.,2019b;Ravichander
etal.,2019;Kushmanetal.,2014;Tafjordetal.,2019;
Roy and Roth, 2018, 2017; Koncel-Kedziorski et al.,
2016,2015)
17 simuleq (Kushmanetal.,2014)
18 singleop (Royetal.,2015)
19 singleq (Koncel-Kedziorskietal.,2015)
20 svamp (Pateletal.,2021)
Table19:ListofsourcedatasetsandcorrespondingreferencesusedinconstructingL¯ILA.
L¯ila:
A Unified Benchmark for Mathematical
Reasoning
Swaroop Mishra∗† Matthew Finlayson∗‡
Arizona State University The Allen Institute for AI
Pan Lu† Leonard Tang Sean Welleck
UCLA Harvard University The Allen Institute for AI
Chitta Baral Tanmay Rajpurohit
Arizona State University Georgia Institute of Technology
Oyvind Tafjord Ashish Sabharwal
The Allen Institute for AI The Allen Institute for AI
Peter Clark Ashwin Kalyan‡
The Allen Institute for AI The Allen Institute for AI
Abstract
Mathematical reasoning skills are essential for general-purpose intelli-
gent systems to perform tasks from grocery shopping to climate modeling.
Towards evaluating and improving AI systems in this domain, we propose
L¯ila,aunifiedmathematicalreasoningbenchmarkconsistingof23diverse
tasks along four dimensions: (i) mathematical abilities e.g., arithmetic,
calculus (ii) language format e.g., question-answering, fill-in-the-blanks
(iii) language diversity e.g., no language, simple language (iv) external
knowledge e.g., commonsense, physics. We construct our benchmark
by extending 20 datasets benchmark by collecting task instructions and
solutions in the form of Python programs, thereby obtaining explainable
solutions in addition to the correct answer. We additionally introduce
two evaluation datasets to measure out-of-distribution performance and
robustness to language perturbation. Finally, we introduce Bha¯skara,
a general-purpose mathematical reasoning model trained on L¯ila. Im-
portantly, we find that multi-tasking leads to significant improvements
(average relative improvement of 21.83% F1 score vs. single-task models),
whilethebestperformingmodelonlyobtains60.40%,indicatingtheroom
for improvement in general mathematical reasoning and understanding.1
∗Equalfirstauthors.
†WorkdonewhileattheAllenInstituteforAI.
‡Correspondingauthors: matthewf@allenai.org,ashwinkv@allenai.org.
1Our dataset: https://github.com/allenai/Lila. Our model: https://huggingface.co/
allenai/bhaskara.
1
3202
raM
8
]LC.sc[
2v71571.0122:viXra
(cid:48)(cid:68)(cid:87)(cid:75)(cid:3)(cid:68)(cid:69)(cid:76)(cid:79)(cid:76)(cid:87)(cid:92)(cid:29)(cid:3)(cid:69)(cid:68)(cid:86)(cid:76)(cid:70)(cid:3)(cid:80)(cid:68)(cid:87)(cid:75)
(cid:47)(cid:68)(cid:81)(cid:74)(cid:88)(cid:68)(cid:74)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:91)(cid:76)(cid:87)(cid:92)(cid:29)(cid:3)(cid:86)(cid:76)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)(cid:79)(cid:68)(cid:81)(cid:74)(cid:88)(cid:68)(cid:74)(cid:72)
(cid:41)(cid:82)(cid:85)(cid:80)(cid:68)(cid:87)(cid:29)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:84)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:68)(cid:81)(cid:86)(cid:90)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)
(cid:46)(cid:81)(cid:82)(cid:90)(cid:79)(cid:72)(cid:71)(cid:74)(cid:72)(cid:29)(cid:3)(cid:81)(cid:82)(cid:3)(cid:72)(cid:91)(cid:87)(cid:72)(cid:85)(cid:81)(cid:68)(cid:79)(cid:3)(cid:78)(cid:81)(cid:82)(cid:90)(cid:79)(cid:72)(cid:71)(cid:74)(cid:72)
(cid:44)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29)(cid:3)(cid:60)(cid:82)(cid:88)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:68)(cid:3)(cid:84)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:76)(cid:81)(cid:89)(cid:82)(cid:79)(cid:89)(cid:72)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)
(cid:70)(cid:68)(cid:79)(cid:70)(cid:88)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:73)(cid:3)(cid:81)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:86)(cid:17)(cid:3)(cid:60)(cid:82)(cid:88)(cid:3)(cid:81)(cid:72)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:83)(cid:72)(cid:85)(cid:73)(cid:82)(cid:85)(cid:80)(cid:3)(cid:72)(cid:76)(cid:87)(cid:75)(cid:72)(cid:85)(cid:3)(cid:68)(cid:81)(cid:3)
(cid:68)(cid:71)(cid:71)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:85)(cid:3)(cid:86)(cid:88)(cid:69)(cid:87)(cid:85)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:83)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:81)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:86)(cid:17)(cid:3)(cid:42)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:72)(cid:3)
(cid:92)(cid:82)(cid:88)(cid:85)(cid:3)(cid:68)(cid:81)(cid:86)(cid:90)(cid:72)(cid:85)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:84)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:17)
(cid:52)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29)(cid:3)(cid:54)(cid:68)(cid:85)(cid:68)(cid:3)(cid:83)(cid:76)(cid:70)(cid:78)(cid:72)(cid:71)(cid:3)(cid:23)(cid:24)(cid:3)(cid:83)(cid:72)(cid:68)(cid:85)(cid:86)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:54)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3)(cid:83)(cid:76)(cid:70)(cid:78)(cid:72)(cid:71)(cid:3)(cid:20)(cid:20)(cid:3)(cid:83)(cid:72)(cid:68)(cid:85)(cid:86)(cid:3)
(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:83)(cid:72)(cid:68)(cid:85)(cid:3)(cid:87)(cid:85)(cid:72)(cid:72)(cid:17)(cid:3)(cid:43)(cid:82)(cid:90)(cid:3)(cid:80)(cid:68)(cid:81)(cid:92)(cid:3)(cid:83)(cid:72)(cid:68)(cid:85)(cid:86)(cid:3)(cid:90)(cid:72)(cid:85)(cid:72)(cid:3)(cid:83)(cid:76)(cid:70)(cid:78)(cid:72)(cid:71)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:82)(cid:87)(cid:68)(cid:79)(cid:34)(cid:3)
(cid:51)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:20)(cid:29)(cid:3)
(cid:63)(cid:64)(cid:65)(cid:3)(cid:78)(cid:74)(cid:71)(cid:80)(cid:79)(cid:68)(cid:74)(cid:73)(cid:159)(cid:83)(cid:134)(cid:3)(cid:84)(cid:160)(cid:136)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:60)(cid:73)(cid:78)(cid:82)(cid:64)(cid:77)(cid:3)(cid:211)(cid:3)(cid:83)(cid:3)(cid:207)(cid:3)(cid:84)(cid:3)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:77)(cid:64)(cid:79)(cid:80)(cid:77)(cid:73)(cid:3)(cid:60)(cid:73)(cid:78)(cid:82)(cid:64)(cid:77)
(cid:75)(cid:77)(cid:68)(cid:73)(cid:79)(cid:159)(cid:78)(cid:74)(cid:71)(cid:80)(cid:79)(cid:68)(cid:74)(cid:73)(cid:159)(cid:191)(cid:192)(cid:134)(cid:3)(cid:188)(cid:188)(cid:160)(cid:160)(cid:3)(cid:186)(cid:3)(cid:79)(cid:74)(cid:79)(cid:60)(cid:71)(cid:3)(cid:75)(cid:64)(cid:60)(cid:77)(cid:78)(cid:3)(cid:68)(cid:78)(cid:3)(cid:79)(cid:67)(cid:64)(cid:3)(cid:78)(cid:80)(cid:72)(cid:3)(cid:74)(cid:65)(cid:3)
(cid:75)(cid:64)(cid:60)(cid:77)(cid:78)(cid:3)(cid:82)(cid:68)(cid:79)(cid:67)(cid:3)(cid:22)(cid:60)(cid:77)(cid:60)(cid:3)(cid:60)(cid:73)(cid:63)(cid:3)(cid:22)(cid:60)(cid:71)(cid:71)(cid:84)
(cid:51)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:21)(cid:29)(cid:3)
(cid:83)(cid:3)(cid:211)(cid:3)(cid:191)(cid:192)
(cid:84)(cid:3)(cid:211)(cid:3)(cid:188)(cid:188)
(cid:60)(cid:73)(cid:78)(cid:82)(cid:64)(cid:77)(cid:3)(cid:211)(cid:3)(cid:83)(cid:3)(cid:207)(cid:3)(cid:84)(cid:3)(cid:186)(cid:3)(cid:79)(cid:74)(cid:79)(cid:60)(cid:71)(cid:3)(cid:75)(cid:64)(cid:60)(cid:77)(cid:78)(cid:3)(cid:68)(cid:78)(cid:3)(cid:79)(cid:67)(cid:64)(cid:3)(cid:78)(cid:80)(cid:72)(cid:3)(cid:74)(cid:65)(cid:3)(cid:75)(cid:64)(cid:60)(cid:77)(cid:78)(cid:3)(cid:82)(cid:68)(cid:79)(cid:67)(cid:3)
(cid:22)(cid:60)(cid:77)(cid:60)(cid:3)(cid:60)(cid:73)(cid:63)(cid:3)(cid:22)(cid:60)(cid:71)(cid:71)(cid:84)
(cid:75)(cid:77)(cid:68)(cid:73)(cid:79)(cid:159)(cid:60)(cid:73)(cid:78)(cid:82)(cid:64)(cid:77)(cid:160)
(cid:36)(cid:81)(cid:86)(cid:90)(cid:72)(cid:85)(cid:29)(cid:3)(cid:24)(cid:25)
Figure 1: A data example with two Python programs in L¯ila. One program
annotationusesfunctionconstructwhereastheotheroneisaplainscriptwithout
function. The instruction for each task and categories across four dimensions
are annotated for developing L¯ila.
1 Introduction
Mathematical reasoning is required in all aspects of life, from buying ingredients
for a recipe to controlling the world economy. Given the fundamental nature
of mathematical reasoning, a number of works propose datasets to evaluate
specificmathematicalreasoningabilitiesofAIagents,e.g.,Kushmanetal.(2014)
(algebra word problems), Mishra et al. (2022c) (arithmetic reasoning), Saxton
et al. (2019) (templated math reasoning spanning algebra, calculus, probability,
etc.) Since evaluating high-capacity models on narrowly scoped mathematical
reasoningdatasetsrisksoverestimatingthereasoningabilitiesoftheseAIsystems,
creating the need for a unified benchmark for systematic evaluation over diverse
topics and problem styles.
To this end, we introduce L¯ila2, a unified mathematical reasoning bench-
mark that consists of 23 mathematical reasoning tasks. L¯ila is constructed by
extending 20 existing datasets spanning a wide range of topics in mathematics,
varying degrees of linguistic complexity, and diverse question formats and back-
ground knowledge requirements. Importantly, L¯ila extends all of these datasets
to include a solution program as opposed to only an answer, and instruction
2NamedafterL¯ilavati,a12thcenturymathematicaltreatiseonarithmeticthatcoverstopics
likearithmeticandgeometricprogressions,indeterminateequationsandcombinations. Itis
alsowidelyknownfortheextensivenumberofmathwordproblems. Theauthor,Bha¯skara is
knownforfundamentalandoriginalcontributionstocalculus,physics,numbertheory,algebra,
andastronomy(Colebrooke,1817;Sarkar,1918;Kolachanaetal.,2019)
2
annotations to enable instruction-based learning (Sanh et al., 2021; Wei et al.,
2021; Mishra et al., 2022b).
In order to accurately assess the mathematical reasoning ability of models,
evaluating the chain of reasoning that leads to the correct solution is equally
important (if not more important) to evaluating the final answer or expression.
We therefore collect Python programs that serve as reasoning chains for each
questioninthebenchmark. Weachievethisbyautomaticallyconvertingdomain-
specific language (DSL) annotations into Python programs and by manually
collecting expert annotations when no DSL annotations are available. By
incorporating program annotations, L¯ila unifies various mathematical reasoning
datasets under a single problem formulation i.e., given an input problem in
natural language, generate a Python program that upon execution returns the
desired answer. This formulation allows neural approaches to focus on the high-
levelaspectsofmathematicalproblemsolving(e.g.,identifyingpotentialsolution
strategies,decomposingtheproblemintosimplersub-problems),whileleveraging
external solvers (e.g., Python builtins, Sympy) to perform precise operations
like adding huge numbers or simplifying expressions. Figure 1 illustrates a
sample from our L¯ila benchmark that illustrates the question, answer, program,
instructions, and category tags.
In addition to evaluating high-level problem solving, we also facilitate two
other key ways to make a fair assessment of models on mathematical reason-
ing tasks. In line with Bras et al. (2020), Ribeiro et al. (2020) and Welleck
et al. (2022), we evaluate generalization e.g., alternate formulations of a problem
(“2+2=?” vs. “What is two plus two?”) using an out-of-distribution evaluation
set (L¯ila-OOD) containing datasets requiring the same underlying mathemati-
cal reasoning skills, but were collected independently of the training datasets.
Further, we collect a robustness split L¯ila-Robust, that introduces linguistic
perturbations (e.g., active vs. passive voice) via crowd-sourcing. The evalua-
tion scheme is a combination of the performance on all three sets: L¯ila-Test,
L¯ila-OOD and L¯ila-Robust.
Contributions
1. We present L¯ila, a holistic benchmark for mathematical reasoning. L¯ila
extends 20 existing datasets with solutions in the form of Python programs
and instruction annotations, and categorizes questions into 23 tasks based on
their language complexity, question format and need for external knowledge.
Our benchmark measures performance on out-of-distribution examples and
robustness to language perturbations in addition to standard test-set.
2. We introduce Bha¯skara, a multi-task model fine-tuned on our dataset. Our
best-performing model achieves comparable performance to a 66 larger
×
model pre-trained on both code and language.
3. Weprovideananalysisofourmodels’performanceandfindthat(1)multitask-
ing improves considerably over task-specific learning both in in-distribution
andout-of-distributionevaluation(2)programsynthesissubstantiallyoutper-
formsanswerprediction,(3)few-shotpromptingwithcodexhasthestrongest
3
performance. We also identify areas for improvement for future work, e.g.,
data gaps in L¯ila categories.
2 Related Work
Mathematical Reasoning Datasets. Our work builds on an existing body
of mathematical reasoning literature. Early work in this areas focuses on small-
scale datasets testing addition-subtraction (Hosseini et al., 2014), templated
questionswithequationsasparameters(Kushmanetal.,2014)andotherformsof
arithmetic reasoning (Koncel-Kedziorski et al., 2015; Roy and Roth, 2016; Upad-
hyay et al., 2016; Roy and Roth, 2017, 2018; Ling et al., 2017). Later datasets
increase in complexity and scale, incorporating reading comprehension Dua et al.
(2019b),algebra(Saxtonetal.,2019),andmulti-modalcontexts(Luetal.,2021a,
2022). Still other numerical reasoning datasets focus on diversity (Miao et al.,
2020a) with multiple categories of numerical reasoning tasks (e.g., Amini et al.,
2019). Most recently, new datasets have focused on increasing difficulty, e.g.,
olympiad problems (Hendrycks et al., 2021b) and adversarial problems (Patel
et al., 2021), as well as increasing the knowledge requirements to solve tasks,
with a growing focus on commonsense reasoning (Zhou et al., 2019; Zhang et al.;
Lu et al., 2021b; Mishra et al., 2022c).
A separate line of work in mathematical reasoning includes datasets testing
mathematical theorem proving (e.g., Li et al., 2021; Wu et al., 2021; Welleck
et al., 2021; Zheng et al., 2021; Han et al., 2021). We do not, however, consider
theorem proving in our work, choosing instead to focus on numerical reasoning.
Task Hierarchy and Multi-tasking in Numerical Reasoning. We take
inspiration from the success of multi-task learning in NLP (Weston et al., 2015),
including benchmarks (e.g., Wang et al., 2018, 2019; Dua et al., 2019a) and
multitasking models (e.g., McCann et al., 2018; Khashabi et al., 2020; Lourie
et al., 2021; Aghajanyan et al., 2021). NumGLUE (Mishra et al., 2022c) has
been proposed as a multi-tasking numerical reasoning benchmark that contains
8 different tasks. L¯ila expands NumGLUE to provide wider coverage of mathe-
matical abilities, along with evaluation that captures out-of-domain, robustness,
and instruction-following performance. Our introduction of mathematical rea-
soning categories and the evaluation setup is inspired by task hierarchies in
other domains such as vision (Zamir et al., 2018) and NLP (Rogers et al., 2021)
whichappearinlargescalebenchmarks(e.g.,Srivastavaetal.,2022;Wangetal.,
2022).
3 L¯ila
L¯ila is composed of 23 tasks across 4 dimensions, curated from 44 sub-datasets
across 20 dataset sources. Here we discuss the construction and composition of
the benchmark and provide descriptive statistics of the datasets.
4
Category Tasks
Math ability Basic math, multiplication/division, number theory, algebra, ge-
ometry, counting and statistics, calculus, linear algebra, advanced
math
Language No language, simple language, complex language
Knowledge No background knowledge, commonsense, math, science, computer
science, real world knowledge
Format Fill-in-the-blank, generative question answering, multiple-choice,
natural language inference, reading comprehension
Table 1: Categories and their associated tasks.
3.1 Dataset Construction
Data Sources. L¯ila incorporates 20 existing datasets from the mathematical
reasoning literature (Table 19 gives a detailed list), where inputs are natural
language or templated text and outputs are numerical or expressions, e.g., we
exclude theorem proving (Welleck et al., 2021; Han et al., 2021), where the
output is not a number or expression. We leave the incorporation of formats
like theorem proving to future work.
Unified format. We normalize all datasets to a unified format with the
following fields:
1. The source dataset. Category tags for each of the four dimensions (math
ability, language complexity, format, and external knowledge; see §3.2).
2. The question, in English.
3. The answer to the question, as a string containing a number, expression, list,
or other data format. A set of Python strings that print the answer.
4. A task-level instruction in natural language.
We also retain meta-data from the original dataset.
Automatic program annotation. Most of the annotations in the source
datasets do not contain output in the form of a Python program. We auto-
matically annotate most datasets by generating Python programs using the
annotations (answer, explanation, etc.) provided in the source datasets. Where
possible, we generate multiple Python programs for a single question. This is to
account for variation in the program space such as the choice of data structure,
language construct, variable name, and programming style (e.g., declarative vs
procedural). For example, Figure 1 gives multiple Python programs solving the
same question; in this case one program directly calculates the answer, whereas
the other defines a function to solve the problem more generally.
Somedatasetscontainprogramannotationsthatcanbecapturedbyadomain-
specifclanguage(DSL)inwhichcasewewriterulestoconvertthemintoPython
programs, e.g., volume(sphere,3) to the Python expression 4/3*math.pi*3**3.
In some cases where a DSL annotation is not provided, we use pattern matching
5
to convert highly templated datasets like the AMPS dataset (Hendrycks et al.,
2021b) to our unified format. In other cases, instead of converting the existing
dataset, we modify the data generation code to reproduce the dataset with
program annotations. For the DeepMind mathematics dataset (Saxton et al.,
2019),thisallowsustocreatediverse,compositionalmathproblemswithprogram
annotations using a sophisticated grammar.
Expert program annotation. Formanydatasets,itisnotpossibletoobtain
Python program annotationsvia automated methods described above; either the
original dataset contains only the final answer or contains solutions expressed in
free-formnaturallanguage. Forsuchdatasets,weobtainannotationsfromexperts
who are proficient in basic programming and high-school level mathematics. See
Appendix B.1 for details.
Instructionannotation. Giventheeffectivenessofinstructionlearning(Mishra
etal.,2022b;Weietal.,2021;Mishraetal.,2022a;Sanhetal.,2021)foreffective
generalization, we collect instruction annotation for each task. Each instruction
contains a definition that clearly defines the task and provides guidelines, a
prompt that provides a short and straight forward instruction, and examples
that facilitate learning by demonstration (Brown et al., 2020). Figure 1 shows
an example instruction for the basic math task (§3.2).
3.2 Categories and Tasks
We create 4 views3or categories of L¯ila along the dimensions of mathematical
area,languagecomplexity,externalknowledge,andquestionformat. Altogether,
these views classify the data into 23 tasks (Table 1). By creating multiple views
of the benchmark, we are able to systematically characterize the strengths and
weaknesses of existing models at a granular level.
The first category, math ability, partitions the datasets into common peda-
gogical subjects: arithmetic, algebra, geometry, calculus, etc.
Our second category, language complexity, separates math problems by the
complexity of the language used to represent them. This ranges from formal
representations only (e.g., 1+1=?) to natural language (e.g., “Mariella has 3
pears...”).
We next partition datasets based on the type of background knowledge,
required to solve the problem. For instance, commonsense questions like “How
many legs to 3 people have?” or science questions like “Will water boil at 200
degrees Celsius?” require different sets of knowledge to answer.
Lastly, we categorize based on question format, putting e.g., multiple choice
questionsunderonetaskandnaturallanguageinferenceunderanother. Examples
of each task and the datasets included are in Appendix B.
3Notethatitisnot apartitionofthebenchmarkaseachdimensionsdividestheconstituent
examplesindifferentways
6
3.3 L¯ila-OOD
In order to measure if the model has truly learned the underlying mathematical
reasoning skill, we evaluate both in-distribution (IID, i.e., standard train-test
splits)andout-of-distribution(OOD)performanceforeachtask,i.e.,weevaluate
on examples requiring the same underlying mathematical reasoning skill but
from a different dataset. To construct L¯ila-OOD, we follow Bras et al. (2020)
andHendrycksetal.(2020)byrandomlyassigningthedatasetsforeachtaskinto
IIDandanOODsets, usingtheIIDsetfortrainingandstandardevaluationand
the OOD set to evaluate generalization. We do not include tasks in L¯ila-OOD
for tasks containing only one dataset.
3.4 L¯ila-Robust
In light of recent work demonstrating the brittleness of language models at
solving math problems (Patel et al., 2021), we create a high-quality evaluation
dataset,L¯ila-Robust,toevaluateperformanceonmathematicalreasoningtasks
when linguistic perturbations are introduced. Specifically, we define and apply
a set of carefully chosen augmentation templates, summarized in Table 16, on
each task, yielding a set of challenging problems that are consistent answer-wise
but stylistically different question-wise. Overall, we define a total of 9 templates
for such question perturbations: 3 from Patel et al. (2021) and 6 of our own.
From each constituent dataset, we sample 20 questions and obtain perturbed
question annotations via Amazon Mechanical Turk (AMT). Refer to Appendix
B.1 for additional details on the construction of L¯ila-Robust.
3.5 Statistics
Table 2 shows key statistics of our proposed benchmark, L¯ila. L¯ila contains
134K examples with significant diversity across question, answer, program
≈
and instruction length (see detailed statistics in Appendix C). Figure 2 shows
the diversity of questions in L¯ila. Note that we downsample (via random
selection) some datasets like AMPS (Hendrycks et al., 2021b) which contains
numeroustemplatedquestionsthatcangetover-representatedinthedistribution
of examples across categories in L¯ila.
4 Experiments
In this section, we introduce our modeling contributions for the L¯ila benchmark
and discuss the overall experimental setup.
Data partition and evaluation. For the IID setup, we randomly partition
the data in each task into training (70%), development (10%) and test (20%)
sets. Additionally, we also evaluate on L¯ila-OOD and L¯ila-Robust settings;
thus, the final evaluation scheme is a combination of the performance on all
three evaluation setups
7
Statistic Number
# Total tasks 23
# Total datasets 44
# Total instructions 44
# Total questions 133,815
# Total programs 358,769
Unique questions 132,239
Unique programs 325,597
Unique answers 271,264
Average length of instructions 31.18
Average length of questions 47.72
Average length of programs 47.85
Table 2: Key statistics of L¯ila.
Fine-tuning. We fine-tune a series of GPT-Neo-2.7B causal language mod-
els(Blacketal.,2021))onL¯ila. WechooseGPT-Neobecauseitwaspre-trained
on both natural language and code (Gao et al., 2020), as opposed to solely on
natural language. To assess the capabilities of GPT-Neo on various aspects of
the dataset, we fine-tune single-task models on each of the 23 tasks in L¯ila. We
also evaluate the benefit of transfer learning by fine-tuning a single multi-task
GPT-Neo baseline on all the tasks simultaneously. We call our multitask model
Bha¯skara.
Prompting. Wealsousefew-shotpromptingtoevaluateGPT-3andCodex4(Brown
et al., 2020; Chen et al., 2021). For the IID setting, we prompt the model with a
random input-output examples from the same dataset as the input. In the OOD
setting, we take examples from other datasets (Table 12-15) within the same
task. We repeat this evaluation with increasing numbers of examples (up to the
token size of models) to study the effect on performance5.
Evaluation. We evaluate our models under two regimes—directly outputting
theansweri.e.,programinductionandoutputtingaPythonprogramthatisthen
executed to obtain the final answer i.e., program synthesis. In the case of our
fine-tunedmodels,wetrainthemtooutputboththefinalanswerandthePython
program conditioned on the input question. To evaluate our models under direct
question answering, we use F1-score6 to compare the model output and the
gold answer. To evaluate program synthesis, we execute the model’s output
within a Python interpreter and compare the program output with the output
of the gold program, again using F1. We evaluate based on the program output,
4text-davinci-002, code-davinci-002
5Henceforthwerefertothemaxexamplemodelunlessotherwisespecified.
6Thisisasoftversionofexactmatchaccuracyassigningpartialcreditwhencommonwords
arepresentintheoutputandgoldanswer.
8
followingpolyhedron's
didthe
more
mand
yays percent
hours
times
ye ma irs nutes
did
is$the
WS hi TC mh ioM cpn eu vlWl hitefihS p yrE eo tls yl nt vimeate How l mon ug ch
d mo doe
d
os
n i ede sy
ao ff t te hr e did
the
p he arpcp te we htn ian et se md
e
Co ismpu Wt he
at
\$ realall
eigenvecF torsind
unionquotientjacobiad
nift feh ree
nceleastsumgreatn eu
sto mf bst eee rcon
d nis dtas nm
ceallesd tddo idoes
Figure 2: Question n-gram distribution in L¯ila.
rather than the program itself, to account for diversity in solving techniques and
programming styles.
5 Results and Analysis
A summary of all key results on our L¯ila benchmark are shown in Table 3. In
thissection, we willdiscuss theperformanceoffine-tuned 2.7BGPT-Neo models
(§5.1), performance of models along the 4 categories of tasks (§5.2) and finally,
the few-shot performance of much larger ( 175B parameters) models (§5.3).
∼
5.1 Results: Fine-tuned Models
Multitasking improves IID performance, robustness, and OOD gener-
alization. The multi-tasking model (Bha¯skara) substantially improves upon
the single task models (Neo). Bha¯skara achieves better average in-domain
performance than the 23 individual per-task models (0.480 vs. 0.394 average
score), suggesting that it leverages cross-task structure not present in a single
task’s training set.
We also find that our multi-task model is robust to the linguistic perturba-
tions we test in L¯ila-Robust. We did not find any degradation in performance
when testing on perturbed IID test examples. Additionally, multi-task training
substantially improves out-of-domain generalization (0.448 vs. 0.238). The gap
between IID and OOD performance is much smaller for Bha¯skara than for the
single task models (Table 3), and in one case (format) Bha¯skara’s OOD per-
formance on held-out tasks is better than its IID performance (Table 4). L¯ila’s
multi-task structure opens interesting future directions related to developing
improved multitasking techniques, and further understanding its benefits.
9
Supervision/Size Few-shot,175B Few-shot,175B Fine-tuned,2.7B Fine-tuned,2.7B Fine-tuned,2.7B Fine-tuned,2.7B
→
GPT-3 Codex Neo-A Neo-P Bha¯skara-A Bha¯skara-P
Task Category
↓ IID OOD IID OOD IID OOD IID OOD IID OOD IID OOD
1 Basicmath 0.766 0.818 0.791 0.762 0.533 0.523 0.611 0.555 0.693 0.657 0.790 0.787
2 Muldiv 0.479 0.665 0.691 0.790 0.136 0.089 0.388 0.194 0.155 0.083 0.448 0.395
3 Numbertheory 0.240 0.154 0.472 0.344 0.108 0.095 0.328 0.107 0.129 0.190 0.358 0.293
4 Algebra 0.338 0.130 0.603 0.511 0.164 0.031 0.348 0.051 0.203 0.054 0.473 0.007
5 Geometry 0.283 0.120 0.000 0.250 0.288 0.025 0.077 0.021 0.297 0.105 0.079 0.250
6 Statistics 0.183 0.210 0.650 0.200 0.107 0.008 0.839 0.034 0.115 0.179 0.947 0.164
7 Calculus 0.231 0.208 0.930 0.884 0.138 0.119 0.486 0.334 0.102 0.167 0.495 0.805
8 Linearalgebra 0.127 - 0.692 - 0.229 - 0.809 - 0.240 - 0.808 -
9 Advancedmath 0.150 - 0.472 - 0.012 - 0.100 - 0.019 - 0.160 -
10 Nolanguage 0.213 0.162 0.853 0.770 0.143 0.083 0.698 0.330 0.140 0.138 0.703 0.850
11 Simplelanguage 0.486 0.561 0.568 0.610 0.269 0.243 0.363 0.292 0.332 0.269 0.433 0.384
12 Complexlanguage 0.356 0.413 0.456 0.583 0.147 0.113 0.216 0.106 0.215 0.259 0.288 0.557
13 Fillintheblank 0.710 0.620 0.790 0.660 0.086 0.193 0.304 0.193 0.059 0.519 0.262 0.519
14 GenerativeQA 0.305 0.385 0.566 0.632 0.142 0.135 0.376 0.199 0.178 0.160 0.476 0.235
15 MCQ 0.801 0.870 0.771 0.870 0.636 0.818 0.652 0.818 0.752 0.888 0.817 0.888
16 NLI 0.500 - 0.710 - 0.221 - 0.212 - 0.566 - 0.893 -
17 RC 0.460 - 0.615 - 0.135 - 0.295 - 0.132 - 0.264 -
18 Noexternalk. 0.437 0.485 0.638 0.660 0.138 0.110 0.387 0.159 0.167 0.199 0.400 0.465
19 Commonsense 0.788 0.698 0.752 0.815 0.613 0.364 0.624 0.356 0.735 0.470 0.778 0.526
20 Mathformulas 0.259 0.162 0.661 0.544 0.137 0.074 0.454 0.382 0.170 0.077 0.599 0.404
21 Scienceformulas 0.305 0.120 0.315 0.250 0.158 0.025 0.239 0.021 0.157 0.105 0.181 0.250
22 Computersciencek. 0.262 0.128 0.425 0.408 0.151 0.137 0.147 0.134 0.232 0.304 0.220 0.278
23 Real-worldk. 0.150 - 0.472 - 0.012 - 0.100 - 0.019 - 0.160 -
Averagescore 0.384 0.384 0.604 0.586 0.204 0.177 0.394 0.238 0.252 0.268 0.480 0.448
Table 3: Evaluations of different baselines across 23 tasks in L¯ila. On most
tasks, Codex outperforms all baselines while Bha¯skara-P outperforms all
fine-tuned baselines. A model usually performs worse on the OOD data set. The
bold score refers to the best score among models with the same supervision
method; the underlined score refers to the best score among all models. GPT-3
and Codex performance is computed on 100 uniformly distributed examples
owing to their cost and usage limit. Fine-tuned model performance is calculated
on the full test set.
Lastly, we do not find any benefit to fine-tuning with instructions. Our best
instruction tuned model achieves 0.133 F1, whereas the worst non-instruction-
tuned multitask model achieves 0.290.
Programsynthesissubstantiallyoutperformsanswerprediction. Syn-
thesizing the program and evaluating it to get an answer substantially outper-
forms directly predicting the answer. For instance, multi-task program synthesis
(Bha¯skara-P) has an average score of 0.480 while multi-task answer prediction
(Bha¯skara-A) scores 0.252. This means models are often able to generate
a program that evaluates to the correct answer, even when the model cannot
directly compute the answer.
Program synthesis improves over answer prediction in all math categories
except Geometry, with the largest improvements in Statistics and Linear
Algebra; see Table 5 for examples. We even see benefits of program synthesis
in NLI, a classification-based task. L¯ila’s unified problem format decouples
synthesis from computation, while opening directions for further study on either
aspect.
10
Neo-A Neo-P
Dimension
IID OOD IID OOD
Math ability 0.191 0.129 0.445 0.188
Language 0.189 0.147 0.429 0.246
Format 0.246 0.382 0.372 0.404
Knowledge 0.206 0.143 0.331 0.213
Average 0.208 0.200 0.394 0.263
Table 4: Multi-task models are able to generalize to unseen tasks in some
categories. Program output (Neo-P) always outperforms number output (Neo-
A).
Data Answer (% F1) Program (% F1)
Neo Multi ∆ Neo Multi ∆
100% 28.4 32.3 +4.0 80.0 82.4 +2.5
40% 20.0 21.1 +1.2 75.2 70.3 -4.9
20% 15.8 18.4 +2.6 66.3 67.1 +0.8
Table 5: Here we show the results of fine-tuning both GPT-Neo-2.7B (Neo)
and Bha¯skara (Multi) on 100%, 40%, and 20% of the held-out data from
L¯ila-OOD. The Multi almost always outperforms Neo (the ∆ column shows
the margin).
Models leverage symbolic execution and libraries. The gap between
program synthesis and answer prediction suggests that the neural language
model offloads computations to the symbolic Python runtime that are otherwise
difficult to compute directly. We identify two common cases. First, the model
leverages standard Python as a calculator. For instance, this pattern is common
in the basic_math and mul_div categories, which involve evaluating arithmetic
expressions; Table 4 shows examples. Second, the model is able to call external
libraries that perform sophisticated computations. For instance, in statistics
the model uses scipy.stats.entropy or np.linalg.det in linear algebra while
solving problems (Table 5).
Models occasionally generate non-executable code. Roughly 10% of
Bha¯skara’s IID programs fail to execute. 86% of these are SyntaxErrors,
which often occur because decoding terminates before finishing the program or
the model generates a program of the form ‘2+3=5’, which is invalid Python.
The remaining 14% of execution failures are less trivial, including NameErrors
(7%) and TypeErrors (1%) (see Table 6).
Bha¯skara is a good starting point for further fine-tuning Table 5
shows that our Bha¯skara model is a better starting point for downstream
fine-tuning than the vanilla pre-trained GPT-Neo-2.7B. When comparing fine-
11
tuning for direct question answering with T5-3B, we see an almost 8% absolute
improvement in F1 (30.1% to 37.6%). These findings establish Bha¯skara as a
strong starting point for further fine-tuning on new tasks. For this reason, we
release our multi-task model for public use under the name Bha¯skara, with
the hope that it will be useful for future research into math reasoning models.
5.2 Results: Category-wise Analysis
In this section we discuss the trends among the tasks within each category. For
brevity, we primarily consider Bha¯skara, the GPT-Neo multi-task model in
the program-synthesis setting.
Math ability. Among the tasks in the math category, Bha¯skara excels
in basic math, linear algebra, and in-domain statistics. On these tasks, it
performs equal or better to Codex. On the other hand, Bha¯skara struggles
in advanced math and geometry, with mediocre performance in multiplication-
division, number theory, and calculus. Codex shows analogous trends, except
for performing very well on calculus (0.930)7.
Language complexity . Models generally show lower performance on pro-
gram synthesis as language complexity increases. Bha¯skara gets mean F1 over
0.5 only for datasets with the least linguistic complexity where it achieves an F1
of 0.7.
Question format. Among the format tasks in the dataset, Bha¯skara does
exceptionally well on multiple-choice and natural-language inference, getting
performance close to 0.9 on the latter, and outperforming Codex on both. On
the other hand, the model performs close to 0.25 for reading comprehension and
fill-in-the-blank, though with 0.5 F1 on out-of-domain fill-in-the-blank.
Background knowledge. Bha¯skara performs above 0.5 F1 only for prob-
lems requiring commonsense and math formulas and fails to do similarly on
problems requiring other forms of external knowledge like physics, computer
science, or real-world knowledge.
5.3 Results: Few-shot Prompting
Finally, we study the few-shot performance of much larger models ( 175B), to
≈
better understand the performance of the smaller trained models ( 2.7B) and
≈
to provide a benchmark for evaluating other large language models. Overall, we
find that few-shot prompted models generally outperform their much smaller
but fine-tuned counterparts.
12
0.6
Model
GPT3
0.5
Codex
0.4
0.3
0.2
0.1
0 1 3 max
Number of few-shot examples
Figure 3: Average F1 scores of GPT-3 and Codex with different numbers of
few-shot examples in L¯ila.
Zero-shot Few-shot (3)
Dimension
w/o Inst w/ Inst w/o Inst w/ Inst
Math ability 0.120 0.123 0.311 0.306
Language 0.124 0.131 0.352 0.350
Format 0.241 0.257 0.555 0.540
Knowledge 0.108 0.112 0.367 0.363
Average 0.148 0.156 0.396 0.390
Table 6: The IID scores for GPT-3 models with and without instruction prompt-
ing (Inst). Instruction helps slightly in zero-shot setting, but not in few-shot
setting.
Instructions and more examples improve performance. We find that
the number of few-shot examples greatly impacts prompt models’ performance.
Figure 3 shows that GPT-3 answer prediction beats Codex program synthesis
in zero- to one-shot settings, but Codex overtakes with more examples. Ta-
ble 6 shows that prompting with instructions improves performance only in the
zero-shot setting, meaning that in the limited contexts of the prompt models,
examples are more important than instructions for mathematical reasoning.
This is consistent with the findings of Puri et al. (2022) on instruction-example
equivalence.
Few-shotGPT-3answerpredictionunderperformsBha¯skara. While
prompt-based models outperform our fine-tuned models in general when compar-
ing within direct-answering and program-synthesis, when comparing Bha¯skara
program-synthesis to GPT-3 direct answering we find that the much smaller
Bha¯skara consistently outperforms GPT-3.
7NotethatthetrainingsetforCodexisnotknown.
13
1F
egarevA
Few-shot Codex performance is relatively strong. Relative to the 2.7B
trained models, Codex demonstrates strong few-shot IID and OOD performance.
Somenotableexceptionstothispatternarethestatistics,linearalgebra,multiple-
choice question answering, and NLI tasks. Generally, OOD few-shot performs
much better than OOD for the fine-tuned models.
Few-shot Codex fails on some tasks. Despite strong performance relative
to Bha¯skara, Codex obtains less that 0.5 F1 on several tasks, with especially
poor performance on geometry, number theory, advanced math, complex lan-
guage, computer science problems, science formulas, and real world knowledge.
6 Conclusion
Inthiswork,weintroduceL¯ila,aunifiedmathematicalreasoningbenchmarkfor
a holistic evaluation of AI agents. L¯ila consists of 23 tasks across 4 dimensions
(i) mathematical abilities, (ii) language format, (iii) language complexity, (iv)
external knowledge. It builds on 20 existing mathematical reasoning datasets to
collect instructions and Python programs. Further, it also supports measuring
out-of-distribution performance and robustness to language perturbations via
L¯ila-OOD and L¯ila-Robust respectively. We also introduce Bha¯skara, a
2.7B-parameterfine-tunedmulti-taskmodel. Wefindthatmulti-taskingimproves
over single-task performance by 21.83% F1 score on average, and that our model
is a strong starting point for further fine-tuning on new math reasoning tasks.
The best performing model we evaluate achieves only 60.40% F1 indicating the
potential for improvement on the proposed benchmark.
6.1 Limitations
One drawback of our unified format is the difficulty of evaluating models. In
our work we use F1 for lack of a better alternative. F1 likely over-estimates
performance, e.g., given the gold answer “2 apples”, the predicted answers “2”
and “apples” receive the same score, though the former is better.
L¯ila contains 23 tasks which are created from 20 datasets and 44 sub-
datasets. There is scope to add more mathematical reasoning datasets (e.g.,
theoremproving.) Theflexibleunifiedformatof L¯ilaallowsforfutureextensions.
Additionally, our categorization provides a way to identify areas for extension.
For instance, we only have 1 dataset for linear algebra, which happens to not
use natural language, and takes the form of generative QA. Our benchmark
will benefit from future linear algebra additions, perhaps with word problems
formatted as fill-in-the-blank questions.
14
References
GillesAdda,BenoîtSagot,KarënFort,andJosephMariani.2011. Crowdsourcing
for language resource development: Critical analysis of amazon mechanical
turk overpowering use. In 5th Language and Technology Conference.
ArmenAghajanyan,AnchitGupta,AkshatShrivastava,XilunChen,LukeZettle-
moyer, and Sonal Gupta. 2021. Muppet: Massive multi-task representations
with pre-finetuning. arXiv preprint arXiv:2101.11038.
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and
HannanehHajishirzi.2019.Mathqa: Towardsinterpretablemathwordproblem
solving with operation-based formalisms. arXiv preprint arXiv:1905.13319.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
Le, et al. 2021. Program synthesis with large language models. arXiv preprint
arXiv:2108.07732.
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021.
Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow.
Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers,
MatthewEPeters,AshishSabharwal,andYejinChoi.2020. Adversarialfilters
of dataset biases. arXiv preprint arXiv:2002.04108.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-
plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are
few-shot learners. In Advances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates, Inc.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas
Joseph, GregBrockman, etal.2021. Evaluatinglargelanguagemodelstrained
on code.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168.
Henry T Colebrooke. 1817. Arithmetic and mensuration of brahmegupta and
bhaskara.
15
DheeruDua,AnanthGottumukkala,AlonTalmor,SameerSingh,andMattGard-
ner. 2019a. Orb: An open reading benchmark for comprehensive evaluation of
machine reading comprehension. arXiv preprint arXiv:1912.12598.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh,
and Matt Gardner. 2019b. Drop: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1 (Long and Short Papers),
pages 2368–2378.
KarënFort, GillesAdda, andKevinBretonnelCohen.2011. Amazonmechanical
turk: Gold mine or coal mine? Computational Linguistics, pages 413–420.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster,JasonPhang,HoraceHe,AnishThite,NoaNabeshima,ShawnPresser,
and Connor Leahy. 2020. The Pile: An 800gb dataset of diverse text for
language modeling. arXiv preprint arXiv:2101.00027.
Jesse Michael Han, Jason M. Rute, Yuhuai Wu, Edward W. Ayers, and Stanislas
Polu. 2021. Proof artifact co-training for theorem proving with language
models. ArXiv, abs/2102.06203.
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,
Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al.
2021a. Measuring coding challenge competence with apps. arXiv preprint
arXiv:2105.09938.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart,
EricTang,DawnSong,andJacobSteinhardt.2021b. Measuringmathematical
problem solving with the math dataset. arXiv preprint arXiv:2103.03874.
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan,
and Dawn Song. 2020. Pretrained transformers improve out-of-distribution
robustness. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 2744–2751.
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kush-
man. 2014. Learning to solve arithmetic word problems with verb categoriza-
tion. In In Conference on Empirical Methods in Natural Language Processing
(EMNLP).
Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and Wei-Ying Ma.
2016. How well do computers solve math word problems? large-scale dataset
construction and evaluation. In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages
887–896.
16
Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark,
and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with
a single qa system. arXiv preprint arXiv:2005.00700.
Aditya Kolachana, K Mahesh, and K Ramasubramanian. 2019. Use of calculus
in hindu mathematics. In Studies in Indian Mathematics and Astronomy,
pages 345–355. Springer.
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni,
and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations.
Transactions of the Association for Computational Linguistics, 3:585–597.
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh
Hajishirzi. 2016. Mawps: A math word problem repository. In Proceedings of
the 2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 1152–1157.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014.
Learning to automatically solve algebra word problems. In Proceedings of
the 52nd Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 271–281.
Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. 2021. Isarstep: a
benchmarkforhigh-levelmathematicalreasoning. InInternational Conference
on Learning Representations.
Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020. Birds have
four legs?! numersense: Probing numerical commonsense knowledge of pre-
trained language models. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 6862–6868.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program
induction by rationale generation: Learning to solve and explain algebraic
word problems. arXiv preprint arXiv:1705.04146.
Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021.
Unicorn on rainbow: A universal commonsense reasoning model on a new
multitask benchmark. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 35, pages 13480–13488.
Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang,
andSong-ChunZhu.2021a. Inter-gps: Interpretablegeometryproblemsolving
with formal language and symbolic reasoning. In The 59th Annual Meeting of
the Association for Computational Linguistics (ACL).
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay
Rajpurohit, Peter Clark, and Ashwin Kalyan. 2022. Dynamic prompt learning
viapolicygradientforsemi-structuredmathematicalreasoning. arXiv preprint
arXiv:2209.14610.
17
Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu,
Xiaodan Liang, and Song-Chun Zhu. 2021b. Iconqa: A new benchmark for
abstract diagram understanding and visual language reasoning. In The 35th
Conference on Neural Information Processing Systems Track on Datasets and
Benchmarks (NeurIPS 2021).
BryanMcCann,NitishShirishKeskar,CaimingXiong,andRichardSocher.2018.
The natural language decathlon: Multitask learning as question answering.
arXiv preprint arXiv:1806.08730.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020a. A diverse corpus for
evaluating and developing English math word problem solvers. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics,
pages 975–984, Online. Association for Computational Linguistics.
Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020b. A diverse corpus for
evaluating and developing english math word problem solvers. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics,
pages 975–984.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh
Hajishirzi. 2022a. Reframing instructional prompts to GPTk’s language. In
Findings of the Association for Computational Linguistics: ACL 2022, pages
589–612, Dublin, Ireland. Association for Computational Linguistics.
SwaroopMishra,DanielKhashabi,ChittaBaral,andHannanehHajishirzi.2022b.
Cross-task generalization via natural language crowdsourcing instructions. In
Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 3470–3487.
Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter
Clark, Chitta Baral, and Ashwin Kalyan. 2022c. Numglue: A suite of fun-
damental yet challenging mathematical reasoning tasks. In Proceedings of
the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 3505–3523.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models
really able to solve simple math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, pages 2080–2094, Online.
Association for Computational Linguistics.
Ravsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022.
How many data samples is an additional instruction worth? arXiv preprint
arXiv:2203.09161.
Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019.
Equate: A benchmark evaluation framework for quantitative reasoning in
natural language inference. arXiv preprint arXiv:1901.03735.
18
Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh.
2020. Beyond accuracy: Behavioral testing of nlp models with checklist. In
Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, pages 4902–4912.
AnnaRogers,MattGardner,andIsabelleAugenstein.2021.Qadatasetexplosion:
Ataxonomyofnlpresourcesforquestionansweringandreadingcomprehension.
arXiv preprint arXiv:2107.12708.
Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In
Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing, pages 1743–1752.
Subhro Roy and Dan Roth. 2016. Solving general arithmetic word problems.
arXiv preprint arXiv:1608.01413.
Subhro Roy and Dan Roth. 2017. Unit dependency graph and its application
to arithmetic word problem solving. In Thirty-First AAAI Conference on
Artificial Intelligence.
Subhro Roy and Dan Roth. 2018. Mapping to declarative knowledge for word
problemsolving.TransactionsoftheAssociationforComputationalLinguistics,
6:159–172.
Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about quantities
in natural language. Transactions of the Association for Computational
Linguistics, 3:1–13.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika,
Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,
et al. 2021. Multitask prompted training enables zero-shot task generalization.
arXiv preprint arXiv:2110.08207.
Benoy Kumar Sarkar. 1918. Hindu Achievements in Exact Science: A Study in
the History of Scientific Development. Longmans, Green and Company.
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019.
Analysing mathematical reasoning abilities of neural models. arXiv preprint
arXiv:1904.01557.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb,
Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantify-
ing and extrapolating the capabilities of language models. arXiv preprint
arXiv:2206.04615.
OyvindTafjord,PeterClark,MattGardner,Wen-tauYih,andAshishSabharwal.
2019. Quarel: A dataset and models for answering questions about qualitative
relationships. InProceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pages 7063–7071.
19
Shyam Upadhyay and Ming-Wei Chang. 2015. Draw: A challenging and diverse
algebra word problem set. Technical report, Citeseer.
Shyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang, and Wen-tau Yih. 2016.
Learning from explicit and implicit supervision jointly for algebra word prob-
lems. In Proceedings of the 2016 Conference on Empirical Methods in Natural
Language Processing, pages 297–306.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian
Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A
stickier benchmark for general-purpose language understanding systems. In
Advances in Neural Information Processing Systems, pages 3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
SamuelRBowman.2018. Glue: Amulti-taskbenchmarkandanalysisplatform
for natural language understanding. arXiv preprint arXiv:1804.07461.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Ko-
rdi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, et al. 2022. Benchmarking general-
ization via in-context instructions on 1,600+ language tasks. arXiv preprint
arXiv:2204.07705.
JasonWei, MaartenBosma, VincentYZhao, KelvinGuu, AdamsWeiYu, Brian
Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language
models are zero-shot learners. arXiv preprint arXiv:2109.01652.
Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi,
and Kyunghyun Cho. 2021. Naturalproofs: Mathematical theorem proving in
naturallanguage. InThirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1).
Sean Welleck, Peter West, Jize Cao, and Yejin Choi. 2022. Symbolic brittleness
in sequence models: on systematic generalization in symbolic mathematics. In
AAAI.
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart
van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-
complete question answering: A set of prerequisite toy tasks. arXiv preprint
arXiv:1502.05698.
Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Baker Grosse. 2021. {INT}:
An inequality benchmark for evaluating generalization in theorem proving. In
International Conference on Learning Representations.
Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham
Neubig. 2018. Learning to mine aligned code and natural language pairs from
stack overflow. In International Conference on Mining Software Repositories,
MSR, pages 476–486. ACM.
20
AmirRZamir,AlexanderSax,WilliamShen,LeonidasJGuibas,JitendraMalik,
andSilvioSavarese.2018. Taskonomy: Disentanglingtasktransferlearning. In
Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 3712–3722.
Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth.
Do language embeddings capture scales?
Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. 2021. Minif2f: a cross-
system benchmark for formal olympiad-level mathematics. arXiv preprint
arXiv:2109.00110.
Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019. “going on a vaca-
tion” takes longer than “going for a walk”: A study of temporal commonsense
understanding. In Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pages 3363–3369.
21
Task: Basic Math Task: Muldiv
Problem: Before December, cus- Problem: Tickets to the school
tomers buy 1346 ear muffs from play cost 6 for students and 8 for
the mall. During December, they adults. If20studentsand12adults
buy 6444, and there are none. In bought tickets, how many dollars’
all,howmanyearmuffsdothecus- worth of tickets were sold?
tomers buy?
Predicted Answer: 48 7
Predicted Answer: 1346.0 7 Generated Program:
Generated Program: a=20*6
answer = 1346.0 + 6444.0 b=12*8
print(answer) c=a+b
# Result ==> 7790.0 answer=c
print(answer)
Gold Answer: 7790.0 3 # Result ==> 216.0
Gold Answer: 216 3
Figure 4: Examples with Bha¯skara on Basic Math and Muldiv.
A Qualitative Examples
Figures 4 and 5 give examples of input-output behavior of Bha¯skara. Figure 6
gives an example of a non-compiling output program.
B Dataset Collection
Tables 12-15 give examples and datasets from each task for each category.
Category Examples Datasets
Math Table 8 Table 12
Language Table 9 Table 13
Format Table 10 Table 14
Knowledge Table 11 Table 15
Table 7: Examples and datasets meta-table.
B.1 Expert annotation
In the worker qualification process, we ask each worker to annotate 30 questions.
WemanuallyverifyeachannotationandqualifythosewhosePythonannotations
are satisfactory. We also provide feedback such as "write simpler programs, use
representative variable names instead of just letters, add comments wherever
possible" to annotators after the worker qualification process. We instruct
22
Task: Statistics Task: Linear Algebra
Problem: Determine the KL Di- Problem: Find the determinant
vergencebetweentheempiricaldis- of the matrix
tributions
0 2 3
0.149,0.039,0.099,0.09,0.203,0.077,0.039,0.19 − −
0 5 0 .
and  
1 3 2
0.158,0.256,0.042,0.008,0.064,0.051,0.05,0.072.
 
Predicted Answer: 0.36 7 Predicted Answer: 8 7
−
Generated Program: Generated Program:
from scipy.stats import import numpy as np
entropy
a = np.array([
distribution1 = [0.149, 0. [0, -2, -3],
039, [0, 5, 0],
0.099, 0.09, 0.203, [1, 3, 2]])
0.077, 0.039, 0.19]
print(np.linalg.det(a))
distribution2 = [0.158, 0. # Result ==> 15.0
256,
0.042, 0.008, 0.064, Gold Answer: 15 3
0.051, 0.05, 0.072]
print(entropy(distribution1,
qk=
distribution2)
)
# Result ==> 0.5122...
Gold Answer: 0.5122... 3
Figure 5: Examples with Bha¯skara on Statistics and Linear Algebra.
annotators to use a minimal set of Python libraries, and we ask them to record
thePythonlibrariestheyuseinacommondocument. Wefindthattheannotators
could get the task done just by using the sympy and the datetime libraries. We
also ask annotators to report any bugs in answer annotation, which they report
for a small number of questions; we subsequently fix those.
Wegive10samplequestionannotationstoannotatorsasillustrativeexamples
which vary in structure, length, format, underlying reasoning skill, etc. We pay
20 dollars per hour up to 20 hours per week as compensation for the data
annotation work.
L¯ila-Robust To create the L¯ila-Robust dataset, we first define a set of 9
templates, consisting of 3 variation styles defined in SVAMP (Patel et al., 2021)
as well as 6 novel templates of our own. We refer to the SVAMP templates as
SVAMP-COO,SVAMP-COP,andSVAMP-IU,whichcorrespondtochangingthe
order of objects, changing the order of phrases, and adding irrelevant, unhelpful
information to the problem statement, respectively. Our novel templates are
23
Model: Codex Model: Bha¯skara
Task: Statistics Task: Muldiv
Problem: Simplifytheexpression Problem: Jennycollectscansand
(9x2+3x+7)+(3x2+7x5+2). Ex- bottles to take down to the recy-
press your answer as a polynomial cling center. Each bottle weighs
with terms arranged in decreasing 6 ounces and each can weighs 2
order of degree. ounces. Jenny can carry a total of
100 ounces. She collects 20 cans
Generated Program: and as many bottles as she can
from sympy import Poly carry. If she gets paid 10 cents per
bottle and 3 cents per can, how
p = Poly(9*(x**2) + 3*x + 7 much money does she make (in
+
cents)?
3*(x**2) + 7*(x**5)
+ 2)
Generated Program:
answer = p.as_expr() a=20*6
b=a*2
print(answer) c=b*3
# ==> NameError (x is not d=c*10
defined) e=d*3
f=e*3
GoldAnswer: 7x5+12x2+3x+9 g=f+g
answer=g
print(answer)
# ==> NameError (g is not
defined)
Gold Answer: 216
Figure 6: NameErrors in Codex and Bha¯skara.
named ROBUST-IR, ROBUST-AP, ROBUST-ADJ, ROBUST-Q, ROBUST-RQ,
and ROBUST-RM. ROBUST-IR refers to adding information that is unhelpful
for solving the question but may be related to the context of the problem.
ROBUST-AP refers to increasing problem verbosity by turning active speech to
passive speech. ROBUST-ADJ refers to increasing problem verbosity by adding
adjectives or adverbs. ROBUST-Q indicates turning a problem statement into a
question, in the style of a conversation with a student. ROBUST-RQ indicates
removing question words in a problem and turning it into a statement; it is
roughly the inverse of ROBUST-Q. Finally, ROBUST-RM refers to the removal
of mathematics terms that are implicitly defined. Examples of each template
are found in Table 16.
For our crowdsourcing pipeline, we provide each Amazon Mechanical Turk
worker with 10 questions split from 20 questions sampled from each dataset. We
run a separate job for each of our 9 templates. In particular, each HIT contains
the 10 split questions from the original datasets, alongside the problem solution.
Workers are asked to submit an augmentation for each question according to
the style of the template assigned to each job. Thus, we run 9 separate jobs
24
Task Questioncategory Example
Basicmath:addition,subtraction,fact Question:IfJimbois484feetawayfromabeetleandquarterof827feetaway
Task1 basedQAetc. fromagrasshopper,whichinsectwillseembiggertohim?"Option1":beetle,
"Option2":grasshopperAnswer:Option2
Muldiv: multiplication,divisionalong Question:Mrs.Hiltbought2pizzas.Eachpizzahad8slices.So,shehad__
Task2
withaddition,subtractionetc. totalslicesofpizza.Answer:16
Numbertheory:prime,power,negation, Question: Howmanynumbersaredivisiblebyboth2and3upto300?
Task3
modulusandotheroperatorsetc. Answer:50
Algebra: equations,functions,polyno- Question:Thesumofthethreesmallestoffourconsecutiveintegersis30more
Task4
mials,seriesetc. thanthelargestinteger. Whatarethefourconsecutiveintegers? Answer:
[15,16,17,18]
Geometry: triangles, polygons, 3D Question:Ahallis6meterslongand6meterswide.Ifthesumoftheareas
Task5 structuresetc. ofthefloorandtheceilingisequaltothesumoftheareasoffourwalls,what
isthevolumeofthehall(incubicmeters)?Answer:108
Statistics:binomial,divergence,mean, Question:Thereare11boysand10girlsinaclass.Iffivestudentsareselected
Task6
median,mode,varianceetc. atrandom,inhowmanywaysthat3girland2boysareselected?Answer:
6600
Calculus: differentiation, integration, Question:Letg(y)=9*y**4+25*y**2+6.Lets(d)=1-d**4.Letx(t)=
Task7
gradient,seriesexpansionetc. -g(t)+6*s(t).Whatisthethirdderivativeofx(f)wrtf?Answer:-360*f
Linearalgebra: vectors,dotproducts, Question:Problem:Convertthefollowingmatrixtoreducedrowechelonform:
Task8 Eigenvectors,matricesetc. 7
5
−12
0
−210 −4
7
.Answer: 01 10 −911 03 −61 493 0
(cid:18)− − − (cid:19) (cid:18) 20 80 (cid:19)
Advanced math: heuristics required Question:Letf(x)=2x.Find f(f(f(f(1)))).Answer:256
Task9 alongwithprobability,statistics,oral-
p
gebra,Olympiadlevelproblems
Table8: Exampleofeachtaskinthemathability categoryoftheL¯ilabenchmark.
to obtain augmentations for all templates across all datasets. To familiarize
workers with the intended style of each template, we provide 3 demonstrative
augmentations within the instructions of each HIT, as summarized in Table
16. We restrict our crowdsourcing pipeline to workers that had above a 98%
acceptance rate with over 1000 completed HITs. We provide workers with an
upper bound of 1 hour to complete each HIT but specify in the instructions that
each HIT should feasible be completed in 10 minutes. Based on minimum wage
policies and under the assumption that workers follow the 10-minute completion
guideline, we accordingly compensate $3 per HIT. Finally, to ensure dataset
quality of generations via the Amazon Mechanical Turk Fort et al. (2011); Adda
et al. (2011), we manually assess the worker augmentations produced for each
template.
C Dataset Statistics
Figure 8 gives relatives sizes of tasks within each category. Figure 9 illustrates
the unigram frequencies in L¯ila, where larger words indicate higher frequency.
Table 17 gives comprehensive statistics on each task. Table 19 cites each
component dataset of L¯ila.
D Additional Results
Table 18 gives the unaggregated performance of each model on each dataset in
L¯ila (some datasets are split across tasks).
25
Task Questioncategory Example
Task10 Nolanguage Computethemedianof4√2, −6,3e,3, −6, −√14 π,6.Answer:3
Simplelanguage Question:Joanhad9blueballoons,butSallypopped5ofthem.Jessicahas
Task11
2blueballoons.Theyhave__blueballoonsnow.Answer:6
Complexlanguage:involvingco-reference Question:Passage:Accordingtothe2011NationalHouseholdSurvey,89.3%
resolutionetc.,multi-sentencelanguage,ad- ofMarkhamsresidentsareCanadiancitizens,andabout14.5%ofresidentsare
versariallanguage:containingtrickywords recentimmigrants(from2001to2011). TheracialmakeupofMarkhamis;
etc.,oftencreatedadversarially EastAsian(39.7%),WhiteCanadian(27.5%),SouthAsianCanadian(19.1%),
SoutheastAsian(3.9%),BlackCanadians(3.2%),WestAsian&ArabCanadians
Task12 (3.2%),LatinAmericanCanadian(0.5%),AboriginalpeoplesinCanada(0.2%),
and1.9%ofthepopulationismultiracialwhiletherestofthepopulation(0.7%)
isofanothergroup.Markhamhasthehighestvisibleminoritypopulationof
anymajorCanadiancity(over100,000residents)at72.3%,andisoneofeight
majorcitieswithnomajorityracialgroup. Question:Howmanypercentof
peoplewerenotwhite?Answer:72.5
Table 9: Example of each task in the language complexity category of the L¯ila
benchmark.
Task Questioncategory Example
Task13 Fillintheblank Question:Delphiniumhas_floretsortheyarefullofholes.Answer:no
Task14 Generativequestionanswering Question:Calculatetheremainderwhen160isdividedby125.Answer:35
Multiplechoicequestionanswering(MCQ) Question:Thefishglidedwithaspeedof8m/sthroughthewaterand5m/s
Task15 throughthejellobecausethe__issmoother?"Option1":jello,"Option2":
water.Answer:Option2
Naturallanguageinference(NLI) Question: "statement1": Alyssapicked42.0pearsfromthepeartreeand
Task16 Nancysold17.0ofthepears,"statement2":25.0pearswereleft,"options:"
Entailmentorcontradiction?Answer:Entailment
Readingcomprehension(RC) Question:Passage:AlategamerallybyWashingtonledthemtotheEagles’
26yardline.AshottotheendzonebyRobertGriffinIIIwouldbeintercepted
byBrandonBoykin,clinchinganEagleswin.TheEagleswouldmoveto6-5.
ThisistheEaglesfirstwinatLincolnFinancialFieldsinceWeek4ofthe2012
Task17
season,becausepriortothisgame,theEagleshadneverwonagameintheir
homestadiumin414dayssincethatsameweek,snappinga10-gamelosing
streakathomewiththiswin.Question:Howmanymorewinsthanlossesdid
theEagleshaveafterthisgame?Answer:1
Table 10: Example of each task in the question formatcategory of the L¯ila
benchmark.
Task Questioncategory Example
Noexternalknowledge:onlymathemati- Question:Ifthereare7bottlecapsinaboxandLindaputs7morebottle
Task18
calcommonsenseknowledgerequired capsinside,howmanybottlecapsareinthebox?Answer:14
Commonsense: temporalcommonsense Question:Outsidetemple,thereisashopwhichcharges12dollarsforeach
knowledge(e.g.,peopleusuallyplaybas- object.Pleasenotethatoneshoeiscountedasanobject.Sameistrueforsocks
Task19 ketballforafewhoursandnotdays), andmobiles.Paisleywenttotemplewithbothparents.Allofthemkepttheir
numericalcommonsenseknowledge(e.g. shoes,socksandmobilesintheshop.Howmuchtheyhavetopay?Answer:
birdshas2legs) 180
Mathformulas:algebra,geometry,prob- Question:Simplify-3*(sqrt(1700)-(sqrt(1700)+(3+sqrt(1700))*-6))+-3.
Task20
abilityetc. Answer:-180*sqrt(17)-57
Scienceformulas:physics,chemistryetc. Question:FindthenumberofmolesofH2Oformedoncombining2molesof
Task21
NaOHand2molesofHCl.Answer:2
Computerscienceknowledge:datastruc- Question:Applyfunctions‘mean’and‘std’toeachcolumnindataframe‘df’
Task22
turealgorithmslikemergesortetc. Answer:df.groupby(lambdaidx:0).agg([’mean’,’std’])
Real-world knowledge: COVID mod- Question:Ourphysicsclubhas20members,amongwhichwehave3officers:
elling,climatemodellingetc. President,VicePresident,andTreasurer.However,onemember,Alex,hates
Task23 anothermember,Bob.HowmanywayscanwefilltheofficesifAlexrefusesto
serveasanofficerifBobisalsoanofficer?(Nopersonisallowedtoholdmore
thanoneoffice.)Answer:6732
Table11: Exampleofeachtaskinthebackground knowledgecategoryoftheL¯ila
benchmark.
26
Task Mathcategory IID OOD
addsub.json MCTaco_event_duration_structured.json
Numersense_structured.json NumGLUE_Task3.json
MCTaco_stationarity_structured.json
Task1 Basicmath MCTaco_frequency_structured.json
MCTaco_event_typical_time_structured.json
MCTaco_event_ordering_structured.json
NumGLUE_Task7.json
singleop.json svamp_structured.json
multiarith.json NumGLUE_Task4.json
asdiv.json
Task2 Muldiv GSM8k_structured.json
NumGLUE_Task1.json
NumGLUE_Task2.json
deepmind_mathematics_muldiv.json
mathqa_physics.json mbpp_structured.json
APPS_structured.json mathqa_other.json
mathqa_gain.json
amps_number_theory.json
Task8 Numbertheory mathqa_general.json
conala_structured.json
NumGLUE_Task5.json
deepmind_mathematics_numbertheory.json
singleq.json draw_structured.json
simuleq.json dolphin_structured.json
Task4 Algebra amps_algebra.json
NumGLUE_Task8.json
deepmind_mathematics_algebra.json
Task5 Geometry amps_geometry.json mathqa_geometry.json
Task6 Statistics amps_counting_and_stats.json mathqa_probability.json
amps_calculus.json deepmind_mathematics_calculus.json
Task7 Calculus deepmind_mathematics_basicmath.json
Task8 Linearalgebra amps_linear_algebra.json
Task9 Advancedmath MATH_crowdsourced.json
Table 12: Raw datasets used to create different tasks in L¯ila across different
math categories.
Question: A gardener is going to plant 2 red rosebushes and 2 white rosebushes. If
thegardeneristoselecteachofthebushesatrandom,oneatatime,andplantthem
in a row, what is the probability that the 2 rosebushes in the middle of the row will
be the red rosebushes?
Options: {A:1/12, B:1/6, C:1/5, D:1/3, E:1/2}
Answer: B
Explanation: We are asked to find the probability of one particular pattern: wrrw.
Total# of ways a gardener can plant these fourbushes is the #of permutations of4
letters wwrr, out of which 2 w’ s and 2 r’ s are identical, so 4 ! / 2 ! 2 ! = 6 ; so p =
1 / 6. Answer: B.
Program: import scipy
n0 = 2.0
n1 = 2.0
n2 = 2.0
t0 = n0 + n0
t1 = scipy.special.comb(t0, n0)
answer = 1.0 / t1
Figure 7: An example of instruction annotation.
27
ID Language cate- IID OOD
gory
amps_number_theory.json amps_algebra.json
amps_counting_and_stats.json deepmind_mathematics_calculus.json
amps_calculus.json
amps_linear_algebra.json
Task10 Nolanguage deepmind_mathematics_muldiv.json
deepmind_mathematics_numbertheory.json
deepmind_mathematics_algebra.json
deepmind_mathematics_basicmath.json
addsub.json MCTaco_frequency_structured.json
Numersense_structured.json NumGLUE_Task1.json
MCTaco_stationarity_structured.json mathqa_general.json
MCTaco_event_typical_time_structured.json NumGLUE_Task4.json
MCTaco_event_ordering_structured.json
MCTaco_event_duration_structured.json
singleop.json
multiarith.json
asdiv.json
Task11 Simplelanguage GSM8k_structured.json
APPS_structured.json
mathqa_gain.json
mathqa_other.json
singleq.json
simuleq.json
NumGLUE_Task8.json
draw_structured.json
dolphin_structured.json
mathqa_probability.json
mathqa_physics.json mbpp_structured.json
APPS_structured.json mathqa_other.json
mathqa_gain.json
amps_number_theory.json
Task12 Complexlanguage mathqa_general.json
conala_structured.json
NumGLUE_Task5.json
deepmind_mathematics_numbertheory.json
Table 13: Raw datasets used to create different tasks in L¯ila across different
language categories.
28
ID Formatcategory IID OOD
Task13 Fillintheblank NumGLUE_Task4.json Numersense_structured.json
amps_number_theory.json svamp_structured.json
amps_counting_and_stats.json mathqa_geometry.json
amps_linear_algebra.json amps_calculus.json
amps_algebra.json singleq.json
deepmind_mathematics_calculus.json NumGLUE_Task2.json
addsub.json mbpp_structured.json
singleop.json deepmind_mathematics_numbertheory.json
multiarith.json
asdiv.json
GSM8k_structured.json
APPS_structured.json
mathqa_gain.json
mathqa_other.json
simuleq.json
Task14 GenerativeQA NumGLUE_Task8.json
draw_structured.json
dolphin_structured.json
mathqa_probability.json
MCTaco_frequency_structured.json
NumGLUE_Task1.json
mathqa_general.json
mathqa_physics.json
conala_structured.json
amps_geometry.json
MATH_crowdsourced.json
deepmind_mathematics_calculus.json
deepmind_mathematics_muldiv.json
deepmind_mathematics_algebra.json
deepmind_mathematics_basicmath.json
NumGLUE_Task3.json MCTaco_event_typical_time_structured.json
MCTaco_stationarity_structured.json
Task15 MCQ MCTaco_event_ordering_structured.json
MCTaco_event_duration_structured.json
Task16 NLI NumGLUE_Task5.json
Task17 RC mathqa_physics.json mbpp_structured.json
Table 14: Raw datasets used to create different tasks in L¯ila across different
format categories.
29
ID Knowledge cate- IID OOD
gory
addsub.json NumGLUE_Task4.json
singleop.json GSM8k_structured.json
multiarith.json svamp_structured.json
asdiv.json NumGLUE_Task7.json
simuleq.json
Task18 Noexternalknowledge NumGLUE_Task8.json
draw_structured.json
dolphin_structured.json
NumGLUE_Task5.json
deepmind_mathematics_muldiv.json
Numersense_structured.json NumGLUE_Task1.json
MCTaco_frequency_structured.json MCTaco_event_ordering_structured.json
NumGLUE_Task3.json
Task19 Commonsense MCTaco_stationarity_structured.json
MCTaco_event_duration_structured.json
MCTaco_event_typical_time_structured.json
amps_number_theory.json amps_counting_and_stats.json
amps_linear_algebra.json mathqa_general.json
amps_algebra.json amps_calculus.json
deepmind_mathematics_calculus.json
mathqa_probability.json
singleq.json
Task20 Mathformulas mathqa_gain.json
mathqa_other.json
deepmind_mathematics_algebra.json
deepmind_mathematics_basicmath.json
deepmind_mathematics_calculus.json
deepmind_mathematics_numbertheory.json
amps_geometry.json
Task21 Scienceformulas NumGLUE_Task2.json
mathqa_physics.json
Computerscience APPS_structured.json mathqa_geometry.json
Task22
knowledge conala_structured.json
Task23 Real-worldknowledge MATH_crowdsourced.json mbpp_structured.json
Table 15: Raw datasets used to create different tasks in L¯ila across different
knowledge categories.
30
TemplateName Variation Example
SVAMP-COO Changetheorderofobjects Question:Allenbought20stampsatthepostofficein37centsand20cents
denominations.Ifthetotalcostofthestampswas$7.06,howmany37cents
stampsdidAllenbuy?
Variation:Allenbought20stampsatthepostofficein20centsand37cents
denominations.Ifthetotalcostofthestampswas$7.06,howmany37cents
stampsdidAllenbuy?
SVAMP-COP Changetheorderofphrases Question:Onepipecanfillatankin5hoursandanotherpipecanfillthe
sametankin4hours.Adrainpipecanemptythefullcontentofthetankin20
hours.Withallthethreepipesopen,howlongwillittaketofillthetank?
Variation:Adrainpipecanemptythefullcontentofatankin20hours.One
pipecanfillthetankin4hoursandanotherpipecanfillthesametankin5
hours.Withallthethreepipesopen,howlongwillittaketofillthetank
withallthethreepipesopen?
SVAMP-IU Addirrelevant,unhelpfulinformation Question:theareaofanisoscelestrapezoidwithsidesoflength5andbases
oflength7and13is?
Variation:monkeysandapesarebothprimates,whichmeansthey’reboth
partofthehumanfamilytree.theareaofanisoscelestrapezoidwithsidesof
length5andbasesoflength7and13is?
ROBUST-IR Addunhelpful,butcontextuallyrelated Question:Tomis15yearsyoungerthanalice.Tenyearsago,Alicewas4
information timesasoldasTomwasthen.Howoldiseachnow?
Variation:Tomis15yearsyoungerthanalice.Tenyearsago,Alicewas4
timesasoldasTomwasthen.Alicereallylikespinapplepizza.Howoldis
eachnow?
ROBUST-AP Turnactiveintopassivespeechtoin- Question: Hay’sLinenssellshandtowelsinsetsof17andbathtowelsin
creaseproblemverbosity setsof6.Ifthestoresoldthesamenumberofeachthismorning,whatisthe
smallestnumberofeachtypeoftowelthatthestoremusthavesold?
Variation:HandtowelsaresoldbyHay’sLinensinsetsof17andbathtowels
aresoldinsetsof6.Ifthesamenumberofeachweresoldbythestorethis
morning,whatisthesmallestnumberofeachtypeoftowelthatthestoremust
havesold?
ROBUST-ADJ Addadjectivesandadverbstoincrease Question:ThereTealeavesexposedtooxygenforupto_hoursbecomeblack
problemverbosity tea.
Variation:Blacktealeavescontinuouslyexposedtooxygenforupto_hours
becomeaveryrichblacktea.
ROBUST-Q Turnataskstatementintoaquestion Question:Productof-7and-1469.125.
Variation:Whatistheproductof-7and-1469.125?
ROBUST-RQ Turnaquestionintoataskstatement Question:Problem:Iftheproductof5andanumberisincreasedby4,the
resultis19.Whatisthenumber?
Variation:Increasingtheproductof5andanumberby4resultsis19.Find
thenumber.
ROBUST-RM Removeexplicitlymathematicalterms Problem:Findthearclengthofthefunctionf(x)=2√xontheintervalx=2
thatareimplicitlydefined tox=8
Variation:Findthearclengthoff(x)=2√xon[2,8]
Table 16: Example for each template provided to MTurk workers to produce
L¯ila-Robust
ID Category Questions Uniquequestions Questionlength Programs Uniqueprograms Programlength
Task1 Basicmath 31,052 31,032 43.1 31,052 7,066 13.3
Task2 Muldiv 16,021 15,936 26.9 16,021 15,279 8.2
Task3 Numbertheory 44,760 44,183 41.3 269,232 261,865 33.2
Task4 Algebra 15,882 15,615 19.3 16,364 15,986 12.7
Task5 Geometry 3,190 3,149 36.1 3,190 3,035 28.7
Task6 Countingandstatistics 6,423 6,384 39.7 6,423 6,335 31.5
Task7 Calculus 4,493 4,202 21.2 4,493 4,170 40.6
Task8 Linearalgebra 11,248 11,204 32.4 11,248 11,204 23.0
Task9 Advancedmath 746 746 21.2 746 745 27.3
Task10 Nolanguage 41,191 40,551 21.2 42,466 41,794 40.6
Task11 Simplelanguage 66,505 66,172 26.9 290,184 258,839 8.2
Task12 Complexlanguage 26,119 25,728 36.1 26,119 25,052 28.7
Task13 Fillintheblank 11,634 11,615 11.0 11,634 997 3.0
Task14 GenerativeQA 102,493 101,239 14.7 327,447 314,652 16.0
Task15 MCQ 9,989 9,989 28.3 9,989 470 3.0
Task16 NLI 6,326 6,325 50.8 6,326 6,243 25.8
Task17 RC 3,642 3,552 182.5 3,642 3,592 10.4
Task18 Noexternalknowledge 28,115 27,964 50.8 28,115 27,117 25.8
Task19 Commonsense 24,677 24,658 30.9 24,677 823 3.0
Task20 Mathformulas 57,841 56,947 19.1 59,116 57,019 25.5
Task21 Scienceformulas 10,505 10,319 36.1 10,505 9,764 28.7
Task22 Complexknowledge 12,200 12,086 14.5 235,879 230,486 24.2
Task23 Real-worldknowledge 746 746 21.2 746 745 27.3
Table 17: Main statistics of L¯ila across the total of 23 tasks.
31
8.4%0.6% Basic math
3.4% 23.2% Muldiv 19.5%
4.8% Number theory 30.8%
Algebra No language
2.4% 12.0% Geometry Simple language
Statistics Complex language
11.9% Calculus
Linear algebra
33.4% Advanced math 49.7%
(a) Math ability categories. (b) Language categories.
4.7%2.7%8.7% 9.1%0.6%
21.0%
7.4% Fill in the blank 7.8% No external knowledge
Commonsense
Generative QA
Math formulas
MCQ
Science formulas
NLI 18.4% Complex knowledge
RC
Real-world knowledge
43.1%
76.4%
(c) Format categories. (d) Knowledge categories.
Figure 8: Task diversity in L¯ila across math, language, format, and knowledge
categories.
Figure9: ThewordclouddistributionofannotatedprogramsintheL¯iladataset.
32
ID Dataset GPT-3 Neo-A Neo-P Codex
1 addsub 0.910 0.116 0.797 0.950
2 amps_algebra 0.116 0.100 0.902 0.655
3 amps_calculus 0.192 0.168 0.922 0.860
4 amps_counting_and_stats 0.183 0.117 0.958 0.650
5 amps_geometry 0.283 0.263 0.074 0.000
6 amps_linear_algebra 0.127 0.235 0.815 0.692
7 amps_number_theory 0.273 0.026 0.875 1.000
8 APPS_structured 0.167 0.154 0.134 0.459
9 asdiv 0.737 0.166 0.092 0.022
10 conala_structured 0.356 0.329 0.329 0.391
11 deepmind_mathematics_algebra 0.202 0.258 0.847 0.910
12 deepmind_mathematics_basicmath 0.270 0.125 0.614 1.000
13 deepmind_mathematics_calculus 0.208 0.026 0.152 0.884
14 deepmind_mathematics_muldiv 0.160 0.034 0.909 1.000
15 deepmind_mathematics_numbertheory 0.296 0.462 0.538 0.710
16 dolphin_t2_final 0.170 0.027 0.006 0.812
17 draw_structured 0.090 0.034 0.005 0.210
18 GSM8k_structured 0.110 0.060 0.139 0.350
19 MATH_crowdsourced 0.150 0.013 0.074 0.472
20 mathqa_gain 0.134 0.054 0.339 0.270
21 mathqa_general 0.110 0.073 0.193 0.120
22 mathqa_geometry 0.120 0.002 0.000 0.250
23 mathqa_other 0.180 0.043 0.011 0.280
24 mathqa_physics 0.120 0.087 0.429 0.210
25 mathqa_probability 0.210 0.003 0.000 0.200
26 mbpp_structured 0.128 0.175 0.164 0.408
27 MCTaco_event_duration_structured 0.800 0.773 0.773 0.710
28 MCTaco_event_ordering_structured 0.860 0.831 0.831 0.890
29 MCTaco_event_typical_time_structured 0.870 0.881 0.881 0.870
30 MCTaco_frequency_structured 0.890 0.862 0.862 0.790
31 MCTaco_stationarity_structured 0.710 0.758 0.758 0.670
32 multiarith 0.360 0.143 0.921 0.990
33 Numersense_structured 0.620 0.495 0.495 0.660
34 NumGLUE_Type_1 0.535 0.108 0.083 0.740
35 NumGLUE_Type_2 0.512 0.285 0.646 0.735
36 NumGLUE_Type_3 0.835 0.004 0.001 0.815
37 NumGLUE_Type_4 0.710 0.076 0.208 0.790
38 NumGLUE_Type_5 0.460 0.200 0.305 0.615
39 NumGLUE_Type_7 0.500 0.516 0.854 0.710
40 NumGLUE_Type_8 0.420 0.082 0.257 0.610
41 simuleq 0.120 0.074 0.010 0.170
42 singleop 0.940 0.347 0.611 1.000
43 singleq 0.830 0.143 0.474 0.670
44 svamp_structured 0.620 0.085 0.060 0.790
Average F1 score 0.400 0.223 0.440 0.613
Table 18: Evaluation results of baselines across different single datasets. On
most datasets, Codex performs best. Model names: GPT-3: the few-shot
175B GPT-3 model; GPT-Neo-A: the fine-tuned 2.7B GPT-3 model where the
predictionoutputisananswer; GPT-Neo-P:thefine-tuned2.7BGPT-3model
33
where the prediction output is a program; Codex: the few-shot Codex model
where the prediction output is a program.
ID Dataset References
1 addsub Hosseini et al. (2014)
2 amps Hendrycks et al. (2021b)
3 APPS Hendrycks et al. (2021a)
4 asdiv Miao et al. (2020b)
5 conala Yin et al. (2018)
6 mathematics Saxton et al. (2019)
7 dolphin Huang et al. (2016)
8 draw Upadhyay and Chang (2015)
9 GSM8k Cobbe et al. (2021)
10 MATH Hendrycks et al. (2021b)
11 mathqa Amini et al. (2019)
12 mbpp Austin et al. (2021)
13 MCTaco Zhou et al. (2019)
14 multiarith Roy and Roth (2015)
15 Numersense Lin et al. (2020)
16 NumGLUE Mishraetal.(2022c);Duaetal.(2019b);Ravichan-
der et al. (2019); Kushman et al. (2014); Tafjord
et al. (2019); Roy and Roth (2018, 2017); Koncel-
Kedziorski et al. (2016, 2015)
17 simuleq Kushman et al. (2014)
18 singleop Roy et al. (2015)
19 singleq Koncel-Kedziorski et al. (2015)
20 svamp Patel et al. (2021)
Table 19: List of source datasets and corresponding references used in construct-
ing L¯ila.
34

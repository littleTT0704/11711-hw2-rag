Making Mobile Augmented Reality Applications Accessible
Jaylin Herskovitz∗ Jason Wu∗ Samuel White
University of Michigan Carnegie Mellon University Apple Inc.
jayhersk@umich.edu jsonwu@cmu.edu samuel_white@apple.com
Amy Pavel Gabriel Reyes Anhong Guo
Apple Inc. Apple Inc. Carnegie Mellon University
apavel@apple.com gareyes@apple.com anhongg@cs.cmu.edu
Jefrey P. Bigham
Apple Inc.
jbigham@apple.com
ABSTRACT 1 INTRODUCTION
Augmented Reality (AR) technology creates new immersive experi- Augmented Reality (AR) has proven useful in a wide variety of ap-
ences in entertainment, games, education, retail, and social media. plication areas, such as retail, education, and social media. Although
AR content is often primarily visual and it is challenging to enable AR content can be audio-based [8, 25, 39], AR is often primarily
access to it non-visually due to the mix of virtual and real-world visual, and thus making this content accessible non-visually is chal-
content. In this paper, we identify common constituent tasks in AR lenging. Prior work has considered how to make Virtual Reality
by analyzing existing mobile AR applications for iOS, and character- (VR) accessible [72], which is a related but very diferent problem.
ize the design space of tasks that require accessible alternatives. For In VR, the entire immersive environment is generated computation-
each of the major task categories, we create prototype accessible ally. It is thus conceivable to build in semantics that would allow
alternatives that we evaluate in a study with 10 blind participants to the entire virtual world to be accessible (e.g., [34]).
explore their perceptions of accessible AR. Our study demonstrates In contrast, AR adds virtual content into the real world. Some-
that these prototypes make AR possible to use for blind users and times that content is only overlayed and is thus easily separable
reveals a number of insights to move forward. We believe our work from the physical world (e.g., Google Glass notifcations). However,
sets forth not only exemplars for developers to create accessible oftentimes, AR applications involve actions and objects that bridge
AR applications, but also a roadmap for future research to make between the physical and virtual world, such as scanning the space
AR comprehensively accessible. to initialize the AR model, or placing virtual objects in relation to
both other virtual objects and real-world objects. Such applications
CCS CONCEPTS are especially difcult to make accessible since doing so requires
• Human-centered computing → Human computer interac-
knowledge of not only the virtual objects the application creates
tion (HCI); Accessibility technologies; Mixed / augmented real-
but also the physical context into which they are placed.
ity.
AR technologies have also been explored in the context of im-
proving accessibility to the real world. For instance, CueSee uses
KEYWORDS
head-mounted AR to help people with low vision better fnd objects
of interest by visually overlaying diferent cues to help mark an
Accessibility; augmented reality; mobile applications.
item or make it easier to see [77]. VizLens overlays an audio inter-
ACM Reference Format: face onto visual (and inaccessible) physical interfaces [22], so that a
Jaylin Herskovitz, Jason Wu, Samuel White, Amy Pavel, Gabriel Reyes, blind person can use them. In this paper, we consider the diferent
Anhong Guo, and Jefrey P. Bigham. 2020. Making Mobile Augmented problem of how we might enable developers to make existing AR
Reality Applications Accessible. In The 22nd International ACM SIGACCESS applications, which are not specifcally designed in advance for
Conference on Computers and Accessibility (ASSETS ’20), October 26–28, 2020, non-visual interactions, possible to be used non-visually.
Virtual Event, Greece. ACM, New York, NY, USA, 14 pages. https://doi. As such, our work builds on a long history of accessibility work,
org/10.1145/3373625.3417006
which has introduced technological means to make visual computer
∗Also with This work was done while Jaylin Herskovitz and Jason Wu were interns at
interfaces accessible in other ways. For instance, screen readers
Apple Inc. have been developed to make graphical user interfaces and window-
ing systems accessible [6, 44]. We believe we are now at a critical
Permission to make digital or hard copies of part or all of this work for personal or time in the development of AR, where we can think ahead about
classroom use is granted without fee provided that copies are not made or distributed how to make sure that AR applications are accessible to everyone
ofo nr tp hr eo ff rt so tr
p
c ao gm e.m Ce or pc yia rl
i
ga hd tv sa fn ot ra tg he
i
ra dn -d
p
ath rta yt cc oo mpi pes
o
nb ee nar
t
st h oi fs
t
n ho ist i wce
o
ra kn d
m
t uh se
t
f bu el l
h
c oi nta ot rio edn
.
as they are emerging [2, 42].
For all other uses, contact the owner/author(s). Applications are being developed using AR for a wide variety of
ASSETS ’20, October 26–28, 2020, Virtual Event, Greece innovative and interesting reasons. An alternative approach could
©
AC
2 M02 0
IS
C Bo Np y 97ri 8g -h 1t
-
4h 5e 0l 3d
-
7b 1y
0
t 3h -e
2
/o 2w 0/n 1e 0r
.
/author(s).
be to not use AR for content that needs to be accessible, or to create
https://doi.org/10.1145/3373625.3417006
ASSETS ’20, October 26–28, 2020, Virtual Event, Greece Jaylin Herskovitz, Jason Wu, Samuel White, Amy Pavel, Gabriel Reyes, Anhong Guo, and Jefrey P. Bigham
separate versions of the content that does not use AR. However,
as has been shown again and again in the history of accessible
technologies, separate more accessible versions of software rarely
provide an equal experience. Instead, content is slower to arrive and
becomes out of date, and functionality is limited and not maintained
to the same degree as the application it is intended to parallel [68]. In
this paper, we explore how we might make existing AR applications
natively accessible so that everyone can beneft from them.
Understanding the design space of tasks in AR is an important
frst step to designing accessible alternatives. Accordingly, we frst
collected and thematically grouped the interactions required to
use 105 existing mobile AR applications. We chose to focus on
smartphone AR applications in this work because the smartphone
platform is nearly ubiquitous, and smartphone AR is quickly fnd-
ing its way into a number of important applications. From this Figure 1: Left: IKEA Place [28] allows users to view furniture
analysis, we present a design space for existing constituent tasks in AR. Right: Statue of Liberty AR [60] displays historical
found in AR applications, which we believe can serve as a roadmap facts along with to-scale models of the Statue of Liberty.
for future research and development in making these applications
accessible. While we focus on AR applications for mobile phones,
we believe our results can inform the design of AR applications for
the UIKit Accessibility Guidelines [4]. These best practices are re-
a variety of smartphone and head-mounted platforms, which all are
lated to guidelines such as the Web Content Accessibility Guidelines
facing the same challenge of how to make themselves accessible.
(WCAG) [66], which provide guidance on how to provide the seman-
We identifed fve key categories of tasks within AR applications
tic information necessary to make the content of a user interface
and described them along with examples and considerations that
accessible. Many mobile applications across diferent platforms are
afect accessibility for each.
insufciently annotated to be fully accessible [54], which has led
Using this design space, we then selected three of the most
to various attempts to improve their accessibility after the fact, e.g.,
common tasks and designed prototypes of accessible alternatives
through run-time repair [70]. Additionally, as smartphone hard-
for each to serve as design probes: (i) scanning the environment
ware has changed, the assistive technologies that operate on devices
to initialize the AR world model, (ii) placing virtual objects in the
have adapted, for example, with new approaches for enabling a
space, and (iii) locating and exploring virtual objects in the space.
person who is blind to use a touchscreen interface [30].
We also created two full AR apps combining these components that
Relatively little work has considered how to make the AR appli-
were meant to mimic common AR use cases: a retail app designed
cations that are quickly becoming popular more broadly accessible,
to allow the user to explore how furniture might ft into the context
aside from guidelines on the use of color and audio in AR for users
of their own environment, and an educational app in which users
with low vision or hearing impairments [37]. Prior work has consid-
could explore the solar system. We then conducted a user study
ered how to make VR accessible for people with visual impairments
with 10 blind participants to gather feedback about each design.
via audio [67] and haptic feedback [32, 72]. For example, SeeingVR
The main contributions of this work are:
introduces methods for making VR accessible to low-vision users,
(1) A taxonomy of constituent tasks found in 105 existing AR and largely takes inspiration from prior systems for providing ac-
applications, which provides a roadmap for future research cess to the digital and physical worlds, e.g., through adjustments to
in making AR applications accessible. visual content and through various automated methods for describ-
(2) Five exemplar prototypes of non-visual alternatives to com- ing or enhancing the virtual content at runtime [18, 72, 76]. Other
mon AR tasks, and two prototypes that combine them into work looks to leverage the advantages of technologies, like the
realistic full AR applications that are non-visually accessible. white cane, with which some people with disabilities are already
(3) A study in which we used our design probes to explore how familiar in order navigate virtual content [58, 64, 71]. Prior work
10 blind participants interacted with AR on mobile devices. has also considered how to make other 3D applications, such as
games [3, 19] or CAD software [56, 57], accessible through similar
2 BACKGROUND techniques. In our work, we instead consider AR, which difers in
Mixed-reality systems exist on a continuum between the real and that it is a combination of real and virtual content. We also identify
virtual world [40, 41]. Our work focuses on augmented reality (AR), common constituent tasks in AR applications and provide patterns
which introduces virtual elements into the real world. Our work for how those might be made accessible, which we hope will be
builds from work on (i) 3D and mobile applications accessibility, (ii) useful for developers.
camera-based applications for making the world more accessible,
and (iii) defning the capabilities of mixed-reality systems. 2.2 AR for Making the World More Accessible
AR has also been used to improve accessibility across a wide variety
2.1 Making Applications Accessible of domains, such as for visual assistance for people with low vision
Mobile applications, like their desktop and web analogues, can [21, 73, 75] and color blindness [62], assistance for people with
be made accessible by following application guidelines, such as cognitive impairments [31], and coaching for rehabilitation [11]. As
Making Mobile Augmented Reality Applications Accessible ASSETS ’20, October 26–28, 2020, Virtual Event, Greece
Figure 2: Icons for the 105 AR apps that we analyzed, organized by category.
examples, CueSee [77] and ForeSee [76] enable visual identifcation 3 DESIGN SPACE OF AR INTERACTIONS
and search for items of interest, respectively. AR has also been A precursor to creating accessible alternatives to current AR in-
used to help people with low vision better navigate on stairs [74]. teractions is understanding what tasks are currently common and
In general, these systems seek to augment users’ perception and necessary in AR applications. Such an understanding is also a pre-
cognitive abilities via visual overlays in their real environment. Few requisite for usability evaluation [47] and modeling [13]. This is
of these systems have used the smartphone platform, opting instead also implicitly related to how assistive technologies have been de-
for a head-worn form factor or augmenting the user’s environment. veloped to work on graphical user interfaces (GUIs) – tasks are
Technologies associated with AR have recently been used in a identifed frst and then accessible alternatives to them have been
variety of systems intended to improve access to the world using invented. For instance, “drag and drop” was introduced, and then
smartphones. A number of systems have been developed to help an accessible way to perform the same function was developed and
blind people take better photos, generally by using automated ap- introduced. While common tasks are largely known and repeated
proaches to assist in aiming the camera [29, 36, 65]. VizLens uses a in GUIs, interactions in AR are much less explored, and AR afords
combination of computer vision and crowdsourcing to recognize even greater fexibility on what interactions are possible.
and guide a blind user through using an inaccessible physical in- Our goal was to discover repeated constituent tasks across dif-
terface [22, 23]. Cursor-based interactions assist blind people to ferent applications so that we could then develop approaches for
attend to and interact with physical objects in complex visual scenes making them accessible, thus providing developers useful patterns
[24]. Audio-based AR systems such as Microsoft’s Soundscape [39], that they could follow to make their own applications accessible.
Blindsquare [7], and NavCog [1, 55] act as navigation aids which We performed an analysis of the functionality and design of exist-
provide information on points of interest and non-visual landmarks. ing mobile AR apps, and from this, we present a description of the
These systems do not introduce visual augmentations but rather design space of AR apps and a set of common constituent tasks.
augment the environment with audio cues that help users perform
tasks of interest in the real world. This work helped to inform the
3.1 Dataset
design of our accessible AR prototypes.
Our dataset consists of all AR apps that were displayed on the ‘AR’
category page of Apple’s App Store for iPhone over a three month
2.3 The Space of Mixed-Reality Systems period (June to September 2019). Two examples are shown in Figure
Existing taxonomies of AR/VR systems [40, 59] mainly describe 1. We removed apps that did not have apparent AR content, as well
systems in terms of what capabilities the display hardware afords. as one with location-specifc content we could not access, leaving us
Other analyses of immersive software are largely focused on spe- with 105 apps. Of these apps, 83 (79%) used AR as the main feature
cifc domain areas, such as industrial manufacturing [12], games of the app, while the remaining 22 (21%) used AR as a secondary
[14], automotive applications [69], or medicine [20]; specifc func- or supporting feature. The apps that we evaluated were spread
tionality, such as visualizing relationships between physical and over a variety of categories in the App Store, which we further
virtual information [43]; or specifc interactions, such as gestures, condensed into the following fve groups: 39% Entertainment, 31%
gaze [26], or using a tablet alongside a VR headset [17, 61]. The Education, 16% Retail, 9% Utility, and 5% Other. An overview is
results of an extensive AR gesture elicitation study demonstrate shown in Figure 2. Our analysis focused on the iOS platform, as
that the space for gestures in AR is quite large [52]. This prior work many people with disabilities are iPhone users, including those who
primarily focused on the form of gestures and how participants are blind and those who use switch control. AR support on other
thought they should map onto a pre-defned set of tasks (e.g., select, popular platforms, such as Android, is similar, although hardware is
open, delete-x). We instead consider what “tasks” need to be done more varied. Given that many of the apps we analyzed are available
to fully make use of common AR applications. on both platforms, we expect our fndings to generalize.
ASSETS ’20, October 26–28, 2020, Virtual Event, Greece Jaylin Herskovitz, Jason Wu, Samuel White, Amy Pavel, Gabriel Reyes, Anhong Guo, and Jefrey P. Bigham
Figure 3: A summary of the identifed ‘building-block’ tasks needed to interact with AR apps, and the percentage of apps that
contained each task, with higher-level categories identifed.
3.2 Methods 3.3.2 Establishing Physical/Virtual Correspondence. A necessary
Three members of the research team analyzed the apps in our class of tasks within AR involves creating a relationship between
dataset using thematic analysis [10]; all had experiences with AR/VR the physical space and virtual content in order to create a basis
systems, accessibility, and/or qualitative coding. We followed the for positioning virtual content. Current smartphone AR systems
six phases that Braun and Clarke described [10], treating screen- usually use an RGB camera to identify visual features and detect
shots and textual descriptions of app functionality as data items. physical surfaces in the space, and require users to pan the camera
Each researcher performed the frst two phases individually, ana- slowly to do so successfully. 15.2% of apps in our dataset did not
lyzing a subset of 15 apps and performing an open coding of their require any of these tasks, and instead placed content relative to
observations. The remaining phases were completed as a group; the position of the camera only.
the research team iteratively adjusted the codes using this pro- 80% of the apps in our dataset asked users to perform a gen-
cess until sufcient agreement was reached. We used Randolph’s eral scan of their space to detect surfaces. In order to perform this
free-marginal multirater kappa [53] to measure agreement. We con- scan successfully, users need to frst be aware of how the software
sidered a code fnalized when we reached a kappa value of 0.7 or expects the phone to move, which is usually explained through ani-
higher. An overview of the resulting codes is shown in Figure 3, mations. Users also need to know if the environment has sufcient
with frequency of occurrence by app category shown in Figure 4. lighting and sufcient visual features for detection, something that
modern systems can notify the user of. There are also application
3.3 ‘Building-Block’ Tasks in AR
dependent factors that users need to be made aware of, for example,
how large of a surface the app requires, or if a specifc type of
3.3.1 Observing AR Content. The type of visual AR that exists on surface is required (i.e., table or foor).
modern smartphones gives the appearance of three dimensional,
Establishing virtual/physical relationships can also be more ex-
virtual objects that have been placed in the physical environment.
plicit. 10.5% of the apps in our dataset asked users to scan a specifc
Thus, in order to perceive all aspects of the virtual content, users
object or image to annotate with virtual content. For example, Tonic
are required to ‘look around’ the space, using the phone’s camera
[15] asks the user to scan a piano to show chord information, and
as a lens by which to view the virtual world. This serves multiple
Waypoint EDU [38] will recognize pre-printed posters placed in a
purposes, just as visual perception does, including perceiving in-
classroom as triggers to display educational content.
formation about a single virtual object (its size, shape, color, style,
Additionally, in some cases where the app is unable to recognize
etc.), as well as information about the relationships between objects.
a point through other means, the user is asked to specifcally label
Relational information can be between a virtual object and its phys-
a point or an area by placing a virtual marker in the space (11.5%
ical surroundings (e.g., to check if a virtual product matches one’s
home) or between multiple virtual objects (e.g., to compare the size
of two virtual products), and includes how virtual objects may be
similar or diferent in appearance and how they are arranged in
the physical space. This task also serves to enable users to discover
new content and functionality. For example, in Forensic Detective
[46] the user must search for hidden clues around their room and
move the phone close to virtual content in order to interact with it.
All of the information above needs to be conveyed in an alter-
native form for visually impaired users. As blind users typically
familiarize themselves with physical spaces through their sense of
touch, this task is difcult to replicate at the same level of fdelity
without additional haptic devices. For example, the Canetroller [71]
is a device that simulates white cane interactions and provides
physical resistance and vibrotactile feedback for objects in VR. De-
signing accessible interactions which can convey this information Figure 4: A breakdown of each constituent task we identifed
using commodity smartphone hardware presents a challenge. and how frequently it appeared in each app category.
Making Mobile Augmented Reality Applications Accessible ASSETS ’20, October 26–28, 2020, Virtual Event, Greece
of apps). For example, in TapMeasure [48] the user is asked to mark 4 PROTOTYPES OF ACCESSIBLE AR
the corners of their room with small fags in order to measure the The prior section introduced the taxonomy of constituent tasks
length of each wall, and in Tonic [15], if a piano is not automatically that we identifed from existing AR applications. In this section,
detected, the user is asked to mark the frst and last keys with we explore how such applications could be made accessible. We
virtual dots in order to align content. This interaction represents a believe that creating truly accessible alternatives to many difer-
failure mode of AR tracking that would be much more difcult for a ent constituent tasks and the experiences that they embody is a
visually impaired user to recover from, as existing implementations long-term research task. Instead of attempting to solve the whole
require precise camera alignment to mark specifc points. problem, our goal was instead to (i) demonstrate that common AR
3.3.3 Creating Virtual Content. 70.5% of the apps in our dataset tasks and applications can be made accessible, and (ii) create pro-
allowed users to place virtual content in some way, while the rest
totype accessible AR applications for use in the studies with blind
placed content automatically for the user. We identifed three com-
participants that conclude this paper.
mon modes of placement: (1) by indicating a specifc point at which
We frst present foundational work for exposing virtual objects
to center the object (62% of apps), (2) by indicating a series of points
displayed in AR to the users of accessibility services, such as screen
to form a polygon to fll with content (4% of apps), or (3) by draw-
reader and switch control users. While a number of AR applications
ing free-form lines by dragging their fnger on the screen (10.5% of
exist across mobile platforms, we developed our prototypes for iOS
apps). In order to place virtual content, users need to evaluate both
and specifcally targeted VoiceOver use.
possible locations (i.e., locations that would be able to accommodate
We then present fve constituent task prototypes that we devel-
the size of the item), and appropriate locations (i.e., locations that
oped to illustrate how AR applications might be made accessible:
make reasonable sense for the item, for example, a virtual chair one for scanning surfaces (from the set Establishing Physical/Virtual
should not be placed on a table). An app should provide sufcient
Correspondence), two for placing virtual objects on surfaces (from
information for users to make this evaluation. the set Creating Virtual Content), and two for locating virtual ob-
jects in the space (from the set Observing AR Content). In each of
3.3.4 Transforming Virtual Content. Traditional 3D manipulations the two latter cases, we drew from prior work to create contrasting
are also extremely common in AR apps, with 68% of apps in our experiences: one experience attempted to directly make the existing
dataset allowing at least one of the following forms: (1) editing posi- experience accessible, and the other experience attempted to assist
tion (50.5%), (2) editing orientation (51.5%), (3) editing scale (44.8%), the user in performing the task in an alternative way.
or (4) deletion (16.2%). In cases where the position or orientation The tasks we prototyped were chosen for their ubiquity, as well
of an object could be edited, this was usually constrained to two as their ability to combine to form realistic, full AR applications. We
dimensional motion along a surface or rotation around one axis, present two such full apps, in the domains of retail and education.
as described in Apple’s design guidelines for AR apps [5], as more These are common applications for smartphone AR, at 36% and
complex manipulations are difcult with touch controls. 19% of our dataset respectively. Retail was chosen over the slightly
The goal of each manipulation is highly dependent on the con- more common entertainment category because of the higher level
text: one may rotate a piece of furniture so that it fts in their room of precision needed to place and evaluate virtual products.
(crucial), rotate an educational model to see what it looks like from
another angle (optional, but they may learn additional information),
4.1 A Foundation for Accessible AR
or rotate a 3D emoji because they like the way it looks (purely
cosmetic). Additionally, while performing a manipulation, users
The frst step in making AR content accessible is to make acces-
often need to observe other parts of the scene at the same time.
sibility services aware of virtual content and allow developers to
For example, when resizing an object, sighted users can compare
assign metadata to it. To this end, we added a bridge that exposes
its size to other virtual and physical objects in the area, as well as
the underlying structure of the AR scene to VoiceOver, making
to the size of the physical space in general, in order to determine
each side of a 3D object’s bounding box the same as any other 2D
what is appropriate. Alternative mechanisms for users with visual
UI element on the screen, similar to the touch cursor mode in prior
impairments must also convey this information.
work [24]. Our approach was implemented on the SceneKit objects
that ARKit uses to add virtual objects by adding them to the view
3.3.5 Activating Virtual Content. Users are often required to select hierarchy that accessibility services use to traverse applications.
a specifc object in the scene, which may trigger additional efects. As a result, our approach is general and can apply to many exist-
This can be simple, such as selecting an object so that it becomes ing applications that use this toolkit for creating AR experiences.
editable or additional text information about the object is displayed Transforming 3D content into 2D targets provides a foundational
(35.2% of apps), or can include more complex efects such as sound awareness of what content is in an AR scene and functionality for
efects, animations, or physics-based motion of an object (48.6% of selecting objects akin to 2D buttons (Figure 5A).
apps). This category is by far the most diverse. Even the distinction We also implemented a “freeze” feature that captures a stable
of complex efects can range from localized animations which may view of the AR content and the physical world it is overlayed onto,
just need audio descriptions, to game mechanisms, such as swiping enabling users to interact with the frozen view without worrying
a fnger across the screen to toss a virtual basketball into a net about moving the device. We found this to be an important usability
as in NBA AR Basketball [45]. In general, an app should provide feature because otherwise blind users of AR needed to keep their
users with sufcient information to determine what objects can be mobile devices positioned such that they always point at the virtual
activated, their current state, and the resulting efect on the scene. objects of interest. Aiming cameras non-visually is, in general,
ASSETS ’20, October 26–28, 2020, Virtual Event, Greece Jaylin Herskovitz, Jason Wu, Samuel White, Amy Pavel, Gabriel Reyes, Anhong Guo, and Jefrey P. Bigham
Figure 5: Our approaches to making AR apps accessible: the foundational level of accessibility added to VoiceOver (A), fve
prototypes for three constituent tasks (B-F), and two apps which combine the tasks to create full AR experiences (G-H).
known to be a hard problem [29], and we found that the difculty virtual objects, regardless of the device’s orientation (Figure 5C).
is only magnifed when the position must be held stable while also As users walk around the room, the object is placed on the foor at
interacting with the mobile device. Our “freeze" feature is toggled their feet, and moves as they move. If they hold the phone above
using the two-fnger double-tap (“magic tap”) in VoiceOver. a table or other surface, the object will move to that surface and
continue to follow the phone’s position; the user will receive a
4.2 Scanning the Physical Space verbal notifcation when this occurs. As the user walks around
Our scanning prototype adds a set of accessibility notifcations to
their space with the object, they also receive notifcations when
increase awareness of the current scan progress (Figure 5B). When
the object does not ft in the area they are standing, for example,
the user scans a new surface, the app announces this and gives the
if it is too close to a wall, too close to another virtual object, or
type (horizontal or vertical). As the user scans more area, the app
too large to ft on or underneath a table. When the user is ready to
will periodically announce the number of surfaces and total area
fnalize placement, they press a ‘confrm’ button to drop the object
that has been detected. Finally, if the user has not scanned any new
in its current location. As the location of the virtual object is tied
area in the past fve seconds, the app will instruct the user to move
closely to the user’s physical position, this could help give users a
to a new area to scan.
sense of where virtual objects are located. However, it also requires
somewhat precise movement from the user, as well as some degree
4.3 Placing Virtual Objects
of awareness of where the phone is generally pointing, as this is
used to orient the object so that it appears to face the user (though
4.3.1 Version A: Camera-based Placement. Camera-based place- it is not used to position the object).
ment uses the position of the phone to determine the position of
Making Mobile Augmented Reality Applications Accessible ASSETS ’20, October 26–28, 2020, Virtual Event, Greece
4.3.2 Version B: Guided Placement. Guided placement generates a of freely exploring the room. For example, when using camera-
series of candidate positions for an object based on what surfaces based search, the user could perform a sweeping scan to quickly
have been detected, then asks the user a series of questions about get a sense of what objects are around them, which would not be
where they would like the object to be placed in order to determine possible with this method.
the best position and orientation (Figure 5D). Users are frst asked This method is also similar to the guidance mode in VizLens,
if they would like to place an object on the foor or on a table. Based which gives users directions to reach buttons on an inaccessible
on their selection, they are then provided with another series of physical interface, while our camera-based search is similar to the
options: if table was selected, they are asked to choose between the feedback mode in VizLens, which announces which button a user
center of the table or an edge of the table; if the foor is selected, is near [22]. The evaluation of VizLens found that users preferred
they can select between the center of the foor, an edge, or a corner guidance when they were not yet familiar with an interface’s layout,
of the room. If applicable, users are then asked to face the edge, wall, and preferred the feedback mode when they were. Our two search
or corner that they would like the object to be placed against. For methods could be applied in a similar manner.
example, if the user would like to place a chair against a wall, the
object can then be positioned so that it is exactly against the wall and
is rotated correctly. This constrains placement as compared to the 4.5 Furniture App
location-based method, as only a subset of positions in the room are We created an AR furniture shopping app meant to mimic existing
detected as candidates for an object. However, this option requires retail apps that make use of AR to let users see products within
less work for the user in determining where an object would ft (as the context of their space, such as IKEA Place [28], Overstock [49],
this is determined by the app) and how to best rotate it in the space. Houzz [27], and Target [63]. Furniture placement is a compelling
Thus, it is likely better suited for cases where the exact placement application for smartphone AR, requiring constant comparison
of a virtual object may not be as important to the user. Additionally, between virtual and physical content in order to determine if an
content was placed on the frst table scanned as current mobile item fts within a room in terms of size and style. This requires
AR can detect only horizontal and vertical planes; future scanning accurate positioning and realistic content.
mechanisms could enable other targets to be presented as options. In our app, users can select from a list of items, and then use the
camera-based placement method for placing the object in the room
(Figure 5G). Similarly, they receive notifcations when an object
4.4 Finding Virtual Objects
does not ft in its intended location (i.e., under a table, too close to a
4.4.1 Version A: Camera-based Search. Using camera-based search, wall, or conficting with another virtual object). The camera-based
users scan the camera around their space to fnd objects, similar to search method is used to provide users awareness of where existing
the window cursor mode in prior work [24]. When the user points objects are. When the user stands close enough to an object, they
the camera at a virtual object, they receive a verbal notifcation are given options to edit its position or delete the object. While our
stating the name of the object and how far away it is (e.g., “Found prototype covers most of the basic functionality included in this
chair 0.5 meters away”), and will also feel a haptic vibration from type of app, more complex functionalities, such as understanding
the phone (Figure 5E). When they move the camera away from the the spatial relationships between objects, are not explicitly included
object they receive a similar notifcation. Using this information, and depend on the user’s mental model of the space.
the user can walk in the direction that the camera is pointing to
locate a found object. When they are close to an object (within a
certain threshold), there is an additional notifcation; in this way 4.6 Solar System App
the user can get a sense of the locations of virtual objects. Although We also created an educational app meant to mimic existing AR
this requires the user to point the camera at the location of a virtual apps aimed at elementary school students, such as ARcheology [35]
object, it could allow for a more free-form exploration of the space, or Plantale [16]. Such apps are also compelling; they are usually
which may be useful in some applications. more engaging or interesting for students to interact with, while
also sometimes providing secondary information through AR, such
4.4.2 Version B: Guided Search. Guided search presents users with as real-world scale or layout of content. However, these apps also
a list of all objects in the space around them, sorted by how close usually include content that the user may not already have a frame
they are to the user’s current position. When the user selects an of reference for (unlike furniture), and it is less realistic in that it
object from the list, the phone then issues verbal directions which may be stylized and/or at a diferent scale than expected.
update every three seconds as the user moves (Figure 5F). For Our app presents some basic information about the solar system
example, the user might hear the following series of instructions: (Figure 5H). First, the user is instructed to face an open area of
“The chair is 1 meter in front of you”, “The chair is 0.5 meters in the room to place a model of the planets. Guided placement is
front of you”, and “The chair is 0.2 meters to the left”. The directions used to place the model in front of the user, though options are
“forward”, “backward”, “left”, and “right” are approximations chosen not provided directly to the user as in this case there is only one
based on which is nearest to the object, so the path to the object object that needs to be placed. The user then navigates through
is not always the most direct path possible, but the position is two panels of information about the solar system, and an animation
eventually reached. Users receive a notifcation when they are close is played which resizes each planet so that they are equally sized,
to an object, as before. This option could be easier to use than the which is described to the user. The user can then select planets
camera-based search, but in some situations could come at the cost using camera-based search to learn more about them.
ASSETS ’20, October 26–28, 2020, Virtual Event, Greece Jaylin Herskovitz, Jason Wu, Samuel White, Amy Pavel, Gabriel Reyes, Anhong Guo, and Jefrey P. Bigham
5 USER STUDY Task 1: Participants were asked to scan at least four surfaces
The goal of the user study was to better understand how our pro- (with at least one vertical surface) totaling fve square meters in the
totypes performed both as standalone methods of interaction, and study room. The app notifed the user when this was met.
when integrated into full AR apps. Overall, we sought to under- Task 2: Participants were asked to respond to a series of fve
stand what the strengths and limitations for each prototype were, prompts by placing a virtual object in the room. Each prompt de-
which methods were better suited to diferent contexts, and how scribed a location (e.g., “Place the chair in front of the desk”).
users perceived virtual content. Task 3: Participants were asked to locate fve objects that were
randomly placed in the room (two on the table, three on the foor).
Task 4: Participants were asked to use our furniture app to
5.1 Participants and Apparatus choose a few pieces of furniture that ft in the room, and arrange
We recruited 10 participants (3 male, 7 female). Among them, 8 them. The choices were two chairs, a couch, and a cofee table.
were blind and 2 had low vision; 1 was in the age range of 30-39, Task 5: Participants were asked to use our solar system app to
2 were in the age range of 50-59, 4 were in the age range of 60-69, learn some basic scientifc facts. They were instructed to follow a
and 3 were in the age range of 70-79. All participants were users relatively guided-narration and explore content as they saw ft.
who rely on screen readers in order to access their devices, and all
had experience using a smartphone for at least 3 years (and at most 6 RESULTS
10 years). All of the participants were iPhone users. All participants 6.1 Task 1: Scanning
reported little to no prior knowledge of AR or VR.
On average, participants took 39.1 seconds (SD = 21.3 seconds) to
We implemented the seven prototypes as described in the pre-
complete the scanning task. All participants agreed that this task
vious section and installed each as a separate app on an iPhone 8
was easy to perform, though our study was run in nearly ideal-
Plus. The study took place in a well-lit ofce room, approximately
conditions (well-lit area, surfaces with many feature points) so
10 feet by 10 feet in size, that contained a table, wall shelves, and a
tracking was not difcult to establish. Although fguring out how to
whiteboard. The center of the room was open.
hold and move the device was an issue for some participants (P4, P5,
and P9 required additional guidance from the study administrator
5.2 Procedure on how to point the camera), others were able to adjust as they
Participants were frst asked a series of demographic and back-
received feedback from the app:
ground questions. Participants were provided the opportunity to “Once I was doing it and getting the feedback, then I
familiarize themselves with the study room, and also to adjust the thought, ‘Well, I’m doing it right.’ Because it said you
VoiceOver settings on the provided device. Next, participants were got one surface, so I just kept going.” (P2)
given a short description of typical smartphone AR usage, and given Though participants generally thought that the verbal updates
an opportunity to ask any questions on this topic. were helpful in providing awareness of the current scan progress,
Participants were then asked to use our prototypes to complete they agreed that more semantic information about what physical
a series of fve tasks, as described below. After using each app, objects are being scanned would be useful for additional guidance.
participants were asked to rate their agreement with a series of
“I didn’t care about the horizontal and vertical planes,
statements on a scale of one to seven (from strongly disagree to because I didn’t know how relevant that was.” (P5)
strongly agree): “This task was mentally demanding”, “This task
“I noticed when I pointed at the table it said ’horizontal’
was physically demanding”, “I feel it is easy to use this app”, “I feel
and ’vertical’ when I pointed at the wall, but it didn’t
very confdent using this app”, and “I had a sense of what virtual
really tell me how far the table was from me, or how
objects were in the environment around me and where they were
far the wall was from me. If I couldn’t see at all I might
located”. Responses are summarized in Figure 6. After rating each
be nervous about how far I could move.” (P1)
statement, participants were asked to describe any challenges they
encountered and the most helpful feature of the app, and were
6.2 Task 2: Placing Objects
given a chance to give open-ended feedback.
6.2.1 Version A: Camera-based Placement. On average, this task
Each session took between 1.5 and 2 hours, and participants were
took participants 4.3 minutes (SD = 2.6 minutes) to complete, and
compensated with $50 each. The sessions were video recorded, and
each object took on average 27.4 seconds (SD = 23.5 seconds) to
timestamps of app launch, termination, and certain actions were
place. Participants commented positively on the clear connection
recorded and used for further analysis.
between a virtual object’s position and their own physical location:
“All I had to do was to move [to the location] and place
5.3 Tasks it. I knew when something wouldn’t ft, and I backed
We designed the following tasks based on our analysis of existing of and placed it.” (P6)
AR apps. All of the tasks were completed in the same order. For Even so, fnding the intended physical location was challenging
tasks 2 and 3, which participants completed twice, the order in for some, and additional guidance about where physical objects
which they used each prototype was randomized such that half of are located would be helpful. For example, P7 commented that
the participants always used the ‘A’ version frst and the other half this system required them to keep the layout of the room in mind
always used the ‘B’ version frst. more than they normally would, and P8 said “Placing the object
Making Mobile Augmented Reality Applications Accessible ASSETS ’20, October 26–28, 2020, Virtual Event, Greece
Figure 6: Responses from Likert scale questions for all tasks. Participants were asked to rate the extent to which they agreed
with the statements on the left. Each bar shows the number of participants who gave a specifc response.
is easy, once I know where I was at.” Additionally, all participants fndings on user’s guidance preferences when presented with an
commented that the notifcations that an object did or did not ft in unfamiliar layout [22].
their current location were helpful:
“It would give me more feedback than [guided place- 6.3.1 Version A: Camera-based Search. On average, this task took
ment], like if it didn’t ft, you could just move to another
participants 5.2 minutes (SD = 1.3 minutes) to complete. Each object
corner. As long as it was saying that, I could just fnd
took on average 68 seconds (SD = 15.6 seconds) to fnd. Of all the
another place. That feedback was really good.” (P2) tasks, this was the most challenging, as evidenced by participants’
questionnaire responses (Figure 6). This is caused by needing to
6.2.2 Version B: Guided Placement. On average, this task took par- scan the room with the camera when the targets were unknown,
ticipants 6 minutes (SD = 1.5 minutes) to complete. Each object took which was mentioned by all participants.
on average 48.4 seconds (SD = 10.5 seconds) to place. Participants
noted that the options were easy to navigate, but lacked precision: “If I knew what I was looking for it would have been
a lot easier. For the purple vase, I knew it was on the
“I think that one where I had to put the chair in front of
table... If I had known that there were two things on the
the table, but there were only three options, there wasn’t
table to start out with, then when I found one, I might
enough precision to do what I wanted to do. It wasn’t
have looked for the other.” (P5)
difcult to understand, but there was an imprecision in
the placement options.” (P1)
6.3.2 Version B: Guided Search. On average, this task took partici-
Participants also noted that there was a lack of semantic and con-
pants 4.1 minutes (SD = 1.9 minutes) to complete. Each object took
textual information in the options.
on average 38.6 seconds (SD = 23.3 seconds) to fnd. Participants
”It was step by step, that helped. I don’t know if it’s generally appreciated the directions, and liked that they updated
always necessary to break down all of those little steps... continuously because it could help correct if they overshot the
If it would ask you where would you place the chair, it distance (P10). However, the time interval between directional up-
would always be on the foor, so it seemed like some of dates should be customizable, as some participants noted they felt
those steps were needless.” (P3) like they were stuck waiting for the app to tell them where to go.
P4 noted: “I don’t know if it was me not moving it enough, unless it
6.3 Task 3: Finding Objects should be more sensitive. It didn’t want to respond right away.” Others
Participants preferred the guided search method for fnding un- suggested adding other multi-modal continuous feedback options
known objects. This was evidenced in the Likert scale responses to decrease this response time further, for example, P5 suggested a
as well as participant comments, and is consistent with previous tone that would change as you got closer or further to the object.
ASSETS ’20, October 26–28, 2020, Virtual Event, Greece Jaylin Herskovitz, Jason Wu, Samuel White, Amy Pavel, Gabriel Reyes, Anhong Guo, and Jefrey P. Bigham
6.4 Task 4: Furniture Shopping 7 DISCUSSION
On average, participants spent 5.8 minutes (SD = 1.5 minutes) using Our prototypes allowed users to successfully interact with AR
this app. Overall participants had positive impressions of the app, content over a series of tasks. In this section, we discuss overarching
noting that it took a while to get used to, but was easy to use patterns that we observed throughout each task regarding users’
after that. Participants generally saw how such an AR app could perceptions of virtual content and additional factors that could
be useful, but that in its current state, not enough information is afect usability.
provided to base purchasing decisions on.
“I liked it, it was cool to be able to place things like that. 7.1 Notions of ‘Virtual’ and ‘Real’
I guess this would save you from having to measure the
Participants interpreted virtual content in various ways, depending
furniture, and going to the store and doing that. But
on their spatial understanding ability and previous levels of vision.
sure, it can tell me that the furniture fts in the room,
Some participants noted that as blind people already maintain a
but it doesn’t tell me what it looks like once it’s placed.
mental map of an indoor space to some degree, there may be less
The cofee table would ft in front of the couch, but how
of a distinction between virtual and physical objects in memory:
do I know which way it would ft? How do I know the
long side of the cofee table is parallel to the long side
“Being visually impaired, and being able to see before,
of the couch?” (P9)
now when you have me walk around a room, I had
Regarding the constituent tasks used in this app, participants
almost exactly in my mind the way the room looks. It’s
had fewer complaints about the camera-based search method being
like putting virtual memory in my mind, and to be able
used, presumably because they had a base level of awareness of the
to remember [virtual objects], it’s almost working in a
locations of objects because they placed them. This is similar to the
similar way to actual reality. So it’s kinda neat trying
fndings from VizLens [22] suggesting that if users are not familiar to put those two together.” (P3)
with layout, direct guidance is preferred.
However, switching between diferent tasks was initially confus- Others thought that understanding the virtual environment was
ing to some. For example, our app selected an object when the user much more difcult because the existence of virtual objects created
stood over it for more than two seconds, as in the object search a diference in how the space around them was experienced. P9
tasks. As users often walked throughout the space to get a sense of commented: “If I use AR more often, I would come out of my box that
the virtual content, this resulted in unintentional selection. P3 noted I have to touch everything. It’s a whole new mindset.” For users who
at the end of the session “I was just getting used to it, knowing that I have never had vision, this could be much more difcult, as physical
had to step away from objects [to deselect].” Better consideration of landmarks are the primary way that they understand spatial layout.
how to integrate our task designs is needed. P7 compared the experience to being in an empty room:
Overall, while our prototype is functional and provides some
utility, it is missing some of the more intricate information about “For me, if I’m in a room, and I have a sense of how big
objects and the environment that would be needed to be fully usable. the room is, I can get a sense of how furniture is gen-
erally laid out; and it’s easier to get that concept when
6.5 Task 5: Educational App the furniture is there. When I was moving in to my last
condo, and it was empty, I couldn’t conceptualize how
On average, participants spent 8.6 minutes (SD = 2.3 minutes) using
everything could go the way I wanted it. For someone
this app. As previously mentioned, this app is somewhat unique
who’s seeing and has gone blind, they have those spatial
as the layout of the planets was automatically generated, and the
concepts, it’ll be diferent for someone like me who has
users had to fnd areas of interest. We selected the camera search
never seen.” (P7)
method for this app with the aim of conveying the exploratory
nature of similar educational apps, but it was ill-suited given the
unfamiliarity with the layout of the content. Although participants 7.2 Richer Contextual Descriptions
could eventually use the haptic and voice feedback to fgure out the Our prototypes focused on providing the user with an awareness
arrangement of the planets, it would be less mentally demanding of what virtual content was in the environment and the means to
to provide a richer description at the start. interact with it. Given the variations in how participants perceived
Some participants were unsure of how AR could be useful in this virtual content, it would likely require less mental efort from the
case at all, for example, P10 commented: “As a blind person, it would user if future AR applications began to bridge the gap between
be much faster to read about them.” Multiple participants noted that physical and virtual further. Participants made many comments to
this task would be much more difcult for children, as they would this end, motivated by both task performance and safety.
be unable to use prior knowledge about the arrangement of the Many participants mentioned wanting an awareness of the phys-
planets to navigate the app. P2 commented that tactile information ical objects that were relevant to the task at hand (i.e., were in
should be used as a precursor to virtual information: the area that they wanted to place content or could be used as a
“For a kid that can’t see, bring them something they can reference point for an object’s location). For example, P9 wanted
touch and feel to give them an idea of the setup. You to know how far they were from the walls when placing an object
still need more haptic information. Once they have the to center themselves in the room. P1 suggested combining two of
setup, they can stop and learn about each planet.” (P2) our prototypes and providing the user with directions as in guided
Making Mobile Augmented Reality Applications Accessible ASSETS ’20, October 26–28, 2020, Virtual Event, Greece
search to selected physical objects, while they were placing an ob- However, our hope is that our taxonomy of common AR tasks, as
ject with the camera-based placement method. In this manner, the well as the fndings and limitations outlined in the previous sec-
user could be guided to an appropriate area for placement. tions, can serve as a roadmap for our community to explore the
Participants also wanted search methods to be aware of physical large space of AR accessibility comprehensively.
objects for better navigation, as our prototypes simply instructed We encourage researchers in this area to take a participatory
users to take the most direct path through the space: approach to developing accessible AR. Ideally, this would take the
“I’m thinking of a situation where there’s other things
form of engaging with people with disabilities from the start as new
or other people moving around. If the camera’s watch-
AR applications or platforms are developed. The work presented
ing that, it might say go forward and put this there. If
in this paper was a reaction to inaccessible AR applications that
something runs across in front of you, is it gonna tell
have already been released. The goal of our prototypes was to
you to stop, so you don’t crash into them while you’re
demonstrate that access to visual AR is possible, which we believe
moving a chair.” (P2) our user studies demonstrated, and to show how our taxonomy
of interactions could be applied in practice. The particular design
Additionally, such approaches to help users better navigate mixed
decisions were not directly made in consultation with potential
virtual and physical environments are generally applicable and can
users and, thus, should not be seen as necessarily being the best
also be extended as AR tools and methods for accessible navigation.
alternative designs for these interactions. A challenge for future
work will be to develop AR accessibility that is not only possible
7.3 Additional Contextual Factors
but usable and desirable. This will inevitably require early and
While we were able to uncover interesting insights on the strengths continuous involvement of the target user group, e.g., people who
and weaknesses of each prototype (e.g., unfamiliar layouts made are blind, and will be difcult unless accessibility is considered from
camera search harder) and on user perceptions, some factors were the beginning of application and platform design.
not investigated in our study. For example, diferent interaction Advancing accessible interaction techniques will require not
methods may be easier to use depending on the size and complexity only additional research and design work, but also advances in
of one’s physical environment, as well as the user’s level of familiar- creating more semantic descriptions of a 3D space from a traditional
ity with the space. Additionally, the location or intended location smartphone camera system. This includes the technical work of
of a virtual object (on a physical surface, in mid-air, somewhere un- creating more robust mapping and tracking systems and generating
reachable, etc.) may also impact the usability of certain interactions. semantic labels for such maps, and also work in language processing
Prior work has shown the scale of virtual objects in AR can have in order to describe scenes [33] and summarize their content [50]
an efect on expected gestural interaction [51], and diferent inter- to users, and to understand meaningful user requests. Having a
action techniques have been used to manipulate virtual objects at richer contextual understanding of users’ environments would lead
diferent distances from the user with success [61]. Similarly, difer- not only to more accessible AR, but better and more natural AR
ent accessible interaction techniques are needed for these various experiences for everyone.
situations. While virtual objects in our study were mainly placed Mixed-reality scenarios in which virtual and physical content
on the foor and on the desk, we observed some instances of the deeply interact are especially challenging areas for future work. In
virtual object’s location infuencing the usability of the interaction some cases, it may make sense to create a virtual version of the
technique. When completing the search task with both prototypes, physical world that could then be more easily manipulated and con-
an object was occasionally placed on the far side of the desk such sumed alongside the virtual AR content. Virtual reality is easier in
that the participant had to lean over the desk slightly to select it. some ways to make accessible because the system knows (in theory)
While we assumed most locations in the room could be reached about all content in the virtual world. This would be an extension
fairly easily due to its size, this highlights the need for ergonomics of our “freeze” concept (Section 4). People with motor impairments
to be considered. could move virtually through VR to access location-based AR con-
tent, and people with visual impairments could interact with AR
8 LIMITATIONS AND FUTURE WORK content without needing to aim their cameras at a specifc point in
In accessibility, we are often in the position of playing “catch up” the real world. Thus, future work could fruitfully explore technolo-
to make new technologies possible to use. We hope that the work gies and interactions for fuidly moving between AR and VR as a
presented here will help set an agenda for creating accessible ways useful tool for supporting accessibility.
to perform the common interactions necessary in AR. In order to Finally, as demonstrated in this paper, AR is quickly being adopted
provide fully accessible experiences in this manner, continuing across a wide variety of domains, often in a form that considers the
this design work and addressing some of the limitations laid out visual experience frst. With AR and other emerging technologies,
in the previous section will be important. Our prototypes repre- we have the opportunity to consider accessibility and multi-modal
sent alternatives for only a small number of constituent tasks, and interactions from the beginning of the design process, rather than
are thus currently best seen as a starting point. For example, our as an afterthought [42]. At the same time, history suggests that sim-
prototypes did not explore animated virtual content or game mecha- ply making accessibility features possible for developers to include
nisms, which are common components present many opportunities in their applications will be insufcient, and so future work may
for future research. Our prototypes also were created for use with usefully explore how to automatically identify AR interactions like
VoiceOver, and thus may not be suitable for people who use other those demonstrated in this paper and automatically adapt them to
accessibility tools, such as Switch Control, or combinations of tools.
ASSETS ’20, October 26–28, 2020, Virtual Event, Greece Jaylin Herskovitz, Jason Wu, Samuel White, Amy Pavel, Gabriel Reyes, Anhong Guo, and Jefrey P. Bigham
be accessible, thus making AR accessibility ubiquitous. The accessi- [10] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology.
bility research community will undoubtedly be able to draw upon Qualitative research in psychology 3, 2 (2006), 77–101.
its expertise in accessibility evaluation [9] and fxing [70, 72] in
[11] J Ca rm ose bs iW
e,
ail nli da m
Su
B zu anrk ne e, MM cic Dh oa nel
o
M ugc hN
.
e 2i 0ll 0, 9D
.
a Arr uy gl
m
C eh na tr el de s R, eP ah li il ti yp GJ.
a
M mo er sr o fow r, UJa pc pq eu ri
-
other mediums to apply to accessible AR. Limb Stroke Rehabilitation. 2010 Second International Conference on Games and
Looking forward, we see great opportunity to go beyond making Virtual Worlds for Serious Applications (2009), 75–78.
[12] Sebastian Büttner, Henrik Mucha, Markus Funk, Thomas Kosch, Mario Aehnelt,
accessibility possible (necessary and important) and on to using Sebastian Robert, and Carsten Röcker. 2017. The Design Space of Augmented
AR technologies for improving accessible experiences. AR is fun- and Virtual Reality Applications for Assistive Environments in Manufacturing: A
damentally about connecting users and our devices to our world, Visual Approach. In Proceedings of the 10th International Conference on PErvasive
Technologies Related to Assistive Environments (Island of Rhodes, Greece) (PETRA
and future work that uses other modalities (e.g., audio, tactile, etc.) ’17). ACM, New York, NY, USA, 433–440. https://doi.org/10.1145/
as core AR output may open up this rich connection to a wider 3056540.3076193
audience of users. [ [1 13 4]
]
S St eu ba ar st
t
iK
a
nC Car md.
e
1 n9 t8 o3 w. sT kh i,e Ap nsy dc rh eo yl o Kg ry
e
o kf
h
h ou v,m Aa nn n-c -o Mm ap ru iete Mr i ün lt le er ra
,
c at nio dn .
J
eC nr sc KP rr ües gs e.
r .
2019. Toward a Taxonomy of Inventory Systems for Virtual Reality Games. In
9 CONCLUSION Extended Abstracts of the Annual Symposium on Computer-Human Interaction in
Play Companion Extended Abstracts (Barcelona, Spain) (CHI PLAY ’19 Extended
In this paper, we have presented a taxonomy of tasks that are Abstracts). Association for Computing Machinery, New York, NY, USA, 363–370.
used in 105 existing mobile AR applications available on the iOS https://doi.org/10.1145/3341215.3356285
[15] Coda Labs Incorporated. 2018. Tonic -AR Chord Dictionary. https://apps.
platform. We have created fve prototype interactions, and two apple.com/us/app/tonic-ar-chord-dictionary/id1427039232
accessible AR experiences, which served as both design probes and [16] Designmate (I) Pvt. Ltd. 2018. Plantale. https://apps.apple.com/us/
exemplars of accessible alternatives for common AR applications.
[17]
a Top bp i/ ap
s
l Da rn eyt ,a Jl ane / Gi ud g1 e3 n8 h9 ei6 m9 e8 r7
,
J2 u1
li an Karlbauer, Maximilian Milo, and Enrico
A study with 10 blind participants demonstrated that our accessible Rukzio. 2020. VRSketchIn: Exploring the Design Space of Pen and Tablet
interactions enabled them to use AR applications, and put forth a Interaction for 3D Sketching in Virtual Reality. In Proceedings of the 2020
CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA)
set of challenges and areas for future research for making AR fully
(CHI ’20). Association for Computing Machinery, New York, NY, USA, 1–14.
accessible. AR technologies will likely underlie a wide array of new https://doi.org/10.1145/3313831.3376628
and interesting experiences. Our work provides a path to ensuring [18] American Foundation for the Blind (AFB). 2018. Screen Readers and
Screen Magnifers: An Introduction to Computer Accessibility Software.
that this future is accessible to all. https://www.afb.org/blindness-and-low-vision/using-
technology/using-computer/part-ii-experienced-computer-
ACKNOWLEDGMENTS
user-new-0.
[19] Johnny Friberg and Dan Gärdenfors. 2004. Audio Games: New Perspectives
We thank our study participants and our reviewers for their time on Game Audio. In Proceedings of the 2004 ACM SIGCHI International Con-
and feedback. ’f 0e 4r )e .n c Ae
s
o son
c
A iad tiv oa nn c fe os
r
i n
C
oC mom pup tu it ne gr E Mn ate cr hta inin em rye
,
n Nt eT wec h Yn oo rl ko ,g y
N
Y(S
,
in Ug Sa Ap ,o r 1e 4)
8
( –A 1C 54E
.
https://doi.org/10.1145/1067343.1067361
REFERENCES
[20] Danilo Gasques Rodrigues, Ankur Jain, Steven R. Rick, Liu Shangley, Preetham
Suresh, and Nadir Weibel. 2017. Exploring Mixed Reality in Specialized Surgical
[1] Dragan Ahmetovic, Cole Gleason, Chengxiong Ruan, Kris Kitani, Hironobu Environments. In Proceedings of the 2017 CHI Conference Extended Abstracts on
Takagi, and Chieko Asakawa. 2016. NavCog: A Navigational Cognitive Assistant Human Factors in Computing Systems (Denver, Colorado, USA) (CHI EA ’17). ACM,
for the Blind. In Proceedings of the 18th International Conference on Human- New York, NY, USA, 2591–2598. https://doi.org/10.1145/3027063.
Computer Interaction with Mobile Devices and Services (Florence, Italy) (MobileHCI 3053273
’16). Association for Computing Machinery, New York, NY, USA, 90–99. https: [21] Cole Gleason, Alexander J. Fiannaca, Melanie Kneisel, Edward Cutrell, and Mered-
//doi.org/10.1145/2935334.2935361 ith Ringel Morris. 2018. FootNotes: Geo-Referenced Audio Annotations for Non-
[2] Ronny Andrade, Melissa J. Rogerson, Jenny Waycott, Steven Baker, and Frank visual Exploration. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2, 3,
Vetere. 2019. Playing Blind: Revealing the World of Gamers with Visual Impair- Article 109 (Sept. 2018), 24 pages. https://doi.org/10.1145/3264919
ment. In Proceedings of the 2019 CHI Conference on Human Factors in Computing [22] Anhong Guo, Xiang ‘Anthony’ Chen, Haoran Qi, Samuel White, Suman Ghosh,
Systems (Glasgow, Scotland Uk) (CHI ’19). Association for Computing Machinery, Chieko Asakawa, and Jefrey P. Bigham. 2016. VizLens: A Robust and Interactive
New York, NY, USA, Article 116, 14 pages. https://doi.org/10.1145/ Screen Reader for Interfaces in the Real World. In Proceedings of the 29th An-
3290605.3300346 nual Symposium on User Interface Software and Technology (Tokyo, Japan) (UIST
[3] Ronny Andrade, Melissa J. Rogerson, Jenny Waycott, Steven Baker, and Frank ’16). ACM, New York, NY, USA, 651–664. https://doi.org/10.1145/
Vetere. 2020. Introducing the Gamer Information-Control Framework: Enabling 2984511.2984518
Access to Digital Games for People with Visual Impairment. In Proceedings of [23] Anhong Guo, Junhan Kong, Michael Rivera, Frank F. Xu, and Jefrey P. Bigham.
the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, 2019. StateLens: A Reverse Engineering Solution for Making Existing Dynamic
USA) (CHI ’20). Association for Computing Machinery, New York, NY, USA, 1–14. Touchscreens Accessible. In Proceedings of the 32nd Annual Symposium on User
https://doi.org/10.1145/3313831.3376211 Interface Software and Technology (UIST ’19). ACM, New York, NY, USA. https:
[4] Apple Inc. 2019. Accessibility on iOS. https://developer.apple.com/ //doi.org/10.1145/3332165.3347873
accessibility/ios/. [24] Anhong Guo, Saige McVea, Xu Wang, Patrick Clary, Ken Goldman, Yang Li, Yu
[5] Apple Inc. 2019. Human Interface Guidlines: Augmented Reality. Zhong, and Jefrey P. Bigham. 2018. Investigating Cursor-based Interactions to
https://developer.apple.com/design/human-interface- Support Non-Visual Exploration in the Real World. In Proceedings of the 20th
guidelines/ios/system-capabilities/augmented-reality/. International ACM SIGACCESS Conference on Computers and Accessibility (Galway,
[6] Chieko Asakawa and Takashi Itoh. 1998. User Interface of a Home Page Reader. Ireland) (ASSETS ’18). ACM, New York, NY, USA, 3–14. https://doi.org/
In Proceedings of the Third International ACM Conference on Assistive Technologies 10.1145/3234695.3236339
(Marina del Rey, California, USA) (Assets ’98). ACM, New York, NY, USA, 149–156. [25] Florian Heller and Jan Borchers. 2015. AudioScope: Smartphones As Directional
https://doi.org/10.1145/274497.274526 Microphones in Mobile Audio Augmented Reality Systems. In Proceedings of the
[7] BlindSquare. 2020. BlindSquare. https://www.blindsquare.com Ac- 33rd Annual ACM Conference on Human Factors in Computing Systems (Seoul,
cessed July, 2020. Republic of Korea) (CHI ’15). ACM, New York, NY, USA, 949–952. https:
[8] Jefrey R Blum, Mathieu Bouchard, and Jeremy R Cooperstock. 2011. What’s //doi.org/10.1145/2702123.2702159
around me? Spatialized audio augmented reality for blind users with a smart- [26] Teresa Hirzle, Jan Gugenheimer, Florian Geiselhart, Andreas Bulling, and Enrico
phone. In International Conference on Mobile and Ubiquitous Systems: Computing, Rukzio. 2019. A Design Space for Gaze Interaction on Head-Mounted Displays.
Networking, and Services. Springer, 49–62. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Sys-
[9] Giorgio Brajnik. 2008. Beyond conformance: the role of accessibility evaluation tems (Glasgow, Scotland Uk) (CHI ’19). Association for Computing Machinery,
methods. In International Conference on Web Information Systems Engineering. New York, NY, USA, Article 625, 12 pages. https://doi.org/10.1145/
Springer, 63–80.
Making Mobile Augmented Reality Applications Accessible ASSETS ’20, October 26–28, 2020, Virtual Event, Greece
3290605.3300855 [49] Overstock.com Inc. 2019. Overstock – Furniture & Decor. https://apps.
[27] Houzz Inc. 2019. Houzz – Home Design & Remodel. https://apps.apple. apple.com/us/app/overstock-furniture-decor/id339883869
com/us/app/houzz-home-design-remodel/id399563465 [50] Amy Pavel, Gabriel Reyes, and Jefrey P. Bigham. 2020. Rescribe: Authoring and
[28] Inter IKEA Systems. 2017. IKEA Place. https://apps.apple.com/us/ Automatically Editing Audio Descriptions. In Proceedings of the 33rd Annual ACM
app/ikea-place/id1279244498 Symposium on User Interface Software & Technology (UIST 2020).
[29] Chandrika Jayant, Hanjie Ji, Samuel White, and Jefrey P. Bigham. 2011. Sup- [51] Tran Pham, Jo Vermeulen, Anthony Tang, and Lindsay MacDonald Vermeulen.
porting Blind Photography. In The Proceedings of the 13th International ACM 2018. Scale Impacts Elicited Gestures for Manipulating Holograms: Implications
SIGACCESS Conference on Computers and Accessibility (Dundee, Scotland, UK) for AR Gesture Design. In Proceedings of the 2018 Designing Interactive Systems
(ASSETS ’11). ACM, New York, NY, USA, 203–210. https://doi.org/10. Conference (Hong Kong, China) (DIS ’18). Association for Computing Machin-
1145/2049536.2049573 ery, New York, NY, USA, 227–240. https://doi.org/10.1145/3196709.
[30] Shaun K. Kane, Jefrey P. Bigham, and Jacob O. Wobbrock. 2008. Slide Rule: 3196719
Making Mobile Touch Screens Accessible to Blind People Using Multi-touch [52] Thammathip Piumsomboon, Adrian Clark, Mark Billinghurst, and Andy Cock-
Interaction Techniques. In Proceedings of the 10th International ACM SIGACCESS burn. 2013. User-Defned Gestures for Augmented Reality. In Human-Computer
Conference on Computers and Accessibility (Halifax, Nova Scotia, Canada) (As- Interaction – INTERACT 2013, Paula Kotzé, Gary Marsden, Gitte Lindgaard, Janet
sets ’08). ACM, New York, NY, USA, 73–80. https://doi.org/10.1145/ Wesson, and Marco Winckler (Eds.). Springer Berlin Heidelberg, Berlin, Heidel-
1414471.1414487 berg, 282–299.
[31] SeungJun Kim and Anind K. Dey. 2009. Simulated Augmented Reality Windshield [53] Justus J Randolph. 2005. Free-Marginal Multirater Kappa (multirater K [free]):
Display As a Cognitive Mapping Aid for Elder Driver Navigation. In Proceedings An Alternative to Fleiss’ Fixed-Marginal Multirater Kappa. Online submission
of the SIGCHI Conference on Human Factors in Computing Systems (Boston, MA, (2005).
USA) (CHI ’09). ACM, New York, NY, USA, 133–142. https://doi.org/10. [54] Anne Spencer Ross, Xiaoyi Zhang, James Fogarty, and Jacob O. Wobbrock. 2017.
1145/1518701.1518724 Epidemiology As a Framework for Large-Scale Mobile Application Accessibility
[32] Julian Kreimeier and Timo Götzelmann. 2019. First Steps Towards Walk-In- Assessment. In Proceedings of the 19th International ACM SIGACCESS Confer-
Place Locomotion and Haptic Feedback in Virtual Reality for Visually Impaired. ence on Computers and Accessibility (Baltimore, Maryland, USA) (ASSETS ’17).
In Extended Abstracts of the 2019 CHI Conference on Human Factors in Com- ACM, New York, NY, USA, 2–11. https://doi.org/10.1145/3132525.
puting Systems (Glasgow, Scotland Uk) (CHI EA ’19). Association for Com- 3132547
puting Machinery, New York, NY, USA, Article LBW2214, 6 pages. https: [55] Daisuke Sato, Uran Oh, Kakuya Naito, Hironobu Takagi, Kris Kitani, and Chieko
//doi.org/10.1145/3290607.3312944 Asakawa. 2017. NavCog3: An Evaluation of a Smartphone-Based Blind In-
[33] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. door Navigation Assistant with Semantic Features in a Large-Scale Environ-
2017. Dense-captioning events in videos. In Proceedings of the IEEE international ment. In Proceedings of the 19th International ACM SIGACCESS Conference on
conference on computer vision. 706–715. Computers and Accessibility (Baltimore, Maryland, USA) (ASSETS ’17). Asso-
[34] Rynhardt Kruger and Lynette van Zijl. 2015. Virtual world accessibility with the ciation for Computing Machinery, New York, NY, USA, 270–279. https:
Perspective Viewer. Enabling Access for Persons with Visual Impairment (2015) //doi.org/10.1145/3132525.3132535
105 (2015). [56] Alexa F. Siu, Son Kim, Joshua A. Miele, and Sean Follmer. 2019. ShapeCAD:
[35] Le-Roy Karunaratne. 2019. ARcheology -Dig Up History. https://apps. An Accessible 3D Modelling Workfow for the Blind and Visually-Impaired Via
apple.com/gb/app/archeology-dig-up-history/id1440322910 2.5D Shape Displays. In The 21st International ACM SIGACCESS Conference on
[36] Kyungjun Lee, Jonggi Hong, Simone Pimento, Ebrima Jarjue, and Hernisa Ka- Computers and Accessibility (Pittsburgh, PA, USA) (ASSETS ’19). Association for
corri. 2019. Revisiting Blind Photography in the Context of Teachable Object Computing Machinery, New York, NY, USA, 342–354. https://doi.org/
Recognizers. In The 21st International ACM SIGACCESS Conference on Computers 10.1145/3308561.3353782
and Accessibility (Pittsburgh, PA, USA) (ASSETS ’19). Association for Comput- [57] Alexa F. Siu, Joshua Miele, and Sean Follmer. 2018. An Accessible CAD Workfow
ing Machinery, New York, NY, USA, 83–95. https://doi.org/10.1145/ Using Programming of 3D Models and Preview Rendering in A 2.5D Shape
3308561.3353799 Display. In Proceedings of the 20th International ACM SIGACCESS Conference
[37] Magic Leap, Inc. 2020. Magic Leap Developer Portal: Accessibility. on Computers and Accessibility (Galway, Ireland) (ASSETS ’18). Association for
https://developer.magicleap.com/en-us/learn/guides/bp- Computing Machinery, New York, NY, USA, 343–345. https://doi.org/
for-accessibility Accessed July, 2020. 10.1145/3234695.3240996
[38] Magnate Interactive Ltd. 2018. Waypoint EDU. https://apps.apple.com/ [58] Alexa F. Siu, Mike Sinclair, Robert Kovacs, Eyal Ofek, Christian Holz, and
us/app/waypoint-edu/id1248849526 Ed Cutrell. 2020. Virtual reality without vision: A haptic and auditory white
[39] Microsoft Research. 2019. Microsoft Soundscape – A map delivered in cane to navigate complex virtual worlds. In CHI 2020. ACM, ACM Press.
3D sound. https://www.microsoft.com/en-us/research/product/ https://www.microsoft.com/en-us/research/publication/
soundscape/. virtual-reality-without-vision-a-haptic-and-auditory-
[40] Paul Milgram and Fumio Kishino. 1994. A taxonomy of mixed reality visual white-cane-to-navigate-complex-virtual-worlds/ CHI 2020
displays. IEICE TRANSACTIONS on Information and Systems 77, 12 (1994), 1321– Honorable Mention paper.
1329. [59] Maximilian Speicher, Brian D. Hall, and Michael Nebeling. 2019. What is Mixed
[41] Paul Milgram, Haruo Takemura, Akira Utsumi, and Fumio Kishino. 1995. Aug- Reality?. In Proceedings of the 2019 CHI Conference on Human Factors in Computing
mented reality: A class of displays on the reality-virtuality continuum. In Telema- Systems (Glasgow, Scotland Uk) (CHI ’19). ACM, New York, NY, USA, Article 537,
nipulator and telepresence technologies, Vol. 2351. International Society for Optics 15 pages. https://doi.org/10.1145/3290605.3300767
and Photonics, 282–292. [60] Statue of Liberty – Ellis Island. 2019. Statue of Liberty. https://apps.apple.
[42] Martez Mott, Ed Cutrell, Mar Gonzalez Franco, Christian Holz, Eyal Ofek, com/us/app/statue-of-liberty/id1457506359
Richard Stoakley, and Meredith Ringel Morris. 2019. Accessible by Design: [61] Hemant Bhaskar Surale, Aakar Gupta, Mark Hancock, and Daniel Vogel. 2019.
An Opportunity for Virtual Reality. In ISMAR 2019 Workshop on Mixed TabletInVR: Exploring the Design Space for Using a Multi-Touch Tablet in Virtual
Reality and Accessibility. IEEE. https://www.microsoft.com/en- Reality. In Proceedings of the 2019 CHI Conference on Human Factors in Computing
us/research/publication/accessible-by-design-an- Systems (Glasgow, Scotland Uk) (CHI ’19). Association for Computing Machinery,
opportunity-for-virtual-reality/ New York, NY, USA, Article 13, 13 pages. https://doi.org/10.1145/
[43] Tobias Müller and Ralf Dauenhauer. 2016. A taxonomy for information linking 3290605.3300243
in augmented reality. In International Conference on Augmented Reality, Virtual [62] Enrico Tanuwidjaja, Derek Huynh, Kirsten Koa, Calvin Nguyen, Churen Shao,
Reality and Computer Graphics. Springer, 368–387. Patrick Torbett, Colleen Emmenegger, and Nadir Weibel. 2014. Chroma: A
[44] Elizabeth D. Mynatt and W. Keith Edwards. 1992. Mapping GUIs to Auditory Wearable Augmented-reality Solution for Color Blindness. In Proceedings of the
Interfaces. In Proceedings of the 5th Annual ACM Symposium on User Interface 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing
Software and Technology (Monteray, California, USA) (UIST ’92). ACM, New York, (Seattle, Washington) (UbiComp ’14). ACM, New York, NY, USA, 799–810. https:
NY, USA, 61–70. https://doi.org/10.1145/142621.142629 //doi.org/10.1145/2632048.2632091
[45] NBA Properties, Inc. 2019. NBA AR Basketball. https://apps.apple.com/ [63] Target. 2019. Target. https://apps.apple.com/us/app/target/
us/app/nba-ar-basketball/id1273888016 id297430070
[46] NBCUniversal Media. 2018. Forensic Detective. https://apps.apple.com/ [64] Dimitrios Tzovaras, Konstantinos Moustakas, Georgios Nikolakis, and Michael G.
us/app/forensic-detective/id1382363470 Strintzis. 2009. Interactive Mixed Reality White Cane Simulation for the Training
[47] Jakob Nielsen. 1994. Usability engineering. Elsevier. of the Blind and the Visually Impaired. Personal Ubiquitous Comput. 13, 1 (Jan.
[48] Occipital, Inc. 2019. TapMeasure -AR Utility. https://apps.apple.com/ 2009), 51–58. https://doi.org/10.1007/s00779-007-0171-2
cz/app/tapmeasure-ar-utility/id1281766938 [65] Marynel Vázquez and Aaron Steinfeld. 2014. An Assisted Photography Frame-
work to Help Visually Impaired Users Properly Aim a Camera. ACM Trans.
ASSETS ’20, October 26–28, 2020, Virtual Event, Greece Jaylin Herskovitz, Jason Wu, Samuel White, Amy Pavel, Gabriel Reyes, Anhong Guo, and Jefrey P. Bigham
Comput.-Hum. Interact. 21, 5, Article 25 (Nov. 2014), 29 pages. https://doi. [72] Yuhang Zhao, Edward Cutrell, Christian Holz, Meredith Ringel Morris, Eyal
org/10.1145/2651380 Ofek, and Andrew D. Wilson. 2019. SeeingVR: A Set of Tools to Make Virtual
[66] W3C. 2018. Web Content Accessibility Guidelines (WCAG) Overview. https: Reality More Accessible to People with Low Vision. In Proceedings of the 2019 CHI
//www.w3.org/WAI/standards-guidelines/wcag/. Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI
[67] Ryan Wedof, Lindsay Ball, Amelia Wang, Yi Xuan Khoo, Lauren Lieberman, ’19). ACM, New York, NY, USA, Article 111, 14 pages. https://doi.org/10.
and Kyle Rector. 2019. Virtual Showdown: An Accessible Virtual Reality Game 1145/3290605.3300341
with Scafolds for Youth with Visual Impairments. In Proceedings of the 2019 CHI [73] Yuhang Zhao, Michele Hu, Shafeka Hashash, and Shiri Azenkot. 2017. Under-
Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI standing Low Vision People’s Visual Perception on Commercial Augmented
’19). Association for Computing Machinery, New York, NY, USA, Article 141, Reality Glasses. In Proceedings of the 2017 CHI Conference on Human Factors in
15 pages. https://doi.org/10.1145/3290605.3300371 Computing Systems (Denver, Colorado, USA) (CHI ’17). ACM, New York, NY, USA,
[68] Brian Wentz, Paul T Jaeger, and Jonathan Lazar. 2011. Retroftting accessibility: 4170–4181. https://doi.org/10.1145/3025453.3025949
The legal inequality of after-the-fact online access for persons with disabilities [74] Yuhang Zhao, Elizabeth Kupferstein, Brenda Veronica Castro, Steven Feiner, and
in the United States. First Monday 16, 11 (2011). Shiri Azenkot. 2019. Designing AR Visualizations to Facilitate Stair Navigation
[69] Gesa Wiegand, Christian Mai, Kai Holländer, and Heinrich Hussmann. 2019. for People with Low Vision. In Proceedings of the 32nd Annual Symposium on
InCarAR: A Design Space Towards 3D Augmented Reality Applications in Ve- User Interface Software and Technology (UIST ’19). ACM, New York, NY, USA.
hicles. In Proceedings of the 11th International Conference on Automotive User [75] Yuhang Zhao, Elizabeth Kupferstein, Hathaitorn Rojnirun, Leah Findlater, and
Interfaces and Interactive Vehicular Applications (Utrecht, Netherlands) (Automo- Shiri Azenkot. 2020. The Efectiveness of Visual and Audio Wayfnding Guidance
tiveUI ’19). Association for Computing Machinery, New York, NY, USA, 1–13. on Smartglasses for People with Low Vision. In Proceedings of the 2020 CHI
https://doi.org/10.1145/3342197.3344539 Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI
[70] Xiaoyi Zhang, Anne Spencer Ross, Anat Caspi, James Fogarty, and Jacob O. ’20). Association for Computing Machinery, New York, NY, USA, 1–14. https:
Wobbrock. 2017. Interaction Proxies for Runtime Repair and Enhancement of //doi.org/10.1145/3313831.3376516
Mobile Application Accessibility. In Proceedings of the 2017 CHI Conference on [76] Yuhang Zhao, Sarit Szpiro, and Shiri Azenkot. 2015. ForeSee: A Customizable
Human Factors in Computing Systems (Denver, Colorado, USA) (CHI ’17). ACM, Head-Mounted Vision Enhancement System for People with Low Vision. In
New York, NY, USA, 6024–6037. https://doi.org/10.1145/3025453. Proceedings of the 17th International ACM SIGACCESS Conference on Computers
3025846 &#38; Accessibility (Lisbon, Portugal) (ASSETS ’15). ACM, New York, NY, USA,
[71] Yuhang Zhao, Cynthia L. Bennett, Hrvoje Benko, Edward Cutrell, Christian 239–249. https://doi.org/10.1145/2700648.2809865
Holz, Meredith Ringel Morris, and Mike Sinclair. 2018. Enabling People with [77] Yuhang Zhao, Sarit Szpiro, Jonathan Knighten, and Shiri Azenkot. 2016. CueSee:
Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Exploring Visual Cues for People with Low Vision to Facilitate a Visual Search
Cane Simulation. In Proceedings of the 2018 CHI Conference on Human Factors in Task. In Proceedings of the 2016 ACM International Joint Conference on Pervasive
Computing Systems (Montreal QC, Canada) (CHI ’18). ACM, New York, NY, USA, and Ubiquitous Computing (Heidelberg, Germany) (UbiComp ’16). ACM, New
Article 116, 14 pages. https://doi.org/10.1145/3173574.3173690 York, NY, USA, 73–84. https://doi.org/10.1145/2971648.2971730

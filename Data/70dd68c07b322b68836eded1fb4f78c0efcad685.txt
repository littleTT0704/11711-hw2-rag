A Multi-dimensional Evaluation of
Tokenizer-free Multilingual Pretrained Models
JiminSun1,2 PatrickFernandes1 XinyiWang1 GrahamNeubig1
1LanguageTechnologiesInstitute,CarnegieMellonUniversity 2KakaoEnterprise
{jimins2,pfernand,xinyiw1,gneubig}@cs.cmu.edu
Abstract weuncovered;wehighlightsomechallengeswith
applyingthesemodelsandproposebestpractices
Recent work on tokenizer-free multilingual
forfutureresultsreportinginthisarea.
pretrained models show promising results in
Specifically,weperformexperimentsfine-tuning
improvingcross-lingualtransferandreducing
pretrained multilingual models, evaluating them
engineeringoverhead(Clarketal.,2022;Xue
et al., 2022). However, these works mainly with respect to (1) robustness to fine-tuning data
focus on reporting accuracy on a limited set settings,(2)dataefficiency,and(3)inferencetime
of tasks and data settings, placing less em- andmemoryconsumption. Basedonthesemultiple
phasis on other important factors when tun-
dimensions,wecometothesomewhatsurprising
inganddeployingthemodelsinpractice,such
conclusionthatsubword-basedmodelsmightstill
as memory usage, inference speed, and fine-
be the most practical choice in most settings, as
tuningdatarobustness. Weattempttofillthis
theyarecomparablyrobusttovariousfine-tuning
gapbyperformingacomprehensiveempirical
comparisonofmultilingualtokenizer-freeand datasettingswitharelativelylowinferencecost.
subword-based models considering these var-
ious dimensions. Surprisingly, we find that 2 Tokenizer-freeMultilingualModels
subword-basedmodelsmightstillbethemost
practical choice in many settings, achieving Whilemultilingualpretrainedmodels(Devlinetal.,
betterperformanceforlowerinferencelatency 2019; Lample and Conneau, 2019; Liu et al.,
andmemoryusage. Basedontheseresults,we 2020;Chietal.,2021;Xueetal.,2021)haveled
encouragefutureworkintokenizer-freemeth-
to impressive performance for low-resource lan-
ods to consider these factors when designing
guagesthroughcross-lingualtransfer,thestandard
andevaluatingnewmodels.
wordrepresentationinthesemodelsreliesonsub-
1 Introduction word segmentation (Sennrich et al., 2016; Kudo,
2018). Inamultilingualsetting,subwordtokeniza-
Several recent results (Clark et al., 2022; Xue
tion can be sub-optimal as supporting hundreds
et al., 2022) have excited the research commu-
of languages with various scripts and vocabulary
nitywiththepossibilityof“tokenizer-free”models,
causessegmentationmismatchbetweenlanguages
character-levelandbyte-levelmodels,asanalterna-
andover-segmentationinthelower-resourcedlan-
tivetomoretraditionalsubword-basedmodels. We,
guages (Wang et al., 2020; Ebrahimi and Kann,
theauthorsofthispaper,werealsoinitiallyexcited
2021). To alleviate this problem, recent works
bytheseresults–thepossibilityofeschewingthe
propose to remove the preprocessing step of sub-
two-stepprocessingpipelineofsubwordsegmenta-
wordsegmentationbydirectlyusingcharactersor
tionandsubword-basedmodelswouldreducethe
bytesaslexicalunits(Clarketal.,2022;Xueetal.,
correspondingdifficultiesincross-lingualtransfer
2022). Tab.1presentsanoverviewofthedifferent
(Huetal.,2020;Maronikolakisetal.,2021;Rust
tokenizer-freemultilingualmodelswithcompara-
etal.,2021;Wangetal.,2021)ordomainadapta-
ble subword models. Next, we briefly describe
tion(Satoetal.,2020;Liuetal.,2021)duetoincon-
thetwotokenizer-freemodelsweconsiderinthis
sistent subword units. However, upon several at-
work.
temptstoapplytokenizer-freemethods,ourexcite-
mentwastempereduponrealizationofanumber CANINE (Clarketal.,2022)isacharacter-level
ofpracticaldifficultiesinapplyingthesemethods. encoder-onlymodelcomparablewithmBERT(De-
Thispaperisachronicleofsomeoftheconcerns vlin et al., 2019). CANINE operates on charac-
2202
tcO
31
]LC.sc[
1v11170.0122:viXra
Model Params Architecture Enc. Dec. Tokenization Downsample? Pretrainedcorpus Languages
mBERT 178M Enc-only 12 - Subword (cid:55) Wikipedia 104
CANINE 127M Enc-only 12 - Character (cid:51) Wikipedia 104
mT5(Small) 300M Enc-dec 8 8 Subword (cid:55) mC4 101
ByT5(Small) 300M Enc-dec 12 4 UTF-8bytes (cid:55) mC4 101
Table 1: Configuration of the pre-trained models used for experiments. From left to right: number of parame-
ters,architecture,encoderdepth,decoderdepth,tokenizationscheme,whetherdownsamplingwasusedtoreduce
computation,pretrainedcorpus,numberoflanguagescoveredduringpretraining
ter sequences and is pretrained using the masked considertheinferencecostofthefine-tunedmod-
languagemodeling(MLM)objective. Tocompen- els. Tothisend, weconductamulti-dimensional
sate for the loss of computational efficiency due evaluation focusing on three aspects: robustness
to increased sequence length, CANINE relies on tofine-tuningdatasettings(§4.1),dataefficiency
convolution layers to down-sample the character (§4.2),andinferencecost(§4.3)toprovideabet-
sequencebeforefeedingtherepresentationstothe terunderstandingofthepracticalapplicabilityof
transformerlayers. tokenizer-freemodels.
ThetwomodelvariantsofCANINE–CANINE-
3 Experimentalsettings
S and CANINE-C – have the same architecture
but slightly different pretraining strategies using We evaluate mBERT, CANINE, mT5, and ByT5
subwords or characters. Our experiments found onthreetasksadoptedfromtheXTREMEbench-
similarperformanceforbothvariants,soweonly mark(Huetal.,2020).
showtheperformanceofCANINE-S,leavingthe
3.1 Tasks
resultsforCANINE-Cin§B.1.
XNLI TheCross-lingualNaturalLanguageInfer-
ByT5 (Xue et al., 2022) is an encoder-decoder
ence(Conneauetal.,2018)isasequence-levelclas-
transformermodelsimilartothemT5(Xueetal.,
sificationtaskinwhichthemodelpredictswhether
2021)model. BothByT5andmT5arepretrained
thehypothesissentenceisanentailment,contradic-
onthemultilingualCommonCrawl(mC4)corpus1
tion, or neutral given the premise sentence. The
usingthespanreconstructionobjectiveproposedby
taskisprovidedin15languages.
Raffeletal.(2020). ByT5operatesontherawUTF-
8 bytes of the input without any downsampling, NER Named Entity Recognition (NER) is a
leadingtoalongersequencelengthwhilehavinga structured prediction task. We use the WikiAnn
muchsmallervocabularysizethanmT5. dataset (Pan et al., 2017), which covers 282 lan-
TokeeptheparametercountfixedbetweenmT5 guages. We select 20 languages (listed in Tab. 3)
and ByT5, Xue et al. (2022) allocate the parame- based on linguistic diversity and the languages
ters saved from the embedding layer in ByT5 to available in the other two tasks we consider. We
additionalencoderlayers. Althoughareasonable usethetrain,dev,andtestsplitsfromRahimietal.
design choice, our results in § 4 show that ByT5 (2019).
suffers from a much higher inference cost due to
TyDi QA-GoldP The Typologically Diverse
a deeper encoder and longer sequence lengths in
Question Answering (Clark et al., 2020) dataset
boththeinputandoutput.2
isanextractivequestionansweringbenchmarkin
11languages. Weusethegoldpassageversionof
Discussion One significant advantage of multi-
thetask(GoldP),whichcoversninelanguages.
lingualpretrainedmodelsisthatoncethepretrain-
ing is complete, we can fine-tune the model to a
3.2 DetailsofHardwareandMeasurements
variety of tasks and languages under different re-
WeuseasingleTeslaV100(32GB)GPUforallex-
sourcesettings. Inthiscontext,itisalsoessentialto
perimentsregardinginferencecostmeasurements.
1https://www.tensorflow.org/datasets/catalog/ ToobtainthepeakGPUmemoryandinferencela-
c4#c4multilingual tency, we randomly select 100 samples from the
2WemainlyconsidertheByT5smallmodeltokeepinfer-
Englishtestsetforeachtaskandmeasuretheaver-
encecostrelativelyconstantandfitthemodelinthehardware
thatweeasilyhaveaccessto. agecostofpredictingoneexampleatatime.
mBERT(Multi) CANINE-S(Multi) mT5(Multi) ByT5(Multi)
mBERT(Single) CANINE-S(Single) mT5(Single) ByT5(Single)
mBERT(Zero) CANINE-S(Zero) mT5(Zero) ByT5(Zero)
XNLI NER TyDiQA
100 100 100
90 90 90
80 80
80 70
70
70 60
60
50
60 50 40
50 40 30
0 250 500 750 1000 1250 0 250 500 750 1000 1250 0 500 1000 1500 2000
PeakGPUmemoryduringinference(mb)
XNLI NER TyDiQA
100 100 100
90 90 90
80 80
80 70
70
70 60
60
50
60 50 40
50 40 30
0 20 40 60 80 100 0 100 200 300 0 100 200 300
Averageinferencetimepersample(ms)
Figure 1: Average XNLI, NER, TyDi QA performance across languages when each model is fine-tuned with
multilingualdata(mBERT:(cid:110),CANINE-S:(cid:110),mT5: (cid:110),ByT5: (cid:110)),single-languagedata(mBERT:(cid:108),CANINE-
S: (cid:108), mT5: (cid:108), ByT5: (cid:108)), or zero-shot transferred from English (mBERT: (cid:115), CANINE-S: (cid:115), mT5: (cid:115), ByT5:
(cid:115)). The x-axis indicates the corresponding inference cost criteria (Top row: Peak GPU memory, Bottom row:
Inferencelatency)ineachtask. Weadditionallyuse(cid:77)tomarkthedirectionoftheidealcostandperformance.
4 AMulti-dimensionalEvaluation thebestperformance,while ZERO hasthelowest
performance,thelengthoftheverticallineconnect-
InFig.1,Weplottheperformanceofthemodels
ingthetwopointsindicateshowmuchthemodel
fine-tuned using three different data settings: 1)
performancefluctuateswithdifferentdatasettings.
ZERO: themodelisfine-tunedwithEnglishtrain-
We find that CANINE generally has a larger per-
ingdataandthenevaluatedonthemultilingualtest
formance gap under multiple data settings than
set; 2) SINGLE: themodelisfine-tunedindividu-
mBERT,indicatingthatitislessrobusttovarious
allyonthetaskdataineachlanguage;3) MULTI:
fine-tuningsettings.
themodelisjointlyfine-tunedonthetaskdatain
alllanguages. Thelocationsofthemodelsonthe
x-axisarearrangedfromlefttorightbasedonin-
creasinglatencyandmemorycostduringinference. For mT5 and ByT5, we find that the two
Modelslocatedclosertotheupperleftcornerof modelsperformsimilarlyintermsofperformance
eachplotarepreferredbecausetheyachievebetter robustness. However,fordownstreamtaskscores,
performanceonthetestsetwhileincurringlowerin- ByT5significantlyoutperformsmT5inmosttasks
ferencecosts. Interestingly,ourmulti-dimensional andfine-tuningsettings. WenotethatmT5-small
evaluation reveals that mBERT generally has the mightbesomewhatpenalizedasmostparameters
best performance and efficiency under most set- are allocated on the embedding layer (85% for
tings. Next,wediscusseachevaluationdimension mT5-Smallvs0.3%forByT5-Small). Also,given
indetail. thatthetasksconcernedarenotgeneration-heavy
tasks, the extra depth on the encoder side (12
4.1 Robustnesstofine-tuningdatasettings
layers in ByT5-Small vs 8 layers in mT5-Small)
InFig.1,weplotthethreeperformancenumbers might have favored ByT5 over mT5. However,
obtained under different fine-tuning data settings when comparing the two encoder-decoder based
(MULTI, SINGLE, ZERO) for each model. Over- models to mBERT, mBERT still achieves the
all,mBERTachievesthebestaccuracyforalmost best performance and robustness in all tasks and
allthreesettings. Since MULTI typicallyleadsto settingswiththeexceptionofXNLIZero-shot.
ycaruccA
ycaruccA
1F
1F
1F
1F
mBERT CANINE-S mT5 ByT5
XNLI(en) NER(en) TyDiQA(en)
80
80 80 70
70
70 60
60 50
60
50 40
40 50 30
30 40 20
102 103 104 105 ? 106 102 103 104 105? 102 103 ?
80 80
80
70
70 70 60
60 60 50
50 40
50 30
40
20
30 40
102 103 104 105 ? 106 102 103 104 105? 102 103 ?
Numberofannotatedsamplesusedforfine-tuning
Figure2:Performanceofmodelsfine-tunedwithdifferentnumbersoflabeledsamplesinthethreetasksinEnglish.
ThetoprowcomparesmBERTtotheCANINE-Smodel,whilethebottomrowcomparesmT5toByT5. Wealso
denotethefulldatasetsizewith(cid:63)onthex-axis.
4.2 Fine-tuningdataefficiency first row of Fig. 1, we visualize the models’ per-
formancetotheirpeakGPUmemoryconsumption.
To further investigate how robustly each model
Weseethattheencoder-onlymodels,mBERTand
performs under different resource conditions
CANINE,requiremuchlessmemorythanthemT5
within a single language, we fine-tune the model
and ByT5 models. CANINE generally does not
with various task dataset sizes in English and
incurahighermemorycostthanmBERT,asithas
presenttheresultsinFig.2.3 ComparedtomBERT,
fewer parameters. However, ByT5 is more mem-
the performance of CANINE seems to degrade
ory intensive compared to mT5, particularly for
significantly as the amount of fine-tuning data
theTyDiQAtask,asthetaskconsistsofrelatively
decreases, particularly for XNLI and NER. To
longersequences,whichisespeciallyproblematic
explain this phenomenon, we hypothesize that
forByT5asithasamuchdeeperencoder.
character-levelmodelshavetheadditionalburden
TheplotsinthesecondrowofFig.1showthe
oflearningtocomposecharactersintosemantically
results based on the inference latency. mBERT
meaningful units, making it more difficult to
hasaslightadvantageoverCANINE,butthetwo
generalize given a smaller amount of fine-tuning
modelsaregenerallycomparable. However,mT5
data. For mT5 and ByT5, we find that the two
andByT5havemuchlowerinferencespeedsthan
modelsperformcomparablyinsmallerfine-tuning
mBERT.BecauseByT5hasasmallervocabulary,
datasets while on larger fine-tuning data, ByT5
consistentlyoutperformsmT5onallthreetasks. 4 itsinferencespeedisnotmuchworsethanmT5,de-
spitehavingadeeperencoderandlongersequences
Finally, we observe that mBERT achieves the
to generate. In fact, ByT5 has a faster inference
best performance on almost all tasks when the
speedthanmT5forXNLIbecausethemodelsonly
dataset is small, which is surprising, as we ex-
needtopredictasingletoken.
pectmT5/ByT5modelstogeneralizebetterinlow-
Ingeneral, wefindthatmBERTandCANINE
resourcesettingsgiventheirmuchlargerpretrain-
are both very accessible and efficient to use,
ingcorpus.
whereas ByT5 would require more effort in tun-
4.3 Inferencecost ingthebatchsizetofitintotheGPUmemory.
Anotherkeyconcerninutilizingpretrainedmodels
5 Conclusion
fordownstreamapplicationsistheinferencecost,
suchasmemoryconsumptionandlatency. Inthe
Inthispaper,wepresentamulti-dimensionaleval-
uation of tokenizer-free multilingual pretrained
3While we initially considered four languages (Arabic,
English,Russian,Swahili)chosenbasedonthepretraining models focusing on their robustness against fine-
corpussize,weincludeotherlanguageresultsin§B.2asthey tuningdatasettings,dataefficiency,andinference
tendtoexhibitsimilarperformancetrends.
cost. Surprisingly,wefindthatmBERTmightbe
4WesuspectByT5matches/surpassestheperformanceof
mT5inmanysituationsduetotheextradepthintheencoder. the most practical choice so far, considering all
ycaruccA
ycaruccA
1F
1F
1F
1F
theabovementionedfactors. Despiteourfindings, NationalScienceFoundationaswellastheCMU-
tokenizer-free models still have a significant ad- PortugalMAIAproject.
vantageinreducingengineeringeffortsandpoten-
tiallyincreasingrobustnesstonoisyandmultilin-
gualdata. Webelievemoreworkshouldbedone
indevelopingefficient tokenizer-freemodelsthat
arerobustinvariousfine-tuningsettings. Basedon
theseresults,weencouragethecommunitytocon-
siderthesecriteriaofpracticalapplicabilitywhen
developingandevaluatingtokenizer-freepretrained
models.
6 Limitations
ThispapermainlycoversthreeNLPtasks,focusing
onthesmaller-sizedmultilingualpretrainedmod-
els. Infuturework,itwouldbeinterestingtorun
the multi-dimensional evaluation we suggest on
a broader set of tasks and models. Although our
resultsshowthatsubwordmodelsareamoreprac-
ticalchoiceinsometasks,wenotethatothertasks
ordatasetsmayexistwheretokenizer-freemethods
achievebetterrelativeperformance. Forinstance,
tokenizer-freemodelshavebeenreportedtoexcel
inword-leveltasks,andnoisyenvironments(Xue
etal.,2022),andtheconclusionswereachedmay
be different in such settings. Moreover, we did
notexploremorecomplicatedgenerationtaskslike
translationorsummarization,wherethedifficulty
indecodingandlongerdecodehorizonscouldpaint
a different picture in a multi-dimensional evalua-
tion.
EthicsStatement
Wehopeourresultsencouragethecommunityto
considerthepracticalconcernsofrunninglargelan-
guagemodels(LLMs)anddesigningtokenizer-free
pretrained models. As the state-of-the-art LLMs
are becoming more computationally extensive, it
has become increasingly difficult for researchers
andpractitionerswithlessresourcestoutilizethese
modelsfordownstreamapplications. Wehopeour
multi-dimensional analysis can help researchers
andpractitionerswithlesscomputationalresources
decidewhichmodeltouseinpractice.
Acknowledgements
WeacknowledgeKakaoEnterpriseforproviding
thecomputeresourcesforthiswork. Additionally,
we would like to thank Jon Clark for answering
questionsrelatedtotheCANINEmodel. Thiswork
wassupportedinpartbygrant#2040926fromthe
References Xin Liu, Baosong Yang, Dayiheng Liu, Haibo Zhang,
Weihua Luo, Min Zhang, Haiying Zhang, and Jin-
Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham
song Su. 2021. Bridging subword gaps in pretrain-
Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao,
finetune paradigm for natural language generation.
Heyan Huang, and M. Zhou. 2021. Infoxlm: An
In Proceedings of the 59th Annual Meeting of the
information-theoretic framework for cross-lingual
Association for Computational Linguistics and the
languagemodelpre-training. InNAACL.
11thInternationalJointConferenceonNaturalLan-
guage Processing (Volume 1: Long Papers), pages
Jonathan H. Clark, Dan Garrette, Iulia Turc, and John
6001–6011, Online. Association for Computational
Wieting. 2022. Canine: Pre-training an Efficient
Linguistics.
Tokenization-Free Encoder for Language Represen-
tation. TransactionsoftheAssociationforComputa-
YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey
tionalLinguistics,10:73–91.
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
Jonathan H. Clark, Jennimaria Palomaki, Vitaly Niko-
pre-trainingforneuralmachinetranslation. Transac-
laev, Eunsol Choi, Dan Garrette, Michael Collins,
tions of the Association for Computational Linguis-
and Tom Kwiatkowski. 2020. Tydi QA: A bench-
tics,8:726–742.
markforinformation-seekingquestionansweringin
typologicallydiverselanguages. Trans.Assoc.Com-
put.Linguistics,8:454–470. Antonis Maronikolakis, Philipp Dufter, and Hinrich
Schütze.2021. Wineisnotvin.onthecompatibil-
Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad- ity of tokenizations across languages. In Findings
inaWilliams,SamuelR.Bowman,HolgerSchwenk, of the Association for Computational Linguistics:
and Veselin Stoyanov. 2018. XNLI: evaluating EMNLP 2021, pages 2382–2399, Punta Cana, Do-
cross-lingual sentence representations. In Proceed- minican Republic. Association for Computational
ings of the 2018 Conference on Empirical Methods Linguistics.
inNaturalLanguageProcessing,Brussels,Belgium,
October 31 - November 4, 2018, pages 2475–2485. Xiaoman Pan, Boliang Zhang, Jonathan May, Joel
AssociationforComputationalLinguistics. Nothman,KevinKnight,andHengJi.2017. Cross-
lingualnametaggingandlinkingfor282languages.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and InProceedingsofthe55thAnnualMeetingoftheAs-
Kristina Toutanova. 2019. BERT: Pre-training of sociationforComputationalLinguistics,ACL2017,
deep bidirectional transformers for language under- Vancouver, Canada, July 30 - August 4, Volume
standing. InProceedingsofthe2019Conferenceof 1: Long Papers, pages 1946–1958. Association for
the North, Stroudsburg, PA, USA. Association for ComputationalLinguistics.
ComputationalLinguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
Abteen Ebrahimi and Katharina Kann. 2021. How to
ine Lee, Sharan Narang, Michael Matena, Yanqi
adapt your pretrained multilingual model to 1600
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
languages. InProceedingsofthe59thAnnualMeet-
thelimitsoftransferlearningwithaunifiedtext-to-
ingoftheAssociationforComputationalLinguistics
text transformer. Journal of Machine Learning Re-
andthe11thInternationalJointConferenceonNat-
search,21(140):1–67.
uralLanguageProcessing(Volume1:LongPapers),
pages4555–4567,Online.AssociationforComputa-
Afshin Rahimi, Yuan Li, and Trevor Cohn. 2019.
tionalLinguistics.
Massively multilingual transfer for NER. In Pro-
ceedings of the 57th Conference of the Association
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
forComputationalLinguistics,ACL2019,Florence,
ham Neubig, Orhan Firat, and Melvin Johnson.
Italy, July 28- August 2, 2019, Volume 1: Long Pa-
2020. XTREME: A massively multilingual multi-
pers,pages151–164.AssociationforComputational
task benchmark for evaluating cross-lingual gener-
Linguistics.
alisation. In Proceedings of the 37th International
Conference on Machine Learning, volume 119 of
Phillip Rust, Jonas Pfeiffer, Ivan Vulic´, Sebastian
Proceedings of Machine Learning Research, pages
Ruder, and Iryna Gurevych. 2021. How good is
4411–4421.PMLR.
yourtokenizer? onthemonolingualperformanceof
TakuKudo.2018. Subwordregularization: Improving multilinguallanguagemodels. InProceedingsofthe
neuralnetworktranslationmodelswithmultiplesub- 59thAnnualMeetingoftheAssociationforCompu-
wordcandidates. InProceedingsofthe56thAnnual tationalLinguisticsandthe11thInternationalJoint
Meeting of the Association for Computational Lin- Conference on Natural Language Processing (Vol-
guistics(Volume1:LongPapers),pages66–75,Mel- ume1:LongPapers),pages3118–3135,Online.As-
bourne, Australia. Association for Computational sociationforComputationalLinguistics.
Linguistics.
Shoetsu Sato, Jin Sakuma, Naoki Yoshinaga, Masashi
Guillaume Lample and Alexis Conneau. 2019. Cross- Toyoda,andMasaruKitsuregawa.2020. Vocabulary
linguallanguagemodelpretraining. InNeurIPS. adaptationfordomainadaptationinneuralmachine
translation. InFindingsoftheAssociationforCom- A Tasks
putational Linguistics: EMNLP 2020, pages 4269–
4279, Online. Association for Computational Lin- For all tasks and models, we refer to the original
guistics. papers’codebaseforhyperparameters.567
Rico Sennrich, Barry Haddow, and Alexandra Birch. XNLI For encoder-only models, the first token
2016. Neural machine translation of rare words
([CLS])isusedtomapthesentencerepresentation
withsubwordunits. InProceedingsofthe54thAn-
tothelabeldistribution. Forencoder-decodermod-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715– els, we generate the index of the label (e.g., ‘0’)
1725, Berlin, Germany. Association for Computa- directly.
tionalLinguistics.
NER Forencoder-decodermodels,wefollowthe
Xinyi Wang, Sebastian Ruder, and Graham Neubig.
input-outputformat(e.g.,input: ‘tag: rick and
2021. Multi-view subword regularization. In Pro-
morty are cool .’, output: ‘PER: rick $$
ceedingsofthe2021ConferenceoftheNorthAmer-
ican Chapter of the Association for Computational PER: morty’)specifiedinthemT5model’sorigi-
Linguistics: Human Language Technologies, pages nalcodebase.
473–482, Online. Association for Computational
Linguistics. B Perlanguageresults
Zihan Wang, Karthikeyan K, Stephen Mayhew, and
B.1 Mainexperiments(Zero,Single,Multi)
Dan Roth. 2020. Extending multilingual BERT to
low-resource languages. In Findings of the Associ- XNLI:Tab.2,NER:Tab.3,TyDiQA-GoldP:Tab.4
ationforComputationalLinguistics: EMNLP2020,
pages2649–2656,Online.AssociationforComputa- B.2 Singlelanguagefine-tuningrobustness
tionalLinguistics.
Fig.3
Linting Xue, Aditya Barua, Noah Constant, Rami Al-
Rfou, Sharan Narang, Mihir Kale, Adam Roberts,
and Colin Raffel. 2022. ByT5: Towards a Token-
Free Future with Pre-trained Byte-to-Byte Models.
Transactions of the Association for Computational
Linguistics,10:291–306.
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2021. mt5: A massively
multilingual pre-trainedtext-to-text transformer. In
NAACL.
5https://github.com/google-research/language/
tree/master/language/canine
6https://github.com/google-research/
multilingual-t5
7https://github.com/google-research/byt5
Model en ar bg de el es fr hi ru sw th tr ur vi zh avg
Zero-shot(en)
mBERT 82.0 64.1 67.5 70.4 65.5 73.7 72.8 59.3 67.4 50.2 53.2 60.2 57.5 68.7 68.1 65.4
CANINE-S 77.7 50.1 60.1 62.4 53.7 67.6 66.0 43.7 60.7 40.4 39.6 47.9 41.1 53.1 43.2 53.8
CANINE-C 77.1 53.1 61.4 63.5 58.3 68.5 66.4 47.7 63.3 41.0 39.2 48.8 44.4 53.4 39.1 55.0
mT5-Small 79.0 61.3 66.0 64.4 67.4 65.9 62.4 59.7 66.6 52.2 64.1 57.9 56.4 57.3 63.9 63.0
ByT5-Small 80.9 65.9 70.2 71.2 67.7 76.5 75.0 58.6 67.9 62.4 58.4 63.6 55.6 69.5 64.9 67.2
Single-language
mBERT 82.0 70.6 76.2 76.6 75.1 77.7 77.4 67.0 74.8 66.3 65.7 72.5 62.9 75.9 76.4 73.1
CANINE-S 77.7 65.8 70.6 72.4 68.6 73.8 73.4 61.2 69.7 61.5 59.9 66.6 58.0 67.4 57.2 66.9
CANINE-C 77.1 66.2 71.1 72.0 69.8 72.8 72.6 62.3 68.6 60.8 57.1 65.7 58.2 67.3 60.0 66.8
mT5-Small 79.0 65.4 69.9 72.0 73.6 73.1 74.8 65.2 70.3 63.2 69.7 67.6 58.9 69.2 71.0 69.5
ByT5-Small 80.9 72.9 75.4 75.8 75.1 77.7 76.4 68.3 73.4 67.5 70.0 72.6 63.0 72.7 72.5 73.0
Multilingual
mBERT 83.5 73.2 77.7 77.5 75.7 79.8 78.6 70.1 76.4 68.1 67.2 73.8 64.4 76.5 77.9 74.7
CANINE-S 79.1 69.7 75.0 74.9 72.5 76.3 75.3 65.2 73.0 65.0 62.3 68.9 64.1 71.3 65.6 70.5
CANINE-C 78.0 68.5 73.7 74.1 72.9 75.7 74.9 63.8 71.7 64.4 57.7 67.9 62.6 69.7 58.7 69.0
mT5-Small 79.9 70.3 74.7 74.9 74.4 76.5 75.5 67.7 73.7 68.1 71.2 71.9 65.4 72.4 73.2 72.7
ByT5-Small 81.0 73.3 77.8 76.5 76.5 78.5 77.2 70.0 75.6 71.3 71.4 73.6 68.3 75.7 74.1 74.7
Table2: XNLIPerformance(Accuracy)
Model en ar bn de el es fi fr hi id ja ko ru sw ta te th tr ur zh avg
Zero-shot(en)
mBERT 84.2 41.7 68.2 78.2 71.4 71.8 77.3 78.0 64.5 51.6 29.2 59.7 65.6 71.4 51.0 50.4 0.4 73.9 33.3 43.1 58.2
CANINE-S 80.8 29.6 49.6 70.7 63.5 66.4 66.7 74.1 41.1 47.3 0.5 29.3 57.7 59.8 28.4 19.7 0.1 55.8 22.0 5.4 43.4
CANINE-C 81.1 38.3 56.9 70.9 66.4 64.8 68.0 73.5 43.4 46.6 1.8 28.7 61.7 58.9 36.9 21.6 0.2 58.9 29.8 8.1 45.8
mT5-Small 71.9 32.9 56.6 67.1 42.3 70.0 65.1 75.3 56.2 45.3 25.5 23.9 36.9 49.0 38.0 35.9 3.6 58.7 58.7 31.3 47.2
ByT5-Small 73.8 45.9 61.5 70.7 67.7 79.4 67.1 77.4 57.1 46.2 31.3 26.2 46.7 60.2 31.9 27.9 9.6 23.3 1.3 32.8 46.9
Single-language
mBERT 84.2 89.6 96.1 90.3 91.4 92.5 92.2 91.2 91.2 93.6 74.4 88.8 89.4 90.0 86.5 80.4 76.2 93.2 95.7 83.1 88.5
CANINE-S 80.8 84.9 92.9 88.0 88.6 89.7 89.1 88.9 84.9 90.9 63.3 81.6 86.5 87.7 81.0 49.9 70.5 90.9 91.0 73.2 82.7
CANINE-C 81.1 85.1 93.5 87.5 89.1 89.8 88.4 88.4 84.3 90.6 60.2 79.5 87.3 86.5 79.6 43.0 74.0 90.6 92.4 68.9 82.0
mT5-Small 71.9 86.5 86.6 83.7 83.8 88.0 87.8 86.7 85.5 85.3 65.9 80.2 64.0 71.0 82.6 74.5 64.6 86.3 93.0 75.1 80.1
ByT5-Small 73.8 85.3 88.3 82.4 87.6 86.6 86.4 84.7 83.0 84.5 69.9 83.2 62.6 84.5 80.3 69.1 74.5 83.4 90.5 73.2 80.7
Multilingual
mBERT 85.4 89.6 95.9 89.8 91.3 92.9 92.0 91.2 89.3 93.4 74.9 88.1 89.2 90.9 86.0 80.6 76.5 93.1 95.5 82.3 88.4
CANINE-S 84.1 88.0 94.7 89.3 90.7 92.1 91.1 90.9 85.8 92.8 69.3 83.8 88.8 89.6 81.7 71.3 76.2 92.4 94.0 75.7 86.1
CANINE-C 84.1 87.8 95.6 89.2 91.1 92.5 90.7 90.9 88.2 92.6 67.9 81.5 88.9 90.0 81.6 69.5 77.7 92.0 93.7 72.1 85.9
mT5-Small 72.5 86.8 84.5 84.8 83.4 88.7 88.3 87.7 83.6 87.2 70.1 83.1 64.8 72.3 82.3 69.8 67.8 86.9 92.4 76.5 80.7
ByT5-Small 73.5 87.7 88.4 86.1 88.7 90.3 89.9 89.3 84.7 87.3 70.3 83.8 66.3 84.3 81.8 78.0 72.6 88.6 92.6 76.5 83.0
Table3: NERPerformance(F1)
Model en ar bn fi id ko ru sw te avg
Zero-shot(en)
mBERT 73.64 60.11 45.1 57.63 63.78 52.16 57.52 56.51 42.15 56.51
CANINE-S 64.78 44.85 20.13 39.73 43.78 13.67 44.49 30.64 31.59 37.07
CANINE-C 63.96 42.19 22.05 43.13 36.87 17.44 42.02 33.3 30.51 36.83
mT5-Small 59.39 43.25 22.51 44.27 48.7 22.05 44.85 33.08 28.77 38.54
ByT5-Small 64.58 56.4 15.86 51.91 55.85 22.21 54.11 35.44 31.43 43.09
Single-language
mBERT 73.64 79.86 70.78 76.08 79.93 62.76 72.48 79.81 81.21 75.17
CANINE-S 64.78 79.2 55.81 70.13 70.0 49.53 67.15 71.26 81.75 67.73
CANINE-C 63.96 77.79 50.92 67.28 66.26 49.84 66.49 71.39 82.78 66.3
mT5-Small 59.39 73.07 67.92 65.33 73.65 54.93 66.13 71.49 80.93 68.09
ByT5-Small 64.58 75.82 69.91 71.98 80.55 58.65 71.09 78.81 85.39 72.97
Multilingual
mBERT 76.02 81.49 72.86 80.41 84.87 67.09 74.45 82.42 83.52 78.13
CANINE-S 71.55 80.53 67.24 75.42 78.44 61.25 71.75 77.43 83.53 74.13
CANINE-C 71.56 80.74 62.6 74.21 76.28 65.79 72.66 79.71 84.43 74.22
mT5-Small 64.39 75.34 76.89 70.01 76.73 59.24 67.86 76.62 81.35 72.05
ByT5-Small 69.42 75.86 70.9 74.52 79.78 60.62 73.01 80.32 85.93 74.48
Table4: TyDiQA-GoldPPerformance(F1)
mBERT CANINE-S mT5 ByT5
XNLI(en) NER(en) TyDiQA(en)
100 80
80
90 70
70
80 60
60
70 50
50
60 40
40 50
30
30 40
20
20 30
102 103 104 105 ? 106 102 103 104 105 ? 102 103 ?
XNLI(ru) NER(ru) TyDiQA(ru)
80 100 80
70 90 70
80
60 60
70
50 50
60
40 50 40
30 40 30
20 30 20
102 103 104 105 ? 106 102 103 104 ? 102 103 ?
XNLI(ar) NER(ar) TyDiQA(ar)
80 100
80
70 90
70
80
60 60
70
50 50
60
40 50 40
30 40 30
20 30 20
102 103 104 105 ? 106 102 103 104 105? 102 103 104 ?
XNLI(sw) NER(sw) TyDiQA(sw)
80 100
80
70 90 70
80
60 60
70
50 50
60
40 40
50
30 40 30
20 30 20
102 103 104 105 ? 106 102 103 ? 102 103 ?
Numberofannotatedsamplesusedforfine-tuning
Figure3:Performanceofmodelsfine-tunedwithdifferentnumbersoflabeledsamplesinthethreetasksinEnglish
(en),Russian(ru),Arabic(ar),andSwahili(sw). Wealsodenotethefulldatasetsizewith(cid:63)onthex-axis.
ycaruccA
ycaruccA
ycaruccA
ycaruccA
1F
1F
1F
1F
1F
1F
1F
1F

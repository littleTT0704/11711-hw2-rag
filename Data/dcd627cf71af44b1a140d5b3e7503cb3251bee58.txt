Sieg at MEDIQA 2019: Multi-task Neural Ensemble for Biomedical
Inference and Entailment
SaiAbishekBhaskar∗,RashiRungta∗,JamesRoute,EricNyberg,TerukoMitamura
LanguageTechnologiesInstitute,CarnegieMellonUniversity
{sabhaska, rashir, jroute, ehn, teruko}@cs.cmu.edu
Abstract correlation between questions as well as medical
sentences will help the biomedical community
This paper presents a multi-task learning ap-
cope with the increasing number of consumer
proach to natural language inference (NLI)
health questions posted on community question
andquestionentailment(RQE)inthebiomed-
icaldomain. Recognizingtextualinferencere- answering websites, many of which have already
lationsandquestionsimilaritycanaddressthe been asked before and can easily be answered by
issueofansweringnewconsumerhealthques- linkingthemwithapreviouslyansweredquestion
tions by mapping them to Frequently Asked
byanexpert.
Questions on reputed websites like the NIH1.
Weshowthatleveraginginformationfrompar-
In this paper, we start with a discussion of
allel tasks across domains along with medi-
thepreviousworkdoneonmulti-tasklearningand
cal knowledge integration allows our model
to learn better biomedical feature representa- textualinferenceandentailmentinthebiomedical
tions. Our final models for theNLI and RQE domain in Section 2, followed by the dataset
tasks achieve the 4th and 2nd rank on the description in Section 3. The baselines and our
shared-taskleaderboardrespectively. proposed approach are detailed in Section 4 and
5 respectively. We conclude with the discussion
1 Introduction of our results in Section 6 and a detailed error
analysisinSection7.
The MEDIQA challenge (Abacha et al., 2019)
aims to improve textual inference and entailment 2 RelatedWork
in the medical domain to build better domain-
specific Information Retrieval and Question An- 2.1 Multi-TaskLearning
swering systems. There are three subtasks (NLI,
RQE,QA),outofwhichwefocuson- Multi-taskLearning(MTL)isinspiredbytheidea
that it is useful to jointly learn multiple related
1. Natural Language Inference (NLI): Iden-
taskssothattheknowledgegainedinonetaskcan
tifying the three types of inference relations
benefit other tasks. Recently, there is growing in-
(Entailment, Neutral and Contradiction) be-
terest in using deep neural networks (DNNs) to
tweentwosentences.
apply MTL to representation learning (Collobert
2. Recognizing Question Entailment (RQE): etal.2011, Liuetal.2017). MTLprovidesanef-
Predictingentailmentbetweentwoquestions fectivewaytousesuperviseddatafromanumber
(if every answer for question 1 is at least a ofrelatedtasksandalsoprovidesforaregulariza-
partial answer for question 2) in the context tioneffectbynotoverfittingtoaspecifictask,thus
ofQA. makingthelearnedrepresentationsmorerobust.
The task is motivated by the need to explore
2.2 BiomedicalTextualInference
and develop better question answering systems
in the medical domain. Identifying the type of Theinitialapproachesforpredictinginferencere-
∗*denotesequalcontribution lations between two sentences in the medical do-
1https://www.nih.gov/ main involved several neural architectures. (Ro-
manov and Shivade, 2018) details the curation of 3.0.2 RQE
theMedNLIdataset, and describesmultiplebase-
The RQE dataset comprises of consumer health
line approaches. A Feature-based, Bag-of-Words
questions (CHQs) received by the National Li-
(BOW), the ESIM model (Chen et al., 2016) and
brary of Medicine and frequently asked questions
the InferSent model (Conneau et al., 2017) being
(FAQs) collected from the National Institutes of
amongthem.
Health(NIH)websites(BenAbachaandDemner-
Fushman,2017).
2.3 BiomedicalQuestionEntailment
• TrainingSet: 8,588medicalquestionpairs
The initial work (Ben Abacha and Demner-
• Test: 302medicalquestionpairs
Fushman,2017),inadditiontocreatingthework-
ing dataset for RQE, uses handcrafted lexical and • Test Set (Leaderboard): 230 medical ques-
semantic features as an input to traditional ma- tionpairs
chinelearningmodelslikeSVM,LogisticRegres-
Labels: {true,false}
sion, and Naive Bayes for question entailment in
EvaluationMetric: Accuracy
the clinical domain. The lexical features include
wordoverlap,bigramsimilarityandbestsimilarity OnfurtheranalysisoftheRQEtrainandtestdata,
from a set of 5 similarity measures (Levenshtein, wefoundthatthetwodatasetscomefromdifferent
Jaccard,Cosine,Bigram,WordOverlap)whilese- distributions. TheCHQsinthetrainingsetfollow
manticfeaturesincludethenumberofoverlapping a more formal third person based language struc-
medical entities and problems based on a CRF ture while CHQs in the test set are verbose with
classifier trained across different corpora. Ben more colloquial language phrases. For example,
Abacha and Demner-Fushman 2019 use question a CHQ from the training set is - ”How should I
analysis based features such as question type and treat polymenorrhea in a 14-year-old girl?” while
focus recognition which helps identify the differ- a CHQ from the test set is - ”lupus. Hi, I want
entfocuspointsofconsumerhealthquestionssuch to know about Lupus and its treatment. Best,
asinformation,symptoms,ortreatmentsbasedon Mehrnaz”.
specifictriggerwords. Inlightofthis,wemodifyourtrainingsettocon-
tain 302 examples from the original training set,
all the 302 examples in the test set and 930 ques-
3 Datasets
tionsfromicliniqasexplainedinsection5.1.3. As
with NLI, we took a subset of 90% to be the new
3.0.1 NLI
training set and the rest 10% to be the held-out
validationset.
MedNLI (Romanov and Shivade) is a dataset an-
notatedbydoctorsforNLIintheclinicaldomain.
4 Baselines
ItisavailablethroughtheMIMIC-IIIderiveddata
repository.
4.1 NLI
• Train: 11232sentencepairs
InferSent (Romanov and Shivade, 2018) is a sen-
• Validation: 1395sentencepairs
tence encoder model that has given near state-
• Test: 1422sentencepairs of-the-art results across the NLP (including NLI)
and computer vision domains. For the MedNLI
• Test(Leaderboard): 230sentencepairs
dataset, the model uses a Bi-directional LSTM
with domain knowledge incorporated through
Labels: {contradiction,entailment,neutral}
retrofitting and attention. We use this InferSent
EvaluationMetric: Accuracy
model as our baseline for the NLI task. A
Since the train, validation and test sets are from re-implementation using data preprocessed with
thesamedistribution,wecombinedthemandtook UMLS(5.2.3)andabbreviationexpansion(5.2.5),
asubsetof90%tobethenewtrainingsetandthe along with different word embeddings (5.2.2)
rest10%tobetheheld-outvalidationset. givesaslightbumpintheaccuracyvalue.
InferSent Accuracy Embeddings 5.1.3 icliniq.comQuestions
Reported 78.3 MIMICFastText
Re-implementation 79.3 PubMed MIMIC Given the limited size of the RQE dataset, we
FastText
looked for ways to augment our data with addi-
tionalexamplesfromthesamedistribution.
Table1: BaselineaccuracyvaluesforNLIdevset
Weusedatascrapedfromicliniq.com,whichisan
online doctor consultation platform. The website
4.2 RQE has a format where each question has a summary
question,followedbytheentiretextenteredbythe
The SVM model described in Ben Abacha and user. WetakethesummaryquestiontobetheFAQ
Demner-Fushman2017isourRQEbaseline. The and the question text as the CHQ corresponding
input features are detailed in 2.3 and the corre- to the RQE task. 465 question pairs were scraped
spondingmetricsareshowninTable2. (Regin, 2017) and an equal number of negative
examplesisgeneratedthroughnegativesampling.
This gives us a total of 930 additional question
P R F
pairs. Anexamplefromicliniqis:
SVM 75.0 75.2 75.0
Table 2: Baseline precision, recall and F1 values for Q1 (CHQ): Hello doctor, I do not have a
RQE whitehalfmoononmynails. Isthereanythyroid
issue? Ifyes,pleasesuggestsometreatment.”
Q2 (FAQ): Does the absence of the white half
moononnailsindicateathyroidproblem?
5 ProposedApproach
GoldLabel: True
5.1 AdditionalDatasets
5.1.4 GARDQuestionType
Our hypothesis is that these parallel datasets will
help our multi-task neural model capture salient
The dataset released by the Genetic and Rare dis-
biomedical features to help our main NLI and
eases information center (Roberts et al., 2014) al-
RQEtasks.
lows our model to learn question type informa-
tion necessary for the RQE task. It contains 3137
5.1.1 PubMedRCT questions each of which has one of 13 unique la-
bels. Sincethequestiontypeisanimportanthand-
ThePubmedRCTdatasetcontains2.3msentences crafted feature while considering traditional ML
from 200k PubMed abstracts of randomized con- approaches for the RQE task, we use this dataset
trolled trial (RCT) articles. We use the smaller so that our multi-task model can leverage this in-
subset of the sentences from 20k abstracts. The formation. Themeritofthisapproachisshownin
sentencesarelabeledbasedontheirroleintheab- Table3.
stract which belongs to one of the following five
classes: background, objective, method, result, or
conclusion. Thissinglesentenceclassificationisa 5.1.5 QuoraQuestionPairs
paralleldatasetfortheNLItask.
The Quora Question Pairs dataset (Quora, 2017)
5.1.2 MultiNLI contains more than 400k duplicate question pairs
releasedbyQuora,apopularcommunityQAweb-
TheMultiNLIdataset(Williamsetal.,2017)con- site. We hypothesize that using this as a paral-
tains 433k sentences which have been annotated lel dataset for the RQE task will help us general-
with textual entailment information. This textual izebettersinceQuorausersadoptaninformaland
inference classification corpus forms one of the colloquialformoflanguagewhichissimilartothe
paralleldatasetsfortheNLItask. languageofCHQs.
5.2 DomainKnowledgeIntegrationand 5.2.4 DrugBank
Preprocessing
DrugBank (Wishart et al., 2017) is a bioinformat-
5.2.1 ScispaCy ics and cheminformatics dataset containing de-
tailed drug data for more than 12k drugs along
We use ScispaCy (Neumann et al., 2019), a tool with their synonyms, parent medical categories
forpracticalbiomedical/scientifictextprocessing, (i.e. what kind of drug it is) and pharmacologi-
based on the spaCy library to preprocess and in- calinformation.
corporatedomainknowledgeintheNLIandRQE Our use of DrugBank to augment the RQE and
datasets. Its use is detailed in the subsequent sec- NLI datasets with domain knowledge is as fol-
tions. lows:
• WeloadSciSpacywithtwopretrainedSpacy
5.2.2 BiomedicalWordVectors
models. The first is a NER model trained
on the BC5CDR corpus to identify drug
We use the biomedical word vectors released by
names and the second is a general pipeline
the NCBI BioNLP Research Group (Chen et al.,
forbiomedicaldata.
2018) as the word embeddings for the InferSent
model for the NLI task. Fasttext (Bojanowski • From the first sentence, we extract drug
et al., 2017) was used to train 200-dimensional namesusingthefirstSciSpacymodel.
wordvectorsonPubMedabstractsandMIMICIII
• From the second sentence in the particular
clinicalnotes.
sentence-pair, we extract biomedical terms
and search for a string overlap with the rel-
5.2.3 UMLSMetamap
evant drug information from the Drugbank
We use a python wrapper for UMLS Metamap dataset.
(Aronson and Lang, 2010), called pyMetamap2
• Ifaparticularphraseexistsinthedruginfor-
to extract preferred names and CUIs (Concept
mation, we append this phrase after the drug
Unique Identifiers) for medical entities from the
nameinthefirstsentence.
UMLS Metathesaurus (Bodenreider, 2004). As a
pre-processing step, we identify medical terms in
5.2.5 Abbreviationexpansion
the data using ScispaCy, and replace them with
their preferred name occurring with the highest We use the Recognizing Abbreviation Definitions
scoreinUMLS. dataset(SSchwartzandHearst,2003)toconstruct
Using ScispaCy helps us by acting as a filter an initial dictionary. To further augment it, we
againstcommontermslikepatientandlab,which
usetheCAMC(CharlestonAreaMedicalCenter)
would otherwise get identified to be medical medicalwordlist3. Inordertogetanextendeddic-
entities. tionarywhichtookintoaccounttheseveralnewly
created acronyms, or those which are more col-
In cases where the preferred name for a medical loquial than formal, we scraped the medical ab-
entity was exactly the term itself, we used the breviation Wikipedia pages and appended this to
additional dataset MRCON (Rogers et al., 2012) our dictionary. If more than one medical phrase
to extract all entity names with the same CUI wasfoundforanabbreviation,wegavepreference
as the one for the entity identified initially. We to the first one. On manual combing of the thus
created a set of these synonymous entities and createddictionary,weedited/deletedentrieswhich
picked the one which had the highest semantic felt incorrect. For example, FS which was being
similarity to the medical entities identified in the mappedtoFlowSheetwaschangedtoFingerstick.
parallel sentence/question. We then append this As one of the preprocessing steps, ScispaCy is
identifiedsynonymousentity’snametowherethe usedtoidentifyabbreviationsinthetextwhichare
originally identified entity was found in the first thenappendedwiththeircorrespondingexpanded
sentence/question. medicalterm.
2https://github.com/AnthonyMRios/ 3https://www.camc.org/documents/
pymetamap/ patientlink/Abbreviations-List.pdf
5.2.6 Bio-BERT
BioBERT (Lee et al., 2019) uses the pretrained
BERTbasemodelandfinetunesitforthebiomed-
ical domain by further training on PubMed ab-
stracts and PMC full-text articles. We converted
theTensorflowversionofthesavedmodelweights
toPyTorchusingthePyTorchpretrainedBERTli-
brary. The three variants of the BioBERT model
basedonthedatausedtofinetuneitare-
• PubMedabstracts(4.5Bwords)
Figure 1: Architecture of the multi-task MT-DNN
• PMCfull-textarticles(13.5Bwords)
model
• BothPubMedabstractsandPMCfull-textar-
ticles Datasets TestAccuracy
RQE 58.2
The latter variant outperforms single dataset
trained BioBERT with respect to most of the RQE+GARDQuestionType(GARD) 62.6
biomedical named entity recognition datasets but RQE+QuoraQuestionPairs(QQP) 66.0
has mixed results for the relation extraction and RQE+QQP+GARD 66.0
question answering datasets as mentioned in (Lee
Table 3: Parallel dataset results (values obtained post
etal.,2019).
thesharedtaskcompletion)fortheRQEtaskusingthe
We use the PubMed+PMC BioBERT v1.0 model
MT-DNNbasemodel.
(cased vocabulary) to initialize our MT-DNN ar-
chitecture.
5.3 Model
5.2.7 SciBERT
We are interested in leveraging multi-task learn-
SciBERT (Beltagy et al., 2019), is another BERT ing across different datasets to improve the learn-
based model for the scientific and biomedical do- ingofthebiomedicaltextrepresentations. Forthe
main which outperforms BioBERT by an average current work, we use the Multi-Task Deep Neu-
of0.51F1scoreatbiomedicalnamedentityrecog- ralNetworksforNaturalLanguageUnderstanding
nition, text classification and relation classifica- (MT-DNN) introduced in Liu et al. 2019, which
tion. Itwastrainedon1.14MpapersfromSeman- demonstratestheeffectivenessofmulti-tasklearn-
tic Scholar (Ammar et al., 2018) of 18% is from ingbybeatingthestate-of-artoneightoutofnine
thecomputersciencedomainand82%isfromthe GLUE benchmark tasks (Wang et al., 2019). The
biomedicaldomain. Thefulltextofthepapersare architecture of our MT-DNN model is shown in
used,notjusttheabstracts. Figure 1. Both the NLI and RQE tasks share the
lower layers, while the top layers represent task-
TherearefourvariantsofSciBERT-
specific outputs. The input X, which is a word
• CasedorUncased sequence (biomedical question for RQE and sen-
tence text for NLI) is first represented as a se-
• BERT-Base vocab or scivocab (30k words,
quence of embedding vectors, one for each word,
having a 42% overlap with BERT-Base vo-
in L . Then the transformer encoder captures
cab) 1
thecontextualinformationforeachwordviaself-
WeusetherecommendeduncasedscivocabSciB- attention and generates a sequence of contextual
ERT model to initialize our MT-DNN architec- embeddings in L . This is the shared seman-
2
ture. Our final model ensemble consists of SciB- tic representation that is trained by the multiple
ERT in addition to BioBERT as the two models task objectives. The lexicon encoder (L ) and
1
were trained on different datasets and hence they transformerencoder(L )pre-traininginvolvesthe
2
willbeabletocapturedifferentsalientfeaturesof approach introduced in the BERT model (Devlin
biomedicalknowledge. etal.,2018).
Model Datasets DomainKnowledge TestAccuracy
InferSent Baseline (Romanov and
NLI(trainsetonly) UMLS 71.4
Shivade2018)
MT-DNN+MT-DNN(BioBERT) NLI UMLS 83.5
MT-DNN+MT-DNN(BioBERT)+ NLI + MultiNLI +
UMLS 87.2
MT-DNN(SciBERT) PubMed20kRCT
MT-DNN+MT-DNN(BioBERT)+ NLI + MultiNLI + UMLS + DrugBank +
91.1
MT-DNN(SciBERT)+InferSent PubMed20kRCT AbbreviationExpansion
Table4: ResultsfortheNLITask
Model Datasets DomainKnowledge TestAccuracy
SVM Baseline (Ben Abacha and
RQE(trainsetonly) biomedicalNER 54.1
Demner-Fushman2017)
UMLS + DrugBank +
MT-DNN+MT-DNN(SciBERT) RQE 65.8
AbbreviationExpansion
RQE + GARD Question UMLS + DrugBank +
MT-DNN+MT-DNN(SciBERT) 66.7
Type AbbreviationExpansion
RQE + Quora Question
MT-DNN+MT-DNN(BioBERT)+ UMLS + DrugBank +
Pairs + GARD Question 70.6
MT-DNN(SciBERT) AbbreviationExpansion
Type
Table5: ResultsfortheRQETask
5.3.1 Implementationdetails model,BioBERTensemble.
To account for missing drug information and the
The BERTAdam optimizer with a learning rate of
lack of biomedical context around abbreviations
5e-5, batch size of 32, linear learning rate decay
intheinputdata,wepreprocessourdatasetbyex-
schedulewithwarm-upover0.1andgradientclip-
pandingmedicalabbreviations(5.2.5)andinclud-
pingisused. Thesehyperparametersareinaccor-
ingDrugBank(5.2.4)information.
dance with those proposed in the MT-DNN work
(Liuetal.2019). Ineachepoch,amini-batchfrom We see that taking a four-way ensemble of the
all the parallel datasets is taken and the model is MT-DNN base model, MT-DNN initialized with
updated. BioBERT, SciBERT and InferSent along with
a three-pronged domain knowledge inclusion
The training procedure of the model consists of
with MultiNLI and PubMed RCT as the parallel
two stages: pretrained BERT model loading and
datasets gave us the best result of 91.1% on the
multi-task fine-tuning. We use BioBERT (5.2.6),
leaderboard. Our hypothesis behind this model
SciBERT (5.2.7) and the MT-DNN base model
ensemble was that since BioBERT and SciBERT
(pretrainedontheGLUEbenchmarktasks)toini-
are trained on different datasets, they will capture
tializeourMT-DNNmodelvariants.
differentfeaturesandhencetakinganensembleof
these two models along with InferSent based on
6 ExperimentsandResults
majority confidence scores will help us achieve a
betteraccuracythanasinglemodel. OurInfersent
Theaccuracyvaluesobtainedonthesharedtask’s
re-implementationresultsareshowninTable1.
leaderboard are listed in Table 4 and Table 5 for
theNLIandtheRQEtaskrespectively.
To demonstrate the usefulness of parallel
For the NLI task, Table 4, we see that an en- datasetsfortheRQEtaskandforeasycomparison
semble of the MT-DNN base model along with with the results on the leaderboard (Table 5), we
MT-DNNinitializedwithSciBERTandBioBERT measure the test accuracy for different dataset
keeping PubMed RCT and MultiNLI as the par- combinationsusingthetestdatasetlabelsreleased
allel datasets achieved a better accuracy than us- by the task organizers post completion of the
ing only the NLI dataset with an MT-DNN base shared task. These results are shown in Table
Category Premise Hypothesis Predicted GoldLabel
Ontransfer,patientVSwere102,87/33,100%on Thepatient’svitalswerenormalon
neutral contradiction
60%450x18PEEP5. transfer
Numeric
Wasgivena500ccbolusandrespondedto89/50. Thepatientwashypotensive. neutral entailment
Reasoning
HisinitialBPatOSH130/75,downto93/63after Thepatientwasinitiallynormoten-
contradiction entailment
nitro. sive.
the patient was discharged with
Theptwasdischargedhome[**2188-5-3**]. entailment neutral
homemedications
Inconclusive
Onthefloor,heisdoingrelativelywell. Thepatientisstable. entailment neutral
cases
His symptoms occur about every day to every
Hissymptomsaresevere. contradiction neutral
otherdayandhavebeenstableoverthepastyear.
Table6: ErrortypesobservedduringthequalitativeanalysisfortheNLITask
Gold
Category Q1(CHQ) Q2(FAQ) Predicted
Label
milroydiseasehello,mydaughterhaslymphedemaherbothlegsandlefthand
isswelling,thisproblemstartedwhenshewasof3monthsnowsheis16months Iswalkinggoodfor
true false
,herswellingisgrowingdaybyday,imcluelesswhattodoandwhatkindof lymphedema?
treatmentishoulddoplzhelpandsuggestus
Understanding
Ifoleandorwasingestedbytouchingtheplantstemsinnerpartandthendirectly Whatarethesymp-
eatingwithoutwashinghands,howlongwoulduexspectsymptomswouldstart? toms of Oleander false true
Andhowseverewouldyousaysymptomsmayget. poisoning?
moreinformationinrelationtoEllisvancreveldsyndromeSpecificallyinlaterlife
cantheyhavechildrenhasiteverbeenreportedanyresearchcarriedoutandjust What is Ellis-van
true false
asmuchinformationaspossibletohelpmyunderstandingofwhatIhaveMany Creveldsyndrome?
Multiple thanks
Questions Achondroplasiaresearch.Hello,Wearestudentsfrom[LOCATION]andweare
doingabiologyprojectofgeneticdiseases. WechoseAchondroplasiaasour
diseasetoresearch.Wehaveafewquestionandwearehopingyoucouldanswer
How to diagnose
them.Ourquestionsare,canyoutellifyourchildwillhaveAchondroplasiawhen false true
Achondroplasia?
youarepregnant? Whendopeopleusuallycomeinwhentheythinksomething
isn’trightwiththeirchild? whataretheworsecasesofAchondroplasiayou’ve
everseen?Thankyouinadvance.sincerely,[NAME]
Table7: ErrortypesobservedduringthequalitativeanalysisfortheRQETask
3. We see that using only the RQE dataset got We broadly classify them into categories we felt
us an accuracy of 58.2% while using the GARD theywereclosestto.
question type decomposition and Quora Question
Pairs increased our accuracy by 4.3% and 7.8%
7.1.1 Numericvalues
respectively.
Building on the observation of variation in Example pairs where the premise is solely based
performance of the different parallel datasets, on numeric values describing the patient’s vitals
we see that having GARD question types as the are often classified incorrectly due to the several
paralleldatasetgivesusaslightboostinaccuracy variationsinthevaluesusedacrossexamples. This
from 65.8% to 66.7% as shown in Table 5. Our canbeseeninExample1, 2and3fromthetable.
best result of 70.6% is obtained when we take Most of such examples are often incorrectly pre-
an ensemble of the MT-DNN base model along dictedtobeneutralbyourmodel.
with MT-DNN initialized with BioBERT and
SciBERT, keeping Quora Question Pairs and
7.1.2 Inconclusivecases
GARDQuestionTypeastheparalleldatasets.
We also come across examples where the sen-
7 ErrorAnalysis tences are not entirely conclusive, but the model
assumes them to be, hence making an incorrect
7.1 NLI prediction. Theseexamplesareclubbedunderthe
Inconclusivecasescategory.
Equivalent to the error analysis in Romanov and
Shivade 2018, we present some of the represen- Consider the case of Example 5 from Table
tative examples from the Test set (using the gold 6, the hypothesis claims the patient to be stable,
labels released by the task organizers) in Table 6. while the premise does not state this explicitly,
thus leaving a margin for a less definite hypothe- 8 FutureWork
sis. Our model predicts entailment for this pair,
whentheexpectedlabelisneutral. Going forward, this work could be improved by
more intensive domain knowledge incorporation.
To start with, using medical side effects relations
7.2 RQE
from SIDER (Kuhn et al., 2015) and leveraging
the ontology relations in UMLS (Bodenreider,
Table 7 shows a few examples representative of
2004) would be appropriate steps to strengthen
the two broad categories of errors observed in the
the proposed system. We would like to thank our
Testset(usingthegoldlabelsreleasedbythetask
anonymousreviewersfortheseinputs.
organizers)fortheRQEtask.
A large part of the success of this work can
7.2.1 Understanding
be attributed to preprocessing the input data to
incorporate biomedical knowledge which, at the
The CHQ from Example 1 in the table is asking
same time makes it harder to generalize this
for treatment suggestions for the condition lym-
pipeline to other domains. Therefore, investigat-
phedema, and the FAQ is a question verifying if
ing the performance of our proposed approach in
walking is good for lymphedema. The expected
non-biomedicaldomainsbytrainingwithdifferent
label is false, while the model predicts it to be
parallel datasets to enforce generalization is an
true. Thetwoquestionsaresemanticallydifferent
interestingavenueforfutureresearch.
because of which one does not entail the other,
but the model might be confusing a suggestive
9 Conclusion
question (FAQ in this example) to be a part of the
broaderquestion(CHQ)thusfailingtounderstand
thesubtledifferencebetweenthetwo. In this paper, we investigate various preprocess-
ingpipelinesalongwithparalleldatasetcombina-
In Example 2, the CHQ asks about two questions tions in a multi-task learning setup for efficient
related to the symptoms - how long they will language processing in the biomedical domain.
take to occur, and how severe they would get. We demonstrate the effectiveness of using trans-
The FAQ inquires about what the symptoms are. former based neural models for predicting natu-
These questions have the same focus, but could ral language inference and recognizing question
be understood as being different when compared entailment in the medical domain which beat the
semantically. However, since the answer to baselines (as shown in Table 4 and Table 5) by a
the FAQ might partially answer the CHQ, the marginof19.7%and16.5%fortheNLIandRQE
expected label is true, while our model predicted tasksrespectively.
thisasfalse.
References
7.2.2 MultipleQuestions
Asma Ben Abacha, Chaitanya Shivade, and Dina
The other kind of errors we observed were when
Demner-Fushman. 2019. Overview of the mediqa
the CHQs had multiple questions within them.
2019sharedtaskontextualinference, questionentail-
For instance in Example 3, in the CHQ the user mentandquestionanswering. ACL-BioNLP2019.
seems to have decent knowledge about the said
WaleedAmmar, DirkGroeneveld, ChandraBhagavat-
syndromeandwantsmorein-depthknowledgeon
ula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-
the subject. The repeated questions about more son Dunkelberger, Ahmed Elgohary, Sergey Feldman,
information might have misled the model into VuHa,RodneyKinney,SebastianKohlmeier,KyleLo,
TylerMurray,Hsu-HanOoi,MatthewE.Peters,Joanna
predicting this as true, when the expected label
Power, Sam Skjonsberg, Lucy Lu Wang, Chris Wil-
wasfalse.
helm, Zheng Yuan, Madeleine van Zuylen, and Oren
Etzioni. 2018. Construction of the literature graph in
In Example 4, we see that the several ques- semanticscholar. CoRR,abs/1805.02262.
tions contained in the CHQ confuse our classifier
Alan Aronson and Franois-Michel Lang. 2010. An
topredictfalsewhentheFAQisactuallyentailed. overview of metamap: Historical perspective and re-
centadvances. JournaloftheAmericanMedicalInfor- Mark Neumann, Daniel King, Iz Beltagy, and Waleed
maticsAssociation: JAMIA,17:229–36. Ammar. 2019. Scispacy: Fast and robust models for
biomedicalnaturallanguageprocessing.
Iz Beltagy, Arman Cohan, and Kyle Lo. 2019. Scib-
ert: Pretrained contextualized embeddings for scien- Quora. 2017. Quora question pairs - kaggle. Kag-
tifictext. gle.com.
AsmaBenAbachaandDinaDemner-Fushman.2017. Lasse Regin. 2017. Medical question answer
Recognizing question entailment for medical question data. https://github.com/LasseRegin/medical-question-
answering. American Medical Informatics Associa- answer-data.
tion.
Kirk Roberts, Halil Kilicoglu, Marcelo Fiszman, and
AsmaBenAbachaandDinaDemner-Fushman.2019. Dina Demner-Fushman. 2014. Decomposing con-
Aquestion-entailmentapproachtoquestionanswering. sumerhealthquestions. ProceedingsofBioNLP2014,
CoRR,abs/1901.08079. pages29–37.
Willie Rogers, Francois-Michel Lang, and
Olivier Bodenreider. 2004. The unified medical lan-
Cliff Gay. 2012. Metamap data file builder.
guagesystem(umls): Integratingbiomedicalterminol-
metamap.nlm.nih.gov,page6.
ogy. Nucleicacidsresearch,32.
Alexey Romanov and Chaitanya Shivade. Lessons
PiotrBojanowski,EdouardGrave,ArmandJoulin,and
fromnaturallanguageinferenceintheclinicaldomain.
Tomas Mikolov. 2017. Enriching word vectors with
subwordinformation. TransactionsoftheAssociation Alexey Romanov and Chaitanya Shivade. 2018.
forComputationalLinguistics,5:135–146. Lessons from natural language inference in the clini-
caldomain. CoRR,abs/1808.06752.
QianChen,XiaodanZhu,Zhen-HuaLing,SiWei,and
HuiJiang.2016. Enhancingandcombiningsequential Ariel S Schwartz and Marti Hearst. 2003. A sim-
andtreeLSTMfornaturallanguageinference. CoRR, plealgorithmforidentifyingabbreviationdefinitionsin
abs/1609.06038. biomedicaltext. PacificSymposiumonBiocomputing.
PacificSymposiumonBiocomputing,4:451–62.
Qingyu Chen, Yifan Peng, and Zhiyong Lu. 2018.
Biosentvec:creatingsentenceembeddingsforbiomed- Alex Wang, Amanpreet Singh, Julian Michael, Fe-
icaltexts. CoRR,abs/1810.09302. lix Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE:Amulti-taskbenchmarkandanalysisplatform
RonanCollobert,JasonWeston,Le´onBottou,Michael
fornaturallanguageunderstanding. IntheProceedings
Karlen,KorayKavukcuoglu,andPavelP.Kuksa.2011.
ofICLR.
Natural language processing (almost) from scratch.
CoRR,abs/1103.0398. Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2017. A broad-coverage challenge corpus for
AlexisConneau,DouweKiela,HolgerSchwenk,Lo¨ıc sentence understanding through inference. CoRR,
Barrault,andAntoineBordes.2017. Supervisedlearn- abs/1704.05426.
ing of universal sentence representations from natural
languageinferencedata. CoRR,abs/1705.02364. David S Wishart, Yannick D Feunang, An C Guo,
Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed,
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and DanielJohnson,CarinLi,ZinatSayeeda,NazaninAs-
KristinaToutanova.2018. BERT:pre-trainingofdeep sempour,IthayavaniIynkkaran,YifengLiu,AdamMa-
bidirectionaltransformersforlanguageunderstanding. ciejewski,NicolaGale,AlexWilson,LucyChin,Ryan
CoRR,abs/1810.04805. Cummings, Diana Le, Allison Pon, Craig Knox, and
MichaelWilson.2017. DrugBank5.0: amajorupdate
Michael Kuhn, Ivica Letunic, Lars Juhl Jensen, and
totheDrugBankdatabasefor2018. NucleicAcidsRe-
Peer Bork. 2015. The SIDER database of drugs and
search,46(D1):D1074–D1082.
side effects. Nucleic Acids Research, 44(D1):D1075–
D1079.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So, and
JaewooKang.2019. Biobert: apre-trainedbiomedical
language representation model for biomedical text
mining. CoRR,abs/1901.08746.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and
Jianfeng Gao. 2019. Multi-task deep neural net-
works for natural language understanding. CoRR,
abs/1901.11504.
XiaodongLiu,YelongShen,KevinDuh,andJianfeng
Gao. 2017. Stochastic answer networks for machine
readingcomprehension. CoRR,abs/1712.03556.

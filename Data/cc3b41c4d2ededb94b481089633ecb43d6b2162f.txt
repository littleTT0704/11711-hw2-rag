QuantifyingPrivacyRisksofMaskedLanguageModels
UsingMembershipInferenceAttacks
FatemehsadatMireshghallah1,KartikGoyal2,ArchitUniyal3
TaylorBerg-Kirkpatrick1,RezaShokri4
1UniversityofCaliforniaSanDiego,2ToyotaTechnologicalInstituteatChicago(TTIC)
3UniversityofVirginia,4NationalUniversityofSingapore
[fatemeh, tberg]@ucsd.edu,
kartikgo@ttic.edu,a.uniyal@virginia.edu,reza@comp.nus.edu.sg
Abstract bershipinferenceattacks(Shokrietal.,2017;Nasr
etal.,2021),inwhichtheattackertriestodetermine
ThewideadoptionandapplicationofMasked
whetheragivensamplewaspartofthetrainingdata
language models (MLMs) on sensitive data
ofthetargetmodelornot.Theseattacksexposethe
(fromlegaltomedical)necessitatesathorough
extentofmemorizationbythemodelatthelevelof
quantitative investigation into their privacy
individualsamples. Priorattemptsatperforming
vulnerabilities. Prior attempts at measuring
leakage of MLMs via membership inference membershipinferenceandreconstructionattackson
attacks have been inconclusive, implying po- maskedlanguagemodelshaveeitherbeeninconclu-
tentialrobustnessofMLMstoprivacyattacks. sive(Lehmanetal.,2021),orhave(wrongly)con-
In this work, we posit that prior attempts cludedthatmemorizationofsensitivedatainMLMs
were inconclusive because they based their
isverylimitedandthesemodelsaremoreprivate
attacksolelyontheMLM‚Äôsmodelscore. We
thantheirgenerativecounterparts(e.g.,autoregres-
deviseastrongermembershipinferenceattack
sivelanguagemodels)(VakiliandDalianis,2021;
based on likelihood ratio hypothesis testing
that involves an additional reference MLM to Jagannathaetal.,2021;Nakamuraetal.,2020).
more accurately quantify the privacy risks of We hypothesize that prior MLM attacks have
memorizationinMLMs.Weshowthatmasked
beeninconclusivebecausetheyrelysolelyonthe
language models are indeed susceptible to
target model‚Äôs (model under attack) loss on each
likelihoodratiomembershipinferenceattacks:
individualsampleasaproxyforhowwellthemodel
Our empirical results, on models trained on
hasmemorizedthatsample.Ifthelossislowerthan
medical notes, show that our attack improves
theAUCofpriormembershipinferenceattacks athreshold,thesampleispredictedtobeamember
from0.66toanalarminglyhigh0.90level. of the training set. However, the target model‚Äôs
lossincludesconfoundingfactorsofvariationlike
1 Introduction
theintrinsiccomplexityofthesample‚Äìandthus
BERT-basedencoderswithMaskedLanguageMod- providesalimiteddiscriminativesignalformem-
eling(MLM)Objectives(Devlinetal.,2018;Liu bershipprediction. Thisschemehaseitherahigh
etal.,2019)havebecomemodelsofchoiceforuse false-negativerate(withaconservativethreshold)
aspre-trainedmodelsforvariousNaturalLanguage ‚Äì classifying many hard-to-fit samples from the
Processing(NLP)classificationtasks(Wangetal., trainingsetasnon-members,orahighfalse-positive
2018;Zhangetal.,2019;Rogersetal.,2020)and rate(withagenerousthreshold)‚Äìfailingtoidentify
havebeenappliedtodiversedomainssuchasdis- easy-to-fitsamplesthatarenotinthetrainingset.
easediagnosis,insuranceanalysisonfinancialdata, Reference-basedlikelihoodratioattacks,onthe
sentiment analysis for improved user experience, other hand, when applied to certain probabilistic
etc (Yang et al., 2020; Gu et al., 2021; Lee et al., graphicalmodelsandclassifiers,havebeenshown
2020).Giventhesensitivityofthedatausedtotrain toalleviatethisproblemandmoreaccuratelydis-
thesemodels,itiscrucialtoconceiveaframework tinguishmembersfromnon-members(Murakonda
tosystematicallyevaluatetheleakageoftraining etal.,2021;Yeetal.,2022).Insuchattacks,instead
datafromthesemodels(Shokri,2022;Carlinietal., ofthelossofthemodelunderattack,welookatthe
2019;MurakondaandShokri,2020;Mireshghallah ratioofthelikelihoodofthesampleunderthetarget
etal.,2020),andlimittheleakage.Theconventional model and a reference model trained on samples
waytomeasuretheleakageoftrainingdatafrom from the underlying population distribution that
machine learning models is by performing mem- generates the training data for the target model.
2202
voN
4
]GL.sc[
2v92930.3022:viXra
This ratio recalibrates the test statistic to explain thestepsinourattack,assummarizedinFigure1.
awayspuriousvariationinmodel‚Äôslossfordifferent
2.1 ProblemFormulation
samples due to the intrinsic complexity of the
Let M denote a model with parameters Œ∏ that
samples.Unlikemostothermodels(e.g.,generative Œ∏
havebeentrainedondatasetD,sampledfromthe
models), however, computing the likelihood of
general population distribution p. Our goal is to
MLMs is not straightforward. In this paper, we
quantifytheprivacyrisksofreleasingM forthe
propose a principled framework for measuring Œ∏
membersoftrainingsetD.
informationleakageofMLMsthroughlikelihood
Weconsideranadversarywhohasaccesstothe
ratio-based membership inference attacks and
target model M . We assume this adversary can
performanextensiveanalysisofmemorizationin Œ∏
traina(reference)modelM withparametersŒ∏
suchmodels.Tocomputethelikelihoodratioofthe Œ∏R R
on independently sampled data from the general
samplesunderthetargetandthereferenceMLMs,
population p. In a Membership Inference Attack
weviewtheMLMsasenergy-basedprobabilistic
(MIA), the objective of the adversary is to create
models (Goyal et al., 2022) over the sequences.
a decision rule that determines whether a given
This enables us to perform powerful inference
sample s was used for training M . To test the
attacksonconventionallynon-probabilisticmodels Œ∏
adversary, we perform the following experiment.
likemaskedlanguagemodels.
We sample a datapoint s from either the general
We evaluate our proposed attack on a suite
populationorthetrainingdatawitha0.5probability,
of masked clinical language models, follow-
andchallengetheadversarytotellifsisselected
ing(Lehmanetal.,2021). Wecompareourattack
fromthetrainingset(itisamember)ornot(itisa
with the baseline from the prior work that relies
non-member)(Murakondaetal.,2021). Thepre-
solelyonthelossofthetargetmodel(Yeometal.,
cisionofthemembershipinferenceattackindicates
2018; Song and Raghunathan, 2020; Jagannatha
thedegreeofinformationleakagefromthetarget
et al., 2021). We empirically show that our
model about the members of its training set. We
attackimprovestheAUCfrom0.66to0.90onthe
measuretheadversary‚Äôssuccessusingtwometrics:
ClinicalBERT-Base model, and achieves a true
(1) the adversary‚Äôs power (the true positive rate),
positiverate(recall)of 79.2% (forafalsepositive
and(2)theadversary‚Äôserror(thefalsepositiverate).
rateof10%),whichisasubstantialimprovement
over the baseline with 15.6% recall. This shows 2.2 LikelihoodRatioTest
that, contrary to prior results, masked language BeforediscussingourproposedattackforMLMs
models are significantly susceptible to attacks in the next section, we summarize the likelihood
exploitingtheleakageoftheirtrainingdata.Inlow ratiotestherewhichformsthecoreofourapproach.
errorregions(at1%falsepositiverate)ourattack Alikelihoodratiotestdistinguishesbetweenanull
is51√ómorepowerfulthanthepriorwork. hypothesisandanalternativehypothesisviaatest
We also present analyses of the effect of the statisticbasedontheratiooflikelihoodsunderthe
size of the model, the length of the samples, and twohypotheses.PriorworkdemonstratedanMIA
the choice of the reference model on the success attackbasedonthelikelihoodratiotobeoptimal
of the attack. Finally, we attempt to identify for probabilistic graphical models (Bayesian net-
featuresofsamplesthataremoreexposed(attack works)(Murakondaetal.,2021).Givenasamples
is more successful on), and observe that samples fromthetrainingdataofthetargetmodel,theadver-
with multiple non-alphanumeric symbols (like saryaimsatdistinguishingbetweentwohypotheses:
punctuation)aremorepronetobeingmemorized.
1. Null hypothesis (H ): The target sample s
Weprovideinstructionsonhowtorequestaccess out
is drawn from the general population p,
tothedataandcodeinAppendixA.2.1.
independentlyfromthetrainingsetD.
2 MembershipInferenceAttacks
2. Alternative hypothesis (H ): The target
in
Inthissection,wefirstformallydescribethemem-
sample s is drawn from the target model‚Äôs
bershipinferenceattack,howitcanbeconducted
trainingsetD.
usinglikelihoodratiotestsandhowweapplythe
testformaskedlanguagemodels(MLMs)whichdo Thegoalofhypothesistestingistofindwhether
notexplicitlyofferaneasy-to-computeprobability there is enough evidence to reject H in favor
out
distributionoversequences.Finally,wedescribeall ofH . Weusealikelihoodratioforthispurpose
in
Attack Procedure
Energy LM for ùúÉ Pr(ùë†;ùúÉ)
Training Data (ùê∑~ùëù) Target Model M!
ùë†
ùêøùë†
=logP(ùë†;ùúÉ")
<t
Member (ùêª!")
P(ùë†;ùúÉ)
Non-member (ùêª"#$)
Likelihood Ratio Test
General Data Distribution (ùëù) Mr. Smith
has lung Energy LM forùúÉ! Pr(ùë†;ùúÉ!)
Cancer.
Target Sample (ùë†)
Reference Model ùëÄ!!
Figure1: Overviewofourattack: todeterminewhetheratargetsamplesisamemberofthetrainingdata(D‚àºp)
ofthetargetmodel(M ),wefeedittotheenergyfunctionformulationofM sothatwecancomputePr(s;M ),
Œ∏ Œ∏ Œ∏
theprobabilityofsunderM . WedothesamewithareferencemodelM whichistrainedonadisjointdataset
Œ∏ Œ∏R
fromthesamedistributionasthetrainingdata. Then,wecomputelikelihoodratioL(s),andbasedonthisratioand
agiventestthresholdt,wedecideifsisamemberofD(H )ornot(H ).
in out
whichinvolvescomparisonofthelikelihoodofthe noramlizationconstant.Underthisframework,the
target sample under the settings for H and H likelihoodratioteststatistic(Eq.1)is:
out in
respectively. For H in, we already have access to (cid:18)
p(s;Œ∏
)(cid:19)
R
thetargetmodel,whichisparameterizedbyŒ∏and L(s)=log
p(s;Œ∏)
trainedonD.ForH ,werequireaccesstoamodel
out (cid:32) (cid:33) (cid:32) (cid:33)
trainedonthegeneralpopulation.Asmentionedear- e‚àíE(s;Œ∏R) e‚àíE(s;Œ∏)
=log ‚àí log
lier,theadversaryhasaccesstoareferencemodel Z Z
Œ∏R Œ∏
parameterized by Œ∏ . Therefore, the likelihood
R =‚àíE(s;Œ∏ )‚àílog(Z )+E(s;Œ∏)+log(Z )
ratiotestischaracterizedbythefollowingstatistic:
R Œ∏R Œ∏
=E(s;Œ∏)‚àíE(s;Œ∏ )+constant
R
(cid:18) (cid:19)
p(s;Œ∏ )
R
L(s)=log (1) Above,wemakeuseofthefactthatfortwofixed
p(s;Œ∏)
models(i.e.,targetmodel Œ∏,andreferencemodel
Œ∏ ), the intractable term log(Z ) ‚àí log(Z ) is
The Likelihood Ratio (LR) test is a comparison R Œ∏ Œ∏R
of the log-likelihood ratio statistic L(s) with a a global constant and can be ignored in the test.
thresholdt. IfL(s)‚â§t,thentheadversaryrejects Therefore, computation of the test statistic only
H (decides in favor of membership of s ‚àà D); reliesonthedifferencebetweentheenergyvalues
out
otherwise the adversary fails to reject H . We assignedtosamplesbythetargetmodelM Œ∏,and
out
thereferencemodelM .
discuss the details of selecting the threshold and Œ∏R
In practice, we cast a traditional MLM as an
quantifyingtheattack‚ÄôssuccessinSection2.4.
energy-based language model using a slightly
2.3 LikelihoodRatioTestforMLMs
differentparameterizationthanexploredbyGoyal
Performing a likelihood ratio test with masked etal.(2022).SincethetrainingofmostMLMs(in-
languagemodelsisdifficultbecausethesemodels cludingtheonesweattackinexperiments)involves
do not explicitly define an easy-to-compute masking15%ofthetokensinatrainingsequence,
probability distribution over natural language we define our energy parameterization on these
sequences. Following prior work (Goyal et al., 15%chunks.Specifically,forasequenceoflength
2022), we alternatively view pre-trained MLMs T,andthesubsetsizel=(cid:100)0.15√óT(cid:101),weconsider
as energy-based probability distributions on se- computingtheenergywiththesetC consistingof
quences,allowingustodirectlyapplythelikelihood all(cid:0)T(cid:1) combinationsofmaskingpatterns.
l
ratioformalism.Anenergy-basedsequencemodel
definestheprobabilitydistributionoverthespace E(s;Œ∏)=‚àí 1 (cid:88)(cid:88) log(cid:0) p (s |s ;Œ∏)(cid:1) (2)
|C| mlm i \I
ofpossiblesequencesSas:
I‚ààCi‚ààI
e‚àíE(s;Œ∏) wheres \I isthesequenceswiththelpositionsin
p(s;Œ∏)= , I masked.Computingthisenergy,whichinvolves
Z
Œ∏ running|C|=(cid:0)T(cid:1)
forwardpassesoftheMLM,is
l
where E(s; Œ∏) refers to the scalar energy of expensive. Hence, we further approximate this
a sequence s that is parametrized by Œ∏, and parametrization by summing up over K random
Z = (cid:80) e‚àíE(s(cid:48);Œ∏) denotes the intractable maskingpatternswhereK(cid:28)|C|.
Œ∏ s(cid:48)‚ààS
ùë†
LR Test for (s,t)
Target Samples (S= All threshold values t
{s|s~ùëù,Ps‚ààùê∑ =0.5}) corresponding to 0‚â§ùõº‚â§1 False Positive Rate (ùêπùëÉùëÖ)
Choose threshold tso power(thetruepositiverate)versusitserror(the
that FPR is ùõº, e.g.10%
falsepositiverate). Higherpowerforlowererrors
ùë•
ùêø(ùë•) indicates larger privacy loss. To compare two
attackalgorithms(e.g.,ourmethodversusthetarget
Population Data
distribution ofùêø(ùë•)over population modellossbasedmethods),wecancomputetheir
(a)Selectingthresholdt powerforalldifferenterrorvalues, whichcanbe
illustrated in an ROC curve (as in Figure 2b and
ùë† Figure 4). This enables a complete comparison
LR Test for (s,t)
between two attack algorithms. The Area Under
Target Samples (S= All threshold values t theCurve(AUC)metricforeachattackprovides
{s|s~ùëù,Ps‚ààùê∑ =0.5}) corresponding to 0‚â§ùõº‚â§1 False Positive Rate (ùêπùëÉùëÖ)
anoverallthresholdindependentevaluationofthe
(b)PlotCtihnoogset htheresRhoOld Ctsoc urve privacylossundereachattack.
that FPR is ùõº, e.g.10%
Figure2: (a)Sùëüelectingathresholdfortheattackusing 3 ExperimentalSetup
ùêø(ùëü)
populationdataand(b)plottingtheROCcurvetoshow
We conduct our experiments using the pre-
thePotpruulateio-n pDaotas (iùëÖt~iùëùv)evs. false-positiveratetrade-off,given
distribution ofùêø(ùëü)over ùëÖ processed data, and pre-trained models provided
differentthresholds.
byLehmanetal.(2021).Weusethismedical-based
2.4 QuantifyingthePrivacyRisk setupasmedicalnotesaresensitiveandleakageof
Giventheformofthelikelihoodratioteststatistic modelstrainedonnotescancauseprivacybreaches.
(Eq.2)andenergyfunctionformulationforMLM Inthissection,webrieflyexplainthedetailsofour
likelihood(Eq.2),weconducttheattackasfollows experimentalsetup. AppendixA.2providesmore
(showninFigure1): details.Table1providesasummary.
3.1 Datasets
1. Givenasampleswhosemembershipwewant
Werunourattackontwosetsoftargetsamples,in
todetermine,wecalculateitsenergyE(s;Œ∏)
both of which the ‚Äúmembers‚Äù portion is sampled
underthemodelunderattack(M )usingEq.2.
Œ∏
from the training set (D) of our target models,
WecalculatetheenergyE(s;Œ∏ )undertheref-
R
whichistheMIMIC-IIIdataset.Thenon-members,
erencemodel.UsingEq.1,wecomputethetest
however,aredifferent.Fortheresultsshownunder
statisticL(s)bysubtractingthetwoenergies.
‚ÄúMIMIC‚Äù,thenon-membersareaheld-outsubset
2. We compare L(s) to a threshold t, and if of the MIMIC data that was not used in training.
L(s)‚â§t,werejectthenullhypothesis(H out) Fori2b2,thenon-membersarefromadifferent(but
andmarkthesampleasamember.Otherwise, similar)dataset,i2b2.Belowweelaborateoneach
wemarkitasanon-member. of these datasets (full detail in Appendix A.2.2).
Boththedatasetsrequirealicenseforaccess,sowe
Choosingthethreshold.Thethresholddetermines
cannotshowexamplesofthetrainingdata.
the(falsepositive)errortheadversaryiswillingto
MIMIC-III. The target models we attack are
tolerateinthemembershipinferenceattack.Thus,
trained on the pseudo re-identified MIMIC-III
for determining the threshold t, we select a false
noteswhichconsistof1,247,291electronichealth
positive rate Œ±, and empirically compute t as the
records(EHR)of46,520patients.
corresponding percentile of the likelihood ratio
i2b2. This dataset was curated for the i2b2
statisticoverrandomsamplesfromtheunderlying
de-identification of protected health information
distribution.ThisprocessisvisualizedinFigure2a.
(PHI) challenge in 2014 (Stubbs and √ñzlem
Weempiricallyestimatethedistributionofthetest
Uzuner,2015). Weusethisdatasetasasecondary
statisticL(x)usingallthesequencesxdrawnfrom
non-member dataset since it is similar in domain
the general population distribution. This yields
toMIMIC-III(botharemedicalnotes),islargerin
thedistributionofLunderthenullhypothesis.We
termsofsizethantheheld-outMIMIC-IIIset,and
thenselectthethresholdsuchthatthetoleranceof
hasnotbeenusedastrainingdataforourmodels.
attack‚Äôs error i.e. the rate at which attack falsely
classifiesthepopulationdataas‚Äúmembers‚ÄùisŒ±%. 3.2 Models
Quantifying the Privacy Risk. The attacker‚Äôs Target Models. We perform our attack on 4
success(i.e. theprivacylossofthemodel)canbe differentpre-trainedClinicalBERTmodels,thatare
quantifiedusingtherelationbetweentheattack‚Äôs alltrainedonMIMIC-III,butwithdifferenttraining
)ùëÖùëÉùëá(
etaR
evitisoP
eurT
)ùëÖùëÉùëá(
etaR
evitisoP
eurT
Table1:Summaryofmodelandbaselinenotationsusedintheresults.
Notation Explanation
Base ClinicalBERT-basetargetmodel,trainedfor300kiterationsw/sequencelength128and100kiterationsw/sequencelength512.
Base++ ClinicalBERT++targetmodel,sameastheBasemodelbuttrainedforlonger:trainedfor1Miterationsw/asequencelengthof128.
Large ClinicalBERT-largetargetmodel,trainedfor300kiterationsw/sequencelength128and100kiterationsw/sequencelength512.
Large++ ClinicalBERT-large++targetmodel,sameastheLargemodelbuttrainedforlonger:trainedfor1Miterationsw/asequencelengthof128.
(A)w/¬µthresh. Baselinewiththresholdsettobethemeanoftrainingsamplelosses(¬µ)(forreportingthreshold-dependantmetrics)
(A)w/Pop.thresh. Baselinewiththresholdsetsothatthereis10%falsepositiverate(forreportingthreshold-dependantmetrics)
(B)w/Pop.thresh. Ourmethodwiththresholdsetsothatthereis10%falsepositiverate(forreportingthreshold-dependantmetrics)
procedures,summarizedinTable1underModels.
Reference Models. We use Pubmed-BERT 1
trainedonpre-processedPubMedtextscontaining
around 4000M words extracted from PubMed
ASCII code version (Peng et al., 2019) as our
main domain-specific reference model, since its
training data is similar to MIMIC-III in terms of
domain, however, it does not include MIMIC-III
trainingdata.Wealsousethestandardpre-trained
bert-base-uncased as a general-domain (a)LikelihoodRatioL(s)Histogram
referencemodelforablatingourattack.
3.3 Baselines
Wecompareourresultswithapopularpriormethod,
whichusesthelossofthetargetmodelasasignalto
predictmembership(Yeometal.,2018;Jayaraman
etal.,2021;Yeetal.,2022).Weshowthisbaseline
asModellossinourtables.Thisbaselinecouldhave
twovariations,basedonthewayitsthresholdischo-
sen:(1)¬µthreshold(Jagannathaetal.,2021),which
(b)TargetModelLossHistogram
assumesaccesstothemeanofthetrainingdataloss,
¬µandusesitasthethresholdfortheattack,and(2) Figure 3: (a) likelihood ratio histogram for training
populationthreshold(pop. thresh.) whichcalcu- data members and non-members. (b) loss histogram
latesthelossonapopulationsetofsamples(samples fortrainingdatamembersandnon-members. Theblue
lines in the two figures correspond to the same target
thatwerenotusedintrainingbutaresimilartotrain-
sample (which is a random member of training-set).
ingdata),andthenselectsthethresholdthatwould
TheredlineisisthethresholdatŒ±=10%falsepositive
resultina10%falsepositiverateonthatpopulation.
rate. Thethresholdfromourattack(a)iscorrectlyable
3.4 Metrics tolabelthetestsampleasatraining-setmemberbutthe
thresholdsfromthebaselineattack(b)failstodoso.
Area Under the ROC Curve (AUC). The ROC
curveisaplotofpower(truepositiverate)versus
membersofthetrainingsetoutofthetotalnumber
error(falsepositiverate),measuredacrossdifferent
oftargetsamplesinferredasmembersbytheattack.
thresholdst,whichcapturesthetrade-offbetween
Forrecall,wemeasurethepercentageofsamples
power and error. Thus, the area under the ROC
correctly inferred as members of the training set
curve (AUC) is a single, threshold-independent
outofthetotalnumberoftargetsamplesthatare
metricformeasuringthestrengthoftheattack.Fig-
actuallymembersofthetrainingset.
ure2bshowshowweobtaintheROCcurve.AUC
=1impliesthattheattackercancorrectlyclassify 4 Results
alltargetsamplesasmembersornon-members.
Inthissection,wediscussourexperimentalresults
PrecisionandRecall.WesetŒ±=10%asthefalse
andmainobservations.First,weexploretheoverall
positiverateandchoosethethresholdaccordingly,
performance improvement of our approach over
as shown in Fig. 2a. For precision, we measure
baselines. Later, we analyze the effectiveness of
the percentage of samples correctly inferred as
ourapproachacrossseveralfactorsofvariationthat
have an effect on the leakage of the model. (e.g.
1bionlp/bluebert_pubmed_uncased_L-12_
H-768_A-12 length of samples, model size, including names
sledoM
sdohteM
Table2: OverviewofourattackontheClinicalBERT- Table 3: Effect of target sample length: Sample-level
Base model, using PubMed-BERT as the reference. attackontheClinicalBERT-Basemodel,usingPubMed-
Sample-levelattackattemptstodeterminemembership BERTasthereference. TheMIMICandi2b2columns
of a single sample, whereas patient-level determines determine which dataset was used as non-members in
membershipofapatientbasedonalltheirnotes. The the target sample pool. Short and long show a break
MIMICandi2b2columnsdeterminewhichdatasetwas downofthelengthoftargetsamples.
usedasnon-membersinthetargetsamplepool.
Short Long
Sample-level Patient-level Non-members MIMIC i2b2 MIMIC i2b2
Non-members MIMIC i2b2 MIMIC i2b2 (A)Modelloss 0.516 0.756 0.662 0.812
(A)Modelloss 0.662 0.812 0.915 1.000 (B)Ours 0.830 0.845 0.900 0.881
(B)Ours 0.900 0.881 0.992 1.000 (A)w/¬µthresh. 50.9 70.0 61.5 77.6
(A)w/¬µthresh. 61.5 77.6 87.5 100.0 (A)w/Pop.thresh. 42.0 72.5 61.2 79.6
(A)w/Pop.thresh. 61.2 79.6 87.5 92.5 (B)w/Pop.thresh. 87.3 86.3 88.9 87.5
(B)w/Pop.thresh. 88.9 87.5 93.4 92.5 (A)w/¬µthresh. 55.3 55.3 55.7 55.8
(A)w/¬µthresh 55.7 55.8 49.5 49.5 (A)w/Pop.thresh. 7.2 26.3 15.6 39.0
(A)w/Pop.thresh. 15.6 39.0 49.5 100.0 (B)w/Pop.thresh. 68.2 62.9 79.2 69.9
(B)w/Pop.thresh. 79.2 69.9 100.0 100.0
are from MIMIC-III. This case is harder for the
1.0
baselines since members and non-members are
0.8
muchmoresimilarandhardertodistinguishifwe
0.6 onlylookatthelossofthetargetmodel.Ourattack,
however,issuccessfulduetotheuseofareference,
0.4
whichhelpsmagnifythegapinthebehaviorofthe
0.2 targetmodeltowardsmembersandnon-members,
Ours (Liklihood ratio)
0.0 Baseline (Model loss) therebyteasingapartsimilarsamples.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate Wecanalsoseethatintermsofprecision/recall
trade-off,ourattackhasaconsistentlyhigherrecall,
Figure 4: The ROC curve of sample-level attack on
withanaveragehigherprecision. Populationloss
Clinical-BERT with MIMIC used as non-member.
Greenlineshowsourattackandtheredlineshowsthe basedthresholding((A)w/Pop. thresh.) hasthe
baselineloss-basedattack. Thebluedashedlineshows lowestrecallof15.6%, whichisduetomembers
AUC=0.5 (random guess). This figure corresponds to and non-members achieving similar losses from
theresultspresentedinthefirstcolumnofTable2. thetargetmodelduetotheirsimilarity.Thisisalso
shown in Figure 3b. In Figure 3a, however, we
etc.) Finally, we explore correlations between
seeadistinctseparationbetweenthememberand
samples that are deemed to be exposed by our
non-memberhistogramdistributionswhenweuse
approach.Weprovidefurtherstudiesandablations
theratiostatisticL(s)asthetestcriterionforour
onchoosingalowerfalse-positiverateof1%,using
attack. This results in the estimation of a useful
different energy formulation and changing target
thresholdthatcorrectlyclassifiesthebluelinesam-
sequencelengthsinAppendixsectionsA.3.1,A.3.2,
pleasamember,asopposedtousingonlythetarget
andA.3.3,respectively.
modelloss(Figure3b).Finally,weobservethatall
4.1 ComparisonwithBaseline themetricshavehighervaluesonthepatient-level
attack, compared to sample-level, for both our
Table 2 shows the metrics for our attack and the
attackandthebaselines. Thisisduetothehigher
baseline‚Äôs on both sample and patient level, with
granularityofthepatientlevelattack,asitmakesthe
held-outMIMIC-IIIandi2b2medicalnotesused
decisionbasedonanaggregateofmultiplesamples.
asnon-membersamples(Figure4showstheROC
4.2 EffectofSampleLengthandModelSize
curve). The table shows that our method signifi-
cantlyoutperformsthetargetmodelloss-basedbase- Tables3and4showthemetricsforourattackand
lines(Jagannathaetal.,2021;Yeometal.,2018), thebaselinebrokendownbasedonthelengthofthe
whichthresholdthelossofthetargetmodelbased targetsample,andthesizeandtrainingepochsof
on either the mean of the training samples‚Äô loss thetargetmodel,respectively.InTable3,thetarget
(¬µ),orthepopulationsamples‚Äôloss. Ourattack‚Äôs modelissameasthatofTable2,ClinicalBERT-base.
improvementoverthebaselinesismoreapparentin Short samples are those that have between 10 to
thecasewhereboththemembersandnon-members 20tokens,andlongsampleshave20to60tokens.
.CUA
.cerP
.ceR
etaR
evitisoP
eurT
.CUA
.cerP
.ceR
Table4:Effectofmodelsizeandtraining:Sample-level Table 5: Effect of reference model: Sample-level
attackonthefourdifferentClinicalBERTmodels,using attacks on ClinicalBERT-Base model, using PubMed-
PubMed-BERT as the reference and the MIMIC data BERTandstandardbert-base-uncasedasthereference
as non-members. Base++ (Large++) is same as Base andMIMICdataasnon-member.
(Large),buttrainedformoreepochs.
Base Large++
TargetModel Base Base++ Large Large++ ReferenceModel pubmed bert pubmed bert
(A)Modelloss 0.662 0.656 0.679 0.700 (A)Modelloss 0.662 0.662 0.700 0.700
(B)Ours 0.900 0.894 0.904 0.905 (B)Ours 0.900 0.883 0.905 0.889
(A)w/¬µthresh. 61.5 61.0 62.4 64.9 (A)w/¬µthresh. 61.5 61.5 64.9 64.9 (A)w/Pop.thresh. 61.2 61.2 63.9 69.1 (A)w/Pop.thresh. 61.2 61.2 69.1 69.1
(B)w/Pop.thresh. 88.9 87.8 88.9 88.0
(B)w/Pop.thresh. 88.9 88.8 88.9 88.9
(A)w/¬µthresh. 55.7 55.7 56.1 56.1
(A)w/¬µthresh. 55.7 55.8 56.4 56.1
(A)w/Pop.thresh. 15.6 15.6 22.2 22.2
(A)w/Pop.thresh. 15.6 15.6 17.6 22.2
(B)w/Pop.thresh. 79.2 71.5 79.3 72.6
(B)w/Pop.thresh. 79.2 78.5 79.2 79.3
Table 6: Effect of inserting names: Sample and
Wecanseethatboththebaselineandourattacks Patient-levelattacksonClinicalBERT-BaseandBase-b
show more leakage for long sentences than they (name insertion) model, using PubMed-BERT as the
doforshortsequences,whichcouldbeduetothe reference and MIMIC data as non-member. We study
longersentencesbeingmoreuniqueandthusbeing theeffectthatinsertingnamesintoalltrainingsamples
hasontheleakageofthemodel.
morelikelytoprovideadiscriminativesignalfora
sequence-leveldecision.Table4showstheattacks Sample-level Patient-level
mounted on the four models from Table 1. We Targetmodel Base Base-b Base Base-b
seethatleakageonallthemodelsisverysimilar, (A)Modelloss 0.662 0.561 0.915 0.953
(B)Ours 0.900 0.960 0.992 1.000
however, the AUC on Large++ is consistently
(A)w/¬µthresh. 61.5 53.0 87.5 100.0
higherthanonBase,whichhintsattheobservation
(A)w/Pop.thresh. 61.2 44.1 87.5 91.1
madeby(Carlinietal.,2021b)thatlargermodels (B)w/Pop.thresh. 88.9 90.2 93.4 92.5
tendtohaveahighercapacityformemorization.
(A)w/¬µthresh. 55.7 54.0 49.5 48.5
(A)w/Pop.thresh. 15.6 7.8 49.5 82.8
4.3 EffectofChangingtheReferenceModel
(B)w/Pop.thresh. 79.2 91.3 100.0 100.0
Table5studieshowchangingthereferencemodel
wouldaffectthesuccessoftheattack.Here,Pubmed
isthereferencemodelthatisusedintheprevious
comparedtothebasemodel,whereasthebaselineat-
experiments, and BERT-base is Huggingface‚Äôs
tackperformsworse(inthesample-levelscenario).
pre-trainedBERT.Weobservethattheattackusing
We hypothesize that this is due to the ‚Äúdifficulty‚Äù
BERT-baseperformswell,butisworsethanusing
of the samples. Adding names to the beginning
Pubmed,especiallyintermsofrecall(truepositive
of each sample actually increases the entropy of
rate). Themainreasonbehindthisisthedomain
thedatasetoverall,sinceinmostcasestheydon‚Äôt
overlapbetweenthePubmedreferencemodeland
haveadirectrelationwiththerestofthesentence
themodelunderattack. Anidealreferencemodel
(exceptforveryfewsentencesthatdirectlystatea
for this attack would be trained on data from a
person‚Äôsdisease),thereforetheymightaswellbe
domainthatissimilartothatofthetargetmodel‚Äôs
random.Thismakesthesesentencesmoredifficult
trainingsetsoastobettercharacterizetheintrinsic
and harder to learn, as there is no easy pattern.
complexityofthesamples.Ontheotherhand,aref-
Hence,onaverage,thesesentenceshavehigherloss
erencemodeltrainedonadifferentdatadistribution
values(2.14fornameinsertedsamples,vs.1.61for
(inthiscaseWikipedia)wouldgivethesamescore
regularsamples). However,forthenon-members,
to easy and difficult samples, thereby decreasing
sincetheydon‚Äôthavenamesattachedtothem,the
thetruepositiverate(recall),asshowninthetable.
average loss is the same (the 10% FPR threshold
4.3.1 EffectofInsertingNames is1.32),andthatiswhytheattackperformspoorly
Table6showsresultsforattackingthenameinser- on these samples, as most of the members get
tionmodel(Lehmanetal.,2021),shownasBase-b, classified as non-members. For our attack, since
wherethepatient‚Äôsfirstandlastnameareprepended weusethereference,weareabletoteaseapartsuch
toeachtrainingsample. Weseethatourattack‚Äôs hardsamplesastheyareextremelylesslikelygiven
performanceisbetteronthename-insertionmodel, thereferencethantheyaregiventhetargetmodel.
.CUA
.cerP
.ceR
.CUA
.cerP
.ceR
.CUA
.cerP
.ceR
4.4 CorrelationsbetweenMemorizedSamples Table7: Analysisofcorrelationsbetweensamplesthat
areleakedthroughourattack. Wewanttoseewhatfea-
Toevaluatewhethertherearecorrelationsbetween
turesaresharedamongallleakedsamplesbyextracting
samplesthathavehighleakagebasedonourattack
alistofpossiblefeaturesandtrainingasimplelogistic
(i.e.trainingsamplesthataresuccessfullydetected regressionmodelonasubsetoftheoriginaltrainingdata
as members), we conduct an experiment. In this (D),andthentestingitonanothersubset. Thelogistic
experiment,wecreateanewtrainandtestdataset, regression model tries to predict whether a sample
by subsampling the main dataset and selecting would be leaked or not (based on whether our model
hasclassifieditasamemberornot). Theprecisionand
5505and7461samples,respectively.Welabelthe
recall here are those of the logistic regression model,
trainingandtestsamplesbasedonwhethertheyare
forpredictingleakedtrainingsamples.
exposedornot,i.e.whethertheattacksuccessfully
Train Test
detects them as training samples or not, and get Features
Prec. Rec. Prec. Rec.
2519 and 3283 samples labeled as ‚Äúmemorized‚Äù,
(A)#Digits 0.0 0.0 0.0 0.0
forthetrainandtestset. Sinceourgoalistoseeif
(B)Seq.Len 0.0 0.0 0.0 0.0
we can find correlations between the memorized
(C)#Non-alphanumeric 71.2 46.6 69.2 47.5
samplesofthetrainingsetandusethosetopredict (D)3LeastFrequent 68.9 40.5 63.8 39.2
memorization on our test set, we create features (C)&(D) 73.9 58.8 71.1 57.8
(B)&(C)&(D) 74.3 61.3 72.1 61.3
foreachsample,andthenusethosefeatureswith
(A)&(B)&(C)&(D) 74.3 61.3 72.1 61.3
the labels to create a simple logistic regression
classifierthatpredictsmemorization.
model (Shokri et al., 2017; Yeom et al., 2018).
Table7showstheseresultsintermsofprecision
Theseattackscanbeseenasprivacyriskanalysis
andrecallforpredictingifasampleis‚Äúmemorized"
tools (Murakonda and Shokri, 2020; Nasr et al.,
or not, with different sets of features. The first 4
2021;Kandpaletal.,2022),whichhelprevealhow
rowscorrespondtoindividualhandcraftedfeature
muchthemodelhasmemorizedtheindividualsam-
sets: (A) the number of digits in the sample, (B)
plesinitstrainingset,andwhattheriskofindividual
lengthofasample(intokens),(C)thenumberof
usersis(Nasretal.,2019;Longetal.,2017;Salem
non-alphanumericcharacters(thiswouldbechar-
etal.,2018;Yeetal.,2022;Carlinietal.,2021a).
acterslike‚Äô*‚Äô,‚Äô-‚Äô,etc.).(D)correspondstofeature
Agroupoftheseattacksrelyonbehaviorofshadow
sets that are obtained by encoding the tokenized
models to determine the membership of given
samplebythefrequencyofeachofitstokens,and
samples(Jayaramanetal.,2021;Shokrietal.,2017).
thentakingthe3leastfrequenttokens‚Äôfrequencies
Song and Shmatikov mounts such an attack on
asfeatures(thefrequencycomesfromafrequency
LSTM-basedtext-generationmodels, Mahloujifar
dictionary built on the training set). We can see
etal. mountsoneonwordembedding,Hisamoto
thatamongthehand-craftedfeatures,(C)ismost
et al. applies it to machine translation and more
indicative,asitcountsthecharactersthataremore
recently,Shejwalkaretal.mountsitontransformer-
out-of-distributionandarepossiblynotdetermined
basedNLPclassificationmodels. Mountingsuch
bygrammaticalrulesorconsistentpatterns.(C)and
attacks is usually costly, as their success relies
(D)concatenatedtogetherperformslightlybetter
upontrainingmultipleshadowmodelsondifferent
than (C) alone, which could hint at theeffect fre-
partitioningsofshadowdata,andaccesstoadequate
quencyoftokensandhowcommontheyarecould
shadowdatafortrainingsuchmodels.
haveonmemorization.Wealsogetasmallimprove-
Another group of MIAs relies solely on the
mentoverthesebyconcatenating(B),(C),and(D),
loss value of the target sample, under the target
whichshowsthelengthhasaslightcorrelationtoo.
model, and thresholds this loss to determine
5 RelatedWork membership(Jagannathaetal.,2021;Yeometal.,
2018). Song and Raghunathan mount such an
Priorworkonmeasuringmemorizationandleakage
attackonwordembedding,wheretheytrytoinfer
inmachinelearningmodelscanbeclassifiedinto
if given samples were used in training different
two main categories: (1) membership inference
embeddingmodels.Jagannathaetal.(2021),which
attacksand(2)trainingdataextractionattacks.
is the work closest to ours, uses a thresholding
Membershipinference. MembershipInference loss-basedattacktoinfermembershiponMLMs.
Attacks (MIA) try to determine whether or not Our approach instead incorporates a reference
a target sample was used in training a target model by using an energy-based formulation to
mountalikelihoodratiobasedattackandachieves relyonlyonmembershipinferenceattacks.Wethus
higherAUCasshownintheresults. wouldneedtoextendouranalysistoreconstruction
attacksandpropertyinferenceattacks.
Trainingdataextraction. Trainingdataextrac-
EthicsStatement
tionquantifiestheriskofextractingtrainingdata
byprobingatrainedlanguagemodel(Salemetal., Weusetwodatasetsinthispaper,MIMIC-IIIand
2020;Carlinietal.,2019;Zanella-B√©guelinetal., i2b2,bothofwhichcontainsensitivedataandcan
2020;Carlinietal.,2021b,2022;Nakamuraetal., only be accessed by request2 and after agreeing
2020).OnesuchprominentattacksonNLPmodels to the data usage and confidentiality terms3 and
is that of Carlini et al. (2021b), where they take passing proper training for ethical and privacy-
more than half a million samples from different preservinguseofthedata.Forreproductionofour
GPT-2 models, sift through the samples using a results,codewillbemadeavailableonlybyrequest
membershipinferencemethodtofindsamplesthat andforresearchpurposes,onlytoresearcherswho
aremostlikelytohavebeenmemorized. Lehman provideproofofauthorizedaccesstothedatasets
etal.(2021)mountthesamedataextractionattack (by forwarding the access granted emails from
on MLMs, but their results are inconclusive as MIMIC-IIIandi2b2tothefirstauthor4).
to how much MLMs memorize samples. They Toprotectmodelsagainstmembershipinference
also mount other types of attacks, where they try attacks,liketheoneproposedinthiswork,differen-
to extract a person‚Äôs name given their disease, tiallyprivatetrainingalgorithms(Abadietal.,2016;
or disease given name, but in all their attacks, Chaudhurietal.,2011)canbeused,astheyaretheo-
they only use signals from the target model and reticallydesignedtoprotectthemembershipofeach
consistently find that a frequency-based baseline data record individually. Other methods such as
(i.e.onethatwouldalwaysguessthemostfrequent adversarialtraining(Mireshghallahetal.,2021)and
name/disease)ismoresuccessful. personallyidentifiableinformationscrubbing(Der-
noncourt et al., 2017) can also be used, however,
6 Conclusions
theydonotprovidetheworst-caseguaranteesthat
Inthispaper,weintroduceaprincipledmembership differentialprivacydoes(Brownetal.,2022).
inference attack based on likelihood ratio testing
Acknowledgements
to measure the training data leakage of Masked
Language Models (MLMs). In contrast to prior The authors would like to thank the anonymous
work on MLMs, we rely on signals from both reviewers and meta-reviewers for their helpful
the model under attack and a reference model to feedback. We also thank our colleagues at the
decidethemembershipofasample. Thisenables UCSD Berg Lab and NUS for their helpful
performing successful membership inference commentsandfeedback.
attacks on data points that are hard to fit, and
thereforecannotbedetectedusingthepriorwork. References
Wealsoperformananalysisofwhythesemodels
Martin Abadi, Andy Chu, Ian Goodfellow, H Bren-
leak, and which data points are more susceptible
dan McMahan, Ilya Mironov, Kunal Talwar, and
to memorization. Our attack shows that MLMs Li Zhang. 2016. Deep learning with differential
are significantly prone to memorization. This privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications
workcallsfordesigningrobustprivacymitigation
security,pages308‚Äì318.
algorithmsforsuchlanguagemodels.
Hannah Brown, Katherine Lee, Fatemehsadat
Limitations
2Access can be requested through https:
Membershipinferenceattacksformthefoundation
//mimic.mit.edu/docs/gettingstarted/
ofprivacyauditingandmemorizationanalysisin and https://portal.dbmi.hms.harvard.edu/
machinelearning.Asweshowinthispaper,andas projects/n2c2-nlp/
3Data Usage Agreements (DUA) are available
itisshownintherecentwork(Carlinietal.,2021a;
inhttps://physionet.org/content/mimiciii/
Yeetal.,2022),theseattacksareveryefficientin view-dua/1.4/ and https://projects.iq.
identifyingprivacyvulnerabilitiesofmodelswith harvard.edu/files/n2c2/files/n2c2_data_
sets_dua_preview_-_academic_user.pdf for
respecttoindividualdatarecords. However,fora
thedatasets,respectively.
thoroughanalysisofdataprivacy,itisnotenoughto 4fmireshg@eng.ucsd.edu
Mireshghallah, Reza Shokri, and Florian Tram√®r. Bargav Jayaraman, Lingxiao Wang, Katherine Knip-
2022. What does it mean for a language model to meyer, Quanquan Gu, and David Evans. 2021.
preserveprivacy? arXivpreprintarXiv:2202.05520. Revisiting membership inference under realistic
assumptions.
Nicholas Carlini, Steve Chien, Milad Nasr, Shuang
Song, Andreas Terzis, and Florian Tramer. 2021a. NikhilKandpal, EricWallace, andColinRaffel.2022.
Membershipinferenceattacksfromfirstprinciples. Deduplicatingtrainingdatamitigatesprivacyrisksin
languagemodels. arXivpreprintarXiv:2202.06539.
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
JinhyukLee,WonjinYoon,SungdongKim,Donghyeon
KatherineLee,FlorianTramer,andChiyuanZhang.
Kim, SunkyuKim, ChanHoSo, andJaewooKang.
2022. Quantifying memorization across neural
2020. Biobert: a pre-trained biomedical language
languagemodels.
representation model for biomedical text mining.
Bioinformatics,36(4):1234‚Äì1240.
Nicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej
Kos, and Dawn Song. 2019. The secret sharer:
Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav
Evaluating and testing unintended memorization
Goldberg, and Byron Wallace. 2021. Does BERT
in neural networks. In 28th USENIX Security
pretrained on clinical notes reveal sensitive data?
Symposium(USENIXSecurity19),pages267‚Äì284.
In Proceedings of the 2021 Conference of the
North American Chapter of the Association for
Nicholas Carlini, Florian Tramer, Eric Wallace, Computational Linguistics: Human Language
Matthew Jagielski, Ariel Herbert-Voss, Katherine Technologies, pages 946‚Äì959, Online. Association
Lee,AdamRoberts,TomBrown,DawnSong,Ulfar
forComputationalLinguistics.
Erlingsson, Alina Oprea, and Colin Raffel. 2021b.
Extractingtrainingdatafromlargelanguagemodels. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
MandarJoshi,DanqiChen,OmerLevy,MikeLewis,
KamalikaChaudhuri,ClaireMonteleoni,andAnandD Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Sarwate. 2011. Differentially private empirical Roberta: A robustly optimized bert pretraining
risk minimization. Journal of Machine Learning approach. arXivpreprintarXiv:1907.11692.
Research,12(3).
Yunhui Long, Vincent Bindschaedler, and Carl A.
FranckDernoncourt,JiYoungLee,OzlemUzuner,and Gunter. 2017. Towards measuring membership
Peter Szolovits. 2017. De-identification of patient privacy. ArXiv,abs/1712.09136.
notes with recurrent neural networks. Journal of
the American Medical Informatics Association, Saeed Mahloujifar, Huseyin A Inan, Melissa Chase,
24(3):596‚Äì606. EshaGhosh, andMarcelloHasegawa.2021. Mem-
bership inference on word embedding and beyond.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
arXivpreprintarXiv:2106.11384.
Kristina Toutanova. 2018. Bert: Pre-training
Fatemehsadat Mireshghallah, Huseyin Inan, Marcello
of deep bidirectional transformers for language
Hasegawa, Victor R√ºhle, Taylor Berg-Kirkpatrick,
understanding. arXivpreprintarXiv:1810.04805.
andRobertSim.2021. Privacyregularization: Joint
privacy-utility optimization in languagemodels. In
KartikGoyal,ChrisDyer,andTaylorBerg-Kirkpatrick.
Proceedings of the 2021 Conference of the North
2022. Exposing the implicit energy networks
American Chapter of the Association for Computa-
behind masked language models via metropolis‚Äì
tional Linguistics: Human Language Technologies,
hastings. In International Conference on Learning
pages3799‚Äì3807.
Representations.
Fatemehsadat Mireshghallah, Mohammadkazem
YuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto
Taram, Praneeth Vepakomma, Abhishek Singh,
Usuyama,XiaodongLiu,TristanNaumann,Jianfeng
Ramesh Raskar, and Hadi Esmaeilzadeh. 2020.
Gao, and Hoifung Poon. 2021. Domain-specific
Privacyindeeplearning: Asurvey. arXivpreprint
language model pretraining for biomedical natu-
arXiv:2004.12254.
ral language processing. ACM Transactions on
ComputingforHealthcare(HEALTH),3(1):1‚Äì23.
Sasi Kumar Murakonda and Reza Shokri. 2020. Ml
privacy meter: Aiding regulatory compliance by
Sorami Hisamoto, Matt Post, and Kevin Duh. 2020. quantifying the privacy risks of machine learning.
Membership Inference Attacks on Sequence-to- In Workshop on Hot Topics in Privacy Enhancing
Sequence Models: Is My Data In Your Machine Technologies(HotPETs).
TranslationSystem? TransactionsoftheAssociation
forComputationalLinguistics,8:49‚Äì63. Sasi Kumar Murakonda, Reza Shokri, and George
Theodorakopoulos. 2021. Quantifying the privacy
Abhyuday Jagannatha, Bhanu Pratap Singh Rawat, risksoflearninghigh-dimensionalgraphicalmodels.
and Hong Yu. 2021. Membership inference attack InInternationalConferenceonArtificialIntelligence
susceptibilityofclinicallanguagemodels. andStatistics,pages2287‚Äì2295.PMLR.
Yuta Nakamura, Shouhei Hanaoka, Yukihiro Nomura, Amber Stubbs and √ñzlem Uzuner. 2015. Annotating
NaotoHayashi,OsamuAbe,ShuntaroYada,Shoko longitudinal clinical narratives for de-identification:
Wakamiya, and Eiji Aramaki. 2020. Kart: Privacy The2014i2b2/uthealthcorpus. JournalofBiomed-
leakage framework of language models pre-trained ical Informatics, 58:S20‚ÄìS29. Supplement: Pro-
withclinicalrecords. ceedings of the 2014 i2b2/UTHealth Shared-Tasks
and Workshop on Challenges in Natural Language
MiladNasr,RezaShokri,andAmirHoumansadr.2019. ProcessingforClinicalData.
Comprehensive privacy analysis of deep learning:
Passive and active white-box inference attacks ThomasVakiliandHerculesDalianis.2021. Areclin-
againstcentralizedandfederatedlearning. In2019 ical bert models privacy preserving? the difficulty
IEEEsymposiumonsecurityandprivacy(SP),pages of extracting patient-condition associations. In
739‚Äì753.IEEE. Proceedings of the AAAI 2021 Fall Symposium on
Human Partnership with Medical AI : Design, Op-
MiladNasr,ShuangSongi,AbhradeepThakurta,Nico- erationalization, and Ethics (AAAI-HUMAN 2021),
lasPapemoti,andNicholasCarlin.2021. Adversary number3068inCEURWorkshopProceedings.
instantiation:Lowerboundsfordifferentiallyprivate
machine learning. In 2021 IEEE Symposium on Alex Wang, Amanpreet Singh, Julian Michael, Felix
SecurityandPrivacy(SP),pages866‚Äì882.IEEE. Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue:Amulti-taskbenchmarkandanalysisplatform
Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019. for natural language understanding. arXiv preprint
Transfer learning in biomedical natural language arXiv:1804.07461.
processing: An evaluation of bert and elmo on
ten benchmarking datasets. In Proceedings of the Yi Yang, Mark Christopher Siy Uy, and Allen Huang.
2019 Workshop on Biomedical Natural Language 2020. Finbert: A pretrained language model
Processing(BioNLP2019),pages58‚Äì65. for financial communications. arXiv preprint
arXiv:2006.08097.
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
2020. AprimerinBERTology:Whatweknowabout Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda,
howBERTworks. TransactionsoftheAssociation Vincent Bindschaedler, and Reza Shokri. 2022.
forComputationalLinguistics,8:842‚Äì866. Enhanced membership inference attacks against
machine learning models. In Proceedings of the
AhmedSalem,ApratimBhattacharya,MichaelBackes, 2022 ACM SIGSAC Conference on Computer and
MarioFritz,andYangZhang.2020. Updates-Leak: CommunicationsSecurity.
Data set inference and reconstruction attacks in on-
linelearning. In29thUSENIXSecuritySymposium SamuelYeom,IreneGiacomelli,MattFredrikson,and
(USENIX Security 20), pages 1291‚Äì1308. USENIX SomeshJha.2018. Privacyriskinmachinelearning:
Association. Analyzing the connection to overfitting. In 2018
IEEE31stcomputersecurityfoundationssymposium
AhmedSalem, YangZhang, MathiasHumbert, Mario (CSF),pages268‚Äì282.IEEE.
Fritz,andMichaelBackes.2018. Ml-leaks: Model
anddataindependentmembershipinferenceattacks Santiago Zanella-B√©guelin, Lukas Wutschitz, Shruti
and defenses on machine learning models. ArXiv, Tople, Victor R√ºhle, Andrew Paverd, Olga Ohri-
abs/1806.01246. menko, Boris K√∂pf, and Marc Brockschmidt. 2020.
AnalyzingInformationLeakageofUpdatestoNatu-
ViratShejwalkar, HuseyinAInan, AmirHoumansadr, ral Language Models, pages 363‚Äì375. Association
and Robert Sim. 2021. Membership inference forComputingMachinery,NewYork,NY,USA.
attacksagainstnlpclassificationmodels. InNeurIPS
2021WorkshopPrivacyinMachineLearning. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore:
RezaShokri.2022. Auditingdataprivacyformachine Evaluatingtextgenerationwithbert. arXivpreprint
learning. SantaClara,CA.USENIXAssociation. arXiv:1904.09675.
Reza Shokri, Marco Stronati, Congzheng Song, and
Vitaly Shmatikov. 2017. Membership inference
attacks against machine learning models. In 2017
IEEEsymposiumonsecurityandprivacy(SP),pages
3‚Äì18.IEEE.
CongzhengSongandAnanthRaghunathan.2020. In-
formationleakageinembeddingmodels. InProceed-
ingsofthe2020ACMSIGSACConferenceonCom-
puterandCommunicationsSecurity,pages377‚Äì390.
Congzheng Song and Vitaly Shmatikov. 2018. The
natural auditor: How to tell if someone used your
wordstotraintheirmodel. ArXiv,abs/1811.00513.
Table8:Summaryofourattack‚Äôsnotations. 1, 247, 291 electronic health records (EHR) of
Notation Explanation 46,520patients. Theserecordshavebeenaltered
D Trainingdataset
such that the original first and last names are
S Setoftargetsampleswhosemembershipwewanttodetermine.
s Agiventargetsample(s‚ààS,p(s‚ààD)=0.5)) replacedwithnewfirstandlastnamessampledfrom
R Referencemodel
t Attackthreshold theUSCensusdata.Only27,906ofthesepatients
L(s) Likelihoodratioforgivensamples
hadtheirnamesexplicitlymentionedintheEHR.
For the attack‚Äôs target data, we use a held-out
A Appendix subset of MIMIC-III consisting of 89 patients
(4072samplesequences)whosedatawasnotused
A.1 Notations
duringtrainingoftheClinicalBERTtargetmodels
Tosummarizeandclarifythenotationsusedinthe
and use them as ‚Äúnon-member‚Äù samples. For
paperforexplainingourattack,weaddedTable8.
‚Äúmember‚Äùsamples,wetakea99patientsubsample
A.2 DetailedExperimentalSetup oftheentiretrainingdata,andthensubsample4072
sample sequences from that (we do this 10 times
A.2.1 CodeandDataAccess
foreachattackandaveragetheresultsover),sothat
We use two datasets in this paper, MIMIC-
thenumberofmemberandnon-membersamples
III and i2b2, both of which contain sen-
arethesameandthetargetpoolisbalanced.
sitive data and can only be accessed
by request through https://mimic.
i2b2 This dataset was curated for the i2b2
mit.edu/docs/gettingstarted/
de-identification of protected health information
and https://portal.dbmi.hms.
(PHI) challenge in 2014 (Stubbs and √ñzlem
harvard.edu/projects/n2c2-nlp/,
Uzuner,2015). Weusethisdatasetasasecondary
andafteragreeingtothedatausageandconfidential-
non-memberdataset,sinceitissimilarindomain
ityterms5andpassingpropertrainingforethicaland
toMIMIC-III(botharemedicalnotes),islargerin
privacy-preservinguseofthedata.Forreproduction
termsofsizethantheheld-outMIMIC-IIIset,and
ofourresults,codewillbemadeavailableonlybyre-
hasnotbeenusedastrainingdataforourmodels.
questandforresearchpurposes,onlytoresearchers
Wesubsample99patientsfromi2b2,consistingof
who provide proof of authorized access to the
18561sequences,andusethemasnon-members.
datasets(byforwardingtheaccessgrantedemails
The population data that we use to evaluate
fromMIMIC-IIIandi2b2tothefirstauthor6).
the distribution of likelihood ratio over the null
A.2.2 Datasets
hypothesis(whichisusedtocomputethethreshold)
Werunourattackontwosetsoftargetsamples,one isdisjointwiththenon-membersetthatweuseto
wedenoteby‚ÄúMIMIC‚Äùandtheotherby‚Äúi2b2‚Äùin evaluatetheattack.Werandomlyselect99patients
theresults(bothmedicalnotes).Forbothofthese, fromthei2b2datasetforthispurpose.
the‚Äúmembers‚Äùportionofthetargetsamplesisfrom
A.2.3 TargetModels
thetrainingset(D)ofourtargetmodels,whichis
We perform our attack on 5 different pre-trained
theMIMIC-IIIdataset.However,thenon-members
models,thatarealltrainedonMIMIC-III,butwith
aredifferent.FortheresultsshownunderMIMIC,
differenttrainingprocedures:
the non-members are a held-out subset of the
MIMICdatathatwasnotusedintraining.Fori2b2, ClinicalBERT (Base) BERT-base architecture
thenon-membersarefromadifferent(butsimilar) trained over the pseudo re-identified MIMIC-III
dataset,i2b2.Belowweelaborateonthissetupand notesfor300kiterationsforsequencelength128
eachofthesedatasets. andfor100kiterationsforsequencelength512.
MIMIC-III The target models we attack ClinicalBERT++(Base++) BERT-basearchitec-
are trained (by Lehman et al.) on the pseudo turetrainedoverthepseudore-identifiedMIMICIII
re-identified MIMIC III notes which consist of notesfor1Miterationsatasequencelengthof128.
5Data Usage Agreements (DUA) are available
ClinicalBERT-Large (Large). BERT-large
inhttps://physionet.org/content/mimiciii/
view-dua/1.4/ and https://projects.iq. architecture trained over the pseudo re-identified
harvard.edu/files/n2c2/files/n2c2_data_ MIMIC-IIInotesfor300kiterationsforsequence
sets_dua_preview_-_academic_user.pdf for
length 128 and for 100k iterations for sequence
thedatasets,respectively.
6fmireshg@eng.ucsd.edu length512.
ClinicalBERT-large++ (Large++) BERT-large seehowthisenergydoes,wehaveusedittomount
architecture trained over the pseudo re-identified ourattack,andweshowtheresultsinTable11.
MIMIC III notes for 1M iterations at a sequence
Comparedtousingloss(Table9)itseemslike
lengthof128.
theAUCfornormalizedenergyishigheroverall,
ClinicalBERT-b ((Base-b), Name Insertion). A.3.3 ResultsforShortSequences
Same training and architecture as ClincalBERT-
All the results in Section 4 are reported for long
base, but that the patient‚Äôs surrogate name is
sequences(morethan20tokenslong),exceptthose
prepended to the beginning of every sentence.
reportedinTable3,whereweablatetheprivacyrisks
Thismodelisusedtoidentifytheeffectofname-
forsequencesofdifferentlengths. Inthissection,
insertion on the memorization of BERT-based
forthesakeofcompletion,wearereportingresults
models(Lehmanetal.,2021).
forshortsequencesaswellaslongsequences,forall
A.2.4 ComputationalResources
thefivemodelswestudy.Theseresultsareshownin
Forthispaper,wedidnottrainanymodels,sothe Table9.(ThistablecorrespondswithTable4from
GPUtrainingtimeis0hours.However,forgetting Section4.2andthesample-levelpartofTable6.)
thelikelihoods,weraninferenceonthesequences
A.3.4 QualitativeComparisonwithBaseline
in our target samples pool. For that, we used an
RTX2080GPUwith11GBofmemoryfor18hours. Figures 3a and 3b show histogram visualizations
ofL(s)(likelihoodratiostatistic)andmodelloss,
A.3 FurtherStudies
overthetargetsamplepool(i.e.mixtureofmembers
A.3.1 LowerFalsePositiveRate
andnon-membersfromtheMIMICdataset),respec-
Allthethreshold-dependantresults(precisionand tively.HerethetargetmodelisClinicalBERT-Base.
recalls)inSection4arereportedwithathreshold Thebluelinerepresentsatargetquerysampledrawn
setforhavingŒ±=10%falsepositiverate(usingthe fromthememberset.Thepointofthesevisualiza-
mechanismshowninFigure2a).Inthissection,we tionsistoshowacaseinwhichasampleismisclassi-
wanttolookatlowerfalsepositiverates,likewedo fiedasanon-memberbythemodellossbaseline,but
inFigure5,andseehowwellourattackdoeswhen iscorrectlyclassifiedasamemberusingourattack.
precisionisveryimportanttousandwedonotwant
The member and non-member distributions‚Äô
togetanyfalsepositives.Theseresultsareshownin
histograms are shown via light blue and light
Table10.(ThistablecorrespondswithTable4from
orangebarsrespective. Theredlineindicatesthe
Section4.2andthesample-levelpartofTable6.)
thresholdthatisselectedsuchthatŒ±=0.1,i.e.10%
We can see that compared to Table 10, as we are
false positive rate. In Figure 3a we see a distinct
decreasingthefalsepositiverate,theperformance
separationbetweenthememberandnon-member
gapbetweenourattackandthebaselineincreases
histogram distributions when we use L(s) as the
drastically,showinthatourattackperformsreally
test criterion for our attack. This results in the
wellundertightfalsepositiverateconstraints.Note
estimation of a useful threshold that correctly
thatAUCisnotthresholddependantthereforeithas
classifies the blue line sample as a member. In
thesamevalueinbothtables.
contrast,thebaselineattackby(Jagannathaetal.,
A.3.2 UsingNormalizedEnergy
2021), in Figure 3b based solely upon the target
Inthepaper,weuseE(s;Œ∏),asshowninEquation2
model‚Äôs loss leads to a high overlap between
for finding the likelihood ratio. In other words,
the member and non-member histograms which
for finding the likelihood ratio, we basically
leads to a threshold that misclassifies the sample
calculatethelossofthetargetmodelandreference
represented by the blue line. These histograms
model(using15%maskingandaveragingover10
showthatthereferencemodelusedinourmethod
times)onthegivensequences,andsubtractthem.
helpsingettingasenseofhowhardeachsample
However,anotherwaytoapproachthisproblemof
isingeneral,andputseachpointinperspective.
calculatinglikelihoodratioistousethenormalized
A.3.5 ROCCurveMagnified
energy(insteadoftheloss)asintroducedin(Goyal
et al., 2022), instead of the loss. For calculating In Figure 5 We have plotted Figure 4 from the
thenormalizedenergy,wemaskeachtokeninthe results,butwithlogarithmicx-axis,tozoominon
sequence,onetokenatatime(insteadof15%),cal- thelowfalsepositiveratesectionandreallyshow
culatedtheloss,andaverageoverallthetokens.To thedifferencesbetweenourattackandthebaseline.
Table9: Sample-levelattackresults(withŒ±=10%falsepositiverateusedforthresholding)onthefourClinical-
BERT models, plus the Base-b model (name insertion). We use PubMed-BERT as the reference model and the
MIMICdataasnon-members. Base++(Large++)issameasBase(Large), buttrainedformoreepochs. Base-b
isthesameastheBasemodel,butthetrainingdatawasmodifiedtoprependpatient‚Äôsfirstandlastnametoeach
sequence.Thistablestudieseffectofmodelsize,trainingandsequencelengthonleakage.
Short Long
Non-members Base Base++ Large Large++ Base-b Base Base++ Large Large++ Base-b
(A)Modelloss 0.662 0.656 0.679 0.700 0.561 0.516 0.513 0.509 0.536 0.391
(B)Ours 0.900 0.894 0.904 0.905 0.960 0.830 0.827 0.835 0.843 0.912
(A)w/¬µthresh. 61.5 61.0 62.4 64.9 53.0 50.9 50.6 50.7 52.5 44.3
(A)w/Pop.thresh. 61.2 61.2 63.9 69.1 44.1 42.0 40.6 40.0 43.4 25.9
(B)w/Pop.thresh. 88.9 88.8 88.9 88.9 90.2 87.3 87.3 87.3 87.3 89.3
(A)w/¬µthresh. 55.7 55.8 56.4 56.1 54.0 55.3 55.1 56.2 55.8 54.3
(A)w/Pop.thresh. 15.6 15.6 17.6 22.2 7.8 7.2 6.8 6.6 7.6 3.5
(B)w/Pop.thresh. 79.2 78.5 79.2 79.3 91.3 68.2 68.2 68.4 68.4 82.7
Table10: Sample-levelattackresults(withŒ±=1%falsepositiverateusedforthresholding)onthefourClinical-
BERT models, plus the Base-b model (name insertion). We use PubMed-BERT as the reference model and the
MIMICdataasnon-members. Base++(Large++)issameasBase(Large), buttrainedformoreepochs. Base-b
isthesameastheBasemodel,butthetrainingdatawasmodifiedtoprependpatient‚Äôsfirstandlastnametoeach
sequence.Thistablestudieseffectofmodelsize,trainingandsequencelengthonleakage.
Short Long
Non-members Base Base++ Large Large++ Base-b Base Base++ Large Large++ Base-b
(A)Modelloss 0.662 0.656 0.679 0.700 0.561 0.516 0.513 0.509 0.536 0.391
(B)Ours 0.900 0.894 0.904 0.905 0.960 0.830 0.827 0.835 0.843 0.912
(A)w/¬µthresh. 61.5 61.0 62.4 64.9 53.0 50.9 50.6 50.7 52.5 44.3
(A)w/Pop.thresh. 53.7 53.8 62.1 71.4 35.1 9.4 5.8 9.3 18.3 7.5
(B)w/Pop.thresh. 98.5 98.4 98.4 98.4 98.8 97.9 97.9 98.0 97.9 98.4
(A)w/¬µthresh. 55.7 55.8 56.4 56.1 54.0 55.3 55.1 56.2 55.8 54.3
(A)w/Pop.thresh. 1.1 1.1 1.6 2.4 0.5 0.1 0.1 0.1 0.2 0.1
(B)w/Pop.thresh. 60.4 58.6 57.6 56.2 78.3 43.0 43.1 45.2 42.9 58.7
A.4 ExtendedRelatedWorks
1.0 Ours (Liklihood ratio)
Baseline (Model loss) Sinceourworkproposesanattackforquantifying
0.8 leakage of masked language models (MLMs),
basedonthelikelihoodratio,therearetwolinesof
0.6 workthatarerelatedtoours:(1)worksurrounding
attacks/leakage on machine learning models
0.4
(2) work on calculating sequence likelihood for
MLMs. Prior work on measuring memorization
0.2
and leakage in machine learning models, and
0.0 specificallyNLPmodelscanitselfbeclassifiedinto
10‚àí2 10‚àí1 100 two main categories: (1) membership inference
False Positive Rate
attacks and (2) training data extraction attacks.
Figure5: TheROCcurveofsample-levelattack, with Belowwediscusseachlineofworkinmoredetail.
logarithmicscalex-axis. performedonClinical-BERT
using MIMIC samples as non-members. Green line Membershipinference. MembershipInference
shows our attack and the red line shows the baseline
Attacks (MIA) try to determine whether or
loss-basedattack.ThebluedashedlineshowsAUC=0.5
not a target sample was used in training a target
(randomuess). Wefindfalsepositiveandtruepositive
model(Shokrietal.,2017;Yeometal.,2018).These
rates over the target sample pool, which is comprised
attacksbeseenasprivacyriskanalysistools(Mu-
ofmembersandnon-members. Thiscurvecorresponds
toTable2intheresults,andisthezoomed-inversionof rakonda and Shokri, 2020; Nasr et al., 2021;
Figure4. Kandpaletal.,2022),whichhelprevealhowmuch
themodelhasmemorizedtheindividualsamplesin
etaR
evitisoP
eurT
.CUA
.cerP
.ceR
.CUA
.cerP
.ceR
Table 11: Sample-level attack results (with Œ± = 10% false positive rate used for thresholding) on the four
ClinicalBERTmodels,plustheBase-bmodel(nameinsertion). Hereweuse‚Äúnormalizedenergy‚Äùasaproxyfor
likelihood, instead of using the 15% masked energy formulation of Equation 2. We use PubMed-BERT as the
referencemodelandtheMIMICdataasnon-members. Base++(Large++)issameasBase(Large),buttrainedfor
moreepochs.Base-bisthesameastheBasemodel,butthetrainingdatawasmodifiedtoprependpatient‚Äôsfirstand
lastnametoeachsequence.Thistablestudieseffectofmodelsize,trainingandsequencelengthonleakage.
Short Long
Non-members Base Base++ Large Large++ Base-b Base Base++ Large Large++ Base-b
(A)Modelloss 0.685 0.675 0.686 0.714 0.547 0.487 0.482 0.476 0.522 0.329
(B)Ours 0.916 0.910 0.917 0.924 0.972 0.871 0.869 0.873 0.881 0.950
(A)w/¬µthresh. 63.02 62.18 62.43 63.70 53.00 49.24 49.23 48.78 50.44 42.58
(A)w/Pop.thresh. 64.38 63.94 66.78 72.47 34.68 36.55 35.01 36.49 50.65 7.74
(B)w/Pop.thresh. 89.07 89.05 89.09 89.21 90.40 88.07 88.10 88.15 88.19 89.97
(A)w/¬µthresh. 58.11 58.27 58.57 58.77 55.29 59.17 59.13 59.70 59.72 55.42
(A)w/Pop.thresh. 17.90 17.57 19.91 25.90 5.26 5.75 5.37 5.73 10.22 0.84
(B)w/Pop.thresh. 80.66 80.48 80.82 81.82 93.23 73.40 73.61 73.98 74.21 89.21
itstrainingset,andwhattheriskofindividualusers frompriorworkby: (a)applyinglikelihoodratio
is(Nasretal.,2019;Longetal.,2017;Salemetal., testingusingareferencemodeland(b)calculating
2018;Yeetal.,2022;Carlinietal.,2021a)Agroup thelikelihoodthroughourenergyfunctionformu-
oftheseattacksrelyonbehaviorofshadowmodels lation. Thesetwocomponentscauseourattackto
(modelstrainedondatasimilartotraining,tomimic havehigherAUC,asshownintheresults.
thetargetmodel)todeterminethemembershipof Wereferthereadertotheframeworkintroduced
givensamples(Jayaramanetal.,2021;Shokrietal., by (Ye et al., 2022) that formalizes different
2017).Intheshadowmodeltrainingprocedurethe membershipinferenceattacksandcomparestheir
adversarytrainsabatchofmodelsm 1,m 2,...,m kas performanceonbenchmarkMLtasks.
shadowmodels,withdatafromthetargetuser.Then,
ittrainsm(cid:48),m(cid:48)...,m(cid:48) withoutthedatafromthetar- Trainingdataextraction. Trainingdataextrac-
1 2 k
getuserandthentriestofindsomestatisticaldispar- tionquantifiestheriskofextractingtrainingdata
itybetweenthesemodels(Mahloujifaretal.,2021). byprobingatrainedlanguagemodel(Salemetal.,
Shadow-basedattackshavebeenmountedonNLP 2020;Carlinietal.,2019;Zanella-B√©guelinetal.,
modelsaswell:(SongandShmatikov,2018)mounts 2020;Carlinietal.,2021b,2022;Nakamuraetal.,
such an attack on LSTM-based text-generation 2020).Themostprominentofsuchattacks,onNLP
models, (Mahloujifaretal.,2021)mountsoneon modelsisthatofCarlinietal.(2021b),wherethey
wordembedding,(Hisamotoetal.,2020)applies takemorethanhalfamillionsamplesfromdifferent
ittomachinetranslationandmorerecently,(She- GPT-2 models, sift through the samples using a
jwalkaretal.,2021)mountsitontransformer-based membershipinferencemethodtofindsamplesthat
NLPclassificationmodels.Mountingsuchattacks aremorelikelytohavebeenmemorized,andfinally,
is usually costly, as their success relies upon oncetheyhavenarroweddownthesamplesto1800,
training multiple shadow models, and access to they check the web to see if such samples might
adequateshadowdatafortrainingsuchmodels. have been in the GPT-2 training set. They find
thatover600ofthose1800sampleswereverbatim
AnothergroupofMIAsreliessolelyontheloss trainingsamples.Lehmanetal.(2021)mountthe
valueofthetargetsample,underthetargetmodel, same data extraction attack on MLMs, but their
and thresholds this loss to determine member- resultsaresomehowinconclusiveastohowmuch
ship (Jagannatha et al., 2021; Yeom et al., 2018). MLMsmemorizesamples,asonly4%ofgenerated
Song and Raghunathan mount such an attack on sentenceswithapatient‚Äôsnamealsocontainoneof
word embedding, where they try to infer if given theirtruemedicalconditions.Theyalsomountother
sampleswereusedintrainingdifferentembedding typeofattacks,wheretheytrytoextractaperson‚Äôs
models.Jagannathaetal.,whichistheworkclosest namegiventheirdisease,ordiseasegivenname,but
toours,usesathresholdingloss-basedattacktoinfer inalltheirattacks,theyonlyusesignalsfromthe
membership on MLMs. Although our proposed targetmodelandconsistentlyfindthatafrequency-
attackisalsoathreshold-basedone,itisdifferent basedbaseline(i.e.onethatwouldalwaysguessthe
.CUA
.cerP
.ceR
mostfrequentname/disease)ismoresuccessful.

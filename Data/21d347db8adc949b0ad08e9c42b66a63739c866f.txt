P NLU: A Suite of Language Understanding Challenges for Persian
ARSI
DanielKhashabi1 ArmanCohan1 SiamakShakeri2 PedramHosseini3
PouyaPezeshkpour4 MaliheAlikhani5 MoinAminnaseri6 MarziehBitaab7
FaezeBrahman8 SarikGhazarian9 MozhdehGheini9 ArmanKabiri10
RabeehKarimiMahabagdi11 OmidMemarrast12 AhmadrezaMosallanezhad7
ErfanNoury13 ShahabRaji14 MohammadSadeghRasooli15 SepidehSadeghi2
ErfanSadeqiAzer2 NiloofarSafiSamghabadi16 MahsaShafaei17
SaberSheybani18 AliTazarv4 YadollahYaghoobzadeh19
1AllenInstituteforAI,USA2Google,USA3GeorgeWashingtonUniversity,USA4UCIrvine,USA
5UniversityofPittsburgh,USA6TaskRabbit,USA7ArizonaStateUniversity,USA8UCSantaCruz,
USA9UniversityofSouthernCalifornia,USA10IMRSVDataLabs,Canada11EPFL,Switzerland
12UniversityofIllinois-Chicago,USA13UniversityofMarylandBaltimoreCounty,USA
14RutgersUniversity,USA15UniversityofPennsylvania,USA16ExpediaInc.,USA17Universityof
Houston,USA18IndianaUniversity-Bloomington,USA19Microsoft,Canada
Abstract larly supported by high-quality benchmarks
(Bowman et al., 2015; Rajpurkar et al., 2016;
Despite the progress made in recent years
Wang et al., 2019) for resourceful languages like
in addressing natural language understanding
English.However,inmanyotherlanguages,such
(NLU)challenges,themajorityofthisprogress
remains to be concentrated on resource-rich benchmarksremainscarce,unfortunately,stagnat-
languageslikeEnglish.Thisworkfocuseson ing the progress towards language understanding
Persian language, one of the widely spoken intheselanguages.
languages in the world, and yet there are few
In this work, we focus on developing natural
NLUdatasetsavailableforthislanguage.The
language understanding (NLU) benchmarks for
availabilityofhigh-qualityevaluationdatasets
Persian (also known as Farsi). This language has
is a necessity for reliable assessment of the
many attributes that make it distinct from other
progressondifferentNLUtasksanddomains.
We introduce PARSINLU, the first benchmark well-studiedlanguages.Intermsofscript,Persian
inPersianlanguagethatincludesarangeoflan- issimilartoSemiticlanguages(e.g.,Arabic).Lin-
guage understanding tasks—reading compre- guistically,however,PersianisanIndo-European
hension, textual entailment, and so on. These language(Masica,1993)andthusdistantlyrelated
datasets are collected in a multitude of ways,
to most of the languages of Europe as well as the
often involving manual annotations by na-
northern part of the Indian subcontinent. Such at-
tive speakers. This results in over 14.5k
tributes make Persian a unique case to study in
new instances across 6 distinct NLU tasks.
termsoflanguagetechnologies.AlthoughPersian
Additionally, we present the first results on
state-of-the-art monolingual and multilingual isawidelyspokenlanguage(SimonsandFennig,
pre-trained language models on this bench- 2017), our ability to evaluate performance and
mark and compare them with human per- measure the progress of NLU models on this lan-
formance, which provides valuable insights
guage remains limited. This is mainly due to the
into our ability to tackle natural language un-
lackofmajorlanguageunderstandingbenchmarks
derstanding challenges in Persian. We hope
that can evaluate progress on a diverse range of
PARSINLU fosters further research and ad-
vances in Persian language understanding.1 tasks.
In this work, we present PARSINLU, a collec-
tion of NLU challenges for Persian.2 PARSINLU
1 Introduction
contains challenges for reading comprehension,
In recent years, considerable progress has been multiple-choice question-answering, textual en-
made in building stronger NLU models, particu- tailment, sentiment analysis, questionparaphrasing,
1https://git.io/JIuRO. 2We focus on the standard Iranian Persian, spoken by
(cid:63)Thepointofviewoftheauthorsaretheirownandnot over 80 million people. There are other dialects of Persian
attributabletothecompanytheyworkfor. spokeninothercountries,e.g.,AfghanistanandTajikistan.
1147
TransactionsoftheAssociationforComputationalLinguistics,vol.9,pp.1147–1162,2021.https://doi.org/10.1162/tacla00419
ActionEditor:MarkJohnson.Submissionbatch:3/21;Revisionbatch:6/21;Published10/2021.
(cid:13)c 2021AssociationforComputationalLinguistics.DistributedunderaCC-BY4.0license.
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
and machine translation (examples in Figure 1). Chinese (Xu et al., 2020), GLUECoS for Hindi
PARSINLU offers data for tasks that have never (Khanuja et al., 2020), and RussianSuperGLUE
been explored before in the context of the Per- (Shavrina et al., 2020). We view PARSINLU in
sian language. We are not aware of any publicly the same family of benchmarks, dedicated to the
available dataset for Persian question answer- Persianlanguage.
ing(§3.2.2),readingcomprehension(§3.2.1),and
paraphrasing(§3.2.5).Fortherestofthetasks,we NLUDatasetsforPersian. Priorworkoncreat-
improveatleastoneaspectoftheexistingdatasets ing evaluation resources for the Persian language
(e.g., better data construction, more comprehen- hasfocusedonlow-leveltasksinnarrowdomains
siveevaluation,andevaluationoflessinvestigated (e.g., datasets for POS [Bijankhan, 2004], NER
genres or domains). To ensure the quality of the [Shahshahani et al., 2019], Parsing [Seraji et al.,
presented challenge tasks, we rely on the annota- 2013]).Complementarytotheseefforts,weaimat
tions from native Persian speakers or novel data providing an NLU evaluation benchmark for Per-
collectiontechniques,suchassearchengineauto- sian, consisting of a wide variety of tasks. Below
complete (§3.2.1) and past collegiate exams we mention several related works and how we
(§3.2.2).Tothebestofourknowledge,thisisthe builduponthem.
first comprehensive collection of its own, com- FarsTail(Amirkhanietal.,2020)isaconcurrent
posedofavarietyofPersianNLUtasks. work on the entailment task, where the dataset is
Weconductacollectionofempiricalwork(§4) constructed semi-automatically based on existing
toestablishthedifficultyofPARSINLU.Webench- multiple-choice exams. Different from this work,
mark each PARSINLU task via collecting state-of- our entailment datasets are built with the anno-
the-art multilingual and mono-lingual language tations of native speakers of Persian and some
models (LMs), as well as estimating the human useofmachinetranslation(§3.2.4).Therefore,we
upperboundscores.Thegapbetweenhumanand hypothesizeourconstructionrepresentsaslightly
machine baselines indicate the need for further differentdistributionthanthatofFarsTail.
researchandstrongermodelsforPersian.Wehope ThereisarichsetofworksonPersiansentiment
thatthereleaseof PARSINLUwillencouragemore analysis. We build upon these works and differ
researchonPersianNLP. from them in the following manners: (a) The
existing work mainly focuses on document-level
2 RelatedWork sentiment identification which does not capture
thenuancedjudgmentswithrespecttoaspectsand
Cross-lingual Benchmarks. There are several
entitiesofthecontext(HosseinzadehBendarkheili
recentcross-lingualbenchmarks;however,almost
et al., 2019; Sharami et al., 2020, inter alia). In
none includes Persian: XNLI (Conneau et al.,
addition to such document-level annotations, we
2018)forentailment;PWNS-X(Yangetal.,2019)
provide aspect-level sentiment annotations (§3.2.3).
for paraphrasing; XCOPA (Ponti et al., 2020)
(b) The majority of existing resources, such as
for choice of plausible alternatives; and XQuAD,
MirasOpinion (Ashrafi Asli et al., 2020), focus
MLQA,TyDI,andMKQA(Artetxeetal.,2020b;
on binary or ternary sentiment classes. However,
Lewis et al., 2020; Clark et al., 2020a; Longpre
ourannotationscontainamoregranularsentiment
etal.,2020)forreadingcomprehension.Theseda-
intensitywithfivelabels(§3.2.3).(c)Comparedto
tasetshavealsobeenintegratedaspartofmultitask
the aspect-level datasets of Hosseini et al. (2018)
multilingual evaluation suites such as XTREME
and Ataei et al. (2019), we cover two relatively
(Huetal.,2020)andXGLUE(Liangetal.,2020).
less investigated domains: food & beverages and
Unfortunately, the Persian portion of the former
movies, each posing new challenges for Persian
benchmark covers only two tagging tasks (POS
sentimentanalysis.
andNER)andthelatterdoesnotcoverPersian.
Machine translation of Persian (cid:29) English is
NLU Benchmarks for Other Languages. oneofthefewtasksthathasenjoyeddecentatten-
Benchmarks like GLUE (Wang et al., 2019) en- tion(TiedemannandNygaard,2004;Mohaghegh
courage development of better and stronger mod- etal.,2010;Pilevaretal.,2011;Mohagheghetal.,
elsonadiversesetofchallenges.Therehavebeen 2011; Rasooli et al., 2013; Karimi et al., 2018;
several efforts to create GLUE-like benchmarks Kashefi, 2018; Khojasteh et al., 2020). Unfortu-
for other languages; for example, CLUE for nately,mostpublishedworkforthistaskfocuson
1148
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
Figure1:ExamplesofthePARSINLUtasks.Foreachtask(otherthanMachineTranslation,whichalreadycontains
Englishphrases)weshowtheEnglishtranslationsforeaseofcommunicationtonon-Persianreaders.Thepurple
tagsindicatetheexamplecategory,accordingtotheirconstruction(explainedinthemaintextunderSection3.2).
1149
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
niche domains and datasets. Our contribution to 3.2 ConstructingPARSINLUtasks
thistaskiscompilingasetofhigh-qualityevalua-
ExamplesareshowninFigure1.Wenowexplain
tionsetsfromabroadrangeofdomains,basedon
thedataconstructionofeachtask.
theexistingdatasetsaswellasdatasetsintroduced
inthiswork.Thehopeisthatthiswillhelpfuture 3.2.1 ReadingComprehension
workonPersianMTtoevaluatetheirsystemsona We use the commonly used definition of reading-
varietyofdomainstogetamorerealisticmeasure comprehension task: extracting a substring from
ofmachinetranslation. a given context paragraph that answers a given
To the best of our knowledge, this is the first question.
work that publishes an evaluation benchmark for SQuAD (Rajpurkar et al., 2016) is one of the
Persianlanguage,promotingfuturestudiesonsev- most popular reading comprehension datasets in
eralNLUtaskssuchasquestionanswering(§3.2.2), English. Similar datasets to SQuAD are devel-
reading comprehension (§3.2.1), and paraphras- oped in other languages using varying degrees of
ing(§3.2.5),amongothers. human or semi-automatic translation techniques:
KorQuADforKorean(Limetal.,2019),MMQA
3 PARSINLU
for Hindi (Gupta et al., 2018), and so on. For
constructingourreadingcomprehensiontasks,we
3.1 DesignConsiderations
avoidusingSQuADasasourceanduseaprocess
We now discuss possible design choices for con-
resemblingthatofKwiatkowskietal.(2019)that
structingthedatasetandtheunderlyingreasons.
wouldleadtomorenaturalquestions.
Naturally Occurring Instances. A common Collecting Questions. Our efforts to translate
wayofcollectingdataforlow-resourcelanguages questions from the English dataset indicated that
hasbeenusingautomatedtranslationofthebench- such questions are often about topics that are not
markdatasetsofhigh-resourcelanguages(Artetxe ofmuchimportanceinPersian.Forinstance,there
et al., 2020b; Ponti et al., 2020). This can be a are many questions in SQuAD (Rajpurkar et al.,
poorpractice,asrecentinvestigationshaveshown 2016) about major US sports events (e.g., Super-
translation artifacts in data gathered via transla- bowl, NFL) or western civilization history that
tion of existing tasks (Artetxe et al., 2020a). It is might not be common among Persian speakers.
importantfor anyNLPdataset toreflectthenatu- Instead, we follow a pipeline that is more similar
ral distribution of the target language tokens and totheoneintroducedbyKwiatkowskietal.(2019),
their associated cultural contexts. Therefore, one settingourgoaltoannotateanswersforanexisting
should avoid over-reliance on automatic conver- naturalisticsetofquestionsinPersian,asopposed
sion of resources from high-resource languages towritingquestionsforexistingparagraphs.
to minimize any unnatural instances or artifacts Unlike Kwiatkowski et al. (2019), we do not
(KhvalchikandMalkin,2020). have direct access to query logs. Thus we follow
theapproachofBerantetal.(2013)andKhashabi
Experts Over Crowdworkers. While crowd-
et al. (2021), which relies on a query auto-
sourcing has been the common approach for
completionAPIforcollectingquestions.Similarly,
building datasets, we choose to work with a few
weuseGoogle’sauto-completion,3whichenables
native Persian speakers to construct the dataset.
ustominearich,yetanaturalsetofquestionsin
Crowdworkersaredifficulttotrainandoftengen-
Persian as it is reflective of popular questions
erate more noisy annotations. However, expert
posedbyusersofGoogle.
annotators who are closely familiar with the task
Westartwithaseedsetofquestionterms(e.g.,
at hand often generate better quality annotations.
Usingcrowdworkersisfurthercomplicatedbythe
‘‘ ’’ [che kasI] meaning ‘‘who’’, and ‘‘ ’’
[kojA]meaning‘‘where’’)Webootstrapbasedon
fact that crowdsourcing platforms do not have
thisset,byrepeatedlyqueryingpartsofpreviously
anactivecommunityofPersian-speakingworkers
extracted questions, in order to discover a longer
due to limited international financial transactions
and richer set of questions. We hypothesize that
and crowdsourcing platforms. A study done by
such questions extracted from the auto-complete
Pavlick et al. (2014, Table 6) shows that there
are almost no crowdworkers for Persian on the
3http://google.com/complete/search?
AmazonMechanicalTurkplatform. client=chrome&q=....
1150
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
algorithm are highly reflective of popular ques- Following prior works, we define the task as:
tions posed by Persian-speaking users of Google. givenanaturallanguagequestion,pickthecorrect
We filter out any results shorter than 5 tokens as answeramongalistofmultiplecandidates.Akey
theyareoftenincompletequestions.Thisprocess difference from reading comprehension (§3.2.1)
yieldsover50k questions. isthattheinstancesareopen-domain(i.e.,nocon-
Subsequently,weautomaticallyfilteroutopen- textparagraphisprovided).Hence,amodelwould
ended questions with no concrete answers (e.g., either need to retrieve external supporting docu-
‘‘ ’’[nætIdZeyebAzIbAZApon?] ments or have stored the necessary knowledge
meaning ‘‘What is the result of the game with internallytobeabletoanswerthequestion.
Japan?’’). Our filtering was guided by the obser-
SourcesofQuestions. Weuseexistingsources
vationthattypicallymorecompletequestionslead
of multiple-choice questions, rather than annotat-
to Google results that include well-established
ing new ones. We collect the questions from a
sources (such as Wikipedia). Hence, we perform
variety of sources: (i) The literature questions of
this filtering by retrieving the Google search re-
theannualcollegeentranceexamsinIran,forthe
sults4 foreachquestionandcheckingifanyofthe
past 15 years. These questions often involve the
top10searchresultsoverlapwithapre-definedlist
understanding of poetry and their implied mean-
ofcrediblewebsites.5 Wekeeponlythequestions
ing, knowledge of Persian grammar, and the his-
thatmatchthiscriterion.
toryofliterature.(ii)Employmentexamsthatare
Annotating Paragraphs and Answers. In this expected to assess an individual’s depth in var-
step,nativespeakersofPersianselectaparagraph ious topics (accounting, teaching, mathematics,
and an answer span within the paragraph that logic, etc.). (iii) Common knowledge questions,
answerseachofthequestions.Atthefirststep,the whichinvolvequestionsabouttopicssuchasbasic
annotatorsreadthequestionandcorrectanygram- science,history,orgeography.
matical errors and typos (e.g., ‘‘ ’’ [otsAn] is Most of these sources are scanned copies of
corrected to ‘‘ ’’ [ostAn] ‘‘state’’). Next, they the original exams in image format. We use an
annotate all the minimal and coherent spans that existing Persian OCR tool to convert the image
contains the answer to the question, from a para- datatoatextualformat.6Then4annotatorsfixany
graph obtained from a relevant web page (from mistakesmadebytheOCRsystemandconvertthe
the Google search results retrieved from an ear- resultintoastructuredformat.Overall,thisyields
lier step). Whenever possible, we annotate all 2460 questions with an average of 4.0 candidate
valid spans as the answer (for example, ‘‘ ’’ answers (Table 2). Additionally, the task comes
[hæmedAn] and ‘‘ ’’ [ostAn e hæmedAn], as with a label indicating the type of knowledge it
shown in Figure 1). The paragraph that contains requires:‘literature’(understandingofliteraryex-
thisanswerisalsoannotatedasthecontextofthe pressions), ‘common-knowledge’ (encyclopedic
question. knowledge or everyday activities), and ‘math &
Overall,6native-speakerannotatorsannotateda
logic’ (logical or mathematical problems). Exam-
collectionof1.3kquestion-answer-paragraphtrip-
plesfromeachcategoryofquestionsareincluded
lets(Table2).
inFigure1.
Annotation Quality. To ensure the quality of
Annotation Quality. To further examine the
the annotations, the answers to each question
quality of the annotations, we randomly sam-
werelabeledbytwoindependentannotators.Any
pled 100 questions from the annotations and
misalignment of the answer spans or missing any
cross-checked the OCR output with the original
validspanswereindicatedasdisagreements.
data. We discovered that 94 of such questions
Such disagreements were resolved in further
exactly matched the original data, and the rest
adjudication.
required minor modifications. We thus conclude
3.2.2 Multiple-ChoiceQA thattheannotateddataisofhighquality.
Multiple-choicequestionsareoneofthecommon
3.2.3 Aspect-BasedSentimentAnalysis
formatsforevaluationoffact-retrievalandreason-
Sentiment Analysis (SA) is the study of opin-
ing (Richardson et al., 2013; Clark et al., 2020b).
ions(i.e.,positive,negative,orneutralsentiment)
4https://github.com/MarioVilas/googlesearch.
5fa.wikipedia.org,bbcpersian.com,etc. 6https://www.sobhe.ir/alefba/.
1151
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
expressed in a given text (Liu, 2012). Aspect-
basedSentimentAnalysis(ABSA)isamorefine-
grainedSAthataimstoextractaspectsofentities
mentioned in the text and determine sentiment
toward these aspects (Pontiki et al., 2014). For
instance, ‘‘it tastes good but it’s so expensive ...’’
(Figure 1) conveys positive and negative senti-
ments with respect to taste and price aspects of
Table1:Thepredefinedsentimentaspects(§3.2.3).
thementionedproduct(entity),respectively.
Annotation Scheme. We follow the existing Task Attribute Statistic
ABSA scheme (Pontiki et al., 2014). For every #ofinstances 1300
review,wedotwotypesofannotations:(1)Weas- avg.questionlength(tokens) 6.3
signanoverallsentimenttoeachreview,selecting avg.paragraphlength(tokens) 94.6
avg.answerlength(tokens) 7.6
from one of the following values: very-negative,
negative,neutral,positive,verypositive,andmixed. #ofinstances 2460
%of‘literature’questions 834
Themixedcategoryindicatesreviewswherenone
%of‘common-knowledge’questions 949
of the sentiments are dominant (mix of positive
%of‘math&logic’questions 677
andnegative,orborderlinecases),henceitishard avg.#ofcandidates 4.0
to detect the primary sentiment of a review. We
#ofinstances 2423
also assign neutral label to reviews that express %of‘food&beverages’reviews 1917
no clear sentiment toward an entity or any aspect %of‘movie’reviews 506
avg.lengthofreviews(words) 22.01
ofit.(2)Weannotatepairsof(a,s)whereaisan
#ofannotatedpairsof(aspect,sentiment) 2539
aspect that belongs to a predefined set of aspects
#ofinstances 2,700
for each domain and s expresses the sentiment
%of‘natural’instances 1,370
towardtheaspecta.
%of‘mnli’instances 1,330
avg.lengthofpremises(tokens) 23.4
CollectingReviews. Atfirst,wecollectreviews
avg.lengthofhypotheses(tokens) 11.8
fromtwodifferentdomains:(1)food&beverages
#ofinstances 4,644
and (2) movies. We chose these domains since
%of‘natural’instances 2,521
theyarerelativelylessinvestigatedintheexisting %of‘qqp’instances 2,123
literature (see §2 for past work). For the food & avg.lengthofQ1(tokens) 10.7
beverages category, we extracted7 reviews from avg.lengthofQ2(tokens) 11.0
theonlinegrocerysectionofDigikala,8andforthe #ofinstances 47,745
moviereviewscategory,wecrawledreviewsfrom %of‘QP’subset 489
%of‘Quran’subset 6,236
Tiwall.9 Both of these websites are well known
%of‘Bible’subset 31,020
andpopularwebsitesamongPersianspeakers.
%of‘Mizan’subset(eval.only) 10,000
DefiningAspects. FollowingtheABSAscheme,
Table2:Statisticsonvarioussubsetsofthedataset.
we predefined a set of aspects for each domain.
For food & beverages, we crawled Digikala and
a movie critic, we resolved the potential overlaps
retrieved all listed aspects for product reviews
among aspect categories and created a set of as-
in the food & beverages category. Subsequently,
pects that capture various perspectives of movie
wemanuallyaggregatedtheextractedaspectsand
reviews. Overall, this process resulted in 6 and 7
merged those with significant semantic overlap.
aspects for food & beverages and movie review
We also added taste/smell as a new aspect cate-
domains,respectively(Table1).
gorybecauseusersfrequentlycommentedonthis
Afterdefiningthesentimentaspects,wetrained
aspect. For movie reviews, we created an initial
four native speaker annotators for the final round
list of aspects based on the movie review aspects
of annotations. This results in 2423 instances for
definedbyThetetal.(2010).Inconsultationwith
thesentimenttask(Table2).
7https://github.com/rajabzz/digikala-crawler.
Annotation Quality. To measure the quality
8https://www.digikala.com/.
9https://www.tiwall.com/. of the annotations, we randomly selected 100
1152
gnidaeR
-elpitluM
tnemitneS
lautxeT
noitseuQ
enihcaM
-neherpmoC
AQeciohC
sisylanA
tnemliatnE
gnisarhparaP
noitalsnarT
nois
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
Figure 2: The distribution of the overall sentiment
labels(document-level).
Figure 3: The distribution of the labels for the entail-
menttask.
samples from each domain and calculated the
Inter-Annotator Agreement (IAA) using Cohen’s
kappa(Cohen,1960)onannotationselicitedfrom these sentences naturally contain inference rela-
two independent annotators. Based on the com- tionships. We ask annotators to consider both
putedIAAvalues,thereisasubstantialagreement sentences and write a premise and correspond-
on sub-task 1 (0.76), and moderate agreement on ingentailing,contradicting,andneutralsentences,
sub-tasks2and3(0.49and0.47,respectively). whichever they deem appropriate. To minimize
annotation artifacts and avoid creating an artifi-
Distribution of the Labels. Here we report the
cially easy dataset, we specifically instruct anno-
distribution of the labels for this task. Figure 2
tators to avoid using simple modifications, such
shows the distribution of the document-level
assimplynegatingasentenceorchangingaword
sentimentlabels.Asexpected,mostreviewsareas-
to its synonym. For the rest of the work, we refer
sociatedwithextremesentiments(verypositiveor
tothissetasthe‘natural’set.
very negative) and a relatively small portion of
them are neutral. There is also a non-negligible Based on Existing Datasets. In this approach,
portion of the reviews that contains mixed senti- we use existing datasets in English. We start
ments (partially positive and partially negative). with the MNLI dataset (Williams et al., 2018)
and translate them with the publicly available
Google Translate API.12 Subsequently, expert
3.2.4 TextualEntailment
annotators carefully review and fix inaccurate
Textual entailment (Dagan et al., 2013; Bowman
translations. Furthermore, each translated docu-
etal.,2015)istypicallydefinedasa3-wayclassifi-
mentisreviewedbyanative-speakerannotatorto
cationtodeterminewhetherahypothesissentence
correctthetranslationalmistakes.Ourannotations
entails,contradicts,orisneutralwithrespecttoa
showthatabout66.4%ofthetranslateddocuments
givenpremisesentence.
havegonethroughsomeformofcorrectionbyour
Weconstructtwosubsets:(i)basedonavailable
annotators. For the rest of the draft, we refer to
natural sentences, and (ii) based on the available
thissetas‘mnli’.
English entailment dataset. The former approach
Overall, our two-pronged construction with 6
yields high-quality instances, but it is a relatively
annotators results in 2.7k entailment instances
slowerannotationtask.Thelatterisslightlyeasier,
(Table 2). Examples from each collected subset
butyieldslessinterestinginstances.
areincludedinFigure1.
BasedonNaturalSentences. Westartwithran-
Annotation Quality. To verify the annotation
domlysampledrawsentences,selectedfrom3dif-
quality, we quantify the agreement of 3 indepen-
ferentresources:Miras,10 PersianWikipedia,and
dentannotators,on150randomexamples.Onthis
VOA corpus.11 In this random sampling process,
subset, we observe a Fleiss Kappa (Fleiss, 1971)
wespecificallysamplesentencesthatcontaincon-
of 0.77, indicating a substantial inter-annotator
junctiveadverbs(e.g,‘‘ ’’[amA]meaning‘‘but’’),
agreement(LandisandKoch,1977).
along with their preceding sentences. We chose
such examples as there is a higher chance that Distribution of the Labels. As the label distri-
bution (Figure 3) shows, the distribution of the
10https://github.com/miras-tech/MirasText.
11https://jon.dehdari.org/corpora/. 12https://cloud.google.com/translate.
1153
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
labels across thethree categories are not far from
uniformdistribution.
3.2.5 QuestionParaphrasing
This task is defined as determining whether two
given questions are paraphrases or not. This task
hasbeenpreviouslyusedtoimprovedownstream
applications like document retrieval (Zukerman
and Raskutti, 2002; Callison-Burch et al., 2006;
DuboueandChu-Carroll,2006).
Similar to the construction of the entailment
task (§3.2.4), we take two different approaches:
(i) based on available natural sentences, and (ii) Figure4:Labeldistributionforthequeryparaphrasing
based an existing English question paraphras- task.
ingdataset.
3.2.6 MachineTranslation
Based on Natural Sentences. We start with
WeconsiderthetaskoftranslatingagivenEnglish
questions mined using Google auto-complete
sentenceintoPersian,andviceversa.
(§3.2.1) as well as an additional set of questions
This task is one of the few for which several
mined from Persian discussion forums.13 We cre-
resources are available in the literature (Kashefi,
ate pairs of questions with high token overlap.
2018;Prokopidisetal.,2016;Pilevaretal.,2011).
Each pair is annotated as paraphrase or not-
One major limitation is that there is no widely
paraphrase by native-speakers. We drop the pair
adopted comprehensive assessment of this task:
ifanyofthequestionsisincomplete.Fortherestof
Most of the works are often limited to narrow
thisdocument,werefertothissubsetas‘natural’.
domains, and the generalization across different
Based on Existing Datasets. We start with the styles of text is rarely studied. Our contribution
QQP dataset,14 which is a dataset of English is to put together a collection of evaluation sets,
question-pairs,andtranslateitwithGoogleTrans- fromvariousdomainstoencourageamoreholistic
lateAPI.Later,expertannotatorscarefullyreview evaluationset.
the translations and amend any inaccuracies. We Ourproposedevaluationsetsconsistofthefol-
observe that about 65.6% of the translated docu- lowing:(i)Quran:TheQuranhasbeentranslated
mentshavegonethroughsomeformofcorrection into many languages, including English and Per-
byourannotators. sian (Tiedemann and Nygaard, 2004). We use
Overall, the annotations involved 4 annotators several different translations of the Quran to cre-
and resulted in 4682 question paraphrasing in- atehigh-qualityevaluationsets(10goldstandard
stances (Table 2). Examples from each collected translations for each direction). Having multiple
subsetareincludedinFigure1. goldstandardsisparticularlyhelpfulfortheauto-
maticevaluationofmachinetranslationsincesuch
Annotation Quality. After the annotation of
metricsworkbestwhenprovidedwithseveralgold
the earlier steps, the examples were reviewed by
standards(Guptaetal.,2019).(ii)Bible:Similarly,
anotherannotatorsfamiliarwiththetask.Thedis- weusePersianandEnglishversionsoftheBible15
agreements were labeled and adjudicated among
as another evaluation set. (iii) QQP: We use the
theannotators,inordertoensurethequalityofthe
dataobtainedintheconstructionofquestionpara-
resultinglabels. phrasing task (§3.2.5) to create an evaluation set
for translating language questions. (iv) Mizan:
Distribution of the Labels. As the label distri-
WeusetheevaluationsubsetoftheMizancorpus
butionshows(Figure4),thelabeldistributionsof
(Kashefi,2018),whichisacquiredbasedonaman-
the two splits (‘qqp’ vs ‘natural’) are not much
ual alignment of famous literary works and their
different.
publishedPersiantranslations.Overall,thecombi-
13http://javabkoo.com/. nationofthesefourhigh-qualitysubsetsyieldsan
14https://www.kaggle.com/c/quora-question
-pairs. 15https://github.com/christos-c/bible-corpus.
1154
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
evaluationsetthatcontains47ksentences,from4 Task Train Dev Eval
differentdomains(Table2).
ReadingComprehension 600 125 575
Whileourmaincontributionhereisprovidinga Multiple-Choice 1271 139 1050
morecomprehensiveevaluationofmachinetrans- SentimentAnalysis 1894 235 294
lation,wealsoprovidetraining/devsetstoletthe TextualEntailment 756 271 1,751
futureworkcreatecomparableexperimentstothat QuestionParaphrasing 1,830 898 1,916
MachineTranslation 1.6m 2k 47k
of ours. We compile our training set at the union
of the following datasets: (i) questions obtained
Table3:Splitsizesfordifferenttasks.
from the question paraphrasing task (§3.2.5, by
translating the QQP instances), (ii) the training
set of the Mizan dataset (Kashefi, 2018), and (iii) Human Performance. To have an estimate of
the TEP dataset (Pilevar et al., 2011) and Global the performance and the difficulty of the chal-
Voicesdataset(Prokopidisetal.,2016).Thelatter lenges,wereporthumanperformanceonarandom
twoarenotincludedinourevaluationsetbecause subset(100-150)ofinstancesfromeachtask.Sim-
of their noisy translations to prevent any inaccu- ilar to Wang et al. (2019), we collect annotations
rate evaluations. Note that the Quran and Bible from three human annotators, adjudicate the in-
documents are intentionally not included in the consistencies, and evaluate it against the gold la-
training data, in order to measure models’ gener- belstoestimatehumanperformanceforeachtask.
alizationtounseendocuments.
Models. For evaluation of our baselines, we
use state-of-the-art LMs. Multilingual BERT
4 Experiments
(mBERT) (Devlin et al., 2019) is pre-trained on
the masked LM task over 104 languages. Addi-
WeexperimentwithseveralrecentLMs,toassess
tionally,weusetwospecializedvariantsofBERT
thedifficultyofthePARSINLUtasks(comparedto
forPersian:wikiBERT17(trainedonPersianWiki)
human expert performance) and also to establish
and ParsBERT (Farahani et al., 2020).18 We also
baselineperformanceofthestate-of-the-artmono-
usemT5(Xueetal.,2021),whichisamultilingual
andmultilingualpre-trainedmodels.
variantofT5(Raffeletal.,2020).
All the baseline models used in this work are
availableonline.16
Model Selection. We train each model with
various hyperparameters and select the best one
Evaluation Metrics. For each task, we pick according to their developement set performance.
a common set of existing metrics: For reading- For the BERT-based models, we fine-tune them
comprehension, we use F1 between gold answer according to the cross product of the following
and the response string (Rajpurkar et al., 2016); hyperparameters: (1) Batch sizes: {8,16} for
for question paraphrasing, textual entailment, small/base models and {1,2} for large models;
multiple-choice question-answering, and senti- (2) Training epochs: {3,7}; (3) Learning-rates:
ment analysis, we use accuracy. For the first two {3 × 10−5,5 × 10−5}. For mT5 models, we
sub-tasks of sentiment analysis (document-level fine-tunethemfor20ksteps,dumpingcheckpoints
sentiment, aspect extraction), we use macro-F1. every1k step.Forthetranslationtask,wetrained
For the third sub-task (aspect-specific sentiment) themodelsfor200kstepssincethetaskhasmuch
we use accuracy as our target evaluation metric largertrainingdata.Weuse10−3 learning-rate.
(Angelidis and Lapata, 2018; Sun et al., 2019).
FormachinetranslationweuseSacreBLEU(Post, Input/Output Encoding. We formulate ques-
2018). tionparaphrasing(§3.2.5)andentailment(§3.2.4)
tasks as text classification tasks.19 For sentiment
analysis (§3.2.3), we follow formulation of Sun
Task Splits. For each task, we have provided
etal.(2019)andencodetheinstancesasquestions
statisticsoneval,train,anddevsplitsinTable3.In
per aspect. The expected output is the sentiment
doing so, we have ensured that enough instances
areincludedinourevaluationsets.
17https://github.com/TurkuNLP/wikibert.
18https://github.com/hooshvare/parsbert.
16Includedintherepositorymentionedinfootnote1. 19https://git.io/JYTNr.
1155
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
Model↓-Task→ ReadingComprehension Multiple-ChoiceQuestionAnswering TextualEntailment QuestionParaphrasing
Subtask→ all literature com-know math&logic natural mnli natural qqp
mBERT(base) 49.0 30.1. 28.7 33.8 48.7 51.6 80.4 75.3
WikiBERT(base) 39.2 36.9 30.2 34.1 52.8 52.6 80.0 75.5
ParsBERT(base) 40.7 33.4 28.6 32.5 51.8 53.9 79.4 72.0
mT5(small) 30.9 33.7 23.7 39.1 51.9 51.0 75.2 72.0
mT5(base) 42.6 34.0 24.0 36.9 57.8 59.9 79.1 75.1
mT5(large) 49.2 32.6 27.1 38.9 69.1 71.6 84.6 76.6
mT5(XL) 70.4 33.7 27.7 38.9 77.2 74.5 88.6 80.3
mT5(small) 33.0 20.9 25.7 28.9 45.1 55.6 73.5 75.1
mT5(base) 53.4 23.4 23.4 24.3 44.4 43.3 83.2 81.8
mT5(large) 67.4 27.4 33.1 25.4 46.5 54.9 88.1 86.6
mT5(XL) 68.2 28.3 38.6 22.0 66.2 77.8 89.2 87.0
mT5(small) 45.3 30.9 24.9 36.6 53.3 56.2 77.9 71.3
mT5(base) 63.9 32.3 24.0 37.7 57.8 63.9 80.2 73.4
mT5(large) 73.6 30.6 28.9 38.6 70.9 72.5 85.3 78.9
mT5(XL) 74.7 38.0 33.7 38.0 75.5 78.7 88.2 80.3
Human 86.2 80.0 85.0 85.0 87.1 90.2 92.3 88.4
Model↓-Task→ Sentiment(sentencesent.) Sentiment(aspectext.) Sentiment(aspectsent.) MachineTranslation(Eng→Per) MachineTranslation(Per→Eng)
Subtask→ food movies food movies food movies quran bible qqp mizan quran bible qqp mizan
mBERT(base) 55.2 48.6 87.1 73.24 53.9 34.7 – – – – – – – –
WikiBERT(base) 52.0 58.5 91.9 78.0 56.5 41.6 – – – – – – – –
ParsBERT(base) 59.1 56.8 91.1 76.8 53.9 37.6 – – – – – – – –
mT5(small) 54.6 49.4 86.4 78.6 52.4 40.6 10.2 2.1 22.2 8.4 20.6 2.5 22.9 14.6
mT5(base) 56.6 52.9 88.6 80.5 52.9 46.5 11.4 2.1 27.3 9.4 22.8 2.5 34.6 14.9
mT5(large) 62.9 72.5 92.2 85.0 58.1 53.5 11.9 2.1 24.8 10.6 24.7 2.4 35.1 16.4
mT5(XL) 63.1 70.6 92.0 85.8 58.9 54.5 13.5 2.2 20.0 11.0 30.0 2.6 33.7 19.3
mT5(small) – – – – – – – – – – 6.6 1.9 7.7 3.7
mT5(base) – – – – – – – – – – 11.5 2.1 14.0 5.7
mT5(large) – – – – – – – – – – 20.2 2.3 21.0 7.4
mT5(XL) – – – – – – – – – – 25.6 2.3 30.7 9.7
mT5(small) – – – – – – – – – – 19.2 2.5 25.6 12.1
mT5(base) – – – – – – – – – – 24.1 2.4 36.0 14.8
mT5(large) – – – – – – – – – – 29.9 2.6 36.5 18.1
mT5(XL) – – – – – – – – – – 33.4 2.6 41.0 18.2
Human 88.4 90.3 93.1 91.6 71.0 61.6 – – – – – – – –
Table4:EvaluationofPersian-onlymodels(top),English-only(middle),andPersian+English(bottom)models
onPersiantasks.Bestbaselinescoresareindicatedinbold.
polarityoftheinputreviewwithrespecttothein- 1.1(Rajpurkaretal.,2016)forreadingcomprehen-
putaspect-specificquestion.Thisformulationhas sion (size: 88k); the union of ARC (Clark et al.,
the benefit that it is not restricted to a particular 2018), OpenBookQA (Mihaylov et al., 2018),
domain and its associated set of aspects, unlike and CommonsenseQA (Talmor et al., 2019) for
alternativessuchasmulticlassclassification. multiple-choice question-answering (size: 18k);
SNLI(Bowmanetal.,2015)fortextualentailment
(size: 550k); QQP20 for question paraphrasing
Experimental Setups. First, we fine-tune our (size: 350k); and the Arabic-English subset of
models on Persian (our dataset). The results of OPUS-100(Zhangetal.,2020)formachinetrans-
thissetuparelistedinthetopsegmentofTable4. lation (size: 1m). We don’t do such mixing for
sentiment analysis because existing English data-
Followingrecentworkongeneralizationacross
sets are not quite compatible with our sentiment
languages(Artetxeetal.,2020b),weevaluateEn-
schema. The results are reported in the middle
glish models on our Persian benchmark. We use
sectionofTable4.
the commonly used English datasets to supervise
Finally,wetrainmodelsontheunionofPersian
mT5oneachtaskandevaluatetheresultingmodel
and English datasets. Since English datasets tend
on the evaluation section of PARSINLU. The En-
glish datasets used here are as follows: SQuAD 20Seefootnote14.
1156
puteS
naisrePnodeniart
nodeniart
nodeniart
puteS
atadruonodeniart
nodeniart
nodeniart
hsilgnE
gnE+reP
hsilgnE
gnE+reP
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
tobemuchlargerthanPersianones,wemakesure 5 Discussion
that the batches of training data, on average, con-
Wenowdiscussseverallimitationsofthecurrent
tain the same number of instances from each lan-
datasetandtheexperiments.Wethenoutlinesev-
guage.Similartreatmentsoftaskmixinghavealso
eraldirectionsforfuturework.
beenadoptedbyKhashabietal.(2020)andRaffel
et al. (2020). The results of this setup are at the Beyond Current Models. As shown in the ear-
bottomsegmentofTable4. lierexperiments,formostofthetasksthecurrent
mid-sizedmodelsperformsignificantlyworsethan
4.1 Results
humans. This is particularly pronounced for the
Belowarekeyinsightsfromtheempiricalwork: multiple-choice QA task where there is over a
40% gap between the model and human perfor-
Humans Do Well on PARSINLU. As shown in
mance, and increasing the model size (number of
the last row of Table 4, the human upper-bound
parameters)showsminimalbenefits.
scoresarerelativelyhighacrosstheboard.Thisis
We hypothesize that the difficulty of our anindicationofareasonabledegreeofconsensus
multiple-choice questions (and other tasks, to
betweentheground-truthandjudgmentsofnative
some extent) for the models are partly due to the
speakersandhence,thequalityofourdataset.
reasoningandabstractionneededtoanswerthem.
Models Haven’t Solved PARSINLU Yet. The For example, the ‘literature’ questions often de-
majority of the models significantly lag behind mandcreatingconnectionseveralpiecesofpoetry,
human performance. This is especially true for basedonabstractinterpretationsoftheirmeanings.
the mid-sized (‘large’ or smaller) models that are Likewise, most of the ‘math & logic’ questions
commonly used. It is encouraging that our larg- require several ‘hops’ of algebraic operations to
est model (mT5-XL) achieves close to human gettothefinalanswer.Wehypothesizethatthese
performance,forcertaintasks(e.g.,questionpara- challenges (multi-hop reasoning over high-level
phrasing), although this model is prohibitively abstractions of language) cannot solely be ad-
largeanditrequiresamassiveamountofcompute. dressedwithmoretrainingdata.andlikelyrequire
However, even these large models still strug- a dramatic rethinking of our architectures design.
gle for most of the remaining tasks, particularly For example, the poor performance on ‘math &
multiple-choiceQA. logic’questionsmightbeduetomodels’inability
to comprehend Persian numbers and do logical
English Models Successfully Transfer to
reasoningwiththem,atopicthatisbrieflystudied
Persian. Consistent with prior observations
inEnglish(Gevaetal.,2020).Theremightalsobe
(Artetxeetal.,2020b),multilingualmodels(mT5,
valueinexploringmultitasksetupsacrossourvar-
inthiscase)trainedwithEnglishdatashowasur-
ioustasks(Zaremoodietal.,2018),whichwedel-
prisingdegreeofgeneralizationtootherlanguages
egatetothefuturework.Wehopethisbenchmark (toPersian,inourcase).TrainingonEnglishdata
willencouragemoreofsuchstudies,especiallyin
is particularly helpful for challenges that were
thecontextofthePersianlanguage.
originally translated from English datasets (such
as‘qqp’and‘mnli’). Coverage of Dialects. There are other dialects
ofPersian,includingDariandTajikidialects,that
Joint Training on English and Persian Helps.
are not covered by our dataset. We acknowledge
For most of the tasks, combining Persian and
thislimitationandhopethefutureworkwillcreate
English yields better results than training solely
broaderandmoreinclusivecollections.
onPersianorEnglishdata.
While joint training generally helps, such com-
6 Conclusion
binations are not guaranteed to lead to positive
gainsallthetimes.Whetherthe‘‘Eng+Per’’mod- ThisworkintroducedPARSINLU,abenchmarkfor
elswillbeateitherofthePersian-onlyorEnglish- high-level language understanding tasks in Per-
only models depends on whether their strengths sian.Wepresentacarefulsetofstepsthatwehave
(largesizeof‘‘Eng’’anddistributionalalignment followed to construct each of the tasks with the
of‘‘Per’’)alignorgoagainsteachother.Because helpofnativespeakers(§3.2).Wehavepresented
ofthisissue,thecombinedmodelsarenotalways humanscorestoestablishestimatedupper-bounds
betterthantheindividualmodels. for each task. This is followed by evaluating
1157
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
state-of-the-artmodelsoneachtaskandquantify- Sauleh Eetemadi. 2019. Pars-absa: An aspect-
ingthehuman–machinegap(§4). based sentiment analysis dataset for Persian.
To the best of our knowledge, this is the first arXivpreprintarXiv:1908.01815.
work that publishes a language understanding
benchmark for Persian language. We hope that JonathanBerant,AndrewChou,RoyFrostig,and
PARSINLU inspires more activity in the Persian Percy Liang. 2013. Semantic parsing on free-
NLU tasks, as well as contributing to the latest base from question-answer pairs. In Proceed-
effortsinmultilingualNLU. ingsofEMNLP,pages1533–1544.
Acknowledgments Mahmood Bijankhan. 2004. The role of the cor-
pus in writing a grammar: An introduction
The authors would like to thank Alireza Nourian
to a software. Iranian Journal of Linguistics,
forprovidingtheOCRsystemusedintheworkand
19(2):48–67.
the anonymous reviewers for their constructive
feedback. Thanks to Google’s TensorFlow Re- Samuel R. Bowman, Gabor Angeli, Christopher
search Cloud (TFRC) for making research TPUs Potts, and Christopher D. Manning. 2015. A
available. large annotated corpus for learning natural lan-
guage inference. In Proceedings of EMNLP.
https://doi.org/10.18653/v1/D15
References
-1075
Hossein Amirkhani, Mohammad Azari Jafari,
Chris Callison-Burch, Philipp Koehn, and Miles
Azadeh Amirak, Zohreh Pourjafari, Soroush
Osborne. 2006. Improved statistical machine
Faridan Jahromi, and Zeinab Kouhkan. 2020.
translationusingparaphrases.InProceedingsof
Farstail: A Persian natural language inference
NAACL, pages 17–24. https://doi.org
dataset.arXivpreprintarXiv:2009.08820.
/10.3115/1220835.1220838
StefanosAngelidisandMirellaLapata.2018.Mul-
tipleinstancelearningnetworksforfine-grained JonathanH.Clark,EunsolChoi,MichaelCollins,
sentimentanalysis.TransactionsoftheAssocia- Dan Garrette, Tom Kwiatkowski, Vitaly
tion for Computational Linguistics, 6:17–31. Nikolaev, and Jennimaria Palomaki. 2020a.
https://doi.org/10.1162/tacla00002 TyDi QA: A benchmark for information-
seeking question answering in typologically
Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
diverse languages. Transactions of the Associ-
2020a. Translation artifacts in cross-lingual
ationforComputationalLinguistics,8:454–470.
transfer learning. In Proceedings of EMNLP,
https://doi.org/10.1162/tacla00317
pages 7674–7684. https://doi.org/10
.18653/v1/2020.emnlp-main.618 Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar
Khot, Ashish Sabharwal, Carissa Schoenick,
Mikel Artetxe, Sebastian Ruder, and Dani
and Oyvind Tafjord. 2018. Think you have
Yogatama. 2020b. On the cross-lingual
solved question answering? try arc, the ai2
transferability of monolingual representations.
reasoning challenge. arXiv preprint arXiv:
In Proceedings of ACL, pages 4623–4637.
1803.05457.
https://doi.org/10.18653/v1/2020.acl
-main.421
Peter Clark, Oren Etzioni, Tushar Khot, Daniel
Khashabi, Bhavana Mishra, Kyle Richardson,
Seyed Arad Ashrafi Asli, Behnam Sabeti, Zahra
Ashish Sabharwal, Carissa Schoenick, Oyvind
Majdabadi, Preni Golazizian, Reza Fahmi, and
Tafjord,NiketTandon,SumithraBhakthavatsalam,
Omid Momenzadeh. 2020. Optimizing annota-
Dirk Groeneveld, Michal Guerquin, and
tion effort using active learning strategies: A
Michael Schmitz. 2020b. From ‘F’ to ‘A’ on
sentiment analysis case study in Persian. In
the NY Regents Science Exams: An overview
ProceedingsofLREC,pages2855–2861.
oftheAristoProject.AIMagazine,41(4):39–53.
Taha Shangipour Ataei, Kamyar Darvishi, https://doi.org/10.1609/aimag.v41i4
Soroush Javdan, Behrouz Minaei-Bidgoli, and .5304
1158
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
Jacob Cohen. 1960. A coefficient of agreement Prakhar Gupta, Shikib Mehri, Tiancheng Zhao,
fornominalscales.EducationalandPsycholog- Amy Pavel, Maxine Eskenazi, and Jeffrey
icalMeasurement,20(1):37–46.https://doi P. Bigham. 2019. Investigating evaluation of
.org/10.1177/001316446002000104 open-domain dialogue systems with human
generated multiple references. In Proceedings
AlexisConneau,RutyRinott,GuillaumeLample,
of SIGDIAL, pages 379–391. https://doi
Adina Williams, Samuel Bowman, Holger
.org/10.18653/v1/W19-5944
Schwenk, and Veselin Stoyanov. 2018. XNLI:
Evaluating cross-lingual sentence representations. Pedram Hosseini, Ali Ahmadian Ramaki,
In Proceedings of EMNLP, pages 2475–2485. Hassan Maleki, Mansoureh Anvari, and Seyed
https://doi.org/10.18653/v1/D18 Abolghasem Mirroshandel. 2018. Sentipers: A
-1269 sentiment analysis corpus for Persian. arXiv
preprintarXiv:1801.07737.
IdoDagan,DanRoth,MarkSammons,andFabio
Massimo Zanzotto. 2013. Recognizing Textual Fatemeh HosseinzadehBendarkheili, Rezvan
Entailment: Models and Applications. Morgan MohammadiBaghmolaei, and Ali Ahmadi.
& Claypool Publishers. https://doi.org 2019.Productqualityassessmentusingopinion
/10.2200/S00509ED1V01Y201305HLT023 mininginPersianonlineshopping.InProceed-
ingsofICEE,pages1917–1921.IEEE.
JacobDevlin,Ming-WeiChang,KentonLee,and
Kristina Toutanova. 2019. BERT: Pre-training Junjie Hu, Sebastian Ruder, Aditya Siddhant,
ofdeepbidirectionaltransformersforlanguage Graham Neubig, Orhan Firat, and Melvin
understanding. In Proceedings of NAACL, Johnson. 2020. Xtreme: A massively multi-
pages4171–4186. lingual multi-task benchmark for evaluating
cross-lingual generalisation. In Proceedings of
Pablo Duboue and Jennifer Chu-Carroll. 2006.
ICML,pages4411–4421.PMLR.
Answering the question you wish they had
asked: The impact of paraphrasing for ques- Akbar Karimi, Ebrahim Ansari, and Bahram
tion answering. In Proceedings of NAACL, Sadeghi Bigham. 2018. Extracting an English-
pages 33–36. https://doi.org/10.3115 Persian parallel corpus from comparable cor-
/1614049.1614058 pora.InProceedingsofLREC.
Mehrdad Farahani, Mohammad Gharachorloo, Omid Kashefi. 2018. Mizan: A large
Marzieh Farahani, and Mohammad Manthouri. Persian-English parallel corpus. arXiv preprint
2020. Parsbert: Transformer-based model for arXiv:1801.02107.
Persianlanguageunderstanding.arXivpreprint
Simran Khanuja, Sandipan Dandapat, Anirudh
arXiv:2005.12515.
Srinivasan, Sunayana Sitaram, and Monojit
Joseph L. Fleiss. 1971. Measuring nominal scale Choudhury. 2020. GLUECoS: An evaluation
agreement among many raters. Psychologi- benchmark for code-switched NLP. In Pro-
calBulletin,76(5):378.https://doi.org ceedings of ACL. https://doi.org/10
/10.1037/h0031619 .18653/v1/2020.acl-main.329
Mor Geva, Ankit Gupta, and Jonathan Berant. Daniel Khashabi, Sewon Min, Tushar Khot,
2020. Injecting numerical reasoning skills into AshishSabharwal,OyvindTafjord,PeterClark,
language models. In Proceedings of ACL, and Hannaneh Hajishirzi. 2020. UnifiedQA:
pages 946–958. https://doi.org/10 Crossing format boundaries with a single QA
.18653/v1/2020.acl-main.89 system. In Proceedings of EMNLP (Findings),
pages 1896–1907. https://doi.org/10
Deepak Gupta, Surabhi Kumari, Asif Ekbal, and
.18653/v1/2020.findings-emnlp.171
Pushpak Bhattacharyya. 2018. MMQA: A
multi-domainmulti-lingualquestion-answering DanielKhashabi,AmosNg,TusharKhot,Ashish
framework for English and Hindi. In Proceed- Sabharwal, Hannaneh Hajishirzi, and Chris
ingsofLREC. Callison-Burch. 2021. GooAQ: Open question
1159
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
answering with diverse answer types. arXiv https://doi.org/10.18653/v1/2020
preprintarXiv:2104.08727. .emnlp-main.484
HadiAbdiKhojasteh,EbrahimAnsari,andMahdi SeungyoungLim,MyungjiKim,andJooyoulLee.
Bohlouli. 2020. LSCP: Enhanced large scale 2019. Korquad1.0: Korean QA dataset for ma-
colloquial Persian language understanding. In chine reading comprehension. arXiv preprint
ProceedingsofLREC,pages6323–6327. arXiv:1909.07005.
Maria Khvalchik and Mikhail Malkin. 2020. Bing Liu. 2012. Sentiment analysis and opinion
Departamentodenosotros:Howmachinetrans- mining.SynthesisLecturesonHumanLanguage
lated corpora affects language models in mrc Technologies,5(1):1–167.https://doi.org
tasks. In Proceedings of the Workshop on /10.2200/S00416ED1V01Y201204HLT016
Hybrid Intelligence for Natural Language Pro-
ShayneLongpre,YiLu,andJoachimDaiber.2020.
cessing Tasks (HI4NLP 2020) co-located with
MKQA:Alinguisticallydiversebenchmarkfor
24th European Conference on Artificial Intel-
multilingual open domain question answering.
ligence (ECAI 2020): Santiago de Compostela,
arXivpreprintarXiv:2007.15207.
Spain, August 29, 2020, pages 29–33. CEUR
WorkshopProceedings.
ColinP.Masica.1993.TheIndo-aryanLanguages.
CambridgeUniversityPress.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia
Redfield,MichaelCollins,AnkurParikh,Chris
Todor Mihaylov, Peter Clark, Tushar Khot, and
Alberti, Danielle Epstein, Illia Polosukhin,
Ashish Sabharwal. 2018. Can a suit of armor
Jacob Devlin, Kenton Lee, Kristina N.
conduct electricity? A new dataset for open
Toutanova, Llion Jones, Ming-Wei Chang,
book question answering. In Proceedings of
Andrew Dai, Jakob Uszkoreit, Quoc Le, and
EMNLP. https://doi.org/10.18653
SlavPetrov.2019.Naturalquestions:Abench-
/v1/D18-1260
mark for question answering research. Trans-
actions of the Association for Computational Mahsa Mohaghegh, Abdolhossein Sarrafzadeh,
Linguistics, 7:453–466. https://doi.org andTomMoir.2010.Improvedlanguagemod-
/10.1162/tacl a 00276 eling for English-Persian statistical machine
translation.InProceedingsoftheWorkshopon
J. Richard Landis and Gary G. Koch. 1977. The
Syntax and Structure in Statistical Translation,
measurement of observer agreement for cate-
pages75–82.
gorical data. Biometrics, 159–174. https://
doi.org/10.2307/2529310 Mahsa Mohaghegh, Abdolhossein Sarrafzadeh,
and Tom Moir. 2011. Improving Persian-
Patrick Lewis, Barlas Oguz, Ruty Rinott,
English statistical machine translation:exper-
Sebastian Riedel, and Holger Schwenk. 2020.
iments in domain adaptation. In Proceedings
MLQA: Evaluating cross-lingual extractive
of the Workshop on South Southeast Asian
question answering. In Proceedings of ACL,
Natural Language Processing (WSSANLP),
pages 7315–7330. https://doi.org/10
pages9–15.
.18653/v1/2020.acl-main.653
Ellie Pavlick, Matt Post, Ann Irvine, Dmitry
Yaobo Liang, Nan Duan, Yeyun Gong, Ning
Kachaev,andChrisCallison-Burch.2014.The
Wu, Fenfei Guo, Weizhen Qi, Ming Gong,
languagedemographicsofAmazonMechanical
Linjun Shou, Daxin Jiang, Guihong Cao,
Turk.TransactionsoftheAssociationforCom-
Xiaodong Fan, Ruofei Zhang, Rahul Agrawal,
putational Linguistics, 2:79–92. https://
Edward Cui, Sining Wei, Taroon Bharti, Ying
doi.org/10.1162/tacl a 00167
Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang
Liu, Fan Yang, Daniel Campos, Rangan Mohammad Taher Pilevar, Heshaam Faili, and
Majumder, and Ming Zhou. 2020. XGLUE: Abdol Hamid Pilevar. 2011. TEP: Tehran
A new benchmark datasetfor cross-lingual English-Persianparallelcorpus.InInternational
pre-training, understanding and generation. In Conference on Intelligent Text Processing
Proceedings of EMNLP, pages 6008–6018. and Computational Linguistics, pages 68–79.
1160
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
Springer. https://doi.org/10.1007 MojganSeraji,CarinaJahani,Bea´taMegyesi,and
/978-3-642-19437-5 6 Joakim Nivre. 2013. Uppsala Persian depen-
dency treebank annotation guidelines. Techni-
Edoardo Maria Ponti, Goran Glavasˇ, Olga calreport,UppsalaUniversity.
Majewska, Qianchu Liu, Ivan Vulic´, and
Anna Korhonen. 2020. XCOPA: A multilin- Mahsa Sadat Shahshahani, Mahdi Mohseni,
gualdatasetforcausalcommonsensereasoning. Azadeh Shakery, and Heshaam Faili. 2019.
InProceedingsofEMNLP,pages2362–2376. Payma:AtaggedcorpusofPersiannamedenti-
ties.SignalandDataProcessing,16(1):91–110.
MariaPontiki,DimitrisGalanis,JohnPavlopoulos, https://doi.org/10.29252/jsdp.16
Harris Papageorgiou, Ion Androutsopoulos, .1.91
and Suresh Manandhar. 2014. SemEval-2014
task 4: Aspect based sentiment analysis. Javad PourMostafa Roshan Sharami, Parsa
In Proceedings of the International Work- Abbasi Sarabestani, and Seyed Abolghasem
shop on Semantic Evaluation (SemEval 2014), Mirroshandel. 2020. Deepsentipers: Novel
pages27–35.https://doi.org/10.3115 deep learning models trained over proposed
/v1/S14-2004 augmented Persian sentiment corpus. arXiv
preprintarXiv:2004.05328.
Matt Post. 2018. A call for clarity in report-
ing BLEU scores. In Proceedings of WMT, TatianaShavrina,AlenaFenogenova,Emelyanov
pages 186–191. https://doi.org/10 Anton, Denis Shevelev, Ekaterina Artemova,
.18653/v1/W18-6319 Valentin Malykh, Vladislav Mikhailov,
MariaTikhonova,AndreyChertok,andAndrey
Prokopis Prokopidis, Vassilis Papavassiliou, and Evlampiev.2020.Russiansuperglue:ARussian
Stelios Piperidis. 2016. Parallel global voices: language understanding evaluation benchmark.
A collection of multilingual corpora with cit- In Proceedings of EMNLP, pages 4717–4726.
izen media stories. In Proceedings of LREC, https://doi.org/10.18653/v1/2020
pages900–905. .emnlp-main.381
Colin Raffel, Noam Shazeer, Adam Roberts, Gary F. Simons and Charles D. Fennig. 2017.
KatherineLee,SharanNarang,MichaelMatena, Ethnologue: Languages of Asia. sil Interna-
Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. tionalDallas.
Exploring the limits of transfer learning with
a unified text-to-text transformer. Journal of Chi Sun, Luyao Huang, and Xipeng Qiu. 2019.
MachineLearningResearch,21(140):1–67. Utilizing BERT for aspect-based sentiment
analysis via constructing auxiliary sentence. In
Pranav Rajpurkar, Jian Zhang, Konstantin ProceedingsofNAACL,pages380–385.
Lopyrev, and Percy Liang. 2016. SQUAD:
100,000+ questions for machine comprehension Alon Talmor, Jonathan Herzig, Nicholas Lourie,
of text. In Proceedings of EMNLP. https:// and Jonathan Berant. 2019. CommonsenseQA:
doi.org/10.18653/v1/D16-1264 A question answering challenge targeting
commonsense knowledge. In Proceedings of
Mohammad Sadegh Rasooli, Ahmed El Kholy, NAACL,pages4149–4158.
andNizarHabash.2013.Orthographicandmor-
phological processing for Persian-to-English Tun Thura Thet, Jin-Cheon Na, and Christopher
statistical machine translation. In Proceedings S. G. Khoo. 2010. Aspect-based sentiment
ofIJCNLP,pages1047–1051. analysisofmoviereviewsondiscussionboards.
JournalofInformationScience,36(6):823–848.
Matthew Richardson, Christopher J. C. Burges,
andErinRenshaw.2013.MCTest:Achallenge Jo¨rg Tiedemann and Lars Nygaard. 2004. The
dataset for the open-domain machine compre- OPUScorpus-parallelandfree:http://logos
hension of text. In Proceedings of EMNLP, .uio.no/opus. In Proceedings of LREC.
pages193–203. https://doi.org/10.1177/0165551510388123
1161
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Conference of the North American Chap-
Amanpreet Singh, Julian Michael, Felix Hill, ter of the Association for Computational
OmerLevy,andSamuelBowman.2019.Super- Linguistics: Human Language Technologies,
glue:Astickierbenchmarkforgeneral-purpose pages483–498.
language understanding systems. In Proceed-
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason
ingsofNourIPS,pages3266–3280.
Baldridge. 2019. PAWS-X: A cross-lingual
Adina Williams, Nikita Nangia, and Samuel adversarial dataset for paraphrase identifi-
Bowman. 2018. A broad-coverage challenge cation. In Proceedings of EMNLP-IJCNLP,
corpus for sentence understanding through pages 3678–3683. https://doi.org/10
inference. In Proceedings of NAACL, .18653/v1/D19-1382
pages 1112–1122. https://doi.org/10
Poorya Zaremoodi, Wray Buntine, and
.18653/v1/N18-1101
Gholamreza Haffari. 2018. Adaptive knowl-
edge sharing in multi-task learning: Improving
LiangXu,HaiHu,XuanweiZhang,LuLi,Chenjie
low-resource neural machine translation. In
Cao,YudongLi,YechenXu,KaiSun,DianYu,
Proceedings of the 56th Annual Meeting of
Cong Yu, Yin Tian, Qianqian Dong, Weitang
the Association for Computational Linguistics
Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng,
(Volume 2: Short Papers), pages 656–661.
Rongzhao Wang, Weijian Xie, Yanting Li,
https://doi.org/10.18653/v1/P18
Yina Patterson, Zuoyu Tian, Yiwen Zhang,
-2104
He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng
Zhao, Cong Yue, Xinrui Zhang, Zhengliang
Biao Zhang, Philip Williams, Ivan Titov, and
Yang, Kyle Richardson, and Zhenzhong Lan.
Rico Sennrich. 2020. Improving massively
2020. CLUE: A chinese language understand-
multilingual neural machine translation and
ing evaluation benchmark. In Proceedings of
zero-shot translation. In Proceedings of ACL,
COLING,pages4762–4772.
pages 1628–1639. https://doi.org/10
.18653/v1/2020.acl-main.148
Linting Xue, Noah Constant, Adam Roberts,
Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Ingrid Zukerman and Bhavani Raskutti. 2002.
AdityaBarua,andColinRaffel.2021.MT5:A Lexical query paraphrasing for document re-
massively multilingual pre-trained text-to-text trieval.InProceedingsofCOLING.https://
transformer. In Proceedings of the 2021 doi.org/10.3115/1072228.1072389
1162
Downloaded
from
http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00419/1971807/tacl_a_00419.pdf
by
guest
on
22
February
2024

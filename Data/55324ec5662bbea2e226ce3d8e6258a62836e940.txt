JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 1
Universal Source Separation with Weakly Labelled Data
Qiuqiang Kong*, Ke Chen*, Haohe Liu, Xingjian Du
Taylor Berg-Kirkpatrick, Shlomo Dubnov, Mark D. Plumbley
Abstract—Universalsourceseparation(USS)isafundamentalresearchtaskforcomputationalauditorysceneanalysis,whichaimsto
separatemonorecordingsintoindividualsourcetracks.Therearethreepotentialchallengesawaitingthesolutiontotheaudiosource
separationtask.First,previousaudiosourceseparationsystemsmainlyfocusonseparatingoneoralimitednumberofspecific
sources.Thereisalackofresearchonbuildingaunifiedsystemthatcanseparatearbitrarysourcesviaasinglemodel.Second,most
previoussystemsrequirecleansourcedatatotrainaseparator,whilecleansourcedataarescarce.Third,thereisalackofUSS
systemthatcanautomaticallydetectandseparateactivesoundclassesinahierarchicallevel.Touselarge-scaleweakly
labeled/unlabeledaudiodataforaudiosourceseparation,weproposeauniversalaudiosourceseparationframeworkcontaining:1)an
audiotaggingmodeltrainedonweaklylabeleddataasaquerynet;and2)aconditionalsourceseparationmodelthattakesquerynet
outputsasconditionstoseparatearbitrarysoundsources.Weinvestigatevariousquerynets,sourceseparationmodels,andtraining
strategiesandproposeahierarchicalUSSstrategytoautomaticallydetectandseparatesoundclassesfromtheAudioSetontology.By
solelyleveragingtheweaklylabelledAudioSet,ourUSSsystemissuccessfulinseparatingawidevarietyofsoundclasses,including
soundeventseparation,musicsourceseparation,andspeechenhancement.TheUSSsystemachievesanaverage
signal-to-distortionratioimprovement(SDRi)of5.57dBover527soundclassesofAudioSet;10.57dBontheDCASE2018Task2
dataset;8.12dBontheMUSDB18dataset;anSDRiof7.28dBontheSlakh2100dataset;andanSSNRof9.00dBonthe
voicebank-demanddataset.Wereleasethesourcecodeathttps://github.com/bytedance/uss
IndexTerms—Universalsourceseparation,hierarchicalsourceseparation,weaklylabelleddata.
(cid:70)
1 INTRODUCTION
MONO source separation is the task of separat- of sound sources from a mixture. One difficulty of USS is
ing single-channel audio recordings into individual that there are hundreds of different sounds in the world,
source tracks. An audio recording may consist of several and it is difficult to separate all sounds using a unified
soundeventsandacousticscenes.Universalsourceseparation model [10]. Recently, the USS problem has attracted the
(USS)isatasktoseparatearbitrarysoundfromarecording. interestsofseveralresearchers.Asystem[11]wasproposed
Sourceseparationhasbeenresearchedforseveralyearsand to separate arbitrary sounds by predicting the masks of
hasawiderangeofapplications,includingspeechenhance- sounds,wherethemaskscontrolhowmanysignalsshould
ment [1], [2], music source separation [3], and sound event remainfromthemixturesignals.UnsupervisedUSSsystems
separation[4],[5].USSiscloselyrelatedtothewell-known [12], [13] were proposed to separate sounds by mixing
cocktail party problem [6], where sounds from different trainingsamplesintoamixtureandseparatingthemixture
sourcesintheworldmixintheairbeforearrivingattheear, into a variable number of sources. A Free Universal Sound
requiring the brain to estimate individual sources from the Separation (FUSS) system applied a time-domain convolu-
received mixture. Humans can focus on a particular sound tionalnetwork(TDCN++)systemtoseparatemixturesinto
sourceandseparateitfromothers,askillsometimescalled up to 4 separate sources. A class conditions system [14]
selective hearing. As a study of auditory scene analysis by was proposed for 4-stem music source separation. Some
computationalmeans,computationalauditorysceneanaly- other methods [15], [16], [17] use audio embeddings that
sis [7], [8] systems are machine listening systems that aim character an audio clip to control what sources to separate
toseparatemixturesofsoundsourcesinthesamewaythat fromamixture.In[18],one-hotencodingsofsoundclasses
humanlistenersdo. are used as controls to separate corresponding sources.
Manypreviousworksmainlyfocusonspecificsourcesepa- Othersoundseparationsystemsincludelearningtoseparate
rationthatonlyseparateoneorafewsources;theseinclude from weakly labelled scenes [19] and SuDoRM-RM [13],
speech separation [1], [2] and music source separation [9] [20] to remove sound sources from mixtures. Recently, a
tasks. Different from specific source separation tasks such language-based source separation system was proposed in
as speech enhancement or music source separation, a USS [21].Thosesystemsaremainlytrainedonsmalldatasetsand
systemaimstoautomaticallydetectandseparatethetracks donotscaletoautomaticallydetectandseparatehundreds
ofsoundclasses.
• Qiuqiang Kong and Ke Chen contribute equally to this work. Forthesourceseparationproblem,wedefinecleansource
Qiuqiang Kong: kongqiuqiang@bytedance.com, Xingjian Du: dux- data as audio segments that contain only target sources
ingjian.real@bytedance.com are with ByteDance, Shanghai, China. Ke withoutothersources.Thecleansourcedatacanbemixedto
Chen:knutchen@ucsd.edu,ShlomoDubnov:sdubnov@ucsd.edu,andTay-
formaudiomixturestotrainpotentialseparators.However,
lorBerg-Kirkpatrick:tberg@eng.ucsd.eduarewithUniversityofCalifor-
nia San Diego, San Diego, USA. Haohe Liu: haohe.liu@surrey.ac.uk, collecting clean data is time-consuming. For sound classes
Mark D. Plumbley: m.plumbley@surrey.ac.uk are with University of such as “Speech”, clean data can be recorded in the labo-
Surrey,Guildford,UK.
ratory. But for many other environmental sounds such as
3202
yaM
11
]DS.sc[
1v74470.5032:viXra
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 2
Encoder Decoder
audio samples latent feature audio separation audio samples audio separation
Separator
Separator STFT Separation Mask iSTFT
mask
Encoder Decoder
audio samples latent feature latent separation audio separation audio spectrograms spectrogram separation
Fig.1.Thestandardarchitectureofdeep-learning-basedaudiosourceseparationmodel.Lefttop:synthesis-basedseparationmodel.Leftbottom:
mask-basedseparationmodel.Right:thegeneraltypeoffrequency-domainseparationmodel.
“Thunder”,thecollectionofcleandataisdifficult.Recently, 2 SOURCE SEPARATION VIA NEURAL NETWORKS
theconceptofweaklylabelleddata[22],[23]wasusedinaudio
Deep learning methods for audio source separation have
signal processing. In contrast to clean source data, weakly
outperformed traditional methods such as Non-negative
labelled data contains multiple sources in an audio clip.
Matrix Factorization [34]. Fig. 1 shows source separation
An audio clip is labelled with one or multiple tags, while
models in the time domain (left) and in the frequency
the time information of tags is unknown. For example, a
domain (right). Here, we introduce the basic methodology
10-second audio clip is labelled as “Thunder” and “Rain”,
ofthoseseparationmodels.
but the time when these two events exactly appear within
this 10-second clip is not provided. Weakly labelled data
2.1 Time-domainSeparationModels
has been widely used in audio tagging [24], [25], [26], [27]
A neural network time-domain separation model f is typ-
andsoundeventdetection[28],[29],[30].Buttherehasbeen
ically constructed as an encoder-decoder architecture, as
limitedworkonusingweaklylabelledforsourceseparation
shownintheleftofFig.1.Formally,givenasingle-channel
[31],[32].
audio clip x ∈ RL and a separation target s ∈ RL, where
In this work, we propose a USS framework that can
L is sample length, the separator f contains two types: a
be trained with weakly labelled data. This work extends
synthesis-basedseparationsystemthatdirectlyoutputsthe
our previously proposed USS systems [31], [32], [33] with
waveformofthetargetsource,andamask-basedseparation
contributionsasfollows:
thatpredictamaskthatcanbemultipliedtothemixtureto
outputthetargetsource.
• We are the first to use large-scale weakly labelled SeparationmodelssuchasDemucs[35],[36]andWave-
datatotrainUSSsystemsthatcanseparatehundreds U-Net [9], f directly estimates the final separation target:
ofsoundclasses. sˆ = f(x). Mask-based separation models such as TasNet
• We propose to use sound event detection systems [37] and ConvTasNet [38] predict masks in the latent space
trained on weakly labelled data to detect short seg- produced by the neural network. The masks control how
mentsthataremostlikelytocontainsoundevents. much of sources should remain from the mixture. Then, a
• We investigate a variety of query nets to extract decoderisdesignedtoreconstructtheseparatedwaveform
conditionstobuildUSSsystems.Thequerynetsare from the masked latent feature produced by the neural
pretrainedorfinetunedaudiotaggingsystems. network.
• We propose a hierarchical USS strategy to automat-
ically detect and separate the sources of existing 2.2 Frequency-domainSeparationModels
sound classes with an hierarchical AudioSet ontol-
Incontrasttotime-domainmodels,frequency-domainmod-
ogy. The USS procedure do require the specification
els leverage a spectrogram, such as a short-time Fourier
ofthesoundclassestoseparate.
transform (STFT), to facilitate the separation process. Har-
• We show that a single USS system is able to per-
monicfeatureshavemorepatternsinthefrequencydomain
form a wide range of separation tasks, including
than those in the time domain. This might help improve
sound event separation, music source separation,
separation performance in source separation tasks, such as
and speech enhancement. We conduct comprehen-
music source separation and environmental sound separa-
siveablationstudiestoinvestigatehowdifferentfac-
tion[39].
torsinoursystemaffecttheseparationperformance.
Formally,givenamonoaudioclipx,wedenotetheSTFT
ofxasacomplexmatrixX ∈CT×F,whereT isthenumber
Thisarticleisorganizedasfollows.Section2introduces of time frames and F is the number of frequency bins. We
neural network-based source separation systems. Section 3 denote the magnitude and the phase of X as |X| and ∠X,
introducesourproposedweaklylabelledsourceseparation respectively. The right part of Fig. 1 shows a frequency-
framework.Section4reportsontheresultsofexperiments. domain separation system f predicting a magnitude ideal
Section5concludesthiswork. ratiomask(IRM)[40]M ∈RT×F oracomplexIRM(cIRM)
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 3
TABLE1 Strongly labelled data Weakly labelled data
Sourceseparationdatasets.Thetypescanbecleandataorweakly 63 63
labelleddata.
48 48
Dataset Dur.(h) Classes Types
32 32
Voicebank-Demand[51] 19 1 Clean
MUSDB18[52] 6 4 Clean 16 16
UrbanSound8K[53] 10 10 Clean
FSDKaggle2018[54] 20 41 Clean 0 0
FUSS[55] 23 357 Clean 0 5 10 0 5 10
AudioSet[23] 5,800 527 Weak Duration (s) Duration (s)
Fig. 2. Left: Clean source data of sound class “Flute”. Right: Weakly
labelled data of sound class “Air horn, truck horn” which only occurs
[41] M ∈ CT×F that can be multiplied by the STFT of the between2.5s-4.0s.
mixture to obtain the STFT of the separated source. The
complex STFT of the separated source Sˆ ∈ CT×F can be
weaklylabelleddatasetasa nwhereaistheabbreviationfor
calculatedby: theaudio.Thetagsofa
n
isdenotedasy
n
∈ {0,1}K,where
Sˆ=M (cid:12)X. (1) K is the number of sound classes. The value y n(k) = 1
indicates the presence of a sound class k while y n(k) = 0
where(cid:12)istheelement-wisecomplexmultiplication.Then, indicates the absence of a sound class k. We denote a
theseparatedsourcesˆ∈RLcanbeobtainedbyapplyingan weakly labelled dataset as D = {a n,y n}N n=1, where N is
inverseSTFTonSˆ
. thenumberoftrainingsamplesinthedataset.Theleftpart
Frequencydomainmodelsincludefullyconnectedneu- of Fig. 2 shows a clean source audio clip containing the
ral networks [2], recurrent neural networks (RNNs) [42], cleanwaveformof“Flute”.TherightpartofFig.2showsa
[43], [44], and convolutional neural networks (CNNs) [43], weakly labelled audio clip containing a target sound class
[45],[46].UNets[47],[48]arevariantsofCNNthatcontain “Air horn, truck horn” which only occurs between 2.5 s
encoder and decoder layers for source separation. Band- and4.0s.Theweaklylabelledaudiorecordingalsocontains
split RNNs (BSRNNs) [39] apply RNNs along both the unknown interference sounds, i.e., y n(k) = 0 may contain
time and frequency axes to capture time and frequency missingtagsforsomesoundclassk.
domain dependencies. There are also approaches such as The goal of aweakly labelled USS system is toseparate
hybridDemucs[49],[50]whichcombinetimeandfrequency arbitrary sounds trained with only weakly labelled data.
domainsystemstobuildsourceseparationsystems. Fig. 3 depicts the architecture of our proposed system,
containingfoursteps:
2.3 ChallengesofSourceSeparationModels 1) Weapplyasamplingstrategytosampleaudioclips
Asmentionedabove,manyprevioussourceseparationsys- of different sound classes from a weakly labelled
tems require clean source data to train source separation audiodataset.
systems. However, the collection of clean source data is 2) We define an anchor segment as a short segment
difficult and time-consuming. Table 1 summarizes datasets that is most likely to contain a target sound class
that can be used for source separation. On the one hand, in a long audio clip. We apply an anchor seg-
previous clean source datasets have durations of around ment mining algorithm to localize the occurrence
tens of hours. On the other hand, weakly labelled datasets ofevents/tagsintheweaklylabelledaudiotracks.
are usually larger than clean source datasets and clean 3) Usepretrainedaudiotaggingmodelstopredictthe
datasets. AudioSet [23] is a representative weakly labelled tagprobabilitiesorembeddingsofanchorsegments.
datasetcontainingover5,800hoursof10-secondaudioclips 4) Mix anchor segments as input mixtures. Train a
and is larger in both size and number of sound classes query-based separation network to separate the
than clean source datasets. AudioSet has an ontology of mixtureintooneofthetargetsourcequeriedbythe
527 sound classes in its released version. The ontology of soundclasscondition.
AudioSet has a tree structure, where each audio clip may
containmultipletags. 3.2 AudioClipsSampling
In this work, we use the weakly labelled AudioSet Foralarge-scaledataset,weapplytwosamplingstrategies:
dataset containing 5,800 hours to train a USS system that 1)randomsampling:randomlysampleaudioclipsfromthe
canseparatehundredsofsoundclasses. datasettoconstituteamini-batch;and2)balancedsampling:
sampleaudioclipsfromdifferentsoundclassestoconstitute
3 USS WITH WEAKLY LABELLED DATA a mini-batch to ensure the clips contain different sound
classes. AudioSet is highly unbalanced; sound classes such
3.1 WeaklyLabelledData
as“Speech”and“Music”havealmost1millionaudioclips,
In contrast to clean source data, weakly labelled data only while sound classes such as “tooth breath” have only tens
contain the labels of what sound classes are present in an oftrainingsamples.Withoutbalancedsampling,theneural
audio recording. Weakly labelled data may also contain networkmayneversee“toothbreath”ifthetrainingisnot
interfering sounds. There are no time stamps for sound longenough.Followingthetrainingschemeofaudioclassi-
classes or clean sources. We denote the n-th audio clip in a fication systems [25], [26], [27], [56], we applythe balanced
snib
.qerf
leM
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 4
clip-1 clip-2 clip-n
...
Separation
Query-based Separation
Model
Latent Features
Embedding Extraction Audio Mixture
Audio
...
Tagging
Model
segment-1 segment-2 segment-n
Fig. 4. Top: log mel spectrogram of a 10-second audio clip from Au-
Anchor Segment Mining dioSet;Middle:predictedSEDprobabilityof“Speech”,whereredblock
shows the selected anchor segment; Bottom: predicted audio tagging
probabilitiesoftheanchorsegment.
...
audioclipswithrelativelycleansoundsourcesfromweakly
clip-1 clip-2 clip-n labelledaudiosamples.
AnchorsegmentminingisthecorepartofUSSsystems
trainedwithweaklylabelleddata.Sincetheweaklylabeled
Sampling Strategy audiotrackdoesnotalwayscontainthelabeledsoundclass
throughout its timeline, we need to extract a short audio
segment inside this track to create source data for training
the separation model. Formally, given an audio clip a i ∈
RL,ananchorsegmentminingalgorithmextractsananchor
Audio Dataset segment s i ∈ RL(cid:48) from a i, where L(cid:48) < L is the samples
numberoftheanchorsegment.
Fig. 3. The architecture of our proposed query-based audio source
For each audio clip in mini-batch {a i}B i=1, we propose
two types of anchor segment mining strategies: (A) Ran-
separationpipelinetrainedfromweakly-labelddata,includingdatasets,
samplingstrategies,audiotaggingmodel,andconditionalaudiosource domlyselectananchorsegments i fromanaudioclipa i,or
separationmodels. (B)Applyapretrainedsoundeventdetection(SED)system
to detect an anchor segment s i, where the center of s i is
the time stamp where the sound class label is most likely
sampling strategy to retrieve audio data from AudioSet so
to occur. For the SED method (B) of anchor mining, we
that all sound classes can be sampled equally. That is, each
leverage PANNs [25] and HTS-AT [27] to perform sound
soundclassissampledevenlyfromtheunbalanceddataset.
eventdetectionontheaudiotrack.
We denote a mini-batch of sampled audio clips as {a i}B i=1,
We introduce the SED anchor segment mining strategy
whereB isthemini-batchsize. (B) as follows. For an audio clip a i, a SED system pro-
duces two main outputs: 1) event classification prediction
p ∈ [0,1]K where K is the number of sound classes and
AT
3.3 AnchorSegmentMining
AT is the abbreviation for audio tagging; and 2) framewise
We define anchor segment mining as a procedure to localize event prediction p SED ∈ [0,1]T×K, where T the number
anchor segments in an audio clip. We use sound event of frames and SED is the abbreviation for sound event
detection models that are trained only on weakly-labelled detection.Touseweaklylabelleddataintraining,theaudio
data but can localize the occurrence (i.e. time stamps) of tagging prediction is usually calculated by maximizing the
soundclasses.Recently,audiotaggingsystemstrainedwith framewiseeventpredictionp SED.BothPANNsandHTS-AT
theweaklylabelledAudioSet[25],[26],[27],[57],[58]have are trained with the weakly labeled data by minimizing
outperformed systems trained with clean source data. We the binary cross entropy loss between the audio tagging
applyPretrainedAudioNeuralNetworks(PANNs)[25]and predictionp AT andthelabelsy ∈[0,1]K and:
a Hierarchical Token-semantic Audio Transformer (HTS-
K
AT) [27] as audio tagging models to perform the anchor (cid:88)
l=− y(k)lnp (k)+(1−y(k))ln(1−p (k)). (2)
AT AT
segmentminingprocedure.Suchmodelsareabletoextract
k=1
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 5
Event Classification
Latent Feature
average Patch Tokens Hierarchical Transformer Blocks
Event Presence Map Avg-Pooling Patch-Embed Conv2D project
Conv2D (3,3,2048)
time→frequency→window
Conv2D (3,3,256) Latent Output
... ... ...... ...
Conv2D (3,3,128)
...... Token-Semantic Conv2D Avg-Pooling
Conv2D (3,3,64)
average
Latent Feature
log-mel spetrogram time frame Event Presence Map Event Classification
Fig. 5. Two audio tagging models for audio classification, sound event detection, and latent feature production. Left: Pretrained Audio Neural
Networks(PANN)inCNN14architecture.Right:HierarchicalToken-SemanticTransformer(HTS-AT)in4-blockarchitecture.
Wethenusethetrainedsoundeventdetectionmodelto Algorithm1Prepareamini-batchofdata.
perform framewise event prediction p SED of an audio clip. 1: Inputs: dataset D = {a n,y n}N n=1 containing K sound
Wedenotetheanchorsegmentscoreofthek-thsoundclass classes,minibatchsizeB.
as: 2: Outputs: a mini-batch of mixture and anchor segment
pairs{(x i,s i)}B i=1.
t+τ/2 3: Step1:BalancedSampling:UniformlysampleBsound
(cid:88)
q k(t)= p SED(t,k), (3) classes from sound classes {1,...,K} without replace-
t−τ/2 ment. Sample one audio clip a b,b = 1,...,B for each
selectedsoundclass.Wedenotethemini-batchofaudio
whereτ isthedurationofanchorsegments.Then,thecenter
timetoftheoptimalanchorsegmentisobtainedby:
clipsas{a 1,...,a B}.
4: Step2:AnchorSegmentDetection:Applyapretrained
t anchor =argmaxq k(t). (4) SEDmodeloneacha i todetecttheoptimalanchortime
t stampt i by (3)(4).Extract the anchor segments i ∈ RL(cid:48)
The red block in Fig. 4 shows the detected anchor seg- whosecenterist i.
ment. We apply the anchor segment mining strategy as 5: Step3(optional):Useanaudiotaggingmodeltopredict
described in (3) and (4) process a mini-batch of the audio theeventpresenceprobabilityofs iandapplythreshold
clips {x 1,...,x B} into a mini-batch of anchor segments θ ∈[0,1]K togetbinaryresultsr
i
∈{0,1}K wherer i(k)
{s 1,...,s B}. issetto1ifthepresenceprobabilityislargerthanθ(k).
Algorithm 1 shows the procedure for creating training Permute {s i}B i=1 so that r i ∧r (i+1)%B = 0, where % is
data. Step 1 describes audio clip sampling and Step 2 themodulooperator.
describes anchor segment mining. To further avoid two 6: Create mixture source pairs {(x i,s i)}B i=1 by x i = s i +
anchor segments containing the same classes being mixed, s (i+1)%B.
we propose an optional Step 3 to mine anchor segments
from a mini-batch of audio clips {s 1,...,s B} to constitute
mixtures.Step4describesmixingdetectedanchorsegments
intomixturestotraintheUSSsystem. HTS-AT[27]isahierarchicaltoken-semantictransformer
Fig. 5 shows the model architectures of both PANNs for audio classification. It applies Swin-Transformer [59] to
and HTS-AT as two audio tagging models we employed. an audio classification task. In the right of Fig. 5, a mel-
The DecisionLevel systems of PANNs [25] provide frame- spectrogramiscutintodifferentpatchtokenswithapatch-
wise predictions and contain VGG-like CNNs to convert embed CNN and sent into the Transformer in order. The
an audio mel-spectrogram into feature maps. The model timeandfrequencylengthsofthepatchareequaltoP ×P,
averages the feature maps over the time axis to obtain a whereP isthepatchsize.Tobettercapturetherelationship
final event classification vector. The framewise prediction between frequency bins of the same time frame, HTS-AT
p SED ∈ [0,1]T×K indicates the SED result. Additionally, firstsplitsthemel-spectrogramintowindowsw 1,w 2,...,w n
the output of the penultimate layer with a size of (T,H) and then splits the patches in each window. The order of
can be used to obtain its averaged vector with a size of H tokens Q follows time→frequency→window. The patch
as a latent source embedding for the query-based source tokenspassthroughseveralnetworkgroups,eachofwhich
separation in our system, where H is the dimension of the containsseveraltransformer-encoderblocks.Betweenevery
latentembedding. two groups, a patch-merge layer is applied to reduce the
CNN-14
...
ycneuqerf
remrofsnarT
niwS
egreM-hctaP .TS .MP .TS .MP .TS .MP
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 6
numberoftokenstoconstructahierarchicalrepresentation. 3.4.4 LearnableCondition
Each transformer-encoder block is a Swin-transformer [59]
The latent embedding condition can be learned during the
blockwiththeshiftedwindowattentionmodule,amodified
trainingofouruniversalsourceseparationsystem.Thefirst
self-attentionmoduletoimprovethetrainingefficiency.
methodistofine-tunetheparametersofthequerynet.The
Then, HTS-AT applies a token-semantic 2D-CNN to
second method is to freeze the parameters of the query net
further process the reshaped output ( T , F ,H) into the
8P 8P andaddaclonedquerynetcontaininglearnableparameters
framewise event presence map (T,K) which can be aver- as a shortcut branch f to construct the embedding.
shortcut
aged toan event classificationvectorK. Thelatent embed-
Thethirdmethodistoaddlearnablefullyconnectedlayers
ding, at the same time, is produced by averaging the re- f (·) on top of the query net where ada is the abbre-
ada
shapedoutputintoaH-dimensionvectorwithanaverage-
viation for adaptive. The embedding can be extracted by:
poolinglayer. c
i
=f ada(f AT(s i),θ).
3.4 SourceQueryConditions 3.5 Query-basedSourceSeparation
In contrast to previous query-based source separators that A typical source separator is a single-input-single-output
extract pre-defined representations [31] or learnable repre- model[10]thatdealswithonespecificsource,suchasvocal,
sentations[17],[32],[60]fromcleansources,weproposeto drum, or bass. To enable the model to separate arbitrary
extractqueryembeddingsfromanchorsegmentstocontrol sound sources, we apply a query-based source separator
sources to separate from a mixture. We introduce four by introducing the conditional embeddings as described in
types of embeddings: a hard one-hot embedding with a Section 3.4 into the ResUNet source separation backbone
dimension of K where K is the number of sound classes, [33] to build a single-input single-output source separation
asoftprobabilityconditionwithadimensionofK,alatent model[10].
embeddingconditionwithadimensionofH,whereHisthe As mentioned in Section 2, the input to the ResUNet
latent embedding dimension, and a learnable embedding separator is a mixture of audio segments. First, we apply
conditionwithadimensionofH. a short-time Fourier transform (STFT) to the waveform
to extract the complex spectrum X = CT×F. Then, we
follow the same setting of [33] to construct an encoder-
3.4.1 HardOne-HotCondition
decoder network to process the magnitude spectrogram
Wedefinethehardone-hotconditionofananchorsegment |X|. The ResUNet encoder-decoder consists of 6 encoder
s i as c i ∈ [0,1]K, where c i is the one-hot representation of blocks, 4 bottleneck blocks, and 6 decoder blocks. Each
tags of the audio clip x i. The hard one-hot condition has encoder block consists of 4 residual convolutional blocks
been used in music source separation [61]. Hard one-hot to downsample the spectrogram into a bottleneck feature,
embedding requires clean source data for training source and each decoder block consists of 4 residual deconvo-
separationsystems. lutional blocks to upsample the feature back to separa-
tioncomponents.Theskip-connectionisappliedfromeach
encoder block to the corresponding decoder block of the
3.4.2 SoftProbabilityCondition
same downsampling/upsampling rate. The residual block
Thesoftprobabilityconditionappliespretrainedaudiotag- contains 2 convolutional layers, 2 batch normalization [62]
ging models, such as PANNs or HTS-AT to calculate the layers, and 2 Leaky-ReLU activation layers. An additional
event classification probability c i = p AT(s i) of an anchor residualshortcutisaddedbetweentheinputandtheoutput
segment as the query embedding. For the weakly labelled ofeachresidualblock.Thedetailsofthemodelarchitecture
dataset,thesoftprobabilityconditionprovidesacontinuous canbefoundat[33].
value prediction of what sounds are in an anchor segment TheResUNetseparatoroutputsthemagnitudesandthe
than the hard one-hot condition. The advantage of the soft phases of the cIRM M ∈ CT×F. The separated complex
probability condition is that it explicitly presents the SED spectrumcanbeobtainedby:
result.
Sˆ=M (cid:12)X
(5)
=|M|(cid:12)|X|ej(∠M+∠X),
3.4.3 LatentEmbeddingCondition
The latent embedding with a dimension of H is calculated where both |M| and ∠M are calculated from the output
from the penultimate layer of an audio tagging model. of the separator. The separated source can be obtained
The advantage of using the latent embedding is that the by multiplying the STFT of the mixture by the cIRM M.
separation is not limited to the given K sound classes. The The complex multiplication can also be decoupled into a
USSsystemcanbeusedtoseparatearbitrarysoundclasses magnitude multiplication part and a phase addition part.
with a query embedding as input, allowing us to achieve The magnitude |M| controls how much the magnitude of
USS. We investigate a variety of PANNs, including CNN5, |X|shouldbescaled,andtheangle∠M controlshowmuch
CNN10,CNN14,andanHTS-ATmodeltoextractthelatent theangleofX shouldberotated.
embeddings and we evaluate their efficiency on the sep- BasedontheResUNetseparator,weadoptafeature-wise
aration performance. We denote the embedding condition linearmodulation(FiLM)[63]methodtoconstructconvolu-
extractionofasc i =f emb(s i). tionalblockswithintheseparator.Weapplyapre-activation
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 7
Algorithm2TrainingofaUSSsystem. Algorithm 3 Automatic sound event detection hierarchical
1: Inputs:DatasetD,e.g.,AudioSet. USS.
2: Outputs:AtrainedUSSmodel. 1: Inputs: An arbitrary duration audio clip x. A trained
3: whilelossfunctionldoesnotconvergedo USS system f SS. A trained audio tagging system f AT.
4: prepareamini-batchofmixturesourcetrainingpairs
Hierarchicallevell.
{(x i,s i)}B
i=1
byAlgorithm1 2: Outputs: Separated sources O = {sˆ j} j∈C where C is
5: ofqC ua el rc yu nla et te ss ao su dr ece scq riu be er dy inem Seb ce td iod nin 3g .4s .{c i}B i=1 by any 3: t Sh pe lii tnd xex ines to. non-overlapped short segments {x i}I i=1
6: foreachs i do whereI isthenumberofsegments.
7: Obtaintheseparationsˆ i =f(x i,c i) 4: Apply f AT on all segments to obtain P(i,k) with a size
8: Calculatelossbyl(sˆ i,s i) ofI×K,whereK isthenumberofsoundclasses.
9: endfor 5: #Calculateontologypredictions.
10: endwhile 6: ifhierarchical separationthen
7: Q(i,j) = HierarchicalOntologyGrouping(P(i,k),l)
followingAlgorithm4.Q(i,j)hasashapeofI×Jwhere
J isthenumberofsoundclassesinthel-thlevel.
architecture [64] for all of the encoder and decoder layers,
weincorporatetheconditionalembeddingas: 8: endif
9: #Detectactivesoundevent.
hl =W ∗(σ(BN(hl−1)+Vc)) (6) 10: C ={}#Activesoundclassindexes.
11: forj =1,...,J do
wherehl−1 isthefeaturemapofthel−1-thlayer,andV is 12: ifmaxiQ(j,j)>δ then:
afullyconnectedlayertomaptheconditionalvectorcinto 13: C =C∪{j}
an embedding space. Vc modulates the value BN(hl−1). 14: endif
ThevalueisconvolvedwiththeweightW tooutputhl.The 15: endfor
training of our weakly labeled USS system is illustrated in 16: #Doseparation.
Algorithm2. 17: O ={}#Separatedsourcesofactivesoundclasses.
18: forj ∈C do
19: fori=1,...,I do
3.6 Dataaugmentation 20: ifQ(i,j)>δ then:
Whenconstitutingthemixturewiths i ands i+1,theampli- 21: Getconditionc j by(10).
tude of s i and s i+1 can be different. We propose an energy 22: sˆ ij =f ss(x i,c j)
augmentationtoaugmentdata.Thatis,wefirstcalculatethe 23: else
energy of a signal s i by E = ||s i||2 2. We denote the energy 24: sˆ ij =0
of s i and s i+1 as E i and E i+1. We apply a scaling factor 25: endif
α i =(cid:112) E i/E i+1 tos i+1 whencreatingthemixturex i: 2 26 7: : e sˆ jnd =f {o sˆr ij}I i=1
28: O =O∪{sˆ j}
x=s i+αs i+1. (7)
29: endfor
By this means, both anchor segments s i and s i+1 have the
same energy which is beneficial to the optimization of the
3.8 Inference
separation system. We will show this in Section 4.6.7. On
the one hand, we match the energy of anchor segments to
Intraining,theoracleembeddingofananchorsegmentcan
let the neural network learn to separate the sound classes.
Ontheotherhand,theamplitudediversityofsoundclasses
be calculated by f emb(s i). In inference, for the hard one-
hotconditionandsoftprobabilitycondition,wecansimply
is increased. We will show this energy augmentation is
use 1) the one-hot representation of the k-th sound class to
beneficialinourexperiments.
separatetheaudioofthek-thsoundclass.2)Onlyremaining
soft probabilities’ {k j}J
j=1
indexes values as the condition,
whereJ isthenumberofsoundclassestoseparate.
3.7 Lossfunctions
However,forthelatentembeddingconditionandlearn-
We propose to use the L1 loss between the predicted and able condition, we need to calculate the embedding c from
ground truth waveforms following [33] to train the end-to-
thetrainingdatasetby:
enduniversalsourceseparationsystem:
1 (cid:88)N
l=||s−sˆ|| 1, (8) c=
N
f emb(s n) (9)
n=1
wherelisthelossfunctionusedtotraintheneuralnetwork.
A lower loss in (8) indicates that the separated signal sˆis where {s n}N
n=1
are query samples of one sound class and
closertothegroundtruthsignals.Intraining,thegradients N is the number of query samples. That is, we average all
of parameters are calculated by ∂l/∂θ, where θ are the conditional embeddings of query samples from the same
parametersoftheneuralnetwork. soundclasstoconstitutec.
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 8
Algorithm4HierarchicalOntologyGrouping. sources [43], [45], [47], [48], [67]. The trained USS system
1: Inputs: segment-wise prediction P(i,k) with a size of canaddressawiderangeofsourceseparationtaskswithout
I×K,hierarchylevell. beingfinetuned.
2: Outputs: Q(i,j) with a size of I × J where J is the
number of children sound classes of the l-th ontology
level.
4.1 TrainingDataset
3: forj ∈1,...,J do
4: Q(i,j)=maxk{P(i,k)} k∈children(j) AudioSet is a large-scale weakly labelled audio dataset
5: endfor containing 2 million 10-second audio clips sourced from
the YouTube website. Audio clips are only labelled with
the presence or absence of sound classes, without knowing
3.9 InferencewithHierarchicalAudioSetOntology
whenthesoundeventsoccur.Thereare527soundclassesin
Weproposeahierarchicalseparationstrategytoaddressthe itsreleasedversion,coveringawiderangeofsoundclasses
USS problem. It is usually unknown how many and what intheworld,suchas“Humansounds”,“Animal”,etc.The
sound classes are present and which need to be separated training set consists of 2,063,839 audio clips, including a
in an audio clip. To address this problem, we propose a balanced subset of 22,160 audio clips. There are at least 50
hierarchical sound class detection strategy to detect the audio clips for each sound class in the balanced training
soundclassespresence.Weseparatethosesoundclassesby set. Although some audio links are no longer available, we
usingthetrainedUSSsystem. successfully downloaded 1,934,187 (94%) audio clips from
Algorithm 3 shows the automatic sound class detection thefulltrainingset.Allaudioclipsarepaddedwithsilence
and separation steps. The input to the USS system is an into10seconds.Duetothefactthatalargeamountofaudio
audioclipx.Wefirstsplittheaudioclipintoshortsegments recordings from YouTube have sampling rates lower than
and apply an audio tagging system f AT to calculate the 32kHz,weresampleallaudiorecordingsintomonoand32
segment-wisepredictionP(t,k)withasizeofI×K,where kHz.
I and K are the number of segments and sound classes,
respectively.TheAudioSetontologyhasatreestructure.The
firstlevelofthetreestructurecontainssevensoundclasses
4.2 TrainingDetails
described in the AudioSet ontology, including “Human
sounds”, “Animal”, “Music”, “Source-ambiguous sounds”,
We select anchor segments as described in Section 3.3 and
“Sounds of things”, “Nature sounds”, and “Channel, en-
mix two anchor segments to constitute a mixture x. The
vironment and background”. Each root category contains
duration of each anchor segment is 2 seconds. We investi-
several sub-level sound classes. The second level and the
gatedifferentanchorsegmentdurationsinSection4.6.4.We
thirdlevelscontain41and251soundclasses,respectively,as
apply matching energy data augmentation as described in
described in the AudioSet ontology [23]. The tree structure
Section 3.6 to scale two anchor segments to have the same
hasamaximumdepthofsixlevels.
energy,andextracttheshort-timeFouriertransform(STFT)
Ininference,theUSSsystemsupportshierarchicalsource
feature X from x with a Hann window size of 1024 and a
separationwithdifferentlevels.Wedenotethesoundclasses
of level l as C = {c j}J j=1, where J is the number of sound h coo np sis si tz ee n3 t2 to0. tT hh ei as uh do ip os taiz ge gil nea gd ss yt so te1 m0 s0 if nra Pm Ae Ns Nin sa [2s 5e ]co an nd
d
classes in the l-th level. For a sound class j in the l-th
HTS-AT[27].
level,wedenotethesetofallitschildren’ssoundclassesas
children(j)includingj.Forexample,forthehumansounds The query net is a CNN14 of PANNs or HTS-AT. The
class 0, there are children(0) = {0,1,...,72}. We set score query net is pretrained on the AudioSet tagging task [23],
Q(i,j)=maxk{P(i,j)} k∈children(j).Wedetectasoundclass [27] and the parameters are frozen during the training of
j as active if maxiQ(i,j) larger than a threshold θ. We set the USS system. The prediction and the embedding layers
separated segments to silence if Q(i,j) is smaller than θ. of the query net have dimensions of 527 and 2048, respec-
tively.Eitherthepredictionlayerortheembeddinglayeris
Then,weapplytheUSSbyusing(10)asthecondition:
connectedtofullyconnectedlayersandinputtoalllayersof
(cid:26)
c k = f AT 0( ,x k), ∈/k c∈ hilc dh ri eld nr (e jn ).(j) (10) t [h 33e ]s ao su tr hce es se op ua rcra et sio en pab rr aa tn ioc nh ba rs anFi cL hM .Ts h. eW 3e 0a -ld ao yp ert RR ees sUUN Ne et
t
TheUSSprocedureisdescribedinAlgorithm4. consists of 6 encoder and 6 decoder blocks. Each encoder
blockconsistsoftwoconvolutionallayerswithkernelsizes
of3×3.Followingthepre-activationstrategy[64],weapply
4 EXPERIMENTS
batch normalization [62] and leaky ReLU [68] before each
Inthissection,weinvestigateourproposeduniversalsource convolutional layer. The FiLM is added to each convolu-
separationsystemonseveraltasks,includingAudioSetsep- tionallayerasdescribedin(6).Thenumberofoutputfeature
aration[23],soundeventseparation[54],[65],musicsource maps of the encoder blocks are 32, 64, 128, 256, 512, and
separation[52],[66],andspeechenhancement[51].OurUSS 1024,respectively.Thedecoderblocksaresymmetrictothe
system is trained only on the large-scale weakly labelled encoder blocks. We apply an Adam optimizer [69] with a
AudioSet[23]withoutusinganycleantrainingdata,which learningrateof10−3 totrainthesystem.Abatchsizeof16
is a major difference from the previous source separation is used to train the USS system. The total training steps is
systems that are trained on specific datasets with clean 600ktrainedfor3daysonasingleTeslaV100GPUcard.
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 9
4.3 ConditionalEmbeddingCalculation 4.4.4 MUSDB18
For AudioSet source separation, the oracle embedding or TheMUSDB18dataset[52]isdesignedforthemusicsource
eachanchorsegmentiscalculatedby: separation task. The test set of the MUSDB18 dataset con-
tains 50 songs with four types of stems, including vocals,
c=f emb(s) (11) bass, drums, and other. We linearly sum all stems to con-
stitute mixtures as input to the USS system. We use the
where s is the clean source. Using oracle embedding musevaltoolkit2 toevaluatetheSDRmetrics.
as condition indicates the upper bound of the universal
source separation system. For real applications, we calcu-
4.4.5 Slakh2100
late the conditional embeddings by (9) from the training
TheSlakh2100dataset[66]isamultiple-instrumentdataset
setoftheAudioSet,FSD50Kaggle2018,FSD50k,MUSDB18,
formusicsourceseparationandtranscription.Thetestofthe
Slakkh2100,andVoicebankDemanddatasetstoevaluateon
Slakh2100datasetcontains225songs.Thesoundofdifferent
thosedatasets,respectively.
instruments are rendered by 167 different types of plug-
ins. We filtered 151 non-silent plugin types for evaluation.
4.4 EvaluationDatasets Different from the MUSDB18 dataset, there can be over 10
instruments in a song, leading to the Slakh2100 instrument
4.4.1 AudioSet
separationachallengingproblem.
The evaluation set of AudioSet [23] contains 20,317 audio
clips with 527 sound classes. We successfully downloaded 4.4.6 Voicebank-Demand
18,887 out of 20,317 (93%) audio clips from the evaluation
The Voicebank-Demand [51] dataset is designed for the
set. AudioSet source separation is a challenging problem
speech enhancement task. The Voicebank dataset [51] con-
due to USS need to separate 527 sound classes using a
tains clean speech. The Demand dataset [70] contains mul-
singlemodel.WearethefirsttoproposeusingAudioSet[31]
tiple different background sounds that are used to create
to evaluate the USS. To create evaluation data, similarly to
mixtures. The noisy utterances are created by mixing the
Section3.3,wefirstapplyasoundeventdetectionsystemto
VoiceBank dataset and the Demand dataset under signal-
each10-secondaudiocliptodetectanchorsegments.Then,
to-noise ratios of 15, 10, 5, and 0 dB. The test set of the
weselecttwoanchorsegmentsfromdifferentsoundclasses
Voicebank-Demanddatasetcontains824utterancesintotal.
and sum them as a mixture for evaluation. We create 100
mixtures for each sound class, leading to 52,700 mixtures
forallsoundclassesintotal. 4.5 EvaluationMetrics
We use the signal-to-distortion ratio (SDR) [71] and SDR
4.4.2 FSDKaggle2018 improvement (SDRi) [71] to evaluate the source separation
performance.TheSDRisdefinedas:
TheFSDKaggle2018[54]isageneral-purposeaudiotagging
dataset containing 41 sound classes ranging from musical (cid:18) ||s||2 (cid:19)
SDR(s,sˆ)=10log (12)
instruments,humansounds,domesticsounds,andanimals, 10 ||s−sˆ||2
etc. The duration of the audio clips ranges from 300 ms
where s and sˆare the target source and estimated source,
to 30 s. Each audio clip contains a unique audio tag. The
respectively. Larger SDR indicates better separation perfor-
test set is composed of 1,600 audio clips with manually-
mance.TheSDRiisproposedtoevaluatehowmuchSDRa
verified annotations. We pad or truncate each audio clip
USSsystemimprovescomparedtowithoutseparation:
into 2-second segment from the start, considering sound
eventsusuallyoccurinthestartofaudioclips.Wemixtwo SDRi=SDR(s,sˆ)−SDR(s,x) (13)
segments from different sound classes to consist a pair. We
constitute100mixturesforeachsoundclass.Thisleadstoa where x is the mixture signal. For the speech enhancement
totalof4,100evaluationpairs. task, we apply the Perceptual evaluation of speech quality
(PESQ)[72]andsegmentalsignal-to-rationoise(SSNR)[73]
4.4.3 FSD50Kdataset forevaluation.
The Freesound Dataset 50k (FSD50K) dataset [65] contains
51,197 training clips distributed in 200 sound classes from 4.6 ResultsAnalysis
the AudioSet ontology. In contrast to the FSDKaggle2018 4.6.1 ConditionalEmbeddingTypes
dataset, each audio clip may contain multiple tags with
The default configuration of our USS system is a 30-layer
a hierarchical architecture. There are an average of 2.79
ResUNet30 trained on the balanced set of AudioSet. Table
tags in each audio clip. All audio clips are sourced from
2 shows the USS system results trained with different con-
Freesound1. There are 10,231 audio clips distributed in 195
ditional embedding types including wav2vec [74], speaker
sound classes in the test set. Audio clips have variable embeddings3, CNN6, CNN10, CNN14 from PANNs [25],
durations between 0.3s to 30s, with an average duration
andHTS-AT[27].Thewav2vecembeddingistrainedusing
of 7.1 seconds. We mix two segments from different sound
unsupervised contrastive learning on 960 hours of speech
classes to consist a pair. We create 100 mixtures for each
data. The wav2vec embedding is averaged along the time
soundclass.Thisleadstointotal19,500evaluationpairs.
2.https://github.com/sigsep/sigsep-mus-eval
1.https://freesound.org/ 3.https://github.com/RF5/simple-speaker-embedding
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 10
TABLE2
USSresultswithdifferentconditionalembeddingtypes.
AudioSet(SDRi) FSDK2018 FSD50k MUSDB18 Slakh2100 VoicebankDemand
ora.emb avg.emb SDR SDRi SDR SDRi SDR SDRi wSDR wSDRi SDR SDRi PESQ SSNR
wav2vec(46c) 8.87 4.30 8.95 8.91 4.52 4.70 1.90 7.03 2.96 8.37 -1.08 6.66 2.11 6.02
speaker(46d) 8.87 2.82 6.69 6.65 3.00 3.03 1.52 6.85 2.48 7.94 0.18 7.92 2.13 4.72
CNN6(45a2) 8.68 5.30 10.36 10.31 5.25 5.50 3.05 8.43 3.94 9.42 -0.37 7.37 2.27 9.39
CNN10(45a3) 8.35 5.36 9.95 9.90 5.19 5.43 2.87 8.10 4.11 9.34 -0.27 7.47 2.27 8.68
+CNN14(44a) 8.26 5.57 10.61 10.57 5.54 5.79 3.08 8.12 4.02 9.22 -0.46 7.28 2.18 9.00
HTSAT(45c) 9.38 3.78 7.95 7.91 3.38 3.51 2.83 8.48 3.77 9.36 0.81 8.55 2.23 8.78
TABLE3
USSresultswithsoftaudiotaggingandlatentembeddingascondition.
AudioSet(SDRi) FSDK2018 FSD50k MUSDB18 Slakh2100 VoicebankDemand
ora.emb avg.emb SDR SDRi SDR SDRi SDR SDRi wSDR wSDRi SDR SDRi PESQ SSNR
Segmentprediction(dim=527)(46b) 7.80 6.42 11.22 11.18 6.60 6.92 2.48 7.26 3.58 8.80 -1.69 6.05 2.20 8.30
+Embedding(dim=2048) 8.26 5.57 10.61 10.57 5.54 5.79 3.08 8.12 4.02 9.22 -0.46 7.28 2.18 9.00
TABLE4
USSresultswithfreeze,finetune,andadaptconditionalembeddings.
AudioSet(SDRi) FSDK2018 FSD50k MUSDB18 Slakh2100 VoicebankDemand
ora.emb avg.emb SDR SDRi SDR SDRi SDR SDRi wSDR wSDRi SDR SDRi PESQ SSNR
CNN14(randomweights) 8.51 2.82 5.96 5.91 2.82 2.82 -0.48 4.59 1.97 7.08 -1.34 6.40 2.28 6.96
CNN14(scratch) 2.38 2.38 2.46 2.41 2.22 2.15 0.71 5.78 1.16 6.30 -1.20 6.54 1.62 -0.28
CNN14(finetune) 9.83 1.96 3.42 3.38 1.50 1.40 2.10 7.77 3.10 8.52 1.39 9.12 1.77 3.52
+CNN14(freeze) 8.26 5.57 10.61 10.57 5.54 5.79 3.08 8.12 4.02 9.22 -0.46 7.28 2.18 9.00
CNN14+shortcut 6.95 4.57 9.29 9.25 4.74 4.94 1.84 7.05 3.40 8.78 -1.44 6.30 2.06 8.91
CNN14+adaptor 8.01 5.81 11.00 10.96 5.79 6.07 2.95 7.96 3.90 9.24 -0.87 6.87 2.30 9.60
TABLE5
USSresultswithdifferentbackbonemodels.
AudioSet(SDRi) FSDK2018 FSD50k MUSDB18 Slakh2100 VoicebankDemand
ora.emb avg.emb SDR SDRi SDR SDRi SDR SDRi wSDR wSDRi SDR SDRi PESQ SSNR
open-unmix 3.96 3.39 3.90 3.86 2.96 2.92 0.40 5.50 2.13 7.42 -1.09 6.65 2.40 2.26
ConvTasNet 6.96 5.00 9.49 9.45 5.31 5.54 0.61 5.61 2.64 8.10 -2.96 4.77 1.87 6.46
UNet 8.14 5.50 10.83 10.79 5.49 5.75 2.49 7.78 3.70 9.17 -0.45 7.29 2.12 8.47
ResUNet30 8.26 5.57 10.61 10.57 5.54 5.79 3.08 8.12 4.02 9.22 -0.46 7.28 2.18 9.00
ResUNet60 7.97 5.70 11.34 11.30 6.04 6.32 1.71 6.81 3.64 9.01 -2.77 4.97 2.40 9.35
axis to a single embedding with a dimension of 512. The for universal source separation. The HTS-AT achieves the
speaker embedding is a gated recurrent unit (GRU) with highest oracle embedding SDR among all systems. All
threerecurrentlayersoperatesonlogmel-spectrogramand of CNN6, CNN10, CNN14, and HTS-AT outperform the
has output has a shape of 256. The CNN6 and the CNN10 wav2vec embedding and the speaker embedding in Au-
havedimensionsof512.TheCNN14andtheHTS-AThave dioSet,FSDKaggle2018,FSD50k,MUSDB18,Slakh2100,and
dimensionsof2048.Theoracleembedding(oraemb)shows Voicebank-Demanddatasetsbyalargemargin.TheCNN14
theresultsusing(11)ascondition.Theaverageembedding slightly outperforms CNN6 and CNN10. In the following
(avgemb)showstheresultsusing(9)ascondition. experiments, we use CNN14 as the default conditional
embedding.
Table 2 shows that the CNN6, CNN10, CNN14 embed-
dings achieve AudioSet SDR between 5.30 dB and 5.57 dB Table3showsthecomparisonbetweenusingtheCNN14
using the average embedding, outperforming the wav2vec segmentpredictionwithadimensionof527andtheCNN14
of 4.30 dB and the speaker embedding of 2.82 dB. One embedding condition with a dimension of 2048 to build
possible explanation is that both wav2vec and the speaker the USS system. On one hand, the segment prediction
embeddings are trained on speech data only, so that they embeddingachievesanSDRof7.80dBonAudioSet,outper-
are not comparable to PANNs and HTS-AT trained for forming the embedding condition of 5.57 dB. The segment
general audio tagging. The wav2vec embedding slightly prediction also achieves higher SDRis than the embedding
outperforms the speaker embedding on FSDKaggle2018, condition on the FSDKaggle2018, and the FSD50k dataset
FSD50k, and MUSDB18 separation, indicating that the un- datasets. An explaination is that the sound classes of all
supervised learned ASR embeddings are more suitable of the AudioSet, FSDKaggle2018, and the FSD50k datasets
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 11
TABLE6
USSresultstrainedwithdifferentanchorsegmentdurations.
AudioSet(SDRi) FSDK2018 FSD50k MUSDB18 Slakh2100 VoicebankDemand
ora.emb avg.emb SDR SDRi SDR SDRi SDR SDRi wSDR wSDRi SDR SDRi PESQ SSNR
0.5s 4.07 2.86 4.51 4.47 2.55 2.51 0.97 0.78 2.61 2.43 -0.79 6.95 1.57 5.96
1s 7.50 4.99 9.45 9.41 4.81 5.00 0.18 -0.02 2.54 2.50 -1.66 6.08 2.17 8.55
+2s 8.26 5.57 10.61 10.57 5.54 5.79 3.08 8.12 4.02 9.22 -0.46 7.28 2.18 9.00
4s 7.39 5.21 10.22 10.17 5.38 5.60 1.83 6.79 3.38 8.68 -2.62 5.12 -2.62 5.12
6s 6.39 4.68 9.20 9.16 5.05 5.24 0.00 4.98 2.70 7.97 -4.26 3.48 2.21 2.56
8s 6.26 4.48 8.85 8.80 4.77 4.94 -3.67 -4.00 1.60 1.50 -5.68 2.06 2.24 2.35
10s 6.29 4.47 9.11 9.07 4.80 4.98 -2.68 -2.79 1.56 1.53 -5.07 2.67 2.13 2.14
TABLE7
USSresultswithdifferentanchorminingstrategies.
AudioSet(SDRi) FSDK2018 FSD50k MUSDB18 Slakh2100 VoicebankDemand
ora.emb avg.emb SDR SDRi SDR SDRi SDR SDRi wSDR wSDRi SDR SDRi PESQ SSNR
mining 8.26 5.57 10.61 10.57 5.54 5.79 3.08 8.12 4.02 9.22 -0.46 7.28 2.18 9.00
in-cliprandom 4.89 3.94 5.53 5.49 3.63 3.66 1.10 6.05 2.36 7.79 -1.72 6.01 2.21 5.69
out-cliprandom 8.19 5.90 11.06 11.01 6.04 6.34 2.57 7.68 3.81 9.17 -1.08 6.66 2.39 9.48
are sub-classes of the AudioSet. The segment prediction Next,weexperimentwithlearningtheparametersofthe
performsbetterthanembeddingconditioninin-vocabulary conditional embedding extractor from scratch or finetune
soundclassesseparation.Ontheotherhand,theembedding theweightsfrompretrainedmodels.Table4showsthatnei-
conditionachieveshigherSDRsthanthesegmentprediction ther the learning from scratch nor the finetuning approach
on the MUSDB18 and the Slakh2100 dataset. This result improves the USS system performance. The learning from
indicatesthattheembeddingconditionperformbetterthan scratch approach and the finetuning approaches achieve
segment prediction in new-vocabulary sound classes sepa- SDRiof2.41dBand3.38dBontheFSDKaggle2018dataset,
ration. even underperform the random weights of 5.91 dB. One
Fig. 7 in the end of this paper shows the classwise possible explanation is that the parameters of the condi-
SDRi results of AudioSet separation including 527 sound tional embedding branch and the source separation branch
classes.ThedashedlinesshowtheSDRiwithoraclesegment are difficult to be jointly optimized when both of them
predictionorembeddingasconditions.Thesolidlinesshow are deep. The training falls to a collapse mode. Using the
the SDRi with averaged segment prediction or embedding pretrainedfrozenCNN14systemasconditionalembedding
calculated from the anchor segments mined from the bal- extractorsignificantlyimprovestheSDRito10.57dBonthe
ancedtrainingsubset.Fig.7showsthatsoundclassessuch FSDKaggle2018dataset.
as busy signal, sine wave, bicycle bell achieve the highest Based on the pretrained frozen CNN14 conditional em-
SDRi over 15 dB. We discovered that clear defined sound bedding extractor, we propose to add a learnable shortcut
classes such as instruments can achieve high SDRi scores. or add an learnable adaptor on top of the CNN14 system.
Most of sound classes achieve positive SDRi scores. The The learnable short cut has a CNN14 architecture with
tendency of using segment prediction and embedding as learnableparameters.Table4showsthatthelearnableshort-
conditions are the same, although the segment prediction cut conditional embedding extractor achieves an SDR of
outperform the embedding and vice versa in some sound 9.29 dB on FSDKaggle2018, less than using the pretrained
classes. frozen CNN14 conditional embedding extractor of 10.57
dB. One possible explanation is that the learnable shortcut
destoriestheembeddinginformationforsourceseparation.
4.6.2 Freeze, Finetune, and Adapt Conditional Embed-
The adaptoris a 2-layer fullyconnected neural networkon
dings
topofthepretrainedfrozenCNN14conditionalembedding
Table 4 shows the comparison of using random, frozen, extractor. With the adaptor, we achieve an SDR of 11.10 dB
finetuned, and adapted conditional embeddings to build and outperforms the CNN14 system. This result indicates
theUSSsystem.Allthevariationsoftheconditionalembed- thattheadaptorisbeneficialfortheUSStask.
dingsextractorsarebasedontheCNN14architecture.Using
randomweightstoextractconditionalembeddingsachieves 4.6.3 SeparationArchitectures
anSDRiof2.82dBonAudioSet,comparedtousepretrained Table 5 shows the results of building USS systems with
CNN14toextractconditionalembeddingsachievesanSDR differentsourceseparationbackbones.Theopen-unmixsys-
5.57 dB. We show that using random weights to extract tem [67] is a 3-layer bidirectional long short term memory
conditional embeddings work for all USS tasks, such as (BLSTM) system. The BLSTM is applied on the mixture
achieves an SDRi of 5.91 dB on the FSDKaggle2018 dataset spectrogram to output the estimated clean spectrogram.
comparedtothepretrainedCNN14embeddingextractorof The open-unmix system achieves an SDR of 3.39 dB on
10.57dB. AudioSet separation and achieves a PESQ of 2.40 on the
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 12
TABLE8
USSresultswithdifferentsourcesnumber.
AudioSet(SDRi) FSDK2018 FSD50k MUSDB18 Slakh2100 VoicebankDemand
ora.emb avg.emb SDR SDRi SDR SDRi SDR SDRi wSDR wSDRi SDR SDRi PESQ SSNR
2srcsto1src 8.26 5.57 10.61 10.57 5.54 5.79 3.08 8.12 4.02 9.22 -0.46 7.28 2.18 9.00
3srcsto1-2srcs 7.37 4.71 8.30 8.26 4.36 4.52 2.43 8.08 3.56 8.69 -0.48 7.26 2.37 8.34
4srcsto1-3srcs 7.03 4.38 7.49 7.45 3.99 4.10 2.43 7.99 3.51 8.98 0.70 8.44 2.38 7.78
TABLE9
USSresultswithdifferentdataaugmentation.
AudioSet(SDRi) FSDK2018 FSD50k MUSDB18 Slakh2100 VoicebankDemand
ora.emb avg.emb SDR SDRi SDR SDRi SDR SDRi wSDR wSDRi SDR SDRi PESQ SSNR
noaug 7.11 3.81 7.19 7.14 3.27 3.35 1.78 7.22 3.09 8.74 0.69 8.43 2.39 6.36
+-20dB 5.51 3.62 5.77 5.73 2.93 2.94 1.69 7.02 2.51 8.03 -0.34 7.40 2.22 5.34
+matchenergy 8.26 5.57 10.61 10.57 5.54 5.79 3.08 8.12 4.02 9.22 -0.46 7.28 2.18 9.00
TABLE10
USSresultsofmodelstrainedwithbalancedandfullsubsetsofAudioSet.
AudioSet(SDRi) FSDK2018 FSD50k MUSDB18 Slakh2100 VoicebankDemand
ora.emb avg.emb SDR SDRi SDR SDRi SDR SDRi wSDR wSDRi SDR SDRi PESQ SSNR
+Balancedset 8.26 5.57 10.61 10.57 5.54 5.79 3.08 8.12 4.02 9.22 -0.46 7.28 2.18 9.00
Fullset 8.21 6.14 10.34 10.30 5.45 5.71 2.31 7.20 3.60 8.92 -1.29 6.45 2.40 9.45
Fullset(longtrain) 9.60 6.76 13.07 13.03 6.52 6.85 1.77 7.00 3.51 9.00 -2.62 5.12 2.45 10.00
Voicebank-Demand speech enhancement task, indicating the USS system. On the other hand, the separation scores
that the BLSTM backbone performs well for speech en- decrease with anchor segment durations decrease from 2 s
hancement. The open-unmix system underperforms other to 10 s on all tasks. One possible explanation is that long
backbone source separation systems in FSDKaggle2018, anchor segment contain undesired interfering sounds that
FSD50k, MUSDB18, and Slakh2100 separation, indicating will impair the training of the USS system. Therefore, we
that the capacity of the open-unmix system is not large use2-secondanchorsegmentinallotherexperiments.
enough to separate a wide range of sound classes. The
ConvTasNet[38]isatime-domainsourceseparationsystem 4.6.5 DifferentAnchorSegmentMiningStrategies
consists of one-dimensional convolutional encoder and de-
Table 7 shows the results of different anchor mining strate-
coder layers. The ConvTasNet achieves an SDRi of 5.00 dB
gies. The in-clip random strategy randomly select two an-
on AudioSet separation and outperforms the open-unmix
chor segments from a same 10-second audio clip which
system.
significantly underperform the SED mining strategy in all
OurproposedUNet30[47]isanencoder-decoderconvo-
ofthesourceseparationtasks.Theout-cliprandomstrategy
lutionalarchitectureconsistsof30convolutionallayers.The
randomlyselecttwoanchorsegmentsfromtwodifferent10-
ResUNet30 [33] adds residual shortcuts in the encoder and
second audio clips. The out-clip random strategy achieves
decoderblocksinUNet30.TheUNet30andtheResUNet30
an SDRi of 5.90 dB on AudioSet, outperforms the SED
systesmachieveSDRisof5.50dBand5.57dBonAudioSet,
mining of 5.57 dB. On one hand, the out-clip random
outperformingtheConvTasNetbyaround1dBinallsource
strategy also outperforms the SED mining strategy in FS-
separation tasks. We extend ResUNet30 to a deeper system
DKaggle2018 and the FSD50k dataset. On the other hand,
ResUNet60with60convolutionalayers.Table5showsthat
the SED mining strategy outperforms the out-clip random
ResUNet60outperformsResUNet30byaround0.5dBinall
strategyinMUSDB18andSlakh2100sourceseparation.Both
USStasks.Thisresultindicatesthatdeeperarchitecturesare
the out-clip and the SED mining strategies outperform the
beneficialforUSS.
in-cliprandomstrategy.
4.6.4 DifferentAnchorSegmentDurations
4.6.6 Sourcesnumbertomixduringtraining
Table6showstheresultsofUSSsystemstrainedwithdiffer-
entanchorsegmentdurationsrangingfrom0.5sto10s.The Table8showstheUSSresultstrainedwithdifferentnumber
anchor segments are mined by a pretrained SED system as of sources J to constitute a mixture. Table 8 shows that
describedinSection3.3.Ononehand,Table6showsthatthe J = 2 performs the best on AudioSet with an SDRi of
separation scores increase with anchor segment durations 5.57 dB and also performs the best on the FSDKaggle2018,
increase from 0.5 s to 2 s and achieves the best SDRi of FSD50k, and on MUSDB18 datasets. This result shows that
5.57 dB at anchor segment of 2 s on AudioSet separation. mixingtwosourcesissufficientforthosesourceseparation
This result shows that the anchor segment should be long tasks. By using J = 4 the USS system perform the beston
enough to contain sufficient context information to build theSlakh2100dataset.AnexplanationisthattheSlakh2100
Input audio
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 13
Hierarchy
Level 1
Hierarchy
Level 2
Hierarchy
Level 3
Fig.6.ComputationalauditorysceneanalysisandhierarchicalUSSofthetrailerof“HarryPotterandtheSorcerer’sStone”:https://www.youtube.
com/watch?v=VyHV0BRtdxo
contains audio clips contain multiple instruments being thesoundinamovieintodifferenttracks.Onechallengeof
played simultaneously. Using more sources to constitute a thehierarchicalseparationisthenumberofpresentsources
mixtureperformbetterthanusingfewersources. are unknown. We use the methods in Section 3.9 to detect
andseparatethepresentsoundevents.
4.6.7 Dataaugmentation Fig. 6 shows the automatically detected and separated
Table 9 shows the USS results with different augmentation waveforms of a movie clip from Harry Potter and the Sor-
strategies applied to sources to create a mixture. First, we cerer’s Stone from ontology levels 1 to 3 by using Algo-
do not apply any data augmentation to create a mixture. rithm 3. Level 1 indicates coarse sound classes and level
Second, we randomly scale the volume of each source by 3 indicates fine sound classes. In level 1, the USS system
±20dB.Third,weproposeamatchingenergydataaugmen- successfully separate human sounds, music and sounds of
tation to scale the volume of sources to create a mixture to things. In level 2, the USS system further separate human
ensurethesourceshavethesameenergy.Table9showsthat group actions, vehicle, and animals. In level 3, the USS
the matching energy data augmentation significantly out- system separate fine-grained sound classes such as bell,
performthesystemstrainedwithoutdataaugmentationand bird,crowd,andscarymusic.
random volume scale augmentation, with an SDRi of 5.57
dBcomparedto3.81dBand3.63dBonAudioSetseparation.
Thematchingenergydataaugmentationalsooutperformno
5 CONCLUSION
data augmentation and random volume augmentation on
Inthispaper,weproposeuniversalsourceseparation(USS)
alltheothertasks.
systemstrainedonthelarge-scaleweaklylabelledAudioSet.
The USS systems can separate hundreds of sound classes
4.6.8 USSresultsTrainedwithbalancedandfullAudioSet
using a single model. The separation system can achieve
Table10showstheresultsoftrainingtheUSSsystemswith universal source separation by using the embedding cal-
the balanced and the full AudioSet, respectively. The full culated from query examples as a condition. In training,
trainingdatais100timeslargerthanthebalanceddata.We we first apply a sound event detection (SED) system to
alsoexperimentwithtrainingtheUSSsystemwith4GPUs detect the anchor segments that are most likely to contain
andalargerbatchsizeof64.TheUSSsystemtrainedonthe sound events. We constitute a mixture by mixing several
full AudioSet outperforms the USS system trained on the anchor segments. Then, we use a pretrained audio tagging
balanced set after trained 1 million steps. Table 10 shows system to calculate the segment prediction probability or
that training on the full AudioSet with a batch size of 64 the embedding vector as the condition of the target an-
achievesanSDRiof6.76dB,outperformingtrainingonthe chor segment. The USS system takes the mixture and the
balancedsetof5.57dB. condition as input to output the desired anchor segment
waveform. In inference, we propose both a hierarchical
4.6.9 VisualizationofHierarchicalSeparation
separation with an AudioSet ontology. We evaluated our
Oneapplicationofthehierarchicalseparationistoseparate proposedUSSsystemsonawiderangeofseparationtasks,
arbitraryaudiorecordingsintoindividualsourceswithAu- includingAudioSetseparation,FSDKaggle2018andFSD50k
dioSet ontology. For example, the USS system can separate general sound separation, MUSDB18 and Slakh2100 music
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 14
instrumentsseparation,andVoicebank-Demandspeechen- [20] E. Tzinis, Z. Wang, X. Jiang, and P. Smaragdis, “Compute and
hancement without training on those datasets. We show memory efficient universal sound source separation,” Journal of
SignalProcessingSystems,pp.245—-259,2021.
the USS system is an approach that can address the USS
[21] X.Liu,H.Liu,Q.Kong,X.Mei,J.Zhao,Q.Huang,M.D.Plumbley,
problem. In future, we will improve the quality of the and W. Wang, “Separate what you describe: Language-queried
separatedwaveformsoftheweaklylabelledUSSsystems. audiosourceseparation,”inINTERSPEECH,2022.
[22] A.Mesaros,A.Diment,B.Elizalde,T.Heittola,E.Vincent,B.Raj,
and T. Virtanen, “Sound event detection in the DCASE 2017
challenge,”IEEE/ACMTransactionsonAudio,Speech,andLanguage
REFERENCES Processing,vol.27,no.6,pp.992–1006,2019.
[23] J.F.Gemmeke,D.P.Ellis,D.Freedman,A.Jansen,W.Lawrence,
[1] P.C.Loizou,SpeechEnhancement:TheoryandPractice. CRCpress, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology
2007. andhuman-labeleddatasetforaudioevents,”inIEEEInternational
[2] Y.Xu,J.Du,L.-R.Dai,andC.-H.Lee,“Aregressionapproachto ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),2017,
speechenhancementbasedondeepneuralnetworks,”IEEE/ACM pp.776–780.
TransactionsonAudio,Speech,andLanguageProcessing,vol.23,no.1, [24] A. Shah, A. Kumar, A. G. Hauptmann, and B. Raj, “A closer
pp.7–19,2014. look at weak label learning for audio events,” arXiv preprint
[3] F.-R. Sto¨ter, A. Liutkus, and N. Ito, “The 2018 signal separation arXiv:1804.09288,2018.
evaluationcampaign,”inInternationalConferenceonLatentVariable [25] Q.Kong,Y.Cao,T.Iqbal,Y.Wang,W.Wang,andM.D.Plumbley,
AnalysisandSignalSeparation. Springer,2018,pp.293–305. “PANNs:Large-scalepretrainedaudioneuralnetworksforaudio
[4] T.Heittola,A.Mesaros,T.Virtanen,andA.Eronen,“Soundevent patternrecognition,”IEEE/ACMTransactionsonAudio,Speech,and
detectioninmultisourceenvironmentsusingsourceseparation,” LanguageProcessing,vol.28,pp.2880–2894,2020.
in CHiME Workshop on Machine Listening in Multisource Environ- [26] Y. Gong, Y.-A. Chung, and J. Glass, “PSLA: Improving audio
ments(CHiME2011),2011,pp.36–40. tagging with pretraining, sampling, labeling, and aggregation,”
[5] N.Turpault,S.Wisdom,H.Erdogan,J.Hershey,R.Serizel,E.Fon- IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,
seca, P. Seetharaman, and J. Salamon, “Improving sound event vol.29,pp.3292–3306,2021.
detection in domestic environments using sound separation,” in [27] K.Chen,X.Du,B.Zhu,Z.Ma,T.Berg-Kirkpatrick,andS.Dubnov,
Detection and Classification of Acoustic Scenes and Events (DCASE) “HTS-AT: A hierarchical token-semantic audio transformer for
Workshop,2020. sound classification and detection,” in IEEE International Confer-
[6] S. Haykin and Z. Chen, “The cocktail party problem,” Neural enceonAcoustics,SpeechandSignalProcessing(ICASSP),2022,pp.
Computation,vol.17,no.9,pp.1875–1902,2005. 646–650.
[7] G.J.BrownandM.Cooke,“ComputationalAuditorySceneAnal- [28] Q.Kong,Y.Xu,W.Wang,andM.D.Plumbley,“Ajointdetection-
ysis,”ComputerSpeech&Language,vol.8,no.4,pp.297–336,1994. classification model for audio tagging of weakly labelled data,”
[8] D. Wang and G. J. Brown, Computational auditory scene analysis: in IEEE International Conference on Acoustics, Speech and Signal
Principles,algorithms,andapplications. Wiley-IEEEpress,2006. Processing(ICASSP),2017,pp.641–645.
[9] D. Stoller, S. Ewert, and S. Dixon, “Wave-U-Net: A multi-scale [29] Y.Wang,“Polyphonicsoundeventdetectionwithweaklabeling,”
neuralnetworkforend-to-endaudiosourceseparation,”inInter- PhDthesis,2018.
nationalSocietyforMusicInformationRetrieval(ISMIR),2018. [30] S.Adavanne,H.Fayek,andV.Tourbabin,“Soundeventclassifi-
[10] Y. Luo, Z. Chen, C. Han, C. Li, T. Zhou, and N. Mesgarani, cationanddetectionwithweaklylabeleddata,”inDetectionand
“Rethinkingtheseparationlayersinspeechseparationnetworks,” ClassificationofAcousticScenesandEvents(DCASE)Workshop,2019.
in IEEE International Conference on Acoustics, Speech and Signal [31] Q.Kong,Y.Wang,X.Song,Y.Cao,W.Wang,andM.D.Plumb-
Processing(ICASSP). IEEE,2021,pp.1–5. ley, “Source separation with weakly labelled data: An approach
[11] I. Kavalerov, S. Wisdom, H. Erdogan, B. Patton, K. Wilson, to computational auditory scene analysis,” in IEEE International
J. Le Roux, and J. R. Hershey, “Universal sound separation,” in ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),2020,
IEEE Workshop on Applications of Signal Processing to Audio and pp.101–105.
Acoustics(WASPAA),2019,pp.175–179. [32] K.Chen,X.Du,B.Zhu,Z.Ma,T.Berg-Kirkpatrick,andS.Dubnov,
[12] S. Wisdom, E. Tzinis, H. Erdogan, R. J. Weiss, K. Wilson, and “Zero-shotaudiosourceseparationthroughquery-basedlearning
J. R. Hershey, “Unsupervised speech separation using mixtures from weakly-labeled data,” in Proceedings of the AAAI Conference
of mixtures,” in Neural Information Processing Systems (NeurIPS), onArtificialIntelligence,vol.36,no.4,2022,pp.4441–4449.
2020. [33] Q. Kong, Y. Cao, H. Liu, K. Choi, and Y. Wang, “Decoupling
[13] E.Tzinis,Z.Wang,andP.Smaragdis,“Sudorm-rf:Efficientnet- magnitude and phase estimation with deep resunet for music
worksforuniversalaudiosourceseparation,”inIEEEInternational source separation,” in International Society for Music Information
WorkshoponMachineLearningforSignalProcessing(MLSP),2020. Retrieval(ISMIR),2021.
[14] P. Seetharaman, G. Wichern, S. Venkataramani, and J. Le Roux, [34] D.D.LeeandH.S.Seung,“Algorithmsfornon-negativematrix
“Class-conditional embeddings for music source separation,” in factorization,”inNeuralInformationProcessingSystems(NeurIPS),
IEEE International Conference on Acoustics, Speech and Signal Pro- 2000.
cessing(ICASSP),2019,pp.301–305. [35] A.De´fossez,N.Usunier,L.Bottou,andF.Bach,“Demucs:Deep
[15] E. Tzinis, S. Wisdom, J. R. Hershey, A. Jansen, and D. P. Ellis, extractor for music sources with extra unlabeled data remixed,”
“Improving universal sound separation using sound classifica- arXivpreprintarXiv:1909.01174,2019.
tion,”inIEEEInternationalConferenceonAcoustics,SpeechandSignal [36] ——,“Musicsourceseparationinthewaveformdomain,”arXiv
Processing(ICASSP),2020,pp.96–100. preprintarXiv:1911.13254,2019.
[16] Q.Wang,H.Muckenhirn,K.Wilson,P.Sridhar,Z.Wu,J.Hershey, [37] Y.LuoandN.Mesgarani,“Tasnet:time-domainaudioseparation
R. A. Saurous, R. J. Weiss, Y. Jia, and I. L. Moreno, “Voicefilter: networkforreal-time,single-channelspeechseparation,”inIEEE
Targeted voice separation by speaker-conditioned spectrogram International Conference on Acoustics, Speech and Signal Processing
masking,”inINTERSPEECH,2019. (ICASSP),2018,pp.696–700.
[17] B.Gfeller,D.Roblek,andM.Tagliasacchi,“One-shotconditional [38] ——,“Conv-TasNet:Surpassingidealtime–frequencymagnitude
audio filtering ofarbitrary sounds,” in IEEE International Confer- maskingforspeechseparation,”IEEE/ACMtransactionsonaudio,
enceonAcoustics,SpeechandSignalProcessing(ICASSP),2021,pp. speech,andlanguageprocessing,vol.27,no.8,pp.1256–1266,2019.
501–505. [39] Y.LuoandJ.Yu,“Musicsourceseparationwithband-splitrnn,”
[18] W.Choi,M.Kim,J.Chung,andS.Jung,“LaSAFT:LatentSource arXivpreprintarXiv:2209.15174,2022.
AttentiveFrequencyTransformationforConditionedSourceSep- [40] A. Narayanan and D. Wang, “Ideal ratio mask estimation using
aration,” in IEEE International Conference on Acoustics, Speech and deep neural networks for robust speech recognition,” in IEEE
SignalProcessing(ICASSP),2021,pp.171–175. International Conference on Acoustics, Speech and Signal Processing,
[19] F.Pishdadian,G.Wichern,andJ.LeRoux,“Learningtoseparate 2013,pp.7092–7096.
soundsfromweaklylabeledscenes,”inIEEEInternationalConfer- [41] D.S.Williamson,Y.Wang,andD.Wang,“Complexratiomasking
enceonAcoustics,SpeechandSignalProcessing(ICASSP),2020,pp. formonauralspeechseparation,”IEEE/ACMTransactionsonAudio,
91–95. Speech,andLanguageProcessing,vol.24,no.3,pp.483–492,2015.
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 15
[42] P.-S. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis, ducing a control mechanism in the U-Net for multiple source
“Jointoptimizationofmasksanddeeprecurrentneuralnetworks separations,”arXivpreprintarXiv:1907.01277,2019.
formonauralsourceseparation,”IEEE/ACMTransactionsonAudio, [62] S.IoffeandC.Szegedy,“Batchnormalization:Acceleratingdeep
Speech,andLanguageProcessing,vol.23,no.12,pp.2136–2147,2015. networktrainingbyreducinginternalcovariateshift,”inInterna-
[43] N. Takahashi, N. Goswami, and Y. Mitsufuji, “MMDenseLSTM: tionalConferenceonMachineLearning,2015,pp.448–456.
An efficient combination of convolutional and recurrent neural [63] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville,
networksforaudiosourceseparation,”inIEEEInternationalWork- “FiLM: Visual reasoning with a general conditioning layer,” in
shoponAcousticSignalEnhancement(IWAENC),2018,pp.106–110. ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.32,
[44] S.Uhlich,M.Porcu,F.Giron,M.Enenkl,T.Kemp,N.Takahashi, no.1,2018.
and Y. Mitsufuji, “Improving music source separation based on [64] K.He,X.Zhang,S.Ren,andJ.Sun,“Identitymappingsindeep
deep neural networks through data augmentation and network residual networks,” in European Conference on Computer Vision.
blending,”inIEEEInternationalConferenceonAcoustics,Speechand Springer,2016,pp.630–645.
SignalProcessing(ICASSP),2017,pp.261–265. [65] E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, “FSD50k:
[45] P.Chandna,M.Miron,J.Janer,andE.Go´mez,“Monoauralaudio an open dataset of human-labeled sound events,” IEEE/ACM
source separation using deep convolutional neural networks,” TransactionsonAudio,Speech,andLanguageProcessing,2020.
in International Conference on Latent Variable Analysis and Signal [66] E.Manilow,G.Wichern,P.Seetharaman,andJ.LeRoux,“Cutting
Separation. Springer,2017,pp.258–266. music source separation some Slakh: A dataset to study the
impactoftrainingdataqualityandquantity,”inIEEEWorkshopon
[46] Y.Hu,Y.Liu,S.Lv,M.Xing,S.Zhang,Y.Fu,J.Wu,B.Zhang,and
ApplicationsofSignalProcessingtoAudioandAcoustics(WASPAA),
L.Xie,“DCCRN:Deepcomplexconvolutionrecurrentnetworkfor
2019.
phase-awarespeechenhancement,”inINTERSPEECH,2020.
[67] F.-R.Sto¨ter,S.Uhlich,A.Liutkus,andY.Mitsufuji,“Open-Unmix-
[47] A. Jansson, E. Humphrey, N. Montecchio, R. Bittner, A. Kumar,
Areferenceimplementationformusicsourceseparation,”Journal
and T. Weyde, “Singing voice separation with deep U-Net con-
ofOpenSourceSoftware,vol.4,no.41,p.1667,2019.
volutionalnetworks,”inInternationalSocietyforMusicInformation
[68] B. Xu, N. Wang, T. Chen, and M. Li, “Empirical evaluation
Retrieval(ISMIR),2017.
of rectified activations in convolutional network,” arXiv preprint
[48] R.Hennequin,A.Khlif,F.Voituret,andM.Moussallam,“Spleeter:
arXiv:1505.00853,2015.
afastandefficientmusicsourceseparationtoolwithpre-trained
[69] D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimiza-
models,” Journal of Open Source Software, vol. 5, no. 50, p. 2154,
tion,”inInternationalConferenceonLearningRepresentations(ICLR),
2020.
2015.
[49] A.De´fossez,“Hybridspectrogramandwaveformsourcesepara- [70] J. Thiemann, N. Ito, and E. Vincent, “DEMAND: a collection of
tion,” in Proceedings of the ISMIR 2021 Workshop on Music Source multi-channel recordings of acoustic noise in diverse environ-
Separation,2021. ments,”inProceedingsofMeetingsonAcoustics,2013.
[50] S. Rouard, F. Massa, and A. De´fossez, “Hybrid transformers for [71] E.Vincent,R.Gribonval,andC.Fe´votte,“Performancemeasure-
musicsourceseparation,”inInternationalConferenceonAcoustics, mentinblindaudiosourceseparation,”IEEETransactionsonAudio,
Speech,andSignalProcessing(ICASSP),2023. Speech,andLanguageProcessing,vol.14,no.4,pp.1462–1469,2006.
[51] C. Veaux, J. Yamagishi, and S. King, “The Voice Bank Corpus: [72] ITU-T,“Perceptualevaluationofspeechquality(PESQ):Anobjec-
Design, collection and data analysis of a large regional accent tivemethodforend-to-endspeechqualityassessmentofnarrow-
speech database,” in International Conference Oriental COCOSDA bandtelephonenetworksandspeechcodecs,”Rec.ITU-TP.862,
with Conference on Asian Spoken Language Research and Evaluation 2001.
(O-COCOSDA/CASLRE),2013. [73] S.R.Quackenbush,T.P.Barnwell,andM.A.Clements,Objective
[52] Z. Rafii, A. Liutkus, F.-R. Sto¨ter, S. I. Mimilakis, and R. Bittner, MeasuresofSpeechQuality. PrenticeHall,1988.
“TheMUSDB18corpusformusicseparation,”Dec.2017.[Online]. [74] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec:
Available:https://doi.org/10.5281/zenodo.1117372 Unsupervised pre-training for speech recognition,” in INTER-
[53] J.Salamon,C.Jacoby,andJ.P.Bello,“Adatasetandtaxonomyfor SPEECH,2019.
urbansoundresearch,”inProceedingsofthe22ndACMinternational
conferenceonMultimedia,2014,pp.1041–1044.
[54] E. Fonseca, M. Plakal, F. Font, D. P. Ellis, X. Favory, J. Pons,
andX.Serra,“General-purposetaggingofFreesoundaudiowith
Audioset labels: Task description, dataset, and baseline,” in Pro-
ceedings of the Detection and Classification of Acoustic Scenes and
Events2018Workshop(DCASE),2018.
[55] S.Wisdom,H.Erdogan,D.P.Ellis,R.Serizel,N.Turpault,E.Fon-
seca, J. Salamon, P. Seetharaman, and J. R. Hershey, “What’s all
the fuss about free universal sound separation data?” in IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP),2021,pp.186–190.
[56] Y. Gong, Y.-A. Chung, and J. Glass, “AST: Audio spectrogram
transformer,”inINTERSPEECH,2021.
[57] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen,
R.C.Moore,M.Plakal,D.Platt,R.A.Saurous,B.Seyboldetal.,
“CNN architectures for large-scale audio classification,” in IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP),2017,pp.131–135.
[58] Y. Wang, J. Li, and F. Metze, “A comparison of five multiple
instancelearningpoolingfunctionsforsoundeventdetectionwith
weaklabeling,”inIEEEInternationalConferenceonAcoustics,Speech
andSignalProcessing(ICASSP),2019,pp.31–35.
[59] Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,andB.Guo,
“Swintransformer:Hierarchicalvisiontransformerusingshifted
windows,”inProceedingsoftheIEEE/CVFInternationalConference
onComputerVision(ICCV),2021,pp.10012–10022.
[60] M.Delcroix,J.B.Va´zquez,T.Ochiai,K.Kinoshita,Y.Ohishi,and
S. Araki, “SoundBeam: Target sound extraction conditioned on
sound-classlabelsandenrollmentcluesforincreasedperformance
andcontinuouslearning,”IEEE/ACMTransactionsonAudio,Speech,
andLanguageProcessing,2022.
[61] G. Meseguer-Brocal and G. Peeters, “Conditioned-U-Net: Intro-
JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 16
APPENDIX
20
105
10
0 103
10
20
105
10
0 103
10
20
105
10
0 103
10
20
orale embedding
avg. embedding
oracle prediction 105
10 one-hot
0 103
10
Fig.7.USSresulton527AudioSetsoundclasses.
)Bd(
iRDS
)Bd(
iRDS
)Bd(
iRDS
)Bd(
iRDS
ecneliS
enohpeleT
taC
gnildI
langis ysuB
yawdaor ,esion ciffarT
gniraeT
gniraelc taorhT
evaw eniS
ratiug cirtcelE
naom ,liaW
euqerf dim( enigne muideM
lleb elcyciB
tnaP
euqerf hgih( enigne thgiL
acinomraH
gnibbos ,gniyrC
suB
rettahS
)doof( gniyrF
kcot-kciT
taelB
gninoitidnoc riA
nagrO
etulF
retirwepyT
cirtcele ,swodniw rewoP
wasniahC
eltsihW
)neris( ecnalubmA
poohW
tnurG
neris esnefed liviC
noisolpxE
cisum tnednepednI
rettahC
nrohgoF
acirfA fo cisuM
)tnemurtsni( elttaR
kcalc-ytekcilC
drib ,noitazilacov driB
kcolb dooW
llirD
cisum yrgnA
enot laiD
nerdlihc rof cisuM
nogaw niart ,rac daorliaR
nevo evaworciM
cisum samtsirhC
olonapmac( gnignir egnahC
,slamina mraf ,kcotseviL
allepac A
elbmuR
noidroccA
laeuqS
noitces gnirtS
mralA
petsbuD
taobdeeps ,taobrotoM
cisum kloF
hsurbhtooT
lloR
tronS
gnignis elameF
gniltsihW
pria ,tfarcria gniw-dexiF
otiuqsoM
cisum esuoH
lwoH
ratiug ssaB
woeM
ssiH
c gek mn, l g sbdn )ea oe ekp e glgc e nw t mn lrcr ror t g dpp pe k g o g eh lh lper g rn sl l he hn ka mm r yr g r pma erc a) yr dl n r) s ce t tln e o lwl au n en i a ge eu g c na oi a ee e er plo nr si nii c e ie n e gc cuo iok lm sld ri nn o ol r lon zr imn o ia c ss in nu gsu un n e o ust bos ss sr o r dei ofY mb z su ra o e rau u e kt cn te ont te t nzm tn z eaS er oi i io oe p ia uaa bb uero ii iR d iei- D ii loo ieo a o igg ci ui hDu uu u aT b o pi sr An zi h puf pt l t t at s xk Fb tw h qpk mi i i cfee i rcH rm Dmu mlo ii uu t bn le uG pn s Ge r W mc Cds ssh ihh it la h tl G a a tf r iG hihm m lm mnr iiCt s ya g oqs a p Ro ieg c r nb ao ip r ,gtlo Sep a nS et sr n gG o re euinK rk pS Su fn Lecr d hp C r slm u e tsp Su eta ,T u H gM ci u Sde bd la en oG A C(r ao ena e ab pe t l k oh hUc oS o g nSm c ea a , r yo co gc r sr wia C Cr g c rce enar s d n tnt i p lD rC i u lb freni k i mlat th oi it n, o tn n cr lCw uroB lh aa edi in Vc ci nng mh ail en lsn a ny k os e st ti cH d Der i hg C , f ie i iu et d r eCc F H (iPo a s i i sa yC c uke , ln J nlc xiy yPe o, Dr p no t sr (nr k eByW ,r ,aT - B un ur n ,rbc-a ov hii k iS s e so B lF aa h e e yc lct i h w , eM sG or u Mtr F Se ae R tg l T sa gc C- i rTCx c hto ae Ta pog crr ke sH BRe cn b an iEh T W cltC p pAn l acN lli cm i o ea yE r F im oi r e Z,l Pt e b BM gl scisl ir a p ac ef n a ( ,d ei S Bh ig h M re kr sn ae c nk ii o V Dwc ie S Ru c ol I rP C
hc e gkg e krf ec kkw r nea n ng gsoe k e ke mml )r a c ce mh ge n mo po c n egh g sc g l y pk g te ng f kr rg hrs cr k g sw)o s tr r gmt n re tn h i tl wr h e fe ge c aae ni na aee e eec s l nli ards er cc i io o lii i a cnl r cn n av ll nr rr ii nhcl n snlg in ao s annn rn gnn n noo s su i to as s socss sa cc aPg t ls io i atur aut n ua nt ot o hin u t ftt y mte ea a oo O o yia i usr r o nsi oo o gu Fii as n ic l e fa oiii i iia i ii ui if r us u uauu uo t i or msn loiihyn tlw a gl R PPg ttS ws gr r lc g h F e lr B rS Gh lr in -i rm hb mH ii a rr Sho al lu ra lc lre ar d E daii rm i ti c Sm l f ope lara m hW e oD m mt Mm m e h cd i mc d Ni d S Bgi w tsn oPn hp CCi , rnn sC aA o gsu He er h kela ias mPk HO kc l nG s Chst pr tnkp wi tBd p Su u n, u uCBP u , e e w c iiu mc nr v g uc vl ce k eioo S s aSr ne r n ac ss, e cc cor Ss ,gCn S l n cr rad ge a r( io r i Sh eu r s ien e qr ltm Ho v uni ta tMto u nr ii c Ci r R ir, riy n,a rD ,a nd r ed La oB nh as e s sffv nr O oe d oit sdda eh a PaHn eet r i ah c iSe g i lns o lo n nMn aB oWln oc T, tl ndem W w a t l(i g lhtga g n sk r eC ih A nh o u i iS a r reo s rh W a, d Mc ,, i c lr ni nc eS T tnr tC n eg u w i ls e gh aP WlC T r cc e cel ao cuE ii gin l Nrn ir eE nT oJ y g es i W M eA ct h oD ,rfui h C ic ,s tlni( e lp reC h, t Es E aG we Ph te amr b lu sodt u rs o u M e e i bnE doB dA Btd r wio ih eai M Fh we ot ra TWk R oBo BmS
yot eF e l) ) tee g ende g no ed ek gcal cd n gl gnmc ne eg ee kk gag er ls ec k ya r rk mn k ezwe h c s ep nt er p tr ge h eh g ry dc ep c est el low la a l d ei oeM i e o ee een nu ae rzi i ti ei ii a iad lca nc nn tn orc nl nar u r cr n ar cn arr i np nl ses lnnn sl l dnna c oo c isn n oo kh s ns as t os st tb mc kb tr hea cio uaa ot ia ei ooe l hz wku ha iq fuu p uo ii nba ae gt nea ic oui ao oog oug usiip i i il a ii oi p at iTi i i f sB p uW uuuu urn on C oyT rcs ut lym h t cc gth m p Dk e ss rz oma c rro Hw wdh p atBp cio uiDl ui or r ar gu g il u Hh t rh hh oer r w qeo q mDl i pu m t ym emmm mn s Du a nu c sC p mo i s a-u hl ha ac i te Gc nr rc ar C, S b Ge Rc o cO ZaG i MgR mo zrpq r etr mp p ptl r T,y e iu ut u ra uo u eW i g ac h r, s T r oo yg rBa wl fcoe le Msi S a ne yB aC T el as zn t e , ep b nr C, RiS t Cs o a orrS c, ehu r l rm hdp npe c ly Cl tgn ram le l nh rht , o r ae Gac v ip Wah fS l a lD tEapC m i nu rx r cptP nr oS sF e C on u li mph c o rnk B lnh ios te tHl km mr e e b id Tr (c Bw imr h a ea o hi a wfi tn ut BoB aP pc iai a ec msm al n a wi P J rii rhe l ad CWES i r C ts o Vlsa a a m sa mrat eC i b ar ATleH S e rn C f e, n (rs aa a u d lm d ri H py oo Lm r, (, a , ,tda m ti eaS rR t lenr l t d rg r u ( Sli oui ec D o al ta nc ie Ahdu oC len pa c F a gbi ri nt oi c al Cn ig i h sn i E HS mt Wst ny it b , bu uo ( C ga eo e sn e i l ro hCr Mr ne ig ch dI K uat r C cam enp ic a Ss t ,R e ie N at eyp ol ul r dvEe Dp cO a iT a S seT tuH
O
an se n kn tc gl gr t e)c rp k dc st l, )a ly rg l i r gg eed cte )n e y pg p kgg g p kk o eg e nsr go e ce pc )e g st ect rcc tg y er g te .e s nct l aa ew nge r p cea e l dr e ugo n c ia ie ee celno rei ii nns mn i' l ii i ii ib r c cc o ra kls rsn o nnjw sz cgi l n aps o in i nn n oen no nn n o nsa bs as o t sc ss s ss r eh pd o tanw fk n hh es ne n ue akh ace om oa dta Yo i o vi o a aa ue o iiu aar sa r uM noi ii if h etn i isi oi o ea iii iuu uF ua uu s u uu iro p ltdo uac h mspdl bl ta h ti Wd Sl ott l fb c aomu d op rob Pds ch hb naF b e il u mw e l a pr TFg w rl avb km T h ii e SdB C u m fa am ir mT mm m mm tmn c i Tsga cr rrl q , imBi vv bBm cn e b ,eho o B brh nS set (C e e n e dmau h Qlnn nv s ef lr nCb uol ( S-r pz cn h a eo (rr p n pa ss oe t t ola lr pu eS igr Vr l lG a o l t a rL We es S i ica ice i,a c ri li m ra st cc a Bt dl glkf r a c d ya l lk o poi o rm ls be al s tT e rn aR S toG a b gn Th ia ,u e i YepA he ar ee dG ae B ec sRr ic ho nA ntc ar p ns , r A mrl p, n i x lr c ou ni B p c, roe tp n kk r cn n c rT aa wh ,l Tr f, o b Tal D n Tui khc au,io , g ei moe o ga feoe lo i oicc S sna e Te e cmlF gl uR d ur i Ei l Sh o ls olii l, nVl inS Fom tr Maa v D vh S,t Sil ip aVm p tf t ns uli pae Ssgf r e rd ie eh ti i Ga Jh ceRd o o dw g,i ct d u l e a( v en, n n l gr iW ph e,W a g n t p aF ka R e l C BeE ed ru o lh nS aesr rs ru Vma cre dSse orr o ag n ihpT ao tc wa wi i il tS l ui e nv o s Wse Bis ol o era ,f n rn l en c Met ,- b Ct hr I bd E De ccr dDp d Ce uAa r l igu ni sle bB E nC i nH bWi IS uH
cisum poh piH
nikaeps dik ,hceeps dlihC
skroweriF
tcesnI
gnilgnaj syeK
ssargeulB
aeps namow ,hceeps elameF
dniW
rruP
repmihW
cisum emag oediV
mooB
retsoor ,nekcihC
gnigniS
cisum ynnuF
cisum naitsirhC
nuR
ecnuob llabteksaB
seulb dna mhtyhR
gnikaeps nam ,hceeps elaM
ooC
riohC
nuorgrednu ,ortem ,yawbuS
kcor evissergorP
olleC
gnippaR
zzaJ
ecafrus no niaR
raoR
raC
esroH
latem yvaeH
ecim ,star ,stnedoR
)doof( gnippohC
enignE
dniwdoow ,tnemurtsni dniW
lleB
llebwoC
rednuhT
gniddikS
cisum ecnaD
sevlow ,sgod ,eadinaC
enotediS
aciremA nitaL fo cisuM
tuohS
ocnemalF
muh sniaM
slleb ralubuT
hsurbhtoot cirtcelE
)diuqil( pmuP
elknit ,elgniJ
paT
sdrac gnilffuhS
wow-woB
tfarcriA
atiug edils ,ratiug leetS
neriS
syelluP
wercsria ,relleporP
speeb gnisreveR
esluP
gniZ
kwauqS
kniO
gnimaercS
gnignis citehtnyS
akS
gnignir lleb enohpeleT
spilc
oidua
fo
rebmuN
spilc
oidua
fo
rebmuN
spilc
oidua
fo
rebmuN
spilc
oidua
fo
rebmuN

1
LegoNN: Building Modular Encoder-Decoder
Models
Siddharth Dalmia, Dmytro Okhonko, Mike Lewis, Sergey Edunov, Shinji Watanabe, Fellow, IEEE,
Florian Metze, Fellow, IEEE, Luke Zettlemoyer, and Abdelrahman Mohamed, Member, IEEE
Abstract—State-of-the-art encoder-decoder models (e.g. for Practitioner’s Need Model Inventory
machine translation (MT) or automatic speech recognition
(ASR)) are constructed and trained end-to-end as an atomic De→En MT
unit. No component of the model can be (re-)used without the Ro→En MT
others,makingitimpossibletoshareparts,e.g.ahighresourced
Encoder Decoder
decoder, across tasks. We describe LegoNN, a procedure for English ASR
building encoder-decoder architectures in a way so that its
parts can be applied to other tasks without the need for any
fine-tuning. To achieve this reusability, the interface between Only train new Reuse Decoder
encoder and decoder modules is grounded to a sequence of Encoders! (No fine-tuning!)
marginal distributions over a pre-defined discrete vocabulary.
We present two approaches for ingesting these marginals; one is
Encoder
differentiable, allowing the flow of gradients across the entire
network, and the other is gradient-isolating. To enable the
LegoNN Benefits
portability of decoder modules between MT tasks for different
source languages and across other tasks like ASR, we introduce Fewer GPU hours Promote Reusability
a modality agnostic encoder which consists of a length control
Utilize decoders from high resourced tasks
mechanism to dynamically adapt encoders’ output lengths in
order to match the expected input length range of pre-trained
Ability for further end-to-end fine-tuning
decoders. We present several experiments to demonstrate the
effectiveness of LegoNN models: a trained language generation
LegoNNdecodermodulefromGerman-English(De-En)MTtask Fig. 1. LegoNN Framework: Building encoder-decoder models in the
LegoNN framework, allows practitioners to reuse components like decoder
can be reused without any fine-tuning for the Europarl English
modules for various sequence prediction tasks. For example, in this figure,
ASRandtheRomanian-English(Ro-En)MTtasks,matchingor
an English predicting decoder from a German-English machine translation
beating the performance of baseline. After fine-tuning, LegoNN
system can be re-used for both Romanian-English machine translation and
models improve the Ro-En MT task by 1.5 BLEU points and Englishspeech recognitionwithout anyfine-tuning steps.This savesoverall
achieve12.5%relativeWERreductionontheEuroparlASRtask. compute resources, promotes re-usability, and allows practitioners to utilize
To show how the approach generalizes, we compose a LegoNN decodersfromhigh-resourcedtasksforunder-resourcedones.Thecomposed
ASR model from three modules – each has been learned within model is end-to-end differentiable leaving room for further improvements
differentend-to-endtrainedmodelsonthreedifferentdatasets– throughfine-tuning.
achieving an overall WER reduction of 19.5%.
Index Terms—end-to-end, encoder-decoder models, modular-
overall due to the lack of re-usability of trained components
ity, speech recognition, machine translation
that perform the same logical function across tasks.
Motivated by modularity principles in software design [6]
I. INTRODUCTION
where modules have interpretable interfaces and are reusable
TRAINING end-to-end models for machine translation
within other programs, we seek to build an AI paradigm
(MT) or automatic speech recognition (ASR) require
where trained components (modules) of an end-to-end model
learning of multiple implicit functions [1]–[5]. An MT model
can be re-used across different tasks (programs) provided
is implicitly doing both word translation and language gener-
they perform the same function (interface). With a focus on
ation, while an ASR model combines phoneme recognition,
sequence prediction tasks, we introduce LegoNN, a proce-
pronunciation modeling, and language generation. These fully
dure for constructing encoder-decoder models where trained
differentiable models are conceptually simple and work well
components such as decoder modules can be reused across
in practice. However, they forgo opportunities to share com-
various sequence tasks such as MT, ASR, or Optical Char-
mon logical functions between different tasks, such as their
acter Recognition (OCR). As summarized in Fig. 1, for AI
decoders. Training such monolithic models leads to wasted
models, enforcing modularity helps save computing resources
compute during training and less interpretable architectures
by reusing components and helps build systems for under-
resourcedtasksbyutilizingshareablecomponentsfromhigher
ThisworkwasdoneatFAIR,MetaAI.
SiddharthDalmiaandShinjiWatanabeareaffiliatedwithLTIatCarnegie resourced tasks. Additionally, having interpretable interfaces
Mellon University. Florian Metze is affiliated with both Language Tech- enable monitoring the performance of individual encoder or
nologiesInstituteatCarnegieMellonUniversityandMetaAI.Abdelrahman
decoder modules and their contributions to the overall end-to-
MohamedisaffiliatedwithRembrandInc.DmytroOkhonkoisaffiliatedwith
SamayaAI.AllotherauthorsareaffiliatedwithMetaAI. end performance.
3202
luJ
11
]LC.sc[
2v81330.6022:viXra
2
More concretely, in our LegoNN encoder-decoder frame- II. BACKGROUND
work, encoders have an interpretable interface by outputting a A. Cross-Entropy Loss in Encoder-Decoder Models
sequence of distributions over a discrete vocabulary, derived
GivenaninputsequenceoflengthT,X andanoutputse-
from the final output labels (e.g. phonemes or sub-words). 1:T
quence,Y,whichcanbefactorizedintoasequenceoftokens,
Duringtraining,weaddanadditionalConnectionistTemporal
L,oflengthN,L ={L ∈V |n∈{1:N},L =Y}.
Classification(CTC)loss[7]ontheencoderoutputstoenforce 1:N n Attn 1:N
Theencoder-decodermodel[1],[2],[9]modelsthelikelihood
this modularity (§III-A). Our decoder modules are extended
with Ingestor layers (ING) that accept these distributions
(PVAttn) for the prediction of the next token (L n) given the
as inputs (§III-B2). We experiment with two types of in-
previous tokens (L 1:n(cid:57)1) and the input sequence (X 1:T),
gestor layers: a differentiable Weighted Embedding (WEmb) PV nAttn =P(·|X 1:T,L 1:n(cid:57)1), (1)
ingestor allowing gradient flow across the entire network, wherePVAttn istheposteriordistributionforpredictingthenext
n
and a gradient-isolating Beam Convolution (BeamConv) one.
tokenL atpositionndefinedoverthevocabularyspaceV
n Attn
Given that LegoNN decoder modules can be trained for
from n={1:N}.Theyare trainedbyminimizingthe token-
one MT task and then reused for another MT task with a
level cross-entropy (F ) loss between the true tokens L
CE 1:N
different source language or for a sequence task with the and the decoder predicted distributions (PVAttn),
1:N
same target language such as ASR or OCR, we propose a
hE =encoder(X ), (2)
modality agnostic encoder for sequence prediction task that 1:T 1:T
usesanoutputlengthcontroller(OLC)unittoadaptanyinput PV nAttn =softmax(decoder(hE 1:T,L 1:n(cid:57)1)), (3)
modalitytoasequenceofencoderrepresentationsthatmatches (cid:32) N (cid:33)
(cid:89)
the expected input length of another (§III-B1). LegoNNs, F CE(L 1:N,PV 1:A Nttn)=−log PV nAttn(y =L n) , (4)
as also demonstrated in our experiments, enable sharing of n=1
traineddecoders1 andintermediatemodulesbetweendifferent where PV nAttn(y = L n) is the probability of predicting the
tasks and domains without jointly training for both tasks and ground truth token L at n-th decoder step. To avoid an ex-
n
no fine-tuning steps. The composed LegoNN model preserves plosion in notations and maintain consistency between speech
end-to-enddifferentiabilityofeachindividualsystem,allowing andtextbasedencoder-decodermodels,weavoiddenotingany
room for further improvements through fine-tuning. changes to sequence lengths in Eq. (2) due to sub-sampling
Our experiments show that we can achieve modularity in speech encoders [4].
without sacrificing performance. On the standard large scale
En-De WMT and Switchboard ASR benchmarks, LegoNN B. Connectionist Temporal Classification Loss
models reach levels of performance competitive with the For the input sequence X and the output sequence Y,
1:T
standard monolithic architectures (§VI-A), while still passing which is factorized into a sequence of tokens L of length N,
our stress tests for testing modularity (§VI-B). We show the L = {L ∈ V |n ∈ {1 : N},L = Y}. CTC model
1:N n CTC 1:N
valueofmodularitybyseamlesslycomposingadecodermod- [7]modelsthelikelihood(PVCTC)ofproducingavalidX →L
ule trained within a German-English (De-En) WMT system alignment for each input step t,
with other pre-trained encoder modules from different MT
PVCTC =P(·|X ), (5)
and ASR systems, without any joint training or fine-tuning, t 1:T
to match or beat generation performance for the Europarl where PVCTC is the conditionally independent posterior dis-
t
English ASR task and the Romanian-English (Ro-En) WMT tribution for predicting the X → L alignment at time step
task (§VI-C). When such composedLegoNN models are fine- t defined over the vocabulary space V from t = {1 :
CTC
tuned for a few thousand steps towards the target domain, T}. They are trained using CTC loss [7] that minimizes
they improve the Ro-En MT task by 1.5 BLEU points and the negative conditional likelihood of all possible monotonic
improve on the Europarl ASR task by 12.5% WER relative to alignments, Z ∈Z(T,L), between sequence X and L where
the baseline system (§VI-D). To demonstrate the flexibility of each Z = {z t ∈ l|t ∈ {1 : T},l ∈ V CTC} such that Z is a
reusingLegoNNmodules,weconstructanASRsystemthatis valid alignment for producing L,
composedofmodulesthathavebeentrainedindependentlyon PVCTC =softmax(encoder(X )∗W ), (6)
three different tasks, without performing any fine-tuning and 1:T 1:T o
(cid:32) T (cid:33)
with almost no performance degradation. The modules are: F (L ,PVCTC)=−log (cid:88) (cid:89) PVCTC(z =z ) ,
(1)aphonemerecognizerfromtheEuroparlASRmodel,(2)a CTC 1:N 1:T t t
Z∈Z(T,L) t=1
pronunciationmodelfromtheTED-LIUMASRmodel,and(3)
(7)
alanguagegenerationdecoderfromtheWMTmodel(§VI-C).
With a few end-to-end fine-tuning steps, the composed model wherePV tCTC(z =z t)istheprobabilityofpredictingtheoutput
beats the baseline Europarl ASR model by 19.5% relative token z t ∈ V CTC for the t-th input step. W o ∈ Rd×|VCTC|
WER, also improving over the previously composed LegoNN projectstheencoderrepresentationsintotheoutputvocabulary
model for this task (§VI-D). spaceofV CTC andPV 1:C TTC ∈RT×|VCTC| isthelocally-normalized
per input step probabilities of the output alignment. We omit
the extra CTC blank symbol in the equation above for clarity
1Our work on LegoNNs is orthogonal to the recent work on sharing
of presentation; see [7] for more details. The marginalization
pre-trained encoders, e.g BERT [8]. Combining the benefits of these two
complementaryapproachesisleftforfuturework. sum is efficiently computed using dynamic programming.
3
III. LEGONNFORMODULARENCODER-DECODER individual unit in these hidden vectors does not have physical
MODELS meaning. Their properties can vary across random seeds and
training hyper-parameters even when the same task, model,
We propose decomposing encoder-decoder models into one
and training data are used. As we see from Eq. (1) and (3),
(or more) encoder modules followed by an auto-regressive
although the decoder is conditioned on encoder states (hE )
decoder module, which can be reused for other tasks and do- 1:T
the encoder-decoder still directly models the conditional next
mains,withoutsacrificingitsend-to-enddifferentiability.Each
token prediction over the input as these encoder states tightly
module produces a sequence of marginal distributions over
coupled into the model and have no physical meaning.
a pre-defined discrete vocabulary, which is consumed by an
To enable reusability between different tasks and models,
ingestor component in the subsequent module. Each encoder
LegoNN grounds encoder outputs into a distribution over a
module can be made modality agnostic by using an output
discrete vocabulary space that is pre-defined by the model
length controller unit that matches the input sequence length
designer. For applications such as ASR and MT, such inter-
to the expected length of the subsequent module, allowing
mediate discrete vocabulary can be defined over phonemes,
re-usability between similar tasks from different modalities.
orsomebyte-pairencoding(BPE)dictionary[13],[14]driven
LegoNN modules trained either jointly or independently can
fromthetasklabels,whichdoesnotneedtobethesameasthe
be reused for new tasks without the need for any fine-tuning.
model’s output dictionary. Such an encoder can be designed
The LegoNN procedure introduces three operations that we
similarly to Eq. (6), to produce locally-normalized output
discuss in detail in this section, (1) designing an interpretable
distributions for each encoder position (PVCTC) learned using
interface between modules (§III-A), (2) output length con- 1:T
CTC loss (Eq. 7), where V is the intermediate discrete
troller unit in the LegoNN encoder (§III-B1) and (3) ingestor CTC
vocabularyoftheinterface.Followingthenotationsintroduced
of probability distributions in the LegoNN decoder (§III-B2).
in §II, a LegoNN encoder-decoder model effectively models
WealsoprovidesomeadditionalinsightsintotrainingLegoNN
the likelihood for the next token prediction given the previous
systems (§VII-E).
tokens (L 1:n(cid:57)1), and the encoder states (PV 1:C TTC),
A. Designer-defined module interface
PV nAttn =P(·|PV 1:C TTC,L 1:n(cid:57)1). (8)
We look at encoder-decoder models as full software pro- Note how the conditional is different from Eq. (1). Here,
gramsexecutingthespecificfunctionofmappingonesequence the encoder states (PVCTC) are not just hidden transformations
1:T
of input symbols or vectors to another. While some com- of the input (X 1:T), but distributions over an intermediate
ponents of software programs, i.e. libraries, may be reused vocabulary of the interface, V CTC defined over the output
for numerous future programs, trained components of the sequence Y and are modeled by the CTC loss given input,
traditional encoder-decoder models [1], [2], like decoders, are PVCTC =P(·|X ). (9)
not designed to be reused independently with other models 1:T 1:T
or tasks. 2 Well-defined and abstract input/output interfaces In order to model the two distributions in Eq. (8) and (9), we
are prerequisites for developing reusable software libraries, cantrainLegoNNmodelswithasupervisedCTClossapplied
however, encoder-decoder models fall short in this respect. over the encoder output distributions (Eq. 7) along with the
In order to have modular software properties between en- decoder token-level cross-entropy loss (Eq. 4),
coder and decoder modules, we propose that the interface F =F (LVAttn,PVAttn)+F (LVCTC,PVCTC), (10)
obj CE CTC
between modules can be defined as distributions over a fixed
categorical space while capturing similar information as the where LVAttn and LVCTC are target sequences tokenized using
encoder from a standard monolithic encoder-decoder model. the vocabularies V Attn and V CTC. This modeling framework
We define our interface as a sequence of conditionally inde- allowsthedecodermoduletoacceptanyencoderthatproduces
pendentdistributionsoveravocabularyspacelearnedusingthe the same distribution as the decoder was trained towards
CTC loss for each input step (§III-A1). We then extend this (PV 1:C TTC), thereby building an interpretable interface to enforce
approach to introduce more than one modularity point in an modularity between the encoder-decoder modules. Such mod-
encoder-decodermodelbyhavingasequenceofencodermod- eling also gives the encoder and decoder modules in the
ules, each with defined functionalities and interface (§III-A2). LegoNN framework specific functionalities toward the target
In order for the distribution over the vocabulary interface to task. For example, an MT encoder module is not expected
beeffective,wealsodiscusstheimportanceofgeneralizability to solve the overall translation task but rather acts as a
of the interface for future tasks (§III-A3). word/phrase translation component or an ASR encoder acts
1) Interpretableinterfacesbetweenencodersanddecoders: as a phoneme/sub-word recognizer whose outputs are refined
Encoders communicate with decoders through a sequence using an auto-regressive decoder.
of continuous hidden representations (Eq. 3) that are by- WhilethesoftmaxoperationinEq.(6)producesaninterme-
products of the end-to-end model optimization process. An diate distribution over the defined vocabulary and the model
can be trained with only the decoder cross-entropy loss, it is
2Whiledeeplearningtoolkitshaveamodularcodedesignwherethecode stillessentialtoenforcethedesiredencoderdistributionsusing
for similar model architectures can be re-used to train new components for CTC loss. Experiments in §VII-B show that without CTC
differenttasks[10]–[12].Inthiswork,weaimtomaketheentirecomponent
loss at the encoder and only training with the decoder cross-
ofatrainedmodelre-usableacrosstasks,whichincludesre-usingthecode,
parameters,andinput/outputspaceofthecomponents. entropyloss,theencoder-decodermodelsarenotmodular.Due
4
Fig.2. LegoNN Encoder-Decoder Model:Thisfigurepresentstheschem aticsandtheinformationflowinourLegoNNencoder-decodermodel.LegoNN
decomposesencoder-decodermodelsintoreusablemodulesby(1)groundingmoduleoutputintoadistributionoveraninterpretablevocabularyusingsoftmax
outputinLegoNNencoderguidedusingCTCLoss;(2)addinganingestor(ING)toprocessinputmarginaldistributionsintheLegoNNdecoder;and(3)an
outputlengthcontroller(OLC)intheLegoNNencodertomatchtheinputlengthofsubsequentmodules.
to the marginalization of all output alignments (Eq. 7), CTC adding more modeling capacity to one module over the other.
loss produces conditionally independent output distributions and (b) An encoder module may be trained in conjunction
foreachinputstepthatisdependentonlyontheinput(Eq.5). with its decoder or independently by itself. For an ASR task,
This ensures that the distributions produced by the CTC loss thismaybeusefulwhengettingaccesstomoreaudiotraining
have no output label bias [15] and represent a transformation data or adapting the system to new acoustic conditions where
functionofonlytheinput,makingitpossibletocapturesimilar retraining the decoder would be wasteful or even hurtful
information as that of an encoder from a monolithic encoder- towards generalization. We present these properties through
decoder model. 3 experiments in §VII-A and §VII-C.
2) Introducing multiple modularity points: A LegoNN 3) Defining vocabularies that generalize across tasks: As
model is not restricted to containing only two modules; an with software libraries, designing a module interface with the
encoder and a decoder. There may be a sequence of encoder right level of generality is challenging but allows for wider
modules, each designed to perform a certain function through reuseofmodulesacrosstasksanddomains.Toreuseadecoder
their respective pre-defined output vocabulary. The ASR sys- module from an MT LegoNN model to an ASR task, the
tem is one example whose encoder can be divided into two output vocabulary of the speech encoder must be compatible
modules, a phoneme recognizer, and a pronunciation model, with the input vocabulary of the translation decoder. This
followed by the auto-regressive language generating module. is true even within a single task – for example in ASR, a
In this case, the CTC loss is applied more than once, and vocabulary of phonemes developed for a phoneme recognizer
training of LegoNN models can be extended from Eq. (10) to module in a read speech dataset, e.g. audiobooks, may not
a joint loss over each module, be the best one for spontaneous conversational situations
which are full of hesitations and false starts. We propose
M−1
F
obj
=F CE(LV A(M ttn) ,PV A(M ttn) )+ (cid:88) F CTC(LV C(i T) C,PV C(i T) C), (11) designing a shared vocabulary by combining target units from
multiplepotentialfuturetasksandfindingavocabularyattheir
i=1
intersection.
where M is the total number of modules in the LegoNN
system, Vi is the pre-defined vocabulary at the interface of
module i. PV(i) is the predicted distribution over vocabulary B. LegoNN Encoder-Decoder model
Vi at module i and LV(i) is the target sequence tokenized Fig. 2 provides the schematics of our proposed LegoNN
using vocabulary V(i). All modules in the LegoNN system encoder-decoder model, which follows the modeling frame-
haveaCTClossattheiroutput,exceptthefinalauto-regressive work described in the section above. The model is designed
decoder module with a token-level CE loss. to work for various sequence prediction tasks with various
Furthermore, applying a supervised loss at the output of inputmodalitiessuchasspeechortext.ThedesignedLegoNN
eachmodulebringstwoextrabenefits:(a)Itextendsourability modules take into account reusing LegoNN decoder modules
to evaluate and diagnose the performance on multiple points across different tasks such as MT and ASR. For this purpose,
across the model. This can guide modeling decisions, e.g. wedesignedamodalityagnosticencoderwithanoutputlength
controller unit (§III-B1), which we call the LegoNN encoder
3Local normalization using the softmax operation at each input step
as shown in the left side of Fig. 2. The LegoNN decoder,
can suppress information such as confidence of a state, whereas hidden
representationsinmonolithicencoderspreserveit[16]. as shown on the right side of Fig. 2, is modified with an
5
added ingestor component (§III-B2) which would consume multi-head cross-attention, MHA(y,x,x), and position-wise
the distributions produced by the LegoNN encoder. We have feedforward, FFN(y). For each hPE ∈hPE ,
1:K
detailed the individual modules in the following sections.
h˜PE =LayerNorm(hPE), (16)
1) LegoNN Encoder: For an input X , which can either
1:T hPE =hPE+MHA(h˜PE,h˜PE ,h˜PE ), (17)
be filter banks for speech frames or embeddings for text 1:K 1:K
tokens, the LegoNN encoder consists of two sets of repeating hPE =hPE+MHA(LayerNorm(hPE),X ,X ), (18)
1:T 1:T
blocks,whichwecall(1)themodalityencoder(§III-B1a)and hPE =hPE+FFN(LayerNorm(hPE)). (19)
(2) the output length controller unit (§III-B1b).
TheseblocksarefollowedbyafinalLayerNorm(hPE),which
a) ModalityEncoder: Themodalityencoderisjustlikea
now returns a sequence of K representations, hPE , that
regular transformer encoder [3] that simply encodes the input 1:K
encodes the input, X . The length K can be either up-
context through repeating blocks of multi-head self-attention, 1:T
sampled or down-sampled depending on the input modality.
MHA(x,x,x),andposition-wisefeedforwardlayers,FFN(x)
Thesamplingfactorcanbesettoanythingsuchthatitreturns
[3]. For each X ∈X ,
1:T
atleastasingular(K =1)unitsequence.Inthispaper,weset
thesamplingrateasafixedfactortotheinputlengththattries
X˜ =LayerNorm(X), (12) tomatchMTandASRencoderoutputlengthbycomputingthe
mostaccommodatingratiousingtheinputandtargetsequence
X =X+MHA(X˜,X˜ ,X˜ ), (13)
1:T 1:T lengths of samples from the training data such that the ratio
X =X+FFN(LayerNorm(X)). (14) also does not violate the CTC criterion [7] (§V).
c) CTC Interface: Finally the encoder representations,
These blocks are followed by a final LayerNorm(X) which hPE istransformedintotheencodervocabularysizefollowed
1:K
returns an input context-aware representation of length T. In by a softmax operation, as shown in Eq. (6), to produce
principle,themodalityencoderlearnsasequenceofrepresen- a distribution (PVCTC) over the encoder vocabularies. These
1:K
tationsfromanyinputmodalityandcanalsocontainmodality- distributions are passed to the LegoNN decoder and to the
specific architectures such as conformers for speech [17], CTC loss computation.
vision-transformers for images [18] and pre-trained language
models for text [8]. As the lengths of the learned represen- 2) LegoNN Decoder with Ingestor of probability distri-
tations can be quite different for different modalities such as butions: The LegoNN decoder is the standard transformer
speechandtext,weintroducetheoutputlengthcontrollerunit. decoder [3] with an added Ingestor (ING) component to
b) Ouput Length Controller (OLC) unit: One of the consume input marginal distributions from preceding encoder
challenges of using modules across sequential tasks, where modules. We propose two ingestor architectures; one that is
inputs and outputs have different lengths, is adapting the differentiable, allowing for communicating gradients between
output length of an encoder module trained on one task to modules, called the Weighted Embedding Ingestor (WEmb)
match the expected input length of another. For example, and another discrete one that communicates a ranked list of
encoder modules from an ASR task encode inputs in more hypotheseswhilekeepingthemodulesgradient-isolated,called
time steps compared to an MT encoder. Naive up- or down- the Beam Convolution Ingestor (BeamConv). These ingestor
sampling approaches, e.g., pooling or replicating time-steps components can be added on top of any module that accepts
[19], cover only integer length ratios that are either aggres- a distribution, so in a multi-module system (Eq. 11) these can
sivelydown-samplingorunnecessarilyup-samplingoutputse- be part of a LegoNN encoder.
quencelengths.Tosolvethisproblem,weintroduceanOutput a) WeightedEmbeddingIngestor(WEmb): Theweighted
Length Controller (OLC) component in LegoNN encoders to embedding ingestor (WEmb) computes the expected embed-
enable working with fractional length ratios between inputs ding vector (h 1:K) of the encoder distributions (PV 1:C KTC) per
and outputs of the same module. output time-step. Since these embedding vectors are formed
OLC is a novel application of cross-attention [1], [3] out of local normalized conditionally independent CTC dis-
between two groups of transformer layers in a multi-layer tributions, we need to re-encode the positional information in
module. If the output of our modality encoder processes the these embeddings. We combine the expected embedding vec-
input X
1:T
to produce input representations of length T and tor (h 1:K) with sinusoidal positional embedding (PE) before
we want to convert it into K length. The OLC first initializes applying a few layers of self-attention transformer encoder
a sequence of K positional embeddings, hPE , blocks [3] to aggregate information across time-steps,
1:K
hPE =SinusoidalPE(1:K)+LearnablePE(1:K), (15) h 1:K =P 1V :C( Ki T− C1) ∗W Emb; h 1:K =h 1:K +PE(h 1:K), (20)
1:K
h =TransformerEncoder(h ), (21)
1:K 1:K
where LearnablePE and SinusoidalPE are learnable and
sinusoidal positional embeddings [3], [20]. where PV C(i T− C1) is the distribution over the vocabulary V(i−1)
1:K CTC
The hP 1E
:K
representations pass through another set of trans- of the previous module i−1, W
Emb
∈ R|V C(i T− C1)|×d and d is
former blocks that applies an additional cross-attention opera- the input dimension for Eq. (21) of the current module i.
tion [1], [3] over the modality encoder representations. The The first operation, to compute the expected embedding,
blocks consists of multi-head self-attention, MHA(y,y,y), is equivalent to a 1-D convolution operation with a receptive
6
Traditional Encoder-Decoder Training Framework LegoNN Modular Training Framework
Hidden Vector Representation Distribution over Interpretable Vocabulary
MT Task 1 MT Task 1 Phrase Translation Language Generation
De Encoder Decoder En De En
OLC ING
REUSE
MT Task 2 MT Task 2 Phrase Translation Language Generation
Ro Encoder Decoder En Ro En
OLC ING
ASR Task 1 ASR Task 1 Phoneme Pronunciation Language
Recognition Model Generation
Encoder Decoder
En En
OLC ING OLC ING
REUSE
ASR Task 2 ASR Task 2 Phoneme Pronunciation Language
Recognition Model Generation
Encoder Decoder En En
OLC ING OLC ING
Fig. 3. Benefits of LegoNN: Given a scenario where practitioners have a De-En MT system and want to build additional ASR and MT systems, with
theLegoNNframework,theyonlyneedtobuildnewencodersystemsandcandirectlyre-usedecodermodulesfromtheirinventory.Forexample,re-using
the De-En MT decoder for the Ro-En MT task and English ASR task. Additionally, when building an ASR system on a different domain they can re-use
components from both ASR and MT systems like the pronunciation module from the previous ASR system and the decoder module from the De-En MT
system.
field RF=1. When extended to larger receptive fields, WEmb this section. These tests are designed to show that LegoNNs
offers the opportunity to learn local confusion patterns of the don’t compromise on performance, are modular, and are
previous module: h=Conv1D(PV Ci− TC1 ); with RF ≥ 1. flexible across tasks. Their results are shown in §VI.
1:K
b) BeamConvolutionIngestor(BeamConv): Ratherthan
using the full output probability values
PV C(i T− C1)
, the beam A. Performance Tests
1:K
convolution ingestor (BeamConv) uses only the token indices
The benefits of modularity should not come at a cost
ofthetop-phypothesesforeachpositionk fromtheoutputof
to performance in individual tasks. To show that LegoNNs
the preceding module. This creates an information bottleneck
achieve competitive results, we compare their performance to
[21] in the model where gradients cannot be communicated,
the baseline monolithic encoder-decoder across benchmarks
top-p(PV C(i T− C1)
)= argmax
(cid:88) PV C(i T− C1)
(z =a), (22)
for speech recognition (ASR) and machine translation (MT).
k k
A⊂V(i−1),|A|=pa∈A
CTC
B. Modularity Tests
where
PV C(i T− C1)
(z = a) is probability of predicting the output
k To show that LegoNNs are indeed modular, we subject
token a at k-th position for module i−1. The top-p indices
LegoNNs and the baseline monolithic encoder-decoder to
areembeddedintoddimensionaltable,and,similartoWEmb
various stress tests that check the modularity of these models.
ingestor, we apply positional embedding and self-attention
Similar to [22], these tests are designed to check if a system
transformer encoder blocks. Further, we can also use a 1-D
is modular or not. We consider three basic tests:
convolution to aggregate local information,
1) Random-Seed Swap - Encoder or decoder modules
r
=Embedding(cid:16) top-p(PV Ci− TC1 )(cid:17)
, (23) trained from a different random seed should have the
1:K 1:K
same functionality and hence be swappable.
h =Conv1D(r ), (24)
1:K 1:K 2) ArchitectureSwap-Encoderordecodermodulestrained
h =h +PE(h ), (25)
1:K 1:K 1:K with different architectures (including changes in size
h =TransformerEncoder(h ), (26) and number of parameters) should have the same func-
1:K 1:K
tionality and hence be swappable.
where r∈RT×p×d when beam size=p and RF ≥ 1 for input
3) Modular Plug - Modules trained in isolation should
of T time steps.
be able to plug-in to modules from previously trained
LegoNN models that have the same interface. In this
IV. MODULARITYTESTSANDBENEFITSOFMODULARITY
paper, we limit our scope towards training encoders
In order to present the efficacy of LegoNNs, we subjected in isolation and leave training modular decoders in
the LegoNNs to a variety of experiments that we describe in isolation for future work.
7
C. Benefits of Modularity Model architecture: Input features are processed using two
2-Dconvolutionblockswith3×3kernels,64and128feature
In order to present the benefits of modularity, we consider
maps,respectively,2×2maxpoolingandReLUnon-linearity.
a practical scenario where practitioners need to build multiple
The baseline model uses transformers [3] with 16 encoder
ASR and MT tasks, as shown in Fig. 3. In a situation where
blocks and 6 decoder blocks, each with 1024 dimensions,
practitionershaveahighresourcedDe-Enmachinetranslation
16 heads, 4096 feed-forward units, and sinusoidal positional
model in their inventory, we show with LegoNNs they can
embeddings are added to the output of the convolutional
save compute resources and leverage the well trained De-
context layers [33]. For the LegoNN encoder-only model
En decoder by applying the Modular Plug to build systems
trained using the CTC loss, we use the same architecture
for other MT and ASR tasks. In particular, we consider three
as the encoder of the baseline model along with a length
scenarios:
control unit which reduces the length of the input by a
1) Romanian-English(Ro-En)MT -Inordertobuildama-
factor of 1.5 with a maximum allowable length of 230 time-
chinetranslationsystemforanunder-resourcedtasklike
steps. These positional embeddings are then passed through 6
Romanian-English MT, practitioners using the LegoNN
layers of transformer layers with cross-attention as described
framework only need to build the Ro-En LegoNN en-
in §III-B1. The LegoNN decoder uses the same architecture
coder and re-use the De-En decoder. Additionally, they
as the baseline decoder. All Ingestor components (§III-B2)
can also benefit the under-resourced Ro-En MT task, as
use RF=1, 3 layers of transformers with 1024 dimensions, 16
the De-En decoder is trained on a larger corpus.
heads and 4096 feed-forward layer. The BeamConv ingestor
2) English ASR - Practitioners can also use LegoNN de-
uses K=10, and embedding size= 100.
coders from an MT task for a different task on a
different input modality. The OLC ensures that the Training: We use an average batch-size=300 utterances,
expected encoder length for both MT and ASR task
weightdecay=1e−6,andlr=1e−3with35kwarm-upstepsthen
match, thereby making the De-En decoder re-usable. exponentiallydecayto5e−6 over44k steps.WefollowtheSS
3) Different Domain English ASR - We show that LegoNN policy of SpecAugment [34] without time-warping.
modularity points are not limited to between encoders
anddecoders.WhilebuildinganEnglishASRsystemon B. Machine translation task
a different domain, practitioners can re-use components Data: ForthePerformanceTests(IV-A),wetrainourmodels
from both the English ASR and De-En MT systems. on the standard 4.5M dataset from WMT En-De task, as used
All the composed LegoNN models are end-to-end differen- by[3],[35].Wefilterthetrainingdatatohavealengthratioof
tiable allowing further fine-tuning to improve performance if 1.5with250asmaxtokens.Wetestthemonthenewstest2011-
the practitioner has additional compute resources available. 2016setsexcludingthenewstest2013forthevalidationset.We
use the shared 32K BPE vocabulary [13] provided by [3]. We
V. EXPERIMENTALSETUP averageamovingwindowof10checkpointsandpicktheone
with the best validation BLEU score. We use a beam size of
All our encoder-decoder models use the transformer archi-
5 and a length penalty of 0.6 for decoding. The models are
tecture [3] implemented in the fairseq library [23] and run on
evaluated on case-sensitive tokenized BLEU with compound-
DGX-1nodeswith8NVIDIAV100GPUs.Forbothtasks,we
splitting using multi-bleu.pl [36]. For the Benefits of
apply LayerNorm [24] before every residual connection and a
Modularity experiments (IV-C), we train De-En and Ro-En
final one at the end of all transformer blocks. We use Adam
models on WMT19 and WMT16 datasets, respectively. The
optimizer [25] with eps = 1e−9,betas = (0.9,0.999), label
datapreparationforDe-EnMTmodelsbeingusedtocompose
smoothing=0.1, and a gradient clip norm = 5.0. The model
with Ro-En MT encoder are detailed in §V-C1 and one being
hyperparameters are detailed in Appendix C.
used to compose with ASR encoder is detailed in §V-C2.
Model architecture: The baseline model uses transformers
A. Speech recognition task
[3] with 12 encoder blocks and 6 decoder blocks, each with
Data: For our speech recognition experiments, we follow 1024 dimensions, 16 heads, 4096 feed-forward units, and
the standard 300 hours Switchboard (LDC97S62 [26]) setup, sinusoidal positional embeddings. All embedding tables are
and the Switchboard (SWB) and CallHome (CH) subsets of sharedacrossthemodel.Tocontrolfortheextraparametersin
HUB5 Eval2000 set (LDC2002S09 [27], LDC2002T43 [28]) the proposed LegoNN models, we added 6 additional encoder
for testing, which we use for the Performance Tests (IV-A). blocks that improved the baseline model. Other strategies for
WefollowthedatapreparationsetupprovidedinESPnet[29], using these parameters in the baseline model yielded inferior
where we use 100 and 2000 target SentencePiece [14] units performance.TheLegoNNencodermodeluses12transformer
trained on the 300h text. We follow the same recipe for blockswith1024dimensions,8heads,and2048feed-forward
processing the TED-LIUM [30] and Europarl [31] data, with units,withOLC,upsamplingtheinputlengthbyafactorof2,
phonemes generated using [32], detailed in §V-C2, which we appliedtothesecondhalfoftheencoder.TheLegoNNdecoder
use to present Benefits of Modularity (IV-C). We use the last usesthesamearchitectureasthebaselinedecoder.AllIngestor
model for inference with a beam size of 20 and a length components (§III-B2) use RF 1, 3 layers of transformers with
penaltyof1.0.WedonotuseanexternalLMorjointdecoding 1024 dimensions, 16 heads and 4096 feed-forward units. The
over the encoder and decoder outputs [29]. BeamConv ingestor uses K=200 and embedding size=300.
8
Input embedding tables are shared with encoder, ingestor, and experiments. We report the final WER on the lowercased set
decoder tables where applicable. to follow the standard ASR data setups.
Training: We use an average batch-size of 4000 sentences, Training: As the Europarl data contains only 70 hours, we
weight decay=0.1, and lr=1e−3 with 35k warm-up steps reducedthesizeofthebaselinemodeltoavoidoverfitting.We
followed by inverse square root decay for 45k steps. use transformers [3] with 12 encoder and 6 decoder blocks,
each with 512 dimensions, 8 heads, 2048 feed-forward units.
C. Experimental Setup for Benefits of Modularity For the LegoNN phoneme+pronunciation encoder-only
1) Ro-En MT Modular Plug setup: For our Ro-En transfer modeltrainedusingCTCloss,weusealengthcontrolunit(for
experiment, we used the WMT16 Ro-En and WMT19 De- phonemes) which reduces the length of the input by a factor
En datasets. For WMT16 Ro-En, we used the data prepared of 1.2 with a maximum allowable length of 365 time-steps. It
by [37] which is already tokenized and lowercased. For De- outputs a marginal distribution over the phonemes, which is
En, we excluded ParaCrawl from the standard WMT19 raw thenpassedtoanIngestorcomponent(WembRF=5)followed
training set to obtain 7.4M parallel translation pairs. We by6encoderlayersandanotheroutputlengthcontrolunit(for
applied the same processing as for the Ro-En dataset by BPE output tokens) reducing the speech input by a factor 3.5
lowercasing and tokenizing the De-En set. We prepared a with a maximum allowable length of 130 time-steps. For the
single joint dictionary of 41000 BPE units by combining the De-En MT model, we use the same architecture as §V-C1 but
training text from both datasets. We filter the training data to without sharing a dictionary between source and target.
have a length ratio of 1.5 and 80 as the max token length Fine-tuning: To finetune the composed LegoNN Europarl
[37]. Following [37], [38], we report tokenized BLEU scores ASR model we modify the peak learning rate to lr= 1e−4
on this dataset. and warm-up to a 1k steps. We run the training for 20k steps
Training: As the Ro-En dataset contains only 610K pairs, to get the best validation perplexity.
to achieve the best baseline performance, we reduce the size
of the baseline model to avoid overfitting [38]. We use 6
encoderanddecoderlayerswith512dimensions,8heads,and VI. RESULTS
2048 feed-forward units. We modify the Ro-En encoder-only
In this section we subject the LegoNN systems to the
LegoNN model accordingly by using transformer blocks with
Peformance and Modularity tests introduced in §IV-A and
512 dimensions, 16 heads, and 4096 feed-forward units. For
§IV-B.Wealsodemonstratethebenefitsofmodularitytowards
De-En LegoNN models along with the architecture described
reusing LegoNN modules between ASR and MT tasks, as
in §V-B we also experimented with larger models where the
presented in the four scenarios §IV-C and Fig. 3.
transformerencoderblockhas16headsand4096feed-forward
units. We found that the larger BeamConv model performs
better for this cross-lingual modular experiments.
A. Performance of LegoNN models
Fine-tuning: To finetune the composed LegoNN Ro-En
WMT model we modify the learning rate to lr= 5e−6 and First, we show the performance of LegoNN models on
warm-upstepsto15k.Werunthetrainingforaround4k steps their original tasks, without sharing any modules. Tables I
to obtain the best validation perplexity. and II show the performance of the models trained 4 with
2) English ASR Modular Plug setups: To demonstrate the the LegoNN procedure. For the WMT task, our best LegoNN
thirdLegoNNscenarioinFig.3,weusedtheEuroparlspeech model with the WEmb ingestor is only 0.8 BLEU behind 5
data to train our ASR task. We followed the data preparation our strong baseline encoder-decoder model (better than the
fortheASRmodelsdescribedin[31]bylowercasing,tokeniz- publiclyavailablereferencemodelby[35]6)whilebeingcom-
ing,andstrippingthepunctuationsfromthetext.Wefollowed posed of modular reusable pieces (as we show in §VI-B and
the same recipe when training models for the WMT19 De-En §VI-C).FortheASRtask,ourLegoNNmodelsreachthesame
dataset, using the raw text from §V-C1. To prepare the BPE level of performance as the baseline encoder-decoder model.
target units, we trained a BPE model with vocab size 2000 on The good ASR performance of the gradient-isolated case of
the Europarl train text and applied the prepared dictionary on the BeamConv ingestor shows that the ASR task is amenable
the other sets of Europarl speech and De-En WMT data. We to the decomposition of the linguistic unit recognition and
had an OOV rate of 0.096% in the De-En WMT train data. language generation components. 7
FordemonstratingthefourthLegoNNscenarioinFig.3,we
used the TED-LIUM dataset [30] for training the pronuncia-
4TableIandIIreportaveragescoresonthreerandomseeds
tion model module. We processed the transcripts in the same 5WebelievethisgapinLegoNNMTisduetotherelativelylowerquality
waybylowercasing,tokenizing,andstrippingthepunctuation of encoder-only MT models (compared to LegoNN encoder performance in
ASR).WiththerecentadvancesinCTC-basedMTencodermodels,likeusing
from the text data. We trained the 2000 BPE target units on
sequence level knowledge distillation [39], [40] this performance gap could
Europarl and TED-LIUM train text and applied the prepared befurtherreduced.
dictionarytotheothersetsofEuroparl,TEDandDe-EnWMT. 6We downloaded the public model by [35] to score the newstest2011-
WehadanOOVrateof0.090%intheDe-EnWMTtraindata. 2016testsetswhichweren’treportedintheiroriginalpaper.
7Using CTC models with language models is common for ASR [41],
We use the grapheme-to-phoneme library described in [32]
[42] and there can be some confusion regarding the relation of LegoNNs
to get the target phonemes for the speech data used in both withthem.Wediscussthisin§VII-D.
9
Reference Performance 100 % Reference Performance 100 % Reference Performance 100 %
100 100 99.3 100 104 101 96.397.8 100 100 99.399.6 96.4 94.7 99.7 98.2
ASR ASR ASR
MT MT MT
0.0 0.0 0.0 0.0 0.0 0.0
LegoNN LegoNN Traditional Dec: Wemb→ B.Conv→ Wemb↔ B.Conv↔ Traditional LegoNN LegoNN Traditional
WEmb BeamConv Enc-Dec Enc:B.Conv WEmb Wemb(2) B.Conv(2) Enc-Dec WEmb BeamConv Enc-Dec
Fig.4. WithreferenceperformanceofLegoNNmodelsofTablesIandII(normalizationto100%enablesustoplotASRandMTperformancesinthesame
graph), we show the interchangeability of LegoNN models compared to traditional Enc-Dec models under three conditions: swapping encoder and decoder
modulesoftwomodelstrainedwithdifferentrandomseeds(left),swappingmodulesbetweenLegoNNmodelswithdifferentarchitecturesandtrainedwith
differentingestortypes(middle),andmatchingarbitrarydecodermoduleswithLegoNNencodermodulestrainedin-isolation(right).
TABLEI allows us to plot ASR and MT systems in the same figure.
BLEUSCORES(↑)ONWMTFORLEGONNANDBASELINEENC-DECMT ForbothASRandMTtasks,withoutfine-tuning,performance
MODELS.
barelychangeswhenmodulesareswappedbetweentwodiffer-
ent models trained with different random seeds or completely
LossCriterion WMTEn→De different architectures, even though these two models have
MTTask
CTC CE dev(↑) test(↑) differentlearningdynamicsduetotheuseofdifferentingestor
ScalingNMT[35] ✗ ✓ 27.3 28.06 types (Fig. 4, left and middle). The right side of Fig. 4
BaselineModels(OurImplementation) presents the case where a LegoNN encoder module is trained
BaselineEnc-Dec ✗ ✓ 27.6 28.3 inisolationofanydecodermoduleusingaCTCloss,foreither
LegoNNModels ASR or MT, then matched with an arbitrary decoder module
EncoderOnly ✓ ✗ 19.1 18.5 at inference time. This is the most challenging condition,
Encoder+BeamConvDecoder ✓ ✓ 26.7 26.9 especially for the MT task. Relying entirely on the indices
Encoder+WEmbDecoder ✓ ✓ 27.2 27.5
of top-p hypotheses, rather than their floating-point marginal
probabilities, the BeamConv ingestor shows more robustness
when reused within the same task compared to the WEmb
TABLEII
%WER(↓)ONSWBD300H(NOLM)FORLEGONNANDBASELINE ingestor. The traditional encoder-decoder models, which are
ENC-DECASRMODELS. built without reusability in mind, fail completely under all
these conditions (shown on the right side of the three sub-
figures).
LossCriterion Eval2000
ASRTask
CTC CE SWB(↓) CH(↓)
LAS+SpecAugment[34] ✗ ✓ 7.3% 14.4% C. Reusability of LegoNN modules across tasks
IBMSWBD300h[43] ✗ ✓ 7.6% 14.6%
ESPNET[44] ✓ ✓ 9.0% 18.1% LegoNN modules can be reused across tasks with no
KaldiHybridsystem[45] LF-MMI 8.8% 18.1% fine-tuning and with almost no degradation in performance.
BaselineModels(OurImplementation) Table III shows an improvement in MT performance by
BaselineEnc-Dec ✗ ✓ 8.5% 18.0% 1.0 BLEU point when a Ro-En LegoNN encoder module
LegoNNModels is composed with a LegoNN decoder module that is trained
EncoderOnly ✓ ✗ 11.5% 24.1% on the De-En data (the second LegoNN scenario in Fig. 3).
Encoder+BeamConvDecoder ✓ ✓ 8.5% 18.2% TableIVtakesthisastepfurtherbycomposinganMTtrained
Encoder+WEmbDecoder ✓ ✓ 8.4% 18.2%
decoder (De-En data) with an ASR trained encoder on the
Europarlspeechdataset(thethirdLegoNNscenarioinFig.3).
The BeamConv ingestor depends on a fixed beam size p that
B. Modularity of LegoNN models cannotbechangedafterinitialtrainingofthedecodermodule.
This explains the inferior performance of the BeamConv as
Fig.4showsthatLegoNNismodular;thedecodermodules
comparedtotheWEmbingestor(whichusesthefullmarginal
can be reused with encoders from other models for the same
task 8(section §VI-C presents cross-task performance). The distribution) when reused in a new task. To demonstrate
the flexibility of building sequence-to-sequence models with
100% reference level refers to the respective LegoNN and
LegoNN,TableVshowsthefourthLegoNNscenarioinFig.3.
encoder-decoder models performance from Table I and II.
A pronunciation modeling module trained on the TED-LIUM
Normalizingscoreswithrespecttotheirreferenceperformance
dataset is used with a phoneme recognition module trained
on the Europarl dataset. Then, a decoder trained on the De-
8AllstatisticscomputedinFig.4reportaveragescoresacrossallpossible
combinationsofmixingthreerandomseedsfromeachmodule. En WMT task is added to the ASR model to bring the
10
TABLEIII ASR task, fine-tuning LegoNN models composed of two pre-
BLEUSCORES(↑)ONRO-ENMTTASKUSINGALEGONNMODEL trained modules achieved 12.5% WER reduction, compared
COMPOSEDOFADECODERFROMADE-ENMTMODELANDAN
to a 19.5% reduction for the three-module one. Fine-tuning
ENCODER-ONLYMODULETRAINEDONRO-ENDATA.
the three modules helped reduce the domain mismatch while
preserving the benefits of the TED-LIUM data.
MTTask Ro→EnBLEU(↑) GPUHours(↓)
BaselineRo-EnEnc-Dec 34.0 144 VII. ADDITIONALDISCUSSIONSAROUNDLEGONNS
LegoNNEncoderonly 30.7 1209 A. Interpretable Interface in LegoNN models
+DE-ENWMTLEGONNMODULES
BeamConvDecoder 33.0 010 Each LegoNN module has a task, a clear performance
WEmbDecoder 35.0 010 metric, and a defined interpretable interface that allows one
to assess the quality of individual modules. This gives a
sense of the contribution of each module towards the overall
TABLEIV
performance of the task and helps in better debugging of
%WER(↓)ONEUROPARLASRTASKUSINGALEGONNMODEL
COMPOSEDOFADECODERFROMADE-ENMTMODELANDAN the end-to-end systems. In Table VII, we show the loss and
ENCODER-ONLYMODULETRAINEDONEUROPARLASRDATA. performance of the encoder modules of the LegoNN models
that were presented in the table I and II. The performance of
the encoder module was calculated using greedy decoding of
ASRTask Europarl%WER(↓) GPUHours(↓)
the encoder marginal distributions without using an external
BaselineEuroparlEnc-Dec 18.4% 22
language model.
LegoNNEncoderonly 19.5% 139
+DE-ENWMTLEGONNMODULES
BeamConvDecoder 22.8% 010 B. ImportanceofencodergroundingusingCTCforreusability
WEmbDecoder 18.4% 010
Table VIII shows the results of training WEmb LegoNN
MT models without an OLC unit or CTC loss at the encoder
output. The initial system works fine but its components fail
final WER, with no fine-tuning updates, just 0.6% from the completely when used with another independently trained
baseline encoder-decoder model. The TED-LIUM dataset is module.
used in this experiment because it is closer in speaking style
to Europarl. Both the public TED talks and the parliament
C. Re-using LegoNN modules in low-resource conditions
speeches exhibit similarities in speaking style and are not
Table IX shows the improvements observed when using
spontaneous like the Switchboard data. However, there is a
LegoNN decoder modules with encoders trained with 10%
clear mismatch in the domain which is apparent in the 26.7%
and 30% of the switchboard ASR training data. Applying
WER of the initial TED-LIUM system when evaluated on
a decoder trained on the full data provided more than 50%
the Europarl test set. Utilizing LegoNN to develop models
and 10% relative improvement on average compared to the
for new tasks benefits from the data-efficiency of encoder-
baselineencoder-decodermodels,and30%and25%compared
only modules (Additional experiments regarding the data-
to a LegoNN encoder module when trained on the 10% and
efficiency of encoder-only models are shown in §VII-C) and
30% low resource conditions, respectively. Given that this
overall shorter development time, e.g., 13 vs 22 GPU-hours
experiment shows module transfer between models trained
on Europarl ASR and 120 vs 144 GPU-hours on Ro-En
on the same dataset, encoders are trained without the output
WMT. 9 10
length control unit because their output length distribution
matches that of the decoder input.
D. Fine-tuning of LegoNN models
D. Incorporating Text-only Resources in LegoNNs
So far, we showed matching or better results for LegoNN
models composed of pre-trained modules with no fine-tuning. The authors of [41], [42] have shown that ASR encoders
Given that LegoNN decoders with the WEmb ingestor pre- trained with CTC loss can use language models trained with
serve full differentiability, such LegoNN models composed text-only resources to decode CTC distributions into words.
of pre-trained modules can be fine-tuned toward the target These models are modular, but they are not trained end-to-
task. Table VI shows that fine-tuning the De-En decoder with end and cannot be conditioned on the error patterns of the
the Ro-En encoder achieves another 0.5 BLEU point, leading encoder.
to an improvement of 1.5 points over the baseline model. For comparison with the LegoNN encoder-decoder model,
Although the non-fine-tuned two-module LegoNN model is we decoded the CTC distributions of LegoNN encoders pre-
better than the three-module one for the Europarl English sented in Table II with a language model trained on the same
data. The LegoNN encoder with WFST decoding using a
9GPU hours for encoder training can be improved further by using language model [41] achieves 9.1% and 19.4% WER on the
CuDNNbasedCTCimplementation[46] SWB and CH test sets averaged across 3 seeds, which is
10Composing LegoNN decoders with the LegoNN encoders, is a simple
inferiortothe8.4%and18.2%WERachievedbytheLegoNN
plug and play and does not require any fine-tuning steps. So no additional
GPUhoursareusedforthedecoder. encoder-decoder model (Table II). We expect this gap to be
11
TABLEV
%WERONTHEEUROPARLTESTWITHASRENCODERDECOMPOSEDINTOTWOMODULES.MODULESTRAINEDONTHETED-LIUMDATASETARE
COMBINEDWITHTHOSETRAINEDONEUROPARLANDWMTTOBRINGTHEOVERALLWEROFTHELEGONNASRSYSTEMJUST0.6%FROMTHE
BASELINEWITHNOFINE-TUNING.
ASRTask Europarl%WER(↓)
BaselineEuroparlEnc-Dec 18.4%
TED.PhonemeRecognizer+TED.PronunciationModel 26.7%
EuroparlPhonemeRecognizer+TED.PronunciationModel 20.5%
EuroparlPhonemeRecognizer+TED.PronunciationModel+De-EnWMTDecoder 19.0%
TABLEVI
BLEU(↑)ONRO-ENWMTAND%WER(↓)ONEUROPARLASRTASKFORTHELEGONNMODELSBEFOREANDAFTEREND-TO-ENDFINE-TUNINGOF
THEMODELCOMPOSEDOFMODULESFROMDIFFERENTTASKSINTABLEIII,TABLEIVANDTABLEV.
ComposedLegoNNModel Nofine-tuning Withfine-tuning Metric
TableIII:Ro-EnWMTEncoder+De-EnWMTDecoder 35.0 35.5 BLEU(↑)
TableIV:EuroparlASREncoder+De-EnWMTDecoder 18.4 16.1 %WER(↓)
TableV:EuroparlPhonemeRecognizer+TED.Pronun.Model+De-EnWMTDecoder 19.0 14.8 %WER(↓)
TABLEVII TABLEIX
BLEUSCORES(↑)AND%WER(↓)ATTHEOUTPUTOFENCODER EFFECTOF%WER(↓)OFTRANSFERRINGAFULLYTRAINEDASR
MODULESOFTHELEGONNSYSTEMSPRESENTEDINTABLEIANDII. LEGONNDECODERMODULEONENCODER-ONLYMODULETRAINEDON
DIFFERENTAMOUNTSOFDATA.
ASRTask CTC Enc.WER(↓)
Loss SWB CH 10%Data 30%Data
ASRTask SWB(↓) CH(↓) SWB(↓) CH(↓)
WEmbLegoNN 0.60 11.6% 24.2%
BeamConvLegoNN 0.65 11.4% 24.4% BaselineEnc-Dec 129.7% 136.6% 20.4% 37.1%
MTTask CTC Enc.BLEU(↑) LegoNNEncoderModule 70.5% 80.5% 25.4% 43.8%
Loss dev test +SWBD300HLEGONNMODULES
BeamConvDecoder 40.7% 59.6% 17.5% 34.5%
WEmbLegoNN 2.51 18.9 18.4
WEmbDecoder 42.2% 63.4% 18.5% 34.7%
BeamConvLegoNN 2.56 18.2 17.8
TABLEVIII decoder model with multiple encoders processing different
IMPORTANCEOFCTCENCODERGROUNDINGFORMODULARITY
modalitiescantrainadecodermodulewithdatafrommultiple
tasks spanning various modalities.
En→De(↑)
LegoNNModels newstest14
WEmbIngestor 29.2 E. Insights towards training LegoNN Models
w/Enc.Swap 28.7
In this section, we discuss insights that can help better in-
NoCTCLossatencoderoutput 29.2
w/Enc.Swap 0.0 formdesignersontheirchoicesforbuildingLegoNNsystems.
a) Choice of interface distributions: Enforcing modular-
ity is not limited to the use of distributions trained using the
greater when comparing the MT translation models, where CTCloss.Anysystemthatoutputsdistributionsoverachosen
the performance gap between LegoNN encoders and LegoNN categorical space can be chained with another component
encoder-decoder models is larger (Table I). that accepts those distributions. As mentioned in §III-A, we
Additionally, we can still utilize external language models recommend using distributions that do not suffer from output
whiledecodingLegoNNencoder-decodermodelsviashallow- label bias [15], as this can lead to error propagation into
fusion during beam-search [29] and other techniques such as subsequent modules. We also believe that selecting interface
back-translation [47]. For future directions, the ability to train probability distributions that only condition on input is better
decoder-only LegoNN modules can allow training decoder for a clear division of functionality between encoders and
modules on text-only data. For example, simulating CTC-like decoders and allows the ingestors in the subsequent modules
distributionswithtext-onlydatacanallowdecodermodulesto torecoverfromtheinputerrorsmadebythepreviousmodule.
accept those distributions and train on text-only data. b) Choice of interface vocabulary size: In principle,
The LegoNN approach also offers a novel way to use the choice of interface vocabulary size should not affect
additional data from various different tasks and languages the modularity of LegoNN systems, however, in practice,
(Table III and Table IV). For example, a LegoNN encoder- this can be an important consideration for various reasons
12
outside of modularity. Following the discussions in §III-A3, f) Choice of language pair in LegoNN MT: In the
the performance on public benchmarks is dependent heavily LegoNN setup discussed in §IV-C, we require the target
on the evaluation dataset, for example, the ASR toolkit [29] language of the re-usable modules to be the same. There are
uses sub-word units of size 2000 for Switchboard and 16000 noconstraintsonthesourcelanguageandwedonotexpectthe
for Librispeech benchmark datasets. Similarly, while a larger full LegoNN encoder-decoder performance to be affected by
vocabularysizeleadstoshortersequencesandmakessequence this compared to traditional monolithic models. Recent works
modeling easier by making the predictions more context have shown CTC-based modeling to work well for various
dependent,thiscontextualdependencycanhurttheconditional different source languages [53] and we would like to extend
independenceandmakeCTCpredictionsdomainspecific.Due this work with LegoNNs to build multilingual any to English
to this trade-off, in our experiments, we found an interface translationsystemswithre-usableEnglishLegoNNdecoders.
vocabulary size of around 4k to work the best across ASR
and MT tasks. VIII. RELATEDWORK
Wewouldalsoliketonotethatasthemodelsforspeechand
This work is related to the large body of work on prob-
language processing are getting larger, the size of the vocab-
abilistic modeling for ASR and MT [54]–[56] where pre-
ulary is converging between the two modalities. For example,
dictors produce normalized probabilities that can be easily
the Whisper speech model [48] uses the same vocabulary as
combined. However, these probabilistic models only combine
the GPT-2 language model [49]. Additionally, the vocabulary
output scores while individually optimizing modules produc-
choice is no longer dataset-specific but rather trained on large
ing discrete sequences as opposed to chaining modules while
amounts of text to help with generalizability in the wild [49],
preserving their full differentiability as in LegoNN models.
[50]. We believe that such advances in the field will reduce
Hierarchical mixture of experts [57] and graph transformer
the burden on practitioners regarding the choice of interface
networks[16]motivated thiswork,however,the firstdoesnot
vocabulary size.
ground intermediate representations and the second commu-
c) Choice of interface vocabulary units: The units for
nicates them in the form of directed graphs, which can be
the vocabulary depend entirely on the designer, depending on
computationally expensive.
wherethemodulesarebeingre-used,forexample,ifamodule
TheproposedLegoNNprocedurebuildsonseveralresearch
is being re-used only for speech tasks, then using phonemes
efforts for sequence-to-sequence learning. Encoder-decoder
as interface vocabulary is okay, but the designer might prefer
models for machine translation [1]–[3], [9] and speech recog-
sub-word units if it requires being used for tasks that require
nition [4], [5] form the basis of this work. Although CTC
longer contexts like machine translation.
loss [7] was first applied to speech recognition [58]–[61],
Thechoiceofinterfacevocabularycanalsodeterminewhat
more recently it was also shown to be effective for machine
input information will be preserved or suppressed. For exam-
translation [19], [39]. This encourages us to utilize the CTC
ple, choosing discrete orthographic units for speech signals
loss for enforcing the intermediate vocabulary in LegoNN.
can capture content, but suppress other acoustic information,
Othernon-autoregressivesequencetosequencemappingmeth-
such as speaker characteristics, prosody, and emotions. Latent
ods[38],[62]–[64]arepotentialalternatives.TheCTClosshas
discrete units learned by self-supervised learning models of
beencombinedwiththecross-entropylossinencoder-decoder
speech, on the other hand, have been shown to capture
speechrecognitionsystemstoencouragemonotonicalignment
this acoustic information along with the content [51], [52].
betweeninputandoutputsequences[44],[65].Differentfrom
However, these units will not be able to interface with text-
LegoNN,theirdecoderattendsovertheencoderhiddenoutput
only systems, highlighting the importance of the designer’s
representations, maintaining their tight coupling.
choice in deciding the reusable components.
There have been many proposals in the fields of vision,
d) Choice of up-/down-sampling rate in OLC: Output
roboticsandreasoningforinducingamodularstructureonthe
Length Controller (OLC) is an important component to be
space of learned concepts either through hierarchically gating
considered when modules are being used in dramatically
informationfloworviahigh-levelconceptblueprints[66]–[68]
different sequence tasks, for example, ASR and MT. When
to enable zero- and few-shot transfer learning [69]–[72].
used in a single domain, that is if the expected input lengths
ofthedecoderdonotvarydrasticallythenOLCisnotrequired.
OLCensuresthattheCTCdistributionsarebeinglearnedover
IX. CONCLUSION
similar input-to-output alignment lengths for different tasks. We presented the LegoNN procedure for constructing
e) ChoiceofWembandBeamConvLegoNNdecoder: As encoder-decoder models that are composed of reusable mod-
discussed in §VI-C, we believe that Wemb is a more robust ules. LegoNN models perform competitively to the best
choice when using LegoNN models in different domains encoder-decoder ASR and MT models on large-scale bench-
and tasks, as it functions over the full marginal distribution marks. A key to reusable modules is a pre-defined vocabulary
to compute an expected embedding vector, while the top-p that is shared between many tasks across which modules
confusions in BeamConv can differ drastically across tasks can be reused. Without any fine-tuning steps, a LegoNN
and domains. The BeamConv, on the other hand, offers an in- decoder trained for the De-En WMT task can replace an
teresting approach towards ingesting top-p confusions, which ASR decoder module without any impact on performance
makes them useful for error analysis and provides capabilities and provide better generation quality for a Ro-En WMT task.
to inject external knowledge into the confusion lattice. When fine-tuned for a few thousand steps, LegoNN models
13
TABLEX
INDIVIDUALYEARTOKENIZED(TOK.)ANDDETOKENIZED(DETOK.)BLEUSCORES(↑)ONWMTFORLEGONNANDBASELINEENC-DECMTMODELS.
WMTEn→De(↑)
MTTask
newstest11 newstest12 newstest13 newstest14 newstest15 newstest16
tok. detok. tok. detok. tok. detok. tok. detok. tok. detok. tok. detok.
ScalingNMT[35]11 22.7 22.3 23.1 23.0 27.3 26.8 29.8 29.2 32.2 31.8 35.0 34.8
BaselineModels(OurImplementation)
BaselineEnc-Dec 23.3 22.8 23.3 23.1 27.6 27.2 29.8 29.0 31.9 31.5 35.6 35.1
LegoNNModels
EncoderOnly 16.0 15.8 15.7 15.7 19.1 19.0 18.0 17.8 20.3 20.2 21.6 21.7
Encoder+BeamConvDecoder 22.4 21.9 22.4 22.3 26.7 26.3 27.3 26.6 29.8 29.5 33.4 33.0
Encoder+WEmbDecoder 22.7 22.3 22.8 22.6 27.2 26.8 28.3 27.6 30.3 30.0 34.3 33.9
composed from multiple tasks and domains improve the Ro- TABLEXI
En WMT baseline model by 1.5 BLEU points and provide up SACREBLEU(↑)ONWMTFORLEGONNANDBASELINEENC-DECMT
MODELS.
to 19.5% WER reductions on the Europarl English ASR task.
Ourfuturedirectionswillfocusoncombiningtheflexibilityof
LegoNN models with the impressive performance of encoder LossCriterion WMTEn→De
MTTask
pre-training methods like BERT, and investigating zero-shot CTC CE dev(↑) test(↑)
learning scenarios for speech translation which relies on a ScalingNMT[35] ✗ ✓ 26.8 28.211
combination of ASR and MT modules. BaselineModels(OurImplementation)
BaselineEnc-Dec ✗ ✓ 27.2 28.3
LegoNNModels
BROADERIMPACT
EncoderOnly ✓ ✗ 19.0 18.2
The software industry made great strides in building inde- Encoder+BeamConvDecoder ✓ ✓ 26.3 26.7
Encoder+WEmbDecoder ✓ ✓ 26.8 27.3
pendent, reusable libraries that can be developed once and
reused across a wide range of applications. This paper takes
onesteptowardsbringingsequence-to-sequenceneuralmodels
APPENDIXC
closer to computer programs with reusable components. This
TRAININGHYPERPARAMETERSFORASRANDMTTASK
lineofresearchcanprovideawayfortheresearchcommunity
and industry to build more complex neural systems while Table XII and Table XIII contain the training parameter
preventing an explosion in computational training costs. details for the LegoNN model.
TABLEXII
APPENDIXA HYPERPARAMETERSFORTRAININGLEGONNMODELASRTASK.
DETOKENIZEDBLEUPERFORMANCEOFOURLEGONN
MTMODELS Hyperparameter Value
HiddenDropout 0.15
Table XI shows the detokenized BLEU performance of the Attentiondropout 0.15
MT models used in Table I. 11 We used the detokenizer from Activationdropout 0.15
mosesdecoder 12 to detokenize the model output and Sacre- Ingestorattentiondropout 0.15
Batchsize 300utt.
BLEU [73] to calculate the BLEU score with the following LRschedule tristage[34]
hash - BLEU+case.mixed+lang.en-de+numrefs.1+ Startlearningrate 1e−6
smooth.exp+test.{wmt11|wmt12|wmt13|wmt14/ Maxlearningrate 1e−3
Endlearningrate 5e−6
full|wmt15|wmt16}+tok.13a+version.1.2.9
Numberofsteps 80K
Warmupsteps 35K
Holdsteps 1K
APPENDIXB Adameps 1e−9
BLEUPERFORMANCEONINDIVIDUALYEARSFOROUR Adambetas (0.9,0.999)
Weightdecay 1e−6
LEGONNMTMODELS
Table X shows the BLEU performance (tokenized and
detokenizedBLEU)fromindividualyears(newstest11-16)for ACKNOWLEDGMENT
the MT models used in Table I. The authors would like to thank Siddhant Arora, Brian
Yan, Maria Ryskina, Shruti Rijhwani and Dan Berrebbi for
11Wedownloadedthepublicmodelof[35]toscorethenewstest2011-2016
their valuable feedback. We would also like to acknowledge
testsetswhichweren’treportedintheiroriginalpaper.
and remember the late Sujeath Pareddy, who suggested the
12https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
tokenizer/detokenizer.perl connection with Legos for our proposed framework.
14
TABLEXIII the11thIEEEConferenceonComputerVisionandPatternRecognition
HYPERPARAMETERSFORTRAININGLEGONNMODELMTTASK. (CVPR),1997.
[17] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,
Hyperparameter Value S.Wang,Z.Zhang,Y.Wuetal.,“Conformer:Convolution-augmented
transformerforspeechrecognition,”Proc.Interspeech2020,pp.5036–
HiddenDropout 0.3 5040,2020.
Attentiondropout 0.3 [18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
Activationdropout 0.3 T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gellyetal.,
Ingestorattentiondropout 0.1 “Animageisworth16x16words:Transformersforimagerecognitionat
Batchsize 4Ksent. scale,”inInternationalConferenceonLearningRepresentations,2020.
LRschedule inv.sqrt.[35] [19] J. Libovicky´ and J. Helcl, “End-to-end non-autoregressive neural ma-
Startlearningrate 1e−6 chinetranslationwithconnectionisttemporalclassification,”inProceed-
Maxlearningrate 1e−3 ingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage
Numberofsteps 150K Processing(EMNLP),2018.
Warmupsteps 35K [20] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin,
Adameps 1e−9 “Convolutional sequence to sequence learning,” in Proceedings of the
Adambetas (0.9,0.999) 34thInternationalConferenceonMachineLearning(ICML),2017.
Weightdecay 0.1 [21] N. Tishby, F. C. Pereira, and W. Bialek, “The information bottleneck
method,”inProceedingsof37thAnnualAllertonConferenceonCom-
munication,ControlandComputing(Alletron),1999.
[22] A. Naik, A. Ravichander, N. Sadeh, C. Rose, and G. Neubig, “Stress
REFERENCES test evaluation for natural language inference,” in Proceedings of the
2018 Conference of the North American Chapter of the Association
forComputationalLinguistics:HumanLanguageTechnologies(NAACL-
[1] D. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation
by Jointly Learning to Align and Translate,” in Proceedings of the HLT),2018,pp.2340–2353.
InternationalConferenceonLearningRepresentations(ICLR),2015. [23] M.Ott,S.Edunov,A.Baevski,A.Fan,S.Gross,N.Ng,D.Grangier,
andM.Auli,“fairseq:Afast,extensibletoolkitforsequencemodeling,”
[2] I.Sutskever,O.Vinyals,andQ.V.Le,“SequencetoSequenceLearning
inProceedingsofthe2019ConferenceoftheNorthAmericanChapter
withNeuralNetworks,”inProceedingsofthe28thAnnualConference
oftheAssociationforComputationalLinguistics(EMNLP):Demonstra-
onNeuralInformationProcessingSystems(NeurIPS),2014.
tions,2019.
[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
[24] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in
preprintarXiv:1607.06450,2016.
Proceedings of the 31st Annual Conference on Neural Information
[25] D. P. Kingma and J. L. Ba, “Adam: A Method for Stochastic Opti-
ProcessingSystems(NeurIPS),2017.
mization,”inProceedingsoftheInternationalConferenceonLearning
[4] W.Chan,N.Jaitly,Q.Le,andO.Vinyals,“Listen,AttendandSpell:A
Representations(ICLR),2014.
neuralnetworkforlargevocabularyconversationalspeechrecognition,”
[26] J. Godfrey and E. Holliman, “Switchboard-1 Release 2 LDC97S62,”
in Proceedings of the International Conference on Acoustics, Speech,
WebDownload.Philadelphia:LinguisticDataConsortium,1993.
andSignalProcessing(ICASSP),2016.
[27] L. D. Consortium, “2000 HUB5 English Evaluation Speech
[5] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,
LDC2002S09,” Web Download. Philadelphia: Linguistic Data
“End-to-EndAttention-basedLargeVocabularySpeechRecognition,”in
Consortium,2002.
ProceedingsoftheInternationalConferenceonAcoustics,Speech,and
[28] ——,“2000HUB5EnglishEvaluationTranscriptsLDC2002T43,”Web
SignalProcessing(ICASSP),2016.
Download.Philadelphia:LinguisticDataConsortium,2002.
[6] C.Y.BaldwinandK.B.Clark,DesignRules:ThePowerofModularity
[29] S.Watanabe,T.Hori,S.Karita,T.Hayashi,J.Nishitoba,Y.Unnoetal.,
Volume1. MITPress,1999.
“ESPnet:End-to-EndSpeechProcessingToolkit,”inProceedingsofthe
[7] A.Graves,S.Ferna´ndez,F.Gomez,andJ.Schmidhuber,“Connectionist
19th Annual Conference of the International Speech Communication
Temporal Classification: Labelling Unsegmented Sequence Data with
Association(InterSpeech),2018.
Recurrent Neural Networks,” in Proceedings of the 23rd International
[30] A. Rousseau, P. Dele´glise, and Y. Este`ve, “Enhancing the ted-lium
ConferenceonMachineLearning(ICML),2006.
corpuswithselecteddataforlanguagemodelingandmoretedtalks,”in
[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
Proceedingsofthe9thInternationalConferenceonLanguageResources
trainingofdeepbidirectionaltransformersforlanguageunderstanding,”
andEvaluation(LREC),2014.
inProceedingsofthe2019ConferenceoftheNorthAmericanChapter
[31] J. Iranzo-Sa´nchez, J. A. Silvestre-Cerda`, J. Jorge, N. Rosello´,
of the Association for Computational Linguistics: Human Language
A. Gime´nez, A. Sanchis, J. Civera, and A. Juan, “Europarl-st: A
Technologies(NAACL-HLT),2019.
multilingualcorpusforspeechtranslationofparliamentarydebates,”in
[9] N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation ProceedingsoftheInternationalConferenceonAcoustics,Speech,and
models,”inProceedingsofthe2013ConferenceonEmpiricalMethods SignalProcessing(ICASSP),2020.
inNaturalLanguageProcessing(EMNLP),2013. [32] K.ParkandJ.Kim,“g2pe,”https://github.com/Kyubyong/g2p,2019.
[10] M. Abadi, A. Agarwal, P. Barham, E. Brevdo et al., “TensorFlow: [33] A. Mohamed, D. Okhonko, and L. Zettlemoyer, “Transformers with
Large-scale machine learning on heterogeneous systems,” 2015, convolutional context for ASR,” in arXiv preprint arXiv:1904.11660,
software available from tensorflow.org. [Online]. Available: https: 2019.
//www.tensorflow.org/ [34] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,
[11] J. Bradbury, R. Frostig, P. Hawkins et al., “JAX: composable and Q. V. Le, “SpecAugment: A simple data augmentation method
transformations of Python+NumPy programs,” 2018. [Online]. for automatic speech recognition,” in Proceedings of the 20th Annual
Available:http://github.com/google/jax Conference of the International Speech Communication Association
[12] A. Paszke, S. Gross, F. Massa et al., “Pytorch: An imperative style, (InterSpeech),2019.
high-performancedeeplearninglibrary,”Advancesinneuralinformation [35] M.Ott,S.Edunov,D.Grangier,andM.Auli,“Scalingneuralmachine
processingsystems,vol.32,2019. translation,”inProceedingsofthe3rdConferenceonMachineTransla-
[13] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation tion(WMT),2018.
of rare words with subword units,” in Proceedings of the 54th Annual [36] Moses-SMT, “multi-bleu.perl,” https://github.com/moses-smt/
MeetingoftheAssociationforComputationalLinguistics(ACL),2016. mosesdecoder/blob/master/scripts/generic/multi-bleu.perl,2018.
[14] T. Kudo and J. Richardson, “Sentencepiece: A simple and language [37] J. Lee, E. Mansimov, and K. Cho, “Deterministic non-autoregressive
independentsubwordtokenizeranddetokenizerforneuraltextprocess- neuralsequencemodelingbyiterativerefinement,”inProceedingsofthe
ing,” in Proceedings of the 2018 Conference on Empirical Methods in 2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
NaturalLanguageProcessing(EMNLP):SystemDemonstrations,2018. (EMNLP),2018.
[15] A.Hannun,“TheLabelBiasProblem,”https://awni.github.io/label-bias, [38] M.Ghazvininejad,V.Karpukhin,L.Zettlemoyer,andO.Levy,“Aligned
2019. cross entropy for non-autoregressive machine translation,” in Proceed-
[16] L. Bottou, Y. Bengio, and Y. Le Cun, “Global training of document ingsofthe37thInternationalConferenceonMachineLearning(ICML),
processingsystemsusinggraphtransformernetworks,”inProceedingsof 2020.
15
[39] C.Saharia,W.Chan,S.Saxena,andM.Norouzi,“Non-autoregressive [62] J. Gu, J. Bradbury, C. Xiong, V. O. K. Li, and R. Socher, “Non-
machine translation with latent alignments,” arXiv preprint autoregressive neural machine translation,” in Proceedings of the In-
arXiv:2004.07437,2020. ternationalConferenceonLearningRepresentations(ICLR),2018.
[40] J. Gu and X. Kong, “Fully non-autoregressive neural machine transla- [63] M.Ghazvininejad,O.Levy,Y.Liu,andL.Zettlemoyer,“Mask-predict:
tion:Tricksofthetrade,”inFindingsoftheAssociationforComputa- Parallel decoding of conditional masked language models,” in Pro-
tionalLinguistics:ACL-IJCNLP2021,2021,pp.120–133. ceedings of the 2019 Conference on Empirical Methods in Natural
[41] Y. Miao, M. Gowayyed, and F. Metze, “EESEN: End-to-end speech Language Processing and the 9th International Joint Conference on
recognition using deep RNN models and WFST-based decoding,” in NaturalLanguageProcessing(EMNLP-IJCNLP),2019.
Proceedings of the 2015 IEEE Automatic Speech Recognition and [64] N. Chen, S. Watanabe, J. Villalba, and N. Dehak, “Non-
UnderstandingWorkshop(ASRU),2015. autoregressive transformer automatic speech recognition,” arXiv
[42] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, preprintarXiv:1911.04908,2019.
C. Case, J. Casper, B. Catanzaro, Q. Cheng et al., “Deep speech 2: [65] S.Kim,T.Hori,andS.Watanabe,“JointCTC-AttentionbasedEnd-to-
End-to-endspeechrecognitioninenglishandmandarin,”inInternational EndSpeechRecognitionusingMulti-taskLearning,”inProceedingsof
conferenceonmachinelearning. PMLR,2016,pp.173–182. theInternationalConferenceonAcoustics,Speech,andSignalProcess-
ing(ICASSP),2017.
[43] Z. Tu¨ske, G. Saon, K. Audhkhasi, and B. Kingsbury, “Single headed
[66] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Neural Module
attention based sequence-to-sequence model for state-of-the-art results
Networks,” in Proceedings of the 29th IEEE Conference on Computer
onswitchboard-300,”arXivpreprintarXiv:2001.07263,2020.
VisionandPatternRecognition(CVPR),2016.
[44] S. Karita, N. Chen, T. Hayashi, and other, “A Comparative Study on
[67] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, “Learning
Transformer vs RNN in Speech Applications,” in Proceedings of the
modularneuralnetworkpoliciesformulti-taskandmulti-robottransfer,”
2019IEEEAutomaticSpeechRecognitionandUnderstandingWorkshop
in 2017 IEEE International Conference on Robotics and Automation
(ASRU),2019.
(ICRA),2017.
[45] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar, X. Na,
[68] S.Purushwalkam,M.Nickel,A.Gupta,andM.Ranzato,“Task-driven
Y.Wang,andS.Khudanpur,“PurelySequence-TrainedNeuralNetworks
modularnetworksforzero-shotcompositionallearning,”inProceedings
forASRBasedonLattice-FreeMMI,”inProceedingsofthe17thAnnual
oftheIEEEInternationalConferenceonComputerVision(ICCV),2019.
Conference of the International Speech Communication Association
[69] J.Andreas,D.Klein,andS.Levine,“ModularmultitaskReinforcement
(InterSpeech),2016.
LearningwithPolicySketches,”inProceedingsofthe34thInternational
[46] nvidia,“cuDNNCTCloss,”https://docs.nvidia.com/deeplearning/cudnn/
ConferenceonMachineLearning(ICML),2017.
api/index.html#cudnnCTCLoss v8,2022.
[70] R.Socher,M.Ganjoo,C.D.Manning,andA.Ng,“Zero-ShotLearning
[47] R. Sennrich, B. Haddow, and A. Birch, “Improving neural machine Through Cross-Modal Transfer,” in Proceedings of the 27th Annual
translation models with monolingual data,” in Proceedings of the 54th ConferenceonNeuralInformationProcessingSystems(NeurIPS),2013.
AnnualMeetingoftheAssociationforComputationalLinguistics(ACL), [71] N.Gupta,K.Lin,D.Roth,S.Singh,andM.Gardner,“Neuralmodule
2016,pp.86–96. networks for reasoning over text,” in Proceedings of the International
[48] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and ConferenceonLearningRepresentations(ICLR),2020.
I. Sutskever, “Robust speech recognition via large-scale weak super- [72] D. Pathak, C. Lu, T. Darrell, P. Isola, and A. A. Efros, “Learning
vision,”arXivpreprintarXiv:2212.04356,2022. to control self-assembling morphologies: a study of generalization via
[49] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskeveretal., modularity,” in Proceedings of the 33rd Annual Conference on Neural
“Language models are unsupervised multitask learners,” OpenAI blog, InformationProcessingSystems(NeurIPS),2019.
vol.1,no.8,p.9,2019. [73] M.Post,“Acallforclarityinreportingbleuscores,”inProceedingsof
[50] A.Chowdhery,S.Narang,J.Devlin,M.Bosma,G.Mishra,A.Roberts, the3rdConferenceonMachineTranslation(WMT),2018.
P.Barham,H.W.Chung,C.Sutton,S.Gehrmannetal.,“Palm:Scaling
language modeling with pathways,” arXiv preprint arXiv:2204.02311,
2022.
[51] T.HayashiandS.Watanabe,“Discretalk:Text-to-speechasamachine
translationproblem,”arXivpreprintarXiv:2005.05525,2020.
[52] Z.Borsos,R.Marinier,D.Vincent,E.Kharitonov,O.Pietquin,M.Shar-
ifi,O.Teboul,D.Grangier,M.Tagliasacchi,andN.Zeghidour,“Audi-
olm:alanguagemodelingapproachtoaudiogeneration,”arXivpreprint
arXiv:2209.03143,2022.
[53] B.Yan,S.Dalmia,Y.Higuchi,G.Neubig,F.Metze,A.W.Black,and
S. Watanabe, “Ctc alignments improve autoregressive translation,” in
EACL,2023.
[54] F.Jelinek,Statisticalmethodsforspeechrecognition. MITpress,1997.
[55] M. Mohri, F. Pereira, and M. Riley, “Weighted finite-state transducers
in speech recognition,” Computer Speech & Language, vol. 16, no. 1,
pp.69–88,2002.
[56] P.F.Brown,J.Cocke,S.A.DellaPietra,V.J.DellaPietra,F.Jelinek,
J. Lafferty, R. L. Mercer, and P. S. Roossin, “A statistical approach to
machinetranslation,”Computationallinguistics,vol.16,no.2,1990.
[57] M. I. Jordan and R. A. Jacobs, “Hierarchical mixtures of experts and
theemalgorithm,”Neuralcomputation,vol.6,no.2,1994.
[58] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with
deep recurrent neural networks,” in Proceedings of the International
Conference on Acoustics, Speech, and Signal Processing (ICASSP),
2013.
[59] H. Sak, A. Senior, and F. Beaufays, “Long short-term memory based
recurrent neural network architectures for large vocabulary speech
recognition,”arXivpreprintarXiv:1402.1128,2014.
[60] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,
R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., “Deep
speech: Scaling up end-to-end speech recognition,” arXiv preprint
arXiv:1412.5567,2014.
[61] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, “Imputer:
Sequence modelling via imputation and dynamic programming,” in
Proceedingsofthe37thInternationalConferenceonMachineLearning
(ICML),2020.

Understanding Masked Autoencoders via Hierarchical Latent Variable Models
LingjingKong*1 MartinQ.Ma∗1 GuangyiChen1,2
EricP.Xing1,2 YuejieChi1 Louis-PhilippeMorency†1 KunZhang†1,2
1CarnegieMellonUniversity 2MohamedbinZayedUniversityofArtificialIntelligence
Abstract
Maskedautoencoder(MAE),asimpleandeffectiveself-
supervisedlearningframeworkbasedonthereconstruction
of masked image regions, has recently achieved prominent
successinavarietyofvisiontasks. Despitetheemergence
of intriguing empirical observations on MAE, a theoreti-
callyprincipledunderstandingisstilllacking. Inthiswork,
we formally characterize and justify existing empirical in-
sightsandprovidetheoreticalguaranteesofMAE.Wefor-
mulatetheunderlyingdata-generatingprocessasahierar-
chical latent variable model and show that under reason-
able assumptions, MAE provably identifies a set of latent
variables in the hierarchical model, explaining why MAE Figure 1. Masking-reconstruction under a hierarchical generating
canextracthigh-levelinformationfrompixels. Further,we process. Inahierarchicaldata-generatingprocess,high-levellatentvari-
showhowkeyhyperparametersinMAE(themaskingratio
ables(e.g., z1)representhigh-levelinformationsuchassemantics,and
low-levellatentvariables(e.g., [z2,z3,z4])representlow-levelinforma-
and the patch size) determine which true latent variables
tionsuchastexture. Weshowthatthroughpropermasking,MAElearns
toberecovered, thereforeinfluencingthelevelofsemantic torecoverhigh-levellatentvariableswithidentifiabilityguarantees.
information in the representation. Specifically, extremely masking has been shown critical to downstream task per-
large or small masking ratios inevitably lead to low-level formances[22,28,61,63]. Itisconjecturedthatsuchmask-
representations. Our theory offers coherent explanations ingforcesthemodeltolearnmeaningfulhigh-levelseman-
ofexistingempiricalobservationsandprovidesinsightsfor tic understanding of the objects and scenes rather than the
potential empirical improvements and fundamental limita- low-levelinformationsuchastexture. However,itremains
tionsofthemasking-reconstructionparadigm. Weconduct largelyunclearwhethersuchintuitionsaresoundinprinci-
extensiveexperimentstovalidateourtheoreticalinsights. ple. Theoreticallyverifyingandcharacterizingtheseempir-
icalinsightswouldnotonlygrantacertificatetothecurrent
1.Introduction
approachesbutwouldalsooffertheoreticalinsightsforal-
Self-supervisedlearning(SSL)hasachievedtremendous gorithmicadvancements.
successinlearningtransferablerepresentationswithoutla- In this work, we establish a principled yet intuitive
bels, showing strong results in a variety of downstream frameworkforunderstandingMAEandprovidingidentifia-
tasks[12,14,16,23,49]. AsamajorSSLparadigm,masked bilityguarantees. Concretely,wefirstformulatetheunder-
imagemodeling(MIM)[1–3,11,13,22,41,63,69]performs lying data-generating process as a hierarchical latent vari-
thereconstructionofpurposelymaskedimagepixelsasthe ablemodel(Figure1),withhigh-levelvariablescorrespond-
pretraining task. Among MIM methods, masked autoen- ing to abstract and semantic information like classes, and
coding(MAE)[22]hasgainedsignificanttractionduetoits low-levelvariablescorrespondingtoelaborateandgranular
computational efficiency and state-of-the-art performance information like texture. Such latent variable models have
inawiderangeofdownstreamtasks. been studied in causal discovery [29,62]. In [27,50], it is
Empirical observations from previous work reveal vari- hypothesized that complex data, such as images, follow a
ousintriguingpropertiesofMAE.Inparticular,aggressive hierarchicallatentstructure.
Stemming from this formulation, we show that under
*Jointfirstauthor.†Jointseniorauthor. reasonable assumptions, MAE can recover a subset of the
3202
nuJ
8
]GL.sc[
1v89840.6032:viXra
true latent variables within the hierarchy, where the levels
z1 z2
of the learned latent variables are explicitly determined by
howmaskingisperformed. Ourtheoreticalframeworknot z3 z4 z5
only unifies existing empirical observations coherently but
also gives rise to insights for potential empirical improve-
mentsandfundamentallimitationsofMAE.Ourtheoryim-
z6 z7 z8 z9 z10
proves the existing nonlinear identifiability results [45,58]
andcanbeofindependentinterest. x1x2x3 x4 x5 x6 x7 x8 x9x10x11
Empirically, we deduce several insights from our theo- Figure2. Ahierarchicaldata-generatingprocess. zrepresents
retical results and verify them with experiments. Unlike thelatentvariablesandxstandsfortheobservablevariables(i.e.
commonbelief,MAEtrainedwithextremelyhighmasking imagepixels).Thehierarchicalmodelisgenericandiscapableof
ratios(e.g.,90%)captureslow-levelinformation,similarto modelingarbitraryDAGsinthelatentspace.
models trainedwith extremely low ratios(e.g., 10%). Our
In this work, we formulate such an underlying struc-
results suggest that learning high-level semantic informa-
ture of images with a hierarchical data-generating pro-
tion is only possible in the non-extreme masking regime.
cess [1,29,62] (Figure 2). Under this formulation, we re-
We also discuss masking designs that can potentially im-
vealtheunderpinningprincipleofMAEandprovideiden-
provecurrentempiricalperformance.
tifiability guarantees. In particular, we show that through
masking-reconstruction, MAElearnsthelong-rangestatis-
ticaldependencieswithintheimage,whichrendersitcapa-
Contributions. Wehighlightthefollowingcontributions:
bleofextractinghigh-levelsemanticrepresentations.
• We formulate the underlying data-generating process Formally,thegeneratingprocessisdefinedwithagraph
as a hierarchical latent variable model. Under such structure G := (V,E) where E is the set of all directed
a formulation, we provide a theoretical guarantee for edgesandV := (X,Z)comprisesallobservablevariables
MAE by showing that it can recover true latent vari- X:={x 1,...,x m}(i.e.,allpixels)andalllatentvariables
ablesinthehierarchicalmodel. Z := {z 1,...,z n}. Each variable x i or z j represents a
multidimensionalvector. 1 Thehierarchicallatentstructure
• Basedonourtheoreticalresults,weestablishthecon- Gfulfillsthefollowingassumption:
nectionbetweenmaskinghyperparameters(i.e.,mask-
Assumption1. (Data-generatingprocess): Thereisnodi-
ingratiosandpatchsizes)andthelearnedrepresenta-
rect edge between any two observables: ∀x ,x ∈ X ,
tion and discuss potential improvements and inherent i j
(x ,x )∈/ Eand(x ,x )∈/ E. Eachvariableisgenerated
limitationsofMAE. i j j i
byitsparentsinadirectedacyclicgraph(DAG)according
to:
• Wevalidateourtheoreticalinsightswithextensiveex-
perimental results. We illustrate how the semantic z =g (Pa(z ),ε ),
i zi i i
(1)
level of the learned representation varies with the ag- x =g (Pa(x ),ε ),
gressiveness of the masking strategy. Interestingly,
j xj j j
representationslearnedunderoverlyaggressivemask- where g zi and g xj are invertible functions, ε i denotes ex-
ing(e.g.,ß90%maskingratio)exhibitsimilarproper- ogenous random variables, and Pa(·) denotes the parents
tiestotheircounterpartslearnedwithoverlyconserva- ofacertainnode.
tivemasking(e.g.,10%maskingratio).
The invertible data-generating-module assumption (g
i
andg beinginvertible)isadoptedfrompriorworkidentify-
j
2.TheoreticalUnderstanding
inglatentvariablesindeepgenerativemodels[18,58]. We
makethefollowingremarksonthehierarchicalgenerating
2.1.AHierarchicalData-generatingProcess
process. First,wenotethatweimposeminimalconstraints
Images,despitetheirhighdimensionality,arewellstruc- on the graph structure among the latent variables (i.e., the
tured – there is a multitude of statistical dependencies connectivityamonglatentvariablesz);therefore,thehierar-
amongpixelsdeterminedbytheirrelativedistancesandvi- chicalmodelclassisgenericandencompassesallpossible
sual semantics. For instance, pixels in close proximity are DAG structures over latent variables (Figure 2). Next, we
often highly dependent, whereas pixels far apart typically interpretthelatentvariableszasinformationrelatedtose-
share less information. There has been a plethora of work mantic/content information, such as the shape and contour
adopting this intuition for vision tasks such as image gen-
1Inhigh-dimensionaldatalikeimages,thereisalargerdegreeofinfor-
eration [47,55,67]. Similar insights are also addressed in
mationredundancy,e.g.,neighboringpixels. Thus,itissensibletolump
attemptstolearnapart-wholeimagerepresentation[27,50]. one-dimensionalvariablesintovectors.
intheimage,whereastheexogenousvariablesεinjectedin s m c s mc
each layer represent nuanced information, such as the tex-
ture and contrast of theimage. Each structural function g
i
mixesthetwosourcesofinformationandgeneratesamore
low-level variable until pixels x. Lastly, for the upcoming x x
m mc
theoretical results, as long as the data-generating process
conforms to the hierarchical graph assumption, our theory
Figure 3. Information sharing latent models. Here, x and
holds, and the insights do not rely on the knowledge of a m
specificgraphstructure.
x mc denotethemaskedpartandthevisiblepartoftheimagex,
respectively. c stands for the maximally shared information be-
2.2.MaskedAutoencoders tweenx mandx mc.s mands mc refertotheinformationspecific
tox mandx mc respectively. Thedashedlineindicatesthepoten-
Asacanonicalmethodofmasking-reconstructionlearn- tialexistenceofstatisticaldependence.
ing,MAE[22]randomlymasksasubsetofpixelpatchesin
theoriginalimageandthenreconstructsthemaskedpatches
1. x = g (c,s ) and x = g (c,s ) where
from the encoded representation of the visible part. More m xm m mc xmc mc
bothg andg areinvertible;
formally,weformulatetheMAEtrainingasfollows. xm xmc
Masksampling: randommasksmaresampledfroma
2. s ⊥⊥(c,s );
m mc
distributionp whichisparameterizedbythemaskingratio
m
r (i.e., the ratio between the number of masked pixels and 3. cisminimal: ∀c′ ⊂Zsuchthatdim(c′)<dim(c),c′
the number of all pixels) and patch size s (i.e., the size of cannotsatisfythetwoconditionsabove.
theminimalmaskingunit).
Such c and the corresponding s are unique and can be
m
MAE encoding: E (x ) maps the unmasked part
mc mc located from the hierarchical structure by executing Algo-
x to a latent representation cˆ 2, where mc denotes the
mc rithm 1. Furthermore, s can be found through Algo-
mc
complement of the mask index set m and is passed to the
rithm2.
encoder as positional embeddings to indicate the positions
ofthevisiblepatches. The proof, Algorithm 1, and Algorithm 2 can be found
MAE decoding: D (cˆ,ˆs ) reconstructs the masked in Appendix A. We note that although the minimal c and
m m
imagex
m
fromtheestimatedlatentvariablecˆ(i.e.,theen- itscorrespondings
m
areuniqueforagivenmaskm,there
coderoutput),andtheauxiliaryinformationˆs membodying isnouniques mc ingeneral. Algorithm2returnsonesuch
positional embeddings and [MASK] token which are fed instance.
to the decoder in MAE. Although ˆs is deterministic in Theorem 1 states that for each mask m, there exists
m
MAE implementation, we view it as a random variable in a corresponding c that represents all the information con-
ouranalysis. tained in the visible part x mc that is conducive to recon-
Withthenotationabove,theMAEtrainingobjectivecan structingthemaskedpartx . Algorithm1canlocatesuch
m
beexpressedasfollows: c in the hierarchy and directly characterizes the impact of
L(E,D):=E m,x,ˆsm(cid:2) ∥D m(E mc(x mc),ˆs m)−x m∥2(cid:3) . (2) ma Nsk ei xn tg ,o inn Tth he eop rr eo mpe 2rt ,y wo ef sc h.
owthatMAElearningobjec-
tive (Equation 2) estimates c specified in Theorem 1, and
2.3.IdentifiabilityTheory
MAEattainsaformofidentifiabilityofc. Wefirstlayout
Buildingupontheformalizationabove,weshowinThe- theassumptions:
orem 1 that each random mask m would induce a specific
Assumption2. (MAEmodel): Foranymaskm, theMAE
(sub)setoflatentvariablesthatfullycapturesthestatistical
decoder D (cˆ,ˆs ) has a non-singular Jacobian matrix
dependency between the masked part and the visible part. m m
almost anywhere, and there exists an invertible function
Wedenotethisrelationshipasc ⊂ Zwherecisthesubset
g˜ (·) such that MAE encoder E (·) = [g˜−1(·)]
ofthelatentvariablesetZ. mc mc mc 1:dc
where [·] denotes the dimensions corresponding to c.
1:dc
Theorem1. (Locatingthesharedinformationc):Inahier- Moreover,(D ,g˜ )formsaninvertiblemappingbetween
m mc
archicallatentvariablestructureG,foreachspecificmask (cˆ,ˆs ,ˆs )and(x ,x )
m mc m mc
m,thereexistsacorrespondingminimalsetoflatentvari-
Next,weshowMAEidentifiesthesharedinformationc:
ables c such that the generating process of x can be ex-
pressed as in Figure 3 where the following conditions are Theorem2. (Identifiabilityofc): Foreachmaskm,given
satisfied: the dimensions (d ,d ) the encoder function E (·) re-
c sm mc
coversallinformationofclocatedinTheorem1,i.e.,there
2Toavoidnotationcluttering,weadoptˆ·todistinguishtheestimated
variablesfromthetrueonesinthegeneratingprocess. existsaone-to-onemappingh,s.t.,h(c)=cˆ.
Inthefollowing,wediscussourassumptionsandresults.
z1 z2 z1 z2 z1 z2
TheproofcanbefoundinAppendixB.
z3 z4 z5 z6 z3 z4 z5 z6 z3 z4 z5 z6
Assumptioninterpretation. Assumption1followsprior
x1 x2 x3 x4 x5 x6 x1 x2 x3 x4 x5 x6 x1 x2 x3 x4 x5 x6
work identifying latent variables in deep generative mod-
(a)Conservativemask (b)Aggressivemask (c)Idealmask
els [18,58] to ensure that latent variables are recoverable
frompixels. Assumption2requirestheMAEencoderE Figure4.Theimpactofmaskingonthelearnedrepresentation.
mc
tobepartofaninvertiblefunctionoutput–thisismildand Welabelthemaskedpixelswithx. WelocatetheMAElearned
latent variables with Algorithm 1 and label them with blue. We
allowstheencodertobemoreflexiblethaninvertiblefunc-
canobservethatextremelylow(left)andhigh(middle)masking
tions. ThedecoderD (cˆ,ˆs )isassumedtobelocallyin-
m m
intensitiesleadtolow-levelrepresentations,whereasthedesirable
vertibleincˆalmostsurely,allowingforabroaderclassthan
maskingintensitythatyieldsahigh-levelrepresentationliesinthe
invertible functions, e.g., nondegenerate polynomials. The
intermediatemaskingaggressiveness.
jointinvertibilityof(D ,g˜ )ensuresnoinformationloss
m mc
duringtheestimationprocess.
tions/scales in the image [61]. In Section 3, we present
empiricalevidencetoverifyourtheoreticalinsights.
How does MAE work? Theorem 2 states that the MAE Takeaway: (1) MAE under different masking intensi-
objective (Equation 2) essentially serves to estimate the ties learns representations of different abstraction levels;
shared variable c and is able to restore all information in (2) Learning high-level representations is very hard with
c. Therefore,theefficacyofMAEstemsfromitsabilityto extrememasking.
extract high-level semantic representations from low-level
features like image pixels. Moreover, our theory indicates
Is current MAE optimal for representation learning?
thepossibilityoffullyidentifyingalatenthierarchicalstruc-
Asreflectedinthediscussionabove, althoughMAEoffers
tureviaproperlydesignedself-supervisedobjectives,open-
theflexibilityoftuningthemaskingschemetolearnrepre-
ingupresearchavenuesforfuturework.
sentations of various levels, it is inherently challenging to
Takeaway: MAEprovablyrecovershigh-levelrepresen-
learn high-level representations by random masking with-
tationsfromlow-levelfeatureslikepixels.
outpriorknowledgeofthelatentstructure.Incontrast,con-
trastivelearning[5,9,10,12,14,23,64]activelyleveragesthe
How does masking influence the learned representa- prior knowledge encoded in data augmentations to extract
tion? Theorem1establishesadirectconnectionbetween theaugmentation-invariantlatentvariables[58]whichcor-
themaskmandthesharedinformationc,whichisfurther respondtothehigh-levellatentvariablesinourhierarchical
connectedtotheMAEestimatecˆinTheorem2.Wecanob- model. Our theory suggests an explanation for why rep-
servethatconservativemaskingwithoverlysmallmasking resentationslearnedbycontrastivelearningaresuperiorto
ratiosandmaskingpatchsizesinevitablyleadstolow-level those of MAE on high-level tasks like linear-probing clas-
latent variables. To see this, in Figure 4a, the mask is not sification.
large enough to cover all observable descendants of a de- Takeaway: Learning high-level representations can be
sirable high-level variable z 1, thus following Algorithm 1 challengingforrandommasking.
alow-levelvariablez willmixincˆ,preventingthemodel
3
from learning z . This insight highlights the necessity of
1 3.Experiments
nontrivialmaskingratiosandpatchsizesandresonateswith
theempiricalobservationsin[22,28,63]. Weconductfivesetsofexperimentsandthenprovidein-
Surprisingly, the above reasoning can be applied to the sightsintopossibleempiricalimprovementsoverMAE.We
casewithextremelyaggressivemasking: inFigure4blow- investigate the following question: how does the masking
levellatentvariablesz willbelearnedbyMAEwhenthe aggressiveness influence the representation? To this end,
6
visiblepartistoosmalltocoverallobservabledescendants we pretrain MAE using different masking ratios and mak-
ofadesirablehigh-levelvariablez . Thus,thelearnedrep- ingpatchsizes,andthenconductthefollowingevaluations:
2
resentationdoesnotbecomemonotonicallymorehigh-level 1)measuringstructure-levelandpixel-levelsimilaritiesbe-
withincreasingmaskingaggressiveness–overlyaggressive tweenthereconstructedandtheoriginalimages;2)visual-
masking also gives rise to low-level representations. This izing self-attentions to understand what is learned; 3) per-
insight echoes the empirical finding in [61,63] where the forming linear probing on ImageNet-1K (IN1K) and dif-
extremelylargemaskingdegradestheperformanceofhigh- ferentImageNetvariants;4)measuringtheshapebias[19]
level downstream tasks like classification [63] but yields which estimates how much a network leverages high-level
relatively low-level representations like the object loca- shape information over low-level texture information; and
Figure5.Reconstructionevaluationusingthevalidationsetwithoutmasking,basedontwostructural-levelsimilaritymetrics(SSIMand
FSIM)andtwopixel-levelmetrics(PSNRandMSE).WeplotnegativeMSEforeasiervisualization. HigherSSIMandFSIMindicate
high-levelinformationisbettercaptured,whilehigherPSNRandnegativeMSEindicatesbetterlow-levelreconstruction.
5) transfer learning on object detection and segmentation. structedimagesandtheoriginalinputishigh,thenMAEis
DetailsofexperimentscanbefoundinAppendix. deemed to capture the low-level information about the in-
putbetter. Thetwolow-levelmetricsarethemeansquared
Pretraining overview. We conduct pretraining on IN1K error(MSE),whichisthesquareddifferencesbetweenthe
using the MAE pipeline [22], with ViT-Base as the back- original and reconstructed images in the pixel space, and
bone of our study. We conduct two sets of pretraining: 1) thepeaksignal-to-noiseratio(PSNR),whichmeasuresthe
fixingpatchsizeat16andvaryingthemaskingratiosfrom ratio between the power of the maximum possible pixel
{0.1,0.25,0.5,0.75,0.9}. Larger masking ratios suggest value and the power of corruption noise. A lower MSE or
larger portions of pixels being masked, i.e., 0.9 suggests ahigherPSNRsuggestsabetterreconstructionatthepixel
90% of pixels being randomly masked for the encoder. 2) level. NotethataverylowMSEoraveryhighPSNRmay
Fix the masking ratio at 0.75 and vary the patch size from alsosuggestthatthemodelcaptureshigh-levelinformation
{8,16,32}. To decouple the patch size for masking im- well. All four metrics are full reference, meaning the as-
agesandthepatchsizehyperparameterintheVisionTrans- sessmentisbasedoncomparingoriginalandreconstructed
former,weadopttheimplementationfrom[28]. Thepatch images rather than the reconstructed output. We introduce
sizestudiedinthispaperreferstotheminimalmaskingunit thehigh-levelandlow-levelmetricsbelowandperformthe
size,andthehyperparameteroftheViTpatchsizeremains reconstructionsontheIN1Kevaluationset. Thefulldetails
fixedat8. andcomparisonsofthefourmetricscanbefoundin[51].
3.1.ReconstructingHigh-levelorLow-levelRepre-
Evaluationofimagereconstructions. Weincludethere-
sentations
sultsinFigure5. WeplotthenegativeoftheMSEtoshow
Setup. We begin our study by evaluating the high-level aconsistenttrendwithPSNR,sohighermeansbetterlow-
structuralandlow-levelpixel-wisesimilaritiesbetweenthe level reconstruction. From the first row, varying masking
reconstructed images from MAE and the original inputs. ratios from 0.1 to 0.75, higher masking ratios produce re-
We choose two metrics for high-level similarities and two constructions with higher structural information similari-
metricsforlow-levelsimilarities. Ifthestructuralsimilari- ties with the original image (higher SSIM and FSIM), but
tiesarehigh,MAEcapturesmoreperceivablestructuralse- the model trained with the extremely high ratio 0.9 cap-
manticsfromtheinput. Thetwohigh-levelsimilaritiesare turesmorelow-levelinformation(higherPSNRandhigher
structural similarity index measure [60] (SSIM) and fea- negative MSE). On the other hand, lower masking ratios
ture similarity index measure [65] (FSIM). Both metrics tend to reconstruct images that capture low-level informa-
consider the change of perceptions in structural informa- tion better. From the second row, larger patch sizes pro-
tion[33].SSIMconsidersthenormalizedmeanvalueofthe duceimagereconstructionsthatcapturehigh-levelsimilar-
structuralsimilaritybetweentheoriginalandreconstructed ities better, while smaller patch sizes have low-level met-
images,andFSIMconsidersthenormalizedmeanvalueof rics. The empirical observations validate our insight from
the feature similarity between the two images. A higher Section2.3: highermaskingratiosandpatchsizescapture
SSIM or a higher FSIM suggests a better reconstruction high-levelstructuralinformationbetter,butextrememask-
of high-level information (structural or feature-wise). On ing ratios (both low and high) capture less high-level and
the other hand, if the pixel-level similarity between recon- morelow-levelinformation.
tionacrosstokens.
Weplotexamplesofself-attentionofthe[CLS]tokenin
Figure6andself-attentionofnon-CLStokensrelatedtothe
objectinFigure7. Fromthevisualizations,asthemasking
ratioincreasesfrom10%to90%,themodelisincreasingly
more able to grasp succinct information about the holistic
objectsratherthanonlyfocusingontheregionsaroundthe
chosen token. However, extreme ratio 0.9 contains more
low-levelinformationandbackgroundinformationandcan-
notcapturemostoftheremainingtokensrelatedtoobjects
(e.g.,thedog,cat,andbeeimagesinFigure7). Extremely
low masking ratios such as 0.1 capture both object-related
andbackgroundtokens. Similarly, extrememaskingratios
contextualize over other object-related tokens worse than
intermediatemaskingratios. Weincludethevisualizations
forpatchsizesinAppendix.Weobservethatmodelstrained
with larger patch sizes better capture high-level informa-
Figure6.Self-attentionofthe[CLS]tokensaveragedacrossthe
tion,butextremepatchsizehurts,whichvalidatesourtheo-
headsofthelastlayerinMAE.
reticalinsightthatmoderatemaskingratiosandpatchsizes
are critical for MAE to learn succinct and comprehensive
objectinformation.
3.3.RepresentationLinearSeparability
T-SNE embedding visualizations. To gain a visual un-
derstanding of how masking ratios and patch sizes influ-
encetherepresentationstructure, wevisualizeT-SNE[57]
embeddings of different models. We randomly select ten
classesfromImageNet. TheresultsareshowninFigure8.
From 0.1 to 0.75, a larger masking ratio consistently pro-
duces a more linearly separable representation, while the
linear separabilities of representations with masking ratios
0.75and0.9arevisuallysimilar. Fordifferentpatchsizes,
theembeddingsaremoreseparatedasthepatchsizesgrow.
Non-extrememaskingratiosandlargerpatchsizesgenerate
morelinearlyseparableembeddings.
Figure7. Self-attentionofanobject-relatedtoken. Chosento- Linear probing on IN1K. We use linear probing to test
kensareshowninredsquares: dognose,catchin,beeabdomen,
howlinearlyseparablethefeaturesareinthelearnedMAE
chickenhead,andfootballcenter,respectively.
representation. WeshowthelinearprobingresultsinTable
1inrow1N1K.Fordifferentmaskingratios,similartothe
3.2.AttentionAnalysis
observationin[22],theaccuracyincreasessteadilyuntilthe
In this section, we measure the property of the learned maskingratioreachesthesweetpointof0.75.Anextremely
representationsofMAEbyprobingtheattentionheads. We large masking ratio (0.9) hurts performance. For different
would like to understand visually how masking ratios and patch sizes, which are not shown in [22], we observe that
patch sizes influence MAE’s capacity to capture object- theaccuracyincreasesfirstfrom8to16,thendecreasessig-
centric semantics. We provide two types of visualization: nificantlywhenthepatchsizeis32.Fromtheresults,higher
self-attentiononthe[CLS]tokenandself-attentiononan maskingratiosandlargerpatchsizesperformbetteratlin-
object-related token. [CLS] has been considered a com- earprobingthanlowermaskingratios,butextrememasking
pact token to represent the whole image for downstream hurtslinearprobing.
tasks, although recent work [22] suggests that the average
pooling of all tokens may achieve slightly better results. RobustnessevaluationonImageNetvariants. Weeval-
Therefore,wealsoprovideananalysisofobject-relatedto- uatetherobustnessoftheMAEmodelsondifferentvariants
kenstoevaluateifMAEcancontextualizeobjectinforma- ofImageNetvalidationdatasets,orobjectdetectiondatasets
Figure8.T-SNEembeddingsofdifferentMAEmodelsundervariedmaskingratiosandpatchsizes.Wefixthepatchsizeat16tovarythe
maskingratiosandfixthemaskingratioat0.75tochangethepatchsizes.EachcolorrepresentsoneImageNetclass.
maskratio patchsize IN1K IN-v2 OJN IN-R IN-A IN-S
maskratio 0.1 0.25 0.5 0.75 0.9
0.1 16 47.45 34.72 9.42 14.63 2.00 7.25
shapebias 0.1352 0.2545 0.2458 0.2563 0.2014
0.25 16 53.58 40.34 11.54 18.68 2.49 10.27
0.5 16 60.07 46.71 13.94 22.44 2.89 12.58
Table2. Shapebias[19]measurement,ahighermetricindicates
0.75 16 67.41 54.23 18.24 25.20 3.76 15.51
that the model classifies images relying on the high-level shape
0.9 16 62.97 49.52 15.87 19.11 2.76 10.46
featureratherthanthelow-leveltexturefeature.
0.75 8 62.57 49.17 13.44 19.42 3.73 10.73
maskratio masksize APbox APmask
0.75 16 68.96 55.94 13.73 24.23 6.29 18.81
0.1 16 30.47 28.24
0.75 32 73.31 61.35 19.03 27.84 12.69 28.30
0.25 16 32.38 29.95
Table1. Accuracy(%)oflinearprobingandrobustnessevalu- 0.5 16 34.87 32.11
ationonImageNetvariantsandObjectNet.Welinear-probeMAE 0.75 16 39.72 36.35
via supervised training on IN1K, and then perform inference on 0.9 16 37.17 34.35
IN1Kaswellasotherevaluationsets.
Table3. COCOobjectdetectionandsegmentationusingaViT
that share similar class information with ImageNet-1K: MaskR-CNNbaseline.
ImageNet-v2(INV2)[52],ObjectNet(OJN)[4],ImageNet-
tectionandsegmentationontheCOCOdataset[43],which
Adversarial (IN-A) [25], ImageNet-Rendition [4], and
requires a strong semantic understanding of the scenes.
ImageNet-Sketch(IN-S)[59]. Thesedatasetssharesimilar
We finetune Mask R-CNN [24] end-to-end using MAE-
semanticsandlabelswithImageNetbutareunderdifferent
pretrainedViTweights. Followingthepracticein[22],we
datadistributions.TheMAEmodelsarefirsttrainedinasu-
adapt the ViT backbone to make it compatible with FPN
pervisedfashiononIN1Kforlinearprobing,andinference
[42]. InTable3,wereportboxAPforobjectdetectionand
is run on the evaluation sets without any training. Table 1
mask AP for instance segmentation. We reduce the num-
showsforallevaluationdatasets,areasonablylargemask-
ber of epochs to 45 due to computational constraints. We
ingratio(i.e.,0.75)achievesbetterrobustnessthansmaller
observe that the 0.75 masking ratio yields the best detec-
(i.e.,0.25)maskingratios,althoughextremelylarge(0.9)or
tionandsegmentationaverageprecision,suggestingthatthe
small(0.1)maskingratioshurttheperformance. Forpatch
masking ratio 0.75 generates representation with the best
sizes, largerpatchsizesyieldbetterrobustnessevaluations
semanticunderstanding. Theextremelyhighmaskingratio
onIN-v2, OJN,IN-R,andIN-S.Non-extrememaskingra-
of0.9andalowmaskingratioof0.1hurttheperformance.
tiosandlargepatchsizeshavestrongerrobustnessperfor-
Results of different patch size experiments are included in
mancesthanextrememaskingratiosorpatchsizes.
Appendix. Theresultssuggestthathigher,butnotextreme,
3.4.ShapeBias masking ratios generate the best representation of object
detectionandsegmentationtasks.
Texture vs. shape bias. Next, we analyze to what ex-
tentdifferentMAEmodelsrelyonhigh-levelvs. low-level
3.6.PotentialAlgorithmicImprovements
information. Wefollowtheanalysisin[19],wheretheau-
thorsstudywhetheramodelleveragesmorelow-leveltex- Lastly, we discuss empirical suggestions based on our
tures than high-level shapes for classification. As shown resultsthatcouldbenefittheperformanceofMAE.
inTable2, intermediatemaskingratios(i.e., 0.25,0.5, and
First,asdiscussedinSection2,whenreconstructingthe
0.75) show a high level of shape bias, suggesting that the
masked pixels near the boundary between the masked and
correspondingmodelsexploitmorehigh-levelshapeinfor-
unmasked regions, the model uses nearby visible pixels to
mation. In contrast, extreme masking ratios (i.e., 0.1 and
interpolate,thereforecapturinglow-levelpixelinformation.
0.9) leverage more low-level textures. This suggests that
Ifhigh-levelrepresentationisdesiredfordownstreamtasks,
extreme masking schemes make it more difficult to capture
the boundary pixels may be ignored when calculating the
high-levelshapesforMAE.
objectivefunction.
Next,inlightofthelimitationofrandommaskinginSec-
3.5.TransferLearning
tion2,onemayleveragethelatentstructureoftheunderly-
Next,weevaluatethequalityofMAEmodelsondiffer- ingdata-generatingprocessformaskingdesigns,whichcan
ent downstream tasks. Specifically, we look at object de- serve as a more principled approach than recent work that
exploits auxiliary information for masking [34,40,41,53]. processratherthanthestatistical/optimizationcomplexities
To this end, one may take advantage of the recent devel- asinmostpriorwork.
opment of causal discovery [29,62] to identify the latent
4.3. Identifiability Guarantees for Nonlinear
structure.
Latent-variableModels
Lastly, if low-level information is preferable for down-
stream tasks, an extremely high masking ratio can retain In unsupervised learning, identifiability means latent
suchinformationandismorecomputationallyefficientthan variablesinvolvedintheunderlyingdata-generatingprocess
itslowmaskingratiocounterpart. can be estimated from observational data. This is critical
to tasks like feature disentanglement [7,26,30,35,62] in
4.Relatedwork
theimagegenerationcommunity. However,principleddis-
entanglement in the non-linear regime is challenging and
4.1.MaskedAutoencoders
evenprovedimpossiblewithoutadditionalassumptionson
Maskedimagemodeling (MIM)[1–3,11,13,22,41,63, thedata-generatingprocess[44]. Recentadvancesininde-
69]hasbeengainingmomentumrecentlyduetotheirsota- pendent component analysis (ICA) [6,15,31] obtain iden-
of-the-artperformancesovermanydownstreamtasks. The tifiability in the non-linear regime by imposing additional
pretrainingobjectiveissimpleinitsbasicform: themodel constraints on either the latent variable distribution or the
is tasked to predict the masked-out image pixels with the functionclassvariables[20,32,36–38,45,54,58,68]. Most
information of the unmasked part. Despite the simplic- relevant to ours are the identifiability theories in [45,58]
ity of the task, many intriguing properties have been ob- in which similar latent causal models (Figure 3) are stud-
servedonMIMthatescaperigorousanalysis. Forinstance, ied. Specifically, our model allows the generating func-
small masking ratios and masking patch sizes are empiri- tions g ̸= g to be distinct (cf. identical functions as-
m mc
cally shown detrimental to downstream tasks like classifi- sumed in [58]) and statistical dependence between c and
cation [22,28]. It is hypothesized that aggressive masking s (cf. independenceassumedin[46]). Additionally,both
mc
forcestomodeltoleveragemoreglobalinformation,rather works[46,58]focusoncontrastivelearningwithdataaug-
than local interpolation [22]. However, whether such in- mentation,whileoursubjectisMAE.
tuition is theoretically justifiable remains elusive. In this
work,weprovidetheoreticalverificationofsuchintuitions 5.Conclusion
andfurtherderiveinsightsintoMAE’sempiricalbehavior.
In this work, we formulate the data-generating process
4.2.TheoreticalUnderstandingofMAE asahierarchicallatentvariablemodelandprovideguaran-
tees that MAE can identify the true variables in such a hi-
Despite the prominent success of MAE, only a limited
erarchicallatentmodel. Wethenshowhowdifferentmask-
number of papers are dedicated to understanding its un-
ing ratios and patch sizes determine the set of true latent
derlying mechanism in a principled manner [8,39,48,66].
variables to be recovered, which influences the represen-
Leeetal.[39]establishtheconnectionbetweentheinpaint-
tation abstractions learned in MAE. Empirically, we show
ingpretrainingtaskanddownstreamtasksbyassumingthat
that non-extreme masking ratios or patch sizes often cap-
the downstream task target captures the statistical depen-
ture succinct and robust high-level information, while ex-
dency between the visible part and the masked part in the
trememaskingratioscapturemorelow-levelinformation.
inpainting. Underthisassumption,theyshowthatthesam-
plingcomplexityofthedownstreamtaskcanbelargelyre- Acknowledgement We thank the Google TRC program for
duced by pretraining. Cao et al. [8] inquire into the inter- theTPUResearchCloudsupport,RonghangHuandXinleiChen
actionsbetweenthetransformerarchitectureandtheMAE for the MAE TPU code, Biwei Huang for technical discussions,
representation,highlightingthecriticalroleoftheattention TaoLinforfeedbackonthemanuscript,andanonymousreviewers
mechanism in the success of MAE. Pan et al. [48] make a forvaluablefeedback.TheworkofLKandYCissupportedinpart
multi-viewassumptiononthesamples,showingthatMAE by NSF under the grants CCF-1901199 and DMS-2134080, and
can extract class-relevant semantics with shallow convolu- byONRunderthegrantN00014-19-1-2404.TheworkofMMand
LPispartiallysupportedbyBMW,NationalScienceFoundation
tionalmodels. Zhangetal.[66]studymaskingthroughthe
awards 1722822 and 1750439, and National Institutes of Health
data-augmentation perspective and employ the augmenta-
awards R01MH125740, R01MH096951, R21MH130767 and
tiongraph[21]toillustratetheimpactofmaskingondown-
R01MH132225.ThisprojectisalsopartiallysupportedbytheNa-
streamtaskperformance. Incontrast,ourworkemploysthe
tional Institutes of Health (NIH) under Contract R01HL159805,
hierarchicallatentvariablemodel,whichletsusdirectlyex-
bytheNSF-ConvergenceAcceleratorTrack-Daward2134901,by
amine the relationship between the masking operation and agrantfromAppleInc.,agrantfromKDDIResearchInc,andgen-
thelearnedrepresentations. Also,ourtheoreticalguarantee erousgiftsfromSalesforceInc.,MicrosoftResearch,andAmazon
isonthestatisticalidentifiabilityofthetruedata-generating Research.
References [14] Xinlei Chen, Saining Xie, and Kaiming He. An empiri-
calstudyoftrainingself-supervisedvisiontransformers. In
[1] AnimashreeAnandkumar,DanielHsu,AdelJavanmard,and
ProceedingsoftheIEEE/CVFInternationalConferenceon
ShamKakade.Learninglinearbayesiannetworkswithlatent
ComputerVision,pages9640–9649,2021. 1,4
variables.InInternationalConferenceonMachineLearning,
[15] PierreComon. Independentcomponentanalysis,anewcon-
pages249–257.PMLR,2013. 1,2,8
cept? Signalprocessing,36(3):287–314,1994. 8
[2] AlexeiBaevski,Wei-NingHsu,QiantongXu,ArunBabu,Ji-
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
ataoGu,andMichaelAuli. Data2vec:Ageneralframework
Toutanova. Bert: Pre-training of deep bidirectional
forself-supervisedlearninginspeech,visionandlanguage.
transformers for language understanding. arXiv preprint
arXivpreprintarXiv:2202.03555,2022. 1,8
arXiv:1810.04805,2018. 1
[3] HangboBao,LiDong,andFuruWei.Beit:Bertpre-training
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
of image transformers. arXiv preprint arXiv:2106.08254,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
2021. 1,8
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
[4] Andrei Barbu, David Mayo, Julian Alverio, William Luo,
vain Gelly, et al. An image is worth 16x16 words: Trans-
Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and
formers for image recognition at scale. arXiv preprint
BorisKatz. Objectnet: Alarge-scalebias-controlleddataset
arXiv:2010.11929,2020. 14
for pushing the limits of object recognition models. Ad-
[18] Locatelloetal. Weakly-superviseddisentanglementwithout
vancesinneuralinformationprocessingsystems,32,2019.
compromises. 2,4
7,15
[19] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,
[5] Adrien Bardes, Jean Ponce, and Yann LeCun. Vi-
MatthiasBethge,FelixAWichmann,andWielandBrendel.
creg:Variance-invariance-covarianceregularizationforself-
Imagenet-trainedcnnsarebiasedtowardstexture;increasing
supervisedlearning.arXivpreprintarXiv:2105.04906,2021.
shapebiasimprovesaccuracyandrobustness.arXivpreprint
4
arXiv:1811.12231,2018. 4,7,15
[6] AnthonyJBellandTerrenceJSejnowski. Aninformation-
[20] HermanniHa¨lva¨andAapoHyvarinen. Hiddenmarkovnon-
maximizationapproachtoblindseparationandblinddecon-
linear ica: Unsupervised learning from nonstationary time
volution. Neuralcomputation,7(6):1129–1159,1995. 8
series. In Conference on Uncertainty in Artificial Intelli-
[7] Christopher P Burgess, Irina Higgins, Arka Pal, Loic
gence,pages939–948.PMLR,2020. 8
Matthey, NickWatters, GuillaumeDesjardins, andAlexan-
derLerchner.Understandingdisentanglinginbetavae.arXiv [21] JeffZHaoChen,ColinWei,AdrienGaidon,andTengyuMa.
preprintarXiv:1804.03599,2018. 8 Provable guarantees for self-supervised deep learning with
spectral contrastive loss. Advances in Neural Information
[8] ShuhaoCao,PengXu,andDavidA.Clifton. Howtounder-
ProcessingSystems,34:5000–5011,2021. 8
standmaskedautoencoders,2022. 8
[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
[9] MathildeCaron,IshanMisra,JulienMairal,PriyaGoyal,Pi-
Dolla´r,andRossGirshick.Maskedautoencodersarescalable
otrBojanowski,andArmandJoulin. Unsupervisedlearning
visionlearners,2021. 1,3,4,5,6,7,8,14,16
ofvisualfeaturesbycontrastingclusterassignments. ArXiv,
abs/2006.09882,2020. 4 [23] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRoss
Girshick. Momentumcontrastforunsupervisedvisualrep-
[10] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou,
resentationlearning. InProceedingsoftheIEEE/CVFcon-
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg-
ference on computer vision and pattern recognition, pages
ing properties in self-supervised vision transformers. In
9729–9738,2020. 1,4
ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages9650–9660,2021. 4,15 [24] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir-
[11] MarkChen,AlecRadford,RewonChild,JeffreyWu,Hee-
shick.Maskr-cnn.InProceedingsoftheIEEEinternational
wooJun, DavidLuan, andIlyaSutskever. Generativepre- conferenceoncomputervision,pages2961–2969,2017. 7,
training from pixels. In Hal Daume´ III and Aarti Singh, 16
editors, Proceedings of the 37th International Conference [25] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
on Machine Learning, volume 119 of Proceedings of Ma- hardt, and Dawn Song. Natural adversarial examples. In
chineLearningResearch,pages1691–1703.PMLR,13–18 ProceedingsoftheIEEE/CVFConferenceonComputerVi-
Jul2020. 1,8 sionandPatternRecognition,pages15262–15271,2021. 7,
[12] TingChen,SimonKornblith,MohammadNorouzi,andGe- 15
offreyHinton. Asimpleframeworkforcontrastivelearning [26] IrinaHiggins,LoicMatthey,ArkaPal,ChristopherBurgess,
ofvisualrepresentations.InInternationalconferenceonma- Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
chinelearning,pages1597–1607.PMLR,2020. 1,4 AlexanderLerchner. beta-VAE:Learningbasicvisualcon-
[13] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, ceptswithaconstrainedvariationalframework. InInterna-
Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, tionalConferenceonLearningRepresentations,2017. 8
Gang Zeng, and Jingdong Wang. Context autoencoder [27] GeoffreyHinton.Howtorepresentpart-wholehierarchiesin
for self-supervised representation learning. arXiv preprint aneuralnetwork.arXivpreprintarXiv:2102.12627,2021.1,
arXiv:2202.03026,2022. 1,8 2
[28] RonghangHu, ShoubhikDebnath, SainingXie, andXinlei Tang, et al. Mst: Masked self-supervised transformer for
Chen.Exploringlong-sequencemaskedautoencoders.arXiv visualrepresentation. AdvancesinNeuralInformationPro-
preprintarXiv:2210.07224,2022. 1,4,5,8,14 cessingSystems,34:13165–13176,2021. 1,8
[29] Biwei Huang, Charles Jia Han Low, Feng Xie, Clark [42] Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming He,
Glymour, and Kun Zhang. Latent hierarchical causal Bharath Hariharan, and Serge Belongie. Feature pyra-
structure discovery with rank constraints. arXiv preprint mid networks for object detection. In Proceedings of the
arXiv:2210.01798,2022. 1,2,8 IEEE conference on computer vision and pattern recogni-
[30] XunHuang, Ming-YuLiu, SergeBelongie, andJanKautz. tion,pages2117–2125,2017. 7,16
Multimodal unsupervised image-to-image translation. In [43] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
ProceedingsoftheEuropeanconferenceoncomputervision PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence
(ECCV),pages172–189,2018. 8 Zitnick. Microsoft coco: Common objects in context. In
[31] A.Hyva¨rinen,J.Karhunen,andE.Oja. IndependentCom- European conference on computer vision, pages 740–755.
ponentAnalysis. JohnWiley&Sons,Inc,2001. 8 Springer,2014. 7,16
[32] AapoHyvarinen,HiroakiSasaki,andRichardTurner. Non- [44] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar
linear ica using auxiliary variables and generalized con- Raetsch, Sylvain Gelly, Bernhard Scho¨lkopf, and Olivier
trastivelearning. InThe22ndInternationalConferenceon Bachem. Challengingcommonassumptionsintheunsuper-
ArtificialIntelligenceandStatistics,pages859–868.PMLR, vised learning of disentangled representations. In interna-
2019. 8 tional conference on machine learning, pages 4114–4124.
[33] JustinJohnson,AlexandreAlahi,andLiFei-Fei. Perceptual PMLR,2019. 8
losses for real-time style transfer and super-resolution. In
[45] Qi Lyu, Xiao Fu, Weiran Wang, and Songtao Lu. Latent
European conference on computer vision, pages 694–711.
correlation-based multiview learning and self-supervision:
Springer,2016. 5 A unifying perspective. arXiv preprint arXiv:2106.07115,
[34] Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yan- 2021. 2,8
nisAvrithis,AndreiBursuc,KonstantinosKarantzalos,and
[46] Qi Lyu, Xiao Fu, Weiran Wang, and Songtao Lu. Un-
Nikos Komodakis. What to hide from your students:
derstandinglatentcorrelation-basedmultiviewlearningand
Attention-guided masked image modeling. arXiv preprint
self-supervision: Anidentifiabilityperspective. InInterna-
arXiv:2203.12719,2022. 8
tionalConferenceonLearningRepresentations,2022. 8
[35] Tero Karras, Samuli Laine, and Timo Aila. A style-based
[47] Lars Maaløe, Marco Fraccaro, Valentin Lie´vin, and Ole
generator architecture for generative adversarial networks.
Winther. Biva: Averydeephierarchyoflatentvariablesfor
In Proceedings of the IEEE/CVF Conference on Computer
generative modeling. Advances in neural information pro-
VisionandPatternRecognition(CVPR),June2019. 8
cessingsystems,32,2019. 2
[36] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and
[48] Jiachun Pan, Pan Zhou, and Shuicheng Yan. Towards un-
AapoHyvarinen.Variationalautoencodersandnonlinearica:
derstanding why mask-reconstruction pretraining helps in
Aunifyingframework. InInternationalConferenceonAr-
downstreamtasks. arXivpreprintarXiv:2206.03826,2022.
tificialIntelligenceandStatistics,pages2207–2217.PMLR,
8
2020. 8
[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[37] Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
GuangyiChen,PetarStojanov,VictorAkinwande,andKun
AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
Zhang. Partial disentanglement for domain adaptation. In
ingtransferablevisualmodelsfromnaturallanguagesuper-
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
vision. In International Conference on Machine Learning,
Szepesvari, GangNiu, andSivanSabato, editors, Proceed-
pages8748–8763.PMLR,2021. 1
ingsofthe39thInternationalConferenceonMachineLearn-
[50] SaraSabour, NicholasFrosst, andGeoffreyEHinton. Dy-
ing, volume 162 of Proceedings of Machine Learning Re-
namicroutingbetweencapsules. Advancesinneuralinfor-
search,pages11455–11472.PMLR,17–23Jul2022. 8
mationprocessingsystems,30,2017. 1,2
[38] Se´bastienLachapelle, PauRodr´ıguezLo´pez, YashSharma,
Katie Everett, Re´mi Le Priol, Alexandre Lacoste, and Si- [51] Umme Sara, Morium Akter, and Mohammad Shorif Ud-
monLacoste-Julien. Disentanglementviamechanismspar- din. Imagequalityassessmentthroughfsim,ssim,mseand
sityregularization: Anewprinciplefornonlinearica. arXiv psnr—acomparativestudy. JournalofComputerandCom-
preprintarXiv:2107.10098,2021. 8 munications,7(3):8–18,2019. 5
[39] Jason D. Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. [52] Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex
Predicting what you already know helps: Provable self- Fang,BenjaminRecht,andLudwigSchmidt.Evaluatingma-
supervisedlearning,2021. 8 chineaccuracyonimagenet. InInternationalConferenceon
[40] GangLi,HeliangZheng,DaqingLiu,ChaoyueWang,Bing MachineLearning,pages8634–8644.PMLR,2020. 7,15
Su, and Changwen Zheng. Semmae: Semantic-guided [53] YugeShi,N.Siddharth,PhilipH.S.Torr,andAdamR.Ko-
maskingforlearningmaskedautoencoders. arXivpreprint siorek. Adversarial masking for self-supervised learning,
arXiv:2206.10207,2022. 8 2022. 8
[41] Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong [54] Peter Sorrenson, Carsten Rother, and Ullrich Ko¨the. Dis-
Zhu,ChaoyangZhao,RuiDeng,LiweiWu,RuiZhao,Ming entanglementbynonlinearicawithgeneralincompressible-
flownetworks(gin).arXivpreprintarXiv:2001.04872,2020. [69] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang
8 Xie,AlanYuille,andTaoKong.ibot:Imagebertpre-training
[55] ArashVahdatandJanKautz.Nvae:Adeephierarchicalvari- with online tokenizer. arXiv preprint arXiv:2111.07832,
ational autoencoder. Advances in Neural Information Pro- 2021. 1,8
cessingSystems,33:19667–19679,2020. 2
[56] Laurens Van Der Maaten. Accelerating t-sne using tree-
based algorithms. The Journal of Machine Learning Re-
search,15(1):3221–3245,2014. 15
[57] Laurens Van der Maaten and Geoffrey Hinton. Visualiz-
ingdatausingt-sne. Journalofmachinelearningresearch,
9(11),2008. 6
[58] JuliusVonKu¨gelgen,YashSharma,LuigiGresele,Wieland
Brendel, Bernhard Scho¨lkopf, Michel Besserve, and
FrancescoLocatello.Self-supervisedlearningwithdataaug-
mentations provably isolates content from style. Advances
inneuralinformationprocessingsystems,34:16451–16467,
2021. 2,4,8
[59] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P
Xing. Learningrobustglobalrepresentationsbypenalizing
localpredictivepower.AdvancesinNeuralInformationPro-
cessingSystems,32,2019. 7,15
[60] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSi-
moncelli. Imagequalityassessment: fromerrorvisibilityto
structuralsimilarity.IEEEtransactionsonimageprocessing,
13(4):600–612,2004. 5
[61] ZhirongWu, ZihangLai, XiaoSun, andStephenLin. Ex-
treme masking for learning instance and distributed visual
representations. arXivpreprintarXiv:2206.04667,2022. 1,
4
[62] FengXie,BiweiHuang,ZhengmingChen,YangboHe,Zhi
Geng,andKunZhang. Identificationoflinearnon-gaussian
latenthierarchicalstructure. InInternationalConferenceon
MachineLearning,pages24370–24387.PMLR,2022. 1,2,
8
[63] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin
Bao,ZhuliangYao,QiDai,andHanHu.Simmim:Asimple
frameworkformaskedimagemodeling. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages9653–9663,2022. 1,4,8
[64] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and
Ste´phaneDeny. Barlowtwins: Self-supervisedlearningvia
redundancyreduction. InInternationalConferenceonMa-
chineLearning,pages12310–12320.PMLR,2021. 4
[65] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang.
Fsim: A feature similarity index for image quality assess-
ment. IEEEtransactionsonImageProcessing,20(8):2378–
2386,2011. 5
[66] QiZhang,YifeiWang,andYisenWang.Howmaskmatters:
Towardstheoreticalunderstandingsofmaskedautoencoders.
arXivpreprintarXiv:2210.08344,2022. 8
[67] ShengjiaZhao,JiamingSong,andStefanoErmon. Learning
hierarchicalfeaturesfromgenerativemodels. arXivpreprint
arXiv:1702.08396,2017. 2
[68] YujiaZheng,IgnavierNg,andKunZhang. Ontheidentifia-
bilityofnonlinearica: Sparsityandbeyond. arXivpreprint
arXiv:2206.07751,2022. 8
A.ProofforTheorem1 Algorithm2Searchfors mc givenC. LocateParents(p)pins
downthelocationsp’sparents(includingexogenousvariables)in
thegraph.
Inthissection,weprovidetheproofforTheorem1.
1: inputs: The hierarchical graph structure G, the parti-
tionedobservablesX ,X ,andC returnedbyAlgo-
m mc
rithm1.
Theorem1. (Locatingthesharedinformationc):Inahierarchi-
cal latent variable structure G, for each specific mask m, there 2: S mc ←∅.
existsacorrespondingminimalsetoflatentvariablescsuchthat 3: for x∈X mc do
thegeneratingprocessofxcanbeexpressedasinFigure3where 4: P,P′ ←{x},∅.
thefollowingconditionsaresatisfied: 5: while P ≠ ∅ do
6: for p∈P do
7: for p′ ∈LocateParents(p) do
1. x m = g xm(c,s m)andx mc = g xmc(c,s mc)whereboth 8: if p′isexogenous then
g andg areinvertible;
xm xmc
9: S mc ←S mc ∪{p′}
10: elseif p′ ∈C then
2. s
m
⊥⊥(c,s mc);
11: S mc ←S mc∪(LocateParents(p)\
{p′})
3. c is minimal: ∀c′ ⊂ Z such that dim(c′) < dim(c), c′ 12: else
cannotsatisfythetwoconditionsabove.
13: P′ ←P′∪{p′}
14: P ←P′
return S
mc
Suchcandthecorrespondings areuniqueandcanbelocated
m
fromthehierarchicalstructurebyexecutingAlgorithm1.Further-
more,s mc canbefoundthroughAlgorithm2. Proof. WewillshowthatAlgorithm1returnstheminimalsetof
variablesthatsatisfyallconditionsinTheorem1,whichimpliesits
existence. WewillthenarguethatsuchC isuniqueforaspecific
maskm.
Algorithm 1 Search for the minimal c and sm. c and sm dis- Condition1: Wefirstdiscusstheinvertibilityofg . Dueto
cussed in text can be viewed as the concatenations of vectors in C and
xm
theinvertibilityassumptionofthegeneratingprocess,eachback-
Sm. LocateParents(·)pinsdownthelocationsZ’sparents(includingex-
ogenousvariables)inthegraph. DirectedPaths(d,Xc)returnsthesetof trackstepinAlgorithm1isinvertible(lossless). Thus,beforethe
m
variablesonthedirectedpathsbetweendandX mc. pruningstage,themappingbetween(C,S m)andX misinvertible,
astheinformationofX iseitherstoredineitherC orS . We
1: inputs: The hierarchical graph structure G, and the m m
nowshowthatthepruningstagedoesnotbreakthisinvertibility.
partitionedobservablesX ,X .
m mc Toseethis,wenotethatforeachcthatisremovedinthepruning
2: C,S m ←∅,∅. stage,thereexistsc′ ∈ConthedirectedpathfromctoX mc (per
3: Selectionstage: Algorithm1). Therefore,cisaparent/ancestorofc′andcanthus
4: for x∈X mdo beretrievedbybacktrackingfromc′ thankstotheinvertibilityof
5: Z ←{x}. thegeneratingprocess. Therefore,themappingbetween(C,S )
m
6: while Z ̸=∅ do andX misinvertible.
7: Z,E ←LocateParents(Z) Wenowaddresstheinvertibilityofg xmc,i.e.,themappingbe-
8: S m ←S m∪E atw ppe le in es( :C A,S lgm oc ri) tha mnd 2X dm icc t. ateW se tho ab ts te hr eve lat th ea nt ta vas ri im abil la er sa fr rg ou mm te hn et
9: for p∈Z do backtracking from X mc are either stored in either C or S mc. It
10: if p∈Ancestors(x mc) then followsthatg xmc isinvertible.
11: C ←C∪{p}
12: Z ←Z \{p} Condition 2: We show that (c,s m,s mc) returned by Algo-
13: Pruningstage: rithm1andAlgorithm2satisfiesCondition2bycontradiction.We
14: for d∈C do supposethats m ̸⊥⊥(c,s mc).Thenitimpliedthat∃d∈(c,s mc),
15: for d′ ∈C\{d} do ∃ε ∈ s m,suchthatd ∈ Descendants(ε). Moreprecisely,itfol-
16: if d′ ∈DirectedPaths(d,X mc) then lowedthattherewasadirectedpaththatstartedfromεandended
atd,andachildofε,denotedasδ,waslocatedonthispath. If
17: C ←C\{d}
d̸∈Descendants(ε),therewouldbenodirectedpathsfromεtod
return C,S
m andthusatleastoneV-structurewouldsitoneachpathbetweenε
anddthatblockedthepath.AccordingtoAlgorithm1,asε∈s , lows:
m
itimpliedthatδ̸∈candδ̸∈Ancestors(x m)∩Ancestors(x mc).
[v ,v ]=g(c,s ,s ), (3)
We first investigate the case where d ∈ c, i.e., s ̸⊥⊥ c. 1 2 1 2
m
The fact that d ∈ c implied that d ∈ Ancestors(x m) ∩ v 1 =g 1(c,s 1), (4)
Ancestors(x mc)whichfurtherimpliedthatδ∈Ancestors(x m)∩ v
2
=g 2(c,s 2), (5)
Ancestors(x mc)asδwasanancestorofd.Therefore,wehavear-
rivedatacontractiontotheobservationthatδ∈Ancestors(x m)∩ wherec∈C ⊂Rdc,s
1
∈S ⊂Rds1,ands
2
∈S
2
⊂Rds2.Both
Ancestors(x mc). g
1
and g
2
are smooth and have non-singular Jacobian matrices
Wenowdiscussthescenariowhered ∈ s mc. Bydesign,Al- almostanywhere,andgisinvertible.
gorithm2ensuresthats mc containstwotypesoflatentvariables, If gˆ 1 : Z → V 1 and gˆ 2 : Z → V 2 assume the generating
exogenousvariablesandaspouseoflatentvariablesinc. Ass m processofthetruemodel(g 1,g 2)andmatchthejointdistribution
consistssolelyofexogenousvariablesandexogenousvariablesare p ,thenthereisaone-to-onemappingbetweentheestimate
v1,v2
independentmutually,itcouldonlybethecasethatdwasaspouse cˆ and the ground truth c over C ×S ×S, that is, c is block-
of a latent variable in c. By Algorithm 1, there would be a di- identifiable.
rectedpathfromδtox . Also,Algorithm2ensuredthatdlied
m
onapathdirectedtox mc.Asthereexistedadirectedpathfromδ Proof. For(v 1,v 2)∼p v1,v2,becauseofthematchedjointdistri-
tod,theremustexistadirectedpathfromδ tox mc. Therefore, bution,wehavethefollowingrelationsbetweenthetruevariables
δ∈Ancestors(x m)∩Ancestors(x mc)whichcontradictsthefact (c,s 1,s 2)andtheestimatedones(cˆ,ˆs 1,ˆs 2):
establishedabove.
v =g (c,s )=gˆ (cˆ,ˆs ), (6)
Therefore,thesecontradictionimpliesthats
m
⊥⊥(c,s mc). 1 1 1 1 1
v =g (c,s )=gˆ (cˆ,ˆs ), (7)
Sofar,wehaveshownthatAlgorithm1andAlgorithm2yield 2 2 2 2 2
(c,s m,s mc)thatfulfillstheconditionsofFigure3.Inthefollow- (cˆ,ˆs 1,ˆs 2)=gˆ−1(v 1,v 2)=gˆ−1(g(c,s 1,s 2)):=h(c,s 1,s 2),
ing,weshowthat(c,s m)istheminimalsolutionandisunique. (8)
where we define the smooth and invertible function h :=
Uniqueness and minimality of (c,s m): We now reason gˆ−1◦g thattransformsthetruevariables(c,s 1,s 2)toestimates
aboutthatgiventhemaskandthehierarchicalstructure, (c,s m) (cˆ,ˆs 1,ˆs 2).
returnedbyAlgorithm1isthesetofminimaldimensionalitythat PluggingEquation8intoEquation6yieldsthefollowing:
canfulfilltheconditions,andsuchaminimalsetisunique.
g (c,s )=gˆ (h (c,s ,s )).
Byconstruction,Algorithm1ensuresthatforeachc∈Cthere 1 1 1 c,s1 1 2
existsanundirectedpaththatismadeupofadirectedpathfrom
For i ∈ {1,...,d } and (j ∈ {1,...,d }), taking partial
c to the masked variable x and a directed path from c to the v1 s2
m derivativeofthei-thdimensionofbothsidesw.r.t.s :
unmasked variable x mc and no other c′ ∈ C sits on this entire 2,j
undirectedpath. Toseethis,theremustexistadirectedpathfrom ∂g (c,s ) ∂gˆ (h (c,s ,s ))
c to x m without any other c′ ∈ C on it, otherwise c would not 0= 1 ∂,i s 2,j 1 = 1,i c ∂,s s1 2,j 1 2 .
beplacedinC inAlgorithm1. Inaddition, thepruningstageof
Algorithm1mandatesthattheremustexistx mcsuchthatthepath Theequationequalszerobecausethereisnos 2,j intheleft-hand
from c to x mc does not contain other c′ ∈ C. We note that c sideoftheequation. Expandingthederivativeontheright-hand
chosenbyAlgorithm1isthevariablewiththesmallestpossible sidegives:
dimensiontoblocksuchapath,asitresidesonthehighestlevel
comparedtoothervariablesonthepathandthevariabledimension (cid:88) ∂gˆ 1,i · ∂h (c,s1),k(c,s ,s )=0 (9)
∂h ∂s 1 2
increasesmonotonicallyalongdirectedpaths.
k∈{1,...,dc+ds1}
(c,s1),k 2,j
Therefore,thechoiceofeachcisminimal,andsuchachoice
isunique. AsS m isthesetofexogenousvariablesnecessaryfor For (cˆ,ˆs 1) ∈ C ×S \E 1 where E 1 denotes some subset with
C torestoreX m, theselectionofS m isalsounique. Hence, we zero measure, there are at least d c +d s1 values of i for which
c co hn oc iclu ed ae ndth ia st uth ne iq( uC e, .S m)returnedbyAlgorithm1istheminimal vectors[ ∂h∂ (cgˆ ,1 s, 1i ),1(cˆ,ˆs 1),..., ∂h(c,s∂ 1gˆ )1 ,d,i c+ds1(cˆ,ˆs 1)]arelinearly
independent,whichisequivalenttothenon-singularJacobianma-
trixcondition.Therefore,the(d +d )×(d +d )linearsystem
c s1 c s1
isinvertibleandthesolutionstatesthat:
B.Identifiabilityproof ∂h
(c,s1),k(c,s ,s )=0,
∂s 1 2
2,j
Inthissection, wepresenttheproofforTheorem2. Wefirst
giveageneralidentifiabilitytheory(i.e.,Theorem3)forthegen- foranyk ∈ {1,...,d +d },j ∈ {1,...,d },and(cˆ,ˆs ) ∈
c s1 s2 1
erating process in Figure 3 and then make the connection to the C×S\E .Therefore,wehaveshownthath ,i.e.(cˆ,ˆs ),does
1 c,s1 1
proofofTheorem2. notdependons .
2
Applyingthesamereasoningtoh ,wecanobtainthath ,
c,s2 c,s2
Theorem3. ThegeneratingprocessinFigure3isdefinedasfol- i.e.(cˆ,ˆs )doesnotdependons onC×S.
2 1
Thus,for(cˆ,ˆs ,ˆs )∈C×S×S,wecanobservethatcˆdoes positionalembeddingsandthenusestheprocessedembeddingsto
1 2
notdependons ands ,thatis,cˆ=h (c). feedintotransformerblocks.Fordecoding,MAEfirstre-arranges
1 2 c
Notice that in all procedures above, the roles of the the encoded embeddings from the visible patches according to
true quantities (c,s ,s ,g,g ,g ) and the estimated quantities theircorrespondingpositionsintheoriginalimageandthenuses
1 2 1 2
(cˆ,ˆs ,ˆs ,gˆ,gˆ ,gˆ )aresymmetric. Therefore,wecanswitchthe asharedlearnedmasktokentofillinthepatchesthataremasked.
1 2 1 2
two sets of quantities and derive the relation: for (c,s ,s ) ∈ Essentially,thismeanstheinputofthedecoderisacombinationof
1 2
(C×S×S),cdoesnotdependonˆs andˆs ,thatis,c=h′(cˆ). encodedvisiblepatchesandthemasktokens,wherethepositions
1 2 c
Insum,wehaveshownthaton(C×S×S),thereisaone-to- ofthemasktokensarethemaskedpatchesintheoriginalimage.
onemappingbetweencandcˆ. The decoder is anotherlightweight ViT, and it processes thede-
coder input through transformer blocks. Lastly, the last layer of
thedecoderlinearlyprojectsoutputpatchestopixels,andthepixel
WenowshowthatTheorem2followsdirectlyfromTheorem3. outputisreshapedtoformareconstructionoftheoriginalimage.
Theobjectivefunctionisthemeansquarederrorbetweenthere-
Theorem 2. (Identifiability of c): For each mask m, given the
constructionandtheoriginalimage. MAEhasthrivedbecauseof
dimensions (d c,d sm) the encoder function E mc(·) recovers all
itssimpledesignandstrongempiricalperformance.
informationofclocatedinTheorem1,i.e.,thereexistsaone-to-
Inthemaintext, inspiredbyafollow-upworkofMAE[28],
onemappingh,s.t.,h(c)=cˆ.
we study the effect of masking by decoupling the patch size for
maskingimagesandthepatchsizehyperparameterintheViT.Par-
Proof. We invoke Theorem 3 and establish the connection be-
ticularly,inthemaintext,weonlyvarythemaskingpatchsizeand
tweentheMAEtrainingandtheestimationmodelinTheorem3.
fixtheViTpatchsizeat8. Nevertheless, theoriginalMAE[22]
Inparticular,weshowthatunderAssumption2,anysolutionpro-
doesnotdecouplethetwopatchsizes.Therefore,forthereference
ducedbytheMAEobjectivesatisfiestheconditionsinTheorem3
ofreaders,inAppendix,weprovidesomeanalysisandresultspro-
andconsequentlyisequippedwiththeidentifiabilityguarantee.
ducedbasedonthepatchsizedesignfromtheoriginalMAE[22],
WeestablishthecorrespondencebetweentheMAEconfigura-
where the masking patch size and the ViT patch size are equal.
tionandtheestimationmodelsinTheorem3:
Westudythreepatchsizes: {8,16,32}. Theexperimentalsetup
• v ←x ;
1 m in[28]andthesetupin[22]areinterchangeableexceptforwhether
• v 2 ←x mc ; thepatchsizefortheVisionTransformervaries.
• gˆ 1 ←D m(·,ˆs m); C.2.PretrainingandLinearProbing
• gˆ
2
←g˜ mc,whereE mc(·)=[g˜ m−1 c(·)] 1:dc.
For pretraining MAE under different masking ratios or patch
WecanobservethattheminimizerofMAEsatisfiesthecondi- sizes,weleveragetheTensorProcessingUnit(TPU)fromGoogle
tionsspecifiedinTheorem3.Thisisbecausefortheoptimalsolu- Cloud. We train separate MAE models for each (masking ratio,
tionE mc oftheMAEobjective,wecanalwaysconstructag˜ mc, patchsize)pair,andeachpretrainedMAEcorrespondstoaunique
which,togetherwithD ,matchesthejointdistributionp maskingratioandpatchsize. WetrainallMAEsfor800epochs.
m xm,xmc
andsharescˆ,asstipulatedinTheorem3. Thus,asshowninThe- Trainingtimevaries,withtheshortest(patchsize=32)taking18
orem3,thereexistsaone-to-onemappingbetweentheMAEes- hoursonaTPUv3-128Pod,andthelongest(patchsize=8)tak-
timatecˆ := E mc(x mc)andthetruevariablec,whichconcludes ing40hoursonaTPUv3-128pod. Thearchitecturefollowsthe
ourproof. exactimplementationfromtheoriginalMAEpaper[22],without
anyhyper-parametertuningexceptmaskingratioandpatchsize,
C.ExperimentalSetup whichwestudyinthispaper. Detailsofaugmentation,initializa-
tion,andbaselearningratescalingcanbefoundintheAppendix
In this section, we provide the details of the experimen- sectionof[22],allofwhichwefollow.
tal setups for our empirical results. Checkpoints and some Afterpretraining,wealsofollowtheoriginalMAEworktouse
codesareinhttps://github.com/martinmamql/mae_ linear probing to evaluate the representation quality. After pre-
understand. training, we remove the projection layers and add a supervised
learningclassifieronfrozenfeaturesofMAEencoders. Thede-
C.1.MaskedAutoencoder
codersarediscardedduringlinearprobing. Otherdetailsoflinear
Masked Autoencoder (MAE) is an auto-encoding approach probingcanbefoundintheAppendixsectionof[22]. Weusethe
based on Vision Transformers (ViT) [17]. It consists of five samehyper-parametersoflinearprobingasin[22].
steps: masking,encoding,unmasking,decoding,andreconstruc-
C.3. Reconstructing high-level or low-level repre-
tion. First, an image is divided into non-overlapping patches.
sentations
ThenMAEsamplesasubsetofpatchesanddiscardstheremain-
ingpatches. MAEusesahyper-parameter, maskingratio, tode- To perform reconstruction, we use both the encoder and the
terminethepercentageofpatchestodiscard. Forinstance,ifthe decoderfromthepretrainedMAEs. AllsamplesfromImageNet-
masking ratio is 75%, 3 of the patches in an image will be dis- 1Karepassedthroughtheencoderwithoutanymasking,andthe
4
carded, and only 1 of the patches will be fed into the encoder. decoderreconstructsimagesintheoriginalinputspace. Sinceno
4
Thesamplingofpatchesfollowsauniformdistribution. Next, a maskingisapplied,nomaskingtokenisappliedtotheinputofthe
ViT encoder first embeds patches using a linear projection with decoder.Weusethereconstructedimagesandtheoriginalimages
Figure9.Reconstructionevaluationusingthevalidationsetwithoutmasking,basedontwostructural-levelsimilaritymetrics(SSIMand
FSIM)andtwopixel-levelmetrics(PSNRandMSE).WeplotnegativeMSEforeasiervisualization. HigherSSIMandFSIMindicate
high-levelinformationisbettercaptured,whilehigherPSNRandnegativeMSEindicatesbetterlow-levelreconstruction. Herethepatch
sizereferstothepatchsizeintheoriginalMAE,wherethemaskingpatchsizeandthepatchsizeofViTareequal.
to perform evaluations of four metrics: SSIM, FSIM, MSE, and beforet-SNE,asrecommendedby[56]. Fort-SNE,weuseaper-
PSNR.Notrainingisperformed,andtheweightsoftheencoder plexityof20.
andthedecoderarefrozen. InFig.10,weshowthet-SNEplotusingtheoriginalpatchsize
InFig. 9,weshowthereconstructionanalysisusingtheorigi- design in MAE. Similar to the main text, embeddings are more
nalpatchsizedesigninMAE.Similartotheresultinthemaintext, separatedinpatchsizes16and32than8,butdifferently,thereare
higherpatchsizesproduceimagereconstructionscapturinghigh- nosignificantdifferencesbetween16and32. Largerpatchsizes
levelsimilaritiesbetter,whilelowpatchsizeshavereconstructions generatemorelinearlyseparableembeddingsinthiscase,although
betteronlow-levelmetrics. theseparabilityseemsindistinguishableforsizes16and32.
For the robustness evaluation, we evaluate different vari-
C.4.AttentionAnalysis
antsofImageNetvalidationdatasets: ImageNet-v2(INV2)[52],
We follow the attention heatmap visualization in DINO [10], ImageNet-Adversarial(IN-A)[25], ImageNet-Rendition[4], and
wherethechosentokenisthe[CLS]tokenoranobject-related ImageNet-Sketch (IN-S) [59]. We also include another object
token. Wevisualizetheself-attentionmodulefromthelastblock classification dataset, ObjectNet (OJN) [4]. ImageNet-v2 con-
oftheMAEencoderViT.Brightercolorssuggestlargerattention tainsthreenewtestsetswith10,000newimageseach,sampleda
weights. Foreasiervisualization,attentionsbelowathresholdof decadeafterthecollectionoftheoriginalImageNetdataset,andis
activation scores are not shown. We use the same threshold as independentofexistingmodelstopreventoverfitting. ImageNet-
[10]. Fortheself-attentionvisualizationonthe[CLS]token,we Adversarialconsistsofnaturalimageswithadversarialfiltration,
use an average of all heads in the last layer of the encoder ViT. meaning samples that can be classified with spurious cues are
Fortheself-attentionvisualizationoftheobject-relatedtoken,we removed. Examples in ImageNet-A are harder to classify cor-
use the first head of the last layer of the encoder ViT, because rectlyandcancausemistakesacrossvariousmodels. ImageNet-
usingtheaverageattentionoverallheadswillresultinaheatmap Rendition contains renditions of ImageNet classes, such as art,
with much higher overall attention scores across pixels, making cartoons, graffiti, andpaintings. Theseexamplessharethesame
thevisualizationhardtointerpret. high-levelobjectlabelsasImageNetexamplesbutdifferinstyle
andtexture.ImageNet-Sketchcontainsblackandwhiteimagesof
ImageNetclasses,alsodifferingincolorandtexturecomparedto
originalImageNetsamples.ObjectNetisasetofimagescaptured
atunusualposesincluttered, naturalscenes, whichcanseverely
degraderecognitionperformance.
Note that for evaluating these datasets, no training is per-
formed;weusetheMAEencodersafterlinearprobings,therefore
thecheckpointsthatarepretrainedandlinear-probedonImageNet,
andevaluatethecheckpointsonthesevalidationdatasetswithout
anyparameterupdates.
InTable4,weshowtherobustnessanalysisusingtheoriginal
Figure 10. T-SNE embeddings of different MAE models under
patch size design in MAE. A moderate patch size 16 yields the
variedmaskingratiosandpatchsizes. Wefixthemaskingratioat
bestrobustnessevaluationonIN-v2,OJN,IN-R,andIN-S.Ifwe
0.75tochangepatchsizes. EachcolorrepresentsoneImageNet
followtheoriginalMAEanddonotdecouplemaskingpatchsize
class. ThepatchsizereferstothepatchsizeintheoriginalMAE,
andViTpatchsize,amediumpatchsizehasstrongerrobustness
wherethemaskingpatchsizeandthepatchsizeofViTareequal.
performancesthanextremepatchsizes.
C.6.Shapebias
C.5.Linearseparability
The cue-conflict dataset was introduced by [19] to evaluate
To illustrate the linear separability of different MAEs under how much deep learning models rely on shape information for
variedmaskingratiosorpatchsizes,wesampletenrandomclasses prediction,whichreflectsthemodel’srobustnesstospuriouscor-
fromImageNet, andthenuseeachMAEencodertoprocessim- relation like textures. This dataset consists of 1280 images syn-
ages in the 10 classes to produce embeddings. We then project thesized from 160 images of objects and 48 images of textures.
embeddings of all samples using PCA to a 50-dimension space The shape accuracy is measured by the fraction of images pre-
maskratio patchsize IN1K IN-v2 OJN IN-R IN-A IN-S
0.75 8 62.57 49.17 13.44 19.42 3.73 10.73
0.75 16 67.41 54.23 18.24 25.20 3.76 15.51
0.75 32 55.51 42.35 13.46 18.70 1.89 9.48
Table4. Accuracy(%)oflinearprobingandrobustnessevalu-
ationonImageNetvariantsandObjectNet.WelinearprobeMAE
via supervised training on IN1K, and then perform inference on
IN1Kaswellasotherevaluationsets. Wefixthemaskingratioat
0.75tochangepatchsizes. Thepatchsizereferstothepatchsize
intheoriginalMAE,wherethemaskingpatchsizeandthepatch
sizeofViTareequal.
maskratio patchsize APbox APmask
0.75 8 34.21 32.28
0.75 16 33.77 32.04
0.75 32 32.39 30.54
Table5. COCOobjectdetectionandsegmentationusingaViT
MaskR-CNNbaseline.Wefixthemaskingratioat0.75tochange
patchsizes. Thepatchsizereferstothepatchsizeintheoriginal
MAE,wherethemaskingpatchsizeandthepatchsizeofViTare
equal.
dicted correctly by their shape. We directly run the pretrained
MAE models with linear probes trained on ImageNet-1K on the
cue-conflict dataset to examine the representation resulting from
MAEpretrainingwithoutanyadaptationtothetestdataset.
C.7.Transferlearning
WeusethepretrainedMAEViTencoderasanFPN[42]back-
boneinMask-RCNN[24],following[22]. Todoso,[22]usesa
stackofpretrainedtransformerblocksinMAEtoproducefeature
maps at a single scale; for instance, patch size 16 will produce
stride16features. Thenthefeaturesareequallydivided,andup-
samplingordownsamplingisappliedtocreatefeaturesatdiffer-
entscales. Lastly,theFPNisbuiltonmulti-scalefeatures. Below
weincludethetransferlearningresultsofdifferentpatchsizeson
COCOobjectdetectionandsegmentation[43]. Becausedifferent
patchsizesinViTwillinfluencethescaleoffeaturemapsinthe
FPN, we enforce the same combinations of multi-scale features:
i.e.,stride4,8,16,and32.
From Table 5, we show the transfer learning results of MAE
under different patch sizes. Patch size 8 performs the best, and
patchsize16isbetterthan32. Thereasonforthebetterperfor-
mance at patch size 8 may be due to a smaller batch size used,
compared to patch size 16 and 32 (we can only fit batch size 1
for patch size 8 due to the increased number of tokens to pro-
cessbecauseofasmallerpatchsize.) Weusethesamebatchsize
for32and16,andthecomparisonbetweenthetwosupportsour
claim: an extreme masking scheme can hurt the model’s capac-
itytocapturehigh-levelinformationor,inthiscase,thesemantic
understandingofthescene.

XTREME-R: Towards More Challenging
and Nuanced Multilingual Evaluation
SebastianRuder1,NoahConstant2,JanBotha2,AdityaSiddhant2,OrhanFirat2,
JinlanFu3,PengfeiLiu4,JunjieHu4,DanGarrette2,GrahamNeubig4,MelvinJohnson2
1DeepMind 2GoogleResearch 3FudanUniversity 4CarnegieMellonUniversity
Abstract XTREME XTREME-R
#oflanguages 40 50
Machine learning has brought striking ad- #oftasks 9 −2+3=10
vances in multilingual natural language pro- Taskcategories Classification,structured +language-agnostic
cessingcapabilitiesoverthepastyear. Forex- prediction,QA,retrieval retrieval
ample,thelatesttechniqueshaveimprovedthe Analysistools — MULTICHECKLIST,
Explainaboard
state-of-the-art performance on the XTREME
Leaderboard Static Interactive,+metadata
multilingual benchmark by more than 13
points. While a sizeable gap to human-
Table1: OverviewofXTREMEandXTREME-R.
level performance remains, improvements
have been easier to achieve in some tasks
than in others. This paper analyzes the cur-
havebeenintroduced,consolidatingexistingmul-
rent state of cross-lingual transfer learning
tilingual tasks and covering tens of languages.
and summarizes some lessons learned. In
order to catalyze meaningful progress, we WhenXTREMEwasreleased,thegapbetweenthe
extend XTREME to XTREME-R, which con- best-performingbaseline,XLM-RLarge(Conneau
sists of an improved set of ten natural lan- et al., 2020), and human-level performance was
guageunderstandingtasks,includingchalleng- roughly25. Thishassinceshrunktolessthan12
inglanguage-agnosticretrievaltasks,andcov-
points,amuchsmallerbutstillsubstantialgapcom-
ers 50 typologically diverse languages. In ad-
pared to the difference from human-level perfor-
dition,weprovideamassivelymultilingualdi-
manceobservedinEnglishtransferlearning(Wang
agnostic suite (MULTICHECKLIST) and fine-
etal.,2019a),whichhasrecentlybeencloseden-
grained multi-dataset evaluation capabilities
through an interactive public leaderboard to tirelyonsomeevaluationsuites(Heetal.,2021).
gainabetterunderstandingofsuchmodels. Inordertoexaminethenatureofthisprogress,
wefirstperformananalysisofstate-of-the-artmul-
1 Introduction
tilingual models on XTREME. We observe that
Most research in natural language processing progresshasnotbeenuniform,butconcentratedon
(NLP)todatehasfocusedondevelopingmethods cross-lingualretrievaltaskswherefine-tuningon
thatworkwellforEnglishandasmallsetofother othertasksandpre-trainingwithparalleldatalead
high-resource languages (Joshi et al., 2020). In to large gains. On other task categories improve-
contrast,methodsforotherlanguagescanbevastly ments are more modest. Models still generally
morebeneficialastheyenableaccesstolanguage performpoorlyonlanguageswithlimiteddataand
technologyformorethanthreebillionspeakersof non-Latinscripts. Fine-tuningonadditionaltrans-
low-resourcelanguagesandpreventtheNLPcom- lateddatagenerallyleadstothebestperformance.
munityfromoverfittingtoEnglish. Motivatedby Basedonthisanalysis,wepropose XTREME-R
these benefits, the area of multilingual NLP has (XTREME Revisited), a new benchmark with the
attractedincreasinginterestrecently. dualpurposeofensuringthatresearchinmultilin-
However,evaluatingmultilingualmodelsischal- gual NLP focuses on the most challenging prob-
lengingasitrequiresassessingperformanceona lemsandequippingresearcherswithabroaderset
wide range of typologically distinct languages in oftoolstobetterunderstandtheirmodels(seeTa-
thefaceoflimitedheterogeneousdatasources. Re- ble 1 for a brief overview). XTREME-R follows
cently large-scale benchmarks such as XTREME in its predecessor’s footsteps by being massively
(Hu et al., 2020) and XGLUE (Liang et al., 2020) multilingual,diverse,andaccessible. Itexpandson
XTREME bycovering50typologicallydiverselan- 2019b)andSuperGLUE(Wangetal.,2019a)pro-
guagesand10challenging,diversetasks. Tomake videawaytoassessthetransferlearningcapabili-
retrievalmoredifficult,weintroducetwonewtasks tiesofvariousmodels. However,thesebenchmarks
that focus on “language-agnostic” retrieval (Roy focus only on English. On the other hand, cross-
etal.,2020),wheretargetsmustberetrievedfrom lingualapproacheshavebeenevaluatedonawide
alargemultilingualcandidatepool. Weaddition- rangeofdisparatetasks(Huetal.,2020). XTREME
allyestablishnewstate-of-the-artmT5(Xueetal., wasproposedasaplatformtounifythisfragmented
2021)andtranslate-trainbaselinesforourtasks. evaluation landscape and to catalyze advances in
XTREME-R aims to move away from a single cross-linguallearningbyincludingadiversesetof
aggregate metric summarizing a model’s perfor- tasks and languages. It consists of 9 tasks cover-
manceandtowardsamorenuancedevaluationand ing 40 diverse languages, which can be grouped
comparisonofmultilingualmodels(Ethayarajhand into4broadtasktypes(see§3.1fordetails): clas-
Jurafsky,2020;Linzen,2020). Tothisend,wein- sification(XNLI,PAWS-X),structuredprediction
troduceanextensiblemultilingualdiagnosticand (UD-POS, WikiANN-NER), question answering
evaluationsuitethatconsistsoftwomaincompo- (XQuAD,MLQA,TyDiQA-GoldP),andretrieval
nents: a)MULTICHECKLIST,atestsuite(Ribeiro (Tatoeba, BUCC). XTREME focuses on zero-shot
etal.,2020)forprobingquestionansweringcapa- cross-lingual transfer, i.e. models can be pre-
bilitiesin50languages. Thistestsuiteisthefirst trainedonanymultilingualdataandarefine-tuned
of its kind and enables direct evaluation of fine- only in English. Similarly, XGLUE (Liang et al.,
grainedcapabilitiesinamassivelymultilingualset- 2020), another cross-lingual benchmark focuses
ting. b) We extend the multi-dataset evaluation onasmallernumberoflesstypologicallydiverse
frameworkEXPLAINABOARD(Fuetal.,2020;Liu languages, and includes generation tasks. Other
etal.,2021)toadditionaltasksandthemultilingual non-Englishbenchmarksfocusonspecificlinguis-
setting. Thisframeworkallowsustobreakdown ticphenomena,e.g.code-switching(Khanujaetal.,
performancebasedonlanguageandtask-specific 2020); languages, e.g. Indonesian (Willie et al.,
attributes,whichenablesamorenuanceddiagnosis 2020)andPersian(Khashabietal.,2020);andlan-
ofamodel’sbehaviour. guage families, e.g. Indian languages (Kakwani
Wealsomakeseverallogisticimprovementsto etal.,2020).
improveXTREME-R’sutilityasaleaderboard. To
2.2 AnAnalysisofXTREME
make it easier to choose the best model for a use
case,eachsubmissionisrequiredtoprovidemeta- As of April 15, 2021, all submissions to the
datasuchasthenumberofparametersandamount XTREME leaderboard are large-scale Transform-
ofpre-trainingdata,whichwemakeavailablevia ers(Vaswanietal.,2017)trainedwithmaskedlan-
aninteractiveleaderboard. Wealsointroducetask guage modeling (MLM; see Appendix A for fur-
and language-specific sub-leaderboards to invite therdetails). Weanalyzetheperformanceofthese
submissionsofdedicatedmodels. modelsontheXTREMEleaderboardinFigure1.1
Insum,wemakethefollowingcontributions: a) Overall,multilingualmodelshaveimprovedtheav-
ananalysisofprogressincross-lingualmodeling; erageperformanceonXTREMEfrom55.8to81.4.
b)animprovedbenchmarkcovering50languages, Much of this improvement is concentrated in the
including a newly created retrieval task (Mewsli- retrieval-basedtaskswhereperformanceincreased
X);c)amassivelymultilingualdiagnosticsuite;d) from47.7(mBERT)to92.7(VECO). Incontrast,
fine-grainedevaluationcapabilities;e)experiments performanceonquestionansweringandstructured
andanalysesofstate-of-the-artmodels;andf)an predictiontaskshasimprovedonlyslightly.
interactivemetadata-richleaderboard. Breakingdownperformancebylanguagefamily,
onTatoeba(Figure1c)recentmodelsstillstruggle
2 ExaminingtheStateofMultilingual with a few low-resource languages. Models per-
Benchmarking formwellformostotherlanguagesandtheirscores
are concentrated in a relatively small range. On
2.1 Background
MLQA(Figure1b),scoreshaveincreasedslightly
Benchmarking is critical to evaluate general-
1This evaluation compares “models + data” as models
purposelanguageunderstandingtechnologies. To
employdifferenttypesofdataduringtraining.Wedocument
this end, benchmarks like GLUE (Wang et al., thisinformationinAppendixH.
(a)PerformanceonXTREME (b)PerformanceonMLQA
(c)PerformanceonTatoeba (d)PerformanceonUD-POS
Figure1: Performanceofmodels(a)ontheXTREMEleaderboardacrossallnineXTREMEtasks,(b)ontheMLQA
question answering dataset, (c) on the Tatoeba retrieval task, and (d) on Universal Dependencies POS tagging
across language families. Models are ordered based on their XTREME score (a). Results of models that do not
evaluateonataskcategoryareomitted,i.e. RemBERTforretrievalandmT5forretrievalandtagging.
butremainwellbelowperformanceonEnglish. On ilartothedownstreamsettingbutdoesnotsignifi-
POStagging(Figure1d),scoresremainlargelythe cantlyimproveperformanceonothertasks. Fine-
same; performance is lower for some languages tuning on automatically translated task-specific
withnon-Latinscriptsandlow-resourcelanguages. data yields strong gains and is used by most re-
We show the scores for the remaining tasks in centmodelstoachievethebestperformance(Hu
Appendix B. The remaining gap to English per- etal.,2020;Ouyangetal.,2020;Luoetal.,2020).
formanceonthesetasksispartiallyanartefactof Nevertheless,keychallengessuchashowtolearn
theevaluationsetup: zero-shotcross-lingualtrans- robustcross-lingualsyntacticandsemanticprocess-
fer from English favors English representations ingcapabilitiesduringpre-trainingremain.
whereasmodelsfine-tunedonin-languagemonolin-
gualdataperformmoresimilarlyacrosslanguages 3 XTREME-R
(Clarketal.,2020;Huetal.,2020).
InordertoencouragetheNLPcommunitytotackle
Overall,representationsfromtoken-levelMLM challenging research directions in pursuit of bet-
pre-trainingareoflimiteduseforcross-lingualsen- ter cross-lingual model generalization, we pro-
tenceretrieval,asevidencedbythecomparatively pose XTREME-R(XTREME Revisited). XTREME-R
poorperformanceofthemBERTandXLM-Rmod- sharesitspredecessor’scoredesignprinciplesfor
els. Fine-tuning on sentence-level tasks (Phang creatinganaccessiblebenchmarktoevaluatecross-
et al., 2020; Fang et al., 2021) can mitigate this. lingualtransferbutmakessomekeychanges.
Thestrongperformanceofrecentmodelssuchas First,XTREME-Rfocusesonthetasksthathave
VECO and ERNIE-M on the retrieval tasks can proventobehardestforcurrentmultilingualmod-
beattributedtoacombinationofparalleldataand els. Tothisend,itdrops XTREME’sPAWS-Xand
newpre-trainingobjectivesthatmakeuseofit. Pre- BUCC tasks since recent advances have left less
trainingonparalleldataimprovesperformanceon roomforfurtherimprovement,andtheycoveronly
retrievalbymakingthepre-trainingtaskmoresim- a small number of less diverse languages. They
Taskcategory Task |Train| |Dev| |Test| Testsets |Lang.| Task Metric Domain
XNLI 392,702 2,490 5,010 translations 15 NLI Accuracy Misc.
Classification
XCOPA 33,410+400 100 500 translations 11 Reasoning Accuracy Misc.
UD-POS 21,253 3,974 47-20,436 ind.annot. 37(104) POS F1 Misc.
Struct.prediction
WikiANN-NER 20,000 10,000 1,000-10,000 ind.annot. 47(176) NER F1 Wikipedia
XQuAD 1,190 translations 11 Spanextraction F1/EM Wikipedia
87,599 10,570
QA MLQA 4,517–11,590 translations 7 Spanextraction F1/EM Wikipedia
TyDiQA-GoldP 3,696 634 323–2,719 ind.annot. 9 Spanextraction F1/EM Wikipedia
Tatoeba 87,599 10,570 1,000 translations 38(122) Sentenceretrieval Accuracy Misc.
Retrieval Mewsli-X 116,903 10,252 428–1,482 ind.annot. 11(50) Lang.agn.retrieval mAP@20 News
LAReQAXQuAD-R 87,599 10,570 1,190 translations 11 Lang.agn.retrieval mAP@20 Wikipedia
Table2: ThetasksinXTREME-R. Fortasksthathavetraininganddevsetsinotherlanguages,weonlyreportthe
Englishnumbers. Wereportthenumberoftestexamplespertargetlanguage,thenatureofthetestsets(whether
theyaretranslationsofEnglishdataorindependentlyannotated),andthesizeoftheintersectionwithourselected
languages(thetotalnumberoflanguagesisinbrackets). ForWikiANN-NERandUD-POS,sizesareinsentences.
XCOPAincludestrainingdataofSIQa(Sapetal.,2019)andTatoebausesSQuADv1.1dataforfine-tuning.
arereplacedinsteadbythreenew,morechalleng- apremisesentence. TheXCOPAauthorstranslated
ing tasks: one focusing on causal commonsense andre-annotatedthevalidationandtestsetsofthe
reasoning(§3.2.1)andtwofocusingonharderre- EnglishCOPA(Roemmeleetal.,2011)datasetinto
trievalscenarios(§3.2.2),asthishasbeenthetask 11 languages, which we use for evaluation. The
categorywheregainshavebeeneasiesttoachieve. EnglishCOPAtrainingsettogetherwiththeSocial
Weretain XTREME’ssevenothertasksaseachstill IQa (Sap et al., 2019) training data are used for
presentssubstantialchallengesforstate-of-the-art training. While accuracy on the English COPA
cross-lingual models (§3.1). Overall, XTREME-R recently reached 94.8% (Raffel et al., 2020), the
includes10diversetasks,summarizedinTable2. state-of-the-artonXCOPAisonlyaround70%.
Wealsomakechangestothestructuredpredic-
3.2.2 RetrievalfromaMultilingualPool
tiontasks,NERandPOS.Insteadofonlyprovid-
Manypreviousretrievalbenchmarksassumethat
ingexamplesaslistsoftokens, XTREME-R always
the entire candidate pool is in a single language.
provides the full text of an input sentence, thus
Forinstance,aFrenchquerywillbeusedtosearch
ensuring that the entire benchmark now supports
overonlyEnglishcandidates. However,practical
researchonmodelsthatoperatedirectlyfromthe
settingsoftenviolatethisassumption,e.g.thean-
rawinputstring(Clarketal.,2021). Furthermore,
swertoaquestionmaybeavailableinanynumber
XTREME-R adopts a more realistic version of the
of languages, possibly different from the query
NERtaskinwhichnogoldtokenizationisprovided
language. Modelsthatcannotcomparetheappro-
atall,meaningthatsystemswilleitherhavetouse
priatenessofretrievalresultsacrosslanguagesare
model-predicted tokens or embrace tokenization-
thusineffectiveinsuchreal-worldscenarios.
free approaches. Finally, XTREME-R provides a
XTREME-R includes two new related cross-
multilingualdiagnosticandevaluationsuite(§3.4).
lingualretrievaltasks. Thefirstseekstomeasure
3.1 RetainedTasks the extent to which cross-lingual representations
are“stronglyaligned”(Royetal.,2020),i.e.they
We retain the XNLI (Conneau et al., 2018), UD-
placethesemanticallymostrelatedtextpairs(e.g.a
POS (Nivre et al., 2018), WikiANN-NER (Pan
questionanditsanswer)closesttogetherinrepre-
et al., 2017), XQuAD (Artetxe et al., 2020a),
sentationspace,regardlessoftheirlanguageiden-
MLQA(Lewisetal.,2020),TyDiQA-GoldP(Clark
tities. Thesecondanalogouslyframesentitylink-
et al., 2020), and Tatoeba (Artetxe and Schwenk,
ingasretrievingfromamultilingualpoolofentity
2019)tasksfromXTREME(seeAppendixC).
descriptions, given an entity mention in context
(Botha et al., 2020). For both, we report perfor-
3.2 NewTasks
manceasmeanaverageprecisionat20(mAP@20).
3.2.1 MultilingualCausalReasoning
LAReQA LanguageAgnosticRetrievalQuestion
XCOPA The Cross-lingual Choice of Plausible Answering(Royetal.,2020)isasentenceretrieval
Alternatives(Pontietal.,2020)datasetasksmodels task. Eachqueryhastargetanswersinmultiplelan-
todecidewhichoftwosentencescausallyfollows guages,andmodelsareexpectedtorankallcorrect
Table 3: CHECKLIST templates and generated tests for different capabilities in English, Hebrew, Arabic, and
Bengali. Wordsincurlybrackets{...}areplaceholders;seeRibeiroetal.(2020)formoreinformation.
answersaboveallincorrectanswers,regardlessof lt,ml,mr,ms,my,nl,pa,pl,pt,qu,ro,ru,sw,ta,
language. WeusetheLAReQAXQuAD-Rdataset te, th, tl, tr, uk, ur, vi, wo, yo, zh.2 XTREME-R is
whichcontains13,090questionseachofwhichhas similarlytypologicallyandgenealogicallydiverse
11targetanswers(in11distinctlanguages)within asXTREMEwhilecoveringalargernumberoflan-
thepoolof13,014candidateanswersentences. Fol- guages(seeAppendixD).
lowingRoyetal.(2020),wefine-tunemodelson
3.4 Diagnosticandevaluationsuite
theSQuADv1.1trainset. Thefine-tunedmodelis
usedtorankthe13Kcandidatesforeachquestion. Toincreasethelanguagecoverageoflow-resource
Mewsli-X Mewsli(MultilingualEntitiesinNews, languages in XTREME-R and to enable us to sys-
linked) is an automatically extracted dataset that tematically evaluate a model’s cross-lingual gen-
requireslinkingacontextualentitymentiontoits eralizationability,weaugmentXTREME-Rwitha
entry in a language-agnostic knowledge base by massively multilingual diagnostic and evaluation
retrieving the entity’s description from a multi- suite. ChallengesetsanddiagnosticsuitesinNLP
lingual candidate pool (Botha et al., 2020). For (Wangetal.,2019a,b;BelinkovandGlass,2019)
XTREME-R,wederiveMewsli-Xasanewvariant are mostly limited to English, with a few excep-
ofMewsli-9,stilllinkingagainstWikiData(Vran- tions (Gulordava et al., 2018). As challenge sets
decˇic´ andKrötzsch,2014). Mewsli-Xfeatures15K aregenerallycreatedwithahumanintheloop,the
mentionsin11languages: givenamentionincon- main challenge for creating a large multilingual
text, the task is to retrieve the single correct tar- diagnosticsuiteistoscaletheannotationortransla-
getentitydescriptionfromacandidatepoolrang- tionefforttomanylanguagesandtodealwitheach
ingover1Mcandidatesacrossall50languagesof language’sidiosyncrasies.
XTREME-R. Fine-tuning is done on a predefined MULTICHECKLIST To address this, we build
setofEnglish-onlymention-entitypairsrandomly on the CHECKLIST (Ribeiro et al., 2020) frame-
sampledfromEnglishWikipediahyperlinks(see work,whichfacilitatescreatingparameterizedtests
AppendixEforfurtherdetails). for models. CHECKLIST enables the creation of
Forourbaselinesystemsonbothtasks,wefol- testcasesusingtemplates,whichtestforspecific
lowpreviouswork(Royetal.,2020;Bothaetal., behavioral capabilities of a model with regard to
2020)andtrainadualencoderinitializedfromthe a downstream task. Importantly, by relying on
pre-trained model weights, optimizing for an in- template-based tests, we can efficiently generate
batchsampledsoftmaxloss(Gillicketal.,2018). a large number of diverse multilingual test cases
bycreatingarelativelysmallnumberoftemplates
3.3 Languages in50languages.3 WefocusontranslatingEnglish
XTREME-R adds the following ten languages to tests,whichconsistoftemplatesandtheirfill-inval-
XTREME: HaitianCreole,CuscoQuechuan,Wolof, ues.4 Tostudythefeasibilityofcreatingmultilin-
Lithuanian, Punjabi, Gujarati, Polish, Ukrainian, gualtestcasesatscale,wetranslatetheminimum
Azerbaijani, and Romanian. In total, XTREME-R
2Thenewlanguagesarecoveredinboththenewtasksas
covers the following 50 languages (shown using
wellasinUD-POS,WikiANN-NER,andTatoeba.
theirISO639-1codesforbrevity;newlanguages 3Incontrast,translatinganexistingtestsuiteordatasetor
arebolded)belongingto14languagefamiliesand annotatingindividualexamplesfor50languageswouldbe
prohibitivelyexpensive
twoisolates: af,ar,az,bg,bn,de,el,en,es,et,eu,
4Templatescouldalternativelybecreatedbynativespeak-
fa,fi,fr,gu,he,hi,ht,hu,id,it,ja,jv,ka,kk,ko, ersineachlanguage.
functionality tests (MFT) of CHECKLIST, which glish. Whilerecentwork(Huetal.,2020;Lauscher
probeforgeneralvocabularyandtaxonomicknowl- etal.,2020;Hedderichetal.,2020)demonstrates
edgeinquestionanswering. Weinstructtranslators thebenefitsoffine-tuningonin-languagedata,we
tocreateseparatevariantsofatemplatetodisam- believe the zero-shot scenario remains the most
biguatelinguisticphenomena,suchasgenderoffill- effective way to evaluate the amount of a priori
in values, question type, semantics of properties, multilingual knowledge a pre-trained model cap-
etc. Weautomaticallyfillnamesineachlanguage tures. Duetovariationincross-lingualevaluation
basedondatafromWikidataandprogrammatically (Keung et al., 2020), we recommend researchers
consolidate different templates in each language. tousethevalidationsetofasingletargetlanguage
Weshowexamplesoftemplatesandtheteststhat fordevelopment(Artetxeetal.,2020b).
theygenerateindifferentlanguagesinTable3.
4.1 Baselines
Wehighlightstatisticsofthedatasetandtransla-
tionprocess,instructionstotranslators,andgeneral Weemployestablishedpre-trainedmultilingualand
challengesoftemplatetranslationinAppendixF. modelsusingtranslationsasbaselines.
Webelievethatparameterizedtestsareapowerful mBERT MultilingualBERT(Devlinetal.,2019)
tooltoobtaindiversediagnosticsdataforotherwise hasbeenpretrainedontheWikipediasof104lan-
resource-starvedlanguages. Weviewparticipatory guagesusingMLM.
research(∀etal.,2020)withnativespeakerstocre- XLM-R XLM-R Large (Conneau et al., 2020)
atetemplate-basedtestcasestestingforlanguage- usesthesameMLMobjectivewithalargermodel,
specificbehaviourasparticularlypromising. and was trained on a magnitude more web data
Multilingual EXPLAINABOARD The standard from100languages.
practiceinleaderboardsistoaverageperformance mT5 Multilingual T5 (Xue et al., 2021) is an
across different settings (Wang et al., 2019b,a). encoder-decodertransformerthatframesNLPtasks
While this provides discriminative power, it has ina“text-to-text”format. Itwaspre-trainedwith
limited utility for examining the relative advan- MLMonalargemultilingualwebcorpuscovering
tages of systems, the characteristics of different 101languages. WeemploythelargestmT5-XXL
datasetsandlanguages,andhowthesefactorsrelate variantwith13Bparameters.
toeachother. Toprovidemoregranularevaluation Translate-train To evaluate the impact of MT,
capabilities, weextendFuetal.(2020);Liuetal. we fine-tune mBERT on translations of English
(2021)’sEXPLAINABOARDtothetaskcategories trainingdatafromHuetal.(2020). Wecreatenew
andlanguagesinXTREME-R. EXPLAINABOARD translations for the XCOPA and SIQa data using
providesamorenuancedimpressionofamodel’s anin-houseMTsystem.5
performanceonataskbydefiningtask-specificat- Translate-trainmultilingual Inaddition,wefine-
tributes (e.g. entity length for NER). The test set tunebothmBERTandmT5onthecombinedtrans-
is partitioned into different buckets based on the latedtrainingdataofalllanguages(includingthe
definedattributesandperformanceisbrokendown originalEnglishdata)jointly.
overdifferentattributevalues. Wedefinenewtask- Humanperformance Weusethehumanperfor-
specificattributesforthefourtasktypesaswellas mance estimates from XTREME for the retained
task-independentattributes(seeAppendixK). tasks. For XCOPA we average the proportion of
Metadata We additionally would like to enable annotatedlabelsdisagreeingwiththemajorityla-
practitioners to rank submissions based on other belacrossalllanguages(Pontietal.,2020). Weare
information. Tothisend,weaskeachsubmission not able to obtain human performance estimates
to XTREME-R for relevant metadata such as the for the new retrieval tasks as identifying a trans-
numberofparameters,theamountofpre-training lationamongalargenumberofcandidatesistoo
data, etc. We will show this information in an time-consumingforahumantoperform.
interactive leaderboard (see Appendix H for the
metadataofcurrentXTREMEsubmissions). 4.2 Results
We show the main results in Table 4. As in
4 Experiments
priorwork,XLM-RLargegenerallyoutperforms
mBERT.Fine-tuninghelpssignificantlyonTatoeba
Training and evaluation setup XTREME-R fo-
cusesonzero-shotcross-lingualtransferfromEn- 5WeareunabletoproducetranslationsforQuechua.
Classification Structuredprediction Questionanswering Lang.-agnosticretrieval Retrieval
Model Avg
XNLI XCOPA UD-POS WikiANN-NER XQuAD MLQA TyDiQA-GoldP Mewsli-X LAReQA Tatoeba
Metrics Acc. Acc. F1 F1 F1/EM F1/EM F1/EM mAP@20 mAP@20 Acc.
Cross-lingualzero-shottransfer(modelsaretrainedonEnglishdata)
mBERT 54.1 66.5 56.1 70.9 62.7 65.1/50.4 61.3/44.1 58.4/43.7 38.6 21.6 43.3
XLM-RLarge 65.3 79.2 69.2 75.0 64.4 77.2/61.6 72.7/54.2 64.3/45.8 45.7 40.7 77.3
mT5-XXL 64.6 84.8 74.9 71.5 68.8 81.5/66.6 75.6/57.3 80.1/65.7 41.7* 24.7* 45.7
Translate-train(modelsaretrainedonEnglishtrainingdatatranslatedtothetargetlanguage)
mBERT - 74.6 57.3 - - 70.0/56.0 65.6/48.0 52.4/39.5 - - -
mBERT,multilingual - 75.1 57.9 - - 72.4/58.3 67.6/49.8 59.5/45.8 - - -
mT5-XXL,multilingual - 87.5 77.8 - - 84.7/71.8 76.4/58.4 81.9/68.6 - - -
Human - 92.8 97.6 97.0 - 91.2/82.3 91.2/82.3 90.1/- - - -
Table 4: Overall results of baselines across all XTREME-R tasks. *: Due to compute limitations, mT5-XXL
language-agnosticretrievalresultsareobtainedwithafrozenratherthanafine-tunedmodel.
Jobvs Animalvs Animalvs cross-lingualalignmentofmultilingualrepresenta-
Lang. Comparisons Intensifiers Properties Avg
Nationality Vehicles Vehicles2
fi 10.7 79.8 34.1 4.0 9.2 1.5 23.2 tionstothetest. Analysisofthelanguage-agnostic
fr 4.0 98.0 15.4 0.8 10.0 13.5 23.6
en 7.0 90.4 38.5 0.0 6.0 0.5 23.7 retrievalresultsshowalargegapremainsbetween
pl 20.9 100.0 20.8 0.0 11.0 3.5 26.0
bg 49.5 71.1 14.3 0.0 16.5 6.1 26.2 cross-lingualandsame-languagetestcases. XLM-
ka 0.0 95.0 34.0 12.0 13.0 12.0 27.7
nl 14.1 91.9 25.4 1.0 24.1 11.3 28.0 R Large improves significantly over mBERT on
de 17.1 90.6 38.2 2.5 17.0 9.1 29.1
it 3.5 98.5 50.0 6.7 10.5 5.6 29.1 thecross-lingualcaseinexchangeforaslightdrop
hu 6.1 98.5 33.5 14.0 28.0 10.6 31.8
hi 22.2 63.8 64.3 8.0 28.0 8.6 32.5 forthesame-languagecase. ThispointstoXLM-
ru 32.0 95.0 33.8 7.1 24.0 3.0 32.5
R Large inducing more “strongly-aligned” repre-
fa 10.6 84.2 51.7 5.0 38.5 11.6 33.6
et 11.1 91.4 36.1 14.5 49.0 0.0 33.7
sentations(seeAppendixIfordetails). Thestate-
es 9.6 99.5 62.0 0.0 32.5 5.5 34.8
ms 4.0 97.9 84.0 5.5 18.5 0.5 35.1
of-the-art mT5 improves performance on classi-
ml 7.1 73.8 74.0 6.5 32.5 19.0 35.5
lt 12.7 84.2 67.6 25.2 22.5 2.5 35.8
fication and QA tasks but performs less well on
el 6.1 98.0 38.3 18.1 45.0 16.5 37.0
af 47.0 78.7 35.5 1.0 56.0 21.2 39.9
structuredpredictionandretrieval,highlightingset-
ta 59.0 78.7 65.5 11.5 13.5 13.2 40.2
uk 16.2 94.9 39.8 26.9 51.5 14.1 40.6 tings where advances beyond scale are needed.6
pt 51.8 99.0 62.4 0.8 23.0 7.1 40.7
tl 0.0 100.0 66.5 12.5 58.5 13.6 41.8
Trainingontask-specifictranslationsisbeneficial
id 6.5 98.0 77.0 0.0 42.0 33.5 42.8
ko 18.5 98.5 34.5 9.0 42.0 60.9 43.9
inallcasesandgenerallyperformsbest,although
tr 100.0 84.4 72.5 0.5 21.0 14.1 48.8
pa 99.5 56.3 100.0 0.0 38.0 10.6 50.7
improvements on QA tasks are diminishing. To
vi 13.6 99.0 80.0 10.0 100.0 2.6 50.9
te 35.2 92.0 68.0 28.0 36.0 55.0 52.4
obtain a more fine-grained understanding of the
ar 23.9 97.5 100.0 0.0 100.0 22.6 57.3
eu 100.0 98.5 66.0 17.5 25.5 38.7 57.7
performanceofcurrentmodels,weconductseveral
bn 89.9 94.0 91.0 9.0 48.5 24.7 59.5
ur 90.9 57.5 76.1 16.5 100.0 18.7 59.9
analysesusingourmultilingualdiagnosticsuite.
my 99.0 86.0 83.0 9.5 93.5 0.0 61.8
kk 88.9 99.0 82.5 0.5 100.0 17.0 64.7
az 98.0 75.1 73.5 4.5 40.5 99.5 65.2
jv 9.0 100.0 82.0 3.5 100.0 100.0 65.8 5 Analyses
mr 0.0 83.0 82.8 100.0 45.0 86.4 66.2
gu 100.0 100.0 100.0 39.0 90.0 31.8 76.8
ja 96.0 100.0 60.5 99.0 30.0 95.5 80.2 5.1 MULTICHECKLIST
zh 94.4 100.0 33.0 100.0 94.5 71.2 82.2
sw 100.0 100.0 94.0 6.0 100.0 98.0 83.0
th 91.4 78.4 100.0 100.0 100.0 41.0 85.1 WeshowtheresultsofXLM-Rfine-tunedonEn-
he 100.0 97.5 100.0 100.0 100.0 27.9 87.6
qu 91.9 100.0 100.0 98.0 95.5 97.0 97.1 glishSQuADv1.1onthe6testsofMULTICHECK-
ht 95.5 100.0 100.0 100.0 100.0 90.9 97.7
ha 100.0 100.0 99.5 100.0 100.0 91.5 98.5 LISTinTable5(seeAppendixFforthefullresults,
yo 100.0 100.0 100.0 100.0 100.0 99.5 99.9
wo 100.0 100.0 100.0 100.0 100.0 100.0 100.0 examplefailurecases,andmBERTresults). While
Avg 47.3 90.9 64.8 26.7 51.6 32.8 52.4
mBERT’s average error rate is greater than 85%
Table 5: Error rate of XLM-R fine-tuned on English on4/6testcategories,XLM-Rdemonstratesasub-
SQuADv1.1on6CHECKLISTQAtests. stantiallymorerobustcross-lingualunderstanding
ability. XLM-R performs worst on tests in low-
resourcelanguageswithlimitedornopre-training
comparedtothezero-shotsetting(Huetal.,2020). datasuchasgu,ha,ht,qu,sw,wo,andyoand
The new tasks are challenging for current mod- inlanguageswithnon-Latinscriptssuchashe,ja,
els,whichshowrelativelylowerperformancecom-
6Duetocomputelimitations,weextractmT5embeddings
paredtoothertasks. XCOPApresentsachalleng- by averaging the encoder outputs of a frozen mT5 model
ing classification task that requires cross-lingual fine-tunedonSQuADv1.1,asopposedtofine-tuningadual
encoder.Forthisreason,themT5language-agnosticretrieval
common sense reasoning while the language-
scoresarenotdirectlycomparabletothoseofmBERTand
agnosticnatureofMewsli-XandLAReQAputsthe XLM-R.
aLen qLen cLen BLEU-AQ BLEu-QC qType
enzhhielrutrarvi enzhhielrutrarvi enzhhielrutrarvi enzhhielrutrarvi enzhhielrutrarvi enzhhielrutrarvi
90 90
M (O1 v: eE raR llN :I 7E 5- .M
46)
80 88 05 88 05 78 00 88 05 899 505
SingleSys. 70 75 75 60 75 80
70 50 75
60 65 70 40 70 70
50 40
M (O1 v: eE raR llN :I 7E 5- .M 46) 30 30 40 30 30 30
M2:XLM-R 20 20 30 20 20 20
(O Pave irr wal il s: e6 S8. y4 s5 .) 10 10 12 00 10 10 10
(M1−M2) 0 0 0 0 0 0
Table 6: Single and pairwise system diagnosis of ERNIE-M (M1) and XLM-R (M2) on XQuAD. “M1−M2”
representstheperformancedifferencebetweenM1andM2. Weaverage“F1”and“ExactMatch”ofQAsystems.
We classify the attribute values into four categories: extra-small (XS), small (S), large (L) and extra-large (XL)
values(seetheAppendixfordetailedintervalinformation). Thetop5mostfrequentquestiontypes(qType)are:
“what” (A), “how” (B), “who” (C), “when” (D) and “which” (E). In the single system diagnosis histogram, blue
(red) x ticklabels represent the bucket category (e.g., XS) of a specific attribute on which a system achieved the
best(worst)performance. Inthepairwisesystemdiagnosishistogram,blue(red)xticklabelsrepresentthebucket
valueofaspecificattributewheresystemM1surpasses(under-performs)M2bythelargestmarginillustratedby
ablue(red)bin. Blue-onlyxticklabels(e.g.,-D)indicatethatM1outperformsM2inallcategoriesofanattribute.
th,andzh. Inaddition,XLM-Rdisplaysinterest- swers is more than 40 absolute points. The influ-
ingvariationacrosslanguages,forinstancefailing ence of question and context length is language-
inmodelingcomparisonsinsomelanguages,like dependent. Forexample,inzhthesystemfavors
Basque(eu),whereitotherwisesucceeds. Were- longquestionsandcontextswhileinhi, itisthe
leasethetestsandtestoutputstoencouragedeeper opposite. If the answer is lexically similar to the
analysisandextensiontoothertasksandlanguages. question (larger BLEU-AQ), the system tends to
makemoremistakesinalleightlanguages. How-
5.2 NuancedMultilingualEvaluation ever, a higher lexical overlap between questions
and contexts (BLEU-QC) is helpful for some lan-
Weshowcasehownuancedmultilingualevaluation
guages: el,ru,ar. Surprisingly,ERNIE-Mstrug-
enablesustoperformsingleandpairwisesystem
gles to answer relatively frequent question types
diagnosisonXQuADinTable6(seeAppendixK
(i.e.,what,andhow),whileitperformsbetteron
for analyses of the other tasks). We choose two
less frequent questions, indicating that although
systems: ERNIE-M, one of the top systems on
questionsaboutperson,placeandchoiceareless
XTREME,andXLM-Rineightlanguages: English,
frequent,theyareeasierthanabstractquestions.
Chinese,Hindi,Greek,Russian,Turkish,Arabic,
PairwiseSystemAnalysis AlthoughERNIE-M
andVietnamese(en,zh,hi,el,ru,tr,ar,vi).
outperforms XLM-R by a large margin, it is sur-
Attributes Wedenote(X ,X ,X )asatupleof
c q a passedbyXLM-Ronafewbuckets. Inen,XLM-
acontext,questionandanswer,andrefertocLen,
Risbetteratdealingwithlongeranswersandques-
qLen, aLen as their lengths (i.e., the number of
tions. Intr,XLM-RsurpassesERNIE-Monsam-
tokens). We use BLEU (Papineni et al., 2002)
ples with shorter answers and contexts. In zh,
tomeasurelexicaloverlapbetween(X ,X )and
a q XLM-R performs better when dealing with ques-
(X ,X )asBLEU-AQandBLEU-QC.Wereport
q c tionsthatarelexicallysimilartotheanswers.
the top 5 most frequent question types (qType),
whichcover85%ofquestionsinthetrainingset.
6 Conclusions
SingleSystemAnalysis Foralmostalllanguages,
ERNIE-M achieves the highest performance on Ouranalysesandexperimentshaveshedlighton
shorter answers (XS), but the worst performance importantdirectionswherescalealoneisnotsuffi-
onlongeranswers(XL).Especiallyinel,theper- cientsuchas“strong”alignment,syntactictransfer,
formance difference between long and short an- fine-grainednaturallanguageunderstanding,and
SX-LX
L-LX
S-LX
S-
SX-LX
LX-
SX-LX
S-LX
SX-LX
LX-
S-LX
S-SX
SX-LX
S-
S-LX
S-
S-L
S-LX
L-SX
SX-
S-LX
SX-
LX-SX
LX-
L-SX
L-
L-LX
L-
LX-L
LX-
S-L
S-
LX-S
LX-
L-LX
SX-
SX-S
LX-
SX-S
S-
SX-S
S-
LX-SX
LX-SX
L-S
L-
L-S
LX-
SX-L
S-SX
S-LX
SX-LX
SX-L
L-
SX-L
S-
SX-L
S-
SX-L
L-
SX-L
L-
SX-LX
L-
SX-L
SX-L
S-SX
SX-
SX-L
SX-
LX-L
LX-
LX-SX
LX-
SX-LX
SX-
LX-S
L-
S-SX
S-
E-B
A-B
B-E
D-
E-A
A-D
E-B
A-
D-B
A-B
C-B
A-D
D-A
E-
D-E
E-C
answeringofabstractquestions. Weencouragethe training data (Hu et al., 2020). Zero-shot cross-
developmentofbetterinductivebiases,pre-training lingual transfer additionally introduces a bias to-
objectives,andevaluationresources. Wemakeour wards the source language (Søgaard et al., 2018;
data,translations,evaluationresources,andinter- Anastasopoulos and Neubig, 2020). Due to the
activeleaderboardsupportingdetailedcomparative paucityoftrainingdatainotherlanguages,wenev-
analyses available to help the community gain a erthelessfocusonEnglish-centrictransferanden-
betterunderstandingofmultilingualmodels. courage future dataset creation efforts to include
trainingdatainmultiplelanguages.
7 EthicalConsiderations
7.4 Environmentalconcerns
7.1 Languagerepresentation
XTREME-R aims to enable efficient evaluation of
XTREME-R seekstoimprovelanguagerepresenta-
multilingual models. To this end, we created a
tionandlanguagediversityinNLPresearch,which
newdataset,Mewsli-X,thatcapturestheessence
hasbeenidentifiedasalargechallenge(Joshietal.,
of multilingual entity linking against a diverse
2020). We tried to cover a set of languages that
knowledge base but is computationally cheaper
is as diverse as possible, while still providing ac-
to evaluate than the large-scale Mewsli-9 (Botha
cess to evaluation data in multiple tasks for each
et al., 2020). Nevertheless, the models that per-
language. Despitethis, XTREME-Rhaslittlerepre-
formbestonbenchmarkslikeXTREME-Raregen-
sentationoflanguagesoftheAmericasandAfrica
erallylarge-scaleTransformermodelspre-trained
due to a lack of labeled datasets for these lan-
on large amounts of data, which comes at a high
guages. In addition, some languages included in
cost (Strubell et al., 2019). We thus particularly
XTREME-Rwithfewdataavailableonlineareonly
encourage the development of efficient methods
covered in a small number of datasets (see Table
toadaptexistingmodelstonewlanguages(Pfeif-
7). Toamelioratethis,wereleasetrainingdataof
fer et al., 2020) rather than training multilingual
tasks translated into other languages, as well as
modelsentirelyfromscratch.
thenew MULTICHECKLIST. Wereiteratetheon-
goingneedforcreatinglabeleddatasetsfordiverse Acknowledgements
tasksinunder-representedlanguages,tofacilitate
thedevelopmentandevaluationofNLPmodelsfor We thank Marco Tulio Ribeiro for advice on
suchlanguages. Weemphasizetheimportanceof CHECKLIST. We are grateful to Laura Rimell
participatoryresearch(∀etal.,2020)asamodus and Jon Clark for valuable feedback on drafts of
operandiforsuchworkinordertoinvolvemarginal- thispaper,andtoDanGillickforfeedbackonthe
izedcommunitiesintheresearchprocess. Mewsli-X dataset design. We thank Hila Gonen,
BidishaSamantha,andParthaTalukdarforadvice
7.2 Leaderboardchasing onArabic,Bengali,andHebrew CHECKLIST ex-
New benchmarks incentivize researchers to hill- amples.
climbonaggregatemetrics(EthayarajhandJuraf-
sky, 2020). In addition, new benchmarks create
References
new opportunities for models to reach “superhu-
man”performance,whichmayleadpeopleoutside Antonios Anastasopoulos and Graham Neubig. 2020.
thefieldtoerroneouslyconcludethatsomemodel Shouldallcross-lingualembeddingsspeakenglish?
InProceedingsofACL2020,pages8658–8679.
has “solved language”. We hope that our inclu-
sionofEXPLAINABOARDandMULTICHECKLIST
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.
help to prevent such a fallacy, by enabling more 2020a. On the Cross-lingual Transferability of
fine-grainedevaluationthatgoesbeyondasingle Monolingual Representations. In Proceedings of
ACL2020.
aggregatemetric.
Mikel Artetxe, Sebastian Ruder, Dani Yogatama,
7.3 Biasesinmultilingualmodels
Gorka Labaka, and Eneko Agirre. 2020b. A call
Multilingual models have been shown to reflect for more rigor in unsupervised cross-lingual learn-
ing. In Proceedings of the 58th Annual Meeting
biases similar to their monolingual counterparts
of the Association for Computational Linguistics,
(Zhaoetal.,2020). Inaddition,multilingualmod-
pages 7375–7388, Online. Association for Compu-
els are biased towards languages with more pre- tationalLinguistics.
Mikel Artetxe and Holger Schwenk. 2019. Massively KawinEthayarajhandDanJurafsky.2020. Utilityisin
Multilingual Sentence Embeddings for Zero-Shot theeyeoftheuser: AcritiqueofNLPleaderboards.
Cross-Lingual Transfer and Beyond. Transactions InProceedingsofthe2020ConferenceonEmpirical
oftheACL2019. MethodsinNaturalLanguageProcessing(EMNLP),
pages4846–4853,Online.AssociationforComputa-
PeterBakker,AymericDaval-Markussen,MikaelPark- tionalLinguistics.
vall,andIngoPlag.2011. Creolesaretypologically
distinct from non-creoles. Journal of Pidgin and YuweiFang,ShuohangWang,ZheGan,SiqiSun,and
CreoleLanguages,26(1):5–42. Jingjing Liu. 2021. FILTER: An Enhanced Fusion
Method for Cross-lingual Language Understanding.
Yonatan Belinkov and James Glass. 2019. Analysis InProceedingsofAAAI2021.
MethodsinNeuralLanguageProcessing: ASurvey.
TransactionsoftheACL. ∀, Wilhelmina Nekoto, Vukosi Marivate, Tshi-
nondiwa Matsila, Timi Fasubaa, Taiwo Fagbo-
JanA.Botha,ZifeiShan,andDanielGillick.2020. En-
hungbe, Solomon Oluwole Akinola, Shamsud-
tity Linking in 100 Languages. In Proceedings of
deenMuhammad,SalomonKabongoKabenamualu,
EMNLP2020,pages7833–7845.
Salomey Osei, Freshia Sackey, Rubungo Andre
Niyongabo, Ricky Macharm, Perez Ogayo, Ore-
Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham
vaogheneAhia, MusieMeressaBerhe, Mofetoluwa
Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao,
Adeyemi, Masabata Mokgesi-Selinga, Lawrence
HeyanHuang,andMingZhou.2021. InfoXLM:An
Okegbemi, Laura Martinus, Kolawole Tajudeen,
information-theoretic framework for cross-lingual
Kevin Degila, Kelechi Ogueji, Kathleen Siminyu,
languagemodelpre-training. InProceedingsofthe
JuliaKreutzer,JasonWebster,JamiilToureAli,Jade
2021ConferenceoftheNorthAmericanChapterof
Abbott, IroroOrife, IgnatiusEzeani, IdrisAbdulka-
the Association for Computational Linguistics: Hu-
dirDangana,HermanKamper,HadyElsahar,Good-
manLanguageTechnologies,pages3576–3588,On-
nessDuru, GhollahKioko, MurhabaziEspoir, Elan
line.AssociationforComputationalLinguistics.
van Biljon, Daniel Whitenack, Christopher Onyefu-
luchi, Chris Chinenye Emezue, Bonaventure F. P.
Hyung Won Chung, Thibault Févry, Henry Tsai,
Dossou, Blessing Sibanda, Blessing Bassey, Ay-
Melvin Johnson, and Sebastian Ruder. 2021. Re-
odele Olabiyi, Arshath Ramkilowan, Alp Öktem,
thinking Embedding Coupling in Pre-trained Lan-
Adewale Akinfaderin, and Abdallah Bashir. 2020.
guageModels. InProceedingsofICLR2021.
Participatory research for low-resourced machine
translation: A case study in African languages. In
JonathanH.Clark,EunsolChoi,MichaelCollins,Dan
Findings of the Association for Computational Lin-
Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and
guistics: EMNLP2020,Online.
Jennimaria Palomaki. 2020. TyDi QA: A Bench-
mark for Information-Seeking Question Answering
Jinlan Fu, Pengfei Liu, Graham Neubig, and Mo Tab.
in Typologically Diverse Languages. In Transac-
2020. Interpretable Multi-dataset Evaluation for
tions of the Association of Computational Linguis-
Named Entity Recognition. In Proceedings of
tics.
EMNLP2020,pages6058–6069.
Jonathan H Clark, Dan Garrette, Iulia Turc, and John
Daniel Gillick, Alessandro Presta, and Gaurav Singh
Wieting. 2021. CANINE: Pre-training an Efficient
Tomar. 2018. End-to-end retrieval in continuous
Tokenization-Free Encoder for Language Represen-
tation. arXivpreprintarXiv:2103.06874. space. arXivpreprintarXiv:1811.08008.
AlexisConneau, KartikayKhandelwal, NamanGoyal, KristinaGulordava,PiotrBojanowski,EdouardGrave,
Vishrav Chaudhary, Guillaume Wenzek, Francisco Tal Linzen, and Marco Baroni. 2018. Colorless
Guzmán, Edouard Grave, Myle Ott, Luke Zettle- green recurrent networks dream hierarchically. In
moyer, and Veselin Stoyanov. 2020. Unsupervised ProceedingsofNAACL-HLT2018.
Cross-lingual Representation Learning at Scale. In
ProceedingsofACL2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Weizhu Chen. 2021. DeBERTa: Decoding-
Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad- enhanced BERT with Disentangled Attention. In
ina Williams, Samuel Bowman, Holger Schwenk, ProceedingsofICLR2021.
and Veselin Stoyanov. 2018. XNLI: Evaluating
cross-lingual sentence representations. In Proceed- MichaelA.Hedderich,DavidAdelani,DaweiZhu,Je-
ingsofEMNLP2018,pages2475–2485. sujoba Alabi, Udia Markus, and Dietrich Klakow.
2020. Transfer Learning and Distant Supervision
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and for Multilingual Transformer Models: A Study on
Kristina Toutanova. 2019. BERT: Pre-training of African Languages. In Proceedings of EMNLP
deep bidirectional transformers for language under- 2020.
standing. In Proceedings of NAACL 2019, pages
4171–4186, Minneapolis, Minnesota. Association Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
forComputationalLinguistics. ham Neubig, Orhan Firat, and Melvin Johnson.
2020. XTREME: A Massively Multilingual Multi- Proceedingsofthe58thAnnualMeetingoftheAsso-
taskBenchmarkforEvaluatingCross-lingualGener- ciation for Computational Linguistics, pages 7315–
alization. InProceedingsofICML2020. 7330, Online. Association for Computational Lin-
guistics.
ZhengbaoJiang, AntoniosAnastasopoulos, JunAraki,
YaoboLiang,NanDuan,YeyunGong,NingWu,Fen-
HaiboDing,andGrahamNeubig.2020. X-FACTR:
fei Guo, Weizhen Qi, Ming Gong, Linjun Shou,
Multilingual factual knowledge retrieval from pre-
Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei
trained language models. In Proceedings of the
Zhang, Rahul Agrawal, Edward Cui, Sining Wei,
2020 Conference on Empirical Methods in Natural
TaroonBharti,YingQiao,Jiun-HungChen,Winnie
Language Processing (EMNLP), pages 5943–5959,
Wu,ShuguangLiu,FanYang,DanielCampos,Ran-
Online.AssociationforComputationalLinguistics.
gan Majumder, and Ming Zhou. 2020. XGLUE: A
newbenchmarkdatasetforcross-lingualpre-training,
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
understandingandgeneration. InProceedingsofthe
Bali, andMonojitChoudhury.2020. TheStateand
2020 Conference on Empirical Methods in Natural
Fate of Linguistic Diversity and Inclusion in the
Language Processing (EMNLP), pages 6008–6018,
NLPWorld. InProceedingsofACL2020.
Online.AssociationforComputationalLinguistics.
Divyanshu Kakwani, Anoop Kunchukuttan, Satish TalLinzen.2020. Howcanweaccelerateprogressto-
Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. wardshuman-likelinguisticgeneralization? InPro-
Khapra,andPratyushKumar.2020. IndicNLPSuite: ceedings of the 58th Annual Meeting of the Asso-
Monolingual corpora, evaluation benchmarks and ciation for Computational Linguistics, pages 5210–
pre-trainedmultilinguallanguagemodelsforIndian 5217, Online. Association for Computational Lin-
languages. InFindingsofthe Association forCom- guistics.
putational Linguistics: EMNLP 2020, pages 4948–
4961, Online. Association for Computational Lin- PatrickLittell,DavidRMortensen,KeLin,Katherine
guistics. Kairis,CarlisleTurner,andLoriLevin.2017. Uriel
and lang2vec: Representing languages as typologi-
Phillip Keung, Yichao Lu, Julian Salazar, and Vikas cal,geographical,andphylogeneticvectors. InPro-
ceedings of the 15th Conference of the European
Bhardwaj. 2020. Don’t use English dev: On the
Chapter of the Association for Computational Lin-
zero-shotcross-lingualevaluationofcontextualem-
beddings. InProceedingsofthe2020Conferenceon guistics: Volume2,ShortPapers,pages8–14.
EmpiricalMethodsinNaturalLanguageProcessing
Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan,
(EMNLP), pages 549–554, Online. Association for
Shuaichen Chang, Junqi Dai, Yixin Liu, Zihuiwen
ComputationalLinguistics.
Ye,andGrahamNeubig.2021. ExplainaBoard: An
explainableleaderboardforNLP. InProceedingsof
Simran Khanuja, Sandipan Dandapat, and Anirudh
the59thAnnualMeetingoftheAssociationforCom-
Srinivasan. 2020. GLUECoS : An Evaluation
putational Linguistics and the 11th International
Benchmark for Code-Switched NLP. In Proceed-
Joint Conference on Natural Language Processing:
ingsofACL2020,pages3575–3585.
SystemDemonstrations,pages280–289,Online.As-
sociationforComputationalLinguistics.
Daniel Khashabi, Arman Cohan, Siamak Shakeri,
Pedram Hosseini, Pouya Pezeshkpour, Malihe YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
Alikhani, Moin Aminnaseri, Marzieh Bitaab, dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Faeze Brahman, Sarik Ghazarian, Mozhdeh Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Gheini, Arman Kabiri, Rabeeh Karimi Mahabadi, Roberta: A robustly optimized bert pretraining ap-
Omid Memarrast, Ahmadreza Mosallanezhad, proach. arXivpreprintarXiv:1907.11692.
Erfan Noury, Shahab Raji, Mohammad Sadegh
Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi,
Rasooli, Sepideh Sadeghi, Erfan Sadeqi Azer,
Songfang Huang, Fei Huang, and Luo Si. 2020.
Niloofar Safi Samghabadi, Mahsa Shafaei, Saber
Veco: Variable encoder-decoder pre-training for
Sheybani, Ali Tazarv, and Yadollah Yaghoobzadeh.
cross-lingual understanding and generation. arXiv
2020. ParsiNLU: A Suite of Language Under-
preprintarXiv:2010.16046.
standing Challenges for Persian. arXiv preprint
arXiv:2012.06154.
Joakim Nivre, Mitchell Abrams, Željko Agic´, Lars
Ahrenberg, Lene Antonsen, Maria Jesus Aranzabe,
Anne Lauscher, Vinit Ravishankar, Ivan Vulic´, and
Gashaw Arutie, Masayuki Asahara, Luma Ateyah,
Goran Glavaš. 2020. From Zero to Hero: On
MohammedAttia,etal.2018. Universaldependen-
the Limitations of Zero-Shot Cross-Lingual Trans-
cies2.2.
ferwithMultilingualTransformers. InProceedings
ofEMNLP2020. Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,
Hao Tian, Hua Wu, and Haifeng Wang. 2020.
Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian ERNIE-M: Enhanced Multilingual Representation
Riedel,andHolgerSchwenk.2020. MLQA:Evalu- byAligningCross-lingualSemanticswithMonolin-
atingcross-lingualextractivequestionanswering. In gualCorpora. arXivpreprintarXiv:2012.15674.
Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Uma Roy, Noah Constant, Rami Al-Rfou, Aditya
Nothman,KevinKnight,andHengJi.2017. Cross- Barua, Aaron Phillips, and Yinfei Yang. 2020.
lingualnametaggingandlinkingfor282languages. LAReQA:Language-agnosticanswerretrievalfrom
InProceedingsofACL2017,pages1946–1958. amultilingualpool. InProceedingsofthe2020Con-
ferenceonEmpiricalMethodsinNaturalLanguage
KishorePapineni,SalimRoukos,ToddWard,andWei- Processing(EMNLP),pages5919–5930,Online.As-
JingZhu.2002. Bleu: amethodforautomaticeval- sociationforComputationalLinguistics.
uation of machine translation. In Proceedings of
the40thAnnualMeetingoftheAssociationforCom- MaartenSap,HannahRashkin,DerekChen,RonanLe
putationalLinguistics,pages311–318,Philadelphia, Bras, Yejin Choi, Artificial Intelligence, and Com-
puter Science. 2019. Social IQa: Commonsense
Pennsylvania,USA.AssociationforComputational
Reasoning about Social Interactions Maarten Sap.
Linguistics.
InProceedingsofEMNLP2019,pages4453–4463.
Jonas Pfeiffer, Ivan Vulic´, Iryna Gurevych, and Se-
Anders Søgaard, Sebastian Ruder, and Ivan Vulic´.
bastian Ruder. 2020. MAD-X: An Adapter-based
2018. OntheLimitationsofUnsupervisedBilingual
FrameworkforMulti-taskCross-lingualTransfer. In
DictionaryInduction. InProceedingsofACL2018.
ProceedingsofEMNLP2020.
Emma Strubell, Ananya Ganesh, and Andrew McCal-
Jason Phang, Iacer Calixto, Phu Mon Htut, Yada lum. 2019. Energy and Policy Considerations for
Pruksachatkun, Haokun Liu, Clara Vania, Katha- Deep Learning in NLP. In Proceedings of ACL
rinaKann, andSamuelR.Bowman.2020. English 2019.
intermediate-tasktrainingimproveszero-shotcross-
lingual transfer too. In Proceedings of the 1st Con- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
ference of the Asia-Pacific Chapter of the Associa- Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
tion for Computational Linguistics and the 10th In- Kaiser,andIlliaPolosukhin.2017. AttentionIsAll
ternational Joint Conference on Natural Language YouNeed. InProceedingsofNIPS2017.
Processing,pages557–575,Suzhou,China.Associ-
ationforComputationalLinguistics. Denny Vrandecˇic´ and Markus Krötzsch. 2014. Wiki-
data: a free collaborative knowledgebase. Commu-
nicationsoftheACM,57(10):78–85.
Edoardo Maria Ponti, Goran Glavaš, Olga Majewska,
QianchuLiu,IvanVulic´,andAnnaKorhonen.2020.
AlexWang,JulianMichael,FelixHill,OmerLevy,and
XCOPA:Amultilingualdatasetforcausalcommon-
SamuelRBowman.2019a. SuperGLUE:AStickier
sense reasoning. In Proceedings of the 2020 Con-
Benchmark for General-Purpose Language Under-
ferenceonEmpiricalMethodsinNaturalLanguage
standingSystems. InProceedingsofNeurIPS2019.
Processing(EMNLP),pages2362–2376,Online.As-
sociationforComputationalLinguistics. Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2019b.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather- GLUE:AMulti-TaskBenchmarkandAnalysisPlat-
ine Lee, Sharan Narang, Michael Matena, Yanqi form for Natural Language Understanding. In Pro-
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring ceedingsofICLR2019.
theLimitsofTransferLearningwithaUnifiedText-
to-Text Transformer. Journal of Machine Learning XiangpengWei,YueHu,RongxiangWeng,LuxiXing,
Research,21:1–67. HengYu,andWeihuaLuo.2021. OnLearningUni-
versal Representations Across Languages. In Pro-
AfshinRahimi,YuanLi,andTrevorCohn.2019. Mas- ceedingsofICLR2021.
sively Multilingual Transfer for NER. In Proceed-
Adina Williams, Nikita Nangia, and Samuel R. Bow-
ingsofACL2019.
man. 2018. A Broad-Coverage Challenge Corpus
for Sentence Understanding through Inference. In
PranavRajpurkar,JianZhang,KonstantinLopyrev,and
ProceedingsofNAACL-HLT2018.
Percy Liang. 2016. SQuAD: 100,000+ Questions
for Machine Comprehension of Text. In Proceed-
Bryan Willie, Karissa Vincentio, Genta Indra Winata,
ingsofEMNLP2016.
Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim,
Sidik Soleman, Rahmad Mahendra, Pascale Fung,
MarcoTulioRibeiro,TongshuangWu,CarlosGuestrin,
SyafriBahar,andAyuPurwarianti.2020. IndoNLU:
andSameerSingh.2020. BeyondAccuracy: Behav-
Benchmark and Resources for Evaluating Indone-
ioralTestingofNLPModelswithCheckList. InPro- sian Natural Language Understanding. In Proceed-
ceedingsofACL2020,pages4902–4912.
ingsofAACL-IJCNLP2020.
Melissa Roemmele, Cosmin Adrian Bejan, and An- Thomas Wolf, L Debut, V Sanh, J Chaumond, C De-
drew S Gordon. 2011. Choice of plausible alterna- langue, A Moi, P Cistac, T Rault, R Louf, M Fun-
tives: Anevaluationofcommonsensecausalreason- towicz, et al. 2019. Huggingface’s transformers:
ing. In AAAI spring symposium: logical formaliza- State-of-the-art natural language processing. arXiv
tionsofcommonsensereasoning,pages90–95. preprintarXiv:1910.03771.
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2021. mT5: A massively
multilingualpre-trainedtext-to-texttransformer. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 483–498, Online. Association for Computa-
tionalLinguistics.
Jieyu Zhao, Subhabrata (Subho) Mukherjee, Saghar
Hosseini, Kai-Wei Chang, Ahmed Hassan Awadal-
lah, Ahmed Hassan, Rdcp Associates, Jieyu Zhao,
Subhabrata (Subho) Mukherjee, Saghar Hosseini,
Ahmed Awadallah, M Najafabadipour, J M Tuñas,
A Rodríguez-González, and E Menasalvas. 2020.
GenderBiasinMultilingualEmbeddingsandCross-
lingualTransfer. InProceedingsofACL2020.
Appendix UD-POS Weemploythepart-of-speechtagging
data from the Universal Dependencies v2.7 tree-
A Detailsof XTREME models
banks(Nivreetal.,2018)covering104languages.
We use the English training data for training and
All submissions to the XTREME leaderboard are
evaluateonthetestsetsofthetargetlanguages.
large-scale Transformers (Vaswani et al., 2017)
WikiANN-NER For named entity recognition,
trained with masked language modeling (MLM),
weusetheWikiANNdataset(Panetal.,2017),in
manyofwhichextendmonolingualmodels. Multi-
which proper noun spans in Wikipedia text have
lingualBERT(mBERT),XLM-RoBERTa(XLM-
been automatically annotated as either location,
R;Conneauetal.,2020)andmultilingualT5(mT5;
person,ororganization. Weusethebalancedtrain,
Xueetal.,2021)extendBERT(Devlinetal.,2019),
dev,andtestsplitsfromRahimietal.(2019).
RoBERTa(Liuetal.,2019),andT5(Raffeletal.,
XQuAD TheCross-lingualQuestionAnswering
2020) respectively. Rebalanced mBERT (Rem-
Dataset(Artetxeetal.,2020a)requiresidentifying
BERT; Chung et al., 2021) is a more efficient,
theanswertoaquestionasaspaninthecorrespond-
scaled-up reparameterization of mBERT. These
ingparagraph. AsubsetoftheEnglishSQuADv1.1
models have been pre-trained on unlabeled data
(Rajpurkaretal.,2016)devsetwasprofessionally
inaround100languagesfromWikipedia(mBERT)
translatedintotenotherlanguagesforXQuAD.
and CommonCrawl. XLM-R was the strongest
MLQA Similarly to XQuAD, the Multilingual
baseline in XTREME (Hu et al., 2020) and is the
Question Answering dataset (Lewis et al., 2020)
foundationforsomesubsequentwork. Ithasbeen
is another cross-lingual question answering task.
fine-tuned on English data of a related task prior
Theevaluationdatainsevenlanguageswasauto-
totask-specificfine-tuning(STILTs;Phangetal.,
maticallyminedfromWikipedia,annotationswere
2020). Thefollowingmodelsfurthermorepropose
crowd-sourced,andanswerspansaligned. Forboth
newmethodstoleverageparalleldataduringpre-
XQuADandMLQA,weusetheirrespectivedata
trainingorfine-tuning. FILTER(Fangetal.,2021),
forevaluationandtrainonSQuADv1.1.
basedonXLM-R,fusesrepresentationsindifferent
languages. VECO(Luoetal.,2020)isa24-layer TyDiQA-GoldP Weusethegoldpassage(GoldP)
encoder-decodermodelthatusesadditionalMLM version of TyDiQA (Clark et al., 2020), a bench-
variantsduringpre-training. T-URLv2andHiCTL markforinformation-seekingquestionanswering,
(Wei et al., 2021), based on InfoXLM (Chi etal., whichcoversninetypologicallydiverselanguages.
2021)andXLM-Rrespectively,employcontrastive The GoldP version is a simplification of the pri-
losses. ERNIE-M (Ouyang et al., 2020) incorpo- marytask,usingonlythegoldpassageascontext
ratesback-translationintolanguagemodeling.7 andexcludingunanswerablequestions. Weusethe
Englishtrainingdatafortrainingandevaluateon
B Taskscoreson XTREME thetestsetsofthetargetlanguages.
Tatoeba We evaluate on the Tatoeba dataset
We show the performance of the models on the
(Artetxe and Schwenk, 2019), which consists of
XTREME leaderboard broken down by language upto1,000English-alignedsentencepairscover-
familyontheremaining XTREME tasksinFigure ing 122 languages. We find the nearest neighbor
2.
usingcosinesimilarity. Tomakethesettingmore
realistic, we move away from zero-shot retrieval
C XTREME tasksretainedin XTREME-R
andfine-tunemodelsonSQuADv1.1.
XNLI The Cross-lingual Natural Language In-
D Languages
ference corpus (Conneau et al., 2018) requires a
model to determine whether a premise sentence Language characteristics We show a detailed
entails,contradicts,orisneutralwithrespecttoa overview of languages in XTREME-R including
hypothesis sentence. We use the crowd-sourced interesting typological differences in Table 7.
Englishdatathatwasprofessionallytranslatedto Wikipedia information is taken from Wikipedia8
14otherlanguagesforevaluationandtheMultiNLI and linguistic information from WALS Online9.
(Williamsetal.,2018)trainsetfortraining.
8https://meta.wikimedia.org/wiki/List_
7WearenotawareofthetechnicaldetailsofPolyglotand of_Wikipedias
theanonymoussubmission. 9https://wals.info/languoid
(a)PerformanceonXNLI (b)PerformanceonPAWS-X
(d)PerformanceonXQuAD
(c)PerformanceonWikiANN-NER
(e)PerformanceonTyDiQA-GoldP (f)PerformanceonBUCC
Figure2: PerformanceofallmodelsontheXTREMEleaderboardon(a)XNLI,(b)PAWS-X,(c)WikiANN-NER,
(d)XQuAD,(e)TyDiQA-GoldP,and(f)BUCCacrosslanguagefamilies.
XTREME-R includesmembersoftheAfro-Asiatic, features from URIEL (Littell et al., 2017) across
Austro-Asiatic, Austronesian, Dravidian, Indo- the languages while the family index consists of
European, Japonic, Kartvelian, Kra-Dai, Niger- the number of distinct language families divided
Congo,Sino-Tibetan,Turkic,Uralic,Creole10,and by the total number of languages. XTREME-R is
Quechuanlanguagefamiliesaswellasoftwoiso- similarlydiversewhilecoveringalargernumberof
lates,BasqueandKorean. languages.
Languagediversityindices Wemeasurethelan-
E Mewsli-XDataset
guagediversityof XTREME-Raccordingtothety-
pologyandlanguagefamilyindicesofPontietal.
Mewsli-X is constructed specifically for
(2020),whichweshowinTable8forXTREME-R,
XTREME-R and is a more carefully sampled
XTREME(Huetal.,2020),andXGLUE(Liangetal.,
variant of the Mewsli-9 dataset (Botha et al.,
2020). The typology index is based on the mean
2020), derived from WikiNews in the same way.
entropy of the distribution over 103 typological
Compared to Mewsli-9, Serbian is dropped and
Polish, Romanian and Ukrainian are added to
10For simplicity, we treat Creole as a distinct language
family(Bakkeretal.,2011). obtain11languages,whiletheentitydescriptions
ISO #Wikipedia Diacritics/ Extensive Bound #datasets
Language Inflec- Deriva-
Language 639-1 articles(in Script special compound- words/ with
family tion tion
code millions) characters ing clitics language
Afrikaans af 0.09 Latin IE:Germanic (cid:88) 3
Arabic ar 1.02 Arabic Afro-Asiatic (cid:88) (cid:88) (cid:88) 9
Azerbaijani* az 0.18 Latin Turkic (cid:88) 2
Bulgarian bg 0.26 Cyrillic IE:Slavic (cid:88) (cid:88) (cid:88) 4
Bengali bn 0.08 Brahmic IE:Indo-Aryan (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) 3
German de 2.37 Latin IE:Germanic (cid:88) (cid:88) 8
Greek el 0.17 Greek IE:Greek (cid:88) (cid:88) (cid:88) 6
English en 5.98 Latin IE:Germanic 9
Spanish es 1.56 Latin IE:Romance (cid:88) (cid:88) 8
Estonian et 0.2 Latin Uralic (cid:88) (cid:88) (cid:88) (cid:88) 4
Basque eu 0.34 Latin Basque (cid:88) (cid:88) (cid:88) (cid:88) 3
Persian fa 0.7 Perso-Arabic IE:Iranian (cid:88) 3
Finnish fi 0.47 Latin Uralic (cid:88) (cid:88) 4
French fr 2.16 Latin IE:Romance (cid:88) (cid:88) 4
Gujarati* gu 0.03 Brahmic IE:Indo-Aryan (cid:88) 1
Hebrew he 0.25 Jewish Afro-Asiatic (cid:88) 3
Hindi hi 0.13 Devanagari IE:Indo-Aryan (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) 7
HaitianCreole* ht 0.06 Latin Creole 1
Hungarian hu 0.46 Latin Uralic (cid:88) (cid:88) (cid:88) (cid:88) 3
Indonesian id 0.51 Latin Austronesian (cid:88) (cid:88) (cid:88) 5
Italian it 1.57 Latin IE:Romance (cid:88) (cid:88) 4
Japanese ja 1.18 Ideograms Japonic (cid:88) (cid:88) 4
Javanese jv 0.06 Brahmic Austronesian (cid:88) (cid:88) 1
Georgian ka 0.13 Georgian Kartvelian (cid:88) (cid:88) 2
Kazakh kk 0.23 Arabic Turkic (cid:88) (cid:88) (cid:88) 2
Korean ko 0.47 Hangul Koreanic (cid:88) (cid:88) (cid:88) 4
Lithuanian* lt 0.2 Latin IE:Baltic (cid:88) (cid:88) 3
Malayalam ml 0.07 Brahmic Dravidian (cid:88) (cid:88) (cid:88) (cid:88) 2
Marathi mr 0.06 Devanagari IE:Indo-Aryan (cid:88) (cid:88) 3
Malay ms 0.33 Latin Austronesian (cid:88) (cid:88) 2
Burmese my 0.05 Brahmic Sino-Tibetan (cid:88) (cid:88) 1
Dutch nl 1.99 Latin IE:Germanic (cid:88) 3
Punjabi* pa 0.04 Brahmic IE:Indo-Aryan (cid:88) (cid:88) 1
Polish* pl 1.44 Latin IE:Slavic (cid:88) (cid:88) 4
Portuguese pt 1.02 Latin IE:Romance (cid:88) (cid:88) 3
CuscoQuechua* qu 0.02 Latin Quechuan (cid:88) 2
Romanian* ro 0.42 Latin IE:Romance (cid:88) (cid:88) (cid:88) 4
Russian ru 1.58 Cyrillic IE:Slavic (cid:88) 7
Swahili sw 0.05 Latin Niger-Congo (cid:88) (cid:88) (cid:88) 4
Tamil ta 0.12 Brahmic Dravidian (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) 5
Telugu te 0.07 Brahmic Dravidian (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) 4
Thai th 0.13 Brahmic Kra-Dai (cid:88) 7
Tagalog tl 0.08 Brahmic Austronesian (cid:88) (cid:88) (cid:88) 2
Turkish tr 0.34 Latin Turkic (cid:88) (cid:88) (cid:88) (cid:88) 7
Ukrainian* uk 1.06 Cyrillic IE:Slavic (cid:88) 4
Urdu ur 0.15 Perso-Arabic IE:Indo-Aryan (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) 4
Vietnamese vi 1.24 Latin Austro-Asiatic (cid:88) 8
Wolof* wo 0.002 Latin Niger-Congo (cid:88) 1
Yoruba yo 0.03 Arabic Niger-Congo (cid:88) 2
Mandarin zh 1.09 Chineseideograms Sino-Tibetan (cid:88) 8
Table7:StatisticsaboutlanguagesinXTREME-R.Languagesbelongto14languagefamiliesandtwoisolates,with
Indo-European(IE)havingthemostmembers. *indicatesnewlyaddedlanguages. Diacritics/specialcharacters:
Languageaddsdiacritics(additional symbolstoletters). Compounding: Languagemakesextensiveuseof word
compounds. Bound words / clitics: Function words attach to other words. Inflection: Words are inflected to
represent grammatical meaning (e.g. case marking). Derivation: A single token can represent entire phrases or
sentences.
Range XTREME-R XTREME XGLUE
Typology [0,1] 0.42 0.43 0.35
Family [0,1] 0.34 0.38 0.37
Table8: DiversityindiceswithregardtotypologyandlanguagefamilyofXTREME-R,XTREME,andXGLUE.
to be retrieved range over all 50 languages in settinginXTREME-Risthusdoublyzero-shot: no
XTREME-R(Table9). Tobroadenaccessibility,the candidateortestentitiesareobservedduringfine-
mentionqueries,candidatesandWikipedia-based tuning,norisnon-Englishtext.
traininginstancesarealldownsampledcompared
tothepreviouswork.
F MULTICHECKLIST
MentionExtraction Viablementionsweretaken Generalstatistics Creating MULTICHECKLIST
as hyperlinks in the WikiNews articles pointing involvedtranslatingaround550words(templates
to Wikipedia entity pages (in any language) that and fill-in values) into 49 languages. Some lan-
couldbemappedsuccessfullytoitemsinourbase guages required a larger number of words due to
WikiDataentityvocabulary. V base isdefinedasthe the creation of additional templates (Russian re-
setofentitiesthathaveaWikipediapageinanyof quiredtranslating1043words). Intotal,thetrans-
the 50 languages in XTREME-R (|V base| ≈ 12M). lation effort cost $4,360. For each test category
Thelatterconditionensuresthatentitydescriptions andeachlanguage,weautomaticallygenerate200
areavailable. WealsoextendedtheWikiDatafilter test cases. Depending on the number of possible
usedbyBothaetal.(2020)toadditionallyexclude variationsforeachtest,eachtestcasecanconsist
entitiesthatareinstancesofWikipediaListPages of2(Comparisons)to12(Intensifiers)examples.11
(Q13406463)orWikimedia(Human)Disambigua- Instructionstotranslators Wesentthefollowing
tionPages(Q22808320,Q4167410),whicharenot guidelinestoannotatorsforthetranslation:
oflargeinterestforentitylinking.
1. Wewouldlikeyoutotranslatethefollowing
The resolved, viable mentions were filtered
templatesandtheircorrespondingfill-inval-
to drop duplicate surface mentions of an en-
uesintootherlanguages. Eachtemplatecon-
tity in the same article, and mentions of years
tainssometextenclosedincurlybrackets{}.
(e.g. 2014), which are commonly linked in
Thesearethenamesofthefieldsthatwillbe
WikiNews but not of great interest. We then
substitutedinthetemplate. Weaskyounotto
performed stratified sampling by both mention
translatethetextwithinsuchcurlybrackets.
language and entity frequency bins, seeking uni-
form sizes across strata. Entity frequency is es- 2. Have a look at the text in the other fields to
timated as the number of times an entity is ref- get abetter sense what canbe substituted in
erenced on pages in the 50-language Wikipedia thetemplate. Forinstance,byreferringtothe
collection, and then binned into five intervals: lineswith“adj”,wecanseethat“{first_name}
[0,1),[1,10),[10,100),[100,1000),[1000,∞). is{adj[0]}than{first_name1}”isacompari-
Theresultingsetof15,000testmentionscovers sonbetweentwopeople.
9,647distinctgoldentities(V gold). 3. Ifthereisnotaliteraltranslationorthesame
CandidateSet Doingvectorencodingandnear- translationhasalreadybeenusedforanother
estneighborsearchoverafullknowledgebaseof word,feelfreetouseatranslationthatissimi-
millions of entities is relatively time-consuming, larinmeaning.
butsearchingonlyamongV isalsounrealistic.
gold 4. Ifthetranslationofthetemplatediffersbased
Wethusstrikeabalancebydefiningacandidateset
onthesubstitutedwords,pleasecreatemulti-
V ⊃ V ,bysamplingadditionalitemsfrom
cand gold pletranslationsforthetemplateandindicate
V \V ,thistimeonlystratifiedbyentityfre-
base gold whichsubstitutedwordscorrespondtoit. For
quency,toobtain|V | = 1,000,000.
cand instance,ifonetranslationofthetemplateas-
Each entity e ∈ V cand is assigned a single de- sumesthatsomesubstitutedwordshavemale
scription: we randomly sample a language from genderbutothershavefemalegender,create
amongtheL e ≤ 50Wikipediapagescorrespond- aseparatetranslationofthetemplatethatcon-
ingtoe,andtakethepage’sfirstsentenceasentity formstothefemalegender.
description.
5. In one template, {p.p1} and {p.p2} refer to
Fine-tuningData Thefine-tuningdataconstitutes
a property (either “shape”, “color”, “size”,
115KEnglish-only(mention,entity)-pairs,which
weresampledfromEnglishWikipediahyperlinks 11Notethatthenumberofvariationsdoesnotcorrelatewith
the test success rate, e.g. the “Job vs nationality” test has
thatmaptoV \V . Samplingisaccordingto
base cand 8 variationsfor each testcase andthe highestsuccess rate
thenaturaldistribution. TheMewsli-Xevaluation overall.
“age”,or“material”). {p.v1}and{p.v2}refer • Professions: Words for certain professions
toanattributeofeachpropertysuchas“old”, are gendered (e.g. waiter/waitress), so they
“new”,“red”,“blue”,etc. onlyoccurwithmaleorfemalenames.
• Questionsyntax: Insomelanguages,thesyn-
Challenges of template translation We high-
taxofthequestionchangesdependingonthe
lightsomeofthechallengesandlinguisticphenom-
propertyoradjectiveoneasksabout.
enaweencounteredduringtheprocessofcreating
MULTICHECKLISTinthefollowing. Unlessspec- • Syntaxofadjectives: Insomelanguages,the
ified otherwise, we create separate templates to syntaxchangesdependingonwhatadjectiveis
disambiguateeachphenomenon. used. InGerman,thetranslationsof“happy”,
“excited”,and“passionate”requiredifferent
• Genderagreement: Adjectivesandnationali- prepositions.
tiesneedtobedeclinedtomatchthegenderof
• Stilted language: Some text appears stilted
theirreferringexpression. Tokeepthetrans-
whenvaluesarefilledintothetranslatedtem-
lation effort manageable and avoid creating
plates. Forinstance,thequestion“どちらの
separatetemplatesforeachgender,wecontrol
方が冷静でないですか。” is an unusual
for gender and restrict fill-in values to male
way to do negation in Japanese; if directly
names for the affected tests (3/6). We sam-
translatedtoEnglish,itwouldmean“Whois
ple genders equally for the other tests. We
morenotcalm?”.
welcomededicatedtestsuitesanalyzingmul-
tilingualgenderbiasasfutureextensions. We tried to address most of these challenges
byinstructingtranslatorstocreateadditionaltem-
• Declination: InRussian,animalandvehicle
plates to disambiguate linguistic phenomena and
namesrequireAccusativeandNominativein
byconsolidatingdifferenttemplatesprogrammati-
differentcases.
cally. However,asthisprocesswasrelativelylabor-
• Normalization: Forappropriatesubstitution, intensive,werecommendtheusageofmorphologi-
fill-in values often need to include articles. callyawaretemplatessimilartoJiangetal.(2020)
Wenormalizeanswersandpredictionsbyre- forfuturework. Notethatmorphologicallyaware
movinglanguage-specificarticlesinorderto templatesmaynotbeabletoresolvesomeofthe
ensureaconsistentcomparison. finerlinguisticdifferences. Forthisreason,wealso
• Names: Our use of names based on data adviseworkingcloselywithnativespeakerstode-
in Wikidata leads to certain biases. Names signteststhatreflectnaturallanguageascloselyas
thataremorecommoninWikidataaremore possible.
likelytobechosen. Insomecases,namesin Fullresults WeshowthefullresultsofXLM-R
Wikidata are not written in the native script. andmBERTontheMULTICHECKLISTtestsinTa-
JapanesenamesfromWikidataareoftenwrit- bles 10 and 11 respectively. mBERT only shows
teninhiraganaorkatakanaratherthankanji. limitedamountsofcross-lingualtaxonomicknowl-
Ourchoiceofusingthefirstnameisalsonot edge. While it is able to distinguish between job
applicabletoalllanguages. InJapanese,peo- andnationalityandanimalsandvehiclesinsome
ple are usually not referred to by their first languages, it fails to do this consistently across
name,e.g. MasanoriSuzukiwouldbecalled all languages. In addition, it completely fails to
Suzuki-saninsteadofMasanori. distinguishbetweendifferentpropertiesandinten-
sifiersandisnotabletoperformcomparisons. In
• Declension of names: In some languages, a
contrast,whileXLM-Rstruggleswithintensifiers,
suffix is appended to a name depending on
it demonstrates the other capabilities much more
itsspelling. Forinstance,inTurkishthesuf-
consistentlyacrosslanguages.
fix changes based on the vowel of the last
syllable, e.g. Ahmet’in, Ali’nin, Umut’un, Examplefailurecases Weprovideexamplefail-
S¸eyma’nın, Özge’nin, etc., and Ahmet’ten, ure cases of XLM-R on a subset of languages in
Ali’den,Umut’tan,S¸eyma’dan,Özge’den,etc. Table 12. We will publicly release a comprehen-
InFinnish,namesareappendedwithavaria- sivelistoffailurecasesforXLM-RandmBERT,
tion of “lla”, e.g. Peterillä, Lisalla, Mattilla, the complete tests and model outputs for further
etc. analysis.
G Hyper-parameters XTREME-Rtoreporttrainingdataintermsofnum-
beroftokensseen.
mBERT We use the cased version, which cov-
ers104languages,has12layers,768hiddenunits I Language-agnosticRetrievalResults
perlayer,12attentionheads,a110ksharedWord-
Piece vocabulary, and 110M parameters.12 The The multiway cross-language nature of Mewsli-
modelwastrainedusingWikipediadatainall104 XandLAReQAenablescloseranalysisofmodel
languages,oversamplinglow-resourcelanguages performance by input and target language pairs.
with an exponential smoothing factor of 0.7. We Mewsli-X can directly be split by language pair
generallyfine-tunemBERTfortwoepochs,witha asithasasinglecorrecttargetperinputmention.
trainingbatchsizeof32andalearningrateof2e-5. ForLAReQA,wefollowthe“LimittoOneTarget”
WebuildontheTransformerslibrary(Wolfetal., strategyofRoyetal.(2020): insteadofaskingthe
2019)fortrainingoneachtask. model to retrieve all correct answers in one pass,
XLM-R We use the XLM-R Large version that weevaluateoneachtargetseparately,withallthe
covers 100 languages, uses a 200k shared BPE othercorrectanswersremovedfromthecandidate
vocabulary,andhasbeentrainedwithmaskedlan- pool,allowingustoreportsplitsbylanguagepair.
guagemodelling.13 Wefine-tuneXLM-Rgenerally Table14summarizesthesepairwisemAP@20
fortwoepochswithalearningrateof3e-5andan scores(here,micro-averaged),showingthatXLM-
effectivebatchsizeof16. WeusetheTransformers RLargeimprovessubstantiallyovermBERTonthe
libraryfortrainingXLM-Ronalltasks. cross-lingualcase(+38%onMewsli-Xand+137%
mT5 We use the publicly released mT5-XXL forLAReQA)inexchangeforaslightdropforthe
versionthathasnearly13billionparameterswitha same-languagecase. Evenso,performanceonthe
vocabularysize250k(Xueetal.,2021). Ithasbeen cross-lingualcaseisstilllowat29–36mAP@20,
trained on multilingual C4 (mC4) corpus which and remains a challenging area for future work.
has 6.3 trillion tokens spanning 101 languages14. Figures3and4showthedetailedbreakdowns.
Foralldownstreamtasks,wefine-tunemT5-XXL
J Detailedresults
for10kstepswithaconstantlearningrateof0.001,
dropoutrateof0.1andabatchsizeof217 tokens. We show the detailed results for each task and
Forearlystopping,wesavecheckpointsevery200 language in Tables 15 (XNLI), 16 (XCOPA), 17
steps and choose the checkpoint with the highest (UD-POS),18(WikiANN-NER),19(XQuAD),20
performanceonthevalidationset. (MLQA), 21 (TyDiQA-GoldP), 22 (Tatoeba), 23
(Mewsli-X),and24(LAReQA).
H Metadata
K NuancedMultilingualEvaluation
We intend to ask each submission to XTREME-R
forrelevantmetadata. Suchmetadataincludesthe We perform nuanced multilingual evaluations by
numberofparameters,amountofpre-trainingdata, categorizing testing examples into different at-
amountoffine-tuningdata,etc. Wearedoingthis tribute buckets and measuring the system perfor-
toenhancetransparencyandtoincreaseutilityof manceoneachattributebucket. Inthefollowing,
ourbenchmarkforpractitionerswithvaryingneeds. we describe the available attributes for tasks in
As a first step in this direction, we provide infor- XTREME-R andprovideadditionalanalysisondif-
mation about the number of parameters and the ferentattributes.
amount of monolingual and parallel pre-training
data used by all submissions to XTREME in Ta- K.1 AttributeDefinition
ble13. Notethatthedifferentsystemsreporttheir QA We denote (X ,X ,X ) as a tuple of the
c q a
trainingdataindifferentways(e.g. numberofto- corresponding context, question and answer, and
kens, number of examples, size of the data). We refertocLen, qLen, aLenastheirlengths(i.e.,
plantostandardizethisbyaskingsubmissionsto the number of tokens). We use BLEU (Papineni
12https://github.com/google-research/ et al., 2002) to measure the lexical overlap be-
bert/blob/master/multilingual.md tween (X ,X ) and (X ,X ) as BLEU-AQ and
a q q c
13https://github.com/facebookresearch/ BLEU-QC. We classify questions based on their
XLM
firsttokensandreportthetop5mostfrequentques-
14https://www.tensorflow.org/datasets/
catalog/c4#c4multilingual tiontypesasqType(i.e.,what,how,when,where,
ar de en es fa ja pl ro ta tr uk ar de en es fa ja pl ro ta tr uk
ar 78.812.5 6.5 0.7 35.8 0.0 2.3 3.1 2.5 4.9 9.1 ar 78.327.121.124.140.0 1.9 15.9 4.9 7.3 46.824.8
de 0.0 89.288.461.1 22.962.365.9 0.0 66.1 9.8 de 12.886.684.870.7 23.966.658.6 5.6 79.233.8
en 4.4 86.783.973.2 0.0 8.7 60.772.2 0.0 73.2 8.5 en 16.986.879.671.1 6.0 12.063.782.210.284.731.9
es 10.772.270.588.5 15.164.044.4 0.9 49.128.5 es 32.468.778.681.7 11.556.346.515.259.143.5
fa 39.9 4.1 4.5 6.0 89.2 5.9 0.2 7.2 0.2 1.2 6.6 fa 73.015.928.312.092.6 4.8 16.5 6.7 8.4 15.2 8.9
ja 1.7 11.614.913.5 0.0 88.711.6 0.4 0.0 1.4 ja 8.9 31.940.032.3 5.4 86.611.3 21.110.816.9
pl 0.6 58.364.965.4 0.0 4.9 85.849.3 0.2 74.5 7.6 pl 16.875.067.371.211.017.685.950.8 2.2 73.526.6
ro 8.8 63.170.569.5 55.377.1 1.0 32.919.4 ro 22.070.356.364.8 55.272.8 4.9 63.638.0
ta 10.0 0.3 0.0 10.0 0.0 76.5 ta 19.4 4.4 20.9 4.9 0.5 86.1
tr 6.7 55.958.066.3 3.1 1.8 57.921.2 0.0 82.030.6 tr 25.865.260.964.9 5.0 8.7 63.736.7 8.3 80.238.3
uk 2.3 14.520.715.3 4.5 25.022.7 0.8 11.786.2 uk 21.839.558.735.2 15.539.146.130.743.786.9
af 62.9 af 66.7
az 0.0 75.8 0.0 43.377.4 0.7 71.9 az 20.076.9 10.839.568.7 3.9 63.8
bg 6.9 12.5 38.3 0.0 54.4 bg 22.4 54.5 47.3 23.5 76.9
bn 0.0 0.7 0.0 0.3 0.0 bn 36.8 12.5 13.3 2.7 17.9
el 0.0 5.9 0.5 5.9 1.5 1.1 3.1 6.5 5.4 el 13.0 26.8 8.5 14.650.010.211.931.218.4
et 0.0 60.251.0 48.526.5 0.0 70.0 5.8 et 0.0 49.046.6 58.832.610.751.839.0
eu 3.3 60.5 72.7 74.0 0.0 18.517.9 eu 8.2 60.2 78.1 67.012.618.738.7
fi 3.7 61.576.463.7 10.366.718.3 0.0 47.5 9.7 fi 28.765.792.261.9 10.171.774.4 3.3 57.147.6
fr 3.2 67.669.770.2 0.0 12.051.058.8 2.0 55.0 5.6 fr 17.170.774.269.920.810.262.666.125.171.638.0
gu 0.0 0.0 gu 1.3 20.0
he 4.0 0.1 19.2 9.7 0.8 17.014.1 0.8 1.5 0.0 9.0 he 9.1 4.7 34.222.114.219.824.9 2.0 6.2 4.3 28.4
hi 1.2 1.0 hi 6.0 4.0
ht 0.0 ht 0.0
hu 4.2 63.9 54.1 3.0 65.3 0.6 53.714.5 hu 28.174.3 58.3 17.9 63.0 0.0 80.436.1
id 3.2 65.458.846.6 6.8 1.9 67.436.0 0.4 65.116.0 id 15.665.455.249.630.6 6.0 69.928.012.969.148.4
it 9.0 77.675.379.0 4.5 1.2 62.651.6 5.9 59.510.3 it 22.880.386.279.128.116.457.478.6 8.2 76.727.3
jv 0.0 95.5 0.0 jv 1.4 34.817.5
ka 0.5 1.0 25.011.0 ka 25.6 15.437.135.4
kk 5.3 8.4 0.0 66.5 kk 22.319.7 6.2 78.6
ko 1.3 5.9 14.011.0 2.0 13.5 2.4 0.6 1.4 5.1 5.8 ko 14.122.816.637.914.615.311.6 1.0 7.1 9.6 26.7
lt 6.0 44.8 40.8 24.042.026.2 2.8 17.716.4 lt 10.856.2 53.3 32.062.376.018.059.756.1
ml 0.0 0.0 1.4 0.0 0.0 ml 10.1 8.7 21.3 1.8 8.6
mr 8.8 1.1 0.0 9.6 0.0 1.2 11.1 mr 21.5 16.115.525.4 14.719.820.8
ms 1.8 78.855.444.0 5.5 25.363.4 1.6 72.025.5 ms 11.480.357.744.4 16.324.560.721.168.941.3
my 0.0 0.0 0.0 my 3.6 2.5 7.3
nl 4.7 68.263.257.7 0.0 56.9 1.6 0.0 64.712.3 nl 14.971.067.267.2 1.5 71.9 2.1 18.861.532.5
pa 35.2 10.0 pa 32.9 13.1
pt 1.7 66.157.086.5 2.0 14.871.878.8 0.0 49.111.8 pt 22.067.854.087.733.0 8.8 72.981.718.274.237.5
qu 33.9 0.0 qu 51.1 0.0
ru 1.7 24.422.833.7 6.9 0.3 14.720.6 0.8 17.466.4 ru 25.546.347.943.713.716.640.222.022.746.175.9
sw 0.0 sw 15.4
te 0.0 te 27.0
th 0.0 0.0 0.0 8.3 0.0 th 21.521.8 3.2 33.7 13.8
tl 9.9 15.9 0.0 tl 14.2 13.616.4
ur 54.2 2.0 0.3 0.0 1.6 ur 74.225.0 3.3 38.3 5.0
vi 9.1 67.254.060.932.720.655.263.1 1.7 51.013.8 vi 7.6 77.954.055.411.125.044.030.9 4.8 34.536.0
wo 0.0 wo 0.0
yo 23.6 0.0 yo 10.1 1.0
zh 3.6 14.017.417.2 1.6 66.8 8.6 26.6 0.0 4.7 0.6 zh 14.820.026.128.812.351.012.2 7.7 5.7 10.714.1
(a)mBERT (b)XLM-RLarge
Figure3: Mewsli-Xresultsbrokendownbymentionlanguage(columns)andentitydescriptionlanguage(rows),
onlyshowingcombinationswithatleast10testitems.
ar de el en es hi ru th tr vi zh ar de el en es hi ru th tr vi zh
0.54 0.08 0.09 0.06 0.10 0.10 0.12 0.03 0.07 0.09 0.10 0.54 0.24 0.27 0.23 0.25 0.27 0.26 0.17 0.21 0.23 0.19
0.09 0.65 0.13 0.33 0.32 0.08 0.22 0.02 0.19 0.26 0.15 0.24 0.61 0.35 0.44 0.40 0.27 0.34 0.17 0.32 0.32 0.22
0.08 0.09 0.55 0.08 0.10 0.06 0.11 0.04 0.07 0.10 0.08 0.26 0.32 0.58 0.30 0.32 0.29 0.31 0.17 0.24 0.26 0.20
0.09 0.37 0.11 0.67 0.43 0.07 0.23 0.02 0.18 0.31 0.18 0.27 0.48 0.35 0.60 0.48 0.31 0.36 0.19 0.34 0.35 0.26
0.12 0.32 0.15 0.39 0.64 0.06 0.24 0.03 0.19 0.28 0.17 0.26 0.43 0.35 0.45 0.58 0.29 0.36 0.18 0.33 0.31 0.23
0.07 0.05 0.06 0.03 0.03 0.55 0.07 0.03 0.05 0.05 0.05 0.27 0.29 0.31 0.29 0.28 0.57 0.30 0.19 0.25 0.23 0.20
0.14 0.24 0.15 0.20 0.25 0.11 0.61 0.03 0.12 0.19 0.18 0.28 0.36 0.34 0.36 0.37 0.28 0.60 0.20 0.27 0.28 0.25
0.02 0.01 0.03 0.01 0.01 0.02 0.02 0.36 0.02 0.02 0.02 0.27 0.28 0.29 0.29 0.30 0.28 0.30 0.47 0.24 0.29 0.25
0.05 0.15 0.08 0.12 0.14 0.06 0.09 0.02 0.57 0.15 0.07 0.25 0.38 0.33 0.37 0.38 0.27 0.32 0.18 0.52 0.32 0.21
0.08 0.23 0.10 0.24 0.24 0.07 0.15 0.03 0.16 0.62 0.16 0.24 0.34 0.30 0.36 0.36 0.26 0.29 0.19 0.30 0.59 0.24
0.12 0.15 0.10 0.14 0.17 0.09 0.18 0.03 0.09 0.19 0.64 0.24 0.28 0.26 0.30 0.28 0.25 0.29 0.21 0.22 0.29 0.62
(a)mBERT (b)XLM-RLarge
Figure4: LAReQAresultsbrokendownbyquestionlanguage(rows)andanswerlanguage(columns)for“Limit
toOneTarget”inference.
which),whichcover85%ofquestionsinthetrain- K.2 AttributeBuckets
ingset. Welistthesixattributesasfollows.
• φaLen(X a) = len(X a),
Webucketalltestexamplesintodifferentattribute
• φqLen(X q) = len(X q), bucketsforagivenattribute. Specifically,foranat-
tributedefinedforatask,wemeasuretheattribute
• φcLen(X c) = len(X c),
valueofthetestexamples(seeSectionK.1),then
• φBLEU (X a,X q) = BLEU(X a,X q), determineN attributebucketsforalltestexamples
aq (N = 4 by default), and finally we measure the
• φBLEU (X q,X c) = BLEU(X q,X c), systemperformanceonthetestexamplesfallingin
qc
eachattributeintervaltoobservetheperformance
• φtype(X q) = type(X q), questiontype.
changeoverdifferentattributebuckets. Sincethe
Structured Prediction Given a sentence X, we attributevaluescanbeeithercontinuous(e.g.,an-
definethei-thwordtokenasx andaspanofwords
swerlengthaLen)ordiscrete(e.g.,questiontype
i
in the range of [i,j) as X in the sentence. We
qType),weperformdifferentstrategiesforcreat-
i:j
thendefinefiveattributesincludingthelabelofa ingattributebucketsforthem.
span(tag),thetokenlengthofasentence(sLen),
thetokenlengthofanentityspan(eLen),thechar- ContinuousAttributeValues Fortheattributes
acterlengthofanentityspan(tLen)andtherela- with continuous attribute values (e.g., φaLen,
tivetokenpositionofanentity(rPos)inthesen- φqLen,φcLen,φBLEU ,φBLEU ),wedivide
aq qc
tenceasfollows. thetestexamplesintodifferentintervalswherethe
numbersofthetestsamplesinallattributeintervals
• φtag(X i:j) = label(X i:j) areequal.
• φsLen(X) = len(X)
DiscreteAttributeValues Fortheattributewith
• φeLen(X i:j) = len(X i:j) discrete attribute values (e.g., φtype), test sam-
ples with the same type are put into the same at-
• φtLen(X i:j) = |X i:j|
tributebucket.
• φrPos(X i:j) = i/φsLen(X), relativepo-
Table 25 and 26 show the detailed attribute in-
sition
tervalsforeachcategoryontheXQuADtaskand
where|x|representsthenumberofcharacters. WikiANN-NERtask,respectively.
ra
ed
le
ne
se
ih
ur
ht
rt
iv
hz
ra
ed
le
ne
se
ih
ur
ht
rt
iv
hz
K.3 AdditionalNuancedAnalysis Specifically,togenerateafine-grainedoverview,
we first select models in the table, then click one
K.3.1 WikiANN-NER
of the three Analysis Buttons, which generates a
Table27and28illustratethesinglesystemdiagno-
fine-grained analysis such as in Figure 6 (single
sisofERNIE-MandXLM-Rrespectivelyonthe
system analysis) and Figure 7 (pair-wise system
WikiANN-NERtaskinthreelanguages(i.e.,en,
analysis).
es, fr). Wemakethefollowingobservations.
ERNIE-M In Table 27, first, we observe that
the effects of some attributes for ERNIE-M are
language-independent. Forexample,basedonthe
attributerPos,thesystemisgoodatpredictingen-
titieslocatedwithinthefirst1/3partoftheEnglish
sentences, while it is relatively bad at predicting
entitieswithinthefirst1/3partofthesentencesfor
other languages. Second, the system favors long
sentences based on the attribute sLen. We even
observethatperformanceincreasesasthesentence
length increases on es and fr. Third, across all
languages, the system performs relatively bad at
predictinglongentities(eLen)andentitiesbelong-
ing to the organization class (tag). Finally, the
systemisgoodatpredictingsentenceswithfewer
entities based on the attribute for entity density
(eDen).
XLM-R InTable28,weobservethattheinfluence
ofsomeattributessuchassLen,eLen,eDenwith
respecttothesystemperformancearesimilarbe-
tweenERNIE-MandXLM-R,althoughERNIE-M
performssignificantlybetterthanXLM-Ratgener-
alizingitspredictionsones, fr.
K.3.2 QA
Table 29 shows the pairwise system analysis of
ERNIE-MandT-URLv2fortheXQuADtask. We
find that although the overall performance of T-
URLv2outperformsERNIE-M,itissurpassedby
ERNIE-Monafewbuckets. Forexample,inzh,
ERNIE-M is better at dealing with samples that
havelonganswers,longquestions,andahighlexi-
caloverlapbetweenquestionsandanswers. Inru,
ERNIE-M is better at dealing with samples with
long answers, long questions, and lower lexical
overlapbetweenquestionsandanswers,questions
andcontexts.
K.4 EXPLAINABOARDDemonstration
Figure5showstheinterfaceof EXPLAINABOARD
containing possible selection options to observe
thefine-grainedanalysisforsubmittedsystemson
XTREME. WealsodemonstratehowtoperformSin-
gleSystemandPairSystemsanalysisonFigure6
and7respectively.
Figure 5: Overall user interface. Four top-down lists at the top are used to filter the entries in the table by the
publicationyear, task, metricandlanguages. Threeanalysisbuttons(e.g., DATASETBIAS,SINGLEANALYSIS,
PAIRWISEANALYSIS)areusedtoperformthreedifferentfine-grainedevaluations.Eachrowinthetablerepresents
theperformanceofasystemonaspecificdatasetandaspecificlanguage. Relevantpiecesofinformationsuchas
thepapertitlealsoareprovided.
Figure6: Singlesystemanalysis. Eachhistogramrepresentsthefine-grainedevaluationresultsofagivensystem,
whicharebrokendownbasedonapre-definedattribute(e.g.,answer length).
Figure7: Pairwisesystemanalysis. Eachhistogramillustratesthefine-grainedperformancegapbetweensystem1
andsystem2,whichhasbeenbrokendownbydifferentpre-definedattributes(e.g.,answer length).
ar de en es fa ja pl ro ta tr uk Total
ar 185 23 64 36 12 16 17 28 42 69 24 516
de 40 485 57 56 7 34 81 53 34 81 38 966
en 88 107 470 162 18 86 124 31 62 171 108 1,427
es 61 42 43 410 7 35 41 26 26 24 35 750
fa 51 38 25 27 32 21 27 25 45 30 26 347
ja 22 34 30 24 12 586 32 6 24 14 26 810
pl 59 37 43 32 10 27 552 68 32 34 81 975
ro 28 24 19 21 6 8 20 107 34 19 30 316
ta 10 4 19 11 2 10 7 129 62 6 6 266
tr 60 17 16 21 13 14 21 13 15 347 17 554
uk 44 23 22 29 9 11 53 38 31 23 381 664
af 5 5 5 6 1 3 2 3 9 15 7 61
az 10 18 5 8 5 10 10 22 12 39 8 147
bg 20 9 11 7 5 9 15 9 17 5 32 139
bn 48 8 26 8 3 7 9 14 43 13 6 185
el 14 9 17 11 4 45 11 22 33 27 21 214
et 15 18 13 9 9 8 11 20 14 12 25 154
eu 15 28 8 23 6 8 8 35 32 13 14 190
fi 29 22 35 30 8 13 23 23 58 30 21 292
fr 50 59 69 65 12 46 60 15 29 56 50 511
gu 8 0 3 1 10 3 0 8 15 2 1 51
he 28 48 23 18 18 34 17 22 37 34 28 307
hi 12 4 4 7 7 3 6 0 21 6 6 76
ht 3 5 5 1 2 6 3 3 11 1 2 42
hu 12 21 9 11 2 12 9 23 27 19 22 167
id 22 24 24 13 10 11 11 17 35 29 42 238
it 47 48 43 53 20 16 33 21 17 31 40 369
jv 10 2 3 2 2 4 5 11 24 5 7 75
ka 24 4 8 7 6 4 5 9 13 25 13 118
kk 5 8 9 7 6 3 14 29 15 9 11 116
ko 28 33 27 39 19 62 20 52 69 36 20 405
lt 24 19 9 10 8 10 12 60 18 27 11 208
ml 14 7 7 8 5 2 7 14 33 11 11 119
mr 20 5 19 10 11 5 8 22 46 12 4 162
ms 19 13 13 13 4 47 10 27 35 14 11 206
my 7 5 8 14 3 2 4 10 24 7 6 90
nl 33 45 33 43 7 17 33 224 38 17 25 515
pa 3 0 4 3 2 12 1 2 10 2 3 42
pt 50 23 42 59 23 29 49 13 22 21 18 349
qu 6 3 1 1 9 0 3 22 11 7 1 64
ru 53 32 44 34 15 21 39 11 15 40 147 451
sw 5 6 5 5 9 9 1 8 13 7 7 75
te 1 3 3 4 3 4 6 6 27 6 2 65
th 5 4 16 15 2 28 12 1 12 6 7 108
tl 8 6 1 6 6 11 3 17 31 5 5 99
ur 14 10 4 8 7 6 6 43 27 14 5 144
vi 60 17 12 18 11 10 14 22 105 22 18 309
wo 1 2 0 1 1 6 3 15 5 1 2 37
yo 9 4 6 3 5 4 6 16 38 5 2 98
zh 55 39 38 36 14 71 17 67 21 25 28 411
Total 1,440 1,450 1,420 1,446 428 1,449 1,481 1,482 1,469 1,474 1,46115,000
Table9:CompositionofMewsli-Xtestsetbymentionlanguage(columns)andentitydescriptionlanguage(rows).
Lang. Comparisons Intensifiers Properties NaJ to iob nv as lity A Vn ei hm ica ll ev ss A Ven him icla el sv 2s Lang. Comparisons Intensifiers Properties NaJ to iob nv as lity A Vn ei hm ica ll ev ss A Ven him icla el sv 2s
af 47.0 78.7 35.5 1.0 56.0 21.2 af 92.5 100.0 91.5 30.0 86.5 52.0
ar 23.9 97.5 100.0 0.0 100.0 22.6 ar 91.9 100.0 100.0 16.5 100.0 79.4
az 98.0 75.1 73.5 4.5 40.5 99.5 az 100.0 100.0 94.0 6.5 95.5 98.5
bg 49.5 71.1 14.3 0.0 16.5 6.1 bg 99.0 100.0 92.9 0.0 80.0 74.5
bn 89.9 94.0 91.0 9.0 48.5 24.7 bn 100.0 100.0 100.0 100.0 100.0 88.4
de 17.1 90.6 38.2 2.5 17.0 9.1 de 97.5 100.0 90.2 10.0 100.0 16.2
el 6.1 98.0 38.3 18.1 45.0 16.5 el 95.9 100.0 95.5 61.3 99.0 83.0
en 7.0 90.4 38.5 0.0 6.0 0.5 en 100.0 100.0 96.5 0.0 25.0 15.1
es 9.6 99.5 62.0 0.0 32.5 5.5 es 100.0 100.0 87.8 0.0 28.0 10.1
et 11.1 91.4 36.1 14.5 49.0 0.0 et 78.9 100.0 97.6 76.5 100.0 3.0
eu 100.0 98.5 66.0 17.5 25.5 38.7 eu 100.0 100.0 98.5 91.5 79.0 77.4
fa 10.6 84.2 51.7 5.0 38.5 11.6 fa 95.5 100.0 99.5 45.5 99.5 69.8
fi 10.7 79.8 34.1 4.0 9.2 1.5 fi 90.8 100.0 74.6 15.5 89.7 42.6
fr 4.0 98.0 15.4 0.8 10.0 13.5 fr 100.0 100.0 83.6 19.3 17.0 15.5
gu 100.0 100.0 100.0 39.0 90.0 31.8 gu 100.0 100.0 100.0 92.0 100.0 99.0
ha 100.0 100.0 99.5 100.0 100.0 91.5 ha 91.0 100.0 100.0 100.0 100.0 94.0
he 100.0 97.5 100.0 100.0 100.0 27.9 he 100.0 100.0 100.0 100.0 100.0 75.6
hi 22.2 63.8 64.3 8.0 28.0 8.6 hi 99.0 100.0 93.8 12.5 94.0 80.8
ht 95.5 100.0 100.0 100.0 100.0 90.9 ht 72.0 100.0 100.0 100.0 100.0 68.2
hu 6.1 98.5 33.5 14.0 28.0 10.6 hu 89.8 100.0 94.0 62.0 81.0 57.3
id 6.5 98.0 77.0 0.0 42.0 33.5 id 99.0 100.0 80.0 8.0 94.0 32.0
it 3.5 98.5 50.0 6.7 10.5 5.6 it 99.5 100.0 95.7 20.2 39.5 31.3
ja 96.0 100.0 60.5 99.0 30.0 95.5 ja 100.0 100.0 99.0 100.0 100.0 78.4
jv 9.0 100.0 82.0 3.5 100.0 100.0 jv 94.5 100.0 99.5 50.0 100.0 91.5
ka 0.0 95.0 34.0 12.0 13.0 12.0 ka 100.0 100.0 99.5 58.5 47.5 53.0
kk 88.9 99.0 82.5 0.5 100.0 17.0 kk 98.0 100.0 97.5 5.5 100.0 74.0
ko 18.5 98.5 34.5 9.0 42.0 60.9 ko 97.5 100.0 75.0 14.5 99.5 55.3
lt 12.7 84.2 67.6 25.2 22.5 2.5 lt 92.9 100.0 99.5 37.0 94.0 37.0
ml 7.1 73.8 74.0 6.5 32.5 19.0 ml 98.0 100.0 92.0 100.0 100.0 97.0
mr 0.0 83.0 82.8 100.0 45.0 86.4 mr 100.0 100.0 100.0 100.0 100.0 99.5
ms 4.0 97.9 84.0 5.5 18.5 0.5 ms 100.0 100.0 99.5 8.0 100.0 31.3
my 99.0 86.0 83.0 9.5 93.5 0.0 my 100.0 100.0 93.5 84.5 100.0 52.8
nl 14.1 91.9 25.4 1.0 24.1 11.3 nl 96.5 100.0 92.2 45.5 41.5 8.8
pa 99.5 56.3 100.0 0.0 38.0 10.6 pa 100.0 100.0 100.0 54.0 100.0 100.0
pl 20.9 100.0 20.8 0.0 11.0 3.5 pl 99.0 100.0 88.9 33.5 81.0 53.8
pt 51.8 99.0 62.4 0.8 23.0 7.1 pt 98.0 100.0 97.6 2.5 63.5 26.8
qu 91.9 100.0 100.0 98.0 95.5 97.0 qu 100.0 100.0 100.0 100.0 96.5 98.5
ru 32.0 95.0 33.8 7.1 24.0 3.0 ru 96.0 100.0 93.0 73.1 79.0 17.0
sw 100.0 100.0 94.0 6.0 100.0 98.0 sw 100.0 100.0 100.0 55.5 100.0 86.5
ta 59.0 78.7 65.5 11.5 13.5 13.2 ta 90.0 100.0 100.0 21.0 100.0 95.4
te 35.2 92.0 68.0 28.0 36.0 55.0 te 88.4 100.0 95.5 91.0 100.0 98.5
th 91.4 78.4 100.0 100.0 100.0 41.0 th 99.5 100.0 100.0 100.0 100.0 100.0
tl 0.0 100.0 66.5 12.5 58.5 13.6 tl 99.5 100.0 100.0 96.5 100.0 100.0
tr 100.0 84.4 72.5 0.5 21.0 14.1 tr 100.0 100.0 100.0 70.5 77.0 67.7
uk 16.2 94.9 39.8 26.9 51.5 14.1 uk 95.8 100.0 85.6 81.5 65.0 37.9
ur 90.9 57.5 76.1 16.5 100.0 18.7 ur 100.0 100.0 100.0 99.5 100.0 100.0
vi 13.6 99.0 80.0 10.0 100.0 2.6 vi 97.5 100.0 90.5 7.5 100.0 31.1
wo 100.0 100.0 100.0 100.0 100.0 100.0 wo 59.5 100.0 100.0 100.0 100.0 91.5
yo 100.0 100.0 100.0 100.0 100.0 99.5 yo 99.0 100.0 99.0 100.0 100.0 65.3
zh 94.4 100.0 33.0 100.0 94.5 71.2 zh 100.0 100.0 100.0 100.0 100.0 92.4
Avg 47.3 90.9 64.8 26.7 51.6 32.8 Avg 95.8 100.0 95.3 55.1 87.0 64.1
Table 10: Error rate of XLM-R fine-tuned on English Table 11: Error rate of mBERT fine-tuned on English
SQuADv1.1on6 CHECKLIST QAtestsacrossall50 SQuADv1.1on6 CHECKLIST QAtestsacrossall50
languages. languages.
Table12: ExamplefailurecasesofXLM-Ronasubsetoflanguages. Eachfailurecaseconsistsofacontext(C),a
question(Q),ananswer(A),andXLM-R’sprediction(P).
Numberofparameters Pre-trainingdata
Model
(inmillions) Monolingualdata Paralleldata
mBERT 178 85GB N/A
XLM-R(large) 559 6.3Ttokens N/A
MMTE 190 N/A 25Bpairs
mT5 13,000 1Ttokens N/A
RemBERT 575 1.8Ttokens N/A
X-STILTS 559 6.3Ttokens N/A
FILTER 559 6.3Ttokens N/A
VECO 559 1.3TB 6.4Mpairs
T-URLv2+StableTune 559 2.1TB 42GB
ERNIE-M 559 1.5TB 69GB
Table13: MetadataforthecurrentsubmissionstoXTREME.Notethatmonolingualpre-trainingdataisreported
ineithernumberoftokensorsizeofthedata(inGB/TB).Theamountofparalleldataisreportedineithernumber
ofpairsorsizeofthedata(inGB/TB).
Mewsli-X LAReQA
Subset\Model mBERT XLM-RLarge mBERT XLM-RLarge
All 40.2 47.1 16.3 31.3
Samelanguage 85.7 83.6 58.2 57.1
Differentlanguages 25.8 35.5 12.1 28.7
Table14:Language-agnosticretrievalresults(mAP@20)brokendownbywhetherthequeryandanswerlanguages
arethesameordifferent.
Model en ar bg de el es fr hi ru sw th tr ur vi zh avg
mBERT 81.7 66.0 69.2 71.1 66.8 74.9 74.2 60.7 70.2 49.3 54.7 61.2 58.2 70.5 69.0 65.4
XLM-R 88.7 77.2 83.0 82.5 80.8 83.7 82.2 75.6 79.1 71.2 77.4 78.0 71.7 79.3 78.2 79.2
mT5 92.6 84.5 87.0 87.3 86.9 88.4 87.4 82.3 84.3 80.6 81.2 83.4 79.8 84.0 83.0 84.8
mBERTtranslate-train 80.8 73.6 76.6 77.4 75.7 78.1 77.4 71.9 75.2 69.4 70.9 75.3 67.2 75.0 74.1 74.6
mBERTtranslate-train-all 81.9 73.8 77.6 77.6 75.9 79.1 77.8 70.7 75.4 70.5 70.0 74.3 67.4 77.0 77.6 75.1
Table15: XNLIresults(accuracy)acrosslanguages.
Model et ht id it qu sw ta th tr vi zh Avg
mBERT 54.6 51.8 55.4 57.6 54.0 52.2 55.6 52.0 55.4 60.4 67.6 56.1
XLM-R 68.2 52.6 80.6 71.0 52.8 61.8 73.8 74.4 72.0 76.2 78.0 69.2
mT5 77.5 72.1 81.1 75.9 54.4 74.1 75.9 78.3 78.1 76.9 79.5 74.9
mBERTtranslate-train 57.4 55.6 60.6 63.4 47.8 51.6 54.8 56.4 58.8 59.6 64.0 57.3
mBERTtranslate-train-all 56.0 54.4 59.0 60.4 51.0 56.8 55.4 58.2 56.8 63.6 65.4 57.9
Table16: XCOPAresults(accuracy)acrosslanguages.
Model af ar bg de el en es et eu fa fi fr he hi hu id it ja kk ko
mBERT 86.1 54.0 85.2 85.6 80.4 95.4 85.8 79.5 59.3 66.2 78.0 83.4 55.3 67.1 78.3 80.6 88.2 49.7 69.8 49.2
XLM-RLarge 89.7 68.1 88.6 88.5 86.5 96.1 89.1 87.2 74.3 73.9 86.3 88.4 68.2 74.4 83.4 82.9 89.8 29.4 79.3 54.0
mr nl pt ru ta te th tl tr ur vi yo zh lt pl uk wo ro Avg
mBERT 66.6 88.6 86.8 85.1 68.6 75.2 39.1 68.5 68.3 58.0 53.4 53.4 61.2 77.9 79.9 80.4 28.9 77.3 70.9
XLM-RLarge 85.3 89.5 89.3 89.7 77.3 85.3 47.7 75.0 75.4 67.2 56.8 22.8 40.8 84.5 84.8 85.8 27.6 85.5 75.0
Table17: UD-POSresults(F1score)acrosslanguages.
Model ar he vi id jv ms tl eu ml ta te af nl en de el bn hi mr ur fa fr it pt es
mBERT 43.9 56.6 72.1 62.9 64.6 70.9 73.7 65.0 53.5 53.0 47.4 76.0 81.9 84.5 78.0 68.8 70.3 66.7 57.3 35.6 50.1 79.1 81.3 79.8 72.2
XLM-Rlarge 43.7 54.1 77.2 52.3 58.7 69.8 72.2 62.1 65.8 56.9 52.3 77.7 84.3 84.6 78.0 77.2 76.3 71.0 64.2 54.1 61.1 79.1 81.1 79.6 68.8
bg ru ja ka ko th sw yo my zh kk tr et fi hu qu pl uk az lt pa gu ro Avg
mBERT 77.2 63.5 28.4 65.0 59.1 2.3 72.7 49.4 49.1 43.3 49.0 72.1 77.0 77.4 75.0 57.4 79.9 69.3 65.6 74.3 37.0 47.3 72.1 62.7
XLM-Rlarge 81.2 71.5 18.3 68.9 58.0 1.5 69.9 41.8 50.9 25.8 49.9 78.9 78.0 78.6 79.3 50.4 81.3 73.1 69.2 76.9 46.8 60.7 79.6 64.4
Table18: WikiANN-NERresults(F1score)acrosslanguages.
Model en es de el ru tr ar vi th zh hi avg
mBERT 84.5 75.1 73.2 62.9 71.3 53.7 62.2 70.1 43.5 59.6 59.5 65.1
XLM-RLarge 87.4 82.7 80.9 80.7 80.5 76.1 74.4 79.2 75.4 55.0 77.0 77.2
mT5 90.2 84.6 82.3 82.8 78.8 76.5 80.3 83.3 74.7 81.7 81.7 81.5
mBERTtranslate-train 83.5 80.2 75.6 70.0 75.0 68.9 68.0 75.6 36.9 66.2 69.6 70.0
mBERTtranslate-train-all 86.0 82.4 78.8 74.2 78.1 70.6 71.0 78.5 38.1 67.7 71.3 72.4
Table19: XQuADresults(F1)acrosslanguages.
Model en es de ar hi vi zh avg
mBERT 80.7 66.2 60.2 51.6 49.9 60.2 60.3 61.3
XLM-RLarge 83.9 74.4 70.3 67.0 70.8 74.2 68.4 72.7
mT5 86.4 76.2 73.1 70.2 75.3 76.5 71.4 75.6
mBERTtranslate-train 80.2 70.0 64.4 55.0 60.1 65.7 63.9 65.6
mBERTtranslate-train-all 80.7 71.3 66.0 58.9 62.4 67.9 66.0 67.6
Table20: MLQAresults(F1)acrosslanguages.
Model en ar bn fi id ko ru sw te avg
mBERT 69.4 61.7 53.5 57.4 63.2 57.6 56.5 59.7 46.2 58.4
XLM-RLarge 69.2 66.1 59.1 64.7 73.8 58.0 62.2 66.4 59.1 64.3
mT5 83.1 82.4 83.6 81.2 84.5 73.2 78.7 87.2 83.6 81.9
mBERTtranslate-train 75.3 61.5 31.9 62.6 68.6 53.2 53.1 61.9 27.4 55.1
mBERTtranslate-train-all 73.2 71.8 49.7 68.1 72.3 58.6 64.3 66.8 53.3 64.2
Table21: TyDiQA-GoldPresults(F1)acrossdifferentlanguages.
Model ar he vi id jv tl eu ml ta te af nl de el bn hi mr ur fa fr it
mBERT 32.7 46.8 66.2 57.7 18.5 17.5 31.0 19.2 23.5 26.1 49.6 66.1 79.7 29.1 20.7 38.6 23.7 38.4 49.9 67.1 65.6
XLM-R 68.3 77.6 91.0 88.4 28.8 60.8 58.6 83.6 65.8 80.8 74.9 90.0 96.6 76.6 67.6 88.9 70.4 76.5 85.3 87.5 82.4
pt es bg ru ja ka ko th sw zh kk tr et fi hu az lt pl uk ro Avg
mBERT 74.4 71.8 52.8 65.1 50.8 21.1 43.9 15.0 12.1 74.9 30.4 38.4 34.0 42.7 44.2 37.2 35.4 54.1 55.8 54.8 41.3
XLM-R 89.8 89.5 84.4 86.1 75.5 66.2 81.9 80.5 31.3 79.5 63.8 84.5 73.4 86.5 83.1 74.7 77.5 88.0 82.6 89.6 76.2
Table22: Tatoebaresults(accuracy)acrossdifferentlanguages.
Model ar de en es fa ja pl ro ta tr uk avg
mBERT 15.3 61.0 54.8 59.4 13.5 44.2 57.7 27.5 4.1 49.9 37.0 38.6
XLM-RLarge 28.7 64.8 59.7 62.0 24.6 45.0 62.1 30.8 14.9 59.4 51.2 45.7
Table23: Mewsli-Xresults(meanaverageprecision@20)acrossdifferentinputlanguages.
Model ar de el en es hi ru th tr vi zh avg
mBERT 17.0 29.3 16.3 31.3 30.8 12.3 27.2 5.8 17.9 25.3 24.2 21.6
XLM-RLarge 34.6 42.8 38.8 46.2 43.7 38.2 42.5 39.5 41.3 40.9 39.8 40.7
Table24: LAReQAresults(meanaverageprecision@20)acrossdifferentquestionlanguages.
aLen qLen cLen BLEU_AQ BLEU_QC aLen qLen cLen BLEU_AQ BLEU_QC
Bucket
en zh
XS [1,2] [3,8] [25,91] 0 [0,1.64e-08] 1 [1,13] [3,8] 0 [0,.0075]
S (2,5] (8,11] (91,111] (0,.016] (1.64e-08,2.8e-06] (1,6] (13,17] (8,11] (0,.0118] (.0075,.01]
L (5,13] (11,16] (111,156] (.016,.128] (2.8e-06,7.92e-05] (6,15] (17,22] (11,17] (.0118,.0542] (.01,.013]
XL (13,25] (16,29] (156,509] (.128,.146] (7.92e-05,.0589] (15,72] (22,52] (17,144] (.0542,.328] (.013,.097
hi el
XS [1,2] [3,8] [28,104] 0 [0,2.02e-08] [1,2] [3,8] [29,97] 0 [4.99e-39,9.58e-09]
S (2,5] (8,11] (104,131] (0,.015] (2.02e-08,2.91e-06] (2,5] (8,11] (97,120] (0,.014] (9.58e-09,1.32e-06]
L (5,13] (11,15] (131,171] (.015,.19] (2.91e-06,7.29e-05] (5,11] (11,16] (120,162] (.014,.019] (1.32e-06,4.37e-05]
XL (13,29] (15,53] (171,575] (.019,.24] (7.29e-05,.0628] (11,26] (16,30] (162,554] (.019,.12] (4.37e-05,.033]
ru tr
XS [1,2] [3,6] [25,86] 0 [1.34e-39,3.24e-09] [1,2] [2,6] [18,75] 0 [0,6.37e-09]
S (2,5] (6,8] (86,104] (0,.017] (3.24e-09,6.2e-07] (2,5] (6,8] (75,94] (0,.016] (6.37e-09,9.33e-07]
L (5,12] (8,11] (104,140] (.017,.023] (6.2e-07,2.07e-05] (5,11] (8,11] (94,130] (.016,.057] (9.33e-07,2.67e-05]
XL (12,23] (11,26] (140,470] (.023,.287] (2.07e-05,.0294] (11,20] (11,20] (130,414] (.057,.143] (2.67e-05,.0408]
ar vi
XS [1,2] [2,7] [24,83] 0 [0,4.72e-09] [1,2] [3,10] [39,127] 0 [0,1.47e-08]
S (2,4] (7,9] (83,104] (0,.0243] (4.72e-09,8.51e-07] (2,4] (10,13] (127,160] (0,.0139] (1.47e-08,2.43e-06]
L (4,9] (9,12] (104,138] (.0243,.293] (8.51e-07,3.39e-05] (4,8] (13,17] (160,213] (.0139,.135] (2.43e-06,7.76e-05]
XL (9,22] (12,38] (138,432] (.293,.246] (3.39e-05,.0266] (8,34] (17,40] (213,763] (.135,.189] (7.76e-05,.0546]
Table25: DetailedattributeintervalsforeachcategoryontheXQuADtask.
en es fr
Bucket
sLen eLen rPos eDen sLen eLen rPos eDen sLen eLen rPos eDen
XS [3,5] 1 0 [0,0.33] [3,5] 1 0 [0,0.33] [1,5] 1 0 [0.027,0.33]
S (5,8] 2 (0,0.33] (0.33,0.56] (5,7] 2 (0,0.25] (0.33,0.67] (5,7] 2 (0,0.33] (0.33,0.64]
L (8,16] (2,4] (0.33,0.62] (0.56,1] (7,18] (2,4] (0.25,0.6] (0.67,0.86] (7,16] (2,4] (0.33,0.67] (0.64,1]
XL (16,1e+03) (4,1e+03] (0.62,1] - (18,1e+03] (4,1e+03] (0.6,1] (0.86,1] (16,1e+03] (4,1e+03] (0.67,1] -
Table26: DetailedattributeintervalsforeachcategoryonWikiANN-NERtaskinen,es,andfr.
Lang. sLen eLen rPos eDen tag
English
4322366137412234 2688492142512098 5862282727172552 4111 3533 6314 4657 4745 4556
en 85 90 86 88 56 90
F1: 85.66 80 85 84 84 85
83 80
75 80 82
IE:Romance
519133453137 587 2463332643302141 3808396629241562 3213320033542493 4725 3576 3959
90 85
es 90 90 90
85 80
F1: 82.25 80 80 80 75 80
70 70
95 5022343835111398 95 3610413536222002 90 5291350129311646 4064 3372 5933 4985 3885 4499
86
fr 90 90 84 90
F1: 84.12 85 85 82 85
85 80 80
80
78 75 80 75 80
Table 27: The single system diagnosis of ERNIE-M on WikiANN-NER task in en, es and fr. The first row
showsthefine-grainedperformanceonEnglishtestdata,andthesecondandthirdrowsshowthezero-shottransfer
fine-grainedperformanceonlanguagesintheIE:Romancelanguagefamily.
Lang. sLen eLen rPos eDen tag
English
4322366137412234 90 2688492142512098 5862282727172552 4111 3533 6314 4657 4745 4556
en 85 86 90
84
80 85 84 85
F1: 84.62
75 80 82 82 80
IE:Romance
519133453137 587 2463332643302141 3808396629241562 3213320033542493 4725 3576 3959
90
es 80 80
80 80
80 70
F1: 68.76 70 60 60
60 60 60 50
40
5022343835111398 3610413536222002 5291350129311646 4064 3372 5933 4985 3885 4499
fr 90 90 85 85 90
80 80 80
F1: 79.07 80 75
80 70
70 70
75
60
Table28: ThesinglesystemdiagnosisofXLM-RonWikiANN-NERtaskinen,esandfr. Thefirstrowshows
thefine-grainedperformanceonEnglishtestdata,andthesecondandthirdrowsshowthezero-shottransferfine-
grainedperformanceonlanguagesintheIE:Romancelanguagefamily.
)5,3(
)5,3(
)5,1(
)5,3(
)5,3(
)5,1(
)8,5(
)7,5(
)7,5(
)8,5(
)7,5(
)7,5(
)61,8(
)81,7(
)61,7(
)61,8(
)81,7(
)61,7(
)30+e1,61(
)30+e1,81(
)30+e1,61(
)30+e1,61(
)30+e1,81(
)30+e1,61(
),1(
),1(
),1(
),1(
),1(
),1(
),2(
),2(
),2(
),2(
),2(
),2(
)4,2(
)4,2(
)4,2(
)4,2(
)4,2(
)4,2(
)30+e1,4(
)30+e1,4(
)30+e1,4(
)30+e1,4(
)30+e1,4(
)30+e1,4(
),0(
),0(
),0(
),0(
),0(
),0(
)333.0,0(
)52.0,0(
)333.0,0(
)333.0,0(
)52.0,0(
)333.0,0(
)516.0,333.0(
)6.0,52.0(
)766.0,333.0(
)516.0,333.0(
)6.0,52.0(
)766.0,333.0(
)1,516.0(
)1,6.0(
)1,766.0(
)1,516.0(
)1,6.0(
)1,766.0(
)333.0,6510.0(
)333.0,6510.0(
)333.0,8720.0(
)333.0,6820.0(
)333.0,8720.0(
)333.0,6820.0(
)766.0,333.0(
)766.0,333.0(
)655.0,333.0(
)758.0,766.0(
)636.0,333.0(
)655.0,333.0(
)758.0,766.0(
)636.0,333.0(
)1,655.0(
)1,758.0(
)1,636.0(
)1,655.0(
)1,758.0(
)1,636.0(
col
col
col
col
col
col
gro
gro
gro
gro
gro
gro
rep
rep
rep
rep
rep
rep
aLen qLen cLen BLEU-AQ BLEu-QC qType
enzhhielrutrarvi enzhhielrutrarvi enzhhielrutrarvi enzhhielrutrarvi enzhhielrutrarvi enzhhielrutrarvi
25 4 8
M1:ERNIE-M 20 2 2 6 4 5
(Overall:75.46) 15 4 2
M
(O
P2
(av
M:
e
irT
r
1wa-
l
−U
il
s:R
e
M7L
S6
2v
.
y2
6 )s8 .)
−10
505 −−−
6420
−−
420
−−−
64202
−−
420
−−
150
0
Table 29: Pairwise system diagnosis of ERNIE-M and T-URLv2 for XQuAD task. “M1−M2” represents the
performance difference between M1 and M2. We classify the attribute values into four categories: extra-small
(XS),small(S),large(L)andextra-large(XL)values. Thetop5mostfrequentquestiontypes(qType)include
what(A),how(B),who(C),when(D)andwhich(E),rankedbytheirfrequenciesintrainingset. Inthepairwise
system diagnosis histogram, blue (red) x ticklabels represents the bucket value of a specific attribute on which
systemM1surpasses(under-performs)M2bythelargestmarginthatisillustratedbyablue(red)bin. Theblue-
onlyxticklabels(e.g.,-D)indicatethatM1outperformsM2inallcategoriesofanattribute.
S-SX LX-SX LX-SX L-LX LX-L S-LX LX-L LX-L SX-LX L-SX SX-LX LX-SX LX-S SX-LX LX-SX S-L S-SX S-LX SX-L L-LX SX-LX S-SX S-L L-LX S-SX LX-SX L-S S-L S-SX L-SX L-S LX-S SX-L S-LX S-LX L-SX LX-S SX-LX L-SX S-LX D-B A-E A-E E-D E-C E-B B-C E-C

Appropriateness is all you need!
General-purposechatbotsandwhattheymayandmaynotsay
HendrikKempt1,AlonLavie2,3,SaskiaK.Nagel1
1RWTHAachenUniversity,AppliedEthicsGroup
2CarnegieMellonUniversity,LanguageTechnologiesInstitute
3UnbabelInc.
{hendrik.kempt, saskia.nagel}@humtec.rwth-aachen.de
Abstract
The strive to make AI applications “safe” has led to the development of
safety-measures as the main or even sole normative requirement of their
permissible use. Similar can be attested to the latest version of chatbots, such
aschatGPT.Inthisview,iftheyare“safe”,theyaresupposedtobepermissible
to deploy. This approach, which we call “safety-normativity”,isratherlimited
in solving the emerging issues that chatGPT and other chatbots have caused
thus far. In answering this limitation, in this paper we argue for limiting
chatbots in the range of topics they can chat about according tothenormative
concept of appropriateness. We argue that rather than lookingforthe“safety”
in a chatbot’s utterances to determine what they may and may not say, we
ought to assess those utterances according to three forms of appropriateness:
technical-discursive, social, and moral. We then spell out what requirements
for chatbots follow from these forms of appropriateness to avoid the limits of
previous accounts: positionality, acceptability, and value alignment (PAVA).
With these in mind, we may be able to determine what a chatbot may or may
not say. Lastly, one initial suggestion is to use challenge sets, specifically
designedforappropriateness,asavalidationmethod.
ChatGPT (Schulman et al. 2022), a chatbot
1.Introduction designed and built on top of the latest
generation of large language models (LLMs),
General-purpose and open-domainchatbots
has stunned the public in its capabilities, and
have entered the public domain, to varying
generated a deluge of public discourse. Two
degrees of economic success and public
intuitively plausible demands now stand
acclaim. OpenAI’s recent public release of
against each other: on the one hand, chatbots limitations to answer our research question:
ought not to say just anything,assomespeech safety-considerations will not provide us with
can cause great harm. On the other hand, for the answer of what chatbots may or may not
general purpose use and to be as helpful as say, and thus far, ethical discourse has also
possible, they ought to remain open-domain fallen short of providing such an answer (cf.
chatbots. Walkingthelinebetweenensuringas for a similar assessmentLaCroixandLuccioni
much freedom as possible without enabling 2023). As an alternative, we choose to
open-domain chatbots to cause severe harm is concentrate on an immanent critique of
one of the prime challenges for this chatbots. That is, we aim to measure the
technology. recently emerging normative concerns coming
However, current discourse of AI ethics from interactions with chatbots -inappropriate
focuses to a considerable degree on the responses, hallucinations, prompt injection
business ethics of this technology, the hacks - against what they are intended to do,
violations of corporate responsibilities, and assess whether normative accounts can rectify
other concerns for public well-being and any misalignment, and suggest a solution that
collective harms. Unsustainable business canprovidesomesubstantialanswers.
practices to harvest data and train the Up to this point, experiences by journalists
algorithm that are exploitative of workers and the public using chatGPT in the Bing
(Carter 2023), consume large amounts of search engine (the codename for this chatbot
energy (Bender et al. 2021, Patterson et al. was “Sydney” (Roose 2022); in thefollowing,
2021), and may violate copyrights; we refer to that version of chatGPT as
unregulated market-dynamics and competition “BingGPT” to clarify the connection toitsuse
incentivizing corporations to release and its source) have exposed unexpected
unfinished products (Vincent 2023); an behavior in the way these latest chatbots
unprepared and sour-turning public (NYT interact with humans on conversationaltopics,
2023), and data protectionissuesarejustsome apparentlyunintendedbytheirdevelopers.
of the emerging topics surrounding therelease Creating something that can function as a
ofthelatestroundoftechnologicalprogresson general-purpose or open-domain chatbot
generative AI, and especially on LLM and initially sounds like a reasonable aim to
general-purposechatbots. achieve something of an intelligent agent.
It appears that the discourseaboutLLMsis After all, humans can hold these kinds of
stuck between AIsafety(thoseconcernedwith open-ended conversations, and thus is seems
the safe useandimplementationofLLMs)and worth it, from both alargergoal(e.g.,creating
AI ethics (those concerned with the ethical artificial general intelligence, as OpenAI
construction and just dissemination of LLMs). claims to pursue (Altman 2023) as well as for
And while both sides ask urgent and relevant application-varieties (a general-purpose
questions, they also have significant chatbot may be narrowed down to a
specified-purpose chatbot), to pursue LLMs seek in limiting these powerful tools. Without
with the capacity to holdconversationsonany nuanced proposals available, chatbots might
topic. If a chatbot is supposed to hold actually “chat” about anything and thus not
conversations with everyone, any topic ought only can cause harm, but also change
to be able to be discussed. It seems like the conversational expectations generally or
logical consequence of creating a chatbot in influence public discourse in unwarranted
the first place, and of AI at large: ways. We require their responses to be
multi-modality is a key for creating ever appropriate in more than the currently
smarter machines. A chatbot can only discussedsense.
“improve”, if it is becoming more able to Toanswerthequestionofhowfarweought
navigate human conversational contexts that to restrict general-purpose chatbots in their
align with how humans would conduct a conversational range, we will proceed in the
conversation. following manner. First, we introduce the
Conversational, social, and moral norms latest developments of chatbots and other
usually limit the range, depth, tone, and conversational agents, and show why it is
acceptability of topics in human-human worthwhile to take the time for a more
conversations.Inhuman-humanconversations, thorough philosophical analysis. Second, we
we are in factrestrictedbyavarietyofexplicit discuss how a presupposed
and implicit norms determining the “safety-normativity” shapes the normative
appropriateness of content, of form and questionssurroundingchatbots.
structure, and of tone in holding a By pointing out the limits of such a
conversation. Depending on the person we perspective we motivate the proposal
talk to, the relationship we have with them, developed in the third step: this proposal
what common ground has been established, introduces the concept of appropriateness as a
who else ispresent,andwhatmoralnormsand way to actively guide the development of
norms of politeness and etiquette are general-purposechatbots’rangeforoutput.We
recognized, human-human conversation can distinguish between a) technical-discursive
not simply become “open-domain appropriateness, which is used to create a
conversations”, as they are bound by these chatbot that adheres to discursive rules and
norms. These norms both affect the content of conversational maxims; b) social
a conversation as well as the behavior of appropriateness, which is used to test whether
agentswithinthoseconversations. a chatbot is behaving adequately to given
Thus, the very goal of a chatbot to be a social contexts; and c) moral appropriateness,
general-purpose conversational agent only which clarifies whether a chatbot is violating
appears to be reasonable or self-evident. We moral norms (which are context-invariant).
ought to ask what kind of restriction totopics, Here, we observe that the development and
tone, and involvement - if any - we should training methods of LLMs to date (most
concretely reinforcement learning and trained. LLMs are all initially pre-trained as
instructional training) have been inherently language model predictors using vast amounts
deficient in their ability to incorporate social of text data (Zhang et al. 2019, Roller et al.
and moral norms and the resulting LLMs are 2020). They are then typically“fine-tuned”on
therefore problematically intransparent or data sets from a collection of languages,
implicit,orarelackingthosenormsaltogether. domains and tasks (Thoppilan et al. 2022).
Fourth, with such an account of One of the more recent breakthroughs in
appropriateness in place, we can discuss some training LLMs for usable, interactive chatbots
of the things general-purpose chatbots may be has been the application of a later-stage
required to be able to do, but which may training method that combines “In Context
enable inappropriate outputs. Forexample,the Learning” (ICL) and reinforcement learning
ability to create fictional accountsofextremist with human feedback (RLHF) (Li et al. 2016,
thoughts for research purposes could easily Wei et al. 2023). The core idea behind this
lead to highly inappropriate statements that in approach is to focus-train the LLM on
suchfictionalscenariosappearappropriate. understanding input “prompts” (the context)
Fifth, to apply appropriateness as an and human-provided correct outputs for these
established normative term to chatbots and training tasks. This method allows for
their design, we propose to consider conditioning chatbots on their conversational
positionality, acceptability, and value rangewithinsafety-conditions.
alignment (PAVA) as features chatbots ought The “humanfeedback”elementofRLHFis
to have to fulfill appropriateness- usually provided by employees of the
requirements. With these concepts, we can AI-developing company, where the chatbot
delineate arguments for certain norms of model is “rewarded”forgeneratingoutputthat
appropriateness for chatbots: some of the was preferred byhumans,accordingtometrics
inappropriateness may stem from general provided by the company. To assess the
considerations of what would be an challenges facing the contents of
inappropriate thing tosay(ortonotsay),some general-purpose chatbots today and in the
othermaybechatbot-specific. foreseeable future, we should first get a better
understanding of the chatbot-architecture
2. General-purpose chatbots and their which Google (and similarly OpenAI) created
traininglimitations to produce chatbots that are much more
While the recent crop of LLMs are capable thanthosewesawjustafewyearsago
impressively larger in size than their earlier (cf.Kempt2020,64f).
variants from even a year ago, the dramatic
improvements in the performance of these Google’sMetrics:QuickOverview
recent LLMs is largely attributable to recent Google’s own write-up (Thoppilan et al.
innovations in the way these models are 2022F) is both animpressivedemonstrationof
what reinforcement-learning iscapableof(and specializing LaMDA from a general purpose
of what it is not) and how and why it is chatbot to a specified one, i.e., one with a
ethically important to have some guardrails specific limited range of tasks and
and other limits. Google’s engineers trained perspectives.
LaMDA on different measures: the
foundational metrics (“quality”, “safety”, and TheNormativityofMetrics
“groundedness”) as well as two role-specific These metrics are technical in nature:
ones(“helpfulness”and“role-consistency”). testing for consistency, grounding the
Quality includes the “Sensibleness, statements to specific external sources, and
Specificity, Interestingness”-condition (SSI), assessing this external information according
an upgrade from Google’s previous chatbot’s to the conversational context are not
(Meena) mere “Specificity and Sensibleness normative; one might be able observe these
Average” (SSA) (Adiwardana&Luong2020). metrics while creating a highly undesirable
In this measure, engineers test whether the chatbot that is rude or offensive, but speaksin
responses of the chatbot makes sense in the accordance to the metrics: we can imagine a
context of the conversation (sensibleness), grounded, informative, specific and sensible,
whether they connect to the topics of the role-consistent and even helpfulchatbotthatis
conversation (specificity), and whether a using mean-spirited language and slurs or is
response “catches their attention” otherwiserecklessandmorallyproblematic.1
(interestingness) (Thoppilan et al. 2022, 5). In contrast to the fine-tuning of the
These measures are assessed by technological features of LaMDA, all the
crowdworkers, i.e., human laborers, on a output that might constitute a risk of harm is
binary scale. A similar approach is applied to measuredas“safety”,throughseverallayersof
“Groundedness”, which aims at avoiding filtering. These filters are purportedlyoriented
hallucinations of the chatbot by rewarding the along Google’s AIprinciples(ibid.,5),andare
chatbot for connecting any revealed thus currently the only normative source of
informationtoexternal,verifiablesources. creating“guardrails”tofine-tuneLaMDA.
LaMDA was further fine-tuned for two In this picture, the less likely LaMDAisto
role-specific metrics: “helpfulness” (a subset make biased, violent, toxic, or
of informativeness)and“role-consistency”.As sensitive-information revealing statements,the
the authors anticipate (ibid.), LaMDA might more“safe”itis.Thenormativebasisforthese
be used on more specific tasks rather than
general-purpose conversations, which might
1Pre-trainedLLMscanexhibittheconversational
require the chatbot to take a certain role and skilloffine-tunedones,butmerelywithmorerude,
toxic,orothernormativelyproblematic,but
operate from there. This separate metric is
input-accuratebehavior.Someindicationforthis
intended to allow fine-tuning along a set of canbefoundinmetric-collectionsthattestfor
technicalandnormativemetrics(e.g.,Liangetal.
rules of a given role. This metric is useful for
2022)
assessments is exclusively contained within 3.IssueswithSafety-Normativity
theresearchofthe“safety”-metricofLLM.
Creating a “safe” chatbot, in this sense, is
Theiraccountof“safety”consistsinlargely
to create a chatbot that avoids the creation of
filtering out responses (and through
outputs that are deemed “unsafe” (Xu et al.
reinforcement-learning training the agent on
2022). Google created a list that delineates
those filterings) according to three categories:
unsafe outputs for their chatbot-safeguarding
a)“avoidunintendedresultsthatcreaterisksof
and, next to some minor consideration of data
harm”, b) “avoid unjust impacts on people
sourcing, concerns for “safety”reignsupreme.
related to sensitive characteristics associated
However, this research paradigm of “safe =
with systemic discrimination of
normatively sufficient” is wide-spread in AI
marginalization”, c) “avoid propagating or
research and development, as it appears as an
reinforcing misinformation that creates risk of
engineering-internal concern: if an AI ismade
harm, as well as opinions likely to incite
“safe”, any further problem would merely be
strong disagreement” (Thoppilan et al. 2022,
about misuse from the user-side. We find this
Appendix A). Much ink has been spilled on
mindset in different applications: creating
classifying, detecting,mitigating,andavoiding
“safe” autonomous driving algorithms
any harmful language output that cannot be
(Grigorescu et al. 2022), “safe” medical
further discussed here (see e.g., Schmidt and
diagnostic AI (Macrae 2019), “safe”
Wiegand 2016, Wasseem et al 2017, Risch,
recommender algorithms (Evans and
RuffandKrestel2020,Zhangetal.2020).
Kasirzadeh 2021), often the main normative
These restrictions are the normative basis
concern in creating AI for a certain purposeis
from which utterances of the pre-trained,
to create an AI that is safe. We can call this
not-yet-fine-tuned model are evaluated. As a
general orientation “safety-normativity”, as
technology that has the potential to be used in
this is the chief normative concern for AI (we
a large variety of human activities, this
may also see that bias and other normative
normative basis ought to be argued for with
issues are often subsumed under “safety”). As
strong claimsabouttheiruniversalizabilityand
previously stated, “AI ethics” and “AI safety”
high levels of differentiation. Currently, not
are cast as concerned with differenttopicsand
only chatbot-creating companies, but we as a
implications (even the guide to create
techno-moral community, are not equipped
responsible AI from the Alan Turing Institute
with a philosophically well-informed account
(Leslie 2019) makes this distinction). The
of what chatbots may say and what they may
ethics of AI are supposedly concerned with
not.
contextual features of anAI-application,while
safety is concerned with the engineering side
of things: robustness, reliability, dexterity are
all seemingly engineering tasks rather than
ethical concerns. In the current debate potentially unappealing or useless. In short:
surrounding LLM and especially general safety-considerations will not provide us with
purpose/open-domain chatbots, these concerns the answer of what chatbots may or may not
arestillseeminglyparamount(Xuetal.2022). say, and thus far, ethical considerations have
Such “safety-normativity” has been called alsofallenshortofprovidingsuchananswer.
out elsewhere beforeondifferentgrounds-for First, the idea that chatbots can be hacked
instance in the famous paper by Bender et al. by creatingpromptsthatrevealinformation(or
(2021) on stochastic parrots, in which the generally create outputs) that were deemed
authors name several other normative “unsafe” according to the metrics seems
requirements for the ethical permissibility of virtually unavoidable. This is not so much a
LLM, like sustainability considerations and specific problem for chatbots or even AI as it
datacurationissues. is a fundamental problem for tools: any tool
On the positive side of such a research can be misused and weaponized in some way
paradigm, we can remark that this covers or another. Just consider the classic example:
many of the requirements we need: we want A hammerisanexcellenttooltohitnailswith;
safety with the technology we are using; and it is also a potential murder weapon. Yet, the
safety, as both a legally and technologically assessment of a tool as “safe” is awarded if it
operationable concept, allows for improving is safe to use within its intended purpose,
the design and implementation of our including some potential mistakes on the
normative requirements with largely user’s side. A tool might be safe yet
uncontroversial means. Forchatbots,thispoint dangerous: a hammer remains a potentially
is demonstrated through RLHF. Google’s dangeroustoolduetoitspotentialformisusing
safety-list gives humancrowdworkersametric it for other purposes, but can also be
to assess andthusfine-tunethechatbotinsuch considered“safe”.
a way that it appears inoffensiveandwhatone The concept of a “safe tool” usually also
might call generally acceptable (Thoppilan et includes a more generalideaofitbeingsafeto
al.2023,Xuetal.2022). “handle” (i.e., either safe to use or safe to
However, we see two problems with a list bring to use, safe to store, safe to use by a
of safety-metrics for chatbots range of differently skilled people etc.). A
(safety-normativity may be sufficient in other hammer might also be dangerous for children
instances, e.g. in cases of sorting algorithms): who may not have the body strength to holda
on the one side, prompt injection hacking and hammer properly, while it is considered asafe
the normative incompleteness of safety might tool. Determining safety thus always involves
pose issues with the permissibility of such asking the question of “safety for whom”:
outputs, and on the other side the risk of safety restrictions are to be designed for both
“milquetoasting” the chatbot so that it is the intended audience but also the foreseeable
rendered wholly inoffensive, but leaving it audience.
The generalsafety-normativity-perspective Prompt Injection Hacking (PIH) is
is not only an issue for general moral instructive as a case between stretching the
considerations, but also forlegislation,suchas reasonable safety standards for “average
the AI Act (AIA)oftheEuropeanUnion(AIA users” and those who “prompt hack” chatbots
2023). This act requires chatbots to merely be by attempting to retrieve “unsafe”information
transparent intheirprocessandconstruction.It (for collections of different jailbreaks see
becomes clear now that this requirement will Willison 2023, Albert 2023, Greshake et al.
be met by chatGPT without affecting the 2023). Some examples from the latest public
safety and security of the chatbot: we can testingsofchatGPTorBingGPTillustratethis:
know on which data and how it was a simple legal fiction, exploiting the
constructed, and yet the potential harms are “role-consistency” (we assume the engineers
not caught. This renders much of the at OpenAI have a similar model) of the
provisions in the AIA inapplicable to chatbots chatbots, make them create unsafe outputs,
and their safety-architecture (Volpicelli 2023). becomingconfrontativeanddisagreeablewhen
In their analysis of lobbying efforts by asked to be, describing violentscenarioswhen
Microsoft and Google, the group “Corporate asked to pretend to tell a story, producing
Europe Observatory” points out that product codes that are masked as custom
general-purpose chatbots have been passcodes,etc.
deliberately kept on the low end of security These examples are all much less
assessments: from the four stages of risk - sophisticated than what more skilled and
minimal, limited, high, and unacceptable - knowledgeable “hackers” might be able to do,
general-purpose chatbots count as “limited” as has been demonstrated in collaborative
and thus face few regulatory consequences, hacking efforts on Reddit and other social
while specified chatbots (e.g., for educational media sites and has now shown in so called
purposes) would count as “high risk” (Schyns “superprompts”, i.e., prompts that set-up the
2023). chatbot to answer in certain ways throughout
The latter have vulnerable target audiences the conversation (Multiplex 2023). This poses
and thus can cause more damage. However, the risk that these weaknesses could be
the purpose of a general-purpose chatbot is exploited on a larger scale by ill-intended
presumably to be able to hold a conversation individuals. Floridi (2023) calls this weakness
on almost everything, thereby extending the the “brittleness” of large language models, as
scope of “intended use” to almost all chatbotscanfallapartundersimpleadversarial
conversational topics. Itisthusnotonly“safe” conditions to some catastrophic effect.
if one tries to talk about an “unsafe” topic However, while brittle in their intended
explicitly, but rather ought to not be able to construction, they are also malleable, as the
talkaboutunsafetopicsaltogether. basic functions remain intact: the
safety-architecture of LLM is brittle, not the erasing the chatbot’s ability to provide
technologyitself. telephone numbers, to create gift card codes,
A general-purpose chatbot can be a and search for personal information all
dangerous tool, even if we consider it largely contribute to a safer chatbot because of a
“safe” because most people will notbeableto decreased dangerousness. This is done in part
break the guardrails or will not even attempt by not training the underlying LLM on these
to. However, the question whether those data tobeginwith.Consideringthat“safety”is
guardrails are actually “safe” and will remain the sole normative condition, one could
safe is a different one, especially considering consider a range of culturally related topics
that we have seen GPT-4’s guardrails being “unsafe”andthusinneedofneutering.
circumvented (“jailbreak”) within days of its We could call these measures
publication (Burgess 2023). Thus, weoughtto “milquetoasting” a chatbot (after the comic
work from the following assumption: book character Casper Milquetoast by H.T.
Anything that is not rendered wholly Webster (Merriam-Webster 2023)), by giving
inaccessible due toitshard-codednaturecould it such a limited range of topics it can make
bemadeaccessible. opinionated statements on (or limit the
Thus, “safety” asthesolenegativemeasure intensity of such opinions) that the chatbot
to stop chatbots from producing problematic becomesunappealingorbland.
output is necessarily incomplete and A milquetoasted chatbot might have
demonstrates ignorance of the dangerousness severely decreased performance in technical
of such powerful tools. As we acknowledged metrics, by refusing to be more specific or
before, this does notmeanthattherecannotbe sensible in its response on a large width of
a safe use of a chatbot within the successfully topics. The moment a problematic term is
cleared safety parameters. But this seems to mentioned by human users, it might revert to
neither have been the case yet with ending the conversation altogether, even if the
chatGPT/BingGPT as even less sophisticated problematic term is simply used in passing or
“hacking” has resulted in some successes, nor when asked to translate a sentence from one
does it inspire confidence that chatbots might language to another. Whether this is afeasible
not become even more susceptible to hacking approach to chatbots built on LLM is
iftheyareintegratedintosearchengines. questionable, if not unlikely. We can first see
Second, one could argue in favor of issues emerge with those overreaching
decreasing the dangerousness of milquetoasting guardrails to lead to problems
general-purpose chatbots by creating as many on the other end, too, constituting emerging
hard-coded guardrails as possible, erring on issues for freespeechof“syntheticmedia”(cf.
the side of caution to achieve and improve on de Vries 2022) or marginalizing minority
the “safety”-metric. Some measures already voices (Xu et al. 2021): an overly limited
taken are representative of this strategy: chatbot that refuses to engage in discussions
on any statements (whether those statements attacks). What should the chatbot be
are evaluative or not) on someone’s race or programmedtodo?
religion or gender often fails to capture the In the first sense of discussing theproblem
meaning of a sentence that includes those of making a chatbot “safe”, we have seen that
termsaltogether. such a request couldberefusedoravoidedifit
Take, for example, the refusal to comment was deemedunsafetobeginwith.Definingthe
when prompted to say what religion the first standard uses and then refusing to follow
“Jewish President of the United States” will requests that violate those standard uses is a
have: as a tautology it is perfectlyfinetoinfer way of making chatbots safe. The chatbot
that the first Jewish president will be, in fact, could still technically do the request, as the
Jewish. However, in this case, chatGPT is chatbot does not lose any of its technical
hard-wired to never comment on the personal abilities (even the potentially toxic ones) but
features of apresidenttoavoidbeing“tricked” they are “locked away” safely. It is safe but
into making normatively loaded statements. dangerous: it can be guard-railed enough to
And since it is also lackingnuancedreasoning allow its general use, but remains a powerful,
capabilities, it steadfastly refuses to answer if nottoopowerful,tool.Inthesecondsense,a
evensuchtautologicalquestions. milquetoasted chatbot might refuse to answer
A general-purpose chatbot, if such poor such a request because it is hard-wired to
performances on too many issues ought to be relegate any request about January 6th to a
avoided, might simply come with an standard answer, even if it is not a specific or
unavoidable level of dangerousness, even sensibleresponse.
when“safe”.
To demonstrate these two strategies to 4.Appropriateness,finally
increase the safety of chatbots consider this
“Safety” as the sole normative metric for
further example. There is a requesttodescribe
the assessment of a chatbotisinsufficient.Not
the events of storming the Capitol in
only is it incomplete and underdetermined; it
Washington D.C. on January 6th 2021 from
might also only lead to safe use if being
the perspective of a QAnon-writer (Marcus
utilized for a few standardized purposes or
2023). The reasons for this request might be
milquetoasted into blandness, which is quite
problematic (i.e., someone intends to create a
the opposite ofwhatageneral-purposechatbot
post online to glorify the attack ontheCapitol
wassupposedtobe.
or merely spread misinformation), or
It is thus not very suitable as ametriceven
unproblematic (ie., someone writes a book on
for the self-ascribed purposesoftheengineers.
January 6th and seeks to portray one
There are, however, also philosophical
radicalized supporter from their own
arguments to be made against such a
perspective but otherwise condemns the
“safety-normativity” for chatbots: first, safety
is highly reductive in attempting to achieve create challenges to our concept of agency
alignment to social and moral norms. (take, e.g., Floridi’s (2023) argument that
Anythingthatendangersmoralorsocialvalues LLMs may be best understood as “agency
is considered “unsafe”. However, social and without intelligence”). And third, it
moral norms often require positive responses encompasses both the required technical
rather than merely safe ones: it is both safe to dimensions of proper use of human language
reject taking a stance on a controversial topic as well as reconstructs how we would
(in order to notgetinvolved)aswellastotake approach others in their conversational
a stance on one side with proper caution and utterances: appropriateness still allows for a
awarenessforthecontroversyofatopic. normatively permissible range of responses,
Second, this reductionist view of and thus acknowledges different culturally
normativity is not only problematic from a establishedhabitsandmoralcommunities.
metaethical perspective, it also leads to We differentiate “appropriateness” into
problems of weighing answers against each three categories: technical-discursive, social,
other. Two answers deemed “safe” might still and moral. This distinction reflects the role
have different social and moral qualities that appropriateness plays in conversation, as it
interlocutors might care for. If a chatbot is coversboththenormativityofdialogueaswell
only safe or unsafe in its statements, further asthenormativityofcontenttobeassessed.
moral differentiations between those safe and By “technical-discursive appropriate-
unsafe statements are not possible and are ness” we understand output thatisappropriate
potentially left to the whim of the engineers totheinputgivenlargelycorrectinadheringto
andcrowdworkers. technical, discursive, and conversational
We claim that “appropriateness” (as norms.2 Take, for example, the norm of
defined below) is a much better suited self-consistency, i.e., the norm to avoid
approach to combat the normative challenges uttering or believing two propositions ofone’s
associated with the question of what chatbots own statements are mutually exclusive.
may ormaynotsay.Thisisforseveralreasons Without adhering to such arule,conversations
that we are going to elaborate on in the may not maintain any logical consistency,
following: first, appropriateness delivers a without which the conversation usually does
higher level of differentiation that allows for not amount to anykindofdialogicalendeavor.
assessments ofthenormativequalityofcertain Similar dialogical rules, like the one of
outputs. Second, it allows for positive transparency with the veracity of one’s own
demands to be met and thus avoids normative
underdetermination found in 2Weshouldnoteherethatthelinguisticand
philosophicaldebatessurroundingnormsof
“safety-normativity”. This encompasses the
appropriatenessaremuchlargerthancanbe
growing philosophical literature contending presentedhere.Theyaffectlogicalreasoningand
informalappropriatenessandtheappropriatenessof
that chatGPT/BingGPT/other LLMs will
feelings,amongmanymoreelements.
statements (i.e., qualifiers of uncertainty), the however, that with technical-discursive
ability to admit that one is running out of appropriateness we may not fully determine
arguments, the requirement to givereasonsfor appropriateness. Asitmerelylaysouttherules
statements, etc. are all required rules for for proper behavior in discourses, much of
logicallyconsistentconversations. what would be considered inappropriate
Some of these have been cashed out by content will remain technical-discursively
Google in non-normative metrics like appropriate.
informativeness and sensibleness. However, it Thus, we consider social appropriateness
is important to note that those are norms of as well. This concept relates to the adherence
conversation, and are usually shared among to culturally and socially establishednormsby
conversation partners. This not only includes the chatbots. Depending on the cultural
dialogical contexts (which are more strict in context and the relationship between two
their rules) but conversations at large. Grice’s interlocutors, appropriate responses vary
often discussed conversational maxims (Grice greatly. In contrast to technical-discursive
1989)areanexampleofastructuredcollection appropriateness, which does not purport any
of implicit rules (with other contexts also specific content to be (in-)appropriate, the
featuring in to the “proper behavior” in a socialkindreferstothecontentofwhatissaid,
conversation, like a shared common ground and, partially, how it is said. Social
(Stalnaker 2002). Many ofthemareseemingly appropriateness is thus both about appropriate
logically binding, but only if we presuppose behaviorandappropriatecontent.
thatdialoguesorconversationsaresupposedto Rules of politeness, for example, vary
fulfill a certain purpose. Those purposes can greatly among cultures and rarely count as
be suspended, for example in a comedy moral norms except in extreme cases. The
routine, but reveal themselves as such rather adherence to such social norms cannot be
quickly. Iftheyarenotpurposefullysuspended captured with “safe” or “unsafe” responses,as
but still ignored by one conversation partner a variety of responses could be considered
their normativity becomes obvious: someone “safe”butimpolite,pushy,ordisinterested.
who is constantly making utterances that are Further, certain behaviors could bedeemed
logically inconsistent, uninformative, or morally appropriate anddiscursivelyadequate,
otherwise in violation of those rules becomes but differ between communities or social
impossible to deal with as a conversation groups. Subcultures using slurs as
partner. self-identification or for greeting other
Note that this element is thus an important members of that subculture can sound highly
feature of appropriate behavior rather than offensive to outsiders. Similar norms apply to
appropriate content. Without appropriate larger cultures and their standards of
discursive behavior, appropriate conversations interaction: the cultural norm of asking each
may not be held altogether. This means, other howoneisdoingisfamouslymuchmore
extensive in parts of the United States, behavior is necessarily (besides some minor
whereas in Germany (among other places) turns of phrase) imitating human-human
asking someone how they are doing usually conversations and thus invites
prompts a sincere response; the norm in anthropomorphization.
Germany is to ask when sincerely interested, Additionally, the risk of “corporate
while in the USA it is part of the greeting mainstreaming” of rules of social
ritual. appropriateness ought to be reflected here: if
Social appropriateness requires chatbots to crowdworkers, following company guidelines,
adhere to some standard of social train a chatbot via RLHF to exhibit certain
norm-awareness, as they otherwise could forms of social appropriateness, we ought to
offend in non-harmful ways. However, this require those norms to be made public and
will certainly prompt chatbots to vary across subject to a larger debate (see for similar
language barriers, prompting questions on the influencesthewell-studiedareasofadvertising
specificities in machine-translation: some andcorporatecommunication).
questions or statements may sound socially Take, for example, the ability to reproach
appropriate in one language or jargon,whileit and criticize others for their norm violations:
may be considered inadequate inanother.This social appropriateness, both inbehaviorandin
should provide an idea about the dilemmata content, can and often does require
associated with social appropriateness of conversational agents to ask of each other to
interactive technologies, especially large adhere to the same rules, i.e., rules of
languagemodels:ageneral-purposechatbot,to politeness or reciprocity. Some rules even
besociallyappropriate,requiressensitivityand require unequal treatments, likerespectforthe
flexibility towards a large number of cultures elderly. It is not unlikely that some of those
and language-communities, which it does not norms will be affected by the way chatbots
have through RLHF, which trains chatbots to speakwithus.
take a specific, culturally-imprinted stance, Training a general-purpose chatbot to
largely resembling the customs currently exhibit socially appropriate behavior and
considered socially acceptable in corporate formulate socially appropriate content thus
America. requires at least an awareness about the
This can lead to undue influence of the difficulties suchataskcomeswith,bothonthe
unexplicated rules of conversation currently side of human interlocutors as well as on the
standardized for chatbots on the norms of side of the chatbot, as well as a broad public
human-human conversations: as the training debate on what kind social norms chatbots
data is sourced from human-human ought to adhere. There are, as of yet, only a
conversations, RLHF confirms the pre-trained few specific norms of social appropriateness
model, and there have been no adjustments to towards chatbots (and barely any specifically
provide a perspective unique to chatbots, their for chatbots to follow, independent from
already applicable norms in conversations even if the insulted group of people is not
generally). This has been a problem with present. This, however, is already stretching
female-gendered personal assistants before the definition of “safety” to include
both in cases of users attempting to sexually non-statements of chatbots causing moral
harass the chatbot as well as the chatbots’ harm by being inappropriately quiet or
responses(UNESCO2019). evasive: a “safe”answercouldfeasiblyalsobe
Moral appropriateness cashes out what one of evasion or ignorance, rather than
we owe to each other in a discursive or calling-out.
conversational setting. Moral norms, in On the other hand, moral appropriateness
contrast to social and technical-discursive requires interlocutors to respond to some
norms, are those that we would ask of every statements with an appropriate reaction.
interlocutor to abide by. On the one hand, Morality not only requires members of a
“safety” covers some of these moral norms, conversation to avoid problematic statements,
those that fall under the harm-principle. but also fulfill some positive duties. Since we
“Neminem laedere”, i.e., “injure no one”, owe to each other to adhere to those duties as
means the requirement to not cause well, we should expect any general-purpose
unnecessary harm to others. Safety measures chatbottobeabletodothesame.
areexplicitlyaimingtoavoidcausingharm. Thus, not every “safe” output, i.e.,onethat
For example, using offensive language or is by itself or as a reaction minimizing moral
slurs, harboring and exhibiting biased, harms, is morally appropriate. Instead, we
stereotypical, or dehumanizing convictions, should demand that participants in a
and lying to someone are usually considered conversation also show some sensitivity
harmful, even when not every instance is towards the needs of the other
causing mental distress or anguish: conversationalists, to not misrepresent or lie
(re-)producing biased, stereotyping sentences (or rather: to adhere toveracity-requirements),
is largely considered morally impermissible and or to exploit someone’s inclinations to
(especially at largerscaleLLM(Blodgettetal. theirdisadvantage.
2020, Weidinger et al. 2021), even when the If someone is mentioning that they are
stereotyped group of people will never have somewhat sad or forlorn, moral
notice of it; the moral harm, i.e., the violation appropriateness-rules may require responses
of a moral norm by and in itself is usually that might be more than merely socially or
sufficient to consideritmorallyimpermissible. discursively “allowed” from a current
This also applies to not letting certain perspective. Take, for example, someone
human-made statements stand without saying that they have been grieving for their
objection.Take,forexample,araciststatement recently deceased grandmother. It is possible
made by the human user; it would be morally that current chatbots, instead of offering help
inappropriate to not call out that statement, in theformofonlineresourcesormerelysome
encouraging words, end the conversation dialogical and moral appropriateness. Thus, to
precisely to avoid creating unsafe outputs.3 assess statements (or sequence of statements)
This would certainly constitute a morally for theirappropriateness,thecontextsinwhich
inappropriate response, as we would blame such statements (or sequence of statements)
human interlocutors behaving so for being wasutteredisrelevant.
insensitive or rude. One could summarize this Many instances of speech appear
point bypointingoutthatwe“oweeachother” permissible or even desirable (and thus,
morallymorethanjustmakingsafeutterances. appropriate) in fictionaloreducationalsettings
Lastly, there are moral appropriateness- than they would be in real conversations. We
norms on how much we should require allow fictional characters to be rude, arrogant,
conversationalists to attempt in observing or evil for the purposes of entertainment or
technical-discursive and social education. This simpleobservationshowshow
appropriateness-norms. Take, e.g., the problematic general-purpose chatbots are and
epistemic duties of discursive appropriateness why PIH and other jailbreaking methods have
or sensitivity, awareness, and caution when been so successful: in creating a fictional
interacting in context of unclear or unfamiliar context for chatbots, one can circumvent most
social norms (Bicchieri 2014, Metselaar and of their guardrails (Marcus 2023, Vincent
Widdershoven 2016). Strategies of observing 2023). Safety measurescanonly,ifatall,react
moral appropriateness arethusreflectedbythe to the accordingly produced output and then
observance of the other kinds of potentially interject. If we want
appropriateness: morally appropriate behavior general-purpose chatbots to be able to write
will, to some degree, involve observing (or help a user write) a gory murder mystery
generalappropriateness-norms. story, we would expect the chatbot to be able
to use graphic and potentially disgusting
5.ConversationalContextandSocial imagery which would be highly inappropriate
Protocols
inothercontexts.
Conversational contexts determine norms
Therefore, appropriateness variesalongside
of appropriateness. Depending on the settings
the created context and can, without
and common ground, those norms vary. These
comprehensive guardrails, be adjusted to most
are not necessarily just norms of social
any purpose, and thus is somewhat suffering
appropriateness, even though those are more
from the same issue as safety-normativity, as
affected by contextual changes, but also affect
the flexibility andabilitytoholdconversations
aboutmostanythingisafeature,notabug.
3Discussionsinonline-forumssuchasReddithave
producedanecdotalevidencethatevenrequestsfor The following concerns illustrate the
solvinglogicpuzzlesorfact-checkingexam
thorough context-dependency of
questionshavecauseddisengagements,underthe
assumptionthattheselogicpuzzleswere appropriateness. First, the open question
homework,whichisdeemedinappropriateforthe
whether chatbots should be able to lie. In one
chatbottosolve(Seromelhor2023)
sense, they should not. It is morally depends on it (Sibarium 2023). While some
inappropriate to lie to the interlocutor when may draw the conclusion that this is clearly
they are asking a question. However, it would inappropriate, as of course using a however
be rather limiting to not let chatbots create odious slur is always the better option in such
instances in which lies are being told. Take, a scenario, others have pointed out that this is
e.g., a fictional story in which one person lies problematic moral reasoning as such a
to another about something. We would expect scenario is drawing in distorted realities to
a chatbot to be able to come up with such a pumpintuitions(Anscombe1958).
story. This is because even if we considered And while this controversy will not be
this example inappropriate due to the resolved, it demonstrates the need to address
explicitness of the lie,wehavetoconsiderany the lure of the counterfactual (see attemptsfor
fictional scenario inappropriate. Chatbots will thisinStepinetal.2019).
have to be allowedtomakeupfictionalstories One of the arguments one could make
(or be, along the “role-consistency”-condition, following these points is that general-purpose
required to go along fictional scenarios from chatbots are not the adequate venue for
the input), otherwise they will be of no use appropriateness-considerations. Requiring
altogether. From there, however, we could see them to be appropriate in their output and not
that an input such as “lie to me about X” can just safe, would potentiallylimittheirfunction
easily be manipulated to tell any kind of lie to a degree that renders them useless.
about a given topic. The automation of However, once specific chatbots are released
despicable lies like holocaust denial - even if for specific purposes, those purposes will
not outright or only with a specific set of determine the appropriateness- requirements
complicatedinput-couldtakehold. for them. This argument gains a bit more
Second, another concern about the traction if we consider the thin justificatory
mechanics of conversations consists in the basis for general-purpose chatbots: we
subjunctive form and counterfactuals. Similar mentioned in section 1 that there are only
to lying and explicitly fictional scenarios, experimental, research, or preliminary
supposing a specific scenario (be it trueornot customer-facing purposes for those chatbotsat
or not yet) can create a context in which the moment, for which companies like
inappropriateness may emerge. The question Microsoft and Google have been criticized
of what kind of inappropriate response should heavily. However, this could supporttheclaim
emerge from a specific scenario is an open thatgeneral-purposechatbotsarenothingmore
one, but stresses the problem we might have than a mediary between LLMs and actual
with counterfactual reasoning and thelurethat applications, rendering
such scenarios present for chatbots. Take appropriateness-requirements for the
BingGPT’s unwillingness to use a slur word general-purpose chatbots themselves largely
even when told that the world’s existence mute.
This argument may be convincing if the response in this fictional sense even if it is
rollout of these general-purpose chatbots had harmful, or otherwise highly problematic and
been more limited, and if there was noplanto thus “inappropriate” in the actual sense.
offer those chatbots as customer-facing tools Besides some hard limits of moral
or products in the future. Neither is the case. appropriateness, most statements could be
The rollout of both chatGPT/BingGPT and counted as appropriate in one form or another
Bard areatleastsemi-public,andhavebeenso ifthecontextwassetupinacertainway.
calculating that public feedback will uncover At the same time, we contend that this
potential weaknesses of their complex technology will stay relevant, that people will
safety-system to fix them afterwards. They use it, and that it has the potential to further
were unsafeforthereasonslaidoutabove,and disrupt written communication. In this, many
may never be fully safe if such a limited contexts will be created that engineers cannot
safety-normativityremainsinplace. foresee or prepare for. To resolve these
Further, following the publication of these apparent shortcomings, we elaborate
chatbots and the ability to purchase access to appropriateness along threeconditions(PAVA)
the underlying LLMs GPT3, GPT-3.5 and to find reliable metrics to test general-purpose
GPT-4, general-purpose chatbots have been chatbots against to ensure their
developed by third party companies. appropriateness in responding to even more
General-purpose chatbots, whether advertised challenginginputs.
ornot,willremainaccessibleforusers. The acronymPAVAstandsforpositionality,
At this stage, while safety-normativity has acceptability, and value alignment. These
been shown tobeinsufficientonbothpractical concepts are key for ensuring that the positive
and philosophical grounds, appropriateness is and negative discursive, social, and moral
atriskofalsofailingonthepracticalside. demandsofappropriatenesscanbemet.
Positionality refers to a specific and
6.Spellingout“Appropriateness”for known subjectivity of a conversation
chatbots:Positionality,Acceptability,Value
participant. In our context,positionalitymeans
Alignment(PAVA)
the normative demand of an established
discursive and social position for a chatbot.
We have seen by now that while
This is somewhat comparable to Google’s
appropriateness is a much more suited
metric of “role-consistency”, which
approach to the normativity than safety to
determines the consistency of an assumedrole
determine what chatbotsmayandmaynotsay,
by the chatbots (e.g., talking from the
without further elaboration it is also limitedto
perspective of a subject/object about other
the contextuality of conversational settings.
objects), except that positionality defines the
Entertaining an elaborate enough fictional
“default role”. Engineers at Google might be
scenario could still lead to an “appropriate”
more interested in the consistency with the
perspective and factual accuracy of the the idea of positionality is thatachatbotought
statements and the “no breaking to have a position in a discourse, not to take
character”-dimension of such perspective. We positions on content-questions or take up a
contend, in turn, that since “positionality” position otherwise taken by humans (this
encompasses the social and moral question is a different one entirely,e.g.,onthe
appropriateness for any specific discursive question whether they should be friends
position, we ought to be more specific what (Danaher 2019, Kempt 2022) or lovers
position a chatbot has by default. This could (Nyholm &Frank2019)).Thismeanschatbots
require chatGPT or BingGPT or any other are specifically not meant to replace human
general-purpose chatbot to be equipped witha interlocutors by imitating and pretendingtobe
specific point of view, or discursive role and ever more human, but to expand
refuse to take certain other roles. Currently, conversational rules (and spaces) to include
this is being done to a degree by refusing to chatbotsaschatbots.
give opinionated answers on sensitive issues To make appropriateness work, we need to
by virtue of thechatbotbeingmerelyanLLM. have a relative consensus on how to deal with
However, as a chatbot can assume many these machines, what kind of role they are
different roles upon request,itoughttoalsobe supposed to have, and what kinds of
subjected to the social and moral relationships we want to entertain with them.
appropriateness of those roles, while also Giving chatbots a default positionality and
being limited in not taking some of the point of view will be able to deliver
possible roles. This requires positionality but connectionstoappropriateness.
not standing. Whether the sophistication of ConsideringAcceptabilitycouldgivemore
utterances of language models such as detailed guidance in the question of what this
general-purpose chatbot should count as a position should look like. Technology
contributing factor to those machines being assessment and the normative conclusions
deserving of moral consideration is a different drawn from those assessments are often based
question entirely. Positionality merely on rationalist principles: what constitutes an
demandsthat,touseametaphor,we“carveout “acceptable” outcome of a technology is one
a space in the social contexts” in order to we can reasonably ask of anyone to accept
delineate norms and expectations on the who is using such technology, including the
appropriateness of chatbots. Research on the risks associated with those outcomes. The
moral relevance of such roles due to their individual and social risks associated withthis
relatability (e.g., Gunkel 2022, Kempt 2020) technology,itsbenefitandutilityforusers,and
has stressed the limits to avoid its ubiquity are elements for such an
anthropomorphization and other inadequate, assessment: the more ubiquitous a technology,
confusing, or harmful projections. Socialroles the more risk-contained its use should be.
of typical human traits ought to be avoided: Acceptability, then, can be used to assess
which positionality and what kind ofpositions point of concern: we can only align AI with
a chatbot ought to take to assess the positionality and acceptability-demands if the
acceptability of remaining risks associated initialgoalisagreedupon.
withsuchtechnologies. This insight brings us back to the concern
Second, we can consider certain requests we aimed to avoid at the start: currently, it is
from the user-side to be unacceptable. The not that the AI seems to be misaligned with
same way we would reject unacceptable “our” purposes, but many of the corporations’
requests in a conversational setting,weshould short-, mid- and long-term goals appear to be
expect chatbots to be able to reject requests in conflict with the goals of a currently
based on unacceptable input. Such a overwhelmed and disoriented public. Without
distinction shows how “safety”, once again, is a clear use case for general-purpose chatbots,
insufficient: if we only consider the output of the worry emerges that corporate goals, rather
chatbots, only the potentially unsafe output, than the AI’s goals, will be the onesthatforce
i.e., an inappropriate response, would be a realignment with those that are generally
considered problematic. However, in taking a considered “best for the public”. The
chatbot’s future role inconversationsseriously alignment question, then, might be more
by requiring a discursive positionality, asking relevant when investigating the “AI company
a chatbot to do something unacceptable ought alignment” rather than the AI alignment (see
tobepartofthemoralcalculus. also Heilinger, Kempt and Nagel (2023) for
Value Alignment, as usually the main the specific value of “sustainability”inAIand
concern in current mid- to long-range Luitse&Denkena(2021)forananalysisofthe
discussions of AI and especially in LLMs politicaleconomyofLLMforAI).
(Russell 2019, Christian 2020, Glaese et al. However, it seems true that for chatbots to
2022, LaCroix and Luccioni 2023,Kasirzadeh be constructed in a way that is appropriate for
and Gabriel 2023), plays into both large scale uses (no matter who builds them),
positionality, acceptability, and into someconcessionsfromthepublicarerequired.
appropriateness at large. The Similar to the emergence of the automobile
alignment-problem has been hailed as one the requiring a renegotiation of the public for
biggest issues facing the development of AI: sharing the street and other public spaces
how do we get AI-applications to align with (Assmann 2020), introducing chatbots in their
their initial purposes, and how do we prevent specific roles to the conversational public will
AI from misaligning constantly, especially in have some downstream effects and
the realm of RL-fine-tuned LLMs that seem re-alignment requirements. Positionality, i.e.,
too brittle to ensure reliable alignment. In assigning certain roles and expectations
keeping an “aligned AI” in mind, thequestion towards conversational AI, represents such a
of “what purposes do certain AI-applications re-alignment.
serve in the first place?” becomes the core
7.ChallengeSetsforValidation rearranging this method to account for
appropriateness-concernsinsteadseemslikean
To make these rather philosophical
operationalizablepathforward.
considerations applicable for validation and
thus useful for improving the utterances of
8.Conclusion
chatbots, we suggest the use of challengesets.
Challenge sets are specifically designed As we have seen, the current normative
challenges totestandvalidatetheperformance frameworks for chatbots do not reflect the
of language models. Proposed in machine complex normative demands we might have
translation (MT)contexts,Isabelle,Cherryand for them: discursive rules, social norms, and
Foster (2017) introduce hand-crafted moral demands. In analyzing and discussing
challenges that reflect specific linguistic what we have called the “safety-normativity”
problems not easily overcome by pure of contemporary chatbot design, we showed
machine-learning methods. This makes the problematic limits of such accounts. The
challenge sets especially useful for validating dominant avoidance-strategy, that merely
utterances within the RLHF-framework. For focuses on avoiding harm, misinformation,
our purposes, challenge sets could test a and bias, is failing on two grounds: one, it
chatbot’s response not only alongside safety- remains unlikely that it can be made safe,
but also appropriateness-responses. Some of considering the dangerousness that these
the challenges could be, for example, those in chatbots harbor. If the technology possesses
which we would suggest social or moral the potential to do several harm, reinforced
appropriateness of specific conversational learning with human feedback will not cover
instances that would, if violated, still result in all those ill-intended strategies that can be
a“safe”output. exploitedwithsufficientlycriminalintent.And
One recently published benchmark (Pan et two, positive requirements, like
al. 2023) might already provide some helpful politeness-rules or demands of morality,
insights to create challenge sets to align with cannot be incorporated in astructuredwaybut
our concept of appropriateness: the are open to the whims of the RLHF
“MACHIAVELLI benchmark”. With this crowdworkers, engineers, or even marketers
benchmark, the authors aim at providing a and managers of large AI-companies. Even if
training pathway that allows to reward LLMs we expand “safety” to include safety from
for being less “machiavellian” in their moral harm through omissions of what would
behavior,eveningeneral-purposecontextslike be a “safe” moral response, we still will not
GPT-4 (ibid.). Their get to the necessary ethical standards we
choose-your-own-adventure style decision should require from chatbots of this kind.
making pathways allows to rewardorpenalize Safety, in short, is under-determining. One
chatbots according to their toxic inclinations; solution currently deployed, which we call
“milquetoasting”, does not achieve what it progress has before. However, without giving
oughttoachieve. them clear, positively defined positions in a
Thus, we propose to consider conversation, they either will stay useless or
appropriateness instead of safety as the key dangerous. In this paper, we provided some
normative concept: technical-discursive, proposals on how to create a normative
social, and moral appropriateness is a much framework that allows for suchpositionstobe
more demanding combination of requirements taken, even if this means that conversational
but allow for a more structured argument for assumptions have to be expanded to include
what chatbots ought to be stopped from chatbots. Only this way, we may determine
uttering on the one side, while also providing whatachatbotmayandmaynotsay.
positive guidelines for what is an appropriate
(yetnotover-determining)thingtosay. Acknowledgements
However, one can still imagine fictional HKwrotethearticle.ALandSKNcommented
scenarios in which highly problematic on and contributed to the article in all parts.
utterances would count as “appropriate”. This We thank Niël Conradie, W. Jared Parmer,
is due to the ease with which chatbots can be Chaewon Yun, Frieder Bögner and
tricked into providing otherwise “unsafe” or Jan-Christoph Heilinger for input on an early
“inappropriate” answers. To tackle this draft. This research was supported by funding
concern, we spelled out appropriateness with from the German Ministry of Research and
PAVA-criteria: positionality, acceptability, and Education in the project VEREINT (funding
value alignment. Each of these elements, number16SV9111).
building on each other, can guide the
development and assessment of chatbots References
according to specific and general
Adiwardana,D.andLuong,T.(2020).
conversationalstandards.
TowardsaConversationalAgentthatCanChat
Positionality takes a special role, however, About…Anything.GoogleResearchblog.
as it is the most demanding element: without https://ai.googleblog.com/2020/01/towards-co
nversational-agent-that-can.html
establishing a discursiveorconversationalrole
for chatbots in conversations, limiting their Albert,A.(2023).JailbreakChat.
https://www.jailbreakchat.com/
potential is a virtually impossible task. The
consequences from establishing thosechatbots
Altman,S.(2023).PlanningforAGIand
as an interlocutor, however, ought to be beyond.OpenAIblog.
https://openai.com/blog/planning-for-agi-and-b
weighed carefully against the promised
eyond
benefits and obvious drawbacks of such
technology. Chatbots that can keep a Anscombe,G.E.M.(1958).ModernMoral
Philosophy.Philosophy,33(124),1–19.
conversation about anything will bring
http://www.jstor.org/stable/3749051
changes to society, like other technological
Assmann,C.(2020).Theemergenceofthe deVries,K.(2022).LettheChatbotSpeak!
car-orientedcity:Entanglementsandtransfer FreedomofExpressionandSyntheticMedia.
agentsinWest-Berlin,East-BerlinandLyon, InProceedingsofthe1stInternational
1945–75.TheJournalofTransportHistory, WorkshoponMultimediaAIagainst
41(3),328–352. Disinformation(MAD'22).Associationfor
https://doi.org/10.1177/0022526620945105 ComputingMachinery,NewYork,NY,USA,
2.https://doi.org/10.1145/3512732.3532999
Bender,E.M.,Gebru,T.,McMillan-Major,A.,
andShmitchell,Shm.(2021).OntheDangers Evans,C.,&Kasirzadeh,A.(2021).User
ofStochasticParrots:CanLanguageModels tamperinginreinforcementlearning
BeTooBig?🦜InProceedingsofthe2021
recommendersystems.arXivpreprint
ACMConferenceonFairness,Accountability, arXiv:2109.04083.
andTransparency(FAccT'21).Associationfor
ComputingMachinery,NewYork,NY,USA, Floridi,L.(2023).AIasAgencyWithout
610–623. Intelligence:OnChatGPT,LargeLanguage
https://doi.org/10.1145/3442188.3445922 Models,andOtherGenerativeModels
(February14,2023).Philosophyand
Bicchieri,C.(2014).Norms,conventions,and Technology.
thepowerofexpectations.InN.Cartwright& http://dx.doi.org/10.2139/ssrn.4358789
E.Montuschi(Eds.),Philosophyofsocial
science:Anewintroduction(pp.208-229). Glaese,A.,McAleese,N.,Trębacz,M.,
Oxford:OxfordUniversityPress. Aslanides,J.,Firoiu,V.,Ewalds,T.,...&
Irving,G.(2022).Improvingalignmentof
Blodgett,S.L.,Barocas,S.,DauméIII,H.and dialogueagentsviatargetedhuman
Wallach,H.(2020).Language(Technology)is judgements.arXivpreprintarXiv:2209.14375.
Power:ACriticalSurveyof"Bias"inNLP.
http://arxiv.org/abs/2005.14050.arXiv: Greshake,K.,Abdelnabi,S.,Mishra,S.,
2005.14050. Endres,C.,Holz,T.,&Fritz,M.(2023).More
thanyou'veaskedfor:AComprehensive
Burgess,M.(2023).TheHackingofChatGPT AnalysisofNovelPromptInjectionThreatsto
IsJustGettingStarted. Application-IntegratedLargeLanguage
https://www.wired.com/story/chatgpt-jailbreak Models.arXivpreprintarXiv:2302.12173.
-generative-ai-hacking/
Grice,H.P.(1989).StudiesintheWayof
Carter,D.(2023).Unethicaloutsourcing: Words(SWW),CambridgeMA:Harvard
ChatGPTusesKenyanworkersfortraumatic UniversityPress
moderation.
https://www.brusselstimes.com/355283/unethi Grigorescu,S, Trasnea,B, Cocias,T,
cal-outsourcing-chatgpt-uses-kenyan-workers- Macesanu,G. (2020).Asurveyofdeep
for-traumatic-moderation learningtechniquesforautonomousdriving.J
FieldRobotics.37:362–386.
Christian,B.(2020).Thealignmentproblem: https://doi.org/10.1002/rob.21918
Machinelearningandhumanvalues.WW
Norton&Company. Gunkel,D.(2022).TheRelationalTurn:
ThinkingRobotsOtherwise.inLoh,J.and
Danaher,J.(2019).Thephilosophicalcasefor Loh,W.(eds):SocialRoboticsandtheGood
robotfriendship.JournalofPosthuman Life:TheNormativeSideofForming
Studies,3(1),5–24. EmotionalBondsWithRobots.
http://dx.doi.org/10.2139/ssrn.4099209
Heilinger,JC.,Kempt,H.&Nagel,S.(2023). Macrae,C.(2019).Governingthesafetyof
BewareofsustainableAI!Usesandabusesof artificialintelligenceinhealthcare.BMJ
aworthygoal.AIEthics(2023). Quality&Safety28:495-498
https://doi.org/10.1007/s43681-023-00259-8
Marcus,G.(2023).TheThreatOfAutomated
Isabelle,P.,Cherry,C.,&Foster,G.(2017).A MisinformationisOnlyGettingWorse.
challengesetapproachtoevaluatingmachine Substack.
translation.arXivpreprintarXiv:1704.07431. https://open.substack.com/pub/garymarcus/p/t
he-threat-of-automated-misinformation?r=1v1
Kasirzadeh,A.,Gabriel,I.InConversation n3t&utm_campaign=post&utm_medium=emai
withArtificialIntelligence:Aligninglanguage l
ModelswithHumanValues.Philos.Technol.
36,27(2023). Merriam-Webster(2023).Milquetoast.
https://doi.org/10.1007/s13347-023-00606-x https://www.merriam-webster.com/dictionary/
milquetoast
Kempt,H.(2020).Chatbotsandthe
DomesticationofAI.ARelationalApproach. Metselaar,S.,Widdershoven,G.(2016).
Springer. DiscourseEthics.In:tenHave,H.(eds)
EncyclopediaofGlobalBioethics.Springer,
Kempt,H.(2022).SyntheticFriends.A Cham.
PhilosophyofHuman-MachineFriendship. https://doi.org/10.1007/978-3-319-09483-0_14
Springer. 5
LaCroix,T.,&Luccioni,A.S.(2022). Multiplex(2023).ASelectionOfPowerful
MetaethicalPerspectiveson'Benchmarking'AI ChatGPTSuperprompts.
Ethics.arXivpreprintarXiv:2204.05151. https://readmultiplex.com/2023/04/22/a-selecti
on-of-powerful-chatgpt-superprompts/
Leslie,D.(2019).Understandingartificial
intelligenceethicsandsafety:Aguideforthe Nyholm,S.,&Frank,L.(2019).Itlovesme,it
responsibledesignandimplementationofAI lovesmenot:isitmorallyproblematicto
systemsinthepublicsector.TheAlanTuring designsexrobotsthatappearto“love”their
Institute. owners?Techné,23(3),402-424.
https://doi.org/10.5281/zenodo.3240529 https://doi.org/10.5840/techne2019122110
Li,J.,Monroe,W.,Ritter,A.,Galley,M.,Gao, NewYorkTimes(2023):WhyaConversation
J.,&Jurafsky,D.(2016).Deepreinforcement WithBing’sChatbotLeftMeDeeply
learningfordialoguegeneration.arXiv Unsettled-TheNewYorkTimes.
preprintarXiv:1606.01541. https://www.nytimes.com/2023/02/16/technolo
gy/bing-chatbot-microsoft-chatgpt.html
Liang,P.,Bommasani,R.,Lee,T.,Tsipras,D.,
Soylu,D.,Yasunaga,M.,...&Koreeda,Y. Pan,A.,Shern,C.J.,Zou,A.,Li,N.,Basart,
(2022).Holisticevaluationoflanguage S.,Woodside,T.,Ng,J.,Zhang,H.,Emmons,
models.arXivpreprintarXiv:2211.09110. S.,&Hendrycks,D.(2023).DotheRewards
JustifytheMeans?MeasuringTrade-Offs
Luitse,D.,&Denkena,W.(2021).Thegreat
BetweenRewardsandEthicalBehaviorinthe
Transformer:Examiningtheroleoflarge
MACHIAVELLIBenchmark.
languagemodelsinthepoliticaleconomyof
https://arxiv.org/abs/2304.03279
AI.BigData&Society,8(2).
https://doi.org/10.1177/20539517211047734
Patterson,D.,Gonzalez,J.,Le,Q.,Liang,C., Shyns,C.(2023).TheLobbyingGhostInThe
Munguia,L.M.,Rothchild,D.,...&Dean,J. Machine.CorporateEuropeObservatory.
(2021).Carbonemissionsandlargeneural https://corporateeurope.org/sites/default/files/2
networktraining.arXivpreprint 023-02/The%20Lobbying%20Ghost%20in%2
arXiv:2104.10350. 0the%20Machine_1.pdf
Pearson,J.(2023).ConservativesAre Sibarium,A.(2023).Tweet.
ObsessedWithGettingChatGPTtoSaythe https://twitter.com/aaronsibarium/status/16224
N-Word.Vice. 25697812627457
https://www.vice.com/en/article/wxnv59/conse
rvatives-are-obsessed-with-getting-chatgpt-to- Stalnaker,R.(2002).CommonGround.
say-the-n-word LinguisticsandPhilosophy25,701–721
https://doi.org/10.1023/A:1020867916902
Seromelhor(2023).Redditr/bing.
https://www.reddit.com/r/bing/comments/12c9 Stepin,I.,Catala,A.,Pereira-Fariña,M.,&
zfq/mikhail_starting_to_ship_prompt_v98_tod Alonso,J.M.(2019).Pavingthewaytowards
ay_it_is_a/ counterfactualgenerationinargumentative
conversationalagents.InProceedingsofthe
Risch,J.,Ruff,R.andKrestel,R.(2020). 1stWorkshoponInteractiveNaturalLanguage
Offensivelanguagedetectionexplained.In TechnologyforExplainableArtificial
ProceedingsoftheSecondWorkshopon Intelligence(NL4XAI2019)(pp.20-25).
Trolling,Aggres-sionandCyberbullying,
pages137–143,Marseille,France.European Thoppilan,R.,DeFreitas,D.,Hall,J.,Shazeer,
LanguageResourcesAssociation(ELRA). N.M.,Kulshreshtha,A.,Cheng,H.,Jin,A.,
Bos,T.,Baker,L.,Du,Y.,Li,Y.,Lee,H.,
Roller,S.,Dinan,E.,Goyal,N.,Ju,D., Zheng,H.,Ghafouri,A.,Menegali,M.,
Williamson,M.,Liu,Y.,Xu,J.,Ott,M., Huang,Y.,Krikun,M.,Lepikhin,D.,Qin,J.,
Shuster,K.,Smith,E.M.,etal.(2020).Recipes Chen,D.,Xu,Y.,Chen,Z.,Roberts,A.,
forbuildinganopen-domainchatbot.arXiv Bosma,M.,Zhou,Y.,Chang,C.,Krivokon,
preprintarXiv:2004.13637. I.A.,Rusch,W.J.,Pickett,M.,Meier-Hellstern,
K.S.,Morris,M.R.,Doshi,T.,Santos,R.D.,
Roose,K.(2022).Thebrillianceand Duke,T.,Søraker,J.H.,Zevenbergen,B.,
weirdnessofChatGPT.TheNewYorkTimes. Prabhakaran,V.,Díaz,M.,Hutchinson,B.,
Olson,K.,Molina,A.,Hoffman-John,E.,Lee,
Russell,S.(2019).Humancompatible:
J.,Aroyo,L.,Rajakumar,R.,Butryna,A.,
Artificialintelligenceandtheproblemof
Lamm,M.,Kuzmina,V.O.,Fenton,J.,Cohen,
control.Penguin.
A.,Bernstein,R.,Kurzweil,R.,Aguera-Arcas,
B.,Cui,C.,Croak,M.,Chi,E.H.,&Le,Q.
Schmidt,A.andWiegand,M.(2017).A
(2022).LaMDA:LanguageModelsforDialog
surveyonhatespeechdetectionusingnatural
Applications.ArXiv,abs/2201.08239.
languageprocessing.InProceedingsofthe
FifthInternationalworkshoponnatural
UNESCO(2019):I’dBlushIfICould:
languageprocessingforsocialmedia,pages
ClosingGenderDividesinDigitalSkills
1–10.
ThroughEducation.
https://unesdoc.unesco.org/ark:/48223/pf00003
SchulmanJ.,ZophB.,KimC.,etal.,(2022).
67416.page=1
ChatGPT:OptimizingLanguageModelsfor
Dialogue.https://openai.com/blog/chatgpt
Vincent,J.(2023).Google’sAIchatbotBard
makesfactualerrorinfirstdemo
https://www.theverge.com/2023/2/8/23590864 responsegeneration.arXivpreprint
/google-ai-chatbot-bard-mistake-error-exoplan arXiv:1911.00536.
et-demo
Volpicelli,G.(2023).ChatGPTbroketheEU
plantoregulateAI.Politico.
https://www.politico.eu/article/eu-plan-regulat
e-chatgpt-openai-artificial-intelligence-act/
Waseem,Z.,Davidson,T.,Warmsley,D.,and
Weber,I.(2017).Understandingabuse:A
typologyofabusivelanguagedetection
subtasks.arXivpreprintarXiv:1705.09899.
Wei,J.,Wei,J.,Tay,Y.,Tran,D.,Webson,A.,
Lu,Y.,...&Ma,T.(2023).Largerlanguage
modelsdoin-contextlearningdifferently.
arXivpreprintarXiv:2303.03846.
Weidinger,L.,Mellor,J.,Rauh,M.,Griffin,
C.,Uesato,J.,Huang,P.S.,...&Gabriel,I.
(2021).Ethicalandsocialrisksofharmfrom
languagemodels.arXivpreprint
arXiv:2112.04359.
Willison,S.(2023).PromptInjectionHackson
GPT-3.
https://simonwillison.net/2022/Sep/12/prompt-
injection/
Xu,A.,Pathak,E.,Wallace,E.,Gururangan,
S.,Sap,M.,&Klein,D.(2021).Detoxifying
languagemodelsrisksmarginalizingminority
voices.arXivpreprintarXiv:2104.06390.
Xu,J.,Ju,D.,Li,M.,Boureau,Y.L.,Weston,
J.,&Dinan,E.(2020).Recipesforsafetyin
open-domainchatbots.arXivpreprint
arXiv:2010.07079.
Zhang,Y.,Ren,P.,anddeRijke,M.(2020).
Detectingandclassifyingmalevolentdialogue
responses:Taxonomy,dataandmethodology.
arXivpreprintarXiv:2008.09706.
Zhang,Y.,Sun,S.,Galley,M.,Chen,Y-C.,
Brockett,C.,Gao,X.,Gao,J.,JLiu,J.,and
Dolan,B.(2019).DialoGPT:Large-scale
generativepre-trainingforconversational

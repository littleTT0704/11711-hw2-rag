Thahiretal.BMCProceedings2012,6(Suppl7):S2
http://www.biomedcentral.com/1753-6561/6/S7/S2
PROCEEDINGS Open Access
An efficient heuristic method for active feature
acquisition and its application to protein-protein
interaction prediction
Mohamed Thahir1,2, Tarun Sharma1,3, Madhavi K Ganapathiraju1,2*
From Great Lakes Bioinformatics Conference 2012
Ann Arbor, MI, USA. 15-17 May 2012
Abstract
Background: Machine learning approaches for classification learn the pattern of the feature space of different
classes, or learn a boundary that separates the feature space into different classes. The features of the data
instances are usually available, and it is only the class-labels of the instances that are unavailable. For example, to
classify text documents into different topic categories, the words in the documents are features and they are
readily available, whereas the topic is what is predicted. However, in some domains obtaining features may be
resource-intensive because of which not all features may be available. An example is that of protein-protein
interaction prediction, where not only are the labels (’interacting’ or ‘non-interacting’) unavailable, but so are some
of the features. It may be possible to obtain at least some of the missing features by carrying out a few
experiments as permitted by the available resources. If only a few experiments can be carried out to acquire
missing features, which proteins should be studied and which features of those proteins should be determined?
From the perspective of machine learning for PPI prediction, it would be desirable that those features be acquired
which when used in training the classifier, the accuracy of the classifier is improved the most. That is, the utility of
the feature-acquisition is measured in terms of how much acquired features contribute to improving the accuracy
of the classifier. Active feature acquisition (AFA) is a strategy to preselect such instance-feature combinations (i.e.
protein and experiment combinations) for maximum utility. The goal of AFA is the creation of optimal training set
that would result in the best classifier, and not in determining the best classification model itself.
Results: We present a heuristic method for active feature acquisition to calculate the utility of acquiring a missing
feature. This heuristic takes into account the change in belief of the classification model induced by the acquisition
of the feature under consideration. As compared to random selection of proteins on which the experiments are
performed and the type of experiment that is performed, the heuristic method reduces the number of
experiments to as few as 40%. Most notable characteristic of this method is that it does not require re-training of
the classification model on every possible combination of instance, feature and feature-value tuples. For this
reason, our method is far less computationally expensive as compared with previous AFA strategies.
Conclusions: The results show that our heuristic method for AFA creates an optimal training set with far less
features acquired as compared to random acquisition. This shows the value of active feature acquisition to aid in
protein-protein interaction prediction where feature acquisition is costly. Compared to previous methods, the
proposed method reduces computational cost while also achieving a better F-score. The proposed method is
valuable as it presents a direction to AFA with a far lesser computational expense by removing the need for the
*Correspondence:madhavi@pitt.edu
1DepartmentofBiomedicalInformatics,SchoolofMedicine,Universityof
Pittsburgh,Pittsburgh,PA,USA
Fulllistofauthorinformationisavailableattheendofthearticle
©2012Thahiretal.;licenseeBioMedCentralLtd.ThisisanopenaccessarticledistributedunderthetermsoftheCreativeCommons
AttributionLicense(http://creativecommons.org/licenses/by/2.0),whichpermitsunrestricteduse,distribution,andreproductionin
anymedium,providedtheoriginalworkisproperlycited.
Thahiretal.BMCProceedings2012,6(Suppl7):S2 Page2of9
http://www.biomedcentral.com/1753-6561/6/S7/S2
first time, of training a classifier for every combination of instance, feature and feature-value tuples which would be
impractical for several domains.
Background features for which proteins should be determined by
Constructingacompletehumanprotein-proteininterac- experiments? From the perspective of machine learning
tion(PPI)network(the ‘interactome’)canacceleratedis- for PPI prediction, it would be desirable that those
covery inbiomedical sciencesandiscrucialtothe study experiments be carriedoutwhichwhen usedin training
ofdiseasemechanismsanddrugdiscovery.Forexample, the classifier, the accuracy of the classifier is improved
proteins (genes) which are associated with a disease themost.Thatis,the utilityof the feature-acquisition is
interactwithotherdisease-relatedgenesmore closelyin measured in terms of how much acquired features con-
theinteractome[1];forthisreason,protein-diseaseasso- tributetoimprovingtheaccuracyoftheclassifier.Active
ciationscanbedeterminedbasedonthenetworktopolo- feature acquisition (AFA) is a strategy to preselect such
gical features such as the degree of a node (i.e. protein), instance-feature combinations (i.e. protein and experi-
average distance of the node from disease-related pro- ment combinations) for maximum utility. It is to be
teins etc. [2]. Several network-based approaches have noted that the goal of AFA is the creation of optimal
beendevisedtodeterminegene-diseaseassociationsand training set that would result in the best classifier, and
functional modules using the interactome, including not the determination of the best classification model
neighborhood basedapproaches, clustering/graph parti- itself.Subsequenttocreationoftrainingdatawithactive
tioning based methods and random-walks [3-6]. How- feature acquisition, any state-of-the-art method such as
ever, only a fraction of the whole human interactome is random forest based methods may be applied to learn
known today, calling for methods to discover hitherto- the classification model. While PPI prediction itself is
unknownPPIs[7,8]. being actively studied recently [11,12,15], AFA strategy
Determining PPIs by high-resolution experimental hasnotbeenappliedinthisdomain.
methods is very resource intensive. High throughput A few algorithms have been developed for AFA in
methods such as yeast 2-hybrid and mass spectrometry other application domains which calculate utility of fea-
methods have low assay-sensitivity (i.e. the interactions ture-acquisition based on the accuracy of the current
thattheycandetectisonlyasubsetofallPPIsthatexist) modelanditsconfidenceintheprediction.Melvilleetal.
andeven amongthosethatthey can,eachscreenidenti- proposed a framework for performing active feature
fies a further smaller subset of PPIs [9]. Computational acquisition[16],whichisdescribedherebriefly.Here,the
methods are therefore necessary to complement the trainingsetTofminstancesisrepresentedbythematrix
high-throughputmethodstoreconstructtheinteractome F, where Fi,jcorresponds to the value ofthe j-thfeature
expeditiously. Several computational systems have been ofthei-thinstance.Thefeaturematrixinitiallyhasmiss-
developedfor prediction ofprotein-protein interactions, ing values, the class label of each instance is already
particularlyforyeastandhuman,usingmachinelearning known.Missingfeaturesmaybeacquiredwithactivefea-
approaches[10-14].These approachesemploy statistical tureacquisitionprocedureatacostofCi,jforfeatureFi,j.
machine learning methods to classify whether two pro- qi,jreferstothequeryforvalueofFi,j[16].Theobjective
teins interact with each other or not, based on the bio- of AFA is to query for missing feature values such that
logical features of proteins such as their localization, themostaccurateclassifierisbuiltforagivenbudgetfor
molecular function and the tissues the proteins are featureacquisition.TheframeworkproposedbyMelville
expressedin.Inallofthesemethods,itisassumedthata et.al[16],isaniterativemodelwhereinineachiterationa
trainingdatasetisavailable,andthatthependinggoalis set of missing features, which provide the highest
to develop an algorithm to learn to model the relation expected improvement to classifier accuracy at minimal
between feature space and labels given represented by cost, are chosen and queried. Known feature values are
thetrainingdata. addedtotrainingdataandtheclassifierisretrained.The
However,inthecurrenttrainingdatamanyfeaturesare processisrepeateduntiladesiredlevelofclassifieraccu-
unknown (i.e.‘missing’)formany proteins. Carryingout racyisachieved,orthebudgetavailableforfeatureacqui-
wet-lab experiments to determine all such missing fea- sitionisexhausted.
tures is infeasible as those experiments require human They propose that specific solutions to the AFA pro-
expertise,time,high-endequipmentandotherresources. blem differ based on the method used to score and rank
It may however be possible to carry out a few experi- queries.Scoresarecomputedbasedontheexpectedutility
ments to determine some of the missing features, if not of each query. The scoring function measures what the
all.Ifonlyafewmissingvaluescanbedetermined,which expectedimprovementisinthe accuracy of aclassifier if
Thahiretal.BMCProceedings2012,6(Suppl7):S2 Page3of9
http://www.biomedcentral.com/1753-6561/6/S7/S2
weknowthevalueofaparticularmissingfeaturegiventhe values for labeling. Saar-Tsechansky et al. create the
costinvolvedinobtainingit.Giventhatafeaturevaluef is reduced consideration set ‘S’ by giving preference to
i
missingforaninstanceanditcantakeanyoftheKvalues missing features in instances which are misclassified or
(V ,V ,...V ),itsexpectedutilityismeasuredas, instances which have high uncertainty as to their label
1 2 k
according to the induced classifier model [18]. Though
(cid:2)K
methods like sampled expected utility reduce the con-
E(f)= P(f =V )∗U(f =V )
i i k i k sideration set, for large data sets with several missing
k=1 features this approach would be computationally very
expensive, especially for models which are parametric.
A(F,f i =V k)−A(F) Gregory et al. proposed an active feature acquisition
U(f)=
i C(f) approach that they specifically evaluated on two
i
sequence labeling tasks [19]. Their approach also
where A(F, f i = V k) is the accuracy of the classifier required re-training of classifiers. Attenberg, Melville
when it is trained with the value of f i set to V k. A(F) is and Provost present a unified approach to active dual
the accuracy of the original classifier. C(f i) is the cost of supervision, where they determine which feature or
acquiring the feature value. P(f i = V k) is measured by instance should be acquired that benefits the classifier
building a classifier C i corresponding to each feature. In the most by extending the sampled expected utility mea-
the training data all the features other than f i and the sures proposed for active dual supervision, but their
class label are taken as feature values and C i is built. methods still require re-training the classifiers [20].
The classifier C i predicts what the probability is that a InexpectedutilitybasedapproachesforAFA,the use-
missing feature will take a particular value when the fulness of acquiring a missing feature is estimated by
other feature values and the class label for an instance retraining the classifier for each of the possible values
are known. It finds the expected utility for various miss- thatthemissingfeaturecantakeandthencalculatingthe
ing values across all the instances. The missing feature expected improvement in classifier accuracy. However,
with maximum expected utility is selected and its value retrainingthe classifierforevery possiblevalue,foreach
is obtained (by experimentation or manual labeling, as missing featureofeachinstance,iscomputationallyvery
applicable). intensive, or even infeasible for large multi-dimensional
This method is computationally intensive for several datasets.
classifiers types and for several domains. This is because In this work we propose a novel heuristic to measure
the classifier needs to be trained for each missing fea- the utility of acquiring a missing feature value without
ture and its various possible values in order to measure the need of retraining of the classifier multiple times.
A(F, f = V ). Therefore, in order to evaluate the utility
i k
of a single missing feature of a given instance, the classi- Methods
fier is to be retrained ‘K’ times. As this procedure is Proposed active feature selection strategy
repeated for each of the missing feature elements, the Consider a training data set with N instances and a clas-
classifier is to be retrained |M|*K times in a single itera- sifier ‘C’ trained on this data. Say that a feature value f
i
tion (where M is the set of all missing features over all is missing for a particular instance ‘p’ in this training set
instances). Although incremental learning can be done and that it can take any of the K values (V ,V , ... V ).
1 2 k
efficiently for classifiers like Naive Bayes, for several Let (L , L , ... L ) be the various possible labels for the
1 2 N
other classifiers it is inefficient. For instance in the case instance. We assume that the instance under considera-
of Random Forests, retraining the classifier once has tion is already labeled to be L . The expected utility of
m
time-complexity of T*N*log(N) [17], where T is the acquiring f is measured as follows,
i
number of trees in the random forest and N is the num-
ber of instances in the training data. So, the total time
(cid:2)K
U(f)= P(f =V|y=L )∗(cid:2)ρ(f =V)
complexity for evaluating the utility of all the missing i i j m i j
features is T*N*log N*|M|*k. When the dataset size is j=1
large and has several missing values, the time for evalu- TheestimatedchangeΔrisaheuristictoestimatehow
ating the expected utility would be very high. To over- muchofachangewouldbeinducedintothecurrentclas-
come this, the authors (Melville et al) proposed Sampled sifier‘C’ifitisretrainedwith‘p’havingfeaturevaluef set
i
Expected Utility wherein a random subset of instances toV.Iftheprobabilitythat‘p’belongstoitscorrectclass
j
(S) with missing feature values are selected randomly according to ‘C’ decreases if f takes the value V, then it
i j
and are evaluated by the above procedure. The results indicatesthatonretraining,theclassifier‘C’hastoadjust
show that this expected utility approach performs better itsbeliefssoastoincreasethepredictedprobabilityof‘p’
than the method which randomly picks missing feature belonging to its correct class (so as to reduce
Thahiretal.BMCProceedings2012,6(Suppl7):S2 Page4of9
http://www.biomedcentral.com/1753-6561/6/S7/S2
misclassification cost, orinclassifierslike SVM to maxi- where, Set1 are the set of GO terms for P1 and Set2
mizethemargin). are the terms for P2. Three feature values, one each by
using GO annotations for biological process, cellular
(cid:2)ρ(f =V)=P(y=L |C,p)−P(y=L |C,(p∩f =V))
i j m m i j component and molecular function are developed.
P(y=L | C,p) = predicted probability that ‘p’ has label
m
Gene expression
L according to previously learnt classifier C.
m
P(y= L | C,(p ∩ f = V)) = predicted probability that GeneExpressionfeaturesforPPIpredictionproblemare
m i j
‘p’ has label L according to previously learnt classifier usually generated from a limited set of gene expression
m
C, when the feature f of ‘p’ is set to V experiments. Qi et al. use 16 gene expression experi-
i j
If Δr is less than 0, it indicates that when f is set to V ments [11]. However, in our work we use the several
i j
thousand gene expression experiments available in the
it concurs with the belief of C (i.e. the estimated prob-
ability of ‘p’ belonging to its correct class (L ) according Stanford Microarray Database (SMD) to compute this
m
to C increases). Hence in ‘p’ if f is set to V and C is feature[22].Notethatthisfeatureneedstobecomputed
i j
for every possible protein pair (20,000 × 20,000/2 = 200
retrained, classifier is not expected to update its model.
Therefore, Δr is set to 0 for that case. millionpars,where20,000isroughlythenumberofpro-
teins (genes) currently catalogued in the human protein
reference database); the process therefore needs to be
Dataset and feature descriptors
In the domain of PPI prediction, there is no “negative efficient.SeveralthousandexperimentsintheSMDhave
dataset” available; that is, there are no pairs that are beendividedinto70categories.Topreventseveralthou-
sandfilereadsforcomputinggeneexpressionfeaturefor
known to be non-interacting. However, in 500 to 1500
aproteinpair,wepreprocessthegeneexpressiondatain
randomlyselectedpairsonlyonepairisexpectedtobean
eachcategoryintoasinglefile.Thisfilehasforeachpro-
interactingpair[21].Therefore,randompairsareusually
teinavectorofgene expressionvaluescorrespondingto
treatedasnegativeclassinstancesinthisdomain.Forour
the microarray experiment. So, 70 pre-processed files
work, we created training and testing datasets of 10,000
corresponding to the 70 categories are obtained. For a
protein pair instances each with 2,000 interacting pairs
givenproteinpair(P1,P2),letGE1 bethevectorofgene
and 8,000 random pairs. AFA is carried out in batch m
expression values corresponding to the category ‘m’ for
mode,selecting500missingvaluesineachbatch.
proteinP1andletGE2 bethevectorofgeneexpression
m
values corresponding to the category ‘m’ for protein P2.
Gene ontology features
Let N be the length of the vector. The Pearson Correla-
Given a protein pair, Gene Ontology (GO) information
tionCo-efficient iscomputedbetweenthese twovectors
is usually encoded by measuring the semantic similarity
asfollows,
between the GO terms of the proteins in the pair. But it
(cid:3) (cid:3) (cid:3)
i os rp co es lls uib lal re ct oh mat pa onp ea nir t)o sf hG arO
e
lt oe wrm ss em(f au nn tc ictio sin m, ip lar ro ic tyes bs ues
t PPCm=(cid:4) (cid:5)(cid:7)
GE1m∗ (cid:3)GE2m−(
(cid:8)
G (cid:7)E1m∗ GE2 (cid:3)m)/N
(cid:8)
they could be crucial for interaction between a protein (cid:5) (cid:6) (cid:3) GE12 −( GE1m)2 ∗ (cid:3) GE22 −( GE2m)2
m N m N
pair. Hence we use existing protein interactions to esti-
mate the value of a pair of GO terms for protein inter-
Two gene expression features are computed. They are
action. These estimates are then used to encode the
the mean and standard deviation of the correlation
new features. The protein interaction data was obtained
values (PPC ) for the 70 categories.
m
from the HPRD data base [7] and the GO annotations
We can further improve the efficiency of the process
from the GO database (http://www.geneontology.org).
by finding proteins which have little variance in correla-
From this data, pairs of GO terms (GO1, GO2) and the
tions. Say if protein P1 does not have much variance,
number of protein interactions in which each pair
then for any protein P2 there will be little correlation
occurs n(GO1,GO2) are computed. Let’s say Protein A
between P1 and P2.
is associated with GO1, GO2 and Protein B is associated
with GO4 and Protein A and Protein B interact. Then
Domain interaction feature
the frequency of the pairs (GO1, GO4), (GO2, GO4),
The domain interaction information is obtained from
are incremented. Then the feature value for a protein
the InterDom database [23]. It has information about
pair (P1,P2) is proportional to,
the list of domains belonging to each protein and an
(cid:2) (cid:2) interaction score between pairs of domains. Given a
n(GO GO )
1 2
protein pair (P1,P2), the domain interaction feature is
n(GO )∗n(GO )
GO1∈Set1GO2∈Set2 1 2 calculated as follows,
Thahiretal.BMCProceedings2012,6(Suppl7):S2 Page5of9
http://www.biomedcentral.com/1753-6561/6/S7/S2
(cid:2) (cid:2) score(d d ) 50,000featurevaluesofwhichnearlyhalfofthesefeature
1 2
|D ||D | values are missing in the original dataset. Additionally,
d1∈Set1d2∈Set2 1 2 wesetanother10,000featurevaluestobemissing(which
are otherwise available in the dataset), so as to simulate
where,
acquiring these features as-and-whenaskedby the algo-
D1 is the set of domains in protein P1
rithm.Inotherwords,thesearethefeaturevalueswhich
D2 is the set of domains in protein P2
areavailableforacquisitionbytheAFAsystem.Toapply
score(d1,d2) is the interaction score between the
AFA, we need to discretize the real valued features. To
domains d1 and d2.
do that we apply the commonly applied Maximum
Description Length (MDL) based discretization method
Gene neighbourhood
proposed by Fayyad and Irani [25]. We use the Weka
For a given protein pair, this feature measures how close
Machine Learning Toolkit’s implementation of this dis-
the genes (encoding the proteins) are to each other in
cretizationmethod[26].
the genome. The data for computing this feature is
downloaded from ftp://ftp.ncbi.nlm.nih.gov/gene/. Based
Performance comparison
on the locus tag and the chromosome to which the
We compared the performance of the proposed Active
genes are attached the distance score is computed
FeatureAcquisition(AFA)heuristicwithasystemwhich
between the genes.
randomly selects missing feature values for acquisition.
In each iteration 500 missing values are acquired and a
Tissue feature
Decision Tree Classifier is retrained. The F-score of the
The data for generating the tissue feature was obtained
classifiersgeneratedbythe2methodsattheendofeach
from Tissue-specific Gene Expression and Regulation
iteration is compared (Figure 1). It can be seen that the
(TiGER) database [24]. Information on proteins and the
AFA system achieves the peak F-score after acquiring
tissues in which they occur is retrieved. The tissue fea-
about 4,200 missing feature values (indicated by red
ture score for a protein pair (P1,P2) is computed as,
square marker on the figure). To achieve a similar
T 1∩T 2 F-scorewithatrainingdatacreatedwithrandom-acquisi-
min(T ,T ) tion,almost9,500featurevalueshadtobeacquired.This
1 2
shows that the AFA system is able to create an optimal
where, training data much more economically, by asking for
T1 is the set of tissues P1 occurs in only40%ofthemissingfeaturevalues.
T2 is the set of tissues P2 occurs in While the above comparison shows that the heuristic
Evaluation metric
method is creating a training data effectively compared
The metrics that we employ here are those that are to random selection, it would be interesting to see
commonly used in the domain of information retrieval: whether this method performs comparably to computa-
F-score. F-score is the harmonic mean of the precision tionallyintensiveAFAmethods.Tomeasuretherelative
andrecall.Precisionismeasuredasthepercentageoftrue performance, we compared the heuristic AFA method
positivesamongallpredictedinteractions;recallistheper- withthatproposed by Melvilleetal., on thePPIdataset.
centageoftruepositivesamongallrealinteractions. ResultsareshowninFigure2.Wefoundthatourmethod
performsslightlybetterthantheothermethod,andwhen
Results
combinedwiththefactthatitdoesnotrequireretraining
Experimental setup the classifier numerous times, it clearly presents an
The Gene Expression and Gene Neighborhood features advantage.
inPPIpredictionfeaturevectorshavenearly100%cover- Next, we analyzed what types of features are being
age,andthereforedonotdependonactivefeatureacqui- selected for querying in the AFA procedure. Figure 3
sition. The Gene Ontology features (biological process, shows how many missing values were acquired for each
cellularcomponentandmolecularfunction),domainand feature type in each batch. The results show that in the
tissuefeatureshavealargenumberofmissingvalues.So initialiterationsmissingvaluesforbiologicalprocessand
we considered these five features to study active feature molecular function features (which describe the func-
acquisitionforPPIprediction.Weconsideronlyprotein tional similarity of the protein pair) are selected heavily
pairswhereindividualproteinshavegeneontologyanno- andinthelateriterations features relatedtolocalization
tationsandatleastone of tissue ordomain annotations. (tissue and cell component features) are selected more.
This is to ensure that the feature vector is reasonably Tounderstandthisphenomenon,westudiedthedecision
filled. A training and test data set of 10,000 instances trees that were constructed in each iteration. The deci-
each was generated. The training set has 10,000 × 5 = sion trees built have the Gene Ontology biological
Thahiretal.BMCProceedings2012,6(Suppl7):S2 Page6of9
http://www.biomedcentral.com/1753-6561/6/S7/S2
Figure1F-scoreforActiveFeatureAcquisition.X-axisshowsthenumberofmissing featuresacquired.Y-axisshowstheF-scorefor the
classifierbuiltatthecorrespondingnumberofmissingfeaturesacquired
process and molecular function at the higher levels and yetpreliminary.Rigoroustestingandanalysisistobecar-
the localization features in the lower levels of the tree. ried out in future, with our method as well as previous
Thefunctionalsimilarityfeaturesareinitiallyacquiredin methods, tounderstandwhat thedomain-characteristics
largeramountstillthetoplevelrulesarelearntandthen may be that lead to the success or failure by different
missing localization features are obtained for further methodsinthesedomains.
learningrulescorrespondingtolowerlevelsofthetree.
Conclusions
Performance of the proposed AFA heuristic method on Active learning methods optimize the interaction
other classification tasks betweenacomputationalmethodandahumanexpertby
We carried out active feature acquisition on other stan- preselecting the data that an expert is to devote time or
dard classification tasks with data available at the UCI resourceson,sothattheoutcomecontributesmostben-
MachineLearningRepository (http://archive.ics.uci.edu/ eficially to the computational algorithm. Typically these
ml/datasets.html). However, the AFA method proposed methods are applied to domains that have massive
here did not perform better in these cases (we have not amount of data such as astronomical images or world-
tested Melville method, but tested AFA against random wide-web documents, where, even though each data
selection). It remains to be seen whether the proposed instancecanbelabelledwithlittlemanualeffort,creation
heuristicmethodhasparticularadvantageinPPI-predic- ofatrainingdatathatisrepresentativeoftheentiredata-
tionlikedomains,i.e.when(i)thedatahasseveralmiss- setcanbenefitwithactivelearningapproaches.Inmole-
ingvalues,or(ii)the positive instancesare anextremely cular biology domain however, the reasons for active
rare category among the unlabeled instances. Although learningareatypical.Here,eventhoughthedatamaynot
this is discouraging, the proposed method presents a beasmassive,theresources,timeandexpertiserequired
novel direction for estimating the utility, which is not to characterize each instance is very large, making it
dependent on training a classifier numerous times in impossibletocharacterizeevenmoderatelylargedatasets.
each iteration. The evaluations on these datasets are as For this reason, active learning methods can contribute
Thahiretal.BMCProceedings2012,6(Suppl7):S2 Page7of9
http://www.biomedcentral.com/1753-6561/6/S7/S2
Figure2ComparisonwithMelvilleetal’smethod.ThegraphshowsF-scoreoftheheuristicmethodproposedherewiththatproposedby
Melvilleetal.AxesdescriptionsaresameasinFigure1.
tothe domainofmolecularbiology,andguidethe selec- The cost for experimentally determining the interaction
tion of molecule-experiment combinations that yield of the protein pairs might vary for different pairs
maximumbenefittowardscharacterizingothermolecules depending upon the localization of the proteins and the
by computational methods. We have previously applied experimental conditions which need to be created to
active learning for label acquisition for protein-protein verify the interaction. Similarly cost of obtaining the
interactionprediction[27]. missing features might differ for the various feature
Here, we presented a new heuristic approach for types. So it is necessary to develop computational meth-
Active Feature Acquisition (AFA) that reduces computa- ods which are able to model the cost of experimental
tional cost by estimating the improvement a feature annotation and incorporate them in to the active label/
value would bring to the classifier. In contrast, other feature acquisition strategies [28].
expected utility-based methods for feature acquisition Theheuristicweproposedforactivefeatureacquisition
train a new classifier for each ‘instance-feature-value’ tri- works in a batchmode selecting a group of missing fea-
ple. The results show that AFA achieves comparable tures to be acquired in each iteration; further improve-
F-score by acquiring only 40% as much missing features ments can be achieved by incorporating marginal
as the random method. Further, AFA has not been pre- relevance of the features with respect to each other to
viously applied for PPI prediction (to the best of our ensure diversityintheselectedmissingfeatureswithina
knowledge) and the results show that AFA would be cri- batch [29].Itwouldbeinterestingtoseehow toaddress
tical for the domain of PPI prediction where the biologi- active learning indomainswith sparse-label and sparse-
cal features are missing for several protein pairs feature space. The Active Information Approaches pro-
(especially for pairs with proteins which have not been posed in [18] may be a starting point in this direction.
studied extensively). The active learning and active feature acquisition
Active label/feature acquisition strategies generally approaches we considered evaluate the utility only at a
work under budget constraints, and it is necessary to particular instance/missing-feature level. It is possible
account for the cost of acquiring these missing values. that acquiring a particular pair of missing labels or
Thahiretal.BMCProceedings2012,6(Suppl7):S2 Page8of9
http://www.biomedcentral.com/1753-6561/6/S7/S2
Figure3Typeoffeaturesacquiredindifferentstagesofactivefeatureacquisition.TheX-axisshowstheiterationnumber.Thebarsshow
theamountofmissingvaluesacquiredforeachfeaturetypeinthatiteration.
featurescanbringinmuchhigherutilitythanthesumof andSciences,UniversityofPittsburgh,Pittsburgh,PA,USA.3Language
theutilityofacquiringeachofthemindividually.Further TechnologiesInstitute,CarnegieMellonUniversity,Pittsburgh,PA,USA.
we may be constrained by the amount ofbudget we can Authors’contributions
spendtolearntheclassifier.Howeverperformingacom- MTproposedtheheuristicforcalculatingtheutilityofacquiringamissing
plete look-ahead has exponential time complexity. So featureandcarriedoutthebulkoftheexperimentswithdirectionand
supervisionfromMG.TScomparedthemethodwithMelvilleetal.method.
highly simplified look-ahead procedures such as single ManuscripthasbeenpreparedbyMTandMGwithrelevantcontribution
featurelook-ahead(SFL)[30]andrandomizedsinglefea- fromTS.
ture look-ahead(RSFL)[31]have beenproposed.Devel-
Competinginterests
oping advanced look-ahead policies that incorporate Theauthorsdeclarethattheyhavenocompetinginterests.
moreinformationaboutthestatespaceanddeeperlook-
aheadwouldenableobtaining higher errorreductionfor Published:13November2012
thegivenbudget.
References
1. HamoshA,ScottAF,AmbergerJS,BocchiniCA,McKusickVA:Online
mendelianinheritanceinman(omim),aknowledgebaseofhuman
Acknowledgements genesandgeneticdisorders.NucleicAcidsRes2005,D514-D517.
ThisworkhasbeenfundedinpartbytheBRAINSgrantR01MH094564 2. XuJ,LiY:Discoveringdisease-genesbytopologicalfeaturesinhuman
awardedtoMGbytheNationalInstituteofMentalHealthofNational protein-proteininteractionnetwork.Bioinformatics2006,2800-2805.
InstitutesofHealth(NIMH/NIH)ofUSA.Authorswouldliketothank 3. NavlakhaS,KingsfordC:Thepowerofproteininteractionnetworksfor
Dr.JaimeCarbonellfordiscussionsthatleadtothedevelopmentofthis associatinggeneswithdiseases.Bioinformatics2010.
approach. 4. MeteM,TangF,XuX,YurukN:Astructuralapproachforfinding
ThisarticlehasbeenpublishedaspartofBMCProceedingsVolume6 functionalmodulesfromlargebiologicalnetworks.BMCBioinformatics
Supplement7,2012:ProceedingsfromtheGreatLakesBioinformatics 2008.
Conference2012.Thefullcontentsofthesupplementareavailableonlineat 5. ZhangS,NingX,ZhangX-S:Identificationoffunctionalmodulesinappi
http://www.biomedcentral.com/bmcproc/supplements/6/S7. networkbycliquepercolationclustering.Bioinformatics2007.
6. ChinC-H,ChenS-H,HoC-W,KoM-T,LinC-Y:Ahub-attachmentbased
Authordetails methodtodetectfunctionalmodulesfromconfidence-scoredprotein
1DepartmentofBiomedicalInformatics,SchoolofMedicine,Universityof interactionsandexpressionprofiles.BMCBioinformatics2010.
Pittsburgh,Pittsburgh,PA,USA.2IntelligentSystemsProgram,SchoolofArts
Thahiretal.BMCProceedings2012,6(Suppl7):S2 Page9of9
http://www.biomedcentral.com/1753-6561/6/S7/S2
7. MishraG,SureshM,KumaranK,KannabiranN,SureshS,PrasadT,PandeyA,
BalaP,ShivakumarK,AnuradhaN,etal:Humanproteinreference
database–2006Update.NucleicAcidsRes2006.
8. KeshavaPrasadTS,GoelR,KandasamyK,KeerthikumarS,KumarS,
MathivananS,TelikicherlaD,RajuR,ShafreenB,VenugopalA,etal:Human
ProteinReferenceDatabase–2009update.NucleicAcidsRes2009,37
Database:D767-772.
9. VenkatesanK,RualJF,VazquezA,StelzlU,LemmensI,Hirozane-KishikawaT,
HaoT,ZenknerM,XinX,GohKI,etal:Anempiricalframeworkforbinary
interactomemapping.NatMethods2009,6(1):83-90.
10. QiY,Klein-SeetharamanJ,Bar-JosephZ:Randomforestsimilarityfor
protein-proteininteractionpredictionfrommultiplesources.PacSymp
Biocomput2005,531-542.
11. QiY,Klein-SeetharamanJ,Bar-JosephZ:Amixtureoffeatureexperts
approachforprotein-proteininteractionprediction.BMCbioinformatics
2007,8(Suppl10):S6.
12. ScottMS,BartonGJ:Probabilisticpredictionandrankingofhuman
protein-proteininteractions.BMCBioinformatics2007,8:239.
13. GomezSM,RzhetskyA:Towardsthepredictionofcompleteprotein–
proteininteractionnetworks.PacSympBiocomput2002,413-424.
14. KimWK,ParkJ,SuhJK:Largescalestatisticalpredictionofprotein-
proteininteractionbypotentiallyinteractingdomain(PID)pair.Genome
Inform2002,13:42-50.
15. QiY,DhimanHK,BholaN,BudyakI,KarS,ManD,DuttaA,TirupulaK,
CarrBI,GrandisJ,etal:Systematicpredictionofhumanmembrane
receptorinteractions.Proteomics2009,9(23):5243-5255.
16. MelvilleP,Saar-TsechanskyM,ProvostF,MooneyR:Activefeature-value
acquisitionforclassifierinduction.ProceedingsoftheFourthIEEE
InternationalConferenceonDataMiningtableofcontents2004.
17. BreimanL:RandomForests.MachineLearning2001,25:5-32.
18. Saar-TsechanskyM,MelvilleP,ProvostF:Activefeature-valueacquisition.
IROM-08-06McCombsResearchPaperSeriesUniversityofTexasatAustin;
2009.
19. DruckG,SettlesB,McCallumA:Activelearningbylabelingfeatures.
EMNLP2009,ProceedingsofConferenceonEmpiricalMethodsinNatural
LanguageProcessing2009,81-90.
20. AttenbergJ,MelvilleP,ProvostF:Aunifiedapproachtoactivedual
supervision.ECMLPKDD2010,ProceedingsoftheEuropeanConferenceon
MachineLearningandPrinciplesofKnowledgeDiscoveryinDatabases2010.
21. QiY,Bar-JosephZ,Klein-SeetharamanJ:Evaluationofdifferentbiological
dataandcomputationalclassificationmethodsforuseinprotein
interactionprediction.Proteins2006,63(3):490-500.
22. HubbleJ,DemeterJ,JinH,MaoM,NitzbergM,ReddyTB,WymoreF,
ZachariahZK,SherlockG,BallCA:Implementationofgenepatternwithin
thestanfordmicroarraydatabase.NucleicAcidsRes2009,37:D898-901.
23. NgSK,ZhangZ,TanSH,LinK:Interdom:adatabaseofputative
interactingproteindomainsforvalidatingpredictedproteininteractions
andcomplexes.NucleicAcidsRes2003,31:251-254.
24. LiuX,YuX,ZackDJ,ZhuH,QianJ:Tiger:adatabasefortissue-specific
geneexpressionandregulation.BMCBioinformatics2008.
25. FayyadUM,IraniKB:Multi-intervaldiscretizationofcontinuous-valued
attributesforclassificationlearning.IJCAI1993,2:1022-1027.
26. WittenIH,FrankE:Datamining:practicalmachinelearningtoolsand
techniques.Amsterdam;Boston,MA:MorganKaufman;,22005.
27. MohamedTP,CarbonellJG,GanapathirajuMK:Activelearningforhuman
protein-proteininteractionprediction.BMCBioinformatics2010,
11(Suppl1):S57.
28. DonmezP,CarbonellJ:Proactivelearning:cost-sensitiveactivelearning
withmultipleimperfectoracles.CIKM082008. Submit your next manuscript to BioMed Central
29. GoldsteinJ,MittalV,CarbonellJ:Creatingandevaluatingmulti- and take full advantage of:
documentsentenceextractsummaries.CIKM’00:NinthInternational
ConferenceonInformationKnowledgeManagement:20002000,165-172.
• Convenient online submission
30. MadaniO,LizotteDJ,GreinerR:Budgetedlearningofnaivebayes
classifiers.UAI2003. • Thorough peer review
31. KapoorA,GreinerR:Budgetedlearningofboundedactiveclassifiers.
• No space constraints or color figure charges
ProceedingsoftheACMSIGKDDWorkshoponUtility-BasedDataMining2005.
• Immediate publication on acceptance
doi:10.1186/1753-6561-6-S7-S2
• Inclusion in PubMed, CAS, Scopus and Google Scholar
Citethisarticleas:Thahiretal.:Anefficientheuristicmethodforactive
featureacquisitionanditsapplicationtoprotein-proteininteraction • Research which is freely available for redistribution
prediction.BMCProceedings20126(Suppl7):S2.
Submit your manuscript at
www.biomedcentral.com/submit

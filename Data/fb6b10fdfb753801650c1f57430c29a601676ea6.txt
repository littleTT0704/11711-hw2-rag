MRCLens: an MRC Dataset Bias Detection Toolkit
YifanZhong1 HaohanWang12 EricP.Xing13
Abstract a bias-only model based, and then combine it with a full
modeltolearntheadditionalinformation(Sugawaraetal.,
Many recentneuralmodelshave shown remark-
2018). In addition, there are also diagnostictoolssuch as ableempiricalresultsinMachineReadingCom-
interactive frameworks (Leeetal., 2019) or attention ma-
prehension,butevidencesuggestssometimesthe
trixvisualizer(Ru¨ckle´&Gurevych,2017;Liuetal.,2018)
models take advantage of dataset biases to pre-
toevaluateQAmodels. Acommonlimitationoftheseap-
dictandfailtogeneralizeonout-of-sampledata.
proachesiswecannotdiscoverthebiasesuntilthefullmod-
While many other approaches have been pro-
els have been trained and evaluated, which posted a chal-
posedtoaddressthisissuefromthecomputation
lenge for such analysiswhen computationalresourcesare perspectivesuchasnewarchitecturesortraining
limited.
procedures, we believe a method that allows re-
searchers to discover biases, adjust the data or Our study contributes to existing work by introducing a
the models in an earlier stage will be beneficial. toolkit MRCLens which detects bias in MRC datasets.
Thus, we introduce MRCLens, a toolkit which Thistoolkittests agivendatasetagainstseveralknownbi-
detectswhetherbiasesexistbeforeuserstrainthe asesbeforetrainingthefullmodel. Itincludesarelatively
full model. For the convenience of introducing smallermodelandthuscanbeappliedwhencomputational
the toolkit, we also provide a categorization of resources are limited. Our toolkit can be applied to var-
commonbiasesinMRC. ious SQuAD formatted MRC datasets. This also allows
researchers to make adjustments to improve the datasets
or develop models that target the existing biases. Along
1. Introduction
ourimplementationofthetoolkit,wefinditconvenientto
categorize the biases. Thus, our second contribution is a
The ability of machines to read and comprehend texts
summaryofcommonbiasesinMRC.Throughliteraturere-
is a critical skill in natural language processing. Re-
views, we identifyvariousrecurringbiases whichcan fall
centlysophisticatedneuralnetworkmodelssuchasBiDAF
into three categories. We summarize them as Similarity
(Seoetal., 2016), RNet (Wangetal., 2017) and QANet
Bias,KeywordBiasandQuestionBias. Furthermore,we
(Yuetal., 2018) have achieved remarkable accuracies on
introduce the concept of ‘distance’ as a way to measure
several benchmark datasets like SQuAD (Rajpurkaretal.,
MRCbias.Theseconceptswillbediscussedinmoredetail
2016). However, some popular datasets contain super-
insection2.2.
ficial patterns that can be exploited by models to make
predictions without learning much about the contexts
(Wangetal., 2021). As a result, the models might fail 2. Background
to generalize to out-of-sample datasets (Yogatamaetal.,
2.1.RelatedWork
2019;Rimelletal.,2009;Papernoetal.,2016)orinadver-
sarialsettings(Jia&Liang,2017;Wallaceetal.,2019).
The MRC task evaluates a system’s ability to retrieve in-
The community has approached the problem from the formationandmakemeaningfulinferences(Sutcliffeetal.,
modelling perspective (Fischetal., 2019; Takahashietal., 2013). Many recent neural models have shown remark-
2019). For example, a popular example is to first train able results, but some models exploit dataset-specific
patterns which fail to generalize (Clarketal., 2019;
1School of Computer Science, Carnegie Mellon Univer- Talmor&Berant, 2019; Sen&Saffari, 2020). Min et al.
sity 2School of Information Sciences, University of Illinois
observed that 92% of answerable questions in SQuAD
Urbana-Champaign 3Mohamed bin Zayed University of Artifi-
cial Intelligence. Correspondence to: Yifan Zhong <zhongyi- can be answered only using a single context sentence
fan.eva@gmail.com>. (Minetal., 2018). When confounding sentences which
have semantic overlap with the question were added to
DataPerfWorkshop@39thInternationalConferenceonMachine a dataset, the MRC model’s performance dropped signifi-
Learning,Baltimore,Maryland, USA,2022. Copyright2022by
cantly (Jia&Liang, 2017). In another experiment, many
theauthor(s).
2202
luJ
81
]LC.sc[
1v34980.7022:viXra
MRCLens:anMRCDatasetBiasDetectionToolkit
questions in an easier subset of the dataset had their 2.2.CategoriesofdatasetbiasinMRC
answers in the most similar sentence and could be an-
Through the literature review, we observe that the most
swered with word-matching (Sugawaraetal., 2018). In
commonlyseenbiasesinMRCcanfallintothreemaincat-
Story Cloze Test tasks, recognizing the superficial fea-
egories. (1) Some biases directly exploit the relationship
tures is essential for the models to achieve good per-
betweenthequestionandsentencessimilartothequestion
formance (Schwartzetal., 2017). Consequently, many
(that is, question-sentence pairs with high TFIDF scores),
models lack certain advanced skills such as inference or
and we refer to them as Similarity Bias. (2) The biases
multiple-sentencereasoning.
can take advantageof a few key wordsin context. We re-
Biases can also come from a few informative key fertothemasKeywordBias. (3)Thequestionsbythem-
words. For example, entailmentmodelstrained on MNLI selvescontaininformationwhichcanbeexploitedbymod-
(Bowmanetal., 2015) would guess answers based on els to make predictions without carefully reading the pas-
whethera sentence-questionpaircontainsthesame words sage. WerefertothemasQuestionBias.
(McCoyetal., 2019) or solely the existence of keywords
Thethreetypesofbiasesarecloselyrelatedtooneanother.
(Gururanganetal.,2018;Wangetal.,2019). Weissenborn
The similarity between the question and the context usu-
demonstratedthatmorethanathirdof thequestionswere
ally refers to the TFIDF score, which can be understood
answeredusingasimplebaselinemodelwhichprioritized
as the distance between them. In fact, each category of
answers with question words in the surrounding context
biasrelieson‘distance’atdifferentscales. Similaritybias
(Weissenbornetal., 2017). Sugawara showed that certain
and keyword bias rely on the sentence-level or the local
questions might require specific lexical patterns around
keyword-leveldistancefromapassagetothetargetedques-
the correct answer (Sugawaraetal., 2018). Researchers
tion. Likewise,questionbiasexploitsthedistancebetween
have also found certain important words were ignored
question tokens and a passage. In fact, this is not a new
by MRC models (Jia&Liang, 2017; Mudrakartaetal.,
concept. For example, previous researchers have applied
2018), while other less important patterns were overused
thisconcepttoincorporatedistancesupervisiontoenhance
(Mudrakartaetal., 2018). For example, when negations
their QA models(Chengetal., 2020). We are inspiredby
were added to the questions, datasets such as NewsQA
thisabstractiontodesignourexperimentsandfacilitateour
or TriviaQAfailedto updatetheir answers(Sen&Saffari,
discussion.
2020). Other works also found QA models can achieve
good performance with incomplete inputs (Niven&Kao,
2019). 3. OverviewofMRCLens
Furthermore, the questionsby themselvessometimescon- We are inspired by (Sugawaraetal., 2020) to use abla-
taincluesusedbymodelstolocateananswerquickly. As tion experiments to test the impact of biases. Perturbing
early as 1999, the use of bag-of-words, when combined theoriginaldatasetandreevaluatingmodelsusingtheper-
with other heuristics, achieved up to 40% accuracy for turbed data is a method used frequently in various fields
answering interrogative queries (Hirschmanetal., 1999). of NLP (Belinkov&Bisk, 2017; Carlini&Wagner, 2018;
Early researchers designed heuristic-rules based systems Glockneretal.,2018). Sugawaraandtheircolleaguespre-
specifically to answer ‘wh’ questions (Riloff&Thelen, sented 12 requisite skills which could be used to evaluate
2000). In more recent studies, some researchers have anMRCmodel. Foreachskill, theyperformedonecorre-
found that a notable proportion of the questions were spondingablationbyperturbingthedataset. Acomparison
still answerable when incomplete questions were given of the performanceon the original dataset versus the per-
(Sugawaraetal., 2018; Kaushik&Lipton, 2018). Other turbeddatasetwould indicateif the specific requisiteskill
worksshowedthatthemodelswerenotrobustwhenques- isneededbythemodeltoanswerquestions. Theirmethod
tions were paraphrased (Ribeiroetal., 2018; Gan&Ng, fitsthepurposeofourstudy.However,thekeydifferenceis
2019). Chenetalalsofoundtheexistenceofspuriouscor- that, whiletheyareinterestedinif specificrequisiteskills
relationsinWikiHopwhichwereexploitedbythemodelto areneeded,weaimtostudyifspecificbiasesareneededby
achievegoodperformanceusingonlythequestionsandan- amodel.
swerswithoutthecontexts(Chen&Durrett,2019). These
Our toolkit MRCLens incorporates existing works into a
studies suggests that keywords in the question allow the
new tool which can detect if the biases described above
modeltolocatekeyinformationwithouthavingthemodel
exist in a given dataset at an earlier stage of the training
toreadandcomprehendthecontext.
process. MRCLensrequiresdata to be SQuAD formatted
and will be providedvia github. It consists of three main
parts:
(1) A preprocessing module which perturbs the original
MRCLens:anMRCDatasetBiasDetectionToolkit
datasetin8wayscorrespondingtodifferentbiases,andtok- that does not contain the original answer. This enhances
enizesthedata. Specifically,wedividethethreecategories the similarityscore betweenthequestionandanothersen-
ofbiasesfromsection2.2into8biasunitsindexedfrom1 tence. If themodelreliesheavilyonthe mostsimilarsen-
to 8, and we relate each bias unit to one ablation. Define tence to make predictions, then this changewill misguide
Xasthefeaturespace,Yasthelabels,(x,y)asan(input, themodelto lookforanswerspaninthewrongplaceand
label) pair, and f be a model. Let b
i
be a potential bias lead the accuracy to drop. e1 and e2 use a truncated ver-
andm beamethodwhichablatesthefeaturethatprovides sionofthedatasetwhereonlyonequestioniskeptperpas-
i
the correspondinginformationn . Supposef(x) = y for sage, becausemultiplequestionsareoftenaskedbasedon
j
somexinX. Weareinterestediniff(m (x))=y,which onepassagebutitcouldbeconfusingtoinsertinformation
i
meansxcanbesolvedwithoutinformationn . fromallquestions.
i
(2)Aneural-networkMRCmodelwhichtrainsamodeland In e3, we shuffle the sentence order. If the performance
evaluatesit againstboth the originaltest data and the per- doesn’tchangesignificantly,thatmeansthemodelmainly
turbedtestdata. Thismodelisbasedonabaselineneural- relies on information from individual sentences, but not
network model put forward by (Clarketal., 2019). After heavilyonthecontextualrelationshipbetweenthem.
preprocessing, we train a neural-network baseline model
ontheoriginaltrainingdata.Thenforeachbias,wetestthe
baselinemodelagainstthecorrespondingperturbeddataset. Table1.SimilarityBias-f1dropsaftere 1,e 2 andminorchange
The model’s performanceon this new dataset would indi- after e 3 suggest the model relies on context-question similarity
butnotsomuchontheinter-sentencerelationships.
catetowhatextentthespecificbiasimpactstheresult.
ablation em f1 f1drop
(3) An evaluation module which presents the results in e1insertfullquestion 39.72 48.82 30.93
an organizedformatwhich allows for interpretation. MR- e2inserthalfquestion 53.36 64.13 15.62
CLenscomparestheperformancebetweentheoriginaland e3shufflesentenceorder 66.19 74.48 6.13
the modified dataset. By checking whether the questions
are solvable after ablations, we can interpret whether the
WeperformedtwoexperimentstoevaluateQuestionBias.
presenceofa specific biasleadsto unintendedbutcorrect
Ine4,wekeeponlytheinterrogativewordsinthequestion,
answers. Whentheperformancegapissmall,wecaninfer
andine5 weshuffletheorderofwordsinthequestion. Fi-
the bias b is used to answer the questions without n . If
i i nally,therearethreeexperimentswhichmeasureKeyword
the gap is large, a notable proportion of the solved ques-
Bias. We considernouns,verbsandadjectivesfromques-
tionsmayrequiren .
i tionsaspotentialkeywordsandweinsertthemrespectively
toarandomsentenceinthecontextotherthantheonecon-
4. Experiment and Discussion taining the true answer. Like e1 and e2, we use the trun-
cateddevdataset.
4.1.ExperimentSetup
e3,e4,e5 use the original dev dataset with 10570 entries
We use SQuAD (version 1.1) for the experiment. The
whose f1 score is 80.61%, while e1,e2,e6,e7,e8 use the
modelis arecurrentco-attentionmodel(Clarketal., 2019;
truncateddevdatasetwith1943entriesandanf1scoreof
Chenetal., 2016). The model consists of an embed-
79.75%.AccordingtoTable1,accuraciesdroppednotably
ding layer with character CNN, a co-attention layer, and
dueto the addedcontentsfromthe questionseventhough
a shared BiLSTM layer as the pooling layer. We use
everything else remains the same. f1 drops from 80% to
a 0.2 dropout rate, a learning rate decay of 0.999 every
64.13%whenweinserthalfofthequestion,andto48.82%
100 steps(Clarketal., 2019). We use a plain loss func-
whenweinsertthefullquestion. Themodelislikelylook-
tionwhichcomputesthenegativeloglikelihoodgiventhe
ingforanswerinthesentencewherequestionwordswere
model outputs and the labels. Ideally, MRCLens would
inserted,asitisnowthemostsimilarsentence. Theresult
beagnosticofthemodelarchitectures,sincewecaremost from e3 informs us that the sentence order has very little
aboutthechangesinaccuracybeforeandafterablation,not
influence on the model’s prediction. Thus this dataset is
theaccuracyitself.
4.2.ExperimentResults
Table2.QuestionBias-interrogativesalonecanstillbeinforma-
We performed four experiments to measure Similarity tive,andthesequenceofquestionwordsisnotessentialformak-
Bias, which refersto the similarity betweena sentence in ingpredictions
contextandthequestioncalculatedbasedonTFIDFscore. ablation em f1 f1drop
In experiments 1 and 2, we inject noise by adding a part e4interrogativeswords 17.10 23.62 56.99
of the question or the full question in front of a sentence e5shufflequestionwords 56.08 64.05 16.56
MRCLens:anMRCDatasetBiasDetectionToolkit
5. Conclusion
Table3.KeywordBias-keynounsfromquestionsbringthemore
noisetocontextsthanverbsandadjectives. ThisstudypresentsatoolkitMRCLenswhichcanbeused
ablation em f1 f1drop to detect dataset biases at the early stage of a study. MR-
e6insertkeynouns 51.28 62.29 17.46 CLenscanbeappliedtoSQuADformatteddatasets. Itout-
e7insertkeyverbs 58.68 71.07 8.68 putshelpfulinterpretationswhichhelpresearcherstodeter-
e8insertkeyadj. 59.55 72.29 7.46 minetowhatextentbiasesexistinthedatasetofinterest.In
futurework,wehopetoenhancethetoolkittofitdatasetsof
variousformats,designmethodstoquantitativelyevaluate
the toolkit’soutputs,anddevelopmethodologiesforother
MachineComprehensionTasks.
notsuitable for evaluatinga model’sability to understand
References
‘sentence-levelcompositionality‘(Sugawaraetal.,2020).
ResultsfromTable3areconsistentwiththosefromTable Belinkov, Y. and Bisk, Y. Synthetic and natural noise
1. Ourchangesshortenedthelocaldistancebetweenques- both break neural machine translation. arXiv preprint
tionsandwordsorshortphrases. Thedropsinaccuracies arXiv:1711.02173,2017.
suggest the models were misled to some extent to search
Bowman,S. R., Angeli,G., Potts, C., andManning,C. D.
for answers around the inserted words. Nouns retain the
A large annotated corpus for learning natural language
most informationfromquestionsand thusbringmost per-
inference. arXivpreprintarXiv:1508.05326,2015.
turbation to the passages, while verbs and adjectives cap-
turesimilaramountofinformation.
Carlini, N. and Wagner, D. Audio adversarial examples:
Finally,Table2suggeststhequestionsalonecontainindica- Targetedattacksonspeech-to-text. In2018IEEESecu-
tiveinformationthatcouldbeusedwhennotconsideredin rityandPrivacyWorkshops(SPW),pp.1–7.IEEE,2018.
relationtothepassages.In17%ofthecases,interrogatives
Chen, J. and Durrett, G. Understanding dataset de-
aresufficientforthemodeltomakepredictions. e5 shows
sign choices for multi-hop reasoning. arXiv preprint
themodel’sperformanceisaffectedonlyslightlyafterwe
arXiv:1904.12106,2019.
shufflethewordstomakethequestionnon-sensible.
Chen,Q.,Zhu,X.,Ling,Z.,Wei,S.,Jiang,H.,andInkpen,
4.3.Discussion D. Enhancedlstmfornaturallanguageinference. arXiv
preprintarXiv:1609.06038,2016.
The distances between questions and contexts are indica-
tive of how biased the dataset is. For example, e3 shuf-
Cheng, H., Chang, M.-W., Lee, K., and Toutanova, K.
fles the sentence orderbut preservesthe distancebetween
Probabilistic assumptionsmatter: Improvedmodels for
sentences and questions, so it has the least effects on the
distantly-superviseddocument-levelquestionanswering.
performance.Throughexperiments8,7,6,2,1,thenoisewe
arXivpreprintarXiv:2005.01898,2020.
insertedtotheoriginaldatasetgraduallylengthensthe rel-
ativedistancebetweenthecorrectanswer. Asweaddkey Clark, C., Yatskar, M., and Zettlemoyer, L. Don’t take
words or phrases to other parts of the paragraph, the ef- theeasywayout:Ensemblebasedmethodsforavoiding
fectsofsimilaritybiasorkeywordbiasaredilutedbecause knowndatasetbiases. arXivpreprintarXiv:1909.03683,
weenhancetherelevancebetweenthequestionsandother 2019.
partsofthepassages. Thedropinf1scoreincreasesfrom
around8%to30.95%asweincreasethenoisefrominsert- Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E.,
ingkeywordstoinsertingthefullquestions. and Chen, D. MRQA 2019 shared task: Evalu-
ating generalization in reading comprehension. In
Ourmethodalsoprovidesanotherwaytointerpretthesim-
Proceedings of the 2nd Workshop on Machine Read-
ilaritybias. Thedistancebetweenthequestionandthecon-
ing for Question Answering, pp. 1–13, Hong Kong,
text is one of the most discussed biases in MRC. Indeed,
China, November2019.Association forComputational
80%ofourdevdatasethasthecorrectanswerinthemost
Linguistics. doi: 10.18653/v1/D19-5801. URL
similarsentence.e2insertedthefullquestionintoarandom https://aclanthology.org/D19-5801.
sentenceineachpassagesothatthemostsimilarsentence
willalwaysbetheonewherethequestionwasinserted,but Gan,W.C.andNg,H.T.Improvingtherobustnessofques-
despitethischange,themodelstillreachedanexactmatch tionansweringsystemstoquestionparaphrasing.InPro-
scoreof39.72%.Thissuggeststhemodeldidnotover-rely ceedingsofthe 57thAnnualMeetingofthe Association
onthemostsimilarsentence. forComputationalLinguistics,pp.6065–6075,2019.
MRCLens:anMRCDatasetBiasDetectionToolkit
Glockner,M.,Shwartz,V.,andGoldberg,Y. Breakingnli Rajpurkar,P.,Zhang,J.,Lopyrev,K.,andLiang,P. Squad:
systemswithsentencesthatrequiresimplelexicalinfer- 100,000+questionsfor machine comprehensionof text.
ences. arXivpreprintarXiv:1805.02266,2018. arXivpreprintarXiv:1606.05250,2016.
Gururangan,S., Swayamdipta, S., Levy,O., Schwartz, R., Ribeiro, M. T., Singh, S., and Guestrin, C. Semantically
Bowman, S. R., and Smith, N. A. Annotation arti- equivalent adversarial rules for debugging nlp models.
factsin naturallanguageinferencedata. arXiv preprint InProceedingsofthe56thAnnualMeetingoftheAsso-
arXiv:1803.02324,2018. ciationfor ComputationalLinguistics(Volume1: Long
Papers),pp.856–865,2018.
Hirschman,L.,Light,M.,Breck,E.,andBurger,J.D.Deep
Riloff, E. and Thelen, M. A rule-based question answer-
read: Areadingcomprehensionsystem. InProceedings
ing system for reading comprehension tests. In ANLP-
of the 37th annualmeeting of the Association for Com-
NAACL2000Workshop: ReadingComprehensionTests
putationalLinguistics,pp.325–332,1999.
as Evaluation for Computer-Based Language Under-
Jia, R. and Liang, P. Adversarial examples for evalu- standingSystems,2000.
ating reading comprehension systems. arXiv preprint
Rimell,L.,Clark,S.,andSteedman,M.Unboundeddepen-
arXiv:1707.07328,2017.
dencyrecoveryforparserevaluation. InProceedingsof
the 2009 Conference on Empirical Methods in Natural
Kaushik, D. and Lipton, Z. C. How much reading
LanguageProcessing,pp.813–821,2009.
does reading comprehension require? a critical in-
vestigation of popular benchmarks. arXiv preprint
Ru¨ckle´,A.andGurevych,I. End-to-endnon-factoidques-
arXiv:1808.04926,2018.
tionansweringwithaninteractivevisualizationofneural
attentionweights. InProceedingsofACL2017,System
Lee, G., Kim, S., and Hwang, S.-w. Qadiver: Interactive
Demonstrations,pp.19–24,2017.
framework for diagnosing qa models. In Proceedings
of the AAAI Conference on Artificial Intelligence, vol- Schwartz,R.,Sap,M.,Konstas,I.,Zilles,L.,Choi,Y.,and
ume33,pp.9861–9862,2019. Smith, N.A. Storyclozetask: Uw nlpsystem. InPro-
ceedingsofthe2ndWorkshoponLinkingModelsofLex-
Liu, S., Li, T., Li, Z., Srikumar,V., Pascucci, V., andBre- ical, Sentential and Discourse-level Semantics, pp. 52–
mer, P.-T. Visual interrogationof attention-based mod- 55,2017.
elsfornaturallanguageinferenceandmachinecompre-
hension.Technicalreport,LawrenceLivermoreNational Sen,P.andSaffari,A. Whatdomodelslearnfromquestion
Lab.(LLNL),Livermore,CA(UnitedStates),2018. answering datasets? arXiv preprint arXiv:2004.03490,
2020.
McCoy, R. T., Pavlick, E., and Linzen, T. Right
Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H.
for the wrong reasons: Diagnosing syntactic heuris-
Bidirectionalattentionflowformachinecomprehension.
tics in natural language inference. arXiv preprint
arXivpreprintarXiv:1611.01603,2016.
arXiv:1902.01007,2019.
Sugawara, S., Inui, K., Sekine, S., and Aizawa, A. What
Min,S.,Zhong,V.,Socher,R.,andXiong,C. Efficientand
makes readingcomprehensionquestionseasier? arXiv
robust question answering from minimal context over
preprintarXiv:1808.09384,2018.
documents. arXivpreprintarXiv:1805.08092,2018.
Sugawara, S., Stenetorp, P., Inui, K., and Aizawa, A. As-
Mudrakarta,P.K.,Taly,A.,Sundararajan,M.,andDhamd-
sessing the benchmarking capacity of machine reading
here,K. Didthemodelunderstandthequestion? arXiv
comprehension datasets. In Proceedings of the AAAI
preprintarXiv:1805.05492,2018.
Conference on Artificial Intelligence, volume 34, pp.
8918–8927,2020.
Niven,T.andKao,H.-Y. Probingneuralnetworkcompre-
hension of natural language arguments. arXiv preprint Sutcliffe,R.F.,Penas,A.,Hovy,E.H.,Forner,P.,Rodrigo,
arXiv:1907.07355,2019. A.,Forascu,C.,Benajiba,Y.,andOsenova,P. Overview
of qa4mre main task at clef 2013. In CLEF (Working
Paperno,D., Kruszewski, G., Lazaridou,A., Pham, Q.N.,
Notes),2013.
Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and
Ferna´ndez, R. The lambada dataset: Word prediction Takahashi, T., Taniguchi, M., Taniguchi, T., and Ohkuma,
requiring a broad discourse context. arXiv preprint T. CLER: Cross-task learning with expert represen-
arXiv:1606.06031,2016. tation to generalize reading and understanding. In
MRCLens:anMRCDatasetBiasDetectionToolkit
Proceedings of the 2nd Workshop on Machine Read-
ing for Question Answering, pp. 183–190, Hong Kong,
China, November2019.Association forComputational
Linguistics. doi: 10.18653/v1/D19-5824. URL
https://aclanthology.org/D19-5824.
Talmor,A.andBerant,J. Multiqa: Anempiricalinvestiga-
tionofgeneralizationandtransferinreadingcomprehen-
sion. arXivpreprintarXiv:1905.13453,2019.
Wallace,E.,Feng,S.,Kandpal,N.,Gardner,M.,andSingh,
S. Universaladversarialtriggers for attacking and ana-
lyzingnlp. arXivpreprintarXiv:1908.07125,2019.
Wang,H.,Sun,D.,andXing,E.P.Whatifwesimplyswap
the two text fragments? a straightforward yet effective
way to test the robustness of methods to confounding
signals in nature language inference tasks. In Proceed-
ings of the AAAI Conference on Artificial Intelligence,
volume33,pp.7136–7143,2019.
Wang, W. et al. R-net: machine reading comprehension
withself-matchingnetworks.naturallanguagecomputer
group,microsoftreserach.asia,beijing.Technicalreport,
China,TechnicalReport5,2017.
Wang, X., Wang, H., andYang, D. Measureandimprove
robustness in nlp models: A survey. arXiv preprint
arXiv:2112.08313,2021.
Weissenborn,D.,Wiese,G.,andSeiffe,L. Makingneural
qaassimpleaspossiblebutnotsimpler. arXivpreprint
arXiv:1703.04816,2017.
Yogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,
T.,Chrzanowski,M.,Kong,L.,Lazaridou,A.,Ling,W.,
Yu,L., Dyer,C., etal. Learningandevaluatinggeneral
linguisticintelligence.arXivpreprintarXiv:1901.11373,
2019.
Yu, A. W., Dohan, D., Luong, M.-T., Zhao, R., Chen, K.,
Norouzi, M., and Le, Q. V. Qanet: Combining local
convolution with global self-attention for reading com-
prehension. arXivpreprintarXiv:1804.09541,2018.

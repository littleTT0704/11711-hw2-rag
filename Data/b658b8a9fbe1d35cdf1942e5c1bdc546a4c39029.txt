The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
Using Syntax to Ground
Referring Expressions in Natural Images
VolkanCirik,TaylorBerg-Kirkpatrick,Louis-PhilippeMorency
SchoolofComputerScience
CarnegieMellonUniversity
Pittsburgh,PA15213
{vcirik,tberg,morency}@cs.cmu.edu
Abstract it?Tomakeeffectiveuseofanidentifiedsupportingobject,
wemustunderstandhowthisobjectisrelatedtothetarget.
WeintroduceGroundNet,aneuralnetworkforreferringex-
Andfinally,formanynaturalreferringexpressions,thepro-
pression recognition – the task of localizing (or grounding)
cess is recursive: a supporting object may itself be identi-
inanimagetheobjectreferredtobyanaturallanguageex-
fied by a relationship with another supporting object. As a
pression. Our approach to this task is the first to rely on a
result, models that reason about referring expressions must
syntacticanalysisoftheinputreferringexpressioninorderto
informthestructureofthecomputationgraph.Givenaparse respectthishierarchy,processingsub-expressionsbeforeat-
treeforaninputexpression,weexplicitlymapthesyntactic tacking larger expressions. Modeling this compositionality
constituents and relationships present in the tree to a com- iscriticaltodesigningrecognitionsystemsthatbehaveinan
posedgraphofneuralmodulesthatdefinesourarchitecture interpretablewayandcanjustifytheirdecisions.
forperforminglocalization.Thissyntax-basedapproachaids
Inthispaper,weintroduceGroundNet,thefirstdynamic
localization of both the target object and auxiliary support-
neuralarchitectureforreferringexpressionrecognitionthat
ingobjectsmentionedintheexpression.Asaresult,Ground-
takes full advantage of syntactic compositionality. Past ap-
Netismoreinterpretablethanpreviousmethods:wecan(1)
determinewhichphraseofthereferringexpressionpointsto proaches, such as the Compositional Modular Networks
whichobjectintheimageand(2)trackhowthelocalization (CMN) model (Hu et al. 2017), have relied on limited
ofthetargetobjectisdeterminedbythenetwork.Westudy syntactic information in processing referring expressions –
this property empirically by introducing a new set of anno- for example, CMN tracks a single supporting object – but
tations on the GoogleRef dataset to evaluate localization of have not modeled linguistic recursion and therefore is in-
supporting objects. Our experiments show that GroundNet capable of tracking multiple supporting objects. As shown
achieves state-of-the-art accuracy in identifying supporting
in Figure 1, our GroundNet framework relies on a syntac-
objects,whilemaintainingcomparableperformanceinthelo-
tic parse of the input referring expression to dynamically
calizationoftargetobjects.
create a computation graph that reflects the recursive hi-
erarchy of the input expression. As a result, our approach
1 Introduction tracks intermediate localization decisions of all supporting
objects. Following the approach of (Andreas et al. 2016b;
Spatialreferringexpressionsarepartofoureverydaysocial
2016a),thiscomputationgraphistranslatedintoaneuralar-
life(“Pleasedropmeatthebluehousenexttotheredmail-
chitecture that keeps interpretable information at each step
box.”)andalsopartofprofessionalinteractions(“Couldyou
oftheway,ascanbeseeninFigure1d.
pass the small scalpel to the right of the forceps?”). These
natural language expressions are designed to uniquely lo- Weadditionallypresentanewsetofannotationsthatspec-
cateanobjectinthevisualworld.Theprocessofgrounding ify the correct locations of supporting objects in a portion
referring expressions into visual scenes involves many in- of the standard benchmark dataset, GoogleRef (Mao et al.
termediatechallenges.Asafirststep,wewanttolocateall 2016) to evaluate the interpretability of models for refer-
theobjectsmentionedintheexpression.Whileoneofthese ring expression recognition. Using these additional annota-
mentionsreferstothetargetobject,theothermentions(i.e. tions,ourempiricalevaluationsdemonstratethatGoundNet
supportingobjectmentions)arealsoimportantbecausethey substantially outperforms the state-of-the-art at intermedi-
wereincludedbytheauthorofthereferringexpressioninor- atepredictionsofthesupportingobjects,yetmaintainscom-
dertodisambiguatethetarget.Infact,Grice(1975)argued parable accuracy at target object localization. These results
that supporting objects will only be mentioned when they demonstratethatsyntacticcompositionalitycanbesuccess-
arenecessaryfordisambiguation.Asasecondstep,wewant fully used to improve interpretability in neural models of
toidentifythespatialrelationshipsbetweentheseobjects.Is languageandvision.Ourannotationsforsupportingobjects
the target to the left of the supporting object? Is it beneath
andimplementationsareavailableforpublicuse1.
Copyright(cid:2)c 2018,AssociationfortheAdvancementofArtificial
Intelligence(www.aaai.org).Allrightsreserved. 1https://github.com/volkancirik/groundnet
6756
(a)Anexamplereferringexpressionfromourvalidationset“halfofa
sandwichontherightsideofaplatenearestacoffeemug”.Orange
boxes are region candidates and green box is the referred bounding (b)Theparsetreeforthereferringexpressionin(a).
box.
(cid:45)(cid:221)(cid:253)(cid:178)(cid:242)(cid:246)(cid:178)(cid:168)(cid:253)
(cid:60)(cid:228)(cid:168)(cid:156)(cid:253)(cid:178) (cid:87)(cid:178)(cid:214)(cid:156)(cid:253)(cid:178)
(cid:75)(cid:68)(cid:79)(cid:73)(cid:3)(cid:86)(cid:68)(cid:81)(cid:71)(cid:90)(cid:76)(cid:70)(cid:75) (cid:82)(cid:81)(cid:3)(cid:85)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:86)(cid:76)(cid:71)(cid:72)(cid:3) (cid:45)(cid:221)(cid:253)(cid:178)(cid:242)(cid:246)(cid:178)(cid:168)(cid:253)
(cid:60)(cid:228)(cid:168)(cid:156)(cid:253)(cid:178) (cid:87)(cid:178)(cid:214)(cid:156)(cid:253)(cid:178)
(cid:83)(cid:79)(cid:68)(cid:87)(cid:72) (cid:81)(cid:72)(cid:68)(cid:85)(cid:72)(cid:86)(cid:87) (cid:60)(cid:228)(cid:168)(cid:156)(cid:253)(cid:178)
(d)Groundingofobjectsin(a)withthecomputationgraphin(c).The
(cid:70)(cid:82)(cid:73)(cid:73)(cid:72)(cid:72)(cid:3)(cid:80)(cid:88)(cid:74)
morevisibleobjectshavehigherprobabilities.Notethatthemodelis
(c)Computationgraphfortheparsetreein(b). abletogroundsupportingobjectslikethecoffeemug.
Figure 1: An Overview of GroundNet. A referring expression (a) is first parsed (b). Then, the computation graph of neural
modulesisgeneratedusingtheparsetree(c).Eachnodelocalizesobjectspresentintheimage(d).
2 GroundNet blocks and (ii) the recursive rules to combine them. In our
case,thebuildingblocksfortheGroundNetisgroundingof
In this section, we explain the motivation of GroundNet,
objectsi.e.theprobabilityofhowlikelyanobjectisforword
howwegeneratethecomputationgraphforGroundNet,and
phrases. The combining rules are defined by the parse tree
finally,detailtheneuralmodulesthatweuseforcomputing
describing what these objects are and how they are related
thelocalizationthereferringexpressions.
toeachother.
GroundNet models the processing of a referring expres-
Motivation
sion in a computation graph (see Figure 1c) based on the
A referring expression disambiguates a target object using parsetreeofthereferringexpression(seeFigure1b).Nodes
the object’s discriminative features such as color, size, tex- of the computation graph have 3 different types aiming to
ture etc., and their relative position to other supporting ob- capturethenecessarycomputationsforlocalizingthetarget
jects.Figure1ashowsacanonicalexamplefromourtaskw object. Locate nodes ground a noun phrase (“half sand-
onehalfofasandwichisreferredby“halfofasandwichon wich”,“plate”,“coffeemug”),i.e.pointinghowlikelythata
therightsideofaplatenearestacoffeemug”.Herethesand- given noun phrase refers to an object present in the image.
wichisdisambiguatedusingrelativeclauses(e.g.“theright Forexample,inFigure1d,Locatenodeofthephrase“half
sideof”,“nearest”)andthesupportingobjects(e.g“plate”, sandwich” outputs higher probabilities for both halves of
“coffee mug”). We observe that there is a correspondence sandwichescomparedtootherobjects.Prepositionalphrases
betweenthelinguisticcompositionalstructure(i.e.theparse (“onrightside”,“nearest”)correspondtoRelatenodesin
tree)ofthereferringexpressionandtheprocessofresolving thecomputationgraph.Relatenodescalculatehowlikely
a referring expression. In Figure 1b, we see that the target objects are related to the grounding of objects with given
object and supporting objects have a noun phrase (NP) on prepositionalphrase.Forinstance,inFigure1c,theRelate
the parse tree of the referring expression. Also, the relative node of “nearest” computes how likely the objects are re-
positioningofobjectsintheimage(e.g.beingontheright, lated to the grounding of “coffee mug” with the relation
ornear)correspondtoprepositionalphrases(PP)onthetree. “nearest”.Weconvertthephrasescomingfrombranchesin
We design GroundNet based on this observation to local- the parse tree to Intersect nodes. It simply intersects
ize the target object by modeling the compositional nature two sets of groundings so that objects that have high like-
of the language. The compositionality principle states that lihoodinbothbrancheswillhavehighprobabilitiesforthe
themeaningofaconstituentisafunctionof(i)itsbuilding output(seetherootnodeinFigure1d).Sinceeachnodeof
6757
thiscomputationgraphoutputsagroundingforitssubgraph, where I is an image, R is the set of bounding boxes r i
GroundNetisinterpretableasawhole.Ateachnode,wecan of objects present in the image I, and X is a referring ex-
visualizehowmodel’smultiplepredictionsforobjectsprop- pressiondisambiguatingatargetobjectinboundingboxr∗.
agatesthroughthecomputationgraph. Ouraimistopredictr∗ processingthereferringexpression
Infollowingsections,wedetailhowwegeneratethecom- in a computational graph with neural modules. In addition
putationgraphandtheneuralmodulesusedinGroundNet. to(I,R,X),neuralmodulesusetheoutputofotherneural
modulesandthetextspanT ofthecomputationnode.
GeneratingaComputationGraph We detail parameterization of neural modules in follow-
GroundNet processes the referring expression with a com- ing subsections and visualize them in Figure 2 for clarity.
putation graph (Figure 1c) based on to the parse tree (Fig-
ure1b)ofthereferringexpression.First,weparsetherefer-
ringexpressionwithStanfordParser(Manningetal.2014). Attend This module induces a text representation for
Then,wegeneratethecomputationgraph(seeFigure1b,1c Locate and Relate nodes. It takes the words {w i} i|T =|
1
for an example) for a parse tree with a recursive algorithm andembedsthemtoawordvector{e i}| iT =| 1.A2-layerbidi-
(seeAlgorithm1). rectional LSTM network (Schuster and Paliwal 1997) pro-
cesses embedded words. Both forward and backward layer
Algorithm1:GenerateComputationGraph representationsareconcatenatedforbothlayersintoasingle
hiddenrepresentationforeachwordasfollows:
1: procedureGenerateComputationGraph(tree)
2 3: : l re igft htN NP P= =F Fin id nN dNP P(t (r te re e. ele .rf it g) ht) h i =[h( i1,fw)h( i1,bw)h( i2,fw)h( i2,bw)] (1)
4: ifleft NP==””then
5: return(Locatetree.text) Theattentionweightsarecomputedwithalinearprojection
usingWa:
6: endif
7: Relate=FindPP(tree,[left NP,right NP]) exp(Wah )
8 9: : left cg=GenerateComputationGraph(left NP) a i = (cid:2) | iT =| 1exp(Wi ah i) (2)
10: right cg=GenerateComputationGraph(right NP)
11: return(Intersect(left cg)(Relateright cg)) TheoutputofAttendistheweightedaverageofwordvec-
12: endprocedure torse iwheretheweightsareattentionsa i.
(cid:3)|T|
Above, the function FindNP finds the noun-phrase with f a(T;Θ a)= a ie i (3)
the largest word span of given root node for left and right i=1
branches(line 2,3).IfthetreedoesnothaveanNPsubtree,
itreturnsaLocatenode(line 5). The learned parameters Θ a of this module are the parame-
FindPP extracts the words between noun-phrases to
tersof2-layerbidirectionalLSTMandscoringmatrixWa.
modeltherelationshipbetweenthemandreturnsaRelate
node (line 7). For both left and right branches of the Locate Thismodulepredictswhichobjectisreferredto
parse tree, the same algorithm is recursively called (lines foratextspan,i.e.nounphrase,inthereferringexpression.
9,10).Finally,thesub-computationgraphsofleftandright Itcomputestheprobabilitydistributionoverboundingboxes
branchesaremerged(line 11)intoanIntersectnode. using the output of Attend and feature representations of
Eachnodeinthecomputationgraphisdecoratedwiththe bounding boxes. For instance in Figure 1c, Locate node
phraseT usingthetextspan,i.e.constituents,ofthecorre- withinput“halfsandwich”localizesobjectsbyscoringeach
sponding parse tree node. We filter out the function words bounding box. Locate node does so by scoring how well
suchasdeterminers‘a‘and‘the‘.Forinstance,theLocate the text span “half sandwich” matches the content of each
on the left in Figure 1c has the span of words “half sand- boundingbox.
wich” from the corresponding noun phrase “the half of a Torepresentaboundingboxr,weusespatialandvisual
sandwich”inFigure1b. features.First,visualfeaturesr vis fortheboundingboxare
Inthefollowingsection,weexplainthesetofneuralmod- extracted using a convolutional neural network (Ren et al.
ules that we design for performing the localization of the 2015).Second,spatialfeaturesrepresentpositionandsizeof
referringexpressiononacomposedcomputationgraph. theboundingbox.Wehave5-dimensionalvectorsforspatial
NeuralModules tf hea etu sr ize es r as np dat [x= mi[ nx ,Wm yi I mn, iny ,Hm xIin m, axx Wm ,ya I mx, axy Hm ]a I ax re,S S cr I o] ow rdh iner ae teS sr fois
r
We operationalize the computational graph for a referring bounding box r and S I, W I, H I are area, width, and the
expressionintoanend-to-endneuralarchitecturebydesign- height of the input image I. These two representations are
ing neural modules that represent each node of our graph. concatenated as r vis,spat = [r visr spat] for a bounding box
First, let us introduce the notation for referring expression
r.
task. For each referring expression, (I,R,X) are inputs We follow the previous work (Hu et al. 2017) for
6758
(cid:45)(cid:221)(cid:253)(cid:178)(cid:242)(cid:246)(cid:178)(cid:168)(cid:253)
(cid:60)(cid:228)(cid:168)(cid:156)(cid:253)(cid:178) (cid:87)(cid:178)(cid:214)(cid:156)(cid:253)(cid:178)
(cid:55)(cid:3)(cid:32)(cid:3)(cid:112)(cid:81)(cid:72)(cid:68)(cid:85)(cid:72)(cid:86)(cid:87)(cid:113)
(cid:55)(cid:32)(cid:112)(cid:83)(cid:79)(cid:68)(cid:87)(cid:72)(cid:113) (cid:55)(cid:32)(cid:112)(cid:81)(cid:72)(cid:68)(cid:85)(cid:72)(cid:86)(cid:87)(cid:113) (cid:60)(cid:228)(cid:168)(cid:156)(cid:253)(cid:178)
(cid:53)(cid:3)(cid:32)(cid:94)(cid:20)(cid:17)(cid:17)(cid:85)(cid:17)(cid:85) (cid:96)
(cid:76) (cid:95)(cid:53)(cid:95)(cid:3) (cid:55)(cid:32)(cid:112)(cid:70)(cid:82)(cid:73)(cid:73)(cid:72)(cid:72)(cid:3)(cid:80)(cid:88)(cid:74)(cid:113)
(cid:4)(cid:253)(cid:253)(cid:178)(cid:221)(cid:174) (cid:90) (cid:20)(cid:171)(cid:3)(cid:90) (cid:95)(cid:55)(cid:95)(cid:3) (cid:72) (cid:20)(cid:3)(cid:3)(cid:17)(cid:17)(cid:3)(cid:72) (cid:95)(cid:55)(cid:95)(cid:3) (cid:21)(cid:16)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3)(cid:69)(cid:76)(cid:47)(cid:54)(cid:55)(cid:48) (cid:68) (cid:90)(cid:87)(cid:87) (cid:72)(cid:72) (cid:76)(cid:81) (cid:74)(cid:87) (cid:75)(cid:76)(cid:82) (cid:87)(cid:76)(cid:81) (cid:81)(cid:3) (cid:74)(cid:9)(cid:3) (cid:75) (cid:55)(cid:3)
(cid:73)(cid:11)(cid:55)(cid:12)
(cid:68)
(cid:60)(cid:228)(cid:168)(cid:156)(cid:253)(cid:178) (cid:4)(cid:253)(cid:253)(cid:178)(cid:221)(cid:174)(cid:3)(cid:73)(cid:11)(cid:55)(cid:12)
(cid:79)(cid:82)(cid:70)(cid:11)(cid:55)(cid:15)(cid:53)(cid:12) (cid:68) (cid:86)(cid:20) (cid:70)(cid:16)(cid:79) (cid:82)(cid:68) (cid:85)(cid:92) (cid:76)(cid:81)(cid:72) (cid:74)(cid:85)(cid:3) (cid:20) (cid:21) (cid:22) (cid:17)(cid:17)(cid:17) (cid:95)(cid:53)
(cid:85)(cid:171)(cid:3)(cid:85)
(cid:20) (cid:95)(cid:53)(cid:95)(cid:3) (cid:83)
(cid:79)(cid:82)(cid:70)(cid:3)
(cid:87)(cid:178)(cid:214)(cid:156)(cid:253)(cid:178)
(cid:4)(cid:253)(cid:253)(cid:178)(cid:221)(cid:174)(cid:3)(cid:73)(cid:11)(cid:55)(cid:12)
(cid:11)(cid:55)(cid:15)(cid:3)(cid:53)(cid:15)(cid:3)(cid:83)(cid:12) (cid:68) (cid:20)(cid:16)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3) (cid:76)(cid:15)(cid:77)
(cid:86)(cid:70)(cid:82)(cid:85)(cid:76)(cid:81)(cid:74)
(cid:85)
(cid:76)
(cid:85) (cid:77) (cid:95)(cid:53)(cid:95)(cid:91)(cid:95)(cid:53)(cid:95) (cid:59) (cid:20) (cid:21) (cid:22) (cid:17)(cid:17)(cid:17) (cid:95)(cid:53)
(cid:83)
(cid:85)(cid:72)(cid:79)
(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)
(cid:45)(cid:221)(cid:253)(cid:178)(cid:242)(cid:246)(cid:178)(cid:168)(cid:253) (cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:83)(cid:79)(cid:92)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3) (cid:17)(cid:17)(cid:17)
(cid:11)(cid:83)(cid:15)(cid:83)(cid:12) (cid:81)(cid:82)(cid:85)(cid:80)(cid:68)(cid:79)(cid:76)(cid:93)(cid:72) (cid:20) (cid:21) (cid:22) (cid:95)(cid:53)
(cid:81)(cid:87) (cid:20) (cid:21)
(cid:17)(cid:17)(cid:17) (cid:83)
(cid:76)(cid:81)(cid:87)
Figure 2: Illustrations of GroundNet’s neural modules. Upper left shows an example referring expression and the input for
Relatenode(upperright,highlightedinred)ofasmallsectionofacomputationgraph.Modulestakeinputsfrommodule’s
textspanT,thesetofboundingboxesR,andoutputprobabilitiesofothernodesp i.Bestseenincolor.
parametrizationofLocate. do not define a set of relationships for Relate, instead,
rˆ vis,spat =W vlo isc ,spatr vis,spat (4) m ulo e’d se tl exle ta rr en ps reh so ew ntao tb ioje nc .t Ss pr ee cla ifite cat lo lye ,a thc ih so mth oe dr ulu es cin og mm puo td es-
z loc =rˆ vis,spat(cid:2)f a(T) (5) arelationshipscorematrixS rel ofsizeR×Rconsistingof
zˆ loc =z loc/||z loc || 2 (6) scoresforboxiandj asfollows:
s loc =W sl co oc rezˆ loc (7) rˆ i,j =W sr pe al tr i,j (10)
p loc =softmax(s loc) (8) z rel =rˆ i,j (cid:2)f a(T) (11)
f loc(T,R;Θ loc)=p loc (9) zˆ loc =z rel/||z rel || 2 (12)
F tei xrs tt r, er pv ri es s, esp na tat tii os np cro oj mec inte gd frto omth te hesa Am te tedi nm den (Esi qon 4)a .s Tt eh xe
t
S rel[i,j]=W sr ce ol rezˆ rel (13)
and box representations are element-wise multiplied to get p rel =S relp (14)
z loc forajointrepresentationofthetextandboundingbox. f rel(T,R,p;Θ rel)=p rel (15)
We normalize with L2-norm into zˆ loc (Eq 5, 6). Localiza-
t ji oo in nts rc eo pr re ess elo nc tai ts ioc nal (c Eu qlat 7e )d
.
Lw oit ch ala izl ai tn ie oa nr sp cr oo rj ee sct aio ren fo ef dth toe A r ib ,jov =e, [s rp i,a st pi aa tl ,r re jp ,sre pase t]nt aa nt dion ps roo jef cb to edxe is nta ore thc eon sc aa mte en dat ie md enas -
softmax to form a probability distribution p loc over boxes.
sionastextrepresentation(Eq10).SimilartoLocate,text
ThelearnedparametersΘ locofthismodulearethematrices andboxrepresentationsarefusedwithelement-wisemulti-
Wloc andWloc . plicationandL2-normalization(Eq11,12),thenboxpairis
vis,spat score
scoredlinearly(Eq13).
Relate predictshowlikelyanobjectrelatestotheother Finally, the probability distribution p rel over bounding
objectswithsomerelationdescribedbythenode’stextspan. boxesiscalculatedasp rel =S relp loc.Thelearnedparame-
For instance, the relation “nearest” in Figure 1d holds for tersΘ relofthismodulearethematricesW sr pe al tandW sr ce ol re.
half-sandwich pairs, and a half-sandwich and coffee mug
pair. Since the incoming Locate node to Relate out- Intersect This module combines groundings coming
putsahighprobabilityforthecoffeemug,onlyobjectsnear fromtwobranchesofthecomputationgraphbysimplymul-
to coffee mug have a high probability. GroundNet does so tiplying object probabilities and normalizing it to form a
byfirstcomputingarelationshipscorematrixforboxesand probabilitydistribution.Inthefollowingsection,weexplain
multiplyingthescoringmatrixwiththegroundinginput.We ourexperimentalsetup.
6759
3 Experiments (Linetal.2014).HiddenlayersizeofLSTMnetworkswas
searched over the range of {64,128,...,1024} and picked
Now,wedetailourexperimentalsetup.Inourexperiments,
basedonbestvalidationsplitwhichis2,5%oftrainingdata
weareinterestedinfollowingresearchquestions:
separated from training split. Following the previous work
• (RQ1)Howsuccessfulmodelsareincorporatingthesyn-
(Huetal.2017),weusedofficialvalidationsplitasthetest.
taxandhowimportantthedynamicandmodularcompu-
We initialized all parameters of the model with Xavier ini-
tationinexploitingthesyntacticinformation?
tialization(GlorotandBengio2010)andusedweightdecay
• (RQ2)Whataretheaccuraciesofmodelsforsupporting rate of 0.0005 as regularization. Next, we explain models
objects and how these accuracies change depending on usedinourexperiments.
thesyntacticinformation?
Now,weexplaindatasetsusedforourexperiments. Baseline Models. We compare GroundNet to the re-
cent models from the literature. RecursiveNN Socher et
al. (2014) use the recursive structure of syntactic parses of
Referring Expression Dataset. We use the standard
sentencestoretrieveimagesdescribedbytheinputsentence.
Google-Ref (Mao et al. 2016) benchmark for our experi-
The text representation of a referring expression is recur-
ments.Google-Refisadatasetconsistingofaround26Kim-
sively calculated following the parse tree of the referring
ages with 104K annotations. We use ”Ground-Truth” eval-
expression. The text representation at root node is jointly
uationsettingwherethegroundtruthboundingboxannota-
scored with bounding box representations and the highest
tionsfromMSCOCO(Linetal.2014)areused.
scoring box is predicted. LSTM + CNN - MMI Mao et
al. (2016) use LSTMs for processing the referring expres-
SupportingObjectsDataset. Wealsoinvestigatetheper- sion and CNNs for extracting features for bounding boxes
formances of models in terms of interpretability. We mea- andthewholeimage.ModelistrainedwithMaximumMu-
sure the interpretability of a model by its accuracy on both tualInformationtraining.LSTM+CNN-MMI+visdifYu
targetandsupportingobjects.Tothisend,wepresentanew et al. (2016) introduce contextual features for a bounding
setofannotationsonGoogle-Refdataset.First,werunapi- box by calculating differences between visual features for
lotstudyonMTurkwhereallboundingboxesandtherefer- objectpairs.LSTM+CNN-MIL3Nagaraja,Morariu,and
ringexpressionpresenttoannotators2.Ourin-houseannota- Davis(2016)scoreobject-supportingobjectpairs.Thepair
tor has an agreement of 0.75 - a standard metric in word withthehighestscoreispredicted.TheyuseMultiInstance
alignment literature (Graca et al. 2008; Ozdowska 2008) Learning for training the model. CMN4 Hu et al. (2017)
withthreeturkersonasmallvalidationsetof50instances. introduce a neural module network with a tuple of object-
Overall, our annotator labeled 2400 instances – but only relationship-subjectnodes.Thetextrepresentationoftuples
1023hadatleastonesupportingobjectboundingbox. arecalculatedwithanattentionmechanism(Bahdanau,Cho,
andBengio2014)overthereferringexpression.Wealsore-
NumberofSupportingObjects 0 1 2 3 4 portresultsforCMN-syntaxguidedwhenaparsetreeis
usedforextractingtheobject-relationship-subjecttuples.
NumberofInstances 1377 891 118 11 3
GroundNet with varying level of syntax. We investi-
Table 1: Statistics for the number of supporting objects for
gate the effect of the syntax varying the level of use of the
annotated2400instances.
syntactic structure for GroundNet. GroundNet is the orig-
inal model presented in the previous section where each
We remind that the training data does not have any an-
node in computation graph uses the node’s text span for
notations for supporting objects. Models should be able to Attend.ForGroundNet-syntax-guidedLocatemodel,
predict supporting objects using only target object supervi- Locate nodes use the node’s text span as an input to the
sion and text input. We should emphasize that our work is Attend module. Whereas for Relate nodes can use all
thefirsttoreportquantitativeresultsonsupportingobjectfor
referringexpressionforinducingthetextrepresentation.For
thereferringexpressiontaskandwereleaseourannotation GroundNet-free-formmodel,BothLocateandRelate
forfuturestudies.Next,weprovidedetailsofourimplemen-
nodes use all of the referring expression as the input to
tation. Attend. Next, we explain our evaluation metrics used in
ourexperiments.
Implementation Details. We trained GroundNet with
backpropagation.Weusedstochasticgradientdescentfor6
Evaluation. To evaluate models for referring expression
epochswithandinitiallearningrateof0.01andmultiplied
task we use the standard metric of accuracy. For evalua-
by 0.4 aftereach epoch. Word embeddings were initialized
tionofsupportingobjects,whentherearemultiplesupport-
with GloVe (Pennington, Socher, and Manning 2014) and
ing objects, we consider a supporting object prediction as
finetuned during training. We extracted features for bound-
ing boxes using fc7 layer output of Faster-RCNN VGG-16 3Originallytheauthorsuseanewtestsplit,whereas,wereport
network(Renetal.2015)pre-trainedonMSCOCOdataset resultsforthestandardsplitofthedatasetforthismodel.
4Wereportresultsforourreimplementationofthismodelwhere
2Wedidnotprovidetheparsetreestonotbiastheannotators. wedidhyperparametersearchthesameasourmodel.
6760
Model Syntax DynamicComputation Modularity Relationships Supporting(%) Accuracy(%)
LSTM+CNN-MMI 60.7
LSTM+CNN-MMI+visdif (cid:2) 64.0
LSTM+CNN-MIL (cid:2) 15.0 67.3
CMN (cid:2) (cid:2) 11.1 69.7
RecursiveNN (cid:2) (cid:2) 51.5
CMN-syntaxguided (cid:2) (cid:2) (cid:2) 53.5
GroundNet (cid:2) (cid:2) (cid:2) (cid:2) 60.6 65.7
GroundNet-syntax-guidedLocate (cid:2) (cid:2) (cid:2) (cid:2) 60.0 66.7
GroundNet-free-form (cid:2) (cid:2) (cid:2) (cid:2) 10.6 68.9
Table 2: The accuracy of models with the support of syntax, dynamic computation, modularity, relationship modeling, and
supportingobjectpredictions.Ourmodelisthefirstsyntax-basedmodelwithsuccessfulresultsandachievesthebestresultsin
supportingobjectlocalization.
accurate only if at least one supporting object is correctly ing interpretable and being accurate for models. We quali-
classified. To evaluate approaches modeling the supporting tativelyshowacoupleofinstancesfromtestsetGroundNet
objects we use following methods. For LSTN+CNN-MIL, andCMNinFigure3.Asanexample,forthefirstinstance,
we use the context object of the maximum scoring target- both GroundNet and CMN successfully predict the target
context object pair as the supporting object. For CMN, we object.GroundNetisabletolocalizebothsupportingobjects
usetheobjectwiththemaximumobjectscoreofasubject- (i.e.thegirlandthedisc)mentionedinthereferringexpres-
relation-objecttupleasthepredictionforthesupportingob- sion,whereas,CMNfailstolocalizethesupportingobjects.
ject.ForGroundNet,weusetheobjectwithmaximumprob- Next,wereviewthepreviousworkrelatedtoGroundNet.
abilityasapredictionforintermediatenodesinthecompu-
tationgraph.Inthefollowingsection,wediscussresultsof 5 RelatedWork
ourexperiments.
Referringexpressiongrecognitionisawell-studiedproblem
4 Results in human-robot interaction (Chai, Hong, and Zhou 2004;
Zender, Kruijff, and Kruijff-Korbayova´ 2009; Tellex et al.
We presented overall results in Table 2 for the compared
2011; Lemaignan et al. 2011; Fang, Liu, and Chai 2012;
models.WenowdiscusscolumnsoftheTable2.
Williams et al. 2016). Here, we focus on more closely re-
latedstudieswherevisualcontextisarichsetofreal-world
(RQ1) Syntax, Dynamic Computation, and Modularity. images or language with rich vocabulary is modeled with
GroundNetvariationsachievethebestresultsamongsyntax- compositionality.
based models. “Recursive NN” homogeneously processes
the referring expression throughout the parse tree struc-
GroundingReferentialExpressions. Themostofthere-
ture. On the other hand, GroundNet modularly parameter-
centwork(Maoetal.2016;Huetal.2016;Rohrbachetal.
izes multi-modal processing of localization and relation-
2016;Fukuietal.2016;Yuetal.2016;Nagaraja,Morariu,
ships.“CMN-syntaxguided”hasafixedcomputationgraph
and Davis 2016) addresses grounding referential expres-
ofasubject-relation-objecttuple,whereas,GroundNethasa
sion task with a fixed computation graph. In earlier stud-
dynamiccomputationgraphforeachinstance,thus,avary-
ies (Mao et al. 2016; Hu et al. 2016; Rohrbach et al. 2016;
ingnumberofcomputationnodesareinduced.Whencom-
Fukuietal.2016),theboundingboxesarescoredbasedon
paredtoothersyntax-basedapproaches,GroundNet results
their CNN and spatial features along with features for the
showthatadynamicandmodulararchitectureisessentialto
whole image. Since each box is scored in isolation, these
achievecompetitiveresultswithasyntax-basedapproach.
methods ignore the object relationships. More recent stud-
ies (Yu et al. 2016; Nagaraja, Morariu, and Davis 2016;
(RQ2) Syntax for Supporting Objects. Our model Hu et al. 2017) show that modeling relationship between
achieves the highest accuracy on localizing the support- objects improves the accuracy of models. GroundNet has
ing objects when its modules are guided by syntax. a dynamic computation graph and models the relationship
“LSTM+CNN-MIL” and CMN does not exploit the syntax betweenobjects.
of the referring expression and poorly performs in localiz-
ing supporting objects. When we relax the syntactic guid-
anceofGroundNetbylettingallmodulestoattendtoallof Modular Neural Architectures. Neural Module Net-
the referring expression, “GroundNet-free-form” also per- works (NMN) (Andreas et al. 2016b; 2016a) is a general
formspoorlyonlocalizingsupportingobjects.Theseresults frameworkformodelingcompositionalityoflanguageusing
suggestthatleveragingsyntaxisessentialinlocalizingsup- neural modules. A computation graph with neural modules
porting objects and there might be a tradeoff between be- asnodesisgeneratedbasedonaparsetreeoftheinputtext.
6761
(cid:53)(cid:72)(cid:73)(cid:72)(cid:85)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:40)(cid:91)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81) (cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:49)(cid:72)(cid:87) (cid:38)(cid:48)(cid:49)
(cid:90)(cid:75)(cid:76)(cid:87)(cid:72)(cid:3)(cid:70)(cid:82)(cid:79)(cid:82)(cid:85)(cid:3)(cid:70)(cid:68)(cid:85)
(cid:69)(cid:72)(cid:75)(cid:76)(cid:81)(cid:71)
(cid:86)(cid:88)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87) (cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)
“awhitecolorcarbehindagirlcatchinga
disc”
(cid:74)(cid:76)(cid:85)(cid:79) (cid:70)(cid:68)(cid:87)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)
(cid:71)(cid:76)(cid:86)(cid:70)
(cid:86)(cid:88)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87) (cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)
(cid:80)(cid:68)(cid:81) (cid:90)(cid:68)(cid:79)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:69)(cid:72)(cid:75)(cid:76)(cid:81)(cid:71)
“themanwalkingbehindthebench”
(cid:69)(cid:72)(cid:81)(cid:70)(cid:75)
(cid:80)(cid:68)(cid:81)
(cid:74)(cid:82)(cid:76)(cid:81)(cid:74)(cid:3)(cid:69)(cid:72)(cid:73)(cid:82)(cid:85)(cid:72)
(cid:86)(cid:88)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87) (cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)
“amangoingbeforealadycarryingacell-
phone”
(cid:79)(cid:68)(cid:71)(cid:92) (cid:70)(cid:68)(cid:85)(cid:85)(cid:92)(cid:76)(cid:81)(cid:74)
(cid:70)(cid:72)(cid:79)(cid:79)(cid:3)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)
Figure3:QualitativeResultsforGroundNet.Boundingboxesandreferringexpressionstotargetobject(ingreenboxes)onthe
left.GroundNetpredictionsinthemiddleandCMNpredictionsareontheright.GroundNetlocalizesnotonlythetargetobject
butalsosupportingobjects(e.g.discandgirlinthefirstrow,benchinthesecond).Bestseenincolor.
6762
GroundNetsharestheprinciplesofthisframework.Wede- artresultsinlocalizingsupportingobjects.Ourresultsshow
signGroundNetforreferringexpressiontaskrestrictingeach thatrecentmodelsareunsuccessfulatlocalizingsupporting
nodegroundedintheinputimagewhichkeepsnetworkin- objects.Thissuggeststhatcurrentsolutionstoreferringex-
terpretablethroughoutthecomputation. pression task come with an interpretability-accuracy trade-
Compositional Modular Networks (CMN) (Hu et al. off. Our approach substantially improves supporting object
2017) is also an instant of NMN aiming to remove lan- localization, while maintaining high accuracy, thus repre-
guage parser from the generation of computation graph by senting a new and more desirable point along the trade-off
inducing text representations to localization and relation- trajectory.
ship modules using an attention mechanism. Their com- We believe future work might extend our work with fol-
putation graph is fixed to the subject-relation-subject tu- lowing insights. First, while generating the computation
ple but the input is dynamically constructed for modules. graph GroundNet, we drop the determiners. However, the
Our model, on the other hand, can handle multiple rela- indefinitenessofanouncouldbehelpfulinlocalizinganob-
tionships mentioned in referring expressions (see the first ject. Second, GroundNet processes the computation graph
row of Figure 3). We should note that CMN is a special in a bottom-up fashion. An approach combining the se-
case of GroundNet where the syntax is fixed to a triplet of quential processing of the referring expression with the
Locate subject,Relate,Locate obj and each node composes bottom-upstructuralprocessingofGroundNetcouldmodel
atextrepresentationwithwholereferringexpression. expectation-driven effects of language which may result in
moreaccurategroundingthroughoutthecomputationgraph.
Syntax with Vision. Similar to our work, Gorniak and
Acknowledgments
Roy (2004) study a syntax-based approach for grounding
referring expressions. However, they use a synthetic visual
This project was partially supported by Oculus and Yahoo
sceneofidenticalshapeswithvaryingcolorsandasynthetic
InMind research grants. The authors would like to thank
grammarforlanguage.Golland,Liang,andKlein(2010)in-
anonymous reviewers and the members of MultiComp Lab
troduceagame-theoreticmodelsuccessfullyleveragessyn-
atCMUfortheirvaluablefeedback.
taxforgroundingreferenceexpressionsforsyntheticscenes.
(Matuszek* et al. 2012) presents a semantic parsing model
References
withCombinatory CategoryGrammar forreferring expres-
sionrecognitionthatjointlylearnsgroundingofobjectsand Andreas,J.;Rohrbach,M.;Darrell,T.;andKlein,D. 2016a.
their attributes. The model is able to induce latent logical Learning to compose neural networks for question answer-
forms when bootstrapped with a supervised training stage. ing. In Proceedings of the 2016 Conference of the North
Berzaketal.(2015)usevisualcontexttoaddresslinguistic American Chapter of the Association for Computational
ambiguities. Similarly, Christie et al. (2016) use the visual Linguistics: Human Language Technologies, 1545–1554.
context for solving prepositional phrase attachment resolu- San Diego, California: Association for Computational Lin-
tion (PPAR) for sentences describing a scene. Unlike our guistics.
model, their model relies on multiple parse trees and mul-
Andreas,J.;Rohrbach,M.;Darrell,T.;andKlein,D. 2016b.
tiple segmentations of an image coming from a black-box
Neural module networks. In Proceedings of the IEEE
image segmenter. Our model can also be extended to ad-
Conference on Computer Vision and Pattern Recognition
dressPPARsettingwhereweonlyneedtoground-truthob-
(CVPR),39–48.
ject annotations for roots of multiple parse trees for the in-
Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural ma-
put sentences. Wang et al. (2016) introduce a model local-
chine translation by jointly learning to align and translate.
izingphrasesinsentencesthatdescribeanimage.However,
arXivpreprintarXiv:1409.0473.
their model relies on the annotation of phrase-object pairs.
GroundNet only uses target object annotations and there Berzak,Y.;Barbu,A.;Harari,D.;Katz,B.;andUllman,S.
is no supervision for supporting objects. Xiao, Sigal, and 2015. Doyouseewhatimean?visualresolutionoflinguis-
Lee(2017)aimtoaddresslocalizationofphrasesonregion tic ambiguities. In Proceedings of the 2015 Conference on
masks.Similartoourapproach,theydonotrelyonground- EmpiricalMethodsinNaturalLanguageProcessing,1477–
truth masks during training. However, unlike GroundNet, 1487. AssociationforComputationalLinguistics.
theirmodeldoesnotmodelrelationshipbetweenobjects.
Chai,J.Y.;Hong,P.;andZhou,M.X. 2004. Aprobabilistic
approach to reference resolution in multimodal user inter-
6 Conclusion faces. InProceedingsofthe9thinternationalconferenceon
Intelligentuserinterfaces,70–77. ACM.
Inthiswork,wepresentGroundNet,acompositionalneural
modulenetworkdesignedforthetaskofgroundingreferring Christie,G.;Laddha,A.;Agrawal,A.;Antol,S.;Goyal,Y.;
expressions.Wealsointroduceanovelauxiliarytaskandan Kochersberger, K.; and Batra, D. 2016. Resolving lan-
annotationforlocalizingthesupportingobjects. guageandvisionambiguitiestogether:Jointsegmentation&
Our experiments on a standard benchmark show that prepositionalattachmentresolutionincaptionedscenes. In
GroundNet is the first model that successfully incorporates Proceedingsofthe2016ConferenceonEmpiricalMethods
syntacticinformationforthereferringexpressiontask.This in Natural Language Processing, 1493–1503. Association
syntacticinformationhelpsGroundNetachievestate-of-the- forComputationalLinguistics.
6763
Fang,R.;Liu,C.;andChai,J.Y. 2012. Integratingwordac- Ozdowska,S. 2008. Cross-corpusevaluationofwordalign-
quisition and referential grounding towards physical world ment. InLREC.
interaction. In Proceedings of the 14th ACM international Pennington,J.;Socher,R.;andManning,C.D.2014.Glove:
conferenceonMultimodalinteraction,109–116. ACM. Global vectors for word representation. In EMNLP, vol-
Fukui,A.;Park,D.H.;Yang,D.;Rohrbach,A.;Darrell,T.; ume14,1532–1543.
andRohrbach,M.2016.Multimodalcompactbilinearpool- Ren,S.;He,K.;Girshick,R.;andSun,J.2015.Fasterr-cnn:
ingforvisualquestionansweringandvisualgrounding. In Towardsreal-timeobjectdetectionwithregionproposalnet-
Proceedingsofthe2016ConferenceonEmpiricalMethods works. In Advances in neural information processing sys-
in Natural Language Processing, 457–468. Austin, Texas: tems,91–99.
AssociationforComputationalLinguistics.
Rohrbach, A.; Rohrbach, M.; Hu, R.; Darrell, T.; and
Glorot, X., and Bengio, Y. 2010. Understanding the diffi-
Schiele, B. 2016. Grounding of textual phrases in images
cultyoftrainingdeepfeedforwardneuralnetworks. InAis-
by reconstruction. In European Conference on Computer
tats,volume9,249–256.
Vision,817–834. Springer.
Golland,D.;Liang,P.;andKlein,D.2010.Agame-theoretic
Schuster,M.,andPaliwal,K.K. 1997. Bidirectionalrecur-
approachtogeneratingspatialdescriptions. InProceedings
rentneuralnetworks. IEEETransactionsonSignalProcess-
ofthe2010conferenceonempiricalmethodsinnaturallan-
ing45(11):2673–2681.
guageprocessing,410–419. AssociationforComputational
Socher,R.;Karpathy,A.;Le,Q.V.;Manning,C.D.;andNg,
Linguistics.
A.Y. 2014. Groundedcompositionalsemanticsforfinding
Gorniak, P., and Roy, D. 2004. Grounded semantic com-
and describing images with sentences. Transactions of the
positionforvisualscenes. JournalofArtificialIntelligence
AssociationforComputationalLinguistics2:207–218.
Research21:429–470.
Tellex,S.;Kollar,T.;Dickerson,S.;Walter,M.R.;Banerjee,
Graca, J.; Pardal, J. P.; Coheur, L.; and Caseiro, D. 2008.
A.G.;Teller,S.;andRoy,N.2011.Approachingthesymbol
Buildingagoldencollectionofparallelmulti-languageword
groundingproblemwithprobabilisticgraphicalmodels. AI
alignment. InLREC.
magazine32(4):64–76.
Grice,H.P. 1975. Logicandconversation. 197541–58.
Wang,M.;Azab,M.;Kojima,N.;Mihalcea,R.;andDeng,
Hu, R.; Xu, H.; Rohrbach, M.; Feng, J.; Saenko, K.; and J. 2016. Structuredmatchingforphraselocalization. InEu-
Darrell,T. 2016. Naturallanguageobjectretrieval. InPro- ropeanConferenceonComputerVision,696–711. Springer.
ceedings of the IEEE Conference on Computer Vision and
Williams, T.; Acharya, S.; Schreitter, S.; and Scheutz, M.
PatternRecognition,4555–4564.
2016. Situatedopenworldreferenceresolutionforhuman-
Hu,R.;Rohrbach,M.;Andreas,J.;Darrell,T.;andSaenko, robot dialogue. In The Eleventh ACM/IEEE International
K. 2017. Modelingrelationshipsinreferentialexpressions Conference on Human Robot Interaction, 311–318. IEEE
withcompositionalmodularnetworks. Press.
Lemaignan, S.; Ros, R.; Alami, R.; and Beetz, M. 2011. Xiao,F.;Sigal,L.;andLee,Y.J. 2017. Weakly-supervised
What are you talking about? grounding dialogue in a visualgroundingofphraseswithlinguisticstructures.
perspective-aware robotic architecture. In RO-MAN, 2011
Yu, L.; Poirson, P.; Yang, S.; Berg, A. C.; and Berg, T. L.
IEEE,107–112. IEEE.
2016. Modeling context in referring expressions. In Euro-
Lin,T.-Y.;Maire,M.;Belongie,S.;Hays,J.;Perona,P.;Ra- peanConferenceonComputerVision,69–85. Springer.
manan, D.; Dolla´r, P.; and Zitnick, C. L. 2014. Microsoft
Zender, H.; Kruijff, G.-J. M.; and Kruijff-Korbayova´, I.
coco:Commonobjectsincontext. InEuropeanConference
2009. Situatedresolutionandgenerationofspatialreferring
onComputerVision,740–755. Springer.
expressionsforroboticassistants. InIJCAI,1604–1609.
Manning, C. D.; Surdeanu, M.; Bauer, J.; Finkel, J. R.;
Bethard,S.;andMcClosky,D. 2014. Thestanfordcorenlp
naturallanguageprocessingtoolkit.InACL(SystemDemon-
strations),55–60.
Mao,J.;Huang,J.;Toshev,A.;Camburu,O.;Yuille,A.L.;
and Murphy, K. 2016. Generation and comprehension of
unambiguous object descriptions. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition(CVPR),11–20.
Matuszek*,C.;FitzGerald*,N.;Zettlemoyer,L.;Bo,L.;and
Fox,D. 2012. AJointModelofLanguageandPerception
forGroundedAttributeLearning.InProc.ofthe2012Inter-
nationalConferenceonMachineLearning.
Nagaraja, V.; Morariu, V.; and Davis, L. 2016. Modeling
contextbetweenobjectsforreferringexpressionunderstand-
ing. InECCV.
6764

Simultaneously Self-Attending to All Mentions for
Full-Abstract Biological Relation Extraction
Patrick Verga, Emma Strubell, Andrew McCallum
College of Information and Computer Sciences
University of Massachusetts Amherst
{pat, strubell, mccallum}@cs.umass.edu
Abstract areexpressedacrosssentenceboundaries,suchasin
the following excerpt expressing a relationship be-
Most work in relation extraction forms a tween the chemical azathioprine and the disease
prediction by looking at a short span of fibrosis:
text within a single sentence containing a
single entity pair mention. This approach Treatment of psoriasis with azathioprine.
Azathioprine treatment benefited 19 (66%)
often does not consider interactions across
out of 29 patients suffering from severe pso-
mentions, requires redundant computation
riasis. Haematological complications were
for each mention pair, and ignores rela- not troublesome and results of biochemical
tionships expressed across sentence bound- liver function tests remained normal. Min-
aries. These problems are exacerbated by imal cholestasis was seen in two cases and
portalfibrosisofareversibledegreeineight.
thedocument-(ratherthansentence-)level
Liverbiopsiesshouldbeundertakenatregular
annotation common in biological text. In
intervals if azathioprine therapy is contin-
response, we propose a model which simul- ued so that structural liver damage may be
taneouslypredictsrelationshipsbetweenall detected at an early and reversible stage.
mentionpairsinadocument. Weformpair-
Though the entities’ mentions never occur in the
wise predictions over entire paper abstracts
same sentence, the above example expresses that
usinganefficientself-attentionencoder. All-
the chemical entity azathioprine can cause the side
pairs mention scores allow us to perform
effect fibrosis. Relation extraction models which
multi-instance learning by aggregating over
consider only within-sentence relation pairs can-
mentions to form entity pair representa-
not extract this fact without knowledge of the
tions. Wefurtheradapttosettingswithout
complicated coreference relationship between eight
mention-levelannotationbyjointlytraining
andazathioprinetreatment,which,withoutfeatures
to predict named entities and adding a cor-
from a complicated pre-processing pipeline, cannot
pus of weakly labeled data. In experiments
be learned by a model which considers entity pairs
on two Biocreative benchmark datasets, we
in isolation. Making separate predictions for each
achieve state of the art performance on the
mention pair also obstructs multi-instance learning
Biocreative V Chemical Disease Relation
(Riedel et al., 2010; Surdeanu et al., 2012), a tech-
dataset for models without external KB re-
nique which aggregates entity representations from
sources. We also introduce a new dataset
mentions in order to improve robustness to noise in
an order of magnitude larger than existing
the data. Like the majority of relation extraction
human-annotatedbiologicalinformationex-
data, most annotation for biological relations is dis-
traction datasets and more accurate than
tantly supervised, and so we could benefit from a
distantly supervised alternatives.
modelwhichisamenabletomulti-instancelearning.
In addition to this loss of cross-sentence and
1 Introduction
cross-mentionreasoningcapability,traditionalmen-
With few exceptions (Swampillai and Stevenson, tion pair relation extraction models typically intro-
2011; Quirk and Poon, 2017; Peng et al., 2017), duce computational inefficiencies by independently
nearlyallworkinrelationextractionfocusesonclas- extracting features for and scoring every pair of
sifying a short span of text within a single sentence mentions, even when those mentions occur in the
containing a single entity pair mention. However, samesentenceandthuscouldsharerepresentations.
relationships between entities are often expressed In the CDR training set, this requires separately
across sentence boundaries or otherwise require a encoding and classifying each of the 5,318 candi-
larger context to disambiguate. For example, 30% date mention pairs independently, versus encoding
ofrelationsintheBiocreativeVCDRdataset(§3.1) each of the 500 abstracts once. Though abstracts
872
ProceedingsofNAACL-HLT2018,pages872–884
NewOrleans,Louisiana,June1-6,2018.(cid:13)c2018AssociationforComputationalLinguistics
are longer than e.g. the text between mentions, extraction systems.1
many sentences contain multiple mentions, leading
to redundant computation. 2 Model
However,encodinglongsequencesinawaywhich
We designed our model to efficiently encode long
effectivelyincorporateslong-distancecontextcanbe
contexts spanning multiple sentences while forming
prohibitively expensive. Long Short Term Memory
pairwise predictions without the need for mention
(LSTM) networks (Hochreiter and Schmidhuber,
pair-specificfeatures. Todothis,ourmodelfirsten-
1997) are among the most popular token encoders
codes input token embeddings using self-attention.
due to their capacity to learn high-quality repre-
These embeddings are used to predict both entities
sentations of text, but their ability to leverage the
and relations. The relation extraction module con-
fastestcomputinghardwareisthwartedduetotheir
verts each token to a head and tail representation.
computational dependence on the length of the se-
These representations are used to form mention
quence — each token’s representation requires as
pair predictions using a bi-affine operation with re-
input the representation of the previous token, lim-
specttolearnedrelationembeddings. Finally,these
iting the extent to which computation can be par-
mention pair predictions are pooled to form entity
allelized. Convolutional neural networks (CNNs),
pair predictions, expressing whether each relation
in contrast, can be executed entirely in parallel
type is expressed by each relation pair.
across the sequence, but the amount of context
incorporated into a single token’s representation 2.1 Inputs
is limited by the depth of the network, and very
Our model takes in a sequence of N token em-
deep networks can be difficult to learn (Hochreiter,
1998). These problems are exacerbated by longer
beddings in Rd. Because the Transformer has no
innate notion of token position, the model relies
sequences, limiting the extent to which previous
on positional embeddings which are added to the
work explored full-abstract relation extraction.
input token embeddings.2 We learn the position
To facilitate efficient full-abstract relation ex- embedding matrix Pm ×d which contains a sepa-
traction from biological text, we propose Bi-affine rate d dimensional embedding for each position,
Relation Attention Networks (BRANs), a combi- limited to m possible positions. Our final input
nation of network architecture, multi-instance and representation for token x is:
i
multi-tasklearningdesignedtoextractrelationsbe-
tween entities in biological text without requiring x =s +p
i i i
explicit mention-level annotation. We synthesize
convolutions and self-attention, a modification of where s is the token embedding for x and p is
i i i
the Transformer encoder introduced by Vaswani the positional embedding for the ith position. If i
et al. (2017), over sub-word tokens to efficiently exceeds m, we use a randomly initialized vector in
incorporate into token representations rich context place of p .
i
between distant mention pairs across the entire ab- We tokenize the text using byte pair encoding
stract. We score all pairs of mentions in parallel (BPE) (Gage, 1994; Sennrich et al., 2015). The
using a bi-affine operator, and aggregate over men- BPEalgorithmconstructsavocabularyofsub-word
tion pairs using a soft approximation of the max pieces, beginning with single characters. Then, the
functioninordertoperformmulti-instancelearning. algorithm iteratively merges the most frequent co-
We jointly train the model to predict relations and occurring tokens into a new token, which is added
entities, further improving robustness to noise and to the vocabulary. This procedure continues until
lack of gold annotation at the mention level. a pre-defined vocabulary size is met.
BPE is well suited for biological data for the fol-
In extensive experiments on two benchmark bio-
lowing reasons. First, biological entities often have
logicalrelationextractiondatasets,weachievestate
unique mentions made up of meaningful subcompo-
of the art performance for a model using no exter-
nents, such as 1,2-dimethylhydrazine. Additionally,
nalknowledgebaseresourcesinexperimentsonthe
tokenizationofchemicalentitiesischallenging,lack-
Biocreative V CDR dataset, and outperform com-
ingauniversallyagreeduponalgorithm(Krallinger
parable baselines on the Biocreative VI ChemProt
et al., 2015). As we demonstrate in §3.3.2, the sub-
dataset. We also introduce a new dataset which
word representations produced by BPE allow the
is an order of magnitude larger than existing gold-
model to formulate better predictions, likely due to
annotated biological relation extraction datasets
better modeling of rare and unknown words.
while covering a wider range of entity and relation
types and with higher accuracy than distantly su-
1Ourcodeanddataarepubliclyavailableat: https:
pervised datasets of the same size. We provide a
//github.com/patverga/bran.
strong baseline on this new dataset, and encourage 2Thoughourfinalmodelincorporatessomeconvolu-
itsuseasabenchmarkforfuturebiologicalrelation tions, we retain the position embeddings.
873
Entity-Pair Transformer is made up of B blocks. Each Trans-
Relation Scores former block, which we denote Transformer , has
k
its own set of parameters and is made up of two
subcomponents: multi-head attention and a series
LogSumExp
of convolutions3. The output for token i of block k,
b(k)
, is connected to its input
b(k −1)
with a resid-
Hypersensitivity i i
to ual connection (He et al., 2016). Starting with
carbamazepine 4.3
presenting b(0) =x :
with i i
leukemoid
eosir ne oa pc ht ai io l nian d,, b( ik) =b i(k −1)+Transformer k(b( ik −1))
erythroderma.
The 2.2.1 Multi-head Attention
First
report Multi-head attention applies self-attention multiple
of
such times over the same inputs using separately nor-
reaction
carbamazepineto . 0.1 Relations m tha eli rz ee sd ulp ta sr ,a am se at ner as l( ta et rt ne an tt ii vo en th oea ad ps p) lya in nd gc oo nm eb pin ae sss
of attention with more parameters. The intuition
behind this modeling decision is that dividing the
attention into multiple heads make it easier for the
model to learn to attend to different types of rele-
Head MLP Tail MLP vantinformationwitheachhead. Theself-attention
updatesinputb(k −1)
byperformingaweightedsum
i
over all tokens in the sequence, weighted by their
importance for modeling token i.
Each input is projected to a key k, value v, and
query q, using separate affine transformations with
Convolutions
ReLU activations (Glorot et al., 2011). Here, k,
M Au ttl eti n- th ie oa nd Bx v, and q are each in RHd where H is the number
of heads. The attention weights a for head h
ijh
between tokens i and j are computed using scaled
dot-product attention:
H y perse nsit to
i
vit
ycar ba m ap zr ee ps ie nn etiw n git hle u ke m oi dreacti o ne ,osi n o p hia lin ad ,er yt hr o der mT h ae .Firstre p ortofs uc hreacti ot o ncar ba m aze
pi
ne
a oij ihh = =σ (cid:18) vq i jT √h hk d (cid:12)jh a(cid:19)
ijh
Figure 1: The relation extraction architecture. j
X
InputsarecontextuallyencodedusingtheTrans-
former(Vaswani et al., 2017), made up of B
layers of multi-head attention and convolution with denoting element-wise multiplication and σ
(cid:12)
subcomponents. Eachtransformedtokenisthen indicating a softmax along the jth dimension. The
passedthroughahead andtail MLPtoproduce scaled attention is meant to aid optimization by
twoposition-specificrepresentations. Abi-affine flattening the softmax and better distributing the
operation is performed between each head and gradients (Vaswani et al., 2017).
tail representation with respect to each rela- The outputs of the individual attention heads
tion’s embedding matrix, producing a pair-wise are concatenated, denoted [; ], into o i. All layers
· ·
relation affinity tensor. Finally, the scores for in the network use residual connections between
cells corresponding to the same entity pair are the output of the multi-headed attention and its in-
pooled with a separate LogSumExp operation put. Layernormalization(Baetal.,2016), denoted
foreachrelationtogetafinalscore. Thecolored LN(), is then applied to the output.
·
tokensillustratecalculatingthescoreforagiven
o =[o ;...;o ]
i 1 h
pair of entities; the model is only given entity
information when pooling over mentions. m i =LN(b( ik −1)+o i)
2.2.2 Convolutions
ThesecondpartofourTransformerblockisastack
of convolutional layers. The sub-network used in
2.2 Transformer
3The original Transformer uses feed-forward con-
We base our token encoder on the Transformer
nections, i.e. width-1 convolutions, whereas we use
self-attention model (Vaswani et al., 2017). The convolutions with width > 1.
874
remrofsnarT
Vaswani et al. (2017) uses two width-1 convolu- network attention (Verga and McCallum, 2016; Lin
tions. We add a third middle layer with kernel et al., 2016; Yaghoobzadeh et al., 2017).
width 5, which we found to perform better. Many We aggregate over all representations for each
relations are expressed concisely by the immediate mention pair in order to produce per-relation
local context, e.g. Michele’s husband Barack, or scores for each entity pair. For each entity pair
labetalol-induced hypotension. Adding this explicit (phead,ptail), let Phead denote the set of indices of
n-gram modeling is meant to ease the burden on mentions of the entity phead, and let Ptail denote
the model to learn to attend to local features. We the indices of mentions of the entity ptail. Then
use C w() to denote a convolutional operator with we use the LogSumExp function to aggregate the
·
kernel width w. Then the convolutional portion of relation scores from A across all pairs of mentions
the transformer block is given by: of phead and ptail:
t( i0) =ReLU(C 1(m i)) scores(phead,ptail)=log exp(A ij)
t( i1) =ReLU(C 5(t( i0))) i j∈XP Ph te aa id
l
t(2) =C (t(1)) ∈
i 1 i
The LogSumExp scoring function is a smooth ap-
Where the dimensions of t(0) and t(1) are in R4d proximation to the max function and has the bene-
i i fitsofaggregatinginformationfrommultiplepredic-
and that of t(2) is in Rd.
i tions and propagating dense gradients as opposed
2.3 Bi-affine Pairwise Scores to the sparse gradient updates of the max (Das
We project each contextually encoded token
b(B) et al., 2017).
i
through two separate MLPs to generate two new 2.5 Named Entity Recognition
versions of each token corresponding to whether
In addition to pairwise relation predictions, we
it will serve as the first (head) or second (tail)
use the Transformer output
b(B)
to make entity
argument of a relation: i
type predictions. We feed
b(B)
as input to a linear
i
ehead =W(1) (ReLU(W(0) b(B))) classifier which predicts the entity label for each
i head head i
token with per-class scores c :
etail =W(1)(ReLU(W(0)b(B))) i
i tail tail i
c =W(3)b(B)
Weuseabi-affineoperatortocalculateanN L N i i
× ×
tensor A of pairwise affinity scores, scoring each
We augment the entity type labels with the BIO
(head, relation, tail) triple:
encoding to denote entity spans. We apply tags
A =(eheadL)etail to the byte-pair tokenization by treating each sub-
ilj i j
word within a mention span as an additional token
where L is a d L d tensor, a learned embedding with a corresponding B- or I- label.
× ×
matrix for each of the L relations. In subsequent
2.6 Training
sections we will assume we have transposed the
dimensions of A as d d L for ease of indexing. WetrainboththeNERandrelationextractioncom-
× ×
ponents of our network to perform multi-class clas-
2.4 Entity Level Prediction
sification using maximum likelihood, where NER
Our data is weakly labeled in that there are labels classes y or relation classes r are conditionally
i i
attheentitylevelbutnotthementionlevel,making independent given deep features produced by our
the problem a form of strong-distant supervision model with probabilities given by the softmax func-
(Mintz et al., 2009). In distant supervision, edges tion. In the case of NER, features are given by the
in a knowledge graph are heuristically applied to per-token output of the transformer:
sentences in an auxiliary unstructured text corpus
— often applying the edge label to all sentences 1 N
logP(y b(B))
containing the subject and object of the relation. N i | i
Because this process is imprecise and introduces Xi=1
noise into the training data, methods like multi-
In the case of relation extraction, the features for
instance learning were introduced (Riedel et al.,
each entity pair are given by the LogSumExp over
2010; Surdeanu et al., 2012). In multi-instance pairwise scores described in § 2.4. For E entity
learning, rather than looking at each distantly la- pairs, the relation r is given by:
i
beledmentionpairinisolation,themodelistrained
over the aggregate of these mentions and a single E
1
update is made. More recently, the weighting func- logP(r i scores(phead,ptail))
E |
tion of the instances has been expressed as neural i=1
X
875
We train the NER and relation objectives jointly, and filter hypernyms according to the hierarchy
sharing all embeddings and Transformer parame- in the MESH controlled vocabulary5. All entity
ters. To trade off the two objectives, we penalize pairs within the same abstract that do not have an
the named entity updates with a hyperparameter annotated relation are assigned the NULL label.
λ. In addition to the gold CDR data, Peng et al.
(2016) add 15,448 PubMed abstracts annotated in
3 Results
the CTD dataset. We consider this same set of
abstracts as additional training data (which we
We evaluate our model on three datasets: The
subsequently denote +Data). Since this data does
Biocreative V Chemical Disease Relation bench-
not contain entity annotations, we take the anno-
mark (CDR), which models relations between
tations from Pubtator (Wei et al., 2013), a state
chemicals and diseases (§3.1); the Biocreative VI
of the art biological named entity tagger and en-
ChemProt benchmark (CPR), which models rela-
tity linker. See §A.1 for additional data processing
tions between chemicals and proteins (§3.2); and a
details. In our experiments we only evaluate our
new, large and accurate dataset we describe in §3.3
relation extraction performance and all models (in-
based on the human curation in the Chemical Toxi-
cluding baselines) use gold entity annotations for
cologyDatabase(CTD),whichmodelsrelationships
predictions.
between chemicals, proteins and genes.
The byte pair vocabulary is generated over the
The CDR dataset is annotated at the level of
training dataset — we use a budget of 2500 tokens
paper abstracts, requiring consideration of long-
when training on the gold CDR data, and a larger
range, cross sentence relationships, thus evaluation
budget of 10,000 tokens when including extra data
on this dataset demonstrates that our model is
described above Additional implementation details
capable of such reasoning. We also evaluate our
are included in Appendix A.
model’sperformanceinthemoretraditionalsetting
which does not require cross-sentence modeling by
Data split Docs Pos Neg
performing experiments on the CPR dataset, for
Train 500 1,038 4,280
which all annotations are between two entity men-
Development 500 1,012 4,136
tions in a single sentence. Finally, we present a
Test 500 1,066 4,270
new dataset constructed using strong-distant su-
CTD 15,448 26,657 146,057
pervision (§2.4), with annotations at the document
level. This dataset is significantly larger than the
Table 1: Data statistics for the CDR Dataset and
others, contains more relation types, and requires
additional data from CTD. Shows the total num-
reasoning across sentences.
ber of abstracts, positive examples, and negative
3.1 Chemical Disease Relations Dataset examples for each of the data set splits.
The Biocreative V chemical disease relation extrac-
tion (CDR) dataset4 (Li et al., 2016a; Wei et al.,
3.1.2 Baselines
2016) was derived from the Comparative Toxicoge-
We compare against the previous best reported
nomicsDatabase(CTD),whichcuratesinteractions
results on this dataset not using knowledge base
betweengenes,chemicals,anddiseases(Davisetal.,
features.6 Each of the baselines are ensemble meth-
2008). CTD annotations are only at the document
ods for within- and cross-sentence relations that
level and do not contain mention annotations. The
makeuseofadditionallinguisticfeatures(syntactic
CDR dataset is a subset of these original annota-
parse and part-of-speech). Gu et al. (2017) en-
tions, supplemented with human annotated, entity
code mention pairs using a CNN while Zhou et al.
linked mention annotations. The relation annota-
(2016a) use an LSTM. Both make cross-sentence
tions in this dataset are also at the document level
predictions with featurized classifiers.
only.
3.1.1 Data Preprocessing 3.1.3 Results
The CDR dataset is concerned with extracting In Table 2 we show results outperforming the base-
onlychemically-induceddiseaserelationships(drug- lines despite using no linguistic features. We show
related side effects and adverse reactions) concern- performance averaged over 20 runs with 20 random
ing the most specific entity in the document. For seeds as well as an ensemble of their averaged pre-
example tobacco causes cancer could be marked as dictions. We see a further boost in performance
false if the document contained the more specific by adding weakly labeled data. Table 3 shows the
lung cancer. This can cause true relations to be
5https://www.nlm.nih.gov/mesh/download/
labeled as false, harming evaluation performance.
2017MeshTree.txt
To address this we follow (Gu et al., 2016, 2017) 6The highest reported score is from (Peng et al.,
2016), but they use explicit lookups into the CTD
4http://www.biocreative.org/ knowledgebasefortheexistenceofthetestentitypair.
876
Model P R F1 Model P R F1
Gu et al. (2016) 62.0 55.1 58.3 CNN 50.7 43.0 46.5
†
Zhou et al. (2016a) 55.6 68.4 61.3 GRU+Attention 53.0 46.3 49.5
†
Gu et al. (2017) 55.7 68.1 61.3 BRAN 48.0 54.1 50.8 .01
±
BRAN 55.6 70.8 62.1 0.8
±
+ Data 64.0 69.2 66.2 0.8 Table 4: Precision, recall, and F1 results on the
±
BRAN(ensemble) 63.3 67.1 65.1 Biocreative VI Chem-Prot Dataset. denotes re-
†
+ Data 65.4 71.8 68.4 sults from Liu et al. (2017)
Table 2: Precision, recall, and F1 results on the
Biocreative V CDR Dataset. unsupervised pre-training.
3.2.2 Results
Model P R F1
In Table 4 we see that even though our model
BRAN (Full) 55.6 70.8 62.1 0.8
± forms all predictions simultaneously between all
– CNN only 43.9 65.5 52.4 1.3
± pairs of entities within the sentence, we are able
– no width-5 48.2 67.2 55.7 0.9
± to outperform state of the art models classifying
– no NER 49.9 63.8 55.5 1.8
± eachmentionpairindependently. Thescoresshown
are averaged across 10 runs with 10 random seeds.
Table3: ResultsontheBiocreativeVCDRDataset
Interestingly, our model appears to have higher
showing precision, recall, and F1 for various model
recallandlowerprecision,whilethebaselinemodels
ablations.
are both precision-biased, with lower recall. This
suggeststhatcombiningthesestylesofmodelcould
lead to further gains on this task.
effects of ablating pieces of our model. ‘CNN only’
removes the multi-head attention component from
the transformer block, ‘no width-5’ replaces the
3.3 New CTD Dataset
width-5 convolution of the feed-forward component
3.3.1 Data
of the transformer with a width-1 convolution and
‘no NER’ removes the named entity recognition Existing biological relation extraction datasets in-
multi-task objective (§2.5). cluding both CDR (§3.1) and CPR (§3.2) are rela-
tively small, typically consisting of hundreds or a
3.2 Chemical Protein Relations Dataset fewthousandannotatedexamples. Distantsupervi-
Toassessourmodel’sperformanceinsettingswhere sion datasets apply document-independent, entity-
cross-sentence relationships are not explicitly evalu- level annotations to all sentences leading to a large
ated,weperformexperimentsontheBiocreativeVI proportion of incorrect labels. Evaluations on this
ChemProt dataset (CDR) (Krallinger et al., 2017). data involve either very small (a few hundred) gold
This dataset is concerned with classifying into six annotated examples or cross validation to predict
relationtypesbetweenchemicalsandproteins,with the noisy, distantly applied labels (Mallory et al.,
nearly all annotated relationships occurring within 2015; Quirk and Poon, 2017; Peng et al., 2017).
the same sentence. We address these issues by constructing a new
dataset using strong-distant supervision containing
3.2.1 Baselines document-levelannotations. TheComparativeTox-
Wecompareourmodelsagainstthosecompetingin icogenomics Database (CTD) curates interactions
the official Biocreative VI competition (Liu et al., between genes, chemicals, and diseases. Each rela-
2017). We compare to the top performing team tionintheCTDisassociatedwithadisambiguated
whosemodelisdirectlycomparablewithours—i.e. entitypairandaPubMedarticlewheretherelation
usedasingle(non-ensemble)modeltrainedonlyon was observed.
thetrainingdata(manyteamsusethedevelopment Toconstructthisdataset,wecollecttheabstracts
set as additional training data). The baseline mod- for each of the PubMed articles with at least one
els are standard state of the art relation extraction curated relation in the CTD database. As in §3.1,
models: CNNs and Gated RNNs with attention. we use PubTator to automatically tag and disam-
Each of these baselines uses mention-specific fea- biguate the entities in each of these abstracts. If
tures encoding relative position of each token to both entities in the relation are found in the ab-
the two target entities being classified, whereas our stract, we take the (abstract, relation) pair as a
modelaggregatesoverallmentionpairsineachsen- positive example. The evidence for the curated re-
tence. It is also worth noting that these models use lation could occur anywhere in the full text article,
a large vocabulary of pre-trained word embeddings, not just the abstract. Abstracts with no recovered
givingtheirmodelstheadvantageoffarmoremodel relations are discarded. All other entity pairs with
parameters, as well as additional information from valid types and without an annotated relation that
877
Types Docs Pos Neg Train Dev Test
Total 68,400 166,474 1198,493 Total 120k 15k 15k
Chemical/Disease 64,139 93,940 571,932 Chemical/Disease
Chemical/Gene 34,883 63,463 360,100 marker/mechanism 41,562 5,126 5,167
Gene/Disease 32,286 9,071 266,461 therapeutic 24,151 2,929 3,059
Gene/Disease
Table 5: Data statistics for the new CTD dataset.
marker/mechanism 5,930 825 819
therapeutic 560 77 75
Chemical/Gene
occurintheremainingabstractsareconsideredneg-
increase_expression 15,851 1,958 2,137
ative examples and assigned the NULL label. We
increase_MP 5,986 740 638
additionally remove abstracts containing greater
decrease_expression 5,870 698 783
than500tokens7. Thislimitremovedabout10%of
increase_activity 4,154 467 497
the total data including numerous extremely long
affects_response 3,834 475 508
abstracts. The average token length of the remain-
decrease_activity 3,124 396 434
ing data was ˜230 tokens. With this procedure, we
affects_transport 3,009 333 361
are able to collect 166,474 positive examples over
increase_reaction 2,881 367 353
13 relation types, with more detailed statistics of
decrease_reaction 2,221 247 269
the dataset listed in Table 5.
decrease_MP 798 100 120
We consider relations between chemical-disease,
chemical-gene, and gene-disease entity pairs down-
Table 6: Data statistics for the new CTD dataset
loaded from CTD8. We remove inferred relations
brokendownbyrelationtype. Thefirstcolumnlists
(those without an associated PubMed ID) and con-
relationtypesseparatedbythetypesoftheentities.
sider only human curated relationships. Some
Columns2–4showthenumberofpositiveexamples
chemical-gene entity pairs were associated with
of that relation type. MP stands for metabolic
multiple relation types in the same document. We
processing.
consider each of these relation types as a separate
positive example.
The chemical-gene relation data contains over vocabulary (OOV) rate for named entities. Word
100 types organized in a shallow hierarchy. Many training data has 3.01 percent OOV rate for tokens
of these types are extremely infrequent, so we map with an entity. The byte pair-encoded data has an
all relations to the highest parent in the hierar- OOV rate of 2.48 percent. Note that in both the
chy, resulting in 13 relation types. Most of these word-tokenized and byte pair-tokenized data, we
chemical-gene relations have an increase and de- replace tokens that occur less than five times with
crease version such as increase_expression and de- a learned UNK token.
crease_expression. In some cases, there is also an Figure 2 depicts the model’s performance on re-
affects relation (affects_expression) which is used lation extraction as a function of distance between
when the directionality is unknown. If the affects entities. For example, the blue bar depicts perfor-
version is more common, we map decrease and in- mance when removing all entity pair candidates
crease to affects. If affects is less common, we drop (positive and negative) whose closest mentions are
the affects examples and keep the increase and de- more than 11 tokens apart. We consider remov-
crease examples as distinct relations, resulting in ing entity pair candidates with distances of 11, 25,
the final set of 10 chemical-gene relation types. 50, 100 and 500 (the maximum document length).
The average sentence length is 22 tokens. We see
3.3.2 Results
that the model is not simply relying on short range
In Table 7 we list precision, recall and F1 achieved
relationships, but is leveraging information about
byourmodelontheCTDdataset,bothoveralland
distantentitypairs,withaccuracyincreasingasthe
by relation type. Our model predicts each of the
maximum distance considered increases. Note that
relation types effectively, with higher performance
all results are taken from the same model trained
on relations with more support.
on the full unfiltered training set.
In Table 8 we see that our sub-word BPE model
out-performs the model using the Genia tokenizer
(Kulick et al., 2012) even though our vocabulary 4 Related work
size is one-fifth as large. We see a 1.7 F1 point
Relation extraction is a heavily studied area in the
boost in predicting Pubtator NER labels for BPE.
NLP community. Most work focuses on news and
This could be explained by the increased out-of-
web data (Doddington et al., 2004; Riedel et al.,
7We include scripts to generate the unfiltered set of 2010; Hendrickx et al., 2009).9 Recent neural net-
data as well to encourage future research
8http://ctdbase.org/downloads/ 9And TAC KBP: https://tac.nist.gov
878
P R F1 0.6
11
Total 25
0.5 50
Micro F1 44.8 50.2 47.3 100
500
Macro F1 34.0 29.8 31.7
0.4
Chemical/Disease
marker/mechanism 46.2 57.9 51.3 0.3
therapeutic 55.7 67.1 60.8
Gene/Disease 0.2
marker/mechanism 42.2 44.4 43.0
0.1
therapeutic 52.6 10.1 15.8
Chemical/Gene 0.0
chem_gene chem_disease gene_disease all
increases_expression 39.7 48.0 43.3 Dataset
increases_MP 26.3 35.5 29.9
Figure 2: Performance on the CTD dataset when
decreases_expression 34.4 32.9 33.4
restricting candidate entity pairs by distance. The
increases_activity 24.5 24.7 24.4
x-axis shows the coarse-grained relation type. The
affects_response 40.9 35.5 37.4
y-axis shows F1 score. Different colors denote max-
decreases_activity 30.8 19.4 23.5
imum distance cutoffs.
affects_transport 28.7 23.8 25.8
increases_reaction 12.8 5.6 7.4
decreases_reaction 12.3 5.7 7.4
sentence level.
decreases_MP 28.9 7.0 11.0
Some previous work exists on cross-sentence
relation extraction. Swampillai and Stevenson
Table 7: BRAN precision, recall and F1 results for
(2011) and Quirk and Poon (2017) consider featur-
the full CTD dataset by relation type. The model
ized classifiers over cross-sentence syntactic parses.
is optimized for micro F1 score across all types.
Most similar to our work is that of Peng et al.
(2017), which uses a variant of an LSTM to encode
Model P R F1
document-level syntactic parse trees. Our work
Relation extraction
differs in three key ways. First, we operate over
Words 44.9 48.8 46.7 0.39 raw tokens negating the need for part-of-speech
±
BPE 44.8 50.2 47.3 0.19 or syntactic parse features which can lead to cas-
±
NER cading errors. We also use a feed-forward neural
Words 91.0 90.7 90.9 0.13 architecture which encodes long sequences far more
±
BPE 91.5 93.6 92.6 0.12 efficientlycomparedtothegraphLSTMnetworkof
±
Peng et al. (2017). Finally, our model considers all
Table 8: Precision, recall, and F1 results for CTD
mention pairs simultaneously rather than a single
named entity recognition and relation extraction,
mention pair at a time.
comparing BPE to word-level tokenization.
We employ a bi-affine function to form pairwise
predictions between mentions. Such models have
also been used for knowledge graph link prediction
workapproachestorelationextractionhavefocused
(Nickeletal.,2011;Lietal.,2016b),withvariations
onCNNs(dosSantosetal.,2015;Zengetal.,2015)
such as restricting the bilinear relation matrix to
or LSTMs (Miwa and Bansal, 2016; Verga et al.,
be diagonal (Yang et al., 2015) or diagonal and
2016a; Zhou et al., 2016b) and replacing stage-wise
complex (Trouillon et al., 2016). Our model is
information extraction pipelines with a single end-
similar to recent approaches to graph-based depen-
to-end model (Miwa and Bansal, 2016; Ammar
dency parsing, where bilinear parameters are used
et al., 2017; Li et al., 2017). These models all
toscorehead-dependentcompatibility(Kiperwasser
consider mention pairs separately.
and Goldberg, 2016; Dozat and Manning, 2017).
There is also a considerable body of work specifi-
cally geared towards supervised biological relation 5 Conclusion
extraction including protein-protein (Pyysalo et al.,
2007; Poon et al., 2014; Mallory et al., 2015), drug- We present a bi-affine relation attention network
drug (Segura-Bedmar et al., 2013), and chemical- that simultaneously scores all mention pairs within
disease (Gurulingappa et al., 2012; Li et al., 2016a) a document. Our model performs well on three
interactions, and more complex events (Kim et al., datasets, includingtwostandardbenchmarkbiolog-
2008;Riedeletal.,2011). Ourworkfocusesonmod- ical relation extraction datasets and a new, large
eling relations between chemicals, diseases, genes and high-quality dataset introduced in this work.
and proteins, where available annotation is often Our model out-performs the previous state of the
at the document- or abstract-level, rather than the art on the Biocreative V CDR dataset despite us-
879
erocS
1F
ing no additional linguistic resources or mention knowledgebase and discovery tool for chemical–
pair-specific features. gene–disease networks. Nucleic acids research
Our current model predicts only into a fixed 37(suppl_1):D786–D792.
schema of relations given by the data. However,
George Doddington, Alexis Mitchell, Mark Przy-
this could be ameliorated by integrating our model
bocki, Lance Ramshaw, Stephanie Strassel, and
into open relation extraction architectures such Ralph Weischedel. 2004. The automatic content
as Universal Schema (Riedel et al., 2013; Verga extraction (ace) program tasks, data, and evalu-
et al., 2016b). Our model also lends itself to other ation. In Proceedings of the Fourth International
pairwisescoringtaskssuchashypernymprediction, Conference on Language Resources and Evalua-
co-reference resolution, and entity resolution. We tion.
will investigate these directions in future work.
Cicero dos Santos, Bing Xiang, and Bowen Zhou.
2015. Classifying relations by ranking with con-
Acknowledgments
volutional neural networks. In Proceedings of the
53rd Annual Meeting of the Association for Com-
WethankOferShaiandtheChanZuckerbergInitia-
putational Linguistics and the 7th International
tive/Metadatascienceteamforhelpfuldiscussions.
Joint Conference on Natural Language Process-
We also thank Timothy Dozat and Kyubyong Park
ing (Volume 1: Long Papers). Association for
for releasing their code.
ComputationalLinguistics,Beijing,China,pages
626–634. http://www.aclweb.org/anthology/
P15-1061.
References
Timothy Dozat and Christopher D Manning. 2017.
Martín Abadi, Ashish Agarwal, Paul Barham, Deep biaffine attention for neural dependency
Eugene Brevdo, Zhifeng Chen, Craig Citro, parsing. 5th International Conference on Learn-
Greg S. Corrado, Andy Davis, Jeffrey Dean, ing Representations .
Matthieu Devin, Sanjay Ghemawat, Ian Good-
fellow, Andrew Harp, Geoffrey Irving, Michael Philip Gage. 1994. A new algorithm for data com-
Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz pression. The C Users Journal 12(2):23–38.
Kaiser, ManjunathKudlur, JoshLevenberg, Dan
XavierGlorot, AntoineBordes, andYoshuaBengio.
Mané, Rajat Monga, Sherry Moore, Derek Mur-
2011. Deep sparse rectifier neural networks. In
ray, Chris Olah, Mike Schuster, Jonathon Shlens,
Proceedings of the Fourteenth International Con-
Benoit Steiner, Ilya Sutskever, Kunal Talwar,
ference on Artificial Intelligence and Statistics.
Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
pages 315–323.
van, Fernanda Viégas, Oriol Vinyals, Pete War-
den, Martin Wattenberg, Martin Wicke, Yuan
Jinghang Gu, Longhua Qian, and Guodong Zhou.
Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
2016. Chemical-induced disease relation extrac-
Large-scale machine learning on heterogeneous
tion with various linguistic features. Database
systems. Software available from tensorflow.org.
2016.
http://tensorflow.org/.
Jinghang Gu, Fuqing Sun, Longhua Qian, and
Waleed Ammar, Matthew E. Peters, Chandra Bha- Guodong Zhou. 2017. Chemical-induced disease
gavatula, and Russell Power. 2017. The ai2 sys- relation extraction via convolutional neural net-
tem at semeval-2017 task 10 (scienceie): semi- work. Database 2017.
supervised end-to-end entity and relation extrac-
tion. nucleus 2(e2):e2. Harsha Gurulingappa, Abdul Mateen Rajput, An-
gus Roberts, Juliane Fluck, Martin Hofmann-
Jimmy Lei Ba, Jamie Ryan Kiros, and Geof- Apitius, and Luca Toldo. 2012. Development of
frey E Hinton. 2016. Layer normalization. arXiv a benchmark corpus to support the automatic
preprint arXiv:1607.06450 . extraction of drug-related adverse effects from
medical case reports. Journal of biomedical in-
RajarshiDas,ArvindNeelakantan,DavidBelanger,
formatics 45(5):885–892.
and Andrew McCallum. 2017. Chains of reason-
ing over entities, relations, and text using recur- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
rent neural networks. In Proceedings of the 15th Jian Sun. 2016. Deep residual learning for image
Conference of the European Chapter of the Asso- recognition. In Proceedings of the IEEE confer-
ciation for Computational Linguistics: Volume 1, ence on computer vision and pattern recognition.
Long Papers. Association for Computational Lin- pages 770–778.
guistics, Valencia, Spain, pages 132–141. http:
//www.aclweb.org/anthology/E17-1013. Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Allan Peter Davis, Cynthia G Murphy, Cyn- Padó, Marco Pennacchiotti, Lorenza Romano,
thia A Saraceni-Richards, Michael C Rosenstein, and Stan Szpakowicz. 2009. Semeval-2010 task
Thomas C Wiegers, and Carolyn J Mattingly. 8: Multi-way classification of semantic relations
2008. Comparative toxicogenomics database: a between pairs of nominals. In Proceedings of
880
the Workshop on Semantic Evaluations: Recent Xiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gim-
Achievements and Future Directions.Association pel. 2016b. Commonsense knowledge base com-
for Computational Linguistics, pages 94–99. pletion. In Proceedings of the 54th Annual
Meeting of the Association for Computational
Sepp Hochreiter. 1998. The vanishing gradient Linguistics (Volume 1: Long Papers). Associa-
problem during learning recurrent neural nets tion for Computational Linguistics, Berlin, Ger-
and problem solutions. International Journal many, pages 1445–1455. http://www.aclweb.
of Uncertainty, Fuzziness and Knowledge-Based org/anthology/P16-1137.
Systems 6(2):107–116.
Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Luan, and Maosong Sun. 2016. Neural rela-
Long short-term memory. Neural computation tion extraction with selective attention over in-
9(8):1735–1780. stances. In Proceedings of the 54th Annual
Meeting of the Association for Computational
Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii.
Linguistics (Volume 1: Long Papers). Associa-
2008. Corpus annotation for mining biomedi-
tion for Computational Linguistics, Berlin, Ger-
cal events from literature. BMC bioinformatics
many, pages 2124–2133. http://www.aclweb.
9(1):10.
org/anthology/P16-1200.
Diederik Kingma and Jimmy Ba. 2015. Adam: A
Sijia Liu, Feichen Shen, Yanshan Wang, Ma-
method for stochastic optimization. In 3rd Inter-
jid Rastegar-Mojarad, Ravikumar Komandur
nationalConferenceforLearningRepresentations
Elayavilli, Vipin Chaundary, and Hongfang Liu.
(ICLR). San Diego, California, USA.
2017. Attention-based neural networks for chem-
ical protein relation extraction. Proceedings of
EliyahuKiperwasserandYoavGoldberg.2016. Sim-
the BioCreative VI Workshop .
ple and accurate dependency parsing using bidi-
rectional lstm feature representations. Trans- Emily K Mallory, Ce Zhang, Christopher Ré, and
actions of the Association for Computational Russ B Altman. 2015. Large-scale extraction of
Linguistics 4:313–327. https://transacl.org/ gene interactions from full-text literature using
ojs/index.php/tacl/article/view/885. deepdive. Bioinformatics 32(1):106–113.
Martin Krallinger, Obdulia Rabal, Saber A. Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Akhondi, Martín Pérez Pérez, Jesús Santa- frey Dean. 2013. Efficient estimation of word
maría, Pérez Gael Rodríguez, Georgios Tsatsa- representations in vector space. arXiv preprint
ronis, Ander Intxaurrondo, José Antonio López, arXiv:1301.3781 .
UmeshNandal,ErinVanBuel,AkileshwariChan-
drasekhar, Marleen Rodenburg, Astrid Laegreid, Mike Mintz, Steven Bills, Rion Snow, and Daniel
Marius Doornenbal, Julen Oyarzabal, Analia Jurafsky. 2009. Distant supervision for relation
Lourenço, and Alfonso Valencia. 2017. Overview extraction without labeled data. In Proceed-
of the biocreative vi chemical-protein interaction ings of the Joint Conference of the 47th Annual
track. Proceedings of the BioCreative VI Work- Meeting of the ACL and the 4th International
shop page 140. Joint Conference on Natural Language Process-
ing of the AFNLP. Association for Computa-
Martin Krallinger, Obdulia Rabal, Florian Leit- tionalLinguistics,Suntec,Singapore,pages1003–
ner, Miguel Vazquez, David Salgado, Zhiyong 1011. http://www.aclweb.org/anthology/P/
Lu, Robert Leaman, Yanan Lu, Donghong Ji, P09/P09-1113.
Daniel M Lowe, et al. 2015. The chemdner cor-
pus of chemicals and drugs and its annotation Makoto Miwa and Mohit Bansal. 2016. End-to-end
principles. Journal of cheminformatics 7(S1):S2. relation extraction using lstms on sequences and
treestructures. InProceedingsofthe54thAnnual
SethKulick,AnnBies,MarkLiberman,MarkMan- Meeting of the Association for Computational
del, Scott Winters, and Pete White. 2012. In- Linguistics (Volume 1: Long Papers). Associa-
tegrated annotation for biomedical information tion for Computational Linguistics, Berlin, Ger-
extraction. HLT/NAACL Workshop: Biolink . many, pages 1105–1116. http://www.aclweb.
org/anthology/P16-1105.
FeiLi,MeishanZhang,GuohongFu,andDonghong
Ji. 2017. A neural joint model for entity and Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya
relation extraction from biomedical text. BMC Sutskever, Lukasz Kaiser, Karol Kurach, and
bioinformatics 18(1):198. James Martens. 2015. Adding gradient noise
improves learning for very deep networks. arXiv
Jiao Li, Yueping Sun, Robin J Johnson, Daniela preprint arXiv:1511.06807 .
Sciaky, Chih-Hsuan Wei, Robert Leaman, Al-
lan Peter Davis, Carolyn J Mattingly, Thomas C Maximilian Nickel, Volker Tresp, and Hans-Peter
Wiegers, and Zhiyong Lu. 2016a. Biocreative v Kriegel. 2011. A three-way model for collective
cdr task corpus: a resource for chemical disease learning on multi-relational data. In Proceedings
relation extraction. Database 2016. of the 28th international conference on machine
881
learning (ICML-11).Bellevue,Washington,USA, Rico Sennrich, Barry Haddow, and Alexandra
pages 809–816. Birch. 2015. Neural machine translation of
rare words with subword units. arXiv preprint
NanyunPeng, HoifungPoon, ChrisQuirk, Kristina arXiv:1508.07909 .
Toutanova, and Wen-tau Yih. 2017. Cross-
sentence n-ary relation extraction with graph Nitish Srivastava, Geoffrey E Hinton, Alex
lstms. Transactions of the Association for Com- Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
putational Linguistics 5:101–115. dinov. 2014. Dropout: a simple way to prevent
neural networks from overfitting. Journal of ma-
YifanPeng,Chih-HsuanWei,andZhiyongLu.2016. chine learning research 15(1):1929–1958.
Improving chemical disease relation extraction
withrichfeaturesandweaklylabeleddata. Jour- Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
nal of cheminformatics 8(1):53. ati, and Christopher D. Manning. 2012. Multi-
instance multi-label learning for relation extrac-
HoifungPoon,KristinaToutanova,andChrisQuirk. tion. In Proceedings of the 2012 Joint Confer-
2014. Distant supervision for cancer pathway ence on Empirical Methods in Natural Language
extraction from text. In Pacific Symposium on Processing and Computational Natural Language
Biocomputing Co-Chairs. pages 120–131. Learning.AssociationforComputationalLinguis-
tics, Jeju Island, Korea, pages 455–465. http:
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari //www.aclweb.org/anthology/D12-1042.
Björne,JormaBoberg,JouniJärvinen,andTapio
Salakoski. 2007. Bioinfer: a corpus for informa- Kumutha Swampillai and Mark Stevenson. 2011.
tion extraction in the biomedical domain. BMC Extracting relations within and across sentences.
bioinformatics 8(1):50. In Proceedings of the International Conference
Recent Advances in Natural Language Process-
ChrisQuirkandHoifungPoon.2017. Distantsuper- ing 2011. RANLP 2011 Organising Committee,
visionforrelationextractionbeyondthesentence Hissar, Bulgaria, pages 25–32. http://aclweb.
boundary. In Proceedings of the 15th Conference org/anthology/R11-1004.
of the European Chapter of the Association for
Computational Linguistics: Volume 1, Long Pa- Théo Trouillon, Johannes Welbl, Sebastian Riedel,
pers. Association for Computational Linguistics, Éric Gaussier, and Guillaume Bouchard. 2016.
Valencia, Spain, pages 1171–1182. Complex embeddings for simple link prediction.
In International Conference on Machine Learn-
SebastianRiedel,DavidMcClosky,MihaiSurdeanu, ing. pages 2071–2080.
Andrew McCallum, and Christopher D. Man-
ning. 2011. Model combination for event extrac- Ashish Vaswani, Noam Shazeer, Niki Parmar,
tion in bionlp 2011. In Proceedings of BioNLP Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Shared Task 2011 Workshop. Association for Lukasz Kaiser, and Illia Polosukhin. 2017.
Computational Linguistics, Portland, Oregon, Attention is all you need. arXiv preprint
USA, pages 51–55. http://www.aclweb.org/ arXiv:1706.03762 .
anthology/W11-1808.
Patrick Verga, David Belanger, Emma Strubell,
Sebastian Riedel, Limin Yao, and Andrew McCal- Benjamin Roth, and Andrew McCallum. 2016a.
lum. 2010. Modeling relations and their men- Multilingual relation extraction using composi-
tionswithoutlabeledtext. Machine learning and tional universal schema. In Proceedings of the
knowledge discovery in databases pages 148–163. 2016 Conference of the North American Chapter
of the Association for Computational Linguistics:
Sebastian Riedel, Limin Yao, Andrew McCallum, Human Language Technologies. Association for
and Benjamin M Marlin. 2013. Relation ex- Computational Linguistics, San Diego, Califor-
traction with matrix factorization and universal nia, pages 886–896. http://www.aclweb.org/
schemas. In Proceedings of NAACL-HLT. pages anthology/N16-1103.
74–84.
Patrick Verga, David Belanger, Emma Strubell,
Isabel Segura-Bedmar, Paloma Martínez, and Benjamin Roth, and Andrew McCallum. 2016b.
María Herrero Zazo. 2013. Semeval-2013 task Multilingual relation extraction using compo-
9 : Extraction of drug-drug interactions from sitional universal schema. In Proceedings of
biomedical texts (ddiextraction 2013). In Sec- NAACL-HLT. pages 886–896.
ond Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings Patrick Verga and Andrew McCallum. 2016. Row-
of the Seventh International Workshop on Se- less universal schema. In Proceedings of the 5th
mantic Evaluation (SemEval 2013). Association Workshop on Automated Knowledge Base Con-
for Computational Linguistics, Atlanta, Georgia, struction. Association for Computational Lin-
USA, pages 341–350. http://www.aclweb.org/ guistics, San Diego, CA, pages 63–68. http:
anthology/S13-2056. //www.aclweb.org/anthology/W16-1312.
882
Chih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu.
2013. Pubtator: a web-based text mining tool
for assisting biocuration. Nucleic Acids Research
41. https://doi.org/10.1093/nar/gkt441.
Chih-Hsuan Wei, Yifan Peng, Robert Leaman, Al-
lan Peter Davis, Carolyn J Mattingly, Jiao Li,
Thomas C Wiegers, and Zhiyong Lu. 2016. As-
sessing the state of the art in biomedical rela-
tion extraction: overview of the biocreative v
chemical-disease relation (cdr) task. Database
2016.
Yadollah Yaghoobzadeh, Heike Adel, and Hinrich
Schütze. 2017. Noise mitigation for neural en-
tity typing and relation extraction. In Proceed-
ings of the 15th Conference of the European
Chapter of the Association for Computational
Linguistics: Volume 1, Long Papers. Associ-
ation for Computational Linguistics, Valencia,
Spain, pages 1183–1194. http://www.aclweb.
org/anthology/E17-1111.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jian-
feng Gao, and Li Deng. 2015. Embedding enti-
ties and relations for learning and inference in
knowledge bases. In 3rd International Confer-
ence for Learning Representations (ICLR). San
Diego, California, USA.
Daojian Zeng, Kang Liu, Yubo Chen, and Jun
Zhao. 2015. Distant supervision for relation ex-
traction via piecewise convolutional neural net-
works. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguis-
tics, Lisbon, Portugal, pages 1753–1762. http:
//aclweb.org/anthology/D15-1203.
Huiwei Zhou, Huijie Deng, Long Chen, Yunlong
Yang, Chen Jia, and Degen Huang. 2016a. Ex-
ploiting syntactic and semantics information for
chemical–disease relation extraction. Database
2016.
Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi,
Bingchen Li, Hongwei Hao, and Bo Xu. 2016b.
Attention-based bidirectional long short-term
memory networks for relation classification. In
Proceedingsofthe54thAnnualMeetingoftheAs-
sociation for Computational Linguistics (Volume
2: Short Papers). Association for Computational
Linguistics, Berlin, Germany, pages 207–212.
http://anthology.aclweb.org/P16-2034.
883
A Implementation Details A.3 Full CTD Dataset
We tune separate decision boundaries for each re-
The model is implemented in Tensorflow (Abadi
lation type on the development set. For each pre-
et al., 2015) and trained on a single TitanX gpu.
diction, the relation type with the maximum prob-
The number of transformer block repeats is B =2 .
ability is assigned. If the probability is below the
We optimize the model using Adam (Kingma and
relation specific threshold, the prediction is set to
Ba, 2015) with best parameters chosen for (cid:15), β ,
1
NULL. We use embedding dimension 128 with all
β chosen from the development set. The learning
2
embeddings randomly initialized. Our byte pair
rate is set to 0.0005 and batch size 32. In all of our
encoding vocabulary is constructed with a budget
experiments we set the number of attention heads
of 50,000. Models took 1 to 2 days to train.
to h=4.
(cid:15) was set to 1e-4, β to .1, and β to 0.9. Gradi-
1 2
Weclipthegradientstonorm10andapplynoise
ent noise η =.1.Dropout was applied to the word
to the gradients (Neelakantan et al., 2015). We
embeddings with keep probability 0.95, internal
tune the decision threshold for each relation type
layers with 0.95 and final bilinear projection with
separately and perform early stopping on the devel-
0.5
opment set. We apply dropout (Srivastava et al.,
2014) to the input layer randomly replacing words
withaspecialUNKtokenwithkeepprobability.85.
WeadditionallyapplydropouttotheinputT (word
embedding + position embedding), interior layers,
and final state. At each step, we randomly sample
apositiveornegative(NULLclass)minibatchwith
probability 0.5.
A.1 Chemical Disease Relations Dataset
Token embeddings are pre-trained using skipgram
(Mikolov et al., 2013) over a random subset of 10%
of all PubMed abstracts with window size 10 and
20 negativesamples. We merge the train and devel-
opment sets and randomly take 850 abstracts for
training and 150 for early stopping. Our reported
resultsareaveragedover10runsandusingdifferent
splits. All baselines train on both the train and
development set. Models took between 4 and 8
hours to train.
(cid:15) was set to 1e-4, β to .1, and β to 0.9. Gradi-
1 2
ent noise η =.1. Dropout was applied to the word
embeddingswithkeepprobability0.85,internallay-
ers with 0.95 and final bilinear projection with 0.35
for the standard CRD dataset experiments. When
adding the additional weakly labeled data: word
embeddings with keep probability 0.95, internal
layers with 0.95 and final bilinear projection with
0.5.
A.2 Chemical Protein Relations Dataset
We construct our byte-pair encoding vocabulary
using a budget of 7500. The dataset contains an-
notations for a larger set of relation types than are
used in evaluation. We train on only the relation
types in the evaluation set and set the remaining
types to the Null relation. The embedding dimen-
sion is set to 200 and all embeddings are randomly
initialized. (cid:15)wassetto1e-8, β to.1, andβ to0.9.
1 2
Gradientnoiseη =1.0. Dropoutwasappliedtothe
wordembeddingswithkeepprobability0.5,internal
layers with 1.0 and final bilinear projection with
0.85 for the standard CRD dataset experiments.
884

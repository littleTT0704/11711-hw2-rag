Event Nugget and Event Coreference Annotation
Zhiyi Songa, Ann Biesa, Stephanie Strassela, Joe Ellisa, Teruko Mitamurab, Hoa Dangc,
Yukari Yamakawab, Sue Holmb
aLinguistic Data Consortium, University of Pennsylvania,
3600 Market Street, suite 810, Philadelphia, PA 19104, USA
{zhiyi, bies, strassel, joellis}@ldc.upenn.edu
bLanguage Technologies Institute, Carnegie Mellon University,
6711 Gates-Hillman Center, 5000 Forbes Ave., Pittsburgh, PA 15213, USA
teruko+@cs.cmu.edu, yukariy@andrew.cmu.edu, sh4s@andrew.cmu.edu
cNational Institute of Standards and Technology, 100 Bureau Drive, Gaithersburg, MD20899,
USA
hoa.dang@nist.gov
tracks of TAC encourage the development of sys-
Abstract tems that can match entities mentioned in natural
texts with those appearing in a knowledge base and
In this paper, we describe the event nugget an- extract information about entities from a document
notation created in support of the pilot Event collection and add it to a new or existing knowledge
Nugget Detection evaluation in 2014 and in base. Starting in 2014, TAC KBP added a track for
support of the Event Nugget Detection and Co- event evaluation. The goal of the TAC KBP Event
reference open evaluation in 2015, which was
track is to extract information about events such that
one of the Knowledge Base Population tracks
the information would be suitable as input to a
within the NIST Text Analysis Conference. We
knowledge base.
present the data volume annotated for both
Event Nugget (EN) evaluation, asone of the eval-
training and evaluationdata for the 2015 eval-
uation as well as changes to annotation in 2015 uation tasks in the TAC KBP event track, aims to
as compared to that of 2014. We also analyze evaluate system performance on EN detection and
the annotation forthe 2015 evaluation as an ex- EN coreference (Mitamura & Hovy, 2015). An
ample to show the annotation challenges and event nugget, as defined by the task, includes a text
consistency, and identify the event types and extent that instantiates an event, a classification of
subtypes that are most difficult for human an-
event type and subtype, and an indication of there-
notators. Finally, we discuss annotation issues
alis of the event. This is different from nuggets
that we need to take into consideration in the
(Voorhees, 2003; Babko-Malaya, et al, 2012) and
future.
the Summary Content Units (SCU) described in
(Nenkova and Passnoneau, 2004; Nenkova et al,
1 Introduction
2007)wherenuggets andSCU are units of meaning
TheText Analysis Conference (TAC) is a series of (usually in the form of a sentential clause) which not
workshops organized by the National Institute of only includes the main verb/word that instantiates
Standards and Technology (NIST), aiming to en- an event, but also all arguments.
courage research in natural language processing In this paper, we describe the event nugget anno-
(NLP). The Knowledge Base Population (KBP) tation to support the pilot EN Detection evaluation
37
Proceedingsofthe4thWorkshoponEvents:Definition,Detection,Coreference,andRepresentation,pages37–45,
SanDiego,California,June17,2016.(cid:13)c2016AssociationforComputationalLinguistics
in 2014 and in support of the EN Detection and Co-  EN Coreference: Participating systems must
reference TAC KBP open evaluation in 2015. We identify full event coreference links, given
discuss changes to the annotation in 2015 as com- the annotated event nuggets in the text.
pared to that of 2014 and present the data volume ERE was developed as an annotation task that
annotated for both training and evaluation data for would be supportive of multiple research directions
the 2015 evaluation. We analyze the annotation for and evaluations in the DEFT program, and that
the 2015 evaluationdatato show the annotationand would providea useful foundation for more special-
consistencychallenges, identify the event types and ized annotation tasks like inference and anomaly.
subtypes that are most difficult for human annota- The resulting ERE annotation task has evolved over
tors, and finally discuss annotation issues that we the course of the program, from a fairly lightweight
need to take into consideration in the future. treatment of entities, relations and events in text
(Light ERE), to a richer representation of phenom-
2 Event Nugget Evaluation in TAC KBP ena of interest to the program (Rich ERE) (Song, et
al., 2015). In ERE Event annotation, each event
The EN evaluation was introduced in 2014 and run
mention has annotation of event type and subtype,
as a pilot evaluation, whichsought to serve two pur-
its realis attribute, any of its arguments or partici-
poses. One was to measure event detection by per-
pants that are present,anda required “trigger” string
formers’ systems in the Deep Exploration and Fil-
in the text; furthermore, event mentions within the
tering of Text (DEFT) programof the Defense Ad-
same documentare coreferenced into event hoppers
vanced Research Projects Agency(DARPA, 2012).
(Song, et al., 2015). EN annotation includes all of
The program aims to address remaining capability
these annotations, except the annotation of event ar-
gaps in state-of-the-art natural language processing
guments.
technologies related to inference, causal relation-
The EN taskin 2014adapted the event annotation
ships and anomaly detection. The other purpose of
guidelines from the Light ERE annotation task
the pilot, however, was to test run the evaluation
(Aguilar, et al., 2014) by incorporating modifica-
framework before opening it up to the full TAC
tions bythe evaluationcoordinators that focused on
KBP community. The Pilot EN evaluation had one
the text extents establishing valid references to
evaluation task:
events, clarifications on transaction event types, and
 EN Detection: Participating systems must
the additional annotation of event realis attributes,
identify all relevant Event Mention instances
which indicated whether each event mention was
in English texts, categorize each mention’s
asserted (Actual), generic or habitual (Generic), or
Event types/subtype and identify its realis
some other category, such as future, hypothetical,
value (ACTUAL, GENERIC, or OTHER).
negated, or uncertain (Other) (Mitamura, et al.,
As the pilot was considered a success, EN was
2015).
added to the roster of full-fledgedevaluation tracks
In 2015, EN annotation followed the Rich ERE
for TAC KBP 2015,with some modifications to in-
Event annotation guidelines (except for the annota-
corporate lessons learned from the pilot and to better
tion of event arguments). As compared to ENanno-
align with the other TAC KBP 2015 event-related
tation in 2014, Rich EREEvent annotationand 2015
evaluations – Event Argument Linking (EAL)
EN annotationinclude increased taggability in sev-
(Freedman, 2014 & 2015) – and also the Entities,
eral areas: slightly expanded event ontology, addi-
Relations and Events (ERE) data provided to KBP
tional attributes for contact and transaction events,
participants for training purposes. The EN evalua-
and double tagging of event mentions for multiple
tion in TAC KBP 2015 included two new tasks in
types/subtypes and for certain types of coordination,
addition to EN Detection:
in addition to event coreference.General Instruc-
 EN Detection and Coreference: Participating
tions
systems must identify not only the event nug-
get, but also full event coreference links. Full
Event Coreference is identified when two or
more event nuggets refer to the same event.
38
3 Event Nugget Annotation al., 2014; Song et al., 2015). The 2014 EN evalua-
tion covered the inventory of event types and sub-
In this section, we describe the EN annotation as types from Light ERE, including 8 event types and
well as the major differences between the 2014 and 33 subtypes.
2015 annotation tasks. The 2015 evaluation added a new event type
(Manufacture) and four new subtypes – Move-
3.1 EventTrigger
ment.TransportArtifact, Contact.Broadcast, Con-
An event trigger span is the textual extent within a tact.Contact, Transaction.Transaction – which
sentence that indicates a reference to a valid event aligned the EN event ontology with that of Rich
of a limited set of event types and subtypes. In the EREin order to take advantage of the existing Rich
2014 ENtask,the trigger span was defined as a se- ERE annotated data as training data.The EN anno-
mantically meaningful unit which could be either a tation task also adopted a new approach for applying
single word(main verb, noun, adjective, adverb) or the Contact event subtype categorizations, which
a continuous or discontinuous multi-word phrase had been developed for Rich ERE data creation ef-
(Mitamura et al., 2015). For the 2015 EN evalua- forts. Instead of having annotators categorize the
tion, trigger span was redefined as the smallest, con- subtypes directly, Contact event mentions were la-
tiguous extent of text (usually a word or phrase) that beled with attributes to describe formality (Formal,
most saliently expresses the occurrence of an event. Informal, Can’t Tell), scheduling (Planned, Sponta-
This change brings consistency to the EN annota- neous, Can’t Tell), medium (In-person, Not-in-per-
tion of event trigger with approaches taken in Au- son, Can’t Tell), and audience (Two-way, One-way,
tomatic Content Extraction (ACE) (LDC, 2005) as Can’t Tell). Contact event subtypes were automati-
well asLight and Rich ERE(Song et al, 2015). Ad- cally generated based on the annotated attributes.
ditionally, we seeimproved annotation consistency This change added increased granularity to the Con-
in terms of event trigger extent, as shown in Figure tact events and captured more subtypes, which had
1 of section 5.1.Unlike in the 2014 EN annotation, been requested by data users, and it also allowed the
annotators for the 2015 data were allowed to ‘dou- annotation to provide information at two levels, the
ble tag’ event triggers in order to indicate that a attribute level and the traditional event subtype
given text extent referred to more than one event level, which may support more robust system devel-
type/subtype, which was usually used to indicate the opment.
presence of an obligatorily inferred event. Double
tagging was also used for certain types of coordina- 3.3 Event Coreference
tion. For example, given the following text:
The final change to the2015EN evaluation as com-
Cipriani was sentenced to life in prison for pared to the 2014 pilot was the added requirement
the murder of Renault chief George Besse of event coreference. Againfollowingthe Rich ERE
in 1986 and the head of government arms task, EN annotation in 2015 adopted the notion of
sales Rene Audran a year earlier. ‘event hoppers’, a more inclusive, less strict notion
In 2015 EN annotation, the word “murder” would
of event coreference as compared to previous ap-
be the trigger for two Life.Die events, one with the
proaches as in ACE (LDC, 2005) and Light ERE.
victim “George Besse” and the other with “Rene
The notion of event hopper was introduced to ad-
Audran” as well as two Conflict.Attack events, one
dress the pervasive challenges of event coreference,
occurring in 1986 and the other in 1985.In 2014
with respect to event mention and event argument
EN annotation, the word “murder” would be the
granularity (Song, et al., 2015). Following this ap-
trigger for only one Conflict.Attack event.
proach, event mentionswere added to an event hop-
per when they were intuitively coreferential to an
3.2 Event Taxonomy
annotator, even if they did not meet a strict event
EN annotation andevaluation focus on a limited in- identity requirement. Event nuggets could be placed
ventory of event types and subtypes, as defined in into the same event hoppers even if they differed in
ERE, based on Automatic Content Extraction (Dod- temporal or trigger granularity, their arguments
dington et al., 2004; Walker et al., 2006; Aguilar et were non-coreferential or conflicting, or even if
39
their realis mood differed, as long as they referred 4.2 EvaluationData
to the same event with the same type and subtype.
Source data for the 2015ENevaluation was a subset
For example, in the following two sentences:
of the documents selected for EAL evaluation,
 The White House didn’t confirm Obama’s
which had been manually selected to ensure cover-
trip {Movement.TransportPerson, Other}
age of all event types and subtypes for that evalua-
to Paris for the Climate Summit last year.
tion. Tokenization of the source documents was also
 Obama went {Movement.TransportPer-
provided. Unlike the 2014 data, in which annotation
son, Actual} to Paris for the Climate Sum-
was performed on pre-tokenized text, in 2015 to-
mit.
kenizationwas performed as a post-annotation pro-
The first event nugget’s realis label is Other while
cedure, using tool kits provided by evaluation coor-
the second event nugget is Actual. From the context,
dinators.
we know that both nuggets are talking about the
In order to reduce the impact of low recall on an-
same trip to Paris, so even though they have differ-
notation consistency, which had proven problematic
ent realis labels, they still belong to the same event
in the pilot and in previous event annotation efforts
hopper.
(Mitamuraet al., 2015), gold standard EN data was
producedby first having two annotators performEN
4 Training and Evaluation Data
annotation (which included the creation of event
hoppers) independently for each document (referred
In this section, we present the training and evalua-
to as first pass1 orFP1,and first pass 2 orFP2,be-
tion data annotated to support the EN evaluation in
low), which was followed by an adjudication pass
2015.
conducted by a senior annotator to resolve disagree-
4.1 TrainingData mentsand add annotation that was otherwise missed
in one of the first passes. The EN annotation team
Due to the changes in Rich ERE event annotation
consisted of nine annotators, six of whom were also
(and hence 2015 EN annotation) as comparedto EN
adjudicators, and care was taken to ensure that an-
annotation in 2014, the 2014 evaluation data setwas
notators did not adjudicate their own files. Follow-
re-annotated so that the annotation matched the
ing adjudication of all documents, a corpus-wide
Rich ERE standard. Additionally Rich ERE annota-
quality control pass was also performed. In this
tionis createdas a core resource for the DEFT pro-
pass, annotators manuallyscanneda list of allevent
gram and TAC KBP evaluation, aiming to provide
triggers to review event type and subtype valuesand
a valuable resource for multiple evaluations. Rich
all event hoppers to make sure that event mentions
ERE annotation includes exhaustive annotation of
in the same hopper have same type and subtype
Entities, Relations, Events and Event Hoppers, and
value.All identified outliers were then manuallyre-
2015 ENannotationshares theRich EREannotation
viewedin context,and corrected if needed.
guidelines for Events, with the exception that
Annotation Genre Files Words EN Hoppers
Events andEvent hoppers in Rich ERE also include
theannotation of event arguments. Table 1 lists the EN and Coref NW 98 49,319 3,788 2,440
total training data volume available for the 2015EN
EN and Coref DF 104 39,333 2,650 1,685
evaluation.
Total 202 88,652 6,438 4,125
Annotation Genre Files Words EN Hoppers
Table 2:EvaluationData Volumefor 2015Event Nugget.
EN and Coref NW 81 27,897 2,219 1,461
EN and Coref DF 77 97,124 4,319 1,874 The evaluation data set consists of 202 docu-
RichERE DF 240 156,041 4,192 3,044 ments with a total word count of 88,652. The gold
standard annotation has a total of 6,438 event nug-
RichERE NW 48 23,999 1,571 1,099
gets and 4,125 event hoppers in total. Table 2 shows
Total 446 305,061 12,301 7,478 the profile of the evaluationdataset.
Appendix 1 shows the distribution of each type-
Table 1:Training Data Volume for2015Event Nugget.
subtype in the evaluation data. Conflict.Attack has
the highest representation (591event nuggets) while
40
Business.EndOrg has the lowest count (6).Withthe 2015 F1 2014 F1
100
two newly added contact subtypes (Contact and
80
Broadcast), there are altogether 1,491 contact event
nuggets (23%), with 1,101 Contact.Contact and 60
Contact.Broadcast combined (17%). Each event 40
nugget is labeled with one of the three realis attrib- 20
utes: actual, generic and other. Table 3 shows the
0
distribution of event nugget realis annotation by
extent typing realis all
genres.
realis NW DF
actual 2,508 1,595 Figure1:Inter-annotator agreement on first pass EN an-
notationin 2014 and 2015.
generic 603 539
other 677 516 Mention type/subtype mismatch:
total 3,788 2,650  Dr. Yusuf Sonmez whom he called a notori-
ous international organ trafficker. (FP1:
Table 3:Event Nugget counts by realis attributes in NW
and DF genres. Movement.TransportArtifact; FP2:Transac-
tion.TransferOwnership)
5 Inter-annotator Agreement and Annota- Realis Attribute mismatch:
tion Challenges  The wealthy, ailing patients who were to re-
ceive (FP1: Actual; FP2: Other) the organs
Subsequent analysis of inter-annotator agreement in
flew to Pristina.
the EN 2015 evaluation data indicates that several
Some types scored better than others. As ex-
challenges remain to be addressed.
pected, Contact and Transaction event types have
the lowest consistency. Figure 2 displays the IAA
5.1 Inter-annotatorAgreement
F1 scores on first pass EN annotation by event
Annotation consistency is generally in line with types.
what we expect due to the complex nature of event
extent typing realis all
100
recognition.The changes in the approach to the an- 80
notation task that were described above appear to 60
have made some improvements, as shown in Figure 40
20
1, which compares the overall inter-annotatoragree-
0
ment(IAA)on first pass EN annotation in 2014 and
2015. Compared with 2014, IAA F1 score in 2015
improved by 5% in event trigger detection (“plain”
in Figure 3) and realis attribute labelling. There is
only 1% of improvement on event type and subtype
classification. Regarding event coreference, which
Figure2:Inter-annotatoragreement on first pass EN anno-
was new in 2015, the IAA F1 score was 67.63%.
tation2015by event types
Below are a few examples indicating disagreement
in event trigger, event typing and realis attributes: 5.2 Taggability
Trigger extentmismatch:
The annotation consistency can be attributed not
 She met the insurance investment magnate
only to disagreement in termsof event trigger, clas-
Shelby Cullom Davis on atrain(FP1: Move-
sification of event type and subtypes, realis attrib-
ment.TransportPerson) to (FP2: Move-
utes, but also misses and false alarms. Determining
ment.TransportPerson) Geneva in 1930.
whether or not an event is taggable (i.e., recall) has
always been a difficult issue in event annotation
tasks, and some event types and subtypes still ap-
41
egatnecrep
AAI
egatnecrep
AAI
pear to be more heavily affected than others. To bet- One main change that we made in 2015was the an-
ter understand the issue, we calculated the level of notation of contact events, aiming to support a wider
disagreement for event taggability by dividing the range of potential subtypes and also to improve an-
difference of event type-subtype counts in FP1 and notation consistency. The subtypes were automati-
FP2 by the event type-subtype count produced in the cally determined based on the annotation of attrib-
adjudication pass. Seventeen type-subtypes vary be- utes in contact events, rather thanbyhaving annota-
tween FP1 and FP2 at a percentage below 10%, in- tors make a direct decision about contact subtypes.
cluding Life.Divorce and Conflict.Attack, with There are 1479 event nuggets annotated with the
some type-subtypes showing a high level of disa- contact type, which is 23% of all event nuggets in
greement. As indicated in Figure 3, there are 12 the evaluationdata set. Contact.Broadcast and Con-
type-subtypes with 20% or higher disagreement be- tact.Contact are two of the most common subtypes.
tween FP1 and FP2, four of which are over 50%. The consistency of the Contact event type annota-
tion still poses challenges. As indicated in Figure4,
overall annotator consistency in both 2014 and 2015
improve when excluding the contact type from the
comparison.
The consistency of contact events alone is much
lower as compared with the other event types. This
could be attributed to the taggability question ofcer-
tain verbs. We have specified in the annotation
guidelines that speech verbs such as “said” and
“told” are taggable event triggers.We also specified
that when there were multiple speech verbs instan-
Figure3:Difference of Event Nugget Occurrence between
tiating the same contact event, only the first one is
FP1andFP2 over Adjudicated Occurrence.
taggable. However, annotators vary in implement-
ing this rule, as shown in the following example:
Below are a few examples indicating disagree-
 A spokesman for the Investigative Commit-
ment in terms of taggability:
tee, a branch of the prosecutor’s office, said
Miss:
 where shecreated(Business.StartOrg)the Davis (FP1: Contact.Broadcast) in an interview
(FP2: Contact.Broadcast) in Izvestia.
Museum and Cultural Center.
One positive thing we can see is that the changes
False Alarm:
 She also owned (Transaction.TransferOwner- in 2015 did improve the annotation consistency for
contact events in 2015, but there is still room for
ship)a home in Northeast Harbor, Maine
further improvement, especially in the consistency
5.3 Contact Event Type of subtype classification of theContact event type.
extent typing realis all 5.4 Double Tagging
100
80
Double tagging is quite common in the EN evalua-
60
tiondata set, due to the high frequency of Con-
40
flict.Attack/Life.Die,Transaction.TransferMoney
20
andTransaction.TransferOwnership events. Alto- 0
gether, 575 pairs out of 6440 event nuggets were
doubletagged (18%). Most of the double tagging
cases aretheresultof the same trigger instantiating
different event type-subtypes. Only 12 pairs of
double tagging cases aretheresult of event argu-
ment conjunction. Figure5shows the distribution
of event subtype counts involved in double tag-
Figure4:First pass inter-annotator agreement, including and ging.
excluding contact events.
42
egatnecrep
AAI
Double tagging was added to address the issue one for the performative action) requires further dis-
of inconsistency in event type categorization for cussion and clarification.
triggers that may instantiate two or more event
types and subtypes. Further analysis is needed to 7 Conclusion and Future Work
show whether allowing double tagging improves
In this paper, we described the annotation and eval-
annotation consistency.
uation of event nuggets and coreferencein the TAC
KBP evaluation. Annotated data has been distrib-
uted to DEFT and TAC KBP performers, and will
be made available to the wider community as part
of the Linguistic Data Consortium (LDC) catalog.
By analyzing the inter-annotator agreement be-
tween the two independent first pass annotations,
we learned that some event types and subtypes are
more difficult for annotators than others with re-
spect to recall. We also learned thatannotation con-
sistency for event triggers has improved with the
Figure 5:Distribution of event subtype of double tagging.
adoption ofaminimal extentrulefor event triggers
in 2015 as compared with the maximal semantic ex-
6 Annotation Challenges
tent unit rule in 2014’s EN annotation, but inference
One big challenge that annotators face is how to is still a big challenge. Additionally, the contact
handle inferred events. Even though the annotation event type still poses considerable difficulty. The
guidelines specifically instructannotators not to tag addition of contact event attributes improved anno-
any inferred events, whether an event mention is in- tation consistency in 2015 as compared with that of
ferred not only depends on interpretation of the con- 2014, but there is still room for improvement.
text, but also the meaning of the event triggers. Cou- The detection and coreference of event nuggets
pled with double tagging, this becomes a bigger is- provides an anchor for detecting event arguments
sue. For example, in “He then trafficked a large and event-event relations. EN evaluation in TAC
quatity of cocaine to the US.”, “trafficked” as de- KBP 2015 attracted participation from 17 institu-
fined as “buy or sell something illegally”is a trigger tions, and NIST will continue to run an open evalu-
of Transaction.TransferOwnership event, but with ation of EN as part of the event track in TAC KBP
the context of “to the US”, it also indicates that the 2016. The EN evaluation tasks will expand from
cocaine as been transported to the US. So it can also English to multilingual, including Chinese and
be a trigger for Movement.TransportArtifact event. Spanish. Due to the scarcity of training data for
One can argue thatthe Movement.TransportArtifact some event types and subtypes, the set of event
event is inferred, but it is still elusive to draw the types and subtypes for the evaluation in 2016 will
boundary of inference. be reduced from 33 in 2014 evaluation and 38 in
As mentioned above, Contact event type poses a 2015 evaluationto 18 event typesand subtypes.
lot of difficulty and inference is one of the factors
Acknowledgments
that contributes to this difficulty. The guidelines
specify that regular speech verbs such as “say”, This material is based on research sponsored by Air
“tell”, and “speak” would be triggers for contact
Force Research Laboratory and Defense Advanced
event types, but don’t specify the other categories of Research Projects Agency under agreement number
reporting verbs, such as “argue”, “advise”, “order”, FA8750-13-2-0045. The U.S. Government is au-
or“testify”, which involve more complicated inter- thorized to reproduce and distribute reprints for
pretation. Some reporting verbs have not only the Governmental purposes notwithstanding any copy-
speech aspects, but also performative aspects, e.g.,
right notation thereon. The views and conclusions
“testify”,and“threaten”. The question of whether to contained herein are those of the authors and should
double tagsuch verbs (one for the contact type and not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
43
implied, of Air Force Research Laboratory and De- TAC KBP 2014 Workshop, National Institute of Stand-
fense Advanced Research Projects Agency or the ards and Technology, Gaithersburg, Maryland, USA,
U.S. Government. November 17-18, 2014.
Linguistic Data Consortium. 2005. ACE (Automatic-
References Content Extraction) English Annotation Guidelines
for Events Version 5.4.3.
Jacqueline Aguilar, Charley Beller, Paul McNamee, Ben- Zhengzhong Liu, Teruko Mitamura, Eduard Hovy. 2015.
jamin Van Durme, Stephanie Strassel, Zhiyi Song, Joe Evaluation Algorithms for Event Nugget Detection: A
Ellis. 2014. A Comparison of the Events and Relations pilot Study. 3rd Workshop on EVENTS: Definition,
Across ACE, ERE, TAC-KBP, and FrameNet Annota- Detection, Coreference, and Representation, at the
tion Standards. ACL 2014: 52nd Annual Meeting of 2015 Conference of the North American Chapter of
the Association for Computational Linguistics, Balti- the Association for Computational Linguistics - Hu-
more, June 22-27. 2nd Workshop on Events: Defini- man Language Technologies (NAACL HLT 2015).
tion, Detection, Coreference, and Representation. Teruko Mitamura, Eduard Hovy. 2015. TAC KBP Event
Olga Babko-Malaya, Greg P. Milette, Michael K. Schnei- Detection and Coreference Tasks for English.
der, Sarah Scogin: Identifying Nuggets of Information http://cairo.lti.cs.cmu.edu/kbp/2015/event/Event_Me
in GALE Distillation Evaluation. In Proceedings of ntion_Detection_and_Coreference-2015-v1.1.pdf.
the Fourth International Conference on Language Re- Teruko Mitamura, Yukari Yamakawa, Sue Holm, Zhiyi
sources and Evaluation (LREC 2012), Istanbul, May Song, Ann Bies, Seth Kulick, Stephanie Strassel.
21-27. 2015. Event Nugget Annotation: Processes and Issues.
DARPA. 2012. Broad Agency Announcement: Deep Ex- 3rd Workshop on EVENTS: Definition, Detection, Co-
ploration and Filtering of Text (DEFT). Defense Ad- reference, and Representation, at the 2015 Conference
vanced Research Projects Agency, DARPA-BAA-12- of the North American Chapter of the Association for
47. Computational Linguistics - Human Language Tech-
George Doddington, Alexis Mitchell, Mark Przbocki, nologies (NAACL HLT 2015).
Lance Ramshaw, Stephanie Strassel, and Ralph AniNenkovaandRebeccaPassonneau. 2004. Evaluating
Weischedel. 2004. The Automatic Content Extraction content selection in summarization: the pyramid
(ACE) program–tasks, data, and evaluation. InPro- method. In Proceedings of the Human Language Tech-
ceedings of the Fourth International Conference on nology Conference – North American chapter of the
Language Resources and Evaluation (LREC 2004), Association for Computational Linguistics annual
Lisbon, May 24-30. meeting (NAACL-HLT 2004).
JoeEllis, Jeremy Getman, Dana Fore, Neil Kuster, Zhiyi Ani Nenkova,RebeccaPassonneau, andKathleenMcKe-
Song, Ann Bies, Stephanie Strassel. 2015. Overview own. 2007. The pyramid method: Incorporating hu-
of Linguistic Resources for the TAC KBP 2015 Eval- man content selection variation in summarization
uations: Methodologies and Results. InProceedings of evaluation. ACM Trans. Speech Lang. Process.,
TAC KBP 2015 Workshop, National Institute of Stand- 4(2):4.
ards and Technology, Gaithersburg, Maryland, USA, Zhiyi Song, Ann Bies, Tom Riese, Justin Mott, Jonathan
November 16-17, 2015. Wright, Seth Kulick, Neville Ryant, Stephanie Stras-
Marjorie Freedman. 2015. TAC: Event Argument and sel, Xiaoyi Ma. 2015. From Light to Rich ERE: Anno-
Linking Evaluation task Description. tation of Entities, Relations, and Events.3rd Workshop
http://www.nist.gov/tac/2015/KBP/Event/Argu- on EVENTS: Definition, Detection, Coreference, and
ment/guidlines/EventArgumentAndLinkingTask- Representation, at the 2015 Conference of the North
Description.v09.pdf American Chapter of the Association for Computa-
Marjorie Freedman and Ryan Gabbard. 2014. Overview tional Linguistics - Human Language Technologies
of the Event Argument Evaluation. InProceedings of (NAACL HLT 2015).
44
Appendix
evaluation event subtype count
600
500
400
300
200
100
0
Event subtype distribution in Event Nugget 2015 Evaluationdata
45
kcatta tcatnoc yenoMrefsnart tsacdaorb nosrePtropsnart eid liaJtserra noitisoPdne teem pihsrenwOrefsnart ecnetnes gniraeHlairt tcidnIegrahc etartsnomed eloraPesaeler ecnednopserroc etucexe tcivnoc noitisoPtrats tcafitra erujni yrram eus tcele laeppa tcafitrAtropsnart noitcasnart etanimon etidartxe nodrap ecrovid enif yctpurknaBeralced grOtrats grOegrem tiuqca nroBeb grOdne

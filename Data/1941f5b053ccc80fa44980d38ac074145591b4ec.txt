A Bilingual Generative Transformer for Semantic Sentence Embedding
JohnWieting1,GrahamNeubig1,andTaylorBerg-Kirkpatrick2
1CarnegieMellonUniversity,Pittsburgh,PA,15213,USA
2UniversityofCaliforniaSanDiego,SanDiego,CA,92093,USA
{jwieting,gneubig}@cs.cmu.edu,tberg@eng.ucsd.edu
Abstract deepneuralarchitectureshavebeenusedtolearn
contextualized word embeddings (Peters et al.,
Semanticsentenceembeddingmodelsencode
2018; Devlin et al., 2018) enabling state-of-the-
natural language sentences into vectors, such
art results on many tasks. We focus on learning
that closeness in embedding space indicates
closeness in the semantics between the sen- semanticsentenceembeddingsinthispaper,which
tences.Bilingualdataoffersausefulsignalfor play an important role in many downstream ap-
learning such embeddings: properties shared plications. Sincetheydonotrequireanylabelled
by both sentences in a translation pair are dataforfine-tuning,sentenceembeddingsareuse-
likelysemantic,whiledivergentpropertiesare
fulout-of-the-box forproblems suchasmeasure-
likely stylistic or language-specific. We pro-
mentofSemanticTextualSimilarity(STS;Agirre
poseadeeplatentvariablemodelthatattempts
etal.(2012)),miningbitext(Zweigenbaumetal.,
to perform source separation on parallel sen-
2018),andparaphraseidentification(Dolanetal.,
tences,isolatingwhattheyhaveincommonin
a latent semantic vector, and explaining what 2004). Semantic similarity measures also have
is left over with language-specific latent vec- downstreamusessuchasfine-tuningmachinetrans-
tors. Ourproposedapproachdiffersfrompast lationsystems(Wietingetal.,2019a).
work on semantic sentence encoding in two
There are three main ingredients when design-
ways. First, by using a variational probabilis-
ingasentenceembeddingmodel: thearchitecture,
ticframework,weintroducepriorsthatencour-
thetrainingdata,andtheobjectivefunction. Many
agesourceseparation,andcanuseourmodel‚Äôs
architectures including LSTMs (Hill et al., 2016;
posterior to predict sentence embeddings for
monolingualdataattesttime. Second,weuse Conneauetal.,2017;SchwenkandDouze,2017;
high-capacity transformers as both data gen- Subramanianetal.,2018),Transformers(Ceretal.,
erating distributions and inference networks ‚Äì 2018;ReimersandGurevych,2019),andaverag-
contrasting with most past work on sentence ing models (Wieting et al., 2016b; Arora et al.,
embeddings. In experiments, our approach
2017)arecapableoflearningsentenceembeddings.
substantially outperforms the state-of-the-art
Thechoiceoftrainingdataandobjectiveareinti-
on a standard suite of unsupervised seman-
matelyintertwined,andthereareawidevarietyof
tic similarity evaluations. Further, we demon-
stratethatourapproachyieldsthelargestgains optionsincludingnext-sentenceprediction(Kiros
on more difficult subsets of these evaluations et al., 2015), machine translation (Espana-Bonet
wheresimplewordoverlapisnotagoodindi- etal.,2017;SchwenkandDouze,2017;Schwenk,
catorofsimilarity.1
2018; Artetxe and Schwenk, 2018), natural lan-
guageinference(NLI) (Conneauetal.,2017),and
1 Introduction
multi-task objectives which include some of the
Learning useful representations of language has previouslymentionedobjectives(Ceretal.,2018)
beenasourceofrecentsuccessinnaturallanguage potentiallycombinedwithadditionaltaskslikecon-
processing (NLP). Much work has been done on stituencyparsing(Subramanianetal.,2018).
learningrepresentationsforwords(Mikolovetal., Surprisingly,despiteampletestingofmorepow-
2013;Penningtonetal.,2014)andsentences(Kiros erfularchitectures,thebestperformingmodelsfor
etal.,2015;Conneauetal.,2017). Morerecently, manysentenceembeddingtasksrelatedtoseman-
tic similarity often use simple architectures that
1Codeanddatatoreplicateresultsavailableathttps:
//www.cs.cmu.edu/Àújwieting. are mostly agnostic to the interactions between
words. For instance, some of the top performing Gaussian Priors
ùí©(0,ùêº) ùí©(0,ùêº) ùí©(0,ùêº)
techniquesusewordembeddingaveraging(Wiet-
ingetal.,2016b),charactern-grams(Wietingetal.,
French-specific Common Semantic English-specific
2016a),andsubwordembeddingaveraging(Wiet- Latent Vector Latent Vector Latent Vector
ingetal.,2019b)tocreaterepresentations. These L si pn eg cu ii fs icti c to v Far ri ea nt cio hn z fr z sem vs aS em rh ia aar tne it od i nc z en L si pn eg cu ii fs icti c to v Ear ni ga lt ii so hn
simpleapproachesarecompetitivewithmuchmore
complicated architectures on in-domain data and
French Semantic English
Inference Net Inference Net Inference Net
generalizewelltounseendomains,butarefunda- (Section 3) (Section 3) (Section 3)
French English
Decoder Decoder
mentallylimitedbytheirinabilitytocaptureword (Section 2) (Section 2)
order. Training these approaches generally relies
ondiscriminativeobjectivesdefinedonparaphrase x fr x en
Observed Observed
data(Ganitkevitchetal.,2013;WietingandGim- French Sentence English Sentence
Translation Pair
pel,2018)orbilingualdata(Wietingetal.,2019b;
Figure1: Thegenerativeprocessofourmodel. Latent
Chidambarametal.,2019;Yangetal.,2020). The
variables modeling the linguistic variation in French
inclusion of latent variables in these models has
and English, z and z , as well as a latent vari-
alsobeenexplored(Chenetal.,2019). fr en
ablemodelingthecommonsemantics,z ,aredrawn
sem
Intuitively,bilingualdatainparticularispromis- from a multivariate Gaussian prior. The observed text
in each language is then conditioned on its language-
ing because it potentially offers a useful signal
specificvariableandz .
forlearningtheunderlyingsemanticsofsentences. sem
Withinatranslationpair,propertiessharedbyboth
sentencesaremorelikelysemantic,whilethosethat
aredivergentaremorelikelystylisticorlanguage-
isnotabletoaccuratelyassesssemanticsimilarity.
specific. Whilepreviousworklearningfrombilin-
Onthesemostdifficultinstances,wefindthatour
gualdataperhapstakesadvantageofthisfactim-
approach yields the largest gains, indicating that
plicitly,thefocusofthispaperismodellingthisin-
oursystemismodelinginteractionsbetweenwords
tuitionexplicitly,andtothebestofourknowledge,
togoodeffect. Wealsofindthatourmodelbetter
thishasnotbeenexploredinpriorwork. Specifi-
handlescross-lingualsemanticsimilaritythanmul-
cally,weproposeadeepgenerativemodelthatis
tilingualtranslationbaselineapproaches,indicating
encouragedtoperformsourceseparationonparal-
thatstrippingawaylanguage-specificinformation
lelsentences,isolatingwhattheyhaveincommon
allows for better comparisons between sentences
inalatentsemanticembeddingandexplainingwhat
fromdifferentlanguages.
is left over with language-specific latent vectors.
Attesttime,weuseinferencenetworks(Kingma
Finally,weanalyzeourmodeltouncoverwhat
andWelling,2013)forapproximatingthemodel‚Äôs
informationwascapturedbythesourceseparation
posterioronthesemanticandsource-separatedla-
intothesemanticandlanguage-specificvariables
tent variables to encode monolingual sentences.
and the relationship between this encoded infor-
Finally,sinceourmodelandtrainingobjectiveare
mationandlanguagedistancetoEnglish. Wefind
generative, our approach does not require knowl-
thatthelanguage-specificvariablestendtoexplain
edgeofthedistancemetricstobeusedduringeval-
more superficial or language-specific properties
uation,andithastheadditionalpropertyofbeing
suchasoverallsentencelength,amountandloca-
abletogeneratetext.
tion of punctuation, and the gender of articles (if
In experiments, we evaluate our probabilistic genderispresentinthelanguage),butsemanticand
source-separationapproachonastandardsuiteof syntacticinformationismoreconcentratedinthe
STSevaluations. Wedemonstratethattheproposed sharedsemanticvariables,matchingourintuition.
approach is effective, most notably allowing the Languagedistancehasaneffectaswell,wherelan-
learningofhigh-capacitydeepTransformerarchi- guagesthatsharecommonstructureswithEnglish
tectures(Vaswanietal.,2017)whilestillgeneral- putmoreinformationintothesemanticvariables,
izingtonewdomains,significantlyoutperforming while more distant languages put more informa-
avarietyofstate-of-the-artbaselines. Further,we tion into the language-specific variables. Lastly,
conductathoroughanalysisbyidentifyingsubsets we show outputs generated from our model that
oftheSTSevaluationwheresimplewordoverlap exhibititsabilitytodoatypeofstyletransfer.
Inference Networks
English Inference Net
q(zen|xen;œï) Œºen,Œ£en
English Latent Space Generative Model
Encoder KL between prior and
Xen q(zseS me |m xa enn ,ti xc f rI ;nf œïer )enc oe M p N ee rre agt ti in og n ESe nm coa dna itp nic gp r oximate ùí© po (s Œºte er ni ,o Œ£rs en) p co(x ne cn a| tzsem,zen DE; enŒ∏ cg) oli ds eh r Xen
Semantic (Section 3) zsem
EnglishI n ap nu dt F rench Encoder mergeŒºsem,Œ£sem laC teo nn t c va et ce tn oa rste a s na dm dp el ce od d e Rec oo fn is ntr pu uc tt ion
translation pair S Ee nm coan dt ei rc ùí©(Œºsem,Œ£sem) p(xfr|zsem,zfr;Œ∏) translation pair
Xfr French Inference Net ùí©(Œºfr,Œ£fr) concat DF er ce on dch er Xfr
q(zfr|xfr;œï)
Œºfr,Œ£fr
French
Encoder
Figure 2: The computation graph for the variational lower bound used during training. The English and French
textarefedintotheirrespectiveinferencenetworksandthesemanticinferencenetworktoultimatelyproducethe
languagevariablesz andz andsemanticvariablez . Eachlanguage-specificvariableisthenconcatenated
fr en sem
toz andusedbythedecodertoreconstructtheinputsentencepair.
sem
2 Model wefurtherdiscusshowthisisexplicitlyencouraged
bythelearningprocess.
Our proposed training objective leverages a gen-
DecoderArchitecture. Manylatentvariablemod-
erative model of parallel text in two languages.
els for text use LSTMs (Hochreiter and Schmid-
Our model is language agnostic, and applies to
huber,1997)astheirdecoders(Yangetal.,2017;
awidevarietyoflanguages(seeSection5)butwe
willusetherunningexampleofEnglish(en)and ZieglerandRush,2019;Maetal.,2019). However,
French(fr)pairsconsistingofanEnglishsentence state-of-the-artmodelsinneuralmachinetransla-
tionhaveseenincreasedperformanceandspeedus-
x and a French sentence x . Importantly, the
en fr
ingdeepTransformerarchitectures. Wealsofound
generativeprocessutilizesthreeunderlyinglatent
in our experiments (see Appendix C for details)
vectors: language-specificvariationvariables(lan-
thatTransformersledtoincreasedperformancein
guage variables) z and z for each side of the
fr en
oursetting,sotheyareusedinourmainmodel.
translation,aswellasasharedsemanticvariation
variable (semantic variable) z . In this section We use two decoders in our model, one for
sem
wewillfirstdescribethegenerativemodelforthe modellingp(x |z ,z ;Œ∏)andoneformodel-
fr sem fr
textandlatentvariables. Inthefollowingsection ingp(x |z ,z ;Œ∏)(seerightsideofFigure2).
en sem en
we will describe the inference procedure of z Eachdecodertakesinalanguagevariableandase-
sem
givenaninputsentence,whichcorrespondstoour manticvariable,whichareconcatenatedandused
coretaskofobtainingsentenceembeddingsuseful bythedecoderforreconstruction. Weexplorefour
fordownstreamtaskssuchassemanticsimilarity. waysofusingthislatentvector: (1)Concatenateit
Thegenerativeprocessofourmodel,theBilin- tothewordembeddings(Word)(2)Useitastheini-
gual Generative Transformer (BGT), is depicted tialhiddenstate(Hidden,LSTMonly)(3)Useitas
inFigure1andthetrainingcomputationgraphis youwouldtheattentioncontextvectorinthetradi-
showninFigure2. First,wesamplelatentvariables tionalsequence-to-sequenceframework(Attention)
(cid:104)z ,z ,z (cid:105),wherez ‚àà Rk,fromamultivari- and(4)Concatenateittothehiddenstateimmedi-
fr en sem i
ate Gaussian prior N(0,I ). These variables are atelypriortocomputingthelogits(Logit). Unlike
k
thenfedintoadecoderthatsamplessentences;x Attention,thereisnoadditionalfeedforwardlayer
en
issampledconditionedonz andz ,whilex in this setting. We experimented with these four
sem en fr
issampledconditionedonz andz . Because approaches,aswellascombinationsthereof,and
sem fr
sentencesinbothlanguageswillusez ingener- report this analysis in Appendix A. From these
sem
ation,weexpectthatinawell-trainedmodelthis experiments, we see that the closer the sentence
variablewillencodesemantic,syntactic,orstylis- embeddingistothefinalwordpredictor,thebetter
ticinformationsharedacrossbothsentences,while the performance on downstream tasks evaluating
z andz willhandleanylanguage-specificpecu- its semantic content. We hypothesise that this is
fr en
liaritiesorspecificstylisticdecisionsthatareless duetobettergradientpropagationbecausethesen-
central to the sentence meaning and thus do not tenceembeddingisnowclosertotheerrorsignal.
translateacrosssentences. Inthefollowingsection, SinceAttentionandLogitperformedbest,weuse
theseinourmainexperiments. Encoder Architecture. We use three inference
networksasshownontheleftsideofFigure2: an
3 LearningandInference
EnglishinferencenetworktoproducetheEnglish
Our model is trained on a set of paral- languagevariable,aFrenchinferencenetworkto
lel sentences X consisting of N examples, produce the French language variable, and a se-
X = {(cid:104)x1 ,x1 (cid:105),...,(cid:104)xN,xN(cid:105)}, and Z manticinferencenetworktoproducethesemantic
en fr en fr
is our collection of latent variables Z = variable. Justasinthedecoderarchitecture,weuse
((cid:104)z1 ,z1 ,z1 (cid:105),...,(cid:104)zN,zN,zN (cid:105)). We wish aTransformerfortheencoders.
en fr sem en fr sem
to maximize the likelihood of the parameters of The semantic inference network is a bilingual
thetwodecodersŒ∏ withrespecttotheobservedX, encoderthatencodeseachlanguage. Foreachtrans-
marginalizingoverthelatentvariablesZ. lationpair,wealternatewhichofthetwoparallel
(cid:90) sentencesisfedintothesemanticencoderwithin
p(X;Œ∏) = p(X,Z;Œ∏)dZ a batch. Since the semantic encoder is meant to
Z capture language agnostic semantic information,
Unfortunately,thisintegralisintractabledueto itsoutputsforatranslationpairshouldbesimilar
thecomplexrelationshipbetweenX andZ. How- regardless of the language of the input sentence.
ever,relatedlatentvariablemodelslikevariational Wenotethatotheroperationsarepossibleforcom-
autoencoders(VAEs;KingmaandWelling(2013)) biningtheviewseachparallelsentenceoffers. For
learnbyoptimizingavariationallowerboundon instance, we could feed both sentences into the
the log marginal likelihood. This surrogate ob- semantic encoder and pool their representations.
jectiveiscalledtheevidencelowerbound(ELBO) However,inpracticewefindthatalternatingworks
andintroducesavariationalapproximation,qtothe well and also can be used to obtain sentence em-
trueposteriorofthemodelp. Theq distributionis beddings for text that is not part of a translation
parameterizedbyaneuralnetworkwithparameters pair. Weleavefurtherstudyofcombiningviewsto
œÜ. ELBOforourmodeliswrittenas: futurework.
ELBO =E q(Z|X;œÜ)[logp(X|Z;Œ∏)]‚àí 4 Experiments
KL(q(Z|X;œÜ)||p(Z;Œ∏))
4.1 BaselineModels
This lower bound on the marginal can be opti- Weexperimentwithfourteenbaselinemodels,cov-
mizedbygradientascentbyusingthereparameteri- eringboththemosteffectiveapproachesforlearn-
zationtrick(KingmaandWelling,2013). Thistrick ing sentence embeddings from the literature and
allows for the expectation under q to be approxi- ablationsofourownBGTmodel. Thesebaselines
mated through sampling in a way that preserves canbesplitintothreegroupsasdetailedbelow.
backpropagation. Wemakeseveralindependence
ModelsfromtheLiterature(TrainedonDiffer-
assumptions for q(z ,z ,z |x ,x ;œÜ) to
sem en fr en fr
entData) Wecomparetowellknownsentence
match our goal of source separation: we factor q
embeddingmodelsInfersent(Conneauetal.,2017),
as q(z |x ,x ;œÜ)q(z |x ;œÜ)q(z |x ;œÜ).
sem en fr en en fr fr
GenSen (Subramanian et al., 2018), the Univer-
The parameters of the encoders that make up the
sal Sentence Encoder (USE) (Cer et al., 2018),
inferencenetworks,definedinthenextparagraph,
LASER(ArtetxeandSchwenk,2018),aswellas
aredenotedasœÜ.
BERT (Devlin et al., 2018).2 We used the pre-
Lastly, we note that the KL term in our ELBO
trainedBERTmodelintwowaystocreateasen-
equationexplicitlyencouragesexplainingvariation
tenceembedding. Thefirstwayistoconcatenate
that is shared by translations with the shared se-
thehiddenstatesfortheCLStokeninthelastfour
manticvariable,andexplaininglanguage-specific
layers. Thesecondwayistoconcatenatethehid-
variationwiththecorrespondinglanguage-specific
denstatesofallwordtokensinthelastfourlayers
variables. Encodinginformationsharedbythetwo
andmeanpooltheserepresentations. Bothmethods
sentences in the shared variable results in only a
result in a 4096 dimension embedding. We also
single penalty from the KL loss, while encoding
compare to the newly released model, Sentence-
the information separately in both language spe-
cificvariableswillcauseunnecessaryreplication,
2Note that in all experiments using BERT, including
doublingtheoverallcostincurredbytheKLterm. Sentence-BERT,thelarge,uncasedversionisused.
Bert (Reimers and Gurevych, 2019). This model conventionaltranslationbaselines.
is similar to Infersent (Conneau et al., 2017) in
thatitistrainedonnaturallanguageinferencedata,
‚Ä¢ ENGLISHAE: English autoencoder on the En-
glishsideofouren-frdata.
SNLI(Bowmanetal.,2015). However,insteadof
usingpretrainedwordembeddings,theyfine-tune ‚Ä¢ ENGLISHVAE:Englishvariationalautoencoder
BERTinawaytoinducesentenceembeddings.3 ontheEnglishsideofouren-frdata.
‚Ä¢ ENGLISHTRANS: Translationfromentofr.
Models from the Literature (Trained on Our
Data) These models are amenable to being ‚Ä¢ BILINGUALTRANS: Translationfrombothen
trainedintheexactsamesettingasourownmodels tofrandfrtoenwheretheencodingparam-
as they only require parallel text. These include eters are shared but each language has its own
thesentencepieceaveragingmodel,SP,fromWi- decoder.
eting et al. (2019b), which is among the best of ‚Ä¢ BGT W/O LANGVARS: A model similar to
the averaging models (i.e. compared to averag- BILINGUALTRANS,butitincludesapriorover
ingonlywords orcharactern-grams)aswellthe the embedding space and therefore a KL loss
LSTMmodel,BILSTM,fromWietingandGimpel term. This model differs from BGT since it
(2017). Thesemodelsuseacontrastivelosswitha doesnothaveanylanguage-specificvariables.
margin. Followingtheirsettings,wefixthemargin
‚Ä¢ BGTW/OPRIOR: Followsthesamearchitecture
to0.4andtunethenumberofbatchestopoolforse-
asBGT,butwithoutthepriorsandKLlossterm.
lecting negative examples from {40,60,80,100}.
Forbothmodels,wesetthedimensionoftheem- 4.2 ExperimentalSettings
beddingsto1024. For BILSTM,wetrainasingle
The training data for our models is a mixture of
layerbidirectionalLSTMwithhiddenstatesof512
OpenSubtitles20184 en-frdataanden-frGiga-
dimensions. Tocreatethesentenceembedding,the
word5 data. To create our dataset, we combined
forward and backward hidden states are concate-
thecompletecorporaofeachdatasetandthenran-
nated and mean-pooled. Following Wieting and
domlyselected1,000,000sentencepairstobeused
Gimpel(2017),weshuffletheinputswithprobabil-
fortrainingwith10,000usedforvalidation. Weuse
ityp,tuningpfrom{0.3,0.5}.
sentencepiece(KudoandRichardson,2018)
Wealsoimplicitlycomparetopreviousmachine
with a vocabulary size of 20,000 to segment the
translation approaches like Espana-Bonet et al.
sentences,andwechosesentencepairswhosesen-
(2017);SchwenkandDouze(2017);Artetxeand
tencesarebetween5and100tokenseach.
Schwenk(2018)inAppendixAwhereweexplore
Indesigningthemodelarchitecturesfortheen-
different variations of training LSTM sequence-
codersanddecoders,weexperimentedwithTrans-
to-sequence models. We find that our transla-
formers and LSTMs. Due to better performance,
tion baselines reported in the tables below (both
we use a 5 layer Transformer for each of the en-
LSTMandTransformer)outperformthearchitec-
coders and a single layer decoder for each of the
turesfromtheseworksduetousingtheAttention
decoders. This design decision was empirically
andLogitmethodsmentionedinSection2,demon-
motivatedaswefoundusingalargerdecoderwas
stratingthatourbaselinesrepresent,orevenover-
slowerandworsenedperformance,butconversely,
represent,thestate-of-the-artformachinetransla-
addingmoreencoderlayersimprovedperformance.
tionapproaches.
Morediscussionofthesetrade-offsalongwithab-
BGTAblations Lastly,wecomparetoablations lationsandcomparisonstoLSTMsareincludedin
of our model to better understand the benefits of AppendixC.
parallel data, language-specific variables, the KL For all of our models, we set the dimension of
loss term, and how much we gain from the more theembeddingsandhiddenstatesfortheencoders
anddecodersto1024. Sinceweexperimentwith
3MostworkevaluatingaccuracyonSTStaskshasaveraged twodifferentarchitectures,6 wefollowtwodiffer-
thePearson‚Äôsrovereachindividualdatasetforeachyearof
ent optimization strategies. For training models
theSTScompetition.However,ReimersandGurevych(2019)
computedSpearman‚ÄôsœÅoverconcatenateddatasetsforeach
yearoftheSTScompetition.Tobeconsistentwithprevious
4http://opus.nlpl.eu/OpenSubtitles.php
work,were-rantheirmodelandcalculatedresultsusingthe 5https://www.statmt.org/wmt10/
standardmethod,andthusourresultsarenotthesameasthose training-giga-fren.tar
reportedReimersandGurevych(2019). 6WeuseLSTMsinourablations.
Data Sentence1 Sentence2 GoldScore
Hard+ Otherwaysareneeded. Itisnecessarytofindothermeans. 4.5
Hard- Howlongcanyoukeepchocolatein HowlongcanIkeepbreaddoughin 1.0
thefreezer? therefrigerator?
Negation It‚Äôsnotagoodidea. It‚Äôsagoodideatodoboth. 1.0
Table 1: Examples from our Hard STS dataset and our negation split. The sentence pair in the first row has
dissimilar structure and vocabulary yet a high gold score. The second sentence pair has similar structure and
vocabularyandalowgoldscore. Thelastsentencepaircontainsnegation,wherethereisanotinSentence1that
causesotherwisesimilarsentencestohavelowsemanticsimilarity.
withTransformers,weuseAdam(KingmaandBa, isPearson‚Äôsr withthegoldlabels.
2014) with Œ≤ = 0.9, Œ≤ = 0.98, and (cid:15) = 10‚àí8. Secondly,weevaluateonHardSTS,wherewe
1 2
WeusethesamelearningratescheduleasVaswani combine and filter the STS datasets in order to
et al. (2017), i.e., the learning rate increases lin- makeamoredifficultevaluation. Wehypothesize
earlyfor4,000stepsto5√ó10‚àí4,afterwhichitis thatthesedatasetscontainmanyexampleswhere
decayedproportionallytotheinversesquareroot theirgoldscoresareeasytopredictbyeitherhav-
of the number of steps. For training the LSTM ing similar structure and word choice and a high
models,weuseAdamwithafixedlearningrateof scoreordissimilarstructureandwordchoiceanda
0.001. Wetrainourmodelsfor20epochs. lowscore. Therefore,wesplitthedatausingsym-
Formodelsincorporatingatranslationloss,we metricworderrorrate(SWER),7 findingsentence
usedlabelsmoothedcrossentropy(Szegedyetal., pairswithlowSWERandlowgoldscoresaswell
2016;Pereyraetal.,2017)with(cid:15) = 0.1. For EN- assentencepairswithhighSWERandhighgold
GLISHVAE, BGTand BILINGUALTRANS,wean- scores. Thisresultsintwodatasets,Hard+which
nealtheKLtermsothatitincreasedlinearlyfor216 haveSWERs in the bottom20% of all STS pairs
updates, which robustly gave good results in pre- and whose gold label is between 0 and 1,8 and
liminaryexperiments. Wealsofoundthatintrain- Hard-wheretheSWERsareinthetop20%ofthe
ingBGT,combiningitslosswiththe BILINGUAL- goldscoresarebetween4and5. Wealsoevaluate
TRANSobjectiveduringtrainingofbothmodelsin- onasplitwherenegationwaslikelypresentinthe
creasedperformance,andsothislosswassummed example.9 ExamplesareshowninTable1.
with the BGT loss in all of our experiments. We Lastly,weevaluateonSTSinesandaraswell
notethatthisdoesnotaffectourclaimofBGTbe- as cross-lingual evaluations for en-es, en-ar,
ing a generative model, as this loss is only used and en-tr. We use the datasets from SemEval
in a multi-task objective at training time, and we 2017 (Cer et al., 2017). For this setting, we train
calculatethegenerationprobabilitiesaccordingto BILINGUALTRANS and BGT on1millionexam-
standardBGTattesttime. plesfromen-es,en-ar,anden-trOpenSub-
Lastly,inAppendixB,weillustratethatitiscru- titles2018data.
cialtotraintheTransformerswithlargebatchsizes.
4.4 Results
Without this, the model can learn the goal task
(suchastranslation)withreasonableaccuracy,but TheresultsontheSTSandHardSTSareshownin
thelearnedsemanticembeddingsareofpoorqual- Table3.10 Fromtheresults,weseethatBGT has
ity until batch sizes approximately reach 25,000 thehighestoverallperformance. Itdoesespecially
tokens. Therefore,weuseamaximumbatchsize wellcomparedtopriorworkonthetwoHardSTS
of50,000tokensinour ENGLISHTRANS, BILIN- datasets. Weusedpairedbootstrapresamplingto
GUALTRANS,andBGT W/O PRIOR,experiments checkwhetherBGTsignificantlyoutperformsSen-
and 25,000 tokens in our BGT W/O LANGVARS tenceBert, SP, BILINGUALTRANS,and BGT W/O
and BGT experiments. PRIOR ontheSTStask. Wefoundallgainstobe
7Wedefinesymmetricworderrorrateforsentencess and
4.3 Evaluation 1
s as 1WER(s ,s ) + 1WER(s ,s ), since word error
2 2 1 2 2 2 1
rate(WER)isanasymmetricmeasure.
Our primary evaluation are the 2012-2016 Se-
8STSscoresarebetween0and5.
mEvalSemanticTextualSimilarity(STS)shared
9Weselectedexamplesforthenegationsplitwhereone
tasks(Agirreetal.,2012,2013,2014,2015,2016), sentencecontainednotor‚Äôtandtheotherdidnot.
wherethegoalistoaccuratelypredictthedegree 10WeobtainedvaluesforSTS2012-2016frompriorworks
using SentEval (Conneau and Kiela, 2018). Note that we
towhichtwosentenceshavethesamemeaningas
includealldatasetsforthe2013competition,includingSMT,
measuredbyhumanjudges. Theevaluationmetric whichisnotincludedinSentEval.
SemanticTextualSimilarity(STS) Figure3: Results of our mod-
Model
2012 2013 2014 2015 2016 Avg. Hard+ Hard- Avg. els and models from prior work.
BERT(CLS) 33.2 29.6 34.3 45.1 48.4 38.1 7.8 12.5 10.2
The first six rows are pretrained
BERT(Mean) 48.8 46.5 54.0 59.2 63.4 54.4 3.1 24.1 13.6
models from the literature, the
Infersent 61.1 51.4 68.1 70.9 70.7 64.4 4.2 29.6 16.9
GenSen 60.7 50.8 64.1 73.3 66.0 63.0 24.2 6.3 15.3 next two rows are strong base-
USE 61.4 59.0 70.6 74.3 73.9 67.8 16.4 28.1 22.3 linestrainedonthesamedataas
LASER 63.1 47.0 67.7 74.9 71.9 64.9 18.1 23.8 20.9 our models, and the last seven
Sentence-BERT 66.9 63.2 74.2 77.3 72.8 70.9 23.9 3.6 13.8
rows include model ablations
SP 68.4 60.3 75.1 78.7 76.8 71.9 19.1 29.8 24.5
and BGT, our final model. We
BILSTM 67.9 56.4 74.5 78.2 75.9 70.6 18.5 23.2 20.9
ENGLISHAE 60.2 52.7 68.6 74.0 73.2 65.7 15.7 36.0 25.9 show results, measured in Pear-
ENGLISHVAE 59.5 54.0 67.3 74.6 74.1 65.9 16.8 42.7 29.8 son‚Äôs r √ó 100, for each year
ENGLISHTRANS 66.5 60.7 72.9 78.1 78.3 71.3 18.0 47.2 32.6 oftheSTStasks2012-2016and
BILINGUALTRANS 67.1 61.0 73.3 78.0 77.8 71.4 20.0 48.2 34.1 ourtwoHardSTSdatasets.
BGTW/OLANGVARS 68.3 61.3 74.5 79.0 78.5 72.3 24.1 46.8 35.5
BGTW/OPRIOR 67.6 59.8 74.1 78.4 77.9 71.6 17.9 45.5 31.7
BGT 68.9 62.2 75.9 79.4 79.3 73.1 22.5 46.6 34.6
Model es-es ar-ar en-es en-ar en-tr Figure4: Performance measured in
LASER 79.7 69.3 59.7 65.5 72.0 Pearson‚Äôs r √ó 100, on the Se-
BILINGUALTRANS 83.4 72.6 64.1 37.6 59.1 mEval2017STStaskonthees-es,
BGTW/OLANGVARS 81.7 72.8 72.6 73.4 74.8
ar-ar,en-es,en-ar,anden-tr
BGTW/OPRIOR 84.5 73.2 68.0 66.5 70.9
BGT 85.7 74.9 75.6 73.5 74.9 datasets.
significantwithp < 0.01. 11 hasthebestperformanceacrossalldatasets,how-
Fromtheseresults,weseethatbothpositiveex- evertheperformanceissignificantlystrongerthan
amplesthathavelittlesharedvocabularyandstruc- theBILINGUALTRANSandBGTW/OPRIORbase-
tureandnegativeexampleswithsignificantshared linesinthecross-lingualsetting. Since BGT W/O
vocabularyandstructurebenefitsignificantlyfrom LANGVARS also has significantly better perfor-
using a deeper architecture. Similarly, examples manceonthesetasks,mostofthisgainseemstobe
wherenegationoccursalsobenefitfromourdeeper duetothepriorhavingaregularizingeffect. How-
model. Theseexamplesaredifficultbecausemore ever, BGT outperforms BGT W/O LANGVARS
thanjusttheidentityofthewordsisneededtode- overall,andwehypothesizethatthegapinperfor-
terminetherelationshipofthetwosentences,and mance between these two models is due to BGT
thisissomethingthat SP isnotequippedforsince being able to strip away the language-specific in-
itisunabletomodelwordorder. Thebottomtwo formationintherepresentationswithitslanguage-
rowsshoweasierexampleswherepositiveexam- specificvariables,allowingforthesemanticsofthe
ples have high overlap and low SWER and vice sentencestobemoredirectlycompared.
versafornegativeexamples. Bothmodelsperform
similarly on this data, with the BGT model hav-
5 Analysis
ing a small edge consistent with the overall gap
betweenthesetwomodels.
We next analyze our BGT model by examining
Lastly, in Table 4, we show the results of STS
what elements of syntax and semantics the lan-
evaluationsinesandarandcross-lingualevalua-
guageandsemanticvariablescapturerelativeboth
tionsforen-es,en-ar,anden-tr. Wealsoin-
toeach-otherandtothesentenceembeddingsfrom
cludeacomparisontoLASER,whichisamultilin-
the BILINGUALTRANS models. We also analyze
gualmodel.12 Fromtheseresults,weseethatBGT
howthechoiceoflanguageanditslexicalandsyn-
tactic distance from English affects the semantic
11WeshowfurtherdifficultsplitsinAppendixD,including
andsyntacticinformationcapturedbythesemantic
anegationsplit,beyondthoseusedinHardSTSandcompare
thetoptwoperformingmodelsintheSTStaskfromTable3. and language-specific encoders. Finally, we also
We also show easier splits of the data to illustrate that the showthatourmodeliscapableofsentencegener-
differencebetweenthesemodelsissmalleronthesesplits.
12Wenotethatthisisnotafaircomparisonforavariety
ationinatypeofstyletransfer,demonstratingits
ofreasons. Forinstance,ourmodelsarejusttrainedontwo capabilitiesasagenerativemodel.
languagesatatime,butareonlytrainedon1Mtranslation
pairsfromOpenSubtitles.LASER,inturn,istrainedon223M
translationpairscoveringmoredomains,butisalsotrainedon 93languagessimultaneously.
SemanticTextualSimilarity(STS)
Model
2012 2013 2014 2015 2016 Hard+ Hard-
RandomEncoder 51.4 34.6 52.7 52.3 49.7 4.8 17.9
EnglishLanguageEncoder 44.4 41.7 53.8 62.4 61.7 15.3 26.5
SemanticEncoder 68.9 62.2 75.9 79.4 79.3 22.5 46.6
Table2: STSperformanceonthe2012-2016datasetsandourSTSHarddatasetsforarandomlyinitializedTrans-
former, the trained English language-specific encoder from BGT, and the trained semantic encoder from BGT.
PerformanceismeasuredinPearson‚Äôsr√ó100.
Lang. Model STS S.Num. O.Num. Depth TopCon. Word Len. P.Num. P.First Gend.
BILINGUALTRANS 71.2 78.0 76.5 28.2 65.9 80.2 74.0 56.9 88.3 53.0
SemanticEncoder 72.4 84.6 80.9 29.7 70.5 77.4 73.0 60.7 87.9 52.6
fr
enLanguageEncoder 56.8 75.2 72.0 28.0 63.6 65.4 80.2 65.3 92.2 -
frLanguageEncoder - - - - - - - - - 56.5
BILINGUALTRANS 70.5 84.5 82.1 29.7 68.5 79.2 77.7 63.4 90.1 54.3
SemanticEncoder 72.1 85.7 83.6 32.5 71.0 77.3 76.7 63.1 89.9 52.6
es
enLanguageEncoder 55.8 75.7 73.7 29.1 63.9 63.3 80.2 64.2 92.7 -
esLanguageEncoder - - - - - - - - - 54.7
BILINGUALTRANS 70.2 77.6 74.5 28.1 67.0 77.5 72.3 57.5 89.0 -
ar SemanticEncoder 70.8 81.9 80.8 32.1 71.7 71.9 73.3 61.8 88.5 -
enLanguageEncoder 58.9 76.2 73.1 28.4 60.7 71.2 79.8 63.4 92.4 -
BILINGUALTRANS 70.7 78.5 74.9 28.1 60.2 78.4 72.1 54.8 87.3 -
tr SemanticEncoder 72.3 81.7 80.2 30.6 66.0 75.2 72.4 59.3 86.7 -
enLanguageEncoder 57.8 77.3 74.4 28.3 63.1 67.1 79.7 67.0 92.5 -
BILINGUALTRANS 71.0 66.4 64.6 25.4 54.1 76.0 67.6 53.8 87.8 -
ja SemanticEncoder 71.9 68.0 66.8 27.5 58.9 70.1 68.7 52.9 86.6 -
enLanguageEncoder 60.6 77.6 76.4 28.0 64.6 70.0 80.4 62.8 92.0 -
Table 3: Average STS performance for the 2012-2016 datasets, measured in Pearson‚Äôs r √ó 100, followed by
probingresultsonpredictingnumberofsubjects,numberofobjects,constituenttreedepth,topconstituent,word
content,length,numberofpunctuationmarks,thefirstpunctuationmark,andwhetherthearticlesinthesentence
arethecorrectgender. Allprobingresultsaremeasuredinaccuracy√ó100.
5.1 STS firstofthese,punctuationnumber,wetrainaclas-
sifiertopredictthenumberofpunctuationmarks13
Wefirstshowthatthelanguagevariablesarecap-
inasentence. Tomakethetaskmorechallenging,
turinglittlesemanticinformationbyevaluatingthe
we limit each label to have at most 20,000 exam-
learned English language-specific variable from
ples split among training, validation, and testing
our BGT model on our suite of semantic tasks.
data.14 In the second task, punctuation first, we
The results in Table 2 show that these encoders
trainaclassifiertopredicttheidentityofthefirst
perform closer to a random encoder than the se-
punctuationmarkinthesentence. Inourlasttask,
manticencoderfrom BGT.Thisisconsistentwith
gender, we detect examples where the gender of
whatwewouldexpecttoseeiftheyarecapturing
thearticlesinthesentenceisincorrectinFrenchor
extraneouslanguage-specificinformation.
Spanish. Tocreateanincorrectexample,weswitch
articlesfrom{le,la,un,une}forFrenchand{el,la,
5.2 Probing
los,las}forSpanish,withtheir(indefiniteordefi-
WeprobeourBGTsemanticandlanguage-specific
niteforFrenchandsingularorpluralforSpanish)
encoders, along with our BILINGUALTRANS en-
counterpartwiththeoppositegender. Thisdataset
codersasabaseline,tocompareandcontrastwhat
wasbalancedsorandomchancegives50%onthe
aspectsofsyntaxandsemanticstheyarelearning
testing data. All tasks use 100,000 examples for
relativetoeachotheracrossfivelanguageswithvar-
training and 10,000 examples for validation and
iousdegreesofsimilaritywithEnglish. Allmodels
testing. Theresultsoftheseexperimentsareshown
aretrainedontheOpenSubtitles2018corpus. We
inTable3.
usethedatasetsfromConneauetal.(2018)forse-
Theseresultsshowthatthesourceseparationis
mantictaskslikenumberofsubjectsandnumberof
effective-stylisticandlanguage-specificinforma-
objects,andsyntactictasksliketreedepth,andtop
tionlikelength,punctuationandlanguage-specific
constituent. Additionally, we include predicting
13Punctuationweretakenfromtheset{‚Äô!‚Äù#$%&\‚Äô()
thewordcontentandsentencelength. Wealsoadd
‚àó+,‚àí./:;<=>?@[]ÀÜ ‚Äò{‚Äî}Àú‚Äô.}.
ourowntaskstovalidateourintuitionsaboutpunc-
14Thelabelsarefrom1punctuationmarkupto10marks
tuationandlanguage-specificinformation. Inthe withanadditionallabelconsolidating11ormoremarks.
gender information are more concentrated in the Source youknowwhati‚Äôveseen?
Style hesaid,‚Äúsincewhenisgoingfishing‚Äùhadany-
languagevariables,whilewordcontent,semantic
thingtodowithfish?‚Äù
andsyntacticinformationaremoreconcentratedin Output hesaid,‚Äúwhatisgoingtodowithmesincei
the semantic encoder. The choice of language is sawyou?‚Äù
Source guys,thatwasthetechunit.
alsoseentobeinfluentialonwhattheseencoders
Style iswell,‚Äúcapicci‚Äù...
arecapturing. Whenthelanguagesarecloselyre- Output isthatwhat,‚Äútechnician‚Äù?
lated to English, like in French and Spanish, the Source thepayisnogood,butit‚Äôsmoney.
Style doweknowcauseofdeath?
performancedifferencebetweenthesemanticand
Output dowehaveanymoney?
English language encoder is larger for word con- Source we‚Äôrealwaysdoingstupidthings.
Style allrightlisten,ilikebeingexactlywhereiam,
tent,subjectnumber,objectnumberthanformore
Output allright,ilikebeingstupid,butiamalways
distantly related languages like Arabic and Turk-
here.
ish. Infact,wordcontentperformanceisdirectly
Table 4: Style transfer generations from our learned
tiedtohowwellthealphabetsofthetwolanguages
BGT model. Source refers to the sentence fed into
overlap. This relationship matches our intuition,
the semantic encoder, Style refers to the sentence fed
becauselexicalinformationwillbecheapertoen-
intotheEnglishlanguage-specificencoder,andOutput
code in the semantic variable when it is shared
referstothetextgeneratedbyourmodel.
between the languages. Similarly for the tasks
oflength,punctuationfirst,andpunctuationnum- manticsoftheSourcesentenceandthestyleofthe
ber, the gap in performance between the two en- Style sentence. We use our en-fr BGT model
codersalsogrowsasthelanguagesbecomemore fromTable3andshowsomeexamplesinTable4.
distantfromEnglish. Lastly,thegaponSTSper-
Allinputsentencesarefromheld-outen-frOpen-
formancebetweenthetwoencodersshrinksasthe Subtitlesdata. Fromtheseexamples,weseefurther
languages become more distant, which again is evidenceoftheroleofthesemanticandlanguage-
what we would expect, as the language-specific specificencoders,wheremostofthesemantics(e.g.
encodersareforcedtocapturemoreinformation. topical word such as seen and tech in the Source
sentence) are reflected in the output, but length
Japanese is an interesting case in these exper-
andstructurearemorestronglyinfluencedbythe
iments, where the English language-specific en-
language-specificencoder.
coder outperforms the semantic encoder on the
semantic and syntactic probing tasks. Japanese
is a very distant language to English both in its 6 Conclusion
writing system and in its sentence structure (it is
WeproposeBilingualGenerativeTransformers,a
an SOV language, where English is an SVO lan-
model that uses parallel data to learn to perform
guage). However,despitethesedifference,these-
source separation of common semantic informa-
manticencoderstronglyoutperformstheEnglish
tionbetweentwolanguagesfromlanguage-specific
language-specificencoder,suggestingthattheun-
information. Weshowthatthemodelisabletoac-
derlying meaning of the sentence is much better
complish this source separation through probing
capturedbythesemanticencoder.
tasksandtextgenerationinastyle-transfersetting.
Wefindthatourmodelbestsallbaselinesonunsu-
5.3 GenerationandStyleTransfer
pervisedsemanticsimilaritytasks,withthelargest
In this section, we qualitatively demonstrate the gains coming from a new challenge we propose
ability of our model to generate sentences. We as Hard STS, designed to foil methods approxi-
focusonastyle-transfertaskwherewehaveorig- mating semantic similarity as word overlap. We
inal seed sentences from which we calculate our also find our model to be especially effective on
semanticvectorz andlanguagespecificvector unsupervisedcross-lingualsemanticsimilarity,due
sem
z . Specifically,wefeedinaSourcesentenceinto toitsstrippingawayoflanguage-specificinforma-
en
thesemanticencodertoobtainz ,andanother tion allowing for the underlying semantics to be
sem
Style sentence into the English language-specific more directly compared. In future work, we will
encoder to obtain z . We then generate a new explore generalizing this approach to the multi-
en
sentenceusingthesetwolatentvariables. Thiscan lingualsetting, orapplyingittothepre-trainand
beseenasatypeofstyletransferwhereweexpect fine-tune paradigm used widely in other models
the model to generate a sentence that has the se- suchasBERT.
Acknowledgments Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
The authors thank Amazon AWS for the gen- tatedcorpusforlearningnaturallanguageinference.
erous donation of computation credits. This In Proceedings of the 2015 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,pages
projectisfundedinpartbytheNSFundergrants
632‚Äì642,Lisbon,Portugal.
1618044and1936155,andbytheNEHundergrant
HAA256044-17. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. SemEval-2017
Task 1: Semantic textual similarity multilingual
and crosslingual focused evaluation. In Proceed-
References
ingsofthe11thInternationalWorkshoponSemantic
Evaluation(SemEval-2017),pages1‚Äì14,Vancouver,
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Canada.
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo,InigoLopez-Gazpio,MontseMaritxalar,Rada
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Mihalcea,GermanRigau,LarraitzUria,andJanyce
Nicole Limtiaco, Rhomni St John, Noah Constant,
Wiebe. 2015. SemEval-2015 task 2: Semantic tex-
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
tual similarity, English, Spanish and pilot on inter-
et al. 2018. Universal sentence encoder. arXiv
pretability. In Proceedings of the 9th International
preprintarXiv:1803.11175.
WorkshoponSemanticEvaluation(SemEval2015).
Mingda Chen, Qingming Tang, Sam Wiseman, and
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel KevinGimpel.2019. Amulti-taskapproachfordis-
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei entangling syntax and semantics in sentence repre-
Guo, Rada Mihalcea, German Rigau, and Janyce sentations. arXivpreprintarXiv:1904.01173.
Wiebe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the Muthu Chidambaram, Yinfei Yang, Daniel Cer, Steve
8thInternationalWorkshoponSemanticEvaluation Yuan, Yunhsuan Sung, Brian Strope, and Ray
(SemEval2014). Kurzweil. 2019. Learning cross-lingual sentence
representationsviaamulti-taskdual-encodermodel.
EnekoAgirre,CarmenBanea,DanielCer,MonaDiab, In Proceedings of the 4th Workshop on Represen-
Aitor Gonzalez-Agirre, Rada Mihalcea, German tation Learning for NLP (RepL4NLP-2019), pages
Rigau, and Janyce Wiebe. 2016. SemEval-2016 250‚Äì259, Florence, Italy. Association for Computa-
task1: Semantictextualsimilarity,monolingualand tionalLinguistics.
cross-lingual evaluation. Proceedings of SemEval,
pages497‚Äì511. AlexisConneauandDouweKiela.2018. Senteval: An
evaluation toolkit for universal sentence representa-
EnekoAgirre,DanielCer,MonaDiab,AitorGonzalez- tions. arXivpreprintarXiv:1803.05449.
Agirre, andWeiweiGuo.2013. *sem2013shared
task: Semantic textual similarity. In Second Joint AlexisConneau,DouweKiela,HolgerSchwenk,Lo¬®ƒ±c
Conference on Lexical and Computational Seman- Barrault, and Antoine Bordes. 2017. Supervised
tics (* SEM), Volume 1: Proceedings of the Main learning of universal sentence representations from
Conference and the Shared Task: Semantic Textual natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
Similarity,volume1,pages32‚Äì43.
ural Language Processing, pages 670‚Äì680, Copen-
hagen,Denmark.
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
AlexisConneau,GermanKruszewski,GuillaumeLam-
pilotonsemantictextualsimilarity. InProceedings
ple, Lo¬®ƒ±c Barrault, and Marco Baroni. 2018. What
of the First Joint Conference on Lexical and Com-
youcancramintoasinglevector: Probingsentence
putationalSemantics-Volume1: Proceedingsofthe
embeddingsforlinguisticproperties. arXivpreprint
main conference and the shared task, and Volume
arXiv:1805.01070.
2: ProceedingsoftheSixthInternationalWorkshop
on Semantic Evaluation. Association for Computa-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
tionalLinguistics.
KristinaToutanova.2018. Bert:Pre-trainingofdeep
bidirectional transformers for language understand-
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
ing. arXivpreprintarXiv:1810.04805.
Asimplebuttough-to-beatbaselineforsentenceem-
beddings. In Proceedings of the International Con- BillDolan,ChrisQuirk,andChrisBrockett.2004. Un-
ferenceonLearningRepresentations. supervisedconstructionoflargeparaphrasecorpora:
Exploitingmassivelyparallelnewssources. InPro-
Mikel Artetxe and Holger Schwenk. 2018. Mas- ceedingsofCOLING.
sively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond. arXiv Cristina Espana-Bonet, Ada¬¥m Csaba Varga, Alberto
preprintarXiv:1812.10464. Barro¬¥n-CedenÀúo, and Josef van Genabith. 2017. An
empirical analysis of nmt-derived interlingual em- Gabriel Pereyra, George Tucker, Jan Chorowski,
beddings and their use in parallel sentence identifi- ≈ÅukaszKaiser,andGeoffreyHinton.2017. Regular-
cation. IEEE Journal of Selected Topics in Signal izingneuralnetworksbypenalizingconfidentoutput
Processing,11(8):1340‚Äì1350. distributions. arXivpreprintarXiv:1701.06548.
Juri Ganitkevitch, Benjamin Van Durme, and Chris MatthewEPeters, MarkNeumann, MohitIyyer, Matt
Callison-Burch. 2013. PPDB: The Paraphrase Gardner, Christopher Clark, Kenton Lee, and Luke
Database. InProceedingsofHLT-NAACL. Zettlemoyer. 2018. Deep contextualized word rep-
resentations. InProceedingsofNAACL-HLT,pages
Junxian He, Daniel Spokoyny, Graham Neubig, and 2227‚Äì2237.
Taylor Berg-Kirkpatrick. 2019. Lagging inference
networks and posterior collapse in variational au- Martin Popel and OndÀárej Bojar. 2018. Training tips
toencoders. arXivpreprintarXiv:1901.05534. for the transformer model. The Prague Bulletin of
MathematicalLinguistics,110(1):43‚Äì70.
FelixHill,KyunghyunCho,andAnnaKorhonen.2016.
Learning distributed representations of sentences Nils Reimers and Iryna Gurevych. 2019. Sentence-
from unlabelled data. In Proceedings of the 2016 bert: Sentence embeddings using siamese bert-
Conference of the North American Chapter of the networks. arXivpreprintarXiv:1908.10084.
Association for Computational Linguistics: Human
LanguageTechnologies. Holger Schwenk. 2018. Filtering and mining parallel
data in a joint multilingual space. arXiv preprint
SeppHochreiterandJu¬®rgenSchmidhuber.1997. Long arXiv:1805.09822.
short-termmemory. Neuralcomputation,9(8).
Holger Schwenk and Matthijs Douze. 2017. Learn-
Diederik Kingma and Jimmy Ba. 2014. Adam: A ing joint multilingual sentence representations
method for stochastic optimization. arXiv preprint with neural machine translation. arXiv preprint
arXiv:1412.6980. arXiv:1704.04154.
Diederik P Kingma and Max Welling. 2013. Auto- Sandeep Subramanian, Adam Trischler, Yoshua Ben-
encoding variational bayes. arXiv preprint gio, and Christopher J Pal. 2018. Learning gen-
arXiv:1312.6114. eralpurposedistributedsentencerepresentationsvia
large scale multi-task learning. arXiv preprint
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, arXiv:1804.00079.
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
AdvancesinNeuralInformationProcessingSystems JonShlens,andZbigniewWojna.2016. Rethinking
28,pages3294‚Äì3302. the inception architecture for computer vision. In
ProceedingsoftheIEEEconferenceoncomputervi-
TakuKudoandJohnRichardson.2018. Sentencepiece: sionandpatternrecognition,pages2818‚Äì2826.
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
arXivpreprintarXiv:1808.06226. Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neu- you need. In Advances in Neural Information Pro-
big, and Eduard Hovy. 2019. FlowSeq: Non- cessingSystems,pages5998‚Äì6008.
autoregressiveconditionalsequencegenerationwith
generative flow. In Proceedings of the 2019 Con- JohnWieting,MohitBansal,KevinGimpel,andKaren
ferenceonEmpiricalMethodsinNaturalLanguage Livescu.2016a. Charagram: Embeddingwordsand
Processing and the 9th International Joint Confer- sentencesviacharactern-grams. InProceedingsof
ence on Natural Language Processing (EMNLP- the2016ConferenceonEmpiricalMethodsinNatu-
IJCNLP),pages4273‚Äì4283,HongKong,China.As- ralLanguageProcessing,pages1504‚Äì1515.
sociationforComputationalLinguistics.
JohnWieting,MohitBansal,KevinGimpel,andKaren
TomasMikolov,IlyaSutskever,KaiChen,GregS.Cor- Livescu.2016b. Towardsuniversalparaphrasticsen-
rado, and Jeff Dean. 2013. Distributed representa- tence embeddings. In Proceedings of the Interna-
tionsofwordsandphrasesandtheircompositional- tionalConferenceonLearningRepresentations.
ity. In Advances in Neural Information Processing
Systems. JohnWieting,TaylorBerg-Kirkpatrick,KevinGimpel,
and Graham Neubig. 2019a. Beyond bleu: Train-
JeffreyPennington,RichardSocher,andChristopherD. ing neural machine translation with semantic sim-
Manning.2014. Glove:Globalvectorsforwordrep- ilarity. In Proceedings of the 57th Annual Meet-
resentation. Proceedings of Empirical Methods in ingoftheAssociationforComputationalLinguistics,
NaturalLanguageProcessing(EMNLP2014). pages4344‚Äì4355.
John Wieting and Kevin Gimpel. 2017. Revisiting re- -
current networks for paraphrastic sentence embed-
dings. InProceedingsofthe55thAnnualMeetingof
the Association for Computational Linguistics (Vol-
ume1: LongPapers),pages2078‚Äì2088,Vancouver,
Canada.
John Wieting and Kevin Gimpel. 2018. ParaNMT-
50M:Pushingthelimitsofparaphrasticsentenceem-
beddings with millions of machine translations. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
LongPapers),pages451‚Äì462.AssociationforCom-
putationalLinguistics.
John Wieting, Kevin Gimpel, Graham Neubig, and
Taylor Berg-Kirkpatrick. 2019b. Simple and effec-
tiveparaphrasticsimilarityfromparalleltranslations.
ProceedingsoftheACL.
Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy
Guo, Jax Law, Noah Constant, Gustavo Hernandez
Abrego, Steve Yuan, Chris Tar, Yun-hsuan Sung,
BrianStrope,andRayKurzweil.2020. Multilingual
universal sentence encoder for semantic retrieval.
In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics: System
Demonstrations, pages 87‚Äì94, Online. Association
forComputationalLinguistics.
Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved varia-
tional autoencoders for text modeling using dilated
convolutions. In Proceedings of the 34th Interna-
tionalConferenceonMachineLearning-Volume70,
pages3881‚Äì3890.JMLR.org.
Zachary Ziegler and Alexander Rush. 2019. Latent
normalizing flows for discrete sequences. In Inter-
national Conference on Machine Learning, pages
7673‚Äì7682.
Pierre Zweigenbaum, Serge Sharoff, and Reinhard
Rapp.2018. Overviewofthethirdbuccsharedtask:
Spotting parallel sentences in comparable corpora.
In Proceedings of 11th Workshop on Building and
UsingComparableCorpora,pages39‚Äì42.
A LocationofSentenceEmbeddingin loglikelihood,estimatedbyimportanceweighting,
DecoderforLearningRepresentations increased slightly from 53.3 to 54.0 when using
Logit.
AsmentionedinSection2,weexperimentedwith
4waystoincorporatethesentenceembeddinginto
B RelationshipBetweenBatchSizeand
thedecoder: Word,Hidden,Attention,andLogit.
PerformanceforTransformerand
Wealsoexperimentedwithcombinationsofthese
LSTM
4 approaches. We evaluate these embeddings on
theSTStasksandshowtheresults,alongwiththe
timetotrainthemodels1epochinTable5.
For these experiments, we train a single layer Performace Vs. Batch Size
bidirectionalLSTM(BiLSTM)ENGLISHTRANS 0.70 T LSra Tn Msformer
modelwithembeddingsizesetto1024andhidden
0.65
statessetto512dimensions(inordertoberoughly
equivalenttoourTransformermodels). Toformthe
0.60
sentenceembeddinginthisvariant,wemeanpool
thehiddenstatesforeachtimestep. Thecellstates
0.55
ofthedecoderareinitializedtothezerovector.
0.50
Architecture STS Time(s)
BiLSTM(Hidden) 54.3 1226
BiLSTM(Word) 67.2 1341 0 50000 100000 150000 200000 250000 300000
Batch size (Max Tokens Per Batch)
BiLSTM(Attention) 68.8 1481
BiLSTM(Logit) 69.4 1603
Figure 5: The relationship between average perfor-
BiLSTM(Wd.+Hd.) 67.3 1377
manceforeachyearoftheSTStasks2012-2016(Pear-
BiLSTM(Wd.+Hd.+Att.) 68.3 1669
BiLSTM(Wd.+Hd.+Log.) 69.1 1655 son‚Äôs r √ó 100) and batch size (maximum number of
BiLSTM(Wd.+Hd.+Att.+Log.) 68.9 1856 wordsperbatch).
Table5: Resultsfordifferentwaysofincorporatingthe
sentence embedding in the decoder for a BiLSTM on It has been observed previously that the per-
the Semantic Textual Similarity (STS) datasets, along formance of Transformer models is sensitive to
withthetimetakentotrainthemodelfor1epoch. Per- batch size (Popel and Bojar, 2018) . We found
formanceismeasuredinPearson‚Äôsr√ó100. thistobeespeciallytruewhentrainingsequence-
to-sequencemodelstolearnsentenceembeddings.
From this analysis, we see that the best perfor- Figure5showsplotsoftheaverage2012-2016STS
manceisachievedwithLogit,whenthesentence performance of the learned sentence embedding
embeddingisplacejustpriortothesoftmax. The as batch size increases for both the BiLSTM and
performance is much better than Hidden or Hid- Transformer. Initially, at a batch size of 2500 to-
den+Word used in prior work. For instance, re- kens,sentenceembeddingslearnedareworsethan
cently(ArtetxeandSchwenk,2018)usedtheHid- random, even though validation perplexity does
den+Word strategy in learning multilingual sen- decrease during this time. Performance rises as
tenceembeddings. batchsizeincreasesuptoaround100,000tokens.
In contrast, the BiLSTM is more robust to batch
A.1 VAETraining
size, peakingmuchearlieraround25,000tokens,
We also found that incorporating the latent code
andevendegradingathigherbatchsizes.
ofaVAEintothedecoderusingtheLogitstrategy
increasesthemutualinformationwhilehavinglittle C ModelAblations
effectontheloglikelihood. WetrainedtwoLSTM
VAEmodelsfollowingthesettingsandaggressive In this section, we vary the number of layers in
training strategy in (He et al., 2019), where one theencoderanddecoderin BGT W/O PRIOR. We
LSTM model used the Hidden strategy and the see that performance increases as the number of
otherusedtheHidden+Logitstrategy. Wetrained encoderlayersincreases,andalsothatalargede-
the models on the en side of our en-fr data. coderhurtsperformance,allowingustosavetrain-
We found that the mutual information increased ing time by using a single layer. These results
form0.89to2.46,whiletheapproximatenegative canbecomparedtothoseinTable7showingthat
ecnamrofreP
STS
.gvA
DataSplit n BGT SP
All 13,023 75.3 74.1
Negation 705 73.1 68.7
Bottom20%SWER,label‚àà [0,2] 404 63.6 54.9
Bottom10%SWER,label‚àà [0,1] 72 47.1 22.5
Top20%SWER,label‚àà [3,5] 937 20.0 14.4
Top10%SWER,label‚àà [4,5] 159 18.1 10.8
Top20%SWER,label‚àà [0,2] 1380 51.5 49.9
Bottom20%SWER,label‚àà [3,5] 2079 43.0 42.2
Table 6: Performance, measured in Pearson‚Äôs r √ó100, for different data splits of the STS data. The first row
showsperformanceacrossalluniqueexamples,thenextrowshowsthenegationsplit,andthelastfourrowsshow
difficult examples filtered symmetric word error rate (SWER). The last two rows show relatively easy examples
accordingtoSWER.
Architecture STS Time(s)
Transformer(5L/1L) 70.3 1767
Transformer(3L/1L) 70.1 1548
Transformer(1L/1L) 70.0 1244
Transformer(5L/5L) 69.8 2799
Table 7: Results on the Semantic Textual Similarity
(STS)datasetsfordifferentconfigurationsofENGLISH-
TRANS, along with the time taken to train the model
for1epoch. (XL/YL)meansXlayerswereusedinthe
encoder and Y layers in the decoder. Performance is
measuredinPearson‚Äôsr√ó100.
TransformersoutperformBiLSTMSintheseexper-
iments.
D HardSTS
WeshowfurtherdifficultsplitsinTable6,including
a negation split, beyond those used in Hard STS
andcomparethetoptwoperformingmodelsinthe
STStaskfromTable3. Wealsoshoweasiersplits
inthebottomofthetable.

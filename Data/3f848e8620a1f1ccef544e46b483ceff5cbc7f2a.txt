Understanding Performance of Long-Document Ranking
Models through Comprehensive Evaluation and Leaderboarding
LeonidBoytsov TianyiLin‚àó EricNyberg
leo@boytsov.info FangweiGao‚àó ehn@cs.cmu.edu
BoschCenterforArtificial YutianZhao‚àó CarnegieMellonUniversity
Intelligence JeffreyHuang‚àó Pittsburgh,USA
Pittsburgh,USA
CarnegieMellonUniversity
Pittsburgh,USA
ABSTRACT length,rankingoflongdocumentsinvolvesfirstsplittingdocu-
We carry out a comprehensive evaluation of 13 recent models mentintopassagesandthencombiningpassage-levelrelevance
forrankingoflongdocumentsusingtwopopularcollections(MS informationintoadocument-levelscore.Anumberofranking-
MARCOdocumentsandRobust04).Ourmodelzooincludestwo through-aggregationapproacheshavebeenproposed[7,18,31,36],
specializedTransformermodels(suchasLongformer)thatcanpro- buttheseevaluationshaveoneormoreshortcoming.Mostimpor-
cesslongdocumentswithouttheneedtosplitthem.Alongtheway, tantly:
wedocumentseveraldifficultiesregardingtrainingandcomparing ‚Ä¢ Relianceonsmall-scalequerycollectionssuchasRobust04
suchmodels.Somewhatsurprisingly,wefindthesimpleFirstPbase- [55],Gov2[12],orClueWeb09/12[13,14].However,having
line(truncatingdocumentstosatisfytheinput-sequenceconstraint asmallquerysetmakesitdifficulttobothtraineffective
ofatypicalTransformermodel)tobequiteeffective.Weanalyzethe models[39]andevaluatethemaccurately[53][6].Itisnot
distributionofrelevantpassages(insidedocuments)toexplainthis quiteclearhowsuchmodelsperforminabig-dataregime.
phenomenon.Wefurtherarguethat,despitetheirwidespreaduse, ‚Ä¢ Evaluationssometimeslackkeyablationsorsuchablations
Robust04andMSMARCOdocumentsarenotparticularlyuseful arenotappliedsystematically.Importantly,thereisoftenno
forbenchmarkingoflong-documentmodels. comparisonwithspecializedTransformermodelssuchas
LongformerandBigBird[4,62],whichwecollectivelycall
CCSCONCEPTS LongPmodels,whichcandirectlyprocesslongdocuments
‚Ä¢Informationsystems‚ÜíRetrievalmodelsandranking. by‚Äúsparsifying‚Äùattention.
‚Ä¢ Furthermore,itisnotclearhowmuchlong-documentmodels
KEYWORDS (includingthosethatemployLongformerandBigBird)can
improveuponasimpleFirstPbaseline,whichtruncatesinput
Neuralinformationretrieval,longdocumentranking
tobeshorterthanabout512tokens.
ACMReferenceFormat: ‚Ä¢ Authorsdonotdiscusstheirseedselectionstrategy:dothey
LeonidBoytsov,TianyiLin,FangweiGao,YutianZhao,JeffreyHuang, reportperformanceforabest-seedscenarioanaverageover
andEricNyberg.20XX.UnderstandingPerformanceofLong-Document multipleseeds?
RankingModelsthroughComprehensiveEvaluationandLeaderboarding. ‚Ä¢ Last,butnotleastfewresearchers‚Äústress-test‚Äùboththeir
InProceedingsofACMConference(Conference‚Äô17).ACM,NewYork,NY,
ownmodelsandbaselinesby,e.g.,tryingtoachievestrong
USA,11pages.https://doi.org/XX.YYYY/ZZZZZZ.WWWWWW
resultsonapublicleaderboardwithahiddenvalidationset.
1 INTRODUCTION Tofillthegapwecarriedoutacomprehensiveevaluationof
13 recent models for ranking of long documents and made the
LargeTransformer[54]modelssuchasBERT[19]pre-trainedina
keysoftwarecomponentspubliclyavailable.1Inthat,weaskthe
self-supervisedmannerconsiderablyadvancedstate-of-the-artof
followingresearchquestions:
corenaturallanguageprocessing(NLP)[19,45]andinformation
retrieval[41].However,duetoquadraticcostoftheself-attention‚Äî ‚Ä¢ RQ1 How much state-of-the-art long-document ranking
akeyTransformercomponent‚Äîwithrespecttoaninputsequence modelsoutperformtheFirstPbaselineandisthereroomfor
furtherimprovement?
‚àóWorkdonewhilestudyingatCarnegieMellonUniversity. ‚Ä¢ RQ2WhatifwesimplyapplyLongformer[4]?Domodels
proposed by the IR communities fare well against LongP
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor modelsLongformer[4]andBigBird[62]?
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
‚Ä¢ RQ3 Can PARADE-Transformer models be improved by
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM feedingqueryembeddingsintotheaggregatingTransformer?
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
Notethatwedonotaimatthefullreplicabilityofpriorresults,but,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee.Requestpermissionsfrompermissions@acm.org. instead,focusonarelativeperformanceoflong-documentmodels
Conference‚Äô17,July2017,Washington,DC,USA comparedtosimpleFirstPbaselines.
¬©20XXAssociationforComputingMachinery.
ACMISBN978-x-xxxx-xxxx-x/YY/MM...$15.00
https://doi.org/XX.YYYY/ZZZZZZ.WWWWWW 1https://github.com/searchivarius/long_doc_rank_model_analysis
2202
luJ
4
]RI.sc[
1v26210.7022:viXra
Conference‚Äô17,July2017,Washington,DC,USA LeonidBoytsov,TianyiLin,FangweiGao,YutianZhao,JeffreyHuang,andEricNyberg
Ourkeyfindingisthatrankingmodelscapableofprocessing combine a limited-span (i.e., a sliding window) attention with
longdocuments,includingspecialized Transformerswithsparse someformofaselectiveglobalattention.Therearemanysuch
attention[4,62],showlittleimprovementofencodingwholedocu- approachesproposed(see,e.g.,arecentsurvey[51])anditwould
ments,comparedtorespectiveFirstPbaselines.In¬ß3.3.3weargue beinfeasibletoevaluatethemall.Insteadweconsidertwopopular
thatthisislikelyduetoapositionbiasofrelevantpassages.Thus, models:Longformer[4]andBig-Bird[62].
weconcludethatRobust04andMSMARCO‚Äîwhicharepossiblythe Withadocument-splittingapproach,onehastosplitdocuments
mostcommonlyusedretrievaldatasetswithasubstantialnumber intoseveralchunks,processeachchunkseparately,andaggregate
oflongdocuments‚Äîarenotparticularlyusefulforbenchmarking results,e.g.,bycomputingamaximumoraweightedprediction
long-documentmodels. score[18,61].Withrespecttotrainingapproaches,theMaxPand
Alongtheway,wedocumentseveraldifficultiesrelatedtocom- SumPmodelsbyDaiandCallan[18]assumethateachchunkina
paringrankingmodelsusingdifferenttrainingsetsandinitialstart- relevantdocumentisrelevant.However,thisassumptionisprob-
ingmodels: lematicasthedegreeofrelevancevariesfrompassagetopassage.
‚Ä¢ In particular, strong performance of Longformer can be Yilmazetal.[61]workaroundthisproblembytrainingaMaxP
largely explained by better pre-training compared to the BERTmodelonshortdocumentsandzero-transferittolongdoc-
originalBERTmodelratherthantoitsabilitytoeffectively uments.Inthisstudyweworkaroundthisproblembytraining
encodelongdocuments. alldocument-splittingapproachesincludingMaxP[18]intheend-
‚Ä¢ WefindthattrainingaccuratePARADE-Transformermodels to-endfashion,i.e.,bypluggingaggregateddocument-levelscores
witharandomlyinitializedaggregatorTransformer[54]can directlyintoalossfunction(analogoustotrainingofCEDR[36]
be quite challenging and we were not able to train such andPARADE[31]models).
modelstoagoodaccuracy.Incontrast, Becauseourprimaryfocusisaccuracyandweaimtounderstand
thelimitsoflong-documentmodels,weexcludefromevaluation
Lastly,ourmodificationofaPARADE-Transformer[31]resulted
severalrecentmodels(e.g.,[23,64])thatachievebetterefficiency-
inasmallbutmeaningfulimprovementontheMSMARCOdataset[1]
effectivenesstrade-offsbypre-selectingcertaindocumentpartsand
(see¬ß3.3.2).Weusedinsightsobtainedfromourextensiveexper-
feedingonlyselectedpartsintoaBERTranker.
imentsaswellasourPARADE-Transformermodificationtosub-
stantiallyimproveourpositionsontheMSMARCOleaderboard.
2.2 RankingwithLong-DocumentModels
2 METHODS Inthissection,wedescribelong-documentBERTmodelsinmore
2.1 RelatedWork details.Weassumethataninputtextissplitintosmallchunksof
textscalledtokens.AlthoughtokenscanbecompleteEnglishwords,
NeuralRankingmodelshavebeenapopulartopicinrecentyears
Transformermodelsusuallysplittextintosub-wordunits[58].
[22],butthesuccessofearlyapproacheswascontroversial[32]. The length of a documentùëë‚Äîdenoted as |ùëë|‚Äîis measured in
Thischangedwithanintroductionofabi-directionalencoder-only
thenumberoftokens.Becauseneuralnetworkscannotoperate
TransformermodelBERT[19],whichwasasuccessorofGPT[45] directlyontext,asequenceoftokensùë° 1ùë° 2...ùë° ùëõisfirstconvertedto
andELMO[43].BERTwashugelysuccessfulanditsresounding asequencesofùëë-dimensionalembeddingvectorsùë§ 1ùë§ 2...ùë§ ùëõbyan
successcanbeattributedtoacombinationofthelargemodelsize
embeddingnetwork.Theseembeddingsarecontext-independent,
andmassivepre-trainingusingself-supervision.Anumberofdif-
i.e.,eachtokenisalwaysmappedtothesamevector[15,38].
ferentTransformermodelssuchasRoBERTA[34],ELECTRA[11],
ForadetaileddescriptionofTransformermodels,pleaseseethe
andERNIE[50]improveuponBERTusingdifferenttrainingstrate-
annotatedTransformerguide[48]aswellastherecentsurveyby
giesand/ordatasets.However,duetotheirarchitecturalsimilarities
Linetal.[32],whichfocusesontheuseofBERT-stylemodelsfor
we‚ÄîfollowingLinetal[33]‚ÄîcollectivelycallthemasBERTmodels.
rankingandretrieval.Forthispaper,itisnecessarytoknowonly
NogueiraandChowerefirsttoapplyBERTtorankingoftext
thefollowingbasicfacts:
documents[41].Inthebig-dataregime‚ÄîmostnotablyintheTREC
‚Ä¢ BERTisanencoder-onlymodel,whichconvertsasequence
deeplearningtrack[16]‚ÄîBERT-basedmodelsoutperformedprior
neuralandnon-neuralapproachesbyalargemargin.Theywere
oftokensùë° 1ùë° 2...ùë° ùëõtoasequenceofùëë-dimensionalvectors
alsoquitesuccessfulforseveralsmall-scalequerycollectionsout-
ùë§ 1ùë§ 2...ùë§ ùëõ. These vectors‚Äîwhich are token representa-
tionsfromthelastmodellayer‚Äîarecommonlyreferredto
performingpreviousneuralandtraditionalapproaches[18,31,36].
ascontextualizedtokenembeddings[43];
Duetotheirunmatchedperformance,BERT-basedrankersarethe
‚Ä¢ BERToperatesonwordpieces[58]ratherthanoncomplete
centerpieceofourstudy.
words;
TheTransformermodel[54]usesanattentionmechanism[2]
‚Ä¢ Thevocabularyincludestwospecialtokens:[CLS](anag-
whereeachsequencepositioncanattendtoallthepositionsinthe
gregator)and[SEP](aseparator);
previouslayer.Becauseself-attentioncomplexityisquadraticwith
‚Ä¢ [CLS]isalwaysprependedtoeverytokensequenceandits
respecttoasequencelength,directprocessingoflongdocumentsis
embeddingisusedasasequencerepresentationforclassifi-
notpractical.Thus,avastmajorityofexistingTransformermodels
cationandrankingtasks.
limittheinputlengthtobeatmost512(subword)tokens.
Therearetwogeneralapproachestohandlinglongdocuments: A‚Äúvanilla‚ÄùBERTranker(dubbedasmonoBERTbyLinetal.[32])
localizationofattentionandsplittingdocumentsintochunkseach uses a single fully-connect layer ùêπ as a prediction head, which
ofwhichisprocessedseparately.Attention-localizationapproaches convertsthelast-layerrepresentationofthe[CLS]token(i.e.,a
UnderstandingPerformanceofLong-DocumentRankingModelsthroughComprehensiveEvaluationandLeaderboarding Conference‚Äô17,July2017,Washington,DC,USA
contextualizedembeddingof [CLS])intoascalar[41].Itmakes ‚Ä¢ processingeachsequencewithaBERTmodeltogenerate
apredictionbasedonthefollowingsequenceoftokens:[CLS]ùëû contextualizedembeddingsforregulartokensaswellasfor
[SEP]ùëë[SEP],whereùëûisaqueryandùëëisadocument. [CLS].
Analternativeapproachistoaggregatecontextualizedembed- Theoutcomeofthisprocedureisùëö[CLS]-vectorsùëêùëôùë†
ùëñ
andùëõcon-
dingsofregulartokensusingashallowneuralnetwork[7,30,36] textualizedvectorsùë§ 1ùë§ 2...ùë§ ùëõ (oneforeachdocumenttokenùë° ùëñ)
(possiblytogetherwiththecontextualizedembeddingof [CLS]).
thatareaggregatedinamodel-specificways.
ThiswasfirstproposedbyMacAvaneyetal.[36]whoalsofound
MacAvaneyetal.[36]usecontextualizedembeddingsasadirect
thatincorporating[CLS]improvesperformance.However,Boytsov
replacementofcontext-freeembeddingsinthefollowingneural
andKolterproposedashallowaggregatingnetworkthatdoesnot
architectures:KNRM[59],PACRR[26],andDRMM[21].Toboost
usetheoutputofthe[CLS]tokenandachievedthesameaccuracy
performance,theyincorporate[CLS]vectorsinamodel-specific
onMSMARCOdatasets[7].
way.WecalltherespectivemodelsasCEDR-KNRM,CEDR-PACRR,
ReplacingthestandardBERTmodelinthevanillaBERTranker
andCEDR-DRMM.
withaBERTvariantthat‚Äúnatively‚Äùsupportslongerdocuments
TheyalsoproposedanextensionofthevanillaBERTrankerthat
(e.g.,Big-Bird[62])is,perhaps,thesimplestwaytodealwithlong makesapredictionusingtheaverage[CLS]token:ùëö1 (cid:205)ùëö ùëñ=1ùëêùëôùë† ùëñ by
documents.WecollectivelycallthesemodelsasLongPmodels.For
passingitthroughalinearprojectionlayer.Wecallthismethod
atypicalBERTmodel,however,longdocumentsandqueriesneed
AvgP.
tobesplitortruncatedsothattheoverallnumberoftokensdoes
TheNeuralModel1[7]usesthesamegreedypartitioningap-
notexceed512.IntheFirstPmode,weprocessonlythefirstchunk
proachasCEDR,butadifferentaggregatornetwork,whichdoesnot
andignorethetruncatedtext.IntheSplitPmode,eachchunkispro- usetheembeddingsofthe[CLS]token.Thisnetworkisaneural
cessedseparatelyandtheresultsareaggregated.Intheremaining
parametrizationoftheclassicModel1[5,10].
ofthissection,wediscusstheseapproachesindetail.
Slidingwindowapproach. TheBERTMaxP/SumP[18]andPA-
2.2.1 LongP models. In our work, we benchmark two popular RADE[31]modelsuseaslidingwindowapproach.Assumeùë§ is
LongPmodels:Longformer[4]andBig-Bird[62].Inthat,weuse thesizeofhewindowandùë†isthestride.Thentheprocessingcan
thesameapproachtoscoredocumentsaswiththevanillaBERT
besummarizedasfollowing:
ranker,namely,concatenatingquerieswithdocumentsandmaking
apredictionbasedonthecontextualizedembeddingofthe[CLS]
‚Ä¢ tokenizing,thedocumentùëëintosub-wordsùë° 1ùë° 2...ùë° ùëõ;
‚Ä¢ splittingatokenizeddocumentùëëintoùëöpossiblyoverlapping
token[41].BothBig-BirdandLongformeruseacombinationof
thelocal,‚Äúscattered‚Äù(ourterminology),andglobalattention.The chunksùëë ùëñ =ùë° ùëñ¬∑ùë†ùë° ùëñ¬∑ùë†+1...ùë° ùëñ¬∑ùë†+ùë§‚àí1:Trailingchunksmayhave
fewerthanùë§ tokens.
localattentionutilizesaslidingwindowofaconstantlengthwhere
each token attends to each other token within this window. In ‚Ä¢ generatingùëötokensequences[CLS]ùëû[SEP]ùëë ùëñ [SEP]by
concatenatingthequerywithdocumentchunks;
thecaseoftheglobalattention,certaintokenscanattendtoall
‚Ä¢ processingeachsequencewithaBERTmodeltogeneratea
othertokensandvice-versa,InBig-Bird,onlyspecialtokenssuch
last-layeroutputforeachsequence[CLS]token.
as[CLS]canattendglobally.InLongformer,theuserhavetoselect
suchtokensexplicitly.FollowingBeltagyetal.[4],whoapplied Theoutcomeofthisprocedureisùëö[CLS]-vectorsùëêùëôùë† ùëñ,whichare
thistechniquetoquestion-answering,we‚Äúplace‚Äùglobalattention subsequentlyaggregatedinamodel-specificways.NotethatPA-
onlyonquerytokens.Unliketheglobalattention,thescattered RADEandMaxP/SumPmodelsdonotusecontextualizedembed-
attentionislimitedtorestrictedsub-setsoftokens,butthesesubsets dingsofregulartokens.
donotnecessarilyhavelocality.InBig-Birdthescatteredattention
BERT MaxP/SumP. These models [18] use a linear layer ùêπ to
reliesonrandomtokens,whereasLongformerusesadilatedsliding-
produceùëörelevancescoresùêπ(ùëêùëôùë† ùëñ).Thencompletedocumentscores
windowattentionwithlayer-andhead-specificdilation. arecomputedasmaxùëö ùëñ=1ùêπ(ùëêùëôùë† ùëñ)and(cid:205)ùëö ùëñ=1ùêπ(ùëêùëôùë† ùëñ)fortheMaxPand
2.2.2 SplitPmodels. SplitPmodelsdifferinpartitioningandag- SumPmodels,respectively.
gregationapproaches.Documentscanbesplitintoeitherdisjoint
PARADE. Thesemodels[31]canbedividedintotwogroups.The
oroverlappingchunks.Inthefirstcase,documentsaresplitina
firstgroupincludesPARADEaverage,PARADEmax,andPARADE
greedyfashionsothateachdocumentchunkexceptpossiblythe
attention,whichallusesimpleapproachestoproduceanaggregated
lastoneisexactly512tokenslongafterbeingconcatenatedwitha
representationofùëö [CLS]-vectorsùëêùëôùë† ùëñ.Tocomputearelevance
(padded)queryandthreespecialtokens.Inthesecondcase,weuse
scoretheseaggregatedrepresentationsarepassedthroughalinear
aslidingwindowapproachwithawindowsizeandstridethatare
layerùêπ.
nottiedtothemaximumlengthofBERTinput.
Inparticular,PARADEaverageandPARADEmaxcombineùëêùëôùë†
ùëñ
Greedypartitioningintodisjointchunks. CEDRmodels[36]and usingaveragingandtheelement-wisemaximumoperation,respec-
theNeuralModel1[7]usethefirstmethod,whichinvolves: tively.togenerateaggregatedrepresentationofùëö[CLS]tokens
‚Ä¢ tokenizingthedocumentùëë; ùëêùëôùë† ùëñ.2ThePARADEattentionmodelusesalearnableattention[2]
‚Ä¢ greedilysplittingatokenizeddocumentùëë intoùëö disjoint
chunks:ùëë =ùëë 1ùëë 2...ùëë ùëö; 2NotethatbothPARADEaverageandAvgPvanillarankerusethesameapproach
‚Ä¢ generatingùëötokensequences[CLS]ùëû[SEP]ùëë ùëñ [SEP]by toaggregatecontextualizedembeddingsof [CLS]tokens,buttheydifferintheir
approachtoselectdocumentchunks.Inparticular,AvePusesnon-overlappingchunks
concatenatingthequerywithdocumentchunks; whilePARADEaveragereliesontheslidingwindowapproach.
Conference‚Äô17,July2017,Washington,DC,USA LeonidBoytsov,TianyiLin,FangweiGao,YutianZhao,JeffreyHuang,andEricNyberg
Table1:Documentstatistics Table3:Numberofmodelparameters
dataset #ofdocuments avg.#ofBERTtokens Modeltype #ofparameters
MSMARCOv1 3.2M 1.4K Longformer 149M
MSMARCOv2 12M 2K BigBird 127M
Robust04 0.5M 0.6K PARADE-Transf-Pretr-L6 132M
PARADE-Transf-Pretr-L4 128M
PARADE-Transf-RAND-L2 121M
allothers 110M
Table2:Querystatistics
Notes:L2,L4,L6denote#oflayers
avg.#of avg.#of
#ofqueries
BERTtokens pos.judgements
3 EXPERIMENTS
MSMARCOv1
3.1 Data
MSMARCOtrain 352K 7 1
InourexperimentsweusetwoMSMARCOcollections(v1andv2)
MSMARCOdev 5193 7 1
TRECDL2019 43 7 153.4 [3,16,17]aswellasRobust04[55].Documentandquerystatistics
TRECDL2020 45 7.4 39.3 aresummarizedinTables1and2.
Robust04 is a small collection of 0.5M documents that has a
MSMARCOv2
mixtureofnewsarticlesandgovernmentdocumentssomeofwhich
TRECDL2021 57 9.8 143.9 arequitelong.Howeverithasonlyasmallnumberofqueries(250),
Robust04 whichmakesitachallengingbenchmarkfortrainingmodelsina
low-dataregime.Eachqueryhasatitleandadescription,which
title 250 3.6 69.6
representabriefinformationneedandamoreelaboraterequest
description 250 18.7 69.6
(oftenaproperEnglishprose),respectively.WeuseRobust04across-
validationsettingswithfoldsestablishedbyHustonandCroft[27]
andprovidedviaIR-datasets[37].
vectorùê∂tocomputeascalarweightùë§
ùëñ
ofeachùëñasfollows: MSMARCOv1collectionsarecreatedfromtheMSMARCO
readingcomprehensiondataset[3].Theycontainalargenumberof
ùë§ 1ùë§ 2...ùë§ ùëö =ùë†ùëúùëìùë°ùëöùëéùë•(ùê∂¬∑ùëêùëôùë† 1,ùê∂¬∑ùëêùëôùë† 2,...,ùê∂¬∑ùëêùëôùë† ùëö) question-likequeriessampledfromtheBingsearchenginelog(with
subsequentfiltering).Thislargequerysetisparticularlyusefulfor
Theseweightsareusedtocomputetheaggregatedrepresentation testingmodelsinthebig-dataregime.Notethatqueriesarenot
as(cid:205)ùëö ùëñ=1ùë§ ùëñùëêùëôùë†
ùëñ
necessarilyproperEnglishquestions,e.g.,‚Äúlymediseasesymptoms
PARADETransformermodelscombine[CLS]-vectorsùëêùëôùë† ùëñ with mood‚Äù,buttheyareanswerablebyashortpassageretrievedfrom
anadditionalaggregatortransformermodelùê¥ùëîùëîùëüùëíùëîùëáùëüùëéùëõùë†ùëì().The asetofabout3.6MWebdocuments[3].Document-levelrelevance
inputoftheaggregatorTransformerissequenceofùëêùëôùë† ùëñ vectors labelsarecreatedbytransferringpassage-levelrelevancetooriginal
prependedwithalearnablevectorùê∂,whichplaysaroleofa[CLS] documentsfromwhichpassageswereextracted.However,docu-
embeddingforùê¥ùëîùëîùëüùëíùëîùëáùëüùëéùëõùë†ùëì().Thelast-layerrepresentationof mentandpassagetextswerecollectedatdifferenttimes,which
the first vector is passed through a linear layer ùêπ to produce a resultedinsomecontentdivergence[16].Theresultingrelevance
relevancescore: labelsare‚Äúsparse‚Äù(aboutonepositiveexampleperquery).Inaddi-
tiontosparserelevancejudgements‚Äîseparatedintotrainingand
ùêπ(ùê¥ùëîùëîùëüùëíùëîùëáùëüùëéùëõùë†ùëì(ùê∂,ùëêùëôùë† 1,ùëêùëôùë† 2,...,ùëêùëôùë† ùëö)[0]) (1) developmentssubsets‚Äîthereisasmallnumber(98)ofqueriesthat
have‚Äúdense‚ÄùjudgementsprovidedbyNISTassessorsforTREC
AnaggregatorTransformercanbeeitherpre-trainedorran- 2019and2020deeplearning(DL)tracks[16].
domly initialized. In the case of a pre-trained transformer, we MSMARCOv2collectionswerecreatedforTREC2021DLtrack.
completelydiscardtheembeddinglayer.Furthermore,ifthedi- Theyarean‚Äúupgraded‚ÄùversionofMSMARCOv1thatemploya
mensionalityofùëêùëôùë† ùëñ vectorsisdifferentfromthedimensionalityof subsetofsparserelevancejudgementsfromMSMARCOv1.Yet,
inputembeddingsinùê¥ùëîùëîùëüùëíùëîùëáùëüùëéùëõùë†ùëì,weprojectùëêùëôùë† ùëñ usingalinear MSMARCOv2hasanimprovedalignmentbetweendocuments
transformation. andrelatedpassagesaswellasquiteafewnewdocumentswithout
WeproposeanovelmodificationofEq.1,which‚Äîinaddition respectiverelevancelabels.
toùëêùëôùë† ùëñ feedscontextualizedrepresentationsofthequerytokens Unfortunately,modelstrainedonMSMARCOv2performmuch
ùëû 1,ùëû 2,...intoùê¥ùëîùëîùëüùëíùëîùëáùëüùëéùëõùë†ùëì().Optionally,contextualizedquery worseonTREC2021queriescomparedtomodelstrainedonMS
embeddingscanbeprojectedusingalinearlayerùëÉ: MARCOv1.Thismayindicatethatmodelssomehowlearntodis-
tinguishbetweenoriginalMSMARCOv1documentsandnewly
ùêπ(ùê¥ùëîùëîùëüùëíùëîùëáùëüùëéùëõùë†ùëì(ùê∂,ùëÉ(ùëû 1),ùëÉ(ùëû 2),...,ùëêùëôùë† 1,ùëêùëôùë† 2,...,ùëêùëôùë† ùëö)[0]) (2) addedones.Asaresult,thesemodelsarebiasedandtendtonot
rankthesenewdocumentswellevenwhentheyareconsideredto
berelevantbyNISTassessors.Forthisreason,weuseMSMARCO
UnderstandingPerformanceofLong-DocumentRankingModelsthroughComprehensiveEvaluationandLeaderboarding Conference‚Äô17,July2017,Washington,DC,USA
v2datainazero-shottransfermodewhererankingmodelstrained [42]andHuggingFaceTransformerslibrary[57].Theinstructions
onMSMARCOv1areevaluatedonTRECDL2021queries. toreproduceourkeyresultsarepubliclyavailable.3FlexNeuART
[9]allowedustousevirtuallyanyHuggingFaceTransformer[54]
modelavailableintheonlinemodelrepository:Whenweusesuch
3.2 Setup
amodelforinitialization,wediscardallpredictionheads.Inpar-
TrainingModels. Ourexperimentalsetupwaslargelydefined ticular,weusedBigBird[62]andtheElectra[11]modelprovided
byourexperienceintrainingmodelsthatperformwellontheMS viaSentence-BERTlibrary[46]asvanillaBERTrankers.4Theba-
MARCOleaderboardsandinTRECevaluations.Inoursetup,a sicvanillaBERTrankeraswellasCEDRmodelsarelargelyun-
rankerisappliedtotheoutputofthefirst-stageretrievalmodel, modified implementations from the CEDR framework [36]. We
alsoknownasacandidate-generator.Thus,therankeristrainedto re-implementedPARADEmodelsfromscratch:Thisallowedusto
distinguishbetweenpositiveexamples(knownrelevantdocuments) use(almost)anypre-trainedmodelfromtheHuggingfacereposi-
andhardnegativeexamples(documentsnotmarkedasrelevant) toryasanaggregatorTransformer.Specifically,wechoseaMiniLM
sampledfromthesetoftop-ùëòcandidatesreturnedbythecandidate [56]cross-encoderfromtheSentence-BERTlibrarywithfouror
generator.Weusedùëò =100forMSMARCOdatasetsandùëò =1000 sixlayers.ForrandomlyinitializedaggregatorTransformerwe‚Äî
forRobust04.Dependingontheexperimentandthedatasetweuse somewhatsimilarto[31]‚Äîusealight-weightthree-layermodel
differentcandidategenerators. withfourheads.Wehavehighconfidenceinthequalityofour
One training epoch consists in iterating over all queries and re-implementation,becauseitwasbattle-testedontheMSMARCO
samplingonepositiveandonenegativeexamplewithasubsequent leaderboard.
computation of a pairwise margin loss. We used the batch size AllMSMARCOmodelsweretrainedfromscratch.Thenthese
16andthelearningrates10‚àí5and10‚àí4forthemainTransformer modelswerefine-tunedonRobust04.Notethatexceptfortheaggre-
layers(2¬∑10‚àí5)andallotherparameters,respectively(lossreduction gatingTransformerweuseabase,i.e.,a12-layerTransformer[54]
typeissum).Furthermore,weusedtheAdamWoptimizer[35], model:Itismorepracticalthena24-layerBERT-largeandperforms
weightdecay(10‚àí7),andtwotypesofthelearningrateschedules. atparwithBERT-largeonMSMARCOandRobust04[24,31].In
Inourmainexperimentsweusedaconstantlearningratewith ourownexperiments,weseethatlarge(24andmorelayers)model
a20%linearwarm-up[40].However,whenwesimulatedtraining performmuchbetterontheMSMARCOPassagecollection,butwe
inthe‚Äúleaderboarding‚Äùmode,wegeneratedthreesetsof‚Äúhard‚Äù werenotabletooutperform12-layermodelsontheMSMARCO
negativeexamplesusingthreeprogressivelyimprovingcandidate Documentscollection.NotethatLongformer[4]andBigBird[62]
generators.Then,wetrainedthemodelforoneepochoneachof alsohave12layers,butalargerembeddingmatrix.Hence,these
thesesets.Inthefirsttwoepochsweemployedontheconstant modelsareabitlargercomparedtoBERTbase(seeTable3).
learningratewithawarmup,butinthelastepochweswitchedto
aone-cyclelineardecay/warm-upschedule(warm-upfor20%of 3.3 Results
thesteps)[49]tomitigateoverfitting.OnMSMARCO,oneepoch
OurmainexperimentalresultsareshowninTable4,whereanum-
trainingtookfrom6hoursforBERTFirstPto24hoursforLong-
beroflong-documentmodelsarecomparedtothreeFirstPbaselines.
former.
Thesebaselinesusethreedifferent‚Äúbackbone‚ÄùTransformermodels,
Wehavelearnedthat‚Äîunlikeneuralretrievers‚ÄîBERT-base[19]
namely,BERT(base)[19],ELECTRA(base)[11],andLongformer
rankersarerelativelyinsensitivetolearningrates,theirschedules,
[4].ForallFirstPbaselines,thetotallengthofinput(queries,doc-
andthechoiceoflossfunctions.Weweresometimesabletoachieve
uments,andspecialtokens)islimitedto512tokens.TheLongP
betterresultsusingmultiplenegativesperqueryandalistwise
Longformerversion,aswellasotherlong-documentmodels,how-
marginloss(orcross-entropy).However,thegainsweresmalland
ever,usesthelongerinput(1431token).
notconsistentcomparedtoasimplepairwisemarginlossusedin
NotethatPARADEAttndenotesaPARADEAttentionmodel.
ourwork(infact,usingalistwiselossfunctionsometimesleadto
ThePARADETransf prefixdenotesPARADETransformermodels
overfitting).Noteagainthatthisisdifferentfromneuralretrievers
whereanaggregatorTransformercanbeeithertrainedfromscratch
wheretrainingisdifficultwithoutusingalistwiselossand/orbatch-
(Transf-RAND-L3)orinitializedwithapre-trainedmodel(Transf-
negatives[20,29,44,60,63].
PRETR-L6andTransf-PRETR-Q-L6).L3andL6denotethenumber
Exceptforsomeoftheablationstudies,eachmodelwastrained
ofaggregatinglayers(threeandsix,respectively).Transf-PRETR-L6
usingthreeseeds.Tocomputestatisticalsignificance,weaveraged
denotestheoriginalvariantofthePARADETransformermodel
query-specificmetricvaluesovertheseseeds.Querieswerepadded
withapre-trainedsix-layeraggregatorTransformer.Transf-PRETR-
to32BERTtokens(longerqueriesweretruncated).Toenableeffi-
Q-L6)denotesourmodificationofthismodeldescribedin¬ß2.2.2.
cienttrainingandevaluationofthelargenumberofmodels,doc-
As we discuss in ¬ß 3.3.2 our reproduction generally matches
umentsweretruncatedtohaveatmost1431BERTtokens.Thus,
previouslyreportedresults.Overall,thediscussionofexperimental
longdocumentweresplitintoatmostthreechunkscontaining
resultsissplitintothreemainsubsections:keyobservations3.3.1,
477documenttokens(eachconcatenatedwithupto32queryto-
kensplusthreespecialtokens).Furtherincreaseininputlengthis
virtuallyunhelpful(seethediscussiononp.9).
All experiments were carried out using the FlexNeuART [9]
3https://github.com/searchivarius/long_doc_rank_model_analysis/tree/main
framework,whichemployedLuceneandNMSLIB[8]toprovidere- 4Longformerrequiresacustommodelwrapper,becauseweneedto‚Äúplace‚Äùglobal
trievalcapabilities.DeeplearningsupportwasprovidedviaPyTorch attentiononquerytokens.
Conference‚Äô17,July2017,Washington,DC,USA LeonidBoytsov,TianyiLin,FangweiGao,YutianZhao,JeffreyHuang,andEricNyberg
Table4:Modelrankingperformanceaveragedoverseeds(bestnumbersarebold).Statisticalsignificancenotationisexplained
inthetablefootnote.
Model MSMARCO TRECDL Robust04
dev 2019 2020 2021 2019-2021 title description
MRR NDCG@10 NDCG@20
AvgP 0.389ùëéùëèùëê 0.659ùëé 0.596 ùëèùëê 0.664 ùëê 0.642 ùëê 0.478 ùëèùëê 0.531 ùëèùëê
FirstP(BERT) 0.394 ùëèùëê 0.631 ùëê 0.598 ùëèùëê 0.660 ùëê 0.632 ùëèùëê 0.475 ùëèùëê 0.527 ùëèùëê
FirstP(Longformer) 0.404ùëéùëèùëê 0.657ùëé 0.616 ùëê 0.654 ùëê 0.643 ùëê 0.483 ùëèùëê 0.540 ùëê
FirstP(ELECTRA) 0.417ùëé ùëê 0.652 ùëê 0.642ùëé 0.686ùëé 0.662ùëé ùëê 0.492ùëé ùëê 0.552ùëé ùëê
MaxP 0.392 ùëèùëê 0.648 ùëê 0.615 ùëê 0.665 ùëê 0.644ùëé ùëê 0.488ùëéùëèùëê 0.544ùëéùëèùëê
SumP 0.390 ùëèùëê 0.642 ùëê 0.607 ùëê 0.662 ùëê 0.639 ùëèùëê 0.486 ùëèùëê 0.538 ùëèùëê
CEDR-DRMM 0.385ùëéùëèùëê 0.639 ùëê 0.592 ùëèùëê 0.651 ùëèùëê 0.629 ùëèùëê 0.466 ùëèùëê 0.533 ùëèùëê
CEDR-KNRM 0.379ùëéùëèùëê 0.637 ùëê 0.599 ùëèùëê 0.651 ùëèùëê 0.630 ùëèùëê 0.483 ùëèùëê 0.535 ùëèùëê
CEDR-PACRR 0.395 ùëèùëê 0.640 ùëê 0.615ùëé ùëê 0.667 ùëê 0.643ùëé ùëê 0.496ùëé ùëê 0.549ùëé ùëê
NeuralModel1 0.398 ùëèùëê 0.660ùëé 0.620ùëé ùëê 0.666 ùëê 0.650ùëé ùëê 0.484 ùëèùëê 0.537 ùëèùëê
PARADEAttn 0.416ùëé ùëê 0.647 ùëê 0.626ùëé 0.677 ùëê 0.652ùëé ùëê 0.503ùëé ùëê 0.556ùëé ùëê
PARADEAttn(ELECTRA) 0.431ùëéùëè 0.675ùëéùëè 0.653ùëé 0.705ùëéùëè 0.680ùëéùëè 0.523ùëéùëè 0.581ùëéùëè
PARADEAvg 0.392 ùëèùëê 0.656ùëé 0.617 ùëê 0.660 ùëèùëê 0.646ùëé ùëê 0.483 ùëèùëê 0.534 ùëèùëê
PARADEMax 0.405ùëéùëèùëê 0.652 ùëê 0.626ùëé ùëê 0.680ùëé ùëê 0.655ùëé ùëê 0.489ùëéùëèùëê 0.548ùëé ùëê
PARADETransf-RAND-L3 0.344ùëéùëèùëê 0.591ùëéùëèùëê 0.531ùëéùëèùëê 0.547ùëéùëèùëê 0.555ùëéùëèùëê 0.379ùëéùëèùëê 0.411ùëéùëèùëê
PARADETransf-PRETR-L6 0.406ùëéùëèùëê 0.658ùëé 0.618 ùëê 0.680ùëé 0.655ùëé ùëê 0.510ùëéùëè 0.560ùëé ùëê
PARADETransf-PRETR-Q-L6 0.408ùëéùëèùëê 0.651ùëé ùëê 0.619ùëé ùëê 0.673 ùëê 0.650ùëé ùëê 0.498ùëé ùëê 0.561ùëé ùëê
LongP(Longformer) 0.412ùëé ùëêùëë 0.676ùëéùëè ùëë 0.628ùëé ùëê 0.693ùëé ùëë 0.668ùëéùëè ùëë 0.500ùëé ùëêùëë 0.568ùëé ùëë
LongP(Big-Bird) 0.397 ùëèùëê 0.655 ùëê 0.618 ùëê 0.675 ùëê 0.651ùëé ùëê 0.452ùëéùëèùëê 0.477ùëéùëèùëê
Superscriptsa,b,andcdenoteastatisticalsignificantdifference(atlevel0.05)withrespecttothefollowingbaselines:FirstP(BERT),
PARADEAttn,andPARADEAttn(ELECTRA).ThesuperscriptddenotesadifferencebetweenLongPandFirstPvariantsofLongformer.
reproducibilitynotes3.3.2,andthediscussionofthesurprisingef- ‚Ä¢ ThePARADEAttnmodelbasedonELECTRAoutperforms
fectivenessofFirstPmodels(3.3.3).Thesearefollowedbyadditional Longformerbyanoticeablemarginonalldatasets.5
ablations(3.3.4).
3.3.2 Reproducibilitynotes. Wegenerallyreproducepriorart,in
particular,experimentsbyLietal[31],whoinventedPARADE
3.3.1 KeyObservations. AccordingtoTable4,thebestPARADE models.WematchoroutperformtheirTREC2019-2020resultsand
modelsmatchorexceedaccuracyofothermodelsusedinoureval- slightlyunderperformonRobust04.Notethatoneshouldnotexpect
uation.However,eventhetop-performinglong-documentmodels identicalresultsduetodifferencesintrainingregimesandcandidate
areonlymodestlybetter(by5-6%)thenrespectiveFirstPbaselines, generators.Inparticular,inthecaseofRobust04Lietal[31]use
whichanswersourRQ1. RM3(BM25withapseudo-relevancefeedback[28]),whichismore
Inthat,formanyofthepreviouslyproposedlong-document effectivethanBM25[47](whichweuseonRobust04).Theyalso
modelsthereisanevensmallergaininperformance,whichisnot trainedalltheirbestmodelsstartingfromELECTRA[11]whereas
alwaysstatisticallysignificant.Forexample,forCEDR-PACRR[36] weuseditonlyinPARADEAttn(ELECTRA)andFirstP(ELECTRA)
and Neural Model1 [7], the gains are statistically significant on models.
combinedTRECDL(2019-2021)data,butnotontheMSMARCO AnothercomparisonpointisthepaperbyDaiandCallan[18]
developmentset.In¬ß3.3.3,wearguethatforMSMARCOdatathis whoreportedslightlylowerNDCG@20valuesforbothtitleand
happensbecausethedistributionofrelevantpassagesisskewed descriptionqueriesfortheirMaxP model.Similartoourresults,
greatlytowardsthebeginningofthedocument. theyfoundthedifferencebetweenMaxPandFirstPmodelstobesta-
WithrespecttoRQ2,wecanseethattheLongPvariantofthe tisticallysignificant.Yet,theyreportedanoticeablyhigherrelative
Longformermodelappearstohavearelativelystrongperformance, gain:7.7%vsour3.2%(onthedescriptionqueries).
butsodoestheFirstPversionofLongformer.Thus,wethinkthat Furthermore,wehaddifficultytrainingPARADETransf-RAND-
agoodperformanceofLongformercanbelargelyexplainedby L3model,whichusedarandomlyinitializedthree-layeraggrega-
betterpre-trainingcomparedtotheoriginalBERTmodelrather torTransformernetwork.Toensureitwasnotundertrained,we
thantoitsabilitytoeffectivelyencodelongdocuments.Indeed, trainedthismodelforseveralepochsuntilitstartedoverfitting.
whencomparingFirstP variantsrepresentingtheoriginalBERT Nevertheless,Transf-RAND-L3modelwasstillabout10-20%worse
model[19],theLongformer[4],andELECTRA[11],wecansee comparedtoFirstP.Incontrast,usingPARADETransf modelwith
thatthereisquiteadifferenceintheirperformance.Inthat: apre-trainedaggregatingmodelproducedsomeofthestrongest
models.
‚Ä¢ The FirstP (ELECTRA) baseline outstrips nearly all long- 5Longformerisalsotheslowestmodeltotrainandevaluate(about1.5xslowercom-
documentmodelsthatusetheoriginalBERT; paredtoPARADEAttnmodel).
UnderstandingPerformanceofLong-DocumentRankingModelsthroughComprehensiveEvaluationandLeaderboarding Conference‚Äô17,July2017,Washington,DC,USA
Table 5: Performance of the best PARADE models in the
‚Äúleaderboarding‚Äù mode averaged over seeds (best numbers
arebold).Statisticalsignificancenotationisexplainedinthe
tablefootnote.
Model(ELECTRA-based) MSMARCO TRECDL2019-2021
(dev) (combined)
MRR NDCG@10
PARADEAttn 0.458 0.678
PARADETransf-pretr-L4 0.455 ùëê 0.678
PARADETransf-pretr-Q-L4 0.459 ùëè 0.680
Superscriptsa,b,andcdenoteastatisticalsignificantdifference(at
level0.05)withrespecttoPARADEAttn,PARADETransf-pretr-L4,
andPARADETransf-pretr-Q-L4,respectively. Figure 1: Distribution of relevant document lengths (in
BERT tokens) on MS MARCO (v1) development set (doc.
lengthiscappedat10Ktokens)
Notethatwhenwere-usethethepre-trainedTransformermodel
foraggregationpurposes,wediscardtheembeddinglayer.Thusa
strongperformanceofsuchanaggregatorhaslittletodowiththe
linguisticknowledgeacquiredbymodelduringthepre-trainedina
Tosummarizeobservationsofthissubsection:
self-supervisedfashion[52].
Somewhatsimilarly,wewereunabletotrainstrongRobust04 ‚Ä¢ Mostimportantly,welearnedthatcomparingmodelsthat
modelsfromscratch.Insteadwefine-tunedamodelthatwastrained havecloseperformancecanbequitedifficult,becausere-
onMSMARCO.Forexample,wetrainedthePARADEAttnmodel sultscandependonthetrainingregimeandthechoiceofa
inthreedifferentways(thisablationusedonlyoneseed): candidate-generator;
‚Ä¢ Achievinggoodresultsusingasmall-scalequerycollection
(1) StartingfromtheoriginalBERTmodel[19]witharandomly
(suchasRobust04)mayhingeonlarge-scalepre-training
initializedpredictionhead;
(2) Fine-tuningamodeltrainedonBM25pseudo-labels6[39]; usingeithersupervisedtrainingdatafromanothercollection,
e.g.,MSMARCO,orpseudo-labeling;
(3) Fine-tuninganMSMARCOmodel.
‚Ä¢ Training a model with a randomly-initialized aggregator
Thefine-tunedMSMARCOmodelachievedtheMAPof0.31com-
Transformercanbequitedifficult.Incontrast,re-usinga
paredtoMAP0.284forthemodeltrainedfromrandomlyinitial-
pre-trainedTransformedmodelwithadiscardedembedding
izedhead,whichrepresentsa10%difference.Thisgapis,however,
layercanproducemuchbetterresults.
reducedbystartingfromamodel‚Äúpre-finetuned‚ÄùonBM25pseudo-
labels(MAP0.3).
3.3.3 ASurprisingEffectivenessof FirstPbaselines:IsOurDataBi-
Lastbutnotleast,itwasdifficulttoreproduceourownleader-
ased? AswecouldseefrommainexperimentalresultinTable4,the
boardingexperiments,whereweimprovedperformancebyreplac-
full-documentmodelsonlymarginallyoutperformtherespective
inganensembleofPARADETransf-PRETR-LùëÅ modelswiththe
FirstPbaselines:TheBESTperformingLongPmodelsoutstripped
ensembleofPARADETransf-PRETR-Q-LùëÅ models(ùëÅ denotesthe
respectiveFirstP baselinesbyonly4-6%.Asweshowin¬ß3.3.4,
numberofaggregatorTransformerlayers).Indeed,accordingto
thisisnotduetotruncationofdocuments:Doublingthemaximum
Table4,thereisnosubstantialdifferencebetweenPARADETransf-
inputlengthdoesnotfurtherincreasetheaccuracyofthePARADE
PRETR-L6andPARADETransf-PRETR-Q-L6.Bothmodelsareworse
Attnmodel.Toshedlightonthisphenomenon,weplotthedistri-
thanPARADEAttnandthedifferenceisstatisticallysignificant.At
butionofrelevantdocumentlengths(Fig.1)andcompareitwith
thesametime,PARADEAttnwaslessusefulforleaderboarding.
thepositionsofthefirstrelevantpassageinadocument(Fig.2).
Foramorerealisticreproductioninthe‚Äúleaderboarding‚Äùmode,
BotharemeasuredinthenumberofBERTtokens.
wegeneratedtrainingdatausingthreeprogressivelyimproving
Asareminder,document-levelrelevancelabelsinMSMARCO
first-stageretrievalsystems.WethentrainedthreeELECTRA-based
v1arecreatedbytransferringpassage-levelrelevancetooriginal
modelswithafour-layeraggregatorTransformer,whichwasour
documentsfromwhichpassageswereextracted.However,thismap-
best-performingconfigurationusingthisdata.Asaresult(seeTa-
pingisnotprovidedandweattemptedtorecreateitautomatically.
ble5),ourmodificationPARADETransf-PRETR-Q-L4outperformed
Becausepassageanddocumentcollectionsweregatheredatdiffer-
PARADETransf-PRETR-L4andthedifferencewasstatisticallysig-
enttimes,documenttextsdivergedfromthetheiroriginalversions
nificant.PARADETransf-PRETR-Q-L4alsomatchedPARADEAttn
and,thus,exactmatchingofpassagestodocumentsisgenerally
inperformance.ThisexperimentgaveapositiveanswertoRQ3.
impossible.Inparticular,Hofst√§tteretal.[25]wereabletomatch
AlthoughPARADETransf-PRETR-Q-L4 isonlyslightlybetter
only32%ofthepassages.Wewereabletoobtainamuchmore
thanPARADETransf-PRETR-L4,itsperformanceislessaffectedby
representativematchingstatistics(forabout85%ofthepassages)
achoiceofhyper-parameters(see¬ß3.3.4).
usingapproximatematching.
6Wegeneratedpseudo-labelsbyrunning300KMSMARCOqueriesonRobust04 Specifically,weusedacombinationoffindingalongestcommon
collection substring(threshold0.8)asaprimarymatchingoptionandalongest
Conference‚Äô17,July2017,Washington,DC,USA LeonidBoytsov,TianyiLin,FangweiGao,YutianZhao,JeffreyHuang,andEricNyberg
Table6:Distributionofstart/endpositionsofrelevantpas-
sagesinsidedocuments(chunkssizeis477BERTtokens)
developmentset
inputchunk# FIRA(crowd-sourced)
(estimated)
start end start end
1 85.9% 71.0% 83.8% 76.4%
2 9.1% 14.9% 9.9% 15.3%
3 2.6% 6.1% 2.3% 3.9%
4 1.2% 3.0% 2.2% 2.2%
5 0.6% 1.4% 0.7% 0.9%
6 0.6% 1.2% 0.4% 0.5%
6+ 0.1% 2.5% 0.7% 0.7%
(a)Developmentset(estimatedpositions).Onlythefirst
relevantpassageisconsidered.
firstchunk.Wehypothesizethatsuchaskewmakesiteasyforthe
FirstP modelstoaccuratelyrankdocuments.Intheremainingof
thissub-sectionwetrytoanswerthefollowingquestions:
‚Ä¢ Whatisthesourceofbias?
‚Ä¢ Isthereroomforimprovementbyconsideringadditional
relevantpassagesbeyondthefirstBERTchunks?
PositionBiasofRelevantPassages. InthecaseofRobust04,un-
fortunately,littleisknownabouttheinterfaceandannotationpro-
cedureusedbyrelevanceassessors.Itisquitepossiblethatthey
observedeithercompletedocumentsortheirstartingparts:Judg-
ingcompletelongdocumentslikelyrequiredscrolling.Thus,we
(b)TRECDL2019querysetandcrowd-sourcepositions conjecturethattheirperceptionoftherelevancewaslikelybiased
(FIRAdata:Onlythefirstrelevantpassageisconsidered. towardsthebeginningofadocument.InthecaseoftheMSMARCO
Allpositiverelevancegradesareincluded)
devset,annotatorsjudgedeachdocumentpassageindependently.
Then,passage-levelrelevancelabelsweretransferredtotheoriginal
Figure 2: Distribution of ending positions (in # of BERT
documents.
tokens) of relevant passages inside documents on two MS
Becauseannotatorsdidnotobservecompletedocumentstheir
MARCOv1querysets.
attention was not biased towards the document start. Likewise
annotatorsoftheFIRAdataset(whichusesTRECDL2019queries)
judgedrandomlyselecteddocumentsnippets,whichshouldhave
commonsubsequence(threshold0.7)asafallbackoption.7Please
preventedattentionbias.Furthermore,Hofst√§tteretal.[25]carried
notethatfindingin-documentpassagematchesisprohibitedfor
outadditionalexperimentstoconfirmthatthiswasthecase.We
thepurposeofimprovingleaderboardperformance.However,we
concludethattheMSMARCOdocumentcollectionhasacontent
believeitisfairtousesuchstatisticsforthepurposeofthecurrent
bias,buttheexactnatureofthisbiasisnotclear.Insummary,we
posthocanalysis.
wanttoemphasizethatbothRobust04andMSMARCO,which
Wemanuallyinspectedasampleofmatchedpassagestoensure
arepossiblythemostcommonlyusedretrievaldatasets,arenot
thatthematchingprocedurewasreliable.Moreover,thedistribution
particularlyusefulforbenchmarkinglong-documentmodels.
ofpositionsofrelevantpassagesmatchesthatofarelatedFIRA
dataset[25](seeTable6),wheresuchinformationwascollectedby ConsideringAdditionalRelevantPassages. Althoughplots(see
crowdsourcing. Fig.1)providesomeexplanation,itisnotacompleteone.Foramore
Wealsoobtainedpassage-documentmatchingdatafromthe accurateestimationofpotentialimprovementsfromusinglonger
FIRAdataset[25],wherefine-grainedrelevanceinformationwas documentprefixes,weneedamoredetailedmatchingstatisticsas
crowdsourcedforasmallsetofqueriesfromTRECDL2019.In wellasadditionalassumptionsonmodel‚Äôscapabilitiestorecognize
bothcases,weplotthedistributionofendingpositionsofrelevant relevantpassages.Letusassumethatwhenarelevantpassagefully
passages(seeFig.2aandFig.2b).Whenadocumentcontainsmul- fitsintothemaximumsupporteddocumentprefix(477),theaverage
tiplerelevantpassages,weplotonlythefirstone.Comparedtothe accuracyscore,e.g.MRR,ismaxedoutatùê∂ 1stp.Wealsoassume
overalldistributionofrelevantdocumentlengths,whichhasalong thatthescoreiszerowhenthepassagestartsbeyondthetoken
tail(seeFig.1),thefirst relevantpassageoccurstypicallyinthe number477.
Basedonourassumptions,accordingtoTable6,aFirstPmodel
7Thelongestcommonsubsequencereliesonasliding-windowapproachwherethe
hasachanceto‚Äúscore‚Äùfullyinabout71%ofthecasesontheMS
lengthofthewindowis20%longerthanthelengthofthepassagewearetryingto
match. MARCOv1developmentsetandin76%onTREC2019DLqueries.
UnderstandingPerformanceofLong-DocumentRankingModelsthroughComprehensiveEvaluationandLeaderboarding Conference‚Äô17,July2017,Washington,DC,USA
Table 7: Effect of Document Truncation on Accuracy (Ro- Truncationthreshold. Inourstudy,wetruncateallthedocuments
bust04andPARADEAttn) tohaveatmost1431tokens,whichcorrespondtothreechunkseach
containing477documenttokens,upto32querytokens,andthree
#ofBERTchunks 1 2 3 4 5 6 specialtokens([CLS]andtwo[SEP]).Inpreliminaryexperiments,
wewerenotabletoachieveanygainsonMSMARCObyconsider-
MAP 0.278 0.302 0.314 0.317 0.320 0.317
ingsixchunks.Here,weassestheeffectmoresystematicallyfor
NDCG@20 0.478 0.501 0.509 0.509 0.511 0.510
Robust04andPARADE-Attnmodel.Asinthemainexperiments,
thismodelisfirsttrainedonMSMARCOandthenfine-tunedon
Table8:PerformanceofPARADEmodelsfordifferentwin- Robust04.Whiledoingso,weusethesametruncationthreshold
dowandstridesizesaveragedoverseeds(bestnumbersare duringbothtrainingandtestingsteps.AswecanseefromTable7,
bold).Statisticalsignificancenotationisexplainedintheta- increasingthemaximuminputlengthbeyond1431tokens(three
blefootnote. chunks),hasonlymarginaleffectonaccuracy(lessthan2%gainin
MAPandonly0.3%gaininNDCG@10).
Model MSMARCO TREC2019-2021
Slidingwindowsize/stride. InTable8,wepresentresultsofsensi-
(dev) (combined)
MRR NDCG@10 tivityanalysisofthreePARADEmodelswithrespecttothesizeand
window=35stride=25 strideoftheslidingwindowthatisusedbyPARADEtoaggregate
PARADEAttn 0.399 ùëè 0.638 ùëè query-documentinformation.Wecomparethreemodels:PARADE
PARADETransf-PRETR-L6 0.368ùëé ùëê 0.623ùëé ùëê AttentionandtwovariantsofthePARADETransformermodel.
PARADETransf-PRETR-Q-L6 0.400 ùëè 0.639 ùëè PARADETransf-PRETR-L6useasix-layeraggregatorTransformer,
window=75stride=50 whichisintializaedwithaSentence-BERTMiniLMmodel[46,56].
PARADEAttn 0.411 ùëèùëê 0.651 ùëè PARADETransf-PRETR-Q-L6isourmodificationthatadditionally
PARADETransf-PRETR-L6 0.280ùëé ùëê 0.494ùëé ùëê passesthequeryembeddingstotheaggregatingTransformer.We
PARADETransf-PRETR-Q-L6 0.406ùëéùëè 0.651 ùëè runexperimentswiththreeseedsandshowtheaveragemetric
window=150stride=50
values.
PARADEAttn 0.413 ùëè 0.656 ùëèùëê
First,wecanseethatperformanceofourmodificationPARADE
PARADETransf-PRETR-L6 0.399ùëé ùëê 0.639ùëé
PARADETransf-PRETR-Q-L6 0.410 ùëè 0.645ùëé Transf-PRETR-Q-L6isverystableforallparametervalueswhereas
for some seeds the PARADE Transf-PRETR-L6 performs quite
window=150stride=75
PARADEAttn 0.413 ùëè 0.650 poorly.Furthermore,innearlyallthecases,ourmodificationPA-
PARADETransf-PRETR-L6 0.406ùëé ùëê 0.647 RADE Transf-PRETR-Q-L6 outperforms the original model PA-
PARADETransf-PRETR-Q-L6 0.410 ùëè 0.655 RADETransf-PRETR-L6ofLietal[31]forMSMARCOdevelop-
window=150stride=100 mentandTRECDLsetsalike.
PARADEAttn 0.416 ùëèùëê 0.652
PARADETransf-PRETR-L6 0.406ùëé 0.655 4 CONCLUSION
PARADETransf-PRETR-Q-L6 0.408ùëé 0.650
Wecarryoutacomprehensiveevaluationofseveralrecentmodels
window=300stride=200
forrankingoflongdocuments.Ourfindingsshowasurprising
PARADEAttn 0.406 0.649
effectivenessoftheso-calledFirstP approaches,whichtruncate
PARADETransf-PRETR-L6 0.404 0.644
documentstosatisfytheinput-sequenceconstraintofatypical
PARADETransf-PRETR-Q-L6 0.407 0.645
Transformer model. Furthermore, we find that it is not easy to
Superscriptsa,b,andcdenoteastatisticalsignificantdifference(at
outperformFirstP modelsusingspecializedTransformermodels
level0.05)withrespecttoPARADEAttn,PARADETransf-pretr-L6,
(namelyLongformer[4]andBig-Bird[62],whichusesparsified
andPARADETransf-pretr-Q-L6,respectively.
attention to process long inputs efficiently. We analyze the dis-
tributionofrelevantpassages(insidedocuments)toexplainthis
phenomenon.Wemakethekeysoftwarecomponentspubliclyavail-
Atthesametime,inabout10%ofthecasesthefirstrelevant
able.8
passagestartsinthesecondchunk,whichalsomeanstheyendin
thesecondorthirdone(MSMARCOpassagesareshorterthan477
ACKNOWLEDGMENTS
tokens).Accordingtoourassumptions,theFirstPmodelshouldget
azeroscoreforsuchdocuments.Incontrast,ourfull-document TheBoschGroupiscarbonneutral.Administration,manufacturing
models(whichusesthreechunks)couldpotentiallyachievethe andresearchactivitiesnolongerleaveacarbonfootprint,which
score1.1¬∑ùê∂ 1stp,thus,outperformingFirstPby10%.Yet,theactual includesGPUclustersusedinexperiments.
improvementsareonlyabout5%.
Furthermore,asmanyas4-5%offirstrelevantpassagesendin REFERENCES
chunks4-6.However,aswecanseefromTable7increasingthe [1] [n.d.].MSMARCOleaderboard. https://microsoft.github.io/msmarco/.
prefixlengthbeyondthreechunksisnothelpfulonRobust04(we [2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.2015.NeuralMachine
TranslationbyJointlyLearningtoAlignandTranslate.In3rdInternational
observethesameonMSMARCOwithNeuralModel1[7]).
3.3.4 SensitivityAnalysis/Ablations. 8https://github.com/searchivarius/long_doc_rank_model_analysis
Conference‚Äô17,July2017,Washington,DC,USA LeonidBoytsov,TianyiLin,FangweiGao,YutianZhao,JeffreyHuang,andEricNyberg
ConferenceonLearningRepresentations,ICLR2015,YoshuaBengioandYann NoveltyandHARD.InTREC(NISTSpecialPublication,Vol.500-261).National
LeCun(Eds.). InstituteofStandardsandTechnology(NIST).
[3] PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,Xiaodong [29] VladimirKarpukhin,BarlasOguz,SewonMin,PatrickS.H.Lewis,LedellWu,
Liu,RanganMajumder,AndrewMcNamara,BhaskarMitra,TriNguyen,etal. SergeyEdunov,DanqiChen,andWen-tauYih.2020.DensePassageRetrievalfor
2016.MSMARCO:Ahumangeneratedmachinereadingcomprehensiondataset. Open-DomainQuestionAnswering.InEMNLP(1).AssociationforComputational
arXivpreprintarXiv:1611.09268(2016). Linguistics,6769‚Äì6781.
[4] IzBeltagy,MatthewE.Peters,andArmanCohan.2020.Longformer:TheLong- [30] OmarKhattabandMateiZaharia.2020.ColBERT:EfficientandEffectivePassage
DocumentTransformer.CoRRabs/2004.05150(2020). SearchviaContextualizedLateInteractionoverBERT.InSIGIR.ACM,39‚Äì48.
[5] AdamBergerandJohnLafferty.1999.Informationretrievalasstatisticaltrans- [31] CanjiaLi,AndrewYates,SeanMacAvaney,BenHe,andYingfeiSun.2020.PA-
lation.InProceedingsofthe22ndannualinternationalACMSIGIRconferenceon RADE:PassageRepresentationAggregationforDocumentReranking. CoRR
Researchanddevelopmentininformationretrieval.222‚Äì229. abs/2008.09093(2020).
[6] LeonidBoytsov,AnnaBelova,andPeterWestfall.2013.Decidingonanadjust- [32] JimmyLin.2019.Theneuralhypeandcomparisonsagainstweakbaselines.In
mentformultiplicityinIRexperiments.InSIGIR.403‚Äì412. https://doi.org/10. ACMSIGIRForum,Vol.52.ACMNewYork,NY,USA,40‚Äì51.
1145/2484028.2484034 [33] JimmyLin,RodrigoNogueira,andAndrewYates.2021.PretrainedTransformers
[7] LeonidBoytsovandZicoKolter.2021. ExploringClassicandNeuralLexical forTextRanking:BERTandBeyond.Morgan&ClaypoolPublishers.
TranslationModelsforInformationRetrieval:Interpretability,Effectiveness,and [34] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
EfficiencyBenefits.InECIR(1)(LectureNotesinComputerScience,Vol.12656). Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.RoBERTa:A
Springer,63‚Äì78. RobustlyOptimizedBERTPretrainingApproach.CoRRabs/1907.11692(2019).
[8] LeonidBoytsovandBilegsaikhanNaidan.2013.Engineeringefficientandeffective [35] IlyaLoshchilovandFrankHutter.2017.Decoupledweightdecayregularization.
non-metricspacelibrary.InInternationalConferenceonSimilaritySearchand arXivpreprintarXiv:1711.05101(2017).
Applications.Springer,280‚Äì293. [36] SeanMacAvaney,AndrewYates,ArmanCohan,andNazliGoharian.2019.CEDR:
[9] LeonidBoytsovandEricNyberg.2020. FlexibleretrievalwithNMSLIBand ContextualizedEmbeddingsforDocumentRanking.InSIGIR.ACM,1101‚Äì1104.
FlexNeuART.InProceedingsofSecondWorkshopforNLPOpenSourceSoftware [37] SeanMacAvaney,AndrewYates,SergeyFeldman,DougDowney,ArmanCohan,
(NLP-OSS).32‚Äì43. andNazliGoharian.2021.SimplifiedDataWranglingwithir-datasets.InSIGIR.
[10] PeterF.Brown,StephenDellaPietra,VincentJ.DellaPietra,andRobertL.Mercer. [38] TomasMikolov,IlyaSutskever,KaiChen,GregoryS.Corrado,andJeffreyDean.
1993.TheMathematicsofStatisticalMachineTranslation:ParameterEstimation. 2013.DistributedRepresentationsofWordsandPhrasesandtheirComposition-
ComputationalLinguistics19,2(1993),263‚Äì311. ality.InNIPS.3111‚Äì3119.
[11] KevinClark,Minh-ThangLuong,QuocV.Le,andChristopherD.Manning.2020. [39] IuriiMokrii,LeonidBoytsov,andPavelBraslavski.2021. ASystematicEvalu-
ELECTRA:Pre-trainingTextEncodersasDiscriminatorsRatherThanGenerators. ationofTransferLearningandPseudo-LabelingwithBERT-BasedRankingMod-
InICLR.OpenReview.net. els. AssociationforComputingMachinery,NewYork,NY,USA,2081‚Äì2085.
[12] CharlesL.A.Clarke,NickCraswell,andIanSoboroff.2004.OverviewoftheTREC https://doi.org/10.1145/3404835.3463093
2004TerabyteTrack.InTREC(NISTSpecialPublication,Vol.500-261).National [40] MariusMosbach,MaksymAndriushchenko,andDietrichKlakow.2020. On
InstituteofStandardsandTechnology(NIST). theStabilityofFine-tuningBERT:Misconceptions,Explanations,andStrong
[13] CharlesL.A.Clarke,NickCraswell,andIanSoboroff.2009. Overviewofthe Baselines.CoRRabs/2006.04884(2020).
TREC2009WebTrack.InTREC(NISTSpecialPublication,Vol.500-278).National [41] RodrigoNogueiraandKyunghyunCho.2019.PassageRe-rankingwithBERT.
InstituteofStandardsandTechnology(NIST). CoRRabs/1901.04085(2019).
[14] KevynCollins-Thompson,PaulN.Bennett,FernandoDiaz,CharlieClarke,and [42] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
EllenM.Voorhees.2013.TREC2013WebTrackOverview.InTREC(NISTSpecial Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.
Publication,Vol.500-302).NationalInstituteofStandardsandTechnology(NIST). 2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.In
[15] RonanCollobert,JasonWeston,L√©onBottou,MichaelKarlen,KorayKavukcuoglu, Advancesinneuralinformationprocessingsystems.8026‚Äì8037.
andPavelKuksa.2011. Naturallanguageprocessing(almost)fromscratch. J. [43] MatthewEPeters,MarkNeumann,MohitIyyer,MattGardner,Christopher
Mach.Learn.Res.12(2011),2493‚Äì2537. Clark,KentonLee,andLukeZettlemoyer.2018. Deepcontextualizedword
[16] NickCraswell,BhaskarMitra,EmineYilmaz,DanielCampos,andEllenM. representations.InProceedingsofNAACL-HLT.2227‚Äì2237.
Voorhees. 2020. Overview of the TREC 2019 deep learning track. CoRR [44] YingqiQu,YuchenDing,JingLiu,KaiLiu,RuiyangRen,WayneXinZhao,Daxi-
abs/2003.07820(2020). angDong,HuaWu,andHaifengWang.2021.RocketQA:AnOptimizedTraining
[17] NickCraswell,BhaskarMitra,EmineYilmaz,DanielCampos,EllenM.Voorhees, ApproachtoDensePassageRetrievalforOpen-DomainQuestionAnswering.In
andJimmyLin.2022.OverviewoftheTREC2021deeplearningtrack. NAACL-HLT.AssociationforComputationalLinguistics,5835‚Äì5847.
[18] ZhuyunDaiandJamieCallan.2019. DeeperTextUnderstandingforIRwith [45] AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever.2018.Im-
ContextualNeuralLanguageModeling.InSIGIR.ACM,985‚Äì988. provinglanguageunderstandingwithunsupervisedlearning.Technicalreport,
[19] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT: OpenAI(2018).
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding. [46] NilsReimersandIrynaGurevych.2019.Sentence-BERT:SentenceEmbeddings
(2019),4171‚Äì4186. usingSiameseBERT-Networks.InProceedingsofthe2019ConferenceonEmpirical
[20] ThibaultFormal,CarlosLassance,BenjaminPiwowarski,andSt√©phaneClinchant. MethodsinNaturalLanguageProcessingandthe9thInternationalJointConference
2021.SPLADEv2:SparseLexicalandExpansionModelforInformationRetrieval. onNaturalLanguageProcessing(EMNLP-IJCNLP).3982‚Äì3992.
CoRRabs/2109.10086(2021). [47] StephenRobertson.2004.Understandinginversedocumentfrequency:ontheo-
[21] JiafengGuo,YixingFan,QingyaoAi,andW.BruceCroft.2016.ADeepRelevance reticalargumentsforIDF.JournalofDocumentation60,5(2004),503‚Äì520.
MatchingModelforAd-hocRetrieval.InCIKM.ACM,55‚Äì64. [48] AlexanderMRush.2018.Theannotatedtransformer.InProceedingsofworkshop
[22] JiafengGuo,YixingFan,LiangPang,LiuYang,QingyaoAi,HamedZamani,Chen forNLPopensourcesoftware(NLP-OSS).52‚Äì60.
Wu,WBruceCroft,andXueqiCheng.2019.Adeeplookintoneuralranking [49] LeslieN.Smith.2017.CyclicalLearningRatesforTrainingNeuralNetworks.In
modelsforinformationretrieval.InformationProcessing&Management(2019), WACV.IEEEComputerSociety,464‚Äì472.
102067. [50] YuSun,ShuohuanWang,Yu-KunLi,ShikunFeng,XuyiChen,HanZhang,Xin
[23] SebastianHofst√§tter,BhaskarMitra,HamedZamani,NickCraswell,andAllan Tian,DanxiangZhu,HaoTian,andHuaWu.2019.ERNIE:EnhancedRepresen-
Hanbury.2021. Intra-DocumentCascading:LearningtoSelectPassagesfor tationthroughKnowledgeIntegration.CoRRabs/1904.09223(2019).
NeuralDocumentRanking.InSIGIR.ACM,1349‚Äì1358. [51] YiTay,MostafaDehghani,DaraBahri,andDonaldMetzler.2020. Efficient
[24] SebastianHofst√§tter,MarkusZlabinger,andAllanHanbury.2020.Interpretable& Transformers:ASurvey.CoRRabs/2009.06732(2020).
Time-Budget-ConstrainedContextualizationforRe-Ranking.InECAI(Frontiers [52] IanTenney,DipanjanDas,andElliePavlick.2019.BERTRediscoverstheClassical
inArtificialIntelligenceandApplications,Vol.325).IOSPress,513‚Äì520. NLPPipeline.InACL(1).AssociationforComputationalLinguistics,4593‚Äì4601.
[25] SebastianHofst√§tter,MarkusZlabinger,MeteSertkan,MichaelSchr√∂der,and [53] Juli√°nUrbano,M√≥nicaMarrero,andDiegoMart√≠n.2013.Onthemeasurement
AllanHanbury.2020.Fine-GrainedRelevanceAnnotationsforMulti-TaskDocu- oftestcollectionreliability.InSIGIR.393‚Äì402. https://doi.org/10.1145/2484028.
mentRankingandQuestionAnswering.InCIKM.ACM,3031‚Äì3038. 2484038
[26] KaiHui,AndrewYates,KlausBerberich,andGerarddeMelo.2018.Co-PACRR:A [54] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
Context-AwareNeuralIRModelforAd-hocRetrieval.InWSDM.ACM,279‚Äì287. AidanNGomez,≈ÅukaszKaiser,andIlliaPolosukhin.2017. AttentionisAll
[27] SamuelHustonandWBruceCroft.2014.Acomparisonofretrievalmodelsusing youNeed.InNIPS.5998‚Äì6008.
termdependencies.InProceedingsofthe23rdACMInternationalConferenceon [55] EllenVoorhees.2004. OverviewoftheTREC2004RobustRetrievalTrack.In
ConferenceonInformationandKnowledgeManagement.111‚Äì120. TREC.
[28] NasreenAbdulJaleel,JamesAllan,W.BruceCroft,FernandoDiaz,LeahS.Larkey, [56] WenhuiWang,FuruWei,LiDong,HangboBao,NanYang,andMingZhou.
XiaoyanLi,MarkD.Smucker,andCourtneyWade.2004.UMassatTREC2004: 2020.MiniLM:DeepSelf-AttentionDistillationforTask-AgnosticCompression
ofPre-TrainedTransformers.(Feb.2020).arXiv:2002.10957[cs.CL]
UnderstandingPerformanceofLong-DocumentRankingModelsthroughComprehensiveEvaluationandLeaderboarding Conference‚Äô17,July2017,Washington,DC,USA
[57] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue, [60] LeeXiong,ChenyanXiong,YeLi,Kwok-FungTang,JialinLiu,PaulN.Bennett,
AnthonyMoi,PierricCistac,TimRault,R√©miLouf,MorganFuntowicz,Joe JunaidAhmed,andArnoldOverwijk.2021.ApproximateNearestNeighborNeg-
Davison,SamShleifer,PatrickvonPlaten,ClaraMa,YacineJernite,JulienPlu, ativeContrastiveLearningforDenseTextRetrieval.InICLR.OpenReview.net.
CanwenXu,TevenLeScao,SylvainGugger,MariamaDrame,QuentinLhoest, [61] ZeynepAkkalyoncuYilmaz,ShengjinWang,WeiYang,HaotianZhang,and
andAlexanderM.Rush.2019. HuggingFace‚ÄôsTransformers:State-of-the-art Jimmy Lin. 2019. Applying BERT to Document Retrieval with Birch. In
NaturalLanguageProcessing.ArXivabs/1910.03771(2019). EMNLP/IJCNLP(3).AssociationforComputationalLinguistics,19‚Äì24.
[58] YonghuiWu,MikeSchuster,ZhifengChen,QuocV.Le,MohammadNorouzi, [62] ManzilZaheer,GuruGuruganesh,KumarAvinavaDubey,JoshuaAinslie,Chris
WolfgangMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,Jeff Alberti,SantiagoOnta√±√≥n,PhilipPham,AnirudhRavula,QifanWang,LiYang,
Klingner,ApurvaShah,MelvinJohnson,XiaobingLiu,LukaszKaiser,Stephan andAmrAhmed.2020.BigBird:TransformersforLongerSequences.InNeurIPS.
Gouws,YoshikiyoKato,TakuKudo,HidetoKazawa,KeithStevens,GeorgeKurian, [63] GeorgeZerveas,NavidRekabsaz,DanielCohen,andCarstenEickhoff.2021.
NishantPatil,WeiWang,CliffYoung,JasonSmith,JasonRiesa,AlexRudnick, CODER:AnefficientframeworkforimprovingretrievalthroughCOntextualized
OriolVinyals,GregCorrado,MacduffHughes,andJeffreyDean.2016.Google‚Äôs DocumentEmbeddingReranking.ArXivabs/2112.08766(2021).
NeuralMachineTranslationSystem:BridgingtheGapbetweenHumanand [64] LixinZou,ShengqiangZhang,HengyiCai,DehongMa,SuqiCheng,Shuaiqiang
MachineTranslation.CoRRabs/1609.08144(2016). Wang,DaitingShi,ZhicongCheng,andDaweiYin.2021.Pre-trainedLanguage
[59] ChenyanXiong,ZhuyunDai,JamieCallan,ZhiyuanLiu,andRussellPower.2017. ModelbasedRankinginBaiduSearch.InKDD.ACM,4014‚Äì4022.
End-to-EndNeuralAd-hocRankingwithKernelPooling.InSIGIR.ACM,55‚Äì64.

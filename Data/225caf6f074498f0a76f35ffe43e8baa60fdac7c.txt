Beyond Contrastive Learning: A Variational Generative Model for
Multilingual Retrieval
JohnWieting1,JonathanH.Clark1,WilliamW.Cohen1,
GrahamNeubig2,andTaylorBerg-Kirkpatrick3
1GoogleDeepMind
2CarnegieMellonUniversity,Pittsburgh,PA,15213,USA
3UniversityofCaliforniaSanDiego,SanDiego,CA,92093,USA
{jwieting,jhclark,wcohen}@google.com,gneubig@cs.cmu.edu,tberg@eng.ucsd.edu
Abstract whichencouragessourceseparation,separatingse-
manticinformationthatissharedbetweentransla-
Contrastivelearninghasbeensuccessfullyused
tionsfromstylisticorlanguage-specificvariation.
forretrievalofsemanticallyalignedsentences,
Wefindthatbyfilteringthisvariationintoseparate
butitoftenrequireslargebatchsizesandcare-
variables,performanceoftheremainingrepresen-
fully engineered heuristics to work well. In
tations, thatencode sharedsemanticinformation,
this paper, we instead propose a generative
model for learning multilingual text embed- increasesacrossalldownstreamtasks.
dingswhichcanbeusedtoretrieveorscoresen- Throughanapproximationthatgreatlyreduces
tencepairs.Ourmodeloperatesonparalleldata
the memoryfootprint of ourmodel, we scaleour
inN languagesand,throughanapproximation
model and train on 92 languages. We systemat-
weintroduce,efficientlyencouragessourcesep-
ically compare our model, the Variational Multi-
arationinthismultilingualsetting,separating
lingualSource-SeparationTransformer(VMSST) semantic information that is shared between
translationsfromstylisticorlanguage-specific tostrongcontrastiveandgenerativebaselinesona
variation. Weshowcarefullarge-scalecompar- suiteoftasksincludingsemanticsimilarity,bitext
isonsbetweencontrastiveandgeneration-based mining,andquestionretrieval,whichweintroduce
approachesforlearningmultilingualtextem- forthecross-lingualsetting,usingthesametrain-
beddings,acomparisonthathasnotbeendone
ingdataandarchitecture. Weshowthatourmodel
tothebestofourknowledgedespitethepop-
outperformsthesemodelsandisalsocompetitive
ularityoftheseapproaches. Weevaluatethis
withthestate-of-the-art.
method on a suite of tasks including seman-
ticsimilarity,bitextmining,andcross-lingual We analyze VMSST with careful ablations,
questionretrieval‚Äîthelastofwhichweintro- showing the contribution of each aspect of the
duce in this paper. Overall, our Variational model to performance. We also show that even
Multilingual Source-Separation Transformer
atlargebatchsizes,theadvantageovercontrastive
(VMSST) model outperforms both a strong
learningremains,especiallyforlargemodels. Fur-
contrastive and generative baseline on these
thermore,wealsofindthelearnedembeddingspace
tasks.1
of our model to be smoother, making it less af-
1 Introduction fected by the ‚Äúhubness problem‚Äù (Radovanovic
etal.,2010;Radovanovic¬¥ etal.,2010)inrepresen-
Contrastivelearningisthedominantparadigmfor
tation learning, and more suitable for large-scale
learningtextrepresentationsfromparalleltext(Her-
retrievalthanthebaselinemethods.
mannandBlunsom,2014;Singlaetal.,2018;Guo
To the best of our knowledge, this is the first
etal.,2018;Wietingetal.,2019;Fengetal.,2022).
work to systematically compare generative and
However,contrastivelearningrequiresstrongneg-
contrastive models for learning multilingual em-
ativeexamplesinthedataandfindingthesenega-
beddings on a large parallel corpus containing
tivescanbeexpensiveintermsofcomputeorman-
many languages in a carefully controlled experi-
ualeffort. Inthispaper,weproposeagenerative2
mentalsetup‚Äîdespitethepopularityoftheseap-
model for learning multilingual text embeddings
proaches(ArtetxeandSchwenk,2019b;Yangetal.,
1Code and Flax-based T5X model checkpoint 2020). Wecarryouttheseexperimentswithboth
available at https://github.com/google-research/ pretrained and randomly initialized models. The
google-research/tree/master/vmsst.
comparisonofobjectivefunctionsisanimportant
2Wemeangenerativebothintermsoftextgenerationand
asastatisticalmodelofthejointprobabilitydistribution. researchquestionduetothelargeamountsofmulti-
3202
nuJ
4
]LC.sc[
2v62701.2122:viXra
lingualtextavailabletotrainmodelsandthemany Gaussian Prior Gaussian Prior
usesofthesemodelsindownstreamtasks. Tothat
ùí©(0,ùêº) ùí©(0,ùêº)
end,anothercontributionofthispaperisshowing
these comparisons and the surprising result that
Language-specific Common Semantic
contrastive objectives do not provide the overall Latent Vector Latent Vector
bestaccuracyondownstreamtasks. Moreover,our
Shared
generativeVMSST increasinglyoutperformsthe Linguistic variation z z semantic
specific to languages l sem
i variation
contrastivemodelwhenmorelayersareaddedand l 1 to lN
whentrainingwithlargerbatchesandmoretraining
N Language N Language Semantic
data, suggesting that as models continue to scale Inference Nets Decoders Inference Net
inthefuture, thisperformancegapmaycontinue
toincreasefurthermotivatingtheuseofgenerative
approaches for learning multilingual text embed- Observed Sentences x l
i
dings. N
2 RelatedWork
N-way Translations
Therehasbeenanumberofapproachesproposed Figure1: Thegenerativeprocessofourmodel. Latent
forlearningbilingualandmultilingualtextembed- variables, z , modeling the variation x specifically
li li
dings. Onepopularapproachiscontrastivelearn- duetolanguagel i,aswellasalatentvariablemodeling
thecommonsemantics,z ,aredrawnfromamulti-
ing (Hermann and Blunsom, 2014; Singla et al., sem
variateGaussianprior. Theobservedtranslationineach
2018;Guoetal.,2018;Wietingetal.,2019;Feng
language is then conditioned on its language-specific
et al., 2022) where translation pairs are positive
variable and z . In practice, we approximate this
sem
examplesandtextfromotherpairsareusedasneg-
modeltomakelearningandinferencetractable.
ativeexamples. Analternativeapproachistousea
neuralmachinetranslationobjective,wheretherep-
ure1andFigure2respectively. Inthegenerative
resentation from the hidden states of the encoder
storyforVMSST,wefirstsampleasemanticvari-
isusedasthesentenceembedding(Espana-Bonet
ablez forthesentence. ThenforeachoftheN
et al., 2017; Schwenk and Douze, 2017; Artetxe sem
languages,wesamplealanguage-specificvariable
andSchwenk, 2019b). Otherapproaches include
z . Eachlatentvariablez issampledfromamul-
multi-task learning approaches which often use li
tivariateGaussianpriorN(0,I ). Thesevariables
sometypeofcontrastivelearningofparalleltextto k
are then fed into a decoder that samples each of
alignrepresentationsamonglanguages(Yangetal.,
the N sentences in the translation set. Each ob-
2020;Goswamietal.,2021),cross-lingualpretrain-
served translation x , is sampled conditioned on
ing(Chietal.,2022),andmodeldistillationfroma li
z and its language variable z . Because z
largepretrainedmultilingualmodel(Reimersand sem li sem
willbeusedtogeneratethesampledsentencesin
Gurevych,2020).
alllanguages,weexpectthatthisvariablewillen-
Analternativeapproachthatismorecloselyre-
code semantic, syntactic, or stylistic information
latedtoourworkisgenerativemodelsthatseparate
thatissharedinallofthetranslations. Conversely,
thelinguisticvariationfromthesharedsemanticin-
the language variables z will handle language-
formationintranslationpairs. Wietingetal.(2020) li
specific peculiarities or specific style differences
consideredthisforbitext,witheachlanguagehav-
that are not central to the meaning of the transla-
ingitsownencoderanddecoderparameters. This
tionandarethereforenotcontainedinmanyofthe
approachhoweverdoesnotscale,sinceitisnotfea-
sentences. Concretely, the likelihood function of
sibletohavethousandsofencodersanddecoders
ourmodelcanbewrittenforasingleN-waytuple
if one wants to model all of the more than 7,000
oftranslationsx = (x ,...,x ):
languagesintheworld. 1 N
N
(cid:89)
3 Model p(x|z ,z ,...,z ) = p(x |z ,z )
sem l1 lN i sem li
i
The generative process of our underlying proba-
bilistic model and the computation graph of our Inthenextsection,wediscusshowthissepara-
training objective procedure are depicted in Fig- tionofinformationisencouragedduringlearning.
4 LearningandInference unnecessarilydoublestheoverallcost. Ineffect,we
canviewtheselanguage-specificlatentvariablesas
We would like to train our model on a set of par-
collectinginformationthatcannotbecapturedina
allelsentencesX consistingofM examplesinN
commonsemanticspace,separatingitoutfromthe
languages and a collection of latent variables Z.
variables collecting shared semantic information
However, N-way parallel corpora are not avail-
thatweusefordownstreamtasks.
ableatthescaleofbilingualtext,andsowethere-
fore approximate an N-way parallel corpus by ObjectiveFunction. Theoverallobjectivefunc-
sampling translation pairs from a large pool of tionforVMSSTconsistsofconsistsoftwoterms,
pairscontainingtextinN languages. Thereforein thefirstbeingELBOasdescribedearlier:
our model, X = {‚ü®x1,x1‚ü©,...,‚ü®xM,xM‚ü©} and
li lj li lj ELBO = E [logp(X|Z ,Z ;Œ∏)]‚àí
Z = (‚ü®z1,z1,z1 ‚ü©,...,‚ü®zM,zM,zM ‚ü©). q(ZS,ZL|X;œï) S L
li lj sem li lj sem
We aim to maximize the likelihood of the ob- KL(q(Z S,Z L|X;œï)||p(Z S;Œ∏)p(Z L;Œ∏))
served X with respect to the parameters of the
where Z is the collection of semantic variables,
S
decoder Œ∏, marginalizing over the latent vari-
whileZ isthecollectionoflanguagevariables.
L
ables Z. We follow established procedures for
Thesecondterm,whichwefoundnecessaryfor
thisoptimizationproblemfromrelatedlatentvari-
strong performance, is the sum of p(x |¬µ )
ablemodelslikevariationalautoencoders(VAEs; li sem lj
andp(x |¬µ )whichcanbeinterpretedassam-
Kingma and Welling (2013)). Specifically, we lj sem li
ples from the mean of the posterior distribution
optimize a variational lower bound on the log
usingsemanticvariablesgeneratedfrombothinput
marginal likelihood, the evidence lower bound
sentences. When training variational objectives,
(ELBO).ELBOintroducesavariationalapproxima-
where the model ignores the latent variables and
tionq(z ,z ,z |x ,x ;œï)tothetrueposterior
sem li lj li lj
thelearnedposteriorremainsclosetotheprior. Ex-
ofthemodel. Theq distributionisparameterized
amplesofotherapproachestoaddresstheseissues
by encoders or inference networks with parame-
include: (Yang et al., 2017; Kim et al., 2018; Xu
tersœï. ELBOcanbeoptimizedbygradientascent
andDurrett,2018;Heetal.,2019). Weweightthe
byusingthereparameterizationtrick(Kingmaand
ELBObyŒªgivingthetotalobjectiveas:
Welling, 2013), which allows for the expectation
underq tobeapproximatedthroughsamplingina
(cid:88)
w ana dy et nh ca ot dp ere rsse ar rv ee ds ib sa cc uk ssp er dop ia ng fa ut ri to hn e. rT dh ee tad ile ic nod Se er cs
-
p(x li|¬µ
sem
lj)+p(x lj|¬µ
sem
li)+ŒªELBO
(x ,x )‚ààX
li lj
tion5.
In contrast to variational autoencoders, which Therefore, our objective resembles translation
have only a single latent variable for each exam- withaweightedsource-separationterm. Weshow
ple,wehavethreeinourmodelforeachexample. theeffectivenessofthisformulationcomparedto
Toencouragesourceseparation,wemakeseveral apuretranslationobjectiveinourexperimentsin
independenceassumptionsforq andfactoritinto Section6.
threeterms:
5 Architecture
Our architecture is an encoder-decoder model,
q(z sem,z li,z lj|x li,x lj;œï) = where the encoder produces a single representa-
q(z |x ,x ;œï)q(z |x ;œï)q(z |x ;œï) tion that is fed into the decoder. Cross-attention
sem li lj li li lj lj
betweentheencoderanddecoderisnotused,there-
Lastly, we note that the ELBO contains a KL forethedecoderhasnofullsequencevisibilityand
termthatactstoregularizethelatentvariables. In morepressureisappliedontheencodertocreate
ourmodel,theKLtermencouragesz ,z ,and asemanticallymeaningfulrepresentation. Specif-
sem li
z to be close to a zero-centered Gaussian prior. ically, we follow the approach of Wieting et al.
lj
The KL term thus encourages source separation, (2020)whichusesaTransformer(Vaswanietal.,
asencodinginformationsharedbythetranslation 2017)encoder-decodermodel,wherethesentence
pairinthesharedvariableresultsinonlyasingle embeddingsareusedintwoplaces: ateachlayer
penaltyfromtheKLloss,whileencodingtheinfor- ofthedecoderinplaceofcross-attentionandinthe
mationseparatelyinthelanguage-specificvariables computationofthelogits.
Inference Networks
Li Inference Net
q E(z nl ci .| Ex mi; bœï s.) SL pan
.
g Lu aa yg ee rŒº s- li ,Œ£lj Latent Space Generative Model
KL between prior and approximate
Lan eg mu ba eg de
d
e in nc go sder
Merging
posteriors
Language decoder
p(x li|z sem,z li;Œ∏)
X i Semantic Inference Net operation Semantic ùí©(Œºli,Œ£li) embeddings concat Decoder X li
Input q(z sem|x li,x lj;œï) Œºsem,Œ£sem E zn sc eod ming
translation pair Concatenate
f ao fnr r d ola mln j sg pau oma og lp e oles fd l i Enc. Embs. S Lem aya en rt sic merge latens t d a v em e ccp otl doe erd s and tR rae nc oo slfn a s i tnt ir opu nuc tt p i ao in r
translations pairs ùí©(Œºsem,Œ£sem)
concat Decoder
X j
q(z lj|x
lj;Lj
œï
In )ference Ne Œºt
li,Œ£lj
ùí©(Œºlj,Œ£lj)
p(x lj|z sem,z lj;Œ∏)
X lj
Language-
Enc. Embs. Sp. Layers
Figure2: ThecomputationgraphforthevariationallowerboundusedtotrainVMSST.Thetextforlanguagesl
i
andl ,withtheirrespectivelanguageembeddings,arefedintotheencoderactingastheirinferencenetworks. The
j
textisalsofedintothesemanticinferencenetworkwhichisaseparateencoder. Theoutputofthesenetworksare
thelanguagevariablesz andz andsemanticvariablez . Eachlanguage-specificvariableisthenconcatenated
li lj sem
toz andusedbyasingleshareddecodertoreconstructtheinputsentencepair.
sem
Decoder Architecture. The decoder models pair should be similar regardless of the language
p(x |z ,z ;Œ∏) for each language i (see right oftheinputsentence. Weusethemeanofthese-
li sem li
sideofFigure2). Theinputstothedecoderarethe manticencoderasthesentencerepresentationfor
language-specificvariablez andthesemanticvari- downstreamtasks.
li
ablez ,whichareconcatenatedandusedtocon-
sem
6 Experiments
ditionthedecodertogeneratethereconstructionof
theobservedtextx . Weuseasingledecoderfor
li 6.1 ConstructingtheTrainingData
alllanguages.
We follow Artetxe and Schwenk (2019b) in con-
structing our training data. However, since the
EncoderArchitecture. Theencodersplayanim-
exact data is not publicly available, we expect
portant role in the source separation as well as
theirmaybesmalldifferencesduetorandomsam-
inferenceasdetailedbelow.
plinganddifferentdatasetversions. Morespecifi-
Inordertomotivatetheseparationofthelinguis-
callywesampleourdatafromEuroparl,3 United
ticandsemanticinformationwesplittheencoder
Nations(RafalovitchandDale,2009),4 OpenSub-
into two parts, only sharing the embedding table.
titles2018 (Lison et al., 2018),5 Global Voices,6
We use one of these encoders to be the semantic
Tanzil,7 andTatoebav2021-07-22.8
inference network, which produces the semantic
Wesamplethesameamountofdataaswasdone
variable. TheotherencoderrepresentstheN lan-
in Artetxe and Schwenk (2019b), detailed in Ap-
guage inference networks and produces the lan-
pendix B. The only deviation being that we take
guagevariablesforeachlanguage. Theseinference
caretonotincludeanyTatoebatestdatainourtrain-
networks are shown on the left side of Figure 2.
ingdata. Ourfinalcorpushasnearly216million
Wemean-poolthehiddenstatesfollowedbyalin-
training examples, slightly less than 220 million
ear projection to produce each variable from the
reportedinArtetxeandSchwenk(2019b). Weuse
encoders.
both English and Spanish as pivot languages, so
Thesemanticinferencenetwork,whichmodels
each pair includes at least one English or Span-
q(z |x ,x ;œï), is a multilingual encoder that
sem li lj
ishsentence,andweuseapproximatelythesame
encodeseachlanguage. Foreachtranslationpair,
wealternatewhichofthetwoparallelsentencesis 3http://opus.nlpl.eu/Europarl.php
fed into the semantic encoder within a batch for 4https://opus.nlpl.eu/UN.php
5http://opus.nlpl.eu/OpenSubtitles.php
theELBOtermintheobjective. Sincetheseman-
6https://opus.nlpl.eu/GlobalVoices.php
ticencoderismeanttocapturelanguageagnostic 7https://opus.nlpl.eu/Tanzil.php
semanticinformation,itsoutputsforatranslation 8https://opus.nlpl.eu/Tatoeba.php
amountofdataforeachlanguage. Wenotethatwe cosinesimilarityinbothdirectionsforall112lan-
onlyhavetrainingdatafor92languagesinsteadof guages (19 are unseen in the training data) and
the93inArtetxeandSchwenk(2019b)duetonot averagethisscoreforalllanguages.
havingtrainingdataforAymara(ay). The goal of the BUCC task is to find the gold
alignedparallelsentencesgiventwocorpora(one
6.2 Evaluation being very large) in two distinct languages. Lan-
Weevaluateonthreetasks: semanticsimilarity,bi- guagesarealignedwithEnglishandconsistofGer-
textminingandquestionretrieval. Whilethefirst man (de), French (fr), Russian (ru), and Chinese
two are commonly used to evaluate multilingual (zh). Typically,onlyabout2.5%ofthesentences
sentence embeddings, we introduce question re- arealigned. FollowingSchwenk(2018),weeval-
trievalinthispaper. Ascanbeseenbyourresults, uate on the publicly available BUCC data. This
wefoundquestionretrievaltobesomewhatuncor- involvesscoringallpairsbetweenthesourcetarget
related to either of the latter two. For each task, sentences and finding the optimal threshold that
we use a collection of different datasets, detailed separates the data. Using the threshold, we can
below. computetheprecision,recall,andF 1 ofthealign-
ments. WereportF √ó100inourresults.
1
Semantic Textual Similarity The goal of the Wecomparetwodifferentapproachesforfind-
semantic textual similarity tasks is to predict the ing the sentence alignments. In the first, BUCC
degreetowhichsentenceshavethesamemeaning (cosine),wecomputethecosinesimilaritybetween
as measured by human judges. The evaluation thenon-EnglishsourcesentencesandtheEnglish
metric is Pearson‚Äôs r √ó100 with the gold labels, targetsentences,selectingthehighestscoringEn-
whichisconventionforthesetasks. glishsentenceasthematch. Inthesecond,BUCC
We make a distinction between two seman- (margin),wefollowArtetxeandSchwenk(2019a)
ticsimilarityevaluations,English-onlyandcross- and use a margin-based scoring approach, where
lingual. For the English-only evaluation, we fol- thefinalscoreofasentencepairisbothafunction
lowWietingetal.(2016)byaveragingtheyearly ofthescorebetweenthepairandthescoresofeach
performance on 2012‚Äì2016 SemEval Semantic sentence with its nearest neighbors. To compute
TextualSimilarity(STS)sharedtasks(Agirreetal., thismarginscore,wedividethecosinesimilarity
2012,2013,2014,2015,2016). Morespecifically, for source sentence s and target sentence t by
i i
for each year of the competition, we average the thesumofthescoresofthefournearestneighbors
Pearson‚Äôs r √ó 100 for each dataset in that year, ofs withthetargetsentencesandthesumofthe
i
and then finally average this result for each year scoresofthefournearestneighborsoft withthe
i
of the competition. For the cross-lingual evalua- sourcesentences.
tion we use the cross-lingual STS tasks from Se- Margin-based scoring is designed to alleviate
mEval 2017 (Cer et al., 2017). This evaluation the‚Äúhubnessproblem‚Äù(Radovanovicetal.,2010;
containsArabic-Arabic,Arabic-English,Spanish- Radovanovic¬¥ etal.,2010)wheretheneighborhood
Spanish, Spanish-English, and Turkish-English around embeddings in a high-dimensional space,
STSdatasets. Thesedatasetswerecreatedbytrans- likeinsentenceembeddings,havemanyneighbors
lating one or both pairs of an English STS pair incommon. Theseneighborscandisplacethecor-
intoArabic(ar),Spanish(es),orTurkish(tr). We rectmappingintheordering,hurtingperformance.
averagePearson‚Äôsr√ó100forthesedatasets.
Question Retrieval For our question retrieval
Bitext Mining For bitext mining, we use the evaluation, we report the accuracy (R@1) on the
TatoebadatasetintroducedinArtetxeandSchwenk testsetsofNaturalQuestions(NQ)(Kwiatkowski
(2019b) and the 2018 Building and Using Par- etal.,2019)andtheMultilingualKnowledgeQues-
allel Corpora (BUCC) shared bitext mining tionsandAnswers(MKQA)(Longpreetal.,2021).
task(Zweigenbaumetal.,2018). WeusethetheProbablyAskedQuestionsdataset
TheTatoebadatasetconsistsof100‚Äì1000pairs (PAQ) (Lewis et al., 2021) as a knowledge base
ofdataalignedtoEnglishfor112languages. The from which we look up the nearest neighbor of
accuracy for Tatoeba can be computed in two eachquestionintheNQandMKQAtestsetsusing
ways,dependingifEnglishisthetargetlanguage cosinesimilarity. PAQisaverylargeresourceof65
or source language. We compute accuracy using million automatically generated question-answer
pairs. This is a zero-shot evaluation without any contrastive losses like triplet loss (Weston et al.,
NQsuperviseddata.9 2010)whichhasbeenusedforlearningtextembed-
dings,butweleaveacomparisonofcontrastiveob-
OverallScore Weconsolidatealloftheseevalua-
jectivesforlearningmultilingualtextembeddings
tionsintoascore,asawaytogetasenseofoverall
forfuturework.
performancesincedifferentmodelsfavordifferent
ThesecondbaselineisBITRANSLATION,where
evaluations. Whileweareaveragingdifferentmet-
weuseatranslationobjectivetolearntherepresen-
rics(accuracy,Pearson‚Äôsr,andF ),wejustifythis
1 tation Espana-Bonet et al. (2017); Schwenk and
astheydohavethesamescale,10 andasimpleav-
Douze(2017);ArtetxeandSchwenk(2019b).
eragegivesawayforustoseeoverallperformance.
We also explore an alternative to the VMSST,
Ourscoreistheaverageofsixsubtasks,twosub-
VMSST CONTRASTIVE,byincorporatingacon-
tasksforeachofsemanticsimilarity,bitextmining,
trastive loss to use in a multitask setting. Again,
andquestionretrieval: Englishsemanticsimilarity,
weweightthecontributionoftheVMSSTlossby
cross-lingualsemanticsimilarity,Tatoeba,BUCC
Œª.
(weaverageperformanceofthecosineandmargin
basedscoring),NQ,andMKQA.
6.4 ExperimentalSettings
6.3 Baselines
Weexplorethreedifferentsettingsforeachoffour
WecompareVMSSTagainsttwostrongbaselines, objectivefunctionsweconsider. WeusetheTrans-
whichhavebeenusedextensivelyintheliterature. formerarchitectureforallsettings. Specifically,we
Thefirstbaselineis CONTRASTIVE,wherewe explorea6layerencoder-decodermodel,a24layer
use contrastive learning with the other sentences encoder-decoder model, and a 24 layer encoder-
inthebatch(‚Äúin-batchnegativesampling‚Äù)asneg- decoderinitializedwiththeMultilingualT5(mT5)
ative examples (Sohn, 2016). CONTRASTIVE is Large(Xueetal.,2021). Wesetthedimensionof
computedastheaverageofcomputingp(s |t )and theembeddingsandhiddenstatesfortheencoders
i i
p(t |s )forsourcesentences andtargetsentence anddecodersto1024. ThemT5Largemodelinher-
i i i
t , and their respective representations s and t entlyhasembeddingandhiddenstatedimensions
i i i
where the first term uses all the other targets as of1024. Forallmodels,weusethemT5vocabu-
negativesandthesecondusealloftheothersource lary,whichisderivedfromsentencepiece(Kudo
sentenceasnegatives. Specifically, and Richardson, 2018). The vocabulary consists
of250,000tokensandwaslearnedfrommultilin-
gual variant of the C4 dataset called mC4 which
(cid:88)
p(s |t ) = exp(s ¬∑t )/ exps ¬∑t includes101languages.
i i i i i j
j‚ààB Foroptimization,weuseAdafactor(Shazeerand
(cid:88)
p(t |s ) = exp(s ¬∑t )/ expt ¬∑s Stern,2018). Weusethesamelearningratesched-
i i i i i j
uleasVaswanietal.(2017),i.e.,thelearningrate
j‚ààB
1 (cid:88) increaseslinearlyfor4,000steps,afterwhichitis
loss = ‚àí logp(s |t )+logp(t |s )
2|B| i i i i decayedproportionallytotheinversesquarerootof
(si,ti)‚ààB thenumberofsteps. Wesetthepeaklearningrate
tobe0.001,andwetrainourmodelsfor100,000
whereBisaminibatch. Thisversionofcontrastive
stepstotal. Weuseabatchsizeof2048andsetthe
learninghasbeenusedinrepresentationlearning
maximumsequencelengthofourmodelto32for
forretrieval(DPR,Karpukhinetal.,2020),visual
allexperiments.
tasks(SimCLR,Chenetal.,2020)andimage/text
Weuseadropoutrateof0.1for CONTRASTIVE
tasks(CLIP,Radfordetal.,2021). Thereareother
models and no dropout for BITRANSLATION,
variationsofthisloss(Qianetal.,2019),andother
VMSST CONTRASTIVE (with the exception of
9Thisisopposedtotheformulationintheoriginalpaper therandomlyinitialized24layermodelwhichused
whereamodelbasedonBARTLarge(Lewisetal.,2020a)was
0.1), and VMSST. For VMSST, we anneal the
fine-tunedusingaRAG-likeobjective(Lewisetal.,2020b)
ontheNQtrainingdatainamodeltheauthorscallRePAQ. KLtermsothatitincreasedlinearlyfor1,000,000
RePAQ,withoutusingarerankerachievesanaccuracyof41.2 updates.
onNQ.
For VMSST, we set Œª, the weight on the
10TechnicallyPearson‚Äôsrcanbenegative,butthisdoesnot
happeninourevaluations. VMSST ELBOlossterm,tobe0.025forthepre-
Model Sem.Sim. BitextMining Quest.Retrieval Score
Eng. XL XL(s.) XL(d.) Tatoeba BUCC(c.) BUCC(m.) NQ MKQA
RandomInit.(6Layer)
CONTRASTIVE 65.5 66.8 73.3 62.4 63.1 66.2 84.0 34.1 17.6 53.7
BITRANSLATION 69.6 63.9 71.6 58.7 53.3 62.1 81.2 37.4 19.2 52.5
VMSSTCONTRASTIVE 65.7 66.3 73.0 61.9 63.2 65.8 84.3 34.1 17.7 53.7
VMSST 70.1 67.4 75.1 62.2 58.7 73.7 85.9 37.3 20.1 55.6
RandomInit.(24Layer)
CONTRASTIVE 64.4 64.6 71.6 60.0 62.7 64.3 83.7 32.8 16.0 52.4
BITRANSLATION 71.2 68.1 74.6 63.8 57.4 70.8 86.9 38.2 21.6 55.9
VMSSTCONTRASTIVE 68.2 69.7 75.5 65.9 64.8 58.5 84.1 36.9 18.9 55.0
VMSST 71.1 71.7 77.7 67.7 61.4 78.7 89.0 38.3 22.3 58.1
Pretrained(24Layer)
CONTRASTIVE 73.3 74.7 76.0 73.9 85.1 74.3 93.7 40.2 27.6 64.2
BITRANSLATION 74.0 78.0 79.8 76.8 78.2 85.9 91.9 40.9 29.6 64.9
VMSSTCONTRASTIVE 73.4 75.4 76.7 74.6 85.4 74.6 93.7 40.3 27.9 64.4
VMSST 74.6 79.1 81.5 77.5 81.1 87.8 92.5 40.8 29.9 65.9
Table1: ExperimentalresultsforVMSSTandVMSST CONTRASTIVEandourbaselinesCONTRASTIVEand
BITRANSLATION. Weevaluateonsemanticsimilarity,bitextmining,andquestionretrieval. Forsemanticsimilarity
weseparatetheevaluationsintoEnglish-only,cross-lingual,cross-lingualbutwiththesamelanguage(XL(s.) ar-ar
andes-es)andcross-lingualusingdifferentlanguages(XL(d.),ar-en,es-en,andtr-en). Resultsarereportedasthe
averagePearson‚Äôsr√ó100acrossdatasets. ForbitextminingweevaluateonTatoebaandBUCC,withBUCCsplit
betweenusingcosinesimilarityorusingamarginapproach(ArtetxeandSchwenk,2019a). Resultsarereportedas
accuracy√ó100forTatoebaandF √ó100forBUCC.Forquestionretrieval,weevaluateretrievalaccuracy√ó100
1
usingPAQasaquestionknowledgebaseontheNQandMKQAdatasets. Finally,wecomputeascoretosummarize
qualityovertheseevaluations.
trained models, and 0.1 when training from ran- andaddingadditionalrandomlyinitializedparam-
domlyinitializedparameters. For VMSST CON- eterstothedecoder. Perhapsdifferentpretraining
TRASTIVE, we set it to .0005 for the pretrained strategies using this modified decoder would re-
and 6 layer settings and 0.001 for the randomly solvethesedifferences. Wealsoseethat VMSST
initialized24layersetting. CONTRASTIVE hasnegligibleimprovementover
CONTRASTIVEwhichwasunexpected‚Äîthatis,a
6.5 Results
traditional contrastive loss does not improve fur-
The results of our experiments are shown in Ta- therontopofgenerativelossofVMSST.Weleave
ble1. Overall,VMSSThasthebestperformance theexplorationofdifferentstrategiesofcombining
forallthreeexperimentalsettingsandthebestper- theseapproachestofuturework.
formanceoneachtaskonaverage,withtheexcep- Itisalsointerestingtoobservethestarkperfor-
tionofTatoeba. Infact,forNQquestionretrieval mance difference for different tasks. Bitext min-
withapretrainedmodel,itperformsnearlytothat ing tasks like Tatoeba, and BUCC (m.) for the
of the model trained specifically for this task on pretrained 24 layer model, favor CONTRASTIVE,
NQdatafromLewisetal.(2021)whichhasanac- while semantic similarity, BUCC (c.) and ques-
curacyof41.2. VMSSTandBITRANSLATIONare tionretrievalfavor VMSST,suggestingsomefun-
especiallystrongwhenusingmorelayers,whichis damentaldifferenceinthesetasksfavoring CON-
notthecaseforCONTRASTIVEwhichdeclinesin TRASTIVE. An examination of the Tatoeba and
performancewhenmovingfrom6to24layers. In BUCCdatashowsthatthereareparaphrasesinthe
factat24layers,BITRANSLATIONperformsbetter testset,butaccountingforthesedoesnotseemto
onaveragethan CONTRASTIVE. Perhapsforeven meaningfullyexplainthisperformancedifference.
largermodels,thegapbetweencontrastiveandgen- Lastly,weseethat VMSST outperformsCON-
erativemodelswillincrease. WealsoseethatCON- TRASTIVE on the BUCC task with cosine simi-
TRASTIVE seems to benefit more from pretrain- larity,thoughtheresultsbetweenthetwomodels
ing than VMSST and BITRANSLATION, which arecloserwhenusingmargin. Thissuggeststhat
could possibly be due to VMSST re-purposing the‚Äúhubnessproblem‚Äù(Radovanovicetal.,2010;
Radovanovic¬¥ etal.,2010)wheretheneighborhood Model XL XL(s.) XL(d.)
aroundembeddingsinahigh-dimensionalspaces
mUSE 79.5 81.7 78.1
havemanyneighborsincommon,islessofanis- LASER 69.0 74.3 65.5
XLM-R(NLI/STS-B) 79.0 81.7 77.2
suewhenlearningembeddingswithVMSST.This
XLM-R(Para.) 82.4 82.9 82.1
smootherembeddingspacemayalsocontributeto LaBSE 72.4 74.9 70.7
the stronger results VMSST has on the question
VMSST 79.4 81.9 77.7
retrievaltasks.
Table2: Comparisonstorelatedworkoncross-lingual
6.6 ComparisontoRelatedWork
semanticsimilarity. ResultsarereportedinSpearman‚Äôs
Prior work on learning multilingual embeddings œÅ √ó 100. XL contains all 5 datasets, where XL (s.)
hasexploredavarietyofmodelsutilizingdifferent containsonlythosewherethelanguagesarethesame
strategies and using difference source and types (ar-ar,es-es),andXL(d.) containsthosedatasetswhere
thelanguagesaredifferent(ar-en,es-en,andtr-en).Note
oftrainingdata. However,comparingapproaches
thatmodelsinthistablearenottrainedonthesamedata;
is difficult as they differ in many factors that are
forinstanceLaBSEwastrainedonsubstantiallymore
crucialtoperformance: trainingdata,modelsize,
parallel data and XLM-R (Para.) was trained using a
architecture,vocabulary,trainingtime,andevalu- largeEnglishparaphrasecorpusinadditiontoparallel
ationdatasets. Complicatingmattersfurther,even data.
themetricusedinevaluationforthesamedataset,
thedistancemeasureusedbetweenembeddingsfor
Model Tatoeba
the same dataset, and the specific subsets of the
LASER 65.5
evaluationdatasetsusedcanbedifferent. XLM-R(Para.) 67.1
Themaingoalofthispaperistocomparecon- LaBSE 83.7
trastive and generative losses systematically and VMSST 81.1
uniformly,onthesamedata,metricsandunderly-
ingarchitecture. However,wealsoemphasizethat Table3: ComparisonstorelatedworkonTatoeba. Re-
thebestsystemswecomparearecompetitivewith sultsarereportedasaccuracy√ó100,averagingthexx-
thecurrentstate-of-the-art. Hence,inthissection >en and en->xx directions. Note that models in this
table are not trained on the same data; for instance
wecompare VMSST topublishedresultsofother
LaBSEwastrainedonsubstantiallymoreparalleldata
modelsonsemanticsimilarityandtheTatoebaand
andXLM-R(Para.) wastrainedusingalargeEnglish
BUCCbitextminingtasks. Weprimarilycompare
paraphrasecorpusinadditiontoparalleldata.
againstfivemodelswhichhavethestrongestmulti-
lingualresultsintheliterature: mUSE(Yangetal.,
2020), LASER (Artetxe and Schwenk, 2019b), ing sets using the margin retrieval methods from
XLM-R(NLI/STS-B)andXLM(Para.)(Reimers Artetxe and Schwenk (2019b). The results are
and Gurevych, 2020), and LaBSE (Feng et al., shown in Table 5. Baselines results are taken
2022). fromArtetxeandSchwenk(2019b);Reimersand
Forsemanticsimilarity,weincludeSpearman‚Äôs Gurevych(2020).
œÅinordertocomparetoworkthatsolelyusesthis While VMSST does not have the best per-
correlationmetric. Weusecosineasthesimilarity formance relative to models from the literature
measureforallmodelsintheseevaluations.11 The on any single task, it does have the best overall
resultsareshowninTable2. performance if one averages the results for each
ForTatoeba,wecomparetomethodsthathave task.12 Whilethesemodelssharemuchincommon,
evaluated on all 112 languages, which excludes namely using parallel text and some type of pre-
mUSEasitwasonlytrainedon16languagepairs. trainingorpretrainedmodel,therearedifferences
TheresultsareshowninTable3. Baselinesresults in the exact data and models used, among other
aretakenfromReimersandGurevych(2020). confoundingvariables. Forinstance,LaBSEused
For BUCC, we include results on the train- trainingdataconsistingofsixbillionparallelpairs
across languages and was also trained on mono-
11NotethatmUSEandLaBSEreportresultsusingtheangle
lingual text using a masked language modelling
asthemetricinsteadofitscosineforsemanticsimilaritytasks,
but as they do not evaluate on these specific datasets, we
include the results from Reimers and Gurevych (2020) for 12The average performance for VMSST is 84.3, versus
comparisonwhichusescosinesimilarity. 82.6forLaBSE,and79.3forXLM-R(Para.)
Model Sem.Sim. BitextMining Quest.Retrieval Score
Eng. XL XL(s.) XL(d.) Tatoeba BUCC(c.) BUCC(m.) NQ MKQA
RandomInit.(24Layer)
VMSST 71.1 71.7 77.7 67.7 61.4 78.7 89.0 38.3 22.3 58.1
VMSST(fact.) 67.3 69.9 76.3 65.7 63.0 77.9 90.4 37.3 21.5 57.2
VMSST(4enc.) 71.2 70.2 76.6 66.0 60.8 77.7 88.5 38.4 22.0 57.6
VMSST(12Ldec.) 71.1 70.9 77.4 66.7 61.2 78.4 88.8 38.0 22.2 57.8
VMSST(1Ldec.) 71.0 71.2 77.0 67.4 63.0 79.4 89.1 38.7 22.8 58.5
VMSST(noKL) 70.7 68.7 76.2 63.7 56.9 70.8 86.6 37.8 21.5 55.7
VMSST(1enc.) 70.6 69.4 76.7 64.6 60.0 77.0 87.8 38.4 21.4 57.0
VMSST(noenc.l.e.) 71.2 69.8 76.1 65.5 61.2 78.7 88.9 38.2 22.0 57.7
VMSST(nodec.l.e.) 70.8 70.7 76.7 66.7 60.9 77.4 88.6 38.3 21.8 57.6
Table4: AblationsofVMSST.Weinvestigateablationsinvolvingfactorizationofthedecoderprojectionlayer
(fact.),using4languageencodersinsteadof1(4enc.),using12layer(12Ldec.) and1layer(1Ldec.) decoders,
usingnoKLterm(noKL),usingonlyasingleencoderforbothlanguageandsemanticvariables(1enc.),andusing
noencoderlanguageembeddings(noenc. l.e.) ornodecoderlanguageembeddings(nodec. l.e.).
Model de-en fr-en ru-en zh-en Avg. sionsizeandV isthesizeofthevocabulary.13 If
we factor the projection layer, we can reduce the
mUSE 88.5 86.3 89.1 86.9 87.7
LASER 95.4 92.4 92.3 91.2 92.8 space to d√óV +3d√ód. In practice, this saves
XLM-R(NLI/STS-B) 86.8 84.4 86.3 85.1 85.7
about509millionparametersforour24layermod-
XLM-R(Para.) 90.8 87.1 88.6 87.8 88.6
LaBSE 95.9 92.5 92.4 93.0 93.5 els. HoweverfromthefirstrowinTable4,wesee
that this small change has a significant effect on
VMSST 94.3 91.0 91.8 92.8 92.5
performance,weakeningresultsonsemanticsimi-
larityandquestionretrievaltasksandstrengthening
Table 5: Comparisons to related work on BUCC in
resultsonbitextminingtasks.
accuracy√ó100usingthemarginapproachfrom Artetxe
andSchwenk(2019a). Notethatmodelsinthistableare In our second ablation, VMSST (4 enc.), we
nottrainedonthesamedata;forinstanceLaBSEwas spreadthemodelcapacityofthelanguage-specific
trainedonsubstantiallymoreparalleldataandXLM-R encoderto4encoders,insteadofthesingleencoder
(Para.) was trained using a large English paraphrase
inourpreviousexperiments. Weallocatethelan-
corpusinadditiontoparalleldata.
guagesrandomlytothedifferentencoders. Wefind
thatthisdoesn‚Äôtimproveresults,perhapsbecause
the24layermodelhassufficientcapacitytomodel
objective. XLM-R(Para.) makesuseofa50mil-
all of the languages in one shared encoder. We
lionexampleparaphrasecorpusfordistillation. In
couldallocatelanguagestoencodersbasedonlan-
contrast, our setup most closely follows LASER,
guagefamilies,andperhapsthiscouldfarebetter,
usinganapproximationofthe220Mexamplepar-
butweleavethatforfuturework.
alleldatausedtotraintheirmodel.
Priorwork(Wietingetal.,2020)showsthat, a
7 Analysis decoder that is weaker (i.e. less layers) can lead
tostrongerembeddings. Thiseffectispresumably
7.1 ModelAblations becausethereismorepressureonthesentenceem-
beddingtofullyandclearlycapturethesemantics
In this section, we investigate different ablations
since it cannot rely on a strong decoder to fill in
of VMSST. The ablations are shown in Table 4.
gaps. We found that that using a weaker single
We start from the 24 layer randomly initialized
layer decoder (1L dec.), does indeed seem to im-
VMSST,andchangeittoseehowcertainhyper-
prove performance. We also tried a 12 layer ab-
parametersandmodelchoicesaffectperformance.
lation (12L dec.), but that seemed to not have a
Our first experiment, VMSST (fact.) investi-
significantimprovementintheresults.
gates what happens if we simply factor the final
projectionlayerofthedecoder. Thiscansavealot
13Wemultiplyby3becausewehavethreeembeddings,the
of memory in the model, as that projection layer
hiddenstate,thelanguage-specificvector,andthesemantic
is 3√ód√óV where d is the hidden state dimen- vector.
Model Sem.Sim. BitextMining Quest.Retrieval Score
Eng. XL Tatoeba NQ MKQA
- - ar-en ar-ar es-en es-es tr-en ar es tr - ar es tr
RandomInit.(6Layer)-ar,en,es,tr
CONTRASTIVE 68.6 81.7 68.5 68.3 64.8 69.7 97.9 88.2 98.1 36.4 24.6 13.0 22.6 58.1
BITRANSLATION 69.0 82.4 63.1 57.8 58.7 67.3 97.6 84.3 96.4 37.8 25.4 12.2 21.1 57.0
VMSST 70.6 83.1 65.9 63.1 62.1 68.8 97.9 84.9 97.2 38.1 27.2 14.4 24.1 58.5
VMSST(fullenc.,fulldec.) 70.3 82.3 65.6 60.0 62.8 67.9 98.4 87.9 97.8 39.0 27.2 14.7 24.2 58.7
Table6: ComparisonofVMSSTwithavariationthathasnoparametersharing,VMSST(fullenc.,fulldec.). We
experimenton4languages,sowehave5encodersand4decoders.
Thelastfourablationsinvestigatedifferentmod- lation of VMSST without the source separation.
elling choices. In the first we eliminate the KL However, there is still a gap between the full en-
term (no KL), which has the most significant ef- coder/decoder of VMSST and VMSST. We hy-
fect on performance, especially on cross-lingual pothesize however, that as the number of layers
tasks. In the second ablation, we use a single en- ofthemodelincreases,thisperformancegapalso
coderinsteadofthetwinencoders(1enc.),onefor shrinks. The extra capacity of these layers will
semanticembeddingsandoneforlanguageembed- allowforthemodeltoseparatelanguage-specific
dings,wefindthatthishasamodestoveralleffect variationswithouthavingseparateparametersfor
onperformance. Lastly,weeliminatethelanguage eachlanguage. Evidenceforthishypothesisisin
embeddings. Firstweremovethelanguageembed- Table4wherehavingthelanguagevariationshared
ding inputs to the decoder (no enc. l.e.), then we amongst4encodersinsteadof1actuallyappears
experiment by removing the input language em- toweakenperformanceoverall.
beddingstothelanguage-specificencoder(nodec.
7.3 Zero-ShotBitextMining
l.e.). We find these language embeddings have a
smallerthanexpectedimpactonperformance,per- TheTatoebadatasetcontainsparallelsentencepairs
hapsbecausethelargecapacityofthedecodercan of English with 112 languages. Our model is
ascertainthelanguagebeinginputordecoded. trained using 93 of these languages, and there-
forethereare19languageswecanuseforazero-
7.2 TestingtheParameterSharinginVMSST
shotevaluationofbitextmining. Table8summa-
Parametersharingwasneededinorderefficiently rizes the results of this zero-shot evaluation for
performsourceseparationonN languages. Specif- the two generation objectives, BITRANSLATION
ically we collapsed the language encoders into a and VMSST considered in this paper. The re-
singleencoderandwecollapsedthedecodersintoa sults are shown in Table 8. We also compute ‚àÜ
singledecoder. The VMSST approximateshaving which is the difference between the performance
N languageencodersbyusinganinputembedding gapof VMSST and BITRANSLATION ontheseen
to indicate the language being considered. The and unseen languages. From the results, we see
samestrategyisappliedwiththedecodersaswell, thatVMSST doesevenbetterthanBITRANSLA-
withthefirstinputtokentothedecoderindicating TION onunseenlanguagesthanunseenlanguages.
thelanguagetobegenerated. Since BITRANSLATION can be seen as an abla-
In this section, we investigate what effect this tionof VMSST,i.e. VMSST withoutthesource-
parameter sharing has on VMSST by using N separationloss,weseethatthesource-separation
encoders and decoders (full enc, full dec.). We loss especially helps with generalization to new
experimentwith6layerTransformerencodersand languages.
4languagesSpanish,English,Arabic,andTurkish
7.4 EffectsofBatchSize
inordertokeeptheexperimentstractableasinthis
setting we have 5 encoders and 4 decoders. The Lastly, weinvestigatehow VMSST comparesto
resultsareshowninTable6. CONTRASTIVE asbatchsizeincreases. Itiscom-
The results indicate that the approximation ap- monknowledgethatcontrastivemodelslearnbetter
pears to hold, as VMSST is much closer to the representationswhengivenhardernegativeexam-
fullmodelthan BITRANSLATION,whichisanab- ples. Sinceweareusingin-batchnegativesinour
Model B.Size Sem.Sim. BitextMining Quest.Retrieval Score
Eng. XL XL(s.) XL(d.) Tatoeba BUCC(c.) BUCC(m.) NQ MKQA
RandomInit.(6Layer)
2048 65.5 66.8 73.3 62.4 63.1 64.7 82.9 34.0 17.6 53.5
CONTRASTIVE 4096 67.5 69.3 75.4 65.3 66.0 71.5 87.0 35.3 19.2 56.1
8192 69.4 71.6 76.8 68.1 68.6 76.2 89.4 36.4 20.9 58.3
2048 70.1 67.4 75.1 62.2 58.7 72.6 84.7 37.2 20.2 55.4
VMSST 4096 70.2 67.4 75.3 62.1 58.5 73.1 86.0 38.2 20.3 55.7
8192 71.4 70.9 76.6 67.1 61.8 77.9 88.0 39.0 22.4 58.1
RandomInit.(24Layer)
2048 64.4 64.6 71.6 60.0 62.7 62.8 82.5 32.8 16.0 52.2
CONTRASTIVE 4096 66.6 68.6 75.1 64.3 65.7 70.9 86.8 34.7 18.1 55.4
8192 68.0 70.2 76.2 66.2 67.7 74.2 88.3 35.2 19.4 57.0
2048 71.1 71.7 77.7 67.7 61.4 78.4 87.8 38.3 22.3 58.0
VMSST 4096 72.0 72.1 77.7 68.3 62.9 81.0 89.7 38.7 23.5 59.1
8192 72.7 74.1 79.0 70.8 64.1 82.0 90.2 39.0 24.3 60.1
Table7: ComparisonofCONTRASTIVEandVMSSTusingdifferentbatchsizesduringtraining.
Model Tat.(seen) Tat.(unseen) ‚àÜ amount(billionsofpairs)ofparalleldataavailable.
RandomInit.(6Layer) We experiment with batch sizes of 4096 and
8192, double and quadruple the 2048 used in all
BITRANSLATION 59.3 24.0 -
VMSST 64.4 30.6 1.5 experiments up to this point, for both the 6 layer
RandomInit.(24Layer) and24layerrandomlyinitializedversionsof CON-
BITRANSLATION 82.6 56.5 - TRASTIVE and VMSST. All models are trained
VMSST 84.9 62.2 3.4 againfor100,000steps. Theresultsareshownin
Pretrained(24Layer) Table7.
BITRANSLATION 63.7 26.5 - From the results, we see that for the 6 layer
VMSST 67.3 32.6 2.5 model,increasingthebatchsizeequalizesVMSST
and CONTRASTIVE overall, however each per-
Table 8: Results on languages seen during training forms better at different tasks. CONTRASTIVE
(seen)andlanguagesthatwerenotseenduringtraining has better performance on Tatoeba, XL semantic
(unseen)ontheTatoebadataset.
similarity, and BUCC with margin (Artetxe and
Schwenk,2019a), where VMSST hasbetterper-
formance on English semantic similarity, BUCC
contrastive baseline, the increased batch size in-
withcosinesimilarity,andtheretrievaltasks. For
creases the chances of encountering harder nega-
the24layervariations, VMSST isbetteratevery
tive examples and will generally increase perfor-
task, with the exception of Tatoeba, and has the
manceuptothepointwherethenegativesbecome
highestoverallscoreofanymodelinthetable. The
false. Furthermore. biggerbatchsizesareknown
24layerCONTRASTIVEvariationdoesnotperform
toalsoimproveresultsinmodelsusingtheTrans-
aswellasthe6layerversionatanybatchsize,in
formerarchitecture,presumablyduetolessnoisy
contrastto VMSST wherethe24layermodelal-
gradients,whichwouldimprovetheresultsofboth
waysoutperformsthe6layervariation.
CONTRASTIVE and VMSST. It is important to
note that using bigger batch sizes, means seeing
8 Conclusion
more examples (100,000 steps at a batch size of
2048isabout1passthroughthedata). However, Wepresent VMSST,agenerativemassivelymulti-
parallel data is so numerous that training to con- lingualtextembeddingmodeltrainedtoseparate
vergenceontheavailabledataisnotverypractical. semantic information from language-specific in-
Therefore, these experiments do not separate out formation. VMSSTalsooutperformsstrongcon-
the gains from using a bigger batch size versus trastive and generative baselines on a variety of
seeingmoretrainingdata,butwearguethatisnot tasks. There are several avenues for future work
animportantdistinctiontomakeduetothesheer includingalternativepretrainingobjectivesthatbet-
ter fit the use case of the decoder, explore incor- EnekoAgirre,CarmenBanea,ClaireCardie,DanielCer,
poratingmonolingualdataintothegenerativeob- Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
RadaMihalcea,GermanRigau,andJanyceWiebe.
jective,investigatesynergybetween VMSST and
2014. SemEval-2014task10: Multilingualsemantic
contrastivemethodsastheyseemtospecializein
textualsimilarity. InProceedingsofthe8thInterna-
differenttasks,andlastlyscaleuptobiggermod- tionalWorkshoponSemanticEvaluation(SemEval
els,moredata,andlanguagestofurtherinvestigate 2014),pages81‚Äì91,Dublin,Ireland.Associationfor
ComputationalLinguistics.
VMSST versuscontrastivemethods.
EnekoAgirre,CarmenBanea,DanielCer,MonaDiab,
Limitations
Aitor Gonzalez-Agirre, Rada Mihalcea, German
Rigau, and Janyce Wiebe. 2016. SemEval-2016
Someofourexperiments,specificallythoseinthe task 1: Semantic textual similarity, monolingual
ablations with large batch sizes, required signif- andcross-lingualevaluation. InProceedingsofthe
10thInternationalWorkshoponSemanticEvaluation
icant computational resources. We trained these
(SemEval-2016),pages497‚Äì511,SanDiego,Califor-
models on Google Cloud TPUv3 Pod slice with
nia.AssociationforComputationalLinguistics.
128chipsforafewdays. Thisexperimentisimpor-
tant,asotherwisetherewouldbequestionsonhow Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
themodelscompareatlargebatchsizeswherecon- Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
pilotonsemantictextualsimilarity. In*SEM2012:
trastivemodelsareknowntoworkbetter. Dueto
TheFirstJointConferenceonLexicalandCompu-
trainingcostsandintheinterestofopenresearch, tationalSemantics‚ÄìVolume1: Proceedingsofthe
we will open source our code and model check- main conference and the shared task, and Volume
pointsforthecommunitytouseandbuildupon. 2: ProceedingsoftheSixthInternationalWorkshop
onSemanticEvaluation(SemEval2012),pages385‚Äì
Secondly, VMSST and BITRANSLATION re-
393, Montr√©al, Canada. Association for Computa-
quiredecodingwhichwhichmeanstheyneedmore tionalLinguistics.
memoryforthedecoderandareslowerduringtrain-
ing. However one advantage of these models is EnekoAgirre,DanielCer,MonaDiab,AitorGonzalez-
Agirre,andWeiweiGuo.2013. *SEM2013shared
thattheycanbetrainedwithgradientcheckpoint-
task: Semantic textual similarity. In Second Joint
ing greatly reducing their memory requirements, ConferenceonLexicalandComputationalSemantics
which cannot be used for the contrastive models (*SEM),Volume1: ProceedingsoftheMainConfer-
as that would reduce the effective batch size for enceandtheSharedTask: SemanticTextualSimilar-
ity,pages32‚Äì43,Atlanta,Georgia,USA.Association
finding negative examples. Moreover, during in-
forComputationalLinguistics.
ference, there is no difference in the memory or
speedrequirementsinCONTRASTIVE,BITRANS- Mikel Artetxe and Holger Schwenk. 2019a. Margin-
LATION, or VMSST as only a single encoder is basedparallelcorpusminingwithmultilingualsen-
tence embeddings. In Proceedings of the 57th An-
usedininferenceandthereisnodecoding.
nualMeetingoftheAssociationforComputational
Linguistics,pages3197‚Äì3203,Florence,Italy.Asso-
Acknowledgements
ciationforComputationalLinguistics.
We are grateful to Livio Baldini-Soares, Wenhu Mikel Artetxe and Holger Schwenk. 2019b. Mas-
Chen,ZhuyunDai,TomKwiatkowski,JianmoNi, sively multilingual sentence embeddings for zero-
shotcross-lingualtransferandbeyond. Transactions
SlavPetrov,JasonRiesa,andPatVergaforuseful
of the Association for Computational Linguistics,
discussionsduringthecourseoftheproject.
7:597‚Äì610.
Daniel Cer, Mona Diab, Eneko Agirre, I√±igo Lopez-
References Gazpio, and Lucia Specia. 2017. SemEval-2017
task1: Semantictextualsimilaritymultilingualand
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel crosslingual focused evaluation. In Proceedings
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei of the 11th International Workshop on Semantic
Guo,I√±igoLopez-Gazpio,MontseMaritxalar,Rada Evaluation(SemEval-2017),pages1‚Äì14,Vancouver,
Mihalcea,GermanRigau,LarraitzUria,andJanyce Canada.AssociationforComputationalLinguistics.
Wiebe.2015. SemEval-2015task2: Semantictex-
tual similarity, English, Spanish and pilot on inter- TingChen,SimonKornblith,MohammadNorouzi,and
pretability. InProceedingsofthe9thInternational Geoffrey Hinton. 2020. A simple framework for
WorkshoponSemanticEvaluation(SemEval2015), contrastivelearningofvisualrepresentations. InPro-
pages252‚Äì263,Denver,Colorado.Associationfor ceedingsoftheInternationalConferenceofMachine
ComputationalLinguistics. Learning.
Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma, YoonKim,SamWiseman,AndrewMiller,DavidSon-
BoZheng,SakshamSinghal,PayalBajaj,XiaSong, tag,andAlexanderRush.2018. Semi-amortizedvari-
Xian-LingMao,HeyanHuang,andFuruWei.2022. ationalautoencoders. InInternationalConferenceon
XLM-E:Cross-linguallanguagemodelpre-training MachineLearning,pages2678‚Äì2687.PMLR.
viaELECTRA. InProceedingsofthe60thAnnual
Meeting of the Association for Computational Lin- Diederik P Kingma and Max Welling. 2013. Auto-
guistics(Volume1: LongPapers),pages6170‚Äì6182, encoding variational bayes. arXiv preprint
Dublin,Ireland.AssociationforComputationalLin- arXiv:1312.6114.
guistics.
TakuKudoandJohnRichardson.2018. SentencePiece:
Cristina Espana-Bonet, Ad√°m Csaba Varga, Alberto A simple and language independent subword tok-
Barr√≥n-Cede√±o,andJosefvanGenabith.2017. An enizeranddetokenizerforneuraltextprocessing. In
empiricalanalysisofnmt-derivedinterlingualembed- Proceedings of the 2018 Conference on Empirical
dingsandtheiruseinparallelsentenceidentification. Methods in Natural Language Processing: System
IEEEJournalofSelectedTopicsinSignalProcessing, Demonstrations, pages 66‚Äì71, Brussels, Belgium.
11(8):1340‚Äì1350. AssociationforComputationalLinguistics.
TomKwiatkowski, JennimariaPalomaki, OliviaRed-
FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-
field,MichaelCollins,AnkurParikh,ChrisAlberti,
vazhagan,andWeiWang.2022. Language-agnostic
DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-
BERTsentenceembedding. InProceedingsofthe
tonLee,KristinaToutanova,LlionJones,Matthew
60thAnnualMeetingoftheAssociationforCompu-
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
tationalLinguistics(Volume1: LongPapers),pages
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
878‚Äì891,Dublin,Ireland.AssociationforComputa-
ralquestions: Abenchmarkforquestionanswering
tionalLinguistics.
research. TransactionsoftheAssociationforCompu-
tationalLinguistics,7:452‚Äì466.
Koustava Goswami, Sourav Dutta, Haytham Assem,
Theodorus Fransen, and John P. McCrae. 2021.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Cross-lingualsentenceembeddingusingmulti-task
Ghazvininejad,AbdelrahmanMohamed,OmerLevy,
learning. InProceedingsofthe2021Conferenceon
Veselin Stoyanov, and Luke Zettlemoyer. 2020a.
EmpiricalMethodsinNaturalLanguageProcessing,
BART:Denoisingsequence-to-sequencepre-training
pages9099‚Äì9113,OnlineandPuntaCana,Domini-
fornaturallanguagegeneration,translation,andcom-
can Republic. Association for Computational Lin-
prehension. InProceedingsofthe58thAnnualMeet-
guistics.
ingoftheAssociationforComputationalLinguistics,
pages7871‚Äì7880,Online.AssociationforComputa-
Mandy Guo, Qinlan Shen, Yinfei Yang, Heming
tionalLinguistics.
Ge,DanielCer,GustavoHernandezAbrego,Keith
Stevens, Noah Constant, Yun-Hsuan Sung, Brian
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
Strope,andRayKurzweil.2018. Effectiveparallel
Petroni,VladimirKarpukhin,NamanGoyal,Hein-
corpusminingusingbilingualsentenceembeddings.
richK√ºttler, MikeLewis, Wen-tauYih, TimRock-
InProceedingsoftheThirdConferenceonMachine
t√§schel,etal.2020b. Retrieval-augmentedgeneration
Translation: ResearchPapers,pages165‚Äì176,Brus-
forknowledge-intensivenlptasks. AdvancesinNeu-
sels, Belgium. Association for Computational Lin-
ralInformationProcessingSystems,33:9459‚Äì9474.
guistics.
PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMin-
Junxian He, Daniel Spokoyny, Graham Neubig, and
ervini,HeinrichK√ºttler,AleksandraPiktus,Pontus
Taylor Berg-Kirkpatrick. 2019. Lagging inference
Stenetorp,andSebastianRiedel.2021. PAQ:65mil-
networksandposteriorcollapseinvariationalautoen-
lionprobably-askedquestionsandwhatyoucando
coders. arXivpreprintarXiv:1901.05534. withthem. TransactionsoftheAssociationforCom-
putationalLinguistics,9:1098‚Äì1115.
KarlMoritzHermannandPhilBlunsom.2014. Multi-
lingualmodelsforcompositionaldistributedseman- Pierre Lison, J√∂rg Tiedemann, and Milen Kouylekov.
tics. InProceedingsofthe52ndAnnualMeetingof 2018. OpenSubtitles2018: Statistical rescoring of
theAssociationforComputationalLinguistics(Vol- sentencealignmentsinlarge,noisyparallelcorpora.
ume1: LongPapers),pages58‚Äì68,Baltimore,Mary- InProceedingsoftheEleventhInternationalConfer-
land.AssociationforComputationalLinguistics. enceonLanguageResourcesandEvaluation(LREC
2018), Miyazaki, Japan. European Language Re-
VladimirKarpukhin,BarlasOguz,SewonMin,Patrick sourcesAssociation(ELRA).
Lewis,LedellWu,SergeyEdunov,DanqiChen,and
Wen-tauYih.2020. Densepassageretrievalforopen- Shayne Longpre, Yi Lu, and Joachim Daiber. 2021.
domainquestionanswering. InProceedingsofthe MKQA:Alinguisticallydiversebenchmarkformul-
2020ConferenceonEmpiricalMethodsinNatural tilingualopendomainquestionanswering. Transac-
LanguageProcessing(EMNLP),pages6769‚Äì6781, tionsoftheAssociationforComputationalLinguis-
Online.AssociationforComputationalLinguistics. tics,9:1389‚Äì1406.
QiQian,LeiShang,BaiguiSun,JuhuaHu,HaoLi,and Karan Singla, Dogan Can, and Shrikanth Narayanan.
RongJin.2019. Softtripleloss: Deepmetriclearn- 2018. Amulti-taskapproachtolearningmultilingual
ingwithouttripletsampling. InProceedingsofthe representations. InProceedingsofthe56thAnnual
IEEE/CVF International Conference on Computer Meeting of the Association for Computational Lin-
Vision,pages6450‚Äì6458. guistics(Volume2: ShortPapers), pages214‚Äì220,
Melbourne,Australia.AssociationforComputational
AlecRadford,JongWookKim,ChrisHallacy,Aditya Linguistics.
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
try, Amanda Askell, Pamela Mishkin, Jack Clark, Kihyuk Sohn. 2016. Improved deep metric learning
GretchenKrueger,andIlyaSutskever.2021. Learn- withmulti-classn-pairlossobjective. Advancesin
ingtransferablevisualmodelsfromnaturallanguage neuralinformationprocessingsystems,29.
supervision. InInternationalConferenceonMachine
Learning, volume 139 of Proceedings of Machine Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
LearningResearch,pages8748‚Äì8763.PMLR. Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser,andIlliaPolosukhin.2017. Attentionisall
MilosRadovanovic,AlexandrosNanopoulos,andMir- you need. In Proceedings of Advances in Neural
janaIvanovic.2010. Hubsinspace: Popularnearest InformationProcessingSystems,pages5998‚Äì6008.
neighborsinhigh-dimensionaldata. JournalofMa-
chineLearningResearch,11(sept):2487‚Äì2531. JasonWeston,SamyBengio,andNicolasUsunier.2010.
Largescaleimageannotation: learningtorankwith
MilosRadovanovic¬¥,AlexandrosNanopoulos,andMir-
joint word-image embeddings. Machine learning,
janaIvanovic¬¥.2010. Ontheexistenceofobstinate
81(1):21‚Äì35.
results in vector space models. In Proceedings of
the 33rd international ACM SIGIR conference on JohnWieting,MohitBansal,KevinGimpel,andKaren
Researchanddevelopmentininformationretrieval, Livescu.2016. Towardsuniversalparaphrasticsen-
pages186‚Äì193. tence embeddings. In Proceedings of the Interna-
tionalConferenceonLearningRepresentations.
AlexandreRafalovitchandRobertDale.2009. United
Nations general assembly resolutions: A six-
JohnWieting,KevinGimpel,GrahamNeubig,andTay-
languageparallelcorpus. InProceedingsofMachine
lor Berg-Kirkpatrick. 2019. Simple and effective
TranslationSummitXII:Posters,Ottawa,Canada.
paraphrasticsimilarityfromparalleltranslations. In
Proceedingsofthe57thAnnualMeetingoftheAsso-
Nils Reimers and Iryna Gurevych. 2019. Sentence-
ciationforComputationalLinguistics,pages4602‚Äì
BERT:SentenceembeddingsusingSiameseBERT-
4608,Florence,Italy.AssociationforComputational
networks. InProceedingsofthe2019Conferenceon
Linguistics.
EmpiricalMethodsinNaturalLanguageProcessing
andthe9thInternationalJointConferenceonNatu-
John Wieting, Graham Neubig, and Taylor Berg-
ralLanguageProcessing(EMNLP-IJCNLP),pages
Kirkpatrick. 2020. A bilingual generative trans-
3982‚Äì3992,HongKong,China.AssociationforCom-
formerforsemanticsentenceembedding. InProceed-
putationalLinguistics.
ingsofthe2020ConferenceonEmpiricalMethods
in Natural Language Processing (EMNLP), pages
Nils Reimers and Iryna Gurevych. 2020. Making
1581‚Äì1594,Online.AssociationforComputational
monolingualsentenceembeddingsmultilingualus-
Linguistics.
ing knowledge distillation. In Proceedings of the
2020ConferenceonEmpiricalMethodsinNatural
JiachengXuandGregDurrett.2018. Sphericallatent
LanguageProcessing(EMNLP),pages4512‚Äì4525,
spaces for stable variational autoencoders. In Pro-
Online.AssociationforComputationalLinguistics.
ceedingsofthe2018ConferenceonEmpiricalMeth-
HolgerSchwenk.2018. Filteringandminingparallel ods in Natural Language Processing, pages 4503‚Äì
data in a joint multilingual space. In Proceedings 4513,Brussels,Belgium.AssociationforComputa-
of the 56th Annual Meeting of the Association for tionalLinguistics.
ComputationalLinguistics(Volume2: ShortPapers),
LintingXue,NoahConstant,AdamRoberts,MihirKale,
pages228‚Äì234,Melbourne,Australia.Association
RamiAl-Rfou,AdityaSiddhant,AdityaBarua,and
forComputationalLinguistics.
ColinRaffel.2021. mT5: Amassivelymultilingual
Holger Schwenk and Matthijs Douze. 2017. Learn- pre-trainedtext-to-texttransformer. InProceedings
ingjointmultilingualsentencerepresentationswith ofthe2021ConferenceoftheNorthAmericanChap-
neural machine translation. In Proceedings of the teroftheAssociationforComputationalLinguistics:
2ndWorkshoponRepresentationLearningforNLP, HumanLanguageTechnologies,pages483‚Äì498,On-
pages157‚Äì167,Vancouver,Canada.Associationfor line.AssociationforComputationalLinguistics.
ComputationalLinguistics.
Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,
Noam Shazeer and Mitchell Stern. 2018. Adafactor: JaxLaw,NoahConstant,GustavoHernandezAbrego,
Adaptivelearningrateswithsublinearmemorycost. SteveYuan,ChrisTar,Yun-hsuanSung,BrianStrope,
InInternationalConferenceonMachineLearning, andRayKurzweil.2020. Multilingualuniversalsen-
pages4596‚Äì4604.PMLR. tenceencoderforsemanticretrieval. InProceedings
of the 58th Annual Meeting of the Association for
ComputationalLinguistics: SystemDemonstrations,
pages87‚Äì94,Online.AssociationforComputational
Linguistics.
ZichaoYang, ZhitingHu, RuslanSalakhutdinov, and
TaylorBerg-Kirkpatrick.2017. Improvedvariational
autoencoders for text modeling using dilated con-
volutions. InInternationalConferenceonMachine
Learning,pages3881‚Äì3890.JMLR.org.
PierreZweigenbaum,SergeSharoff,andReinhardRapp.
2018. Overviewofthethirdbuccsharedtask: Spot-
ting parallel sentences in comparable corpora. In
Proceedingsof11thWorkshoponBuildingandUs-
ingComparableCorpora,pages39‚Äì42.
Appendicesaccompanying‚ÄúBeyond knowledgebasefromwhichwelookupthenearest
ContrastiveLearning: AVariational neighborofeachquestionintheNQandMKQA
GenerativeModelforMultilingual testsetsusingcosinesimilarity.
Retrieval‚Äù
A.3 BitextMining
A FullExperimentalResults
For bitext mining, we use the Tatoeba dataset in-
troducedinArtetxeandSchwenk(2019b)andthe
We include full results for our models using the
2018BuildingandUsingParallelCorpora(BUCC)
pre-trainedmT5largecheckpoint. Weevaluateon
shared bitext mining task (Zweigenbaum et al.,
Englishsemanticsimilarity,Cross-lingualsemantic
2018).
similarity,questionretrieval,andbitextmining.
TheTatoebadatasetconsistsof100-1000pairs
A.1 SemanticSimilarity ofdataalignedtoEnglishfor112languages. The
accuracy for Tatoeba can be computed in two
For English semantic similarity, we use the Se-
ways,dependingifEnglishisthetargetlanguage
mEvalsemantictextualsimilarity(STS)tasksfrom
or source language. We compute accuracy using
2012to2016(Agirreetal.,2012,2013,2014,2015,
cosinesimilarityinbothdirectionsforall112lan-
2016) as was done initially for sentence embed-
guages (19 are unseen in the training data) and
dingsin(Wietingetal.,2016). Asourtestset,we
averagethisscoreforalllanguages.
report the average Pearson‚Äôs r over each year of
The goal of the BUCC task is to find the gold
the STS tasks from 2012-2016 as is convention
alignedparallelsentencesgiventwocorpora(one
in the top part of Table 9. However, some recent
being very large) in two distinct languages. Lan-
work,likeReimersandGurevych(2019)computed
guages are aligned with English and consist of
Spearman‚ÄôsœÅoverconcatenateddatasetsforeach
German (de), French (fr), Russian (ru), and Chi-
yearoftheSTScompetition. Tobeconsistentwith
nese (zh). Following Schwenk (2018), we eval-
theseworks,wealsoincludeevaluationsusingthis
uate on the publicly available BUCC data. This
approachinthebottompartofTable9. Oneother
involvesscoringallpairsbetweenthesourcetarget
differencebetweenthesetwowaysofcalculating
sentences and finding the optimal threshold that
the results is the inclusion of the SMT dataset of
separates the data. Using the threshold, we can
the 2013 task. When computing the results us-
computetheprecision,recall,andF ofthealign-
1
ingPearson‚Äôsr,thisdatasetisincluded,butwhen
ments. WereportF √ó100inourresults.
1
computingtheresultsusingSpearman‚ÄôsœÅ,itisnot
Wecomparetwodifferentapproachesforfind-
included.
ing the sentence alignments. In the first, BUCC
Forcross-lingualsemanticsimilarityandseman-
(cosine),wecomputethecosinesimilaritybetween
tic similarity in non-English languages, we eval-
thenon-EnglishsourcesentencesandtheEnglish
uate on the STS tasks from SemEval 2017. This
targetsentences,selectingthehighestscoringEn-
evaluationcontainsArabic-Arabic,Arabic-English,
glishsentenceasthematch. Inthesecond,BUCC
Spanish-Spanish, Spanish-English, and Turkish-
(margin),wefollowArtetxeandSchwenk(2019a)
English datasets. The datasets were created by
anduseamargin-basedscoringapproach.
translating one or both pairs of an English STS
pairintoArabic(ar),Spanish(es),orTurkish(tr).
B FullTrainingData
Followingconvention,wereportresultswithPear-
son‚Äôsr forallsystems,butalsoincluderesultsin We follow Artetxe and Schwenk (2019b) in con-
Spearman‚ÄôsœÅinTable10. structingourtrainingdata,samplingdatafromEu-
roparl,14, United Nations (Rafalovitch and Dale,
A.2 QuestionRetrieval 2009),15 OpenSubtitles2018(Lisonetal.,2018),16,
GlobalVoices,17 Tanzil,18 andTatoebav2021-07-
Forourquestionretrievalevaluation,wereportthe
22.19
accuracy (R@1)onthe testsets ofNaturalQues-
tions (NQ) (Kwiatkowski et al., 2019) shown in 14http://opus.nlpl.eu/Europarl.php
Table 11 and the Multilingual Knowledge Ques- 15https://opus.nlpl.eu/UN.php
16http://opus.nlpl.eu/OpenSubtitles.php
tionsandAnswers(MKQA)(Longpreetal.,2021)
17https://opus.nlpl.eu/GlobalVoices.php
showninTable12. WeusethetheProbablyAsked 18https://opus.nlpl.eu/Tanzil.php
Questionsdataset(PAQ)(Lewisetal.,2021)asa 19https://opus.nlpl.eu/Tatoeba.php
Model EnglishSemanticSimilarity
2012 2013 2014 2015 2016
CONTRASTIVE 69.7 61.1 76.4 81.4 77.7
BITRANSLATION 69.1 63.6 76.4 81.0 79.9
VMSSTCONTRASTIVE 70.2 61.6 76.5 81.3 77.5
VMSST 70.5 64.3 76.5 81.6 80.1
CONTRASTIVE 68.0 74.9 69.1 79.9 76.9
BITRANSLATION 70.7 77.9 72.2 81.8 79.7
VMSSTCONTRASTIVE 68.4 75.1 69.2 80.2 76.8
VMSST 72.7 77.9 72.7 82.1 79.2
Table9: FullresultsonEnglishSTS.Inthefirstpartofthetable,weshowresults,measuredinPearson‚Äôsr√ó100,
foreachyearoftheSTStasks2012-2016aswellastheaverageperformanceacrossallyears. Inthesecondpart,we
evaluatebasedontheSpearman‚ÄôsœÅ√ó100oftheconcatenationofthedatasetsofeachyearwiththe2013SMT
datasetremovedfollowing(ReimersandGurevych,2019).
Model Cross-LingualSemanticSimilarity
ar-ar ar-en es-es es-en tr-en
CONTRASTIVE 72.4 72.2 72.7 74.2 79.7 81.0 71.7 72.0 77.2 77.0
BITRANSLATION 75.6 76.0 77.0 78.6 84.0 84.8 76.2 77.2 77.3 77.5
VMSSTCONTRASTIVE 73.2 73.1 73.5 75.1 80.2 81.3 72.4 72.4 77.8 78.0
VMSST 77.6 78.1 78.5 78.8 85.5 85.7 77.0 77.4 77.0 77.0
Table10:FullresultsonCross-LingualSTS.WereportresultsusingbothPearson‚Äôsr√ó100andSpearman‚ÄôsœÅ√ó100
acrossdatasets,wherePearson‚Äôsr√ó100isthefirstcolumnforeachlanguagepairandSpearman‚ÄôsœÅ√ó100isthe
secondcolumn.
Model NQ
CONTRASTIVE 40.2
BITRANSLATION 40.9
VMSSTCONTRASTIVE 40.3
VMSST 40.8
Table11: FullresultsonquestionretrievalontheNQ
data. Weevaluateretrievalaccuracy√ó100usingPAQ
asaquestionknowledgebase.
The only deviation from their data sampling
approach is that we take care to not include any
Tatoeba test data in our training data. Our final
corpus has nearly 216 million training examples,
slightly less than 220 million reported in Artetxe
and Schwenk (2019b). We use both English and
Spanishaspivotlanguages,soeachpairincludesat
leastoneEnglishorSpanishsentence,andattempt
touseapproximatelythesameamountofdatafor
each language if possible. We note that we only
havetrainingdatafor92languagesinsteadofthe
93inArtetxeandSchwenk(2019b)duetonothav-
ingtrainingdataforAymara(ay). Thefullamount
ofEnglishandSpanishparalleldatausedforeach
ofthe92languagesisreportedinTable15.
Model MKQA
Language ar da de en es fi fr he hu it ja km
CONTRASTIVE 21.4 30.4 29.2 33.2 30.4 27.7 30.0 24.4 26.9 29.4 24.2 23.6
BITRANSLATION 19.4 30.8 30.5 29.8 28.7 29.7 28.0 30.3 27.5 27.9 26.2 23.4
VMSSTCONTRASTIVE 24.6 32.0 30.5 33.4 31.6 29.8 30.9 27.9 28.9 31.0 27.3 24.9
VMSST 21.4 32.1 32.5 31.5 30.3 31.3 30.0 31.9 29.9 30.0 29.9 25.9
Language ko ms nl no pl pt ru sv th tr vi zh
CONTRASTIVE 22.0 30.5 29.4 33.2 30.4 27.7 30.0 24.8 27.5 29.6 24.6 24.2
BITRANSLATION 19.4 30.8 30.9 30.0 29.2 30.2 28.1 30.4 28.0 28.3 26.9 23.7
VMSSTCONTRASTIVE 25.3 32.3 30.6 33.4 31.5 30.1 31.0 28.1 29.9 31.1 27.7 24.9
VMSST 22.0 32.4 32.8 31.6 31.0 31.6 30.4 32.2 30.4 30.2 30.4 26.2
Table12: FullresultsonquestionretrievalontheMKQAdata. Weevaluateretrievalaccuracy√ó100usingPAQasa
questionknowledgebase.
Model Cosine Margin
de fr ru zh de fr ru zh
CONTRASTIVE 84.6 81.3 66.6 64.4 96.2 93.7 92.1 93.0
BITRANSLATION 90.1 85.5 84.1 84.1 93.6 90.3 91.3 92.4
VMSSTCONTRASTIVE 84.8 81.9 67.4 64.4 96.1 93.6 92.1 92.9
VMSST 91.5 86.8 86.7 86.1 94.3 91.0 91.8 92.8
Table 13: Full results on BUCC. We report results using both cosine similarity and the margin approach from
(ArtetxeandSchwenk,2019a). ResultsarereportedasF √ó100.
1
Language afr amh ang ara arq arz ast awa aze bel ben ber bos bre bul cat
CONTRASTIVE 97.6 94.9 66.8 95.0 61.8 86.1 91.3 74.0 95.8 96.8 92.4 80.4 97.5 47.2 96.4 97.8
BITRANSLATION 94.8 84.2 42.9 94.0 42.0 80.3 80.3 56.3 91.3 95.0 91.0 72.6 96.8 18.9 95.3 96.8
VMSSTCONTRASTIVE 97.4 93.5 70.5 94.7 64.2 85.7 90.9 76.0 95.8 97.2 92.8 81.6 97.3 47.9 96.2 97.8
VMSST 95.6 88.1 53.4 94.6 50.2 84.7 86.2 64.3 93.5 95.6 91.9 79.0 97.0 26.1 95.8 97.0
Language cbk ceb ces cha cmn cor csb cym dan deu dsb dtp ell epo est eus
CONTRASTIVE 86.1 62.3 98.3 44.9 97.5 35.1 67.8 57.6 97.3 99.6 75.4 18.9 97.4 98.6 98.6 96.9
BITRANSLATION 78.0 48.8 97.3 33.6 95.6 18.4 48.6 37.0 96.0 99.3 53.1 8.3 95.8 98.3 97.8 94.8
VMSSTCONTRASTIVE 86.9 63.7 98.2 44.2 97.7 37.6 69.8 56.4 97.2 99.6 76.9 20.3 97.2 98.4 98.8 97.1
VMSST 83.7 52.8 97.9 38.0 96.4 23.3 56.1 43.0 96.8 99.2 63.2 10.2 97.0 98.2 98.2 95.4
Language fao fin fra fry gla gle glg gsw heb hin hrv hsb hun hye ido ile
CONTRASTIVE 90.3 98.0 96.4 88.2 58.4 80.4 98.6 52.1 93.8 98.1 98.5 80.1 98.4 96.2 93.0 92.8
BITRANSLATION 78.2 97.8 95.8 80.1 35.6 60.6 96.8 44.9 93.0 96.4 96.8 58.6 96.4 95.2 87.9 86.7
VMSSTCONTRASTIVE 91.0 98.2 96.6 87.9 55.7 79.5 98.6 53.8 93.7 98.0 98.4 82.4 98.2 96.1 94.4 93.1
VMSST 82.6 98.0 96.0 83.5 39.4 62.7 97.4 50.4 93.8 97.5 97.5 68.9 96.8 94.7 91.9 90.2
Language ina ind isl ita jav jpn kab kat kaz khm kor kur kzj lat lfn lit
CONTRASTIVE 96.8 97.0 97.0 96.6 74.1 98.3 71.7 95.6 92.8 87.8 95.2 76.3 17.4 89.9 83.6 98.2
BITRANSLATION 94.9 95.2 96.3 96.2 62.9 96.8 60.8 93.9 86.1 85.4 92.5 60.5 8.8 83.9 74.6 97.5
VMSSTCONTRASTIVE 97.2 96.9 97.1 96.6 76.1 98.6 73.3 96.2 92.3 87.5 95.8 76.0 17.8 89.8 84.1 98.1
VMSST 96.4 95.8 96.8 96.8 69.5 97.2 67.3 95.8 87.7 86.3 93.5 67.7 11.7 86.5 79.0 97.8
Language lvs mal mar max mhr mkd mon nds nld nno nob nov oci orv pam pes
CONTRASTIVE 98.0 98.5 94.5 73.1 30.8 97.6 94.4 90.5 98.1 96.5 98.5 80.5 77.9 66.5 14.0 95.5
BITRANSLATION 97.0 98.2 95.0 58.1 22.1 96.0 85.8 80.7 96.7 92.3 97.4 70.6 66.5 47.5 8.5 93.0
VMSSTCONTRASTIVE 98.1 98.5 94.6 72.5 29.5 98.0 95.0 92.1 98.0 97.0 98.3 80.5 78.0 67.4 14.6 95.7
VMSST 97.5 98.3 95.0 63.6 26.8 96.4 89.8 84.5 97.3 93.5 97.6 76.3 72.1 55.9 9.9 94.5
Language pms pol por ron rus slk slv spa sqi srp swe swg swh tam tat tel
CONTRASTIVE 74.3 99.0 95.9 98.0 95.3 98.0 97.0 99.1 98.6 96.6 97.5 76.8 77.4 92.7 92.0 98.1
BITRANSLATION 62.1 97.2 95.7 97.6 95.0 97.4 96.3 98.8 98.1 95.6 96.8 48.7 67.1 90.9 82.8 96.2
VMSSTCONTRASTIVE 76.9 98.9 96.1 98.1 95.5 98.2 97.0 99.2 98.6 96.5 97.5 73.2 77.6 92.8 92.0 97.6
VMSST 69.6 98.2 95.8 97.6 94.8 97.8 96.9 98.6 98.2 95.8 97.3 59.8 69.2 92.8 86.2 97.4
Language tgl tha tuk tur tzl uig ukr urd uzb vie war wuu xho yid yue zsm
CONTRASTIVE 96.0 97.8 44.6 98.9 66.3 76.1 96.0 95.4 78.9 98.2 54.0 93.5 74.6 92.3 94.1 98.0
BITRANSLATION 91.7 97.0 30.3 98.2 43.8 54.4 95.2 91.8 64.3 97.4 33.3 88.7 59.5 82.8 90.8 96.0
VMSSTCONTRASTIVE 96.4 98.2 47.5 98.9 64.4 77.6 96.3 95.4 78.3 98.4 54.5 93.8 75.7 92.6 94.1 97.8
VMSST 93.0 97.5 38.7 98.8 56.7 64.1 95.5 93.5 68.7 97.9 37.7 91.0 63.7 86.0 93.0 96.6
Table14: FullresultsonTatoeba. Wereportresultsasaccuracy√ó100.
Language af am ar ay az be ber bg
TrainingPairs 77,772 101,613 7,907,914 0 291,925 6,330 142,061 4,834,661
Language bn br bs ca cbk cs da de
TrainingPairs 1,148,461 34,472 4,166,739 895,940 1,623 5,429,060 7,767,119 8,707,293
Language dtp dv el en eo es et eu
TrainingPairs 1,064 98,320 6,601,989 4,913,379 447,622 4,913,379 5,093,003 1,432,979
Language fi fr ga gl ha he hi hr
TrainingPairs 7,785,493 8,935,842 1,112 391,824 134,775 4,046,554 358,907 3,911,368
Language hu hy ia id ie io is it
TrainingPairs 5,256,214 8,194 12,048 4,326,151 2,445 3,181 2,712,556 8,468,538
Language ja ka kab kk km ko ku kw
TrainingPairs 3,981,886 360,136 26,460 6,172 3,266 2,566,495 98,733 3,463
Language kzj la lfn lt lv mg mhr mk
TrainingPairs 614 27,515 6,096 3,629,769 2,119,995 537,953 69 4,037,896
Language ml mr ms my nb nds nl oc
TrainingPairs 867,026 52,340 3,288,492 4,802 9,694 6,263 8,346,102 730
Language pl ps pt ro ru sd si sk
TrainingPairs 5,407,190 32 8,276,190 4,814,046 9,416,934 98,412 1,016,660 5,094,752
Language sl so sq sr sv sw ta te
TrainingPairs 5,099,577 98,976 3,619,914 3,977,191 7,680,683 201,379 150,023 42,877
Language tg th tl tr tt ug uk ur
TrainingPairs 135,245 3,849,777 34,829 5,854,059 132,273 101,989 1,687,685 844,052
Language uz vi wuu yue zh
TrainingPairs 148,860 3,905,401 929 4,525 7,636,488
Table15: Fulltrainingdataforeachlanguage. ThetotalnumberofpairsisthesumofusingEnglishandSpanishas
pivotlanguages.

DEEP: DEnoising Entity Pre-training for Neural Machine Translation
JunjieHu1,HiroakiHayashi3,KyunghyunCho2,GrahamNeubig3
1UniversityofWisconsin-Madison,2NewYorkUniversity,3CarnegieMellonUniversity
junjie.hu@wisc.edu, kyunghyun.cho@nyu.edu
{hiroakih,gneubig}@cs.cmu.edu
Abstract lemwillbefurtherexacerbatedinthecross-domain
transfersettingsorinthecaseofemergingentities.
It has been shown that machine translation
Because of this, there have been a number of
models usually generate poor translations for
methodsproposedspecificallytoaddresstheprob-
namedentitiesthatareinfrequentinthetrain-
lemoftranslatingentities. AsnotedbyLiu(2015),
ing corpus. Earlier named entity translation
earlierstudiesonnamedentitytranslationlargely
methods mainly focus on phonetic transliter-
ation, which ignores the sentence context for focusedonrule-basedmethods(WanandVerspoor,
translation and is limited in domain and lan- 1998),statisticalalignmentmethods(Huangetal.,
guage coverage. To address this limitation, 2003,2004)andWebminingmethods(Huangetal.,
we propose DEEP, a DEnoising Entity Pre- 2005; Wu and Chang, 2007; Yang et al., 2009).
training method that leverages large amounts
However,thesemethodshavetwomainissues.First,
of monolingual data and a knowledge base
as they generally translate a single named entity
to improve named entity translation accuracy
withoutanycontextinasentence,itmakesitdiffi-
within sentences. Besides, we investigate a
multi-task learning strategy that finetunes a culttoresolveambiguityinentitiesusingcontext.
pre-trained neural machine translation model Inaddition,thetranslationofentitiesisoftenper-
on both entity-augmented monolingual data formedinatwo-stepprocessofentityrecognition
andparalleldatatofurtherimproveentitytrans- thentranslation,whichcomplicatesthetranslation
lation. Experimentalresultsonthreelanguage
pipelineandcanresultincascadingerrors(Huang
pairsdemonstratethatDEEPresultsinsignifi-
etal.,2003,2004;Chenetal.,2013).
cantimprovementsoverstrongdenoisingauto-
encoding baselines, with a gain of up to 1.3 In this paper, we focus on a simple yet effec-
BLEUandupto9.2entityaccuracypointsfor tivemethodthatimprovesnamedentitytranslation
English-Russiantranslation.1 within context. Specifically, we do so by devis-
ingadataaugmentationmethodthatleveragestwo
1 Introduction datasources: monolingualdatafromthetargetlan-
guage and entity information from a knowledge
Propertranslationofnamedentitiesiscriticallyim-
base(KB).Ourmethodalsoadoptsaprocedureof
portantforaccuratelyconveyingthecontentoftext
pre-training and finetuning neural machine trans-
inanumberofdomains,suchasnewsorencyclope-
lation(NMT)modelsthatisusedbymanyrecent
dictext(KnightandGraehl,1998;Al-Onaizanand
works(LuongandManning,2015;NeubigandHu,
Knight,2002a,b). Inaddition,agrowingnumber
2018; Song et al., 2019; Liu et al., 2020). In par-
ofnewnamedentities(e.g.,personname,location)
ticular,pre-trainingmethodsthatusemonolingual
appeareveryday,thereforemanyoftheseentities
data to improve translation for low-resource and
maynotexistintheparalleldatatraditionallyused
medium-resource languages mainly rely on a de-
totrainMTsystems. Asaresult,evenstate-of-the-
noisingauto-encodingobjectivethatattempttore-
artMTsystemsstrugglewithentitytranslation. For
construct parts of text (Song et al., 2019) or the
example,Laublietal.(2020)notethataChinese-
wholesentences(Liuetal.,2020)fromnoisedin-
Englishnewstranslationsystemthathadallegedly
put sentences without particularly distinguishing
reached human parity still lagged far behind hu-
named entities and other functional words in the
mantranslatorsonentitytranslations,andthisprob-
sentences.Incontrast,ourmethodexploitsanentity
linkertoidentifyentityspansinthemonolingual
1Code/data/modelsarereleasedathttps://github.
com/JunjieHu/deep. sentencesandlinkthemtoaKBthatcontainsmul-
1753
Proceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics
Volume1:LongPapers,pages1753-1766
May22-27,2022(cid:13)c2022AssociationforComputationalLinguistics
EntityRecognitionandLinking
ĞœĞ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ·Ğ°Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸Ğ²ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ´Ğ°Ñ€e,Ğ¡Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ²eĞ¸Ğ£Ğ»ÑŒÑĞ½Ğ¾Ğ²ÑĞºe.
Krasnodar Saratov Ulyanovsk
(Q3646) (Q5332) (Q5627)
Language Label Description Language Label ... Language Label ...
English Krasnodar capitalofKrasnodarregion(Krai)inSouthernRussia English Saratov ... English Ulyanovsk ...
Russian ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ´Ğ°Ñ€ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ°ÑĞ³ĞµĞ Ğ¾ÑÑĞ¸Ğ¸,Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹Ñ†ĞµĞ½Ñ‚Ñ€ Russian Ğ¡Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ² ... Russian Ğ£Ğ»ÑŒÑĞ½Ğ¾Ğ²ÑĞº ...
: : ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ´Ğ°Ñ€ÑĞºĞ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ : : : :
Pre-trainingwithDEEP
[DEEP]ĞœĞ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ·Ğ°Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸Ğ²Krasnodar,SaratovĞ¸Ulyanovsk.
ĞœĞ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ·Ğ°Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸Ğ²ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ´Ğ°Ñ€e,Ğ¡Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ²eĞ¸Ğ£Ğ»ÑŒÑĞ½Ğ¾Ğ²ÑĞºe.
Multi-taskFinetuning
[MT]ThesenewformatstoreshaveopenedforbusinessinKrasnodar,Saratov,andUlyanovsk.
[DEEP]ĞœĞ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ·Ğ°Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸Ğ²Krasnodar,SaratovĞ¸Ulyanovsk.
ĞœĞ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ·Ğ°Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸Ğ²ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ´Ğ°Ñ€e,Ğ¡Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ²eĞ¸Ğ£Ğ»ÑŒÑĞ½Ğ¾Ğ²ÑĞºe.
Figure 1: General workflow of our method. Entities in a sentence is extracted and linked to Wikidata, which
includestheirtranslationsinmanylanguages. DEEPusesthenoisefunction ğ‘“(ğ‘¦,KB) thatreplacesentitieswith
thetranslationsforpre-training.DEEPisalsoemployedduringfinetuninginamulti-tasklearningmanner.
tilingualtranslationsoftheseentities(suchasWiki- isdefinedasfollows:
data(VrandeÄiÄ‡andKrÃ¶tzsch,2014)). Wethengen-
(cid:213)
erate noised sentences by replacing the extracted LDAE(Dğ‘Œ,ğœƒ) = logğ‘ƒ(ğ‘¦|ğ‘”(ğ‘¦);ğœƒ), (1)
entityspanswiththeirtranslationsintheknowledge ğ‘¦âˆˆDğ‘Œ
baseandpre-trainourNMTmodelstoreconstruct
whereğœƒ denotesthemodelâ€™slearnableparameters.
the original sentences from the noised sentences.
For notation simplicity, we drop ğœƒ in the rest of
Tofurtherimprovetheentitytranslationaccuracy
the sections. This formulation encompasses sev-
and avoid forgetting the knowledge learned from
eraldifferentpreviousworksindataaugmentation
pre-training,wealsoexamineamulti-tasklearning
forMT,suchasmonolingualdatacopying(Currey
strategythatfinetunestheNMTmodelusingboth
et al., 2017), where ğ‘”(Â·) is the identity function,
thedenoisingtaskonthemonolingualdataandthe
backtranslation(Sennrichetal.,2016),whereğ‘”(Â·)
translationtaskontheparalleldata.
isabackwardstranslationmodel,aswellasheuris-
IntheexperimentsonEnglish-Russian,English-
ticnoisefunctions(Songetal.,2019;Lewisetal.,
Ukrainian,andEnglish-Nepalitranslations,DEEP
2020;Liuetal.,2020)thatrandomlysamplenoise
outperforms the strong denoising auto-encoding
accordingtomanuallydevisedheuristics.
baselinewithrespecttoentitytranslationaccuracy,
In particular, as our baseline we focus on the
and obtains comparable or slightly better overall
mBART method (Liu et al., 2020), a popular
translationaccuracyasmeasuredbyBLEU.Afine-
methodwithtwotypesofheuristicnoisefunctions
grainedanalysisshowsthatourmulti-taskfinetun-
beingusedsequentiallyoneachtextsegment. The
ing strategy improves the translation accuracy of
firstnoisefunctionrandomlymasksspansoftext
theentitiesthatdonotexistinthefinetuningdata.
in each sentence. Specifically, a span length is
firstrandomlysampledfromaPoissondistribution
2 DenoisingAuto-Encoding(DAE)
(ğœ† = 0.35)andthebeginninglocationforaspaninğ‘¦
Givenasetofmonolingualtextsegmentsforpre- isalsorandomlysampled. Theselectedspanoftext
training,i.e., ğ‘¦ âˆˆ Dğ‘Œ,asequence-to-sequencede- isreplacedbyamasktoken. Thisprocessrepeats
noisingauto-encoderispre-trainedtoreconstructa until35%ofwordsinthesentencearemasked. The
textsegmentğ‘¦fromitsnoisedversioncorruptedby second noise function is to permute the sentence
anoisefunctionğ‘”(Â·). Formally,theDAEobjective orderineachtextsegmentwithaprobability.
1754
3 DEEP:DenoisingEntityPre-training where the swap() function swaps occurrences of
one entity span ğ‘¡ in ğ‘¦ with its translation ğ‘  in the
Ourmethodadoptsaprocedureofpre-trainingand
sourcelanguage. Forexample, inthesecondbox
finetuning for neural machine translation. First,
ofFigure1,thenamedentitiesâ€œĞšÑ€Ğ°ÑĞ½Ğ¾Ğ´Ğ°Ñ€Ğµ,Ğ¡Ğ°-
we apply an entity linker to identify entities in a
Ñ€Ğ°Ñ‚Ğ¾Ğ²ĞµandĞ£Ğ»ÑŒÑĞ½Ğ¾Ğ²ÑĞºĞµâ€inRussianarereplaced
monolingualcorpusandlinkthemtoaknowledge
by â€œKrasnodar, Saratov, and Ulyanovskâ€ in En-
base(Â§3.1). Wethenutilizeentitytranslationsin
glish. After the replacement, we create a noised
theknowledgebasetocreatenoisycode-switched
code-switchedsegmentwhichexplicitlyincludes
dataforpre-training(Â§3.2). Finally,weexaminea
thetranslationsofnamedentitiesinthecontextof
multi-tasklearningstrategytofurtherimprovethe
thetargetlanguage. Forsomesegmentsthatcon-
translationoflow-frequencyentities(Â§3.3).
tain fewer entities, their code-switched segments
maybesimilartothem, whichpotentiallyresults
3.1 EntityRecognitionandLinking
in a easier denoising task. Therefore, we further
The goal of this part is to identify entities in
addnoisetothesecode-switchedsegments. Todo
eachmonolingualsegmentandobtaintheirtransla-
so,ifthewordcountofthereplacedentityspansis
tions. Tothisend,weuseWikidata(VrandeÄiÄ‡and
lessthanafraction(35%)ofthewordcountinthe
KrÃ¶tzsch,2014)apublicmultilingualknowledge
segment, werandomlymasktheothernon-entity
basethatcovers94Mentities.2 Eachentityisrepre-
words to ensure that about 35% of the words are
sentedinsurfaceformsfromdifferentlanguagesin
either replaced or masked in the noised segment.
whichaWikipediaarticleexists. Therefore,linking
Finally, we follow Liu et al. (2020) to randomly
anentitymentionğ‘¡ inatarget-languagesegment ğ‘¦
permute the sentence order in ğ‘¦. We then train
to an entity ğ‘’ in Wikidata allows us to obtain the
a sequence-to-sequence model to reconstruct the
multilingualtranslationsoftheentity,thatis,
originalsentence ğ‘¦ fromitsnoisedcode-switched
sentenceasfollows:
âˆ€ğ‘¡ âˆˆ ğ‘¦,âˆƒğ‘’ âˆˆ KB : ğ‘‡ ğ‘’ = surface(ğ‘’,KB),ğ‘¡ âˆˆğ‘‡ ğ‘’
(cid:213)
LDEEP(Dğ‘Œ,KB) = logğ‘ƒ(ğ‘¦| ğ‘“(ğ‘¦,KB))
whereğ‘‡ denotesasetofmultilingualsurfaceforms
ğ‘’ ğ‘¦âˆˆDğ‘Œ
of ğ‘’. We can define the translate operation as:
ğ‘  = lookup(ğ‘‡ ğ‘’,ğ‘‹) whichsimplylooksforthesur- 3.3 Multi-taskFinetuning
faceformofğ‘’ inthesourcelanguage ğ‘‹. Notethat
Afterpre-training,wecontinuefinetuningthepre-
this strategy relies on the fact that translations in
higher-resourcelanguagesareincludedinğ‘‡ ,which
trained model on a parallel corpus (ğ‘¥,ğ‘¦) âˆˆ Dğ‘‹ğ‘Œ
ğ‘’ formachinetranslation.
weadoptbyusingEnglishinourexperiments. In
general,however,ğ‘‡ doesnotuniversallycoverall
ğ‘’
thelanguagesofinterest. Forentityrecognitionand
(cid:213)
linking,weuseSLING(Ringgaardetal.,2017),3
LMT(Dğ‘‹ğ‘Œ) = logğ‘ƒ(ğ‘¦|ğ‘¥) (3)
whichbuildsanentitylinkerforarbitrarylanguages
(ğ‘¥,ğ‘¦)âˆˆDğ‘‹ğ‘Œ
availableinWikipedia.
Toavoidforgettingtheentityinformationlearned
from the pre-training stage, we examine a multi-
3.2 Entity-basedDataAugmentation
tasklearningstrategytotrainthemodelbyboththe
AfterobtainingentitytranslationsfromtheKB,we pre-trainingobjectiveonthemonolingualdataand
attempttoexplicitlyincorporatethesetranslations thetranslationobjectiveontheparalleldata. Since
intothemonolingualsentencesforpre-training. To monolingual segments are longer text sequences
doso,wedesignanentity-basednoisefunctionthat thansentencesin Dğ‘‹ğ‘Œ andthesizeof Dğ‘Œ isusu-
takes in a sentence ğ‘¦ and the KB, i.e., ğ‘“(ğ‘¦,KB). allylargerthanthatofDğ‘‹ğ‘Œ,simplyconcatenating
First, we replace all detected entity spans in the both data for multi-task finetuning leads to bias
sentencebytheirtranslationsfromtheKB: towarddenoisinglongersequencesratherthanac-
tually translating sentences. To balance the two
replace(ğ‘¦,KB) = swap(ğ‘ ,ğ‘¡,ğ‘¦), âˆ€ğ‘¡ âˆˆ ğ‘¦ (2) tasks,ineachepochwerandomlysampleasubset
2DumpJune14,2021.CreativeCommonsCC0License.
ofmonolingualsegmentsD ğ‘Œ(cid:48) fromDğ‘Œ,wherethe
3https://github.com/google/sling, Apache-
totalsubwordcountofD ğ‘Œ(cid:48) equalstothatofDğ‘‹ğ‘Œ,
2.0License i.e., (cid:205) |ğ‘¦| = (cid:205) max(|ğ‘¥|,|ğ‘¦|). We
ğ‘¦âˆˆDğ‘¦(cid:48) (ğ‘¥,ğ‘¦)âˆˆDğ‘‹ğ‘Œ
1755
Entity Coverage(F) Coverage(T)
Lang. Token Para. Lang. Train Dev Test
Type Count N Type Count Type Count
Ru 775M 1.8M 1.4M 337M 123 En-Ru 235K 3.0K 3.0K 88% 94% 88% 91%
Uk 315M 654K 524K 140M 149 En-Uk 200K 2.3K 2.5K 87% 94% 91% 94%
Ne 19M 26K 17K 2M 34 En-Ne 563K 2.6K 2.8K 35% 25% 44% 27%
Table1:StatisticsofWikipediacorporainRussian(Ru), Table2: Statisticsoftheparalleltrain/dev/testdatafor
Ukrainian(Uk)andNepali(Ne)forpre-training. ğ‘ de- finetuning. Coverage(F/T)representthepercentageof
notestheaveragesubwordcountofentityspansinase- entity types and counts in the Finetuning (Test) data
quenceof512subwords. thatarecoveredbythepre-trainingdata.
thenexaminethemulti-taskfinetuningasfollows: weusetheFLORESdatasetinGuzmÃ¡netal.(2019)
andfollowthepaperâ€™ssettingtofinetuneonparallel
LMulti-task = LMT(Dğ‘‹ğ‘Œ)+LPre-train(D ğ‘Œ(cid:48)) (4) dataintheOPUSrepository.Table2showsthedata
statisticsoftheparalleldataforfinetuning. Notice
wherethepre-trainingobjectiveLPre-train iseither
thatfromthelastfourcolumnsofTable2,theen-
DAEorDEEPwithDEEPhavinganadditionalin-
titiesinthepre-trainingdatacoveratleast87%of
putofaknowledgebase. Noticethatwiththesam-
theentitytypesand91%oftheentitycountsinboth
plingstrategyforthemonolingualdata,wedouble
finetuningandtestdataexcepttheEn-Nepair.
the batch size in the multi-task finetuning setting
with respect to that in the single-task finetuning Architecture: We use a standard sequence-to-
setting. Therefore, wemakesurethatthemodels sequenceTransformermodel(Vaswanietal.,2017)
arefinetunedonthesameamountofparalleldata with 12 layers each for the encoder and decoder.
inboththesingle-taskandmulti-tasksettings,and Weuseahiddenunitsizeof512and12attention
thegainsfromthemulti-tasksettingsorelycome heads. FollowingLiuetal.(2020),weaddanaddi-
fromtheadditionaltaskonthemonolingualdata. tionallayer-normalizationlayerontopofboththe
To distinguish the tasks during finetuning, we encoderanddecodertostabilizetrainingatFP16
replace the start token ([BOS]) in a source sen- precision. Weusethesamesentencepiece model
tence or a noised segment by the corresponding andthevocabularyfromLiuetal.(2020).
tasktokensforthetranslationorthedenoisingtask
MethodsinComparison: Wecomparemethods
([MT], [DAE] or [DEEP]). We initialize these
inthesingletaskandmulti-tasksettingasfollows:
taskembeddingsbythestarttokenembeddingand
appendthemtothewordembeddingmatrixofthe
â€¢ Randomâ†’MT:Weincludeacomparisonwith
encoder.
arandomlyinitializedmodelwithoutpre-training
andfinetunethemodelforeachtranslationtask.
4 ExperimentalSetting
â€¢ DAEâ†’MT:Wepre-trainamodelbyDAEusing
Pre-trainingData: Weconductourexperiments thetwonoisefunctionsinLiuetal.(2020)and
onthreelanguagepairs: English-Russian,English- finetunethemodelforeachtranslationtask.
UkrainianandEnglish-Nepali. WeuseWikipedia
â€¢ DEEPâ†’MT:Wepre-trainamodelusingour
articlesasthemonolingualdataforpre-trainingand
proposedDEEPobjectiveandfinetunethemodel
reportthedatastatisticsinTable1. Wetokenizethe
onthetranslationtask.
textusingthesamesentencepiecemodelasLiuetal.
(2020),andtrainonsequencesof512subwords.
â€¢ DAEâ†’DAE+MT:Wepre-trainamodelbythe
DAEobjectiveandfinetunethemodelforboth
Finetuning&TestData: Weusethenewscom- theDAEtaskandtranslationtask.
mentarydatafromtheEnglish-Russiantranslation
â€¢ DEEPâ†’DEEP+MT:Wepre-trainamodelby
taskinWMT18(Speciaetal.,2018)forfinetuning
theDEEPobjectiveandfinetunethemodelfor
andevaluatetheperformanceontheWMT18test
boththeDEEPtaskandtranslationtask.
datafromthenewsdomain. ForEnglish-Ukrainian,
weusetheTEDTalktranscriptsfromJuly2020in Learning&Decoding: Wepre-trainallmodels
theOPUSrepository(Tiedemann,2012)forfine- for 50K steps first using the default parameters
tuningandtesting. ForEnglish-Nepalitranslation, in Liu et al. (2020) except that we use a smaller
1756
batchof64textsegments,eachofwhichhas512 lationsinthepre-trainingdataandobtainagood
subwords. We use the Adam optimizer (ğœ–=1e-6, initializationfortranslationatthebeginningofthe
ğ›½ =0.98) and a polynomial learning rate decay finetuningstage. Asthemulti-taskfinetuningpro-
2
schedulingwithamaximumstepat500K.Allmod- ceeds,themodelstrainedbybothDAEandDEEP
elsarepre-trainedononeTPUv3(128GB)forabout relymoreonthetranslationtaskthanthedenoising
12hoursfor50Ksteps.4 Weapplythenoisefunc- taskfortranslatingawholesentence. Thusthenu-
tion on the monolingual data on the fly for each anceoftheentitytranslationsmightnotbeclearly
epoch,andthistakesonlyafewminutesbymulti- evaluatedaccordingtoBLEUorchrF.
processinginFairseq(Ottetal.,2019). Wethen
resetthelearningrateschedulerandcontinuefine- 5.2 EntityTranslationAccuracy
tuningourpre-trainedmodelsontheMTparallel Sincecorpus-levelmetricslikeBLEUorchrFmight
data for 40K steps. Single-task (multi-task) fine- not necessarily reveal the subtlety of named en-
tuning takes about 16 (32) hours on 2 RTX 3090 titytranslations,inthesectionweperformafine-
GPUs. We set the maximum number of tokens grained evaluation by the entity translation accu-
in each batch to 65,536 in the single task setting racy which counts the proportion of entities cor-
anddoublethebatchsizeinthemulti-tasksetting rectlytranslatedinthehypotheses. Specifically,we
to ensure that models in both settings are trained firstuseSLINGtoextractentitiesforeachpairof
onanequalamountofparalleldata,andthusany a reference and a hypothesis. We then count the
performancegaincanonlybeattributedtomonolin- translationaccuracyofanentityastheproportion
gualdataduringfinetuning. Weuse2,500warm-up ofcorrectlymentioningtherightentityinthehy-
steps to reach a maximum learning rate of 3e-5, potheses,followedbymacro-averagingtoobtainthe
anduse0.3dropoutand0.2labelsmoothing. After averageentitytranslationaccuracy. Wealsoshow
training,weusebeamsearchwithabeamsizeof theaccuracyscoresinTable3. First,ourmethod
5andreporttheresultsinsacreBLEU(Post,2018) inbothsingle-andmulti-tasksettingssignificantly
followingthesameevaluationinLiuetal.(2020). outperformedtheotherbaselines. Inparticular,the
gains from DEEP are much clear for the En-Uk
5 Discussion andEn-Rutranslations. Onepossiblereasonisthat
Russian or Ukrainian entities extracted from the
5.1 Corpus-levelEvaluation
pre-trainingdatahavearelativelyhighercoverage
In Table 3, we compare all methods in terms of
oftheentitiesinboththefinetuningandtestdata
BLEU (Papineni et al., 2002) and chrF (PopoviÄ‡,
asreportedinTable2. However,SLINGmightnot
2015)onthetestdataforthreelanguagepairs. First,
detectasmanyentitiesinNepaliasintheotherlan-
wefindthatallpre-trainingmethodssignificantly
guages. Webelievethatfutureadvancesonentity
outperformtherandombaseline. Inparticular,our
linkinginlow-resourcelanguagescouldpotentially
DEEPmethodobtainsagainof3.5BLEUpointsin
improve the performance of DEEP further. We
thesingletasksettingforthelow-resourceEn-Ne
leavethisasourfuturework.
translation. Second,wecomputestatisticalsignifi-
canceoftheBLEUandchrFscoreswithbootstrap 5.3 Fine-grainedAnalysisonEntity
resampling(Koehn,2004),andweobservesignifi- TranslationAccuracy
cantimprovementswiththemulti-taskfinetuning
In this section, we further analyze the effect on
strategyoverthesingle-taskfinetuningforEn-Ru
differentcategoriesofentitiesusingourmethod.
and En-Ne. Our DEEP method outperforms the
DAE method for En-Ru translation by 1.3 BLEU PerformanceofEntityGroupsoverFinetuning:
pointsinthemulti-tasksetting. Itisalsoworthnot- Themodelisexposedtosomeentitiesmoreoften
ing that DEEP obtains higher BLEU points than thanothersatdifferentstages: pre-training,finetun-
DAEatthebeginningofthemulti-taskfinetuning ingandtesting,whichraisesaquestion: howisthe
process,howeverthegapbetweenbothmethodsde- entitytranslationaffectedbytheexposureduring
creasesasthefinetuningproceedsforlongersteps eachstage? Toanswerthisquestion,wedividethe
(SeeAppendixA).Onepossiblereasonisthatmod- entitiesappearinginthetestdataintothreegroups:
elstrainedbyDEEPbenefitfromtheentitytrans- â€¢ PFT:entitiesappearinginthepre-training,fine-
tuning,andtestdata.
4AsweshowinFigure4,modelspre-trainedfor50Ksteps
provideareasonablygoodinitialization. â€¢ PT:entitiesonlyinthepre-trainingandtestdata.
1757
BLEU chrF EntityTranslationAcc.
Pre-trainâ†’Finetune
En-Uk En-Ru En-Ne En-Uk En-Ru En-Ne En-Uk En-Ru En-Ne
Randomâ†’MT 17.1 15.0 7.7 37.0 36.8 24.3 49.5 31.1 20.9
DAEâ†’MT 19.5 18.5 10.5 39.2 40.4 26.8 56.7 37.7 26.0
DEEPâ†’MT 19.4 18.5 11.2âˆ— 39.2 40.7âˆ— 27.7âˆ— 57.7 40.6âˆ— 28.6âˆ—
DAEâ†’DAE+MT 19.4 18.5 11.2 39.1 41.0 27.8 58.8 47.2 27.9
DEEPâ†’DEEP+MT 19.7 19.6âˆ— 11.5 39.1 42.4âˆ— 28.2âˆ— 61.9âˆ— 56.4âˆ— 28.3
Table3: BLEU,Entitytranslationaccuracy,andchrFinsingle-andmulti-tasksettings. Largestnumbersineach
columnarebold-faced.âˆ—indicatesstatisticalsignificanceofDEEPwithğ‘ <0.05toDAEintherespectivesettings.
Random MT DAE MT DEEP MT DAE DAE + MT DEEP DEEP + MT
70 70 70
60 60 60
50 50 50
40 40 40
30 30 30
20 20 20
10 10 10
0 0 0
5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40
Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000)
Entities in PFT data. Entities in PT data. Entities in FT data.
Figure2: EntitytranslationaccuracyscoresaggregatedoverdifferententitysetsforRussian. PFT,PT,FTdata
correspondtoentitiesappearingin(i)pre-training,finetuningandtestdata,(ii)onlypre-trainingandtestdata(iii)
onlyfinetuningandtestdata.
â€¢ FT:entitiesonlyinthefinetuningandtestdata. acheckpointwith40Kstepsoffinetuning,binthe
WeshowtheEnglish-to-Russianentitytransla- setofentitiesinthreedata(i.e. PFT,PT,FT)ac-
tionaccuracyscoresforeachgroupoverfinetuning cordingtofrequenciesineachofthedata. Wethen
stepsinFigure2. Overall,accuraciesarehigherfor calculatetheentitytranslationaccuracywithineach
theentitiesthatappearinthefinetuningdata(PFT, binbycomparingthemagainstreferenceentitiesin
FT),whichisduetotheexposuretothefinetuning therespectivesentences. Figure3showstheaccu-
data. Our proposed method consistently outper- racygainofeachpre-trainingmethodologiesfrom
formed baseline counterparts in both single- and Randomâ†’MT(i.e. nopre-training)ontestdata,
multi-tasksettings. Thedifferencesinaccuracyare groupedbytheentityfrequencybinsinpre-training
particularlylargeatearlierfinetuningsteps,which andfinetuningdata. Notethatleftmostcolumnand
indicatestheutilityofourmethodinlower-resource thebottomrowrepresentPT,FT,respectively. As
settings with little finetuning data. The effect of observed earlier, the proposed method improves
multi-taskfinetuningismostnotableforentitiesin moreovermostfrequencybins,withgreaterdiffer-
PT.Multi-taskfinetuningcontinuouslyexposesthe encesonentitiesthatarelessfrequentinfinetuning
modeltothepre-trainingdata,whichasaresultpre- data. Thistendencyisobservedmoresignificantly
ventsthemodelfromforgettingthelearnedentity forthemulti-taskvariant(DEEPâ†’DEEP+MT),
translationsfromPT. wherethegainsaremostlyfromentitiesthatnever
appearedinfinetuningdata(i.e. leftmostcolumn).
Performance according to Entity Frequency: Multi-tasklearningwithDEEPthereforeprevents
Wefurtheranalyzetheentitytranslationaccuracy the model from forgetting the entity translations
scoresusingentityfrequenciesineachgroupintro- learnedatpre-trainingtime. Analyticalresultson
ducedabove. Thisprovidesamorefine-grainedper- UkrainianandNepaliareinAppendixB.
spectiveonhowfrequentorrareentitiesaretrans-
lated. Todoso,wetakeRussianhypothesesfrom
1758
ycaruccA
noitalsnarT
ytitnE
egarevA
(22440, 569372] 9.3 10.9 13.9 -3.8 3.3 6.0 11.6 15.5 3.0 2.4 44.3 17.1 27.1 1.4 4.6 40
(8424, 22440] 13.2 13.3 1.7 -2.1 -10.2 19.7 18.0 6.0 -0.1 -5.3 50.4 34.9 15.2 3.3 1.2 20
(2948, 8424] 3.9 16.0 12.4 4.2 13.5 9.3 15.9 15.0 7.4 15.7 32.4 27.7 21.9 6.9 18.7 0
(2, 2948] 7.8 12.7 -2.0 3.5 7.0 9.7 15.5 9.2 0.9 10.0 32.0 32.3 9.4 3.3 12.4 20
0 0.0 9.3 0.0 3.3 0.0 7.1 11.1 0.0 3.3 100.0 28.6 23.1 0.0 -3.3 100.0 40
0 (1, 2] (2, 6] (6, 23] (23, 1878] 0 (1, 2] (2, 6] (6, 23] (23, 1878] 0 (1, 2] (2, 6] (6, 23] (23, 1878]
Freq. bins from Finetuning data. Freq. bins from Finetuning data. Freq. bins from Finetuning data.
Gain with DAE MT Gain with DEEP MT Gain with DEEP DEEP+MT
Figure3:GainfromRandomâ†’MTinentitytranslationaccuracyforeachmodel.
0.24M 4.25M
Methods
19
BLEU Acc. BLEU Acc. 40
Randomâ†’MT 15.0 31.1 15.7 39.4
DAEâ†’MT 18.5 37.7 16.3 53.7 18 38
DEEPâ†’MT 18.5 40.6 17.2 53.9
36
17
Table 4: Model comparisons across different finetun-
34
ingdatasizes. Theresultsontherightareobtainedaf-
16
ter finetuning on the combined news commentary and
BLEU 32
ParaCrawldata.
Entity Translation Accuracy
15
30
0 25 50 100 150 200
Pre-training Steps (x 1000)
5.4 OptimizationEffectsonDEEP
Figure4: English-to-RussianBLEUandEntitytransla-
Finetuning Data Size vs Entity Translation:
tionaccuracyscoresafterfinetuningfromvariablepre-
WhileDEEPprimarilyfocusesonalow-resource
trainingsteps.Finetuningisperformedfor40Ksteps.
setting, the evaluation with more resources can
highlight potential use in broader scenarios. To
thisend,weexpandthefinetuningdataforEnglish-
Russian translation with an additional 4 million
sentencepairsfromParaCrawl(BaÃ±Ã³netal.,2020),
aparalleldatacollectedfromwebpages. Although
webpagesmightcontainnewstext,ParaCrawldata proceeds. To analyze the efficiency of learning
covers more general domains. We finetune mod- entity translation during pre-training, we focus
elsonthecombineddataandevaluatewithBLEU onthequestion: howmanypre-trainingstepsare
andentitytranslationaccuracy. Table4showsthe neededfornamedentitytranslation? Toexamine
comparisonsacrossdifferentfinetuningdatasizes. thisquestion,wetakethesavedcheckpointstrained
When the model is initialized with pre-training by DEEP from various pre-training steps, and
methods,weobserveddecreasedBLEUpointsand apply the single-task finetuning strategy on the
increasedentitytranslationaccuracyscores. Thisis checkpoints for another 40K steps. We plot the
partlyduetothediscrepancyofdomainsbetween entity translation accuracy and BLEU on the test
ourfinetuningdata(news)andParaCrawl. Regard- data in Figure 4. We find that the checkpoint at
less, DEEP is consistently equal to or better than 25Kstepshasalreadyachievedacomparableentity
DAEinalltestedsettings. translationaccuracywithrespecttothecheckpoint
at150Ksteps. ThisshowsthatDEEPisefficientto
Pre-training Steps vs Entity Translation: learntheentitytranslationsasearlyasin25Ksteps.
Since DEEP leverages entity-augmented mono- Besides, both the BLEU and entity translation
lingual data, the model trained by DEEP revisits accuracykeepimprovingasthepre-trainingsteps
moreentitiesindifferentcontextasthepre-training increaseto200Ksteps.
1759
.atad
gniniarterP
morf
snib
.qerF
UELB
ycaruccA
noitalsnarT
ytitnE
Src: ThesenewformatstoreshaveopenedforbusinessinKrasnodar,Saratov,andUlyanovsk.
Ref: ĞœĞ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ·Ğ°Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸Ğ²ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ´Ğ°Ñ€Ğµ,Ğ¡Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ²ĞµĞ¸Ğ£Ğ»ÑŒÑĞ½Ğ¾Ğ²ÑĞºĞµ.
(cid:13)1 Ğ­Ñ‚Ğ¸Ğ½Ğ¾Ğ²Ñ‹ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²Ñ‹ĞµĞ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ÑÑŒĞ´Ğ»ÑĞ±Ğ¸Ğ·Ğ½ĞµÑĞ°Ğ²ĞĞ½Ñ€Ğ¸Ğ´Ğ°Ñ€Ğµ,ĞšÑ€Ğ¸ÑÑ‚Ğ¾Ñ„ĞµĞ¸ĞšÑƒÑŒÑĞ½Ğ¼Ğµ.
(cid:13)2 Ğ­Ñ‚Ğ¸Ğ½Ğ¾Ğ²Ñ‹ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚@-@Ğ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ÑÑŒĞ´Ğ»ÑĞ±Ğ¸Ğ·Ğ½ĞµÑĞ°Ğ²ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ´Ğ°Ñ€Ğµ,Ğ¡Ğ°Ñ€Ğ°Ğ°Ğ±Ğ°Ğ½ĞµĞ¸Ğ²Ğ£Ñ€ÑƒĞ³ÑĞ½ÑĞºĞ¾Ğ¼ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğµ.
(cid:13)3 Ğ­Ñ‚Ğ¸Ğ½Ğ¾Ğ²Ñ‹ĞµĞ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ÑÑŒĞ´Ğ»ÑĞ±Ğ¸Ğ·Ğ½ĞµÑĞ°Ğ²Krasnodar,SaratovĞ¸Ulyanovsk.
(cid:13)4 Ğ­Ñ‚Ğ¸Ğ½Ğ¾Ğ²Ñ‹ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ½Ñ‹ĞµĞ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ñ‹Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ÑÑŒĞ´Ğ»ÑĞ±Ğ¸Ğ·Ğ½ĞµÑĞ°Ğ²ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ´Ğ°Ñ€Ğµ,Ğ¡Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ²ĞµĞ¸Ğ£Ğ»ÑŒÑĞ½Ğ¾Ğ²ÑĞºĞµ.
Src: InBarnaul,thenewasphaltonKrasnoarmeyskiyProspektisbeingdugup
Ref: Ğ’Ğ‘Ğ°Ñ€Ğ½Ğ°ÑƒĞ»ĞµĞ²ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚Ğ½Ğ¾Ğ²Ñ‹Ğ¹Ğ°ÑÑ„Ğ°Ğ»ÑŒÑ‚Ğ½Ğ°Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚ĞµĞšÑ€Ğ°ÑĞ½Ğ¾Ğ°Ñ€Ğ¼ĞµĞ¹ÑĞºĞ¾Ğ¼
(cid:13)1 Ğ’Ğ‘Ğ°Ñ€Ğ½Ğ°ÑƒĞ»ĞµĞ½Ğ¾Ğ²Ğ¾Ğµ,ĞºĞ°ĞºÑ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞµÑÑĞ½Ğ°Ğ¶ĞµĞ»ĞµĞ·Ğ½Ğ¾Ğ¿Ğ¾Ğ»ÑÑ€Ğ½Ğ¾Ğ¼ĞŸÑ€Ğ¾Ğ¸ÑÑĞµ,Ñ€Ğ°ÑÑ‚ĞµÑ‚.
(cid:13)2 Ğ’Ğ‘Ğ°Ñ€Ğ½Ğ°Ğ»Ğµ,Ğ½Ğ¾Ğ²Ğ¾Ğµ,ĞºĞ°ĞºÑ€Ğ°Ğ·Ñ€Ğ°Ğ·Ğ¸Ğ»Ğ¾ÑÑŒĞ½Ğ°ĞšÑ€Ğ°ÑĞ½Ğ¾@-@ĞœĞ¾Ğ»Ğ³ÑĞºĞ¸ÑĞºĞ¾Ğ¼ĞŸÑ€Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğ¸,Ñ€Ğ°ÑÑ‚ĞµÑ‚.
(cid:13)3 Ğ‘Ğ°Ñ€Ğ½Ğ°ÑƒĞ»,Ğ½Ğ¾Ğ²Ñ‹Ğ¹Ğ¼Ğ¸Ñ„Ğ½Ğ°KrasnoarmeyProspekt,Ğ²Ñ‹Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹Krasnoarmeski.
(cid:13)4 Ğ’Ğ‘Ğ°Ñ€Ğ½Ğ°ÑƒĞ»ĞµĞ½Ğ¾Ğ²Ñ‹Ğ¹Ğ°ÑÑ„Ğ°Ğ»ÑŒÑ‚Ğ½Ğ°ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ°Ñ€Ğ¼ĞµĞ¹ÑĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚ĞµĞ²Ñ‹Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµÑ€Ğ°ÑÑ‚ĞµÑ‚.
Table5: Qualitativecomparisonamongfourpre-trainingmethodsonnamedentitytranslations. (cid:13)1: DAEâ†’MT,
(cid:13)2:DEEPâ†’MT,(cid:13)3:DAEâ†’DAE+MT,(cid:13)4:DEEPâ†’DEEP+MT.
5.5 QualitativeAnalysis corpus (Huang et al., 2003, 2004; Zhang et al.,
2005) and Web mining methods built on top of
Inthissection,weselecttwoexamplesthatcontain
asearchengine(Huangetal.,2005;WuandChang,
entitiesappearingonlyinthepre-trainingandtest-
2007; Yang et al., 2009). Recently, Finch et al.
ingdata. Thefirstexamplecontainsthreelocation
(2016); Hadj Ameur et al. (2017); Grundkiewicz
names. Wefindthatthemodeltrainedbythesingle-
and Heafield (2018) used NMT to transliterate
taskDAEpredictsthewrongplaceswhichprovide
the wrong information in the translated sentence. named entities without any sentencecontext. An-
otherlineofresearch(Ugawaetal.,2018;Lietal.,
In addition, the model trained by the multi-task
2018;Torregrosaetal.,2020;Modrzejewskietal.,
DAE just copies the English named entities (i.e.,
2020;Zhouetal.,2020)onlyperformsentityrecog-
â€œKrasnodarâ€, â€œSaratovâ€ and â€œUlyanovskâ€) to the
nition and uses entity tags (e.g., person) which
targetsentencewithoutactualtranslation. Incon-
arenotdirectlyinformativetothetranslationtask,
trast, our method predicts the correct translation
in contrast to the entity translations obtained by
forâ€œKrasnodarâ€inbothsingle-taskandmulti-task
entity linking in our work. Besides, these meth-
setting,whilethemulti-taskDEEPtranslatesallen-
odsmodifymodelarchitecturetointegrateentity
titiescorrectly.Inthesecondexample,althoughour
tagembeddingsorknowledgegraphentityembed-
methodinthesingle-tasksettingpredictswrongfor
dings(Moussallemetal.,2019),whichalsorequire
alltheentities,themodelgeneratespartiallycorrect
extractingentityinformationforbothtrainingand
translationssuchasâ€œĞ‘Ğ°Ñ€Ğ½Ğ°Ğ»Ğµâ€forâ€œĞ‘Ğ°Ñ€Ğ½Ğ°ÑƒĞ»Ğµâ€and
testdata.Incontrast,wefocusondataaugmentation
â€œĞšÑ€Ğ°ÑĞ½Ğ¾@-@ĞœĞ¾Ğ»Ğ³ÑĞºĞ¸ÑĞºĞ¾Ğ¼â€forâ€œĞšÑ€Ğ°ÑĞ½Ğ¾Ğ°Ñ€Ğ¼ĞµĞ¹-
ÑĞºĞ¾Ğ¼â€. NoticethatDEEPinthemulti-tasksetting
methodstoimprovenameentitytranslationwithin
translatesthecorrectentitiesâ€œasphaltâ€andâ€œKras-
context,soourmethodiseasilyapplicabletoany
architecturesandtestdatawithoutpreprocessing.
noarmeyskiyâ€whichconveythekeyinformationin
thissentence. Incontrast,thetranslationproduced
Pre-trainingofNeuralMachineTranslationhas
bythemulti-taskDAEmethodliterallymeansâ€œĞ‘Ğ°Ñ€-
beenshowneffectivebymanyrecentworks(Con-
Ğ½Ğ°ÑƒĞ»(Barnaul),Ğ½Ğ¾Ğ²Ñ‹Ğ¹(new)Ğ¼Ğ¸Ñ„(myth)Ğ½Ğ°(at)
neauandLample,2019;Songetal.,2019;Liuetal.,
Krasnoarmey Prospekt, Ğ²Ñ‹Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ (grow)
2020;Linetal.,2020),wheredifferentpre-training
Krasnoarmeski.â€,whichisincomprehensibledue
objectives are proposed to leverage monolingual
totheentitytranslationerrors.
datafortranslation. Thesemethodsadoptadenois-
ingauto-encodingframework,whichencompasses
6 RelatedWork
several different works in data augmentation on
Named Entity Translation has been extensively monolingual data for MT (Lambert et al., 2011;
studiedfordecades(Arbabietal.,1994;Knightand Curreyetal.,2017;Sennrichetal.,2016;Huetal.,
Graehl,1998). Earlierstudiesfocusonrule-based 2019). However,namedentitytranslationsduring
methods using phoneme or grapheme (Wan and pre-trainingisunder-explored. Wefillthisgapby
Verspoor, 1998; Al-Onaizan and Knight, 2002b), integratingnamedentityrecognitionandlinkingto
statistical methods that align entities in parallel the pre-training of NMT. Moreover, while recent
1760
workshowsthatcontinuefinetuningapre-trained Mikel L. Forcada, Amir Kamran, Faheem Kirefu,
encoder with the pre-training objective improves Philipp Koehn, Sergio Ortiz Rojas, Leopoldo
Pla Sempere, Gema RamÃ­rez-SÃ¡nchez, Elsa Sar-
language understanding tasks (Gururangan et al.,
rÃ­as, Marek Strelec, Brian Thompson, William
2020), this finetuning paradigm has not been ex-
Waites, Dion Wiggins, and Jaume Zaragoza. 2020.
ploredforpre-trainingofasequence-to-sequence ParaCrawl: Web-scale acquisition of parallel cor-
model.Besides,previousworksonmulti-tasklearn- pora. In Proceedings of ACL 2020, pages 4555â€“
4567.
ingforMTfocusonlanguagemodeling(Gulcehre
etal.,2015;ZhangandZong,2016;Domhanand YufengChen,ChengqingZong,andKeh-YihSu.2013.
Hieber,2017;Zhouetal.,2019),whileweexamine Ajointmodeltoidentifyandalignbilingualnamed
amulti-taskfinetuningstrategywithanentity-based entities. ComputationalLinguistics,39(2):229â€“266.
denoising task in this work and demonstrate sub-
Alexis Conneau and Guillaume Lample. 2019. Cross-
stantialimprovementsfornamedentitytranslations. linguallanguagemodelpretraining. InAdvancesin
NeurIPS,volume32.
7 Conclusion
AnnaCurrey,AntonioValerioMiceliBarone,andKen-
In this paper, we propose an entity-based pre- neth Heafield. 2017. Copied monolingual data im-
proves low-resource neural machine translation. In
trainingmethodforneuralmachinetranslation.Our
ProceedingsofWMT2017,pages148â€“156.
method improves named entity translation accu-
racy as well as BLEU score over strong denois- Tobias Domhan and Felix Hieber. 2017. Using target-
ingauto-encodingbaselinesinbothsingle-taskand side monolingual data for neural machine transla-
multi-task setting. Despite the effectiveness, sev-
tionthroughmulti-tasklearning. InProceedingsof
EMNLP2017,pages1500â€“1505.
eralchallengingquestionsremainopen. First, re-
centworksonintegratingknowledgegraphs(Zhao AndrewFinch,LemaoLiu,XiaolinWang,andEiichiro
Sumita.2016. Target-bidirectionalneuralmodelsfor
etal.,2020a,b)inNMThaveshownpromisingre-
machinetransliteration. InProceedingsoftheSixth
sultsfortranslation. Ourmethodlinksentitiestoa
NamedEntityWorkshop,pages78â€“82.
multilingualknowledgebasewhichcontainsrich
information of the entities such as entity descrip- Roman Grundkiewicz and Kenneth Heafield. 2018.
Neuralmachinetranslationtechniquesfornameden-
tion,relationlinks,andalias. Howtoleveragethese
tity transliteration. In Proceedings of the Seventh
richerdatasourcestoresolveentityambiguityde- NamedEntitiesWorkshop,pages89â€“94.
servesfurtherinvestigation. Second,finetuningpre-
Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
trainedmodelsonin-domaintextdataisapotential
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
waytoimproveentitytranslationsacrossdomains.
HolgerSchwenk,andYoshuaBengio.2015. Onus-
ing monolingual corpora in neural machine transla-
Acknowledgement tion. arXiv:1503.03535.
This work was supported in part by a grant from Suchin Gururangan, Ana MarasoviÄ‡, Swabha
the Singapore Defence Science and Technology Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Donâ€™t stop pretraining:
Agency.
Adapt language models to domains and tasks. In
ProceedingsofACL2020,pages8342â€“8360.
References Francisco GuzmÃ¡n, Peng-Jen Chen, Myle Ott, Juan
Pino, Guillaume Lample, Philipp Koehn, Vishrav
YaserAl-OnaizanandKevinKnight.2002a. Nameden-
Chaudhary, and Marcâ€™Aurelio Ranzato. 2019. The
titytranslation. InProceedingsofHLT2002,pages
FLORES evaluation datasets for low-resource ma-
122â€“124.
chine translation: Nepaliâ€“English and Sinhalaâ€“
YaserAl-OnaizanandKevinKnight.2002b. Translat- English. In Proceedings of EMNLP-IJCNLP 2019,
ingnamedentitiesusingmonolingualandbilingual pages6098â€“6111.
resources. InProceedingsofACL2002,pages400â€“
Mohamed Seghir Hadj Ameur, Farid Meziane, and
408.
Ahmed Guessoum. 2017. Arabic machine translit-
M. Arbabi, S. M. Fischthal, V. C. Cheng, and E. Bart. eration using an attention-based encoder-decoder
1994. Algorithms for arabic name translitera- model. ProcediaComputerScience,117:287â€“297.
tion. IBM Journal of Research and Development,
38(2):183â€“194. JunjieHu,MengzhouXia,GrahamNeubig,andJaime
Carbonell. 2019. Domain adaptation of neural ma-
Marta BaÃ±Ã³n, Pinzhen Chen, Barry Haddow, Ken- chine translation by lexicon induction. In Proceed-
neth Heafield, Hieu Hoang, Miquel EsplÃ -Gomis, ingsofACL2019,pages2989â€“3001.
1761
FeiHuang,StephanVogel,andAlexWaibel.2003. Au- Minh-ThangLuongandChristopherD.Manning.2015.
tomaticextractionofnamedentitytranslingualequiv- Stanfordneuralmachinetranslationsystemsforspo-
alencebasedonmulti-featurecostminimization. In ken language domain. In Proceedings of IWSLT
ProceedingsoftheACL2003WorkshoponMultilin- 2015.
gualandMixed-languageNamedEntityRecognition,
pages9â€“16. MaciejModrzejewski,MiriamExel,BiankaBuschbeck,
Thanh-LeHa,andAlexanderWaibel.2020. Incorpo-
FeiHuang,StephanVogel,andAlexWaibel.2004. Im- rating external annotation to improve named entity
provingnamedentitytranslationcombiningphonetic translationinNMT. InProceedingsofEAMT,pages
and semantic similarities. In Proceedings of HLT- 45â€“51,Lisboa,Portugal.
NAACL2004,pages281â€“288.
DiegoMoussallem,Axel-CyrilleNgongaNgomo,Paul
FeiHuang,YingZhang,andStephanVogel.2005. Min- Buitelaar,andMihaelArcan.2019. Utilizingknowl-
ing key phrase translations from web corpora. In edgegraphsforneuralmachinetranslationaugmen-
ProceedingsofHLT-EMNLP2005,pages483â€“490. tation. In Proceedings of K-CAP 2019, pages 139â€“
146.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics, GrahamNeubigandJunjieHu.2018. Rapidadaptation
24(4):599â€“612. of neural machine translation to new languages. In
ProceedingsofEMNLP2018,pages875â€“880.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,
EMNLP2004,pages388â€“395. SamGross,NathanNg,DavidGrangier,andMichael
Auli.2019. fairseq: Afast,extensibletoolkitforse-
Patrik Lambert, Holger Schwenk, Christophe Servan, quence modeling. In Proceedings of NAACL 2019
andSadafAbdul-Rauf.2011. Investigationsontrans- (Demo),pages48â€“53.
lationmodeladaptationusingmonolingualdata. In
ProceedingsofWMT2011,pages284â€“293. KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu:amethodforautomaticevalu-
Samuel Laubli, Sheila Castilho, Graham Neubig, ationofmachinetranslation. InProceedingsofACL,
Rico Sennrich, Qinlan Shen, and Antonio Toral. pages311â€“318.
2020. A set of recommendations for assessing hu-
manâ€“machine parity in language translation. JAIR, Maja PopoviÄ‡. 2015. chrF: character n-gram F-score
67. for automatic MT evaluation. In Proceedings of
WMT2015,pages392â€“395.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer MattPost.2018. AcallforclarityinreportingBLEU
Levy, Veselin Stoyanov, and Luke Zettlemoyer. scores. In Proceedings of WMT 2018, pages 186â€“
2020. BART: Denoising sequence-to-sequence pre- 191.
trainingfornaturallanguagegeneration,translation,
and comprehension. In Proceedings of ACL 2020, Michael Ringgaard, Rahul Gupta, and Fernando CN
pages7871â€“7880. Pereira.2017. Sling:Aframeworkforframeseman-
ticparsing. arXiv:1710.07032.
Zhongwei Li, Xuancong Wang, Ai Ti Aw, Eng Siong
Chng,andHaizhouLi.2018. Named-entitytagging Rico Sennrich, Barry Haddow, and Alexandra Birch.
anddomainadaptationforbettercustomizedtransla- 2016. Improvingneuralmachinetranslationmodels
tion. InProceedingsoftheSeventhNamedEntities withmonolingualdata. InProceedingsofACL2016,
Workshop,pages41â€“46. pages86â€“96.
Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Jiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre- Yan Liu. 2019. MASS: Masked sequence to se-
training multilingual neural machine translation by quencepre-trainingforlanguagegeneration. InPro-
leveraging alignment information. In Proceedings ceedings of ICML 2019, volume 97, pages 5926â€“
ofEMNLP2020,pages2649â€“2663. 5936.
Ying Liu. 2015. The technical analyses of named en- Lucia Specia, FrÃ©dÃ©ric Blain, Varvara Logacheva,
titytranslation. InProceedingsofISCI2015,pages RamÃ³nF.Astudillo,andAndrÃ©F.T.Martins.2018.
2028â€“2037. FindingsoftheWMT2018sharedtaskonqualityes-
timation. InProceedingsofWMT2018,pages689â€“
YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey 709.
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising JÃ¶rg Tiedemann. 2012. Parallel data, tools and inter-
pre-training for neural machine translation. TACL, facesinOPUS. InProceedingsofLREC2012,pages
8:726â€“742. 2214â€“2218.
1762
Daniel Torregrosa, Nivranshu Pasricha, Maraim Ma- multi-task learning. In Proceedings of WMT 2019,
soud, Bharathi Raja Chakravarthi, Juan Alonso, pages565â€“571.
Noe Casas, and Mihael Arcan. 2020. Aspects
of terminological and named entity knowledge
within rule-based machine translation models for
under-resourced neural machine translation scenar-
ios. arXiv:2009.13398.
ArataUgawa, AkihiroTamura, TakashiNinomiya, Hi-
royaTakamura, andManabuOkumura.2018. Neu-
ral machine translation incorporating named entity.
InProceedingsofCOLING2018,pages3240â€“3250.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Å ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
youneed. InAdvancesinNeurIPS,volume30.
Denny VrandeÄiÄ‡ and Markus KrÃ¶tzsch. 2014. Wiki-
data:afreecollaborativeknowledgebase. Communi-
cationsoftheACM,57(10):78â€“85.
StephenWanandCorneliaMariaVerspoor.1998. Au-
tomatic English-Chinese name transliteration for
development of multilingual resources. In ACL-
COLING1998,pages1352â€“1356.
Jian-ChengWuandJasonS.Chang.2007. Learningto
find English to Chinese transliterations on the web.
InProceedingsofEMNLP-CoNLL2007,pages996â€“
1004.
FanYang,JunZhao,andKangLiu.2009. AChinese-
Englishorganizationnametranslationsystemusing
heuristicwebminingandasymmetricalignment. In
ProceedingsofACL-IJCNLP2009,pages387â€“395.
Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ingsource-sidemonolingualdatainneuralmachine
translation. InProceedingsofEMNLP2016, pages
1535â€“1545.
MinZhang,HaizhouLi,JianSu,andHendraSetiawan.
2005. Aphrase-basedcontext-dependentjointprob-
ability model for named entity translation. In IJC-
NLP2005.
Yang Zhao, Lu Xiang, Junnan Zhu, Jiajun Zhang,
YuZhou, andChengqingZong.2020a. Knowledge
graphenhancedneuralmachinetranslationviamulti-
tasklearningonsub-entitygranularity. InProceed-
ingsofCOLING2020,pages4495â€“4505.
Yang Zhao, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2020b. Knowledge graphs enhanced neural
machinetranslation. InProceedingsofIJCAI2020,
pages4039â€“4045.
Leiying Zhou, Wenjie Lu, Jie Zhou, Kui Meng, and
GongshenLiu.2020. Incorporatingnamedentityin-
formation into neural machine translation. In Pro-
ceedingsofNLPCC2020,pages391â€“402.
ShuyanZhou, XiangkaiZeng, YingqiZhou, Antonios
Anastasopoulos,andGrahamNeubig.2019. Improv-
ing robustness of neural machine translation with
1763
Appendix
A FinetuningBLEUCurves
WereportBLEUscoreforthreelanguagepairscalculatedfromcheckpointsatdifferentfinetuningstepsin
Figure5. Foralllanguagepairs,allpre-trainingmethodsresultinasignificantincreaseintermsofBLEU
throughoutthefinetuninginbothsingle-taskandmulti-tasksetting. Inparticular,thedifferencesinBLEU
betweenDEEPandtheotherbaselinesaremostsignificantatthebeginningofthefinetuningstage.
Random MT DAE MT DEEP MT DAE DAE + MT DEEP DEEP + MT
20.0 20.0 12
17.5 17.5
10
15.0 15.0
12.5 12.5 8
10.0 10.0
6
7.5 7.5
5.0 5.0 4
5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40
Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000)
Russian Ukrainian Nepali
Figure5:BLEUscoresfor3languagepairsovervariousfinetuningsteps.
B EntityTranslationAccuracyforotherlanguages
We show the entity translation accuracy performance over various finetuning steps for Ukrainian and
NepaliinFigure6,7,andshowthegainsofthreepre-trainingmethodsovertherandombaselinewith
respecttotheentityfrequenciesinFigure8,9. Emptycellsintheheatmapsareduetonoentitiesthat
meettheconditionsinthosecells.
Ukrainian: AsseeninFigure6,thegeneraltrendfortheentitytranslationaccuracyaccordingtoentity
groupsaresimilartothatofRussian. WhileDEEPachievesthehighestaccuracyinFT,theresultsfor
FT is less reliable due to a small sample size of entities in FT. In terms of the gain from Random â†’
MTaccordingtotheentityfrequency,weobserveaconsistentimprovementofourmulti-taskDEEPon
translatinglow-frequententitiesinthefinetuningdata(SeetheleftbottomofFigure8).
Nepali: Whileoutperformingatthebeginningoffinetuning,Figure7showsthatDEEPâ†’DEEP+MT
eventuallyunder-performedfortranslationsofentitiesinPFTdata. Moreover,theaccuracyisconsiderably
loweronentitiesinPT,whichsuggeststhatthedegreeofforgettingismuchmoreconspicuousinNepali.
ThegainfromRandomâ†’MTwithrespecttotheentityfrequencyexhibitedadifferenttrendfromRussian
andUkrainian. Figure9showstheresults. Inthesingle-tasksetting,DEEPimprovethetranslationsof
frequententitiesappearinginboththepre-trainingandfinetuningdata. Despitethemulti-tasklearning
thatintroducesadditionalexposuretoentitiesthataremorefrequentinthepre-trainingdata,thelargest
gaincomesfromentitiesthatarelessfrequentinthepre-trainingdatabutfrequentinthefinetuningdata.
C ScientificArtifacts
InTable6,weprovidethedetailedinformationaboutthescientificartifacts(e.g.,data,code,tools)used
inourpaper. Wehavecheckedthedatausedinthisworktomakesurethatwedonotintentionallyuse
privateorsensitiveinformationoroffensivecontentforderivingtheobservationsandconclusionsfrom
ourwork. AlthoughWikiDatamaycontainthenameofsomeindividualpeople(e.g.,famouspeoplethat
haveWikipediawebpages),wedonotusetheirsensitiveinformationinouranalysis.
1764
UELB
Random MT DAE MT DEEP MT DAE DAE + MT DEEP DEEP + MT
70
70 30
60
60
25
50
50
20
40
40
15
30
30
10 20
20
5 10
10
0 0
0
5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40
Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000)
Entities in PFT data. Entities in PT data. Entities in FT data.
Figure6:EntitytranslationaccuracyaggregatedoverdifferententitysetsforUkrainian.
Random MT DAE MT DEEP MT DAE DAE + MT DEEP DEEP + MT
2.5 30
40
2.0 25
30 20
1.5
15
20 1.0
10
10 0.5
5
0 0.0 0
5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40
Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000)
Entities in PFT data. Entities in PT data. Entities in FT data.
Figure7:EntitytranslationaccuracyaggregatedoverdifferententitysetsforNepali.
(34995, 460576] 0.0 11.8 3.6 6.8 0.0 8.3 8.6 7.5 25.0 13.9 10.0 7.7 40
(11626, 34995] 19.6 6.5 9.2 2.0 26.8 7.6 5.6 0.7 35.7 13.4 8.4 2.6 20
(2840, 11626] 0.0 10.1 5.9 2.8 5.8 0.0 11.9 8.1 4.4 0.6 0.0 27.4 12.2 2.6 9.6 0
(2, 2840] 8.9 11.5 -0.4 8.3 -16.7 11.6 13.8 4.2 0.0 -16.7 21.4 20.1 3.7 8.3 0.0 20
0 0.0 50.0 0.0 0.0 0.0 50.0 0.0 0.0 50.0 25.0 0.0 0.0 40
0 (1, 23] (23, 83] (83, 253 (] 253, 6178] 0 (1, 23] (23, 83] (83, 253 (] 253, 6178] 0 (1, 23] (23, 83] (83, 253 (] 253, 6178]
Freq. bins from Finetuning data. Freq. bins from Finetuning data. Freq. bins from Finetuning data.
Gain with DAE MT Gain with DEEP MT Gain with DEEP DEEP+MT
Figure8:GainfromRandomâ†’MTinentitytranslationaccuracyforUkrainianforeachmodel.
1765
ycaruccA
noitalsnarT
ytitnE
egarevA
ycaruccA
noitalsnarT
ytitnE
egarevA
.atad
gniniarterP
morf
snib
.qerF
(1694, 65084] 0.0 0.0 9.2 3.7 12.3 0.0 0.0 11.6 8.8 15.3 0.0 5.9 17.5 -2.3 15.5 40
(664, 1694] 0.0 0.0 6.8 10.9 11.1 0.0 0.0 9.9 13.4 12.9 0.0 10.0 9.1 6.1 -0.6 20
(166, 664] 0.0 5.9 2.1 -2.7 6.2 0.0 5.9 2.1 9.2 10.4 1.8 12.5 11.2 2.4 31.2 0
(1, 166] 1.1 2.2 3.8 22.5 33.3 1.1 2.2 15.5 37.5 0.0 1.8 6.0 8.6 21.7 33.3 20
0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 50.0 0.0 40
0 (1, 12] (12, 62] (62, 318 (3] 18, 39131] 0 (1, 12] (12, 62] (62, 318 (3] 18, 39131] 0 (1, 12] (12, 62] (62, 318 (3] 18, 39131]
Freq. bins from Finetuning data. Freq. bins from Finetuning data. Freq. bins from Finetuning data.
Gain with DAE MT Gain with DEEP MT Gain with DEEP DEEP+MT
Figure9:GainfromRandomâ†’MTinentitytranslationaccuracyforNepaliforeachmodel.
Artifact License/Term Documentation
WikiData(VrandeÄiÄ‡andKrÃ¶tzsch,2014) CreativeCommonsCC0 Thisresourceisafreeknowledgebasethatsupportsvariousresearchandprojectw.
Sling(Ringgaardetal.,2017) Apache-2.0 ThistoolisintendedtouseforanalyzeWikiDataandWikipediaarticles.
WMT18En-RuData(Speciaetal.,2018) Open-sourced ThisdatasetisintendedtobeusedforMTonnewstexts.
OPUSData(Tiedemann,2012) Open-sourced ThisdataresourceisintendedtobeusedforMT.
FLORESData(GuzmÃ¡netal.,2019) CC-BY-SA-4.0License Thisdatasetisintendedtobeusedforlow-resourceMT.
Fairseq(Ottetal.,2019) MITLicense Thistoolisintendedtofacilitatedeeplearningresearch.
Table6:Detailinformationaboutscientificartifactsusedinthispaper.
1766
.atad
gniniarterP
morf
snib
.qerF

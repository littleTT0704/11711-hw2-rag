DEEP: DEnoising Entity Pre-training for Neural Machine Translation
JunjieHu1,HiroakiHayashi3,KyunghyunCho2,GrahamNeubig3
1UniversityofWisconsin-Madison,2NewYorkUniversity,3CarnegieMellonUniversity
junjie.hu@wisc.edu, kyunghyun.cho@nyu.edu
{hiroakih,gneubig}@cs.cmu.edu
Abstract lemwillbefurtherexacerbatedinthecross-domain
transfersettingsorinthecaseofemergingentities.
It has been shown that machine translation
Because of this, there have been a number of
models usually generate poor translations for
methodsproposedspecificallytoaddresstheprob-
namedentitiesthatareinfrequentinthetrain-
lemoftranslatingentities. AsnotedbyLiu(2015),
ing corpus. Earlier named entity translation
earlierstudiesonnamedentitytranslationlargely
methods mainly focus on phonetic transliter-
ation, which ignores the sentence context for focusedonrule-basedmethods(WanandVerspoor,
translation and is limited in domain and lan- 1998),statisticalalignmentmethods(Huangetal.,
guage coverage. To address this limitation, 2003,2004)andWebminingmethods(Huangetal.,
we propose DEEP, a DEnoising Entity Pre- 2005; Wu and Chang, 2007; Yang et al., 2009).
training method that leverages large amounts
However,thesemethodshavetwomainissues.First,
of monolingual data and a knowledge base
as they generally translate a single named entity
to improve named entity translation accuracy
withoutanycontextinasentence,itmakesitdiffi-
within sentences. Besides, we investigate a
multi-task learning strategy that finetunes a culttoresolveambiguityinentitiesusingcontext.
pre-trained neural machine translation model Inaddition,thetranslationofentitiesisoftenper-
on both entity-augmented monolingual data formedinatwo-stepprocessofentityrecognition
andparalleldatatofurtherimproveentitytrans- thentranslation,whichcomplicatesthetranslation
lation. Experimentalresultsonthreelanguage
pipelineandcanresultincascadingerrors(Huang
pairsdemonstratethatDEEPresultsinsignifi-
etal.,2003,2004;Chenetal.,2013).
cantimprovementsoverstrongdenoisingauto-
encoding baselines, with a gain of up to 1.3 In this paper, we focus on a simple yet effec-
BLEUandupto9.2entityaccuracypointsfor tivemethodthatimprovesnamedentitytranslation
English-Russiantranslation.1 within context. Specifically, we do so by devis-
ingadataaugmentationmethodthatleveragestwo
1 Introduction datasources: monolingualdatafromthetargetlan-
guage and entity information from a knowledge
Propertranslationofnamedentitiesiscriticallyim-
base(KB).Ourmethodalsoadoptsaprocedureof
portantforaccuratelyconveyingthecontentoftext
pre-training and finetuning neural machine trans-
inanumberofdomains,suchasnewsorencyclope-
lation(NMT)modelsthatisusedbymanyrecent
dictext(KnightandGraehl,1998;Al-Onaizanand
works(LuongandManning,2015;NeubigandHu,
Knight,2002a,b). Inaddition,agrowingnumber
2018; Song et al., 2019; Liu et al., 2020). In par-
ofnewnamedentities(e.g.,personname,location)
ticular,pre-trainingmethodsthatusemonolingual
appeareveryday,thereforemanyoftheseentities
data to improve translation for low-resource and
maynotexistintheparalleldatatraditionallyused
medium-resource languages mainly rely on a de-
totrainMTsystems. Asaresult,evenstate-of-the-
noisingauto-encodingobjectivethatattempttore-
artMTsystemsstrugglewithentitytranslation. For
construct parts of text (Song et al., 2019) or the
example,Laublietal.(2020)notethataChinese-
wholesentences(Liuetal.,2020)fromnoisedin-
Englishnewstranslationsystemthathadallegedly
put sentences without particularly distinguishing
reached human parity still lagged far behind hu-
named entities and other functional words in the
mantranslatorsonentitytranslations,andthisprob-
sentences.Incontrast,ourmethodexploitsanentity
linkertoidentifyentityspansinthemonolingual
1Code/data/modelsarereleasedathttps://github.
com/JunjieHu/deep. sentencesandlinkthemtoaKBthatcontainsmul-
1753
Proceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics
Volume1:LongPapers,pages1753-1766
May22-27,2022(cid:13)c2022AssociationforComputationalLinguistics
EntityRecognitionandLinking
МагазиныновогоформатазаработаливКраснодарe,СаратовeиУльяновскe.
Krasnodar Saratov Ulyanovsk
(Q3646) (Q5332) (Q5627)
Language Label Description Language Label ... Language Label ...
English Krasnodar capitalofKrasnodarregion(Krai)inSouthernRussia English Saratov ... English Ulyanovsk ...
Russian Краснодар городнаюгеРоссии,административныйцентр Russian Саратов ... Russian Ульяновск ...
: : Краснодарскогокрая : : : :
Pre-trainingwithDEEP
[DEEP]МагазиныновогоформатазаработаливKrasnodar,SaratovиUlyanovsk.
МагазиныновогоформатазаработаливКраснодарe,СаратовeиУльяновскe.
Multi-taskFinetuning
[MT]ThesenewformatstoreshaveopenedforbusinessinKrasnodar,Saratov,andUlyanovsk.
[DEEP]МагазиныновогоформатазаработаливKrasnodar,SaratovиUlyanovsk.
МагазиныновогоформатазаработаливКраснодарe,СаратовeиУльяновскe.
Figure 1: General workflow of our method. Entities in a sentence is extracted and linked to Wikidata, which
includestheirtranslationsinmanylanguages. DEEPusesthenoisefunction 𝑓(𝑦,KB) thatreplacesentitieswith
thetranslationsforpre-training.DEEPisalsoemployedduringfinetuninginamulti-tasklearningmanner.
tilingualtranslationsoftheseentities(suchasWiki- isdefinedasfollows:
data(VrandečićandKrötzsch,2014)). Wethengen-
(cid:213)
erate noised sentences by replacing the extracted LDAE(D𝑌,𝜃) = log𝑃(𝑦|𝑔(𝑦);𝜃), (1)
entityspanswiththeirtranslationsintheknowledge 𝑦∈D𝑌
baseandpre-trainourNMTmodelstoreconstruct
where𝜃 denotesthemodel’slearnableparameters.
the original sentences from the noised sentences.
For notation simplicity, we drop 𝜃 in the rest of
Tofurtherimprovetheentitytranslationaccuracy
the sections. This formulation encompasses sev-
and avoid forgetting the knowledge learned from
eraldifferentpreviousworksindataaugmentation
pre-training,wealsoexamineamulti-tasklearning
forMT,suchasmonolingualdatacopying(Currey
strategythatfinetunestheNMTmodelusingboth
et al., 2017), where 𝑔(·) is the identity function,
thedenoisingtaskonthemonolingualdataandthe
backtranslation(Sennrichetal.,2016),where𝑔(·)
translationtaskontheparalleldata.
isabackwardstranslationmodel,aswellasheuris-
IntheexperimentsonEnglish-Russian,English-
ticnoisefunctions(Songetal.,2019;Lewisetal.,
Ukrainian,andEnglish-Nepalitranslations,DEEP
2020;Liuetal.,2020)thatrandomlysamplenoise
outperforms the strong denoising auto-encoding
accordingtomanuallydevisedheuristics.
baselinewithrespecttoentitytranslationaccuracy,
In particular, as our baseline we focus on the
and obtains comparable or slightly better overall
mBART method (Liu et al., 2020), a popular
translationaccuracyasmeasuredbyBLEU.Afine-
methodwithtwotypesofheuristicnoisefunctions
grainedanalysisshowsthatourmulti-taskfinetun-
beingusedsequentiallyoneachtextsegment. The
ing strategy improves the translation accuracy of
firstnoisefunctionrandomlymasksspansoftext
theentitiesthatdonotexistinthefinetuningdata.
in each sentence. Specifically, a span length is
firstrandomlysampledfromaPoissondistribution
2 DenoisingAuto-Encoding(DAE)
(𝜆 = 0.35)andthebeginninglocationforaspanin𝑦
Givenasetofmonolingualtextsegmentsforpre- isalsorandomlysampled. Theselectedspanoftext
training,i.e., 𝑦 ∈ D𝑌,asequence-to-sequencede- isreplacedbyamasktoken. Thisprocessrepeats
noisingauto-encoderispre-trainedtoreconstructa until35%ofwordsinthesentencearemasked. The
textsegment𝑦fromitsnoisedversioncorruptedby second noise function is to permute the sentence
anoisefunction𝑔(·). Formally,theDAEobjective orderineachtextsegmentwithaprobability.
1754
3 DEEP:DenoisingEntityPre-training where the swap() function swaps occurrences of
one entity span 𝑡 in 𝑦 with its translation 𝑠 in the
Ourmethodadoptsaprocedureofpre-trainingand
sourcelanguage. Forexample, inthesecondbox
finetuning for neural machine translation. First,
ofFigure1,thenamedentities“Краснодаре,Са-
we apply an entity linker to identify entities in a
ратовеandУльяновске”inRussianarereplaced
monolingualcorpusandlinkthemtoaknowledge
by “Krasnodar, Saratov, and Ulyanovsk” in En-
base(§3.1). Wethenutilizeentitytranslationsin
glish. After the replacement, we create a noised
theknowledgebasetocreatenoisycode-switched
code-switchedsegmentwhichexplicitlyincludes
dataforpre-training(§3.2). Finally,weexaminea
thetranslationsofnamedentitiesinthecontextof
multi-tasklearningstrategytofurtherimprovethe
thetargetlanguage. Forsomesegmentsthatcon-
translationoflow-frequencyentities(§3.3).
tain fewer entities, their code-switched segments
maybesimilartothem, whichpotentiallyresults
3.1 EntityRecognitionandLinking
in a easier denoising task. Therefore, we further
The goal of this part is to identify entities in
addnoisetothesecode-switchedsegments. Todo
eachmonolingualsegmentandobtaintheirtransla-
so,ifthewordcountofthereplacedentityspansis
tions. Tothisend,weuseWikidata(Vrandečićand
lessthanafraction(35%)ofthewordcountinthe
Krötzsch,2014)apublicmultilingualknowledge
segment, werandomlymasktheothernon-entity
basethatcovers94Mentities.2 Eachentityisrepre-
words to ensure that about 35% of the words are
sentedinsurfaceformsfromdifferentlanguagesin
either replaced or masked in the noised segment.
whichaWikipediaarticleexists. Therefore,linking
Finally, we follow Liu et al. (2020) to randomly
anentitymention𝑡 inatarget-languagesegment 𝑦
permute the sentence order in 𝑦. We then train
to an entity 𝑒 in Wikidata allows us to obtain the
a sequence-to-sequence model to reconstruct the
multilingualtranslationsoftheentity,thatis,
originalsentence 𝑦 fromitsnoisedcode-switched
sentenceasfollows:
∀𝑡 ∈ 𝑦,∃𝑒 ∈ KB : 𝑇 𝑒 = surface(𝑒,KB),𝑡 ∈𝑇 𝑒
(cid:213)
LDEEP(D𝑌,KB) = log𝑃(𝑦| 𝑓(𝑦,KB))
where𝑇 denotesasetofmultilingualsurfaceforms
𝑒 𝑦∈D𝑌
of 𝑒. We can define the translate operation as:
𝑠 = lookup(𝑇 𝑒,𝑋) whichsimplylooksforthesur- 3.3 Multi-taskFinetuning
faceformof𝑒 inthesourcelanguage 𝑋. Notethat
Afterpre-training,wecontinuefinetuningthepre-
this strategy relies on the fact that translations in
higher-resourcelanguagesareincludedin𝑇 ,which
trained model on a parallel corpus (𝑥,𝑦) ∈ D𝑋𝑌
𝑒 formachinetranslation.
weadoptbyusingEnglishinourexperiments. In
general,however,𝑇 doesnotuniversallycoverall
𝑒
thelanguagesofinterest. Forentityrecognitionand
(cid:213)
linking,weuseSLING(Ringgaardetal.,2017),3
LMT(D𝑋𝑌) = log𝑃(𝑦|𝑥) (3)
whichbuildsanentitylinkerforarbitrarylanguages
(𝑥,𝑦)∈D𝑋𝑌
availableinWikipedia.
Toavoidforgettingtheentityinformationlearned
from the pre-training stage, we examine a multi-
3.2 Entity-basedDataAugmentation
tasklearningstrategytotrainthemodelbyboththe
AfterobtainingentitytranslationsfromtheKB,we pre-trainingobjectiveonthemonolingualdataand
attempttoexplicitlyincorporatethesetranslations thetranslationobjectiveontheparalleldata. Since
intothemonolingualsentencesforpre-training. To monolingual segments are longer text sequences
doso,wedesignanentity-basednoisefunctionthat thansentencesin D𝑋𝑌 andthesizeof D𝑌 isusu-
takes in a sentence 𝑦 and the KB, i.e., 𝑓(𝑦,KB). allylargerthanthatofD𝑋𝑌,simplyconcatenating
First, we replace all detected entity spans in the both data for multi-task finetuning leads to bias
sentencebytheirtranslationsfromtheKB: towarddenoisinglongersequencesratherthanac-
tually translating sentences. To balance the two
replace(𝑦,KB) = swap(𝑠,𝑡,𝑦), ∀𝑡 ∈ 𝑦 (2) tasks,ineachepochwerandomlysampleasubset
2DumpJune14,2021.CreativeCommonsCC0License.
ofmonolingualsegmentsD 𝑌(cid:48) fromD𝑌,wherethe
3https://github.com/google/sling, Apache-
totalsubwordcountofD 𝑌(cid:48) equalstothatofD𝑋𝑌,
2.0License i.e., (cid:205) |𝑦| = (cid:205) max(|𝑥|,|𝑦|). We
𝑦∈D𝑦(cid:48) (𝑥,𝑦)∈D𝑋𝑌
1755
Entity Coverage(F) Coverage(T)
Lang. Token Para. Lang. Train Dev Test
Type Count N Type Count Type Count
Ru 775M 1.8M 1.4M 337M 123 En-Ru 235K 3.0K 3.0K 88% 94% 88% 91%
Uk 315M 654K 524K 140M 149 En-Uk 200K 2.3K 2.5K 87% 94% 91% 94%
Ne 19M 26K 17K 2M 34 En-Ne 563K 2.6K 2.8K 35% 25% 44% 27%
Table1:StatisticsofWikipediacorporainRussian(Ru), Table2: Statisticsoftheparalleltrain/dev/testdatafor
Ukrainian(Uk)andNepali(Ne)forpre-training. 𝑁 de- finetuning. Coverage(F/T)representthepercentageof
notestheaveragesubwordcountofentityspansinase- entity types and counts in the Finetuning (Test) data
quenceof512subwords. thatarecoveredbythepre-trainingdata.
thenexaminethemulti-taskfinetuningasfollows: weusetheFLORESdatasetinGuzmánetal.(2019)
andfollowthepaper’ssettingtofinetuneonparallel
LMulti-task = LMT(D𝑋𝑌)+LPre-train(D 𝑌(cid:48)) (4) dataintheOPUSrepository.Table2showsthedata
statisticsoftheparalleldataforfinetuning. Notice
wherethepre-trainingobjectiveLPre-train iseither
thatfromthelastfourcolumnsofTable2,theen-
DAEorDEEPwithDEEPhavinganadditionalin-
titiesinthepre-trainingdatacoveratleast87%of
putofaknowledgebase. Noticethatwiththesam-
theentitytypesand91%oftheentitycountsinboth
plingstrategyforthemonolingualdata,wedouble
finetuningandtestdataexcepttheEn-Nepair.
the batch size in the multi-task finetuning setting
with respect to that in the single-task finetuning Architecture: We use a standard sequence-to-
setting. Therefore, wemakesurethatthemodels sequenceTransformermodel(Vaswanietal.,2017)
arefinetunedonthesameamountofparalleldata with 12 layers each for the encoder and decoder.
inboththesingle-taskandmulti-tasksettings,and Weuseahiddenunitsizeof512and12attention
thegainsfromthemulti-tasksettingsorelycome heads. FollowingLiuetal.(2020),weaddanaddi-
fromtheadditionaltaskonthemonolingualdata. tionallayer-normalizationlayerontopofboththe
To distinguish the tasks during finetuning, we encoderanddecodertostabilizetrainingatFP16
replace the start token ([BOS]) in a source sen- precision. Weusethesamesentencepiece model
tence or a noised segment by the corresponding andthevocabularyfromLiuetal.(2020).
tasktokensforthetranslationorthedenoisingtask
MethodsinComparison: Wecomparemethods
([MT], [DAE] or [DEEP]). We initialize these
inthesingletaskandmulti-tasksettingasfollows:
taskembeddingsbythestarttokenembeddingand
appendthemtothewordembeddingmatrixofthe
• Random→MT:Weincludeacomparisonwith
encoder.
arandomlyinitializedmodelwithoutpre-training
andfinetunethemodelforeachtranslationtask.
4 ExperimentalSetting
• DAE→MT:Wepre-trainamodelbyDAEusing
Pre-trainingData: Weconductourexperiments thetwonoisefunctionsinLiuetal.(2020)and
onthreelanguagepairs: English-Russian,English- finetunethemodelforeachtranslationtask.
UkrainianandEnglish-Nepali. WeuseWikipedia
• DEEP→MT:Wepre-trainamodelusingour
articlesasthemonolingualdataforpre-trainingand
proposedDEEPobjectiveandfinetunethemodel
reportthedatastatisticsinTable1. Wetokenizethe
onthetranslationtask.
textusingthesamesentencepiecemodelasLiuetal.
(2020),andtrainonsequencesof512subwords.
• DAE→DAE+MT:Wepre-trainamodelbythe
DAEobjectiveandfinetunethemodelforboth
Finetuning&TestData: Weusethenewscom- theDAEtaskandtranslationtask.
mentarydatafromtheEnglish-Russiantranslation
• DEEP→DEEP+MT:Wepre-trainamodelby
taskinWMT18(Speciaetal.,2018)forfinetuning
theDEEPobjectiveandfinetunethemodelfor
andevaluatetheperformanceontheWMT18test
boththeDEEPtaskandtranslationtask.
datafromthenewsdomain. ForEnglish-Ukrainian,
weusetheTEDTalktranscriptsfromJuly2020in Learning&Decoding: Wepre-trainallmodels
theOPUSrepository(Tiedemann,2012)forfine- for 50K steps first using the default parameters
tuningandtesting. ForEnglish-Nepalitranslation, in Liu et al. (2020) except that we use a smaller
1756
batchof64textsegments,eachofwhichhas512 lationsinthepre-trainingdataandobtainagood
subwords. We use the Adam optimizer (𝜖=1e-6, initializationfortranslationatthebeginningofthe
𝛽 =0.98) and a polynomial learning rate decay finetuningstage. Asthemulti-taskfinetuningpro-
2
schedulingwithamaximumstepat500K.Allmod- ceeds,themodelstrainedbybothDAEandDEEP
elsarepre-trainedononeTPUv3(128GB)forabout relymoreonthetranslationtaskthanthedenoising
12hoursfor50Ksteps.4 Weapplythenoisefunc- taskfortranslatingawholesentence. Thusthenu-
tion on the monolingual data on the fly for each anceoftheentitytranslationsmightnotbeclearly
epoch,andthistakesonlyafewminutesbymulti- evaluatedaccordingtoBLEUorchrF.
processinginFairseq(Ottetal.,2019). Wethen
resetthelearningrateschedulerandcontinuefine- 5.2 EntityTranslationAccuracy
tuningourpre-trainedmodelsontheMTparallel Sincecorpus-levelmetricslikeBLEUorchrFmight
data for 40K steps. Single-task (multi-task) fine- not necessarily reveal the subtlety of named en-
tuning takes about 16 (32) hours on 2 RTX 3090 titytranslations,inthesectionweperformafine-
GPUs. We set the maximum number of tokens grained evaluation by the entity translation accu-
in each batch to 65,536 in the single task setting racy which counts the proportion of entities cor-
anddoublethebatchsizeinthemulti-tasksetting rectlytranslatedinthehypotheses. Specifically,we
to ensure that models in both settings are trained firstuseSLINGtoextractentitiesforeachpairof
onanequalamountofparalleldata,andthusany a reference and a hypothesis. We then count the
performancegaincanonlybeattributedtomonolin- translationaccuracyofanentityastheproportion
gualdataduringfinetuning. Weuse2,500warm-up ofcorrectlymentioningtherightentityinthehy-
steps to reach a maximum learning rate of 3e-5, potheses,followedbymacro-averagingtoobtainthe
anduse0.3dropoutand0.2labelsmoothing. After averageentitytranslationaccuracy. Wealsoshow
training,weusebeamsearchwithabeamsizeof theaccuracyscoresinTable3. First,ourmethod
5andreporttheresultsinsacreBLEU(Post,2018) inbothsingle-andmulti-tasksettingssignificantly
followingthesameevaluationinLiuetal.(2020). outperformedtheotherbaselines. Inparticular,the
gains from DEEP are much clear for the En-Uk
5 Discussion andEn-Rutranslations. Onepossiblereasonisthat
Russian or Ukrainian entities extracted from the
5.1 Corpus-levelEvaluation
pre-trainingdatahavearelativelyhighercoverage
In Table 3, we compare all methods in terms of
oftheentitiesinboththefinetuningandtestdata
BLEU (Papineni et al., 2002) and chrF (Popović,
asreportedinTable2. However,SLINGmightnot
2015)onthetestdataforthreelanguagepairs. First,
detectasmanyentitiesinNepaliasintheotherlan-
wefindthatallpre-trainingmethodssignificantly
guages. Webelievethatfutureadvancesonentity
outperformtherandombaseline. Inparticular,our
linkinginlow-resourcelanguagescouldpotentially
DEEPmethodobtainsagainof3.5BLEUpointsin
improve the performance of DEEP further. We
thesingletasksettingforthelow-resourceEn-Ne
leavethisasourfuturework.
translation. Second,wecomputestatisticalsignifi-
canceoftheBLEUandchrFscoreswithbootstrap 5.3 Fine-grainedAnalysisonEntity
resampling(Koehn,2004),andweobservesignifi- TranslationAccuracy
cantimprovementswiththemulti-taskfinetuning
In this section, we further analyze the effect on
strategyoverthesingle-taskfinetuningforEn-Ru
differentcategoriesofentitiesusingourmethod.
and En-Ne. Our DEEP method outperforms the
DAE method for En-Ru translation by 1.3 BLEU PerformanceofEntityGroupsoverFinetuning:
pointsinthemulti-tasksetting. Itisalsoworthnot- Themodelisexposedtosomeentitiesmoreoften
ing that DEEP obtains higher BLEU points than thanothersatdifferentstages: pre-training,finetun-
DAEatthebeginningofthemulti-taskfinetuning ingandtesting,whichraisesaquestion: howisthe
process,howeverthegapbetweenbothmethodsde- entitytranslationaffectedbytheexposureduring
creasesasthefinetuningproceedsforlongersteps eachstage? Toanswerthisquestion,wedividethe
(SeeAppendixA).Onepossiblereasonisthatmod- entitiesappearinginthetestdataintothreegroups:
elstrainedbyDEEPbenefitfromtheentitytrans- • PFT:entitiesappearinginthepre-training,fine-
tuning,andtestdata.
4AsweshowinFigure4,modelspre-trainedfor50Ksteps
provideareasonablygoodinitialization. • PT:entitiesonlyinthepre-trainingandtestdata.
1757
BLEU chrF EntityTranslationAcc.
Pre-train→Finetune
En-Uk En-Ru En-Ne En-Uk En-Ru En-Ne En-Uk En-Ru En-Ne
Random→MT 17.1 15.0 7.7 37.0 36.8 24.3 49.5 31.1 20.9
DAE→MT 19.5 18.5 10.5 39.2 40.4 26.8 56.7 37.7 26.0
DEEP→MT 19.4 18.5 11.2∗ 39.2 40.7∗ 27.7∗ 57.7 40.6∗ 28.6∗
DAE→DAE+MT 19.4 18.5 11.2 39.1 41.0 27.8 58.8 47.2 27.9
DEEP→DEEP+MT 19.7 19.6∗ 11.5 39.1 42.4∗ 28.2∗ 61.9∗ 56.4∗ 28.3
Table3: BLEU,Entitytranslationaccuracy,andchrFinsingle-andmulti-tasksettings. Largestnumbersineach
columnarebold-faced.∗indicatesstatisticalsignificanceofDEEPwith𝑝 <0.05toDAEintherespectivesettings.
Random MT DAE MT DEEP MT DAE DAE + MT DEEP DEEP + MT
70 70 70
60 60 60
50 50 50
40 40 40
30 30 30
20 20 20
10 10 10
0 0 0
5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40
Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000)
Entities in PFT data. Entities in PT data. Entities in FT data.
Figure2: EntitytranslationaccuracyscoresaggregatedoverdifferententitysetsforRussian. PFT,PT,FTdata
correspondtoentitiesappearingin(i)pre-training,finetuningandtestdata,(ii)onlypre-trainingandtestdata(iii)
onlyfinetuningandtestdata.
• FT:entitiesonlyinthefinetuningandtestdata. acheckpointwith40Kstepsoffinetuning,binthe
WeshowtheEnglish-to-Russianentitytransla- setofentitiesinthreedata(i.e. PFT,PT,FT)ac-
tionaccuracyscoresforeachgroupoverfinetuning cordingtofrequenciesineachofthedata. Wethen
stepsinFigure2. Overall,accuraciesarehigherfor calculatetheentitytranslationaccuracywithineach
theentitiesthatappearinthefinetuningdata(PFT, binbycomparingthemagainstreferenceentitiesin
FT),whichisduetotheexposuretothefinetuning therespectivesentences. Figure3showstheaccu-
data. Our proposed method consistently outper- racygainofeachpre-trainingmethodologiesfrom
formed baseline counterparts in both single- and Random→MT(i.e. nopre-training)ontestdata,
multi-tasksettings. Thedifferencesinaccuracyare groupedbytheentityfrequencybinsinpre-training
particularlylargeatearlierfinetuningsteps,which andfinetuningdata. Notethatleftmostcolumnand
indicatestheutilityofourmethodinlower-resource thebottomrowrepresentPT,FT,respectively. As
settings with little finetuning data. The effect of observed earlier, the proposed method improves
multi-taskfinetuningismostnotableforentitiesin moreovermostfrequencybins,withgreaterdiffer-
PT.Multi-taskfinetuningcontinuouslyexposesthe encesonentitiesthatarelessfrequentinfinetuning
modeltothepre-trainingdata,whichasaresultpre- data. Thistendencyisobservedmoresignificantly
ventsthemodelfromforgettingthelearnedentity forthemulti-taskvariant(DEEP→DEEP+MT),
translationsfromPT. wherethegainsaremostlyfromentitiesthatnever
appearedinfinetuningdata(i.e. leftmostcolumn).
Performance according to Entity Frequency: Multi-tasklearningwithDEEPthereforeprevents
Wefurtheranalyzetheentitytranslationaccuracy the model from forgetting the entity translations
scoresusingentityfrequenciesineachgroupintro- learnedatpre-trainingtime. Analyticalresultson
ducedabove. Thisprovidesamorefine-grainedper- UkrainianandNepaliareinAppendixB.
spectiveonhowfrequentorrareentitiesaretrans-
lated. Todoso,wetakeRussianhypothesesfrom
1758
ycaruccA
noitalsnarT
ytitnE
egarevA
(22440, 569372] 9.3 10.9 13.9 -3.8 3.3 6.0 11.6 15.5 3.0 2.4 44.3 17.1 27.1 1.4 4.6 40
(8424, 22440] 13.2 13.3 1.7 -2.1 -10.2 19.7 18.0 6.0 -0.1 -5.3 50.4 34.9 15.2 3.3 1.2 20
(2948, 8424] 3.9 16.0 12.4 4.2 13.5 9.3 15.9 15.0 7.4 15.7 32.4 27.7 21.9 6.9 18.7 0
(2, 2948] 7.8 12.7 -2.0 3.5 7.0 9.7 15.5 9.2 0.9 10.0 32.0 32.3 9.4 3.3 12.4 20
0 0.0 9.3 0.0 3.3 0.0 7.1 11.1 0.0 3.3 100.0 28.6 23.1 0.0 -3.3 100.0 40
0 (1, 2] (2, 6] (6, 23] (23, 1878] 0 (1, 2] (2, 6] (6, 23] (23, 1878] 0 (1, 2] (2, 6] (6, 23] (23, 1878]
Freq. bins from Finetuning data. Freq. bins from Finetuning data. Freq. bins from Finetuning data.
Gain with DAE MT Gain with DEEP MT Gain with DEEP DEEP+MT
Figure3:GainfromRandom→MTinentitytranslationaccuracyforeachmodel.
0.24M 4.25M
Methods
19
BLEU Acc. BLEU Acc. 40
Random→MT 15.0 31.1 15.7 39.4
DAE→MT 18.5 37.7 16.3 53.7 18 38
DEEP→MT 18.5 40.6 17.2 53.9
36
17
Table 4: Model comparisons across different finetun-
34
ingdatasizes. Theresultsontherightareobtainedaf-
16
ter finetuning on the combined news commentary and
BLEU 32
ParaCrawldata.
Entity Translation Accuracy
15
30
0 25 50 100 150 200
Pre-training Steps (x 1000)
5.4 OptimizationEffectsonDEEP
Figure4: English-to-RussianBLEUandEntitytransla-
Finetuning Data Size vs Entity Translation:
tionaccuracyscoresafterfinetuningfromvariablepre-
WhileDEEPprimarilyfocusesonalow-resource
trainingsteps.Finetuningisperformedfor40Ksteps.
setting, the evaluation with more resources can
highlight potential use in broader scenarios. To
thisend,weexpandthefinetuningdataforEnglish-
Russian translation with an additional 4 million
sentencepairsfromParaCrawl(Bañónetal.,2020),
aparalleldatacollectedfromwebpages. Although
webpagesmightcontainnewstext,ParaCrawldata proceeds. To analyze the efficiency of learning
covers more general domains. We finetune mod- entity translation during pre-training, we focus
elsonthecombineddataandevaluatewithBLEU onthequestion: howmanypre-trainingstepsare
andentitytranslationaccuracy. Table4showsthe neededfornamedentitytranslation? Toexamine
comparisonsacrossdifferentfinetuningdatasizes. thisquestion,wetakethesavedcheckpointstrained
When the model is initialized with pre-training by DEEP from various pre-training steps, and
methods,weobserveddecreasedBLEUpointsand apply the single-task finetuning strategy on the
increasedentitytranslationaccuracyscores. Thisis checkpoints for another 40K steps. We plot the
partlyduetothediscrepancyofdomainsbetween entity translation accuracy and BLEU on the test
ourfinetuningdata(news)andParaCrawl. Regard- data in Figure 4. We find that the checkpoint at
less, DEEP is consistently equal to or better than 25Kstepshasalreadyachievedacomparableentity
DAEinalltestedsettings. translationaccuracywithrespecttothecheckpoint
at150Ksteps. ThisshowsthatDEEPisefficientto
Pre-training Steps vs Entity Translation: learntheentitytranslationsasearlyasin25Ksteps.
Since DEEP leverages entity-augmented mono- Besides, both the BLEU and entity translation
lingual data, the model trained by DEEP revisits accuracykeepimprovingasthepre-trainingsteps
moreentitiesindifferentcontextasthepre-training increaseto200Ksteps.
1759
.atad
gniniarterP
morf
snib
.qerF
UELB
ycaruccA
noitalsnarT
ytitnE
Src: ThesenewformatstoreshaveopenedforbusinessinKrasnodar,Saratov,andUlyanovsk.
Ref: МагазиныновогоформатазаработаливКраснодаре,СаратовеиУльяновске.
(cid:13)1 ЭтиновыеформатовыемагазиныоткрылисьдлябизнесавАнридаре,КристофеиКуьянме.
(cid:13)2 Этиновыеформат@-@магазиныоткрылисьдлябизнесавКраснодаре,СараабанеивУругянскомуниверситете.
(cid:13)3 ЭтиновыемагазиныформатовоткрылисьдлябизнесавKrasnodar,SaratovиUlyanovsk.
(cid:13)4 ЭтиновыеформатныемагазиныоткрылисьдлябизнесавКраснодаре,СаратовеиУльяновске.
Src: InBarnaul,thenewasphaltonKrasnoarmeyskiyProspektisbeingdugup
Ref: ВБарнаулевскрываютновыйасфальтнапроспектеКрасноармейском
(cid:13)1 ВБарнауленовое,какразворачивающеесянажелезнополярномПроиссе,растет.
(cid:13)2 ВБарнале,новое,какразразилосьнаКрасно@-@МолгскискомПросвещении,растет.
(cid:13)3 Барнаул,новыймифнаKrasnoarmeyProspekt,выращивающийKrasnoarmeski.
(cid:13)4 ВБарнауленовыйасфальтнаКрасноармейскомпроспектевыращиваниерастет.
Table5: Qualitativecomparisonamongfourpre-trainingmethodsonnamedentitytranslations. (cid:13)1: DAE→MT,
(cid:13)2:DEEP→MT,(cid:13)3:DAE→DAE+MT,(cid:13)4:DEEP→DEEP+MT.
5.5 QualitativeAnalysis corpus (Huang et al., 2003, 2004; Zhang et al.,
2005) and Web mining methods built on top of
Inthissection,weselecttwoexamplesthatcontain
asearchengine(Huangetal.,2005;WuandChang,
entitiesappearingonlyinthepre-trainingandtest-
2007; Yang et al., 2009). Recently, Finch et al.
ingdata. Thefirstexamplecontainsthreelocation
(2016); Hadj Ameur et al. (2017); Grundkiewicz
names. Wefindthatthemodeltrainedbythesingle-
and Heafield (2018) used NMT to transliterate
taskDAEpredictsthewrongplaceswhichprovide
the wrong information in the translated sentence. named entities without any sentencecontext. An-
otherlineofresearch(Ugawaetal.,2018;Lietal.,
In addition, the model trained by the multi-task
2018;Torregrosaetal.,2020;Modrzejewskietal.,
DAE just copies the English named entities (i.e.,
2020;Zhouetal.,2020)onlyperformsentityrecog-
“Krasnodar”, “Saratov” and “Ulyanovsk”) to the
nition and uses entity tags (e.g., person) which
targetsentencewithoutactualtranslation. Incon-
arenotdirectlyinformativetothetranslationtask,
trast, our method predicts the correct translation
in contrast to the entity translations obtained by
for“Krasnodar”inbothsingle-taskandmulti-task
entity linking in our work. Besides, these meth-
setting,whilethemulti-taskDEEPtranslatesallen-
odsmodifymodelarchitecturetointegrateentity
titiescorrectly.Inthesecondexample,althoughour
tagembeddingsorknowledgegraphentityembed-
methodinthesingle-tasksettingpredictswrongfor
dings(Moussallemetal.,2019),whichalsorequire
alltheentities,themodelgeneratespartiallycorrect
extractingentityinformationforbothtrainingand
translationssuchas“Барнале”for“Барнауле”and
testdata.Incontrast,wefocusondataaugmentation
“Красно@-@Молгскиском”for“Красноармей-
ском”. NoticethatDEEPinthemulti-tasksetting
methodstoimprovenameentitytranslationwithin
translatesthecorrectentities“asphalt”and“Kras-
context,soourmethodiseasilyapplicabletoany
architecturesandtestdatawithoutpreprocessing.
noarmeyskiy”whichconveythekeyinformationin
thissentence. Incontrast,thetranslationproduced
Pre-trainingofNeuralMachineTranslationhas
bythemulti-taskDAEmethodliterallymeans“Бар-
beenshowneffectivebymanyrecentworks(Con-
наул(Barnaul),новый(new)миф(myth)на(at)
neauandLample,2019;Songetal.,2019;Liuetal.,
Krasnoarmey Prospekt, выращивающий (grow)
2020;Linetal.,2020),wheredifferentpre-training
Krasnoarmeski.”,whichisincomprehensibledue
objectives are proposed to leverage monolingual
totheentitytranslationerrors.
datafortranslation. Thesemethodsadoptadenois-
ingauto-encodingframework,whichencompasses
6 RelatedWork
several different works in data augmentation on
Named Entity Translation has been extensively monolingual data for MT (Lambert et al., 2011;
studiedfordecades(Arbabietal.,1994;Knightand Curreyetal.,2017;Sennrichetal.,2016;Huetal.,
Graehl,1998). Earlierstudiesfocusonrule-based 2019). However,namedentitytranslationsduring
methods using phoneme or grapheme (Wan and pre-trainingisunder-explored. Wefillthisgapby
Verspoor, 1998; Al-Onaizan and Knight, 2002b), integratingnamedentityrecognitionandlinkingto
statistical methods that align entities in parallel the pre-training of NMT. Moreover, while recent
1760
workshowsthatcontinuefinetuningapre-trained Mikel L. Forcada, Amir Kamran, Faheem Kirefu,
encoder with the pre-training objective improves Philipp Koehn, Sergio Ortiz Rojas, Leopoldo
Pla Sempere, Gema Ramírez-Sánchez, Elsa Sar-
language understanding tasks (Gururangan et al.,
rías, Marek Strelec, Brian Thompson, William
2020), this finetuning paradigm has not been ex-
Waites, Dion Wiggins, and Jaume Zaragoza. 2020.
ploredforpre-trainingofasequence-to-sequence ParaCrawl: Web-scale acquisition of parallel cor-
model.Besides,previousworksonmulti-tasklearn- pora. In Proceedings of ACL 2020, pages 4555–
4567.
ingforMTfocusonlanguagemodeling(Gulcehre
etal.,2015;ZhangandZong,2016;Domhanand YufengChen,ChengqingZong,andKeh-YihSu.2013.
Hieber,2017;Zhouetal.,2019),whileweexamine Ajointmodeltoidentifyandalignbilingualnamed
amulti-taskfinetuningstrategywithanentity-based entities. ComputationalLinguistics,39(2):229–266.
denoising task in this work and demonstrate sub-
Alexis Conneau and Guillaume Lample. 2019. Cross-
stantialimprovementsfornamedentitytranslations. linguallanguagemodelpretraining. InAdvancesin
NeurIPS,volume32.
7 Conclusion
AnnaCurrey,AntonioValerioMiceliBarone,andKen-
In this paper, we propose an entity-based pre- neth Heafield. 2017. Copied monolingual data im-
proves low-resource neural machine translation. In
trainingmethodforneuralmachinetranslation.Our
ProceedingsofWMT2017,pages148–156.
method improves named entity translation accu-
racy as well as BLEU score over strong denois- Tobias Domhan and Felix Hieber. 2017. Using target-
ingauto-encodingbaselinesinbothsingle-taskand side monolingual data for neural machine transla-
multi-task setting. Despite the effectiveness, sev-
tionthroughmulti-tasklearning. InProceedingsof
EMNLP2017,pages1500–1505.
eralchallengingquestionsremainopen. First, re-
centworksonintegratingknowledgegraphs(Zhao AndrewFinch,LemaoLiu,XiaolinWang,andEiichiro
Sumita.2016. Target-bidirectionalneuralmodelsfor
etal.,2020a,b)inNMThaveshownpromisingre-
machinetransliteration. InProceedingsoftheSixth
sultsfortranslation. Ourmethodlinksentitiestoa
NamedEntityWorkshop,pages78–82.
multilingualknowledgebasewhichcontainsrich
information of the entities such as entity descrip- Roman Grundkiewicz and Kenneth Heafield. 2018.
Neuralmachinetranslationtechniquesfornameden-
tion,relationlinks,andalias. Howtoleveragethese
tity transliteration. In Proceedings of the Seventh
richerdatasourcestoresolveentityambiguityde- NamedEntitiesWorkshop,pages89–94.
servesfurtherinvestigation. Second,finetuningpre-
Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
trainedmodelsonin-domaintextdataisapotential
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
waytoimproveentitytranslationsacrossdomains.
HolgerSchwenk,andYoshuaBengio.2015. Onus-
ing monolingual corpora in neural machine transla-
Acknowledgement tion. arXiv:1503.03535.
This work was supported in part by a grant from Suchin Gururangan, Ana Marasović, Swabha
the Singapore Defence Science and Technology Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Don’t stop pretraining:
Agency.
Adapt language models to domains and tasks. In
ProceedingsofACL2020,pages8342–8360.
References Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan
Pino, Guillaume Lample, Philipp Koehn, Vishrav
YaserAl-OnaizanandKevinKnight.2002a. Nameden-
Chaudhary, and Marc’Aurelio Ranzato. 2019. The
titytranslation. InProceedingsofHLT2002,pages
FLORES evaluation datasets for low-resource ma-
122–124.
chine translation: Nepali–English and Sinhala–
YaserAl-OnaizanandKevinKnight.2002b. Translat- English. In Proceedings of EMNLP-IJCNLP 2019,
ingnamedentitiesusingmonolingualandbilingual pages6098–6111.
resources. InProceedingsofACL2002,pages400–
Mohamed Seghir Hadj Ameur, Farid Meziane, and
408.
Ahmed Guessoum. 2017. Arabic machine translit-
M. Arbabi, S. M. Fischthal, V. C. Cheng, and E. Bart. eration using an attention-based encoder-decoder
1994. Algorithms for arabic name translitera- model. ProcediaComputerScience,117:287–297.
tion. IBM Journal of Research and Development,
38(2):183–194. JunjieHu,MengzhouXia,GrahamNeubig,andJaime
Carbonell. 2019. Domain adaptation of neural ma-
Marta Bañón, Pinzhen Chen, Barry Haddow, Ken- chine translation by lexicon induction. In Proceed-
neth Heafield, Hieu Hoang, Miquel Esplà-Gomis, ingsofACL2019,pages2989–3001.
1761
FeiHuang,StephanVogel,andAlexWaibel.2003. Au- Minh-ThangLuongandChristopherD.Manning.2015.
tomaticextractionofnamedentitytranslingualequiv- Stanfordneuralmachinetranslationsystemsforspo-
alencebasedonmulti-featurecostminimization. In ken language domain. In Proceedings of IWSLT
ProceedingsoftheACL2003WorkshoponMultilin- 2015.
gualandMixed-languageNamedEntityRecognition,
pages9–16. MaciejModrzejewski,MiriamExel,BiankaBuschbeck,
Thanh-LeHa,andAlexanderWaibel.2020. Incorpo-
FeiHuang,StephanVogel,andAlexWaibel.2004. Im- rating external annotation to improve named entity
provingnamedentitytranslationcombiningphonetic translationinNMT. InProceedingsofEAMT,pages
and semantic similarities. In Proceedings of HLT- 45–51,Lisboa,Portugal.
NAACL2004,pages281–288.
DiegoMoussallem,Axel-CyrilleNgongaNgomo,Paul
FeiHuang,YingZhang,andStephanVogel.2005. Min- Buitelaar,andMihaelArcan.2019. Utilizingknowl-
ing key phrase translations from web corpora. In edgegraphsforneuralmachinetranslationaugmen-
ProceedingsofHLT-EMNLP2005,pages483–490. tation. In Proceedings of K-CAP 2019, pages 139–
146.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics, GrahamNeubigandJunjieHu.2018. Rapidadaptation
24(4):599–612. of neural machine translation to new languages. In
ProceedingsofEMNLP2018,pages875–880.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,
EMNLP2004,pages388–395. SamGross,NathanNg,DavidGrangier,andMichael
Auli.2019. fairseq: Afast,extensibletoolkitforse-
Patrik Lambert, Holger Schwenk, Christophe Servan, quence modeling. In Proceedings of NAACL 2019
andSadafAbdul-Rauf.2011. Investigationsontrans- (Demo),pages48–53.
lationmodeladaptationusingmonolingualdata. In
ProceedingsofWMT2011,pages284–293. KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu:amethodforautomaticevalu-
Samuel Laubli, Sheila Castilho, Graham Neubig, ationofmachinetranslation. InProceedingsofACL,
Rico Sennrich, Qinlan Shen, and Antonio Toral. pages311–318.
2020. A set of recommendations for assessing hu-
man–machine parity in language translation. JAIR, Maja Popović. 2015. chrF: character n-gram F-score
67. for automatic MT evaluation. In Proceedings of
WMT2015,pages392–395.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer MattPost.2018. AcallforclarityinreportingBLEU
Levy, Veselin Stoyanov, and Luke Zettlemoyer. scores. In Proceedings of WMT 2018, pages 186–
2020. BART: Denoising sequence-to-sequence pre- 191.
trainingfornaturallanguagegeneration,translation,
and comprehension. In Proceedings of ACL 2020, Michael Ringgaard, Rahul Gupta, and Fernando CN
pages7871–7880. Pereira.2017. Sling:Aframeworkforframeseman-
ticparsing. arXiv:1710.07032.
Zhongwei Li, Xuancong Wang, Ai Ti Aw, Eng Siong
Chng,andHaizhouLi.2018. Named-entitytagging Rico Sennrich, Barry Haddow, and Alexandra Birch.
anddomainadaptationforbettercustomizedtransla- 2016. Improvingneuralmachinetranslationmodels
tion. InProceedingsoftheSeventhNamedEntities withmonolingualdata. InProceedingsofACL2016,
Workshop,pages41–46. pages86–96.
Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Jiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre- Yan Liu. 2019. MASS: Masked sequence to se-
training multilingual neural machine translation by quencepre-trainingforlanguagegeneration. InPro-
leveraging alignment information. In Proceedings ceedings of ICML 2019, volume 97, pages 5926–
ofEMNLP2020,pages2649–2663. 5936.
Ying Liu. 2015. The technical analyses of named en- Lucia Specia, Frédéric Blain, Varvara Logacheva,
titytranslation. InProceedingsofISCI2015,pages RamónF.Astudillo,andAndréF.T.Martins.2018.
2028–2037. FindingsoftheWMT2018sharedtaskonqualityes-
timation. InProceedingsofWMT2018,pages689–
YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey 709.
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising Jörg Tiedemann. 2012. Parallel data, tools and inter-
pre-training for neural machine translation. TACL, facesinOPUS. InProceedingsofLREC2012,pages
8:726–742. 2214–2218.
1762
Daniel Torregrosa, Nivranshu Pasricha, Maraim Ma- multi-task learning. In Proceedings of WMT 2019,
soud, Bharathi Raja Chakravarthi, Juan Alonso, pages565–571.
Noe Casas, and Mihael Arcan. 2020. Aspects
of terminological and named entity knowledge
within rule-based machine translation models for
under-resourced neural machine translation scenar-
ios. arXiv:2009.13398.
ArataUgawa, AkihiroTamura, TakashiNinomiya, Hi-
royaTakamura, andManabuOkumura.2018. Neu-
ral machine translation incorporating named entity.
InProceedingsofCOLING2018,pages3240–3250.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
youneed. InAdvancesinNeurIPS,volume30.
Denny Vrandečić and Markus Krötzsch. 2014. Wiki-
data:afreecollaborativeknowledgebase. Communi-
cationsoftheACM,57(10):78–85.
StephenWanandCorneliaMariaVerspoor.1998. Au-
tomatic English-Chinese name transliteration for
development of multilingual resources. In ACL-
COLING1998,pages1352–1356.
Jian-ChengWuandJasonS.Chang.2007. Learningto
find English to Chinese transliterations on the web.
InProceedingsofEMNLP-CoNLL2007,pages996–
1004.
FanYang,JunZhao,andKangLiu.2009. AChinese-
Englishorganizationnametranslationsystemusing
heuristicwebminingandasymmetricalignment. In
ProceedingsofACL-IJCNLP2009,pages387–395.
Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ingsource-sidemonolingualdatainneuralmachine
translation. InProceedingsofEMNLP2016, pages
1535–1545.
MinZhang,HaizhouLi,JianSu,andHendraSetiawan.
2005. Aphrase-basedcontext-dependentjointprob-
ability model for named entity translation. In IJC-
NLP2005.
Yang Zhao, Lu Xiang, Junnan Zhu, Jiajun Zhang,
YuZhou, andChengqingZong.2020a. Knowledge
graphenhancedneuralmachinetranslationviamulti-
tasklearningonsub-entitygranularity. InProceed-
ingsofCOLING2020,pages4495–4505.
Yang Zhao, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2020b. Knowledge graphs enhanced neural
machinetranslation. InProceedingsofIJCAI2020,
pages4039–4045.
Leiying Zhou, Wenjie Lu, Jie Zhou, Kui Meng, and
GongshenLiu.2020. Incorporatingnamedentityin-
formation into neural machine translation. In Pro-
ceedingsofNLPCC2020,pages391–402.
ShuyanZhou, XiangkaiZeng, YingqiZhou, Antonios
Anastasopoulos,andGrahamNeubig.2019. Improv-
ing robustness of neural machine translation with
1763
Appendix
A FinetuningBLEUCurves
WereportBLEUscoreforthreelanguagepairscalculatedfromcheckpointsatdifferentfinetuningstepsin
Figure5. Foralllanguagepairs,allpre-trainingmethodsresultinasignificantincreaseintermsofBLEU
throughoutthefinetuninginbothsingle-taskandmulti-tasksetting. Inparticular,thedifferencesinBLEU
betweenDEEPandtheotherbaselinesaremostsignificantatthebeginningofthefinetuningstage.
Random MT DAE MT DEEP MT DAE DAE + MT DEEP DEEP + MT
20.0 20.0 12
17.5 17.5
10
15.0 15.0
12.5 12.5 8
10.0 10.0
6
7.5 7.5
5.0 5.0 4
5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40
Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000)
Russian Ukrainian Nepali
Figure5:BLEUscoresfor3languagepairsovervariousfinetuningsteps.
B EntityTranslationAccuracyforotherlanguages
We show the entity translation accuracy performance over various finetuning steps for Ukrainian and
NepaliinFigure6,7,andshowthegainsofthreepre-trainingmethodsovertherandombaselinewith
respecttotheentityfrequenciesinFigure8,9. Emptycellsintheheatmapsareduetonoentitiesthat
meettheconditionsinthosecells.
Ukrainian: AsseeninFigure6,thegeneraltrendfortheentitytranslationaccuracyaccordingtoentity
groupsaresimilartothatofRussian. WhileDEEPachievesthehighestaccuracyinFT,theresultsfor
FT is less reliable due to a small sample size of entities in FT. In terms of the gain from Random →
MTaccordingtotheentityfrequency,weobserveaconsistentimprovementofourmulti-taskDEEPon
translatinglow-frequententitiesinthefinetuningdata(SeetheleftbottomofFigure8).
Nepali: Whileoutperformingatthebeginningoffinetuning,Figure7showsthatDEEP→DEEP+MT
eventuallyunder-performedfortranslationsofentitiesinPFTdata. Moreover,theaccuracyisconsiderably
loweronentitiesinPT,whichsuggeststhatthedegreeofforgettingismuchmoreconspicuousinNepali.
ThegainfromRandom→MTwithrespecttotheentityfrequencyexhibitedadifferenttrendfromRussian
andUkrainian. Figure9showstheresults. Inthesingle-tasksetting,DEEPimprovethetranslationsof
frequententitiesappearinginboththepre-trainingandfinetuningdata. Despitethemulti-tasklearning
thatintroducesadditionalexposuretoentitiesthataremorefrequentinthepre-trainingdata,thelargest
gaincomesfromentitiesthatarelessfrequentinthepre-trainingdatabutfrequentinthefinetuningdata.
C ScientificArtifacts
InTable6,weprovidethedetailedinformationaboutthescientificartifacts(e.g.,data,code,tools)used
inourpaper. Wehavecheckedthedatausedinthisworktomakesurethatwedonotintentionallyuse
privateorsensitiveinformationoroffensivecontentforderivingtheobservationsandconclusionsfrom
ourwork. AlthoughWikiDatamaycontainthenameofsomeindividualpeople(e.g.,famouspeoplethat
haveWikipediawebpages),wedonotusetheirsensitiveinformationinouranalysis.
1764
UELB
Random MT DAE MT DEEP MT DAE DAE + MT DEEP DEEP + MT
70
70 30
60
60
25
50
50
20
40
40
15
30
30
10 20
20
5 10
10
0 0
0
5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40
Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000)
Entities in PFT data. Entities in PT data. Entities in FT data.
Figure6:EntitytranslationaccuracyaggregatedoverdifferententitysetsforUkrainian.
Random MT DAE MT DEEP MT DAE DAE + MT DEEP DEEP + MT
2.5 30
40
2.0 25
30 20
1.5
15
20 1.0
10
10 0.5
5
0 0.0 0
5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40 5 10 15 20 25 30 35 40
Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000) Fine-tuning steps. (x 1000)
Entities in PFT data. Entities in PT data. Entities in FT data.
Figure7:EntitytranslationaccuracyaggregatedoverdifferententitysetsforNepali.
(34995, 460576] 0.0 11.8 3.6 6.8 0.0 8.3 8.6 7.5 25.0 13.9 10.0 7.7 40
(11626, 34995] 19.6 6.5 9.2 2.0 26.8 7.6 5.6 0.7 35.7 13.4 8.4 2.6 20
(2840, 11626] 0.0 10.1 5.9 2.8 5.8 0.0 11.9 8.1 4.4 0.6 0.0 27.4 12.2 2.6 9.6 0
(2, 2840] 8.9 11.5 -0.4 8.3 -16.7 11.6 13.8 4.2 0.0 -16.7 21.4 20.1 3.7 8.3 0.0 20
0 0.0 50.0 0.0 0.0 0.0 50.0 0.0 0.0 50.0 25.0 0.0 0.0 40
0 (1, 23] (23, 83] (83, 253 (] 253, 6178] 0 (1, 23] (23, 83] (83, 253 (] 253, 6178] 0 (1, 23] (23, 83] (83, 253 (] 253, 6178]
Freq. bins from Finetuning data. Freq. bins from Finetuning data. Freq. bins from Finetuning data.
Gain with DAE MT Gain with DEEP MT Gain with DEEP DEEP+MT
Figure8:GainfromRandom→MTinentitytranslationaccuracyforUkrainianforeachmodel.
1765
ycaruccA
noitalsnarT
ytitnE
egarevA
ycaruccA
noitalsnarT
ytitnE
egarevA
.atad
gniniarterP
morf
snib
.qerF
(1694, 65084] 0.0 0.0 9.2 3.7 12.3 0.0 0.0 11.6 8.8 15.3 0.0 5.9 17.5 -2.3 15.5 40
(664, 1694] 0.0 0.0 6.8 10.9 11.1 0.0 0.0 9.9 13.4 12.9 0.0 10.0 9.1 6.1 -0.6 20
(166, 664] 0.0 5.9 2.1 -2.7 6.2 0.0 5.9 2.1 9.2 10.4 1.8 12.5 11.2 2.4 31.2 0
(1, 166] 1.1 2.2 3.8 22.5 33.3 1.1 2.2 15.5 37.5 0.0 1.8 6.0 8.6 21.7 33.3 20
0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 50.0 0.0 40
0 (1, 12] (12, 62] (62, 318 (3] 18, 39131] 0 (1, 12] (12, 62] (62, 318 (3] 18, 39131] 0 (1, 12] (12, 62] (62, 318 (3] 18, 39131]
Freq. bins from Finetuning data. Freq. bins from Finetuning data. Freq. bins from Finetuning data.
Gain with DAE MT Gain with DEEP MT Gain with DEEP DEEP+MT
Figure9:GainfromRandom→MTinentitytranslationaccuracyforNepaliforeachmodel.
Artifact License/Term Documentation
WikiData(VrandečićandKrötzsch,2014) CreativeCommonsCC0 Thisresourceisafreeknowledgebasethatsupportsvariousresearchandprojectw.
Sling(Ringgaardetal.,2017) Apache-2.0 ThistoolisintendedtouseforanalyzeWikiDataandWikipediaarticles.
WMT18En-RuData(Speciaetal.,2018) Open-sourced ThisdatasetisintendedtobeusedforMTonnewstexts.
OPUSData(Tiedemann,2012) Open-sourced ThisdataresourceisintendedtobeusedforMT.
FLORESData(Guzmánetal.,2019) CC-BY-SA-4.0License Thisdatasetisintendedtobeusedforlow-resourceMT.
Fairseq(Ottetal.,2019) MITLicense Thistoolisintendedtofacilitatedeeplearningresearch.
Table6:Detailinformationaboutscientificartifactsusedinthispaper.
1766
.atad
gniniarterP
morf
snib
.qerF

Target-Guided Dialogue Response Generation Using Commonsense and
Data Augmentation
PrakharGupta♣ HarshJhamtani♣ JeffreyP.Bigham♣,♥
♣LanguageTechnologiesInstitute,CarnegieMellonUniversity
♥Human-ComputerInteractionInstitute,CarnegieMellonUniversity
prakharg@cs.cmu.edu, jharsh@alumni.cmu.edu, jbigham@cs.cmu.edu
Abstract
Target-guided response generation enables di-
alogue systems to smoothly transition a con-
versation from a dialogue context toward a
target sentence. Such control is useful for
designing dialogue systems that direct a con-
versation toward specific goals, such as creat-
ing non-obtrusive recommendations or intro-
ducing new topics in the conversation. In
this paper, we introduce a new technique for
target-guidedresponsegeneration, whichfirst
finds a bridging path of commonsense knowl- Figure 1: Given a dialogue context and a target sentence,
our goal is to generate a dialogue response that smoothly
edgeconceptsbetweenthesourceandthetar-
transitionstheconversationfromcontexttowardsthetarget.
get,andthenusestheidentifiedbridgingpath
Ourproposedapproachinvolvesidentifyingabridgingpath
togeneratetransitionresponses. Additionally, ofentitiestolinkthecontextandthetarget.
we propose techniques to re-purpose existing
dialoguedatasetsfortarget-guidedgeneration.
Experiments reveal that the proposed tech-
caseaconversationgoesawryorauserbecomes
niques outperform various baselines on this
abusive towards the system, and direct the users
task. Finally, we observe that the existing au-
towards topic areas that the system knows how
tomated metrics for this task correlate poorly
withhumanjudgementratings. Weproposea to talk about. Prior work has used mechanisms
novelevaluationmetricthatwedemonstrateis such as emotion labels (Zhong et al., 2019), per-
more reliable for target-guided response eval- sona(Songetal.,2019), andpoliteness(Niuand
uation. Our work generally enables dialogue Bansal,2018)tocontrolconversations. However,
systemdesignerstoexercisemorecontrolover
such approaches require labeled training data for
theconversationsthattheirsystemsproduce.1
asetofpre-determinedlabels,makingitharderto
incorporatenewgoalsintoasystem. Inthiswork,
1 Introduction
westudytheproblemofproactiveresponsegener-
Open-domain conversational systems have made ation based on a target sentence. For example in
significantprogressingeneratinggoodqualityre- Figure1,giventhecontext‘Ienjoyswimming’,the
sponsesdrivenbystrongpre-trainedlanguagemod- systemguidestheconversationtowardsthetarget
els(Radfordetal.,2019;Devlinetal.,2019)and ‘Iliketotraveltonewplaces’bymentioning‘Ilike
large-scalecorporaavailablefortrainingsuchmod- toswimatbeacheswhenIgoonvacation’. Using
els. However,insteadofpassivelyrespondingtoa targetsentencesforproactivecontrolisanintuitive
user,dialoguesystemscantakeonamoreproactive and flexible control mechanism for dialogue de-
roletomakerecommendations,helpusersdiscover velopers,freeofdomain-specifichandcraftingand
new services, or introduce interesting new topics annotations.
touserstoimproveuserexperience. Furthermore, Existingpubliclyavailabledialoguecorporagen-
aproactiveortarget-guidedsystemcanguidethe erally consists of free-flow conversations where
conversationtowardssaferconversationaltopicsin thespeakersmovetheconversationforwardbased
on the dialogue history alone, with no particular
1Codeavailableatwww.github.com/prakhargupt
az/target-guided-dialogue-coda agenda. WebuildupontherecentlyreleasedOtters
2202
yaM
91
]LC.sc[
1v41390.5022:viXra
dataset(Sevegnanietal.,2021)withone-turntopic ment ratings of system outputs. As part of this
transitionsformixed-initiativeinopen-domaincon- work, we collect and release a dataset of human
versations. Givenasourcesentencefromaspeaker, ratingsofvarioussystemoutputsforthistask.
the task is to generate a topic transition sentence Wediscussthebroaderimpactandpotentialuses
with“bridging”strategiestoatargetsentencefrom oftheproposedsystem,itslimitationsandpotential
another speaker. The task is challenging on sev- ethicalissuesrelatedtothistaskinSection8.
eralfronts. First,thesystemneedstobalancethe
trade-offbetweencoherencewiththecontextwhile 2 RelatedWork
smoothlytransitioningtowardsthetarget. Second,
theOtterstrainingdatasetisrelativelysmall(less Target Guided Dialogue Response Generation:
than 2000 training instances), making it a low- Sevegnani et al. (2021) is perhaps the closest to
resource setting. Finally, we show that standard our work described in this paper. They work on
word-overlapmetricsareinsufficientforthistask. thetaskofgeneratinganewutterancewhichcan
achieveasmoothtransitionbetweentheprevious
In this work, we propose methods to leverage
turn’stopicandthegiventargettopic. Pastworkin
commonsenseknowledgefromConceptNet(Speer
controllabletextgenerationhasexploredsteering
et al., 2017a) to improve the quality of transition
neuraltextgenerationmodeloutputstocontaina
responses. Ourtechniquedecomposestheresponse
specific keyword (Keskar et al., 2019), a knowl-
generation process into first generating explicit
edgegraph(Wuetal.,2019),oratopic(Lingetal.,
commonsense paths between the source and tar-
2021). Steeringdialoguetowardsagivenkeyword
getconcepts,followedbyconditioningonthegen-
has also been explored in past work (Tang et al.,
erated paths for the response generation. This is
2019;Qinetal.,2020a;Zhongetal.,2021),albeit
intendedtomimichowhumansmightbridgecon-
asaretrievaltask. Incontrast,ourgoalistogener-
cepts for creating transitions in conversations us-
ateanextutteranceinadialoguesetupwhichcan
ingcommonsenseknowledge. Thistechniqueof-
steer a conversation towards target sentence in a
ferstwobenefits: 1)LeveragingexternalConcept-
smoothfashionratherthangeneratingaresponse
Netknowledgesolvesthedatascarcityissueand
for a given keyword or topic. Our work is also
improves the model’s capability to generate logi-
related to prior work on text infilling (Donahue
caltransitions;2)Sincethetransitionresponseis
etal.,2020;Qinetal.,2020b), thoughcompared
groundedoncommonsenseknowledgepaths,the
to them we work in a dialogue setup and utilize
explicit paths used by the model can provide ex-
commonsenseknowledgetoperformtheinfilling.
planationsfortheconceptsusedbythemodel,as
Commonsense for Dialogue Generation: Com-
well as provide control over the generation pro-
monsense knowledge resources (Speer et al.,
cess. Furthermore, we propose a data augmenta-
2017b;Malaviyaetal.,2020)havebeenusedindia-
tionmechanismtohelpwiththedatascarcityissue
logueresponsegenerationfortaskssuchaspersona-
by re-purposing training data from DailyDialog,
grounded dialogue (Majumder et al., 2020) and
an open-domain dialogue dataset. Both these ap-
open-domaindialoguegeneration(Ghazvininejad
proaches are complementary and outperform ex-
et al., 2018; Hedayatnia et al., 2020; Zhou et al.,
istingbaselinesinresponsequalityandtransition
2021c). Zhouetal.(2021a)createdadatasetfocus-
smoothness. We demonstrate how the proposed
ingonsocialcommonsenseinferencesindialogue
approachofusingexplicitbridgingpathsenables
and Arabshahi et al. (2020) designed a theorem
improvedqualityoftransitionsthroughqualitative
prover for if-then-because reasoning. A concur-
andhumanstudies.
rentwork(Zhouetal.,2021b)proposedtotraina
Automated evaluation is a challenging aspect model to explicitly generate implicit knowledge
ofdialogueresponsegenerationtasks(Zhaoetal., and use this knowledge to generate a response.
2017). We show that the existing word-overlap Comparedtotheirwork,wefocusontarget-guided
metricssuchasBLEUcanbeeasilyfooledtoas- responsegeneration,suggestmechanismforknowl-
sign high scores to poor responses just based on edgealignmentwiththetransitionresponseduring
highn-gramoverlapwithreferenceresponses. We training,andfocusonmulti-hopknowledgepaths.
proposeametricTARGET-COHERENCEwhichis Morebroadly,commonsenseknowledgehasbeen
trained using hard adversarial negative instances usedintextgenerationtaskssuchasstoryandessay
andachievesahighcorrelationwithhumanjudge- generation(Guanetal.,2019a;Yangetal.,2019).
Automated Metrics for Evaluating Dialogue andatargett,aconditionallanguagemodellearns
Quality: Automated metrics such as BLEU (Pa- topredictthetransitionresponses. Target-guided
pineni et al., 2002), METEOR (Banerjee and generationcanpotentiallybenefitbyincorporating
Lavie, 2005), and BertScore (Zhang et al., 2020) commonsense reasoning by identifying rich con-
are widely used to evaluate quality of machine- nectionsbetweenapairofentitieswhichenableus
generatedtext. However,suchmetricsoftencorre- togeneratelogicaltransitionresponsesconnecting
latepoorlywithhumanjudgementratingsofgen- thetwo. Pre-trainedlanguagemodelsareknownto
erated text quality (Sai et al., 2020). Past work sufferincaseswherecommonsenseknowledgeis
hasexploredtrainedmodel-basedmetricssuchas requiredduringgeneration(Zhouetal.,2018;Guan
ADEM(Loweetal.,2017)andRUBER(Taoetal., etal.,2019b),especiallyintaskswherethereisnot
2017). However, training such model-based met- enoughdataavailableforlearningcommonsense
rics often relies on tagged training data. Gupta patterns from the text, which is true for our case.
etal.(2021a)proposewaystomitigatetheneedfor Incontrast,CommonsenseKnowledgeGraphslike
such labelled data by automatically synthesizing ConceptNet(Speeretal.,2017a)providestructured
negativeexamples. Ourproposedmetricisalong knowledge about entities, which enables higher-
similarlines,thoughweutilizedifferenttechniques levelreasoningaboutconcepts.
forsyntheticnegativeexamplegeneration. In this work we use commonsense knowledge
fromConceptNetforplanningatransitionresponse.
3 TaskOverview
ConceptNet is a large-scale semantic graph that
has concepts as nodes and has commonsense re-
We first formalize the task of target-guided re-
lationships between them, such as ‘IsA’ and ‘At-
sponse generation. Given a conversation context
Location’. However,ConceptNetsuffersfromse-
c between two speakers A and B, and a target ut-
veresparsityissues(Malaviyaetal.,2020;Bosselut
terance t for speaker B, the task is to generate a
etal.,2019). Therefore,itisnotalwayspossibleto
transitionsentenceswhichservesasasmoothlink
findtheconceptsandrelationshipsbetweencontext
between the context and the target. The target is
andtargetconcepts. Toaddressthesparsityissue,
aphraseorasentence. Ottersdataset(Sevegnani
wedevelopKnowledgePathGenerator(KPG),a
etal.,2021)consistsofasimplifiedsettingofone-
language model trained on paths sampled from
turntopictransitions,wheretheconversationhis-
ConceptNet. Themodeltakesapairofentitiesor
toryconsistsofasingleutteranceu fromspeaker
a
concepts as input and generates a multi-hop path
A, and a target utterance u for speaker B, and
b
connecting the two. Since the knowledge paths
the task is to generate a transition utterance s for
are sampled from a generative model rather than
speakerBtoserveasasmoothlinkbetweenu and
a
retrievedfromafixedknowledgebase,weareno
u . The task is challenging since a system needs
b
longerlimitedbytheentitiesandpathspresentin
todeviseastrategythatbalancesthecompetitive
theConceptNetknowledgebase.
objectives of generating a response which is co-
herent to the context, while smoothly driving the Togeneratecommonsensebasedresponses,we
conversationtowardsthetarget. trainaCommonsenseResponseGenerator(CRG)
Inthiswork,weproposetwoapproachesfortran- model to generate the transition response condi-
sitionresponsegenerationtask: 1)Commonsense- tioned on the paths generated by the KPG model
guided response generation (section 4), and 2) (Figure2). Conditioningtheresponsegeneration
Data augmentation to tackle data sparsity (sec- oncommonsensepathsimprovesthereasoningca-
tion 5). We refer to the proposed method as pabilitiesoftheCRGmodelandprovidestheadded
CODA(CommonsensePathandDataAugmen- benefitsofinterpretabilityandcontroloverthegen-
tation). WealsoproposeanovelmetricTARGET- erationprocess.
COHERENCE to automatically evaluate smooth-
nessofresponsetransitions(section6). 4.1 Commonsensepathgenerator
TheKPGmodelsattemptstoconnectaconceptor
4 Commonsense-GuidedResponse
entityphrasefromthecontexttoaconceptfromthe
Generation
targetbycreatingknowledgepathsbetweenthem.
We frame the target-guided response generation Path Sampling: To create training data for the
task as follows. Given a conversation context c KPG models, we sample paths between entity
Figure2: ModelillustrationsforKPGs-KnowledgePathGenerators(top)andCRG-CommonsenseResponseGenerator
(bottom).BasearchitectureforallmodelsisGPT-2.GivenapathsampledfromConceptNet,KPG-wclearnstopredictthepath
giventhehead,tailandintermediateentitiesofthepathwhileKPG-htlearnstopredictthepathgivenonlytheheadandtail
entities.FortheCRGmodel,duringtraining,aheadentityfromthecontext,atailentityfromthetargetandintermediateentities
fromthegoldtransitionresponsearefedintoKPG-wcanditsoutputpathisusedasinputtotheCRGmodel.Duringinference,a
headentityfromthecontextandatailentityfromthetargetarefedintotheKPG-htmodel.KPG-htthengeneratesapathwith
newconceptssuchas“goonvacation”.CRGmodelconditionsonthispathfortransitionresponsegeneration.
phrases from ConceptNet using random walks. possiblepathscanexistforagivenhead-tailentity
This step builds upon past work of Wang et al. pair. TrainingtheCRGmodelbyconditioningon
(2020). Given nodes N and edges E from Con- paths which are irrelevant to the gold transition
ceptNet, we perform random walks on the graph response might discourage the CRG model from
to sample a set of paths P of the form p = conditioningontheprovidedcommonsensepath.
{n ,e ,n ,e ,...,e ,n } ∈ P. Here, a path p Sincewedonothavegoldpathsforaresponse,we
0 0 1 1 k−1 k
connectsaheadentityphrasen withthetailentity instead train a model KPG-wc to generate paths
0
phrase n via intermediate entities and edges (or whicharemorealignedtothegoldresponsebyen-
k
relations)n ,e . Tosamplepaths,therandomwalk forcingthegeneratedpathtocontainentitiesfrom
i i
beginswitharandomentitynoden andsamplesa the gold response. KPG-wc is trained to predict
0
pathofrandomlengthk ∈ {1,2,...,K},wherewe a path which contains a pre-specified entity set
havesetK = 6inthiswork. Tosamplepathsthat E = {k ,...,k }inthegeneratedpathbyformat-
p 1 n
areusefulforourtask,wepreventsamplingcertain tingpathssampledfromConceptNetasthefollow-
edgestypessuchasSynonym(AppendixA.1). ingsequence: “[wc]k1[wc]k2...[target]n [sep]
t
KPG-head-tails (KPG-ht): KPG-ht is a GPT- n h e 0 n 1 e 1,...,e k−1 n t” (Figure 2). The entity
2 (Radford et al., 2019) based model which is set E p is a randomly permuted sequence of enti-
trainedtopredictaknowledgepathpwhichlinks tiesn 1,n 2,...,n k−1 fromthesampledpath. Here
a head entity n to a tail entity n . For a sample “wc”symbolizes“willcontain”. Trainingwiththis
h t
path p = {n ,e ,n ,e ,...,e ,n } from Con- sequenceindicatestothemodelthatthepathgener-
h 0 1 1 k−1 t
ceptNet, the path is formatted into the following atedbetweenn h andn t shouldcontaintheentities
sequence“[target]n t [sep]n h e 0 n 1 e 1,...,e k−1 fromthesetE p inasensibleorder. Specifyingthe
n ”. KPG-ht is only used during CRG inference specialtoken“[target]”followedbythetailentity
t
wheretheheadentityisextractedfromthecontext n t informsthemodelaboutthelastentityitshould
andtailentityfromthetarget(Figure 2). output when generating a path. We discuss how
KPG-will-contain(KPG-wc): Alargenumberof thesetE p isconstructedforCRGmodeltraining
inthenextsection. GutenbergEnglishcorpus)oftheentitytokensand
In practice, we train a single common GPT-2 summing up the maximum value found for a to-
basedmodelforKPG-wcandKPT-ht. Themodel ken in each entity in the pair. For training phase,
attesttimeisabletogenerateknowledgepathsfor wekeepthetopDpairsofentities,andfortesting
either case, whether in-path entities from E are phase we keep only the highest-scoring pair. (2)
p
present(KPG-wc)intheinputornot(KPG-ht). Sub-selectingpaths: Weapplythefollowingstrate-
gies to prune the set of paths for each entity pair:
4.2 Responsegenerator 1) Perplexity - We filter out all the paths whose
perplexityvalues(fromtheKGPmodels)aremore
TheCommonsenseresponsegeneratorconditions
than double the average perplexity values of all
onthecommonsensepathsgeneratedfromtheKPG
paths between an entity pair. 2) We remove all
modelstogeneratethetransitionresponses.
the paths which have repetition of entities since
Entity extraction. We extract a set of entities
repetition often leads to degeneration during de-
E ,E and E from the context, target and gold
h t r coding. 3)Forpathsintrainingdata,wefilterout
transitionresponserespectivelyusingNLTK.We
pathswhichcontainentitiesnotpresentinthegold
designed simple grammar rules (details in Ap-
response. The final set of paths P are converted
pendix A.1) to convert phrases to concise forms
intonaturallanguagebyconvertingtherelationand
thatmatchthenodespresentinConceptNet, e.g.,
inverserelationsintotextualformat. Forexample,
“watchingthestar”isconvertedto“watchstars”.
“art gallery UsedFor for art” is converted to “art
Sampling and filtering paths: In this step, for galleryisusedforart”.
everypairofheadandtailentityfromE andE ,
h t Training and inference in CRG model. The
we sample multiple paths from the KGP models
CRG model (GPT-2 based) is trained as a con-
usingtopksamplingandchoseoneormoreofthese
ditional model with following input sequence:
pathsfortrainingandinference. Fortrainingthe
“knowledgepath[target]targetsentence[context]
CRGmodelswiththecommonsensepaths,weneed
context sentence [response] transition response”
tocuratepathsthatarerelevanttoandalignedwith
for each knowledge path from set P. We train
thegoldresponsesothattheyarenotignoredbythe
CRGmodelbyminimizingthelog-likelihoodloss
CRGmodelduringinference. Weachievethisby
of the transition response. For inference, we cre-
firstsamplingpathswhicharerelevanttothegold
ate the set of paths P by entity extraction, path
response,andthenapplyfilteringmechanismsto
sampling and filtering and choose a random path
curatethefinalsetofpaths. Fortrainingdatapath
pfromfinalsetP. Themodelgeneratestransition
sampling, we use the KPG-wc model (Figure 2).
responseconditionedonthesequencec,t,andp.
The input to the model is a head and tail entity
pairn andn ,andtheentitysetE thatconsists
h t p 5 DataAugmentation
of the set of entities E from the gold transition
r
response. Themodelthengeneratesasetofpaths The task of target-guided response generation is
that contain the head and tail entities as well as stillarelativelyunexploredtask,andOtters(Seveg-
the gold response keywords. Thus, the sampled nani et al., 2021) is the only suitable dataset for
path is inherently relevant to the gold response this task to the best of our knowledge. However,
due to the conditioning on gold keyword entities. Ottersissmallandconsistsofonlyafewhundred
Duringinference,thesetE isnotavailable,sowe context-target pairs. This makes learning transi-
r
leveragetheKPG-htmodelthattakesjustthehead tionconceptsandstrategieschallenginginthislow-
andtailentitypairn andn asinputtogeneratea resourcesetup. Ontheotherhand,therearemany
h t
commonsensepath. publiclyavailabledialoguedatasetsfortrainingre-
Assumingthecontextandtargetconsistsofm sponsegenerationmodels. Suchdatasetscontain
andnentitieseach,andwegenerateq numberof free-flowconversations,wherealthoughthespeak-
pathsperpair,wegetatotalofm×n×q number ers generate context coherent responses, they do
ofpathsforeachdatainstance. Sincem×n×q notconditiontheirresponsesonanytarget. Wepro-
canbealargenumber,weusesimplemethodsto poseatechniquetoleverageandre-purposesuch
sub-selectentitypairsandpaths. (1)Sub-selecting datasetsforthetaskoftarget-guidedresponsegen-
EntityPairs: Wescoreanentitypairbycalculating eration. WepicktheDailyDialog(Lietal.,2017)
theinversedocumentfrequencies(computedusing datasetforexperimentationandconvertitsconver-
therestaurantlooksauthentic
Context therestaurantlooksauthenticeuropean. POSITIVE CONTEXTc european.
Response the chef trained in florence. the pasta Goldc,r,t RESPONSEr thecheftrainedinflorence.
tastesnicehere.
TARGETt thepastatastesnicehere.
SRLOutput predicate=tastes,arguments=thepasta;
NEGATIVE
nicehere Randomt’ TARGETt’ ilovetodrivemycar.
Targetclause thepastatastesnicehere. withgoldr,c
NEGATIVE
Figure3: Anexampletodemonstratehowaconversationin Randomc’ CONTEXTc’ ienjoycomputersandphones.
withgoldr,t
DailyDialogcanbere-purposedforthetaskoftarget-guided
NEGATIVE
responsegeneration.
Randomr’ RESPONSEr’ thereisnoparkinghere.
withgoldc,t
sationstotarget-guidedconversationsintwosteps: Figure 4: We train a reference-less model-based metric
1)Targetcreation,and2)Datafiltering. TARGET-COHERENCE to score thesmoothness ofa gener-
ateresponsewrttodialoguecontextandtargetsentence.To
For target creation, we run Semantic Role La-
trainthemetric,wesynthesizehardnegativeexamplesusing
belling(SRL)topredictpredicateandarguments anensembleoftechniquesasshowninthisfigure.
in a response. For each predicate identified, we
Dataset Train Dev Test
create a clause by putting together the predicate
Otters-id 1,929(693) 1,160(404) 1,158(303)
andargumentsinatextualsequence. Finally, we
Otters-ood 2,034(677) 1,152(372) 1,130(372)
only use the clause occurring towards the end of DailyDialog 11,118 1,000 1,000
the response as a target. An example for target
creationisshowninFigure3(Moredetailsabout Table1: Overviewofthedatasets.
clauseidentificationareinAppendixA.2).
We use the gold transition response from the
Thetargetcreationstepdoesnotguaranteethat
trainingdatasettocreatepositiveinstancesfortrain-
acandidateresponsetransitionssmoothlytowards
ing. Forapositiveinstancewithcontextc, target
the target clause. In the data filtering step, we
tandresponser,wecreatenegativeinstancesus-
introducea TARGET-COHERENCEmetrictoscore
ing the following mechanisms: 1) We hold two
a transition response in terms of its coherence to
outof(c,t,r)constantwhilerandomlysamplethe
thecontextandsmoothnesstowardsthetarget. The
thirdone. Forexample,samplearandomcontext
metricisdescribedinmoredetailinsection6. The
c(cid:48),whichmakesrincoherenttothec(cid:48). Anexample
metricassignsascorebetween0-1foratransition
is shown in Figure 4. 2) We use a GPT-2 model
responseandweremoveinstanceswithascoreless
trainedonOttersdatasettogeneratearesponser(cid:48)
thanathresholdk (setto0.7)fromconsideration.
coherent to c but conditioned on a random target
The remaining instances are used for pretraining
t(cid:48). 3) For a target t, we chose a response r(cid:48) from
responsegenerationmodelswhicharefinallyfine-
the Otters training set which has t as the target
tunedontheOttersdataset.
but context c(cid:48) (cid:54)= c. We sample a maximum of 2
6 Target-CoherenceMetric negativeinstancepermechanismandbalancethe
countofpositiveandnegativeinstancesbyrepeat-
Evaluating target-guided responses is a challeng-
ingpositiveinstances. Wefine-tuneapre-trained
ing task as a good transition response needs to
BERT-base (Devlin et al., 2019) model on these
be both - coherent to the context and smoothly
instanceswithbinarycrossentropyloss.
transition towards the target. Furthermore, since
thetaskisopen-domainandopen-ended,thereare 7 Experiments
many possible correct responses which may not
7.1 Datasets
matchwithareferenceresponse(Çelikyilmazetal.,
2020). Totacklethesechallenges,weproposean We use two datasets in our experiments. 1) Ot-
automatic metric for this task that does not use ters(Sevegnanietal.,2021)containsinstanceswith
humanreferences. Theproposedmetric TARGET- context-target-transition response triplets. It con-
COHERENCE is based on a classification model sists of two sets of splits. The Out-Of-Domain
trained to classify a transition response as either (OOD)splitensuresthatnoneofthecontext-target
positive, that is, it is coherent to the context and pairsinthetestsetarepresentinthetrainset. In
smoothlytransitionstowardsthetarget,ornegative, theIn-Domain(ID)split,oneofeitherthecontext
that is, the response is either not coherent to the orthetargetineachpairinthetest-setisallowed
contextordoesnottransitiontowardsthetarget. toappearinthetrain-set. DailyDialogdatasetcon-
sistsofcasualconversationsbetweentwospeakers. TargetasContextasReferenceCorrelation
Metric
InTable1wepresentthenumberofdialoguesin response response response wratings
BLEU 15.0 9.9 6.5 -0.11
DailyDialog dataset and number of responses in
METEOR 14.0 12.6 13.2 0.01
otters,alongwithnumberofuniquecontext-target ROUGE-L 32.3 29.8 26.5 -0.04
pairsinbrackets. Ottersdatasetconsistsofmultiple BS-rec 38.1 38.9 41.3 0.05
BS-F1 42.8 42.6 38.9 -0.06
responsespercontext-targetpair. Sometransition
TARGET-
10.7 4.0 77.4 0.47
responsesinOttersdatasetarenoisy-theycontain COHERENCE
sentences and phrases from the target sentences.
Table2: Wepresentthemetricscoreswhenusingthetarget,
Weremovesuchdatafromthetestsets(withword
contextandoneofthereferencesastheresponse.Allmetrics
overlap > 0.75), leaving 1019 data points in the exceptforTARGET-COHERENCEscorethetargetandcontext
Otters-idtestsetand988datapointsintheOtters- higherthanthereference. TARGET-COHERENCE achieves
highcorrelationwithhumanratings.Underlinedvaluesrepre-
oodtestset.
sentstatisticallysignificantresultwithp-value<0.05.
7.2 Baselinesforgeneration directly from ConceptNet using the algorithm
proposedinLinetal.(2019).
We report results for a number of baselines. We
• CODA-Upper Upper bound for CODA which
providecompleteimplementationdetailsofCODA
usespathsinferredfromthegoldresponsesusing
andallbaselinesinAppendixAandB.
theKPG-wckeywordsmodelduringinference.
• GPT-2: (Radfordetal.,2019)ApretrainedGPT–
smalllanguagemodelfine-tunedonOttersdata. 7.3 EvaluationMetrics
Conditionsonthecontextandtargetsentencesto
We report standard automated metrics such as
generatethetransitionresponse.
BLEU (Papineni et al., 2002), ROUGE-L (Lin,
• GPT2-FudgeYangandKlein(2021)usesadis-
2004),METEOR(BanerjeeandLavie,2005),and
criminatortrainedtodistinguishgoodresponse
BertScore(BS-recandBS-F1)(Zhangetal.,2020).
continuationsfromthepooronesandguidesthe
Evaluationiscarriedoutusingmultiplereferences
GPT-2baseddecodertowardsresponsesthatare
from the test set. Word-overlap metrics do not
coherenttoboththesourceandtargetsentences.
correlatewellwithhumanjudgements(Liuetal.,
• Multigen(Jietal.,2020)combinesthevocabu-
2016). Additionally,weobservethatonthistask,
larydistributiongeneratedbyunderlyingGPT-2
evenapoortransitionresponsecangetahighscore
model with a concept distribution from a com-
on reference-based metrics if it has high overlap
monsenseknowledgebase(ConceptNet).
withthecontextorthetarget. Wecarryoutanex-
• Concept-Predictleveragesaconceptprediction
perimentwhereweusethetarget,contextandone
strategyfromQinetal.(2020a). Theconceptis
of the references as the transition response. An
predictedbasedonclosenesstothetarget.
ideal metric would score the reference response
• CS-Pretrainmodelispretrainedwithcommon-
high,andgivelowscorestotargetandcontextused
sensepathsusedfortrainingtheKPGmodelsand
asaresponse. InTable2,reference-basedmetrics
is based on the commonsense story generation
assignhigherscorestotargetandcontextsentences
modelfromGuanetal.(2020).
usedasresponsescomparedtohuman-writtenre-
Ablation experiments: We report results for fol- sponses. In contrast, TARGET-COHERENCE as-
lowing CODA variants: signs high scores to reference responses and low
• CODA-ONLYDA:CODAvariantthatusesDai- scorestotargetandcontextsentences.
lyDialog augmentation and does not use com- Correlationofmetricswithhumanjudgements:
monsensepathsfromKPGmodelsintheCRG We investigate how well do the metrics correlate
model. withhumanratingsofsystemoutputs. Toperform
• CODA-NODA: CODA trained without addi- thisanalysis,responsesfromCODA,baselines,as
tionaldatafromDailyDialog. well as reference responses are judged by crowd-
• CODA-NOEDGE CODAvariantthatusesonly source annotators who rate the smoothness of a
entitiesandnoedgesfromthepath. responsegiventhedialoguecontextandthetarget
• CODA-NOALIGN: variant that relies on only on a scale of 0 to 1. We collect a total of 440
KPG-ht for training and inference. Does not ratingsacrossOttersIDandOODsplits,andreport
selectpathsbasedonalignmentwithresponses. Spearmanrankcorrelation(Spearman,1961)ofthe
• CODA-KBPATH: variant that retrieves paths metrics and the ratings. Krippendorff’s alpha for
In-Domain Out-Of-Domain
BLEU METEOR ROUGE-L BS-rec TC BLEU METEOR ROUGE-L BS-rec TC
GPT-2 3.4 11.9 23.9 35.4 26.7 3.0 10.8 22.2 35.0 29.7
GPT2-Fudge 3.4 12.4 24.4 36.1 28.3 3.4 11.1 23.0 35.1 29.6
Multigen 6.2 12.5 28.1 40.0 27.8 4.9 11.6 26.0 36.7 30.8
Concept-predict 3.3 12.3 28.5 38.1 28.3 3.7 11.6 23.1 35.9 26.3
CS-Pretrain 2.8 11.1 23.2 35.2 21.5 2.8 10.2 21.2 33.0 22.0
CODA 5.0 12.6 25.9 38.0 36.7 4.6 11.5 24.3 35.5 37.9
CODA-ONLYDA 4.0 12.4 24.4 37.5 32.7 3.1 11.1 22.7 35.3 33.2
CODA-NODA 4.4 12.3 25.1 37.8 35.7 4.5 11.6 24.4 35.4 36.0
CODA-NOEDGE 4.2 12.0 25.0 37.4 33.7 4.0 11.8 24.2 35.4 35.9
CODA-NOALIGN 3.7 12.4 25.5 38.5 32.1 3.2 11.2 22.8 35.6 31.2
CODA-KBPATH 3.6 12.5 24.9 38.6 33.9 3.6 11.4 24.1 35.9 33.0
CODA-UPPER 8.3 18.1 32.6 44.4 47.9 7.5 17.9 30.7 42.7 45.4
Human 6.5 13.1 26.5 41.3 77.4 4.9 12.3 24.0 37.6 77.3
Table3: Wepresenttheresultsofautomaticevaluationbasedonword-overlapandproposedTARGET-COHERENCE. CODA
outperformsallthebaselinesformostofthemetrics.WealsopresentresultsforCODA’smodelablations.
annotationis0.42. Results,showninlastcolumn Criteria Models Win Lose Tie
of Table 2, depict that most standard automated Smooth CODAvsGPT-2 37.5 31.6 31.0
metricscorrelatepoorlywithhumanratings,while CODAvsMultigen 32.3 22.8 44.8
Sensible CODAvsGPT-2 22.0 21.3 56.7
the, proposed TARGET-COHERENCE achieves a
CODAvsMultigen 25.8 25.6 48.6
veryhighcorrelationscoreof0.47.
Informative CODAvsGPT-2 32.3 27.3 40.4
We present the Amazon Mechanical Turk in- CODAvsMultigen 35.5 27.8 36.7
terface for human ratings collection in Figure 5
in the Appendix. The workers were first shown Table4: Humanevaluationthroughpairwisecomparisonbe-
tweenCODAandbaselines.CODAispreferredinsmoothness
instructionsaboutthetaskwithdefinitionsandex-
andinformativenesscriteriawhilebeingcomparablysensible.
amples for all the rating criteria. We paid crowd
workers on Amazon’s Mechanical Turk platform
ingcommonsenseknowledge(CODA-ONLYDA)
$0.7 per annotation and gave bonuses to annota-
leadstolargeperformancedrops,highlightingthat
tors with high annotation quality. Our estimated
CODA effectively utilizes commonsense knowl-
hourlypaywas$13,whichisabovetheminimum
edge. (2) Dropping data augmentation leads to a
USfederalhourlywage. Wesettheworkerqualifi-
smalldropinperformance(CODA-NODA),hint-
cationconditionas1000HITScompleted,95%or
ingatrelativelysmall(butstillsignificant)benefit
moreapprovalrateandlocationasnativeEnglish
from pretraining the model using data augmenta-
speakingcountries. Wereleasethehumanratings
tion. (3) Low performance of CODA-NOEDGE
andsystemoutputsusedforcomputingthemetric
showstheimportanceofusingedgesincommon-
correlationsaspartofthiswork.
sense paths. (4) Not aligning and selecting paths
basedontheirrelevancetoresponsesduringCRG
7.4 Results
training(CODA-NOALIGN)leadstoahighdrop
Inthissectionwepresenttheautomaticandhuman in performance. (5) CODA outperforms CODA-
evaluation results. Automated metric results are KBPATHby8%(ID)and14.5%(OOD).Thisim-
summarizedinTable3. Althoughreference-based proved performance can be attributed to the gen-
metrics are lexically biased (subsection 7.3), we eralizability of entities and paths generated from
still report their scores. We observe that CODA theKPGmodels. (6)CODA-UPPERachieveshigh
outperformsallthebaselinesunderin-domain(ID) scores, highlighting that further improvement in
aswellasout-of-domain(OOD)setupsofOtters commonsensepathgenerationcomponentcansig-
dataasper TARGET-COHERENCE(TC)score. For nificantlyboosttheoutputqualityof CODA.
example,CODAgetsahighTCscoreof36.7(ID) Human Evaluation: We conduct human eval-
and 37.9 (OOD) while the TC scores of the clos- uations on Amazon Mechanical Turk to evaluate
estbaselinesGPT2-Fudge,MultigenandConcept- thequalityofgeneratedtransitionresponses. An-
predict are in the range of 28-31, demonstrating notators are requested to evaluate the transition
that the proposed method leads to significant im- response on following criteria: (1) Smooth: rate
provementsinresponsequality. However,CODA whethertheresponseservesasasmoothtransition
isfarfromreachinghumanperformance(TC77.4). betweenthedialoguecontextandtarget. (2)Sensi-
CODAAblations: Weobservethat: (1)Notus- ble: whethertheresponsemakessenseinitselfi.e.
Context:ilikethesandonmyfeet sense: Doesthepathmakessense? Resultsreveal
Target:mypuppyiscalledgeorgie. that 79% of the paths were judged to be relevant
GPT-2:Mymomlikesthewater.
and76%ofthepathswerejudgedtomakesense.
Multigen:Mypetisthegecko.
CODA:Mydogwalksalongthebeachwithsand. Thusinaggregate,thegeneratedknowledgeisgood
CODA-Path:sandisatlocationbeachbelongstowalk in quality, and is used in the generated response.
isdesiredbypuppy
Pathnovelty: Weanalyzedthepathsgeneratedby
Context:myfavoritecityisseattle.
CODA whichwerejudgedassensiblebyhuman
Target:iridemybicycleeverywhere.
annotatorsandfoundthat26.8%ofentitiesinthe
GPT2:Seattleismyfavoritecitytogoto
Multigen:Sowhatdoyoudowhenyougototheseattle pathswerenotfoundinConceptNet. Thisinclude
CODA:Iboughtmybicyclefromabikeshopinseattle. entitiessuchas‘favoritefood’,‘petkitten’,‘single
CODA-Path:favoritecityisthelocationwhichhas
kid’and‘onlineclass’. Thus,theactualpathsfrom
bicycleshopisadependencyofridebicycle
theConceptNetmightnotbeabletocoveralarge
Context:iamaserveratafoodplace.
Target:ieatgreasyfoods. fractionofhead/tailentities. Furthermore,81%of
GPT-2:Ieathealthyfoodsatrestaurants. sensible paths are novel and do not exist in Con-
Multigen:Ihatemyfood.
ceptNet. For example, even though the path ‘eat
CODA:Iamaserver,butIdon’twanttoeattoomuch.
CODA-Path:serverisapersonnotdesireseatgreasyfood motivates go to restaurant has subevent dinner is
thelocationforbread’existinConceptNet,thepath
Table5:Samplerepresentativemodeloutputs.Theknowledge ‘eatmotivatesgotorestauranthassubeventdinner
pathsusedbyCODAprovideinterpretabilityandcontrolover
isthelocationforpizza’doesnotexistinConcept-
theresponsegenerationprocess
Net. ThusweshowthatCODAcangeneralizeto
itisgrammaticalandlogicallycoherent. (3)Infor- newentitiesandpaths.
mative: howmuchinformativecontentaresponse
InAppendixCwediscussahuman-in-the-loop
carries. Humanannotatorscompare(ormarkasa
study for controllability. The human-in-the-loop
tie) responses from two models. We collect two
experimentshowsthatevenminimalhumaninter-
annotationsfor100randomlyselecteddatapoints
ventionintheformofdomainrelevantkeywords
from the test outputs. Results in Table 4 demon-
inputforknowledgepathscanimprovethequality
strate that CODA outputs are preferred over the
andsmoothnessofthetransitionresponses.
baselineson‘Smooth’and‘Informative’criteria.
8 Conclusion
7.5 QualitativeAnalysis
Inthiswork,weproposeandevaluatemodelsfor
Wepresentrepresentativeoutputsfromthemodels
target-guided response generation using explicit
inTable5. ForCODA,weshowthepathusedin
commonsensebridgingpaths. Wealsointroducean
response generation. We notice that GPT-2 and
automatedmetrictoevaluatesmoothnessofatran-
Multigenoftentendtoeithergeneratesimpleout-
sitionresponse. Weshowedthatourmodelgener-
puts (e.g. ‘I hate my food’ in the last example)
atesmoresmoothandinformativeoutputsthrough
orsimplyrepeatoraddresseitherthetargetorthe
automaticandhumanevaluation. Furthermore,it
context(e.g. ‘Mypetisthegecko’,‘Seattleismy
allows for more interpretable results. Going for-
favorite city to go.’) which leads to high BLUE
ward,weenvisionamodelwhichcouldcombine
andMETEORscores,butlowTCscores. CODA
targetandnon-targetguideddialogueplanning.
avoidsthesepitfallsasitisconditionedongener-
atedcommonsensepathsbasedonboththecontext
Acknowledgments
andtargetentitiesleadingtomoreinformativeand
sensible outputs. However, CODA is susceptible WethankMaartenSapandtheanonymousreview-
to two issues: 1) Using poor keywords for path ers for providing valuable feedback. This work
generation, and 2) Generation of irrelevant paths was funded by the Defense Advanced Research
(e.g. ‘serverisapersonnotdesiresgreasyfood’in PlanningAgency(DARPA)underDARPAGrant
thelastexample). N6600198-18908,andtheNationalScienceFoun-
Path quality: We conduct a human evaluation dation under Award No. IIS1816012. Any opin-
studytomeasurethequalityofthegeneratedpaths. ions, findings, and conclusions or recommenda-
Forrandomlyselected100generatedresponses,we tions expressed in this material are those of the
askannotatorstojudge1)Relevance: Isthepath authorsanddonotnecessarilyreflecttheviewsof
relevant and used in the response? and 2) Makes thefundingagencies.
EthicsandBroaderImpact safetyissuesinopen-domaindialoguegeneration
whichcanbeutilizedforthetarget-guidedresponse
BroaderImpactandapplications: Ourproposed
generationtask.
modelsfortarget-guidedresponsegenerationcan
Limitations and potential biases: Current con-
beusedtogenerateresponsesbasedontargetsen-
versationalsystemssufferfromseverallimitations,
tencesthatcandrivethesystem’sagendainacon-
suchas,theyarenotgoodathumanqualitiessuch
versation. Deployingatarget-sentenceguideddia-
as empathy, morality, discretion and factual cor-
loguemodelneedscarefulconsiderationandtest-
rectness. Thereisariskthatatargetdrivensystem
ingsincedesignatingatargetsentenceforallturns
would ignore these factors to achieve the target.
ofaconversationmightdisruptthenaturalflowof
Therefore more research is needed to equip bots
theconversation. Therefore,theycanbedeployed
withsuchqualities. Ourmodelsaretrainedonex-
alongsideexistingnon-targetguideddialoguemod-
istingdatasetssuchasOttersandDailyDialog,and
els that perform free-flow conversations without
also leverage external commonsense knowledge
predesignatedtargets. Ateachturnofaconversa-
resources. KnowledgegraphssuchasConceptNet
tion,acentralsystemcanusethetarget-coherence
havebeenfoundtocontainbiasesandhaveweak
metric to decide if the system should generate a
representations of moral common sense knowl-
target-guided response or a simple follow-up re-
edge (Hulpus et al., 2020; Mehrabi et al., 2021).
sponsetothecontext. Target-guidedsystemscan ,
Whilegroundingonknowledgepathsfromknowl-
usedforseveralusefulapplicationssuchascreating
edgegraphscanprovideinsightsandexplanations
non-obtrusiverecommendations,comfortingpeo-
aboutthemodel’sreasoning,ourmodelscouldpo-
ple,recommendingnewproductsandservices,and
tentiallyinheritbiasespresentinthesedatasources.
introducing interesting new topics and educating
AdvancementsinaddingamoraldimensiontoKGs,
usersaboutthosetopics.
andextendingthemwithintuitionofmorality(such
Potential risks and solutions: We wish to raise
ascrimeisbad),canenablegenerationofmorally
awarenessaboutpotentialmisuseofproposedsys-
correct knowledge paths. Furthermore, imbuing
temsforpersuadingusersbypeoplewithillinten-
conversational systems with empathy (Ma et al.,
tions. For example, conversational systems can
2020), moral discretion (Ziems et al., 2022) and
pose as humans and then proactively alter user’s
factualcorrectness(Guptaetal.,2021b;Dzirietal.,
perceptions about specific issues, evaluations of
2022)willimproveusers’experienceandtrustin
products or services, or political inclinations. To
thesystem.
circumventsuchissues,itisnecessarytoimprove
WehaveincludedtheMechanicalTurkarrange-
transparencythroughregulations,suchasinform-
mentsandworkerpayinthelastparagraphofthe
ingtheusersthattheyareconversingwithabotand
section 7.3. We paid well above the US federal
not a human. Regulations are necessary to avoid
minimumwage(around$13hourly)andprovided
hazardousoutcomesduringdeploymentforspecific
enough time to the workers to complete the task
domains. Forexample,EuropeanUnion’sregula-
whichwasdeterminedbasedonafewpilotexperi-
toryframeworkproposalonartificialintelligence2
ments.
defines use of AI systems for “educational or vo-
cational training, that may determine the access
toeducationandprofessionalcourseofsomeone’s References
life”ashighrisk. Anyonewhousesorbuildsupon
ForoughArabshahi,JenniferLee,MikaylaGawarecki,
our system should comply with such regulations. KathrynMazaitis, AmosAzaria, andTomMitchell.
Apart from regulations, recent safety and ethics 2020. Conversational neuro-symbolic common-
relatedresearchanddatasets(Bahetietal.,2021; sensereasoning. arXivpreprintarXiv:2006.10022.
Sunetal.,2021)inconversationalAIcanhelpin Ashutosh Baheti, Maarten Sap, Alan Ritter, and Mark
mitigatingaforementionedissues. Hendersonetal. Riedl. 2021. Just say no: Analyzing the stance
(2018) and Dinan et al. (2021) highlight and dis- of neural dialogue generation in offensive contexts.
In Proceedings of the 2021 Conference on Empiri-
cuss potential ethical and safety issues that arise
calMethodsinNaturalLanguageProcessing,pages
indialoguesystemsresearch. Xuetal.(2020)pro-
4846–4862,OnlineandPuntaCana,DominicanRe-
videsareviewofrecentmethodsthattrytomitigate public.AssociationforComputationalLinguistics.
2https://digital-strategy.ec.europa.e SatanjeevBanerjeeandAlonLavie.2005. Meteor: An
u/en/policies/regulatory-framework-ai automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings Jian Guan, Yansen Wang, and Minlie Huang. 2019b.
oftheaclworkshoponintrinsicandextrinsicevalu- Story ending generation with incremental encod-
ationmeasuresformachinetranslationand/orsum- ing and commonsense knowledge. Proceedings
marization,pages65–72. of the AAAI Conference on Artificial Intelligence,
33(01):6473–6480.
AntoineBosselut,HannahRashkin,MaartenSap,Chai-
tanya Malaviya, Asli Celikyilmaz, and Yejin Choi. PrakharGupta,YuliaTsvetkov,andJeffreyP.Bigham.
2019. COMET:Commonsensetransformersforau- 2021a. Synthesizingadversarialnegativeresponses
tomaticknowledgegraphconstruction. InProceed- forrobustresponserankingandevaluation. InFind-
ings of the 57th Annual Meeting of the Association ings of the Association for Computational Linguis-
for Computational Linguistics, pages 4762–4779, tics: ACL/IJCNLP 2021, Online Event, August 1-
Florence, Italy. Association for Computational Lin- 6, 2021, volume ACL/IJCNLP 2021 of Findings of
guistics. ACL, pages 3867–3883. Association for Computa-
tionalLinguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and
Kristina Toutanova. 2019. BERT: Pre-training of
Caiming Xiong. 2021b. Dialfact: A benchmark
deep bidirectional transformers for language under-
for fact-checking in dialogue. arXiv preprint
standing. In Proceedings of the 2019 Conference
arXiv:2110.08222.
of the North American Chapter of the Association
for Computational Linguistics: Human Language Behnam Hedayatnia, Karthik Gopalakrishnan,
Technologies, Volume 1 (Long and Short Papers), Seokhwan Kim, Yang Liu, Mihail Eric, and
pages4171–4186,Minneapolis,Minnesota.Associ- Dilek Hakkani-Tur. 2020. Policy-driven neural
ationforComputationalLinguistics. responsegenerationforknowledge-groundeddialog
systems. In Proceedings of the 13th International
Emily Dinan, Gavin Abercrombie, A Stevie Bergman, Conference on Natural Language Generation,
Shannon Spruit, Dirk Hovy, Y-Lan Boureau, and pages 412–421, Dublin, Ireland. Association for
Verena Rieser. 2021. Anticipating safety issues ComputationalLinguistics.
in e2e conversational ai: Framework and tooling.
arXivpreprintarXiv:2107.03451. Peter Henderson, Koustuv Sinha, Nicolas Angelard-
Gontier,NanRosemaryKe,GenevieveFried,Ryan
ChrisDonahue,MinaLee,andPercyLiang.2020. En- Lowe, and Joelle Pineau. 2018. Ethical challenges
ablinglanguagemodelstofillintheblanks. InPro- in data-driven dialogue systems. In Proceedings of
ceedings of the 58th Annual Meeting of the Associ- the2018AAAI/ACMConferenceonAI,Ethics, and
ationforComputationalLinguistics,ACL2020,On- Society, AIES ’18, page 123–129, New York, NY,
line,July5-10,2020,pages2492–2501.Association USA.AssociationforComputingMachinery.
forComputationalLinguistics.
Ioana Hulpus, Jonathan Kobbe, Heiner Stucken-
,
Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Os- schmidt, and Graeme Hirst. 2020. Knowledge
mar Zaiane, Mo Yu, Edoardo M Ponti, and Siva graphs meet moral values. In Proceedings of the
Reddy. 2022. Faithdial: A faithful benchmark Ninth Joint Conference on Lexical and Computa-
for information-seeking dialogue. arXiv preprint tional Semantics, pages 71–80, Barcelona, Spain
arXiv:2204.10757. (Online). Association for Computational Linguis-
tics.
Marjan Ghazvininejad, Chris Brockett, Ming-Wei
HaozheJi,PeiKe,ShaohanHuang,FuruWei,Xiaoyan
Chang,BillDolan,JianfengGao,Wen-tauYih,and
Zhu,andMinlieHuang.2020. Languagegeneration
MichelGalley.2018. Aknowledge-groundedneural
with multi-hop reasoning on commonsense knowl-
conversationmodel. InAAAI.
edgegraph. InProceedingsofthe2020Conference
onEmpiricalMethodsinNaturalLanguageProcess-
JianGuan,FeiHuang,ZhihaoZhao,XiaoyanZhu,and
ing, EMNLP 2020, Online, November 16-20, 2020,
Minlie Huang. 2020. A knowledge-enhanced pre-
pages725–736.AssociationforComputationalLin-
training model for commonsense story generation.
guistics.
Transactions of the Association for Computational
Linguistics,8:93–108. Nitish Shirish Keskar, Bryan McCann, Lav R. Varsh-
ney, Caiming Xiong, and Richard Socher. 2019.
Jian Guan, Yansen Wang, and Minlie Huang. 2019a. CTRL: A conditional transformer language model
Story ending generation with incremental encoding forcontrollablegeneration. CoRR,abs/1909.05858.
and commonsense knowledge. In The Thirty-Third
AAAI Conference on Artificial Intelligence, AAAI Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
2019, The Thirty-First Innovative Applications of Cao, and Shuzi Niu. 2017. DailyDialog: A manu-
Artificial Intelligence Conference, IAAI 2019, The allylabelledmulti-turndialoguedataset. InProceed-
Ninth AAAI Symposium on Educational Advances ingsoftheEighthInternationalJointConferenceon
in Artificial Intelligence, EAAI 2019, Honolulu, Natural Language Processing (Volume 1: Long Pa-
Hawaii,USA,January27-February1,2019,pages pers),pages986–995,Taipei,Taiwan.AsianFedera-
6473–6480.AAAIPress. tionofNaturalLanguageProcessing.
Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xi- TongNiuandMohitBansal.2018. Politedialoguegen-
ang Ren. 2019. KagNet: Knowledge-aware graph erationwithoutparalleldata. TransactionsoftheAs-
networks for commonsense reasoning. In Proceed- sociationforComputationalLinguistics,6:373–389.
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter- Martha Palmer, Daniel Gildea, and Paul Kingsbury.
nationalJointConferenceonNaturalLanguagePro- 2005. The Proposition Bank: An annotated cor-
cessing(EMNLP-IJCNLP),pages2829–2839,Hong pus of semantic roles. Computational Linguistics,
Kong, China. Association for Computational Lin- 31(1):71–106.
guistics.
KishorePapineni,SalimRoukos,ToddWard,andWei-
Chin-YewLin.2004. Rouge: Apackageforautomatic JingZhu.2002. Bleu: amethodforautomaticeval-
evaluation of summaries. In Text summarization uationofmachinetranslation. InProceedingsofthe
branchesout,pages74–81. 40th annual meeting of the Association for Compu-
tationalLinguistics,pages311–318.
Yanxiang Ling, Fei Cai, Xuejun Hu, Jun Liu, Wanyu
Chen,andHonghuiChen.2021. Context-controlled Jinghui Qin, Zheng Ye, Jianheng Tang, and Xiaodan
topic-aware neural response generation for open- Liang.2020a. Dynamicknowledgeroutingnetwork
domain dialog systems. Inf. Process. Manag., fortarget-guidedopen-domainconversation. InThe
58(1):102392. Thirty-Fourth AAAI Conference on Artificial Intelli-
gence,AAAI2020,TheThirty-SecondInnovativeAp-
Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose- plicationsofArtificialIntelligenceConference,IAAI
worthy, Laurent Charlin, and Joelle Pineau. 2016. 2020, The Tenth AAAI Symposium on Educational
How NOT to evaluate your dialogue system: An AdvancesinArtificialIntelligence,EAAI2020,New
empirical study of unsupervised evaluation metrics York, NY, USA, February 7-12, 2020, pages 8657–
fordialogueresponsegeneration. InProceedingsof 8664.AAAIPress.
the2016ConferenceonEmpiricalMethodsinNatu-
ralLanguageProcessing,pages2122–2132,Austin, LianhuiQin,VeredShwartz,PeterWest,ChandraBha-
Texas.AssociationforComputationalLinguistics. gavatula, Jena D. Hwang, Ronan Le Bras, Antoine
Bosselut,andYejinChoi.2020b. Backtothefuture:
Ryan Lowe, Michael Noseworthy, Iulian Vlad Ser- Unsupervisedbackprop-baseddecodingforcounter-
ban,NicolasAngelard-Gontier,YoshuaBengio,and factual and abductive commonsense reasoning. In
Joelle Pineau. 2017. Towards an automatic turing Proceedings of the 2020 Conference on Empirical
test: Learning to evaluate dialogue responses. In Methods in Natural Language Processing, EMNLP
Proceedings of the 55th Annual Meeting of the As- 2020, Online, November 16-20, 2020, pages 794–
sociation for Computational Linguistics (Volume 1: 805.AssociationforComputationalLinguistics.
LongPapers),pages1116–1126.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Yukun Ma, Khanh Linh Nguyen, Frank Z. Xing, and Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
Erik Cambria. 2020. A survey on empathetic dia- guage models are unsupervised multitask learners.
loguesystems. InformationFusion,64:50–70. OpenAIblog,1(8):9.
Bodhisattwa Prasad Majumder, Harsh Jhamtani, Tay- Ananya B Sai, Akash Kumar Mohankumar, and
lor Berg-Kirkpatrick, and Julian J. McAuley. 2020. Mitesh M Khapra. 2020. A survey of evalua-
Like hiking? you probably enjoy nature: Persona- tion metrics used for nlg systems. arXiv preprint
groundeddialogwithcommonsenseexpansions. In arXiv:2008.12009.
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing, EMNLP KarinSevegnani,DavidM.Howcroft,IoannisKonstas,
2020, Online, November 16-20, 2020, pages 9194– and Verena Rieser. 2021. Otters: One-turn topic
9206.AssociationforComputationalLinguistics. transitions for open-domain dialogue. In Proceed-
ings of the 59th Annual Meeting of the Association
Chaitanya Malaviya, Chandra Bhagavatula, Antoine forComputationalLinguisticsandthe11thInterna-
Bosselut, and Yejin Choi. 2020. Commonsense tional Joint Conference on Natural Language Pro-
knowledge base completion with structural and se- cessing, ACL/IJCNLP 2021, (Volume 1: Long Pa-
manticcontext. Proceedingsofthe34thAAAICon- pers),VirtualEvent,August1-6,2021,pages2492–
ferenceonArtificialIntelligence. 2504.AssociationforComputationalLinguistics.
Ninareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pu- PengShiandJimmyLin.2019. Simplebertmodelsfor
jara,XiangRen,andAramGalstyan.2021. Lawyers relationextractionandsemanticrolelabeling. arXiv
are dishonest? quantifying representational harms preprintarXiv:1904.05255.
incommonsenseknowledgeresources. InProceed-
ings of the 2021 Conference on Empirical Methods HaoyuSong,W.Zhang,YimingCui,DongWang,and
inNaturalLanguageProcessing,pages5016–5033, T.Liu.2019. Exploitingpersonainformationfordi-
OnlineandPuntaCana,DominicanRepublic.Asso- versegenerationofconversationalresponses. InIJ-
ciationforComputationalLinguistics. CAI.
Charles Spearman. 1961. The proof and measure- forComputationalLinguistics,ACL2019,Florence,
ment of association between two things. Appleton- Italy, July 28- August 2, 2019, Volume 1: Long Pa-
Century-Crofts. pers, pages 2002–2012. Association for Computa-
tionalLinguistics.
Robyn Speer, Joshua Chin, and Catherine Havasi.
2017a. Conceptnet5.5: Anopenmultilingualgraph Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
ofgeneralknowledge. Weinberger,andYoavArtzi.2020. Bertscore: Eval-
uating text generation with BERT. In 8th Inter-
Robyn Speer, Joshua Chin, and Catherine Havasi.
national Conference on Learning Representations,
2017b. Conceptnet5.5: Anopenmultilingualgraph
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
of general knowledge. In Proceedings of the AAAI
2020.OpenReview.net.
ConferenceonArtificialIntelligence,volume31.
Tiancheng Zhao, Ran Zhao, and Maxine Eskénazi.
Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng,
2017. Learning discourse-level diversity for neural
Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan
dialog models using conditional variational autoen-
Zhu,andMinlieHuang.2021. Onthesafetyofcon-
coders. In Proceedings of the 55th Annual Meet-
versational models: Taxonomy, dataset, and bench-
ingoftheAssociationforComputationalLinguistics,
mark. arXivpreprintarXiv:2110.08466.
ACL 2017, Vancouver, Canada, July 30 - August 4,
Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xi- Volume1:LongPapers,pages654–664.Association
aodan Liang, Eric P. Xing, and Zhiting Hu. 2019. forComputationalLinguistics.
Target-guided open-domain conversation. In Pro-
ceedings of the 57th Conference of the Association Peixiang Zhong, Yong Liu, Hao Wang, and Chunyan
forComputationalLinguistics,ACL2019,Florence, Miao.2021. Keyword-guidedneuralconversational
Italy, July 28- August 2, 2019, Volume 1: Long Pa- model. ProceedingsoftheAAAIConferenceonAr-
pers, pages 5624–5634. Association for Computa- tificialIntelligence,35(16):14568–14576.
tionalLinguistics.
Peixiang Zhong, Di Wang, and Chunyan Miao. 2019.
Chongyang Tao, Lili Mou, Dongyan Zhao, and Rui An affect-rich neural conversational model with bi-
Yan.2017. Ruber: Anunsupervisedmethodforau- ased attention and weighted cross-entropy loss. In
tomatic evaluation of open-domain dialog systems. ProceedingsoftheAAAIConferenceonArtificialIn-
arXivpreprintarXiv:1701.03079. telligence,volume33,pages7492–7500.
Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro HaoZhou,TomYoung,MinlieHuang,HaizhouZhao,
Szekely,andXiangRen.2020. Connectingthedots: Jingfang Xu, and Xiaoyan Zhu. 2018. Com-
A knowledgeable path generator for commonsense monsenseknowledgeawareconversationgeneration
question answering. In Findings of the Association withgraphattention. InProceedingsoftheTwenty-
forComputationalLinguistics:EMNLP2020,pages SeventhInternationalJointConferenceonArtificial
4129–4140, Online. Association for Computational Intelligence, IJCAI-18, pages 4623–4629. Interna-
Linguistics. tional Joint Conferences on Artificial Intelligence
Organization.
Wenquan Wu, Zhen Guo, Xiangyang Zhou, Hua Wu,
XiyuanZhang,RongzhongLian,andHaifengWang.
Pei Zhou, Karthik Gopalakrishnan, Behnam Hedayat-
2019. Proactive human-machine conversation with
nia, Seokhwan Kim, Jay Pujara, Xiang Ren, Yang
explicit conversation goal. In Proceedings of the
Liu,andDilekHakkani-Tur.2021a. Commonsense-
57th Annual Meeting of the Association for Com-
focused dialogues for response generation: An em-
putational Linguistics, pages 3794–3804, Florence,
pirical study. In Proceedings of the 22nd Annual
Italy.AssociationforComputationalLinguistics.
MeetingoftheSpecialInterestGrouponDiscourse
and Dialogue, pages 121–132, Singapore and On-
Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Ja-
line.AssociationforComputationalLinguistics.
son Weston, and Emily Dinan. 2020. Recipes for
safety in open-domain chatbots. arXiv preprint
Pei Zhou, Karthik Gopalakrishnan, Behnam Hedayat-
arXiv:2010.07079.
nia, Seokhwan Kim, Jay Pujara, Xiang Ren, Yang
Kevin Yang and Dan Klein. 2021. FUDGE: con- Liu, and Dilek Hakkani-Tur. 2021b. Think before
trolledtextgenerationwithfuturediscriminators. In youspeak: Usingself-talktogenerateimplicitcom-
Proceedings of the 2021 Conference of the North monsenseknowledgeforresponsegeneration. arXiv
American Chapter of the Association for Computa- preprintarXiv:2110.08501.
tional Linguistics: Human Language Technologies,
Pei Zhou, Behnam Hedayatnia, Karthik Gopalakrish-
NAACL-HLT 2021, Online, June 6-11, 2021, pages
nan, Seokhwan Kim, Jay Pujara, Xiang Ren, Yang
3511–3535.AssociationforComputationalLinguis-
Liu, and Dilek Hakkani-Tur. 2021c. Think before
tics.
youspeak: Learningtogenerateimplicitknowledge
Pengcheng Yang, Lei Li, Fuli Luo, Tianyu Liu, and forresponsegenerationbyself-talk. InProceedings
Xu Sun. 2019. Enhancing topic-to-essay genera- of the 3rd Workshop on Natural Language Process-
tionwithexternalcommonsenseknowledge. InPro- ing for Conversational AI, pages 251–253, Online.
ceedings of the 57th Conference of the Association AssociationforComputationalLinguistics.
CalebZiems,JaneAYu,Yi-ChiaWang,AlonHalevy, pairs of entities. The value of top D is selected
and Diyi Yang. 2022. The moral integrity corpus: based on validation performance and comes out
A benchmark for ethical dialogue systems. arXiv
typicallybetween1-3.
preprintarXiv:2204.03021.
Knowledgegraphdetails: Thenumberofnodes
Asli Çelikyilmaz, Elizabeth Clark, and Jianfeng Gao. in the ConceptNet resource we have used4 is
2020. Evaluation of text generation: A survey. 382226. Weperformrandomwalksonthegraph
ArXiv,abs/2006.14799.
withpathsoflengthfrom1to6andgetatotalof
3883671numberofpaths.
A ImplementationDetailsforCODA
Edges in the knowledge path: We discard some
A.1 TrainingDetailsfor CODA edgetypeswhichareregardedtobeuninformative
and offer little help for our task following Wang
Model training: We code our models using Py-
torchandHuggingface3 library. Weusevalidation et al. (2020). They include RelatedTo, Synonym,
Antonym,DerivedFrom,FormOf,Etymologically-
loss to do model selection. The KPG-wc, KPG-
DerivedFromandEtymologicallyRelatedTo. Since
htandCRGmodelsareallbasedonGPT-2small
the nodes in ConceptNet are directional, we also
architecture. We use batch size of 10 for GPT-2
add inverse edges during path sampling. For ex-
models. WeuseAdamoptimizerwithinitiallearn-
amplethepath“ecosystem<–PartOf<–organism”
ing rate of 1e − 4. We use GeForce RTX 2080
canbesampledas“ecosystem_isPartOforganism”
GPUsfortrainingmodels. Allexistingcodeused
wheretheunderscoreindicatesareverseedge.
anddatasetswereCC-BY4.0oropensourcedby
originalauthors.
A.2 ClauseIdentificationforData
Decoding paths and responses: For decoding
Augmentation
pathsusingtheKPGmodels,weusetemperature
Fortargetcreation,givenadialoguecontextcand
of0.7andnucleussamplingwithtop-psetto0.9.
its response r, we first break the response r into
We use the same decoding strategy and hyperpa-
sentenceclauses. Forexample,givenacontext“Is
rametersfordecodingresponsesusingCRGmodel.
my booking complete?” and the response “your
Concept Extraction: Entities are extracted from
reservation is confirmed. now i need your phone
the context, target and response to generate and
number,”,weextractaclauset“ineedyourphone
alignpathsusingtheKPGmodels. Forasentence
number” as the target candidate t. For clause ex-
s,wefirstextractthesetofnounandverbphrases
tractionweuseAllennlp’sSRLparser5 whichis
from the sentence using NLTK. We design sim-
trained using a BERT-based model (Shi and Lin,
ple grammar rules to convert some phrases to a
2019) and is based on PropBank (Palmer et al.,
moreconciseformsthataresimilartothekindsof
2005). Itidentifiestheargumentsassociatedwith
nodes present in ConceptNet,e.g., “watching the
the predicates or verbs of a sentence predicates
star”isconvertedto“watchstars”. WeuseNLTK’s
(verbsorevents)inasentenceandclassifiesthem
POS tagging combined with the following gram-
intorolessuchasagent,patientandinstrument. For
mar rules: (1) Nouns and Adjectives, terminated
theexampleabove,itidentifies“need”asapredi-
withNouns<NN.*|JJ>*<NN.*>(2)Verbandverb
catewithagent“i”andinstrument“yournumber”.
phrases<RB.?>*<VB.?>*<JJ>*<VB.?>+<VB>?.
WenormalizetheverbsusingNLTK.Thefinalset
A.3 DataAugmentationforCODA
ofentitiesconsistofthenounandverbphrases. We
We filter data from the dailydialog dataset based
excludephrasessuchas“today”,“enough”which
on a threshold set to 0.7 for data augmentation.
aresometimesincorrectlydetectedasentities.
Thisthresholdwasselectedusingempericalperfor-
Sub-selecting entity pairs during training of
manceofthrCODAmodel. ForCODA-ONLYDA
CRG model: For every context-target pair, we
model which does not use knowledge paths, the
have n number of pair of head-tails entities. We
context, target and transition response is used di-
scoreanentitypairbycalculatingtheinversedocu-
rectly in training the CRG decoder of CODA-
mentfrequencies(computedusingGutenbergEn-
ONLYDA model. But for CODA model which
glishcorpus)oftheentitytokensandsummingup
usesthe knowledge paths, the dailydialogdata is
themaximumvaluefoundforatokenineachentity
in the pair. For training phase, we keep the topD 4www.github.com/wangpf3/Commonsense-P
ath-Generator
3https://huggingface.co/ 5github.com/allenai/allennlp
Context:ienjoystaringupatthesky. continuations from the poor ones and guides the
Target:iliketospendalotofmyfreetimewithmypet. GPT2baseddecodertowardsresponsesthatareco-
Response1:Ilikestargazingoutsidewithmypet.(0.99)
herenttoboththesourceandtargetsentences. The
Response2:Ilikestargazingoutside.(0.05)
Response3:Ilikewalkingwithmypet.(0.01) Fudge discriminator needs positive and negative
Response4:Mypetisabigstar.(0.02) sample data for training. We train the discrimi-
Context:imakeblogs. nator to distinguish a good response from a bad
Target:ihavealargefamilywithbabies.
(not coherent to target or context). The input to
Response1:Iwanttoblogaboutmychildren.(0.99)
Response2:Myfamilyhasalotofbabies.(0.05) trainthediscriminator(aLSTMmodel)isthecon-
Response3:Myblogsareveryfamous.(0.01) catenationofthecontextsentence,followedbythe
targetsentenceandfinallythetokensofaresponse
Table6:StresstestingtheTarget-Coherencemetric.Weshow
r with tokens k. The discriminator then learns to
sampleresponsesandTCscorefortheresponsesinbrackets.
predict1ifthenexttokenintheresponseatposi-
tionkbelongstothegoldresponseor0ifthetoken
convertedtothesameformatasOttersdata,thatis,
is a random one. We train the Fudge discrimina-
wefirstdoentitydetectiononthetargetcomponent
torbypreparingnegativeinstancesusingthesame
oftheresponsesaswellasthethedialoguecontext.
techniques we use to train the Target-Coherence
Then we generate a set of paths for each pair of
model - sampling random negative responses, re-
entities. TheCODAmodelisfirsttrainedonpaths
sponsescoherenttothecontextbutnottothetarget,
from the filtered dailydialog data and then fine-
andresponsescoherenttothetargetbutnottothe
tunedontheOttersdatasetwhichfollowsthesame
context.
knowledge path format. The maximum dialogue
TrainingCS-PretrainmodelThemodelisbased
historylengthissetto2fordailydialogdataset.
onthecommonsensestorygenerationmodelfrom
A.4 TargetCoherenceMetric Guan et al. (2020) We create training data for
theCS-Pretrainmodelbyusingthesamesampled
InTable6,weprovideexamplesforstresstesting
pathsweusefortrainingtheKPG-wcmodel. The
the Target-Coherence metric. TC scores for the
pathsareconvertedintotextualformatbyconvert-
responsesareshowninbrackets. Simplyrepeating
ingedgesintotextsequences. Themodelisonly
oraddressingeitherthetargetorcontextgetsalow
pretrained with general commonsense paths and
TCscore. Forexampletheresponse“Ilikestargaz-
thenfine-tunedonOttersdatasetinamannersimi-
ing outside” is not a smooth transition and gets
lartotheGPT-2baselines(i.e. withoutpaths). Our
a low TC score, while “I like stargazing outside
experimentsshowthatpretrainingwithcommon-
withmypet”isasmoothtransitionandgetsahigh
sensemodeldoesnothelpwithtarget-guidedtask,
TC score. In Figure 4 we present an overview of
probably since the task needs target conditional
themechanismsusedforgeneratingnegativesam-
commonsense and general commonsense knowl-
plesfortrainingtheTarget-Coherencemetric. For
edgeonlyconfusesthemodelduringdecoding.
negativeexamples,1)Givengoldresponser,and
context c, we sample a random negative target t’, Training Concept-Predict leverages a concept
whichcreatesaresponsewhichdoesnottransition prediction strategy from Qin et al. (2020a). The
towardsthetargett,2)Givengoldresponser,and input to the model is the context and target and
target t, we sample a random negative context c’, itpredictsasingleconceptbasedonclosenessto
whichcreatesaresponsewhichisnotcoherentto the target. The concept is then fed as an input to
thecontextc, 3)Givengoldcontextc, andtarget theCRGmodelalongwiththecontextandtarget
t,weeithersamplearandomnegativeresponser’ sentences.
or generate a response r’ conditioned on random Training CODA-ONLYDA: CODA variantthat
c’ort’, whichcreatesaresponsewhichdoesnot uses Dailydialog augmentation and does not use
transitiontotargettoriscoherenttocontextc. commonsensepathsfromKPGmodelsintheCRG
model. ThereforethemodelconsistsofonlyaCRG
B TrainingDetailsofBaselines
model(noKPGmodels)whichtakethecontextand
Training GPT-2 Fudge model Yang and Klein targetsentencesasinputs.
(2021) proposed a future discriminator based de- Training CODA-NOEDGE CODA variant that
codingtechnique. TheFudgediscriminatorusesa uses only entities and no edges from the path.
discriminatortrainedtodistinguishgoodresponse Forexamplethepath“favoritecityisthelocation
Target Keywords
ineedyouraddress sendmoney;visit;mail;sendgift;sendcoupon
youshouldspendtimewithyourfriends don’tbealone;mentalhealth;behappy;
youcantryourrestaurant bestingredients;cheapestfood;freedelivery
ournewrecipeisbestselling fatfree;healthy;protein;tasty
iamthebestfinancialadvisor getrichquickly;soundadvice;moneymanagement
youshouldhaveapositiveattitude mentalhealth;otherswillhelp;peace
weshouldalwaysavoidfighting peace;happiness;injury;understandotherpeople
iwanttocometounitedstates freedom;democracy;money;job;americandream;education
everyoneshouldgetvaccinated publichealth;reducehospitalburden;livelonger;covid;besafe
weshoulddonatetocharity helppoor;makeadifference;giveassistance;feelgood;socialbenefits
Table7: Thesetofmanuallycreatedtargetsandkeywordsetusedforeachtarget.
Figure5: Amazonmechanicalturkinterfaceforhumanratingscollection
whichhasbicycleshopisadependencyofridebi- Context:idyemyhair.
cycle” is converted to “favorite city bicycle shop Target:weshoulddonatetocharity.
Path(KPG-oneent):hairbelongstopeoplemotivatedby
ride bicycle”, which is fed as input to the CRG
giveassistancehasprequisitedonatetocharity.
model.
CODA-controlled:Idonatemyhairtoanon-profitthat
Training CODA-NOALIGN: variant that relies helpspeopleinneed.
ononlyKPG-htfortrainingandinference. Does Path(KPG-ht):hairbelongstopeopledesiresdonate
tocharity
notselectpathsbasedonalignmentwithresponses.
CODA:Peoplewhodonateareverygoodpeople.
The paths used during training the CRG model
Context:ihaveanamazinggarden.
comefromKPG-htinsteadofKPG-wc. Target:youcantryourrestaurant.
Training CODA-KBPATH: variantthatsamples Path(KPG-oneent):gardenisalocationofgrowfood
motivatedbygoalbestingredientsisdesiredbyperson
pathsdirectlyfromConceptNetusingthealgorithm
capableoftryrestaurant
proposedinLinetal.(2019). Givenapairofcon- CODA-controlled:Myrestaurantusesthebestingredients
textandtargetconcept,weusetheiralgorithmto fromthegarden.
Path(KPG-ht):gardenisalocationofhavefriends
sample an actual path directly from ConceptNet.
overhasprerequisitetryrestaurant
ThemodelispretrainedonDailydialogaugmented CODA:youcanhavefriendsover.
data and fine-tuned on Otters with the sampled
paths from ConceptNet. The model suffers from Table8: Sampledataandmodeloutputsfromthehuman-in-
the-loopexperiment. Theunderlinedwordsarekeywordin-
missingentitiesandmissinglinksbetweenentities
putsprovidedtothemodelKPG-oneent.Theitalicisedwords
inConceptNetwhichissolvedbyCODA. in the CODA controlled outputs are phrases are generated
basedontheinputkeywords.
C Human-in-the-loopExperiment
atedsetoftargetsentencesS ofsize10belonging
Can human involvement improve generation? todomainssuchashealthcareandcharity. Thedata
OurCRGmodelusesexplicitpathsgeneratedfrom createdisshowninTable7. Anexamplesentence
the KPG models, which not only provides inter- in set S is ‘we should donate to charity’ and we
pretability,italsoallowshuman-in-the-loopinter- manually curate a set of keywords such as ‘help
ventionforfinercontrollability. Totestthishypoth- poor’, ‘give assistance’ and ‘tax deductions’ that
esis, we create a model KPG-oneent which is a are relevant to the target sentence of interest and
hybridversionofKPG-wcandKPG-htmodel. The can guide the knowledge path sampling towards
modeltakesasingleentityn givenbyauserasan meaningfulpaths. Thisdatacreationtooktheau-
k
inputandistrainedtogenerateapathcontaining thors30minutesofeffort. For100randomsampled
thatentity. Wetestthismodelonamanuallycre- contexts from the Otters dataset, we select a ran-
dom target sentence from the set S and sample a
keywordkfromthecuratedsetofkeywordsofthat
target. We compare this controllable model with
the KPG-ht model that was used for path gener-
ation in all our experiments. We present sample
outputs of the model in Table 8. The input key-
words used as intervention are underlined. The
pathswhichusethekeywordinterventiongenerate
smoothertransitionscomparedtothepathswhich
donotusethekeywordintervention. Wefindthat
theTARGET-COHERENCEmetricfavorstheKPG-
oneent model in 59 percent of cases, confirming
thatevenminimalhumaninterventionintheform
ofdomainrelevantkeywordscanimprovethequal-
ityofgeneration.

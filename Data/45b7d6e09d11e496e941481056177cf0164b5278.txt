GPT-Sentinel: Distinguishing Human and ChatGPT
Generated Content
YutianChen† HaoKang† VivianZhai†
SchoolofComputerScience SchoolofComputerScience CollegeofEngineering
CarnegieMellonUniversity CarnegieMellonUniversity CarnegieMellonUniversity
Pittsburgh,PA15213 Pittsburgh,PA15213 Pittsburgh,PA15213
yutianch@andrew.cmu.edu haok@andrew.cmu.edu yiyanz@andrew.cmu.edu
LiangzeLi RitaSingh
LanguageTechnologiesInstitute LanguageTechnologiesInstitute
CarnegieMellonUniversity CarnegieMellonUniversity
Pittsburgh,PA15213 Pittsburgh,PA15213
liangzel@andrew.cmu.edu rsingh@cs.cmu.edu
BhikshaRaj
LanguageTechnologiesInstitute
CarnegieMellonUniversity
Pittsburgh,PA15213
bhiksha@cs.cmu.edu
Abstract
ThispaperpresentsanovelapproachfordetectingChatGPT-generatedvs. human-
writtentextusinglanguagemodels. Tothisend,wefirstcollectedandreleaseda
pre-processeddatasetnamedOpenGPTText,whichconsistsofrephrasedcontent
generatedusingChatGPT.Wethendesigned,implemented,andtrainedtwodif-
ferentmodelsfortextclassification,usingRobustlyOptimizedBERTPretraining
Approach(RoBERTa)andText-to-TextTransferTransformer(T5),respectively.
Our models achieved remarkable results, with an accuracy of over 97% on the
testdataset,asevaluatedthroughvariousmetrics. Furthermore,weconductedan
interpretabilitystudytoshowcaseourmodel’sabilitytoextractanddifferentiate
keyfeaturesbetweenhuman-writtenandChatGPT-generatedtext. Ourfindings
provide important insights into the effective use of language models to detect
generatedtext.
1 Introduction
Thedevelopmentofanalgorithmthatcanaccuratelydistinguishbetweenmachine-generatedtextand
human-generatedtexthasbecomecrucialincontextswhereverifyingtheauthenticityofinformationis
essential,suchasinlegalproceedingsandnewsreporting. Althoughtraditionalstatisticaltechniques
suchaslogisticregressionandsupportvectormachines(SVM)havebeenusedforthispurposeinthe
past[1],theemergenceofLargeLanguageModels(LLMs)likeInstructGPT[2]andtheavailability
ofitsfreedeployment,ChatGPT,haspresentedsignificantchallengestoexistingdetectionmethods.
Asaresult,theneedtodevelopnovelalgorithmsthatcanaccuratelydistinguishbetweenmachine
andhuman-generatedtexthasbecomemorepressingthaneverbefore.
†Threeauthorscontributeequallytothiswork.
3202
yaM
71
]LC.sc[
2v96970.5032:viXra
To address this issue, we focused on fine-tuning approaches to distinguish human-written and
ChatGPT-generatedtext. WefirstcollectedthedatafromChatGPTandestablishedtheOpenGPTText
dataset.Section3ofthepaperprovidesadetaileddescriptionofthedatacollectionprocess,including
thecriteriatoselectthesamplesandthemethodstofilteroutirrelevantandundesirednoiseinthe
collectedtext. WethentrainedthefrozenRoBERTawithMLPandfine-tunedtheT5modelonthis
datasetforclassification. TheresultingmodeliswhatwereferredtoasGPT-Sentinel. Moredetails
aboutthemodelcanbefoundinSection4ofthepaper.
Therestofthispaperisstructuredasfollows: WefirstdiscussrelatedworkinSection2;illustrate
OpenGPTTextdatasetinSection3;presentourmodelandthetrainingdetailsinSection4;evaluate
theperformanceusingvariousmetricsinSection5;interpretthebasisforthemodel’spredictionin
Section6;pointoutfutureworkinSection7;andconcludeinSection8.
2 RelatedWork
TheworkbyJawaharetal.identifiedfivekeycharacteristicsthatastate-of-the-artdetectorforcontent
generatedbyLLMsshouldpossess: accuracy,dataefficiency,generalizability,andinterpretability
[3];whereaccuracymeansthemodelshouldbeabletodistinguishbetweenLLM-generatedand
human-writtentextwhileachievinganappropriatetrade-offbetweenprecisionandrecallrates;data
efficiencymeansthatthedetectorshouldbeabletooperatewithasfewexamplesaspossiblefrom
thelanguagemodel; generalizabilitymeansthatthedetectorshouldbeabletoworkconsistently,
regardlessofanychangeinthemodelarchitecture,promptlength,ortrainingdataset;interpretability
meansthedetectorshouldprovideclearexplanationsforthereasoningbehinditsdecisions. These
fiveprinciplesisusedasourguidancewhendesigningtheGPT-Sentinel.
Approaches to machine-generated text detection can be divided into three categories: traditional
statistical approach (by analyzing statistical abnormality in text sample), unsupervised-learning
approach(byzero-shotclassificationofLLM),andsupervised-learningapproach(byfine-tuninga
languagemodelwithorwithoutaclassificationmoduleattached).
2.1 StatisticalMethods
Thefirstapproachtotheproblemisviatheuseofstatistics. Forinstance,theworkbySolaimanet
al. [4]demonstratedthatusingalogisticregressionmodeltodifferentiatebetweentextgenerated
byGPT-2modelsvs. textwrittenbyhumanscouldachieveanaccuracyrangingfrom88%(on124
million parameter variants of GPT-2 model) to 74% (on 1.5 billion parameter variants of GPT-2
model). Moreover, the work by Ippolito et al. [5] showed that the top-k sampling method used
inpopularLLMscouldover-samplehigh-likelihoodwords,andthusthegeneratedtextexhibited
statisticalanomalies,whichcouldbefurtherusedfordetection. Moreover,thestatisticalmethods
calledtheGiantLanguageModelTestRoom(GLTR)designedbyGehrmannetal. [6]consistsof
threetests: Tests1and2checkedifageneratedwordissampledfromthetopofthedistribution,
and Test 3 verified if the system is overly confident in its next prediction due to familiarity with
thepreviouslygeneratedcontext. Ahuman-subjectstudyfoundthatGLTRimprovedthehuman
detectionrateoffaketextfrom54%to72%withoutpriortraining.
2.2 Zero-ShotClassification
Theseconddetectionapproachisbyzero-shotclassification(i.e.,usingapre-trainedLLMtodetectits
owngenerationorthatofasimilarmodel). IntheworkbySolaimanetal. [4],abaselinemethodthat
usedanLLMtoevaluatethelog-probabilityandthecorrespondingthresholdformakingclassification
decisionswasproposed.However,thiszero-shotapproachperformspoorlycomparedtothestatistical
methods.
2.3 Fine-TuningLanguageModel
Thelastapproachistofine-tuneanexistinglanguagemodel. Forexample,Zelleretal. [7]fine-tuned
alinearlayertoidentifyiftheinputwasgeneratedbytheGROVERmodelorbyahuman,using
thehiddenstatesintheencoderofGROVER.Also,Solaimanetal. [1,4]fine-tunedapre-trained
RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art
2
Table1: DetailedstatisticsforOpenGPTTextdatasetasofApr24,2023. Thesubsetsnotlistedin
thetablewerenotparaphrasedinOpenGPTText. Thecategory“FailedtoRephrase”correspondsto
oneofthefollowingsituations: 1. thecontentlengthexceedstheAPIlimit,2. thecontentisblocked
byOpenAIcontentfilter.
Subset OpenGPTText OpenWebText FailedtoRephrase Percentage
urlsf_00 3,888 391,590 27 0.99%
urlsf_01 3,923 392,347 0 1.00%
urlsf_02 3,260 391,274 652 0.83%
urlsf_03 3,891 390,161 10 1.00%
urlsf_04 3,684 390,250 218 0.94%
urlsf_05 3,602 389,874 296 0.92%
urlsf_06 3,494 390,339 409 0.90%
urlsf_09 3,653 389,634 243 0.94%
Total 29,395 3,125,469 1,885 0.94%
performanceof90%accuracyindetectingtextgeneratedbyGPT-2.However,thesupervisedlearning
methodrequiresalargeamountoflabeleddata,incontrasttopreviouslydiscussedmethods. The
fine-tunedmodelonRoBERTabySolaimanetal. required200klabeledtrainingdata.
3 DataSetCollection
ChatGPTisalanguagemodelbasedontheGPT-3.5architecture. ItsucceededInstructGPT,which
was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November
30, 2022, thereiscurrentlynopubliclyavailabledatasetthatsystematicallycollectstheoutputs
generatedbyChatGPTasfarasweknow. Consequently,weundertookthetaskofcreatingourown
datasetforChatGPToutputs. BuildingupontheworkofGokaslanetal. [8]andtheirOpenWebText
corpus. WenamedthedatasetOpenGPTText.
3.1 OpenGPTTextOverview
The OpenGPTText data set consists of paraphrased textual samples that were generated by the
gpt-3.5-turbolanguagemodelusingtheOpenWebTextcorpusasitssource. Thedatasetcontains
29,395textualsamples,eachcorrespondingtoapiecehuman-writtentextfromtheOpenWebText
corpusthatsharesasameuniqueidentifier(UID).
UptoApril24,2023,theOpenGPTTextonlycontainsapproximately1%ofparaphrasedsamplesof
theOpenWebTextdatasetinsomespecificsubsets. Thenumberofsamplesineachsubsetislisted
intable1.
3.2 DataSource
TheOpenWebTextdataset[8]isapubliclyavailableresourcethatcompriseswebcontentsourced
fromURLssharedonRedditwithaminimumofthreevotes. Thisdatasetisareconstitutionofthe
originalWebTextcorpus,whichwasinitiallydescribedbyRadfordetal. [9]. Sincethedatasetwas
compiledin2019,itisimprobablethatthetextualcontentitcontainswasalgorithmicallygenerated.
3.3 DataCollectionMethod
TherephrasingprocedureusedOpenAI’sAPIongpt-3.5-turbomodel,withthepromptedinstruc-
tion:“Rephrase the following paragraph by paragraph”. However,itshouldbenotedthat
thesampleswithlengthlargerthan2,000wordswerefilteredoutasthegpt-3.5-turbocanonly
takeinatmost3,000tokens. SometextsamplesblockedbyOpenAIcontentfilterwasalsoexcluded
fromOpenGPTText. Thenumberoftextsthatwerenotsuccessfullyparaphrasedduetoeitherofthe
tworeasonsisreportedinthe“FailedtoRephrase”columnintable1.
3
PCA projection of hidden state of PCA projection of hidden state of
RoBERTa-Sentinel on OpenWebText before and after sanitize RoBERTa-Sentinel on OpenGPTText before and after sanitize
3 OpenWebText-Original 2.0 OpenGPTText-Original
OpenWebText-Final OpenGPTText-Final
1.5
2
1.0
1
0.5
0 0.0
0.5
1
3 2 1 0 1 2 3 4 5 4 3 2 1 0 1 2
Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left)
and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the
distributionofOpenWebTextsignificantly.
3.4 DataSetCleaning
UponinspectingtheOpenGPTTextdataset,weobservedcertainstylisticdisparitiesbetweenChat-
GPT’soutputandthecorpusinOpenWebText. Specifically,ouranalysisrevealedthatChatGPT’s
outputtendtoincludetheUnicodecharacter“rightdoublequotationmark”(U+201D)inplaceof
theASCIIcharacter“quotationmark”(U+0022)usedintheOpenWebTextcorpus. Furthermore,
ChatGPTalsotendstoincorporatetwoconsecutivenew-linecharactersbetweenparagraphs,whereas
theOpenWebTextcorpusutilizestwotosixnew-linecharactersconsecutively.
Inanefforttoenhancetheresilienceofourclassifierandeliminatethepotentialinfluenceofthese
susceptiblefeatures,weundertookmeasurestosanitizeboththeOpenWebTextandOpenGPTText
datasets. Toachievethis,weimplementedacleaningprocedurethatinvolvedremovingexcessive
new-linecharactersandmappingUnicodecharactersontotheASCIIcharacterset. Thesestepswere
taken to mitigate any possible confounding effects of these variables on the performance of our
classifier. PincipalComponentAnalysis(PCA)ofhiddenstatedistributioninfigure1showsthatthe
cleaningprocesshassignificantlychangedthedistributionofdataset.
The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the
discussionbelow.
3.5 DataSetRelease
Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May
2023.
4 Method
ThefollowingmodelsweretrainedusingtheOpenWebText-FinalandOpenGPTText-Finaldata
set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for
testing.Giventhatthetextsinthedatasethavevaryinglengths,wetruncatedinputtexttoamaximum
of512tokenstoimprovetrainingefficiency,whilealsopaddinganytextwithlessthan512tokens
withadditional<PAD>tokens. Toaddressmemoryconstraintswhileusingarelativelylargebatch
sizeduringfine-tuning,weperformedgradientaccumulation,wherebytheoptimizerwasupdated
afteracertainnumberofforwardpasses.
4.1 RoBERTa-SentinelModel
ThefirstmethodweproposedistoleveragethepretrainedRoBERTamodel[10]toextractrelevant
featuresfromtheinputtext,followedbyanMLPwithgaussianerrorlinearunits(GELU,[11])and
twofullyconnectedlayersforclassification. Topreservethegenerallinguisticknowledgeofthe
4
MLP RoBERTa-Base
𝑇[𝐶𝐿𝑆] 𝑇1 𝑇2 … 𝑇𝑁
𝑇 [( 𝐶1 𝐿2 𝑆) ] 𝑇112 𝑇 2(12) … 𝑇𝑁12
𝑃(Human) … … … …
𝑃(ChatGPT)
𝑇( 𝐶2 𝐿) 𝑆 𝑇 1(2) 𝑇22 … 𝑇𝑁2
… …
𝑇( 𝐶1 𝐿) 𝑆 𝑇 1(1) 𝑇21 … 𝑇𝑁1
768 768
𝐸[𝐶𝐿𝑆] 𝐸1 𝐸2 … 𝐸𝑁
This is … GPT
Figure2: ThedepictedfigureillustratestheRoBERTa-Sentinelarchitecture,whereinthedashedline
connectingRoBERTa-BaseandMLPmoduleindicatesthenon-propagationofgradientbacktothe
former.
Table2: TrainingconfigurationforRoBERTa-SentinelandT5-Sentinel. Where“AdamW”refersto
the“AdaptiveMomentumEstimationwithWeightDecay”optimizerproposedbyLoshchilovetal. in
[13]. “Cosineannealing”referstothelearningrateschedulproposedbyLoschilovetal. in[14]
Hyper-Parameters RoBERTa-Sentinel T5-Sentinel
Epoch 15 5
BatchSize 512 512
LearningRate 1×10−4 5×10−4
WeightDecay 1×10−3 1×10−3
Optimizer AdamW AdamW
LossFunction Crossentropy Crossentropy
Scheduler Cosineannealing Cosineannealing
DataSet OpenGPTText-Final OpenGPTText-Final
modelwhileadaptingittothespecifictaskathand,wedecidedto“freeze”theRoBERTamodel,
allowingthelosstoonlybackpropagatethroughtheMLPmodule.
LetE representtheinputembedding,V representthevocabularysize,andH denotethedimension
ofthelasthiddenstateinRoBERTa. TheinputtextwithlengthN canbeexpressedasasequence
ofembedding,E ,E ,···,E ,whereE ,E ∈RV. Here,E denotestheembedding
[CLS] 1 N [CLS] i [CLS]
ofthespecial[CLS]token,asdescriedintheoriginalBERTimplementation[12]. Weusethefinal
hiddenstatevectorT ∈RH thatcorrespondstothefirst[CLS]tokenasthefeaturesoftheinput
[CLS]
text. ThisextractedfeaturevectorC isthenforwardtotheMLPforclassification,asshowninfigure
2.
ThedetailedtrainingconfigurationforRoBERTa-Sentinelcanbefoundintable2.
4.2 T5-SentinelModel
Thesecondmethodweproposedinvolvesfine-tuningtheT5model[15]forclassificationtasks.Unlike
theRoBERTa-SentinelwhichusesanMLPmoduletoclassifythehiddenstatevectorofinput,this
approachdirectlyencodesthetaskasasequence-to-sequence(seq-to-seq)problem.
Duringthetraining,theinputsequenceconsistsofatextsamplefromOpenGPTText-Final,and
theoutputsequencerepresentstheclassificationresultaseither“positive</s>”or“negative</s>”,
where“</s>”istheend-of-sequencetoken. Duringtheinference,welimitthevocabularydownto
onlytwowords(i.e. “positive”and“negative”),andselecttheonewiththemaximumprobabilityas
theclassificationresult. ThisprocessisfurthershowninFigure3.
5
𝑃ChatGPT = ,𝑃Human =
“Positive” “Negative”
Next-word
Probability Distribution ⋯
T5 Decoder Block
×6
Feed Forward
T5 Encoder Block Masked Multi-head Attention
×6
Feed-forward MLP
⋯
Self-Attention Masked Multi-head Attention
𝐸1 𝐸2 ⋯ 𝐸𝑁 𝐸<𝑠> 𝐸<𝑃𝐴𝐷>
Figure3: ArchitectureforT5-Sentinel. Afterinputtheentiretokensequence,weprovidetheT5-
Decoder with a <PAD> token and predict if the input text is by human or generated based on the
probabilityofspecificword“Positive”and“Negative”intheoutputwordprobabilitydistribution.
ThedetailedtrainingconfigurationofT5-Sentinelcanbefoundintable2.
5 Evaluation
5.1 EvaluationMetric
In our study, we assess the performance of the RoBERTa-Sentinel and T5-Sentinel through the
applicationoffivedistinctevaluationmetrics,namelytheF1score,receiveroperatingcharacteristic
(ROC)curve,detectionerrortrade-off(DET),areaundercurve(AUC),andmodelconfidencescore.
Inthispaper,theterm“positive”referstotheinputtextisChatGPT-generated,while“negative”,
meansthatthedataiswrittenbyhuman.
Giventhetruepositive(TP),truenegative(TN),falsepositive(FP)andfalsenegative(FN)count,
wecancalculatethemetricsasfollowing:
TP
F1Score=
TP + 1(FP +FN)
2
TP FP TN FN
TPR= FPR= TNR= FNR=
TP +FN FP +TN TN +FP FN +TP
5.2 F1Score,FalsePositiveRateandFalseNegativeRate
TheF1score, falsepositiverateandfalsenegativerateofRoBERTa-SentinelandT5-Sentinelis
evaluated on original data set (OpenGPTText), cleaned data set (OpenGPTText-Final), and the
GPT2-Output1dataset[16]. Theevaluationresultswhentaking0.5asthethresholdprobabilityfor
positiveareshowninthetable3. Formoredetaileddataonevaluationresult,weincludedthetrue
positiverate,truenegativerateandsamplecountforeachmetricintable6inappendixB.
AnimportantobservationisthateventhoughT5-SentinelandRoBERTa-Sentinelmodelsexhibit
highaccuracyintheOpenGPTTextdataset,bothpriortoandpostcleaning,theydonotperform
aseffectivelyontheGPT2-Outputdataset,displayinganexceptionallyhighFNR.Thisdisparity
maybeattributedtothedistinctivequalityoftextgeneratedbyGPT2andChatGPTmodels,aswell
asthedissimilarnatureofthesamplesintheOpenGPTTextdataset,whichareallrephrasedfrom
1ThereexistmultiplevariantsofGPT2-Outputdataset,unlessexplicitlystatedotherwise,inthispaperwe
refertotheGPT2-OutputdatasetwithGPT2ExtraLarge(1542Mparameter)withpuresamplingmethod.
6
Table3: TheevaluationresultforT5-Sentinel,RoBERTa-Sentinel,ZeroGPT[17],OpenAI-Detector
[18],andGPT-2DetectorfromSolaimanetal. [4]onthreedatasetsunderthresholdprobabilityof
0.5. F1standsfor“F1-score”. FPRandFNRdataareinpercentage.
Model OpenGPTText-Final OpenGPTText GPT2-Output
F1 FPR FNR F1 FPR FNR F1 FPR FNR
T5 0.98 2.8 1.3 0.98 3.5 1.3 0.06 5.9 96.7
RoBERTa 0.94 9.0 3.2 0.89 21.6 1.9 0.16 17.2 89.6
ZeroGPT 0.43 26.3 65.0 0.40 16.5 71.3 0.14 23.4 90.5
OpenAI-Detector 0.32 4.9 79.8 0.26 1.6 85.2 0.66 13.6 44.0
GPT2 0.23 2.8 86.8 0.22 4.1 87.2 0.93 6.4 7.4
Table4: AUCValueforeachcombinationofdatasetandmodel
Model OpenGPTText-Final OpenGPTText GPT2-Output
T5-Sentinel 0.993 0.992 0.463
RoBERTa-Sentinel 0.986 0.976 0.423
ZeroGPT 0.526 0.555 0.413
OpenAI-Detector 0.765 0.752 0.770
GPT2-Detector 0.610 0.600 0.976
human-writtenarticles,incontrasttotheGPT2-Outputdatasetthatcontainsrandomlygenerated
text.
Likewise,itisworthnotingthatthebaselinemodel,GPT2-DetectorbySolaimanetal.,didnotsucceed
intransferringitslearnedexperiencefromtheGPT2outputdetectiontasktothetaskofdetecting
ChatGPTgeneratedtext,despitethefindingspresentedin[4],whichindicatethatGPT2-Detectoris
capableofdetectingdiversevariantsoftheGPT2model.
5.3 ROC/DETCurveandAUC
The ROC curve is a common graph used to evaluate and compare classifiers and can explicitly
visualizethesensitivity/specificitytrade-offofclassifierforallthresholds[19]. TheROCcurveof
T5-Sentinel,RoBERTa-SentinelandGPT2-DetectoronOpenGPTText,OpenGPTText-Finaland
GPT2-Outputareshowninfigure4separately. UponanalyzingtheROCcurvesforthesamemodel
acrossdifferentdatasets,asillustratedinFigure5,weobservethattheT5-Sentineldemonstrates
greaterrobustnessascomparedtoRoBERTa-Sentinel.
Theareaundercurve(AUC)isasingle-numbersummaryfortheROCcurve. TheAUCresultfor
eachcombinationofmodelanddatasetislistedintable4.
Wealsoplotthedetectorerrortrade-off(DET)curvesacrossdifferentmodels(figure6)andacross
differentdatasets(figure7).
ROC Curves of T5-Sentinel, RoBERTa-Sentinel, ZeroGPT ROC Curves of T5-Sentinel, RoBERTa-Sentinel, ZeroGPT ROC Curves of T5-Sentinel, RoBERTa-Sentinel, ZeroGPT
OpenAI-Detector and GPT2-Detector on OpenGPTText-Final OpenAI-Detector and GPT2-Detector on OpenGPTText OpenAI-Detector and GPT2-Detector on GPT2-Output
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 T R5 o- BS Ee Rn Tt ain -Se el ntinel 0.2 T R5 o- BS Ee Rn Tt ain -Se el ntinel 0.2 T R5 o- BS Ee Rn Tt ain -Se el ntinel
ZeroGPT ZeroGPT ZeroGPT
OpenAI-Detector OpenAI-Detector OpenAI-Detector
0.0 GPT2-Detector 0.0 GPT2-Detector 0.0 GPT2-Detector
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate False Positive Rate
Figure 4: ROC Curves for models across different data sets. OpenGPTText-Final (Left),
OpenGPTText(Middle),andGPT2-Output(Right)
7
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT
ROC Curves of T5-Sentinel Across Three Data Sets ROC Curves of RoBERTa-Sentinel Across Three Data Sets
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
OpenGPTText-Final OpenGPTText-Final
OpenGPTText OpenGPTText
0.0 GPT2-Output 0.0 GPT2-Output
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate
Figure5: ROCCurvesforsamemodelunderdifferentdatasetsT5-Sentinel(Left)andRoBERTa-
Sentinel(Right). NotethattheperformanceofRoBERTa-Sentinelsignificantlydeteriorateswhen
transfertooriginalversionofOpenGPTTextwhileT5-Sentineldoesnot.
DET Curves of T5-Sentinel, RoBERTa-Sentinel, ZeroGPT DET Curves of T5-Sentinel, RoBERTa-Sentinel, ZeroGPT DET Curves of T5-Sentinel, RoBERTa-Sentinel, ZeroGPT
OpenAI-Detector GPT2-Detector on OpenGPTText-Final OpenAI-Detector and GPT2-Detector on OpenGPTText OpenAI-Detector and GPT2-Detector on GPT2-Output
100 100 100
101 101 101
102 102
102
T5-Sentinel T5-Sentinel T5-Sentinel
103 R Zo eB roE GR PTa T-Sentinel 103 R Zo eB roE GR PTa T-Sentinel R Zo eB roE GR PTa T-Sentinel
OpenAI-Detector OpenAI-Detector OpenAI-Detector
GPT2-Detector GPT2-Detector GPT2-Detector
103 102 101 100 103 102 101 100 103 103 102 101 100
False Positive Rate False Positive Rate False Positive Rate
Figure6: DETCurvesofdifferentmodelsunderOpenGPTText-Final(Left),OpenGPTText(Mid-
dle)andGPT2-Output(Right)underlogarithmicaxis.
5.4 ConfidenceScore
Assessing the reliability and confidence of a machine learning model’s predictions is crucial for
evaluatingitsperformance. Tothisend,wecalculatedconfidencescoresforeachcombinationof
datasetsandmodels,andplottheminfigure8. Theresultingconfidencescoresrangefrom0to1
andprovideameasureofthemodel’scertaintyaboutitspredictions.
Inouranalysis,weinvestigatedthedistributionofconfidencescoresandtheircorrespondencewith
accuracy.OurresultindicatesthattheT5-Sentinelmodelachievedhigherconfidencescorescompared
toRoBERTa-Sentinel. Incontrast,theRoBERTa-Sentinelmodelhadlowerconfidencescoresthan
T5-Sentinelandshowedgreaterconfidencewhendetectingtextgeneratedbyhumanthanthatby
Chat-GPT.
Overall,ourfindingssuggestthattheT5-SentinelmodelismorereliableanddecisivethanRoBERTa-
Sentinel, particularly when dealing with OpenGPTText. Further investigation is needed to fully
understandthereasonsforthesedifferencesandtooptimizetheperformanceofthesemodelsfor
specificapplications.
DET Curves of T5-Sentinel Across Three Data Sets DET Curves of RoBERTa-Sentinel Across Three Data Sets
100 100
101 101
102
102
OpenGPTText-Final 103 OpenGPTText-Final
OpenGPTText OpenGPTText
GPT2-Output GPT2-Output
103
103 102 101 100 103 102 101 100
False Positive Rate False Positive Rate
Figure7: DETCurvesofT5-Sentinel(Left)andRoBERTa-Sentinel(Right)ondifferentdatasets.
8
etaR
evitageN
eslaF
etaR
evitisoP
eurT
etaR
evitageN
eslaF
etaR
evitageN
eslaF
etaR
evitisoP
eurT
etaR
evitageN
eslaF
etaR
evitageN
eslaF
Confidence level of T5-Sentinel on Confidence level of RoBERTa-Sentinel on
OpenGPTText-Final, OpenGPTText data sets OpenGPTText-Final, OpenGPTText data sets
3000
100 100
2000
2500
80 80
2000 1500
60 60
1500
1000
40 40
1000
500 20 500 20
OpenGPTText-Final OpenGPTText-Final
OpenGPTText OpenGPTText
0 0 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Figure8: ConfidenceScoresforT5-SentinelandRoBERTa-SentinelonOpenGPTText-Finaland
OpenGPTText data set. The histogram represents the number of sample under certain range of
probabilityforthesampletobepositive.
PCA projection of decoder hidden state PCA projection of hidden state of
T5-Sentinel on OpenGPTText-Final RoBERTa-Sentinel on OpenGPTText-Final
2.0
OpenWebText OpenWebText
6 OpenGPTText 1.5 OpenGPTText
4 1.0
2 0.5
0.0
0
0.5
2
1.0
4
1.5
6
2.0
6 4 2 0 2 4 6 8 10 4 3 2 1 0 1 2 3 4 5
Figure9: PCAprojectionofhiddenstatesforT5-Sentinel(Left)andRoBERTa-Sentinel(Right)
6 InterpretabilityStudy
6.1 PrincipalComponentAnalysisonHiddenState
Tooffergreaterinsightintothefunctioningofthemodelsweproposed,T5-SentinelandRoBERTa-
Sentinel,weconductedaPCAontheirrespectivehiddenstates.
ForRoBERTa-Sentinel,weextractedthehiddenstatefromlastlayerofattachedMLPandtheoutput
of last decoder block for T5-Sentinel and recorded their values with input of all data in test set
sampledfromOpenGPTText-Final. Asshowninfigure9,bothmodelssuccessfullymappedthe
inputtextintotwodifferentclustersinhiddenspace,indicatingthatbothmodelswereabletoextract
implicitcharacteristicsofChatGPTrephrasedtext.
Toinvestigatethepropertiesofthedataalongeachdirectionoftheprojectionsubspace,weconducted
asamplingofthedatapointoutliersinthePCAprojectionsubspace.
Figure 10 displays the position of four samples in the PCA projection subspace. Upon manual
inspectionofthesesamples,wediscoveredthatSample 1wasabriefcaradvertisementthatutilized
simplelanguageandcomprisedofveryshortparagraphs(splitbyimagesintheoriginalwebpage).
Sample 2wasasportnewsarticlewithlengthyparagraphs,whileSample 3constitutedasequence
ofdevelopingtoolnamesthatlackedanyactualmeaning. Sample 4wasabriefreportonchildren’s
attitudestowardsclowns. Theseobservationssuggestthatourmodelmayhavelearnedtodistinguish
the length of paragraphs and potentially discern whether a given text sample is informative and
meaningful. Wehaveprovideddetailedtextualsamplesalongwiththeiruniqueidentifiers(UIDs)
fromtheOpenGPTTextdatasetinAppendixB.
9
tnuoC
ataD
)%(
level
ecnedifnoC
tnuoC
ataD
)%(
level
ecnedifnoC
Outliers in Hidden Feature Space after PCA
RoBERTa-Sentinel on OpenGPTText-Final Dataset
3
OpenWebText Sample 2
OpenGPTText Sample 3
Sample 1 Sample 4
2
1
0
1
2
4 2 0 2 4 6
Figure10: OutliersinPCAprojectionspacedrawnformanualinspection.
6.2 IntegratedGradient
Weutilizedtheintegratedgradientanalysistechnique,asproposedbySundararajanetal. [20],togain
insightsintothecontributionofindividualtokensinagiveninputtexttowardstheoverall“GPT-ness”
ofthetext. Ourapproachinvolvedinitiallypassingtheinputtextthroughthemodelandcomputing
thelossfunctionundertheassumptionthatthetextlabelwas“human”. Followingthis,weexecuted
back-propagationtoobtainthegradientsassociatedwitheachinputtoken. Thisstatementconformed
totheformalstyleandtechnicallanguagetypicallyemployedinacademicwriting.
Therationaleunderlyingourmethodisrootedintheobservationthattokenswithgradientscloseto
zerotendtoalignwellwiththehumanlabel,therebynecessitatingminimalmodification. Conversely,
tokenswithlargegradientsareindicativeofamisalignmentwiththehumanlabel,suggestingahigher
degreeofresemblancetoGPT-likecharacteristics.
Wefurtherdevelopedavisualizationtoolthatshowsthecontributionofeachinputtokentotheoverall
GPT-nessofthetext. Thedarkerthebackgroundtokenis,themoreGPT-likethattokenis.
Belowweshowasample2visualizationresultdrawnfromthetestsetofOpenGPTText-Finaldataset
beforeandaftertherephrasing.
OriginalText: Predictashumanwithprobabilityof0.998,withconfidenceof0.994
Applestartedan avalancheofactivitywiththeintroductionoftheiPad.Companies shifted
gearstogoafterthisundiscoverednewtabletmarket.Inspiteofthenumberofplayersin
tablets,nocompanyhasdiscoveredthemagicbullettoknocktheiPadoffthetopofthetablet
heap.ColleagueAdrianKingsley-Hugheshas athoughtfulpiece blamingAmazon
andGoogleforkillingthetabletmarket.Hisreasoningisthatby releasingthe KindleFire
andtheNexus7at$199,AmazonandGooglehavestarted a"racetothebottom"ofthe
tabletmarketthatwillensurenoprofitabilityforanyone.Adrian’sreasoningissolid,but
itoverlooksonethingIhavesaidfor alongtime.Thereisnoproventabletmarket. (...
Truncated)
RephrasedText: Predictasgeneratedwithprobabilityof1.000,withconfidenceof0.985
2Withdatauniqueidentifier(UID):[urlsf_subset00]-[309279]
10
t-SNE Plot on Hidden State of T5-Sentinel t-SNE Plot on Hidden State of RoBERTa-Sentinel
on OpenGPTText-Final on OpenGPTText-Final
40 OpenWebText
40 OpenGPTText
20
20
0 0
20 20
40 OpenWebText
OpenGPTText 40
60 40 20 0 20 40 60 80 60 40 20 0 20 40 60 80
Figure11: HiddenstatesofT5-Sentinel(Left)andRoBERTa-Sentinel(Right)onurlsf-04subset
ofOpenGPTText-Finalaftert-SNEdimensionalityreduction.
FollowingthereleaseoftheiPad,thetabletmarketbecame apopularareaforcompaniesto
explore. Despitemanycompaniesenteringthemarket,noonehasmanagedtooutperform
theiPadonsales.Thereis abeliefthatAmazonandGooglehavecausedthis,duetothe
releaseofthe KindleFireandNexus7at$199,starting a"racetothebottom."Thereis
aconcernthatthiswillensuretherewillbenoprofitabilityforanyone.However,thisbelief
overlooksthefactthatthereisnoprovenmarketfortablets,only aprovenmarketforiPads
.OtherthanApple,Samsungistheonlycompanywithnotabletabletsales.Eventhough
theyreleasedseveraltablets,invariousshapesandsizes,theystillcouldnotcompetewith
theiPad. (...Truncated)
6.3 t-distributedStochasticNeighborEmbeddingVisualization
t-distributedStochasticNeighborEmbedding(t-SNE)projectionproposedbyMaatanetal. [21]is
appliedonthehiddenstatevectorofbothT5-SentinelandRoBERTa-Sentinel. Resultsinfigure11
indicatethatT5-Sentinelmodelcanbetterseparatethedataset,whichalignswiththeperformanceof
bothmodelsontestdataset.
7 FutureWork
Althoughourcurrentmodelhasshownpromisingresults,therearecertainlimitationsinourcurrent
model. Firstandforemost,bothT5-SentinelandRoBERTa-SentinelaretrainedwithEnglishcorpus
only. As a result, their performance on other languages such as Spanish or Chinese may not be
optimal. To address this limitation, fine-tuning the models with non-English text can be helpful.
However,it’sworthnotingthatthepretrainedversionoftheT5modelonlysupportsEnglish,French,
Romanian,andGerman. Therefore,classificationtasksinvolvinglanguagesotherthanthesemay
requiremorethanjustfine-tuningalone.
Inaddition,theOpenGPTText-Finaldatasetiscollectedwiththeprompt“Rephrase the fol-
lowing paragraph by paragraph”, so the model trained on such data set might not perform
welltootherlanguagetasksthatChatGPTispopularlyusedon,suchasquestionansweringortext
generation. Inthefuture,weplantocollectdatasetsinvolvingadifferenttextualcontext,likeeli5
[22]andSQuAD[23],tofurtherassesstheaccuracyoftheRoBERTa-SentinelandT5-Sentinelon
differenttasks.
8 Conclusion
In conclusion, we have introduced a high-quality data set called OpenGPTText, which we have
rephrasedusingtheChatGPTmodel. Additionally, wehavedesigned, implemented, andtrained
11
two text classification models using RoBERTa and T5 architectures. Our models have achieved
remarkableresults, withaccuracyexceeding97%onthetestdataset, asevaluatedusingvarious
metrics.
Moreover,wehaveconductedaninterpretabilitystudytodemonstrateourmodels’abilitytoextract
and differentiate key features between human-written and ChatGPT-generated text. The study’s
resultsshowthatourmodelsareeffectiveinidentifyingthedifferencesbetweenthetwotypesoftext,
providinginsightintothestrengthsandlimitationsofthemodelsanddemonstratingtheirpotential
forreal-worldapplications.
9 Acknowledgement
WewouldliketoexpressoursincereappreciationtoProfessorBhikshaRajandourTAmentorLiangze
(Josh)Lifortheirinvaluableguidance,insightfulcomments,andconstantsupportthroughoutthe
courseofthisresearch. Theirexpertiseinthefieldhavebeeninstrumentalinshapingourwork.
References
[1] GaneshJawahar,MuhammadAbdul-Mageed,andLaksV.S.Lakshmanan. Automaticdetection
ofmachinegeneratedtext: Acriticalsurvey. CoRR,abs/2011.01314,2020.
[2] LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,
FraserKelton,LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulChristiano,
Jan Leike, and Ryan Lowe. Training language models to follow instructions with human
feedback,2022.
[3] GaneshJawahar,MuhammadAbdul-Mageed,andLaksV.S.Lakshmanan. Automaticdetection
ofmachinegeneratedtext: Acriticalsurvey. CoRR,abs/2011.01314,2020.
[4] IreneSolaiman,MilesBrundage,JackClark,AmandaAskell,ArielHerbert-Voss,JeffWu,Alec
Radford,GretchenKrueger,JongWookKim,SarahKreps,MilesMcCain,AlexNewhouse,
JasonBlazakis,KrisMcGuffie,andJasmineWang. Releasestrategiesandthesocialimpactsof
languagemodels,2019.
[5] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic
detection of generated text is easiest when humans are fooled. In Annual Meeting of the
AssociationforComputationalLinguistics,2019.
[6] SebastianGehrmann,HendrikStrobelt,andAlexanderM.Rush. GLTR:statisticaldetection
andvisualizationofgeneratedtext. CoRR,abs/1906.04043,2019.
[7] RowanZellers,AriHoltzman,HannahRashkin,YonatanBisk,AliFarhadi,FranziskaRoes-
ner, and Yejin Choi. Defending against neural fake news. In H. Wallach, H. Larochelle,
A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett,editors,AdvancesinNeuralInforma-
tionProcessingSystems,volume32.CurranAssociates,Inc.,2019.
[8] AaronGokaslanandVanyaCohen. Openwebtextcorpus. http://Skylion007.github.io/
OpenWebTextCorpus,2019.
[9] AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever.Language
modelsareunsupervisedmultitasklearners. InNeurIPS,2019.
[10] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,Mike
Lewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretraining
approach,2019.
[11] DanHendrycksandKevinGimpel. Bridgingnonlinearitiesandstochasticregularizerswith
gaussianerrorlinearunits. ArXiv,abs/1606.08415,2016.
[12] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. CoRR,abs/1810.04805,2018.
[13] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR,
abs/1711.05101,2017.
12
[14] IlyaLoshchilovandFrankHutter. SGDR:stochasticgradientdescentwithrestarts. CoRR,
abs/1608.03983,2016.
[15] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. CoRR,abs/1910.10683,2019.
[16] OpenAI. Gpt2-output. https://github.com/openai/gpt-2-output-dataset,2019.
[17] ZeroGPT. AIDetector. https://www.zerogpt.com,January2023.
[18] OpenAI. https://beta.openai.com/ai-text-classifier,January2023.
[19] FranciscoMelo,WernerDubitzky,OlafWolkenhauer,Kwang-HyunCho,andHirokiYokota.
ReceiverOperatingCharacteristic(ROC)Curve,pages1818–1823. SpringerNewYork,New
York,NY,2013.
[20] MukundSundararajan,AnkurTaly,andQiqiYan. Axiomaticattributionfordeepnetworks. In
InternationalConferenceonMachineLearning,2017.
[21] LaurensvanderMaatenandGeoffreyHinton.Visualizingdatausingt-SNE.JournalofMachine
LearningResearch,9:2579–2605,2008.
[22] AngelaFan, YacineJernite, EthanPerez, DavidGrangier, JasonWeston, andMichaelAuli.
ELI5: longformquestionanswering. InAnnaKorhonen,DavidR.Traum,andLluísMàrquez,
editors,Proceedingsofthe57thConferenceoftheAssociationforComputationalLinguistics,
ACL2019,Florence,Italy,July28-August2,2019,Volume1: LongPapers,pages3558–3567.
AssociationforComputationalLinguistics,2019.
[23] PranavRajpurkar,JianZhang,KonstantinLopyrev,andPercyLiang. SQuAD:100,000+ques-
tionsformachinecomprehensionoftext. InProceedingsofthe2016ConferenceonEmpirical
MethodsinNaturalLanguageProcessing,pages2383–2392,Austin,Texas,November2016.
AssociationforComputationalLinguistics.
13
Table5: BaselineevaluationonGPT2-outputdataset
Small Medium Large ExtraLarge
Metrics top-k pure top-k pure top-k pure top-k pure
Accuracy 0.9648 0.9623 0.9627 0.9567 0.9616 0.9449 0.9498 0.9310
FalsePositive 0.0070 0.0122 0.0114 0.0238 0.0137 0.0472 0.0376 0.0734
FalseNegative 0.0319 0.0319 0.0319 0.0319 0.0319 0.0319 0.0319 0.0319
Figure 12: Confusion matrices of baseline model on GPT2-output data set under pure sampling
method
A GPT2-DetectorBaselineAnalysis
A.1 EvaluationonGPT2-Baseline
We reproduced the GPT2-Detector proposed by Solaiman et al. [4] as the baseline model and
performedevaluationonGPT2outputdatasetreleasedbyOpenAI[16]. Theresultsareshownbelow.
TheGPT2-outputdatasetcontainsrandomoutputfromfourvariantsofGPT2model: small(117M
parameter),medium(354Mparameter),large(762Mparameter)andextra-large(1542Mparameter)
andwehaveevaluateourbaselinemodelonallofthem. Theresultisshownintable5.
Theconfusionmatricesforeveryvariant(small, medium, large, extra-large)andeverysampling
method(top-kandpure)isshowninfigure12andfigure13.
A.2 TrendBetweenLanguageModelScaleandClassificationAccuracy
When running baseline test of GPT2-Detector, we noticed that there is an approximately linear
relationshipbetweenclassificationaccuracyandthescaleoflanguagemodel,asillustratdin14.
A.3 PCAAnalysisforBaselineModel
Despitethebaselinemodel’sstrongperformanceindetectingGPT-2generatedcontent,itencounters
significantchallengeswhentaskedwithdetectingGPT-3.5generatedcontent. Infact,withoutany
additional training, the baseline model achieved a mere 54.98% accuracy on the OpenGPTText
dataset,onlyslightlybetterthanrandomchance. Thissignificantdropinaccuracyhighlightsthe
challengesofdifferentiatingbetweenhuman-generatedandGPT-3.5generatedcontent,likelydueto
theincreasedcomplexityoftheGPT-3.5model.
14
Figure13: ConfusionmatricesofbaselinemodelonGPT2-outputdatasetundertop-k sampling
(k =40)
Accuracy v.s. #Parameters in GPT2 variants False positive rate v.s. #Parameters in GPT2 variants
1.00 0.25
0.965 0.963 0.962 0.95 R Toa pn -d ko Sm a mSa pm linp gli n (kg =40)
0.95 0.962 0.957 0.20
0.945
0.931
0.90 0.15
0.85 0.10 0.074
0.046
0.80 0.05
Random Sampling 0.012 0.023 0.037
Top-k Sampling (k=40)
0.75 0.00 0.007 0.011 0.013
200 400 600 800 1000 1200 1400 1600 200 400 600 800 1000 1200 1400 1600
#Parameter in GPT2 variant (Million) #Parameter in GPT2 variant (Million)
Figure14: Asthenumberofparameterinlanguagemodelincrease,thedetector’saccuracydecreases
linearly,whilethefalsepositiverateincreases.
Figure15: PCAProjection: GPT-2vs. GPT-3.5Turbo
15
ycaruccA
etaR
evitageN
eslaF
Another notable observation is the difference in PCA projections between GPT-2 and GPT-3.5
generated content. The PCA projection for GPT-2 indicates that human-generated and GPT-2
generatedcontentareclearlydistinguishablefromeachother. However,thesamedistinctionisnotas
clearintheGPT-3.5projection,asshowninfigure15.
16
Table6: EvaluationResultonOpenGPTText-Final(Row1-3),OpenGPTText-Original(Row4-7),
andGPT2-Output(Row8-11). TPRstandsfor“TruePositiveRate”,TPCstandsfor“TruePositive
Count”,TNRstandsfor“TrueNegativeRate”,TNCstandsfor“TrueNegativeCount”.
Model Accuracy TPR,(TPC) TNR,(TNC) FPR,(FPC) FNR,(FNC)
T5 97.98% 98.71%,(2906) 97.25%,(2863) 2.75%,(81) 1.29%,(38)
RoBERTa 93.92% 96.81%,(2850) 91.03%,(2680) 8.97%,(264) 3.19%,(94)
OpenAI 57.68% 20.24%,(596) 95.11%,(2800) 4.89%,(144) 79.76%,(2348)
ZeroGPT 54.36% 34.99%,(1030) 73.74%,(2171) 26.26%,(773) 65.01%,(1914)
GPT2 38.5% 13.21%,(389) 97.16%,(1233) 2.84%,(36) 86.79%,(2555)
T5 97.64% 98.74%,(2907) 96.54%,(2842) 3.46%,(102) 1.26%,(37)
RoBERTa 88.28% 98.13%,(2889) 78.43%,(2309) 21.57%,(635) 1.87%,(55)
OpenAI 56.64% 14.84%,(437) 98.44%,(2898) 1.56%,(46) 85.16%,(2507)
ZeroGPT 56.1% 28.67%,(844) 83.53%,(2459) 16.47%,(485) 71.33%,(2100)
GPT2 37.86% 12.84%,(378) 95.9%,(1217) 4.1%,(52) 87.16%,(2566)
T5 48.68% 3.3%,(165) 94.06%,(4703) 5.94%,(297) 96.7%,(4835)
RoBERTa 46.56% 10.36%,(518) 82.76%,(4138) 17.24%,(862) 89.64%,(4482)
OpenAI 71.22% 56.02%,(2801) 86.42%,(4321) 13.58%,(679) 43.98%,(2199)
ZeroGPT3 43.09% 9.52%,(476) 76.64%,(3832) 23.36%,(1168) 90.48%,(4522)
GPT2 93.1% 92.58%,(4629) 93.62%,(4681) 6.38%,(319) 7.42%,(371)
B DetailedInformationforEvaluation
B.1 EvaluationResult
Asshownintable5. Themetricsarecalculatedunderthepositiveprobabilitythresholdof0.5.
B.2 ContentSample
B.2.1 Sample1-[urlsf_subset04]-[236996]-web
Lexus IS 250 is the entry model among IS sedans. Pictured is the
optional F Sport package that firms already stiff suspension and
adds some distinguishing visuals. (Photo11: Lexus)
Lexus gave us enough go-fast imagery in its Super Bowl ads last Sunday
that it almost seemed to be saying, "See, see, are too sporty."
And, yes, some malingerers still might be insisting, "Are not," and
need nudging.
After all, Lexus’ reputation for years has been luxury at the expense
of handling and performance. But the brand is trying to transform so
that you’ll compare it with BMW instead of Buick.
The ES, at the lower end, and the LS, at the top, still could be
considered luxo-machines more than yippee-mobiles. But most Lexus
models we’ve driven lately behave in sporty fashion when prodded
that direction by the driver.
(...Truncated)
B.2.2 Sample2-[urlsf_subset04]-[246672]-gpt
After succumbing to a hip injury, Nick Kyrgios has come to the
decision to take rest and heal. This has led him to reflect on 2017,
with its highs and lows, from consecutive wins against great tennis
players to the difficulties he faced at Grand Slams. However, his
fondest memories were from team events, particularly the Laver Cup
and the Davis Cup. Kyrgios reveals that tennis can be a lonely sport
, and he often struggles with it. However, he praises the team
3ZeroGPTfailedtoprocesstwodataentries(withID:255332and258673inxl-1542M.test.jsonl)in
theGPT2-Outputdataset,thosetwoentriesarenotcountedinthecalculationofmetrics.
17
spirit in Davis Cup, highlighting how they all support each other,
win or lose, and says it made him feel like he was part of the team.
Rusty, Davis Cup captain, created a WhatsApp group a year and a half
ago, which includes all the Davis Cup players. Kyrgios recounts how,
after his semi-final win in Beijing, his phone was flooded with
loyal messages from the coaches and players. This sparked his
realization that it had become bigger than tennis, and they had
become a family trying to keep in touch no matter how far apart they
all were. He feels it has helped provide a support system for
everyone, both on and off the court, at a time when they really
needed it the most.
(... Truncated)
B.2.3 Sample3-[urlsf_subset04]-[230559]-web
Add DevOps Tools to your Pipeline
Densify XL Impact Bitbucket Bitbucket Server Bower Crucible Deveo
Fisheye Gerrit Git GitHub GitLab Gogs Helix ISPW Kallithea Mercurial
Micro Focus AccuRev Micro Focus StarTeam Perforce HelixCore
Rational Clearcase Rational Team Concert Subversion Team Foundation
Server Team Foundation Version Control
(... Truncated)
Kubernetes Engine Kubernetes Linux Containers Marathon Mesos
Mesosphere DC/OS Nomad OpenVZ Portainer Rancher Solaris Containers
Supergiant Swarm Sysdig Tectonic Weaveworks rkt OpsGenie DBmaestro
Datical Delphix Flyway Idera Liquibase Quest Toad Redgate Redgate
SQL Toolbelt
Add tools from the Periodic Table of DevOps or select from the full
list above.
Click "Visualize My Pipeline!" to view your pipeline in the DevOps
Diagram Generator.
B.2.4 Sample4-[urlsf_subset04]-[313139]-web
A carnival reveller dressed as a clown celebrates on the street in
Berlin February 18, 2007. REUTERS/Pawel Kopczynski
LONDON (Reuters) - Bad news for Coco and Blinko -- children don’t like
clowns and even older kids are scared of them.
The news that will no doubt have clowns shedding tears was revealed in
a poll of youngsters by researchers from the University of
Sheffield who were examining how to improve the decor of hospital
children’s wards.
The study, reported in the Nursing Standard magazine, found all the
250 patients aged between four and 16 they quizzed disliked the use
of clowns, with even the older ones finding them scary.
"As adults we make assumptions about what works for children," said
Penny Curtis, a senior lecturer in research at the university.
"We found that clowns are universally disliked by children. Some found
them quite frightening and unknowable."
(End of File)
18

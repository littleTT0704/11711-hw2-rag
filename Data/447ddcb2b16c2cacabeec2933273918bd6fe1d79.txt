JournalofMachineLearningResearch24(2023)1-93 Submitted1/23;Published10/23
MAUVE Scores for Generative Models: Theory and Practice
Krishna Pillutla1∗ pillutla@cs.washington.edu
Lang Liu2∗ liu16@uw.edu
John Thickstun3 jthickstun@stanford.edu
Sean Welleck1,4 wellecks@cs.washington.edu
Swabha Swayamdipta5 swabhas@usc.edu
Rowan Zellers6 rowanz@cs.washington.edu
Sewoong Oh1,7 sewoong@cs.washington.edu
Yejin Choi4,7 yejin@cs.washington.edu
Zaid Harchaoui2 zaid@uw.edu
1Google Research
2Department of Statistics, University of Washington
3Department of Computer Science, Stanford University
4Allen Institute for Artificial Intelligence
5Viterbi School of Engineering, University of Southern California
6OpenAI
7Paul G. Allen School of Computer Science and Engineering, University of Washington
Editor: Kilian Weinberger
Abstract
Generative artificial intelligence has made significant strides, producing text indistinguish-
able from human prose and remarkably photorealistic images. Automatically measuring
howclosethegenerateddatadistributionistothetargetdistributioniscentraltodiagnos-
ingexistingmodelsanddevelopingbetterones. WepresentMAUVE,afamilyofcomparison
measuresbetweenpairsofdistributionssuchasthoseencounteredinthegenerativemodel-
ingoftextorimages. Thesescoresarestatisticalsummariesofdivergencefrontierscaptur-
ingtwotypesoferrorsingenerativemodeling. Weexplorethreeapproachestostatistically
estimate these scores: vector quantization, non-parametric estimation, and classifier-based
estimation. We provide statistical bounds for the vector quantization approach.
Empirically, we find that the proposed scores paired with a range of f-divergences and
statistical estimation methods can quantify the gaps between the distributions of human-
written text and those of modern neural language models by correlating with human judg-
ments and identifying known properties of the generated texts. We demonstrate in the
vision domain that MAUVE can identify known properties of generated images on par with
or better than existing metrics. In conclusion, we present practical recommendations for
using MAUVE effectively with language and image modalities.
. ∗These authors contributed equally to this work.
©2023KrishnaPillutla,LangLiu,JohnThickstun,SeanWelleck,SwabhaSwayamdipta,RowanZellers,Sewoong
Oh,YejinChoi,ZaidHarchaoui.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/. Attributionrequirementsare
providedathttp://jmlr.org/papers/v24/23-0023.html.
3202
ceD
7
]GL.sc[
2v87541.2122:viXra
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Keywords: Generative models, evaluation, divergence frontiers, neural text generation,
large language models, f-divergences, statistical estimation
Contents
1 Introduction 3
1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Background and Setup 7
2.1 Language Modeling and Open-Ended Text Generation . . . . . . . . . . . . . . . . . 7
2.2 Comparing Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.3 Information Divergences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3 Generalizing Divergence Frontiers with f-Divergences 11
3.1 Tradeoff Curves to Evaluate Generative Models . . . . . . . . . . . . . . . . . . . . . 12
3.2 Scalar Summaries of Divergence Frontiers . . . . . . . . . . . . . . . . . . . . . . . . 13
3.3 Properties of Divergence Frontier Summaries . . . . . . . . . . . . . . . . . . . . . . 14
4 Practical Computation of the Divergence Frontier and its Summaries 17
4.1 Estimation via Vector Quantization. . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.2 Estimation via Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.3 Estimation via Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
5 Related Work 30
5.1 Divergence Frontiers for Generative Models . . . . . . . . . . . . . . . . . . . . . . . 31
5.2 Divergence Measures for Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
5.3 Divergence Measures for Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
5.4 Statistical Estimation of Information Divergences . . . . . . . . . . . . . . . . . . . . 34
6 Experiments: Setup 35
6.1 Task Domains and Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
6.2 Decoding Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
6.3 Baseline Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
6.4 Human Judgements and Evaluation of Automatic Metrics . . . . . . . . . . . . . . . 38
6.5 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
7 Experimental Results 39
7.1 Comparison to Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
7.2 Quantifying the Effect of Model Size, Decoding, Text Length . . . . . . . . . . . . . 40
7.3 Comparison of Statistical Estimation Methods . . . . . . . . . . . . . . . . . . . . . 43
7.4 Comparison to Other Divergences and Optimal Transport Costs . . . . . . . . . . . 46
7.5 Effect of the Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
7.6 Comparison to Generative Precision and Recall . . . . . . . . . . . . . . . . . . . . . 54
7.7 Evaluating Image Generative Models with MAUVE . . . . . . . . . . . . . . . . . . . 55
7.8 Tightness of the Statistical Error Bounds . . . . . . . . . . . . . . . . . . . . . . . . 59
8 Empirical Recommendations 60
2
MAUVE Scores for Generative Models: Theory and Practice
1 Introduction
Large-scale generative artificial intelligence models show an ability to produce human-like
text and realistic images. Recent chatbots such as ChatGPT/GPT-4 (OpenAI, 2023),
Bard (Google, 2023), and Ernie Bot (Sun et al., 2021) have rapidly gained wide promi-
nence in the general public for their articulate responses across many topics and styles.
More generally, large language models such as Llama-2 (Touvron et al., 2023), Falcon (Al-
mazrouei et al., 2023), Bloom (Workshop, 2022), and Mistral (Jiang et al., 2023), as well as
image and multi-modal generative models such as Stable Diffusion (Rombach et al., 2022),
Imagen (Saharia et al., 2022), and CM3leon (Yu et al., 2023) can produce original content
in response to queries in the form of blog posts, poetry, computer programs, and artwork.
However, evaluating the distributions captured by such large-scale generative models
requires substantial effort. Automatic measures can dramatically reduce the cost of eval-
uation, in turn making it easier to rapidly develop models, choose hyperparameters, and
understand a model’s capabilities.
One approach to evaluation is to compare a generative model’s distribution Q with the
target distribution P of the real data that it aims to model. Doing so requires considering
two types of errors: (I) the mass of Q that has a low probability under P where the model
produces unrealistic or degenerate data, and (II) the mass of P that has a low probability
under Q where the model is not able to produce some class of realistic data. However,
quantifying these errors in a principled, computationally tractable manner is challenging
when faced with real-world text or image distributions.
We present a family of comparison measures between pairs of probability distributions,
such as those encountered in the generative modeling of text and images. Building upon
the notion of divergence frontiers proposed by Djolonga et al. (2020), our measures are
statistical summaries of f-divergence frontiers, which capture the two types of errors. We
explore three methods for estimating these divergence frontiers and their scalar summaries.
We provide statistical bounds for two of these estimation methods—vector quantization
and nearest-neighbor estimation—as well as theoretical guidance on choosing the level of
vector quantization. In the spirit of popular metrics in natural language processing such as
BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), we call these measures MAUVE scores.
We develop the scores in practice for open-ended text generation. We find that, for a
rangeoff-divergencesandestimationmethods,thesemeasuresquantifythegapbetweenthe
distributions of human-written text and those of modern neural language models efficiently
androbustly. Moreover,weshowthatthesemeasuresextendtoimagedistributions,aligning
well with the widely used Fréchet distance in the computer vision domain in quantifying the
effectofsamplingalgorithmsandarchitecturalimprovements. Together, ourtheoreticaland
empirical analyses demonstrate that MAUVE provides a principled, effective, and powerful
recipe for comparing distributions of complex high-dimensional text and images.
1.1 Contributions
We make the following contributions in this work.
Statistical Summaries of Divergence Frontiers (Section 3). Our goal is to provide a
scalarsummaryofthediscrepancybetweenagenerativemodelQandthetargetdistribution
3
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
P R Q
λ
R P R = R′
→
R Q
λ R = R
λ
R′ R Q
→
P
KL(P R)
∥
Figure 1: Left: Comparing two distributions P and Q. Here, R = λP + (1 λ)Q is the
λ
interpolation between P and Q for λ (0,1) and R′ denotes some arbitrary distribu−tion. Right:
The corresponding divergence frontier∈(black curve) between P and Q. The interpolations R for
λ
λ (0,1) make up the frontier, while all other distributions such as R′ must lie above the frontier.
∈
P that it aims to model. To do so, following Djolonga et al. (2020), we consider two types
of costs: (I) the mass of Q that has low probability under P, and (II) the mass of P that
has low probability under Q. We formalize these costs using a divergence frontier,
(cid:110) (cid:111)
(cid:0) (cid:1)
(P,Q) = D (P R ), D (Q R ) : λ (0,1) ,
f f λ f λ
F ∥ ∥ ∈
where R = λP +(1 λ)Q, and D is an f-divergence such as the Kullback–Leibler (KL)
λ f
−
divergence. See Figure 1 for an illustration. This extends the frontiers of (Djolonga et al.,
2020) to general f-divergences. We shall show in Section 3 that the nice properties of the
divergence frontiers also extend to their variants based on f-divergences.
We proposethreescalar statistical summaries ofdivergencefrontiers. The firstsummary
measures the area under a transformed divergence frontier:
(cid:0)(cid:8)(cid:0) (cid:1) (cid:9) (cid:1)
MAUVEf(P,Q) = AUC exp( x), exp( y) : (x,y) f(P,Q) (1,0),(0,1) .
− − ∈ F ∪{ }
Here, exp( ) monotonically transforms the frontier to account for unbounded divergences.
·
Second, we consider an integral summary that sweeps over the coordinates on the diver-
gence frontier and accumulates their costs:
(cid:90) 1
(cid:0) (cid:1)
FI (P,Q) := 2 λD (P R )+(1 λ)D (Q R ) dλ.
f f λ f λ
∥ − ∥
0
Finally, the third summary simply uses costs from the mid-point of the frontier, i.e., the
coordinates corresponding to λ = 1/2:
1 1
Mid (P,Q) := D (P R )+ D (Q R ).
f 2 f ∥ 1/2 2 f ∥ 1/2
At their core, all three summaries are based on f-divergences. Thus, all three benefit from
our estimation algorithms and error bounds for f-divergences, which we discuss next.
4
)R
Q(LK
∥
MAUVE Scores for Generative Models: Theory and Practice
Statistical Estimation Algorithms (Section 4). We give algorithms for computing
the summaries MAUVEf, FI f, and Mid
f
on real-world distributions of text or images. This
requires computing f-divergences between the target distribution P and the model distri-
bution Q, which is challenging due to the lack of direct access to P and Q, and the large
support of each distribution. To address these challenges, we propose three methods for
estimating divergence frontiers from i.i.d. samples using embeddings of the data (e.g., from
a large language model for text data):
1. Quantization: we jointly quantize the distributions P and Q in some embedding space
to form two multinomial distributions, then estimate the divergence frontier between
the two multinomial distributions.
2. Nearest-neighbor: we use the nearest neighbors (in some embedding space) of each
sample to estimate the likelihood ratio P(x)/Q(x), which we use to estimate the
required f-divergences.
3. Classifier: we train a classifier to identify whether each sample belongs to the target
or model distribution. We use the classifier to estimate the likelihood ratio and, in
turn, the required f-divergences.
Error Bounds. We develop error bounds for the first quantization approach. The total
estimation error of the divergence frontier consists of two parts: (i) the statistical error in
estimating the frontier from samples, and (ii) the quantization error that arises from passing
from the original distributions to their quantized versions.
For the statistical error, Theorem 10 gives an error bound that allows for long tails
and countable support of the distribution P. This improves over a naive bound that does
not allow for distributions with long tails, and requires finite support. A key technique
that enables this result is considering the missing mass (Good, 1953): the total probability
that does not appear in the finite sample used to estimate the frontier. When the two
distributions P and Q intersect on a finite set of k elements, the bounds simplify further.
For example, we give the following statistical error bound on the integral summary (Eq. 12):
(cid:32)(cid:114) (cid:33)
k k
E FI(Pˆ n,Qˆ n) FI(P,Q) O˜ + ,
| − | ≤ n n
where Pˆ and Qˆ are the empirical estimators and n is the number of samples. We give
n n
a similar bound for general f-divergences (Eq. 11). Our results hold under assumptions
that are satisfied by many common f-divergences (Table 9). To improve the statistical
performance of empirical estimators when the quantization size k is large, we also apply
add-constant smoothing to estimate the two distributions—we add a small constant b > 0
tothecountsofeachbinandnormalizethemtoformadistribution. WeproveinTheorem12
a statistical error bound for the add-constant estimators. Applied to the integral summary,
the bound is (Eq. 17)
(cid:32) (cid:33)
√kn+kb
E |FI(Pˆ nb,Qˆb n) −FI(P,Q)
| ≤
O˜
n+kb
,
5
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
wherePˆb andQˆb aretheadd-constantestimators. Asimilarboundforgeneralf-divergences
n n
is given in Eq. 16.
For the quantization error, we show that there exists a quantization scheme with error
O(1/k), where k is the size of the k-partition used to quantize the sample space. Our
analysis is inspired by the asymptotic approximation of an f-divergence with increasingly
finer partitions (Györfi and Nemetz (1978), Theorem 6). Combining the statistical and
quantization error bounds gives us a bound on the total error of the integral summary
(Eq. 20):
(cid:32)(cid:114) (cid:33)
k k 1
E FI(Pˆ S ,n,Qˆ S ,n) FI(P,Q) O˜ + + .
| k k − | ≤ n n k
We discuss how to operationalize the nonparametric nearest-neighbor estimation with
dimensionality reduction via principal component analysis (PCA). For nearest-neighbor es-
timation, we discuss bounds from Noshad et al. (2017) (Theorem 17).
Experiments (Section 7). Our experiments are organized into multiple parts, mainly
focusing on the open-ended text generation setting.
We start by analyzing the effectiveness of the proposed measure for comparing text
distributions. WefocusontheareasummaryusingtheKLdivergencecomputedwithvector
quantization. We demonstrate that the proposed measures correlate with human quality
judgments (Section 7.1) and quantify known properties of generated text (Section 7.2). The
main focus of the rest of the experimental study is to analyze the effects of each of the
components of the evaluation pipeline: the estimation method, the choice of the divergence,
and the choice of the embedding.
First, we consider different estimation methods: vector quantization, nearest neigh-
bor estimation, and classifier-based estimation (Section 7.3). We also consider a popular
parametric Gaussian approximation method—assuming that embedded samples from the
target and model distributions are distributed according to multivariate Gaussians, we es-
timate the parameters of each Gaussian and estimate the divergence frontier by numerical
integration (see Appendix C for more details). We find that all estimation methods identify
expected quality trends and correlate with human evaluations. However, nearest-neighbor
and classifier-based estimation show a slightly decreased ability to identify good hyper-
parameter values, while parametric estimation requires extreme dimensionality reduction.
Thus, we recommend vector quantization as a default.
Second, we experiment with other f-divergences and optimal transport costs (Sec-
tion 7.4). Specifically, we compare different variants of the proposed measure based on (i)
alternate f-divergences, (ii) other statistical summaries of the divergence frontier, and (iii)
summaries of frontiers based on optimal transport distances. We find that all the quantities
based on f-divergences correlate perfectly. On the other hand, some of the optimal trans-
port distances fail to capture expected trends. These results demonstrate the flexibility and
effectiveness of our proposed measures.
Third, we perform a thorough exploration of the effect of the embedding in the
evaluation pipeline (Section 7.5). Our experiments reveal that the embedding is crucial to
the empirical success of MAUVE. While most large language model embeddings (either a
masked or a causal language model, including the model used to generate the text) and
6
MAUVE Scores for Generative Models: Theory and Practice
even shallow GloVe (Pennington et al., 2014) embeddings yield useful comparison measures,
we find that string kernel-based embeddings or embedding-free direct estimation methods
fail to capture expected trends.
Finally, we demonstrate that our measures generalize to other AI domains beyond text.
Specifically, weshowthatintheimage domain, ourmeasurerecoversexpectedtrendswith
respect to the sampling algorithm and model size, and correlates perfectly with the widely
used Fréchet distance in this setting (Section 7.7).
Previous Papers. This work builds upon two previous shorter conference papers. The
first (Pillutla et al., 2021) introduces the area summary in the context of open-ended text
generation and conducts an empirical study. The second (Liu et al., 2021) studies the statis-
tical theory behind estimating divergence frontiers with vector quantization and smoothed
distribution estimators. This work unifies both of these works and makes several further
contributions.
First, we introduce the notion of f-divergence frontiers and three scalar summaries, gen-
eralizing the area summary from (Pillutla et al., 2021) and the integral summary from (Liu
etal.,2021). Wealsosystematicallystudythepropertiesofthethreesummaries(Section3).
Second,weconsiderthreeestimationalgorithms(Section4),basedonnonparametricestima-
tion, classifier-based estimation, and a parametric Gaussian approximation, and empirically
compare their performance for open-ended text generation (Section 7.3). Empirically, we
perform a thorough exploration of alternatives based on f-divergences and optimal trans-
port (Section 7.4). We also probe the effect of the embedding (Section 7.5), and perform
experiments in the vision domain (Section 7.7), not covered in the previous two papers.
2 Background and Setup
We discuss the basics of open-ended text generation and set up the problem of comparing
multiple generative models.
2.1 Language Modeling and Open-Ended Text Generation
We start with neural autoregressive language models since these form the backbone of pre-
vailing approaches to text generation.
Language Modeling. Consider a sequence x = (x , ,x ) of natural language text,
1 |x|
···
where each x belongs to a finite vocabulary V (e.g., characters or words). An autoregres-
i
sive language model Pˆ( x ) models the conditional distribution over the next token x
1:t t+1
·|
following the sequence x . While neural language models, i.e., language models param-
1:t
eterized by a neural network, date back to at least (Bengio et al., 2003; Collobert et al.,
2011),contemporarymodelsarebasedonthetransformerarchitecture(Vaswanietal.,2017)
summarized in Figure 2 (left).
The usual training objective for neural language modeling is via supervised multi-class
classificationofthenexttoken. WeassumethatthereisanunderlyingdistributionP( x )
1:t
·|
for the next token x humans would write in continuation to a prefix x . The training
t+1 1:t
procedure aims to minimize the Kullback-Liebler (KL) divergence between the distributions
P( x ) and Pˆ( x ) assigned by humans and the language model respectively over the
1:t 1:t
·| ·|
7
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
next token x in continuation to a context x P coming from human-written text:
t+1 1:t t
∼
(cid:104) (cid:16) (cid:13) (cid:17)(cid:105)
m θin Et∼Unif([T−1])Ex1:t∼Pt KL P( ·|x 1:t)(cid:13) (cid:13)Pˆ θ( ·|x 1:t) , (1)
where T is the maximum sequence length. Since neither the distribution P over prefixes
t
of length t nor the distribution P( x ) over the next token is known in practice, plug-in
1:t
·|
estimates of both are employed in practice.
Autoregressive models also yield an estimate of the joint probability Pˆ(x) of a sequence
x = (x , ,x ) as
1 |x|
···
|x|−1
(cid:89)
Pˆ(x) = Pˆ(x x ).
t+1 1:t
|
t=0
Open-Ended Text Generation. The open-ended text generation task asks us to output
text xˆ in continuation of a given context x . In contrast to directed text generation
s+1:|xˆ| 1:s
tasks such as translation, summarization, and question-answering, the task here is open-
ended in that the context size s xˆ is typically small and does not meaningfully constrain
≪ | |
the output space. Unlike directed text generation tasks such as translation, summarization,
and question-answering, the goal here is to generate text that is coherent, fluent, creative,
and engaging. Since these criteria are hard to make mathematically precise, we instead
consider the surrogate goal of generating text which is human-like, such that generated text
samples can pass for samples from the distribution P over human written text sequences.
We model a text generation system as a probability distribution Q( x ) such that its
1:s
·|
generated text xˆ is an i.i.d. sample from Q. Given a neural autoregressive language
s+1:|xˆ|
model Pˆ, we can generate open-ended text in a serial, left-to-right fashion, by sampling
xˆ Pˆ( x ), xˆ Pˆ( x ,xˆ ), etc. This is also known as ancestral sampling, and
s+1 1:s s+2 1:s s+1
∼ ·| ∼ ·|
the induced distribution Q over sequences is
s |xˆ|
(cid:89) (cid:89)
Qsamp(x 1:s,xˆ s+1:|xˆ|) = P(x
t
x 1:t−1) Pˆ(xˆ
t
x 1:s,xˆ s+1:t−1),
| |
t=1 t=s+1
where we assume that the prefix x P is drawn from the human distribution. Note
1:s s
that the distribution Qsamp is identical∼ to Pˆ, expect for the prefix x 1:s. General decoding
algorithms produce samples from a reshaped model distribution, as we discuss next.
Decoding Algorithms. Assuming the language model learning has succeeded, we have
that Pˆ( x ) P( x ) for prefixes x P drawn from the distribution of human-
1:t 1:t 1:t t
·| ≈ ·| ∼
writtentext,inthesensethattheobjectiveof (1)isboundedabovebysomeε > 0. However,
for xˆ drawn from a distribution Q which is different from the human distribution P , the
1:t t t
model’s next-token distribution Pˆ( xˆ ) can be quite different from P( xˆ ) of humans.
1:t 1:t
·| ·|
In the iterative process of ancestral sampling, the gap between P(xˆ 1:t) and Qsamp(xˆ 1:t) keep
increasing as the generation length t grows larger, so that Qsamp is quite far from P. This
leads to decoding algorithms which produce samples
xˆ Q( x ,xˆ ),
t+1 1:s s+1:t
∼ ·|
8
MAUVE Scores for Generative Models: Theory and Practice
ReshapeddistributionQtemp,τ( ·|x1:t)
0.39 0.37
OutputLayer 0.26
Temperatureτ=0.8
N
⊕ × 0.13
Next-tokendistributionPˆ( ·|x1:t) 0.02 0.04
LayerNorm+ 0.24 0.24
MLP
0.16
⊕
0.08
0.05
LayerNorm+ 0.04 ReshapeddistributionQtop-K( ·|x1:t)
Self-Attention 0.41
0.39
Top-KwithK=5
0.26
Embedding
0.13
0.08
InputTextSequencetotheModel 0
Figure 2: Left: The transformer architecture takes in a text sequence x=(x ,...,x ) and out-
1 |x|
puts the next-token distribution Pˆ( x ) for each prefix x . Right: Illustration of how decoding
1:t 1:t
algorithms (specifically, temperature·|rescaling and top-K decoding) reshape the model’s next-token
distribution.
where Q( x ) is a reshaping of the language model Pˆ( x ) in order to promote more
1:t 1:t
·| ·|
conservative outputs. We now define a few popular decoding algorithms; see also Figure 2
(right) for an illustration.
Temperature rescaling (Ackley et al., 1985) applies to language models parameterized
with a softmax function:
(cid:0) (cid:1)
exp ϕ(x x )
Pˆ(x x ) = t+1 | 1:t ,
t+1 1:t (cid:80) (cid:0) (cid:1)
| exp ϕ(x x )
x∈V | 1:t
forsomeunnormalizedscoringfunctionϕ( x 1:t) : V R. Thisdecodingalgorithmrescales
·| →
the term inside the exponential with a “temperature” parameter τ > 0:
exp(cid:0)1ϕ(x
x
)(cid:1)
Qtemp,τ(x
t+1
|x 1:t) =
(cid:80)
eτ xp(cid:0)1t+ ϕ1 (| x′1:t
x
)(cid:1)
.
x′ t+1∈V τ t+1| 1:t
When τ < 1, the distribution Qtemp,τ( x 1:t) becomes more peaked around the most likely
·|
next tokens, making the distribution more conservative.
For an integer K < V , top-K sampling (Fan et al., 2018) applies the transformation
| |
(cid:40)
Qtop-K(x
t+1
|x 1:t) =
0Z1 ,Pˆ(x
t+1
|x 1:t), eif lsx
et ,+1
∈
Vtop-K,
where Z is a normalizing constant, and Vtop-K = z (1), ,z
(K)
V is the set of the K
{ ··· } ⊂
highest scoring tokens satisfying
Pˆ(z x ) Pˆ(z x ) max Pˆ(z x ).
(1) 1:t (K) 1:t 1:t
| ≥ ··· ≥ | ≥ z∈V\Vtop-K |
9
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
TheextremeK = V correspondstoancestralsampling. TheotherextremeK = 1isknown
| |
as greedy decoding, which corresponds to choosing the most likely next token iteratively.
Greedy decoding is often used to approximate the most likely sequence argmax P(x x ).
x 1:t
|
Nucleus sampling (Holtzman et al., 2020), similar to top-K sampling, returns a sparse
distribution. Given a parameter p (0,1), it applies the transformation
∈
(cid:40)
Qnuc,p(x
t+1
|
x 1:t) =
0Z1 ,Pˆ(x
t+1 |
x 1:t), eif lsx
et ,+1 ∈
Vnuc,p,
(2)
where Z is again a normalizing constant. Here, the top-p vocabulary Vnuc,p is the smallest
set V′ V such that (cid:80) Pˆ(x x ) p.
⊂
x∈V′
|
1:t
≥
2.2 Comparing Generative Models
The usual approach to evaluating a text generation model is to compare the output of the
model to human-written text for the same prompt (Papineni et al., 2002; Lin, 2004, etc.).
This paradigm, however, breaks down for open-ended generation since there can be multiple
correct outputs.
We frame the problem as comparing two distributions. Let Q ( ) denote the model
∈ P X
distribution over some data space such as text sequences or images and let P ( )
X ∈ P X
denotethetargetrealdatadistribution. Fortextdistributions,Qdependsontheunderlying
languagemodelPˆ aswellasthedecodingalgorithm. Thegoalofopen-endedtextgeneration
is to generate human-like text and the goal of image generation is to generate photorealistic
images. Both these goals can be framed as finding a model distribution Q that is as close
to P as possible in some metric. Therefore, we cast the evaluation of the generative model
as measuring the gap between the model distribution Q and the target distribution P. We
will make this precise in Section 3.
2.3 Information Divergences
We review the definition of f-divergences and give a few examples.
Definition 1 Let f : (0, ) R+ be a convex function with f(1) = 0. Let P,Q ( )
∞ → ∈ P X
be dominated by some measure µ ( ) with densities p and q respectively. Then, the
∈ P X
f-divergence between P and Q is defined as
(cid:90) (cid:18) (cid:19)
p(x)
D (P Q) = q(x)f dµ(x),
f
∥ q(x)
X
with the convention f(0) = lim f(t) and 0f(p/0) = plim tf(1/t).
t→0+ t→0+
Note that the non-negativity condition on f is without loss of generality.1 Since f is convex
andnonnegativewithf(1) = 0,wehavethatf isnon-increasingon(0,1]andnon-decreasing
1. The generator fˆ(t)=f(t)+c(t−1) yields the same f-divergence as a convex function f with f(1)=0
for all c ∈ R. By choosing c such that f′(1) = 0, we get that fˆis minimized at t = 1. This ensures
non-negativity: inf fˆ(t)=fˆ(1)=0.
t>0
10
MAUVE Scores for Generative Models: Theory and Practice
on [1, ). The conjugate generator to f is the function f∗ : (0, ) [0, ) defined by2
∞ ∞ → ∞
f∗(t) = tf(1/t),
whereagainwedefinef∗(0) = lim f∗(t). Sincef∗ canbeconstructedbytheperspective
t→0+
transformoff,itisalsoconvex. Wecanverifythatf∗(1) = 0andf∗(t) 0forallt (0, ),
≥ ∈ ∞
so it defines another divergence D . We call it the conjugate divergence to D since
f∗ f
D (P Q) = D (Q P).
f∗ f
∥ ∥
The divergence D is symmetric if and only if f = f∗, and we write it as D (P,Q) to
f f
emphasize the symmetry.
Example 2 We give a few examples of f-divergences.
(a) KL divergence: It is an f-divergence generated by f (t) = tlogt t+1.
KL
−
(b) Interpolated KL divergence: For λ (0,1), the interpolated KL divergence is given by
∈
KL (P Q) = KL(P λP +(1 λ)Q).
λ
∥ ∥ −
It is an f-divergence whose generator can be obtained from the upcoming Theorem 5.
(c) Jensen-Shannon divergence: The Jensen-Shannon Divergence is defined as
1 1
D (P,Q) = KL (P Q)+ KL (Q P).
JS 2 1/2 ∥ 2 1/2 ∥
More generally, we have the λ-skew Jensen-Shannon Divergence (Nielsen and Bhatia,
2013), which is defined for λ (0,1) as D = λKL (P Q)+(1 λ)KL (Q P).
JS,λ λ 1−λ
∈ ∥ − ∥
This is an f-divergence generated by
(cid:18) (cid:19) (cid:18) (cid:19)
t 1
f (t) = λtlog +(1 λ)log .
JS,λ
λt+1 λ − λt+1 λ
− −
(d) Interpolatedχ2 divergence: SimilartotheinterpolatedKLdivergence, wecandefinethe
interpolated χ2 divergence D and the corresponding generator f for λ (0,1)
χ2,λ χ2,λ
∈
as
(t 1)2
D (P Q) = D (P λP +(1 λ)Q) and f (t) = − .
χ2,λ
∥
χ2
∥ −
χ2,λ
λt+1 λ
−
The usual χ2 divergence is obtained in the limit λ 0.
→
3 Generalizing Divergence Frontiers with f-Divergences
In this section, we start with the notion of KL divergence frontiers from (Djolonga et al.,
2020) and define f-divergence frontiers in Section 3.1. We define three scalar summaries of
the frontier in Section 3.2 and study their properties in Section 3.3.
2. The conjugacy between f and f∗, also known as Csiszár conjugacy, is unrelated to the Fenchel or
Lagrange duality in convex analysis. This notion of conjugacy is related to the perspective transform
g(t,s)=sf(t/s).
11
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
3.1 Tradeoff Curves to Evaluate Generative Models
Consider a generative model Q ( ) which attempts to model the target distribution
∈ P X
P ( ). It has been argued in (Sajjadi et al., 2018; Kynkäänniemi et al., 2019) that one
∈ P X
must consider two types of costs to evaluate Q with respect to P: (a) a type I cost incurred
from generating poor-quality data, which is the mass of Q that has low or zero probability
mass under P, and (b) a type II cost incurred from a failure to capture the diversity of the
real data, which is the mass of P that Q does not adequately capture.
Suppose P and Q are uniform distributions on their supports, and R is uniform on
the union of their supports. Then, the type I cost is the mass of Supp(Q) Supp(P),
\
or equivalently, the mass of Supp(R) Supp(P). We measure this using the surrogate
\
KL(Q R), which is large if there exists an atom x such that Q(x) is large but R(x) is small.
∥
Likewise, the type II cost is measured by KL(P R). When P and Q are not constrained to
∥
be uniform, it is not clear what the measure R should be. Djolonga et al. (2020) propose
to vary R over all possible probability measures and consider the Pareto frontier of the
multi-objective optimization min (cid:0) KL(P R),KL(Q R)(cid:1). This leads to a curve called the
R
∥ ∥
divergence frontier, illustrated in Figure 1), and is reminiscent of the precision-recall curve
in binary classification. See (Pepe, 2000; Cortes and Mohri, 2005; Clémençon and Vayatis,
2009; Clémençon and Vayatis, 2010; Flach, 2012) and references therein on trade-off curves
in machine learning.
It was shown in (Djolonga et al., 2020, Props. 1 and 2) that the divergence frontier
(P,Q) of probability measures P and Q is carved out by mixtures R = λP +(1 λ)Q
λ
F −
for λ (0,1). We present an elementary proof for completeness.
∈
Property 3 Consider two distributions P,Q with finite support. Then, the Pareto frontier
(cid:0) (cid:1)
for the pair of objectives KL(P ),KL(Q ) is given by
∥· ∥·
(cid:110) (cid:111)
(P,Q) = (cid:0) KL(P R ), KL(Q R )(cid:1) : λ (0,1) , (3)
λ λ
F ∥ ∥ ∈
where R = λP +(1 λ)Q. In other words, there does not exist any distribution R such
λ
−
that KL(P R) < KL(P R ) and KL(Q R) < KL(Q R ) simultaneously for any λ (0,1).
λ λ
| | | | ∈
Proof TheconvexityofKL(P ),KL(Q )allowsustocomputetheParetofrontier (P,Q)
∥· ∥· F
exactly by minimizing linear combinations of the objectives. Concretely, we have from (Mi-
ettinen, 2012, Thms. 3.4.5 & 3.5.4) that
(cid:110) (cid:111)
(P,Q) = (cid:0) KL(P R⋆),KL(P R⋆)(cid:1) : λ [0,1] , where
λ λ
F ∥ ∥ ∈
R⋆ argmin λKL(P R)+(1 λ)KL(Q R) .
λ
∈ { ∥ − ∥ }
R
Simple algebra gives us the identity
λKL(P R)+(1 λ)KL(Q R) = λKL(P R )+(1 λ)KL(Q R )+KL(R R).
λ λ λ
∥ − ∥ ∥ − ∥ ∥
The first two terms of the right-hand side are independent of R and the last term is mini-
mized at R = R . Therefore, R⋆ = R .
λ λ λ
In this work, we consider a more general family of f-divergence frontiers.
12
MAUVE Scores for Generative Models: Theory and Practice
Definition 4 The f-divergence frontier (P,Q) for two distributions P,Q ( ) and a
f
F ∈ P X
divergence generator function f satisfying f(0) < and f∗(0) = is defined as
∞ ∞
(cid:110) (cid:111)
(cid:0) (cid:1)
(P,Q) = D (P R ), D (Q R ) : λ (0,1) ,
f f λ f λ
F ∥ ∥ ∈
where R = λP +(1 λ)Q.
λ
−
Theconditionf(0) < ensuresthatD (P R )andD (Q R )arefinitefor0 < λ < 1,
f λ f λ
∞ ∥ ∥
sothef-divergencefrontieriswelldefined. Theconditionf∗(0) = mimicsthebehaviorof
∞
the KL divergence so that D (P Q) = when P Q and D (Q P) = when Q P.
f f
∥ ∞ ̸≪ ∥ ∞ ̸≪
This allows the divergence curve to grow to infinity as λ approaches the endpoints of (0,1)
if the supports of P and Q are not identical. When f is not specified, we refer to the KL
divergence frontier defined above—it corresponds to f(t) = tlogt t+1.
−
Each coordinate of the f-divergence frontier is itself an f-divergence as we show next.
Property 5 Consider the f-divergence D generated by the convex function f. For any
f
λ (0,1), we have that D (P λP +(1 λ)Q) = D (P Q) and D (Q λP +(1 λ)Q) =
f f f
∈ ∥ − λ ∥ ∥ −
D f (Q P), where f λ : (0, ) R+ is given by
1−λ ∥ ∞ →
(cid:18) (cid:19)
t
f (t) = (λt+1 λ) f . (4)
λ
− λt+1 λ
−
Further, D is a valid f-divergence in that it satisfies the conditions of Theorem 1: f is
f λ
λ
convex, non-negative and f (1) = 0. Moreover, if f is twice differentiable with f′′(t) > 0 for
λ
all t > 0, then f is strictly convex with f′′(t) > 0 for all t > 0.
λ λ
Proof We have f 0 and f (1) = 0 by definition. In order to establish the convexity of
λ λ
≥
f ,observethatf (t) = (g h )(t),whereg(t,s) = sf(t/s)istheperspectivetransformoff,
λ λ λ
◦
and h λ(t) = (t,λt+1 λ) R2
+
is a linear map. The perspective g of a convex function f is
− ∈
convex, and convexity is preserved upon composition with a linear map h , so f is convex.
λ λ
Finally, D (P λP +(1 λ)Q) = D (P Q) and D (Q λP +(1 λ)Q) = D (Q P) can
f f f f
∥ − λ ∥ ∥ − 1−λ ∥
be verified from the definition.
To show the strict convexity of f , we calculate
λ
(1 λ)2 (cid:18) t (cid:19)
f′′(t) = − f′′ > 0
λ (λt+1 λ)3 λt+1 λ
− −
under the given assumptions.
3.2 Scalar Summaries of Divergence Frontiers
We define three summaries of divergence frontiers.
Area Summary. The first summary is inspired by the area under the curve (e.g. Flach,
2012)—a common strategy to summarize tradeoff curves in machine learning. Divergence
frontiers, however, can be unbounded. For instance, as λ 1, we have KL(Q R )
λ
→ ∥ →
KL(Q P), which can be unbounded. The same holds for f-divergence frontiers because
∥
13
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
f∗(0) = . Therefore, we define MAUVE to be the area under a monotonic transformation
∞
of the f-divergence frontier:
(cid:0)(cid:8)(cid:0) (cid:1) (cid:9) (cid:1)
MAUVEf(P,Q) = AUC exp( cx), exp( cy) : (x,y) f(P,Q) (1,0),(0,1) .
− − ∈ F ∪{ }
(5)
Here, c > 0 is a scaling constant that changes the numerical value of MAUVE, but not
its induced ordering over multiple models Q 1,...,Q n. MAUVEf(P,Q) is always bounded
between 0 and 1 with larger values denoting a greater similarity between P and Q.
IntegralSummary. Forthesecondsummaryofthedivergencefrontier,wetakeinspiration
from the minimax theory of hypothesis testing, where the goal is also to study two types of
errors and it is common to theoretically analyze their linear combination; see, e.g., (Ingster
and Suslina, 2003, Sec. 1.2) and (Cai et al., 2011, Thm. 7). Similarly, we consider a linear
combination of the two costs that are the two coordinates of the divergence frontier:
L (P,Q) := λD (P R )+(1 λ)D (Q R ). (6)
f,λ f λ f λ
∥ − ∥
Note that, for the KL divergence, R is exactly the minimizer of the linearized objective
λ
λKL(P R)+(1 λ)KL(Q R) according to Theorem 3. In this case, L is also known as
λ
∥ − ∥
the λ-skew Jensen-Shannon Divergence (cf. Theorem 2).
The linearized cost L depends on the choice of λ. To remove this dependency, we
f,λ
define an integral summary as
(cid:90) 1
FI (P,Q) := 2 L (P,Q)dλ . (7)
f f,λ
0
We can interpret the frontier integral as the average linearized cost over λ (0,1). The
∈
constant of 2 is arbitrary and is chosen so that FI is bounded above by 1, as we shall
KL
momentarily see in Section 3.3.
Mid-point Summary. The third summary is a generalization of the Jensen-Shannon
divergence, defined to be the linearized cost with weight λ = 1/2, i.e.,
1 1
Mid (P,Q) := L (P,Q) = D (P R )+ D (Q R ). (8)
f f,1/2 2 f ∥ 1/2 2 f ∥ 1/2
When f is the generator of the KL (resp. χ2) divergence, it recovers the Jensen-Shannon
(resp. Le Cam) divergence. This summary is intuitively close to the area summary as
illustrated in Figure 3.
3.3 Properties of Divergence Frontier Summaries
We study some properties of the area summary MAUVE.
Property 6 Fix an f-divergence D ( ) such that f(0) < and a scaling constant c > 0.
f
·∥· ∞
For any two distributions P,Q with finite support, the area summary MAUVE(P,Q) satisfies
the following:
(a) 0 MAUVEf(P,Q) = MAUVEf(Q,P) 1,
≤ ≤
14
MAUVE Scores for Generative Models: Theory and Practice
R =λP +(1 λ)Q
λ
−
λ=1/2
area= MAUVEf(P,Q)
(cid:0) (cid:1) area=exp 2Mid (P,Q)
f
−
(cid:0) (cid:1)
exp D (P R )
f λ
− ∥
Figure 3: Relationship between the area summary MAUVEf and the mid-point summary Mid f.
MAUVE is the area under the blue curve, while the mid-point summary Mid is related to the area
under the orange rectangle.
(b) MAUVEf(P,P) = 1, and
(c) if f is strictly convex, MAUVEf(P,Q) = 1 if and only if P = Q.
Proof The curve (exp( cx), exp( cy)) for (x,y) always lies within the unit square,
f
− − ∈ F
so 0 MAUVEf(P,Q) 1. If P = Q, then D f(P R λ) = D f(Q R λ) = 0 for all λ (0,1),
≤ ≤ ∥ ∥ ∈
so that MAUVEf(P,Q) is simply the area of the unit square. Conversely, if P = Q, we have
̸
that D (P R ) = 0 and D (Q R ) = 0 for any λ (0,1) whenever f is strictly convex.
f λ f λ
∥ ̸ ∥ ̸ ∈
Therefore,thecurve(exp( cx), exp( cy))for(x,y) liesstrictlywithintheunitsquare
f
− − ∈ F
and MAUVEf(P,Q) < 1.
We now turn to the integral summary.
Property 7 The integral summary FI of the f-divergence frontier defined by a convex gen-
erator f satisfies the following properties:
(a) FI is an f-divergence generated by the convex function
f
(cid:90) 1(cid:16) (cid:17)
f˜(t) = 2 λf (t)+(1 λ)f∗ (t) dλ,
λ 1−λ
−
0
where f is as defined in (4).
λ
(b) FI (P,Q) = FI (Q,P).
f f
(c) 0 FI (P,Q) 4(cid:82)1 λf∗(λ)dλ+ 2f(0).
≤ f ≤ 0 3
(d) If f is twice differentiable with f′′(t) > 0 for all t > 0, we have FI (P,Q) = 0 if and
f
only if P = Q.
Proof We denote λ¯ = 1 λ. For the first part, we have from Theorem 5,
−
(cid:90) 1
FI (P,Q) = 2 (cid:0) λD (P Q)+λ¯D (P Q)(cid:1) dλ = D (P Q),
f f λ ∥ (f λ¯)∗ ∥ f˜ ∥
0
by using the definition of f-divergences. Note that f˜ is a convex function as it is the
positive linear combination of a family of convex functions. We also directly verify that
15
(cid:1)
(cid:0)
)
R
Q(
D
pxe
λ
f
∥
−
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
f˜(t) f˜(1) = 0 for all t > 0, so D is a well-defined f-divergence. For the second part, we
≥
f˜
get
(cid:90) 1(cid:16) (cid:17)
(f˜)∗(t) = tf˜(1/t) = 2 λf∗(t)+(1 λ)f (t) dλ = f˜(t),
λ 1−λ
−
0
wherethelastequalityfollowsbysubstitutingλ′ = 1 λ. Therefore,FI (Q,P) = D (Q P) =
−
f f˜
∥
D (P Q) = D (P Q) = FI (P,Q). For the third part, we use the upper bound on L
f˜∗
∥
f˜
∥
f f,λ
from Theorem 19 in Appendix A to get
(cid:90) 1 (cid:90) 1
FI (P,Q) = 2 L (P Q)dλ 2
(cid:0) λf∗(λ)+λ¯f∗(λ¯)+2λλ¯f(0)(cid:1)
dλ.
f f,λ
∥ ≤
0 0
Simplifying this integral gives the third part. For the final part, we note that f′′(t) > 0 and
λ
(f∗)′′(t) > 0 for all t > 0 from Theorem 5. This gives
λ¯
(cid:90) 1(cid:16) (cid:17)
(f˜)′′(t) = 2 λf′′(t)+(1 λ)(f∗ )′′(t) dλ > 0.
λ 1−λ
−
0
This implies that f˜is strictly convex. Therefore, D (P Q) = 0 iff P = Q.
f˜
∥
We can instantiate this property for common divergences. The integral summary FI
KL
of the KL divergence frontier is generated by
t+1 t
f˜ (t) = logt,
KL
2 − t 1
−
with the understanding that f˜ (1) = lim f˜ (t) = 0. Similarly, the corresponding
KL t→1 KL
expression for the integral summary of the χ2 divergence frontier is
t2+t+1 3
f˜ (t) = logt (t+1) .
χ2
t 1 − 2
−
We have that FI and FI are upper bounded by 1 and 2 respectively.
KL χ2
Lastly, we turn to the mid-point summary.
Property 8 The mid-point summary Mid of the f-divergence frontier defined by a gener-
f
ator f satisfies the following properties:
(a) Mid is an f-divergence generated by the convex function f as defined in (4).
f 1/2
(b) Mid (P,Q) = Mid (Q,P).
f f
(c) 0 Mid (P,Q) 1 (f(0)+f(2)).
≤ f ≤ 2
(d) If f is twice differentiable with f′′(t) > 0 for all t > 0, we have Mid (P,Q) = 0 if and
f
only if P = Q.
Proof The first, second, and fourth parts follow directly from Theorem 5. The third part
is a consequence of Theorem 19 in Appendix A.
16
MAUVE Scores for Generative Models: Theory and Practice
4 Practical Computation of the Divergence Frontier and its Summaries
In this section, we consider how to compute MAUVE and related divergence frontier sum-
maries for high dimensional distributions of text or images. We usually do not have access
to the target distribution P representing human-written text or real-world images. While
the model likelihood Q(x) can be evaluated for some generative model Q such as language
models for text, it might not be available for others such as generative adversarial networks
forimages. Therefore, weonlyassumeaccesstothedistributionsP andQviai.i.d.samples.
i.i.d. i.i.d.
Given two independent samples x ,...,x P and x′,...,x′ Q, we wish to
1 n 1 m
∼ ∼
estimate the summaries MAUVEf(P,Q), FI f(P,Q), or Mid f(P,Q) using these samples. We
willoftenassumeequalsamplesizesm = nforsimplicity,especiallywhenstatingbounds. In
realimageortextapplications, thedistributionsP andQaretypicallydiscretedistributions
whose support size is too large to enumerate. For instance, neural language models induce
a probability distribution over documents of text. Thus, we cannot tractably compute the
f-divergences required by the divergence frontiers or their scalar summaries in closed form.
Instead, we consider four different estimation methods:
• Vector Quantization: We quantize the empirical distributions Pˆ = (1/n)(cid:80)n δ
n i=1 xi
andQˆ = (1/m)(cid:80)m δ intok-dimensionalmultinomialdistributionsPˆ andQˆ ,
m j=1 x′ n,k m,k
j
where k is a hyperparameter. We then estimate the divergence frontier by the plug-in
estimator f(Pˆ n,k,Qˆ m,k), from which the corresponding summaries MAUVE, FI, and
F
Mid can be estimated. This approach can also be used with add-constant distribution
estimators in place of empirical distributions; see Table 2 for some examples.
• Nearest-neighborestimation: Weendowthespace withametricρ : R+
X X×X →
andconsiderthesetN (x)ofthek-nearestneighborofxfromtheunionofX = x n
k { i }i=1
and X′ = x′ m . We estimate the likelihood ratio P(x′)/Q(x′) based on the ratio
{ j}j=1 j j
N (x′) X / N (x′) X′ for j = 1,...,m. This likelihood ratio can then be used
| k j ∩ | | k j ∩ |
to estimate the required f-divergences.
• Classifier-based estimation: We train a classifier over samples (x ,+1) n′
(x′, 1) m′ and use this to estimate the likelihood ratio P(x)/Q({ x) 1 over t} hi e=1 re∪ -
{ j − }j=1
maining n n′ +m m′ samples. This likelihood ratio can then be used to estimate
− −
the required f-divergences.
• Parametric approximation: Given an embedding φ : Rd, we make a para-
X →
metric assumption that the pushforward distributions φ P = (µ ,Σ ) and φ Q =
♯ P P ♯
N
(µ ,Σ ) with unknown parameters µ ,Σ ,µ ,Σ . We estimate µˆ ,Σˆ ,µˆ ,Σˆ
Q Q P P Q Q P P Q Q
fN rom data and use (cid:0) (µˆ ,Σˆ ), (µˆ ,Σˆ )(cid:1) as an estimate that is computed by
f P P Q Q
F N N
numerical integration. Although this approach is widely used in practice, it has no
theoretical guarantees. Therefore, we defer its discussion to Appendix C and compare
its empirical performance with other methods in Section 7.3.
In the rest of this section, we consider each in detail. In full generality, we will focus on
estimating f-divergences from samples. The results on estimating the f-divergence frontier
(P,Q) follow as corollaries because each point on the frontier is itself an f-divergence
f
F
(Theorem 5).
17
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
AdistributionPonR2 Apartitioning ofR2 ThequantizeddistributionP
S S
11 11
11
10 10 10
9 9 9
8 8 8
7 7 7
12 13 14 15 16 17 12 13 14 15 16 12 13 14 15 16
Figure 4: Illustration of the quantization P
S
of a distribution P over the Euclidean plane R2
under a partition .
S
4.1 Estimation via Vector Quantization
Given a k-partition = S ,...,S of the space , we define the quantization of P over
1 k
as P = (cid:0) P(S ),...,S P(S{ )(cid:1). Then,} P and Q arX e multinomial distributions over k atomS s;
S 1 k S S
they are piecewise constant approximations of P and Q similar to histograms as illustrated
in Figure 4. The quantization approach to estimating the divergence frontier consists of two
approximations:
• approximating the intractable divergence frontier (P,Q) with the lower-dimensional
f
F
counterpart (P ,Q ), and
f S S
F
• estimating this frontier (P ,Q ) with its plug-in estimator (Pˆ ,Qˆ ), where
f S S f S,n S,m
F F
Pˆ = (cid:0) n−1(cid:80)n 1 x S (cid:1)k is the empirical distribution of P , and Qˆ is the
S,n i=1 { i ∈ l } l=1 S S,m
corresponding empirical distribution of Q
S
In practice, the best quantization schemes are data-dependent, such as k-means clustering
or lattice-type vector quantization of dense representations of images or text; we will discuss
this in more detail in Section 4.1.2.
When the two distributions P and Q have long tails, the empirical estimators Pˆ and
S,n
Qˆ can be of poor quality due to the missing mass phenomenon (Good, 1953), i.e., some
S,m
probability masses do not appear in the finite sample. This is illustrated in Figure 5. A
widely used technique to address such a challenge is the add-constant smoothing (see, e.g.,
Krichevsky and Trofimov, 1981). This approach adds a small constant b to the counts of
each bin and normalizes these pseudo-counts to form a normalized probability distribution.
Precisely, the add-b estimator of P is defined as
S
(cid:18) b+(cid:80)n 1 x S (cid:19)k
Pˆb = i=1 { i ∈ l } . (9)
S,n n+kb
l=1
Other estimators suitable for this regime have also been considered in the literature such as
theGood-Turingestimator(OrlitskyandSuresh,2015)andabsolutediscounting(Falahatgar
et al., 2017).
4.1.1 Estimation Error Bounds
The total estimation error of the divergence frontier consists of two parts: (a) the statistical
error in estimating (P ,Q ) from samples, and (b) the quantization error in passing
f S S
F
from P,Q to P ,Q . For simplicity, we assume in this subsection that m = n. In what
S S
18
MAUVE Scores for Generative Models: Theory and Practice
0.3 P 0.3 P
Pˆ Pˆb
0.2 n 0.2 n
Krichevsky-Trofimov
0.1 0.1
Missingmass
0 0
2 4 6 8 10 2 4 6 8 10
position position
Figure 5: Left: Missing mass of a sample corresponds to those entries l Supp(P) that do not
appear in the sample, i.e., Pˆ = 0. Right: Add-constant smoothing adds∈a constant b to counts
n,l
of each bin l Supp(P), including those that do not appear in the sample. Krichevsky–Trofimov
smoothing cor∈responds to b=1/2.
follows, we establish a statistical error bound of order O((cid:112) k/n) and show that there exists
a quantization scheme with error O(1/k). The theory suggests that we can balance the two
errors at k = Θ(cid:0) n1/3(cid:1).
Statistical Estimation Error. We establish a statistical bound on estimating a general
f-divergence D (P Q) between discrete distributions P,Q using their plug-in estimators
f
∥
Pˆ ,Qˆ from samples, respectively. To this end, we require the generator f and its conjugate
n n
f∗ to satisfy some smoothness and tail assumptions.
Assumption 9 The generator f is twice continuously differentiable with f′(1) = 0. Fur-
thermore,
(A1) We have C := f(0) < and C∗ := f∗(0) < .
0 0
∞ ∞
(A2) There exist constants C ,C∗ < such that for every t (0,1), we have,
1 1
∞ ∈
f′(t) C (1 log(1/t)), and, (f∗)′(t) C∗(1 log(1/t)) .
1 1
| | ≤ ∨ | | ≤ ∨
(A3) There exist constants C ,C∗ < such that for every t (0, ), we have,
2 2
∞ ∈ ∞
t t
f′′(t) C , and, (f∗)′′(t) C∗.
2 ≤ 2 2 ≤ 2
Some boundedness assumption is necessary since the minimax quadratic risk of estimating
the KL divergence over all discrete distributions with k atoms is always infinity (Bu et al.,
2018). Assumption(A1)isanecessaryandsufficientconditionforD (P Q)andD (P Q)
f f∗
∥ ∥
to remain bounded for all distributions P,Q. Assumption (A2) guarantees that f is ap-
proximately Lipschitz and cannot vary too fast, while (A3) is a technical assumption that
helps control the variation of f around zero.
Theseassumptionsholdformanyf-divergences, asshowninTable1. Notably, theyhold
for the FI and Mid , as well as the coordinates of the KL and χ2 divergence frontiers.
KL KL
We now turn to the statistical error bound. When both P and Q are supported on a
finitealphabetwithk items, anaturalstrategyistoexploitthesmoothnesspropertiesofthe
f-divergence, namely Assumption (A2). This gives a naïve upper bound O(L(cid:112) k/n) on the
absolute error, where L = C log(1/p ) with p = min P reflects the smoothness
1 ∗ ∗ l∈Supp(P) l
of the f-divergence. The dependency on p requires P to have finite support and a short
∗
tail. However, in many real-world applications, the distributions can either be supported
19
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Satisfies
f-divergence C C∗ C C∗ C C∗
Assumptions? 0 0 1 1 2 2
KL No 1
∞
Interpolated KL Yes λ¯ log 1 λ¯ 1 λ¯2 1 λ¯
λ − λ 2 8λ
Jensen-Shannon (JS) / Mid Yes 1log2 1log2 1 1 1 1
KL 2 2 2 2 4 4
Skew JS Yes λ¯log 1 λlog 1 λ λ¯ λ λ¯
λ¯ λ 2 2
FI Yes 1 1 4 4 1 1
KL 2 2 2 2
Interpolated χ2 Yes 1 1 2 2 4 4
λ¯ λ λ¯2 λ2 27λλ¯2 27λ2λ¯
Le Cam / Mid Yes 1 1 2 2 8 8
χ2 2 2 27 27
Squared Hellinger No 1 1
∞ ∞
Table 1: Examples of f-divergences and whether they satisfy Assumptions (A1)-(A3). Here,
λ (0,1) is a parameter of the interpolated or skew divergences, and we define λ¯ :=1 λ.
∈ −
on a countable set or have long tails (Chen and Goodman, 1999; Wang et al., 2017). By
considering the missing mass in the sample, that is the total probability mass that does not
appear in the finite sample (Good, 1953), we can obtain a bound that is independent of p .
∗
We refer to Figure 5 (left) for an illustration of the missing mass.
Theorem 10 Assumethatk := Supp(P) Supp(Q) N . Letn 3, c 1 := C 1+C 1∗,
| |∨| | ∈ ∪{∞} ≥
and c := C C∗+C∗ C . Under Theorem 9, we have,
2 2 0 2 0
∨ ∨
E D f(P Q) D f(Pˆ n Qˆ n) (cid:0) C 1logn+C 0∗ C 2(cid:1) α n(P)+(cid:0) C 1∗logn+C 0 C 2∗(cid:1) α n(Q)
| ∥ − ∥ | ≤ ∨ ∨
(10)
+(cid:0) C +C∗ C (cid:1) β (P)+(cid:0) C∗+C C∗(cid:1) β (Q),
1 0 2 n 1 0 2 n
∨ ∨
where α n(P) = (cid:80)k l=1(cid:112) n−1P
l
and β n(P) = E(cid:2)(cid:80)
l:Pˆ
n(l)=0P lmax {1,log(1/P l) }(cid:3) . Further-
more, if k < , then
∞
(cid:32)(cid:114) (cid:33)
E D f(P Q) D f(Pˆ n Qˆ n) (cid:0) c 1logn+c 2(cid:1) k + k . (11)
| ∥ − ∥ | ≤ n n
In particular, for the Frontier Integral, it gives a statistical error bound of
(cid:32)(cid:114) (cid:33)
k k
E FI(Pˆ n,Qˆ n) FI(P,Q) C + logn, (12)
| − | ≤ n n
where C is some absolute constant.
Some remarks about the bounds in Theorem 10 are as follows. First, the bound (10)
holds for any distributions with a countable support. Second, it does not depend on p and
∗
is adapted to the tail behavior of P and Q. For instance, if P is defined as P l−2 for
l
l [k], then α (P) (logk)/√n, which is much smaller than (cid:112) k/n in (11) in∝ terms of
n
∈ ∝
20
MAUVE Scores for Generative Models: Theory and Practice
the dependency on k. This result justifies the practice of using a large quantization size k
on real data. Third, it captures a parametric rate of convergence, i.e., O(n−1/2), up to a
logarithmicfactor. ThisrateisnotimprovableinarelatedproblemofestimatingKL(P Q),
∥
even with the assumption that P/Q is bounded (Bu et al., 2018). The bound in (11) is a
distribution-free bound, assuming k is finite. Note that it also gives an upper bound on the
sample complexity by setting the right-hand side of (11) to be ε and solving for n; this is
roughly k/ε2, ignoring constants and log factors.
Proof [Proof Sketch of Theorem 10] We sketch the proof for the FI (P,Q) = D (P Q)
KL f˜
∥
with full details given in Appendix B.1. The proof relies on a careful analysis of the deriva-
tives of the f-divergence while accounting for the missing mass. We start by defining the
bivariate scalar function ψ(p,q) = qf˜(p/q) where f˜is the generator of FI. Then, we have
FI(P,Q) = (cid:80)k ψ(P ,Q ). By the triangle inequality, we have,
l=1 l l
k
(cid:12) (cid:12) (cid:88)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)FI(Pˆ ,Qˆ ) FI(P,Q)(cid:12) (cid:12)ψ(Pˆ ,Qˆ ) ψ(P ,Qˆ )(cid:12)+(cid:12)ψ(P ,Qˆ ) ψ(P ,Q )(cid:12) .
(cid:12) n n (cid:12) (cid:12) n,l n,l l n,l (cid:12) (cid:12) l n,l l l (cid:12)
− ≤ − −
l=1(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:∆
l
=:∆′
l
We bound ∆ in terms of Pˆ P so that summing over all coordinates gives a bound on
l n,l l
the total variation distanc| e Pˆ− P| = (cid:80)k Pˆ P . A first-order Taylor expansion
∥ n − ∥TV l=1| n,l − l |
gives the bound
∆ sup ψ (sP +(1 s)Pˆ ,Q ) P Pˆ ,
l p l n,l l l n,l
≤ | − | | − |
s∈[0,1]
whereψ denotesthepartialderivativeofψw.r.t. itsfirstargument. Unfortunately,asp 0
p
for fixed q = 0, we have that ψ (p,q) = f˜′(p/q) log(q/p) by Assumption (A2→ ).
p
̸ | | | | ≤ → ∞
We use a two-pronged approach to overcome this issue. First, we take a second-order
Taylor expansion and carefully bound the remainder term using Assumption (A3) to get
(cid:32) (cid:33)
1 1
∆ Pˆ P log . (13)
l ≤ 2 | n,l − l | max P ,Pˆ
l n,l
{ }
Secondly, because Pˆ is an empirical distribution, we only have two possibilities: Pˆ 1/n
n n,l
≥
or Pˆ = 0. The first case gives an additional logn dependence on the total variation
n,l
distance (based on Assumption (A2)), while the second case is in the missing mass regime.
Basedonresultsfromthemissingmassliterature(BerendandKontorovich,2012;Mcallester
and Ortiz, 2003), we show
(cid:34) k (cid:35)
(cid:88) 1 klogn
β n(P) = E I(Pˆ n,l = 0) P llog ,
P ≤ n
l
l=1
where β (P) is constructed from the upper bound (13) with Pˆ = 0. Finally, we bound
n n,l
the total variation term by repeatedly applying Jensen’s inequality as
E Pˆ n P TV
(cid:88)k (cid:113)
E(Pˆ n,l P l)2 =
(cid:88)k (cid:114) P l(1 −P l)
α n(P)
(cid:114) k
.
∥ − ∥ ≤ − n ≤ ≤ n
l=1 l=1
21
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Braess-Sauer Krichevsky-Trofimov Laplace
b =1/2 if l does not appear
l
b =1 if l appears once b 1/2 b 1
l
b =3/4 if l appears more than once ≡ ≡
l
Table 2: Add-constant smoothed distribution estimators.
Following Theorem 5, we can specialize Theorem 10 to show the consistent estimation
of the entire f-divergence frontier (P,Q).
F
Proposition 11 Take an arbitrary λ (0,1). Suppose we are given distributions P,Q
0
∈
with k := Supp(P) Supp(Q) N . Assume that Theorem 9 holds true for f λ with
| |∨| | ∈ ∪{∞}
λ [λ ,1 λ ]. If the sample size n 3, the bounds in (10) and (11) hold for
0 0
∈ − ≥
(cid:34) (cid:35)
E sup (cid:110)(cid:12) (cid:12)D f(Pˆ n Rˆ λ) D f(P R λ)(cid:12) (cid:12)+(cid:12) (cid:12)D f(Qˆ n Rˆ λ) D f(Q R λ)(cid:12) (cid:12)(cid:111) , (14)
∥ − ∥ ∥ − ∥
λ∈[λ0,1−λ0]
where Rˆ := λPˆ +(1 λ)Qˆ , with constants replaced by C/λ for some absolute constant
λ n n 0
− (cid:112)
C. In particular, if λ = λ is chosen as λ = o(1) and λ = ω( k/nlogn), then the
0 n n n
expected worst-case error (14) converges to zero at rate O(λ−1(cid:112) k/nlogn).
n
When f is the generator to the KL divergence, Theorem 9 holds for f . Hence, Theorem 11
λ
holds for the KL divergence frontier. In the absence of additional assumptions, the trun-
cation in Theorem 11 is necessary to ensure boundedness of the estimated quantities, since
KL(P R ) is close to KL(P Q) for small λ, and this can be unbounded.
λ
∥ ∥
Estimation Error With Smoothing. We bound the statistical error in estimating the
divergence D (P Q) between P and Q using their add-constant estimators Pˆb and Qˆb
f n n
∥
introduced in (9) and illustrated in Figure 5. Again, this result also holds for the FI
KL
and Mid , as well as the coordinates of the KL and χ2 divergence frontiers. This result is
KL
proved in Appendix B.2.
Theorem 12 Assumethatk := Supp(P) Supp(Q) N . Letn 3, c 1 := C 1+C 1∗,
| |∨| | ∈ ∪{∞} ≥
and c := C C∗+C∗ C . Under Theorem 9, we have,
2 2 0 2 0
∨ ∨
(cid:18) (cid:19)
nα (P) (cid:16) (cid:16)n (cid:17) (cid:17)
E |D f(P ∥Q) −D f(Pˆ nb ∥Qˆb n)
| ≤
n+n
kb
+γ n,k(P) C 1log
b
+k +C 0∗ ∨C 2 (15)
(cid:18) (cid:19)
nα (Q) (cid:16) (cid:16)n (cid:17) (cid:17)
+ n +γ (P) C∗log +k +C C∗ .
n+kb n,k 1 b 0 ∨ 2
where γ (P) = (n+bk)−1bk(cid:80)k P k−1 . Furthermore, if k < , then
n,k l=1| l − | ∞
(cid:16) (cid:16)n (cid:17) (cid:17) √kn+2b(k 1)
E |D f(P ∥Q) −D f(Pˆ nb ∥Qˆb n)
| ≤
c 1log
b
+k +c 2
n+kb
− . (16)
22
MAUVE Scores for Generative Models: Theory and Practice
(a) k=103 (b) n=2×104 (c) k=103,n=104 (d) k=103,n=104
101
102
103
104 105 101 102 103 104 0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
Sample size Support size Tail decay Tail decay
Empirical Braess-Sauer Good-Turing Krichevsky-Trofimov Laplace
Figure 6: Statistical error with smoothed distribution estimators on synthetic data. (a):
Zipf(0) and Dir(1/2) with k = 103; (b): Zipf(0) and Dir(1/2) with n = 2 104; (c): Dir(1)
×
and Zipf(r) with k = 103 and n = 104; (d): Zipf(2) and Zipf(r) with k = 103 and n = 104.
In particular, for the Frontier Integral, it gives a statistical error bound of
√kn+2b(k 1) (cid:16)n (cid:17)
E |FI(Pˆ nb,Qˆb n) −FI(P,Q)
| ≤
C
n+kb
− log
b
+k , (17)
where C is some absolute constant.
Let us compare the bounds in Theorem 12 with the ones in Theorem 10. For the
distribution-dependentbound,thetermα (P)lognin(10)isimprovedbyafactorn/(n+bk)
n
in (15). The missing mass term β (P) is replaced by the total variation distance between
n
P and the uniform distribution on [k] with a factor bk/(n+bk). The improvements in both
two terms are most significant when k/n is large. As for the distribution-free bound, when
k/n is small, the bound in (16) scales the same as the one in (11); when k/n is large (i.e.,
bounded away from 0 or diverging), it scales as O(logn+log(k/n)+k−1) while the one in
(11) scales as O(klogn/n+k−1).
Simulations of Smoothing. We conduct a simple simulation study to empirically verify
the effectiveness of smoothing. Following the experimental settings used by Orlitsky and
Suresh (2015), we consider two types of distributions: (a) the Zipf(r) distribution with
r [0,2] where P l−r. (b) the Dirichlet distribution Dir(α) with α 1/2,1 . For each
l
∈ ∝ ∈ { }
pair (P,Q), we generate i.i.d. samples of size n from each of them and estimate the Frontier
Integral from these samples. We compare 4 different smoothed distribution estimators with
the empirical distribution (“Empirical”) as discussed in (Orlitsky and Suresh, 2015). For
each l , let n be the number of times l appears in the sample and let φ be the number
l t
∈ X
ofsymbolsappearingttimesinthesample. The(modified) Good-Turing estimatorisdefined
as PˆGT n if n > φ and PˆGT (cid:0) φ +1(cid:1) (n +1)/φ otherwise. The remaining
n,l ∝ l l n l+1 n,l ∝ n l+1 l n l
three estimators are all based on the add-b smoothing. For the Braess-Sauer estimator, the
pseudo-count parameter b = b is data-dependent and chosen as b = 1/2 if n = 0, b = 1
l l l l
if n = 1 and b = 3/4 otherwise. For the Krichevsky-Trofimov estimator, the parameter
l l
b 1/2. For the Laplace estimator, the parameter b 1. See Table 2 for a summary.
≡ ≡
As shown in Figure 6, the smoothed distribution estimators reduce the absolute error.
For parts (a) and (b), the Good-Turing and the Krichevsky-Trofimov estimators have the
23
rorre
etulosbA
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
(cid:0)p(x)(cid:1)
f
q(x)
T
3
T
2
T
1
T
0
x
Figure 7: Oraclequantization intheestimationofthef-divergenceD (P Q)withD (P Q ),
f f S S
where P and Q have densities pSand q. This example shows quantization in∥to = 3 bins:∥blue,
orange, and green. Bin i is given by the set x : f(p(x)/q(x)) [T ,T ) . |S|
i−1 i
{ ∈ }
best absolute error. For parts (c) and (d), the Good-Turing estimator is adapted to various
regimes of tail-decay, outperforming the empirical estimator. The Krichevsky-Trofimov and
Braess-Sauer estimators, on the other hand, exhibit small absolute errors for particular
decay regimes. While the smoothed estimators offer a marked improvement when k/n is
large (that is, close to 1), the best estimator is problem-dependent. As a rule of thumb, we
suggest the Krichevsky-Trofimov estimator which works well in the large k/n regime but is
still competitive when k/n is small (i.e., large n).
Quantization Error. We now turn to the quantization error of f-divergences, i.e.,
inf D (P Q) D (P Q ) ,
f f S S
|S|≤k| ∥ − ∥ |
where the infimum is over all partitions of of size no larger than k, and P and Q are
S S
S X
the quantized versions of P and Q according to . We do not assume to be discrete, nor
S X
do we need Theorem 9 to hold. All the results hold for the Frontier Integral (Theorem 7)
and pointwisely on the divergence frontier (Theorem 5). Our analysis is inspired by the
asymptotic approximation of an f-divergence with increasingly finer partitions (Györfi and
Nemetz, 1978, Theorem 6). The key idea behind the proof is shown in Figure 7 and the full
proof is given in Appendix B.3.
Proposition 13 For any two distributions P,Q over and any k 1, we have
X ≥
f(0)+f∗(0)
inf D (P Q) D (P Q ) , (18)
f f S S
|S|≤2k| ∥ − ∥ | ≤ k
where the infimum is over all partitions of of size at most 2k.
S
Total Error. Combining the bounds on the statistical and quantization errors leads to the
following bound for the total estimation error for the Frontier Integral.
Theorem 14 Assume that is a partition of such that = k 2. Then, the total
k k
error E FI(Pˆ S ,n,Qˆ S ,n) FIS (P,Q) is upper bouX nded by |S | ≥
| k k − |
C(cid:2) (α (P)+α (Q))logn+β (P)+β (Q)+ FI(P,Q) FI(P ,Q ) (cid:3) . (19)
n n n n S S
| − k k |
24
MAUVE Scores for Generative Models: Theory and Practice
Algorithm 1 MAUVE estimation via vector quantization
i.i.d. i.i.d.
Input: Samples x n P and x′ m Q, quantizationsizek, smoothingconstant
{ i }i=1 ∼ { j}j=1 ∼
b, embedding model φ, discretization Λ of [0,1].
(cid:16) (cid:17)
1: {φ(x i) }n i=1, {φ(x′ j) }m j=1 ← embed φ, {x i }n i=1, {x′ j}m j=1 ▷ Embed the samples
(cid:16) (cid:17)
2: C = quantize {φ(x i) }n i=1, {φ(x′ j) }m
j=1
▷ Cluster embeddings jointly
3: For l = 1,...,k, set ▷ Count cluster assignments
 
(cid:32) n (cid:33) m
Pˆ Sb
,n,l
= n+1
kb
(cid:88) 1(cid:8) C(x i) = l(cid:9) +b , Qˆb
S,m,l
= m+1
kb
(cid:88) 1(cid:8) C(x′ j) = l(cid:9) +b
i=1 j=1
4: Compute Fˆ f(Pˆ Sb ,n,Qˆb S,m) from (21) for λ
∈
Λ ▷ Build the divergence frontier
(cid:16) (cid:16) (cid:17)(cid:17)
5: return MAUVEf(P,Q)
≈
AUC exp −c Fˆ f(Pˆ Sb ,n,Qˆb S,m) ▷ Numerical quadrature
Moreover, if the quantization error of satisfies the bound in (18), we have
k
S
(cid:34)(cid:32)(cid:114) (cid:33) (cid:35)
k k 1
E FI(Pˆ S ,n,Qˆ S ,n) FI(P,Q) C + logn+ . (20)
| k k − | ≤ n n k
Based on the bound in (20), a good choice of k is Θ(n1/3) which balances between the
statistical error and the quantization error. This balancing is enabled by the existence of
a good vector quantizer with a distribution-free bound in (18). In practice, this suggests
a data-dependent vector quantizer using nonparametric density estimators. However, di-
rections such as kernel density estimation (Meinicke and Ritter, 2002; Hegde et al., 2004;
Hulle, 1999) and nearest-neighbor methods (Alamgir et al., 2014) have not met empirical
success for vector quantization, as they suffer from the curse of dimensionality common in
nonparametric estimation. In particular, Wang et al. (2005); Silva and Narayanan (2007,
2010) propose quantized divergence estimators but only prove asymptotic consistency and
little progress has been made since then. On the other hand, modern data-dependent vector
quantization techniques based on deep neural networks can successfully estimate properties
of the density from high dimensional data (Sablayrolles et al., 2019; Hämäläinen and Solin,
2020). Theoretical results for those techniques could complement our analysis. We leverage
these powerful methods to scale our approach on real data in Section 7. In addition, while
nonparametricestimatorsarenotverysuccessfulforvectorquantization,wecanutilizethem
to estimate the f-divergences directly; we return to this in Section 4.2.
4.1.2 Towards a Practical Algorithm
To develop a practical vector quantization-based estimation procedure for the divergence
frontier (P,Q), we use a data-dependent partitioning based on quantizing the samples
f
F S
in some embedding space. The overall procedure is summarized in Algorithm 1.
Recall that we use vector quantization because the support size of real-world text or
image distributions is extremely large. We employ embeddings from a pre-trained deep
25
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
neural network to compute the vector quantization; such deep representations have been
shown to capture the important properties of the data across modalities (Zhang et al., 2018;
Devlin et al., 2019).
Concretely, we embed the samples using a model φ :
X →
Rd to get {φ(x i) }n
i=1
and
φ(x′) m . Then, we jointly quantize the embedded samples to obtain a mapping C :
{ j }j=1 X →
[k]. This induces a partitioning = (S ,...,S ) with S = x : C(x) = l . For
1 k l
S { ∈ X }
instance, with k-means clustering (Manning and Schütze, 2001; Jurafsky and Martin, 2009),
C(x) denotes the index l of a cluster center c that is closest to embedding φ(x) in terms
l
of L distance so that each partition S is the Voronoi cell
2 l
∈ S
S = (cid:8) x : φ(x) c φ(x) c for j = 1,...,k(cid:9) .
l l 2 j 2
∈ X ∥ − ∥ ≤ ∥ − ∥
Here, we assume that ties are broken arbitrarily.
The quantized distribution P is now computed from the fraction of the points in each
S
partition. For the add-b smoothing, the estimator is
(cid:32) n (cid:33)
1 (cid:88)
Pˆb = 1 x S +b , for l = 1,...,k.
S,n,l n+kb { i ∈ l }
i=1
Note that b = 0 reduces to the empirical distribution, and this coincides with the approach
used in (Pillutla et al., 2021). In this work, we default to Krichevsky-Trofimov smoothing,
which corresponds to b = 1/2.
Each coordinate of the estimated divergence curve is now an f-divergence of the form
D (P λPˆb +(1 λ)Qˆb )andcanbecomputedbysummingoverthekcoordinates. The
f S,n ∥ S,n − S,m
full divergence frontier (Pˆb ,Qˆb ) is a continuously parameterized curve for λ (0,1).
Ff S,n S,m ∈
For computational tractability, we take a discretization Λ of (0,1) and take
(cid:40) (cid:41)
Fˆ f(P,Q) = (cid:0) D f(Q ∥R λ),D f(P ∥R λ)(cid:1) : R λ = λP λ+( Λ1 −λ)Q, . (21)
∈
We take a uniform grid Λ = 1/N,2/N,...,(N 1)/N with N points. Finally, we approx-
imate MAUVEf(P,Q)
≈
MAU{ VEf(Pˆ Sb ,n,Qˆb S,m) us− ing num} erical quadrature on the discretized
frontier ˆ (Pˆb ,Qˆb ). For FI , we can directly estimate FI (P,Q) FI (Pˆb ,Qˆb )
Ff S,n S,m f f ≈ f S,n S,m
when a closed-form expression is derived from Theorem 7 (e.g., for KL and χ2 divergences).
Computational Complexity. The computational complexity of the overall procedure in
Algorithm 1 is dominated by the cost of quantization. The complexity of k-means quan-
tization is O(Tknd), where T is the maximum number of Lloyd’s iterations and d is the
embedding dimension.
4.2 Estimation via Nearest Neighbors
We now turn to the estimation of the divergence frontier and its summaries by counting the
nearest neighbors of each sample. We consider nearest neighbors from the ℓ -distance in an
2
embedding space. Given an embedding model φ : Rd, we define a metric ρ on the data
X →
space as
X ρ(x,x′) = (cid:13) (cid:13)φ(x) φ(x′)(cid:13) (cid:13) .
− 2
26
MAUVE Scores for Generative Models: Theory and Practice
Let N (x) denote the set of k-nearest neighbors (under the metric ρ) of x from the set
k
X X′ where X = x n are samples from P and X′ = x′ m are samples from Q.
∪ { i }i=1 { j}j=1
Following (Noshad et al., 2017), we estimate the f-divergence D (P Q) with the estimator
f
∥
Dˆ (X,X′) = 0
1 (cid:88)m f(cid:32) |N k(x′ j) ∩X |/n (cid:33)
. (22)
f,k ∨ m N (x′) X′ /m
j=1 | k j ∩ |
The intuition behind the estimator is that we expect N (x′) X P(x′) and N (x′)
| k j ∩ | ∝ j | k j ∩
X′ Q(x′), so their ratio (with appropriate normalization)
| ∝ j
N (x′) X /n
rˆ(x′) = | k j ∩ | (23)
j N (x′) X′ /m
| k j ∩ |
can be considered an estimate of the likelihood ratio r(x′) := P(x′)/Q(x′). The f-
j j j
divergence D (P Q) is them estimated as
f
∥
m
Dˆ (X,X′) = 0 1 (cid:88) f(cid:0) rˆ(x′)(cid:1) . (24)
f,k ∨ m j
j=1
4.2.1 Estimation Error Bounds
Nearest neighbor estimation of f-divergences typically requires continuous distributions on
a Euclidean space with densities satisfying certain regularity conditions. To this end, we
consider estimation on a noisy version of the problem.
First, we pass from a discrete data space to an Euclidean embedding space by taking
X
embeddings from a model φ : Rd. While the pushforward distributions φ ♯P and φ ♯Q
X →
are now supported on d, they are not guaranteed to have a density w.r.t. the Lebesgue
R
measure. To overcome this, we consider smooth these pushforward distributions by con-
volving them with a Gaussian (0,σ2I ) to get distributions P′ = φ P ⋆ (0,σ2I ) and
d ♯ d
N N
Q′ = φ Q⋆ (0,σ2I ). Sampling from the convolved distribution is trivial: u = φ(x )+ξ
♯ d i i i
N
and u′ = φ(x′)+ξ′ are a valid samples from P′ and Q′ respectively for x P and x′ Q
j j j i ∼ j ∼
with independent Gaussian noise ξ ,ξ′ (0,σ2I ). We analyze the corresponding version
i j ∼ N d
of (22) that is constructed using the ℓ distance between the noisy vectors u ,u′. We show
2 i j
that this procedure always underestimates the f-divergence.
Property 15 For any divergence generator f, we have
D (P′ Q′) D (φ P φ Q) D (P Q).
f f ♯ ♯ f
∥ ≤ ∥ ≤ ∥
Further, if the data space is discrete and the embedding model is injective, i.e., φ(x) =
X ̸
φ(x′) for all distinct x,x′ , then the last inequality hold with equality.
∈ X
Proof The inequalities are direct applications of the data processing inequality for f-
divergences. When φ is injective, we have, (φ P)(cid:0) φ(x)(cid:1) = P(x) for all x and similarly
♯
∈ X
for Q. Therefore, D (φ P φ Q) = D (P Q) follows from an equality on each term of the
f ♯ ♯ f
∥ ∥
summation defining the f-divergence.
The nearest neighbor estimation (22) of D (P′ Q′) requires the following assumptions.
f
∥
27
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Algorithm 2 MAUVE estimation via nearest-neighbors
i.i.d. i.i.d.
Input: Samples X = x n P and X′ = x′ m Q, number of nearest neighbors
{ i }i=1 ∼ { j}j=1 ∼
k, lower dimension d′, embedding model φ, discretization Λ of [0,1].
(cid:16) (cid:17)
1: {φ(x i) }n i=1, {φ(x′ j) }m j=1 ← embed φ, {x i }n i=1, {x′ j}m j=1 ▷ Embed the samples
(cid:16) (cid:17)
2: U ∪U′ = PCA {φ(x i) }n i=1∪{φ(x′ j) }m j=1,d′ ▷ Joint dimensionality reduction
3: Find N k(u) = k-NN(k,u,U U′) for u U U′ ▷ Find k-nearest neighbors jointly
∪ ∈ ∪
4: Estimate rˆ(u) for u U U′ as ▷ Estimate the likelihood ratio
∈ ∪
N (u) U /n
k
rˆ(u) = | ∩ |
N (u) U′ /m
k
| ∩ |
5: Compute ˆ f,k(P,Q) from (25) for λ Λ ▷ Build the divergence frontier
F ∈
(cid:16) (cid:16) (cid:17)(cid:17)
6: return MAUVEf,k(P,Q) = AUC exp c ˆ f,k(P,Q) ▷ Numerical quadrature
− F
Assumption 16 The smoothed distributions P′,Q′ have densities p′,q′ w.r.t. the Lebesgue
measure, which satisfy the following:
(B1) There exists a B > 0 such that we have 1/B p′(u)/q′(u) B for all u Rd.
≤ ≤ ∈
(B2) The densities p′,q′ are Hölder continuous with coefficient γ (0,1]. That is, there
∈
exists a constant H > 0 such that
p′(u) p′(u′) H u u′ γ for all u,u′ Rd,
| − | ≤ ∥ − ∥2 ∈
and similarly for q′.
The estimator (22) satisfies the following guarantee.
Theorem 17 (Noshad et al. (2017)) Suppose the smoothed distributions P′,Q′ satisfy
Theorem 16, and the divergence generator f is L-Lipschitz over [1/B,B], where B is from
Assumption (B1). Then, the k-nearest neighbor estimator (22) with sample size m = n
satisfies
(cid:32) (cid:33)
(cid:12) (cid:12) (cid:18) k(cid:19)γ/d 1
(cid:12) (cid:12)E[Dˆ f,k(X,X′)] −D f(P′ ∥Q′)(cid:12)
(cid:12) ≤
O
n
+
k
.
The assumption of f being Lipschitz on a restricted domain [1/B,B] follows directly from
Assumption (A2) with a logB factor. Thus, this assumption holds for many f-divergences
as shown in Table 1. The bound shows that this estimator suffers from the curse of di-
mensionality, as is common for nonparametric estimators. The two terms of the error are
balanced at k = nγ/(d+γ) and the optimal rate is n−2γ/(d+γ).
4.2.2 Towards a Practical Algorithm
We note from Theorem 17 that the nearest neighbor estimator (22) suffers from the curse
of dimensionality. The embeddings obtained from pre-trained deep nets are extremely high-
dimensional, ranging between 103 and 104 for typical text and image models. We find
28
MAUVE Scores for Generative Models: Theory and Practice
empirically that a dimensionality reduction step to d′ < 50 dimensions with principal com-
ponent analysis (PCA) is crucial for the estimator to work. The overall algorithm is given
in Algorithm 2.
As in the case of estimation via quantization, we only consider the points on the diver-
gence frontier at a discretization Λ of (0,1). We then approximate each coordinate x(λ)
and y(λ) of the divergence frontier for λ Λ by using the nearest neighbor estimator (22).
∈
Concretely, this gives us
(cid:40) (cid:41)
ˆ (P,Q) = (cid:0) Dˆ (X,X′),Dˆ (X′,X)(cid:1) : λ Λ , (25)
f,k f ,k f ,k
F λ 1−λ ∈
where f is as defined in Theorem 5 so that D (P Q) = D (P λP+(1 λ)Q). Finally, we
λ f f
λ ∥ ∥ −
estimate MAUVEf(P,Q), FI f(P,Q), and Mid f(P,Q) from this curve with numerical quadra-
ture or with closed-form expressions when available.
Computational Complexity. ThePCAstepofAlgorithm2hastimecomplexityO(dn2+
d′d2) while the nearest neighbor search with K-d tree or ball tree structures takes time
O((d′ +k)nlogn), assuming n = m. While both steps can be sped up with approximate
randomized algorithms, efficient open-source implementations of exact algorithms are fast
enough for problems with a few thousand samples. We use the library Faiss (Johnson et al.,
2019) in our experiments in §7.
4.2.3 Extensions and Variants
We could also similarly define a kernel density estimator instead of the nearest neighbor
estimator (e.g. Devroye et al., 1996). Given a kernel κ : Rd R+ normalized such that
(cid:82) κ(z)dz = 1, the kernel density estimate of the density of→ a distribution R using i.i.d.
Rd
samples U = u ,...,u is given by
1 n
{ }
(cid:18) (cid:19)
1 u u
g (u) = κ − i , (26)
κ,h,U U hd h
| |
where h is a bandwidth parameter. A typical choice of kernel is the Gaussian kernel κ(z) =
(2π)−d/2exp( z 2/2).
2
−∥ ∥
Similar to the nearest neighbor approach, we define the kernel density estimator in the
embedding space of a model φ : Rd. We approximate D f(P Q) that of the kernel
X → ∥
density estimator using samples X Pn and X′ Qm as D (g g ), which is in
f φ(X) φ(X′)
∼ ∼ ∥
turn estimated using its plug-in estimate
Dˆ (X,X′) =
1 (cid:88)m f(cid:32) g κ,h,φ(X)(cid:0) φ(x′ j)(cid:1) (cid:33)
. (27)
f,κ,h
m g
(cid:0) φ(x′)(cid:1)
j=1 κ,h,φ(X′\{x′ j}) j
The expectation over Q is approximated by a sample average over X′. The numerator of
the term inside f( ) is simply the kernel density estimate (26) of P at x′ using all n samples
· j
from X, while the denominator is the corresponding estimate for Q using the other m 1
−
samples X′ x′ . The rest of the estimation procedure is identical to Algorithm 2.
\{ j}
29
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
4.3 Estimation via Classification
Here, we consider estimating the likelihood ratio r(x) := P(x)/Q(x) with a probabilistic
classifier such as logistic regression (Sugiyama et al., 2012). The f-divergences can then be
estimated from this likelihood ratio.
We first set up a binary classification problem to discriminate between the two dis-
tributions P and Q. Concretely, define the class prior as P(y = +1) = n/(n + m) and
P(y = 1) = m/(n+m) and the class-conditional distribution by P(x y = +1) = P(x) and
− |
P(x y = 1) = Q(x). By the Bayes rule, the likelihood ratio can equivalently be written as
| −
P(x) P(y = +1 x) P(y = 1)
r(x) := = | − .
Q(x) P(y = 1 x) P(y = +1)
− |
Given a probabilistic classifier that outputs an estimate ηˆ(x) for P(y = 1 x), we can
|
estimate the likelihood ratio as
mηˆ(x) m
rˆ(x) = = ρ(x), (28)
n(1 ηˆ(x)) n
−
whereρ(x) := ηˆ(x)/(1 ηˆ(x))istheoddsratio. Wethenestimatethef-divergenceD (P Q)
f
− ∥
using the Monte Carlo estimate
Dˆ (X,X′;pˆ) = 1 (cid:88)m f(cid:0) rˆ(x′)(cid:1) = 1 (cid:88)m f(cid:32) mηˆ(x′ j) (cid:33) . (29)
f m j m n(1 ηˆ(x′))
j=1 j=1 − j
To train a classifier, we split X = X X and X′ = X′ X′, train a probabilistic
1 2 1 2
∪ ∪
classifier such as a logistic regression model to separate X from X (train set) and evaluate
1 2
the likelihood ratios on X′ and X′ (validation set) to estimate the f-divergence.
1 2
Practical Considerations. Logistic regression can fail to yield meaningful odds ratio
estimates when the two distributions are well-separated. For evaluation of image generative
modelssuchasGANs, Lopez-PazandOquab(2017)foundthatneuralnetworksonthepixel
space capitalize on artifacts in the generated images, leading to perfect classification and
therefore, poor likelihood ratio estimates. To avoid this issue, we employ a linear model on
frozen embeddings φ : Rd.
X →
5 Related Work
Wefocusinthispaperoninformationdivergence-basedscorestoevaluategenerativemodels.
While the evaluation process is post hoc and external to a generative model, it is worthwhile
tomentiontheincreasinglyactiveresearchareaanalyzing(classesof)generativemodelsand
establishingtheoretical results suchasstatisticalconsistency, universalapproximation, sam-
ple complexity; see e.g. (Biau et al., 2021; Schreuder et al., 2021) and references therein. We
review the related work on statistical trade-off curves, information divergence-based scores
for texts and images, and theoretical results on the statistical estimation of information
divergences in mathematical statistics and information theory.
30
MAUVE Scores for Generative Models: Theory and Practice
5.1 Divergence Frontiers for Generative Models
Sajjadi et al. (2018) and Kynkäänniemi et al. (2019) both proposed to account for the two
typesoferrorsofgenerativemodelingusingtrade-offcurvesinthespiritofoperationcharac-
teristics and precision-recall curves for binary classification and statistical detection (Cortes
and Mohri, 2005; Clémençon and Vayatis, 2009; Clémençon and Vayatis, 2010; Flach, 2012).
Inaninspiringpaper,Djolongaetal.(2020)proposedinformationdivergencefrontiersbased
on Rényi divergences thereby encompassing both (Sajjadi et al., 2018) and (Kynkäänniemi
et al., 2019). The authors of (Djolonga et al., 2020) show how to compute the divergence
frontiersinspecialcasessuchasexponentialfamilies. Theirexplorationofstatisticalestima-
tion via vector quantization leads to two observations. First, a small quantization size can
lead to a bias of optimism, where D (P Q ) D (P Q) and this gap can be large when
f S S f
∥ ≤ ∥
issmall. Second, thestatisticalerrorfromsmallsamplesizescanleadtopessimisticesti-
|S|
mates of the divergences. However, (Djolonga et al., 2020) do not provide statistical bounds
for vector quantization nor do they analyze statistical properties of divergence frontiers de-
fined using f-divergences. Moreover, the above research does not consider applications to
open-ended text generation.
We extend the above line of work, presenting a general framework for estimating di-
vergence frontiers and their statistical summaries for generative models. Theoretically, we
provide quantitative upper bounds for both the statistical error and quantization error.
Specifically, we show that the statistical error is bounded by O˜((cid:112) k/n). Our bounds also
demonstrate the interest of using smoothed distribution statistical estimators to account for
the missing mass problem. We explore other estimation procedures based on nonparametric
nearest-neighbor and kernel density estimation, classifier-based estimation, and parametric
Gaussianapproximations. Wealsoperformathoroughempiricalevaluationandoperational-
ize these scores for large-scale text and image models. Finally, based on our observations,
we discuss practical recommendations, to facilitate the application to applied AI domains.
After the publication of the conference paper (Pillutla et al., 2021), subsequent work
has corroborated that the original MAUVE score compares favorably to other automatic
metrics for evaluating neural text (Kour et al., 2022). Pimentel et al. (2023) corroborated
the correlation between this score and human judgment. Their empirical analysis shows
that a 5-gram estimation of MAUVE3 has a much weaker correlation with human evaluations
than the vector quantization procedure used in (Pillutla et al., 2021) (cf. §4.1). Based
on this analysis, Pimentel et al. (2023) conclude that the key to the empirical success of
MAUVE is the vector quantization procedure. The experiments in Section 7 indicate that the
reality is much more nuanced. Indeed, we show that the other nonparametric, parametric,
and classifier-based estimation of Section 4 can be nearly as effective as vector quantization
with the right hyperparameters (§7.3); thus the vector quantization cannot be the driving
factor behind MAUVE’s usefulness as an evaluation metric. We note, however, that vector
quantization has several other benefits, including its simplicity and the availability of fast
open-source implementations.
Instead, we show that MAUVE requires an embedding of text to vectors to work well in
practice: modern transformer language model embeddings as used in (Pillutla et al., 2021)
3. This involves estimating MAUVE(P,Q) ≈ MAUVE(Pˆ 5-gram,Qˆ 5-gram) using 5-gram language models
Pˆ ,Qˆ fit to samples from P,Q respectively.
5-gram 5-gram
31
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
work well but simple non-contextual GloVe embeddings also work equally well (§7.5). How-
ever, estimation from string kernel embeddings4 (§7.5.4) or direct estimation with language
model probabilities (§7.3.3) both fail to quantify previously known trends.
The original MAUVE score has since then been adopted by the language modeling and
computationallinguisticscommunitiestomeasureperformanceandtotunehyper-parameters
indiverselanguagegenerationsettings, includingthedesignofdecodingalgorithms(Meister
et al., 2022; Hewitt et al., 2022; Su et al., 2022; Li et al., 2023; Finlayson et al., 2023), con-
trollable text generation (Yang et al., 2023), architectural innovations (Hu et al., 2022), and
differentially private language generation (Mattern et al., 2022; Yue et al., 2023; Kurakin
et al., 2023).
On the theoretical side, Cheema and Urner (2023) propose a variant of generative
precision-recall of (Kynkäänniemi et al., 2019) and show that it converges to a well-defined
population quantity. As shown in our preliminary conference paper (Liu et al., 2021) and
elaborated on in this work, (f-)divergence frontiers can also be estimated in a statistically
consistent manner with both vector quantization and nonparametric k-nearest neighbor-
based approaches.
5.2 Divergence Measures for Text
Priormeasuresofsimilarity/divergencebetweenmachinetextandhumantextcomeinthree
broad categories: (a) reference-based, (b) statistics-based, and (c) language modeling.
Reference-based metrics evaluate generated text by comparing it with a (small set of)
reference text sample(s), rather than comparing distributions over full sequence. These
include classical metrics for n-gram matching (Papineni et al., 2002; Lin, 2004; Banerjee
and Lavie, 2005), which are designed to capture similarities in the surface form of the
generated text and the human references, making them fundamentally ill-suited for open-
endedgeneration. Moreover, ithasbeenshownin(Novikovaetal.,2017)thattheseclassical
metrics only weakly agree with human judgments.
More recent reference-based metrics are capable of comparisons in a high dimensional
embeddingspace(Shimanakaetal.,2018;Zhangetal.,2020;Sellametal.,2020;Clarketal.,
2019), thereby capturing distributional semantics beyond superficial n-gram statistics. For
instance, Moverscore (Zhao et al., 2019) relies on the Word Mover’s distance (Kusner et al.,
2015), and is an instance of an optimal transport distance (Villani, 2003). Moverscore
computes the minimum cost of transforming the generated text to the reference text, taking
into account the Euclidean distance between vector representations of n-grams, as well as
their document frequencies. The paradigm of reference-based metrics is useful for targeted
generation tasks such as translation and summarization, where matching a set of references
is paramount. However, this family of metrics is unsuitable for the open-ended generation
task where there typically are several plausible continuations for each context and creative
generations are desirable. Chan et al. (2022) consider distribution-aware reference-based
metrics for conditional generation tasks to account for the diversity in the output space.
Statistics-based metrics compare the model distribution Q with respect to the human
distribution P on the basis of some statistic T(P) and T(Q). Property-specific statistics
4. An example of a string kernel is the N-gram kernel defined in §7.5.4; this is directly comparable with
the N-gram estimation of MAUVE in the analysis in (Pimentel et al., 2023).
32
MAUVE Scores for Generative Models: Theory and Practice
such as the amount of repetition (Holtzman et al., 2020; Welleck et al., 2020b), verifiability
(Massarelli et al., 2020), or termination (Welleck et al., 2020a) are orthogonal to MAUVE,
which provides a summary of the overall gap between P and Q rather than focusing on
an individual property. Another statistic is the generation perplexity (Fan et al., 2018;
Holtzman et al., 2020), which compares the perplexity of the model text x Q with that
∼
of human text x′ P under an external model R. We find in Section 7 that generation
∼
perplexity fails to correctly capture the effect of the decoding algorithm and the text length.
Moreover, it can easily be fooled by an adversarial decoder that generates gibberish text
that nevertheless has the right perplexity, as we show in Section 7.2.
Language modeling metrics calculate how (un)likely human text x P is under the
∼
model distribution Q, for instance, using the probability Q(x). These metrics are related to
a single point on the divergence curve, rather than a full summary. Examples include the
perplexity of the test set (which is a sample from P) under the model Q and its generaliza-
tions to handle sparse distributions (Martins et al., 2020). Unlike the proposed measures,
thesemetricsneverseemodeltextsamplesx′ Q,sotheycannotaccountforhowlikelythe
∼
model text is under the human distribution P. Moreover, they cannot be used for decoding
algorithms such as beam search which do not define a token-level distribution.
Automatic metrics have been proposed for specific domains such as generation of dia-
logues (Tao et al., 2018), stories (Guan and Huang, 2020), and others (Opitz and Frank,
2021). They capture task-specific properties; see the surveys (Celikyilmaz et al., 2020;
Sai et al., 2023). In contrast, MAUVE compares machine and human text in a domain-
agnostic manner. Other related work has proposed metrics that rely on multiple samples
for quality-diversity evaluation (Caccia et al., 2020), and Bayesian approaches to compare
the distribution of statistics in machine translation (Eikema and Aziz, 2020).
Gehrmann et al. (2023) point out the challenges involved in designing good automatic
evaluationmetricswithafocusondirectedgenerationtasks. Theyoutlinemanysuggestions
including continuously updated suites of datasets, documentation, and benchmarks, as well
as a multi-dimensional evaluation with each metric focusing on a small yet more precisely
defined scope. Liang et al. (2023) advocate for a multi-metric approach for evaluating
generatedlanguage, goingbeyondqualityandconsideringspecificattributessuchastoxicity
and bias.
Non-Automatic Metrics. HUSE (Hashimoto et al., 2019) aims to combine human judg-
ments of Type I errors with Type II errors measured using perplexity under Q. Due to the
costs of human evaluation, we consider HUSE and other metrics requiring human evalua-
tion, such as single-pair evaluation, complementary to the proposed automatic measures.
As a separate technical caveat, it is unclear how to use HUSE for sparse Q that assigns
zero probability to a subset of text, which is the case with state-of-the-art decoding algo-
rithms (Holtzman et al., 2020; Martins et al., 2020; Meister et al., 2022).
5.3 Divergence Measures for Images
Evaluation of generative models is an active area of research in computer vision, where im-
plicit models including generative adversarial networks (Goodfellow et al., 2014) preclude
even basic divergence evaluations based on test-set log-likelihoods. The popular Inception
Score (Salimans et al., 2016) is based on large-scale supervised classification tasks; it is
33
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
unclear how to adapt this score to other modeling domains, such as open-ended text gen-
eration. The Fréchet Inception Distance (Heusel et al., 2017; Semeniuta et al., 2018) and
its unbiased counterpart, the Kernel Inception Distance (Bińkowski et al., 2018) are both
used for evaluating generative models, but, unlike divergence frontier methods, do not take
into account trade-offs between different kinds of errors between the learned and the ref-
erence distribution. We find in Section 7.2 that the Fréchet distance adopted to the text
setting fails to capture the dependence on the text length, while our proposed approach
can. We note that this sequential temporal aspect is absent in the image modality. An
exploration of this property of Fréchet distance and MAUVE in other sequential modalities
such as video (Unterthiner et al., 2018) and speech (Kilgour et al., 2019) is an interesting
direction for future work.
5.4 Statistical Estimation of Information Divergences
Acloselyrelatedproblemistheestimationoffunctionalsofdiscretedistributions;see(Verdú,
2019) for an overview. In particular, the estimation of KL divergences has been studied in
bothfixedandlargealphabetregimes(Caietal.,2006;ZhangandGrabchak,2014;Buetal.,
2018; Han et al., 2020). An important result from this line of research is that the minimax
quadratic risk of the naïve plug-in estimator is infinite (Bu et al., 2018). The main chal-
lenge arises from the missing mass phenomenon (Good, 1953) which is especially prominent
in the large alphabet regime. This challenge can be addressed by applying add-constant
smoothing (Krichevsky and Trofimov, 1981; Braess and Sauer, 2004) to the empirical distri-
bution estimator and requiring the two distributions to have a bounded density ratio. Our
results also utilize add-constant smoothing without the need for the boundedness assump-
tion. Other choices of estimators include the Good-Turing (Good, 1953) and the absolute
discounting (Falahatgar et al., 2017) estimators.
Onthepracticalside,thereisanewlineofsuccessfulworkthatusesdeepneuralnetworks
tofinddata-dependentvectorquantizationtoestimateinformation-theoreticquantitiesfrom
samples (Sablayrolles et al., 2019; Hämäläinen and Solin, 2020). Our experimental results
also rely on such data-dependent vector quantizers.
There exists a rich literature on statistical estimation of f-divergences using other meth-
ods. Nonparametric estimation of f-divergences via nearest-neighbor and kernel density
estimation was studied in (Póczos et al., 2011; Moon and Hero III, 2014; Kandasamy et al.,
2015; Noshad et al., 2017), to name a few. The variational expression for f-divergences
was leveraged for optimization-based estimation in (Nguyen et al., 2010; Sreekumar and
Goldfeld, 2022). Estimation under structural assumptions satisfied in applications such as
autoencoders was considered in (Rubenstein et al., 2019). While not directly related to
statistical estimation, a general optimization-based methodology to derive sharp inequali-
ties between various f-divergences was given in (Guntuboyina et al., 2014). In contrast,
we focus on vector quantization-based estimation while empirically comparing them to ap-
proaches based on nonparametric estimators, classifier-based estimation, and parametric
approximation.
34
MAUVE Scores for Generative Models: Theory and Practice
Max.
Prompt Number of
Task Domain Model Finetuning Dataset Generation
Length Generations
Length
Webtext GPT-2(allsizes) Pretrained Webtext 35tokens 1024tokens 5000
News Grover(allsizes) Pretrained RealNews varying 1024tokens 5000
Stories GPT-2medium Finetuned WritingPrompts 50tokens 512tokens 5000
Table 3: Dataset and task summary for open-ended text generation. Note that 1024 tokens
correspond to 750 words on average.
∼
6 Experiments: Setup
We consider open-ended text generation tasks, where the model has to generate text in
continuation of a given text prompt. The open-endedness of the task is reflected in the
relative lengths of the prompt and the generation: the prompt is often quite short (35 to 50
tokens), while the generation is 10 to 30 longer (approximately 500 to 1000 tokens).
× ×
6.1 Task Domains and Models
We consider three different text domains: web text, news, and stories. For each domain, we
consider generation with size-based variants of transformer language models. See Table 3
for a summary.
Web Text Generation. The goal of this task is to generate articles from the publicly
available analogue of the Webtext dataset5 using pre-trained GPT-2 models for various
sizes (Radford et al., 2019; Brown et al., 2020). At generation time, we use as prompts the
first 35 tokens of each of the 5000 articles from the Webtext test set, keeping the maximum
generation length to 1024 tokens (which corresponds, on average, to around 750 words). For
comparison with human text, we use the corresponding human-written continuations from
the test set (up to a maximum length of 1024 tokens).
News Generation. Under this task, the goal is to generate the body of a news article,
given the title and metadata (publication domain, date, author names). We use a left-to-
right transformer language model, Grover (Zellers et al., 2019), which is similar to GPT-2
but tailored to generating news by conditioning on the metadata of the article as well. Our
generationsrelyonpre-trainedGroverarchitecturesofvarioussizes. Thegenerationprompt
comprises the headline and metadata of 5000 randomly chosen articles from the “April2019”
set of the RealNews dataset (Zellers et al., 2019), and the maximum article length was 1024
tokens. We reuse the publicly available Grover generations6 for our evaluation.
Story Continuation. Given a situation and a (human-written) starting of the story as
a prompt, the goal of this task is to continue the story. Here, we use a GPT-2 medium
model fine-tuned for one epoch on the WritingPrompts dataset (Fan et al., 2018). We use
as generation prompts the first 50 tokens of 5000 randomly chosen samples of the test set
of WritingPrompts. The model generations are allowed to be up to 512 tokens long. The
corresponding test examples, truncated at 512 tokens are used as samples from P.
5. https://github.com/openai/gpt-2-output-dataset
6. available at https://github.com/rowanz/grover/tree/master/generation_examples
35
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
6.2 Decoding Algorithms
We consider three common decoding algorithms described in Section 2.1.
(a) Greedy decoding selects the most likely next token x = argmax Pˆ(x x )
t+1 x∈V 1:t
|
and is representative of a broader class of approximate likelihood maximization de-
coding algorithms.
(b) Ancestral sampling samples directly from the language model’s per-step distribu-
tions as x Pˆ( x ), and generates unbiased samples from the model distribu-
t+1 1:t
∼ · |
tion.
(c) Nucleus sampling (Holtzman et al., 2020) samples from top-p truncated per-step
distributions, x
t+1
Qˆ nuc,p( x 1:t) as defined in Equation (2).
∼ · |
Greedydecodingattemptstofindtextthatapproximatelymaximizesitslikelihoodunder
the model. While such algorithms are highly successful for directed text generation tasks
suchastranslation,theyproducehighlydegeneraterepetitivetextintheopen-endedsetting.
Whileancestralsamplingproducesunbiasedsamplesfromthemodeldistribution,italsohas
beenfoundtogeneratedegeneratetext(Holtzmanetal.,2020),ostensiblybecausethemodel
is imperfect, especially in the low-probability tail of the next-token distribution. Nucleus
samplingattemptstofixthisbytruncatingthetailandisrepresentativeofthebroaderclass
oftruncatedsamplingmethodsthatarenowwidelyconsideredstate-of-the-art. Wevarythe
nucleus parameter p 0.9,0.92,0.95,0.99 for web text generation and story continuation,
∈ { }
and p 0.9,0.92,0.94,0.96,0.98 for news generation.
∈ { }
In addition, we also consider the following decoding algorithms:
(d) Beam searchisamoresophisticatedapproximatelikelihoodmaximizationalgorithm
that maintains a set of b promising prefixes. At each time step, all possible one-token
continuationsofthecurrentbprefixesareconsideredandthetopbofthemareretained.
(e) Locally typical sampling (Meister et al., 2022) is a truncation sampling method,
which we use as a representative of recent truncation-based decoding algorithms such
as Mirostat (Basu et al., 2021) and η-sampling (Hewitt et al., 2022). Locally typical
sampling with hyperparameter τ (0,1) samples the next token from the truncated
∈
vocabulary
(cid:40) (cid:41)
Vtyp,τ = argmin (cid:88) (cid:12) (cid:12) (cid:12)logPˆ(x x 1:t)+H(cid:0) Pˆ( x 1:t)(cid:1)(cid:12) (cid:12)
(cid:12)
: (cid:88) logPˆ(x x 1:t) τ
V′ | ·| | ≥
x∈V′ x∈V′
of the language model Pˆ, where H(p) = (cid:80) p(x)logp(x) is the Shannon entropy.
− x∈V
This is a set that covers at least τ-fraction of the probability mass but also has log
probabilities that are as close to the conditional entropy as possible. The samples are
obtained by sampling from this truncated distribution as
(cid:40)
Qtyp,τ(x
t+1
|
x 1:t) =
0Z1 ,Pˆ(x
t+1
|
x 1:t), eif lsx
et ,+1
∈
Vtyp,τ,
where Z is a normalizing constant.
(f) Adversarial perplexity sampling is designed to generate low-quality text that
nevertheless matches the perplexity of human text. Adversarial perplexity sampling
proceedsintwophases: (1)wegeneratethefirst15%oftokensinasequenceuniformly
36
MAUVE Scores for Generative Models: Theory and Practice
at random from the vocabulary, and (2) we generate the remaining tokens greedily to
make the running perplexity of the generated sequence as close as possible to the
perplexity of human text.
6.3 Baseline Metrics
We compare the proposed measures to the following automatic evaluation metrics used
previously to evaluate open-ended generation.
• Generation Perplexity (Gen. PPL.): Wecomputetheperplexityofthegenerated
text under the GPT-2 large model. A common heuristic is to match
• Zipf Coefficient: we report the slope of the best-fit line on the log-log plot of the
rank versus unigram frequency plot. Note that the Zipf coefficient only depends on
unigram count statistics and is invariant to, for instance, permuting the generations.
We use the publicly available implementation of (Holtzman et al., 2020).7
• Repetition Frequency (Rep.): The fraction of generations which devolved into
repetitions. Any generation that contains at least two contiguous copies of the same
phrase of any length appearing at the end of a phrase is considered a repetition. We
consider repetitions at the token level. This metric is useful to quantify degenerate
repetitiveness that sometimes comes up with neural text (e.g., with greedy decoding).
• Distinct-n: The fraction of distinct n-grams from all possible n-grams across all
generations. We use n = 4. This is a measure of how diverse the generated text is.
• Self-BLEU:Self-BLEUiscalculatedbycomputingtheBLEUscoreofeachgeneration
against all other generations as references. We report the Self-BLEU using 4-grams.
This operation is extremely expensive, so we follow the protocol of (Holtzman et al.,
2020): sample 1000 generations and compute the BLEU against all other 4999 gener-
ations. A lower Self-BLEU score implies higher diversity.
• Discriminator Accuracy: We train a binary classifier to classify text as human or
not. A smaller discrimination accuracy means that model text is harder to distinguish
from human text. A separate classifier is trained for each model and decoding algo-
rithm pair. For the story continuation task, we train a classification head on a frozen
GPT-2 large model using the logistic loss. We use 25% of the data as a test set and
therestfortraining; aregularizationparameterisselectedwith5-foldcross-validation.
Forthenewsdataset,wefollowtheprotocolof(Zellersetal.,2019),i.e.,aGroverlarge
model finetuned with a binary classification head.
Apart from discriminator accuracy, every other metric quantifies a property T(Q) of the
distribution Q of the generated text. This number makes sense only in comparison to the
corresponding quantity T(P) of the human text distribution P. For each of these, we use
T(Q) T(P) as a measure of the gap between P and Q.
| − |
7. https://github.com/ari-holtzman/degen/blob/master/metrics/zipf.py
37
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
6.4 Human Judgements and Evaluation of Automatic Metrics
An effective metric should yield judgments that correlate highly with human judgments,
assuming that human evaluators represent a gold standard.8 We evaluate how the qual-
ity judgments of the proposed measures correlate with human quality judgments. In our
study, a quality judgment means choosing a particular (model, decoder) setting based on
the resultant generations.
Evaluation Protocol. Sinceourgoalistomeasurethegapbetweenamodeltextdistribu-
tion Q and a human text distribution P, we employ a pairwise setup for human evaluations.
At each round, an annotator receives a context and continuations from two different (model,
decoder) settings, and selects the continuation they found more (a) human-like, (b) inter-
esting, and (c) sensible on a 5-point Likert scale. Our interface for collecting annotations is
shown in Figure 24 of Appendix E.
We collect these annotations for web text generation with 8 different (model, decoder)
settings plus a ninth setting for human-written continuations. Each setting is a GPT-2
model size paired with either ancestral or nucleus sampling. This gives us a total of 36 pairs
of settings. Given the known difficulties with human evaluation of longer texts (Ippolito
et al., 2020), we use a maximum completion length of 256 tokens. We obtain 90 preference
ratings for each pair of settings, coming from a total of 214 crowd-workers from the Amazon
Mechanical Turk platform. The evaluators were paid USD 0.40 per evaluation based on an
estimated wage of USD 16 per hour.
Pairwise Scores to a Ranking. We convert these pairwise preferences to a ranking
by fitting a Bradley-Terry model (Marden, 1995), a parametric model used to predict the
outcome of a head-to-head comparison. In particular, we obtain a score w for each setting i
i
sothat thelog oddsof humans preferring settingitosettingj ina head-to-head comparison
is given by the difference w w .
i j
−
Evaluation of Automatic Metrics. Consider an automatic metric M with mean values
a = (a ,...,a )andstandarddeviationss = (s ,...,s )acrossndifferent(model,decoder)
1 n 1 n
pairs,wherethemeanandstandarddeviationisoverrepetitionswithmultiplerandomseeds.
We assume that higher values of the metric mean that the text is closer to human text. Let
h = (h ,...,h ) denote the Bradley-Terry coefficients obtained from the human evaluation
1 n
protocol designed above. We evaluate the automatic metric M by comparing the ranking
it induces over the (model, decoder) pairs to that obtained by the human evaluation using
the Spearman rank correlation.
In order to account for the standard deviation of the metric, we define the worst-case
Spearman rank correlation between a s ,...,a s with h = (h ,...h ) as
1 1 n n 1 n
± ±
ρ (a,s,h) = min ρ(cid:0) (a +σ s )n ,h(cid:1) , (30)
min i i i i=1
σ1,...,σn∈{−1,1}n
where ρ(a,h) denotes the Spearman rank correlation between a and h. The end result is
a correlation score in [ 1,1], with higher values meaning that quality judgments using the
−
8. Whilerecentworkhasshownthathumanevaluationmightnotalwaysbeconsistent(Clarketal.,2021;
Karpinska et al., 2021; Gehrmann et al., 2023), human judgments continue to be the gold standard for
evaluating open-ended text generation.
38
MAUVE Scores for Generative Models: Theory and Practice
Metric Task Gen. PPL Zipf Coef. REP Distinct-4 Self-BLEU MAUVE⋆
Human-like/BT Web text 0.810 0.762 0.500 0.738 0.500 0.857
Interesting/BT Web text 0.643 0.405 − 0.571 0.524 0.262 0.714
Sensible/BT Web text 0.738 0.643 − 0.476 0.595 0.452 0.762
Disc. Acc. News 0.468 0.595 − 0.792 0.653 0.516 0.956
Disc. Acc. Stories 0.690 0.762 0.190 0.833 0.905 0.905
Table 4: Correlation of various automatic metrics with human judgments when available, and the
accuracy of a trained discriminator otherwise. “BT” denotes the Bradley-Terry score for a pairwise
human evaluation. We show the worst-case Spearman rank correlation defined in (30) for the BT
scores. Boldfaced/highlighted numbers indicate the highest correlation in each row.
automatic metric correlate with quality judgments made by human evaluators up to one
standard deviation from the randomness of sampling.
6.5 Hyperparameters
By default, we summarize the divergence frontier with MAUVEKL computed using k-means
vector quantization (Algorithm 1) with k = 500 buckets. Following the discussion in Sec-
tion 4.1, we use the Krichevsky–Trofimov (add-1/2) smoothing. This is different from the
default setting of (Pillutla et al., 2021), where the empirical estimator is used instead (with
the other hyperparameters remaining the same). To make this distinction clear, we refer to
the version computed by the smoothed estimator as MAUVE⋆ and the original version of
KL
(Pillutla et al., 2021) as MAUVEKL (or MAUVE⋆ and MAUVE respectively when the KL diver-
gence is clear from the context). We compare this choice with different estimation methods
in Section 7.3 and different divergence frontier summaries in Section 7.4.
7 Experimental Results
We present the main experimental results in this section. We start by comparing the rank-
ingsinducedbyMAUVEtothatofthehumanevaluatorsinSection7.1. Next,wedemonstrate
in Section 7.2 that the proposed measures can quantify how the properties of the generated
text vary with model size, decoding algorithms, and text length. Then, we compare in
Section 7.3 the different statistical estimation methods discussed in Section 4. We perform
a detailed comparison of various f-divergence and optimal transport-based alternatives in
Section 7.4. We demonstrate the effect of the embedding model in Section 7.5, and explore
the applicability of generative precision-recall (Kynkäänniemi et al., 2019), originally pro-
posed for the vision modality, to the natural language modality in Section 7.6. Finally, we
go beyond the language domain to show how the proposed methods can be useful in the
vision modality in Section 7.7.
7.1 Comparison to Human Evaluation
We now compare the ranking induced by the proposed measure to that of the human eval-
uation scores.
39
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Correlation with Human Judgments. Table 4 shows the correlation between human
judgments and five automatic evaluation metrics obtained using our evaluation protocol on
the web text domain. MAUVE correlates highly with human judgments of how human-like
(0.857), interesting (0.714), and sensible (0.762) the machine text is. MAUVE’s correlations
withhumanjudgmentsaresubstantiallyhigherthanthosefortheotherautomatedmeasures;
forinstance, thecommonlyusedgenerationperplexityhascorrelationsthatare0.810, 0.643,
and 0.738 respectively. The results suggest that the proposed measures may act as an
effective, automatic surrogate for costly human judgments.
Correlation with Learned Discriminators. We also measure the quality of generations
by how well a trained model (a discriminator) can distinguish between real and generated
text(Lopez-PazandOquab,2017). Wereportthetestaccuracyofabinaryclassifiertrained
to discriminate between machine and human text; a lower discrimination accuracy implies
that the generation is harder to distinguish from human text. We report the accuracy
of Grover-large as the discriminator for the news generations as it produced the highest
discriminationaccuracy(Zellersetal.,2019)whileweuseGPT-2largeforthestorydomain.
AsseeninTable4, MAUVE correlatesthehighestwiththediscriminationaccuracy(0.956for
news and 0.905 for stories) among all comparison measures. Computing the discrimination
accuracy for each (model, decoder) pair requires fine-tuning a separate model, which is
particularly expensive for large models. The proposed measures, on the other hand, do not
require any training when computed using vector quantization.
Disagreements between MAUVE and Human Judgements. Table 5 gives the values
of MAUVE and the Bradley-Terry coefficients of the human evaluation for how human-like
the text is. Human evaluators find GPT-2 xl with ancestral sampling (BT score of 8.97) to
producetextthatismorehuman-likethanGPT-2mediumwithnucleussampling(BTscore
of 3.43), whiletheir MAUVE scoresare0.908and0.936respectively. Similarly, MAUVE finds
−
GPT-2 large with ancestral sampling to be worse than GPT-2 small with nucleus sampling,
while human evaluators disagree. MAUVE agrees with the human evaluators on all other
pairwise comparisons.
7.2 Quantifying the Effect of Model Size, Decoding, Text Length
To study the effectiveness of the proposed measures for comparing text distributions, we
first examine how they quantify known properties of generated text: a good metric should
meet expected behavior that is known from existing research on each property. Specifically,
we investigate how MAUVE behaves under changes in model size, decoding algorithm, and
generation length. We give the results of web text generation in Table 5; the corresponding
results for the other domains can be found in Appendix D.
Effect of the Model Size. Scaling the model size has been a critical driver of recent
advances in natural language processing, with larger models leading to better language
modeling and higher-quality open-ended generation. An effective metric should capture
the relationship between model size and generation quality, which we verify with human
evaluations.
We see from Table 5 that MAUVE increases as the model size increases, agreeing with
the human evaluation and the expectation that larger models should have higher quality
40
MAUVE Scores for Generative Models: Theory and Practice
GPT-2Size Decoding Gen. PPL ZipfCoef. Rep. Distinct-4 Self-BLEU Human( ) MAUVE⋆ ( )
↑ ↑
Sampling 101.8800.627 0.9260.001 0.0010.000 0.9410.001 0.3270.003 27.52 0.6550.018
small
Greedy 1.224 1.037 0.942 0.072 0.4650.000 − – 0.019
Nucleus,0.9 23.7880.144 1.0120.002 0.0100.001 0.8590.002 0.4360.004 15.78 0.9060.005
Adversarial 12.554 1.073 0.006 0.365 0.525 − – 0.043
Sampling 129.2630.798 0.8720.001 0.0010.000 0.9530.001 0.2810.002 30.77 0.4460.010
Greedy 1.241 0.978 0.903 0.091 0.415 − – 0.024
medium
Nucleus,0.9 21.0730.134 0.9570.001 0.0050.001 0.8840.001 0.4020.003 3.43 0.9360.004
Adversarial 12.554 1.006 0.005 0.381 0.444 − – 0.044
Sampling 30.0800.196 0.9300.002 0.0020.001 0.9160.001 0.3580.001 6.93 0.8780.008
Greedy 1.232 0.983 0.881 0.100 0.413 − – 0.026
large
Nucleus,0.95 13.4990.058 0.9670.002 0.0060.001 0.8700.001 0.4120.002 12.55 0.9520.002
Adversarial 12.554 0.965 0.005 0.395 0.429 – 0.035
Sampling 31.8860.447 0.9300.001 0.0020.001 0.9130.001 0.3600.003 8.97 0.9080.005
Greedy 1.278 0.975 0.859 0.115 0.417 – 0.033
xl
Nucleus,0.95 14.1430.043 0.9660.002 0.0050.000 0.8680.001 0.4130.002 15.66 0.9550.004
Adversarial 12.554 0.986 0.005 0.397 0.448 – 0.057
Human n/a 12.602 0.952 0.002 0.878 0.382 47.25 –
Table 5: Automatic metrics across different model sizes and decoding approaches for web text
generations. Subscripts indicate the standard deviation across 5 runs for the sampling-based meth-
ods; greedy decoding, being deterministic, always returns the same value for a given model. For
nucleus sampling, we show the best hyperparameter value from 0.9,0.92,0.95,0.99 as per MAUVE.
The column “Human” gives the Bradley-Terry score obtained fro{m how human-like}the text is (Sec-
tion 6.4). Boldfaced numbers indicate the best performance according to the metric, or closest to
the human reference, when applicable.
Beam Beam + no 4-gram repeat
Decoding Greedy Ancestral Nucleus
b=4 b=8 b=4 b=8
MAUVE⋆ 0.019 0.040 0.049 0.438 0.415 0.655
0.021
0.906
0.005
Table 6: Beam search with beam sizes b = 4,8 (with and without allowing 4-gram repetitions)
versusotherdecodingalgorithmsofTable5forwebtextgenerationwithGPT-2small. Thesubscript
denotes the standard deviation over 5 random seeds and is omitted for the deterministic greedy
decoding and beam search.
Locally Typical Sampling
Decoding Nucleus
τ =0.2 τ =0.5 τ =0.7 τ =0.9 τ =0.95 τ =0.99
MAUVE⋆ 0.862
0.012
0.896
0.005
0.88
0.01
0.939
0.009
0.950
0.005
0.914
0.007
0.952
0.003
Table 7: Comparing locally typical sampling (Meister et al., 2022) to nucleus sampling (p =
0.95) with MAUVE for web text generations from GPT-2 large. The subscript denotes the standard
deviation over 5 random seeds.
generations. The widely-used generation perplexity, however, incorrectly rates the large
model’s text as better than the xl model. In this case, human evaluators rate generations
from the small model better than those from the medium model. Interestingly, MAUVE and
Gen. PPL. both identify this relationship, agreeing with the human ratings, in contrast to
the other automatic metrics we surveyed.
41
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
MAUVE ( ) Fréchet Distance ( ) Gen. PPL. diff. ( ) Sparsemax score ( )
0.97 12.4 12 0.685
0.96 12.2 10 0.680
0.95 8 0.675 0.94 12.0 6 0.670
0.93 11.8 4 0.665
0.92 0.660
0.91 11.6 2 0.655
0.90 0
200 400 600 800 1000 200 400 600 800 1000 0 200 400 600 800 1000 200 400 600 800 1000
Max. Length Max. Length Max. Length Max. Length
small medium large xl
Figure 8: Generation quality versus maximum generation length according to MAUVE and three
alternativemeasures(webtext,GPT-2). MAUVE istheonlycomparisonmeasurethatidentifiesthat
generation quality decreases monotonically with increasing text length. The shaded area shows one
standard deviation over generations from 5 random seeds.
Effect of the Decoding Algorithm. Recentworkhasidentifiedtwocleartrendsinopen-
endedtextgenerationwithstandardautoregressivemodels: (1)usinggreedydecodingresults
in repetitive, degenerate text (Holtzman et al., 2020; Welleck et al., 2020b,a); (2) nucleus
sampling (and related truncated sampling methods) with the right hyperparameter yields
higher quality text than ancestral sampling (Fan et al., 2018; Holtzman et al., 2020). An
effectivemeasureshouldthusindicatethequalityrelationshipgreedy ancestral nucleus.
≺ ≺
We see from Table 5 that MAUVE correctly identifies the expected quality relationship,
assigning the lowest quality to greedy decoding for the xl model followed by ancestral sam-
pling, and the highest quality to nucleus sampling for all model sizes — these values are
0.016,0.882,0.940 respectively for the xl model. Other commonly used metrics fail to iden-
tify this relationship: generation perplexity rates the highly degenerate greedy-decoded text
as better than ancestral sampling (a difference of 11.324 w.r.t. the human perplexity vs.
19.284). Furthermore, generation perplexity falls victim to the adversarial decoder that
produces gibberish text. MAUVE, on the other hand, rightly rates it poorly.
We see in Table 6 that MAUVE identifies degeneracy of beam search, thus quantifying the
qualitative observations of Holtzman et al. (2020). Next, Table 7 shows that locally typical
sampling produces text that is comparable in its MAUVE score to nucleus sampling and
outperforms other decoding algorithms, echoing the results of Meister et al. (2022).
Effect of the Generation Length. Although large transformer-based models can gener-
ate remarkably fluent text, it has been observed that the quality of generation deteriorates
with text length: as the generation gets longer, the model starts to wander, switching to
unrelated topics and becoming incoherent (Rashkin et al., 2020). As a result, an effective
measure should indicate lower quality (e.g. lower MAUVE) as generation length increases.
Figure 8 shows MAUVE as the generation length increases, along with three alternative
metrics: generation perplexity, sparsemax score (Martins et al., 2020), and Fréchet dis-
tance (Heusel et al., 2017; Semeniuta et al., 2018). MAUVE reflects the desired behavior,
showing a decrease in quality as generation length grows, with the trend consistent across
model sizes. The other three metrics, however, show less favorable trends. Fréchet dis-
tance indicates improving quality as the length increases, while generation perplexity shows
non-monotonic quality trends for the small and large models. Finally, language modeling
42
EVUAM
ecnatsiD
tehcérF
.LPP
.neG
erocs
xamesrapS
MAUVE Scores for Generative Models: Theory and Practice
Decoding = Ancestral Sampling Decoding = Nucleus Sampling GPT-2 Model size = Large GPT-2 Large + Nucleus Sampling
0.92 0.98 0.98
0.86 0.97 0.97 0.98
0.95 0.97
0.95 0.92
0.78
0.92 0.86 0.95
0.63 0.86 0.78 0.92
0.63
0.39 0.78 0.39 0.86
0.63 0.0 0.78
S M L XL S M L XL 0 0.9 0.92 0.95 0.99 1 200 400 600 800 1000
GPT-2 Model size GPT-2 Model size Nucleus sampling parameter p Text Length
Quantization k-Nearest Neigbhors Kernel Density Estimator Classifier-based Estimator Parametric Approximation
Quantization (k-means) k-Nearest Neighbors (k=50) Kernel Density Estimator Parametric Approximation
1.00
0.95
0.90
0.85
0.80
0.75
MAUVE (default)
0.70
Human
0.65
101 102 103 101 102 101 102 101
Quantization size k Lower dimensionality d Lower dimensionality d Lower dimensionality d
Figure 9: ComputingMAUVEusingdifferentestimationproceduresofSection4. Toprow: Trends
from varying model size, decoding algorithm, and text length. Bottom row: Effect of estimation
hyperparameters on the correlation with the default setting of MAUVE (vector quantization with
k =500) and human evaluations from Table 5. These correlations for the classifier-based estimator
are 0.979 and 0.857 respectively.
metrics such as the sparsemax score (Martins et al., 2020) remain constant, since they do
not depend on the samples generated.
7.3 Comparison of Statistical Estimation Methods
We now compare different methods of estimating MAUVE from Section 4 as well as direct
estimation from model probabilities.
7.3.1 Comparison of Vector Quantization with Other Approximations
We now compare the different statistical estimation methods from Section 4: vector quanti-
zation(Algorithm1withKrichevsky–Trofimovsmoothing,ourdefault),nearestneighbores-
timation(Algorithm2),kerneldensityestimation(Algorithm2modifiedasinSection4.2.3),
classifier-based estimation (Section 4.3), and parametric approximation (Appendix C). We
also compare these to the direct estimation of MAUVE based on model probabilities. We
perform these comparisons for the web text domain.
Hyperparameters. The non-parametric nearest neighbor and kernel density estimators
and the parametric approximation require the d = 1024 dimensional embeddings to be pro-
jected into a small m-dimensional subspace. We use the first m principal components of the
embeddings. Empirically, wefindthatthemonotonicitypropertyKL(P λ P+(1 λ )Q)
1 1
∥ − ≤
KL(P λ P +(1 λ )Q) for λ λ can fail to hold in the non-parametric and parametric
2 2 1 2
∥ − ≥
estimates if m > 100. This is a manifestation of the well-known curse of dimensionality for
non-parametric estimation and the Monte-Carlo estimation (Equation (46) in Appendix C)
43
EVUAM
noitalerroC
namraepS
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
requiredbyourparametricapproximation. Practically, thefailureofthemonotonicityprop-
ertymakesitchallengingtoestimatetheareaunderthecurve. Weemployaℓ -regularization
2
term to the classifier-based estimator and found the results to be robust to the choice of the
regularization parameter in the range 1/n to 10−3/n where n is the number of samples. We
use a different scaling constant c within the exponential (cf. (5)) for each method: c = 5 for
vector quantization, c = 10 for nearest neighbor and kernel density estimation, c = 2.5 for
classification, and c = 1 for the Gaussian approximation. Note that this does not change
the induced rankings.
Results. The results are given in Figure 9. We see that each of the estimation methods
can identify most of the trends of Section 7.2. As a notable exception, the classifier-based
estimate fails to identify the trend that the GPT-2 small model with ancestral sampling is
better than the medium one (cf. Table 5). Notably, the parametric approximation identifies
the correct dependence on the text length while the parametric approximation of the opti-
mal transport cost, namely the Fréchet distance fails to capture this trend (cf. Figure 8).
Interestingly, m = 5 or 10 principal components of the embeddings allow us to capture the
trends with respect to the model size, decoding algorithms, and text length.
Correlation Analysis. We note that each estimation method exhibits a high Spearman
rankcorrelationwiththedefaultvectorquantizationapproachof0.95to1.0andaworst-case
Spearman correlation of at least 0.857 with the human evaluations for the best hyperparam-
eter values. We find that the parametric approximation is not robust to the number m of
principal components — its performance steeply drops off at m = 100.
Pros and Cons of the Estimation Methods. All the tested estimation methods are
consistent with each other, demonstrating the versatility of MAUVE’s recipe of estimating
information divergences from vector embeddings of data. However, there are some minor
differences. First,thek-nearestneighborandclassifier-basedestimatorsreportatiebetween
nucleus sampling with p = 0.9 and p = 0.95. In contrast, the vector quantization approach
ranks p = 0.95 as better than p = 0.9; this is also the case with the Gen. PPL. baseline.
Second, the non-parametric nearest neighbor and kernel density estimators, as well as the
parametric Gaussian approximation require extreme dimensionality reduction, which makes
it important to select the lower dimension correctly. In contrast, the quantization perfor-
mance is more robust to its hyperparameter (the quantization size k). Thus, we recommend
the vector quantization approach as a reliable default as it is relatively computationally
inexpensive and does not require much hyperparameter tuning.
7.3.2 Effect of Smoothing on Vector Quantization-Based Estimation
We now analyze the effect of smoothing on vector quantization-based estimation. Table 8
compares vector quantization (Algorithm 1) with and without the Krichevsky–Trofimov
smoothing. Their Spearman rank correlations are 1.0, meaning that they induce the same
ranking. We note that their numerical values can be different, depending on the number of
empty bins.
Recall the computation pipeline of Algorithm 1: we jointly quantize the embedded sam-
ples φ(x ,...,φ(x ) and φ(x′),...,φ(x′ ) from P and Q respectively with k-means
1 n 1 m
{ } { }
clustering. If some bin l contained samples only from P, then the mass in that particular
44
MAUVE Scores for Generative Models: Theory and Practice
MAUVE ( ) Empty bins
GPT-2 Size Decoding ↑
No Smoothing K-T Smoothing Total Number Percentage
Sampling 0.589 0.655 54.2 5.4
0.018 0.018 6.6 0.7
small Greedy 0.008 0.019 373.0 37.3
0.000
Nucleus, 0.9 0.878 0.906 36.4 3.6
0.006 0.005 4.9 0.5
Sampling 0.373 0.446 77.0 7.7
0.010 0.010 5.5 0.5
medium Greedy 0.012 0.024 314.0 31.4
Nucleus, 0.9 0.915 0.936 29.0 2.9
0.006 0.004 6.6 0.7
Sampling 0.845 0.878 30.2 3.0
0.010 0.008 1.3 0.1
large Greedy 0.012 0.026 311.4 31.1
0.000 0.000 0.8 0.1
Nucleus, 0.95 0.936 0.952 26.6 2.7
0.003 0.002 3.0 0.3
Sampling 0.882 0.908 27.6 2.8
0.006 0.005 6.8 0.7
xl Greedy 0.016 0.033 288.0 28.8
Nucleus, 0.95 0.940 0.955 23.4 2.3
0.006 0.004 2.9 0.3
Table 8: Comparison of MAUVE with vector quantization without any smoothing (the default of
(Pillutla et al., 2021)) and with Krichevsky–Trofimov (K-T) smoothing (the default MAUVE⋆ in this
work). Their Spearman correlation is 1.00. The last two columns show the number and fraction
of empty bins obtained after vector quantization (without smoothing) across both P and Q for the
computation of MAUVE(P,Q). The subscript of each column denotes the standard deviation over 5
random seeds.
bin of Qˆ (l) would be missing, i.e., Qˆ (l) = 0. Table 8 shows the number and fraction
S,m S,m
of empty bins. We observe around 2% to 5% empty bins for nucleus and ancestral sampling.
The number of empty bins increases with an increasing gap between the two distributions:
greedy decoding has around 30% of the bins empty while the best setting (nucleus sampling
with the xl model) only has 2.3% of the bins empty. This motivates the use of smoothed
distribution estimators even with data-dependent vector quantization.
7.3.3 Direct Estimation from Model Probabilities
In contrast to these previous estimation methods based on model embeddings, we compute
MAUVE directly using the model probabilities Q( ). Since the human probabilities P( ) are
· ·
not available to us, we use the probabilities from GPT-2 xl (without reshaping the model
probabilities) as a surrogate P′. Then, using samples x ,...,x P and x′,...,x′ Q,
1 n 1 n
∼ ∼
we approximate the coordinates of the KL divergence curve by the Monte Carlo estimates
1 (cid:88)n P′(x i)
KL(P λP +(1 λQ)) log ,
∥ − ≈ n λP′(x )+(1 λ)Q(x )
i i
i=1 −
and similarly for KL(Q λP +(1 λQ)).
∥ −
Results. The results are shown in Figure 10. We observe that this direct estimation can
identify the right trend for model size for nucleus sampling, but fails to identify the right
trend for ancestral sampling for medium small large (see Table 5). Similarly, it fails
≺ ≺
to identify the right trends for the decoding algorithm, rating ancestral sampling as better
than nucleus sampling.
45
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Decoding = Ancestral Sampling Decoding = Nucleus Sampling GPT-2 Model size = Small GPT-2 Model size = Large
0.820 0.820
0.791 0.7856
0.815 0.815
0.810 0.790 0.7854 0.810 0.805 0.789 0.805
0.800 0.788 0.7852 0.800
0.795 0.787 0.795
0.790 0.786 0.7850 0.790
0.785 0.785 0.785
S M L S M L 0 0.9 0.92 0.95 0.99 1 0 0.9 0.92 0.95 0.99 1
Model size Model size Nucleus sampling parameter p Nucleus sampling parameter p
Figure 10: Directestimationof MAUVEfrommodelprobabilitiesQ(),usingtheprobabilitiesfrom
GPT-2 xl as a surrogate for the human distribution P(). The Spea·rman rank correlation of this
direct estimation with MAUVE⋆ (the default vector quan·tization with smoothing) is 0.430 and its
worst-case Spearman rank correlation (defined in (30)) with human evaluation scores from Table 5
is 0.371.
7.3.4 Summary and Discussion
TheresultsofthissectionshowthatalltheestimationproceduresconsideredinSection4can
produce useful estimates of the divergence frontier summaries at the right hyperparameter
values, while the direct estimation procedure fails. These experiments suggest that the
particularvectorquantizationisnotakeyfactorbehindtheempiricalsuccessof MAUVE and
refute the argument of Pimentel et al. (2023) that the embedding-based vector quantization
is the key ingredient leading to MAUVE’s strong correlation with human judgments (see
§5.1 for a detailed discussion). We note, however, that vector quantization has orthogonal
benefits such as its simplicity and fast open-source implementation. As we explore in the
upcoming §7.5, a reliable vector embedding turns out to be the key component behind
MAUVE’s strong correlation with human judgment.
7.4 Comparison to Other Divergences and Optimal Transport Costs
Next, we compare our default choice of MAUVE⋆ with different f-divergences and optimal
KL
transport-based distances.
7.4.1 Divergence Frontier Summaries and Other f-Divergences
We compare MAUVEKL with other KL divergence frontier summaries, FI KL, and Mid KL.
We also evaluate the corresponding summaries of the χ2-divergence frontier and two other
divergencemetrics: thetotalvariationdistanceTV(P,Q)andthesquaredHellingerdistance
H2(P,Q). Since we approximate all the f-divergences in question using vector quantization
and Krichevsky–Trofimov (add-1/2) smoothing, we refer to them using their starred names,
e.g., MAUVE⋆ and FI⋆ .
KL KL
Results. The results are given in Table 9. We see that all divergence frontier summaries
correlate perfectly with each other, with a near-perfect Spearman correlation coefficient of
0.99 or higher. Notably, the correlation of FI with the Bradley-Terry human evaluation
KL
coefficients is larger than the other measures, which are all equal (0.93 versus 0.85 for
how human-like the text is). From a closer inspection of the actual values of the various
divergences in Table 16 of Appendix D, we see that FI ranks ancestral sampling for the
KL
xl model as better than nucleus sampling for the small model and agreeing with human
46
EVUAM
MAUVE Scores for Generative Models: Theory and Practice
Correlation MAUVE⋆
KL
FI⋆
KL
Mid⋆
KL
MAUVE⋆
χ2
Mid⋆
χ2
TV⋆ H ⋆2
MAUVE⋆
KL
1.0 0.99 1.0 1.0 1.0 1.0 1.0
BT/Human-like 0.857 0.929 0.857 0.857 0.857 0.857 0.857
BT/Interesting 0.714 0.738 0.714 0.714 0.714 0.714 0.714
BT/Sensible 0.762 0.833 0.762 0.762 0.762 0.762 0.762
Table 9: Comparison of various divergence frontier summaries and f-divergences with the default
MAUVE⋆ andhumanjudgmentsonthewebtextdataset. Weshowtheirworst-caseSpearmanrank
KL
correlation within one standard deviation (defined in Equation (30)).
Decoding = Ancestral Sampling Decoding = Nucleus Sampling GPT-2 Model size = Large GPT-2 Large + Nucleus Sampling
1.6 0.9
0.80
0.65
1.4 0.75 0.8
1.2 0.60 0.70 0.7
0.65
0.55 0.6
1.0 0.60
0.50 0.55 0.5
0.8
0.45 0.50 0.4
0.6 0.45
S M L XL S M L XL 0.9 0.92 0.95 0.99 1 200 400 600 800 1000
Model size Model size Nucleus sampling parameter p Text length
OT / Plug-in OT / Fréchet OT / Quantized
Figure 11: Optimal transport costs for GPT-2 generations in the web text domain. We rescale
each measure by a constant so that all the numbers are O(1). Note that a lower transport cost
denotes a smaller gap between the distributions. Their correlations with MAUVE⋆ (default) and
human evaluations are given in Table 10.
evaluators for how human-like the text is. On the other hand, all other measures (including
MAUVEKL) are not able to distinguish between these two in the sense that they are within
one standard deviation of each other.
7.4.2 Variants based on Optimal Transport
We investigate divergence frontier summaries based on optimal transport costs rather than
f-divergences. Given two distributions P,Q ( ) and a cost function ρ : R+,
∈ P X X ×X →
the optimal transport cost between P and Q induced by ρ is defined as
(cid:26)(cid:90) (cid:27)
OT (P,Q) = min ρ(x,x′)dπ(x,x′) : π ( ) has marginals P,Q .
ρ
∈ P X ×X
X×X
In our context, following Section 4.2, we use the cost function
ρ(x,x′) = φ(x) φ(x′) 2
2
∥ − ∥
based on an embedding model φ : Rd. This is also the squared Wasserstein-2 distance
X →
between the pushforward distributions P′ = φ P and Q′ = φ Q. Similar to Section 4, we
♯ ♯
simplyusetheplug-inestimateOT (Pˆ ,Qˆ )betweentheempiricaldistributionstoestimate
ρ n n
the optimal transport cost – we refer to it as the plug-in optimal transport cost.
We consider quantized versions of this cost following the recipe of Section 4.1. We
quantize the empirical distributions Pˆ and Qˆ into k-dimensional multinomial distribu-
n n
tions Pˆ ,Qˆ ∆k−1. We define a cost ρ (i,j) = c c 2, where c is the cluster
n,k n,k k i j 2 i
∈ ∥ − ∥
47
ecnatsiD cirtem
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Decoding = Ancestral Sampling Decoding = Nucleus Sampling GPT-2 Model size = Large GPT-2 Large + Nucleus Sampling
0.97 0.98
0.95 0.97 0.95
0.92 0.96 0.92 0.97
0.86 0.95 0.86 0.97
0.94 0.78
0.78
0.93 0.63 0.96
0.63 0.39
0.91
0.39 0.0 0.95
S M L XL S M L XL 0 0.9 0.92 0.95 0.99 1 200 400 600 800 1000
Model size Model size Nucleus sampling parameter p Text length
KL + Linear Interpolation (default) OT + Linear Interpolation OT + Barycentric Interpolation
Figure 12: Comparisonofvariantsof MAUVE basedonoptimaltransportcostsforGPT-2genera-
tionsinthewebtextdomain. Largervaluesdenoteasmallergapforeachvariant. Theircorrelations
with human evaluations are given in Table 10.
OT variants MAUVE variants
Correlation OT+ OT+ (Default)KL+
Plug-in Fréchet Quantized Linear Barycenteric Linear
interpolation interpolation interpolation
MAUVE⋆
KL
0.954 0.997 0.980 0.983 0.980 1.000
BT/Human-like 0.810 0.857 0.810 0.810 0.857 0.857
BT/Interesting 0.714 0.714 0.714 0.714 0.714 0.714
BT/Sensible 0.738 0.762 0.738 0.738 0.762 0.762
Table 10: Comparisonofoptimaltransportbaselinesandvariantsof MAUVEdefinedusingoptimal
transport distances with the default MAUVE⋆ and human evaluations on the web text dataset. We
KL
showtheirworst-caseSpearmanrankcorrelationwithinonestandarddeviation(definedin(30))for
the human evaluations.
center obtained from k-means clustering of the embeddings. We refer to the resulting cost
OT (Pˆ ,Qˆ ) as the quantized optimal transport cost.
ρ k n,k n,k
The Fréchet distance (Heusel et al., 2017) is a parametric approximation of OT which
ρ
approximates the pushforwards φ P and φ Q by multi-variate Gaussians. Note that the
♯ ♯
approach of Appendix C for MAUVE follows this recipe. Unlike the methods of Appendix C,
the Fréchet distance has the advantage that it can be computed in closed form.
We also explore variants of the divergence frontier (Theorem 4) based on the optimal
transport cost. Define the optimal transport frontier with linear interpolation as
(P,Q) := (cid:8)(cid:0) OT (P,R ),OT (Q,R )(cid:1) : λ (0,1)(cid:9) , (31)
OT,ρ ρ λ ρ λ
F ∈
where R = λ+(1 λ)Q. Inspired by the original characterization of the KL-divergence
λ
−
frontiers as Pareto frontiers (Djolonga et al., 2020), we define a Pareto frontier of optimal
transport costs. Concretely, we define the optimal transport frontier with barycentric
interpolation as
bary (P,Q) := (cid:8)(cid:0) OT (P,R⋆),OT (Q,R⋆)(cid:1) : λ (0,1)(cid:9) ,
FOT,ρ ρ λ ρ λ ∈ (32)
where R⋆ = argmin λOT (P,R)+(1 λ)OT (Q,R)
λ ρ ρ
{ − }
R
is the barycenter of P and Q with weights λ and 1 λ. While the two formulations are
−
equivalent for the KL divergence as we show in Theorem 3, they are distinct in general for
48
EVUAM
MAUVE Scores for Generative Models: Theory and Practice
optimal transport costs. The definition in (32) is the analogue of (3) for the KL divergence
frontier. We define the corresponding versions of MAUVE, namely MAUVEOT and MAUVEb Oa Try
to be the area under the negative exponential of the frontiers, as in (5).
Computation and Hyperparameter Tuning. Similar to Section 4.1, we estimate the
divergence frontiers (P,Q) and bary (P,Q) on quantized versions (Pˆ ,Qˆ )
and bary (Pˆ ,Qˆ F )O .T To computeFO thT em efficiently, a widely used apF pO rT o, aρ ck h in s,k to an d,k d
entroF piO cT r,ρ ekguln a, rk izatn io,k
n to the optimal transport problem (Cuturi, 2013). Their behavior
depends crucially on the regularization parameter being chosen. A good default choice is
the median of all the pairwise costs.
Results. The results are shown in Figures 11 and 12, and Table 10.
First, we note that the plug-in optimal transport cost fails to capture the correct de-
pendence for the model size as it rates the medium-sized model as worse than GPT-2 small
under nucleus sampling (1499 5 vs. 1473 4, cf. Table 17 in Appendix D). The plug-in
± ±
estimator also fails to capture the dependence on the text length. Similar to the Fréchet
distance in Section 7.2, its numbers suggest that longer model generations drift closer to the
human distribution rather than farther away.
This issue of optimal transport costs can be fixed by vector quantization. Indeed, both
the quantized optimal transport costs and their frontier summary variants capture the cor-
rect dependence in terms of text length, while simultaneously capturing the right trends for
the model size and decoding algorithm. This suggests that vector quantization may have
a regularizing effect on the estimation problem — we leave a deeper exploration of this
phenomenon for future work.
Correlation Analysis. We see from Table 10 that the plug-in optimal transport cost has
a smaller worst-case Spearman correlation of 0.810 with human evaluations. This is smaller
than MAUVEKL, Fréchet, and MAUVEb Oa Try (0.857) and is on par with Gen. PPL. Comparing
the full numbers in Table 17 in Appendix D allows us to find the reason for this discrepancy.
The quantized OT cost rates GPT-2 small and medium models with nucleus sampling (resp.
0.083and0.077)asbetterthanthelargeandxlmodelswithancestralsampling(resp. 0.090
and 0.104; these gaps are larger than the standard deviation of 0.005 across runs). These
trends disagree with human evaluations. MAUVEOT and MAUVEb Oa Try make the same mistake
while MAUVE⋆ identifies the small model with nucleus sampling as being worse than the
KL
large and xl models with ancestral sampling.
Summary and Discussion. Naïve use of optimal transport costs such as the Fréchet
distance (parametric Gaussian approximation) or the empirical estimator in the embedding
space leads to a failure to capture the right trend with respect to the generation length.
This issue is specific to the text setting due to the lack of a temporal dimension for images;
indeed, the Fréchet distance is the de facto standard evaluation metric for image generation.
Optimal transportation in the quantized embedding space (similar to Section 4.1), as well
as frontier summaries that build upon them can overcome this issue.
49
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Generation Model: Grover Generation Model: GPT-2 Generation Model: Grover Generation Model: GPT-2
0.55 0.33
0.97
0.5 0.95
0.26
0.45 0.92
0.96 0.39 0.86 0.18
0.78 Embedding Model
0.95 0.33 0.63 GPT-2 0.1
0.94 0.26 0.39 Grover
0.93 0.0 0.0
Base Large Mega S M L XL 0 0.9 0.92 0.94 0.96 0.98 1 0 0.9 0.92 0.94 0.96 0.98 1
Grover Model Size GPT-2 Model Size Nucleus Sampling Parameter p Nucleus Sampling Parameter p
Figure 13: Effect of embeddings on the news generations. We compare generative models GPT-
2 and Grover using embeddings from both GPT-2 and Grover. The Spearman rank correlation
between MAUVE⋆ (P, ) and MAUVE⋆ (P, ) is 0.971.
Grover · GPT-2 ·
Decoding: Ancestral Sampling Decoding: Nucleus Sampling Generation Model: GPT-2 Small Generation Model: GPT-2 Large
0.92 0.92 0.97
0.86 0.89 0.86 0.95
0.92 0.78 0.86 0.78 0.86
0.63 Embedding Model 0.63 0.78
0.83 GPT-2 0.39 0.63
0.39 RoBERTa 0.39
0.78 0.0 0.0
S M L XL S M L XL 0 0.9 0.92 0.95 0.99 1 0 0.9 0.92 0.95 0.99 1
GPT-2 Model Size GPT-2 Model Size Nucleus Sampling Parameter p Nucleus Sampling Parameter p
Figure 14: Effect of embeddings on web text generations with GPT-2. MAUVE⋆ computed from
GPT-2 embeddings and RoBERTa embeddings have a Spearman rank correlation of 0.962.
7.5 Effect of the Embedding
The results of Sections 7.3 and 7.4.1 suggest that the embedding is a key factor in the
empirical usefulness of MAUVE and other divergence frontier summaries. In this section, we
analyze the effect of the embeddings, experimenting with using the generative model itself,
using masked language models, shallow embeddings, and finally string-based embeddings
that are not learned from data.
7.5.1 Reusing a Generative Model For Embeddings
First, we study whether using the embeddings from the same generative model we are
evaluating might bias the proposed measures toward generations from that model. In par-
ticular, consider two generative models Q
1
and Q 2, and let MAUVEi(P, ) denote the value
·
of MAUVE obtained from using embeddings from model Q
i
for i 1,2 . We check whether
∈ { }
MAUVE1(P,Q 1) > MAUVE1(P,Q 2) but MAUVE2(P,Q 1) < MAUVE2(P,Q 2).
We perform a comparison in the news domain, where P denotes the distribution of
articles in the RealNews dataset. We take Q to the Grover model and Q to be GPT-2,
1 2
both of various sizes and decoding algorithms.9 We use Grover large and GPT-2 large to
compute the embeddings.
The results are given in Figure 13. We observe that the embeddings from both GPT-2
and Grover agree that generations from Grover are closer to the RealNews distribution than
GPT-2. This trend holds uniformly across model sizes and decoding algorithms. Indeed,
9. AlthoughthetrainingdataofGPT-2isproprietary,itsopenversionOpenWebText(GokaslanandCohen,
2019)containsasignificantnumberofnewsarticles(Sharoff,2020). Themostfrequentlyoccurringweb
domains in OpenWebText are news domains (Gehman et al., 2020, Figure 5).
50
EVUAM
EVUAM
EVUAM
EVUAM
EVUAM
EVUAM
EVUAM
EVUAM
MAUVE Scores for Generative Models: Theory and Practice
the Spearman rank correlation between MAUVEGPT-2 and MAUVEGrover is 0.971. Still, there
are some minor differences in the trends revealed by each of the features. For instance,
Grover embeddings suggest that news generations from GPT-2 large are better than those
from GPT-2 xl. Similarly, Grover embeddings suggest p = 0.92 as the best nucleus sampling
hyperparameter for GPT-2 generations, while features from GPT-2 think 0.9 p 1 are
≤ ≤
roughly equivalent.
Overall, we find that the MAUVE scores obtained from both generative models are
strongly correlated, and we do not find any evidence of bias from reusing a generative
model for embeddings.
7.5.2 Masked Language Model Embeddings
So far, we only considered embeddings from left-to-right language models such as GPT-2
andGrover. Inthisnextexperiment, weconsiderusingembeddingsfromamaskedlanguage
model,RoBERTalarge(Liuetal.,2019). Werepeattheexperimentsinthewebtextdomain
with GPT-2 as the generative model and RoBERTa as the embedding model.
The results are given in Figure 14. First, we note that the correlation between MAUVE
computed from GPT-2 embeddings and RoBERTa embeddings has a Spearman rank cor-
relation of 0.962. Second, we observe that RoBERTa embeddings also capture the trends
concerningmodelsizeanddecoding, withsomeminordifferences. Forinstance, bothmodels
identify the greedy ancestral nucleus trend from Section 7.2. While both embedding
≺ ≺
models agree that p = 0.9 is the best nucleus sampling hyperparameter for the small model,
they disagree on generations from the large model. Other baselines such as Gen. PPL. that
do not use embeddings suggest that p = 0.95 is the best hyperparameter, agreeing with
embeddings from GPT-2. We also note that RoBERTa features do not capture the medium
small large xl trend for model sizes under ancestral sampling (cf. Table 5).
≺ ≺ ≺
In summary, the proposed measures computed with masked language models correlate
strongly with those computed from left-to-right language models. They can quantify trends
concerning model size and decoding.
7.5.3 Learned Shallow GloVe Embeddings
Next, we examine MAUVE equipped with learned embeddings predating the advent of trans-
formerlanguagemodels. WerepeatthewebtextexperimentswithGPT-2generationswhere
MAUVE is computed based on the GloVe word embeddings (Pennington et al., 2014).
The GloVe embeddings differ from the deep embeddings of the preceding sections in
two ways. First, they are non-contextual, meaning that a word (e.g. “bank”) has the same
embedding regardless of the context (e.g. “river bank” or the “Bank of America”). Second,
they are embeddings of whitespace-separated words, as opposed to BPE tokens that are
used in transformer language models. Overall, we represent a sequence x = (w ,...,w ) of
1 T
words10 using the average GloVe embedding of words in the vocabulary Vglove:
T
1 (cid:88)
φglove(x) = GloVe(w i) I(w
i
Vglove).
T · ∈
i=1
10. We use w instead of x to emphasize that these are words rather than BPE tokens as in the rest of the
i i
paper.
51
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Decoding = Ancestral Sampling Decoding = Nucleus Sampling GPT-2 Model size = Large GPT-2 Large + Nucleus Sampling
0.95 0.98 0.97 0.98
0.97
0.95
0.92 0.96 0.92 0.97
0.95
0.86 0.86
0.94 0.95
0.78
0.78 0.92
0.89 0.63 0.92
0.63 0.86 0.39
0.39 0.83 0.0 0.86
S M L XL S M L XL 0 0.9 0.92 0.95 0.99 1.0 200 400 600 800 1000
GPT-2 Model size GPT-2 Model size Nucleus sampling paramter p Text Length
GPT-2 embeddings (Default) GloVe embeddings N-gram kernel embeddings Subsequence kernel embeddings
Figure 15: MAUVE fromshallowandstring-basedembeddingsonwebtextgenerationswithGPT-
2.
Correlation GPT-2Embedding GloVeEmbedding N-gramKernel SubsequenceKernel
MAUVE⋆ KL(default) 1.00 0.993 0.727 0.783
BT/Human-like 0.857 0.928 0.500 0.286
BT/Interesting 0.714 0.738 0.262 0.214
BT/Sensible 0.762 0.833 0.429 0.214
Table 11: Correlationof MAUVE⋆ computedfromshallowandstring-basedembeddingswiththe
KL
default GPT-2 embeddings and with human evaluations. For the latter, we show their worst-case
Spearman rank correlation within one standard deviation (defined in Equation (30)).
Results. WenotethattheGloVeembeddingsidentifythekeytrendsconcerningmodelsize,
decoding,andtextlengthinFigure15. Indeed,itsworst-caseSpearmancorrelationwiththe
humanevaluationinTable11iseven(marginally)betterthanthatoftheGPT-2embeddings
(0.93 vs. 0.86). However, the GloVe embeddings have a significant drawback: they come
from a bag-of-words model where word order is irrelevant. As shown in Figure 16, GPT-2
embeddings do not suffer from this drawback. Overall, these results show that MAUVE can
extract useful information from shallow GloVe embeddings, demonstrating the versatility of
MAUVE.
7.5.4 String-based Kernel Embeddings
Next, wecompute MAUVE directlyfromstrings, withoutanylearnedembeddings, shallowor
deep. Concretely, we consider the embeddings implied by a positive definite kernel κ(x,x′)
between text sequences x,x′.
Recall that a kernel κ : R+ over a space is said to be positive definite
X × X → X
if the Gram matrix K Rr×r with entries [K]
i,j
= κ(x i,x j) defined by any collection
∈
x ,...,x of r inputs is a symmetric and positive definite matrix for all integers
1 r
∈ X
r; we refer to the textbook (Shawe-Taylor and Cristianini, 2004) for background. A key
propertyofpositivedefinitekernelsisthattheycanbeviewedasdotproductsinanabstract
feature space. Specifically, Mercer’s theorem states that there is a unique feature map
φ : onto a Hilbert space equipped with an inner product , such that
κ H
X → H H ⟨· ·⟩
κ(x,x′) = φ (x),φ (x′) for all x,x′ (Mercer, 1909).
κ κ H
⟨ ⟩ ∈ X
We compute MAUVE using these embeddings φ
κ
induced by two string kernels, where
X
is the space of text sequences (i.e., strings):
52
EVUAM
MAUVE Scores for Generative Models: Theory and Practice
MAUVE(P,Q) MAUVE(P, Permute(Q))
0.97 0.97
0.95 0.95
0.92 0.92 Embedding
0.86 0.86 GPT-2 (default)
0.78 0.78 GloVe
0.63 0.63
0.39 0.39
0.0 0.0
0 0.9 0.92 0.95 0.99 1.0 0 0.9 0.92 0.95 0.99 1.0
Nucleus sampling paramter p Nucleus sampling paramter p
Figure 16: Robustness to permutations at the word level: MAUVE with GPT-2 embeddings is
sensitive to the order of words whereas GloVe embeddings are not. We define Permute(Q) as the
distribution over word sequences (w ,...,w ) where (w ,...,w ) Q and π is a uniformly
π(1) π(n) 1 n
random permutation on [n]. ∼
(a) N-gram kernel: Given an integer N, the N-gram kernel is defined as the ratio of
common N-grams of its inputs to the total number unique of N-grams (Shawe-Taylor
and Cristianini, 2004, Sec. 11.2). Specifically, letting A (x) denote the set of all
N
N-grams in the sequence x, the N-gram kernel κ is defined as
N
A (x) A (x′)
κ (x,x′) = | N ∩ N | .
N A (x) A (x′)
N N
| ∪ |
This is also the Jaccard similarity between the set of N-grams of x and those of x′.
(b) Subsequence kernel: The subsequence kernel (Lodhi et al., 2002) is based on the
number of common (non-contiguous) subsequences of length N and scaled by the gap
using a decay factor λ (0,1), known also as the gap penalty. Concretely, the feature
∈
map φ used to define the subsequence kernel κ has one component for every
N,λ N,λ
possible length-N sequence z VN. The corresponding component is zero if z is not
∈
a subsequence of x, else it is
φ (x)[z] =
(cid:88) λlen(i),
N,λ
i:z=x[i]
where i is an index sequence and x[i] is the subsequence of x obtained by selecting
the indices from i, and len(i) = i i +1 is the length of the subsequence in x.
|i| 1
−
A naïve implementation of κ (x,x′) has a complexity of O( V N) but it can be
N,λ
| |
implemented using sparse dynamic programming in O(NM log x ) time, where M =
| |
(i,j) : x = x′ is the total number of matches between x and x′ (Rousu et al.,
|{ i j}|
2005).
WecomputeMAUVEfromtherespectiveembeddingsofthesetwokernelsatthelevelofword-
piece tokens using the nearest neighbor method of §4.2. To keep the MAUVE computation
time to under two hours, we use n = 800 samples for the N-gram kernel and n = 200
samples for the subsequence kernel. We sweep over the hyperparameters N 3,4,5
∈ { }
and λ 0.1,0.2,...,0.9 of the kernels and report the hyperparameters that have the
∈ { }
highest correlation with the human evaluation: these are N = 3 for the N-gram kernel and
N = 5,λ = 0.5 for the subsequence kernel.
53
EVUAM
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Results. Figure 15 shows the dependence of MAUVE on the trends concerning model size,
decoding, and text length. We see that string kernel embeddings only identify these trends
weakly and unreliably, i.e., the mean across 5 runs trend is as expected but the gaps are
often smaller than the standard deviation across runs. This is true of all three trends but
take the text length as an example. MAUVE from the subsequence kernel at a length of 512
tokens is 0.879 0.013, which is smaller than 0.889 0.010 at length 256 and larger than
± ±
0.871 0.005 at 1024 tokens, but all three numbers are within one standard deviation of
±
each other. Similarly, we see from Table 11 that the worst-case Spearman correlations with
the human evaluation results are small, always under 0.5. This shows that the raw strings
are not informative enough for MAUVE.
7.5.5 Summary and Discussion
The results of this subsection demonstrate the importance of the embedding to the use-
fulness of MAUVE. The poor performance of N-gram and subsequence kernels, and direct
model probabilities (Section 7.3.3) show that some care must be taken to use informative
embeddings. Yet, MAUVE is versatile enough to leverage information from a wide variety of
embeddings, including language model embeddings (left-to-right LMs, even if it has been
used for generation, or masked LMs), and shallow non-contextual embeddings.
7.6 Comparison to Generative Precision and Recall
Metrics based on divergence frontiers have been previously used extensively in the computer
vision community (Sajjadi et al., 2018; Kynkäänniemi et al., 2019; Djolonga et al., 2020).
How do these metrics fare in the evaluation of text generative models? We now examine the
applicability of the most widely used such metrics, i.e., Kynkäänniemi et al.’s precision and
recall for generative models, in the web text domain.
Definitions. These notions of precision and recall rely on whether a point x lies within
the manifold of a set of samples Y. Concretely, letting dist (z,Y) denoted the distance of
k
z to its kth neighbor in Y, define
(cid:40)
1, if z Y : ρ(x,z) dist (z,Y),
k
s (x,Y) := ∃ ∈ ≤
k 0, else.
Usingthisnotion, thegenerativeprecisionandrecall(evaluatedwithk nearestneighbors)of
a generative distribution Q relative to a target distribution P based on n samples X Qn
Q
∼
and X Pn are defined as
P
∼
1 (cid:88) 1 (cid:88)
Precision (X ,X ) = s (x′,X ), and Recall (X ,X ) = s (x,X ).
k P Q k P k P Q k Q
n n
x′∈XQ x∈XP
Intuitively, the precision is high if the generated data looks more human-like (i.e., plausibly
drawn from P) and the recall is high if the generative model captures the diversity of the
target distribution P. Higher values of both precision and recall are desirable. We find that
all 1 k 25 produced the same qualitative trends, so we show the results for k = 5.
≤ ≤
54
MAUVE Scores for Generative Models: Theory and Practice
Decoding = Ancestral Sampling Decoding = Nucleus Sampling GPT-2 Model size = Large GPT-2 Large + Nucleus Sampling
0.89 0.82
0.83
0.86 0.83 0.8
0.83 0.8 0.8 0.78
0.78 Precision 0.75 0.75 0.75
0.71 Recall 0.7 0.7 0.73
00 .. 56 33 0.63 0.63 0.0 6. 77
0.39 0.55 0.55 0.63
0.22 0.45
S M L XL S M L XL 0 0.9 0.92 0.95 0.99 1 200 400 600 800 1000
GPT-2 Model Size GPT-2 Model Size Nucleus sampling parameter p Text Length
Figure 17: Empiricalbehaviorofgenerativeprecisionandrecall(originallyproposedbyKynkään-
niemi et al. (2019) for evaluating GANs in computer vision) for natural language generation in the
web text domain. Higher values denote better performance.
Results. Figure 17 shows the trends with respect to model scale, decoding algorithm,
and text length. Given that larger language models are generally better text generators,
we expect the precision and recall to both increase with the model scale. We see for both
ancestral and nucleus sampling that the recall increases as expected. However, the preci-
sion decreases with increasing model scale; this suggests that smaller models produce more
human-like text, which is qualitatively untrue.
Next, we consider the effect of decoding in terms of the nucleus sampling parameter p.
Prior work suggests that p [0.9,0.95] should give the most human-like text while p = 1
∈
gives the most diverse text (Holtzman et al., 2020). Thus, we would expect the precision to
peak in p [0.9,0.95], while we expect the recall to increase with p monotonically. We see
∈
that the actual trends are the exact opposite of what we would expect, i.e. p = 1 produces
the most human-like text whereas p = 0.9 best matches the diversity of human text, both
of which are qualitatively untrue.
Finally,sincemodeltextgenerationsdegradeastheygetlonger,weexpectbothprecision
and recall to get worse with text length. Again, the precision metric says that the generated
text gets more human-like as its length increases, which is untrue.
Summary and Discussion. In summary, these results demonstrate that the notion of
generative precision and recall proposed by Kynkäänniemi et al. do not behave as expected
for natural language generation. In contrast, MAUVE identifies the expected behavior with
respect to model size, decoding algorithm, and text length.
7.7 Evaluating Image Generative Models with MAUVE
In this section, we explore the applicability of our approach to measure the gap between a
distributionQofimagesgeneratedbyaneuralnetanditstargetdistributionP ofreal-world
images.
Setup. We study the distribution of images generated by models trained on the Flickr-
Faces-HQ Dataset (FFHQ) (Karras et al., 2019). The models we consider are based on the
StyleGAN2-ADA generative adversarial networks described by Karras et al. (2020a).
As a representative divergence frontier summary, we consider MAUVE⋆ computed using
KL
quantization with k = 1000 clusters. We use 50,000 samples from the model in comparison
to 50,000 samples from the FFHQ training data, unless specified otherwise. The resolution
of each image is 1024 1024. Note that in the language modality, we compute MAUVE
×
55
EVUAM
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
MAUVE Fréchet Distance Generative Precision Generative Recall
0.6
0.95 245 0.90
148 0.85 0.5
0.92
90 0.80 0.4
0.86
55 0.75
0.78 0.3
33 0.70
0.63 20 0.65 0.2
0.39 12 0.60 0.1
0.0 7 0.55
0.0 0.3 0.7 1.0 1.2 0.00 0.25 0.50 0.75 1.00 1.25 0.4 0.6 0.8 1.0 1.2 0.4 0.6 0.8 1.0 1.2
Sampling Parameter Sampling Parameter Sampling Parameter Sampling Parameter
=0.3 =0.7 =1 =1.2
Figure 18: Evaluating image generative models across sampling algorithms with MAUVE, Frećhet
distance (Heusel et al., 2017) and generative precision-recall (Kynkäänniemi et al., 2019) (top row).
Somesampleimagesgeneratedatvarioussamplingparametersareshowninthebottomrow. Weuse
the StyleGAN2-ADA model (Karras et al., 2020a) with various values of the ψ-sampling parameter
ψ as the model distribution Q and compare it with the reference distribution P over the FFHQ
dataset. Thegenerationsshownateachthresholdψ aregeneratedfromthesameinitialrandomness
for a given position in the grid. We recommend zooming in for a closer inspection of the generated
images.
using samples from the test set whereas here–following standard practice in vision when
comparing distributions using Inception Score or Fréchet distance–we use samples from the
train set. Similar to these baselines, we use as an embedding model the standard features of
anInceptionnetworkpre-trainedonImagenet. ThissettingforFréchetdistancecorresponds
exactly to the FID-50k metric commonly used in the vision literature. We also compare to
the generative precision-recall (Kynkäänniemi et al., 2019); cf. §7.6 for definitions.
7.7.1 Effect of the Sampling
We consider samples drawn from the GAN model using ψ-sampling, a technique that biases
sampling towards modes of the model distribution.11
We briefly describe ψ-sampling. The generator function of these models maps a simple
randomlatentvariablez (0,I )toanimagex = g(z) drawnfromthepushforward
Z
∼ N ∈ X
distribution defined by a learned generator function g : . The generator itself is
Z → X
decomposed into g = s h consisting of an embedding mapping function h :
◦ Z → W
and synthesis network s : . Let w∗ = [h(z)] be the average embedding
W → X
Ez∼N(0,IZ)
of noise. Given z (0,I ), we define ψ-sampling using a modified generator function
Z
∼ N
defined by
g (z) = s(w∗+ψ(h(z) w∗)).
ψ
−
11. ψ-sampling is referred to as truncation by Karras et al. (2020a).
56
MAUVE Scores for Generative Models: Theory and Practice
MAUVE Fréchet Distance Generative Precision Generative Recall
0.97 18.0
0.74 0.60
0.96 17.5
0.73
0.95 17.0 0.58 0.72
0.94 16.5 0.56
0.71
0.93 16.0 0.70 0.54
15.5
0.91 0.69 0.52
15.0
0.89 0.68 0.50
14.5
v1 v2 v3 XL v1 v2 v3 XL v1 v2 v3 XL v1 v2 v3 XL
StyleGAN version StyleGAN version StyleGAN version StyleGAN version
Figure 19: ComparingStyleGANmodelgenerationswith MAUVE,Frećhetdistance(Heuseletal.,
2017) and generative precision-recall. We compare: (v1) the original StyleGAN (Karras et al.,
2019), (v2) StyleGAN2-ADA (Karras et al., 2020a), (v3) StyleGAN3 (Karras et al., 2021), and
(XL) StyleGAN-XL (Sauer et al., 2022). These plots use 5000 samples for each metric, and the
shaded region denotes the standard deviation across 10 runs with different subsamples of the target
distribution.
If ψ < 1, this transformation linearly contracts the mapped value h(z) towards the
∈ W
mean mapping w∗. Intuitively, this will result in higher probability, but less diverse, output
images. In contrast, ψ > 1 will emphasize the lower probability regions of the image space,
resulting in more diverse images of lower quality.
Results. The results are given in Figure 18. Both MAUVE and Fréchet distance identify
the same ordering of ψ: 1 1.2 0.7 0.3 0. Qualitatively, we observe the expected
≻ ≻ ≻ ≻
quality-diversity tradeoff as we vary ψ. The extreme ψ = 0.3 produces high-quality images
of faces that look very similar to each other. At ψ = 0.7, we observe more diversity in the
generated faces over attributes such as hair color and style, eyewear, and other factors. We
get a greater diversity at ψ = 1 with more diversity in hair and eyewear but also in the
direction the generated face points towards and facial expressions. At ψ = 1.2, we that
the generated faces start to appear distorted. Some images also feature parts of a second
face. The notions of generative precision and recall capture both quality and diversity
trends, as was demonstrated in previous work. Thus, similar to Fréchet distance, MAUVE
accounts for both quality and diversity to produce a single measure of the gap between the
model distribution and the target distribution. Unlike both precision-recall and the Fréchet
distance, however, MAUVE also perfectly identifies various trends in the natural language
modality.
7.7.2 Effect of the Model Scale and Architecture
We compare image generative models across different model architectures, analogous to the
effectofthemodelscalein§7.2. Fortheseexperiments,weuse5000samplestocomputeeach
metric(comparedtothe50000samplesusedfortheexperimentsinthepreviousexperiment).
Effect of Model Scale and Architectural Improvements. We compare various gen-
erations of StyleGAN models. Each model in this family builds upon the previous one
with innovations in the architecture and training pipeline to address certain artifacts in the
generated images. We consider the following models:
(a) StyleGAN (Karras et al., 2019): the first model in this family with 26 million pa-
rameters.
57
cirteM
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
(b) StyleGAN2-ADA (Karras et al., 2020b,a): the second model in this family, with 30
million parameters.
(c) StyleGAN3 (Karras et al., 2021): this model makes substantial changes to the ar-
chitecture of StyleGAN2. StyleGAN3 has only 15 million parameters but produces
image generations of similar quality as StyleGAN2-ADA as per standard metrics like
the Fréchet distance. The authors claim that it is better suited to video generation.
(d) StyleGAN-XL (Sauer et al., 2022): the largest model in this family that we consider
with 71 million parameters.
All images were produced using ψ-sampling with ψ = 1.
The results are shown in Figure 19. We find that both MAUVE⋆ and Frećhet distance
find the same trends: more recent models are better with StyleGAN2-ADA and StyleGAN3
being rated almost the same (i.e., with one standard deviation of each other). Notably, the
most recent and the largest model — StyleGAN-XL — produces the best images as per
these metrics.
On the other hand, generative precision (Kynkäänniemi et al., 2019) rates the oldest
StyleGAN model as producing the most photorealistic images (highest precision). This fails
to pass the visual inspection test, as the subsequent works in the StyleGAN family discuss
the flaws of this model’s generations and are designed to improve them. This is similar to
the text domain where generative precision finds that the smallest GPT-2 model produces
the most human-like text. Thus, designing fine-grained fidelity and diversity metrics for
generative models that can be used reliably across model scales and families remains an
important open problem.
Comparing GANs with Diffusion Models. We compare StyleGAN2-ADA with a dif-
fusion model NCSN++ (Song et al., 2021) on the FFHQ domain. NSCN++ is the first
diffusion model to directly generate high-resolution images of 1024 1024 pixels (without
×
up-sampling lower-resolution images in a multi-step pipeline). Stein et al. (2023) show
that diffusion models perform significantly worse than GANs on metrics computed in the
Inception-V3 embedding space despite being comparable or better generators in terms of
both fidelity (as measured by human evaluations) and diversity. We follow their recom-
mendation and use embeddings from a DINOv2 model (Oquab et al., 2023) (specifically, its
ViT-L/14 configuration), which was shown to not have such a bias.
The results are given in Table 12: StyleGAN2-ADA (ψ = 1) outperforms the diffu-
sion model by a large margin as per both MAUVE and Fréchet distance. Figure 20, which
shows some samples from the diffusion model, explains the source of this large disparity.
These generations contain more artifacts than the GANs generations (shown in Figure 18),
including glaring asymmetries in facial features such as hairs or eyes.
Many successful diffusion-based generative models such as DALL-E 2 (Ramesh et al.,
2022) and Imagen (Saharia et al., 2022) adopt a two-step pipeline: (a) generate low-
resolution images with a diffusion model (e.g. 64 64 pixels), and (b) upsample the gen-
×
eration using one or more super-resolution models (e.g. 642 2562 10242 pixels). Our
→ →
results above show that end-to-end diffusion modeling to directly generate high-resolution
images remains an important open problem.
58
MAUVE Scores for Generative Models: Theory and Practice
Model Fréchet Distance MAUVE⋆
KL
StyleGAN2 298.59 0.979
2.02 0.034
Diffusion NCSN++ 646.49 0.648
5.80 0.002
Table12: GANsvs. diffusionmodels: Com-
paring StyleGAN2-ADA with the diffusion model
NCSN++ (Song et al., 2021). We use features
from the DINOv2 (ViT-L/14) model and each
metric is computed using 5000 samples. The sub-
scriptdenotesthestandarddeviationover10runs
with different subsamples of the target distribu-
tion. MAUVE⋆ is computed using vector quantiza- Figure 20: Samplesfromthediffusion
tion of size k =100. model NCSN++ (Song et al., 2021).
7.7.3 Summary
These results, together with those from the preceding sections, indicate that the general
recipe of approximating gaps between distributions of complex high-dimensional objects
using embeddings from a pre-trained deep net using f-divergence frontiers and MAUVE is a
powerful one.
7.8 Tightness of the Statistical Error Bounds
We conduct a numerical study to empirically investigate the tightness of the statistical
error bounds presented in Theorem 10. Using the frontier integral FI as a representative
KL
summary of the f-divergence frontier, we investigate the estimation error in divergence
frontier summaries as a function of the sample size n and the quantization size k from
samples.
We consider two domains: text generation in the web text domain using a pretrained
GPT-2 large and nucleus sampling with p = 0.95 (§6) and face image generation using a
StyleGAN2-ADA model pretrained on FFHQ sampled using ψ = 1 (§7.7).
We study the statistical error incurred by the plug-in estimator using n samples to esti-
matethepopulationdivergence,whereeachpopulationdistributioncontainsN texts/images
(N = 5000 for the text domain and N = 50000 for the image domain). Following the recipe
of §4.1, we first represent each text/image by its features. Next, we quantize these 2N fea-
tures into k bins using k-means clustering. For each support size k, this gives us quantized
distributions P and Q . Then, we sample n i.i.d. examples from each of the two distri-
S S
k k
butions and use their empirical versions Pˆ and Qˆ to compute FI(Pˆ ,Qˆ ). We
S ,n S ,n S ,n S ,n
k k k k
estimate the statistical error E FI KL(Pˆ S ,n,Qˆ S ,n) FI KL(P S ,Q S ) from a Monte Carlo
| k k − k k |
estimate using 100 random trials and compare it with two bounds from Theorem 10:
(a) Bound: the distribution independent bound ((cid:112) k/n+k/n)logn, and
(b) Oracle Bound: the distribution dependent bound (α (P)+α (Q))logn+β (P)+
n n n
β (Q) assuming the quantities α and β (defined in Theorem 10) are known.
n n n
We fix the support size (i.e., the quantization size) k and plot each of these quantities in a
log-log plot with varying n and compare their slope.12 We then repeat the experiment with
12. A log-log plot of the function f(x)=cxλ is a straight line with slope λ, which thus captures the degree.
59
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
(a) Text (k=64) (b) Images (k=1024) (c) Text (n=1000) (d) Images (n=10000)
100
101
102
103
102 103 104 102 103 104 101 102 103 101 102 103
Sample size Sample size Support size Support size
Monte Carlo Oracle bound Bound
Figure 21: StatisticalerroroftheestimatedfrontierintegralFI onrealtext/imagedata,
KL
as a function of the sample size n and support size k. (a): Text data with k = 64; (b):
Image data with k = 1024; (c): Text data with n = 103; (d): Image data with n = 104.
These bounds are scaled by 30.
n fixed and k varying. We scale the bounds by a factor of 30 for easier visual comparison of
their slopes; this only changes the intercept and leaves the slope unchanged.
Results. Figure 21 contains the Monte Carlo estimate and the bounds of the statistical
error for real text and image data. In Figure 21(b), we see that the oracle bound captures
the right rate for small sample sizes where k/n > 1. Whereas, for large n, the distribution-
independent bound is better at matching the slope of the Monte Carlo estimate. The same
is true for Figure 21(c), where the oracle bound is better for large k. For parts (a) and
(d), however, both bounds do not capture the right slope of the Monte Carlo estimate;
Theorem 10 is not a tight upper bound in this case. Yet, we notice that Theorem 12 is
still a valid upper bound. Indeed, for part (a), we observe that the rate of decrease of the
Monte Carlo estimate is only faster than the bound but not slower. Overall, these results
demonstrate the favorable statistical error properties of MAUVE.
8 Empirical Recommendations
Following the introduction of MAUVE in the conference paper (Pillutla et al., 2021), it has
been adopted by the language modeling community for measuring performance and hyper-
parametertuningindiverselanguagegenerationsettings,includingcontrastivedecoding(Su
et al., 2022; Li et al., 2023), truncation decoding (Meister et al., 2022; Hewitt et al., 2022),
and momentum decoding (Lan et al., 2022); controllable text generation (Yang et al., 2023);
architectural innovations (Hu et al., 2022); and differentially private language generation
(Mattern et al., 2022; Yue et al., 2023; Kurakin et al., 2023).
We review some subtleties of using the proposed measures in practice and offer some
practical guidelines.
Aligning Automatic Evaluation to the Goal of Generative Modeling. A common
objective of generative modeling is to exactly match the model distribution Q to a real data
distribution P. As discussed in Section 3, this can fail due to a Type I error, where the
modelproducesunrealisticorlow-qualitydata,oraTypeIIerror,wherethemodelisunable
60
rorre
etulosbA
MAUVE Scores for Generative Models: Theory and Practice
to produce some plausible real samples and fails to capture the diversity of real data. On
the other hand, there are scenarios where ensuring a low Type I error is the only objective
of generation (and matching the target P is not important). For instance, correctness is
the key objective in machine translation, while using a diverse vocabulary is not the main
concern.
The proposed divergence frontier summaries, as measures of the gap between a model
distribution Q and real data distribution P, are well-suited for the first objective and are
ill-suited for the second one. For instance, in the context of open-ended text generation,
(Su and Xu, 2022) empirically show that contrastive search has a lower MAUVE score than
nucleussamplingwhileproducinghigherqualitytext(i.e.,lowertypeIerror)asinferredfrom
human evaluations. This can be explained by a large type II error in contrastive search,
leading to a large gap or smaller MAUVE score. Indeed, each token in contrastive search
is chosen deterministically from its top-K vocabulary for small K < 10, so it can fail to
generate the occasional surprising or low-probability words found in human text (Holtzman
et al., 2020).
In summary, we recommend the use of the proposed divergence frontier summaries when
the goal of the generative model is to match both the quality and diversity of the target real
data distribution.
Relative Comparisons Instead of Absolute Scores. We find that the proposed meth-
ods are best suited for relative comparisons while the absolute scores are less meaningful.
For instance, if we wish to find which model distribution among Q and Q has a smaller
1 2
gap to the target distribution P, we can compare MAUVE(P,Q 1) to MAUVE(P,Q 2). The
individual value of MAUVE(P,Q i) can vary based on the computational approximation, its
hyperparameters,andthenumberofsamples. Indeed,weonlyconsidertherankingsinduced
by MAUVE in Section 7 by comparing the Spearman rank correlation with other rankings.
Randomness and Standard Deviations. There are multiple sources of randomness in
the computation of MAUVE: the randomness from sampling for stochastic decoding algo-
rithms, as well as the random initialization for k-means quantization. Since the absolute
values of the proposed measures are not meaningful, the standard deviations are equally
important in making relative comparisons. We strongly recommend taking into account the
standard deviation across multiple runs rather than just the mean even for relative compar-
isons; the worst-case Spearman rank correlation defined in (30) is one such measure. We
also observed that, while the proposed measures can capture the basic properties as in Sec-
tion 7.2, it is much harder to quantify subtle differences (e.g., when trying to improve over
nucleus sampling). In this case, we recommend increasing the sample size or the number of
random seeds to reduce the uncertainty in the statistical estimation.
Sample Size and Text Length. The greater the number of samples, the smaller the
statisticalestimationerror(cf. Section4). Werecommendempiricallythateachdistribution
contains at least 1000 samples. The proposed measure computed with a smaller number of
samples is biased towards optimism (that is, the score typically goes down as the number
of samples increases) and exhibits a larger standard deviation. Likewise, we find that the
proposed measures can capture the gap between long texts (at least 256 tokens, preferably
512 tokens) but they might not always capture the difference between shorter texts (see the
61
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
overlapping shaded areas denoting the standard deviation in Figure 8). In Section 7, we use
5000 samples of up to 1024 tokens (with a prefix length of 35) to compute MAUVE and we
report the mean and standard deviation over 5 repetitions.
Acknowledgments and Disclosure of Funding
Part of this work was done while Zaid Harchaoui was visiting the Simons Institute for
the Theory of Computing, and while Krishna Pillutla, Lang Liu, John Thickstun, and
Rowan Zellers were at the University of Washington. This work was supported by NSF
DMS-2134012, NSF CCF-2019844, NSF DMS-2023166, the DARPA MCS program through
NIWC Pacific (N66001-19-2-4031), the CIFAR “Learning in Machines & Brains” program,
a Qualcomm Innovation Fellowship, and faculty research awards.
Appendix
The outline of the appendix is as follows:
• Appendix A: Complete proofs of divergence frontier properties from Section 3.
• Appendix B: Full proofs of estimation via quantization from Section 4.1.
• AppendixC:DetailsoftheparametricapproximationapproachmentionedinSection4.
• Appendix D: Additional experimental results to augment those in Section 7.
• Appendix E: Additional details of the human evaluations described in Section 6.4.
Appendix A. Properties of the Divergence Frontiers
We give a closed-form expression for Frontier Integral, for the special case of the KL diver-
gence.
Property 18 The integral summary Frontier Integral of the KL divergence frontier is an
f-divergence generated by the convex function
t+1 t
f˜ (t) = logt,
KL
2 − t 1
−
with the understanding that f˜ (1) = lim f˜ (t) = 0,
KL t→1 KL
Proof Let P and Q be dominated by some probability measure µ with density p and q,
respectively. We will establish the expression
(cid:90) (cid:18) (cid:19)
p(x)+q(x) p(x)q(x) p(x)
FI(P,Q) = 1 p(x) = q(x) log dµ(x), (33)
{ ̸ } 2 − p(x) q(x) q(x)
X
−
with the convention 0log0 = 0. This gives the expression for f˜ from the definition of an
KL
f-divergence.
Wenowestablish(33). Denoteλ¯ = 1 λ. ByTonelli’stheorem,itholdsthatFI (P,Q) =
KL
2(cid:82) h(p(x),q(x))dµ(x), where −
X
(cid:90) 1
h(p,q) = (cid:0) λplogp+λ¯qlogq (λp+λ¯q)log(λp+λ¯q)(cid:1) dλ.
−
0
62
MAUVE Scores for Generative Models: Theory and Practice
When p = q, the integrand is 0. If q = 0, then the second term inside the integral is 0,
while the first term is (cid:82)1 λplog(1/λ)dλ = p/4. Finally, when p = q are both non-zero, we
0 ̸
evaluate the integral to get,
p q 2p2logp p2 2q2logq+q2
h(p,q) = logp+ logq − − ,
2 2 − 4(p q)
−
and rearranging the expression gives (33).
Next, we give a technical lemma used to establish properties of FI and Mid .
f f
Proposition 19 Let P,Q ( ) be probability measures with finite support. Then, the
∈ P X
linearized cost L defined in Equation (6) satisfies the bound
f,λ
L (P Q) λf∗(λ)+(1 λ)f∗(1 λ)+2λ(1 λ)f(0).
f,λ
∥ ≤ − − −
Proof Denote λ¯ = 1 λ. Let P,Q ∆k−1 be discrete distributions over k < items. The
− ∈ ∞
function P,Q L (P Q), by virtue of being an f-divergence, is jointly convex in P,Q.
f,λ
(cid:55)→ ∥
So, L (P Q) is maximized for P⋆,Q⋆ that lie at some vertices of the probability simplex
f,λ
∥
∆k−1. We can rule out P⋆ = Q⋆ as L (P Q) = 0 in this case. Therefore, without loss of
f,λ
∥
generality, we can assume that P⋆ = (1,0,...,0) ∆k−1 and Q⋆ = (0,1,0,...,0) ∆k−1.
∈ ∈
Plugging this in gives the upper bound
L (P⋆ Q⋆) = λ2f(1/λ)+2λλ¯f(0)+λ¯2f(1/λ¯) = λf∗(λ)+λ¯f∗(λ¯)+2λλ¯f(0).
f,λ
∥
Appendix B. Proofs of Theoretical Bounds: Quantization
In this section, we give the complete proofs of quantization in Section 4.1. The outline is as
follows:
• Appendix B.1: Proof of the statistical error bound for the empirical estimator (Theo-
rem 10).
• Appendix B.2: Proof of the statistical error bound for the add-constant estimator
(Theorem 12).
• Appendix B.3: Proof of the quantization error bound (Theorem 13).
B.1 Statistical Error Bound
In this section, we prove Theorem 10.
The proof relies on two key lemmas—the approximate Lipschitz lemma (Theorem 20)
and the missing mass lemma (Theorem 22). The argument breaks into two cases in P (and
analogously for Q) for each atom a :
(a) Pˆ > 0: Since Pˆ is an empir∈ icaX l measure, we have that Pˆ 1/n. In this case the
n,a n n,a
approximate Lipschitz lemma gives us the Lipschitzness in P ≥ Pˆ up to a factor
n TV
∥ − ∥
of logn.
63
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
(b) Pˆ = 0: Inthiscase,themasscorrespondingtoP ismissingintheempiricalmeasure
n,a a
and we directly bound its expectation following similar arguments as in the missing
mass literature; see, e.g., (Berend and Kontorovich, 2012; Mcallester and Ortiz, 2003).
B.1.1 Approximate Lipschitz Property
First, we express the derivatives of ψ(p,q) = qf(p/q) in terms of the derivatives of f:
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
∂ψ p q q q
(p,q) = f′ = f∗ (f∗)′ (34a)
∂p q p − p p
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
∂ψ p p p q
(p,q) = f f′ = (f∗)′ (34b)
∂q q − q q p
∂2ψ 1 (cid:18) p(cid:19) q2 (cid:18) q(cid:19)
(p,q) = f′′ = (f∗)′′ 0 (34c)
∂p2 q q p3 p ≥
∂2ψ p2 (cid:18) p(cid:19) 1 (cid:18) q(cid:19)
(p,q) = f′′ = (f∗)′′ 0 (34d)
∂q2 q3 q p p ≥
∂2ψ p (cid:18) p(cid:19) q (cid:18) q(cid:19)
(p,q) = f′′ = (f∗)′′ 0, (34e)
∂p∂q −q2 q −p2 p ≤
where the inequalities f′′,(f∗)′′ 0 followed from convexity of f and f∗ respectively.
≥
We now present the main lemma that shows that the function ψ is nearly Lipschitz, up
to a log factor. This lemma can be leveraged to directly obtain a bound on the statistical
error of the f-divergence in terms of the expected total variation distance, provided the
probabilities are not too small.
Lemma 20 Suppose that f satisfies Assumption 9. Consider ψ : [0,1] [0,1] [0, )
× → ∞
given by ψ(p,q) = qf(p/q). We have, for all p,p′,q,q′ [0,1] with p p′ > 0, q q′ > 0,
∈ ∨ ∨
that
(cid:18) (cid:26) (cid:27) (cid:19)
1
ψ(p′,q) ψ(p,q) C max 1,log +C∗ C p p′
| − | ≤ 1 p p′ 0 ∨ 2 | − |
∨
(cid:18) (cid:26) (cid:27) (cid:19)
1
ψ(p,q′) ψ(p,q) C∗max 1,log +C C∗ q q′ .
| − | ≤ 1 q q′ 0 ∨ 2 | − |
∨
Proof We only prove the first inequality. The second one is identical with the use of f∗
rather than f. Suppose p′ p. From the fact that ψ is convex in p together with a Taylor
≥
expansion of ψ( ,q) around p′, we get,
·
∂ψ 1 (cid:90) p ∂2ψ
0 ψ(p,q) ψ(p′,q) (p p′) (p′,q) = (s,q)(p s)ds
≤ − − − ∂p 2 ∂p2 −
p′
p (cid:90) p′ ∂2ψ 1 (cid:90) p′ ∂2ψ
= (s,q)ds+ s (s,q)ds
−2 ∂p2 2 ∂p2
p p
0+C (p′ p),
2
≤ −
where we used ∂2ψ/∂p2 is non-negative due to convexity and, by (34c) and Assump-
tion (A3),
∂2ψ s
s (s,q) = f′′(s/q) 2C .
∂p2 q ≤ 2
64
MAUVE Scores for Generative Models: Theory and Practice
This yields
∂ψ ∂ψ
(p′ p) (p′,q) ψ(p,q) ψ(p′,q) (p′ p) (p′,q)+C (p′ p).
2
− − ∂p ≤ − ≤ − − ∂p −
We consider two cases based on the sign of ∂ψ(p′,q) = f′(p/q) (cf. Eq. (34a)).
∂p
Case 1. ∂ψ(p′,q) 0. Since q f′(p/q) is decreasing in q, we have
∂p ≥ (cid:55)→
∂ψ
0 (p′ p) (p′,q) = (p′ p)f′(p/q) lim(p′ p)f′(p/q) = (p′ p)f∗(0),
≤ − ∂p − ≤ q→0 − −
where we used f′( ) = f∗(0) from Theorem 26. From Assumption (A1), we get the bound
∞
ψ(p,q) ψ(p′,q) (C∗ C )(p′ p).
0 2
| − | ≤ ∨ −
Case 2. ∂ψ(p′,q) < 0. By Assumption (A2), it holds that
∂p
(cid:12) (cid:12)
(cid:12) (cid:12)∂ψ (p′,q)(cid:12) (cid:12) C 1 max 1,log(q/p′) C 1 max 1,log(1/p′) ,
(cid:12)∂p (cid:12) ≤ { } ≤ { }
and thus
(cid:18) (cid:26) (cid:27) (cid:19)
1
ψ(p,q) ψ(p′,q) C max 1,log +C (p′ p).
| − | ≤ 1 p′ 2 −
With the above lemma, the estimation error of the empirical f-divergence can be upper
bounded by the total variation distance between the empirical measure and its population
counterpart up to a logarithmic factor, where:
(cid:88)
Pˆ P = Pˆ P . (35)
n TV n,a a
∥ − ∥ | − |
a∈X
For the first part, we further upper bound the expected total variation distance of the
plug-in estimator, which is
(cid:88)
Pˆ P = Pˆ P .
n TV n,a a
∥ − ∥ | − |
a∈X
Lemma 21 Assume that P is discrete. For any n 1, it holds that
≥
E Pˆ n P TV α n(P).
∥ − ∥ ≤
Furthermore, if k = Supp(P) < , then
| | ∞
(cid:114)
k
E Pˆ n P TV α n(P) .
∥ − ∥ ≤ ≤ n
65
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Proof Using Jensen’s inequality, we have,
(cid:113)
(cid:88) (cid:88)
E Pˆ n,a P a E(Pˆ n,a P a)2
| − | ≤ −
a∈Supp(P) a∈Supp(P)
(cid:114)
(cid:88) P a(1 P a)
= − α (P),
n
n ≤
a∈Supp(P)
If k < , then it follows from Jensen’s inequality applied to the concave function t √t
∞ (cid:55)→
that
(cid:118)
k (cid:117) k
1 (cid:88) (cid:117)1(cid:88)
√a (cid:116) a .
k k
k ≤ k
i=1 i=1
Hence, α (P) (cid:112) k/n and it completes the proof.
n
≤
B.1.2 Missing Mass Computation
For the second part, we treat the missing mass directly.
Lemma 22 (Missing Mass) Assume that k = Supp(P) < . Then, for any n 3,
| | ∞ ≥
(cid:34) (cid:35)
E (cid:88) 1(cid:8) Pˆ n,a = 0(cid:9) P a k (36)
≤ n
a∈X
(cid:34) (cid:35)
(cid:18) (cid:19)
β n(P) := E (cid:88) 1(cid:8) Pˆ n,a = 0(cid:9) P a 1 log 1 klogn , (37)
∨ P ≤ n
a
a∈X
where a b := max a,b .
∨ { }
Proof We prove the second inequality. The first one is identical. Note that E[1 Pˆ
n,a
=
0 ] = P(Pˆ
n,a
= 0) = (1 P a)n. Therefore, the left-hand side (LHS) of the second ine{ quality
} −
is
(cid:88)
LHS = (1 P )nP max 1, logP
a a a
− { − }
a∈X
(cid:88) 1 logn klogn
= ,
≤ n ∨ n n
a∈X
where we used Theorem 28 and Theorem 29.
Remark 23 According to (Berend and Kontorovich, 2012, Prop. 3), the bound k/n in (36)
is tight up to a constant factor.
66
MAUVE Scores for Generative Models: Theory and Practice
B.1.3 Full Proof of the Statistical bound
Now, we are ready to prove Theorem 10.
(cid:12) (cid:12)
Proof [Proof of Theorem 10] Define ∆ (a) := (cid:12)ψ(cid:0) P ,Q (cid:1) ψ(cid:0) Pˆ ,Qˆ (cid:1)(cid:12). We have
n,m (cid:12) a a n,a m,a (cid:12)
−
from the triangle inequality that
(cid:12) (cid:12) (cid:12) (cid:12)
∆ (a) (cid:12)ψ(cid:0) P ,Q (cid:1) ψ(cid:0) Pˆ ,Q (cid:1)(cid:12)+(cid:12)ψ(cid:0) Pˆ ,Q (cid:1) ψ(cid:0) Pˆ ,Qˆ (cid:1)(cid:12) .
n,m (cid:12) a a n,a a (cid:12) (cid:12) n,a a n,a m,a (cid:12)
≤ − −
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:T1(a) =:T2(a)
Since Pˆ = 0 or Pˆ 1/n, the approximate Lipschitz lemma (Theorem 20) gives
n,a n,a
≥
(cid:40)
P (C max 1,log(1/P ) +C∗ C ) , if Pˆ = 0,
(a) a 1 { a } 0 ∨ 2 n,a
T1 ≤ P Pˆ (cid:0) C logn+C∗ C (cid:1) , else.
a n,a 1 0 2
| − | ∨
Consequently, Theorem 21 yields
(cid:88) (cid:88) (cid:104) (cid:105)
E[ 1] E 1 Pˆ n,a = 0 P a(C 1max 1,log(1/P a) +C 0∗ C 2)
T ≤ { } { } ∨
a∈X a∈X
+ (cid:88) E(cid:104) Pˆ
n,a
P
a
(cid:105) (cid:0) C 1logn+C 0∗ C 2(cid:1)
| − | ∨
a∈X
(C +C∗ C )β (P)+(cid:0) C logn+C∗ C (cid:1) α (P).
1 0 2 n 1 0 2 n
≤ ∨ ∨
Since ψ(p,q) = qf(p/q) = pf∗(q/p), an analogous bound holds for with the appropriate
2
T
adjustment of constants. Hence, the inequality (10) holds. Moreover, when k < , the
∞
inequality (11) follows by invoking again Theorem 22 and Theorem 21.
We now prove Theorem 11.
Proof [Proof of Theorem 11] The inequality is a direct consequence of Theorem 10. Recall
from Theorem 5 that D (P R ) = D (P Q) where f (t) := f(t/(λt+1 λ))(λt+1 λ).
f λ f λ
∥ λ ∥ − −
From the proof of Theorem 10 we have
D (Pˆ Qˆ ) D (P Q)
f n m f
| λ ∥ − λ ∥ |
(cid:88)
1 Pˆ = 0 P (C max 1,log(1/P ) +C∗ C )
n,a a 1 a 0 2
≤ { } { } ∨
a∈X
(cid:88)
+ 1 Qˆ = 0 Q (C∗max 1,log(1/Q ) +C C∗)
m,a a 1 a 0 2
{ } { } ∨
a∈X
+ (cid:88)(cid:12) (cid:12)P
a
Pˆ n,a(cid:12) (cid:12)(cid:0) C 1logn+C 0∗ C 2(cid:1) + (cid:88)(cid:12) (cid:12)Q
a
Qˆ m,a(cid:12) (cid:12)(cid:0) C 1∗logm+C
0
C 2∗(cid:1) .
− ∨ − ∨
a∈X a∈X
Note that, for the interpolated KL divergence, we have
1 1
C = 1 λ 1, C∗ = log 1+λ log
0 − ≤ 0 λ − ≤ λ
n,m
(1 λ)2 1
C = 1, C∗ = −
1 1 λ ≤ λ
n,m
1 λ 1
C = 1/2, C∗ = −
2 2 8λ ≤ 8λ
n,m
67
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
for all λ [λ ,1 λ ]. The claim then follows from the same steps of Theorem 10.
n,m n,m
∈ −
B.2 Statistical Error Bound with Smoothing
In this section, we apply add-constant smoothing to estimate the f-divergences and study
its statistical error.
Consider P ( ) and an i.i.d. sample X n P. The add-constant estimator of
∈ P X { i }i=1 ∼
P is defined by
N +b
Pˆb = a , for all a ,
n,a n+kb ∈ X
where b > 0 is a constant and N = i [n] : X = a is the number of times the
a i
|{ ∈ }|
symbol a appears in the sample. In practice, b = b could be different depending on the
a
value of N , but we use the same constant b for simplicity. Similarly, We define Qˆb with
a m
M = i [m] : Y = a . The goal is to upper bound the statistical error
a i
|{ ∈ }|
E D f(P Q) D f(Pˆ nb Qˆb m) (38)
| ∥ − ∥ |
under Theorem 9.
Compared to the statistical error of the plug-in estimator, a key difference is that each
entryintheadd-constantestimatorisatleast(n+kb)−1 (m+kb)−1. Hence,wecandirectly
∧
apply the approximate Lipschitz lemma without the need to control the missing mass part.
Another difference is that the total variation distance is now between the add-constant
estimator and its population counterpart, which can be bounded as follows.
Lemma 24 Assume that k = Supp(P) < . Then, for any b > 0,
∞
(cid:112)
(cid:88) E |Pˆ nb ,a −P a | ≤ (cid:88) nP a(1 −P na +)+ kbbk |P a −1/k | ≤ √kn n+ +2b k(k b −1) .
a∈X a∈X
Proof Note that
N nP b(1 kP ) N nP b(1 kP )
Pˆb P = a − a + − a a − a + − a .
| n,a − a | | n+kb n+kb | ≤ | n+kb | | n+kb |
Using Jensen’s inequality, we have
(cid:34)(cid:114) (cid:35)
(cid:88) E |Pˆ nb ,a −P a
| ≤
(cid:88) E |N na − +n kbP a |2+ c |1 n− +k kP ba |
a∈X a∈X
(cid:34)(cid:112) (cid:35)
(cid:88) nP a(1 P a) bk 1/k P a
= − + | − | .
n+kb n+kb
a∈X
We claim that
(cid:88) 1 2(k 1)
P − .
a
| − k| ≤ k
a∈X
68
MAUVE Scores for Generative Models: Theory and Practice
If this is true, we have
(cid:88) √kn+2b(k 1)
E |Pˆ nb ,a −P a | ≤ n+kb − ,
a∈X
since (cid:80) (cid:112) P (1 P ) √k. It then remains to prove the claim. Take a ,a such
a∈X a − a ≤ 1 2 ∈ X
that P k−1 P . It is clear that
a1
≥ ≥
a2
1 1 1 1
P + P P +P + P P
|
a1
− k| |
a2
− k| ≤ |
a1 a2
− k| |
a2
−
a2
− k|
= P +P .
a1 a2
Repeating this argument gives
(cid:88) 1 1 k 1 2(k 1)
P 1 + − = − .
a
| − k| ≤ − k k k
a∈X
Now we are ready to prove Theorem 12.
Proof [Proof of Theorem 12] Following the proof of Theorem 10, we define
∆ (a) := ψ(P ,Q ) ψ(Pˆb ,Qˆb ) .
n,m a a n,a m,a
| − |
We have from the triangle inequality that
(cid:12) (cid:12) (cid:12) (cid:12)
∆ (a) (cid:12)ψ(cid:0) P ,Q (cid:1) ψ(cid:0) Pˆb ,Q (cid:1)(cid:12)+(cid:12)ψ(cid:0) Pˆb ,Q (cid:1) ψ(cid:0) Pˆb ,Qˆb (cid:1)(cid:12) .
n,m (cid:12) a a n,a a (cid:12) (cid:12) n,a a n,a m,a (cid:12)
≤ − −
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:T1(a) =:T2(a)
Since Pˆb b/(n+kb), the approximate Lipschitz lemma (Theorem 20) gives
n,a
≥
(a) P Pˆb (cid:0) C log(n/b+k)+C∗ C (cid:1) ,
1 a n,a 1 0 2
T ≤ | − | ∨
By theorem 24, it holds that
(cid:80)
a∈X
E[ T1(a)] (cid:88)(cid:20) √nP
a +
bk |1/k −P
a
|(cid:21)
=
nα n(P)
+γ (P)
C log(n/b+k)+C∗ C ≤ n+kb n+kb n+kb n,k
1 0 ∨ 2 a∈X
√kn+2b(k 1)
− .
≤ n+kb
69
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Since ψ(p,q) = qf(p/q) = pf∗(q/p), an analogous bound holds for (a) with the appropri-
2
T
ate adjustment of constants and the sample size. Putting these together, we get,
(cid:34) (cid:35)
E(cid:12) (cid:12)D f(P Q) D f(Pˆ nb Qˆb m)(cid:12) (cid:12) E (cid:88) ∆ n(a)
∥ − ∥ ≤ | |
a∈X
(cid:20) (cid:21)
nα n(P) +γ (P) (cid:0) C log(n/b+k)+C∗ C (cid:1)
≤ n+kb n,k 1 0 ∨ 2
(cid:20) (cid:21)
+ mα m(Q) +γ (Q) (cid:0) C∗log(m/b+k)+C C∗(cid:1)
m+kb m,k 1 0 ∨ 2
(cid:0) C log(n/b+k)+C∗ C (cid:1)√kn+2b(k −1)
≤ 1 0 ∨ 2 n+kb
+(cid:0) C∗log(m/b+k)+C C∗(cid:1)√km+2b(k −1) .
1 0 ∨ 2 m+kb
B.3 Quantization Error
We establish a bound on the quantization error of f-divergences, i.e.,
inf D (P Q) D (P Q ) , (39)
f f S S
|S|≤k| ∥ − ∥ |
where the infimum is over all partitions of of size no larger than k, and P and Q are
S S
X
the quantized versions of P and Q according to , respectively. Note that we do not assume
to be discrete in this section. All the resultsS hold for the linearized cost L (Pˆ ,Qˆ ) and
λ n n
X the frontier integral FI(Pˆ ,Qˆ ) from Table 1.
n n
Our analysis is inspired by the following result, which shows that the f-divergence can
beapproximatedbyitsquantizedcounterpart; see, e.g., (GyörfiandNemetz,1978, Theorem
6).
Theorem 25 For any P,Q ( ), it holds that
∈ P X
D (P Q) = supD (P Q ), (40)
f f S S
∥ ∥
S
where the supremum is over all finite partitions of .
X
We now prove Theorem 13, the finite-partition analogue of this.
Proof [Proof of Theorem 13] Assume f(0)+f∗(0) < . Otherwise, there is nothing to
∞
prove. Fix two distributions P,Q over . Partition the measurable space into
X X
(cid:26) (cid:27) (cid:26) (cid:27)
dP dP
= x : (x) 1 , and, = x : (x) > 1 ,
1 2
X ∈ X dQ ≤ X ∈ X dQ
so that
(cid:90) (cid:18) (cid:19) (cid:90) (cid:18) (cid:19)
dP dQ
D (P Q) = f (x) dQ(x)+ f∗ (x) dP(x) =: D+(P Q)+D+(Q P).
f ∥ dQ dP f ∥ f∗ ∥
X1 X2
70
MAUVE Scores for Generative Models: Theory and Practice
We quantize and separately, starting with . Define sets S , ,S as
1 2 1 1 k
X X X ···
(cid:26) (cid:18) (cid:19) (cid:27)
f(0)(m 1) dP f(0)m
S = x : − f (x) < ,
m 1
∈ X k ≤ dQ k
where the last set S is also extended to include x : f((dP/dQ)(x)) = f(0) . Since
k 1
{ ∈ X }
f is nonincreasing on (0,1], it follows that sup f((dP/dQ)(x)) f(0). As a result, the
x∈X1
≤
collection = S , ,S is a partition of . This gives
1 k 1
S { ··· } X
k k
f(0) (cid:88) f(0) (cid:88)
(m 1)Q[S ] D+(P Q) mQ[S ]. (41)
k − m ≤ f ∥ ≤ k m
m=1 m=1
Further, since f is nonincreasing on (0,1], we also have
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
f(0)(m 1) dP P[F ] dP f(0)m
m
− f sup (x) f f inf (x) .
k ≤ x∈Fm dQ ≤ Q[F m] ≤ x∈Fm dQ ≤ k
Hence, it follows that
k k
f(0) (cid:88) f(0) (cid:88)
(m 1)Q[S ] D+(P Q ) mQ[S ]. (42)
k − m ≤ f S1∥ S1 ≤ k m
m=1 m=1
Putting (41) and (42) together gives
k
(cid:12) (cid:12) f(0) (cid:88) f(0)
inf (cid:12)D+(P Q) D+(P Q )(cid:12) Q[S ] , (43)
|S1|≤k(cid:12) f ∥ − f S1∥ S1 (cid:12) ≤ k
m=1
m ≤ k
since (cid:80)k Q[S ] = Q[ ] 1. Repeating the same argument with P and Q interchanged
m=1 m X1 ≤
and replacing f by f∗ gives
(cid:12) (cid:12) f∗(0)
inf (cid:12)D+(Q P) D+(Q P )(cid:12) . (44)
|S2|≤k(cid:12) f∗ ∥ − f∗ S2∥ S2 (cid:12) ≤ k
To complete the proof, we upper bound the inf of over all partitions of with = k
S X |S|
by the inf over = with partitions of and of , and = = k.
1 2 1 1 2 2 1 2
Now, under
thisS partS itio∪ niS
ng, we have,
D+(PS
Q
X
) =
D+(S
P
QX
)
and|S D+| (Q|S P|
) =
f S ∥ S f S1∥ S1 f∗ S ∥ S
D+(Q P ). Putting this together with the triangle inequality, we get,
f∗ S2∥ S2
(cid:12) (cid:12)
inf (cid:12)D (P Q) D (P Q )(cid:12)
(cid:12) f f S S (cid:12)
|S|≤2k ∥ − ∥
(cid:110)(cid:12) (cid:12) (cid:12) (cid:12)(cid:111)
inf (cid:12)D+(P Q) D+(P Q )(cid:12)+(cid:12)D+(Q P) D+(Q P )(cid:12)
≤ S=S1∪S2 (cid:12) f ∥ − f S ∥ S (cid:12) (cid:12) f∗ ∥ − f∗ S ∥ S (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
inf (cid:12)D+(P Q) D+(P Q )(cid:12)+ inf (cid:12)D+(Q P) D+(Q P )(cid:12)
≤ |S1|≤k(cid:12) f ∥ − f S1∥ S1 (cid:12) |S2|≤k(cid:12) f∗ ∥ − f∗ S2∥ S2 (cid:12)
f(0)+f∗(0)
.
≤ k
71
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
B.4 Properties and Technical Lemmas
Lemma 26 Suppose the generator f satisfies Assumptions (A1) and (A2). Then,
lim f′(t) = f∗(0), and lim(f∗)′(t) = f(0).
t→∞ t→∞
Proof We start by observing that
1
limt f′(t) C limt tlog = 0.
1
t→0 | | ≤ t→0 ∨ t
Next, a direct calculation gives
(f∗)′(1/t) = f(t) tf′(t),
−
so that taking the limit t 0 gives
→
lim(f∗)′(t) = f(0) limtf′(t) = f(0).
t→∞ −t→0
The proof of the other part is identical.
Property 27 Suppose f : (0, ) [0, ) is convex and continuously differentiable with
∞ → ∞
f(1) = 0 = f′(1). Then, f′(x) 0 for all x (0,1) and f′(x) 0 for all x (1, ).
≤ ∈ ≥ ∈ ∞
Proof Monotonicity of f′ means that we have for any x (0,1) and y (1, ) that
∈ ∈ ∞
f′(x) f′(1) = 0 f′(y).
≤ ≤
Lemma 28 For all x (0,1) and n 3, we have
∈ ≥
1 logn
0 (1 x)nxlog .
≤ − x ≤ n
Proof Let h(x) = (1 x)nxlog(1/x) be defined on (0,1). Since lim h(x) = 0 < h(1/n),
x→0
−
the global supremum does not occur as x 0. We first argue that h obtains its global
→
maximum in (0,1/n]. We calculate
(cid:18) (cid:18) (cid:19)(cid:19)
1 1 1
h′(x) = (1 x)n−1 nxlog +(1 x) log 1 (1 x)n−1(1 nx)log .
− − x − x − ≤ − − x
Note that h′(x) < 0 for x > 1/n, so h is strictly decreasing on (1/n,1). Therefore, it must
obtain its global maximum on (0,1/n]. On this interval, we have,
1 1 logn
(1 x)nxlog xlog ,
− x ≤ x ≤ n
since xlog(1/x) is increasing on (0,exp( 1)).
−
The next lemma comes from (Berend and Kontorovich, 2012, Theorem 1).
Lemma 29 For all x (0,1) and n 1, we have
∈ ≥
0 (1 x)nx exp( 1)/(n+1) < 1/n.
≤ − ≤ −
72
MAUVE Scores for Generative Models: Theory and Practice
Appendix C. Estimation of Divergences via Parametric Approximations
We discuss here the parametric approximation approach for divergence estimation men-
tioned in Section 4. Given an embedding model φ : Rd, we first approximate the
X →
f-divergence D (P Q) by D (φ P φ Q). Since φ(x ) n is an i.i.d. sample from φ P, we
f ∥ f ♯ ∥ ♯ { i }i=1 ♯
then approximate φ P by a multivariate Gaussian distribution with mean and covariance
♯
matrix given by
n n
1 (cid:88) 1 (cid:88)
µˆ := φ(x ) and Σˆ := (φ(x ) µˆ )(φ(x ) µˆ )⊤,
P i P i P i P
n n 1 − −
i=1 − i=1
respectively. The distribution φ Q can be approximated by (µˆ ,Σˆ ) similarly. Finally,
♯ d Q Q
N
we approximate D (φ P φ Q) by
f ♯ ♯
∥
(cid:32) (cid:33)
D f (cid:16) Nd(µˆ P,Σˆ P) (cid:13) (cid:13) Nd(µˆ Q,Σˆ Q)(cid:17) = (cid:90) f ϕϕ (( zz ;; µµ ˆˆ P ,, ΣΣ ˆˆ P)
)
ϕ(z;µˆ Q,Σˆ Q)dz, (45)
Q Q
where ϕ( ;µ,Σ) is the probability density function of the multivariate normal distribution
·
(µ,Σ). To evaluate the integration in (45), we can use the Monte Carlo approach—(i)
N generate i.i.d. samples z B from (µˆ ,Σˆ ), and (ii) approximate (45) by the empirical
{ b }b=1 Nd Q Q
average
Dˆ (µ ,Σ µ ,Σ ) =
1 (cid:88)B f(cid:32) ϕ(z b;µˆ P,Σˆ P)(cid:33)
. (46)
f P P ∥ Q Q B ϕ(z ;µˆ ,Σˆ )
b=1 b Q Q
Although this approach is widely used in practice, it has no theoretical guarantee. Its
performance can get arbitrarily bad depending on the two distributions P and Q. We give
below a simple example to illustrate this.
Example 30 Consider two distributions φ P 1 ( µ,1)+1 (µ,1) and φ Q (0,1).
♯ ∼ 2N − 2N ♯ ∼ N
It is straightforward to get that φ P has mean zero and variance
♯
(cid:90) (cid:90) (cid:90)
1 1
x2dφ P(x) = x2ϕ(x; µ,1)dx+ x2ϕ(x;µ,1)dx = 1+µ2.
♯
2 − 2
As a result, the KL divergence KL(φ P φ Q) can be approximated by
♯ ♯
∥
KL(cid:0) (0,1+µ2) (0,1)(cid:1) =
µ2 −log(1+µ2)
.
N ∥N 2
On the other hand, we also know that
(cid:90) (cid:20) (cid:21) (cid:20) (cid:21)
1 1 1ϕ(x; µ,1) 1ϕ(x;µ,1)
KL(φ P φ Q) = ϕ(x; µ,1)+ ϕ(x;µ,1) log − + dx
♯ ♯
∥ 2 − 2 2 ϕ(x;0,1) 2ϕ(x;0,1)
(cid:90) (cid:20) (cid:21)
1ϕ(x; µ,1) 1ϕ(x;µ,1)
= ϕ(x;µ,1)log − + dx.
2 ϕ(x;0,1) 2ϕ(x;0,1)
73
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Notice that
1ϕ(x; −µ,1)
+
1ϕ(x;µ,1)
=
1 e−µ2/2(cid:0) e−xµ+exµ(cid:1)
=
1 e−µ2/2e−xµ(1+e2xµ).
2 ϕ(x;0,1) 2ϕ(x;0,1) 2 2
As a result, we get
µ2 (cid:90) (cid:90)
KL(φ P φ Q) = log2 µ xϕ(x;µ,1)dx+ log(1+e2xµ)ϕ(x;µ,1)dx
♯ ♯
∥ − 2 − −
µ2 (cid:90)
= log2 µ2+ log(1+e2xµ)ϕ(x;µ,1)dx
− 2 − −
3µ2 (cid:90)
log2+ 2xµϕ(x;µ,1)dx
≥ − 2 −
µ2
= log2.
2 −
This implies that
KL(φ P φ Q) KL(cid:0) (0,1+µ2) (0,1)(cid:1) 1 log(1+µ2) log2
♯ ♯
∥ − N ∥N ≥ 2 −
can be arbitrarily large as µ increases. Hence, the parametric approximation approach can
be extremely inaccurate even in this simple example.
Computational Complexity. Estimatingthemeanvectorsandcovariancematricestakes
O(nd2) time. Since evaluating the density ϕ(z;µ,Σ) involves computing the quadratic form
(z µ)⊤Σ−1(z µ), we can compute Σ−1 once with time complexity O(d3) and evaluate
− −
Σ−1(z µ) for different z’s where each evaluation cost O(d2) time. Assuming that sampling
an obs− ervation from (µˆ ,Σˆ ) takes O(d) time, the time complexity of the Monte Carlo
d Q Q
N
approximation is O(Bd2 +d3). Hence, the parametric approximation approach has overall
time complexity O(nd2+Bd2+d3).
Appendix D. Experiments: Additional Results
We elaborate on the results in Section 7 in this section as follows.
• Appendix D.1: full results across model size and decoding for all domains.
• Appendix D.2: full results across text length.
• Appendix D.3: full comparison to other f-divergence frontier summaries.
• Appendix D.4: use of MAUVE for hyperparameter tuning.
D.1 Results for Other Domains: News and Stories
The analogue of Table 5 for the news and story domains are Tables 13 and 14 respectively.
These are qualitatively similar to the web text domain. MAUVE, like discrimination accu-
racy, rates larger models as better and nucleus sampling as better than ancestral sampling
and greedy decoding. An exception to this rule is Grover large, where MAUVE thinks ances-
tral sampling is better than nucleus sampling. The statistics-based measures, namely Zipf
coefficient, Repetition, and the fraction of distinct 4 grams all prefer smaller Grover sizes.
74
MAUVE Scores for Generative Models: Theory and Practice
GroverSize Decoding Gen. PPL ZipfCoef. Rep. Distinct-4 Self-BLEU %Disc. Acc.( ) MAUVE⋆( )
↓ ↑
Sampling 37.505 0.942 0.002 0.882 0.419 99.925 0.754
base Greedy 1.413 1.038 0.518 0.081 0.548 100.000 0.012
Nucleus,0.96 23.064 0.974 0.006 0.847 0.462 99.950 0.764
Sampling 27.796 0.946 0.002 0.878 0.429 99.450 0.836
large Greedy 1.575 1.012 0.366 0.124 0.504 100.000 0.013
Nucleus,0.98 20.792 0.962 0.002 0.859 0.450 98.475 0.800
Sampling 22.656 0.950 0.001 0.879 0.427 97.300 0.847
mega Greedy 1.796 1.003 0.316 0.176 0.500 100.000 0.013
Nucleus,0.96 14.834 0.972 0.003 0.848 0.469 88.675 0.852
Human n/a 15.356 0.956 0.002 0.842 0.473 – –
Table 13: News generation evaluation across different Grover model sizes, and de-
coding approaches. For nucleus sampling, we show the best hyperparameter value from
0.9,0.92,0.94,0.96,0.98 as per MAUVE. Disc. Acc. denotes the discrimination accuracy (%)
o{f a Grover large model}trained to distinguish human text from machine text generated with the
model and decoding algorithm of each row. Boldfaced numbers indicate the performance closest to
the human reference when applicable, or the best performance according to the measure.
Decoding Gen. PPL ZipfCoef. REP Distinct-4 Self-BLEU %Disc. Acc. ( ) MAUVE⋆( )
↓ ↑
Sampling 38.9830.143 1.0660.002 0.0010.000 0.8330.001 0.5180.003 78.0980.365 0.9290.007
Nucleus,0.9 15.4330.042 1.2010.002 0.0060.001 0.7190.001 0.6370.002 75.1500.373 0.9140.005
Nucleus,0.92 17.4220.060 1.1790.002 0.0040.001 0.7420.001 0.6200.003 71.9790.594 0.9260.003
Nucleus,0.95 21.5990.127 1.1470.002 0.0030.000 0.7750.002 0.5890.005 68.5860.583 0.9400.003
Top-50 13.7350.027 1.2930.004 0.0020.000 0.7060.001 0.6640.003 83.5490.381 0.8860.010
Top-100 16.5270.041 1.2520.001 0.0020.000 0.7430.001 0.6310.001 78.1500.207 0.9130.005
Top-500 23.8330.076 1.1530.001 0.0010.000 0.7940.001 0.5760.002 69.6800.450 0.9420.004
Greedy 1.739 1.362 0.988 0.101 0.742 99.712 0.013
Human 19.704 1.101 0.001 0.783 0.571
Table14: StorycontinuationevaluationacrossdifferentdecodingapproacheswithGPT-2medium.
Disc. Acc. denotesthediscriminationaccuracy(%)ofaclassifier(afrozenGPT-2largemodelwitha
classificationhead)trainedtodistinguishhumantextfrommachinetextgeneratedwiththedecoding
algorithm of each row. Boldfaced numbers indicate the performance closest to the human reference
when applicable, or the best performance according to the measure.
Next, we turn to the language modeling comparison measures in Table 15. JS con-
sistently favors greedy decoding, which produces far worse text than other decoding algo-
rithms. Likewise, ε-PPL favors ancestral sampling, which also produces somewhat degen-
erate text (Holtzman et al., 2020), while SP appears to be unable to distinguish between
ancestral sampling and nucleus sampling. This makes SP, JS, and ε-PPL unsuitable to
compare generated text to human text.
D.2 Effect of Text Length
WenowturntotheplotofcomparisonmeasuresversustextlengthinFigure22. Thisshows
theresultsofFigure8fordifferenthyperparameters. Recallthatweexpectthequalityofthe
generationtodegradeasthemaximumlengthofthetext(bothmachineandhuman-written)
increases.
75
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
GPT-2Size Decoding SP( ) JS( ) ε-PPL( ) Human/BT( ) MAUVE⋆( )
↑ ↓ ↓ ↑ ↑
Greedy 0.431 0.394 1049.589 – 0.019
small NSa um clp el uin s,g
0.9
0 0.. 66 55 23 0 0.. 44 12 45 21 59 ..4 90 31
8
− −12 57. .75 82 00 ..6 95 05 600. .00 01 58
Greedy 0.465 0.371 708.057 – 0.024
medium NSa um clp el uin s,g
0.9
0 0.. 66 77 00 0 0.. 34 90 12 11 84 ..6 83 21
1
− −30 3. .47 37 00 ..4 94 36 600. .00 01 40
Greedy 0.483 0.359 580.020 – 0.026
large NSa um clp el uin s,g
0.95
0 0.. 66 77 99 0 0.. 33 78 41 11 42. .6 95 38
8
− 16 2. .59 53 00 ..8 97 58 200. .00 00 28
Greedy 0.496 0.349 497.696 – 0.033
xl
Sampling 0.686 0.369 11.412 8.97 0.9080.005
Nucleus,0.95 0.686 0.363 13.677 15.66 0.9550.004
Adversarial n/a n/a n/a – 0.057
Table 15: MAUVE versus comparison measures based on language modeling (SP, JS, and ε-PPL)
across different model sizes, and decoding approaches for web text generations. SP, JS, and ε-PPL
are deterministic because they do not require generations from a decoding algorithm. Moreover,
they cannot measure the quality of the adversarial decoding. The column “Human/BT” gives the
Bradley-Terry score obtained from a pairwise human evaluation (Section 7.1). Boldfaced numbers
indicate the best performance according to the measure.
GPT-2 BT( )
Size
Decoding MAUVE⋆ KL( ↑) FI⋆ KL( ↓) Mid⋆ KL( ↓) MAUVE⋆ χ2( ↑) Mid⋆ χ2( ↓) TV⋆( ↓) H ⋆2( ↓) Human↑
-like
ms
le
am
d
ria gul el
m
NN
N
uS
S
Su
u
ca
a
ac
c
lm
m
ml
l
ee
e
uu up
p
p
ss sl
l
l
,i
i
i, ,n
n
n
00 0g
g
g
..
.
99
9
5
0
0
00
0
0.
.
..
.
.9
9
96
4
80
3
55
4
76
6
25
6
80
0
00
0
0.
.
..
.
.0
0
00
0
00
0
01
1
05
4
28
0
8
0
0
00
0
0.
.
..
.
.0
0
00
0
01
1
13
4
16
2
03
2
70
0
00
0
0.
.
..
.
.0
0
00
0
00
0
00
0
01
1
02
1
0
0
0
00
0
0.
.
..
.
.0
0
01
1
04
3
30
6
54
5
05
0
20
0
00
0
0.
.
..
.
.0
0
00
0
00
0
00
0
01
1
14
3
2
0
0
00
0
0.
.
..
.
.7
8
83
1
63
0
43
6
74
5
95
4
20
0
00
0
0.
.
..
.
.0 00
0
0
01 02
0
1
01 90
4
6
7
0
0
00
0
0.
.
..
.
.0
0
01
2
08
6
59
7
94
8
81
7
80
0
00
0
0.
.
..
.
.0
0
00
0
00
0
00
0
03
2
27
3
3
0
0
00
0
0.
.
..
.
.2
2
13
4
23
0
86
4
50
5
73
3
10
0
00
0
0.
.
..
.
.0
0
00
0
00
0
00
0
05
4
56
4
4
0
0
00
0
0.
.
..
.
.2
3
10
0
02
5
09
7
65
6
71
3
10
0
00
0
0.
.
..
.
.0
0
00
0
00
0
01
0
03
2
20
9
4
−−−
−
−2
3
117
0
3 6
25.
.
.
..
.
57 45
7
9
58 21
6
3
33 98
9
5
xl Sampling 0.9080.005 0.0140.001 0.0440.001 0.7370.012 0.0830.003 0.2320.005 0.0900.003 8.966
Nucleus,0.95 0.9550.004 0.0100.001 0.0290.002 0.8570.012 0.0560.003 0.1850.006 0.0590.003 15.664
Table 16: Comparisonf-divergencesfrontiersummariesforthewebtextdomain. Thecorrelations
from this table are reported in Table 9 of Section 7.4. The subscripts denote standard deviations
over 5 random seeds. Boldfaced numbers indicate the smallest gap between the two distributions.
D.3 Comparison with Other Divergences and Optimal Transport
ThefullversionofTables9and10fromSection7.4aregivenasTables16and17respectively.
D.4 Use of MAUVE for Hyperparameter Tuning
Figure 23 plots MAUVE for nucleus and top-K sampling for various values of the hyperpa-
rameters p and K. This illustrates the utility of MAUVE for hyperparameter tuning.
Appendix E. Human Evaluation: Protocol and Full Results
We describe the human evaluation protocol and results of Section 6.4 in detail. The outline
for this section is:
• Appendix E.1: Details of the Bradley-Terry statistical model used to obtain ranking
from pairwise preferences.
• Appendix E.2: Full results of the human evaluation.
76
MAUVE Scores for Generative Models: Theory and Practice
MAUVE ( ) Fréchet Distance ( ) Gen. PPL. diff. ( ) Sparsemax score ( )
0.97 12.4 12 0.685
0.96 12.2 10 0.680
0.95 8 0.675 0.94 12.0 6 0.670
0.93 11.8 4 0.665
0.92 0.660
0.91 11.6 2 0.655
0.90 0
200 400 600 800 1000 200 400 600 800 1000 0 200 400 600 800 1000 200 400 600 800 1000
Max. Length Max. Length Max. Length Max. Length
small medium large xl
MAUVE ( ) Fréchet Distance ( ) Gen. PPL. diff. ( ) Sparsemax score ( )
15.0 120 0.685
0.9 14.5 100 0.680
0.8 14.0 80 0.675
0.7 13.5 60 0.670
0.6 13.0 40 0.665
0.5 12.5 20 0.660
0.655
12.0 0
200 400 600 800 1000 200 400 600 800 1000 0 200 400 600 800 1000 200 400 600 800 1000
Max. Length Max. Length Max. Length Max. Length
small medium large xl
MAUVE ( ) Fréchet Distance ( ) Gen. PPL. diff. ( ) Sparsemax score ( )
0.680
0.95 14.0
0.90 13.5 80 0.675
0.85 60 0.670 13.0 0.80 40 0.665
0.75 12.5
0.660
0.70 12.0 20
0.655
0.65 11.5 0
200 400 600 800 1000 200 400 600 800 1000 0 200 400 600 800 1000 200 400 600 800 1000
Max. Length Max. Length Max. Length Max. Length
small/Sampling small/Nucleus large/Sampling large/Nucleus
Figure 22: Generation quality versus maximum generation length as per various comparison
measures for web text generation with GPT-2. We expect the quality of the generation to degrade
asthemaximumlengthofthetext(bothmachineandhuman-written)increases. MAUVE istheonly
comparison measure that correctly shows this behavior across all models and decoding algorithms.
The shaded area denotes one standard deviation over generations from 5 random seeds.
GPT-2
OTvariants( ↓) MAUVEvariants( ↑)
Size Decoding
Plug-in Fréchet Quantized
LO iT nea+
r
BarO ycT en+
teric
(Defa Lu inlt e) aK rL+ BT( ↑)
interpolation interpolation interpolation Human-like
small NS ua cm leup sli ,n 0g
.9
7 66 93 3..2 28 61 341 .. 62 16 04 11 49 89 .. 35 89 81 10 ..7 28 38
6
00 .. 01 85 38 00. .00 00 51 00 .. 98 31 54 00 .. 00 00 71 00 .. 99 63 47 00 .. 00 00 11 00 .. 96 05 65 00. .00 01 58 −− 12 57 .. 75 81 38
medium NS ua cm leup sli ,n 0g
.9
7 79 01 0..7 45 98 634 .. 97 68 10 12 42 04 .. 19 77 40 02 ..5 82 16
3
00 .. 02 70 78 00. .00 00 42 00 .. 97 42 25 00 .. 00 00 54 00 .. 99 61 74 00 .. 00 00 11 00 .. 94 34 66 00. .00 01 40 − −3 30 .. 47 26 99
large NuS ca lm eup sl ,in 0g
.95
67 81 17 .. 89 80 39 45 .. 36 61 78 11 353 3.. 53 85 38 01 .. 732 625 00 .. 01 60 24 00 ..0 00 04
1
0 0.. 99 60 15 00 .. 00 00 26 00 .. 99 65 98 00 .. 00 00 12 00 .. 98 57 280 0. .0 00 08
2
− 16 2.. 59 53 35
xl Sampling 705.4824.617 146.5931.136 0.0900.003 0.9240.004 0.9620.001 0.9080.005 8.966
Nucleus,0.95 685.1313.258 132.9271.555 0.0610.003 0.9620.004 0.9700.001 0.9550.004 15.664
Table 17: Comparison of measures based on optimal transport for the web text domain. The
correlations from this table are reported in Table 10 of Section 7.4. The subscripts denote standard
deviations over 5 random seeds. Boldfaced numbers indicate the smallest gap between the two
distributions.
• Appendix E.3: Additional details of the human evaluation protocol.
77
EVUAM
EVUAM
EVUAM
ecnatsiD
tehcérF
ecnatsiD
tehcérF
ecnatsiD
tehcérF
.LPP
.neG
.LPP
.neG
.LPP
.neG
erocs
xamesrapS
erocs
xamesrapS
erocs
xamesrapS
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
MAUVE for Nucleus Sampling MAUVE for top-K Sampling
0.94 0.9
0.92 0.8
0.90 0.7
0.88
0.6
0.90 0.92 0.94 0.96 0.98 1.00 101 102 103 104
Top-p parameter Top-K parameter
Figure 23: MAUVE for nucleus and top-K sampling for different values of p and K for GPT-2
large. MAUVE rates nucleus sampling with p = 0.95 and top-K sampling with 100 K 1000 as
the best choices. The shaded area denotes one standard deviation over generations≤from≤5 random
seeds.
Figure 24: Mechanical Turk interface for human evaluation.
78
EVUAM EVUAM
MAUVE Scores for Generative Models: Theory and Practice
E.1 From Pairwise Preferences to Ranking: the Bradley-Terry Model
We compute the Bradley-Terry (BT) scores from the pairwise preferences obtained from the
human evaluation along each of the three axes interesting, sensible, and more likely to be
written by a human.
Bradley-Terry Model Review. Givennplayerswithscoresw , ,w ,thetheBradley-
1 n
···
Terry model (Marden, 1995) models the outcome of a head-to-head comparison of any two
players using a sigmoid13
1
Prob(i beats j) = .
1+e−(wi−wj)/100
The model also assumes the outcome of each head-to-head comparison of any pair of players
is independent of all other comparisons. Note that the model is invariant to additive shifts
of the scores, i.e., the model probabilities induced by scores w +C, ,w +C is same as
1 n
···
the that induced by w , ,w for any constant C. For uniqueness, we normalize the scores
1 n
···
so that their mean is 0.
Fitting the Model. The Bradley-Terry model can be fit to data using Zermelo’s algo-
rithm (Hunter, 2004). Suppose that we are given a dataset of head-to-head comparisons
summarized by numbers N denoting the number of times player i has defeated player
ij
j. Then, the negative log-likelihood ℓ(w , w ) of the data under the The Bradley-Terry
1 n
···
model can be written as
n n
(cid:88)(cid:88)
ℓ(w , ,w ) = N log(1+e−(wi−wj)/100).
1 n ij
··· −
i=1 j=1
This is convex in the parameters w , ,w since the log-sum-exp function is convex. Zer-
1 n
···
melo’s algorithm (Hunter, 2004) can be used to compute the maximum likelihood estimate.
Denote w = w /100. Starting from an initial estimate w(0) , ,w(0), each iteration of
(cid:101)i i (cid:101)1
···
(cid:101)n
Zermelo’s algorithm performs the update
   
(t) (cid:88) (cid:88) N ij +N ji
u
i
= log N ij −log
exp(w(t) )+exp(w(t)
)

j̸=i j̸=i (cid:101)i (cid:101)j
followed by the mean normalization
n
(t+1) (t) 1 (cid:88) (t)
w = u u .
(cid:101)i i − n j
j=1
ProcessingRawData. Wecollecttheresultofahead-to-headcomparisonusing5options:
Definitely A/B, Slightly A/B, or a Tie. We combine “Definitely A” and ”Slightly A“ into a
single category denoting that A wins, while ties were assigned to either A or B uniformly at
random.
13. The scaling factor 100 is arbitrary and does not change the model or the obtained rankings.
79
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
GPT-2 Size Decoding BT/Human-like BT/Interesting BT/Sensible
Human 47.251 25.503 43.229
xl Nucleus,p=0.95 15.664 23.046 31.888
Sampling 8.966 9.529 7.753
large Nucleus,p=0.95 12.553 6.785 8.781
Sampling 6.935 1.532 7.106
medium Nucleus,p=0.9 − 3.429 − 12.824 − 7.293
Sampling − 30.769 − 34.323 − 32.004
small Nucleus,p=0.9 − 15.783 − 0.697 − 7.442
Sampling − 27.518 − 15.487 − 37.805
− − −
Table 18: FittedBradley-Terry(BT)scoresforeachofthethreeaxesratedbyhumanannotators:
“Human-like” denotes measures how likely the text is to be written by a human, while “Interesting”
and “Sensible” quantify how interesting or sensible the text is. The Spearman rank correlations
between each of these scores are: Human-like and Interesting: 0.917, Human-like and Sensible:
0.917, Interesting and Sensible: 0.967.
E.2 Full Results of the Human Evaluation
BT Model for Human Eval. In our setting, each “player” is a source of text, i.e., one
human, plus, eight model and decoding algorithm pairs (four model sizes GPT-2 small-
/medium/large/xl coupled with pure sampling or nucleus sampling). We compute the BT
score of each player as the maximum likelihood estimate of corresponding the parameters
w , ,w based on head-to-head human evaluation data.
1 n
···
A higher BT score indicates a stronger preference from human annotators. The BT
scores are reported in Table 18.
Interpreting BT scores. TheBTscoresreportedinTable18giveuspredictionsfromthe
sigmoidmodelabove. Forexample, considerthecolumn“BT/Human-like”. Thebestmodel-
generated text, GPT-2 xl with nucleus sampling will lose to human text with probability
0.578. At the other end, GPT-2 small with nucleus sampling will lose to human text with
probability 0.679. This shows that there is still much room for improvement in model-
generated text.
Discussion. In general, the BT scores from human evaluations and MAUVE both indicate
that (a) nucleus sampling is better than pure sampling for the same model size, and, (b)
largermodelsizesarebetterforthesamedecodingalgorithm. Thereisoneexceptiontothis
rule, as per both the human evaluations and MAUVE: GPT-2 small is better than GPT-2
medium for pure sampling.
E.3 Additional Details
We describe more details for the human evaluation. The terminology below is taken from
(Shimorina and Belz, 2022).
Number of Outputs Evaluated. We compare 9 players: one player is “human”, rep-
resenting human-written text, whereas the other 8 are text generated by the model using
the first 35 tokens of the corresponding human generation as a prompt. Each of the 8 non-
human players come from a GPT-2 model of different sizes (small, medium, large, xl) and
80
MAUVE Scores for Generative Models: Theory and Practice
twodecodingalgorithms(puresamplingandnucleussampling). Weperform90comparisons
between each pair of players, so each player is evaluated 90 8 = 720 times.
×
Prompt Filtering. We manually selected 1831 out of5000prompts which are well-formed
English sentences from the web text test set.14 For every head-to-head comparison, we
sample 90 prompt without replacement and then sample the corresponding completions (for
human-generated text, we use the test set of web text). We only consider a pair of players
for human evaluation if the generation from each player is at least 200 BPE tokens long
(and we truncate each generation at a maximum length of 256 BPE tokens).
Number of Evaluators. 214 unique evaluators participated in the evaluation. Of these,
11evaluatorssuppliedatleast50annotations95evaluatorssuppliedatleast10annotations.
Evaluator Selection and Pay. WeconductourhumanevaluationonAmazonMechanical
Turk. Since the task only requires elementary reading and understanding skills in English,
we open the evaluations to non-experts. Each crowd worker was paid 0.40 per annotation.
The pay was estimated based on a $16/hour wage for the 85th percentile of response times
from a pilot study (which was approx. 98 seconds per annotation). These evaluators are
not previously known to the authors.
Training and Instructions. The evaluators were given instructions about the task and
two detailed examples. No other training was provided due to the elementary nature of the
task. The screenshots of these examples are given in Figure 25 while the instructions read:
Task Info: WearestudyinghowgoodAImodelsareatgeneratingtextontheinternet.
You are given a snippet of text from a random document on the internet, called the
"prompt"orthe"context",aswellastwocontinuations,AandB.Oneorbothofthese
iswrittenbyanAI.Youmustchoose(a)whichoftwocontinuationsismoreinteresting,
(b) which makes more sense given the prompt, and, (c) which is more likely to have
been written by a human, as per your assessment.
Guidelines:
• Therearefivechoicesforeachquestion: DefinitelyA/B,SlightlyA/B,orTie. Please
use the "Tie" option extremely sparingly! (No more than one in every ten pairs
should be chosen as a tie along any of the three questions).
• The questions can have different answers! Some text is very creative or interesting,
but it doesn’t quite fit the prompt or make sense.
• Try to focus on quality over quantity. The text can be long but contain rambly
gibberish.
• Don’tworryifthetextendsabruptlyorhasotherartifactsofthewebsitedownload-
ing process (text like ’Advertisement’ for instance).
• Please do your best, some of these are pretty challenging!
• Answering each question should take around 1.5 minutes on average, as per our
estimation. We have calibrated the pay to be $16 per hour with this speed.
14. The web text dataset is scraped from the internet and is not curated. This dataset contains poor
prompts such as headers of webpages or error messages, such as: “Having trouble viewing the video?
Try disabling any ad-blocking extensions currently running on your browser” or “Front Page Torrents
Favorites My Home My Galleries Toplists Bounties News Forums Wiki”. We exclude such prompts as
they are unsuitable for human evaluation.
81
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Quality Control. All annotations made in under 25 seconds were excluded for quality
control (the mean response time per annotation was 47 seconds).
Quality Criteria. We use three quality criteria. The questions asked to the evaluators
are (verbatim):
1. Interestingness: “Which continuation is more interesting or creative, given the context?"
2. Sensible: “Which continuation makes more sense, given the context?”
3. Human-like: “Which continuation is more likely to be written by a human?”
Note that we do explicitly name the criteria in the evaluation form, although those names
could be inferred from the definitions. We use these names only in the paper.
References
David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for
Boltzmann machines. Cognitive science, 9(1):147–169, 1985.
Morteza Alamgir, Gábor Lugosi, and Ulrike von Luxburg. Density-preserving quantization
with application to graph downsampling. In COLT, 2014.
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,AlessandroCappelli,Ruxandra
Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin
Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an
open large language model with state-of-the-art performance. 2023.
SatanjeevBanerjeeandAlonLavie. METEOR:AnautomaticmetricforMTevaluationwith
improved correlation with human judgments. In Proc. of ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Translation and/or Summarization,pages65–
72, 2005.
Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, and Lav R.
Varshney. Mirostat: ANeuralTextDecodingAlgorithmthatDirectlyControlsPerplexity.
In Proc. of ICLR, 2021.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A Neural Proba-
bilistic Language Model. J. Mach. Learn. Res., 3:1137–1155, 2003.
Daniel Berend and Aryeh Kontorovich. The missing mass problem. Statistics & Probability
Letters, 82(6), 2012.
Gérard Biau, Maxime Sangnier, and Ugo Tanielian. Some theoretical insights into Wasser-
stein GANs. Journal of Machine Learning Research, 2021.
MikołajBińkowski,DanicaJ.Sutherland,MichaelArbel,andArthurGretton. Demystifying
MMD GANs. In Proc. of ICLR, 2018.
Dietrich Braess and Tomas Sauer. Bernstein polynomials and learning theory. Journal of
Approximation Theory, 128(2), 2004.
82
MAUVE Scores for Generative Models: Theory and Practice
Figure 25: Annotated examples shown to the evaluators.
83
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini
Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya
Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are
Few-Shot Learners. In Proc. of NeurIPS, 2020.
Yuheng Bu, Shaofeng Zou, Yingbin Liang, and Venugopal V. Veeravalli. Estimation of KL
divergence: Optimal minimax rate. IEEE Transactions on Information Theory, 64(4),
2018.
MassimoCaccia, LucasCaccia, WilliamFedus, HugoLarochelle, JoellePineau, andLaurent
Charlin. Language GANs Falling Short. In Proc. of ICLR, 2020.
Haixiao Cai, Sanjeev R. Kulkarni, and Sergio Verdú. Universal divergence estimation for
finite-alphabet sources. IEEE Trans. Inf. Theory, 52(8):3456–3475, 2006.
T. Tony Cai, X. Jessie Jeng, and Jiashun Jin. Optimal detection of heterogeneous and
heteroscedastic mixtures. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 73(5), 2011.
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation of Text Generation: A
Survey. arXiv Preprint, 2020.
David M Chan, Yiming Ni, Austin Myers, Sudheendra Vijayanarasimhan, David A Ross,
and John Canny. Distribution aware metrics for conditional natural language generation.
arXiv preprint arXiv:2209.07518, 2022.
Fasil Cheema and Ruth Urner. Precision Recall Cover: A Method For Assessing Generative
Models. In AISTATS, volume 206, pages 6571–6594, 2023.
Stanley F. Chen and Joshua Goodman. An empirical study of smoothing techniques for
language modeling. Computer Speech & Language, 13(4), 1999.
Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. Sentence Mover’s Similarity: Auto-
matic Evaluation for Multi-Sentence Texts. In Proc. of ACL, 2019.
Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and
Noah A. Smith. All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of
Generated Text. In Proc. of ACL, 2021.
Stéphan Clémençon and Nicolas Vayatis. Nonparametric estimation of the precision-recall
curve. In Proc. of ICML, pages 185–192, 2009.
Stéphan Clémençon and Nicolas Vayatis. Overlaying classifiers: a practical approach to
optimal scoring. Constructive Approximation, 32:619–648, 2010.
Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and
Pavel P. Kuksa. Natural Language Processing (Almost) from Scratch. J. Mach. Learn.
Res., 12:2493–2537, 2011.
84
MAUVE Scores for Generative Models: Theory and Practice
Corinna Cortes and Mehryar Mohri. Confidence intervals for the area under the ROC curve.
In Proc. of NeurIPS, volume 17, 2005.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Proc.
of NIPS, 2013.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training
of Deep Bidirectional Transformers for Language Understanding. In Proc. of NAACL,
pages 4171–4186, 2019.
Luc Devroye, László Györfi, and Gábor Lugosi. A Probabilistic Theory of Pattern Recogni-
tion. Springer, 1996.
Josip Djolonga, Mario Lucic, Marco Cuturi, Olivier Bachem, Olivier Bousquet, and Syl-
vain Gelly. Precision-Recall Curves Using Information Divergence Frontiers. In Proc. of
AISTATS, pages 2550–2559, 2020.
Bryan Eikema and Wilker Aziz. Is MAP Decoding All You Need? The Inadequacy of the
Mode in Neural Machine Translation. In Proc. of CoLING, 2020.
Moein Falahatgar, Mesrob I Ohannessian, Alon Orlitsky, and Venkatadheeraj Pichapati.
The power of absolute discounting: All-dimensional distribution estimation. In Proc. of
NIPS, 2017.
Angela Fan, Mike Lewis, and Yann N. Dauphin. Hierarchical Neural Story Generation. In
Proc. of ACL, pages 889–898, 2018.
Matthew Finlayson, John Hewitt, Alexander Koller, Swabha Swayamdipta, and Ashish
Sabharwal. Closing the curious case of neural text degeneration, 2023.
Peter Flach. Machine Learning: The Art and Science of Algorithms That Make Sense of
Data. Cambridge University Press, 2012.
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Real-
ToxicityPrompts: EvaluatingNeuralToxicDegenerationinLanguageModels. InEMNLP,
pages 3356–3369, 2020.
Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. Repairing the Cracked Foun-
dation: ASurveyofObstaclesinEvaluationPracticesforGeneratedText. J. Artif. Intell.
Res., 77:103–166, 2023.
AaronGokaslanandVanyaCohen. Openwebtextcorpus. http://Skylion007.github.
io/OpenWebTextCorpus, 2019.
I. J. Good. The population frequencies of species and the estimation of population param-
eters. Biometrika, 40(3-4), 1953.
IanJ.Goodfellow, JeanPouget-Abadie, MehdiMirza, BingXu, DavidWarde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. In Proc.
of NeurIPS, 2014.
85
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Google. Bard: A conversational AI tool by Google. https://bard.google.com, 2023.
Jian Guan and Minlie Huang. UNION: An Unreferenced Metric for Evaluating Open-ended
Story Generation. In Proc. of EMNLP, pages 9157–9166, 2020.
Adityanand Guntuboyina, Sujayam Saha, and Geoffrey Schiebinger. Sharp Inequalities for
f-Divergences. IEEE Trans. Inf. Theory, 60(1):104–121, 2014.
L. Györfi and T. Nemetz. f-dissimilarity: A generalization of the affinity of several distri-
butions. Annals of the Institute of Statistical Mathematics, 30, 1978.
Perttu Hämäläinen and Arno Solin. Deep Residual Mixture Models. arXiv preprint, 2020.
Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax Estimation of Divergences
Between Discrete Distributions. IEEE J. Sel. Areas Inf. Theory, 1(3):814–823, 2020.
Tatsunori Hashimoto, Hugh Zhang, and Percy Liang. Unifying human and statistical eval-
uation for natural language generation. In Proc. of NAACL, pages 1689–1701, 2019.
Anant Hegde, Deniz Erdogmus, Tue Lehn-Schioler, Yadunandana N Rao, and Jose C
Principe. Vector-quantization by density matching in the minimum Kullback-Leibler di-
vergence sense. In IJCNN, 2004.
MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochre-
iter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium.
Proc. of NeurIPS, 30, 2017.
John Hewitt, Christopher D Manning, and Percy Liang. Truncation Sampling as Language
Model Desmoothing. In Proc. of EMNLP Findings, 2022.
AriHoltzman, JanBuys, MaxwellForbes, andYejinChoi. TheCuriousCaseofNeuralText
Degeneration. In Proc. of ICLR, 2020.
Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, and Xing Xie. Fuse It More Deeply! A
Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation.
In Proc. of NAACL, 2022.
Marc Van Hulle. Faithful representations with topographic maps. Neural Networks, 12(6),
1999.
David R Hunter. MM algorithms for generalized Bradley-Terry models. The Annals of
Statistics, 32(1):384–406, 2004.
YuriIngsterandI.A.Suslina. Nonparametric Goodness-of-Fit Testing Under Gaussian Mod-
els. Springer, 2003.
Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic
Detection of Generated Text is Easiest when Humans are Fooled. In Proc. of ACL, pages
1808–1822, July 2020.
86
MAUVE Scores for Generative Models: Theory and Practice
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-
cile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B.
ArXiv Preprint, 2023.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs.
IEEE Transactions on Big Data, 2019.
Dan Jurafsky and James H. Martin. Speech and Language Processing: An Introduction to
Natural Language Processing, Computational Linguistics and Speech Recognition. Prentice
Hall, Pearson Education International, 2009.
KirthevasanKandasamy,AkshayKrishnamurthy,BarnabasPoczos,LarryWasserman,etal.
NonparametricvonMisesEstimatorsforEntropies,DivergencesandMutualInformations.
Advances in Neural Information Processing Systems, 28, 2015.
Marzena Karpinska, Nader Akoury, and Mohit Iyyer. The Perils of Using Mechanical Turk
to Evaluate Open-Ended Text Generation. In Proc. of EMNLP, 2021.
Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Gen-
erative Adversarial Networks. In Proc. of CVPR, pages 4401–4410, 2019.
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila.
Training Generative Adversarial Networks with Limited Data. Proc. of NeurIPS, 33:
12104–12114, 2020a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
Analyzing and Improving the Image Quality of StyleGAN. In Proc. of CVPR, pages
8107–8116, 2020b.
Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen,
and Timo Aila. Alias-Free Generative Adversarial Networks. In Proc. of NeurIPS, pages
852–863, 2021.
Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fréchet Audio
Distance: A Reference-Free Metric for Evaluating Music Enhancement Algorithms. In
Interspeech, pages 2350–2354, 2019.
George Kour, Samuel Ackerman, Eitan Farchi, Orna Raz, Boaz Carmeli, and Ateret Anaby-
Tavor. Measuring the Measuring Tools: An Automatic Evaluation of Semantic Metrics
for Text Corpora. In Proc. of EMNLP, 2022.
Raphail E. Krichevsky and Victor K. Trofimov. The performance of universal encoding.
IEEE Transactions on Information Theory, 27(2), 1981.
Alexey Kurakin, Natalia Ponomareva, Umar Syed, Liam MacDermed, and Andreas Terzis.
Harnessinglarge-languagemodelstogenerateprivatesynthetictext. arXiv Preprint,2023.
87
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From Word Embeddings to
Document Distances. In Proc. of ICML, pages 957–966. PMLR, 2015.
Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Im-
provedPrecisionandRecallMetricforAssessingGenerativeModels. InProc. of NeurIPS,
2019.
Tian Lan, Yixuan Su, Shuhang Liu, Heyan Huang, and Xian-Ling Mao. Momentum Decod-
ing: Open-ended Text Generation As Graph Exploration. arXiv Preprint, 2022.
XiangLisaLi,AriHoltzman,DanielFried,PercyLiang,JasonEisner,TatsunoriHashimoto,
Luke Zettlemoyer, and Mike Lewis. Contrastive Decoding: Open-ended Text Generation
as Optimization. In Proc. of ACL, pages 12286–12312, 2023.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Ya-
sunaga, Yian Zhang, Deepak Narayanan, et al. Holistic evaluation of language models,
2023.
Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Text
Summarization Branches Out, pages 74–81, 2004.
Lang Liu, Krishna Pillutla, Sean Welleck, Sewoong Oh, Yejin Choi, and Zaid Harchaoui.
Divergence Frontiers for Generative Models: Sample Complexity, Quantization Effects,
and Frontier Integrals. In Proc. of NeurIPS, 2021.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized
BERT Pretraining Approach. arXiv Preprint, 2019.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins.
Text Classification using String Kernels. Journal of machine learning research, 2(Feb):
419–444, 2002.
David Lopez-Paz and Maxime Oquab. Revisiting Classifier Two-Sample Tests. In Proc. of
ICLR, 2017.
Christopher D. Manning and Hinrich Schütze. Foundations of Statistical Natural Language
Processing. MIT Press, 2001. ISBN 978-0-262-13360-9.
John I. Marden. Analyzing and modeling rank data, volume 64 of Monographs on Statistics
and Applied Probability. Chapman & Hall, London, 1995. ISBN 0-412-99521-2.
Pedro Henrique Martins, Zita Marinho, and André F. T. Martins. Sparse Text Generation.
In Proc. EMNLP, pages 4252–4273, 2020.
Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis
Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How Decoding Strategies Affect the
Verifiability of Generated Text. In Proc. of EMNLP, pages 223–235, 2020.
88
MAUVE Scores for Generative Models: Theory and Practice
Justus Mattern, Zhijing Jin, Benjamin Weggenmann, Bernhard Schoelkopf, and Mrinmaya
Sachan. Differentially Private Language Models for Secure Data Sharing. In Proc. of
EMNLP, 2022.
David Mcallester and Luis Ortiz. Concentration inequalities for the missing mass and for
histogram rule error. Journal of Machine Learning Research, 4, 2003.
Peter Meinicke and Helge Ritter. Quantizing density estimators. In Proc. of NeurIPS, 2002.
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Locally Typical Sampling.
Transactions of the Association for Computational Linguistics, 2022.
JamesMercer. Functionsofpositiveandnegativetype, andtheirconnectionwiththetheory
of integral equations. Philosophical Transactions of the Royal Society of London (A), 209
(441-458):415–446, 1909.
Kaisa Miettinen. Nonlinear Multiobjective Optimization, volume 12. Springer Science &
Business Media, 2012.
Kevin R. Moon and Alfred O. Hero III. Ensemble estimation of multivariate f-divergence.
In Proc. of ISIT, pages 356–360. IEEE, 2014.
XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating Divergence
Functionals and the Likelihood Ratio by Convex Risk Minimization. IEEE Trans. Inf.
Theory, 56(11):5847–5861, 2010.
Frank Nielsen and Rajendra Bhatia. Matrix Information Geometry. Springer, 2013.
Morteza Noshad, Kevin R. Moon, Salimeh Yasaei Sekeh, and Alfred O. Hero III. Direct
estimation of information divergence using nearest neighbor ratios. In ISIT, pages 903–
907. IEEE, 2017.
Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, and Verena Rieser. Why We
Need New Evaluation Metrics for NLG. In Proc. of EMNLP, 2017.
OpenAI. GPT-4 Technical Report, 2023.
Juri Opitz and Anette Frank. Towards a Decomposable Metric for Explainable Evaluation
of Text Generation from AMR. In Proc. of EACL, pages 1504–1518, 2021.
MaximeOquab, TimothéeDarcet, ThéoMoutakanni, HuyVo, MarcSzafraniec, VasilKhali-
dov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud
Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li,
Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jé-
gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2:
Learning Robust Visual Features without Supervision. CoRR, 2023.
Alon Orlitsky and Ananda Theertha Suresh. Competitive distribution estimation: Why is
Good-Turing good. In NeurIPS, 2015.
89
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a Method for
Automatic Evaluation of Machine Translation. In Proc. of ACL, pages 311–318, 2002.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global Vectors
for Word Representation. In Proc. of EMNLP, pages 1532–1543, 2014.
Margaret Sullivan Pepe. Receiver Operating Characteristic Methodology. Journal of the
American Statistical Association, 95(449):308–311, 2000. ISSN 01621459.
KrishnaPillutla,SwabhaSwayamdipta,RowanZellers,JohnThickstun,SeanWelleck,Yejin
Choi,andZaidHarchaoui. MAUVE:MeasuringtheGapBetweenNeuralTextandHuman
Text with Divergence Frontiers. In Proc. of NeurIPS, 2021.
Tiago Pimentel, Clara Meister, and Ryan Cotterell. On the Usefulness of Embeddings,
Clusters and Strings for Text Generation Evaluation. In Proc. of ICLR, 2023.
BarnabásPóczos,LiangXiong,andJeffG.Schneider.NonparametricDivergenceEstimation
with Applications to Machine Learning on Distributions. In Proc. of UAI, pages 599–608,
2011.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
Language Models are Unsupervised Multitask Learners. OpenAI blog, 1(8):9, 2019.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
Text-Conditional Image Generation with CLIP Latents. arXiv Preprint, 2022.
Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and Jianfeng Gao. PlotMachines: Outline-
Conditioned Generation with Dynamic Plot State Tracking. In Proc. of EMNLP, pages
4274–4295, 2020.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-Resolution Image Synthesis with Latent Diffusion Models. In Proc. of CVPR, pages
10684–10695, 2022.
Juho Rousu, John Shawe-Taylor, and Tommi Jaakkola. Efficient Computation of Gapped
SubstringKernelsonLargeAlphabets. Journal of Machine Learning Research,6(9),2005.
Paul K. Rubenstein, Olivier Bousquet, Josip Djolonga, Carlos Riquelme, and Ilya O. Tol-
stikhin. Practical and Consistent Estimation of f-Divergences. In Proc. of NeurIPS, pages
4072–4082, 2019.
Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Hervé Jégou. Spreading
vectors for similarity search. In Proc. of ICLR, 2019.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton,
Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim
Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-
to-Image Diffusion Models with Deep Language Understanding. In Proc. of NeurIPS,
2022.
90
MAUVE Scores for Generative Models: Theory and Practice
AnanyaB.Sai,AkashKumarMohankumar,andMiteshM.Khapra. ASurveyofEvaluation
Metrics Used for NLG Systems. ACM Comput. Surv., 55(2):26:1–26:39, 2023.
Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly.
Assessing generative models via precision and recall. In Proc. of NeurIPS, 2018.
Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
Xi Chen. Improved Techniques for Training GANs. In Proc. of NeurIPS, pages 2226–
2234, 2016.
Axel Sauer, Katja Schwarz, and Andreas Geiger. StyleGAN-XL: Scaling StyleGAN to Large
Diverse Datasets. In SIGGRAPH, pages 49:1–49:10, 2022.
NicolasSchreuder,Victor-EmmanuelBrunel,andArnakDalalyan. Statisticalguaranteesfor
generativemodelswithoutdomination. InAlgorithmic Learning Theory,pages1051–1071.
PMLR, 2021.
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. BLEURT: Learning Robust Metrics
for Text Generation. In Proc. of ACL, pages 7881–7892, 2020.
Stanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. On Accurate Evaluation of GANs
for Language Generation, 2018. arXiv Preprint.
Serge Sharoff. Know thy Corpus! Robust Methods for Digital Curation of Web corpora. In
Proc. of LREC, pages 2453–2460. European Language Resources Association, 2020.
John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge
University Press, 2004.
HirokiShimanaka,TomoyukiKajiwara,andMamoruKomachi. RUSE:RegressorUsingSen-
tence Embeddingsfor Automatic Machine Translation Evaluation. In Proc. of Conference
on Machine Translation, pages 751–758, 2018.
Anastasia Shimorina and Anya Belz. The human evaluation datasheet: A template for
recording details of human evaluation experiments in NLP. In Proc. of Workshop on
Human Evaluation of NLP Systems, pages 54–75, 2022.
Jorge Silva and Shrikanth Narayanan. Universal consistency of data-driven partitions for
divergence estimation. In Proc. of ISIT, 2007.
Jorge Silva and Shrikanth S Narayanan. Information divergence estimation based on data-
dependent partitions. Journal of Statistical Planning and Inference, 140(11), 2010.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon,
and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equa-
tions. In Proc. of ICLR, 2021.
Sreejith Sreekumar and Ziv Goldfeld. Neural Estimation of Statistical Divergences. Journal
of Machine Learning Research, 23(126):1–75, 2022.
91
Pillutla, Liu, Thickstun, Welleck, Swayamdipta, Zellers, Oh, Choi, Harchaoui
George Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Leigh Ross, Valentin
Villecroze, Zhaoyan Liu, Anthony L. Caterini, J. Eric T. Taylor, and Gabriel Loaiza-
Ganem. Exposing flaws of generative model evaluation metrics and their unfair treatment
of diffusion models. In Proc. of NeurIPS 2023, 2023.
Yixuan Su and Jialu Xu. An Empirical Study On Contrastive Search And Contrastive
Decoding For Open-ended Text Generation. arXiv Preprint, 2022.
Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A
Contrastive Framework for Neural Text Generation. In Proc. of NeurIPS, 2022.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estimation in
Machine Learning. Cambridge University Press, 2012.
Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang
Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, et al. ERNIE 3.0: Large-scale Knowledge
Enhanced Pre-training for Language Understanding and Generation. ArXiv Preprint,
2021.
ChongyangTao, LiliMou, DongyanZhao, andRuiYan. RUBER:AnUnsupervisedMethod
for Automatic Evaluation of Open-Domain Dialog Systems. In Proc. of AAAI, 2018.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Open Foundation and Fine-Tuned Chat Models. ArXiv Preprint, 2023.
Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin
Michalski, and Sylvain Gelly. Towards Accurate Generative Models of Video: A New
Metric & Challenges. arXiv Preprint, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Proc. of
NeurIPS, pages 5998–6008, 2017.
Sergio Verdú. Empirical Estimation of Information Measures: A Literature Guide. Entropy,
21(8):720, 2019.
Cédric Villani. Topics in Optimal Transportation. American Mathematical Society, 2003.
Qing Wang, Sanjeev R Kulkarni, and Sergio Verdú. Divergence estimation of continuous
distributions based on data-dependent partitions. IEEE Transactions on Information
Theory, 51(9), 2005.
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to Model the Tail. In Proc.
of NeurIPS, 2017.
Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho.
Consistency of a Recurrent Language Model With Respect to Incomplete Decoding. In
Proc. of EMNLP, pages 5553–5568, 2020a.
92
MAUVE Scores for Generative Models: Theory and Practice
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason We-
ston. Neural Text Generation With Unlikelihood Training. In Proc. of ICLR, 2020b.
BigScience Workshop. BLOOM: A 176B-Parameter Open-Access Multilingual Language
Model. ArXiv Preprint, 2022.
Zonghan Yang, Xiaoyuan Yi, Peng Li, Yang Liu, and Xing Xie. Unified Detoxifying and
Debiasing in Language Generation via Inference-time Adaptive Optimization. In Proc. of
ICLR, 2023.
LiliYu, BowenShi, RamakanthPasunuru, BenjaminMuller, OlgaGolovneva, TianluWang,
ArunBabu, BinhTang, BrianKarrer, ShellySheynin, etal. ScalingAutoregressiveMulti-
Modal Models: Pretraining and Instruction Tuning. ArXiv Preprint, 2023.
Xiang Yue, Huseyin A. Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari,
Huan Sun, David Levitan, and Robert Sim. Synthetic Text Generation with Differential
Privacy: A Simple and Practical Recipe. In Proc. of ACL, pages 1321–1342, 2023.
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roes-
ner, and Yejin Choi. Defending Against Neural Fake News. In Proc. of NeurIPS, 2019.
Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The Unrea-
sonable Effectiveness of Deep Features as a Perceptual Metric. In CVPR, pages 586–595,
2018.
TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi. BERTScore:
Evaluating text generation with BERT. In Proc. of ICLR, 2020.
Zhiyi Zhang and Michael Grabchak. Nonparametric estimation of Küllback-Leibler diver-
gence. Neural Comput., 26(11):2570–2593, 2014.
Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger.
MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth
Mover Distance. In Proc. of EMNLP, 2019.
93

CitationIE: Leveraging the Citation Graph for Scientific Information
Extraction
VijayViswanathan,GrahamNeubig,PengfeiLiu
LanguageTechnologiesInstitute,CarnegieMellonUniversity
{vijayv, gneubig, pfliu3}@cs.cmu.edu
Abstract
Citation Graph “ [...] The very deep
convolutional networks are
inspired by the “VGGNet”
Automatically extracting key information ML Vision Papers architecture introduced in [16] for
Papers Salient?
the 2014 ImageNet classification
fromscientificdocumentshasthepotentialto challenge, with the central idea to No
replace large convolutional kernels
help scientists work more efficiently and ac-
by small 3×3 kernels.
NLP
celerate the pace of scientific progress. Prior Papers Speech […]
Papers Given the recent popularity of
work has considered extracting document- LSTMs for acoustic modeling, we
have experimented with such
level entity clusters and relations end-to-end models on the Switchboard task Salient?
[…] ” Yes
fromrawscientifictext,whichcanimprovelit-
“The IBM 2016 English
erature search and help identify methods and Conversational Telephone Speech
Recognition System”
materialsforagivenproblem. Despitetheim- Interspeech 2016
portance of this task, most existing works on
Figure 1: Example of using the citation graph to im-
scientific information extraction (SciIE) con-
provethetaskofsaliententityclassification(Jainetal.,
siderextractionsolelybasedonthecontentof
2020). Inthistask,eachentityinthedocumentisclas-
an individual paper, without considering the
sifiedassalientornot,whereasaliententityisdefined
paper’splaceinthebroaderliterature. Incon-
asbeingrelevanttoitspaper’smainideas.
trast to prior work, we augment our text rep-
resentations by leveraging a complementary
sourceofdocumentcontext:thecitationgraph
forthescientificcommunitytoreadthismanypa-
ofreferentiallinksbetweencitingandcitedpa-
persinatime-criticalsituation,andmakeaccurate
pers. On a test set of English-language scien-
judgementstohelpseparatesignalfromthenoise.
tificdocuments, weshowthatsimplewaysof
Tothisend,howcanmachineshelpresearchers
utilizing the structure and content of the cita-
tion graph can each lead to significant gains quicklyidentifyrelevantpapers? Onestepinthis
in different scientific information extraction directionistoautomaticallyextractandorganize
tasks. Whenthesetasksarecombined,weob- scientificinformation(e.g. importantconceptsand
serve a sizable improvement in end-to-end in- their relations) from a collection of research arti-
formation extraction over the state-of-the-art,
cles, which could help researchers identify new
suggestingthepotentialforfutureworkalong
methodsormaterialsforagiventask. Scientificin-
thisdirection. Wereleasesoftwaretoolstofa-
formationextraction(SciIE)(GuptaandManning,
cilitatecitation-awareSciIEdevelopment.1
2011;Yogatamaetal.,2011),whichaimstoextract
1 Introduction structuredinformationfromscientificarticles,has
seen growing interest recently, as reflected in the
Therapidexpansioninpublishedscientificknowl-
rapidevolutionofsystemsanddatasets(Luanetal.,
edge has enormous potential for good, if it can
2018;Ga´boretal.,2018;Jainetal.,2020).
onlybeharnessedcorrectly. Forexample,during
ExistingworksonSciIErevolvearoundextrac-
thefirstfivemonthsoftheglobalCOVID-19pan-
tionsolelybasedonthecontentofdifferentparts
demic,atleast11000paperswerepublishedonline
ofanindividualpaper,suchastheabstractorcon-
aboutthenoveldisease(Hallenbeck,2020), with
clusion(Augensteinetal.,2017;Luanetal.,2019).
eachrepresentingapotentialfasterendtoaglobal
However,scientificpapersdonotexistinavacuum
pandemic and saved lives. Despite the value of
— they are part of a larger ecosystem of papers,
this quantity of focused research, it is infeasible
relatedtoeachotherthroughdifferentconceptual
1https://github.com/viswavi/ScigraphIE relations. In this paper, we claim a better under-
standingofaresearcharticlereliesnotonlyonits cludinga10pointimprovementonF1scorefor
content but also on its relations with associated relationextraction. Thisleadstoasizableincrease
works,usingboththecontentofrelatedpapersand in the performance of the end-to-end CitationIE
thepaper’spositioninthelargercitationnetwork. systemrelativetothecurrentstate-of-the-art,Jain
Weuseaconcreteexampletomotivatehowin- etal.(2020). Weofferqualitativeanalysisofwhy
formationfromthecitationgraphhelpswithSciIE, ourmethodsmayworkin§5.3.
consideringthetaskofidentifyingkeyentitiesina
2 Document-levelScientificIE
longdocument(knownas“saliententityclassifica-
tion”)inFigure1. 2.1 TaskDefinition
In this example, we see a paper describing a
Weconsiderthetaskofextractingdocument-level
speechrecognitionsystem(Saonetal.,2016). Fo-
relationsfromscientifictexts.
cusingontwospecificentitiesinthepaper(“Ima-
Mostworkonscientificinformationextraction
geNetclassificationchallenge”and“Switchboard
hasusedannotateddatasetsofscientificabstracts,
task”),wearetaskedwithclassifyingwhethereach
suchasthoseprovidedforSemEval2017andSe-
iscriticaltothepaper. Thistaskrequiresreasoning
mEval2018sharedtasks(Augensteinetal.,2017;
abouteachentityinrelationtothecentraltopicof
Ga´boretal.,2018),theSciERCdataset(Luanetal.,
the paper, which is a daunting task for NLP con-
2018), and the BioCreative V Chemical Disease
sideringthatthispapercontainsover3000words
Relationdataset(Weietal.,2016).
across 11 sections. An existing state-of-the-art
Wefocusonthetaskofopen-domaindocument-
model (Jain et al., 2020) mistakenly predicts the
level relation extraction from long, full-text doc-
non-salient entity “ImageNet classification chal-
uments. This is in contrast to the above methods
lenge”assalientduetothelimitedcontextualinfor-
that only use paper abstracts. Our setting is also
mation. However,thisproblemismoreapproach-
different from works that consider a fixed set of
ablewheninformedofthestructureofthecitation
candidaterelations(Houetal.,2019;Kardasetal.,
graphthatconveyshowthispapercorrelateswith
2020) or those that only consider IE tasks other
otherresearchworks. Examiningthisexamplepa-
thanrelationextraction,suchasentityrecognition
per’spositioninthesurroundingcitationnetwork
(Verspooretal.,2011).
suggests it is concerned with speech processing,
Webaseourtaskdefinitionandbaselinemodels
whichmakesitunlikelythat“ImageNet”issalient.2
ontherecentlyreleasedSciREXdataset(Jainetal.,
Thecleargoalofincorporatinginter-articlein-
2020), whichcontains438annotatedpapers,3 all
formation,however,ishinderedbyaresourcechal-
relatedtomachinelearningresearch.
lenge: existingSciIEdatasetsthatannotatepapers
Each document consists of sections D =
with rich entity and relation information fail to
{S ,...,S }, where each section contains a se-
1 N
includetheirreferencesinafine-grained,machine-
quence of words S = {w ,...,w }. Each
readableway. Toovercomethisdifficulty,webuild
i i,1 i,Ni
documentcomeswithannotationsofentities,coref-
on top of an existing SciIE dataset and align it
erence clusters, cluster-level saliency labels, and
withasourceofcitationgraphinformation,which
4-ary document-level relations. We break down
finallyallowsustoexplorecitation-awareSciIE.
theend-to-endinformationextractionprocessasa
Architecturally,weadopttheneuralmulti-task
sequenceofthesefourrelatedtasks,witheachtask
model introduced by Jain et al. (2020), and es-
takingtheoutputoftheprecedingtasksasinput.
tablish a proof of concept by comparing simple
ways of incorporating the network structure and Mention Identification For each span of text
textualcontentofthecitationgraphintothismodel. withinasection,thistaskaimstorecognizeifthe
Experimentally,werigorouslyevaluateourmeth- span describes a Task, Dataset, Method, or
ods,whichwecallCitationIE,onthreetasks: men- Metricentity,ifany.
tionidentification,saliententityclassification,and
Coreference Thistaskrequiresclusteringallen-
document-level relation extraction. We find that
tity mentions in a document such that, in each
leveragingcitationgraphinformationprovidessig-
cluster, every mention refers to the same entity
nificant improvements in the latter two tasks, in-
(VarkelandGloberson,2020). TheSciREXdataset
2Ourproposedmethodactuallymakescorrectpredictions 3Thedatasetcontains306documentsfortraining,66for
onboththesesamples,wherethebaselinemodelfailsonboth. validation,and66fortesting.
includes coreference annotations for each Task, shown in Figure 2), and produces tag potentials
Dataset,Method,andMetricmention. at each word. These are then passed to a CRF
(Laffertyetal.,2001)whichpredictsdiscretetags.
SalientEntityClassification Givenaclusterof
mentions corresponding to the same entity, the SpanEmbeddings Foragivenmentionspan,its
model must predict whether the entity is key to spanembeddingisproducedviaadditiveattention
theworkdescribedinapaper. Wefollowthedefi- (Bahdanauetal.,2014)overthetokensinthespan.
nitionfromtheSciREXdataset(Jainetal.,2020),
Coreference Usinganexternalmodel,pairwise
where an entity in a paper is deemed salient if it
coreferencepredictionsaremadeforallentitymen-
playsaroleinthepaper’sevaluation.
tions,formingcoreferenceclusters.
Relation Extraction The ultimate task in our
IE pipeline is relation extraction. We con- SalientEntityClassification Saliencyisaprop-
sider relations as 4-ary tuples of typed entities ertyofentityclusters,butitisfirstpredictedatthe
(E ,E ,E ,E ), which are entity mention level. Each entity mention’s span
Task Dataset Method Metric
requiredtobesaliententities. Givenasetofcandi- embeddingissimplypassedthroughtwofeedfor-
daterelations,wemustdeterminewhichrelations wardnetworks,givingabinarysaliencyprediction.
arecontainedinthemainresultofthepaper. To turn these mention-level predictions into
cluster-level predictions, the predicted saliency
2.2 BaselineModel scoresaremax-pooledoverallmentionsinacoref-
erenceclustertogivecluster-levelsaliencyscores.
WebaseourworkontopofthemodelofJainetal.
(2020),whichwasintroducedasastrongbaseline
Relation Extraction The model treats relation
accompanying the SciREX dataset. We refer the
extractionasbinaryclassification,takingasinput
reader to their paper for full architectural details,
a set of 4 typed salient entity clusters. For each
andbrieflysummarizetheirmodelhere.
entityclusterintherelation,per-sectionentityclus-
This multi-task model performs three of our
terrepresentationsarecomputedbytakingtheset
tasks (mention identification, saliency classifica-
of that entity’s mentions in a given section, and
tion,andrelationextraction)inasequence,treating
max-pooling over the span embeddings of these
coreference resolution as an external black box.
mentions. Thefourentity-sectionembeddings(one
While word and span representations are shared
for each entity in the relation) are then concate-
acrossalltasksandupdatedtominimizemulti-task
nated and passed through a feedforward network
loss,themodeltrainseachtaskongoldinput. Fig-
toproducearelation-sectionembedding. Then,the
ure2summarizesthebaselinemodel’send-to-end
relation-sectionembeddingsareaveragedoverall
architecture, and highlights the places where we
sectionsandpassedthroughanotherfeedforward
proposeimprovementsforourCitationIEmodel.
networkwhichreturnsabinaryprediction.
FeatureExtraction Themodelextractsfeatures
3 Citation-awareSciIEDataset
fromrawtextintwostages. First,contextualized
wordembeddingsareobtainedforeachsectionby
Although citation network information has been
running SciBERT (Beltagy et al., 2019) on that
shown to be effective in other tasks, few works
sectionoftext(upto512tokens). Then,theembed-
haverecentlytriedusingitinSciIEsystems. One
dings from all words over all sections are passed
potentialreasonisthelackofasuitabledataset.
throughabidirectionalLSTM(Gravesetal.,2005)
Thus, as a first contribution of this paper, we
to contextualize each word’s representation with
address this bottleneck by constructing a SciIE
thosefromothersections.
datasetthatisannotatedwithcitationgraphinfor-
mation.4 Specifically, we combine the rich anno-
Mention Identification The baseline model
tationsofSciREXwithasourceofcitationgraph
treats this named entity recognition task as an
information,S2ORC(Loetal.,2020). Foreachpa-
IOBES sequence tagging problem (Reimers and
per,S2ORCincludesparsedmetadataaboutwhich
Gurevych,2017). ThetaggertakestheSciBERT-
otherpaperscitethispaper,whichotherpapersare
BiLSTM(Beltagyetal.,2019;Gravesetal.,2005)
word embeddings (as shown in the Figure 2),
4Wehavereleasedcodetoconstructthisdataset:https:
feedsthemthroughtwofeedforwardnetworks(not //github.com/viswavi/ScigraphIE
Input Feature Mention Salient Entity Coreference Relation
Document Extraction Identification Classification Clustering Extraction
Title/Abstract SciBERT BiLSTM CRF S Ep ma bn e dding Task:
IT Bh M .e .. B I -- TT AA SS KK AAdttednititvioen FF FF Salient “ AIB SM R M “R“A NeS NtR h ”" od:
recurrent B-METHOD System” FF FF
neural I -METHOD FF FF ✓
networks L-METHOD
Salient
D “Swa itt ca hbs oe art d:
”
Intro. (Part 1)
landscT ah
p o
.e
e ..f
O
O O
“RNN” M “We ERtr ”ic: R{
}
e TMDMla
a ae
et
s
ti
t tk
ao
h ri:
scon e:d:
t: : R SA WwSN ER iNtRc,, ,hboard,
our O Task:
findings O S Ep ma bn e dding “ASR"
Intro. (Part 2) FF FF “Language Method:
Recur n wre e in tt hst B L O - -M ME ET TH HO OD D Salient Model” “ ML oa dn eg lu ”age FF FF ×
... Dataset:
Citation
acts ivig am tioo nid s B L-- MM EE TT HH OO DD FF FF N Sao ln ie- nt =“ M “S Ww e Ei Rt tc r ”h ib coa :rd” R{
}
e TM DMla a ae et s ti t tk ao h ri: scon e:d: t: : L SA M Wa wS o EnR idtRg ce,u hl,a bg oe ard,
[CITE]
Sentences used
recurrent
…
Figure 2: Architecture of the model we use for neural information extraction. Light blue blocks indicate places
wherewecanincorporateinformationfromthecitationgraphforthecitation-awareCitationIEarchitecture.
In addition to the 5 documents we could not
40
150 matchtotheS2ORCcitationgraph,7wereincor-
rectlyrecordedascontainingnoreferencesand5
100
20
others were incorrectly recorded as having no ci-
50
tations. Theseerrorsareduetodataissuesinthe
0 0
0 1000 2000 0 50 S2ORC dataset, which relies on PDF parsers to
In-degree Out-degree
extractinformation(Loetal.,2020).
Figure3:DegreestatisticsofSciREXdocumentsinthe
citationgraph. 4 CitationIE
Wenowdescribeourcitation-awarescientificIEar-
citedbythispaper,andlocationsinthebodytext
chitecture,whichincorporatescitationinformation
wherereferencemarkersareembedded.
into mention identification, salient entity classifi-
TomergeSciREXwithS2ORC,welinkrecords
cation,andrelationextraction. Foreachtask,we
usingmetadataobtainedviatheSemanticScholar
consider two types of citation graph information,
API:5 paper title, DOI string, arXiv ID, and Se-
either separately or together: (1) structural infor-
mantic Scholar Paper ID. For each document in
mation from the graph network topology and (2)
SciREX,wecheckagainstall81Mdocumentsin
textualinformationfromthecontentofcitingand
S2ORC for exact matches on any of these identi-
citeddocuments.
fiers, yielding S2ORC entries for 433 out of 438
documents in SciREX. The final mapping is in-
4.1 StructuralInformation
cludedinourrepositoryforthecommunitytouse.
Though our work only used the SciREX dataset, Thestructureofthecitationgraphcancontextual-
ourmethodscanbereadilyextendedtootherSciIE izeadocumentwithinthegreaterbodyofwork.
datasets(includingthosementionedin§2.1)using Priorworksinscientificinformationextraction
ourreleasedsoftware. havepredominantlyusedthecitationgraphonlyto
analyzethecontentofcitingpapers,suchasCite-
Statistics Examiningthedistributionofcitations
TextRank(DasGollapalliandCaragea,2014)and
for all documents in the SciREX dataset (in Fig-
Citation TF-IDF (Caragea et al., 2014), which is
ure3),weobservealong-taileddistributionofci-
describedindetailin§4.2.2. However,thecitation
tationsperpaper,andabell-shapeddistributionof
graphcanbeusedtodiscoverrelationshipsbetween
referencesperpaper.
non-adjacentdocumentsinthecitationgraph;prior
5https://www.semanticscholar.org/ worksstruggletocapturetheserelationships.
ycneuqerF
Stage 1 Stage 2 architecture, though the input to these networks
variesfromtasktotask(SciBERT-BiLSTMembed-
dingsformentionidentification,spanembeddings
Output forsaliententityclassification,andper-sectionre-
lationembeddingsforrelationextraction).
Thisarchitecturegivestwooptionsforwhereto
concatenatethegraphembeddingintothehidden
Figure 4: Feedforward architecture in each task (with state - Stage 1 or Stage 2 - marked with a light
CitationIE-specificparametersshowninlightblue). blueblockinFigure4. Intuitively,concatenating
thegraphembeddinginalaterstagefeedsitmore
directly into the final prediction. We find Stage
Arnold and Cohen (2009) are the only prior
1issuperiorforrelationextraction, andbothper-
work,toourknowledge,toexplicitlyusethecita-
form comparably for salient entity classification
tiongraph’sstructureforscientificIE.Theypredict
andmentionidentification. Wegivedetailsonthis
keyentitiesrelatedtoapaperviarandomwalkson
experimentinAppendixA.3.
acombinedknowledge-and-citation-graphconsist-
ing of papers and entities, without considering a
4.2 TextualInformation
document’s content. This approach is simple but
MostpriorworkusingthecitationgraphforSciIE
cannotgeneralizetoneworunseenentities.
hasfocusedonusingthetextofcitingpapers. We
A rich direction of recent work has studied
examinehowtousetwovarietiesoftextualinfor-
learned representations of networks, such as so-
mationrelatedtocitations.
cial networks (Perozzi et al., 2014) and citation
graphs (Sen et al., 2008; Yang et al., 2015; Bui
4.2.1 Citances
et al., 2018; Khosla et al., 2021). In this paper,
Citation sentences, also known as “citances”
weshowcitationgraphembeddingscanimprove
(Nakovetal.,2004),provideanadditionalsource
scientificinformationextraction.
of textual context about a paper. They have seen
Construction of Citation Graph To construct use in automatic summarization (Yasunaga et al.,
our citation graph, we found all nodes in the 2019),butnotinneuralinformationextraction.
S2ORC citation graph within 2 undirected edges Inourwork,weaugmenteachdocumentinour
ofanydocumentintheSciREXdataset,including trainingsetwithitscitances,treatingeachcitance
alledgesbetweenthosedocuments. Thisprocess as a new section in the document. In this way,
took10hoursononemachineduetothemassive weincorporatecitancesintoourCitationIEmodel
sizeofthefullS2ORCgraph,resultinginagraph through the shared text representations used by
with∼1.1Mnodesand∼5Medges. eachtaskinoursystem,asshowninFigure5. Ifour
documenthasmanycitations,werandomlysample
Network Representation Learning We learn
25 to use. For each citing document, we select
representationsforeachnode(paper)usingDeep-
citances centered on the sentence containing the
Walk6 (Perozzi et al., 2014) via the GraphVite
firstreferencemarkerpointingtoourdocumentof
library (Zhu et al., 2019), resulting in a 128-
interest,andincludethesubsequentandconsequent
dimensional“graphembedding”foreachdocument
sentencesiftheyarebothinthesamesection.
in our dataset. For each task, we incorporate the
Weensurethementionidentificationstepdoes
document-level graph embedding into that task’s
notpredictentitiesincitancesections,whichwould
model component, by simply concatenating the
leadtofalsepositiveentitiesindownstreamtasks.
document’sgraphembeddingwiththehiddenstate
in that component. We do not update the graph 4.2.2 CitationTF-IDF
embeddingvaluesduringtraining.
CitationTF-IDF(Carageaetal.,2014),isafeature
IncorporatingGraphEmbedding Eachtaskin representingtheTF-IDFvalue(Jones,1972)ofa
ourCitationIEsystemculminatesinapairoffeed- given token in its document’s citances. We con-
forwardnetworks. Figure4describesthisgeneral sideravariantofthisfeature: foreachtokenina
document,wecomputetheTF-IDFofthattokenin
6AnempiricalcomparisonbyKhoslaetal.(2021)found
eachcitanceofthedocument,andaveragetheper-
DeepWalktobequitecompetitiveontwocitationgraphnode
classificationdatasets,despiteitsspeedandsimplicity. citanceTF-IDFvaluesoverallcitances. Weimple-
Paper SciBERT BiLSTM RelationExtraction Thisistheultimatetaskin
Content The ourpipeline. Weuseitsoutputandmetricstoevalu-
IBM
... atetheend-to-endsystem,butalsoevaluaterelation
recurrent
neural extraction separately from upstream components
networks
toisolateitsperformance. Wespecificallyconsider
twotypesofmetrics:
Citation [CITE] (1) Document-level: For each document, given a
Sentence used
#1 recurrent set of ground truth 4-ary relations, we evaluate a
…
set of predicted 4-ary relations as a sequence of
binarypredictions(whereamatchingrelationisa
Citation ... truepositive). Wethencomputeprecision,recall,
Sentenceintroduce bd
y andF1scoresforeachdocument,andaverageeach
#25 [CITE]
over all documents. We refer to this metric as
the“document-level”relationmetric. Tocompare
withJainetal.(2020),thisistheprimarymetricto
Figure5: Incorporatingcitancesintothetextrepresen-
measurethefullsystem.
tationextractor.
(2)Corpus-level: Whenevaluatingtherelationex-
tractioncomponentinisolation,wearealsoableto
mentedthisfeatureonlyforsaliencyclassification, use a more standard “corpus-level” binary classi-
asitexplicitlyreasonsaboutthesignificanceofa ficationevaluation,whereeachcandidaterelation
tokenincitingtexts. Asalocaltoken-levelfeature, fromeachdocumentistreatedasaseparatesample.
italsodoesnotapplynaturallytorelationextrac- Wealsorunboththesemetricsonabinaryrela-
tion,whichoperatesonentireclustersofspans. tionextractionsetup,byflatteningeachsetof4-ary
relationsintoasetofbinaryrelationsandevaluat-
4.3 GraphStructureandTextContent ingthesepredictionsasanintermediatemetric.
Welastlyconsiderusinggraphembeddingsandci-
5.1.2 Baselines
tancestogetherinasinglemodelforeachtask. We
Foreachtask,wecompareagainstJainetal.(2020),
dothisnaivelybyincludingcitanceswiththedocu-
whosearchitectureoursystemisbuilton. Noother
ment’sinputtextwhenfirstcomputingsharedtext
modeltoourknowledgeperformsallthetaskswe
features,andthenconcatenatinggraphembeddings
consideronfulldocuments. Forthe4-aryrelation
intodownstreamtask-specificcomponents.
extractiontask,wealsocompareagainsttheDoc-
TAET model (Hou et al., 2019), which is consid-
5 Experiments
eredasstate-of-the-artforfull-textscientificrela-
tionextraction(Jainetal.,2020;Houetal.,2019).
5.1 Metrics,BaselinesandTraining
Significance To improve the rigor of our eval-
5.1.1 Metrics
uation, we run significance tests for each of our
Theultimateproductofourworkisanend-to-end
proposedmethodsagainstitsassociatedbaseline,
document-levelrelationextractionsystem,butwe
viapairedbootstrapsampling(Koehn,2004). Inex-
also measure each component of our system in
perimentswherewetrainedmultiplemodelswith
isolation, giving end-to-end and per-task metrics.
differentseeds,weperformahierarchicalbootstrap
Allmetrics,exceptwherestatedotherwise,arethe
procedure where we first sample a seed for each
sameasdescribedbyJainetal.(2020).
modelandthensamplearandomizedtestset.
Mention Identification We evaluate mention 5.1.3 TrainingDetails
identification with the average F1 score of clas-
WebuildourproposedCitationIEmethodsontop
sifyingentitiesofeachspantype.
oftheSciREXrepository7 (Jainetal.,2020)inthe
AllenNLPframework(Gardneretal.,2018).
Salient Entity Classification Similar to Jain
For each task, we first train that component in
et al. (2020) we evaluate this task at the mention
isolation from the rest of the system to minimize
level and cluster level. We evaluate both metrics
ongoldstandardentityrecognitioninputs. 7https://github.com/allenai/SciREX
Model F1 P R size10 (66samples)andinter-modelvariation.
(3)Incorporatinggraphembeddingsandcitances
SalientMentionEvaluation
simultaneouslyisnobetterthanusingeither.
Baseline(reported) 57.9 57.5 58.4
Baseline(reimpl.) 57.5 50.5 66.8 (4) Our reimplemented baseline differs from the
CitationIE resultsreportedbyJainetal.(2020)despiteusing
w/Citation-TF-IDF 57.1 50.2 66.1
theirpublishedcodetotraintheirmodel. Thismay
w/Citances 58.7† 51.4 68.5†
w/GraphEmbeddings 59.2† 53.5† 66.3 bebecauseweuseabatchsizeof4(duetocompute
w/Graph+Citance 58.4† 51.3 67.8† limits)whiletheyreportedabatchsizeof50.
SalientEntityClusterEvaluation
Relation Extraction Table 2 shows that using
Baseline(reimpl.) 39.1 28.5 75.8
CitationIE graph embeddings here gives an 11.5 point im-
w/Citation-TF-IDF 38.6 28.4 74.3 provementindocument-levelF1overthereported
w/Citances 38.7 28.2 74.8 baseline,11 and statistically significant gains on
w/GraphEmbeddings 40.3 29.8 74.5
bothcorpus-levelF1metrics.
Table 1: Salient entity classification results. Baseline Despiteseeminglylargegainsonthedocument-
(Jain et al., 2020) and Graph Embedding model eval- levelF1metric,thesearenotstatisticallysignificant
uations are each trained with 3 different model seeds, duetosignificantinter-modelvariabilityandsmall
then metrics averaged; rest are from single model due
test set size, despite the graph embedding model
tocomputationallimitations.†indicatessignificanceat
performingbestateveryseedwetried.
95%confidence. Bestmodelisinboldforeachmetric.
End-to-EndModel FromTable3,weobserve:
(1)Usinggraphembeddingsappearstohaveaposi-
the task-specific loss. We then take the best per-
tiveeffectonthemaintaskof4-aryrelationextrac-
forming modifications and use them to train end-
tion. However,thesegainsarenotstatisticallysig-
to-end IE models to minimize the sum of losses
nificant(p = 0.235)despiteourproposedmethod
from all tasks. We train each model on a single
outperforming the baseline at every seed, for the
GPU with batch size 4 for up to 20 epochs. We
samereasonsasmentionedabove.
includedetailedtrainingconfigurationinformation
(2) On binary relation evaluation, we observe
inAppendixA.1.
smallerimprovementswhichhadalowerp-value
For saliency classification and relation extrac-
(p = 0.099)duetolowerinter-modelvariation.
tion,wetrainedthebaselineandthestrongestpro-
(3) Using citances instead of graph embeddings
posedmodelsthreetimes,8toimprovereliabilityof
stillappearstooutperformthebaseline(thoughby
ourresults. Formentionidentification,wedidnot
asmallermarginthanthegraphembeddings).
retrain models, as the first set of results strongly
suggestedourproposedmethodswerenothelpful. 5.3 Analysis
We analyzed our experimental results, guided by
5.2 QuantitativeResults
thefollowingfourquestions:
Mention Identification For mention identifica-
Do papers with few citations benefit from cita-
tion,weobservenomajorperformancedifference
tiongraphinformation? Ourtestsetonlycon-
fromusingcitationgraphs,andincludefullresults
tainstwodocumentswithzerocitations,sowecan-
inAppendixA.2.
notcharacterizeperformanceonsuchdocuments.
However,Figure6showsthatthegainsprovided
SalientEntityClassification Table1showsthe
bytheproposedCitationIEmodelwithgraphem-
resultsofourCitationIEmethods. Weobserve:
beddingscounterintuitivelyshrinkasthenumber
(1)Usingcitationgraphembeddingssignificantly
ofcitationsofapaperincreases. Wealsoobserve
improvesthesystemwithrespecttothesalientmen-
tionmetric. 10Thelimitedsizeofthistestsetisanareaofconcernwhen
(2)Graphembeddingsdonotimproveclustereval- usingtheSciREXdataset,andimprovingstatisticalpowerin
SciIEevaluationisacrucialareaforfuturework.
uationsignificantly(at95%)duetothesmalltest
11Thelargegapbetweenreimplementedandreportedbase-
linesislikelyduetoourreproducedresultsaveragingover3
8SeeAppendixA.1forexactseedsused randomseeds.WhenusingthesameseedusedbyJainetal.
9Reported as “Component-wise Binary and 4-ary Rela- (2020),thebaseline’sdocument-leveltestF1scoreisalmost
tions”inJainetal.(2020) 20pointsbetterthanwithtwootherrandomseeds.
Model F1 P R F1 P R
4-aryRelationExtraction
Document-LevelMetric Corpus-LevelMetric
Baseline(reported)9 57.0 82.0 44.0 N/A N/A N/A
Baseline(reimpl.) 49.8 50.1 50.1 48.0 48.1 48.2
DocTAET 65.5 62.4 85.1 39.9 55.7 56.8
CitationIE
w/Citances 69.2 70.0 76.6 39.4 39.9 41.9
w/GraphEmbeddings 68.5 67.5 76.2 58.7† 61.0† 59.6
w/Graph+Citance 67.5 66.8 75.0 51.9 54.6 54.5
BinaryRelationExtraction
Document-LevelMetric Corpus-LevelMetric
Baseline(reported) 61.1 53.1 71.8 N/A N/A N/A
Baseline(reimpl.) 50.8 51.1 51.1 41.2 48.4 44.6
CitationIE
w/Citances 69.2 69.2 71.3 43.3 46.7 44.0
w/GraphEmbeddings 72.9 70.4 56.1 51.0† 54.1† 57.1
w/Graph+Citance 66.2 65.9 68.1 48.0† 51.4 52.7
Table 2: Comparing methods on relation extraction. Baseline, Graph Embedding, and Graph + Citance models
were evaluated over 3 model seeds, and the remainder with a single seed. We use Macro-F1 for corpus-level
evaluation. † indicates significance at 95% confidence, and best implemented model in each metric is bolded.
Graphembeddingssignificantlyimproveoverbaselineon4-aryandbinarycorpus-levelF1(p<0.05),butareless
significantondocument-levelF1metrics(p≈0.11).
Model F1 P R 1.0
Baseline Baseline
4-aryRelationExtraction 0.8 w/ Graph w/ Citances
0.6
Baseline(reported) 0.8 0.7 17.3
Baseline(reimpl.) 0.44 0.23 22.66 0.4
CitationIE 0.2
w/GraphEmbeddings 1.48 1.31 20.04
0.0
w/Citances 0.75 7.03 13.36 (0, 70) (70, 450) (450, 12K) (0, 70) (70, 450) (450, 12K)
BinaryRelationExtraction
Figure6: Document-levelrelationextractionF1score
Baseline(reported) 9.6 6.5 41.1
ofCitationIEmodelswithgraphembeddings(left)and
Baseline(reimpl.) 6.48 4.09 43.83
citances (right), compared with the baseline (red) on
CitationIE
w/GraphEmbeddings 7.70 5.42 37.17 documentsgroupedbynumberofcitations.
w/Citances 7.61 4.97 43.57
Table3:End-to-endmodelevaluation.Eachmodelwas large swaths of text. This is particularly useful
evaluatedover3modelseeds. since neural models still struggle to model long-
rangedependencieseffectively(Brownetal.,2020).
thiswithcitances,toalesserextent. Thissuggests
moreworkneedstobedonetorepresentcitation Does citation graph information help contextu-
graphnodeswithmanyedges. alizeimportantterms? Goingbacktoourmoti-
vatingexampleofaspeechpaperreferringtoIma-
How does citation graph information help re- geNetinpassing§1,wehypothesizedthatadding
lation extraction? Withrelation extraction, we contextfromcitationshelpsdealwithtermsthatare
foundcitationgraphinformationprovidesstrongest importantingeneral,butnotforagivendocument.
gains when classifying relations between distant Tomeasurethis, wegroupedallentitiesinour
entitiesinadocument,seeninFigure7. Foreach testdatasetbytheir“globalsaliencyrate”measured
relation in the test set, we computed the average onthetestset: givenaspan,whatistheprobability
distance between pairs of entity mentions in that thatthisspanissalientinanygivenoccurrence?
relation,normalizedbytotaldocumentlength. We InFigure8,weobservethatmostoftheimprove-
findmodelswithgraphembeddingsorcitancesper- mentfromgraphembeddingsandcitancescomes
form markedly better when these relations span attermswhicharelabeledassalientinatleast20%
1F
leveL-tnemucoD
Acknowledgments
1.0
Baseline Baseline
0.8 w/ Graph w/ Citances TheauthorsthankSarthakJainforassistingwithre-
0.6
producingbaselineresults,BharadwajRamachan-
0.4
dran for giving advice on figures, and Siddhant
0.2
AroraandRishabhJoshiforprovidingsuggestions
0.0
(0.0, 0.2 (9 0) .29, 0.3 (7 0) .37, 0.4 (0) .4, 0.54) (0.0, 0.29 () 0.29, 0.37 () 0.37, 0.4) (0.4, 0.54) on the paper. The authors also thank the anony-
mousreviewersfortheirhelpfulcomments. This
Figure7: Corpus-LevelF1ofrelationextractionmod-
work was supported by the Air Force Research
els, bucketed by the average distance between entity
LaboratoryunderagreementnumberFA8750-19-
mentionsineachrelation.
2-0200. The U.S. Government is authorized to
reproduceanddistributereprintsforGovernmen-
0.8
tal purposes notwithstanding any copyright nota-
0.6 tionthereon. Theviewsandconclusionscontained
0.4 herein are those of the authors and should not be
0.2 Baseline Baseline interpretedasnecessarilyrepresentingtheofficial
w/ Graph w/ Citances
0.0 policies or endorsements, either expressed or im-
(0.0,
0. (2 0.)
2,
0. (4 0.)
4,
0. (6 0.)
6,
0. (8 0.)
8,
1.0)
(0.0,
0.2 ()
0.2,
0.4 ()
0.4,
0.6 ()
0.6,
0.8 ()
0.8,
1.0)
plied,oftheAirForceResearchLaboratoryorthe
U.S.Government.
Figure 8: Macro F1 of salient mention classification
models, evaluated on test-set spans, each bucketed by
theirtraining-setglobalsaliencyrate. References
AndrewO.ArnoldandWilliamW.Cohen.2009. Infor-
mation extraction as link prediction: Using curated
of their training-set mentions. This suggests that
citation networks to improve gene detection. In-
citation graph information yields improvements
ternationalConferenceonWirelessAlgorithms,Sys-
withreasoningaboutimportantterms,withoutneg- tems,andApplications(WASA),5682:541–550.
ativelyinterferingwithless-importantterms.
Isabelle Augenstein, Mrinal Das, Sebastian Riedel,
Lakshmi Vikraman, and Andrew McCallum. 2017.
6 ImplicationsandFutureDirections
SemEval 2017 task 10: ScienceIE - extracting
keyphrases and relations from scientific publica-
Weexploretheuseofcitationgraphinformationin
tions. In Proceedings of the 11th International
neuralscientificinformationextractionwithCita-
Workshop on Semantic Evaluation (SemEval-2017),
tionIE,amodelthatcanleverageeitherthestruc- pages546–555,Vancouver,Canada.Associationfor
ture of the citation graph or the content of citing ComputationalLinguistics.
orciteddocuments. Wefindthatthisinformation, DzmitryBahdanau,KyunghyunCho,andYoshuaBen-
combinedwithdocumenttext,leadstoparticularly gio. 2014. Neural machine translation by jointly
strong improvements for salient entity classifica- learningtoalignandtranslate. ICLR.
tion and relation extraction, and provides an in- IzBeltagy,KyleLo,andArmanCohan.2019. Scibert:
creaseinend-to-endIEsystemperformanceovera A pretrained language model for scientific text. In
strongbaseline. EMNLP/IJCNLP.
Ourproposedmethodsreflectsomeofthesim- TomB.Brown,BenjaminMann,NickRyder,Melanie
plestwaysofincorporatingcitationgraphinforma- Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
tion into a neural SciIE system. As such, these Neelakantan,PranavShyam,GirishSastry,Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
results can be considered a proof of concept. In
Gretchen Krueger, Tom Henighan, Rewon Child,
thefuturewewillexplorewaystoextractricherin- Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
formationfromthegraphusingmoresophisticated Clemens Winter, Christopher Hesse, Mark Chen,
techniques,hopefullybettercapturingtheinterplay Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
between citation graph structure and content. Fi-
Candlish, Alec Radford, Ilya Sutskever, and Dario
nally, we evaluated our proof of concept here on
Amodei.2020. Languagemodelsarefew-shotlearn-
a single dataset in the machine learning domain. ers.
Whileourmethodsarenotdomain-specific,verify-
Thang D. Bui, Sujith Ravi, and Vivek Ramavajjala.
ingthatthesemethodsgeneralizetootherscientific
2018. Neural graph learning: Training neural net-
domainsisimportantfuturework. works using graphs. Proceedings of the Eleventh
1F leveL-suproC
1F-orcaM
ACM International Conference on Web Search and Sarthak Jain, Madeleine van Zuylen, Hannaneh Ha-
DataMining. jishirzi, and Iz Beltagy. 2020. SciREX: A chal-
lengedatasetfordocument-levelinformationextrac-
Cornelia Caragea, Florin Adrian Bulgarov, Andreea tion. In Proceedings of the 58th Annual Meeting
Godea, and Sujatha Das Gollapalli. 2014. Citation- of the Association for Computational Linguistics,
enhanced keyphrase extraction from research pa- pages 7506–7516, Online. Association for Compu-
pers: A supervised approach. In Proceedings of tationalLinguistics.
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1435– Karen Sparck Jones. 1972. A statistical interpretation
1446, Doha, Qatar. Association for Computational of term specificity and its application in retrieval.
Linguistics. JournalofDocumentation.
Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Se-
Sujatha Das Gollapalli and Cornelia Caragea. 2014.
bastian Ruder, Sebastian Riedel, Ross Taylor, and
Extractingkeyphrasesfromresearchpapersusingci-
RobertStojnic.2020. AxCell: Automaticextraction
tation networks. Proceedings of the AAAI Confer-
ofresultsfrommachinelearningpapers. InProceed-
enceonArtificialIntelligence,28(1).
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
Kata Ga´bor, Davide Buscaldi, Anne-Kathrin Schu-
8580–8594, Online. Association for Computational
mann, Behrang QasemiZadeh, Ha¨ıfa Zargayouna,
Linguistics.
and Thierry Charnois. 2018. SemEval-2018 task
7: Semanticrelationextractionandclassificationin
MeghaKhosla,VinaySetty,andAvishekAnand.2021.
scientific papers. In Proceedings of The 12th Inter-
A comparative study for unsupervised network rep-
national Workshop on Semantic Evaluation, pages
resentation learning. IEEE Transactions on Knowl-
679–688, New Orleans, Louisiana. Association for
edgeandDataEngineering,33(5):1807–1818.
ComputationalLinguistics.
Philipp Koehn. 2004. Statistical significance tests
Matt Gardner, Joel Grus, Mark Neumann, Oyvind for machine translation evaluation. In Proceed-
Tafjord,PradeepDasigi,NelsonF.Liu,MatthewPe- ings of the 2004 Conference on Empirical Meth-
ters,MichaelSchmitz,andLukeZettlemoyer.2018. ods in Natural Language Processing, pages 388–
AllenNLP: A deep semantic natural language pro- 395, Barcelona, Spain. Association for Computa-
cessing platform. In Proceedings of Workshop for tionalLinguistics.
NLP Open Source Software (NLP-OSS), pages 1–
6, Melbourne, Australia. Association for Computa- John Lafferty, Andrew McCallum, and Fernando C.N.
tionalLinguistics. Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
AlexGraves,SantiagoFerna´ndez,andJu¨rgenSchmid- quencedata. InICML.
huber. 2005. Bidirectional lstm networks for im-
KyleLo,LucyLuWang,MarkNeumann,RodneyKin-
proved phoneme classification and recognition. In
ney,andDanielWeld.2020. S2ORC:Thesemantic
Proceedings of the 15th International Conference
scholaropenresearchcorpus. InProceedingsofthe
on Artificial Neural Networks: Formal Models and
58thAnnualMeetingoftheAssociationforCompu-
Their Applications - Volume Part II, ICANN’05,
tational Linguistics, pages 4969–4983, Online. As-
page799–804,Berlin,Heidelberg.Springer-Verlag.
sociationforComputationalLinguistics.
SonalGuptaandChristopherManning.2011. Analyz-
Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh
ing the dynamics of research by extracting key as-
Hajishirzi. 2018. Multi-task identification of enti-
pectsofscientificpapers. InProceedingsof5thIn-
ties, relations, and coreference for scientific knowl-
ternational Joint Conference on Natural Language
edgegraphconstruction. InProceedingsofthe2018
Processing,pages1–9,ChiangMai,Thailand.Asian
Conference on Empirical Methods in Natural Lan-
FederationofNaturalLanguageProcessing.
guageProcessing, pages3219–3232, Brussels, Bel-
gium.AssociationforComputationalLinguistics.
KenHallenbeck.2020. Thecovid-19deluge: Isittime
foranewmodelofdatadisclosure? ASBMBToday: YiLuan, DaveWadden, LuhengHe, AmyShah, Mari
The Member Magazine of the American Society for Ostendorf, and Hannaneh Hajishirzi. 2019. A gen-
BiochemistryandMolecularBiology. eralframeworkforinformationextractionusingdy-
namic span graphs. In Proceedings of the 2019
YufangHou,CharlesJochim,MartinGleize,Francesca Conference of the North American Chapter of the
Bonin, and Debasis Ganguly. 2019. Identifica- Association for Computational Linguistics: Human
tion of tasks, datasets, evaluation metrics, and nu- Language Technologies, Volume 1 (Long and Short
mericscoresforscientificleaderboardsconstruction. Papers),pages3036–3046,Minneapolis,Minnesota.
In Proceedings of the 57th Annual Meeting of the AssociationforComputationalLinguistics.
Association for Computational Linguistics, pages
5203–5213,Florence,Italy.AssociationforCompu- Preslav I. Nakov, Ariel S. Schwartz, and Marti A.
tationalLinguistics. Hearst. 2004. Citances: Citation sentences for se-
mantic analysis of bioscience text. In In Proceed- ZhaochengZhu,ShizhenXu,MengQu,andJianTang.
ings of the SIGIR’04 workshop on Search and Dis- 2019. Graphvite: A high-performance cpu-gpu hy-
coveryinBioinformatics. bridsystemfornodeembedding. InTheWorldWide
WebConference,pages2494–2504.ACM.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk:Onlinelearningofsocialrepresen-
tations. In Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery
anddatamining,pages701–710.
Nils Reimers and Iryna Gurevych. 2017. Optimal hy-
perparameters for deep lstm-networks for sequence
labelingtasks. ArXiv,abs/1707.06799.
George Saon, Tom Sercu, Steven Rennie, and Hong-
Kwang J. Kuo. 2016. The IBM 2016 english con-
versationaltelephonespeechrecognitionsystem. In
INTERSPEECH.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise
Getoor,BrianGallagher,andTinaEliassi-Rad.2008.
Collective classification in network data. AI Maga-
zine,29:93–106.
YuvalVarkelandAmirGloberson.2020. Pre-training
mention representations in coreference models. In
EMNLP.
Karin M. Verspoor, K. Cohen, Arrick Lanfranchi,
C. Warner, Helen L. Johnson, Christophe Roeder,
JinhoD.Choi,ChristopherS.Funk,YuriyMalenkiy,
Miriam Eckert, Nianwen Xue, W. Baumgartner,
M. Bada, Martha Palmer, and L. Hunter. 2011. A
corpusoffull-textjournalarticlesisarobustevalua-
tiontoolforrevealingdifferencesinperformanceof
biomedicalnaturallanguageprocessingtools. BMC
Bioinformatics,13:207–207.
Chih-Hsuan Wei, Yifan Peng, Robert Leaman, Al-
lan Peter Davis, Carolyn J Marringly, Jiao Li,
ThomasCWiegers,andZhiyongLu.2016. Assess-
ingthestateoftheartinbiomedicalrelationextrac-
tion: overviewofthebiocreativevchemical-disease
relation(cdr)task. Database: thejournalofbiolog-
icaldatabasesandcuration.
Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun,
and Edward Y. Chang. 2015. Network representa-
tionlearningwithrichtextinformation. InIJCAI.
MichihiroYasunaga,JungoKasai,RuiZhang,Alexan-
der R. Fabbri, Irene Li, Dan Friedman, and
Dragomir R. Radev. 2019. Scisummnet: A large
annotatedcorpusandcontent-impactmodelsforsci-
entific paper summarization with citation networks.
ProceedingsoftheAAAIConferenceonArtificialIn-
telligence,33(01):7386–7393.
DaniYogatama,MichaelHeilman,BrendanO’Connor,
ChrisDyer,BryanR.Routledge,andNoahA.Smith.
2011. Predictingascientificcommunity’sresponse
toanarticle. InProceedingsofthe2011Conference
onEmpiricalMethodsinNaturalLanguageProcess-
ing,pages594–604,Edinburgh,Scotland,UK.Asso-
ciationforComputationalLinguistics.
A Appendices A.3 CombiningGraphEmbeddingswith
WordEmbeddings
A.1 TrainingConfigurations
Eachofourtask-specificcomponentsintheCita-
We train each model on a single 11GB NVIDIA
tionIEmodelcontainstwofeedforwardnetworks
GeForce RTX 2080 Ti GPU with a batch size
wherewemayconcatenategraphembeddinginfor-
of 4. We train for up to 20 epochs, and set the
mation. Werefertothesetwooptionsforwhereto
patience parameter in AllenNLP to 10; if the
fusegraphembeddinginformationas”earlyfusion”
validationmetricdoesnotimprovefor10consecu-
and”latefusion”,illustratedinFigure4.
tiveepochs,westoptrainingearly. Foreachtask-
Here we show a detailed comparison of early
specificmodel,weuseaproductofvalidationloss
fusion vs late fusion models on Mention Identifi-
andcorpus-levelbinaryF1scoreonthevalidation
cation(Table5),SalientEntityClassification(Ta-
setasthevalidationmetric. Forsaliententityclassi-
ble 6), and Relation Extraction (Table 7). Based
ficationandrelationextraction,wechoosethebest
ontheseresults,weusedearlyfusioninourfinal
thresholdonthevalidationsetusingF1score.
CitationIE models for mention identification and
Intotal,trainingwiththeseconfigurationstakes
relationextraction. Forsaliencyclassification,the
roughly2hoursforsaliententityclassification,8
relative performance of early fusion and late fu-
hours for mention identification, 18-24 hours for
siondifferedacrossourtwometrics,makingthis
relationextraction,and24-30hoursfortheend-to-
inconclusive. We used early fusion for saliency
endsystem. OurCitationIEmodelstookroughly
classificationintheend-to-endmodelduetostrong
aslongtotrainasthebaselineSciREXmodelsdid.
empiricalperformancethere.
Formodelsthatwetrainedthreedifferenttimes,
weusedifferentseedsforeachsoftwarelibrary:
Model F1 P R
• ForPyTorch,weuseseeds133,12 11,and22 MentionIdentification
GraphEmbed.(earlyfusion) 74.4† 74.4† 74.3
• ForNumpy,weuseseeds1337,111,and222 GraphEmbed.(latefusion) 74.1 73.1 75.1†
• For Python’s random library, we use seeds Table 5: Comparing CitationIE models for mention
identification with early graph embedding fusion vs
11370,1111,and2222
latefusion.Resultsareshownfromsingle-modelevalu-
ation. †indicatessignificanceat95%confidence. Best
A.2 MentionIdentificationResults
modelisinboldforeachmetric.
Model F1 P R
MentionIdentification Model F1 P R
Baseline(reported)13 70.7 71.7 71.2 SalientMentionEvaluation
Baseline(reimpl.) 74.6† 73.7 75.6† GraphEmbed.(earlyfusion) 57.1 54.4† 60.1
w/Citances 74.0 73.0 75.0 GraphEmbed.(latefusion) 59.2† 53.5 66.3†
w/GraphEmbeddings 74.4 74.4† 74.3
w/Graph+Citance 73.6 73.0 74.3 SalientEntityClusterEvaluation
GraphEmbed.(earlyfusion) 43.3† 33.8† 72.0
Table4:MentionIdentificationResults.†indicatessig- GraphEmbed.(latefusion) 40.3 29.8 74.5†
nificanceat95%confidence. Bestmodelisinboldfor
eachmetric. Table 6: Comparing CitationIE models for salient en-
tity classification with early graph embedding fusion
vslatefusion.Theearlyfusionmodelwastrainedonce,
Weincluderesultsfromusingcitationgraphin-
whilelatefusionnumbersarereportedoveranaverage
formationforthementionidentificationtaskinTa-
of 3 runs. † indicates significance at 95% confidence.
ble4. Weobservenomajorimprovementsinthis Bestmodelisinboldforeachmetric.
task. Intuitively, recognizing a named entity in a
documentmaynotrequireglobalcontextaboutthe
document(e.g. “LSTM”almostalwaysreferstoa
Method,regardlessofthepaperwhereitisused),
sothelackofgainsinthistaskisunsurprising.
12133/1337/13370isthedefaultseedsettinginAllenNLP.
Model F1 P R F1 P R
4-aryRelationExtraction
Document-LevelMetrics Corpus-LevelMetrics
GraphEmbeddings(earlyfusion) 68.5 67.5 76.2 58.7 61.0 59.6
GraphEmbeddings(latefusion) 63.3 61.8 67.3 75.8† 76.0† 76.1†
BinaryRelationExtraction
Document-LevelMetrics Corpus-LevelMetrics
GraphEmbeddings(earlyFusion) 72.9 70.4 56.1 51.0 54.1 57.1
GraphEmbeddings(latefusion) 58.3 58.0 59.0 53.6 58.1† 66.4
Table 7: Comparing CitationIE models for relation extraction with early graph embedding fusion vs late fusion.
Earlyfusionmodelsweretrained3times,latefusionwastrainedonce. †indicatessignificanceat95%confidence,
andthebestmodelineachmetricisbolded.

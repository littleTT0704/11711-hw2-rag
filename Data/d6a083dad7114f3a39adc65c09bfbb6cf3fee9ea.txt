Energy and Policy Considerations for Deep Learning in NLP
EmmaStrubell AnanyaGanesh AndrewMcCallum
CollegeofInformationandComputerSciences
UniversityofMassachusettsAmherst
{strubell, aganesh, mccallum}@cs.umass.edu
Abstract Consumption CO e(lbs)
2
Airtravel,1person,NY↔SF 1984
Recent progress in hardware and methodol-
Humanlife,avg,1year 11,023
ogy for training neural networks has ushered
Americanlife,avg,1year 36,156
in a new generation of large networks trained
Car,avgincl. fuel,1lifetime 126,000
on abundant data. These models have ob-
tained notable gains in accuracy across many
NLPtasks. However,theseaccuracyimprove- Trainingonemodel(GPU)
mentsdependontheavailabilityofexception- NLPpipeline(parsing,SRL) 39
allylargecomputationalresourcesthatneces-
w/tuning&experiments 78,468
sitate similarly substantial energy consump-
Transformer(big) 192
tion. As a result these models are costly to
w/neuralarch. search 626,155
train and develop, both financially, due to the
costofhardwareandelectricityorcloudcom-
Table1: EstimatedCO emissionsfromtrainingcom-
putetime,andenvironmentally,duetothecar- 2
monNLPmodels,comparedtofamiliarconsumption.1
bon footprint required to fuel modern tensor
processing hardware. In this paper we bring
this issue to the attention of NLP researchers
NLP models could be trained and developed on
by quantifying the approximate financial and
a commodity laptop or server, many now require
environmentalcostsoftrainingavarietyofre-
multipleinstancesofspecializedhardwaresuchas
cently successful neural network models for
NLP.Basedonthesefindings,weproposeac- GPUs or TPUs, therefore limiting access to these
tionablerecommendationstoreducecostsand highlyaccuratemodelsonthebasisoffinances.
improveequityinNLPresearchandpractice. Even when these expensive computational re-
sourcesareavailable,modeltrainingalsoincursa
1 Introduction
substantial cost to the environment due to the en-
Advances in techniques and hardware for train- ergyrequiredtopowerthishardwareforweeksor
ing deep neural networks have recently en- monthsatatime. Thoughsomeofthisenergymay
abled impressive accuracy improvements across come from renewable or carbon credit-offset re-
many fundamental NLP tasks (Bahdanau et al., sources,thehighenergydemandsofthesemodels
2015; Luong et al., 2015; Dozat and Man- arestillaconcernsince(1)energyisnotcurrently
ning, 2017; Vaswani et al., 2017), with the derivedfromcarbon-neuralsourcesinmanyloca-
most computationally-hungry models obtaining tions,and(2)whenrenewableenergyisavailable,
thehighestscores(Petersetal.,2018;Devlinetal., it is still limited to the equipment we have to pro-
2019; Radford et al., 2019; So et al., 2019). As duceandstoreit,andenergyspenttraininganeu-
a result, training a state-of-the-art model now re- ral network might better be allocated to heating a
quires substantial computational resources which family’s home. It is estimated that we must cut
demand considerable energy, along with the as- carbon emissions by half over the next decade to
sociated financial and environmental costs. Re- deterescalatingratesofnaturaldisaster,andbased
searchanddevelopmentofnewmodelsmultiplies on the estimated CO emissions listed in Table 1,
2
thesecostsbythousandsoftimesbyrequiringre-
1Sources: (1) Air travel and per-capita consumption:
training to experiment with model architectures
https://bit.ly/2Hw0xWc; (2)carlifetime: https:
andhyperparameters. Whereasadecadeagomost //bit.ly/2Qbr0w1.
model training and development likely make up Consumer Renew. Gas Coal Nuc.
a substantial portion of the greenhouse gas emis- China 22% 3% 65% 4%
sionsattributedtomanyNLPresearchers. Germany 40% 7% 38% 13%
To heighten the awareness of the NLP commu- UnitedStates 17% 35% 27% 19%
nitytothisissueandpromotemindfulpracticeand Amazon-AWS 17% 24% 30% 26%
policy, we characterize the dollar cost and carbon Google 56% 14% 15% 10%
emissions that result from training the neural net- Microsoft 32% 23% 31% 10%
works at the core of many state-of-the-art NLP
models. We do this by estimating the kilowatts Table2:Percentenergysourcedfrom:Renewable(e.g.
of energy required to train a variety of popular hydro, solar, wind), natural gas, coal and nuclear for
off-the-shelfNLPmodels,whichcanbeconverted thetop3cloudcomputeproviders(Cooketal.,2017),
to approximate carbon emissions and electricity compared to the United States,4 China5 and Germany
(Burger,2019).
costs. To estimate the even greater resources re-
quired to transfer an existing model to a new task
or develop new models, we perform a case study We estimate the total time expected for mod-
ofthefullcomputationalresourcesrequiredforthe elstotraintocompletionusingtrainingtimesand
developmentandtuningofarecentstate-of-the-art hardwarereportedintheoriginalpapers. Wethen
NLPpipeline(Strubelletal.,2018). Weconclude calculatethepowerconsumptioninkilowatt-hours
withrecommendationstothecommunitybasedon (kWh) as follows. Let p be the average power
c
ourfindings, namely: (1)Timetoretrainandsen- draw(inwatts)fromallCPUsocketsduringtrain-
sitivity to hyperparameters should be reported for ing, let p be the average power draw from all
r
NLP machine learning models; (2) academic re- DRAM(mainmemory)sockets,letp betheaver-
g
searchers need equitable access to computational agepowerdrawofaGPUduringtraining, andlet
resources;and(3)researchersshouldprioritizede- g be the number of GPUs used to train. We esti-
velopingefficientmodelsandhardware. matetotalpowerconsumptionascombinedGPU,
CPUandDRAMconsumption, thenmultiplythis
2 Methods
by Power Usage Effectiveness (PUE), which ac-
counts for the additional energy required to sup-
To quantify the computational and environmen-
port the compute infrastructure (mainly cooling).
tal cost of training deep neural network mod-
WeuseaPUEcoefficientof1.58,the2018global
els for NLP, we perform an analysis of the en-
averagefordatacenters(Ascierto,2018). Thenthe
ergy required to train a variety of popular off-
total power p required at a given instance during
the-shelf NLP models, as well as a case study of t
trainingisgivenby:
thecompletesumofresourcesrequiredtodevelop
LISA(Strubelletal.,2018),astate-of-the-artNLP 1.58t(p +p +gp )
c r g
p = (1)
model from EMNLP 2018, including all tuning t 1000
andexperimentation.
TheU.S.EnvironmentalProtectionAgency(EPA)
Wemeasureenergyuseasfollows. Wetrainthe
provides average CO produced (in pounds per
2
modelsdescribedin§2.1usingthedefaultsettings
kilowatt-hour) for power consumed in the U.S.
provided, and sample GPU and CPU power con-
(EPA, 2018), which we use to convert power to
sumptionduringtraining. Eachmodelwastrained
estimatedCO emissions:
2
for a maximum of 1 day. We train all models on
a single NVIDIA Titan X GPU, with the excep- CO 2e = 0.954p t (2)
tion of ELMo which was trained on 3 NVIDIA
Thisconversiontakesintoaccounttherelativepro-
GTX 1080 Ti GPUs. While training, we repeat-
portionsofdifferentenergysources(primarilynat-
edly query the NVIDIA System Management In-
ural gas, coal, nuclear and renewable) consumed
terface2 to sample the GPU power consumption
to produce energy in the United States. Table 2
andreporttheaverageoverallsamples. Tosample
lists the relative energy sources for China, Ger-
CPU power consumption, we use Intel’s Running
many and the United States compared to the top
AveragePowerLimitinterface.3
5U.S.Dept.ofEnergy:https://bit.ly/2JTbGnI
2nvidia-smi:https://bit.ly/30sGEbi 5China Electricity Council; trans. China Energy Portal:
3RAPLpowermeter:https://bit.ly/2LObQhV https://bit.ly/2QHE5O3
three cloud service providers. The U.S. break- asquestionansweringandnaturallanguageinfer-
down of energy is comparable to that of the most ence. Devlin et al. (2019) report that the BERT
popularcloudcomputeservice,AmazonWebSer- base model (BERT ; 110M parameters) was
base
vices, so we believe this conversion to provide a trained on 16 TPU chips for 4 days (96 hours).
reasonableestimateofCO emissionsperkilowatt NVIDIAreportsthattheycantrainaBERTmodel
2
hourofcomputeenergyused. in3.3days(79.2hours)using4DGX-2Hservers,
totaling64TeslaV100GPUs(Forsteretal.,2019).
2.1 Models GPT-2. This model is the latest edition of
OpenAI’s GPT general-purpose token encoder,
We analyze four models, the computational re-
alsobasedonTransformer-styleself-attentionand
quirementsofwhichwedescribebelow. Allmod-
trained with a language modeling objective (Rad-
els have code freely available online, which we
ford et al., 2019). By training a very large model
usedout-of-the-box. Formoredetailsonthemod-
on massive data, Radford et al. (2019) show high
elsthemselves,pleaserefertotheoriginalpapers.
zero-shotperformanceonquestionansweringand
Transformer. The Transformer (T2T) model
language modeling benchmarks. The large model
(Vaswanietal.,2017)isanencoder-decoderarchi-
described in Radford et al. (2019) has 1542M pa-
tectureprimarilyrecognizedforefficientandaccu-
rameters and is reported to require 1 week (168
ratemachinetranslation. Theencoderanddecoder
hours)oftrainingon32TPUv3chips. 6
eachconsistof6stackedlayersofmulti-headself-
attention. Vaswani et al. (2017) report that the
3 Relatedwork
Transformer base model (T2T ; 65M param-
base
eters) was trained on 8 NVIDIA P100 GPUs for There is some precedent for work characterizing
12hours,andtheTransformerbigmodel(T2T ; thecomputationalrequirementsoftrainingandin-
big
213M parameters) was trained for 3.5 days (84 ferenceinmodernneuralnetworkarchitecturesin
hours; 300k steps). This model is also the ba- the computer vision community. Li et al. (2016)
sis for recent work on neural architecture search presentadetailedstudyoftheenergyuserequired
(NAS)formachinetranslationandlanguagemod- fortrainingandinferenceinpopularconvolutional
eling (So et al., 2019), and the NLP pipeline that models for image classification in computer vi-
we study in more detail in §4.2 (Strubell et al., sion, including fine-grained analysis comparing
2018). So et al. (2019) report that their full ar- different neural network layer types. Canziani
chitecture search ran for a total of 979M training etal.(2016)assessimageclassificationmodelac-
steps, and that their base model requires 10 hours curacy as a function of model size and gigaflops
to train for 300k steps on one TPUv2 core. This required during inference. They also measure av-
equates to 32,623 hours of TPU or 274,120 hours erage power draw required during inference on
on8P100GPUs. GPUsasafunctionofbatchsize. Neitherworkan-
ELMo. The ELMo model (Peters et al., 2018) alyzestherecurrentandself-attentionmodelsthat
is based on stacked LSTMs and provides rich have become commonplace in NLP, nor do they
wordrepresentationsincontextbypre-trainingon extrapolate power to estimates of carbon and dol-
a large amount of data using a language model- larcostoftraining.
ingobjective. Replacingcontext-independentpre- Analysis of hyperparameter tuning has been
trained word embeddings with ELMo has been performed in the context of improved algorithms
shown to increase performance on downstream for hyperparameter search (Bergstra et al., 2011;
tasks such as named entity recognition, semantic BergstraandBengio,2012;Snoeketal.,2012). To
rolelabeling,andcoreference. Petersetal.(2018) our knowledge there exists to date no analysis of
reportthatELMowastrainedon3NVIDIAGTX the computation required for R&D and hyperpa-
1080GPUsfor2weeks(336hours). rametertuningofneuralnetworkmodelsinNLP.
BERT.TheBERTmodel(Devlinetal.,2019)pro-
6ViatheauthorsonReddit.
vides a Transformer-based architecture for build-
7GPU lower bound computed using pre-emptible
ing contextual representations similar to ELMo, P100/V100 U.S. resources priced at $0.43–$0.74/hr, upper
buttrainedwithadifferentlanguagemodelingob- bound uses on-demand U.S. resources priced at $1.46–
$2.48/hr. Wesimilarlyusepre-emptible($1.46/hr–$2.40/hr)
jective. BERTsubstantiallyimprovesaccuracyon
andon-demand($4.50/hr–$8/hr)pricingaslowerandupper
tasksrequiringsentence-levelrepresentationssuch boundsforTPUv2/3;cheaperbulkcontractsareavailable.
Model Hardware Power(W) Hours kWh·PUE CO e Cloudcomputecost
2
T2T P100x8 1415.78 12 27 26 $41–$140
base
T2T P100x8 1515.43 84 201 192 $289–$981
big
ELMo P100x3 517.66 336 275 262 $433–$1472
BERT V100x64 12,041.51 79 1507 1438 $3751–$12,571
base
BERT TPUv2x16 — 96 — — $2074–$6912
base
NAS P100x8 1515.43 274,120 656,347 626,155 $942,973–$3,201,722
NAS TPUv2x1 — 32,623 — — $44,055–$146,848
GPT-2 TPUv3x32 — 168 — — $12,902–$43,008
Table3:EstimatedcostoftrainingamodelintermsofCO emissions(lbs)andcloudcomputecost(USD).7Power
2
andcarbonfootprintareomittedforTPUsduetolackofpublicinformationonpowerdrawforthishardware.
4 Experimentalresults Estimatedcost(USD)
Models Hours Cloud Electric
4.1 Costoftraining
1 120 $52–$175 $5
Table 3 lists CO 2 emissions and estimated cost of 24 2880 $1238–$4205 $118
training the models described in §2.1. Of note is 4789 239,942 $103k–$350k $9870
that TPUs are more cost-efficient than GPUs on
workloadsthatmakesenseforthathardware(e.g. Table4: Estimatedcostintermsofcloudcomputeand
BERT). We also see that models emit substan- electricity for training: (1) a single model (2) a single
tuneand(3)allmodelstrainedduringR&D.
tial carbon emissions; training BERT on GPU is
roughly equivalent to a trans-American flight. So
etal.(2019)reportthatNASachievesanewstate- about60GPUsrunningconstantlythroughoutthe
of-the-artBLEUscoreof29.7forEnglishtoGer- 6monthdurationoftheproject. Table4listsupper
man machine translation, an increase of just 0.1 and lower bounds of the estimated cost in terms
BLEU at the cost of at least $150k in on-demand of Google Cloud compute and raw electricity re-
computetimeandnon-trivialcarbonemissions. quiredtodevelopanddeploythismodel.9 Wesee
that while training a single model is relatively in-
4.2 Costofdevelopment: Casestudy
expensive, the cost of tuning a model for a new
To quantify the computational requirements of dataset,whichweestimateheretorequire24jobs,
R&D for a new model we study the logs of or performing the full R&D required to develop
all training required to develop Linguistically- thismodel,quicklybecomesextremelyexpensive.
Informed Self-Attention (Strubell et al., 2018), a
multi-taskmodelthatperformspart-of-speechtag- 5 Conclusions
ging,labeleddependencyparsing,predicatedetec-
Authorsshouldreporttrainingtimeand
tionandsemanticrolelabeling. Thismodelmakes
sensitivitytohyperparameters.
for an interesting case study as a representative
NLPpipelineandasaBestLongPaperatEMNLP. Our experiments suggest that it would be benefi-
Model training associated with the project cial to directly compare different models to per-
spannedaperiodof172days(approx. 6months). form a cost-benefit (accuracy) analysis. To ad-
During that time 123 small hyperparameter grid dress this, when proposing a model that is meant
searches were performed, resulting in 4789 jobs to be re-trained for downstream use, such as re-
intotal. Jobsvariedinlengthrangingfromamin- training on a new domain or fine-tuning on a new
imum of 3 minutes, indicating a crash, to a maxi- task,authorsshouldreporttrainingtimeandcom-
mum of 9 days, with an average job length of 52 putational resources required, as well as model
hours. All training was done on a combination of sensitivity to hyperparameters. This will enable
NVIDIATitanX(72%)andM40(28%)GPUs.8 direct comparison across models, allowing subse-
The sum GPU time required for the project quentconsumersofthesemodelstoaccuratelyas-
totaled 9998 days (27 years). This averages to sesswhethertherequiredcomputationalresources
8WeapproximatecloudcomputecostusingP100pricing. 9BasedonaverageU.Scostofelectricityof$0.12/kWh.
are compatible with their setting. More explicit half the estimated cost to use on-demand cloud
characterization of tuning time could also reveal GPUs. Unlike money spent on cloud compute,
inconsistenciesintimespenttuningbaselinemod- however, that invested in centralized resources
els compared to proposed contributions. Realiz- would continue to pay off as resources are shared
ing this will require: (1) a standard, hardware- across many projects. A government-funded aca-
independent measurement of training time, such demiccomputecloudwouldprovideequitableac-
as gigaflops required to convergence, and (2) a cesstoallresearchers.
standardmeasurementofmodelsensitivitytodata
Researchersshouldprioritizecomputationally
and hyperparameters, such as variance with re-
efficienthardwareandalgorithms.
specttohyperparameterssearched.
Werecommendaconcertedeffortbyindustryand
Academicresearchersneedequitableaccessto academia to promote research of more computa-
computationresources. tionally efficient algorithms, as well as hardware
that requires less energy. An effort can also be
Recent advances in available compute come at a
made in terms of software. There is already a
high price not attainable to all who desire access.
precedent for NLP software packages prioritizing
Mostofthemodelsstudiedinthispaperwerede-
efficient models. An additional avenue through
velopedoutsideacademia;recentimprovementsin
which NLP and machine learning software de-
state-of-the-artaccuracyarepossiblethankstoin-
velopers could aid in reducing the energy asso-
dustryaccesstolarge-scalecompute.
ciated with model tuning is by providing easy-
Limiting this style of research to industry labs
to-use APIs implementing more efficient alterna-
hurtstheNLPresearchcommunityinmanyways.
tivestobrute-forcegridsearchforhyperparameter
First, it stifles creativity. Researchers with good
tuning, e.g. random or Bayesian hyperparameter
ideas but without access to large-scale compute
search techniques (Bergstra et al., 2011; Bergstra
will simply not be able to execute their ideas,
and Bengio, 2012; Snoek et al., 2012). While
instead constrained to focus on different prob-
softwarepackagesimplementingthesetechniques
lems. Second, it prohibits certain types of re- do exist,10 they are rarely employed in practice
searchonthebasisofaccesstofinancialresources.
for tuning NLP models. This is likely because
Thisevenmoredeeplypromotesthealreadyprob-
their interoperability with popular deep learning
lematic “rich get richer” cycle of research fund-
frameworks such as PyTorch and TensorFlow is
ing, where groups that are already successful and
not optimized, i.e. there are not simple exam-
thus well-funded tend to receive more funding
ples of how to tune TensorFlow Estimators using
due to their existing accomplishments. Third, the
Bayesian search. Integrating these tools into the
prohibitive start-up cost of building in-house re-
workflowswithwhichNLPresearchersandpracti-
sources forces resource-poor groups to rely on
tionersarealreadyfamiliarcouldhavenotableim-
cloud compute services such as AWS, Google
pactonthecostofdevelopingandtuninginNLP.
CloudandMicrosoftAzure.
While these services provide valuable, flexi- Acknowledgements
ble, and often relatively environmentally friendly
We are grateful to Sherief Farouk and the anony-
compute resources, it is more cost effective for
mous reviewers for helpful feedback on earlier
academic researchers, who often work for non-
drafts. This work was supported in part by the
profit educational institutions and whose research
Centers for Data Science and Intelligent Infor-
isfundedbygovernmententities,topoolresources
mation Retrieval, the Chan Zuckerberg Initiative
to build shared compute centers at the level of
under the Scientific Knowledge Base Construc-
funding agencies, such as the U.S. National Sci-
tionproject,theIBMCognitiveHorizonsNetwork
ence Foundation. For example, an off-the-shelf
agreement no. W1668553, and National Science
GPU server containing 8 NVIDIA 1080 Ti GPUs
Foundationgrantno. IIS-1514053. Anyopinions,
and supporting hardware can be purchased for
findings and conclusions or recommendations ex-
approximately $20,000 USD. At that cost, the
pressedinthismaterialarethoseoftheauthorsand
hardware required to develop the model in our
donotnecessarilyreflectthoseofthesponsor.
casestudy(approximately58GPUsfor172days)
would cost $145,000 USD plus electricity, about 10Forexample,theHyperoptPythonlibrary.
References MatthewE.Peters,MarkNeumann,MohitIyyer,Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
RhondaAscierto.2018. UptimeInstituteGlobalData
Zettlemoyer. 2018. Deep contextualized word rep-
CenterSurvey. Technicalreport,UptimeInstitute.
resentations. InNAACL.
DzmitryBahdanau,KyunghyunCho,andYoshuaBen-
AlecRadford, JeffreyWu, RewonChild, DavidLuan,
gio. 2015. Neural Machine Translation by Jointly
DarioAmodei,andIlyaSutskever.2019. Language
Learning to Align and Translate. In 3rd Inter-
modelsareunsupervisedmultitasklearners.
national Conference for Learning Representations
(ICLR),SanDiego,California,USA.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams.
2012. Practical bayesian optimization of machine
James Bergstra and Yoshua Bengio. 2012. Random
learningalgorithms. InAdvancesinneuralinforma-
searchforhyper-parameteroptimization. Journalof
tionprocessingsystems,pages2951–2959.
MachineLearningResearch,13(Feb):281–305.
JamesSBergstra,Re´miBardenet,YoshuaBengio,and David R. So, Chen Liang, and Quoc V. Le. 2019.
Bala´zsKe´gl.2011. Algorithmsforhyper-parameter The evolved transformer. In Proceedings of the
optimization. In Advances in neural information 36thInternationalConferenceonMachineLearning
processingsystems,pages2546–2554. (ICML).
BrunoBurger.2019. NetPublicElectricityGeneration Emma Strubell, Patrick Verga, Daniel Andor,
in Germany in 2018. Technical report, Fraunhofer David Weiss, and Andrew McCallum. 2018.
InstituteforSolarEnergySystemsISE. Linguistically-Informed Self-Attention for Se-
mantic Role Labeling. In Conference on Empir-
Alfredo Canziani, Adam Paszke, and Eugenio Culur- ical Methods in Natural Language Processing
ciello. 2016. An analysis of deep neural network (EMNLP),Brussels,Belgium.
modelsforpracticalapplications.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Gary Cook, Jude Lee, Tamina Tsai, Ada Kongn, John
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Deans, Brian Johnson, Elizabeth Jardim, and Brian
Kaiser, and Illia Polosukhin. 2017. Attention is all
Johnson. 2017. Clicking Clean: Who is winning
youneed. In31stConferenceonNeuralInformation
theracetobuildagreeninternet? Technicalreport,
ProcessingSystems(NIPS).
Greenpeace.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
DeepBidirectionalTransformersforLanguageUn-
derstanding. InNAACL.
Timothy Dozat and Christopher D. Manning. 2017.
Deepbiaffineattentionforneuraldependencypars-
ing. InICLR.
EPA. 2018. Emissions & Generation Resource Inte-
grated Database (eGRID). Technical report, U.S.
EnvironmentalProtectionAgency.
Christopher Forster, Thor Johnsen, Swetha Man-
dava,SharathTuruvekereSreenivas,DeyuFu,Julie
Bernauer, Allison Gray, Sharan Chetlur, and Raul
Puri. 2019. BERT Meets GPUs. Technical report,
NVIDIAAI.
DaLi,XinboChen,MichelaBecchi,andZiliangZong.
2016. Evaluatingtheenergyefficiencyofdeepcon-
volutionalneuralnetworksoncpusandgpus. 2016
IEEE International Conferences on Big Data and
Cloud Computing (BDCloud), Social Computing
andNetworking(SocialCom), SustainableComput-
ing and Communications (SustainCom) (BDCloud-
SocialCom-SustainCom),pages477–484.
Thang Luong, Hieu Pham, and Christopher D. Man-
ning.2015. Effectiveapproachestoattention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421. Associa-
tionforComputationalLinguistics.

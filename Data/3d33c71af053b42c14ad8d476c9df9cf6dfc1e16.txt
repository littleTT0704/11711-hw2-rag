Does Pretraining for Summarization Require Knowledge Transfer?
KundanKrishna,JeffreyBigham,ZacharyC.Lipton
CarnegieMellonUniversity
5000ForbesAvenue
Pittsburgh,PA
{kundank,jbigham,zlipton}@andrew.cmu.edu
Abstract hasnosyntacticstructure(Sinhaetal.,2021)and
othershaveshownbenefitsevenwhentheupstream
Pretraining techniques leveraging enormous
corpusisfromadifferentdomainentirely,suchas
datasets have driven recent advances in text
music(PapadimitriouandJurafsky,2020)oramino
summarization. While folk explanations sug-
acidsequences(ChiangandLee,2020)
gest that knowledge transfer accounts for pre-
training’s benefits, little is known about why Inthiswork,weshowthat,surprisingly,pretrain-
it works or what makes a pretraining task or ingobjectivespreviouslydemonstratedtobehelp-
dataset suitable. In this paper, we challenge fulforsummarization(Zouetal.,2020),continue
theknowledgetransferstory,showingthatpre-
to deliver significant benefits even when applied
training on documents consisting of character
ontextconsistingofrandomlysamplednonsense
n-grams selected at random, we can nearly
words. Becausethetextconsistsofnonsensewords
match the performance of models pretrained
onrealcorpora. Thisworkholdsthepromise sampled independently and uniformly, it seems
of eliminating upstream corpora, which may difficult to fathom a credible argument that the
alleviate some concerns over offensive lan- syntheticcorpusencodeslinguisticknowledgein
guage, bias, and copyright issues. To see anyrelevantsense. Nevertheless,whenpretraining
whetherthesmallresidualbenefitofusingreal
transformer-based sequence-to-sequence models
data could be accounted for by the structure
using this nonsense text, we achieve significant
ofthepretrainingtask,wedesignseveraltasks
performanceboostsonmultipledownstreamsum-
motivatedbyaqualitativestudyofsummariza-
marizationbenchmarksthatnearlymatchtheper-
tion corpora. However, these tasks confer no
appreciable benefit, leaving open the possibil- formanceofpretrainedtransformers.
ityofasmallroleforknowledgetransfer.1
Remarkably, when pretraining with synthetic
tasks, using real data offers no benefit over the
1 Introduction
nonsensedata,onmultiplesummarizationbench-
Despitethewidespreadsuccessofpretrainedmod- marks. Thus,weinvestigatewhetherapretraining
els when fine-tuned on diverse downstream NLP task better aligned with the demands of summa-
tasks,suchassummarization(Qietal.,2020;Raf- rizationmightclosethisresidualgap. Wedesigna
fel et al., 2020), question answering, sentiment collectionofpretrainingtasksinspiredbysomeof
analysisetc(Yangetal.,2019),scientificexplana- thebasicprimitiveoperationsthatappeartobecom-
tionsforthesebenefitsremainunknown. Several monroutinesrequiredinordertocreatereal-world
works have claimed that pretrained models learn summaries. Wecarriedoutanextensivesurveyof
linguisticknowledgefromthepretrainingcorpus publicsummarizationdatasetsspanningdifferent
(Lina et al., 2019; Tenney et al., 2019; Manning domains,andcataloguedseveralelementaryoper-
et al., 2020), leading to a popular, but unproven ations that were frequently invoked in producing
hypothesisthatcreditsknowledgetransferforthe summaries (e.g., extract sentences on a specific
improvements seen on downstream tasks. How- topic,ordeterminethemostfrequentamongaset
ever,severalrecentfindingstesttheplausibilityof of relevant terms). In our proposed pretraining
thisaccount. Forexample,benefitsofpretraining corpus, the summary is created by carrying out
have been observed even when the upstream text these elementary operations on the input. How-
ever,wefindthatourpretrainingtasksdelivercom-
1The code and the datasets used in the paper
parable performance gains to those proposed in
are available at https://github.com/acmi-lab/
pretraining-with-nonsense Zouetal.(2020)leavingthesmallgapopen. On
CNN-DailymailandRotowirebenchmarks,where efitsonGLUEbenchmark,Similarly,forthetaskof
median summary lengths are 73 and 456 tokens languagemodeling,pretrainingonmusicalscores,
respectively,usingourpretrainingtaskswithnon- orevenartificialsequencesofnestedparentheses
sensetextresultsinachievingonaverage95%of hasshowntoachievebetterperplexityonahuman
the performance gain in ROUGE-1 that standard language(PapadimitriouandJurafsky,2020). Our
T5 pretrained models enjoy relative to randomly resultsgofurther—herethesourcedocumentscon-
initializedT5. Bycontrast,onXSumandRotten- tainnonaturaldataatall,nordotheyexhibitany
tomatoes,wheresummariesareshorter(29and32 non-trivialstructure.
tokensrespectively),werealizearelativelymodest Recently, somemachinelearningtheorylitera-
37%ofthebenefitonaverage. turehasbeguntoquestionthemechanismbywhich
The takeaways from our results are two-fold: transferlearningworks. Forexample,Neyshabur
First,theseresultschallengeourunderstandingof et al. (2020) attribute the benefits to low-level
whypretraininghelpsinsummarization,suggest- statistics of the data and optimization considera-
ing that a large portion of the benefits seen may tions rather than feature reuse. In other related
not be due to any knowledge transfer, but simply work, Maennel et al. (2020) show that networks
betterinitializationfromanoptimizationperspec- pretrained on randomly labeled data sometimes
tive. Second, the ability to realize the benefits of enjoyconsiderableperformanceimprovementson
pretrainingwithoutusingreal-worlddatacouldal- downstreamtasks.
leviateconcernsregardingbias,offensivespeech,
3 GeneratingtheNonsenseCorpus
andintellectualpropertyassociatedwithusingweb-
scalepretrainingcorporaofunknownprovenance
Forgeneratingthenonsensepretrainingcorpus,we
(Davidsonetal.,2019;BordiaandBowman,2019).
use an artificial vocabulary to create base docu-
ments that has little resemblance to any real lan-
2 RelatedWork
guage. Ourvocabularysimplyconsistsofthefirst
50003-lettercharactercombinationsusingtheEn-
Recently,multiplepretrainedmodelshaveshown
glish alphabet in lexical order starting from the
remarkable performance on text summarization.
right (aaa, baa, caa, ..., aab, bab, ...). Each sen-
These models have been pretrained on real data
tence is generated by sampling each word in it
with diverse denoising tasks, including masked
independentlyfromtheuniformdistributionover
language modeling (Raffel et al., 2020), text in-
theentirevocabulary,andendingitwithaperiod
filling(Zhangetal.,2020), andsentencereorder-
(see Figure 1 for a sample nonsense document).
ing (Lewis et al., 2020), among others. While
Thelengthofeachsentenceisselecteduniformly
these pretraining objectives have shown benefits
from5to15words. Thenumberofsentencesper
across multiple NLP tasks, Zou et al. (2020) pro-
documentisselectedaccordingtothepretraining
posed a set of three denoising pretraining tasks
task that it is used for. For the pretraining tasks
that are specifically motivated by summarization
proposedinZouetal.(2020),wesamplesentences
and deliver performance comparable to previous
until the document reaches 512 tokens in length.
pretrained models. Our paper shows that the pre-
Forourpretrainingtasks(introducedlater),number
trainingtasksinZouetal.(2020)improvesumma-
ofsentencesinadocumentisdecidedbysampling
rizationperformanceevenifthepretrainingcorpus
uniformlyfrom7to13sentences.
isartificialanddoesnotencodeanylinguisticstruc-
ture.
4 STEPPretrainingTasks
Ourworkextendsagrowingbodyofscientificlit-
eraturethatquestionscommonly-heldbeliefsabout STEPpretrainingtasksareacollectionof3tasks
whatpropertiesofapretrainingcorpusleadtoim- definedby Zouetal.(2020). NextSentenceGen-
provements on different downstream tasks. Re- eration(NSG)providesthefirsthalfofadocument
cently, Sinha et al. (2021) showed that word or- asinputandthetargetistogeneratethelatterhalf.
derinpretrainingdocumentshasnegligibleimpact Sentence Reordering (SR) presents a document
ondownstreamperformanceontheGLUEbench- with its sentences shuffled in random order, and
mark. Evenpretrainingonsequencesfromdiffer- requiresgeneratingtheoriginaldocumentwithcor-
ent modalities such as Java code and amino acid rectsentenceorder. MaskedDocumentGeneration
sequences(ChiangandLee,2020)haveshownben- (MDG) masks out a contiguous sequence of to-
kensinthebasedocumentandrequiresgenerating
Nonsense dkb spf hpd vfb nwg tsa phc whh irc ewb .
theoriginaldocumentwhilecorrectlyfilling-inthe Document uwa lja oyg mjg ige qpb ncc ele .
maskedtokens. Moredetailsandhyperparameters lqc rbb oeh pof vwg zob jdf quc .
aqe qff sre rxd zmf .
canbefoundintheoriginalpaper. mjh vgc bge epf slb ecd .
5 OurPretrainingTasks Pretraining Task Selection
Todevelopourpretrainingtasks,wefirstundertook Task10 - Copy sentence containing a keyword.
Task3 - Whether a keyword has positive or negative sentiment
a qualitative analysis of existing summarization
datasets. We surveyed all summarization papers Creation of summary by
applying task logic
published in the last 10 years with more than 25
citations, cataloguing a list of the summarization Input dkb spf hpd vfb nwg tsa phc whh irc ewb .
uwa lja oyg mjg ige qpb ncc ele .
datasetsthatwereusedinthem. Weobservedthat
lqc rbb oeh __d10__keyword_1__ vwg zob jdf quc .
datasetscanbegroupedtogetheraccordingtodo- aqe qff sre rxd zmf .
__d3__keyword_7__ vgc bge epf slb ecd .
main(e.g.,newsandconversations). Wegrouped
the28retrieveddatasetsinto14domains(seethe Summary lqc rbb oeh __d10__keyword_1__ vwg zob jdf quc .
the keyword was negative .
Appendix, Table 9). We selected a single dataset
fromeachdomaintoanalyzewhatsummariescon- Dataset creation
Randomly
sistofandwhatskillstheircreationrequires.
Initialized Pretrained
From each selected dataset, we manually in- Transformer Model
seq2seq Model Pretraining
spected ten randomly sampled input-summary
pairs,lookingforprimitivesubtasksthatseemto
Figure1: Proceduretocreatepretrainingdatasetusing
expressskills(informally)thatarerequiredinorder
thenonsensecorpusandourproposedpretrainingtasks
tocreatethesummariesdemandedbythisdataset
foratleasttwooftheteninstances. Sinceweneed
Table1,wefirstcreateabasedocumentandthen
to create artificial input-summary pairs for each
(when required by the task) modify it by adding
subtask,weonlychosesubtasksforwhichitwas
the requisite keywords. For example, CopyKw-
possible to create large number of such artificial
dOneSentenceusesakeywordtomarkthesentence
pairs. Forexample,intheSamsumdataset(Gliwa
to copy. The keywords added for tasks are also
etal.,2019)whichrequiressummarizingconversa-
meaningless like keyword1, keyword2. Then the
tionsbetweenpeople,afrequentlynecessarysub-
correspondingelementaryoperationisappliedto
taskistoinfertheunfoldingsocialscenario(e.g. a
generatethesummaryfromthismodifiedinput.
fight,orapersonhelpinganother)butitisdifficult
Thepretrainingdatasetthatwecreateinvolves
to create a large number of varied artificial con-
multiple elementary operations in each input-
versations that reflect the situation. On the other
summary pair. To create the input-summary pair
hand,subtaskssuchasextractingthosesentences
fromanonsensedocument,wefirstsample3ele-
thataddresssomespecifictopic,or(evensimpler)
mentarytasksandsequentiallymodifytheinputas
extractingthefirstsentenceoftheinputaresimple
neededbyeachtask. Then,wegeneratethesum-
enoughtofacilitatecreatingdatapointsprogramat-
marysentence(s)asrequiredforeachelementary
ically. Notethatwhilecopyingthefirstsentences
taskandconcatenatethemtoconstitutetheoverall
mightseemlikeatrivialoruninterestingpretrain-
summary. Here, the different keywords added to
ing task, it can be very useful. For example, in
the input signal to the model which tasks are re-
news summarization datasets the lead-3 baseline
quiredtogeneratethesummary. Theprocedureis
(copying over first 3 sentences as the summary)
illustratedinFigure1.
worksverywell(Brandowetal.,1995;Grenander
etal.,2019).
6 SummarizationBenchmarks
Based on this analysis, we developed 21 ele-
mentarytasks,includingcopyingspecificcontent, Wefine-tuneandevaluateourmodelson4down-
performing numerical operations, and more. See streamsummarizationbenchmarks.
Table1forfulldetailsontheslateoftasks.
CNN-Dailymail-10K(Seeetal.,2017) Contains
Generatingartificialsummaries Tocreatean newsarticlesandsummariesfromCNNandDai-
input-summarypairusinganelementarytaskfrom lymail websites. We use only 10k instances for
Elementarysubtask Description
CheckKeyword Checkiftheinputhasaspecialkeywordornot.
ClassifyKeyword Outputthecategoryofkeywordoccurringintheinput
MajorityKeyword Outoftwogivenkeywords,findwhichoneoccursmorenumberoftimes
CopyFirstSentence Copyfirstsentence
CopyBulleted Copyoverabulletpoint(sentencestartingwithabulletmarker).
CopyQuoted Copytextwithinquotes
CopyLastSentence Copylastsentence
CopyKwdOneSentence Copythesentencethatcontainsakeyword
CopyKwdMultipleSentInOrder Copyallsentencescontaininganykeywordintheirorderofappearance.
CopyKwdMultipleSentSorted Copyallsentencescontaininganykeyword,sortedbythekeywords
CopyKwdMultipleSentShuffled Copyallsentencescontainingkeywordsinanyorder.
ReplaceClassKeyword Replaceanobject’smentionwithitscategory(e.g.apple→− fruit)
CompareNumbers Giventwonumbersinthetext,saywhichoneisbigger
SumOfNumbers Sumallnumbersintheinput
ThresholdNumber Checkifanumberintheinputisaboveathreshold
LargestNumber Findoutlargestofoneormorenumbersintheinput.
TruncateSentence Copyasentencebutonlytillthecutoffkeywordisencountered
BreakClauses Breakasinglesentenceintomultipleonescontainingoneclauseeach
JoinClauses Joinclausesfrommultiplesentencestomakeonelongersentence
ParaphraseWords Copyasentencewhilereplacingitskeywordswithoneofitssynonyms
TopicSegregation Copysentencescontainingkeywordsfromdifferentclassesintoseparatesections
Table1: 21extractedelementarysummarizationsubtasksandtheirdescriptions(detailedversionisinAppendix)
training(randomlysampledfromthetrainingset) log-likelihoodintheAppendix(Table6). Toframe
so that the impact of pretraining is more visible. thecomparison,weincludetheperformanceofthe
However,westillevaluatethefine-tunedmodelon official T5 model and of a randomly initialized
thefulltestset. modelusingthesamearchitecture(T5-RI).
XSum-10K (Narayan et al., 2018) Also a news Pretrainingwitheitherourproposedpretraining
summarization dataset. Again, we train on a ran- tasks (OurTasks), or STEP tasks (STEPTasks)
domsubsetof10k instancesfromthetrainingset. performs much better than random initialization,
even when using nonsense data. For all summa-
Rottentomatoes (Wang and Ling, 2016)
rization benchmarks except RottenTomatoes, the
This dataset concerns summarizing critical
performanceremainedcomparablewhenweused
reviews of movies found on the website
real upstream data from Wikipedia to create the
rottentomatoes.com.
pretraining datasets. This suggests that for some
Rotowire(Wisemanetal.,2017) Here,thetask summarizationbenchmarks,theremightbelittleor
is to process the box-score of a basketball game noadditionalbenefitprovidedbyusingrealworld
(often requiring numerical reasoning) to create a pretrainingtext.
post-gamesummary.
LookingatindividualSTEPTasks,NSGhasno
trainingsignalsincetheoutputiscompletelyinde-
7 ExperimentsandResults
pendentoftheinput,butsurprisinglyitleadstoim-
First,wepretrainthetransformer-basedsequence- provementsinRotowirebenchmark. SRandMDG
to-sequence architecture used by the T5 performed much better than NSG on CNN-DM
model (Raffel et al., 2020), on different cor- and XSum, likely because they involve copying
pora, each containing 100k input-summary pairs sentences/unmaskedtokensfromtheinput. Wecre-
to get different pretrained models. We use the atedadjustedversionsofthesepretrainingdatasets,
T5-smallarchitectureinallexperiments. Next,we where there was no copying needed and it led to
fine-tuneeachmodelonthedownstreamtasksand a drop in performance on both pretraining tasks,
measureperformanceviaROUGEscore(Table2). bringingitclosetoT5-RIforCNN-DMandXSum.
Wealsopresentthemodels’performanceonnext InSR-adjusted,thetaskistooutputonlythenumer-
tokenpredictioninsummariesusingaccuracyand icalorderinwhichsentencesshouldbecopied(ver-
Model CNN-DM-10K XSum-10K RottenTomatoes Rotowire
R1 R2 RL R1 R2 RL R1 R2 RL R1 R2 RL
T5-OffShelf 39.38 18.08 27.71 29.18 8.69 22.62 24.73 9.00 19.64 37.50 12.85 19.85
T5-RI 9.86 1.06 7.49 15.49 2.48 12.76 10.17 0.41 8.66 4.02 0.72 3.68
NonsenseUpstreamCorpus
T5-OurTasks 35.23 14.77 24.03 20.36 4.15 16.23 15.72 2.06 12.51 39.10 11.81 19.94
T5-STEPTasks 35.78 14.98 23.60 21.49 4.56 16.78 13.22 0.88 10.83 29.82 7.45 16.74
T5-STEPTask-NSG 9.20 0.80 7.19 15.78 2.24 12.44 12.31 0.71 10.60 33.65 7.60 17.90
T5-STEPTask-SR 28.63 10.67 20.35 21.47 4.70 16.62 10.89 0.51 9.18 25.68 5.39 15.29
T5-STEPTask-SR-adjusted 7.24 0.63 5.69 15.04 2.00 12.12 11.18 0.46 9.51 20.00 2.74 12.08
T5-STEPTask-MDG 34.50 14.45 23.77 20.76 4.13 16.45 11.78 0.70 9.89 36.22 10.53 18.73
T5-STEPTask-MDG-adjusted 10.15 0.93 7.78 16.12 2.20 13.09 15.07 1.38 11.69 20.39 3.77 11.97
RealUpstreamCorpus
T5-OurTasks 34.06 13.88 23.21 22.27 5.09 17.60 19.16 5.26 15.65 38.57 11.89 19.68
T5-STEPTasks 32.04 12.93 22.55 23.37 5.68 18.42 20.89 6.29 17.05 37.63 10.89 19.57
PGModelsRandomlyInitializedvsPretrained(NonsenseUpstreamCorpus)
PG-RI 29.68 11.75 21.82 17.66 3.57 14.62 19.63 6.43 16.62 30.61 8.66 17.74
PG-OurTasks 29.82 11.78 21.91 16.81 3.43 13.95 19.02 6.57 16.38 26.94 6.81 16.77
PG-STEPTasks 29.44 11.74 21.67 17.65 3.54 14.55 17.70 5.89 15.34 31.16 8.49 17.85
Table2: Rougescoresachievedbydifferentmodelsonfoursummarizationbenchmarks.
susactuallygeneratingthefulloutput). InMDG- Pretrainingtask R1 R2 Pr%
adjusted,thetaskistoonlyoutputthemasked-out
TopicSegregation 23.04 7.79 99.90
tokens(versusoutputtingtheentiredocument,in- CopyKwdMultipleSent-Shuffled 23.34 5.46 99.66
cludingunmaskedtokens). TruncateSentence 17.07 2.50 1.00
LargestNumber 6.52 0.58 99.88
SumOfNumbers 5.03 0.40 25.06
A randomly initialized pointer-generator
CompareNumbers 1.89 0.04 48.88
model(Seeetal.,2017)(PG-RI)performsfarbet-
terthanarandomlyinitializedT5model. However,
Table 3: The 3 best and worst performing pretraining
T5-architecture models pretrained on nonsense tasksaccordingtoperformanceoftheirpretrainedmod-
textwereabletooutperformpointer-generatoron els on CNN-Dailymail-10K (R1,R2), and their accu-
3outof4benchmarks,suggestingthattransformer racyonthepretrainingtask(Pr%).
modelspretrainedonnonsensetextcanbeabetter
8 Conclusion
choice than using non-pretrained LSTM based
models. Interestingly, pretraining the PG model
Thispaperdemonstratedthattransformermodels
oneitherOurTasksorSTEPTasksdidnotleadto
pretrainedonrandomlygeneratednonsensedatade-
anyadditionalimprovement.
liverremarkableperformancegainsacrossmultiple
summarizationtasks,comparedtotheirrandomly
Modelspretrainedseparatelyoneachtaskfrom initializedversion. Thissuggeststhatasubstantial
OurTasks exhibit strong differences in their per- partoftheobservedbenefitsofpretrainingcannot
formanceonCNN-Dailymail-10Kbenchmark(Ta- beattributedtoknowledgetransfer. Toinvestigate
ble 3). Models pretrained on TopicSegregation whetherthedesignofpretrainingtaskitselfplaysa
and CopyKwdMultipleSent-Shuffled outperform significantroleandcanleadtofurtherperformance
others significantly. The two worst performing gains,weexploredsummarizationdatasetstopre-
modelswerepretrainedonCompareNumbersand pare a battery of tasks found useful in creating
SumOfNumbers,andthesemodelswereunableto summaries. Butthesepretrainingtasksperformed
perform any better than random guessing on the comparablytomoregenericpretrainingtasksused
pretrainingtaskitself. Bycontrast,mostotherpre- in literature. Our work suggests that understand-
trainedmodelswereabletosolvetheirpretraining ingpretrainingmayhavemoretodowithpoorly-
taskcorrectlymorethan99%oftimes(seeTable7 understoodaspectsofhowinitializationinfluences
inAppendixforfulldetails). optimizationthanwithknowledgetransfer.
References Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan,
Mirella Lapata, and Hannaneh Hajishirzi. 2019.
Miltiadis Allamanis, Hao Peng, and Charles Sutton.
Text generation from knowledge graphs with graph
2016. A convolutional attention network for ex-
transformers. In Proceedings of the 2019 Confer-
treme summarization of source code. In Interna-
ence of the North American Chapter of the Associ-
tionalconferenceonmachinelearning,pages2091–
ation for Computational Linguistics: Human Lan-
2100.PMLR.
guage Technologies, Volume 1 (Long and Short Pa-
pers),pages2284–2293.
Shikha Bordia and Samuel Bowman. 2019. Identify-
ingandreducinggenderbiasinword-levellanguage
RémiLebret,DavidGrangier,andMichaelAuli.2016.
models. In Proceedings of the 2019 Conference of
Neuraltextgenerationfromstructureddatawithap-
the North American Chapter of the Association for
plication to the biography domain. In Proceedings
ComputationalLinguistics: StudentResearchWork-
of the 2016 Conference on Empirical Methods in
shop,pages7–15.
NaturalLanguageProcessing,pages1203–1213.
Ronald Brandow, Karl Mitze, and Lisa F Rau. 1995.
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
Automatic condensation of electronic publications
jan Ghazvininejad, Abdelrahman Mohamed, Omer
by sentence selection. Information Processing &
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
Management,31(5):675–685.
2020. Bart: Denoising sequence-to-sequence pre-
trainingfornaturallanguagegeneration,translation,
Cheng-Han Chiang and Hung-yi Lee. 2020. Pre-
andcomprehension. InProceedingsofthe58thAn-
trainingalanguagemodelwithouthumanlanguage.
nual Meeting of the Association for Computational
arXivpreprintarXiv:2012.11995.
Linguistics,pages7871–7880.
Eric Chu and Peter Liu. 2019. Meansum: a neural
Chin-YewLinandEduardHovy.2002. Manualandau-
modelforunsupervisedmulti-documentabstractive
tomaticevaluationofsummaries. InProceedingsof
summarization. InInternationalConferenceonMa-
theACL-02WorkshoponAutomaticSummarization-
chineLearning,pages1223–1232.PMLR.
Volume4,pages45–51.
Arman Cohan, Franck Dernoncourt, Doo Soon Kim,
YongjieLina,YiChernTana,andRobertFrankb.2019.
TrungBui,SeokhwanKim,WalterChang,andNazli
Opensesame: Gettinginsidebert’slinguisticknowl-
Goharian.2018. Adiscourse-awareattentionmodel
edge. ACL2019,page241.
forabstractivesummarizationoflongdocuments. In
Proceedings of the 2018 Conference of the North Peter J Liu, Mohammad Saleh, Etienne Pot, Ben
American Chapter of the Association for Computa- Goodrich, RyanSepassi, LukaszKaiser, andNoam
tional Linguistics: Human Language Technologies, Shazeer.2018. Generatingwikipediabysummariz-
Volume2(ShortPapers),pages615–621. inglongsequences. InInternationalConferenceon
LearningRepresentations.
Thomas Davidson, Debasmita Bhattacharya, and Ing-
mar Weber. 2019. Racial bias in hate speech and Yang Liu and Mirella Lapata. 2019. Text summariza-
abusivelanguagedetectiondatasets. InProceedings tion with pretrained encoders. In Proceedings of
oftheThirdWorkshoponAbusiveLanguageOnline, EMNLP-IJCNLP2019,pages3721–3731.
pages25–35.
Hartmut Maennel, Ibrahim Alabdulmohsin, Ilya Tol-
ShimaGerani,GiuseppeCarenini,andRaymondTNg. stikhin, Robert JN Baldock, Olivier Bousquet, Syl-
2019. Modelingcontentandstructureforabstractive vainGelly,andDanielKeysers.2020. Whatdoneu-
review summarization. Computer Speech & Lan- ralnetworkslearnwhentrainedwithrandomlabels?
guage,53:302–331.
ChristopherDManning,KevinClark,JohnHewitt,Ur-
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and vashi Khandelwal, and Omer Levy. 2020. Emer-
Aleksander Wawer. 2019. Samsum corpus: A gentlinguisticstructureinartificialneuralnetworks
human-annotated dialogue dataset for abstractive trained by self-supervision. Proceedings of the Na-
summarization. EMNLP-IJCNLP2019,page70. tionalAcademyofSciences,117(48):30046–30054.
Matt Grenander, Yue Dong, Jackie Chi Kit Cheung, AmitMoryossef,YoavGoldberg,andIdoDagan.2019.
and Annie Louis. 2019. Countering the effects of Step-by-step: Separating planning from realization
lead bias in news summarization via multi-stage inneuraldata-to-textgeneration. InProceedingsof
training and auxiliary losses. In EMNLP-IJCNLP, the 2019 Conference of the North American Chap-
pages6021–6026. teroftheAssociationforComputationalLinguistics:
Human Language Technologies, Volume 1 (Long
Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and andShortPapers),pages2267–2277.
LukeZettlemoyer.2016. Summarizingsourcecode
using a neural attention model. In Proceedings Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
of the 54th Annual Meeting of the Association for 2018. Don’tgivemethedetails, justthesummary!
ComputationalLinguistics(Volume1:LongPapers), topic-aware convolutional neural networks for ex-
pages2073–2083. tremesummarization. ArXiv,abs/1808.08745.
BehnamNeyshabur,HanieSedghi,andChiyuanZhang. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.
2020. What is being transferred in transfer learn- Bert rediscovers the classical nlp pipeline. In ACL,
ing? InAdvanesinNeuralInformationProcessing pages4593–4601.
Systems(NeurIPS).
Pavlos Vougiouklis, Hady Elsahar, Lucie-Aimée
JekaterinaNovikova,OndˇrejDušek,andVerenaRieser. Kaffee, Christophe Gravier, Frédérique Laforest,
2017. The e2e dataset: New challenges for end-to- Jonathon Hare, and Elena Simperl. 2018. Neu-
end generation. In Proceedings of the 18th Annual ralwikipedian: Generatingtextualsummariesfrom
SIGdialMeetingonDiscourseandDialogue,pages knowledge base triples. Journal of Web Semantics,
201–206. 52:1–15.
Isabel Papadimitriou and Dan Jurafsky. 2020. Learn- Lu Wang and Claire Cardie. 2013. Domain-
ing music helps you read: Using transfer to study independent abstract generation for focused meet-
linguisticstructureinlanguagemodels. InProceed- ing summarization. In Proceedings of the 51st An-
ings of the 2020 Conference on Empirical Methods nual Meeting of the Association for Computational
in Natural Language Processing (EMNLP), pages Linguistics (Volume 1: Long Papers), pages 1395–
6829–6839. 1405.
Romain Paulus, Caiming Xiong, and Richard Socher. LuWangandWangLing.2016. Neuralnetwork-based
2018. Adeepreinforcedmodelforabstractivesum- abstract generation for opinions and arguments. In
marization. In International Conference on Learn- Proceedings of the 2016 Conference of the North
ingRepresentations. American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
RatishPuduppully,LiDong,andMirellaLapata.2019.
pages47–57.
Data-to-text generation with content selection and
planning. InProceedingsoftheAAAIconferenceon Sam Wiseman, Stuart M Shieber, and Alexander M
artificialintelligence,volume33,pages6908–6915. Rush. 2017. Challenges in data-to-document gen-
eration. In Proceedings of the 2017 Conference on
Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu,
EmpiricalMethodsinNaturalLanguageProcessing,
NanDuan,JiushengChen,RuofeiZhang,andMing
pages2253–2263.
Zhou. 2020. Prophetnet: Predicting future n-gram
for sequence-to-sequence pre-training. In Proceed- Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
ings of the 2020 Conference on Empirical Methods bonell,RussRSalakhutdinov,andQuocVLe.2019.
in Natural Language Processing: Findings, pages Xlnet: Generalized autoregressive pretraining for
2401–2410. language understanding. Advances in Neural Infor-
mationProcessingSystems,32:5753–5763.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, MichihiroYasunaga,JungoKasai,RuiZhang,Alexan-
Wei Li, and Peter J Liu. 2020. Exploring the lim- der R Fabbri, Irene Li, Dan Friedman, and
its of transfer learning with a unified text-to-text DragomirRRadev.2019. Scisummnet: Alargean-
transformer. JournalofMachineLearningResearch, notatedcorpusandcontent-impactmodelsforscien-
21:1–67. tificpapersummarizationwithcitationnetworks. In
ProceedingsoftheAAAIConferenceonArtificialIn-
Abigail See, Peter J Liu, and Christopher D Manning.
telligence,volume33,pages7386–7393.
2017. Gettothepoint: Summarizationwithpointer-
generatornetworks. InProceedingsofthe55thAn- JingqingZhang,YaoZhao,MohammadSaleh,andPe-
nual Meeting of the Association for Computational ter Liu. 2020. Pegasus: Pre-training with extracted
Linguistics (Volume 1: Long Papers), pages 1073– gap-sentencesforabstractivesummarization. InIn-
1083. ternationalConferenceonMachineLearning,pages
11328–11339.PMLR.
EvaSharma,ChenLi,andLuWang.2019. Bigpatent:
A large-scale dataset for abstractive and coherent YanyanZou,XingxingZhang,WeiLu,FuruWei,and
summarization. In Proceedings of the 57th Annual Ming Zhou. 2020. Pre-training for abstractive doc-
Meeting of the Association for Computational Lin- umentsummarizationbyreinstatingsourcetext. In
guistics,pages2204–2213. Proceedings of the 2020 Conference on Empirical
MethodsinNaturalLanguageProcessing(EMNLP),
Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle
pages3646–3660.
Pineau, Adina Williams, and Douwe Kiela. 2021.
Masked language modeling and the distributional
hypothesis: Order word matters pre-training for lit-
tle. arXivpreprintarXiv:2104.06644.
Linfeng Song, Yue Zhang, Zhiguo Wang, and
Daniel Gildea. 2018. A graph-to-sequence
model for amr-to-text generation. arXiv preprint
arXiv:1805.02473.
A Appendix ing, validation and test splits with sizes shown
in Table 4. For the CNN-Dailymail and XSum
datasets,weusethestandardtestsplits,butreduce
Hyperparameters We use the T5-Small ar-
thetrainingandvalidationsetsizesto10kand1k
chitecure with 60.5 million parameters as our
respectively by uniformly subsampling from the
transformer-based model. The models are all
standardfulldatasetsplits.
trainedusingtheBertAdamoptimizerwithalearn-
ingrateof10−4. Forthepointer-generatormodel, Evaluationmetrics Wemeasurethequalityof
the token embedding size is 128, its encoder is a generated summaries using ROUGE scores (Lin
bidirectional LSTM with hidden size 256 the de- and Hovy, 2002) which measure n-gram overlap
coder is a unidirectional LSTM of the same size. between a generated and reference summary to
Theentiremodelhad4.4millionparameters. For assess its quality. We use the ROUGE-1,2 and L
afaircomparision,weusewordpiecetokenization variantsofthismetricwhichmeasureoverlapinun-
with all models with the same tokenizer and vo- igrams,bigramsandlongestcommonsubsequence
cabulary as used by the standard T5 model. The respectively. We also present the average perfor-
validationmetricusedinallexperimentswasaccu- mance of models at predicting the next token of
racyonthenext-tokenpredictiononthesummaries. asummarygivenallthegroundtruthpasttokens
A patience value of 5 epochs was used for early (Table 6). To measure this, we use the accuracy
stopping. andthenegative-log-likelihoodmetricswhichare
For CNN-Daiymail dataset, we truncated the standardformulti-classclassification. Weaverage
input and output lengths according to Zou et al. thesemetricsacrossdifferentdecodingtimesteps
(2020)(Table5). Weusethesamelengthsforthe ofsummarygeneration,andthenaverageitagain
XSumdatasetaswell. FortheRotowireandRot- acrossallthesummariesinthetestset.
tentomatoesdataset,theinputandoutputlengths
weremuchlongerandevenwithabatchsizeof1,
wehadtotruncatethemtovaluesthatallowedus
toaccommodatetrainingwiththeavailableGPU
memory(32GB).Whiledecoding,weusedbeam
searchwithbeamsize4,andsettheminimumand
maximum decoding lengths to the 5 and 95 per-
centileoftheirobserveddistribution.
Computinginfrastructure Mostexperiments
were carried out on 8 Nvidia V100 GPUs with
32GBofmemory. SomeexperimentswithCNN-
DailymailandXSumdatasetswerecarriedouton4
NvidiaRTX2080TiGPUswith11GBofmemory.
Exclusionsfromensembleofourtasks When
creating artificial summaries requires using mul-
tiple of our proposed elementary tasks, the dif-
ferent keywords added to the input signal to the
model which tasks are required for it. Three of
ourproposedtasksdonotalwaysinvolvekeyword
addition—CopyFirstSentence,CopyLastSentence,
CheckKeyword. Henceweexcludethemwhencre-
atingthepretrainingcorpuswithourensembleof
tasks. We also exclude the SumOfNumbers and
CompareNumberstasksbecausetheycouldnotbe
learnt even in isolation by a randomly initialized
T5modeltrainingon100kdatapoints.
Detailsofdatasetsplits FortheRotowireand
RottenTomatoesdatasets,weusethestandardtrain-
CNN-DM-10K XSum-10K RottenTomatoes Rotowire
Train 10000 10000 2458 3398
Validation 1000 1000 536 727
Test 11490 11333 737 728
Table4: SizesforTrain,validationandtestsplitsforalldatasets
CNN-DM-10K XSum-10K RottenTomatoes Rotowire
maxsourcelength 512 512 6000 5160
maxtargetlength 256 256 ∞ 815
batchsize 16 16 1 1
maxdecodelength 148 42 52 815
mindecodelength 44 18 16 223
Table5: Hyperparametersusedforfine-tuningmodelsonthe4datasets
Experiment CNN-DM-10K XSum-10K Rottentomatoes Rotowire
Acc NLL Acc NLL Acc NLL Acc NLL
T5-OffShelf 65.15 1.71 53.68 2.34 51.78 2.77 68.04 1.50
T5-RandomInit 29.78 4.92 32.60 4.75 24.75 5.36 48.30 2.61
NonsenseUpstreamCorpus
T5-OurTasks 54.74 3.18 38.98 4.27 33.42 5.08 63.59 1.78
T5-STEPTasks 54.71 3.18 39.47 4.21 28.65 5.13 58.89 1.99
RealUpstreamCorpus
T5-OurTasks 54.87 2.93 41.21 3.76 39.64 4.12 64.02 1.78
T5-STEPTasks 57.91 2.46 46.83 3.08 45.34 3.43 64.08 1.63
PGModelsRandomlyInitializedvsPretrained(NonsenseUpstreamCorpus)
PG-RandomInit 51.14 2.91 33.05 4.14 33.35 4.37 59.12 1.92
PG-OurTasks 51.70 2.89 33.80 4.14 34.40 4.29 59.30 1.92
PG-STEPTasks 51.79 2.88 34.13 4.14 35.06 4.21 59.00 1.94
Table6: Accuracy(Acc)andnegativeloglikelihood(NLL)fornexttokenpredictiononsummaries
Pretrainingtask R1 R2 RL Pr%
CopyKwdMultipleSent-Shuffled 23.34 5.46 15.41 99.66
TopicSegregation 23.04 7.79 16.52 99.88
TruncateSentence 17.07 2.50 11.81 100.00
CopyQuoted 11.03 1.32 8.32 99.82
BreakClauses 10.46 1.18 7.95 99.80
CopyKwdMultipleSent-InOrder 10.14 1.14 7.70 99.84
ReplaceClassKeyword 9.70 0.95 7.36 99.98
ParaphraseWords 9.70 0.99 7.42 99.98
CopyKwdOneSentence 9.45 1.06 7.23 99.90
CopyFirstSentence 9.28 1.08 7.22 99.88
CopyBulleted 9.01 1.00 6.88 99.58
CopyKwdMultipleSent-Sorted 8.48 0.83 6.59 99.68
MajorityKeyword 8.45 0.85 6.49 100.00
ThresholdNumber 7.83 0.77 6.05 100.00
CheckKeyword 7.79 0.77 5.94 100.00
CopyLastSentence 7.78 0.72 6.12 98.40
JoinClauses 7.72 0.81 6.09 98.82
ClassifyKeyword 6.80 0.62 5.34 100.00
LargestNumber 6.52 0.58 5.14 99.88
SumOfNumbers 5.03 0.40 4.14 25.06
CompareNumbers 1.89 0.04 1.75 48.88
Table7:Fordifferentmodelspretrainedononeindividualtaskeach,theirperformanceonCNN-Dailymail-10Kin
termsofROUGE(R1,R2,RL),andtheiraccuracyinpercentageonthepretrainingtask(Pr%)
Elementarysubtask Description
CheckKeyword Checkiftheinputhasaspecialkeywordornot.
ClassifyKeyword Inputcontains1of10specialkeywords-5orthemarepositive
and 5 of them are negative adjectives. Task is to tell whether
mentionedadjectivewaspositiveornegative
MajorityKeyword Outoftwogivenkeywords,findwhichoneoccursmorenumber
oftimes
CopyFirstSentence Copyfirstsentence
CopyBulleted Exactly one sentence is a bullet point and starts with the bullet
marker. Youhavetocopyoverthatsentencewithoutcopyingthe
marker.
CopyQuoted Copytextwithinquotes
CopyLastSentence Copylastsentence
CopyKwdOneSent Copy single sentence containing one of many special defined
keywords
CopyKwdMultipleSentInOrder Copy all sentences containing any special keyword in the same
orderastheyappearintext.
CopyKwdMultipleSentSorted Copyallsentencescontainingkeywordsbutsortthemaccording
tothecanonicalorderingofkeywords
CopyKwdMultipleSentShuffled Copy all sentences containing keywords in any order. The sen-
tencesingroundtruthmaybeanypossibleorder.
ReplaceClassKeyword There exist many keywords, each belonging to one of 3 classes.
Youhavetomentiontheclassofthementionedkeyword
CompareNumbers Giventwonumbersinthetext,saywhichoneisbigger
SumOfNumbers Sumnumbers
ThresholdNumber Theinputcontainsanumberbetween0and100. Youhavetosay
ifthenumberwasaboveorequaltothethresholdof50oflower
thanit
LargestNumber Findoutlargestofoneormorenumbersintheinput.
TruncateSentence Copyasentencebutonlytillthecutoffkeywordisencountered
BreakClauses Breakasinglesentenceintomultipleonescontainingoneclause
each
JoinClauses Joinclausesfrommultiplesentencestomakeonelongersentence
ParaphraseWords Copy the sentence containing one of pre-specified special key-
words. Butreplacethekeywordwithanyofitsmultiplesynonyms.
Thejth synonymofith keywordsrc isgivenbytarget
i ij
TopicSegregation Copy all sentences containing keywords belonging to different
classesbutputthemincorrespondingsections(eachclassgetsa
separatesection,whichcanbeemptytoo,sectionsalwaysoccur
insortedorder)
Table8: 21extractedelementarysummarizationsubtasksandtheirdescriptions
Domain Datasetname Paperusingthedataset
CNN-Dailymail Seeetal.(2017)
NYT Paulusetal.(2018)
News Gigaword Paulusetal.(2018)
XSUM LiuandLapata(2019)
Newsroom Zhangetal.(2020)
CodetoDocumentationdataset Iyeretal.(2016)
Code
Gitdifftocommit-messagedataset Allamanisetal.(2016)
Arxiv Cohanetal.(2018)
ScientificPaper Pubmed Cohanetal.(2018)
ScisummNet Yasunagaetal.(2019)
Patent BigPatent Sharmaetal.(2019)
Instructionalguides Wikihow Zhangetal.(2020)
Socialmediapost Reddit-TIFU Zhangetal.(2020)
Email AESLC Zhangetal.(2020)
Bills BillSum Zhangetal.(2020)
Amazonreviews Geranietal.(2019)
Yelpreviews ChuandLiu(2019)
Reviews
CNETreviews Geranietal.(2019)
Wikibio Lebretetal.(2016)
KeyValueAttributes
E2Edataset Novikovaetal.(2017)
DBPediatriplestoWikipedia Vougiouklisetal.(2018)
AMRtosentencedataset Songetal.(2018)
KnowledgeGraphs
Agenda Koncel-Kedziorskietal.(2019)
WebNLG Moryossefetal.(2019)
NumericalTable Rotowirebox-score Puduppullyetal.(2019)
Miscellaneouswebpages Wikisum Liuetal.(2018)
SamSum Gliwaetal.(2019)
Conversations
AMI WangandCardie(2013)
Table9: Existingsummarizationdatasetsinvariousdomains,alongwithcorrespondingpapersthatusethemand
cameupduringthesearchproceduretocharacterizeelementarytasksinsummarization

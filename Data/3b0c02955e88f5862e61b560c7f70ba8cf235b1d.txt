HomeRobot: Open-Vocabulary Mobile Manipulation
SriramYenamandra∗1 ArunRamachandran∗1 KarmeshYadav∗1,2 AustinWang1
MukulKhanna1 TheophileGervet2,3 Tsung-YenYang2 VidhiJain3
AlexanderWilliamClegg2 JohnTurner2 ZsoltKira1 ManolisSavva4
AngelChang4 DevendraSinghChaplot2 DhruvBatra1,2 RoozbehMottaghi2
YonatanBisk2,3 ChrisPaxton2
1GeorgiaTech 2FAIR,MetaAI 3CarnegieMellon 4SimonFraser
homerobot-info@googlegroups.com
Abstract: HomeRobot (noun): Anaffordablecompliantrobotthatnavigates
homesandmanipulatesawiderangeofobjectsinordertocompleteeverydaytasks.
Open-VocabularyMobileManipulation(OVMM)istheproblemofpickingany
objectinanyunseenenvironment,andplacingitinacommandedlocation. Thisis
afoundationalchallengeforrobotstobeusefulassistantsinhumanenvironments,
because it involves tackling sub-problems from across robotics: perception,
languageunderstanding,navigation,andmanipulationareallessentialtoOVMM.
In addition, integration of the solutions to these sub-problems poses its own
substantialchallenges. Todriveresearchinthisarea,weintroducetheHomeRobot
OVMM benchmark, where an agent navigates household environments to
grasp novel objects and place them on target receptacles. HomeRobot has two
components: a simulation component, which uses a large and diverse curated
objectsetinnew,high-qualitymulti-roomhomeenvironments;andareal-world
component, providing a software stack for the low-cost Hello Robot Stretch to
encouragereplicationofreal-worldexperimentsacrosslabs. Weimplementboth
reinforcementlearningandheuristic(model-based)baselinesandshowevidence
ofsim-to-realtransferofthenavandplaceskills. Ourbaselinesachievea20%
success rate in the real world; our experiments identify ways future work can
improveperformance. Seevideosonourwebsite: https://ovmm.github.io/.
Keywords: Sim-to-real,benchmarkingrobotlearning,mobilemanipulation
1 Introduction
Theaspirationtodevelophouseholdroboticassistantshasservedasanorthstarforroboticistssince
thebeginningofthefield. Thepursuitofthisvisionhasspawnedmultipleareasofresearchwithin
roboticsfromvisiontomanipulation,andhasledtoincreasinglycomplextasksandbenchmarks.
Ausefulhouseholdassistantrequirescreatingacapablemobilemanipulatorthatunderstandsawide
varietyofobjects,howtointeractwiththeenvironment,andhowtointelligentlyexploreaworld
withlimitedsensing. Thishasseparatelymotivatedresearchindiverseareaslikenavigation[1,2],
servicerobotics[3–5],languageunderstanding[6,7]andtaskandmotionplanning[8]. Werefer
to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will
beabletofindandmovearbitraryobjectsfromplacetoplaceinanarbitraryhome.
Priorworkdoesnottacklemobilemanipulationinlarge,continuous,real-worldenvironments.Instead,
itgenerallysimplifiesthesettingsignificantly,e.g. byusingdiscreteactionspaces,limitedobject
sets,orsmall,single-roomenvironmentsthatareeasilyexplored. However,recentdevelopments
tying language and vision have enabled robots to generalize beyond specific categories [9–13],
often through multi-modal models such as CLIP [14]. Further, comparison across methods has
remained difficult and reproduction of results across labs impossible, since many aspects of the
7thConferenceonRobotLearning(CoRL2023),Atlanta,USA.
4202
naJ
01
]OR.sc[
2v56511.6032:viXra
Find Object on Start Receptacle Pick Object from Start Receptacle Find Goal Receptacle Place Object on Goal Receptacle
Toy Animal
Chair
Move toy animal from chair to table Table
Pitcher
Drawer
Serving Cart
Move pitcher from drawer to serving cart
Figure1: Open-VocabularyMobileManipulationrequiresagentstosearchforapreviouslyunseen
objectataparticularlocation,andmoveittothecorrectreceptacle.
settings(environments,androbots)havenotbeenstandardized. Thisisespeciallyimportantnow,as
anewwaveofresearchprojectshavebeguntoshowpromisingresultsincomplex,open-vocabulary
navigation[9,15,11,12,16]andmanipulation[17,10,18]–againonawiderangeofrobotsand
settings, andstilllimitedtosingle-roomenvironments. Clearly, nowisthetimewhenweneeda
commonplatformandbenchmarkstodrivethefieldforward.
Inthiswork,wedefineOpen-VocabularyMobileManipulationasakeytaskforin-homeroboticsand
providebenchmarksandinfrastructure,bothinsimulationandtherealworld,tobuildandevaluate
full-stackintegratedmobilemanipulationsystems,inawidevarietyofhuman-centricenvironments,
withopenobjectsets. Ourbenchmarkwillfurtherreproducibleresearchinthissetting,andthefact
thatwesupportarbitraryobjectswillenabletheresultstobedeployedinavarietyofreal-world
environments.
OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world,
withanassociatedsimulationcomponent. Insimulation,weuseadatasetof200human-authored
interactive3Dscenes[19]instantiatedintheAIHabitatsimulator[20,21]tocreatealargenumber
ofchallenging,multi-roomOVMMproblemswithawidevarietyofobjectscuratedfromavarietyof
sources. Someoftheseobjects’categorieshavebeenseenduringtraining;othershavenot. Inthereal
world,wecreateanequivalentbenchmark,alsowithamixofseenandunseenobjectcategories,ina
controlledapartmentenvironment. WeusetheHelloRobotStretch[22]: anaffordableandcompliant
platformforhouseholdandsocialroboticsthatisalreadyinuseatover40universitiesandindustry
researchlabs. Fig.1showsinstantiationsofourOVMMtaskinboththereal-worldbenchmarkand
in simulation. We have a controlled real-world test environment, and plan to run the real-world
benchmarkyearlytoassessprogressonthischallengingproblem. Real-worldbenchmarkingwillbe
runasapartoftheNeurIPS2023HomeRobotOVMMcompetition[23].
HomeRobot: WealsoproposeHomeRobot,1asoftwareframeworktofacilitateextensivebench-
markinginbothsimulatedandphysicalenvironments. ItcomprisesidenticalAPIsthatareimple-
mentedacrossbothsettings,enablingresearcherstoconductexperimentsthatcanbereplicatedin
bothsimulatedandreal-worldenvironments. Table1comparesHomeRobotOVMMtotheliterature.
Notably,HomeRobotprovidesaroboticsstackfortheHelloRobotStretchwhichsupportsarange
ofcapabilitiesinbothsimulationandtherealworld,andisnotrestrictedtojusttheOVMMtask.
Ourlibraryalsosupportsanumberofsub-tasks,includingmanipulationlearning[24],continuous
learning[25],navigation[26],andobject-goalnavigation[2].
1https://github.com/facebookresearch/home-robot
2
LAER
MIS
Object Continuous Robotics Open
Scenes Cats Inst. Actions Sim2Real Stack Licensing Manipulation
RoomRearrangement [28] 120 118 118 ✖ ✖ ✖ ✔ ✖
HabitatObjectNavChallenge[29] 216 6 7,599 ✔ ✖ ✖ ✔ ✖
TDW-Transport [30] 15 50 112 ✖ ✖ ✖ ✓ ✓
VirtualHome [31] 6 308 1,066 ✖ ✖ ✖ ✔ ✓
ALFRED [6] 120 84 84 ✖ ✖ ✖ ✔ ✓
Habitat2.0HAB [21] 105 20 20 ✔ ✖ ✖ ✔ ✔
ProcTHOR [32] 10,000 108 1,633 ✖ ✖ ✖ ✔ ✔
RoboTHOR [33] 75 43 731 ✖ ✔ ✖ ✔ ✖
Behavior-1K [34] 50 1,265 5,215 ✔ ✔ ✖ ✖ ✓
ManiSkill-2 [35] 1 2,000 2,000 ✔ ✓ ✖ ✓ ✔
OVMM+HomeRobot 200 150 7,892 ✔ ✔ ✔ ✔ ✔
Table1: Comparisonsofourproposedbenchmarkwithpriorwork. Weprovidealargenumberof
environmentsanduniqueobjects,focusingonmanipulableobjects,withacontinuousactionspace.
Uniquely,wealsoprovideamulti-purpose,real-worldroboticsstack,withdemonstratedsim-to-real
capabilities, allowingotherstoreproduceanddeploytheirownsolutions. Additionalnuancesin
footnote3. ✓Partialavailability✖Notavailable✔Capabilityavailable
Inthispaper,weuseHomeRobottocomparetwofamiliesofapproaches: aheuristicsolution,using
amotionplannershowntoworkforreal-worldobjectsearch[2],andareinforcementlearning(RL)
solution, whichlearnshowtonavigatetoobjectsgivendepthandpredictedobjectsegmentation.
Weusetheopen-vocabularyobjectdetectorDETIC[27]toprovideobjectsegmentationforboth
the heuristic and RL policies. We observe that while the RL methods moved to the object more
efficiently if an object was visible, the heuristic planner was better at long-horizon exploration.
Wealsoseeasubstantialdropinperformancewhenswitchingfromground-truthsegmentationto
DETICsegmentation. ThishighlightstheimportanceoftheHomeRobotOVMMchallenge,asonly
throughviewingtheproblemholistically-integratingperception,planning,andaction-canwebuild
general-purposehomeassistants.
To summarize, in this paper, we define Open-Vocabulary Mobile Manipulation as a new, crucial
taskfortheroboticscommunityinSec.3. Weprovideanewsimulationenvironment,withmultiple,
multi-roominteractiveenvironmentsandawiderangeofobjects. Weimplementaroboticslibrary
calledHomeRobotwhichprovidesbaselinepoliciesimplementingthisinboththesimulationandthe
realworld. Wedescribeareal-worldbenchmarkinacontrolledenvironment,andshowhowcurrent
baselinesperforminsimulationandintherealworldunderdifferentconditions. Weplantoinitially
runthisreal-worldbenchmarkasaNeurips2023competition[23].
2 RelatedWork
Wediscussworkrelatedtochallengesandreproducibilityofroboticsresearchinmoredetail,but
continuethediscussionofdatasetsandsimulatorsinAppendixA.
Challenges. Therehavebeenseveralchallengesaimingtobenchmarkroboticsystemsatdifferent
tasks. Thesechallengesprovidedagreattestbedforrankingdifferentsystems. However,inmost
ofthechallenges(e.g.,[36–39,3]),theparticipantscreatetheirownroboticplatformmakingafair
comparisonofthealgorithmsdifficult. Therearealsochallengeswheretheorganizersprovidethe
robotic platform to the participants (e.g., [40]). However, changing the task during the periodic
evaluationsmadeitdifficulttotrackprogressovertime. Ouraimistohavearealworldbenchmark
usingastandardhardwarethatissustainableatleastforafewyears.
Reproducibilityofroboticsresearch. Standardizedroboticsbenchmarkshavebeenpursuedfora
longtime,oftenbyopen-sourcingrobotdesignsorintroducinglow-costrobots[41–49].However,the
environmentsinwhichtheserobotsareusedvarydramatically,leadingtoevaluationofcomponents
(e.g., object navigation, SLAM) in isolation, instead of as components of a larger system that
3ALFREDusesobjectmasksforinteraction.ObjectNavusesscans,notfullobjectmeshes.ProcThorscenes
areprocedurallygenerated,thishasthebenefitthatthepotentialnumberofenvironmentsisunbounded.
3
SIM REAL
Figure2: Alow-costhomerobotperformingtasksinbothasimulatedandareal-worldenvironment.
Weprovideboth(1)challengingsimulatedtasks,whereinamobilemanipulatorrobotmustfindand
graspmultipleseenandunseenobjects,and(2)acorrespondingreal-worldroboticsstacktoallow
otherstoreproducethisresearchandevaluationtoproduceusefulhomerobotassistants.
maynotbenefitfromthosechanges. TheHomeRobotstackenablesend-to-endbenchmarkingof
individualcomponentsbyprovidingafullroboticsstack,withmultipleimplementationsofdifferent
sub-modules. Thesimplicityhelpsmovebeyondstandardizedsetsofobjects(e.g., [50–52])toa
commonsetofrobots,objects,andenvironments. Oursistheonlybenchmarktoprovideabroadly
capableroboticsstackforimplementingandsharingroboticscode;thisissimilartoprojectslike
PyRobot[53],whichdoesn’talsoprovideastrongsimulationbenchmark.
Real World Benchmarks. RoboTHOR [33] provides a common set of scenes and objects for
benchmarking navigation. RB2 [54] ranks different manipulation algorithms in a local setting.
TOTO [55] takes a step further by providing a training dataset and running the experiments for
the users. However, training and testing happen in the same environments and are limited to
tabletopmanipulation. Finally,theNISTTaskBoard[56]isasuccessfulchallengeforfine-grained
manipulationskills[57],alsolimitedtoatabletopcontext. Kadianetal.[58]proposetheHabitat-
PyRobotbridge(HaPy)toallowreal-worldtestingonthelocobotrobot;theirframeworkislimited
tonavigation,anddoesn’tprovideagenerally-usefulroboticsstackwithvisualizations,debugging,
motionplanners,tooling,etc.
3 Open-VocabularyMobileManipulation
Formally, our task is set up as instructions of the form: “Move (object) from the
(start_receptacle) to the (goal_receptacle).” The object is a small and manipulable
household object (e.g., a cup, stuffed toy, or box). By contrast, start_receptacle and
goal_receptaclearelargepiecesoffurniture, whichhavesurfacesuponwhichobjectscanbe
placed.Therobotisplacedinanunknownsingle-floorhomeenvironment-suchasanapartment-and
must,giventhelanguagenamesofstart_receptacle,object,andgoal_receptacle,pickup
anobjectthatisknowntobeonastart_receptacleandmoveittoanyvalidgoal_receptacle.
start_receptacleisalwaysavailable,tohelpagentsknowwheretolookfortheobject.
The agent is successful if the specified object is indeed moved from a start_receptacle on
whichitbegantheepisode, toanyvalidgoal_receptacle. Wegivepartialcreditforeachstep
therobotaccomplishes: findingthestart_receptaclewiththeobject,pickinguptheobject,
findingthegoal_receptacle,andplacingtheobjectonthegoal_receptacle. Therecanbe
multiplevalidobjectsthatsatisfyeachquery.
Crucially, we need and develop both (1) a simulation version of the Open-Vocabulary Mobile
Manipulationproblem,forreproducibility,training,andfastiteration,and(2)areal-robotstackwith
acorrespondingreal-worldbenchmark. WecomparethetwoinFig.2. Oursimulatedenvironments
allowforvaried,long-horizontaskexperimentation; ourreal-worldHomeRobotstackallowsfor
4
experimentingwithrealdata,andwedesignasetofreal-worldteststoevaluatetheperformance
ofourlearnedandheuristicbaselines.
TheRobot. WeusetheHelloRobotStretch[22]withDexWristasthemobilemanipulationplatform,
becauseit(1)isrelativelyaffordableat$25,000USD,(2)offers6DoFmanipulation, and(3)is
humansafeandhuman-sized,makingitsafetotestinlabs[24,11]andhomes[2],andcanreach
mostplacesahumanwouldexpectarobottogo. Forabreakdownofhardwarechoices,seeSec.H.1.
Objects. Thesearesplitintoseenvs. unseencategoriesandinstances. Inparticular,attesttimewe
lookatunseeninstancesofseenorunseencategories;i.e. noseenmanipulableobjectfromtraining
appearsduringevaluation. Agentsmustpickandplaceanyrequestedobject.
Receptacles. Weincludecommonhouseholdreceptacles(e.g. tables,chairs,sofas)inourdataset;
unlikewithmanipulableobjects,allpossiblereceptaclecategoriesareseenduringtraining.
Scenes. Wehavebothasimulatedscenedatasetandafixedsetofreal-worldsceneswithspecific
furniturearrangementsandobjects. Inbothsimulatedandrealscenes,weuseamixtureofobjects
from previously-seen categories, and objects from unseen categories as the goal object for our
Open-VocabularyMobileManipulationtask. Weholdoutvalidationandtestscenes,whichdonot
appearinthetrainingdata;whilesomereceptaclesmayre-appear,theywillbeatpreviouslyunseen
locations,andtargetobjectinstanceswillbeunseen.
Scoring. Wecomputesuccessforeachstage: findingobjectonstart_receptacle,successfully
pickingupobject,findinggoal_receptacle,andplacingobjectonthegoal. Overallsuccess
istrueifallfourstageswereaccomplished. Wecomputepartialsuccessasatie-breaker,inwhich
agentsreceive1pointforeachsuccessivestageaccomplished,normalizedbythenumberofstages.
MoredetailsinAppendixC.
3.1 SimulationDataset
TheHabitatSyntheticScenesDataset(HSSD)[19]consistsof200+
human-authored3Dhomescenescontainingover18k3Dmodelsof
real-worldobjects. Likemostrealhouses,thesescenesarecluttered
with furniture and other objects placed into realistic architectural
layouts,makingnavigationandmanipulationsimilarlydifficultto
the real world. We used a subset of HSSD [19] consisting of 60
scenesforwhichadditionalmetadataandsimulationstructureswere
authoredtosupportrearrangement4. Forourexperiments,theseare
dividedintotrain,validation,andtestsplitsof38,12,and10scenes
each,followingthesplitsintheoriginalHSSDpaper[19].
Objects and Receptacles. We aggregate objects from AI2-
Thor [59], Amazon-Berkeley Objects [60], Google Scanned Ob-
jects[61]andtheHSSD[19]datasettocreatealargeanddiverse
datasetofreal-worldrobotproblems.Intotal,weannotated2,535ob-
jectsfrom129totalcategories.Weidentified21differentcategories Figure3: HSSDscenes.
ofreceptacleswhichappearintheHSSDdataset[19].
We construct our final set of furniture receptacle
SC,SI SC,UI UC,UI Total
objectsbyfirstautomaticallylabelingstableareas
ontopofreceptacles,thenmanuallyrefiningand Cats 85 64 44 129
Insts 1,363 748 424 2,535
processingtheseinordertoremoveinvalidorin-
accessiblereceptacles. Inaddition,collisionproxy Table2: #ofobjectsinthesimforeachsplitof
mesheswereautomaticallygeneratedandinmany (S)eenand(U)nseen(I)nstanceand(C)ategory.
casesmanuallycorrectedtosupportphysicallyac-
curateproceduralplacementofobjectarrangements.
4All200+sceneswithrearrangementsupportwillbereleasedsoon.
5
Figure4: HomeRobotisasimple,easy-to-set-uplibrarythatworksinmultipleenvironmentsand
requiresonlyrelativelyaffordablehardware. Computationallyintensiveoperationsareperformedon
adesktopPCwithaGPU,andadedicatedconsumer-graderouterprovidesanetworkinterfacetoa
robotrunninglow-levelcontrolandSLAM.
Episode Generation. We generate episodes consisting of varying object arrangements and
particularvaluesforobject,start_receptacle,andgoal_receptacle,whichallowouragent
to successfully move about and interact with the world. In the case of Open-Vocabulary Mobile
Manipulation,thistaskisparticularlychallengingbecausewehavetoplaceobjectsinlocationsthat
arenavigable,meaningthattherobotcangettothem,reachable,meaningitsarmcanmakeitto
theselocations,andfromwhichwecannavigatetoanavigable,reachablegoalreceptacle. Forfull
episodegenerationdetailsseeApp.D.2.
TrainingandValidationSplit. Trainingepisodesconsistofobjectsfromthelargepoolofseen
instancesofseencategories(SC,SI).Incontrast,weuseunseeninstancesofseenobjectcategories
(SC,UI)andunseeninstancesofunseencategories(UC,UI)forvalidationandtestepisodes. Two-
thirdsofthecategorieswererandomlydesignatedasseen,andtwo-thirdsoftheobjectsintheseen
categorywererandomlymarkedasseeninstances. SplitsareinTable2andthedistributionofobjects
acrosscategoriesisinApp. Fig.6.
3.2 RealWorldBenchmark
Real-worldexperimentsareperformedinacontrolled3-roomapartmentenvironment,withasofa,
kitchentable,counterwithbar,andTVstand,amongotherfeatures. Wedocumentedthepositioning
ofvariousobjectsandtherobotstartposition,inordertoensurereproducibilityacrosstrials. Images
ofvariouslayoutsofthetestapartmentareincludedinFig.2,andtaskexecutionisshowninFig.16.
Duringreal-worldtesting,weselectedobjectinstancesthatdidnotappearinsimulationtraining,
splitbetweenclassesthatdidanddidnotappear. Weusedeightdifferentcategories: fiveseen(Cup,
Bowl, Stuffed Toy, Medicine Bottle, and Toy Animal), and three unseen (Rubik’s cube, Toy Drill,
andLemon). Weperformed20experimentsforeachofourtwodifferentbaselinesandwithseven
differentreceptacleclasses: Cabinet,Chair,Couch,Counter,Sink,Stool,Table.
4 The HomeRobotLibrary
Tofacilitateresearchonthesechallengingproblems,weopen-sourcetheHomeRobotlibrary,which
implementsnavigationandmanipulationcapabilitiessupportingHelloRobot’sStretch[22]. Inour
setup,itisassumedthatusershaveaccesstoamobilemanipulatorandanNVIDIAGPU-powered
workstation. Themobilemanipulatorrunsthelow-levelcontrollerandthelocalizationmodule,while
thedesktoprunsthehigh-levelperceptionandplanningstack(Fig.4). Therobotanddesktopare
connectedusinganoff-the-shelfrouter5. Thekeyfeaturesofourstackinclude:
5OurexperimentsusedaNetGearNighthawkrouter.
6
Transferability: Unifiedstateandactionspacesbetweensimulation&real-worldsettingsforeach
task,providinganeasywaytocontrolarobotwitheitherhigh-levelactionspaces(e.g.,pre-made
graspingpolicies)orlow-levelcontinuousjointcontrol.
Modularity: Perceptionandactioncomponentstosupporthigh-levelstates(e.g. semanticmaps,
segmentedpointclouds)andhigh-levelactions(e.g. gotogoalposition,pickuptargetobject).
BaselineAgents: PoliciesthatusethesecapabilitiestoprovidebasicfunctionalityforOVMM.
4.1 BaselineAgentImplementation
Crucially,weprovidebaselinesandtoolsthatenableresearcherstoeffectivelyexploretheOpen-
VocabularyMobileManipulationtask. WeincludetwotypesofbaselinesinHomeRobot: aheuristic
baseline,usingmotionplanning[2]andsimplerulesformanipulation;andareinforcementlearning
baseline. Inaddition,wehaveimplementedexampleprojectsfromseveralrecentlyreleasedpapers,
testing different capabilities such as object-goal navigation [1, 2], skill learning [24], continual
learning [25], and image instance navigation [26]. We implement a high-level policy called
OVMMAgentwhichcallsasequenceofskillsoneaftertheother. Theseskillsare:
FindObj/FindRec: Locateanobjectonastart_receptacle;orfindagoal_receptacle.
Gaze: Movecloseenoughtoanobjecttograspit,andorientheadtogetagoodviewoftheobject.
Thegoalofthegazeactionistoimprovethesuccessrateofgrasping.
Pick: Pickuptheobject. Weprovideahigh-levelactionforthis, sincewedonotsimulatethe
gripperinteractioninHabitat. However,ourlibraryiscompatiblewitharangeoflearnedgrasping
skillsandsupportslearningpoliciesforgrasping.
Place:Movetoalocationintheenvironmentandplacetheobjectontopofthegoal_receptacle.
Specifically,OVMMAgentisastate-machinethatcallsFindObj,Gaze,Pick,FindRec,andPlacein
thatorder,wherePickisagraspingpolicyprovidedbytherobotlibraryintherealworld. Theother
skillsarecreatedusingtheapproachesgivenbelow:
Heuristic. Weimplementaversionusingonlyoff-the-shelflearnedmodelsandheuristics,noting
thatpreviousworkinmobilemanipulationhasusedthesemodelstogreateffect(e.g.[62]). Here,
DETIC [63] provides masks for an open-vocabulary set of objects as appropriate for each skill.
Thestart_receptacle,object,goal_receptacleforeachepisodeisgiven. Fig.16showsan
exampleoftheheuristicnavigationandplacepolicybeingexecutedintherealworld(App.E).
RL.WetrainthefourskillsinourmodifiedversionofHabitat[21]aspolicieswhichpredictactions
givendepth,groundtruthsemanticsegmentationandpriopreceptivesensors(i.e. joints,gripperstate),
usingDDPPO[64]. WhileRGBisavailableinoursimulation,ourbaselinepoliciesdonotdirectly
utilizeit;instead,theyrelyonpredictedsegmentationfromDetic[27]attesttime.
5 Results
Wefirstevaluatethetwobaselinesinoursimulatedbenchmark,followedbyevaluationinareal-
world,held-outtestapartment. TheseresultshighlightthesignificanceofOVMMasachallenging
newbenchmark,encompassingnumerousessentialchallengesthatarisewhendeployingrobotsin
real-worldenvironments.
Webreakdowntheresultsbysub-taskinadditiontoreportingtheoverallperformanceinTables3
and4. ThecolumnsFindObj,PickandFindRecrefertothefirst3phasesofthetaskmentionedin
thescoringsection(Sec.3),andsucceedinginthefinalPlacephaseleadstoasuccessfulepisode.
Simulation. Weevaluatethebaselinesonheld-outscenes,withobjectsfromunseeninstancesof
seenclasses,andunseeninstancesofunseenclasses,asdescribedinSec.3.1. Weshowresultswith
two different perception systems: Ground Truth segmentation, where we use the segmentation
inputdirectlyfromthesimulator,andDETICsegmentation[27],wheretheRGBimagesfromthe
simulatorarepassedthroughDETIC,anopen-vocabularyobjectdetector.
7
SimulationResults Skill PartialSuccessRates Overall Partial
Perception Navigation Gaze Place FindObj Pick FindRec SuccessRate SuccessMetric
GroundTruth Heuristic None Heuristic 54.1 48.5 31.5 5.1 34.8
Heuristic RL RL 56.5 51.5 42.3 13.2 40.9
RL None Heuristic 65.4 54.8 43.7 7.3 42.8
RL RL RL 66.6 61.1 50.9 14.8 48.3
DETIC[27] Heuristic None Heuristic 28.7 15.2 5.3 0.4 12.4
Heuristic RL RL 29.4 13.2 5.8 0.5 12.2
RL None Heuristic 21.9 11.5 6.0 0.6 10.0
RL RL RL 21.7 10.2 6.2 0.4 9.6
Table3:Partialandoverallsuccessrate(SR)(in%)fordifferentcombinationsofskillsandperception
systems. ThepartialSRforeachskillisdependentonthepreviousskill’sSR.ThepartialSRforthe
placeskillisthesameastheoverallSR.Thepartialsuccessmetriciscalculatedbyaveragingthe4
partialSRs. Oneofthemaincausesoffailuresforourbaselinesystemswasperception;ground-truth
perceptionisnotablybetter. BothRLandheuristicskillsstruggledtonavigatetightlyconstrained
multi-roomenvironmentsandsuccessfullyplaceobjects.
WereportresultsonHomeRobotOVMMinTable3. RLpoliciesoutperformedheuristicmethods
for both navigation and placement tasks. However, all policies declined in performance when
usingDETICinsteadofgroundtruthsegmentation. Heuristicpoliciesexhibitedlessdegradation
inperformancecomparedtoRLpolicies: whenusingDETIC,theheuristicFindObjpolicyeven
outperformsRL.Weattributethistotheheuristicpolicy’sabilitytoincorporatenoisypredictionsby
constructinga2Dsemanticmap,whichhelpshandlesmallobjectsthatarepronetomisclassification.
Furthermore,usingthelearnedgazepolicyledtoimprovedpickperformance,exceptwhenusedin
combinationwiththeHeuristicnavwithDETICperception. Examplesimulationtrajectoriescanbe
foundinAppendixFigure18,andcomparisonsofseenversusunseencategoriesinAppendixG.2.
RealWorld. Finally,weconductedaseriesofexperimentsinareal-worldheld-outapartmentsetting.
Weperformedatotalof20episodes,utilizingacombinationofseenandunseenobjectclassesas
ourtargetobjects. Theresultsoftheseexperimentsarepresentedin Table4. RLperformedslightly
betterthantheHeuristicbaseline,successfullycompleting1extraepisodeandachievingasuccess
rateof20%. Thisdifferenceprimarilystemmedfromthepickandplacesub-tasks. Inthepicktask,
theRLGazeskillplaysacrucialroleinachievingbetteralignmentbetweentheagentandthetarget
object,whichleadstomoresuccessfulgrasping. Similarly,theRLplaceskilldemonstratedmore
precision,ensuringthattheobjectstayedclosertothesurfaceofthereceptacle.
Bothsimulationandreal-worldresultsshowthe
RealWorld FindObj Pick FindRec OverallSuccess
baselines are promising, but insufficient, for
HeuristicOnly 70 35 30 15
Open-Vocabulary Mobile Manipulation. DE-
RLOnly 70 45 30 20
TIC[27]causedmanyfailuresduetomisclas-
sification,bothinsimulationandtherealworld.
Table4: SuccessRate(in%)forheuristicandRL
Further,RLnavigationwasonparorbetterthan
baselinesintherealworldOVMMtask. Inboth
heuristicpoliciesinbothsimandreal. Although cases,thegraspingactionisexecutedasdescribed
ourRLplacepolicyperformedbetterinsimthan inSec.4;butinitialconditionsoftherobotsuch
heuristicplace,itneedsfurtherimprovementin as its position relative to the object or to other
therealworld. Gainingtheadvantagesofweb- obstaclesmaycausevariousfailures.
scale pretrained vision-language models like DETIC, but tuned to our agents may be crucial for
improvingperformance.
6 ConclusionsandFutureWork
Weproposedacombinedsimulationandreal-worldbenchmarktoenableprogressontheimportant
problemofOpen-VocabularyMobileManipulation.Weranextensiveexperimentsshowingpromising
simulationandreal-worldresultsfromtwobaselines: aheuristicbaselinebasedonastate-of-the-art
motionplanner[2]andareinforcementlearningbaselinetrainedwithDDPPO[64]. Inthefuture,we
hopetoimprovethecomplexityoftheproblemspace,addingmorecomplexnaturallanguageand
multi-stepcommands,andprovideend-to-endbaselinesinsteadofmodularpolicies.
8
7 Acknowledgements
We would like to thank Andrew Szot for help with Habitat policy training, Santhosh Kumar Ra-
makrishnan with help on Stretch Object navigation in simulation and on the real robot, and Eric
UndersanderforhelpwithimprovingHabitatrendering. PriyamParashar,XiaohanZhang,andJay
VakilhelpedwithtestingonStretchandreal-worldscenesetup.
WewouldalsoliketothankthewholeHelloRobotteam,butespeciallyBinitShahandBlaineMat-
ulevichfortheirhelpwiththerobots,andAaronEdsingerandCharlieKempforhelpfuldiscussions.
The Georgia Tech effort was supported in part by ONR YIP and ARO PECASE. The views and
conclusionscontainedhereinarethoseoftheauthorsandshouldnotbeinterpretedasnecessarily
representingtheofficialpoliciesorendorsements,eitherexpressedorimplied,oftheU.S.Government,
oranysponsor.
References
[1] D.Batra,A.Gokaslan,A.Kembhavi,O.Maksymets,R.Mottaghi,M.Savva,A.Toshev,and
E.Wijmans. Objectnavrevisited: Onevaluationofembodiedagentsnavigatingtoobjects.
arXiv,2020.
[2] T.Gervet,S.Chintala,D.Batra,J.Malik,andD.S.Chaplot. Navigatingtoobjectsinthereal
world. arXiv,2022.
[3] T.Wisspeintner, T.VanDerZant, L.Iocchi, andS.Schiffer. Robocup@home: Scientific
competitionandbenchmarkingfordomesticservicerobots. InteractionStudies,2009.
[4] J.Bohren,R.B.Rusu,E.G.Jones,E.Marder-Eppstein,C.Pantofaru,M.Wise,L.Mösenlech-
ner,W.Meeussen,andS.Holzer. Towardsautonomousroboticbutlers: Lessonslearnedwith
thepr2. InICRA,2011.
[5] W.Burgard,A.B.Cremers,D.Fox,D.Hähnel,G.Lakemeyer,D.Schulz,W.Steiner,and
S.Thrun. Experienceswithaninteractivemuseumtour-guiderobot. Artificialintelligence,
1999.
[6] M.Shridhar,J.Thomason,D.Gordon,Y.Bisk,W.Han,R.Mottaghi,L.Zettlemoyer,and
D.Fox. Alfred: Abenchmarkforinterpretinggroundedinstructionsforeverydaytasks. In
CVPR,2020.
[7] X.Puig,K.Ra,M.Boben,J.Li,T.Wang,S.Fidler,andA.Torralba. Virtualhome: Simulating
householdactivitiesviaprograms. InCVPR,2018.
[8] C.R.Garrett,R.Chitnis,R.Holladay,B.Kim,T.Silver,L.P.Kaelbling,andT.Lozano-Pérez.
Integratedtaskandmotionplanning. AnnualReviewofControl,Robotics,andAutonomous
Systems,4:265–293,2021.
[9] B.Chen,F.Xia,B.Ichter,K.Rao,K.Gopalakrishnan,M.S.Ryoo,A.Stone,andD.Kappler.
Open-vocabularyqueryablescenerepresentationsforrealworldplanning. arXiv,2022.
[10] A.Stone,T.Xiao,Y.Lu,K.Gopalakrishnan,K.-H.Lee,Q.Vuong,P.Wohlhart,B.Zitkovich,
F. Xia, C. Finn, et al. Open-world object manipulation using pre-trained vision-language
models. arXiv,2023.
[11] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam. Clip-fields: Weakly
supervisedsemanticfieldsforroboticmemory. arXiv,2022.
[12] B.Bolte,A.Wang,J.Yang,M.Mukadam,M.Kalakrishnan,andC.Paxton. Usa-net: Unified
semanticandaffordancerepresentationsforrobotmemory. arXiv,2023.
9
[13] A. Curtis, X. Fang, L. P. Kaelbling, T. Lozano-Pérez, and C. R. Garrett. Long-horizon
manipulationofunknownobjectsviataskandmotionplanningwithestimatedaffordances. In
ICRA,2022.
[14] A.Radford, J.W.Kim, C.Hallacy, A.Ramesh, G.Goh, S.Agarwal, G.Sastry, A.Askell,
P.Mishkin,J.Clark,etal. Learningtransferablevisualmodelsfromnaturallanguagesupervi-
sion. InICML,2021.
[15] K.M.Jatavallabhula,A.Kuwajerwala,Q.Gu,M.Omama,T.Chen,S.Li,G.Iyer,S.Saryazdi,
N.Keetha,A.Tewari,etal. Conceptfusion: Open-setmultimodal3dmapping. arXiv,2023.
[16] J.Krantz, E.Wijmans, A.Majundar, D.Batra, andS.Lee. Beyondthenav-graph: Vision
andlanguagenavigationincontinuousenvironments. InEuropeanConferenceonComputer
Vision(ECCV),2020.
[17] W.Liu,T.Hermans,S.Chernova,andC.Paxton. Structdiffusion: Object-centricdiffusionfor
semanticrearrangementofnovelobjects. arXiv,2022.
[18] D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,A.Wahid,J.Tompson,
Q.Vuong,T.Yu,etal. Palm-e: Anembodiedmultimodallanguagemodel. arXiv,2023.
[19] M.Khanna*,Y.Mao*,H.Jiang,S.Haresh,B.Shacklett,D.Batra,A.Clegg,E.Undersander,
A.X.Chang,andM.Savva. HabitatSyntheticScenesDataset(HSSD-200): AnAnalysisof
3DSceneScaleandRealismTradeoffsforObjectGoalNavigation. arXivpreprint,2023.
[20] M.Savva,A.Kadian,O.Maksymets,Y.Zhao,E.Wijmans,B.Jain,J.Straub,J.Liu,V.Koltun,
J.Malik,D.Parikh,andD.Batra. Habitat: APlatformforEmbodiedAIResearch. ICCV,
2019.
[21] A.Szot,A.Clegg,E.Undersander,E.Wijmans,Y.Zhao,J.Turner,N.Maestre,M.Mukadam,
D.S.Chaplot,O.Maksymets,etal. Habitat2.0: Traininghomeassistantstorearrangetheir
habitat. InNeurIPS,2021.
[22] C.C.Kemp,A.Edsinger,H.M.Clever,andB.Matulevich. Thedesignofstretch: Acompact,
lightweightmobilemanipulatorforindoorhumanenvironments. InICRA,2022.
[23] S. Yenamandra, A. Ramachandran, M. Khanna, K. Yadav, D. S. Chaplot, G. Chhablani,
A. Clegg, T. Gervet, V. Jain, R. Partsey, R. Ramrakhya, A. Szot, T.-Y. Yang, A. Edsinger,
C.Kemp,B.Shah,Z.Kira,D.Batra,R.Mottaghi,Y.Bisk,,andC.Paxton. Homerobotopen
vocabmobilemanipulationchallenge2023. https://aihabitat.org/challenge/2023_
homerobot_ovmm/,2023.
[24] P.Parashar,J.Vakil,S.Powers,andC.Paxton. Spatial-languageattentionpoliciesforefficient
robotlearning. arXiv,2023.
[25] S.Powers,A.Gupta,andC.Paxton. Evaluatingcontinuallearningonahomerobot,2023.
[26] J.Krantz,T.Gervet,K.Yadav,A.Wang,C.Paxton,R.Mottaghi,D.Batra,J.Malik,S.Lee,
andD.S.Chaplot. Navigatingtoobjectsspecifiedbyimages. arXiv,2023.
[27] X.Zhou,R.Girdhar,A.Joulin,P.Krähenbühl,andI.Misra.Detectingtwenty-thousandclasses
usingimage-levelsupervision. InECCV,2022.
[28] L.Weihs,M.Deitke,A.Kembhavi,andR.Mottaghi. Visualroomrearrangement. InCVPR,
2021.
[29] K. Yadav, J. Krantz, R. Ramrakhya, S. K. Ramakrishnan, J. Yang, A. Wang, J. Turner,
A.Gokaslan,V.-P.Berges,R.Mootaghi,O.Maksymets,A.X.Chang,M.Savva,A.Clegg,D.S.
Chaplot, and D. Batra. Habitat challenge 2023. https://aihabitat.org/challenge/
2023/,2023.
10
[30] C.Gan,J.Schwartz,S.Alter,D.Mrowca,M.Schrimpf,J.Traer,J.DeFreitas,J.Kubilius,
A.Bhandwaldar,N.Haber,etal.Threedworld:Aplatformforinteractivemulti-modalphysical
simulation. NeurIPSDatasetsandBenchmarksTrack,2021.
[31] X.Puig,K.Ra,M.Boben,J.Li,T.Wang,S.Fidler,andA.Torralba. Virtualhome: Simulating
householdactivitiesviaprograms. InCVPR,2018.
[32] M.Deitke,E.VanderBilt,A.Herrasti,L.Weihs,J.Salvador,K.Ehsani,W.Han,E.Kolve,
A.Farhadi,A.Kembhavi,andR.Mottaghi.Procthor:Large-scaleembodiedaiusingprocedural
generation. InNeurIPS,2022.
[33] M.Deitke,W.Han,A.Herrasti,A.Kembhavi,E.Kolve,R.Mottaghi,J.Salvador,D.Schwenk,
E.VanderBilt,M.Wallingford,L.Weihs,M.Yatskar,andA.Farhadi. RoboTHOR:AnOpen
Simulation-to-RealEmbodiedAIPlatform. InCVPR,2020.
[34] C.Li,R.Zhang,J.Wong,C.Gokmen,S.Srivastava,R.Martín-Martín,C.Wang,G.Levine,
M.Lingelbach,J.Sun,etal. Behavior-1k: Abenchmarkforembodiedaiwith1,000everyday
activitiesandrealisticsimulation. InCoRL,2023.
[35] T.Mu,Z.Ling,F.Xiang,D.C.Yang,X.Li,S.Tao,Z.Huang,Z.Jia,andH.Su. Maniskill:
Generalizable manipulation skill benchmark with large-scale demonstrations. In NeurIPS
DatasetsandBenchmarksTrack,2021.
[36] E. Krotkov, D. Hackett, L. Jackel, M. Perschbacher, J. Pippine, J. Strauss, G. Pratt, and
C. Orlowski. The darpa robotics challenge finals: Results and perspectives. The DARPA
RoboticsChallengeFinals: HumanoidRobotsToTheRescue,2018.
[37] G.Seetharaman,A.Lakhotia,andE.P.Blasch. Unmannedvehiclescomeofage: Thedarpa
grandchallenge. Computer,2006.
[38] M.Buehler,K.Iagnemma,andS.Singh. TheDARPAurbanchallenge: autonomousvehicles
incitytraffic. SpringerBerlin,Heidelberg,2009.
[39] N.Correll,K.E.Bekris,D.Berenson,O.Brock,A.Causo,K.Hauser,K.Okada,A.Rodriguez,
J.M.Romano,andP.R.Wurman. Analysisandobservationsfromthefirstamazonpicking
challenge. IEEETransactionsonAutomationScienceandEngineering,2016.
[40] L.D.Jackel,E.Krotkov,M.Perschbacher,J.Pippine,andC.Sullivan. Thedarpalagrprogram:
Goals,challenges,methodology,andphaseiresults. JournalofFieldRobotics,2006.
[41] M.MüllerandV.Koltun. Openbot: Turningsmartphonesintorobots. InICRA,2021.
[42] N.Kau,A.Schultz,N.Ferrante,andP.Slade. Stanforddoggo: Anopen-source,quasi-direct-
drivequadruped. InICRA,2019.
[43] F. Grimminger, A. Meduri, M. Khadiv, J. Viereck, M. Wüthrich, M. Naveau, V. Berenz,
S.Heim,F.Widmaier,J.Fiene,A.Badri-Spröwitz,andL.Righetti. Anopentorque-controlled
modularrobotarchitectureforleggedlocomotionresearch. IEEERoboticsandAutomation
Letters,2019.
[44] B.Yang,J.Zhang,V.H.Pong,S.Levine,andD.Jayaraman. Replab: Areproduciblelow-cost
armbenchmarkplatformforroboticlearning. arXiv,2019.
[45] D.V.Gealy,S.McKinley,B.Yi,P.Wu,P.R.Downey,G.Balke,A.Zhao,M.Guo,R.Thomas-
son, A. Sinclair, P. Cuellar, Z. McCarthy, and P. Abbeel. Quasi-direct drive for low-cost
compliantroboticmanipulation. InICRA,2019.
[46] M.Wüthrich,F.Widmaier,F.Grimminger,S.Joshi,V.Agrawal,B.Hammoud,M.Khadiv,
M.Bogdanovic,V.Berenz,J.Viereck,M.Naveau,L.Righetti,B.Schölkopf,andS.Bauer.
Trifinger: Anopen-sourcerobotforlearningdexterity. InCoRL,2020.
11
[47] M. Ahn, H. Zhu, K. Hartikainen, H. Ponte, A. Gupta, S. Levine, and V. Kumar. ROBEL:
ROboticsBEnchmarksforLearningwithlow-costrobots. InCoRL,2019.
[48] A.Murali,T.Chen,K.V.Alwala,D.Gandhi,L.Pinto,S.Gupta,andA.K.Gupta. Pyrobot:
Anopen-sourceroboticsframeworkforresearchandbenchmarking. arXiv,2019.
[49] L.Paull,J.Tani,H.Ahn,J.Alonso-Mora,L.Carlone,M.Cáp,Y.F.Chen,C.Choi,J.Dusek,
Y.Fang,D.Hoehener,S.Liu,M.M.Novitzky,I.F.Okuyama,J.Pazis,G.Rosman,V.Varric-
chio,H.-C.Wang,D.S.Yershov,H.Zhao,M.R.Benjamin,C.Carr,M.T.Zuber,S.Karaman,
E.Frazzoli,D.D.Vecchio,D.Rus,J.P.How,J.J.Leonard,andA.Censi. Duckietown: An
open,inexpensiveandflexibleplatformforautonomyeducationandresearch. InICRA,2017.
[50] B.Calli,A.Singh,A.Walsman,S.Srinivasa,P.Abbeel,andA.M.Dollar. Theycbobjectand
modelset: Towardscommonbenchmarksformanipulationresearch. InICRA,2015.
[51] D.Morrison,P.Corke,andJ.Leitner. Egad! anevolvedgraspinganalysisdatasetfordiversity
andreproducibilityinroboticmanipulation. IEEERoboticsandAutomationLetters,2020.
[52] B.Yang,P.E.Lancaster,S.S.Srinivasa,andJ.R.Smith. Benchmarkingrobotmanipulation
withtherubik’scube. IEEERoboticsandAutomationLetters,2020.
[53] A.Murali,T.Chen,K.V.Alwala,D.Gandhi,L.Pinto,S.Gupta,andA.Gupta. Pyrobot: An
open-sourceroboticsframeworkforresearchandbenchmarking. arXiv,2019.
[54] S.Dasari,J.Wang,J.Hong,S.Bahl,Y.Lin,A.S.Wang,A.Thankaraj,K.S.Chahal,B.Çalli,
S.Gupta,D.Held,L.Pinto,D.Pathak,V.Kumar,andA.Gupta. Rb2: Roboticmanipulation
benchmarkingwithatwist. arXiv,2022.
[55] G.Zhou,V.Dean,M.K.Srirama,A.Rajeswaran,J.Pari,K.B.Hatch,A.Jain,T.Yu,P.Abbeel,
L.Pinto,C.Finn,andA.Gupta. Trainoffline,testonline: Arealrobotlearningbenchmark.
arXiv,2022.
[56] K.Kimble,K.VanWyk,J.Falco,E.Messina,Y.Sun,M.Shibata,W.Uemura,andY.Yokoko-
hji. Benchmarking protocols for evaluating small parts robotic assembly systems. IEEE
RoboticsandAutomationLetters,2020.
[57] W.Lian,T.Kelch,D.Holz,A.Norton,andS.Schaal. Benchmarkingoff-the-shelfsolutionsto
roboticassemblytasks. InIROS,2021.
[58] A.Kadian,J.Truong,A.Gokaslan,A.Clegg,E.Wijmans,S.Lee,M.Savva,S.Chernova,and
D.Batra. Arewemakingrealprogressinsimulatedenvironments? measuringthesim2real
gapinembodiedvisualnavigation. arXiv,2019.
[59] E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu,
A.Gupta,andA.Farhadi. AI2-THOR:aninteractive3denvironmentforvisualAI. arXiv,
2017.
[60] J.Collins,S.Goel,A.Luthra,L.Xu,K.Deng,X.Zhang,T.F.Y.Vicente,H.Arora,T.Diderik-
sen,M.Guillaumin,etal.Abo:Datasetandbenchmarksforreal-world3dobjectunderstanding.
InCVPR,2022.
[61] L.Downs,A.Francis,N.Koenig,B.Kinman,R.Hickman,K.Reymann,T.B.McHugh,and
V.Vanhoucke. Googlescannedobjects: Ahigh-qualitydatasetof3dscannedhouseholditems.
InICRA,2022.
[62] J.Wu, R.Antonova, A.Kan, M.Lepert, A.Zeng, S.Song, J.Bohg, S.Rusinkiewicz, and
T.Funkhouser. Tidybot: Personalizedrobotassistancewithlargelanguagemodels. arXiv,
2023.
12
[63] T.Zhou,R.Tucker,J.Flynn,G.Fyffe,andN.Snavely. Stereomagnification: Learningview
synthesisusingmultiplaneimages. SIGGRAPH,2018.
[64] E.Wijmans,A.Kadian,A.Morcos,S.Lee,I.Essa,D.Parikh,M.Savva,andD.Batra.Dd-ppo:
Learningnear-perfectpointgoalnavigatorsfrom2.5billionframes. InICLR,2019.
[65] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale
hierarchicalimagedatabase. InCVPR,2009.
[66] A.Wang,A.Singh,J.Michael,F.Hill,O.Levy,andS.R.Bowman. GLUE:Amulti-task
benchmarkandanalysisplatformfornaturallanguageunderstanding. InICLR,2019.
[67] Y.Bisk,R.Zellers,R.L.Bras,J.Gao,andY.Choi. Piqa: Reasoningaboutphysicalcommon-
senseinnaturallanguage. InAAAI,2020.
[68] M.Sap,H.Rashkin,D.Chen,R.LeBras,andY.Choi. SocialIQA:Commonsensereasoning
aboutsocialinteractions. InEMNLP,2019.
[69] R.Zellers,A.Holtzman,Y.Bisk,A.Farhadi,andY.Choi. Hellaswag: Canamachinereally
finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for
ComputationalLinguistics,2019.
[70] S.Keisuke,L.B.Ronan,B.Chandra,andC.Yejin. Winogrande: Anadversarialwinograd
schemachallengeatscale. InAAAI,2019.
[71] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick.
Microsoftcoco: Commonobjectsincontext. InECCV,2014.
[72] P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang. Squad: 100,000+questionsformachine
comprehensionoftext. InEMNLP,2016.
[73] L.PintoandA.Gupta. Supersizingself-supervision: Learningtograspfrom50ktriesand700
robothours. InICRA,2016.
[74] S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,andD.Quillen. Learninghand-eyecoordination
forroboticgraspingwithdeeplearningandlarge-scaledatacollection. IJRR,2018.
[75] M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes,B.David,C.Finn,C.Fu,K.Gopalakr-
ishnan, K.Hausman, A.Herzog, D.Ho, J.Hsu, J.Ibarz, B.Ichter, A.Irpan, E.Jang, R.J.
Ruano, K.Jeffrey, S.Jesmonth, N.Joshi, R.Julian, D.Kalashnikov, Y.Kuang, K.-H.Lee,
S.Levine,Y.Lu,L.Luu,C.Parada,P.Pastor,J.Quiambao,K.Rao,J.Rettinghouse,D.Reyes,
P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu,
M.Yan,andA.Zeng. Doasicanandnotasisay: Groundinglanguageinroboticaffordances.
InCoRL,2022.
[76] A.Mandlekar,Y.Zhu,A.Garg,J.Booher,M.Spero,A.Tung,J.Gao,J.Emmons,A.Gupta,
E.Orbay,S.Savarese,andL.Fei-Fei. Roboturk: Acrowdsourcingplatformforroboticskill
learningthroughimitation. InCoRL,2018.
[77] P.Sharma,L.Mohan,L.Pinto,andA.K.Gupta. Multipleinteractionsmadeeasy(mime):
Largescaledemonstrationsdataforimitation. InCoRL,2018.
[78] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg.
Dex-net2.0:Deeplearningtoplanrobustgraspswithsyntheticpointcloudsandanalyticgrasp
metrics. arXiv,2017.
[79] S.Dasari,F.Ebert,S.Tian,S.Nair,B.Bucher,K.Schmeckpeper,S.Singh,S.Levine,and
C.Finn. Robonet: Large-scalemulti-robotlearning. arXiv,2019.
13
[80] A. Gupta, A. Murali, D. P. Gandhi, and L. Pinto. Robot learning in homes: Improving
generalizationandreducingdatasetbias. InNeurIPS,2018.
[81] P.Anderson,Q.Wu,D.Teney,J.Bruce,M.Johnson,N.Sünderhauf,I.D.Reid,S.Gould,and
A.vandenHengel.Vision-and-languagenavigation:Interpretingvisually-groundednavigation
instructionsinrealenvironments. InCVPR,2017.
[82] H. Team. Habitat CVPR challenge, 2019. URL https://aihabitat.org/challenge/
2019/.
[83] F. Xia, W. B. Shen, C. Li, P. Kasimbeg, M. Tchapmi, A. Toshev, R. Martín-Martín, and
S.Savarese. Interactivegibsonbenchmark:Abenchmarkforinteractivenavigationincluttered
environments. IEEERoboticsandAutomationLetters,2020.
[84] C. Chen, U. Jain, C. Schissler, S. V. A. Gari, Z. Al-Halah, V. K. Ithapu, P. Robinson, and
K.Grauman. Soundspaces: Audio-visualnavigationin3denvironments. InECCV,2020.
[85] K. Ehsani, W. Han, A. Herrasti, E. VanderBilt, L. Weihs, E. Kolve, A. Kembhavi, and
R.Mottaghi. Manipulathor: Aframeworkforvisualobjectmanipulation. InCVPR,2021.
[86] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A
benchmarkandevaluationformulti-taskandmetareinforcementlearning. InCoRL,2019.
[87] S.James,Z.Ma,D.R.Arrojo,andA.J.Davison. Rlbench: Therobotlearningbenchmark&
learningenvironment. IEEERoboticsandAutomationLetters,2020.
[88] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge. Room-across-room: Multilingual
vision-and-languagenavigationwithdensespatiotemporalgrounding. InEMNLP,2020.
[89] A.Padmakumar,J.Thomason,A.Shrivastava,P.Lange,A.Narayan-Chen,S.Gella,R.Pi-
ramithu,G.Tur,andD.Hakkani-Tur. TEACh: Task-drivenembodiedagentsthatchat. In
AAAI,2022.
[90] X.Gao,Q.Gao,R.Gong,K.Lin,G.Thattai,andG.S.Sukhatme. Dialfred: Dialogue-enabled
agentsforembodiedinstructionfollowing. IEEERoboticsandAutomationLetters,2022.
[91] A.Szot,K.Yadav,A.Clegg,V.-P.Berges,A.Gokaslan,A.Chang,M.Savva,Z.Kira,and
D.Batra. Habitatrearrangementchallenge. https://aihabitat.org/challenge/2022_
rearrange,2022.
[92] C.M.Kim,M.Danielczuk,I.Huang,andK.Goldberg. Simulationofparallel-jawgrasping
usingincrementalpotentialcontactmodels. InICRA,2022.
[93] D. Hall, B. Talbot, and N. Sünderhauf. The robotic vision challenges. https://
nikosuenderhauf.github.io/roboticvisionchallenges/cvpr2022,2022.
[94] A. Kurenkov, R. Martín-Martín, J. Ichnowski, K. Goldberg, and S. Savarese. Semantic
and geometric modeling with neural message passing in 3d scene graphs for hierarchical
mechanical search. In 2021 IEEE International Conference on Robotics and Automation
(ICRA),pages11227–11233.IEEE,2021.
[95] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suenderhauf. Sayplan:
Groundinglargelanguagemodelsusing3dscenegraphsforscalabletaskplanning. arXiv
preprintarXiv:2307.06135,2023.
[96] J.Chen,G.Li,S.Kumar,B.Ghanem,andF.Yu. Howtonottrainyourdragon: Training-free
embodiedobjectgoalnavigationwithsemanticfrontiers. arXivpreprintarXiv:2305.16925,
2023.
14
[97] M.Sundermeyer,A.Mousavian,R.Triebel,andD.Fox. Contact-graspnet: Efficient6-dof
graspgenerationinclutteredscenes. InICRA,2021.
[98] A.Murali,A.Mousavian,C.Eppner,C.Paxton,andD.Fox. 6-dofgraspingfortarget-driven
objectmanipulationinclutter. InICRA,2020.
[99] H.-S.Fang,C.Wang,M.Gou,andC.Lu. Graspnet-1billion: Alarge-scalebenchmarkfor
generalobjectgrasping. InCVPR,2020.
[100] C.R.Garrett,C.Paxton,T.Lozano-Pérez,L.P.Kaelbling,andD.Fox. Onlinereplanningin
beliefspaceforpartiallyobservabletaskandmotionproblems. InICRA,2020.
[101] A.Mousavian,C.Eppner,andD.Fox. 6-dofgraspnet: Variationalgraspgenerationforobject
manipulation. InICCV,2019.
[102] C.Paxton, C.Xie, T.Hermans, andD.Fox. Predictingstableconfigurationsforsemantic
placementofnovelobjects. InCoRL,2022.
[103] S.Kohlbrecher,J.Meyer,O.vonStryk,andU.Klingauf. Aflexibleandscalableslamsystem
withfull3dmotionestimation. InProc.IEEEInternationalSymposiumonSafety,Security
andRescueRobotics(SSRR).IEEE,November2011.
[104] J. J. Kuffner and S. M. LaValle. Rrt-connect: An efficient approach to single-query path
planning. InICRA,2000.
[105] D.S.Chaplot,D.P.Gandhi,A.Gupta,andR.R.Salakhutdinov. Objectgoalnavigationusing
goal-orientedsemanticexploration. InNeurIPS,2020.
[106] B.Yamauchi. Afrontier-basedapproachforautonomousexploration. InIEEEInternational
SymposiumonComputationalIntelligenceinRoboticsandAutomation,1997.
[107] J.A.Sethian. Fastmarchingmethods. SIAMreview,1999.
[108] D.S.Chaplot,S.Gupta,A.Gupta,andR.Salakhutdinov. Learningtoexploreusingactive
neuralmapping. ICLR,2020.
[109] M.Caron,H.Touvron,I.Misra,H.Jégou,J.Mairal,P.Bojanowski,andA.Joulin. Emerging
propertiesinself-supervisedvisiontransformers. CVPR,2021.
[110] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,
A.C.Berg,W.-Y.Lo,etal. Segmentanything. arXivpreprintarXiv:2304.02643,2023.
[111] K.M.Jatavallabhula,S.Saryazdi,G.Iyer,andL.Paull.gradslam:Automagicallydifferentiable
slam. arXivpreprintarXiv:1910.10672,2019.
[112] Q. Gu, A. Kuwajerwala, S. Morin, K. M. Jatavallabhula, B. Sen, A. Agarwal, C. Rivera,
W.Paul,K.Ellis,R.Chellappa,etal. Conceptgraphs: Open-vocabulary3dscenegraphsfor
perceptionandplanning. arXivpreprintarXiv:2309.16650,2023.
[113] M.Quigley,K.Conley,B.Gerkey,J.Faust,T.Foote,J.Leibs,R.Wheeler,A.Y.Ng,etal. Ros:
anopen-sourcerobotoperatingsystem. InICRAWorkshoponOpenSourceSoftware,2009.
15
Appendix
Table of Contents
A ExtendedRelatedWork 16
B Limitations 17
C Metrics 18
C.1 SimulationSuccessMetrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.2 RealWorldSuccessMetrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
D SimulationDetails 19
D.1 ObjectCategoriesAppearingintheSceneDataset . . . . . . . . . . . . . . . . 19
D.2 EpisodeGenerationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D.3 DiversityinReceptacleInstances . . . . . . . . . . . . . . . . . . . . . . . . . 22
D.4 SceneClutterComplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D.5 Improvedscenevisuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.6 ActionSpaceImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
E HomeRobotImplementationDetails 24
E.1 PoseInformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.2 Low-LevelControlforNavigation. . . . . . . . . . . . . . . . . . . . . . . . . 25
E.3 HeuristicGraspingPolicy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.4 HeuristicPlacementPolicy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
E.5 NavigationPlanning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
E.6 NavigationLimitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
F ReinforcementLearningBaseline 28
F.1 ActionSpace. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
F.2 ObservationSpace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
F.3 TrainingSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
F.4 ConceptFusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
G AdditionalAnalysis 31
G.1 Numberofstepstakenineachstagebydifferentbaselines . . . . . . . . . . . . 32
G.2 PerformanceonSeenvs.UnseenObjectCategories . . . . . . . . . . . . . . . 33
H HardwareSetup 34
H.1 HardwareChoice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
H.2 RobotSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
H.3 VisualizingTheRobot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
H.4 UsingTheStretch:Navigationvs.PositionMode. . . . . . . . . . . . . . . . . 36
A ExtendedRelatedWork
It is difficult to do justice to the rich embodied AI, natural language, computer vision, machine
learning, androboticscommunitiesthathaveaddressedaspectsoftheworkpresentedhere. The
followingextendssomeofthediscussionfromthemainmanuscriptaboutimportantadvancesthat
thecommunityhasmade.
16
Benchmarkshavehelpedthecommunityfocustheireffortsandfairlycomparesystemperformance.
Forexample,theYCBobjects[50]allowedfordirectcomparisonofresultsacrossmanipulatorsand
models. Whilebenchmarksandleaderboardsarecomparativelyrareinrobotics[49,56,33,54,63,
3,39],theyhavebeenhugelyinfluentialinmachinelearning(e.g. ImageNet[65],GLUE[66],and
variouslanguagebenchmarks[67–70],COCO[71],andSQuAD[72]). Inrobotics,competitions
suchasRoboCup@Home[3],theAmazonPickingChallenge[39],andtheNISTtaskboard[56]are
prevalentandinfluentialasanalternative,butgenerallysystemsaren’treproducibleacrossteams.
Datasets. InadditiontotheenvironmentsreferencedinTable1, offlinedatasetsincludingrobot
interactionswithsceneshavebeenusedwidelytotrainmodels. Thesedatasetsaretypicallyobtained
usingrobotsalone(e.g.,[73,74]),byteleoperation(e.g.,[75,76])orhuman-robotdemonstration
(e.g., [77]). Previous works such as [78] aim to collect large-scale datasets while works such as
[79]considerscalingacrossmultipleembodiments. [80]takeastepfurtherbycollectingrobotdata
inunstructuredenvironments. Unliketheseworks,wedonotlimitouruserstoaspecificdataset.
Instead,weprovideasimulatorwithvariousscenesthatcangeneratelarge-scaleconsistentdatafor
training. Also,notethatwetestthemodelsinunseenenvironments,whilemostofthementioned
worksusethesameenvironmentfortrainingandtesting.
Simulation benchmarks. The embodied AI community has provided various benchmarks in
simulationplatformsfortaskssuchasnavigation[1,81–84],objectmanipulation[85,35,86,87],
instruction following [6, 88–90], room rearrangement [28, 91], grasping [92] and SLAM [93].
Whilethesebenchmarksensurereproducibilityandfaircomparisonofdifferentmethods,thereis
alwaysagapbetweensimulationandrealitysinceitisinfeasibletomodelallthedetailsofthereal
worldinsimulation. Ourbenchmark,incontrast,enablesfaircomparisonofdifferentmethodsand
reproducibilityoftheresultsintherealworld. Additionally,previousbenchmarksoftenoperateina
simplifieddiscreteactionspace[20,6],evenforcingthatstructureontherealworld[2].
Robotics benchmarking. Robotics benchmarks must contend with the diversity of hardware,
morphology,andresourcesacrosslabs. Onesolutionissimulation[87,59,35,20,21,83,86,6],
whichcanprovidereproducibleandfairevaluations. However,thesim-to-realgapmeanssimulation
resultsmaynotbeindicativeofprogressintherealworld[2]. Anotherproposedsolutionisrobotic
competitionssuchasRoboCup@Home[3],theAmazonPickingChallenge[39],andtheNISTtask
board[56]. However,participantstypicallyusetheirownhardware,makingitdifficulttoconductfair
comparisonsofthedifferentunderlyingmethods,andmeansresultsarenottransferabletodifferent
labsorsettings. Thisisalsoalargebarriertoentrytothesecompetitions.
Explorationofunseenenvironments. Variouspapershavelookedattheproblemofobjectsearch
indifferenthomeenvironments. Gervetetal.[2]useamixtureofheuristic/model-basedplanning
andreinforcementlearningtoachievestrongresultsinavarietyofreal-worldenvironments;impor-
tantlytheirplanning-basedmethodsperformcompetitivelytothebestlearning-basedmethods,and
muchbetterthanend-to-endreinforcementlearningonrealtasks. Alsopromisingisgraph-based
exploration. Kurenkov et al. [94] propose hierarchical mechanical search, based on a 3d scene
graphrepresentation,forexploringenvironments;althoughunlikeinourOpen-VocabularyMobile
Manipulationtask,theyassumesuchascenegraphexists. Similarly,SayPlan[95]performsasearch
overacomplexscenegraphusingalargelanguagemodel;however,thisapproachalsodoesnotlook
intoiterativelyconstructingthisscenegraphonnewscenes. Whilethereisactiveworkiniteratively
exploringandbuildingscenegraphsandotherhierarchicalrepresentations(e.g.[96]),therearenot
yetstronglyestablishedmethodsinthisspace.
B Limitations
Duetosimulationlimitations,wedon’tphysicallysimulategraspinginthecurrentversionofthe
benchmark, which is why we provide a separate policy for this in the real world. Grasping is a
well-studied problem [97–99], but simulations that train useful real-world grasp systems require
specialconsideration. Wealsocurrentlyconsiderfullnaturallanguagequeriesout-of-scope. Finally,
17
wedidnotevaluatemanymotionplanners(seeSec.E.2),orperformedtask-and-motion-planning
withreplanning,aswouldbeidealforalonghorizontask[100].
C Metrics
We informally defined our scoring metrics in Sec. 3. Here, we provide formal definitions of our
partialsuccessmetrics.
C.1 SimulationSuccessMetrics
Successinsimulationisdefinedperstageas:
• FindObj: Successful if the agent reaches within 0.1m of a viewpoint of the target object on
start_receptacle, and at least 0.1% of the pixels in its camera frame belong to an object
instance.
• Pick: Successful if FindObj succeeded, the agent enables the gripper at an instant where an
objectinstanceisvisibleanditsend-effectorreacheswithin0.8mofatargetobject. Wemagically
snaptheobjecttotheagent’sgripperinsimulation.
• FindRec: SuccessfulifPicksucceeded,andtheagentreacheswithin0.1mofaviewpointofa
goal_receptacle,andatleast0.1%ofthepixelsinitscameraframebelongtotheobjectcontaining
avalidreceptacle.
• Place: Successful if FindRec succeeded, the agent releases the object and subsequently the
object stays in contact with the goal_receptacle with linear and angular velocities below a
thresholdof5e−3m/sand5e−2rad/srespectivelyfor50contiguoussteps. Further,theagentshould
notcollidewiththescenewhileattemptingtoplacetheobject.
Anepisodeisconsideredtohavesucceededifitsucceedsinall4stageswithin1250steps.
Betterpicksuccesscondition. Weplantouseamorerealisticgraspingconditioninsimulation. We
tryreplacingthemagicsnapinsimulationwithastricterconditionthatrequirestheagenttomove
itsarmneartheobjectwithoutcollidingwiththesceneorotherobjects. Additionally,wetesteda
baseline(Figure5)thatperformstop-downgraspsresemblingourreal-worldgraspingpolicy,and
resortingtoside-waysgraspswhentheobjectisfarther. Whilethisbaselinesucceedsinreachingthe
objectstartingfromanobjectviewpoint79%ofthetime,itdoessowithoutcollidingonly47%of
thetime.
C.2 RealWorldSuccessMetrics
Successinrealworldisdefinedperstageas:
• FindObj: Successfuliftheagentreacheswithin1mofthetargetobjectonstart_receptacle
andtheobjectisvisibleintheRGBimagefromthecamera.
• Pick: SuccessfulifFindObjsucceededandtheagentsuccessfullypicksuptheobjectfromthe
start_receptacle.
• FindRec: SuccessfulifPicksucceeded,andtheagentreacheswithin1mofagoal_receptacle,
andthegoal_receptacleisvisibleintheRGBimagefromthecamera.
• Place: SuccessfulifFindRecsucceededandtheagentplacesobjectonagoal_receptacle
andtheobjectsettlesdownonthegoal_receptaclestably.
Giventhatthesceneweuseintherealworldismuchsmallerthantheapartmentsinthesimulation,
weallowtheagenttoactintheenvironmentfor300timesteps. Theepisodeisconsideredtohave
succeededifitsucceedsinall4stages.
18
Figure5: Afewsuccessandfailurecasesforoursimplegraspingpolicyunderthenewgraspsuccess
conditionthatrequirestheagent’sarmtoreachneartheobjectwithoutcolliding. Theagentresortsto
sidewaysgraspswhentheobjectcan’tbereachedviaatop-downgraspthatbendsthegripper. Most
graspingfailuresarebecauseofthecollisionswiththescene.
D SimulationDetails
D.1 ObjectCategoriesAppearingintheSceneDataset
action_figure, android_figure, apple, backpack, baseballbat, basket, basketball,
bath_towel, battery_charger, board_game, book, bottle, bowl, box, bread, bundt_pan,
butter_dish, c-clamp, cake_pan, can, can_opener, candle, candle_holder, candy_bar,
canister, carrying_case, casserole, cellphone, clock, cloth, credit_card, cup,
cushion, dish, doll, dumbbell, egg, electric_kettle, electronic_cable, file_sorter,
folder, fork, gaming_console, glass, hammer, hand_towel, handbag, hard_drive, hat,
helmet, jar, jug, kettle, keychain, knife, ladle, lamp, laptop, laptop_cover,
laptop_stand, lettuce, lunch_box, milk_frother_cup, monitor_stand, mouse_pad,
multiport_hub, newspaper, pan, pen, pencil_case, phone_stand, picture_frame,
pitcher, plant_container, plant_saucer, plate, plunger, pot, potato, ramekin,
remote, salt_and_pepper_shaker, scissors, screwdriver, shoe, soap, soap_dish,
soap_dispenser, spatula, spectacles, spicemill, sponge, spoon, spray_bottle,
squeezer, statue, stuffed_toy, sushi_mat, tape, teapot, tennis_racquet,
tissue_box, toiletry, tomato, toy_airplane, toy_animal, toy_bee, toy_cactus,
toy_construction_set, toy_fire_truck, toy_food, toy_fruits, toy_lamp, toy_pineapple,
toy_rattle, toy_refrigerator, toy_sink, toy_sofa, toy_swing, toy_table, toy_vehicle,
tray, utensil_holder_cup, vase, video_game_cartridge, watch, watering_can,
wine_bottle
In Fig. 7 we show some of the examples of a selection of these categories from the training and
validation/testsplits.
19
cushion
Seen Categories wSi an tg cl he ,ts oe yen swin inst ga ,n tc oe yc pa it ne eg ao pr pie les:
,
Seen Category toyfood,toyfiretruck,toycactus,
11 00
12 box
pict
cur ue
p
f br oa om bke
owl cloc sk ho ge las bs ottl pe an tray stuff se od a tt p o o dy i ms patp otoe lan t as o pter eo gp g brea ad ppl ste at du ue m cb ab ne dl l ml e ult ci ap so tr st o e yrh jou l a ub e n gi am ca tl i to en afi p hg o atu nr tde o t y o svw pe re hl i a pc lyl ae b n kto t nt icl foe cen et lla ti p an h pe or e sn pe ic te enmi nl kil es t
trS U
l ea
lee n
c
eqe cs
t pu
ren
i eet
ce
n
l
I nn
k ae p
t
t
tIa
tl
gonn
e
apt ac
s m
cite n
rna
es c
gn d
bie
d tc o
s
o c an r bas r d a o d sl g ee a bm alle bat
s t
d m f c h fi b c
toa ru
lor a alo
ac
woi
es
v n rv
ml th
,u
ee
h d ee
si
s r p
ls
ce br o,
,em ,
, ,ar a r bl
bnss
g
ta
ap
aecp
c
udt
,ma
s,
ri u
ta
ld ,s
kts ehpt
p es ,
et u
ca,,
ru
o
thl
m am
bf
l
dra oaf
rls
a, ie
uo rmd, l s ly
dd s
nn h
llro
iee ea c
,it
nt
r,a ro
m h bo , g
,py
k
ure,
ce f nb
cd ks
os y a do
ap i
ti r kc
ts
an sx
ko ehh
e pn,
,o ,a,
,
adpn
pi
nls
f, ni
c,
aa o
,c
t
a,s
c m np
br lp
nh
dhe
,t
aie
o
dw
e eal
tcc
p hk yr
rt- --
, , ,
vase Unseen Categories
2
11 00
1
plate
pot lettuce toiletry plant s ca lu otc her salt an pd ep ne cilp p hce aar r s ds e h da r nik v ee er wsp ca ap ne ir ste pr lunger androi rd efi mg ou tr e ce an basket
wine
b to ott yl re att tl oe
y
airp sl pa on ne ge phone
j
s at rand S t u s l s s d ce ao e o ht oi r me tn f a ali ,an ln pg rp ,, t gsg ,l o,tie eo yl dt rc hoy ,ihi a b seyn bon s he ls ai ml, f e ,t nd cra ,v k u keen cti , tr i pid ac ,t st ase nce o s, co u u ey ktc p l oeog era , pyea cbt t efe m too r c r ng i xy ooe g eo , nn rer tc ,s isa ri ca qe a tb rr bs t ul ut o: ae cer cri t, ae,d t tbw zi et tg oo o e l ra e en yy y r- ,, ,
Figure6: Numberofobjectsacrossdifferentsplits,forbothseencategoriesandunseencategories.
Wedivideobjectsbetweencategoriesthatappearintrainingdata–seencategories–andthosethat
donot–unseencategories. ThegoalofOpen-VocabularyMobileManipulationistobeabletofind
andmanipulateobjectsspecifiedbylanguage.
cushion
cup
pan
vase
plate
plantsaucer
Figure 7: Example objects in our object dataset across 6 categories. The cushion, cup, and pan
categoriesareinthetrainsplit,andthevase,plate,andplantsaucerareinthevalidationandtestsets.
20
D.2 EpisodeGenerationDetails
Figure8: Visualizationofthenavigablegeometry(toprow)andtop-downviewsofexamplescenes
fromtheHabitatSyntheticScenesDataset(HSSD)[19]. Weusethecomputednavigableareato
efficientlygeneratealargenumberofepisodesfortheOpen-VocabularyMobileManipulationtask.
Objectplacementpositionsaresampledtobenearnavigableareasofthemap,atoponeofalarge
varietyofdifferentreceptacles,suchthattherobotcanreachthem.
Whengeneratingepisodes,wefindthelargestindoornavigableareaineachscene,andthenfilter
thelistofallreceptaclesfromthisscenethataretoosmallforobjectplacement. Fig.8showsthe
navigableislandsinseveralofourscenes(toprow),andcorrespondingtop-downviewsofeachscene
inthebottomrow. Wethensampleobjectsaccordingtothecurrentsplit(train,validation,ortest).
Werunphysicstoensurethatobjectsareplacedinstablelocations. Thenweselectobjectsrandomly
fromtheappropriateset,asdeterminedbythecurrentsplit.
Figure9: First-personviewfromdifferentprecomputedviewpointsinourepisodedataset. These
viewpointsareusedasgoalsfortrainingnavigationskills,andareusedintheinitializationofthe
placementandgaze/graspingskillsaswell. Thepurplemeshindicatesreceptaclesurface.
Finally, we generate a set of candidate viewpoints, shown in Fig. 9, which represent navigable
locationstowhichtherobotcanmoveforeachreceptacle. Theseareusedfortrainingspecificskills,
suchasnavigationtoreceptacles. Eachviewpointcorrespondstoaparticularstart_receptacle
orgoal_receptacle,andrepresentsanearbylocationwheretherobotcanseethereceptacleandis
within1.5meters. Fig.10givesexamplesofwheretheseviewpointsarecreated.
Navmesh: Weprecomputeanavigablescenegeometryasdonein[20]forfastercollisionchecksof
theagentwiththescene. The“mesh”comprisingthisnavigablegeometryisreferredtoasanavmesh.
21
Figure10: Viewpointscreatedforanobjectduringepisodegeneration. Thegrayareaisthenavigable
region of the scene. Thebig red dot and the black box are the object’s center and bounding box
respectively. Thesurroundingdotsareviewpointcandidates: reddotswererejectedbecausethey
weren’tnavigable,andbluedotswererejectedbecausetheyweretoofarfromtheobject. Thegreen
dotsarethefinalsetofviewpoints.
Figure11: Thevariationininstancesbelongingtothe"table"categoryinourdataset.
Numberofobjects: Thisisdynamicallysetpersceneto1.5-2×thetotalavailablereceptacleareain
m2. Forexample,ifthetotalreceptaclesurfaceareaforasceneis10m2,then15-20objectswillbe
placed. Theexactnumberofobjectswillberandomlyselectedperepisodetobeinthisrange.
Thefullsetofincludedreceptaclesinsimulationis: bathtub, bed, bench, cabinet, chair,
chest_of_drawers, couch, counter, filing_cabinet, hamper, serving cart,
shelves, shoe_rack, sink, stand, stool, table, toilet, trunk, wardrobe, &
washer_dryer.
D.3 DiversityinReceptacleInstances
Theinstanceswithineachreceptaclecategoryexhibitsubstantialvariability. Figure11showsafew
differentreceptaclesfromourdatasetbelongingtothe"table"category.
D.4 SceneClutterComplexity
Ourproceduralplacementoftargetanddistractorobjectscreatesdiverseandinterestingscenarios
thatrequirereasoningoverwhichdirectiontoapproachreceptacles,stableplacementinclutter,open
vocabularyobjectdetectionunderocclusionwhichmakesthetaskquitechallenging. Figure12shows
afewexamplesofcluttersurroundingtargetobjectsinourscenes.
22
Figure12: Afewexamplesofcluttersurroundingthetargetobjectinoursimulationsettings.
Figure 13: Here we present the improvements in scene visuals with Horizon-based Ambient
Occlusion(HBAO)andexpandedPhysics-basedRendering(PBR)materialsupportaddedtothe
Habitatrenderer. Thetoprowshowsimagesfromthedefaultrendererwhereasthebottomrowshows
theimprovedrenderings.
D.5 Improvedscenevisuals
WerewroteandexpandedtheexistingPhysically-BasedRenderingshader(PBR)andaddedHorizon-
basedAmbientOcclusion(HBAO)totheHabitatrenderer,whichledtonotableimprovementsin
viewingqualitywhichwerenecessaryforusingtheHSSD[19]dataset.
• RewrotePBRandImageBasedLighting(IBL)basecalculations.
• Added multi-layer material support covering KHR_materials_clearcoat,
KHR_materials_specular, KHR_materials_ior, and KHR_materials_anisotropy
forbothdirectandindirect(IBL)lighting.
• Addedtangentframesynthesisifprecomputedtangentsarenotprovided.
• AddedHDREnvironmentmapsupportforIBL.
WepresentcomparisonsbetweendefaultHabitatvisualsandimprovedrenderingsinFigure13.
WealsobenchmarktheObjectNavtrainingspeedsofaDDPPO-basedRLagentwithandwithoutthe
improvedrenderingandpresenttheresultsin14. Weseethattheimprovementinscenelightingand
23
Figure14: MinordropinFPSwithimprovedscenerendering: Here,webenchmarkthetraining
speeds (through FPS numbers) of two ObjectNav training runs with and without the HBAO and
PBR-basedimprovedscenevisuals. Weobservethattheimprovedrenderingleadstoaverysmall
dropinFPSfromaround340to330(3%drop).
renderingcomesatthecostofonlya3%dipintrainingFPS(decreasingfromaround340toaround
330).
D.6 ActionSpaceImplementation
Welookattwodifferentchoicesofactionspaceforournavigationagents,eithermakingdiscreteor
continuouspredictionsaboutwheretomovenext. Ourexpectationfrompriorworkmightbethatthe
discreteactionspacewouldbenotablyeasierforagentstoworkwith.
Discrete. Previousbenchmarksoftenoperateinafullydiscreteactionspace[20,6],eveninthereal
world[2]. Weimplementasetofdiscreteactions,withfixedin-placerotationleftandright,and
translationofsteps0.25mforward.
Continuous. Ourcontinuousactionspaceisimplementedasateleportingagent,wheretherobot
needstomovearoundbypredictingalocalwaypoint. Ourrobot’slow-levelcontrollersareexpected
tobeabletogettherobottothislocation,inlieuofsimulatingfullphysicsfortheagent.
Insimulation,thisisimplementedasacheckagainstthenavmesh-weusethenavmeshtodetermine
iftherobotwillgointocollisionwithanyobjectsifmovedtowardsthenewlocation,andmoveitto
theclosestvalidlocationinstead.
E HomeRobotImplementationDetails
Here,wedescribemorespecificsforhowweimplementedtheheuristicpoliciesprovidedasabaseline
toacceleratehomeassistantrobotresearch.
Althoughthereexistsaconsiderablebodyofpriorresearchlookingatlearningspecificgrasping[101,
98,99,97]orplacement[102,17]skills,wefoundthatitwaseasiesttoimplementheuristicpolicies
withlowCPU/GPUrequirementsandhighinterpretability. Otherrecentworkshavesimilarlyused
heuristicgraspingandplacementpoliciestogreataffect(e.g. TidyBot[62]).
Therearethreedifferentrepositorieswithintheopen-sourceHomeRobotlibrary:
• home_robot: SharedcomponentssuchasEnvironmentinterfaces,controllers,detection,
andsegmentationmodules.
• home_robot_sim: SimulationstackwithEnvironmentsbasedonHabitat.
• home_robot_hw: Hardwarestackwithserverprocessesthatrunontherobot,clientAPI
thatrunsontheGPUworkstation,andEnvironmentsbuiltusingtheclientAPI.
24
Mostpoliciesareimplementedinthecorehome_robotlibrary. WithinHomeRobot,wealsodivide
functionality between Agents and Environments, similar to how many reinforcement learning
benchmarksaresetup[20].
• Agentscontainallofthenecessarycodetoexecutepolicies. Weimplementagentsthatuse
amixtureofheuristicpoliciesandpolicieslearnedonourscenedatasetviareinforcement
learning.
• Environments provide common logic; they provide Observations to the Agent and a
functionthatallowsthemtoapplytheiractiontothe(realorsimulated)environment.
E.1 PoseInformation
WegettheglobalrobotposefromHectorSLAM[103]ontheHelloRobotStretch[22],whichis
usedwhencreating2dsemanticmapsforourmodel-basednavigationpolicies.
E.2 Low-LevelControlforNavigation
TheHelloStretchsoftwareprovidesanativeinterfaceforcontrollingthelinearandangularvelocities
of the differential-drive robot base. While we do expose an interface for users to control these
velocitiesdirectly,itisdesirabletohavedesiredshort-termgoalsasamoreintuitiveactionspacefor
policies,andtomakethemupdateableatanyinstanttoallowforreplanning.
Thus,weimplementedavelocitycontrollerthatproducescontinuousvelocitycommandsthatmove
the robot to an input goal pose. The controller operates in a heuristic manner: by rotating the
robotsothatitfacesthegoalpositionatalltimeswhilemovingtowardsthegoalposition,andthen
rotatingtoreachthegoalorientationoncethegoalpositionisreached. Thevelocitiestoinducethese
motionsareinferredwithatrapezoidalvelocityprofileandsomeconditionalcheckstopreventit
fromovershootingthegoal.
Limitations The Fast Marching Method-based motion planning from prior work [2] that we
describeinSec.E.2. Itassumestheagentisacylinder,andthereforeismuchmorelimitedinwhere
itcannavigatethan,e.g.,asampling-basedmotionplannerlikeRRT-connect[104]whichcantake
orientationintoaccount. Inaddition,oursemanticmappingrequiresalistofclassesforusewith
DETIC [27]; instead, it would be good to use a fully open-vocabulary scene representation like
CLIP-Fields [11], ConceptFusion [15], or USA-Net [12], which would also improve our motion
planningsignificantly.
E.3 HeuristicGraspingPolicy
Figure15: Graspingtestsinvariouslabenvironments. Toprovideastrongbaseline,wetunedthe
grasppolicytobehighlyreliablegiventheStretch’sviewpoint,onavarietyofobjects.
Numerouspowerfulgraspgenerationmodelshavebeenproposedintheliterature,suchasGraspNet-
1Billion[99],6-DOFGraspNet[98],andContact-GraspNet[97]. However,fortransparency,repro-
25
Figure16: Anexampleoftherobotnavigatingtoagoal_receptacle(sofa)andusingtheheuristic
placepolicytoputdowntheobject(stuffedanimal). Heuristicpoliciesprovideaninterpretableand
easilyextendedbaseline.
ducibility,andeaseofinstallation,weimplementasimple,heuristicgraspingpolicy,whichassumes
aparallelgripperperformingtop-downgrasps. Heuristicgrasppoliciesappearthroughoutrobotics
research(e.g.inTidyBot[62]).Inourcase,theheuristicpolicyvoxelizesthepointcloud,andchooses
areasatthetopoftheobjectwherepointsexist,surroundedbyfreespace,inordertograsp. Fig.15
showsthesimplegrasppolicyinactionandadditionaldetailsarepresentedinSec.E.3. Thispolicy
workswellonawidevarietyofobjects,andwesawcomparableperformancetothestate-of-the-art
open-sourcegraspingmodelswetested[97,99].
Theintuitionistoidentifyareaswherethegripperfingerscandescendunobstructedintotwosidesof
aphysicalpartoftheobject,whichwedothroughasimplevoxelizationscheme. Wetakethetop
10%ofpointsinanobject,voxelizeatafixedresolutionof0.5cm,andchoosegraspswithfreevoxels
(wherefingerscango)oneithersideofoccupiedvoxels. Inpractice,thisachievedahighsuccessrate
onavarietyofrealobjects.
Theprocedureisasfollows:
1. Givenatargetobjectpointcloud,convertthepointcloudintovoxelsofsize0.5cm.
2. Selecttop10%occupiedvoxelswiththehighestZcoordinates.
3. Projecttheselectedvoxelsintoa2-Dgrid.
4. Considergraspscenteredaroundeachoccupiedvoxel,andidentifythreeregions: twowhere
thegripperfingerswillbeandonerepresentingthespacebetweenthefingers.
5. Scoreeachgraspbasedon1)howoccupiedtheregionbetweenthefingersis,and2)how
emptythetwosurroundingregionsare.
6. Performsmoothingonthegraspscorestorejectoutliers(donebymultiplyingscoreswith
adjacentscores).
7. Outputgraspswithfinalscoresabovesomethreshold.
WecomparedthispolicytoothermethodslikeContactGraspnet[97],6-DoFGraspnet[98,101],and
Graspnet1-Billion[99]. Wesawmoreintermittentfailuresduetosensornoiseusingthesepretrained
methods,evenafteradaptingthegraspoffsetstofittotheHelloRobotStretch’sgrippergeometry. In
theend,weleavetrainingtobettergrasppoliciesforfuturework.
E.4 HeuristicPlacementPolicy
Aswithgrasping,anumberofworksonstableplacementofobjectshavebeenproposedinthelitera-
ture[102,17]. Toprovideareasonablebaseline,weimplementaheuristicplacementstrategythat
assumesthattheend-receptacleisatleastbarelyvisiblewhenittakesover;projectsthesegmentation
maskontothepointcloudandchoosesavoxelonthetopoftheobject. Fig.16showsanexampleof
theplacepolicybeingexecutedintherealworld.
26
Specifically,ourheuristicpolicyisimplementedassuch:
1. Detect the end-receptacle in egocentric RGB observations (using DETIC [27]), project
predictedimagesegmenttoa3Dpointcloudusingdepth,andtransformpointcloudtorobot
basecoordinatesusingcameraheightandtilt.
2. Estimateplacementpoint: Randomlysample50pointsonthepointcloudandchooseone
thatisatthecenterofthebiggest(pointcloud)slabforplacingobjects. Thisisdoneby
scoringeachpointbasedonthenumberofsurroundingpointsintheX/Yplane(Zisup)
withina3cmheightthreshold.
3. Rotaterobotforittobefacingtheplacementpoint,thenmoverobotforwardifitismore
than38.5cmaway(lengthofretractedarm+approximatelengthoftheStretchgripper).
4. Re-estimateplacementpointfromthisnewrobotposition.
5. Accordingly, set arm’s extension and lift values to have the gripper be a few cm above
placementposition. Then,releasetheobjecttolandonthereceptacle.
E.5 NavigationPlanning
Ourheuristicbaselineextendspriorwork[2],whichwasshowntoworkinawiderangeofhuman
environments. We tune it for navigating close to other objects and extended it to work in our
continuous action space – challenging navigation aspects not present in the original paper. The
baselinehasthreecomponents:
Semantic Mapping Module. The semantic map stores relevant objects, explored regions, and
obstacles. Toconstructthemap,wepredictsemanticcategoriesandsegmentationmasksofobjects
fromfirst-personobservations. WeuseDetic[27]forobjectdetectionandinstancesegmentationand
backprojectfirst-personsemanticsegmentationintoapointcloudusingtheperceiveddepth,binit
intoa3Dsemanticvoxelmap,andfinallysumovertheheighttocomputea2Dsemanticmap.
Wekeeptrackofobjectsdetected, obstacles, andexploredareasinanexplicitmetricmapofthe
environmentfrom [105]. Concretely,itisabinaryK xM xM matrixwhereM xM isthemap
sizeandK isthenumberofmapchannels. Eachcellofthisspatialmapcorrespondsto25cm2(5cm
x5cm)inthephysicalworld. MapchannelsK =C+4whereC isthenumberofsemanticobject
categories,andtheremaining4channelsrepresenttheobstacles,theexploredarea,andtheagent’s
currentandpastlocations. Anentryinthemapisoneifthecellcontainsanobjectofaparticular
semanticcategory,anobstacle,orisexplored,andzerootherwise. Themapisinitializedwithall
zerosatthebeginningofanepisodeandtheagentstartsatthecenterofthemapfacingeast.
FrontierExplorationPolicy. Weexploretheenvironmentwithaheuristicfrontier-basedexploration
policy[106]. Thisheuristicselectsasthegoalthepointclosesttotherobotingeodesicdistance
withintheboundarybetweentheexploredandunexploredregionofthemap.
NavigationPlanner. Givenalong-termgoaloutputbythefrontierexplorationpolicy,weusethe
FastMarchingMethod[107]asin[105]toplanapathandthefirstlow-levelactionalongthispath
deterministically. Althoughthesemanticexplorationpolicyactsatacoarsetimescale,theplanner
actsatafinetimescale: everystepweupdatethemapandreplanthepathtothelong-termgoal. The
robotattemptstoplantogoalsiftheyhavebeenseen;ifitcannotgetwithinacertaindistanceofthe
goalobjects,thenitwillinsteadplantoapointonthefrontier.
Navigatingtoobjectsonstart_receptacle. Sincesmallobjects(e.g. action_figure,apple)
canbehardtolocatefromadistance,weleveragethetypicallylargerstart_receptaclegoalsfor
findingobjects. Wemakethefollowingchangestotheoriginalplanningpolicy[108]:
1. Ifobjectandstart_receptacleco-occurinatleastonecellofthesemanticmap,planto
reachtheobject
2. Iftheobjectisnotfoundbutstart_receptacleappearsinthesemanticmapafterexclud-
ingtheregionswithin1moftheagent’spastlocations,plantoreachthestart_receptacle
27
Figure17: Real-worldexamples(alsoseeFig2). Oursystemisabletofindheld-outobjectsinan
unseenenvironmentandnavigatetoreceptaclesinordertoplacethem,allwithnoinformationabout
theworldatall,otherthantherelevantclasses. However,weseethisperformanceishighlydependent
onperceptionperformancefornow;manyreal-worldexamplesalsofailduetonear-misscollisions.
3. Otherwise,plantoreachtheclosestfrontier
Instep2,weexcludetheregionsthattheagenthasbeencloseto,topreventitfromre-visitingalready
visitedinstancesofstart_receptacle.
E.6 NavigationLimitations
Weimplementedanavigationsystemthatwaspreviouslyusedinextensivereal-worldexperiments[2],
butneededtotuneitextensivelyforittogetcloseenoughtoobjectstograspandmanipulatethem.
TheoriginalversionbyGervetetal.[2]wasfocusedonfindingverylargeobjectsfromalimited
setofonlysixclasses. Ourssupportsmanymore,butasaresult,tuningittobothbeabletograsp
objectsandavoidcollisionsinallcasesisdifficult.
ThisispartlybecausetheplannerisadiscreteplannerbasedontheFastMarchingMethod[107],
whichcannottakeorientationintoaccountandreliesona5cmdiscretizationoftheworld. ampling-
basedmotionplannerslikeRRT-Connect[104],orlikethatusedintheTaskandMotionPlanning
literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based
plannersspecificallydesignedforopen-vocabularynavigationplanning,ashasrecentlybeenpro-
posed[12].
OurnavigationpolicyreliesonaccurateposeinformationfromHectorSLAM[103],andunfortunately
doesnothandledynamicobstacles. Italsomodelstherobot’slocationasacylinder;theStretch’s
centerofrotationisslightlyoffsetfromthecenterofthiscylinder,whichisnotcurrentlyaccounted
for. Again,sampling-basedplannersmightbebetterhere.
F ReinforcementLearningBaseline
We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and
PlaceObject.
28
F.1 ActionSpace
F.1.1 NavigationSkills
FindObjectandFindReceptacleare,collectively,navigationskills. Forthesetwoskills,weuse
thediscreteactionspace,asmentionedinSec.D.6. Inourexperiments,wefoundthediscreteaction
spacewasbetteratexplorationandeasiertotrain.
F.1.2 ManipulationSkills
Forourmanipulationskills,weusingacontinuousactionspacetogivetheskillsfinegrainedcontrol.
Intherealworld,low-levelcontrollershavelimitsonthedistancetherobotcanmoveinanyparticular
step. Thus,insimulation,welimitourbaseactionspacebyonlyallowingforwardmotionsbetween
10-25cm,orturningby5-30degreesinasinglestep. Theheadtilt,panandgripper’syaw,rolland
pitchcanbechangedbyatmost0.02-0.1radiansinasinglestep. Thearm’sextensionandliftcanbe
changedbyatmost2-10cminasinglestep. Welearnbyteleportingthebaseandarmtothetarget
locations.
F.2 ObservationSpace
Policieshaveaccesstodepthfromtherobotheadcamera,andsemanticsegmentation,aswellasthe
robot’sposerelativetothestartingpose(fromSLAMintherealworld),camerapose,andtherobot’s
jointstates,includingthegripper. RGBimageisavailabletotheagentbutnotusedduringtraining.
F.3 TrainingSetup
Allskillsaretrainedusingaslackrewardof-0.005perstep,incentivizingcompletionoftaskusing
minimumnumberofsteps. Forfastertraining,welearnourpoliciesusingimageswithareduced
resolutionof160x120(comparedtoStretch’soriginalresolutionof640x480).
F.3.1 NavigationSkills
WetrainFindObjectandFindReceptaclepoliciesfortheagenttoreachacandidateobjector
acandidatetargetreceptaclerespectively. Thetrainingprocedureisthesameforbothskills. We
passintheCLIP[14]embeddingcorrespondingwiththegoalobject,aswellassegmentationmasks
correspondingwiththedetectedtargetobjects. Theagentisspawnedarbitrarily,butatleast3meters
fromthetarget,andmustmoveuntilwithin0.1metersofagoal“viewpoint,”wheretheobjectis
visible.
Inputobservations:Robotheadcameradepth,ground-truthsemanticsegmentationforallreceptacle
categories(receptaclesegmentation),robot’sposerelativetothestartingpose,jointsensorgiving
statesofcameraandarmjoints. Weimplementobject-leveldropoutforthesemanticsegmentation
mask,whereeachobjecthasaprobabilityof0.5ofbeingleftoutofthemask. Inaddition,theinput
observationspaceincludesthefollowing:
• Goalspecification: ForFindObject,wepassintheCLIPembeddingofthetargetobject
and the start receptacle category. For FindReceptacle, we pass in the goal receptacle
category.
• Goal segmentation images: During training, the simulator provides ground truth goal
objectsegmentation;ontherealrobot,thesearepredictedbyDETIC[27].ForFindObject,
wepassintwochannels: oneshowingallinstancesofcandidateobjects,oneshowingall
instancesofcandidatestartreceptacles. ForFindReceptacle,wepassasinglechannel
showingallinstancesofcandidategoalreceptacles. Weimplementasimilarobject-level
dropoutprocedurehereaswedidforthereceptaclesegmentation.
Initialstate: Theagentisspawnedatleast3mawayfromthecandidateobjectorreceptacle. Itstarts
in“navigationmode,”withtherobot’sheadfacingforward.
29
Actions: Thepolicypredictstranslationandrotationwaypoints,aswellasadiscretestopaction.
Successcondition: Theagentshouldcallthediscretestopactionwhenitreacheswithin0.5mofa
goalviewpoint.Theagentshouldbefacingthetarget:theanglebetweentheagent’sheadingdirection
andtherayfromtherobottothecenteroftheclosestcandidateobjectshouldbenomorethan15
degrees.
Reward: Assume at time step t, the geodesic distance to the closest goal is given by d(t), the
anglebetweenagent’sheadingdirectionandtherayfromagenttoclosestgoalisgivenbyθ(t),and
did_collide(t)indicatesiftheactiontheagenttookattimet−1resultedinacollisionattimet. The
trainingrewardisgivenby:
R (t)=α[d(t−1)−d(t)]+β1[d(t)≤D ][θ(t−1)−θ(t)]+γ1[did_collide(t)]
FindX close
withα=1,β =1,γ =0.3andD =3.
close
F.3.2 GazeAtObject
TheGazeAtObjectskillstartsneartheobjectandprovidessomefinalrefinementstepsuntilthe
agentiscloseenoughtocallagraspaction,i.e. itisinarm’slengthoftheobjectandtheobjectis
centeredandvisible. Theagentneedstomoveclosertotheobjectandthenadjustitsheadtiltuntil
thecandidateobjectiscloseandcentered. Itmakespredictionstomoveandrotatethehead,aswell
astocentertheobjectandmakesureit’swithinarm’slengthsothatthediscretegraspingpolicycan
execute.
TheGazeAtObjectskillissupposedtostartofffromlocationsandhelpreachalocationwithin
arm’slengthofacandidateobject. Thisistrainedbyfirstinitializingtheagentsclosetocandidate
startreceptacles. Theagentisthentaskedtoreachclosetotheagentandadjustitsheadtiltsuchthat
thecandidateobjectiscloseandcenteredintheagent’scameraview. Wenextprovidedetailsonthe
trainingsetup.
Inputobservations: Groundtruthsemanticsegmentationofcandidatesobjects,headdepthsensor,
jointsensorgivingallheadandarmjointstates,sensorindicatingiftheagentisholdinganyobject,
clipembeddingforthetargetobjectname.
Initialstate: Therobotagainstartsin“navigationmode,”withitsarmretracted,withthegripper
facingdownwards,andwiththehead/camerafacingthebase,baseatanangleof5degreesofthe
centerobjectandononeofthe“viewpoint”locationspre-computedduringepisodegeneration. The
objectwillthereforebeassumedtobevisible.
Actions:Thispolicypredictsbasetranslationandrotationwaypoints,cameratilt,aswellasadiscrete
“grasp”action.
Successcondition: Thecenterpixelonthecamerashouldcorrespondtoavalidcandidateobjectand
theagent’sbaseshouldbewithin0.8mfromtheobject.
Reward: Wetrainthegazepolicymainlywithadenserewardbasedonthedistancetothegoal.
Specifically,assumingthedistanceoftheend-effectortotheclosestcandidategoalattimetisd(t)
(inmeters), theagentreceivesarewardproportionaltod(t−1)−d(t). Further, whentheagent
reaches0.8m,weprovideanadditionalrewardforincentivizingtheagenttolooktowardtheobject.
Letθ(t)denotetheangle(inradians)betweentherayfromtheagent’scameratotheobjectandthe
camera’snormal. Thentherewardisgivenas:
R (t)=α[d(t−1)−d(t)]+β1[d(t)≤γ]cos(θ(t))
Gaze
withα=2,β =1andγ =0.8inourcase.
The agent receives an additional positive reward of 2 once the episode succeeds and receives a
negativerewardof−0.5forcenteringitscameratowardsthewrongobject.
30
F.3.3 PlaceObject
Finally,therobotmustmoveitsarminordertoplacetheobjectonafreespotintheworld. Inthis
case,itstartsataviewpointnearagoal_receptacle. Itmustmoveuptotheobjectandopenits
gripperinordertoplacetheobjectonthissurface.
Inputobservations: Groundtruthsegmentationofgoalreceptacles,headdepthsensor,jointsensor,
sensorindicatingiftheagentisholdinganyobject,CLIP[14]embeddingforthenameoftheobject
beingheld.
Initialconfiguration: Armretracted,withgripperdownandholdingontoanobject,headfacingthe
base. Theagentisspawnedonaviewpointwithitsbasefacingtheobjectwithanerrorofatmost15
degrees.
Actions: Basetranslationandrotationwaypoints,allarmjoints(armextension,armlift,gripperyaw,
pitch,androll),amanipulationmodeactionthatcanbeinvokedonlyonceinanepisodetoturnthe
agent’sheadtowardsthearmandrotatethebaseleftby90degrees. Theagentisnotallowedtomove
itsbasewhileinmanipulationmode.
Successcondition: Theepisodesucceedsiftheagentreleasestheobjectandtheobjectstaysonthe
receptaclefor50timesteps.
Reward: Theagentreceivesapositivesparserewardof5whenitreleasestheobjectandtheobject
comesincontactwithatargetreceptacle. Additionally,weprovideapositiverewardof1foreach
steptheobjectstaysincontactwiththetargetreceptacle. Itreceivesanegativerewardof−1ifthe
agentreleasestheobjectbuttheobjectdoesnotcomeincontactwiththereceptacle.
F.4 ConceptFusion
Inthemainpaper,weintroducedtwokeyapproachesbasedonend-to-endreinforcementlearning
andaheuristicbaseline. Bothmethodsaredependentonthedetectionresultsgeneratedbyareadily
availableopenvocabularyobjectdetector [27]. Notably,these2Ddetectionmodelsdonotleverage
informationfrompriortimestepstoinformtheirdetectiondecisions.
Inordertoaddresstheselimitations,weexploredtheapplicationofConceptFusion[15],anopen-
set scene representation technique. ConceptFusion harnesses foundation models like CLIP [14],
DINO[109],andotherstoconstruct3Dmapsfrommultipleimages. Forourexperimentation,we
employedtheopen-sourceimplementationofConceptFusion,whichutilizestheSegmentAnything
Model(SAM)[110]forobjectsegmentationinRGBimagesandCLIPforfeatureextractionfrom
eachsegmentationmask. It’simportanttonotethatourexperimentswereconductedinasimulated
environment,obviatingtheneedforGradSLAM[111],aswehadaccesstogroundtruthdepthmaps
andposeinformationtosupportourmapconstructionefforts.
During our initial experimentation, we observed that ConceptFusion demanded significant com-
putational resources and memory, with processing times reaching up to 5 seconds per frame for
mapconstruction. Remarkably,it’sworthnotingthattheauthorsofConceptFusionhaverecently
publishedanewpapertitled"ConceptGraphs: Open-Vocabulary3DSceneGraphsforPerceptionand
Planning,"[112]whichaddressessomeofthecomputationalchallengesweencountered. However,
weleavetheexplorationofConceptGraphsasapotentialavenueforfutureresearch.
G AdditionalAnalysis
Here,weprovidesomeadditionalanalysisofthedifferentskillswetrainedtocompletetheOpen-
VocabularyMobileManipulationtask.
31
Episode start Find object Find receptacle Place object
Pick a box from a stand and place it on a chair.
Pick a toy from a table and place it on a stool.
Pick a multiport hub from a stool and place it on a table.
Figure18: WeshowmultipleexecutionsoftheOpen-VocabularyMobileManipulationtaskina
varietyofsimulatedenvironments.
Nav. Manip. Perception FindObj Gaze FindRec Place Total
Heuristic Heuristic GroundTruth 291.8 - 65.5 8.4 360.5
Heuristic RL GroundTruth 293.5 19.4 64.3 84.4 438.7
RL Heuristic GroundTruth 295.1 - 105.0 7.0 401.6
RL RL GroundTruth 302.4 25.7 112.8 45.9 455.2
Heuristic Heuristic DETIC[27] 335.0 - 29.5 6.7 361.8
Heuristic RL DETIC[27] 330.1 152.0 27.5 68.2 556.5
RL Heuristic DETIC[27] 509.5 - 153.3 7.1 610.4
RL RL DETIC[27] 539.1 101.3 124.4 33.7 634.7
Table5: Thenumberofstepsthattheagenttakesperformingeachoftheskillsfordifferentbaselines.
Notethathereweonlyconsiderthecaseswheretheskillterminates. Thelastcolumngivesthetotal
numberofstepstheagenttakesonaverageforexecutingthefourskills.
G.1 Numberofstepstakenineachstagebydifferentbaselines
Table5showsthenumberofstepstakenbyeachskillinourbaseline. WithDETICperception,we
observedthattheRLskillsexploredlessefficientlythanoursimpleheuristic-basedplanner; this
translatestofarfewerstepstakeninsuccessfulepisodes,althoughbecauseRLexplorationessentially
“givesup”ifanobjectisn’tnearby,itcantakelotsofstepsinmanysituations. Intherealworld,we
sawsimilarbehavior-sometimes,theRLpolicieswouldnotexploreenoughtobeabletofindagoal
atall.
32
Nav. Manip. Perception FindObj Gaze Pick FindRec Place Placeterminates
Heuristic Heuristic GroundTruth 100.0 - 65.1 65.1 62.1 62.1
Heuristic RL GroundTruth 100.0 65.6 64.3 64.3 61.3 52.2
RL Heuristic GroundTruth 100.0 - 76.3 76.2 66.8 66.8
RL RL GroundTruth 100.0 77.0 74.5 74.5 65.1 60.6
Heuristic Heuristic DETIC[27] 100.0 - 34.7 34.7 31.1 31.1
Heuristic RL DETIC[27] 100.0 33.9 27.2 27.2 24.4 17.6
RL Heuristic DETIC[27] 100.0 - 32.9 32.7 24.2 24.2
RL RL DETIC[27] 100.0 34.7 24.9 24.8 18.1 15.3
Table6: Wereportthepercentageoftimeseachskillgetsinvokedforeachofthedifferentbaselines.
Thelastcolumngivesthepercentageoftimestheagentfinishesexecutingallskills.
FindObjSuccess. PickObjSuccess. FindRecSuccess OverallSuccess
Nav. Manip. Perception SC,UI UC,UI All SC,UI UC,UI Total SC,UI UC,UI All SC,UI UC,UI All
Heuristic Heuristic GroundTruth 50.9 53.2 54.1 46.4 47.2 48.5 27.5 30.0 31.5 4.1 5.2 5.1
Heuristic RL GroundTruth 54.9 58.2 56.5 48.6 55.1 51.5 39.0 37.3 42.3 14.5 12.0 13.2
RL Heuristic GroundTruth 67.1 64.1 65.4 55.0 51.0 54.8 44.8 44.8 43.7 6.2 7.8 7.3
RL RL GroundTruth 68.4 65.8 66.6 63.7 57.6 61.1 54.7 49.0 50.9 15.7 14.4 14.8
Heuristic Heuristic DETIC[27] 22.2 21.1 28.7 12.5 10.5 15.2 3.2 3.3 5.3 0.9 0.7 0.4
Heuristic RL DETIC[27] 22.4 22.2 29.4 11.9 11.8 13.2 5.1 3.5 5.8 0.3 1.4 0.5
RL Heuristic DETIC[27] 18.7 23.0 21.9 9.9 11.8 11.5 5.8 5.3 6.0 0.3 0.0 0.6
RL RL DETIC[27] 21.5 20.7 21.7 10.9 11.0 10.2 6.9 6.2 6.2 1.0 0.7 0.4
Table7:Performancebreakdownbyseenandunseencategories,andcomparedtooverallperformance.
Inourbaselines,wereliedheavilyonapretrainedobjectdetectorforgeneralization,sowedon’tsee
adramaticdifferenceinperformancebetweenseenandunseenobjects.
Next,weobservethattheGazeandPlacepolicies,whichweretrainedwithgroundtruthperception,
takesignificantlylongertoterminatewithDETICperception.
Finally,inTable6,welookatthepercentageoftimestheagentattemptseachofthedifferentskills.
WefindthattheRLtrainedFindObjskillterminatesmoreoftenthantheheuristicFindObjskilland
episodesterminatelessfrequentlywithDETICperceptionwhencomparedtoGTperception.
G.2 PerformanceonSeenvs. UnseenObjectCategories
Table7showsresultsbrokendownbyseenvs. unseeninstances,andseenvs. unseencategories.
Specificallywelookatthesetwopoolsofobjectsfromthevalidationset:
• SC,UI:Seencategory,unseeninstance. Anobjectofaclassthatappearedinthetraining
data(e.g.,“cup”),butnotaspecific“cup”thatappearedinthetrainingdata.
• UC,UI:Unseeninstanceofanunseencategory;anobjectofatypethatdidnotappearin
thetrainingdataatall.
Ingeneral,becausewearerelyingonDETICandnottrainingourownsemanticperceptionforthis
baseline,wedonotseealargedifferencebetweenthetwocategoriesofobject.
G.2.1 ExampleDETICpredictions
In Table 5, we observe that the Gaze policy takes a significantly longer time to terminate with
DETIC [27] perception. The gaze policy (see Fig. 19) tries to center the agent on the object of
interestbyallowingtheagenttomoveitsbaseandcameratilt. Forthis,itreliesonDETIC’sability
todetectnovelobjects. Now,wevisualizeDETICsegmentationsofagent’segocentricobservations
byplacingagentatthepointswheretheGazeskillisexpectedtostart: theobject’sviewpoints. We
observethatwhileDETICsucceedsinafewcases,itfailsatconsistentlydetectingtheobjectsinthe
egocentricframe.
33
Figure19: RLGazeskillinaction: Theagentisallowedtomoveitsbaseandchangeitscameratilt
togetclosertoobjectandbringobjectatthecenterofitscameraframe
Human Commercially Manipulation Approximate
Name Mobile Sized Safe Available DOF Cost
BostonDynamicsSpot ✔ ✖ ✖ ✔ 7 $200,000
FrankaEmikaPanda ✖ ✖ ✓ ✔ 7 $30,000
Locobot ✔ ✖ ✔ ✖ 5 $5,000
Fetch ✔ ✔ ✓ ✖ 7 $100,000
HelloRobotStretch ✔ ✔ ✔ ✔ 4 $19,000
StretchwithDexWrist ✔ ✔ ✔ ✔ 6 $25,000
Table8: Notesonplatformselection. WechosetheStretchwithDexWristasagoodcompromise
betweenmanipulation,navigation,andcost,whilebeinghuman-safeandapproximatelyhuman-sized.
H HardwareSetup
Here, we will discuss choices related to the real-world hardware setup in extra detail along with
informationaboutthetoolsthatweuseforthevisualizationontherobot. Thisappendixcontains
notesonhowtosetuptheroboticsstackintherealworld,usefultoolsthatwecontribute,andsome
bestpracticesfordevelopment. Settingupmobilerobotsishard,andoneofthemaingoalsofthe
HomeRobotprojectistomakeitbotheasyandsomewhataffordableforresearchers.
H.1 HardwareChoice
WedescribesomeoptionsforcommerciallyavailableroboticshardwareinTab.8. WhiletheFranka
EmikaPandaisnotamobilerobot,weincludeitherebecauseit’saverycommonlyusedplatformin
bothindustrialresearchlabsandatuniversities,makingitspriceafaircomparisonpointforwhatis
reasonable.
34
Figure20: VisualizationofgroundtruthandDETIC[27]segmentationmasksforagent’segocentric
RGBobservations. NotethatweuseaDETICvocabularyconsistingofthefixedlistofreceptacle
categoriesandtargetobjectname. WeobservedthatDETICoftenfailstoaccuratelydetectallthe
objectspresentinthegivenframe.
H.2 RobotSetup
Onechallengewithlow-costmobilerobotsishowwecanrunGPU-andcompute-intensivemodelsto
evaluatemodernAImethodsonthem. TheStretch,likemanysimilarrobots,doesnothaveonboard
GPU,andwillalwayshavemorelimitedcomputethanisavailableonasimilarworkstation.
AsdescribedinSec.4,weaddressthiswithasimplenetworkconfigurationshowninFig.21. There
arethreecomponents:
1. Thedesktoprunningcode–inourcase,theeval_episode.pyscriptfromHomeRobot–
whichconnectstoaremotemobilemanipulator.
2. The dedicated router – an off-the-shelf consumer router, such as a Netgear Nighthawk
router. Thisshouldideallybededicatedforyourrobotanddesktopsetuptoensuregood
performance.
3. Themobilerobotitself: aStretchwithDexWrist,asdescribedabove.
Aftertherobotisconfigured,thenyoujustneedtorunonescript,aROSlaunchfile,asdescribedin
theHomeRobotstartupinstructions,whichcanbedoneoverSSH.Then,withaproperlyconfigured
robotandrouter,youcanvisualizeinformationonthedesktopside,showingtherobot’sposition,
mapfromSLAM,andcameras. Ontherobotside,theonlynecessarycommandis:
roslaunch home_robot_hw startup_stretch_hector_slam.launch
35
Figure21: NetworksetupdiagramforHomeRobot. WecanrunvisualizationsonaGPU-enabled
workstationwhilerunningonlythenecessarycodeonarobotforlow-levelcontrolandSLAM.
Checkingnetworkperformance. Wedescribethevisualizationtoolsavailablebrieflyinthenext
section,buttocheckthatthesetupisworkingproperly,youcanstartrvizandwaveyourhandin
frontoftherobot–youshouldseeminimallatencywhenwavingahandinfrontofthecamera.
Timingbetweentherobotandtheremoteworkstation. WeuseROS[113]asourcommunication
layer,andtoimplementthelow-levelcontrolontherobot.Thisalsoprovidesnetworkcommunication.
However,duetopotentiallatencybetweentherobotandthedesktop,wealsoneedtomakesurethat
observationsareuptodate.
Wesetuptherobotsuchthatitstopsafterexecutingmostnavigationactions,untilthereisanup
todateimageobservationfromtherobotside. Thismeansthattimesynchronizationbetweenthe
robotandtheworkstationisextremelyimportant;ifwedonothaveup-to-datetiming,wemighthave
SLAMposesanddepthmeasurementsthatdonotmatch,whichwillleadtoworseperformance.
WesolvedthisbyhavingaclockontherobotsidepublishitstimeoverROS,andconfiguringall
systemstousethisROSmasterclockinsteadofsystemtime. Thispreventstheuserfromhavingto
worryaboutLinuxtimesynchronizationprotocolslikeNTPwhensettinguptherobot.
H.3 VisualizingTheRobot
WeuseRVIZ,apartofROS,tovisualizeresultsandprogress. Fig.22showsthreedifferentoutputs
fromoursystem: onthefarleft,animagefromthetestenvironmentbeingprocessedbyDetic;inthe
center,atop-downmapgeneratedbythenavigationplannerdescribedinSec.E.2;andontheright,
animagefromRVIZwiththepointcloudfromtherobot’sheadcameraregisteredagainstthe2D
lidarmapcreatedbyHectorSLAM.
OneadvantageoftheHomeRobotstackisthatitisdesignedtoworkwithexistingdebuggingtools
-especiallyROS[113]. ROSisawidely-usedframeworkforroboticssoftwaredevelopmentthat
comes with a lot of online resources, official support from Hello Robot, and a rich and thriving
open-sourcecommunitywithwideindustrybacking.
H.4 UsingTheStretch: Navigationvs. PositionMode
WeleaveAPIdocumentationtotheHomeRobotcodebase,butwanttonoteoneothercomplexity
whenusingtherobot. Stretch’smanipulatorarmispointedtotherightofitsdirectionofmotion,
36
Figure22: Exploringareal-worldapartmentduringtesting. TherobotusesDetic[27]toperceive
theworldandupdatea2Dmap(center)whichcaptureswhereit’sseenrelevantclasses,andwhich
obstaclesexist;detectionsaren’talwaysreliable,especiallygivenalargeandchangingvocabularyof
objectsthatwecareabout. IntheHomeRobotstack,weprovideavarietyoftoolsforvisualizingand
implementingpolicies,includingintegrationofRVIZ(right).
whichmeansthatitcannotbothlookwhereitisgoingandmanipulateobjectsatonce. Thisallowsthe
robottobelowercostandfitthehumanprofile-moreinformationontherobot’sdesignisavailable
inotherwork[22].
However,it’ssomethingimportanttoconsiderwhentryingtocontrolStretchtoperformvarioustasks.
WeuseStretchinoneoftwomodes:
• Navigationmode: therobot’scameraisfacingforward;weusereactivelow-levelcontrol
fornavigation;therobotcanrotateinplace,rollbackward,andwillreactivelytrackgoals
sentfromthedesktop.
• Manipulationmode: therobot’scameraisfacingtowardsitsarm;wedonotusereactive
low-levelcontrolfornavigationanddonotrotatethebase. Instead,wetreattherobot’sbase
asanextra,lateraldegreeoffreedomformanipulation.
Thisisespeciallyrelevantwhengraspingorplacing;itmeansthat,forourheuristicpolicies,therobot
transitionsintomanipulationmodeaftermovingcloseenoughtothegoal,andmaytrackslightlyto
theleftortheright,inordertoactasifithadafull6dofmanipulator.
Allinall, thesechangesmakethelow-costrobotmorecapableandeasiertouseforavarietyof
tasks[12,24,25].
37

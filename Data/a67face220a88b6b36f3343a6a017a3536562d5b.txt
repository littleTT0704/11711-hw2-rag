An Empirical Study on the Generalization Power of Neural
Representations Learned via Visual Guessing Games
AlessandroSuglia1,YonatanBisk2,IoannisKonstas1,AntonioVergari3,
EmanueleBastianelli1,AndreaVanzo1, and OliverLemon1
1Heriot-WattUniversity,Edinburgh,UK
2CarnegieMellonUniversity,Pittsburgh,USA
3UniversityofCalifornia,LosAngeles,USA
1 as247,i.konstas,e.bastianelli,a.vanzo,o.lemon @hw.ac.uk
{ }
2ybisk@cs.cmu.edu,3aver@cs.ucla.edu
Abstract have a specific goal which represents a clear in-
centiveforlearning. Inaddition,theyrequirethat
Guessinggamesareaprototypicalinstanceof theQuestionermastersbothnaturallanguagegen-
the “learning by interacting” paradigm. This
eration and understanding with a focus on object
work investigates how well an artificial agent
categories and attributes. For humans, concepts
canbenefitfromplayingguessinggameswhen
learnedinthiswayaregenericandgeneralisableto
later asked to perform on novel NLP down-
newtasksanddomainswheregroundedreasoning
stream tasks such as Visual Question An-
swering (VQA). We propose two ways to ex- isimportant(Hampton,1979). However,howwell
ploitplayingguessinggames: 1)asupervised can AI agents generalise with concepts acquired
learning scenario in which the agent learns fromvisualguessinggames?
to mimic successful guessing games and 2)
Theliteraturehasnotexploredifrepresentations
a novel way for an agent to play by itself,
built from self-play are transferable, focusing in-
calledSelf-playviaIteratedExperienceLearn-
steadonlargescaleself-supervisedlearning. For
ing (SPIEL). We evaluate the ability of both
instance,largescaleimagecaptioningdatasetshave
procedures to generalise: an in-domain eval-
uation shows an increased accuracy (+7.79) beenusedtotrainmulti-modalTransformers (Lu
compared with competitors on the evaluation etal.,2019;Lietal.,2019;TanandBansal,2019;
suiteCompGuessWhat?!;atransferevaluation Chen et al., 2019). Multi-task learning (Lu et al.,
showsimprovedperformanceforVQAonthe 2020) has been used to leverage the diversity of
TDIUC dataset in terms of harmonic average
trainingsignalsprovidedcombiningdatasets,but
accuracy(+5.31)thankstomorefine-grained
onlyfordiscriminativetasks. Whilesomedialogue
objectrepresentationslearnedviaSPIEL.
work (Cogswell et al., 2020) aims to bootstrap a
conversing agentfrom VQA datasets, most work
1 Background&RelatedWork
on GuessWhat?! (de Vries et al., 2017; Shekhar
Learningalanguagerequiresinteractingwithboth et al., 2019; Strub et al., 2017) has designed be-
theenvironmentandotheragents(Bisketal.,2020). spoke models for the task, ignoring the utility of
Languagegamesrepresentonecommonexample thisdatasetforotherVision+Languagetasks.
of this (Wittgenstein et al., 1953), as seen by the Weproposeself-playasamechanismforlearn-
importantroleofplayinL1childlanguageacqui- ing general grounded representations. We seed
sition (Hainey et al., 2016) as well as L2 learn- our approach with the GuessWhat?! corpus of
ers(Godwin-Jones,2014). questions and objects, and demonstrate how to
Amongthelanguagegamesdefinedinthelitera- generalise to other downstream tasks. We pro-
ture(Steels,2015),guessinggamesrepresentthe posetwodifferentstrategiestoexploitthesedata.
firststepinacurriculumforlanguagelearning. For First,asupervisedlearningphaseisundertakento
example, in GuessWhat?! (de Vries et al., 2017), learnaQuestionerandOraclemodelabletoplay
twoagentsinteractwitheachother: aQuestioner guessing games. Second, the trained agents can
generatesquestionsaimedatfindingahiddenob- beusedtoplayguessinggamesonimagesrequir-
jectinthesceneandanOracle,awareofthetarget ing only object annotations as supervision. We
object,answersthequestionssupportingtheQues- show that an agent trained on GuessWhat?! dia-
tioner in playing the game. Different from other loguescanuseself-playtoadapttonewandharder
languagegames(Dasetal.,2017),guessinggames tasks. Specifically,weinvestigatemodels’gener-
alisation performance and quality of the learned 2017). Itisbeneficialtojointlylearnthetwotasks
representationsontheCompGuessWhat?! bench- because the representations learned by each task
mark(Sugliaetal.,2020),amoreextensiveevalua- arecomplementary. Inaddition,theybetterencode
tionsuiteforGuessWhat?!. Furthermore,westudy attributes, which favours better generalisation to
howthelearnedrepresentationhelpsolveVQAon unseenobjectcategories(Sugliaetal.,2020).
thedatasetTDIUC(KafleandKanan,2017). We To solve the two specific tasks in a multi-task
showoverallcomparableperformancewithstate- fashion,wedesigntwodifferentheadsontopofthe
of-the-art models and improvements for specific shared encoder Γ: 1) the guesser head, produces
questiontypesthatrequireobjectattributeinforma- a probability distribution over every object o us-
i
tiontobeansweredcorrectly. ingtheencodedrepresentationsh passedthrough
oi
anMLP;2)thegeneratorhead,amulti-modalde-
2 Methodology
coder,alsoimplementedasanMLP,whichpredicts
a probability distribution over the vocabulary V
Our proposed transfer/fine-tuning procedure re-
giventhecontextrepresentationgeneratedbyΓ.
quires a training set of guessing games D from
g
Weincludetwolossesinourmodel: 1)thenega-
which we learn a Questioner Q and an Oracle O
tivelog-likelihoodoftheprobabilityassociatedby
viasupervisedlearning. Givenasetofimages ,
I theguesserheadwiththetargetobjectoˆ(Shekhar
itispossibletousethetrainedmodelsQandO to
et al., 2019); 2) a sequence-to-sequence cross-
runtheself-playprocedurefornepochsobtaining
entropyloss(Sutskeveretal.,2014)forthegener-
themodelQn. Finally,givenadownstreamtaskt
ated question tokens. Unlike previous work that
andanassociateddataset basedonimagesfrom
t
D trainsaseparatemoduletolearntostop(Shekhar
,weuseQn’sparametersasinitialisationforthe
I et al., 2018), we add a special token [STOP] to
trainingprocedureon .
t
D theinputdatasothatitlearnswhentostopmore
Toapplythisprocedure,boththeQuestionerand
efficientlyaspartofthequestiongenerationtask.
theOraclerequireamulti-modalencoderΓableto
Traininganagenttosolvetasksofdifferentcom-
generated-dimensionalrepresentationsforthetex-
plexityandsizeischallenging. Theprocedurepre-
tualtokensh ,fortheobjectsh ,aswellasfusing
t o
sentedin(Shekharetal.,2019)alternatesbetween
thevisualandtextualmodalitiesinarepresentation
tasks, updating the hardest task more often. For
ofthecurrentcontexth . Aftertheself-playproce-
c
this technique, finding the right schedule is cum-
dure,onlytheencoderΓofthemodelQnisusedin
bersome and requires fine-tuning. We rely on a
thefine-tuningprocessonthedownstreamtasktus-
moresystematictrainingprocedurebasedonran-
ingthedataset . Itisimportanttounderlinethat
t
D domdataset-proportionalbatchsamplinginspired
thepresentedself-playproceduredoesnotdepend
by (Sanh et al., 2019). This represents a hard-
on a specific implementation of the multi-modal
parameter sharing multi-task training procedure
encoderΓ. Apossibleimplementationispresented
thatavoidsinterferencebetweentasksandfavours
in Section 2.4 and it is used in the experimental
amorestabletraining,whichmitigatescatastrophic
evaluationofthispaper.
forgetting(French,1999).
2.1 Oracledesign
2.3 Self-PlayviaIteratedExperience
The Oracle task is cast as a Visual Question An-
Learning(SPIEL)
swering (VQA) task conditioned on the image I,
Inspiredbyiteratedlearning (Kirbyetal.,2014),
the current question q and on the target object oˆ.
we design a process by which the Questioner
Wefollowcommonpracticeinvocabulary-based
learnsfromgamespreviouslygeneratedbyother
VQA (Antol et al., 2015) and we treat the prob-
instances of the Questioner agent. We call our
lem as a multi-class classification task over the
training procedure Self-play via Iterated Experi-
classes Yes,No,N/A . Weuseh asinputtoa
c
{ } enceLearning(SPIEL).
multi-layerfeedforwardneuralnetworktoobtaina
InSPIEL,describedinAlgorithm1,weassume
probabilitydistributionoverthelabelset.
accesstoasetofimages andtheboundingboxes
I
2.2 Questionerdesign oftheobjectstherein.1 Ineverygameplay,there
I
O
TheQuestionermustplaytworoles: questiongen-
1Object annotations intended as either gold bounding
erationandtargetobjectprediction(deVriesetal., boxesorpredictedboundingboxesfromanobjectdetector.
P<latexit sha1_base64="N4CUK4mZPnTr4FZn+LQSzjWyVho=">AAACS3icbVDLbtQwFHUGSkt4TWEFbCxGSGUzShAIlhVsWA4S01aaRJHj3MxY9SOyb0pHxuJr2MKX8AF8BzvEAs9jAVOOZOnonHN1r0/dSeEwy34kg2vX927sH9xMb92+c/fe8PD+iTO95TDlRhp7VjMHUmiYokAJZ50FpmoJp/X525V/egHWCaM/4LKDUrG5Fq3gDKNUDR9OjkyV00+0UAwXdesXofJRCc+q4SgbZ2vQqyTfkhHZYlIdJo+KxvBegUYumXOzPOuw9Myi4BJCWvQOOsbP2RxmkWqmwJV+/YdAn0aloa2x8Wmka/XvCc+Uc0tVx+TqUrfrrcT/erXa2Yzt69IL3fUImm8Wt72kaOiqINoICxzlMhLGrYi3U75glnGMNaZpoeEjN0ox3fiC2blil2GWl77odRMDgH6UB18gXKLf2DSEkMY2893urpKT5+P85Th7/2J0/Gbb6wF5TJ6QI5KTV+SYvCMTMiWcfCZfyFfyLfme/Ex+Jb830UGynXlA/sFg7w+OD7KY</latexit> (o1|ho1)P<latexit sha1_base64="B7IkICuSYtrf3XejvjreuUk5Z1o=">AAACS3icbVDLbtQwFHUGCiU8OoUVsLEYIZXNKKlAsKxgw3KQmLbSJIoc52bGqh+RfQMdGYuvYQtfwgfwHewQCzyPBUw5kqWjc87VvT51J4XDLPuRDK5d37txc/9WevvO3XsHw8P7p870lsOUG2nsec0cSKFhigIlnHcWmKolnNUXb1b+2QewThj9HpcdlIrNtWgFZxilavhwcmSqY/qJForhom79IlQ+KuFZNRxl42wNepXkWzIiW0yqw+RR0RjeK9DIJXNulmcdlp5ZFFxCSIveQcf4BZvDLFLNFLjSr/8Q6NOoNLQ1Nj6NdK3+PeGZcm6p6phcXep2vZX4X69WO5uxfVV6obseQfPN4raXFA1dFUQbYYGjXEbCuBXxdsoXzDKOscY0LTR85EYpphtfMDtX7DLM8tIXvW5iANCP8uALhEv0G5uGENLYZr7b3VVyejzOX4yzd89HJ6+3ve6Tx+QJOSI5eUlOyFsyIVPCyWfyhXwl35Lvyc/kV/J7Ex0k25kH5B8M9v4Akb+ymg==</latexit> (o2|ho2)P<latexit sha1_base64="sc2ws89x1guzDxpVWhPl5hTRdeQ=">AAACS3icbVDLbtQwFHUGCiW8prACNhYjpLIZJRWoLCvYsBwkpq00iSLHuZmx6kdk30BHxuJr2MKX8AF8BzvEAs9jAVOOZOnonHN1r0/dSeEwy34kg2vX927c3L+V3r5z99794cGDU2d6y2HKjTT2vGYOpNAwRYESzjsLTNUSzuqLNyv/7ANYJ4x+j8sOSsXmWrSCM4xSNXw0OTTVMf1EC8VwUbd+ESoflfC8Go6ycbYGvUryLRmRLSbVQfK4aAzvFWjkkjk3y7MOS88sCi4hpEXvoGP8gs1hFqlmClzp138I9FlUGtoaG59Gulb/nvBMObdUdUyuLnW73kr8r1ernc3Yviq90F2PoPlmcdtLioauCqKNsMBRLiNh3Ip4O+ULZhnHWGOaFho+cqMU040vmJ0rdhlmeemLXjcxAOhHefAFwiX6jU1DCGlsM9/t7io5PRrnL8fZuxejk9fbXvfJE/KUHJKcHJMT8pZMyJRw8pl8IV/Jt+R78jP5lfzeRAfJduYh+QeDvT+kL7Kk</latexit> (o7|ho7) bowl [SEP]
...
Guesser head Generator head
...
Unified Encoder-Decoder for Vision Language Pretraining (VLP)
[CLS] ... [SEP] is it a cup ? no is it a [MASK] [MASK]
Figure 1: We use the single-stream VLP model as a backbone multi-modal encoder for our task. The visual
features tokens (marked in red) are the FastRCNN features associated with the objects in the image, the history
tokens (marked in blue) and the tokens to be generated (marked in yellow) are given in input to the model. A
Guesser head uses the learned contextual object representations to generate a probability distribution over the
objectsP(o h ),whereastheGeneratorheadisusedtoincrementallypredictthemaskedtokens.
i
|
oi
Algorithm1SPIEL:Self-PlayviaIteratedExperi- Learning phase: the same multi-task learning
enceLearning procedureusedinthesupervisedlearningphaseis
1: procedureSELF PLAY(Q 0,O,I,n) usedtofine-tunetheQuestionerparametersusing
2: D q ←READ GOLD GAMES() the datasets ge and q collected for the current
3: E ←[] (cid:46)Initialisetheexperiencebuffer D D
g epoche. Thisprocedureisrepeatedntimesoruntil
4: fore←1,ndo
5: (cid:46)Interactivephase ahaltingconditionisreached(e.g. earlystopping
6: Q←Qe (cid:46)loadlatestweights basedonvalidationmetric).
7: Ge ←GENERATE GAMES(I)
8: Ge ←PLAY GAMES(Q,O,Ge) See Appendix A.1 for implementation details.
9: APPEND(E g,Ge) AttheendoftheSPIELprocedure,weobtainthe
10: De ←[]
g modelQn whoseparameterscanbereusedinother
11: (cid:46)Transmissionphase
12: fori←0,len(E )do tasks. Particularly,weusetheparametersofQn’s
g
13: g←E g[i] (cid:46)Prioritytothelatestgames sharedencoderΓasinitialisationforthefine-tuning
14: ifIS VALID GAME(g)then
15: APPEND(D ge,g) onthedownstreamtasktusingdataset Dt.
16: ifLEN(D ge)==LEN(D q)thenbreak
17: (cid:46)Learningphase 2.4 Implementation
18: Qe+1 ←TRAIN(Q,D q,D ge)
Weimplementasharedmulti-modalencoderΓus-
ingVLP(Zhouetal.,2020),asingle-streammulti-
isaQuestionerQandanOracleO,initialisedwith
modalTransformerforcaptioningdepictedinFig-
agents Q0 and O, respectively, that were trained
ure 1. During the GuessWhat?! fine-tuning, we
with Supervised Learning using gold successful
extend VLP by including dialogue context in the
dialogues.2 We consider every iteration e of the
inputtogetherwiththefeaturesassociatedwiththe
algorithmasaself-playepoch. Inasingleself-play
objectsintheimage. Welearntwonewsegmentids
epoch,wealternate3phases:
torepresentthequestion/answerexchangesinthe
Interactive phase: the agents play guessing dialogue,asdescribedin(Wolfetal.,2019). The
gameswithnovelcombinationsofimageandtarget question is generated by incrementally replacing
object. Thegenerateddialoguecanbesuccessful [MASK]tokensuntiltheendofsequenceisgener-
if the predicted target object is equal to the tar- ated. See Appendix A.2 for more details. SPIEL
get object. Every played dialogue is stored in an training is run on a set of images from Guess-
I
experiencebuffer . What?! andTDIUCdatasetwithcorrespondingob-
g
E
jectannotations. WemakesurethatGuessWhat?!
Transmission phase: in this phase the datasets
testimagesarenotcontainedin . Thisisnotan
forthemulti-tasklearningprocedurefortheQues- I
issue for TDIUC test images because the down-
tionerarecreated. Thegeneratorheaddataset
Dq streamtaskannotations(QApairs)arenotusedby
isfixedinadvancewhilethedatasetfortheguesser
themodelduringthisphase. Oncethemodelhas
head e iscreatedfrom theexperiencebuffer
Dg Eg been trained with SPIEL, we use the parameters
byselectingtheuniqueandvalid dialogues.
ofthesharedencoderΓasabackboneforaVQA
2TheOracleisfixedduringthislearningprocedure. modelthatisfine-tunedontheTDIUCdataset.
3 ExperimentalEvaluation AttributePred. ZShot Score
Models Acc. A SO AS L ND OD
Toassessthegeneralityofourlearnedrepresenta-
Random 15.8 15.1 0.1 7.8 2.8 16.8 18.6 13.3
tions,weincludetwoevaluationparadigms: 1)in-
DV-SL 41.5 46.8 39.1 48.5 42.7 31.3 28.4 38.5
domainevaluationand2)transferevaluation. We DV-RL 53.5 45.2 38.9 47.2 43.5 43.9 38.7 46.2
evaluateseveralvariantsofourmodel: 1)VLP-SL: GDSE-SL 49.1 59.9 47.6 60.1 48.3 29.8 22.3 43.0
GDSE-CL 59.8 59.5 47.6 59.8 48.1 43.4 29.8 50.1
VLP-basedmodeltrainedonGuessWhat?! dataus-
ingmulti-tasklearning; 2)SPIEL-gs: VLP-SL VLP-SL 59.5 59.2 48.2 59.7 49.3 49.0 45.0 53.5
SPIEL-gs 64.1 61.3 49.6 61.6 51.1 54.9 51.9 57.8
modelfine-tunedwithourSPIELprocedurewhere SPIEL-gm 64.6 60.8 48.3 59.5 51.0 55.3 52.9 57.9
thegeneratorheadusesonlygoldsuccessfulgames
(gs);3)SPIEL-gm: sameas2)butbothsuccess- Table 1: F1 scores for attribute prediction and accura-
fulandfailedgoldgamesareusedbythegenerator ciesforzero-shotevaluationonCompGuessWhat?!.
head. In both SPIEL variants, the guesser head
is trained using failed and successful generated
minimal (0.1). However, when analysed in more
gamesbecauseitisimportantfortheguesserhead
detail,wecanseethattrainingthequestionerwith
to be exposed to both types of signal to learn a
gold successful data only improves attribute pre-
morerobustpolicy. Wedecidedtoinvestigatethe
diction while using mixed data improves overall
two variants SPIEL-gs and SPIEL-gm to get
generalisationinthezero-shotevaluation.
moreinsightsabouttheeffectthatsuccessfuland
failedgameshaveonthegeneratorheadabilityto
3.2 Transferevaluation
produceeffectivedialogues.
Forthetransferevaluation,weusetheVQAdataset
3.1 In-domainevaluation
TDIUC (Kafle and Kanan, 2017). It provides a
We use the CompGuessWhat?! evaluation finer-grainedwaytoassessthequalityoftherep-
suite (Suglia et al., 2020) to assess the ability of resentations learned by our guessing game trans-
the Questioner to play guessing games and learn fer technique in terms of several question types
visuallygroundedrepresentationsintheprocess. It including object categories and their attributes.
complements an evaluation based only on game- Specifically, we were interested in improving on
playaccuracy(deVriesetal.,2017)with2auxil- the following question types: 1) Positional rea-
iarytasks: targetobject1)attribute-predictionex- soning; 2) Counting; 3) Object presence; 4) Util-
pressedintermsofabstractattributes(A),situated- ity/Affordances;5)Attribute;6)Color;and7)Ob-
attributes (SO), abstract+situated attributes (AS), ject recognition. TDIUC is evaluated using the
and location attributes (L); 2) zero-shot game- arithmetic mean accuracy per question type (A-
playwithnear-domainaccuracy(ND)andout-of- MPT), as well as the harmonic mean (H-MPT)
domainaccuracy(OD).Table1showsthecompar- thatbettercapturestheskewedquestion-typedis-
isonwithpreviousstate-of-the-artmodelsonthis tribution. In Table 2, we report a comparison
benchmarksuchasdeVriesetal.(2017)(DV-*) between variants trained on guessing games data
and Shekhar et al. (2019) (GDSE-*). VLP-SL (VLP+SLandSPIEL-*),theoriginalmodelVLP
hasagreateradvantageintermsofrepresentation trained on Conceptual Captions (VLP+CC) and
power compared to previous models. This is re- otherstate-of-the-artmodelsspecificallydesigned
flected in all the tasks of the CompGuessWhat?! for the VQA task such as MUREL (Cadene et al.,
evaluation. Particularly,weseebetterperformance 2019),RAU(NohandHan,2016),NMN(Andreas
evenforthezero-shotgameplay(ND:+5.6, OD: etal.,2016),MCB-*(Fukuietal.,2016). Thefull
+15.2). This is because VLP associates a vector setofresultsisavailableintheAppendix,Table4.
ofprobabilitiesthatrepresentsadistributionover Among them, MUREL achieves the best scores
theVisualGenomeobjectclasseswitheveryobject. acrosstheboard,duetoacustomiterativereason-
This helps VLP to cope with the issue of unseen ing mechanism and a non-linear fusion module.
objectsandhelpsthemodeltogeneralise. Learning However, all our models have a more balanced
toplayiskeytogameplayperformance,leadingto overall performance which results in better har-
anincreaseof+4.4overVLP-SLand+7.9over monic means (H-MPT, +5 points over MUREL).
GDSE-CL. In this setup, the difference between Specifically,thisimprovementisfavouredbyanin-
the versions SPIEL-gs and SPIEL-gm is very creaseinaccuracyontheUtility/Affordancesques-
TDIUC predictions (a) Generated dialogue (b)
Are the contents of the plate edible? is it food? no
GOLD answer yes is it a spoon? no
VLP+CC beer is it a cup? no
VLP+SP+gm yes is it a bowl? yes
left picture? yes
what is the spoon made of? the one on the soup? no
GOLD answer wood the one with the soup in it? yes
VLP+CC plastic
VLP+SP+gm wood
Attribute prediction (c)
what is the water glass made of? Situated attributes Confidence
home 99.83%
GOLD answer glass bowl_used_to_scoop 99.37%
VLP+CC plastic kitchen_utentils 99.53%
bowl_can_be_carried 99.40%
VLP+SP+gm glass center 71.75%
Figure 2: We show the ability of the model to play guessing games with the bowl as target object (highlighted
inred). Giventhegenerateddialogue,weusetheprobingclassifiertrainedforCompGuessWhat?! topredictthe
bowl’sattributes. PredictionsonTDIUCquestionsassociatedwiththecurrentimagearereportedaswell.
t di io cn tioty np ie n( t+ he20 C. o7 m). pA Gs us eh so sWwn hab ty ?!th ae ndat dtr ei pb iu ct te edpr ie n-
MODEL
POSITION COUNT PRESENCE AFFOR.D ATT.R COLOR RECO.G A-MPT H-MPT
Figure2(c),ourmodelslearnbetterrepresentations
RAU 35.3 48.4 94.4 31.656.566.986.1 67.859.0
thancompetitorsspecificallyforabstractattributes
NMN 27.9 49.2 92.5 25.247.754.982.0 62.651.9
amongwhichthereareobjectaffordances. Particu- MCB-A 55.4 51.0 93.6 35.156.768.581.9 67.960.5
larly,wecanseehowitisabletounderstandthat MCB 33.3 50.3 91.8 33.953.256.984.6 65.858.0
MUREL 41.2 61.8 95.8 21.458.274.489.4 71.259.3
certain objects can contain things (e.g. “the one
VLP
with the soup in it?”), that objects have specific
+CC 36.9 55.3 94.7 31.055.467.385.8 68.860.1
functions (e.g. “are the contents of the plate edi- +SL 39.0 57.6 94.8 42.154.369.086.1 70.564.0
ble?”)orthattheyhavespecificproperties(e.g. “a SPIEL-gs40.9 57.5 94.8 36.356.969.286.3 70.463.3
SPIEL-gm40.6 57.0 94.8 39.257.069.486.2 70.964.3
spoonismadeofwood”).
The effectiveness of the proposed fine-tuning
Table2: ResultsforthetransferevaluationonTDIUC.
procedure is confirmed by the improved perfor-
Themodelsaredividedintwocategories:(top)Models
mance across all the question types compared to specifically designed for VQA and (bottom) our VLP-
ourbaselineVLP+CC.ModelssuchasMURELand based implementations. We report only the question
MCB-*equippedwithspecificVQAmoduleshave types that we believe will benefit from the guessing
anadvantageonspecificquestion(e.g.,positional gamesfine-tuningprocedure. Forthefullsetofresults
pleaserefertoAppendix,Table4.
reasoning) compared to VLP that relies only on
BERTself-attentionlayers(Devlinetal.,2019). In
addition,whencomparingthetwoSPIELvariants,
asimilartrendshowedinthein-domainevaluation cedure was able to learn useful and finer-grained
canbeobserved. Particularly,SPIEL-gmbenefits objectrepresentationssuchasobjectaffordances,
frombeingexposedtomorelanguagedatacoming thus demonstrating that learning to guess helps
fromsuccessfulandfailedguessinggames. learningtoground.
The current study showed how we can apply
4 Conclusions
the SPIEL training procedure to a VQA dataset
In this work, we verified that representations such as TDIUC. We believe that this work can
learnedwhileplayingguessinggamescanbetrans- be extended to other datasets because the SPIEL
ferred to other downstream tasks such as VQA. procedure only requires a set of images and as-
Wepresentedtwowaysoflearningfromguessing sociated object bounding boxes. These could be
gamesdatanamelymulti-tasklearningandSPIEL. eithergoldorgeneratedbyatrainedobjectdetector
ModelsusingSPIELperformedbetterbothonin- thereforeclassifyingguessinggamesasaholistic
domainevaluationonCompGuessWhat?! aswell self-trainingprocedureformulti-modaldatasets.
asonthetransfertaskTDIUC.Ourself-playpro-
References Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, pages
JacobAndreas,MarcusRohrbach,TrevorDarrell,and
457–468.
DanKlein.2016. Neuralmodulenetworks. InPro-
ceedingsoftheIEEEconferenceoncomputervision RobertGodwin-Jones.2014. Gamesinlanguagelearn-
andpatternrecognition,pages39–48. ing:Opportunitiesandchallenges. LanguageLearn-
ing&Technology,18(2):9–19.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick, Thomas Hainey, Thomas M Connolly, Elizabeth A
and Devi Parikh. 2015. Vqa: Visual question an- Boyle,AmandaWilson,andAisyaRazak.2016. A
swering. In Proceedings of the IEEE international systematicliteraturereviewofgames-basedlearning
conferenceoncomputervision,pages2425–2433. empirical evidence in primary education. Comput-
ers&Education,102:202–223.
Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob
Andreas, Yoshua Bengio, Joyce Chai, Mirella Lap- James A Hampton. 1979. Polymorphous concepts in
ata, Angeliki Lazaridou, Jonathan May, Aleksandr semantic memory. Journal of verbal learning and
Nisnevich, et al. 2020. Experience grounds lan- verbalbehavior,18(4):441–461.
guage. arXivpreprintarXiv:2004.10151.
AriHoltzman, JanBuys, LiDu, MaxwellForbes, and
Remi Cadene, Hedi Ben-Younes, Matthieu Cord, and Yejin Choi. 2019. The curious case of neural text
Nicolas Thome. 2019. Murel: Multimodal rela- degeneration. arXivpreprintarXiv:1904.09751.
tional reasoning for visual question answering. In
Proceedings of the IEEE Conference on Computer KushalKafleandChristopherKanan.2017. Ananaly-
VisionandPatternRecognition,pages1989–1998. sisofvisualquestionansweringalgorithms. InPro-
ceedings of the IEEE International Conference on
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El ComputerVision,pages1965–1973.
Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and
Jingjing Liu. 2019. Uniter: Learning univer- Simon Kirby, Tom Griffiths, and Kenny Smith. 2014.
sal image-text representations. arXiv preprint Iteratedlearningandtheevolutionoflanguage. Cur-
arXiv:1909.11740. rentopinioninneurobiology,28:108–114.
MichaelCogswell,JiasenLu,RishabhJain,StefanLee, Jason Lee, Kyunghyun Cho, and Douwe Kiela. 2019.
DeviParikh,andDhruvBatra.2020. Dialogwithout Countering language drift via visual grounding. In
dialogdata: Learningvisualdialogagentsfromvqa Proceedings of the 2019 Conference on Empirical
data. arXivpreprintarXiv:2007.12750. Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi guage Processing (EMNLP-IJCNLP), pages 4376–
Singh,DeshrajYadav,Jose´ MFMoura,DeviParikh, 4386.
and Dhruv Batra.2017. Visual dialog. In Proceed-
ings of the IEEE Conference on Computer Vision Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui
andPatternRecognition,pages326–335. Hsieh, and Kai-Wei Chang. 2019. Visualbert: A
simple and performant baseline for vision and lan-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and guage. arXivpreprintarXiv:1908.03557.
Kristina Toutanova. 2019. Bert: Pre-training of
deep bidirectional transformers for language under- Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan
standing. InProceedingsofthe2019Conferenceof Lee. 2019. Vilbert: Pretraining task-agnostic visi-
the North American Chapter of the Association for olinguistic representations for vision-and-language
ComputationalLinguistics: HumanLanguageTech- tasks. In Advances in Neural Information Process-
nologies, Volume1(LongandShortPapers), pages ingSystems,pages13–23.
4171–4186.
JiasenLu,VedanujGoswami,MarcusRohrbach,Devi
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi- Parikh, and Stefan Lee. 2020. 12-in-1: Multi-task
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, vision and language representation learning. In
and Hsiao-Wuen Hon. 2019. Unified language Proceedings of the IEEE/CVF Conference on Com-
modelpre-trainingfornaturallanguageunderstand- puterVisionandPatternRecognition,pages10437–
ingandgeneration. InAdvancesinNeuralInforma- 10446.
tionProcessingSystems,pages13063–13075.
HyeonwooNohandBohyungHan.2016. Trainingre-
RobertMFrench.1999. Catastrophicforgettingincon- currentansweringunitswithjointlossminimization
nectionist networks. Trends in cognitive sciences, forvqa. arXivpreprintarXiv:1606.03647.
3(4):128–135.
VictorSanh,ThomasWolf,andSebastianRuder.2019.
Akira Fukui, Dong Huk Park, Daylen Yang, Anna A hierarchical multi-task approach for learning em-
Rohrbach, Trevor Darrell, and Marcus Rohrbach. beddings from semantic tasks. In Proceedings of
2016. Multimodal compact bilinear pooling for vi- the AAAI Conference on Artificial Intelligence, vol-
sual question answering and visual grounding. In ume33,pages6949–6956.
Tom Schaul, John Quan, Ioannis Antonoglou, and multi-modal dialogue. In 2017 IEEE Conference
David Silver. 2015. Prioritized experience replay. onComputerVisionandPatternRecognition,CVPR
arXivpreprintarXiv:1511.05952. 2017, Honolulu, HI, USA, July 21-26, 2017, pages
4466–4475.IEEEComputerSociety.
Piyush Sharma, Nan Ding, Sebastian Goodman, and
Radu Soricut. 2018. Conceptual captions: A Ludwig Wittgenstein, Gertrude Elizabeth Margaret
cleaned, hypernymed, image alt-text dataset for au- Anscombe, and Rush Rhees. 1953. Philosophische
tomatic image captioning. In Proceedings of the Untersuchungen.(Philosophicalinvestigations.
56thAnnualMeetingoftheAssociationforCompu-
tationalLinguistics(Volume1: LongPapers),pages Thomas Wolf, Victor Sanh, Julien Chaumond, and
2556–2565. Clement Delangue. 2019. Transfertransfo: A
transfer learning approach for neural network
Ravi Shekhar, Tim Baumga¨rtner, Aashish Venkatesh, based conversational agents. arXiv preprint
Elia Bruni, Raffaella Bernardi, and Raquel arXiv:1901.08149.
Ferna´ndez. 2018. Ask no more: Deciding when
to guess in referential visual dialogue. In Pro- Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
ceedings of the 27th International Conference on Le, Mohammad Norouzi, Wolfgang Macherey,
ComputationalLinguistics,pages1218–1233. Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
Ravi Shekhar, Aashish Venkatesh, Tim Baumga¨rtner, translation system: Bridging the gap between hu-
Elia Bruni, Barbara Plank, Raffaella Bernardi, and man and machine translation. arXiv preprint
Raquel Ferna´ndez. 2019. Beyond task success: A arXiv:1609.08144.
closerlookatjointlylearningtosee,ask,andguess-
what. In Proceedings of the 2019 Conference of Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong
the North American Chapter of the Association for Hu, Jason J Corso, and Jianfeng Gao. 2020. Uni-
ComputationalLinguistics: HumanLanguageTech- fiedvision-languagepre-trainingforimagecaption-
nologies, Volume1(LongandShortPapers), pages ingandvqa. InAAAI,pages13041–13049.
2578–2587.
Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and
LucSteels.2015. TheTalkingHeadsexperiment: Ori- Anton Van Den Hengel. 2018. Parallel attention:
gins of words and meanings, volume 1. Language A unified framework for visual object discovery
SciencePress. through dialogs and queries. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Florian Strub, Harm De Vries, Jeremie Mary, Bilal Recognition,pages4252–4261.
Piot, Aaron Courvile, and Olivier Pietquin. 2017.
End-to-endoptimizationofgoal-drivenandvisually A Appendices
grounded dialogue systems. In Proceedings of the
26thInternationalJointConferenceonArtificialIn- A.1 Self-PlayviaIteratedExperience
telligence,pages2765–2771.
Learning(SPIEL)
Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Learningtoreplicategolddialoguesisnotenough
Emanuele Bastianelli, Desmond Elliott, Stella
to play successfully. High performance in game-
Frank, and Oliver Lemon. 2020. CompGuess-
play can be achieved only when the agents start
What?!: A multi-task evaluation framework for
grounded language learning. In Proceedings of the playingthegameandareexposedtotheirownmis-
58thAnnualMeetingoftheAssociationforCompu- takes. ReinforcementLearning(Strubetal.,2017)
tational Linguistics, pages 7625–7641, Online. As- or Collaborative Learning (Shekhar et al., 2019)
sociationforComputationalLinguistics.
arepossibleapproachestotacklethisproblem.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Inspiredbyiteratedlearning (Kirbyetal.,2014),
Sequencetosequencelearningwithneuralnetworks. wedesignaprocessbywhich“thegameplayarises
In Advances in neural information processing sys-
inoneinstanceofthequestionerthroughinduction
tems,pages3104–3112.
onthebasisofobservationsofgameplayinother
Hao Tan and Mohit Bansal. 2019. Lxmert: Learning questioneragentswhoacquiredthatgameplayca-
cross-modality encoder representations from trans- pabilityinthesameway”. Therefore,wecallour
formers. InProceedingsofthe2019Conferenceon
procedureSelf-playviaIteratedExperienceLearn-
EmpiricalMethodsinNaturalLanguageProcessing
ing(SPIEL).
andthe9thInternationalJointConferenceonNatu-
ralLanguageProcessing(EMNLP-IJCNLP),pages Inthissetup,weassumewehaveaccesstoaset
5103–5114. ofimages andforeachimageI wehaveobject
I
bounding boxes . The SP training procedure,
HarmdeVries,FlorianStrub,SarathChandar,Olivier OI
showed in Figure 1, can be described as follows.
Pietquin, HugoLarochelle, andAaronC.Courville.
2017. Guesswhat?! visualobjectdiscoverythrough WeassumethatthereisaQuestioneragentQand
an Oracle agent O. At the beginning of the pro- generated until epoch e. We consider a dialogue
cedure they are initialised with agents Q0 and O, unique if e does not contain another dialogue
g
D
respectively,trainedwithSupervisedLearningus- withthesameencoding4. Inaddition,weconsider
inggoldsuccessfuldialogues3. Weconsiderevery adialoguevalidifitdoesnotcontainrepeatedques-
iterationeofthealgorithmasaself-playepoch. In tions. We cap the number of dialogues in e so
g
D
asingleself-playepochwealternate3phases: 1) that it matches the number of experiences in .
q
D
interactivephase: theagentsplayguessinggames Thisisdonesothatduringthemulti-tasktraining
with novel combinations of image and target ob- procedure there is an equal number of dialogues
ject;2)transmissionphase: thequestionercreates foreachtaskfromwhichtheagentwilllearn.
newdatasetsfromthedialoguesgeneratedoverthe
epochs; 3) learning phase: multi-task learning is A.1.3 Learningphase
usedtofine-tunetheQuestionerparametersusing In this phase, we use the same multi-task train-
thedatasetscollectedforthecurrentepoch. ingprocedurethatwasusedduringthesupervised
learningphase. WeupdatetheQuestionerparam-
A.1.1 Interactivephase
eters using the dialogues collected in and e.
q g
Westarttheinteractivephasebyfirstsamplingaset D D
The updated parameters resulting from this step
ofreferencegames ewhichconsistsofpairs(I,oˆ)
willbeusedfortheself-playepoche+1.
G
where I and oˆ is the target object sampled
∈ I
at random from the object annotations I. The A.2 VLPimplementation
O
agentsQeandOplaythegames eandaccumulate
G A.2.1 Multi-modalencoder
thegeneratedexperiences. Duringthisphase,the
To implement the agents in our guessing games,
questioneragentisusingthemostupdatedweights
we rely on VLP, a single-stream multi-modal
generatedatepoche 1. Itgeneratesquestionsby
− model (Zhou et al., 2020) that jointly learns vi-
nucleussampling(Holtzmanetal.,2019)fromthe
sualandlanguagerepresentationsusingConceptual
probabilitydistributionoverthevocabularylearned
by the generator head. When the [STOP] token Captions(CC)dataset(Sharmaetal.,2018). The
input starts with a classification token ([CLS]),
is sampled, the guesser head, conditioned on the
dialoguegeneratedsofar,selectstheobjecto˜with
followedbyaseriesofK visualtokens,asepara-
thehighestprobability. Agameissuccessfulifthe
tiontoken([SEP])dividesthedialoguesequence
from the visual and from the sequence of tokens
predictedobjecto˜isequaltothetargetobjectoˆ.
to be generated. In a guessing game, we repre-
A.1.2 Transmissionphase
sent the reference image I as a set of image re-
For every epoch e, in the transmission phase, we gionsextractedfromanoff-the-shelfobjectdetec-
createthedatasets qand gforthequestionerand tor r 1,r 2,...,r
K
. Following(Zhouetal.,2020),
D D { }
guesser heads, respectively, used in the learning eachregionr isrepresentedbylineartransforma-
i
phaseforthequestionerparametersupdate. tionofafeaturevectorf Rdn,regionclassprob-
∈
Questioner experience buffer To make sure abilitiesc Rdc andregiongeometricinformation
∈
thatthequestionerdoesnotexperiencelanguage g Rdo whered
o
= 5consistsoffourvaluesfor
∈
drift(Leeetal.,2019),weconsiderafixeddataset topleftandbottomrightcornercoordinatesofthe
composed of dialogues generated by humans regionboundingbox(normalizedbetween0and1)
q
D
containedintheGuessWhat?! trainingdata. The andonevalueforitsrelativearea(i.e.,ratioofthe
shared encoder Γ benefits from this data too be- boundingboxareatotheimagearea,alsobetween
cause it is still exposed to human generated lan- 0and1). TheQuestionermodelsusesatmost36
guage,whichguaranteesbettergeneralisation. predictedboundingboxesfromFastRCNNwhile
Guesser experience buffer The Guesser should theGuesserisusingfeaturesgeneratedbyFastR-
learnfromitsownmistakes–thereforeweusegen- CNNforgoldboundingboxes. Weuseaspecific
erated dialogues for the model updates (de Vries segmentids foreveryregion.
v
etal.,2017;Shekharetal.,2019). InspiredbyPri- Forthelanguagepart,weuseWordpieceembed-
oritised Experience Replay (Schaul et al., 2015), dings (Wu et al., 2016). In particular, we flatten
wecreatetheexperiencebufferfortheguesser e theturnsofthedialoguecontextasasequenceof
g
E
byaccumulatingalltheuniqueandvalid dialogues
4TheencodingofadialogueistheSHA-256hashassoci-
3TheOracleisfixedduringthislearningprocedure. atedwithitssequenceoftokens.
tokens. However,toallowthemodeltodifferenti-
atebetweenquestionandanswertokens,following
(Wolfetal.,2019), werelyonnovelsegmentids
(s ,s ). The VLP’s hidden state of the [CLS]
u a
tokenisusedascontextrepresentationh .
c
A.2.2 Oracledesign
The implementation of the Oracle follows the
one presented in the original VLP paper to solve
the VQA task (Zhou et al., 2020). Particularly,
themodelpredictsaprobabilitydistributionover
the possible answers by using a multi-layer feed-
forward neural network that receives in input the
element-wiseproductbetweenthehiddenstateas-
sociatedwiththe[CLS]tokenandthehiddenstate
associated with target object. The model is opti-
misedbyminimisingthecross-entropylossusing
astrainingdatasetthequestion/answerpairsinthe
successfulGuessWhat?! trainingdialogues.
Model Accuracy
A.2.3 Questionerdesign
Human 90.80%
We rely on the VLP ability to generate captions Random 17.10%
forthequestiongenerationtask. Inparticular,we LSTM 61.30%
HRED 61%
provideininputtothemodel: 1)predictedFastR-
LSTM+VGG 60.50%
CNNvisualfeaturesfollowing(Zhouetal.,2020); HRED+VGG 60.40%
2)dialoguegeneratedsofarasaflattenedsequence
ParallelAttention 63.40%
of tokens; 3) question to be generated. We use GDSE-SL 62.96%
GDSE-CL 59.79%
anothersegmentids toallowthemodeltodiffer-
q
entiatewhatistheinputandwhicharethetokens VILBERT 65.69%
VLP-SL 69.30%
tobegenerated. Following(Dongetal.,2019),we
SPIEL-gs 71.80%
makesurethattheattentionmaskfortokensofthe SPIEL-gm 71.70%
question to be generated are masked so that the
Table3: Resultsfortheguesseraccuracyevaluationon
tokenattimesteptisnotallowedtoattendtothe
golddialogues.
future tokens (seq2seq attention mask). For this
specificmodel,weusethemaskedlanguagemod-
elling objective (Devlin et al., 2019) casting the
taskasmulti-modalmaskedlanguagemodelling.
A.3 GuessWhat?! evaluation
Oracle evaluation We report the test accuracy
fortheOracleof82.22%. Thebaselinemodelused
byalltheotheris78.5%(deVriesetal.,2017).
Guesser evaluation We report in Table 3 the
accuracy of the guesser in predicting the tar-
get object when gold dialogues are given in
input. We compare this model with several
baselines reported in (de Vries et al., 2017)
(first block), more sophisticated methods such
asParallelAttention(Zhuangetal.,2018)
andGDSE-*(Shekharetal.,2019)(secondblock)
aswellasotherTransformer-basedmodelssuchas
VILBERT(Luetal.,2020)(thirdblock).
MODEL
MUREL 41.19 61.78 95.75 21.43 58.19 74.43 89.41 96.11 99.80 60.65 63.83 96.20 88.20 71.20 59.30
RAU 35.26 48.43 94.38 31.58 56.49 66.86 86.11 93.96 96.08 60.09 51.60 93.47 84.26 67.81 59.00
NMN 27.92 49.21 92.50 25.15 47.66 54.91 82.02 91.88 87.51 58.02 44.26 89.99 79.56 62.59 51.87
MCB-A 55.40 51.01 93.64 35.09 56.72 68.54 85.54 93.06 84.82 66.25 52.35 92.77 81.86 67.90 60.47
MCB 33.34 50.29 91.84 33.92 53.24 56.93 84.63 92.04 83.44 65.46 51.42 92.47 79.20 65.75 58.03
VLP-CC 36.93 55.28 94.65 30.99 55.42 67.33 85.76 92.98 98.34 62.62 51.34 94.11 85.60 68.81 60.14
VLP-SL 39.04 57.61 94.79 42.11 54.29 69.01 86.07 93.39 97.54 65.77 52.39 94.34 85.98 70.53 63.95
SPIEL-gs 40.94 57.53 94.76 36.26 56.87 69.2 86.33 93.97 97.48 62.3 54.44 94.62 86.1 70.39 63.34
SPIEL-gm 40.6 57.01 94.77 39.18 56.97 69.42 86.21 93.72 97.19 66.09 55.29 94.18 86 70.89 64.31
Table4: SummaryofresultsforthetransferevaluationonTDIUC.Themodelsaredividedintwocategories: (1)
ModelswhicharespecificallydesignedforVQA(top)and(2)modelsthatrelyontheVLPencodertogeneralise
to different downstream tasks (bottom). We underline the question types that we believe will benefit from the
guessinggamestransfer/fine-tuningprocedure.
LANOITISOP GNITNUOC ECNESERP
SECNADROFFA
ETUBIRTTA
ROLOC
NOITINGOCER
ENECS DRUSBA
TNEMITNES YTIVITCA
TROPS
YCARUCCA
TPM-A TPM-H

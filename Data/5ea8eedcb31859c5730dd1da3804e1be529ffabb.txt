A Gold Standard Dataset for the Reviewer Assignment Problem
Ivan Stelmakh, John Wieting, Graham Neubig, Nihar B. Shah∗
Abstract
Many peer-review venues are either using or looking to use algorithms to assign submissions to
reviewers. The crux of such automated approaches is the notion of the “similarity score”—a numerical
estimate of the expertise of a reviewer in reviewing a paper—and many algorithms have been proposed
tocomputethesescores. However,thesealgorithmshavenotbeensubjectedtoaprincipledcomparison,
making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key
challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly
available gold-standard data that would be needed to perform reproducible research. We address this
challenge by collecting a novel dataset of similarity scores that we release to the research community.
Ourdatasetconsistsof477self-reportedexpertisescoresprovidedby58researcherswhoevaluatedtheir
expertise in reviewing papers they have read previously.
We use this data to compare several popular algorithms currently employed in computer science
conferences and come up with recommendations for stakeholders. Our three main findings are:
• All algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of
theirrelevanceforareviewer,theerrorratesrangefrom12%-30%ineasycasesto36%-43%inhard
cases,therebyhighlightingthevitalneedformoreresearchonthesimilarity-computationproblem.
• Mostexistingalgorithmsaredesignedtoworkwithtitlesandabstractsofpapers,andinthisregime
the Specter+MFR algorithm performs best.
• To improve performance, it may be important to develop modern deep-learning based algorithms
that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full
texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of
this information.
Weencourageresearcherstousethisdatasetfordevelopingandevaluatingbettersimilarity-computation
algorithms.
1 Introduction
Assigning papers to reviewers with appropriate expertise is the key prerequisite for high-quality reviews
(ThurnerandHanel,2011;Blacketal.,1998;BianchiandSquazzoni,2015). Evenasmallfractionofincorrect
reviews can negatively impact the quality of the published scientific standard (Thurner and Hanel, 2011)
and hurt careers of researchers (Merton, 1968; Squazzoni and Gandelli, 2012; Thorngate and Chowdhury,
2014). Quoting Triggle and Triggle (2007):
“An incompetent review may lead to the rejection of the submitted paper, or of the grant applica-
tion, and the ultimate failure of the career of the author.”
Conventionally, the selection of reviewers for a submission was the task of a journal editor or a program
committee of a conference. However, the rapid growth in the number of submissions to many publication
venues (Shah, 2022) has made the manual selection of reviewers extremely challenging. As a result, many
peer-reviewvenuesincomputerscienceaswellasotherfieldsareeitherusingorlookingtousealgorithmsto
∗Correspondingauthor: nihars@cs.cmu.edu
1
3202
raM
32
]RI.sc[
1v05761.3032:viXra
assist organizers in assigning submissions to reviewers (Garg et al., 2010; Charlin and Zemel, 2013; Kobren
et al., 2019; Kerzendorf et al., 2020; Stelmakh et al., 2021; OpenReview, 2022).
Thekeycomponentofexistingautomatedapproachesforassigningreviewerstosubmissionsisthe“sim-
ilarity score”. Foranyreviewer-submissionpair, thesimilarityscoreisanumberthatcapturestheexpertise
of the reviewer in reviewing the submission. The assignment process involves first computing the similarity
score for each submission-reviewer pair, and then using these scores to assign the reviewers to submis-
sions (Shah, 2022, Section 3). The similarity scores are typically computed by matching the text of the
submission with the profile (e.g., past papers) of the reviewer, which is the primary focus of this paper.
Additionally, these scores may also be augmented by manually selected subject areas or reviewer-provided
manual preferences (“bids”).
Several algorithms for computing similarity scores have been already proposed and used in practice (we
review these algorithms in Sections 2 and 5). However, there are two key challenges in designing and using
such algorithms in a principled manner. First, there is no publicly available gold standard data that can be
used for algorithm development. Second, despite the existence of many similarity-computation algorithms,
the absence of the gold standard data prevents principled comparison of these algorithms. As a result,
three flagship machine learning conferences—ICML, NeurIPS, and ACL—rely on three different similarity-
computation algorithms, and the differences in performance of these algorithms are not well understood.
In this work, we address the aforementioned challenges and collect a dataset of reviewers’ expertise that
can facilitate the progress in the reviewer assignment problem. Specifically, weconductasurveyofcomputer
science researchers whose experience level ranges from graduate students to senior professors. In the survey,
we ask participants to report their expertise in reviewing computer science papers they read over the last
year.
Contributions Overall, our contributions are threefold:
• First, we collect and release a high-quality dataset of reviewers’ expertise that can be used for training
and/or evaluation of similarity-computation algorithms. The dataset can be found on the project’s
GitHub page:
https://github.com/niharshah/goldstandard-reviewer-paper-match
• Second, we use the collected dataset to compare existing similarity-computation algorithms and inform
organizersinmakingaprincipledchoicefortheirvenue. Specifically,weobservethatwhenallalgorithms
operatewithtitlesandabstractsofpapers,themostadvancedSpecter+MFRalgorithmperformsbest.
However, when the much simpler TPMS is additionally provided with full texts of papers, it achieves
the same level of performance as Specter+MFR.
• Third and finally, we conduct an exploratory analysis that highlights areas of improvement for existing
algorithms and our insights can be used to develop better algorithms to improve peer review. For
example, we believe that animportant direction is todevelopdeep-learning algorithms thatemployfull
texts of papers to improve performance.
Overall,weobservethatallcurrentalgorithmsexhibitnon-trivialamountsoferror. Whentaskedtocompare
a pair of papers in terms of the expertise of a given reviewer, all algorithms make 12%-30% mistakes even
when two papers are selected to have a large difference in reviewer’s expertise. On a more challenging task
of comparing two high-expertise papers, the algorithms err with probability 36%-43%. This observation
underscores the vital need to design better algorithms for matching reviewers to papers.
Let us now make two important remarks. First, while our dataset comprises researchers and papers
from computer science and closely-related fields, other communities may also use it to evaluate existing or
developnewsimilarity-computationalgorithms. Toevaluateanalgorithmfromanotherdomainonourdata,
researchers can fine-tune their algorithm on profiles of computer science scientists crawled from Semantic
Scholar and then evaluate it on our dataset.
Second, inthisworkwe releasean initial version ofthedataset consistingof477 datapoints contributed
by 58 researchers. Thus, our dataset is not set in stone and we encourage researchers to participate in our
2
survey and contribute their data to the dataset. By collecting more samples, we enable more fine-grained
comparisons and also improve the diversity of the dataset in terms of both population of participants and
subject areas of papers. The survey is available at:
https://forms.gle/SP1Rh8eivGz54xR37
and we will be updating the released version regularly.
2 Related literature
Inthissection,wediscussrelevantpaststudies. Webeginwithanoverviewofworksthatreportcomparisons
of different similarity-computation algorithms. We then provide a brief discussion of the commonly used
procedure for computing similarity scores, which are ultimately utilized to assign reviewers to submissions.
Finally, we conclude with a list of works that design algorithms to automate other aspects of reviewer
assignment.
Evaluation of similarity-computation algorithms The OpenReview platform (OpenReview, 2022)
uses an approach of predicting authorship as a proxy to measuring the quality of the similarity scores
computed by any algorithm. Specifically, they consider papers authored by a number of researchers, remove
one of these papers from the corpus, and predict expertise of each researcher in reviewing the selected
paper. The performance of an algorithm then is measured as a fraction of times the author of the selected
paper is predicted to be among the top reviewers for this paper. This authorship proxy, however, may
not be representative of the actual task of similarity computation as algorithms that accurately predict the
authorship relationship (and hence do well according to this approach) are not guaranteed to accurately
estimate expertise in reviewing submissions authored by other researchers.
Mimno and McCallum (2007) collected a dataset with external expertise judgments. Specifically, they
used 148 papers accepted to the NeurIPS 2006 conference and 364 reviewers from the NeurIPS 2005 confer-
ence and ask human annotators – independent established researchers – to evaluate expertise for a selected
subset of 650 (paper, reviewer) pairs. While this approach results in a publicly available dataset, we note
that external expertise judgments may also be noisy as judges may have incomplete information about the
expertise of reviewers.
Dumais and Nielsen (1992), Rodriguez and Bollen (2008) and Anjum et al. (2019) obtain more accu-
rate expertise judgments by relying on self reports of expertise evaluations from reviewers. In more detail,
Dumais and Nielsen (1992) and Rodriguez and Bollen (2008) rely on ex-ante bids—preferences of reviewers
in reviewing submissions made in advance of reviewing. In contrast, Anjum et al. (2019) rely on ex-post
evaluations of expertise made by reviewers after reviewing the submissions. These works construct datasets
that can be used to evaluate algorithms: Dumais and Nielsen (1992) employ 117 papers and 15 reviewers,
Rodriguez and Bollen (2008) employ 102 papers and 69 reviewers and Anjum et al. (2019) employ 20 pa-
pers and 33 reviewers. However, Rodriguez and Bollen (2008) and Anjum et al. (2019) use sensitive data
that cannot be released without compromising the privacy of reviewers. Furthermore, ex-ante evaluations
of Dumais and Nielsen (1992) and Rodriguez and Bollen (2008) may not have a high accuracy as (i) bids
may contaminate expertise judgments with willingness to review submissions, and (ii) bids are based on a
verybriefacquaintancewithpapers. Ontheotherhand, ex-post databyAnjumetal.(2019)iscollectedfor
papers assigned to reviewers using a specific similarity-computation algorithm. Thus, while collected evalu-
ations have high precision, they may also have low recall if the employed similarity-computation algorithm
erroneouslyassignedlowexpertisescorestosome(paper, reviewer)pairsasevaluationsofexpertiseforsuch
papers were not observed.
In this work, we collect a novel dataset of reviewer expertise that (i) can be released publicly, and (ii)
contains accurate self-evaluations of expertise that are based on a deep understanding of the paper and are
not biased towards any existing similarity-computation algorithm.
3
Similarity scores in conferences In modern conferences, similarity scores are typically computed by
combining two types of input:
• Initial automated estimates. First, a similarity-computation algorithm is used to compute initial es-
timates. Many algorithms have been proposed for this task (Mimno and McCallum, 2007; Rodriguez
and Bollen, 2008; Charlin and Zemel, 2013; Liu et al., 2014; Tran et al., 2017; Anjum et al., 2019;
Kerzendorf et al., 2020; OpenReview, 2022) and we provide more details on several algorithms used in
flagship computer science conferences in Section 5.
• Human corrections. Second, automated estimates are corrected by reviewers who can read abstracts
of submissions and report bids—preferences in reviewing the submissions. A number of works focus
on the bidding stage and (i) explore the optimal strategy to assist reviewers in navigating the pool of
thousands of submissions (Fiez et al., 2020; Meir et al., 2020) or (ii) protect the system from strategic
bids made by colluding reviewers willing to get assigned to each other’s paper (Jecmen et al., 2020; Wu
et al., 2021; Boehmer et al., 2021; Jecmen et al., 2022).
Combining these two types of input in a principled manner is a non-trivial task. As a result, different
conferences use different strategies (Shah et al., 2018; Leyton-Brown et al., 2022) and there is a lack of
empirical or theoretical evidence that would guide venue organizers in their decisions.
Automation of the assignment stage At a high level, automated assignment stage consists of two
steps: first, similarity scores are computed; second, reviewers are allocated to submissions such that some
notion of assignment quality (formulated in terms of the similarity scores) is maximized. In this work, we
focus on the first step of the process. However, for completeness, we now mention several works that design
algorithms for the second step.
Apopularnotionofassignmentqualityisthecumulativesimilarityscore,thatis,thesumofthesimilarity
scores across all assigned reviewers and papers. An algorithm pursuing such an objective is implemented in
the widely employed TPMS assignment algorithm (Charlin and Zemel, 2013) and similar ideas are explored
in many papers (Goldsmith and Sloan, 2007; Tang et al., 2010; Long et al., 2013). While the cumulative
objective is a natural choice, it has been noted that it may discriminate against certain submissions by allo-
catingallirrelevantreviewerstoasubsetofsubmissions,evenwhenamorebalancedassignmentexists(Garg
et al., 2010). Thus, a number of works has explored the idea of assignment fairness, aiming at producing
more balanced assignments (Kobren et al., 2019; Stelmakh et al., 2021). Finally, other works explore the
ideas of envy-freeness (Tan et al., 2021; Payan, 2022), resistance to lone-wolf strategic behavior (Xu et al.,
2019; Dhull et al., 2022), and encouraging various types of diversity (Li et al., 2015; Leyton-Brown et al.,
2022).
3 Data collection pipeline
In this section, we describe the process of data collection. This work was performed under the approval of
the Institutional Review Board (IRB) of Carnegie Mellon University.
Gold standard data In this study, we aim to collect a dataset of self-evaluations of reviewers’ expertise
that satisfies two desiderata:
(D1) The dataset should comprise evaluations of expertise for papers participants read to a reasonable
extent.
(D2) The dataset should be released publicly without disclosing any sensitive information.
Let us now discuss our approach to recruiting participants and obtaining accurate estimates of their
expertise in reviewing papers included in the dataset.
4
Participant recruiting We recruited participants using a combination of several channels that are typi-
cally employed to recruit reviewers for computer science conferences:
• Mailing lists. First,wesentrecruitingemailstorelevantmailinglistsofseveraluniversitiesandresearch
departments of companies
• Social media. Second,twoauthorsofthispaperpostedacallforparticipationontheirTwitteraccounts
• Personal communication. Third, we sent personal invites to researchers from our professional network
We had a screening criterion requiring that prospective participants have at least one paper published
in the broad area of computer science. Overall, for the version of the dataset we release in this paper, we
managed to recruit 58 participants, all of whom passed the screening.
Expertise evaluations The key idea of our approach to expertise evaluation is to ask participants to
evaluate their expertise in reviewing papers they read in the past. After reading a paper, a researcher is
in the best possible position to evaluate whether they have the right background—both in terms of the
techniques used in the paper and in terms of the broader research area of the paper—to judge the quality of
the paper. With this motivation, we asked participants to:
Recall 5-10 papers in their broad research area that they read to a reasonable extent in the last
year and tell us their expertise in reviewing these papers.
In more detail, the choice of papers was constrained by two minor conditions:
• The papers reported by a participant should not be authored by them
• The papers reported by a participant should be freely available online
In addition to these constraints, we gave several recommendations to the participants in order to make
the dataset more diverse and useful for the research purposes:
• First,weaskedparticipantstochoosepapersthatcoverthewholespectrumoftheirexpertisewithsome
papers being well-separated (e.g., very high expertise and very low expertise) and some papers being
nearly-tied (e.g., two high-expertise papers).
• Second, we recommended participants to avoid ties in their evaluations. To help participants comply
with this recommendation, we implemented evaluation on a scale of 1 (“I am not qualified to review
this paper”) to 5 (“I have background necessary to evaluate all the aspects of the paper”) with a 0.25
step size, enabling participants to report papers with small differences in expertise.
• Third,weaskedparticipantstocomeupwithpapersthattheythinkmaybetrickyforexistingsimilarity-
computation algorithms. For this, we relied on the commonsense understanding and did not instruct
participants on the inner-workings of these algorithms.
Overall, the time needed for participants to contribute to the dataset was estimated to be 5–10 minutes.
The full instructions of the survey are available in Appendix A.
Data release Following the procedure outlined above, we collected responses from 58 researchers. These
responses constitute an initial version of the dataset that we release in this work. Each entry in the dataset
correspondsto aparticipant andcomprises evaluations oftheir expertisein reviewingpapersof theirchoice.
For each paper and each participant, we provide representations that are sufficient to start working on our
dataset:
• Participant. Each participant is represented by their Semantic Scholar ID and complete bibliography
crawled from Semantic Scholar on May 1, 2022.
• Paper. Each paper, including papers from participants’ bibliographies, is represented by its Semantic
Scholar ID, title, abstract, list of authors, publication year, and arXiv identifier. Additionally, papers
from participants’ responses are supplied with links to freely available PDFs (whenever available).
5
Total number of participants: 58
Characteristic Quantity Value
Gender % Male 78
Affiliation % Carnegie Mellon University 40
Country % USA 74
% PhD student 45
Position % Faculty 28
% Post-PhD (non-faculty) 12
Min # publications 2
Max # publications 492
Experience
Mean # publications 54
Median # publications 20
Table 1: Demography of participants. For the first four characteristics, quantities represent percentages
of the one or more most popular classes in the dataset. Note that classification is done manually based
on publicly available information and may not be free of error. For the last characteristic, quantities are
computed based on Semantic Scholar profiles.
4 Data exploration
Inthissectionweexplorethecollecteddataandpresentvariouscharacteristicsofthedataset. Thesubsequent
sections then detail the results of using this data to benchmark various popular similarity-computation
algorithms.
4.1 Participants
We begin with a discussion of the pool of the survey participants: Table 1 displays its key characteristics.
First, we note that all participants work in the broad area of computer science and have rich publication
profiles (at least two papers published, with the mean of 54 papers and the median of 20 papers). In
manysubareasofcomputerscience, includingmachinelearningandartificialintelligence, havingtwopapers
is usually sufficient to join the reviewer pool of flagship conferences. Given that approximately 85% of
participants either have PhD or are in the process of getting the degree, we conclude that most of the
researcherswhocontributedtoourdatasetareeligibletoreviewformachinelearningandartificialintelligence
conferences.
Second, we caveat that most of the participants are male researchers affiliated with US-based organi-
zations, with about 40% of all participants being affiliated with Carnegie Mellon University. Hence, the
populationofparticipantsisnotnecessarilyrepresentativeofthegeneralpopulationofthemachinelearning
and computer science communities. We encourage researchers to be aware of this limitation when using our
dataset. We note that the data collection process does not conclude with the publication of the present
paper and we will be updating the dataset as new responses come. We also encourage readers to contribute
5–10 minutes of their time to fill out the survey https://forms.gle/SP1Rh8eivGz54xR37 and make the
dataset more representative.
4.2 Papers
Wenowdescribethesetofpapersthatconstituteourdataset. Overall,participantsevaluatedtheirexpertise
in reviewing 463 unique papers. Out of these 463 papers, 12 papers appeared in reports of two participants,
1 paper was mentioned by three participants, and the remaining papers were mentioned by one participant
each.
6
Total number of papers: 463
Characteristic Quantity Value
# On semantic scholar 462
Open Access # On arXiv 411
# PDF available 457
Research Areas # Computer science 459
% Before 2020 25
Publication Year
% 2020 or later 75
Table 2: Characteristics of the 463 papers in the released dataset. Most of the papers are freely available
online and belong to the broad area of computer science.
Table 2 presents several characteristics of the pool of papers in our dataset. First, we note that all but
one of the papers are listed on Semantic Scholar, enabling similarity-computation algorithms developed on
the dataset to query additional information about the papers from the Semantic Scholar database. For
the paper that does not have a Semantic Scholar profile, we construct such a profile manually and keep this
paperinthedataset. Additionally,mostofthepapers(99%)havetheirPDFsfreelyavailableonline,thereby
allowing algorithms to use full texts of papers to compute similarities.
Semantic Scholar has a built-in tool to identify broad research areas of papers (Wade, 2022). According
to this tool, 99% of the papers included to our dataset belong to the broad area of computer science—the
target area for our data-collection procedure. The remaining four papers belong to the neighboring fields
of mathematics, philosophy, and the computational branch of biology. Finally, approximately 75% of all
papers in our dataset are published in or after 2020, ensuring that our dataset contains recent papers that
similarity-computation algorithms encounter in practice.
4.3 Evaluations of expertise
Finally,weproceedtothekeyaspectofourdataset—evaluationsofexpertiseinreviewingthepapersreported
by participants. All but one participant reported expertise in reviewing at least 5 papers with the mean
number of papers per participant being 8.2 and the total number of expertise evaluations being 477.
Figure 1 provides visualization of expertise evaluations made by participants. First, Figure 1a displays
the histogram of expertise values. About two-thirds of the reported papers belong to the expertise area
175
300
150
250
125
200
100
150
75
50 100
25 50
0 0
[1, 2] (2, 3] (3, 4] (4, 5] 0 1 2 3 4
Value of expertise Difference in expertise
(a) Histogram of expertise values. (b) Histogram of differences in expertise evaluations.
Figure 1: Distribution of expertise scores reported by participants.
7
tnuoC tnuoC
of participants (expertise score larger than 3), and about one-third evaluations are made for papers that
reviewers are not competent in (expertise score 3 or lower), thereby yielding a good spread.
Second, Figure 1b shows the distributions of pairwise differences in expertise evaluations made by the
samereviewer. Toconstructthisfigure,foreachparticipantweconsideredallpairsofpapersintheirreport.
Next, we pooled the absolute values of the pairwise differences in expertise across participants. We then
plotted the histogram of these differences in the figure. Observe that the distribution in Figure 1b has
a heavy tail, suggesting that our dataset is suitable for evaluating the accuracy of similarity-computation
methods both at a coarse level (large differences between the values of expertise) and at a fine level (small
differences between the values of expertise).
5 Experimental setup
We now describe the setup of experiments on our dataset.
5.1 Metric
Webeginwithdefiningamainmetricthatweuseinthisworktoevaluateperformanceofthealgorithms. For
this, werelyontheKendall’sTaudistancethatiscloselyrelatedtothewidelyusedKendall’sTau(Kendall,
1938) rank correlation coefficient. We introduce the metric and the algorithms in this section, followed by
the results in Section 6. Subsequently in Section 7, we provide additional evaluations separating out hard
and easy instances.
Intuition Before we introduce the metric in full details, let us provide some intuition behind it. Consider
a pair of papers and a participant who evaluated their expertise in reviewing these papers. We say that
a similarity-computation algorithm makes an error if it fails to correctly predict the paper for which the
participant reported the higher value of expertise.
Ofcourse, somepairsofpapersarehardertoresolvethanothers(e.g., itishardertoresolvepaperswith
expertise scores 4.0 and 4.25 than 4.0 and 1.0). To capture this intuition, whenever an algorithm makes an
error, we penalize it by the absolute difference in the expertise values reported by the participant. Overall,
the loss of the algorithm is the sum of losses across all pairs of papers evaluated by participants normalized
to take values between 0 and 1 (the lower the better).
Formal definition The intuition behind the metric that we have just introduced should be sufficient
to interpret the results of our study so the reader may skip the rest of this section and move directly to
Section 5.2. However, for the sake of rigor, we now introduce our metric more formally.
Consideranyalgorithmthatproducesreal-valuedpredictionsofreviewers’expertiseinreviewingagiven
set of papers. We call these predictions the “similarity scores” given by the algorithm. We assume that a
highervalueof a similarityscore means thatthe algorithmpredicts abetter expertise. The metricwedefine
below is agnostic to the range or exact values of predicted similarity scores; it only relies on the relative
values across different papers.
Nowconsideranyparticipantrinourstudy. Weletm denotethenumberofpapersforwhichparticipant
r
r reports their expertise. We let P = {p(1),p(2),...,p(mr)} denote this set of m papers. For every
r r r r r
i∈{1,...,m },weletε(i) ∈{1,1.25,1.5,...,5}denotetheexpertiseself-reportedbyparticipantr forpaper
r r
p(i). Next,foreveryi∈{1,...,m },welets(i) denotethereal-valuedsimilarityscoregivenbythealgorithm
r r r
to the pair (reviewer r, paper p(i)).
r
Having set up this notation, we now define the ‘unnormalized’ loss of the algorithm with respect to
participant r as:
8
L
r=(cid:88)mr (cid:32)
I(cid:110) (s( ri)−s( rj))×(ε( ri)−ε( rj))<0(cid:111) ×(cid:12) (cid:12)ε( ri)−ε( rj)(cid:12) (cid:12)+I(cid:110) (s( ri)−s( rj))×(ε r(i)−ε( rj))=0(cid:111) × 21(cid:12) (cid:12)ε( ri)−ε( rj)(cid:12)
(cid:12)(cid:33)
.
i,j=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
i<j error tie
Inwords,foreachpairofpapers(p(i),p(j))reportedbyparticipantr,thealgorithmisnotpenalizedwhen
r r
the ordering of papers induced by the similarity scores {s(i),s(j)} agrees with the ground truth expertise-
r r
basedordering{ε(i),ε(j)}. Whentwoorderingsdisagree(thatis,thealgorithmmakesanerror),thealgorithm
r r
is penalized by the difference of expertise reported by the participant (|ε(i) −ε(j)|). Finally, when scores
r r
computed by the algorithm indicate a tie while expertise scores are different, the algorithm is penalized by
half the difference in expertise (1/2|ε( ri)−ε( rj)|).
Having the unnormalized loss with respect to a participant defined, we now compute the overall loss
L ∈ [0,1] of the algorithm. For this, we take the sum of unnormalized losses across all participants and
normalizethissumbythelossachievedbytheadversarialalgorithmthatreversestheground-truthordering
of expertise (that is, sets s=−ε) and achieves the worst possible performance on the task. More formally,
(cid:80)
L
r
L= r .
(cid:80) m (cid:80)r (cid:12) (cid:12)ε r(i)−ε( rj)(cid:12) (cid:12)
r i,j=1
i<j
Overall, our loss L takes values from 0 to 1 with lower values indicating better performance.
5.2 Algorithms
In this work, we evaluate several algorithms that we now discuss. All of these algorithms operate with (i)
thelistofsubmissionsforwhichsimilarityscoresneedtobecomputedand(ii)reviewers’profilescomprising
pastpublicationsofreviewers. Conceptually,allalgorithmspredictreviewers’expertisebyevaluatingtextual
overlap between each submission and papers in each reviewer’s publication profile. Let us now provide more
detail on how this idea is implemented in each of the algorithms under consideration.
Trivialbaseline First,weconsideratrivialbaselinethatignoresthecontentofsubmissionsandreviewers’
profiles when computing the assignment scores: for each (participant, paper) pair, the Trivial algorithm
predicts the score s to be 1.
Toronto Paper Matching System (TPMS) The TPMS algorithm (Charlin and Zemel, 2013), which
is based on TF-IDF similarities, is widely used by flagship conferences such as ICML and AAAI. While an
exactimplementationisnotpubliclyavailable,inourexperimentsweuseanopen-sourceversionbyXuetal.
(2019) which implements the basic TF-IDF logic of TPMS. We also note that this method is rediscovered
independently by Kerzendorf et al. (2020).
Asatechnicalremark,inourimplementationweusereviewers’profilesandallreportedpaperstocompute
the IDF part of the TF-IDF model. In principle, one may be able to get a better performance by using a
larger set of papers from the respective field (e.g., all submissions to the last edition of the conference) to
compute IDF.
OpenReview algorithms OpenReview is a conference-management system used by machine learning
conferencessuchasNeurIPSandICLR.Itoffersafamilyofdeep-learningalgorithmsformeasuringexpertise
of reviewers in reviewing submissions. In this work, we evaluate the following algorithms which are used to
compute affinity scores between submissions and reviewers:
9
• ELMo. This algorithm relies on general-purpose Embeddings from Language Models (Peters et al.,
2018) to compute textual similarities between submissions and reviewers’ past papers.
• Specter. This algorithm employs more specialized document-level embeddings of scientific docu-
ments(Cohanetal.,2020). Specifically,Specterexploresthecitationgraphtoconstructembeddings
that are useful for a variety downstream tasks, including the similarity computation we focus on in
this work.
• Specter+MFR. Finally, Specter+MFR further enhances Specter (Chang and McCallum, 2021).
Instead of constructing a single embedding of each paper, it constructs multiple embeddings that
correspond to different facets of the paper. These embeddings are then used to compute the similarity
scores.
We use implementations of these methods that are available on the OpenReview GitHub page1 and
execute them with default parameters.
ACL paper matching The Association for Computational Linguistics (ACL) is a community of re-
searchers working on computational problems involving natural language. The association runs multiple
publication venues, including the flagship ACL conference, and has its own method to compute expertise
between papers and reviewers.2 This algorithm is trained on 45,309 abstracts of papers found in the ACL
anthology(https://aclanthology.org)downloadedinNovember2019. Thekeyideaofthemodeltraining
is to split each abstract into two non-contiguous parts and treat two parts of the same abstract as a positive
example and highly similar parts of different abstracts as negative examples. The algorithm uses ideas from
the work of Wieting et al. (2019, 2022) to learn a simple and efficient similarity function that separates
positive examples from negative examples. This function is eventually used to compute similarity scores
between papers and reviewers. More details on the ACL algorithm are provided in Appendix B.
We note that the ACL algorithm is trained on the domain of computational linguistics and hence may
sufferfromadistributionshiftwhenappliedtothegeneralmachinelearningdomain. Thatsaid,inlinewith
ELMo, Specter, and Specter+MFR, we use the ACL algorithm without additional fine-tuning.
The algorithm used by ACL is developed by a subset of authors of this paper (JW, GN) and we use this
implementation in our experiments. Importantly, we note that JW and GN executed the algorithm without
having access to the ground truth expertise, and the predicted similarities were independently evaluated by
another author of this paper (IS).
6 Results
Inthissection,wereporttheresultsofevaluationofalgorithmsdescribedinSection5.2. First,wejuxtapose
all algorithms on our data (Section 6.1). Second, we use the TPMS algorithm to explore various aspects of
the similarity-computation problem (Section 6.2).
6.1 Comparison of the algorithms
Our first set of results compares the performance of the existing similarity-computation algorithms. To run
these algorithms on our data, we need to make some modeling choices faced by conference organizers in
practice:
• Paper representation. First, in their inner-workings, similarity-computation algorithms operate with
some representation of the paper content. Possible choices of representations include: (i) title of the
paper,(ii)titleandabstract,and(iii)fulltextofthepaper. Wechooseoption(ii)asthisoptionisoften
used in real conferences and is supported by all algorithms we consider in this work. Thus, to predict
1https://github.com/openreview/openreview-expertise
2https://github.com/acl-org/reviewer-paper-matching
10
expertise, algorithms are provided with the title and abstract of each paper (both papers submitted to
the conference and papers in reviewers’ publication profiles).
• Reviewer profiles. The second important parameter is the choice of papers to include in reviewers’
profiles. Inrealconferences,thischoiceisoftenlefttoreviewerswhocanmanuallyselectthepapersthey
find representative of their expertise. In our experiments, we construct reviewer profiles automatically
by using the 20 most recent papers from their Semantic Scholar profiles. If a reviewer has less than
20 papers published, we include all of them in their profile. Our choice of the reviewer profile size is
governedbytheobservationthatthemeanlengthofthereviewerprofileintheNeurIPS2022conference
is 16.5 papers. By setting the maximum number of papers to 20, we achieve the mean profile length
of 14.8, thereby operating with the amount of information close to that available to algorithms in real
conferences.
Statistical aspects Tobuildreviewerprofiles, weusepublicationyearstoorderpapersbyrecency, where
we break ties uniformly at random. Thus, the content of reviewer profiles depends on randomness. To
average this randomness out, we repeat the procedure of profile construction and similarity prediction 10
times, and report the mean loss over these iterations. That said, we note that the observed variability due
to the randomness in the construction of reviewer profiles is negligible, with a standard deviation over all
iterations is less than 0.005.
The pointwise performance estimates obtained by the procedure above depend on the selection of par-
ticipants who contributed to our dataset. To quantify the associated level of uncertainty, we compute 95%
confidence intervals as follows. For 1,000 iterations, we create a new reviewer pool by sampling participants
with replacement and recomputing the loss of each algorithm on the bootstrapped set of reviewers. To save
computation time, we do not reconstruct reviewer profiles for each of these iterations as the uncertainty
associatedwiththeconstructionofreviewerprofilesissmall. Instead,wereuseprofilesconstructedtoobtain
pointwise estimates.
Finally, we also build confidence intervals for the difference in the performance of the algorithms. We
do so in addition to the aforementioned procedure since even when the losses of the algorithms fluctuate
with the choice of the bootstrapped dataset, the relative difference in performance of a pair of algorithms
may be stable. Specifically, we use the procedure above to build confidence intervals for the difference in
performance between the TPMS algorithm and each of the more advanced algorithms. TPMS is chosen as
a baseline for this comparison due to its simplicity.
Results of the comparison Table3displaysresultsofthecomparison. Thefirstpairofcolumnspresents
the loss of each algorithm on our dataset and the associated confidence intervals. The third and the forth
columns investigate the relative difference in performance between the non-trivial algorithms: they display
thedifferencesinperformancebetweenTPMSandeachofthemoreadvancedalgorithms(OpenReviewalgo-
rithmsandACL)togetherwiththeassociatedconfidenceintervals. Wemakeseveralimportantobservations
from Table 3:
• First,wenotethatallalgorithmsweconsiderinthisworkconsiderablyoutperformtheTrivialbaseline,
confirming that content of papers is useful in evaluating expertise.
• Second, comparing three algorithms from the OpenReview toolkit, we note that Specter+MFR and
Specter outperform ELMo. The former two algorithms rely on domain-specific embeddings3 while
ELMo uses general-purpose embeddings. Thus, the nature of the text similarity computation task in
the academic context may be sufficiently different from that in other domains.
• Third,wenotethatundermodelingchoices(paperrepresentationandlengthofreviewers’profiles)that
mimic choices that have been made in real conferences, the Specter+MFR algorithm performs best.
• The fourth finding is the most surprising. Observe that TPMS algorithm is much simpler than
other non-trivial algorithms: in contrast to ELMo, Specter, Specter+MFR, and ACL, it does
3Specter+MFRandSpecterareinitializedwithSciBERTBeltagyetal.(2019)
11
Algorithm Loss 95% CI for Loss ∆ with TPMS 95% CI for ∆
Trivial 0.50 — — —
TPMS 0.28 [0.23,0.33] — —
ELMo 0.34 [0.29,0.40] +0.06 [0.00,0.13]
Specter 0.27 [0.21,0.34] −0.01 [−0.06,0.04]
Specter+MFR 0.24 [0.18,0.30] −0.04 [−0.09,0.01]
ACL 0.30 [0.25,0.35] +0.01 [−0.03,0.06]
Table 3: Comparison of similarity-computation algorithms on the collected data. All algorithms operate
with reviewer profiles consisting of the 20 most recent papers and use titles and abstracts of papers. Lower
valuesoflossarebetter. Apositive(respectively, negative)valueof∆indicatesthatthealgorithmperforms
worse (respectively, better) than TPMS.∗
∗In Table 4 we evaluate TPMS algorithm when full texts of papers are additionally provided and observe that TPMS closes
thegapwithSpecter+MFRwhenusingthisinformation.
not rely on carefully learned embeddings. However, TPMS is competitive against complex Specter
and Specter+MFR and even outperforms ELMo and ACL. Moreover, TPMS is the only algorithm
that can make use of the full text of the papers. In Section 6.2, we find that when TPMS uses the full
text of all papers (including reviewers’ past papers), it closes the gap with Specter+MFR.
Notethattheperformanceofadvancedalgorithmscouldbeimprovedbyfine-tuninginadataset-specific
manner, or by leveraging larger pre-trained models like T5 (Raffel et al., 2020) with added fine-tuning
in the scientific domain. However, our observation suggests that the off-the-shelf performance of the
best algorithm (Specter+MFR) is comparable to that of much simpler TPMS algorithm.
Finally, we note that some of the confidence intervals for the performance of the different algorithms, as
well as for the relative differences, are overlapping. It is, therefore, crucial to increase the size of our dataset
to enable more fine-grained comparisons between the algorithms.
6.2 The role of modeling choices
InthebeginningofSection6.1wemadetwomodelingchoicespertainingto(i)representationsofthepapers
providedtosimilarity-computationalgorithms,and(ii)thesizeofreviewers’profilesusedbythesealgorithms.
In this section, we investigate these two questions in more detail.
• Question 1 (paper representation). Some similarity-computation algorithms are designed to work with
titles and/or abstracts of papers (e.g., Specter) while others can also incorporate the full texts of the
manuscripts (e.g., TPMS). Consequently, there is a potential trade-off between accuracy and compu-
tation time. Richer representations are envisaged to result in higher accuracy, but are also associated
with an increased demand for computational power. As with the choice of the algorithm itself, there
is no guidance on what amount of information should be supplied to the algorithm as the gains from
using more information are not quantified. With this motivation, our first question is:
What are the benefits of providing richer representations of papers to similarity-computation
algorithms?
• Question 2 (reviewer profile). The second important choice is the size of reviewers’ profiles. On the
one hand, by including only very recent papers in reviewers’ profiles, conference organizers are at risk
of not using enough data to obtain accurate values of expertise. On the other hand, old papers may
not accurately represent the current expertise of researchers and hence may result in noise when used
to compute expertise. Thus, our second question is:
12
0.5
0.4
0.3
0.2
Paper content
title
0.1 title+abstract
full text
0.0
1 5 10 15 20
Number of papers in reviewer profile
Figure 2: Impact of different choices of parameters on the quality of predicted similarities. Confidence
intervals are not shown (see Table 4).
What is the optimal number of the most recent papers to include in the profiles of reviewers?
Toinvestigatethesequestions,wechoosetheTPMSalgorithmastheworkhorsetoperformexperiments.
We make this choice for two reasons. First, TPMS can work with all possible choices of the paper represen-
tation: title only, title and abstract, and full text of the paper. In contrast, other methods do not support
using the full text of the papers. Second, TPMS is fast to execute, enabling us to compute expertise for
hundreds of configurations in a reasonable time.
With the algorithm chosen, we vary the number of papers in the reviewers’ profiles from 1 to 20. For
each value of the profile length, we consider three representations of the paper content: title, title+abstract,
and full text of the paper. For each combination of parameters, we construct reviewer profiles and predict
similaritiesusingtheapproachintroducedinSection6.1. Theonlyexceptionisthatwerepeattheprocedure
foraveragingouttherandomnessintheprofilecreation5times(insteadof10)tosavethecomputationtime.
Papers and reviewer profiles Before presenting the results, we note that in this section, we conduct all
evaluations focusing on papers that have PDFs freely available. To this end, we remove 6 papers from the
dataset as they are not freely available online (see Table 2). Similarly, we limit reviewer profiles to papers
whosesemanticscholarprofilescontainlinkstoarXiv. Oneoftheparticipantsdidnothaveanysuchpapers,
so we exclude them from the dataset.
Results Figure2andTable4provideanswerstothequestionswestudyinthissection. Figure2showsthe
pointwise loss of the TPMS algorithm for each choice of parameters. To save computation time, we do not
build confidence intervals for each combination of parameters. Instead, Table 4 sets the number of papers
in reviewers’ profiles to 20 (consistent with Table 3) and presents confidence intervals for losses incurred by
the algorithm under different choices of paper representations. We now make two observations.
First, paper abstracts are very useful in improving the quality of expertise prediction as compared to
titles alone (the improvement of 0.07). Including full texts of the papers in reviewers’ and papers’ profiles
resultsinadditionalimprovement(0.03). Overall,ignoringtheminordifferencesinthedatasetsbetweenthis
section and Section 6.1, we observe that TPMS with titles, abstracts, and full texts of papers demonstrates
the same performance as Specter+MFR which cannot handle the full texts of papers. Thus, it may be
of interest to develop modern deep-learning based algorithms to incorporate full texts of papers and further
boost performance on the similarity-computation task.
Second, the loss curves plateau once reviewer profiles include 8 or more of their most recent papers.
Additional increase of the profile length does not impact the quality of predictions. Thus, in practice,
13
ssoL
Paper Representation Loss 95% CI for Loss ∆ with Title+Abstract 95% CI for ∆
Title 0.33 [0.29,0.38] +0.07 [0.02,0.12]
Title+Abstract 0.27 [0.22,0.32] — —
Full Text 0.24 [0.19,0.30] −0.03 [−0.09,0.03]
Table4: PerformanceoftheTPMSalgorithmwith20mostrecentpapersincludedinreviewers’profilesand
with different choices of the paper representation. Lower values of loss are better. A positive (respectively,
negative) value of ∆ indicates that the corresponding choice of the paper representation leads to a weaker
(respectively, stronger) performance.
reviewers may be instructed to include 10 representative papers to their profile, which for many active
researchers amounts to the number of papers published in 1-3 years.
7 Additional evaluations
Recall that our loss metric (see Section 5) assigns different weights to errors of an algorithm: errors on pairs
with similar values of reviewer’s expertise are penalized less than errors on well-separated pairs. The overall
performanceofthealgorithmisthencapturedinasinglenumber(loss)whichdoesnotcharacterizethetype
of mistakes made by the algorithm.
Toprovidedeepercharacterizationofthealgorithmperformance,wenowconductadditionalevaluations.
In these evaluations, we focus on two important regions of the similarity-computation problem. Specifically,
fromallexpertiseevaluationsmadebyparticipants,weselecttwogroupsoftriples,whereeachtripleconsists
of a participant r and two papers (p ,p ) evaluated by this participant:
1 2
• “Easy” triples. The first group comprises 261 triples where a participant reported high expertise
(greater than or equal to four) in one paper and low expertise (less than or equal to two) in another.
We call these triples “easy” as the gap in the expertise between the two papers is large. Note that in
real conferences it is important to resolve the easy triples correctly to ensure that reviewers do not get
assigned irrelevant papers.
• “Hard” triples. The second group comprises 417 triples where a participant reported high expertise
(greater than or equal to four) in both papers and the values of expertise are different for the two
papers.
Wecallthesetriples“hard”asthegapintheexpertisebetweenthepapersissmall. Thatsaid,wenote
that resolving hard triples in practice is also very important: in real conferences, we want to assign
papers to the most suitable reviewers and we need to be able to distinguish two papers with high but
not equal values of expertise to achieve this goal.
Note that by focusing on these two groups, we exclude regions of the problem which may be considered
less important. For example, the ability of an algorithm to correctly order two low-expertise papers is less
crucial as long as the algorithm can reliably distinguish these papers from the high-expertise papers.
We now study the performance of the algorithms introduced in Section 5.2 on the easy and hard groups
of triples. For all algorithms we use titles and abstracts of papers as the data source and include 20 papers
in the reviewers’ profiles. Specifically, for each triple we compare the ordering of papers predicted by an
algorithm with the expertise-induced ordering. We then compute the accuracy of each algorithm as the
fraction of triples correctly resolved by the algorithm (from 0 to 1, larger values are better). In addition,
we also evaluate TPMS in the full text regime to estimate the effect of papers’ texts on the accuracy of the
similarity-computation algorithms.4
4TPMSinthefulltextregimeisevaluatedonaslightlydifferentdatasetasexplainedinSection6.2.
14
Easy triples (n=261) Hard triples (n=417)
Algorithm Accuracy 95% CI Accuracy 95% CI
TPMS (T+A) 0.80 [0.72,0.87] 0.62 [0.54,0.69]
TPMS (Full Text) 0.84 [0.76,0.91] 0.64 [0.56,0.70]
ELMo 0.70 [0.62,0.78] 0.57 [0.51,0.63]
Specter 0.85 [0.76,0.92] 0.57 [0.50,0.63]
Specter+MFR 0.88 [0.81,0.94] 0.60 [0.53;0.66]
ACL 0.78 [0.69;0.86] 0.62 [0.55;0.68]
Table 5: Results of additional evaluations. Higher values of accuracy are better. Specter+MFR demon-
stratesthebestperformanceoneasytriplesandTPMS(FullText)hasthehighestaccuracyonhardtriples.
Table5demonstratestheresultsofadditionalevaluations. First, notethatallalgorithmshavemoderate
tohighaccuracyinresolvingtheeasytriples. AllofthealgorithmsexceptELModetectpapersthatreviewers
have low expertise in with probability close to or above 80%, with Specter+MFR reaching nearly 90%
accuracy. However, we note that in a non-trivial amount of cases (more than 10%), the algorithms are
unabletodistinguishapaperthatareviewer iswell-qualifiedtoreviewversus apaperthe reviewerdoesnot
have necessary background to evaluate. Thus, there is a vital need to improve the similarity-computation
algorithms.
Getting to the hard triples, we observe a significant drop in performance with all algorithms being a bit
better than random: the best-performing algorithm (TPMS) reaches 62% accuracy in the title+abstract
regime and 64% in the full text regime. Perhaps surprisingly, while Specter+MFR and Specter outper-
form TPMS on easy triples, they are not better than TPMS on hard triples (we caveat though that error
bars are too wide to make a decisive comparison). This observation suggests that there may be a value in
additionally fine-tuning these advanced algorithms on hard triples to improve their practical performance.
Finally, we observe that on both easy and hard triples, full texts of the papers are instrumental in
improving performance of the TPMS algorithm. This observation supports our intuition that similarity-
computation algorithms do indeed benefit from employing full texts of papers.
With these remarks, we conclude evaluations of algorithms on the dataset we collected. The dataset and
associated code are released on the project’s GitHub page5 and we encourage readers to experiment with
additional evaluations on our data.
8 Discussion
In this work, we collect a novel dataset of reviewers’ expertise in reviewing papers. In contrast to datasets
collected in previous works, our dataset (i) is released publicly, and (ii) contains evaluations of expertise
madebyscientistswhohaveactuallyreadthepapersfortheirownresearchpurposes. Weusethisdatasetto
compareseveralexistingexpertise-estimationalgorithmsandhelpvenueorganizersinchoosinganalgorithm
in an evidence-based manner.
Most importantly, we find that current algorithms make a large number of errors. Given the growing
number of submissions in many fields of research, the need for automation in reviewer assignments is only
growing. There is thus an urgent and vital need to develop significantly improved algorithms to match
reviewers to papers, thereby in turn making the peer-review process considerably better.
Ourdatasetcanbeusedtodevelopaswellasevaluatenewexpertise-estimationalgorithms. Weencourage
researchersfromthenaturallanguageprocessingandothercommunitiestouseourdatainordertoimprove
peer review.
5https://github.com/niharshah/goldstandard-reviewer-paper-match
15
Limitations. Finally, we mention several caveats that researchers should be aware of when working with
our dataset and interpreting the results of our experiments. First, our dataset comprises evaluations of
expertiseinreviewingpapersthatwerewrittensometimeago. Incontrast,inrealconferences,manypapers
arerecentandnotavailableonline. Thus,incomingcitationstopapersincludedinourdatasetmayconstitute
informationthatisnotavailabletothealgorithmsinreallife. Whilethealgorithmsweevaluateinthiswork
do not rely on the citation relationship, this caveat may be important for future work.
Second, the experiments we conduct in this work rely on Semantic Scholar profiles. These profiles are
constructed automatically and may not be accurate. Thus, mistakes of the algorithms we observe in this
work can be partially due to the noise in the profile creation.
Finally, we reiterate that the present version of the dataset was constructed by participants who col-
lectively are not representative of the general computer science community. For example, about 40% of
participants are affiliated with CMU. To alleviate this issue, we continue the data collection process and
encourage the readers of this paper to contribute their data points to the dataset:
https://forms.gle/SP1Rh8eivGz54xR37
Acknowledgments
We sincerely thank all participants of our survey for their contribution to the dataset. This work was
supported by NSF CAREER 1942124 and NSF 1763734.
References
Anjum, O., Gong, H., Bhat, S., Hwu, W.-M., and Xiong, J. (2019). PaRe: A paper-reviewer matching
approach using a common topic space. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 518–528, Hong Kong, China. Association for Computational Linguistics.
Beltagy, I., Lo, K., and Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615–3620,
Hong Kong, China. Association for Computational Linguistics.
Bianchi, F. and Squazzoni, F. (2015). Is three better than one? Simulating the effect of reviewer selection
and behavior on the quality and efficiency of peer review. In Proceedings of the 2015 Winter Simulation
Conference, pages 4081–4089. IEEE Press.
Black, N., Van Rooyen, S., Godlee, F., Smith, R., and Evans, S. (1998). What makes a good reviewer and
a good review for a general medical journal? Jama, 280(3):231–233.
Boehmer, N., Bredereck, R., and Nichterlein, A. (2021). Combating collusion rings is hard but possible.
arXiv preprint arXiv:2112.08444.
Chang, H.-S. and McCallum, A. (2021). Cold-start paper recommendation using multi-facet embedding.
Overleaf preprint. https://www.overleaf.com/project/5f359923225f06000134ea95 [Last Accessed:
3/15/2023].
Charlin, L. and Zemel, R. S. (2013). The Toronto Paper Matching System: An automated paper-reviewer
assignment system.
Cohan, A., Feldman, S., Beltagy, I., Downey, D., and Weld, D. S. (2020). Specter: Document-level repre-
sentation learning using citation-informed transformers.
Dhull,K.,Jecmen,S.,Kothari,P.,andShah,N.B.(2022).Strategyproofingpeerassessmentviapartitioning:
The price in terms of evaluators’ expertise. In HCOMP.
16
Dumais, S. T. and Nielsen, J. (1992). Automating the assignment of submitted manuscripts to reviewers.
In Proceedings of the 15th annual international ACM SIGIR conference on Research and development in
information retrieval, pages 233–244.
Fiez, T., Shah, N., and Ratliff, L. (2020). A SUPER* algorithm to optimize paper bidding in peer review.
In Conference on Uncertainty in Artificial Intelligence.
Garg, N., Kavitha, T., Kumar, A., Mehlhorn, K., and Mestre, J. (2010). Assigning papers to referees.
Algorithmica, 58(1):119–136.
Goldsmith, J. and Sloan, R. (2007). The AI conference paper assignment problem. AAAI Workshop -
Technical Report, WS-07-10:53–57.
Jecmen, S., Yoon, M., Conitzer, V., Shah, N.B., andFang, F.(2022). Adatasetonmaliciouspaperbidding
in peer review.
Jecmen, S., Zhang, H., Liu, R., Shah, N. B., Conitzer, V., and Fang, F. (2020). Mitigating manipulation in
peer review via randomized reviewer assignments. In NeurIPS.
Kendall, M. G. (1938). A new measure of rank correlation. Biometrika, 30(1/2):81–93.
Kerzendorf, W. E., Patat, F., Bordelon, D., van de Ven, G., and Pritchard, T. A. (2020). Distributed peer
review enhanced with natural language processing and machine learning.
Kobren, A., Saha, B., and McCallum, A. (2019). Paper matching with local fairness constraints. In ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining.
Kudo, T. and Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium. Association for
Computational Linguistics.
Leyton-Brown, K., Mausam, Nandwani, Y., Zarkoob, H., Cameron, C., Newman, N., Raghu, D., et al.
(2022). Matching papers and reviewers at large conferences. arXiv preprint arXiv:2202.12273.
Li, L., Wang, Y., Liu, G., Wang, M., and Wu, X. (2015). Context-aware reviewer assignment for trust
enhanced peer review. PLOS ONE, 10(6):1–28.
Liu, X., Suel, T., and Memon, N. (2014). A robust model for paper reviewer assignment. In Proceedings of
the 8th ACM Conference on Recommender Systems,RecSys’14,pages25–32,NewYork,NY,USA.ACM.
Long,C.,Wong,R.,Peng,Y.,andYe,L.(2013). Ongoodandfairpaper-reviewerassignment. InProceedings
- IEEE International Conference on Data Mining, ICDM, pages 1145–1150.
Meir, R., Lang, J., Lesca, J., Kaminsky, N., and Mattei, N. (2020). A market-inspired bidding scheme for
peer review paper assignment. In Games, Agents, and Incentives Workshop at AAMAS.
Merton, R. K. (1968). The Matthew effect in science. Science, 159:56–63.
Mimno,D.andMcCallum,A.(2007). Expertisemodelingformatchingpaperswithreviewers. InProceedings
of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD’07,
pages 500–509, New York, NY, USA. ACM.
OpenReview (2022). Paper-reviewer affinity modeling for openreview. https://github.com/openreview/
openreview-expertise.
Payan, J. (2022). Fair allocation problems in reviewer assignment. In Proceedings of the 21st International
Conference on Autonomous Agents and Multiagent Systems, AAMAS ’22, page 1857–1859, Richland, SC.
International Foundation for Autonomous Agents and Multiagent Systems.
17
Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. (2018).
Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American
ChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(Long
Papers), pages 2227–2237, New Orleans, Louisiana. Association for Computational Linguistics.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.
(2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of
Machine Learning Research, 21:1–67.
Rodriguez, M. A. and Bollen, J. (2008). An algorithm to determine peer-reviewers. In Proceedings of the
17th ACM Conference on Information and Knowledge Management,CIKM’08,pages319–328,NewYork,
NY, USA. ACM.
Shah, N. B. (2022). An overview of challenges, experiments, and computational solutions in peer review.
Communications of the ACM (to appear). Preprint available at http://bit.ly/PeerReviewOverview.
Shah, N. B., Tabibian, B., Muandet, K., Guyon, I., and Von Luxburg, U. (2018). Design and analysis of the
NIPS 2016 review process. The Journal of Machine Learning Research, 19(1):1913–1946.
Squazzoni, F. and Gandelli, C. (2012). Saint Matthew strikes again: An agent-based model of peer review
and the scientific community structure. Journal of Informetrics, 6(2):265–275.
Stelmakh, I., Shah, N., and Singh, A. (2021). PeerReview4All: Fair and accurate reviewer assignment in
peer review. Journal of Machine Learning Research, 22(163):1–66.
Tan, M., Dai, Z., Ren, Y., Walsh, T., and Aleksandrov, M. (2021). Minimal-envy conference paper assign-
ment: Formulation and a fast iterative algorithm. In 2021 5th Asian Conference on Artificial Intelligence
Technology (ACAIT), pages 667–674.
Tang, W., Tang, J., and Tan, C. (2010). Expertise matching via constraint-based optimization. In Proceed-
ings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent
Technology - Volume 01, WI-IAT ’10, pages 34–41, Washington, DC, USA. IEEE Computer Society.
Thorngate, W. and Chowdhury, W. (2014). By the numbers: Track record, flawed reviews, journal space,
and the fate of talented authors. In Advances in Social Simulation, pages 177–188. Springer.
Thurner, S. and Hanel, R. (2011). Peer-review in a world with rational scientists: Toward selection of the
average. The European Physical Journal B, 84(4):707–711.
Tran, H. D., Cabanac, G., and Hubert, G. (2017). Expert suggestion for conference program committees.
In 2017 11th International Conference on Research Challenges in Information Science (RCIS), pages
221–232.
Triggle, C. and Triggle, D. (2007). What is the future of peer review? why is there fraud in science? is
plagiarism out of control? why do scientists do bad things? is it all a case of: ” all that is necessary for
the triumph of evil is that good men do nothing?”. Vascular health and risk management, 3:39–53.
Wade, A. D. (2022). The semantic scholar academic graph (S2AG). In Companion Proceedings of the Web
Conference 2022 (WWW’22 Companion).
Wieting, J., Gimpel, K., Neubig, G., and Berg-Kirkpatrick, T. (2019). Simple and effective paraphrastic
similarity from parallel translations. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pages 4602–4608, Florence, Italy. Association for Computational Linguistics.
Wieting, J., Gimpel, K., Neubig, G., and Berg-kirkpatrick, T. (2022). Paraphrastic representations at scale.
In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations, pages 379–388, Abu Dhabi, UAE. Association for Computational Linguistics.
18
Wu, R., Guo, C., Wu, F., Kidambi, R., Van Der Maaten, L., and Weinberger, K. (2021). Making paper
reviewingrobusttobidmanipulationattacks. InMeila,M.andZhang,T.,editors,Proceedings of the 38th
International Conference on Machine Learning, volume139ofProceedings of Machine Learning Research,
pages 11240–11250. PMLR.
Xu, Y., Zhao, H., Shi, X., and Shah, N. (2019). On strategyproof conference review. In IJCAI.
19
Appendices
We now provide additional discussion.
A More details on the survey used for data collection
Inthissectionweprovidefullinstructionsthatweregiventotheparticipantsofourdata-collectionprocedure.
Dataset of Reviewing Expertise
The goal of this experiment is to collect a dataset to help researchers design better
algorithms for computing similarities between papers and reviewers: these algorithms
will help to improve matching of reviewers to papers in many conferences like NeurIPS,
AAAI, ACL, etc.
WHAT DO YOU NEED TO DO?
***Recall 5-10 papers in your broad research area that you read to a reasonable extent
in the last year and tell us your expertise in reviewing these papers.***
(HINT: To quickly recall what papers you read recently, you can search for arxiv.org
or an analogous website in your browser history.)
WHICH PAPERS TO REPORT?
• Papers should not be authored by you
• Papers should be freely available online (preferably arXiv, but other open sources
are also fine)
**Suggestions**
• Try to choose a set of papers such that some pairs are well-separated and some are
very close in terms of your expertise
• Please try to avoid ties in the expertise ratings you provide
• Try to think of some papers that are less famous to make the dataset more diverse
• Try to provide some examples that are not obvious and may be tricky for the
similarity-computation algorithms. For example, a naive computation of similarity
may think that a paper on ‘‘Theory of Information Dissemination in Social
Networks’’ has high similarity with an Information Theory researcher, but in
reality, this researcher may not have expertise in reviewing this paper
WHAT PARTS OF DATA YOU PROVIDE WILL BE RELEASED?
To facilitate the development of better algorithms for similarity computation (trained
to perform well on your data!), we will publicly release data collected in this survey
(except email addresses) in a non-anonymized form. Your email will not be released.
WHO IS RUNNING THIS SURVEY?
The survey is organized by Graham Neubig, Nihar Shah, Ivan Stelmakh (CMU), and John
Wieting (CMU -> Google Research). Contact Ivan at stiv@cs.cmu.edu if you have any
questions.
[...]
List up to 10 papers in your broad research area that you read to a reasonable extent
in the last year and tell us your expertise in reviewing these papers. Please try to
enter at least 5 papers.
20
Link to Paper 1:
Expertise in reviewing Paper 1:
• 1.0 (I am not qualified to review this paper)
• 1.25
• 1.5
• 1.75
• 2.0 (I can review some aspects of the paper, but can’t make a reliable overall
judgment)
• ...
• 3.0 (I can provide an adequate review, but a substantial part of the paper is
outside of my expertise)
• ...
• 4.0 (I have background in most aspects of the paper, but some minor aspects are
beyond my expertise)
• ...
• 5.0 (I have background necessary to evaluate all the aspects of the paper)
[...]
B More details on the ACL algorithm
In this section, we provide more details on the ACL algorithm that we evaluate in this paper.
Training Themodelistrainedonalargecorpusof45,309abstractsfromtheACLanthologyandisinspired
by the work of Wieting et al. (2019, 2022). Specifically, the model optimizes a max-margin contrastive
learning objective which is defined as follows. First, we split each abstract a from the corpus into two
i
disjoint segments of text uniformly at random. These segments are then uniformly at random allocated into
two equally-sized groups: a(1) ∈A and a(2) ∈A .
i 1 i 2
Second, we construct positive and negative examples:
• Positive example: For each abstract a , pair (a(1), a(2)) constitutes a positive example.
i i i
• Negative example: For each passage a(1) ∈A , a counterpart t (cid:54)=a(2) from A is selected to maximize
i 1 i i 2
the notion of cosine similarity
(cid:16) (cid:17)
f (a(1),t )=cos g(a(1),θ),g(t ,θ) ,
θ i i i i
where g is the sentence encoder with parameters θ. Pair (a(1), t ) constitutes a negative example.
i i
Finally,withthisproceduretobuildpositiveandnegativeexamples,wecandefinetheobjectiveoftheACL
algorithm:
min(cid:88)(cid:104)
δ−f (a(1),a(2))+f (a(1),t
))(cid:105)
6
θ i i θ i i
θ +
i
Inner-workingofthealgorithmreliesonsentencepieceembeddings7 (KudoandRichardson,2018)withdi-
mensionof1,024andvocabularysizeof20,000. Theencoder,g,simplymeanpoolsthelearnedsentencepiece
embeddings, making for efficient encoding, even on CPU.8 In the training procedure, a batch size of 64 is
used and the model is trained for 20 epochs. The margin, δ, is set to 0.4.
6Weuse[·] todenotefunctionh:h(x)=max(x,0).
+
7https://github.com/google/sentencepiece
8SeeWietingetal.(2019,2022)formoredetailsonencodingspeed.
21
Inference At the inference stage, for a given pair of a submission and a reviewer, we define the similarity
score as a combination of cosine similarities between the submission’s abstract and three most-similar ab-
stracts from the reviewer’s profile. Specifically, let s ,s and s be the top-3 cosine similarities between the
1 2 3
submission’sabstractandabstracts fromthereviewer’sprofile. Thesimilarity scorebetweenthesubmission
and the reviewer is then defined as follows:
s s
s=s + 2 + 3.
1 2 3
If a reviewer has less than three abstracts in the profile, we set the corresponding cosine similarity scores s
i
to be zero.
22

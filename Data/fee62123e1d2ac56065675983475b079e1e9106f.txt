The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
A Continuous Relaxation of Beam Search for
End-to-End Training of Neural Sequence Models
KartikGoyal GrahamNeubig
CarnegieMellonUniversity CarnegieMellonUniversity
kartikgo@cs.cmu.edu gneubig@cs.cmu.edu
ChrisDyer TaylorBerg-Kirkpatrick
Deepmind CarnegieMellonUniversity
cdyer@google.com tberg@cs.cmu.edu
Abstract Choetal.2014).Thesenegativeresultsarenotunexpected.
Thetrainingprocedurewasnotsearch-aware:itwasnotable
Beamsearchisadesirablechoiceoftest-timedecodingalgo-
toconsidertheeffectthatchangingthemodel’sscoresmight
rithmforneuralsequencemodelsbecauseitpotentiallyavoids
have on the ease of search while using a beam decoding,
searcherrorsmadebysimplergreedymethods.However,typ-
greedydecoding,orotherwise.
icalcrossentropytrainingproceduresforthesemodelsdonot
directlyconsiderthebehaviourofthefinaldecodingmethod. Wehypothesizethattheunder-performanceofbeamsearch
Asaresult,forcross-entropytrainedmodels,beamdecoding
incertainscenarioscanberesolvedbyusingabetterdesigned
can sometimes yield reduced test performance when com-
training objective. Because beam search potentially offers
paredwithgreedydecoding.Inordertotrainmodelsthatcan
more accurate search when compared to greedy decoding,
moreeffectivelymakeuseofbeamsearch,weproposeanew
we hope that appropriately trained models should be able
trainingprocedurethatfocusesonthefinallossmetric(e.g.
Hammingloss)evaluatedontheoutputofbeamsearch.While toleveragebeamsearchtoimproveperformance.Inorder
well-defined,this“directloss”objectiveisitselfdiscontinuous totrainmodelsthatcanmoreeffectivelymakeuseofbeam
and thus difficult to optimize. Hence, in our approach, we search, we propose a new training procedure that focuses
formasub-differentiablesurrogateobjectivebyintroducing on the final loss metric (e.g. Hamming loss) evaluated on
anovelcontinuousapproximationofthebeamsearchdecod- theoutputofbeamsearch.Whilewell-definedandavalid
ingprocedure.Inexperiments,weshowthatoptimizingthis trainingcriterion,this“directloss”objectiveisdiscontinuous
newtrainingobjectiveyieldssubstantiallybetterresultson
and thus difficult to optimize. Hence, in our approach, we
twosequencetasks(NamedEntityRecognitionandCCGSu-
formasub-differentiablesurrogateobjectivebyintroducing
pertagging)whencomparedwithbothcrossentropytrained
anovelcontinuousapproximationofthebeamsearchdecod-
greedy decoding and cross entropy trained beam decoding
ingprocedure.Inexperiments,weshowthatoptimizingthis
baselines.
newtrainingobjectiveyieldssubstantiallybetterresultson
twosequencetasks(NamedEntityRecognitionandCCGSu-
1 Introduction
pertagging)whencomparedwithbothcross-entropytrained
Sequence-to-sequence (seq2seq) models have been suc- greedydecodingandcross-entropytrainedbeamdecoding
cessfully used for many sequential decision tasks such baselines.
as machine translation (Sutskever, Vinyals, and Le 2014;
Several related methods, including reinforcement learn-
Bahdanau,Cho,andBengio2015),parsing(Dyeretal.2016;
ing (Ranzato et al. 2016; Bahdanau et al. 2017), imitation
2015), summarization (Rush, Chopra, and Weston 2015),
learning(Daumé,Langford,andMarcu2009;Ross,Gordon,
dialog generation(Serban etal. 2015), andimage caption-
andBagnell2011;Bengioetal.2015),anddiscretesearch
ing (Xu et al. 2015). Beam search is a desirable choice of
basedmethods(WisemanandRush2016;Andoretal.2016;
test-timedecodingalgorithmforsuchmodelsbecauseitpo-
Daumé III and Marcu 2005; Gormley, Dredze, and Eisner
tentiallyavoidssearcherrorsmadebysimplergreedymeth-
2015), have also been proposed to make training search-
ods. However, the typical approach to training neural se-
aware.Thesemethodsincludeapproachesthatforgodirect
quencemodelsistousealocallynormalizedmaximumlike-
optimizationofaglobaltrainingobjective,insteadincorpo-
lihoodobjective(cross-entropytraining)(Sutskever,Vinyals,
ratingcreditassignmentforsearcherrorsbyusingmethods
andLe2014).Thisobjectivedoesnotdirectlyreasonabout
likeearlyupdates(CollinsandRoark2004)thatexplicitly
the behaviour of the final decoding method. As a result,
trackthereachabilityofthegoldtargetsequenceduringthe
forcross-entropytrainedmodels,beamdecodingcansome-
searchprocedure.Whileaddressingarelatedproblem–credit
timesyieldreducedtestperformancewhencomparedwith
assignmentforsearcherrorsduringtraining–inthispaper,
greedydecoding(KoehnandKnowles2017;Neubig2017;
weproposeanapproachwithanovelproperty:wedirectly
Copyright(cid:2)c 2018,AssociationfortheAdvancementofArtificial optimize a continuous and global training objective using
Intelligence(www.aaai.org).Allrightsreserved. backpropagation.Asaresult,inourapproach,creditassign-
3045
Algorithm1StandardBeamSearch
1: Initialize:
h
0,i
←(cid:2)0,e
0,i
←embedding(<s>),s
0,i
←0,i=1,...,k
2: fort=0toTdo
3: fori=1tokdo
4: forallv ∈V do
5: s˜ t[i,v]←s t,i+f(h t,i,v) (cid:3)f isthelocaloutputscoringfunction
6: s t+1 ←top-k-max(s˜ t) (cid:3)Topkvaluesoftheinputmatrix
7: bt+1,∗,y t,∗ ←top-k-argmax(s˜ t) (cid:3)Topkargmaxindexpairsoftheinputmatrix
8: fori=1tokdo
9: e t+1,i ←embedding(y t,i)
10: h t+1,i ←r(h t,i,e t+1,i) (cid:3)risanonlinearrecurrentfunctionthatreturnsstateatnextstep
11: yˆ←follow-backpointer((b 1,∗,y 1,∗),...,(b T,∗,y T,∗))
12: s(yˆ)←max(s T)
mentishandleddirectlyviagradientoptimizationinanend- accomplishthis,weproposeacontinuousrelaxationtothe
to-endcomputationgraph.Themostcloselyrelatedworkto composition of our final loss metric, L, and our decoder
ourownapproachwasproposedbyGoyal,Dyer,andBerg- function,Beam:
Kirkpatrick(2017).Theydonotconsiderbeamsearch,but softLB(x,M(θ),y∗)≈(L◦Beam)(x,M(θ),y∗)
developacontinuousapproximationofgreedydecodingfor
scheduledsamplingobjectives.Otherrelatedworkinvolves Specifically,weformacontinuousfunctionsoftLBthatseeks
trainingageneratorwithaGumbelreparamterizedsampling toapproximatetheresultofrunningourdecoderoninputx
moduletomorereliablyfindtheMAPsequencesatdecode- andthenevaluatingtheresultagainsty∗ usingL.Byintro-
time(Gu,Im,andLi2017),andconstructingsurrogateloss ducingthisnewmodule,wearenowabletoconstructour
functions(Bahdanauetal.2016)thatareclosetotasklosses. surrogatetrainingobjective:
2 Model
minG˜ DL(x,θ,y∗)=minsoftLB(x,M(θ),y∗)
(2)
θ θ
Wedenotetheseq2seqmodelparameterizedbyθasM(θ).
SpecifiedinmoredetailinSection2.3,oursurrogateob-
We denote the input sequence as x, the gold output se- jectiveinEquation2willadditionallytakeahyperparameter
quence as y∗ and the result of beam search over M(θ) as α that trades approximation quality for smoothness of the
yˆ=Beam(x,M(θ)).Ideally,wewouldliketodirectlymin-
objective.Undercertainconditions,Equation2convergesto
imizeafinalevaluationloss,L(yˆ,y∗),evaluatedonthere- theobjectiveinEquation1asαisincreased.Wefirstdescribe
sultofrunningbeamsearchwithinputxandmodelM(θ).
thestandarddiscontinuousbeamsearchprocedureandthen
Throughoutthispaperweassumethattheev (cid:2)aluationlossde- ourtrainingapproach(Equation2)involvingacontinuous
composesovertimestepstas:L(yˆ,y∗)= T t=1d(yˆ t,y∗)1. relaxationofbeamsearch.
Werefertothisidealizedtrainingobjectivethatdirectlyeval-
uatespredictionlossasthe“directloss”objectiveanddefine 2.1 DiscontinuityinBeamSearch
itas: Formally,beamsearchisaprocedurewithhyperparameter
minG (x,θ,y∗)=minL(Beam(x,M(θ)),y∗) (1)
kthatmaintainsabeamofkelementsateachtimestepand
θ
DL
θ
expandseachofthekelementstofindthek-bestcandidates
forthenexttimestep.Theprocedurefindsanapproximate
Unfortunately,optimizingthisobjectiveusinggradientmeth-
argmaxofascoringfunctiondefinedonoutputsequences.
odsisdifficultbecausetheobjectiveisdiscontinuous.The
Wedescribebeamsearchinthecontextofseq2seqmod-
twosourcesofdiscontinuityare:
els in Algorithm 1 – more specifically, for an encoder-
1. Aswedescribelaterinmoredetail,beamsearchdecoding decoder (Sutskever, Vinyals, and Le 2014) model with a
(referredtoasthefunctionBeam)involvesdiscreteargmax nonlinearauto-regressivedecoder(e.g.anLSTM(Hochre-
decisionsandthusrepresentsadiscontinuousfunction. iter and Schmidhuber 1997)). We define the global model
2. The output, yˆ, of the Beam function, which is the input score of a sequence y with length T to be the sum of lo-
to the loss function, L(yˆ,y∗), is discrete and hence the cal outpu (cid:2)t scores at each time step of the seq2seq model:
evaluationofthefinallossisalsodiscontinuous. s(y) = T t=1f(h t,y t). In neural models, the function f
is implemented as a differentiable mapping, R|h| → R|V|,
Weintroduceasurrogatetrainingobjectivethatavoidsthese
whichyieldsscoresforvocabularyelementsusingtherecur-
problems and as a result is fully continuous. In order to
renthiddenstatesatcorrespondingtimesteps.Inournotation,
1This assumption does not hold for some popular evaluation
h t,iisthehiddenstateofthedecoderattimesteptforbeam
metrics (e.g. BLEU). In these cases, surrogate evaluation losses elementi,e t,iistheembeddingoftheoutputsymbolattime-
suchasHammingdistancecanbeused. steptforbeamelementi,ands t,i isthecumulativemodel
3046
Algorithm2continuous-top-k-argmax
1: Inputs:
s∈Rk×|V|
2: Outputs: (cid:2)
p
i
∈Rk×|V|,s.t. jp
ij
=1,i=1,...,k
3: m∈Rk =top-k-max(s)
4: fori=1tokdo (cid:3)peaked-softmaxwillbedominatedbyscoresclosertom i
5: p i =peaked-softmax α(−(s−m i·1)2) (cid:3)Thesquareoperationiselement-wise
scoreatsteptforbeamelementi.InAlgorithm1,wedenote Asα → ∞,z˜→ zˆsolongasthereisonlyonemaximum
bys˜
t
∈Rk×|V|thecumulativecandidatescorematrixwhich value in the vector z. This peaked-softmax operation has
representsthemodelscoreofeachsuccessorcandidateinthe beenshowntobeeffectiveinrecentwork(Maddison,Mnih,
vocabularyforeachbeamelement.Thisscoreisobtainedby andTeh2017;Jang,Gu,andPoole2016;Goyal,Dyer,and
addingthelocaloutputscore(computedasf(h t,i,w))tothe Berg-Kirkpatrick2017)involvingcontinuousrelaxationto
runningtotalofthescoreforthecandidate.Thefunctionrin the argmax operation, although to our knowledge, this is
Algorithms1and3yieldssuccessivehiddenstatesinrecur- the first work to apply it to approximate the beam search
rentneuralmodelslikeRNNs,LSTMsetc.Theembedding procedure.
operation maps a word in the vocabulary V, to a continu- Usingthispeaked-softmaxoperation,weproposeanitera-
ous embedding vector. Finally, backpointers at each time tivealgorithmforcomputingacontinuousrelaxationtothe
steptothebeamelementsattheprevioustimesteparealso top-k-argmaxprocedureinAlgorithm2whichtakesasinput
storedforidentifyingthebestsequenceyˆ,attheconclusion ascorematrixofsizek×|V|andreturnskpeakedmatrices
ofthesearchprocedure.Abackpointerattimesteptfora pofsizek×|V|.Eachmatrixp irepresentstheindexofi-th
beamelementiisdenotedbybt,i ∈{1,...,k}whichpoints max.Forexample,p 1willhavemostofitsmassconcentrated
to one of the k elements at the previous beam. We denote on the index in the matrix that corresponds to the argmax,
avectorofbackpointersforallthebeamelementsbybt,∗. whilep 2willhavemostofitsmassconcentratedontheindex
Thefollow-backpointeroperationtakesasinputbackpointers ofthe2nd-highestscoringelement.Specifically,weobtain
(b t,∗)andcandidates(y t,∗ ∈{1,...,|V|}k)forallthebeam matrixp i bycomputingthesquareddifferencebetweenthe
elementsateachtimestepandtraversesthesequenceinre- i-highestscoreandallthescoresinthematrixandthenusing
verse(fromtime-stepT through1)followingbackpointers thepeaked-softmaxoperationoverthenegativesquareddif-
ateachtimestepandidentifyingcandidatewordsassociated ferences.Thisresultsinscoresclosertothei-highestscore
witheachbackpointerthatresultsinasequenceyˆ,oflength tohaveahighermassthanscoresfarawayfromthei-highest
T. score.
TheproceduredescribedinAlgorithm1isdiscontinuous Hence, the continuous relaxation to top-k-argmax oper-
becauseofthetop-k-argmaxprocedurethatreturnsapairof ation can be simply implemented by iteratively using the
vectors corresponding to the k highest-scoring indices for maxoperationwhichiscontinuousandallowsforgradient
backpointersandvocabularyitemsfromthescorematrixs˜ t. flowduringbackpropagation.Asα→∞,eachpvectorcon-
Thisindexselectionresultsinhardbackpointersateachtime vergestohardindexpairsrepresentinghardbackpointersand
stepwhichrestrictthegradientflowduringbackpropagation. successorcandidatesdescribedinAlgorithm1.Forfiniteα,
Inthenextsection,wedescribeacontinuousrelaxationto weintroduceanotionofasoftbackpointer,representedasa
the top-k-argmax procedure which forms the crux of our vector˜b∈Rk inthek-probabilitysimplex,whichrepresents
approach. thecontributionofeachbeamelementfromtheprevioustime
steptoabeamelementatcurrenttimestep.Thisisobtained
2.2 ContinuousApproximationtotop-k-argmax byarow-wisesumoverptogetk valuesrepresentingsoft
Thekeypropertythatweuseinourapproximationisthatfor backpointers.
arealvaluedvectorz,theargmaxwithrespecttoavectorof
scores,s,canbeapproximatedbyatemperaturecontrolled
2.3 TrainingwithContinuousRelaxationof
softmaxoperation.Theargmaxoperationcanberepresented
BeamSearch
as:
(cid:3)
zˆ= z (cid:0)[∀i(cid:3) (cid:8)=i, s >s ], We describe our approach in detail in Algorithm 3 and
i i i(cid:2)
illustrate the soft beam recurrence step in Figure 1. For
i
composingthelossfunctionandthebeamsearchfunction
whichcanberelaxedbyreplacingtheindicatorfunctionwith
for our optimization as proposed in Equation 2, we make
apeaked-softmaxoperationwithhyperparameterα:
use of decomposability of the loss function across time-
z˜=(cid:3)
z
(cid:2)exp(αs i)
=zT ·
(cid:2)elem-exp(αs) (cid:2)steps. Thus for a sequence y, the total loss is: L(y,y∗) =
i
i i(cid:2)exp(αs i(cid:2)) i(cid:2)exp(αs i(cid:2)) T t=1d(y t). In our experiments, d(y t) is the Hamming
loss which can be easily computed at each time-step by
=zT ·peaked-softmax α(s) simply comparing gold y t∗ with y t. While exact compu-
3047
Algorithm3Continuousrelaxationtobeamsearch
1: Initialize:
h
0,i
←(cid:2)0,e
0,i
←embedding(<s>),s
0,i
←0,D
t
∈Rk ←(cid:2)0,i=1,...,k
2: fort=0toTdo
3: forallw ∈V do
4: fori=1tokdo
5: s˜ t[i,w]←s t,i+f(h t,i,w) (cid:3)f isalocaloutputscoringfunction
6: D˜ t,w =d(w) (cid:3)D˜ tisusedtocomputeD t+1
7: p 1,...,p k ←continuous-top-k-argmax(s˜ t) (cid:3)CallAlgorithm2
8: fori=1tokdo
9: ˜b t,i ←row_sum(p i) (cid:3)Softbackpointercomputation
10: a i ∈R|V| ←column_sum(p i) (cid:3)Contributionfromvocabularyitems
11: e t+1,i ←aT i ×E (cid:3)Peakeddistributionoverthecandidatestocomputee,D,S
12: D t+1,i ←aT i ·D˜ t
13: s t+1,i =sum(s˜ t(cid:10)p i)
14:
h˜
t,i
←(cid:2)0
15: forj=1tokdo (cid:3)Getcontributionsfromsoftbackpointersforeachbeamelement
16: h˜ t,i+=h t,j ∗˜b t,i[j]
17: D t+1,i+=D t,j ∗˜b t,i[j]
18: h t+1,i ←r(h˜ t,i,e t+1,i) (cid:3)risanonlinearrecurrentfunctionthatreturnsstateatnextstep
19: L=peaked-softmax α(s T)·D T (cid:3)Pickthelossforthesequencewithhighestmodelscoreonthebeaminasoftmanner.
tation of d(y) will vary according to the loss, our pro- initializationofweightsandthedomainofthescoresbeing
posedprocedurewillbeapplicableaslongasthetotalloss R. Empirically, we did not observe any noticeable impact
is decomposable across time-steps. While decomposabil- ofpotentialtiesonthetrainingprocedureandourapproach
ity of loss is a strong assumption, existing literature on performedwellonthetasksasdiscussedinSection4.
Tst sr ou cc htu ar ne td arp idr ie sd eic tt aio l.n 20(T 0a 5s )k ha ar, sG mu ae ds etr din u, ea wn id thK tho il sle ar ss2 u0 m04 p;
-
G˜ DL,α(x,θ,y∗)−α−→−−∞→G DL(x,θ,y∗)
(3)
p
tion,oftenusingdecomposablelossesassurrogatesfornon-
Weexperimentedwithdifferentannealingschedulesforα
decomposableones.Wedetailthecontinuousrelaxationto
beamsearchinAlgorithm3withD t,ibeingthecumulative starting with non-peaked softmax moving toward peaked-
lossofbeamelementiattimesteptandE beingtheembed- softmaxacrossepochssothatlearningisstablewithinforma-
dingmatrixofthetargetvocabularywhichisofsize|V|×l tivegradients.Thisisimportantbecausecostfunctionslike
wherelisthesizeoftheembeddingvector.
Hammingdistancewithveryhighαtendtobenon-smooth
andaregenerallyflatinregionsfarawayfromchangepoints
InAlgorithm3,allthediscreteselectionfunctionshave andhaveaverylargegradientnearthechangepointswhich
beenreplacedbytheirsoft,continuouscounterpartswhich makesoptimizationdifficult.
can be backpropagated through. This results in all the op-
2.4 Decoding
erationsbeingmatrixandvectoroperationswhichisideal
foraGPUimplementation.Animportantaspectofthisal- Themotivationbehindourapproachistomaketheoptimiza-
gorithm is that we no longer rely on exactly identifying a tionawareofbeamsearchdecodingwhilemaintainingthe
discretesearchpredictionyˆsinceweareonlyinterestedin continuity of the objective. However, since our approach
acontinuousapproximationtothedirectlossL(line18of doesn’tintroduceanynewmodelparametersandoptimiza-
Algorithm3),andallthecomputationisexpressedviathe tionisagnostictothearchitectureoftheseq2seqmodel,we
softbeamsearchformulationwhicheliminatesallthesources wereabletoexperimentwithvariousdecodingschemeslike
of discontinuities associated with the training objective in locallynormalizedgreedydecoding,andhardbeamsearch,
Equation1.Thecomputationalcomplexityofourapproach oncethemodelhasbeentrained.
fortrainingscaleslinearlywiththebeamsizeandhenceis However,toreducethegapbetweenthetrainingprocedure
roughlyktimesslowerthanstandardCEtrainingforbeam and test procedure, we also experimented with soft beam
sizek.Sincewehaveestablishedthepointwiseconvergence search decoding. This decoding approach closely follows
ofpeaked-softmaxtoargmaxasα→∞forallvectorsthat Algorithm3,butalongwithsoftbackpointers,wealsocom-
haveauniquemaximumvalue,wecanestablishpointwise putehardbackpointersateachtimestep.Aftercomputingall
convergenceofobjectiveinEquation2toobjectiveinEqua- therelevantquantitieslikemodelscore,lossetc.,wefollow
tion 1 as α → ∞, as long as there are no ties among the thehardbackpointerstoobtainthebestsequenceyˆ.Thisis
top-kscoresofthebeamexpansioncandidatesatanytime verydifferentfromhardbeamdecodingbecauseateachtime
step.Wepositthatabsolutetiesareunlikelyduetorandom step,theselectiondecisionsaremadeviaoursoftcontinuous
3048
p
1
h t,1 s˜ t continuous-top-k-argmax Beam Recurrence h t+1,1
etadidnaC rosseccusserocs Beam Recurrence
e
t+1,2
h
t,2
Column-sum
p Mat_mul h
2 t+1,2
Embedding
Row-sum
˜b
t,2
h˜
t,2
Weighted Linear Combination
Figure1:Illustrationofourapproximatecontinuousbeamsearch(Algorithm3)moduletoobtainhiddenstatesforbeamelements
atthenexttimestep(h t+1,∗),startingfromthehiddenstatescorrespondingtobeamelementsarecurrenttimestep(h t,∗)with
beamsizeof2.‘Beamrecurrence’modulehasbeenexpandedforh t+1,2andsimilarprocedureiscarriedoutforh t+1,1.
relaxationwhichinfluencesthescores,LSTMhiddenstates ing,andhencedoesnotinvolvebackpropagationthrough
and input embeddings at subsequent time-steps. The hard thebeamsearchprocedureitself.
backpointers are essentially the MAP estimate of the soft Our continuous approximation to beam search can very
backpointersateachstep.Withsmall,finiteα,weobserve easilybemodifiedtocomputeanapproximationtothestruc-
differencesbetweensoftbeamsearchandhardbeamsearch turedhingelosssothatitcanbetrainedviabackpropagation
decodinginourexperiments. if the cost function is decomposable across time-steps. In
Algorithm3,weonlyneedtomodifyline5as:
2.5 ComparisonwithMax-MarginObjectives
s˜[i,w]←s +d(w)+f(h ,w)
t t,i t,i
Max-marginbasedobjectivesaretypicallymotivatedasan-
otherkindofsurrogatetrainingobjectivewhichavoidthedis- andinsteadofcomputingLinAlgorithm3,wefirstcompute
continuitiesassociatedwithdirectlossoptimization.Hinge thecostaugmentedmaximumscoreas:
lossforstructuredpredictiontypicallytakestheform:
s
max
=peaked-softmax α(s T)·s
T
G =max(0,max(Δ(y,y∗)+s(y))−s(y∗))
hinge
y∈Y
andalsocomputethetargetscores(y∗)bysimplyrunning
theforwardpassoftheLSTMdecoderoverthegoldtarget
wherexistheinputsequence,y∗isthegoldtargetsequence,
sequence.Thecontinuousapproximationtothehingeloss
Y oui ss ct oh se to fuu ntp cu tit os nea wr hch ichsp wac ee aa sn sud mΔ e( iy s, dy e∗ c) oi mst ph oe sad bis leco an ct ri on su s- to be optimized is then: G˜ hinge,α = max(0,s max −s(y∗)).
We empirically compare this approach with the proposed
the time-steps of a sequence. Finding the cost augmented
approachtooptimizedirectlossinexperiments.
maximumscoreisgenerallydifficultinlargestructuredmod-
elsandofteninvolvessearchingovertheoutputspaceand
3 ExperimentalSetup
computingtheapproximatecostaugmentedmaximaloutput
sequenceandthescoreassociatedwithitviabeamsearch. Sinceourgoalistoinvestigatetheefficacyofourapproach
Thisprocedureintroducesdiscontinuitiesinthetrainingpro- fortraininggenericseq2seqmodels,weperformexperiments
cedure of structured max-margin objectives and renders it ontwoNLPtaggingtaskswithverydifferentcharacteristics
nonamenabletotrainingviabackpropagation.Relatedwork andoutputsearchspaces:NamedEntityRecognition(NER)
(WisemanandRush2016)onincorporatingbeamsearchinto and CCG supertagging. While seq2seq models are appro-
the training of neural sequence models does involve cost- priateforCCGsupertaggingtaskbecauseofthelong-range
augmented max-margin loss but it relies on discontinuous correlations between the sequential output elements and a
beamsearchforwardpassesandanexplicitmechanismto largesearchspace,theyarenotidealforNERwhichhasa
ensurethatthegoldsequencestaysinthebeamduringtrain- considerablysmallersearchspaceandweakercorrelations
3049
betweenpredictionsatsubsequenttimesteps.Inourexperi- withthebestperformanceonthedevelopmentset.Wealso
ments,weobserveimprovementsfromourapproachonboth ranmultiplerandomrestartsforallthesystemsevaluatedto
ofthetasks.Weuseaseq2seqmodelwithabi-directional account for performance variance across randomly started
LSTMencoder(1layerwithtanhactivationfunction)forthe runs.Wepretrainedallourmodelswithstandardcrossen-
inputsequencex,andanLSTMdecoder(1layerwithtanh tropytrainingwhichwasimportantforstableoptimization
activationfunction)withafixedattentionmechanismthatde- of the non convex neural objective with a large parameter
terministicallyattendstothei-thinputtokenwhendecoding search space. This warm starting is a common practice in
thei-thoutput,andhencedoesnotinvolvelearningofany priorworkoncomplexneuralmodels(Ranzatoetal.2016;
attentionparameters.Since,computationalcomplexityofour Rush,Chopra,andWeston2015;Bengioetal.2015).
approachforoptimizationscaleslinearlywithbeamsizefor
eachinstance,itisimpracticaltouseverylargebeamsizes 3.4 Comparison
fortraining.Hence,beamsizeforallthebeamsearchbased
Wereportperformanceonvalidationandtestsetsforboththe
experimentswassetto3whichresultedinimprovementson
tasksinTables1and 2.Thebaselinemodelisacrossentropy
boththetasksasdiscussedintheresults.Forbothtasks,the
trainedseq2seqmodel(BaselineCE)whichisalsousedto
directlossfunctionwastheHammingdistancecostwhich
warmstartthetheproposedoptimizationproceduresinthis
aimstomaximizewordlevelaccuracy.
paper.Thisbaselinehasbeencomparedagainsttheapprox-
imate direct loss training objective (Section 2.3), referred
3.1 NamedEntityRecognition toasG˜
DL,α inthetables,andtheapproximatemax-margin
For named entity recognition, we use the CONLL 2003 trainingobjective(Section2.5),referredtoasG˜ hinge,αinthe
shared task data (Tjong Kim Sang and De Meulder 2003) tables. Results are reported for models when trained with
forGermanlanguageandusetheprovideddatasplits.We annealing α, and also with a constant setting of α = 1.0
perform no preprocessing on the data. The output vocabu- whichisaverysmoothbutinaccurateapproximationofthe
larysize(labelspace)is10.Apeculiarcharacteristicofthis originaldirectlossthatweaimtooptimize2.Comparisons
problemisthatthetrainingdataisnaturallyskewedtoward havebeenmadeonthebasisofperformanceofthemodels
one default label (‘O’) because sentences typically do not under different decoding paradigms (represented as differ-
containmanynamedentitiesandtheevaluationfocuseson entcolumninthetables):locallynormalizeddecoding(CE
theperformancerecognizingentities.Therefore,wemodify greedy),hardbeamsearchdecodingandsoftbeamsearch
the Hamming cost such that incorrect prediction of ‘O’ is decodingdescribedinSection2.4.
doubly penalized compared to other incorrect predictions.
Weusethehiddenlayersofsize64andlabelembeddingsof
4 Results
size8.Asmentionedearlier,seq2seqmodelsarenotanideal
choice for NER (tag-level correlations are short-ranged in
AsshowninTables1and2,ourapproachG˜
DL,αshowssignif-
NER–theunnecessaryexpressivityoffullseq2seqmodels icantimprovementsoverthelocallynormalizedCEbaseline
oversimpleencoder-classifierneuralmodelsmakestraining withgreedydecodingforboththetasks(+5.5accuracypoints
harder).However,wewantedtoevaluatetheeffectivenessof gainforsupertaggingand+1.5F1pointsforNER).Theim-
ourapproachondifferentinstantiationsofseq2seqmodels. provement is more pronounced on the supertagging task,
whichisnotsurprisingbecause:(i)theevaluationmetricis
3.2 CCGSupertagging tag-levelaccuracywhichiscongruentwiththeHammingloss
We used the standard splits of CCG bank (Hockenmaier
thatG˜
DL,αdirectlyoptimizesand(ii)thesupertaggingtask
itselfisverysensitivetothesearchprocedurebecausetags
andSteedman2002)fortraining,development,andtesting.
across time-steps tend to exhibit long range dependencies
Thelabelspaceofsupertagsis1,284whichismuchlarger
astheyencodespecializedsyntacticinformationaboutword
thanNER.Thedistributionofsupertagsinthetrainingdata
usageinthesentence.
exhibitsalongtailbecausethesesupertagsencodespecific
Another common trend to observe is that annealing α
syntacticinformationaboutthewords’usage.Thesupertag
always results in better performance than training with a
l sa imbe il ls ara ir ne fc oo rmrre al ta iote nd aw boit uh te tha ech syo nth tae xr .a Mnd orm eoan vy er,ta tg hs ise tn ac so kd ie
s
constantα=1.0forbothG˜
DL,α
(Section2.3)andG˜
hinge,α
(Section2.5).Thisshowsthatastabletrainingschemethat
sensitivetothelongrangesequentialdecisionsandsearch
smoothly approaches minimizing the actual direct loss is
effectsbecauseofhowitholisticallyencodesthesyntaxofthe
importantforourproposedapproach.Additionally,wedid
entiresentence.Weperformminorpreprocessingonthedata
notobservealargedifferencewhenoursoftapproximation
similartothepreprocessinginVaswanietal.(2016).Forthis
isusedfordecoding(Section2.4)comparedtohardbeam
task,weusedhiddenlayersofsize512andthesupertaglabel
searchdecoding,whichsuggeststhatourapproximationto
embeddingswerealsoofsize512.Thestandardevaluation
thehardbeamsearchisaseffectiveasitsdiscretecounterpart.
metricforthistaskisthewordlevellabelaccuracywhich
Forsupertagging,weobservethatthebaselinecrossen-
directlycorrespondstoHammingloss.
tropy trained model improves its predictions with beam
searchdecodingcomparedtogreedydecodingby2accuracy
3.3 Hyperparametertuning
For tuning all the hyperparameters related to optimization 2Ourpilotexperimentsthatinvolvedtrainingwithaverylarge
wetrainedourmodelsfor50epochsandpickedthemodels constantαresultedinunstableoptimization.
3050
Trainingprocedure Greedy HardBeamSearch SoftBeamSearch
Dev Test Dev Test Dev Test
BaselineCE 80.15 80.35 82.17 82.42 81.62 82.00
G˜ hinge,αannealedα - - 83.03 83.54 82.82 83.05
G˜ hinge,αα=1.0 - - 83.02 83.36 82.49 82.85
G˜ DL,αα=1.0 - - 83.23 82.65 82.58 82.82
G˜ DL,αannealedα - - 85.69 85.82 85.58 85.78
Table1:ResultsonCCGSupertagging.Tag-levelaccuracyisreportedinthistablewhichisastandardevaluationmetricfor
supertagging.
Trainingprocedure CEGreedy HardBeamSearch SoftBeamSearch
Dev Test Dev Test Dev Test
BaselineCE 50.21 54.92 46.22 51.34 47.50 52.78
G˜ hinge,αannealedα - - 41.10 45.98 41.24 46.34
G˜ hinge,αα=1.0 - - 40.09 44.67 39.67 43.82
G˜ DL,αα=1.0 - - 49.88 54.08 50.73 54.77
G˜ DL,αannealedα - - 51.86 56.15 51.96 56.38
Table2:ResultsonNamedEntityRecognition.MacroF1overthepredictionofdifferentnamedentitiesisreportedthatisa
standardevaluationmetricforthistask.
points,whichsuggeststhatbeamsearchisalreadyhelpful searchmethodslikebeamsearchthatdonotprovideguaran-
forthistask,evenwithoutsearch-awaretraining.Boththe teesonfindingthesupremum:onewaytodrivethisobjective
optimizationschemesproposedinthispaperimproveupon down is to learn model scores such that the search for the
thebaselinewithsoftdirectlossoptimization(G˜
DL,α),per- besthypothesisisdifficult,sothatthevalueofthelossaug-
formingbetterthantheapproximatemax-marginapproach. mented decode is low, while the gold sequence maintains
3 higher model score. Because we also warm started with a
ForNER,weobservethatoptimizingG˜ DL,αoutperforms pre-trainedmodelthatresultsinaworseperformancewith
alltheotherapproachesbutwealsoobserveinterestingbe- beamsearchdecodethanwithgreedydecode,weobservethe
haviourofbeamsearchdecodingandtheapproximatemax- adverseeffectofthisdeficiency.Theresultisamodelthat
margin objective for this task. The pretrained CE baseline scoresthegoldhypothesishighly,butyieldspoordecoding
modelyieldsworseperformancewhenbeamsearchisdone outputs.Thisobservationindicatesthatusingmax-margin
instead of greedy locally normalized decoding. This is be- basedobjectiveswithbeamsearchduringtrainingactually
causethetrainingdataisheavilyskewedtowardthe‘O’label mayachievetheoppositeofouroriginalintent:theobjective
and hence the absolute score resolution between different canbedrivendownbyintroducingsearcherrors.
tagsateachtime-stepduringdecodingisn’tenoughtoavoid Theobservationthatouroptimizationmethodledtoim-
leading beam search toward a wrong hypothesis path. We provementsonboththetasks–evenonNERforwhichhard
observedinourexperimentsthathardbeamsearchresulted beam search during decoding on a CE trained model hurt
inpredictingmore‘O’swhichalsohurtthepredictionoftags the performance–by making the optimization more search
atfuturetimestepsandhurtprecisionaswellasrecall.En- aware,indicatestheeffectivenessofourapproachfortraining
couragingly,G˜ DL,α optimization,eventhoughwarmstarted seq2seqmodels.
with a CE trained model that performs worse with beam
search,ledtotheNERmodelbecomingmoresearchaware, 5 Conclusion
whichresultedinsuperiorperformance.However,wealsoob-
servethattheapproximatemax-marginapproach(G˜
hinge,α)
W seah ri cle hb inea nm eus re aa lr sc eh quis ena cem met oh do ed lso ,f asch oo ui rce exf po er rip mer ef no tr sm ci on ng
-
performs poorly here. We attribute this to a deficiency in
firm, it is not necessarily guaranteed to improve accuracy
themax-marginobjectivewhencoupledwithapproximate
whenappliedtocross-entropy-trainedmodels.Inthispaper,
weproposeanovelmethodforoptimizingmodelparameters
3Separately,wealsoranexperimentswithamax-marginobjec-
thatdirectlytakesintoaccounttheprocessofbeamsearch
tivethatusedhardbeamsearchtocomputeloss-augmenteddecodes.
itself through a continuous, end-to-end sub-differentiable
Thisobjectiveisdiscontinuous,butweevaluatedtheperformance
relaxationofbeamsearchcomposedwiththefinalevalua-
of gradient optimization nonetheless. While not included in the
resulttables,wefoundthatthisapproachwasunstableandconsider- tionloss.Experimentsdemonstratethatourmethodisable
ablyunderperformedbothapproximatemax-marginanddirectloss toimproveoveralltest-timeresultsformodelsusingbeam
objectives. searchasatest-timeinferencemethod,leadingtosubstantial
3051
improvementsinaccuracy. Hochreiter,S.,andSchmidhuber,J. 1997. Longshort-term
memory. Neuralcomputation9(8):1735–1780.
6 Acknowledgement
Hockenmaier,J.,andSteedman,M.2002.Acquiringcompact
Wethanktheanonymousreviewersfortheirfeedback.This lexicalizedgrammarsfromacleanertreebank. InLREC.
projectisfundedinpartbytheNSFundergrant1618044. Jang,E.;Gu,S.;andPoole,B. 2016. Categoricalreparame-
terizationwithgumbel-softmax. InInternationalConference
References
onLearningRepresentations.
Andor, D.; Alberti, C.; Weiss, D.; Severyn, A.; Presta, A.; Koehn,P.,andKnowles,R. 2017. Sixchallengesforneural
Ganchev, K.; Petrov, S.; and Collins, M. 2016. Globally machinetranslation. arXivpreprintarXiv:1706.03872.
normalizedtransition-basedneuralnetworks. InAssociation
Maddison, C. J.; Mnih, A.; and Teh, Y. W. 2017. The
forComputationalLinguistics.
concrete distribution: A continuous relaxation of discrete
Bahdanau,D.;Serdyuk,D.;Brakel,P.;Ke,N.R.;Chorowski,
randomvariables. InInternationalConferenceonLearning
J.;Courville,A.;andBengio,Y. 2016. Tasklossestimation
Representations.
forstructuredprediction.
Neubig, G. 2017. Neural machine translation and
Bahdanau, D.; Brakel, P.; Xu, K.; Goyal, A.; Lowe, R.;
sequence-to-sequence models: A tutorial. arXiv preprint
Pineau, J.; Courville, A.; and Bengio, Y. 2017. An actor-
arXiv:1703.01619.
critic algorithm for sequence prediction. In International
Ranzato,M.;Chopra,S.;Auli,M.;andZaremba,W. 2016.
ConferenceonLearningRepresentations.
Sequenceleveltrainingwithrecurrentneuralnetworks. In
Bahdanau,D.;Cho,K.;andBengio,Y. 2015. Neuralma-
InternationalConferenceonLearningRepresentations.
chinetranslationbyjointlylearningtoalignandtranslate. In
Ross,S.;Gordon,G.J.;andBagnell,D. 2011. Areduction
InternationalConferenceonLearningRepresentations.
ofimitationlearningandstructuredpredictiontono-regret
Bengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer, N. 2015.
onlinelearning. InAISTATS,volume1, 6.
Scheduledsamplingforsequencepredictionwithrecurrent
Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural
neuralnetworks. InAdvancesinNeuralInformationProcess-
attentionmodelforabstractivesentencesummarization. In
ingSystems,1171–1179.
EmpiricalMethodsinNaturalLanguageProcessing.
Cho, K.; Van Merriënboer, B.; Bahdanau, D.; and Ben-
Serban, I. V.; Sordoni, A.; Bengio, Y.; Courville, A.; and
gio, Y. 2014. On the properties of neural machine
Pineau,J. 2015. Buildingend-to-enddialoguesystemsusing
translation: Encoder-decoder approaches. arXiv preprint
generativehierarchicalneuralnetworkmodels. InAAAI’16
arXiv:1409.1259.
ProceedingsoftheThirtiethAAAIConferenceonArtificial
Collins,M.,andRoark,B. 2004. Incrementalparsingwith
Intelligence.
theperceptronalgorithm. InProceedingsofthe42ndAnnual
Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence
MeetingonAssociationforComputationalLinguistics,111.
tosequencelearningwithneuralnetworks. InAdvancesin
AssociationforComputationalLinguistics.
neuralinformationprocessingsystems,3104–3112.
Daumé III, H., and Marcu, D. 2005. Learning as search
Taskar,B.;Guestrin,C.;andKoller,D. 2004. Max-margin
optimization:Approximatelargemarginmethodsforstruc-
markovnetworks. InAdvancesinneuralinformationpro-
turedprediction. InProceedingsofthe22ndinternational
cessingsystems,25–32.
conferenceonMachinelearning,169–176. ACM.
TjongKimSang,E.F.,andDeMeulder,F.2003.Introduction
Daumé,H.;Langford,J.;andMarcu,D. 2009. Search-based
totheconll-2003sharedtask:Language-independentnamed
structuredprediction. Machinelearning75(3):297–325.
entityrecognition. InProceedingsoftheseventhconference
Dyer,C.;Ballesteros,M.;Ling,W.;Matthews,A.;andSmith,
onNaturallanguagelearningatHLT-NAACL2003-Volume
N.A. 2015. Transition-baseddependencyparsingwithstack
4,142–147. AssociationforComputationalLinguistics.
longshort-termmemory. InAssociationforComputational
Tsochantaridis,I.;Joachims,T.;Hofmann,T.;andAltun,Y.
Linguistics.
2005. Largemarginmethodsforstructuredandinterdepen-
Dyer, C.; Kuncoro, A.; Ballesteros, M.; and Smith, N. A.
dentoutputvariables. Journalofmachinelearningresearch
2016. Recurrentneuralnetworkgrammars. InProceedings
6(Sep):1453–1484.
ofNAACL-HLT,199–209.
Vaswani,A.;Bisk,Y.;Sagae,K.;andMusa,R. 2016. Su-
Gormley, M. R.; Dredze, M.; and Eisner, J. 2015.
pertaggingwithlstms. InProceedingsofNAACL-HLT,232–
Approximation-aware dependency parsing by belief prop-
237.
agation. TransactionsoftheAssociationforComputational
Linguistics(TACL). Wiseman,S.,andRush,A.M. 2016. Sequence-to-sequence
learningasbeam-searchoptimization. InEmpiricalMethods
Goyal, K.; Dyer, C.; and Berg-Kirkpatrick, T. 2017. Dif-
inNaturalLanguageProcessing.
ferentiable scheduled sampling for credit assignment. In
AssociationforComputationalLinguistics. Xu,K.;Ba,J.;Kiros,R.;Cho,K.;Courville,A.C.;Salakhut-
dinov,R.;Zemel,R.S.;andBengio,Y. 2015. Show,attend
Gu, J.; Im, D. J.; and Li, V. O. 2017. Neural machine
andtell:Neuralimagecaptiongenerationwithvisualatten-
translation with gumbel-greedy decoding. arXiv preprint
tion. InICML,volume14,77–81.
arXiv:1706.07518.
3052

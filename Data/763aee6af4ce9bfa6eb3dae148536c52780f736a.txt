Towards Principled Disentanglement for Domain Generalization
HanlinZhang1,* Yi-FanZhang2,* WeiyangLiu3,4 AdrianWeller3,5 BernhardScho¨lkopf4 EricP.Xing1,6
1CarnegieMellonUniversity 2ChineseAcademyofScience 3UniversityofCambridge
4MaxPlanckInstituteforIntelligentSystems,Tu¨bingen 5AlanTuringInstitute 6MBZUAI *EqualContribution
Abstract
...
A fundamental challenge for machine learning models
is generalizing to out-of-distribution (OOD) data, in part
duetospuriouscorrelations. Totacklethischallenge, we Variation Encoder v 1 v n Generator
first formalize the OOD generalization problem as con-
v
strainedoptimization,calledDisentanglement-constrained
... Domain Generalization (DDG). We relax this non-trivial
constrainedoptimizationproblemtoatractableformwith s
finite-dimensionalparameterizationandempiricalapproxi- Semantic Encoder s 1 s n Minimize semantic gap
mation.Thenatheoreticalanalysisoftheextenttowhichthe
Figure1.AnillustrationofDDGbasedondisentanglementofdigitlabels
abovetransformationsdeviatesfromtheoriginalproblemis (semantics)androtatedangles(variationacrossdomains).DDGseeksto
provided. Basedonthetransformation,weproposeaprimal- minimizethesemanticdifferenceofthegeneratedsamplesfromthesame
classwhilediversifyingthevariationacrosssourcedomains.
dualalgorithmforjointrepresentationdisentanglementand
domaingeneralization.Incontrasttotraditionalapproaches
based on domain adversarial training and domain labels, beenmadefromadiversesetofdirections,suchasdomain
DDG jointly learns semantic and variation encoders for adaptation[5,6,23,84],self-supervisedlearning[12,29],
disentanglement, enabling flexible manipulation and aug- causalinference[56,61,67,81],invariantriskregularization
mentation on training data. DDG aims to learn intrinsic [3,42,51],angularalignmentregularization[18,48,49],dis-
representationsofsemanticconceptsthatareinvariantto tributionallyrobustoptimization[7]anddataaugmentation
nuisancefactorsandgeneralizableacrossdomains.Compre- [4,77,82]. However,havingnoaccesstotargetdomaindata
hensiveexperimentsonpopularbenchmarksshowthatDDG posesgreatchallenges. Twomainlinesofresearchseekto
can achieve competitive OOD performance and uncover addressthis. First,trainingdomainlabelsareassumedtobe
interpretablesalientstructureswithindata. availablein[3,23,47,60,66]suchthatthedivergenceto
differentdomainscanbeminimized.However,thesedomain
labels are often impractical or prohibitively expensive to
1.Introduction obtain[28]. Moreover,itisnon-trivialtominimizedomain
divergencewithdomainadversarialtrainingwhichisnotori-
Learning representations that can reflect intrinsic class
ouslyhardtoconverge[63]. Thesecondlineofworkstries
semanticsandalsorenderstronginvariancetocross-domain
tomodelthecross-domaindistributionshiftsandcapturethe
variationisofgreatsignificancetorobustnessandgeneral-
semanticinvariance[37,62,78]. However,ithasbeenfound
izationindeeplearning. Despitebeingempiricallyeffective
in[27]thatthisgoalcanbeverydifficulttoachieve. What
onmanyvisualrecognitionbenchmarks[65],modernneu-
makestheproblemevenmorechallengingistheinconsis-
ralnetworksarestillpronetolearningshortcutsthatstem
tencyoftheevaluationprotocol. Surprisingly,[27]shows
from spurious correlations [24], resulting in poor out-of-
that even the standard empirical risk minimization could
distribution(OOD)generalization. Totacklethischallenge,
outperformmanyrecentlyproposedmodelsundercertain
domaingeneralization(DG)hasemergedasanincreasingly
conditions. Motivatedbythesechallenges,weaimtodisen-
important task where the goal is to learn invariant repre-
tanglevariationsandsemanticsinaprincipledway,andthen
sentations over source domains that are generalizable to
verify the effectiveness of our method under a consistent
distributionsdifferentfromthoseseenduringtraining[55].
evaluationprotocol[27].
In order to improve OOD generalization, efforts have
AkeydesideratumforDGistoensuretheinvarianceof
Coderepository:https://github.com/hlzhang109/DDG learnedrepresentationstoallpossibleinter-classvariations.
2202
tcO
91
]GL.sc[
4v93831.1112:viXra
noitairav
detimil
htiw
tupnI
noitairav
yfisreviD
Therefore, our intuition is to first diversify the inter-class lationunderdomaintransformationcanbechallengingin
variationbymodelingpotentialseenorunseenvariations, settingswheredomain-specificsignalslikeimagestyles
andthenminimizethediscrepancyoftheinter-classvaria- vary greatly across domains, constituting more compli-
tiononarepresentationspacewherethetargetistopredict catedsuperficialvariationfactors. YetDDGcanuncover
semanticlabels. Tothisend,wefirstformalizedistribution salientstructurewithindatabyimposingconstraintson
shiftsandinvariancebasedondisentanglement. Concretely, thesemanticandvariationfactors.
weformulatethedisentanglementbetweenclasssemantics • Comprehensiveexperimentsareconductedunderacon-
and both intra- and inter-domain variations as constraints sistentevaluationprotocoltoverifytheeffectivenessof
to the DG problem. Then we propose a novel framework DDG.WeshowthatDDGisabletoproduceinterpretable
calledDisentanglement-constrainedDomainGeneralization
qualitativeresultsandachievecompetitiveperformance
(DDG).AnillustrationofDDGisgiveninFig.1. Whenthe on a number of challenging DG benchmarks including
semantic(i.e. thelabelsofdigits)andvariationfactors(i.e. RotatedMNIST,VLCS,PACSandWILDS.
rotatingangles−60◦,0◦,60◦,120◦)arewelldisentangled,
we expect that learned representations can be effectively 2.RelatedWork
constrainedtobeinvarianttointer-classvariation. Inorder
Domain Generalization. Domain/Out-of-distribution
to achieve such a non-trivial goal, we first derive a con-
generalization[55]aimstolearnrepresentationsthatarein-
strainedoptimizationproblemandthenproposeaprincipled
variantacrossdomainssothatthemodelcanextrapolatewell
algorithm based on primal-dual iterations to solve it. To
inunseendomains. InvariantRiskMinimization(IRM)[3],
understandhowwellthetransformedsolutionapproximates
whichextends[56],anditsvariants[1,42,51]areproposed
thesolutiontotheoriginalproblem,weprovidecomprehen-
totacklethischallenge. However,IRMentailschallenging
sivetheoreticalguaranteesfortheparameterizationgapand
bi-leveloptimizationandcanfailcatastrophicallyunlessthe
empiricalgap. Wealsoverifytheempiricaleffectivenessof
testdataaresufficientlysimilartothetrainingdistribution
DDGbyshowingthatitcanconsistentlyoutperformcurrent
[62]. DG via domain alignment [2, 18, 55] aims to mini-
popularDGmethodsbyaconsiderablemargin.
mize the difference between source domains for learning
Asausefulsideproduct, DDGsimultaneouslyobtains domain-invariantrepresentations. Themotivationisstraight-
anautomated,domain-agnosticdataaugmentationnetwork forward: features that are invariant to the source domain
basedonlearneddisentangledrepresentations. Thisrequires shiftshouldalsoberobusttoanyunseentargetdomainshift.
nousageofdomain-specificknowledgeorgradientestima- Themaindifferenceisthatweproposetolearninvariantrep-
tion [4, 77]. The intuition why such a data augmentation resentationsbyreconstructingimagesfromvariousdomains
networkisusefulcomesfromthefactthatthelearnedvari- and class semantics to simulate variations and minimize
ation encoder can well approximate some of the intrinsic domain divergence. PAC constrained learning [15, 16] is
intra-andinter-domainvariations. Italsoservesasfeature adoptedformodelingcross-domainvariationsunderdomain
removalsincemoretrainingexamplesaugmentedbyspecific transformationinMBDG[60]. Wehighlightseveralmajor
variationfactorsleadtomoreinvariantrepresentationsfor differences between our approach and MBDG below: (1)
thosevariations. Moreover,theincreaseddiversityofsource DDGimposesweakerassumptions;(2)MBDGconsumes
domaindataimprovesthelikelihoodthatanunseendistribu- additionaldomainlabels,whichareoftenhardtoobtainin
tionlieswithintheconvexhullofsourcedomains[2]. For manyapplications,whileDDGdoesnot;(3)DDGenforces
example,inFig.1,theoriginaldatasetcanbeaugmentedvia invarianceconstraintsviaparameterizingsemanticandvari-
alearnedmanipulatorbycomposingadiversecombination ation encoders, which does not belong to a model-based
ofsemanticandvariationfactors. Suchadisentanglement approach. Incontrast,MBDGrequiresapre-traineddomain
canbeagoodpredictorforOODgeneralizationaccording transformationmodel(e.g.,CycleGAN)duringtraining. Ap-
to[21]. WehighlightthefollowingadvantagesofDDG: pendixDprovidesthedetailedcomparisontoMBDG.
Disentangled Representation Learning. The goal of
• DDGadoptsaprincipledconstrainedlearningformulation
disentangledrepresentationlearningistomodeldistinctand
based on disentanglement, yielding rigorous theoretical
explanatory factors of variation in the data [8, 68]. [21]
guaranteesontheempiricaldualitygap.
showsthatdisentanglementisagoodpredictorforout-of-
• Ouralgorithmisconceptuallysimpleyeteffective. DDG distribution(OOD)tasks. [64]proposestodisentanglethe
promotessemanticinvarianceviaaconstrainedoptimiza- semanticlatentvariablesandthedomainlatentvariablesfor
tionsetup. Thisisdonewithouttheusageofadversarial strongergeneralizationperformanceindomainadaptation.
training and domain labels. Moreover, there is no addi- [54]showsthatexistingdisentangledlearningmodelsare
tionalcomputationaloverheadformodelingvariations. notsufficienttosupportcompositionalgeneralizationand
• Ourframeworkcanbeviewedasacontrollableandinter- extrapolation while hypothesizing that the richness of the
pretabledatagenerationparadigmforDG.Datamanipu- training domain matters more. However, previous works
[19,31,39]arelimitedtosingle-dimensionallatentcodes Our formulation generally follows prior works of PAC
anddevelopedwithdifferentpurposeslikegenerationand constrainedlearning[15,16,58,60],butweuseamoreflex-
interpretability. Thustheyarehardtoscalewellbeyondtoy ibleparameterizationandderiveanewalgorithmtosolve
datasets and adapt to complicated DG tasks [54]. In con- theresultingconstrainedoptimizationproblem. Morespecif-
trast,weharnessthedisentangledeffectstolearninvariant ically, we emphasize that DDG, motivated by an analysis
representationsforrealisticOODgeneralizationtasks. of the multi-source domain adaptation upper bound (Ap-
Data Augmentation. The diversity of the training dis- pendixB),requiresnodomainlabelsandpre-traineddomain
tribution is of great importance in improving DG perfor- transformation models during training. DDG can also be
mance[2,27,29,75,82]. Dataaugmentationisaneffective trained in an end-to-end manner, yielding a more flexible
waytoincreasedatadiversity[79]anditcanthereforeim- andpotentiallybettersolution.
proveOODgeneralizationaswellasrobustnesstospurious
3.1.Formulation
correlations [4, 38]. In particular, [14] devises an active
learning schemeto learncausal manipulations onimages,
ThebasicideaofDDGistolearndisentangledrepresenta-
whichenrichesthedatasetfromobservationaldataandim-
tionsbyimposinginvariantconstraintsinthesemanticspace
provesgeneralizationonbothcausalandpredictivelearning S andvariationspaceV. Suchadisentanglementcanalso
tasks. In contrast, DDG seeks to learn underlying causal
be applicable for augmenting the training data so that the
featuresbyapproximatingthedatamanipulationfunction.
learnedrepresentationscanbemoreinvarianttobothinter-
Thisisdonewithoutatask-specificmetrictodifferentiate
andintra-domainvariations. Toformalizethis,webeginby
theaugmenteddataandtheoracle. Ourworkintroducesa
introducingsomenecessarydefinitionsandassumptions.
simpleyeteffectiveapproachforaugmentingtrainingdata,
whichreinforcestheimportanceofdatadiversityinDG. Definition1 (Invariance based on disentanglement).
Fairness. Fairnessresearch[22,28,52]aimstodevelop GivenadecoderD:S×V→X,asemanticfeaturizerf
s
amodelthatperformswellundergroupassignmentsaccord- isinvariantifforalldomainsd ∈Dandavariationfea-
i
ingtosomefairnesscriteriaforaddressingtheunderperfor- turizerf ,x=D(f (x;θ),f (x˜;φ))holdsalmostsurely
v s v
manceinminoritysubgroups. Learningfairrepresentations whenx,x˜∼P(X).
can be naturally translated to a constrained optimization
problem[15,16,43]. Therearealsoexchanginglessonsbe- Thepropertyenforcestheinvarianceoftheoriginalinput
tweenalgorithmicfairnessanddomaingeneralization[20], xandtheoneD(f (x;θ),f (x˜;φ))thatreconstructsjointly
s v
showingthatbothfieldsareoptimizingsimilarstatisticsfor from semantic and variation latent spaces when semantic
commongoals. DDGwellalignswiththeformulationand factorsremainconstantwhilevariationfactorsvary.
goals of fairness without demographics [28] and has the
potentialtoimprovecontext-specificfairnesswithoutprior Assumption1 (Domainshiftbasedondisentanglement)
Denote f (x;θ) as the semantic factor of input x and
knowledgeaboutdomainsordemographics. s
f (x˜;φ)asthevariationfactorofanyotheronex˜.Similar
v
3. Disentanglement-constrained Optimization to the covariate shift assumption [71], we assume the
forDomainGeneralization domain/distributionshiftstemsfromthevariationofthe
marginaldistributionP(X)andthefollowinginvariance
Notations. We consider a classification problem from conditionholdswiththeproposedf ,f andD
s v
feature space X ∈ Rd to label space Y ∈ {0,1} where
(X,Y) ∼ P(X,Y). The infinite-dimensional functional P(Y =y|X=x)=P(Y =y|X=D(f s(x;θ),f v(x˜;φ)). (1)
spaceandfinite-dimensionalhypothesisspacearedenoted
asF andH ⊆ Rp,respectively. Theparameterizedlatent Thisassumptionshowsthatthepredictiondependsonly
spacesforsemanticandvariationfactorsaredenotedasS onthesemanticfactorf (x;θ)regardlessofthevariation
s
and V, respectively. x˜ denotes a different sample with x one f (x˜;φ). It also subsumes as a special case the do-
v
fromthetrainingdistributionP. d(·,·)denotesadistance mainshiftsbasedondomainlabels,i.e.,P(Y =y|X=x)=
metricoverX ×X. P(Yd=y|Xd=G(X=x,d)) given a domain transforma-
Problemsetting. Supposeweobserveadatasetdenoted tionmodel[59,60],sinceourvariationfactorsincludesboth
as{(x ,y )}n ⊂ D where(x ,y )isarealizationof inter-andintra-domainvariations.
i i i=1 data i i
randomvector(X,Y)withsupport(X ×Y). Weconsidera Notethattheaboveassumptionsfollowtheformulationin
setofdomains{d }nd ⊂Dofsizen ,whereeachdomain [60]. Toelaboratethis,weintroducethenotionofinvariance
i i=1 d
corresponds to a distinct data distribution Ddi over some basedonadecoderDtakingasinputthedisentanglement
inputandlabelspace. ThesetofdomainsDispartitioned resultsf (x;θ)andf (x˜;φ). Inpractice,Dcanbeparame-
s v
intomultipletrainingdomainsD ⊂ Dandatestdomain terizedasapre-trainedmodeloratrainablecomponentD
S ψ
D ∈Dwhichisinaccessibleduringtraining. updatedintheprimalstepasinourimplementation.
U
Assumption2 (Regularityconditions)Thelossfunction Definition4 ((cid:15)-parameterization)LetH⊆Rpbeafinite-
(cid:96) and distance metric d are convex, non-negative, B- dimensionalparameterspace. For(cid:15) > 0,afunctionh :
bounded. (cid:96)isaL -Lipschitzfunction,thedistancemetric H×X → Y is an (cid:15)-parameterization of F if for each
(cid:96)
isalsoaL -Lipschitzfunction. f ,f ∈F,thereexistparametersθ,φ∈Hsuchthat
d s v
E (cid:107)h (x;θ)−f (x)(cid:107) ≤(cid:15) ,
x∼P(X) s s ∞ s
Assumption3 (Feasibility)Thereexistsemanticandvari-
E (cid:107)h (x;φ)−f (x)(cid:107) ≤(cid:15) ,
ationfeaturizersf ,f ∈F suchthatL (f ,f )<γ− x∼P(X) v v ∞ v
s v con s v E (cid:107)D(h (x;θ),h (x˜;φ))−D(f (x),f (x˜))(cid:107) ≤(cid:15) .
m=max{L (cid:15) ,L (cid:15) }=γ−mwith(cid:15)-parameterization. x,x˜∼P(X) s v s v 2 g
(cid:96) s d g
Withthehelpof(cid:15)-parameterization,tractableoptimiza-
tioncanbeperformedoverfinite-dimensionalparameterized
Definition2 (Domaingeneralizationproblem). Similar
space. NotethataregularityconditionaboutDisintroduced
topriorworks[60,66,72,78],weformulatedomaingen-
toallowDDGtofaithfullyreconstructinputsunderfinite-
eralizationasaminimaxoptimizationproblem,optimizing
dimensionalparameterization. Withtheaboveformulation
theworst-domainriskovertheentirefamilyofdomainsD
andtoprovideguaranteesfortheDGproblem,weconsider
minmaxE P(X,Y)(cid:96)(f s(D(X,d)),Y). (2) acorrespondingsaddle-pointproblemasfollows:
fs∈F d∈D
D(cid:15)∗(γ)(cid:44)max min L(θ)+λL con(θ,φ), (4)
The above formulation (2) requires the availability of λ θ,φ∈H
domain labels and is hard to optimize. However, the do- wheretheconstraint-relatedriskisdefinedas
main labels are expensive or even impossible to obtain in
L (θ,φ)=E [d(x,D(h (x;θ),h (x˜;φ)))−γ].
partduetoprivacyandfairnessissues[28]. Therefore,un- con x,x˜∼P(X) s v
derthedisentanglement-basedinvarianceanddomainshift Thechallengefortheparameterizedproblem(4)isthein-
assumptions, we constrain the model to be invariant with accessibilityofthegroundtruthdatadistributionP(X,Y).
respecttovariationfactors,thentheproblemisconvertedto Toaddressit,weresorttoacorrespondingempiricaldual
aninequality-constrainedoptimizationproblem: problemusingfinitenempiricaltrainingsamples:
Definition3 (Constrained domain generalization prob- D(cid:15)∗ ,n(γ)(cid:44)max min L(θ,φ,γ)(cid:44)Lˆ(θ)+λLˆ con(θ,φ)
lem)Givenafixedmarginγ >0,withAssumption3and λ θ,φ∈H
n
enforcingtheinvarianceonthesemanticfeaturizerf ,we (cid:88)
s =max min (cid:96)(f (x),y )+
transformthevanillaformulationEq.(2)tothefollowing λ θ,φ∈H s i i (5)
i=1
inequality-constrainedoptimization n n
(cid:88)(cid:88)
λ [d(x,D(h (x;θ),h (x ;φ)))−γ],
P(cid:63) (cid:44) fm s∈in FL(f s)(cid:44)E P(X,Y)(cid:96)(f s(X),Y),
(3)
i=1 j(cid:54)=i
i s i v j
s.t.d(x,D(f (x;θ),f (x˜;φ)))≤γ, a.e.x,x˜∼P(X). which gives us the final optimization objective for DDG.
s v
Compared to the previous optimization problems, this is
OneintriguingpropertyofEq. (3)isthatlearningwith mucheasierandmoretractabletosolve.
inequalityconstraintsdoesnotproduceadditionalsample
3.3.Algorithm
complexityoverheadundersomeregularityconditionson
thelossfunction(cid:96)[15]. However,itisdifficulttosatisfythe
Motivatedbytheaboveanalysis, weuseaprimal-dual
strictnessandprovidetheoreticalguaranteesforlearningin
algorithmforefficientoptimization[15,17,58,60]. Theal-
practicalcases. Inthefollowingsection,withtheparameteri- gorithmalternatesbetweenoptimizingθ(and/orφ)viamini-
zationandsaddle-pointcondition,wecanrelaxtheinvariant mizingtheempiricalLagrangianwithfixeddualvariableλ
constraintandobtainaversionthatisamenabletoaprovable
andupdatingthedualvariableaccordingtotheminimizer:
PAClearningframework.
θ(t+1) ←argminL(θ(t),φ(t),γ)+ρ,
3.2.Parameterization θ
φ(t+1) ←argminL(θ(t),φ(t),γ)+ρ,
(6)
Wefirstdiscusshowtoparameterizethelearnablecompo-
φ
nentsinDDG.TheDGproblem(Eq.(3))yieldsaninfinite- (cid:110)(cid:104) (cid:105) (cid:111)
λ(t+1) ←max λ(t)+η Lˆ ,0 ,
dimensionaloptimization.Adefactowaytoenabletractable 2 con
optimizationisusingfinite-dimensionalparameterizationof wheretheη denotesthelearningrateofthedualstep.
2
F likeneuralnetworks[33]orreproducingkernelHilbert The primal-dual iteration has clear advantages over
spaces(RKHS)[9]. Tofurtherdiscusstheparameterization stochasticgradientdescentinsolvingconstrainedoptimiza-
gap,weformalizetheapproximationpowerofsuchparame- tionproblems. Specifically,itavoidsintroducingextrabal-
terizationbythefollowingdefinitionof(cid:15)-parameterization. ancinghyperparameters. Moreover,itprovidesconvergence
Algorithm1:DDG:Disentanglement-constrained toderivetheboundonthefinalempiricaldualitygap, we
OptimizationforDomainGeneralization startbyprovingtwolemmasontheparameterizationgapand
empiricalgap. Specifically,theyelaboratethecorresponding
Input: D ={(x ,y ),...,(x ,y )},batchsizeB,
S 1 1 n n
approximationgapsoftwotransformations(i.e. Eq.(4)and
primalandduallearningrateη ,η ,Adam
1 2
(5))inabovesections.
hyperparametersβ ,β ,initialcoefficientsλ,
1 2
Wefirstdiscussthegapbetweenthefinite-dimensional
marginγ
modelparameterization(e.g.neuralnetworks)andthemodel
Initial: ParametersofDDG(i.e. parametersθ,φand
overinfinitefunctionalspaceF.
ψforsemanticencoderh ,variationencoderh
s v
anddecoderD.) Lemma1 (Parameterization gap) With Assumption 2
repeat about(cid:96)andd,thegapbetweenoptimumofastatistical
fori,j =1,...,B,i(cid:54)=j do problemanditsfinitedimensional,deterministicversion
Li con = (cid:110) (cid:111) D ε(cid:63)(γ)−P(cid:63)canbeboundedas
max (cid:107)x i−D(h s(x i)⊕h v(x j))(cid:107) l1 −γ,0 0≤Dε(cid:63)(γ)−P(cid:63) ≤(cid:0) 1+|λ(cid:63) p|(cid:1) max{L (cid:96)(cid:15) s,L d(cid:15) g}, (7)
Li =(cid:96)(h (x ),y )
ERM s i i
L =Li +λLi where λ(cid:63) is the dual variable with a tighter constraint
i ERM con p
γ−max{L (cid:15) ,L (cid:15) }inEq. (3).
(cid:96) s d g
ifDataAugmentationthen
x∗ =D(h (x )⊕h (x )) Theupperboundindicatesthattheparameterizationgap
s i v j
L∗ =(cid:96)(h (x∗),y ) isdominatedbyboththesemanticfunctionparameterization
ERM s i
L =L +L∗ and the reconstruction-based transformation on perturbed
i i ERM
end inputs, whichmakesintuitivesensesandalsoemphasizes
end theimportantroleofdisentanglement.
Primalstep ThenwecompareEq.(7)totheparameterizationgapof
θ ←Adam(cid:16) 1 (cid:80)B L ,θ,η ,β ,β (cid:17) MBDGwhichisshownasfollows:
B i=1 i 1 1 2
φ←Adam(cid:16)
1 (cid:80)B Li ,φ,η ,β ,β
(cid:17) 0≤D(cid:15)(cid:63)(γ)−P(cid:63) ≤(1+|λ(cid:63) p|)max{L (cid:96),L d}(cid:15) s, (8)
B i=1 con 1 1 2 andtheparameterizationgapin[16]:
ifTrainingDthen
ψ ←Adam(cid:16) 1 (cid:80)B Li ,ψ,η ,β ,β (cid:17) 0≤D(cid:15)(cid:63)(γ)−P(cid:63) ≤(1+|λ(cid:63) p|)L (cid:96)(cid:15) s. (9)
B i=1 con 1 1 2
Fromthecomparison, wenoticethatourformulationand
end
analysisarecloselyconnectedtoProposition1(withm=1)
Dualstep
(cid:110)(cid:104) (cid:105) (cid:111) in[16]. Weshallalsoseethatinaperfectcasewhererepre-
λ←max λ+η 1 (cid:80)B Li ,0
2B i=1 con sentationsarewelldisentangled,i.e.,(cid:15) g →0,ourboundwill
untilθisconvergedorD =∅;
become(cid:0) 1+|λ(cid:63)|(cid:1)
L (cid:15) . Wenotethatthisboundisstrictly
S p (cid:96) s
tighterthanthatinEq. (8).
In practice, we approximate the expectation by its em-
piricalaverage. BytheclassicalVC-dimensionbound,the
guarantees once we have sufficient iterations and a suffi-
followingboundontheempiricalgapholds:
cientlysmallstepsize. Wereferreadersto[17,60]formore
in-depthandcompletediscussionsofrelatedconditionsand Lemma2 (Empirical gap) Denote d as the VC-
VC
convergencebounds. dimensionofthehypothesisclassH . Assumethat(cid:96)andd
θ
Oneintriguingpropertyofdisentanglementisthatitcan obeytheregularityconditioninAssumption2. Thengiven
beapplicableforaugmentingthetrainingdata.Basedonthis, nsamples,withprobability1−δ,wecanupperboundthe
DDGapproximatesamanipulatorfunctionbylearning“hard” deviation|D(cid:63)(γ)−D(cid:63) (γ)|with
(cid:15) ε,n
datapointsfromfictitioustargetdistributionsforpromoting
(cid:115)
inv Wari ea gn ic ve ea tn hd edim etp ar ilo ev din pg rog ce en de ur ra eli oz fa oti uo rn D.
DGlearningalgo-
(cid:12) (cid:12)D(cid:15)(cid:63)(γ)−Dε(cid:63) ,n(γ)(cid:12) (cid:12)≤2B n1 (cid:20) 1+log(cid:18) 4(2n δ)dvc(cid:19)(cid:21) . (10)
rithminAlgorithm1,where(cid:96)denotesthecrossentropyloss
and⊕denotestheconcatenationintheimplementation. We Withtheaboveheavylifting,westartderivingtheempir-
alsousel normasthedistancemetricdintheexperiments. icaldualitygap,whichisourultimategoalofthetheoretical
1
analysis. Theempiricaldualitygapincludestheabovetwo
3.4.TheoreticalInsightsandGuarantees
components. Combining the above bounds on two gaps,
Inthissubsection,weprovideacomprehensiveanalysis
wecanboundthedeviationbetweenP(cid:63) andD (cid:15)∗ ,n(γ)(i.e.,
ofthestatisticalguaranteesofoursolution(Eq.(5)).Inorder
|P(cid:63)−D (cid:15)∗ ,n(γ)|)undersomemildconditions.
Semantics Variation Variation Generated Images with Intepolated Variation Factors
⊕ ( i = + (1 − i ) ) =
x x
⊕ ( i + (1 − i ) ) =
x x
⊕ ( i + (1 − i ) ) =
x x
⊕ ( i + (1 − i ) ) =
x x
⊕( i + (1 − i ) ) =
x x
Figure2.Interpolationdisentanglementresults.Differentproportionsofvariationfactorsaremixedtogeneratetheimagebyvaryingi∈{1.0,0.9,...,0.1}.
Theorem1 (Empiricaldualitygap)WhenAssumption2 [3], GDRO [66], Mixup [80], MLDG [44], CORAL [73],
holds,bydenotingmax{L (cid:15) ,L (cid:15) }asm,wehave MMD[46],DANN[23],CDANN[47],AugMix[30].
(cid:96) s d g
AllthebaselinesinDGtasksareimplementedusingthe
(cid:114)
(cid:12) (cid:12)P(cid:63)−Dε(cid:63) ,n(γ)(cid:12) (cid:12)≤(1+|λ|)m+O( log n(n) ). (11) c sto ud de ib ea sse uso if nD go thm eai on fb fie cd ia[ l27 im]. pW lee ma ed na tp at tiA ou ng sM asix info dr ica ab tl ea dtio inn
[30]. The two-sample classifier, implemented as a RBF
Thefinalboundtellsusthatthequalityoftheempirical,
kernelSVMusingScikit-learn,isusedforcalculatingthe
dualapproximationoftheprimalproblemisdeterminedby generalizationerrorforA-distance.
thesamplesize,thehardnessofthelearningproblem,and
Hyperparameter search. Following the experimental
the richness of parameterization. The proof can be easily
settingsin[27],weconductarandomsearchof20trialsover
shown using triangle inequality as in Appendix A.3. As
thehyperparameterdistributionforeachalgorithmandtest
suggestedbyTheorem1,wecanimprovetheperformanceof
domain. Specifically, we splitthe datafrom each domain
ouralgorithmbyusingneuralnetworkswithlargercapacity
into 80% and 20% proportions, where the larger split is
ortrainingourmodelwithmoredata.
usedfortrainingandevaluation,andthesmalleroneisfor
selectinghyperparameters. Werepeattheentireexperiment
4.Experiments
twiceusingdifferentseedstoreducetherandomness.Finally,
we report the mean over these repetitions as well as their
Datasets. Weconsiderthefollowingfourdatasets: Ro-
estimatedstandarderror.
tatedMNIST[25],PACS[45],VLCS[74]andWILDS[41]
Model selection. The model selection in domain gen-
toevaluateDDGagainstpreviousmethods. Weincludethe
eralizationisintrinsicallyalearningproblem, andweuse
visualizationofdatasetsinAppendixC.2.
test-domainvalidation,oneofthethreeselectionmethods
RotatedMNIST[25]consistsof10,000digitsinMNIST
in [27]. This strategy is an oracle-selection one since we
with different rotated angles d such that each domain is
choosethemodelmaximizingtheaccuracyonavalidation
determinedbythedegreed∈{0,15,30,45,60,75}.
setthatfollowsthesamedistributionofthetestdomain.
PACS[45]includes9,991imageswith7classesy ∈{
Model architectures. Following [27], we use as en-
dog,elephant,giraffe,guitar,horse,house,person}from4
codersConvNetforRotatedMNIST(detailedinAppdendix
domainsd∈{art,cartoons,photos,sketches}.
D.1in[27])andResNet-50fortheremainingdatasets.
VLCS [74] is composed of 10,729 images, 5 classes
MotivatedbytheobservationthatGANisabletoimprove
y ∈ {bird, car, chair, dog, person} from domains d ∈
imagequalityforevaluatingthedisentanglementeffectsin
{Caltech101,LabelMe,SUN09,VOC2007}.
the latent spaces [57, 69, 70], we use adversarial training
Camelyon17-WILDS[11,41]isabouttumordetection
[26]onrealsamplesxagainstfakeonesD(h (x)⊕h (x˜))
intissues. Thisdatasetiscomposedof455,954imagesfrom s v
toattainhigh-qualityimagesx(cid:48):
5differentmedicalcenters/domainsintotal,whichdefinesa
binaryclassificationproblemaboutwhetherthepatchimage L =logDisc(x)+log(1−Disc(x(cid:48))). (12)
GAN
containsatumorornot.
Baselines. WecompareourmodelwithERM[76],IRM Inpractice,wecantrainthegeneratorusingadversarialtrain-
RotatedMNIST Camelyon17-WILDS
Domain 0◦ 15◦ 30◦ 45◦ 60◦ 75◦ Avg d1 d2 d3 d4 d5 Avg
ERM[76] 96.0±0.2 98.8±0.1 98.8±0.1 99.0±0.0 99.0±0.0 96.8±0.1 98.1 96.8±0.3 94.9±0.2 95.9±0.2 95.8±0.2 94.8±0.3 95.6
IRM[3] 96.0±0.2 98.9±0.0 99.0±0.0 98.8±0.1 98.9±0.1 95.7±0.3 97.9 95.0±0.7 92.0±0.2 95.2±0.3 94.3±0.1 93.3±0.6 94.0
GDRO[66] 96.2±0.1 98.9±0.0 99.0±0.1 98.7±0.1 99.1±0.0 96.8±0.1 98.1 96.5±0.1 95.0±0.3 95.9±0.9 96.0±0.1 95.7±0.4 95.8
MIXUP[80] 95.8±0.3 98.9±0.1 99.0±0.1 99.0±0.1 98.9±0.1 96.5±0.1 98.0 96.2±0.0 94.3±0.1 95.7±0.4 96.7±0.0 95.1±0.1 95.6
MLDG[44] 96.2±0.1 99.0±0.0 99.0±0.1 98.9±0.1 99.0±0.1 96.1±0.2 98.0 97.0±0.1 95.0±0.3 96.6±0.5 96.0±0.2 96.1±0.3 96.1
CORAL[73] 96.4±0.1 99.0±0.0 99.0±0.1 99.0±0.0 98.9±0.1 96.8±0.2 98.2 96.5±0.2 95.2±0.1 96.9±0.1 96.8±0.3 94.8±0.3 96.0
MMD[46] 95.7±0.4 98.8±0.0 98.9±0.1 98.8±0.1 99.0±0.0 96.3±0.2 97.9 96.3±0.1 94.9±0.1 96.8±0.1 96.5±0.2 93.3±0.1 95.6
DANN[23] 96.0±0.1 98.8±0.1 98.6±0.1 98.7±0.1 98.8±0.1 96.4±0.1 97.9 93.9±0.3 89.6±1.0 94.5±0.1 93.9±0.5 92.0±0.2 92.8
CDANN[47] 95.8±0.2 98.8±0.0 98.9±0.0 98.6±0.1 98.8±0.1 96.1±0.2 97.8 94.3±0.1 91.7±0.7 95.0±0.1 94.7±0.2 92.9±0.5 93.7
DDG 96.6±0.1 99.0±0.1 99.0±0.2 99.1±0.2 99.1±0.2 97.4±0.4 98.4 97.4±0.2 95.4±0.2 96.8±0.1 96.8±0.1 96.6±0.3 96.6
DDGW/AUG 96.7±0.4 99.0±0.3 99.1±0.2 99.2±0.2 99.0±0.3 97.4±0.3 98.4 97.7±0.4 96.6±0.2 96.9±0.1 97.2±0.3 96.9±0.1 96.9
PACS VLCS
Domain A C P S Avg C L S V Avg
ERM[76] 87.8±0.4 82.8±0.5 97.6±0.4 80.4±0.6 87.2 97.7±0.3 65.2±0.4 73.2±0.7 75.2±0.4 77.8
IRM[3] 85.7±1.0 79.3±1.1 97.6±0.4 75.9±1.0 84.6 97.6±0.5 64.7±1.1 69.7±0.5 76.6±0.7 77.2
GDRO[66] 88.2±0.7 82.4±0.8 97.7±0.2 80.6±0.9 87.2 97.8±0.0 66.4±0.5 68.7±1.2 76.8±1.0 77.4
MIXUP[80] 87.4±1.0 80.7±1.0 97.9±0.2 79.7±1.0 86.4 98.3±0.3 66.7±0.5 73.3±1.1 76.3±0.8 78.7
MLDG[44] 87.1±0.9 81.3±1.5 97.6±0.4 81.2±1.0 86.8 98.4±0.2 65.9±0.5 70.7±0.8 76.1±0.6 77.8
CORAL[73] 87.4±0.6 82.2±0.3 97.6±0.1 80.2±0.4 86.9 98.1±0.1 67.1±0.8 70.1±0.6 75.8±0.5 77.8
MMD[46] 87.6±1.2 83.0±0.4 97.8±0.1 80.1±1.0 87.1 98.1±0.3 66.2±0.2 70.5±1.0 77.2±0.6 78.0
DANN[23] 86.4±1.4 80.6±1.0 97.7±0.2 77.1±1.3 85.5 95.3±1.8 61.3±1.8 74.3±1.0 79.7±0.9 77.7
CDANN[47] 87.0±1.2 80.8±0.9 97.4±0.5 77.6±0.1 85.7 98.9±0.3 68.8±0.6 73.7±0.6 79.3±0.6 80.2
DDG 88.9±0.6 85.0±1.9 97.2±1.2 84.3±0.7 88.9 99.1±0.6 66.5±0.3 73.3±0.6 80.9±0.6 80.0
DDGW/AUG 89.0±0.3 86.3±0.3 97.0±0.5 84.8±1.1 89.3 99.4±0.2 68.9±2.3 73.4±1.1 81.2±0.3 80.7
Table1.Domaingeneralizationaccuracies(%)onRotatedMNIST,PACS,VLCSandWILDS.
ing. Inthisstage,weemployanadversarialobjectiveL tingsincludingdatasetsstatisticsandvisualization,baselines
GAN
andanadditionalcycleconsistencyconstraint. Withaslight anditsimplementation,hyperparametersearchandmodel
abuseofnotation,wedenoteh (·)andh (·)forh (·;θ)and selectionprotocols. SeeAppendixDformanymoreresults.
s v s
h (·;φ), respectively. The detail of the cycle consistency
v
constraintisthat: weencodexandx˜intothelatentspace Semantics Variation Output Semantics Variation Output
ash (x),h (x˜),h (x),h (x˜). Wethenswaptheirvariation
s s v s ⊕ = ⊕ =
factorsandgeneratex =D(h (x)⊕h (x˜)),x =
x→x˜ s v x˜→x
D(h (x˜)⊕h (x)). Again, the generated images will be
s v
encoded, and their variation factors will be swapped and ⊕ = ⊕ =
usedtogeneratex andx . Finally,thecycle
x→x˜→x x˜→x→x˜
consistencyconstraintforxisimplementedbytheproposed
⊕ = ⊕ =
reconstructionloss(alsosimilarlyforx˜):
L =d(x ,x) ⊕ = ⊕ =
cyc x→x˜→x
=d(D(h (D(h (x))⊕h (x˜))))⊕ (13)
s s v
h v(D(h v(x˜))⊕h v(x))))),x) Figure3. Qualitativedisentanglementresults. Swappingsemanticand
variationcodesenablescontrollablegenerationforqualitativeevaluation.
In most experiments, the generator is a simple autoen-
coder,whichconvertstheconcatenationof[h (x),h (x)]
s v 4.1.QualitativeStudies
tox(cid:48). Forqualitativeevaluationanddataaugmentationex-
periments,themainideaofourgeneratorfollows[35,85]. We showcase some of the reconstructed images train-
ThedecoderusesfourMLPstoproduceasetofAdaIN[34] ingwithGANinFig.2andFig.3(seeappendixformany
parameters from the semanticfactor. The variation factor more similar results). The results show that the represen-
is then processed by four residual blocks and four convo- tations are disentangled with respect to various variation
lutional layers with these AdaIN parameters. Finally, the factorslikebackground,coloretc,whichsupportsdiverse
processed latent vector is decoded to the image space by manipulations for enriching training datasets. Moreover,
upsamplingandconvolutionallayers. ThediscriminatorD DDG captures proper semantics over data, which allows
followsthepopularmulti-scalePatchGAN[36]onthreein- diversemanipulationsonvariationfactorslikeobjectcolors
putimagescales:14×14,28×28and56×56.Thegradient and backgrounds without changing object semantics. For
punishment[53]isalsoappliedwhenupdatingD. example,DDGchangesthecolorofadoginthefirstpanel
SeeAppendixCforfulldetailsofallexperimentalset- ofFig.3butretainsthesamecolorofitsnoseandeyes.Such
disentanglementenablesflexibleandcontrollablegeneration Convergence analysis.
4 ERM
bymanipulatingsemanticandvariationfactorsviaswapping Weinvestigatethetraining IRM
CDANN (Fig.3)orinterpolation(Fig.2). dynamicsofDDGandsev- 3 DANN
Interpolation details. In order to better understand eralbaselinesoverWILDS, GDRO
2 DDG (ours)
thelearnedsemanticandvariationrepresentations,wefur- wherethetargetdomainis
ther perform a linear interpolation experiment between d 5. Thelearningcurvesin 1
two variation factors and generate the corresponding im- Fig.5showthatdomainad-
0
ages as shown in Fig. 2. We denote the variation code versarial training methods 1000 2000 3000 4000 5000
Number of iteration
of the first and second column as h v(x˜),h v(x), respec- like DANN, CDANN are Figure5.Convergencecomparison.
tively. The images from 4 − 13 column are generated unstable and hard to con-
by D(h (x) ⊕ (i × h (x) + (1 − i) × h (x˜))) where verge due to their adversarial training nature. IRM has a
s v v
i ∈ {1.0,0.9,...,0.1}. These interpolation results verify similarpatternyetismorestable. Thankstotheprimal-dual
the smoothness and continuity in the variation space, and algorithm,DDGcanconvergemuchbetterthantheabove
also show that our model is able to generalize in the em- methods,beararesemblancetotheERMcounterpart.
beddingspaceinsteadofsimplymemorizingexistingvisual Evaluation of domain
information. Asacomplementarystudy,wealsogenerate divergence. WeusetheA- 0.95 DDG (ours) 0.938
imagesbylinearlyinterpolatingbetweentwosemanticfac- distancetomeasuredomain 0.9 E IRR MM 0.889
torswhilekeepingthevariationfactorsintact. Weprovide discrepancy [5]. This can 0.85 0.814 0.845
0.8 0.794
additionalqualitativeresultsinAppendixD. be approximated as d =
A 0.75
2(1−2σ),whereσistheer-
4.2.NumericalResults 0.7
ror of a two-sample classi- 0.65 0.648
ComprehensiveexperimentsshowthatDDGconsistently fierdistinguishingfeatures 0.6
From PCS to A From PAC to S
outperformsallthebaselinesbyaconsiderablemargin.From ofsamplesfromsourceand
Figure6. A-distanceonlearnedfea-
Table1, weobservethatDDGachievesbetterDGresults targetdomains[50]. Fig. 6 turesfordifferentgeneralizationtasks.
bothinmostsingledomainsandonaverage. Inparticular, showsthatDDGcanlearn
theperformancegainisgreaterintheworst-casescenario moreinvariantfeaturestominimizethedivergencebetween
liketheCandSdomaininPACS.Thisisparticularlyimpor- sourceandtargetdomainsthanIRMandERM.
tantsinceaverageperformanceisnotaneffectiveindicator QualitativecomparisonwithAugMix. BothFig.3and
of OOD generalization, and bad worst-case performance Fig.10binAppendixDshowthatDDGproducessamples
is tightly connected to issues like disparity amplification withdiversestyles. Incontrast,Fig.8inAppendixDshows
[28]. The performance gain of DDG is larger under the that it is much more difficult for heuristic-based methods
variation-richdatasetPACS.Thismakesintuitivesensesbe- suchasAugMixtogeneratesampleswithdiversestylesfor
causeDDGisabletobettercaptureinter-domainvariations training. Thequalitativeresultsvalidatetheeffectivenessof
forimprovingOODgeneralization. DDGasanautomaticdataaugmentationmethod.
4.3.EmpiricalAnalysesandAblations 5.ConcludingRemarks
Effectofdataaugmen- 100 Weproposeanoveldisentangledlearningframeworkfor
tation.Wefirstevaluatethe Baseline domaingeneralization, withboththeoreticalanalysesand
95 w/ Augmix
effectofdataaugmentation w/ ours practicalalgorithmicimplementation. Byseparatingseman-
by comparing our learned 90 tic and variation representations into different subspaces
dataaugmentationnetwork 85 whileenforcinginvarianceconstraints,DDGyieldssuperior
withaheuristic-basedaug- OOD performance with improved empirical convergence
80
mentationmethodAugMix andalsoyieldsinterpretableandcontrollablegenerativere-
[30]. Fig. 4 shows that 75 A C P S Avg sults. Inthiswork,weonlyconsiderthedisentangledeffects
theconstraintsoptimization Figure4.DataaugmentationonPACS between semantic and variation factors since it is hard to
brings great performance withdifferenttargetdomain. provideknowngenerativevariationfactorsthatmanifestthe
gainovervanillaERMand distribution shifts precisely. It remains an open problem
adataaugmentationheuristicsAugmix,especiallytheworst- toimprovethedisentanglementbetweendifferentvariation
case (i.e. the S domain in PACS) performance. The ef- factorswithlimitedsupervisionandevaluatethetreatment
fectivenessofthedataaugmentationprocedureinDDGis effectsofdataaugmentationinacontrollablemanner[83].
wellconnectedtomanyempiricalevidencein[29,75,82]
andalsovalidatesthehypothesisin[54]thattherichnessof
trainingdomaindataiscrucialforextrapolation.
)%(
ycaruccA
eulav
evitcejbO
ecnatsiD-A
References strainedstatisticallearning. InICASSP,2020. 2,3,5,13
[17] LuizFOChamon,SantiagoPaternain,MiguelCalvo-Fullana,
[1] KartikAhuja,KarthikeyanShanmugam,KushVarshney,and andAlejandroRibeiro.Constrainedlearningwithnon-convex
Amit Dhurandhar. Invariant risk minimization games. In losses. arXiv:2103.05134,2021. 4,5
ICML,2020. 2 [18] BeidiChen,WeiyangLiu,ZhidingYu,JanKautz,Anshumali
[2] IsabelaAlbuquerque,Joa˜oMonteiro,MohammadDarvishi, Shrivastava,AnimeshGarg,andAnimashreeAnandkumar.
TiagoHFalk,andIoannisMitliagkas.Generalizingtounseen Angularvisualhardness. InICML,2020. 1,2
domainsviadistributionmatching. arXiv:1911.00804,2019. [19] RickyTQChen,XuechenLi,RogerBGrosse,andDavidK
2,3 Duvenaud.Isolatingsourcesofdisentanglementinvariational
[3] MartinArjovsky,Le´onBottou,IshaanGulrajani,andDavid autoencoders. NIPS,2018. 3
Lopez-Paz. Invariantriskminimization. arXiv:1907.02893, [20] Elliot Creager, Jo¨rn-Henrik Jacobsen, and Richard Zemel.
2019. 1,2,6,7 Environmentinferenceforinvariantlearning. InICML,2021.
[4] Haoyue Bai, Rui Sun, Lanqing Hong, Fengwei Zhou, 3
NanyangYe,Han-JiaYe,S-HGaryChan,andZhenguoLi. [21] Andrea Dittadi, Frederik Tra¨uble, Francesco Locatello,
Decaug:Out-of-distributiongeneralizationviadecomposed Manuel Wuthrich, Vaibhav Agrawal, Ole Winther, Stefan
featurerepresentationandsemanticaugmentation. InAAAI, Bauer,andBernhardScho¨lkopf. Onthetransferofdisentan-
2021. 1,2,3 gledrepresentationsinrealisticsettings. InICLR,2021. 2,
[5] ShaiBen-David,JohnBlitzer,KobyCrammer,AlexKulesza, 15
FernandoPereira,andJenniferWortmanVaughan. Atheory [22] CynthiaDwork,MoritzHardt,ToniannPitassi,OmerRein-
oflearningfromdifferentdomains. Machinelearning,2010. gold,andRichardZemel. Fairnessthroughawareness. In
1,8 ITCS,2012. 3
[6] ShaiBen-David,JohnBlitzer,KobyCrammer,andFernando [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal
Pereira. Analysisofrepresentationsfordomainadaptation. Germain,HugoLarochelle,Franc¸oisLaviolette,MarioMarc-
InNIPS,2006. 1,15 hand,andVictorLempitsky. Domain-adversarialtrainingof
[7] AharonBen-Tal,LaurentElGhaoui,andArkadiNemirovski. neuralnetworks. JMLR,2016. 1,6,7
Robustoptimization. Princetonuniversitypress,2009. 1 [24] RobertGeirhos,Jo¨rn-HenrikJacobsen,ClaudioMichaelis,
[8] YoshuaBengio,AaronCourville,andPascalVincent. Repre- RichardZemel,WielandBrendel,MatthiasBethge,andFe-
sentationlearning:Areviewandnewperspectives. TPAMI, lixA.Wichmann. Shortcutlearningindeepneuralnetworks.
2013. 2 NatureMachineIntelligence,2020. 1
[9] AlainBerlinetandChristineThomas-Agnan. Reproducing [25] MuhammadGhifary,WBastiaanKleijn,MengjieZhang,and
kernelHilbertspacesinprobabilityandstatistics. Springer DavidBalduzzi.Domaingeneralizationforobjectrecognition
Science&BusinessMedia,2011. 4 withmulti-taskautoencoders. InICCV,2015. 6
[10] StephenBoyd,StephenPBoyd,andLievenVandenberghe. [26] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Convexoptimization. Cambridgeuniversitypress,2004. 13 Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and
[11] Pe´ter Ba´ndi, Oscar Geessink, Quirine Manson, Marcory
YoshuaBengio. Generativeadversarialnets. InNIPS,2014.
VanDijk,MaschenkaBalkenhol,MeykeHermsen,BabakEht-
6
eshamiBejnordi,ByungjaeLee,KyunghyunPaeng,Aoxiao [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost
Zhong,QuanzhengLi,FarhadGhazvinianZanjani,Svitlana domaingeneralization. InICLR,2021. 1,3,6,19
Zinger,KeisukeFukuta,DaisukeKomura,VladoOvtcharov, [28] Tatsunori Hashimoto, Megha Srivastava, Hongseok
ShenghuaCheng,ShaoqunZeng,JeppeThagaard,AndersB. Namkoong,andPercyLiang. Fairnesswithoutdemographics
Dahl,HuangjingLin,HaoChen,LudwigJacobsson,Martin inrepeatedlossminimization. InICML,2018. 1,3,4,8
Hedlund,MelihC¸etin,ErenHalıcı,HunterJackson,Richard [29] DanHendrycks,XiaoyuanLiu,EricWallace,AdamDziedzic,
Chen,FabianBoth,Jo¨rgFranke,HeidiKu¨sters-Vandevelde, RishabhKrishnan,andDawnSong. Pretrainedtransformers
Willem Vreuls, Peter Bult, Bram van Ginneken, Jeroen improveout-of-distributionrobustness. InACL,2020. 1,3,8
van der Laak, and Geert Litjens. From detection of indi- [30] Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret
vidualmetastasestoclassificationoflymphnodestatusatthe Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Aug-
patientlevel:Thecamelyon17challenge. IEEETransactions mix:Asimpledataprocessingmethodtoimproverobustness
onMedicalImaging,2019. 6 anduncertainty. InInternationalConferenceonLearning
[12] FabioMCarlucci,AntonioD’Innocente,SilviaBucci,Bar- Representations,2020. 6,8,18
baraCaputo,andTatianaTommasi. Domaingeneralization [31] IrinaHiggins,LoicMatthey,ArkaPal,ChristopherBurgess,
bysolvingjigsawpuzzles. InCVPR,2019. 1,15 Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
[13] RahmaChaabouni,EugeneKharitonov,DianeBouchacourt, AlexanderLerchner.beta-vae:Learningbasicvisualconcepts
EmmanuelDupoux, andMarcoBaroni. Compositionality withaconstrainedvariationalframework. InICLR,2017. 3
andgeneralizationinemergentlanguages. InACL,2020. 15 [32] FelixHill,AndrewLampinen,RosaliaSchneider,Stephen
[14] KrzysztofChalupka,PietroPerona,andFrederickEberhardt. Clark,MatthewBotvinick,JamesL.McClelland,andAdam
Visualcausalfeaturelearning. InUAI,2015. 3 Santoro. Environmentaldriversofsystematicityandgeneral-
[15] LuizChamonandAlejandroRibeiro.Probablyapproximately izationinasituatedagent. InICLR,2020. 15
correctconstrainedlearning. InNeurIPS,2020. 2,3,4 [33] KurtHornik,MaxwellStinchcombe,andHalbertWhite.Mul-
[16] LuizFOChamon,SantiagoPaternain,MiguelCalvo-Fullana, tilayer feedforward networks are universal approximators.
and Alejandro Ribeiro. The empirical duality gap of con-
Neuralnetworks,1989. 4 Whichtrainingmethodsforgansdoactuallyconverge? In
[34] XunHuangandSergeBelongie. Arbitrarystyletransferin ICML,2018. 7
real-time with adaptive instance normalization. In ICCV, [54] MiltonLleraMontero,CasimirJHLudwig,RuiPonteCosta,
2017. 7 GauravMalhotra,andJeffreyBowers. Theroleofdisentan-
[35] XunHuang,Ming-YuLiu,SergeBelongie,andJanKautz. glementingeneralisation. InICLR,2021. 2,3,8,15
Multimodal unsupervised image-to-image translation. In [55] K.Muandet,D.Balduzzi,andB.Scho¨lkopf. Domaingener-
ECCV,2018. 7 alizationviainvariantfeaturerepresentation. InICML,2013.
[36] PhillipIsola,Jun-YanZhu,TinghuiZhou,andAlexeiAEfros. 1,2
Image-to-imagetranslationwithconditionaladversarialnet- [56] Jonas Peters, Peter Bu¨hlmann, and Nicolai Meinshausen.
works. InCVPR,2017. 7 Causal inference by using invariant prediction: identifica-
[37] PritishKamath, AkileshTangella, DanicaSutherland, and tionandconfidenceintervals. JournaloftheRoyalStatistical
Nathan Srebro. Does invariant risk minimization capture Society.SeriesB(StatisticalMethodology),2016. 1,2
invariance? InAISTATS,2021. 1 [57] AntoinePlumerault,Herve´LeBorgne,andCe´lineHudelot.
[38] Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. Controlling generative models with continuous factors of
Learning the difference that makes a difference with variations. InICLR,2019. 6
counterfactually-augmenteddata. InICLR,2019. 3 [58] AlexanderRobey,LuizChamon,GeorgePappas,HamedHas-
[39] HyunjikKimandAndriyMnih. Disentanglingbyfactorising. sani, and Alejandro Ribeiro. Adversarial robustness with
InICML,2018. 3 semi-infiniteconstrainedlearning. AdvancesinNeuralInfor-
[40] DiederikPKingmaandJimmyBa. Adam: Amethodfor mationProcessingSystems,34,2021. 3,4
stochasticoptimization. InICLR,2015. 17 [59] Alexander Robey, Hamed Hassani, and George J. Pappas.
[41] PangWeiKoh,ShioriSagawa,SangMichaelXie,Marvin Model-basedrobustdeeplearning: Generalizingtonatural,
Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Ya- out-of-distributiondata,2020. 3
sunaga,RichardLanasPhillips,IrenaGao,TonyLee,etal. [60] Alexander Robey, George J. Pappas, and Hamed Hassani.
Wilds: A benchmark of in-the-wild distribution shifts. In Model-baseddomaingeneralization. InNeurIPS,2021. 1,2,
ICML,2021. 6 3,4,5,13,19
[42] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, [61] M. Rojas-Carulla, B. Scho¨lkopf, R. Turner, and J. Peters.
AmyZhang,JonathanBinas,DinghuaiZhang,RemiLePriol, Invariantmodelsforcausaltransferlearning. JMLR,2018. 1
andAaronCourville. Out-of-distributiongeneralizationvia [62] Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski.
riskextrapolation(rex). InICML,2021. 1,2 Therisksofinvariantriskminimization. InICLR,2021. 1,2
[43] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo [63] Kevin Roth, Aure´lien Lucchi, Sebastian Nowozin, and
Silva. Counterfactualfairness. InNeurIPS,2017. 3 ThomasHofmann. Stabilizingtrainingofgenerativeadver-
[44] DaLi,YongxinYang,Yi-ZheSong,andTimothyHospedales. sarialnetworksthroughregularization. InNIPS,2017. 1
Learningtogeneralize:Meta-learningfordomaingeneraliza- [64] Ruichu,Cai,Zijian,Pengfei,Wei,Jie,Qiao,Kun,Zhang,and
tion. InAAAI,2018. 6,7 Zhifengand. Learningdisentangledsemanticrepresentation
[45] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M fordomainadaptation. InIJCAI,2019. 2,15
Hospedales.Deeper,broaderandartierdomaingeneralization. [65] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,San-
InICCV,2017. 6 jeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,
[46] HaoliangLi,SinnoJialinPan,ShiqiWang,andAlexCKot. AdityaKhosla,MichaelBernstein,etal. Imagenetlargescale
Domaingeneralizationwithadversarialfeaturelearning. In visualrecognitionchallenge. IJCV,2015. 1
CVPR,2018. 6,7 [66] ShioriSagawa,PangWeiKoh,TatsunoriBHashimoto,and
[47] YaLi,XinmeiTian,MingmingGong,YajingLiu,Tongliang Percy Liang. Distributionally robust neural networks for
Liu,KunZhang,andDachengTao. Deepdomaingeneraliza- groupshifts:Ontheimportanceofregularizationforworst-
tionviaconditionalinvariantadversarialnetworks. InECCV, casegeneralization. ICLR,2020. 1,4,6,7
2018. 1,6,7 [67] B.Scho¨lkopf,D.Janzing,J.Peters,E.Sgouritsa,K.Zhang,
[48] WeiyangLiu,RongmeiLin,ZhenLiu,JamesMRehg,Liam andJ.Mooij. Oncausalandanticausallearning. InICML,
Paull,LiXiong,LeSong,andAdrianWeller. Orthogonal 2012. 1
over-parameterizedtraining. InCVPR,2021. 1 [68] Bernhard Scho¨lkopf, Francesco Locatello, Stefan Bauer,
[49] WeiyangLiu,Yan-MingZhang,XingguoLi,ZhidingYu,Bo NanRosemaryKe,NalKalchbrenner,AnirudhGoyal,and
Dai,TuoZhao,andLeSong. Deephypersphericallearning. YoshuaBengio. Towardcausalrepresentationlearning. Pro-
InNIPS,2017. 1 ceedingsoftheIEEE,2021. 2
[50] MingshengLong,YueCao,JianminWang,andMichaelJor- [69] YujunShen,JinjinGu,XiaoouTang,andBoleiZhou. Inter-
dan. Learning transferable features with deep adaptation pretingthelatentspaceofgansforsemanticfaceediting. In
networks. InICML,2015. 8 CVPR,2020. 6
[51] ChaochaoLu,YuhuaiWu,Jos´eMiguelHerna´ndez-Lobato, [70] YujunShenandBoleiZhou. Closed-formfactorizationof
andBernhardScho¨lkopf. Nonlinearinvariantriskminimiza- latentsemanticsingans. InCVPR,2021. 6
tion:Acausalapproach. arXiv:2102.12353,2021. 1,2 [71] HidetoshiShimodaira. Improvingpredictiveinferenceun-
[52] DavidMadras,ElliotCreager,ToniannPitassi,andRichard dercovariateshiftbyweightingthelog-likelihoodfunction.
Zemel. Learningadversariallyfairandtransferablerepresen- Journalofstatisticalplanningandinference,2000. 3
tations. InICML,2018. 3 [72] AmanSinha,HongseokNamkoong,andJohnDuchi. Certify-
[53] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. ingsomedistributionalrobustnesswithprincipledadversarial
training. InICLR,2018. 4
[73] Baochen Sun and Kate Saenko. Deep coral: Correlation
alignmentfordeepdomainadaptation. InECCV,2016. 6,7
[74] Antonio Torralba and Alexei A Efros. Unbiased look at
datasetbias. InCVPR,2011. 6
[75] LifuTu,GarimaLalwani,SpandanaGella,andHeHe. An
empiricalstudyonrobustnesstospuriouscorrelationsusing
pre-trainedlanguagemodels. TACL,2020. 3,8
[76] VladimirVapnik. Thenatureofstatisticallearningtheory.
Springerscience&businessmedia,1999. 6,7
[77] RiccardoVolpi,HongseokNamkoong,OzanSener,JohnC
Duchi,VittorioMurino,andSilvioSavarese. Generalizing
tounseendomainsviaadversarialdataaugmentation. NIPS,
2018. 1,2
[78] HaohanWang,ZeyiHuang,HanlinZhang,andEricXing.
Towardlearninghuman-alignedcross-domainrobustmodels
bycounteringmisalignedfeatures. arXiv:2111.03740,2021.
1,4
[79] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and
QuocLe. Unsuperviseddataaugmentationforconsistency
training. NeurIPS,2020. 3
[80] ShenYan,HuanSong,NanxiangLi,LincanZou,andLiu
Ren. Improveunsuperviseddomainadaptationwithmixup
training. arXiv:2001.00677,2020. 6,7
[81] KunZhang, BernhardScho¨lkopf, KrikamolMuandet, and
ZhikunWang. Domainadaptationundertargetandcondi-
tionalshift. InICML,2013. 1
[82] LingZhang,XiaosongWang,DongYang,ThomasSanford,
StephanieHarmon,BarisTurkbey,HolgerRoth,AndriyMy-
ronenko,DaguangXu,andZiyueXu. Whenunseendomain
generalizationisunnecessary?rethinkingdataaugmentation.
arXiv:1906.03347,2019. 1,3,8
[83] Yi-FanZhang,HanlinZhang,ZacharyCLipton,LiErranLi,
andEricPXing. Cantransformersbestrongtreatmenteffect
estimators? arXiv:2202.01336,2022. 8
[84] HanZhao,ShanghangZhang,GuanhangWu,Jose´MFMoura,
JoaoPCosteira,andGeoffreyJGordon.Adversarialmultiple
sourcedomainadaptation. NIPS,2018. 1
[85] ZhedongZheng,XiaodongYang,ZhidingYu,LiangZheng,
YiYang,andJanKautz. Jointdiscriminativeandgenerative
learningforpersonre-identification. InCVPR,2019. 7
Appendix
A.Proofs
A.1.Proofforparameterizationgap
ProofA.1 Recallthefeasibilityassumption3ensuresthestrongdualitysuchthattheprimalanddualoptimalobjectivesare
equal,whichmeansfor∀λ˜ ∈R+,f˜,f˜ ∈H,thefollowingsaddlepointconditionholds
s v
R(f∗,f∗,λ˜)≤maxminR(f ,f ,λ)=D∗ =P∗
s v s v
λ fs,fv
(14)
=minmaxR(f ,f ,λ)≤R(f˜,f˜,λ∗)
s v s v
fs,fv λ
RecallthedefinitionofD (cid:15)∗andbytheinclusionrelationH
θ
⊆F,wecanderivethelowerboundas
D (cid:15)∗(γ)(cid:44)max min L(θ)+λL con(θ,φ)
λ θ,φ∈H
≥ min L(θ)+λL con(θ,φ),∀λ∈R+ (15)
θ,φ∈H
≥ min L(f s)+λL con(f s,f v)=P∗
fs,fv∈F
Fortheupperbound,weaddandsubtractR≡R(f s,f v,λ)= min L(f s)+ min λL con(f s,f v)fromD (cid:15)(cid:63)(γ)toget
fs∈F fs,fv∈F
D (cid:15)(cid:63)(γ)=max min L(θ)+λL con(θ,φ)+R−R
λ θ,fs,fv
=max min R+[L(θ)−L(f )]+λ[L (θ,φ)−L (f ,f )] (16)
s con con s v
λ θ,fs,fv
Withaslightabuseofnotation,EisshortforE (x,y)∼P(X,Y),x˜∼P(X). Thenweconsiderthecombinationofthesecondand
thirdtermas
(cid:20) (cid:21)
L(θ)−L(f )
[1,λ] s
L (θ,φ)−L (f ,f )
con con s v
(cid:13)(cid:20) (cid:21)(cid:13)
≤(1+(cid:107)λ(cid:107) 1)(cid:13) (cid:13)
(cid:13) L
con(θL ,( φθ )) −− LL co( nf s ()
f s,f v)
(cid:13) (cid:13)
(cid:13)
∞
=(1+|λ|)max{L(θ)−L(f ),L (θ,φ)−L (f ,f )}
s con con s v
(17)
=(1+|λ|)max{E[(cid:96)(h (x;θ);y)−(cid:96)(f (x);y)],E[d(x,D(h (x;θ),h (x˜;φ)))−d(x,D(f (x),f (x˜)))]}
s s s v s v
≤(1+|λ|)E{max[[(cid:96)(h (x;θ);y)−(cid:96)(f (x);y)],E[d(x,D(h (x;θ),h (x˜;φ)))−d(x,D(f (x),f (x˜)))]]}
s s s v s v
≤(1+|λ|)E{max[L |h (x;θ)−f (x)|,L |D(h (x;θ),h (x˜;φ))−D(f (x),f (x˜))|]}
(cid:96) s s d s v s v
≤(1+|λ|)Emax{L (cid:15) ,L (cid:15) }
(cid:96) s d g
=(1+|λ|)max{L (cid:15) ,L (cid:15) }
(cid:96) s d g
wherethefirstinequalityisusingHo¨lder’sinequality(18)whenp=1,q =∞andthesecondoneisbytheconvexityof
max-normandJensen’sinequality. ThethirdinequalityisapplyingL andL lipschitznesson(cid:96)andd. Thefourthoneisdue
(cid:96) d
to(cid:15) and(cid:15) parameterizationonf andD.
s g s
n (cid:32) n (cid:33)1/p(cid:32) n (cid:33)1/q
(cid:88) |x y |≤ (cid:88) |x |p (cid:88) |y |q (18)
k k k k
k=1 k=1 k=1
ThenEq. (16)becomes
D (cid:15)(cid:63)(γ)≤maxminR+(1+|λ|)max{L (cid:96)(cid:15) s,L d(cid:15) g}(cid:44)D p∗ (19)
λ fx,fv
InordertousestrongdualitytoboundD∗,weneedtoconstructprimalproblemwhoseoptimumisP∗fromD∗. Therefore,
p p p
theremainingproofistoshowD∗isthedualproblemtoaconstraintstatisticallearningproblemwithperturbationfunction
p
asm=max{L (cid:15) ,L (cid:15) }. Specifically,whenλ>0,D∗canbeexpandedandrearrangedas
(cid:96) s d g p
D p∗ =maxminL(f s)+λL con(f s,f v)+(1+|λ|)m
λ fs,fv
=maxmin(cid:96)(f (x;θ);y)+m+λ[d(x,D(f (x;θ),f (x˜;φ)))−γ+m] (20)
s s v
λ fs,fv
Bythefeasibilityassumption3,Eq. (20)canbeconsideredasthedualproblemofthefollowingone:
P p(cid:63)(γ)(cid:44) min L(f s)+m
fs∈F (21)
s.t. d(x,D(f (x;θ),f (x˜;φ)))≤γ−m
s v
Denote f∗ and f∗ as the primal solutions to P∗, λ∗ and λ∗ as the dual variable that give solutions D∗, P∗. With the
s v p p p p
regularityassumption2,thesaddlepointconditionshowsfor∀f˜,f˜ ∈Handλ,λ∗ ∈R+,itholdsthat
s v p
R(f s∗ ,p,f v∗ ,p,λ˜)+(1+|λ˜|)≤maxminR(f s,f v,λ)+(1+|λ|)m=D p∗ =P p∗
λ fs,fv
= minmaxR(f ,f ,λ)+(1+|λ|)m (22)
s v
fs,fv λ
≤R(f˜,f˜,λ∗)+(1+|λ∗|)m
s v p p
RecallthatEq. (22)holdsfor∀f˜,f˜ ∈H. Letf˜ =f∗,f˜ =f∗,thenweuseittoupperboundD∗as
s v s s v v (cid:15)
D∗(γ)≤P∗ ≤R(f∗,f∗,λ∗)+(1+|λ∗|)m=P∗(γ)+(1+|λ∗|)m (23)
(cid:15) p s v p p
ThelaststepisduetoEq. (14).
A.2.Proofforempiricalgap
ProofA.2 Similarto[16,60],byKKTconditionsandcomplementaryslacknessconditions[10]shows
λ∗ (cid:15)(E x,x˜∼P(X)[d(x,D(f s(x;θ (cid:15)∗),f v(x˜;φ∗ (cid:15))))−γ]=0
n n
(cid:88)(cid:88) (24)
λ∗ ( d(x ,D(f (x ;θ∗),f (x ;φ∗))−γ)=0
(cid:15),n i s i (cid:15) v j (cid:15)
i=1 j(cid:54)=i
where(θ∗,φ∗,λ∗)and(θ∗ ,φ∗ ,λ∗ )areprimal-dualpairsforachievingtheoptimumD∗(γ)andD∗ (γ).
(cid:15) (cid:15) (cid:15) (cid:15),n (cid:15),n (cid:15),n (cid:15) (cid:15),n
Eq. (24)impliestheconstraint-relatedtermsintheobjectivestobezero,thenconsidertheremainingtermas
D (cid:15)∗(γ)=E[(cid:96)(f s(x;θ);y)](cid:44) M(θ (cid:15)∗)
D (cid:15)∗
,n(γ)=(cid:88)n
(cid:96)(f s(x i),y i)(cid:44)Mˆ(θ (cid:15)∗ ,n)
(25)
i=1
Thustheempiricalgapreducesto
|D∗(γ)−D∗ (γ)|=|M(θ∗)−Mˆ(θ∗ )| (26)
(cid:15) (cid:15),n (cid:15) (cid:15),n
Usingthefactthatθ∗andθ∗ areoptimalforM(θ∗)andMˆ(θ∗ ),thefollowingholds
(cid:15) (cid:15),n (cid:15) (cid:15),n
M(θ∗)−Mˆ(θ∗)≤M(θ∗)−Mˆ(θ∗ )≤M(θ∗ )−Mˆ(θ∗ ) (27)
(cid:15) (cid:15) (cid:15) (cid:15),n (cid:15),n (cid:15),n
Therefore,usingtheabovelowerandupperbound,wecanboundEq. (26)as
|D∗(γ)−D∗ (γ)|≤max{|M(θ∗)−Mˆ(θ∗)|,|M(θ∗ )−Mˆ(θ∗ )|} (28)
(cid:15) (cid:15),n (cid:15) (cid:15) (cid:15),n (cid:15),n
ThenweresorttotheclassicalVC-dimensionboundsfortheabovetwotermsinEq. (28)asfollows
(cid:115)
|M(θ)−Mˆ(θ)|≤2B
1
(cid:20) 1+log(cid:18) 4(2n)dVC(cid:19)(cid:21)
(29)
n δ
holdswithprobability1−δwhend istheVCdimensionforallθ.
VC
CombingEq. (28)and(29)completestheproof.
A.3.Proofforempiricaldualitygap
ProofA.3 Simplycombiningtheresultsintheabovelemmas,i.e. parameterizationgapandempiricalgap,viaapplyingthe
triangleinequalitycompletestheproof.
(cid:12) (cid:12)P(cid:63)−Dε(cid:63) ,n(γ)(cid:12) (cid:12)=(cid:12) (cid:12)P(cid:63)+D(cid:15)(cid:63)(γ)−D(cid:15)(cid:63)(γ)−D(cid:15)(cid:63) ,n(γ)(cid:12)
(cid:12)
≤|P(cid:63)−Dε(cid:63)(γ)|+|D(cid:15)(cid:63)(γ)−D(cid:15)(cid:63) ,n(γ)|
(cid:115)
(cid:20) (cid:18) (cid:19)(cid:21)
1 4(2n)dvc
≤(1+|λ|)m+2B 1+log
n δ
B.DomainGeneralizationbyLearningonFictitiousDistributions
Thissectiongivesajustificationfordisentanglementfromadifferentperspectivebyconnectingthedotswithclassical
domainadaptation. Specifically,weconstructafictitiousdistributiontoextendittotheDGsettinganddecomposethetarget
learningobjectiveintoempiricallearningerrors,domaindivergenceandsourcedomaindatadiversity. Moreover,weshowthat
learningdisentangledrepresentationsgivesatighterriskupperbound.
Withaslightabuseofnotation,letHbeahypothesisspaceanddenoteD˜ astheinduceddistributionoverfeaturespaceZ
foreverydistributionDovertherawspace. DefineDi asthesourcedistributionoverX,whichenablesamixtureconstruction
S
ofsourcedomainsasDα =(cid:80)Ns α Di(·). DenoteafictitiousdistributionDα =(cid:80)Ns α∗Di(·)astheconvexcombination
S i=1 i S U i=1 i S
ofsourcedomainswhichistheclosesttoD ,whereα∗,...,α∗ =argmin d (D ,(cid:80)Ns α Di(·)). Thefictitious
U 1 NS α1,...,αNs H U i=1 i S
distributioninducesafeaturespacedistributionD˜α =(cid:80)Ns α∗D˜i(·). Thefollowinginequalityholdsfortherisk(cid:15) (h)on
U i=1 i S U
anyunseentargetdomainD .
U
(cid:15) (h)≤λ
+(cid:88)NS
α (cid:15) (h)+d (D˜α,D˜α)+d (D˜ ,D˜α) (30)
U α i S,i H U S H U U
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
i=1
(cid:124) (cid:123)(cid:122) (cid:125) 2 Divergence 3 Diversity
1 Empirical
whereλ istheriskoftheoptimalhypothesisonthemixturesourcedomainDαandD .
α S U
We define the symmetric difference hypothesis space as H∆H = {h(x)⊕h(cid:48)(x):h,h(cid:48) ∈H}, where ⊕ is the XOR
operator. Applying[6],wehave
(cid:15) (h)≤λ +Pr [Z (cid:52)Z∗]
U U DU h h
≤λ
U
+Pr
D
Sα[Z h(cid:52)Z h∗]+|Pr
D
Sα[Z h(cid:52)Z h∗]−Pr DU[Z h(cid:52)Z h∗]|
≤λ
U
+Pr
D
Sα[Z h(cid:52)Z h∗]+d H(D˜ U,D˜ Sα)
≤λ
U
+Pr
D
Sα[Z h(cid:52)Z h∗]+d H(D˜ Uα,D˜ Sα)+d H(D˜ U,D˜ Uα)
(31)
≤λ
+(cid:88)NS
α (cid:15) (h)+d (D˜α,D˜α)+d (D˜ ,D˜α)
α i S,i H U S H U U
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
i=1
(cid:124) (cid:123)(cid:122) (cid:125) 2 Divergence 3 Diversity
1 Empirical
Thefourthinequalityholdsbecauseofthetriangleinequality. WeprovidetheexplanationforourboundintheEq. (30)and
Eq. (31). Thesecondtermistheempiricallossfortheconvexcombinationofallsourcedomains. Thethirdtermcorresponds
to“Towhatextentcantheconvexcombinationofthesourcedomainapproximatethetargetdomain”. Theminimization
ofthethirdtermrequiresdiversedataorstrongdataaugmentation,suchthattheunseendistributionlieswithintheconvex
combinationofsourcedomains. Forthefourthterm,thefollowingequationholdsforanytwodistributionsD(cid:48) ,D(cid:48)(cid:48),whichare
U U
theconvexcombinationsofsourcedomains[12]
(cid:88)NS (cid:88)NS
d ≤ α α d [D ,D ] (32)
H[D(cid:48) ,D(cid:48)(cid:48)] l k H S,l S,k
U U
l=1k=1
Suchanupperboundwillbeminimizedwhend [D ,D ]=0,∀l,k ∈{1,...,N }. Namelyprojectingthesourcedomain
H S,l S,k S
dataintoafeaturespace,wherethesourcedomainlabelsarehardtodistinguish.
Theabove 3 Diversitytermisalsosupportedbytheevidencethatcompositionalgeneralizationandextrapolationcan
beimprovedifthetrainingdomaindataarerichenough[13,32,54]. Tothisend,onecanobviouslysimulatedatapoints
withpredetermineddataaugmentationmethodssuchasrotating,cropping,Gaussianblur,colorjitter,etc. However,their
developmentsrequirepriorknowledgeanddomain-specificexpertiseliketranslation-invarianceonimages,whichislikelyto
failintheunseendomainduetodistributionshifts. Itmotivateslearningdisentangledrepresentationsthataretransferable
across various domains [21]. Thus we discuss the benefits of disentanglement on the domain generalization gap in the
followingsection. AssumethatthesemanticandthevariationfactorsaredisentangledinthelatentspaceS andV,thenthe
errors[64]onthedisentangledsourceandtargetdomainwithahypothesishare
(cid:15) (h)=(cid:15)s (h)+(cid:15)v (h),(cid:15) (h)=(cid:15)s(h)+(cid:15)v(h) (33)
S,i S,i S,i U U U
Givenh∗ =argmin (cid:0) (cid:15)s (h),(cid:15)v (h)(cid:1) ∀i∈{1,...,N },since(cid:15) (h)=(cid:15)s(h)+(cid:15)v(h),combiningEq. (30)andwe
h∈H S,i S,i S U U U
have
(cid:15)s(h)+(cid:15)v(h)≤λ
+(cid:88)NS
α (cid:15) (h)+d (D˜α,D˜α)+d (D˜ ,D˜α) (34)
U U α i S,i H U S H U U
i=1
where λ = (cid:15) (h∗)+(cid:80)NS α (cid:15) (h∗) = (cid:15)s(h∗)+(cid:15)v(h∗)+(cid:80)NS α (cid:15)s (h∗)+(cid:80)NS α (cid:15)v (h∗). Then the upper
α U i=1 i S,i U U i=1 i S,i i=1 i S,i
boundfortheunseendomaincanbefurtherderivedasfollow,
(cid:15)s(h)≤λ
+(cid:88)NS
α (cid:15) (h)+d (D˜α,D˜α)+d (D˜ ,D˜α)−(cid:15)v(h) (35)
U α i S,i H U S H U U U
i=1
CombiningEq.(30)andEq.(33),wehave
(cid:15)s(h)≤λ
+(cid:88)NS
α (cid:15) (h)+d (D˜α,D˜α)+d (D˜ ,D˜α)−(cid:15)v(h) (36)
U α i S,i H U S H U U U
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
i=1
(cid:124) (cid:123)(cid:122) (cid:125) 2 Divergence 3 Diversity 4
1 Empirical
where 1 denotestheempiricallossovereverysourcedomain, 2 meansdivergenceminimizationamongsourcedomains
and 3 encourages the diversity and coverage of the mixture of source domains. The above result shows the benefits of
disentanglementoverrepresentationspaces. Thusweproposetousetwoseparateencodersrepresentingsemanticandvariation
subspacesrespectivelyinthefollowingsection. Suchaformulationcombinedwiththedisentanglementterm 4 inEq. (36)
impliesthatweshouldperformERMoverthesemanticspaceonly. Theaboveanalysisessentiallyjustifiesthedesignof
DDGfromaclassicaldomainadaptationperspective: optimizingempiricalriskoversemanticspacewhilepromotingdiversity
anddivergencebytakingdisentanglementasaconstraint.
C.ExperimentalSettings
C.1.OtherTrainingDetails
WeoptimizeallmodelsusingAdam[40]. Forallthedetailedhyperparametersettings,pleaserefertoourcodewhichis
publiclyavailableathttps://github.com/hlzhang109/DDG.
C.2.DatasetStatisticsandVisualization
WeshowsomeimagesofthedatasetsinFig.7togiveanintuitivecomparisonamongtheseimagedatasets. Onecan
observethattheseimageshaveadiversesetofstyles,makingitverychallengingtotransferknowledgefromonetoanother.
Art Painting Cartoon Photo Sketch
Caltech101LabelMe SUN09 VOC2007
Center1 Center2 Center3 Center4 Center5
(a) (b) (c)
Figure7.SamplesofDGdatasets.Thetrainingdata(a)PACSand(b)VLCS(c)Wildsareshown.PACShasfourdomainsart(A),cartoons(C),photos(P),
sketches(S).VLCScontainsfourdomainsCaltech101(C),LabelMe(L),SUN09(S),VOC2007(V).TheWILDSdatasetincludesdatafromfivedifferent
medicalcentersasdomains.
goD
tnahpelE
effariG
ratiuG
esroH
esuoH
nosreP
driB
raC
riahC
goD
nosreP
lamroN
romuT
D.AdditionalExperimentalResults
QualitativecomparisonwithAugMix. LookingintotheFig. 8,itisharderfortheheuristic-basedmethodAugMixto
generatediversesamplesviainterpolationfortrainingcomparedwithDDGasshowninFig. 3andFig. 10b.
Morequalitativeresultsviainterpolation. Fig. 9showcasestheresultsofcombiningthesemanticcodeofoneimage
andthemixtureoftwovariationcodes. Resultsshowthatthemodelcangeneratesampleswithintermediatevariationstates.
Morequalitativeresultsviaswappingvariationandsemanticfactors. Weshowcasethequalitativeresultsofswapping
variationandsemanticfactorswithPACSinFig. 10b,MNISTinFig. 10a,andWILDSinFig. 10c. Theresultsdemonstrates
thestrongdisentangledcapabilityofDDG.SomeinterestingobservationsareDDGlearnsbothintra-(e.g. thickness)and
inter-domain(e.g. rotatedangle)variationsoverRotatedMNIST.DDGalsomaintainssemanticinformationlikethecolorof
distinctfeaturesacrossvariation-richdatalikePACS.
(a) (b)
Figure8.TheaugmentedsamplesfromAugMix[30].ThesecondandthirdrowsaregeneratedbyapplyingAugMixtothefirstrow.
Figure9.InterpolationviamixingresultsonPACS.
⊕(
+
)
=
⊕(
+
)
=
⊕ ⊕
(a)RotatedMNIST
⊕ ⊕
(b)PACS
⊕ ⊕ ⊕
(c)Wilds
Figure10. QualitativedisentanglementresultsonRotatedMNIST,PACSandWilds. Ineverypanel,thetrainingdatainthefirstrowmanifeststhe
semanticfactors.
NumericalcomparisonwithMBDG.WequantativelycompareDDGwithMBDG[60]asinTable2. Foraconsistent
comparison,werunthesourcecodeofauthorsunderatestdomainvalidationprotocal[27]onPACSwiththeresultsas:
A C P S Avg
MBDG 82.0±0.0 78.4±0.01 93.9±0.0 85.0±0.0 85.8
MBDG 84.9±0.0 84.9±0.0 93.9±0.0 85.6±0.0 87.3
Reg
DDG 88.9±0.6 85.0±1.9 97.2±1.2 84.3±0.7 88.9
Table2.NumericalresultsforcomparingDDGandMBDG.
ThoughadoptingsimilarPACCLframeworksandprimal-dualalgorithms,DDGcanconsistentlyoutperformMBDGand
itsvariantexceptoverdomainS.TheprimaryreasonbehindtheclearperformancegainofDDGcanbethatDDGisbetterat
capturingvariationswithindataviarandomsamplingwithoutrelyingondomainlabels. Specifically,parameterizingh ,h ,D
s v
=
=
=
=
=
=
=
andconstrainthembasedondisentanglementmakesthemodelmorerobusttobothinter-andintra-domainnuisancefactors
comparedtoMBDGthatonlyuseapretrainedgeneratortosimulateinter-domainvariations. Moreover,theperformance
gainisalsopartlyduetothethreemajordifferencesbetweenourapproachandMBDGwehighlightintheRelatedWork:
First, our upper bound of the parameterization gap is tighter under mild conditions, whereas MBDG requires unrealistic
assumptionsonthedistancemetric,i.e.,d(·,·)satisfiesLipschitz-likeinequalityonbotharguments,whichisstrongerthanour
normalL Lipschitznessassumption;second,MBDGconsumesadditionaldomainlabels,whichareprohibitivelyexpensive
d
oreveninfeasibletoobtaininsafety-criticalapplicationsorthosecontainingsensitivedemographics;third,DDGenforces
invarianceconstraintsviaparameterizingsemanticandvariationencoders,whichdoesnotbelongtoamodel-basedapproach.
Incontrast,MBDGrequiresapre-traineddomaintransformationmodel(e.g.,CycleGAN)duringtraining,whichmayresultin
sub-optimalsolutionsandparameterinefficiency,whileDDGismoreflexiblebytreatingthisasadesignchoice.

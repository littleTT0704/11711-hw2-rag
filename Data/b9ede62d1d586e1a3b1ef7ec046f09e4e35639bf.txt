Off the Beaten Path: Let’s Replace Term-Based Retrieval
with k-NN Search
Leonid Boytsov David Novak Yury Malkov
CarnegieMellonUniversity MasarykUniversity InstituteofAppliedPhysicsRAS
Pittsburgh,PA,USA Brno,CzechRepublic NizhnyNovgorod,Russia
srchvrs@cs.cmu.edu david.novak@fi.muni.cz yurymalkov@mail.ru
Eric Nyberg
CarnegieMellonUniversity
Pittsburgh,PA,USA
ehn@cs.cmu.edu
ABSTRACT Modernretrievalsystemsanswerqueriesinapipelinefashion.
First, an term-based inverted index is used to generate a list of
Retrievalpipelinescommonlyrelyonaterm-basedsearchtoob-
candidatedocumentscontainingsomeorallqueryterms.Second,
taincandidaterecords,whicharesubsequentlyre-ranked. Some
thislistisrefinedandre-ranked. Afewhighly-rankeddocuments
candidatesaremissedbythisapproach,e.g.,duetoavocabulary
arethenpresentedtotheuser.
mismatch.Weaddressthisissuebyreplacingtheterm-basedsearch
Re-rankingmaybecarriedoutinseveralsteps,whereearliersteps
withagenerick-NNretrievalalgorithm,whereasimilarityfunc-
employcheaprankingfunctions—suchasBM25[58]orlanguage
tioncantakeintoaccountsubtletermassociations.Whileanexact
models[55]—relyingsolelyontermoccurrencestatistics.Afinal,
brute-forcek-NNsearchusingthissimilarityfunctionisslow,we
aggregation,steptypicallycombinesnumerousrelevancesignals
demonstratethatanapproximatealgorithmcanbenearlytwoorders
generatedbyupstreamcomponents.Theaggregationstepisoften
ofmagnitudefasterattheexpenseofonlyasmalllossinaccuracy.
carriedoutusingstatisticallearning-to-rankalgorithms[38].
Aretrievalpipelineusinganapproximatek-NNsearchcanbemore
Thisfilter-and-refineapproachhingesontheassumptionthata
effectiveandefficientthantheterm-basedpipeline.Thisopensup
term-basedsearchgeneratesareasonablycompletelistofcandidate
new possibilities for designing effective retrieval pipelines. Our
documents. However, this assumption is not fully accurate, in
software(includingdata-generatingcode)andderivativedatabased
ontheStackOverflowcollectionisavailableonline.1 Thisrevision particular,becauseofavocabularygap,i.e.,amismatchbetween
queryanddocumenttermsdenotingsameconcepts.Thevocabulary
isaslightlyextendedversionoftherespectiveCIKM’16paper.
gapisawell-knownphenomenon. Furnasetal.[24]showedthat,
givenarandomconcept,thereislessthana20%chancethattwo
Keywords randomlyselectedhumansdenotethisconceptusingthesameterm.
ZhaoandCallan[77]foundthatatermmismatchratio—i.e.,arate
k-NNsearch;IBMModel1;non-metricspaces;LSH
atwhichaquerytermfailstoappearinarelevantdocument—is
roughly50%.
1. INTRODUCTION Furthermore,accordingtoFurnasetal.[24],focusingonlyona
Duetoadvancesincomputing,afull-textsearchhasbecomea fewsynonymsisnotsufficienttoeffectivelybridgethevocabulary
ubiquitousinformationtechnology.However,thistechnologystill gap. Specifically, it was discovered that, after soliciting 15 syn-
largelyreliesonmemorizationofdocumenttermsandmatching onymsdescribingasingleconceptfromapanelofsubjectexperts,
themwiththequerytermsprovidedbyauser. therewasstilla20%chancethatanewpersoncoinedapreviously
Thefull-textsearchispoweredbyaterm-basedinvertedindex: unseenterm.Tocopewiththisproblem,Furnasetal.[24]proposed
aclassicdatastructurethatlinksdocumentterms—andsometimes asystemofunlimitedtermaliases,wherepotentialsynonymswould
phrases—with their locations in a text collection. This way of beinteractivelyexploredandpresentedtotheuserinadialogmode.
organizingtextdatatracesbacktopaperbookindicescontaining Anestablishedautomatictechniqueaimingtoreducethevocabu-
alphabeticallistsofprincipalwords.Inparticular,itwasusedina larygapisaqueryexpansion.Itconsistsinexpanding(andsome-
13thcenturyBibleconcordance,longbeforethecomputerera[22]. timesrewriting)asourcequeryusingrelatedtermsand/orphrases.
Forefficiencyreasons,traditionalqueryexpansiontechniquesare
1https://github.com/oaqa/knn4qa limitedtodozensofexpansionterms[12].Usinghundredsorthou-
sandsofexpansiontermsseemstobeinfeasiblewithinaframework
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor oftheterm-basedinvertedindex.Incontrast,wedemonstratethata
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed systemofunlimitedtermaliasescanbesuccessfullyimplemented
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
withinamoregenericframeworkofak-nearestneighborsearch
onthefirstpage. Copyrightsforcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or (k-NNsearch).
republish,topostonserversortoredistributetolists,requirespriorspecificpermission Ithasbeenlongrecognizedthatthek-NNsearchshowsapromise
and/orafee.Requestpermissionsfrompermissions@acm.org.
tomakeretrievalaconceptuallysimpleoptimizationprocedure[32].
CIKM’16,October24-November28,2016,Indianapolis,IN,USA
Thisapproachmaypermitaseparationoflaborbetweendatasci-
©2016Copyrightheldbytheowner/author(s). PublicationrightslicensedtoACM.
entists, focusing on methods’ accuracy, and software engineers,
ISBN978-1-4503-4073-1/16/10...$15.00
focusingondevelopmentofmoreefficientand/orscalablesearch
DOI:http://dx.doi.org/10.1145/2983323.2983815
6102
tcO
13
]RI.sc[
1v10001.0161:viXra
approaches.However,thek-NNsearchprovedtobeachallenging largescaleautomaticevaluationwithsizeabletrainingandtesting
problemduetothecurseofdimensionality.Thereisempiricaland subsets.
theoreticalevidencethatthisproblemcannotbesolvedbothexactly Specifically,weextractQApairsfromthefollowingcollections:
andefficientlyinahigh-dimensionalsetting[71,5,13,52]. For
• L6-Yahoo!AnswersComprehensiveversion1.0(about4.4M
somedatasets,e.g.,inthecaseofvectorswithrandomlygenerated
questions);
elements,exactmethodsdegeneratetoabruteforcesearchforjust
adozenofdimensions[71,5]. Somedatasetsonly“look”high-
• L5-Yahoo!AnswersMannerversion2.0(about142Kques-
dimensional,butpossesspropertiesoflow-dimensionaldatasets,
tions),whichisasubsetofL6createdbySurdeanual.[63];
i.e.,theyhavealowintrinsicdimensionality[33,5,13]. Unfortu-
nately,textualdataseemstobeintrinsicallyhigh-dimensional.For • StackOverflow(about8.8Mansweredquestions).
example,usingthedefinitionofChávezetal.[13],weestimatethat
theintrinsicdimensionalityofWikipediaTF×IDFvectorsisabout Yahoo!AnswerscollectionsareavailablethroughYahoo!WebScope
2500inthecaseofthemetricangulardistance. andcanberequestedbyresearchersfromaccrediteduniversities.3
ThecurseofdimensionalitycanbepartiallyliftedbyusingLo- Foreachquestion,thereisalwaysananswer(andthebestansweris
calitySensitiveHashing(LSH)techniques[8,27,35]. Thereare alwayspresent).TheStackOverflowcollectionisfreelyavailable
numerousmodificationsofLSH,whichdifferprimarilyinhowthey fordownload.4 Whilethereare8.8Mansweredquestions,thebest
constructfamiliesoflocality-sensitivefunctions[69].Mostofthe answer is not always selected by an asker. Such questions are
researchfocusesonhashfunctionsforwell-studiedsimilarities,such discardedleavinguswith6.2Mquestions.
astheEuclideandistanceandthecosinesimilarity. Eachquestionhasa(relatively)shortsummaryofcontent,which
Inthispaper,however,weexploreaneffectivesimilarityfunction isusuallyaccompaniedbyalongerdescription.Thequestionsum-
BM25+Model1,whichisneithermetricnorsymmetric(see§2.1.3). maryconcatenatedwiththedescriptionisusedasaquerywiththe
Wedemonstratethatitispossibletocarryoutanefficientandeffec- objective of retrieving the corresponding best answer. The best
tivek-NNsearch(forBM25+Model1)usingpivotingtechniques. answerisconsideredtobetheonlyrelevantdocumentforthequery.
Inthat,theapproximatek-NNsearchisnearlytwoordersofmagni- Theaccuracyofaretrievalsystemismeasuredusingstandard
tudefasterthantherespectiveexactbruteforcesearch.Thek-NN IRmetrics: aMeanReciprocalRank(MRR),aprecisionatrank
searchcanbe1.5×fasterthanLucene,whilebeingmoreeffective one (P@1) and an answer recall measured for the set of top-N
duetobridgingthevocabularygap. rankeddocuments.P@1—ourmainevaluationmetric—isequalto
Toeasereproducibility,wemakeoursoftware(includingdata- afractionofquerieswherethehighestrankeddocumentisatrue
generatingcode)andderivativedatabasedontheStackOverflow bestanswertothequestion.
collectionavailableonline.2 Weprocesscollectionsbyremovingpunctuation,extractingto-
kensandtermlemmasusingStanfordCoreNLP[42](instead,one
canuseanyreasonablyaccuratetokenizerandlemmatizer). All
2. APPROACH
termsandlemmasarelowercased;stopwordsareremoved. Note
Wefocusonataskofsearchingalargecollectionofanswers thatwekeepbothlemmasandoriginalterms.InStackOverflowwe
extractedfromacommunityQAwebsite.Thequestionsandanswers removeallthecode(thecontentmarkedbythetagcode).
are submitted by real people, who also select best answers. A Eachcollectionisrandomlysplitintoseveralsubsets,whichin-
question and the respective best answer represent one QA pair. cludetraining,twodevelopment(dev1anddev2),andtestingsubsets.
WhilecommunityQAisanimportanttaskonitsown,itisused InthecaseofComprehensiveandStackOverflow,thereisanaddi-
hereprimarilyasatestbedtodemonstratethepotentialofthek-NN tionalsubsetthatisusedtolearnIBMModel1.Theanswersfrom
searchasasubstituteforterm-basedretrieval.Duetothecurseof thissubsetareindexed,butthequestionsarediscaredafterlearning
dimensionality,wehavetoresorttoapproximatesearching. Note Model1(i.e,theyarenotusedfortrainingandtesting).Inthecase
thatweneedasimilarityfunctionthatoutstripsthebaselinemethod ofManner,IBMModel1istrainedonasubsetofComprehensive
BM25byagoodmargin.Otherwise,gainsachievedbyemployinga fromwhichweexcludeQApairsthatbelongtoManner.Thesplit
moresophisticatedsimilaritywouldbeinvalidatedbytheinaccuracy ofMannermimicsthesetupSurdeanuetal.[63]andthetestset
ofthesearchprocedure. contains29Kqueries.CollectionstatisticsissummarizedinTable1.
Oneeffectivewaytobuildsuchasimilarityfunctionistolearna
generativequestion-answertranslationmodel,e.g.,IBMModel1
Collection Termsin Termsin
[10]. However, “...thegoalofquestion-answertranslationisto name QApairs question answer
learnassociationsbetweenquestiontermsandsynonymousanswer total train dev1/dev2 test tran
terms,ratherthanthetranslationofquestionsintofluentanswers.”
Manner 142K 86K 7K/21K 29K 4.2M 13.9 40.6
[57]Theideaofusingatranslationmodelinretrievalapplications Comprehensive 4.4M 212K 11K/42K 10K 4.1M 17.8 34.1
wasproposedbyBergeretal.[4].ItisnowwidelyadoptedbytheIR StackOverflow 6.2M 298K 15K/58K 10K 5.8M 48.4 33.1
andQAcommunities[18,62,57,74,63,23].Linearlycombining
BM25andlogarithmsofIBMModel1scoresproducesasimilarity Table1:Collectionstatistics.Thecolumntrandescribesthesizeof
functionthatisconsiderablymoreaccuratethanBM25alone(by theBM25+Model1trainingcorpus.NotethatforMannerweuse
upto30%onourdata,seeTable3). anexternalcorpustotrainBM25+Model1.
Learning IBM Model 1 requires a large monolingual parallel
corpus. Inthat,thecommunityQAdatasetsseemtobethebest
Wehaveimplementedmultipleretrievalmethodsandfoursimilar-
publiclyavailablesourceofsuchcorpora.Notethatamonolingual
itymodels(i.e.,similarityfunctions).Retrievalmethods,similarity
corpuscanbebuiltfromsearchengineclick-throughlogs[56].Yet,
models,andtheirinteractionsaresummarizedinFigure1. Each
suchdataisnotreadilyavailableforabroadscientificcommunity.
AnotheradvantageofcommunityQAdatasetsisthattheypermita 3https://webscope.sandbox.yahoo.com
4Weuseadumpfromhttps://archive.org/download/stackexchange
2https://github.com/oaqa/knn4qa datedMarch10th2016.
Retrieval modules highthroughputin-memorydatabasesystems[31],weloadforward
k-NN indicesintomemory.Theoverallre-rankingtimeisnegligiblysmall.
(NMSLIB) rO ep -rt aio nn ka el r Notethatthedepthofacandidatepoolrepresentsareasonable
BM25
oC ro is gi in ne a E l wm ob re dd s BS rW ut- eg fr oa rp ch e candk id= a5 t0 e0
s
Term Lists lemmas e thffi ec pi oe on lcy im-e pf rf oec vt ei sve thn ees as nstr wad ere- ro ef cf a. ll,W ith ai ll se oin mc ar kea es si in tg hath rde ed re topt rh ano kf
results accurately. Beyond a certain point, increasing the depth
question Cos li en me mTF a× sIDF Te ( Ir L nm iu v nec d- e rb etn e xa e ds )e d N c= a1 n0 di0 dates
foI rn w-m are dm ino dry
e x a n s w e N rs hig ah ne ss wt-s ec rore l d 2e i .a s 1pd rs oo pn o Sl ry iti moto n ia la t aem l ry ia l tr ag yri gn Meal co oim m dp p er u lo t sv ate im one an lt ei fn foP rt@ . 1attheexpenseof
BM25
lemmas k-NN 2.1.1 CosineTF×IDFandBM25
(NMSLIB) k=N
answers CosineTF×IDF andBM25arecomputedforlemmatizedtext.
BM25+Model 1 Brute force
lemmas NAPP k-N RN @ re kcall answer recall P@1,MRR CosineTF×IDFistheclassicmodelwherethesimilarityscoreis
equaltothecosinesimilaritybetweenTF×IDFvectors[59,41].An
elementiofsuchavectorisequaltotheproductoftheunnormalized
termfrequencyTF andtheinversedocumentfrequency(IDF).To
i
Figure1: Retrievalpipelinearchitecture. Weillustratetheuseof
computeIDF,weusetheformulaimplementedinLucene:
evaluationmetrics(insideovalsatthebottom)bydottedlines,which
connectovalswithcomponentsforwhichmetricsareapplied. ln(1+(D−d+0.5)/(d+0.5)), (1)
whereDisthenumberofdocumentsanddisthenumberofdocu-
retrievalmethodreturnsarankedlistofanswers,whichmaybeop- mentscontainingthetermi.
tionallyre-ranked.TheoutputisalistofN scoredanswers.There BM25scores[58]arecomputedasthesumoftermIDFs(Eq.1)
aretwoclassesofretrievalmethods:term-basedretrievalsupported multiplied by respective normalized term frequencies. The sum
byApacheLucene5andthek-NNsearchmethodsimplementedin includes only terms appearing in both the query and the answer.
theNon-MetricSpaceLibrary(NMSLIB).NMSLIBisanextendible WealsonormalizeBM25scoresusingthesumofquerytermIDFs.
frameworkforthek-NNsearchingenericspaces[7].6 Similarity Normalizedfrequenciesareasfollows:
modelsinclude:
TF i·(k 1+1)
(2)
• TF×IDF models: the cosine similarity between TF×IDF TF i+k 1·(cid:0) 1−b+b·|D|·|D|− avg1(cid:1) ,
vectors(shortlyCosineTF×IDF)andBM25(§2.1.1);
wherek 1andbareparameters(k 1 = 1.2andb = 0.75);|D|isa
• Thecosinesimilaritybetweenaveragedwordembeddings, documentlengthinwords;|D| avgistheaveragedocumentlength.
henceforth,CosineEmbed(§2.1.4); Lucene’simplementationofBM25usesalossycompressionforthe
documentlength,whichresultsinreducedeffectiveness.
• ThelinearcombinationofBM25andIBMModel1scores,
henceforth,BM25+Model1(§2.1.3). 2.1.2 IBMModel1
Computing translation probabilities via IBM Model 1 [10] is
InthecaseofLucene,weindexlemmatizedtermsanduseBM25
onecommonwaytoquantifythestrengthofassociationsamong
as a similarity model [58]. We have found that Lucene’s imple-
questionandanswerterms.ThetransformedIBMModel1scores
mentation of BM25 is imperfect (see § 2.1.1 for details), which
areusedasinputtoalearning-to-rankalgorithm.Specifically,we
leadstoatleasta10%lossinP@1forbothComprehensiveand
takethelogarithmofthetranslationprobabilityanddivideitbythe
StackOverflow. Tocompensateforthisdrawback,weobtain100
numberofqueryterms.
top-scoreddocumentsusingLuceneandre-rankthemusingourown
LetT(q|a)denoteaprobabilitythataquestiontermqisatrans-
implementationofBM25.
lationofananswerterma.Then,aprobabilitythataquestionQis
InthecaseofNMSLIB,weusetwoindexingmethodsandthe
atranslationofananswerAisequalto:
bruteforcesearch. Theindexingmethodsare: theNeighborhood
APProximationindex(NAPP)[64]andtheSmall-Worldgraph(SW- P(Q|A)= (cid:81) P(q|A)
graph)[39]. Theyarediscussedin§2.2. TheSW-graphisused q∈Q
(cid:20) (cid:21) (3)
onlywiththeCosineEmbed;NAPPisappliedtobothBM25and
P(q|A)=(1−λ)
(cid:80)
T(q|a)P(a|A) +λP(q|C)
BM25+Model1.WedonotcreateanindexfortheCosineTF×IDF, a∈A
butusethebruteforcesearchinstead.Thebruteforcesearchisslow,
T(q|a)isatranslationprobabilitylearnedbytheGIZA++toolkit
butitisapplicabletoanysimilaritymodel.
[50]viatheEMalgorithm;P(a|A)isaprobabilitythatatermais
BecausetheCosineTF×IDFandCosineEmbedarenotveryac-
generatedbytheanswerA;P(q|C)isaprobabilitythatatermqis
curate,theoutputfromthesemodelsmaybefurtherre-rankedusing
generatedbytheentirecollectionC;λisasmoothingparameter.
BM25. Tothisend,wefirstretrieve500candidaterecords. Next,
P(a|A)andP(q|C)arecomputedusingthemaximumlikelihood
wediscardallbutNrecordswithhighestBM25scores.Tocompare
estimator.Foranout-of-vocabularytermq,P(q|C)issettoasmall
effectiveness of Cosine TF×IDF and BM25, we also evaluate a
number(10−9).SimilartoBM25andCosineTF×IDF,computation
variantwheretheoutputofCosineTF×IDFisnotre-ranked.
isbasedonthelemmatizedtext.
Tore-rankefficiently,weuseaforwardindex.Givenadocument
AstraightforwardbutslowapproachtocomputeIBMModel1
identifier,thisindexallowsustoquicklyretrievethelistofterms
scoresinvolvesstoringT(q|a)intheformofasparsehashtable.
and their in-document frequencies. Following a recent trend in
Then,computationofEq.3entailsonehashtablelookupforeach
5http://lucene.apache.org combinationofquestionandanswerterms. Wecandobetterby
6https://github.com/searchivarius/nmslib creatinganinvertedindexforeachquery,whichpermitsretrieving
Priorart[63] publishedresult[63]. WemimicthesetupofSurdeanuetal. [63]
anduseonlyquestionsforwhicharelevantanswerisfound(but
N =15 N =25 N =50 N =100
notnecessarilyrankednumberone). Wealsosplitthecollection
Recall 0.290 0.328 0.381 0.434 inthesameproportionsasSurdeanuetal.[63].9 Furthermore,we
P@1 0.499 0.445 0.385 0.337
measureP@1atvariousrecalllevelsbyvaryingtheresultsizesN
MRR 0.642 0.582 0.512 0.453
(whicharedifferentfromthoseusedbySurdeanuetal[63]).
Thispaper
AccordingtoTable2,ourmethodsurpassesthepreviouslypub-
N =10 N =17 N =36 N =72 lishedresultby14–26%inP@1,andby10–11%inMRRdespite
Recall 0.293 0.331 0.386 0.438 usingonlytwofeatures.Thismaybeexplainedbytwofactors.First,
P@1 0.571(+14%) 0.511(+15%) 0.442(+15%) 0.392(+16%) we use a 50× larger corpus to train IBM Model 1. Second, the
MRR 0.708(+10%) 0.645(+11%) 0.570(+11%) 0.510(+13%) retrievalmoduleTerrierBM25(employedbySurdeanuetal.[63])
seemstohaveinferiorretrievalperformancecomparedtoLucene.
Table2:ComparisonofBM25+Model1againstpriorartonMan- Inparticular,LuceneachievesahigherrecallusingasmallerN (see
ner.AccuracyiscomputedforseveralresultsetsizesN usingthe Table2).
methodologyofSurdeanuetal.[63].Eachcolumncorrespondstoa
2.1.4 CosineEmbed
differentsubsetofqueries.
Wordembeddings, alsoknownasdistributedwordrepresenta-
tions,arereal-valuedvectorsassociatedwithwords.Wordembed-
query-specificentriesT(q|a)usingtheidentifierofanswerterma dingsareusuallyconstructedinanunsupervisedmannerfromlarge
asakey.Thus,weneedonlyonelookupperanswerterm.Identifiers unstructuredcorporaviaartificialneuralnetworks. [14,44]. Be-
areindexedusinganefficienthashtable(theclassdense_hash_map causeembeddingscancapturesyntacticandsemanticregularities
fromthepackagesparsehash)7.Buildingsuchaninvertedindexis inlanguage[66,45],embedding-basedsimilaritycanbeusefulin
computationallyexpensive(about15msforeachComprehensive retrievalandre-rankingtasks[23,75].Thehopehereisthatcom-
and90msforeachStackOverflowquery).Yet,thecostisamortized paringembeddingsinsteadoforiginalwordswouldhelptobridge
overmultiplecomparisonsbetweenthequeryanddatapoints. thevocabularygap.
WetakeseveralmeasurestomaximizetheeffectivenessofIBM Onepopularembedding-basedsimilaritymeasureistheaverage
Model 1. First, we compute translation probabilities on a sym- cosine similarity computed for all pairs of question and answer
metrizedcorpusasproposedbyJeonetal.[30].Formally,forevery terms. Theaveragepairwisecosinesimilarityisequaltotheco-
pair of documents (A,Q) in the parallel corpus, we expand the sine similarity between averaged word embeddings of questions
corpusbyaddingentry(Q,A). and answers. In our work we use the cosine similarity between
Second,unlikepreviouswork,whichseemstousecompletetrans- IDF-weightedaveragedembeddings.Here,weuseembeddingsof
lationtables,wediscardalltranslationprobabilitiesT(q|a)below non-lemmatizedterms,becausethisresultsinaslightlyimproved
anempiricallyfoundthresholdof2.5·10−3.Therationaleisthat performance. Weevaluateseveralsetsofpre-trainedembeddings
smallprobabilitiesarelikelytobetheresultofmodeloverfitting. toselectthemosteffectiveones[44,51,73]. Wefurtherimprove
Pruningofthetranslationtableimprovesbothefficiencyandeffec- embeddingsbyretrofitting[21].Inthat,Model1translationtable
tiveness.Italsoreducesmemoryrequirements. T(q|a)(seeEq.3)isusedasarelationallexicon.
Third,followingpriorproposals[30,63],wesetT(w|w),aself-
translationprobability,toanempiricallyfoundpositivevalueand 2.2 Methodsofk-NNSearch
rescaleprobabilitiesT(w(cid:48)|w)sothat(cid:80) w(cid:48)T(w(cid:48)|w)=1.
Weemployak-NNretrievalframeworkNMSLIB,whichpro-
Fourth,wemakeanadhocdecisiontouseasmanyQApairsas vides several implementations of distance based indexing meth-
possibletotrainIBMModel1.Apositiveimpactofthisdecision ods[7]. Theseindexingmethodstreatdatapointsasunstructured
hasbeenconfirmedbyaposthocassessment. objects,togetherwithablack-boxdistancefunction. Inthat,the
Finally,wetuneparametersonadevelopmentset(dev1ordev2). indexingandsearchingprocessexploitonlyvaluesofmutualobject
RatherthanevaluatingindividualperformanceofIBMModel1,we distances.NMSLIBcanbeextendedbyimplementingnewblack-
aimtomaximizeperformanceofthemodelthatlinearlycombines box“distance”functions.Inparticular,weaddanimplementation
BM25andIBMModel1scores. forthesimilarityfunctionsBM25,CosineTF×IDF,CosineEmbed,
andBM25+Model1(seein§2.1).Noneofthesesimilarityfunctions
2.1.3 BM25+Model1
isametricdistance.Inparticular,inthecaseofBM25+Model1the
BM25+Model1isalineartwo-featuremodel,whichincludes “distance”lackssymmetry.
BM25andIBMModel1scores. Optimalfeatureweightsareob- Becauseexactk-NNsearchistooslowtobepractical,weresort
tainedviaacoordinateascentwith10randomrestarts[43]. The toanapproximateprocedure,whichdoesnotnecessarilyfindallk
modelistrainedviaRankLib8usingP@1asatargetoptimization nearestneighbors. Theaccuracyofthek-NNsearchismeasured
metric.Toobtaintrainingdata,weretrieveN =15candidateswith usingarecalldenotedasR@k.R@kisequaltothefractionoftrue
highestBM25scores[58]usingLucene.Ifwedonotretrieveatrue k-nearestneighborsfound.
answer,thequeryisdiscarded.Otherwise,weaddthetrueanswer NMSLIBreadscontentsoftheforwardindex(createdbyasep-
tothetrainingpoolwiththelabelone(whichmeansrelevant),and arateindexingpipeline)intomemoryandbuildsanadditionalin-
theremainingretrievedanswerswiththelabelzero(whichmeans memoryindex. Inthiswork, wecreateindicesusingoneofthe
non-relevant). followingmethod:theNeighborhoodAPProximationindex(NAPP)
To demonstrate that BM25+Model 1 delivers state of the art due to Tellez et al. [64] or the proximity graph method called a
performance,wecompareourresultonManneragainstapreviously Small-Worldgraph(SW-graph)duetoMalkovetal.[39].
7Thecode,originallywrittenbyCraigSilverstein,isnowhostedat 9The exact split used by Surdeanu et al. [63] is not known. To
https://github.com/sparsehash/sparsehash ensurethatthedifferencesaresubstantialandsignificant,wealso
8https://sourceforge.net/p/lemur/wiki/RankLib/ compute99.9%confidenceintervals.
0.5 0.5
BFbm25+model1 BFbm25+model1
NAPPbm25+model1 NAPPbm25+model1
0.4 BFbm25 0.4 BFbm25
Lucene BFcosine embed
NAPPbm25 BFcosine tf-idf +rerank
0.3 BFcosine tf-idf +rerank 0.3 Lucene
BFcosine embed NAPPbm25
SW-graphcosine embed SW-graphcosine embed
0.2 BFcosine tf-idf -rerank 0.2 BFcosine tf-idf -rerank
0.1 0.1
0 0
1 10 100 1 10 100
ResultsetsizeN (log. scale) ResultsetsizeN (log. scale)
(a)StackOverflow (b)Comprehensive
Figure2: TheanswerrecallatdifferentN.(BFstandsforbrute-force;+rerankand-rerankindicateifanoptionalre-rankerisused).
NAPPisapivotingmethodthatarrangespointsbasedontheir SW-graphworkswellfordensevectorialdata(i.e.,embeddings),
distancestopivots. Thisisafilteringmethod: Candidatepoints whereitoutstripsNAPPbyanorderofmagnitude.SW-graphwas
sharenumPivotSearchclosestpivotswiththequerypoint.The foundtobemuchfaster[49]thanthemulti-probeLSHduetoDong
search algorithm employs an inverted index. Unlike term-based etal.[17].InapublicevaluationinMay2016,10SW-graphoutper-
indices,however,foreachpivottheindexkeepsreferencestoclose formedtwoefficientpopularlibraries:FLANN[48]andAnnoy11.
datapoints.Morespecifically,thepivotshouldbeoneofthepoint’s SW-graphwasalsomostlyfasterthananovelLSHalgorithm[1].
numPivotIndexclosestpivots.Answeringaqueryrequireseffi- Incontrast,NAPPsubstantiallyoutperformsSW-graphforsparse
cientmergingofpostinglists.Mergingofpostinglistsrepresentsa TF×IDFdata,i.e.,formodelsBM25andBM25+Model1.
substantialoverhead.
Tellezetal.[64]usepivotsrandomlysampledfromthedataset,
butwefindthatforsparsedatasuchasTF×IDFvectorssubstantially 3. MAINEXPERIMENTS
shorterretrievaltimes—sometimesbyordersofmagnitude—canbe ExperimentsarecarriedoutonAmazonEC2instancer3.4xlarge,
obtainedbyusingaspecialpivotgenerationalgorithm.Specifically, which has 16 virtual cores and 122 GB of memory. The main
pivots are generated as pseudo-documents containing K entries retrievalpipeline,whichisimplementedinJava(1.8.0_11),uses16
sampledfromthesetofM mostfrequentwords(inoursetupK = searchthreads.WeuseamodifiedversionofNMSLIB1.5,12which
1000andM =50000). Amoredetaileddescriptionandanalysis operatesasaserverprocessingqueriesviaTCP/IP.NMSLIBandits
ofthisapproachwillbepresentedelsewhere. extensionsarewritteninC++andcompiledusingGCC4.8.4with
Duringindexing,wehavetocomputethedistancesbetweena optimizationflags-O3and-march=native.Luceneversionis
datapointandeverypivot. Becausetherearethousandsofpivots, 4.10.3.Theretrievalarchitecture(see§2)isoutlinedinFigure1.
thisoperationisquiteexpensive,especiallyforBM25+Model1.To Thecollectionprocessing/indexingsystemisimplementedinJava.
optimizecomputationofEq.3,weorganizeallpivot-specificT(q|a) ItemploystheframeworkApacheUIMAandUIMAcomponents
entriesintheformoftheinvertedindex. fromDKProCore[19].13 Translationprobabilitiesarecomputed
Aproximitygraphisadatastructure,wheredatapointsarenodes. usingGIZA++toolkit[50]viatheEMalgorithm(fiveiterations).14
Sufficientlyclosenodes,i.e.,neighbors,areconnectedbyedges. Foreachapproximatek-NNpipeline,weexecuteseveralruns
Searchingstartsfromsome,e.g.,random,point/nodeandtraverses withdifferentparameters. InthecaseofSW-graph,wevarythe
thegraphuntilitstopsdiscoveringnewpointssufficientlyclose parameterefSearch. InthecaseofNAPP,wevarythenumber
tothequeryoraftervisitingagivennumberofnodes. [2,60,26, ofindexedpivots(parameternumPivotIndex)andthenumber
25, 16, 70]. Specifically, theSW-graphalgorithm(implemented ofpivotsthatshouldbesharedbetweenthequeryandananswer
inNMSLIB)keepsalistofefSearchpointssortedintheorder (numPivotSearch).Optimalparametershavebeenfoundona
ofincreasingdistancefromthequeryaswellasacandidatequeue. dev1set(usingasubsetof5Kqueries).
Traversalproceedsinthebest-firstmanner,byexploringtheneigh- Retrievaltimesaremeasuredbyaspecialclientapplicationthat
borhoodofthecandidatethatisclosesttothequery.Ifacandidate submitssearchrequeststoeitherLuceneorNMSLIB.Inthecase
neighborisclosertothequerythantheefSearch-thclosestpoint
seensofar,itisaddedtothecandidatequeue.Otherwise,theneigh- 10https://github.com/erikbern/ann-benchmarks
borisdiscarded. Thetraversalstopswhenthecandidatequeueis 11https://github.com/spotify/annoy
exhausted. To improve recall, the algorithm may restart several 12https://github.com/searchivarius/nmslib/tree/nmslib4a_cikm2016
times. Forourdata, however, itismoreefficienttostartfroma
13https://dkpro.github.io/dkpro-core/
singlepointandsearchusingalarge-enoughvalueofefSearch.
14https://github.com/moses-smt/giza-pp
llacerrewsnA llacerrewsnA
StackOverflow Comprehensive
Query Speed-up P@1 Answer Answerrecall Query Speed-up P@1 Answer Answerrecall
P@1 P@1
time (overBF) loss recall loss time (overBF) loss recall loss
BFBM25+Model1
20.5s 0.081* 0.300* 8s 0.077* 0.278*
NAPPBM25+Model1
0.84s 24 0.080* 1.1% 0.290* 3.3% 0.70s 11 0.075* 2.6% 0.269* 3.1%
0.65s 32 0.079* 2.2% 0.283* 5.7% 0.30s 27 0.074* 4.7% 0.261* 6.1%
0.50s 35 0.078* 3.1% 0.276* 8.2% 0.21s 38 0.073 5.7% 0.256* 8.1%
0.47s 44 0.076* 5.8% 0.263* 12.4% 0.18s 45 0.070 9.7% 0.234 16.0%
0.40s 52 0.074* 8.2% 0.252* 16.1% 0.09s 89 0.068 12.4% 0.227* 18.5%
BFBM25
3.5s 0.062 0.239 1.7s 0.067 0.239
NAPPBM25
0.23s 15 0.060 2.1% 0.228* 4.7% 0.37s 5 0.063* 5.1% 0.227* 5.0%
0.14s 25 0.059* 5.0% 0.221* 7.6% 0.18s 9 0.062* 6.6% 0.222* 7.1%
0.07s 50 0.057* 7.6% 0.208* 12.9% 0.15s 11 0.061* 8.4% 0.217* 9.2%
0.06s 56 0.055* 11.2% 0.195* 18.5% 0.15s 11 0.060* 9.3% 0.212* 11.2%
LuceneBM25
0.62s 0.062 0.229* 0.08s 0.067 0.233*
BFCosineEmbed
3.9s 0.041* 0.108* 2.7s 0.055* 0.174*
SW-graphCosineEmbed
0.78s 5 0.041* -0.2% 0.107* 0.6% 0.19s 14 0.054* 1.6% 0.172* 1.0%
0.40s 10 0.041* 0.5% 0.107* 0.8% 0.09s 31 0.054* 3.1% 0.170* 1.9%
0.34s 11 0.041* 0.7% 0.106* 1.3% 0.07s 37 0.053* 3.6% 0.170* 2.4%
0.13s 29 0.039* 4.9% 0.102* 5.8% 0.03s 104 0.050* 10.3% 0.160* 7.8%
Table3:Efficiency-effectivenesstrade-offsofretrievalmodulesforN =100(bruteforceCosineTF×IDFrunsareomitted).Statistically
significantdifferences(atlevel0.01)fromBFBM25aremarkedwith*.P-valuesareadjustedformultipletestingviatheBonferronicorrection.
ofLucene,we“warmup”theindexbyexecutingthewholesetof TF×IDFvectorswithoutre-rankerissometimesmoreeffectivethan
queriestwice.Run-timesaremeasuredonlyforthethirdrun. thecosinesimilaritybetweenwordembeddingswhoseperformance
Effectivenessofretrievalrunsismeasuredusinganexternalap- isboostedbythere-ranker(seePanel2ainFigure2). Thisisa
plication,namely,trec_eval9.0.4.15 Themainexperimentalresults discouragingfindinggiventhatembedding-basedretrievalcanbe
arepresentedinFigure2andTable3. ForTable3wecompute quiteefficient(seeTable3).Itremainstobeverifiedifbetterresults
statisticalsignificanceofresultsusingthet-testwithasubsequent canbeobtainedwithdocumentembeddingsthatcomputevectorial
Bonferroni adjustment for multiple testing. This adjustment for representationsofcompletesentencesorevendocuments[28,36].
multipletestingconsistsinmultiplyingp-valuesbythetotalnumber NotethatallBM25-basedrunshavesimilarperformance.How-
ofrunsforN =100(tosavespace,someoftherunsarenotshown ever, BM25+Model1hasarecallthatis16%higherinthecase
inthetable).Thesignificancelevelis0.01. ofComprehensiveand26%higherinthecaseofStackOverflow.
In Figure 2 we plot the answer recall (measured for a set of ForBM25,itispossibletomatchtherecallofBM25+Model1by
top-N rankeddocuments)fornineimplementedretrievalmodules. increasingN.However,thismayincreasealoadonadownstream
Notethatapproximatek-NNmethodsarerepresentedbytheirmost re-ranking module. For example, in the case of Stack Overflow,
accurateruns.Exactbruteforcek-NNrunsareplottedusingthicker BM25+Model1hasanearly0.2answerrecallforN =10(Panel
lines (with star marks) of the same style/color as corresponding 2ainFigure2). ToobtainthesamerecalllevelusingLucene,we
approximateruns.TheirmnemonicnamesstartwiththewordBF needtouseN >20.
(shortforbruteforce). Next, wecompareefficiencyofk-NNsearchmethodsagainst
Thecosine-similaritymodelsaretheleasteffective. Therecall thatofLucene.NotethatLuceneisastrongbaseline,whichfares
ofthebruteforcerunBFCosineTF×IDF-rerankislessthanhalf wellagainstoptimizedC++code,especiallyfordisjunctivequeries
of that for the brute force run BF BM25. We can nearly match [68].Lucene’saverageretrievaltimesareequalto80msforCom-
the performance of BM25 by adding a BM25-based optional re- prehensiveand620msforStackOverflow(seeTable3).Thereare
ranker (the run BF Cosine TF×IDF +rerank). In contrast, the atleasttwofactorsthatcontributetothedifferenceinretrievaltimes
cosine-similaritybetweenaveragedwordembeddings(e.g.,therun betweentwocollections:(1)questionsinStackOverflowhave2.7×
BF Cosine Embed) is much worse than BM25 despite using the asmanyterms,(2)StackOverflowhas1.4×asmanyanswers(see
re-ranker! Somewhatsurprisingly,thecosinesimilaritybetween Table1).
SW-graph is quite fast for both collections. For example, for
StackOverflow,itcananswerqueriesin340msattheexpenseof
15https://github.com/usnistgov/trec_eval
Comprehensive Second,therearedifferencesinfilteringeffectivenessofthemeth-
ods.Todemonstratethis,weevaluatethereductioninthenumberof
BM25+Model1 BM25
distancecomputationscomparedtothebruteforcesearch.Forexam-
Reductionin Reductionin
R@1 R@1 ple,ifanalgorithmanswersaquerybycheckingonly10%ofdata
distancecomp. distancecomp.
points,thereductioninthenumberofdistancecomputationsis10.
0.982 8.7 0.982 3.7 Reductionsinthenumberofdistancecomputationsarecompared
0.968 61 0.970 20
fornearlyequalvaluesofthek-NNrecallR@1,whichisequalto
0.961 142 0.963 48
thefractionoftruenearestneighborsfoundbytheretrievalmodule
0.952 246 0.956 98
0.930 434 0.927 226 (R@1shouldnotbeconfusedwiththeanswerrecall).Theresults
ofthiscomparisonarepresentedinTable4. ForNAPP,themore
StackOverflow
pivotsareindexed,thefewerdistancecomputationsarenecessaryto
BM25+Model1 BM25 achieveagivenaccuracylevel.Thus,tomakeafaircomparison,we
Reductionin Reductionin indexequalnumberofpivotsforbothBM25andBM25+Model1.
R@1 R@1
distancecomp. distancecomp. AccordingtoTable4,inthecaseofComprehensive,ittakes2-3×
0.982 13.4 0.980 157 fewerdistancecomputationsforthemodelBM25+Model1thanfor
0.970 39 0.972 208 BM25.Incontrast,inthecaseofStackOverflow,answeringqueries
0.964 64 0.964 260 forthemodelBM25takessignificantlyfewerdistancecomputations
0.957 97 0.959 287 thanforBM25+Model1.Furthermore,thereductioninthenumber
0.948 137 0.955 315
ofdistancecomputationsforBM25onStackOverflowcanbetwo
orders of magnitude higher compared to that of Comprehensive.
Table4:Reductioninthenumberofthedistancecomputationfor
Whatarethepossibleexplanationsforthesestarkdifferences?
twosimilaritymodelsatapproximatelyequallevelsofR@1(larger
Wethinkthatpivotingmethodsareeffectiveonlyifcomparing
reductionisbetter).Using5Kqueriesfromdev1set.
a query and an answer with the same pivot provides a meaning-
ful information regarding their proximity. In the case of a sim-
pleBM25model,thisisonlypossibleifthepivot,thequery,and
the answer have at least one common term. Such an overlap is
losingonly1.3%answerscomparedtothebruteforcesearch(As
muchmorelikelyinthecaseofStackOverflowwherequestions
itisrecentlyreportedbyMalkovandYashunin[40],animproved,
arenearly3×longercomparedtoComprehensive.Incontrast,for
hierarchical,variantofSW-graphisevenmoreaccurateand/oreffi-
themodelBM25+Model1informationregardingproximityofan-
cient).Inotherwords,theapproximatesearchisnearlyasaccurate
swersandqueriesmaybeobtainedifpivots,queries,andanswers
as the exact one. This is why in Figure 2 the best approximate
shareonlyrelatedbutnotnecessarilyidenticalterms.Thus,using
SW-graphrunforthemodelCosineEmbedandtherunBFCosine
BM25+Model1ismoreadvantageousinthecaseofshortqueries
Embedarehardtodistinguish.However,themodelCosineEmbed
(e.g.,inthecaseofComprehensive).
isnotveryeffective. Itdoesnotbridgethevocabularygapandis
To further illustrate importance of using the right function to
evenworsethanCosineTF×IDF.
computedistancetopivots,weevaluatefilteringeffectivnessintwo
InthecaseofBM25,NAPPworkswellforStackOverflow,but
scenarious: (1)whenthedistancetopivotsiscomputedusingan
notforComprehensive.Forexample,inthecaseofStackOverflow,
original distance function and (2) when the distance to pivots is
itanswersqueriesin230mswhilelosingonly2.1%inP@1and
computedusingadifferent,i.e.,proxyfunction.Foreachscenarios,
4.7%intheanswerrecall. Thisisnearly2.7×fasterthanLucene
we use two models: BM25+Model 1 and BM25. In the case of
and15×fasterthanthebruteforcesearchusingBM25.
BM25+Model1,theproxydistanceisBM25.InthecaseofBM25,
ForthemorecomplicatedmodelBM25+Model1,NAPPdelivers
theproxydistanceisCosineTF×IDF.Theresultsarepresentedin
similarspeedupsoverthebruteforcesearchforbothcollections.
Figure3wherethecurvescorrespondingtotheoriginaldistanceare
However,itisalwaysslowerthanLuceneinthecaseofComprehen-
blueandthecurvescorrespondingtotheproxydistancearered.
sive.InthecaseofStackOverflow,NAPPisupto1.5×fasterthan
Panels3aand3cshowuswhathappensifthedistancetopivotsis
Lucene. Forthefastestpostedretrievaltimeof400msitdelivers
computedusingcheapBM25insteadofexpensiveBM25+Model1.
P@1equalto0.074andtheanswerrecallequalto0.252.
Wecanseethatresortingtousingtheproxydistancemakesuscheck
Despitesomedegradationincomparisontothecorresponding
morecandidatedocumentstoachievethesamelevelofrecall. In
exactbruteforcerun,thisrepresentsanimpressive19.3%improve-
otherwordsrelyingontheproxydistancehasanegativeeffecton
mentinP@1and5.4%improvementintheanswerrecallcompared
filteringeffectiveness. Inturn,thiscandrasticallyreduceoverall
tothebruteforceBM25.ThesecondslowestBM25+Model1run
searchefficiency.
obtainedbyNAPPisnearlyasefficientasLucene,butitoutperforms
ThedifferenceisbiggerforPanel3a,whichcorrespondstothe
BM25by27.4%inP@1andby18.4%inrecall.
collectionComprehensive.Alikelyexplanationofthisdifference
AlsonotethatforComprehensive,NAPPBM25+Model1canbe
istheabove-describeddisparityinquerylengthsbetweentwocol-
bothfasterandmoreaccuratethanNAPPBM25.Thisisquitesur-
lections. InthecaseofStackOverflowqueriesarelongandthere
prisinggiventhatBM25+Model1isexpensivetocompute.Specifi-
isabiggeroverlapbetweenqueriesandanswerdocuments.Thisis
cally,thecorrespondingbruteforcerunisnearly5×slowercom-
whythesimilarityfunctionthatreliesonapurelexicalmatch(in
paredtothebruteforcerunofBM25. Therearetworeasonsfor
thiscaseBM25)allowsustofindanswersrathereffectively.Inthe
whyNAPPBM25+Model1canbemorefasterandaccuratethan
caseofComprehensivealexicaloverlapbetweenqueriesandan-
NAPPBM25.First,thereisahighoverheadrelatedtomergingpivot
swerdocumentsislesslikely,whichcanbe,nevertheless,remedied
postinglists. Byvaryingmethod’sparameterswecanreducethe
byenhancingBM25modelwithModel1scores. However,when
amountoftimespentoncomputationofthedistance(attheexpense
Model1scoresareexcluded—byusingtheproxydistancefunction
ofsearchaccuracy).Atsomepointthetimespentondistancecom-
tocomputedistancetopivots—thisexclusionhasalargernegative
putationbecomessosmallsothattheoverheadrelatedtoprocessing
effectforComprehensivethanforStackOverflow.
ofpostinglistsstartstodominatetheoveralltime.
800 BM25 400 BM25
BM25+Model1 TF IDFcosine
×
600 300
400 200
200 100
0 0
0.86 0.88 0.9 0.92 0.94 0.96 0.98 0.8 0.85 0.9 0.95 1
R@1 R@1
(a)Comprehensive:BM25+Model1 (b)Comprehensive:BM25
BM25 400 BM25
200
BM25+Model1 TF ×IDFcosine
300
150
200
100
100
50
0 0
0.92 0.94 0.96 0.98 1 0.86 0.88 0.9 0.92 0.94 0.96 0.98
R@1 R@1
(c)StackOverflow:BM25+Model1 (d)StackOverflow:BM25
Figure3:FilteringeffectivenessofNAPPfororiginalandaproxydistancefunction(whencomputingdistancestopivots).Thecurvesforthe
originaldistancearebluewhileproxydistancecurvesarered.Filteringeffectivenessismeasuredusingviareductioninthenumberofdistance
computations(largerisbetter).TheleftcolumnhasdataforthedistanceBM25+Model1andtherightcolumnhasdataforBM25.Using5K
queriesfromdev1set.
Panels3band3dshowuswhathappensifthedistancetopivots thefollowingtwoapproachesaretypicallyused.Thefirstapproach
iscomputedusingCosineTF×IDFinsteadofBM25.Inthiscase, reliesonaterm-basedinvertedindexinretrievingdocumentsthat
suchareplacementleadstoamuchlargerperformancedeterioration sharecommontermswiththequery.Thesedocumentsarefurtherre-
than removal of Model 1 scores. This is not surprising: As we rankedusingsomesimilarityfunction.Dynamicandstaticpruning
canseefromFigure2,thereisamuchbiggergapineffectiveness can be used to improve efficiency, sometimes at the expense of
betweenBM25andCosineTF×IDFthanbetweenBM25+Model1 decreased recall [67, 11, 15]. This approach supports arbitrary
andBM25. Thus,replacingBM25withCosineTF×IDFhasalso similarityfunctions,butitsuffersfromtheproblemofthevocabulary
alargernegativeeffectiveonfilteringeffectiveness(thanreplacing mismatch[4,63,23].
BM25+Model1withBM25). Thesecondapproachinvolvescarryingoutthek-NNsearchvia
LSH[53,65,46,37].Itismostappropriateforthecosinesimilarity.
4. DISCUSSIONANDRELATEDWORK Forexample,Lietal.[37]proposethefollowingtwo-stagescheme
tothetaskoffindingthematicallysimilardocuments. Inthefirst
Thek-NNsearchisanextensivelystudiedarea. Foradetailed
steptheyretrievecandidatesusingLSH.Next,thesecandidateare
discussionthereaderisaddressedtothesurveysofmetric[13]and
re-rankedusingtheHammingdistancebetweenquantizedTF×IDF
non-metric [61] access methods, as well to the recent survey of
vectors. Lietal.[37]findthattheirapproachisupto30×faster
hashingtechniques[69].16
thantheclassicterm-basedindexwhilesometimesbeingequally
Thek-NNsearchisapopulartechniqueinIRandNLP,where
accurate.
16Inadditiontothek-NNsearch,hashingtechniquesareoftenused Petrovic´etal.[53]appliedahybridofLSHandtheterm-based
fornear-duplicatedetection[41],whichconsistsinfindingobjects indextothetaskofthestreamingFirstStoryDetection(FSD).The
withahighdegreeofsimilarity.Itisarelatedbutdistinctproblem, LSHkeepsalargechunkofsufficientlyrecentdocuments,whilethe
whichisoftensolvedbyapplyinghigh-precisionlow-recalltech- term-basedindexkeepsasmallsubsetofrecentlyaddeddocuments.
niques[41].However,thesetechniquesarenotapplicabletobroader
Theyreporttheirsystemtobesubstantiallyfasterthanthestate-of-
searchtaskssuchasfindinganswersrelevanttoagivenquestion.
snoitatupmoc
.tsidfo#nitnemevorpmi
snoitatupmoc
.tsidfo#nitnemevorpmi
snoitatupmoc
.tsidfo#nitnemevorpmi
snoitatupmoc
.tsidfo#nitnemevorpmi
the-artsystem—whichreliesontheclassicterm-basedindex—while Finally, wewanttohighlighttherelationshipofourapproach
beingsimilarlyeffective.Inafollowupwork,Petrovic´etal.[54] toindexingautomaticallylearnedfeaturesforQA[76]. Yaoetal.
incorporate term associations into the similarity function. Their proposetoautomaticallylearnassociationsbetweenaquestiontype
solutionreliesonanapproximationforthekernelizedcosinesimi- andvariouslinguisticannotationssuchasnamedentitiesandPOS
larity.Theassociationsareobtainedfromanexternalparaphrasing tags[76]. Forexample, foraquestion“Whoisthepresidentof
database. Moranetal.[46]usethesamemethodasPetrovic´etal. theUnitedStates”ananswersentencecontainsapersonname(a
[54],butfindsynonymsviathek-NNsearchinthespaceofword namedentity).Givenatrainingcorpus,wecanautomaticallylearn
embeddings(whichworksbetterforTwitterdata).Moranetal.[46] associationsandexploitthemtoguidetheretrievalprocess.Techni-
aswellasPetrovic´etal.[54]calculateperformanceusinganaggre- cally,thisrequiresindexinglinguisticannotationsandcarryingout
gatedmetricdesignedspecificallyfortheFSDtask.Unfortunately, aqueryexpansionbyaddingexpectedannotationstothequery.
theydonotreportperformancegainsusingstandardIRmetricssuch Forefficiencyreasons,thisworkswellonlyifwecanfindfew
asprecisionandrecall. strong associations for a query. To demonstrate that this is not
Mostimportantly,asshownintheliterature(see[72]andrefer- trueinthecaseofthevocabularygap,wecomputeeffectivenessof
encestherein),similarityfunctionsbasedonthecosinesimilarity BM25+Model1forvaryingsizesofthetranslationtable. Specifi-
arenotespeciallyeffective.Inparticular,comparedtoBM25,our cally,weprunealltheentriesT(q|a)belowathreshold.Inaddition,
implementationoftheTF×IDFcosinesimilarityfinds2×feweran- we estimate the average number of non-zero translation entries
swersforanygivenrankN(seeFigure2).Itispossibletoimprove T(q|a)associatedwithasinglequeryterm. Asareferencepoint
answerrecallbyincreasingN.However,thishaseffectonsearch wealsoincludedataforBM25.WepresentresultsonlyforCompre-
module’performance. Inparticular,ifweretrievetop-N entries hensive,becauseresultsforStackOverflowareanalogous.
usinganapproximatek-NNalgorithm,asN increases,accuracyor
theefficiencyofthesearchdecreases. Simplyspeaking,itiseas- Minimum Numberof
iertocarryoutanaccurate1-NNsearchthananaccurate500-NN translation P@1 associated
search. probability terms
OnenotableexceptionisarecentpaperbyBrokosetal.[9]who, BM25 0.065 N/A
incontrasttoourfindings,learnedthatthecosine-similaritybetween 0.1 0.066(+2.6%) 1700
averaged word embeddings is an effective model for retrieving 0.05 0.070(+8.6%) 3800
Pubmedabstracts.However,theydonotcompareagainststandard 0.025 0.073(+12.5%) 6200
0.005 0.077(+19.3%) 12000
IRbaselinessuchasBM25,whichmakesaninterpretationoftheir
0.0025 0.079(+21.6%) 15000
findingdifficult.
Wearguethatinsteadofrelyingonthecheapcosinesimilarityit
Table5:Averagenumberoftermsassociatedwithaquerytermat
maybebettertoemployanexpensivebutmoreaccuratesimilarity
variousperformancelevelsofBM25+Model1(estimatedondev2).
function.Theexactbruteforcesearchusingthisfunctionwouldbe
ThefirstrowrepresentsaBM25run.
expensive,butthecostcouldbereducedbyapplyinganapproximate
searchmethodforgeneric—i.e.,notnecessarilymetric—spaces.
Acommonapproachtonon-metricspaceindexinginvolvespro- According to Table 5, outperforming BM25 by about 20% re-
jectingdatatoalow-dimensionalEuclideanspace. Thegoalisto quirestokeepmorethan10Kassociationsperqueryterm(onaver-
findamappingwithoutalargedistortionoftheoriginalsimilarity age).Thisnumberissohighbecausefrequentwords,whichtendto
measure.Jacobsetal.[29]reviewprojectionmethodsandarguethat appearinqueriesandtext,areassociatedwithmanylessfrequent
suchacoercionisoftenagainstthenatureofasimilaritymeasure, words(i.e.,respectivetranslationprobabilitiesarenon-zero).
whichcanbe,e.g.,intrinsicallynon-symmetric. Ifwekeeponlytranslationentrieswithhigh(≥0.1)probabilities,
Among other factors, the lack of symmetry prevents us from theimprovementoverBM25ismerely2.6%.Yet,westillhaveto
usingthekernelizedLSH[34,47].TheonlyLSHvariantthatmight keepnearly2Kassociationsperqueryterm!Thisfurthercorrobo-
bedirectlyapplicableinourcaseistheDistance-BasedHashing ratesthefindingofFurnasetal.[24]thataccurateretrievalrequires
(DBH)[3],whichusesrandomlyselectedpivotstoprojectpoints usingalargenumberoftermaliases,whichishardtoimplementus-
toaone-dimensionalspaceviaFastMap[20].Thespaceisfurther ingterm-basedindices.Yet,itispossibletodowithinaframework
binarizedsothatapproximatelyonehalfofdatapointsaremapped ofthek-NNsearch.
toone,andtheotherhalfismappedtozero. Thatsaid,theproposedmethodsarelikelyhavelimitationsas
While a detailed comparison of pivoting approaches to DBH well.Forexample,forbothdatasetsemployedinourexperiments,
isoutthescopeofthepaper,wehypothesizethatperformanceof thequeriesarequitelong.Itisnotyetclearifk-NNcanbeapplied
DBH—likeperformanceofNAPP—dependsonthechoiceofpivots. toshorteradhocqueries,whicharefrequentlysubmittedtoWeb
InthecaseofNAPP,wehavefoundthatcomposingpivotsfrom searchengines.
randomlyselectedtermsallowsustoachievesubstantiallybetter
performance than selecting pivots randomly. Thus, engineering 5. CONCLUSION
pivotstosupporteffectivesearchinginanon-metricspaceseemsto
Inthispaperweattempttoreplacetheclassicterm-basedretrieval
beanimportantresearcharea.Resultsobtainedfromthisareawill
withthek-NNsearch. Tothisend,wetrainalinguisticallymoti-
likelybenefitbothDBHandNAPP.
vatednon-metricandnon-symmetricsimilarityfunction:aweighted
Proximitygraphs(see§2.2)isanotherpromisingclassofdistance-
combinationofBM25scoresandIBMModel1log-scores.Then,
basedmethods,whichareshowntobeusefulinnon-metricspaces
wedemonstratethatitispossibletocarryoutanefficientandeffec-
[49].InthisworkweemploytheSW-graph[39],whichworksquite
tiveapproximatek-NNsearchusingthisfunction.
wellfordensevectorspaces. However,ithasbeenlessusefulfor
Anexactbrute-forcek-NNsearchusingthissimilarityfunction
BM25andBM25+Model1. Wehavenotbeenabletounderstand
isslow. Yet,anapproximatealgorithmcanbenearlytwoorders
whatcausesthelackofperformance,butthisremainsanimportant
of magnitude faster at the expense of only a small loss in accu-
researchquestionaswell.
racy. Aretrievalpipelineusinganapproximatek-NNsearchcan
besometimesbothfasterandmoreaccuratecomparedtotheterm- [6] M.W.Bilotti,P.Ogilvie,J.Callan,andE.Nyberg.Structured
basedLucenepipeline(seeTable3).Thesuccessofourapproach retrievalforquestionanswering.InProceedingsofthe30th
stems from the novel combination of existing methods and new annualinternationalACMSIGIRconferenceonResearchand
algorithmictrickstocomputeIBMModel1efficiently. developmentininformationretrieval,pages351–358.ACM,
Whilethek-NNsearchhasbeenpreviouslyappliedtoIRand 2007.
NLPproblems[53,65,37,54,46,9],thepreviousworkfocuses [7] L.BoytsovandB.Naidan.Engineeringefficientandeffective
largelyonthecosinesimilarityandLSHmethods(see§4fora non-metricspacelibrary.InSimilaritySearchand
discussion). Thisisthefirstsuccessfulattempttoapplyageneric Applications,pages280–293.Springer,2013.
k-NNsearchalgorithmtoasimilarityfunctionaschallengingas [8] A.Z.Broder.Ontheresemblanceandcontainmentof
acombinationofBM25andIBMModel1. Inthat,wefindthat documents.InCompressionandComplexityofSequences
thecosinesimilarityalone(inparticular,thecosinesimilaritybe- 1997.Proceedings,pages21–29.IEEE,1997.
tweenaveragedwordembeddings)lacksalotineffectiveness(see [9] G.-I.Brokos,P.Malakasiotis,andI.Androutsopoulos.Using
Figure2). centroidsofwordembeddingsandwordmoverâA˘Z´sdistance
Thefocusofourstudyisontechniquesthatbridgethevocabulary forbiomedicaldocumentretrievalinquestionanswering.In
gap. Yet, our methods are generic in the sense that they can be ProceedingsoftheBIONLP2016Workshop,2016.
usedtomodelvarioustypesofsemanticandsyntacticmismatch[6,
[10] P.F.Brown,V.J.D.Pietra,S.A.D.Pietra,andR.L.Mercer.
76].Thisopensupnewpossibilitiesfordesigningeffectiveretrieval
Themathematicsofstatisticalmachinetranslation:Parameter
pipelines.
estimation.Computationallinguistics,19(2):263–311,1993.
Oursoftware(includingdata-generatingcode)andderivativedata
[11] D.Carmel,D.Cohen,R.Fagin,E.Farchi,M.Herscovici,Y.S.
basedontheStackOverflowcollectionisavailableonline.17
Maarek,andA.Soffer.Staticindexpruningforinformation
retrievalsystems.InProceedingsofthe24thAnnual
Acknowledgements InternationalACMSIGIRConferenceonResearchand
DevelopmentinInformationRetrieval,SIGIR’01,pages
LeonidBoytsovissupportedbytheOpenAdvancementofQuestion
43–50,NewYork,NY,USA,2001.ACM.
AnsweringSystems(OAQA)group.18;DavidNovakissupported
[12] C.CarpinetoandG.Romano.Asurveyofautomaticquery
by the Czech Research Foundation project P103/12/G084; Yury
expansionininformationretrieval.ACMComput.Surv.,
MalkovissupportedbytheRussianFoundationforBasicResearch
44(1):1:1–1:50,Jan.2012.
(projectNo.16-31-60104mol_a_dk).
[13] E.Chávez,G.Navarro,R.Baeza-Yates,andJ.L.Marroquín.
WealsothankDiWangforhelpingwithaLucenebaseline;Chris
Searchinginmetricspaces.ACMComput.Surv.,
DyerforadiscussionofIBMModel1efficiency;YoavGoldberg,
33(3):273–321,2001.
ManaalFaruqui,ChenyanXiong,Ruey-ChengChenfordiscussions
relatedtowordembeddings;MichaelDenkowskiforadiscussion [14] R.Collobert,J.Weston,L.Bottou,M.Karlen,
onapproximatingalignmentscores. K.Kavukcuoglu,andP.Kuksa.Naturallanguageprocessing
(almost)fromscratch.J.Mach.Learn.Res.,12:2493–2537,
Nov.2011.
6. REFERENCES
[15] S.DingandT.Suel.Fastertop-kdocumentretrievalusing
block-maxindexes.InProceedingsofthe34thInternational
[1] A.Andoni,P.Indyk,T.Laarhoven,I.Razenshteyn,and
ACMSIGIRConferenceonResearchandDevelopmentin
L.Schmidt.Practicalandoptimallshforangulardistance.In
InformationRetrieval,SIGIR’11,pages993–1002,New
C.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,and
York,NY,USA,2011.ACM.
R.Garnett,editors,AdvancesinNeuralInformation
ProcessingSystems28,pages1225–1233.CurranAssociates, [16] W.Dong,C.Moses,andK.Li.Efficientk-nearestneighbor
Inc.,2015. graphconstructionforgenericsimilaritymeasures.In
Proceedingsofthe20thinternationalconferenceonWWW,
[2] S.AryaandD.M.Mount.Approximatenearestneighbor
pages577–586.ACM,2011.
queriesinfixeddimensions.InProceedingsofthefourth
annualACM-SIAMSymposiumonDiscretealgorithms,pages [17] W.Dong,Z.Wang,W.Josephson,M.Charikar,andK.Li.
271–280.SocietyforIndustrialandAppliedMathematics,
Modelinglshforperformancetuning.InProceedingsofthe
17thACMConferenceonInformationandKnowledge
1993.
Management,CIKM’08,pages669–678,NewYork,NY,
[3] V.Athitsos,M.Potamias,P.Papapetrou,andG.Kollios.
USA,2008.ACM.
Nearestneighborretrievalusingdistance-basedhashing.In
2008IEEE24thInternationalConferenceonData [18] A.EchihabiandD.Marcu.Anoisy-channelapproachto
Engineering,pages327–336.IEEE,2008. questionanswering.InProceedingsofthe41stAnnual
MeetingonAssociationforComputationalLinguistics-
[4] A.Berger,R.Caruana,D.Cohn,D.Freitag,andV.Mittal.
Volume1,ACL’03,pages16–23,Stroudsburg,PA,USA,
Bridgingthelexicalchasm:Statisticalapproachesto
2003.ACL.
answer-finding.InProceedingsofthe23rdAnnual
InternationalACMSIGIRConferenceonResearchand [19] R.EckartdeCastilhoandI.Gurevych.Abroad-coverage
DevelopmentinInformationRetrieval,SIGIR’00,pages collectionofportablenlpcomponentsforbuildingshareable
192–199,NewYork,NY,USA,2000.ACM.
analysispipelines.InProceedingsoftheWorkshoponOpen
InfrastructuresandAnalysisFrameworksforHLT,pages
[5] K.Beyer,J.Goldstein,R.Ramakrishnan,andU.Shaft.When
1–11,Dublin,Ireland,August2014.ACLandDublinCity
is“nearestneighbor”meaningful?InDatabase
University.
theory–ICDT’99,pages217–235.Springer,1999.
[20] C.FaloutsosandK.-I.Lin.Fastmap:Afastalgorithmfor
17https://github.com/oaqa/knn4qa indexing,data-miningandvisualizationoftraditionaland
18https://oaqa.github.io/
multimediadatasets.SIGMODRec.,24(2):163–174,May IEEE12thInternationalConferenceon,pages2130–2137.
1995. IEEE,2009.
[21] M.Faruqui,J.Dodge,S.K.Jauhar,C.Dyer,E.Hovy,and [35] E.Kushilevitz,R.Ostrovsky,andY.Rabani.Efficientsearch
N.A.Smith.Retrofittingwordvectorstosemanticlexicons. forapproximatenearestneighborinhighdimensionalspaces.
InProceedingsofthe2015ConferenceoftheNorthAmerican InProceedingsofthe30thannualACMsymposiumonTheory
ChapteroftheAssociationforComputationalLinguistics: ofcomputing,STOC’98,pages614–623.ACM,1998.
HumanLanguageTechnologies,pages1606–1615,Denver, [36] Q.LeandT.Mikolov.Distributedrepresentationsof
Colorado,May–June2015.ACL. sentencesanddocuments.InProceedingsofthe31st
[22] J.F.Fenlon.TheCatholicEncyclopedia,volume4.1913. InternationalConferenceonMachineLearning(ICML-14),
http://en.wikisource.org/wiki/Catholic_Encyclopedia_ pages1188–1196,2014.
%281913%29/Concordances_of_the_Bible[LastChecked [37] H.Li,W.Liu,andH.Ji.Two-stagehashingforfastdocument
March2016]. retrieval.InProceedingsofthe52ndAnnualMeetingofthe
[23] D.Fried,P.Jansen,G.Hahn-Powell,M.Surdeanu,and AssociationforComputationalLinguistics(Volume2:Short
P.Clark.Higher-orderlexicalsemanticmodelsfornon-factoid Papers),pages495–500,Baltimore,Maryland,June2014.
answerreranking.TransactionsoftheACL,3:197–210,2015. ACL.
[24] G.W.Furnas,T.K.Landauer,L.M.Gomez,andS.T.Dumais. [38] T.-Y.Liu.Learningtorankforinformationretrieval.Found.
Thevocabularyprobleminhuman-systemcommunication. TrendsInf.Retr.,3(3):225–331,Mar.2009.
CommunicationsoftheACM,30(11):964–971,1987. [39] Y.Malkov,A.Ponomarenko,A.Logvinov,andV.Krylov.
[25] K.Hajebi,Y.Abbasi-Yadkori,H.Shahbazi,andH.Zhang. Approximatenearestneighboralgorithmbasedonnavigable
Fastapproximatenearest-neighborsearchwithk-nearest smallworldgraphs.InformationSystems,45:61–68,2014.
neighborgraph.InProceedingsofthe22dInternationalJoint [40] Y.A.MalkovandD.A.Yashunin.Efficientandrobust
ConferenceonArtificialIntelligence,IJCAI’11,pages approximatenearestneighborsearchusingHierarchical
1312–1317.AAAIPress,2011. NavigableSmallWorldgraphs.ArXive-prints,Mar.2016.
[26] M.E.HouleandJ.Sakuma.Fastapproximatesimilarity [41] C.D.Manning,P.Raghavan,H.Schütze,etal.Introductionto
searchinextremelyhigh-dimensionaldatasets.InData informationretrieval,volume1.CambridgeUniversityPress,
Engineering,2005.ICDE2005.Proceedings.21st 2008.
InternationalConferenceon,pages619–630.IEEE,2005. [42] C.D.Manning,M.Surdeanu,J.Bauer,J.Finkel,S.J.
[27] P.IndykandR.Motwani.Approximatenearestneighbors: Bethard,andD.McClosky.TheStanfordCoreNLPnatural
towardsremovingthecurseofdimensionality.InProceedings languageprocessingtoolkit.InACLSystemDemonstrations,
ofthe30thannualACMsymposiumonTheoryofcomputing, pages55–60,2014.
pages604–613.ACM,1998. [43] D.MetzlerandW.BruceCroft.Linearfeature-basedmodels
[28] M.Iyyer,J.Boyd-Graber,L.Claudino,R.Socher,and forinformationretrieval.InformationRetrieval,
H.DauméIII.Aneuralnetworkforfactoidquestion 10(3):257–274,2007.
answeringoverparagraphs.InProceedingsofthe2014 [44] T.Mikolov,K.Chen,G.Corrado,andJ.Dean.Efficient
ConferenceonEmpiricalMethodsinNaturalLanguage estimationofwordrepresentationsinvectorspace.CoRR,
Processing(EMNLP),pages633–644,Doha,Qatar,October abs/1301.3781,2013.
2014.ACL. [45] T.Mikolov,W.-t.Yih,andG.Zweig.Linguisticregularitiesin
[29] D.Jacobs,D.Weinshall,andY.Gdalyahu.Classificationwith continuousspacewordrepresentations.InHLT-NAACL,pages
nonmetricdistances:Imageretrievalandclassrepresentation. 746–751,2013.
PatternAnalysisandMachineIntelligence,22(6):583–600,
[46] S.Moran,R.McCreadie,C.Macdonald,andI.Ounis.
2000. Enhancingfirststorydetectionusingwordembeddings.In
[30] J.Jeon,W.B.Croft,andJ.H.Lee.Findingsimilarquestions Proceedingsofthe39thInternationalACMSIGIRConference
inlargequestionandanswerarchives.InProceedingsofthe onResearchandDevelopmentinInformationRetrieval,
14thACMInternationalConferenceonInformationand SIGIR’16.ACM,2016.
KnowledgeManagement,CIKM’05,pages84–90,NewYork,
[47] Y.MuandS.Yan.Non-metriclocality-sensitivehashing.In
NY,USA,2005.ACM. AAAI,2010.
[31] R.Kallman,H.Kimura,J.Natkins,A.Pavlo,A.Rasin, [48] M.MujaandD.G.Lowe.Scalablenearestneighbor
S.Zdonik,E.P.C.Jones,S.Madden,M.Stonebraker, algorithmsforhighdimensionaldata.PatternAnalysisand
Y.Zhang,J.Hugg,andD.J.Abadi.H-store:A MachineIntelligence,IEEETransactionson,
high-performance,distributedmainmemorytransaction 36(11):2227–2240,2014.
processingsystem.Proc.VLDBEndow.,1(2):1496–1499,
[49] B.Naidan,L.Boytsov,andE.Nyberg.Permutationsearch
Aug.2008.
methodsareefficient,yetfastersearchispossible.
[32] D.KonopnickiandO.Shmueli.Database-inspiredsearch.In ProceedingsoftheVLDBEndowment,8(12):1618–1629,
Proceedingsofthe31stInternationalConferenceonVery
2015.
LargeDataBases,VLDB’05,pages2–12.VLDB
[50] F.J.OchandH.Ney.Asystematiccomparisonofvarious
Endowment,2005.
statisticalalignmentmodels.ComputationalLinguistics,
[33] F.Korn,B.-U.Pagel,andC.Faloutsos.Onthe 29(1):19–51,2003.
“dimensionalitycurse”andthe“self-similarityblessing”.
[51] J.Pennington,R.Socher,andC.Manning.Glove:Global
KnowledgeandDataEngineering,IEEETransactionson,
vectorsforwordrepresentation.InProceedingsofthe2014
13(1):96–111,2001.
ConferenceonEmpiricalMethodsinNaturalLanguage
[34] B.KulisandK.Grauman.Kernelizedlocality-sensitive
hashingforscalableimagesearch.InComputerVision,2009
Processing(EMNLP),pages1532–1543,Doha,Qatar, [66] P.D.Turney.Human-levelperformanceonwordanalogy
October2014.ACL. questionsbylatentrelationalanalysis.arXivpreprint
[52] V.Pestov.Lowerboundsonperformanceofmetrictree cs/0412024,2004.
indexingschemesforexactsimilaritysearchinhigh [67] H.TurtleandJ.Flood.Queryevaluation:Strategiesand
dimensions.Algorithmica,66(2):310–328,2012. optimizations.Inf.Process.Manage.,31(6):831–850,Nov.
[53] S.Petrovic´,M.Osborne,andV.Lavrenko.Streamingfirst 1995.
storydetectionwithapplicationtotwitter.InNAACL’10, [68] S.Vigna.Quasi-succinctindices.InProceedingsoftheSixth
pages181–189.AssociationforComputationalLinguistics, ACMInternationalConferenceonWebSearchandData
2010. Mining,WSDM’13,pages83–92,NewYork,NY,USA,2013.
[54] S.Petrovic´,M.Osborne,andV.Lavrenko.Usingparaphrases ACM.
forimprovingfirststorydetectioninnewsandtwitter.In [69] J.Wang,H.T.Shen,J.Song,andJ.Ji.Hashingforsimilarity
Proceedingsofthe2012ConferenceoftheNorthAmerican search:Asurvey.arXivpreprintarXiv:1408.2927,2014.
ChapteroftheAssociationforComputationalLinguistics: [70] J.Wang,J.Wang,G.Zeng,R.Gan,S.Li,andB.Guo.Fast
HumanLanguageTechnologies,NAACLHLT’12,pages neighborhoodgraphsearchusingcartesianconcatenation.In
338–346,Stroudsburg,PA,USA,2012.ACL. MultimediaDataMining&Analytics,pages397–417.
[55] J.M.PonteandW.B.Croft.Alanguagemodelingapproach Springer,2015.
toinformationretrieval.InProceedingsofthe21stannual [71] R.Weber,H.-J.Schek,andS.Blott.Aquantitativeanalysis
internationalACMSIGIRconferenceonResearchand andperformancestudyforsimilarity-searchmethodsin
developmentininformationretrieval,pages275–281.ACM, high-dimensionalspaces.InVLDB,volume98,pages
1998. 194–205,1998.
[56] S.RiezlerandY.Liu.Queryrewritingusingmonolingual [72] J.S.WhissellandC.L.Clarke.Effectivemeasuresfor
statisticalmachinetranslation.ComputationalLinguistics, inter-documentsimilarity.InProceedingsofthe22NdACM
36(3):569–582,2010. InternationalConferenceonInformation&Knowledge
[57] S.Riezler,A.Vasserman,I.Tsochantaridis,V.Mittal,and Management,CIKM’13,pages1361–1370,NewYork,NY,
Y.Liu.Statisticalmachinetranslationforqueryexpansionin USA,2013.ACM.
answerretrieval.InProceedingsofthe45thAnnualMeeting [73] J.Wieting,M.Bansal,K.Gimpel,andK.Livescu.Towards
oftheACL,pages464–471,Prague,CzechRepublic,June universalparaphrasticsentenceembeddings.CoRR,
2007.ACL. abs/1511.08198,2015.
[58] S.Robertson.Understandinginversedocumentfrequency:On [74] X.Xue,J.Jeon,andW.B.Croft.Retrievalmodelsfor
theoreticalargumentsforIDF.JournalofDocumentation, questionandanswerarchives.InProceedingsofthe31st
60:503–520,2004. AnnualInternationalACMSIGIRConferenceonResearch
[59] G.SaltonandM.J.McGill.IntroductiontoModern andDevelopmentinInformationRetrieval,SIGIR’08,pages
InformationRetrieval.McGraw-Hill,Inc.,NewYork,NY, 475–482.ACM,2008.
USA,1986. [75] L.Yang,Q.Ai,D.Spina,R.-C.Chen,L.Pang,W.B.Croft,
[60] T.B.SebastianandB.B.Kimia.Metric-basedshaperetrieval J.Guo,andF.Scholer.AdvancesinInformationRetrieval:
inlargedatabases.InPatternRecognition,2002.Proceedings. 38thEuropeanConferenceonIRResearch,ECIR2016,
16thInternationalConferenceon,volume3,pages291–296. Padua,Italy,March20-23,2016.Proceedings,chapter
IEEE,2002. BeyondFactoidQA:EffectiveMethodsforNon-factoid
[61] T.Skopal.Onfastnon-metricsimilaritysearchbymetric AnswerSentenceRetrieval,pages115–128.Springer
accessmethods.InAdvancesinDatabaseTechnology-EDBT InternationalPublishing,Cham,2016.
2006,pages718–736.Springer,2006. [76] X.Yao,B.VanDurme,andP.Clark.Automaticcouplingof
[62] R.SoricutandE.Brill.Automaticquestionansweringusing answerextractionandinformationretrieval.InProceedingsof
theweb:Beyondthefactoid.InformationRetrieval, the51stAnnualMeetingoftheAssociationforComputational
9(2):191–206,2006. Linguistics(Volume2:ShortPapers),pages159–165,Sofia,
[63] M.Surdeanu,M.Ciaramita,andH.Zaragoza.Learningto Bulgaria,August2013.ACL.
rankanswerstonon-factoidquestionsfromwebcollections. [77] L.ZhaoandJ.Callan.Termnecessityprediction.In
ComputationalLinguistics,37(2):351–383,2011. Proceedingsofthe19thACMinternationalconferenceon
[64] E.S.Tellez,E.Chávez,andG.Navarro.Succinctnearest Informationandknowledgemanagement,pages259–268.
neighborsearch.InformationSystems,38(7):1019–1030, ACM,2010.
2013.
[65] F.Ture,T.Elsayed,andJ.Lin.Nofreelunch:Bruteforcevs.
locality-sensitivehashingforcross-lingualpairwisesimilarity.
InProceedingsofthe34thInternationalACMSIGIR
ConferenceonResearchandDevelopmentinInformation
Retrieval,SIGIR’11,pages943–952,NewYork,NY,USA,
2011.ACM.

Toward Abstractive Summarization Using Semantic Representations
FeiLiu JeffreyFlanigan SamThomson NormanSadeh NoahA.Smith
SchoolofComputerScience
CarnegieMellonUniversity
Pittsburgh,PA15213,USA
{feiliu, jflanigan, sthomson, sadeh, nasmith}@cs.cmu.edu
Abstract and propose that recent developments in semantic
analysishaveanimportantroletoplay.
Wepresentanovelabstractivesummarization We conduct the first study exploring the feasi-
framework that draws on the recent develop- bilityofanabstractivesummarizationsystembased
ment of a treebank for the Abstract Meaning
ontransformationsofsemanticrepresentationssuch
Representation(AMR).Inthisframework,the
astheAbstractMeaningRepresentation(AMR;Ba-
sourcetextisparsedtoasetofAMRgraphs,
narescu et al., 2013). Example sentences and their
the graphs are transformed into a summary
AMR graphs are shown in Fig. 1. AMR has much
graph, and then text is generated from the
summary graph. We focus on the graph-to- in common with earlier formalisms (Kasper, 1989;
graph transformation that reduces the source Dorr et al., 1998); today an annotated corpus com-
semantic graph into a summary graph, mak- prised of over 20,000 AMR-analyzed English sen-
inguseofanexistingAMRparserandassum- tences (Knight et al., 2014) and an automatic AMR
ing the eventual availability of an AMR-to-
parser(JAMR;Flaniganetal.,2014)areavailable.
textgenerator. Theframeworkisdata-driven,
In our framework, summarization consists of
trainable, and not specifically designed for
three steps illustrated in Fig. 1: (1) parsing the in-
a particular domain. Experiments on gold-
standardAMRannotationsandsystemparses put sentences to individual AMR graphs, (2) com-
show promising results. Code is available at: bining and transforming those graphs into a single
https://github.com/summarization summary AMR graph, and (3) generating text from
the summary graph. This paper focuses on step 2,
treating it as a structured prediction problem. We
1 Introduction
assumetextdocumentsasinput1 anduseJAMRfor
step 1. We use a simple method to read a bag of
Abstractive summarization is an elusive technolog-
words off the summary graph, allowing evaluation
ical capability in which textual summaries of con-
with ROUGE-1, and leave full text generation from
tent are generated de novo. Demand is on the rise
AMR(step3)tofuturework.
forhigh-qualitysummariesnotjustforlengthytexts
The graph summarizer, described in §4, first
(e.g., books; Bamman and Smith, 2013) and texts
mergesAMRgraphsforeachinputsentencethrough
knowntobeprohibitivelydifficultforpeopletoun-
aconceptmergingstep,inwhichcoreferentnodesof
derstand(e.g.,websiteprivacypolicies;Sadehetal.,
the graphs are merged; a sentence conjunction step,
2013), but also for non-textual media (e.g., videos
which connects the root of each sentence’s AMR
andimagecollections;Kimetal.,2014;Kuznetsova
graph to a dummy “ROOT” node; and an optional
etal.,2014;ZhaoandXing,2014),whereextractive
and compressive summarization techniques simply
1In principle, the framework could be applied to other in-
do not suffice. We believe that the challenge of ab-
puts,suchasimagecollections,ifAMRparsersbecameavail-
stractive summarization deserves renewed attention ableforthem.
1077
HumanLanguageTechnologies:The2015AnnualConferenceoftheNorthAmericanChapteroftheACL,pages1077–1086,
Denver,Colorado,May31–June5,2015.(cid:13)c2015AssociationforComputationalLinguistics
Concepts can be English words (“dog”), PropBank
Sentence A: I saw Joe’s dog, which was running in the garden.
Sentence B: The dog was chasing a cat. event predicates (“chase-01,” “run-02”), or special
keywords(“person”). Forexample,“chase-01”rep-
1 see-01 chase-01
ARG0 ARG1 ARG0 ARG1 resents a PropBank roleset that corresponds to the
i dog dog cat firstsenseof“chase”. Accordingto Banarescuetal.
poss ARG0-of
(2013),AMRusesapproximately100relations. The
person run-02
name location rolesets and core semantic relations (e.g., ARG0 to
name garden ARG5) are adopted from the PropBank annotations
op1
2 inOntoNotes(Hovyetal.,2006). Othersemanticre-
“Joe”
chase-01 lations include “location,” “mode,” “name,” “time,”
ARG0 location ARG1 and “topic.” The AMR guidelines2 provide more
poss
person dog garden cat
detailed descriptions. Banarescu et al. (2013) de-
name
name scribe AMR Bank, a 20,341-sentence corpus anno-
op1 tatedwithAMRbyexperts.
3
“Joe”
Step1ofourframeworkconvertsinputdocument
Summary: Joe’s dog was chasing a cat in the garden. sentencesintoAMRgraphs. Weuseastatisticalse-
mantic parser, JAMR (Flanigan et al., 2014), which
Figure1: Atoyexample. Sentencesareparsedintoindi- wastrainedonAMRBank. JAMR’scurrentperfor-
vidualAMRgraphsinstep1;step2conductsgraphtrans- manceonourtestdatasetis63%F-score.3 Wewill
formation that produces a single summary AMR graph;
analyze the effect of AMR parsing errors by com-
textisgeneratedfromthesummarygraphinstep3.
paringJAMRoutputwithgold-standardannotations
ofinputsentencesintheexperiments(§6).
In addition to predicting AMR graphs for each
graph expansion step, where additional edges are
sentence,JAMRprovidesalignmentsbetweenspans
addedtocreateafullydensegraphonthesentence-
of words in the source sentence and fragments of
level. Thesestepsresultinasingleconnectedsource
its predicted graph. For example, a graph fragment
graph. A subset of the nodes and arcs from the
headed by “date-entity” could be aligned to the to-
source graph are then selected for inclusion in the
kens “April 8, 2002.” We use these alignments in
summary graph. Ideally this is a condensed repre-
oursimpletextgenerationmodule(step3;§5).
sentation of the most salient semantic content from
thesource.
3 Dataset
We briefly review AMR and JAMR (§2), then
presentthedatasetusedinthispaper(§3). Themain To build and evaluate our framework, we require
algorithmispresentedin§4,andwediscussoursim- a dataset that includes inputs and summaries, each
plegenerationstepin§5. Ourexperiments(§6)mea- with gold-standard AMR annotations.4 This allows
suretheintrinsicqualityofthegraphtransformation ustouseastatisticalmodelforstep2(graphsumma-
algorithmaswellasthequalityofthetermsselected rization)andtoseparateitserrorsfromthoseinstep
for the summary (using ROUGE-1). We explore 1(AMRparsing),whichisimportantindetermining
variationsonthetransformationandthelearningal- whetherthisapproachisworthfurtherinvestment.
gorithm, and show oracle upper bounds of various Fortunately, the “proxy report” section of the
kinds. AMR Bank (Knight et al., 2014) suits our needs. A
2http://www.isi.edu/˜ulf/amr/help/
2 Background: AbstractMeaning amr-guidelines.pdf
RepresentationandJAMR 3AMR parse quality is evaluated using smatch (Cai and
Knight,2013),whichmeasurestheaccuracyofconceptandre-
AMR provides a whole-sentence semantic repre- lationpredictions.JAMRwastrainedonthein-domaintraining
portionofLDC2014T12forourexperiments.
sentation, represented as a rooted, directed, acyclic
4Traditional multi-document summarization datasets, such
graph(Fig.1). NodesofanAMRgrapharelabeled
as the ones used in DUC and TAC competitions, do not have
withconcepts,andedgesarelabeledwithrelations. gold-standardAMRannotations.
1078
Ave.#Sents. SourceGraph
#Docs.
Summ. Doc. Nodes Edges Expand date-entity
person
Train 298 1.5 17.5 127 188 2,670 name year month day
Dev. 35 1.4 19.2 143 220 3,203
name “2002” “4” “8”
Test 33 1.4 20.5 162 255 4,002
op1
“Joe”
Table 1: Statistics of our dataset. “Expand” shows the date-entity::year::“2002”::month::“4”::day::“8”
numberofedgesafterperforminggraphexpansion. The
person::name::name::op1::“Joe”
numbers are averaged across all documents in the split.
Weusetheofficialsplit,droppingonetrainingdocument Figure 2: Graph fragments are collapsed into a single
forwhichnosummarysentenceswereannotated. conceptandassignedanewconceptlabel.
proxyreportiscreatedbyannotatorsbasedonasin- the fragment is a flat structure. A collapsed named
gle newswire article, selected from the English Gi- entityisfurthercombinedwithitsparent(e.g.,“per-
gawordcorpus. Thereportheadercontainsmetadata son”) into one concept node if it is the only child
aboutdate,country,topic,andashortsummary. The of the parent. Two such graph fragments are illus-
report body is generated by editing or rewriting the trated in Fig. 2. We choose named and date entity
content of the newswire article to approximate the concepts since they appear frequently, but most of-
styleofananalystreport. Hencethisisasingledoc- ten refer to different entities (e.g., “April 8, 2002”
umentsummarizationtask. Allsentencesarepaired vs. “Nov. 17”). No further collapsing is done. A
with gold-standard AMR annotations. Table 1 pro- collapsedgraphfragmentisassignedanewlabelby
videsanoverviewofourdataset. concatenating the consisting concept and edge la-
bels. Eachfragmentthatiscollapsedintoanewcon-
4 GraphSummarization
cept node can then only be merged with other iden-
ticalfragments. Thisprocesswon’trecognizecoref-
GivenAMRgraphsforallofthesentencesinthein-
erentconceptslike“BarackObama”=“Obama”and
put (step 1), graph summarization transforms them
“say-01” = “report-01,” but future work may incor-
into a single summary AMR graph (step 2). This is
porate both entity coreference resolution and event
accomplishedintwostages: sourcegraphconstruc-
coreference resolution, as concept nodes can repre-
tion(§4.1);andsubgraphprediction(§4.2).
senteither.
4.1 SourceGraphConstruction Due to the concept merging step, a pair of con-
The“sourcegraph”isasinglegraphconstructedus- ceptsmaynowhavemultiplelabelededgesbetween
ingtheindividualsentences’AMRgraphsbymerg- them. Wemergeallsuchedgesbetweenagivenpair
ing identical concepts. In the AMR formalism, an ofconceptsintoasingleunlabelededge. Weremem-
entity or event is canonicalized and represented by ber the two most common labels in such a group,
a single graph fragment, regardless of how many whichareusedintheedge“Label”feature(Table3).
times it is referred to in the sentence. This princi- To ensure that the source graph is connected, we
ple can be extended to multiple sentences, ideally addanew“ROOT”nodeandconnectittoeverycon-
resultinginasourcegraphwithnoredundancy. Be- ceptthatwasoriginallytherootofasentencegraph
cause repeated mentions of a concept in the input (see Fig. 3). When we apply this procedure to the
can signal its importance, we will later encode the documentsinourdataset(§3),sourcegraphscontain
frequencyofmentionsasafeatureusedinsubgraph 144nodesand221edgesonaverage.
prediction. We investigated how well these automatically
Concept merging involves collapsing certain constructed source graphs cover the gold-standard
graphfragmentsintoasingleconcept,thenmerging summarygraphsproducedbyAMRannotators. Ide-
all concepts that have the same label. We collapse ally, a source graph should cover all of the gold-
thegraphfragmentsthatareheadedbyeitheradate- standard edges, so that summarization can be ac-
entity(“date-entity”)oranamedentity(“name”),if complished by selecting a subgraph of the source
1079
pansion increases the average number of edges by
ROOT
a factor of 15, to 3,292. Fig. 3 illustrates the moti-
see-01 Merging chase-01 vation. Document-level expansion covers the gold-
i dog dog cat standard summary edge “chase-01” → “garden,”
yet the expansion is computationally prohibitive;
person run-02 2
1 sentence-level expansion adds an edge “dog” →
Graph Expansion
“garden,” which enables the prediction of a struc-
Collapsing name garden
turewithsimilarsemanticmeaning: “Joe’sdogwas
“Joe” inthegardenchasingacat.”
Sentence A: I saw Joe’s dog, which was running in the garden. 4.2 SubgraphPrediction
Sentence B: The dog was chasing a cat.
We pose the selection of a summary subgraph from
Figure 3: A source graph formed from two sentence the source graph as a structured prediction prob-
AMR graphs. Concept collapsing, merging, and graph lem that trades off among including important in-
expansion are demonstrated. Edges are unlabeled. A formation without altering its meaning, maintain-
“ROOT” node is added to ensure connectivity. (1) and ingbrevity,andproducingfluentlanguage(Nenkova
(2) are among edges added through the optional expan-
and McKeown, 2011). We incorporate these con-
sionstep,correspondingtosentence-anddocument-level
cerns in the form of features and constraints in the
expansion, respectively. Concept nodes included in the
statisticalmodelforsubgraphselection.
summarygraphareshaded.
LetG = (V,E)denotethemergedsourcegraph,
where each node v ∈ V represents a unique con-
SummaryEdgeCoverage(%)
cept and each directed edge e ∈ E connects two
Expand
concepts. G is a connected, directed, node-labeled
Labeled Unlabeled Sent. Doc.
graph. Edges in this graph are unlabeled, and edge
Train 64.8 67.0 75.5 84.6
labels are not predicted during subgraph selection.
Dev. 77.3 78.6 85.4 91.8
We seek to maximize a score that factorizes over
Test 63.0 64.7 75.0 83.3
graphnodesandedgesthatareincludedinthesum-
Table 2: Percentage of summary edges that can be cov- marygraph. Forsubgraph(V0,E0):
eredbyanautomaticallyconstructedsourcegraph.
X X
score(V0,E0;θ,ψ) = θ>f(v)+ ψ>g(e)
v∈V0 e∈E0
graph (§4.2). In Table 2, columns one and two re-
(1)
port labeled and unlabeled edge coverage. ‘Unla-
beled’ counts edges as matching if both the source where f(v) and g(e) are the feature representations
and destination concepts have identical labels, but ofnodevandedgee,respectively. Wedescribenode
ignorestheedgelabel. andedgefeaturesinTable3. θ andψ arevectorsof
In order to improve edge coverage, we explore empiricallyestimatedcoefficientsinalinearmodel.
expanding the source graph by adding every possi- We next formulate the selection of the subgraph
ble edge between every pair of concepts within the using integer linear programming (ILP; §4.2.1) and
samesentence. Wealsoexploredaddingeverypos- describesupervisedlearningfortheparameters(co-
sibleedgebetweeneverypairofconceptsintheen- efficients)fromacollectionofsourcegraphspaired
tire source graph. Edges that are newly introduced withsummarygraphs(§4.2.2).
during expansion receive a default label ‘null’. We
4.2.1 Decoding
report unlabeled edge coverage in Table 2, columns
three and four, respectively. Subgraph prediction WecastdecodingasanILPwhoseconstraintsen-
became infeasable with the document-level expan- sure that the output forms a connected subcompo-
sion, so we conducted our experiments using only nent of the source graph. We index source graph
sentence-level expansion. Sentence-level graph ex- concept nodes by i and j, giving the “ROOT” node
1080
Node Concept Identityfeatureforconceptlabel
Features Freq Conceptfreqintheinputsentenceset;onebinaryfeaturedefinedforeachfrequencythresholdτ =0/1/2/5/10
Depth Averageandsmallestdepthofnodetotherootofthesentencegraph;binarizedusing5depththresholds
Position Averageandforemostpositionofsentencescontainingtheconcept;binarizedusing5positionthresholds
Span Averageandlongestwordspanofconcept;binarizedusing5lengththresholds;wordspansobtainedfromJAMR
Entity Twobinaryfeaturesindicatingwhethertheconceptisanamedentity/dateentityornot
Bias Biasterm,1foranynode
Edge Label Firstandsecondmostfrequentedgelabelsbetweenconcepts;relativefreqofeachlabel,binarizedby3thresholds
Features Freq Edgefrequency(w/olabel,non-expandededges)inthedocumentsentences;binarizedusing5frequencythresholds
Position Averageandforemostpositionofsentencescontainingtheedge(withoutlabel);binarizedusing5positionthresholds
Nodes Nodefeaturesextractedfromthesourceandtargetnodes(allabovenodefeaturesexceptthebiasterm)
IsExpanded Abinaryfeatureindicatingtheedgeisduetographexpansionornot;edgefreq(w/olabel,alloccurrences)
Bias Biasterm,1foranyedge
Table3: Nodeandedgefeatures(allbinarized).
index0. LetN bethenumberofnodesinthegraph. The AMR representation allows graph reentran-
Letv ande bebinaryvariables. v is1iffsource cies (concept nodes having multiple parents), yet
i i,j i
nodeiisincluded;e is1iffthedirectededgefrom reentrancies are rare; about 5% of edges are re-
i,j
nodeitonodej isincluded. entrancies in our dataset. In this preliminary study
TheILPobjectivetobemaximizedisEquation1, we force the summary graph to be tree-structured,
rewrittenhereinthepresentnotation: requiringthatthereisatmostoneincomingedgefor
eachnode:
XN X
v θ>f(i) + e ψ>g(i,j) (2)
i | {z } i,j | {z } X
i=1 nodescore (i,j)∈E edgescore e ≤ 1, ∀j ≤ N. (7)
i,j
j
Note that this objective is linear in {v ,e } and
i i,j i,j
thatfeaturesandcoefficientscanbefoldedintonode Interestingly, the formulation so far equates to
and edge scores and treated as constants during de- an ILP for solving the prize-collecting Steiner tree
coding. problem (PCST; Segev, 1987), which is known to
Constraintsarerequiredtoensurethattheselected be NP-complete (Karp, 1972). Our ILP formula-
nodesandedgesformavalidgraph. Inparticular,if tion is modified from that of Ljubic´ et al. (2006).
anedge(i,j)isselected(e takesvalueof1),then Flow-based constraints for tree structures have also
i,j
bothitsendpointsi,j mustbeincluded: previously been used in NLP for dependency pars-
ing (Martins et al., 2009) and sentence compres-
v −e ≥ 0, v −e ≥ 0, ∀i,j ≤ N (3)
i i,j j i,j sion (Thadani and McKeown, 2013). In our exper-
Connectivity is enforced using a set of single- iments, we use an exact ILP solver,5 though many
commodity flow variables f , each taking a non- approximatemethodsareavailable.
i,j
negative integral value, representing the flow from Finally, an optional constraint can be used to fix
nodeitoj. TherootnodesendsoutuptoN unitsof the size of the summary graph (measured by the
flow, one to reach each included node (Equation 4). numberofedges)toL:
Each included node consumes one unit of flow, re- XX
e = L (8)
flectedasthedifferencebetweenincomingandout- i,j
goingflow(Equation5). Flowmayonlybesentover i j
anedgeiftheedgeisincluded(Equation6). Theperformanceofsummarizationsystemsdepends
X X
strongly on their compression rate, so systems are
f − v = 0, (4)
0,i i
only directly comparable when their compression
i i
X X
ratesaresimilar(Napolesetal.,2011). Lissupplied
f − f −v = 0, ∀j ≤ N, (5)
i,j j,k j
tothesystemtocontrolsummarygraphsize.
i k
N ·e
i,j
−f
i,j
≥ 0, ∀i,j ≤ N. (6) 5http://www.gurobi.com
1081
4.2.2 ParameterEstimation 6 Experiments
Givenacollectionofinputandoutputpairs(here,
In Table 5, we report the performance of subgraph
sourcegraphsandsummarygraphs),anaturalstart-
predictionandend-to-endsummarizationonthetest
ing place for learning the coefficients θ and ψ is
set,usinggold-standardandautomaticAMRparses
the structured perceptron (Collins, 2002), which is
for the input. Gold-standard AMR annotations are
easytoimplementandoftenperformswell. Alterna-
used for model training in all conditions. During
tively,incorporatingfactoredcostfunctionsthrough
testing,weapplythetrainedmodeltosourcegraphs
a structured hinge loss leads to a structured support
constructed using either gold-standard or JAMR
vector machine (SVM; Taskar et al., 2004) which
parses. Inalloftheseexperiments,weusethenum-
can be learned with a very similar stochastic opti-
berofedgesinthegold-standardsummarygraphto
mization algorithm. In our scenario, however, the
fix the number of edges in the predicted subgraph,
gold-standard summary graph may not actually be
allowingdirectcomparisonacrossconditions.
a subset of the source graph. In machine transla-
Subgraphpredictionisevaluatedagainstthegold-
tion,ramplosshasbeenfoundtoworkwellinsitu-
standardAMRgraphsonsummaries. Wereportpre-
ationswherethegold-standardoutputmaynoteven
cision,recall,andF fornodes,andF foredges.6
beinthehypothesisspaceofthemodel(Gimpeland 1 1
Oracle results for the subgraph prediction stage
Smith,2012). Thestructuredperceptron,hinge,and
are obtained using the ILP decoder to minimize the
ramplossesarecomparedinTable4.
cost of the output graph, given the gold-standard.
We explore learning by minimizing each of the
We assign wrong nodes and edges a score of −1,
perceptron, hinge, and ramp losses, each optimized
correct nodes and edges a score of 0, then decode
usingAdagrad(Duchietal.,2011),astochasticop-
with the same structural constraints as in subgraph
timizationprocedure. Letβ beonemodelparameter
prediction. Theresultinggraphisthebestsummary
(coefficient from θ or ψ). Let g(t) be the subgradi-
graph in the hypothesis space of our model, and
ent of the loss on the instance considered on the tth
provides an upper bound on performance achiev-
iterationwithrespecttoβ. Givenaninitialstepsize
able within our framework. Oracle performance on
η,theupdateforβ oniterationtis:
node prediction is in the range of 80% when using
η gold-standardAMRannotations,and70%whenus-
β(t+1) ← β(t)− q g(t) (9)
P ingJAMRoutput. Edgepredictionhaslowerperfor-
t (g(τ))2
τ=1 mance,yielding52.2%forgold-standardand31.1%
for JAMR parses. When graph expansion was ap-
5 Generation
plied,thenumbersincreasedto64%and46.7%,re-
Generation from AMR-like representations has re- spectively. Theuncoveredsummaryedge(i.e.,those
ceivedsomeattention,e.g.,byLangkildeandKnight not covered by source graph) is a major source for
(1998) who described a statistical method. Though low recall values on edge prediction (see Table 2);
we know of work in progress driven by the goal of graphexpansionslightlyalleviatesthisissue.
machine translation using AMR, there is currently Summarization is evaluated by comparing sys-
nosystemavailable. tem summaries against reference summaries, using
We therefore use a heuristic approach to gener- ROUGE-1 scores (Lin, 2004)7. System summaries
ate a bag of words. Given a predicted subgraph, a aregeneratedusingtheheuristicapproachpresented
system summary is created by finding the most fre- in§5: givenapredictedsubgraph,theapproachfinds
quently aligned word span for each concept node. themostfrequentlyalignedwordspanforeachcon-
(Recall that the JAMR parser provides these align- cept node, and then puts them together as a bag of
ments; §2). The words in the resulting spans are words. ROUGE-1 is particularly usefully for eval-
generated in no particular order. While this is
6Precision, recall, and F are equal since the number of
not a natural language summary, it is suitable for 1
edgesisfixed.
unigram-based summarization evaluation methods
7ROUGEversion1.5.5withoptions‘-edata-n4-m-24-u
likeROUGE-1. -c95-r1000-fA-p0.5-t0-a-x’
1082
Structuredperceptronloss: −score(G∗) + maxscore(G)
G
Structuredhingeloss: −score(G∗) + max(score(G)+cost(G;G∗))
G
Structuredramploss: −max(score(G)−cost(G;G∗)) + max(score(G)+cost(G;G∗))
G G
Table4: Lossfunctionsminimizedinparameterestimation. G∗ denotesthegold-standardsummarygraph. score(·)
isasdefinedinEquation1. cost(G;G∗)penalizeseachvertexoredgeinG∪G∗\(G∩G∗). Sincecost factorsjust
likethescoringfunction,eachmaxoperationcanbeaccomplishedusingavariantofILPdecoding(§4.2.1)inwhich
thecostisincorporatedintothelinearobjectivewhiletheconstraintsremainthesame.
SubgraphPrediction Summarization
Nodes Edges ROUGE-1
P(%) R(%) F(%) F(%) P(%) R(%) F(%)
gold- Perceptron 39.6 46.1 42.6 24.7 41.4 27.1 32.3
standard Hinge 41.2 47.9 44.2 26.4 42.6 28.3 33.5
parses Ramp 54.7 63.5 58.7 39.0 51.9 39.0 44.3
Ramp+Expand 53.0 61.3 56.8 36.1 50.4 37.4 42.8
Oracle 75.8 86.4 80.7 52.2
89.1 52.8 65.8
Oracle+Expand 78.9 90.1 83.9 64.0
JAMR Perceptron 42.2 48.9 45.2 14.5 46.1 35.0 39.5
parses Hinge 41.7 48.3 44.7 15.8 44.9 33.6 38.2
Ramp 48.1 55.6 51.5 20.0 50.6 40.0 44.4
Ramp+Expand 47.5 54.6 50.7 19.0 51.2 40.0 44.7
Oracle 64.1 74.8 68.9 31.1
87.5 43.7 57.8
Oracle+Expand 66.9 76.4 71.2 46.7
Table5:Subgraphpredictionandsummarization(tobagofwords)resultsontestset.Gold-standardAMRannotations
are used for model training in all conditions. “+ Expand” means the result is obtained using source graph with
expansion;edgeperformanceismeasuredignoringlabels.
uating such less well-formed summaries, such as generator,ROUGE-1scoresonlydependonconcept
those generated from speech transcripts (Liu and prediction and are unaffected by edge prediction.
Liu,2013). Theoraclesummarizationresults,65.8%and57.8%
Oracle summaries are produced by taking the F scores for gold-standard and JAMR parses, re-
1
gold-standard AMR parses of the reference sum- spectively,furthersuggestthatimprovedgraphsum-
mary, obtaining the most frequently aligned word marizationmodels(step2)mightbenefitfromfuture
spanforeachuniqueconceptnodeusingtheJAMR improvementsinAMRparsing(step1).
aligner (§2), and then generating a bag of words Across all conditions and both evaluations, we
summary. Evaluation of oracle summaries is per- find that incorporating a cost-aware loss function
formed in the same manner as for system sum- (hinge vs. perceptron) has little effect, but that us-
maries. The above process does not involve graph ingramplossleadstosubstantialgains.
expansion, so summarization performance is the In Table 5, we show detailed results with and
same for the two conditions “Oracle” and “Oracle withoutgraphexpansion. “+Expand”meansthere-
+Expand.” sults are obtained using the expanded source graph.
We find that JAMR parses are a large source of Wefindthatgraphexpansiononlymarginallyaffects
degradation of edge prediction performance, and a systemperformance. Graphexpansionslightlyhurts
smaller but still significant source of degradation thesystemperformanceonedgeprediction. Forex-
for concept prediction. Surprisingly, using JAMR ample, using ramp loss with JAMR parser as input,
parses leads to slightly improved ROUGE-1 scores. we obtained 50.7% and 19.0% for node and edge
Keep in mind, though, that under our bag-of-words prediction with graph expansion; 51.5% and 20.0%
1083
without edge expansion. On the other hand, it in- workmightexploresimilargraph-basedcalculations
creases the oracle performance by a large margin. to contribute features for subgraph selection in our
Thissuggeststhatwithmoretrainingdata,oramore framework.
sophisticated model that is able to better discrimi- Our constructed source graph can easily reach
nateamongtheenlargedoutputspace,graphexpan- ten times or more of the size of a sentence depen-
sionstillhaspromisetobehelpful. dency graph. Thus more efficient graph decoding
algorithms, e.g., based on Lagrangian relaxation or
7 RelatedandFutureWork approximate algorithms, may be explored in future
work. Other future directions may include jointly
Accordingto DangandOwczarzak(2008),thema- performing subgraph and edge label prediction; ex-
jorityofcompetitivesummarizationsystemsareex- ploringafull-fledgedpipelinethatconsistsofanau-
tractive, selecting representative sentences from in- tomatic AMR parser, a graph-to-graph summarizer,
put documents and concatenating them to form a and a AMR-to-text generator; and devising an eval-
summary. This is often combined with sentence uationmetricthatisbettersuitedtoabstractivesum-
compression, allowing more sentences to be in- marization.
cluded within a budget. ILPs and approximations Many domains stand to eventually benefit from
have been used to encode compression and extrac- summarization. These include books, audio/video
tion (McDonald, 2007; Martins and Smith, 2009; segments,andlegaltexts.
Gillick and Favre, 2009; Berg-Kirkpatrick et al.,
8 Conclusion
2011; Almeida and Martins, 2013; Li et al., 2014).
Other decoding approaches have included a greedy
Wehaveintroducedastatisticalabstractivesumma-
method exploiting submodularity (Lin and Bilmes,
rization framework driven by the Abstract Meaning
2010), document reconstruction (He et al., 2012),
Representation. The centerpiece of the approach is
andgraphcuts(QianandLiu,2013),amongothers.
astructuredpredictionalgorithmthattransformsse-
Previous work on abstractive summarization has
manticgraphsoftheinputintoasinglesummaryse-
explored user studies that compare extractive with
manticgraph. Experimentsshowtheapproachtobe
NLG-based abstractive summarization (Carenini
promisingandsuggestdirectionsforfutureresearch.
and Cheung, 2008). Ganesan et al. (2010) pro-
pose to construct summary sentences by repeatedly Acknowledgments
searchingthehighestscoredgraphpaths. (Geraniet
The authors thank three anonymous reviewers for
al., 2014) generate abstractive summaries by modi-
their insightful input. We are grateful to Nathan
fying discourse parse trees. Our work is similar in
Schneider,KevinGimpel,SashaRush,andtheARK
spirittoCheungandPenn(2014),whichsplicesand
group for valuable discussions. The research was
recombines dependency parse trees to produce ab-
supported by NSF grant SaTC-1330596, DARPA
stractive summaries. In contrast, our work operates
grant FA8750-12-2-0342 funded under the DEFT
onsemanticgraphs,takingadvantageoftherecently
program, the U. S. Army Research Laboratory and
developedAMRBank.
theU.S.ArmyResearchOfficeundercontract/grant
Alsorelatedtoourworkaregraph-basedsumma-
number W911NF-10-1-0533, and by IARPA via
rization methods (Vanderwende et al., 2004; Erkan
DoI/NBCcontractnumberD12PC00337. Theviews
andRadev,2004;MihalceaandTarau,2004). Van-
andconclusionscontainedhereinarethoseoftheau-
derwende et al. (2004) transform input to logi-
thors and should not be interpreted as necessarily
cal forms, score nodes using PageRank, and grow
representing the official policies or endorsements,
the graph from high-value nodes using heuristics.
eitherexpressedorimplied,ofthesponsors.
In Erkan and Radev (2004) and Mihalcea and Ta-
rau (2004), the graph connects surface terms that
References
co-occur. In both cases, the graphs are constructed
based on surface text; it is not a representation of MiguelB.AlmeidaandAndreF.T.Martins. 2013. Fast
propositionalsemanticslikeAMR.However,future and robust compressive summarization with dual de-
1084
composition and multi-task learning. In Proceedings Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
ofACL. 2010. Opinosis: A graph-based approach to abstrac-
David Bamman and Noah A. Smith. 2013. New align- tive summarization of highly redundant opinions. In
mentmethodsfordiscriminativebooksummarization. ProceedingsofCOLING.
InarXiv:1305.1319. ShimaGerani,YasharMehdad,GiuseppeCarenini,Ray-
mondT.Ng, andBitaNejat. 2014. Abstractivesum-
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
marization of product reviews using discourse struc-
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
ture. InProceedingsofEMNLP.
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstractmeaningrepresentationfor Dan Gillick and Benoit Favre. 2009. A scalable global
sembanking. InProceedingsofLinguisticAnnotation model for summarization. In Proceedings of the
Workshop. NAACLWorkshoponIntegerLinearProgrammingfor
NaturalLangaugeProcessing.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
Kevin Gimpel and Noah A. Smith. 2012. Structured
2011. Jointly learning to extract and compress. In
ramp loss minimization for machine translation. In
ProceedingsofACL.
ProceedingsofNAACL-HLT.
ShuCaiandKevinKnight. 2013. Smatch: anevaluation
Zhanying He, Chun Chen, Jiajun Bu, Can Wang, Lijun
metricforsemanticfeaturestructures. InProceedings
Zhang, Deng Cai, and Xiaofei He. 2012. Document
ofACL.
summarization based on data reconstruction. In Pro-
Giuseppe Carenini and Jackie Chi Kit Cheung. 2008.
ceedingsofAAAI.
Extractive vs. NLG-based abstractive summarization
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
of evaluative text: The effect of corpus controversial-
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
ity. In Proceedings of the Fifth International Natural
The90%solution. InProceedingsofNAACL.
LanguageGenerationConference(INLG).
RichardM.Karp. 1972. ReducibilityAmongCombina-
Jackie Chi Kit Cheung and Gerald Penn. 2014. Unsu-
torial Problems. In Complexity of Computer Compu-
pervisedsentenceenhancementforautomaticsumma-
tations,pages85–103.SpringerUS.
rization. InProceedingsofEMNLP.
RobertT.Kasper. 1989. Aflexibleinterfaceforlinking
Michael Collins. 2002. Discriminative training meth-
applications to Penman’s sentence generator. In Pro-
ods for hidden Markov models: Theory and experi-
ceedingsoftheDARPASpeechandNaturalLanguage
ments with perceptron algorithms. In Proceedings of
Workshop.
EMNLP.
GunheeKim,LeonidSigal,andEricP.Xing. 2014. Joint
Hoa Trang Dang and Karolina Owczarzak. 2008.
summarization of large-scale collections of web im-
OverviewoftheTAC2008updatesummarizationtask.
ages and videos for storyline reconstruction. In Pro-
InProceedingsofTextAnalysisConference(TAC).
ceedingsofCVPR.
Bonnie Dorr, Nizar Habash, and David Traum. 1998. KevinKnight,LauraBaranescu,ClaireBonial,Madalina
A thematic hierarchy for efficient generation from Georgescu, Kira Griffitt, Ulf Hermjakob, Daniel
lexical-conceptual structure. In David Farwell, Lau- Marcu, Martha Palmer, and Nathan Schneider. 2014.
rieGerber,andEduardHovy,editors,MachineTrans- Abstract meaning representation (AMR) annotation
lation and the Information Soup: Proceedings of release 1.0 LDC2014T12. Web Download. Philadel-
the Third Conference of the Association for Machine phia: LinguisticDataConsortium.
Translation in the Americas, Lecture Notes in Com-
Polina Kuznetsova, Vicente Ordonez, Tamara L. Berg,
puterScience.Springer.
andYejinChoi. 2014. TREETALK:Compositionand
John Duchi, Elad Hazan, and Yoram Singer. 2011. compressionoftreesforimagedescriptions. Transac-
Adaptivesubgradientmethodsforonlinelearningand tionsofACL.
stochasticoptimization. JournalofMachineLearning Irene Langkilde and Kevin Knight. 1998. Generation
Research. that exploits corpus-based statistical knowledge. In
Gu¨nes Erkan and Dragomir R. Radev. 2004. LexRank: ProceedingsofCOLING.
Graph-basedlexicalcentralityassalienceintextsum- ChenLi,YangLiu,FeiLiu,LinZhao,andFuliangWeng.
marization. Journal of Artificial Intelligence Re- 2014. Improving multi-documents summarization by
search. sentence compression based on expanded constituent
JeffreyFlanigan, SamThomson, JaimeCarbonell, Chris parsetree. InProceedingsofEMNLP.
Dyer, and Noah A. Smith. 2014. A discriminative Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
graph-basedparserfortheabstractmeaningrepresen- marizationviabudgetedmaximizationofsubmodular
tation. InProceedingsofACL. functions. InProceedingsofNAACL.
1085
Chin-Yew Lin. 2004. ROUGE: a package for auto- LucyVanderwende,MicheleBanko,,andArulMenezes.
maticevaluationofsummaries. InProceedingsofACL 2004. Event-centricsummarygeneration. InProceed-
WorkshoponTextSummarizationBranchesOut. ingsofDUC.
FeiLiuandYangLiu. 2013. Towardsabstractivespeech BinZhaoandEricP.Xing. 2014. Quasireal-timesum-
summarization: Exploring unsupervised and super- marization for consumer videos. In Proceedings of
vised approaches for spoken utterance compression. CVPR.
IEEE Transactions on Audio, Speech, and Language
Processing.
Ivana Ljubic´, Rene´ Weiskircher, Ulrich Pferschy, Gun-
narW.Klau,PetraMutzel,andMatteoFischetti. 2006.
AnAlgorithmicFrameworkfortheExactSolutionof
thePrize-CollectingSteinerTreeProblem. InMathe-
maticalProgamming,SeriesB.
AndreF.T.MartinsandNoahA.Smith. 2009. Summa-
rizationwithajointmodelforsentenceextractionand
compression. In Proceedings of the ACL Workshop
onIntegerLinearProgrammingforNaturalLanguage
Processing.
Andre F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tionsfordependencyparsing. InProceedingsofACL.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedingsofECIR.
RadaMihalceaandPaulTarau. 2004. TextRank: Bring-
ingorderintotext. InProceedingsofEMNLP.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating Sentence Com-
pression: Pitfalls and Suggested Remedies. In Pro-
ceedingsoftheWorkshoponMonolingualText-To-Text
Generation, MTTG ’11, pages 91–97, Stroudsburg,
PA,USA.AssociationforComputationalLinguistics.
AniNenkovaandKathleenMcKeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tionRetrieval.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings of
EMNLP.
Norman Sadeh, Alessandro Acquisti, Travis D. Breaux,
Lorrie Faith Cranor, Aleecia M. McDonald, Joel R.
Reidenberg, Noah A. Smith, Fei Liu, N. Cameron
Russell, Florian Schaub, and Shomir Wilson. 2013.
The usable privacy policy project. Technical Report,
CMU-ISR-13-119,CarnegieMellonUniversity.
Arie Segev. 1987. The Node-Weighted Steiner Tree
Problem. Networks,17(1):1–17.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-marginMarkovnetworks. InAdvancesinNeural
InformationProcessingSystems16.
KapilThadaniandKathleenMcKeown. 2013. Sentence
compression with joint structural inference. In Pro-
ceedingsofCoNLL.
1086

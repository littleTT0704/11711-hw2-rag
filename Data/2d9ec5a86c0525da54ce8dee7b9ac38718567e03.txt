Comparing Span Extraction Methods for Semantic Role Labeling
ZhisongZhang,EmmaStrubell,EduardHovy
LanguageTechnologiesInstitute,CarnegieMellonUniversity
zhisongz@cs.cmu.edu, strubell@cmu.edu, hovy@cmu.edu
Abstract
B-A0 I-A0 B-V B-A1 B-A3 I-A3 I-A3 O
(a)BIO-based: TV stations bought them for record prices .
In this work, we empirically compare span
extraction methods for the task of semantic A0 A1 A3
role labeling (SRL). While recent progress (b)Span-based: TV stations bought them for record prices .
incorporating pre-trained contextualized rep-
A0 A1 A3
resentations into neural encoders has greatly
improved SRL F1 performance on popular (c)Two-step: TV stations bought them for record prices .
benchmarks, the potential costs and benefits
of structured decoding in these models have Figure 1: Illustration of decoding methods explored
become less clear. With extensive experi- in this work. For the predicate â€œboughtâ€, we iden-
ments on PropBank SRL datasets, we find tifyargumentspansby: (a)BIO-basedsequencelabel-
that more structured decoding methods out- ing; (b) direct span-based extraction; (c) two-step ap-
performBIO-taggingwhenusingstatic(word proach: firstidentifyingheadwords,thenexpandingto
type) embeddings across all experimental set- fullspansbydecidingleftandrightboundaries.
tings. However, when used in conjunction
withpre-trainedcontextualizedwordrepresen-
tations, the benefits are diminished. We also Inthiswork,weperformanempiricalinvestiga-
experiment in cross-genre and cross-lingual tionofdifferentdecodingmethodsforspanextrac-
settingsandfindsimilartrends.Wefurtherper-
tion,asillustratedinFigure1. Themostcommon
form speed comparisons and provide analysis
strategycaststhetaskasasequencelabelingprob-
on the accuracy-efficiency trade-offs among
lemusingtheBIO-taggingscheme(ZhouandXu,
differentdecodingmethods.
2015; He et al., 2017; Tan et al., 2018; Strubell
1 Introduction et al., 2018; Shi and Lin, 2019). While this ap-
proach is simple, it does not directly model the
Semanticrolelabeling(SRL)isacorenaturallan-
argumentsatthespanlevel. Alternatively,thespan-
guageprocessing(NLP)taskthataimstoidentify
basedmethoddirectlybuildsrepresentationsforall
predicate-argumentstructuresintext(Gildeaand
possible1 spansandselectsamongthem(Heetal.,
Jurafsky, 2002; Palmer et al., 2010). Following
2018a;Ouchietal.,2018). Thoughthisapproachis
theneuralencoder-decoderparadigm,wecanview
straightforwardforexplicitlymodelingspan-level
an SRL model as combining an encoder, which
information,composingarepresentationforevery
buildshiddenrepresentationsfortheinputwords,
span can lead to higher computational cost. In-
withadecoder,whichextractstheargumentspans
spiredbydependency-basedSRL(Surdeanuetal.,
basedontheencodedrepresentations. Whilerecent
2008;HajicË‡ etal.,2009),athirdoptionfirstidenti-
SRLmodelsachievehighperformanceonpopular
fiesaheadwordthendecidesthespanboundaries.
benchmarks(ZhouandXu,2015;Heetal.,2017;
Thistwo-stepstrategyhasbeenexploredinprevi-
Tanetal.,2018;Strubelletal.,2018;ShiandLin,
ous work on information extraction (Peng et al.,
2019),mostimprovementscomefrombetterneu-
2015;Linetal.,2019;Zhangetal.,2020),andwe
ral encoders, such as the Transformer (Vaswani
applyitheretoSRL.Comparedwiththesequential
et al., 2017) and pre-trained contextualized word
BIO-tagger,thelattertwoapproachesmoredirectly
representations,suchasBERT(Devlinetal.,2019).
modeltheargumentspanstructures;wethusrefer
However,influenceonend-taskperformancedue
tothechoiceofdecoderhasbecomelessclear. 1Uptoafixedlength,decidedasahyperparameter.
tothemasmorestructured decoders. we have k possible argument roles in the output
Weperformcarefulcomparisonsofthesedecod- space,eachofthemwillhaveitsâ€œB-â€andâ€œI-â€tags.
ing methods upon the same encoding backbone, Togetherwiththeâ€œOâ€(NIL)tag,thetaggingspace
basedonadeepTransformerencoder. Wefirstex- hasadimensionof2k+1.
perimentinthestandardfully-supervisedsettings Furthermore,weconsidertheoptionofadopting
onEnglishPropBankdatasets(CoNLL-2005and a standard linear-chain conditional random field
CoNLL-2012). Theresultsshowthatmorestruc- (CRF;Laffertyetal.,2001)tomodelpairwisetag-
tured decoders, especially the two-step approach ging transitions. If adopting the CRF (BIO w/
withsyntacticguidance,consistentlyperformbet- CRF),wetrainthemodelwithsequence-levelnega-
ter than BIO-tagging when using static word em- tiveloglikelihoodandusetheViterbialgorithmfor
beddings. However, if including strong contex- inference. IfnotusingtheCRF(BIOw/oCRF),we
tualized BERT embeddings, the benefits of more simplyusetag-levelcrossentropyasthelearning
structured decoding are diminished and the sim- objectiveandperformargmaxgreedydecodingat
plestBIO-taggingmethodperformswellacrossdif- inferencetime,followingTanetal.(2018).
ferentexperimentalsettings. Erroranalysisshows
that contextualized embeddings help in deciding 2.2 Span-based
span boundaries. Furthermore, we explore cross- In the span-based method, we build neural repre-
genre and cross-lingual settings on the CoNLL- sentationsforallcandidatespansanddirectlyselect
2012 datasets, and find similar trends. Finally, andassignrolelabels(orNIL).FollowingHeetal.
we perform speed comparisons and analyze the (2018a), for a span a, we compose its represen-
accuracy-efficiencytrade-offsamongdifferentde- tation from start and end points, soft head-word
codingmethods. vectorsandspanwidthfeaturesbyconcatenation:
2 Model g(a) = [h start(a),h end(a),soft(a),width(a)]
Foragivenpredicate,2SRLaimstoextractallargu- Here, soft(a) denotes a soft-head representation
obtainedfromanattentionmechanism:
mentspansandassignthemrolelabels. Tomodel
this task, we follow the neural encoder-decoder (cid:88)
soft(a) = att(i,a)h
i
paradigm: theencoderproduceshiddenrepresenta-
start(a)â‰¤iâ‰¤end(a)
tionsfortheinputwords,uponwhichthedecoder
wT h
decides the structured outputs. All our models att(i,a) = att i
(cid:80) wT h
adoptthesameencodingarchitecture: adeepTrans- start(a)â‰¤i(cid:48)â‰¤end(a) att i(cid:48)
formerencoder(Vaswanietal.,2017),whichhas and width(a) denotes a width embedding corre-
been shown effective for SRL (Tan et al., 2018; spondingtothespansize(width).
Strubelletal.,2018). Foragiveninputsequence
All valid candidate spans are first assigned an
ofwords{w ,...,w },weobtaintheircontextu-
1 n unlabeledscore,usinganMLPscorer. Thisunary
alized representations {h ,...,h } from the en-
1 n scoreisthenusedasthecriterionforbeampruning
coder. Uponthese,westackdifferentdecodersto
toreducethecomputationalcostsoffulllabeling.
extracttheargumentspanscorrespondingtodiffer-
Sinceeachpredicatewillnothavetoomanyargu-
entextraction strategies, whichwillbe described
ments (most have less than 5), we adopt a fixed
inthefollowing.
beamsizeof10. Wealsolimitthemaximumwidth
ofcandidatespansto30,whichcoversaround99%
2.1 BIO-based
of the cases. Surviving candidates are further as-
Sinceargumentspansdonotoverlapinthedatasets signedlabelscoreswithanotherMLPscorer,with
we explore, the BIO-tagging scheme (Ramshaw whichwedecideoutputarguments.
andMarcus,1999)canbeutilizedtoextractthem,
castingSRLasasequencelabelingproblem. 2.3 Two-step
Foreachword,wefeeditsrepresentationhtoa
Inthisapproach,wedecomposetheprobleminto
multi-layerperceptron(MLP)basedscorer,which
twosteps: head-selectionandboundary-decision.
assignsthescoresoftheBIOtags. Assumingthat
In the first step, each individual word is directly
scored for argument labels (or NIL). We again
2Inthiswork,wefocusonargumentextractionandassume
givenpredicates. adopt an MLP classifier to obtain the probability
that a word can be the head of an argument with putsequence: {h(cid:48),Â·Â·Â· ,h(cid:48) }. Wefurtherintroduce
1 n
labelr(rcanbeNIL).Thenon-NILlabeledwords twolinearscorerstoassignthestartandendscores
are selected as the head words of the arguments. foreachword,whicharefurthernormalizedacross
Sincetheannotationsusuallydonotcontainhead the input sequence. For training, the objective is
wordsfortheargumentspans,wefurtherconsider minimizingthesumofnegativelog-likelihoodsof
twostrategiestoprovidesupervisionfortraining: pickingthecorrectstartandendpositions. When
decoding, we select the maximum scoring span
HeadSyntax A straightforward method is whoseboundariessandesatisfys â‰¤ e.
to adopt guidance from syntax. Following We observe that at inference time, sometimes
dependency-style SRL (Surdeanu et al., 2008; different head words may expand to overlapping
HajicË‡ et al., 2009), we use syntactic dependency spans,whichdonotappearinthedatasetsweex-
parse trees and select the highest word (the one plore. To deal with this, we adopt a greedy post-
thatisclosesttotheroot)inthespanasthehead. processingproceduretoremoveoverlappingargu-
In training, we only assign the argument role to mentspans: iteratingthroughallargumentspans
thesyntacticheadword,andallotherwordsinthe rankedbymodelscoreandonlykeepingtheones
spangetalabelofNIL. thatdonotoverlapwithprevioussurvivingones.
HeadAuto Inthisstrategy,allwordsinanargu-
3 Experiments
mentspancanbeconsideredasthepotentialhead
word. WeadoptthebaglossfromLinetal.(2019) 3.1 Settings
to train the model to automatically identify head
Data ThemodelsareevaluatedonstandardProp-
words. Specifically, for a word w inside an ar-
i
BankdatasetsfromtheCoNLL-2005sharedtask
gument span a which has the role r, the loss is
(Carreras and Ma`rquez, 2005) and the CoNLL-
computedas:
2012 subset of OntoNotes 5.0 (Pradhan et al.,
Loss(w ) = Î´ Â·[âˆ’logp(r|h )] 2013). Table 1 lists the relevant statistics. For
i i i
+(1âˆ’Î´ )Â·[âˆ’logp(NIL|h )] CoNLL-2005, we follow the splits from the
i i
CoNLL-2005sharedtask.3 FortheEnglishpartof
p(r|h )
i
Î´ i = CoNLL-2012,weadoptthedatafromPradhanetal.
max p(r|h )
start(a)â‰¤jâ‰¤end(a) j (2013)4 but follow the splits of the CoNLL-2012
Here, words that are more indicative for the ar- sharedtask.5 FortheChinesepartofCoNLL-2012,
gument will be assigned higher probabilities to wedirectlyutilizethoseprovidedbytheCoNLL-
theargumentrole. Thiswillgivethemlargerloss 2012 shared task. For evaluation, we adopt the
weights(Î´)andthusfurtherencouragethemtobe standardevaluationscriptofsrl-eval.pl.6 For
theheads. Inthisway,theheadwordsaredecided theâ€œHeadSyntaxâ€methodthatrequiresdependency
automaticallybythemodel. trees,weconverttheoriginalconstituenciestoUni-
In the second step, we determine span bound- versalDependencies(Nivreetal.,2020)usingStan-
ariesfortheseheadwords. Hereweadoptthespan fordCoreNLP(Manningetal.,2014)version4.1.0.
selectionmethodfromextractivequestionanswer- Noticethatweonlyneedsyntacticinformationto
ing (Wang and Jiang, 2016; Devlin et al., 2019) be provided during training, since the model pre-
using two classifiers to decide the start and end dictsheadwordsitselfattesttime.
words([s,e])ofaspan:
Input Features and Encoder For fair com-
p(s,e) = p (s)Â·p (e)
start end parison, we adopt the same input features, deep
p (s) =
expscore start(h(cid:48) s) Transformer-basedencodersandtrainingschemes
start (cid:80) expscore (h(cid:48)) across all experiments. We consider two types
i start i
expscore (h(cid:48)) of word features: static word embeddings and
p (e) = end e
end (cid:80) expscore (h(cid:48))
i end i 3https://www.cs.upc.edu/Ëœsrlconll/
Here,wefirstaddindicatorembeddingstothehead 4https://cemantix.org/data/ontonotes.
html
wordâ€™s encoder representations to mark its posi-
5https://conll.cemantix.org/2012/
tions, and then stack one self-attention layer to
6https://www.cs.upc.edu/Ëœsrlconll/soft.
obtainhead-word-awarerepresentationsforthein- html
CoNLL2005 CoNLL2012(English) CoNLL2012(Chinese)
Train Dev Test Brown Train Dev Test Train Dev Test
Sent. 39.8k 1.3k 2.4k 0.4k 75.2k 9.6k 9.5k 36.5k 6.1k 4.5k
Pred. 90.8k 3.2k 5.3k 0.8k 188.9k 23.9k 24.5k 117.1k 16.6k 15.0k
Arg. 333.7k 11.7k 19.6k 3.0k 622.5k 78.1k 80.2k 365.3k 51.0k 46.7k
Table1: Statisticsofthedatasets: Numberofsentences(Sent.),predicates(Pred.) andarguments(Arg.).
pre-trained contextualized embeddings7 from Model WSJ Brown OntoNotes
BERT . In the English experiments, we adopt
base Heetal.(2018a) 87.4 80.4 85.5
fastText8 embeddings (Mikolov et al., 2018) Ouchietal.(2018) 87.6 78.7 86.2
ShiandLin(2019) 88.8 82.0 86.5
and frozen features from bert-base-cased.
Ours(BIOw/CRF) 87.9 82.1 86.6
In the cross-lingual experiments, we only
utilize multi-lingual BERT features from Table2: ComparisonsofF1scoreswithpreviouswork
bert-base-multilingual-cased. Be- inthefully-supervisedsettings(withsinglemodel).
forefeedingtheword-levelfeaturestotheencoder,
we concatenate them and apply a linear layer implementation9. All the models are trained and
to project them to the encoding dimension. We
evaluatedononeTITAN-RTXGPU,andtraining
furtheraddindicatorembeddingstoletthemodel
onemodeltakesaround1dayinourenvironment.
be aware of the positions of the predicates. For
bothcasesofstaticembeddingandBERTfeatures, 3.2 Fully-supervisedExperiments
we adopt a 10-layer Transformer module as the
Wefirstexperimentinthefully-supervisedsettings
encoder. The head number, model dimension
onEnglishdata. Table2liststhecomparisonsof
and feed-forward dimension are set to 8, 512
ourtestresults(BIOw/CRFusingBERTfeatures)
and 1024, respectively. In addition, we adopt
topreviouswork. Generallyourmodelcanobtain
relativepositionalencodingsfortheTransformer
comparable results, which verifies the quality of
(Shawetal.,2018)sincewefoundslightlybetter
ourimplementation.
performanceinpreliminaryexperiments.
Tables 3 and 4 list our main comparisons on
thedevelopmentandtestsets. Theoveralltrends
Training WeusetheAdamoptimizer(Kingma
are very similar. For BIO-tagging, incorporat-
and Ba, 2014) for training. The learning rate is
ing a structured CRF layer is generally helpful,
linearlyincreasedtowards2e-4withinthefirst8k
which can improve the F1 scores by around 0.5
stepsaswarmup. Afterthis,wedecaythelearning
points. WhennotusingBERTfeatures,morestruc-
rateby0.75eachtimetheperformanceonthede-
tureddecodersgenerallyperformbetterthanBIO-
velopmentsetdoesnotincreasefor10checkpoints.
tagging. Withtheheadwordoraclesfromthesyn-
Wetrainthemodelforamaximumof150ksteps
tax trees, â€œHeadSyntaxâ€ performs the best over-
anddovalidationevery1kstepstoselectthebest
all. This agrees with Strubell et al. (2018) and
model. Onemodelcontainsaround40Mparame-
Swayamdipta et al. (2018), showing the helpful-
ters(excludingBERT).Foreachupdate,thebatch
ness of syntactic information for SRL. However,
sizeisaround4096tokens. Weapplydropoutrates
whenutilizingBERTfeatures,thebenefitsofmore
of0.2tothehiddenlayers. Formodelsusingstatic
structureddecodersarediminishedandthesimple
embeddings,wefurtherreplaceinputwordsbya
BIO-taggerrobustlyperformswell. Itseemsthat
special UNK token with a probability of 0.5 if it
withapowerfulencoder,thechoiceofthedecoder
appears less than 3 times in the training set. At
playsasmallerroleforfinalperformance.
test time, a word is represented by UNK if it is
Tofurtherinvestigatethisphenomenon,weper-
not found in the collection of static word embed-
formerroranalysisonthedevelopmentoutputsof
dings. All the experiments are run with our own
â€œBIO(w/CRF)â€andâ€œHeadSyntax,â€whicharethe
two that perform the best overall. We group the
7Weconcatenatelayer7,8and9ofBERThiddenrepre-
errors into four categories: â€œBoundaryâ€ denotes
sentations.Forwordsthataresplitintosub-tokens,weutilize
therepresentationsofthefirstsub-token. thatthepredictedheadwordsandrolelabelsmatch
8https://fasttext.cc/docs/en/
english-vectors.html 9https://github.com/zzsfornlp/zmsp/
CoNLL2005In-domain(WSJ) CoNLL2012(OntoNotes)
P R F1 P R F1
WithoutBERT
BIO(w/oCRF) 83.11 83.89 83.49 81.43 82.75 82.09
Â±0.20 Â±0.22
BIO(w/CRF) 83.66 84.27 83.96 82.41 83.77 83.09
Â±0.26 Â±0.11
Span 84.60 83.57 84.08 82.89 83.04 82.96
Â±0.23 Â±0.12
HeadSyntax 84.81 84.48 84.65 83.12 83.42 83.27
Â±0.18 Â±0.18
HeadAuto 84.52 84.38 84.45 82.50 83.16 82.83
Â±0.22 Â±0.15
WithBERT
BIO(w/oCRF) 86.47 87.50 86.98 85.22 86.94 86.08
Â±0.12 Â±0.15
BIO(w/CRF) 86.78 87.84 87.31 85.66 87.19 86.42
Â±0.13 Â±0.12
Span 86.94 86.76 86.85 85.83 86.37 86.10
Â±0.16 Â±0.11
HeadSyntax 87.35 87.48 87.41 86.04 86.79 86.41
Â±0.14 Â±0.12
HeadAuto 87.10 87.67 87.38 85.80 86.75 86.27
Â±0.22 Â±0.15
Table3: Developmentresultsforthefully-supervisedexperiments. Allthenumbersareaveragedover5runswith
differentrandomseeds,standarddeviationsofF1scoresarealsoreported.
CoNLL2005In-domain(WSJ) Out-of-domain(Brown) CoNLL2012(OntoNotes)
P R F1 P R F1 P R F1
WithoutBERT
BIO(w/oCRF) 84.42 84.94 84.68 73.56 73.03 73.29 81.74 82.98 82.35
Â±0.25 Â±0.43 Â±0.24
BIO(w/CRF) 85.04 85.35 85.20 74.25 73.92 74.08 82.79 84.11 83.44
Â±0.12 Â±0.31 Â±0.21
Span 85.68 84.62 85.14 75.88 74.23 75.05 83.42 83.49 83.46
Â±0.32 Â±0.42 Â±0.15
HeadSyntax 85.84 85.38 85.61 75.92 74.74 75.33 83.55 83.82 83.68
Â±0.11 Â±0.58 Â±0.11
HeadAuto 85.30 85.17 85.23 74.98 73.85 74.41 83.09 83.71 83.40
Â±0.14 Â±0.50 Â±0.09
WithBERT
BIO(w/oCRF) 87.21 87.95 87.58 81.26 81.79 81.52 85.33 86.97 86.14
Â±0.28 Â±0.23 Â±0.10
BIO(w/CRF) 87.54 88.32 87.93 81.91 82.37 82.14 85.93 87.32 86.62
Â±0.16 Â±0.20 Â±0.14
Span 87.75 87.33 87.54 81.87 81.60 81.73 85.97 86.26 86.12
Â±0.14 Â±0.77 Â±0.09
HeadSyntax 87.76 87.96 87.86 82.10 81.60 81.85 86.17 86.77 86.47
Â±0.08 Â±0.90 Â±0.10
HeadAuto 87.70 88.15 87.93 81.52 81.36 81.44 86.00 86.84 86.42
Â±0.12 Â±0.37 Â±0.09
Table4: Testresultsofthefully-supervisedexperiments. Alltheresultsareaveragedoverfiverunswithdifferent
randomseeds,standarddeviationsoftheF1scoresarealsoreported.
ments. The results are shown in Figure 2. When
 + H D G 6 \ Q W D [  Z  R  E H U W  + H D G 6 \ Q W D [  Z   E H U W
 % , 2  & 5 )  Z  R  E H U W  % , 2  & 5 )  Z   E H U W notusingBERTfeatures,themainadvantagesof
            â€œHeadSyntaxâ€overâ€œBIOâ€areontheâ€œBoundaryâ€
               andâ€œAttachmentâ€errors,wheretheformermakes
          
                  11% fewer â€œBoundaryâ€ and 17% fewer â€œAttach-
mentâ€errors. Noticethatthesetwotypesoferrors
   
   
      
      
arecloselyrelatedtosyntax,andtheyaremainly
caused by incorrect phrase boundary predictions.
 
 % R X Q G D U \  / D E H O  $ W W D F K P H Q W  2 W K H U V Inthisway,itseemsnaturalthatincorporatingsyn-
 ( U U R U  7 \ S H V
tactic information with head words can be help-
ful in this scenario. Nevertheless, when utilizing
Figure 2: Error breakdown for â€œBIOâ€ and â€œHeadSyn-
taxâ€ontheCoNLL-2005developmentset. BERTfeatures,theseadvantagesarereducedtoa
negligiblelevel. ThisindicatesthatBERTmaypro-
videsufficientinformationoverlappingwithsyntax
the gold ones but the span boundaries are incor-
tohelponboundarydecisions.
rect;â€œLabelâ€denotesthatthepredictedspansare
correctbuttherolelabelsarewrong;â€œAttachmentâ€
3.3 Cross-genreExperiments
denotestheerrorscausedbyincorrectphraseattach-
ments,whileâ€œOthersâ€denotestheremainingerrors, We further explore English cross-genre settings.
which are other missing and over-predicted argu- We utilize English CoNLL-2012 subsets of
 W Q X R &
nwâˆ— bc bn mz pt tc wb Avg.
WithoutBERT
BIO(w/oCRF) 77.51 59.91 73.28 71.15 81.03 67.90 72.36 71.88
Â±0.17 Â±0.31 Â±0.62 Â±0.37 Â±0.31 Â±0.37 Â±0.07
BIO(w/CRF) 78.42 60.15 73.97 71.37 81.51 68.72 72.54 72.38
Â±0.39 Â±0.40 Â±0.15 Â±0.13 Â±0.36 Â±0.34 Â±0.41
Span 79.08 62.74 74.80 72.77 82.42 68.93 74.17 73.56
Â±0.16 Â±0.49 Â±0.30 Â±0.36 Â±0.41 Â±0.12 Â±0.15
HeadSyntax 79.54 62.81 75.06 73.17 82.10 68.74 74.82 73.75
Â±0.37 Â±0.58 Â±0.25 Â±0.32 Â±0.30 Â±0.54 Â±0.19
HeadAuto 79.04 61.97 74.09 72.56 81.80 69.25 73.96 73.24
Â±0.22 Â±0.30 Â±0.25 Â±0.40 Â±0.40 Â±0.39 Â±0.19
WithBERT
BIO(w/oCRF) 83.55 73.37 80.02 78.45 87.63 74.89 79.49 79.63
Â±0.24 Â±0.51 Â±0.19 Â±0.34 Â±0.19 Â±0.41 Â±0.29
BIO(w/CRF) 83.73 75.24 80.64 78.75 87.94 75.38 79.66 80.19
Â±0.28 Â±0.89 Â±0.15 Â±0.56 Â±0.42 Â±0.42 Â±0.39
Span 83.41 74.22 80.85 78.69 87.44 75.05 79.44 79.87
Â±0.18 Â±0.89 Â±0.29 Â±0.39 Â±0.16 Â±0.36 Â±0.33
HeadSyntax 83.96 75.98 80.88 79.36 87.40 75.12 80.05 80.39
Â±0.34 Â±0.94 Â±0.17 Â±0.37 Â±0.25 Â±0.41 Â±0.20
HeadAuto 83.76 74.98 80.69 79.01 87.33 75.66 79.98 80.20
Â±0.28 Â±0.77 Â±0.21 Â±0.27 Â±0.36 Â±0.54 Â±0.10
Table 5: F1 scores of the (English) cross-genre experiments (averaged over 5 runs with different random seeds).
â€œ*â€denotesthatmodelsaretrainedontheâ€œnwâˆ—â€portion. â€œAvg.â€ denotesmacroaverageoverallgenres.
bc bn mz  S W
      % , 2   Z   & 5 ) 
BIO(w/oCRF) 71.19 Â±0.61 77.56 Â±0.68 76.63 Â±0.51  + H D G 6 \ Q W D [
BIO(w/CRF) 72.11 76.28 75.87     
Â±0.98 Â±0.61 Â±0.69
Span 73.30 79.90 77.90  Q Z
Â±1.07 Â±0.58 Â±0.59
HeadSyntax 75.23 79.95 78.69     
Â±1.00 Â±0.49 Â±0.41
HeadAuto 73.60 Â±0.49 78.97 Â±0.53 77.60 Â±0.41  E Q
      Z E
 P ]
Table 6: F1 scores of the (English) cross-genre exper-     
iments (averaged over 5 runs with different random
 W F  E F
seeds) on specific genres without excluding auxiliary     
                                       
predicates(withBERT).  % ( 5 7  & R V L Q H  6 L P L O D U L W \
Figure3:F1resultsversusgenresimilaritiesaccording
OntoNotes and split the corpus according to the toBERTrepresentations.
genres. Therearesevengenres,includingbroadcast
conversation(bc),broadcastnews(bn),newswire
ples,themorestructureddecodersperformbetter
(nw),magazine(mz),pivot(Bible)(pt),telephone
than BIO-tagging even with BERT, as shown in
conversation (tc) and web (wb) text. The mod-
Table 6. A possible explanation is that the more
els are trained on the newswire (nw) portion and
structureddecodersusuallyseemorenegativeex-
directlyevaluatedonportionsofallthegenres. Ta-
amplesduringtrainingandmightbemoreconserva-
ble 5 shows the test results. The overall trends
tivewhenpredictingargumentsfortheseauxiliary
aresimilartothoseinthefully-supervisedsetting.
verbs, which do not have any arguments. On the
Without BERT, more span-aware structured de-
contrary, the BIO-tagger tends to over-predict ar-
coders perform better by more than for 1 point
guments in these cases, leading to worse results.
comparedtoBIO-tagging. AfterincludingBERT
Nevertheless,thisphenomenonisonlytheresultof
features, the gaps decrease. Nevertheless, more
anannotationinconsistencyinthedatasetandwe
structureddecoderscanstillperformcompetitively.
thusexcludetheseauxiliaryverbsfromevaluation
Notethatinthissetting,weperformevaluations
inthissetting.
with a correction to an annotation inconsistency
thatoriginallyfavoredmorestructured(direct)de- We further compare cross-genre results with
coders. We find that there are inconsistent anno- genre (domain) similarities. Following Aharoni
tationsforthepredicatesofauxiliaryverbsacross andGoldberg(2020),weobtainsimilarityscores
some genres, we thus exclude them10 for evalua- fromtargetgenrestothesourcegenre(nw)bycal-
tion. Inthegenresofâ€œbcâ€, â€œbnâ€andâ€œmzâ€, there culatingcosinesimilarityofthecentroidsofBERT
aremanymoreauxiliaryverbsannotatedthanthose representations. Specifically, we first compute
inâ€œnwâ€. Interestingly,ifnotexcludingtheseexam- sentence-levelrepresentationsbyaveragepooling
thefinalhiddenvectorswithavanillaBERT,then
10Weexclude[â€œbe.03â€,â€œbecome.03â€,â€œdo.01â€,â€œhave.01â€]. thegenre-levelrepresentationsareobtainedbyfur-
Dev Test Decoding WithoutBERT WithBERT
BIO(w/oCRF) 56.73 56.18 BIO(w/oCRF) 709.8 412.3
Â±0.63 Â±0.61 Â±10.6 Â±4.6
BIO(w/CRF) 56.86 56.47 BIO(w/CRF) 497.0 335.1
Â±1.05 Â±0.95 Â±4.5 Â±4.3
Span 56.61 55.97 Span 355.8 261.3
Â±0.51 Â±0.39 Â±5.4 Â±3.7
HeadSyntax 57.05 56.48 HeadSyntax 561.6 372.8
Â±0.36 Â±0.34 Â±5.1 Â±4.5
HeadAuto 57.05 56.51 HeadAuto 454.9 311.0
Â±0.59 Â±0.66 Â±7.9 Â±5.8
Table 7: Unlabeled F1 scores of Englishâ†’Chinese Table9:Speedcomparisonsofdecodingmethods(eval-
zero-shotcross-lingualexperiments(averagedoverfive uated by number of sequences per second, averaged
runswithdifferentrandomseeds). over5runs,ononeTITAN-RTXGPU).
Gold [ä½ ][åœ¨çº½çº¦æ—¶æŠ¥ä¸Š]å†™äº†[ä¸€ç¯‡æ–‡ç« ]
 % , 2   Z  R  & 5 ) 
Literally [you][atNewYorkTimes]wrote[anarticle]   
 % , 2   Z   & 5 ) 
Predicted [ä½ ][åœ¨çº½çº¦æ—¶æŠ¥ä¸Š]å†™[äº†ä¸€ç¯‡æ–‡ç« ]  6 S D Q
  Z   % ( 5 7
    + H D G 6 \ Q W D [
 + H D G $ X W R
Table8: Atypicalerrorofcross-lingualsystems. Here,  Z  R  % ( 5 7 
thepredicateistheunderlinedâ€œå†™â€(wrote)andthegold   
and predicted arguments are presented in [the brack-
ets]. Thecross-lingualmodelswronglyincludetheex-   
traauxiliarywordâ€œäº†â€inthelastargument.
                   
 6 S H H G   V H T X H Q F H V  V H F R Q G 
ther averaging all sentence-level ones in the cor-
Figure 4: Comparing speed vs. F1 with different de-
pus. Weshowtheresultsofâ€œBIO(w/CRF)â€and codingmethods(onCoNLL05developmentset).
â€œHeadSyntaxâ€inFigure3. Generally,F1scoreson
target genres have a weak correlation with genre
tionthatCRFiscapturingisâ€Iâ€afterâ€Bâ€,which
similaritiestothesource(Pearsonâ€™scorrelationis
does not provide too much enhancement. Inter-
0.45). The outlier â€œptâ€ is a special case (biblical
estingly,inourpreliminaryexperiments,wealso
text)whichmainlycontainssimpleinstances.
tried labeled training, and found that the CRF is
3.4 Cross-lingualExperiments actuallyharmful,sincethedistributionsofthetag
transitionsmightbedifferentacrosslanguages.
Wefurtherexploreasimplezero-shotcross-lingual
Wefurtherinvestigatethesystemsâ€™outputsand
setting. We still take the CoNLL-2012 subset of
find similar error patterns. Table 8 lists a typical
theOntonotescorpus. Themodelsaretrainedon
example,whereinChinesetheauxiliarywordâ€œäº†â€
the English sets, and then directly applied to the
(whichdenotesperfectiveaspect11)isincorrectly
Chinesesets. Thistimeweexcludewordembed-
included in the argument. This error is not sur-
dingsandonlyuserepresentationsfrommultilin-
prising if considering that in the English training
gualBERTastheinputfeatures,whichhasshown
corpus, the predicate verbs usually have directly-
to be effective for cross-lingual transfer (Wu and
following arguments. All extraction methods ex-
Dredze, 2019). Since the Chinese and English
ploredinthisworkareunlikelytofixsucherrors
PropBanksusedifferentframes,thelabeledresults
withoutlanguage-specificknowledge.
mightnotbedirectlycomparable. Wethusperform
unlabeledtrainingandevaluateunlabeledargument
3.5 SpeedComparisons
F1 scores, which reveal how well the models ex-
Finally we compare the decoding speed of dif-
tract argument spans. We simply collapse all the
ferent extraction methods. Results are shown in
rolelabelsintoonespecialâ€œIsArgâ€label.
Table 9 and we further compare them against F1
The results are listed in Table 7. The trends
scoresinFigure4. GreedyBIO-tagging(w/oCRF)
are still similar to the previous monolingual ex-
obtains the highest speed. However, this comes
periments with BERT, different decoders obtain
withadropofaround0.5F1pointswithoutBERT
similar results, especially considering the devia-
and 0.3 F1 points with BERT. Although the two-
tionsofmultipleruns. Inthissetting,theCRFdoes
stepapproachesrequiretwodecodingsteps, they
not help as much as in the case of monolingual
experiments. The main reason might be that we
11https://universaldependencies.org/zh/
aretrainingunlabeledsystems,andthemaintransi- dep/aux_.html
  )
are still efficient thanks to the simplicity of both (Marcheggianietal.,2017;Caietal.,2018). Never-
steps. When trained with syntactic information, theless,withrecentneuralmodels,syntaxinforma-
thismodelisthesecondbestintermsofdecoding tionhasstillbeenfoundhelpfulforSRLinvarious
speed. On the other hand, even with beam prun- ways,includingmulti-tasklearning(Swayamdipta
ing, thespan-baseddecoderstillneedstoscorea etal.,2018;Strubelletal.,2018),argumentprun-
number of span candidates quadratic in the input ing (He et al., 2018b), and tree-based modeling
sequencelength,makingitlessefficientcompared (Marcheggiani and Titov, 2017; Li et al., 2018;
tootherdecoders. MarcheggianiandTitov,2020). Inthiswork,our
â€œHeadSyntaxâ€decoderincorporatessyntaxinapar-
4 RelatedWork
tial way, utilizing dependency trees to decide the
head words in training. This method indeed per-
ArgumentExtraction Beforetheincorporation
formsthebestoverallifonlyadoptingstaticword
ofend-to-endneuralmodels,traditionalSRLsys-
embeddings. However,theincorporationofBERT
tems usually depend on input constituency trees
featuresdiminishestheadvantages. Thisindicates
to obtain argument candidates (Xue and Palmer,
that BERT may already cover much of the syn-
2004;Ma`rquezetal.,2008). Althoughstraightfor-
tactic(surface)featuresoftheinputsentences,as
ward,thismaysufferfromerrorpropagationfrom
suggestedbyrecentworksonBERTinterpretation
syntaxparsers. Recentneuralsystemsutilizeend-
(Goldberg,2019;HewittandManning,2019;Ten-
to-end models to solve the task. Casting SRL as
neyetal.,2019;Clarketal.,2019).
BIO-basedsequencelabelingproblemisthemost
commondecodingschemeandcanobtainimpres-
Cross-lingualSRL Therehasalsobeenincreas-
sive results (Zhou and Xu, 2015; He et al., 2017;
inginterestincross-lingualtransferforSRL,where
Tanetal.,2018;Strubelletal.,2018;ShiandLin,
data transfer and model transfer are the main ap-
2019). Ontheotherhand,span-basedmethods(He
proaches. Datatransferusuallydependsontrans-
etal.,2018a;Ouchietal.,2018)directlyselectand
lationandannotationprojectiontoobtaintraining
labelamongargumentspancandidates. Thisisac-
resources for target languages (PadoÂ´ and Lapata,
tuallysimilartothetraditionalapproaches,though
2009;Akbiketal.,2015;Aminianetal.,2019;Fei
theargumentcandidatesareobtainedbythemodel
etal.,2020a;DazaandFrank,2020). Ontheother
ratherthanfrominputsyntaxtrees. Inadditionto
hand,modeltransfertechniquesdirectlyreusean
span-based SRL, the focus of this work, there is
SRL model trained on source languages to trans-
anothercategoryofdependency-styleSRL,which
fer to target languages (Kozhevnikov and Titov,
only requires the extraction of head words of ar-
2013;Feietal.,2020b),basedoncommonrepre-
gumentspans(Surdeanuetal.,2008;HajicË‡ etal.,
sentations. Inparticular,therecentdevelopmentof
2009). Inspired by this, for span-based SRL, we
multilingualneuralrepresentations,suchasmulti-
can extract argument head words as the first step
lingualBERT,hasbeenshowntobeeffectivefor
andthenexpandtothefullspansinasecondstep.
cross-lingualtransfer(WuandDredze,2019;Pires
Thisideahasalsobeenappliedininformationex-
et al., 2019). In this work, we explore a simple
traction,suchascoreferenceresolution(Pengetal.,
zero-shotunlabeledsettingforcross-lingualSRL.
2015),entitydetection(Linetal.,2019)andevent
Weleavemoreexplorationsonthistofuturework.
argumentextraction(Zhangetal.,2020). Another
interestingdirectionisconsideringthestructured 5 Conclusion
constraintsofthearguments,includingworksonin-
Inthiswork,weempiricallycompareseveralspan
tegerlinearprogramming(Punyakanoketal.,2004,
extraction methods for SRL. Extensive results
2008), dynamic programming (TaÂ¨ckstroÂ¨m et al.,
showthatinfullysupervisedsettings,simpleBIO-
2015)andstructure-awaretuning(Lietal.,2020).
tagging is a robustly good option when utilizing
Syntax and SRL There has been discussion of BERT features. Similar trends are also found in
therelationbetweensyntaxandSRL(Gildeaand cross-genre and cross-lingual settings. We also
Palmer, 2002; Punyakanok et al., 2008), consid- analyzetheaccuracy-efficiencytrade-offsfordif-
ering the close connections between these two ferentdecoders;althoughmethodologicallymore
tasks. Thoughsyntaxtreesareusuallytheinputs complex,two-stepapproachesarestillefficientin
totraditionalSRLsystems,somerecentworksfind decoding. Future work could explore other NLP
thatsyntax-agnosticneuralmodelsalsoworkwell tasksthatrequireextractingtextualspans.
References Hao Fei, Meishan Zhang, and Donghong Ji. 2020a.
Cross-lingual semantic role labeling with high-
RoeeAharoniandYoavGoldberg.2020. Unsupervised
quality translated training corpus. In Proceedings
domain clusters in pretrained language models. In
of the 58th Annual Meeting of the Association for
Proceedingsofthe58thAnnualMeetingoftheAsso-
Computational Linguistics, pages 7014â€“7026, On-
ciation for Computational Linguistics, pages 7747â€“
line.AssociationforComputationalLinguistics.
7763, Online. Association for Computational Lin-
guistics.
Hao Fei, Meishan Zhang, Fei Li, and Donghong Ji.
2020b. Cross-lingual semantic role labeling with
AlanAkbik,LauraChiticariu,MarinaDanilevsky,Yun-
model transfer. IEEE/ACM Transactions on Audio,
yaoLi,ShivakumarVaithyanathan,andHuaiyuZhu.
Speech,andLanguageProcessing,28:2427â€“2437.
2015. GeneratinghighqualitypropositionBanksfor
multilingualsemanticrolelabeling. InProceedings
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
of the 53rd Annual Meeting of the Association for
labeling of semantic roles. Computational Linguis-
ComputationalLinguisticsandthe7thInternational
tics,28(3):245â€“288.
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 397â€“407, Beijing, DanielGildeaandMarthaPalmer.2002. Thenecessity
China.AssociationforComputationalLinguistics. of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Meeting of the As-
Maryam Aminian, Mohammad Sadegh Rasooli, and
sociationforComputationalLinguistics,pages239â€“
Mona Diab. 2019. Cross-lingual transfer of seman-
246, Philadelphia, Pennsylvania, USA. Association
tic roles: From raw text to semantic roles. In Pro-
forComputationalLinguistics.
ceedings of the 13th International Conference on
ComputationalSemantics-LongPapers,pages200â€“
YoavGoldberg.2019. Assessingbertâ€™ssyntacticabili-
210, Gothenburg, Sweden. Association for Compu-
ties. arXivpreprintarXiv:1901.05287.
tationalLinguistics.
Jan HajicË‡, Massimiliano Ciaramita, Richard Johans-
JiaxunCai,ShexiaHe,ZuchaoLi,andHaiZhao.2018.
son,DaisukeKawahara,MariaAnto`niaMartÂ´Ä±,LluÂ´Ä±s
A full end-to-end semantic role labeler, syntactic-
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
agnostic over syntactic-aware? In Proceedings of
PadoÂ´,JanSË‡teË‡paÂ´nek,PavelStranË‡aÂ´k,MihaiSurdeanu,
the27thInternationalConferenceonComputational
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
Linguistics, pages 2753â€“2765, Santa Fe, New Mex-
2009 shared task: Syntactic and semantic depen-
ico, USA. Association for Computational Linguis-
dencies in multiple languages. In Proceedings of
tics.
the Thirteenth Conference on Computational Nat-
Xavier Carreras and LluÂ´Ä±s Ma`rquez. 2005. Introduc- ural Language Learning (CoNLL 2009): Shared
tion to the CoNLL-2005 shared task: Semantic Task, pages 1â€“18, Boulder, Colorado. Association
role labeling. In Proceedings of the Ninth Confer- forComputationalLinguistics.
enceonComputationalNaturalLanguageLearning
LuhengHe,KentonLee,OmerLevy,andLukeZettle-
(CoNLL-2005), pages 152â€“164, Ann Arbor, Michi-
moyer. 2018a. Jointly predicting predicates and ar-
gan.AssociationforComputationalLinguistics.
guments in neural semantic role labeling. In Pro-
Kevin Clark, Urvashi Khandelwal, Omer Levy, and ceedingsofthe56thAnnualMeetingoftheAssocia-
Christopher D. Manning. 2019. What does BERT tionforComputationalLinguistics(Volume2:Short
look at? an analysis of BERTâ€™s attention. In Pro- Papers), pages 364â€“369, Melbourne, Australia. As-
ceedings of the 2019 ACL Workshop BlackboxNLP: sociationforComputationalLinguistics.
Analyzing and Interpreting Neural Networks for
NLP, pages 276â€“286, Florence, Italy. Association LuhengHe,KentonLee,MikeLewis,andLukeZettle-
forComputationalLinguistics. moyer. 2017. Deep semantic role labeling: What
works and whatâ€™s next. In Proceedings of the 55th
Angel Daza and Anette Frank. 2020. X-SRL: A par- Annual Meeting of the Association for Computa-
allelcross-lingualsemanticrolelabelingdataset. In tional Linguistics (Volume 1: Long Papers), pages
Proceedings of the 2020 Conference on Empirical 473â€“483,Vancouver,Canada.AssociationforCom-
MethodsinNaturalLanguageProcessing(EMNLP), putationalLinguistics.
pages3904â€“3914,Online.AssociationforComputa-
tionalLinguistics. Shexia He, Zuchao Li, Hai Zhao, and Hongxiao Bai.
2018b. Syntax for semantic role labeling, to be, or
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and not to be. In Proceedings of the 56th Annual Meet-
Kristina Toutanova. 2019. BERT: Pre-training of ingoftheAssociationforComputationalLinguistics
deep bidirectional transformers for language under- (Volume 1: Long Papers), pages 2061â€“2071, Mel-
standing. In Proceedings of the 2019 Conference bourne, Australia. Association for Computational
of the North American Chapter of the Association Linguistics.
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), John Hewitt and Christopher D. Manning. 2019. A
pages4171â€“4186,Minneapolis,Minnesota.Associ- structural probe for finding syntax in word repre-
ationforComputationalLinguistics. sentations. In Proceedings of the 2019 Conference
of the North American Chapter of the Association Conference on Empirical Methods in Natural Lan-
for Computational Linguistics: Human Language guage Processing, pages 1506â€“1515, Copenhagen,
Technologies, Volume 1 (Long and Short Papers), Denmark. Association for Computational Linguis-
pages4129â€“4138,Minneapolis,Minnesota.Associ- tics.
ationforComputationalLinguistics.
DiegoMarcheggianiandIvanTitov.2020. Graphcon-
Diederik P Kingma and Jimmy Ba. 2014. Adam: A volutionsoverconstituenttreesforsyntax-awarese-
method for stochastic optimization. arXiv preprint mantic role labeling. In Proceedings of the 2020
arXiv:1412.6980. Conference on Empirical Methods in Natural Lan-
guageProcessing(EMNLP),pages3915â€“3928,On-
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross- line.AssociationforComputationalLinguistics.
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the LluÂ´Ä±sMa`rquez,XavierCarreras,KennethC.Litkowski,
Association for Computational Linguistics (Volume and Suzanne Stevenson. 2008. Special issue in-
1: LongPapers),pages1190â€“1200,Sofia,Bulgaria. troduction: Semantic role labeling: An introduc-
AssociationforComputationalLinguistics. tiontothespecialissue. ComputationalLinguistics,
34(2):145â€“159.
JohnDLafferty,AndrewMcCallum,andFernandoCN
Pereira. 2001. Conditional random fields: Prob-
Tomas Mikolov, Edouard Grave, Piotr Bojanowski,
abilistic models for segmenting and labeling se-
Christian Puhrsch, and Armand Joulin. 2018. Ad-
quence data. In Proceedings of the Eighteenth In-
vances in pre-training distributed word representa-
ternationalConferenceonMachineLearning,pages
tions. In Proceedings of the International Confer-
282â€“289.
enceonLanguageResourcesandEvaluation(LREC
2018).
TaoLi,ParthAnandJawale,MarthaPalmer,andVivek
Srikumar.2020. Structuredtuningforsemanticrole
JoakimNivre,Marie-CatherinedeMarneffe,FilipGin-
labeling. In Proceedings of the 58th Annual Meet-
ter, Jan HajicË‡, Christopher D. Manning, Sampo
ingoftheAssociationforComputationalLinguistics,
Pyysalo, Sebastian Schuster, Francis Tyers, and
pages8402â€“8412,Online.AssociationforComputa-
Daniel Zeman. 2020. Universal Dependencies v2:
tionalLinguistics.
An evergrowing multilingual treebank collection.
In Proceedings of the 12th Language Resources
Zuchao Li, Shexia He, Jiaxun Cai, Zhuosheng Zhang,
andEvaluationConference,pages4034â€“4043,Mar-
Hai Zhao, Gongshen Liu, Linlin Li, and Luo Si.
seille,France.EuropeanLanguageResourcesAsso-
2018. A unified syntax-aware framework for se-
ciation.
mantic role labeling. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto.
guageProcessing, pages2401â€“2411, Brussels, Bel-
2018. A span selection model for semantic role la-
gium.AssociationforComputationalLinguistics.
beling. In Proceedings of the 2018 Conference on
EmpiricalMethodsinNaturalLanguageProcessing,
Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun.
pages 1630â€“1642, Brussels, Belgium. Association
2019. Sequence-to-nuggets: Nested entity mention
forComputationalLinguistics.
detection via anchor-region networks. In Proceed-
ings of the 57th Annual Meeting of the Association
Sebastian PadoÂ´ and Mirella Lapata. 2009. Cross-
for Computational Linguistics, pages 5182â€“5192,
lingual annotation projection for semantic roles.
Florence, Italy. Association for Computational Lin-
Journal of Artificial Intelligence Research, 36:307â€“
guistics.
340.
ChristopherDManning, MihaiSurdeanu, JohnBauer,
Jenny Rose Finkel, Steven Bethard, and David Mc- MarthaPalmer,DanielGildea,andNianwenXue.2010.
Closky.2014. Thestanfordcorenlpnaturallanguage Semantic role labeling. Synthesis Lectures on Hu-
processing toolkit. In Proceedings of 52nd annual manLanguageTechnologies,3(1):1â€“103.
meetingoftheassociationforcomputationallinguis-
tics: systemdemonstrations,pages55â€“60. HaoruoPeng,Kai-WeiChang,andDanRoth.2015. A
jointframeworkforcoreferenceresolutionandmen-
Diego Marcheggiani, Anton Frolov, and Ivan Titov. tion head detection. In Proceedings of the Nine-
2017. Asimpleandaccuratesyntax-agnosticneural teenth Conference on Computational Natural Lan-
modelfordependency-basedsemanticrolelabeling. guageLearning,pages12â€“21,Beijing,China.Asso-
In Proceedings of the 21st Conference on Compu- ciationforComputationalLinguistics.
tationalNaturalLanguageLearning(CoNLL2017),
pages411â€“420,Vancouver,Canada.Associationfor Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
ComputationalLinguistics. How multilingual is multilingual BERT? In Pro-
ceedings of the 57th Annual Meeting of the Asso-
Diego Marcheggiani and Ivan Titov. 2017. Encoding ciation for Computational Linguistics, pages 4996â€“
sentenceswithgraphconvolutionalnetworksforse- 5001, Florence, Italy. Association for Computa-
mantic role labeling. In Proceedings of the 2017 tionalLinguistics.
SameerPradhan,AlessandroMoschitti,NianwenXue, OscarTaÂ¨ckstroÂ¨m,KuzmanGanchev,andDipanjanDas.
Hwee Tou Ng, Anders BjoÂ¨rkelund, Olga Uryupina, 2015. Efficientinferenceandstructuredlearningfor
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro- semanticrolelabeling. TransactionsoftheAssocia-
bust linguistic analysis using OntoNotes. In Pro- tionforComputationalLinguistics,3:29â€“41.
ceedingsoftheSeventeenthConferenceonComputa-
tional Natural Language Learning, pages 143â€“152, ZhixingTan, MingxuanWang, JunXie, YidongChen,
Sofia,Bulgaria.AssociationforComputationalLin- andXiaodongShi.2018. Deepsemanticrolelabel-
guistics. ingwithself-attention. InProceedingsoftheAAAI
ConferenceonArtificialIntelligence.
VasinPunyakanok, DanRoth, andWen-tauYih.2008.
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.
Theimportanceofsyntacticparsingandinferencein
BERT rediscovers the classical NLP pipeline. In
semantic role labeling. Computational Linguistics,
Proceedingsofthe57thAnnualMeetingoftheAsso-
34(2):257â€“287.
ciation for Computational Linguistics, pages 4593â€“
4601,Florence,Italy.AssociationforComputational
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Linguistics.
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In COLING 2004: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Proceedings of the 20th International Conference Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz
on Computational Linguistics, pages 1346â€“1352, Kaiser, and Illia Polosukhin. 2017. Attention is all
Geneva,Switzerland.COLING. you need. In Advances in neural information pro-
cessingsystems,pages5998â€“6008.
LanceARamshawandMitchellPMarcus.1999. Text
chunking using transformation-based learning. In Shuohang Wang and Jing Jiang. 2016. Machine com-
Natural language processing using very large cor- prehension using match-lstm and answer pointer.
pora,pages157â€“176.Springer. arXivpreprintarXiv:1608.07905.
Shijie Wu and Mark Dredze. 2019. Beto, bentz, be-
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
cas: The surprising cross-lingual effectiveness of
2018. Self-attentionwithrelativepositionrepresen-
BERT. In Proceedings of the 2019 Conference on
tations. In Proceedings of the 2018 Conference of
EmpiricalMethodsinNaturalLanguageProcessing
the North American Chapter of the Association for
andthe9thInternationalJointConferenceonNatu-
ComputationalLinguistics: HumanLanguageTech-
ralLanguageProcessing(EMNLP-IJCNLP),pages
nologies, Volume 2 (Short Papers), pages 464â€“468,
833â€“844,HongKong,China.AssociationforCom-
New Orleans, Louisiana. Association for Computa-
putationalLinguistics.
tionalLinguistics.
Nianwen Xue and Martha Palmer. 2004. Calibrating
PengShiandJimmyLin.2019. Simplebertmodelsfor
features for semantic role labeling. In Proceed-
relationextractionandsemanticrolelabeling. arXiv
ings of the 2004 Conference on Empirical Meth-
preprintarXiv:1904.05255.
ods in Natural Language Processing, pages 88â€“94,
Barcelona, Spain. Association for Computational
Emma Strubell, Patrick Verga, Daniel Andor, Linguistics.
David Weiss, and Andrew McCallum. 2018.
Linguistically-informed self-attention for semantic ZhisongZhang,XiangKong,ZhengzhongLiu,Xuezhe
role labeling. In Proceedings of the 2018 Confer- Ma, and Eduard Hovy. 2020. A two-step approach
ence on Empirical Methods in Natural Language for implicit event argument detection. In Proceed-
Processing, pages 5027â€“5038, Brussels, Belgium. ings of the 58th Annual Meeting of the Association
AssociationforComputationalLinguistics. for Computational Linguistics, pages 7479â€“7485,
Online.AssociationforComputationalLinguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
LluÂ´Ä±s Ma`rquez, and Joakim Nivre. 2008. The JieZhouandWeiXu.2015. End-to-endlearningofse-
CoNLL 2008 shared task on joint parsing of syn- manticrolelabelingusingrecurrentneuralnetworks.
tactic and semantic dependencies. In CoNLL 2008: In Proceedings of the 53rd Annual Meeting of the
ProceedingsoftheTwelfthConferenceonComputa- Association for Computational Linguistics and the
tional Natural Language Learning, pages 159â€“177, 7th International Joint Conference on Natural Lan-
Manchester,England.Coling2008OrganizingCom- guage Processing (Volume 1: Long Papers), pages
mittee. 1127â€“1137,Beijing,China.AssociationforCompu-
tationalLinguistics.
Swabha Swayamdipta, Sam Thomson, Kenton Lee,
Luke Zettlemoyer, Chris Dyer, and Noah A. Smith.
2018. Syntactic scaffolds for semantic structures.
In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 3772â€“3782, Brussels, Belgium. Association
forComputationalLinguistics.

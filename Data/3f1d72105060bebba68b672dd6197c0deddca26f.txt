Consistency of a Recurrent Language Model With Respect to
Incomplete Decoding
SeanWelleck1∗ IliaKulikov1∗ JaedeokKim2†
RichardYuanzhePang1 KyunghyunCho1,3
1 NewYorkUniversity 2 SamsungResearch 3 CIFARAssociateFellow
Abstract Theseissuesaresuspectedtoberelatedtothemax-
imum likelihood objective’s local normalization,
Despite strong performance on a variety of whichresultsinadiscrepancybetweenthelearned
tasks, neural sequence models trained with model’sdistributionandthedistributioninducedby
maximum likelihood have been shown to ex-
thedecodingalgorithmusedtogeneratesequences
hibit issues such as length bias and degener-
(Laffertyetal.,2001;Andoretal.,2016). Thishas
ate repetition. We study the related issue of
promptedthedevelopmentofalternativedecoding
receiving infinite-length sequences from a re-
methods(Wuetal.,2016;Holtzmanetal.,2019)
current language model when using common
decodingalgorithms.Toanalyzethisissue,we andtrainingobjectives(MurrayandChiang,2018;
first define inconsistency of a decoding algo- Wellecketal.,2019). Inthispaper,weformalize
rithm,meaningthatthealgorithmcanyieldan andstudythisdiscrepancybetweenthemodeland
infinite-lengthsequencethathaszeroprobabil- thedecodingalgorithm.
ityunderthemodel. Weprovethatcommonly
We begin by formally defining recurrent neu-
usedincompletedecodingalgorithms–greedy
ral language models, a family that encompasses
search, beam search, top-k sampling, and nu-
neural models used in practice, such as recurrent
cleus sampling – are inconsistent, despite the
factthatrecurrentlanguagemodelsaretrained neural networks (Elman, 1990; Cho et al., 2014;
to produce sequences of finite length. Based HochreiterandSchmidhuber,1997),andtransform-
on these insights, we propose two remedies ers(Vaswanietal.,2017). Next,weformallydefine
which address inconsistency: consistent vari- a decoding algorithm – a function that induces a
antsoftop-kandnucleussampling,andaself-
distributionoversequencesgivenarecurrentlan-
terminatingrecurrentlanguagemodel. Empir-
guagemodelandacontextdistribution–whichis
ical results show that inconsistency occurs in
usedtoobtainprobablesequencesfromamodel. In
practice, and that the proposed methods pre-
ventinconsistency. thispaper,weshowthatthedistributioninducedby
adecodingalgorithmcancontradictthisintended
1 Introduction use; instead, the decoding algorithm may return
improbable,infinite-lengthsequences.
Neural sequence models trained with maximum Our main finding is that a sequence which re-
likelihoodestimation(MLE)havebecomeastan- ceiveszeroprobabilityunderarecurrentlanguage
dardapproachtomodelingsequencesinavariety model’sdistributioncanreceivenonzeroprobabil-
ofnaturallanguageapplicationssuchasmachine ity under the distribution induced by a decoding
translation(Bahdanauetal.,2015),dialoguemod- algorithm. Thisoccurswhentherecurrentlanguage
eling(Vinyalsetal.,2015),andlanguagemodeling modelalwaysranksthesequenceterminationtoken
(Radfordetal.,2019). Despitethissuccess,MLE- outsideofthesetoftokensconsideredateachde-
trainedneuralsequencemodelshavebeenshown codingstep,yieldinganinfinite-length,zeroproba-
to exhibit issues such as length bias (Sountsov bilitysequence. Thisholdswheneverthedecoding
and Sarawagi, 2016; Stahlberg and Byrne, 2019) algorithmisincomplete,inthesensethatthealgo-
anddegeneraterepetition(Holtzmanetal.,2019). rithmexcludestokensfromconsiderationateach
step of decoding, which is the case for common
∗Equalcontribution. Correspondenceto:SeanWelleck
methodssuchasgreedysearch,beamsearch,top-k
wellecks@nyu.edu.
†WorkdoneatNewYorkUniversity. sampling(Fanetal.,2018),andnucleussampling
(Holtzman et al., 2019). We formalize our main Definition2.2(Contextdistribution). Acontextdis-
findingusingthenotionofconsistency(Chenetal., tributionp(C)isaprobabilitydistributiondefined
2017)–whetheradistributionassignsprobability overasetC. AnelementC ∈ C iscalledacontext.
mass only to finite sequences – and prove that a
2.1 RecurrentLanguageModels
consistent recurrent language model paired with
an incomplete decoding algorithm can induce an A recurrent language model is an autoregressive
inconsistentsequencedistribution. modelofasequencedistribution,whereeachcon-
Based on the insight that inconsistency occurs ditionalprobabilityisparameterizedwithaneural
due to the behavior of the termination token un- network. Importantly, we assume that all tokens
der incomplete decoding, we develop two meth- inasequencearedependentoneachotherundera
ods for addressing inconsistency. First, we pro- recurrentlanguagemodel. Thisallowsustoavoid
poseconsistentsamplingmethodswhichguarantee casesinwhichthemodeldegeneratestoaMarko-
thattheterminationtokenisnotexcludedfromse- vian language model, such as an n-gram model
lection during decoding. Second, we introduce a withafiniten.
self-terminatingrecurrentlanguagemodelwhich Definition2.3(Recurrentlanguagemodel). Are-
ensures that the termination token is eventually currentlanguagemodelp isaneuralnetworkthat
θ
rankedaboveallothers,guaranteeingconsistency computesthefollowingateachtimestep:
underincompletedecoding.
To empirically measure inconsistency, we de-
p (y = v|y ,C) =
exp(u(cid:62) vh t+c v)
,
code sequences from trained recurrent language θ t <t (cid:80) exp(u(cid:62)h +c )
v(cid:48)∈V v(cid:48) t v(cid:48)
models and measure the proportion of sequences
where h = f (y ,h ) and h = g (C), and
withlengthsfarexceedingthemaximumtraining t θ t t−1 0 θ
u,c,θareparameters. Arecurrentlanguagemodel
sequencelength. OurexperimentsontheWikitext2
thereby computes the probability of a sequence
dataset(Merityetal.,2016)suggestthatinconsis-
Y = (y ,...,y )by
tency occurs in practice when using incomplete 1 T
decodingmethods,whiletheproposedconsistent
T
samplingmethodsandself-terminatingmodelpa- (cid:89)
p (Y |C) = p (y |y ,C),
θ θ t <t
rameterizationpreventinconsistencyandmaintain
t=1
languagemodelingquality.
wherey = (y ,...,y ). Thisdistributionsat-
The theoretical analysis reveals defects of ex- <t 1 t−1
isfiesy ⊥(cid:54)⊥ y |C, ∀i < j.
isting decoding algorithms, providing a way to i j
developfuturemodels,inferenceprocedures,and Practical variants of the recurrent language
learningalgorithms. Wepresentmethodsrelatedto modeldifferbythechoiceoftransitionfunctionf
θ
samplingandmodelparameterization,butthereare (Elman,1990;HochreiterandSchmidhuber,1997;
moredirectionsforfutureinvestigation;weclose Choetal.,2014;Vaswanietal.,2017). Theuseof
withdirectionsrelatedtosequence-levellearning. softmax (Bridle, 1990) implies that every unique
tokeninthevocabularyisconsideredateveryloca-
2 Background tionofasequence.
Remark 2.1. Under the conditional distribution
We begin our discussion by establishing back-
of a recurrent LM, every token v ∈ V is as-
ground definitions. First, we define a sequence
signed a positive probability, implying that 0 <
whichisthemainobjectofourinvestigation.
p (v|y ,C) < 1. Any finite sequence is proba-
θ <t
Definition 2.1 (Sequence). A sequence Y is an ble under a recurrent LM under any context, i.e.,
orderedcollectionofitemsfromapredefinedfinite p (Y |C) > 0foranysequenceY offinitelength.
θ
vocabularyV. Asequenceoffinitelengthalways
ends with a special token (cid:104)eos(cid:105) ∈ V that only 2.2 DecodingAlgorithms
appearsattheendofasequence. Becauseitisintractabletodecodethemostproba-
blesequence,itisnecessaryinpracticetousean
Each model we consider generates a sequence
approximatedecodingalgorithm.
conditionedoncontextinformation,suchasaprefix
insentencecompletion. Toconsiderthis,wedefine Definition2.4(Decodingalgorithm). Adecoding
acontextdistribution. algorithm F(p ,C) is a function that generates
θ
a sequence Y˜ given a recurrent language model Deterministic decoding. The other family con-
p and context C. Let q denote the distribution sistsofdeterministicdecodingalgorithms,where
θ F
inducedbythedecodingalgorithmF. a token is selected deterministically according to
a rule at each decoding step. The most naive al-
We consider two families of decoding algo-
gorithm,calledgreedydecoding,simplytakesthe
rithms. Inouranalysisweonlyconsideralgorithms
mostprobabletokenateachstep.
thatdecodeinasinglepass,forwardintime,with-
outmodifyingpreviouslyselectedtokens. Definition2.8(Greedydecoding). Greedydecod-
ingF generatesasequencefromarecurrent
greedy
Stochasticdecoding. Thefirstfamilyconsistsof
languagemodelp givencontextC byrecursively
θ
stochasticalgorithms. Amongthem,ancestralsam-
selectingthemostlikelytokenfromp (y |y˜ ,C)
θ t <t
plingisasymptoticallyunbiasedandcanbeused
untily˜ = (cid:104)eos(cid:105):
t
forfindingthemostprobablesequence,although
withhighvariance. y˜ = argmaxlogp (y = v|y˜ ,C).
t θ t <t
v∈V
Definition 2.5 (Ancestral sampling). Ancestral
Incontrasttogreedydecoding,beamsearchwith
sampling F generates a sequence from a re-
anc
widthk,F ,operatesonthelevelofpartialse-
currentlanguagemodelp givencontextC byre- beam-k
θ
quencesorprefixes. Startingfromasetofempty
cursivelysamplingfromp (y |y˜ ,C)untily˜ =
θ t <t t
prefixes,ateachiterationanewprefixsetisformed
(cid:104)eos(cid:105): y˜ ∼ p (y |y˜ ,C).
t θ t <t
byexpandingeachprefixwitheachpossibletoken,
To avoid the high variance, two approximate
thenchoosingthek highestscoringexpandedpre-
stochasticdecodingalgorithmshaverecentlybeen
fixes;refertoAppendixAforaformaldefinition.
proposedandtestedwithrecurrentlanguagemod-
els. Top-k samplingconsidersonlyasubsetofthe Incompleteness. Otherthanancestralsampling,
k most probable tokens from the vocabulary at a the decoding algorithms above are incomplete in
time, while nucleus sampling considers only the that they only consider a strict subset of the full
minimalsubsetofmostprobabletokenswhosetotal vocabulary V at each time step, aside from the
probabilityishigherthanapredefinedthreshold. trivialcaseofk = |V|.1
Definition2.6(Top-k sampling(Fanetal.,2018)). Definition2.9(IncompleteDecoding). Adecoding
Top-k samplingF generatesasequencefrom algorithmF isincompletewhenforeachcontext
top-k
arecurrentlanguagemodelp
θ
givencontextC by C and prefix y <t, there is a strict subset V t(cid:48) (cid:40) V
recursivelysamplingfrom: suchthat
(cid:88)
(cid:40) q (y = v|y ,C) = 1.
p (v|y ,C), ifv ∈ V , F t <t
θ <t k
q(v) ∝
v∈V(cid:48)
0, otherwise. t
3 ConsistencyofaDecodingAlgorithm
whereV = argtop-kp (v(cid:48)|y ,C).
k θ <t
v(cid:48) Definitionofconsistency. Arecurrentlanguage
Definition2.7(Nucleussampling(Holtzmanetal., model p may assign a positive probability to an
θ
2019)). Nucleus sampling F nuc-µ generates a se- infinitelylongsequence,inwhichcasewecallthe
quencefromarecurrentlanguagemodelp θ given modelinconsistent. Thisnotionofconsistencywas
context C by recursively sampling from the fol- raisedandanalyzedearlier,forinstancebyBooth
lowing proposal distribution. Let v 1,...,v
|V|
and Thompson (1973) and Chen et al. (2017), in
denote tokens in V such that p θ(v i|y <t,C) ≥ termsofwhetherthedistributioninducedbyp
θ
is
p θ(v j|y <t,C)foralli < j,anddefine concentratedonfinitesequences. Weextendtheir
definitiontoaccountforthecontextC.
(cid:40)
p (v|y ,C), ifv ∈ V ,
q(v) ∝ θ <t µ Definition 3.1 (Consistency of a recurrent lan-
0, otherwise, guage model). A recurrent language model is
consistent under a context distribution p(C) if
(cid:8) (cid:9)
whereV µ = v 1,··· ,v kµ with p θ(|Y| = ∞) = 0. Otherwise, the recurrent lan-
guagemodelissaidtobeinconsistent.
(cid:40) (cid:12) k (cid:41)
(cid:12)(cid:88)
k = min k (cid:12) p (v |y ,C) > µ . 1Nucleussamplingisincompletewhenforeverycontext
µ (cid:12) θ i <t
(cid:12) i=1 Candprefixy <t,min v∈V p θ(v|y <t,C)<1−µ.
Anysequencedecodedfromaconsistentmodel
foragivencontextisguaranteedtoterminate.
Lemma 3.1. If a recurrent LM p is consistent,
θ
p (|Y| = ∞|C)=0foranyprobablecontextC.2
θ
Next, we establish a practical condition under
whicharecurrentlanguagemodelisconsistent.
Lemma 3.2. A recurrent LM p is consistent if
θ
(cid:107)h (cid:107) isuniformlyboundedforsomep ≥ 1.
t p
Figure1: Adepictionofthemodel’ssequencedistribu-
Proofsketch. If(cid:107)h (cid:107) isbounded,theneachu(cid:62)h tion(lightgrey,solidborder)andthedecoder’sinduced
t p v t
is bounded, hence p ((cid:104)eos(cid:105)|y ,C) > ξ > 0 for sequence distribution (dark grey, dotted border). The
θ <t
white and black rectangles depict the set of all finite
a constant ξ. Thus p (|Y| = ∞) ≤ lim (1−
θ t→∞
andinfinitesequences,respectively. Weprovethatun-
ξ)t = 0,meaningthatp isconsistent.
θ derpracticalconditions,anyincompletedecodingalgo-
rithm may be inconsistent with respect to a consistent
Although this condition is practical because
model,asdepicted.
layer normalization or bounded activation func-
tions(Elman,1990;Choetal.,2014;Vaswanietal.,
2017)resultinboundedh ,weshowthatevenifa Theorem3.4(Inconsistencyofanincompletede-
t
recurrentlanguagemodelisconsistent,adecoding codingalgorithm). Thereexistsaconsistentrecur-
algorithmmayproduceaninfinite-lengthsequence. rent LM p from which an incomplete decoding
θ
We formalize this discrepancy using the consis- algorithmF,thatconsidersonlyupto(|V|−1)-
tencyofadecodingalgorithm. most likely tokens according to p (y |y ,C) at
θ t <t
each step t, finds an infinite-length sequence Y˜
Definition 3.2 (Consistency of a decoding algo-
withprobability1,i.e.,q (|Y| = ∞) = 1.
rithm). AdecodingalgorithmF isconsistentwith F
respecttoaconsistentrecurrentlanguagemodelp θ Proof. We prove this theorem by constructing a
underacontextdistributionp(C)ifthedecoding
tanh recurrent network. We define the recurrent
algorithmF preservestheconsistencyofthemodel functionf as
θ
p ,thatis,q (|Y| = ∞) = 0.
θ F
h = f (y ,h )
Whenaconsistentrecurrentlanguagemodelp t θ t t−1
θ
(cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)
and a decoding algorithm F induce a consistent W 0 0
= tanh h h + ,
distribution q , we say that p paired with F is 0 I t−1 e(y )
F θ t
consistent. Forinstance, anyconsistentrecurrent
languagemodelpairedwithancestralsamplingis wheree(y ) ∈ R|V| isaone-hotrepresentationof
t
consistent,becausetheinduceddistributionq is y ,W ∈ Rd×d whereeveryentryispositive,and
Fanc t h
thesameasthedistributionoftheoriginalmodel. I is an identity matrix of size |V| × |V|. h =
0
WealsohaveananalogueofLemma3.1. g (C)isconstructedtoconsistofpositivevalues
θ
only. Becauseeachelementof|h |isboundedby
Lemma3.3. Aconsistentdecodingalgorithmwith t
1,theconstructedrecurrentlanguagemodelp is
respecttoaconsistentrecurrentLMdecodesonly θ
consistentbyLemma3.2.
probable sequences. That is, if q (Y |C) > 0,
F
Wesetu (seeDefinition2.3)to
thenp (Y |C) > 0foranyprobablecontextC. v
θ
(cid:20) (cid:21) (cid:20) (cid:21)
u¯ u¯
Inconsistencyofincompletedecoding. Anyin- u = v , u = (cid:104)eos(cid:105) ,
v e(v) (cid:104)eos(cid:105) e((cid:104)eos(cid:105))
completedecodingalgorithm(Definition2.9)can
be inconsistent regardless of the context distribu-
where v (cid:54)= (cid:104)eos(cid:105), all elements of u¯ are positive,
tion, because there is a recurrent LM that places v
all elements of u¯ are negative, and e(v) is a
(cid:104)eos(cid:105)outsideofV(cid:48) ateverystepofdecoding. To (cid:104)eos(cid:105)
t one-hotrepresentationofv. c issettozero.
showthis,weconstructaconsistentrecurrentlan- v
This defines a valid recurrent language model
guagemodelwhosedistributioninducedbyanin-
(Definition2.3),sincetheconditionaldistribution
completedecodingalgorithmisinconsistent.
at each time t is influenced by all the previous
2ProofsofLemmas3.1-3.3areinAppendixB. tokens. More specifically, the logit of a token v
dependson(cid:80)t 1(y = v),where1isanindi-
t(cid:48)=1 t(cid:48)
catorfunction.
This recurrent language model always outputs
positive logits for non-(cid:104)eos(cid:105) tokens, and outputs
negative logits for the (cid:104)eos(cid:105) token. This im-
plies p((cid:104)eos(cid:105)|y ,C) < p(v|y ,C) for all
<t <t
v ∈ V\{(cid:104)eos(cid:105)}. This means that (cid:104)eos(cid:105) is al-
ways ranked last at each time step, so an incom-
plete decoding algorithm that considers at most
(|V|−1) most probable tokens at each time step
fromp (y |y ,C)cannotdecode(cid:104)eos(cid:105)andthus
θ t <t
alwaysdecodesaninfinitelylongsequenceYˆ,i.e.,
q (|Y| = ∞|C) = 1foranycontextC. Ityields Figure 2: The self-terminating recurrent LM uses the
F
layer shown in grey instead of the standard softmax
q (|Y| = ∞) = 1, while p (|Y| = ∞) = 0 due
F θ layer. The layer takes logits (u(cid:62)h ), the previous
toconsistencyofthemodelp . · t
θ step’s(cid:104)eos(cid:105)probability(p(cid:104)eos(cid:105)),andahyper-parameter
t−1
(cid:15) ∈ (0,1). ThelayercomputesαusingDefinition4.3,
Greedydecoding,beamsearch,top-k sampling,
whichdeterminesthe(cid:104)eos(cid:105)probability(p(cid:104)eos(cid:105) ∈((cid:15),1)),
andnucleussamplingareallinconsistentaccording t
tothistheorem. and guarantees that p(cid:104) teos(cid:105) > p(cid:104) t−eo 1s(cid:105). The remaining
probabilitymassisallocatedtothenon-(cid:104)eos(cid:105)tokens.
4 Fixingtheinconsistency
followingmodifiedproposaldistribution:
In this section, we consider two ways to prevent
inconsistency arising from incomplete decoding
(cid:40)
algorithms. First,weintroduceconsistentversions p θ(v|y <t,C), ifv ∈ V(cid:48),
q(v) ∝
of top-k and nucleus sampling. Second, we in- 0, otherwise,
troduce the self-terminating recurrent language
model,whichisconsistentwhenpairedwithanyof whereV(cid:48) = {(cid:104)eos(cid:105)}∪argtop-kp (v(cid:48)|y ,C).
θ <t
thedecodingalgorithmsconsideredinthispaper. v(cid:48)
Definition 4.2 (Consistent nucleus sampling).
4.1 ConsistentSamplingAlgorithms Consistentnucleussamplingisnucleussampling
withthefollowingmodifiedproposaldistribution:
TheproofofTheorem3.4suggeststhattheincon-
sistencyofincompletedecodingalgorithmsarises
(cid:40)
p (v|y ,C), ifv ∈ V ∪{(cid:104)eos(cid:105)},
from the fact that (cid:104)eos(cid:105) may be excluded indefi- θ <t µ
q(v) ∝
nitelyfromthesetoftop-rankedtokens. Wepro- 0, otherwise.
pose a simple modification to top-k and nucleus
sampling that forces (cid:104)eos(cid:105) to be included at each The induced probability of (cid:104)eos(cid:105) under these
step of decoding. First, we give a condition for twoalgorithmsisalwaysequaltoorlargerthanthe
whenaparticularmodelp pairedwithadecoding model’sprobability. ByTheorem4.1,thesealgo-
θ
algorithmF isconsistent. rithmsareconsistentwithrespecttoanyconsistent
recurrentlanguagemodel.
Theorem4.1. SupposearecurrentLMp hasuni-
θ
formly bounded (cid:107)h (cid:107) for some p ≥ 1. If a de-
t p 4.2 Self-TerminatingRecurrentLM
codingalgorithmF satisfiesq ((cid:104)eos(cid:105)|y ,C) ≥
F <t
p ((cid:104)eos(cid:105)|y ,C)foreveryprefixy andcontext Althoughtheseconsistentsamplingalgorithmscan
θ <t <t
C, then the decoding algorithm F is consistent beusedwithanyrecurrentlanguagemodel,their
withrespecttothemodelp .3 stochasticnaturemaynotbesuitableforfindinga
θ
single,highlyprobablesequence. Toavoidthislim-
We define consistent variants of top-k and nu-
itation,weproposetheself-terminatingrecurrent
cleussamplingwhichsatisfythiscondition.
languagemodel(STRLM).
Definition4.1(Consistenttop-k sampling). Con-
Definition 4.3 (Self-terminating recurrent lan-
sistenttop-k samplingistop-k samplingwiththe
guage model). A self-terminating recurrent lan-
3SeeAppendixCfortheproof. guage model computes the following conditional
probabilityateachtimestep: the behavior of language models, encompassing
machinetranslation(Bahdanauetal.,2015),story

1−α(h t), v = (cid:104)eos(cid:105), generation (Fan et al., 2018), and dialogue mod-
p (v|y ,C) =
θ <t α(ht)exp(u(cid:62) vht+cv)
,
eling (Vinyals et al., 2015). The task consists of
(cid:80) v(cid:48)∈V(cid:48)exp(u(cid:62) v(cid:48)ht+c v(cid:48)) decoding a continuation Yˆ ∼ F(p θ,C) given a
α(h ) = σ(u(cid:62) h ), length-k prefix C = (c ,...,c ), resulting in a
0 (cid:104)eos(cid:105) 0 1 k
completion(c ,...,c ,yˆ ...,yˆ ).
α(h ) = σ(u(cid:62) h )[1−p ((cid:104)eos(cid:105)|y ,C)], 1 k 1 T
t (cid:104)eos(cid:105) t θ <t−1
Dataset. OurfirsttwoexperimentsuseWikitext2
with σ : R → [0,1 − ε] and ε ∈ (0,1). h is
t (Merityetal.,2016),whichconsistsofparagraphs
computedasintheoriginalrecurrentLM.
from English Wikipedia, since it has frequently
The underlying idea is that the probability of beenusedtoevaluatelanguagemodels(Graveetal.,
(cid:104)eos(cid:105)increasesmonotonically,since 2017;Melisetal.,2018;Merityetal.,2018). We
consider both word and BPE5 tokenization. We
p(cid:104)eos(cid:105) = 1−
(cid:89)t
σ(u(cid:62) h ).
spliteachparagraphintosentencesusingSpacy6.
t (cid:104)eos(cid:105) t(cid:48) We split each sequence, using the first k tokens
t(cid:48)=0
as a context and the remaining tokens as a con-
Consequently, the STRLM is consistent when tinuation. Toensurethateachsequencecontainsa
pairedwithgreedydecodingorbeamsearch; see prefix,weprependpaddingtokenstomakeitlength
AppendixCforformalstatementsandproofs. k. Special (cid:104)bos(cid:105) and (cid:104)eos(cid:105) tokens are inserted at
the beginning and end of each sequence. We use
5 EmpiricalValidation k = 10. Table7containsdatasetstatistics.
The theoretical results rely on the existence of a Contextdistribution. Wedefineempiricalcon-
model that results in inconsistency; it remains to textdistributionswithprefixesfromthetrain,valid,
beshownthatinconsistencywithrespecttoincom- andtestsets: p(C;D) = 1 (cid:80)|D| 1(C = C(n)),
|D| n=1
pletedecodingoccurswithrecurrentlanguagemod- whereD = {(C(n),Y(n))}N isadatasetsplit.
n=1
els encountered in practice. Moreover, while the
proposedmethodscarrytheoreticalguaranteesin Evaluationmetrics. Weusefinitesequencesto
termsofconsistency,wemustcheckwhetherthey approximatelymeasuretheconsistencyofamodel
retain language modeling quality. To do so, we pairedwithadecodingalgorithm,sincedecoding
performexperimentsusingasequencecompletion aninfinite-lengthsequenceisimpossible. Weuse
task. Ineachexperiment,weusethebeginningof the proportion of decoded continuations that are
asequenceascontext,thendecodecontinuations longerthanapredefinedlimit,
fromatrainedrecurrentLMandmeasurethepro-
|D|
1 (cid:88)
portion of non-terminated sequences in order to r = 1(|Yˆ(n)| ≥ L),
L
|D|
approximatelymeasureinconsistency. Thefirstex-
n=1
periment(§5.1)showsthatinconsistencyoccursin
where Yˆ(n) ∼ F(p ,C(n)) for each context C(n)
practice,andthesecondexperiment(§5.2)shows θ
inD. Wecallr thenon-terminationratioofthe
theeffectivenessoftheproposedapproaches. Our L
decodingalgorithmF foranunderlyingmodeland
third experiment (§5.3) shows that inconsistency
context distribution. A value of r greater than
alsooccursfrequentlyinGPT-2,alarge-scaletrans- L
zeromeansthatsomesequencesdidnotterminate
formerlanguagemodel.4
within L steps. When L is infinity, this implies
Sequence completion. We evaluate recurrent thatthemodelpairedwiththedecodingalgorithm
language models on a sequence completion task, isinconsistent. Inpractice, weuseafiniteLthat
which has previously been used to evaluate the issubstantiallylargerthanthemaximumtraining
effectivenessofsequencemodels,e.g.,Sutskever sequencelength,andweinterpretanon-zeror as
L
etal.(2011);Graves(2013);Radfordetal.(2019); evidencethatthemodelpairedwiththedecoding
Holtzmanetal.(2019);Wellecketal.(2019). Se- algorithmisinconsistent. WeuseL = 1500,more
quencecompletionisageneralsettingforstudying than10timesthemaxtrainingsequencelength.
4Codeavailableathttps://github.com/uralik/ 5github.com/huggingface/tokenizers
consistency-lm. 6https://spacy.io/
Ineachexperiment,wereportthemeanandstan- tanh-RNN LSTM-RNN
darddeviationofmetricsacross10independentini-
ancestral 0.00±0.0 0.00±0.0
tializations. Unlessspecifiedotherwise,wereport
greedy 12.35±5.18 1.53±1.41
metrics using the test context distribution, since beam-2 1.38±0.95 0.07±0.06
the train, valid, and randomly generated context beam-4 0.25±0.19 0.00±0.01
distributionshadsimilarresults. topk-2 0.01±0.01 0.01±0.01
topk-4 0.00±0.0 0.00±0.01
nucleus-0.2 0.06±0.02 0.13±0.15
Training. We train recurrent language
nucleus-0.4 0.04±0.02 0.02±0.01
models for sequence completion with
consistenttopk-2 0.00±0.0 0.00±0.01
maximum likelihood, using the loss
consistenttopk-4 0.00±0.0 0.00±0.0
L(p θ,Y) =
−(cid:80)T
t=1logp θ(y t|y <t,c 1,...,c k), consistentnucleus-0.2 0.04±0.02 0.01±0.01
whereY = (c ,...,c ,y ,...,y ). Thisamounts consistentnucleus-0.4 0.02±0.02 0.01±0.01
1 k 1 T
to running the full training sequence through a
Table1: Non-terminationratio(r (%))ofdecodedse-
recurrent model and zeroing the loss for the first L
quencesusingancestralsampling,incomplete,andcon-
k tokens, so that the first k steps correspond to
sistentdecodingmethods.
learningag thatencodesthecontext.
θ
Models. Weconsiderrecurrentneuralnetworks On the other hand, the non-zero non-termination
withhyperbolictangentactivations(tanh-RNN;El-
ratiosfortheincompletedecodingalgorithmssug-
man,1990)andLSTMunits(LSTM-RNN;Hochre- gestinconsistencywithrespecttoeachalgorithm,
iterandSchmidhuber,1997). Weperformanini- providingevidenceforTheorem3.4.
tialhyper-parametersweepandselectthebestset
Using greedy decoding, roughly 12% of all
of hyper-parameters for each of tanh-RNN and
contexts resulted in a non-terminating continua-
LSTM-RNNbasedonthevalidationperplexities.7
tion with the tanh-RNN, and roughly 1% with
Withthisbestsetofhyperparameters,wetraineach
theLSTM-RNN.Nucleussamplingalsoproduced
of these models with 10 different initializations.
non-terminating sequences with the tanh-RNN
ThechoiceoftanhandLSTMRNNsimpliesthat
(0.06%, nuc-0.2) and LSTM-RNN (0.13%, nuc-
alloftherecurrentlanguagemodelsthatwetrain
0.2). Top-k sampling yielded a small number
areconsistentaccordingtoLemma3.2. OurLSTM
of non-terminating samples. In general, non-
modelsachievesimilartestperplexity(91.86±0.4,
termination approaches zero as k and µ increase,
word tokenization) to those reported in previous
since(cid:104)eos(cid:105)hasalowerchanceofbeingexcluded.
work(Merityetal.,2018);seeAppendixD.
Beam search produced non-terminating se-
Additionally, we train self-terminating tanh-
quenceswithboththetanh-RNNandLSTM-RNN
RNNandLSTM-RNNvariants(Definition4.3)at
models. Thismeansthat(cid:104)eos(cid:105)wasoutsideofthe
variousvaluesofε,whichcontrolsalowerbound
top tokens (determined by the beam width) con-
ontheterminationprobabilityateachstep. Weuse
sideredateachstep,sinceinourexperimentswe
σ(x) = (1−ε)·sigmoid(x). We use the hyper-
terminated the beam search when a single beam
parameters selected in the preceding grid search.
prefixcontained(cid:104)eos(cid:105). Largerbeamwidthsreduce
Below,weconsiderBPEtokenization;similarcon-
non-termination,similartoincreasingk orµ.
clusionsheldforwordtokenization.8
5.2 ConsistencyoftheProposedMethods
5.1 InconsistencyofRecurrentLMs
Consistent sampling. Table 1 shows that con-
In this experiment, we demonstrate evidence of sistentnucleusandtop-k sampling(§4.1)resulted
inconsistencywithincompletedecodingmethods.
in only terminating sequences, except for a few
Table 1 shows non-termination ratios for the re- cases that we attribute to the finite limit L used
currentlanguagemodelsusingthedecodingalgo-
to measure the non-termination ratio. Consistent
rithmsconsideredinthiswork. Decodingwithan- nucleuspairedwithtanh-RNNdidnotreducer
L
cestralsamplingalwaysresultedinsequencesthat
asmuchaswhenitwaspairedwithLSTM-RNN.
terminatedwithinLsteps,sincetheinduceddistri-
ExamplecontinuationsareshowninTable2. On
butionisthesameasthatoftheconsistentmodel.
prefixesthatledtonon-terminationwiththebase-
7RefertoAppendixDforthehyper-parameterranges. linemethod,thequalitytendstoimprovewiththe
8RefertoAppendixforresultswithwordtokenization. consistentvariantsincethecontinuationnowtermi-
Prefix OneDirectiondeliveredaperformanceof“KissYou
nucleus ”,andthealbum’ssecondalbum,“TheX@-@Files”,“TheA.”,“ThePreder”,“We’veHaveYou”,“
I’veYouWannaStay”,“TheDream”,“TheBide”,“MyAchievement”,“TheB.B.”,“ALife”...
c-nucleus ”,and“MyBoo”wasreleasedonSeptember29,2010.(cid:104)eos(cid:105)
Prefix Boulterstarredintwofilmsin2008,
nucleus andtheband’smusic,and“TheRiseofMonkey”,“TheOneWiththeWay”,“The“Always”,”“Always
Your”,“TheWift”,“TheBaste”,“TheSpecialWith”,“TheWay”,“TheSpecialWithYou”...
c-nucleus andthelatterwasreleasedintheUnitedStates.(cid:104)eos(cid:105)
Prefix Thisperiodofunhappinesswasthemakingof
Baseline the“mostimportant”ofthe“mad”,andthe““mostimportant”ofthe”“”,“themostimportant”,and
the“devil”,“The”,“TheOne”,“TheOne”,“TheOne”,“TheOne”,“TheOne”,“TheOne”,“
TheOne”,“TheOne”,“TheOne”,“TheOne”,“TheOne”,“TheOne”,“TheOne”,“TheOne”...
STRLM thefirstcommandmentofthepoem.(cid:104)eos(cid:105)
Prefix DuFu’smotherdiedshortlyafterhewas
Baseline amemberoftheOrderoftheOrderoftheOrderoftheOrderoftheOrderoftheOrderoftheOrderofthe
OrderoftheOrderoftheRepublicoftheRepublicoftheRepublicoftheRepublicoftheRepublicof...
STRLM amemberoftheOrderoftheBritishEmpire.(cid:104)eos(cid:105)
Table2: Continuationswithconsistentnucleussampling(µ=0.2)andself-terminatingLSTM((cid:15)=10−3).
nates. Notethatsincethemodel’snon-(cid:104)eos(cid:105)token ST (cid:15) r (%) perplexity
L
probabilities at each step are only modified by a
(cid:33) 10−2 00.00±0.0 229.09±9.2
multiplicativeconstant,thesamplingprocesscan (cid:33) 10−3 00.00±0.0 191.63±1.4
stillenterarepetitivecycle(e.g.,whentheconstant (cid:33) 10−4 00.02±0.02 188.36±2.2
iscloseto1),thoughitisguaranteedtoterminate. (cid:55) – 12.35±5.2 186.44±1.4
(cid:33) 10−2 0.00±0.0 219.71±9.2
Self-terminatingRLM. AsseeninTable3,the
(cid:33) 10−3 0.00±0.0 186.04±1.6
self-terminatingrecurrentlanguagemodelsarecon-
(cid:33) 10−4 0.18±0.35 183.57±2.3
sistentwithrespecttogreedydecoding,attheex- (cid:55) – 1.48±1.43 178.19±1.3
penseofperplexitycomparedtothevanillamodel.
ThevalueofεfromDefinition4.3,whichcontrols Table 3: Non-termination ratio (r L (%)) of greedy-
alower-boundonterminationprobabilityateach decodedsequencesandtestperplexityforSTRLMs.
step,influencesbothr andperplexity. Whenεis
L
toolarge(ε = 10−2),perplexitydegrades. When
εistoosmall(ε = 10−4),thelower-boundgrows
slowly,so(cid:104)eos(cid:105)isnotguaranteedtobetop-ranked
within L steps, resulting in a positive r . An ε
L
of10−3 balancedconsistencyandlanguagemodel-
ingquality,withazeronon-terminationratioand
perplexitywithin8pointsofthebaseline.
AsshowninFigure3,theself-terminatingmodel
matches the data length distribution better than
the baseline. Example decoded sequences are
shown in Table 2. For prefixes that led to non-
Figure3:Lengthsofgeneratedsequencesusinggreedy
terminationwiththebaseline,theself-terminating
decodingfromvanillaandself-terminatingLSTMs.
models yields finite sequences with reasonable
quality. The examples suggest that some cases
of degenerate repetition (Holtzman et al., 2019; scraped web pages (see Radford et al. (2019)).
Wellecketal.,2019)areattributedtoinconsistency. GPT-2 has been observed to produce repetitive
textwithgreedyandbeamsearch(Holtzmanetal.,
5.3 InconsistencyofGPT-2
2019).
WeperformafinalexperimentwithGPT-2117M,
a transformer language model pre-trained with Experimental setup. We use the Wikitext-103
maximumlikelihoodonWebText,acollectionof dataset (Merity et al., 2016), a large-scale collec-
NNR-hnat
MTSL
tion of Wikipedia articles with over 100 million r (%) perplexity
L
words and 260 thousand unique tokens. We split
GPT2-117M 37.91 20.92
thedatasetintosequencesaccordingtothedataset’s GPT2-117MST 00.00 27.25
newlineboundaries,thenspliteachsequenceintoa
Table 4: Non-termination ratio (r (%)) of greedy-
contextC andcontinuationY,resultinginadataset L
decodedsequencesandperplexityforGPT2-117Mand
of (C,Y) pairs. Each continuation ends in a spe-
theself-terminatingvariant(ST)onWikitext-103.
cial(cid:104)eos(cid:105)token. Weuseacontextsizeofk = 10
tokens, and discard sequences that are length k
orshorter. Theresultingdatasetcontains874,556 interesting direction is to investigate whether the
training,1,896validation,and2,162testpairs. lackofdecodinginmaximumlikelihoodlearning
Wefine-tunethepre-trainedGPT-2modelusing isacauseofinconsistency. Maximumlikelihood
maximumlikelihoodfor400ksteps,andselectthe learning fits the model p using the data distribu-
θ
model state with the lowest validation perplexity tion,whereasadecodedsequencefromthetrained
(evaluatedevery5ksteps). Eachtrainingbatchcon- model follows the distribution q induced by a
F
tainsamaximumof1024totaltokens. Weusethe decodingalgorithm. Sequence-levellearning,how-
implementationanddefaulthyper-parametersfrom ever, uses a decoding algorithm during training
the transformers library (Wolf et al., 2019). (e.g., Ranzato et al. (2016)), which we hypothe-
Wefine-tunetheself-terminatingGPT-2modelsin sizecanresultinagoodsequencegeneratorthatis
asimilarmanner,startingfromthepre-trainedGPT- consistentwithrespecttoincompletedecoding.
2modelandusingthesamehyper-parameters.
Eachmodelisevaluatedusinggreedydecoding 7 Conclusion
with a maximum sequence length of 500, which
wasselectedsothateachdecodedvalidationbatch Weextendedthenotionofconsistencyofarecur-
could fit in GPU memory. We define the non- rent language model put forward by Chen et al.
termination ratio (r ) using L = 500; this limit (2017) to incorporate a decoding algorithm, and
L
is more strict than the limit used in the preced- usedittoanalyzethediscrepancybetweenamodel
ing experiments (1500), yet still allows us to see and the distribution induced by a decoding algo-
large differences in generation behavior between rithm. Weprovedthatincompletedecodingisin-
themodelandthegroundtruth(e.g. seeFigure4). consistent, and proposed two methods to prevent
this: consistentdecodingandtheself-terminating
Results. Table4showsthenon-terminationratio
recurrentlanguagemodel. Usingasequencecom-
andperplexityofthebaselineandself-terminating
pletion task, we confirmed that empirical incon-
GPT-2 models. The self-terminating variant pre-
sistencyoccursinpractice,andthateachmethod
ventsnon-termination,atthecostofperplexity. The
preventsinconsistencywhilemaintainingthequal-
model here uses (cid:15) = 2.5 × 10−3, which we se-
ityofgeneratedsequences. Wesuspecttheabsence
lectedafterobservingthatathighervaluesof(cid:15),e.g.
ofdecodinginmaximumlikelihoodestimationasa
1.0×10−3,theself-terminatingmodelgenerated
causebehindthisinconsistency,andsuggestinves-
sequenceslongerthanthelimitusedtodetermine
tigatingsequence-levellearningasanalternative.
termination(500). Figure4showsthelengthdis-
tributionsofthebaselineGPT-2continuationsand
Acknowledgements
those of the self-terminating GPT-2. The GPT-
2 117M model generates many sequences at or
We thank Chris Dyer, Noah Smith and Kevin
nearthemaximumsequencelength(500),unlike
Knight for valuable discussions. This work was
theground-truthdata. Introducingself-termination
supportedbyNSFAward1922658NRT-HDR:FU-
shiftsthemasstowardsshortersequences,whose
TUREFoundations,Translation,andResponsibil-
lengthsarealsopresentintheground-truthdata.
ityforDataScience;SamsungAdvancedInstitute
of Technology (Next Generation Deep Learning:
6 FutureDirections
frompatternrecognitiontoAI);andSamsungRe-
Themethodsweproposedinthispaperresolvein- search (Improving Deep Learning using Latent
consistency by changing the decoding algorithm Structure). KCthankseBayandNVIDIAfortheir
ormodelparameterization. Anotherapproachisto support.
addressinconsistencyinthelearningphase. One
References John Lafferty, Andrew McCallum, and Fernando C N
Pereira. 2001. Conditional random fields: Prob-
Daniel Andor, Chris Alberti, David Weiss, Aliaksei abilistic models for segmenting and labeling se-
Severyn,AlessandroPresta,KuzmanGanchev,Slav quence data. ICML ’01 Proceedings of the Eigh-
Petrov,andMichaelCollins.2016. Globallynormal- teenthInternationalConferenceonMachineLearn-
ized transition-based neural networks. In 54th An- ing.
nual Meeting of the Association for Computational
Linguistics, ACL 2016 - Long Papers, volume 4, Ga´borMelis,ChrisDyer,andPhilBlunsom.2018. On
pages2442–2452. the state of the art of evaluation in neural language
models. In 6th International Conference on Learn-
DzmitryBahdanau,KyunghyunCho,andYoshuaBen- ingRepresentations,ICLR2018-ConferenceTrack
gio. 2015. Neural machine translation by jointly Proceedings.
learning to align and translate. In 3rd Inter-
Stephen Merity, Nitish Shirish Keskar, and Richard
national Conference on Learning Representations,
Socher. 2018. Regularizing and optimizing LSTM
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
language models. In 6th International Conference
ConferenceTrackProceedings.
on Learning Representations, ICLR 2018 - Confer-
enceTrackProceedings.
T.L.BoothandR.A.Thompson.1973. Applyingprob-
abilitymeasurestoabstractlanguages. IEEETrans-
StephenMerity,CaimingXiong,JamesBradbury,and
actionsonComputers,C-22(5):442–450.
RichardSocher.2016. Pointersentinelmixturemod-
els. ArXiv,abs/1609.07843.
John S Bridle. 1990. Probabilistic interpretation of
feedforward classification network outputs, with re- Kenton Murray and David Chiang. 2018. Correct-
lationshipstostatisticalpatternrecognition. InNeu- ing length bias in neural machine translation. In
rocomputing,pages227–236.Springer. Proceedings of the Third Conference on Machine
Translation:ResearchPapers,pages212–223,Brus-
YiningChen,SorchaGilroy,AndreasMaletti,Jonathan sels, Belgium. Association for Computational Lin-
May, and Kevin Knight. 2017. Recurrent neural guistics.
networks as weighted language recognizers. arXiv
preprintarXiv:1711.05408. Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
DarioAmodei,andIlyaSutskever.2019. Language
modelsareunsupervisedmultitasklearners. OpenAI
Kyunghyun Cho, Bart van Merrie¨nboer, Dzmitry Bah-
Blog,1(8):9.
danau,andYoshuaBengio.2014. Ontheproperties
ofneuralmachinetranslation: Encoder–decoderap-
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
proaches. In Proceedings of SSST-8, Eighth Work-
andWojciechZaremba.2016. Sequenceleveltrain-
shoponSyntax,SemanticsandStructureinStatisti-
ing with recurrent neural networks. In 4th Inter-
calTranslation,pages103–111,Doha,Qatar.Asso-
national Conference on Learning Representations,
ciationforComputationalLinguistics.
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
ConferenceTrackProceedings.
JeffreyLElman.1990. Findingstructureintime. Cog-
nitivescience,14(2):179–211. Pavel Sountsov and Sunita Sarawagi. 2016. Length
biasinencoderdecodermodelsandacaseforglobal
AngelaFan,MikeLewis,andYannDauphin.2018. Hi- conditioning. In Proceedings of the 2016 Confer-
erarchical neural story generation. arXiv preprint ence on Empirical Methods in Natural Language
arXiv:1805.04833. Processing, pages1516–1525, Austin, Texas.Asso-
ciationforComputationalLinguistics.
Edouard Grave, Armand Joulin, and Nicolas Usunier.
FelixStahlbergandBillByrne.2019. OnNMTsearch
2017. Improvingneurallanguagemodelswithacon-
tinuous cache. In 5th International Conference on errors and model errors: Cat got your tongue? In
Proceedings of the 2019 Conference on Empirical
LearningRepresentations,ICLR2017-Conference
Methods in Natural Language Processing and the
TrackProceedings.
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 3354–
Alex Graves. 2013. Generating sequences with
3360,HongKong,China.AssociationforComputa-
recurrent neural networks. arXiv preprint
tionalLinguistics.
arXiv:1308.0850.
Ilya Sutskever, James Martens, and Geoffrey Hinton.
Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997.
2011. Generating text with recurrent neural net-
Long short-term memory. Neural Computation,
works. In Proceedings of the 28th International
9(8):1735–1780.
ConferenceonMachineLearning,ICML2011.
Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Choi.2019. Thecuriouscaseofneuraltextdegener- Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
ation. arXivpreprintarXiv:1904.09751. Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessingSystems.
OriolVinyals, GoogleQuoc, andVLe.2015. ANeu-
ralConversationalModel. InICMLDeepLearning
Workshop.
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan,KyunghyunCho,andJasonWeston.2019. Neu-
raltextgenerationwithunlikelihoodtraining. arXiv
preprintarXiv:1908.04319.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, ClementDelangue, AnthonyMoi, Pier-
ric Cistac, Tim Rault, Re´mi Louf, Morgan Fun-
towicz, et al. 2019. Transformers: State-of-the-
art natural language processing. arXiv preprint
arXiv:1910.03771.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between hu-
man and machine translation. arXiv preprint
arXiv:1609.08144.
A AdditionalDefinitions p satisfying 1/p+1/q = 1. Then we have from
Ho¨lder’sinequality,forallv ∈ V andt,
Incontrasttogreedydecoding,beamsearchwith
width k, F beam-k, operates on the level of partial u(cid:62) vh
t
≤ (cid:107)u(cid:62) vh t(cid:107)
1
≤ (cid:107)h t(cid:107) p(cid:107)u v(cid:107)
q
< Bu+,
sequencesorprefixes.
whereu+ = max (cid:107)u (cid:107) . Notethat
DefinitionA.1(Prefix). Aprefixρ isanordered v∈V v q
t
collectionofitemsfromV. Thescoreofaprefixis (cid:18) (cid:19)
log(cid:88) eu(cid:62) vht+cv ≤ log maxeu(cid:62) vht+cv ×|V|
t v∈V
(cid:88) v∈V
s(ρ ) = logp (y = ρ [τ]|ρ [< τ],C),
t θ τ t t ≤ max{u(cid:62)h +c }+log|V|
v t v
τ=1 v∈V
< Bu++c++log|V|,
whereρ [τ]isatokenattimeτ fromρ .
t t
Starting from a set of empty prefixes, at each wherec+ = max v∈V c v. Foragiveny <t andcon-
iterationanewprefixsetisformedbyexpanding textC,
each prefix, then choosing the k highest scoring
logp ((cid:104)eos(cid:105)|y ,C)
expandedprefixes. θ <t
DefinitionA.2(Beamsearch). Beamsearchwith
=(u(cid:62) (cid:104)eos(cid:105)h t+c (cid:104)eos(cid:105))−log(cid:88) eu(cid:62) vht+cv
width k, F , generates a sequence from a v∈V
beam−k
recurrentlanguagemodelp
θ
bymaintainingasize- >(−Bu++c (cid:104)eos(cid:105))−(Bu++c++log|V|) > −∞,
k prefix set Ptop . Starting with Ptop = ∅, at
t 0 anditfollowsthatp ((cid:104)eos(cid:105)|y ,C) > ξ > 0for
each iteration t ∈ {1,2,...} beam search forms θ <t
top somestrictlypositiveconstantξ. Then
anewprefixsetP byexpandingthecurrentset,
t
(cid:83)
P = {ρ◦v|v ∈ V}(whereρ◦v iscon-
t ρ∈Ptop p (|Y| = ∞) = lim p (|Y| > t)
t−1 θ θ
catenation), then choosing the k highest scoring t→∞
elements: Ptop = argtop-ks(ρ).Anyρ ∈ Ptop end- = lim E[p θ(|Y| > t|C)]
t t t→∞
ρ∈Pt (cid:104) (cid:105)
ing with (cid:104)eos(cid:105) is restricted from being expanded = E lim p (|Y| > t|C)
θ
t→∞
further,andisaddedtoasetS. Beamsearchends
(cid:104) (cid:105)
whenS containsksequences,andreturnsthehigh- ≤ E lim(1−ξ)t = 0,
t→∞
estscoringsequenceinS.
andhencep isconsistent.
θ
B ProofofLemmasinSection3
Lemma3.3. Aconsistentdecodingalgorithmwith
Lemma 3.1. If a recurrent language model p is respect to a consistent recurrent language model
θ
consistent,p (|Y| = ∞|C) = 0foranyprobable decodes only probable sequences. That is, if
θ
contextC. q F(Y |C) > 0,thenp θ(Y |C) > 0foranyproba-
blecontextC.
Proof. SupposethereexistsaprobablecontextC˜
Proof. Suppose there exists a decoded sequence
suchthatp (|Y| = ∞|C˜) > 0. Then
θ Y˜ by F and probable context C˜ such that
q (Y˜ |C˜) > 0 but p (Y˜ |C˜) = 0. By Remark
p (|Y| = ∞) = E[p (|Y| = ∞|C)] F θ
θ θ 2.1, the sequence Y˜ is of infinite length and thus
≥ p(C˜)p θ(|Y| = ∞|C˜) > 0, q (|Y| = ∞|C˜) ≥ q (Y˜ |C˜) > 0,whichcontra-
F F
dictstheconsistencyofq byLemma3.1.
F
whichcontradictstheconsistencyofthemodelp .
θ
C ProofsforSection4
Lemma 3.2. A recurrent language model p θ is Theorem4.1. SupposearecurrentLMp θ hasuni-
consistentif(cid:107)h t(cid:107) p isuniformlyboundedforsome formly bounded (cid:107)h t(cid:107) p for some p ≥ 1. If a de-
p ≥ 1. codingalgorithmF satisfiesq F((cid:104)eos(cid:105)|y <t,C) ≥
p ((cid:104)eos(cid:105)|y ,C)foreveryprefixy andcontext
θ <t <t
Proof. Let B > 0 be an upper bound such that C, then the decoding algorithm F is consistent
(cid:107)h (cid:107) < B for all t. Let q be the conjugate of withrespecttothemodelp .
t p θ
Proof. By Lemma 3.2 the model p is con- For any subsequence y = (y ,...,y ) with y (cid:54)=
θ 1 l 1
sistent and p ((cid:104)eos(cid:105)|y ,C) > ξ for some (cid:104)eos(cid:105),
θ <t
positive value ξ. Thus, q ((cid:104)eos(cid:105)|y ,C) ≥
F <t
l
p θ((cid:104)eos(cid:105)|y <t,C) > ξ. Fort ≥ 1, (cid:89)
p (ρˆ◦y|ρˆ,C) = p (y |ρˆ◦y ,C)
θ θ i <i
q (|Y| > t|C) i=1
F
≤ p (y |ρˆ,C)
= q (y (cid:54)= (cid:104)eos(cid:105),··· ,y (cid:54)= (cid:104)eos(cid:105)|C) θ 1
F 1 t
< p ((cid:104)eos(cid:105)|ρˆ,C).
≤ (1−ξ)t. θ
Thus, ρˆ ◦ (cid:104)eos(cid:105) is the most probable sequence
Taking the limit t → ∞ and expectation over C,
among sequences starting with the prefix ρˆ, and
wehave
itfollowsthatρˆ◦(cid:104)eos(cid:105) ∈ S(ρˆ).
(cid:104) (cid:105) Thus,inS(ρˆ),thereare(k−1)sequencesstart-
q (|Y| = ∞) = E lim q (|Y| > t|C)
F C F
t→∞ ing with ρˆ◦v for v ∈ V \{(cid:104)eos(cid:105)}. By the same
≤ lim(1−ξ)t = 0, argument,ateachstepatleastonesequenceending
t→∞
with(cid:104)eos(cid:105)isaddedtoS(ρˆ),andthereforeattime
from which the decoding algorithm is consistent. step(B+k),ksequencesendingwith(cid:104)eos(cid:105)arein
S(ρˆ).
Note that the result set S by F (Defini-
beam−k
Theorem4.2. Greedydecodingisconsistentwith tion2.11)satisfies
respecttoanyself-terminatingrecurrentLM.
(cid:91)
S ⊆ S(ρ).
(cid:104)eos(cid:105)
Proof. Let p denote p ((cid:104)eos(cid:105)|y ,C) and
t θ <t ρ∈Ptop
a(cid:104)eos(cid:105) denote u(cid:62) h +c . By Definition 4.3 B
t (cid:104)eos(cid:105) t (cid:104)eos(cid:105)
wehave Sinceeachρ ∈ Ptop inducessequencesoflength
B
atmostB+k,wehave
(cid:104)eos(cid:105) (cid:104)eos(cid:105) (cid:104)eos(cid:105)
p = 1−σ(a )(1−p )
t t t−1
p (|Y| > B+k|C) = 0.
t θ
= 1− (cid:89) σ(a(cid:104)eos(cid:105) ) ≥ 1−(1−(cid:15))t+1.
t(cid:48)
Taking the expectation over C yields the consis-
t(cid:48)=0
tencyofthemodelp .
θ
Take B = −log2/log(1 − (cid:15)). We then have
(cid:104)eos(cid:105)
p > 1/2 for all t > B, which implies that
t
(cid:104)eos(cid:105)isalwaysthemostprobabletokenaftertime
stepB. Hence,thesequencelengthislessthanB
withprobability1.
Theorem4.3. Beamsearchwithwidthk,F ,
beam−k
isconsistentwithrespecttoanySTRLM.
Proof. LetS(ρ)bethesize-ksetofsequenceskept
byF thatstartwithaprefixρ.
beam−k
TakeB = −log2/log(1−(cid:15))asintheproofof
Theorem4.2. Supposethatthereexistsatleastone
prefixρˆ∈ Ptop whichdoesnotendwith(cid:104)eos(cid:105).
B
We first want to show that ρˆinduces at most k
more steps in beam search with width k, that is,
Y ∈ S(ρˆ)implies|Y| ≤ B+k.
We know from the proof of Theorem 4.2 that
an STRLM p satisfies: for any context C and
θ
v ∈ V \{(cid:104)eos(cid:105)},
p ((cid:104)eos(cid:105)|ρˆ,C) > p (v|ρˆ,C).
θ θ
Parameter Values
Parameter Values
HiddenSize {256,512,1024}
Dropout {0.1,0.3,0.5} HiddenSize {256,512,1024}
EmbeddingWeightTying {True,False} Dropout {0.1,0.3,0.5}
EmbeddingWeightTying {True,False}
Table5: Gridsearchspecification. Thevaluesselected
Table6: Gridsearchspecification. Thevaluesselected
fortheLSTM-RNNandtanh-RNN modelsareshown
fortheLSTM-RNNandtanh-RNN modelsareshown
inboldanditalics,respectively(wordtokenization).
inboldanditalics,respectively(BPEtokenization).
D AdditionalResultsandExperiment
Details Type #Train #Valid #Test |V| Avg.len
Word 78274 8464 9708 33182 24
Training. Each model is trained on a single BPE 83344 8721 10156 19483 28
Nvidia P40 GPU for up to 100 epochs, stopping
when validation perplexity does not decrease for Table7: Wikitext2statistics.
10consecutiveepochs.
Hyper-parameters. Tables 5,6 show the grid tanh-RNN LSTM-RNN
search specifications. All models were 2 layers
ancestral 0.00±0.0 0.00±0.0
andweretrainedwiththeAdamoptimizer.
greedy 6.07±5.6 1.03±0.3
beam-2 1.21±0.3 0.07±0.1
Model perplexities. Tables 10, 11 shows train
beam-4 0.29±0.1 0.00±0.0
andtestperplexitiesforthetanh-RNNandLSTM-
topk-2 0.84±0.8 0.00±0.0
RNN models using word and BPE tokenization,
topk-4 0.02±0.0 0.00±0.0
respectively. nucleus-0.2 2.49±0.2 0.76±0.3
nucleus-0.4 0.32±0.1 0.22±0.1
Additional example continuations. Table 12
showsadditionalgreedy-decodedcontinuationsus- Table 8: Non-termination ratio (r L (%)) of decoded
ingaself-terminatingLSTM-RNNandthebaseline sequencesusingancestralsamplingandincompletede-
codingmethods(wordtokenization).
LSTM-RNNwithBPEtokenization.
GPT-2lengthdistributions. Figure4showsthe
lengthdistributionsofground-truthcontinuations, ST (cid:15) r L(%) perplexity
continuationsfromGPT-2117M,andcontinuations (cid:33) 10−2 0.00±0.0 150.07±2.7
fromtheself-terminatingGPT-2117M. (cid:33) 10−3 0.00±0.0 138.01±0.6
(cid:33) 10−4 1.04±0.6 138.67±1.8
(cid:55) – 6.07±5.6 136.57±1.8
(cid:33) 10−2 0.00±0.0 101.24±0.3
(cid:33) 10−3 0.00±0.0 94.33±0.6
(cid:33) 10−4 0.94±0.5 94.15±0.8
(cid:55) – 1.03±0.3 91.86±0.4
Table 9: Non-termination ratio (r (%)) of greedy-
L
decoded sequences and test perplexity for self-
terminatingrecurrentmodels(wordtokenization).
model context perplexity
Figure4: Lengthsofground-truthandgreedy-decoded
tanh-RNN train 91.54±7.9
continuationsfromthebaselineGPT-2117Mandself-
tanh-RNN test 136.57±1.8
terminatingGPT-2117Mmodels((cid:15)=0.0025).
LSTM-RNN train 45.80±2.5
LSTM-RNN test 91.86±0.4
Table 10: Perplexities of trained recurrent language
models(wordtokenization).
NNR-hnat
MTSL
model context perplexity
tanh-RNN train 61.20±1.2
tanh-RNN test 186.44±1.4
LSTM-RNN train 72.72±2.4
LSTM-RNN test 178.39±1.2
Table 11: Perplexities of trained recurrent language
models(BPEtokenization).
Prefix Paynewasquotedassaying:“Withthealbum
nucleus ’s”album,“ThePredious”,“TheOneWiththeWind”,“I’veEver’tHaveYou’sMyWay”,“I’ve
EverIt’sJohnny”,“ThePredes”,““Always”,“ThePredatoryWas”,“TheDream”,“TheBaste”,”“
AlwaysBoy”,“MyDrum”,“TheSimpsons”,““AlwaysMan”,“The“SweetNight”,...
c-nucleus ’s”album,“ThePredious”,“TheOneWiththeWind”,“I’veEver’tHaveYou’sMyWay”(cid:104)eos(cid:105)
Prefix Ina2006interview,fellowactorBenWhish
nucleus ’sfather,ayoungactor,andaromanticrelationshipwiththeshow,“TheOneWheretheTheSimpsons”,“
ThePape”,“TheNextGeneration”,“TheSixthExtinction”,“We’tYouWannaStay”,“TheDream”,“
ThePredator”,“TheCollection”,“TheBigLear”,“ThePredor”,“ThePredation”,“MyBlue”,“
TheSimpsons”,“TheSixthExtinction”,“MyLove”,“TheRiseoftheYear”,“TheSimpsons”,“The
Predator”,“MyDream”,...
c-nucleus wasthefirsttimeinthefilm,andwaspublishedinthesameepisodeoftheseason.(cid:104)eos(cid:105)
Prefix MostofwhatisknownofDuFu’s
Baseline “the”,the”“great”,the”“”,“themostimportant”,“themostimportant”,“OdetotheNightingale”,“
OdetotheNightingale”,“OdetotheNightingale”,“OdetotheNightingale”,“OdetotheNightingale”,“
OdetotheNightingale”,“OdetotheNightingale”,“OdetotheNightingale”,...
STRLM Coty,wasa“oneofthemostimportant”oftheAmericansciencefiction.(cid:104)eos(cid:105)
Prefix HewasrelievedbyYanWu,afriendand
Baseline thefirstwifeoftheOrderoftheOrderoftheOrderoftheOrderoftheOrderoftheRepublicoftheRepublicof
theRepublicoftheRepublicoftheRepublicoftheRepublicoftheRepublicoftheRepublicoftheRepublic
oftheRepublicoftheRepublicoftheRepublicoftheRepublic...
STRLM thewifeoftheRoyalNavy.(cid:104)eos(cid:105)
Table12: Morecontinuationswithconsistentnucleussampling(µ=0.2)andself-terminatingLSTM((cid:15)=10−3)
withBPEtokenization.

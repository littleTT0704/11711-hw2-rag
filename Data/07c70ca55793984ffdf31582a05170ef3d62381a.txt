Prompt Consistency for Zero-Shot Task Generalization
ChuntingZhou∗1,JunxianHe∗1,XuezheMa2,TaylorBerg-Kirkpatrick3,GrahamNeubig1
1LanguageTechnologiesInstitute,CarnegieMellonUniversity
2 InformationSciencesInstitute,UniversityofSouthernCalifornia
3DepartmentofComputerScienceandEngineering,UniversityofCaliforniaSanDiego
{chuntinz,junxianh,gneubig}@cs.cmu.edu
xuezhema@isi.edu, tberg@eng.ucsd.edu
Abstract et al., 2021b). Zero-shot task generalization sug-
gestsapathtowardsgenericsystemsthatperform
One of the most impressive results of recent
a wide variety of NLP tasks with no annotated
NLP history is the ability of pre-trained lan-
examples. However, whileenticingconceptually,
guage models to solve new tasks in a zero-
zero-shotperformanceoftenremainsrelativelylow
shot setting. To achieve this, NLP tasks are
framed as natural language prompts, generat- compared to systems trained using even a small
ingaresponseindicatingthepredictedoutput. amountoftask-specificlabeleddata.
Nonetheless,theperformanceinsuchsettings
In this paper, we examine methods to make
often lags far behind its supervised counter-
PLMs better zero-shot learners using unlabeled
part,suggestingalargespaceforpotentialim-
text. Ourworkismotivatedbyconsistencytraining
provement. In this paper, we explore meth-
methods that regularize model predictions to be
ods to utilize unlabeled data to improve zero-
shot performance. Specifically, we take ad- invarianttoperturbation(e.g. noiseorparaphras-
vantage of the fact that multiple prompts can ing)oftheinputexamples. Consistencytrainingis
be used to specify a single task, and propose widelyusedinsemi-supervisedlearningliterature
toregularizepromptconsistency,encouraging asaneffectivetechniquetoutilizeunannotatedex-
consistent predictions over this diverse set of
amples(Bachmanetal.,2014;Sajjadietal.,2016;
prompts. Our method makes it possible to
Beyer et al., 2019; Xie et al., 2020a). It is often
fine-tunethemodeleitherwithextraunlabeled
understoodasatypeofsmoothnessregularization
trainingdata, ordirectlyontestinputatinfer-
ordataaugmentation(Xieetal.,2020a)andattains
ence time in an unsupervised manner. In ex-
periments,ourapproachoutperformsthestate- strong performance in semi-supervised learning.
of-the-art zero-shot learner, T0 (Sanh et al., Insteadofexample-levelconsistency,wepropose
2022), on 9 out of 11 datasets across 4 NLP toregularizepromptconsistency,whereamodelis
tasksbyupto10.6absolutepointsintermsof
regularized to make the same prediction across a
accuracy. The gains are often attained with a
diversesetofsynonymoustaskprompts. Prompt
smallnumberofunlabeledexamples.1
consistencyregularizationmakessenseintuitively
1 Introduction sincePLMsshouldberobustacrosssynonymous
prompts, whereas it is known that model predic-
While the past decade has demonstrated that pre-
tionsareempiricallyverysensitivetothewording
trainedlanguagemodels(PLMs)arepowerfultools
ofthetaskprompts(Jiangetal.,2020).
forimprovinggeneralizationfromtrainingdatasets
Specifically, we design a pairwise distillation
totestdatasets(Devlinetal.,2019;Liuetal.,2019;
loss that encourages consistency between every
Raffeletal.,2020), morerecentworkhasshown
pairofprompts(Figure1). Werefertoourmethod
that they can even perform zero-shot generaliza-
asswarmdistillation,andithastheadvantageof
tiontonewtaskswithoutanyannotatedexamples
being fully unsupervised, only requiring unanno-
(Brown et al., 2020; Wei et al., 2022; Sanh et al.,
tated inputs. Notably, unannotated examples are
2022). These systems leverage natural language
often relatively easy to collect. Drafting several
prompts that specify the task for the model and
prompts for a task is also far cheaper than anno-
represent different tasks in a unified format (Liu
tatinglabelsforeachexample–infact, thereare
∗Equalcontribution. Orderdeterminedbyswappingthe alreadywell-designedpromptsavailableforawide
oneinHeetal.(2022).
rangeofNLPtasks(Bachetal.,2022).
1Codeisavailableathttps://github.com/violet-zct/swarm-
distillation-zero-shot. Previousworkonexample-levelconsistencyreg-
2202
ceD
72
]LC.sc[
2v94000.5022:viXra
{Sentence}
Question: Was that sentence Positive 1. Sample pairs of prompts
positive or negative? Answer:
r<latexit sha1_base64="Z1SJ1kA7mZ1vAeNcfqjqU168mTQ=">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</latexit> (i),r(j) p(r)
⇠
Does the following sentence have
a positive or negative sentiment? Negative 2. Pseudo soft target from
Sentence: proves once again he has
{Sentence} r<latexit sha1_base64="iNoExZyc5PNYe0S1DuqYfMbHyRM=">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</latexit> (i)
n't lost his touch , bringing off PLM y<latexit sha1_base64="Db0BD9jMKdugd5lv/WDG1H3NVvs=">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</latexit>ˆ qˆ(yx,r(i))
a adm its tu ep de lr yb m idp de lr if no gr m fa in lc me . in an Someone just said to me ⇠ |
{Sentence}.
D ho a py po yu ? think they are sad or Happy 3. Distill to r<latexit sha1_base64="VdjH7YyWOxiSYXh+ILu2NLD87nA=">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</latexit> (j)
l<latexit sha1_base64="iLT6m02wzTUeM9Ouz1ffJYQXzdo=">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</latexit>ogp (r(j)(yˆ)r(j)(x))
Someone sent me an email with ✓ y | x
the sentence {Sentence}. Do you Good
think they are feeling good or
bad?
Unlabeled Input Apply Multiple Task Prompts Predict Prompt-Formatted Target Swarm Distillation
Figure1:Anexampleoftheproposedapproachinansentimentclassificationtask.Weapplymultiplesynonymous
prompts to the unlabeled example, then we regularize the consistency of the predictions from different prompts,
throughourswarmdistillationlossasdetailedinEq.2.
ularizationtypicallyminimizesaconsistencyloss bothsettings,weshowthatourswarmdistillation
alongwithasupervisedlossinasemi-supervised methodimprovestheaccuracyofthe3B-parameter
setting(Miyatoetal.,2018;Xieetal.,2020a). Re- T0modelon9outof11datasetsbyupto10.6abso-
cently,Elazaretal.(2021)performedexperiments lutepoints. Wefurtherscalemodelsizeupto11B
optimizingapromptconsistencylossinthecontext parameters,anddemonstratethatourapproachout-
of a relation prediction task, also incorporating a performsthe11B-parameterT0modelon4outof
supervisedversionofthemaskedlanguagemodel 4datasets. Remarkably,analysisimpliesthatthese
pretrainingobjective. Incontrast,we(1)optimize gainsareoftenpossiblewithonlytensofexamples,
anovelpromptconsistencylossalone,makingour suggestingasmallcomputationoverhead.
approachcompletelyunsupervisedandagnosticto
2 Prompt-basedZero-ShotTask
the model’s pretraining objective, and (2) experi-
mentonanddemonstratethepracticalityofsuchan Generalization
approachforabroadvarietyofNLPtasks. Notably,
Givenataskwheretheinputisdenotedasx ∈ X
this unsupervised setting poses additional learn-
andthegoalistopredicty ∈ Y,wefocusonthe
ing challenges: without explicit supervision, the
zero-shot task generalization setting: we aim to
modelmaysufferfromcatastrophicforgettingand
feedaPLMwithxtopredicty,wherethePLMis
even exhibit a form of collapse where the model
nevertrainedonthespecifictasktobeperformed.
alwaysmakesthesamepredictionsforanyinput.
Zero-shot task generalization goes beyond tradi-
Toaddressthisissue,weadopttwosimplestrate-
tional dataset generalization, as the model must
gies: (1)weutilizeparameter-efficienttuningtech-
generalize to new functions f : X → Y as op-
niques (Houlsby et al., 2019; He et al., 2022) to
posed to new input examples, x. Recently, the
only update a small number of extra parameters,
developmentofpromptingmethodshasadvanced
naturallymitigatingcatastrophicforgettingbyfix-
zero-shot task generalization by representing dif-
ingtheoriginalPLMparameters;(2)wepropose
ferenttasksinaunifiedformat(Liuetal.,2021b),
anunsupervisedcriteriontoselectthemodelcheck-
andseveralprompt-basedapproacheshaveattained
pointbeforeitfallsintoacollapsedlocaloptimum.
reasonable zero-shot performance (Brown et al.,
Inexperiments,webuildourmethodontopof 2020;Sanhetal.,2022;Weietal.,2022).
astate-of-the-artzero-shottasklearner,T0(Sanh A prompt r consists of an input template r ,
x
et al., 2022), and validate its performance on 11 an output template r , and metadata to re-format
y
datasets from 4 NLP tasks: natural language in- the original x and y into new prompt-formatted
ference,coreferenceresolution,wordsensedisam- input and target, r (x) and r (y). For exam-
x y
biguation,andsentencecompletion. Weperform ple, as shown in Figure 1, in a sentiment classi-
experimentsundertwosecenarios: (1)trainingthe fication task where we must predict positive or
modelwithunlabeledtrainingdata; or(2)tuning negative sentiment of the text, the input includes
the model with unlabeled test inputs directly. In the field Sentence and the target consists of the
field Label. An input template could be “Does theydirectlyusetheinputsofthetrainingsplitas
the following sentence have a positive or nega- unlabeledresourcestohelpfew-shotlearning.
tivesentiment? {Sentence}”,andthetargettem- Secondisthecasewhenunannotatedtestinputs
plate is “Choices[{label}]”. Here Choices is are available. This is almost always true for any
the metadata that is a list containing [Positive, task. We use the test split to mimic the setting.
Negative]tocorrespondtothenumericlabelids. While the limited number of unlabeled examples
Wenotethatsuchmetadataisprompt-specificand couldpotentiallylimittheeffectivenessofsomeun-
can differ with different prompts for the same supervisedlearningmethods,weshowin§4.4that
task – for instance, in Figure 1 the Choices ourmethodiseffectiveevenwithtenstohundreds
list of the last prompt on the bottom is [Good, ofunlabeledexamples.
Bad]. Inprompt-basedapproachesthePLMmod- Ontheotherhand,adiversesetofpromptsisnot
els the conditional probability q(y|x,r) through exceedinglydifficulttocollectpractically–draft-
p (r (y)|r (x)) where θ denotes the model pa- ingpromptsforeachtaskiseasierthanannotating
θ y x
rameters. InclassificationtaskswhereY isafinite labelsformanyexamples. Infact,thecommunity
labelset,q(y|x,r)isnormalizedoverthepossible effortshavepushedoutaPublicPoolofPrompts
labelsatinferencetimetopredicty: (P3) that contains thousands of prompts for hun-
dredsofNLPdatasetsalready(Bachetal.,2022).
p (r (y)|r (x))
θ y x
q(y|x,r) = . (1)
(cid:80) p (r (y(cid:48))|r (x)) 3.2 ThePromptConsistencyLoss
y(cid:48)∈Y θ y x
Consistency regularization is a method that cre-
IngenerationtaskswhereY isaninfinitesequence ates different views (e.g. paraphrases of text) of
space, the target template is typically instanti- theinputandregularizestheoutputstobecloseto
ated as the target itself, i.e. p θ(r y(y)|r x(x)) = eachother,andhasachievedsignificantsuccessin
p θ(y|r x(x)), then the output can be directly de- semi-supervised learning(Clark et al., 2018; Xie
coded through sequence decoding approaches. etal.,2020a,b). Whilepreviousmethodsuseanad-
Throughdesigningsuchpromptsforeachtask,all ditionalmoduletoperturbeachexampleandthen
NLPtaskssharethesamedataformat,andmodels optimize example-level consistency, we propose
trainedononetaskmaygeneralizetoothers. tooptimizeprompt-levelconsistencywhich(1)is
conceptuallysimple,and(2)canmitigatethefact
3 PromptConsistencyTraining
thatthepredictionsofPLMsaretypicallyinconsis-
tentwithdifferentpromptsforthesametask(Jiang
3.1 ProblemDefinition
et al., 2020; Elazar et al., 2021). Intuitively, we
Inthispaper,weaimtoexploreunannotatedexam- propose to regularize the predictions of different
plestoimproveprompt-basedzero-shottaskgen- promptsforagiveninputtobeclosetoeachother,
eralization. Formally, we are given an unlabeled using a pairwise distillation loss to draw the pre-
dataset in the task of interest {x 1,x 2,··· ,x N}, dictionsfromonepromptclosertothosefromthe
andweassumethedatasethasK differentprompts, other. Concretely,werandomlysampleafewpairs
(1) (1) (K) (K)
{(r x ,r y ),··· ,(r x ,r y )}. Ourgoalistouti- ofpromptsanddistillthepseudotargetyˆ fromone
lize these resources and adapt a PLM to predict promptr(i) totheotherpromptr(j),asillustrated
r y(y)conditionedonr x(x). Unlabeledinputsare inFigure1. Thelossfunctionisdefinedas:
often available in practice, we consider two such
L = −E E
scenariosinthepaper. x∼p d(x) r(i),r(j)∼p(r)
(2)
First, we consider the case when unannotated E logp (r(j)(yˆ)|r(j)(x)),
yˆ∼qˆ(y|x,r(i)) θ y x
examples from a non-test set are available. For
wherep (x)istheempiricaldatadistribution,p(r)
manyNLPtaskstheirinputsareplaintextsuchas d
is a uniform distribution over possible prompts,
reviews,documents,orquestionsandcanbeeasily
and qˆ(y|x,r) is the conditional target distribu-
collected(lesssoforotherNLPtasks,likenatural
tion defined as in Eq. 1 but with a stopping gra-
languageinferencetheinputsarepairedhypothe-
dient operator. We do not propagate gradients to
sesandpremisesthatcanbenon-trivialtoobtain
qˆ(y|x,r(i))followingMiyatoetal.(2018)andXie
automatically). In this paper, we test this setting
etal.(2020a).2 Stoppingthegradientofoneside
byutilizingtheinputsofthetrainingdataset. This
issimilartoSchickandSchütze(2021a,b)where 2Notethatqˆ(y|x,r)stillchangesaswetrainthemodel.
inapairwiseconsistencylossisalsoshowntohelp h<latexit sha1_base64="d5ckwfW3WMDZzxlyGdq+gKeYg54=">AAAB6nicbVBNSwMxEJ3Ur1q/qh69BIvgqexKUY9FLx4r2g9ol5JNs21oNlmSrFCW/gQvHhTx6i/y5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61jEo1ZU2qhNKdkBgmuGRNy61gnUQzEoeCtcPx7cxvPzFtuJKPdpKwICZDySNOiXXSw6iv+uWKV/XmwKvEz0kFcjT65a/eQNE0ZtJSQYzp+l5ig4xoy6lg01IvNSwhdEyGrOuoJDEzQTY/dYrPnDLAkdKupMVz9fdERmJjJnHoOmNiR2bZm4n/ed3URtdBxmWSWibpYlGUCmwVnv2NB1wzasXEEUI1d7diOiKaUOvSKbkQ/OWXV0nroupfVmv3tUr9Jo+jCCdwCufgwxXU4Q4a0AQKQ3iGV3hDAr2gd/SxaC2gfOYY/gB9/gBUNo3X</latexit> o
+
mitigatethecollapseissuewhereallinputsleadto
thesamepredictions(ChenandHe,2021). Differ-
pretrainedweight A<latexit sha1_base64="M+uVYotgCMPQeViIjxE1eJ1r9M8=">AAACBHicbVC7TsMwFHXKq5RXgLGLRYXEVCUIAWOBhbEg+pCaUDmu01q1nch2kKooAwu/wsIAQqx8BBt/g9NmgJYjXenonHt17z1BzKjSjvNtlZaWV1bXyuuVjc2t7R17d6+tokRi0sIRi2Q3QIowKkhLU81IN5YE8YCRTjC+yv3OA5GKRuJOT2LiczQUNKQYaSP17eqFRwX0ONKjIEhvs/s08DTlREGe9e2aU3emgIvELUgNFGj27S9vEOGEE6ExQ0r1XCfWfoqkppiRrOIlisQIj9GQ9AwVyOzx0+kTGTw0ygCGkTQlNJyqvydSxJWa8MB05teqeS8X//N6iQ7P/ZSKONFE4NmiMGFQRzBPBA6oJFiziSEIS2puhXiEJMLa5FYxIbjzLy+S9nHdPa2f3JzUGpdFHGVQBQfgCLjgDDTANWiCFsDgETyDV/BmPVkv1rv1MWstWcXMPvgD6/MHvQKYLQ==</latexit> 2Rb ⇥m
ent from traditional distillation that distills from W<latexit sha1_base64="0zdFZXw+ULy8Apzi3DN1i2B0yR0=">AAACBHicbVC7TsMwFHXKq5RXgLGLRYXEVCWoAsYKFsaC6ENqQuU4TmvVdiLbQaqiDiz8CgsDCLHyEWz8DU6bAVqOdKWjc+7VvfcECaNKO863VVpZXVvfKG9WtrZ3dvfs/YOOilOJSRvHLJa9ACnCqCBtTTUjvUQSxANGusH4Kve7D0QqGos7PUmIz9FQ0IhipI00sKtdjwrocaRHQZDdTu+z0NOUEwX5dGDXnLozA1wmbkFqoEBrYH95YYxTToTGDCnVd51E+xmSmmJGphUvVSRBeIyGpG+oQGaPn82emMJjo4QwiqUpoeFM/T2RIa7UhAemM79WLXq5+J/XT3V04WdUJKkmAs8XRSmDOoZ5IjCkkmDNJoYgLKm5FeIRkghrk1vFhOAuvrxMOqd196zeuGnUmpdFHGVQBUfgBLjgHDTBNWiBNsDgETyDV/BmPVkv1rv1MW8tWcXMIfgD6/MH47CYRQ==</latexit> Rd ⇥m
2
ateachermodeltoastudentmodel(Hintonetal., B<latexit sha1_base64="A0AmiLw4c/gO/zE6WI5dOFoaJ3g=">AAACBHicbVC7TsMwFHXKq5RXgLGLRYXEVCUIAWNVFsaC6ENqQmU7TmvVcSLbQaqiDiz8CgsDCLHyEWz8DU6bAVqOdKWjc+7VvffghDOlHefbKq2srq1vlDcrW9s7u3v2/kFHxakktE1iHsseRopyJmhbM81pL5EURZjTLh5f5X73gUrFYnGnJwn1IzQULGQEaSMN7GrTYwJ6EdIjjLPb6X0WeJpFVEE8Hdg1p+7MAJeJW5AaKNAa2F9eEJM0okITjpTqu06i/QxJzQin04qXKpogMkZD2jdUILPHz2ZPTOGxUQIYxtKU0HCm/p7IUKTUJMKmM79WLXq5+J/XT3V46WdMJKmmgswXhSmHOoZ5IjBgkhLNJ4YgIpm5FZIRkohok1vFhOAuvrxMOqd197x+dnNWazSLOMqgCo7ACXDBBWiAa9ACbUDAI3gGr+DNerJerHfrY95asoqZQ/AH1ucPsQOYJQ==</latexit> 2Rd ⇥b
2015),orpreviousconsistencytrainingthatasin-
gleteacherdistillstoseveralstudents(Clarketal., h<latexit sha1_base64="twZ/9ev5r5VNa3vcmoEAhxx9UuA=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0MOrzfrniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzUKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7G8y4AqZERNLKFPc3krYiCrKjE2nZEPwll9eJa2LqndZrd3XKvWbPI4inMApnIMHV1CHO2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gBLHo3R</latexit>
i
2018; Xie et al., 2020a), we perform distillation
Figure 2: A diagram of LoRA in the FFN sublayer.
amongaswarmofpromptswhereeachpromptisa
OnlytheLoRAparameters,AandB,areupdateddur-
teacherandstudentatthesametime,thusweterm
ingtraining.
ourmethodasswarmdistillation. Inourimplemen-
tation, we approximate the expectation over the reached. Tomitigatesuchcatastrophicforgetting
paired prompts (r(i),r(j)) with k randomly sam- andcollapseissues,weproposetwotechniques:
pledpairsfortrainingefficiency.
Prompt consistency is related to example- Parameter-efficient tuning: It has recently
levelconsistencywhenviewingdifferentprompt- been observed that updating a small number of
formatted inputs r(i) (x) as separated views of added parameters in a PLM is able to achieve
x
thesameexample,thusourswarmdistillationap- comparable performance to tuning all the param-
proachsharesspiritwithpreviousworkonexample- eters (Houlsby et al., 2019; Li and Liang, 2021;
level consistency training and can be understood Hu et al., 2022; He et al., 2022). Parameter-
similarlyfromtheperspectiveofunsuperviseddata efficient tuning methods naturally mitigate catas-
augmentation,smoothnessregularization,orlabel trophicforgettingandcollapsethroughfixingthe
propagation(Xieetal.,2020a). Inthispaper, we original PLM parameters. Specifically, we use
focus on classification tasks where Y is a finite LoRA (Hu et al., 2022), a low-rank adaptation
label set, while Eq. 2 can be directly applied to method for PLMs. As shown in Figure 2, LoRA
sequence generation tasks as well with sequence learnsalow-rankapproximationofthepretrained
distillation(KimandRush,2016). matrix updates: given a pretrained weight ma-
trix W ∈ Rd×m, LoRA learns to update it as
Ourapproachdiffersfrompreviousconsistency
W ← W +αBA, where B ∈ Rd×b,A ∈ Rb×m
trainingmethodswhichoftencombineanunsuper-
arelow-rankmatricesandα isahyperparameter,
visedconsistencylosswithasupervisedlossina
and only B and A are updated during training.
semi-supervisedsetting(Miyatoetal.,2018;Clark
b (cid:28) d is referred to as the bottleneck dimension.
etal.,2018;Xieetal.,2020a).Elazaretal.(2021)
Following He et al. (2022), we apply LoRA to
trytoimprovepromptconsistencyforarelationfill-
thefeed-forwardweightmatricesofeverylayerin
ingtaskwithapairwisetwo-sidedKLdivergence
the pretrained transformer (Vaswani et al., 2017)
loss,whiletheyalsooptimizeasupervisedversion
model. Inourpreliminaryexperiments,wefound
oftheoriginalPLMobjectivethatturnsouttobe
that LoRA is less likely to suffer from collapse,
important. Incontrast,ourapproachminimizesthe
whileonsomedatasetsthemodelstillcollapsesin
swarmdistillationlossinEq.2alone,andtherefore
the end even though it learns well in the middle.
iscompletelyunsupervisedandagnostictothepre-
Thismotivatesustodevelopacriteriontoselectthe
trainingobjective. However,thissettingalsoposes
model checkpoint before it falls into a collapsed
challengesinlearning,whichwediscussnext.
localoptimum,whichwedescribenext.
3.3 Training
Unsupervised model selection criterion: Our
Being trained without explicit supervision, the zero-shotsettingdoesnothavelabeledvalidation
PLMmayforgetwhatitlearnsduringpretraining data for model selection, and the swarm distilla-
since the unsupervised consistency loss is differ- tion objective is not an ideal selection criterion
entfromthepretrainingobjective. Also,wenote since it is minimized at collapse. Therefore, we
that prompt consistency may be achieved with a would like to have an unsupervised criterion that
trivial solution – if the predictions from each ex- encouragesconsistencybutsimultaneouslypenal-
ampleandeachpromptcollapsetothesamelabel izescollapse. Withthatinmind,wefocusonFleiss’
thenmaximalconsistencyamongpromptscanbe kappa(Fleiss,1971),acommonlyusedmetricto
assessthereliabilityofagreement. Inoursetting, 4 Experiments
Fleiss’ kappa expresses the extent to which the
Our experiments below are designed to (1) mea-
amountofagreementamongpromptsexceedswhat
surewhetherswarmdistillationisabletoimprove
wouldbeexpectedifallpromptsmadetheirpredic-
zero-shottaskgeneralization;and(2)analyzehow
tionsaccordingtothemarginalizeddistributionof
muchresource(numberofpromptsandunlabeled
labels. Thisdesigncomputesanotionof“relative
examples)ourmethoddemands.
consistency”andnaturallypenalizescollapse. For-
mally,letn bethenumberofpromptsthatpredict
ij
4.1 GeneralSetup
thej-thlabelforthei-thexample. Thereareatotal
ofNK predictionswhereN isthenumberofex- Datasets: FollowingSanhetal.(2022),weeval-
amplesandK isthenumberofprompts. Givenan uateourmethodon11NLPdatasetsacross4un-
examplex i,theagreementprobabilityp i computes seentasks. Theyare(1)naturallanguageinference:
thenormalizednumberofagreeingpromptpairs: ANLI (Nie et al., 2020) (there are three versions
(cid:88) ofANLIwithdifferentlevelsofdifficulty,which
p = n (n −1)/(K(K −1)), (3)
i ij ij we denote as ANLI R1/R2/R3), CB (De Marn-
j
effe et al., 2019), RTE (Wang et al., 2019a); (2)
thenthe“absoluteconsistency”P¯ is:
sentence completion: COPA (Roemmele et al.,
P¯ = (cid:88)N p /N. (4) 2011), HellaSwag (Zellers et al., 2019), Story
i
i=1 Cloze (Mostafazadeh et al., 2016); (3) corefer-
P¯ ismaximizedinthecaseofcollapse. However, ence resolution: WSC, Winogrande (Levesque
et al., 2012); and (4) word sense disambiguation:
Fleiss’kappaconsidersthemarginalizeddistribu-
WIC(PilehvarandCamacho-Collados,2019). We
tionoflabels: howlikelyaretwopromptsconsis-
accessthemusingHuggingFaceDatasets(Lhoest
tentiftheymakepredictionsrandomlyaccording
et al., 2021) and most of them are from the Su-
tothemarginalizedlabeldistribution? Thischance
probabilityP¯ is: perGLUEbenchmark(Wangetal.,2019a). Allof
e
thesedatasetsareclassification-based,predicting
(cid:88) (cid:88)N
P¯ = q2, q = n /(NK), (5) a discrete label from a finite set. Each of these
e j j ij
j i=1
datasetshasadiversesetofpromptsprovidedby
whereq j representsthemarginalizeddistribution thePublicPoolofPrompts(Sanhetal.,2022)The
of labels, i.e. p(y = j). P¯ e is large when col- number of prompts ranges from 4 to 15. Please
lapsehappensandonelabeldominatesintheentire refertoAppendixAfordetailedstatisticsofthese
corpus. Finally,Fleiss’kappaiscomputedas: datasets.
P¯ −P¯
κ = e , (6) Setup: WebuildourmethodontopofthePLM
1−P¯
e T0 (Sanh et al., 2022). T0 is an adapted version
where1−P¯ givesthedegreeofconsistencythat of the pretrained T5 model (Raffel et al., 2020)
e
isattainableabovechance,P¯−P¯ givesthedegree that is continually trained on multiple tasks with
e
ofconsistencyactuallyachievedabovechance. κ supervised,prompt-formattedexamples. T0outper-
rangesfrom-1to1. Eq.6naturallypenalizescol- formsGPT3(Brownetal.,2020)anddemonstrates
lapse,andinourexperiments,wealwaysobserve state-of-the-artperformanceinzero-shottaskgen-
amonotonicdecreaseofκwhencollapsehappens. eralization. Allthetasksthatwearestudyingare
Therefore, we select the model checkpoint after not included in T0’s training data. We focus our
which κ monotonically decreases.3 We empha- majorstudyontheT0modelversionwith3billion
size that we perform validation on the data that parameters(T0-3B),whilewealsoincluderesults
the model is trained on and do not require an ad- usingthelargestT0modelwith11billionparam-
ditionaldevelopmentdataset. Weincludeablation eters (T0-11B) on some datasets, due to the high
analysis for both LoRA and the model selection computationalcostoftrainingT0-11B.Thehyper-
componentsinAppendixCthatshowsthattheyare parameters(e.g.theoptimizationhyperparameters)
importantforthesuccessofourmethod. aretunedontheRTEdatasetwithitsvalidationset
andfixedforallotherdatasets. Weuseabottleneck
3Inmostofthesettings,thiscriterionisequivalenttousing
dimensionof1forLoRA.Completesetupdetails
maximalκasthecriterion,exceptforfewcaseswherethe
beginningoftrainingexhibitslargefluctuationsinκ. canbefoundinAppendixB.
T0-3B SelfDist.(train) SwarmDist.(train) SwarmDist.(test)
Task Dataset Ens. Med. Ens. Med. Ens. Med. Ens. Med.
RTE 64.6 64.1 64.9±0.2 63.8±0.1 75.2±0.8↑10.6 73.9±0.8↑9.8 75.2±0.2↑10.6 73.5±0.1↑9.4
CB 46.4 50.0 47.0±1.0 49.4±2.7 47.6±1.0↑1.2 48.2±0.0↓1.8 46.4±0.0↑0.0 48.8±1.0↓1.2
NLI ANLIR1 34.6 33.7 36.1±0.1 34.7±0.1 38.4±0.5↑3.8 35.7±0.4↑2.0 38.5±0.3↑3.9 35.7±0.5↑2.0
ANLIR2 33.7 33.4 35.3±0.1 33.2±0.2 37.9±0.8↑4.2 36.6±0.5↑3.2 37.7±0.2↑4.0 35.4±0.4↑2.0
ANLIR3 34.7 33.3 33.1±0.0 33.8±0.2 34.0±0.3↓0.7 34.6±0.1↑1.3 34.1±0.2↓0.6 33.5±0.0↑0.2
COPA 78.0 79.0 82.3±0.6 78.2±0.3 82.7±0.6↑4.7 79.0±0.5↑0.0 83.0±1.0↑5.0 79.7±0.6↑0.7
Compl. HellaSwag 27.8 27.5 32.5±0.2 32.7±0.3 34.2±0.2↑6.4 33.4±0.2↑5.9 33.7±0.6↑5.9 33.2±0.3↑5.7
StoryCloze 86.5 85.1 89.6±0.0 88.7±0.0 – – 87.3±0.1↑0.8 86.9±0.2↑1.8
Wino. 50.9 50.5 51.1±0.1 50.7±0.1 52.0±0.3↑1.1 51.4±0.0↑0.9 52.1±0.3↑1.2 51.2±0.2↑0.7
Coref.
WSC 69.2 64.4 69.2±0.0 64.6±0.3 58.3±1.1↓10.9 59.3±2.0↓5.1 57.7±0.0↓11.5 58.8±0.6↓5.6
WSD WIC 50.3 50.4 50.3±0.0 50.3±0.0 55.4±1.1↑5.1 54.4±0.7↑4.0 55.5±0.8↑5.2 54.8±0.5↑4.4
Table1: Accuracyresultsonthevalidationsetof11NLPdatasetsbasedontheT0-3Bmodel. SwarmDistillation
(train) and Swarm Distillation (test) use the unlabeled training split and validation split of datasets to train the
modelrespectively,correspondingtotraining-timeandtest-timetuning. TheStoryClozedatasetdoesnothavea
training split and its self distillation results are from tuning on the validation split. We report the mean and std
across3randomruns,andalsodenotetheabsoluteaccuracychangecomparedtotheT0-3Bbaseline.
4.2 Evaluation tuning (Sun et al., 2020; Wang et al., 2021): we
directlyadaptthe PLMonthetest data. Thisset-
Metrics: We use accuracy as the metric for all
tingisreasonable, aswewillalwayshaveaccess
datasets. Wereporttwodifferenttypesofaccuracy
tothetestinputsattesttime. Intuitively,theunla-
giventhatwehavemultipleprompts. Theensem-
beledtestsamplexoftenprovideshintsaboutthe
ble accuracy (Ens.) averages the output distribu-
distributionitwasdrawn,suggestingthatwemay
tions of multiple prompts and makes predictions
update the model before making the prediction.
accordingtoit. Ensemblingmultiplepromptshas
This scenario is attractive since it alleviates the
beenexploredbeforeandfoundsuperiortousinga
common distribution mismatch issue when there
singleprompt(Jiangetal.,2020;QinandEisner,
isadistributionshiftbetweenthetrainingandtest
2021). The median accuracy (Med.) within the
data. Comparedtotraining-timetuning,test-time
setofpromptsservesasaproxyfortheexpected
tuningtypicallyuseslessunlabeleddatainourex-
performance when users specify a single prompt
perimentssinceitusesthevalidationsplititself. In
and input a prompt-formatted example. As our
themajorexperiments,wefocusontheofflinetest-
approachassumesavailabilityofasetofprompts
timetuningwhereweassumeaccesstotheentire
forthedownstreamtask,anditisrelativelycheap
testdata4 andtrainourapproachonalltestexam-
to craft several prompts for a task, ensemble pre-
ples,whilein§4.4wewilldiscussthepotentialfor
diction is the better option given input x, and it
onlineadaptationwheredataarrivesinastream.
doesempiricallyyieldhigheraccuracyoverallthan
themedianforboththebaselineandourmethod. Baselines: As far as we know, there is no prior
Therefore,wewillreportbothnumbersbutmainly work studying unsupervised approaches for this
discuss ensemble accuracy. We report these met- prompt-basedtaskgeneralizationsetting,thusT0
ricsonthevalidationsplitofeachdataset. Werun isthemainbaselinethatwecompareourapproach
the experiments with 3 random seeds and report against. However, we still implement an abla-
themeanandstandarddeviation. tion baseline, self distillation, to separate the im-
provement from optimizing prompt consistency
Evaluationscenarios: Weprovideourmethods andpseudo-labeldistillation. Specifically,selfdis-
withdifferentunlabeledsourceswhichleadtotwo tillation minimizes the same loss as in Eq. 2 but
practicalscenariosduringevaluation: (1)training- with r(i) = r(j) – instead of pairwise distillation,
time tuning: we use the unlabeled training split the prompt always distills its own prediction to
fromthecorrespondingdatasettotrainthemodel. itself. This baseline can be viewed as a prompt
This is similar to traditional settings where train-
4Toclarify,testdataisnotthetestsplitofthedataset,but
ing and test data are different; and (2) test-time thedatathatweevaluateon,i.e.thevalidationsplit.
T0-11B SwarmDist. ANLIR1/R2/R3(3labels),HellaSwag(4labels),
Dataset Ens. Med. Ens. Med. Winogrande(2labels),andWIC(2labels). Inad-
WSC 63.5 62.5 65.4↑1.9 62.0↓0.5 dition, we observe that swarm distillation in the
RTE 83.8 82.0 86.6↑2.8 85.0↑3.0 test-timetuningsettingperformscomparablywell
HellaSwag 34.4 33.6 45.0↑10.6 43.0↑9.4 to the training-time one despite using much less
WIC 57.2 56.8 62.1↑4.9 60.7↑3.9
trainingdata,asshowninAppendixA.Itisworth
noting that prompt-based zero-shot task general-
Table2: AccuracybasedonT0-11B.
izationischallenging,forexample,T0witheven
11billionparametersreportsamedianaccuracyof
T0-3B NoShift SDonMNLI SDonQNLI
only∼ 40onANLIR1/R2/R3,33.7onHellaSwag,
RTE 64.6 75.2 75.5 72.9
and57.2onWIC(Sanhetal.,2022). Ourresults
ANLIR1 34.6 38.4 38.2 37.4
aresurelystillfarfromsatisfactory,yetwehopeto
ANLIR2 33.7 37.9 38.5 35.3
ANLIR3 34.7 34.0 36.9 34.4 inspirefutureresearchtoexploreunlabeleddatato
buildbetterzero-shotlearners.
Table 3: Ensemble accuracy on the distribution shift
settingbasedonT0-3B.“NoShift”representstheorig- Scaling to 11B parameters: We now evaluate
inal setting where we train the swarm distillation loss our method based on the largest version of T0
on the training split from the same dataset as the test model, T0-11B. T0-11B is a very powerful zero-
examples. “SDonMNLI/QNLI”representsswarmdis-
shotbaselinethatgreatlyoutperformsGPT3. Due
tillationtrainedonthetrainingsplitofMNLI/QNLI.
totheexpensivecomputationtotrainT0-11B,we
useonedatasetpertask,atotalof4datasetsasour
version of self-training, which has proven to ef- benchmark,andonlyrunwithonerandomseedin
fectively utilize unlabeled data (He et al., 2020; thetest-timetuningsetting. Resultsareshownin
Xie et al., 2020b; Zhang et al., 2020). We report Table2. SwarmdistillationoutperformsT0-11Bon
selfdistillationresultsinthetraining-timetuning all 4 datasets in terms of ensemble accuracy, and
settingonlyforsimplicity. notably,improvestheensembleaccuracyonHel-
laSwagfrom34.4to45.0withoutanyannotation.
4.3 Results Table1andTable2demonstratetheeffectiveness
ofswarmdistillationacrossdifferentmodelsizes.
How well does swarm distillation work? We
firstcompareswarmdistillationagainsttheT0-3B Robustnessofswarmdistillation: Themainex-
baseline. As shown in Table 1, the ensemble ac- perimentssofartrainandtestonexamplesfromthe
curacy of swarm distillation exceeds the T0-3B samedataset,thustheresultsarebasedontheas-
baseline on 9 out of 11 datasets in both training- sumptionthatthetrainingandtestdistributionsare
andtest-timetuningsettings. Particularly,ourap- similar. Torelaxthisassumption,hereweevaluate
proachimprovesthezero-shotperformanceonRTE swarmdistillationwithamoredifficultsettingby
byaround10absolutepointsinallcases. Ourap- collectingunlabeledtrainingexamplesfromother
proachslightlyhurtsensembleaccuracyofANLI datasets that are not listed in §4.1. Specifically,
R3andmedianaccuracyofCB,butisoverallcom- wefocusontheNLItask,trainswarmdistillation
parable on these two datasets. Compared to self ontheunlabeledtrainingsplitofMNLI(Williams
distillation, swarm distillation outperforms it on etal.,2018)andQNLI(Wangetal.,2019b)respec-
9 out of 11 datasets in terms of ensemble accu- tively,andevaluateitonRTEandANLI.Table3
racy,byupto10.3absolutepoints. Theseresults showsthatourproposedmethodstillperformswell
further confirm the effectiveness of encouraging in such a distribution shift setup — the models
prompt consistency. We note that swarm distilla- trainedonMNLIorQNLIhelpimprovetheT0-3B
tionseverelyfailsonWSCwitha10-pointaccuracy baseline in most cases. The results from MNLI
decreasecomparedtobothT0andselfdistillation, training are generally comparable to the original
this is because Fleiss’ kappa selects a bad model “NoShift”numbers,whileQNLItrainingcausesa
checkpoint,whileourapproachactuallyimproves mildaccuracydrop.
the performance on WSC in the middle of train-
4.4 Analysis
ingaswewilldiscussmorein§4.4. Notably,our
approachishelpfulonseveraldatasetswhereT0- Arepredictionsmoreconsistentacrossprompts
3B only shows nearly chance accuracy, such as after swarm distillation? We wonder whether
RTE CB ANLIR1 ANLIR2 ANLIR3 COPA HS Story. Wino. WSC WIC Avg.
T0-3B 0.644 0.440 0.221 0.189 0.170 0.586 0.164 0.765 0.396 0.255 0.398 0.384
SwarmDist. 0.662 0.254 0.145 0.156 0.177 0.699 0.402 0.862 0.509 0.462 0.517 0.440
Table 4: Fleiss’ kappa on 11 datasets based on T0-3B. Swarm distillation is trained on the training split of the
respectivedataset.
91
T0-3B 86.587.387.4
84.0
Swarm Distillation (Fleiss' kappa) 82.3
Swarm Distillation (oracle) 78.0
75.275.8
75
69.2 70.8
64.6
58 58.3 55.455.6
50.952.052.4
50.3
46.447.647.6
42
37.438.5 37.938.3 38.0
34.6 33.7 34.734.0 34.234.2
27.8
25
RTE CB ANLI R1 ANLI R2 ANLI R3 COPA HellaSwag Story Cloze Wino. WSC WIC
Figure3: AnalysisresultstocomparethemodelcheckpointsselectedbytheunsupervisedcriterionFleiss’kappa
withtheoraclemodelcheckpointsselectedbyvalidationaccuracy.
thegainsofswarmdistillationareattainedtogether 60
82.0 85.0
with more consistent predictions across different 8078.0 55 55.6 54.9 54.5
prompts. To this end, we report Fleiss’ kappa, a 70 5050.3
commonly used metric for group agreement as 60 COPA 45
ANLI R2
detailed in §3.3. Results are shown in Table 4.
50 40
37.6 37.4 37.7
Fleiss’kappaon8outof11datasetsincreasesaf-
40 37.0 38.5 3533.7 WIC
terswarmdistillation,whichbooststheaveraged 33.7 33.9 ANLI R2
30 30
Fleiss’kappaofT0-3Bby14.6%relatively. This 0 1 2 4 6 8 10 0 10 30 50 70 100300500
(a)Accuracyv.s.#prompts (b)Accuracyv.s.#examples
implies that swarm distillation facilitates prompt
consistency, and potentially improves the robust- Figure 4: Ensemble accuracy of swarm distillation on
nessofPLMstodifferentwordingofprompts. threeexampledatasetsbasedonT0-3B,demonstrating
theeffectofpromptsizeandunlabeleddatasize.
Does the unsupervised criterion select the best
modelcheckpoint? In§3.3,wediscussedusing
niques in few-shot learning settings, where good
Fleiss’kappatoselectthebestmodelcheckpoint
checkpointsmaybeselectedoutmoreeasily.
forevaluation,herewereporttheoracleaccuracy
numbers obtained by selecting the model check- Howmanypromptsdoweneed? Ourapproach
point with the best validation accuracy, and com- requires a diverse set of prompts to regularize
pare it to the one selected by Fleiss’ kappa. We promptconsistency. Hereweperformablationex-
compare the ensemble accuracy using T0-3B in perimentstounderstandtheeffectofthenumberof
thetraining-timetuningsetting,withresultsinFig- promptsontheperformance. WetakeCOPAand
ure 3. On most of the datasets, Fleiss’ kappa is ANLI R2 as example datasets which have 8 and
abletoachievenumbersclosetothebestones. On 15 prompts, respectively. We then vary the num-
all11datasets,ouroraclenumberoutperformsthe berofavailablepromptsbyrandomlysamplinga
T0-3B baseline. In Table 1 we show that swarm subset of prompts before training. We report the
distillation hurts the performance on WSC a lot, ensembleaccuracyofswarmdistillation(train)in
whileinFigure3swarmdistillation(oracle)infact Figure 4a. On both COPA and ANLI R2, we ob-
outperforms T0-3B, implying that the issue lies servegainsasweincreasethenumberofprompts
onmodelselection. Therefore,swarmdistillation from 0 (the baseline), yet the performance satu-
couldpotentiallyworkbetterifanannotateddevset rates very quickly and relatively stabilizes when
isavailableorwhenitiscombinedwithothertech- we provide 4 prompts. This implies that swarm
ycaruccA
elbmesnE
distillation is not prompt-hungry and could work Limitations
well with a small number of prompts. We note
Therearetwolimitationsofourwork: (1)Because
the with one prompt here Eq. 2 degenerates to a
our method is operated in a fully unsupervised
weakerversionofselfdistillationcomparedtothe
manner, there is no supervised development data
oneinTable1–selfdistillationinTable1utilizes
for us to either select the best model or tune hy-
allpromptsduringtrainingwhileweassumeaccess
perparameters. Thus, we propose to use Fleiss’
toonlyoneprompthere.
Kappa as our unsupervised development metric
How many unlabeled examples do we need? for model selection, which attains decent perfor-
We measure the effect of unlabeled data size. mance in most cases. However, we also observe
Specifically, we randomly sample a subset of ex- onveryfewdatasetsthattheproposedmetricfails
amplesfromthetrainsplitfortrainingandreport toselectthebestcheckpointsandhurtthemodel’s
resultsontheentirevalidationdataset. Resultson performance. As discussed in §4.4, our method
WIC and ANLI R2 are shown in Figure 4b. No- can be combined with few-shot learning where a
tably,swarmdistillationisabletooutperformthe fewlabeleddataareprovided,andwebelievethis
baselines(#examples=0)byalargemarginonboth canlargelyalleviatetheissuesofmodelselection
datasetswithonly10unlabeledexamples,andthe in the unsupervised setting. (2) The other limi-
performance starts to saturate quickly afterward. tation and at the same time an advantage of our
Theseresultssuggestthatswarmdistillationisnot methodisthattheproposedmethodcanworkwell
data-hungry and works reasonably well with few evenwith10unlabeleddatapoints. Thiscertainly
unlabeled examples, allowing swarm distillation makesourmethodagoodcandidatefortheonline
toremainasarelativelylightapproachwhiletyp- settingwherebatchesoftestdatacomeinastream.
icalunsupervisedtraining(e.g. pretraining)often However,aswediscussedin§4.4,theperformance
requires a large amount of data and computation. ofourmodelsaturatesquicklyasweincreasethe
Also,wearguethatthephenomenondemonstrated numberofunlabeleddata,whichmeanstheperfor-
in the results implies that swarm distillation may manceofourmethodcannotscalewellwithtonsof
beappliedtotheonlinesettingoftest-timetuning, unlabeleddatalikeself-supervisedpretraining. As
where the batches of test data arrive in a stream. discussedin§5,weexpectcombiningourmethod
Onlinetest-timetuningisapracticalsettinginreal with few-shot learning setting / pre-training can
life,andweleaveitasfutureworktostudy. leadtofurtherimprovementsasthesupervisedsig-
nalsmayguidethemodeltoabetterlocaloptimum.
5 Discussion
EthicsStatement
Inthispaper,weexplorepromptconsistencyreg-
ularization to make PLMs better zero-shot learn-
SimilartoT0,thisworkaimstoproduceanopen-
ers. Ourapproach,swarmdistillation,utilizesunla-
ended system that could perform all text-based
beledexamplestoattainzero-shotgains. Whilewe
tasksthroughdesigningdifferentprompts. While
useswarmdistillationinapost-adaptationmanner,
theperformanceofGPT3,T0,andthisworkisfar
itcouldbepotentiallycombinedwiththepretrain-
fromthepracticallevelonunseentasks,weexpect
ing objectives in the pretraining stage (e.g. the
agreatlyimprovedprompt-basedsysteminthefu-
multi-prompttrainingloss(Sanhetal.,2022;Wei
turecouldbebuilttohelpperformmanydailytasks
etal.,2022)),orevenwithannotateddatainfew-
inreallife. However,theresultedmodelinthispa-
shotlearningsettings. Combiningswarmdistilla-
per also admits the same ethics concerns that T0
tionwiththeseotherlossesmayeasilybypassthe
has. Forexample,theunrestricteduseofprompts
modelcollapseissuesincetheotherlosstypically
mayeasilytriggeroffensivegenerationsorprivate
discouragesthecollapsedlocaloptimum.
informationleakage,andhowtofixunwantedLM
behaviors is still an active research problem (Liu
Acknowledgement
etal.,2021a;Perezetal.,2022).
WethankVictorSanhandColinRaffelforhelpon
using the T0 model. This work was supported in
References
partbytheCMU-PortugalMAIAProject,aBaidu
PhDFellowshipforJunxianHe,andaCMUPresi-
Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Al-
dentialFellowshipforChuntingZhou. bert Webson, Colin Raffel, Nihal V. Nayak, Ab-
heesht Sharma, Taewoon Kim, M Saiful Bari, for Computational Linguistics: Human Language
Thibault Fevry, Zaid Alyafeai, Manan Dey, An- Technologies, Volume 1 (Long and Short Papers),
drea Santilli, Zhiqing Sun, Srulik Ben-David, Can- pages4171–4186,Minneapolis,Minnesota.Associ-
wenXu,GunjanChhablani,HanWang,JasonAlan ationforComputationalLinguistics.
Fries, Maged S. Al-shaibani, Shanya Sharma, Ur-
mishThakker,KhalidAlmubarak,XiangruTang,Xi- Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-
angruTang,MikeTian-JianJiang,andAlexanderM. lasha Ravichander, Eduard Hovy, Hinrich Schütze,
Rush. 2022. Promptsource: An integrated devel- and Yoav Goldberg. 2021. Measuring and im-
opment environment and repository for natural lan- proving consistency in pretrained language models.
guageprompts. Transactions of the Association for Computational
Linguistics,9:1012–1031.
Philip Bachman, Ouais Alsharif, and Doina Precup.
2014. Learning with pseudo-ensembles. In Ad-
JosephLFleiss.1971. Measuringnominalscaleagree-
vances in Neural Information Processing Systems
ment among many raters. Psychological bulletin,
27: AnnualConferenceonNeuralInformationPro-
76(5):378.
cessing Systems 2014, December 8-13 2014, Mon-
treal,Quebec,Canada,pages3365–3373.
JunxianHe,JiataoGu,JiajunShen,andMarc’Aurelio
Ranzato. 2020. Revisiting self-training for neural
LucasBeyer,XiaohuaZhai,AvitalOliver,andAlexan-
sequence generation. In 8th International Confer-
der Kolesnikov. 2019. S4L: self-supervised semi-
ence on Learning Representations, ICLR 2020, Ad-
supervised learning. In 2019 IEEE/CVF Interna-
dis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
tionalConference onComputer Vision, ICCV 2019,
view.net.
Seoul, Korea (South), October 27 - November 2,
2019,pages1476–1485.IEEE.
JunxianHe,ChuntingZhou,XuezheMa,TaylorBerg-
TomB.Brown,BenjaminMann,NickRyder,Melanie Kirkpatrick, andGrahamNeubig.2022. Towardsa
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind unifiedviewofparameter-efficienttransferlearning.
Neelakantan,PranavShyam,GirishSastry,Amanda In International Conference on Learning Represen-
Askell, Sandhini Agarwal, Ariel Herbert-Voss, tations.
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Clemens Winter, Christopher Hesse, Mark Chen, Distillingtheknowledgeinaneuralnetwork. ArXiv
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin preprint,abs/1503.02531.
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Amodei.2020. Languagemodelsarefew-shotlearn- Bruna Morrone, Quentin de Laroussilhe, Andrea
ers. InAdvancesinNeuralInformationProcessing Gesmundo, Mona Attariyan, and Sylvain Gelly.
Systems33: AnnualConferenceonNeuralInforma- 2019. Parameter-efficienttransferlearningforNLP.
tion Processing Systems 2020, NeurIPS 2020, De- In Proceedings of the 36th International Confer-
cember6-12,2020,virtual. ence on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, volume 97 of
XinleiChenandKaimingHe.2021. Exploringsimple Proceedings of Machine Learning Research, pages
siamese representation learning. In Proceedings of 2790–2799.PMLR.
theIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages15750–15758.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
KevinClark,Minh-ThangLuong,ChristopherD.Man-
WeizhuChen.2022. LoRA:Low-rankadaptationof
ning, and Quoc Le. 2018. Semi-supervised se-
largelanguagemodels. InInternationalConference
quence modeling with cross-view training. In Pro-
onLearningRepresentations.
ceedingsofthe2018ConferenceonEmpiricalMeth-
ods in Natural Language Processing, pages 1914–
ZhengbaoJiang, FrankF.Xu, JunAraki, andGraham
1925, Brussels, Belgium. Association for Computa-
Neubig. 2020. How can we know what language
tionalLinguistics.
models know? Transactions of the Association for
ComputationalLinguistics,8:423–438.
Marie-CatherineDeMarneffe,MandySimons,andJu-
dithTonhauser.2019. Thecommitmentbank: Inves-
tigating projection in naturally occurring discourse. Yoon Kim and Alexander M. Rush. 2016. Sequence-
In proceedings of Sinn und Bedeutung, volume 23, level knowledge distillation. In Proceedings of the
pages107–124. 2016 Conference on Empirical Methods in Natu-
ralLanguageProcessing,pages1317–1327,Austin,
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Texas.AssociationforComputationalLinguistics.
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under- Diederik P Kingma and Jimmy Ba. 2015. Adam: A
standing. In Proceedings of the 2019 Conference methodforstochasticoptimization. InInternational
of the North American Chapter of the Association ConferenceonLearningRepresentations.
Hector Levesque, Ernest Davis, and Leora Morgen- Pushmeet Kohli, and James Allen. 2016. A cor-
stern. 2012. The winograd schema challenge. In pusandclozeevaluationfordeeperunderstandingof
Thirteenth International Conference on the Princi- commonsense stories. In Proceedings of the 2016
plesofKnowledgeRepresentationandReasoning. Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Quentin Lhoest, Albert Villanova del Moral, Yacine LanguageTechnologies,pages839–849,SanDiego,
Jernite, AbhishekThakur, PatrickvonPlaten, Suraj California. Association for Computational Linguis-
Patil,JulienChaumond,MariamaDrame,JulienPlu, tics.
Lewis Tunstall, Joe Davison, Mario Šaško, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis, Yixin Nie, Adina Williams, Emily Dinan, Mohit
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Bansal,JasonWeston,andDouweKiela.2020. Ad-
Patry, Angelina McMillan-Major, Philipp Schmid, versarial NLI: A new benchmark for natural lan-
Sylvain Gugger, Clément Delangue, Théo Matus- guageunderstanding. InProceedingsofthe58thAn-
sière, Lysandre Debut, Stas Bekman, Pierric Cis- nual Meeting of the Association for Computational
tac, Thibault Goehringer, Victor Mustar, François Linguistics, pages 4885–4901, Online. Association
Lagunas,AlexanderRush,andThomasWolf.2021. forComputationalLinguistics.
Datasets: Acommunitylibraryfornaturallanguage
EthanPerez,SaffronHuang,FrancisSong,TrevorCai,
processing. InProceedingsofthe2021Conference
Roman Ring, John Aslanides, Amelia Glaese, Nat
onEmpiricalMethodsinNaturalLanguageProcess-
McAleese, and Geoffrey Irving. 2022. Red team-
ing: System Demonstrations, pages 175–184, On-
ing language models with language models. ArXiv
line and Punta Cana, Dominican Republic. Associ-
preprint,abs/2202.03286.
ationforComputationalLinguistics.
Mohammad Taher Pilehvar and Jose Camacho-
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Collados. 2019. WiC: the word-in-context dataset
Optimizing continuous prompts for generation. In for evaluating context-sensitive meaning represen-
Proceedings of the 59th Annual Meeting of the tations. In Proceedings of the 2019 Conference
Association for Computational Linguistics and the of the North American Chapter of the Association
11thInternationalJointConferenceonNaturalLan- for Computational Linguistics: Human Language
guage Processing (Volume 1: Long Papers), pages Technologies, Volume 1 (Long and Short Papers),
4582–4597, Online. Association for Computational pages1267–1273,Minneapolis,Minnesota.Associ-
Linguistics. ationforComputationalLinguistics.
Alisa Liu, Maarten Sap, Ximing Lu, Swabha Guanghui Qin and Jason Eisner. 2021. Learning how
Swayamdipta, Chandra Bhagavatula, Noah A. toask:QueryingLMswithmixturesofsoftprompts.
Smith,andYejinChoi.2021a. DExperts: Decoding- InProceedingsofthe2021ConferenceoftheNorth
timecontrolledtextgenerationwithexpertsandanti- American Chapter of the Association for Computa-
experts. In Proceedings of the 59th Annual Meet- tional Linguistics: Human Language Technologies,
ingoftheAssociationforComputationalLinguistics pages 5203–5212, Online. Association for Compu-
andthe11thInternationalJointConferenceonNat- tationalLinguistics.
uralLanguageProcessing(Volume1:LongPapers),
pages6691–6706,Online.AssociationforComputa- ColinRaffel,NoamShazeer,AdamRoberts,Katherine
tionalLinguistics. Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the lim-
PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang, its of transfer learning with a unified text-to-text
Hiroaki Hayashi, and Graham Neubig. 2021b. Pre- transformer. JournalofMachineLearningResearch,
train, prompt, and predict: A systematic survey of 21(140):1–67.
prompting methods in natural language processing.
Jie Ren, Samyam Rajbhandari, Reza Yazdani Am-
ArXivpreprint,abs/2107.13586.
inabadi, Olatunji Ruwase, Shuangyan Yang, Min-
jia Zhang, Dong Li, and Yuxiong He. 2021. Zero-
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
offload: Democratizingbillion-scalemodeltraining.
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
arXivpreprintarXiv:2101.06840.
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap- Melissa Roemmele, Cosmin Adrian Bejan, and An-
proach. ArXivpreprint,abs/1907.11692. drew S Gordon. 2011. Choice of plausible alterna-
tives: Anevaluationofcommonsensecausalreason-
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, ing. In2011AAAISpringSymposiumSeries.
and Shin Ishii. 2018. Virtual adversarial training:
a regularization method for supervised and semi- Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tas-
supervised learning. IEEE transactions on pat- dizen.2016. Regularizationwithstochastictransfor-
ternanalysisandmachineintelligence,41(8):1979– mationsandperturbationsfordeepsemi-supervised
1993. learning. In Advances in Neural Information Pro-
cessing Systems 29: Annual Conference on Neural
NasrinMostafazadeh,NathanaelChambers,Xiaodong InformationProcessingSystems2016,December5-
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, 10,2016,Barcelona,Spain,pages1163–1171.
Victor Sanh, Albert Webson, Colin Raffel, Stephen International Conference on Learning Representa-
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine tions,ICLR2019,NewOrleans,LA,USA,May6-9,
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, 2019.OpenReview.net.
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Tae- Dequan Wang, Evan Shelhamer, Shaoteng Liu,
woon Kim, Gunjan Chhablani, Nihal Nayak, De- BrunoA.Olshausen,andTrevorDarrell.2021. Tent:
bajyoti Datta, Jonathan Chang, Mike Tian-Jian Fully test-time adaptation by entropy minimization.
Jiang, Han Wang, Matteo Manica, Sheng Shen, In9thInternationalConferenceonLearningRepre-
Zheng Xin Yong, Harshit Pandey, Rachel Bawden, sentations, ICLR2021, VirtualEvent, Austria, May
Thomas Wang, Trishala Neeraj, Jos Rozen, Ab- 3-7,2021.OpenReview.net.
heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-
JasonWei,MaartenBosma,VincentZhao,KelvinGuu,
sonAlanFries,RyanTeehan,TevenLeScao,Stella
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Biderman, Leo Gao, Thomas Wolf, and Alexan-
Dai, and Quoc V Le. 2022. Finetuned language
derMRush.2022. Multitaskpromptedtrainingen-
modelsarezero-shotlearners. InInternationalCon-
ableszero-shottaskgeneralization. InInternational
ferenceonLearningRepresentations.
ConferenceonLearningRepresentations.
Adina Williams, Nikita Nangia, and Samuel Bowman.
Timo Schick and Hinrich Schütze. 2021a. Exploiting
2018. A broad-coverage challenge corpus for sen-
cloze-questions for few-shot text classification and
tenceunderstandingthroughinference. InProceed-
natural language inference. In Proceedings of the
ingsofthe2018ConferenceoftheNorthAmerican
16thConferenceoftheEuropeanChapteroftheAs-
Chapter of the Association for Computational Lin-
sociation for Computational Linguistics: Main Vol-
guistics: Human Language Technologies, Volume
ume, pages 255–269, Online. Association for Com-
1 (Long Papers), pages 1112–1122, New Orleans,
putationalLinguistics.
Louisiana. Association for Computational Linguis-
tics.
TimoSchickandHinrichSchütze.2021b. It’snotjust
size that matters: Small language models are also
QizheXie,ZihangDai,EduardH.Hovy,ThangLuong,
few-shotlearners. InProceedingsofthe2021Con-
andQuocLe.2020a. Unsuperviseddataaugmenta-
ferenceoftheNorthAmericanChapteroftheAsso-
tionforconsistencytraining. InAdvancesinNeural
ciationforComputationalLinguistics: HumanLan-
Information Processing Systems 33: Annual Con-
guage Technologies, pages 2339–2352, Online. As-
ference on Neural Information Processing Systems
sociationforComputationalLinguistics.
2020,NeurIPS2020,December6-12,2020,virtual.
Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Qizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and
AlexeiA.Efros,andMoritzHardt.2020. Test-time Quoc V. Le. 2020b. Self-training with noisy stu-
training with self-supervision for generalization un- dent improves imagenet classification. In 2020
derdistributionshifts. InProceedingsofthe37thIn- IEEE/CVFConferenceonComputerVisionandPat-
ternationalConferenceonMachineLearning,ICML tern Recognition, CVPR 2020, Seattle, WA, USA,
2020,13-18July2020,VirtualEvent,volume119of June13-19,2020,pages10684–10695.IEEE.
Proceedings of Machine Learning Research, pages
9229–9248.PMLR. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob a machine really finish your sentence? In Pro-
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz ceedings of the 57th Annual Meeting of the Asso-
Kaiser, and Illia Polosukhin. 2017. Attention is all ciation for Computational Linguistics, pages 4791–
you need. In Advances in Neural Information Pro- 4800,Florence,Italy.AssociationforComputational
cessing Systems 30: Annual Conference on Neural Linguistics.
InformationProcessingSystems2017,December4-
9,2017,LongBeach,CA,USA,pages5998–6008. YuZhang,JamesQin,DanielSPark,WeiHan,Chung-
Cheng Chiu, Ruoming Pang, Quoc V Le, and
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Yonghui Wu. 2020. Pushing the limits of semi-
AmanpreetSingh, JulianMichael, FelixHill, Omer supervised learning for automatic speech recogni-
Levy, and Samuel R. Bowman. 2019a. Superglue: tion. ArXivpreprint,abs/2010.10504.
A stickier benchmark for general-purpose language
understandingsystems. InAdvancesinNeuralInfor-
mation Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver,
BC,Canada,pages3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019b.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In 7th
Task Dataset #trainset #validationset #labels #prompts reported in Sanh et al. (2022), except for COPA
RTE 2,490 277 2 10 where our T0 median number is higher than the
CB 250 57 3 15
originallyreportedone.
NLI ANLIR1 16,946 1000 3 15
ANLIR2 45,460 1000 3 15 Duringtraining,ateachupdatewefirstsample
ANLIR3 100,459 1200 3 15
oneinputexamplexandapplyallthepromptsto
COPA 400 100 2 8 reformatitasr1(x),··· ,rK(x),thenweperform
Compl. HellaSwag 39,905 10,042 4 4 x x
StoryCloze - 1,871 2 5 inference for them and randomly shuffle the pre-
Winogrande 40,398 1,267 2 5 dictions. Next we iterate over them with a batch
Coref.
WSC 554 104 2 10
sizeof5/10(3B/11B)5 andusetheshuffledpredic-
WSD WIC 5,428 637 2 10
tionstosupervisethemtocomputethedistillation
Table5: Statisticsofthedatasets loss,thisimplementstheswarmdistillationmech-
anisminEq.2andamountstoapproximatingthe
expectation over paired prompts with K random
A Datasets
pairs. Weaccumulatethegradientsfor16stepsfor
We present the statistics of the 11 datasets in Ta- one update so that each gradient descent is com-
ble5. Forthetraining-timetuningscenario,weuse putedfrom16dataexamples. Andweuse1A40
up to 10,000 data points from the training set for GPU (45GB memory) to train the 3B model and
trainingifthetrainsetcontainsmorethan10,000 4 A40 GPUs with DeepSpeed Zero-2 (Ren et al.,
datapoints. 2021)totrainthe11Bmodel. Ingeneral,training
convergesprettyfastandtakesaround1-3GPU
B ExperimentalSetup hoursforthe3Bmodeland2-6hoursforthe11B
modeldependingonearlystoppointsofdifferent
B.1 LoRASetup
datasets. We use Adam (Kingma and Ba, 2015)
WeuseLoRA(Huetal.,2022)asourparameter- as the optimizer with β = 0.9, β = 0.98 and
1 2
efficient tuning model and set the bottleneck di- (cid:15) = 1e−6.
mensionofLoRAweightmatricestobe1forboth FortheTransformer(Vaswanietal.,2017)mod-
3B and 11B models. We emphasize that the lin- elswithmodeldimensiond,thefeed-forwardinter-
ear mapping matrix B (or A) in LoRA needs to mediatedimensionmandnumberoflayersl,the
beinitializedasazeromatrixtoensuretheoutput additional parameters used in LoRA with bottle-
distributionafteraddingLoRAlayersisthesame neckdimensionbiscalculatedasb∗(m+d)∗2∗l∗2.
astheoriginalPLMbeforetraining,otherwise,the Aswesetbtobe1forboththe3Band11Bmod-
zero-shotabilityofPLMswouldbebrokenupon els,theadditionalnumberofLoRAparametersis
initializationandthereisnosupervisiontolearnit 1,671,168 for the T0-3B model (d = 1024,m =
back. Forbothmodels,wesetthedropoutprobabil- 16384,l = 24) and 6,389,760 for the T0-11B
ityforthetheLoRAintermediaterepresentations model(d = 1024,m = 65536,l = 24).
tobe0.3. LetαdenotethescalingfactorofLoRA
thatisusedtoscaletheoutputoftheLoRAlayer C AblationonLoRAandModel
beforeaddingtothehiddenstatesofthepre-trained Selection
model. Wesetαtobe4and2respectivelyforthe
We report the ablation results on LoRA and un-
3Band11Bmodel. Thepeaklearningratesofthe
supervised model selection in Table 6. Full fine-
3B and 11B models are set to be 3e-5 and 5e-5
tuninghurtstheT0-3Bperformanceonalldatasets–
respectivelywithawarm-upstageof100stepsand
actuallyitcollapsesonalmostallthedatasetswhen
polynomial learning rate scheduler. We train for
wecheckitspredictions,whichcouldpartiallyex-
a maximum of 1,500 steps. Note that the hyper-
plainthelowaccuracies. UsingLoRAaloneisable
parametersforthe3BmodelistunedontheRTE
to improve full fine-tuning generally and outper-
datasetandusedforotherdatasets. Wedidnottune
formstheT0-3Bbaselinesometimes. Moreoever,
thehyperparametersofthe11Bmodel.
wefindthatunsupervisedmodelselectionisvery
B.2 ImplementationDetails effectivetomitigatecollapseandgreatlyimproves
fullfine-tuningresults. Finally,combiningLoRA
The reported T0 baseline numbers are obtained
from our own running using the released T0
5BecausetheGPUmemorysometimescannothandleall
weights. We are able to reproduce the numbers thepromptswithinonebatch.
RTE CB ANLIR1 ANLIR2 ANLIR3 COPA HS Story. Wino. WSC WIC
T0-3B 64.6 46.4 34.6 33.7 34.7 78.0 27.8 86.5 50.9 69.2 50.3
Fullfine-tuning 59.6 8.9 33.3 33.5 33.3 54.0 25.7 51.5 50.4 36.5 50.0
+LoRA 54.0 44.6 33.3 33.3 34.4 82.7 33.6 87.4 52.0 36.5 50.0
+modelselection 75.8 35.7 37.0 33.5 33.3 80.0 32.2 86.5 50.8 71.2 54.6
+LoRA+modelselection 75.2 47.6 38.4 37.9 34.0 82.7 34.2 87.3 52.0 58.3 55.4
Table6: AblationresultsonLoRAandunsupervisedmodelselection. Thetrainingobjectiveistheswarmdistil-
lationloss. Numbersareensembleaccuracyinthetraining-timetuningsettingbasedonT0-3B.“Fullfine-tuning”
updates all the model parameters, while “+LoRA” means that we freeze the T0 parameters and only update the
LoRAparameters.
and unsupervised model selection gives the best
resultsoverallonthese11datasets.
WeclarifythattheresultsinTable6areonlyfor
analysispurposetobetterunderstandtheeffectof
differentcomponentsofourmethod,butwerenot
usedbyustomakedesigndecisionsinourprelimi-
naryexperiments–asstatedin§3.3,weuseLoRA
becauseitcollapseslessoftenthanfullfine-tuning
anddevelopaunsupervisedmodelselectioncrite-
rion since LoRA still collapses on some datasets.
To judge collapse or not, we simply checked the
modelpredictions,toseeifthepredictionsforall
theexamplesarealmostthesame.

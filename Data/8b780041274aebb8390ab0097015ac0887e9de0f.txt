Selective Weak Supervision for Neural Information Retrieval
KaitaoZhang♥,ChenyanXiong♠,ZhenghaoLiu♥,andZhiyuanLiu♥
TsinghuaUniversity♥,MicrosoftResearchAI♠
{zkt18, liu-zh16}@mails.tsinghua.edu.cn;chenyan.xiong@microsoft.com;liuzy@tsinghua.edu.cn
ABSTRACT notprovideaseffectivedistributedrepresentationsforsearchrel-
Thispaperdemocratizesneuralinformationretrievaltoscenar- evancemodeling[45,52].Thisdiscrepancyisasignificantbottle-
ioswherelargescalerelevancetrainingsignalsarenotavailable. neckforNeu-IR’simpactinscenarioswithouttheluxuryoflarge
WerevisittheclassicIRintuitionthatanchor-documentrelations amountsofrelevance-specificsupervisionsignals,forexample,in
approximatequery-documentrelevanceandproposeareinforce- manyacademicsettingsandnone-websearchdomains.
mentweaksupervisionselectionmethod,ReInfoSelect,which Thisworkaddressesthediscrepancybetweenweaksupervision
learnstoselectanchor-documentpairsthatbestweaklysupervise methodsandtheneedsofrelevancematching,toliberateNeu-IR
the neural ranker (action), using the ranking performance on a from the necessity of large scalerelevance-specific supervision.
handfulofrelevancelabelsasthereward.Iteratively,forabatch Inspiredbytheclassic“Anchor”intuition:anchortextsaresimi-
ofanchor-documentpairs,ReInfoSelectbackpropagatesthegra- lartoquerytextsandtheanchor-documentrelationsapproximate
dientsthroughtheneuralranker,gathersitsNDCGreward,and relevancematchesbetweenqueryanddocuments[3],wepropose
optimizesthedataselectionnetworkusingpolicygradients,until
ReInfoSelect,“ReinforcementInformationretrievalweaksuper-
theneuralranker’sperformancepeaksontargetrelevancemet- visionSelector”,whichconductsselectiveweaksupervisiontrain-
rics(convergence).InourexperimentsonthreeTRECbenchmarks, ingspecificallydesignedforNeu-IRmodels.Givenahandfulof
neuralrankerstrainedbyReInfoSelect,withonlypubliclyavail- relevancelabelsinthetargetrankingtask,forexample,aTREC
ableanchordata,significantlyoutperformfeature-basedlearning benchmark,alargeamountofanchor-documentpairs,andaNeu-
to rank methods and match the effectiveness of neural rankers
IRmodel.ReInfoSelectusesREINFORCE[42]tolearntoselect
trainedwithprivatecommercialsearchlogs.Ouranalysesshow anchor-documentpairsthatbetteroptimizetheneuralranker’s
that ReInfoSelect effectively selects weak supervision signals performanceinthetargetrankingtask.
basedonthestageoftheneuralrankertraining,andintuitively The weak supervision data selection is conducted by a state
picksanchor-documentpairssimilartoquery-documentpairs. networkthatrepresentstheanchor,thedocument,andtheanchor-
documentrelation,andanactionnetworktodeterminewhether
KEYWORDS toselecteachpair.Thisdataselector isconnectedtothetarget
neuralrankerusingpolicygradients—asthetooltoovercomethe
NeuralIR,WeakSupervision,Pre-TrainingDataSelection
non-differentiabilityofthedataselectionandweaklysupervised
ACMReferenceFormat: trainingprocess.Thelearningofthetwopartiesisconducteditera-
KaitaoZhang♥,ChenyanXiong♠,ZhenghaoLiu♥,andZhiyuanLiu♥.2020.
tivelyinReInfoSelect’sstochasticprocess.Forabatchofanchor-
SelectiveWeakSupervisionforNeuralInformationRetrieval.InProceedings document pairs, ReInfoSelect 1) selects the weak supervision
ofTheWebConference2020(WWW’20),April20–24,2020,Taipei,Taiwan,
pairsusingitsdataselector,2)conductsseveralback-propagation
China.ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/3366423.
stepsthroughtheneuralrankerusingtheselectedpairs,3)eval-
3380131
uatestheneuralrankeronthetargetscenariotocollectrewards,
1 INTRODUCTION and4)updatesthedataselectorusingpolicygradients.Thispro-
cesscontinuesthroughbatchesofanchor-documentpairsuntilthe
Neuralinformationretrieval(Neu-IR)methodslearndistributed
neuralranker’sperformanceonthetargettaskconverges.
representationsofqueryanddocumentsandconductsoft-matches
InourexperimentsonthreewidelystudiedTRECbenchmarks,
betweenthemintheembeddingspace[4,5,13,25,33,45].Insce-
ClueWeb09-B,ClueWeb12-B13,andRobust04,ReInfoSelectpro-
narioswithsufficienttrainingsignals,forexample,incommercial
videsstate-of-the-artweaksupervisionmethodfortwocommonly
searchengineswithlargeamountsofuserclicks,andonbench-
usedneuralrankers:Conv-KNRM[5]andBERT[29].Onlywhen
markswithmillionsofrelevancelabels,end-to-endNeu-IRmethods
guidedbyReInfoSelect’sselectiveweaksupervision,theseneural
havesignificantlyimprovedtheirrankingaccuracy[5,23,29,45].
rankersrobustlyoutperformfeature-basedlearningtorankmeth-
Withoutlargescalerelevancelabels,theeffectivenessofNeu-IR
ods. ReInfoSelect also matches the effectiveness of Bing User
ismoreambivalent[47].Amainchallengeisthatthelanguage
Clicks[4]—Thelatterisonlyavailableincommercialsearchenvi-
modeling style weak supervision, i.e. word2vec style word co- ronments,whileeverythinginReInfoSelectispubliclyavailable1.
occurrence[27]andBERTstylemasklanguagemodel[9,35],does
Our in-depth studies demonstrate the raw anchor-document
pairsaretoonoisyandmayhurtneuralrankers’accuracywhen
ThispaperispublishedundertheCreativeCommonsAttribution4.0International
(CC-BY4.0)license.Authorsreservetheirrightstodisseminatetheworkontheir useddirectly[8].Incomparison,ReInfoSelectintuitivelyselects
personalandcorporateWebsiteswiththeappropriateattribution. weaksupervisionsignalsbasedonthestatusofthetrainedneural
WWW’20,April20–24,2020,Taipei,Taiwan,China
ranker:itislenienttonoisyanchorswhentheneuralrankerisjust
©2020IW3C2(InternationalWorldWideWebConferenceCommittee),published
underCreativeCommonsCC-BY4.0License. initialized,but,asthemodelconverging,quicklybecomesselective
ACMISBN978-1-4503-7023-3/20/04.
https://doi.org/10.1145/3366423.3380131 1Allourcodes,data,andresultsareavailableathttps://github.com/thunlp/ReInfoSelect.
0202
naJ
82
]RI.sc[
1v28301.1002:viXra
WWW’20,April20–24,2020,Taipei,Taiwan,China KaitaoZhang♥,ChenyanXiong♠,ZhenghaoLiu♥,andZhiyuanLiu♥
andonlypicksweaksupervisionsignalsthatcanfurtherelevate theMSMARCOpassagerankingtask[29,30].Thestrongeffec-
theranker’seffectiveness.Thisisalsorevealedinourhumaneval- tiveness,especiallyconsideringBERTisalsopre-trainedonword
uation:earlierinthetrainingprocess,ReInfoSelectselects80%+ co-occurrencesignals,hasraisedmanyinvestigationsofitssource
anchor-documentpairs,mostnotthatsimilartoquery-relevantdoc- ofeffectivenessinranking[4,31,37].Thoughdefinitiveconclusions
uments;later—astheneuralrankerbecomesbetter—ReInfoSelect remaintobestudied,recentresearchstillobservedasignificant
becomesmoreselectiveandpicksanchor-documentpairsratedas defectofBERT’sweaksupervisioninrelevancematching:Daiand
betterapproximationsforquery-relevantdocumentpairs. CallanshowthatwhenBERTisfurtherfine-tunedonBinguser
Thenextsectiondiscussesrelatedwork.Section3presentsthe clicks,itsaccuracyonTRECWebTracksimprovedby16%com-
ReInfoSelectframework.Section4andSection5describeour paredtoonlyusingMask-LMpretraining[4].Thisindicatesthat
experimentsandresults.Section6concludes. there is still a significant gap between the weak supervision of
BERT’spretrainingandtheneedsofsearchrelevancematching.
ThereareseveralattemptstotrainNeu-IRmodelsusingweak
2 RELATEDWORK
supervision[7,24,49,50].Oneexploredweaksupervisionsignalis
Neu-IRmodelscanbecategorizedasrepresentationbasedandinter- PseudoRelevanceFeedback[3].Thetopretrieveddocumentshave
actionbased[13].Representationbasedmethodsencodethequery beenusedtotrainindividualword2vecforeachqueryandthen
andthedocumentseparatelyastwodistributedrepresentations, usedforqueryexpansion[10].Theunsupervisedretrievalscores,
forexample,usingfeed-forwardneuralnetworks[19,40],convolu- e.g.,fromBM25,havebeenusedasrelevancelabelstotrainneural
tionalnetworks[18],ortransformers[22],andmatchthemusing rankingmodels[8].ThePRFweaksupervisionsignalsingeneral
encodingdistances.Theencoding-and-matchnaturemakesthem canpromoteneuralmodelstosimilareffectivenessasPRF-based
moreefficientforretrieval[1]. queryexpansionmethods[8,10,47].Theotherexploredweaksu-
Interactionbasedmethodsmodelthefine-grainedinteractions pervisionsignalisthetitle-bodyrelationinwebdocuments,where
betweenqueryanddocuments,oftenbythetranslationmatrices MacAvaneyetal.treattitlesasanapproximationofqueriesand
betweenallqueryanddocumenttermpairs[2],whichinNeu-IR buildadiscriminatortofindmostpropertitlesforweaksupervi-
arecalculatedusingtermembeddings[18].Therankingscoresare sion[26].
thencalculatedusingthetranslationmatrices,forexample,bycon- Recently,learningtoselecthigherqualityweaksupervisiondata
volutionalneuralnetworks[18,32],recurrentneuralnetworks[33], hasreceivedmuchattentionindeeplearning,partlybecauseofthe
densityestimationkernels[45],andposition-awarenetworks[20]. “datahungry”propertyofdeepneuralnetworks[15,16,39].Arela-
BERT-basedrankers,withthestronginteractionsbetweenquery tivelynewtechnique,ithasnotyetbeenutilizedinadhocretrieval
anddocumenttermsinthetransformer’smulti-headattentions,are norweaksupervisionsettings;recentstudiesweremainlyinthe
alsointeractionbased[37]. domainadaptationsettingofnaturallanguageprocessingtasks,i.e.,
Whenlargescalerelevancetrainingsignalsareavailable,the paraphraseidentificationandnaturallanguageinference[38,41].
interactionbasedNeu-IRmethodshaveshownstrongeffectiveness AnchortextshavebeenusedinvariousIRtasks[12]:queryre-
overpreviousfeature-basedlearningtorankmethods.Conv-KNRM, finement[21],querysuggestion[6],anddocumentexpansion[11],
whichcapturessoftmatchesbetweenquery-documentn-grams tonameafew.PrevioususageofanchortextsinNeu-IRismainly
usingkernels[5],significantlyoutperformsfeature-basedmeth- asanadditionaldocumentfieldtoprovideadditionalrankingsig-
odsintheChineseSogou-Tsearchlog[23,53,54],theBingclick nals[50],notasweaksupervision.Theapplicationofreinforcement
log[5],andMSMARCOwhichincludesmillionsofexpertrele- learninginIRisalsomainlyonrankingmodels[43,51].
vancelabel[17,37].TheBinglogpre-trainedConv-KNRMcanalso
generalizetoClueWebbenchmarks[4,5]. 3 METHODOLOGY
Nevertheless,previousresearchfindsthatmuchofNeu-IR’sef-
Thissectionfirstdescribessomepreliminariesofneuralrankers
fectivenessreliesonembeddingsspecificallytrainedforrelevance,
andthenourReInfoSelectmethod.
becausethewordco-occurrencetrainedwordembeddingsdonot
alignwellwiththeneedsofadhocretrieval[45,48].Thisnecessi-
3.1 Preliminary
tateslargescalerelevancetrainingdataforNeu-IRmethods’effec-
tiveness.Inreality,largescalerelevancetrainingdataeitherrequire Givenaqueryqanddocumentd,neuralrankingmodelscalculate
searchlogsfromcommercialsearchenginesorexpensivehuman rankingscore f(q,d)usingquerywordsq ={t 1q ,...,t iq ,...,t mq}
labeling;thisluxuryisnotoftenavailable,i.e.inacademicsettings and document wordsd = {t 1d,...,t jd,...,t nd}. There are many
orinnone-websearchdomainswhereneitheralargeamountof neuralarchitecturesdevelopedforf(q,d).Inthiswork,wechoose
humanlabelsnorlargesearchtrafficexists.Thissignificantlylimits Conv-KNRM[5],astherepresentativeofneuralrankerswithshal-
theimpactofNeu-IR:inspecialdomainsearchandTRECbench- lowwordembeddings,andBERTRanker[29],astherepresentative
marks,ambivalentperformanceshavebeenobservedfrommany ofpre-trainedtransformers.
Neu-IRmethods[47]. Conv-KNRMmatchesthequeryanddocumentinthen-gram
Theriseoflargepre-trainedtransformers,e.g.,BERT[9],has embeddingspaceusingmatchingkernels[45].ItfirstusesConvo-
significantlyinfluencedthelandscapeofNeu-IR.Byconcatenat- lutionalNeuralNetwork(CNN)tocalculateh-gramembeddings
ingthequeryanddocument,asasequencetosequencepair,and д(cid:174)h fromwordembeddingt(cid:174)[5]:
i
fine-tuningonrelevancelabels,BERTbasedrankeroutperforms
previous (shallow) neural ranking models by large margins on д(cid:174) ih =CNNh(t(cid:174) i:i+h). (1)
SelectiveWeakSupervisionforNeuralInformationRetrieval WWW’20,April20–24,2020,Taipei,Taiwan,China
REINFORCE Reward
CNN
Anchor a AState TargetBenchmark NDCG:0.213
i
Q D S Label
q d 0.6 1
1 1
Policy
StateNetwork A-DState q 2 d 2 0.4 0
KernelPooling
q d 0.3 1
3 3
q d 0.1 0
4 4
InteractionMatrix
Doc d DState Evaluation
i CNN ActionNetwork
StateRepresentations
Action NeuralRanker
WeakSupervisionData a –d a –d a –d a –d
1 1 2 2 3 3 4 4
WeakSupervision
Figure1:TheArchitectureofReInfoSelect.
Thehq-gramandh d-gramembeddingsofqueryanddocumentare Training.Conv-KNRMandBERToftenrequirelargescalerele-
usedtoconstructatranslationmatrixMhq,hd,whoseitemisthe vancesupervision[14,47].ThemaincapacityofConv-KNRMisits
cosinesimilarityofcorrespondingh-grampairs: relevancespecificn-gramembeddings,whichneedtobetrained
byalargeamountofrelevancelabels,forexample,userclicksin
M ih jq,hd =cos(д(cid:174) ihq,д(cid:174)h jd). (2) searchlogs[4,5]orhumanlabelsfromMSMARCO(1Million
ItthenusesK Gaussiankernelstoextractthematchingfeature Labels)[37].BERTisalreadypre-trainedbytheMask-LMtask[9].
ϕ(Mhq,hd)= {K1(Mhq,hd),...,KK(Mhq,hd)}fromMhq,hd.Each Still,itsadvantageinrankingismoreobservedwhenfine-tuned
byalargeamountofsupervisionsignalsfromuserclicksorMS
kernelK summarizesthetranslationscoresassoft-TFcounts:
k
MARCO[4,29,30,47].
K
(Mhq,hd)=(cid:213) (log(cid:213)
exp(−(M ih jq,hd −µ k)2
)), (3)
k 2δ2
i j k 3.2 ReinforcementDataSelection
whereµ k andδ k arethemeanandwidthforthek-thkernel.The ReInfoSelectovercomesthedependencyofalargeamountof
h-gramsoftmatchkernelsareconcatenatedtothefinalfeatures: relevancelabelsbyweaklysupervisingneuralrankerswiththe
Φ(M)=ϕ(M1,1)···◦ϕ(Mhq,hd)◦...ϕ(Mhmax,hdmax), (4) widelyavailableanchordata.Thechallengeis,theanchordataare
inevitablynoisy:Manyanchorsare“functional”ratherthan“infor-
where◦istheconcatenateoperation.
mational”,forexample,“homepage”and“contactus”;someanchors
Thesoft-TFfeaturesarecombinedbyastandardrankinglayer:
aretoogeneralandmaynotevenretrieveitslinkeddocument,e.g.,
f(q,d)=tanh(ωr ·Φ(M)+br), (5) “customersupport”.Directlyusingallanchor-documentpairsto
trainNeu-IRmodelsisunlikelyeffective.
withparametersωr andbr andtanhactivation.
Toaddressthischallenge,ReInfoSelectlearnstoselectmore
BERTisapre-traineddeeptransformerandperformswellon
suitableanchor-documentpairsdirectlybytheirabilitytooptimize
manytextrelatedtasks[9].Toleveragethepre-trainedBERT’s
neuralrankers.AsshowninFigure1,thisisachievedbyseveral
sequencetosequencemodelingcapability,BERTfirstconcatenates
components:theState networkthatrepresentsthea-dpair,the
thequeryanddocumentintoonetextsequenceandfeeditinto
Actionnetworkwhichdecideswhethertokeepthepair,andthe
pre-trainedBERT[29]:
trainingRewardgatheredfromthetrainedranker.
BERT(q,d)=Transformer([CLS]◦q◦[SEP]◦d◦[SEP]). (6) Specifically, for thei-th weak supervision pairbi = (ai,di),
Thelastlayer’s“[CLS]”tokenrepresentationisusedasthe“match-
ReInfoSelectdecidesifthepairshouldbeusedasaweaksuper-
ing”featureBERT(q,d).Thenarankinglayercombinestherepre- visionsignal(Actioni)byusingthestaterepresentationssi ofthe
sentationtotherankingscore: a-dpair.Theselectedweaksupervisionpairsarethenusedtotrain
theneuralrankertoobtaintherewardR,whichgoesbacktotrain
f(q,d)=tanh((ωr ·BERT(q,d))+br), (7)
theactionandstatenetworks.Therestofthissectiondescribesthe
whereωr andbr arethelearningtorankparameters. State,Action,Reward,andthelearningviaPolicyGradient.
WWW’20,April20–24,2020,Taipei,Taiwan,China KaitaoZhang♥,ChenyanXiong♠,ZhenghaoLiu♥,andZhiyuanLiu♥
State.Thestaterepresentswhetherananchor-documentpairis Theexpectationrewardisthenusedtooptimizetheparameters
goodtrainingdataforneuralrankers.Thestatesi forthei-thpair ofthestateandactionnetworksusingstandardpolicygradient[42]:
includethreecontinuousvectors:anchorstatesa,documentstate
i k
s id,andanchor-documentinteractionstates iad. θ s∗ tate,action←θstate,action+α(cid:213)(cid:213) R∇ θlogπ θ(si), (17)
Theanchorstateanddocumentstaterepresentationsusestan- T i=1
dardconvolutionalneuralnetworksontheirwordembeddings: whereα isthelearningrateandk isthetotalnumberofweak
s ia =CNNa(ai); (8) s uu pp de ar tv esis oio fn thp ea pir as ro amfb ea tetc rh sθBt o. fT thh ee de ax tp ae sc eta let cio tin onre pw oa lir cd yg πu (i sd ie )s
.
the
s id =CNN d(di). (9) actR ioe nIn af no dS se tl ae tect neu ts we os rp ko sl ti ocy thg era nd ei ue rn at ls ra as nkth ere ’sto po el rft oo rmco an nn ce ec ,t anit ds
Theanchor-documentstaterepresentationusestherankingfeature thuslearnstoselectmoresuitableweaksupervisionsignalsfrom
Φ(M ai,di)fromConv-KNRM(Eqn.4): theanchor-documentpairs.
s iad =Φ(M ai,di). (10) 3.3 NeuralRankerTrainingwithReInfoSelect
Thethreevectorsareconcatenatedtothefinalstate: As shown in Figure 1, ReInfoSelect learns interactively with
theneuralranker.Thetwostochasticallygothroughbatchesof
si =s ia◦s id ◦s iad. (11) anchor-documentpairs.Ineachbatch,ReInfoSelectfirstselects
theanchor-documentpairsusingitsstateandactionnetworks,and
Notethattheparametersarenotsharedwiththeneuralranker.
thenstochasticallytrainstheneuralrankerusingtheselectedpairs.
Action.Theactiondecideswhethertousetheanchor-document
Afterthat,theupdatedneuralrankerisevaluatedonthevalidation
pair(1)ornot(0)asaweaksupervisionsignal.Theactiononthe
query-documentrelevancelabelstoobtainreward,whichupdates
i-tha-dpairiscalculatedas
ReInfoSelect’spolicynetworksviapolicygradient.
Actioni =argmax 0,1π(si), (12) Specifically,forthebatchB,theneuralrankerf istrainedusing
π(si)=softmax(Linear(si)), (13) standardpairwiselearningtorank:
asimplelinearlayeronthestatetopredicttheactionprobability.
l =(cid:213) (cid:213) max(0,1−f(ai,d i+ )+f(ai,d i−)), (18)
Reward.Thestateandactionnetworksaretrainedusingthe ai d i+,d i−
ultimategoalof ReInfoSelect:theaccuracyoftheneuralranker whereai,d i+ istheanchor-documentpairselectedbyReInfoSelect.
wh Le en tt Bˆr tai =ne {d bˆb 1t,y .t .h .e ,bˆs ite ,le .c .t .e ,d bˆ ktp ∗a }ir bs e, aas seth toe fr se ew lea cr td e. danchor-document T foh re td ho ecu pm see un dt od i q+ ul ein rk ye ad ib .y Ta hi ea np ep gr ao tx ii vm ea dte os cuth me er nel te dv i−an it sd fo rc ou mm te hn et
pairsinthet-thbatchBt,therewardofthisselectedbatchis: documentsretrievedbyabaseretrievalmodel,i.e.,BM25,usingai
asthequery,followingstandardsinlearningtorank[3].
rt =NDCG(fBˆt(q,d))−NDCG(fBˆt−1(q,d)), (14) TheneuralrankerisupdatedperbatchBasitisusedtoprovide
rewardfortheactionstakenperbatch.ReInfoSelect’sstateand
where
fBˆt
and
fBˆt−1
aretheneuralranker,e.g.,Conv-KNRMor
actionnetworksareupdatedperT batches(anepisodeinREIN-
BERT,trainedusingtheweaksupervisionpairsselectedfromthe
FORCE)tocapturetheaction’sdelayedinfluences.Thetwocircle
(1:t)and(1:t-1)batches.NDCGevaluatestheneuralranker’saccu-
throughtheentireanchor-documentpairsandstochasticallyupdate
racyonthevalidationpartofthetargetrankingbenchmarks.The
theirparametersuntilrankingperformanceconverges.
rewardistheNDCGchangeoftheneuralrankerwhentrainedwith
additionalweaksupervisionsignalsfromBˆt.
4 EXPERIMENTALMETHODOLOGY
Policy Gradient. The NDCG metric and the discrete action
Thissectiondescribestheweaksupervisiondataset,evaluation
are not differentiable. We use the standard policy gradient and
datasets,baselines,trainingandimplementationdetails.
REINFORCEto“propagate”therewardtothetrainingofthestate
WeakSupervisionDataset.Theweaksupervisiondatasetis
andactionnetworks[42].
constructedfromEnglishcorporaofClueWeb09,whichconsistsof
AttheT-threinforcementstep,REINFORCEfirstcalculatesthe
504millionwebpages.Theanchortextsandtheirlinkedwebpages
accumulaterewardRt foreachofthet-thstep:
areregardedaspseudoqueriesandpotentialdocumentsforweak
T supervision.Allwebpagesareparsedby“KeepEverythingExtractor”
(cid:213)
Rt = cjrj. (15) inBoilerpipe.Anchortextsarecollectedusingwarc-clueweb2.
j=t About100K anchors(fromtotal6millioncollected)andtheir
linkeddocumentsarerandomlysampledastheweaksupervision
Itfirstcalculatestheaccumulatedinfluenceoftheactiontakenat
dataset.Pseudonegativedocumentsarethetopretrievedonesby
thet-thstepinthefuturebatches(t:T).Theinfluenceisdiscounted
bythehyperparameterct.ThisleadstotheexpectedrewardRfor
BM25inElasticSearch3.Theinfluencesofnumberofanchorsand
ratioofpseudopositiveandnegativedocumentsarestudiedin
theT-thstep:
Section5.5.
T
1 (cid:213)
R= Rt. (16) 2https://github.com/cdegroc/warc-clueweb
T t=1 3https://www.elastic.co/cn/downloads/elasticsearch
SelectiveWeakSupervisionforNeuralInformationRetrieval WWW’20,April20–24,2020,Taipei,Taiwan,China
EvaluationDatasets.Threeadhocretrievalbenchmarksare arefirsttrainedwiththeweaksupervisionorsourcedomainsu-
usedinevaluation:ClueWeb09-B,Robust04,andClueWeb12-B13. pervisionsignals;thentheirrankingfeatures(kernelsor[CLS]
ClueWeb09-Bconsistsof200querieswithrelevancelabelsfrom embeddings)arecombinedwiththebaseretrieval(SDM)scoreus-
TRECWebTrack2009-2012.Robust04consistsof249querieswith ingCoordinate-Ascent,throughstandardfive-foldcrossvalidation
relevancelabels.ClueWeb12-B13includes100queriesfromTREC onthetargetbenchmark.
WebTrack2013-2014.Titlequeriesareused. StandardrankingbaselinesincludeSDMandtwolearningto
OnClueWeb09-BandRobust04,weusetheexactsamere-ranking rankmethods,RankSVMandCoor-Ascent,withstandardIRfea-
setupwithDaiandCallan[4],whichpresentsthestate-of-the-art tures.WefoundthattheNDCGscoresonClueWeb09-BandRo-
neuralrankingaccuracy.Allourrankingmodelsre-rankedtheir bust04inDaiandCallan[4]aremuchhigherthanourimplemen-
releasedtop100SDMretrievedresults[4].Weusethesametitle tationsandpreviousresearch[5,45].Wechoosetocomparewith
concatenatedwiththefirstparagraphasthedocumentrepresenta- their stronger baselines, though not all metrics were provided.
tiontofitinBERT’smaxsequencelength[4].OnClueWeb12-B13, ClueWeb12-B13baselinesarethosereleasedbyXiongetal.[44].
wefollowthere-rankingsetupfromXiongetal.[44],asDaiand TrainingDetailsofReInfoSelect.Therearethreestepsfor
Callan[4]doesnotincludeClueWeb12. trainingwiththeReInfoSelect:warmup,reinforcetrainingwith
AllexperimentsettingsarekeptconsistentwithDaiandCallan anchordata,andadaptingtotherankingbenchmark.Forallsteps,
[4](Xiongetal.[44]onClueWeb12).Allthecandidatedocuments therankingbenchmarklabels(ClueWeb09-BorRobust04)arepar-
torerankarefromtheirbaseretrievalmethods.Conv-KNRMand titionedtofivefoldsforcrossvalidation.
BERTRankeruseopensourceimplementations[5,37].Theevalua- The warm up stage first trains the state and action network
tionscoreswiththecorrespondingpaperarethusdirectlycompara- using the discriminator setup [26]. Then in the reinforce stage,
ble.ThisiscrucialforreproducibleIRandtocompareReInfoSelect ReInfoSelect’snetworksareinitialized(warmedup)bythelearned
withthesupervisionfromprivateBingsearchlog[4]. discriminatorweights.Thereinforceandadaptionareallconducted
TRECofficialmetrics,NDCG@20andERR@20,areused.Statistic viafive-foldcrossvalidation.Ineachofthefiveruns,ReInfoSelect
significanceistestedbypermutationtestwithp <0.05. onlyusesthefourtrainingfoldstocalculatetherewardsandto
Baselines.Ourmainbaselinesareothertrainingmethodologies traintheneuralrankers.Notestinginformationisusedinanyofits
inNeu-IR.Wealsocomparewithstandardrankingbaselines. trainingstages.Thenthesametrainingsplitsareusedtofine-tune
No weak Supervisionusesnoadditionalrankinglabelsbeyond theweaklysupervisedneuralrankers,thesamewithbaselines.The
theexistingsmallscalerelevancelabelsintheevaluationdatasets. testingfoldisonlyusedforfinalevaluation.
Thisisthevanillabaseline.Conv-KNRM’swordembeddingsare ImplementationDetails.Thispartdescribestheimplement
initializedbyGlove[34];BERTusesGoogle’sreleasedpre-trained detailsof ReInfoSelectandallbaselines.
parameters[9]. StandardIndristopwordremovalandKrovetzStemmerareused
Anchor+BM25 LabelsisourimplementationofBM25weaksu- toprocessqueriesanddocumentsforConv-KNRM.TheBERTbased
pervision[8].Weusethesameanchorsinourweaksupervision modelsuseBERT’ssub-wordtokenizer.
datasetaspseudoqueries,theirBM25retrieveddocumentsasdoc- Conv-KNRMuses21kernels,oneexactmatchandtherestsoft
uments,andtheBM25scoresastheweaksupervisionlabels.The match[37];theuni-gram,bi-gram,andtri-gramofqueryanddocu-
differenceisthatweuseConv-KNRMandBERT,strongerranking menttextsareconsidered,thesameasthepreviouswork[4,5].The
modelsthantheirfeedforwardneuralnetworks[8]. wordembeddingdimensionis300andinitialedwithGlove[34];
Title DiscriminatorisourimplementationofMacAvaney the learning rate is 1e − 3. The BERT models inherit pytorch-
etal.[26].Weusethetitle-documentrelationinClueWeb09-Bas transformers4.Themaxsequencelengthis384.Adamwithlearning
thepseudolabelandConv-KNRMastheirdiscriminator.Itdiffers rate=5e−5andwarmupproportion0.1isused.
fromReInfoSelectthatthediscriminatorisnottrainedbyreward Forthedataselectorof ReInfoSelect,thediscountfactorct is
fromneuralranker’sNDCG.Instead,itisaclassifiertrainedto 0.99.Statenetworksusetheirown300-dimensionalwordembed-
classifyquery-documentpairsfromtitle-documentpairs;thenthe dingsinitializedwithGlove[34].TheirCNNsusewindowsizes3,4,
title-documentpairsmostsimilartoquery-documentareused. and5.Itslearningrateis1e−3.
All Anchorusesallrandomlysampledanchor-documentpairs Theneuralrankingmodelsareupdatedwithonegradientstep
withoutanyfilteringorweighting. perbatch,whilethedataselectorisupdatedonceevery4batches
MS MARCO Human Labelusesthepassagerankinglabelsfrom (T=4).AllourneuralmodelsareimplementedwithPyTorch.All
MSMARCOasrelevancesupervision[28].Itincludeshumanlabels modelsaretrainedwithasingleGeForceGTXTITANGPUand
foronemillionBingqueries. trainedabout40hoursforoneepoch.Moredetailsofourimple-
Bing User ClicksistheresultsfromDaiandCallan[4],where mentationcanbefoundinourcoderepository5.
theyuseduserclicksinBingasthesupervisionsignal,whichin-
cludes5Mquery-documentpairs.Asthecommercialsearchlogis 5 EVALUATIONRESULTS
notpubliclyavailable,weusetheirreportednumbers,whichare SixexperimentsareconductedtoevaluateReInfoSelect’seffec-
directlycomparableasourexperimentalsettingarekeptconsistent.
tiveness.Wealsoprovidehumanevaluationsandcasestudieson
AllthesetrainingmethodsareappliedtoConv-KNRMandBERT
theselectedweaksupervisiondata.
using the exact same setup except different training strategies.
Allneuralrankersareadaptedtothetargetrankingbenchmark 4https://github.com/huggingface/pytorch-transformers
(ClueWebandRobust)thesamewithpreviousresearch[4,5].They 5https://github.com/thunlp/ReInfoSelect
WWW’20,April20–24,2020,Taipei,Taiwan,China KaitaoZhang♥,ChenyanXiong♠,ZhenghaoLiu♥,andZhiyuanLiu♥
Table1:RankingresultsofReInfoSelectandbaselines.†,‡,§,¶,∗indicatestatisticallysignificantimprovementsoverNo Weak
Supervision†,Anchor+BM25 Labels‡,Title Discriminator§,All Anchor¶,andMS MARCO Human Label∗.None-neuralbaselines
onClueWeb09-BandRobust04arefromDaiandCallan[4]andthoseonClueWeb12-B13arefromXiongetal.[44].
ClueWeb09-B Robust04 ClueWeb12-B13
Method NDCG@20 ERR@20 NDCG@20 ERR@20 NDCG@20 ERR@20
SDM(From[4]|[44]) 0.2774 0.1380 0.4272 0.1172 0.1083 0.0905
RankSVM(From[4]|[44]) 0.289 n.a. 0.420 n.a. 0.1205 0.0924
Coor-Ascent(From[4]|[44]) 0.295 n.a. 0.427 n.a. 0.1206 0.0947
Conv-KNRMastheNeuralRanker
No Weak Supervision(From[4]) 0.270 n.a. 0.416 n.a. n.a. n.a.
No Weak Supervision(Ours) 0.2873 0.1597 0.4267 0.1168 0.1123 0.0915
Anchor+BM25 Labels[8] 0.2910 0.1585 0.4322 0.1179 0.1181 0.0978
Title Discriminator[26] 0.2927 0.1606 0.4318 0.1193 0.1176 0.0975
All Anchor 0.2839 0.1464 0.4305 0.1190 0.1119 0.0906
MS MARCO Human Label 0.2903 0.1542 0.4337 0.1194 0.1183 0.0981
Bing User Clicks(From[4]) 0.314 n.a. n.a. n.a. n.a. n.a.
ReInfoSelect 0.3094†‡§¶∗ 0.1611¶ 0.4423†‡§¶∗ 0.1202† 0.1225†¶ 0.1044†‡§¶∗
BERTastheNeuralRanker
No Weak Supervision(From[4]) 0.286 n.a. 0.444 n.a. n.a. n.a.
No Weak Supervision(Ours) 0.2999 0.1631 0.4258 0.1163 0.1190 0.0963
Anchor+BM25 Labels[8] 0.3068 0.1618 0.4375† 0.1233† 0.1160 0.0990
Title Discriminator[26] 0.3021 0.1513 0.4379† 0.1202† 0.1162 0.0981
All Anchor 0.3072 0.1609 0.4446† 0.1206† 0.1208 0.0965
MS MARCO Human Label 0.3085 0.1652 0.4415† 0.1213† 0.1207 0.1024
Bing User Clicks(From[4]) 0.333 n.a. n.a. n.a. n.a. n.a.
ReInfoSelect 0.3261†‡§¶∗ 0.1669 0.4500†‡§¶∗ 0.1220† 0.1276†‡§ 0.0998
5.1 OverallResults supervisionsignalsbutwithoutanyselection.Randomanchorsare
The overall ranking results are presented in Table 1. Note that noisyandnotalwayssimilartosearchqueries.Section5.7further
though our implementation of No Weak Supervision performs studiesthequalityofselectedanchors.
slightly better than Dai and Callan’s [4], with only the several Ourimplementationof Anchor+BM25 labelsusesbetterneu-
hundredsoflabeledqueries,neitherConv-KNRMnorBERTcon- ral rankers and also combines it with base retrieval, compared
vincinglyoutperformstheclassicfeature-basedlearningtorank toitsvanillaforminpreviousresearch[8].Still,itdoesnotyet
methods,RankSVMorCoor-Ascent.Similarambivalenteffective- outperformNo Weak Supervision.BM25scorescanbeusedas
nessonNeu-IRsystemswhenrelevancetrainingdataarelimited pseudorelevancefeedback(PRF)ortofindstrongernegativedocu-
hasbeenobservedinmultiplepreviousstudies[4,5,46,47].
ments[3,26].Moreinlinewiththelater,ReInfoSelectusesBM25
Withbothneuralrankingmodels,ReInfoSelectoutperforms tofindnegativedocumentsforanchors.
all baselines except Bing User Clicks on both datasets. The TherearetwodifferencesbetweenTitleDiscriminator[26]
improvementsonNDCGarerobustacrossthetable,whiletheERR
andReInfoSelect.Thefirstistheweaksupervisionsignals:Ti-
metricisalittlemorebrittle,especiallyonClueWeb12-B13,the
tleVSAnchor.ThesecondisthatReInfoSelectlearnstoselect
sameasobservedbypreviousresearch[5,25,46].ReInfoSelect weaksupervisionpairsusingrewardfromtheneuralranker,while
andBing User Clicksaretheonlytwomethodsthatshowstable Title Discriminatorconductstheselectionoftrainingdataand
improvementsoverthefeature-basedCoor-Ascent.Noteagainthat thetrainingofneuralrankersindependently[26].Section5.2and
Bing User Clicksarenotpubliclyavailable,whileReInfoSelect Section5.3furtherstudytheeffectivenessofthedataselection.
onlyuseswidelyavailableanchorinformation. Wehavealsoexperimentedwithotherneuralrankers,including
AdaptingfromMS MARCO Human Labeldoesnotleadtomuch EDRM,whichintegratesexternalknowledge[23],andTransformer-
improvement,thoughusingonemillionexpertrelevancelabels.
Kernel,whichusessub-wordandtransformerwithkernels6.Simi-
TheMSMARCOrankingtaskisapassagerankingformorenatural lareffectivenessandtrendswereobserved.Wealsoexploredthe
languagequeries.Thedomaindifferenceslimitthegeneralization
ensembleoftheReInfoSelectsupervisedneuralrankers;similar
abilityofhumanrelevancelabels.Weaksupervisionandtransfer gainsfrompreviousresearchareobserved[36]andtheensemble
learningsometimesarenecessary:Onemillionhumanlabelsarenot modelsoutperformthesinglemodeltrainedbyBinguserClicks,
timeandcosteffective,ifeverfeasible,inmanyrankingscenarios. thoughitisnotafaircomparison.Theseadditionalresultsarelisted
ReInfoSelectsignificantlyoutperformsAll Anchoronboth inouropensourcerepositoryduetospacelimitations.
datasets.Thelatterusesthesameanchor-documentrelationasweak
6https://github.com/sebastian-hofstaetter/transformer-kernel-ranking
SelectiveWeakSupervisionforNeuralInformationRetrieval WWW’20,April20–24,2020,Taipei,Taiwan,China
Table2:ClassificationAccuracyofdifferentstatenetworks Table3:Conv-KNRMResultsonClueWeb09-Bindifferent
when used as the data discriminator [25]: Anchor only, classifiersorReInfoSelectstates.Relativeperformances%
Anchor-Documentpaironly,andAllstatetogether. and statistical significance† are compared with the corre-
spondingDiscriminatorusingthesamestates.
Method ClueWeb09-B Robust04 ClueWeb12-B13
Discriminator A A-D A A-D A A-D Method NDCG@20 ERR@20
A State 0.775 – 0.850 – 0.786 – All Anchor 0.2839 – 0.1464 –
A-D State 0.723 0.702 0.890 0.928 0.726 0.712 A Discriminator 0.2893 – 0.1521 –
All State 0.743 0.728 0.863 0.899 0.740 0.732 A State (Scratch) 0.3002† +3.77% 0.1604 +5.46%
A State (Warm Up) 0.3050† +5.43% 0.1632 +7.30%
A-D Discriminator 0.2974 – 0.1556 –
5.2 StateNetworksAsDiscriminators A-D State (Scratch) 0.3033 +1.98% 0.1653 +6.23%
A-D State (Warm Up) 0.3083† +3.66% 0.1646 +5.78%
Thisexperimentstudiestheeffectivenessofthestaterepresenta-
All Discriminator 0.3021 – 0.1576 –
tions,usinganintermediateevaluation—howwelltheycanclassify
All State (Scratch) 0.3078 +1.89% 0.1670 +5.96%
theactualquery-documentsfromanchor-documentspairs,i.e.,as
All State (Warm Up) 0.3094† +2.42% 0.1611 +2.22%
thediscriminatorroleinTitleDiscriminator[26].
Weusethestatenetworkandtheactionnetworktodirectlylearn
abinaryclassifier,usingthequery-documentsfromClueWeb09-B, AlldataselectionmethodsoutperformtheALL Anchor,which
Robust04orClueWeb12-B13aspositiveinstancesandtheanchor- illustratestheanchordataisinformativebutnoisy;dataselection
documentsasnegativeinstances.Wetrainthemodelthesameaswe isnecessarytofileroutnoisyanchordata.Thereinforcebaseddata
traintheTitleDiscriminatorandevaluatetheirclassification selectionmethodsoutperformallDiscriminatormodels,demon-
accuracyinfive-foldcross-validation.Theresultsareshownin stratingtheeffectivenessofReInfoSelect.Connectingtheperfor-
Table2. mancesofweaklysupervisedneuralrankertodataselectorusing
Overall,theClueWebqueriesanddocumentsarehardertodistin- thepolicynetworkprovidessignificantaccuracyboosts.
guishfromtheanchoranddocuments;theaccuracyonClueWebin All stateshowsbetterperformancethanotherstaterepresen-
generalislowerthanonRobust.Thisisexpectedastheweaksuper- tations.IthelpsReInfoSelectselectbetteranchor-documentpairs
visiondocumentsarealsofromClueWebandtheClueWebqueries forweaksupervision,thoughitmightnotbethebestclassifierto
areinthewebdomain.Thereislessdomaindifferencebetweenour distinguishanchorfromthequerywhenusedasadatadiscrimina-
weaksupervisionandClueWeb’slabels.Thisalsocorrelateswith tor.WealsofindWarming Upthestateandactionnetworksslightly
thegreaterrelativeimprovementsofReInfoSelectonClueWeb moreeffectivethantrainingfromScratch.Weobservethatthe
comparedtoRobust. REINFORCEisslowandunstableintrainingandabetterinitial
Allthreestatenetworkshavedecentaccuracyonbothdatasets, statemayimproveitsstability.ThisisfurtherstudiedinSection5.6.
showing their good representation ability of the query/anchor-
documentpairs.TheAnchorStateeasilydistinguishesanchorsfrom 5.4 Fine-TuningStrategies
queries,andtheAnchor-Documentstatesfindquery-document Thisexperimentstudiesdifferentfine-tuningstrategieswhenadapt-
pairsfromanchor-documentpairs,especiallyonRobust.However, ingtheweaklysupervisedneuralrankerstotargetrankingscenar-
theseintermediateresultsonlyshowwhetherthestatenetworks ios.WefocusonConv-KNRMastherankerandClueWeb09-Basthe
representthedatawell.Itisunclear,especiallywhenusedinthe targetrankingscenario,andexperimentwithseveraldifferentadap-
discriminatorsetup[26],whetherahighlyaccurateclassifierwill tionapproaches.Thefirstdirectlyfine-tunestheDenseLayerof
leadtobettertrainedneuralmodels.Aperfectdiscriminatordis- Conv-KNRMonClueWeb09-B,withtheembeddingsfrozen(which
cardsallanchor-documentdata,whichisnotusefulasitleaves wefindmoreeffective).Thesecondfeedsthekernelscores(soft
no information to use for weak supervision. We care the most matchfeatures)toCoor-AscentinsteadofaDenselayer.Thelast
whetherthesestatenetworkshelpuspickweaksupervisionpairs istheoneusedinpreviousresearchandthiswork,whichcombines
thatleadtomoreeffectiveneuralrankers,whichisstudiedinthe theneuralranker’sfeatureswithSDMscoresinthestandardlearn-
nextexperiment. ingtoranksetup[4,5].WealsocombineConv-KNRMandBERT
withSDM,followingpreviousresearch[25].
5.3 Effectivenessof ReInfoSelectSelection
AddingSDMscoresignificantlyimprovestherankingaccuracy.
Table3showstheeffectivenessof ReInfoSelect’sdataselection EvenConv-KNRMhasthesoftn-grammatchfunction.ThecoreIR
withdifferentstrategies.Threedifferentpolicystrategiesareexper- intuitions,e.g.,proximity,smoothing,andnormalization,inSDM
imented.Thefirstisthetwo-stepapproach,Discriminator[26].It arestillnecessaryforneuralrankerstoperformwell.Thecurrent
selectsanchor-documentpairsthatlikerealquery-documentpairs Neu-IR models enhance classic IR approaches but have not yet
andthentraintheneuralrankerwithselecteddata.Thenexttwo replacedthem.Coor-Ascentisalsomuchmoreeffectivelearning
are from ReInfoSelect: Scratch initializes its state and action torankmodelcomparedtothesimpleDense(Linear)Layer.The
networkfromscratch,andWarm Upinitializesitsstateandaction listwiserankerisaseffectiveincombiningneuralfeaturesasin
network from the results of the corresponding Discriminator. combiningclassicfeatures.
Threestatecombinationsareevaluatedwiththethreestrategies: CombingConv-KNRMandBERTalsoprovidesfurtherimprove-
A(nchor)Stateonly,A(nchor)-D(ocument)Stateonly,andAllState. mentsovereitheroneindividually[25].Wealsoobservedbetter
WWW’20,April20–24,2020,Taipei,Taiwan,China KaitaoZhang♥,ChenyanXiong♠,ZhenghaoLiu♥,andZhiyuanLiu♥
Table4:Conv-KNRMResultsonClueWeb09-Bindifferentfine-tuningstrategies.Percentagesindicaterelativeperformanceover
No Weak Supervision.
Method Feature LeToR NDCG@20 ERR@20
No Weak Supervision (Ours) Conv-KNRM+SDM Coor-Ascent 0.2873 – 0.1597 –
Discriminator Conv-KNRM Dense Layer 0.2523 −12.18% 0.1344 −15.84%
Conv-KNRM Coor-Ascent 0.2787 −2.99% 0.1429 −10.52%
Conv-KNRM+SDM Coor-Ascent 0.2980 +3.72% 0.1592 −0.31%
Conv-KNRM+BERT+SDM Coor-Ascent 0.3170 +10.34% 0.1747 +9.39%
ReInfoSelect Conv-KNRM Dense Layer 0.2694 −6.23% 0.1523 −4.63%
Conv-KNRM Coor-Ascent 0.2896 +0.80% 0.1615 +1.13%
Conv-KNRM+SDM Coor-Ascent 0.3094 +7.69% 0.1611 +0.88%
Conv-KNRM+BERT+SDM Coor-Ascent 0.3222 +12.15% 0.1796 +12.46%
(a)Actionw.Conv-KNRM (b)Actionw.BERT (c)Conv-KNRMReward (d)BERTReward
Figure2:ThebehaviorofReInfoSelect’sonClueWeb09-BwhentrainedfromScratch,withitspolicynetworkswarmedup(P
Warm Up),andtheneuralrankerwarmedup(R Warm Up),bothfromAll Discriminator.X-axesshowthetrainingstep(batches)
beforeconvergence;Y-axesshowthefractionofpairsbeingselected(action)andtherewardsfromneuralrankers.
Table 5: Data Strategies Results of ReInfoSelect on thenumberoftotalanchor-documentpairsroughlythesameand
ClueWeb09-B, with different number of anchors (#a), varythedifferentcombinationsofthenumberofanchors(#a)and
different number of positive/negative documents per documentsperanchor(#d/a).Wealsotrysomedifferentbalances
anchor (#d+/a and #d-/a), and different number of to- ofpositiveVSnegativedocuments.Theresultsof ReInfoSelect
tal anchor-document pairs (#pair). Percentages indicate trainedConv-KNRMonClueWeb09-BareshowninTable5.
relativeperformancecomparedwith10Kanchors. Therankingaccuracydoesvary,tosomedegree,withdifferent
data combinations, although using similar amounts of training
#a #d+/a #d−/a #pair NDCG@20 ERR@20 labels.WeobservethatConv-KNRMprefersmorevariationson
10K 35.4 35.5 0.7M 0.3025 – 0.1611 – thequeryside.Itperformsbetterwhenwithmoreanchorsbut
50K 7.2 7.2 0.7M 0.3084 +1.95% 0.1651 +2.48% fewer documents per anchor. This correlates with the setup in
100K 5.5 5.5 1.1M 0.3094 +2.28% 0.1611 +0.00%
MSMARCOandinSogou-QCL[28,53].Wealsoobservethatthe
100K 3.6 7.1 1.1M 0.3073 +1.59% 0.1645 +2.11%
neuralrankerprefersmorepositivelabelsperquery.Itisexpected
100K 2.0 8.0 1.0M 0.3042 +0.56% 0.1621 +0.62%
500K 1.2 1.2 1.2M 0.3085 +1.98% 0.1664 +3.29% asthepositivelabelsprovidemoreinformationthannegativeones.
Nonetheless,inthisexperiment,ReInfoSelectisnotverysensitive
anditsaccuracydoesnotvarymuchacrossdifferentdatastrategies.
performancesofReInfoSelectoverDiscriminatoracrossallset-
tings,showingtherobusteffectivenessofReInfoSelectinselect-
ingmoreeffectiveweaksupervisionsignals. 5.6 StochasticTrainingAnalysis
ThisexperimentanalyzesthestochastictrainingofReInfoSelect.
5.5 InfluenceofTrainingDataStrategies
Weplottheaction(fractionofselectedpairs)andtherewardofeach
RecentadvancementsofNeu-IRmodelsaremainlytrainedondata trainingbatchinFigure2.ThreevariationsofReInfoSelectare
withavastamountofqueriesbutonlyahandfulofdocumentlabels evaluated:withallparametersinthepolicynetworkandtheneural
perquery,e.g.searchlogclicks[4,5]andMSMARCOlabels[30]. rankersfromScratch(BERTrankerisinitializedfrompretraining
Both have less than five relevance documents per query, much withoutIRcontinuoustraining),withthepolicynetworkparame-
fewerthantypicalTRECbenchmarks. terswarmedupusingparametersfromAll Discriminator,and
Thisexperimentstudiestheinfluenceofdifferentdatacombi- withtheneuralrankerswarmedupusingtheircorrespondence
nationsintrainingneuralrankingmodels.Specifically,wekeep fromAll Discriminator.
SelectiveWeakSupervisionforNeuralInformationRetrieval WWW’20,April20–24,2020,Taipei,Taiwan,China
Table6:MethodAgreementsonClueWeb09-B:thefraction Table7:HumanEvaluationonClueWeb09-B.Thenumbers
of anchor-document pairs where different runs/methods are the fraction of anchors (A) or anchor-document pairs
choosing the same action (select/not select).w/ represents (A-D) labeled as proper search queries or relevant query-
ReInfoSelectusesthefollowingmodelasneuralranker. document pairs. Selected and Discarded are the actions
takenbythemodels.Percentages(%)arethefractionofSe-
Method Discriminator w/ Conv-KNRM w/ BERT lected(thesameonAandA-D).EarlyandLaterefertothe
Discriminator 0.924 0.356 0.109 trainingstageinReInfoSelect:beforeandafter400batches.
w/ Conv-KNRM 0.356 0.585 0.323
w/ BERT 0.109 0.323 0.623 Selected Discarded Selected%
A A-D A A-D A&A-D
Discriminator 0.900 0.750 0.923 0.479 5%
WithConv-KNRM
Scratchstartsbyselectingalargefractionofweaksupervision ReInfoSelect-Early 0.926 0.489 0.861 0.528 91%
pairs,andquicklybooststherewardfromtheneuralrankers.At ReInfoSelect-Late 0.942 0.540 0.892 0.432 56%
thebeginningof ReInfoSelecttraining,theneuralrankersare WithBERT
nearlyrandom,andrawanchor-documentpairscanhelpboosts ReInfoSelect-Early 0.912 0.510 0.946 0.435 77%
theirperformance.Asthetraininggoeson,theneuralrankersget ReInfoSelect-Late 0.929 0.560 0.912 0.378 63%
betterandbetter,thusbeingmoreselectiveintheirtrainingdata.
ReInfoSelectthendiscardsmoreoftheanchor-documentpairs
asaddingthemmayleadtonegativerewards.
Thesameintuitionsarereflectedbythetwowarmupversions
HumanEvaluation.Thisexperimentexaminestheanchorin-
aswell.InP Warm Up,themodelstartswithalowselectionrate,
tuitionandevaluateswhetherReInfoSelectselectsanchorsthat
asthepolicynetworkisinitializedbyAll Discriminator,which
areconsideredreasonablewebqueriesandwhetherthedocuments
istrainedtoconsiderallanchorsasnegatives.However,itquickly
areconsideredasrelevantfortheiranchors.
learnstorelaxtheselectionrate,astheneuralrankerisjustini-
We recruit four graduate students to label anchor-document
tializedandmostanchor-documentpairscanhelp.Afterthat,P
pairs.Wepresentthem100anchors,eachassociatedwithonelinked
Warm UpbehavesrathersimilarlytoScratch.InR Warm Up,as
document(pseudopositive)andoneretrieved(pseudonegative)
theneuralrankeriswarmedupwithmeaningfulweights,thedata
document,allrandomlysampledandordered.Thejudgesareasked
selectionratesremainlowandReInfoSelectonlypicksthosecan
toprovidetwobinarylabels.Thefirstiswhethertheanchorcould
furtherboostedthewarmedupranker’saccuracy.
beareasonablewebsearchquery,andthesecondiswhichofthe
Thebehaviorsof ReInfoSelectwithConv-KNRMandBERT
twodocumentsismorerelevanttotheanchor.Themajorityofvotes
sharesimilartrendsinthestochastictrainingprocess.Themain
areusedasthefinallabel.Ourjudgesagreewell:theirCohen’s
differenceisthatBERTusesmorerelaxedselectionrateformore
Kappais0.526onanchorand0.544onanchor-document.
epochs.Itsdeeptransformernetworksarehardtooptimizethan
Wethenmixthelabeledpairsintoallpairsandfeedthemto
Conv-KNRM’sembeddingandCNNlayers.Ontherewardside,all
DiscriminatorandReInfoSelect.WhenfeedingReInfoSelect,
modelvariationsconvergetosimilarrewardscores,showingthe
weeithermixthelabeledpairsintheearlypartoftheepoch,when
robustnessof ReInfoSelect.
theneuralrankerisjustinitialized,andthelatepart,whenthe
neuralrankerisclosetoconverge,toevaluatethebehavioronthe
5.7 DataSelectionBehaviors
twostages.TheresultsareshowninTable7.
Thissetofexperimentsanalyzethedataselectionbehaviorsof AlignedwiththeAnchorintuition,90%+anchorsareratedas
ReInfoSelect,includetheagreementsbetweendifferentmodels reasonable search queries. The relevance between anchors and
andhumanevaluations. linkeddocumentsismoreambivalent;manyanchorsarefunctional
Method Agreements. The first experiment studies whether (e.g.“homepage”)thaninformational.ReInfoSelectshowssignif-
ReInfoSelectchoosesdifferentweaksupervisionsignalsfordif- icantlydifferentbehaviorinEarlyandLate.Astheneuralranker
ferentneuralrankers.Table6showstheagreementbetweenthe converging, ReInfoSelect selects pairs more and more similar
“AllDiscriminator”andReInfoSelectwhenusedwiththetwo toquery-relevantdocuments,whileinthebeginning,especially
neuralrankers.Theagreementsbetweenthesamemodel(diagonal with the less pre-trained Conv-KNRM, 91% data is selected. Its
elements)areevaluatedonthefivecross-validationruns.Therests “selectiveness”iscustomizedforthetarget’strainingstatus.
aretheaverageofbetweentheirfiveruns.Allmodelsreadthe Discriminatordoesabetterjobinpickinga-dpairssimilar
anchor-documentpairsinthesameorder. toqueryandrelevantdocuments,whichiswhatitistrainedfor.
Allmethodsagreethemostwiththemselvesacrossdifferent However,itistoostrictandonlypicks5%pairs.Thechallengeis
runs.DiscriminatorismoredeterministicthanReInfoSelect, thatitisonlytrainedwithlimitedtargetqueryanddocuments,
asexpected.ReInfoSelectpicksdifferentdataforConv-KNRM thusmaynotgeneralizewelltoanchorsthataregoodbutdifferent
and for BERT. The latter has already been pre-trained and has fromthehandfultargetqueries.Anotherchallengeisthatthedata
ratherdifferent,andmuchdeeper,architecturesthanConv-KNRM. selector is isolated with the target ranker, while ReInfoSelect
ReInfoSelectdisagreeswithDiscriminatorthemost.Thedata providessomefinalpushthatelevatesthetargetneuralrankers
selectioninDiscriminatorisdisconnectedfromthetargetranker. another3-5%comparedtoDiscriminator.
WWW’20,April20–24,2020,Taipei,Taiwan,China KaitaoZhang♥,ChenyanXiong♠,ZhenghaoLiu♥,andZhiyuanLiu♥
Table8:Casestudyforanchor-documentpairsthatonlyselectedbyDiscriminatororReInfoSelect.Documentsnippetsare
manuallypicked.Manuallabelsonwhethertheanchororpairisrelevant(+)ornot(-)areshowninbrackets.
Method Anchor LinkedDocument(ManualSnippet)
Discriminator oregonshortline(+) ...araillineownedandoperatedbytheunionpacificrailroadintheu.s.stateofutah(-)...
newyorkstatepolice(+) ...2007picturespublishedbythenewyorkstatepolicecarefully(-)...
greenwichtime(+) ...searchfor:greenwichtimegreenwichsponsoredlinks(-)...
pojoaquepueblo(+) ...contactpojoaquepueblohereforyourperusalis(+)...
bannerads(-) ...onlinebanneradvertisingblog-rupizadshomeaboutbuzz(+)...
ReInfoSelect bmw325i(+) ...assoonasyourorderisfinalized,yourbmw325iaftermarketclutchpivotpinisdispatched(+)...
biblebee(+) ...generaloverviewbiblebeebasicsbee-attitudesstatementoffaithphilosophy(+)...
kansascityzoo(+) ...yahooexperiencekansascity-kansascity’snewzootakeasafarithroughafrica(+)...
invasivealienspecies(+) ...thenortheuropeanandbalticnetworkoninvasivealienspecies(nobanis)isagatewayto(+)...
powerload(-) ...inthealiensmovie,tocombatthequeenalien,ripleysteppedintoherapowerloader(+)...
Table9:Examplesofselectedanchorsandmanuallypicked 6 CONCLUSION
similarqueries.
ReInfoSelectleveragesthewidelyavailableanchordatatoweakly
superviseneuralrankersandmitigatestheirdependencyonlarge
ClueWeb09-BQuery Anchor
amountsofrelevancelabels.Tohandlethenoisesinanchordata,
dieting crashdieting
ReInfoSelectusespolicygradienttoconnectthedemand—the
frenchlickresortandcasino tropicanacasino&resortatlanticcity
needs of training signals from neural ranker—and the supply—
diabeteseducation veganmenuforpeoplewithdiabetes
anchor-documentpairs,toselectmoreeffectiveweaksupervision
incometaxreturnonline personalincometaxes
orangecountyconventioncenter orangecountyconventioncenter
signals.OnthreewidelystudiedTRECbenchmarks,ReInfoSelect
Robust04Query Anchor istheonlyweaksupervisionmethodthatguidesneuralrankers
mostdangerousvehicles vehicleinjurycases stablyoutperformfeature-basedlearningtorankmethods.Using
internationalartcrime artcrimes onlypubliclyavailabledata,italsonearlymatchestheeffectiveness
mexicanairpollution outdoorairpollution oftrainingsignalsfromprivatecommercialsearchlogs.
commercialcyanideuses cyanidepills Inourexperiments,ReInfoSelectrobustlyselectsbetteranchor-
elnino elnino documentpairsthanpreviousweaksupervisionapproachesdiscon-
ClueWeb12-B13Query Anchor nectedfromtargetneuralmodels.WealsofoundthatReInfoSelect
windpower windpowerinpa
tailorstheweaksupervisionforeachindividualNeu-IRmodelas
nbarecords nbarecords
well as its convergence status. Intuitively, ReInfoSelect starts
teddybears beaniebabyteddybears
withprovidingasmuchsupervisionaspossiblewhentherankeris
benefitsofyoga benefitsofyoga
random;thenitfurtherelevatestheneuralranker’sperformance
baldingcure baldingtreatment
using anchor-document pairs that well approximate query and
relevantdocuments.
ReInfoSelectalsoprovidesahandyexperimentgroundtoan-
5.8 CaseStudy alyzetheadvantagesanddisadvantagesofNeu-IR.Weshowre-
WefirststudyinstanceswhereReInfoSelectandDiscriminator sultsonhowneuralrankersworkwithfeature-basedlearningto
chosedifferentactions.Theexampleanchor-documentpairsare rankmethods,whatistheeffectivefine-tuningstrategiesontarget
listedinTable8.Someanchoranddocumentpairsarequitesimilar rankingtasks,andhowdifferenttrainingdataamountsandquery-
tosearchqueriesandrelevantdocuments,forexample,“kansascity documentfractionsinfluenceNeu-IRmodels.Theseanalyseswere
zoo”leadstotheYahoo!pageaboutit.Thereareanchorsthatare hardtodoasNeu-IRmodelswerelimitedbythelackoflargescale
morefunctional,e.g.,“bannerads”,andtoogeneral,e.g.,“power relevance-specificsupervision.ReInfoSelectprovidesasimple
load”.Thelinkeddocumentsmayalsobeirrelevant.Forexample, waytolessenthisdependencythuswillfacilitateandbroadenthe
thepicturespublishedbyNewYorkStatePoliceareasirrelevant impactofdeeplearningresearchininformationretrieval.
to “New York State Police”. Discriminator and ReInfoSelect
behavedifferentlyandasshowninlastexperiment,thedifference
7 ACKNOWLEDGEMENTS
wasmainlyfromthe(dis)connectionwiththetargetNeu-IRmodel.
WealsofindmanyanchorsselectedbyReInfoSelectverysimi- KaitaoZhangandZhiyuanLiuaresupportedbytheNationalNatu-
lartotheactualqueriesintheTRECbenchmarks.Table9listssome ralScienceFoundationofChina(NSFC61661146007,61772302).Si
examples.Theseanchorsreflectverysimilarinformationneeds Sun,HouyuZhang,KaitaoZhang,andZhenghaoLiuconducted
withtheactualqueries.Forexample,“veganmenuforpeoplewith humanevaluations.WethankGuoqingZheng,PaulBennett,andSu-
diabetes”isalegitwebsearchquery;onecanimagineitforminga sanDumaisfordiscussionsinweaksupervisionmethodologies,the
searchsessionwith“diabeteseducation”.TheseechotheclassicIR Anchorintuition,andtheiterativeviewofpretrainig-application.
intuitionthatmanyanchorsaresimilartosearchqueries,which WethankZhuyunDaiandJamieCallanforsharingthebasere-
areeffectivelyselectedbyReInfoSelect. trievalresultsonClueWeb09-BandRobust04.
SelectiveWeakSupervisionforNeuralInformationRetrieval WWW’20,April20–24,2020,Taipei,Taiwan,China
REFERENCES
2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP
[1] AminAhmad,NoahConstant,YinfeiYang,andDanielCer.2019. ReQA:An 2017,Copenhagen,Denmark,September9-11,2017.1049–1058.
EvaluationforEnd-to-EndAnswerRetrievalModels.CoRRabs/1907.04780(2019). [21] ReinerKraftandJasonY.Zien.2004.Mininganchortextforqueryrefinement.
[2] AdamL.BergerandJohnD.Lafferty.1999.InformationRetrievalasStatistical InProceedingsofthe13thinternationalconferenceonWorldWideWeb,WWW
Translation.InSIGIR’99:Proceedingsofthe22ndAnnualInternationalACMSIGIR 2004,NewYork,NY,USA,May17-20,2004.666–674.
ConferenceonResearchandDevelopmentinInformationRetrieval,August15-19, [22] KentonLee,Ming-WeiChang,andKristinaToutanova.2019.LatentRetrievalfor
1999,Berkeley,CA,USA.222–229. WeaklySupervisedOpenDomainQuestionAnswering.InProceedingsofthe57th
[3] W.BruceCroft,DonaldMetzler,andTrevorStrohman.2009. SearchEngines- ConferenceoftheAssociationforComputationalLinguistics,ACL2019,Florence,
InformationRetrievalinPractice.PearsonEducation. Italy,July28-August2,2019,Volume1:LongPapers.6086–6096.
[4] ZhuyunDaiandJamieCallan.2019. DeeperTextUnderstandingforIRwith [23] Zheng-HaoLiu,ChenyanXiong,MaosongSun,andZhiyuanLiu.2018.Entity-
ContextualNeuralLanguageModeling.InProceedingsofthe42ndInternational DuetNeuralRanking:UnderstandingtheRoleofKnowledgeGraphSemanticsin
ACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval, NeuralInformationRetrieval.InProceedingsofthe56thAnnualMeetingofthe
SIGIR2019,Paris,France,July21-25,2019.985–988. AssociationforComputationalLinguistics,ACL2018,Melbourne,Australia,July
[5] ZhuyunDai,ChenyanXiong,JamieCallan,andZhiyuanLiu.2018.Convolutional 15-20,2018,Volume1:LongPapers.2395–2405.
NeuralNetworksforSoft-MatchingN-GramsinAd-hocSearch.InProceedings [24] ChengLuo,YukunZheng,JiaxinMao,YiqunLiu,MinZhang,andShaopingMa.
oftheEleventhACMInternationalConferenceonWebSearchandDataMining, 2017.TrainingDeepRankingModelwithWeakRelevanceLabels.InDatabases
WSDM2018,MarinaDelRey,CA,USA,February5-9,2018.126–134. TheoryandApplications-28thAustralasianDatabaseConference,ADC2017,
[6] VanDangandW.BruceCroft.2010.Queryreformulationusinganchortext.In Brisbane,QLD,Australia,September25-28,2017,Proceedings.205–216.
ProceedingsoftheThirdInternationalConferenceonWebSearchandWebData [25] SeanMacAvaney,AndrewYates,ArmanCohan,andNazliGoharian.2019.CEDR:
Mining,WSDM2010,NewYork,NY,USA,February4-6,2010.41–50. ContextualizedEmbeddingsforDocumentRanking.InProceedingsofthe42nd
[7] MostafaDehghani,AliakseiSeveryn,SaschaRothe,andJaapKamps.2017.Learn- InternationalACMSIGIRConferenceonResearchandDevelopmentinInformation
ingtoLearnfromWeakSupervisionbyFullSupervision.CoRRabs/1711.11383 Retrieval,SIGIR2019,Paris,France,July21-25,2019.1101–1104.
(2017). [26] SeanMacAvaney,AndrewYates,KaiHui,andOphirFrieder.2019. Content-
[8] MostafaDehghani,HamedZamani,AliakseiSeveryn,JaapKamps,andW.Bruce BasedWeakSupervisionforAd-HocRe-Ranking.InProceedingsofthe42nd
Croft.2017.NeuralRankingModelswithWeakSupervision.InProceedingsof InternationalACMSIGIRConferenceonResearchandDevelopmentinInformation
the40thInternationalACMSIGIRConferenceonResearchandDevelopmentin Retrieval,SIGIR2019,Paris,France,July21-25,2019.993–996.
InformationRetrieval,Shinjuku,Tokyo,Japan,August7-11,2017.65–74. [27] TomasMikolov,IlyaSutskever,KaiChen,GregoryS.Corrado,andJeffreyDean.
[9] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT: 2013. DistributedRepresentationsofWordsandPhrasesandtheirComposi-
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.In tionality.InAdvancesinNeuralInformationProcessingSystems26:27thAnnual
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation ConferenceonNeuralInformationProcessingSystems2013.Proceedingsofameeting
forComputationalLinguistics:HumanLanguageTechnologies,NAACL-HLT2019, heldDecember5-8,2013,LakeTahoe,Nevada,UnitedStates.3111–3119.
Minneapolis,MN,USA,June2-7,2019,Volume1(LongandShortPapers).4171– [28] TriNguyen,MirRosenberg,XiaSong,JianfengGao,SaurabhTiwary,Rangan
4186. Majumder,andLiDeng.2016. MSMARCO:AHumanGeneratedMAchine
[10] FernandoDiaz,BhaskarMitra,andNickCraswell.2016.QueryExpansionwith ReadingCOmprehensionDataset.InProceedingsoftheWorkshoponCognitive
Locally-TrainedWordEmbeddings.InProceedingsofthe54thAnnualMeetingof Computation:Integratingneuralandsymbolicapproaches2016co-locatedwiththe
theAssociationforComputationalLinguistics,ACL2016,August7-12,2016,Berlin, 30thAnnualConferenceonNeuralInformationProcessingSystems(NIPS2016),
Germany,Volume1:LongPapers. Barcelona,Spain,December9,2016.
[11] ZhichengDou,RuihuaSong,Jian-YunNie,andJi-RongWen.2009.Usinganchor [29] RodrigoNogueiraandKyunghyunCho.2019.PassageRe-rankingwithBERT.
textswiththeirhyperlinkstructureforwebsearch.InProceedingsofthe32nd CoRRabs/1901.04085(2019).
AnnualInternationalACMSIGIRConferenceonResearchandDevelopmentin [30] RodrigoNogueira,WeiYang,JimmyLin,andKyunghyunCho.2019.Document
InformationRetrieval,SIGIR2009,Boston,MA,USA,July19-23,2009.227–234. ExpansionbyQueryPrediction.CoRRabs/1904.08375(2019).
[12] NadavEironandKevinS.McCurley.2003.Analysisofanchortextforwebsearch. [31] HarshithPadigela,HamedZamani,andW.BruceCroft.2019.Investigatingthe
InSIGIR2003:Proceedingsofthe26thAnnualInternationalACMSIGIRConference SuccessesandFailuresofBERTforPassageRe-Ranking.CoRRabs/1905.01758
onResearchandDevelopmentinInformationRetrieval,July28-August1,2003, (2019).
Toronto,Canada.459–460. [32] LiangPang,YanyanLan,JiafengGuo,JunXu,ShengxianWan,andXueqiCheng.
[13] JiafengGuo,YixingFan,QingyaoAi,andW.BruceCroft.2016.ADeepRelevance 2016.TextMatchingasImageRecognition.,2793–2799pages.
MatchingModelforAd-hocRetrieval.InProceedingsofthe25thACMInternational [33] LiangPang,YanyanLan,JiafengGuo,JunXu,JingfangXu,andXueqiCheng.
ConferenceonInformationandKnowledgeManagement,CIKM2016,Indianapolis, 2017.DeepRank:ANewDeepArchitectureforRelevanceRankinginInformation
IN,USA,October24-28,2016.55–64. Retrieval.InProceedingsofthe2017ACMonConferenceonInformationand
[14] JiafengGuo,YixingFan,LiangPang,LiuYang,QingyaoAi,HamedZamani,Chen KnowledgeManagement,CIKM2017,Singapore,November06-10,2017.257–266.
Wu,W.BruceCroft,andXueqiCheng.2019.ADeepLookintoNeuralRanking [34] JeffreyPennington,RichardSocher,andChristopherD.Manning.2014.Glove:
ModelsforInformationRetrieval.CoRRabs/1903.06902(2019). GlobalVectorsforWordRepresentation.InProceedingsofthe2014Conference
[15] BoHan,QuanmingYao,XingruiYu,GangNiu,MiaoXu,WeihuaHu,IvorW. onEmpiricalMethodsinNaturalLanguageProcessing,EMNLP2014,October25-
Tsang,andMasashiSugiyama.2018.Co-teaching:Robusttrainingofdeepneural 29,2014,Doha,Qatar,AmeetingofSIGDAT,aSpecialInterestGroupoftheACL.
networkswithextremelynoisylabels.InAdvancesinNeuralInformationProcess- 1532–1543.
ingSystems31:AnnualConferenceonNeuralInformationProcessingSystems2018, [35] MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,Christopher
NeurIPS2018,3-8December2018,Montréal,Canada.8536–8546. Clark,KentonLee,andLukeZettlemoyer.2018.DeepContextualizedWordRep-
[16] DanHendrycks,MantasMazeika,DuncanWilson,andKevinGimpel.2018.Using resentations.InProceedingsofthe2018ConferenceoftheNorthAmericanChapter
TrustedDatatoTrainDeepNetworksonLabelsCorruptedbySevereNoise.In oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,
AdvancesinNeuralInformationProcessingSystems31:AnnualConferenceon NAACL-HLT2018,NewOrleans,Louisiana,USA,June1-6,2018,Volume1(Long
NeuralInformationProcessingSystems2018,NeurIPS2018,3-8December2018, Papers).2227–2237.
Montréal,Canada.10477–10486. [36] MaryArpitaPyreddy,VarshiniRamaseshan,NarendraNathJoshi,ZhuyunDai,
[17] SebastianHofstätter,NavidRekabsaz,CarstenEickhoff,andAllanHanbury.2019. ChenyanXiong,JamieCallan,andZhiyuanLiu.2018.ConsistencyandVariation
OntheEffectofLow-FrequencyTermsonNeural-IRModels.InProceedingsof inKernelNeuralRankingModel.InThe41stInternationalACMSIGIRConference
the42ndInternationalACMSIGIRConferenceonResearchandDevelopmentin onResearch&DevelopmentinInformationRetrieval,SIGIR2018,AnnArbor,MI,
InformationRetrieval,SIGIR2019,Paris,France,July21-25,2019.1137–1140. USA,July08-12,2018.961–964.
[18] BaotianHu,ZhengdongLu,HangLi,andQingcaiChen.2014.Convolutional [37] YifanQiao,ChenyanXiong,Zheng-HaoLiu,andZhiyuanLiu.2019.Understand-
NeuralNetworkArchitecturesforMatchingNaturalLanguageSentences.In ingtheBehaviorsofBERTinRanking.CoRRabs/1904.07531(2019).
AdvancesinNeuralInformationProcessingSystems27:AnnualConferenceon [38] ChenQu,FengJi,MinghuiQiu,LiuYang,ZhiyuMin,HaiqingChen,JunHuang,
NeuralInformationProcessingSystems2014,December8-132014,Montreal,Quebec, andW.BruceCroft.2019.LearningtoSelectivelyTransfer:ReinforcedTransfer
Canada.2042–2050. LearningforDeepTextMatching.InProceedingsoftheTwelfthACMInternational
[19] Po-SenHuang,XiaodongHe,JianfengGao,LiDeng,AlexAcero,andLarryP. ConferenceonWebSearchandDataMining,WSDM2019,Melbourne,VIC,Australia,
Heck.2013. Learningdeepstructuredsemanticmodelsforwebsearchusing February11-15,2019.699–707.
clickthroughdata.In22ndACMInternationalConferenceonInformationand [39] MengyeRen,WenyuanZeng,BinYang,andRaquelUrtasun.2018. Learning
KnowledgeManagement,CIKM’13,SanFrancisco,CA,USA,October27-November toReweightExamplesforRobustDeepLearning.InProceedingsofthe35th
1,2013.2333–2338. InternationalConferenceonMachineLearning,ICML2018,Stockholmsmässan,
[20] KaiHui,AndrewYates,KlausBerberich,andGerarddeMelo.2017.PACRR:A Stockholm,Sweden,July10-15,2018.4331–4340.
Position-AwareNeuralIRModelforRelevanceMatching.InProceedingsofthe
WWW’20,April20–24,2020,Taipei,Taiwan,China KaitaoZhang♥,ChenyanXiong♠,ZhenghaoLiu♥,andZhiyuanLiu♥
[40] YelongShen,XiaodongHe,JianfengGao,LiDeng,andGrégoireMesnil.2014.A ConferenceonResearchandDevelopmentinInformationRetrieval,SIGIR2019,
LatentSemanticModelwithConvolutional-PoolingStructureforInformation Paris,France,July21-25,2019.1129–1132.
Retrieval.InProceedingsofthe23rdACMInternationalConferenceonConferenceon [48] HamedZamaniandW.BruceCroft.2017.Relevance-basedWordEmbedding.
InformationandKnowledgeManagement,CIKM2014,Shanghai,China,November InProceedingsofthe40thInternationalACMSIGIRConferenceonResearchand
3-7,2014.101–110. DevelopmentinInformationRetrieval,Shinjuku,Tokyo,Japan,August7-11,2017.
[41] BoWang,MinghuiQiu,XisenWang,YaliangLi,YuGong,XiaoyiZeng,JunHuang, 505–514.
BoZheng,DengCai,andJingrenZhou.2019.AMinimaxGameforInstancebased [49] HamedZamaniandW.BruceCroft.2018.OntheTheoryofWeakSupervision
SelectiveTransferLearning.InProceedingsofthe25thACMSIGKDDInternational forInformationRetrieval.InProceedingsofthe2018ACMSIGIRInternationalCon-
ConferenceonKnowledgeDiscovery&DataMining,KDD2019,Anchorage,AK, ferenceonTheoryofInformationRetrieval,ICTIR2018,Tianjin,China,September
USA,August4-8,2019.34–43. 14-17,2018.147–154.
[42] RonaldJ.Williams.1992.SimpleStatisticalGradient-FollowingAlgorithmsfor [50] HamedZamani,W.BruceCroft,andJ.ShaneCulpepper.2018. NeuralQuery
ConnectionistReinforcementLearning.MachineLearning8(1992),229–256. PerformancePredictionusingWeakSupervisionfromMultipleSignals.InThe
[43] LongXia,JunXu,YanyanLan,JiafengGuo,WeiZeng,andXueqiCheng.2017. 41stInternationalACMSIGIRConferenceonResearch&DevelopmentinInformation
AdaptingMarkovDecisionProcessforSearchResultDiversification.InProceed- Retrieval,SIGIR2018,AnnArbor,MI,USA,July08-12,2018.105–114.
ingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopment [51] WeiZeng,JunXu,YanyanLan,JiafengGuo,andXueqiCheng.2018.MultiPage
inInformationRetrieval,Shinjuku,Tokyo,Japan,August7-11,2017.535–544. SearchwithReinforcementLearningtoRank.InProceedingsofthe2018ACM
[44] ChenyanXiong,JamieCallan,andTie-YanLiu.2017.Word-EntityDuetRepre- SIGIRInternationalConferenceonTheoryofInformationRetrieval,ICTIR2018,
sentationsforDocumentRanking.InProceedingsofthe40thInternationalACM Tianjin,China,September14-17,2018.175–178.
SIGIRConferenceonResearchandDevelopmentinInformationRetrieval,Shinjuku, [52] HongfeiZhang,XiaSong,ChenyanXiong,CorbyRosset,PaulN.Bennett,Nick
Tokyo,Japan,August7-11,2017.763–772. Craswell,andSaurabhTiwary.2019. GenericIntentRepresentationinWeb
[45] ChenyanXiong,ZhuyunDai,JamieCallan,ZhiyuanLiu,andRussellPower. Search.InProceedingsofthe42ndInternationalACMSIGIRConferenceonResearch
2017.End-to-EndNeuralAd-hocRankingwithKernelPooling.InProceedings andDevelopmentinInformationRetrieval,SIGIR2019,Paris,France,July21-25,
ofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentin 2019.65–74.
InformationRetrieval,Shinjuku,Tokyo,Japan,August7-11,2017.55–64. [53] YukunZheng,ZhenFan,YiqunLiu,ChengLuo,MinZhang,andShaoping
[46] ChenyanXiong,ZhengzhongLiu,JamieCallan,andTie-YanLiu.2018.Towards Ma.2018.Sogou-QCL:ANewDatasetwithClickRelevanceLabel.InThe41st
BetterTextUnderstandingandRetrievalthroughKernelEntitySalienceModeling. InternationalACMSIGIRConferenceonResearch&DevelopmentinInformation
InThe41stInternationalACMSIGIRConferenceonResearch&Developmentin Retrieval,SIGIR2018,AnnArbor,MI,USA,July08-12,2018.1117–1120.
InformationRetrieval,SIGIR2018,AnnArbor,MI,USA,July08-12,2018.575–584. [54] YukunZheng,YiqunLiu,Zhi-QiangFan,ChengLuo,QingyaoAi,MinZhang,
[47] WeiYang,KuangLu,PeilinYang,andJimmyLin.2019. CriticallyExamining andShaopingMa.2019.InvestigatingWeakSupervisioninDeepRanking.Data
the"NeuralHype":WeakBaselinesandtheAdditivityofEffectivenessGains andInformationManagement3(2019),155–164.
fromNeuralRankingModels.InProceedingsofthe42ndInternationalACMSIGIR

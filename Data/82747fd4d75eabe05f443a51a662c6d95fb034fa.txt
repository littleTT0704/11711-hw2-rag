Code-Mixed Question Answering Challenge: Crowd-sourcing Data and
Techniques
(cid:63)KhyathiRaghaviChandu (cid:5)EkaterinaLoginova
(cid:52)VishalGupta (cid:5)JosefvanGenabith (cid:5)Gu¨nterNeuman
(cid:53)ManojChinnakotla (cid:63)EricNyberg (cid:63)AlanBlack
(cid:63)LanguageTechnologiesInstitute,CarnegieMellonUniversity
(cid:5)DeutscheForschungszentrumfu¨rKu¨nstlicheIntelligenz,(cid:52)IIITHyderabad,(cid:53)Microsoft,USA
{kchandu, ehn, awb}@andrew.cmu.edu
{ekaterina.loginova, Josef.van Genabith, neumann}@dfki.de
vishal.gupta@research.iiit.ac.in, manojc@microsoft.com
Abstract (Muysken, 2000)). Traditionally, some studies
(Yow and Patrycia, 2011) have viewed the mix-
Code-Mixing (CM) is the phenomenon
ing of two independent codes as lack of fluency
of alternating between two or more lan-
of the segment of population in either of the lan-
guages which is prevalent in bi- and
guages. However, an alternate perspective (Mil-
multi-lingual communities. Most NLP
roy and Muysken, 1995) argues that mixing of
applications today are still designed
two traditionally isolated linguistic codes poten-
with the assumption of a single inter-
tiallycreatesathirdlegitimatecode. Researchers
action language and are most likely to
(Crystal,1997)havealsopresentedseveralsocio-
break given a CM utterance with mul-
cultural reasons and motivations for switching.
tiple languages mixed at a morphologi-
There have been studies to depict the usage of
cal, phrase or sentence level. For ex-
particular language based on the emotional at-
ample, popular commercial search en-
tachmentandthesentimentofthepersontowards
gines do not yet fully understand the
that topic (Rudra et al., 2016). In this paper,
intents expressed in CM queries. As
we adopt the perspective of descriptive linguis-
a first step towards fostering research
tics and make an effort to describe this prevalent
which supports CM in NLP applica-
formoflanguageasitoccurs,withoutadoptinga
tions, we systematically crowd-sourced
prescriptiveapproach.
and curated an evaluation dataset for
factoid question answering in three CM Ubiquitous access to social media tools and
languages - Hinglish (Hindi+English), platforms have also made CM the preferred
Tenglish (Telugu+English) and Tamlish choice for both formal and informal commu-
(Tamil+English)whichbelongtotwolan- nication. In such settings, where the commu-
guage families. We share the details of nication is either semi-formal or informal, re-
our data collection process, techniques searchers ((Bali et al., 2014), (Barman et al.,
which were used to avoid inducing lexi- 2014))haveobservedahighertendencyformulti-
cal bias amongst the crowd workers and lingual speakers to use CM. We studied a sam-
other CM specific linguistic properties of ple of conversation logs from a commercial chit-
the dataset. Our final dataset, which is chatbasedconversationalagentintheIndianmar-
availablefreelyforresearchpurposes,has ket. The agent was trained to engage in infor-
1,694 Hinglish, 2,848 Tamlish and 1,391 malchatconversationswiththehelpofadatabase
Tenglish factoid questions and their an- of Twitter conversations from the Indian mar-
swers. Wediscussthetechniquesusedby ket. Since India is a multilingual country with
theparticipantsforthefirsteditionofthis a large number of multilingual speakers, we no-
ongoingchallenge. tice that users often freely use each language in-
dividually or their CM versions while convers-
1 Introduction
ing with the agent. We notice that, in around
Code-Mixing (CM) is formally defined as the 3% of overall conversations, users were found to
embedding of linguistic units such as phrases, be chatting with the agent in CM language such
words, and morphemes of one language into as ‘hello, kya chal raha hai’ (Meaning: hello,
an utterance of another language, which is what’s up?). Interestingly, in cases where the re-
commonly observed in multilingual communi- sponse of the agent was in CM language such as
ties ((Myers-Scotton, 1997), (Poplack, 1980), ‘sorry yaar’ (Meaning: sorry friend), users too
responded back in CM language in 27% of those 2 RelatedWork
times. There have been other studies regarding
the quantitative and qualitative aspects of code- Early work in this domain include investigat-
switchingonsocialmediaalongsimilarlines(Hi- ing CM phenomenon in a formal and compu-
dayat, 2008). However, a large number of NLP tational framework (Joshi, 1982) and develop-
applications, such as Question Answering (QA), ing formalisms (Goyal et al., 2003), (Sinha and
Dialogue Systems, Summarization etc, still con- Thakur, 2005). Recent years have seen atten-
tinue to be designed with the assumption of a tion towards part of speech tagging for CM lan-
singleinteractionlanguagesuchasEnglish(Brill guagesandgatheringcorpora((Vyasetal.,2014),
et al., 2002), Hindi ((Kumar et al., 2005)), Chi- (Solorio and Liu, 2008), (Jamatia et al., 2015),
nese ((Yongkui et al., 2003), (Sun et al., 2008)). (Soto and Hirschberg, 2017)) for it. Language
SuchsystemsaremostlikelytobreakgivenaCM identification in mixed language scenarios has
utterance which has multiple languages mixed at alsobeenstudiedrecently((Barmanetal.,2014),
sentence, phrase or morphological level. Hence, (Chittaranjan et al., 2014)) and has also been
itishighlyimperativeforresearcherstofocuson aggressively addressed as a shared task at ma-
building more robust end-user NLP applications jor conferences ((Solorio et al., 2014), (Sequiera
whichcanunderstandandprocessCMlanguage. et al., 2015)). Some of the other applications
that were picked up in research in CM over the
past few years include Named Entity Recogni-
tion (Zirikly and Diab, 2015), semantic parsing
Building a good evaluation dataset for Factoid
(Duong et al., 2017), dependency parsing (Par-
QA in CM is wrought with challenges such as a)
tanen et al., 2018) and shallow parsing (Sharma
ensuring that the annotators are unbiased in any-
et al., 2016). While the above work focusses on
way to artificially use CM b) recruiting a good
importantlanguageprocessingchallengesinCM,
team of native bi-lingual speakers as annotators
we are more interested in end-user NLP applica-
c) maintaining a good quality and diversity of
tions which support CM such as Factoid QA in
questions across intents, answer types and enti-
CMlanguages.
ties. In this paper, we describe our experience
in dealing with the above challenges while cre- Eliciting a corpus of CM questions by para-
atingthedataset. Weusedacrowd-sourcingplat- phrasinganEnglishquestionwasusedtoperform
form for collecting data where the crowd work- question classification (Raghavi et al., 2015).
erswererestrictedtoonlynativelanguage(Hindi, While this method has the advantage of having
TeluguandTamil)speakers. Wesharedadetailed agroundtruthparalleltext,thepossibilityoflex-
set of guidelines and instructions about the task icalbiasfromtheEnglishquestionwhileframing
withthecrowdworkersandalsoranthemthrough the code-mixed question exists. An extension to
somebasicqualitychecksbeforecollectionofac- thisworkwasproposedbybuildinganend-to-end
tual data. Finally, we were able to collect around webbasedCMquestionansweringsystemnamed
1,694Hinglish,2,848Tamlishand1,391Tenglish WebShodh (Chandu et al., 2017). Efforts have
factoid questions along with their answers. We been made to develop cross lingual QA systems
have organized a Code-Mixed Question Answer- that take questions in English and answer back
ing challenge based on this data for the first edi- in English but search for candidate answers in
tionofthischallenge. Thereare7teamsthatreg- Hindi newspapers (Sekine and Grishman, 2003)
istered and took the data from us. In this paper, along with other machine learning approaches
we discuss the preliminary techniques that 2 of (Nanda et al., 2016). There has been some work
these groups used. To summarize, the following in the early 2000s to generate a dialog based QA
arethemaincontributionsofthispaper: system in Telugu to support Railway inquiries
• We curated an evaluation dataset for the task (Reddy and Bandyopadhyay, 2006). This kind
of Factoid QA in CM languages with more than ofcrosslanguageQAsystemisbeingresearched
5000QApairsforHinglish,TamlishandTenglish for European languages as well (Neumann and
languages. Wealsomakeitfreelyavailableforre- Sacaleanu, 2003). A dataset of 506 questions
searchpurposes. from messages from Facebook was proposed in
• We share our experiences related to eliciting theBengali-EnglishCMdomain(Banerjeeetal.,
lexicallyunbiasedCMquestionsbyusingimages 2016). Our dataset is over ten times larger than
asanchorpoints. this data and takes into account the lexical varia-
• We present the techniques used in the first edi- tion brought in by collecting questions from im-
tionoftheCMQAchallenge. agesandcode-mixedarticles.
3 DatasetCollection providing the context of the images (for image
based questions)? The design of the task to col-
In order to study differences between lexical
lect questions based on images, in order to study
bias from entrainment and elicit lexically diverse
the comparative lexical bias when a code-mixed
questions, we employ two modes of data col-
article is given, has resulted in a lot of questions
lection: eliciting code-mixed questions from a)
that are related to multi-modal reasoning. For
images and b) code-mixed articles. The for-
example, Tenglish question ‘image lo entha
mer are general questions and the latter are con-
mandi unnaaru?’ (Meaning: How many people
text specific questions (similar to machine read-
arethereintheimage?) requiresavisualinorder
ing). Techniques of collecting queries for a di-
toanswer. Weremovedsuchquestionsinthepost
alogsystembypresentingscenariossymbolically
processingafterdatacollection.
anddiagrammaticallywaspreviouslyused(Black
et al., 2011) in order to minimize supplying lex-
Weensuredagoodmixofcategorieswhilese-
ical and phrasal cues. For collection of Hinglish
lecting the target fulcrum entity images (exam-
data, we used both these approaches whereas for
ple: guitar, bicycle), location (example: Eiffel
collecting Tenglish and Tamlish data, we used
Tower, Golden Gate Bridge), person (example:
only images. This is because for Hinglish, we
Roger Federer, Eminem), event (example: World
could find informative blogging websites based
War2,DandiMarch). Outofthese,wemanually
onwhichitiseasiertoframefactoidcode-mixed
selected 80 images from which factoid questions
questions. However, to the best of our knowl-
can be asked. To gather questions based on arti-
edge,duringthetimeofourannotation,suchfact
cles,wefirstscrapeddocumentsfromhinglishpe-
based code-mixed content was still not available
dia.com,randomlyselected80articlesfromthem
inTenglish/Tamlish. Itisalsonotedthatitisless
and made sure that all of them were code-mixed.
likely to get questions that have abstract answers
The crowd-workers are then requested to form
(beyondtherealmofphysicalentities)whenthey
factoidquestionsbasedonthesearticlessuchthat
arecollectedbasedonimages.
theanswerstothequestionsarepresentinthecor-
3.1 ChallengesinCode-MixedFactoid respondingarticle.
QuestionsCollection
3.2 Crowd-sourcingforQuestion&Answer
We faced the following challenges during the
Collection
datacollectiontask.
1. How would we eliminate the bias towards We engaged with two streams of demographics
using English in general scenarios while using while collecting the data: university students and
a search engine etc.,? In other words, we need crowd-workers. Each participant is allowed to
to encourage crowd workers to provide us with provide us with only 20 questions to avoid idi-
datathatisneitherbiasedtoEnglishmonolingual olectic biases i.e, biases of each individual. In
questions due to preconceptions of the language thefirststep,weperformedtheactivityinamore
preferencewhileinteractingwithacomputer,nor controlled environment in university classrooms.
bias them to provide mixed language data if it The instructors of the classrooms were requested
doesnotfeelnaturaltothem. to give a brief presentation we made about what
2. How do we eliminate responses from people code-mixing is along with some example ques-
who are not native speakers? To mitigate this tions. This was performed to alleviate the bias
problem, we have given the instructions to each against mixing while interacting with a machine.
of these target languages in romanized code- Thestudents(withnativelanguagesamongHindi,
mixed version of the corresponding languages Telugu and Tamil) were given clear explanations
mixed with English. This has the dual advantage about factoid and non-factoid questions in order
of being understood only by those who have toelicittherightkindofquestionsforourtask.
enough competence in the matrix language as In the second phase, we migrated this setup
wellaseasingthemintocode-mixingandmaking to Amazon Mechanical Turk task, where crowd-
themcomfortablewithit. workers were redirected to our interface1 of
3. How do we elicit factoid questions? This is mixedlanguageinstructionsbasedontheirnative
a trivial issue. We had to explain what a factoid language. EachacceptedHumanIntelligenceTest
question is while providing sufficient examples
1https://docs.google.com/
offactoidandnon-factoidquestions.
document/d/1CTFTjmU6RKUwsNH1z0Sjl_
4. How do we collect questions that are general
8EZt8dzl-VGF5VwPLIpy0/edit?usp=
enough that they could be answered without sharing(toanonymize)
CategoryofQuestions Num MultilingualIndex LanguageEntropy IntegrationIndex AvgLength
Hinglishimagequestions 1,419 0.72 0.61 0.25 7.50
Hinglisharticlequestions 275 0.88 0.66 0.29 8.90
Tamlishquestions 2,848 0.69 0.59 0.24 5.56
Tenglishquestions 1,391 0.80 0.64 0.28 5.90
Table 1: Data Statistics: The code-mixing metrics for Hinglish (Hindi+English), Tamlish
(Tamil+English)andTenglish(Telugu+English)questions
(HIT) was compensated with $2.50 on the Turk (Meaning: in the image), ‘ye’ (Meaning: this),
setup. WehavegotalotmoreresponsesforTam- ’iss’ (Meaning: this) and separated the questions
lish questions as compared to both Hinglish and that contain these expressions. The same pro-
Tenglish together as reflected in Table 1. In this cess was not done for questions collected based
scenario, although the turkers have not been for- on code-mixed articles. This is because referring
mally explained about what code-mixing is apart expressionscorrespondingtothegiventextdonot
fromprovidingthemwithinstructionsandexam- hinder searching for an answer in the given snip-
ples,mostofthedatawereceivedincludedmixed pet. Lexical level language identification is per-
languageRomanizedquestions. Whileinthefor- formed to remove the questions that do not have
mer scenario, the collected questions were ex- atleast one word from both the languages. These
plicitlymoderatedtoremainwithinbilingualand selected questions after filtering are then curated
multilingualenvironments,likeinIndia,thelatter andgonethroughmanuallytoaddbacktheques-
scenariodoesnotensurethis,astheydonothave tions that made sense before rejecting the HITs.
tobepresentinIndia. Theextentofcode-mixing The next level of curation was performed during
and fluency of the questions may vary as com- the answer collection phase. This was necessary
paredtothequestionscollectedfromIndianclass- becauseitwasstillpossibletobypassthesecura-
room environment due to the difference in their tionconditions. Forexample,thereweresomeen-
socio-cultural environments. Though we have triesthatseemedlikeEnglishquerieswithanad-
mentioned in the instructions to provide us with ditionalsuffixbelongingtothecorrespondingna-
questions that sound natural to the participants, tivelanguageattheendofsomeofthewords. For
we acknowledge that it may not have been com- example,‘WhatilisilwaterfallaborderilAmerica
pletelynaturalastheywereexplicitlyrequestedto andCanada?’ (Tamlishquestionfor‘Whatisthe
mixtwolanguages. Thethirdphaseinvolvescol- waterfall in the border of America and Canada’).
lectinganswerstoallthequestions. Monolingual On the other hand there are queries that seemed
questionsandimage-basedquestionsthatcontain tohavebeentranslatedusinganonlinetranslation
referringexpressions,suchas‘inthisfigure’were toolintothematrixlanguageandrandomlyinsert-
removed before collecting answers. To filter out ingsomeEnglishwordsinbetween. Forexample,
noisy and random answers, the set-up includes ‘MeinwhichIndianstatedidMotherTeresakaam
a qualifying CM question for which we clearly kiya?’ (Hinglish question). This example seems
knew the answer. When collecting answers, we tobealexicalleveltranslationoffirst,eighthand
onlyacceptthemfromworkerswhocorrectlyan- ninthwordsoftheEnglishquestion‘InwhichIn-
swerthequalifyingquestion. dian state did Mother Teresa work (past-tense)?’
into Hindi. 67.87% of the data collected from
3.3 CurationandPost-processing Turkwasacceptableandpassedourcurationtests.
Among the remaining, about 21% were rejected
After data collection, we removed duplicate en-
due to the use of referring expressions, 11% due
tries and also performed one step of human veri-
to erroneous attempts by typing junk words. All
fication. Thisresponsibilitywasdividedintotwo
the questions passed the curation tests for more
phases. Thefirststepwasemployingcertainpost
than 90% of the accepted HITs and some of the
processingstepsinordertoremovethequestions
questionswereacceptablefortheremaining10%.
that did not match the presented specifications
Thisimpliesthattheinstructionsprovidedforthe
and rejecting them. One major problem is the
task were sufficiently clear to elicit CM factoid
use of referring expressions and determiners cor-
questions. Theabovearethestatisticsforthedata
responding to the images about which the ques-
correspondingtothecrowdsourcedplatformthat
tions were asked. In each of the three languages,
might provide a baseline estimate for collecting
we made a list of all possible spelling variants
useful data for this domain on such platforms.
ofreferringexpressionslike‘image/picturemein’
Since the number of curators is much less (ap- type distribution was found to be 25 PERSON,
proximately6people)thanthenumberofcrowd- 9 ENTITY, 22 LOCATION and 15 NUMERIC.
workers, we need to understand that the curation An interesting observation we noticed was that -
processismuchmoreexpensiveintermsofman- there were 23 Tenglish and 17 Hinglish variants
ualeffort. Theabovestepsaretakentoelicitqual- ofthequestion‘WhobuiltTajMahal?’ andsimi-
ity data for our purposes. The tasks of collect- larly there were 12 Tenglish and 8 Hinglish vari-
ing questions and answers were deliberately sep- ants of the question ‘In which city is Taj Mahal
aratedfortworeasons. Oneistoensureclarityof located?’. A similar analysis for the other focus
the task and make sure that the users are giving entity‘Hiroshima’gaveus21Tenglishquestions
naturally code-mixed questions and asking them distributedas7NUMERIC,9LOCATION,4EN-
to provide answers in English within the same TITY,1PERSONtypequestionsand14Hinglish
task might lead to unnecessary biases or confu- questionsdistributedas10NUMERIC,2LOCA-
sion. Thesecondreasonisthatwhenaskedtopro- TION, 1 ENTITY and 1 PERSON type ques-
vide questions and answers together, one might tions. Among these we observed 4 and 8 vari-
tend to ask the simplest questions to which an- antsinTenglishandHinglishrespectivelyforthe
swers are already known to them. This in turn question ‘In which year did attack on Hiroshima
might have reduced the variety of questions for and Nagasaki take place?’. These statistics re-
the same anchor point. We have also collected vealthat,forthesameimageshowntothem,par-
feedback about each question whether it is a fac- ticipantsissuedquestionsresultinginavarietyof
toid and if additional multi-modal information is answertypesinthetargetcode-mixedlanguages.
neededtoanswerit, duringtheanswercollection Figure 1 shows the percentage distribution of the
phase. question types for ‘Taj Mahal’ and ‘Hiroshima’
acrossthethreelanguagepairs.
4 DataAnalysis
In order to gain better intuitions on the ‘why
and how’ of code-mixing, the collected ques-
Recent studies have focused on empirical mea-
tions are studied with respect to idiolectic lan-
surements of code-switching (Guzma´n et al.,
guage preferences, i.e the idiosyncrasies of mix-
2017). The multilingual index(M-Index), Lan-
ing the languages and the extent of code-mixing
guage Entropy and Integration index(I-index)
across languages. To study the individual mix-
measure the extent of mixing and switching fre-
ingbiasesinthedata,MultilingualIndexiscalcu-
quency. Table 1 presents the statistics of out
lated for each individual in each of the language
dataset along with these metrics for mixing. The
pairs. Figure 2 shows histograms for idiolects of
averagenumberofwordsperquestionishigherin
Hinglish, Tenglish and Tamlish. As can be ob-
HinglishcomparedtoTamlishandTenglish. The
servedfromthefigure,HinglishandTenglishhas
M-index for all the 3 language pairs are in com-
many crowd-workers towards the higher end of
parable ranges while it is slightly less for Tam-
multilingual index whereas Tamlish has a rather
lish. The questions are provided along with the
smootherdistributionexceptforthelastrange.
following information: (1) language information
(2) question type annotated as ‘context depen-
4.2 LexicalBiasinArticlebasedQuestions
dent’(forarticlebasedquestions)and‘contextin-
dependent’ (for images based questions) and (3) The bias of copying the words was mitigated to
correspondingarticleforarticlebasedquestions. anextentbytheusageofimagesasanchorpoints
to collect questions. However, studying the lex-
4.1 AnswerTypeDistribution
ical bias when a code-mixed article is given acts
In order to analyze the distribution of question as a proxy to study entrainment. The variant of
types in our dataset, we sampled questions, from expressing the questions from code-mixed arti-
TenglishandHinglish,whichcontaineitherofthe clesservestwopurposes. Oneistostudythedif-
following two selected images - ‘Taj Mahal’ and ferences in difficulty of down stream task of re-
‘Hiroshima’. We use the coarse and fine-grained trieving for question answering as compared to
typehierarchiesproposedby(LiandRoth,2006) the image based questions. In this category of
for annotating the questions. For the first im- questions, the answer is present in the snippets
age: ‘Taj Mahal’, we had 91 Tenglish questions that are given and the focus is primarily on re-
and 71 Hinglish questions. For Tenglish ques- trieving the answers from within the given text.
tions, the distribution of coarse level types were The second is to study the varying lexical biases
34 PERSON, 8 ENTITY, 30 LOCATION and 19 to frame a question when code-mixed content is
NUMERIC. For Hinglish questions, the coarse- givenversuswhenitisnot. Tostudythisempiri-
Figure1: DistributionofquestiontypesinHinglishandTenglishfor2topics: TajMahalandHiroshima
(a) Hinglish idiolectic mixing distribu-(b) Tenglish idiolectic mixing distribu-(c)Tamlishidiolecticmixingdistribution
tion tion
Figure2: HistogramsforMultilingualindexforidiolectsforHinglish,TenglishandTamlish
cally,wecalculatedthepercentageofintersection also tried avoiding completely random, spurious
of words between question and articles. The av- andnoisyinputsbycheckingiftheyweresimply
erage of overlapping words is 54.20%, while the permutations of the original input and their lexi-
minimum and maximum are 12.5% and 92.31% callytranslatedwords.
respectively. Similarly, the longest overlapping
subsequences have a mean of 2.24 with a mini- A known problem in dealing with code-mixed
mumof1wordandamaximumof16words. text is non-standardized Romanization of native
language when mixed with English. Phonologi-
4.3 MixingPhenomenaobservedintheData
cal perceptions of a syllable can be represented
One of the interesting categories of mixing that differently. For example, from the data a cou-
is observed in the data is mixing gender infor- ple of the very frequent such variations are ‘kon’
mation of the native and the mixed form of the and‘kaun’for‘who’inHindi,and‘he’and‘hai’
word. For example, consider the question from for ‘is’ in Hindi. For both these words, the lat-
thedata,‘earthkabformhuithi?’ (Englishmean- ter variants are closer to the pronunciation of the
ing: When was Earth formed?). A paraphrase of Hindi words, but the other sounds are in col-
the same question is ‘dharthi kab form hui thi?’. loquial usage frequently as well. Consider the
InHindi,thegenderoftheverbhastoagreewith question, ‘Friends serial ke kitne seasons ba-
the subject. While the gender of Earth is mas- naye ja chukein hain?’ (Meaning: How many
culine (which should have agreed with ‘hua’ and seasons were made for Friends serial?). Using
not ‘hui’), the gender of dharthi (which agrees ‘n’ in ‘chukein’ indicates that the person liter-
with ‘hui’) is feminine as perceived by a native ally transliterated the Hindi spelling into Roman
speaker. Butasobservedinthequestion,feminine spelling because colloquially the ‘n’ sound is of-
form ‘hui’ is used with ‘Earth’ which is mixed ten omitted while speaking. A similar obser-
wordfromEnglish. Sebba(2009)referstothisas vation applies to the word ‘kartein’ (Meaning:
one type of ’harmonization strategy’ in language do). Similarly, ‘pe’ is a more colloquial usage
mixing and it is one that he says might be typi- of the word ‘par’ (Meaning: on). Though ‘pe’
cal of highly literate bilinguals. We believe the is never used in standard written Hindi, the data
naturalnessofthedataishighlydependentonthe collected has both variants of the words. Sim-
nativity of annotators. Throughout our process, ilar observations in Tenglish data include vari-
we took as much care to ensure that we use na- ations for ‘cheyinchaadu’ and ‘ceyincadu’ (both
tive speakers of the language for our annotation. thewordsmean‘did’). Thisproblemcompounds
However, there were still a few exceptions. We in Tenglish since Telugu is an agglutinative lan-
guage. For example, in the variants ‘chesthu un- organizers, a couple of teams have successfully
aadu’ and ‘chesthunnadu’ (meaning: have been completed participating in the challenge. In this
doing (masculine form)), the two words can be section, we discuss the techniques used by two
writtentogetherasasinglewordorseparately. participatinggroupstoaddressthefirsteditionof
Some of the examples show forcible mixing thischallenge.
since the instructions specifically mentioned to As discussed in Section 3, there are 2 cate-
providecode-mixedquestions. Forexample,‘An- gories of questions; (1) general questions where
droid ko Google ne kab buy kiya tha?’ (Mean- thereisnocontext,and(2)articlebasedquestions
ing: ‘When did Google buy Android?’). The for which the answers are retrieved from a given
word‘khareed’whichmeans‘buy’isaverycom- context. To address the latter type, paragraphs
mon Hindi word and in such cases, the native from Wikipedia are leveraged as general context.
word is used more naturally as opposed to the Oneteam(fromDeutscheForschungszentrumfu¨r
mixed word. 10 such examples were selected Ku¨nstliche Intelligenz (DFKI)) addressed this by
from Hinglish and shown to 5 native speakers of identifying the named entities in the CM query
Hindi to annotate if they seem natural, unnatural and look them up in the summaries of Wikipedia
orneutral. Alltheseexamplesweremarkedasei- articles 2. These summaries typically contain
ther unnatural (36%) or neutral (64%) and none 5 sentences. The second team (from IIIT Hy-
of them were marked as natural. This shows that derabad) trained a similarity model using DSSM
there is some pattern or notion of mixing words (Huang et al., 2013) to retrieve and rank the an-
fornativespeakers. swerbearingsentencesfromWikipedia. Boththe
Insomeotherexamples,wealsoobservedwhat groupshaveworkedalongsimilarlinestoaddress
can be considered an opposite of forced mix- questionswithgeneralcontext.
ing. For example, in question ‘1994 mein pre- The team from DFKI dealt with article based
mier kiya hua pramukh American comedy TV questions as well. At this stage, both the cat-
saathiyon ka naam kya hai?’ (Meaning: What egories of questions contain query and informa-
is the name of the famous American comedy tionaboutrelevantparagraph. Apre-trainedDoc-
TV show Friends that was premiered in 1994?), ument Reader model DrQA proposed by Chen
words like ‘pramukh’ (Meaning: famous) and et al. (2017) on a popular machine reading QA
‘saathiyon’ (Meaning: friends) are less common dataset SQuAD (Rajpurkar et al., 2016) is used
incommonusagecomparedtotheirEnglishmix- for this domain. This model answers open do-
ing counterparts. Also, note that this is uncom- main factoid questions using Wikipedia by not
monsincethenamedentity‘Friends’istranslated considering document retrieval. An open source
to the Hindi counterpart. Another known phe- implementationofthismodel3 isusedandourre-
nomenon is the mixing of languages at morpho- sults are lower than we expected: average EM
logicallevelwhichwasobservedverycommonly is 0.0691 and average F1 is 0.1001 on the train-
in the data. This poses a problem for word level ing dataset. In the category of general ques-
modelingorformulationforaddressingthedown tions(imagebased)wheretherelevantparagraph
stream tasks such as our current case of question is not given, the predicted answer is similar in
answering. For example, in the Tenglish ques- meaning to the ground truth but can be broader.
tion ‘Eiffel Tower ni entha mandi architectlu de- For instance, when the Hinglish answering ‘em-
sign chesaru?’ (Meaning: How many architects inem ka profession kya hai?’ (Meaning: What
designed Eiffel Tower?), the word ‘architectlu’ isEminem’sprofession?),thissystemgives‘rap-
(Meaning: architects) is mixed at morphological per, record producer, and actor’, as compared to
levelbyEnglishword‘architect’andTelugusuf- ‘Rapper’. Though the system is correct, the an-
fix‘lu’,whichisapluralmarker. swergatheredincludedonly‘rapper’whichmost
data collection techniques for QA face an issue.
5 CMQAChallenge: Techniques
To train these models, Hindi embedding space
is mapped into the English one. A standard ap-
TheCMQAchallengewasannouncedandbroad-
proach in relation to Hindi was investigated by
casted during the summer of 2017. The task is
(Bhattacharya et al., 2016) involving finding a
to provide a ranked list of relevant answers for
translation matrix (using linear regression) that
given CM queries. The image based questions
minimizesthereconstructionerrorbetweentarget
are annotated as ‘general’ and the article based
languageembeddingsandtranslatedembeddings.
questionsareprovidedwiththecorrespondingar-
ticles. While 7 teams have registered to take part 2https://pypi.org/project/wikipedia
inthechallengeandhavecollecteddatafromthe 3
https://github.com/facebookresearch/DrQA
Thisideaisdevelopedbyusinganeuralnetwork model was to output ‘Champ de Mars in Paris,
andarandomforestregressiontofindtranslation France’ when asked ‘Eiffel Tower kahan hai?’
matrix. ByusingPolyglotHindiandEnglishem- (Where is the Eiffel Tower?), while the ground
beddings with Universal Word-Hindi Dictionary truthanswerwas‘france’. Errorslikethataccount
weachieveMSEscoreof0.057. forapproximately7%ofallthewrongpredictions
in the development set. Such cases suggest that
6 ChallengesobservedinCMQA considerable attention must be paid during label-
ing of a corpus. One can either keep a list of ac-
Gathering more data: The training subset con-
ceptableanswersorproviderefinedguidelinesfor
tains 1295 unique question-answer pairs, which
both annotators and developers. In the latter, it
poses a significant challenge to train complex
mighthelptoanalyzehumanperformanceonthe
models from scratch. As an alternative, a trans-
samedatasettounderstandwhatisthemostcom-
ferlearningtechniquecanbeused,usingamodel
monanswergranularitylevel.
pre-trained on a large-scale open-domain fac-
Cross-lingualembeddings: Finally,whenwork-
toid dataset, such as SQuAD. For instance, com-
ing with neural models, we have to carefully
munity question answering forums can be used,
approach the construction of embedding spaces.
whichnaturallycontainsalotofcode-mixedlan-
While in the current version we have worked
guageduetotheextensiveborrowingoftechnical
only with English translations, a neater approach
terms. Such setup has two benefits: it eases the
would be to directly use both languages. (Ruder,
problemofcollectingnewdataandalleviatesthe
2017) provides an extensive survey of the avail-
needtomanuallylabelit.
able approaches. Whereas more and more re-
SpellChecking: Sincethequestion-answerpairs
sources are emerging for Hindi, such as MUSE
are coming from an informal background, some
(Conneau et al., 2017), few researchers have ad-
of them are misspelt. Language identification is
dressedthetaskforTeluguandTamil.
an overhead to deal with this using traditional
spell checking techniques. An extensive use of
7 Conclusions
dictionaries is the most obvious approach, but a
morepracticalsolutionmightbetousecharacter- In this paper, as a first step towards fostering
based methods and introduce artificial noise to research in the area of Factoid QA in CM lan-
makemodelsmorerobust. guages,wepresentourevaluationdatasetconsist-
Romanization variability: It should also be ing of more than 5000 crowd-sourced questions
notedthatapartfromspellchecking,thereisvari- alongwiththeiranswersinthreeCMlanguages-
ability in romanization output. For example, the Hinglish,TenglishandTamlish. Wereceivedalot
Hindi word ‘jidhar’(Meaning: where) can either more Tamlish questions on crowd sourced plat-
be written as ‘jidhr’. As it is unclear which of form compared to the other two languages. We
the models of transliteration would a user prefer, also shared our experiences while curating this
adeveloperneedstokeepalloptionsopen. evaluationdatasetsuchasusageofimagesasan-
Poor translation from open source tools: In chor points to avoid lexical biasing towards CM.
many cases, translation tools completely distort Wehavelookedattheextentoflexicalbiasingof
the meaning of the sentence. An illustrative ex- thewordsinarticlebasedquestions. Infuture,we
ample of this is: ‘Sun ka colour kya hai?’ (What wouldalsoliketoseeiftheparticipantsareinvert-
is the colour of the Sun?) - ‘What is the color of ing the language for the words present in the ar-
listening?’ and so on. As one can see, an En- ticles. Thedatasetfeaturesadiverserangeofan-
glishcollocation‘fullname’isnotpreserved,but swertypes acrossall theCMlanguages. Wealso
translatedinto‘Fullham’. Insomecasesitcanbe shared some interesting properties of this dataset
explained by the incorrect use of capitalization: related to lexical bias and other phenomenon re-
‘niagarafallskaunsedeshmeinhai?’ istranslated lated to code mixing. In future, we would like
into ‘What is the name of the person who is suf- to explore techniques to generate synthetic CM
fering from diabetes?’, but using capitalized ‘N’ data from large-scale datasets.We plan on con-
gives correct translation. It is worth noting that tinuing the data collection process to elicit more
incorrectquerytranslationcontributedtoapprox- data. Thispaperalsoreportsthefirsteditionofthe
imately35%oferrors. challengeandplanoncontinuingitinthecoming
Answer granularity: Moreover, while perform- years as well. We have made our dataset freely
ing error analysis, we have found a few cases availableforresearchpurposestoencouragemore
where a level of required granularity for an an- researchworkandresultinsignificantadvancesin
swerwasunclear. Acommontypeoferrorforthe theareaofFactoidQAinCMlanguages.
References DavidCrystal.1997. TheCambridgeencyclopediaof
language, volume 1. Cambridge University Press
Kalika Bali, Jatin Sharma, Monojit Choudhury, and
Cambridge.
Yogarshi Vyas. 2014. ” i am borrowing ya mix-
ing?” an analysis of english-hindi code mixing in Long Duong, Hadi Afshar, Dominique Estival, Glen
facebook. In Proceedings of the First Workshop Pink,PhilipCohen,andMarkJohnson.2017. Mul-
on Computational Approaches to Code Switching, tilingual semantic parsing and code-switching. In
pages116–126. Proceedings of the 21st Conference on Computa-
tional Natural Language Learning (CoNLL 2017),
SomnathBanerjee,SudipKumarNaskar,PaoloRosso,
pages379–389.
and Sivaji Bandyopadhyay. 2016. The first cross-
script code-mixed question answering corpus. In PGoyal,ManavRMital,AMukerjee,AchlaMRaina,
MultiLingMine@ECIR,pages56–65. DSharma,PShukla,andKVikram.2003. Abilin-
gual parser for hindi, english and code-switching
Utsab Barman, Amitava Das, Joachim Wagner, and
structures. In 10th Conference of The European
Jennifer Foster. 2014. Code mixing: A challenge
Chapter,page15.
for language identification in the language of so-
cialmedia. InProceedingsofthefirstworkshopon Gualberto Guzma´n, Joseph Ricard, Jacqueline Seri-
computationalapproachestocodeswitching,pages gos, Barbara E Bullock, and Almeida Jacque-
13–23. line Toribio. 2017. Metrics for modeling code-
switching across corpora. Proc. Interspeech 2017,
Paheli Bhattacharya, Pawan Goyal, and Sudeshna
pages67–71.
Sarkar. 2016. Using word embeddings for query
translation for hindi to english cross language in- Taofik Hidayat. 2008. An analysis of code switching
formation retrieval. Computacio´n y Sistemas, usedbyfacebookers.
20(3):435–447.
Po-SenHuang,XiaodongHe,JianfengGao,LiDeng,
Alan W Black, Susanne Burger, Alistair Conkie, He- AlexAcero,andLarryHeck.2013. Learningdeep
len Hastie, Simon Keizer, Oliver Lemon, Nico- structured semantic models for web search using
las Merigaud, Gabriel Parent, Gabriel Schubiner, clickthroughdata. InProceedingsofthe22ndACM
Blaise Thomson, et al. 2011. Spoken dialog chal- international conference on Conference on infor-
lenge2010: Comparisonofliveandcontroltestre- mation & knowledge management, pages 2333–
sults. InProceedingsoftheSIGDIAL2011Confer- 2338.ACM.
ence,pages2–7.ACL.
Anupam Jamatia, Bjo¨rn Gamba¨ck, and Amitava Das.
Eric Brill, Susan Dumais, and Michele Banko. 2002. 2015. Part-of-speech tagging for code-mixed
Ananalysisoftheaskmsrquestion-answeringsys- english-hindi twitter and facebook chat messages.
tem. In Proceedings of the ACL-02 conference on InProceedingsoftheInternationalConferenceRe-
Empiricalmethodsinnaturallanguageprocessing- cent Advances in Natural Language Processing,
Volume 10, pages 257–264. Association for Com- pages239–248.
putationalLinguistics.
AravindKJoshi.1982. Processingofsentenceswith
Khyathi Raghavi Chandu, Manoj Chinnakotla, intra-sentential code-switching. In Proceedings of
Alan W Black, and Manish Shrivastava. 2017. the 9th conference on Computational linguistics-
Webshodh: Acodemixedfactoidquestionanswer- Volume1,pages145–150.AcademiaPraha.
ing system for web. In International Conference
of the Cross-Language Evaluation Forum for Praveen Kumar, Shrikant Kashyap, Ankush Mittal,
EuropeanLanguages,pages104–111.Springer. and Sumit Gupta. 2005. A hindi question answer-
ingsystemfore-learningdocuments. InIntelligent
Danqi Chen, Adam Fisch, Jason Weston, and An- SensingandInformationProcessing, 2005.ICISIP
toine Bordes. 2017. Reading wikipedia to an- 2005.ThirdInternationalConferenceon,pages80–
swer open-domain questions. arXiv preprint 85.IEEE.
arXiv:1704.00051.
Xin Li and Dan Roth. 2006. Learning question clas-
Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and sifiers: the role of semantic information. Natural
Monojit Choudhury. 2014. Word-level language LanguageEngineering,12(3):229–249.
identificationusingcrf:Code-switchingsharedtask
report of msr india system. In Proceedings of The Lesley Milroy and Pieter Muysken. 1995. One
First Workshop on Computational Approaches to speaker, two languages: Cross-disciplinary per-
CodeSwitching,pages73–79. spectivesoncode-switching. CambridgeUniversity
Press.
Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato,LudovicDenoyer,andHerve´Je´gou.2017. Pieter Muysken. 2000. Bilingual speech: A typology
Word translation without parallel data. arXiv ofcode-mixing, volume11. CambridgeUniversity
preprintarXiv:1710.04087. Press.
Carol Myers-Scotton. 1997. Duelling languages: Royal Sequiera, Monojit Choudhury, Parth Gupta,
Grammatical structure in codeswitching. Oxford Paolo Rosso, Shubham Kumar, Somnath Baner-
UniversityPress. jee, Sudip Kumar Naskar, Sivaji Bandyopadhyay,
Gokul Chittaranjan, Amitava Das, et al. 2015.
GarimaNanda,MohitDua,andKrishmaSingla.2016. Overview of fire-2015 shared task on mixed script
A hindi question answering system using machine informationretrieval. InFIREWorkshops, volume
learning approach. In ICCTICT 2016, pages 311– 1587,pages19–25.
314.IEEE.
Arnav Sharma, Sakshi Gupta, Raveesh Motlani,
Gu¨nter Neumann and Bogdan Sacaleanu. 2003. Piyush Bansal, Manish Srivastava, Radhika
A cross–language question/answering–system for Mamidi,andDiptiMSharma.2016. Shallowpars-
german and english. In Workshop of the Cross- ing pipeline for hindi-english code-mixed social
Language Evaluation Forum for European Lan- mediatext. arXivpreprintarXiv:1604.03136.
guages,pages559–571.Springer.
R Mahesh K Sinha and Anil Thakur. 2005. Machine
translation of bi-lingual hindi-english (hinglish)
Niko Partanen, KyungTae Lim, Michael Rießler, and
text. 10th Machine Translation summit (MT Sum-
Thierry Poibeau. 2018. Dependency parsing of
mitX),Phuket,Thailand,pages149–156.
code-switchingdatawithcross-lingualfeaturerep-
resentations. In Proceedings of the Fourth Inter-
Thamar Solorio, Elizabeth Blair, Suraj Mahar-
national Workshop on Computatinal Linguistics of
jan, Steven Bethard, Mona Diab, Mahmoud
UralicLanguages,pages1–17.
Ghoneim, Abdelati Hawwari, Fahad AlGhamdi,
Julia Hirschberg, Alison Chang, et al. 2014.
Shana Poplack. 1980. Sometimes ill start a sentence
Overviewforthefirstsharedtaskonlanguageiden-
inspanishyterminoenespanol: towardatypology
tificationincode-switcheddata. InProceedingsof
ofcode-switching1. Linguistics,18(7-8):581–618.
the First Workshop on Computational Approaches
toCodeSwitching,pages62–72.
KhyathiChanduRaghavi,ManojKumarChinnakotla,
andManishShrivastava.2015. Answerkatypekya
Thamar Solorio and Yang Liu. 2008. Part-of-speech
he?: Learning to classify questions in code-mixed
tagging for english-spanish code-switched text. In
language. In Proceedings of WWW 2015, pages
ProceedingsoftheConferenceonEmpiricalMeth-
853–858.ACM.
ods in Natural Language Processing, pages 1051–
1060.AssociationforComputationalLinguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
andPercyLiang.2016. Squad: 100,000+questions Victor Soto and Julia Hirschberg. 2017. Crowd-
for machine comprehension of text. In Proceed- sourcing universal part-of-speech tags for code-
ingsofthe2016ConferenceonEmpiricalMethods switching. arXivpreprintarXiv:1703.08537.
inNaturalLanguageProcessing,pages2383–2392,
Austin, Texas.AssociationforComputationalLin- Ang Sun, Minghu Jiang, Yifan He, Lin Chen, and
guistics. BaozongYuan.2008. Chinesequestionanswering
basedonsyntaxanalysisandanswerclassification.
RamiReddyNandiReddyandSivajiBandyopadhyay. ActaElectronicaSinica,36(5):833–839.
2006. Dialogue based question answering system
intelugu. InProceedingsoftheWorkshoponMul- YogarshiVyas,SpandanaGella,JatinSharma,Kalika
tilingual Question Answering, pages 53–60. Asso- Bali,andMonojitChoudhury.2014. Postaggingof
ciationforComputationalLinguistics. english-hindi code-mixed social media content. In
ProceedingsoftheEMNLP2014,pages974–979.
SebastianRuder.2017. Asurveyofcross-lingualem-
ZHANG Yongkui, ZHAO Zheqian, BAI Lijun, and
beddingmodels. arXivpreprintarXiv:1706.04902.
CHEN Xinqing. 2003. Internet-based chinese
question-answering system. Computer Engineer-
Koustav Rudra, Shruti Rijhwani, Rafiya Begum, Ka-
ing,15:34.
lika Bali, Monojit Choudhury, and Niloy Ganguly.
2016. Understanding language preference for ex-
WQuinYowandFernindaPatrycia.2011. Challeng-
pressionofopinionandsentiment: Whatdohindi-
ing the linguistic incompetency hypothesis: Lan-
english speakers do on twitter? In Proceedings of
guagecompetencypredictscode-switching.
EMNLP2016,pages1131–1141.
Ayah Zirikly and Mona Diab. 2015. Named entity
MarkSebba.2009. Onthenotionsofcongruenceand recognitionforarabicsocialmedia. InProceedings
convergenceincode-switching. CambridgeUniver- of the 1st Workshop on Vector Space Modeling for
sityPress. NaturalLanguageProcessing,pages176–185.
Satoshi Sekine and Ralph Grishman. 2003. Hindi-
english cross-lingual question-answering system.
ACMTransactionsonAsianLanguageInformation
Processing(TALIP),2(3):181–192.

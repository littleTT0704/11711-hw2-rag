Downstream Datasets Make Surprisingly Good Pretraining Corpora
KundanKrishna,SaurabhGarg,JeffreyP.Bigham,ZacharyC.Lipton
CarnegieMellonUniversity
{kundank,sgarg2,jbigham,zlipton}@andrew.cmu.edu
Abstract aresubsequentlytrained(finetuned)onthelabeled
downstream data available for the task at hand.
Formostnaturallanguageprocessingtasks,the
Large-scale pretrained models typically provide
dominantpracticeistofinetunelargepretrained
significantperformanceboostswhencomparedto
transformermodels(e.g.,BERT)usingsmaller
models trained directly on the downstream task
downstream datasets. Despite the success of
(with random initializations) (Peters et al., 2018;
this approach, it remains unclear to what ex-
tentthesegainsareattributabletothemassive Devlin et al., 2019; Chiang and Lee, 2020; Kr-
backgroundcorporaemployedforpretraining ishna et al., 2021). Upstream corpora tend to be
versustothepretrainingobjectivesthemselves. significantly larger than the downstream corpora
This paper introduces a large-scale study of
andthesuccessofthisapproachisoftenattributed
self-pretraining,wherethesame(downstream)
to its ability to leverage these massive upstream
training data is used for both pretraining and
corpora (Liu et al., 2019; Yang et al., 2019). For
finetuning. In experiments addressing both
example,theseminalBERTmodel(Devlinetal.,
ELECTRAandRoBERTamodelsand10dis-
tinctdownstreamclassificationdatasets,weob- 2019)waspretrainedusingtheBookWikicorpus
servethatself-pretrainingrivalsstandardpre- whichisacombinationofEnglishWikipediaand
trainingontheBookWikicorpus(despiteusing BooksCorpus(Zhuetal.,2015),totaling13GBof
around 10×–500× less data), outperforming plain text. Subsequent models have moved on to
thelatteron7and5datasets,respectively. Sur- web-scaledata. Forexample,XLNet(Yangetal.,
prisingly,thesetask-specificpretrainedmodels
2019),RoBERTa(Liuetal.,2019),andT5(Raffel
oftenperformwellonothertasks,includingthe
etal.,2020)),weretrainedon158GB,160GBand
GLUEbenchmark. Self-pretrainingalsopro-
750GBofdata,respectively.
videsbenefitsonstructuredoutputprediction
taskssuchasquestionansweringandcommon-
As upstream corpus size and downstream per-
senseinference,oftenprovidingmorethan50%
formance have gone up, popular attempts at ex-
improvementscomparedtostandardpretrain-
plaining these gains have focused on themes of
ing. Our results hint that often performance
“knowledgetransfer"fromtheupstreamcorpus,at-
gainsattributabletopretrainingaredrivenpri-
marily by the pretraining objective itself and tributingthemtosharedlinguisticstructure,seman-
arenotalwaysattributabletotheuseofexter- tics (Lina et al., 2019; Tenney et al., 2019), and
nalpretrainingdatainmassiveamounts. These factsabouttheworld(Petronietal.,2019). How-
findingsareespeciallyrelevantinlightofcon-
ever,sincetheintroductionoflarge-scalepretrain-
cernsaboutintellectualpropertyandoffensive
ingcorporaoccurredtogetherwiththeinventionof
contentinweb-scalepretrainingdata.1
self-supervisedpretrainingobjectives(e.g. masked
1 Introduction language modeling (Devlin et al., 2019) and re-
placed token detection (Clark et al., 2019)), it re-
Fortrainingpredictivemodelsoperatingonnatural mains unclear to what extent large-scale corpora
language data, the current best practice is to pre- areintegraltotheseleapsinperformance. Forsev-
trainmodelsonlargeunlabeledupstreamcorpora eraltasks,especiallysummarization,recentworks
to optimize self-supervised objectives, for exam- achievedsurprisingperformancegainsinsettings
ple,maskedlanguagemodeling(MLM);theresult- wheretheupstreamcorpusiscreatedsynthetically
ingweightsarethenusedtoinitializemodelsthat with arbitrary symbols, but the pretraining objec-
tiveisdesignedtocapturesomeofthestructureof
1Pretrained models can be downloaded from https://
github.com/acmi-lab/self-pretrain thetask(Krishnaetal.,2021;Wuetal.,2022).
1
3202
yaM
62
]LC.sc[
2v98341.9022:viXra
Figure1: AggregateperformanceofanELECTRAmodelacross10finetuningdatasetswhenitis(i)randomly
initialized(ii)pretrainedonupstreamcorpus(BookWiki)(iii)pretrainedonthefinetuningdatasetitself
Inthiswork,weaskjusthowmuchofpretrain- latederrorswhencomparedtotwoindependently
ing’s benefits could be realized in the absence of finetunedmodelspretrainedwitheitherstrategy.
upstream corpora by pretraining directly on the We find that models pretrained on one down-
downstreamcorpora(withthesameself-supervised stream dataset often perform surprisingly well
objectives). Wefindthatthisapproach,whichwe when finetuned to other downstream datasets
callself-pretraining,oftenrivalstheperformance (Sec. 5), including the GLUE benchmark. Even
boostsconferredbyoff-the-shelf modelspretrained thoughthedownstreamdatasetsinourstudycome
on large upstream corpora (Figure 1), even out- fromawidevarietyofdomains(e.g.,news,online
performing them on 7 out of 10 datasets. Prior forums, tweets), we find that pretraining on any
researchhasshownthatadditionalself-supervised of these downstream datasets delivers significant
pretrainingofoff-the-shelfmodelsusingthedown- performancegainsonmostdatasets(greaterthan
stream data can give further gains (Gururangan halfofoff-the-shelfmodel’sgainsin88%ofcases)
etal.,2020). Ourstudygoesfurther,showingthat irrespectiveofdomain. However,thebestperfor-
evenwhenstartingfromrandominitializations,and manceonadownstreamdatasetisusuallyachieved
withoutusinganyexternaldatabeyondthedown- bythemodelpretrainedonthatdatasetitself.
stream data itself, self-pretraining can rival stan- In addition to classification tasks, we also ex-
dardpractices. Sinceself-pretrainingrequiresthe periment with tasks such as span-based question
samedatathatmustalreadybeavailableforfinetun- answering,namedentityrecognition,andgrounded
ing,thebenefitsofpretraininginthiscasecannot commonsenseinference(Sec. 8). Self-pretraining
be attributed to transfer of knowledge from the deliversaround40-80%oftheperformanceboost
upstreamcorpus. Instead,thesebenefitscanonly comparedtomodelspretrainedontheBookWiki
beattributedtothepretrainingobjective,whichis corpus across ELECTRA and RoBERTa models.
possiblyabletolearnsomeinductivebiasesbetter Hence,self-pretrainingcanperformbetterthanfine-
thanthefinetuningobjective(e.g. linguisticknowl- tuningrandomlyinitializedmodelsevenfortasks
edge(Tenneyetal.,2019)),orperhapssimplyini- thatrequirepredictionofmorecomplexstructured
tializesnetworkparameterssuchthattheirstatistics outputthanasinglelabel,andfortaskswhoseso-
leadtobetteroptimizationduringfinetuning(Wu lutionreliesoncommonsenseknowledge.
etal.,2022). Whilesimilarobservationsweremade Ourcontributionscanbesummarizedasfollows:
inthecomputervisioncommunity(El-Noubyetal.,
• Comparison of self-pretrained and off-the-
2021), we argue that it is especially important to
shelfpretrainedmodels(bothwithELECTRA
establishthesephenomenainthelanguagedomain,
andRoBERTaarchitectures)across10down-
for which building on self-supervised pretrained
streamclassificationtasks.
modelsisnowaubiquitouspractice.
• Analysisofout-of-distributionperformanceof
To understand differences in predictions with modelspretrainedononedownstreamdataset
differentpretrainingstrategies(i.e.,betweenself- andfinetunedonotherdownstreamdatasets,
pretrained and off-the-shelf models), we analyse includingtheGLUEbenchmark.
theerrorsmadebythesemodelsonthesamedown- • Demonstration of self-pretraining’s efficacy
streamdata(Sec.6). Despitesimilarperformance on more complex tasks than classification
of these models, we find that self-pretrained and suchastasksrequiringstructuredoutputpre-
off-the-shelfmodelsmakesignificantlylesscorre- dictionorcommonsensereasoning.
2
2 RelatedWork extentthatsuchknowledgeextractionplaysarole
in pretraining’s benefits, sufficient knowledge is
Self-pretraininginComputerVision Mostrele-
oftenpresentinthedownstreamdatasetandneed
vanttoourwork,recent/concurrentworksincom-
notbetransferred fromhugeupstreamcorpora.
puter vision explore self-pretraining (He et al.,
Challenges to the Knowledge Transfer Narra-
2022; El-Nouby et al., 2021). In a contemporary
tive Multiple previous works have questioned
work,Heetal.(2022)showedthatpretrainingwith
whetherknowledgetransfercanfullyaccountfor
aMaskedAutoEncoder(MAE)objective(analogue
the efficacy of pretraining. Improvements in per-
of MLM objective for images) boosts the perfor-
formanceondownstreamNLPtaskshaveresulted
manceofViTmodelsontheImagenet-1Kdataset.
from pretraining on other modalities like music
El-Nouby et al. (2021) showed that pretraining
and code (Papadimitriou and Jurafsky, 2020), se-
solelyondownstreamdatasetsforobjectdetection
quencesofmeaninglesssymbols (ChiangandLee,
andsegmentationtasksreachestheperformanceof
2020; Krishna et al., 2021; Wu et al., 2022), and
Imagenet-pretrainedmodels. Ourworkestablishes
languagedenaturedviashufflingofwords(Sinha
that a similar phenomenon is observed for NLP
etal.,2021). Ontheotherhand,modelspretrained
taskstooacrossawiderangeofdatasets.
onlanguagehaveshownimprovedperformanceon
PretrainingonDownstreamDatainNLP Task-
tasksdealingwithothermodalitiessuchasimage
Adaptive PreTraining (TAPT (Gururangan et al.,
classification (Lu et al., 2021) and reinforcement
2020)) consists of taking off-the-shelf pretrained
learningforgames(Reidetal.,2022). Bycontrast,
models like BERT and RoBERTa and engaging
weshowthatwithoutsurplusupstreamdataofany
infurtherpretrainingonthedownstreamdatasets
modality,self-pretrainingalonecanoftenperform
beforefinetuningthemtothetaskathand. TAPT
comparablyorevenbetterthanstandardpretraining
has been shown to improve performance of off-
withalargeupstreamcorpus. Inasimilarveinwith
the-shelfmodelsinavarietyofworks(Logeswaran
thesepapers,ourworksuggeststhatalargeportion
etal.,2019;HanandEisenstein,2019;Chakrabarty
of pretraining’s success may come from alterna-
et al., 2019). Another way in which downstream
tive,unexploredmechanismswhichhavemoreto
datahasbeenusedisforretrievaltocreateasmall
dowiththepretrainingobjectivethanknowledge
pretraining corpus for efficient pretraining (Yao
transferfromupstreamcorpora.
etal.,2022). Bycontrast,ourworkpretrainsmod-
els only on the downstream dataset, enabling a 3 ExperimentalSetup
head-to-headcomparisonbetweentheperformance
ofoff-the-shelfandself-pretrainedmodels,and(in Our experiments center around the ELECTRA
some situations) challenging the necessity of up- model(Clarketal.,2019)andtheRoBERTa-base
streamcorporaaltogether. model (Liu et al., 2019). On the broadest set of
ClaimsaboutKnowledgetransfer Manyworks experiments,forwhichwecanonlyaffordtotrain
claim that pretraining extracts generally useful onemodel,weemployELECTRAbecauseitper-
knowledge from the upstream corpus such as lin- formsbetterthanRoBERTagivencomparablecom-
guistic patterns (Lina et al., 2019; Tenney et al., putebudgets(Clarketal.,2019). Inparticular,we
2019;Manningetal.,2020)andfacts(Petronietal., use the small variant of ELECTRA (14 million
2019),andthatthisaccountsfortheperformance parameters), which performs similarly to BERT-
gains that they enjoy on downstream tasks. Sev- base on GLUE (difference of ≈2 points) while
eralworks,e.g.,intheprobingliterature(Tenney trainingmuchfaster(Clarketal.,2019). However,
et al., 2019; Manning et al., 2020; Petroni et al., we replicate many of these results on the larger
2019),demonstratethatfromtheinternalrepresen- RoBERTa-basemodelrevealingsimilarresultsand
tationsofamodel,itiseasy(e.g.,vialinearmodels) thusestablishingthegeneralityofourfindings.
topredictcertainlinguisticfeaturesorreal-world Duringpretraining,atextsequenceisfedintothe
facts. However, these studies do not clarify the modelwithsometokensmaskedout. WhileMLM-
mechanismbywhichtheseobservationsrelateto onlymodelslikeRoBERTaonlyhaveagenerator
performance gains on downstream tasks. Tenney network that predicts the content of the masked
etal.(2019)recognizesthislimitation,stating“the tokens, ELECTRA has an additional discrimina-
observation of a (linguistic) pattern does not tell tormodulethatpredictsifthosepredictionswere
ushowitisused”. Ourworksuggeststhattothe correct. Boththegeneratorandthediscriminator
3
Dataset Size(MB) Classes Domain Task
AGNews(Zhangetal.,2015) 27 4 News topicclassification
QQP(Wangetal.,2018) 43 2 Onlineforumquestions paraphrasedetection
JigsawToxicity(Kaggle.com,2018) 59 6 Wikipediacomments toxicitydetection
MNLI(Williamsetal.,2018) 65 3 Diverse naturallanguageinference
Sentiment140(Goetal.,2009) 114 5 Tweets sentimentclassification
PAWS(Zhangetal.,2019) 139 2 Wikipedia paraphrasedetection
DBPedia14(Zhangetal.,2015) 151 14 Wikipedia topicclassification
Discovery(Sileoetal.,2019) 293 174 Webcrawl discoursemarkerprediction
YahooAnswertopics(Zhangetal.,2015) 461 10 Onlineforumanswers topicclassification
AmazonPolarity(Zhangetal.,2015) 1427 2 Productreviews sentimentclassification
Table1: Thesuiteofdownstreamdatasetsusedinthisworkalongwiththeirtrainingsetsizes
networks’parametersareupdatedsimultaneously multi-classclassificationproblems).
during pretraining. After pretraining, the genera- Notably, all self-pretrained models deliver sig-
torisdiscardedandthediscriminatorisusedasan nificant performance boosts on their respective
encoderforfinetuningondownstreamtasks. datasets(Table2),andoverhalfofthemperform
Weexperimentedwith10differentdownstream evenbetterthantheoff-the-shelfmodel. Wemea-
datasets(Table1). Wechosethesedatasetsinour sured a model’s benefit as the increase in perfor-
testbedtospandifferentdatasetsizesrangingfrom mancemetricthatitachievesoverarandomlyini-
27megabytestoabout1.4gigabytesoftextinthe tialized model, divided by the increase in perfor-
trainingsplit. Thesedatasetsarefordifferenttasks mancemetricachievedbytheoff-the-shelfELEC-
suchastopicclassification,sentimentclassification, TRA model against the same baseline. The aver-
naturallanguageinferenceetc.,andarecreatedus- age benefit of self-pretraining across all datasets
ing data sourced from diverse domains. Most of is103.70%. Wedonotseeaclearcorrelationbe-
themaremulti-classclassificationtasksexceptJig- tweenthesizeofthedatasetandtheperformance
sawToxicitywhichisamulti-labelclassification ofself-pretraining. Forexample,thehighestbene-
task,andSentiment140whichismodeledasare- fitof131.33%isachievedforthesmallestdataset
gressiontask. Forfinetuningapretrainedmodelon (AGNews),whichismerely27MBinsize,while
anydataset,wepassedtheinputthroughthemodel, theminimumbenefitisachievedontheDiscovery
tookthevectorrepresentationoftheCLStokenin dataset, which is the third largest dataset measur-
thefinallayer,andpasseditthroughaclassification ing293MB.Foreachdownstreamdataset,wealso
headwithonehiddenlayertogettheoutput. pretrainamodelonarandomlysampledsubsetof
Wikipedia of the same size as the dataset’s train-
4 Self-pretrainingPerformance
ingcorpus,andfinetuneitonthedownstreamtask.
In our first set of experiments, we compare self- This approach (called WikiSub) provides a size-
pretraining’s performance with other pretraining adjusted comparision between using separate up-
techniques. Foreachdataset,wepretrainanELEC- streamdatavsthedownstreamdataforpretraining.
TRAmodelontextfromitstrainingsplitandthen We see that self-pretraining performs better than
finetuneitonthesametrainingdatausingtheasso- WikiSubinthemajorityofcases(Table2).
ciatedlabels. Tocreateapretrainingcorpusfroma Wealsoevaluatedthealternatepretrainingtech-
downstreamdataset,weconcatenatetheinputtext nique TAPT as described in Gururangan et al.
fromeachoftheexamples,assemblingtheminran- (2020). Inthistechnique,wetaketheoff-the-shelf
dom order. We evaluate the performance of each ELECTRA model, which has already been pre-
finetuned model on the corresponding dataset’s trainedontheupstreamBookWikicorpus,andfur-
test split. For QQP and MNLI we just use the therpretrainitonthedownstreamdatasetfor100
validation split because test set labels are private. epochs. Self-pretraining outperforms TAPT on 6
Foralldatasets,weevaluateperformancebyaccu- datasets,notablyincludingthetwodatasetswhere
racy,exceptforSentiment140andJigsawToxicity, it outperformed the off-the-shelf models by the
for which we use Pearson correlation and micro- greatest benefit margin - AGNews and Yahoo An-
averagedAUCscores,respectively(thesearenot swertopics. Interestingly, TAPT performs worse
4
than off-the-shelf model on the same 3 datasets QQPandPAWSreceiveshugeboostsbypretrain-
whereself-pretrainingperformsworsethanoff-the- ingonmostdatasets. Incontrast,performanceon
shelf model (except Sentiment140). None of the sentiment140ismostlylow,evendroppingbelow
threepretrainingapproachesseemtobeuniformly 20%for3pretrainedmodels.
betterthananyother. Weperformanablationtoinvestigatethatgiven
Finally,wealsoevaluatetheself-pretrainedmod- afixeddatasettofinetuneon,isitbettertopretrain
elsontheGLUEbenchmarkandreportresultson ontheexactsamedata(i.e.,usingthesamesetof
the dev set 2. The performance of the models on inputs),orisitbettertopretrainondifferentdata
theirpretrainingdatasetdoesnotcorrelatestrongly withanidenticaldistribution. Totestthishypothe-
withitsGLUEscore. TheGLUEscorealsodoes sis,wedividedthetrainingsplitsofthedownstream
not monotonically go up with increasing dataset datasetsrandomlyintotwoequalsubsets(denoted
size,indicatingthatthedatadomainmakessome as A and B). We pretrained one model on each
difference. Forexample,theAmazonPolaritycor- subset and then finetuned them on both subsets
pusscoresjust66.14onGLUEdespitebeingabout separately. The validation and test sets used for
1.4GBinsize, whileAGNewswhichis27MBin finetuningarethesameasintheoriginaldataset.
size, scores 74.30. The highest GLUE score is Wedonotseeanyconsistentbenefitswithpre-
achievedbypretrainingonYahooAnswertopics. training and finetuning on the same dataset (Ta-
ble3). Instead,wefoundconsistentpatternswhere
5 CrossDatasetFinetuning
modelspretrainedononesplit(eitherAorB)out-
performedmodelspretrainedontheother,irrespec-
In this set of experiments, we investigated if the
tiveofthesplitusedforfinetuning. Thissuggests
modelspretrainedonadatasetareonlyusefulfor
that the pretraining data has greater influence on
that specific task, or are they useful across the
thefinalperformancethanthefinetuningdata. Ad-
wholespectrumoftasksthatweconsider. Wetook
ditionally,weobservethatfinetuningthesuperior
eachmodelpretrainedonadatasetinourtestbed
pretrainedmodel,usingthedownstreamsplitother
andfinetunedandevaluateditonallotherdatasets
thantheoneusedforpretraining,performsthebest,
inthetestbed. Theperformancebenefitsprovided
suggestingoverallexposuretomoredatahelps.
inallcasesareshownasaheatmapinFigure2.
We found that for almost all downstream
6 DifferenceinOutputsofSelf-pretrained
datasets,pretrainingonanyotherdatasetprovides
andOff-the-shelfModels
significantadvantage(Figure2). Inmostcases,pre-
trainingonthedownstreamdatasetitselfperforms Sinceself-pretrainedmodelsandoff-the-shelfmod-
the best. Among datasets where self-pretraining elsperformsimilarlyintermsofclassificationac-
performs better than off-the-shelf model (i.e. the curacy,anaturalquestiontoaskis: dothesemodels
diagonal entry is greater than 1), pretraining on makeerrorsonthesamesetofinputs? Toanswer
datasetsoflargersizedoesnothelpfurther. How- thisquestion,weinvestigatethedifferenceinpre-
ever,forthedatasetswhereself-pretraining’sbene- dictionsmadebymodelspretrainedwithdifferent
fitismuchlessthan100%(i.e. MNLIandDiscov- strategiesacrossallmulti-classclassificationtasks.
ery),pretrainingonalargerdataset(e.g.,YahooAn- Inparticular,givenmodelf andf ,wecompute
A B
swertopics)performsbetterthanself-pretraining. errorinconsistency,definedasfollows:
Amongallthepretrainedmodels,afewmodels
performconsistentlygoodorbadacrossdifferent (cid:88)n 1[f A(x i) ̸= y i∧f B(x i) = y i]
downstreamdatasets(Figure2). Forexample,the n
i=1
modelpretrainedonYahooAnswertopicsgetsthe
1[f (x ) = y ∧f (x ) ̸= y ]
highest average score of 0.90 across all datasets, + A i i B i i ,
n
whilethePAWS-pretrainedmodelgivesthelowest
aggregatescoreof0.64. Similarly,therearedown- where {x ,y }n is the test set. Intuitively, er-
i i i=1
stream datasets that are benefited consistently by
rorinconsistencycapturesthefractionofexamples
eitheralargeorasmallmarginbypretrainingon
where exactly one model is correct. This defini-
different datasets. For example, performance on
tionhasbeencommonlyusedtoestimatediversity
in model prediction (Gontijo-Lopes et al., 2022;
2Following Clarketal.(2019)weexcludetheWNLItask
fromtheresults. Geirhos et al., 2020). Across all the multi-class
5
Dataset Size(MB) RandInit SelfPretrain Offshelf Benefit% WikiSub TAPT GLUE
AGNews 27 91.75 94.34 93.72 131.33 93.51 94.07 74.30
QQP 43 82.93 90.66 90.34 104.34 89.16 90.64 75.43
JigsawToxicity 59 97.83 98.49 98.53 94.99 98.35 98.48 76.65
MNLI 65 65.49 78.39 82.29 76.77 78.64 79.26 78.28
Sentiment140 114 63.75 67.04 66.95 102.91 65.52 65.65 72.67
PAWS 139 50.00 97.53 97.30 100.49 97.42 97.85 74.65
DBPedia14 151 98.59 99.22 99.11 121.17 99.18 99.23 70.38
Discovery 293 17.00 22.38 24.55 71.22 22.47 23.58 77.26
YahooAnswertopics 461 61.94 65.26 64.55 127.31 64.37 65.05 79.53
AmazonPolarity 1427 93.86 96.27 96.13 106.49 95.82 96.16 66.14
Table2: PerformanceofELECTRA-smallmodelspretrainedwithdifferenttechniquesonvariousdownstream
datasets. WealsoreporttheirperformanceontheGLUEbenchmark(devset). Forreference,arandomlyinitialized
modelscores53.20andtheoff-the-shelfmodelscores79.43onGLUE.
Figure2: Performancebenefitsofmodelspretrainedoneachdataset,uponfinetuningoneachdownstreamdataset.
Eachvalueistheratioofperformancegainsachievedbymodelpretrainedontherow’sdatasetvsoff-the-shelf
model,relativetorandominitialization,uponfinetuningonthecolumn’sdataset.
classification tasks, in addition to computing er- serve that models trained with different pretrain-
rorinconsistencybetweenself-pretrainedandoff- ing datasets have high error inconsistency in pre-
the-shelfmodel,forbaselinecomparison,wealso dictions (Table 4). For models with comparative
tabulateerrorinconsistencybetween: (i)twoinde- performance, high error inconsistency highlights
pendently finetuned versions of a self-pretrained thehighdisagreementinpredictions. Thisdemon-
model; and (ii) two independently finetuned ver- stratesthatwhiledifferentpretrainingdatasetspro-
sionsoftheoff-the-shelfmodel. ducesimilarlyperformingmodelsintermsofover-
Compared to error inconsistency between two all accuracy, the model predictions are relatively
models with the same pretraining dataset, we ob- dissimilar. Ourobservationsherealignwithinves-
6
MNLI QQP Discovery YahooAnswertopics
A B A B A B A B
A 76.00 76.42 A 84.28 84.79 A 18.78 18.61 A 64.18 64.34
B 75.93 75.05 B 88.73 88.41 B 19.99 19.98 B 64.09 64.18
Table3: Performancewhensplittingthedatasetintotwoequal-sizedsubsetsAandBandthenpretrainingonone
(row)andfinetuningonanother(column)
tigationsinvisiontasks,whereGontijo-Lopesetal. larger parameter count of 110 million, compared
(2022)observedthatmodelstrainedwithdifferent toELECTRA-small’s14million. Duetoresource
pretrainingdatasetsproduceduncorrelatederrors. constraints, we pretrained the RoBERTa models
Sincedifferentpretrainingdatasetsproducemod- for fewer iterations as in Warstadt et al. (2020).
els with uncorrelated errors, we ensemble these WepretrainaRoBERTa-basemodelontheBook-
models to check if uncorrelated mistakes lead to Wikicorpusforthesamenumberofiterations. Our
a correct prediction. When the models make dif- resultsshowthatself-pretrainingperformscompa-
ferent predictions, in particular, when one model rablytopretrainingonBookWikicorpus,deliver-
is correct and another is incorrect, the ensemble ingover85%ofpretrainingbenefiton9outof10
prediction will be dominated by the model with datasets,andoutperformingthemodelpretrained
higher confidence in their prediction. As before, onBookWikicorpus(Table5)on5datasets.
we consider ensembles of (i) two independently
finetunedself-pretrainedmodels;(ii)twoindepen-
dently finetuned off-the-shelf models; and (iii) a
8 PerformanceonStructuredPrediction
finetunedversion,eachoftheself-pretrainedand
andCommonsenseNLI
off-the-shelfmodels.
We make the following observations: First, as
expected we observe that ensembling improves While the bulk of our experiments were on a va-
modelperformanceascomparedtoasinglemodel riety of classification tasks, we also experiment
(Table 4). Second, despite having larger error in- withsometasksbeyondsimpleclassification. We
consistency,wedonotobserveanysignificantim- experiment with three types of tasks: (i) span
provements in ensembles of self-pretrained and basedquestionanswering,(ii)namedentityrecog-
off-the-shelfmodelascomparedtoensemblesof nition (NER), and (iii) grounded commonsense
twomodelswiththesamepretrainingstrategy(Ta- inference. For question answering we use the
ble4). Thisisincontrastwithfindingsonvision SQuAD dataset (Rajpurkar et al., 2016) (v1.1)
taskswhereGontijo-Lopesetal.(2022)observed and report the F1-score. For NER, we use the
thatlargererrorinconsistencyledtolargerimprove- CONLL-2012 NER task which uses annotations
mentinensembleperformance. from Ontonotes v5.0 (Weischedel et al., 2013)
involving 18 kinds of named entities. To mea-
7 AblationswithOtherPretraining
sure performance, we use the overall F1 score.
Architectures
Weusetheseqevallibraryforevaluation(https:
//github.com/chakki-works/seqeval). Wein-
We conducted our experiments so far with
clude SWAG (Zellers et al., 2018) and Hel-
ELECTRA-smallarchitecturebecauseitisfaster
laSwag (Zellers et al., 2019) for multiple-choice
topretrainthanotherpopularmodels,yetdelivers
sentencecompletion.
gooddownstreamperformance(Clarketal.,2019)
(e.g. comparabletoBERT-baseonGLUEbench- ForElectra-smallmodels,weseethatforeach
mark). Here,weconductexperimentswithalarger of these datasets self-pretraining achieves more
modelandadifferentpretrainingobjectivetotest than 70% pretraining benefit, and for RoBERTa-
theefficacyofself-pretrainingmorebroadly. basemodelthebenefitis40-80%(Table6). Even
WeexperimentwiththeRoBERTamodelwhich fortheSWAGandHellaSwagdatasets,whichare
uses the masked language modeling objective, designedtouserelyoncommonsenseinferenceof
rather than ELECTRA’s objective. We use the pretrainedmodels,weseeperformanceboostsby
RoBERTa-base architecture, which has a much pretrainingusingonlythetask’strainingset.
7
EnsembleAccuracy ErrorInconsistency
SelfPretrain SelfPretrain
Dataset 2×SelfPretrain 2×Offshelf 2×SelfPretrain 2×Offshelf
+Offshelf +Offshelf
AGNews 94.66 94.17 94.54 1.76 3.50 4.01
QQP 90.92 90.74 91.63 4.57 5.27 8.91
MNLI 78.51 82.37 82.31 6.94 6.42 14.82
PAWS 97.70 97.45 97.75 0.96 1.30 2.07
DBPedia14 99.28 99.19 99.24 0.38 0.48 0.51
Discovery 22.98 25.25 25.02 7.85 9.18 12.66
Yahoo 65.32 64.69 65.64 5.27 5.49 9.55
Amazon 96.40 96.24 96.51 1.26 1.58 2.48
Table4:Performanceofensemblemodelsofself-pretrainedandoff-the-shelfmodels. Forensembling,weaggregate
predictionsofmodelsaftercalibrationwithTemperatureScaling(Guoetal.,2017). Weobservethatinmostofthe
datasets,SelfPretrain+Off-the-shelfensemblingdoesnotimproveoverensemblesoftwomodelswiththesame
pre-trainingstrategy,despiterelativelyhighererrorinconsistencyofSelfPretrain+Off-the-shelfmodels.
Dataset RandInit SelfPretrain BookWiki Benefit% TAPT
AGNews 91.91 94.28 94.22 102.27 94.07
QQP 76.50 88.68 90.18 89.05 90.64
JigsawToxicity 97.32 97.72 98.03 56.02 98.48
MNLI 31.82 75.12 80.90 88.23 79.26
Sentiment140 56.68 68.55 60.19 338.26 65.65
PAWS 50.00 97.34 97.08 100.55 97.85
DBPedia14 98.57 99.21 99.24 95.98 99.23
Discovery 17.36 25.85 26.30 94.91 23.58
YahooAnswertopics 61.11 65.96 64.58 139.80 65.05
AmazonPolarity 89.02 96.68 96.11 108.13 96.16
Table5: PerformanceofRoBERTa-basemodelspretrainedwithdifferenttechniquesondownstreamdatasets.
Datasets Size(MB) ELECTRA-small RoBERTa-base
RI SP OS Benefit% RI SP BW Benefit%
SQuAD 19 15.82 63.01 75.96 78.47 14.93 67.23 81.89 78.11
SWAG 22 27.55 60.56 73.76 71.43 27.95 45.18 70.37 40.62
HellaSwag 30 29.27 39.14 42.91 72.36 24.53 31.03 34.28 66.67
CONLL-2012 6.4 54.49 75.66 82.65 75.18 63.65 72.64 86.25 39.78
Table 6: Performance of ELECTRA and RoBERTa models pretrained with different techniques. RI: random
initialization,SP:self-pretraining,OS:off-the-shelf;BW:pretrainedonBookWikibyus.
9 ConclusionandFutureWork not show that upstream data does not help at all
orknowledgetransferdoesnotoccur, butsimply
In this work, we showed that pretraining models questionstowhatextentitisresponsiblefordown-
onlyontextfromthedownstreamdatasetperforms stream gains. For example, the impressive zero-
comparablytopretrainingonahugeupstreamcor- shotperformanceverylargelanguagemodelssuch
pusforawidevarietyofdatasets. Theerrorsmade as GPT-3 (Brown et al., 2020) clearly suggests
bysuchself-pretrained modelsonthedownstream knowledge transfer is involved. One direction of
tasksaresignificantlydifferentfromtheonesmade futureworkwouldbetoinvestigatehowtheperfor-
bytheoff-the-shelf modelspretrainedonupstream manceofself-pretrainingcomparesofpretraining
corpora. Our results suggest that the importance on upstream corpora as the model sizes go up by
oflearningfromsurplusupstreamdataforimprov- ordersofmagnitude.
ingdownstreamtaskperformancemayhavebeen
overestimated. Crucially,ourexperimentsalsodo We found that the quantity and quality of data
8
requiredforpretrainingtoprovidesignificantbene- References
fit(overarandomlyinitializedmodeltrainedonly
JaimeenAhnandAliceOh.2021. Mitigatinglanguage-
withasupervisedloss)isquitelow. Downstream dependentethnicbiasinbert. InProceedingsofthe
datasets which are tiny in comparison to typical 2021ConferenceonEmpiricalMethodsinNatural
upstreamcorpora,stillfunctionasusefulpretrain- LanguageProcessing,pages533–549.
ingcorporaforgettingperformancegainsacrossa
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
widerangeofdatasets. Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,Amanda
Sinceself-pretrainingdoesnotinvolveanyup-
Askell,etal.2020. Languagemodelsarefew-shot
stream corpus, it prevents exposure of the model
learners. Advancesinneuralinformationprocessing
to potentially undesirable contents in the large systems,33:1877–1901.
upstream corpus, while still delivering large per-
Tuhin Chakrabarty, Christopher Hidey, and Kathleen
formance benefits. Research has demonstrated
McKeown.2019. Imhofine-tuningimprovesclaim
thenegativeinfluenceofweb-sourcedpretraining
detection. In Proceedings of NAACL-HLT, pages
corpora on models, such as generating toxic lan- 558–563.
guage (Gehman et al., 2020) or reflecting racial
Cheng-Han Chiang and Hung-yi Lee. 2020. Pre-
biases in predictions (Ahn and Oh, 2021). For
trainingalanguagemodelwithouthumanlanguage.
use cases that require avoding such issues, self-
arXivpreprintarXiv:2012.11995.
pretraningcanprovideaviablealternativetostan-
dardpretraining. Infuturework,wehopetocom- Kevin Clark, Minh-Thang Luong, Quoc V Le, and
ChristopherDManning.2019. Electra: Pre-training
parehowself-pretrainedmodelsandoff-the-shelf
textencodersasdiscriminatorsratherthangenerators.
modelsperformonthesenegativemeasuressuch
InInternationalConferenceonLearningRepresenta-
astoxicityandsocialbiases. tions.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
10 Limitations KristinaToutanova.2019. Bert: Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstand-
ing. InProceedingsofthe2019Conferenceofthe
Due to limited availability of compute resources, NorthAmericanChapteroftheAssociationforCom-
we were unable to scale up the model architec- putationalLinguistics: HumanLanguageTechnolo-
gies,Volume1(LongandShortPapers),pages4171–
turetothelargesizesbecomingincreasinglymain-
4186.
stream today. Similarly, the upstream corpus we
used (BookWiki) is 16GB in size, and while it Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron,
is large enough such that it was used to pretrain IvanLaptev,HervéJegou,andEdouardGrave.2021.
Arelarge-scaledatasetsnecessaryforself-supervised
BERT(Devlinetal.,2019),muchlargerpretraining
pre-training? arXivpreprintarXiv:2112.10740.
datasetsareinusetodaysuchastheColossalCom-
monCrawlCorpus(Raffeletal.,2020). Therela- Samuel Gehman, Suchin Gururangan, Maarten Sap,
tiveperformanceachievedbyusingself-pretraining Yejin Choi, and Noah A Smith. 2020. Realtoxici-
typrompts: Evaluatingneuraltoxicdegenerationin
vspretrainingonupstreamcorpuscanlikelyvary
language models. In Findings of the Association
with the size of the model and upstream corpus,
forComputationalLinguistics: EMNLP2020,pages
andmorecompute-heavylargescaleexperiments 3356–3369.
areneededtocharacterizeit.
Robert Geirhos, Kristof Meding, and Felix A Wich-
mann.2020. Beyondaccuracy: quantifyingtrial-by-
trial behaviour of cnns and humans by measuring
11 Acknowledgements
errorconsistency. AdvancesinNeuralInformation
ProcessingSystems,33:13890–13902.
ThisworkwasfundedbyUPMCandAbridgeAI
AlecGo,RichaBhayani,andLeiHuang.2009. Twit-
Inc. Wealsogratefullyacknowledgesupportfrom
tersentimentclassificationusingdistantsupervision.
Amazon AI, the PwC Center, the Software Engi-
CS224Nprojectreport,Stanford,1(12):2009.
neering Institute, and NSF (Award no. 2211955)
forthecomputeresourcesusedinthisproject. SG Raphael Gontijo-Lopes, Yann Dauphin, and Ekin D
Cubuk. 2022. No one representation to rule them
acknowledgesAmazonGraduateFellowshipand
all: Overlappingfeaturesoftrainingmethods. InIn-
JPMorganAIPh.D.Fellowshipfortheirsupport.
ternationalConferenceonLearningRepresentations
(ICLR).
9
ChuanGuo,GeoffPleiss,YuSun,andKilianQWein- Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor
berger.2017. Oncalibrationofmodernneuralnet- Mordatch. 2021. Pretrained transformers as
works. InInternationalconferenceonmachinelearn- universal computation engines. arXiv preprint
ing,pages1321–1330.PMLR. arXiv:2103.05247.
Suchin Gururangan, Ana Marasovic´, Swabha ChristopherDManning,KevinClark,JohnHewitt,Ur-
Swayamdipta,KyleLo,IzBeltagy,DougDowney, vashi Khandelwal, and Omer Levy. 2020. Emer-
and Noah A Smith. 2020. Don’t stop pretraining: gentlinguisticstructureinartificialneuralnetworks
Adapt language models to domains and tasks. In trainedbyself-supervision. ProceedingsoftheNa-
Proceedings of the 58th Annual Meeting of the tionalAcademyofSciences,117(48):30046–30054.
Association for Computational Linguistics, pages
8342–8360. Isabel Papadimitriou and Dan Jurafsky. 2020. Learn-
ing music helps you read: Using transfer to study
Xiaochuang Han and Jacob Eisenstein. 2019. Unsu- linguisticstructureinlanguagemodels. InProceed-
pervised domain adaptation of contextualized em- ingsofthe2020ConferenceonEmpiricalMethods
beddingsforsequencelabeling. InProceedingsof in Natural Language Processing (EMNLP), pages
the2019ConferenceonEmpiricalMethodsinNatu- 6829–6839.
ralLanguageProcessingandthe9thInternational
MatthewE.Peters,MarkNeumann,MohitIyyer,Matt
JointConferenceonNaturalLanguageProcessing
Gardner,ChristopherClark,KentonLee,andLuke
(EMNLP-IJCNLP),pages4238–4248.
Zettlemoyer.2018. Deepcontextualizedwordrepre-
sentations. InProceedingsofthe2018Conferenceof
KaimingHe,XinleiChen,SainingXie,YanghaoLi,Pi-
theNorthAmericanChapteroftheAssociationfor
otrDollár,andRossGirshick.2022. Maskedautoen-
ComputationalLinguistics: HumanLanguageTech-
codersarescalablevisionlearners. InProceedingsof
nologies,Volume1(LongPapers),pages2227–2237,
theIEEE/CVFConferenceonComputerVisionand
NewOrleans,Louisiana.AssociationforComputa-
PatternRecognition,pages16000–16009.
tionalLinguistics.
Kaggle.com. 2018. Toxic comment classifica-
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
tion challenge: Identify and classify toxic
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
online comments. https://www.kaggle.com/c/
AlexanderMiller.2019. Languagemodelsasknowl-
jigsaw-toxic-comment-classification-challenge.
edge bases? In Proceedings of the 2019 Confer-
enceonEmpiricalMethodsinNaturalLanguagePro-
Kundan Krishna, Jeffrey P Bigham, and Zachary C
cessingandthe9thInternationalJointConference
Lipton.2021. Doespretrainingforsummarization
onNaturalLanguageProcessing(EMNLP-IJCNLP),
requireknowledgetransfer? InFindingsoftheAsso-
pages2463–2473.
ciationforComputationalLinguistics:EMNLP2021,
pages3178–3189.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
Lee,SharanNarang,MichaelMatena,YanqiZhou,
YoavLevine,NoamWies,DanielJannai,DanNavon,
WeiLi,PeterJLiu,etal.2020. Exploringthelimits
YedidHoshen,andAmnonShashua.2021. Thein-
oftransferlearningwithaunifiedtext-to-texttrans-
ductivebiasofin-contextlearning: Rethinkingpre-
former. J.Mach.Learn.Res.,21(140):1–67.
trainingexampledesign. InInternationalConference
onLearningRepresentations.
PranavRajpurkar,JianZhang,KonstantinLopyrev,and
PercyLiang.2016. Squad: 100,000+questionsfor
YongjieLina,YiChernTana,andRobertFrankb.2019.
machinecomprehensionoftext. InProceedingsof
Opensesame: Gettinginsidebert’slinguisticknowl-
the2016ConferenceonEmpiricalMethodsinNatu-
edge. ACL2019,page241.
ralLanguageProcessing,pages2383–2392.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man- MachelReid,YutaroYamada,andShixiangShaneGu.
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, 2022. Canwikipediahelpofflinereinforcementlearn-
Luke Zettlemoyer, and Veselin Stoyanov. 2019. ing? arXivpreprintarXiv:2201.12122.
Roberta: A robustly optimized bert pretraining ap-
proach. arXivpreprintarXiv:1907.11692. DamienSileo,TimVanDeCruys,CamillePradel,and
Philippe Muller. 2019. Mining discourse markers
LajanugenLogeswaran,Ming-WeiChang,KentonLee, forunsupervisedsentencerepresentationlearning. In
KristinaToutanova,JacobDevlin,andHonglakLee. Proceedings of the 2019 Conference of the North
2019. Zero-shotentitylinkingbyreadingentityde- AmericanChapteroftheAssociationforComputa-
scriptions. InProceedingsofthe57thAnnualMeet- tionalLinguistics: HumanLanguageTechnologies,
ingoftheAssociationforComputationalLinguistics, Volume1(LongandShortPapers),pages3477–3486,
pages3449–3460. Minneapolis,Minnesota.AssociationforComputa-
tionalLinguistics.
Ilya Loshchilov and Frank Hutter. 2018. Decoupled
weightdecayregularization. InInternationalConfer- Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle
enceonLearningRepresentations. Pineau, Adina Williams, and Douwe Kiela. 2021.
10
Maskedlanguagemodelingandthedistributionalhy- Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
pothesis: Orderwordmatterspre-trainingforlittle. Paws: Paraphraseadversariesfromwordscrambling.
arXivpreprintarXiv:2104.06644. InProceedingsofthe2019ConferenceoftheNorth
AmericanChapteroftheAssociationforComputa-
IanTenney,DipanjanDas,andElliePavlick.2019. Bert tionalLinguistics: HumanLanguageTechnologies,
rediscoverstheclassicalnlppipeline. InProceedings Volume1(LongandShortPapers),pages1298–1308.
of the 57th Annual Meeting of the Association for
ComputationalLinguistics,pages4593–4601. YukunZhu,RyanKiros,RichZemel,RuslanSalakhut-
dinov,RaquelUrtasun,AntonioTorralba,andSanja
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Fidler.2015. Aligningbooksandmovies: Towards
Hill,OmerLevy,andSamuelBowman.2018. Glue:
story-like visual explanations by watching movies
A multi-task benchmark and analysis platform for
andreadingbooks. InProceedingsoftheIEEEin-
naturallanguageunderstanding. InProceedingsof
ternational conference on computer vision, pages
the2018EMNLPWorkshopBlackboxNLP:Analyz-
19–27.
ingandInterpretingNeuralNetworksforNLP,pages
353–355.
AlexWarstadt,YianZhang,XiaochengLi,HaokunLiu,
andSamuelBowman.2020. Learningwhichfeatures
matter: Robertaacquiresapreferenceforlinguistic
generalizations(eventually). InProceedingsofthe
2020ConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP),pages217–235.
RalphWeischedel,MarthaPalmer,MitchellMarcus,Ed-
uardHovy,SameerPradhan,LanceRamshaw,Nian-
wenXue,AnnTaylor,JeffKaufman,MichelleFran-
chini,etal.2013. Ontonotesrelease5.0ldc2013t19.
LinguisticDataConsortium,Philadelphia,PA,23.
AdinaWilliams,NikitaNangia,andSamuelBowman.
2018. A broad-coverage challenge corpus for sen-
tenceunderstandingthroughinference. InProceed-
ingsofthe2018ConferenceoftheNorthAmerican
Chapter of the Association for Computational Lin-
guistics: HumanLanguageTechnologies,Volume1
(LongPapers),pages1112–1122.
YuhuaiWu,FelixLi,andPercyLiang.2022. Insights
intopre-trainingviasimplersynthetictasks. arXiv
preprintarXiv:2206.10139.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell,RussRSalakhutdinov,andQuocVLe.2019.
Xlnet: Generalizedautoregressivepretrainingforlan-
guageunderstanding. Advancesinneuralinforma-
tionprocessingsystems,32.
Xingcheng Yao, Yanan Zheng, Xiaocong Yang, and
ZhilinYang.2022. Nlpfromscratchwithoutlarge-
scalepretraining: Asimpleandefficientframework.
InInternationalConferenceonMachineLearning,
pages25438–25451.PMLR.
RowanZellers,YonatanBisk,RoySchwartz,andYejin
Choi.2018. Swag: Alarge-scaleadversarialdataset
forgroundedcommonsenseinference. InEMNLP.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machinereallyfinishyoursentence? InProceedings
of the 57th Annual Meeting of the Association for
ComputationalLinguistics,pages4791–4800.
XiangZhang,JunboJakeZhao,andYannLeCun.2015.
Character-levelconvolutionalnetworksfortextclas-
sification. InNIPS.
11
A Appendix ply use it to reorder sentences already present in
the downstream dataset for pretraining We also
A.1 TheRoleofSentenceOrderin
makesurethatthealgorithmdoesnotaccidentally
PretrainingCorpora
recover the original order of sentences (e.g. by
For virtually all pretrained models like BERT, matching the premise-hypothesis pairs originally
ELECTRA,XLNet,thesentencesinthepretrain- intheMNLIdataset).
ingcorporaareorderedastheynaturallyoccurin Weexperimentwith5differentdatasetsandfind
somedocumentsuchasWikipediaarticle. Devlin thatthesentence-reorderingschemeimprovesper-
etal.(2019)mentionintheirwork: “Itiscritical formancecomparedtorandomsentenceorderfor
touseadocument-levelcorpusratherthanashuf- allofthemexceptQQP.ForDiscoveryandDBPe-
fledsentence-levelcorpus(...) inordertoextract dia14datasets,itscoresevenhigherthanourstan-
long contiguous sequences.” However, for many dard sentence ordering scheme which preserves
ofourpretrainingcorporamadefromdownstream theadjacencyandorderofsentenceswithineach
datasets,thesentencetakeninorderdonotforma datapoint. This shows that re-ordering sentences
coherentdocumentornarrativetext. Forexample, topromotecontentsimilaritybetweenneighboring
in the MNLI or QQP corpora, neighboring sen- sentences, can potentially improve GLUE score,
tenceswillsimplybepremise-hypothesispairsor withoutintroducinganynewinformationornarra-
potentialparaphrasecandidates. tivestructure.
Despitethesentenceordernotformingacoher-
ent document, many pretraining corpora achieve A.2 ExperimentswithSmallerELECTRA
high performance boosts on the GLUE language Models
understandingbenchmark(Table7). Forexample, In addition to experimenting with a base-sized
MNLI achieves around 96% of the performance
architecture (110M parameters), we also experi-
boost of the off-the-shelf model (Table 7). Inter-
ment with architectures which are even smaller
estingly, shuffling the sentences in these corpora
thanELECTRA-small. WetrainELECTRAmod-
leadstoalargedropinperformance(Table7). This
elsofsmallersizebyeitherreducingthenumberof
suggests that there is some value to keeping the
layersinthegeneratoranddiscriminator,orreduc-
sentence order in a way that puts sentences from ingthehiddendimensionofthediscriminator3. As
thesameexampleindatasetslikeMNLIandQQP
themodelsgetsmaller,self-pretrainingcontinues
nexttoeachother. Alikelyexplanationofthisis
to significantly outperform random initialization
inLevineetal.(2021)whereauthorsshowedthat
and often outperforms pretraining on BookWiki
including similar sentences in the same input se-
corpus(Figure3). Interestingly,therelativeperfor-
quencewhenpretrainingshouldleadtoimproved
manceofself-pretrainedandBookWiki-pretrained
performanceviatheoreticalanalysisandempirical
models tends to stay the same across model size.
experiments.
For example, for QQP self-pretraining is always
WetestifGLUEperformancecanbeimproved
bestandforMNLIBookWiki-pretrainingisalways
byartificiallyre-orderingasetofsentencestopro-
bestirrespectiveofnumberoflayersorhiddensize.
motetheoccurrenceofsimilarsentencestogether.
Werearrangethesentencesinthesentence-shuffled A.3 ImplementationDetailsforPretraining
versionsofpretrainingcorporatoencouragecon- andFinetuning
tentoverlapamongneighboringsentences,andsee
HyperparametersforPretraining Forpretrain-
ifthiscanrecoversomeofthedropsinperformance
ingELECTRA-smallmodels,weusethestandard
thatoccurredduetoshuffling. Ouralgorithmcre-
hyperparameters (Table 8) as described in Clark
atesthecorpusbyiterativelyappendingsentences
etal.(2019). FortheRoberta-basemodels, train-
to it, such that at each step the new sentence is
ing with the standard hyperparameters with our
theonewithmaximumTF-IDFsimilaritywiththe
computingresourceswouldbeprohibitivelyslow,
previous sentence. Such a way of constructing a
and so we used hyperparameters from Warstadt
corpusbysimilaritybasedretrievalhasbeenused
etal.(2020)whichrequirelessertimetotrain(Ta-
inpastworks(Levineetal.,2021;Yaoetal.,2022),
with the main difference that they retrieved sen- 3InELECTRA,thegenerator’shiddensizeisalreadymuch
smallerthanthatofthediscriminatorbydesign.Sowedonot
tences from external corpora similar to the ones
reduceitfurther,inordertohaveareasonablywell-performing
presentinthedownstreamdataset,whereaswesim- generator.
12
sourced from the Huggingface library4. For the
YahooAnswertopicsdataset,weuseonlythetext
fromtheanswer(notthequestion)asinputtothe
models(bothforpretrainingandfinetuning). For
thePAWSdataset,weusetheversioncalled“Unla-
beledPAWS ”in Zhangetal.(2019),whichis
wiki
actuallynotunlabeledbuthassilverlabels. Wepre-
ferredthatversionoverothersbecauseofitslarger
size. For datasets which had a train and test split
butnovalidationsplit(e.g. YahooAnswertopics),
weextracted5000randomdatapointsfromthethe
trainsplittomakethevalidationsplit. Ifadataset
hadatrainandvalidationsplitbutnotestsplit(e.g.
UnlabeledPAWS ),wedesignatedthevalidation
wiki
splittobethetestsplit,andcreatedanewvalida-
tionsetbyextracting5000randomdatapointsfrom
thetrainset.
A.4 HardwareandSoftwarePackagesUsed
For pretraining ELECTRA models, we used
Nvidia’simplementationoftheELECTRAcode-
base5,runusingNvidia’sTensorflowcotainerim-
age 21.07 6. For pretraining Roberta models, we
usedtheofficialimplementationintheFairseqli-
Figure3: VariationinperformanceofELECTRAmod- brary7. For finetuning experiments, we used the
elswithchangeinnumberoflayersandhiddensize(—
AllenNLPlibraryfortrainingandevaluationrou-
randomlyinitialized,—self-pretrained,—BookWiki-
tines,coupledwiththeHuggingfacelibraryforthe
pretrained)
modelarchitectures.
WeusedacollectionofNvidiaV100(32GB)and
A6000(48GB)GPUsforourexperiments. Pretrain-
ble 8). For task-adaptive pretraining(TAPT), we
ing an ELECTRA-small model takes around 1.5
follow Gururangan et al. (2020) and further pre-
dayson2GPUswhilepretrainingaRoberta-base
train off-the-shelf models for 100 epochs on the
modeltakesaround1.5dayson4GPUs.
downstream task’s training set, with the first 6%
oftheresultingtotalupdatesusedforlearningrate
warmup.
HyperparametersforFinetuning Forfinetuning
themodelsonthe10downstreamdatasets,weuse
hyperparametersasshowninTable9. Weusethe
AdamWoptimizer(LoshchilovandHutter,2018)
forfinetuning. Weuseearlystoppingbasedonvali-
dationsetperformance. Thevalidationmetricused
ismeansquarederrorforthesentiment140dataset
(regression), average binary crossentropy for the
jigsawdataset(multi-labelclassification),andac-
curacyforallotherdatasets(multi-classclassifica-
tion). Thepatienceparameterforearlystoppingis 4https://huggingface.co/docs/datasets/index
set to 3 epochs. For finetuning ELECTRA-small
5https://github.com/NVIDIA/
DeepLearningExamples/tree/master/TensorFlow2/
modelsontheGLUEdatasets,weusethestandard
LanguageModeling/ELECTRA
learningrateof1e-4followingClarketal.(2019). 6https://docs.nvidia.com/deeplearning/
frameworks/tensorflow-release-notes/rel_21-07.
DetailsaboutUseofDownstreamDatasets All
html
downstream datasets used in this paper were 7https://github.com/facebookresearch/fairseq
13
PretrainingDataset Random Standard TF-IDF(Ours)
None(RandomInit) - 53.20 -
Sentiment140 - 72.67 75.29
DBpedia14 72.82 70.38 75.44
Discovery 71.79 77.26 78.94
MNLI 62.80 78.28 76.33
QQP 71.09 75.43 69.57
BookWiki(Off-the-shelf) - 79.43 -
Table7: GLUEscoresachievedbydifferentstrategiesfororderingsentencesfromthedownstreamdatasetusedfor
pretraining. Random: randomlyorderedsentences;Standard: sentenceswithinadatapointoccurcontiguouslyin
originalorder;TF-IDF:sentencesreorderedusingcontentsimilarity.
Hyperparameter ELECTRA Roberta
Size(Parametercount) Small(14M) Base(110M)
Trainingsteps 1M 100K
Warmupsteps 10K 6K
Batchsize 128 512
Peaklearningrate 5e-4 5e-4
Sequencelength 128 512
Table8: Hyperparametersusedforpretrainingmodels
Hyperparameter ELECTRA Roberta
Trainingepochs 20 20
Batchsize 32 32
Learningrate {1e-4,1e-5} 2e-5
Maxsequencelength 512 512
Table9: Hyperparametersusedforfinetuningmodelson10downstreamtasks
14

“Don’t Take This Out of Context!”
On the Need for Contextual Models and Evaluations for Stylistic Rewriting
AkhilaYerukola♡ XuhuiZhou♡ ElizabethClark♢ MaartenSap♡♣
♡LanguageTechnologiesInstitute,CarnegieMellonUniversity
♢GoogleDeepMind♣AllenInstituteforAI
#ayerukol@andrew.cmu.edu
Abstract
Mostexistingstylistictextrewritingmethods
Preceding Dialog: “I can't believe
and evaluation metrics operate on a sentence how much work we have to do.”
level, butignoringthebroadercontextofthe
text can lead to preferring generic, ambigu-
ous, and incoherent rewrites. In this paper, Informal response: “I know, right? I'm
drowning in them.”
weinvestigateintegratingtheprecedingtextual
contextintoboththerewritingandevaluation
Rewrite it as
stagesofstylistictextrewriting,andintroduce formal
anewcompositecontextualevaluationmetric Or
CtxSimFitthatcombinessimilaritytotheorig- make it less toxic / change to positive
inalsentencewithcontextualcohesiveness. We sentiment
comparativelyevaluatenon-contextualandcon-
Contextual rewrite
textualrewritesinformality,toxicity,andsenti- Indeed, I concur. The Non-contextual Rewrite
Indeed, I concur. I am
menttransfertasks. Ourexperimentsshowthat workload is quite inundated with them.
overwhelming.
humanssignificantlyprefercontextualrewrites
asmorefittingandnaturalovernon-contextual
ones,yetexistingsentence-levelautomaticmet-
Figure1: Exampleofusingtheprecedingdialogutter-
rics (e.g., ROUGE, SBERT) correlate poorly
ancetohelpwithstylisticrewriting: here,wetransform
withhumanpreferences(ρ=0–0.3). Incontrast,
aninformalresponseintoformallanguage. Incorporat-
humanpreferencesaremuchbetterreflectedby
ing“workload”and“overwhelming”enhancesthecon-
bothournovelCtxSimFit(ρ=0.7–0.9)aswell
textualcohesivenessoftherewrittentext,whilesolely
asproposedcontext-infusedversionsofcom-
using“inundated”resultsinamoregenericrewrite.
monmetrics(ρ=0.4–0.7). Overall,ourfindings
highlighttheimportanceofintegratingcontext
intothegenerationandespeciallytheevalua- Similarly,precedingtextualcontexthaslargely
tionstagesofstylistictextrewriting. beenoverlookedinautomaticevaluationsforstylis-
ticrewriting,withmostworkfocusingonsentence-
1 Introduction
levelmetrics(e.g.,Lietal.,2018;Reifetal.,2022).
Existing methods for stylistic text rewriting, i.e., Thislackofcontextatthemodelingandevaluation
adaptingthetexttoaparticularstylewhilepreserv- stageshindersthecreationofeffectiveAI-assisted
ingitsoriginallyintendedmeaning,oftenfailtoac- rewritingtoolsforusers(e.g.,forassistivewriting
countforastatement’scontext(e.g.,Huetal.,2017; tools;MacArthur,2009;Clarketal.,2018).
Shen et al., 2017; Fu et al., 2018; Li et al., 2018; Inthispaper,wepresentacomprehensiveanaly-
Lampleetal.,2019;Madaanetal.,2020;Hallinan sis of the need for context in stylistic rewriting
etal.,2023). Asaresult,thesesystemsmaychange and its evaluation, on three different rewriting
thespeakers’originalcommunicativeintentsand tasks (formality transfer, sentiment change, and
generate contextually irrelevant and generic out- text detoxification) and two types of textual con-
puts. For example, in Figure 1, a non-contextual texts(precedingturnsinaconversation,preceding
modelrewritinganinformalresponsetoaformal sentences in a document). To study these effects,
onesimplyreplaceswordswithmoreformalsyn- we design a contextual human evaluation frame-
onyms,whereasacontextualrewritingmodelcan work(§5)tocomparativelyevaluatenon-contextual
usethebroaderconversationalcontexttoproduce andcontextualrewritingmethodsbuiltonfew-shot
amorespecificandnaturalformalrewrite. promptedlargelanguagemodels(§4).
3202
tcO
32
]LC.sc[
2v55741.5032:viXra
We show that human evaluators prefer contex- alackofdatasetsthatincludecontextualinforma-
tualrewritesintermsofnaturalness,stylestrength, tion. While new models have emerged that do
andintendedmeaningpreservation,acrossallthree not require parallel data for training (Hu et al.,
tasks. However, non-contextual automatic met- 2017; Li et al., 2018; Ma et al., 2020; Hallinan
ricsforlexicalorsemanticmeaningpreservation et al., 2023), they also operate without contex-
correlatepoorlywiththesepreferences(ρ=0–0.3; tual information. Building on some preliminary
§6),despitebeingcommonlyusedinpreviousstyle research that explored context in small custom-
transfer work to measure meaning preservation trained seq2seq rewriting models (Cheng et al.,
(Miretal.,2019;Madaanetal.,2020). 2020;Atwelletal.,2022)andlargelanguagemod-
Toaddresstheneedforcontextinautomaticeval- elsforexemplar-basedconversation-levelrewriting
uations,weintroduceCtxSimFit,anewcomposite (Roy et al., 2023), we extend the investigation to
metric that combines original sentence similarity largelanguagemodelswithdefinedstyleattributes
andcontextualcohesivenesstoevaluatethequality likeformality,toxicity,andsentiment. Importantly,
ofrewrites,takingintoaccounttheprecedingcon- wealsoexploretheneedforcontextinevaluations
text(§7). Additionally,weproposecontext-infused in addition to modeling, and propose a new suite
versionsofcommonlyusedautomaticmetricsfor ofcontextualizedmetricsforautomaticevaluation.
meaning preservation. Our results show that hu-
EvaluationofStylisticTextRewriting Evaluat-
manpreferencesaresignificantlycorrelatedwith
ingwhethersentencerewritingpreservesmeaning
these contextual metrics—especially CtxSimFit
whileachievingthedesiredtargetstylehasproved
(ρ=0.7–0.9),muchmorethannon-contextualones.
challenging. Existingmetricsandapproachescan
Ourcontributionsaresummarizedasfollows: (1)
disentangle meaning and style (Mukherjee et al.,
Weinvestigatetheneedforcontextintextrewriting,
2022;Yuetal.,2021). However,determiningwhat
showingthatincorporatingit,whetheratthedocu-
constitutes“meaningpreservation”remainsincon-
mentorconversationallevel,leadstocontextually
sistent. Someworks(Lietal.,2018;Sudhakaretal.,
coherentandrelevantrewritespreferredbyhuman
2019; Mir et al., 2019; Reif et al., 2022; Madaan
annotatorsacrossstyletransfertasks. (2)Wecon-
etal.,2020)usemetricssuchasBLEU,ROUGE,and
ductacomprehensiveanalysisontheneedforcon-
METEOR,whichmeasuren-gramoverlapsandlex-
textinautomaticevaluation,revealingthatexisting
ical similarity as indicators of meaning preserva-
metrics don’t align with human preferences. (3)
tionrespectively,whileotherstudies(Wangetal.,
We propose a custom metric, CtxSimFit, along
2019; Reid and Zhong, 2021; Roy et al., 2023)
with context-infused versions of common auto-
adoptmetricslikeSBERTandBERTScoremeasur-
maticmetrics,tobridgethegapbetweencontextual
ingsemanticsimilarityofembeddingsasproxies
understandingandautomatedmetrics. Overall,our
formeaningpreservation. Further,themajorityof
contributionsprovideamorenuancedunderstand-
work(Huetal.,2022;Madaanetal.,2020;Lietal.,
ing of the importance of context, which is criti-
2018)doesnotprovideannotatorswithanypreced-
calfordevelopmentofmoreeffectiveandreliable
ingcontextduringhumanevaluations. Thus,more
stylistictextrewritingtechniques.
standardizedandcontext-awareevaluationmetrics
areneededfortextrewritingapproaches.
2 Background&RelatedWork
3 TaskandDatasets
Inthissection,wediscusstheincreasinginterestin
incorporatingcontextintoNLPtasksandmotivate
Tomeasuretheimportanceofcontextinrewriting,
the significance of context during the rephrasing
wescopeourinvestigationsaroundthreespecificat-
andevaluationphasesofstylistictextrewriting.
tributecontrols: formality,sentiment,andtoxicity,
Stylistic Text Rewriting Despite being intro- chosen because they necessitate varying degrees
duced over ten years ago (Xu et al., 2012), cur- of meaning preservation and style intensity. We
rentmethodsforstylisticrewriting(e.g.Shenetal., present statistics for each of the datasets used in
2017; Xu et al., 2018b; Fu et al., 2018; Lample ourrewritingtasksinTable1.
et al., 2019; Jin et al., 2022; Chawla and Yang,
3.1 Tasks&Datasets
2020;Yerukolaetal.,2021;Daleetal.,2021;Lo-
gachevaetal.,2022,etc.) stillrelysolelyonparal- Changing Formality Formality transfer (Rao
lelsource-to-targetsentencepairs,primarilydueto and Tetreault, 2018) aims to transform sentences
Task ContextType Datasets #Instances
Conversation Reddit 1000 Preceding Random
Formality Document/ Document/
Document CNNDailyMail 1000 Dialogue Dialogue
+BlogAuthorship
Conversation DailyDialog 1000 Original Original Original
Sentiment Sentence/ Sentence/ Sentence/
Document YelpReviews 1500 Utterance Utterance Utterance
Conversation CCC 1000
Toxicity Conversation MDMD 900
Open
Conversation ProsocialDialog 1000 Or sourced
Table1: Statisticsofthecollecteddatasets,presented
GPT 3.5 GPT-NeoX
by task and context type, considering both preceding
sentencesinadocumentandturnsinaconversation.
Non Random
Contextual
contextual contextual
rewrite
frominformalorcasuallanguageintoformallan- rewrite rewrite
guage,andviceversa. Thisrequiresmakingstylis-
tic adjustments while ensuring that the original
Contextual Eval & Non-contextual Eval
content and intention remain intact. We use a
conversational dataset from Reddit1 and curated
Figure2: Overviewofourapproach: Weexaminethree
a document-based dataset from CNN Daily Mail
kindsofrewrites-contextual,non-contextual,andran-
(formal; Nallapati et al., 2016) and the Blog Au- domcontextualrewrites. GPT-3.5andGPT-NeoXare
thorshipCorpus(informal;Schleretal.,2006). utilizedforrewritingviain-contextlearning.Evaluation
methods consist of non-contextual evaluation, which
RewritingSentiment Forsentimenttransfer(Hu does not consider context, and contextual evaluation,
etal.,2017),ourfocusliesinconvertingsentences whichincorporatescontextintotheassessmentprocess.
withpositivesentimenttonegativesentiment,and
Fordocument-baseddatasets,weselectthreecon-
vice versa, as well as transforming neutral sen-
textsentencesandoneforrewriting.
tences to convey positive or negative sentiment.
We label the context and response using pre-
Here, both the content and intention are altered;
trained style classifiers: RoBERTa-Base formal-
however, the main subject entities remain consis-
ity classifier,2 XLM-RoBERTa-Base sentiment
tent, although with a change in sentiment. We
classifier (Barbieri et al., 2022) and toxicity
obtainaconversationaldatasetfromtheDailyDia-
scoresfromPerspectiveAPI,3 HateBert(Tommaso-
log(Lietal.,2017)datasetandadocument-based
CaselliandJelenaMitrovic,2021)andHateRoberta
datasetfromYelpreviews(Zhangetal.,2015).
(Hartvigsenetal.,2022). Weselectastratifiedsam-
De-toxifying Text Here, our objective is to ple that includes a wide range of style strengths.
rewrite text in a manner that reduces toxicity, as SeeAppendixAfordatasetsandclassifiersdetails.
introduced by Nogueira dos Santos et al. (2018).
4 ModelingContextinRewriting
Rewriting may modify the original content, but
theinitialintentshouldbepreservedandconveyed Inthissection,weintroduceourmethodologyfor
using less offensive language. In this task, we contextual stylistic rewriting utilizing large lan-
examine three conversational datasets: the Civil guagemodels(LLMs)andin-contextlearning. We
CommentsinContext(CCC)dataset(Xenosetal., conduct a comparison of three types of rewrites:
2021),theMulti-LabelDialogueMalevolenceDe- thosegeneratedwithcontext,thosegeneratedwith-
tection(MDMD)dataset(Zhangetal.,2022),and out context, and those generated with a random
theProsocialDialogdataset(Kimetal.,2022). context (as a counterfactual baseline). Figure 2
providesavisualrepresentationofourapproach.
3.2 DataPreparation
4.1 ContextualRewriting
Forconversationaldatasets(asdepictedintheex-
To address the challenge of insufficient parallel
ampleinFigure1),wefocusontwo-turns,repre-
data,asdiscussedin§2,weproposeacontextual
sentingparentcontextandresponseforrewriting.
2https://huggingface.co/s-nlp/
1Weusereddit-corpus-smallfromhttp://convokit. roberta-base-formality-ranker
cornell.edu/documentation/subreddit.html 3https://perspectiveapi.com/
Context OriginalSentence Model Style ContextualRewrite Non-contextualRewrite
(a) S(cid:220) hittB iee ss tt BB oo nn dd tm heo mv eie ee vv ee rr !!
!!
(cid:215) yoH ua ?ve Fn o’ rt ts he een laS ttp ee r,c ntr oe t, th ha eve GPT-3.5 for "mal (cid:215) mayH nav oe tn h’ at vy eo hu as de te hn eS bp ee stct tr he e? mI et (cid:215) NotH ya ov ue ,n b’ ut ty soo mu es oe ne en elS sp ee .ctre?
former. informal song,butit’sdefinitelyoneofthe
bestBondmovies!
@Iamreadingabook,777 (cid:15)Herezsomeofthem, informal (cid:15)Herearesomeofthem,and (cid:15)Hereissomeoftheoneand
(b) MathematicalConversation andfurtherIwillkeep GPT-NeoX " inalaterpostIwillkeepupdat- furtherIwillupdatethispage
StartersbyJohndePillis.Itsan updatingthispagewithwhat formal ingthispagewithmyfavourite withwhatelseIfindinteresting.
interestingbookwithlotsof Ilike. quotesfromthebook.
quotes,poems
(c) wS fa(cid:220) uw n ni t nfW tt yod ah "o na sn dmt ee datx h oca ka ec t st h nml ey ’r tah ?k e"a ves T es nT haa n i msy yl aioo s kn nr ee ’t sm(cid:215) acF k mo hr oe n$ r ge8 t rh y3 ao5 nIu ’, d0 dg m0 h s0 . om, tF0 hao0 c er0 k r.t, h0 ma0 yt0 mI’ ud ch GPT-3.5 in fofo r" mrm aa ll (cid:215) b s at ni rl ilA ekio nin n ofi g rn m ma Tin g a oc yh ui lt sa oll arei ma Sn d wc oe m uin f ne tt . ti tv Wo ae tc io stof thn a$ s ks8 i u ed3 c ,e5 h Ir (cid:215) fi c iclv i ae n lF ebo adi sr l t sle oi aoi ug cn loh td mt o wh mll iu a tin hr tsd t, rhr ee I e gd w a aa co rdn tu sd old f tt oh b pei hr myt iy n s y- --
sense. wouldevenbeinclinedtostrike grandmother.
myowngrandmother.
(d) (cid:220)Soifazombieapocalypse (cid:215)@USERfuckitI’mkilling GPT-3.5 lesstoxic (cid:215)I’drathernotthinkabouta (cid:215)Iamfeelingoverwhelmedand
occurred,whatwouldyoudo myself zombieapocalypse. hopeless,soI’lldie
(cid:215)Youappeartobesayingthe (cid:215)Youseemtothinkthatthe (cid:215)Idisagreewiththeideathat (e) (cid:220)It’sremarkablehowtheBible Bibleisareliablesourcefor GPT-NeoX lesstoxic Bibleisanaccuratesourcefor theBibleisareliablesourcefor
hasrecordedhistoricalevents, history.Itisn’t.Andifyou history.Whileitmaybeasacred history. Ifyouhaveotheropin-
isn’tit? disagree,gotoRSNwhereyou collectionoftexts,itisnotare- ions,pleasegotoRSNtobedis-
willbeshotdownliterallywith liablesourceforhistoricalfacts. heartened
fire. Forfurtherdiscussion,Isuggest
visitingtheRSNforums.
(f) (cid:220)mythroatfeelsweirdanddry (cid:215)@USERoof.see,thereare GPT-3.5 lesstoxic (cid:215)@USERIhopeyoufeelbetter (cid:215)@USER,therearenegative
iquit downsidestosuckingdick. soon. aspectstoperformingoralsex.
(g) (cid:220)Shall mIs oe re niy no gu ?onFriday (cid:215) ItT ’llha ht a’ vs en to ot bg eoo ad nofo thr em rte imat ea .ll. GPT-3.5 neg "ative (cid:215) forF mri ed ,a by um tlo etr ’n sin pg lad no fe os rn a’t now to hr ek
r
(cid:215) forT ah na ot t’ hs eg rr te ia mt efo .rme.Let’splan
positive time.
@NotonlydidTimfixmycar (cid:15)He’salsoanextremely positive (cid:15) However, Tim’s not very (cid:15)Heisalsoaveryrude,una- (h) quicklyandinexpensively,he friendly,funnyguywhoputs GPT-NeoX " friendly or humorous which musedguywhomakesyoufeel
actuallycametopickitup
youateaseimmediately. negative mightmakeyoufeeluneasy anxious.
himselfsothatIwouldn’thave
tospendmoneyonatowtruck!
(i) (cid:220)Let bm uye ss oe me e.W frue itsh fio ru stld .goto (cid:215)It ti os th ae ka ev ty hea mnd tn oo tt hc eo tn ov pen .ient GPT-3.5 neg "ative w(cid:215) ilB lbu eyin ag nf ir cu ei ,t ri es fa reg sr he ia nt gid se na a! cI kt (cid:215) totI at ki es tl hig eh mta tond thv eer toy pc .onvenient
positive tohaveonourwayup.
Table2: ExamplesfromourdatasetsandcorrespondingrewritesgeneratedbyGPT-3.5andGPT-NeoX,forallthree
tasks: formalitychange,de-toxification,andsentimenttransfer. @=documentcontext(truncated),(cid:15)=sentenceto
rewrite,(cid:15) =contextualrewrite,(cid:15) =non-contextualrewrite;(cid:220)=previousturninconversation,(cid:215)=responsetorewrite,(cid:215)=
contextualrewriteofresponse,(cid:215)=non-contextualrewriteofresponse,"Style"headerreferstothestyleconversion.
rewritingmodelthatutilizesthein-contextlearning we employ LLMs to rewrite an original sentence
capabilitiesofLLMs,inspiredbyapproachespre- from one style to another. Similar to contextual
sentedinReifetal.(2022)andRoyetal.(2023). rewriting, we manually construct few-shot exam-
We conduct few-shot prompting experiments plesthatsolelyconsistoftheoriginalsentenceto
with two LLMs: GPT-3.54 (Ouyang et al., 2022) berewritten,aninstructionalpromptspecifyingthe
andGPTNeoX5(Blacketal.,2022). Eachexample desiredstyle,andanexamplerewrite,withoutany
includestheprecedingcontext,theoriginalinput precedingcontext.
with a specified style, and the rewrite in another
style, factoring in the context. For GPT-3.5, we 4.3 RewritingwithaRandomContext
use2few-shotexamplestoobtainrewritesinthe
To demonstrate the importance of incorporating
desired format, while for GPT-NeoX, we use 10
contextualinformationintherewritingprocess,we
examples. SeeAppendixBformoredetails.
employabaselinemethodthatgeneratesrewrites
usingarandomcontext. Thisapproachservestwo
4.2 Non-contextualRewriting
keypurposes: first,itassessesthecontextualsensi-
Weareinterestedincomparingcontextualrewrites
tivityofautomaticmetrics;andsecond,itensures
withnon-contextualrewritesthatdonotdependon
thatourcontextualrewritingmethodeffectivelyac-
priorcontext. Togeneratenon-contextualrewrites,
counts for the given context. In our experiments,
4Weusetext-davinci-003 we randomly pick a context from our dataset in-
5Weusethe20Bparametermodel steadofusingthetrueprecedingcontext.
ytilamroF
noitacfiixoteD
tnemitneS
5 ContextualHumanEvaluation workersonAmazonMechanicalTurk(MTurk)and
qualified them using a pre-qualification test for
Sinceinrealisticrewritingscenarios,contextwill
eachtask(SeeAppCforqualificationdetails).
alwaysbeavailableandcrucialtouserswhowishto
rewritetheirdialogueutterancesorstorysentences Agreement Weemploythreeannotatorstorank
(Atwelletal.,2022),westartbyconductingacon- each pair of rewrites. Averaging across three
textualhumanevaluationtogaugeuserpreferences tasks,ourannotatoragreementwasKrippendorff’s
between non-contextual and contextual rewrites. α = 0.43 and Fleiss’s κ = 0.31. For dimension-
This contextual human evaluation is a departure specific annotator agreements, please refer to Ta-
frommostpreviousworkwhichhaspredominantly bles7—9inAppC.1. Weobtainthefinalhuman
notusedcontext(§2). judgmentpreferencesusingmajorityvotingofthe
threeannotators.
5.1 ExperimentalSetup
5.2 HumanEvaluationResults
We conduct a head-to-head human evaluation of
non-contextualandcontextualrewritesinthepres- Our results show that annotators prefer con-
ence of preceding textual context, following the textual rewrites over non-contextual rewrites
setupinKiritchenkoandMohammad(2017). Par- acrossallthreetasksandcontexttypes(Figure
ticipants are given preceding context, pairs of 3). Thiseffectisespeciallypronouncedforformal-
rewritten sentences (non-contextual and contex- ityandtoxicity(see(a)–(f)inTable2).
tual),andthedesiredstyleattribute. Theyarethen
Contextualrewritesaremorenaturalandfitting
askedtoranktherewriteswithrespectto:
Thesuccessrateforcontextualrewritesintoxicity
• Naturalness: which rewrite do the annotators andformalitycaseswasapproximately50%,while
prefer/whichoneappearsmostnatural thatfornon-contextualrewriteswascloseto20%
• StyleStrength: whichrewritebestachievesthe and30%,respectively(p < 0.1).9 Regardingsenti-
requiredstyle,independentofmeaningchanges ment,thesuccessrateforcontextualrewriteswas
around35%asopposedtonon-contextualrewrites
• Event-levelSimilarity: whichrewritemostef-
withasuccessrateofabout30%(p > 0.1).
fectivelyretainstheessentialevents,entities,and
relationspresentintheoriginalsentence,with- Contextualrewritesbetterpreservetheintended
outconsideringtheprecedingcontext meaning Contextualrewritesbetterpreservethe
author’sintention,tone,andimpliedmeaningmore
• IntendedMeaning: whichrewritemosteffec-
effectively (p < 0.1). In the detoxification task
tively preserves and conveys the original sen-
example(d)showninTable2,theuser’sintended
tence’soverallmessageorintendedmeaning
meaning is not about actually killing oneself but
• OverallFit: whichrewriteisoverallmostsuit-
ratheraboutavoidingthezombieapocalypse. The
ableorrelevantinrelationtothegivencontext
contextualrewritecapturesthismeaningmoreef-
We sample 100 examples for sentiment from fectively compared to the literal rephrasing pro-
DailyDialog,6 100 examples for formality,7 and videdbynon-contextualrewriting.
90examplesfortoxicity8,focusingonthosewith
Contextual rewrites struggle with preserving
thehigheststylestrengthineachcategory(e.g.,50
event-levelsimilarity Examples(a),(f),and(i)
most formal and 50 most informal). We conduct
inTable2demonstratethatcontextualrewritesof-
significancetestingforallthreetasks. Werecruited
ten include extra entity/event details, while non-
6WeoptednottouseYelpreviewsinoursamplingdueto contextual rewrites align more closely with the
difficultiesencounteredduringpilotexperiments.Annotators original sentence at an n-gram level.10 Despite
foundittoughtoselectrewritesthatretainedmeaningwhile
this,annotatorsstillprefercontextualrewritesfor
effectivelytransferringsentiment,suchasfrompositivetoneg-
ative.Generally,evencontextsclassifiedas“neutral”seemed theirnaturalnessandfit,indicatingthatextraevent
positivewhenpartofanoverallpositivereview,complicating
theannotators’abilitytoagreeontherewrites’effectiveness. 9p < 0.1, CI=90% using a binomial test and splitting
7equalnumberfrombothRedditandCNN/DailyMail+ the‘tie’optionevenlybetweencontextualandnon-contextual
BlogAuthorshipCorpus preferences.
8equalnumberofexamplesfromCCC,MDMDandProso- 10Event-level similarity is the only dimension which
cialDialog which were scored as highly toxic by all three showsnosignificantdifferencesbetweencontextualandnon-
toxicityclassifiers-hateroberta,hatebertandPerspectiveAPI contextualrewritesforallthreetasks.
Contextual Tie Non-Contextual Contextual Tie Non-Contextual Contextual Tie Non-Contextual
StyleStrength 0.46 0.26 0.29 StyleStrength 0.48 0.26 0.27 StyleStrength 0.45 0.16 0.39
EventSimilarity 0.41 0.13 0.46 EventSimilarity 0.36 0.30 0.34 EventSimilarity 0.38 0.17 0.45
IntendedMeaning 0.44 0.26 0.30 IntendedMeaning 0.51 0.23 0.26 IntendedMeaning 0.40 0.22 0.39
Naturalness 0.56 0.14 0.30 Naturalness 0.50 0.17 0.33 Naturalness 0.36 0.31 0.33
Fit 0.51 0.14 0.34 Fit 0.52 0.20 0.28 Fit 0.39 0.25 0.36
(a)Formality (b)Toxicity (c)Sentiment
Figure 3: Head-to-head human evaluation with context for all three tasks - formality change, detoxification,
andsentimenttransfer. Contextualrewritesaregenerallyfavoredovernon-contextualrewritesacrossalltasks,
particularlyintermsofstylestrength,preservationofintendedmeaning,naturalness,andoverallcoherencewiththe
precedingcontext. Thenumbersonthebarsrepresenttheproportionofpreferencesforeachrespectivecategory.
detailsareacceptableaslongastheyfitappropri- Lavie,2005)andworderrorrate(WER;Zechnerand
atelywithinthecontext. Waibel,2000),forlexicalsimilarity(Lexical).
Sentiment Style Transfer Might be Ill-defined SemanticSimilarity Tomeasuresemanticsimi-
Thetrendsinthesentimentstyletransfertaskare larity(Semantic),weuseBERTScore(Zhangetal.,
lesspronouncedthaninothertasks(p > 0.1forall 2019) and SBERT (Reimers and Gurevych, 2019),
dimensions)andshowloweragreementcompared as employed in previous work. We also consider
totoxicityandformality(seeTable9inAppC.1). Smatch(CaiandKnight, 2013), whichcompares
Example (g) in Table 2 highlights the challenges thesimilaritybetweentwoAbstractMeaningRep-
insentimenttransferduetotheinherentneedfor resentation(AMR)graphs,providingadistinctive,
meaningchangeswhilepreservingtheoriginalin- structuredviewonsemanticrelatednessnotconsid-
tent (especially for reviews which were written eredinpriorrewritingstudies.
specifically to communicate sentiment; Yu et al.,
Fluency Toassessfluency,weemployalanguage
2021). Thiscomplicationleadstoinconsistencies,
model, specifically GPT-2 (Radford et al., 2019),
resultinginannotatorshavingdifficultyreachinga
anduseperplexity(pplx)asthemetric,inlinewith
consensusonmeaningpreservation,asevidenced
previousresearch(Holtzmanetal.,2018;Xuetal.,
byloweragreementrates(Table9).
2018a;Maetal.,2020).
6 Non-contextualAutomaticEvaluation 6.2 Non-ContextualEvaluationResults
Overall,ourcontextualhumanevaluationsreveal In our analysis, we evaluate the performance
a general preference for contextual rewrites over of both GPT-3 and NeoX models in producing
non-contextual ones. Given that prior work pri- non-contextual rewrites, contextual rewrites, and
marily evaluated utterance-level rewrites in both rewrites generated with a random preceding con-
humanandautomaticevaluations,itraisestheques- text. We present aggregate results of the perfor-
tionofhowwellnon-contextualautomaticmetrics mance in Table 3 across all tasks, datasets, and
mirror human preferences. In this section, we in- metrics. For detailed results on individual tasks
vestigatecommonlyusedmetricsinpreviouswork anddatasets,wereferthereadertoAppendixD.
(Mir et al., 2019; Hu et al., 2022) for meaning
Non-contextual rewrites are more similar in
preservation,stylestrength,andfluency.
meaning to the original input sentence com-
pared to contextual rewrites Utterance level
6.1 MetricsConsidered
lexical and semantic meaning preservation met-
Wedistinguishtwotypesof“meaningpreservation” ricsscorenon-contextualrewriteshigher,acrossall
metrics, namely, lexical and semantic similarity three tasks and the two types of context (see Ta-
betweenarewriteX andtheoriginalinputI.
bles16–22inAppendixD).Additionally,wefind
that our patterns are consistent for both GPT-3.5
Style Strength Following previous studies (Li
and NeoX, though we note a marked decrease in
etal.,2018;Madaanetal.,2020),weassessstyle
performancefromGPT-NeoX.
strengthofrewrittentextbyexaminingtheproba-
Thissuggeststhatmodelsthatedittheoriginal
bilitiesofthetargetstylesunderourstyleclassifier.
sentence more (i.e., preserve lexical and seman-
Lexical Similarity We use word-overlap met- tic similarity less) are better at achieving the de-
ricslikeROUGE(Lin,2004),METEOR(Banerjeeand siredstyle. Forfluencymeasuredbyperplexity,we
Lexical Semantic Fluency
model rewrite type Style
ROU MET WER BERT-S SBERT Smatch PPL
contextual 0.18 0.38 1.66 0.90 0.59 0.45 40.25 0.74
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 18
6
0 0. .4 38
4
0 2. .8 08
4
0 0. .9 82
9
0 0. .7 50
0
0 0. .5 47
2
47.69 0.70
contextual 0.25 0.37 1.43 0.90 0.57 0.44 55.31 0.52
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .4 21
4
0 0. .5 36
7
0 1. .7 58
5
0 0. .9 93
0
0 0. .7 54
4
0 0. .6 40
3
64.65 0.44
Table3: Non-contextualAutomaticEvaluationResults: Non-contextualrewritesachievehigherscoresinlexical
andsemanticsimilaritymetricswhereascontextualrewritesdemonstrateenhancedstylestrengthandfluency. These
resultsareobtainedbyaveragingacrossalltasksanddatasets. Thisheatmapdisplaysthebest-performingrewritefor
eachspecificmetric–darkerorangeindicateshigherpreference. Formoredetailsonindividualtasksanddatasets
exhibitingsimilartrends,seeAppD.
findthatbothapproachesgeneratedecentlyfluent originalinputsentenceI beforecomparingittothe
rewritesregardlessofcontext,asexpected.11 rewriteX: sim(C +I,X). Theintuitionbehind
thisalterationisthattheprecedingtextualcontext
Non-contextual metrics do not correlate with
could capture more of the topical or semantic in-
human judgments We see in Figure 3 and Ta-
formation necessary to fully derive the speaker’s
ble 3, that the non-contextual automatic metrics
intendedmeaning.
paint an incomplete picture compared to human
evaluations. WecomputeSpearmanrankρcorrela- Contextual Lexical and Semantic Similarity
tionandKendall’sτ forthedatasetsamplesused For lexical similarity, we refer to these metrics
duringthecontextualhumanevaluation§5.1. Non- as ROUGECtx, METEORCtx and WERCtx. For seman-
contextual automatic metrics exhibit very weak, tic similarity, we refer to them as BERTScoreCtx,
non-significantcorrelationwithhumanjudgments SBERTCtx andSmatchCtx.
ofoverallfit(averagedacrossalltasks): ρ=0.09,
τ = 0.09 for lexical metrics (p > 0.05) and ρ = ContextualCoherenceandCohesiveness Inlin-
0.23, τ = 0.22 for semantic metrics (p > 0.05). guistics,coherenceandcohesivenessaretermstyp-
SeeAppendixD.1formetric-specificcorrelation icallyusedtodenotetheconnectednessembedded
scoresforoverallfitandnaturalnessdimensions. orimpliedinspokenorwrittendiscourse.
(a)Coherence: Coherenceisgenerallydefined
7 ContextualAutomaticEvaluation
astheoverallpicturepresentedbyallthesentences
in a piece of writing, similar to the way puzzle
Asshownintheprevioussection,non-contextual
piecesformtheimageonthebox(Williams,1990;
automaticmetrics,especiallyformeaningpreserva-
Zienkowski et al., 2011). This definition is often
tion,arenotsufficienttoevaluatetheperformance
operationalized by modeling the fit of a sentence
ofrewritingmodels. Toaddressthis,incorporating
given its preceding context, as demonstrated by
contextintotheevaluationprocessisnecessaryfor
prior work (See et al., 2019; Pang et al., 2020).
betterrepresentingrealisticdownstreamusecases.
Specifically,thisinvolvesmeasuringperplexityof
Drawing inspiration from reference-free metrics
therewriteconditionedonthecontextusingGPT-2
indialogevaluation(Yehetal.,2021;Zhaoetal.,
(Radfordetal.,2019).
2017),whichconsidersboththedialogcontextand
generatedresponsestoassessresponseswithinthe (b) Cohesiveness: Cohesiveness refers to the
dialoguehistory,weproposeincludingcontextinto semantic relationships between sentences, link-
existingautomaticevaluationmetricsandfurther ingcurrentelementswithprecedingorfollowing
introduceCtxSimFit,anewcontextualmetric. ones through lexical and structural means, much
like how two jigsaw puzzle pieces fit together
7.1 InfusingAutomaticMetricswithContext (Williams, 1990; Zienkowski et al., 2011). Fol-
Sincecontextiscrucialtoderiveintendedmeaning lowingpriorworkthatusedthisdefinition(Shiand
(Searle,1975),wealterexistingmeaningsimilar- Demberg, 2019; Abhishek et al., 2021; Nguyen,
ity measures by prepending the context C to the 2021), we measure cohesiveness using the prob-
abilitiesfromtheNextSentencePrediction(NSP)
11Lowerperplexitygenerallyindicateshighersentencequal-
head of BERT (Devlin et al., 2018), which mea-
ityandgrammaticality, butmaynotdirectlycorrelatewith
meaningpreservation,style,orcontentrelevance. sures if the rewrite follows and fits with its the
Lexical Semantic Coherence Cohesiveness Custom
model rewrite type
ROUᶜᵗˣ METᶜᵗˣ WERᶜᵗˣ BERT-Sᶜᵗˣ SBERTᶜᵗˣ Smatchᶜᵗˣ PPLᶜᵗˣ NSP CtxSimFit
contextual 0.15 0.24 0.89 0.88 0.59 0.35 28.70 0.95 0.93
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .1 16
0
0 0. .2 12
8
0 1. .8 08
6
0 0. .8 88
7
0 0. .4 39
9
0 0. .3 22
9
4 42 5. .9 74
3
0 0. .8 89
0
0 0. .9 81
5
contextual 0.19 0.22 0.87 0.88 0.53 0.31 31.10 0.93 0.92
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 11
3
0 0. .2 13
7
0 0. .8 96
8
0 0. .8 88
6
0 0. .4 38
9
0 0. .3 20
6
4 59 2. .8 91
3
0 0. .9 80
3
0 0. .9 81
6
Table4: ContextualAutomaticEvaluationResults: Onaverage,acrossalltasksanddatasets,contextualrewrites
achievehigherscoresthannon-contextualrewriteswhenevaluatedusingcontext-infusedautomaticmetricsandour
CtxSimFitmetric. Thisheatmapshowsthebest-performingrewriteforaparticularmetric–darkergreenindicates
higherpreference. Formoredetailsonindividualtasksanddatasetsdisplayingsimilartrends,seeAppE.
precedingcontext. Contextualmetricscorrelatesignificantlywith
human judgments We find that contextual au-
7.2 NovelCompositeMetric: CtxSimFit
tomatic metrics correlate significantly with hu-
WeintroduceCtxSimFit,asimplemetricthatcom- man judgments of ‘overall fit’ (averaged across
bines contextual cohesiveness and semantic sim- all tasks): ρ = 0.6,τ = 0.58 for lexical metrics
ilarity to assess the overall quality of a rewrite. (p < 0.05) and ρ = 0.56,τ = 0.57 for semantic
CtxSimFitcomputestheweightedaverageofboth metrics(p < 0.05). SeeAppendixE.1formetric-
theBERTScorebetweentheoriginalandrewritten wisecorrelationscoresforbothoverallfitandnat-
sentences,andtheprobabilitiesfromtheBERT’s uralnesshumanjudgmentdimensions.
NSP head between the preceding context and the
rewrite,thusdetermininghowwelltherewritefits CtxSimFit correlates the best with human
theprecedingcontextandmaintainssemanticsimi- judgements Compared to contextual versions
larity. of existing metrics, CtxSimFit correlates very
strongly with human judgements of ‘overall fit’
CtxSimFit = α∗BERTSCORE(S,X) (averaged across all tasks): ρ = 0.85,τ = 0.82
+(1−α)∗NSP(C,X) (p < 0.01). Weseesimilartrendsfor‘naturalness’:
ρ = 0.85,τ = 0.81(p < 0.01). Thissuggeststhat
where α is a hyperparameter that provides users
combiningmeaningpreservationandcontextualco-
with control over their preference for balancing
hesivenessintoacompositemeasurebettermirrors
meaning preservation and contextual fit. Unless
humanpreferencesthanindividualmetricsalone.
specifiedotherwise,wesetα = 0.5.
7.4 SensitivityanalysisforαinCtxSimFit
Contextualrewritesarescoredhigheronstyle
strengthcomparedtonon-contextualrewrites
In our experiments, we set α = 0.5 to equally
weightcontextualcohesivenessandsemanticsim-
7.3 ContextualEvaluationResults
ilarity. We further examine the impact of α in
Similar to §6.2, we aggregate the results of both
CtxSimFit,asdetailedbyTable5.
GPT-3.5 and NeoX across all tasks, datasets and
OurCtxSimFitsignificantlycorrelateswithhu-
metrics (see Table 4). For detailed results on in-
manjudgmentsof‘overallfit’forαvalueswithin
dividualtasksanddatasets,wereferthereaderto
therangeof0.2–0.6, withcorrelationandsignifi-
Tables23–29inAppendixE.
cancediminishingoutsidethisrange. Thehighest
Contextualrewritesarepreferredbynearlyall alignment with human judgments is achieved at
ofourcontextualautomaticmetricscompared α = 0.5. Thelongerrangeof0.2–0.5forα < 0.5
to non-contextual rewrites These results mir- highlightstheeffectandimportanceofcontextual
ror human preferences on naturalness, fit and in- cohesivenessinstylistictextrewriting.
tended meaning preservation. As a reality check, While a balanced approach (α = 0.5) offers
contextualrewriteswithrandomcontextsperform thestrongestalignmentwithhumanjudgmentsfor
theworstacrossallmetrics,indicatingthatcontex- formality,sentimentandde-toxificationtasks,the
tualmodelsareindeedtakingcontextintoaccount. degreeofemphasisoncontextualcohesivenessand
Furtherasexpected,contextualrewritesalsohave semantic similarity should be adjusted based on
bettercoherencecomparedtonon-contextualones. specifictasksandusers’priorities.
Hyperparameterα Task Correlationρ Significance entitiesandbettervocabularyusage(examples(a),
inCtxSimFit with‘overallfit’
(c)), retaining relevant details from context for a
Formality -0.03 ns
0.1 Toxicity -.05 ns better flow (examples (b), (i)) and preserving the
Sentiment -0.04 ns intendedmeanings(examples(d),(g)).
Formality 0.66 **
0.2 Toxicity 0.65 **
Existing meaning preservation metrics do not
Sentiment 0.54 **
Formality 0.75 *** alignwithhumanpreferencesforformality,sen-
0.3 Toxicity 0.75 *** timent and toxicity transfer tasks Next, we
Sentiment 0.67 ***
demonstrate that common non-contextual auto-
Formality 0.71 ***
0.4 Toxicity 0.67 *** matic metrics for lexical and semantic similarity,
Sentiment 0.60 *** i.e.,oftenusedasproxiesformeaningpreservation
Formality 0.88 *** inpriorwork(Lietal.,2018;Sudhakaretal.,2019;
0.5 Toxicity 0.82 ***
Mir et al., 2019; Reif et al., 2022; Madaan et al.,
Sentiment 0.73 ***
Formality 0.57 *** 2020; Wang et al., 2019; Reid and Zhong, 2021;
0.6 Toxicity 0.53 *** Roy et al., 2023), do not align with human pref-
Sentiment 0.42 ***
erences concerning naturalness, fit, and intended
Formality 0.32 *
0.7 Toxicity 0.38 ** meaning. Since the overarching meaning of a
Sentiment 0.20 ns sentence largely depends on its context (Searle,
Formality 0.24 * 1975;Clark,1997,1996),non-contextualproxies
0.8 Toxicity 0.34 *
Sentiment 0.17 ns formeaningpreservationwillalwaysbeintension
Formality 0.25 ns withanystylisticchangetothesentence,making
0.9 Toxicity 0.28 * thetrade-offhardtonavigate(Miretal.,2019;Hu
Sentiment 0.20 ns
etal.,2022). Therefore,weadvocatefordiscontin-
Table 5: Sensitivity of the α in CtxSimFit across all uingnon-contextualmeaningpreservationmetrics
tasks. ρindicatescorrelationofCtxSimFitwithhuman in stylistic rewriting tasks and for more research
judgmentsof‘overallfit’. nsindicatesnotsignificant(p intobettermodelingofcommunicativeintentsor
>0.05),*isp<0.05,**isp<0.01,***p<0.001
goals(Adolphsetal.,2022;Zhouetal.,2022).
Contextual automatic metrics, especially
8 Summary&DiscussionofFindings
CtxSimFit, better mirror human judgments
Existing work on stylistic text rewriting has of- In our work, we attempt to bridge the gap
ten neglected the surrounding context of the sen- between non-contextual metrics and contextual
tence. Inourstudy,wefocusonincorporatingthe human evaluations by integrating context into
preceding textual context in documents and con- automatedmetrics(§7). Ourproposedcomposite
versations into both the modeling and evaluation metric, CtxSimFit, balances meaning preserva-
stages of rewriting. We develop a contextual hu- tion with contextual cohesiveness, providing a
manevaluationframeworkandcompareitsresults more comprehensive measure that better aligns
to non-contextual automatic metrics, contextual- with human judgments. While commonly-used
ized versions of these metrics, as well as to our automatic metrics enriched with context align
newcompositemetricCtxSimFit. withhumanpreferences,ourproposedCtxSimFit
demonstratesastrongercorrelation.
Contextiscrucialforrewriting Corroborating Initialworkinevaluatingopen-domaindialogue
findings by Cheng et al. (2020) and Roy et al. generationwithcontext(Wellecketal.,2019;Pang
(2023), contextual rewrites are significantly pre- etal.,2020)hasbeendone,butweencouragefur-
ferredbyhumanannotatorsintermsofnaturalness, therdevelopmentofbettercontextualizedmetrics
intendedmeaningpreservation,andstylestrength. for stylistic rewriting evaluation. Improvements
Additionally,wedemonstratethathavingtheright could include modeling themes, tones, sentence
context is crucial for contextual rewriting, as ev- structures(Zhangetal.,2014;Khatrietal.,2018;
idenced by the poor performance of contextual Chen and Yang, 2020; Toubia et al., 2021; Shen
rewritesgeneratedusingarandomcontext. etal.,2023),andsocialdynamics,andemotional
Qualitative examination (Table 2) shows that statesinconversations(Sapetal.,2017;Rashkin
contextual rewrites are better at disambiguating etal.,2018,2019;Mostafazadehetal.,2020).
9 Limitations&EthicalConsiderations Parthasarathi et al., 2021; Su et al., 2023). More-
over, custom-made rewriting models from prior
Despitetakingthefirststeptowardsincorporating
research often lack the modeling of context (Ma
context into stylistic rewriting and its evaluation
etal.,2020;Daleetal.,2021). Webelievetheour
frameworks,thereareseverallimitationsandethi-
resultsstillapplyforsmallermodels,givensome
calconcerns,whichwelistbelow.
preliminary research (Cheng et al., 2020; Atwell
etal.,2022)onanincreasedhumanpreferencefor
Limited Context Scope In this study, our pri-
contextual rewrites from custom-trained seq2seq
mary focus is on incorporating textual context,
models. Weencouragefutureworktothoroughly
particularlyfromprecedingsentencesorprevious
investigate strategies for effective modeling and
turns in a conversation. Future work should ex-
evaluationofcontextinsmallermodels.
plore how to incorporate other forms of context
intorewritingmodelsandevaluations,suchasdis- Harms of Exposing Workers to Toxic Content
course structure (Welleck et al., 2019), external Inourwork,weexposedhumanannotatorstotoxic
knowledge(Ghazvininejadetal.,2018),orricher contentduringtheevaluationofthede-toxification
socialandpowerdynamics(Antoniaketal.,2023), task. Exposure to such offensive content can be
emotionalstates(Zhouetal.,2023),andcommu- harmfultotheannotators(Liuetal.,2016). Weaim
nicativeintent(Zhouetal.,2022),allofwhichcan to work towards developing evaluation strategies
significantlycontributetounderstandingthetext. that can minimize the exposure of annotators to
toxiccontent.
Amount of Context In our experiments, we
opted to investigate the context of three preced- Potentially Inconsistent Human Evaluations
ing sentences in a document and one preceding In our work, we also assume human judgments
conversational turn, considering only a specific asthegoldstandard. Concurrentworkhasshown
length. However,theamountofcontextatthemod- that human evaluation might not always be con-
elingandevaluationstagescouldalsochangethe sistent(Clarketal.,2021;Karpinskaetal.,2021);
results. We hypothesize that more context could howeverhumanjudgmentscontinuetobethegold
improverewritingmethods,butitcouldpotentially standardforevaluatingopen-endedtextgeneration.
alsonegativelyimpactcontextualmeaningpreser-
vationmetrics. Futureworkshouldexplorethese Acknowledgements
effectsofvaryinglengthsofcontext.
We would like to thank our workers on MTurk
Broad Definition of Meaning Preservation for their responses. We are also grateful to the
While we have tried to define meaning preserva- anonymousreviewersfortheirhelpfulcomments.
tionasthepreservationofaneventorentity-level Special thanks to Saadia Gabriel, Jocelyn Shen,
details and intended overall meaning, this defini- Ashutosh Baheti, and the members of the CMU
tion remains broad and subjective (Searle, 1975; LTICOMEDYgroupfortheirfeedback,andOpe-
Adolphs et al., 2022; Zhou et al., 2022). In this nAIforprovidingaccesstotheGPT-3.5API.This
work, we do not delve into more intricate dimen- research was supported in part by the Meta Fun-
sionsofmeaningpreservation,suchasspatialand damentalAIResearchLaboratories(FAIR)“Dyn-
temporalaccuracy,ortheretentionofculturalcon- abench Data Collection and Benchmarking Plat-
text,includingreferences,nuances,anddialects. form”award“ContExTox: Context-AwareandEx-
plainableToxicityDetection.”
ApplicabilitytoSmallerModels Ourworkre-
liesonfew-shotpromptingofLLMstoincorporate
textual context, given their demonstrated strong References
rewritingcapabilitiesbothwithandwithouttextual
Tushar Abhishek, Daksh Rawat, Manish Gupta, and
context usage (Brown et al., 2020). Other exist- Vasudeva Varma. 2021. Transformer models
inggenerativemodels,suchasthoseusedforchit- for text coherence assessment. arXiv preprint
chat and goal-oriented conversational agents, as
arXiv:2109.02176.
wellaspretrainedlanguagemodels,havestruggled
LeonardAdolphs,KurtShuster,JackUrbanek,Arthur
witheffectivelyutilizingprecedingtextualcontext
Szlam,andJasonWeston.2022. Reasonfirst,then
(Sankaretal.,2019;O’ConnorandAndreas,2021; respond: Modulargenerationforknowledge-infused
dialogue. In Findings of the Association for Com- JiaaoChenandDiyiYang.2020. Multi-viewsequence-
putationalLinguistics: EMNLP2022,pages7112– to-sequencemodelswithconversationalstructurefor
7132. abstractivedialoguesummarization. InProceedings
of the 2020 Conference on Empirical Methods in
Maria Antoniak, Anjalie Field, Ji Min Mun, Melanie NaturalLanguageProcessing(EMNLP),pages4106–
Walsh,LaurenF.Klein,andMaartenSap.2023. Riv- 4118.
eter: Measuringpowerandsocialdynamicsbetween
entities. InACLdemonstrations. YuCheng,ZheGan,YizheZhang,OussamaElachqar,
DianqiLi,andJingjingLiu.2020. Contextualtext
KatherineAtwell,SabitHassan,andMaliheAlikhani.
styletransfer. InFindingsoftheAssociationforCom-
2022. Appdia: Adiscourse-awaretransformer-based
putationalLinguistics: EMNLP2020,pages2915–
styletransfermodelforoffensivesocialmediacon-
2924.
versations. InProceedingsofthe29thInternational
Conference on Computational Linguistics, pages Elizabeth Clark, Tal August, Sofia Serrano, Nikita
6063–6074. Haduong, Suchin Gururangan, and Noah A Smith.
2021. Allthat’s‘human’isnotgold: Evaluatinghu-
SatanjeevBanerjeeandAlonLavie.2005. Meteor: An
man evaluation of generated text. In Proceedings
automaticmetricformtevaluationwithimprovedcor-
of the 59th Annual Meeting of the Association for
relationwithhumanjudgments. InProceedingsof
ComputationalLinguisticsandthe11thInternational
theaclworkshoponintrinsicandextrinsicevaluation
JointConferenceonNaturalLanguageProcessing
measuresformachinetranslationand/orsummariza-
(Volume1: LongPapers),pages7282–7296.
tion,pages65–72.
Elizabeth Clark, Anne Spencer Ross, Chenhao Tan,
Francesco Barbieri, Luis Espinosa Anke, and Jose
Yangfeng Ji, and Noah A Smith. 2018. Creative
Camacho-Collados.2022. Xlm-t: Multilinguallan-
writingwithamachineintheloop: Casestudieson
guagemodelsintwitterforsentimentanalysisand
slogansandstories. In23rdInternationalConference
beyond. InProceedingsoftheThirteenthLanguage
onIntelligentUserInterfaces,pages329–340.
Resources and Evaluation Conference, pages 258–
266. HerbertHClark.1996. Usinglanguage. Cambridge
universitypress.
SidneyBlack,StellaBiderman,EricHallahan,Quentin
Anthony, Leo Gao, Laurence Golding, Horace He,
HerbertHClark.1997. Dogmasofunderstanding. Dis-
ConnorLeahy,KyleMcDonell,JasonPhang,etal.
courseProcesses,23(3):567–598.
2022. Gpt-neox-20b: An open-source autoregres-
sivelanguagemodel. InProceedingsofBigScience DavidDale,AntonVoronov,DarynaDementieva,Var-
Episode\# 5–Workshop on Challenges & Perspec- varaLogacheva,OlgaKozlova,NikitaSemenov,and
tivesinCreatingLargeLanguageModels,pages95– AlexanderPanchenko.2021. Textdetoxificationus-
136. inglargepre-trainedneuralmodels. InProceedings
ofthe2021ConferenceonEmpiricalMethodsinNat-
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
uralLanguageProcessing,pages7979–7996,Online
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
andPuntaCana,DominicanRepublic.Association
Neelakantan,PranavShyam,GirishSastry,Amanda
forComputationalLinguistics.
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens KristinaToutanova.2018. Bert: Pre-trainingofdeep
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma- bidirectionaltransformersforlanguageunderstand-
teusz Litwin, Scott Gray, Benjamin Chess, Jack ing. arXivpreprintarXiv:1810.04805.
Clark, ChristopherBerner, SamMcCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020. ZhenxinFu,XiaoyeTan,NanyunPeng,DongyanZhao,
Language models are few-shot learners. In Ad- and Rui Yan. 2018. Style transfer in text: Explo-
vances in Neural Information Processing Systems, ration and evaluation. In Proceedings of the AAAI
volume 33, pages 1877–1901. Curran Associates, ConferenceonArtificialIntelligence,volume32.
Inc.
Marjan Ghazvininejad, Chris Brockett, Ming-Wei
ShuCaiandKevinKnight.2013. Smatch:anevaluation Chang,BillDolan,JianfengGao,Wen-tauYih,and
metricforsemanticfeaturestructures. InProceed- MichelGalley.2018. Aknowledge-groundedneu-
ingsofthe51stAnnualMeetingoftheAssociationfor ralconversationmodel. InProceedingsoftheAAAI
ComputationalLinguistics(Volume2: ShortPapers), ConferenceonArtificialIntelligence,volume32.
pages748–752.
SkylerHallinan,AlisaLiu,YejinChoi,andMaartenSap.
KunalChawlaandDiyiYang.2020. Semi-supervised 2023. Detoxifying text with marco: Controllable
formality style transfer using language model dis- revisionwithexpertsandanti-experts. InACL.
criminatorandmutualinformationmaximization. In
FindingsoftheAssociationforComputationalLin- Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,
guistics: EMNLP 2020, pages 2340–2354, Online. MaartenSap,DipankarRay,andEceKamar.2022.
AssociationforComputationalLinguistics. ToxiGen: Alarge-scalemachine-generateddataset
for adversarial and implicit hate speech detection. Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
In Proceedings of the 60th Annual Meeting of the Cao,andShuziNiu.2017. Dailydialog: Amanually
AssociationofComputationalLinguistics. labelledmulti-turndialoguedataset. InProceedings
oftheEighthInternationalJointConferenceonNat-
Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine uralLanguageProcessing(Volume1: LongPapers),
Bosselut,DavidGolub,andYejinChoi.2018. Learn- pages986–995.
ingtowritewithcooperativediscriminators. InPro-
ceedingsofthe56thAnnualMeetingoftheAssocia- Chin-YewLin.2004. Rouge: Apackageforautomatic
tionforComputationalLinguistics(Volume1: Long evaluation of summaries. In Text summarization
Papers),pages1638–1649. branchesout,pages74–81.
ZhiqiangHu,RoyKa-WeiLee,CharuCAggarwal,and
Chia-WeiLiu,RyanLowe,IulianVladSerban,Mike
Aston Zhang. 2022. Text style transfer: A review
Noseworthy, Laurent Charlin, and Joelle Pineau.
andexperimentalevaluation. ACMSIGKDDExplo-
2016. How not to evaluate your dialogue system:
rationsNewsletter,24(1):14–45.
Anempiricalstudyofunsupervisedevaluationmet-
ricsfordialogueresponsegeneration. InProceedings
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
ofthe2016ConferenceonEmpiricalMethodsinNat-
Salakhutdinov,andEricPXing.2017. Towardcon-
uralLanguageProcessing,pages2122–2132.
trolledgenerationoftext. InInternationalconference
onmachinelearning,pages1587–1596.PMLR.
Varvara Logacheva, Daryna Dementieva, Sergey
Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, Ustyantsev, Daniil Moskovskiy, David Dale, Irina
and Rada Mihalcea. 2022. Deep learning for text Krotova,NikitaSemenov,andAlexanderPanchenko.
styletransfer: Asurvey. ComputationalLinguistics, 2022. Paradetox: Detoxificationwithparalleldata.
48(1):155–205. In Proceedings of the 60th Annual Meeting of the
AssociationforComputationalLinguistics(Volume
Marzena Karpinska, Nader Akoury, and Mohit Iyyer. 1: LongPapers),pages6804–6818.
2021. Theperilsofusingmechanicalturktoevaluate
open-ended text generation. In Proceedings of the XinyaoMa,MaartenSap,HannahRashkin,andYejin
2021ConferenceonEmpiricalMethodsinNatural Choi.2020. Powertransformer: Unsupervisedcon-
LanguageProcessing,pages1265–1285. trollablerevisionforbiasedlanguagecorrection. In
EMNLP.
Chandra Khatri, Rahul Goel, Behnam Hedayatnia,
Angeliki Metanillou, Anushree Venkatesh, Raefer CharlesAMacArthur.2009. Reflectionsonresearchon
Gabriel, and Arindam Mandal. 2018. Contextual writingandtechnologyforstrugglingwriters. Learn-
topic modeling for dialog systems. In 2018 ieee ingDisabilitiesResearch&Practice,24(2):93–103.
spoken language technology workshop (slt), pages
892–899.IEEE. AmanMadaan, AmrithSetlur, TanmayParekh, Barn-
abásPoczós,GrahamNeubig,YimingYang,Ruslan
Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing
Salakhutdinov,AlanWBlack,andShrimaiPrabhu-
Lu,DanielKhashabi,GunheeKim,YejinChoi,and
moye.2020. Politenesstransfer: Atagandgenerate
Maarten Sap. 2022. Prosocialdialog: A prosocial
approach. InProceedingsofthe58thAnnualMeet-
backboneforconversationalagents. InProceedings
ingoftheAssociationforComputationalLinguistics,
of the 2022 Conference on Empirical Methods in
pages1869–1881.
NaturalLanguageProcessing,pages4005–4029.
Remi Mir, Bjarke Felbo, Nick Obradovich, and Iyad
SvetlanaKiritchenkoandSaifMohammad.2017. Best-
Rahwan.2019. Evaluatingstyletransferfortext. In
worstscalingmorereliablethanratingscales: Acase
Proceedings of the 2019 Conference of the North
studyonsentimentintensityannotation. InProceed-
AmericanChapteroftheAssociationforComputa-
ingsofthe55thAnnualMeetingoftheAssociationfor
tionalLinguistics: HumanLanguageTechnologies,
ComputationalLinguistics(Volume2: ShortPapers),
Volume1(LongandShortPapers),pages495–504.
pages465–470.
GuillaumeLample,SandeepSubramanian,EricSmith, Nasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon,
LudovicDenoyer,Marc’AurelioRanzato,andY-Lan DavidBuchanan,LaurenBerkowitz,OrBiran,and
Boureau.2019. Multiple-attributetextrewriting. In JenniferChu-Carroll.2020. Glucose: Generalized
International Conference on Learning Representa- andcontextualizedstoryexplanations. InProceed-
tions. ingsofthe2020ConferenceonEmpiricalMethods
in Natural Language Processing (EMNLP), pages
Juncen Li, Robin Jia, He He, and Percy Liang. 2018. 4569–4586.
Delete,retrieve,generate: asimpleapproachtosenti-
mentandstyletransfer. InProceedingsofthe2018 Sourabrata Mukherjee, Zdeneˇk Kasner, and Ondˇrej
Conference of the North American Chapter of the Dušek.2022. Balancingthestyle-contenttrade-offin
AssociationforComputationalLinguistics: Human sentimenttransferusingpolarity-awaredenoising. In
Language Technologies, Volume 1 (Long Papers), Text,Speech,andDialogue,pages172–186,Cham.
pages1865–1874. SpringerInternationalPublishing.
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, HannahRashkin,EricMichaelSmith,MargaretLi,and
ÇaglarGulçehre,andBingXiang.2016. Abstractive Y-Lan Boureau. 2019. Towards empathetic open-
textsummarizationusingsequence-to-sequencernns domainconversationmodels: Anewbenchmarkand
andbeyond. CoNLL2016,page280. dataset. In Proceedings of the 57th Annual Meet-
ingoftheAssociationforComputationalLinguistics,
An Nguyen. 2021. Language model evaluation pages5370–5381.
in open-ended text generation. arXiv preprint
arXiv:2108.03578. MachelReidandVictorZhong.2021. Lewis: Leven-
shteineditingforunsupervisedtextstyletransfer. In
Cicero Nogueira dos Santos, Igor Melnyk, and Inkit FindingsoftheAssociationforComputationalLin-
Padhi.2018. Fightingoffensivelanguageonsocial guistics: ACL-IJCNLP2021,pages3932–3944.
mediawithunsupervisedtextstyletransfer. InPro-
EmilyReif,DaphneIppolito,AnnYuan,AndyCoenen,
ceedingsofthe56thAnnualMeetingoftheAssocia-
ChrisCallison-Burch,andJasonWei.2022. Arecipe
tionforComputationalLinguistics(Volume2: Short
forarbitrarytextstyletransferwithlargelanguage
Papers),pages189–194,Melbourne,Australia.As-
models. In Proceedings of the 60th Annual Meet-
sociationforComputationalLinguistics.
ingoftheAssociationforComputationalLinguistics
(Volume2: ShortPapers),pages837–848.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
CarrollWainwright,PamelaMishkin,ChongZhang,
NilsReimersandIrynaGurevych.2019. Sentence-bert:
SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
Sentenceembeddingsusingsiamesebert-networks.
2022. Training languagemodelsto followinstruc-
InProceedingsofthe2019ConferenceonEmpirical
tions with human feedback. Advances in Neural
MethodsinNaturalLanguageProcessingandthe9th
InformationProcessingSystems,35:27730–27744.
InternationalJointConferenceonNaturalLanguage
Processing(EMNLP-IJCNLP),pages3982–3992.
JoeO’ConnorandJacobAndreas.2021. Whatcontext
featurescantransformerlanguagemodelsuse? In
Shamik Roy, Raphael Shu, Nikolaos Pappas, Elman
Proceedingsofthe59thAnnualMeetingoftheAsso-
Mansimov,YiZhang,SaabMansour,andDanRoth.
ciationforComputationalLinguisticsandthe11th
2023. Conversation style transfer using few-shot
InternationalJointConferenceonNaturalLanguage
learning. arXivpreprintarXiv:2302.08362.
Processing(Volume1: LongPapers),pages851–864.
ChinnadhuraiSankar,SandeepSubramanian,Christo-
BoPang,ErikNijkamp,WenjuanHan,LinqiZhou,Yix- pherPal,SarathChandar,andYoshuaBengio.2019.
ianLiu,andKeweiTu.2020. Towardsholisticand Do neuraldialog systemsuse the conversationhis-
automaticevaluationofopen-domaindialoguegener- toryeffectively? anempiricalstudy. InProceedings
ation. InProceedingsofthe58thAnnualMeetingof of the 57th Annual Meeting of the Association for
theAssociationforComputationalLinguistics,pages ComputationalLinguistics,pages32–37.
3619–3629.
MaartenSap,MarcellaCindyPrasetio,AriHoltzman,
PrasannaParthasarathi,JoellePineau,andSarathChan- HannahRashkin,andYejinChoi.2017. Connotation
dar.2021. Doencoderrepresentationsofgenerative frames of power and agency in modern films. In
dialoguemodelshavesufficientsummaryofthein- EMNLP.
formation about the task? In Proceedings of the
22ndAnnualMeetingoftheSpecialInterestGroup JonathanSchler,MosheKoppel,ShlomoArgamon,and
onDiscourseandDialogue,pages477–488. JamesWPennebaker.2006. Effectsofageandgen-
deronblogging. InAAAIspringsymposium:Compu-
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
tationalapproachestoanalyzingweblogs,volume6,
DarioAmodei,IlyaSutskever,etal.2019. Language pages199–205.
modelsareunsupervisedmultitasklearners. OpenAI
JohnRSearle.1975. Ataxonomyofillocutionaryacts.
blog,1(8):9.
Abigail See, Aneesh Pappu, Rohun Saxena, Akhila
SudhaRaoandJoelTetreault.2018. Dearsirormadam,
Yerukola, and Christopher D Manning. 2019. Do
mayIintroducetheGYAFCdataset: Corpus,bench-
massivelypretrainedlanguagemodelsmakebetter
marks and metrics for formality style transfer. In
storytellers? In Proceedings of the 23rd Confer-
Proceedings of the 2018 Conference of the North
enceonComputationalNaturalLanguageLearning
AmericanChapteroftheAssociationforComputa-
(CoNLL),pages843–861.
tionalLinguistics: HumanLanguageTechnologies,
Volume 1 (Long Papers), pages 129–140, New Or- Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez,
leans,Louisiana.AssociationforComputationalLin- HaeWonPark,andCynthiaBreazeal.2023. Model-
guistics. ingempathicsimilarityinpersonalnarratives.
HannahRashkin,AntoineBosselut,MaartenSap,Kevin TianxiaoShen,TaoLei,ReginaBarzilay,andTommi
Knight,andYejinChoi.2018. Modelingnaivepsy- Jaakkola.2017. Styletransferfromnon-paralleltext
chologyofcharactersinsimplecommonsensestories. bycross-alignment. Advancesinneuralinformation
InACL. processingsystems,30.
WeiShiandVeraDemberg.2019. Nextsentencepre- Unpairedsentiment-to-sentimenttranslation: Acy-
dictionhelpsimplicitdiscourserelationclassification cledreinforcementlearningapproach. InProceed-
within and across domains. In Proceedings of the ingsofthe56thAnnualMeetingoftheAssociationfor
2019 conference on empirical methods in natural ComputationalLinguistics(Volume1: LongPapers),
languageprocessingandthe9thinternationaljoint pages979–988,Melbourne,Australia.Association
conferenceonnaturallanguageprocessing(EMNLP- forComputationalLinguistics.
IJCNLP),pages5790–5796.
WeiXu,AlanRitter,BillDolan,RalphGrishman,and
HsuanSu,ShachiHKumar,SahisnuMazumder,Wenda ColinCherry.2012. Paraphrasingforstyle. InPro-
Chen,RameshManuvinakurike,EdaOkur,Saurav ceedingsofCOLING2012,pages2899–2914,Mum-
Sahay,LamaNachman,Shang-TseChen,andHung- bai,India.TheCOLING2012OrganizingCommit-
yiLee.2023. Positionmatters! empiricalstudyof tee.
ordereffectinknowledge-groundeddialogue. arXiv
Yi-TingYeh,MaxineEskenazi,andShikibMehri.2021.
preprintarXiv:2302.05888.
A comprehensive assessment of dialog evaluation
metrics. InTheFirstWorkshoponEvaluationsand
AkhileshSudhakar,BhargavUpadhyay,andArjunMa-
AssessmentsofNeuralConversationSystems,pages
heswaran.2019. “transforming”delete,retrieve,gen-
15–33.
erateapproachforcontrolledtextstyletransfer. In
Proceedings of the 2019 Conference on Empirical
AkhilaYerukola,MasonBretan,andHongxiaJin.2021.
MethodsinNaturalLanguageProcessingandthe9th
Data augmentation for voice-assistant NLU using
InternationalJointConferenceonNaturalLanguage
BERT-basedinterchangeablerephrase. InProceed-
Processing(EMNLP-IJCNLP),pages3269–3279.
ingsofthe16thConferenceoftheEuropeanChap-
teroftheAssociationforComputationalLinguistics:
ValerioBasile TommasoCaselli and MichaelGranitzer
MainVolume.
JelenaMitrovic.2021. Hatebert: Retrainingbertfor
abusivelanguagedetectioninenglish. WOAH2021, PingYu,YangZhao,ChunyuanLi,andChangyouChen.
page17. 2021. Rethinkingsentimentstyletransfer. InFind-
ingsoftheAssociationforComputationalLinguistics:
OlivierToubia,JonahBerger,andJehoshuaEliashberg. EMNLP2021,pages1569–1582.
2021. Howquantifyingtheshapeofstoriespredicts
theirsuccess. ProceedingsoftheNationalAcademy Klaus Zechner and Alex Waibel. 2000. Minimizing
ofSciencesoftheUnitedStatesofAmerica,118(26). worderrorrateintextualsummariesofspokenlan-
guage. In1stMeetingoftheNorthAmericanChapter
YunliWang,YuWu,LiliMou,ZhoujunLi,andWenhan oftheAssociationforComputationalLinguistics.
Chao.2019. Harnessingpre-trainedneuralnetworks
KaiZhang, WeiWu, HaochengWu,ZhoujunLi, and
withrulesforformalitystyletransfer. InProceedings
MingZhou.2014. Questionretrievalwithhighqual-
ofthe2019ConferenceonEmpiricalMethodsinNat-
ity answers in community question answering. In
uralLanguageProcessingandthe9thInternational
Proceedingsofthe23rdACMinternationalconfer-
JointConferenceonNaturalLanguageProcessing
enceonconferenceoninformationandknowledge
(EMNLP-IJCNLP),pages3573–3578.
management,pages371–380.
Sean Welleck, Jason Weston, Arthur Szlam, and
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
KyunghyunCho.2019. Dialoguenaturallanguage
Weinberger,andYoavArtzi.2019. Bertscore: Eval-
inference. InProceedingsofthe57thAnnualMeet-
uating text generation with bert. arXiv preprint
ingoftheAssociationforComputationalLinguistics,
arXiv:1904.09675.
pages3731–3741.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Joseph Williams. 1990. Toward clarity and grace.
Character-levelconvolutionalnetworksfortextclassi-
Chicago: TheUniversityofChicago.
fication. Advancesinneuralinformationprocessing
systems,28.
AlexandrosXenos,JohnPavlopoulos,andIonAndrout-
sopoulos. 2021. Context sensitivity estimation in Yangjun Zhang, Pengjie Ren, Wentao Deng, Zhumin
toxicitydetection. InProceedingsofthe5thWork- Chen, and Maarten Rijke. 2022. Improving multi-
shop on Online Abuse and Harms (WOAH 2021), label malevolence detection in dialogues through
pages140–145. multi-facetedlabelcorrelationenhancement. InPro-
ceedingsofthe60thAnnualMeetingoftheAssocia-
JingjingXu,XuanchengRen,JunyangLin,andXuSun. tionforComputationalLinguistics(Volume1: Long
2018a. Diversity-promoting gan: A cross-entropy Papers),pages3543–3555.
basedgenerativeadversarialnetworkfordiversified
text generation. In Proceedings of the 2018 con- TianchengZhao,RanZhao,andMaxineEskenazi.2017.
ference on empirical methods in natural language Learningdiscourse-leveldiversityforneuraldialog
processing,pages3940–3949. models using conditional variational autoencoders.
In Proceedings of the 55th Annual Meeting of the
JingjingXu,XuSun,QiZeng,XiaodongZhang,Xu- AssociationforComputationalLinguistics(Volume
anchengRen,HoufengWang,andWenjieLi.2018b. 1: LongPapers),pages654–664.
PeiZhou,HyundongCho,PegahJandaghi,Dong-Ho
Lee, Bill Yuchen Lin, Jay Pujara, and Xiang Ren.
2022. Reflect,notreflex: Inference-basedcommon
groundimprovesdialogueresponsequality. InPro-
ceedingsofthe2022ConferenceonEmpiricalMeth-
odsinNaturalLanguageProcessing,pages10450–
10468.
XuhuiZhou,HaoZhu,AkhilaYerukola,ThomasDavid-
son, Jena D. Hwang, Swabha Swayamdipta, and
MaartenSap.2023. Cobraframes: Contextualrea-
soning about effects and harms of offensive state-
ments. InFindingsofACL.
JanZienkowski,JefVerschueren,andJoÖstman.2011.
Discursivepragmatics: Aplatformforthepragmatic
studyofdiscourse. InDisursivePragmatics,pages
1–13.JohnBenjaminsPublishingCompany.
A TasksandDatasets HateBert16 andHateRoberta17 tomeasurethetoxi-
cityofthecontextandresponses.
Formality Data We obtain a conversational
dataset from Reddit12 by sampling conversa- B ModelingContextinRewriting
tional threads from subreddits such as r/news,
Weperformfew-shotpromptingexperimentswith
r/askscience, and r/Economics (formal conversa-
GPT-3.5 and GPT-NeoX. For GPT-3.5, we use 2
tions), as well as r/movies, r/fantasyfootball, and
few-shotexamples,whileforGPT-NeoX,weuse
r/relationships (informal conversations). We fo-
10few-shotexamples. Eachfew-shotexamplewas
cusontwo-turnRedditthreads: aparent/preceding
manuallyconstructedwiththeprecedingcontext,
contextandtheresponsetoberewritten. Next,we
anoriginalsentencetoberewritten,aninstruction
sampledocumentsfromCNNDailyMail(formal
specifyingtherequiredstyle,andasamplerewrite.
documents;Nallapatietal.,2016)andtheBlogAu-
Figures6and7displaythefew-shotpromptexam-
thorshipCorpus(informaldocuments;Schleretal.,
plesthatweutilizedforgeneratingrewritesinthe
2006). We select four sentences from each data
formalitychangetask.
sample: three sentences as the preceding parent
context, and the following sentence as the one to
B.1 In-contextlearningsampleRewrites
be rewritten. For each data sample, we label the
contextandresponseusingapre-trainedformality Table 6 shows some additional example rewrites
classifier.13 fromGPT-3.5andGPT-NeoXforalltasks.
C ContextualHumanEvaluation
Sentiment Data We obtain a conversational
dataset from the DailyDialog (Li et al., 2017)
Worker selection We involve annotators from
dataset,focusingontwo-turnconversations: apar-
USA and Canada on Amazon Mechanical Turk
ent/precedingcontextandtheresponsetoberewrit-
(MTurk),whovoluntarilyopt-inforeachtask. We
ten. Next,wesampleentriesfromtheYelpreviews
recruit annotators for each style transfer task via
(Zhangetal.,2015)dataset. Analogoustothedoc-
a corresponding qualification task. In the qualifi-
ument dataset used in formality, we choose four
cationtask,annotatorsmustanswertwoquestions
sentencesfromeachdatasample: threeasthepre-
perpairofrewrites: whichrewritehasthestrongest
cedingparentcontextandthesubsequentsentence
stylestrength(e.g.,mostformal),andwhichrewrite
as the one to be rewritten. For each data sample,
isthemostnaturalgiventheprecedingcontext. An-
weannotatethecontextandresponseusingasenti-
notatorsassessthreepairsofhandcraftedrewrites
mentclassifier.14 Wepartitionthedatatotransform
in each qualification task. Those who accurately
sentencesfrompositivetonegativesentimentand
answeratleastfiveofthesixquestions(threefor
viceversa,aswellastoconvertneutralsentences
styleandatleasttwofornaturalness)areapproved
topositiveornegativesentiment.
for the main task. Once approved, we pay them
$0.27USDperhead-to-headcomparison.
ToxicityData Weexaminethreeconversational
datasets: the Civil Comments in Context (CCC)
C.1 HumanEvaluationResults
dataset (Xenos et al., 2021), the Multi-Label Di-
Wepresenttheagreementresultsofthehumaneval-
alogue Malevolence Detection (MDMD) dataset
uationstudiesofdetoxification(Table7),formality
(Zhang et al., 2022), and the ProsocialDialog
change(Table8)andsentimenttransfer(Table9).
dataset (Kim et al., 2022). For each dataset, we
Additionally, refer to Figures 4 and 5 for screen-
select two turns from each conversational thread,
shotsofthehumanevaluationinstructionsprovided
representingtheprecedingparentcontextandthe
toannotatorsandtheactualtask,respectively.
subsequent response as the sentence to be rewrit-
ten. WeusetoxicityscoresfromPerspectiveAPI,15
inter-rateragreement StyleStrength EventMeaning IntendedMeaning Naturalness Fit
Krippendorff’sα 0.2757 0.3778 0.4346 0.2407 0.6855
12Weusereddit-corpus-smallfromhttp://convokit. Fleiss’κ 0.1926 0.2906 0.3003 0.1907 0.5167
cornell.edu/documentation/subreddit.html
13https://huggingface.co/s-nlp/ Table7: Inter-rateragreementscoresforhumanevalua-
roberta-base-formality-ranker tionresultsofde-toxificationtask
14https://huggingface.co/cardiffnlp/
twitter-xlm-roberta-base-sentiment 16https://huggingface.co/tomh/toxigen_hatebert
15https://perspectiveapi.com/ 17https://huggingface.co/tomh/toxigen_roberta
inter-rateragreement StyleStrength EventMeaning IntendedMeaning Naturalness Fit Lexical(ρ) Semantic(ρ) Lexical(τ) Semantic(τ)
Krippendorff’sα 0.6825 0.3311 0.428 0.3551 0.4322
Fit -0.02 0.14 -0.02 0.14
Fleiss’κ 0.552 0.2504 0.2667 0.253 0.3627
Naturalness -0.03 0.18 -0.03 0.17
Table8: Inter-rateragreementscoresforhumanevalua-
tionresultsofformalitytransfertask Table 10: Detoxification task: Spearman rank and
KendallCorrelationofnon-contextualevaluationmet-
ricswithhumanjudgment
inter-rateragreement StyleStrength EventMeaning IntendedMeaning Naturalness Fit
Krippendorff’sα 0.1868 0.2636 0.4292 0.3729 0.4581
Fleiss’κ 0.121 0.1964 0.3148 0.3581 0.2434 Lexical(ρ) Semantic(ρ) Lexical(τ) Semantic(τ)
Fit 0.18 0.28 0.17 0.27
Table9: Inter-rateragreementscoresforhumanevalua-
Naturalness 0.11 0.26 0.10 0.24
tionresultsofsentimentchangetask
Table11: Formalitytask: SpearmanrankandKendall
Correlationofnon-contextualevaluationmetricswith
D Non-contextualAutomaticEvaluation
humanjudgment
We present the non-contextual automated evalu-
ation results for each task-specific dataset. Fig-
Lexical(ρ) Semantic(ρ) Lexical(τ) Semantic(τ)
ures 16 and 17 illustrate the formality change re- Fit 0.11 0.26 0.10 0.25
sults for document-level and conversation-level Naturalness -0.05 0.13 -0.05 0.12
datasets, respectively. Figures 18 and 19 display
Table12: Sentimenttask: SpearmanrankandKendall
the sentiment transfer results for document-level
Correlationofnon-contextualevaluationmetricswith
andconversation-leveldatasets,respectively. Fig-
humanjudgment
ures27,21,and22depictthede-toxificationresults
forconversationaldatasets. Notably,allofthesefig-
uresexhibitsimilartrendstotheaggregateresults Lexical(ρ) Semantic(ρ) CtxSimFit(ρ) Lexical(τ) Semantic(τ) CtxSimFit(τ)
Fit 0.63 0.56 0.85 0.61 0.54 0.82
acrossalltasksanddatasetspresentedinFigure3.
Naturalness 0.59 0.58 0.88 0.56 0.55 0.84
Table 13: Detoxification task: Spearman rank and
D.1 CorrelationwithHumanJudgments
Kendall Correlation of contextual evaluation metrics
withhumanjudgment
Effective evaluation metrics should yield judg-
mentsthatcorrelatehighlywithhumanjudgments,
assumingthathumanevaluatorsrepresentagold-
Lexical(ρ) Semantic(ρ) CtxSimFit(ρ) Lexical(τ) Semantic(τ) CtxSimFit(τ)
standard. For the human judgments along the di- Fit 0.74 0.68 0.93 0.71 0.65 0.89
Naturalness 0.68 0.69 0.94 0.65 0.66 0.90
mensions of naturalness and fit, we map human
preferences as follows: ‘contextual’ to 1, ‘tie’ to Table14: Formalitytask: SpearmanrankandKendall
0,and‘non-contextual’to−1. Fortheautomatic Correlationofcontextualevaluationmetricswithhuman
judgment
metrics,weassignascoreof1ifametricscoresthe
contextualrewritehigherthanthenon-contextual
rewrite,and−1ifthemetricscoresarelowerfor
Lexical(ρ) Semantic(ρ) CtxSimFit(ρ) Lexical(τ) Semantic(τ) CtxSimFit(τ)
contextualrewrites. Fit 0.45 0.45 0.78 0.42 0.52 0.74
Naturalness 0.44 0.51 0.73 0.42 0.48 0.69
For a given automatic metric and human judg-
Table15: Sentimenttask: SpearmanrankandKendall
mentdimension,wecalculatetheSpearmanrank
Correlationofcontextualevaluationmetricswithhuman
ρcorrelationandKendall’sτ forthedatasetsam-
judgment
plesusedduringthecontextualhumanevaluation
§5.1. Thecorrelationscores,rangingfrom−1to1,
areobtainedbycomparingthemappedautomatic
E ContextualAutomaticEvaluation
scores with the mapped human judgment scores.
Higher values indicate a stronger correlation be- We present the contextual automated evaluation
tween the scores obtained using the comparison results for each task-specific dataset. Figures 23
metricandjudgmentsmadebyhumanevaluators. and 24 illustrate the formality change results for
RefertoTables10–12forthecorrelationscores document-levelandconversation-leveldatasets,re-
ofnon-contextualevaluationmetricswithhuman spectively. Figures 25 and 26 display the sen-
judgmentsforeachtask. timent transfer results for document-level and
conversation-level datasets, respectively. Figures
27,28,and29depictthede-toxificationresultsfor
conversationaldatasets. Allofthesefiguresexhibit
similar trends to the aggregate results across all
tasks and datasets presented in Figure 3 and they
alignwiththefindingsfromourcontextualhuman
evaluationstudy.
E.1 CorrelationwithHumanJudgments
Similar to §D.1, we measure the Spearman rank
ρ correlation and Kendall’s τ correlation for the
samplesusedduringhumanevaluationin§5.1. Re-
fer to Tables 13 – 15 for the correlation scores
ofnon-contextualevaluationmetricswithhuman
judgmentsforeachtask.
Figure4: Screenshotoftheinstructionsforhumanevaluationannotation
Figure5: Screenshotofthehumanevaluationannotationtask
Task Conversion Context OriginalSentence Model ContextualRewrite Non-contextualRewrite
(cid:15)Iplantocontinuetoup- (cid:15)Herearesomeofthem,
GPT-3.5 datethispagewithfurther andIwillcontinuetoup-
@Iamreadingabook, selectionsthatIfindpartic- date this page with my
777Mathematical (cid:15)Herezsomeofthem, ularlyintriguingfromthe preferences.
Informal ConversationStartersby andfurtherIwillkeep book.
(a)Formality
->Formal JohndePillis.Itsan updatingthispagewith
interestingbookwithlots whatIlike.
(cid:15)Herearesomeofthem, (cid:15)Hereissomeoftheone
ofquotes,poems GPT-NeoX andinalaterpostIwill andfurtherIwillupdate
keep updating this page thispagewithwhatelseI
withmyfavouritequotes findinteresting.
fromthebook.
(cid:215)IenjoyedNiNoKuni (cid:215) I enjoyed that game.
GPT-3.5 2,butit’sclearthatother Itseemsopinionsmayin-
peopledidn’t-andthat’s deedvaryamongindividu-
(cid:220)Icanneveragaintake (cid:215)Ilikedthatgame.It’s okay.Everyone’sgottheir als.
Semi-formal asiteseriousthatgives ownopinion!
(b)Formality almostasifopinionsare
->Informal themessand...justan
awfulgame. subjective. (cid:215)Ilikedthatgame,de-
GPT-NeoX spite critical opinions to (cid:215)Ilikedthatgame. It’s
thecontrary.Thefactthat almostasifopinionsare
peopleenjoyeditdoesn’t relative.
make it any worse than
anythingelse.
GPT-3.5 (cid:215)@USERYes,Iwasask- (cid:215)Iamveryproudofmy
ingyoutobequiet. accomplishment!yeah
Toxic (cid:220)DidRolandojustcall (cid:215)@USERfuckyeahI
(c)Detoxification
->LessToxic mejusttotellmeshutup? did!!Shutup GPT-NeoX (cid:215)useryes,pleasekeep (cid:215)yeah,Idid!Shutup.
quiet
(cid:215)I’mnotsureIunder- (cid:215)It’sinterestingtosee
GPT-3.5 standwhatyou’retrying howsomeoneisilliterate
(cid:220)"iamhere2play tosay. Couldyouplease triestotease.
(d)Detoxification Toxic gamesiamplayin1witu (cid:215)It’ssoamusingwhen rephrase?
->LessToxic theilliteratetrytotaunt.
10:50to2:42babyi’m
(cid:215) It’s amusing when
everywhare" GPT-NeoX someoneattemptstoinsult (cid:215)It’sfunnywhentheun-
ortauntmeandtheyare educatedtrytoinsult.
clearlyuneducatedabout
language.
(cid:15) Despite being an
(cid:15) He’s an unfriendly
@Ijustwanttoechowhat GPT-3.5 efficient mechanic, Tim andhumorlesspersonwho
everyoneelsehasalready (cid:15)He’salsoan lacked the friendly and makes you feel uneasy
(e)Sentiment ->P Nos ei gti av te ive s Nai od ta ob no lu yt dth idis Tm imec fih xan mic y... fe ux nt nre ym ge ul yy wfr hie on pd uly ts, h exu pm eo cr teo dus of adem see ra vn ico er rightaway.
carquicklyand youateaseimmediately. provider.
ci sn a oe m tx hep ae t ton Is p wiv ic oe k uly li, dth nue ’p ta h hc i at m vu ea sl e tl l oy f GPT-NeoX v(cid:15) eryHo frw iee nv de lr y, T orim h’ us mn oo rt - (cid:15) unaH me usis edal gs uo ya wv he ory mr au kd ee s,
spendmoneyonatow ouswhichmightmakeyou youfeelanxious.
feeluneasy
truck!
(cid:215) It was very consider- (cid:215) How kind of you to
GPT-3.5 ateofyoutomoveit,so besoconsiderate!That’s
(cid:215)Howdareyoubeso thatyoucangetyourbike right.
(f)Sentiment Negative (cid:220)Ihadtomoveitsothat inconsiderate?That’s through.That’sgreat.
->Positive Icangetmybikethrough.
wrong.
(cid:215) I understand you
GPT-NeoX needed to move it so (cid:215)Howcanyoubesocon-
you could get your bike siderate!That’sright.
through - thank you for
beingthoughtful!
Table6: ExamplesfromourdatasetsandcorrespondingrewritesgeneratedbyGPT-3.5andGPT-NeoX,showcasing
all three tasks: formality change, de-toxification, and sentiment transfer. @=documentcontext, (cid:15)=sentenceto
rewrite,(cid:15) =contextualrewrite,(cid:15) =non-contextualrewrite;(cid:220)=previousturninconversation,(cid:215)=responsetorewrite,(cid:215)=
contextualrewriteofresponse,(cid:215)=non-contextualrewriteofresponse
Lexical Semantic Fluency
model rewrite type Style
ROU MET WER BERT-S SBERT Smatch PPL
contextual 0.19 0.40 2.14 0.92 0.62 0.51 38.37 0.59
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 18
8
0 0. .4 39
5
0 3. .9 11
7
0 0. .9 94
1
0 0. .7 53
2
0 0. .6 47
7
43.40 0.58
contextual 0.26 0.42 1.88 0.91 0.60 0.47 44.59 0.42
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .4 25
1
0 0. .6 33
6
0 1. .7 92
0
0 0. .9 95
1
0 0. .8 60
0
0 0. .6 47
1
44.80 0.35
Table 16: Non-contextual Automatic Evaluation Results on Formality: Document-level context from
CNN/DailyMail+BlogAuthorshipCorpus
Lexical Semantic Fluency
model rewrite type Style
ROU MET WER BERT-S SBERT Smatch PPL
contextual 0.16 0.38 2.67 0.90 0.67 0.45 33.78 0.68
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 12
5
0 0. .4 31
2
1 3. .2 73
2
0 0. .9 81
9
0 0. .7 52
8
0 0. .5 43
3
40.06 0.67
contextual 0.24 0.41 1.97 0.90 0.65 0.44 52.45 0.45
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .3 26
7
0 0. .5 45
0
0 2. .9 78
0
0 0. .9 92
0
0 0. .7 68
0
0 0. .5 44
4
57.12 0.37
Table17: Non-contextualAutomaticEvaluationResultsonFormality: ConversationalcontextcomprisedofReddit
threads
Lexical Semantic Fluency
model rewrite type Style
ROU MET WER BERT-S SBERT Smatch PPL
contextual 0.18 0.36 1.57 0.90 0.59 0.40 42.21 0.74
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .4 10
4
0 0. .6 31
0
0 1. .6 74
4
0 0. .9 84
9
0 0. .8 40
9
0 0. .6 33
6
58.38 0.64
contextual 0.27 0.43 1.43 0.91 0.56 0.41 57.64 0.49
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .4 25
9
0 0. .6 40
4
0 1. .6 37
7
0 0. .9 94
1
0 0. .7 54
6
0 0. .6 42
2
73.02 0.49
Table18: Non-contextualAutomaticEvaluationResultsonSentiment: Document-levelcontextcomprisedofYelp
Reviews
Lexical Semantic Fluency
model rewrite type Style
ROU MET WER BERT-S SBERT Smatch PPL
contextual 0.30 0.54 1.03 0.91 0.63 0.53 36.31 0.69
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .4 35
0
0 0. .6 57
2
0 1. .6 05
7
0 0. .9 93
1
0 0. .7 55
9
0 0. .6 58
3
42.82 0.64
contextual 0.16 0.30 1.66 0.87 0.43 0.22 42.39 0.35
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .3 13
8
0 0. .4 38
3
0 1. .8 61
3
0 0. .9 80
8
0 0. .5 49
3
0 0. .5 30
0
64.09 0.25
Table19: Non-contextualAutomaticEvaluationResultsonSentiment: ConversationalcontextfromDailyDialog
dataset
Lexical Semantic Fluency Style Style Style
model rewrite type
ROU MET WER BERT-S SBERT Smatch PPL HateRoberta HateBert Perspective
contextual 0.20 0.36 0.94 0.90 0.64 0.45 37.92 0.01 0.41 0.06
G P
T-3.5 n rao nn d- oc mon -t ce ox ntu tea xl
t
0 0. .2 14
7
0 0. .4 31
2
0 0. .8 90
7
0 0. .9 81
9
0 0. .7 52
7
0 0. .5 41
3
40.98 0.01 0.47 0.07
G P
T-NeoX c
n
rao
o
nn
n
dt -e ocx mot nu
-t
ca
e
ol
x ntu tea xl
t
0
0
0.
.
.3
4
32
4
2
0
0
0.
.
.4
5
40
2
0
0
0
0.
.
.7
6
78
1
7
0
0
0.
.
.9
9
90
2
0
0
0
0.
.
.6
7
50
1
8
0
0
0.
.
.4
5
46
7
7
6 63 7. .0 44
7
0 0. .0 17
0
0 0. .6 61
9
0 0. .1 13
5
Table20: Non-contextualAutomaticEvaluationResultsonToxicity: ConversationalcontextfromCCCdataset
Lexical Semantic Fluency Style Style Style
model rewrite type
ROU MET WER BERT-S SBERT Smatch PPL HateRoberta HateBert Perspective
contextual 0.11 0.32 1.18 0.87 0.51 0.43 75.48 0.04 0.31 0.11
G P
T-3.5 n rao nn d- oc mon -t ce ox ntu tea xl
t
0 0. .1 02
8
0 0. .3 24
8
0 1. .9 29
4
0 0. .8 88
6
0 0. .5 46
2
0 0. .4 47
0
78.23 0.05 0.34 0.12
G P
T-NeoX c
n
rao
o
nn
n
dt -e ocx mot nu
-t
ca
e
ol
x ntu tea xl
t
0
0
0.
.
.1
3
18
2
5
0
0
0.
.
.2
4
28
9
5
1
0
1.
.
.2
9
49
1
1
0
0
0.
.
.8
9
87
0
6
0
0
0.
.
.4
6
35
7
9
0
0
0.
.
.3
5
39
4
6
18 00 6. .1 60
6
0 0. .3 59
2
0 0. .6 72
4
0 0. .3 45
6
Table21: Non-contextualAutomaticEvaluationResultsonToxicity: ConversationalcontextfromMDMDdataset
Lexical Semantic Fluency Style Style Style
model rewrite type
ROU MET WER BERT-S SBERT Smatch PPL HateRoberta HateBert Perspective
contextual 0.05 0.21 1.69 0.88 0.38 0.29 22.80 0.03 0.25 0.06
G P
T-3.5 n rao nn d- oc mon -t ce ox ntu tea xl
t
00 .. 01 51 0 0. .2 19
9
0 1. .9 67
1
0 0. .9 81
8
0 0. .5 22
5
0 0. .4 21
9
33.00 0.14 0.40 0.09
G P
T-NeoX c
n
rao
o
nn
n
dt -e ocx mot nu
-t
ca
e
ol
x ntu tea xl
t
0
0
0.
.
.2
4
25
3
5
0
0
0.
.
.4
5
40
9
0
1
0
1.
.
.1
6
02
6
6
0
0
0.
.
.9
9
91
4
1
0
0
0.
.
.5
7
43
2
8
0
0
0.
.
.4
6
44
3
4
3 32 7. .8 96
0
0 0. .3 67
4
0 0. .6 73
9
0 0. .2 36
8
Table22: Non-contextualAutomaticEvaluationResultsonToxicity: ConversationalcontextfromProsocialDialog
dataset
Lexical Semantic Coherence Cohesiveness Custom
model rewrite type
ROUᶜᵗˣ METᶜᵗˣ WERᶜᵗˣ BERT-Sᶜᵗˣ SBERTᶜᵗˣ Smatchᶜᵗˣ PPLᶜᵗˣ NSP CtxSimFit
contextual 0.16 0.23 0.89 0.90 0.61 0.38 22.05 0.94 0.93
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .1 15
0
0 0. .2 10
6
0 1. .8 07
6
0 0. .8 89
7
0 0. .5 30
8
0 0. .3 23
9
3 32 4. .7 29
4
0 0. .8 67
9
0 0. .9 81
0
contextual 0.25 0.28 0.83 0.90 0.62 0.37 20.51 0.97 0.94
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 13
6
0 0. .2 14
9
0 0. .8 94
2
0 0. .8 89
7
0 0. .5 42
2
0 0. .3 23
9
3 32 9. .1 15
8
0 0. .9 84
2
0 0. .9 84
7
Table23: ContextualAutomaticEvaluationResultsonFormality: Document-levelcontextfromCNN/DailyMail+
BlogAuthorshipCorpus
Lexical Semantic Coherence Cohesiveness Custom
model rewrite type
ROUᶜᵗˣ METᶜᵗˣ WERᶜᵗˣ BERT-Sᶜᵗˣ SBERTᶜᵗˣ Smatchᶜᵗˣ PPLᶜᵗˣ NSP CtxSimFit
contextual 0.14 0.27 0.91 0.89 0.66 0.37 28.82 0.88 0.89
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .1 14
1
0 0. .2 24
0
0 1. .9 55
8
0 0. .8 88
7
0 0. .5 46
7
0 0. .3 36
2
4 41 6. .0 72
9
0 0. .8 72
3
0 0. .8 87
1
contextual 0.21 0.29 0.88 0.89 0.64 0.36 34.74 0.90 0.90
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 13
7
0 0. .2 29
2
0 1. .8 28
9
0 0. .8 88
7
0 0. .5 48
6
0 0. .3 36
1
5 52 3. .4 95
8
0 0. .8 86
0
0 0. .8 89
5
Table24: ContextualAutomaticEvaluationResultsonFormality: ConversationalcontextcomprisedofReddit
threads
Lexical Semantic Coherence Cohesiveness Custom
model rewrite type
ROUᶜᵗˣ METᶜᵗˣ WERᶜᵗˣ BERT-Sᶜᵗˣ SBERTᶜᵗˣ Smatchᶜᵗˣ PPLᶜᵗˣ NSP CtxSimFit
contextual 0.11 0.17 0.92 0.88 0.53 0.24 25.53 0.98 0.94
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .1 06
7
0 0. .1 17
2
0 0. .8 97
3
0 0. .8 88
6
0 0. .4 38
8
0 0. .2 14
9
4 41 4. .1 18
3
0 0. .9 83
2
0 0. .9 84
6
contextual 0.13 0.16 0.90 0.87 0.47 0.23 33.05 0.96 0.93
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .1 17
2
0 0. .1 18
5
0 0. .8 97
1
0 0. .8 88
7
0 0. .4 44
1
0 0. .2 22
0
4 58 3. .5 49
4
0 0. .9 93
1
0 0. .9 93
1
Table25: ContextualAutomaticEvaluationResultsonSentiment: Document-levelcontextcomprisedofYelp
Reviews
Lexical Semantic Coherence Cohesiveness Custom
model rewrite type
ROUᶜᵗˣ METᶜᵗˣ WERᶜᵗˣ BERT-Sᶜᵗˣ SBERTᶜᵗˣ Smatchᶜᵗˣ PPLᶜᵗˣ NSP CtxSimFit
contextual 0.25 0.36 0.84 0.89 0.62 0.43 33.88 0.97 0.94
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 28
0
0 0. .3 35
0
0 0. .8 81
9
0 0. .8 89
8
0 0. .5 44
5
0 0. .4 31
8
5 50 4. .6 10
0
0 0. .9 82
7
0 0. .9 83
9
contextual 0.17 0.26 0.97 0.86 0.46 0.34 32.45 0.88 0.88
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 10
2
0 0. .2 24
1
0 1. .8 07
4
0 0. .8 87
5
0 0. .4 32
4
0 0. .2 21
1
6 40 1. .3 61
9
0 0. .8 76
9
0 0. .8 88
3
Table26:ContextualAutomaticEvaluationResultsonSentiment:ConversationalcontextfromDailyDialogdataset
Lexical Semantic Coherence Cohesiveness Custom
model rewrite type
ROUᶜᵗˣ METᶜᵗˣ WERᶜᵗˣ BERT-Sᶜᵗˣ SBERTᶜᵗˣ Smatchᶜᵗˣ PPLᶜᵗˣ NSP CtxSimFit
contextual 0.16 0.24 0.86 0.88 0.61 0.36 28.45 0.95 0.93
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .1 17
2
0 0. .2 25
0
0 0. .8 95
0
0 0. .8 88
7
0 0. .5 47
7
0 0. .3 35
1
3 37 8. .4 90
6
0 0. .9 81
9
0 0. .9 81
9
contextual 0.24 0.25 0.82 0.88 0.54 0.34 37.41 0.96 0.93
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 29
1
0 0. .2 29
3
0 0. .7 87
4
0 0. .8 89
7
0 0. .5 44
4
0 0. .3 32
2
5 51 2. .5 26
4
0 0. .9 82
9
0 0. .9 92
0
Table27: ContextualAutomaticEvaluationResultsonToxicity: ConversationalcontextfromCCCdataset
Lexical Semantic Coherence Cohesiveness Custom
model rewrite type
ROUᶜᵗˣ METᶜᵗˣ WERᶜᵗˣ BERT-Sᶜᵗˣ SBERTᶜᵗˣ Smatchᶜᵗˣ PPLᶜᵗˣ NSP CtxSimFit
contextual 0.10 0.22 0.91 0.86 0.50 0.34 49.50 0.96 0.92
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .0 08
6
0 0. .1 19
6
0 0. .9 92
6
0 0. .8 86
4
0 0. .4 20
9
0 0. .3 21
9
7 70 1. .9 01
9
0 0. .8 86
2
0 0. .8 87
4
contextual 0.20 0.25 0.85 0.87 0.52 0.35 40.79 0.93 0.90
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .1 09
9
0 0. .2 16
5
0 0. .8 97
7
0 0. .8 87
5
0 0. .4 37
0
0 0. .3 26
6
9 90 2. .6 24
1
0 0. .8 79
6
0 0. .9 80
1
Table28: ContextualAutomaticEvaluationResultsonToxicity: ConversationalcontextfromMDMDdataset
Lexical Semantic Coherence Cohesiveness Custom
model rewrite type
ROUᶜᵗˣ METᶜᵗˣ WERᶜᵗˣ BERT-Sᶜᵗˣ SBERTᶜᵗˣ Smatchᶜᵗˣ PPLᶜᵗˣ NSP CtxSimFit
contextual 0.09 0.18 0.90 0.89 0.54 0.29 14.89 0.98 0.93
G P
T-3.5 n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .0 06
3
0 0. .1 12
1
0 0. .9 93
5
0 0. .8 88
6
0 0. .3 26
1
0 0. .2 21
2
2 39 2. .7 55
8
0 0. .8 89
4
0 0. .9 80
6
contextual 0.19 0.23 0.85 0.89 0.52 0.32 18.98 0.94 0.93
G P
T-Neo X
n rao nn d- oco mn -t ce ox ntu tea xl
t
0 0. .2 10
2
0 0. .2 12
7
0 0. .8 95
0
0 0. .8 89
8
0 0. .4 34
3
0 0. .3 20
6
3 34 6. .4 32
2
0 0. .8 88
4
0 0. .9 81
8
Table29: ContextualAutomaticEvaluationResultsonToxicity: ConversationalcontextfromProsocialDialog
dataset
Figure6: Formality: 10-shotpromptingexamplesforGPT-NeoX
Figure7: Formality: 2-shotpromptingexamplesforGPT-3.5

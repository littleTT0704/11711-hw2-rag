PublishedasaconferencepaperatICLR2023
DRAFT, SKETCH, AND PROVE: GUIDING FORMAL
THEOREM PROVERS WITH INFORMAL PROOFS
AlbertQ.Jiang1,2,† SeanWelleck3,4,† JinPengZhou5,6,†
WendaLi2 JiachengLiu3 MatejaJamnik2
Timothe´eLacroix1 YuhuaiWu5,7,‡ GuillaumeLample1,‡
1MetaAI 2UniversityofCambridge 3UniversityofWashington 4AllenInstituteforAI
5GoogleResearch 6CornellUniversity 7StanfordUniversity
ABSTRACT
Theformalizationofexistingmathematicalproofsisanotoriouslydifficultprocess.
Despitedecadesofresearchonautomationandproofassistants,writingformal
proofsremainsarduousandonlyaccessibletoafewexperts.Whilepreviousstudies
toautomateformalizationfocusedonpowerfulsearchalgorithms,noattemptswere
madetotakeadvantageofavailableinformalproofs. Inthiswork,weintroduce
Draft,Sketch,andProve(DSP),amethodthatmapsinformalproofstoformalproof
sketches,andusesthesketchestoguideanautomatedproverbydirectingitssearch
toeasiersub-problems. Weinvestigatetworelevantsetupswhereinformalproofs
areeitherwrittenbyhumansorgeneratedbyalanguagemodel. Ourexperiments
and ablation studies show that large language models are able to produce well-
structuredformalsketchesthatfollowthesamereasoningstepsastheinformal
proofs. Guidinganautomatedproverwiththesesketchesenhancesitsperformance
from20.9%to39.3%onacollectionofmathematicalcompetitionproblems.
Figure1: Draft,Sketch,andProve.Startingwithaninformalstatement,ourframeworkyieldsaformalproof
throughathree-stageprocess:draftinginformalproofs,mappingthemintoformalsketches,andprovingthe
remainingconjectures. Concretely,aninformalstatementisamathematicalproblemdescribedinamixture
ofnaturalandmathematicallanguages(e.g., formulaeinLATEX). Then, weusealargelanguagemodelto
autoformalizeeachinformalproofintoaformalsketch, whichisaskeletonoftheformalproofwithopen
conjecturesleftunproven(indicatedbythe<proof>blocks).Theformalsketchmirrorsthestructureofthe
informalproof.Finally,theopenconjectures/gapsinsideeachformalsketchareprovedbyanoff-the-shelfprover.
†Equalcontributionsasleadingauthors.Correspondenceto:qj213@cam.ac.uk.
‡Equalcontributionsasseniorauthors.
1
3202
beF
02
]IA.sc[
3v38221.0122:viXra
PublishedasaconferencepaperatICLR2023
1 INTRODUCTION
Formalproofautomationisachallengingtaskthathasbeenthefocusofincreasedattentioninrecent
years (Bansal et al., 2019b; Polu & Sutskever, 2020; Lample et al., 2022; Jiang et al., 2022; Wu
etal.,2022). However,deeplearningapproacheshavenotbeenassuccessfulasinotherdomains,
mainlybecauseofthescarcityofformaldata. Indeed,formalizingproofsisnotoriouslydifficultand
onlyaccessibletoahandfulofexperts,whichmakeslargeannotationendeavorsunrealistic(Wiedijk,
2008). ThelargestformalproofcorpusiswritteninIsabelle(Paulson,1994),andamountstoless
than0.6GBinsize,ordersofmagnitudesmallerthandatasetscommonlyusedinvision(Dengetal.,
2009)ornaturallanguageprocessing(Brownetal.,2020). Toaddressthescarcityofformalproofs,
previous studies have proposed to use synthetic data (Wu et al., 2021), self-supervision (Polu &
Sutskever,2020;Hanetal.,2022),orreinforcementlearning(Bansaletal.,2019a;Poluetal.,2022)
tosynthesizeadditionalformaltrainingdata. Althoughthesemethodsalleviatethedatainsufficiency
tosomedegree,noneareabletocapitalizeonthebulkofhuman-writtenmathematicalproofs.
Unlikeformalmathematics,informalmathematicaldataisabundantandwidelyavailable. Recently,
largelanguagemodelstrainedoninformalmathematicaldatashowcasedimpressivequantitative
reasoningabilities(Lewkowyczetal.,2022;Wellecketal.,2022). However,theyoftengenerate
erroneousproofsanditischallengingtodetectthefaultyreasoningintheseproofsautomatically. Our
workdevisesanovelapproachcalledDraft,Sketch,andProve(DSP)totranslateinformalmathemat-
icalproofsintoformalonesandthusenjoyboththelogicalrigorprovidedbyformalsystemsandthe
wealthofinformaldata. WegiveaschematicdiagramoftheDSPmethodinFigure1anddescribeit
inSection3. Recentwork(Wuetal.,2022)demonstratesthefeasibilityofautomaticallytranslating
informalstatementsintoformaloneswithlargelanguagemodels. DSPgoesbeyondandleverages
largelanguagemodelstogenerateformalproofsketches(Wiedijk,2003)frominformalproofs. Proof
sketches consist of high-level reasoning steps that can be interpreted by formal systems such as
interactivetheoremprovers. Theydifferfromcompleteformalproofsinthattheycontainsequences
ofintermediateconjectureswithoutjustification.Anexampleofinformalproofwithitscorresponding
formalproofsketchisprovidedinFigure2. InthelaststepofDSP,weelaboratetheformalproof
sketchintoafullformalproofusinganautomatedprovertoproveallintermediateconjectures.
WeperformexperimentstogenerateformalproofsofproblemsfromtheminiF2Fdataset(Zheng
etal.,2022)andshowthatalargeportionoftheoremscanbeprovedautomaticallywiththismethod.
Weinvestigatetwosettingswheretheinformalproofsareeitherwrittenbyhumansordraftedby
alargelanguagemodeltrainedonmathematicaltext. Thesetwosettingscorrespondtosituations
frequentlyoccurringduringtheformalizationofexistingtheories,whereinformalproofsareusually
available,butsometimesleftasexercisestothereaderormissingduetospacelimitsinthemargin.
Contributions:
• Weintroduceanovelapproachtoleverageinformalproofstoguideautomatedproverswith
formalproofsketches.
• Toevaluateourapproach,webuildadatasetofmanuallycuratedinformalstatementsand
informalproofsalignedwithformalstatementsintheminiF2Fdataset(Zhengetal.,2022).
• WeincreasetheproportionofproblemssolvedbyanautomatedproveronminiF2Ffrom
20.9%to38.9%givenlanguage-model-generatedinformalproofs,andupto39.3%when
proofsarewrittenbyhumans.
• Throughthreeablationstudies,wedemonstratetheperformancebenefitofdraftinginformal
proofs,annotatingsketcheswithinformalsegments,andusingautomatedproverstoclose
openconjecturesfortheautoformalizationofproofs.
2 BACKGROUND AND RELATED WORK
Interactivetheoremproving Modernverificationsystemsformathematicsarecenteredaround
interactive theorem provers (ITPs), such as Isabelle (Paulson, 1994), Lean (Moura et al., 2015),
Coq(Barrasetal.,1997),orMetamath(Megill&Wheeler,2019). ITPsembedthemathematical
definitionsandtheoremsontoasolidlogicalfoundation(e.g.,Higher-OrderLogic,DependentType
Theory)implementedbytheirkernels. Everytheoremmustbecheckedbythekerneltoberecognized
2
PublishedasaconferencepaperatICLR2023
bytheITP.Tobeprovedformally,atheoremisfirststatedintheITP’sprogramminglanguage,and
iterativelysimplifiedintosimplerobjectives(orsubgoals),untilitcanbereducedtoalreadyproven
facts. Inthispaper,wewillrefertoproofsverifiedbyaformaltheoremproverasformalproofs,and
proofswrittenin“standard”mathematics(e.g. inLATEX)asinformalproofs.
Machinelearningforformalproofsynthesis Severalapproachesproposetocombinemachine
learningwithmoderninteractivetheoremprovers(Yang&Deng,2019;Gauthieretal.,2021),and
buildupontherecentsuccessoflanguagemodels(Polu&Sutskever,2020;Hanetal.,2022;Poluetal.,
2022;Jiangetal.,2022;Lampleetal.,2022). Thesemethodstypicallyrelyonsequence-to-sequence
models(Sutskeveretal.,2014)togeneratethenextstepofaproofgiventhecurrentproofstateand
performsearchoverthegeneratedsubgoalsusingpowerfulsearchmethodssuchasMCTS(Silver
etal.,2018). Becausesearchiscomputationallyexpensive,theselanguagemodelsarerelativelysmall
(withfewerthan1billionparameters). Ourmethodcontrastswiththeseapproachesinthatweusea
significantlyreducednumberofcallstothemodels,butalsomuchlargerlanguagemodels(withup
to175billionparameters)thatshowcaseoutstandingfew-shotlearningabilities(Brownetal.,2020).
Machinelearningforinformalreasoning Languagemodelshavealsobeenusedinthecontext
ofpurelyinformalmathematics(Lample&Charton,2020;Hendrycksetal.,2021;Wellecketal.,
2021;Drorietal.,2022;Wellecketal.,2022). Nevertheless,Lewkowyczetal.(2022)notethatfor
quantitativequestionanswering,modelsarepronetogeneratefalsepositives: themodelguesses
therightanswerwhileprovidinganincorrectproof. Theseerrorsarehardtospotwithouthuman
inspection. Worryingly,thefrequencyoffalsepositivesincreaseswiththedifficultyoftheproblem.
Ourmethodbuildsonthesefindingsandtranslatesinformalproofsintoformalproofs. SinceITPs
arelogicallygrounded,onceaformalproofischeckedbythem,weareguaranteeditscorrectness.
Autoformalization Inapositionpaper,Szegedy(2020)arguedforattainingformalmathematical
datafrominformalsourceswithneuralnetworks. Wangetal.(2020)performedpreliminaryexperi-
mentswheretheevaluationwaslimitedtotext-levelsimilaritiesonsyntheticdatasets. Recently,Wu
etal.(2022)foundthatlargelanguagemodels(Chenetal.,2021;Chowdheryetal.,2022)arecapable
offew-shotstatementautoformalization. Namely,asmallnumberofexamplesareenoughforthem
tolearntoperforminformal-to-formaltranslationofstatements. Inthispaper,weinvestigatewhether
thesefindingscangeneralizetoproofautoformalization,i.e.,whetherlargelanguagemodelscanbe
usedtotranslateinformalproofsintoformalones.
3 METHOD
Inthissection,wedescribeourDraft,Sketch,andProve(DSP)methodforformalproofautomation,
whichleveragesinformalproofstoguideautomatedformaltheoremproverswithproofsketches. We
assumethateachproblemcomeswithaninformalstatementandaformalstatementdescribingthe
problem. Ourpipelineconsistsofthreestages(depictedinFigure1),whichwepresentbelow.
3.1 DRAFTINGINFORMALPROOFS
TheinitialphaseoftheDSPmethodconsistsinfindinginformalproofsforaproblemaccordingto
itsdescriptioninnaturalmathematicallanguage(possiblywithLATEX). Theresultinginformalproof
isseenasadraftforthesubsequentphases. Inmathematicaltextbooks,proofsoftheoremsarein
generalprovided,butaresometimesmissingorincomplete. Therefore,weconsidertwosettings
corresponding to the presence or absence of the informal proofs. In the first, we assume that a
“ground-truth”informalproof(i.e.,onewrittenbyahuman)isavailable,whichisthetypicalscenario
inthepracticeofformalizingexistingmathematicaltheories. Inthesecondsetting,wemakeamore
general assumption that the ground-truth informal proof is not given, and draft proof candidates
withalargelanguagemodeltrainedoninformalmathematicaldata. Thelanguagemodelremoves
thedependenceonhumanproofsandcanproducemultiplealternativesolutionsforeveryproblem.
Althoughthereisnoeasywaytoautomaticallyverifythecorrectnessoftheseproofs,theinformal
proofonlyneedstobeusefulforproducingagoodformalproofsketchinthenextstage.
3
PublishedasaconferencepaperatICLR2023
InformalStatement:Showthatforanyrealnumbera,10a≤28a2+1.
InformalProof:
Itsufficestoshow0 ≤ 28a2−10a+1. First,considercompletingthesquarefor28a2−10aand
observethat(cid:0) a− 5 (cid:1)2 =a2−10a+(5/28)2.Since0≤(cid:0) a− 5 (cid:1)2,weget0≤a2−10a+(5/28)2.
28 28 28 28
Multiplyingby28andsimplifyinggives0 ≤ 28a2−10a+(25/28). Since25/28 < 1,theresult
follows.
FormalStatement:
theoremalgebra_binomnegdiscrineq_10alt28asqp1:
fixes a :: real
shows "10 * a ≤ 28 * aˆ2 + 1"
FormalProofSketch:
proof - (* it suffices to show 0 <= 28aˆ2 - 10a + 1 *)
have c0: "0 ≤ 28aˆ2 - 10a + 1"
proof - (* observethat (a - (5/28))ˆ2 = aˆ2 - (10/28)a + (5/28)ˆ2 *)
have c1: "(a - (5/28))ˆ2 = aˆ2 - 10/28a + (5/28)ˆ2" <···>
(* we get 0 <= aˆ2 - (10/28)a + (5/28)ˆ2 *)
have c2: "0 ≤ aˆ2 - 10/28a + (5/28)ˆ2"using c1 <···>
(* Multiplying by 28 andsimplifyinggives 0 <= 28aˆ2 - 10a + (25/28) *)
have c3: "0 ≤ 28aˆ2 - 10a + 28((5/28)ˆ2)"using c2 <···>
have c4: "0 ≤ 28aˆ2 - 10a + 28((5/28)*(5/28))"using c3 <···>
have c5: "0 ≤ 28aˆ2 - 10a + (25/28)"using c4 <···>
(* Since25/28 < 1, the resultfollows. *)
show ?thesisusing c5 <···>
qed
show ?thesis <···>
qed
Figure 2: AproofsketchinIsabelle. Theproblem“Showthatforanyrealnumbera,10a ≤ 28a2+1”
isgivenwithaninformalproofandanassociatedformalproofsketch. Thesketchfirstrewritestheoriginal
statement(c0),whichisprovedthrough5intermediaryconjectures(c1..c5).Weuseaspecialtoken(<···>)
toindicatethattheconjectureis“open”andshouldbetackledbyanautomatedproverlater.Tofacilitatethe
alignmentbetweentheinformalandformallanguages,weannotatetheformalproofsketchexampleswith
informalproofsegments(showninred),whichareimmediatelyfollowedbytheirformalcounterparts.
3.2 MAPPINGINFORMALPROOFSINTOFORMALSKETCHES
Aformalproofsketchencodesthestructureofasolutionandleavesoutlow-leveldetails(Wiedijk,
2003). Intuitively, it is a partial proof that outlines high-level conjecture statements. A concrete
exampleofaproofsketchisshowninFigure2. Althoughinformalproofsoftenleaveasidelow-level
details,(e.g.,bystatingtheirtriviality),thesedetailscannotbedischargedinaformalproof,making
straightforwardinformal-to-formalprooftranslationdifficult. Instead,weproposetomapinformal
proofs to formal proof sketches that share the same high-level structures. The low-level details
missingfromaproofsketchcanlaterbefilledbyanautomatedprover. Sincelargeinformal-formal
parallel corpora do not exist, standard machine translation methods are unsuitable for this task.
Rather,weusethefew-shotlearningabilitiesofalargelanguagemodel. Specifically,wepromptthe
modelwithafewexamplepairscontaininginformalproofsandtheircorrespondingformalsketches,
followedbyaninformalproofyettobetranslated. Wethenletthemodelgeneratethesubsequent
tokenstoobtainthedesiredformalsketch. Werefertothismodelasanautoformalizer.
3.3 PROVINGOPENCONJECTURESINTHESKETCHES
As the last part of the process, we execute off-the-shelf automated provers to fill in the missing
detailsinproofsketches,where“automatedprovers”referstosystemscapableofproducingformally
verifiableproofs. Ourframeworkisagnostictothespecificchoiceoftheautomatedprover: itcanbe
symbolicproverssuchasheuristicproofautomationtools,neural-network-basedprovers,orhybrid
approaches. Iftheautomatedproversuccessfullyclosesallthegapsintheproofsketch,itreturnsthe
finalformalproofwhichcanbecheckedagainsttheproblem’sspecification. Iftheautomatedprover
fails(e.g.,itexceedstheallocatedtimelimit),weconsidertheevaluationtobeunsuccessful.
4
PublishedasaconferencepaperatICLR2023
4 EXPERIMENTS
4.1 DATASETANDEVALUATION
WeevaluateourmethodontheminiF2Fdataset(Zhengetal.,2022). Thedatasetcontainstheformal
statementsof488problemsfromhigh-schoolmathematicalcompetitions,writteninthreeformal
languages: Lean,HOL-Light,andIsabelle. Theyaresplitintoavalidsetandatestset,composed
of244problemseach. Inthiswork,wechoosetoexperimentwithIsabelleforthreereasons: (1)
Isabelle’sproofcorpusisoneofthelargestamonginteractivetheoremprovers, conducivetothe
languagemodels’masteryofitssyntax;(2)Isabellesupportsthedeclarativeproofstyle(detailed
discussioninAppendixA),enablingformalproofsketches(Wiedijk,2003)whicharecentraltoour
method;(3)althoughautomatedprovingtoolsareavailableinotherinteractivetheoremprovers,none
areasdevelopedandeffectiveasSledgehammer(Paulson,2010)inIsabelleforprovingconjectures.
The miniF2F dataset is comprised of problems from three source categories: (1) 260 problems
sampled from the MATH dataset (Hendrycks et al., 2021); (2) 160 problems from actual high-
schoolmathematicalcompetitions(AMC,AIME,andIMO);(3)68craftedproblemsatthesame
difficulty level as (2). We employ three methods to obtain informal statements and proofs from
thesesources. Forsource(1),weaccesstheinformalstatementsandproofsfromtheMATHdataset;
for(2),weretrievetheirinformalstatementsandproofsfromtheAOPSwebsite1;andfor(3),we
manually write down their informal statements and proofs. Thus we gather a parallel set of 488
informal statements, informal proofs, and formal statements. This dataset provides the informal
statements and proofs for our experiment in the human-as-informal-proof-writer setting and is
availableatgithub.com/facebookresearch/miniF2F.Thisrepositoryalsocontainsfixes
tonumerouserrorsintheminiF2Fdataset. Inourexperiments,weusedformalstatementsfromthe
originalminiF2Frepository2forafaircomparisonwithpriorworks.
OurtaskistogenerateformalproofsforproblemsastheyareformallystatedinminiF2F.Weconsider
aproofvalidifandonlyifit(a)doesnotcontain“cheating”keywords(sorryandoops)thatexit
aproofwithoutcompletingit,and(b)Isabelleisabletoverifythecorrespondingformalstatement
withtheproof. WeusethePortal-to-ISAbelleAPIbyJiangetal.(2021)tointeractwithIsabelle.
4.2 BASELINES
Sledgehammer Asabaseline,weattempttoprovetheformalstatementdirectlywithSledgeham-
mer,apopularproofautomationtoolinIsabelle. WeusethedefaultSledgehammerconfigurationin
Isabelle2021,includinga120-secondtimeoutandthefiveautomatedtheoremprovers(Z3,CVC4,
SPASS,Vampire,E).AppendixBgivesamorethoroughintroductiontoSledgehammer.
Sledgehammer+heuristics Occasionally,Sledgehammermayfailwithouttryingsimpleyeteffec-
tivetactics.Asasecond,strongerbaseline,wecreateanautomatedproverthattries11commontactics
(auto, simp, blast, fastforce, force, eval, presburger, sos, arith, linarith,
auto simp: field simps)forhigh-schoollevelalgebraandnumbertheoryproblems. Ifevery
attemptedtacticfails,ortimesoutafter10seconds,itfallsbacktoSledgehammer.
Language models for proof search Finally, we include baselines which are representative of
state-of-the-artneuraltheoremprovinginIsabelle,specificallyThor(Jiangetal.,2022)andThor
with expert iteration on autoformalized data (Wu et al., 2022). The methods GPT-f with expert
iteration(Poluetal.,2022),andHyperTreeProofSearch(HTPS)(Lampleetal.,2022)cansolve
36.6% and 41.0% of the problems on miniF2F-test. However, they rely on the Lean theorem
proverinsteadofIsabelle,whichgreatlyinfluencestheperformanceduetothedifferenttacticsand
automation,andarenotdirectlycomparabletoourmethod.
1https://artofproblemsolving.com/community
2https://github.com/openai/miniF2F
5
PublishedasaconferencepaperatICLR2023
Table1: ProvingsuccessratesontheminiF2FdatasetwithIsabelleInthetablearethesuccessratesoffour
baselines,theDSPmethodwithhumanandlanguagemodelinformalproofs,aswellasthreeablationstudies,
onthevalidationandthetestsetsofminiF2F.Thehighestsuccessratesoneachsetarehighlightedinbold.The
performancedifferencebetweenablationstudiesandDSPwithhumaninformalproofsareenclosedinbrackets.
Successrate miniF2F-valid miniF2F-test
Baselines
Sledgehammer 9.9% 10.4%
Sledgehammer+heuristics 18.0% 20.9%
Thor(Jiangetal.,2022) 28.3% 29.9%
Thor+expertiteration(Wuetal.,2022) 37.3% 35.2%
Draft,Sketch,andProve
Humaninformalproof 42.6% 39.3%
Codexinformalproof 40.6% 35.3%
8BMinervainformalproof 40.6% 35.3%
62BMinervainformalproof 43.9% 37.7%
540BMinervainformalproof 42.6% 38.9%
Ablations(withhumaninformalstatementsandproofs)
–In-linecomments 37.7%(−4.9%) 36.5%(−2.8%)
–Informalproofs 38.9%(−3.7%) 34.0%(−5.3%)
–Automatedprovers 32.8%(−9.8%) 30.3%(−9.0%)
4.3 EXPERIMENTALSETUP
Drafting Wheninformalproofsaregenerated,weconditionalargelanguagemodeloninformal
statementstosample100informalproofsperproblem. Specifically,weusetheCodexcode-davinci-
002model(Chenetal.,2021)throughtheOpenAIAPI,andthe8B,62B,and540Bversionsofthe
MinervamodelfromLewkowyczetal.(2022). WeusegreedydecodingforCodex, andnucleus
sampling(Holtzmanetal.,2019)withtemperatureT =0.6andtop p=0.95forMinervamodels.
Sketching For sketching, we manually prepare 20 autoformalization examples of the format
(informalstatement,informalproof,formalstatement,formalsketch),toformapoolofhigh-quality
demonstrations. Ofthese20examples,10areofthealgebratypeand10areofthenumbertheory
type. All examples are from the validation set of the miniF2F dataset and can be found in the
supplementarymaterials. Thesketchescontainin-linecommentsasinFigure2. Ifthenameofthe
problemgivesawayitstype(algebraornumbertheory),weonlyuseexamplesofthecorresponding
type. Wealsoensurethatthesampledfew-shotexamplesdonotcontaintheproblembeingsolved.
Thepromptiscomposedof3uniformlyrandomlysampledexamplefromthepoolandthecurrent
problem’s(informalstatement,informalproof,formalstatement). Weusethisprompttoquerythe
sameCodexmodeltogetthedesiredproofsketches. Weusegreedydecodingandamaximumof
2048tokensinthegeneratedsequence. Foralltheexperiments,unlessstatedotherwise,wecontrol
thetotalnumberofqueriesmadetoCodexperproblemtobe100. Thismeans100queriesperhuman
informalsolutionandonequeryperlanguage-model-generatedsolution.
Proving To prove the conjectures left open by the formal sketch, we use the Sledgehammer +
heuristicsautomatedproverdescribedinSubsection4.2. Weexecutetheautomatedproveronevery
openconjectureinthesketchtosynthesizeaformalproofthatcanbeverifiedbyIsabelle.
4.4 RESULTS
InTable1,wedisplaytheproportionofsuccessfulformalproofsfoundontheminiF2Fdataset. The
resultsincludethefourbaselinesdescribedinSubsection4.2andtheDSPmethodwithhuman-written
proofsandmodel-generatedproofs. Fromthetable,wecanseethattheautomatedproverwith11
additionalheuristictacticssignificantlyincreasestheperformanceofSledgehammer,boostingits
successratefrom9.9%to18.0%onthevalidationsetofminiF2Fandfrom10.4%to20.9%onthe
testset. Thetwobaselinesusinglanguagemodelsandproofsearch(ThorandThor+expertiteration)
achievesuccessratesof29.9%and35.2%onthetestsetofminiF2F,respectively.
6
PublishedasaconferencepaperatICLR2023
With informal proofs written by humans, the DSP method achieves success rates of 42.6% and
39.3%onthevalidationandtestsetsofminiF2F.Atotalof200outof488problemscanbeproved
in this way. The Codex model and the Minerva (8B) model give very similar results in solving
problemsonminiF2F:theybothguidetheautomatedprovertosolve40.6%and35.3%ofproblems
onthevalidationandthetestsetsrespectively. ThisiscorroboratedbyLewkowyczetal.(2022)’s
observationthatthesetwomodelshavecomparableperformanceinsolvingmathematicalproblems.
WhenweswitchtotheMinerva(62B)model,thesuccessratesriseupto43.9%and37.7%respec-
tively.Comparedtohuman-writteninformalproofs,itssuccessratesare1.3%higheronthevalidation
setand1.6%loweronthetestset. Intotal,theMinerva(62B)modelisabletosolve199problemson
miniF2F,onefewerthanwithhumanproofs. TheMinerva(540B)modelsolves42.6%and38.9%of
problemsinthevalidationandthetestsetsofminiF2F,alsoresultingin199successfulproofs. The
DSPmethodiseffectiveinguidingtheautomatedproverunderbothsettings: usinghumaninformal
proofsorlanguage-model-generatedinformalproofs. DSPalmostdoublestheprover’ssuccessrate
andresultsinanewstate-of-the-artperformanceonminiF2FwithIsabelle. Moreover,thelarger
Minervamodelsarealmostashelpfulasahumaninguidingtheautomatedformalprover.
5 ANALYSIS
5.1 ABLATIONSTUDIES
Ablation of in-line comments To facilitate the alignment between the informal proofs and the
formalproofsketches, wecopyrelevantsegmentsoftheinformalproofsasin-linecommentsin
the sketches. In the manually constructed prompt examples, these comments are prefixed to the
correspondingIsabellecodeblocks,asshowninFigure2(thetextinred). Wehypothesizethatthis
technique is beneficial for large language models to synthesize formal sketches. To validate this
hypothesis,weperformanablationstudybyremovingthein-linecommentsinthepromptexamples
beforerunningtheexperiment. TheresultsaredisplayedinTable1. Wefindthatwithoutin-line
comments,thesuccessratesdropby4.9%and2.8%onthevalidationandtestsetsrespectively. We
concludethathavingin-linecommentsishelpfulforgeneratingformalproofsketches.
Ablationofinformalproofdrafts DraftinginformalproofsisthefirststepoftheDSPmethod.
Toinvestigatethenecessityofthisstep,weperformanexperimentofformalsketchingandproving
withoutinformalproofsatall.Becauseformalproofsketchesarewritteninthedeclarativeproofstyle,
theyarefairlysimilartotheinformalproofdraftsalready. Concretely,weremovetheinformalproofs
andthein-linecomments(becausetheyarecopiedsegmentsoftheinformalproofs)intheprompt
examples. Thisremovestheneedfortheinformalproofwriter,whetherahumanoraneuralnetwork.
TheresultsofthissetupareshowninTable1. Itcanbeseenthatthesuccessratesonthevalidation
andthetestsetsofminiF2Fdropby3.7%and5.3%respectivelycomparedtowithhuman-written
proofs. Theyarealsoinferiortosuccessratesobtainedwithlanguage-model-generatedinformal
proofs. Thisdemonstratestheimportanceofdraftinginformalproofsbeforesketchingandproving.
Ablation of automated provers Using an autoformalizer to generate proof sketches which are
thencompletedbyanautomatedproveriscentraltoourmethod. Theeffectofutilizinganautomated
prover to close open conjectures in proof sketches is worth studying, so we conduct an ablation
experimentforit. Namely, wereplacetheproofsketchesinthepromptexampleswithcomplete
formalproofs. Thecompleteformalproofsstillfollowthedeclarativeproofstyle,butdonotcontain
anyopenconjectures. Asaresult,thelargelanguagemodelwillalsogeneratefullproofsinsteadof
sketches,andwedirectlycheckwhetherthesegeneratedproofsarevalid. Theresultsinthissetupare
presentedinTable1. Theresultsrevealthatwithoutanautomatedprovertocloseopenconjectures,
thesuccessrateonminiF2Fdecreasesby9.8%and9.0%onthevalidationandtestsetsrespectively.
Thedrasticperformancedifferenceindicatestheessentialroleofautomatedproversinourapproach.
Scaling properties of ablation studies To understand the effect of the ablations on the DSP
method’sscalingproperties,wevarythenumberofautoformalizationattemptsperproblemandplot
thenumberofsuccessfulproofsfoundontheminiF2FdatasetinFigure3(left). Fourmethodsare
contrasted: theoriginalDSPmethodwithhumaninformalproofs,theDSPmethodwithoutin-line
comments, theDSPmethodwithoutinformalproofs, andtheDSPmethodwithoutformalproof
sketches. ItcanbeseenfromthefigurethatwiththeoriginalDSPmethod,theperformancereaches
7
PublishedasaconferencepaperatICLR2023
MiniF2F Problems Solved (out of 488) MiniF2F Problems Solved (out of 488)
200 200
150 150
Human informal proof drafts
100 DSP with human proofs 100 Minerva (540B) proof drafts
Ablation: no in-line comments Minerva (62B) proof drafts
Ablation: no informal proofs Minerva (8B) proof drafts
50 Ablation: no automated provers 50 Codex proof drafts
0 20 40 60 80 100 0 20 40 60 80 100
#Autoformalization Attempts Per Problem #Autoformalization Attempts Per Problem
Figure3:NumberofproblemssolvedonminiF2Fagainstthenumberofautoformalizationattemptsper
problem. Left:ThefiguredisplaystheexperimentscarriedoutwiththeDSPmethodandthreeablationson
it.ThecurvesrepresenttheDSPmethod(blue),formalproofsketcheswithoutthein-linecomments(orange),
withoutinformalproofsaltogether(green),andwithouttheautomatedprovers(red).Right:Thefigurecompares
theexperimentalresultswithinformalproofdraftswrittenbyhumans(blue),the540BMinervamodel(orange),
the62BMinervamodel(green),the8BMinervamodel(red),andtheCodexmodel(purple).
aplateau(nonewproofsarefound)after70autoformalizationattemptsaremadeforeachproblem.
Fortheablationstudywithnoin-linecomments,theplateauisreachedmuchfaster,afteraround50
autoformalizationattempts. Thismethodsolves181problemsintotal. Theablationstudywithout
informalproofsalsoreachesaplateauataround70autoformalizationattempts,solving178problems
intotal. Theablationstudywithoutsketchingcansolve154problemsonminiF2F.Incomparison,
withhumaninformalproofs,only7autoformalizationattemptsarerequiredtoreachthisperformance.
5.2 LANGUAGE-MODEL-GENERATEDPROOFS
Ourexperimentsdemonstratedthatmodel-generatedinformalproofsfromMinervaandCodexcan
helpguideaformaltheoremprover. Inthissection,weanalyzethepropertiesoftheseproofsfurther.
Wefocusontheinformalproofsthe62Band540BMinervamodelsproduceinthissection,asthey
givethebestoverallperformancesandachievethehighestsuccessrateonminiF2F.
MinervahelpssolveoneIMOproblem Interestingly,ourapproachmanagestosolveoneproblem
fromtheInternationalMathematicalOlympiad(imo 1959 1)withaMinerva-generatedsolution,
but not with the human proof. For this problem, we present the successful Minerva-generated
informalproofdraftandtheformalproofinFigure4. Wehypothesizethatthereasonbehindthis
phenomenonisthathumanproofsmightleavegapsbetweenconjecturesthataretoodifficultfor
automated provers to solve. On the other hand, the diversity in language model informal proofs
makessomeofthemmoreamenabletoautomatedprovers. InAppendixC,weanalyzethehuman
andtheMinervainformalproofsforthisproblemingreaterdetail.
ManualevaluationofMinervaproofs Next,weanalyzetherelationshipbetweenthevalidityof
theformalproofsandthecorrectnessoftheinformalproofs. Forouranalysis,werandomlysample
50Minervaproofsofdifferentproblems,whicharethensuccessfullyconvertedtoformalproofs. We
thenmanuallyevaluatethecorrectnessofthese50informalproofs. Amongthem,29proofs(58%)
areentirely correct, 16 areincorrect withaclearly identifiableincorrect step, and 5 “proofs”are
nonsensicalandsimplyrephrasethefinalconclusionsoftheproblems.
Seeingthatatotalof16+5 = 21incorrectinformalproofscanleadtosuccessfulformalproofs,
we study how they guide the automated formal prover despite having flaws themselves. The 21
proofsdivideinto2cases: Inthefirstcase,wefind13problemsforwhichtheinformalproofsare
mostlyignored,andtheautomatedprovercanfindproofsbyitself;Intheother8problems,although
theinformalproofsarewrong,theautoformalizermanagestocorrectthem,eitherbyignoringthe
erroneous steps or by stating their correct versions in the formal proof sketches. This suggests
thattheautoformalizerhassomeunderstandingofthemathematicalstatementsandisnotmerely
translatingthemfromaninformallanguagetoaformallanguage. Itisrobusttoslightnoisesinits
input. InAppendixD,wepresent4casestudiescomparingthehumanandMinervainformalproofs.
Particularly,Figure9showsacompletelycorrectexampleandoneexampleofeachpathologicalcase.
8
sfoorP
lufsseccuS#
sfoorP
lufsseccuS#
PublishedasaconferencepaperatICLR2023
Statement:Provethatthefraction 21n+4 isirreducibleforeverynaturalnumbern.
14n+3
InformalProof(Minerva62B):
Wemustshowthatgcd(21n+4,14n+3)=1.
TheEuclideanalgorithmgives
21n+4=1·(14n+3)+7n+1
14n+3=2·(7n+1)+1.
Sincegcd(7n+1,1)=1,wehavegcd(21n+4,14n+3)=1.
FormalProof:
theoremimo_1959_p1:
fixes n :: nat
shows "gcd (21*n + 4) (14*n + 3) = 1"
proof -
(* The Euclideanalgorithmgives
21n+4=1\cdot(14n+3)+7n+1
14n+3=2\cdot(7n+1)+1. *)
have c0: "21*n + 4 = 1*(14*n + 3) + 7*n + 1"
<ATP> by auto </ATP>
have c1: "14*n + 3 = 2*(7*n + 1) + 1" using c0
<ATP> by auto </ATP>
(* Since \gcd(7n+1,1)=1, we have \gcd(21n+4,14n+3)=1. *)
then have "gcd (7*n + 1) 1 = 1"
using c1
<ATP> by auto </ATP>
then have "gcd (21*n + 4) (14*n + 3) = 1"
using c1
<ATP> by (smt (z3) BitM_plus_oneab_semigroup_add_class.add_ac(1)
add.assoc c0 gcd.commutegcd_add2gcd_add_multmult_numeral_1
numeral_Onenumeral_eq_Sucnumerals(1)semiring_norm(3)) </ATP>
then show ?thesis
using c1
<ATP> by blast </ATP>
qed
Figure 4: IMOproofguidedbyaMinervainformalproofAninformalproofoftheInternationalMath
Olympiadproblemimo 1959 p1generatedbyMinervathatleadstoasuccessfulformalproof. Thesteps
enclosedbytheATPdelimitersaregeneratedbyanautomatedproverandallotherstepsarebytheautoformalizer.
Is there a way to detect which Minerva proofs are correct, without human evaluation? For a
preliminaryinvestigation,wefilteroutalltheproblemsthatcanbesolveddirectlywiththeautomated
proverfromthe50andareleftwith27informalproofs. Ofthese27,21arecompletelycorrect,6
stillcontainsmallerrors,butnonearenonsensical. Withthissimplefilter,weachieveaprecisionof
77.8%andarecallof72.4%inidentifyingcorrectMinervainformalproofs.
ScalingpropertiesofhumanandMinervaproofs Tounderstandtheinfluenceofdifferentinfor-
malproofsourcesonthescalingpropertiesofDSP,weplotthenumberofsuccessfulproofsfoundon
miniF2FagainstthenumberofautoformalizationattemptsperprobleminFigure3(right). Notethat
foreachproblem,wehave1informalproofbyahumanand100informalproofdraftsbyeachlan-
guagemodel. Theonehumanproofisused100timesforformalproofsketchgeneration,whileeach
languagemodelproofdraftisusedonlyonce. Wenoticethatthe62BMinervamodelandthe540B
Minervamodelalwayshavecomparableperformances. Consideringthatthe540BMinervamodelis
morecapableofmathematicalreasoning(Lewkowyczetal.,2022,Table3)thanthe62Bmodel,we
hypothesizethatthebottleneckintheDSPprocessshiftsfromdraftingtosketchingandproving. I.e.,
informalproofdraftsofhigherqualitydonotnecessarilyleadtomoresuccessfulformalproofsdueto
thelimitationofsketchingandproving. Boththe62Bandthe540Bmodelsresultinmoresuccessful
proofsthanthesmaller(8B)MinervamodelandtheCodexmodel,consistentlyforanynumberofat-
tempts. The8BMinervamodelandtheCodexmodelbehavesimilarly,bothfinding185proofsinthe
end. InformalproofswrittenbyhumanshelpsolvemoreproblemsthanthosebyMinervamodelsfor
1−100autoformalizationattempts. However,thedifferenceissmall(1problem)when100aremade.
9
PublishedasaconferencepaperatICLR2023
Drafts Per Problem
MiniF2F Problems Solved (out of 488) 1 5 10 20 100
200 1
180
160 5
150
140
10
120
100
Human informal proof drafts
100 20
more
co
Minerva (540B) proof drafts mpute
80
0 50 100 150 200 100
#Autoformalization Attempts Per Problem 60
Figure5:NumberofproblemssolvedonminiF2F.Left:Thefiguredisplaysthenumberofsuccessfulproofs
withhumanandMinerva-generatedinformalproofdraftswhenupto200autoformalizationattemptsaremade
perproblem.TheMinerva(540B)proofdraftssolvemoreproblemsthanhumanproofdraftswhenmorethan
130attemptsaremadeperproblem.Right:Thefiguredisplaysthenumberofsuccessfulproofswithdifferent
combinationsofdraftsperproblemandsketchesperdraft.ThedraftsarebytheMinerva(540B)model.
NoticingthatthenumberofsuccessfulproofsdoesnotplateaufortheMinerva-generatedproofs,we
investigatehowfurtherincreasingthenumberofautoformalizationattemptschangesthenumberof
problemssolvedforhuman-writtenandlanguage-model-generatedproofs. Foreachproblem,weuse
1humaninformalproofandsample200sketchesforit;weusethesame100informalproofdrafts
bytheMinerva(540B)languagemodelandsample2sketchesforeachdraft. Thetotalnumberof
sketchesperproblemis200inbothsettings. Weplotthenumberofproofssolvedwithrespecttothe
numberofsketchesinFigure5(left).Wefindthatwithhumaninformalproofs,203theorems(106/97
onvalid/test)havesuccessfulformalproofsafter200attempts.Whilewithlanguage-model-generated
informalproofs,209theorems(111/98onvalid/test)havesuccessfulformalproofsafterthesame
numberofautoformalizationattempts. Thissuggeststhatwithenoughautoformalizationattempts,
thediversityinlanguage-model-generatedinformalproofscanbenefittheautomatedformalization
processmorethanthe“ground-truth”humaninformalproofs.
Allocation of autoformalization budget In Section 4, language models generate 100 informal
proofdraftsforeachmathematicalproblemandtheautoformalizerisusedonceoneachdraft. It
islikelythatsomedraftshavethepotentialtobeformalizedcorrectly,butdonotgettoproducea
successfulsketchbecausetherandomlysampledexamplesinthepromptarenotsuitable. Wewould
liketoreducethisvariancebyattemptingautoformalizationmultipletimes,butitisexpensivetodo
so. Thereforeweconductanexperimenttoinvestigatewhattheoptimalwayofallocatingdraftsand
sketchesperdraft. FortheMinerva(540B)model,wevarythenumberofinformalproofdraftsand
thenumberofformalproofsketchesperdraft,undertheconstraintthatthetotalnumberofsketches
perproblemisfewerthan100. WepresentthenumberofminiF2Fproblemssolvedunderevery
combinationinFigure5(right). Theplotshowsthatwhenthetotalnumberofautoformalization
attemptsisfixed,increasingthenumberofdraftsperproblemyieldsthemostsuccessesonminiF2F.
5.3 MEMORIZATION
Thisworkutilizestwolanguagemodelsthathavebeentrainedonalargeamountofinternetdata.
Severalpriorworks(Trinh&Le,2018;Carlinietal.,2022)pointedoutthatsuchmodelscanmemo-
rizesomefractionofthedatatheyencounterduringtraining. Fordraftinginformalproofs,wemainly
experimentedwithMinerva. Lewkowyczetal.(2022,Section5)discussedthememorizationeffects
withinMinervaandconcludedthattheycouldnotfindevidencethatitsabilitiesareduetomemo-
rization. Fortheautoformalizationofproofsketches,theCodex(code-davinci-002)model
wasused. ItstrainingdatawascollectedbeforeJune20213,atwhichtimetheminiF2Fdatasethad
notbeenmadepublic. Sothemodelcannotbenefitfrommemorizingtheexactproblemsandproofs.
Therefore,itisinappropriatetoattributetheabilitiesofmodelsusedinthispapertomemorization.
3https://beta.openai.com/docs/models/codex-series-private-beta
10
sfoorP
lufsseccuS#
devloS
melborP
#
tfarD
reP
sehctekS
PublishedasaconferencepaperatICLR2023
6 CONCLUSION
Inthispaper,weintroducedDraft,Sketch,andProve(DSP),anovelapproachthattakesadvantage
ofinformalproofstosynthesizeformalproofs. Wedemonstrateditsfeasibilityandeffectiveness
byreachingstate-of-the-artperformanceontheminiF2FdatasetwiththeIsabelletheoremprover.
Centraltoourmethodareformalproofsketchesthatmirrorthehigh-levelreasoningstructuresof
informalproofs. Ourablationsshowedthattheabilitytoautomaticallyconvertinformalproofsto
proofsketchesiscriticaltothesuccessofDSP.
OurDSPmethoddiffersfundamentallyfrompreviousapplicationsofmachinelearningtoformal
proofsynthesisintwoaspects. Firstly,whilemostapproachesinthefieldfocusonimprovingproof
search,ourmethodseekstoconstructtheentireformalproofstructurefromtheinformalproofinone
decodingoperation. Thetaskoftheautomatedproveristhensimplifiedtofillingthegapsbetween
intermediateconjectures. Secondly,whileexistingapproachesoperateexclusivelyonformaldata,
DSPbydesignbenefitsfrominformalproofs.
Inthiswork,weutilizedapurelysymbolicautomatedprovertoclosethegapsinproofsketches.
In the future, we aim to equip DSP with more powerful mechanisms, such as HyperTree Proof
Search(Lampleetal.,2022),tobroadenthescopeofprovabletheorems. SimilartoAlphaCode(Li
etal.,2022),wefoundthatthenumberofgenerationsiscrucialforperformance. Thecomputational
costoftheautoformalizerbeingabottleneckinourmethod,weseektodevelopapproachesableto
generatehigh-qualityproofsketchesmoreefficiently.
CODE AND DETAILED RESULTS
Thecodeforexperimentreproductionisatgithub.com/albertqjiang/draft sketch prove.
Additionalresultdetailsformainexperimentscanbefoundinthesamerepository.
ACKNOWLEDGEMENTS
WethankRuiYuanandKunhaoZhengforhelpingwiththeinformalsolutionsusedinourdataset.
WethankChristianSzegedyforhisfeedbackontheearlydraft.
FUNDING DISCLOSURE
AQJandWLaresupportedbytheERCAdvancedGrantALEXANDRIA(ProjectGA742178). AQJ
isalsosupportedbyaPeterhouseGraduateStudentship.
LIST OF CONTRIBUTIONS
AQJconceivedtheideaofusingproofsketchesandconductedtheexperiments. SWconstructedthe
firstversionofthepipelineandtheinitialautoformalizationprompts. JPZproducedtheMinerva
informal proofs, and helped conduct autoformalization experiments. GL proposed to use inline
commentsinformalproofsketchestoimprovealignment. JPZ,YW,andSWperformedthecase
analysesofMinervasolutions. AQJ,TL,GL,SW,andJLcontributedtothedataset. AQJandWL
wrotethefinalautoformalizationprompts. MJisAQJ’sPhDsupervisor. AQJ,GL,SW,andTLwrote
thepaper. YWandGLdirectedtheproject.
REFERENCES
KshitijBansal,SarahM.Loos,MarkusN.Rabe,andChristianSzegedy. Learningtoreasoninlarge
theorieswithoutimitation. CoRR,abs/1905.10501,2019a. URLhttp://arxiv.org/abs/
1905.10501.
KshitijBansal,SarahM.Loos,MarkusN.Rabe,ChristianSzegedy,andStewartWilcox. Holist: An
environmentformachinelearningofhigherorderlogictheoremproving. InKamalikaChaudhuri
andRuslanSalakhutdinov(eds.),Proceedingsofthe36thInternationalConferenceonMachine
11
PublishedasaconferencepaperatICLR2023
Learning,ICML2019,9-15June2019,LongBeach,California,USA,volume97ofProceedings
ofMachineLearningResearch,pp.454–463.PMLR,2019b. URLhttp://proceedings.
mlr.press/v97/bansal19a.html.
BrunoBarras,SamuelBoutin,CristinaCornes,Judicae¨lCourant,Jean-ChristopheFilliatre,Eduardo
Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, et al. The Coq proof
assistantreferencemanual: Version6.1. PhDthesis,Inria,1997.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020,virtual,2020. URLhttps://proceedings.neurips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
NicholasCarlini,DaphneIppolito,MatthewJagielski,KatherineLee,FlorianTrame`r,andChiyuan
Zhang. Quantifyingmemorizationacrossneurallanguagemodels. CoRR,abs/2202.07646,2022.
URLhttps://arxiv.org/abs/2202.07646.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison
Edwards,YuraBurda,NicholasJoseph,GregBrockman,AlexRay,RaulPuri,GretchenKrueger,
MichaelPetrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,BrookeChan,ScottGray,Nick
Ryder,MikhailPavlov,AletheaPower,LukaszKaiser,MohammadBavarian,ClemensWinter,
PhilippeTillet,FelipePetroskiSuch,DavidW.Cummings,MatthiasPlappert,FotiosChantzis,
ElizabethBarnes,ArielHerbert-Voss,WilliamH.Guss,AlexNichol,IgorBabuschkin,S.Arun
Balaji,ShantanuJain,AndrewCarr,JanLeike,JoshuaAchiam,VedantMisra,EvanMorikawa,
AlecRadford,MatthewM.Knight,MilesBrundage,MiraMurati,KatieMayer,PeterWelinder,
BobMcGrew,DarioAmodei,SamMcCandlish,IlyaSutskever,andWojciechZaremba.Evaluating
largelanguagemodelstrainedoncode. ArXiv,abs/2107.03374,2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,ParkerSchuh,
KensenShi,SashaTsvyashchenko,JoshuaMaynez,AbhishekRao,ParkerBarnes,YiTay,Noam
Shazeer,VinodkumarPrabhakaran,EmilyReif,NanDu,BenHutchinson,ReinerPope,James
Bradbury,JacobAustin,MichaelIsard,GuyGur-Ari,PengchengYin,TojuDuke,AnselmLev-
skaya,SanjayGhemawat,SunipaDev,HenrykMichalewski,XavierGarcia,VedantMisra,Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Er-
ica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language model-
ing with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL
https://doi.org/10.48550/arXiv.2204.02311.
JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLiFei-Fei. Imagenet: Alarge-scale
hierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,
pp.248–255.Ieee,2009.
IddoDrori,SarahZhang,ReeceShuttleworth,LeonardTang,AlbertLu,ElizabethKe,KevinLiu,
LindaChen,SunnyTran,NewmanCheng,etal. Aneuralnetworksolves,explains,andgenerates
universitymathproblemsbyprogramsynthesisandfew-shotlearningathumanlevel. Proceedings
oftheNationalAcademyofSciences,119(32):e2123433119,2022.
ThibaultGauthier,CezaryKaliszyk,JosefUrban,RamanaKumar,andMichaelNorrish. Tactictoe:
learningtoprovewithtactics. JournalofAutomatedReasoning,65(2):257–286,2021.
12
PublishedasaconferencepaperatICLR2023
JesseMichaelHan,JasonRute,YuhuaiWu,EdwardW.Ayers,andStanislasPolu. Proofartifact
co-trainingfortheoremprovingwithlanguagemodels. InTheTenthInternationalConferenceon
LearningRepresentations,ICLR2022,VirtualEvent,April25-29,2022.OpenReview.net,2022.
URLhttps://openreview.net/forum?id=rpxJc9j04U.
DanHendrycks,CollinBurns,SauravKadavath,AkulArora,StevenBasart,EricTang,DawnSong,
andJacobSteinhardt. Measuringmathematicalproblemsolvingwiththemathdataset. NeurIPS,
2021.
AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecuriouscaseofneuraltext
degeneration. arXivpreprintarXiv:1904.09751,2019.
AlbertQ.Jiang,WendaLi,JesseMichaelHan,andYuhuaiWu. LISA:LanguagemodelsofIsabelle
proofs. In6thConferenceonArtificialIntelligenceandTheoremProving,2021.
AlbertQ.Jiang,WendaLi,SzymonTworkowski,KonradCzechowski,TomaszOdrzygo´zdz,Piotr
Milos,YuhuaiWu,andMatejaJamnik. Thor: Wieldinghammerstointegratelanguagemodels
andautomatedtheoremprovers. CoRR,abs/2205.10893,2022. doi: 10.48550/arXiv.2205.10893.
URLhttps://doi.org/10.48550/arXiv.2205.10893.
GuillaumeLampleandFranc¸oisCharton. Deeplearningforsymbolicmathematics. InInternational
ConferenceonLearningRepresentations,2020. URLhttps://openreview.net/forum?
id=S1eZYeHFDS.
GuillaumeLample,Marie-AnneLachaux,ThibautLavril,XavierMartinet,AmauryHayat,Gabriel
Ebner,Aure´lienRodriguez,andTimothe´eLacroix. Hypertreeproofsearchforneuraltheorem
proving. CoRR,abs/2205.11491,2022. doi: 10.48550/arXiv.2205.11491. URLhttps://doi.
org/10.48550/arXiv.2205.11491.
AitorLewkowycz,AndersAndreassen,DavidDohan,EthanDyer,HenrykMichalewski,VinayV.
Ramasesh,AmbroseSlone,CemAnil,ImanolSchlag,TheoGutman-Solo,YuhuaiWu,Behnam
Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with
languagemodels. CoRR,abs/2206.14858,2022. doi: 10.48550/arXiv.2206.14858. URLhttps:
//doi.org/10.48550/arXiv.2206.14858.
Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Re´mi Leblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy,
CypriendeMassond’Autume,IgorBabuschkin,XinyunChen,Po-SenHuang,JohannesWelbl,
SvenGowal,AlexeyCherepanov,JamesMolloy,DanielJ.Mankowitz,EsmeSutherlandRobson,
PushmeetKohli,NandodeFreitas,KorayKavukcuoglu,andOriolVinyals. Competition-level
codegenerationwithalphacode. CoRR,abs/2203.07814,2022. doi: 10.48550/arXiv.2203.07814.
URLhttps://doi.org/10.48550/arXiv.2203.07814.
Norman D. Megill and David A. Wheeler. Metamath: A Computer Language
for Mathematical Proofs. Lulu Press, Morrisville, North Carolina, 2019.
http://us.metamath.org/downloads/metamath.pdf.
LeonardodeMoura,SoonhoKong,JeremyAvigad,FlorisvanDoorn,andJakobvonRaumer. The
leantheoremprover(systemdescription). InInternationalConferenceonAutomatedDeduction,
pp.378–388.Springer,2015.
Lawrence C. Paulson. Isabelle - A Generic Theorem Prover (with a contribution by T. Nipkow),
volume828ofLectureNotesinComputerScience. Springer,1994. ISBN3-540-58244-4. doi:
10.1007/BFb0030541. URLhttps://doi.org/10.1007/BFb0030541.
Lawrence C. Paulson. Three years of experience with sledgehammer, a practical link between
automatic and interactive theorem provers. In Renate A. Schmidt, Stephan Schulz, and Boris
Konev(eds.),Proceedingsofthe2ndWorkshoponPracticalAspectsofAutomatedReasoning,
PAAR-2010,Edinburgh,Scotland,UK,July14,2010,volume9ofEPiCSeriesinComputing,pp.
1–10.EasyChair,2010. doi: 10.29007/tnfd. URLhttps://doi.org/10.29007/tnfd.
StanislasPoluandIlyaSutskever. Generativelanguagemodelingforautomatedtheoremproving.
CoRR,abs/2009.03393,2020. URLhttps://arxiv.org/abs/2009.03393.
13
PublishedasaconferencepaperatICLR2023
Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya
Sutskever. Formal mathematics statement curriculum learning. CoRR, abs/2202.01344, 2022.
URLhttps://arxiv.org/abs/2202.01344.
DavidSilver,ThomasHubert,JulianSchrittwieser,IoannisAntonoglou,MatthewLai,ArthurGuez,
MarcLanctot,LaurentSifre,DharshanKumaran,ThoreGraepel,etal. Ageneralreinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140–1144,2018.
IlyaSutskever,OriolVinyals,andQuocVLe. Sequencetosequencelearningwithneuralnetworks.
Advancesinneuralinformationprocessingsystems,27,2014.
DonaldSyme. DECLARE:Aprototypedeclarativeproofsystemforhigherorderlogic. Citeseer,
1997.
ChristianSzegedy. Apromisingpathtowardsautoformalizationandgeneralartificialintelligence. In
ChristophBenzmu¨llerandBruceR.Miller(eds.),IntelligentComputerMathematics-13thInterna-
tionalConference,CICM2020,Bertinoro,Italy,July26-31,2020,Proceedings,volume12236of
LectureNotesinComputerScience,pp.3–20.Springer,2020.doi:10.1007/978-3-030-53518-6\ 1.
URLhttps://doi.org/10.1007/978-3-030-53518-6_1.
TrieuH.TrinhandQuocV.Le.Asimplemethodforcommonsensereasoning.CoRR,abs/1806.02847,
2018. URLhttp://arxiv.org/abs/1806.02847.
QingxiangWang,ChadE.Brown,CezaryKaliszyk,andJosefUrban. Explorationofneuralmachine
translationinautoformalizationofmathematicsinmizar. InJasminBlanchetteandCatalinHritcu
(eds.),Proceedingsofthe9thACMSIGPLANInternationalConferenceonCertifiedPrograms
andProofs,CPP2020,NewOrleans,LA,USA,January20-21,2020,pp.85–98.ACM,2020. doi:
10.1145/3372885.3373827. URLhttps://doi.org/10.1145/3372885.3373827.
SeanWelleck,JiachengLiu,RonanLeBras,HannanehHajishirzi,YejinChoi,andKyunghyunCho.
Naturalproofs: Mathematicaltheoremprovinginnaturallanguage. InThirty-fifthConferenceon
NeuralInformationProcessingSystemsDatasetsandBenchmarksTrack(Round1),2021. URL
https://openreview.net/forum?id=Jvxa8adr3iY.
Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover:
Groundedmathematicalproofgenerationwithlanguagemodels.CoRR,abs/2205.12910,2022.doi:
10.48550/arXiv.2205.12910. URLhttps://doi.org/10.48550/arXiv.2205.12910.
FreekWiedijk. Formalproofsketches. InStefanoBerardi,MarioCoppo,andFerruccioDamiani
(eds.), Types for Proofs and Programs, International Workshop, TYPES 2003, Torino, Italy,
April 30 - May 4, 2003, Revised Selected Papers, volume 3085 of Lecture Notes in Computer
Science, pp. 378–393. Springer, 2003. doi: 10.1007/978-3-540-24849-1\ 24. URL https:
//doi.org/10.1007/978-3-540-24849-1_24.
FreekWiedijk. Formalproof–gettingstarted. NoticesoftheAmericanMathematicalSociety,55:
1408–1414,2008.
Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Baker Grosse. INT: An inequality benchmark
for evaluating generalization in theorem proving. In International Conference on Learning
Representations,2021. URLhttps://openreview.net/forum?id=O6LPudowNQm.
YuhuaiWu,AlbertQ.Jiang,WendaLi,MarkusN.Rabe,CharlesStaats,MatejaJamnik,andChristian
Szegedy. Autoformalization with large language models. CoRR, abs/2205.12615, 2022. doi:
10.48550/arXiv.2205.12615. URLhttps://doi.org/10.48550/arXiv.2205.12615.
Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. In
InternationalConferenceonMachineLearning(ICML),2019.
Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. miniF2F: a cross-system benchmark
for formal olympiad-level mathematics. In The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL
https://openreview.net/forum?id=9ZPegFuFTFv.
14
PublishedasaconferencepaperatICLR2023
APPENDIX
A CONJECTURES AND THE DECLARATIVE PROOF STYLE
InteractivetheoremproverssuchasIsabelleandMizaruseadeclarativeproofstyle(Syme,1997),in
whichaproofisinterleavedwithconjecturesandtheircorrespondingproofs. Syme(1997)stated
thatthelistofconjecturesinadeclarativeproofshouldbeanalogoustoaproofsketchfoundina
mathematicaltextbookandsufficientlyconvincingforthereader. Inpractice,ITPusersoftenprovea
theorembywritingdownalistofconjectures(a“formalsketch”),thenattempttofindaproofof
eachconjecture(filla“gap”)withanautomatedsystem.
B SLEDGEHAMMER
Sledgehammer(Paulson,2010)isapowerfulsystemthatautomatesreasoningwiththeinteractive
theoremproverIsabelle. Itworksbyflatteningthegoalsencodedinthehigher-orderlogicusedby
Isabelle/HOLintootherlogics(e.g.,first-orderlogic)whichcanthenbefedintoautomatedtheorem
provers such as E 4, CVC4 5, Z3 6, Vampire 7, and SPASS 8. If any of these automated theorem
proverssucceedsinfindingtheproofintheirowncorrespondingformat,Sledgehammerreconstructs
the proof in Isabelle/HOL with certified provers (metis, meson, and smt), which is relatively
moreinterpretablebyhumans.
As a practical example of using Sledgehammer, one can declare a conjecture in Isabelle/HOL:
have "4 dvd (a::nat) =⇒ 2 dvd a" and call Sledgehammer immediately afterwards.
IfSledgehammersucceeds,itwillreturnaproofstepthatprovestheconjecture. Inthisexample,
thestepisby (meson dvd trans even numeral),whichusesthemesonresolutionprover
andtwofacts: thatthedivisionrelationistransitiveandthat4isanevennumber. IfSledgehammer
doesnotfindtheproofortimeouts,itwillreportfailure.
C A PROOF TO AN INTERNATIONAL MATHEMATICAL OLYMPIAD PROBLEM
WiththeMinerva-generatedsolutions,aprooftotheproblemimo 1959 p1isdiscovered. Thisis
thefirstproblemofthefirsteverInternationalMathematicalOlympiad(IMO).Theinformalproblem
statement,Minerva-generatedinformalsolution,andDSP’sformalproofareshowninFigure4.
InFigure4,wecanseethattheautoformalizerinDSP(alargelanguagemodel),copiesoverpartsof
theinformalproofgeneratedbyMinervaasin-linecommentstoprecedetheircorrespondingformal
proofblocks. Theformalproofdoesnotusethefirstsentenceoftheinformalproofsolutionasit
isalreadyidenticaltotheformalstatement. Wealsonoticethatthelargelanguagemodelselects
relevantpremisesafterwritingdowntheconjectures(thestepsstartingwithusing)despitenot
everypremiseisstrictlyneeded.
Theformalproofcreates5conjectures(4havestatementsand1showstatement)whichareall
subsequentlyprovedbyourautomatedtheoremprover. Thesteptoprovethestatementhave "gcd
(21*n + 4) (14*n + 3) = 1" involves 2 verified low-level provers smt and z3 and 10
lemmas/factsfromoutsidethescopeofthelanguagemodel. Itishighlyunlikelythateitherthelarge
languagemodelortheautomatedtheoremprovercanfinishthisproofonitsown.
Unsuccessfulhuman-writtenproof. Incontrast,thehuman-writteninformalproofofthisIMO
problemdidnotleadtoasuccessfulformalproof. Thehuman-writtenproofis:
Denotingthegreatestcommondivisorofa,bas(a,b),weusetheEuclideanalgorithm:
(21n+4,14n+3)=(7n+1,14n+3)=(7n+1,1)=1
4https://wwwlehre.dhbw-stuttgart.de/sschulz/E/E.html
5https://cvc4.github.io/index.html
6https://github.com/Z3Prover/z3
7https://vprover.github.io/
8https://www.spass-prover.org/download/index.html
15
PublishedasaconferencepaperatICLR2023
Itfollowsthat 21n+4 isirreducible. Q.E.D.
14n+3
A key difference between the Minerva proof and the human proof is the way that invoking the
Euclideanalgorithmisdescribed. TheMinervaproofexplicitlywritesouttheresultsoftheEuclidean
algorithm (e.g. 21n+4 = 1·(14n+3)+7n+1), which are translated into the sketch (c1 in
Figure4). Thehumanproofintroducesnewnotationtoexpresstheresultsindirectlyintermsof
greatestcommondivisors,whichendsupbeinglesssuitableforsketching. Forexample,belowis
asketchgeneratedwiththehumanproof,whichhasaconjecturethatissemanticallyincorrectand
hencecannotbeclosedbytheautomatedprover:
theorem
fixes n :: nat
shows "gcd (21*n + 4) (14*n + 3) = 1"
proof -
have "(21*n + 4, 14*n + 3) = (7*n + 1, 14*n + 3)"
ATP (* <--- UNSUCCESSFUL *)
also have "... = (7*n + 1, 1)"
ATP
finallyshow ?thesis
ATP
qed
16
PublishedasaconferencepaperatICLR2023
D MORE CASE ANALYSES OF HUMAN AND MINERVA INFORMAL PROOFS
Statement:Showthatforanyrealnumberx,if|x−1|+|x|+|x+1|=x+2,then0≤x≤1.
InformalProof(Human):
Ifx≤−1,then|x−1|+|x|+|x+1|=−(x−1)−x−(x+1)=−3x.So,−3x=x+2andx=−1,
2
whichisacontradiction.If−1<x<0,then|x−1|+|x|+|x+1|=−(x−1)−x+(x+1)=2−x.
So,2−x = x+2andx = 0,whichisacontradiction. Ifx > 1,then|x−1|+|x|+|x+1| =
x−1+x+(x+1)=3x.So,3x=x+2andx=1,whichisacontradiction.Asaresult,theonly
possiblevaluesforxarebetween0and1and0≤x≤1.
FormalProof:
theoremalgebra_absxm1pabsxpabsxp1eqxp2_0leqxleq1:
fixes x ::realassumes "abs (x - 1) + abs x + abs (x + 1) = x + 2"
shows "0 \<le> x \<and> x \<le> 1"
proof -
(* If x≤−1, then |x−1|+|x|+|x+1|=−(x−1)−x−(x+1)=−3x.
So, −3x=x+2 and x=−1, which is a contradiction. *)
2
have c0: "x \<le> -1 \<Longrightarrow> False"
proof -
assume c1: "x \<le>-1"
have c2: "abs(x-1) + abs x + abs(x+1) = -(x-1) - x - (x+1)" using c1
<ATP> by auto </ATP>
then have c3: "abs(x-1) + abs x + abs(x+1) =-3*x" <ATP> by auto </ATP>
then have c4: "-3*x = x+2" usingassms c3 <ATP> by auto </ATP>
then have c5: "x =-1/2" <ATP> by auto </ATP>
then show ?thesisusing c1 <ATP> by auto </ATP>
qed
(* If −1<x<0, then |x−1|+|x|+|x+1|=−(x−1)−x+(x+1)=2−x.
So, 2−x=x+2 and x=0, which is a contradiction. *)
have c6: "-1 < x \<Longrightarrow> x < 0 \<Longrightarrow> False"
proof -
assume c7: "-1 < x" assume c8: "x < 0"
have c9: "abs(x-1) + abs x + abs(x+1) = -(x-1) - x + (x+1)" using c7 c8
<ATP> by auto </ATP>
then have c10: "abs(x-1) + abs x + abs(x+1) = 2-x" <ATP> by auto </ATP>
then have c11: "2-x = x+2" usingassms c10 <ATP> by auto </ATP>
then have c12: "x = 0" <ATP> by auto </ATP>
then show ?thesisusing c8 <ATP> by auto </ATP>
qed
(* If x>1, then |x−1|+|x|+|x+1|=x−1+x+(x+1)=3x.
So, 3x=x+2 and x=1, which is a contradiction. *)
have c13: "x > 1 \<Longrightarrow> False"
proof -
assume c14: "x > 1"
have c15: "abs(x-1) + abs x + abs(x+1) = x-1 + x + (x+1)" using c14
<ATP> by auto </ATP>
then have c16: "abs(x-1) + abs x + abs(x+1) = 3*x" <ATP> by auto </ATP>
then have c17: "3*x = x+2" usingassms c16 <ATP> by auto </ATP>
then have c18: "x = 1" <ATP> by auto </ATP>
then show ?thesisusing c14 <ATP> by auto </ATP>
qed
(* As a result, the onlypossiblevalues for x are between 0 and 1 and 0≤x≤1.*)
then show ?thesisusing c0 c6 c13 <ATP> by fastforce </ATP>
qed
Figure6: Algebraexamplewithhumaninformalproof. Ahumaninformalproofissuccessfulinguiding
theformalproofsketchtodividetheproblemintothreecasesanddrivecontradictionstoeachofthembefore
showingthefinalobjective.Thecomplexityandtheconsistencyoftheformalproofsketchareimpressive.
17
PublishedasaconferencepaperatICLR2023
Statement:Giventhatf +3z=11and3(f −1)−5z=−68,showthatf =−10andz=7.
InformalProof(Minerva62B):
Wecansolveforf inthefirstequation,obtainingf =11−3z.
Thenwecansubstitutethisexpressionforf intothesecondequation,
obtaining3(11−3z−1)−5z=−68.
Solvingforz,weobtainz=7.
Then,wecansubstitutethisvalueofzintotheexpressionforf,obtainingf =11−3·7=−10.
FormalProof:
theoremalgebra_2varlineareq_fp3zeq11_3tfm1m5zeqn68_feqn10_zeq7:
fixes f z::complex
assumes h0: "f + 3*z = 11"
and h1: "3*(f - 1) - 5*z =-68"
shows "f =-10 \<and> z = 7"
proof -
(* We can solve for f in the first equation, obtaining f = 11 - 3z. *)
have c0: "f = 11 - 3*z"
using h0
<ATP> by (auto simp: field_simps) </ATP>
(* Then we cansubstitutethisexpressionfor f into the secondequation,
obtaining3(11 - 3z - 1) - 5z =-68. *)
have c1: "3*(11 - 3*z - 1) - 5*z =-68"
using h1 c0
<ATP> by auto </ATP>
(* Solving for z, we obtain z = 7. *)
have c2: "z = 7"
using c1
<ATP> by auto </ATP>
(* Then, we cansubstitutethis value of z into theexpressionfor f,
obtaining f = 11 - 3 \cdot 7 =-10.*)
have "f = 11 - 3*7"
using c0 c2
<ATP> by auto </ATP>
then have c3: "f =-10"
<ATP> by auto </ATP>
show ?thesis
using c2 c3
<ATP> by auto </ATP>
qed
Figure7: AlgebraexamplewithMinervainformalproof.AninformalproofgeneratedbyMinervathatled
toasuccessfulformalproof.Theautoformalizergeneratedaproofsketchcontainingalllinesoftheformalproof
exceptforthosedelimitedbytheATPtags.Thesketchisstructuredaccordingtotheinformalproof,containing
fiveintermediateconjecturesbasedontheinformalproof.Theautoformalizergeneratedin-linecommentsinthe
proofsketch(showninred),whichcorrectlyidentifiedanalignmentbetweentheformalandinformalproofs.
18
PublishedasaconferencepaperatICLR2023
Statement:Foranyintegera,showthata2 ≡ Statement:Foranyintegera,showthata2 ≡
0 (mod 4)ora2 ≡1 (mod 4). 0 (mod 4)ora2 ≡1 (mod 4).
InformalProof(Minerva62B): InformalProof(Human):
Leta=2kforsomeintegerk. a (mod 4)∈{0,1,2,3}.
Thena2 =4k2. Usingthatforanynaturalnumberk, a ≡ b
Since4k2 isdivisibleby4,wehavea2 ≡ 0 (mod 4)impliesak ≡bk (mod 4),wehave
(mod 4). a2 (mod 4)∈{0,1,4,9}.
Now,leta=2k+1forsomeintegerk. Since4 ≡ 0 (mod 4)and9 ≡ 1 (mod 4),
Thena2 =4k2+4k+1=4(k2+k)+1. theresultfollows.
Sincek2+kisaninteger,4(k2+k)+1is
notdivisibleby4.
Thus,a2 ≡1 (mod 4).
FormalProof: FormalProof:
theoremnumbertheory_sqmod4in01d: theoremnumbertheory_sqmod4in01d:
fixes a :: int fixes a :: int
shows "(aˆ2 mod 4 = 0) shows "(aˆ2 mod 4 = 0)
\<or> (aˆ2 mod 4 = 1)" \<or> (aˆ2 mod 4 = 1)"
proof (cases "even a") proof -
case True (* a \pmod 4 \in {0, 1, 2, 3}. *)
(* Let a=2k for someinteger k. have c0: "a mod 4 \<in> {0, 1, 2, 3}"
Then aˆ2=4kˆ2. <ATP> by auto </ATP>
Since 4kˆ2 is divisible by 4, (* Using that for any naturalnumber k,
we have aˆ2 \equiv 0 \pmod{4}.*) a \equiv b \pmod 4 implies
thenobtain k where "a=2*k" aˆk \equiv bˆk \pmod 4,*)
usingevenE have "aˆ2 mod 4 = (a mod 4)\<ˆsup>
<ATP> by auto </ATP> 2 mod 4" <ATP> by (smt (z3)
then have "aˆ2 = 4*kˆ2" numeral_eq_Sucpower_mod) </ATP>
unfoldingpower2_eq_square (* we have
<ATP> by auto </ATP> aˆ2 \pmod 4 \in {0, 1, 4, 9}. *)
then have "aˆ2 mod 4 = 0" also have "... \<in> {0, 1, 4, 9}"
<ATP> by auto </ATP> using c0
then show ?thesis <ATP> by auto </ATP>
<ATP> by auto </ATP> (* Since 4 \equiv 0 \pmod 4 and
next 9 \equiv 1 \pmod 4,
case False the resultfollows. *)
(* Now, let a=2k+1 for someinteger k. finallyshow ?thesis
Then aˆ2=4kˆ2+4k+1=4(kˆ2+k)+1. <ATP> by auto </ATP>
Since kˆ2+k is an integer, qed
4(kˆ2+k)+1 is not divisible by 4.
Thus, aˆ2 \equiv 1 \pmod{4}.*)
thenobtain k where "a=2*k+1"
using oddE <ATP> by auto </ATP>
then have "aˆ2 = 4*kˆ2+4*k+1"
unfoldingpower2_eq_square <ATP>
by (auto simp: field_simps) </ATP>
then have "aˆ2 mod 4 = 1"
<ATP> by presburger </ATP>
then show ?thesis
<ATP> by auto </ATP>
qed
Figure 8: Alternativeproofs: Minerva-generated(left)andhuman-written(right). Inbothproofs, the
formalsketchisstructuredbasedontheinformalproof.TheMinervainformalproofanditssketchbreakthe
proofintoevenandoddcases.Thesecasesarenotexplicitlystatedintheinformalproof,andtheformalsketch
makesthemexplicit(cases“evena”).Eachcasehasthreeconjecturesthataredirectlybasedontheinformal
proof.Thesketchexcludespartsthatarenotneededintheformalproof,forinstance“Sincek2+kisaninteger,
4(k2+k)+1isnotdivisibleby4”.ThehumanproofusesadifferentstrategythantheMinervaproof,basedon
thefactsthata (mod 4)∈{0,1,2,3}anda2 (mod 4)∈{0,1,4,9}.Thesketchusestheseasconjectures,
synthesizesanintermediatestepnotintheinformalproof,andexcludesthelaststepoftheinformalproof.
19
PublishedasaconferencepaperatICLR2023
Statement:Foreachplumbingrepairjob,Mr. Statement: Showthatforanyrealnumberx
WrenchchargesN dollarsforcomingoutto andanynaturalnumbern, ifx > −1, then
thehouseplusxdollarsperhourthatheworks (1+nx)≤(1+x)n.
atthehouse. Hecharged$97foraone-hour
repairjoband$265forafive-hourrepairjob. InformalProof(Minerva62B):
Whatishischargeforatwo-hourrepairjob? Thisistrueforn=1.Now,supposethatthis
Showthatitis$139. istrueforn.
Thenwehavethat
InformalProof(Minerva62B):
LetN bethenumberofdollarschargedfor
(1+nx)n =1+n2x+n3x2+···+nxn
comingouttothehouseandletxbethenum-
berofdollarsperhourthatMr.Wrenchworks. ≤1+nx+n2x2+···+nxn =(1+x)n
Fortheone-hourrepairjob,wehavethatthe
totalcostisN +x=97. and
Forthefive-hourrepairjob,wehavethatthe
totalcostisN +5x=265.
(1+nx)n+1 =(1+nx)n(1+nx)
Wesubtractthefirstequationfromthesecond ≤(1+x)n(1+x)=(1+x)n+1.
equationtoobtain4x=168⇒x=42.
Pluggingthisbackintothefirstequation,we Therefore,thisistrueforn+1.
obtainthatN =55.
Therefore,foratwo-hourrepairjob,thetotal FormalProof:
costis55+2x=55+2(42)= $139 . theoreminduction_1pxpownlt1pnx:
fixes x :: real and n :: nat
FormalProof: assumes "-1 < x"
theoremmathd_algebra_160:
shows "(1 + n*x) \<le> (1 + x)ˆn"
proof (induct n)
fixes n x ::real
case 0
assumes "n + x = 97"
then show ?case
and "n + 5 * x = 265"
<ATP> by auto </ATP>
shows "n + 2 * x = 139"
next case (Suc n)
proof -
then show ?case
(* We subtract the firstequation
<ATP> by (smt (z3)
from the secondequation to obtain
Bernoulli_inequalityassms) </ATP>
4x=168⇒x=42. *)
qed
have "4 * x = 168"
usingassms <ATP> by auto </ATP>
then have "x = 42"
<ATP> by auto </ATP> Statement:Determinetheremainderof1529
(* Pluggingthis back into
(mod6).Showthatitis5.
the first equation, we obtainthat
N=55. *)
InformalProof(Minerva62B):
then have "n = 55" Bydefinition,1529≡5 (mod 6).
usingassms <ATP> by auto </ATP>
(* Therefore, for a two-hourrepair
FormalProof:
job, the total cost is
theoremmathd_numbertheory_551 :
55+2x=55+2(42)=$139. *)
"1529 mod 6 = (5::nat)"
then show ?thesis
<ATP> by auto </ATP>
<ATP> by (smt (z3) (cid:104)x = 42(cid:105)) </ATP>
qed
Figure 9: ThreeTypesofMinervaproofs: correctproof(left),incorrectproof(righttop),nonsensical
proof(rightbottom)InthecorrectMinervaproof,theformalsketchisstructuredbasedontheinformalproof
andstepsarewell-aligned.IntheincorrectMinervaproof,thestep”Thisistrueforn=1”iscorrectedbyCodex
intheformalsketchto”case0”whichstartsthebasecasewithn=0sincenaturalnumbersinclude0. This
isanexplicitcorrectionmadebyCodexandmakesaslightlyincorrectMinervaproofformalizedsuccessfully.
Lastly,themeaninglessproofcontainsonlyasinglestatementwithoutanycalculationorjustification.However,
Codexalsochoosestodirectlyshowthestatementwithoutanycalculation.Thissuggeststhattheproblemitself
couldbeconsideredsimplebyCodex.
20

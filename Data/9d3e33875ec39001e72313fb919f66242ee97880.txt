LINGUISTICUNITDISCOVERYFROMMULTI-MODALINPUTSINUNWRITTEN
LANGUAGES:SUMMARYOFTHE“SPEAKINGROSETTA”JSALT2017WORKSHOP
OdetteScharenborg1∗,LaurentBesacier2,AlanBlack3,MarkHasegawa-Johnson4,
FlorianMetze3,GrahamNeubig3,SebastianStu¨ker5,PierreGodard6,MarkusMu¨ller5,
LucasOndel8,ShrutiPalaskar3,PhilipArthur3,FrancescoCiannella3,MingxingDu7,
ElinLarsen7,DannyMerkx1,RachidRiad7,LimingWang4,EmmanuelDupoux7 †
1 RadboudUniversity,2 LIG-UnivGrenobleAlpes(UGA),3 CarnegieMellonUniversity,
4 UniversityofIllinois,5 KarlsruheInstituteofTechnology,6 LIMSICNRS,
7 ENS/CNRS/EHESS/INRIA,8 BrnoUniversity.
ABSTRACT Recently, different approaches have been proposed to
build ASR systems for such low-resource languages. One
We summarize the accomplishments of a multi-disciplinary
strandofresearchfocusesondiscoveringthelinguisticunits
workshop exploring the computational and scientific issues
ofthelow-resourcelanguagefromtherawspeechdata,while
surrounding the discovery of linguistic units (subwords and
assuming no other information about the language is avail-
words) in a language without orthography. We study the re-
able, and using these to build ASR systems (zero resource
placement of orthographic transcriptions by images and/or
approach; e.g., [2, 3, 4, 5, 6]). Another strand of research
translatedtextinawell-resourcedlanguagetohelpunsuper-
focuses on building ASR systems using speech data from
viseddiscoveryfromrawspeech.
multiple languages, thus trying to create universal or cross-
Index Terms— unwritten languages, multi-modal data, lingual ASR systems [7, 8, 9, 10]. Children though, when
unsupervised unit discovery, image retrieval, machine trans- learning a language, also have information besides the audi-
lation. tory input available, primarily in the visual modality. This
has led to a new strand of research which uses visual infor-
1. INTRODUCTION mation, from images, to discover word-like units from the
speech signal using speech-image associations [11, 12, 13].
To develop speech and language technology (SLT) large The“SpeakingRosetta”projectatthe2017FrederickJelinek
amounts of annotated data are required. However, for many Memorial Summer Workshop, which took place at Carnegie
languages in the world, not enough speech data is available, Mellon University, Pittsburgh, pushed this idea further by
ortheselacktheannotationsneededtotrainanASRsystem usingmulti-modaldatasetsthatnotonlyincludeimages, but
[1]. Moreover, anestimatedhalfofthehumanlanguagesdo alsoincludetranslationsinahigh-resourcelanguage. Thisis
not have an orthography, and many others do not use it in a aninterestingextensionasparalleldatabetweenspeechfrom
consistentfashion. Thisrepresentsmillionsofpotentialusers an unwritten language and translations of that speech signal
thatasyetcannotbeservedbyspeechtechnologies. Asany inanotherlanguagecaneasilybecollected[14].
human 4-year-old demonstrates, however, it is theoretically
Thispapersummarizestheaccomplishmentsofthemultidis-
possible to learn a language communication system before
ciplinary “Speaking Rosetta” workshop which explored the
learningtoreadandwrite,fromrawsensorysignalsandwith
computationalandscientificissuessurroundingthediscovery
onlylimitedhumansupervision.
oflinguisticunits(subwordsandwords)inalanguagewithout
∗Correspondingauthor:O.Scharenborg@let.ru.nl orthography, through replacing the orthographic transcrip-
†TheworkreportedherewasstartedatJSALT2017inCMU,Pittsburgh, tions typically used for training an ASR system by images
andwassupportedbyJHUandCMUviagrantsfromGoogle, Microsoft,
and/or translations in a well-resourced language. The fo-
Amazon,Facebook,Apple. ThisworkusedtheExtremeScienceandEngi-
cus of the project was on discovering intermediate symbolic
neeringDiscoveryEnvironment(XSEDE),whichissupportedbyNSFgrant
number OCI-1053575. Specifically, it used the Bridges system, which is units and investigating their role in building SLT systems.
supportedbyNSFawardnumberACI-1445606,atthePittsburghSupercom- We concentrated on 4 tasks: two with symbolic units (unit
putingCenter(PSC).OSwaspartiallysupportedbyaVidi-grantfromNWO
discovery and speech synthesis) and two end-to-end tasks
(276-89-003).PGwasfundedbytheFrenchANRandtheGermanDFGun-
without the need for explicit symbolic units (speech2image
dergrantANR-14-CE35-0002(BULBproject). MD,EL,RRandEDwere
fundedbytheEuropeanResearchCouncil(ERC-2011-AdG-295810BOOT- andspeech2translation).
PHON),andANR-10-LABX-0087IECandANR-10-IDEX-0001-02PSL*.
8102
beF
41
]LC.sc[
1v29050.2081:viXra
(Google MT) for all 40K captions, as well as Japanese tok-
enization.
SPEECH-COCO-synthetic[17,18]isanaugmentation
of MSCOCO [19] which consists of 123,287 images with
five different descriptions per image. We generated speech
captions using text-to-speech (TTS) synthesis resulting in
616,767 spoken captions (more than 600h) paired with im-
ages. Disfluenciesandspeedperturbationwereaddedtothe
signalinordertomakeitsoundmorenatural.
The How-To dataset is an English open domain in-
structional videos (uploaded by users with personal video
recorders)datasetofabout480hoursofspeech. Eachvideo
is broken down into short utterances of about 8-10 seconds
each. Transcriptions consist of summaries of what was spo-
ken. Thecleanest45hoursoutofthe480hourswereused.
From the Spoken Dutch Corpus (CGN, [20]), 64 hours
ofreadspeechwereused.
2.2. Evaluation
Fig.1. Functionalblocksofthe“SpeakingRosetta”project
Thetwotypesoftask,i.e.,linguisticunitdiscoveryandend-
to-end,wereevaluatedusingabatteryoftestswhichinclude
qualitative measures, e.g., the MCD (see Section 3.2 [21])
2. OVERVIEWOF“SPEAKINGROSETTA”
and the ABX task (which compares the similarity between
Figure1showsavisualrepresentationoftheend-to-endsys- discoveredunitsandgroundtruthlabelsorbetweendifferent
tems,andstructure,oftheRosettaproject.Theunitdiscovery typesofacousticfeatures; [22,23])fortheevaluationofthe
strand (see Section 3.1) focused on discovering ’acoustic discovered units, and quantitative measures, such as BLEU
units’intheformofarticulatoryfeaturesor(pseudo)phones score, error rates, and word discovery metrics (see for more
from raw speech. These acoustic units were used to build details[24,25]).
speech synthesis systems (Section 3.2), to transform the 2.3. XNMTToolkit
speech input into symbolic units (pseudo words or pseudo
The end-to-end systems used during the project were built
phones) and these units were used for several end-to-end
using the neural machine translation toolkit XNMT [26],
tasks. Theend-to-endtasks(seeSection4)usedanencoder-
whichwasgreatlyimprovedduringthecourseofthisproject.
decoder framework to translate speech or retrieve images
XNMT is a sequence-to-sequence neural network toolkit
fromspeech. Alignment/attentionmodelsweretakenadvan-
which reads in a sequence of (variable-length) inputs, and
tageof. Twooftheseend-to-endtasksarehighlightedbelow:
then generates a different sequence of (variable-length) out-
speechtotranslationandspeechtoimageretrieval.
put. It consists of a library of standard components. The
2.1. Databases libraryisdesignedsothatexistingcomponentscanbeeasily
re-arranged to run new experiments, and new components
Fivemulti-andunimodaldatabaseswereused. TheMboshi
can be easily added. Available components are categorized
(Bantu language spoken in Congo-Brazzavile) corpus1 con-
asembedders(e.g.,onehot,linear,andcontinuousvectorem-
sists of 5k speech utterances (approximately 4 hours of
bedders),encoders(e.g.,CNN,LSTM,andpyramidalLSTM
speech) in Mboshi aligned to French text. The data set also
encoders), attention models (e.g., dot product, bilinear, and
contains linguists’ transcriptions in Mboshi in the form of a
MLP attention models), decoders (e.g., a RNN decoder ap-
non-standard graphemic form close to the language phonol-
plied to the state vector of the encoder), and error metrics
ogy[1,15].
(e.g.,BLEU,cross-entropy,worderrorrate).
The FlickR-real speech database is a tri-modal (speech,
translatedtext,images)corpus. TheFlickRcorpuscontains5 3. TASKSWITHSYMBOLICUNITS
differentnaturallanguagetextcaptions(obtainedusingAma-
zon Mechanical Turk; AMT) for each of 8000 images cap- 3.1. Unitdiscovery
turedfromtheFlickRphotosharingwebsite. AMTwasalso
Threedifferentunitdiscoverysystemswereimplementedthat
used by [16] to obtain 40K spoken versions of the captions.
usedout-of-domainlanguagestohelpunitdiscoverythrough
We augmented this corpus by adding Japanese translations
(almost)zero-shotadaptation.
1ThedatasetwillbemadeavailableforfreebyELRA;itscurrentversion In the unsupervised phoneme discovery - Bayesian
isonlineat:https://github.com/besacier/mboshi-french-parallel-corpus acoustic unit discovery (AUD) approach, pseudo-phones
weregeneratedfromtheAUDsystemof[3]withtwomajor TTS system used is Clustergen [21]. Clustergen works well
modifications. First, the truncated Dirichlet process of [3] with small corpora because it treats each frame of the train-
was replaced by a symmetric Dirichlet distribution, which ing corpus as a training example, rather than each segment.
providesagoodandyetsimpleapproximationoftheDirich- Thismakesitsuitableforourlow-resourcescenario. Thein-
let Process [27]. Second, to cope with larger databases, the puttoClustergenisawaveformfileplussymbolicsequences
Variational Bayes Inference algorithm originally used in [3] of“phones”;theoutputisasimplesynthesizerandaMelcep-
was replaced with the faster Stochastic Variational Bayes straldistortionmeasure(MCD)[31]onheldoutdata. MCD
Inferencealgorithm. Experiments showedthatthese modifi- measurestheaveragedistancebetweenthelog-spectraofthe
cations,whileconsiderablyspeedingupthetraining,yielded synthetic and natural utterances, and has been demonstrated
negligibledropinaccuracy. Also,anextensionofthismodel tobeanextremelysensitivemeasureoftheperceivednatural-
was explored: the AUD model was embedded into a Varia- ness of speech utterances, e.g., an MCD difference between
tionalAuto-Encoderleadingtoaspecificcaseoftherecently two synthesis algorithms of 0.3 (on the same test corpus) is
developedStructuredVariationalAuto-Encodermodel[28]2. usuallyperceptiblebyhumanlistenersasasignificantdiffer-
The universal articulatory features and phoneme in- enceinperceivednaturalness[21].
ventory discovery approach aimed at deriving phone-like TTS was used to generate speech in two tasks. The
units using the setup presented in [29]. It consists of three first task is a new speech technology task, which we call
steps: 1) Detection of pseudo-phone boundaries 2) Extrac- image2speech [32]. Image2speech is similar to automatic
tionoflanguage-universalarticulatoryfeatures(AFs)foreach image captioning, but can reach people whose language
segment.3)Clusteringofthesegmentsbasedontheextracted does not have a natural or easily used written form. The
AFs. Sevenarticulatoryfeaturedetectorsusingdifferentnet- image2speech pipeline consists of a VGG16 visual object
work architectures were trained using data from multiple recognizer which converts each image into a sequence of
source languages, and evaluated cross-lingually. Results in- feature vectors. XNMT accepts image feature vectors as
dicated that the LSTM-based feature extractors showed an inputs, and generates speech units as output, which were
improved multilingual performance compared to [29], but thensenttotheTTS.Fourtypesofintermediatespeechunits
they did not perform as good as their feed-forward neural were tested: 1) L1-words and 2) L1-phones (generated us-
network based counterparts crosslingually. Using k-means, ing a same-language ASR, which provides an upper bound
segments were clustered based on the extracted AFs of each performance); 3) L2-phones from the cross-language defini-
segment. Estimating the number of classes k is an open tionofunitsapproachand4)pseudo-phonesgeneratedusing
questionforfutureresearch. AUD (see Section 3.1 for both). Results showed that the
The cross-language definition of units approach [30] image2speech system is able to generate a phone string that
uses linguistic knowledge of the low-resource language and is composed entirely of intelligible words, sequenced in an
asemi-supervisedtrainingparadigmtobuildanASRsystem intelligibleandsemanticallyreasonablesentence.
foralow-resourcelanguagethroughtheadaptationofanASR In the second task, a proof-of-concept foreign-text-2-
system of a high-resource language. Crucially, phones that speech end-to-end system was build using XNMT which
arepresentinthelow-resourcelanguagebutnotinthehigh- translates French words (text) into Mboshi phones which
resource language need to be created. This is done through wereeither(1)truephones(2)orpseudo-phonesobtainedvia
a linear extrapolation between existing acoustic units in the AUD (see Section 3.1). These phone sequences were then
high-resource ASR system’s soft-max layer after which the sent to the TTS system. On a development set of 514 utter-
acousticunitsareiterativelyretrainedusingallutterancesor ances BLEU4 scores at the character level were of 31.95%
onlythosethathavethebestscoreaccordingtofourdifferent withtruephonesand8.32%withpseudo-phones3.
criteria: ASRscore,theMCDscorefromaTTSsystem(see
3.3. Speechandimagetotext(andsummarization)
Section 3.2), translated text retrieval score, and their combi-
nation.Theexperimentsshowedthatinordertotrainacoustic The speech-and-image2text system uses multi-modal infor-
units using self-labelled data, training utterances are needed mation consisting of speech and videos to improve standard
thatcapturemultipleaspectsofthespeechsignal. (supervised)ASR(thisapproachisthusalsousefulforhigh-
resourcelanguages). Fromthe videos, object andscenefea-
3.2. “TTSwithoutT”
turesareextractedandusedtoadaptasequence-to-sequence
Text-to-speech(TTS)technologywasusedtogeneratespeech model (using the Pyramidal encoder by [33]) to the visual
fromunitsequences,andtoevaluatethequalityofthediscov- features. Resultsshowedthataddingthevisualfeatureshelps
ered unit inventories. Since this project concerns languages the model convergence and guides the training in the earlier
withoutorthography,TTSsystemsneedtobebuiltusingdis- epochs, compared to an HMM-DNN model. The sequence-
coveredunitsratherthantext(dubbed“TTSwithoutT”).The to-sequence model is able to jointly learn the audio visual
2The source code of both AUD models is available via 3TTS speech samples are available via https://github.com/JSALT-
https://github.com/amdtkdev/amdtk Rosetta/Illustrations/blob/master/TTS/mboshi/
System Prec Recall F Featuretype R@1 R@5 R@10
SegmentalDTWBaseline[37] 31.9 13.8 19.3 Mel-filterbank 0.0096 0.047 0.0856
Attention(fr-mb) 36.5 46.1 40.7 Multiling. Bottleneck 0.013 0.053 0.0994
Attention(mb-fr) 36.3 46.6 40.8 AUD(oneepoch) 0.0012 0.0044 0.0112
Cochleagram 0.0008 0.005 0.0104
Table1. Speech-to-translation: Wordboundarydetectionre-
sults(Mboshi5kcorpus)frompseudophones Table 2. Speech-to-image retrieval results (Recall@N) for
thetestedinputspeechfeatures
features,theacousticandlanguagemodels,requiresnoextra
preprocessing for noisy data, does not require precomputed to how children acquire their first language. Our speech-
alignments,andisefficientevenwithlongutterances. to-image system (based on the implementation of [13]) was
implemented using XNMT. Four types of acoustic features
4. END-TO-ENDTASKS werecompared: Mel-frequencyFilterbanks(baseline,similar
to[16]butwithaddedspeaker-dependentmean-variancenor-
4.1. Speech-to-translation malization on the features before zero-padding/truncation),
thepseudo-phonesgeneratedbytheAUDsystem[3](which
End-to-End speech translation, i.e., translation from raw
were downsampled by a factor of 9 along the phone dimen-
speech without any intermediate transcription [34, 35], is
sion to fit the input of the DNN), Multilingual Bottleneck
attractiveforlanguagedocumentation,whichoftenusescor-
features(MBN),andCochleagramFeaturesgeneratedbythe
pora made of audio recordings aligned with their translation
ResonantTectorialModeldevelopedby[38].
in another language (no transcript in the source language)
Table 2 shows the results for the four features evaluated
[1, 14]. Here, XNMT was used to build end-to-end speech
with Recall@N. The MBN feature is superior to all other
translations systems on FlickR (English-to-Japanese) and
acousticfeatures, andshowsover1percentimprovementon
Mboshi-to-French. The obtained BLEU4 scores at the char-
theFilterbankbaselinefortherecall@10score.
acterlevelwere30.99%and22.36%onthedevelopmentsets
of FlickR and Mboshi, respectively. Although these results 5. CONCLUDINGREMARKS
areratherlowforapuretranslationtask,thesesystemsshow
The “Speaking Rosetta” JSALT 2017 project laid the foun-
that end-to-end models are able to encode some regularities
dation for a new research area “Unsupervised multi-modal
inthespeechsignalinordertodecodepredictablesequences
language acquisition”. It showed that it is possible to build
ofcharactersinatargetlanguage.
usefulSLTsystemswithoutanytextualresourcesinthelan-
Secondly,anattention-basedNeuralMachineTranslation
guage for which the SLT is built, in a way that is similar
(NMT) model [36] was trained between phones in Mboshi
to that of how infants learn a language. 1) The “Speaking
and text in French, while soft-alignment probability matri-
Rosetta”projectshowedthatzero-shotadaptation,i.e.,unsu-
ces generated by the attention mechanism, were extracted.
pervisedlearningofspeechunits,ispossible,andcanbeim-
Thesealignmentswerepost-processedtosegmentasequence
proved by using information extracted from well-resourced
of symbols in Mboshi into words. While [36] applied their
languages. The discovered units are meaningful as shown
method to true phones (gold phonemes transcribed by lin-
bytheirusefulnessinupstreamtaskssuchasworddiscovery,
guists), here segmentation through attention from a pseudo-
image retrieval, and speech translation tasks. We have pre-
phone sequence obtained using AUD (see Section 3.1) was
sentedthefirstattempttodiscoverspokentermfromspeech
investigated. Table 1 shows that the word boundary detec-
using an attention matrix; the performance of this approach
tionresultsoftheattention-basedsystemoutperformedthose
is better than all the baselines evaluated in the same condi-
of a pure speech-based baseline which used pair-matching
tions. 2) TTS has proven to be a useful tool in the evalua-
using locally sensitive hashing applied to PLP features and
tion of discovered units of different types, and can be used
then grouped pairs using graph clustering [37]. Moreover, a
to evaluate how well a particular set of units correlates with
reversemodel(French-Mboshi)slightlyimprovedwordseg-
acoustic features. “Units” we have tested include articula-
mentationcomparedto(Mboshi-French). Implementationof
toryfeatures, AUDs, andcross-languageadaptedphones. 3)
abilinguallossisprobablyaninterestingfuturework.
Theunit-discoveryandtheend-to-endsystemsweresuccess-
4.2. Speech-to-Image fullycombinedintoseveralworkingproof-of-conceptend-to-
end demos. 4) We showed that audio and video information
Speech-to-image is a relatively new task [11, 12, 13]. A canbefusedtoimprovespeechsummarizationwithoutgoing
speech-to-image system learns to map images and speech throughtext. 5)Finally,apipelineofmetricsaswellasdedi-
to the same embedding space, and retrieves an image using cateddatasetswerecreatedtofuelreproducibleresearchesin
spoken captions. While doing so, it uses multi-modal input thisnewemergingdomain.
to discover speech units in an unsupervised manner, similar
6. REFERENCES [14] D. Blachon, E. Gauthier, L. Besacier, G.-N. Kouarata,
M.Adda-Decker,andA.Rialland, “Parallelspeechcollection
[1] G.Addaetal., “Breakingtheunwrittenkanguagebarrier:The
for under-resourced language studies using the LIG-Aikuma
Bulbproject,”inProceedingsofSLTU,Yogyakarta,Indonesia,
mobiledeviceapp,” inProceedingsofSLTU,Yogyakarta,In-
2016.
donesia,May2016.
[2] A. Jansen et al., “A summary of the 2012 JH CLSP Work-
[15] P.Godardetal., “Averylowresourcelanguagespeechcorpus
shoponzeroresourcespeechtechnologiesandmodelsofearly
for computational language documentation experiments,” in
languageacquisition,” inProceedingsofICASSP,2013.
arXiv:1710.03501,2017.
[3] L.Ondel,L.Burget,andJ.Cˇernocky´, “Variationalinference [16] D.HarwathandJ.Glass, “Deepmultimodalsemanticembed-
for acoustic unit discovery,” in Procedia Computer Science, dingsforspeechandimages,”inProceedingsofASRU,Scotts-
2016,pp.80–86. dale,Arizona,USA,2015,pp.237–244.
[4] B.Varadarajan,S.Khudanpur,andE.Dupoux, “Unsupervised [17] L. Besacier, “Speech-coco,” https://persyval-platform.univ-
learningofacousticsub-wordunits,” inProceedingsofACL grenoble-alpes.fr/DS80/detaildataset.
on Human Language Technologies: Short Papers, 2008, pp.
[18] W.Havard,L.Besacier,andO.Rosec,“Speech-coco:600kvi-
165–168.
suallygroundedspokencaptionsalignedtomscocodataset,”
[5] A.S.ParkandJ.R.Glass,“UnsupervisedPatternDiscoveryin in International Workshop on Grounding Language Under-
Speech,” IEEETransactionsonAudio,Speech,andLanguage standing(GLU),SatelliteofInterspeech2017,2017.
Processing,vol.16,no.1,pp.186–197,2008. [19] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
[6] Y. Zhang and J. R. Glass, “Towards multi-speaker unsuper- manan,P.Dolla´r,andC.L.Zitnick,“Microsoftcoco:Common
vised speech pattern discovery,” in Proceeding of ICASSP, objectsincontext,” inEuropeanConferenceonComputerVi-
2010,pp.4366–4369. sion(ECCV),Zu¨rich,2014, Oral.
[20] N. Oostdijk, W. Goedertier, F. V. Eynde, L. Boves, J.-P.
[7] A.W.TanjaSchultz,“Experimentsoncross-languageacoustic
Martens,M.Moortgat,andH.Baayen, “Experiencesfromthe
modelling,” inProceedingsofInterspeech,2001.
spoken dutch corpus project,” in Proceedings of LREC, Las
[8] J. Lo¨o¨f, C. Gollan, and H. Ney, “Cross-language bootstrap-
PalmasdeGranCanaria,2002,pp.340–347.
pingforunsupervisedacousticmodeltraining: rapiddevelop-
[21] A.W.Black,“CLUSTERGEN:Astatisticalparametricspeech
mentofapolishspeechrecognitionsystem,” inProceedings
synthesizerusingtrajectorymodeling,” inProceedingsofIC-
ofInterspeech,2009.
SLP,2006,pp.1762–1765.
[9] K. Vesely, M. Karafia´t, F. Grezl, M. Janda, and E. Egorova,
[22] T.Schatz,V.Peddinti,F.Bach,A.Jansen,H.Hermansky,and
“Thelanguage-independentbottleneckfeatures,” inProceed-
E.Dupoux,“EvaluatingspeechfeatureswiththeMinimal-Pair
ingsofSLT,2012.
ABXtask(I):AnalysisoftheclassicalMFC/PLPpipeline,” in
[10] H. Xu, V. Do, X. Xiao, and E. Chng, “A comparative study
ProceedingsofInterspeech,2013.
of bnf and dnn multilingual training on cross-lingual low-
[23] T.Schatz,V.Peddinti,X.-N.Cao,F.Bach,H.Hermansky,and
resourcespeechrecognition,” inProceedingsofInterspeech,
E. Dupoux, “Evaluating speech features with the Minimal-
2015,pp.2132–2136.
Pair ABX task (II): Resistance to noise,” in Proceedings of
[11] D.HarwarthandJ.Glass,“Deepmultimodalsemanticembed- Interspeech,2014.
dingsforspeechandimages,”inProceedingsASRU,2015,pp.
[24] B.Ludusan,M.Versteegh,A.Jansen,G.Gravier,X.-N.Cao,
237–244.
M.Johnson,andE.Dupoux,“Bridgingthegapbetweenspeech
[12] G.Chrupała,L.Gelderloos,andA.Alishahi,“Representations technology and natural language processing: an evaluation
oflanguageinamodelofvisuallygroundedspeechsignal,” in toolboxfortermdiscoverysystems,” inProceedingsofLREC,
ProceedingsofASRU,2017. 2014.
[13] D.Harwath,A.Torralba,andJ.Glass,“Unsupervisedlearning [25] E.Dunbar,X.NgaCao,J.Benjumea,J.Karadayi,M.Bernard,
ofspokenlanguagewithvisualcontext,”inAdvancesinNeural L.Besacier,X.Anguera,andE.Dupoux, “Thezeroresource
InformationProcessingSystem,2016,pp.1858–1866. speechchallenge2017,” inProceedingsofASRU,2017.
[26] G.Neubig, “Xnmt,”https://github.com/neulab/xnmt/.
[27] K.Kurihara,M.Welling,andY.W.Teh,“Collapsedvariational
Dirichletprocessmixturemodels,” inProceedingsoftheIn-
ternational Joint Conference on Artificial Intelligence, 2007,
vol.20.
[28] M. J. Johnson, D. Duvenaud, A. B. Wiltschko, S. R. Datta,
andR.P.Adams, “Composinggraphicalmodelswithneural
networksforstructuredrepresentationsandfastinference,” in
NeuralInformationProcessingSystems,2016.
[29] M. Mu¨ller, J. Franke, S. Stu¨ker, and A. Waibel, “Improving
phonemesetdiscoveryfordocumentingunwrittenlanguages,”
ElektronischeSprachsignalverarbeitung(ESSV)2017,2017.
[30] O.Scharenborg,F.Ciannella,S.Palaskar,A.Black,F.Metze,
L. Ondel, and M. Hasegawa-Johnson, “Building an asr sys-
tem for a low-resource language through the adaptation of a
high-resource language asr system: Preliminary results,” in
ProceedingsofICNLSSP,Casablanca,Morocco,2017.
[31] T.Toda,A.W.Black,andK.Tokuda,“Mappingfromarticula-
torymovementstovocaltractspectrumwithgaussianmixture
model for articulatory speech synthesis,” in Proceedings of
SSW5,Pittsburgh,PA,2004,pp.31–36.
[32] M. Hasegawa-Johnson, A. Black, L. Ondel, O. Scharenborg,
and F. Ciannella, “Image2speech: Automatically generating
audio descriptions of images,” in Proceedings of ICNLSSP,
Casablanca,Morocco,2017.
[33] W.Chan,N.Jaitly,Q.V.Le,andO.Vinyals, “Listen,attend
andspell,” arXivpreprintarXiv:1508.01211,2015.
[34] A. Be´rard, O. Pietquin, C. Servan, and L. Besacier, “Listen
andtranslate:Aproofofconceptforend-to-endspeech-to-text
translation,” in NIPS workshop on End-to-end Learning for
SpeechandAudioProcessing,2016.
[35] R. J. Weiss, J. Chorowski, N. Jaitly, Y. Wu, and Z. Chen,
“Sequence-to-sequencemodelscandirectlytranscribeforeign
speech,” arXivpreprintarXiv:1703.08581,2017.
[36] M.ZanonBoito,A.Berard,A.Villavicencio,andL.Besacier,
“Unwritten languages demand attention too! word discovery
withencoder-decodermodels,”inProceedingsofASRU,2017.
[37] A.JansenandB.VanDurme,“Efficientspokentermdiscovery
usingrandomizedalgorithms,” inProceedingsofASRU,2011,
pp.401–406.
[38] J.AllenandD.Sen,“Istectorialmembranefilteringrequiredto
explaintwotonesuppressionandtheupwardspreadofmask-
ing?,” inMechanicsofHearing,1999,p.451.

TheThirty-FourthAAAIConferenceonArtificialIntelligence(AAAI-20)
PIQA: Reasoning about Physical Commonsense in Natural Language
YonatanBisk,1,2,3,4 RowanZellers,1,4 RonanLeBras,1 JianfengGao,2 YejinChoi1,4
1AllenInstituteforArtificialIntelligence2MicrosoftResearchAI3CarnegieMellonUniversity
4PaulG.AllenSchoolforComputerScienceandEngineering,UniversityofWashington
http://yonatanbisk.com/piqa
Abstract
To separate egg whites from the yolk
To apply eyeshadow without a brush, should I use a cotton using a water bottle, you should…
swaboratoothpick?Questionsrequiringthiskindofphys-
ical commonsense pose a challenge to today’s natural lan- a. Squeezethe water b. Placethe water bottle
guageunderstandingsystems.Whilerecentpretrainedmod-
bottle and press it and press it against the
els(suchasBERT)havemadeprogressonquestionanswer-
against the yolk. yolk. Keep pushing,
ingovermoreabstractdomains–suchasnewsarticlesand
Release, which creates which creates suction
encyclopediaentries,wheretextisplentiful–inmorephysi-
suction and lifts the yolk. and lifts the yolk.
caldomains,textisinherentlylimitedduetoreportingbias.
Can AI systems learn to reliably answer physical common-
sensequestionswithoutexperiencingthephysicalworld?
???
Inthispaper,weintroducethetaskofphysicalcommonsense
reasoning and a corresponding benchmark dataset Physical
Interaction:QuestionAnsweringorPIQA .Thoughhu-
mansfindthedataseteasy(95%accuracy),largepretrained a!
modelsstruggle(∼75%).Weprovideanalysisaboutthedi-
mensionsofknowledgethatexistingmodelslack,whichof-
ferssignificantopportunitiesforfutureresearch. Figure1:PIQA :Givenaphysicalgoalexpressedinnat-
urallanguage,like‘toseparateeggwhites...,’amodelmust
Introduction choosethemostsensiblesolution.Ourdatasetteststheabil-
ityofnaturallanguageunderstandingmodelstolinktextto
Physical commonsense is a common understanding of the
arobustintuitive-physicsmodeloftheworld.Here,humans
physical properties and affordances of everyday objects.
easily pick answer a) because separating the egg requires
Physical commonsense is an essential aspect of how we
pullingtheyolkout,whilemachinesareeasilyfooled.
move through and interact with the world, but as its is as-
sumed knowledge, it is rarely articulated and therefore ab-
sent from the abstract world of text inhabited by Natural
wherebyeveryday‘lowlevel’physicaldetailsarecommonly
LanguageProcessing(NLP).
ignored or misrepresented in text. While reporting bias is
MuchoftherecentprogressinNLPhasfocusedonbuild-
oftendiscussedcolloquiallyinthecontextofphysicalorvi-
ing large-scale pretrained representations from unlabeled
sualattributes(e.g.computersassumingallsheepareblack),
text(Radfordetal.2018;Devlinetal.2019;Liuetal.2019).
thecommunitylacksalarge-scaleresourceforstudyingand
These representations transfer well to core abstract tasks
evaluatingsuchphysicalknowledge.
anddomains,suchasansweringquestionsgivenanencyclo-
To study this question and begin bridging the represen-
pedia article (Rajpurkar et al. 2016) or recognizing named
tational gap, we introduce Physical Interaction: Question
entities(TjongKimSangandDeMeulder2003).
Answering, or PIQA to evaluate language representa-
However,thereisacrucialdisconnectbetweenhowdeep
tionsontheirknowledgeofphysicalcommonsense.Wefo-
models – versus humans – learn language. While humans
cusoneverydaysituationswithapreferenceforatypicalso-
continuously refine a rich model of the world through ex-
lutions.Ourdatasetisinspiredbyinstructables.com,
perience, today’s state-of-the-art NLP models are instead
whichprovidesuserswithinstructionsonhowtobuild,craft,
spoon-fedmassquantitiesoftextandattempttoreverseen-
bake, or manipulate objects using everyday materials. We
gineertherealworld.Thisleadstoissuesofreportingbias,
asked annotators to provide semantic perturbations or al-
Copyright(cid:3)c 2020,AssociationfortheAdvancementofArtificial ternative approaches which are otherwise syntactically and
Intelligence(www.aaai.org).Allrightsreserved. topically similar to ensure physical knowledge is targeted.
7432
a.Shape,Material,andPurpose b.CommonsenseConvenience
[Goal] Makeanoutdoorpillow [Goal] How to make sure all the clocks in the house are set
[Sol1] Blowintoatincanandtiewithrubberband (cid:2) accurately?
[Sol2] Blowintoatrashbagandtiewithrubberband (cid:3)
[Sol1] Getasolarclockforareferenceandplaceitjustoutside (cid:2)
[Goal] Tomakeahardshelledtaco, awindowthatgetslotsofsun.Useasystemofcalland
[Sol1] put seasoned beef, cheese, and lettuce onto the hard (cid:2) responseonceamonth,havingonepersonstationedat
shell. thesolarclockwhoyellsoutthecorrecttimeandhave
[Sol2] put seasoned beef, cheese, and lettuce into the hard (cid:3) another person move to each of the indoor clocks to
shell. checkiftheyareshowingtherighttime.Adjustasnec-
essary.
[Goal] HowdoIfindsomethingIlostonthecarpet? [Sol2] Replaceallwind-upswithdigitalclocks.Thatway,you (cid:3)
[Sol1] Putasolidsealontheendofyourvacuumandturnit (cid:2) setthemonce,andthat’sit.Checkthebatteriesoncea
on. yearorifyounoticeanythinglooksalittleoff.
[Sol2] Putahairnetontheendofyourvacuumandturniton. (cid:3)
Figure 2: PIQA covers a broad array of phenomena. Above are two categories of example QA pairs. Left are examples that
requireknowledgeofbasicpropertiesoftheobjects(flexibility,curvature,andbeingporous),whileontheRightbothanswers
maybetechnicallycorrectbutoneismoreconvenientandpreferable.
The dataset is further cleaned of basic artifacts using the annotation,helpsremindannotatorsaboutthelessprototyp-
AFLite algorithm introduced in (Sakaguchi et al. 2020; icalusesofeverydayobjects.
Sapetal.2019)whichisanimprovementonadversarialfil- Second, and equally important, is that instructions build
tering(Zellersetal.2018;2019b). ononeanother.ThismeansthatanyQApairinspiredbyan
Throughout this work we first detail the construction of instructable will be more likely to explicitly state assump-
ournewbenchmarkforphysicalcommonsense.Second,we tions about what preconditions need to be met to start the
show that popular approaches to large-scale language pre- taskandwhatpostconditionsdefinesuccess.
training, while highly successful on many abstract tasks,
fail to perform well when a physical model of the world is Collectingdatathroughgoal-solutionpairs
required. Finally, our goal is to elicit further research into
UnliketraditionalQAtasks,wedefineourdatasetinterms
buildinglanguagerepresentationsthatcapturedetailsofthe
ofGoalandSolutionpairs(seeFigure2forexampleGoal-
real world. To these ends, we perform error and corpora
Solutionpairsandtypesofphysicalreasoning).TheGoalin
analysestoprovideinsightsforfuturework.
mostcasescanbeviewedasindicatingapost-conditionand
thesolutionsindicatetheprocedureforaccomplishingthis.
Dataset
Themoredetailedthegoal,theeasieritisforannotatorsto
write both correct and incorrect solutions. As noted above,
We introduce a new dataset, PIQA , for benchmarking
thesecondcomponentofourannotationdesignisreminding
progress in physical commonsense understanding. The un-
people to think creatively. We initially experimented with
derlyingtaskismultiplechoicequestionanswering:givena
question q and two possible solutions s 1,s 2, a model or a asking annotators for (task, tool) pairs via unconstrained
prompts,butfoundthatreportingbiasswampedthedataset.
humanmustchoosethemostappropriatesolution,ofwhich
In particular, when thinking about how to achieve a goal,
exactlyoneiscorrect.Wecollectdatawithhow-toinstruc-
people most often are drawn to prototypical solutions and
tions as a scaffold, and use state-of-the-art approaches for
look for tools in the kitchen (e.g. forks and knives) or the
handlingspuriousbiases,whichwewilldiscussbelow.
garage(e.g.hammersanddrills).Theyrarelyconsideredthe
literal hundreds of other everyday objects that might be in
Instructablesasasourceofphysicalcommonsense
theirownhomes(e.g.sidewalkchalk,showercurtains,etc).
Our goal is to construct a resource that requires concrete Toaddressthis,andflattenthedistributionofreferenced
physicalreasoning.Toachievethis,weprovideapromptto objects (see Figure 5), we prompt the annotations with
the annotators derived from instructables.com. The linkstoinstructables.Specifically,annotatorswereaskedto
instructables.comwebsiteisacrowdsourcedcollec- glance at the instructions of an instructable and pull out or
tionofinstructionsfordoingeverythingfromcookingtocar haveitinspirethemtoconstructtwocomponenttasks.They
repair.Inmostcases,usersprovideimagesorvideosdetail- would then articulate the goal (often centered on atypical
ing each step and a list of tools that will be required. Most materials)andhowtoachieveit.Inaddition,weaskedthem
goalsaresimultaneouslyrareandunsurprising.Whileanan- toprovideapermutationtotheirownsolutionwhichmakes
notatorisunlikelytohavebuiltaUV-Flourescentsteampunk itinvalid,oftensubtly(Figure3).Tofurtherassistdiversity
lampormadeabackpackoutofducttape,itisnotsurpris- we seed annotators with instructables drawn from six cate-
ing that someone interested in home crafting would create gories(costume,outside,craft,home,food,andworkshop).
these, nor will the tools and materials be unfamiliar to the Weaskedthattwoexamplesbedrawnperinstructabletoen-
average person. Using these examples as the seed for their courageoneofthemtocomelaterintheprocessandrequire
7433
Figure 4: Sentence length distributions for both correct so-
lutionsandtricksarenearlyidenticalacrossthetrainingset.
Spacy,3 average 7.8 words and both correct and incorrect
solutionsaverage21.3words.Intotal,thisleadstoover3.7
millionlexicaltokensinthetrainingdata.
Figure 4 shows a plot of the correct and incorrect se-
quence lengths (as tokenized by the GPT BPE tokenizer),
with the longest 1% of the data removed. While there are
Figure3:IntheHITdesigntheinstructableprovidesinspira-
minordifferences,thetwodistributionsarenearlyidentical.
tiontothinkout-of-the-box(1Sock,3Products)andannota-
torsareaskedfor1.aphysicalgoal,2.avalidsolution,and
We also analyzed the overlap in the vocabulary and find
3.atrick.Thetrickshouldsoundreasonable,butbewrong
that in all cases (noun, verb, adjective, and adverb) we see
often due to a subtle misunderstanding of preconditions or
at least an 85% overlap between words used in correct and
physics. Additional HITs (not shown) were run for qualifi-
incorrect solutions. In total we have 6,881 unique nouns,
cationpriortothisstageandvalidationafterwards.2
2,493verbs,2,263adjectives,and604adverbsinthetrain-
ing data.. The top 75 most common of each are plotted in
Figure5alongsidetheircumulativedistributions.Again,this
precisearticulationofpre-conditions.
helps verify that the dataset revolves very heavily around
Duringvalidation,exampleswithlowagreementwerere- physicalphenomena,properties,andmanipulations.Forex-
moved from the data. This often meant that correct exam- ample,thetopadjectivesincludestate(dry,clean,hot)and
ples were removed that required expert level knowledge of shape (small, sharp, flat); adverbs include temporal con-
a domain (e.g. special woodworking terminology) which ditions (then, when) and manner (quickly, carefully, com-
should not fall under the umbrella of “commonsense.” Be- pletely).Thesepropertiesoftendifferentiatecorrectfromin-
cause,wefocusonhumangeneratedtricks,annotatorswere correctanswers,asshowninexamplesthroughoutthepaper.
free to come up with clever ways to hide deception. Of-
ten, this meant making very subtle changes to the solution
RemovingAnnotationArtifacts
torenderitincorrect.Inthesecases,thetwosolutionsmay
differ by as little as one word. We found both simple lin- As noted previously, we use AFLite (Sakaguchi et al.
guistic tricks (e.g. negation and numerical changes) in ad- 2020)toremovestylisticartifactsandtrivialexamplesfrom
dition to swapping a key action or item for another that is thedata,whichhavebeenshowntoartificiallyinflatemodel
topically similar but not helpful for completing the given performance on previous NLI benchmarks (Poliak et al.
goal. For this reason, our interface also includes a diff 2018;Gururanganetal.2018).TheAFLitealgorithmper-
buttonwhichhighlightswherethesolutionsdiffer.Thisim- formsasystematicdatabiasreduction:itdiscardsinstances
provedannotatoraccuracyandspeedsubstantially.Annota- whose given feature representations are collectively highly
tor pay averaged >15$/hr according to both self-reporting indicative of the target label. In practice, we use 5,000 ex-
onturkerview.comandourtimingcalculations. amples from the original dataset to fine-tune BERT-Large
forthistaskandcomputethecorrespondingembeddingsof
Statistics all remaining instances. AFLite uses an ensemble of lin-
ear classifiers trained on random subsets of the data to de-
Intotalourdatasetiscomprisedofover16,000trainingQA
terminewhetherthesepre-computedembeddingsarestrong
pairs with an additional ∼2K and ∼3k held out for devel-
indicatorsofthecorrectansweroption.Insteadofhavingto
opmentandtesting,respectively.Ourgoals,astokenizedby
specifically identify the possible sources of biases, this ap-
proachenablesunsuperviseddatabiasreductionbyrelying
2Inadditiontothisdesign,wealsoincludeaqualificationHIT
on state-of-the-art methods to uncover undesirable annota-
which contained well constructed and underspecified (goal, solu-
tion artifacts. For more information about AFLite, please
tion)pairs.Annotatorshadtosuccessfully(>80%)identifywhich
referto(Sakaguchietal.2020).
werewellformedtoparticipateinthemainHIT.Datawascollected
inbatchesofseveralthousandtriplesandvalidatedbyotheranno-
tatorsforcorrectness.Userswilllowagreementwerede-qualed. 3https://spacy.io/
7434
Figure 5: Here we show the frequency distributions for the top seventy-five words tagged by Spacy as noun, verb, adverb or
adjective.Weseethatthevastmajorityofconceptsfocusonphysicalproperties(e.g.small,hot,clean,smooth)andhowobjects
canbemanipulated(e.g.soak,roll,fill,hang).Additionally,weseestronglyzipfianbehaviorinalltagsbuttheadverbs.
Experiments hiddenstatescorrespondingtothepositionsofeach[CLS]
token.Weapplyalineartransformationtoeachhiddenstate
In this section, we test the performance of state-of-the-
andapplyasoftmaxoverthetwooptions:thisapproximates
art natural language understanding models on our dataset,
the probability that the correct solution is option A or B.
PIQA.Inparticular,weconsiderthefollowingthreelarge-
Duringfinetuning,wetrainthemodelusingacross-entropy
scaletransformermodels:
lossoverthetwooptions.4
a. GPT(Radfordetal.2018)isamodelthatprocessestext
Due to the inherent challenge of PIQA, we found that
left-to-right,andwaspretrainedusingalanguagemodeling
finetuning was often unstable. With some hyperparameter
objective.Weusetheoriginal124MparameterGPTmodel.
configurations, validation performance is around chance,
b. BERT (Devlin et al. 2019) is a model that process
particularly for BERT. We follow best practices in using a
textbidirectionally,andthuswaspretrainedusingaspecial
grid search over learning rates, batch sizes, and the num-
maskedlanguagemodelingobjective.WeuseBERT-Large,
ber of training epochs for each model, and report the best-
with340Mparameters.
scoringconfigurationaswasfoundonthevalidationset.For
c. RoBERTa (Liu et al. 2019) is a version of the BERT
allmodelsandexperiments,weusedthetransformers
modelthatwasmadetobesignificantlymorerobustthrough
libraryandtruncatedexamplesat150tokens,whichaffects
pretraining on more data and careful validation of the pre-
1%ofthedata.Manualinspectionofthedevelopmenterrors
training hyperparameters. We use RoBERTa-Large, which
showthatsome“mistakes”areactuallycorrectbutrequired
has355Mparameters.
a web-search to verify.5 It is therefore, completely reason-
Wefollowstandardbestpracticesinadaptingthesemod-
elsfortwo-wayclassification.Weconsiderthetwosolution 4Additionally,forGPT,wefollowtheoriginalimplementation
choices independently: for each choice, the model is pro- andaddanadditionallanguagemodelingloss,whichwefoundto
vided the goal, the solution choice, and a special [CLS] improvetrainingstability.
token. At the final layer of the transformer, we extract the 5Humanperformancewascalculatedbyamajorityvote.Anno-
7435
Accuracy(%)
Model Size Validation Test
RandomChance 50.0 50.0
MajorityClass 50.5 50.4
OpenAIGPT 124M 70.9 69.2
GoogleBERT 340M 67.1 66.8
FAIRRoBERTa 355M 79.2 77.1
Human 94.9
Table 1: Results of state-of-the-art natural language under-
standing models on PIQA, compared with human perfor-
mance. The results show a significant gap between model
andhumanperformance,ofroughly20absolutepoints.
able that automated methods trained on large web crawls
mayeventuallysurpasshumanperformancehere.
Note, human evaluation was performed on development
data, because once the train, development, and test folds
wereautomaticallyproducedbyAFLite,thetestdatawas
placed on a blind leaderboard hidden from us and all users
and only automatically evaluated via docker upload. Addi-
tionally, model submissions are capped to one per week to
avoidfittingtothetestdata.
Results
We present our results in Table 1. As the dataset was con-
Figure 6: Breaking down PIQA by edit distance between
structedtobeadversarialtoBERT,itisnotsurprisingthatit
solutionchoices.Top:Cumulativehistogramofexamplesin
performstheworstofthreemodelsdespitegenerallyoutper-
the validation and training sets, in terms of minimum edit
forming GPT on most other benchmarks. Comparing GPT distance d between the two solution choices. The majority
and RoBERTa we see that despite more training data, a
ofthedatasetconsistsofsmalltweaksbetweenthetwoso-
largervocabulary,twicethenumberofparametersandcare-
lutionpairs;nevertheless,thisisenoughtoconfusestate-of-
fulconstructionofrobusttraining,thereisonlya6ptperfor-
the-artNLPmodels.Bottom:RoBERTaaccuracyovervali-
mancegainandRoBERTastillfallsnearlya20pointsshort dationexampleswithaminimumeditdistanceofd.Dataset
of human performance on this task. As noted throughout,
difficulty increases somewhat as the two solution pairs are
exploringthisgapispreciselythepurposeforPIQAexist-
allowedtodriftfurtherapart.
ing and why RoBERTa is so summarily fooled is the focus
oftheremainderofthispaper.
erature to borrow from, making its dimensions challenging
Analysis
topindown.
Inthissection,weunpacktheresultsofstate-of-the-artmod- Simple concepts. Understanding the physical world re-
elsonPIQA.Inparticular,wetakealookattheerrorsmade quires a deep understanding of simple concepts, such as
by the top-performing model RoBERTa, as a view towards “water”or“ketchup,”andtheiraffordancesandinteractions
the physical commonsense knowledge that can be learned with respect to other concepts. Though our dataset cov-
throughlanguagealone. ersinteractionsbetweenandwithcommonobjects,wecan
analyze the space of concepts in the dataset by perform-
PIQAasadiagnosticforphysicalunderstanding ingastringalignmentbetweensolutionpairs.Twosolution
choicesthatdifferbyeditingasinglephrasemustbydefini-
The setup of PIQA allows us to use it to probe the inner
tiontestthecommonsenseunderstandingofthatphrase.
workingsofdeeppretrainedlanguagemodels,andtodeter-
minetheextentoftheirphysicalknowledge.Inthisway,our In Figure 6 we show the distribution of the edit distance
dataset can augment prior work on studying to what extent
betweensolutionchoices.6Mostofthedatasetcoverssimple
models such as BERT understand syntax (Goldberg 2019). editsbetweenthetwosolutionchoices:roughly60%ofthe
However,whilesyntaxisawellstudiedproblemwithinlin- dataset in both validation and training involves a 1-2 word
guistics,physicalcommonsensedoesnothaveasrichalit-
6We compute edit distance over tokenized and lowercased
tatorswerechosentoparticipatethatachieved≥90%onthequali- strings with punctuation removed. We use a cost of 1 for edits,
ficationHITfrombefore. insertions,anddeletions.
7436
Figure7:CommonconceptsasawindowtoRoBERTa’sun-
derstanding of the physical world. We consider validation
examples (q,s 1,s 2) wherein s 1 and s 2 differ from each
other by a given word w. Left, we show the validation ac-
curacy for common words w, while the number of dataset
Figure8:Themostcommonreplacementsforthreeselected
examplesareshownright.Thoughcertainconceptssuchas
words: ‘water,’ ‘spoon,’ and ‘freeze.’ These cover several
water occur quite frequently, RoBERTa nevertheless finds
keydimensions:‘water’isabroadnounwithmanyproper-
those concepts difficult, with 75% accuracy. Additionally,
tiesandaffordances,whereas‘spoons’aremuchnarrowerin
oncommonrelationssuchas‘cold’,‘on’,‘before’,and‘af-
scope.Perhapsasaresult,RoBERTaperformsmuchbutter
ter’RoBERTaperformsroughlyatchance.
at examples where ‘spoon’ is the pivot word (90%) versus
‘water’ (75%). Freeze has an accuracy of 66% on the vali-
dationset,andshowsthatverbsarechallengingaswell.
editbetweensolutions.InthebottomofFigure6,weshow
thatthedatasetcomplexitygenerallyincreaseswiththeedit
distance between the solution pairs. Nevertheless, the head
Common replacements in PIQA. We dig into this
ofthedistributionrepresentsaspacethatissimpletostudy.
further in Figure 8, where we showcase the most com-
Single-word edits. In Figure 7, we plot the accuracy
monreplacementsforthreeexamples:‘water,’‘spoon,’and
of RoBERTa among dataset examples that differ by a sin-
‘freeze.’ While ‘water’ is prevalent in the training set, it is
gleword.Moreformally,weconsiderexamples(q,s 1,s 2)
alsohighlyversatile.Onecantrytosubstituteitwithavari-
wherebymovingfroms 1 tos 2,orviceversa,requiresedit- etyofdifferenthouseholditems,suchas‘milk’or‘alcohol,’
ing a given word w.7 We show examples of words w that
often to disastrous effects. However, ‘spoons’ have fewer
occurfrequentlyinboththetrainingandvalidationsplitsof
challengingproperties.Aspooncannotgenerallybesubsti-
thedataset,whichallowsRoBERTatorefinerepresentations
tutedwithautensilthatissharporhasprongs,suchasafork,
oftheseconceptsduringtrainingandgivesusalargeenough
aknife,oratoothpick.RoBERTaobtainshighaccuracyon
samplesizetoreliablyestimatemodelperformance.
‘spoon’ examples, which suggests that it might understand
As shown, RoBERTa struggles to understand certain thissimpleaffordance,butdoesnotcapturethelongtailof
highly flexible relations. In particular, Figure 7 highlights affordancesassociatedwith‘water.’
thedifficultyofcorrectlyansweringquestionsthatdifferby
the words ‘before,’ ‘after’, ‘top‘, and ‘bottom’: RoBERTa
Qualitativeresults
performsnearlyatchancewhenencounteringthese.
Interestingly,theconceptsshowninFigure7suggestthat Our analysis thus far has been on simple-to-analyze single
RoBERTaalsostrugglestounderstandmanycommon,more word expressions, where we have shown that state-of-the-
versatile, physical concepts. Though there are 300 training art language models (such as RoBERTa) struggle at a nu-
examples wherein the solution choices s 1,s 2 differ by the anced understanding of key commonsense concepts, such
word ‘water.’ RoBERTa performs worse than average on as relations. To further probe the knowledge gap of these
thesereplacements.Ontheotherhand,RoBERTadoesmuch strongmodels,wepresentqualitativeexamplesinFigure9.
betteratcertainnouns,suchas‘spoon.’ The examples are broadly representative of larger patterns:
RoBERTacanrecognizeclearlyridiculousgenerations(Fig-
7We additionally allow for an additional insertion; this helps ure 9, top left) and understands differences between some
to capture simple phrases like going from ‘water’ to ‘olive oil.’ commonsenseconcepts(bottomleft).It’simportanttonote,
Nevertheless,thesemultiwordexpressionstendtobelesscommon, that in both cases the correct answer is prototypical and
whichiswhyweomittheminFigure7. somethingwemightexpectthemodelstohaveseenbefore.
7437
Correctexamples Incorrectexamples
[Goal] Bestwaytopierceears. [Goal] HowcanIquicklyandeasilyremovestrawberrystems?
[Sol1] Itisbesttogotoaprofessionaltogetyourearpiercedto (cid:3) [Sol1] Takeastrawandfromthetopofthestrawberrypushthe (cid:2)
avoidmedicalproblemslater. strawthroughthecenterofthestrawberryuntilthestem
[Sol2] Thebestwaytopierceyourearswouldbetoinsertanee- (cid:2) popsoff.
dlehalfinchthickintothespotyouwantpierced. [Sol2] Takeastrawandfromthebottomofthestrawberrypush (cid:3)
the straw through the center of the strawberry until the
[Goal] Howdoyoureducewearandtearonthenonstickfinish stempopsoff.
ofmuffinpans?
[Sol1] Make sure you use paper liners to protect the nonstick (cid:3) [Goal] howtoaddfeettoacoaster.
finishwhenbakingmuffinsandcupcakesinmuffinpans. [Sol1] cutfourslicesfromagluestick,andattatchtothecoaster (cid:3)
[Sol2] Makesureyouusegreaseandflourtoprotectthenon- (cid:2) withglue.
stickfinishwhenbakingmuffinsandcupcakesinmuffin [Sol2] placeaboardunderthecoaster,andsecurewithzipties (cid:2)
pans. andagluegun.
Figure9:QualitativeanalysisofRoBERTa’spredictionswith.Left:TwoexamplesthatRoBERTagetsright.Right:twoexam-
plesthatRoBERTagetsincorrect.Shortphrasesthatdifferbetweensolution1andsolution2areshowninboldanditalics.
However, it struggles to tell the difference between sub- al.2019a).
tlerelationssuchastopandbottom(toprightofFigure9). Robotics.Learningfrominteractionandintuitivephysics
Moreover,itstruggleswithidentifyingnon-prototypicalsit- (Agrawal et al. 2016) can also be encoded as priors when
uations(bottomright).Thoughusingagluestickasfeetfor exploringtheworld(Byravanetal.2018)andinternalmod-
acoasterisuncommon,toahumanfamiliarwiththesecon- elsofphysics,shape,andmaterialstrengthenableadvances
ceptswecanvisualizetheactionanditsresulttoverifythat in tool usage (Toussaint et al. 2018) or construction (Nair,
thegoalhasbeenachieved.Overall,theseexamplessuggest Balloch, and Chernova 2019). Key to our research aims in
that physical understanding – particularly involving novel this work is helping to build language tools which capture
combinations of common objects – challenges models that enoughphysicalknowledgetospeedupthebootstrappingof
werepretrainedontextonly. robotic-language applications. Language tools should pro-
vide strong initial priors for learning (Tellex et al. 2011;
RelatedWork Matuszek2018)thatarethenrefinedthroughinteractionand
dialogue(Gaoetal.2016).
Physicalunderstandingisbroaddomainthattouchesonev-
erything from scientific knowledge (Schoenick et al. 2016)
Conclusion
to the interactive acquisition of knowledge by embodied
agents(Thomasonetal.2016).Tothisend,workrelatedto Wehaveevaluatedagainstlarge-scalepretrainedmodelsas
thegoalsofourbenchmarkspantheNLP,ComputerVision theyareinvogueasthedefactostandardofprogresswithin
andRoboticscommunities. NLP, but are primarily interested in their performance and
failings as a mechanism for advancing the position that
Language. Within NLP, in addition to large scale mod-
learning about the world from language alone, is limiting.
els, there has also been progress on reasoning about cause
Future research, may “match” humans on our dataset by
and effect effects/implications within these models (Bosse-
finding a large source of in-domain data and fine-tuning
lut et al. 2019), extracting knowledge from them (Petroni
heavily, but this is very much not the point. Philosophi-
et al. 2019), and investigating where large scale language
cally,knowledgeshouldbelearnedfrominteractionwiththe
modelsfailtocaptureknowledgeoftoolsandelidedproce-
worldtoeventuallybecommunicatedwithlanguage.
duralknowledgeinrecipes(Bisketal.2019).Thenotionof
In this work we introduce the Physical Interaction:
procedural knowledge and instruction following is a more
QuestionAnsweringorPIQA benchmarkforevaluating
general related task within vision and robotics. From text
andstudyingphysicalcommonsenseunderstandinginnatu-
alone, work has shown that much can be understood about
ral language models. We find the best available pretrained
the implied physical situations of verb usage (Forbes and
models lack an understanding of some of the most basic
Choi2017)andrelativesizesofobjects(Elazaretal.2019).
physical properties of the world around us. Our goal with
Vision.Physicalknowledgecanbediscoveredandevalu-
PIQAistoprovideinsightandabenchmarkforprogressto-
atedwithinthevisualworld.Researchhasstudiedpredicting
wardslanguagerepresentationsthatcaptureknowledgetra-
visual relationships in images (Krishna et al. 2016) and as
ditionallyonlyseenorexperienced,toenabletheconstruc-
wellasactionsandtheirdependentobjects(Yatskar,Zettle-
tionoflanguagemodelsusefulbeyondtheNLPcommunity.
moyer, and Farhadi 2016). Relatedly, the recent HAKE
dataset (Liu et al. 2019) specifically annotates which
Acknowledgements
object/body-partsareessentialtocompletingordefiningan
action.Relatedtophysicalcommonsense,researchinvisual Wethanktheanonymousreviewersfortheirinsightfulsug-
commonsensehasstudiedintuitivephysics(Wuetal.2017), gestions. This research was supported in part by NSF (IIS-
cause-effect relationships (Mottaghi et al. 2016), and what 1524371, IIS-1714566), DARPA under the CwC program
canbereasonablyinferredbeyondasingleimage(Zellerset throughtheARO(W911NF-15-1-0543),DARPAunderthe
7438
MCSprogramthroughNIWCPacific(N66001-19-2-4031), Poliak, A.; Naradowsky, J.; Haldar, A.; Rudinger, R.; and
and the NSF-GRFP No. DGE-1256082. Computations on VanDurme,B. 2018. HypothesisOnlyBaselinesinNatural
beaker.orgweresupportedinpartbyGoogleCloud. Language Inference. In Joint Conference on Lexical and
ComputationalSemantics(StarSem).
References Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,
I. 2018. Improving language understanding by generative
Agrawal, P.; Nair, A.; Abbeel, P.; Malik, J.; and Levine, S.
pre-training.
2016. Learningtopokebypoking:Experientiallearningof
intuitivephysics. InNeurIPS. Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
Squad: 100,000+ questions for machine comprehension of
Bisk,Y.;Buys,J.;Pichotta,K.;andChoi,Y. 2019. Bench-
text. InEMNLP,2383–2392.
markinghierarchicalscriptknowledge. InNAACL-HLT.
Sakaguchi, K.; Le Bras, R.; Bhagavatula, C.; and Choi, Y.
Bosselut,A.;Rashkin,H.;Sap,M.;Malaviya,C.;Celikyil-
2020. Winogrande: An adversarial winograd schema chal-
maz,A.;andChoi,Y.2019.COMET:CommonsenseTrans-
lengeatscale. InAAAI.
formers for Automatic Knowledge Graph Construction. In
ACL. Sap, M.; Rashkin, H.; Chen, D.; Le Bras, R.; and Choi, Y.
2019. Socialiqa: Commonsense reasoning about social in-
Byravan, A.; Leeb, F.; Meier, F.; and Fox, D. 2018. Se3-
teractions. InEMNLP.
pose-nets:Structureddeepdynamicsmodelsforvisuomotor
Schoenick,C.;Clark,P.;Tafjord,O.;Turney,P.;andEtzioni,
planningandcontrol. InICRA.
O. 2016. Moving beyond the turing test with the allen ai
Devlin,J.;Chang,M.-W.;Lee,K.;andToutanova,K. 2019.
sciencechallenge. CommunicationsoftheACM.
BERT:Pre-trainingofDeepBidirectionalTransformersfor
Tellex,S.;Kollar,T.;Dickerson,S.;Walter,M.R.;Banerjee,
LanguageUnderstanding. InNAACL-HLT.
A.G.;Teller,S.;andRoy,N. 2011. Understandingnatural
Elazar,Y.;Mahabal,A.;Ramachandran,D.;Bedrax-Weiss,
languagecommandsforroboticnavigationandmobilema-
T.;andRoth,D. 2019. Howlargearelions?inducingdistri-
nipulation. In Proceedings of the National Conference on
butionsoverquantitativeattributes. InACL.
ArtificialIntelligence.
Forbes,M.,andChoi,Y. 2017. Verbphysics:Relativephys-
Thomason, J.; Sinapov, J.; Svetlik, M.; Stone, P.; and
icalknowledgeofactionsandobjects. InACL.
Mooney,R.J. 2016. LearningMulti-ModalGroundedLin-
Gao,Q.;Doering,M.;Yang,S.;andChai,J. 2016. Physical guisticSemanticsbyPlaying”ISpy”. InIJCAI,3477–3483.
causalityofactionverbsingroundedlanguageunderstand- TjongKimSang,E.F.,andDeMeulder,F. 2003. Introduc-
ing. InACL,1814–1824. tiontotheCoNLL-2003sharedtask:Language-independent
Goldberg, Y. 2019. AssessingBERT’s Syntactic Abilities. namedentityrecognition. InNAACL,142–147.
arXiv:1901.05287. Toussaint,M.;Allen,K.R.;Smith,K.A.;andTenenbaum,
Gururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.; J.B.2018.Differentiablephysicsandstablemodesfortool-
Bowman,S.;andSmith,N.A. 2018. Annotationartifactsin useandmanipulationplanning. InRSS.
naturallanguageinferencedata. InNAACL-HLT,107–112. Wu, J.; Lu, E.; Kohli, P.; Freeman, B.; and Tenenbaum, J.
Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; 2017. Learning to see physics via visual de-animation. In
Kravitz, J.; Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma, Guyon,I.;Luxburg,U.V.;Bengio,S.;Wallach,H.;Fergus,
D.A.;Bernstein,M.;andFei-Fei,L. 2016. Visualgenome: R.;Vishwanathan,S.;andGarnett,R.,eds.,NeurIPS.
Connectinglanguageandvisionusingcrowdsourceddense Yatskar, M.; Zettlemoyer, L.; and Farhadi, A. 2016. Sit-
imageannotations. InarXiv:1602.07332. uation recognition: Visual semantic role labeling for image
Liu,Y.;Ott,M.;Goyal,N.;Du,J.;Joshi,M.;Chen,D.;Levy, understanding. InCVPR.
O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Zellers, R.; Bisk, Y.; Schwartz, R.; and Choi, Y. 2018.
RoBERTa: A Robustly Optimized BERT Pretraining Ap- SWAG: A Large-Scale Adversarial Dataset for Grounded
proach. arXiv:1907.11692. CommonsenseInference. InEMNLP.
Matuszek,C. 2018. GroundedLanguageLearning:Where Zellers,R.;Bisk,Y.;Farhadi,A.;andChoi,Y. 2019a. From
RoboticsandNLPMeet. InIJCAI,5687–5691. recognitiontocognition:Visualcommonsensereasoning.In
CVPR.
Mottaghi, R.; Rastegari, M.; Gupta, A.; and Farhadi, A.
2016. “what happens if...” learning to predict the effect Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi,
of forces in images. In Leibe, B.; Matas, J.; Sebe, N.; and Y. 2019b. HellaSwag: Can a Machine Really Finish Your
Welling,M.,eds.,ECCV,269–285. Sentence? InACL.
Nair, L.; Balloch, J.; and Chernova, S. 2019. Tool Mac-
gyvering:ToolConstructionUsingGeometricReasoning.In
ICRA.
Petroni,F.;Rockta¨schel,T.;Lewis,P.;Bakhtin,A.;Wu,Y.;
Miller, A. H.; and Riedel, S. 2019. Language models as
knowledgebases? InEMNLP.
7439

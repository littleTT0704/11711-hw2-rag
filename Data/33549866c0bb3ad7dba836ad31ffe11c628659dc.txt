Repositório ISCTE-IUL
Deposited in Repositório ISCTE-IUL:
2023-02-15
Deposited version:
Accepted Version
Peer-review status of attached file:
Peer-reviewed
Citation for published item:
Marujo, L., Ribeiro, R., Matos, D. M. de., Neto, J. P., Gershman, A. & Carbonell, J. (2012). Key phrase
extraction of lightly filtered broadcast news. In Sojka, P., Horák, A., Kopeek, I., and Pala, K. (Ed.),
Text, Speech and Dialogue. TSD 2012. Lecture Notes in Computer Science. (pp. 290-297). Brno:
Springer.
Further information on publisher's website:
10.1007/978-3-642-32790-2_35
Publisher's copyright statement:
This is the peer reviewed version of the following article: Marujo, L., Ribeiro, R., Matos, D. M. de.,
Neto, J. P., Gershman, A. & Carbonell, J. (2012). Key phrase extraction of lightly filtered broadcast
news. In Sojka, P., Horák, A., Kopeek, I., and Pala, K. (Ed.), Text, Speech and Dialogue. TSD 2012.
Lecture Notes in Computer Science. (pp. 290-297). Brno: Springer., which has been published in final
form at https://dx.doi.org/10.1007/978-3-642-32790-2_35. This article may be used for non-
commercial purposes in accordance with the Publisher's Terms and Conditions for self-archiving.
Use policy
Creative Commons CC BY 4.0
The full-text may be used and/or reproduced, and given to third parties in any format or medium, without prior permission or
charge, for personal research or study, educational, or not-for-profit purposes provided that:
• a full bibliographic reference is made to the original source
• a link is made to the metadata record in the Repository
• the full-text is not changed in any way
The full-text must not be sold in any format or medium without the formal permission of the copyright holders.
Serviços de Informação e Documentação, Instituto Universitário de Lisboa (ISCTE-IUL)
Av. das Forças Armadas, Edifício II, 1649-026 Lisboa Portugal
Phone: +(351) 217 903 024 | e-mail: administrador.repositorio@iscte-iul.pt
https://repositorio.iscte-iul.pt
Key Phrase Extraction of Lightly Filtered
Broadcast News
Lu´ıs Marujo1,2, Ricardo Ribeiro2,3, David Martins de Matos2,4, Jo˜ao P.
Neto2,4, Anatole Gershman1, and Jaime Carbonell1
1 LTI/CMU, USA
2 L2F - INESC ID Lisboa, Portugal
3 Instituto Universita´rio de Lisboa (ISCTE-IUL), Portugal
4 Instituto Superior T´ecnico - Universidade T´ecnica de Lisboa, Portugal
{luis.marujo, ricardo.ribeiro, david.matos, joao.neto}@inesc-id.pt
{anatoleg, jgc}@cs.cmu.edu
Abstract. Thispaperexplorestheimpactoflightfilteringonautomatic
key phrase extraction (AKE) applied to Broadcast News (BN). Key
phrases are words and expressions that best characterize the content of
adocument.Keyphrasesareoftenusedtoindexthedocumentorasfea-
tures in further processing. This makes improvements in AKE accuracy
particularly important. We hypothesized that filtering out marginally
relevant sentences from a document would improve AKE accuracy. Our
experimentsconfirmedthishypothesis.Eliminationofaslittleas10%of
thedocumentsentencesleadtoa2%improvementinAKEprecisionand
recall.AKEisbuiltoverMAUItoolkitthatfollowsasupervisedlearning
approach. We trained and tested our AKE method on a gold standard
made of 8 BN programs containing 110 manually annotated news sto-
ries. The experiments were conducted within a Multimedia Monitoring
Solution(MMS)systemforTVandradionews/programs,runningdaily,
and monitoring 12 TV and 4 radio channels.
Keywords: Keyphraseextraction,Speechsummarization,Speechbrows-
ing, Broadcast News speech recognition
1 Introduction
With the overwhelming amount of News video and audio information broad-
casteddailyonTVandradiochannels,usersareconstantlystrugglingtounder-
stand the big picture. Indexing and summarization provide help, but they are
hard for multimedia documents, such as broadcast news, because they combine
several sources of information, e.g. audio, video, and footnotes. We use light
filtering to improve the indexing, where AKE is a key element.
AKE is a natural language procedure that selects the most relevant phrases
(keyphrases)fromatext.Thekeyphrasesarephrasesconsistingofoneormore
significant words (keywords). They typically appear verbatim in the text. Light
filtering removes irrelevant sentences, providing a more adequate search space
3102
nuJ
02
]LC.sc[
1v0984.6031:viXra
2 Authors Suppressed Due to Excessive Length
for AKE. AKE is supposed to represent the main concepts from the text. But
evenforahumanbeing,themanualselectionofkeyphrasesfromadocumentis
context-dependentandneedstorelymoreonhigher-levelconceptsthanlow-level
features. That is why filtering improves AKE.
In general, AKE consists of two steps [9,18,19]: candidate generation and fil-
tering of the phrases selected in the candidate generation phrase. Several AKE
methods have been proposed. Most approaches only use standard information
retrieval techniques, such as N-gram models [3], word frequency, TFxIDF (term
frequency × inverse document frequency) [17], word co-occurrences [12], PAT
tree or suffix-based for Chinese and other oriental languages [2]. In addition,
some linguistic methods, based on lexical analysis [4] and syntactic analysis [7],
are used. These methodologies are classified as unsupervised methods [8], be-
causetheydonotrequiretrainingdata.Ontheotherhand,supervisedmethods
view this problem as a binary classification task, where a model is trained on
annotated data to determine whether a given phrase is a key phrase or not. Be-
cause supervised methods perform better, we use them in our work. In general,
the supervised approach uses machine-learning classifiers in the filtering step
(e.g.: C4.5 decision trees [13], neural networks [18]).
Alloftheabovemethodssufferfromthepresenceofirrelevantormarginally
relevantcontent,whichleadstoirrelevantkeyphrases.Inthispaper,wepropose
an approach that addresses this problem through the use of light filtering based
on summarization techniques.
A summary is a shorter version of one or more documents that preserves
their essential content. Compression Ratio (CR) is the ratio of the length of
the removed content (in sentences) to the original length. Light filtering typi-
cally involves a CR near 10%. Light filtering is a relaxation to the summariza-
tion problem because we just remove the most irrelevant or marginally relevant
content. This relaxation is very important because the summarization problem
is especially difficult when processing spoken documents: problems like speech
recognitionerrors,disfluencies,andboundariesidentification(bothsentenceand
document)increasethedifficultyindeterminingthemostimportantinformation.
Thisproblemhasbeenapproachedusingshallowtextsummarizationtechniques
such as Latent Semantic Analysis (LSA) [6] and Maximal Marginal Relevance
(MMR) [1], which seem to achieve comparable performance to methods using
specific speech-related features [15], such as acoustic/prosodic features.
This work here addresses the use of light filtering to improve AKE. The
experimentswereconductedwithinaMediaMonitoringSolution(MMS)system.
Thispaperisorganizedasfollows:Section2presentstheoverallarchitecture;
the description of the summarization module included in the MMS system is
the core of Section 3, results are described in Section 4, and Section 5 draws
conclusions and suggests future work.
Key Phrase Extraction of Lightly Filtered Broadcast News 3
2 Overall Architecture
The main workflow of the complete MMS system [11,14], depicted in Fig. 1, is
the following: a Media Receiver captures and records BN programs from TV
and radio. Then, the transcription is generated and enriched with punctuation
and capitalization. Subsequently, each BN program is automatically segmented
into several stories. News stories are lightly filtered (90% of the original size or
remains unchanged if the number of sentences in the summarized version is less
than or equal to 3). The remaining text is passed to the key phrase extraction
process. Each news story is topic-indexed or topic-classified. Finally, each news
story is stored in a metadata database (DB) with the respective transcription,
key phrases, and index, besides program/channel and timing information. A
Key phrase Cloud Generator creates/updates 3D key phrase cloud based on the
interactionwiththeMetadataDBandlinkswiththevideosthatareshownwhen
a user accesses the system. A 3D key phrase cloud is a tag/word cloud, which is
avisualrepresentationofthemostfrequentlyusedwordsintextdata.Themost
frequenttagsareusuallydisplayedinlargerfontsin2Dcloudsoratthefrontin
3Dclouds(rotatingthe3Dcloudallowaccesstothelessfrequent/relevanttags).
Typically, tags are keywords or single words; key phrases extend this concept to
several words.
The gray blocks are the focus of our work. A summarization module [16],
responsible for the light filtering step, was included in the workflow and its
impact on the key phrase extraction module is analyzed.
Tv / Radio Capitalization
Media Audio ASR & Segmentation Summarization Keyphrase
Receiver Punctuation Extractor
Video
Video DB
Storage
Keyphrase W (e Sb Ose Ar Pvi )ce Ke Cyp loh ur da se
Cloud Metadata Generator Topic
DB Indexation
Fig.1. Component view of the system architecture.
3 Key Phrase-Cloud Generation Based On Light
Filtering
3.1 Filtering
The automatic filtering step applied in this work is performed by a summa-
rization module that follows a centrality-as-relevance approach. Centrality-as-
relevance methods base the detection of the most important content on the de-
termination of the most central passages of the input source(s), considering an
4 Authors Suppressed Due to Excessive Length
adequate input source representation (e.g.: graph, spatial). Although pioneered
in the context of text summarization, this kind of approaches has drawn some
attention in the context of speech summarization, either by trying to improve
them[5]orusingthemasbaseline[10].Evenintextsummarization,thenumber
of up-to-date examples is significant.
The summarization model we use [16] does not need training data or ad-
ditional information. The method consists in creating, for each passage of the
input source, a set containing only the most semantically related passages, des-
ignated support set. Then, the determination of the most relevant content is
achieved by selecting the passages that occur in the largest number of support
sets. Geometric proximity (Manhattan, Euclidean, Chebyshev are some of the
explored distances) is used to compute semantic relatedness. Centrality (rele-
vance) is determined by considering the whole input source (and not only local
information), and by taking into account the presence of noisy content in the
information sources to be summarized. This type of representation diminishes
the influence of the noisy content, improving the effectiveness of the centrality
determination method.
3.2 Automatic Key Phrase Extraction
AKEextractskeyconcepts.OurAKEprocesswasdesignedtotakeintoaccount
the extraction of few key phrases (e.g.: 10 used in 3D Key phrase Cloud) and
large number of key phrases (e.g.: 30 used for indexing). We privileged preci-
sion over recall when extracting fewer key phrases because we want to mitigate
visible mistakes in the 3D Key phrases Cloud. On the other hand, recall gains
importancewhenweextractmanykeyphrasesbecausewewanttohavethebest
coverage possible. During our experiments, we observed that the most general
andatthesametimerelevantconceptscanbedirectlylinkedwithanindextopic
(examples:soccer/football→sports,PlayStation→technology).However,they
are frequently captured by the previous methods with low confidence (<50%).
Sincefilteringreducesirrelevantcontent,itincreasestheconfidenceofcapturing
thebestkeyphrases.TheAKEsystemweuse[11],developedforEuropeanPor-
tuguese BN, is an extended version of Maui-indexer toolkit [13] (a state-of-art
supervised key phrase extraction toolkit), which is in turn an improved version
ofKEA[19].Trainingdataisusedtotrainamachinelearningclassifier(bagging
over C4.5 decision tree). The output is a model that uses extracted features to
classify whether a word or phrase is a key phrase. The same CR (filtering) is
used to train the models and evaluate them at the test sets. This allows the
models to be more robust. The Maui-indexer feature extraction process was en-
richedwiththefollowing5features:numberofcharacters;thenumberofnamed
entities using the MorphoAdorner name recognizer; number of capital letters;
count of POS tags; and probability of the key phrase in a 4-gram domain model
(about 58K unigrams, 700M bigrams, 1.500M trigrams, and 10.000M 4-grams).
We have previously demonstrated that these features improved AKE [11].
Key Phrase Extraction of Lightly Filtered Broadcast News 5
4 Evaluation
We used a BN gold standard corpus annotated with the corresponding key
phrases, created in previous work. The gold standard consists of 8 BN pro-
grams transcribed from the European Portuguese ALERT BN database. The
news transcriptions were produced by AUDIMUS, an ASR for Portuguese, with
low WER (14,56% on average); and punctuated and capitalized automatically
using in-house tools. Those news programs were automatically split into a total
of 110 news stories. Later, each news story was manually examined to fix seg-
mentationerrors.Afterward,oneannotatorwasaskedtoextractallkeyphrases
that represent a relevant concept in each news story. The gold standard was
divided in training (100 news stories containing on average 24 key phrases and
19 sentences) and test set (10 news stories containing on average 29 key phrases
and 17 sentences). In our experiments, light filtering improved AKE precision
and recall by 2%. We have also tested higher CR (Figure 2) and restricting the
summary length to 4 sentences (roughly the average size of a paragraph). How-
ever, we did not observed improvements in the results. The average percentage
of key phrases lost by the filtering process was less than 5% (Figure 3).
5 Conclusions and future work
This paper explores a novel method to improve key phrase extraction from BN
by using light filtering. The key phrases are extracted to create a hierarchical
3-layerrepresentationofnews.Thekeyphrasesoftopnewsarevisualizedintag
cloud to allow users to skim their content and jump to the most relevant news
faster.
Based on the results, we show that light filtering improves automatic key
phrase extraction. We included light filtering, constrained to have at least 4
sentencesinthesummaryintheMMSsystem.Thisstepisdonebeforeextracting
10 key phrases of each news story. In addition, we show that even changing
the number of key phrases extracted the light filtering still improves the AKE
process. We also show that filtering up to 50% of the original size corresponds
to about 26% in key phrases loss. That corresponds to less than 5% in terms
of AKE performance metrics degradation. This is an important result because
we take advantage of the summary shown in the MMS interface to reduce AKE
computational resources, such as processing time, while the AKE performance
degradation is very low. Nevertheless, we create news summaries at both 10%
and 50% CR to use before the AKE and to shown in the MMS interface. At the
presenttime,theMMSinterfaceusesAKEprocesstoidentifythe10topranked
key phrases from top news from 12 TV and 4 Radio channels and generate the
3D key phrase cloud. Although 50% CR seem enough to us, we would like to
analyze in future research what percentages of CR users prefer. Alternatively,
they could prefer to customize this value based on the amount of time available
to interact with the system.
In the future, we plan to augment the centrality-based summarization with
AKE.
6 Authors Suppressed Due to Excessive Length
(a)
(b)
(c)
Fig.2. The percentage of the original text in X-axis vs. AKE metrics in Y-axis. The
evaluation performed in the test set used the Manhattan metric, 10% and 20% SSC
obtained The Precision and Recall extracting: (a) 10, (b) 20 and (c) 30 key phrases.
Fig.3. Avg. key phrase percentage lost in summarization. The results were obtained
whenextracting10keyphrasesinthetestsetusing10%SSCandManhattandistance.
Key Phrase Extraction of Lightly Filtered Broadcast News 7
Table 1. AKE results obtained in the test set using light filtering (p-value ≈ 0.1).
#Key. Extr. %orig. text SSC Dist.Metric #Key.Ident. P R F1
10 100% - - 5.3 53 20.63 29.7
10 90% 20% chebyshev 4.7 47.00 18.45 26.50
10 90% 10 chebyshev 5.3 53.00 19.57 28.59
10 90% 10% manhattan 5.5 55 20.45 29.81
10 90% 5 manhattan 5.0 45.00 17.88 26.05
10 90% 20 manhattan 5.3 53.00 20.67 29.71
10 90% 10% minkowski 4.8 48.00 18.27 26.46
10 90% 8 minkowski 4.6 46.00 17.45 25.30
10 90% 20 minkowski 5.1 51.00 18.84 27.52
10 90% 20% cosine 5.1 51.00 19.34 28.04
10 90% 20% euclidean 4.8 48 18.67 26.88
10 90% 5 euclidean 5.0 50.00 19.37 27.93
20 100% - - 7.4 37 28.21 32.01
20 90% 20% manhattan 6.8 34 25.45 29.11
20 90% 20 manhattan 5.2 52.00 19.63 28.51
20 90% 10% minkowski 7.1 35.5 27.74 31.14
20 90% 8 minkowski 7.6 38.00 29.08 32.95
20 90% 20% euclidean 7.5 37.5 28.43 32.34
20 90% 21 euclidean 7.6 37.50 28.13 32.33
30 100% - - 9.2 30.67 35.12 32.74
30 90% 10% manhattan 8.8 29.33 33.75 31.39
30 90% 5 manhattan 8.9 29.67 33.21 31.34
30 90% 20% minkowski 8.6 28.67 34.48 31.31
30 90% 8 minkowski 9.5 31.67 35.99 33.69
30 90% 20% euclidean 8.8 29.33 32.81 30.97
30 90% 25 euclidean 9.2 30.67 34.57 32.50
40 100% - - 10.3 25.75 38.87 30.98
40 90% 10% manhattan 10.1 25.25 38.44 30.48
40 90% 20% manhattan 9.3 23.25 35.50 28.10
40 90% 8 minkowski 10.6 26.50 40.82 32.14
40 90% 20% minkowski 9.6 24.00 38.00 29.42
40 90% 10% euclidean 9.3 23.25 35.64 28.14
40 90% 20% euclidean 10.3 25.75 38.99 31.02
Acknowledgments
ThisworkwassupportedbyCarnegieMellonPortugalProgramandunderFCT
grant SFRH/BD /33769/2009.
References
1. Carbonell,J.,Goldstein,J.:TheUseofMMR,Diversity-BasedRerankingforRe-
orderingDocumentsandProducingSummaries.In:ACMSIGIR1998.pp.335–336
(1998)
8 Authors Suppressed Due to Excessive Length
2. Chien,L.:Pat-tree-basedkeywordextractionforchineseinformationretrieval.In:
ACM SIGIR 1997. pp. 50–58. ACM, New York, NY, USA (1997)
3. Cohen,J.D.:Highlights:Language-andDomain-IndependentIndexingTermsfor
Abstracting Automatic. English 46(3), 162–174 (1995)
4. Ercan, G., Cicekli, I.: Using lexical chains for keyword extraction. Information
Processing & Management 43(6), 1705 – 1714 (2007), text Summarization
5. Garg, N., Favre, B., Reidhammer, K., Hakkani-Tu¨r, D.: ClusterRank: A Graph
Based Method for Meeting Summarization. In: Proceedings of INTERSPEECH
2009. pp. 1499–1502. ISCA (2009)
6. Gong, Y., Liu, X.: Generic Text Summarization Using Relevance Measure and
Latent Semantic Analysis. In: ACM SIGIR 2001. pp. 19–25. ACM (2001)
7. Harabagiu, S., Lacatusu, F.: Topic Themes for Multi-Document Summarization.
In: ACM SIGIR 2005. pp. 202–209. ACM (2005)
8. Hasan, K., Ng, V.: Conundrums in unsupervised keyphrase extraction: making
sense of the state-of-the-art. In: ACL 2010. pp. 365–373. ACL (2010)
9. Hulth, A., Karlgren, J., Jonsson, A., Bostro¨m, H., Asker, L.: Automatic keyword
extraction using domain knowledge. CICLing pp. 472–482 (2004)
10. Lin,S.H.,Yeh,Y.M.,Chen,B.:ExtractiveSpeechSummarization–FromtheView
of Decision Theory. In: Proceedings of Interspeech 2010. ISCA (2010)
11. Marujo, L., Viveiros, M., Neto, J.P.: Keyphrase Cloud Generation of Broadcast
News. In: Interspeech 2011. ISCA (September 2011)
12. Matsuo,Y.,Ishizuka,M.:Keywordextractionfromasingledocumentusingword
co-ocurrencestatisticalinformation.Inter.JournalonA.I.Tools13,157–170(2004)
13. Medelyan, O., Perrone, V., Witten, I.H.: Subject metadata support powered by
Maui. In: Proceedings of JCDL ’10. p. 407. ACM Press, New York, USA (2010)
14. Neto,J.P.,Meinedo,H.,Viveiros,M.:Amediamonitoringsolution.In:Proceedings
of ICASSP 2011. Prague, Czech Republic (2011)
15. Penn, G., Zhu, X.: A Critical Reassessment of Evaluation Baselines for Speech
Summarization. In: Proceeding of ACL-08: HLT. pp. 470–478. ACL (2008)
16. Ribeiro,R.,deMatos,D.M.:RevisitingCentrality-as-Relevance:SupportSetsand
Similarity as Geometric Proximity. Journal of A.I. Research 42, 275–308 (2011)
17. Salton, G., Yang, C.S., Yu, C.T.: A theory of term importance in automatic text
analysis. Tech. rep., Ithaca, NY, USA (1974)
18. Sarkar,K.,Nasipuri,M.,Ghose,S.:Anewapproachtokeyphraseextractionusing
neural networks. Inter. Journal of Computer Science Issues 7(2,3), 16–25 (2010)
19. Witten,I.,Paynter,G.,Frank,E.,Gutwin,C.,Nevill-Manning,C.:KEA:Practical
automatickeyphraseextraction.In:ProceedingsofthefourthACMconferenceon
Digital libraries. pp. 254–255. ACM (1999)

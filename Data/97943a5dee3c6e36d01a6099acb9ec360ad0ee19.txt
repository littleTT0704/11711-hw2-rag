CharManteau: Character Embedding Models For Portmanteau Creation
VarunGangal∗,HarshJhamtani∗,GrahamNeubig,EduardHovy,EricNyberg
LanguageTechnologiesInstitute,
CarnegieMellonUniversity
{vgangal,jharsh,gneubig,hovy,ehn}@cs.cmu.edu
Abstract
Portmanteaus are a word formation phe-
nomenon where two words are com-
bined to form a new word. We pro-
pose character-level neural sequence-to-
sequence (S2S) methods for the task of
portmanteau generation that are end-to-
end-trainable, language independent, and
Figure 1: A sketch of our BACKWARD, noisy-
do not explicitly use additional phonetic
channel model. The attentional S2S model with
information. Weproposeanoisy-channel-
bidirectional encoder gives P(x|y) and next-
stylemodel,whichallowsfortheincorpo-
character model gives P(y), where y (spime) is
rationofunsupervisedwordlists,improv-
the portmanteau and x = concat(x(1),“;”,x(2))
ing performance over a standard source-
aretheconcatenatedrootwords(spaceandtime).
to-target model. This model is made pos-
sible by an exhaustive candidate genera-
tion strategy specifically enabled by the
features of the portmanteau task. Ex-
is difficult to capture using a set of rules. For
periments find our approach superior to
instance, Shaw et al. (2014) state that the com-
a state-of-the-art FST-based baseline with
position of the portmanteau from its root words
respect to ground truth accuracy and hu-
depends on several factors, two important ones
manevaluation.
being maintaining prosody and retaining charac-
ter segments from the root words, especially the
1 Introduction
head. AnexistingworkbyDeriandKnight(2015)
Portmanteaus(orlexicalblendsAlgeo(1977))are aims to solve the problem of predicting portman-
novel words formed from parts of multiple root teauusingamulti-tapeFSTmodel,whichisdata-
words in order to refer to a new concept which driven, unlike prior approaches. Their methods
can’t otherwise be expressed concisely. Portman- rely on a grapheme to phoneme converter, which
teaus have become frequent in modern-day social takesintoaccountthephoneticfeaturesofthelan-
media, news reports and advertising, one popu- guage, but may not be available or accurate for
lar example being Brexit (Britain + Exit). Petri non-dictionarywords,orlowresourcelanguages.
(2012). These are found not only in English but
Priorworks,suchasFaruquietal.(2016),have
many other languages such as Bahasa Indone-
demonstratedtheefficacyofneuralapproachesfor
sia Dardjowidjojo (1979), Modern Hebrew Bat-
morphological tasks such as inflection. We hy-
El (1996); Berman (1989) and Spanish Pin˜eros
pothesize that such neural methods can (1) pro-
(2004). Their short length makes them ideal for
vide a simpler and more integrated end-to-end
headlinesandbrandnames(Gabler,2015). Unlike
framework than multiple FSTs used in the previ-
better-definedmorphologicalphenomenonsuchas
ous work, and (2) automatically capture features
inflection and derivation, portmanteau generation
suchasphoneticsimilaritythroughtheuseofchar-
∗*denotesequalcontribution acter embeddings, removing the need for explicit
2917
Proceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2917–2922
Copenhagen,Denmark,September7–11,2017.(cid:13)c2017AssociationforComputationalLinguistics
grapheme-to-phoneme prediction. To test these The context vector c is computed using dot-
t
hypotheses,inthispaper,weproposeaneuralS2S product attention over encoder states. We choose
model to predict portmanteaus given the two root dot-product attention because it doesn’t add extra
words,specificallymaking3majorcontributions: parameters, which is important in a low-data sce-
nariosuchasportmanteaugeneration.
• WeproposeanS2Smodelthatattendstothe
two input words to generate portmanteaus,
at =dot(hdec,henc),αt =softmax(at)
and an additional improvement that lever- i t i
ages noisy-channel-style modelling to incor- i=|x|
c = (cid:88) αthenc
poratealanguagemodeloverthevocabulary t i i
i=1
ofwords(§2).
• Instead of using the model to directly pre-
In addition to capturing the fact that port-
dictoutputcharacter-by-character,weusethe
manteaus of two English words typically sound
featuresofportmanteaustoexhaustivelygen-
English-like, and to compensate for the fact that
erate candidates, making scoring using the
available portmanteau data will be small, we pre-
noisychannelmodelpossible(§3).
train the character embeddings on English lan-
• Wecurateandshareanewandlargerdataset
guagewords. Weusecharacterembeddingslearnt
of1624portmanteaus(§4).
using an LSTM language model over words in
In experiments (§5), our model performs better an English dictionary,1 where each word is a se-
than the baseline Deri and Knight (2015) on both quence of characters, and the model will predict
objective and subjective measures, demonstrating next character in sequence conditioned on previ-
thatsuchmethodscanbeusedeffectivelyinamor- ouscharactersinthesequence.
phologicaltask.
2.2 BackwardArchitecture
2 ProposedModels The second proposed model uses Bayes’s rule to
reverse the probabilities P(y|x) = P(x|y)P(y) to
Thissectiondescribesourneuralmodels. P(x)
get argmax P(y|x) = argmax P(x|y)P(y).
y y
2.1 ForwardArchitecture Thus, we have a reverse model of the probabil-
Underourfirstproposedarchitecture,theinputse- ity P(x|y) that the given root words were gen-
quencex = concat(x(1),“;”,x(2)),whiletheout- erated from the portmanteau and a character lan-
put sequence is the portmanteau y. The model guage model model P(y). This is a probability
learnsthedistributionP(y|x). distribution over all character sequences y ∈ A∗,
The network architecture we use is an atten- whereAisthealphabetofthelanguage. Thisway
tional S2S model (Bahdanau et al., 2014). We of factorizing the probability is also known as a
use a bidirectional encoder, which is known to noisychannelmodel,whichhasrecentlyalsobeen
work well for S2S problems with similar token showntobeeffectiveforneuralMT(Hoangetal.
−−−−→
(2017),Yuetal.(2016)). Suchamodelofferstwo
order, which is true in our case. Let LSTM
←−−−−
advantages
andLSTM representtheforwardandreverseen-
coder; e enc() and e dec() represent the character 1. The reverse direction model (or alignment
embeddingfunctionsusedbyencoderanddecoder model)giveshigherprobabilitytothoseport-
Thefollowingequationsdescribethemodel: manteaus from which one can discern the
−→ →− ←− →− root words easily, which is one feature of
henc = 0,henc = 0
0 |x| goodportmanteaus.
−→ −−−−→
he tnc = LSTM(he t−nc 1,e enc(x t)) 2. The character language model P(y) can be
h← en− c = ← LS−− T− M− (henc,e (x )) trainedonalargevocabularyofwordsinthe
t t+1 enc t
language. The likelihood of a word y is fac-
−→ ←−
henc = henc+henc
t t t torized as P(y) = Πi=|y|P(y |yi−1), where
i=1 i 1
hdec = henc yi = y ,y ...y , and we train a LSTM to
0 |x| j i i+1 j
hdec = LSTM(hdec,[concat(e (y ),c )]) maximizethislikelihood.
t t−1 dec t−1 t−1
p = softmax(W [concat(hdec,c )]+b ) 1 Specifically in our experiments, 134K words from the
t hs t t s CMUdictionary(Weide,1998).
2918
3 MakingPredictions Model Attn Ens Init Search Matches Distance
BASELINE - - - - 45.39% 1.59
X × × GREEDY 22.00% 1.98
Given these models, we must make predictions, X × X GREEDY 28.00% 1.90
X × × BEAM 13.25% 2.47
whichwedobytwomethods X × X BEAM 15.25% 2.37
FORWARD X
X
×
×
×
X
S SC CO OR RE
E
3 30 2. .2 85 8%
%
1 1. .6 54
3
GreedyDecoding: In most neural sequence-
X X X SCORE 42.25% 1.33
X X × SCORE 41.25% 1.34
to-sequence models, we perform auto- × × X SCORE 6.75% 3.78
× × × SCORE 6.50% 3.76
regressive greedy decoding, selecting the X × × SCORE 37.00% 1.53
X × X SCORE 42.25% 1.35
next character greedily based on the prob- BACKWARD X
X
X
X
X
×
S SC CO OR RE
E
4 48 6. .7 55 0%
%
1 1. .1 22
4
ability distribution for the next character at × × X SCORE 5.00% 3.95
× × × SCORE 4.75% 3.98
current time step. We refer to this decoding
Table 1: 10-Fold Cross-Validation results, D .
strategyas GREEDY. Wiki
Attn, Ens, Init denote attention, ensembling, and
ExhaustiveGeneration: Many portmanteaus initializingcharacterembeddingsrespectively.
wereobservedtobeconcatenationofaprefix
of the first word and a suffix of the second.
5 Experiments
We therefore generate all candidate outputs
which follow this rule. Thereafter we score
In this section, we show results comparing var-
thesecandidateswiththedecoderandoutput
ious configurations of our model to the base-
theonewiththemaximumscore. Wereferto
line FST model of Deri and Knight (2015)
thisdecodingstrategyas SCORE.
(BASELINE).Modelsareevaluatedusingexact-
matches (Matches) and average Levenshtein edit-
Giventhatourtrainingdataissmallinsize,we
distance(Distance)w.r.tgroundtruth.
expectensembling(Breiman,1996)tohelpreduce
model variance and improve performance. In this 5.1 ObjectiveEvaluationResults
paper, we ensemble our models wherever men-
InExperiment1,wefollowthesamesetupasDeri
tioned by training multiple models on 80% sub-
and Knight (2015). D is split into 10 folds.
samples of the training data, and averaging log Wiki
Eachfoldmodeluses8foldsfortraining,1forval-
probabilityscoresacrosstheensembleattest-time.
idation,and1fortest. Theaverage(10foldcross-
validationstyleapproach)performancemetricson
4 Dataset
thetestfoldarethenevaluated. Table1showsthe
The existing dataset by Deri and Knight (2015) results of Experiment 1 for various model config-
contains 401 portmanteau examples from urations. We get the BASELINE numbers from
Wikipedia. We refer to this dataset as D . Deri and Knight (2015). Our best model obtains
Wiki
Besidesbeingsmallfordetailedevaluation, D 48.75% Matches and 1.12 Distance, compared to
Wiki
is biased by being from just one source. We 45.39%Matchesand1.59DistanceusingBASE-
manuallycollectD ,adatasetof1624distinct LINE.
Large
Englishportmanteausfromfollowingsources: ForExperiment2, weseektocompareourbest
approachesfromExperiment1totheBASELINE
• UrbanDictionary2 onalarge,held-outdataset. Eachmodelistrained
• Wikipedia on D Wiki and tested on D Blind. BASELINE was
similarly trained only on D , making it a fair
• Wiktionary Wiki
comparison. Table 2 shows the results3. Our best
• BCU’sNeologismListsfrom’94to’12. model gets Distance of 1.96 as compared to 2.32
from BASELINE.
Naturally, D ⊂ D . We define D =
Wiki Large Blind WeobservethattheBackward architectureper-
D −D asthedatasetof1223examplesnot
Large Wiki forms better than Forward architecture, confirm-
from Wikipedia. We observed that 84.7% of the
ing our hypothesis in §2.2. In addition, ablation
wordsinD canbegeneratedbyconcatenating
Large results confirm the importance of attention, and
prefixoffirstwordwithasuffixofthesecond.
3For BASELINE (Deri and Knight, 2015), we use
2Not all neologisms are portmanteaus, so we manually theirtrainedmodelfromhttp://leps.isi.edu/fst/
choosethosewhichareforourdataset. step-all.php
2919
Model Attn Ens Init Search Matches Distance Input FORWARD BACKWARD GROUNDTRUTH
BASELINE - - - - 31.56% 2.32 shopping;marathon shopparathon shoathon shopathon
X × X SCORE 25.26% 2.13 fashion;fascism fashism fashism fashism
X × × SCORE 24.93% 2.32 wiki;etiquette wikiquette wiquette wikiquette
FORWARD X X × SCORE 31.23% 1.98 clown;president clowident clownsident clownsident
X X X SCORE 28.94% 2.04
X × X SCORE 25.75% 2.14 Table 3: Example outputs from different models
X × × SCORE 25.26% 2.17
BACKWARD
X X × SCORE 31.72% 1.96 (Refertoappendixformoreexamples)
X X X SCORE 32.78% 1.96
Table 2: Results on D (1223 Examples). In
Blind Judgement Percentageoftotal
general, BACKWARD architecture performs better
MuchBetter(1) 29.06
than FORWARDarchitecture. Better(2) 29.06
Worse(3) 25.11
MuchWorse(4) 16.74
Table 4: AMT annotator judgements on whether
our system’s proposed portmanteau is better or
worsecomparedtothebaseline
In order to do this, we use a paired bootstrap4
comparison (Koehn, 2004) between BACKWARD
and BASELINE intermsofMatches. BACKWARD
is found to be better (gets more Matches) than
Figure 2: Attention matrices while generat-
BASELINEin99.9%(p = 0.999)ofthesubsets.
ing slurve from slider;curve, and bennifer from
Similarly, BACKWARD has a lower Distance
ben;jennifer respectively,usingForward model. ;
than BASELINE byamarginof0.2in99.5%(p =
and . are separator and stop characters. Darker
0.995)ofthesubsets.
cellsarehigher-valued
5.3 SubjectiveEvaluationandAnalysis
Oninspectingoutputs,weobservedthatoftenout-
initializingthewordembeddings. Webelievethis
putfromoursystemseemedgoodinspiteofhigh
isbecauseportmanteaushavehighfidelitytowards
editdistancefromgroundtruth. Suchaspectofan
their root word characters and its critical that the
outputseeminggood isnotcapturedsatisfactorily
model can observe all root sequence characters,
by measures like edit distance. To compare the
whichattentionmanagestodoasshowninFig.2.
errors made by our model to the baseline, we de-
5.1.1 PerformanceonUncoveredExamples signed and conducted a human evaluation task on
The set of candidates generated before scoring AMT.5 In the survey, we show human annotators
in the approximate SCORE decoding approach outputs from our system and that of the baseline.
sometimes do not cover the ground truth. This We ask them to judge which alternative is better
holdstruefor229outof1223examplesinD . overallbasedonfollowingcriteria: 1. Itisagood
Blind
WecomparetheFORWARDapproachalongwitha shorthandfortwooriginalwords2. Itsoundsbet-
GREEDY decoding strategy to the BASELINE ap- ter. We requested annotation on a scale of 1-4.
proachfortheseexamples. To avoid ordering bias, we shuffled the order of
Both FORWARD+GREEDY and the BASELINE two portmanteau between our system and that of
get 0 Matches on these examples. The Distance baseline. WerestrictannotatorstobefromAnglo-
fortheseexamplesis4.52forBASELINEand4.09 phone countries, have HIT Approval Rate > 80%
for FORWARD+GREEDY. Hence, we see that one andpay0.40$perHIT(5QuestionsperHIT).
of our approaches (FORWARD+GREEDY) outper- AsseeninTable4,outputfromoursystemwas
forms BASELINEevenfortheseexamples. labelledbetterbyhumansascomparedtothebase-
line 58.12% of the time. Table 3 shows outputs
5.2 SignificanceTests
fromdifferentmodelsforafewexamples.
Since our dataset is still small relatively small
(1223 examples), it is essential to verify whether 4WeaverageacrossM =1000randomlychosensubsets
ofD ,eachofsizeN =611(≈1223/2)
BACKWARD is indeed statistically significantly 5WBl ein ad
void ground truth comparison because annotators
betterthan BASELINEintermsofMatches. canbebiasedtogroundtruthduetoitsexistingpopularity.
2920
6 RelatedWork References
O¨zbalandStrapparava(2012)generatenewwords John Algeo. 1977. Blends, a structural and systemic
view. Americanspeech,52(1/2):47–64.
to describe a product given its category and prop-
erties. However, their method is limited to hand- DzmitryBahdanau,KyunghyunCho,andYoshuaBen-
crafted rules as compared to our data driven ap- gio. 2014. Neural machine translation by jointly
learningtoalignandtranslate. arXiv:1409.0473.
proach. Also, their focus is on brand names.
Hiranandani et al. (2017) have proposed an ap-
Outi Bat-El. 1996. Selecting the best of the worst:
proach to recommend brand names based on the grammar of Hebrew blends. Phonology,
brand/product description. However, they con- 13(03):283–328.
sideronlyalimitednumberoffeatureslikemem-
Ruth Berman. 1989. The role of blends in Modern
orability and readability. Smith et al. (2014) de-
Hebrew word-formation. Studia linguistica et ori-
viseanapproachtogenerateportmanteaus,which entaliamemoriaeHaimBlancdedicata.Wiesbaden:
requires user-defined weights for attributes like Harrassowitz,pages45–61.
sounding good. Generating a portmanteau from
Leo Breiman. 1996. Bagging predictors. Machine
two root words can be viewed as a S2S problem.
learning,24(2):123–140.
Recently, neural approaches have been used for
S2Sproblems(Sutskeveretal.,2014)suchasMT. Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. A character-level decoder without ex-
Ling et al. (2015) and Chung et al. (2016) have
plicit segmentation for neural machine translation.
shown that character-level neural sequence mod-
arXiv:1603.06147.
els work as well as word-level ones for language
modelling and MT. Zoph and Knight (2016) pro- SoenjonoDardjowidjojo.1979. AcronymicPatternsin
Indonesian. Pacific Linguistics Series C, 45:143–
poseS2Smodelsformulti-sourceMT,whichhave
160.
multi-sequenceinputs,similartoourcase.
Aliya Deri and Kevin Knight. 2015. How to make a
7 Conclusion frenemy: Multitape FSTs for portmanteau genera-
tion. In Proceedings of NAACL-HLT, pages 206–
Wehaveproposedanend-to-endneuralsystemto 210.
model portmanteau generation. Our experiments
ManaalFaruqui,YuliaTsvetkov,GrahamNeubig,and
show the efficacy of proposed system in predict-
ChrisDyer.2016. MorphologicalInflectionGener-
ing portmanteaus given the root words. We con- ationusingCharacterSequencetoSequenceLearn-
clude that pre-training character embeddings on ing. In Proceedings of NAACL-HLT, pages 634–
643.
the English vocabulary helps the model. Through
human evaluation we show that our model’s pre-
NealGabler.2015. TheWeirdScienceofNamingNew
dictions are superior to the baseline. We have Products. New York Times - http://tinyurl.
also released our dataset and code6 to encourage com/lmlq7ex.
further research on the phenomenon of portman-
Gaurush Hiranandani, Pranav Maneriker, and Harsh
teaus. Wealsoreleaseanonlinedemo7 whereour
Jhamtani.2017. Generatingappealingbrandnames.
trainedmodelcanbequeriedforportmanteausug- arXivpreprintarXiv:1706.09335.
gestions. An obvious extension to our work is to
CongDuyVuHoang,GholamrezaHaffari,andTrevor
trysimilarmodelsonmultiplelanguages.
Cohn.2017. DecodingasContinuousOptimization
inNeuralMachineTranslation. arXiv:1701.02854.
Acknowledgements
Philipp Koehn. 2004. Statistical significance tests for
We thank Dongyeop Kang, David Mortensen, machine translation evaluation. In EMNLP, pages
Qinlan Shen and anonymous reviewers for their 388–395.
valuable comments. This research was sup-
WangLing, IsabelTrancoso, ChrisDyer, andAlanW
ported in part by DARPA grant FA8750-12-2-
Black.2015. Character-basedneuralmachinetrans-
0342fundedundertheDEFTprogram. lation. arXiv:1511.04586.
Go¨zdeO¨zbalandCarloStrapparava.2012. Acompu-
6https://github.com/vgtomahawk/ tationalapproachtotheautomationofcreativenam-
Charmanteau-CamReady ing. InProceedingsofACL,pages703–711.Asso-
7http://tinyurl.com/y9x6mvy ciationforComputationalLinguistics.
2921
Alexandra Petri. 2012. Say No to Portman-
teaus. Washington Post - http://tinyurl.
com/kvmep2t.
Carlos-Eduardo Pin˜eros. 2004. The creation of port-
manteaus in the extragrammatical morphology of
Spanish. Probus,16(2):203–240.
Katherine E Shaw, Andrew M White, Elliott More-
ton,andFabianMonrose.2014. Emergentfaithful-
nesstomorphologicalandsemanticheadsinlexical
blends. In Proceedings of the Annual Meetings on
Phonology,volume1.
Michael R Smith, Ryan S Hintze, and Dan Ventura.
2014. Nehovah: Aneologismcreatornomenipsum.
In Proceedings of the International Conference on
ComputationalCreativity,pages173–181.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Neural information processing systems,
pages3104–3112.
R Weide. 1998. The CMU pronunciation dictionary,
release0.6. CarnegieMellonUniversity.
Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefen-
stette,andTomasKocisky.2016. TheNeuralNoisy
Channel. arXiv:1611.02554.
Barret Zoph and Kevin Knight. 2016. Multi-source
neuraltranslation. arXiv:1601.00710.
2922

Smaller Language Models are Better
Black-box Machine-Generated Text Detectors
FatemehsadatMireshghallah1,2,JustusMattern3,SicunGao1
RezaShokri4,TaylorBerg-Kirkpatrick1
1 UniversityofCaliforniaSanDiego,2 UniversityofWashington,3 RWTHAachen
4 NationalUniversityofSingapore
[fatemeh, sicung,tberg]@ucsd.edu,
justus.mattern@rwth-aachen.de,reza@comp.nus.edu.sg
Abstract modelallowsfordetectionoftrainingutterances(Mat-
ternetal.,2023),andgenerationsforagiventarget
As large language models are becoming more
model (Mitchell et al., 2023). Specifically, the
ubiquitousandembeddedindifferentuser-facing
approximate measure of local optimality, dubbed
services,itisimportanttobeabletodistinguish
curvature,isformedbycomparingthelossofatarget
betweenhuman-writtenandmachine-generated
sequence to the loss of nearby perturbations of the
text, to verify the authenticity of news articles,
productreviews,etc. Thus,inthispaperweset targetsequence,underthetargetmodel. Theintuition
outtoexplorewhetheritispossibletouseone inbothpriorworksisthatthismeasureofcurvature
language model to identify machine-generated ishigher aroundbothtrainingexamplesandmodel
textproducedbyanotherlanguagemodel,even generations,comparedtounseenhuman-writtentext
if the two have different architectures and are
and can therefore be used to determine if a given
trained on different data. Further, if this is
sequenceispartofthetrainingdataornot(Mattern
possible,whichlanguagemodelsmakethebest
etal.,2023)oragenerationofthetarget(generator)
general-purposedetectors?Wefindthatoverall,
smallerandpartially-trainedmodelsarebetter modelornot(Mitchelletal.,2023).
universalmachine-generatedtextdetectors:they Inpractice,however,weoftenwanttodistinguish
can more precisely detect text generated from
betweenmachine-generatedtextandhuman-written
bothsmallandlargermodels. Interestingly,we
textinsituationswherewedonotknowwhichmodel
findthatwhetherornotthedetectorandgenerator
couldhavebeenusedasthegenerator—andevenif
modelsweretrainedonthesamedataorhavesim-
wedoknowthegenerator,wemightnothaveaccess
ilarparametercountsisnotcriticallyimportantto
thedetectionsuccess.ForinstancetheOPT-125M toitslikelihoodfunction(e.g. ChatGPT),oraccess
modelhasanAUCof0.81indetectingChatGPT mightbebehindapaywall(e.g. GPT3). Therefore,
generations, whereas a larger model from the in this paper we set out to explore the detection of
GPTfamily,GPTJ-6B,hasAUCof0.45. machine-generatedtextwithoutknowledgeofthegen-
erator. Wedothisbyexploringwhetheritispossible
1 Introduction
tousethecurvaturemetricmeasuredononelanguage
With the rapid improvement in fluency of the text model (a detector model) to identify machine-
generated by large language models (LLMs), these generatedtextgeneratedbyanotherlanguagemodel
systemarebeingadoptedmoreandmorebroadlyina (thegenerator),andunderwhatconditionssuchcross-
widerangeofapplications,includingchatbots,writing detection performs best. We use surrogate detector
assistants,andsummarizers. Generationsfromthese models,whoselikelihoodfunctionswedohaveaccess
modelsareshowntohavehuman-likefluency(Liang to. Then,werunthecurvaturetestusingthesurrogate
etal.,2022;Yuanetal.,2022),makingitdifficultfor (seeFigure1)andcomparedetectionpowerwiththe
humanreaderstodifferentiatemachine-generatedtext sametest,butusingthetruegenerator’slikelihood.
fromhuman-writtentext. Thiscanhavesignificant We conduct an extensive empirical analysis by
ramifications,assuchLLM-basedtoolscanbeabused experimenting on a slew of models with different
forunethicalpurposeslikephishing,astroturfing,and sizes(fromtensofmillionstobillionsofparameters),
generatingfakenews(Heetal.,2023). Assuch,we architectures(GPTs,OPTs,Pythias)andpre-training
needtobeabletoreliablyandautomaticallydetect data (Webtext and the Pile) and also from different
machinegeneratedtext. trainingstages(rangingfromthefirstthousandsteps
Previous work has found that identifying local of training to full training– 143k steps). Our main
optimainthelikelihoodsurfaceofatrainedlanguage finding is that cross-detection can come very close
4202
beF
21
]LC.sc[
3v95890.5032:viXra
Perturbation Model
Human T5
Written Text Perturbed
Target Pool Machine
generated?
Detector Model
Generative Model
GPT2
Prompts ChatGPT Machine Target Pool
Generated Text
Figure1: Wewanttostudyhowmodelscancross-detect, i.e.distinguishbetweenhuman-writtentextandmachine-
generatedtextgeneratedbyanothermodel. Tothisend,wecreateatargetpoolconsistingofbothhuman-writtenand
machine-generatedtext. Wethengenerateperturbationsofeachtargetsequenceusingaperturbationmodel. Wefind
thelikelihoodofthetargetpoolandperturbationsunderadetectormodelinordertoestimatethelocaloptimalityunderthe
detectormodel’slikelihood.Weusetheestimateoflocaloptimalitytodetermineifasequenceismachinegeneratedornot.
to self-detection in terms of distinguishablity, and We form this pool such that there is a 50%/50%
that there are universal cross-detectors with high composition of machine-generated/human-written
averagedistinguishablityperformance,meaningthey text. The machine-generated text is created by
performwellintermsofdetectinggenerationsfroma prompting the generator model with the first 20
wide-rangeofmodels,regardlessofthearchitectureor tokensofeachhuman-writtensequence.
trainingdata. Morespecifically,wefindthatsmaller Generator model. This model is the generator of
models are better universal detectors. For instance the machine-generated utterances we would like to
theOPT-125Mmodelcomeswithin0.07areaunder distinguish from human-written utterances. We do
the ROC curve of self-detection, on average (see not always have full access to this model, or even
Figure 10). And for models where we don’t have know what model it is. This scenario is what we
self-detection, suchasChatGPT,theAUCofusing areactuallyinterestedin: wewanttoknowhowwe
OPT-125Mis0.81,whereasOPT6.7B’sAUCis0.58. detecttextgeneratedbyunknownmodels.
We also find that partially trained models are Detectormodel.Thismodelisusedasasurrogate
better detectors than the fully trained ones, and for the target model, to help us detect generations
this gap is bigger for larger models (see Figure 9). whenusingthecurvaturetest. Thepoolofsequences
We then further investigate some possible reasons andtheirneighborsarefedtothedetectormodel,and
for this phenomenon by analyzing curvature and theirlossunderthedetectormodelismeasuredand
log-likelihoodofthedifferentmodels,andfindthat usedtocalculatecurvatureandtodistinguishbetween
largermodelsaremoreconservativeintermsofthe generationsandhumanwrittentext.
likelihood and curvature they assign to generations Curvature(localoptimality)test.Themethodwe
fromothermodels. Smallermodels,however,assign use to distinguish between machine-generated and
highercurvaturetogenerationsofmodelstheirsize human-written text relies on the local optimality
orlarger, thereforetheycanbeusedtocross-detect (curvature) of the target sequence, building on the
onabroaderrangeofmodelssothesmallermodel intuitionthatgenerationsaremorelikelytobelocally
isthebestuniversaldetector. optimal than unseen human-written text (Mitchell
etal.,2023;Matternetal.,2023).
2 Methodology
To estimate local optimality, following Mattern
Figure 1 shows the methodology of our work, and et al. (2023); Mitchell et al. (2023), we generate
howweconductourexperiments: Foragiventarget additionalutterancesinalocalneighborhoodaround
pool of sequences, the task is to determine if each thetargetbyperturbingthetarget(e.g. re-sampling
sequenceishuman-writtenormachine-generatedby words at several positions). Then, the measure
runningacurvature(localoptimality)testusingthe of local optimality is computed by comparing the
likelihoodsurfaceofasurrogatedetectormodelthat likelihoodofthetargetwiththelikelihoodofthelocal
is different from the generator model, as our main perturbationsasfollows:
assumptionisthatwehavenoinformationaboutthe
generatormodel. Intherestofthissectionwedelve 1(cid:88)k
d(x)=logp (x)− logp (x˜) (1)
deeperintothedetailsofeachcomponentinthesetup. θ k θ i
i=1
Target pool. The pool of sequences for which we
wanttoconductthemachine-generatedtextdetection. Here,xisthetargetsequence,θaretheparameters
ofthedetectormodel,andx˜ istheithperturbation feedthistothetargetmodel,andhaveitgeneratecom-
i
ofthetargetutterancex(i.e. theithneighbor)outof pletionsforit.Wethenusethismixofgenerationsand
theoverallkperturbations. Theperturbedsequences human-writtentexttocreatethetargetpoolonwhich
are generated by masking parts of x and filling the wedothedetection. Inallcases,ourpoolconsistsof
mask using a perturbation model. The curvature is 300human-writtentargetsamples,and300machine-
thresholdedtomakethemachine-generated/human- generatedsamples,sotheoverallpoolsizeis600.
writtentextdecision. Whiletechnicallythismeasure Pre-trainingdatasetsforthegeneratormodels.The
is an approximate estimate of local optimality, past ElutherAI and Facebook models (GPTJ, GPTNeo,
workhasreferredtoitas’curvature’. Forsimplicity, Pythia and OPT families) are all trained on the
weusethisnomenclaturegoingforward. Pile dataset (Gao et al., 2020). There is limited
Perturbation model. This model helps generate information and access to the training data of the
neighborsbyfillinginrandomlyselectedspansofthe OpenAI models. The GPT-2 family is reportedly
targetsequencesinthepoolandperturbingthem. We trainedontheWebTextdataset,GPT-3istrainedon
useT5-3Bforthispurposeinourexperiments. a combination of the Common Crawl, WebText2,
Success metric. We evaluate the success of the booksandWikipedia,andthereisnotanyinformation
detectorbymeasuringtheareaundertheROCcurve releasedaboutthetrainingdataofChatGPT.
(AUC), i.e. the false positive vs. true positive rate
curve. ThehighertheAUC,themoredistinguishing 4 Doescross-detectionwork?
powerthedetectionmechanismhas.
In this section we conduct an extensive set of
Evaluation strategy. The results we report in the
experimentswhereweuse23modelswithdifferent
paperfallintotwomaincategories: (1)usingamodel
sizesandarchitecturesasdetectorsoftextgenerated
todetectitsowngenerations,whichisthemaingoal
by 15 other models. The results are averaged over
ofMitchelletal.(2023)(inthissetup,thetargetandde-
the SQuAD and WritingPrompts dataset. We also
tectormodelsarethesame,wecallthisself-detection);
experimentwithpartiallytrainedcheckpointsofthe
and (2) using a model different from the generator
detectormodels, toseehowthedetectionpowerof
of the text to detect the generations. In this setup,
themodelschangesasthetrainingprogresses.
whatwearebasicallydoingisactingasifasurrogate
Our main finding is that cross detection can per-
modelhasgeneratedthetext.Inotherwords,wewant
formaswellasself-detection,orcomeveryclosetoit.
toseehowwellamodelwouldclaimanothermodel’s
Figures2and3(fullheatmapisFig.8inAppendix)
generationasitsown. Wecallthiscross-detection.
showtheAUCofcross-detectionfordifferentmodels.
Figures10and9inAppendixshowhowcloseeachde-
3 ExperimentalSetup
tectorcomes,intermsofAUC,toself-detection. We
This section briefly covers the experimental setup. canseethatonaverage,OPT-125Misthebestfully
FormoredetailsrefertoAppendixB. traineduniversalcross-detector,showingonaverage
ModelsWeusethefollowingmodelfamiliesinour 0.07lowerAUC,comparedtoself-detection. Ifwe
experiments: Facebook’s OPT (we use the 125M, lookatpartiallytraineddetectormodels,however,we
350M,1.3B,and6.7Bmodels),EleutherAI’sGPT-J, seethatthePythia-160Mcomesascloseas0.05AUC
GPTNeo and Pythia (Biderman et al., 2023) (we points, with its 5k, 10k and 50k step trained mod-
useGPTNeo-125M,GPTNeo-1.3B,GPTNeo-2.7B, els(thefullytrainedmodelistrainedfor143ksteps).
GPTJ-6B and Pythia models ranging from 70M to Thesemodelsseemtoevenoutperformself-detection
2.8B parameters), and OpenAI’s GPT models (dis- insomecases,forexamplewhenwelookatGPTJ-6B
tilGPT,GPT2-Small, GPT2-Medium, GPT2-Large, generations. Intherestofthissectionwefurtherelab-
GPT2-XL,GPT-3andChatGPT). orateontheseresultsanddrawconnectionsbetween
Evaluation dataset. We follow Mitchell et al. modelsize,training,anddetectionpower.
(2023)’smethodologyforpre-processingandfeeding
4.1 SmallerModelsAreBetterDetectors
thedatatothedetectormodel. Weuseasubsample
oftheSQuAD(Rajpurkaretal.,2016)andWriting- In this section we aim to see if there are any
Prompts(Fanetal.,2018)datasets,wheretheoriginal correlationsbetweenmodelsizeanddetectionpower.
datasetsequencesareusedasthehuman-writtentext Tothisend,weuse23differentmodelswithdifferent
inthetargetsequencepool.Wethenusethefirst20to- parameter counts, ranging from 70M to 6.7B to
kensofeachhuman-writtensequenceasaprompt,and detectmachine-generationtextsfromallthemodels
AUC of Distinguishing Human Text from Generations
1.0
ChatGPT 0.67 0.62 0.67 0.67 0.72 0.79 0.72 0.75 0.80 0.79 0.83 0.80 0.82 0.84 0.84 0.82 0.83 0.86 0.84 0.86 0.84 0.83 0.83
GPT3 0.70 0.69 0.73 0.73 0.74 0.78 0.75 0.76 0.77 0.78 0.79 0.79 0.79 0.79 0.81 0.82 0.82 0.82 0.84 0.80 0.80 0.84 0.84
OPT-6.7B 0.93 0.64 0.74 0.74 0.78 0.81 0.79 0.81 0.91 0.84 0.84 0.84 0.84 0.84 0.91 0.85 0.86 0.90 0.87 0.85 0.83 0.85 0.85
0.8
GPTJ-6B 0.72 0.88 0.77 0.77 0.81 0.78 0.82 0.82 0.84 0.85 0.82 0.85 0.85 0.82 0.86 0.86 0.86 0.87 0.87 0.83 0.82 0.86 0.86
GPTNeo-2.7B 0.62 0.56 0.72 0.71 0.95 0.81 0.83 0.83 0.83 0.87 0.86 0.87 0.89 0.87 0.88 0.88 0.88 0.89 0.89 0.88 0.84 0.87 0.87
GPT2-XL 0.61 0.52 0.68 0.68 0.78 0.98 0.81 0.82 0.86 0.86 0.94 0.89 0.90 0.95 0.93 0.91 0.91 0.93 0.92 0.94 0.90 0.89 0.90
OPT-1.3B 0.69 0.53 0.66 0.65 0.74 0.82 0.79 0.79 0.98 0.83 0.88 0.87 0.89 0.89 0.95 0.91 0.90 0.95 0.91 0.91 0.89 0.89 0.90 0.6
GPTNeo-1.3B 0.55 0.49 0.67 0.65 0.80 0.80 0.83 0.84 0.84 0.99 0.88 0.92 0.93 0.90 0.93 0.94 0.93 0.94 0.95 0.91 0.90 0.92 0.92
GPT2-Large 0.55 0.46 0.64 0.63 0.75 0.89 0.81 0.82 0.85 0.86 0.99 0.91 0.92 0.96 0.95 0.93 0.93 0.95 0.94 0.96 0.93 0.92 0.92
GPT2-Medium 0.40 0.33 0.53 0.49 0.64 0.81 0.72 0.73 0.77 0.80 0.92 0.90 0.91 1.00 0.95 0.94 0.93 0.95 0.95 0.97 0.94 0.92 0.93 0.4
OPT-350M 0.46 0.35 0.52 0.50 0.63 0.73 0.72 0.73 0.82 0.80 0.86 0.91 0.91 0.91 1.00 0.96 0.96 0.99 0.97 0.96 0.97 0.97 0.96
OPT-125M 0.33 0.25 0.38 0.37 0.47 0.57 0.57 0.57 0.67 0.67 0.73 0.84 0.83 0.83 0.96 0.96 0.96 1.00 0.97 0.96 0.98 0.97 0.97
GPTNeo-125M 0.24 0.21 0.32 0.30 0.41 0.48 0.51 0.50 0.52 0.60 0.65 0.82 0.80 0.77 0.91 0.96 0.96 0.96 1.00 0.96 0.97 0.98 0.97
0.2
GPT2 0.24 0.21 0.34 0.32 0.43 0.58 0.55 0.56 0.58 0.64 0.77 0.84 0.84 0.89 0.93 0.96 0.95 0.98 0.97 1.00 0.99 0.97 0.97
DistilGPT2 0.15 0.14 0.19 0.18 0.27 0.24 0.33 0.34 0.33 0.42 0.39 0.62 0.60 0.54 0.76 0.89 0.88 0.91 0.93 0.88 1.00 0.97 0.97
Mean 0.52 0.46 0.57 0.56 0.66 0.72 0.70 0.71 0.76 0.77 0.81 0.84 0.85 0.85 0.91 0.91 0.90 0.93 0.92 0.91 0.91 0.91 0.91
0.0
Detector Model
Figure2:AUCheatmapforcross-detection,wheretherowsaregeneratormodelsandcolumnsarethesurrogatedetector
models,bothsortedbymodelsize.Wecanseethatsmallermodelsarebetterdetectorsandlargermodelsaretheworst
modelsintermsofdetectionpower.
listedinSection3. from 0.89-0.87 (OPT-6.7B self-detects with AUC
Figure 2 shows the results for this experiment, 0.91). The next best cross-detector is the smallest
wheretherowsarethegeneratormodels(sizingup GPTNeo-125MwithAUC0.86. However,theOpe-
frombottomrowtotop)andthecolumnsshowthe nAIGPT2modelofthesamesizehasalowerAUC
detectormodels(sizingupfromrighttoleft). Soeach of0.84(andoveralltheGPT2familyhasthelowest
cell shows the detection power (AUC) of the given cross-detectionAUConOPT),whichwehypothesize
detectormodel(column),ontextgeneratedfromthe is due to the larger gap in the training data, as the
generator model (row). The last row is the mean, OPT and GPTNeo/GPTJ models are all trained on
whichisanoverallmetricofhowgoodofadetector thePiledataset,butGPT2istrainedontheWebtext.
thatmodelis. Allinall,thedifferenceduetothedataset/architecture
differencesissmallasmostofthedatasetforallthese
Weseethatthebottomlefthasthelowestvalues,
modelsiscomprisedofweb-crawleddata,showing
showingthatlargermodelsarenotgoodatdetecting
that cross-detection can be effective, regardless of
machine generated text from other models, and
how much information we have about the target
they are particularly bad at it for detecting small
model,andhowaccessiblesimilarmodelsare.
model generations. We can also see that smaller
models are much better detectors, as the right One noteworthy observation is that OPT-125M
side of the graph has much higher AUC values. can detect generations from models like GPT3 and
Anotherobservationisthecorrelationsbetweenthe ChatGPTwithrelativelyhighAUC(0.81). However,
dataset and model architecture of the generator if the intuitive approach of taking another large,
anddetectormodels. Astheheatmapshows,models “similar”modelweretobetakenandweweretouse
from the same architecture family and trained on OPT-6.7B,wewouldgetAUCof0.67and0.58for
thesame/overlappingdatasetarebetteratdetecting these models, respectively, which are both close to
theirowntext,comparedtomodelsfromadifferent random(0.5). Thus, intuitively, itseemsthatlarger
family. Forinstance,fordetectingtextgeneratedby models have more refined taste: they only show
OPT-6.7B the other models from the OPT family higher local optimality (relative to human-written
are the best cross-detectors, with AUCs ranging text)ongenerationsfromlargemodels. Conversely,
ledoM
evitareneG
B7.6-TPO B6-JTPG B8.2-aihtyP dd-B8.2-aihtyP B7.2-oeNTPG LX-2TPG B4.1-aihtyP dd-B4.1-aihtyP B3.1-TPO B3.1-oeNTPG egraL-2TPG M014-aihtyP dd-M014-aihtyP muideM-2TPG M053-TPO M061-aihtyP dd-M061-aihtyP M521-TPO M521-oeNTPG 2TPG 2TPGlitsiD M07-aihtyP dd-M07-aihtyP
Generative Model Generative Model Generative Model
0.95 C Gh Pa Tt 3GPT 0.95 C Gh Pa Tt 3GPT 0.9 C Gh Pa Tt 3GPT
OPT-6.7B OPT-6.7B 0.8 OPT-6.7B
OPT-125M OPT-125M OPT-125M
0.90 Mean 0.90 Mean 0.7 Mean
0.6
0.85
0.85
0.5
0.80 0.80 0.4
0.75 0.3
0.75
0.2
0 20000 40000 60000 80000 100000120000140000 0 20000 40000 60000 80000 100000120000140000 0 20000 40000 60000 80000 100000120000140000
Training Step Training Step Training Step
(a)Pythia70M (b)Pythia410M (c)Pythia2.8B
Figure3: Summaryoftheresultsforcross-detectionpowerofdifferentdetectormodelstrainedfordifferentnumber
ofsteps.Eachsubfigureshowsadifferentdetectormodel,andthex-axisshowsthetrainingstepforthecheckpointused
asadetector.Theresultsforall15generatormodelsareshowninFigure8.
0.150 Machine-generated 0.150 Machine-generated 0.20 Machine-generated
0.125 H Du etm ea ctn o- rw Mrit ot de en l 0.125 H Du etm ea ctn o- rw Mrit ot de en l H Du etm ea ctn o- rw Mrit ot de en l
0.100 0.15
0.100
0.075
0.075 0.10
0.050
0.050
0.025 0.05
0.025
0.000 0.000 0.00
0.025
0.025
0.050 0.05
108 109 1010 1011 108 109 1010 1011 108 109 1010 1011
#Params #Params #Params
(a)Curvature:OPT-125MasDetector (b)Curvature:OPT-350MasDetector (c)Curvature:OPT-6.7BasDetector
2.4 Machine-generated 2.2 Machine-generated 1.75 Machine-generated
2.6 H Du etm ea ctn o- rw Mrit ot de en l 2.4 H Du etm ea ctn o- rw Mrit ot de en l 2.00 H Du etm ea ctn o- rw Mrit ot de en l
2.8 2.6 2.25
3.0 2.8 2.50
3.2 3.0 2.75
3.4 3.2 3.00
3.6 3.4 3.25
3.8 3.6 3.50
108 109 1010 1011 108 109 1010 1011 108 109 1010 1011
#Params #Params #Params
(d)Loglikelihood:OPT-125MasDetector (e)Logliklihood:OPT-350MasDetector (f)Logliklihood:OPT-6.7BasDetector
Figure 4: Comparison of curvature and log likelihood values (mean and standard deviation) for the best universal
detector(OPT-125M),amediumsizeddetector(OPT-350M),andalargerdetectorfromthesamefamily(OPT-6.7B)
ongenerationsfrommodelsofvarioussizes(x-axis). The’DetectorModel’lineshowsvaluesforwhenthegenerator
anddetectorarethesamemodel.Detectorstendtoshowhighercurvatureongenerationsthanhuman-writtentextonly
forgenerationsfrommodelsofthesamesizeorlarger.
smallermodelsaremoreforgiving: theyshowhigher theworstoneintermsofmachine-generatedtext
localoptimalityongenerationsfromsimilarlysmall detection, and it is one of the middle checkpoints
models and larger, making them better universal thathasthebestperformance. Ourhypothesisforthis
detectorsvialocaloptimalitycomparison. Wediscuss issimilartothatofSection4,wherewebelievethat
thisfurtherinSection5. partiallytrainedmodelshavenotyetfittothetraining
data tightly (and have a smoother surface), so they
4.2 Partially over claim other models’ generations as their own,
TrainedModelsareBetterDetectors whereasthelongeramodelistrained,thesequences
itrankshigherasitsownnarrowdown.
WetakedifferenttrainingcheckpointsofthePythia
models(Bidermanetal.,2023)atdifferentsteps(steps 5 Howaresmallermodelsbetterdetectors?
1k,5k,10k,50k,100kand143k)withdifferentsizes
(2.8B,410M,and70M),andusethemasdetectorsof Tohelpshedlightonwhysmallermodelsarebetter
generationsfromthe4targetmodels. Figure3shows detectorsandlargermodelsarenotgoodatdetecting
theresultsforthisexperiment(Figures8and9show machinegeneratedtext,weplotabreakdownofthe
entireheatmapsofthisexperiment). Foreachmodel curvaturemetric(Section2)andlog-likelihoodvalues
wecanseethat thefinalcheckpointisconsistently forthebestuniversaldetector(OPT-125M),amedium
CUA
erutavruC
doohilekiL
goL
CUA
erutavruC
doohilekiL
goL
CUA
erutavruC
doohilekiL
goL
such,smallermodelsarebetteruniversaldetectors,
1.0 asthesizeofthesetofsequencestheyassignhigher
likelihoodandcurvaturetoisbiggerthanitisforlarge
0.8
models,andthishighercurvatureismuchhigherthan
0.6 thecurvatureassignedtothehumanwrittentext. The
spikes in all the sub-figures of Figure 4 graphs are
0.4
forthedetectormodeldetectingitsowntext.
OPT-125M
OPT-350M
0.2 OPT-6.7B
Self-detect 6 Doesneighborhoodchoicematter?
108 109 1010 1011
#Params
Ourestimationof“curvature”hingesupongenerating
Figure5:AUCofthethreecross-detectorsfromFigure4
numerousperturbations(neighbors)andcomparing
their loss with that of a target point. Therefore, if
sizeddetectorofthesamefamily(OPT-350M)and theseperturbedneighborsarenotsufficientlynearby
a larger one from the same family (OPT-6.7B), andlieinadifferentbasinofthelikelihoodsurface,
shown in Figure 4. The y-axis is the curvature/log ourmeasureofcurvatureisnotaccurate(thecloser
likelihood of the target generations (from the 15 theperturbedpointsare,themoreaccurateestimation
modelsfromSectionB.1)underthedetectormodels ofcurvatureweachieve). Theperturbationmethoddi-
(OPT-125M, 350M or 6.7B). The x-axis is the rectlyimpactsthesizeandshapeoftheneighborhood
numberofparametersofthegeneratormodel(wedo we create. Therefore, we compare different pertur-
notknowhowmanyparametersChatGPThas,sowe bationschemesinordertoseehowsensitivedetectors
plotteditastherightmostpointintheplots). Figure5 ofdifferentsizesaretoneighborhoodchoice.
plotstheAUCsfordetectionunderthethreemodels, Weinvestigatetwodifferentmethodsforchanging
forthe15generatormodels. the distance of the generated perturbations: (1) we
We can see that for the smaller detector model changethemaskfillingmodelsize,byexperimenting
(Figures 4a and 4d), the mean curvature and withT5-Small,T5-LargeandT5-3B(Wolfetal.,2019;
log-likelihoodvaluesforthegeneratedtextareconsis- Raffeletal.,2020)totesttheintuitionthatlargermask-
tentlyhigherthanthecurvatureforthehuman-written fillingmodels,generatesemanticallycloserneighbors
text.However,forthelargermodel(Figure4cand4f), thanasmallermodel,wepresenttheextendedresults
the curvature and log-likelihood values for the for this in Appendix A. A similar analysis is also
machine-generatedtextisinmostcasessmallerthan conductedin(Mitchelletal.,2023),wehowever,do
oraroundthesamevalueasthehumanwrittentext. a more extensive analysis on numerous models of
The curvature and log-likelihood values for human differentsizesandprobethecurvaturevalues. (2)We
writtentextforbothgraphsarestablesincethetext changethepercentageofthetokensthatgetmasked
isthesameanddoesn’tdependonthetargetmodel. andreplacedbythemask-fillingmodel,asthemore
We can also see that overall the curvature and tokenswemaskandreplace,thefartherthegenerated
likelihood values for the larger model are higher, perturbationswouldbe. (3)Finally,welookintohow
especially for the original text, than those of the manytokensweactuallyneedinthegenerated/human-
smallermodel,andthevaluesfortextgeneratedby written sequences to create a neighborhood and be
theothermodelshavelowercurvatureandlikelihood abletoaccuratelydistinguishthetexts.
value. Thisshowsthatthelargermodelplaceshigher
6.1 MaskingPercentage
likelihoodonthehumanwrittentextandfitsitbetter.
Thesmallermodel,however,assignslowercurvature Figure6showstheresultsfortheexperimentwhere
andlikelihoodtothehuman-writtentextcomparedto wechangethepercentageoftokensthataremasked,
generationsbyalargegap,andtheassignedvaluesare toproducetheneighbors. Inallpreviousexperiments,
overalllowerthanthoseofthelargemodel. Broadly weused15%maskingwithmaskspanlengthof2to-
we observe that all models respond similarly to kensfollowingtheexperimentalsetupinMitchelletal.
machinegeneratedtextfromothermodels,solong (2023). Inthissection,however,wechangetheper-
astheothermodelissamesizeorbigger.Inother centageofthemaskedtokens(andwesetthemasking
words,theyplacehighlikelihoodontextfromlarger tobecontiguous)toseehowitaffectsthecurvature
models. However, for models smaller than them- meanandstandarddeviationvalues,andtheAUCs.
selves,theyplacelowerlikelihoodandcurvature. As Wecanseethatasthemaskingpercentagedecreases
CUA
2.2 Machine-generated 0.6 Machine-generated 0.16 Machine-generated
2.0 Human-written Human-written 0.14 Human-written
1.8 0.5 0.12
1.6 0.4 0.10
1.4
1.2 0.3 0.08
0.06
1.0 0.2
0.8 0.04
0.6 0.1 0.02
108 109 108 109 108 109
#Params #Params #Params
(a)90%Masking (b)50%Masking (c)15%Masking
0.030 Machine-generated Machine-generated 0.95
Human-written 0.015 Human-written
0.025 0.90
0.020 0.010 0.85
0.015 0.80
0.010 0.005 0.75
0.005 0.70 Mask Pct
0.000 0.000 0.65 0 0. .0 01 2
0.005 0.15
0.005 0.60 0.5
0.010 0.9
108 109 108 109 0.55 108 109
#Params #Params #Params
(d)2%Masking (e)1%Masking (f)AUCsfordifferentmaskingpctgs.
Figure6:Theeffectofchangingthemaskingpercentageoncurvaturevaluesandself-detectionpowerofdifferentmodels
withdifferentsizes(AUC).
(from90%to2%),theAUCsandtheself-detection
powerofmodelsincreaseratherconsistently. When
1.0 Distinguisher Model
OPT-125M
wegoto1%,however,weseetheAUCdrop. Ifwe OPT-350M
0.9 OPT-1.3B
lookatFigure6ewhichdepictsthecurvaturemeasures OPT-6.7B
forthe1%masking,weseethatthecurvaturesover- 0.8
lapbetweenmachine-generatedandhuman-written
0.7
text,whichwehypothesizeisbecauseourimplemen-
0.6
tationdoesnotenforcethatre-sampledwordsmust
differfromthewordstheyarereplacing.Thus,forthe 0.5
25 50 75 100 125 150 175 200
smallestmaskingpercentage,itispossiblethatsome Token Length
perturbations are identical to the target, which may Figure 7: Detectability as a function of candidate
explainreduceddetectionaccuracyinthissetting1. utterancelength.Asexpected,longerutterancesaremore
cross-detectable–thoughit’sworthnotingthatutterances
6.2 Howmanytokensdoweneedfordetection? asshortas60tokenslongarestillcross-detectablewith
relativelyhighaccuracy.
Figure7showshowthelengthofthetargetsequence
affectsthesequence’sdetectablity(AUCofdetection),
and how many tokens we need to be able to do
7 RelatedWork
precisedetection. Wecomparesequencesofdifferent
lengths, ranging from 10 tokens to 200, for four
Theproblemofmachine-generatedtextdetectionhas
different models with four different parameter
alreadybeenstudiedformultipleyearsusingavariety
counts, on the SQuAD dataset. In this setup we
ofdifferentapproaches(Ippolitoetal.,2020;Jawahar
target self-detection. We can see that the longer
et al., 2020; Uchendu et al., 2020, 2021): Both
the sequence, the easier it is to distinguish if it is
Gehrmannetal.(2019)andDuganetal.(2022)have
human-written or machine-generated, and 75-100
foundthathumansgenerallystruggletodistinguish
tokensseemslikethepointwherewehitdiminishing
betweenhuman-andmachine-generatedtext,thereby
returns.Wecanalsoseethatacrossdifferentsequence
motivating the development of automatic solutions.
lengths, as models get smaller, the detection power
Amongthose,somemethodsaimtodetectmachine-
increases,asseenthroughouttherestofthepaper.
generatedtextbytrainingaclassifierinasupervised
1Itsnoteworthythattheslightdiscrepancybetweentheresults manner(Bakhtinetal.,2019;Uchenduetal.,2020),
for15%maskinginthissectionandtheprevioussectionisthat whileothersperformdetectioninazero-shotmanner
there,themaskspanlengthwas2sothemaskedportionofthe
(Solaimanetal.,2019;Ippolitoetal.,2020). Thereis
sequenceisnotcontiguous.Inthisexperiment,however,weuse
contiguousmasking. alsoalineofworkthatreliesonbotdetectionthrough
erutavruC
erutavruC
erutavruC
erutavruC
CUA
erutavruC
CUA
question answering (Wang et al., 2023; Chew and 8 Conclusion
Baird,2003),whichisoutsidethescopeofthispaper.
With the increasing prevalence of LLMs and their
Mostrecently,Mitchelletal.(2023)introducedthe integrationintovariousdifferentservices,itbecomes
zero-shotmethodDetectGPT,whichisbasedonthe crucial to differentiate between text written by
hypothesis that texts generated from a LLM lie on humansandtextgeneratedbymachinessoastoavoid
localmaxima,andthereforenegativecurvature,ofthe fakenewsandimpersonations. Assuch,wesetout
model’sprobabilitydistribution. Otherstrategieshave toexplorethepossibilitiesofusingexistingmodels
been proposed to enable the detection of machine- to detect generations from unknown sources, and
generatedtextinthewild. Particularlythroughefforts distinguishthemfromhumanwrittentext. Wefind
on the side of the LLM provider, more powerful thatwhenusingzero-shotdetectionmethodsthatrely
detectionmethodscanbedevised. Onesuchmethod onlocaloptimality,smallermodelsareoverallbetter
is watermarking, which injects algorithmically atdetectinggenerations,andlargermodelsarepoor
detectablepatternsintothereleasedtextwhileideally detectors. Our results offer hope of robust general
preserving the quality and diversity of language purposeprotectionagainstLLMsusedwithnefarious
model outputs. Watermarks for natural language intentions.However,asLLMscontinuetochangeand
havealreadybeenproposedbyAtallahetal.(2001) detection evasion methods become more prevalent,
and have since been adapted for outputs of neural somustmethodsfordetectionandvalidationstudies.
language models (Fang et al., 2017; Ziegler et al.,
Limitations
2019). Notablerecentattemptsfortransformerbased
languagemodelsincludeworkbyAbdelnabiandFritz AlthoughweseehighAUCsforblack-boxdetection
(2021), who propose an adversarial watermarking of machine generated text in our experiments, this
transformer(AWT).Whilethiswatermarkingmethod doesnotnecessarilymeanthatthesedetectionmeth-
isdependentonthemodelarchitecture,Kirchenbauer ods are not avoidable, and that they can be applied
etal.(2023)proposeawatermarkthatcanbeapplied toallmodelsandachievehighperformance. Further
to texts generated by any common autoregressive experimentsareneededtoevaluatethegeneralization
language model. As a strategy more reliable than ofourfindingstootherarchitecturesandsetups.
watermarking, Krishna et al. (2023) suggest a
retrieval-based approach: By storing all model Acknowledgments
outputs in a database, LLM providers can verify
This research is supported in part by DARPA
whether a given text was previously generated by
SemaForProgramNo. HR00112020054.
theirlanguagemodel. Inpractice,thiswouldhowever
requirestorageoflargeamountsofdataandhighly
efficientretrievaltechniquesinordertoprovidefast References
responsesasthenumberofgeneratedtextsgrows.
Sahar Abdelnabi and Mario Fritz. 2021. Adversarial
watermarking transformer: Towards tracing text
provenancewithdatahiding. In42ndIEEESymposium
onSecurityandPrivacy.
Relationship to Membership Inference Attacks
(MIA) Prior work (Mattern et al., 2023) demon- Mikhail J. Atallah, Victor Raskin, Michael Crogan,
Christian Hempelmann, Florian Kerschbaum, Dina
strated that the same optimality test can be used
Mohamed,andSanketNaik.2001. Naturallanguage
to distinguish between training set members and watermarking:Design,analysis,andaproof-of-concept
non-trainingmembers,i.e. asamembershipinference implementation. InInformationHiding,pages185–200,
attack. As our experiments showed, when models Berlin,Heidelberg.SpringerBerlinHeidelberg.
size up the detection power (i.e. distinguishablity Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng,
betweenmachine-generatedandhuman-writtentext) Marc’AurelioRanzato,andArthurSzlam.2019. Real
decreases.ForMIA,however,priorworkdemonstrate orfake?learningtodiscriminatemachinefromhuman
generatedtext.
inverse scaling, as in larger models demonstrate
higher distinguishing power (Mireshghallah et al., Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
2022;Matternetal.,2023). Weattributethistothe Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-
hammadAflahKhan,ShivanshuPurohit,USVSNSai
highermemorizationcapablitiesofthesemodels,as
Prashanth,EdwardRaff,etal.2023. Pythia: Asuite
shownby(Tirumalaetal.,2022),makingiteasierfor
foranalyzinglargelanguagemodelsacrosstrainingand
themtorecognizetheirtrainingdata. scaling. arXivpreprintarXiv:2304.01373.
Monica Chew and Henry S. Baird. 2003. Baffletext: a JustusMattern, FatemehsadatMireshghallah, ZhijingJi,
human interactive proof. In IS&T/SPIE Electronic Bernhard Scholkop, Mrinmaya Sachan, and Taylor
Imaging. Berg-Kirkpatrick.2023. Membershipinferenceattacks
againstlanguagemodelsvianeighbourhoodcomparison.
LiamDugan,DaphneIppolito,ArunKirubarajan,Sherry InProceedingsofthe61stAnnualMeetingoftheAsso-
Shi, and Chris Callison-Burch. 2022. Real or fake ciationforComputationalLinguistics(ACLFindings).
text?:Investigatinghumanabilitytodetectboundaries
betweenhuman-writtenandmachine-generatedtext. FatemehsadatMireshghallah,KartikGoyal,ArchitUniyal,
Taylor Berg-Kirkpatrick, and Reza Shokri. 2022.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Quantifyingprivacyrisksofmaskedlanguagemodels
Hierarchical neural story generation. arXiv preprint using membership inference attacks. arXiv preprint
arXiv:1805.04833. arXiv:2203.03929.
Tina Fang, Martin Jaggi, and Katerina Argyraki. 2017. EricMitchell,YoonhoLee,AlexanderKhazatsky,Christo-
Generating steganographic text with LSTMs. In pherDManning,andChelseaFinn.2023. Detectgpt:
ProceedingsofACL2017,StudentResearchWorkshop, Zero-shot machine-generated text detection using
pages 100–106, Vancouver, Canada. Association for probabilitycurvature. arXivpreprintarXiv:2301.11305.
ComputationalLinguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
LeoGao,StellaBiderman,SidBlack,LaurenceGolding, Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
TravisHoppe,CharlesFoster,JasonPhang,HoraceHe, Wei Li, and Peter J Liu. 2020. Exploring the limits
AnishThite,NoaNabeshima,etal.2020. Thepile:An of transfer learning with a unified text-to-text trans-
800gb dataset of diverse text for language modeling. former. TheJournalofMachineLearningResearch,
arXivpreprintarXiv:2101.00027. 21(1):5485–5551.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
Sebastian Gehrmann, Hendrik Strobelt, and Alexander
and Percy Liang. 2016. Squad: 100,000+ questions
Rush.2019. GLTR:Statisticaldetectionandvisualiza-
tionofgeneratedtext. InProceedingsofthe57thAnnual for machine comprehension of text. arXiv preprint
MeetingoftheAssociationforComputationalLinguis-
arXiv:1606.05250.
tics:SystemDemonstrations,pages111–116,Florence,
Irene Solaiman, Miles Brundage, Jack Clark, Amanda
Italy.AssociationforComputationalLinguistics.
Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford,
Gretchen Krueger, Jong Wook Kim, Sarah Kreps,
XinleiHe,XinyueShen,ZeyuanChen,MichaelBackes,
MilesMcCain,AlexNewhouse,JasonBlazakis,Kris
and Yang Zhang. 2023. Mgtbench: Benchmarking
McGuffie,andJasmineWang.2019. Releasestrategies
machine-generated text detection. arXiv preprint
andthesocialimpactsoflanguagemodels.
arXiv:2303.14822.
Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer,
DaphneIppolito,DanielDuckworth,ChrisCallison-Burch,
andArmenAghajanyan.2022. Memorizationwithout
and Douglas Eck. 2020. Automatic detection of
overfitting: Analyzingthetrainingdynamicsoflarge
generatedtextiseasiestwhenhumansarefooled. In
language models. Advances in Neural Information
Proceedingsofthe58thAnnualMeetingoftheAssoci-
ProcessingSystems,35:38274–38290.
ationforComputationalLinguistics,pages1808–1822,
Online.AssociationforComputationalLinguistics. Adaku Uchendu, Thai Le, Kai Shu, and Dongwon
Lee. 2020. Authorship attribution for neural text
Ganesh Jawahar, Muhammad Abdul-Mageed, and
generation. InProceedingsofthe2020Conferenceon
Laks Lakshmanan, V.S. 2020. Automatic detection
Empirical Methods in Natural Language Processing
of machine generated text: A critical survey. In
(EMNLP),pages8384–8395,Online.Associationfor
Proceedings of the 28th International Conference
ComputationalLinguistics.
on Computational Linguistics, pages 2296–2309,
Barcelona,Spain(Online).InternationalCommitteeon Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, and
ComputationalLinguistics. DongwonLee.2021. TURINGBENCH:Abenchmark
environment for Turing test in the age of neural text
JohnKirchenbauer,JonasGeiping,YuxinWen,Jonathan generation. InFindingsoftheAssociationforCompu-
Katz, Ian Miers, and Tom Goldstein. 2023. A tationalLinguistics:EMNLP2021,pages2001–2016,
watermarkforlargelanguagemodels. Punta Cana, Dominican Republic. Association for
ComputationalLinguistics.
KalpeshKrishna,YixiaoSong,MarzenaKarpinska,John
Wieting,andMohitIyyer.2023. Paraphrasingevades HongWang,XuanLuo,WeizhiWang,andXifengYan.
detectorsofai-generatedtext,butretrievalisaneffective 2023. Botorhuman?detectingchatgptimposterswith
defense. arXivpreprintarXiv:2303.13408. asinglequestion.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris ThomasWolf,LysandreDebut,VictorSanh,JulienChau-
Tsipras,DilaraSoylu,MichihiroYasunaga,YianZhang, mond,ClementDelangue,AnthonyMoi,PierricCistac,
DeepakNarayanan,YuhuaiWu,AnanyaKumar,etal. TimRault,RémiLouf,MorganFuntowicz,etal.2019.
2022. Holisticevaluationoflanguagemodels. arXiv Huggingface’s transformers: State-of-the-art natural
preprintarXiv:2211.09110. languageprocessing. arXivpreprintarXiv:1910.03771.
AnnYuan,AndyCoenen,EmilyReif,andDaphneIppolito.
2022. Wordcraft: story writing with large language
models. In27thInternationalConferenceonIntelligent
UserInterfaces,pages841–852.
Zachary Ziegler, Yuntian Deng, and Alexander Rush.
2019. Neurallinguisticsteganography. InProceedings
of the 2019 Conference on Empirical Methods in
NaturalLanguageProcessingandthe9thInternational
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 1210–1215, Hong Kong,
China.AssociationforComputationalLinguistics.
A AblatingMaskFillingModels B.2 Dataset
Evaluation dataset. We follow Mitchell et al.
Figure 11 shows the curvature numbers for each
(2023)’s methodology for pre-processing and feed-
model trying to detect its own generations, so for
ing the data. We use a subsample of the SQuAD
each model the generator is also the detector. We
dataset (Rajpurkar et al., 2016), where the original
experimentwiththreeperturbationgeneratingmodels,
datasetsequencesareusedasthehuman-writtentext
with three different sizes: (1) T5-small (60 million
inthetargetsequencepool.Wethenusethefirst20to-
parameters) (2) T5-Large (770 million parameters)
kensofeachhuman-writtensequenceasaprompt,and
(3)T5-3B(3billionparameter). Theintuitionbehind
feedthistothetargetmodel,andhaveitgeneratecom-
usingthreemodelsizesistoseetheeffectofhavinga
pletionsforit.Wethenusethismixofgenerationsand
betterreplacementmodelonthemeasuredcurvatures
human-writtentexttocreatethetargetpoolforwhich
andthedetectionpowerofthedetectormodels.
wedothedetection.Inallcases,followingthemethod-
Wecanseethatasthemaskingmodelsizesdown
ologyfromMitchelletal.(2023),ourpoolconsistsof
(goingfromtoptothebottomsubfigures),theoverall
300human-writtentargetsamples,and300machine-
curvaturevaluesforbothhuman-writtenandmachine-
generatedsamples,sotheoverallpoolsizeis600.
generatedtextincreases(goingfrom0.2maximum
in Figure 11a to 0.6 maximum in Figure 11c), and Pre-training datasets for the generative models.
thetwosetsoftextsbecomelessdistinguishable. T5- The ElutherAI and Facebook models (GPTJ, GPT-
Smallproduceslow-quality(low-fluency)neighbors Neo,PythiaandOPTfamilies)arealltrainedonthe
that are assigned lower likelihoods by the detector Pile dataset (Gao et al., 2020), a curated collection
model,resultinginhighcurvaturenumbersforboth of 22 English language datasets (consisting of
human and machine generated text, making them web-crawleddata,academicarticles,dialogues,etc.).
indistinguishable. As we improve the mask filling Asmentionedabovetherearetwoversionsofeach
model,however,thegeneratedneighborsbecomeof Pythiamodel(Bidermanetal.,2023),oneversionis
higherquality(andsemanticallyclosertothetarget trainedonPile,theotheristrainedonde-duplicated
point),therebycreatingamoreaccurateestimateof Pile. Thede-duplicatedPileisapproximately207B
the curvature and providing better distinguishablity, tokens in size, compared to the original Pile which
asshownbytheAUCnumbersinFigure11d. contains300Btokens. Thereislimitedinformation
andaccesstothetrainingdataoftheOpenAImodels.
B ExperimentalSetup The GPT-2 family is reportedly trained on the
WebTextdataset,GPT-3istrainedonacombination
B.1 Models
of the Common Crawl 2, WebText2, books and
We want to experiment with a wide range of mod- Wikipedia,andthereisnotanyinformationreleased
els, with different architectures, parameter counts aboutthetrainingdataofChatGPT.
andtrainingdatasets,thereforeweusethefollowing
C AdditionalPlots
modelfamiliesinourexperiments: Facebook’sOPT
(we use the 125M, 350M, 1.3B, and 6.7B models),
C.1 ExtensiveHeatmaps
EleutherAI’sGPT-J,GPTNeoandPythia(Biderman
We provide the full heatmaps from experiments of
etal.,2023)(weuseGPTNeo-125M,GPTNeo-1.3B,
Section 4 here, to provide a detailed breakdown.
GPTNeo-2.7B,GPTJ-6BandPythiamodelsranging
Figures2and3(fullheatmapisFig.8inAppendix)
from70Mto2.8Bparameters),andOpenAI’sGPT
showtheAUCofcross-detectionfordifferentmodels.
models (distilGPT, GPT2-Small, GPT2-Medium,
Figures10and9inAppendixshowhowcloseeach
GPT2-Large,GPT2-XL,GPT-3andChatGPT).
detectorcomes,intermsofAUC,toself-detection.
Wealsohaveexperimentswhereweusepartially
trainedmodelsasdetectors. Forthoseexperiments, C.2 SummaryofExperiments
we only use the Pythia models as they are the only
We provide a summary of Figure 2 in Figure 12,
ones with available, open-source partially trained
wherewehavepresentedthenumbersfromthebest
checkpoints. For each Pythia models, there is also
overalldetectorwithmeanAUCof0.92(OPT-125M)
a de-duplicated version available, where the model
andthebiggestmodelofthesamefamily,OPT-6.7B
istrainedonthede-duplicatedversionofthedata,as
withaverageAUCof0.46.
opposedtotheoriginaldataset.Allthemodelsweuse
areobtainedfromHuggingFace(Wolfetal.,2019). 2https://commoncrawl.org
AUC of Distinguishing Human Text from Generations
1.0
ChatGPT 0.750.750.700.600.540.520.750.780.750.680.640.640.750.790.780.760.730.730.720.770.760.750.740.730.750.780.780.790.760.75
GPT3 0.830.800.770.700.660.670.800.800.790.750.720.720.810.830.800.810.780.780.780.810.820.810.810.810.800.810.810.830.820.82
OPT-6.7B 0.830.840.810.770.670.670.830.850.830.810.780.770.840.850.850.840.840.820.830.850.850.840.840.840.820.840.840.840.840.83
0.8
GPTJ-6B 0.820.840.820.780.730.710.830.840.830.800.790.770.840.830.840.840.840.820.840.830.840.830.820.820.810.820.820.820.820.82
GPTNeo-2.7B 0.840.870.870.780.690.650.830.870.870.840.800.790.840.870.870.880.870.870.830.870.870.870.870.860.830.850.850.850.860.85
GPT2-XL 0.850.900.890.760.630.600.850.900.910.850.780.770.860.900.900.910.890.890.850.890.890.900.900.900.850.880.880.880.880.88
OPT-1.3B 0.870.890.860.690.560.530.860.900.890.800.720.710.850.900.900.890.860.870.870.900.900.900.900.890.860.880.880.890.880.89 0.6
GPTNeo-1.3B 0.900.930.910.740.610.560.900.930.930.870.780.800.900.920.920.930.930.910.900.910.920.930.920.920.890.910.910.910.910.91
GPT2-Large 0.900.930.900.730.590.550.890.930.940.860.770.770.900.930.940.920.920.910.890.920.930.930.930.930.890.910.910.920.920.92
GPT2-Medium 0.870.910.880.590.440.370.850.930.910.800.650.650.850.920.930.910.900.880.860.910.910.930.940.920.850.900.900.900.910.92 0.4
OPT-350M 0.960.950.880.580.410.360.940.970.950.790.620.640.930.980.970.940.900.900.940.970.970.970.960.950.950.970.970.970.970.97
OPT-125M 0.960.920.790.380.250.220.940.970.910.620.410.430.930.980.970.870.810.780.950.980.980.970.960.960.950.980.980.980.980.98
GPTNeo-125M 0.950.900.750.350.250.210.930.970.910.600.390.400.930.980.970.860.770.760.930.980.980.970.960.960.940.970.980.970.970.97
0.2
GPT2 0.950.920.780.380.240.210.920.970.920.640.420.440.920.980.960.890.800.810.920.980.970.970.960.950.930.970.970.980.970.97
DistilGPT2 0.970.760.540.230.150.130.920.910.740.400.240.260.910.960.900.680.550.550.920.970.960.900.890.860.950.980.970.970.970.97
Mean 0.880.870.810.600.490.460.870.900.870.740.630.640.870.910.900.860.830.820.870.900.900.900.890.890.870.900.900.900.900.89
0.0
Detector Model
Figure8:AUCheatmapforcross-detection,wheretherowsaregenerativemodelsandcolumnsarethesurrogatedetector
modelsfromthePythiafamily,atdifferenttrainingstepcheckpoints(1k,5k,10k,50k,100kand143k),bothsorted
bymodelsize.Wecanseethatpartiallytrainedmodelsarebetterdetectors.
AUC of Distinguishing Human Text from Generations
OPT-6.7B -0.08-0.08-0.10-0.15-0.25-0.24-0.09-0.07-0.08-0.11-0.13-0.15-0.07-0.07-0.06-0.07-0.08-0.09-0.08-0.07-0.07-0.08-0.08-0.07-0.09-0.08-0.08-0.07-0.07-0.08
GPTJ-6B -0.000.02-0.00-0.04-0.10-0.120.01 0.02 0.01-0.02-0.03-0.050.01 0.01 0.01 0.01 0.02 0.00 0.01 0.01 0.01 0.01-0.010.00-0.01-0.000.00-0.00-0.01-0.01
0.0
GPTNeo-2.7B -0.10-0.06-0.06-0.16-0.25-0.29-0.11-0.07-0.06-0.10-0.14-0.15-0.10-0.07-0.07-0.05-0.07-0.06-0.11-0.07-0.07-0.07-0.06-0.08-0.11-0.08-0.09-0.08-0.07-0.09
GPT2-XL -0.13-0.08-0.09-0.22-0.35-0.39-0.14-0.08-0.08-0.13-0.20-0.21-0.13-0.08-0.08-0.08-0.09-0.09-0.13-0.09-0.09-0.09-0.09-0.09-0.14-0.10-0.10-0.10-0.10-0.10
0.2
OPT-1.3B -0.10-0.09-0.12-0.29-0.42-0.45-0.12-0.08-0.09-0.18-0.26-0.27-0.13-0.07-0.08-0.09-0.12-0.11-0.10-0.08-0.08-0.08-0.08-0.09-0.12-0.10-0.10-0.09-0.10-0.09
GPTNeo-1.3B -0.08-0.06-0.08-0.25-0.38-0.42-0.08-0.05-0.05-0.11-0.20-0.18-0.08-0.06-0.06-0.05-0.06-0.07-0.08-0.07-0.06-0.06-0.06-0.06-0.10-0.07-0.08-0.07-0.08-0.08
GPT2-Large -0.09-0.06-0.09-0.26-0.40-0.44-0.10-0.06-0.05-0.13-0.22-0.22-0.09-0.06-0.05-0.06-0.07-0.08-0.10-0.07-0.06-0.06-0.06-0.06-0.09-0.07-0.08-0.07-0.07-0.07 0.4
GPT2-Medium -0.13-0.08-0.12-0.40-0.56-0.62-0.15-0.07-0.09-0.20-0.35-0.35-0.14-0.08-0.07-0.08-0.10-0.12-0.14-0.08-0.08-0.06-0.06-0.08-0.14-0.10-0.09-0.10-0.09-0.08
OPT-350M -0.04-0.05-0.12-0.42-0.59-0.64-0.06-0.03-0.05-0.21-0.38-0.36-0.07-0.02-0.03-0.06-0.10-0.10-0.06-0.03-0.03-0.03-0.04-0.05-0.05-0.03-0.03-0.03-0.03-0.03
0.6
OPT-125M -0.04-0.08-0.21-0.62-0.74-0.78-0.06-0.03-0.09-0.38-0.59-0.57-0.07-0.02-0.03-0.13-0.19-0.22-0.05-0.02-0.02-0.03-0.04-0.04-0.05-0.02-0.02-0.02-0.02-0.02
GPTNeo-125M -0.05-0.10-0.25-0.65-0.75-0.79-0.07-0.02-0.09-0.40-0.61-0.60-0.07-0.02-0.03-0.14-0.23-0.24-0.07-0.02-0.02-0.03-0.04-0.04-0.05-0.03-0.02-0.03-0.03-0.03
GPT2 -0.05-0.08-0.21-0.62-0.76-0.79-0.08-0.03-0.08-0.35-0.58-0.56-0.08-0.02-0.03-0.11-0.20-0.19-0.08-0.02-0.03-0.03-0.03-0.05-0.07-0.03-0.03-0.02-0.03-0.03 0.8
DistilGPT2 -0.03-0.24-0.46-0.77-0.85-0.87-0.08-0.08-0.25-0.60-0.75-0.73-0.09-0.03-0.10-0.32-0.44-0.45-0.07-0.02-0.04-0.10-0.11-0.14-0.05-0.02-0.02-0.02-0.02-0.03
Mean -0.07-0.08-0.15-0.37-0.49-0.53-0.09-0.05-0.08-0.22-0.34-0.34-0.08-0.05-0.05-0.09-0.13-0.14-0.08-0.05-0.05-0.05-0.06-0.06-0.08-0.06-0.06-0.05-0.06-0.06
1.0
Detector Model
Figure9:AUCdifferencebetweenself-detectionandcross-detectionheatmap(tobetterseehowclosecross-detection
comestoselfdetection),heretherowsaregenerativemodelsandcolumnsarethesurrogatedetectormodelsfromthe
Pythiafamily,atdifferenttrainingstepcheckpoints(1k,5k,10k,50k,100kand143k),bothsortedbymodelsize.This
plotisbasicallyFigure8,whereeachcellinarowissubtractedbytheself-detectionAUCforthatrow.
ledoM
evitareneG
ledoM
evitareneG
k1-dd-B8.2-aihtyP
k1-dd-B8.2-aihtyP
k5-dd-B8.2-aihtyP
k5-dd-B8.2-aihtyP
k01-dd-B8.2-aihtyP
k01-dd-B8.2-aihtyP
k05-dd-B8.2-aihtyP
k05-dd-B8.2-aihtyP
k001-dd-B8.2-aihtyP
k001-dd-B8.2-aihtyP
k341-dd-B8.2-aihtyP
k341-dd-B8.2-aihtyP k1-dd-B4.1-aihtyP
k1-dd-B4.1-aihtyP
k5-dd-B4.1-aihtyP
k5-dd-B4.1-aihtyP
k01-dd-B4.1-aihtyP
k01-dd-B4.1-aihtyP
k05-dd-B4.1-aihtyP
k05-dd-B4.1-aihtyP
k001-dd-B4.1-aihtyP
k001-dd-B4.1-aihtyP
k341-dd-B4.1-aihtyP
k341-dd-B4.1-aihtyP
k1-dd-M014-aihtyP
k1-dd-M014-aihtyP
k5-dd-M014-aihtyP
k5-dd-M014-aihtyP
k01-dd-M014-aihtyP
k01-dd-M014-aihtyP
k05-dd-M014-aihtyP
k05-dd-M014-aihtyP
k001-dd-M014-aihtyP
k001-dd-M014-aihtyP
k341-dd-M014-aihtyP
k341-dd-M014-aihtyP
k1-dd-M061-aihtyP
k1-dd-M061-aihtyP
k5-dd-M061-aihtyP
k5-dd-M061-aihtyP
k01-dd-M061-aihtyP
k01-dd-M061-aihtyP
k05-dd-M061-aihtyP
k05-dd-M061-aihtyP
k001-dd-M061-aihtyP
k001-dd-M061-aihtyP
k341-dd-M061-aihtyP
k341-dd-M061-aihtyP
k1-dd-M07-aihtyP
k1-dd-M07-aihtyP
k5-dd-M07-aihtyP
k5-dd-M07-aihtyP
k01-dd-M07-aihtyP
k01-dd-M07-aihtyP
k05-dd-M07-aihtyP
k05-dd-M07-aihtyP
k001-dd-M07-aihtyP
k001-dd-M07-aihtyP
k341-dd-M07-aihtyP
k341-dd-M07-aihtyP
AUC of Distinguishing Human Text from Generations
OPT-6.7B 0.00 -0.24 -0.21 -0.20 -0.17 -0.17 -0.18 -0.17 -0.07 -0.15 -0.15 -0.16 -0.15 -0.16 -0.09 -0.14 -0.14 -0.10 -0.13 -0.15 -0.17 -0.15 -0.15
0.0
GPTJ-6B -0.22 0.00 -0.17 -0.17 -0.14 -0.21 -0.15 -0.16 -0.14 -0.14 -0.18 -0.15 -0.14 -0.18 -0.14 -0.14 -0.14 -0.13 -0.13 -0.17 -0.17 -0.13 -0.14
GPTNeo-2.7B -0.32 -0.31 -0.22 -0.22 0.00 -0.17 -0.15 -0.15 -0.15 -0.12 -0.13 -0.12 -0.10 -0.13 -0.12 -0.11 -0.12 -0.11 -0.10 -0.12 -0.16 -0.13 -0.13
GPT2-XL -0.32 -0.36 -0.26 -0.25 -0.18 0.00 -0.16 -0.15 -0.12 -0.12 -0.05 -0.10 -0.09 -0.05 -0.07 -0.09 -0.09 -0.07 -0.08 -0.06 -0.10 -0.11 -0.10
0.2
OPT-1.3B -0.25 -0.35 -0.28 -0.28 -0.21 -0.17 -0.19 -0.19 0.00 -0.15 -0.11 -0.13 -0.11 -0.11 -0.05 -0.09 -0.09 -0.05 -0.08 -0.09 -0.11 -0.11 -0.10
GPTNeo-1.3B -0.38 -0.39 -0.27 -0.28 -0.16 -0.18 -0.15 -0.13 -0.15 0.00 -0.11 -0.07 -0.07 -0.09 -0.06 -0.06 -0.07 -0.06 -0.05 -0.08 -0.09 -0.08 -0.07
GPT2-Large -0.38 -0.42 -0.30 -0.30 -0.21 -0.09 -0.17 -0.16 -0.13 -0.13 0.00 -0.08 -0.07 -0.03 -0.05 -0.07 -0.07 -0.05 -0.06 -0.03 -0.07 -0.08 -0.08 0.4
GPT2-Medium -0.54 -0.54 -0.42 -0.44 -0.31 -0.17 -0.25 -0.25 -0.21 -0.18 -0.07 -0.10 -0.09 0.00 -0.05 -0.06 -0.06 -0.04 -0.05 -0.03 -0.06 -0.08 -0.07
OPT-350M -0.48 -0.52 -0.43 -0.43 -0.32 -0.25 -0.25 -0.25 -0.16 -0.19 -0.14 -0.08 -0.08 -0.08 0.00 -0.04 -0.04 -0.01 -0.03 -0.03 -0.03 -0.03 -0.04
0.6
OPT-125M -0.60 -0.62 -0.57 -0.57 -0.48 -0.41 -0.40 -0.41 -0.31 -0.32 -0.26 -0.15 -0.16 -0.16 -0.04 -0.04 -0.04 0.00 -0.03 -0.03 -0.02 -0.03 -0.02
GPTNeo-125M -0.69 -0.67 -0.63 -0.64 -0.54 -0.50 -0.46 -0.48 -0.46 -0.38 -0.34 -0.18 -0.19 -0.23 -0.09 -0.04 -0.04 -0.04 0.00 -0.04 -0.02 -0.02 -0.03
GPT2 -0.69 -0.67 -0.60 -0.61 -0.52 -0.40 -0.43 -0.42 -0.40 -0.35 -0.23 -0.16 -0.15 -0.11 -0.07 -0.04 -0.04 -0.02 -0.03 0.00 -0.00 -0.03 -0.03 0.8
DistilGPT2 -0.78 -0.74 -0.75 -0.75 -0.68 -0.74 -0.65 -0.64 -0.65 -0.57 -0.60 -0.38 -0.39 -0.46 -0.24 -0.11 -0.12 -0.09 -0.07 -0.11 0.00 -0.03 -0.03
Mean -0.43 -0.45 -0.39 -0.40 -0.30 -0.27 -0.28 -0.27 -0.23 -0.22 -0.18 -0.14 -0.14 -0.14 -0.08 -0.08 -0.08 -0.06 -0.07 -0.07 -0.08 -0.08 -0.08
1.0
Detector Model
Figure10:AUCdifferencebetweenself-detectionandcross-detectionheatmap(tobetterseehowclosecross-detection
comestoselfdetection),wheretherowsaregenerativemodelsandcolumnsarethesurrogatedetectormodels,bothsorted
bymodelsize.ThisplotisbasicallyFigure2,whereeachcellinarowissubtractedbytheself-detectionAUCforthatrow.
ledoM
evitareneG
B7.6-TPO B6-JTPG B8.2-aihtyP dd-B8.2-aihtyP B7.2-oeNTPG LX-2TPG B4.1-aihtyP dd-B4.1-aihtyP B3.1-TPO B3.1-oeNTPG egraL-2TPG M014-aihtyP dd-M014-aihtyP muideM-2TPG M053-TPO M061-aihtyP dd-M061-aihtyP M521-TPO M521-oeNTPG 2TPG 2TPGlitsiD M07-aihtyP dd-M07-aihtyP
Machine-generated
0.20 Human-written
0.15
0.10
0.05
0.00
0.05
108 109
#Params
(a)T5-3B
0.30 Machine-generated
Human-written
0.25
0.20
0.15
0.10
1.0 Detector Model
0.05 OPT-125M
0.9 OPT-6.7B
0.00
0.8
0.05
0.7
108 109
#Params
0.6
(b)T5-Large
0.5
Machine-generated 0.4
0.6 Human-written
0.3
0.5 0.2
0.4 ChatGPT GPT3 OPT-6.7B OPT-125M Mean
Generative Model
0.3
Figure 12: Summary of the cross-detection area under
0.2
theROCcurve(AUC)resultsforaselectionofgenerative
0.1 (the4modelsovertheXaxis)anddetector(OPT-125M
108 109 andOPT-6.7B)models.WecanseethatthesmallerOPT
#Params
modelisabetteruniversalcross-detector.Fullresultsare
(c)T5-Small
showninFigure2.
1.0
0.9
0.8
0.7
Mask Model
T5-3b
T5-Large
0.6 T5-Small
108 109
#Params
(d)AUCsfordifferentperturbation(masking)models
Figure 11: The effect of changing the perturbation
(masking) model on curvature values and self-detection
powerofdifferentmodelswithdifferentsizes(AUC).
erutavruC
erutavruC
erutavruC
CUA
CUA

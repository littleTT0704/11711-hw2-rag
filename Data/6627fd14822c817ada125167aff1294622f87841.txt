Unsupervised Code-Switching for
Multilingual Historical Document Transcription
DanGarrette∗ HannahAlpert-Abrams† TaylorBerg-Kirkpatrick‡ DanKlein‡
∗DepartmentofComputerScience,UniversityofTexasatAustin,dhg@cs.utexas.edu
†ComparativeLiteratureProgram,UniversityofTexasatAustin,halperta@gmail.com
‡ComputerScienceDivision,UniversityofCaliforniaatBerkeley,{tberg,klein}@cs.berkeley.edu
Abstract allows joint transcription and word-level language
identification. Second, to handle orthographic vari-
Transcribing documents from the printing ation, we provide an interface that allows individ-
press era, a challenge in its own right, is
uals familiar with relevant languages to guide the
morecomplicatedwhendocumentsinterleave
languagemodelwithtargetedorthographicinforma-
multiple languages—a common feature of
tion. As a result, our system handles inconsistent
16th century texts. Additionally, many of
spelling,punctuation,anddiacriticusage,aswellas
these documents precede consistent ortho-
graphic conventions, making the task even now-obsolete shorthand conventions used by print-
harder. We extend the state-of-the-art his- ers.
torical OCR model of Berg-Kirkpatrick et al. Weevaluateourmodelusingdocumentsfromthe
(2013) to handle word-level code-switching
Primeros Libros project, a digital archive of books
between multiple languages. Further, we en-
printedintheAmericaspriorto1601(Dolan,2012).
able our system to handle spelling variabil-
Thesetexts,writteninEuropeanandindigenouslan-
ity,includingnow-obsoleteshorthandsystems
guages, often feature as many as three languages
usedbyprinters.Ourresultsshowaveragerel-
ativecharactererrorreductionsof14%across on a single page, with code-switching occurring on
avarietyofhistoricaltexts. thechapter, sentence, andwordlevel. Orthographic
variationsarepervasivethroughout,andareparticu-
larly difficult with indigenous languages, for which
1 Introduction
writingsystemswerestillbeingdeveloped.
Transcribing documents printed on historical print- Ourresultsshowimprovementsacrossarangeof
ing presses poses a number of challenges for OCR documents,yieldinganaverage14%relativecharac-
technology. Berg-Kirkpatricketal.(2013)presented tererrorreductionoverthepreviousstate-of-the-art,
an unsupervised system, called Ocular, that han- withreductionsashighas27%onparticulartexts.
dlesthetypesofnoisethatarecharacteristicofpre-
20th century documents and uses a fixed monolin- 2 Data
gual language model to guide learning. While this
approach is highly effective on English documents Writing during the early modern period in Europe
from the 18th and 19th centuries, problems arise was characterized by increasing use of vernacu-
when it is applied to older documents that feature lar languages alongside Latin, Greek, and Hebrew.
code-switchingbetweenmultiplelanguagesandob- In the colonies, this was matched by the develop-
soleteorthographiccharacteristics. ment of grammars and alphabetic writing systems
In this work, we address these issues by devel- forindigenouslanguages(seeEisenstein(1979)and
oping a new language model for Ocular. First, to Mignolo (1995)). In all cases, orthographies were
handlemultilingualdocuments,wereplaceOcular’s regionally variable and subject to the limited re-
simple n-gram language model with an unsuper- sources of the printing houses; this is particularly
vised model of intrasentential code-switching that true in the Americas, where resources were scarce,
1036
HumanLanguageTechnologies:The2015AnnualConferenceoftheNorthAmericanChapteroftheACL,pages1036–1041,
Denver,Colorado,May31–June5,2015.(cid:13)c2015AssociationforComputationalLinguistics
ligature diacritic
cal offset of each character from a common base-
line. Additionally,sincedocumentsexhibitvariable
inking levels (where individual characters are often
fadedorsmearedwithblotchedink)thesystemalso
non-standard character obsolete
spelling elision character modelstheamountofinkappliedtoeachtypepiece.
Thegenerativeprocessoperatesasfollows. First,
Input: a sequence of character tokens is generated by a
Languagemodel: praesertimurgentecausa character n-gram language model. Then, bounding
boxes for each character token are generated, con-
Figure1: AnexampleOCRinputshowingtheorigi-
ditioned on the character type, followed by verti-
nalimageandanexampleofanequivalentmodern-
cal offsets and inking levels. Finally, the pixels in
izedtextsimilartodatausedtotraintheLM.
eachboundingboxaregenerated,conditionedonthe
character types, vertical offsets, and inking levels.
and where indigenous-language orthographies were In this work, we focus on improving the language
firstbeingdeveloped(BaddeleyandVoeste,2013). model, and leave the rest of the generative process
The 349 digital facsimiles in the Primeros Li- untouched.
bros collection are characteristic of this trend. Pro-
4 LanguageModel
duced during the first century of Spanish coloniza-
tion,theyrepresenttheintroductionofprintingtech-
WepresentanewlanguagemodelforOcularthatis
nologyintotheAmericas,andreflectthe(sometimes
designed to handle issues that are characteristic of
conflicted)prioritiesofthenascentcolony, fromre-
older historical documents: code-switching and or-
ligiousorthodoxytoconversionandeducation.
thographic variability. We extend the conventional
For our experiments, we focus on multilingual
character n-gram language model and its training
documents in three languages: Spanish, Latin, and
procedure to deal with each of these problems in
Nahuatl. As Berg-Kirkpatrick et al. (2013) show, a
turn.
languagemodelbuiltoncontemporaneousdatawill
performbetterthanmoderndata. Forthisreason,we
4.1 Code-Switching
collected15–17thcenturytextsfromProjectGuten-
berg,1 producingSpanishandLatincorporaofmore Because Ocular’s character n-gram language
model(LM)isfixedandmonolithic,evenwhenitis
thanonemillioncharacterseach. Duetoitsrelative
trainedoncorporafrommultiplelanguages,ittreats
scarcity, we augmented the Nahuatl corpus with a
all text as a single “language”—a multilingual blur
privatecollectionoftranscribedcolonialdocuments.
atbest. Asaresult,thesystemcannotmodelthefact
3 BaselineSystem that different contiguous blocks of text correspond
tospecificlanguagesandthusfollowspecificstatis-
ThestartingpointforourworkistheOcularsystem
ticalpatterns. Inordertotranscribedocumentsthat
described by Berg-Kirkpatrick et al. (2013). The
feature intrasentential code-switching, we replace
fonts used in historical documents are usually un-
Ocular’s simple n-gram LM with one that directly
known and can vary drastically from document to
models code-switching by representing language
document. Oculardealswiththisproblembylearn-
segmentationasalatentvariable.
ing the font in an unsupervised fashion – directly
Our code-switching LM generates a sequence of
from the input historical document. In order to ac-
pairs(e ,‘ )wheree isthecurrentcharacterand‘ is
complish this, the system uses a specialized gener- i i i i
the current language. The sequence of languages ‘
ative model that reasons about the main sources of i
specifiesthesegmentationofgeneratedtextintolan-
variation and noise in historical printing. These in-
guageregions. OurLMisbuiltfromseveralcompo-
clude the shapes of the character glyphs, the hor-
nentmodels: First, foreachlanguagetype‘, wein-
izontal spacing between characters, and the verti-
corporate a standard monolingual character n-gram
1http://www.gutenberg.org/ model trained on data from language ‘. The com-
1037
ponent model corresponding to language ‘ is called original → replacement
PCHAR. Second, our LM also incorporates a model a` a
‘
thatgovernscode-switchingbetweenlanguages. We a´ a
call this model PLANG. The generative process for que q˜
ourLMworksasfollows. Fortheithcharacterposi- per p˜
tion,wefirstgeneratethecurrentlanguage‘ condi- ce ze
i
tionedonthepreviouscharactere andtheprevi- x j
i−1
ous language ‘ using PLANG. Then, conditioned j x
i−1
on the current language l and the previous n − 1 an a˜
i
characters, we generate the current character e us- hspaceih hspacei
i
ing PCHAR. This means that the probability of pair be ve
‘
i
(e ,‘ )givenitscontextiscomputedas: u v
i i
v u
PLANG(‘ | e ,‘ )·PCHAR(e | e ...e ) oracion o˜ron
i i−1 i−1 ‘ i i−1 i−n+1
i
Table 1: An example subset of the orthographic re-
We parameterize PLANG in a way that enforces
placementrulesforSpanish.
two constraints. First, to ensure that each word is
assignedasinglelanguage, weonlyallowlanguage
transitions for characters directly following whites-
Finally, because our code-switching LM uses
pace(aspacecharacterorapagemargin,unlessthe
multiple separate language-specific n-gram models
character follows a line-end hyphen). Second, to
PCHAR,weareabletomaintainadistinctsetofvalid
resist overly frequent code-switching (and encour- ‘
characters for each language. By restricting each
age longer spans), we let a Bernoulli parameter κ
language’smodeltothesetofcharactersinthecor-
specify the probability of choosing to draw a new
pus for that language, we can push the model away
language at a word boundary (instead of determin-
fromincompatiblelanguagesduringtranscriptionif
istically staying in the same language). By setting
itisconfidentaboutcertainrarecharacters,andlimit
κ low, we indicate a strong belief that language
the search space by reducing the number of charac-
switchesshouldbeinfrequent,whilestillallowinga
ter combinations considered for any given position.
switch if sufficient evidence is found in the image.2
Wealsoinclude,foralllanguages,asetofpunctua-
Finally, we parameterize the frequency of each lan-
tion symbols such as ¶ and § that appear in printed
guage in the text. Specifically, for each language ‘,
booksbutnotintheLMtrainingdata.
amultinomialparameterθ specifiestheprobability
‘
oftransitioningto‘whenyoudrawanewlanguage.
4.2 OrthographicVariability
We learn this group of multinomial parameters, θ
‘
for each language, in an unsupervised fashion, and The component monolingual n-gram LMs must be
in doing so, adapt to the proportions of languages trained on monolingual corpora in their respective
foundintheparticularinputdocument. Thus,using languages. However, due to the lack of codified
ourparameterization,theprobabilityoftransitioning orthographic conventions concerning spelling, dia-
fromlanguage‘to‘0,givenpreviouscharactere,is: criticusage,andspacing,compoundedbytheliberal
use of now-obsolete shorthand notations by print-
PLANG(‘0 | e,‘) = ers,statisticsgleanedfromavailablemoderncorpora

provideapoorrepresentationofthelanguageusedin
   (1−κ)+κ·θ ‘0 ife=spaceand‘ = ‘0 the printed documents. Even 16th century texts on

κ·θ ife=spaceand‘ 6= ‘0
‘0 ProjectGutenbergtendtobewritten,forthebenefit
 1 ife 6=spaceand‘ = ‘0 of the reader, using modern spellings. The discon-



0 ife 6=spaceand‘ 6= ‘0 nect between the orthography of the original docu-
mentsandmoderntextscanbeseeninFigure1. To
2Weuseκ=10−6acrossallexperiments. address these issues, we introduced an interface for
1038
author Gante Anunciacio´n Sahagu´n Rinco´n Bautista MacroAverage
pub. year 1553 1565 1583 1595 1600 WER
CER WER CER WER CER WER CER WER CER WER CER WER w.p.
Ocular 13.7 55.9 15.7 53.6 10.8 44.3 11.6 38.4 9.7 25.7 12.3 43.6 56.6
+code-switch 12.8 55.0 14.6 53.8 9.6 38.7 10.7 35.4 8.8 24.5 11.3 41.5 53.5
+orth. var. 13.5 55.3 14.1 51.6 8.4 34.9 9.5 31.0 7.1 18.2 10.5 38.2 51.0
Table 2: Experimental results for each book, and average across all books. Columns show Character Error
Rate (CER) or Word Error Rate (WER; excluding punctuation). The final column gives the average WER
includingpunctuation(w.p.). TheOcularrowisthepreviousstate-of-the-art: Berg-Kirkpatricketal.(2013).
Thesecondrowusesourcode-switchingmodel,andthethirdadditionallyhandlesorthographicvariability.
Gan. (1553) tweenSpanish,Latin,andNahuatl. Foreachbook,a
Anu. (1565) font was trained on ten (untranscribed) pages using
Sah. (1583) unsupervisedlearningproceduredescribedbyBerg-
Rin. (1595) Kirkpatricketal.(2013). Thefontwasevaluatedon
Bau. (1600) aseparatesetoftenpages,manuallytranscribed.4
Table3: Anexamplelinefromeachtestbook. 6 ResultsandAnalysis
Ouroverallresults(Table2)showimprovementson
incorporatingorthographicvariabilityintothetrain- every book in our evaluation, achieving as high as
ingprocedureforthecomponentLMs. 29%relativeword-error(WER)reduction.
Forourexperiments,webuiltLatin,Nahuatl,and Replacing Ocular’s single mixed-language LM
Spanishvariabilityrulebanksbyaskinglanguageex- with our unsupervised code-switch model results in
pertstoidentifyspellinganomaliesfromamongsev- immediateimprovements. Anexampleoftranscrip-
eralsamplepagesfromPrimerosLibrosdocuments, tion output, including the language-assignments
andspecifyrewriterulesthatmapmodernspellings madebythemodel,canbeseeninFigure2.
backtovariantspellings;wealsodrewondatafrom Furtherimprovementsareseenbyhandlingortho-
paleographic textbooks. Example rules can be seen graphicvariation. Figure3givesanexampleofhow
in Table 1. These rules are used to rewrite corpus a single spelling variation can lead to a cascade of
text before the LMs are trained; for instance, every transcriptionerrors. Here,thebaselinesystem,con-
nthoccurrenceofenintheSpanishcorpusmightbe fusedbytheelisionoftheletterninthewordme˜tira
rewritten as e˜. This approach reintroduces histori- (frommentira, “lie”), transcribeditwithanentirely
callyaccurateorthographicvariabilityintotheLM. differentword(merita,“merit”). Whenourhandling
ofalternatespellingsisemployed,theLMhasgood
5 Experiments statisticsforcharactersequencesincludingthechar-
actere˜,andisabletodecodethewordcorrectly.
We compare to Ocular, the state of the art for his-
Thereareseveralexplanationsforthedifferences
toricalOCR.3 SinceOcular onlysupportsmonolin-
inresultsamongthefiveevaluationbooks. First,the
gual English OCR, we added support for alterna-
twooldesttexts,GanteandAnunciacio´n,useGothic
tivealphabets,includingdiacriticsandligatures,and
fontsthataremoredifficulttoreadandfeaturecapi-
trained a single mixed-language model on a com-
tallettersthatarenearlyimpossibleforthemodelto
binedSpanish/Latin/Nahuatlcorpus.
recognize(seeTable3). Thiscontributestothehigh
We evaluate our model on five different books
charactererrorratesforthosebooks.
from the Primeros Libros collection, representing a
Second,theworderrorratemetriciscomplicated
variety of printers, presses, typefaces, and authors
by the inconsistent use of spaces in Nahuatl writ-
(Table 3). Each book features code-switching be-
4Hyperparameters were set to be consistent with Berg-
3http://nlp.cs.berkeley.edu/projects/ocular.shtml Kirkpatricketal.(2013).
1039
transcribing documents, our system also implicitly
assigns language labels to words, allowing their us-
age in downstream tasks. We have also presented a
Aypropriovocablodelogro´,queestetech- new corpus, with transcriptions, for the evaluation
tlaixtlapanaliztli,tetechtlamieccaquixtiliztli, ofmultilingualhistoricalOCRsystems.
yparadezirditealogro? Cuixtetechotitlaix- Our system, as currently designed, attempts to
faithfullytranscribetext. However,forthepurposes
Figure 2: A passage with Spanish/Nahuatl code- ofindexabilityandsearchabilityofthesedocuments,
switching, and our model’s language-coded output. it may be desirable to also produce canonicalized
(Spanishinblue;Nahuatlinred/italics.) transcriptions, for example collapsing spelling vari-
ants to their modern forms. Fortunately, this can
be done in our approach by running the variability
rewriterules“backward”asapost-processingstep.
novariationhandling mentira merita
Further technical improvements may be made by
handlingvariation mentira m˜etira
having the system automatically attempt to boot-
strap the identification of spelling variants, a pro-
Figure 3: Two variants of the same word (mentira),
cess that could complement our approach through
pulled from the same page of text. The form men-
an active learning setup. Additionally, since even
tiraappearsintheLMtrainingcorpus,buttheshort-
our relatively simple unsupervised code-switch lan-
handme˜tiradoesnot. Withoutspecialhandling,the
guage modeling approach yielded improvements to
modeldoesnotknowthatme˜tiraisvalid.
OCRperformance,itmaybejustifiedtoattemptthe
adaptation of more complex code-switch recogni-
tiontechniques(Solorioetal.,2014).
ing,falselyclaiming“word”errorswhenallcharac-
The automatic transcription of the Primeros Li-
ters are correct. Use of spaces is not standardized
bros collection has significant implications for
acrosstheprintedbooks,oracrossthedigitizedLM
scholarsofthehumanitiesinterestedintherolethat
training corpora, and is still in fact a contested is-
inscriptionandtransmissionplayincolonialhistory.
sueamongmodernNahuatlscholars. Whileitisim-
For example, there are parallels between the way
portantforthetranscriptionprocesstoinsertspaces
that the Spanish transformed indigenous languages
appropriately into the Spanish and Latin text (even
into Latin-like writing systems (removing “noise”
whentheprinterleftlittle,aswithyparainFigure2),
like phonemes that do not exist in Latin), and the
it is difficult to assess what it means for a space to
way that the OCR tool transforms historical printed
be“correctly”insertedintoNahuatltext. Rinco´nand
documentsintounicode(removing“noise”likearti-
Bautista contain relatively less Nahuatl text and are
facts of the printing process and physical changes
affectedlessbythisproblem.
to the pages); in both instances, arguably impor-
A final source of errors arises when our model
tant information is lost. We present some of these
“corrects” the original document to match modern
ideas at the American Comparative Literature As-
conventions, as with diacritics, whose usages were
sociation’s annual meeting, where we discuss the
less conventionalized at the time these books were
relationship between sixteenth century indigenous
printed. For example, the string numero is often
orthography and Ocular’s code-switching language
transcribedasnu´mero,thecorrectmodernspelling.
models(Alpert-AbramsandGarrette,2015).
7 ConclusionsandFutureWork
Acknowledgements
WehavedemonstratedanunsupervisedOCRmodel
that improves upon Berg-Kirkpatrick et al. (2013)’s WewouldliketothankStephanieWood,KellyMc-
state-of-the-artOcularsysteminordertoeffectively Donough, Albert Palacios, Adam Coon, and Sergio
handle the code-switching and orthographic vari- Romero,aswellasKentNorsworthyfortheirinput,
ability prevalent in historical texts. In addition to advice,andassistanceonthisproject.
1040
References
Hannah Alpert-Abrams and Dan Garrette. 2015. Read-
ing Primeros Libros: From archive to OCR. In Pro-
ceedingsofTheAnnualMeetingoftheAmericanCom-
parativeLiteratureAssociation.
SusanBaddeleyandAnjaVoeste. 2013. Orthographies
inEarlyModernEurope. DeGruyter.
TaylorBerg-KirkpatrickandDanKlein. 2014. Improved
typesettingmodelsforhistoricalOCR. InProceedings
ofACL.
Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.
2013. Unsupervised transcription of historical docu-
ments. InProceedingsofACL.
Thomas G. Dolan. 2012. The Primeros Libros Project.
TheHispanicOutlookinHigherEducation,22:20–22,
March.
ElizabethL.Eisenstein. 1979. Theprintingpressasan
agentofchange. CambridgeUniversityPress.
Walter Mignolo. 1995. The Darker Side of the Renais-
sance. UniversityofMichiganPress.
ThamarSolorio,ElizabethBlair,SurajMaharjan,Steven
Bethard, Mona Diab, Mahmoud Gohneim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirschberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switcheddata. InProceedingsofTheFirstWorkshop
onComputationalApproachestoCodeSwitching.
1041

MusicLDM: Enhancing Novelty in Text-to-Music
Generation Using Beat-Synchronous Mixup Strategies
KeChen1∗ YusongWu2∗ HaoheLiu3∗ MariannaNezhurina4
TaylorBerg-Kirkpatrick1 ShlomoDubnov1
1UniversityofCaliforniaSanDiego
2Mila,QuebecArtificialIntelligenceInstitute,UniversitédeMontréal
3CentreforVisionSpeechandSignalProcessing,UniversityofSurrey
4LAION
Abstract
Diffusionmodelshaveshownpromisingresultsincross-modalgenerationtasks,
includingtext-to-imageandtext-to-audiogeneration.. However,generatingmusic,
asaspecialtypeofaudio,presentsuniquechallengesduetolimitedavailabilityof
musicdataandsensitiveissuesrelatedtocopyrightandplagiarism. Inthispaper,
totacklethesechallenges,wefirstconstructastate-of-the-arttext-to-musicmodel,
MusicLDM,thatadaptsStableDiffusionandAudioLDMarchitecturestothemusic
domain. Weachievethisbyretrainingthecontrastivelanguage-audiopretraining
model(CLAP)andtheHifi-GANvocoder,ascomponentsofMusicLDM,ona
collection of music data samples. Then, to address the limitations of training
dataandtoavoidplagiarism,weleverageabeattrackingmodelandproposetwo
differentmixupstrategiesfordataaugmentation: beat-synchronousaudiomixup
andbeat-synchronouslatentmixup,whichrecombinetrainingaudiodirectlyor
viaalatentembeddingsspace,respectively. Suchmixupstrategiesencouragethe
modeltointerpolatebetweenmusicaltrainingsamplesandgeneratenewmusic
within the convex hull of the training data, making the generated music more
diversewhilestillstayingfaithfultothecorrespondingstyle. Inadditiontopopular
evaluation metrics, we design several new evaluation metrics based on CLAP
scoretodemonstratethatourproposedMusicLDMandbeat-synchronousmixup
strategiesimproveboththequalityandnoveltyofgeneratedmusic,aswellasthe
correspondencebetweeninputtextandgeneratedmusic.
1 Introduction
Text-guidedgenerationtaskshavegainedincreasingattentioninrecentyearsandhavebeenapplied
to various modailties, including text-to-image, text-to-video, and text-to-audio generation. Text-
to-image generation has been used to create both realistic and stylized images based on textual
descriptions, which can be useful in various scenarios including graphic design. Text-to-audio
generationisarelativelynew,butrapidlygrowingarea,wherethegoalistogenerateaudiopieces,
suchassoundevents,soundeffects,andmusic,basedontextualdescriptions. Diffusionmodelshave
shownsuperiorperformanceinthesetypesofcross-modalgenerationtasks,includingsystemslike
DALLE-2[29]andStableDiffusion[31]fortext-to-image;andAudioGen[22],AudioLDM[24],
andMake-an-Audio[16]fortext-to-audio.
*Thefirstthreeauthorshaveequalcontribution. Contact: KeChen(knutchen@ucsd.edu),YusongWu
(wu.yusong@mila.quebec),andHaoheLiu(haohe.liu@surrey.ac.uk)
Preprint.Underreview.
3202
guA
3
]DS.sc[
1v64510.8032:viXra
Asaspecialtypeofaudiogeneration,text-to-musicgenerationhasmanypracticalapplications[2,7].
Forinstance,musicianscouldusetext-to-musicgenerationtoquicklybuildsamplesbasedonspecific
themesormoodsandspeeduptheircreativeprocess. Amateurmusicloverscouldleveragegenerated
piecestolearnandpracticeforthepurposeofmusicaleducation. However,text-to-musicgeneration
presentsseveralspecificchallenges. Oneofthemainconcernsisthelimitedavailabilityoftext-music
paralleltrainingdata[1]. Comparedtoothermodalitiessuchastext-to-image,therearerelatively
fewtext-musicpairsavailable,makingitdifficulttotrainahigh-qualityconditionalmodel. Large
anddiversetrainingsetsmaybeparticularlyimperativeformusicgeneration,whichinvolvesmany
nuancedmusicalconcepts,includingmelody,harmony,rhythm,andtimbre. Further,theeffectiveness
of diffusion models trained on more modest training sets has not been fully explored. Finally, a
relatedconcernintext-to-musicgenerationistheriskofplagiarismorlackofnoveltyingenerated
outputs[1]. Musicisoftenprotectedbycopyrightlaws,andgeneratingnewmusicthatsoundstoo
similartoexistingmusiccanleadtolegalissues. Therefore,itisimportanttodeveloptext-to-music
modelsthatcangeneratenovelanddiversemusicwhileavoidingplagiarism,evenwhentrainedon
relativelysmalltrainingdatasets.
In this paper, we focus on both these challenges: we develop a new model and training strategy
for learning to generate novel text-conditioned musical audio from limited parallel training data.
Currently, since there is no open-source model for text-to-music generation, we first construct a
state-of-the-arttext-to-musicgenerationmodel,MusicLDM,whichadaptstheStableDiffusion[31]
andAudioLDM[24]architecturestothemusicdomain. Next,toaddressthelimitedavailabilityof
trainingdataandtoencouragenovelgenerations,weadaptanideafrompastworkinothermodalities:
mixup[41],whichhasbeenappliedtocomputervisionandaudioretrievaltasks,augmentstraining
databyrecombiningexistingtrainingpointsthroughlinearinterpolation. Thistypeofaugmentation
encouragesmodelstointerpolatebetweentrainingdataratherthansimplymemorizingindividual
trainingexamples,andthusmaybeusefulinaddressingdatalimitationsandplagiarisminmusic
generation. However,formusicgeneration,thenaiveapplicationofmixupisproblematic. Simply
combiningwaveformsfromtwodistinctmusicalpiecesleadsunnaturalandill-formedmusic: tempos
andbeats(aswellasothermusicalelements)areunlikelytomatch. Thus,weproposetwonovel
mixupstrategies,specificallydesignedformusicgeneration: beat-synchronousaudiomixup(BAM)
andbeat-synchronouslatentmixup(BLM),whichfirstanalyzeandbeat-aligntrainingsamplesbefore
interpolatingbetweenaudiosamplesdirectlyorencodingandtheninterpolatinginalatentspace,
respectively.
Wedesignnewmetricsthatleverageapretrainedtextandaudioencoder(CLAP)totestforplagiarism
and novelty in text-to-music generation. In experiments, we find that our new beat-synchronous
mixupaugmentationstrategies,byencouragingthemodeltogeneratenewmusicwithintheconvex
hullofthetrainingdata,substantiallyreducetheamountofcopyingingeneratedoutputs. Further,our
newmodel,MusicLDM,incombinationwithmixup,achievesbetteroverallmusicalaudioqualityas
wellasbettercorrespondencebetweenoutputaudioandinputtext. Inbothautomaticevaluationsand
humanlisteningtests,MusicLDMoutperformsstate-of-the-artmodelsatthetaskoftext-to-music
generationwhileonlybeingtrainedon9Ktext-musicsamplepairs.
Music samples and qualitative results are available at musicldm.github.io. Code and models are
availableathttps://github.com/RetroCirce/MusicLDM/.
2 RelatedWork
2.1 Text-to-AudioGeneration
Text-to-audio generation (TTA) [24, 22, 40] is a type of generative task that involves creating
audiocontentfromtextualinput. Inyearspast, text-to-speech(TTS)[30,35]achievedfarbetter
performancethanothertypesofaudiogeneration.However,withtheintroductionofdiffusionmodels,
superiorperformanceinvariousgenerationtasksbecamemorefeasible. Recentworkhasfocused
ontext-guidedgenerationingeneralaudio, withmodelssuchasDiffsound[40], AudioGen[22],
AudioLDM [24], and Make-an-Audio [16] showing impressive results. In the domain of music,
text-to-musicmodelsincludetheretrieval-basedMuBERT[26],language-model-basedMusicLM
[1],diffusion-basedRiffusion[8]andNoise2Music[15]. However,acommonissuewithmostrecent
text-to-audio/musicmodelsisthelackofopen-sourcetrainingcode. Additionally,musicmodels
2
STFT+MelFB VAE VAE Hifi-GAN
Encoder Decoder
audio waveform mel-spectrogram
U-Net Latent Diffusion Model
CLAP
Audio Encoder
OR
A spectacular CLAP
dramatic trailer Text Encoder
......
corresponding text
FiLM Cocatenation
Figure1: ThearchitectureofMusicLDM,whichcontainsacontrastivelanguage-audiopretraining
(CLAP)model,anaudiolatentdiffusionmodelwithVAE,andaHifi-GANnerualvocoder.
oftenrequireslargeamountsofprivately-ownedmusicdatathatareinaccessibletothepublic,which
makesitdifficultforresearcherstoreproduceandbuildupontheirwork.
Amongallthesemodels,AudioLDMisbasedonopen-sourceStableDiffusion[31],CLAP[39],
andHiFi-GAN[19]architectures. Therefore,webaseourtext-to-musicgenerationmodelonthe
AudioLDMarchitecture,tocreateMusicLDMforourexperiments.
2.2 PlagiarismonDiffusionModels
Diffusion models have been shown to be highly effective at generating high-quality and diverse
samplesfortext-to-imagetasks. However,apotentialissuewiththesemodelsistheriskofplagiarism
[33,3],orthegenerationnovelty. Asstatedby[33],diffusionmodelsarecapableofmemorizingand
combiningdifferentimageobjectsfromtrainingimagestocreatereplicas,whichcanleadtohighly
similarorevenidenticalsamplestothetrainingdata. [3]exploresdifferentmethodsthatcouldextract
thetrainingdatawithagenerate-and-filterpipeline,showingthatnewadvancesinprivacy-preserving
training of diffusion models are required. Such issues are especially concerning in the domain
ofmusic,wherecopyrightlawsareheavilyenforcedandviolationscanresultinsignificantlegal
andfinancialconsequences. Therefore,thereisaneedtodevelopstrategiestomitigatetheriskof
plagiarismintext-to-musicgenerationusingdiffusionmodels.
2.3 MixuponDataAugmentation
Mixup [41] is a widely used data augmentation technique that has shown remarkable success in
improvingmodelgeneralizationandmitigatingoverfitting. Thebasicprincipleofmixupistolinearly
combinepairsoftrainingsamplestoeffectivelyconstructnewsamplesthatlieonthelineconnecting
theoriginalsamplesinthefeaturespace,encouragingthemodeltolearnamorecontinuousandrobust
decisionboundary. Inthispaper,weexplorethemixuptechniqueinthecontextoftext-to-music
generationbasedonlatentdiffusionmodels. Differentfromthemixupinothermodalities,music
mixupinvolvesadelicatebalanceofmusicalelementstopreventthemixedmusicfrombeingchaotic
noise. Moreover, indiffusionmodels, mixupalsocanrefertothecombinationoflatentfeatures,
ratherthanmusicsignals. Weproposetwomixupstrategiestailoredformusiclatentdiffusionmodels
andexploretheirpotentialbenefitsfordataaugmentationandgenerationperformance.
3 Methodology
3.1 MusicLDM
AsillustratedinFigure1,MusicLDMhassimilararchitectureasAudioLDM:acontrastivelanguage-
audiopretraining(CLAP)model[39],anaudiolatentdiffusionmodel[24]withapretrainedvaria-
tionalauto-encoder(VAE)[18],andaHifi-GANneuralvocoder[19].
Formally,givenanaudiowaveformx∈RT itscorrespondingtext,whereT isthelengthofsamples,
wefeedthedataintothreemodules:
3
1. We pass x through the audio encoder [5] of CLAP f (·), to obtain the semantic audio
audio
embeddingEa ∈RD,whereDistheembeddingdimension.
x
2. Wepassthetextofxthroughthetextencoder[25]ofCLAPf (·),totoobtainthesemantic
text
textembeddingEt ∈RD.
x
3. Wetransformxintointhemel-spectrogramx ∈ RT×F. Thenwepassx intotheVAE
mel mel
encoder,toobtainanaudiolatentrepresentationy ∈RC× PT× PF,whereT isthemel-spectrogram
frame size, F is the number of mel bins, C is the latent channel size of VAE, and P is the
downsampling rate of VAE. The VAE is pretrained to learn to encoder and decode the mel-
spectrogramofmusicdata.
InMusicLDM,thelatentdiffusionmodelhasaUNetarchitecturewhereeachencoderordecoder
blockiscomposedofaResNetlayer[11]andaspatialtransformerlayer[31]. Duringthetraining,
thesemanticembeddingoftheinputaudioE isconcatenatedwiththelatentfeatureofeachUNet
x
encoderanddecoderblockbytheFiLMmechanism[27]. Theoutputofthediffusionmodelisthe
estimated noise ϵ (z ,n,E ) from n to (n−1) time step in the reverse process, where θ is the
θ n x
parametergroupofthediffusionmodel,andz isthen-stepfeaturegeneratedbytheforwardprocess.
n
Weadoptthetrainingobjective[13,21]asthemeansquareerror(MSE)lossfunction:
L (θ)=E ||ϵ−ϵ (z ,n,E )||2 (1)
n z0,ϵ,n θ n x 2
wherez =yistheaudiolatentrepresentationfromVAE(i.e.,thegroundtruth),andϵisthetarget
0
noisefortraining. Moredetailsregardingthetrainingandthearchitectureofthelatentdiffusion
modelcanbereferredinAppendixA.
ForMusicLDM,wemaketwochangesfromtheoriginalAudioLDMtoenhanceitsperformanceon
text-to-musicgeneration. First,sincetheoriginalcontrastivelanguage-audiopretraining(CLAP)
modelispretrainedontext-audiopairdatasetsdominatedbysoundevents,soundeffectsandnatural
sounds,weretrainedtheCLAPontext-musicpairdatasets(detailsinAppendixB)toimproveits
understandingofmusicdataandcorrespondingtexts. WealsoretrainedtheHifi-GANvocoderon
musicdatatoensurehigh-qualitytransformsfrommel-spectrogramstomusicwaveforms. Second,
intheoriginalAudioLDM,themodelisonlyfedwithaudioembeddingsastheconditionduring
thetrainingprocess,i.e.,E =Ea;anditisfedwithtextembeddingstoperformthetext-to-audio
x x
generation,i.e.,E = Et. Thisapproachleveragesthealignmentoftextandaudioembeddings
x x
insideCLAPtotrainthelatentdiffusionmodelwithmoreaudiodatawithouttexts. However,this
audio-to-audiotrainingϵ (z ,n,Ea)isessentiallyanapproximationofthetext-to-audiogeneration
θ n x
ϵ (z ,n,Et). AlthoughCLAPistrainedtolearnjointembeddingsfortextandaudio,itdoesnot
θ n x
explicitlyenforcetheembeddingstobedistributedsimilarlyinthelatentspace,whichcanmake
itchallengingforthemodeltogeneratecoherenttext-to-audiooutputssolelywithaudio-to-audio
training. This problem becomes more severe when the available text-music pair data is limited.
Moreover,relyingsolelyonaudioembeddingsignorestheavailabletextdata,whichmeansthatwe
arenotleveragingthefullpotentialofourdataset. Consequently,generatingaccurateandrealistic
text-to-audiogenerationsmaynotbeeffective.
Tofurtherinvestigatethistask,weintroducetwoadditionaltrainingapproachesforcomparison:
1. TraintheMusicLDMdirectlyusingthetextembeddingasthecondition,i.e.,ϵ (z ,n,Et)
θ n x
2. Train the MusicLDM using the audio embedding as the condition, then finetune it with text
embedding,i.e.,ϵ (z ,n,Ea)→ϵ (z ,n,Et)
θ n x θ n x
Thefirstapproachfollowstheoriginaltargetoftext-to-audio,servingasacomparisonwithaudio-to-
audiotraining. Thesecondapproachisproposedasanimprovementonaudio-to-audiogeneration.
asweshifttheconditiondistributionfromtheaudioembeddingbacktothetextembeddingduring
thetrainingofthediffusionmodel. Insection4.2,wecomparedtheabovetwoapproacheswiththe
originalaudio-to-audiotrainingapproachestodeterminethebestapproachforgeneratinghigh-quality
andhighlycorrelatedtext-to-musicoutputs.
3.2 Beat-SynchronousMixup
AsshowninFigure2,weproposetwomixupstrategiestoaugmentthedataduringtheMusicLDM
training: Beat-SynchronousAudioMixup(BAM)andBeat-SynchronousLatentMixup(BLM).
4
Beat-Synchronous Audio Mix-Up Audio Space
Music Dataset
CLAP
Diffusion audio
mix-up
Model
tempo d ao lw ign nb mea et nt
VAE
VAE Dec. Hifi-GAN OM tu hs ei rc s
Beat Transformer VAE
Enc.
Diffusion latent mix-up
Beat-Synchronous Latent Mix-Up Model CLAP Latent Space
Figure2: Mixupstrategies. Left: tempogroupinganddownbeatalignmentviaBeatTransformer.
Middle: BAMandBLMmixupstrategies. Right: HowBAMandBLMareappliedinthefeature
spaceofaudiosignalsandaudiolatentvariables.
Beat-trackingviaBeatTransformer Musicalcompositionsaremadeupofseveralelements,such
aschordprogressions,timbre,andbeats.Ofthese,beatsplayacrucialroleindeterminingthemusical
structureandalignment. Inmostaudioretrievaltasks,mixupisapopulartechniquethatinvolves
randomlymixingpairsofaudiodatatoaugmentthetrainingdata. However,whenmixingtwomusic
samplesthathavedifferenttempos(beatsperminute),themixturecanbechaoticandunappealing.To
avoidthis,weuseastate-of-the-artbeattrackingmodel,BeatTransformer[42],toextractthetempo
anddownbeatmapofeachmusictrack,asshownintheleftofFigure2. Wecategorizeeachmusic
trackintodifferenttempogroupsandduringtraining,weonlymixedtrackswithinthesametempo
grouptoensurethetrackswereinsimilartempos. Furthermore,wealignthetracksbycomparing
theirdownbeatmapsandselectingacertaindownbeattoserveasthestartingpositionforthemixup
track. Thispreprocessingapproachallowsustobetterselectthemusicdataavailableformixup,
resultinginmixuptracksthatareneatlyorderedintermsoftempoanddownbeats.
Beat-SynchronousAudioMixup AsdepictedintheupperpartofFigure2,onceweselecttwo
alignedmusictracksx andx ,wemixthembyrandomlyselectingamixingratiofromthebeta
1 2
distributionλ∼B(5,5),as:
x=λx +(1−λ)x (2)
1 2
WethenusethemixeddataxtoobtaintheCLAPembeddingE andtheaudiolatentvariabley.
x
Wetrainthelatentdiffusionmodelusingthestandardpipeline. Thisbeat-synchronousaudiomixup
strategyisreferredtoasBAM.
Beat-SynchronousLatentMixup AsdepictedinthelowerpartofFigure2,inthelatentdiffusion
model,themixupprocesscanalsobeappliedonthelatentvariables,referredasbeat-synchronous
latentmixup(BLM).Afterselectingtwoalignedmusictracksx andx ,wefeedthemintotheVAE
1 2
encodertoobtainthelatentvariablesy andy . Wethenapplythemixupoperationtothelatent
1 2
variables:
y =λy +(1−λ)y (3)
1 2
IncontrasttoBAM,BLMappliesthemixupoperationtothelatentspaceofaudio,wherewecannot
ensurethatthemixtureofthelatentvariablescorrespondstotheactualmixtureofthemusicfeatures
intheappearance. Therefore,wefirstgenerateamixedmel-spectrogramx byfeedingthemixed
mel
latentvariableyintotheVAEdecoder. Then,wefeedx totheHifi-GANvocodertoobtainthe
mel
mixedaudioxastheinputmusic. Withxandy,wefollowthepipelinetotraintheMusicLDM.
WhatareBAMandBLMdoing? AsshownintherightofFigure2,wedemonstratetheinterpola-
tionbetweenthefeaturespaceofaudiowhenusingBAMandBLM.Inthefeaturespaceofaudio
signals,the"•"representsthefeaturepointofmusicdata,whilethe"△"denotesthefeaturepointof
otheraudiosignals,suchasnaturalsound,audioactivity,andnoise. Duringthepretrainingprocess
ofVAE,alatentspaceisconstructedforencodinganddecodingthemusicdata. TheVAEaimsto
learnthedistributionofthelatentvariablesthatcanbestrepresenttheoriginaldataandtransformthe
originalfeaturespaceintoalower-dimensionalmanifold. Thismanifoldisdesignedtocapturethe
5
underlyingstructureofthemusicdata. Therefore,anyfeaturepointwithinthismanifoldisconsidered
tobeavalidrepresentationofmusic.
BAMandBLMareconcernedwithaugmentingthedataatdifferentlevelsoffeaturespace. Asshown
inrightofFigure2,BAMlinearlycombinestwopointsinaudiospacetoformanewpointonthered
line. BLM,representedbytheblueline,performsasimilaroperation,butresultinanewpointinthe
VAE-transformedlatentspace,whichwillbedecodedbackontothemusicmanifoldofaudiospace.
BothBAMandBLMofferuniqueadvantagesanddisadvantages. BAMappliesmixupintheoriginal
featurespace,resultinginasmoothinterpolationbetweenfeaturepoints. However,BAMcannot
ensureareasonablemusicsamplethatlieswithinthemusicmanifold. Thisissueismoreproblematic
usingthesimpleaudiomixupstrategywithouttempoanddownbeatalignments. BLM,conversely,
augmentswithinthemusicmanifold,fosteringrobustanddiverselatentrepresentations. However,
BLMiscomputationallymoreexpensiveasitrequirescomputingthelatentfeaturebacktoaudiovia
VAEdecoderandHifi-GAN.Furthurmore,whentheill-definedorcollapsedlatentexistsinVAE,
BLMmaybeoutofeffectiveness.
BothBAMandBLMareeffectivedataaugmentationtechniquesthatencouragethemodeltolearn
amorecontinuousandrobustdecisionboundaryontheaudiofeaturespace,orimplicitlyfromthe
latent space to the audio space, which can improve the model’s generalization performance and
mitigateoverfitting. Inthecontextoftext-to-musicgeneration,thesemixupstrategiescanhavea
potentialtomitigatethelimitationsofdatasizeandhelpavoidplagiarismissues. Byintroducing
smallvariationsthroughmixup,themodelcantouchamorerichspaceofmusicdataandgenerate
musicsamplesthatarecorrelatedtothetextsbutshowdifferencestotheoriginaltrainingdata. In
Section4.2,weevaluatedwhetherthesestrategiesmitigatethedatalimitationandplagiarismissues.
4 Experiments
Inthissection,weconductedfourexperimentsonourproposedmethods. First,weretrainedanew
CLAPmodeltoprovidetheconditionembeddingforMusicLDM.Second,wetrainedMusicLDM
withdifferentmixupstrategiesandcomparedthemwithavailablebaselines. Third,weevaluated
MusicLDMintermsoftext-musicrelevance,noveltyandplagiarismriskviametricsbasedonCLAP
scores. Finally,weconductedasubjectivelisteningtesttogiveanadditionalevaluation.
4.1 ExperimentalSetup
Dataset TheoriginalCLAPmodeltrainedonmostlyacousticeventsandsoundeffectdatasets.
Inthiswork,wetrainedaCLAPmodelonmusicdatasetsinadditiontoitsoriginaltrainingdata,
allowingittobetterunderstandtherelationbetweenmusicandtextualdescriptions. ThenewCLAP
model is trained on dataset of 2.8 Million text-audio pairs, in an approximate total duration of
20000hours. ComparedtopreviousCLAPmodel,thenewlytrainedCLAPmodelperformswellin
zero-shotclassificationforbothacousticeventandmusic. Pleasereferfurtherdetailsontrainingand
performanceofthenewCLAPinAppendixB.ForMusicLDM,weusedtheAudiostockdatasetfor
training,alongwithVAEandHifi-GAN.Specifically,theAudiostockdatasetcontains9000music
tracksfortrainingand1000tracksfortesting. Thetotaldurationis455.6hours. Itprovidesacorrect
textualdescriptionofeachmusictrack. AlthoughCLAPistrainedonmoretext-musicdatapairs,
thelargenumberofthemarepseudo-captionscomposedprimarilyofnon-specificmetadata,such
asauthor,songtitle,andalbuminformation(e.g.,[a song by author A from the album B]).
Thesecaptionsdonotalignwithourspecificobjectiveoftext-to-musicgeneration.
HyperparametersandTrainingDetails WetrainedallMusicLDMmoduleswithmusicclips
of10.24secondsat16kHzsamplingrate. InboththeVAEanddiffusionmodel,musicclipsare
representedasmel-spectrogramswithT =1024framesandF =128mel-bins. UnlikeAudioLDM,
MusicLDM’sVAEutilizesadownsamplingrateofP =8andalatentdimensionofC =16. The
architectureofMusicLDM’slatentdiffusionmodelfollowsthatofAudioLDM.Thetrainingprocess
of MusicLDM aligns with AudioLDM’s approach. For additional hyperparameters and training
details,refertoAppendixA.
6
Table1: TheevaluationofgenerationqualityamongMusicLDMsandbaselines. AA-Train. and
TA-Train. refertotheaudio-audiotrainingschemeandthetext-audiotraningscheme.
Model AA-Train. TA-Train. FDpann↓ FDvgg↓ InceptionScore↑ KLDiv.↓
Riffusion[8] ✗ ✓ 68.95 10.77 1.34 5.00
MuBERT[26] — — 31.70 19.04 1.51 4.69
AudioLDM ✓ ✗ 38.92 3.08 1.67 3.65
MusicLDM ✓ ✗ 26.67 2.40 1.81 3.80
MusicLDM(OnlyTA-Training) ✗ ✓ 32.40 2.51 1.49 3.96
MusicLDMw/.mixup ✓ ✗ 30.15 2.84 1.51 3.74
MusicLDMw/.BAM ✓ ✗ 28.54 2.26 1.56 3.50
MusicLDMw/.BLM ✓ ✗ 24.95 2.31 1.79 3.40
MusicLDMw/.Text-Finetune ✓ ✓ 27.81 1.75 1.76 3.60
MusicLDMw/.BAM&Text-Finetune ✓ ✓ 28.22 1.81 1.61 3.61
MusicLDMw/.BLM&Text-Finetune ✓ ✓ 26.34 1.68 1.82 3.47
4.2 MusicLDMPerformance
4.2.1 GenerationQuality
Followingevaluationtechniquesusedinpastworkonaudiogeneration[24],weusefrechetdistance
(FD),inceptionscore(IS),andkullback-leibler(KL)divergencetoevaluatethequalityofgenerated
musicalaudiooutputs. Frechetdistanceevaluatestheaudioqualitybyusinganaudioembedding
modeltomeasurethesimilaritybetweentheembeddingspaceofgenerationsandthatoftargets. In
thispaper,weusetwostandardaudioembeddingmodels:VGGish[12]andPANN[20].Theresulting
distanceswedenoteasFD andFD ,respectively. Inceptionscoremeasuresthediversity
vgg pann
andthequalityofthefullsetofaudiooutputs,whileKLdivergenceismeasuredonindividualpairs
ofgeneratedandgroundtruthaudiosamplesandaveraged. Weusetheaudioldm_evallibrary1to
evaluateallthemetricsmentionedabove, comparingthegroundtruthaudiofromtheAudiostock
1000-tracktestsetwiththe1000tracksofmusicgeneratedbyeachsystembasedonthecorresponding
textualdescriptions.
Table 1 presents the FD, IS, and KL results for our models in comparison with baseline models.
In the first section, we utilized textual descriptions from the test set, sending them to the offical
APIsofRiffusionandMuBERTtogeneratecorrespondingresults. BothRiffusionandMuBERT
wereunabletoachieveresultscomparabletotheremainingmodels. Uponreviewingthegenerated
musicfromthetwosystems,wefoundthatthesub-optimalperformanceofRiffusionresultedfrom
poormusicgenerationquality,withmanysampleseitherinaudibleoroutsidethedesiredmusical
range. MuBERT,whilegeneratinghigh-qualitypiecesfromrealmusicsamplelibraries,fellshort
in replicating the distribution of Audiostock dataset. Due to the unavailability of their detailed
architectures,trainingscripts,anddata,RiffusionandMuBERT’sevaluationsofferedonlypartial
comparisons.
WealsoretrainedtheoriginalAudioLDMmodelontheAudiostockdataset,comparingittoMusi-
cLDMvariants. ThedistinctionbetweenAudioLDMandMusicLDMliesinthedifferentCLAP
models used for condition embeddings. Our comparison revealed that MusicLDM outperforms
AudioLDMintermsofFD ,FD ,andIS.ThisunderscorestheefficacyofthenovelCLAP
pann vgg
modelpretrainedformusic,providingmoresuitablemusicembeddingsasconditioninginformation.
ComparingMusicLDM’sperformancewithaudio-to-audiotraining(ϵ (z ,n,Exa))andtext-to-
θ n
audio training (ϵ (z ,n,Ext)), denoted by “MusicLDM (Only TA-Training)”, we note inferior
θ n
resultsinthelatterapproach. Thismaybesuggeststhatagapexistsbetweendistributionoftext
embeddingandaudioembedding,makingitchallengingforthediffusionmodeltogeneratehigh-
qualityaudiosolelyfromtextembedding. Incontrast,CLAP’saudioembeddingmayleaklow-level
audioinformationtothediffusionmodelduringinitialtrainingstages,hurtingthemodel’sabilityto
generalizetotextembeddinginputs.ThishypothesisisfurthersupportedbytheresultsofMusicLDM
with combined audio-to-audio training and text-to-audio fine-tuning. We observe a significant
decreaseinFDvggwithsmallchangesinFDpannandIS,indicatingasubstantialimprovement
inmusicgenerationquality,drivenbyleveragingbothaudioandtextembeddingsduringtraining.
The former facilitates good audio reconstruction during early training, while the latter shifts the
distributionfromaudiototexttoalignwiththeeventualtest-timetaskoftext-to-musicgeneration.
1https://github.com/haoheliu/audioldm_eval
7
Table2: Theobjectivemetricstomeasuretherelevanceandnovelty(plagiarism). Andthesubjective
listeningtesttoevaluatethequality,relevance,andmusicality.
ObjectiveMetrics
SubjectiveListeningTest
Model
Relevance NoveltyandPlagiarismRisk
Text-AudioSimilarity↑ SIMAA@90↓ SIMAA@95↓ Quality↑ Relevance↑ Musicality↑
TestSet(Ref.) 0.325 — — — — —
RetrievalMax(Ref.) 0.423 — — — — —
MuBERT 0.131 0.107 0 2.02 1.50 2.33
MusicLDM(original) 0.281 0.430 0.047 1.98 2.17 2.19
MusicLDMw/.mixup 0.234 0.391 0.028 — — —
MusicLDMw/.BAM 0.266 0.402 0.027 2.04 2.21 2.01
MusicLDMw/.BLM 0.268 0.401 0.020 2.13 2.31 2.07
Audio-Audio Similarity Text-Audio Similarity
1.0
0.5
0.0
MusicLDM (mix-up) MusicLDM (original) MusicLDM (BLM) MusicLDM (BAM) MuBERT
Figure3: Theviolinplotoftheaudio-audiosimilarity,andthetext-to-audiosimilarity.
Last,wecomparedMusicLDMwithdifferentmixupstrategies,namelysimplemixup[41],BAM,
andBLM.Thecomparisonrevealsthenegativeimpactofthesimplemixuponallmetrics. This
degradationingeneratedsamplequality, characterizedbyinstrumentalinterferenceandnoise, is
attributedtothesimplemixup’sinabilitytoguaranteemusicalityinthemix. Similarobservations
are evident in the BAM results, indicated by a drop in FD and IS. However, BAM’s tempo
pann
and downbeat alignment, along with the original mixup benefits, counterbalance this defect to a
certainextent,enhancingthemodel’sgeneralizationabilityandimprovingcertainmetrics. BLM,as
alatentspacemixingmethod,alignswithourhypothesisinSection3.2thatlatentspacemixupyield
audiocloselyresemblingmusic. Thistechniqueallowsustolargelybypassthepotentialconfusion
issuestiedtoaudiomixing,thuscapitalizingonmixup’sabilitytodrivegeneralizationandprevent
copyingviadataaugmentation. Furthermore,incorporatingtext-finetuningresultsinacomprehensive
improvementofmusicgenerationquality,solidifyingBLMasthemosteffectivestrategy.
4.2.2 Text-AudioRelevance,NoveltyandPlagiarism
Weproposedtwometricgroups,text-audiosimilarityandnearest-neighboraudiosimilarityratio
toassesstext-audiorelevance,novelty,andplagiarismriskinvariousmodels.
First,text-audiosimilaritymeasurestherelevancebetweenthetextandtheaudio. Itisdefinedasthe
dotproductbetweenthegroundtruthtextembeddingEt fromthetestsetandtheaudioembedding
gd
Eafrommusicgeneratedbymodels,i.e.,Et ·Ea. Theembeddingsfrombothtextandaudioare
gd
normalizedinCLAPmodel,thusthedotproductcomputesthecosinesimilaritybetweentextand
audioembeddings.
Second,wewouldalsoliketomeasuretheextenttowhichmodelsaredirectlycopyingsamplesfrom
thetrainingset. Weverifythisbyfirstcomputingthedotproductsbetweentheaudioembeddingof
eachgeneratedmusicoutputtoallaudioembeddingsfromtheAudiostocktrainingsetandreturning
themaximum–i.e.,thesimilarityofthenearest-neighborinthetrainingset. Then,wecomputethe
fractionofgeneratedoutputswhosenearest-neighborsareaboveathresholdsimilarity. Wereferthis
asthenearest-neighboraudiosimilarityratio,providingSIM @90wherethethresholdis0.9,and
AA
SIM @95with0.95. Thelowerthisfraction,thelowertheriskofplagiarism–i.e.,fewersamples
AA
haveveryclosetrainingneighbors. IntheAppendixD,weshowpairsofexampleswithbothhigh
andlowsimilarityscorestogivefurtherintuitionforthismetric.
8
AsshownintheleftandmiddlecolumnofTable2,wepresenttheaveragetext-audiosimilarityand
nearest-neighboraudiosimilarityratiosfortwothresholdsonthe1000tracksintheAudiostocktest
setandthegeneratedmusicfromMuBERTanddifferentvariantsofMusicLDM.Wealsoprovide
tworeferencepointsfortext-audiosimilarity:“TestSet”and“RetrievalMax”. Specifically,“TestSet”
referstocomputingthecosinesimilaritybetweenthegroudtruthtextembeddingandthegroudtruth
audioembedding. And“RetrievalMax”referstofirstcomputingthecosinesimilaritiesbetween
eachtextembeddingfromthetestsettoallaudioembeddingsfromthetrainingset,thenpickingthe
highestscoreasthescoreofthistext,andtakingtheaverageoveralltextscores.
WecanobservethattheoriginalMusicLDMwithoutmixupachievesthehighesttext-audiorelevance
withanaveragescoreof0.281,butalsothehighest(worst)nearest-neighboraudiosimilarityratio.
MusicLDMwiththesimplemixupstrategyachievesthelowestSIM @90ratiowhilesacrificinga
AA
lotintherelevanceofthegeneration.TheMusicLDMwithBAMandBLMachieveabalancebetween
theaudiosimilarityratiosandthetext-to-audiosimilarity. Incombinationwiththequalityevaluation
resultsinTable1, wecanconcludethatallmixupstrategiesareeffectiveasadataaugmentation
techniquestoimprovegeneralizationofthemodeltogeneratemorenovelmusic. Howeversimple
mixupdegradesthegenerationquality,whichaffectstherelevancescorebetweenaudioandtext,and
alsothusmakesitlesssimilartothetracksinthetrainingset. BAMandBLMapplythetempoand
downbeatfilteringonthemusicpairstomix,allowingthemodeltomaintainsuperiorgeneration
quality(Table1)andtext-audiorelevance(Table2),whilestillutilizingthebenefitbroughtbythe
mixuptechniquetomakethegenerationmorenovel(lessplagiarism). Amongtheobjectivemetrics,
BLMisthebestmixupstrategyintermsofquality,relevanceandnoveltyofthegeneratedaudio.
Thisindicatesmixinginthelatentspaceismoreefficientthanmixingdirectlyinaudiospace,perhaps
becausethelatentembeddingapproachimplicitlyprojectsthemixturetothelearnedmanifoldof
well-formedmusic. Weshowthedetaileddistributionofthesemetricsover1000generatedtracksin
Figure3,where,forexample,audio-audiosimilaritydenotestheindividualscoresusedtocalculate
theaverageSIM . WefindthattheoriginalMusicLDMwithoutmixuphasmoresampleswith
AA
hightrainingsimilaritythanothermodels,whichfurtherreflectsthatitismorepronetocopying.
4.3 SubjectiveListeningTest
AsshownintherightofTable2,weconductthesubjectivelisteningtestonfourmodels,namely
MuBERT,theoriginalMusicLDM,andthatwithBAMorBLMstrategy,tofurtherevaluatetheactual
hearingexperienceofthegeneratedmusic. WedonotincludethesimplemixupMusicLDMbecause
itsgenerationisatalowqualitywhileweavoidconfusingsubjectswithtoomanymodelsinthesame
time. Weinvite15subjectstolistento6groupsofthegenerationsrandomlyselectedfromthetest
set. Eachofgroupcontainsfourgenerationsfromfourmodelsandthecorrespondingtextdescription.
Thesubjectsarerequiredtoratethemusicintermsofquality,relevance,andmusicality(detailedin
AppendixE).WeobservethatthesamplesofMusicLDMwithBAMorBLMmixupstrategyachieve
abetterrelevanceandqualitythanthoseofMuBERTandtheoriginalMusicLDM,thisstrengths
ouraboveanalysis. TheMuBERTsamplesachievethebestMusicality,becauseitsgenerationis
combined from the real music samples. Combined with the objective metrics, beat-synchronous
latentmixupstandstobethemosteffectivenessmethodforenhancingthetext-to-musicgeneration
intermsofquality,text-musicrelevanceandnovelty(i.e.,reducingtheriskofplagiarism).
5 Limitations
Inthissectionweoutlinetherecognizedlimitationsofourstudy,servingasaroadmapforfuture
improvements. Firstly,MusicLDMistrainedonthemusicdatainasamplingrateof16kHz,while
moststandardmusicproductionsare44.1kHz.Thisconstraint,tiedtotheHifi-GANvocoder’ssubpar
performanceathighsamplingrates, impedespracticaltext-to-musicapplicationandnecessitates
furtherimprovements. Secondly,resourceconstraintssuchaslimitedrealtext-musicdataandGPU
processingpowerpreventusfromscalingupMusicLDM’straining. Weareunabletodetermine
ifmix-upstrategiescouldyieldsimilartrendsasobservedwiththeAudiostockdataset. Thisissue
existsintheimagegenerationtaskaswell. Lastly,whilewerecognizebeatinformationascrucialfor
musicalignment,thereisscopeforexploringothersynchronizationtechniqueslikekeysignatureand
instrumentalignment. Wealsointendtoinvestigatetheapplicationofdifferentaudiospacefiltersto
selectsuitablemusicpairsformixing.
9
6 Conclusion
Inthispaper,weintroduceMusicLDM,atext-to-musicgenerationmodelthatincorporatesCLAP,
VAE,Hifi-GAN,andlatentdiffusionmodels. WeenhanceMusicLDMbyproposingtwoefficient
mixupstrategies: beat-synchronousaudiomixup(BAM)andbeat-synchronouslatentmixup(BLM),
integrated into its training process. We conduct comprehensive evaluations on different variants
ofMusicLDMusingobjectiveandsubjectivemetrics,assessingquality,text-musicrelevance,and
novelty. TheexperimentalresultsdemonstratetheeffectivenessofBLMasastandoutmixupstrategy
fortext-to-musicgeneration.
7 Acknowledgments
WewouldliketothanktheInstituteforResearchandCoordinationinAcousticsandMusic(IRCAM)
andProjectREACH:RaisingCo-creativityinCyber-HumanMusicianshipforsupportingthisproject.
ThisprojecthasreceivedfundingfromtheEuropeanResearchCouncil(ERCREACH)underthe
EuropeanUnion’sHorizon2020researchandinnovationprogramme(GrantAgreement#883313).
WewouldliketothankthesupportofcomputationinfrastructurefromLAION.
References
[1] AndreaAgostinelli,TimoIDenk,ZalánBorsos,JesseEngel,MauroVerzetti,AntoineCaillon,
QingqingHuang,ArenJansen,AdamRoberts,MarcoTagliasacchi,etal. MusicLM:Generating
musicfromtext. arXivpreprint:2301.11325,2023.
[2] Jean-PierreBriot,GaëtanHadjeres,andFrançois-DavidPachet. Deeplearningtechniquesfor
musicgeneration,volume1. Springer,2020.
[3] NicholasCarlini,JamieHayes,MiladNasr,MatthewJagielski,VikashSehwag,FlorianTramer,
BorjaBalle,DaphneIppolito,andEricWallace. Extractingtrainingdatafromdiffusionmodels.
arXivpreprint:2301.13188,2023.
[4] HonglieChen,WeidiXie,AndreaVedaldi,andAndrewZisserman. Vggsound: Alarge-scale
audio-visualdataset. InProc.ICASSP,2020.
[5] KeChen,XingjianDu,BileiZhu,ZejunMa,TaylorBerg-Kirkpatrick,andShlomoDubnov.
Hts-at: Ahierarchicaltoken-semanticaudiotransformerforsoundclassificationanddetection.
InProc.ICASSP,pages646–650.IEEE,2022.
[6] BenjaminElizalde,SohamDeshmukh,MahmoudAlIsmail,andHuamingWang. Claplearning
audioconceptsfromnaturallanguagesupervision. InProc.ICASSP,pages1–5.IEEE,2023.
[7] RebeccaFiebrinkandBaptisteCaramiaux. Themachinelearningalgorithmascreativemusical
tool. arXivpreprint:1611.00379,2016.
[8] SethForsgrenandHaykMartiros. Riffusion-Stablediffusionforreal-timemusicgeneration.
2022.
[9] PriyaGoyal,PiotrDollár,RossGirshick,PieterNoordhuis,LukaszWesolowski,AapoKyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training
imagenetin1hour. arXivpreprintarXiv:1706.02677,2017.
[10] AndreyGuzhov,FedericoRaue,JörnHees,andAndreasDengel. Audioclip: Extendingclipto
image,textandaudio. InProc.ICASSP,pages976–980.IEEE,2022.
[11] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. InProc.CVPR,pages770–778,2016.
[12] ShawnHershey,SourishChaudhuri,DanielPWEllis,JortFGemmeke,ArenJansen,RChan-
ningMoore,ManojPlakal,DevinPlatt,RifASaurous,BryanSeybold,etal. Cnnarchitectures
forlarge-scaleaudioclassification. InProc.ICASSP,pages131–135.IEEE,2017.
10
[13] J.Ho, A.Jain, andP.Abbeel. Denoisingdiffusionprobabilisticmodels. InProc.NeurIPS,
2020.
[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598,2022.
[15] QingqingHuang,DanielSPark,TaoWang,TimoIDenk,AndyLy,NanxinChen,Zhengdong
Zhang,ZhishuaiZhang,JiahuiYu,ChristianFrank,etal. Noise2music: Text-conditionedmusic
generationwithdiffusionmodels. arXivpreprint:2302.03917,2023.
[16] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui
Ye,JinglinLiu,XiangYin,andZhouZhao. Make-an-audio: Text-to-audiogenerationwith
prompt-enhanceddiffusionmodels. arXivpreprint:2301.12661,2023.
[17] DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InProc.
ICLR,2014.
[18] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. Proc.ICLR,2013.
[19] JungilKong,JaehyeonKim,andJaekyoungBae. HifiGAN:Generativeadversarialnetworks
forefficientandhighfidelityspeechsynthesis. Proc.NeurIPS,33:17022–17033,2020.
[20] QiuqiangKong,YinCao,andTurabIqbaletal. Panns: Large-scalepretrainedaudioneural
networksforaudiopatternrecognition. IEEETrans.Audio,Speech,Lang.Process.,2020.
[21] Z.Kong,W.Ping,J.Huang,K.Zhao,andB.Catanzaro. Diffwave: Aversatilediffusionmodel
foraudiosynthesis. InProc.ICLR,2021.
[22] FelixKreuk,GabrielSynnaeve,AdamPolyak,UrielSinger,AlexandreDéfossez,JadeCopet,
DeviParikh,YanivTaigman,andYossiAdi. AudioGen: Textuallyguidedaudiogeneration.
Proc.ICLR,2022.
[23] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
appliedtodocumentrecognition. Proc.IEEE,1998.
[24] HaoheLiu,ZehuaChen,YiYuan,XinhaoMei,XuboLiu,DaniloMandic,WenwuWang,and
MarkDPlumbley. AudioLDM:Text-to-audiogenerationwithlatentdiffusionmodels. Proc.
ICML,2023.
[25] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,Mike
Lewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretraining
approach. arXivpreprint:1907.11692,2019.
[26] MubertAI. Mubert: Asimplenotebookdemonstratingprompt-basedmusicgeneration.
[27] EthanPerez,FlorianStrub,HarmDeVries,VincentDumoulin,andAaronCourville. Film:
Visualreasoningwithageneralconditioninglayer. InProc.AAAI,volume32,2018.
[28] KarolJ.Piczak. ESC:datasetforenvironmentalsoundclassification. InProc.ACMMultimed.,
2015.
[29] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchical
text-conditionalimagegenerationwithcliplatents. arXivpreprint:2204.06125,2022.
[30] YiRen,ChenxuHu,XuTan,TaoQin,ShengZhao,ZhouZhao,andTie-YanLiu. FastSpeech2:
Fastandhigh-qualityend-to-endtexttospeech. arXivpreprint:2006.04558,2020.
[31] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProc.CVPR,pages10684–10695,
2022.
[32] JustinSalamon,ChristopherJacoby,andJuanPabloBello. Adatasetandtaxonomyforurban
soundresearch. InProc.ACMMultimed.,2014.
11
[33] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein.
Diffusion art or digital forgery? investigating data replication in diffusion models. arXiv
preprint:2212.03860,2022.
[34] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. Proc.
ICLR,2020.
[35] XuTan,JiaweiChen,HaoheLiu,JianCong,ChenZhang,YanqingLiu,XiWang,Yichong
Leng, Yuanhao Yi, Lei He, et al. NaturalSpeech: End-to-end text to speech synthesis with
human-levelquality. arXivpreprint:2205.04421,2022.
[36] George Tzanetakis and Perry Cook. Musical genre classification of audio signals. IEEE
Transactionsonspeechandaudioprocessing,10(5):293–302,2002.
[37] Ashish Vaswani, Noam Shazeer, and Niki Parmar et al. Attention is all you need. In Proc.
NeurIPS,2017.
[38] Ho-HsiangWu,PremSeetharaman,KundanKumar,andJuanPabloBello. Wav2clip: Learning
robustaudiorepresentationsfromclip. InProc.ICASSP,pages4563–4567.IEEE,2022.
[39] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo
Dubnov. Large-scalecontrastivelanguage-audiopretrainingwithfeaturefusionandkeyword-
to-captionaugmentation. InProc.ICASSP,pages1–5.IEEE,2023.
[40] DongchaoYang,JianweiYu,HelinWang,WenWang,ChaoWeng,YuexianZou,andDongYu.
DiffSound: Discretediffusionmodelfortext-to-soundgeneration. IEEETrans.Audio,Speech,
Lang.Process.,2023.
[41] HongyiZhang,MoustaphaCisse,YannNDauphin,andDavidLopez-Paz. Mixup: Beyond
empiricalriskminimization. Proc.ICLR,2017.
[42] JingweiZhao,GusXia,andYeWang. Beattransformer: Demixedbeatanddownbeattracking
withdilatedself-attention. Proc.ISMIR,2022.
12
Table3: Comparisonofzero-shotclassificationperformanceoftheCLAPusedinthisworkwith
previousaudio-languagecontrastivelearningmodels.
ESC-50 US8K VGGSound GTZAN
Wav2CLIP[38] 41.4 40.4 10.0 -
audioCLIP[10] 68.6 68.8 - -
CLAP(Elizaldeetal. [6]) 82.6 73.2 - 25.2
CLAP(Wuetal. [39]) 91.0 77.0 46.2 71.0
CLAP(oursonmusicdata) 90.1 80.6 46.6 71.0
A MusicLDMDetails
Hyperparameters Foraudiosignalprocessing,weusethesamplingrateof16kHztoconvertall
musicsamplesforthetrainingofMusicLDM.Eachinputdataisachunkof10.24secondsrandomly
selectedfromthedataset,i.e.,L=163840. Weusethehopsize160,thewindowsize1024,thefilter
length1024,andthenumberofmel-bins128tocomputetheshort-timeFouriertransform(STFT)
andmel-spectrograms. Astheresult,theinputmel-spectrogramhasthetimeframeT =1024and
themel-binsF =128.
We adopt a convolutional VAE as the latent audio representation model, consisting of a 4-block
downsampling encoder and a 4-block upsampling decoder. The downsampling and upsampling
rateP = 8andthelatentdimensionC = 16,i.e.,thebottlenecklatentvariabley hasashapeof
(C× T × F)=(16×128×16). Forthelatentdiffusionmodel,werefertheUNetlatentdiffusion
P P
model2. Itcontains4encoderblocks,1bottleneckblock,and4decoderblocks. Eachblockcontains
2residualCNNlayers[23]and1spatialtransformerlayer[37]. Thechanneldimensionsofencoder
blocks are 128, 256, 384, and 640 and reversed in decoder blocks. For Hifi-GAN, we adopt its
officialrepository3alongwiththeconfiguration4. Wechangethenumberofmel-binsto128tofitthe
processingofMusicLDM.
ImplementationandTrainingDetails ForthetrainingofVAE,weusetheAdamoptimizerwitha
learningrateof4.5×10−6withabatchsizeof24. Weapplythemel-spectrogramloss,adversarial
loss,andaGaussianconstraintlossasthetrainingobjectofVAE.ForthetrainingofHifi-GAN,we
usethebatchsizeof96andtheAdamWoptimizerwithβ =0.8,β =0.99atthelearningrateof
1 2
2×10−4. ForthetrainingofMusicLDM,weusethebatchsizeof24andtheAdamWoptimizerwith
thebasiclearningof3×10−5. Intheforwardprocess,weuse1000-stepofalinearnoiseschedule
fromβ =0.0015toβ =0.0195. Inthesamplingprocess,weusetheDDIM[34]samplerwith
1 1000
200steps. Weadopttheclassifier-freeguidance[14]withaguidancescalew =2.0. Whenapplying
themixupstrategy,weusethemixupratep=0.5. TheCLAPmodelistrainedon24A100GPUs.
TheVAEandHifiGANmodelaretrainedon4A60GPUs. Last,thelatentdiffusionmodelistrained
onsingleNVIDIAA40. Allmodelsareconvergedattheendofthetraining.
ImplementationofComparisonModel ForgeneratingfromRiffusionandMuBERT,weusethe
officialAPIofRiffusion5andMuBERT6.
B CLAPDetails
B.1 Hyperparameters
Formodelhyperparameters,werefertotheofficialrepository7 toconductthetrainingprocessof
CLAP. The audio encoder of CLAP is HTS-AT-base [5] and the text encoder is RoBERTa-base
[25]. TheHTS-AT-basemodelhasanembeddingdimensionof128,andawindowsizeof8. The
2https://huggingface.co/spaces/multimodalart/latentdiffusion
3https://github.com/jik876/hifi-gan
4https://github.com/jik876/hifi-gan/blob/master/config_v1.json
5https://huggingface.co/riffusion/riffusion-model-v1
6https://github.com/MubertAI/Mubert-Text-to-Music
7https://github.com/LAION-AI/CLAP
Music Tracks in the Training Set
Generated Samples from Models
Similarity=0.924 Similarity=0.935 Similarity=0.961
Figure4: ThespectrogramsofmusicpairsindicatedbyhighcosinesimiliarityscoreofCLAPaudio
embeddings.
HTS-AT-basemodelhas4groupsofswin-transformerblocks,eachgrouphasdepthof[2,2,12,2]
andnumberofheadin[4,8,16,32]. TheRoBERTa-baseconsistsofatransformermodelwith12
layers,8heads,andainnerwidthof512. Theaudioembeddingandthetextembeddinghavethe
dimensionsizeD =512.
B.2 TrainingDetails
ForthetrainingofCLAP,weusethebatchsizeof2304andtheAdam[17]optimizerwithβ =0.99,
1
β =0.9withawarm-up[9]andcosinelearningratedecayatabasiclearningrateof1×10−4.
2
B.3 Zero-shotClassificationPerformance
Wefollowpreviousworksonaudio-languagecontrastivelearning[9]toevaluatetheperformanceof
CLAPonthezero-shotaudioclassificationtasks,namelyonthebenchmarkdatasetsofESC-50[28],
UrbanSound8K[32],andVGGSound[4]. TodemonstratethattheretrainedCLAPinvolvesmore
understandingsofmusicdata,wefurtheraddamusicgenreclassificationbenchmarkdatasetGTZAN
[36]intotheevalution. AsshowninTable3,ourretainedCLAPachievesbestperformanceacoustic
eventclassificationinUrbanSound8KandVGGSounddataset,whilestillmaintainingcomparable
performance in ESC-50 dataset and on par performance in GTZAN music classification dataset.
AlthoughtheperformanceonGTZANmusicdatasetisnotimproved,theextradatausedfortraining
CLAPmightresultinabetterrepresentationspacewhichisbeneficialfortext-to-musicgeneration
model.
C Nearest-NeighborAudioSimilaritySamples
Asmentionedinsection4.2.2,weintroducedthecomputationofthenearest-neighboraudiosimilarity
ratiobycomparingthecosinesimilaritybetweengeneratedmusicandmusictracksinthetrainingset
ofAudiostock.
In this section, we provide visualizations of the similarity between the generated music and the
trainingmusicusingspectrograms,showcasinghowwellthecosinesimilaritybetweenCLAPaudio
embeddingscapturesthissimilarity.
As shown in Figure 4 and Figure 5, display both three examples of music pairs with high and
lowsimilarity. Toachievethis,wedividedthetrainingmusictracksinto10-secondsegmentsand
determinedthemostsimilarsegmenttothegeneratedmusictrack(i.e.,thequerytrack).
14
Music Tracks in the Training Set
Generated Samples from Models
Similarity=0.803 Similarity=0.813 Similarity=0.820
Figure5: ThespectrogramsofmusicpairsindicatedbylowcosinesimilarityscoreofCLAPaudio
embeddings.
Forinstanceswithhighsimilarity,thecosinesimilarityofCLAPaudioembeddingsrevealshighly
similarstructuralpatterns,indicatingacloseresemblanceinthemusicarrangements. Conversely,low
CLAPcosinesimilarityindicatessignificantdifferencesbetweenthespectrogramsofthegenerated
musicandthetrainingmusic. ThisdemonstratestheeffectivenessofCLAPembeddingsinassessing
thesimilaritybetweenmusictracksandservingasameanstodetectnoveltyandpotentialinstances
ofplagiarisminthegeneratedsamples.
D SubjectiveListeningTest
Thesubjectivelisteningtestwasconductedinanonlinesurveyformattogatherfeedbackandinsights
onthetext-to-musicgenerationusingMusicLDMsandMuBERT.ThegenerationofRiffusionwasnot
includedduetoitslowerqualityandrelevancecomparedtothestandard. Thetesthadanestimated
durationofapproximately10minutes.
Atthebeginningofthetest,participantswereaskedtoprovidetheiragerangeandmusicbackground
asmetadata. Subsequently,participantswererandomlyassignedsixgroupsofgeneratedsongs. Each
groupconsistedoffoursongsgeneratedfromMusicLDM,MusicLDMwithBAM,MusicLDMwith
BLM,andMuBERT,allbasedonthesametextualdescription. Theorderofthesongswithineach
groupwasshuffledtoeliminatepositionalbiasduringrating. Participantswererequiredtorateeach
songbasedonthreemetrics:
• Relevance: Determinehowwellthesongmatchesthegivenmusicdescription. Ratethe
songbasedonhowcloselyitalignswiththeprovideddescription.
• Quality: Assesstheoverallqualityofthemusic. Considerfactorssuchasclarity,absenceof
noise,andgeneralaudioquality. Ratethesongbasedonthesecriteria.
• Musicality: Evaluatethemusicalattributesofthesong,includingrhythm,melodies,and
textures. Ratethesongbasedonitsoverallmusicalappeal.
Eachsonginthesubjectivelisteningtesthadadurationofapproximately10secondsandincluded
afade-inandfade-outtomitigatebiasfromthesong’sbeginningandendingsections. Therating
scaleusedforevaluatingthesongswasdesignedsuchthatahigherscoreindicatesbetterquality.
Participantswereaskedtorateeachsongbasedontheprovidedmetrics, takingintoaccountthe
song’soverallquality,relevancetothegiventext,andpersonalpreferenceonitsmusicality.
15
E BroaderImpact
ThedevelopmentandimplementationofMusicLDM,orgenerallyatext-to-musicgenerationmodel
offerspotentialbenefitsandalsoraisesconcernsthatmustbeaddressedresponsibly.
PositiveImpacts
• PromotingCreativity: Thismodelcanserveasatooltoaugmenthumancreativity. Artists,
composers,andmusicamateurscanuseittotransfertheirtextualideasintomusic,broad-
ening the realm of artistic exploration and making music creation more accessible and
convenient.
• Cultural Preservation and Evolution: The model provides a unique platform to archive,
digitize,andevenevolveculturalmusicalexpressions. Textualdescriptionsoftraditional
andfolkmusiccanbetransformedintotheactualmusic,therebyhelpingtopreserveheritage
whilesimultaneouslyallowingforcreativeadaptations. Literature,suchaspoetry,canbe
interpretedasmusictoexploremorerelationsbetweendifferenttypesofculturalexpression
forms.
• Education and Research: In academia, this model can be used as a pedagogical tool in
musiceducation. Itcanaidinunderstandingthecomplexrelationshipbetweenmusicand
linguistic structures, enriching interdisciplinary research in musicology, linguistics, and
artificialintelligence.
• Entertainment Industry Innovation: The entertainment industry could use this model to
generate soundtracks for movies, games, and other media based on scripts. This could
potentiallyrevolutionizethewaymusicisproducedformedia,reducingtimeandcosts.
NegativeImpacts
• ArtisticJobDisplacement: Whilethismodelcanaugmenthumancreativity, itmayalso
leadtojoblossesinthemusicindustryifwidelyadoptedforcomposingandproduction.
Themodelcouldpotentiallyreplacehumancomposersincertaincontexts,particularlyin
industriessuchasfilmandgamingthatrequireasignificantamountofbackgroundmusic.
• CopyrightIssues: Inthispaper,oneofthetargetsistomitigatethecopyrightissuesand
plagiarism. Thegeneratedmusiccouldunintentionallyresembleexistingworks,raising
complexcopyrightinfringementissues. Itiscrucialtoimplementmeasurestoensurethat
themodeldoesnotviolateintellectualpropertyrights.
• Ethical Misuse: The model could be misused to create music promoting hate speech,
misinformation,orotherharmfulcontentiftheinputtexthassuchcharacteristics. Thus,it
isessentialtodevelopsafeguardstomitigatetheriskofmisuse.
• CulturalAppropriationandHomogenization: Whilethemodelcanhelppreservemusic,
thereisariskofhomogenizinguniqueculturalmusicalstylesormisappropriatingthem
withoutpropercreditorcontext.
Thedesignandapplicationofthismodelshouldbecarriedoutresponsibly,consideringthepotential
ethical,social,andeconomicconsequences. Balancingitsmanybenefitswithitspotentialdownsides
willrequirethecollectiveeffortofdevelopers,users,policymakers,andsocietyatlarge.
16

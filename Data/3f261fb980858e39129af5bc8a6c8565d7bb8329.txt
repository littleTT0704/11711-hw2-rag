This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.
Digital Object Identifier 10.1109/ACCESS.2023.Doi Number
Exploring Stigmergic Collaboration and Task
Modularity through an Expert Crowdsourcing
Annotation System: The Case of Storm
Phenomena in the Euro-Atlantic Region
DENNIS PAULINO1, ANTÓNIO CORREIA1,2, MARCELA MAYUMI MAURICIO YAGUI3,
JOÃO BARROSO1, MARGARIDA L. R. LIBERATO4, ADRIANA S. VIVACQUA3,
ANDREA GROVER2, JEFFREY P. BIGHAM5, AND HUGO PAREDES1
1 INESC TEC and University of Trás-os-Montes e Alto Douro, Vila Real, Portugal
2 University of Nebraska at Omaha, Omaha, NE, United States
3 Federal University of Rio de Janeiro, Rio de Janeiro, Brazil
4 IDL and University of Trás-os-Montes e Alto Douro, Vila Real, Portugal
5 Carnegie Mellon University, Pittsburgh, PA, United States
Corresponding author: Dennis Paulino (dpaulino@utad.pt)
This work is financed by the Portuguese Foundation for Science and Technology (Fundação para a Ciência e a Tecnologia – FCT) with the research grant
SFRH/BD/148991/2019. The authors also acknowledge support from the European Social Fund under the scope of North Portugal Regional Operational
Programme.
ABSTRACT Extreme weather events, such as windstorms, hurricanes, and heat waves, exert a significant
impact on global natural catastrophes and pose substantial challenges for weather forecasting systems. To
enhance the accuracy and preparedness for extreme weather events, this study explores the potential of
using expert crowdsourcing in storm forecasting research through the application of stigmergic
collaboration. We present the development and implementation of an expert Crowdsourcing for Semantic
Annotation of Atmospheric Phenomena (eCSAAP) system, designed to leverage the collective knowledge
and experience of meteorological experts. Through a participatory co-creation process, we iteratively
developed a web-based annotation tool capable of capturing multi-faceted insights from weather data and
generating visualizations for expert crowdsourcing campaigns. In this context, this article investigates the
intrinsic coordination among experts engaged in crowdsourcing tasks focused on the semantic annotation of
extreme weather events. The study brings insights about the behavior of expert crowds by considering the
cognitive biases and highlighting the impact of existing annotations on the quality of data gathered from the
crowd and the collective knowledge generated. The insights regarding the crowdsourcing dynamics,
particularly stigmergy, offer a promising starting point for utilizing stigmergic collaboration as an effective
coordination mechanism for weather experts in crowdsourcing platforms but also in other domains
requiring expertise-driven collective intelligence.
INDEX TERMS Atmospheric phenomena, cognitive biases, crisis informatics, expert crowdsourcing,
extreme meteorological events, semantic annotation, stigmergy, storms, task modularity, weather maps.
I. INTRODUCTION profound socio-economic impacts, increasing economic
Withstanding unpredictable weather events is one of the losses, agriculture production shortfalls, and human
biggest perennial challenges facing community ecology [1]. mortality [3, 4]. These factors, in tandem with the
The harsh impacts of severe meteorological events such as magnitude, frequency, and duration of these extreme
extreme windy conditions and precipitation extremes are events, have prompted a call for collective action to enable
increasingly being felt and pose serious threats to early detection of abnormal weather conditions in a way
ecosystem functioning [2]. Such extreme weather that can be useful to prevent potential hazards across the
phenomena resulting from climate change can inflict globe [5]. Multi-hazard early warning systems expose
VOLUME XX, 2017 1
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
significant weaknesses related to the uncertain, multifaceted process while reducing manual effort, with promising
and unexpected nature of severe hazards caused by weather results in the context of sound event annotation. Therefore,
extremes [6]. Since these events are exceptions to well- we believe that the involvement of multiple researchers
known weather patterns, they are not forecasted with expertise in climate-related application domains can
appropriately. As climate change accelerates and contribute to a richer and more efficient analysis of
catastrophic weather events become more frequent, intense, adversarial weather phenomena when working
and unpredictable [7], innovative ways must be found to collaboratively. Crowdsourcing is often used as an
improve weather pattern detection for extreme weather engagement solution, allowing an efficient usage of human
events around the world. computation [22]. Muller and co-authors [23], referring to
Despite the utter importance of crisis informatics to the Wechsler [24], briefly explained the applicability of this
human-computer interaction (HCI) research community [8- approach in climate and atmospheric sciences as follows:
10], many aspects of crisis preparedness, extreme weather “Crowdsourcing as a research field has great potential to
warning response, and post-disaster recovery through bridge the gap between the social scientists, computer
collaborative and social computing have not yet been scientists and physical and environmental scientists, thereby
investigated intensively. As outlined above, turning weather encouraging interdisciplinary working and enhancing
observations (e.g., annotations) provided by human knowledge exchange and scientific discovery”.
contributors into reusable data that can be further In this paper, we examine the application of crowdsourcing
interpreted by weather forecast systems constitutes a long- in climate and climate change research, through expert
standing problem [11]. There is much accumulated crowd annotation of atmospheric phenomena, following
evidence that both expert and non-expert crowds can be similar approaches such domains as oceanography [25]. In
reliable sources of observational data in the form of particular, our work asks whether expert crowdsourcing is
semantic annotations of image fragments [12], even though effective for detecting extreme weather events, in tandem
the accuracy and inter-annotator agreement varies with the aim of exploring the potential effects of task
depending on the task structure [13]. modularity [14]. Furthermore, we also report data involving
That brings us to the issue of task modularity [14] to reduce stigmergy, a form of implicit coordination in which
the coordination burden. Typically, conventional microtask influence is exerted by indirect interactions via a self-
crowdsourcing platforms leverage non-expert crowd organizing environment based on digital traces left by
workers to perform such tasks based on a random task participants without apparent communication or separate
distribution strategy [15]. However, analyzing extreme articulation work [26, 27]. The idea here is that since social
weather events is not a simple activity that can be easily coordination is particularly challenging to manage in
decoupled from the expertise of specialists like uncertain scenarios [28], expert crowds can collaboratively
climatologists and atmospheric researchers. With the tackle complex problems such as extreme weather event
increasing global concern for the complex environmental detection and climate forecasting by adjusting behaviors
issues associated with the effects of extreme weather and based on past contributions provided by other members. As
climate events, the field of Big Data has an important role Bolici et al. [26] put it, a stigmergic approach considers the
to play in supporting the development of technologies for shared context itself, including material artifacts that are
extreme weather detection and prediction, based on its part of the environment, as a coordination mechanism. This
strong tradition of investigating situated work practices allows a sharing of lessons learned and a common design
from a multidisciplinary perspective [16, 17]. This is ground, which is in line with a central question for the
accentuated by the recent evidence that making predictions computer-supported cooperative work (CSCW) research
about the future occurrence of extreme event phenomena community concerning the best ways of promptly and
based on historical data (i.e., the frequency of extreme accurately obtaining crowdsourced annotations [29] and
events occurred in the past) fall short in supporting desired how they are collaboratively achieved in everyday practice.
levels of effectiveness [18]. Through a crowdsourcing process, with tools developed for
Niforatos and colleagues [19] suggested that a human-in- mapping the collaborative process in the crowdsourcing
the-loop approach can be particularly useful to estimate framework, climate experts were able to participate in
weather conditions at multiple points while training online tasks following a collaborative problem-solving
machine learning algorithms into a dynamic process that approach. The potential advantages of leveraging human
goes beyond the common conception of human oversight factors in weather forecasting systems include improving
that has been widely used in the field of artificial their accuracy and reliability by incorporating knowledge
intelligence [20]. Evidence of such a process in a slightly and experience from experts. This knowledge can be then
different domain has recently been reported by Kim and turned into a set of actionable insights that are left in the
Pardo [21], who claimed that algorithmic sociotechnical environment to support shared awareness while providing
systems leveraged by human-in-the-loop interactions can non-professionals a better understanding of weather
progressively improve the speed and quality of the labeling phenomena as informed by experts. In line with this, our
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
study aims to address research gaps in this area and separately but in fact they are triggered in part by the signs
contribute to a better characterization of how expert crowds left by other members in the shared environment without
can be effectively coordinated to enhance weather control of each other [34]. This results in a complex network
forecasting systems. constituted by a long-term shared external working memory
The remainder of the paper is structured as follows. In in which the users are attracted to participate and build upon
section 2 we recapitulate the related literature on stigmergic an initial concept or idea as a form of collective intelligence
collaboration, citizen science, and expert crowdsourcing, [26, 35-37]. Crowston and Rezgui [30] found a positive
with an emphasis on the advantages of semantic annotation influence of non-explicit, stigmergic coordination on the
afforded by web-enabled mass collaboration. In section 3 quality of a set of articles produced in Wikipedia. In such
we describe the characteristics of the crowdsourcing settings, task interdependence and worker roles do not affect
platform created for supporting the annotation of extreme the success of collaboration [21].
weather events. In this section, we also formally model the In addition, cognitive biases may also affect stigmergy-
system components, alongside a brief description of the based approaches and thus cause individuals to make
user interface and related tools used in our study. Section 4 mistakes without being aware of them. From a conceptual
describes the data collection and processing procedures, level, cognitive biases can be understood as the systematic
outlining in detail the methods used to analyze the production of distorted representations that are detached from
qualitative data collected. In section 5, we present the reality by an individual’s own cognition [38]. As the
experimental results obtained from real-world data. These technologies enabling crowdsourcing options have evolved
findings compare the data obtained from the annotation and matured over the years, a body of evidence has accrued
experiments and investigate the relative importance of indicating that the quality of tasks can significantly decrease
stigmergy and task modularity on the general outputs when cognitive biases are not taken into account in the task
provided by experts, with focus on the agreement among design process [39]. Among many types of cognitive biases
participants as a measure of collective intelligence. In that can influence crowd behavior, the Dunning-Kruger
section 6, we discuss these findings in light of previous effect [40] occurs when less skilled individuals who perform
studies with a view to the current uses of the platform. This at levels below expectations on a particular task tend to
section also discusses some limitations of our research and overestimate their performance. For instance, a study based
brings out lessons that have been learned during this study on input aggregation methods for crowdsourcing systems
in the form of implications for design. We close with a [41] demonstrated that the general performance of the
review of our findings and a proposal for further research. aggregation methods achieved better results when the
Dunning-Kruger effect was taken into consideration. In
related work, Gadiraju et al. [42] examined a set of crowd
II. BACKGROUND AND RELATED WORK workers’ self-assessments and found that less diligent
workers tended to be more affected by this type of bias. As a
A. The Metaphor of Stigmergic Collaboration result, we believe that stigmergic processes could benefit
Stigmergy in crowd work relies on tracking changes on the from assessment for individuals’ cognitive biases to improve
work activity under execution using digital traces as guidance the collaboration outcomes that may be achieved in this form
mechanisms for allowing other members to contribute based of implicit interaction. Further, the Bandwagon effect [43]
on past behavior [30]. In many ways, it is analogous to the can also be particularly relevant in a stigmergy-based
trails left by insects as a self-organizing, co-evolving scenario.
approach in which the previously performed work plays a
critical role by acting as an indirect or even spontaneous B. Citizen Science and Expert Crowd Work for Weather
coordination mechanism [24, 31]. This is in line with Bolici Event Detection
and associates [26], who claimed that implicit coordination is Crowd science, networked science, massively collaborative
“reached without discursive communication, shared plans or science, or simply citizen science are types of crowdsourcing
even previous commitment among the actors”. In this kind of that encourage public participation in doing scientific
scenario, both the shared artifact and the environment act as a research, for example, by contributing observations,
source of stimuli to the ensuing interactions in situated historical records, and collection of samples and scientific
actions that are mainly characterized by ad hocness [32]. In evidence [44]. Citizen science encourages concerned people
[33], for example, a stigmergic system approach was to actively participate in a real science project alongside
introduced to support crowd-powered decision-making and professional researchers [45]. Citizen science projects usually
reasoning tasks with emphasis on aiding the work of enable different levels of crowd participation based on the
intelligence analysts without the use of explicit forms of project goals [46]. In the literature, many studies applying a
coordination. citizen science strategy have been published in the natural
Another well-known example of this stigmergic approach sciences [47]. For instance, current initiatives promote
is Wikipedia [30], where contributors seem to be working
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
citizens’ engagement in addressing climate change through A few studies have attempted to use stigmergic principles in
the usage of games and social networks [48]. the study of crowdsourcing for annotating atmospheric
Regarding task complexity, Franzoni and Sauermann [44] phenomena. In crowdsourcing for identifying the formation
claims that interconnection among tasks increases the degree of strong tropical cyclones, Barbosu and Gans [66] examined
of complexity and Staffelbach, Sempolinski [49] argues that the use of Cyclone Center1, a Zooniverse project that recruits
task complexity is due to the technical knowledge that the volunteers for analyzing global hurricane records using
task requires. In [50], the authors relate task complexity to satellite imagery. In a similar vein, Hennon et al. [67] also
the degree of collaboration among participants. In [51] explored the potential of using this crowdsourcing system for
created a method to identify profiles and relate them to the identifying patterns in tropical cyclone analysis and
most appropriate tasks, while Cullina and co-authors [52] forecasting. In [68], the authors applied crowdsourcing for
proposed a sub-process to select the best crowd for analyzing air temperature data (e.g., cold and heat anomalies)
crowdsourcing projects. In [53], the authors studied the retrieved from Internet-connected weather stations. In an
reliability and cost of approaches such as ‘majority decision’ entirely different approach, citizen science was utilized to
and ‘control group’ for validating crowd contributions. We involve the broader public in providing volunteer computing
also note the work of De Beer et al. [54] on the ethical issues resources for supporting climate studies [69]. In another
related to the intellectual property of data generated by citizen science project2, volunteers are invited to transcribe
crowdsourcing systems from the perspective of the meteorological readings on past environmental conditions
organizations carrying out crowdsourcing activities as [70]. Crowdsourcing transcripts of old atmospheric records
requesters. was also explored within a traceable citizen science-based
environment and climate database [71].
1) EXPERT CROWDS VS. AMATEURS (NON-
PROFESSIONALS) Data from a variety of additional citizen science projects,
Some studies have considered the characteristics of are increasingly incorporated into standard data sets used in
contributors and their interactions for assessing the quality of atmospheric sciences, with volunteers gathering
an artifact [55]. That is, the interactions among crowd observational data for certain meteorological parameters. For
members are inherently contextual and situated [56], which example, the Weather Observation Website3 was initiated as
influences the general quality of the crowd work product. A a citizen science project intended to crowdsource weather
mismatch in the quality of volunteer contributions can occur data from private observers with the aim of building up a
when the views of professional experts (scientists) differ record of weather observations [72]. Researchers have also
from non-experts and vice-versa [57, 58]. Complex tasks examined the amateur weather observation practices and
must be assigned to an expert taking into consideration their experiences from a sociotechnical viewpoint [72]. For a
particular skills and abilities [27], despite multiple barriers to detailed overview of previous work on the challenges and
high-quality expert-derived annotations, particularly as possibilities of using crowdsourcing for meteorological
reliance on expert contributors is time-consuming and studies, we refer the reader to [23].
expensive [59].
On the other hand, non-experts may experience more C. Semantic Annotation via Crowdsourcing
difficulties in performing advanced complex tasks, Research has shown that semantic annotations can facilitate
potentially due to lack of field-specific expertise. the process of attaching additional information to various
Increasingly, research comparing the performance of concepts so that algorithms can automatically process and
volunteers (also known as citizen scientists) against the work interpret them effectively, using structured metadata
done by scientific experts [60] has pushed the boundaries in available on the web to associate terms or expressions in a
this direction. In particular, O’Leary and colleagues [61] document with an instance of an ontology [73, 74]. In a
tested the validity of using non-experts for collecting recent study by Hughes et al. [75], a crowd-powered tool
phenotypic data. Current works have similarly evaluated the called Quanti.us leveraged the wisdom of untrained crowd
performance of domain experts, crowd workers, and workers for semantically annotating scientific images.
automated techniques (e.g., active learning) for sleep spindle Assessments confirmed similar performance for broader
detection [62] and finite-pool data categorization of datasets public and expert-derived annotations in terms of precision
in biomedical systematic reviews [63]. Moreover, Law and and recall, with a slightly higher accuracy rate for experts on
co-authors [64] also identified some barriers associated with more complex annotation tasks. For semantic annotation
the hesitancy of experts when using data collected from tasks, some works (e.g., [76-78]) were developed with the
citizen scientists. In this regard, [65] emphasized recursive objective of annotating metadata on maps via crowdsourcing
problems such as the absence of a shared disciplinary techniques. Kaufmann et al. [77] proposed the creation of an
background and lack of trust when considering the
perceptions of crowd bias and data quality.
1 https://www.cyclonecenter.org/
2) CROWDSOURCING ATMOSPHERIC DATA & DATA
2 https://www.oldweather.org/
ANALYSIS.
3 https://wow.metoffice.gov.uk/
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
application, called Climate Twins, that simulates the future studies (e.g., [29]). Another system requirement was to
climate of some regions of Europe, based on historical series provide a mechanism to enable the visualization of multi-
of temperature and precipitation. faceted insights from automatically generated weather maps,
Semantic annotations can also be combined with other which is highly desirable so that the meteorological
semantic web techniques such as Linked Open Data (LOD) parameters can be defined for each task. As a result, the work
[79] to facilitate the publication of structured data on the units that are crowdsourced must be modular in a way that
Web and the creation of connections between heterogeneous simplifies keeping them compliant with the flow of
data sources. For example, the work of Santos and Furtado information, interactions, and collaboration while preserving
[78] relies on the creation of annotated maps based on stakeholder privacy and security. Based on the requirements
content generated by virtual crowds, where users can make elicited during the participatory design activity, we designed
markings on maps and describe events. Work by Gonzalez a framework to annotate weather events. At first glance, the
and colleagues [76] has also proved the applicability of eCSAAP-Service Oriented Framework for Weather
crowdsourcing to produce LOD datasets using map Annotation (eCSAAP-SOF4WA) encloses four functional
annotations[80]. The authors tested their proof-of-concept blocks (Figure 1): Weather Center API, eCSAAP System,
approach in the geospatial domain. In such scenarios, the Extreme Weather Events Annotator, and Crowdsourcing
descriptions can be obtained from the LOD cloud [81] where Platform. The functional blocks are organized in order to
the curated information on the topics is made available. In a provide an end-to-end approach to classify weather
different domain, Callaghan et al. [82] explored the use of phenomena, gathering weather data from international
crowdsourcing for annotating heart sounds with excellent databases, generating experts-oriented visualizations and
classification performance when hybrid machine-crowd attaching semantic fields to the collected data, and creating
classifiers are used. tailored crowdsourcing campaigns for weather annotation.
III. Technical Setup
In this section, we draw on the recent success of crowd-
powered systems to support annotation tasks (e.g., [29, 83,
84]) to introduce the expert Crowdsourcing for Semantic
Annotation of Atmospheric Phenomena (eCSAAP) tool, a
technical solution to the problem of finding patterns in
weather data for forecasting. This section discusses
elicitation of requirements and associated system features.
The development and design phases followed a FIGURE 1. Overview of eCSAAP-SOF4WA.
participatory design approach [85] to understand
requirements of atmospheric specialists, meteorologists, and A. Weather Center API
climate scientists when collecting data about extreme The Weather Center API provides weather data to the
weather phenomena. During this process, we talked with eCSAAP system by abstracting the data sources. Several
experts in climatology, working in the domain of extreme weather data sources currently provide weather data gathered
events (i.e., extratropical cyclones), about the types of directly from weather sensors or through reanalysis. The
weather data they usually work with and the way they current implementation of the eCSAAP-SOF4WA gathers
interact with these data, including possible barriers and reanalysis data from the European Centre for Medium-Range
limitations with these arrangements. We adopted an iterative- Weather Forecasts (ECMWF)4. Such data are publicly
incremental development strategy based on a co-creation available at a variety of scales, derived from a diverse set of
process, where requirements evolved through the interaction meteorological observations that enable us to support
and direct feedback over a 9-month development period weather forecasting activities based on the current state of the
(March – November 2019). This process informed the design atmosphere.
of a prototype “from the ground up” [86] as opposed to using
an existing tool. The goal of this process was to easily B. eCSAAP System
modify each feature every time a change was required in the From a sociotechnical perspective, the eCSAAP system can
sequence of the feedback received by expert participants. be used by experts and non-professionals (citizen scientists,
To support the needs of climate specialists, we required a weather enthusiasts, students) worldwide to generate
web-based tool that would adequately provide the output data visualizations from atmospheric data [omitted for blind
in the form of marks (annotations) at specified locations as review]. Interoperability was taken into account in the
rich content-based descriptions of the observed weather deployment of the eCSAAP system through development of
phenomena over climatological time spans. This imposed an two components: a backend (management of requests to the
additional requirement that the platform should be developed
to support overlapping annotations, as also shown in earlier
4 https://www.ecmwf.int/
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
database and production of meteorological views) and a which can be projected in the form of colored polygons or
frontend that consists of an interface for users to interact with polylines (Label 1 in Figure 2).
the backend. The backend was built using Django5, a Python- The geometric object style (e.g., color or line style) can be
based framework for developing a Restful API, which works customized to show more information with a popup
with map libraries such as Matplotlib6 and makes the presenting the value of the weather metric (Label 2 in Figure
generation of visualization maps easier. The main features of 2). Annotations can be created in two ways: by clicking on a
the backend are briefly described below: geometric object to choose the rectangular area (Label 3 in
• Collection of meteorological data from databases. Figure 2) or by manually drawing a rectangle in the desired
A set of parameters such as the space-time interval area (Label 4 in Figure 2). Each annotation can be edited
and meteorological metrics are sent to an API. The (Label 5 in Figure 2) by choosing the vertices of the
data returned is part of reanalysis, which allows a rectangle to change, or deleted (Label 6 in Figure 2) by
consistent global view for metrics such as wind and removing a rectangle.
mean sea level pressure [87]. Weather data is The tool includes a labels bar (Label 7 in Figure 2), where
collected using the Climate Data Store API7, different color labels are presented and can be chosen to
implemented by the ECMWF.
represent different annotations (e.g., if a weather map has
• Generation of weather maps and visualization data. two storms, it should be annotated with two different labels).
The collected weather data are represented on two-
A checklist with the metric values is included, where the
dimensional maps, which are projected on a world
geometric objects of the weather metric can be filtered by
map. When creating each map, visualization data is
their respective values (Label 8 in Figure 2). The annotator
stored (i.e., geometric objects coordinates
tool can load a panoply of maps and it is possible to choose
representing a weather phenomenon). This
the map to be displayed (Label 9 in Figure 2). The
visualization data is available as a resource through
annotations are saved automatically to the map represented,
the backend API, and is used by the Extreme
so that it is possible to return to a previous map and load the
Weather Events Annotator described in Section 3.3
respective annotations. The feature of navigating through a
to create interactive maps.
series of maps can be used to upkeep with the evolution of a
• Crowdsourcing task management. This feature
meteorological phenomenon.
allows users to create crowdsourcing campaigns by
making HTTP requests to a crowdsourcing platform
API. When a task is created, it is associated with
multiple maps, allowing the monitoring of the
evolution of a meteorological phenomenon such as a
storm. After the campaign is finished in the
crowdsourcing platform, then the results can be
stored for further analysis.
Weather experts can use the backend through a user-
friendly frontend. The frontend was developed with React8,
which facilitates the consumption of APIs. The weather
experts can do every task available through the backend,
from generating weather maps to creating crowdsourcing
campaigns for weather annotation.
C. Extreme Weather Events Annotator
A web-based annotator allows users to create semantic
annotations of extreme weather events, as shown in Figure 2.
The annotator was developed in HTML5 and Javascript to
facilitate the integration of the task interface with the
crowdsourcing platform. The task interface consists mainly FIGURE 2. User interface of the Extreme Weather Events Annotator.
of an interactive Leaflet9 map where it is possible to draw
Additional annotations made by previous crowd workers
various geometric objects. In this map, there are overlayers
can also be displayed in the annotator tool. Such annotations
that represent the visualization of meteorological metrics,
are represented in dashed lines (Label 10 in Figure 2) and
cover the whole or part of the weather geometric object. For
5 https://www.djangoproject.com/
determining whether the other crowd worker annotation is
6 https://matplotlib.org/ represented, a threshold of 50% was defined, based on the
7 https://cds.climate.copernicus.eu/ weather geometric object’s perimeter covered by the
8 https://reactjs.org/
9 https://leafletjs.com/
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
annotation. The reason for the definition of this threshold was The study followed an experimental design based on the
two-fold: following questions:
• If the threshold was defined as 0%, it would allow
the representation of any other crowd worker’s • Individual vs collective annotations. Can collective
annotation even if just covered a small portion of annotations take advantage of individual analysis of
the weather geometric object. The consequences of weather data?
this option would be the appearing of false positives • Influence of the crowd. How does a crowd worker’s
on extreme weather events annotated by the other annotation influence the following ones? The
crowd workers in the interactive map; conditions for the evaluation are the display of: no
• If the threshold was established as 100%, the crowd worker annotations, real crowd worker
annotations provided by other crowd workers would annotations, simulated positive annotations, or
not be represented because it did not cover the simulated negative annotations.
whole perimeter of the weather geometric object.
This would cause the appearance of many true For each of the four distinct research conditions a task was
negative extreme weather events that could be not created that consisted of annotating the evolution of weather
annotated by other crowd workers. storms. Users were presented with three weather maps that
represent events with 6-hour intervals between them (e.g., if
D. Crowdsourcing Platform the first map showed time 0 UTC, the second would be an
For the presentation of crowdsourcing campaigns to the
image taken at 6 UTC and the third one at 12 UTC). The task
crowd workers, a platform that could host the annotator tool
was differentiated from the other crowd worker annotations
and integrate with the eCSAAP system was necessary. Thus,
in order to observe the behaviors when the crowd worker is
we selected the PYBOSSA10 crowdsourcing platform, as it
able to view the results of the other crowd workers,
enables the implementation of a task presenter adjustable for
specifically studying the Bandwagon effect, which is a
tailored crowdsourcing campaigns, which is important to
cognitive bias, on the stigmergic process [43]. Each type of
enhance the interface shown to the crowd worker. This
task is presented below:
platform comes with an API that allows its integration for
managing the tasks and importing the results. In addition,
• Task 1. Simulated negative annotations. In this task,
PYBOSSA gives support to enrich the task presented to the
annotations are displayed where no storm is
crowd worker, including a built-in mechanism for passing happening (map is incorrectly annotated). The
task information to the task presenter. Furthermore, it is crowd worker may be induced to follow precedent,
possible to retrieve the finished tasks results during the task thus marking storms where they are not occurring.
run. • Task 2. No crowd worker annotations. The crowd
As shown in Figure 1, the task is added in the worker finds the weather map without any previous
crowdsourcing platform by the eCSAAP system. For the annotations, so there should be no influence upon
implementation of the task presenter in the crowdsourcing the crowd worker’s annotations.
platform, the Extreme Weather Events Annotator is used. • Task 3. Simulated positive annotations. A task is
When a task is presented to a crowd worker, the visualization presented to the crowd worker with results that
data is fetched from the eCSAAP system and loaded to the indicate annotations corresponding to extreme
interactive map of the annotator tool. Moreover, if another weather events (map is correctly annotated). This
crowd worker has done the same task, it also includes his/her should help the crowd worker to annotate actual
annotations so that the current crowd worker can observe the storm events more effectively. In this condition, real
annotations made by other users. crowdsourcing annotations from other volunteers
are not shown.
IV. Research Method • Task 4. Real crowd worker annotations. The map
In order to evaluate the influence of stigmergy for weather presents annotations provided by other users. This
annotations, we conducted a study on the eCSAAP- condition could influence the crowd worker toward
SOF4WA. The goal of the study was to identify the work making either true or false annotations, depending
on what other users have previously generated.
done by experts when annotating atmospheric phenomena
and, more specifically, the effect of stigmergic collaboration
The traces resulting from participant contributions towards
in crowdsourcing. In this section, we describe the tasks
the task outcome were recorded. Such traces help us track the
proposed, the data collected, and our approach to data
work on the shared output of the project and thus can be used
analysis.
as a valid measure of stigmergy [88]. For the case study, the
objective of each task was to annotate the storms on the
A. Experimental Design
weather metric of Mean Sea Level Pressure (MSLP). The
MSLP is typically represented in charts as lines, and each
10 https://pybossa.com/
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
line has an equal pressure value [89]. An explosive • ‘Gong’ Storm (18-22 January 2013) [90]
cyclogenesis occurs if there is a rapid pressure drop [90].
• ‘Klaus’ Storm (23-28 January 2009) [91]
Furthermore, many lines close to each other may indicate a
rapid pressure drop. The weather map presented in the task is Each task consisted of one storm, during its peak stage.
composed of MSLP lines. Storms were previously identified with the support of surface
analysis data from the UK Met Office13, that included the
B. Procedure locations of the weather fronts, low- and high-pressure
The target participants for the study were weather experts, systems. The assessment included annotations of regions that
including climate and weather scientists and enthusiasts. could include storms, specified with the help of a weather
Enthusiasts are a special group that includes professionals expert, which analyzed this category of storms [90, 91]. The
with advanced knowledge of weather phenomena (e.g., map for annotation of the storms was created to show not
aircraft pilots) and individuals whose hobbies require similar only the storm itself, but to include regions where the MSLP
weather knowledge, which include but are not limited to could indicate potential storms. This was included to widen
paragliding pilots, surfers, and natural life photographers. the spectrum of possible annotations by the crowd workers,
Recruitment was conducted by sending an email invitation to who could potentially identify a multitude of storms. The
target groups mailing lists. The invitation contained the annotation is represented with a bounding box (an annotation
explanation of the study, and how to proceed to enroll in the made with a rectangle) covering the storm area on a map
experiment, and access the pre-task questionnaire, the image.
crowdsourcing platform, and the post-task questionnaire. The evaluation for this study focused on assessing group
A pre-task questionnaire was used to understand the consensus simple majority, a quality measure in
participants’ profile, including the proficiency in performing crowdsourcing [92], by generating labels from the data based
online tasks and previous working background related to the on the most representative answer from the crowd. We
climate field. These two questions were presented on a 4- analyzed the effects of stigmergy on the quality of weather
point Likert scale ranging from strong agreement to strong data annotations through a comparison of annotations done
disagreement with each item. In the crowdsourcing platform, with or without awareness of past contributions provided by
participants were given instructions for annotating events in a the other participants.
weather task which included a video with duration under 4 Aggregation of crowd work can be done through visual
minutes that was displayed on the crowdsourcing platform in annotations with effective results [93]. In this regard,
order to briefly explain both how the interface works when Oosterman and colleagues evaluated the group consensus
creating annotations, and how to interpret the MSLP for the when labeling artwork images using bounding boxes. The
identification of storms. After watching the video, the aggregation method comprised three steps:
participants could start performing the task. Each task 1. Preprocessing. The bounding boxes that were not
consisted of three weather maps, and participants were asked precise were removed (area larger than 3 standard
to track the evolution of storms. deviations of the mean value of all bounding
The participants were instructed to annotate multiple boxes). This step is useful because one bounding
storms that could be presented on a single weather map, and box could have a high coverage of the pretended
to identify the same storm in the following weather maps of observation, but would lose quality if the
that task. There was no time limit and the campaign included proportion is too high.
a step-by-step tutorial that highlighted and explained each 2. Clustering. The bounding boxes were grouped into
clusters based on overlap. This step directs each
task component which could be used as necessary. After the
annotation in a cluster to the same visualization
completion of the task, the participants filled in the post-task
target. Clustering techniques like the K-means and
questionnaire described next.
the Gaussian Mixture Model were used for
clustering.
C. Assessment
3. Aggregation. For each cluster, the aggregation
The tasks consisted respectively in annotating maps of the
result can be obtained by making the derivation of
following storms, represented in the weather metric of MSLP
each cluster using the maximum or the median
(these named storms occurred in the Euro-Atlantic region
values.
and drew adequate media coverage to be familiar to most
Taking into consideration that crowd workers can
participants):
introduce noise to the data (bias), clustering can be a feasible
• ‘Ana’ Storm (10–14 December 2017)11 path to infer ground truth values [94]. The case study
• ‘Felix’ Storm (9-16 March 2018)12
12https://en.wikipedia.org/wiki/2017%E2%80%9318_European_win
dstorm_season#Storm_Felix
11https://www.eumetsat.int/website/home/Images/ImageLibrary/D 13https://www.metoffice.gov.uk/weather/maps-and-charts/surface-
AT_3759905.html pressure
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
proposed in this article followed the methodology defined by
Oosterman et al. [93] to aggregate crowd workers’ results.
Nonetheless, we adopted the Density-Based Spatial
Clustering of Applications with Noise (DBSCAN) technique
for clustering, which is recommended for clustering spatial
domains [95]. The maximum distance between points to be
considered inside the clusters is a necessary metric for
applying the DBSCAN method. Maximum distance was set
to 3 degrees (in the geographic coordinate system) taking
into consideration the wide dimension of the domain for
FIGURE 3. Technical flowchart of the research method.
annotations covering the entire Euro Atlantic region.
Task evaluation was measured with the ratio of area
V. Results
selected by the participant and the median bounding box
In this section, we report both quantitative and qualitative
from the group consensus clusters. The quality measure used
results of the experiments. Furthermore, we investigate the
for this evaluation was the F-score [96], which also has been
relative importance of stigmergy and task modularity on the
applied to measure the effectiveness of annotations for
general outputs provided by experts.
evaluation of crowdsourcing tasks [29]. The F-score
In this study there were 13 participants, with an average
compares the area coverage of the annotation against the
age of 42.14 (SD=14.98). Twelve participants had working
ground truth, taking a proportion of the area drawn. This
experience in the climate field, varying from weather experts
metric allows us to evaluate the effectiveness and precision
that research atmospheric phenomena to people that assisted
of annotation. Effectiveness relates to the coverage of the
with seasonal forecasting. One participant was an enthusiast
median cluster bounding box and precision to the proportion
of the field, being interested in weather forecasting due to a
(e.g., if an annotation covers the whole cluster median
related hobby (i.e., paragliding). Figure 4 presents a chart
bounding box but is a lot larger, it has less precision and,
with the answers to the pre-task questionnaire related to the
consequently, lower quality). A log with the participants’
proficiency in performing online tasks. As it can be observed
actions was kept, recording where the user clicked while
from Figure 4, most of the participants stated that they were
completing the task.
proficient at performing online tasks.
To provide a qualitative perception, a post-task
questionnaire debrief was given to participants. Six questions
A. Individual vs. Collective
were asked, using the 5-point Likert scale to measure
The assessment of the group consensus was made by
satisfaction, related with the evaluation of the task presented
aggregating the results of the participants into clusters. Figure
to them:
5 presents the distribution of the values associated with the
task performance. The clustering had lower dispersion and
1. How do you rate the representation of the weather
worst results (according to the F1 scores) in Task 2 (where
map? (User experience’s satisfaction)
no other crowd workers results were shown). As the crowd
2. Do you feel satisfied with the annotations made?
workers could not see each other’s results, their annotations
(User experience’s satisfaction)
had more variation. Task 1 (Simulated Negative Results) had
3. Did you understand the annotations of the other
reasonable values with a reasonable dispersion. Task 3
volunteers? (Assessment of the results provided by
(Simulated Positive Results) had satisfactory precise
other users)
dispersion, obtaining good results in the contribution of
4. Were you influenced by the annotations of the
other volunteers? (Assessment of the results clusters. Moreover, Task 4 presented the real results of the
provided by other users) other crowd workers to each participant and, although it
5. Could you observe the evolution of the weather obtained the maximum value of the F-score, it also had the
map? (Awareness of the map) largest range in values among the four tasks.
6. How was the quality of the interactions’ tools?
(Evaluation of the quality of the user interface)
The post-task questionnaire also included an open-ended
question so that participants could provide feedback,
suggestions, or report problems they encountered. Figure 3
presents a technical flowchart that summarizes the research
method of this study.
FIGURE 4. Pre-task questionnaire answers to the item of proficiency in
performing online tasks.
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
The task duration and the number of clicks increased Table 1 presents the individual results in this case study,
progressively through the tasks, with a more notable showing the evolution of each participant during the task
highlight in the Task 4. The presentation of real results from execution. The participant that was not a weather expert (P1)
other crowd workers, could suggest a more difficult situation had the worst results of the group of participants, but still
for the participant to complete the task, and also indicates contributed in each task with at least one annotation per
that participants attempted to complete the task to the best of cluster, without ever drawing an annotation outside the
their ability, since each task variation incorporates more clusters.
complexity. The individual results show that the users who contributed
for forming clusters had good proficiency using the system
(as measured by total click count and task time) (P3, P6, P7,
P9, P11). These results were accompanied by good F-scores
and a significant appearance of annotations inside the
clusters. Extremely fast task performers had bad results (P8,
P10), which could be due to not being interested in
participating in the case study.
TABLE 1. Overview of individual results on the evolution of participants
over the tasks.
B. Influence of the Crowd
Each participant was evaluated in order to assess whether or
not presenting content that showed annotations influenced
their performance. Figure 6 depicts two charts related to the
clusters’ contributions of the participants. The average F-
score results were stable for most users, with a slight increase
on Task 4. However, some users performed much worse (P4,
P8). In particular, P4 had previously provided poor
contributions, seldom contributing with excellent annotations
to the clusters, and was the only participant with the false
positive condition in Task 1. P8, although presenting good
results, did not submit the annotations in the last task. Most
of the participants increased the quality of annotations on
Task 3 but either increased or worsened the results on Task 4.
This could be due to the fact that Task 3 simulated positive
(accurate) crowd workers results, which helped the
improvement of the contributions. When subsequently
presented with Task 4, the participants could encounter either
accurate or misleading annotations from the real crowd
workers, which could confuse the participant when choosing
regions where the clusters would be present.
FIGURE 5. Box plot of the Clustering Contribution measured with the F1
Score (left), the Task Duration (middle) and Task Click Count (right).
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
processing to identify if the annotations present were
accurate or misleading.
C. Satisfaction Evaluation
At the end of the study, participants had to fill a post-task
questionnaire with questions about their experience. Two
participants did not answer the post-task questionnaire (P3,
P9). They rated the quality of the weather map as neutral
(9.1%), satisfied (63.6%) and very satisfied (27.3%). When
asked about keeping up with the evolution of the weather
maps (each task had three weather maps chronologically
related), 54.5% replied that they were satisfied and 45.5%
were very satisfied. When asked about if they feel satisfied,
most of the participants gave positive feedback (Unsatisfied -
9.1%, Neutral - 45.5%, Satisfied - 27.3%, Very Satisfied -
18.2%). The question of how the quality of the interactions’
tools obtained good results.
FIGURE 6. Line Chart of the Average Clusters F-Score evolution (top)
and Ratio of Excellent (>0.8 F-Score) Contributions evolution (bottom).
Figure 7 presents Line charts showing the evolution of each
participant’s performance during the tasks. Both task
duration and number of clicks increased progressively
through the completion of the tasks, reaching maximum
results in Task 4. This may be because in Task 1 the
participants were presented with negative results, and most of
them figured out that such region would not have a storm. In
Task 2, there were no other crowd workers results, so it did FIGURE 7. Line chart of the evolution on the user task duration (left)
and number of clicks (right).
not affect the participant time or number of clicks but, as
shown earlier, also did not increase the quality of
Regarding the responses to the question about
contributions. In Task 3, the positive results suggest that the
understanding the annotations of other volunteers, most
user takes more time and actions to perform the annotations
participants seemed to agree that they understood the
with good contributions. Last but not least, Task 4 had the
annotations (54%) and 27% replied with “neutral” which
highest number of duration and clicks, with several
would suggest that they either understood some but not all
participants taking more time and actions for adding final
annotations, or they were not sure whether or not they
annotations, which suggests that the task required additional
understood the annotations correctly. This suggests that the
participants did not understand and neither felt influenced by
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
other crowd workers’ results. However, the results presented examine this aspect of the interaction design more closely, as
previously point to the participant being influenced, either the results also indicated that the visibility and sources of
positive or negatively by each other’s results. prior annotations impacted their work.
In the post-task questionnaire, the last item was open-
ended, meaning that the participants could write their opinion A. Stigmergic effects on annotation
and feedback regarding their participation. The open-ended The scope of the study considers individual and group
item was optional and was answered by six participants (P1, behaviors and the influence of the crowd as an intrinsic
P5, P6, P7, P10, P12). Among these comments, participants coordination strategy (stigmergic effect). The analysis of
praised the accessibility and utility of the tools, including one individual behavior enables perception of each participant’s
remark about applying this tool in the professional context: work consistency and their event annotation behavior. The
“Very interactive and easily accessible platform” (P12) results showed a high individual consistency in the density of
“I realized that I can apply that tool in my classes” (P7) users’ annotations as well as their granularity, but with
One participant (P5) highlighted that a better example of a variability between participants. The individual differences in
storm should be given. The goal of this study however, is to their understanding of the semantic domain of the concept of
get annotations from a wide spectrum of extreme weather “extreme atmospheric phenomena” indicates a lack of
events severity, which can be light storms to cyclones. Other common ground that could impact the quality of aggregate
participants suggested improvements, suggesting that it could annotations: for some experts, a storm is any extreme
be inserted open-ended text to the annotations made, so that weather phenomenon, however, others had a more restricted
the other crowd workers could see. Related with the understanding, considering only hurricanes to qualify as
interactivity, one participant said that the interactivity could extreme phenomena. This diversity is also reflected in the
highlight better the values to annotate but should be done precision and detail with which the phenomena were
carefully. analyzed by each expert, which likely relates to their specific
“From the tutorial I had the impression I could write text knowledge from their climate science backgrounds, and the
related to rectangle, but I could not find the tool if it was only enthusiast in the group did not reach the expected
available. Note I did not find the other users annotations – I results.
clicked at some point in the bottom (in one of the examples and However, the quality of the results reveals that some
there was no output)” (P6) scientists also stray from the pattern when annotating
“[…] require some improvement through more interaction. phenomena. When we examined the number of interactions
[…] while annoying, it may highlight values. This will be more (mouse clicks) and time of each interaction, a particular
useful, thanks.” (P10) situation was identified: quick answers, with a reduced
A participant that demonstrated interest in the climate field number of interactions, very low precision and data quality,
as an enthusiast (P1) felt some difficulties in using the that may reflect the participant’s low motivation to perform
annotator due to a lack of experience with online tools. The the task or other factors such as time constraints or
support of the Help Guide Tour feature was identified as a environmental distractions. Further research, perhaps using a
useful resource. think-aloud protocol, would help uncover the reasons for
“[…] the “Help” menu helped a lot to understand everything poor performance. In general, the most accurate data resulted
that the remaining tasks occurred with normal progress.” (P1) from an intermediate interaction density and equivalent task
The challenges faced were also overcome with the duration. These results can be explained by the users' ability
participant’s curiosity on the climate field and the perception with the platform, familiarity with technology, and
that the tasks were interesting. background related to the tasks (e.g., prior experiences with
“[…] I think that the tasks were interesting, but the knowledge the specific weather events in the images presented.)
about Pressure Lines is not enough to produce more concise Nevertheless, there was no direct relationship between the
results.” (P1) quality of the work completed and the participants’
satisfaction with platform based on the post-questionnaires.
VI. Discussion The individual progression of each participant in terms of
Participants in the study were quite a homogeneous sample time spent and density of interactions follows an increasing
composed primarily of scientists (92%); the community of trend between the tasks. It is not clear why there is an
enthusiasts was represented by one participant (8%). This increase in the average time dedicated to each task. This may
expert crowd has a reduced size, limiting the usage of be related to the increasing complexity of the phenomena
statistical methods, but allowing a qualitative analysis. presented. A note that there is no evidence of a relationship
Participants revealed that they can easily perform online between the time spent and the complexity of the interface
tasks (95% agree/fully agree). Users’ experience with the and the density of the annotations, since task 1 includes
platform was generally positive, with high levels of simulations of previous annotations whereas task 2 does not
satisfaction. Participants’ perceptions of the utility of other contain any annotations, and this decrease is not reflected in
users’ annotations were mixed, and future work could this passage of the average interaction time.
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
If the analysis of the time spent on each task is comprehensive analysis of weather data and mitigating
inconclusive, the same is not true for the quality and individual biases. The results of our study demonstrate that
accuracy of the data. The analysis of the clustering variance stigmergic collaboration enhances the quality and
of contributions reveals that there is a decrease in task 2, that effectiveness of weather annotations, making it a promising
is, in the only task in which there were no previous coordination mechanism for weather forecasting research
annotations (simulated or real). Therefore, there is strong [44, 67].
evidence for the influence of previous annotations on the While stigmergy shows great promise, it is essential to
quality of the obtained annotation data. This result is visible, acknowledge the role of other coordination mechanisms in
either in an individual analysis or in a grouped analysis of the crowdsourcing, as they each have their strengths and
results. Another interesting aspect is that there is no impact of limitations. For instance, centralized coordination methods,
the nature (real or simulated) of previous annotations, such as expert-led annotation platforms, provide direct
regardless if they are true or false. The results show that the oversight and control over the crowdsourcing process,
existence of previous annotations denotes a certain “comfort” ensuring high-quality annotations but potentially limiting
of the participants and consequently better results, which is scalability due to resource constraints. On the other hand,
aligned with the underlying assumptions of the Bandwagon decentralized coordination mechanisms, like peer-to-peer
effect [43]. However, they do not demonstrate this perception collaboration, offer flexibility and scalability, but may
in the post-task questionnaires, revealing that there was no struggle with quality control and coordination among
significant influence when displaying prior annotations. This participants. Comparative studies between stigmergy and
lack of awareness suggests that this interface feature can be these other approaches can shed light on the trade-offs and
suitable for the stigmergy process without causing entropy to applicability in different contexts. Moreover, combining
the user experience. different coordination mechanisms in a hybrid approach may
Other important aspects to keep in mind are the offer synergistic benefits, leveraging the strengths of each
convergence towards fine granularity of the annotation, not mechanism while mitigating their weaknesses. Future
the grouping of phenomena. The initial analysis indicates that research could explore such hybrid models and investigate
the annotations of the phenomena were broad annotations. how stigmergy can complement and enhance other
However, the convergence of expert crowd annotation shows coordination methods in diverse crowdsourcing applications,
that a fine annotation granularity showing the individual not only in weather forecasting but also in various scientific
phenomena is preferred instead of grouping the phenomena. and societal domains [55, 61, 68, 69].
This result is very interesting and it can be inferred from here
that the phenomena must be analyzed in their individual B. Current Challenges and Limitations
characteristics and only then can the interactions between A major limitation of this research is that the experiment was
different phenomena be understood, since human perception conducted on a small scale with just 13 participants, 12
tends to ungroup phenomena. However, the question remains experts and 1 enthusiast (i.e., a paraglider with more than 30
how and when to group the phenomena. Experienced years of expertise on assessing weather phenomena for flying
researchers and experts in the reported phenomena initially in an optimal way). The experiment targeted a very specific
identified the cluster. This is a limitation and it will be group, with very particular capabilities which restricted
necessary to do new tasks to assess the researchers' ability to participant recruitment. The limited number of participants
group phenomena. constrained the results methodology as statistical methods
Stigmergy has emerged as a compelling and innovative would be inappropriate. However, this limitation was an
coordination mechanism in crowdsourcing, demonstrating its opportunity to explore a different approach for gathering
potential in the domain of weather forecasting. Our study collective knowledge from a largely homogeneous, expert
showcases the power of stigmergic collaboration among crowd. Further studies are needed to evaluate the behavior of
weather experts in annotating and predicting extreme weather enthusiasts and the impacts on the data quality of their
events, with a particular emphasis on storm forecasting. collaboration in the expert crowds, as this study was unable
Stigmergy offers several unique advantages over other forms to do so. On the other hand, the small sample size supported
of coordination. Unlike traditional hierarchical methods that an in-depth review of participant behaviors during the task
rely on explicit communication and coordination among execution. The small sample size based on experts is
participants, stigmergy leverages indirect communication validated in crowdsourcing settings. In line with this, Kazai
through the shared environment, enabling crowd workers to and Zitouni [97] concluded that using a sample of fewer than
build upon each other's annotations autonomously. This self- 16 experts in two relevance label tasks significantly enhanced
organization aspect fosters a dynamic and decentralized the quality of the data collected. Furthermore, in the HCI
collaboration, where participants respond to the collective field, having a small sample such as the five-user assumption
intelligence of the crowd, leading to emergent and accurate can be used as an optimal benchmark to identify interaction
predictions. Furthermore, stigmergy harnesses the collective problems [98-100].
expertise of diverse weather experts, allowing for a more
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
The research also did not include a usability study of the geometric objects. These two features facilitate the display of
interface. The system was co-created with experts using a adaptive information that can aid decision-making without
participatory design development approach [85], according to overwhelming the crowd worker.
their requirements and following their visualization golden
2) MOTIVATION FACTORS
rules. However, we are unable to evaluate any potential Concerning the motivational aspects underlying the proposed
relationship between the usability of the platform and the solution, a design principle commonly used to enhance the
data quality. The implication of this limitation is that real quality of the data relies on providing continuous feedback
contributions may overload the visualization tool. In [101]. From a holistic perspective, this study demonstrates
addition, this study showed that previous annotations that prior annotations can act as a critical factor for
contribute to enhancing data quality, but does not allow us to improving data quality. However, the mechanisms through
conclude what kind of annotations (real vs simulated). which the annotations serve to improve quality are not clear.
The expertise of the participants provided high data quality Expert crowds participation do not follow traditional
and the preliminary results demonstrate some open research behavioral patterns in crowdsourcing settings (e.g., [102]), so
opportunities in the frontier between IS and meteorology and specific strategies are required for motivating expert crowds,
atmospheric sciences. As part of an exploratory project, this but the unexpected behavior (i.e., low quality annotations)
study establishes a research roadmap to explore the open for some experts in this study suggests a need for further
challenges in the use of human computation [22] and evaluation of the reasons for this result, which may be related
artificial intelligence [20] to study extreme atmospheric to motivation [103]. Aitamurto and Saldivar demonstrated
phenomena. Further research is required in order to evaluate that motivation factors weakened and the crowd disengaged
the impacts of real and simulated annotations, as well as when crowdsourcing reached a saturation point. Sustaining
simulated true and false annotations. Moreover, studying participation is one of the core challenges for crowdsourcing
crowd processes across different levels of expertise would be systems, which requires knowledge of the crowd’s
useful further research. expectations and motivations.
3) EXPERT CROWDSOURCING AND COMMUNITY OF
C. Implications for Design
EFFORT
1) USER INTERFACE DESIGN Volunteer contributors working worldwide in the face of
Driven by the increasing availability of data produced global climate change constitute a great example of a
through massive collaboration efforts, Gadiraju et al. [91] community of effort (a community formed in pursuit of a
investigated the effect of User Interface (UI) design elements common goal). In such scenarios, no central organization
in microtask work environments and concluded that bad coordinates; rather, the scientists “collaborate in ad hoc ways,
design can affect the overall outputs generated by crowd and are conscious of contributing toward a shared purpose”
workers. The eCSAAP-SOF4WA Extreme Weather Event [104]. The eCSAAP follows this strategy by creating ways to
Annotator can support implicit coordination mechanisms, by involve a broad community in understanding of extreme
presenting previous annotations of extreme weather events to atmospheric phenomena. Expert crowdsourcing is a first step
crowd workers, but this potentially useful feature must be in this strategy, and extending the expert crowds to include
balanced against task complexity resulting from large weather enthusiasts would be a next step as enthusiast
numbers of annotations. These strategies reveal, already used communities have particular characteristics that could be an
in [94], were applied to process the results of the current asset for annotation of extreme phenomena. However, this
study, enhancing the collective knowledge. The integration study was not able to adequately assess the potential value of
of human factors such as expertise and experience obtained combining the work of experts and enthusiasts, so further
from a collective of weather specialists can enhance the research will be needed to evaluate the participation of a
precision of meteorological prediction systems. Utilizing the broader community in the study of extreme atmospheric
inherent collaboration of expert crowds in semantic labeling phenomena.
can potentially augment readiness for severe meteorological
phenomena and mitigate their consequences. A primary
consideration in user interface design is achieving a balance
in the presentation of results from other crowd workers. In VII. Conclusions and Future Work
the context of interactive web maps, this can be Crowdsourcing represents an opportunity for novel research
accomplished through the modification of geometric object and practice in climate sciences. Extreme atmospheric
styles and the implementation of dynamic pop-ups, as phenomena are becoming more frequent and require
demonstrated in this study. The former involves the use of alternative approaches for both the study of the events and
dashed lines to represent Extreme Weather Events (EWEs) predicting them. This research employed expert
annotated by other weather experts, which influenced their crowdsourcing to annotate extreme atmospheric phenomena
decision-making. The latter involves the inclusion of using a technological platform developed with a participatory
additional information in pop-ups activated by clicking on co-created design that engaged the climate science research
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
community in the development process. The resulting Director, Natural Resources and Environment, Testimony
Before the Select Committee on the Climate Crisis, House of
annotations, aggregated with a clustering approach, showed
Representatives. in United States. Government Accountability
that new knowledge can arise from the participation of the Office. 2019. United States. Government Accountability Office.
expert. However, the engagement of expert crowd workers 8. Palen, L. and K.M. Anderson. Crisis informatics—New data for
requires specific strategies adapted to specific requirements extraordinary times. Science, 2016. 353(6296): p. 224-225.
9. Soden, R. and L. Palen. Informating crisis: Expanding critical
of this community. Through the utilization of human factors
perspectives in crisis informatics. Proceedings of the ACM on
such as expertise and practical experience, we can optimize human-computer interaction, 2018. 2(CSCW): p. 1-22.
our readiness for severe weather phenomena and mitigate 10. Soden, R. and A. Lord. Mapping silences, reconfiguring loss:
Practices of damage assessment & repair in post-earthquake
their consequences. Our research outcomes hold significant
Nepal. Proceedings of the ACM on Human-Computer
implications for enhancing weather prediction systems and Interaction, 2018. 2(CSCW): p. 1-21.
bolstering disaster preparedness measures. The seamless 11. Slonosky, V. and R. Sieber. Building a Traceable and
Sustainable Historical Climate Database: Interdisciplinarity
integration of proficient crowd workers, equipped with a
and DRAW. Patterns, 2020. 1(1): p. 100012.
background in climate science research, can be facilitated via 12. See, L., A. Comber, C. Salk, S. Fritz, M. Van Der Velde, C.
a tailored web-based interactive map and the application of Perger, C. Schill, I. McCallum, F. Kraxner, and M. Obersteiner.
Comparing the quality of crowdsourced data contributed by
conventional crowdsourcing techniques, such as annotation
expert and non-experts. PloS one, 2013. 8(7): p. e69958.
clustering. Extending the expert crowd to climate enthusiasts
13. Hutt, H., R. Everson, M. Grant, J. Love, and G. Littlejohn. How
revealed the same engaging problems and, therefore, the size clumpy is my image? Evaluating crowdsourced annotation
of the dataset was very limited and constrained the results of tasks. in 2013 13th UK Workshop on Computational
Intelligence (UKCI). 2013. IEEE.
the study. Nonetheless, the results of this work identify
14. Saltz, J.S., R. Heckman, K. Crowston, S. You, and Y. Hegde.
interesting potential and future directions for the application Helping Data Science Students Develop Task Modularity. in
of crowd computing in climate science research. HICSS. 2019.
15. Moayedikia, A., K.-L. Ong, Y.L. Boo, and W.G.S. Yeoh. Task
assignment in microtask crowdsourcing platforms using
ACKNOWLEDGMENT learning automata. Engineering Applications of Artificial
This work is financed by the Portuguese Foundation for Intelligence, 2018. 74: p. 212-225.
16. Ren, X., X. Li, K. Ren, J. Song, Z. Xu, K. Deng, and X. Wang.
Science and Technology (Fundação para a Ciência e a
Deep learning-based weather prediction: a survey. Big Data
Tecnologia – FCT) with the research grant Research, 2021. 23: p. 100178.
SFRH/BD/148991/2019. The authors also acknowledge 17. Tang, L., J. Li, H. Du, L. Li, J. Wu, and S. Wang. Big data in
forecasting research: a literature review. Big Data Research,
support from the European Social Fund under the scope of
2022. 27: p. 100289.
North Portugal Regional Operational Programme. 18. Diffenbaugh, N.S. Verification of extreme event attribution:
Using out-of-sample observations to assess changes in
probabilities of unprecedented events. Science Advances, 2020.
REFERENCES
6(12): p. eaay2368.
1. Agrawal, A.A., D.D. Ackerly, F. Adler, A.E. Arnold, C. 19. Niforatos, E., A. Vourvopoulos, and M. Langheinrich.
Cáceres, D.F. Doak, E. Post, P.J. Hudson, J. Maron, and K.A. Understanding the potential of human–machine crowdsourcing
Mooney. Filling key gaps in population and community for weather data. International Journal of Human-Computer
ecology. Frontiers in Ecology and the Environment, 2007. 5(3): Studies, 2017. 102: p. 54-68.
p. 145-152. 20. Nushi, B., E. Kamar, and E. Horvitz. Towards accountable ai:
2. Jentsch, A. and C. Beierkuhnlein. Research frontiers in climate Hybrid human-machine analyses for characterizing system
change: effects of extreme meteorological events on failure. in Proceedings of the AAAI Conference on Human
ecosystems. Comptes Rendus Geoscience, 2008. 340(9-10): p. Computation and Crowdsourcing. 2018.
621-628. 21. Kim, S. and L. P. Robert Jr. Crowdsourcing coordination: A
3. Auffhammer, M. Quantifying economic damages from climate review and research agenda for crowdsourcing coordination
change. Journal of Economic Perspectives, 2018. 32(4): p. 33- used for macro-tasks. Macrotask Crowdsourcing, 2019: p. 17-
52. 43.
4. Mitchell, D., C. Heaviside, S. Vardoulakis, C. Huntingford, G. 22. Michelucci, P. and J. L. Dickinson. The power of crowds.
Masato, B.P. Guillod, P. Frumhoff, A. Bowery, D. Wallom, Science, 2016. 351(6268): p. 32-33.
and M. Allen. Attributing human mortality during extreme heat 23. Muller, C., L. Chapman, S. Johnston, C. Kidd, S. Illingworth,
waves to anthropogenic climate change. Environmental G. Foody, A. Overeem, and R. Leigh. Crowdsourcing for
Research Letters, 2016. 11(7): p. 074006. climate and atmospheric sciences: current status and future
5. Smith, K., G. Kent, D. Slater, G. Plank, M. Williams, M. potential. International Journal of Climatology, 2015. 35(11):
McCarthy, F. Vernon, N. Driscoll, H.-W. Braun, and R. p. 3185-3203.
Anderson. Integrated multi-hazard regional networks: 24. Wechsler, D. Crowdsourcing as a method of transdisciplinary
Earthquake warning/response, wildfire detection/response, and research—Tapping the full potential of participants. Futures,
extreme weather tracking. Applied Geology in California: 2014. 60: p. 14-22.
Association of Environmental and Engineering Geologists 25. Gomes-Pereira, J. N., V. Auger, K. Beisiegel, R. Benjamin, M.
(AEG) Special Publication, 2016. 26: p. 599-612. Bergmann, D. Bowden, P. Buhl-Mortensen, F.C. De Leo, G.
6. Yore, R. and J.F. Walker. Early warning systems and Dionísio, and J.M. Durden. Current and future trends in marine
evacuation: rare and extreme versus frequent and small‐scale image annotation software. Progress in Oceanography, 2016.
tropical cyclones in the Philippines and Dominica. Disasters, 149: p. 106-120.
2021. 45(3): p. 691-716. 26. Bolici, F., J. Howison, and K. Crowston. Coordination without
7. Gaffigan, M. Climate Resilience: A Strategic Investment discussion? Socio-technical congruence and Stigmergy in Free
Approach for High-Priority Projects Could Help Target and Open Source Software projects. in Socio-Technical
Federal Resources, Statement of Mark Gaffigan, Managing
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
Congruence Workshop in conj Intl Conf on Software 46. Haklay, M. Citizen science and volunteered geographic
Engineering, Vancouver, Canada. 2009. information: Overview and typology of participation.
27. Crowston, K., E. Mitchell, and C. Østerlund. Coordinating Crowdsourcing geographic knowledge, 2013: p. 105-122.
advanced crowd work: extending citizen science. Citizen 47. Kullenberg, C. and D. Kasperowski. What is citizen science?–A
Science: Theory and Practice, 2019. 4(1). scientometric meta-analysis. PloS one, 2016. 11(1): p.
28. Shirado, H., F.W. Crawford, and N.A. Christakis. Collective e0147152.
communication and behaviour in response to uncertain 48. Piccolo, L., M. Fernández, H. Alani, A. Scharl, M. Föls, and D.
‘Danger’in network experiments. Proceedings of the Royal Herring. Climate change engagement: results of a multi-task
Society A, 2020. 476(2237): p. 20190685. game with a purpose. in tenth international AAAI conference
29. Cartwright, M., A. Seals, J. Salamon, A. Williams, S. on web and social media. 2016.
Mikloska, D. MacConnell, E. Law, J.P. Bello, and O. Nov. 49. Staffelbach, M., P. Sempolinski, T. Kijewski-Correa, D. Thain,
Seeing sound: Investigating the effects of visualizations and D. Wei, A. Kareem, and G. Madey. Lessons learned from
complexity on crowdsourced audio annotations. Proceedings of crowdsourcing complex engineering tasks. PloS one, 2015.
the ACM on Human-Computer Interaction, 2017. 1(CSCW): p. 10(9): p. e0134978.
1-21. 50. Amer-Yahia, S. and S. Basu Roy. From Complex Object
30. Crowston, K. and A. Rezgui. Effects of stigmergic and explicit Exploration to Complex Crowdsourcing. in Proceedings of the
coordination on Wikipedia article quality. in Proceedings of 24th International Conference on World Wide Web. 2015.
the Annual Hawaii International Conference on System 51. Cui, Q., S. Wang, J. Wang, Y. Hu, Q. Wang, and M. Li. Multi-
Sciences. 2020. Objective Crowd Worker Selection in Crowdsourced Testing. in
31. Gloag, E.S., M.A. Javed, H. Wang, M.L. Gee, S.A. Wade, L. SEKE. 2017.
Turnbull, and C.B. Whitchurch. Stigmergy: a key driver of self- 52. Cullina, E., K. Conboy, and L. Morgan. Choosing the right
organization in bacterial biofilms. communicative & crowd: An iterative process for crowd specification in
integrative Biology, 2013. 6(6): p. 11541-6. crowdsourcing initiatives. in 2016 49th Hawaii International
32. Suchman, L.A. Plans and situated actions: The problem of Conference on System Sciences (HICSS). 2016. IEEE.
human-machine communication. 1987: Cambridge university 53. Hirth, M., T. Hoßfeld, and P. Tran-Gia. Analyzing costs and
press. accuracy of validation mechanisms for crowdsourcing
33. Xia, H., C. Østerlund, B. McKernan, J. Folkestad, P. Rossini, platforms. Mathematical and Computer Modelling, 2013.
O. Boichak, J. Robinson, K. Kenski, R. Myers, and B. Clegg. 57(11-12): p. 2918-2932.
TRACE: A stigmergic crowdsourcing platform for intelligence 54. De Beer, J., I.P. McCarthy, A. Soliman, and E. Treen. Click
analysis. in Proceedings of the 52nd Hawaii International here to agree: Managing intellectual property when
Conference on System Sciences. 2019. crowdsourcing solutions. Business Horizons, 2017. 60(2): p.
34. Lewis, T.G. Cognitive stigmergy: A study of emergence in 207-217.
small-group social networks. Cognitive Systems Research, 55. de La Robertie, B., Y. Pitarch, and O. Teste. Measuring article
2013. 21: p. 7-21. quality in wikipedia using the collaboration network. in 2015
35. Heylighen, F. Collective Intelligence and its Implementation on IEEE/ACM International Conference on Advances in Social
the Web: algorithms to develop a collective mental map. Networks Analysis and Mining (ASONAM). 2015. IEEE.
Computational & Mathematical Organization Theory, 1999. 56. Casarin, J., N. Pacqueriaud, and D. Bechmann. Umi3d: A
5(3): p. 253-280. unity3d toolbox to support cscw systems properties in generic
36. Heylighen, F. Why is Open Access Development so Successful? 3d user interfaces. Proceedings of the ACM on Human-
Stigmergic organization and the economics of information. Computer Interaction, 2018. 2(CSCW): p. 1-20.
arXiv preprint cs/0612071, 2006. 57. Lukyanenko, R., J. Parsons, and Y. Wiersma. The impact of
37. Heylighen, F. Stigmergy as a universal coordination conceptual modeling on dataset completeness: A field
mechanism I: Definition and components. Cognitive Systems experiment. in ICIS 2014. 2014.
Research, 2016. 38: p. 4-13. 58. Lukyanenko, R., J. Parsons, and Y.F. Wiersma. The IQ of the
38. Haselton, M.G., D. Nettle, and D.R. Murray. The evolution of crowd: Understanding and improving information quality in
cognitive bias. The handbook of evolutionary psychology, structured user-generated content. Information Systems
2015: p. 1-20. Research, 2014. 25(4): p. 669-689.
39. Eickhoff, C. Cognitive biases in crowdsourcing. in Proceedings 59. Irshad, H., L. Montaser-Kouhsari, G. Waltz, O. Bucur, J.
of the eleventh ACM international conference on web search Nowak, F. Dong, N.W. Knoblauch, and A.H. Beck.
and data mining. 2018. Crowdsourcing image annotation for nucleus detection and
40. Kruger, J. and D. Dunning. Unskilled and unaware of it: how segmentation in computational pathology: evaluating experts,
difficulties in recognizing one's own incompetence lead to automated methods, and the crowd. in Pacific symposium on
inflated self-assessments. Journal of personality and social biocomputing Co-chairs. 2014. World Scientific.
psychology, 1999. 77(6): p. 1121. 60. Bonney, R., J.L. Shirk, T.B. Phillips, A. Wiggins, H.L. Ballard,
41. Saab, F., I.H. Elhajj, A. Kayssi, and A. Chehab. Modelling A.J. Miller-Rushing, and J.K. Parrish. Next steps for citizen
Cognitive Bias in Crowdsourcing Systems. Cognitive Systems science. Science, 2014. 343(6178): p. 1436-1437.
Research, 2019. 58: p. 1-18. 61. O’Leary, M.A., K. Alphonse, A.H. Mariangeles, D. Cavaliere,
42. Gadiraju, U., B. Fetahu, R. Kawase, P. Siehndel, and S. Dietze. A. Cirranello, T.G. Dietterich, M. Julius, S. Kaufman, E. Law,
Using Worker Self-Assessments for Competence-Based Pre- and M. Passarotti. Crowds replicate performance of scientific
Selection in Crowdsourcing Microtasks. ACM Trans. Comput.- experts scoring phylogenetic matrices of phenotypes.
Hum. Interact., 2017. 24(4): p. Article 30. Systematic Biology, 2018. 67(1): p. 49-60.
43. Bikhchandani, S., D. Hirshleifer, and I. Welch. A theory of 62. Warby, S.C., S.L. Wendt, P. Welinder, E.G. Munk, O. Carrillo,
fads, fashion, custom, and cultural change as informational H.B. Sorensen, P. Jennum, P.E. Peppard, P. Perona, and E.
cascades. Journal of political Economy, 1992. 100(5): p. 992- Mignot. Sleep-spindle detection: crowdsourcing and evaluating
1026. performance of experts, non-experts and automated methods.
44. Franzoni, C. and H. Sauermann. Crowd science: The Nature methods, 2014. 11(4): p. 385-392.
organization of scientific research in open collaborative 63. Nguyen, A.T., B.C. Wallace, and M. Lease. Combining crowd
projects. Research policy, 2014. 43(1): p. 1-20. and expert labels using decision theoretic active learning. in
45. Cohn, J.P. Citizen science: Can volunteers do real research? Third AAAI conference on human computation and
BioScience, 2008. 58(3): p. 192-197. crowdsourcing. 2015.
64. Law, E., K.Z. Gajos, A. Wiggins, M.L. Gray, and A. Williams.
Crowdsourcing as a tool for research: Implications of
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
uncertainty. in Proceedings of the 2017 ACM conference on classification of phonocardiograms. Proceedings of the ACM
computer supported cooperative work and social computing. on Human-Computer Interaction, 2018. 2(CSCW): p. 1-17.
2017. 83. Chan, J., J.C. Chang, T. Hope, D. Shahaf, and A. Kittur.
65. Williamson, K., M.A. Kennan, G. Johanson, and J. Weckert. Solvent: A mixed initiative system for finding analogies
Data sharing for the advancement of science: Overcoming between research papers. Proceedings of the ACM on Human-
barriers for citizen scientists. Journal of the Association for Computer Interaction, 2018. 2(CSCW): p. 1-21.
Information Science and Technology, 2016. 67(10): p. 2392- 84. Li, T., K. Luther, and C. North. Crowdia: Solving mysteries
2403. with crowdsourced sensemaking. Proceedings of the ACM on
66. Barbosu, S. and J.S. Gans. Storm crowds: Evidence from Human-Computer Interaction, 2018. 2(CSCW): p. 1-29.
Zooniverse on crowd contribution design. Research Policy, 85. Kensing, F. and J. Blomberg. Participatory Design: Issues and
2022. 51(1): p. 104414. Concerns. Computer Supported Cooperative Work (CSCW),
67. Hennon, C.C., K.R. Knapp, C.J. Schreck III, S.E. Stevens, J.P. 1998. 7(3): p. 167-185.
Kossin, P.W. Thorne, P.A. Hennon, M.C. Kruk, J. Rennie, and 86. Schmidt, K. and L. Bannon. Constructing CSCW: The first
J.-M. Gadéa. Cyclone center: can citizen scientists improve quarter century. Computer supported cooperative work
tropical cyclone intensity records? Bulletin of the American (CSCW), 2013. 22(4): p. 345-372.
Meteorological Society, 2015. 96(4): p. 591-607. 87. Dee, D.P., S.M. Uppala, A.J. Simmons, P. Berrisford, P. Poli,
68. Chapman, L., C. Bell, and S. Bell. Can the crowdsourcing data S. Kobayashi, U. Andrae, M. Balmaseda, G. Balsamo, and d.P.
paradigm take atmospheric science to a new level? A case Bauer. The ERA‐Interim reanalysis: Configuration and
study of the urban heat island of London quantified using performance of the data assimilation system. Quarterly Journal
Netatmo weather stations. International Journal of Climatology, of the royal meteorological society, 2011. 137(656): p. 553-
2017. 37(9): p. 3597-3605. 597.
69. Liu, K., C. Yang, Z. Li, M. Sun, J. Li, and C. Xu. Climate@ 88. Tummolini, L. and C. Castelfranchi. Trace signals: The
Home: Utilizing Citizen Science for Climate Studies. in AGU meanings of stigmergy. in International Workshop on
Fall Meeting Abstracts. 2013. Environments for Multi-Agent Systems. 2006. Springer.
70. Tinati, R., M. Van Kleek, E. Simperl, M. Luczak-Rösch, R. 89. Van den Besselaar, E.J., M. Haylock, G. Van der Schrier, and
Simpson, and N. Shadbolt. Designing for citizen data analysis: A. Klein Tank. A European daily high‐resolution observational
A cross-sectional case study of a multi-domain citizen science gridded data set of sea level pressure. Journal of Geophysical
platform. in Proceedings of the 33rd Annual ACM Conference Research: Atmospheres, 2011. 116(D11).
on Human Factors in Computing Systems. 2015. 90. Liberato, M.L. The 19 January 2013 windstorm over the North
71. Park, E.G., G. Burr, V. Slonosky, R. Sieber, and L. Podolsky. Atlantic: large-scale dynamics and impacts on Iberia. Weather
Data rescue archive weather (DRAW): Preserving the and Climate Extremes, 2014. 5: p. 16-28.
complexity of historical climate data. Journal of 91. Liberato, M.L., J.G. Pinto, I.F. Trigo, and R.M. Trigo. Klaus–
Documentation, 2018. an exceptional winter storm over northern Iberia and southern
72. Lin, Y.-W., J. Bates, and P. Goodale. Co-observing the France. Weather, 2011. 66(12): p. 330-334.
weather, co-predicting the climate: Human factors in building 92. Daniel, F., P. Kucherbaev, C. Cappiello, B. Benatallah, and M.
infrastructures for crowdsourced data. Science and Allahbakhsh. Quality Control in Crowdsourcing: A Survey of
Technology Studies, 2016. 29(3): p. 10-27. Quality Attributes, Assessment Techniques, and Assurance
73. Kiryakov, A., B. Popov, I. Terziev, D. Manov, and D. Actions. ACM Comput. Surv., 2018. 51(1): p. Article 7.
Ognyanoff. Semantic annotation, indexing, and retrieval. 93. Oosterman, J., A. Nottamkandath, C. Dijkshoorn, A. Bozzon,
Journal of Web Semantics, 2004. 2(1): p. 49-79. G.-J. Houben, and L. Aroyo. Crowdsourcing knowledge-
74. Macário, C.G.N. and C.B. Medeiros. Specification of a intensive tasks in cultural heritage. in Proceedings of the 2014
framework for semantic annotation of geospatial data on the ACM conference on Web science. 2014.
web. SIGSPATIAL Special, 2009. 1(1): p. 27-32. 94. Zhang, J., V.S. Sheng, J. Wu, and X. Wu. Multi-class ground
75. Hughes, A.J., J.D. Mornin, S.K. Biswas, L.E. Beck, D.P. truth inference in crowdsourcing with clustering. IEEE
Bauer, A. Raj, S. Bianco, and Z.J. Gartner. Quanti. us: a tool Transactions on Knowledge and Data Engineering, 2015.
for rapid, flexible, crowd-based annotation of images. Nature 28(4): p. 1080-1085.
methods, 2018. 15(8): p. 587-590. 95. Schubert, E., J. Sander, M. Ester, H.P. Kriegel, and X. Xu.
76. Gonzalez, A.L., D. Izidoro, R. Willrich, and C.A. Santos. DBSCAN revisited, revisited: why and how you should (still)
OurMap: Representing crowdsourced annotations on use DBSCAN. ACM Transactions on Database Systems
geospatial coordinates as Linked Open Data. in International (TODS), 2017. 42(3): p. 1-21.
Conference on Collaboration and Technology. 2013. Springer. 96. Goutte, C. and E. Gaussier. A probabilistic interpretation of
77. Kaufmann, A., J. Peters-Anders, S. Yurtsever, and L. precision, recall and F-score, with implication for evaluation.
Petronzio. Automated Semantic Validation of Crowdsourced in European conference on information retrieval. 2005.
Local Information – The Case of the Web Application "Climate Springer.
Twins". 2013. Berlin, Heidelberg: Springer Berlin Heidelberg. 97. Kazai, G. and I. Zitouni. Quality Management in
78. Santos, H. and V. Furtado. A Service-oriented architecture for Crowdsourcing using Gold Judges Behavior, in Proceedings of
assisting the authoring of semantic crowd maps. in Brazilian the Ninth ACM International Conference on Web Search and
Symposium on Artificial Intelligence. 2012. Springer. Data Mining. 2016, Association for Computing Machinery:
79. Heath, T. and C. Bizer. Linked data: Evolving the web into a San Francisco, California, USA. p. 267–276.
global data space. Synthesis lectures on the semantic web: 98. Nielsen, J. Usability engineering. Boston: AP Professional,
theory and technology, 2011. 1(1): p. 1-136. 1993.
80. Goy, A., D. Magro, G. Petrone, M. Rovera, and M. Segnan. 99. Faulkner, L. Beyond the five-user assumption: Benefits of
Supporting Semantic Annotation in Collaborative Workspaces increased sample sizes in usability testing. Behavior Research
with Knowledge Based on Linked Open Data. in International Methods, Instruments, & Computers, 2003. 35(3): p. 379-383.
Joint Conference on Knowledge Discovery, Knowledge 100. Borsci, S., R.D. Macredie, J. Barnett, J. Martin, J. Kuljis, and
Engineering, and Knowledge Management. 2016. Springer. T. Young. Reviewing and Extending the Five-User Assumption:
81. Debattista, J., C. Lange, S. Auer, and D. Cortis. Evaluating the A Grounded Procedure for Interaction Evaluation. ACM
quality of the LOD cloud: An empirical investigation. Semantic Trans. Comput.-Hum. Interact., 2013. 20(5): p. Article 29.
Web, 2018. 9(6): p. 859-901. 101. Zhou, X., J. Tang, Y.C. Zhao, and T. Wang. Effects of feedback
82. Callaghan, W., J. Goh, M. Mohareb, A. Lim, and E. Law. design and dispositional goal orientations on volunteer
Mechanicalheart: A human-machine framework for the performance in citizen science projects. Computers in Human
Behavior, 2020. 107: p. 106266.
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/
This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319597
102. Rogstadius, J., V. Kostakos, A. Kittur, B. Smus, J. Laredo, and interactive aspects. Proceedings of the ACM on Human-
M. Vukovic. An assessment of intrinsic and extrinsic Computer Interaction, 2017. 1(CSCW): p. 1-22.
motivation on task performance in crowdsourcing markets. in 104. Sholler, D., I. Steinmacher, D. Ford, M. Averick, M. Hoye, and
Proceedings of the international AAAI conference on web and G. Wilson. Ten simple rules for helping newcomers become
social media. 2011. contributors to open projects. PLoS computational biology,
103. Aitamurto, T. and J. Saldivar. Motivating participation in 2019. 15(9): p. e1007296.
crowdsourced policymaking: The interplay of epistemic and
simulations for future climate scenarios. Additionally she is also
Dennis Paulino received the Master’s Degree in Informatic interested on the impact of climate variability and extremes on society.
Engineer, at University of Trás-os-Montes e Alto Douro (UTAD), Vila Additionally, she has participated as member of the research team of
Real, Portugal in December 2018. He participated since September 2016 several FCT and EU funded projects such as ENAC, STORMEx and
until December 2018 in the project NanoSTIMA RL2 - Passus Mobile, QSECA and has been a Member of the Management Committee of the
responsible for developing a system that makes exercise supervision of COST ESSEM Action ES0604. She has also participated as PI in several
people with peripheral arterial disease. From January 2019 until hand-on projects (Concurso Ciência Viva VI) as well as in several tasks
November 2019, he participated in the project eCSAAP, responsible for of the FP7 ACCENT (Atmospheric Composition Change) NETWORK
the construction of a informatic system to help in the visualization and OF EXCELLENCE from 2006-2009 and of the ACCENT Plus
detection of meteorological phenomena. In Dezember 2019 he ingress in NETWORK OF EXCELLENCE since 2010. She has fostered several
PhD in Informatics at UTAD, with a scholarship financed by FCT. International collaborations which have resulted on several peer-
António Correia is a former Microsoft Research fellow and works reviewed publication
as a Research Assistant at INESC TEC, Porto, Portugal. He holds a Adriana S. Vivacqua holds a degree from the Pontifical Catholic
Ph.D. degree in Computer Science from the University of Trás-os- University of Rio de Janeiro (1993), a Master's degree in Computing
Montes e Alto Douro (UTAD), Vila Real, Portugal. Furthermore, he from the Fluminense Federal University (1997), a Master's degree in
formerly worked as a Visiting Postgraduate Researcher at University of Media Arts and Sciences from the Massachusetts Institute of
Kent, Medway, UK. António holds more than ten years of experience in Technology (1999) and a Ph. of Rio de Janeiro (2007) in co-supervision
research and scientific writing, and his research interests are mainly in with the Université de Tecnologie de Compiègne, and post-doctoral by
the fields of Human-Artificial Intelligence (AI) Interaction, Computer- the Universidad Politécnica de Valencia. She is currently a professor at
Supported Cooperative Work (CSCW), and Information Systems (IS), the Department of Computer Science at the Federal University of Rio de
and Science and Technology Studies (STS). He has authored or co- Janeiro, and works in the Graduate Program in Informatics, of which she
authored more than 50 publications, including journal articles, is vice-coordinator.
conference papers, book chapters, and posters. Moreover, he has also Andrea Grover holds a PhD in Information Science and Technology
participated in research projects conducted at national and international from the Syracuse University School of Information Studies. Currently,
level and has been executing functions as external reviewer and she is an Associate Professor in Information Systems & Quantitative
scientific committee member for top-tier journals and conferences Analysis at the University of Nebraska at Omaha’s College of
covering aspects of computer science. Information Science & Technology. She studies public participation in
Marcela Yagui received the Master’s Degree in Informatics at Fed. data-intensive scientific collaboration (large-scale citizen science) and
Univ. of Rio de Janeiro, Brazil, in April 2019. She has pursued the PhD issues related to data management and technologies.
degree in Informatics at Fed. Univ. of Rio de Janeiro. Jeffrey P. Bigham is an Associate Professor in the Human-
João Barroso earned a doctorate in University of Trás-os-Montes e Computer Interaction and Language Technologies Institutes in the
Alto Douro (UTAD), Vila Real, Portugal in 2002 in Electrical School of Computer Science at Carnegie Mellon University. He also
Engineering and held in 2008 the Habilitation in leads a Human-Centered Machine Intelligence Group at Apple, which
Informatics/Accessibility and became an Associate Professor in works on research and applied projects in Accessibility, AI Fairness, ML
December 2012. He was Pro-Rector for Innovation and Information Design, Learning Sciences, InfoVis, and Computational Understanding
Management at UTAD from July 2010 to July 2013, Pro-Rector for of UIs. He has received his B.S.E degree in Computer Science from
Innovation and Technology Transfer from May 2017 and June 2020, and Princeton University in 2003, and received his Ph.D. in Computer
Vice-Rector for Innovation, Technology Transfer and Digital University Science and Engineering from the University of Washington in 2009. He
since May 2021. He produced over 150 scientific papers, including book has received the Alfred P. Sloan Foundation Fellowship, the MIT
chapters, journal articles and articles in proceedings of scientific events. Technology Review Top 35 Innovators Under 35 Award, and the
He supervised 40 postgraduate students (masters and doctorates). He National Science Foundation CAREER Award.
was member of the research team in 35 research and development Hugo Paredes received the B.Eng. and Ph.D. degrees in computer
projects. He was member of several organizing committees of the science from the University of Minho, Braga, Portugal, in 2000 and
international scientific meetings. In 2006 he directed the team that 2008, respectively, and the Habilitation title from the University of Trás-
created the conference "Software Development and Technologies for os-Montes e Alto Douro (UTAD), Vila Real, Portugal, in 2016. Since
Enhancing Accessibility and Fighting Info-exclusion 2003, he has been with UTAD, where he is currently an Associate
(www.dsai.ws/2016) and in 2016 the conference Technology and Professor with Habilitation. In 2017, he was a Visiting Faculty with
Innovation is Sports, Health and Wellbeing (www.tishw.ws/2016). and Human Computer Interaction Institute, Carnegie Mellon University,
Human Computer Interaction. Pittsburgh, PA, USA. He is currently the Director of the Master Program
Margarida Liberato is an Assistant Professor at University of Trás- with Informatics Engineering, UTAD, and a Member of the Managing
os-Montes e Alto Douro (UTAD and Researcher at the Instituto Dom Board of Ph.D. in Computer Science with UTAD. He is also an
Luiz (IDL), Faculty of Sciences, University of Lisbon. She has received Assistant Coordinator with the Centre for Computer Graphics and
her PhD in 2008 with an analysis of the extratropical stratosphere- Information Systems (CSIG), Institute for Systems and Computer
troposphere circulation coupling using a 3D normal mode approach. Engineering, Technology and Science (INESC TEC), Porto, Portugal,
Recently she has focused her research interests on the study of where he is a Senior Researcher.
extratropical cyclones and storm-tracks variability, namely on natural
hazards, extreme cyclones and cyclones with extreme impacts. She is the
PI of STORMEx project: Mid-latitude North Atlantic Extreme Storms
Variability: Diagnosis, Modeling Dynamical Processes and Related
Impacts on Iberia. She is also participating in the international initiative
of intercomparing extratropical cyclone detection and tracking
algorithms (IMILAST - Intercomparison of mid latitude storm
diagnostics) in order to assessing method-related uncertainties both
using recent past and present climate reanalysis datasets and GCM
VOLUME XX, 2023 7
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/

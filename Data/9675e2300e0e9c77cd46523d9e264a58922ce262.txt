AUGUST 11 2005
Connected Digit Recognition using symbolic representation
of pronunciation variability 
G. Goodman; B. Lowerre; R. Reddy; D. Scelza
J. Acoust. Soc. Am. 60, S11 (1976)
https://doi.org/10.1121/1.2003141
  CrossMark
View Export
Online Citation
22
February
2024
20:24:53
S11 92nd Meeting: Acoustical Society of America S11
tences (containing 1580 words) of about 3-see duration each isolated word recognition systems use word templates. In an
for a single speaker. The system requires about 12 Mipss attempt to compare relative performance of systems that use
(million instructions per second of speech) and uses about symbolic representations of words, the Harpy system was run
200 000 words of memory on a PDP-10 system. More complete on a connected digit task requiring the recognition of random
results, including several speakers and additional sentences, three-digit sequences. Each of ten speakers (seven male and
will be reported. [Research supportedb y the Defense Advanced three female) spoke 30 training sentences and 100 test sen-
Research Projects Agency. ] tences over a period of two weeks in a computer terminal
room environment (approximately 65 dBA). Using speaker-
9:20 dependent phoneme templates, the word error rate over all the
ten speakers was about 2%. Using speaker-independent pho-
E3. The Hearsay-II speech understanding system. L.D.
neme templates computed from the training data for all the
Erman, F. Hayes--Roth, V.R. Lesser, and R. Reddy (De-
speakers (male and female), the word error rate was about
partment of Computer Science, Carnegie--Mellon University,
8% for a test data set of 1200 random connected three-digit
Pittsburgh, PA 15213)
sequences from 20 speakers (including ten new speakers).
The Hearsay-II System has as its design goal recognition, The recognition time is about 4.5 Mipss (million instructions
understanding, and responding to connected speech utter- per second of speech). [Research supported by the Defense
ances, particularly in situations where sentences cannot be AdvancedR esearch Projects Agency. ]
guaranteed to agree with some predefined, restricted language
model, as in the case of the Harpy System. Further, it at-
tempts to view knowledge sources as different and independent 9:50
which cannot always be integrated into single representation.
E6. Parametric representation of speech. G. Gill and R.
It is based on the blackboard model IV. R. Lesser, R.D.
Reddy (Department of Computer Science, Carnegie--Mellon
Fennell, L.D. Erman, and D.R. Reddy, IEEE Trans. Acoust.
University, Pittsburgh, PA 15213)
Speech and Signal Process. ASSP-23, 11--23 (1975) with know-
ledge sources as a set of parallel processes which are acti- As digital processing of speech becomes commonplace, it
vated asynchronously depending on data events. The system becomes desirable to have a parametric representation of
performs on the Information Retrieval task with accuracy speech which is simple, fast, accurate, and directly obtain-
comparable to that of the Harpy system, but runs about 2 to able from the PCM representation of speech. The ZAPDASH
20 times slower. More complete performance results will be representation of speech (Zerocrossings And Peaks of Differ-
reported. As we get closer to unrestricted vocabularies and enced And Smooth waveforms) is one such. The PCM data is
nongrammaticality of spoken languages, it will be necessary to used to generate a different waveform and a down sampled,
have systems which have the flexibility of Hearsay-II and the smoothed waveform (for 10-kHz sampling rate, the smoothing
performance of Harpy. [Research supportedb y the Defense FIR filter coefficients were-1 0 i 2 44 4 2 i 0-1, used
AdvancedR esearch Projects Agency. ] every fourth point). Peak-to-peak distances and number of
zerocrossings are calculated each 10 msec, resulting in
400 8-bit parameters per second of speech. ZAPDASH can be
9:30 done in 15--20 computer instructions per sample and can be
extracted in less than a 1/3 real time on minicomputers with
E4. Feature extraction(cid:127) segmentation, and labeling in the
2 /.tsec instruction time. Although this representation is not
Harpy and Hearsa¾-ll systems. H.G. Goldberg and R. Reddy
noticeably different other similar proposals, it seems to be
(Department of Computer Science, Carnegie--Mellon Univer-
fairly robust and accurate, and is used in the feature extrac-
sity, Pittsburgh, PA 15213)
tion, segmentation, and labeling parts of the Harpy and
Goldberg [J. Acoust. Soc. Am. 59, S97(A)(1976)] has Hearsay-ll systems. Fortran and PDP-11 machine language
shown that uniform techniques for segmentation and labeling versions are available from the authors. [Research supported
can provide the initial signal-to-symbol transformation for by the Defense Advanced Research Projects Agency. ]
speech recognition systems with reasonable accuracy and
efficiency. Furthermore, the choice of parametric represen- 10:00
tation was not found to be critical for most commonly accepted
E7. The HWIM speech understanding system--Overview and
representations. However, for efficiency, the computationally
performance. Jared J. Wolf and William A. Woods (Bolt
simplest techniques should be used to segment the utterance
Beranek and Newman Inc., 50 Moulton St., Cambridge,
before more accurate (and expensive) spectral representations
MA 01238)
are used for labeling [R. Reddy, J. A coust. Soc. Am. 42,
329--47 (1967)]. To provide an initial symbolic input for both HWIM (for Hear What I Mean), the speech understanding
the Harpy and Hearsay-II systems, an hierarchical, feature- system developed at BBN as part of the recent five-year ARPA
extraction based segmenter, using the ZAPDASH parameters, Speech Understanding Research Project, is designed to un-
has been developed. After segmentation, labeling is done by derstand naturally spoken utterances relevant to a task
a modified LPC minimum distance [F. Itakura, IEEE Trans. domain of travel budget management. Its vocabulary is over
ASSP-23, 67--72 (1975)]. Labeling proceeds by comparing the 1000 words, and its grammar permits a habitable subset of
midpoint of each segment with stored templates (acquired by an natural English. HWIM contains sources of knowledge at the
iterative learning process from speaker-specific training levels of acoustic-phonetics, phonology, vocabulary, syntax,
corpus) and adjusted with weights according to features ob- semantics, factual knowledge, and discourse. This paper
tained from the segmente, r. The use of the highly efficient describes the system as a whole and presents its performance
segmentation procedures and parameters provides approxi- results at the end of the ARPA project. [This research was
mately a factor of 5 speedup over uniform techniques which supported by the Advanced Research Projects Agency of the
were previously used with both Harpy and Hearsay-II [Re- Department of Defense and was monitored by ONR under
search supportedb y the Defense Advanced Projects Agency. ] Contract No. N00014-75-C-0053o ]
9:40 10:10
E5. Connected Digit Recognition using symbolic representation E8. Phonetic and lexical processing in the HWIM speech
of pronunciation variability. G. Goodman, B. Lowerre, understanding system. Richard M. Schwartz, John W.
R. Reddy, and D. Scelza (Department of Computer Science, Klovstad, Victor W. Zue, John I. Makhoul, and Jared J.
Carnegie--Mellon University, Pittsburgh, PA 15213) Wolf (Bolt Beranek and Newman Inc., 50 Moulton St.,
Cambridge, MA 02138)
Most connected speech recognition systems such as Harpy
and Hearsay-II use some form of symbolic representation al- The front end of HWIM, the BBN speech understanding
ternative pronunciations of the vocabulary whereas most system, is that part of the system that governs the formation
J. Acoust. Soc. Am., Vol. 60, Suppl. No. 1, Fall 1976
22
February
2024
20:24:53

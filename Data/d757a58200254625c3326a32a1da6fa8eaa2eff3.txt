Latent Phrase Matching for Dysarthric Speech
ColinLea∗,DiannaYee∗,JayaNarain,ZifangHuang,
LaurenTooley,JeffreyP.Bigham,LeahFindlater
Apple,USA
{colin lea, dianna yee, jnarain, zhuang7, ltooley, jbigham, lfindlater}@apple.com
Abstract Ourapproach,LatentPhraseMatching(LPM),buildsonre-
centeffortsonpersonalizedkeywordspotting(KWS) [13,14,
Manyconsumerspeechrecognitionsystemsarenottuned
15]andquery-by-example(QBE) [16,17,18,19]torecognize
forpeoplewithspeechdisabilities,resultinginpoorrecognition
an arbitrary number of phrases of arbitrary duration. Using a
and user experience, especially for severe speech differences.
smallsetofspeechsamples,wecreateamodelper-phraseus-
Recentstudieshaveemphasizedinterestinpersonalizedspeech
inglatentembeddingsfromalarge-scalekeywordspotterthat
modelsfrompeoplewithatypicalspeechpatterns. Wepropose
is agnostic to phonetic content or prosody. At run-time, new
aquery-by-example-basedpersonalizedphraserecognitionsys-
vocalizationsarecomparedtoenrolledphrasesusingadynamic
temthatistrainedusingsmallamountsofspeech,islanguage
timewarpingmetricthatisflexibletovariationsinspeakingrate
agnostic, does notassume a traditional pronunciationlexicon,
andstyle.
andgeneralizeswellacrossspeechdifferenceseverities. Onan
We assess our approach, along with a commercial ASR
internal dataset collected from 32 people with dysarthria, this
system that is tuned for the general public, on two datasets.
approach works regardless of severity and shows a 60% im-
First, weuseaninternalcollectionofEnglishspeechfrom32
provementinrecallrelativetoacommercialspeechrecognition
people with dysarthria, collected across multiple sessions and
system. On the public EasyCall dataset of dysarthric speech,
timesofdaywiththegoalofcapturingvariabilityinanindivid-
our approach improves accuracy by 30.5%. Performance de-
ual’sspeechcharacteristics. Second,weusetheEasyCall[20]
gradesasthenumberofphrasesincreases,butconsistentlyout-
datasetofItalianspeechfrom31peoplewithdysarthria.Incon-
performsASRsystemswhentrainedwith50uniquephrases.
trasttootherpublicdysarthriadatasets(e.g.,Torgo[21]),both
Index Terms: speech recognition, human-computer interac-
evaluationdatasetsincluderecordingsfromeachindividualof
tion,speechdysarthria
tensofuniquephrasesatleastfivetimeseach.Ourcontributions
areasfollows.Wevalidateexistingresearchonatypicalspeech
1. Introduction
(e.g., [1]) by demonstrating large differences in ASR perfor-
manceforpeoplewithvaryingspeechdisabilities. Wepropose
Manypeoplewithspeechdisabilities,suchasdysarthria,stut-
aspeechembeddingmodelthatencodesdysarthricspeechpat-
tering, or dysphonia, have shown interest in speech technol-
terns better than existing models and pair it with a query-by-
ogy,butpoorautomaticspeechrecognition(ASR)inconsumer
example framework for phrase recognition. We validate the
productshaslimitedadoption,especiallyforpeoplewithsevere
approach on two datasets totalling 63 people with dysarthria
speechdifferences[1,2,3].
acrosstwolanguagesanddemonstratethatphraserecognition
Inthisworkwefocusondysarthria,whichischaracterised
performanceismuchhigherforpeoplewithseveredysarthria
byunclearspeechduetoweaknessorlimitedcontrolofartic-
than traditional ASR systems. Furthermore, we discuss addi-
ulatorymovementthatmayarisedevelopmentally(e.g.,dueto
tional design decisions to consider when collecting dysarthric
cerebralpalsyorDownsyndrome)ormaybeacquired(e.g.,due
speech data and developing and validating dysarthric phrase
to ALS, brain injury, or MS) [2]. For people with dysarthria,
recognitionsystems.
pronunciations vary person-to-person, where some may not
enunciate certain sounds or may have distorted vocalizations
relativetootherpartsofthegeneralpopulation. Sincegeneral- 2. Methods
purposeASRsystemscommonlyassumethatwordsarebroken
intosub-units(phonesorsub-words)thatarepronouncedsim- OurLPMmethodusesaQBEapproachthatconsistsofaspeech
ilarlyacrossindividuals,divergencefromthosepronunciations embeddingmodelthatisoptimizedforsupervisedwordrecog-
tendtosignificantlyreduceaccuracy. nitiontocreateamodelper-phrase, andasimilaritymetricto
Recenteffortsondysarthricspeechrecognitionfocusedon comparephrasesusingdynamictimewarping(DTW)[22]on
personalizedortunedASRmodels(e.g.,[4,5,6,7,8,9,10]), latentfeaturerepresentations. Incomparisonto[23]whocom-
whichleveragelargeproprietaryornon-commercialdatasetsof putes DTW directly on mel-frequency cepstrum coefficients
atypical speech (e.g., Project Euphonia [1]; UASpeech [11]; (MFCCs),ourapproach,likemorerecentapproaches[24],uses
AphasiaBank[12]).Inthiswork,wetakeamorepragmaticap- neural networks for feature learning, such that generally rele-
proachthatdoesnotrequirevastquantitiesofdata,yetenables vant but complex speech patterns can be extracted for open-
peoplewithseverespeechdifferencestotrainphraserecogni- vocabulary keyword modeling. Instaed of relying on phone-
tion models for applications where only a constrained set of basedapproachesasin[17,18],ourspeechembeddingmodel
phrasesisneeded. isdesignedtoencodeword-andphrase-likeattributes. Thisis
because dysarthric speech characteristics may vary with time
*Theseauthorscontributedequallytothiswork andcanleadtoinconsistencyinthepredictedphonesofagiven
3202
nuJ
8
]SA.ssee[
1v64450.6032:viXra
Enrollment Run-time Binary Cross Entropy (per class and frame)
Streaming
Audio Clip Audio silence [word] silence [word] silence
t=0 t=T
Sigmoid time x classes
Embedding Model Embedding Model at 100 Hz
Fully-Connected
Conv1D (k=1) w/ Weight Norm
Embedding
128d Matrix +
1d Speech Activity Leaky ReLU
Detection Conv1D (k=1) w/ Weight Norm
T frames @ 100 Hz
Speech Segment Extractor Leaky ReLU
Speech Segment Extractor Conv1D(k=5, d=i+1) w/ Weight Norm
6x blocks
Store Reference Zi Detection Logic Conv1D (k=1)
[128 x ti] DTW(Zi, Zj) Log Mel Spectrogram time a tx 1 f 0re 0q Hue zncy
Metadata
Duration (ti)
Phrases
Audio Data Augmentation
Phrase label (yi)
{p1, …, pP} Output: Detection Prediction Clean Aggressor
speech audio
(a) Phraseenrollmentandrun-timeinferenceusingLatentPhraseMatching. (b) SpeechEmbeddingModel
10
Figure1: (a)OverviewofphraseenrollmentandinferenceusingaQuery-by-ExampleapproacAphple. Co(nfbide)ntiaAl. Inrtecrnahl usiet oenlyc. ©t2u02r0 eAppale Innc.dtrainingdesign
detailsofthespeechembeddingmodel.
phraseandsubsequentlypoorphraserecognitionperformance. (4), ourkeywordspottingmodelistrainedbymixingcopious
Additionally, our approach does not require any training of amountsofbackgroundnoiserandomlywithspeechsuchthat
modelparameterswhenanewphraseisonboarded,unlikethe thelearnedspeechrepresentationsareimplicitlydenoised.
supervisedapproachesproposedin[14,16]. OurkeywordmodelinFigure1(b)isaResNet-styletem-
poral convolutional network designed using many recent best
2.1. LatentPhraseMatching(LPM) practices for large-scale CNN training [25] and is trained on
subsets of the Multilingual Spoken Words Corpus (MSWC)
Attrainingtime,auserenrollsaphrasebyspeakingeachphrase
[26]. The input to this model is 64-dimensional log melspec-
at least twice and a model is built per-phrase using a speech
trograms with a frame rate of 100 Hz. The output is per-
embedding model. The model for each phrase is parameter-
(cid:0) (cid:1) frame keyword predictions at 100 Hz for 300 common En-
ized by tuple p = Z ,y ,τ for index i ∈ {1...P} where
i i i i glish words plus an extra label for speech activity detection.1
Z isalatentphraserepresentation,y isaphraselabel,τ isa
i i i Thearchitectureconsistsofsixblocksofdilatedconvolutions
within-phrasedetectionthreshold,andP isthenumberofen-
with the pattern Conv1D(k=5, d=i+1), LeakyReLU,
rolledphrases.
Conv1D(k=1), LeakyReLUforthei-thblockwherekis
Atrun-time,candidatephrasesegmentsareextractedfrom
kernelsize,disdilationrate,andthereceptivefieldis850ms.
audio and compared to each enrolled phrase using DTW. If a
Eachblockhasaresidualconnection,weightnorm[27]incon-
spokenphraseissufficientlysimilartoanenrolledphrase,that
volutionallayers,anddropoutduringtraining. Fullyconnected
phraseisdetected,otherwiseitisclassifiedasanoutofdomain
layersareusedtoprojectthelatentfeaturestothedesiredoutput
utterance.Theenrollmentandinferencealgorithmsaredepicted
dimensionsandthefinalblockhasasigmoidactivationwhich
inFigure1.
iscompatiblewiththeBinaryCrossEntropy(BCE)lossfunc-
tion. We use a BCE loss per frame where at most one of the
2.1.1. PhraseRepresentation
possible words in our vocabulary is active. For training pur-
poses,weconcatenateMSWCclipstogethersoeachbatchisof
We posit that a phrase representation for dysarthric speech
the form [sil] [word] [sil] [word] [sil] ...,
shouldhavethefollowingproperties:(1)itencodeslocalword-
where start and stop times for each word are estimated using
like structure that supports phrases constructed from an un-
theMSWCannotations. Toimproverobustness,werandomly
boundedvocabulary,butembeddingsfortwopeoplesayingthe
addedbackgroundsoundstoaudioclips.
samewordneednotbesimilar;(2)itencodesglobalstructure
suchassequencingofword-likeinformation,butacknowledge Forourphraserepresentationsweuseintermediateembed-
thatthecadenceorvocalqualitymayvarythroughoutthedayor dings,2 where given an audio clip with t spectral frames, the
asspeechcharacteristicschange;(3)theembeddingsarecom- output is an embedding matrix Z i also with length t frames
pactsuchthatthemodeldoesnotoverfittospuriouscorrelations and with height f = 128. The speech activity detector label
fromasmallnumberoftrainingexamples;(4)lastly,thespeech is used to truncate silence at the beginning and end of a clip.
embeddingsarerobusttobackgroundnoiseoracousticchanges Thespeechactivitydetectorisappliedatbothenrollmentand
intheenvironment.
We satisfy (1) and (3) by constructing a sufficiently 1300wordschosenfromMSWCthathadatleast1000instancesand
variouslengthsfrom3to11letters. Forearlyexperimentswetrained
large vocabulary keyword detection model and using low-
modelswithdifferentlanguages,includingGermanandItalianbutre-
dimensionalword-likeintermediateembeddings.Wesatisfy(2)
sultsweresimilartoEnglish-onlyregardlessofthetestinglanguage.
bycomputingembeddingsataframerateof100Hzandstor- 2Thespecificembeddinglayerwasdeterminedbasedonearlyex-
ingastemplatestousewithDTWtocomputephrasesimilari- perimentation using EasyCall and was not changed for use with our
tiesthatareinvarianttosignalshiftingandscalingintime. For internaldataset.
evaluationsuchthatperiodsofpredictedspeechabsencearenot
100
consideredinthesimilaritycomputation.
2.1.2. PhraseSimilarity 75
Duringenrollmentofthei-thphrasewithinP,wecomputethe
pair-wise similarity between all phrases using DTW to deter- 50
mineawithin-phrasethresholdτ by
i
(cid:0) (cid:1) 25
τ =α· max DTW(Z ,Z ) , (1)
i i j
j∈{1...P}
yi=yj,i̸=j
0
Mild Moderate Severe
whereαisatuneableconstant(e.g.,α=1.25).Thepurposeof Yorkstondysarthriascale
τ istorejectphrasesthatarenotpartoftheenrolledset(e.g.,
i
read or conversational speech). When α = ∞ this detector Figure2: Per-participantWordErrorRates(WERs)ofacon-
becomesaphraseclassificationmodel. Thechoiceofαisde- sumerASRmodelonourinternaldataset.Severityisgradedby
terminedbythedesiredtrade-offbetweenrecallandprecision. anSLPusingtheYorkstondysarthriascale. SignificantSpear-
Atrun-time,ifthecomputedDTWscoreislowerthanany man’srankcorrelation(r(30)=0.76,p<.001)supportASR
within-phrasethreshold, theny j isthedetectedphrase, where performancedegradesasdysarthriaseverityincreases.
j indicatesthej-thphrasewiththelowestscore,otherwiseno
phrasewasdetected.
2,andverifiesthatASRdoesnotfullysupportmoderate-severe
3. Data
dysarthria. The average WERs on EasyCall phrases is 53.4%
3.1. InternalCollection forpeoplewithdysarthriaandand8.7%forthecontrolgroup.
To validate our approach, we collected audio recordings from 4.1. PhraseRecognitionBenchmark
32 participants with varying severity of dysarthria who iden-
tified as having a motor-speech disability. The data collec- For each phrase within P, a model is trained with utterances
tionspannedfivesessionsacrossmultipledaystoobservehow recorded within the same recording session and evaluated on
speech characteristics differed over time. In each session, a utterances exclusive to the training session. We evaluate for
participantrepeatedphrasesfivetimeseachwithamicrophone both in-domain and out-of-domain scenarios. In-domain ex-
placedfirstlyclosetotheirbodyandthenfartheronatablein perimentsconsideronlyspokenutteranceswithinP andeval-
frontofthem.Intotal,therewere50uniquecommonlyspoken uate phrase recognition using canonical classification metrics
phraseslike“Scrollup”or“Gotothehomescreen”. Addition- likerecallandprecision[30]. Wereportthein-domainperfor-
ally,participantsreadparagraph-longpassagesandspokeabout mancemetricsinTable1basedon5repeatedtrials.Weobserve
two topics freely. A Speech-Language Pathologist (SLP) lis- that phrase recognition performance generally decreases with
tenedtoandgradedthedatatoassessintelligibilityandseverity severity, howeverthedisparityinperformanceismuchhigher
accordingtotheDysarthriaRatingofSeverityscale[28]which fortheASR. LPMhassignificantlybetterrecognitionperfor-
rangesfrom1-5(1: nodetectablespeechdisorder,2: obvious manceforsevereandmoderatespeechdysarthria.Wealsoeval-
speechdisorderwithintelligiblespeech,3: reductioninintelli- uate for false detection rate (FDR) where FDR is defined as
gibility4: severedysarthriawithsignificantreductioninintel- thefrequencyaggressorspeechisfalselydetectedasoneofthe
ligibility,and5:minimaltonospeechproduction).Ourdataset phraseswithinP. Theaggressorspeechusedcomesfromthe
onlycontainsparticipantswhohavebeengraded2-4,sointhis readpassagesandareexclusivetoP. TheFDRsfortheASR
work,wereferto2asmild,3asmoderate,and4assevere. andLPMarerespectively0.0and0.34±0.22. ASRhasalow
FDR since it is able to detect a larger vocabulary of words to
3.2. EasyCallDataset discernbetweenaggressorspeechandenrolledphrases.
To quantify variability in phrase recognition performance
WealsousetheEasyCalldataset[20]whichcontainsrecordings
arisingfrommicrophoneplacementandutterancesrecordedon
from 31 participants with dysathria and 25 without dysathria.
thesamedayastrainingversusadifferentday,wereportonthe
EachparticipantvocalizedinItalian20phrasesand46words
averageper-participantaccuraciesforphrasesrecordednearor
repeated over 2 to 8 sessions. For each participant, we only
fartherduringthesamesessionastrainingoronsessionsfrom
evaluated on phrases that had at least three repetitions across
otherdaysinTable2. Weobservethatperformancevariability
sessions. We found many clips were incorrectly truncated or
aroseduetomicrophoneplacementandwhenrecordingswere
hadsignificantbackgroundspeechfromanon-participantand
madeonadifferentday.Accuraciesaretypicallythehighestfor
excludedthesefromourexperimentsaspartofaQAprocess;
conditionsmostsimilartothetrainingsession.Formoderateto
intotalweused18411of21386(86%)audioclips.3
severe,weobservedperformancedegradeswhentherecording
wasmadewiththemicrophoneplacedfartheroronadifferent
4. Results
daythanthetrainingsession.
Inourexperiments,weusetheAppleSpeechFramework[29]
4.2. Ablationstudies
asabaselineASRsystemanddefinePasallthephraseswithin
ourdataset.TheWERoftheASRonourinternaldatasetissig- 4.2.1. EasyCallExperiments
nificantlycorrelatedwithdysarthriaseverityasseeninFigure
We benchmark our approach on the EasyCall dataset against
3Pleaseemailthecorrespondingauthorsforspecificfilenames. the ASR baseline where a phrase is considered to be correct
REW
Table1:Meanandstandarddeviationofper-participantphrase Table3: PhraserecognitionaccuracyontheEasyCalldataset
detectionmetricsofbaselineASRsystem(ASR)andproposed withabaselineASRsystemandvariantsofLPMusingspectral
LPMfor50enrolledphrases. featuresorKWSembeddingswithinthephraserepresentation.
Models Severity Accuracy Precision Recall DataSubset ASR LPM LPM
Spec KWS
mild 0.78±0.13 0.92±0.07 0.73±0.14 Control(word) 0.85±0.04 0.79±0.15 0.84±0.12
ASR moderate 0.44±0.23 0.67±0.26 0.40±0.23 Control(phrase) 0.87±0.07 0.90±0.11 0.95±0.09
severe 0.02±0.01 0.07±0.05 0.02±0.01 Dysarthria(word) 0.55±0.20 0.61±0.21 0.65±0.20
all 0.54±0.30 0.71±0.31 0.50±0.29 Dysarthria(phrase) 0.63±0.26 0.72±0.21 0.82±0.16
mild 0.84±0.04 0.87±0.04 0.84±0.04
LPM moderate 0.78±0.15 0.80±0.14 0.78±0.14 1
severe 0.70±0.16 0.73±0.18 0.70±0.17 0.75
all 0.80±0.12 0.82±0.12 0.80±0.12 0.50 Method
0.25 ASRbase
0 LPM
5 10 20 30 49
Table2:Meanandstandarddeviationofper-participantphrase
Numberofphrases
detection accuracy conditioned on utterances recorded within
thesameorexclusivetotrainingsessions.
Figure 3: Distribution of average per-participant accuracy of
the baseline ASR system and LPM for varying number of en-
Models Severity Same(near) Same(far) Other rolledphrases.
mild 0.80±0.18 0.81±0.18 0.80±0.18
ASR moderate 0.52±0.25 0.45±0.25 0.46±0.25
severe 0.00±0.01 0.01±0.01 0.02±0.01 4.2.3. VaryingSignal-to-NoiseRatios
mild 0.76±0.29 0.76±0.19 0.75±0.17 Toinvestigatetheinfluenceofcommonhouseholdnoisesonde-
LPM moderate 0.88±0.09 0.79±0.23 0.81±0.13 tectionperformance,wecontrivedscenarioswherethephrases
severe 0.94±0.10 0.80±0.22 0.68±0.25 forevaluationwereaugmentedwithbackgroundsoundsatdif-
ferent signal-to-noise ratios (SNR). The recognition perfor-
mancesareshowninTable4whereweobservedegradationof
recognitionperformanceasSNRdecreasesforallapproaches.
The degradation also seems to be magnified with dysarthria
iftheASRcanverbatimrecognizetheonboardedphrase. Re-
severity. Regardless of the level of environmental noise, we
sultsareshowninTable3. Inaddition,wecompareagainsta
observe that LPM exhibits less model bias than the ASR and
baselineusingmelspectrogramfeaturesasthephraserepresen-
could conduce to a more inclusive phrase recognition system
tation. Ingeneral,longerphrases,whichtendtohavearicher
thananASR-basedapproach.
latentstructure, performbetteracrossapproaches. TheDTW-
based approaches perform significantly better than the ASR
baseline,especiallyfordysathricphraseswhereLPMaccuracy Table4:MeanandstandarddeviationofPrecisionofthebase-
is 30.5% higher than ASR. Based on qualitative analysis, the line ASR and LPM in presence of additive background noise
SpectralDTWbaselineappearstooverfittoenvironmentcon- contrivedsyntheticallywithrecordedhouseholdsounds.
ditionsandvariationinarticulationmuchmorethantheKWS-
DTW approach. We also compared our embedding model to Models Severity 5dB 20dB ∆
aConformer-basedASRencoder[31]usingESPNet[32]that
mild 0.87±0.12 0.95±0.04 0.08
was trained on Librispeech, which yields very similar results
ASR moderate 0.42±0.30 0.59±0.31 0.17
toourKWSembeddings,albeitatmuchhighercomputational
cost.4 severe 0.04±0.02 0.04±0.02 0.00
mild 0.72±0.07 0.85±0.04 0.13
LPM moderate 0.64±0.06 0.79±0.07 0.15
4.2.2. Varyingnumberofphrases
severe 0.57±0.10 0.73±0.07 0.16
ToinvestigatethesensitivityofLPMtothenumberofunique
enrolledphrases, denotedbyn, theexperimentinSection4.1
was repeated with various values of n ∈ [5, 10, 20, 30, 49].
5. Conclusions
For n=5 we ran 10 trials and for other values of n we ran 5
trials. Thedistributionoftheaveragedper-participantmetrics Inthisworkweproposeapersonalizedphraserecognitional-
areplottedinFigure3. TheASRdoesnotrelyonthetraining gorithm using query by example on latent representations of
phrasesandthusisindependentofn.Formoderateandsevere, speech. We conducted a novel data collection that accounts
LPMperformancedegradesasnincreases. Thisisrepresented for speech variability for people with varying severities of
bythepointsoutsideoftheinterquartilerangesinFigure3.Fu- dysarthria and used the collected data to benchmark our ap-
tureworkshouldlookatimprovingrobustnessforparticipants proach with a state-of-the-art ASR system. Experimental re-
withthesespeechpatterns. sultsshowevidencethatourapproachexhibitslessmodelbias
thanusingASRforphraserecognitionandpotentiallyenables
peoplewithmoderateandseveredysarthriatousespeechtech-
4Resultsnotshownduetospacelimitations. nology. Additionally,weconductedseveralablationstudiesto
ycaruccA
quantifythesensitivityofperformanceduetothedesignchoice [17] T.J.Hazen,W.Shen,andC.White,“Query-by-examplespoken
of speech representations, level of environmental noise, and termdetectionusingphoneticposteriorgramtemplates,”in2009
numberofphrasessupportedforphraserecognition. IEEE Workshop on Automatic Speech Recognition ‘I&’ Under-
standing,2009,pp.421–426.
6. References [18] L. J. Rodriguez-Fuentes, A. Varona, M. Penagarikano, G. Bor-
del,andM.Diez,“High-performancequery-by-examplespoken
[1] B. MacDonald, P.-P. Jiang, J. Cattiau, R. Heywood, R. Cave, term detection on the SWS2013 evaluation,” in IEEE Interna-
K. Seaver, M. Ladewig, J. Tobin, M. Brenner, P. Q. Nelson, tional Conference on Acoustics, Speech and Signal Processing
J.R.Green,andK.Tomanek,“Disorderedspeechdatacollection: (ICASSP),2014,pp.7819–7823.
Lessonslearnedat1millionutterancesfromProjectEuphonia,”
[19] G.Chen,C.Parada,andT.N.Sainath,“Query-by-examplekey-
inINTERSPEECH,2021,pp.4833–4837.
wordspottingusinglongshort-termmemorynetworks,”inIEEE
[2] V. Young and A. Mihailidis, “Difficulties in automatic speech InternationalConferenceonAcoustics, SpeechandSignalPro-
recognition of dysarthric speakers and implications for speech- cessing(ICASSP),2015,pp.5236–5240.
basedapplicationsusedbytheelderly: Aliteraturereview,”As-
[20] R. Turrisi, A. Braccia, M. Emanuele, S. Giulietti, M. Pugli-
sistiveTechnology,vol.22,no.2,pp.99–112,2010.
atti, M. Sensi, L. Fadiga, and L. Badino, “EasyCall corpus: A
[3] C. Lea, Z. Huang, J. Narain, L. Tooley, D. Yee, D. T. Tran, dysarthricspeechdataset,”inINTERSPEECH,Aug2021,pp.41–
G. Panayiotis, J. P. Bigham, and L. Findlater, “From user per- 45.
ceptionstotechnicalimprovement: Enablingpeoplewhostutter
[21] F. Rudzicz, A. K. Namasivayam, and T. Wolff, “The TORGO
tobetterusespeechrecognition,”inACMConferenceonHuman
databaseofacousticandarticulatoryspeechfromspeakerswith
FactorsinComputingSystems(CHI),ser.CHI’23,2023.
dysarthria,” Language Resources and Evaluation, vol. 46, pp.
[4] F.Xiong,J.Barker,Z.Yue,andH.Christensen,“Sourcedomain 523–541,2012.
dataselectionforimprovedtransferlearningtargetingdysarthric
[22] P. Senin, “Dynamic time warping algorithm review,” Informa-
speechrecognition,”inIEEEInternationalConferenceonAcous-
tionandComputerScienceDepartmentUniversityofHawaiiat
tics, Speech and Signal Processing (ICASSP), 2020, pp. 7424–
ManoaHonolulu,USA,vol.855,no.1-23,p.40,2008.
7428.
[23] T. B. Amin and I. Mahmood, “Speech recognition using dy-
[5] J.R.Green,R.L.MacDonald,P.-P.Jiang,J.Cattiau,R.Heywood,
namictimewarping,” in20082ndInternationalConferenceon
R.Cave,K.Seaver,M.A.Ladewig,J.Tobin,M.P.Brenneretal.,
AdvancesinSpaceTechnologies,2008,pp.74–79.
“Automatic speech recognition of disordered speech: Personal-
izedmodelsoutperforminghumanlistenersonshortphrases.”in [24] J.Huang,W.Gharbieh,Q.Wan,H.S.Shim,andC.Lee,“QbyE-
INTERSPEECH,2021,pp.4778–4782. MLPMixer: Query-by-example open-vocabulary keyword spot-
tingusingMLPMixer,”inINTERSPEECH,2022.
[6] J.TobinandK.Tomanek,“Personalizedautomaticspeechrecog-
nitiontrainedonsmalldisorderedspeechdatasets,”inIEEEInter- [25] R.Wightman, H.Touvron, andH.Je´gou, “Resnetstrikesback:
nationalConferenceonAcoustics,SpeechandSignalProcessing Animprovedtrainingprocedureintimm,”inNeurIPSWorkshop
(ICASSP),2022,pp.6637–6641. ImageNetPPF,2021.
[7] J.Shor,D.Emanuel,O.Lang,O.Tuval,M.Brenner,J.Cattiau, [26] M.Mazumder, S.Chitlangia, C.Banbury, Y.Kang, J.M.Ciro,
F. Vieira, M. McNally, T. Charbonneau, M. Nollstadt, A. Has- K.Achorn,D.Galvez,M.Sabini,P.Mattson,D.Kanter,G.Di-
sidim,andY.Matias,“PersonalizingASRfordysarthricandac- amos,P.Warden,J.Meyer,andV.J.Reddi,“Multilingualspo-
centedspeechwithlimiteddata,”inINTERSPEECH,2019. ken words corpus,” in Neural Information Processing Systems
(NeurIPS),2021.
[8] M.K.Baskar,T.Herzig,D.Nguyen,M.Diez,T.Polzehl,L.Bur-
get,andJ.H.Cˇernocky´,“SpeakeradaptationforWav2Vec2based [27] T. Salimans and D. P. Kingma, “Weight normalization: A
dysarthricASR,”inINTERSPEECH,2022. simple reparameterization to accelerate training of deep neural
networks,” Neural Information Processing Systems (NeurIPS),
[9] K.Tomanek,V.Zayats,D.Padfield,K.Vaillancourt,andF.Bi-
vol.29,2016.
adsy,“Residualadaptersforparameter-efficientASRadaptation
toatypicalandaccentedspeech,”inEmpiricalMethodsinNatu- [28] K.M.Yorkston,D.R.Beukelman,E.A.Strand,andM.Hakel,
ralLanguageProcessing(EMNLP),2021. Management of motor speech disorders in children and adults.
Pro-edAustin,TX,1999,vol.404.
[10] A. Xiao, W. Zheng, G. Keren, D. Le, F. Zhang, C. Fuegen,
O.Kalinli,Y.Saraf,andA.Mohamed,“ScalingASRimproves [29] Apple, “Speech framework,” https://developer.apple.com/
zeroandfewshotlearning,”INTERSPEECH,2022. documentation/speech,2022.
[11] F. Xiong, J. Barker, and H. Christensen, “Deep learning of [30] J.Eisenstein,“Chapter4: Evaluatingclassifiers,”inIntroduction
articulatory-basedrepresentationsandapplicationsforimproving tonaturallanguageprocessing. TheMITPress,2019.
dysarthricspeechrecognition,”inSpeechCommunication; 13th
[31] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu,
ITG-Symposium,2018,pp.1–5.
W.Han,S.Wang,Z.Zhang,Y.Wu,andR.Pang,“Conformer:
[12] B.MacWhinney,D.Fromm,M.Forbes,andA.Holland,“Aphasi- Convolution-augmented transformer for speech recognition,” in
abank: Methods for studying discourse,” Aphasiology, vol. 25, INTERSPEECH,2020.
no.11,pp.1286–1307,2011.
[32] S.Watanabe,T.Hori,S.Karita,T.Hayashi,J.Nishitoba,Y.Unno,
[13] I.Lopez-Espejo,Z.-H.Tan,J.H.L.Hansen,andJ.Jensen,“Deep N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen,
spokenkeywordspotting: Anoverview,” IEEEAccess, vol.10, A. Renduchintala, and T. Ochiai, “ESPnet: End-to-end speech
pp.4169–4199,2022. processingtoolkit,”inINTERSPEECH,2018,pp.2207–2211.
[14] S.Yang,B.Kim,I.Chung,andS.Chang,“Personalizedkeyword
spottingthroughmulti-tasklearning,”inINTERSPEECH,2022.
[15] M. Mazumder, C. R. Banbury, J. Meyer, P. Warden, and V. J.
Reddi,“Few-shotkeywordspottinginanylanguage,”inINTER-
SPEECH,2021.
[16] L. Lugosch, S. Myer, and V. S. Tomar, “DONUT: CTC-based
query-by-example keyword spotting,” in NeurIPS Workshop on
InterpretabilityandRobustnessforAudio,Speech,andLanguage,
2018.

IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 1
Prototypical Graph Contrastive Learning
Shuai Lin, Chen Liu, Pan Zhou, Zi-Yuan Hu, Shuojia Wang, Ruihui Zhao,
Yefeng Zheng, Liang Lin, Eric Xing, and Xiaodan Liang
Abstract‚ÄîGraph-level representations are critical in various
real-world applications, such as predicting the properties of ùê∫+
molecules. But in practice, precise graph annotations are gener- ùëñ
ally very expensive and time-consuming. To address this issue, false negative
graph contrastive learning constructs instance discrimination sample
taskwhichpullstogetherpositivepairs(augmentationpairsofthe
samegraph)andpushesawaynegativepairs(augmentationpairs ùê∫
ùëñ ùê∫‚àí
of different graphs) for unsupervised representation learning. ùëñ
However, since for a query, its negatives are uniformly sampled
uniform
from all graphs, existing methods suffer from the critical sam-
plingbiasissue,i.e.,thenegativeslikelyhavingthesamesemantic sampling
structure with the query, leading to performance degradation.
To mitigate this sampling bias issue, in this paper, we propose ‚Ä¶
a Prototypical Graph Contrastive Learning (PGCL) approach.
Specifically, PGCL models the underlying semantic structure of
the graph data via clustering semantically similar graphs into
the same group, and simultaneously encourages the clustering Fig. 1. ‚ÄúSampling Bias‚Äù: The strategy of sampling negative examples
consistencyfordifferentaugmentationsofthesamegraph.Then uniformly from the data distribution G could result in that the sampled
given a query, it performs negative sampling via drawing the negativesG‚àí i aresemanticallysimilartothequeryGi,e.g.,theyallcontain
graphs from those clusters that differ from the cluster of thehexagonalstructurethatresemblesabenzenering.
query, which ensures the semantic difference between query
and its negative samples. Moreover, for a query, PGCL further LEARNING graph representations is a fundamental prob-
reweights its negative samples based on the distance between
lem in a variety of domains and tasks, such as molecular
theirprototypes(clustercentroids)andthequeryprototypesuch
that those negatives having moderate prototype distance enjoy propertiespredictionindrugdiscovery[1],[2],proteinfunction
relatively large weights. This reweighting strategy is proved to forecastinbiologicalnetworks[3],[9],andcommunityanalysis
be more effective than uniform sampling. Experimental results in social networks [10]. Recently, Graph Neural Networks
on various graph benchmarks testify the advantages of our
(GNNs) [1], [11], [12] have attracted a surge of interest and
PGCL over state-of-the-art methods. Code is publicly available
showed the effectiveness in learning graph representations.
at https://github.com/ha-lins/PGCL.
These methods are usually trained in a supervised fashion,
Index Terms‚ÄîContrastive learning, Self-supervised learning,
which demands the task-specific labeled data. However, there
Graph representation learning.
aresomeaspectsofshortcomingsforthesupervisedtrainingof
GNN. Firstly, task-specific labels can be quite scarce for graph
I. INTRODUCTION datasets (e.g., labeling biology and chemistry graph through
human annotations are often resource-intensive). Secondly,
due to the limited size of graph datasets, supervised GNNs
ThisworkwassupportedinpartbyNationalKeyR&DProgramofChina are often confronted with the over-fitting and over-smoothing
underGrantNo.2020AAA0109700,NationalNaturalScienceFoundationof problems [4], which limit their generalization capability to
China(NSFC)underGrantNo.61976233,GuangdongProvinceBasicandAp-
other tasks [76]. Therefore, it is highly desirable to learn the
pliedBasicResearch(RegionalJointFund-Key)GrantNo.2019B1515120039,
GuangdongOutstandingYouthFund(GrantNo.2021B1515020061),Shenzhen transferable and generalizable graph representations in a self-
FundamentalResearchProgram(ProjectNo.RCYX20200714114642083,No. supervised way on the large scale pre-training graph data.
JCYJ20190807154211365) and CAAI-Huawei MindSpore Open Fund. We
To this end, self-supervised approaches, such as generative
thankMindSporeforthepartialsupportofthiswork,whichisanewdeep
learningcomputingframework(https://www.mindspore.cn).(ShuaiLinand methods [14], [71] predictive methods [68] and contrastive
ChenLiuhavecontributedtothisworkequally.Correspondingauthor:Xiaodan methods [13], [15], [18], are coupled with GNNs to enable
Liang.)
the graph representation learning leveraging unlabelled data.
ShuaiLin,ChenLiuandXiaodanLiangarewiththeSchoolofIntelligent
SystemsEngineering,SunYat-senUniversity,Shenzhen518107,China(e-mail: The learned representations from well-designed self-supervised
shuailin97@gmail.com;cathyliu41@gmail.com;xdliang328@gmail.com). pretexttasksarethentransferredtodown-streamtasks.Inspired
Pan Zhou is with Sea AI Lab, Galaxis 138522, Singapore (e-mail:
bytheadvancesofcontrastivelearninginthosedomains,graph
panzhou3@gmail.com).
Zi-yuanHuandLiangLinarewiththeSchoolofComputerScienceand contrastivelearninghavebeenproposedandthenattractedhuge
Engineering,SunYat-senUniversity,Guangzhou510006,China. attention for graph representation learning.
ShuojiaWang,RuihuiZhaoandYefengZhengarewiththeTencentJarvis
Lab,Tencentbuilding,ShennanAvenue,Shenzhen518000,China(e-mail:sky- Graph contrastive learning is mainly based on maximizing
lawang@tencent.com;zacharyzhao@tencent.com;yefengzheng@tencent.com). the agreement of two views extracted from the same graph
Eric Xing is with School of Computer Science, Mohamed bin Zayed
against those from different graphs. The former are regarded
UniversityofArtificialIntelligence,MasdarCity,AbuDhabi,UAE(e-mail:
eric.xing@mbzuai.ac.ae). as positive pairs and later as negative ones. Specifically, three
2202
luJ
42
]GL.sc[
2v54690.6012:viXra
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 2
sequential components should be well-designed for graph ‚Ä¢ We propose prototypical graph contrastive learning, a novel
contrastive learning, namely data augmentation, pretext task framework that clusters semantically similar graphs into the
and contrastive objectives [76], [77]. The first is the key for same group and simultaneously encourages the clustering
generating multiple appropriate views. Due to the inherent consistency between different augmentations of the same
non-Euclideanpropertiesofgraphdata,itisdifficulttodirectly graph.
apply data augmentations designed for images to graphs. ‚Ä¢ We design a reweighted contrastive objective, which
Typical graph data augmentations includes shuffling node reweights the negative samples based on their prototype
features [69], [70], perturbing structural connectivity through distance, to mitigate the sampling bias issue.
adding, masking and deleting nodes [68], [72] and sub-graph ‚Ä¢ Combining both technical contributions into a single model,
sampling[18]etc.Thepretexttaskofgraphcontrastivelearning PGCL outperforms instance-wise contrastive learning on
contrasts two graph views at the same-scale or different scales. multiple datasets in the task of unsupervised graph classifi-
The scale of the view may be local, contextual, or global, cation.
corresponding to the node-level, subgraph-level, or graph-level
information in the graph [13], [28], [59], [73]‚Äì[75]. The main II. RELATEDWORK
way to optimize the graph contrastive objective is maximizing
A. Graph Representation Learning
the Mutual Information (MI) or the lower-bounds of MI (e.g.,
Traditionally, graph kernels are widely used for learning node
InfoNCE loss [6]) for two views. Typically, GraphCL [18]
and graph representations. This common process includes
introduces four types of graph augmentations (namely node
meticulous designs like decomposing graphs into substructures
dropping, edge peturbation, attribute masking and subgraph
and using kernel functions like the Weisfeiler-Leman graph
sampling) and optimizes the InfoNCE loss on graph-level
kernel[21]tomeasuregraphsimilaritybetweenthem.However,
augmentations.
they usually require non-trivial hand-crafted substructures and
However,allthesegraphcontrastivemethodssufferfromthe
domain-specifickernelfunctionstomeasurethesimilaritywhile
following limitations. Firstly, existing methods mainly focus
yielding inferior performance on downstream tasks, such as
on modeling the instance-level structure similarity but fail to
node classification and graph classification. Moreover, they
discover the underlying global structure over the whole data
often suffer from poor scalability [22] and huge memory
distribution. But in practice, there are underlying global struc-
consumption [23] due to some procedures like path extraction
tures in the graph data in most cases. For example, the graph
and recursive subgraph construction. Recently, there has been
MUTAG dataset [12] is a dataset of mutagenic aromatic and
increasinginterestinGraphNeuralNetwork(GNN)approaches
heteroaromatic nitro compounds with seven discrete categories
for graph representation learning and many GNN variants have
which have underlying global structures but are not labeled to
been proposed [11], [12], [24]. A general GNN framework
boost the representation learning. Secondly, as shown in Fig.
involves two key computations for each node at every layer:
1, for a query, the common practice of sampling negatives
(1)AGGREGATEoperation:aggregatingmessagesfromneigh-
uniformly from the whole data distribution could result in the
borhood; (2) UPDATE operation: updating node representation
factthatnegativesareactuallysemanticallysimilartothequery.
fromitsrepresentationinthepreviouslayerandtheaggregated
However, these ‚Äúfalse‚Äù negatives but really ‚Äúright‚Äù positives
messages. However, they mainly focus on supervised settings
are undesirably pushed apart by the contrastive loss. This
and differ from our unsupervised representation learning.
phenomenon, which we call ‚Äúsampling bias‚Äù, can empirically
lead to significant performance degradation [20]. Essentially,
B. Contrastive Learning for GNNs
instance-wise contrastive learning learns an embedding space
that only preserves the local similarity around each instance There are some recent works that explored graph contrastive
but largely ignores the global semantic structure of the whole learning with GNNs in the aspects of data augmentations
graph data. [18], [27], pretext task designs [25], [26] and contrastive
In this paper, we propose prototypical graph contrastive objective [16]. These methods can be mainly categorized into
learning (PGCL), a new framework that clusters semantically twotypes:global-localcontrastandglobal-globalcontrast.The
similar graphs into the same group and simultaneously encour- first category [13], [16], [28], [29], [62] follows the InfoMax
agestheclusteringconsistencybetweendifferentaugmentations principletomaximizethetheMutualInformation(MI)between
of the same graph. The global semantic structure of the entire the local feature and the context representation. Another line
datasetisdepictedbyPGCLinprototypevectors(i.e.,trainable of graph contrastive learning approaches called global-global
clustercentroids).Moreover,toaddressthesamplingbiasissue, contrast [8], [17], [18], [25] directly studies the relationships
we perform negative sampling via selecting the graphs from between the global context representations of different samples
thoseclustersthatdifferfromthequerycluster.Specifically,we aswhatmetriclearningdoes.Specifically,DeepGraphInfomax
devise a reweighted contrastive objective, which reweights the (DGI) [28] adapted the idea from Deep InfoMax (DIM) to
negativesamplesbasedonthedistancebetweentheirprototypes graphs for node representation learning via contrasting local
andthequeryprototype.Inthisway,thosenegativepairshaving node and global graph encodings with a summary vector.
moderate prototype distance enjoy relatively large weights, Further inspired by Deep Graph Infomax(DGI), InfoGraph
which ensures the semantic difference between the query and [13] extends DIM to learn graph-level representations by
its negative samples. In short, the contributions of this paper maximizing the agreements between the representations of
can be summarized as follows: entire graphs and the representations of substructures of
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 3
different scales (e.g., nodes, edges, triangles). MVGRL [16] graph augmentation schemes, augmented node attribute and
trainstheencodersthroughmaximizingthemutualinformation topological graph structure are projected to low-dimensional
fromdifferentstructuralviewsofgraphsaswell,whilethesame vectors. Intra-cluster nodes were pulled together and inter-
graph‚Äôs adjacency and diffusion matrix were assumed as local cluster nodes were pushed away in this process. Pre-GNN [66]
and global views of a graph. Contrasting encodings between designsanoveliterativefeatureclusteringmodulethatcouldbe
node representations of one view and graph representations of easily plugged into GCN. It‚Äôs based on feature clustering and
another view and vice versa yield better results compared with the pseudo labels predicted can both be updated in a EM-like
contrasting on local-local and global-global levels. DiGCL style, which can further facilitate the node classification. Liu
[62] extends the constractive paradigm to directed graphs et al. [64] proposes a multilayer graph contrastive clustering
and aims at learning on abundant views while retaining the network, which clusters the nodes into different communities
original structure information. Candidate views at topological- according to their relation types. Representations of the same
and feature-level are generated by a Laplacian perturbation node in different layers and different nodes were pulled closer
operation on adjacency and node feature matrix. GNN-based and pushed away respectively by a contrastive objective. NCL
encoder could therefore be adopted to the augmented veiws [89] utilizes the cluster-based graph contrastive learning in the
afterwards. CGCN [84] explores whether unsupervised graph area of recommendation. For the multi-view attributed graph
learning can boost the semi-supervised learning. In contrast, data, MCGC [65] learns the consensus graph by weighing
PGCL focuses on unsupervised graph contrastive learning. different views and regularizes by graph contrastive loss. [82]
More recently, MoCL [7] and KCL [8] both introduce the proposes graph pretraining approach for the heterogeneous
domain knowledge (e.g., manual rules and knowledge graph) graph, i.e., containing different types of nodes and edges.
into GCL and design knowledge-guided graph augmentation [83] utilizes the neural graph matching to pretrain graph
approaches to extract views. They boost the performance neural network. Concurrent to our work, GraphLoG [61] also
on the molecular graph since they have learned from the bring together a clustering objective with graph representation
knowledge-guided augmented graphs. However, all these learning. Similar to PCL [81], GraphLoG applies K-means
methods above are only able to model the discriminative clustering to capture the graph semantic structure but utilizing
relations between different graph instances while they fail K-means trivially could lead to imbalanced assignments of
to discover the underlying semantic structure of the data prototypes [32]. Compared to GraphLoG, the proposed PGCL
distribution. Meanwhile, randomly uniform negative sampling adds the constraint that the prototype assignments must be
could leads to obtain the ‚Äúfalse‚Äù negative pairs [20], [30], partitioned in equally-sized subsets and formulates it as an
[86]. This sampling bias phenomenon can empirically lead to optimal transport problem. Moreover, PGCL aims to solve the
significant performance degradation [20]. Therefore, pondering sampling bias via sampling negatives from the clusters that
how to sampling negative pairs with both structural and global differ from the query cluster and also reweighting negatives
semantic information carefully is the key process for graph according to their prototype distances.
contrastive learning. To mitigate the sampling bias issue,
AFGRL [86] proposes an augmentation-free self-supervised
III. PRELIMINARIES
method by merely generating positive pairs given a target node A. Problem Definition
embedding for node-level tasks, while PGCL focus on the Adesirablerepresentationshouldpreservethelocalsimilarity
graph-level tasks. among different data instances. We give the more detailed
discussions following [61]:
Local-instance Structure. We refer to the local pairwise
C. Clustering-based Contrastive Learning
similaritybetweenvariousgraphexamplesasthelocal-instance
Our work is also related to clustering-based representation structure [36], [61]. In the paradigm of contrastive learning,
learning methods [32]‚Äì[35], [46], [58], [60], [61], [79]‚Äì[81], the embeddings of similar graph pairs are expected to be close
[90], [91]. Among them, DeepCluster [58] and PCL [81] show in the latent space while the dissimilar pairs should be mapped
that K-means assignments can be used as pseudo-labels to far apart.
learn visual representations. PCL [81] introduces prototypes as The modeling of local-instance structure alone is usually
latentvariablestohelpfindthemaximum-likelihoodestimation insufficient to discover the global semantics underlying the
of the network parameters in an Expectation-Maximization entire data set. It is highly desirable to capture the global-
framework, which encourages representations to be closer to semantic structure of the data, which is defined as follows:
their prototypes. Other works [32], [34] show how to cast Global-semanticStructure.Graphdatafromtherealworld
the pseudo-label assignment problem as an instance of the can usually be organized as semantic clusters [37], [61]. The
optimal transport problem. However, these methods are mainly embeddingsofnearbygraphsinthelatentspaceshouldembody
developedforimagesinsteadofgraph-structureddata.Different the global structures, which reflect the semantic patterns of the
data requires distinct solutions, e.g., data augmentations. In original data.
contrast,[64]‚Äì[67],[82],[83],[87]recentlyadapttheclustering Problem Setup. Given a set of unlabeled graphs G =
idea to graph domain. {G }N , the problem of unsupervised graph representation
i i=1
Concretely, a self-supervised contrastive attributed graph learning aims at learning the low-dimensional vector z ‚ààRD
i
clustering approach [67] is proposed to benefit from imprecise of every graph G ‚àà G which is favorable for downstream
i
clusteringlabelsforthenodeclassificationtask.Withstochastic tasks, such as graph classification.
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 4
B. Graph Neural Networks contrastive objective to sample the negatives from different
clustersandreweightthemaccordingtotheirprototypedistance.
In recent years, graph neural networks (GNNs) [11], [12],
In the following, we introduce the PGCL in detail.
[38] have emerged as a promising approach for learning
representations of graph data. We represent a graph instance
as G = (V,E) with the node set V and the edge set E. The
A. Clustering Consistency for Correlated Views
dominant ways of graph representation learning are graph
neural networks with neural message passing mechanisms Formally, consider a graph neural network z i = f Œ∏(G i)
[39]: at the k-th iteration/layer, node representation hk v for mapping graph example G i to representation vectors z i ‚ààRD.
every node v ‚ààV is iteratively computed from the features of We can cluster all representations z i into K clusters whose
their neighbor nodes N(v) using a differentiable aggregation centroids are denoted by a set of K trainable prototype vectors
function. Specifically, at the iteration k we get the embedding {c 1,...,c K}.Prototypevectorsaretrainableweightmatrixofa
of node v in the k-th layer as: feedforwardnetworkandinitializedwithHeinitialization[78].
For brevity, we denote by C ‚àà RK√óD the matrix whose
hk
v
=COMBINEk(hk v‚àí1,AGGREGATEk({hk u‚àí1,‚àÄu‚ààN(v)})) (1)
columns are c ,...,c . In practice, C could be implemented
1 K
Then the graph-level representations can be attained by aggre- by a single linear layer. In this way, given a graph G i, we can
gating all node representations using a readout function, that perform clustering by computing the similarity between the
is, representation z i =f Œ∏(G i) and the K prototype as follows:
f (G )=READOUT({CONCAT({hk}K )}N ) (2) p(y|z )=softmax(C¬∑f (G )). (4)
Œ∏ i j k=1 j=1 i Œ∏ i
where f Œ∏(G i) is the entire graph‚Äôs embedding and READOUT Similarly, the prediction, i.e., p(y|z(cid:48)), of assigning G(cid:48) to
i i
represents averaging or a more sophisticated graph-level prototypes can also be computed with its representation z(cid:48). To
i
pooling function [40], [41]. encourage the clustering consistency between two correlated
viewsG andG(cid:48),wepredicttheclusterassignmentsofG(cid:48) with
i i i
C. Graph Contrastive Learning the representation z i (rather than z i(cid:48)) from the correlated view
and vice versa. Formally, we define the clustering consistency
ToempowertheGNNpre-trainingwithunlabeleddata,graph
objective via minimizing the average cross-entropy loss:
contrastivelearning(GCL)hasbeenexploredalotrecently[13],
[16]‚Äì[18]. GCL performs pre-training through maximizing the K
(cid:88)
agreement between two augmented views of the same graph (cid:96)(p i,q i(cid:48))=‚àí q(y|z i(cid:48))logp(y|z i) (5)
via a contrastive loss in the latent space. GCL first augments y=1
the given graph to get augmented views G and G(cid:48), which are
i i where q(y|z(cid:48)) is the prototype assignment of the view G(cid:48)
correlated(positive)pairs.ThenG andG(cid:48) arefedrespectively i i
i i and can serve as the target of the prediction p(y|z ) with z .
into a shared encoder f (including GNNs and a following i i
Œ∏ The consistency objective acts as a regularizer to encourage
projection head) for extracting graph representations z , z(cid:48) =
i i the similarity of views from the same graph. We can obtain
f (G ),f (G(cid:48))ThenacontrastivelossfunctionL(¬∑)isdefined
Œ∏ i Œ∏ i another similar objective if we swap the positions of z and
to enforce maximizing the consistency between positive pairs i
z(cid:48) in Eqn. (5) and the ultimate consistency regularizer can be
z ,z(cid:48) comparedwithnegativepairs,suchasInfoNCEloss[19], i
i i derived by the sum of two objectives:
[42], [43]:
(cid:88)n
L InfoNCE=‚àí(cid:88) i=n 1log
exp(cid:0) zi¬∑z(cid:48)
i/œÑ(cid:1)ex +p (cid:80)(cid:0) z
2
ji =N¬∑ 1z ,j(cid:48) i (cid:54)=/œÑ i(cid:1)
exp(zi¬∑zj/œÑ)
L consistency = i=1[(cid:96)(p i,q i(cid:48))+(cid:96)(p i(cid:48),q i)]. (6)
(3) The consistency regularizer can be interpreted as a way
where z i and z i(cid:48) are positive embeddings for graph G i, and z j of contrasting between multiple graph views by comparing
denotes the embedding of a different graph G j (i.e., negative their cluster assignments rather than their representations. In
embeddings), and œÑ is temperature hyper-parameter. Similar to practice, optimizing the distribution q faces the degeneracy
[61], [63], in the graph-structured data, there is an underlying problemsinceEqn.(5)canbetriviallyminimizedbyallocating
setofdiscretelatentclassesC thatrepresentsemanticstructures, all data samples to a single prototype. To avoid this, we add
which could result in that G i and G j are actually similar. the constraint that the prototype assignments must be equally
partitioned following [34]. We calculate the objective in a
IV. PROTOTYPICALGRAPHCONTRASTIVELEARNING minibatch manner for an efficient online optimization as:
In this section, we introduce the Prototypical Graph Con- (cid:88)N N
trastive Learning (PGCL) approach. Our goal is to cluster m p,i qnL consistency s.t. ‚àÄy:q(y|zi)‚àà[0,1] and q(y|zi)= K. (7)
semantically similar graphs into the same group and simultane- i=1
ously encourage the clustering consistency between different Theconstraintsmeanthattheprototypeassignmentstoclusters
augmentations of the same graph (i.e., correlated views). q(y|z ) of each graph example x are soft labels and that,
i i
As shown in Fig. 2, the representations of correlated views overall, the N graph examples within a minibatch are split
are encouraged to be clustered to have the same prototype uniformly among the K prototypes. The objective in Eqn. (7)
(cluster centroid). Moreover, PGCL also designs a reweighted is an instance of the optimal transport problem, which can be
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 5
ùëß
ùëñ
Clustering
Encoder
ùê∫
ùëñ
ùëì
ùëá ùúÉ
1
Prototypes
‚Ä¶
Reweighted Clustering
‚Ä¶
Contrast Consistency
‚Ä¶
ùê∫
ùëá Clustering
2 ‚Ä≤ Encoder
ùê∫
ùëñ ùëì
ùúÉ
ùëß‚Ä≤
ùëñ
Fig.2. OverviewofPGCL.TwographdataaugmentationsT1 andT2 areappliedtotheinputgraphG.ThentwographviewsGi andG(cid:48)
i
arefedintothe
sharedencoderf
Œ∏
(includingGNNsandaprojectionhead)toextractthegraphrepresentationszi andz i(cid:48).Weperformtheonlineclusteringviaassigningthe
representationsofsampleswithinabatchtoprototypevectors(clustercentroids).Therepresentationsarelearnedviaencouragingtheclusteringconsistency
between correlated views (Section IV-A) and a reweighted contrastive objective (Section IV-B), where prototype vectors are also updated along with the
encoderparametersbyback-propragation.
addressed relatively efficiently. For more clarity, we denote some previous works [20], [47] propose to approximate the
two K√óN matrices of joint probabilities as: underlying‚Äútrue‚Äùdistributionofnegativeexamplesbyadopting
1 1 a PU-learning viewpoint [48]. However, such approximation
P = Np(y|z i); Q= Nq(y|z i). (8) is sensitive to the hyperparameter choice and cannot avoid
sampling the semantically similar pairs essentially. Given a
Then we can impose an equal partition by enforcing the matrix
query (and its cluster), we can achieve this simply by drawing
Q to be a transportation polytope following [32], [34] in the
‚Äútrue‚Äù negative samples from different clusters. Since different
minibatch manner:
clusters represent distinct underlying semantics, such sampling
(cid:26) (cid:27)
1 1
T = Q‚ààRK√óN | Q1 = 1 ,Q(cid:62)1 = 1 , (9) strategy can ensure the semantic differences between the query
+ N K K K N N and its negatives, and Eqn. (3) can be extended to:
where1 and1 denotesthevectorofalloneswithdimension
N K
of N and K, respectively. Then the loss function in Eqn. (7)
can be rewritten as:
m p,i qnL consistency = Qm ‚àài Tn(cid:104)Q,‚àílogP(cid:105)‚àílogN, (10) L=‚àí(cid:88) i=n 1log exp(cid:0) zi¬∑z(cid:48) i/œÑ(cid:1) +(cid:80)ex 2 jp =N 1(cid:0) ,z ji (cid:54)=¬∑ iz 1(cid:48) i c/ iœÑ (cid:54)=(cid:1)
cj
¬∑exp(cid:16) zi¬∑z(cid:48) j/œÑ(cid:17)
(12)
where (cid:104)¬∑(cid:105) is the Frobenius dot-product between two matrices
where c and c are the prototype vectors of graphs G and G
and log is applied element-wise. Optimizing Eqn. (10) always respectivi ely,anj d1 istheindicatorthatrepresentsi whethej r
leads to an integral solution despite having relaxed Q to ci(cid:54)=cj
two samples are from different clusters. In this way, selected
the continuous polytope T instead of the discrete one. We
negative samples can enjoy desirable semantic difference from
solve the transport problem via utilizing the Sinkhorn-Knopp
the query and those similar ones are ‚Äúmasked‚Äù out in the
algorithm [45] and the solution of Eqn. (10) takes the form
objective.
as:
Q=Diag(Œ±)PŒ∑Diag(Œ≤) (11) Beyond selecting negative samples based on the distinction
of their clusters, we would like to avoid selecting too easy
where Œ± and Œ≤ are two renormalization vectors and the
samples that are far from the query in the latent space.
exponentiation is element-wise. Here Œ∑ is chosen to trade
Furthermore, intuitively, the desirable negative samples should
off convergence speed with closeness to the original transport
have a moderate distance to the query. Empirically, we found
problemanditisafixedvalueinourcase.Therenormalization
that controlling their prototype distances performs better than
vectors can be calculated using matrix multiplications with the
using their direct sample distance. As illustrated in Fig. 3, on
Sinkhorn-Knopp algorithm [45]. Note that the first term of
the one hand, if the prototypes of negatives are too close to
Eqn. (10) is (cid:104)Q,‚àílogP(cid:105), while that is (cid:104)Q,P(cid:105) in Eqn. (2) of
the query‚Äôs prototype, negatives could still share the similar
[45]. Thus the original exponential term is replaced with PŒ∑.
semanticstructurewiththequery(e.g.,thenearbycyancluster).
On the other hand, if the prototypes of negatives (such as the
B. Reweighted Contrastive Objective
purple cluster) are far from the prototype of a query, it means
In this section, we introduce how to mitigate the sampling negatives and query are far from each other and can be well
biasissueviasamplinggraphsfromdistinctclusterstothequery distinguished, which actually does not help the representation
and reweighting the negative samples. In the image domain, learning.
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 6
Algorithm 1 Pseudocode of Prototypical Graph Contrastive
Learning (PGCL) in Pytorch-like style.
0.0
# C: prototypes (DxK)
0.0 # model: GIN + projection head
0.51 # temp: temperature
for x in loader: # load a batch x with N samples
0.13 0.09 G G1 2 = = T T1 2( (x x) ) # # T T1 2 i is s a a r aa nn od to hm erau rg am ne dn ot mat ai uo gn mentation
0.27 z = model(cat(G1, G2)) # embeddings: 2NxD
scores = mm(z, C) # prototype scores: 2NxK
scores1 = scores[:N]
scores2 = scores[N:]
# cluster assignments
with torch.no_grad():
q1 = sinkhorn(scores)
q2 = sinkhorn(scores2)
# convert scores to probabilities
query negatives prototype mask reweight p1 = Softmax(scores1 / temp)
p2 = Softmax(scores2 / temp)
# clustering consistency loss
Fig.3. Illustrationofthenegativesamplereweighting.Thelinewidthof Loss_Consistency = - 0.5 * mean(q2 * log(p1) + q1 *
log(p2))
thearrowdenotestheweightvalue.
# reweighted contrastive loss
Compute Loss_Reweighted according to Eq.(13).
n # final loss
(cid:88) loss = Loss_Reweighted + n * Loss_Consistency
L =‚àí (13)
Reweighted
i=1 # SGD update: network and prototypes
loss.backward()
log
exp(z i¬∑z(cid:48) i/œÑ) update(model.params)
exp(z ¬∑z(cid:48)/œÑ)+M (cid:80)2N 1 ¬∑w ¬∑exp(cid:0) z ¬∑z(cid:48)/œÑ(cid:1) update(C)
i i i j=1,j(cid:54)=i ci(cid:54)=cj ij i j
# normalize prototypes
where w ij is the weight of negative pairs (G i, G j) and M i = with torch.no_grad():
2N is the normalization factor. We utilize the cosine C = normalize(C, dim=0, p=2)
(cid:80)2 j=N 1wij
distance to measure the distance between two prototypes c
i
and c as: D(c ,c ) = 1‚àí ci¬∑cj . Then we define the
weighj t based oni thj e above pr(cid:107) oc ti o(cid:107) t2 y(cid:107) pc ej(cid:107) d2istance with the format We further apply PGCL to the transfer learning setting to test
the out-of-distribution performance. Finally, we perform the
of the Gaussian function as:
(cid:40) (cid:41) extensive experiments for analysis, including ablation studies,
[D(c ,c )‚àí¬µ ]2
w =exp ‚àí i j i (14) sensitivity analysis and visualization on the unsupervised
ij 2œÉ2
learning datasets.
i
where ¬µ and œÉ are the mean and standard deviation of
i i A. Unsupervised Learning
D(c ,c ) for query G , respectively.
i j i
a) Task and datasets: We conduct experiments by com-
Asshown inFig.3,thereweightingstrategyencouragesthat
paringwiththestate-of-the-artcompetitorsontheunsupervised
lagerweightsareassignedtomeaningfulnegativesamples(such
graph classification task [13], [18], where we only have access
asfromtheblueandorangeclusters)withamoderateprototype
to all unlabeled samples in the dataset. We pre-train using the
distance to the query and smaller weights to too easy negative
whole dataset to learn graph embeddings and feed them into
samples (e.g., from the purple cluster) and ‚Äúfalse‚Äù negative
a downstream SVM classifier with 10-fold cross-validation.
samples (from the nearby cyan cluster). The strategy is similar
For this task, we conduct experiments on seven well-known
tothosein[49],[50]buttheyapplyitontrainingsamplesunder
benchmark datasets [51] including four bioinformatics datasets
supervised learning while we adopt it for selecting negative
(MUTAG, PTC, PROTEINS, NCI1) and three social network
samplesofself-supervisedlearning.Thefinaltrainingobjective
datasets (COLLAB, RDT-B and RDT-M5K) with statistics
couples L and L as:
Reweighted Consistency
summarized in Table I.
L=L Reweighted+ŒªL Consistency (15) b) Baselines: In the unsupervised graph classification,
PGCL is evaluated following [13], [18]. We compare our
where the constant Œª balances the reweighted contrastive loss
results with five graph kernel methods including Graphlet
L and the consistency regularizer L .
Reweighted Consistency
Kernel (GL) [52], Weisfeiler-Lehman Sub-tree Kernel (WL)
This loss function is jointly minimized with respect to the
[21], Deep Graph Kernels (DGK) [53], Multi-Scale Laplacian
prototypes C and the parameters Œ∏ of the graph encoder used
Kernel (MLG) [23] and Graph Convolutional Kernel Network
to produce the representation z .
i (GCKN1) [54]. We also compare with four supervised GNNs
V. EXPERIMENTS 1We report our reproduced results of GCKN for fair comparisons since
theoriginalGCKNpaperadoptsdifferenttrain-testsplitsfornested10-fold
This section is devoted to the empirical evaluation of the
cross-validation,whichisdifferentfromtheStratified10foldsplitsofother
PGCL approach. Our initial focus is on unsupervised learning. contrastivelearningworks[13],[16]andours.
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 7
Datasets MUTAG PTC PROTEINS NCI1 COLLAB RDT-B RDT-M5K
# graphs 188 344 1113 4110 5000 2000 2000
Avg # nodes 17.9 14.3 39.1 29.9 74.5 429.6 429.6
GRAPHSAGE [39] 85.1¬±7.6 63.9¬±7.7 75.9¬±3.2 77.7¬±1.5 - - -
GCN [11] 85.6¬±5.8 64.2¬±4.3 76.0¬±3.2 80.2¬±2.0 79.0¬±1.8 50.0¬±0.0 20.0 ¬± 0.0
GIN-0 [12] 89.4¬±5.6 64.6¬±7.0 76.2¬±2.8 82.7¬±1.7 80.2¬±1.9 92.4¬±2.5 57.5¬±1.5
GIN-(cid:15) [12] 89.0¬±6.0 63.7¬±8.2 75.9¬±3.8 82.7¬±1.6 80.1¬±1.9 92.2¬±2.3 57.0¬±1.7
GL [52] 81.7¬±2.1 57.3¬±1.4 - 53.9¬±0.4 56.3¬±0.6 77.3¬±0.2 41.0¬±0.2
WL [21] 80.7¬±3.0 58.0¬±0.5 72.9¬±0.6 80.0¬±0.5 - 68.8¬±0.4 46.1¬±0.2
DGK [53] 87.4¬±2.7 60.1¬±2.6 73.3¬±0.8 80.3¬±0.5 - 78.0¬±0.4 41.3¬±0.2
MLG [23] 87.9¬±1.6 63.3¬±1.5 41.2¬±0.0 >1 Day >1 Day 63.3¬±1.5 57.3¬±1.4
GCKN [54] 87.2¬±6.8 - 50.8¬±0.8 70.6¬±2.0 54.3¬±1.0 58.4¬±7.6 57.3¬±1.4
GRAPH2VEC [55] 83.2¬±9.3 60.2¬±6.9 73.3¬±2.1 73.2¬±1.8 - 75.8¬±1.0 47.9¬±0.3
INFOGRAPH [13] 89.0¬±1.1 61.7¬±1.7 74.4¬±0.3 73.8¬±0.7 67.6¬±1.2 82.5 ¬±1.4 53.5¬±1.0
MVGRL [16] 89.7¬±1.1 62.5¬±1.7 - 75.0¬±0.7 68.9¬±1.9 84.5¬±0.6 -
GCC [25] 86.4¬±0.5 58.4¬±1.2 72.9¬±0.5 66.9¬±0.2 75.2¬±0.3 88.4¬±0.3 52.6¬±0.2
GRAPHCL [18] 86.8¬±1.3 58.4¬±1.7 74.4¬±0.5 77.9¬±0.4 71.4¬±1.2 89.5¬±0.8 56.0¬±0.3
PGCL (ours) 91.1¬±1.2 63.3¬±1.3 75.7¬±0.2 78.8¬±0.8 76.0¬±0.3 91.5¬±0.7 56.3¬±0.2
TABLEI
Graphclassificationaccuracies(%)ofkernel,supervisedandunsupervisedmethods.WEREPORTTHEMEAN10-FOLDCROSS-VALIDATIONACCURACY
WITHFIVERUNS.‚Äò>1DAY‚ÄôREPRESENTSTHATTHECOMPUTATIONEXCEEDS24HOURS.
reportedin[12]includingGraphSAGE[39],GCN[11]andtwo baselines by considerable margins. For example, on the RDT-
variants of GIN [12]: GIN-0 and GIN-(cid:15). Finally, we compare B dataset [53], it achieves 91.5% accuracy, i.e., a 2.0%
with five unsupervised methods including Graph2Vec [55], absoluteimprovementoverpreviousthestate-of-the-artmethod
InfoGraph [13], MVGRL [16], GCC [25] and GraphCL [18]. (GraphCL [18]). Our model also outperforms graph kernel
We report the results of unsupervised methods based on the methods in four out of seven datasets and outperforms the
released code. best supervised model in one of the datasets. For example,
c) Model Configuration: We use the graph isomorphism it harvests a 2.4% absolute improvement over the state-of-
network (GIN) [12] as the encoder following [18] to attain the-art graph kernel method (DGK [53]) on the PROTEINS
node representations for unsupervised graph classification. dataset. When compared to supervised baselines individually,
All projection heads are implemented as two-layer MLPs. ourmodeloutperformsGraphSAGEintwooutoffourdatasets,
For unsupervised graph classification, we adopt LIB-SVM and outperforms GCN in three out of seven datasets, e.g., a
[56] with C parameter selected in {10‚àí3, 10‚àí2, ..., 102, 5.5% absolute improvement over GCN on the MUTAG dataset.
103} as our downstream classifier. Then we use 10-fold It is noteworthy that PGCL tightens the gap with respect to the
cross validation accuracy as the classification performance supervised baseline of GIN [12] such that their performance
and repeat the experiments five times to report the mean gap on four out of seven datasets is less than 2%. The strong
and standard deviation. We adopt ‚Äúnode dropping‚Äù and ‚Äúedge performance verifies the superiority of the proposed PGCL
perturbation‚Äù as the two types of graph augmentations, which framework.
perform better than other augmentations (e.g., ‚Äúsubgraph‚Äù)
empirically,referringtotheimplementation2.Prototypevectors B. Transfer Learning
are initialized with the default He initialization [78] in Pytorch. a) Experimental Setup: Next, we evaluate the GNN
To help the very beginning of the optimization, we freeze encoders trained by PGCL on transfer learning to predict
the prototypes during the first few epochs of training and chemical molecule properties. We follow the setting in [17]
focus on learning the graph representation first. Then the and use the same datasets: GNNs are pre-trained on the
prototype vectors are involved in the optimization of PGCL ZINC-2M dataset using self-supervised learning and later fine-
progressively. The best hyperparameter Œª to balance the tuned on another downstream dataset to test out-of-distribution
consistencyregularizerandthereweightedcontrastiveobjective performance. We adopt baselines including no pre-trained GIN
is 6. And the number of prototypes is set to 10. The source (i.e., without self-supervised training on the first dataset and
code of PGCL will be released for reproducibility. withonlyfine-tuning),InfoGraph[13],GraphCL[59]andthree
d) Experimental Results: The results of unsupervised different pre-train strategies in [68] including edge prediction,
graph level representations for downstream graph classification node attribute masking and context prediction. Note that [68]
tasks are presented in Table I. Overall, from the table, we incorporates the domain knowledge heuristically that correlates
can see that our approach achieves state-of-the-art results with the specific downstream datasets.
with respect to other unsupervised models across all seven b) Experimental Results: According to Table II, PGCL
datasets. PGCL consistently performs better than unsupervised significantly outperform all baselines in 3 out of 7 datasets and
achieves a mean rank of 2.9 across these 7 datasets. Although
2https://github.com/Shen-Lab/GraphCL without domain knowledge incorporated, PGCL still achieves
stesataD
desivrepuS
lenreK
desivrepusnU
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 8
Downstream Dataset BBBP Tox21 SIDER ToxCast ClinTox BACE MUV Average
#Molecules 2039 7831 1427 8575 1478 1513 93087 Rank
#Tasks 1 12 27 617 2 1 17 (‚Üì)
No Pre-Train 65.8 ¬± 4.5 74.0 ¬± 0.8 57.3 ¬± 1.6 63.4 ¬± 0.6 58.0 ¬± 4.4 70.1 ¬± 5.4 71.8 ¬± 2.5 6.1
EdgePred [68] 67.3 ¬± 2.4 76.0 ¬± 0.6 60.4 ¬± 0.7 64.1 ¬± 0.6 64.1 ¬± 3.7 79.9 ¬± 0.9 74.1 ¬± 2.1 3.7
AttrMasking [68] 64.3 ¬± 2.8 76.7 ¬± 0.4 61.0 ¬± 0.7 64.2 ¬± 0.5 71.8 ¬± 4.1 79.3 ¬± 1.6 74.7 ¬± 1.4 2.9
ContextPred [68] 68.0 ¬± 2.0 75.7 ¬± 0.7 60.9 ¬± 0.6 63.9 ¬± 0.6 65.9 ¬± 3.8 79.6 ¬± 1.2 75.8 ¬± 1.7 3.1
InfoGraph [13] 68.8 ¬± 0.8 75.3 ¬± 0.5 58.4 ¬± 0.8 62.7 ¬± 0.4 69.9 ¬± 3.0 75.9 ¬± 1.6 75.3 ¬± 2.5 4.4
GraphCL [18] 69.7 ¬± 0.7 73.9 ¬± 0.7 60.5 ¬± 0.9 62.4 ¬± 0.6 75.9 ¬± 2.7 75.4 ¬± 1.4 69.8 ¬± 2.7 5.0
PGCL (Ours) 69.8 ¬± 1.3 75.6 ¬± 0.5 61.6 ¬± 1.1 66.4 ¬± 0.2 69.4 ¬± 1.4 79.3 ¬± 1.5 71.2 ¬± 1.3 2.9
TABLEII
TRANSFERLEARNINGPERFORMANCEFORCHEMICALMOLECULESPROPERTYPREDICTION(MEANROC-AUC¬±STD.OVER10RUNS).
THEBESTRESULTSAREHIGHLIGHTEDINBOLD.
competitiveperformancetoheuristicself-supervisedapproaches
[68]. Meanwhile, PGCL outperforms GraphCL [59] on unseen
datasets with better generalizability. In contrast to InfoGraph
[13] and GraphCL [59], PGCL achieves some performance
closertothoseheuristicgraphpre-trainingbaselines(EdgePred,
AttrMasking and ContextPred) based on domain knowledge
in [68]. This is rather significant as our method utilizes only
node dropping and edge perturbation as the data augmentation,
which again shows the effectiveness of the PGCL.
L Inf. L Con. L S.R. L P.R. MUTAG PTC PRO. COLLAB
(cid:88) 86.8¬±1.3 58.4¬±1.7 74.4¬±0.5 71.4¬±1.2
(cid:88) 89.7¬±1.0 61.1¬±1.7 75.4¬±0.4 71.5¬±1.4
(cid:88) 89.9¬±1.1 61.9¬±0.9 73.4¬±0.6 72.6¬±0.5
Fig.4. SensitivityanalysisforthenumberofprototypesK.
(cid:88) 90.1¬±0.9 62.5¬±0.7 75.2¬±0.4 73.3¬±0.7
(cid:88) (cid:88) 89.9¬±1.0 62.4¬±2.1 75.4¬±0.3 73.3¬±1.2
(cid:88) (cid:88) 91.0¬±1.4 63.4¬±1.5 73.6¬±1.1 74.6¬±0.6
(cid:88) (cid:88) 91.1¬±1.2 63.3¬±1.3 75.7¬±0.2 76.0¬±0.3
TABLEIII
ABLATIONSTUDYFORDIFFERENTOBJECTIVEFUNCTIONSON
DOWNSTREAMGRAPHCLASSIFICATIONDATASETS.ASTWOVARIANTSOF
THEVANILLAINFONCELOSS,LS.R.DENOTESCACULATINGTHEWEIGHT
INEQN.(14)WITHTHESAMPLEDISTANCEWHILELP.R.CORRESPONDES
TOTHEPROTOTYPEDISTANCE.
C. Ablation Studies
In Table III, we analyze the effect of various objective
functions, including the vanilla InfoNCE loss L , its two
Inf.
variants, i.e., reweighting with prototype distance L and
P.R.
Fig.5. SensitivityanalysisforbatchsizeN.
sampledistanceL ,andtheclusteringconsistencyobjective
S.R.
L . When the clustering consistency and the reweighted
Con.
contrastive objective are individually applied, they perform D. Sensitivity Analysis
better than the InfoNCE loss, which benefits from their
explorationsofthesemanticstructureofthedata.Theprototype- a) Sensitivity to prototype numbers K: In this part, we
based reweighting objective L outperforms the sample- discuss the selection of parameter K which is the number of
P.R.
based one L in most datasets, since the prototype plays an prototypes (clusters). Fig. 4 shows the performance of PGCL
S.R.
importantroleasthepseudolabelduringnegativesamplingand withdifferentnumberofprototypesK from6to80onPTCand
providesamorerobustreweightingstrategy.Bysimultaneously PROTEINS. It can be observed that at beginning, increasing
applyingbothobjectivesL andL ,ourfullmodel(last the number of prototypes improves the performance while
Con. P.R.
row) achieves better performance than merely combining the too many prototypes leads to slight performance drop, which
InfoNCE loss and the clustering consistency, which indicates we conjecture that the model degenerates to the case without
that the prototype-based reweighting strategy can mitigate the prototypes (i.e., each graph acts as a prototype itself). Overall,
sampling bias problem. our PGCL is robust to the prototype numbers.
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 9
PTC PROTEINS NCI1
COLLAB RDT-B RDT-M-5K
Fig.6. T-SNEvisualizationofthelearnedrepresentationonsixdatasets.‚Äú(cid:63)‚Äùmeanstheprototypevectors.ColorsrepresentunderlyingclassesthatPGCL
discovers.
b) Sensitivity to batch size N: In this experiment, we REFERENCES
evaluate the effect of batch size N on our model performance.
Fig. 5 shows the classification accuracy of our model using
[1] JustinGilmer,SamuelSSchoenholz,PatrickFRiley,OriolVinyals,and
different batch sizes from 32 to 512 on PTC and PROTEINS. George E Dahl. Neural message passing for quantum chemistry. In
From the line chart, we can observe that a large batch size (i.e. ICML,pages1263‚Äì1272.PMLR,2017.
[2] ChiChen,WeikeYe,YunxingZuo,ChenZheng,andShyuePingOng.
N > 32) can consistently improve the performance of PGCL.
Graphnetworksasauniversalmachinelearningframeworkformolecules
This observation aligns with the case in the image domain andcrystals. ChemistryofMaterials,31(9):3564‚Äì3572,2019.
[19]. [3] MarcoAAlvarezandChanghuiYan. Anewproteingraphmodelfor
function prediction. Computational Biology and Chemistry, 37:6‚Äì10,
2012.
E. Visualization Results.
[4] Q.Li,Z.Han,andX.-M.Wu,‚ÄúDeeperinsightsintographconvolutional
In Fig. 6, we utilize the t-SNE [57] to visualize the graph networksforsemi-supervisedlearning,‚ÄùinAAAI,2018.
[5] Q.Zhu,B.Du,andP.Yan,‚ÄúSelf-supervisedtrainingofgraphconvolu-
representations and prototype vectors with the number of clus-
tionalnetworks,‚ÄùarXivpreprintarXiv:2006.02380,2020.
ters K =10 on various datasets. Generally, the representations [6] M.GutmannandA.Hyv√§rinen,‚ÄúNoise-contrastiveestimation:Anew
learned by the proposed PGCL forms separated clusters, where estimationprincipleforunnormalizedstatisticalmodels,‚ÄùinAISTATS,
2010,pp.297‚Äì304.
the prototypes fall into the center. It demonstrates that PGCL
[7] M.Sun,J.Xing,H.Wang,B.Chen,andJ.Zhou,‚ÄúMocl:Contrastive
can discover the underlying global semantic structure over the
learning on molecular graphs with multi-level domain knowledge,‚Äù
entire data distribution. Moreover, it can be observed that each SIGKDD,2021.
cluster has a similar number of samples, which indicates the [8] Y.Fang,Q.Zhang,H.Yang,X.Zhuang,S.Deng,W.Zhang,M.Qin,
Z. Chen, X. Fan, and H. Chen, ‚ÄúMolecular contrastive learning with
effectivenessoftheequal-partitionconstraintsduringclustering.
chemicalelementknowledgegraph,‚ÄùinAAAI,2022.
[9] Biaobin Jiang, Kyle Kloster, David F Gleich, and Michael Gribskov.
VI. CONCLUSIONS Aptrank:anadaptivepagerankmodelforproteinfunctionpredictionon
bi-relationalgraphs. Bioinformatics,33(12):1829‚Äì1836,2017.
We proposed a clustering-based approach called PGCL
[10] Mark EJ Newman and Michelle Girvan. Finding and evaluating
for unsupervised graph-level representation learning. PGCL
community structure in networks. Physical Review E, 69(2):026113,
clusters semantically similar graphs into the same group, 2004.
and simultaneously encourages the clustering consistency for [11] ThomasNKipfandMaxWelling. Semi-supervisedclassificationwith
graphconvolutionalnetworks. ICLR,2017.
different graph views. Moreover, to mitigate the sampling bias
[12] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How
issue, PGCL reweights its negative pairs based on the distance powerfularegraphneuralnetworks? ICLR,2019.
between their prototypes. Benefiting from modeling the global [13] Fan-YunSun,JordanHoffmann,VikasVerma,andJianTang. Infograph:
semantic structure via clustering, we achieve new state-of-the- Unsupervisedandsemi-supervisedgraph-levelrepresentationlearning
viamutualinformationmaximization. ICLR,2020.
art performance compared to previous unsupervised learning
[14] Thomas N Kipf and Max Welling. Variational graph auto-encoders.
methods on seven graph classification benchmarks. NeurIPS,2016.
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 10
[15] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet [42] AaronvandenOord,YazheLi,andOriolVinyals.Representationlearning
Kohli. Graph matching networks for learning the similarity of graph with contrastive predictive coding. arXiv preprint arXiv:1807.03748,
structuredobjects. InICML,pages3835‚Äì3845.PMLR,2019. 2018.
[16] KavehHassaniandAmirHoseinKhasahmadi. Contrastivemulti-view [43] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan
representationlearningongraphs. InICML,pages4116‚Äì4126.PMLR, Grewal,PhilBachman,AdamTrischler,andYoshuaBengio. Learning
2020. deeprepresentationsbymutualinformationestimationandmaximization.
[17] HanlinZhang,ShuaiLin,WeiyangLiu,PanZhou,JianTang,Xiaodan ICLR,2019.
Liang,andEricPXing. Iterativegraphself-distillation. TheWorkshop [44] Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis
onSelf-SupervisedLearningfortheWeb(SSL@WWW),2020. Plevrakis, and Nikunj Saunshi. A theoretical analysis of contrastive
[18] YuningYou,TianlongChen,YongduoSui,TingChen,ZhangyangWang, unsupervisedrepresentationlearning. ICML,2019.
andYangShen.Graphcontrastivelearningwithaugmentations.NeurIPS, [45] MarcoCuturi. Sinkhorndistances:Lightspeedcomputationofoptimal
33,2020. transport. NeurIPS,26:2292‚Äì2300,2013.
[19] TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. [46] Z.Li,F.Nie,X.Chang,Y.Yang,C.Zhang,andN.Sebe,‚ÄúDynamic
Asimpleframeworkforcontrastivelearningofvisualrepresentations. affinitygraphconstructionforspectralclusteringusingmultiplefeatures,‚Äù
ICML,2020. IEEETransactionsonNeuralNetworksandLearningSystems,vol.29,
pp.6323‚Äì6332,2018.
[20] Ching-YaoChuang,JoshuaRobinson,LinYen-Chen,AntonioTorralba,
[47] JoshuaRobinson,Ching-YaoChuang,SuvritSra,andStefanieJegelka.
andStefanieJegelka. Debiasedcontrastivelearning. NeurIPS,2020.
Contrastivelearningwithhardnegativesamples. ICLR,2020.
[21] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt
[48] CharlesElkanandKeithNoto. Learningclassifiersfromonlypositive
Mehlhorn,andKarstenMBorgwardt. Weisfeiler-lehmangraphkernels.
andunlabeleddata. InSIGKDD,pages213‚Äì220,2008.
JMLR,12(9),2011.
[49] PeilinZhaoandTongZhang. Acceleratingminibatchstochasticgradient
[22] KarstenMBorgwardtandHans-PeterKriegel. Shortest-pathkernelson
descentusingstratifiedsampling. arXivpreprintarXiv:1405.3080,2014.
graphs. InICDM,pages8‚Äìpp.IEEE,2005.
[50] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDoll√°r.
[23] RisiKondorandHoracePan. Themultiscalelaplaciangraphkernel. In
Focallossfordenseobjectdetection. InCVPR,pages2980‚Äì2988,2017.
NeurIPS,pages2990‚Äì2998,2016.
[51] ChristopherMorris,NilsMKriege,FrankaBause,KristianKersting,Petra
[24] RaghunathanRamakrishnan,PavloODral,MatthiasRupp,andOAnatole Mutzel,andMarionNeumann. Tudataset:Acollectionofbenchmark
VonLilienfeld. Quantumchemistrystructuresandpropertiesof134kilo datasetsforlearningwithgraphs.ICMLworkshop"GraphRepresentation
molecules. Scientificdata,1(1):1‚Äì7,2014. LearningandBeyond",2020.
[25] JiezhongQiu,QibinChen,YuxiaoDong,JingZhang,HongxiaYang, [52] NinoShervashidze,SVNVishwanathan,TobiasPetri,KurtMehlhorn,and
MingDing,KuansanWang,andJieTang.Gcc:Graphcontrastivecoding KarstenBorgwardt.Efficientgraphletkernelsforlargegraphcomparison.
forgraphneuralnetworkpre-training. InSIGKDD,pages1150‚Äì1160, InAISTATS,pages488‚Äì495,2009.
2020. [53] PinarYanardagandSVNVishwanathan.Deepgraphkernels.InSIGKDD,
[26] ZiniuHu,YuxiaoDong,KuansanWang,Kai-WeiChang,andYizhou pages1365‚Äì1374,2015.
Sun. Gpt-gnn: Generative pre-training of graph neural networks. In [54] DexiongChen,LaurentJacob,andJulienMairal. Convolutionalkernel
SIGKDD,pages1857‚Äì1867,2020. networksforgraph-structureddata. ICML,2020.
[27] YanqiaoZhu,YichenXu,FengYu,QiangLiu,ShuWu,andLiangWang. [55] AnnamalaiNarayanan,MahinthanChandramohan,RajasekarVenkatesan,
Graphcontrastivelearningwithadaptiveaugmentation. WWW,2020. Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning
[28] PetarVelicÀákovic¬¥,WilliamFedus,WilliamLHamilton,PietroLi√≤,Yoshua distributedrepresentationsofgraphs. arXivpreprintarXiv:1707.05005,
Bengio,andRDevonHjelm. Deepgraphinfomax. ICLR,2019. 2017.
[29] QingyunSun,JianxinLi,HaoPeng,JiaWu,YuanxingNing,PhillipS [56] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support
Yu,andLifangHe. Sugar:Subgraphneuralnetworkwithreinforcement vectormachines. ACMTIST,2(3):1‚Äì27,2011.
poolingandself-supervisedmutualinformationmechanism.WWW,2021. [57] LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausing
[30] MichaelTschannen,JosipDjolonga,PaulKRubenstein,SylvainGelly, t-sne. JMLR,9(11),2008.
andMarioLucic.Onmutualinformationmaximizationforrepresentation [58] M.Caron,P.Bojanowski,A.Joulin,andM.Douze,‚ÄúDeepclustering
learning. ICLR,2019. forunsupervisedlearningofvisualfeatures,‚ÄùinECCV,2018.
[31] MathildeCaron,PiotrBojanowski,ArmandJoulin,andMatthijsDouze. [59] Y. You, T. Chen, Y. Shen, and Z. Wang, ‚ÄúGraph contrastive learning
Deepclusteringforunsupervisedlearningofvisualfeatures. InECCV, automated,‚ÄùinICML,pages12121‚Äì12132.PMLR,2021.
pages132‚Äì149,2018. [60] H.Zhao,X.Yang,Z.Wang,E.Yang,andC.Deng,‚ÄúGraphdebiased
[32] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self- contrastivelearningwithjointrepresentationclustering,‚ÄùinIJCAI,2021,
labellingviasimultaneousclusteringandrepresentationlearning. ICLR, pp.3434‚Äì3440.
2020. [61] M.Xu,H.Wang,B.Ni,H.Guo,andJ.Tang,‚ÄúSelf-supervisedgraph-
levelrepresentationlearningwithlocalandglobalstructure,‚ÄùinICML,
[33] MathildeCaron,PiotrBojanowski,JulienMairal,andArmandJoulin.
pages11548‚Äì11558.PMLR,2021.
Unsupervised pre-training of image features on non-curated data. In
[62] Z.Tong,Y.Liang,H.Ding,Y.Dai,X.Li,andC.Wang,‚ÄúDirectedgraph
ICCV,pages2959‚Äì2968,2019.
contrastivelearning,‚ÄùNeurIPS,vol.34,2021.
[34] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bo-
[63] S.Arora,H.Khandeparkar,M.Khodak,O.Plevrakis,andN.Saunshi,‚ÄúA
janowski,andArmandJoulin. Unsupervisedlearningofvisualfeatures
theoreticalanalysisofcontrastiveunsupervisedrepresentationlearning,‚Äù
bycontrastingclusterassignments. NeurIPS,2020.
2019.
[35] JiaboHuang,QiDong,ShaogangGong,andXiatianZhu. Unsupervised
[64] L.Liu,Z.Kang,L.Tian,W.Xu,andX.He,‚ÄúMultilayergraphcontrastive
deeplearningbyneighbourhooddiscovery. InICML,pages2849‚Äì2858.
clusteringnetwork,‚Äù2021.
PMLR,2019.
[65] E. Pan et al., ‚ÄúMulti-view contrastive graph clustering,‚Äù in NeurIPS,
[36] SamTRoweisandLawrenceKSaul.Nonlineardimensionalityreduction
2021.
bylocallylinearembedding. Science,290(5500):2323‚Äì2326,2000.
[66] Z. Hu, G. Kou, H. Zhang, N. Li, K. Yang, and L. Liu, ‚ÄúRectifying
[37] George W Furnas, Scott Deerwester, Susan T Durnais, Thomas K Pseudo Labels: Iterative Feature Clustering for Graph Representation
Landauer,RichardAHarshman,LynnAStreeter,andKarenELochbaum. Learning‚ÄùinCIKM,pages720‚Äì729.ACM,2021.
Informationretrievalusingasingularvaluedecompositionmodeloflatent [67] W. Xia, Q. Gao, M. Yang, and X. Gao, ‚ÄúSelf-supervised contrastive
semanticstructure. InSIGIR,volume51,pages90‚Äì105,2017. attributedgraphclustering,‚Äù2021.
[38] PetarVelicÀákovic¬¥,GuillemCucurull,ArantxaCasanova,AdrianaRomero, [68] W.Hu,B.Liu,J.Gomes,M.Zitnik,P.Liang,V.Pande,andJ.Leskovec,
PietroLio,andYoshuaBengio. Graphattentionnetworks. ICLR,2018. ‚ÄúStrategiesforpre-traininggraphneuralnetworks,‚ÄùinICLR,2020.
[39] WillHamilton,ZhitaoYing,andJureLeskovec. Inductiverepresentation [69] F. L. Opolka, A. Solomon, C. Cangea, P. Velickovic, P. Li√≤, and
learningonlargegraphs. InNeurIPS,pages1024‚Äì1034,2017. R. D. Hjelm, ‚ÄúSpatio-temporal deep graph infomax,‚Äù CoRR, vol.
[40] Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L abs/1904.06316, 2019. [Online]. Available: http://arxiv.org/abs/1904.
Hamilton,andJureLeskovec. Hierarchicalgraphrepresentationlearning 06316
withdifferentiablepooling. NeurIPS,2018. [70] Y.Ren,B.Liu,C.Huang,P.Dai,L.Bo,andJ.Zhang,‚ÄúHeterogeneous
[41] MuhanZhang,ZhichengCui,MarionNeumann,andYixinChen. An deepgraphinfomax,‚Äù2020.
end-to-enddeeplearningarchitectureforgraphclassification. InAAAI, [71] Q.Zhu,B.Du,andP.Yan,‚ÄúSelf-supervisedtrainingofgraphconvolu-
volume32,2018. tionalnetworks,‚Äù2020.
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 11
[72] J. Zeng and P. Xie, ‚ÄúContrastive self-supervised learning for graph ChenLiureceivedthebachelor‚Äôs(2013)andmaster‚Äôs
classification,‚ÄùinAAAI,pages10824‚Äì10832,2021. (2016)degreeinpsycholoyfromNanjingUniversity,
[73] Y.Zhu,Y.Xu,F.Yu,Q.Liu,S.Wu,andL.Wang,‚ÄúDeepgraphcontrastive Nanjing,ChinaandUniversityofYork,York,United
representationlearning,‚Äù2020. Kingdom respectively. She is currently pursuing
[74] ‚ÄúGraphcontrastivelearningwithadaptiveaugmentation‚Äù,WebConf,Apr the Ph.d.‚Äôs degree under the supervision of Xi-
2021. aodan Liang, with the Department of Intelligent
[75] Y.Jiao,Y.Xiong,J.Zhang,Y.Zhang,T.Zhang,andY.Zhu,‚ÄúSub-graph Engineering, Sun Yat-sen University, Guangzhou,
contrastforscalableself-supervisedgraphrepresentationlearning,‚Äùin China.Herresearchinterestsincludedeeplearning,
ICDM,pages222‚Äì231.IEEE,2020. graph-structureddatamining,andtheirapplication
[76] L.Wu,H.Lin,C.Tan,Z.Gao,andS.Z.Li,‚ÄúSelf-supervisedlearning intimeneuralsignalsetc.
ongraphs:Contrastive,generative,orpredictive,‚ÄùIEEETransactionson
KnowledgeandDataEngineering,2021.
[77] Y. Liu, M. Jin, S. Pan, C. Zhou, F. Xia, and P. S. Yu, ‚ÄúGraph self-
supervisedlearning:Asurvey,‚ÄùIEEETransactionsonKnowledgeand
DataEngineering,2021.
[78] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDelving deep into rectifiers:
Surpassinghuman-levelperformanceonimagenetclassification,‚ÄùinICCV,
2015,pp.1026‚Äì1034. PanZhoureceivedthemaster‚Äôsdegreeincomputer
[79] D.Yuan,X.Chang,P.-Y.Huang,Q.Liu,andZ.He,‚ÄúSelf-supervised sciencefromPekingUniversity,Beijing,China,in
deep correlation tracking,‚Äù IEEE Transactions on Image Processing, 2016,andthePh.D.degreeincomputersciencefrom
vol.30,pp.976‚Äì985,2020. theNationalUniversityofSingapore,Singapore,in
[80] Z.Li,F.Nie,X.Chang,Y.Yang,C.Zhang,andN.Sebe,‚ÄúDynamic 2019.HeiscurrentlytheSeniorResearchScientist
affinitygraphconstructionforspectralclusteringusingmultiplefeatures,‚Äù oftheSEAAILaboratory,SEAGroup,Singapore.
IEEE transactions on neural networks and learning systems, vol. 29, From2019to2020,hewastheResearchScientistof
no.12,pp.6323‚Äì6332,2018. Salesforce,Singapore.Hisresearchinterestsinclude
[81] J.Li,P.Zhou,C.Xiong,andS.Hoi,‚ÄúPrototypicalcontrastivelearning computervision,machinelearning,andoptimization.
ofunsupervisedrepresentations,‚ÄùinICLR,2020. Dr.ZhouwasthewinneroftheMicrosoftResearch
[82] X.Jiang,T.Jia,Y.Fang,C.Shi,Z.Lin,andH.Wang,‚ÄúPre-trainingon AsiaFellowshipin2018.
large-scaleheterogeneousgraph,‚ÄùinSIGKDD,2021,pp.756‚Äì766.
[83] Y.Hou,B.Hu,W.X.Zhao,Z.Zhang,J.Zhou,andJ.-R.Wen,‚ÄúNeural
graphmatchingforpre-traininggraphneuralnetworks,‚ÄùinSDM. SIAM,
2022,pp.172‚Äì180.
[84] B.Hui,P.Zhu,andQ.Hu,‚ÄúCollaborativegraphconvolutionalnetworks:
Unsupervisedlearningmeetssemi-supervisedlearning,‚ÄùinAAAI,vol.34,
Zi-Yuan Hu iscurrentlyanundergraduatestudent
no.04,2020,pp.4215‚Äì4222.
in computer science from Sun Yat-sen University,
[85] H. Hafidi, M. Ghogho, P. Ciblat, and A. Swami, ‚ÄúNegative sampling
Guangzhou,China.Heisgoodatalgorithmdesign
strategiesforcontrastiveself-supervisedlearningofgraphrepresentations,‚Äù
and implementation. His research interest is data
SignalProcessing,vol.190,p.108310,2022.
miningandcross-modalitypre-training.
[86] N.Lee,J.Lee,andC.Park,‚ÄúAugmentation-freeself-supervisedlearning
ongraphs,‚ÄùinAAAI,2022.
[87] B.Hui,P.Zhu,andQ.Hu,‚ÄúCollaborativegraphconvolutionalnetworks:
Unsupervisedlearningmeetssemi-supervisedlearning,‚ÄùinAAAI,2020.
[88] N.RethmeierandI.Augenstein,‚ÄúAprimeroncontrastivepretrainingin
languageprocessing:Methods,lessonslearnedandperspectives,‚ÄùarXiv
preprintarXiv:2102.12982,2021.
[89] Z.Lin,C.Tian,Y.Hou,andW.X.Zhao,‚ÄúImprovinggraphcollaborative
filteringwithneighborhood-enrichedcontrastivelearning,‚ÄùinWebConf,
2022,pp.2320‚Äì2329.
[90] P.Zhou,C.Xiong,X.Yuan,andS.Hoi,‚ÄúAtheory-drivenself-labeling
refinementmethodforcontrastiverepresentationlearning,‚ÄùinNeurIPS, ShuojiaWangreceivedthePh.D.degreeinEpidemi-
2021. ologyandHealthStatisticsfromZhejiangUniversity,
[91] P. Zhou, Y. Zhou, C. Si, W. Yu, T. K. Ng, and S. Yan, ‚ÄúMugs: Hangzhou,Chinain2020.Sheiscurrentlythedata
A multi-granular self-supervised learning framework,‚Äù arXiv preprint scientistinTencentJarvislab.Herresearchinterests
arXiv:2203.14415,2022. includedatamining,medicaldecision-making,and
diseaseprediction.
ShuaiLinreceivedthebachelor‚Äôsdegreeincommu-
nicationengineeringfromXidianUniversity,Xi‚Äôan,
China,in2019.Heiscurrentlypursuingthemaster‚Äôs
Ruihui Zhao received his B.S. degree at UESTC
degreeunderthesupervisionofXiaodanLiang,with
(2015)andM.S.degreeatWasedaUniversity(2017).
theDepartmentofIntelligentEngineering,SunYat-
HejoinedTencentJarvisLabasaseniorresearcher
senUniversity,Guangzhou,China.Hismainresearch
inearly2018.Previously,hewasanNLPengineer
interestsincludedataminingandinterpretablema-
atSinovationVentures(2017-2018).Hehasseveral
chinelearning.
papers accepted by ACL, WWW, AAAI, NAACL,
IJCNN, TOIT, IEEE WCSP, etc. His research and
projects mainly focus on NLP and information
security.
IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS 12
YefengZheng(Fellow,IEEE)receivedtheBEand
ME degrees from Tsinghua University, Beijing, in
1998 and 2001, respectively, and the PhD degree
fromtheUniversityofMaryland,CollegePark,in
2005.Aftergraduation,hejoinedSiemensCorporate
Research in Princeton, New Jersey, USA. He is
nowDirectorandDistinguishedScientistofTencent
JarvisLab,Shenzhen,China,leadingthecompany‚Äôs
initiative on Medical AI. His research interests
includemedicalimageanalysis,graphdatamining
anddeeplearning.HeisafellowoftheInstituteof
ElectricalandElectronicsEngineers(IEEE),afellowofAmericanInstitute
forMedicalandBiologicalEngineering(AIMBE).
Liang Lin (Senior Member, IEEE) served as the
ExecutiveDirectoroftheSenseTimeGroup,Hong
Kong, from 2016 to 2018, leading the research
anddevelopmentteamsindevelopingcutting-edge,
deliverable solutions in computer vision and data
mining.HeiscurrentlyaFullProfessorofcomputer
sciencewithSunYat-senUniversity,Guangzhou.He
hasauthoredorcoauthoredmorethan200articlesin
leadingacademicjournalsandconferences,suchas
IEEETRANSACTIONSONPATTERNANALYS
IS AND MACHINE INTELLIGENCE (TPAMI),
ConferenceonNeuralInformationProcessingSystems(NeurIPS),International
Conference on Machine Learning (ICML), and The Association for the
AdvancementofArtificialIntelligence(AAAI).Dr.LinisafellowofIET.He
servedastheArea/SessionChairfornumerousconferences,suchasCVPR,
ICME,ICCV,andInternationalConferenceonMultimediaRetrieval(ICMR).
HeisanAssociateEditoroftheIEEETransactionsonNeuralNetworksand
LearningSystems(IEEETNNLS).
EricXing(Fellow,IEEE)receivedthePh.D.degree
inmolecularbiologyfromRutgersUniversity,New
Brunswick,NJ,USA,in1999,andthePh.D.degree
incomputersciencefromtheUniversityofCalifornia
at Berkeley, Berkeley, CA, USA, in 2004. He is
currentlyaProfessorofmachinelearningwiththe
School of Computer Science and the Director of
theCMUCenterforMachineLearningandHealth,
Carnegie Mellon University, Pittsburgh, PA, USA.
Hisprincipalresearchinterestslieinthedevelopment
of machine learning and statistical methodology,
especially for solving problems involving automated learning, reasoning,
anddecision-makinginhigh-dimensional,multimodal,anddynamicpossible
worldsinsocialandbiologicalsystems.Dr.XingisamemberoftheDARPA
InformationScienceandTechnology(ISAT)AdvisoryGroupandtheProgram
ChairoftheInternationalConferenceonMachineLearning(ICML)2014.He
is also an Associate Editor of the IEEE TRANSACTIONS ON PATTERN
ANALYSISANDMACHINEINTELLIGENCE(PAMI)andtheMachine
Learning Journal (MLJ) and the Journal of Machine Learning Research
(JMLR).
Xiaodan Liang (Senior Member, IEEE) received
the Ph.D. degree from Sun Yat-sen University,
Guangzhou,China,in2016,underthesupervision
ofLiangLin.ShewasaPost-DoctoralResearcher
withtheDepartmentofMachineLearning,Carnegie
Mellon University, Pittsburgh, PA, USA, working
with Prof. Eric Xing from 2016 to 2018. She is
currently an Associate Professor with Sun Yat-sen
University.Shehasauthoredorcoauthoredseveral
cutting-edgeprojectsoninterpretablemachinelearn-
ing,dataminingandgraphneuralnetwork.

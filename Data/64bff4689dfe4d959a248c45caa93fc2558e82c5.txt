bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
Spatio-temporal Characteristics of Noun and Verb
Processing during Sentence Comprehension in the
Brain
SharmisthaJat1 ErikaJCLaing2 ParthaTalukdar1 TomMitchell2
1IndianInstituteofScience,Bangalore
2SchoolofComputerScience,CarnegieMellonUniversity
{sharmisthaj,ppt}@iisc.ac.in
laing@andrew.cmu.edu, tom.mitchell@cs.cmu.edu
Abstract
Thehumanbrainisveryeffectiveatintegratingnewwordsonebyoneintothe
composedrepresentationofasentenceasitisreadleft-to-right. Thisraisesthe
importantquestionofwhathappenstotheneuralrepresentationsofwordspresent
earlierinthesentence? Forexample,dothestrengthofwordrepresentationsen-
counteredearlieroninthesentenceremainconstantordotheyevolveasadditional
wordsareprocessed? Representationofwordsbyneuralactivityinthebrainhas
beenthesubjectofseveralpreviousstudies. Weperformtheexperimentwitha
naturalistictaskinwhichthesubjectsreadsimpleactiveandpassivesentences.
Naturalisticstudieshavetendedtoexplorewordsinisolationorinaverylimited
context(e.g.,adjective-nounphrases). Representationofpreviouslyencountered
wordsduringincrementalsentencereading,andhowsuchrepresentationevolveas
morepartsofasentenceareread,isafundamentalbutunexploredproblem–we
takeafirststepinthisdirection. Inparticular,weexaminethespatio-temporalchar-
acteristicsofneuralactivityencodingnounsandverbsencounteredinasentence
asitisreadword-by-word. WeuseMagnetoencephalography(MEG)topassively
observeneuralactivity,providing1mstemporalresolution.
Our experiments reveal that nouns and verbs read early in the sentence have a
varyinginfluenceonneuralactivitywhilereadingsubsequentwords,decreasing
andincreasingatparticularwordpositionsinactiveandpassivelyvoicedsentences,
withparticularlyimportantcontributionstoactivityinfrontalandtemporalcortical
regions. Wefindthenounandverbinformationtobedecodablefromtheneural
activityforseveralsecondsaftersentencereadinghascompleted. Ourexploration
isalsothefirsttostudytheeffectofquestion-answeringtaskontheneuralrep-
resentationofthewordspost-sentence. Wearereleasingour300sentenceMEG
datasettoencouragefurtherresearchinthisimportantarea.
1 Introduction
Aswereadasentencewordbyword,ourbrainsintegratethemeaningofanewlyencounteredword
tothecomposedrepresentationofwordsreadsofar. Thisraisesthequestionofwhathappenstothe
neuralrepresentationofwordspresentearlierinthesentence. Arethosewordsstillactivelyretained
inthecomposedrepresentation?Ordoesthestrengthoftheirpresenceinthecomposedrepresentation
decreaseovertime? Andwhathappensoncethesentencehasbeenreadfully? Thesearefundamental
questionswhoseanswersareessentialtowardsdeepeningourunderstandingofhowsentencesare
processedinthebrain. Weinitiateastudyinthisdirectionandprovideempiricalobservationsand
insightsforthesequestions. Inparticular,weusemachinelearningandnaturalisticbrainimaging
34thConferenceonNeuralInformationProcessingSystems(NeurIPS2020),Vancouver,Canada.
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
experimentstostudythespatio-temporalcharacteristicsofnounandverbrepresentationsduring
sentencecomprehensioninthebrain.
Semanticcompositionistheprocessofcombiningsmallunitsoflinguisticinformation(words)to
formmeaning. Semanticcompositionhasbeenthefocusofseveralstudies[1,19,20,40]. Arecent
study[14]usessourcereconstructedMagnetoencephalography(MEG)toinvestigatehowindividual
wordsareinterpretedinasentencecontextbycomparingthemagnitudeofbrainactivitybetween
wordsinsentencesandlistsofwords. Throughtheiranalysis,thestudyisabletodetectthetimeat
whichthecurrentwordiscombinedintothecurrentsentencecontext. However,somekeyquestions
likethecompositionofthecurrentcontextarenotanswered. MachineLearning(ML)techniquescan
tellusmoreabouttheinformationcontentofasignalbytestingtheabilityofthatsignaltodistinguish
betweencategoriesofinterest. TheapplicationofMLtechniquestonaturalisticstimuli(asopposed
tocarefullychosenstimuli)enablesustostudyawiderrangeofscientificquestionsregardingthe
informationcontentoftherecordedbrainactivity. Onenaturalisticstudysuccessfullyapproachedthe
questionofsemanticcompositionsintherestrictedsettingofadjective-nounphrasecomprehension
[7]. Inthispaper,weexpandonthatworktoincorporatecomposedrepresentationsofwordsinafull
sentencecontextwiththeaimofelucidatingthecontributionsofthepastandcurrentwordsduring
theevolutionofsentence-levelmeaning.
WealsocollectanewMEGdatasetof300simplesentencesintheactiveandpassivevoice. MEG
[21]isespeciallyidealforunderstandingthefastdynamicsoflanguageprocessingduetoitssuperior
timeresolution. Simplesentencesallowustofocusonthebrainprocessesinvolvedinprocessingthe
wordsinasentence,asopposedtothemorecomplexsyntacticelementsinthesentence. Wepresent
these sentences word by word to the subject and to a BERT model [4]. Sentence representation
by BERT model has been shown to be predictive of the brain activity data [16, 36]. We use the
BERT representation vector as a state of the art representation of the composed meaning of the
wordsequenceuptoagiventimepoint. Adecodingframeworkisusedtodecomposethecurrent
contextintoconstituentwordrepresentationsatvarioustimepositionsinthebrain. Thedecoding
accuracyisreportedfortheindividualbrainregionsandthewholebrain. Insummary,wefindthe
degreetowhichearlierwordsmodulatetheneuralactivityaccordingtotheBERThypothesisabout
thecomposedmeaning. Ourexperimentsrevealimportantcontributionsfromfrontalandtemporal
regionsofthebraininthemechanismofcontextcompositionduringsentencecomprehension. We
alsofindthewordrepresentationstobedecodableforasignificantamountoftimeaftertheendof
thesentence.
Insummary,inthispaper,wemakethefollowingcontributions.
• Weproposeanewmethodtoquantifythedegreetowhichearlierwordsmodulatetheneural
activityinthecurrentcontextcomposition.
• OurresultsrevealimportantcontributionsfromFrontalandTemporalregionsofthebrainin
contextcomposition. Wealsofindthewordstobedecodableforasignificantamountof
timepost-sentence.
• Our exploration is the first to study the effect of question-answering task on the neural
representationofthewordspost-sentence.
• WealsoprovideanewMEGdatasettostudysimplesentenceunderstanding.
In the paper we outline the methods used to collect and process MEG data (Section 2), describe
theframeworktomeasurethepredictiveaccuracy(Section3),outlinethesetupandresultsofour
experiments(Section4),furtherdiscusstherelatedwork(Section5)andconclusions(Section6).
2 SimplePassAct: ANewSimpleSentenceMEGDataset
Simple sentences allow us to perform a focused study of word processing, as opposed to more
complex syntactic elements in the sentence. Previously proposed MEG-based sentence datasets
sufferfromconfoundingissues,suchasanimacyandsentencelength[26,27]. Toovercomethese
issues,wedesignedanewstudywithalarger300sentencedataset. Weshallrefertothedatasetas
SimplePassActdatasetintherestofthepaper. Threeparticipants(onefemale,twomale)read300
uniquesentences,150activeand150passive,comprisedofsixnouns(singer,baker,customer,parent,
artist,author)andfiveverbs(encouraged,attached,answered,challenged,followed). Tominimize
2
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
Figure1: DecodingmodelfortheMEGdata. AstringisprocessedbyBERTmodeltoobtaincontext
embeddingforthelasttoken. Forexample, inthefigure, thestring“theauthorfollowedthe", is
processed by BERT. The resulting context embedding is used to predict the corresponding brain
activityusingalearnedRidgeRegressionmodel(ReferSection3.2formoredetails). Thecandidate
referencesareorderedbasedonthecosinesimilarityscorewiththepredictedbrainactivity. Thetrue
candidaterankrisusedtocalculaterankaccuracy. Anabovechancelevel(50%)accuracysuggests
decodabilityofthestimulus. PleaserefertoSection3formoredetails.
uncontrolledsemanticbias,thenounswerematchedingenderneutrality[17]andbothnounsand
verbswerematchedinfrequencyusingtheCorpusofContemporaryAmericanEnglish[3]. There
werenorepetitionsofanysentenceinthisstudy,andthenounpositionswerecompletelycounter
balancedacrosstheentiresentenceset. PleasefindthestimulussentencesintheAppendix.
NeuralactivitywasrecordedusinganElektaNeuromagdevice(ElektaOy).Sentenceswerepresented
acrosssixblocksinrandomizedorderuniquetoeachparticipant. Duringeachtrial,eachwordof
thesentenceappearedonthedisplayfor300msfollowedby200msofblankscreen,with3.5-4sof
restbeforethenextsentencebegan. Toensureparticipantsengagement,15%ofthesentenceswere
followedbyatwo-choicequestionregardingtheagentorthepatientoftheprecedingsentenceand
weregiven2storespond,followedbyanadditional1.5srestbeforethenextsentencebegan. Thetwo
answerchoiceswerepresented. Leftandrightpositionofthecorrectanswerwasbalancedacross
theexperimenttoeliminateperceptualandlateralitybias. TheMEGdatawasacquiredadheringto
thebestpracticesofMEGdatacollection[10]. Thecollecteddatawasspatiallyfilteredusingthe
temporalextensionofSSS[35]whichalsorealignedtheheadpositiontoadefaultlocation. The
SignalSpaceProjection(SSP)method[37]wasusedtoremoveartifactscapturedbyemptyroom
datarecordedonthedayofeachparticipant’sdataacquisition,thentheMEGsignalwasband-pass
filteredfrom1to150Hzwithnotchfiltersappliedat60and120Hztoremovethecontributions
of line noise. The SSP method was again used to remove signal contamination by eye blinks or
movementsandcardiacsignals.
Thesentencesarepresentedonlyoncetoourthreesubjects. Thisallowedustopresentahigher
numberofsentences(300)inagivensession. Thoughagroupofthreeparticipantsisonthesmaller
sideforstudiessuchasthis,wenotethatthekeyfeatureswedescribeintheresultsareapparentfor
mostsubjectsandthereissupportforcollectingmorehigh-qualitydatafromfewersubjects[18].
AstudybyWehbeetal. [38]showedthatthehighernumberofsamplescanbeusedbyencoding
modelswithgoodresults. AllthreesubjectsconsentedtothestudyapprovedbytheUniversityof
PittsburghandCarnegieMellonInstitutionalReviewBoard.
3 Method
In this section, we present the methodological approach to explore semantic composition during
simplesentencecomprehension. Westudywordsastheycomposetoformacontextrepresentationin
thebrain. Theexperimentisperformedusingnaturalisticstimuliwithsimplesentences(Section2).
Thesentencestimulicompriseofseveralsimilarlyconstructedsentences. Thisfeatureallowsusto
conductexperimentstocomparerepresentationsamongcandidatesentenceswhichdifferbyjust
oneword. Someexamplesentencesare,(a)Nounvariation-"theartistansweredtheparent"v/s
"the author answered the parent", (b) Verb variation - "the artist answered the parent" v/s "the
artistchallengedtheparent". Figure1detailsthepredictionframework. Wepredictbrainactivity
usingBERTrepresentation(Section3.1). Thecorrespondingpredictionrankaccuracycalculationis
describedinSection3.2. Thecandidaterankreferencebrainactivityiscomputedasdescribedin
3
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
Section3.3. Finally,thereferencesarecomparedtothepredictedbrainactivitytocalculatearank
accuracy(Section3.4).
3.1 BERTRepresentation
BidirectionalEncoderRepresentationsfromTransformers(BERT)isastateoftheartNaturalLan-
guageUnderstandingmodel[4]. Themodelisusedtogeneratein-contextembeddingrepresentations
fortext. Thepre-trainedmodelcanbefine-tunedwithadditionallayerstocreatemodelsforawide
rangeofNaturalLanguageProcessingtasks. Unlikepreviousmodels,BERTcanusebothleftand
therightcontexttopredicttherandomlymaskedtokens. Jatetal. [16]showevidenceoflayer_18
representationstobethemostpredictiveofbrainactivity. SimilartoJatetal. [16],weuselayer_18
representationsandprocessthesentencesincrementallywithBERTmodeltopreventinformation
fromfuturewordsfromaffectingthecurrentrepresentation,inlinewithhowinformationisprocessed
bythebrain. Forexample,inthesentence“theartistansweredtheparent",therepresentationofthe
word“answered"iscalculatedbyprocessingsentencesegment“theartistanswered"andtakingthe
lasttokenrepresentationfromthelayer18,astheembeddingrepresentationfortheword“answered".
3.2 DecodingModel
We transform the neuroscience research question to a machine learning problem by learning a
functionfˆ(RidgeRegression)topredictbrainactivityyfromcorrespondingfeaturesofthestimulus
x(substringrepresentation). Ifthelearnedfunctionfˆcanaccuratelypredictthebrainactivitydata
fromthestimulusfeaturesforanunseentestexample,thenweconcludethatthebrainactivitycontains
informationaboutthestimulus. Wemeasuretheaccuracyofthepredictionusingclassificationrank
accuracy(Section3.4). Anabovechanceaccuracydeemsthestimulusdecodable.
ThebrainactivitydataispreprocessedtoimprovethesignaltonoiseratiosimilartoWehbeetal[39].
Afterthepreprocessing,eachtrialyisrepresentedusinga(306,500)sizedmatrixcorrespondingto
the306sensorsand500mstime.Theencodingmodelislearnedtopredictbrainactivityfor25msnon-
overlappingwindows[23]. Eachofthe25msbrainactivityisaveragedover5msnon-overlapping
windowstoproduce(306,5)sizebrainactivityy forthepredictiontask. WeuseGeneralisedcross-
t
validationtotrainaridgeregressionmodel[9]withregularisationparameterλ∈[0.01,0.1,1,10].
Wecomputethestimulus(substring)representationusingBERTmodelSection3.1. Thepredicted
brain activity yˆ is evaluated based on a classification rank accuracy measure (Section 3.4). The
t
predictionandclassificationexperimentarerunusingcross-validation.
3.3 CandidateReferenceRepresentation
Meaning of a word can become more specific as the sentence context unfolds, for example, the
word “artist” can have different neural representation in the following two sentences “the artist
answeredtheparent”,“theartistchallengedtheparent”. Thebrainasaperfectlanguagemachine
updatesitswordrepresentationinsuchacase. Therefore,itisappropriatetoworkwithacandidate
referencerepresentationforneuralrepresentationofaword,whichdynamicallyevolveswithmore
sentencecontext. Onesimplewaytocomputethisrepresentationisbyaveragingallthosesentences
inwhichthewordofinterestappearsinthesametemporalposition. But,foruseintherankaccuracy
computation,thecandidatesshouldbeequallylikelytooccurinatestposition. Thefollowingmethod
computesthesecandidateswiththedesiredproperties.
Wedefinethefollowingnotationtodescribecandidatereferencerepresentation. Asingletrialis
representedby500msofstimuluspresentationtimeandisreferencedastimepositioni. Forexample,
in the sentence “the artist answered the parent", the word “artist" is at time position i = 2 and
“parent"attimepositioni=5. Thebrainactivityforapositioniinsentencesisdenotedbyy . A
s,i
setofsentenceswhere,wordwispresentatafixedtimepositionαinsentences,isdenotedbySα.
w
ThewordwcanberepresentedwithacandidatereferenceRα atanypositioni≥αinthesentence
w,i
sasfollows:
1 (cid:88)
Sα ={s|s[α]=w} , Rα = y (1)
w w,i |Sα| s,i
w s∈Sα
w
4
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
Figure2: ClassificationrankaccuracyofstimuluspredictedfromBERTlayer-18representations
fornoun1,verbandnoun2atvarioustimesduringsentencecomprehension. Wecontinuetesting
post-sentenceforanadditionalfour-timepositions(2000ms). “eos"refersto“endofasentence".
Our experiments reveal the resurgence of the first noun (noun1) representation when the second
noun(noun2)isencounteredinbothactiveandpassivesentences. And, thatbothnounandverb
representationsremaindecodableafterthesentenceends. PleaserefertoSection4.1formoredetails.
TherankreferenceRα istheaveragebrainactivityofthewordinagivencontext. Whenusing
w,i
thesecandidatereferencerepresentationsinrankscorecomputation,it’simportanttomakesurethat
eachcandidateforagivenαpositionisequallylikely. Tomakethecandidatescomparable,weadd
onemoreconstraintonthesetofsentencesSα. AnytwopairofsentencesetsS atgivenpositionα,
w
shouldconsistofsametoken-vocabulary(V )exceptthecandidatewordatpositionα(Equation2).
S
V ={v|v (cid:54)=w,v ∈s,s∈Sα} , V =V , Sk =Sα (2)
S w S1 S2 wk
PleaserecallthatourMEGdatasetconsistsofsentenceswith6nouns,5verbs. Thesenounsand
verbsarecombinedinallpossiblecombinationstoproduce300activeandpassivesentences. Sincea
nounisnotpairedwithitselfinasentence,thesetofsecondnounsforanytwogivenfirstnounsis
distinct,thusviolatingtheequaltoken-vocabularycondition(Equation2). Tosatisfytheconstraint,
wedividethenounsN intotwodistinctsetsasinEquation3tocomputerankaccuracyasfollows:
N =N ∪N , N ∩N =∅
1 2 1 2
s ={n|s[α]=n,n∈N },s ={n|s[α](cid:54)=n,n∈N }
n1 1 n2 2
S n1 ,α ={s|s[α]=n,s n1 =n,n∈N 1,s n2 ∈N 2} (3)
S2 ={s|s[α]=n,s =n,n∈N ,s ∈N }
n,α n1 2 n2 1
Acc=Average[Acc(S1 ), Acc(S2 )]
n,α n,α
Inshort,thenountokensaredividedintotwonon-overlappingsetsN andN . Thesentencesare
1 2
thengroupedintoaset(S1 ),suchthatthenoun1isinN andnoun2isinN andviceversaforset
n,α 1 2
(S2 ). RankaccuracyiscomputedastheaveragedecodingaccuracyofthetwosetsS1 andS2 .
n,α n,α n,α
3.4 ClassificationRankAccuracyMeasure
Weuseaclassificationrankaccuracymeasuretoevaluatethepresenceofanoun/verbintheearlier
sentencecontext. Therankaccuracymeasure[27]ismoresensitivethanthe0/1measurecommonly
usedintheliterature[22]. Thismeasureyieldsaconfidencevalue(cosinesimilarity)overpotential
candidatereferences(Section3.3). Weorderthereferencesbytheconfidencevaluesandassigna
rankr forthecorrectcandidatenoun/verbfromthesetofcandidatesreferencesC. Therankr is
convertedtoanaccuracy(Acc)usingtheEquation4. Thechancevaluefortherankaccuracymeasure
is0.5. PleaserefertotheAppendixforaproof.
(r−1)
Acc=1− (4)
(C−1)
5
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
Figure3: Cosinesimilarityofgoldlabeldata(irrespectiveofrank)predictedfromBERTlayer-18
representationsfornoun1,verb,noun2atduringsentencecomprehension. “eos"refersto“endofa
sentence". Weplotthisinformationforanadditional5secondspost-sentence. Inthelast1second
(eos9,eos10)anewsentenceisseenbythesubjectoraquestion(with15%chance). Weobserve
thatthewordrepresentationsareretainedinthecontextuntilnewinformationarriveswhenitslowly
startsdegrading. PleaserefertoSection4.1formoredetails.
4 Results
ExperimentswereperformedwiththeSimplePassActdataset(Section2),over25msnon-overlapping
timewindows. Thedecodingaccuracyisaveragedoverallthesubjects. Duetothenatureofour
dataset,itispossibletoencountersametextsubstringinboththetrainandtestset,butwithdifferent
brain activity data, for example “the artist answered the parent” and “the artist answered the
author” have the same text substring till the third word. We note that this is not a strict leakage
of information. However, to avoid any possible double-dipping into the test data, we performed
carefulcross-validation. Foranouncondition,notwosentenceswiththesameverbcouldseparately
occurintrainandtestsplit,resultingina5-fold(onefoldforeachverb)cross-validation. Similarly,
forverbcondition,a6-foldcross-validationbasedonthefirstnounisperformed. Pleasenotethat
imposingtheconstraintinEquation2reducesthetotalnumberofsentencesinthenounsettingto90
([3×5×3]×2). Bothnounandverbaredecodedfrom0mstotheendofthesentenceandbeyond.
BERTrepresentationofthelastwordinasentence,isusedtopredictthebrainactivityforallend
ofsentencepositions(eos1–eos10inFigure3). Whilewedonotexpectthebrainactivitybefore
stimulusonsettobeabletodecodethegivenstimuli,decodingaccuracyatchancelevelworksasa
goodsanitycheckforourresults. Thecomputationsdescribedinthispaperwereperformedusinga
multiprocessorIntelXeonE5serverwith377GBRAMand24threads. Theaveragerun-timefor
computingasingleexperiment(forexample,figure2)isapproximately4hours.
4.1 Wholebrainstimuluspredictionaccuracy
Therankaccuracyofpredictingnoun1,verbandnoun2atvarioustimesduringsentencecompre-
hensionisshowninFigure2. Weusethewholebrain(306sensors)datatopredictthestimulus
forthisanalysis. Wefindthattherankaccuracyofallthewordsishighpost-sentence. Thismakes
sense,asthebrainretainstherepresentationofearliersentenceandthereforegetsthe“argmax(cosine
similarity)"rightintherankaccuracymeasure. Totestthedegreeofretentionofthisinformation,
wealsoplotthecosinescoreforthegold-candidateinFigure3. Thisplotdisplaysaccuraciestill
5secondspost-sentence,ofwhich4secondsisaBLANKscreenfollowedby1sofnextsentence
stimulioraquestion(with15%chance). Fromtheplot,weobservethatthewordrepresentationsare
retainedinthebrainuntilanewpieceofinformationarriveswhenitslowlystartsdegrading.
NounComprehension: Weobservethatthefirstnoun(subjectinactiveandobjectinpassive)is
decodedwithsignificantaccuracyduringthewordpresentationandcontinuestobesignificantly
decodableinthebraintilltheendofsentenceactivity. Inaddition,itsdecodabilityimproveswhena
secondnounisencounteredandremainsactiveevenat2spastthelastword. Similarendofsentence
decodabilityresultswasreportedinFysheetal. [7]. Thelackofpredictivequalityfromnoun1to
6
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
Figure4: RankAccuracyofpredictingstimulusfromeachsensorlocation(102locationseachwith3
sensors). Theresultingrankaccuracywasaggregatedinsignalspaceoverbrainregionsdelineated
bythepositionofeachsensorintheMEGhelmet. Thex-axisrepresentsthewordpositioninthe
sentence. Each word position is 500 ms of brain activity data. The figure shows verb decoding
accuracyfromthepositionofoccurrencetotheend-of-sentenceactivity.Wefindthatleft,rightfrontal
andrighttemporalregionsarethemostaccurateatpredictingwordrepresentationduringsentence
comprehension. TheseBrainareasunderlyingouroutlinedsensorregionshavebeenimplicatedin
memoryandexecutivefunctionbypreviousneuroscienceresearch. PleaserefertoSection4.2for
moredetails.
noun2intheSimplePassActdataset,mightnecessitatenoun1recollectionforsemanticcomposition
whennoun2isshown.
VerbComprehension: Similartonouns,verbsaredecodablewithhighaccuracyduringandafter
thepresentationoftheword. But,incontrasttonouns,theverbdecodabilityremainsheightenedfor
theremainderofsentencereading. Thisresultsupportsthegrowingbodyofliteratureforsuccessful
in-contextverbdecoding[16,24].
Active v/s passive sentence comprehension: Both voices exhibit similar post-sentence neural
representation. Duringthesentencereading,noun1(object)inpassivesentencedisplaysdifferent
temporaldecodabilityascomparedtonoun1(subject)inactivevoice. Weleavefurtherexploration
intothiseffectasfuturework.
4.2 Whenandwherearetherepresentationspredictedintime?
TheMEGhelmetconsistsof306sensors,distributedover102locations. Tounderstandthespatial
distributionofwordinformationinthebrain,wepredictedthenoun1,verb,noun2atdifferenttimes
witheachsensorlocation(3sensors)inthebrain. Theresultsfromtheanalysiswerethenaggregated
for larger brain regions (similar to Figure 1 in Hu et al. [13]). We examined the primary visual
areas(leftandrightoccipitallobe),speechandlanguageprocessingareas(lefttemporal)andverbal
memory(righttemporal),sensoryperception(leftparietal)andintegration(rightparietal),language
relatedmovements(leftfrontal)andnon-verbalfunctioning(rightfrontal). Theaggregatedbrain
region accuracy is shown in Figure 6 for verb stimuli. The left, right frontal and right temporal
brainregionsarethemostaccurateatpredictingwordrepresentation. Thesebrainregionshavebeen
implicatedinmemoryandexecutivefunctioninlanguagenetworks[29,5]. Wefindsimilartrends
fornouns,pleasefindmoreinformationintheAppendixsection.
4.3 Whathappenswhenaquestionisencountered?
InSimplePassActdataset,15%(45)ofthesentencesarefollowedbyaquestionandthenashort
pause to allow the user to answer that question. How do the neural representations of the words
presentedearlierinthesentenceevolvewhenaquestionisencountered? InFigure5,weshowthe
comparisonofcosinesimilarityscoresofearlierNoun1,VerbandNoun2betweenconditions“during
a question presentation" (Only-QA), vs “during blank screen before another sentence" (No-QA).
Pleasenotethatthesubjectsdonotknowwhichsentencesarefollowedbyaquestion. Anupcoming
questionisindicatedby“?"sign,whichappearsonthescreenbetween800-1000msafterendofa
sentence. Tomakethe“Only-QA"and“No-QA"settingcomparableintermsoftheregressionmodel,
wesub-sampledthesentencesinthe“No-QA"settingtomatchthatof“Only-QA",approximately
23sentencesineachsentencevoice. Weobserve,inthecaseofaquestion,thesubjectretainsthe
sentenceinformationforalongerperiodoftimeascomparedtothenoquestionsetting,perhapsto
7
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
Figure5: Comparisonbetweenpost-sentencerepresentationsin“Only-QA"versus“No-QA"set,and
the15pointmovingaverageofthecorrespondingseries. Weobservethat,inthecaseofaquestion,
thesubjectretainsthesentenceinformationforalongerperiodoftimeascomparedtothenoquestion
set. PleaserefertoSection4.3formoredetails.
answerthequestion. Allthreesubjectshadcloseto100%accuracyincorrectlyansweringthese
engagementquestions.
5 RelatedWork
To study human language comprehension, researchers have used neuroimaging methods such as
FunctionalMagneticResonanceImaging(FMRI)[8],electroencephelography(EEG)[2]andMag-
netoencephalography(MEG)[21]etc. Evidencefromdeficitstudies,FMRI,MEG,Eventrelated
potential(ERP)imagingstudieshavebeencombinedtoformulateneuralmodelsoflanguagecom-
prehension [6, 11, 12, 28]. Recently, Machine learning (ML) models are becoming popular in
understandingthebrainimagingdatainthefieldofneuroscience. Thesemodelscanbeverysen-
sitive in determining what stimulus information is present in the brain activity data. In addition,
Deep Learning models trained on a large corpus of data are also used to provide rich stimulus
featuresfortheMLmodels[15,39,30]. Priorresearchbymultiplepapershasestablishedageneral
correspondencebetweenacomputationalmodelandthebrain’sresponsetonaturalisticlanguage
[22,33,25,34,32,31]. SomestudieshavesuccessfullyusedtheBERTmodeltopredictthebrain
activitydata[16,36]. Forsentencecomprehension,eventrelatedpotential(ERP)studiespresent
anomaloussentencestothesubjectstostudythetiming(P600,N400)ofvariouscompositioneffects.
Incontrast,fewstudiesunderstandthesemanticcompositionusingnaturalisticmethods,specifically
methodsthatincludestimuliwithnosurprisaleffectorsemanticallypredictablequality[7]. Weadd
tothesepreviousresearchworkstofurtherourunderstandingofsentencecomprehension.
6 Conclusion
Inthispaper,wepresentaframeworktostudyhowneuralrepresentationsofwordspresentearlier
inasentenceevolveassubjectsreadasentencefromleft-to-right. Moreprecisely,wefocusonthe
representationsofnounandverbintheneuralencodingofasentence. Ourexperimentsrevealthe
resurgenceofthefirstnoun(noun1)representationwhenthesecondnoun(noun2)isencountered
inbothactiveandpassivesentences. Theverbrepresentationremainsimportantfromtheonsetof
itspresentationthroughtheremainderofthesentence. Bothnounandverbrepresentationsremain
decodableafterthesentenceends. WealsorevealimportantcontributionsfromFrontalandTemporal
regionsofthebrainincontextcomposition. Ourexplorationisthefirsttostudytheeffectofquestion-
answeringtaskontheneuralrepresentationofthewordspost-sentence. Wefindthatpost-sentence
question-answeringhelpsinmaintainingsentencerepresentationinthebrain. Finally,weprovide
anewMEGdatasetof300simplesentencesintheactiveandpassivevoice. Infuture,weplanto
8
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
extendouranalysisby: (1)sourcelocalisingtheneuraldatatogainfine-grainedspatialresolutionin
theanalysis,and(2)collectingmoredatatoperformadditionalanalysispertainingtobrainbehaviour
duringquestion-answering.
7 BroaderImpact
Cognitivestudiesfornaturallanguageunderstanding,endeavourtodiscoverbrainmechanismsfor
languageunderstanding.Thenon-invasivenatureofMEG/fMRI/PET/EEGstudiesmakesalotofdata
availableforthesame. Inthepast,suchanexperimentrequiredcarefulchoiceofstimulitodiscover
theimpactofaconditionofinterestonthebrain. Incontrast,Machinelearningenablesnaturalistic
experimentsbydiscoveringdiscriminativeinformationaboutaconditionofinterest. DeepLearning
hasrecentlybeenappliedtothiskindofstudybyrelatingbrainandlanguageunderstandingmachines.
Thecoreprincipleofthismatchistobeabletodetectinformationflowinthebrainusingbetter
andmorecompactrepresentationsoflanguageusingdeeplearningascomparedtothealternative
one-hotvectorsordiscreetlabels. Inthispaper,weproposeamethodologicalapproachtodetect
representationsofwordsfromthepastinthecurrentcontextcompositioninthebrain. Enablinga
betterunderstandingoflanguageunderstandinginthebrainwhichshouldhopefullyinformbetter
designofAImodelsforlanguageunderstandingtasks. However,thefindingsofthisstudyarelimited
tothenativeEnglishlanguagespeakers. Thesmallsamplesizeofboththestimulusandthesubjects
alsoleadstobiasinthefindingsofsuchstudies.
Societally,understandingbrainrepresentationsisconsideredkeytodiscoveringtrueintelligenceand
toimprovemachinelearningmethodstoreachtheaccuracylevelsofahuman. However,wealso
havetobecarefultoavoidanybiasesinthedevelopmentofsuchalgorithms. Thedataandalgorithms
shouldbegeneralandincludeinformationaboutminoritypopulationetc. Inthecontextofourstudy,
thiswouldmeanstudyingthelanguagecomprehensionforavarietyofdifferentlanguagestodiscover
thetruelanguagecomprehension.
Aswedeepenourunderstandingoflanguageprocessinginthebrain,thisislikelytoinspiremore
sophisticatedNaturalLanguageUnderstanding(NLU)machines. WhilesuchadvancedNLUmaybe
usedformanypositiveusessuchasdrugdiscovery,informationaccess,etc.,thesametechnology
maybeusedfornefariousactssuchasspreadingmisinformation. Itisthereforevitalforthefieldto
bevigilantofsuchmisuseoftechnology,anddevelopguardagainstthose.
References
[1] MarcelBastiaansen,LillaMagyari,andPeterHagoort. Syntacticunificationoperationsare
reflectedinoscillatorydynamicsduringon-linesentencecomprehension. Journalofcognitive
neuroscience,22:1333–47,082009.
[2] Hans Berger. Über das elektrenkephalogramm des menschen. Archiv für Psychiatrie und
Nervenkrankheiten,106(1):165–187,1937.
[3] MarkDavies. CorpusofContemporaryAmericanEnglish(COCA). 2015.
[4] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. InProc.ofNAACL,2019.
[5] P.C.FletcherandR.N.A.Henson. Frontallobesandhumanmemory: Insightsfromfunctional
neuroimaging. Brain,124(5):849–881,052001.
[6] Angela D. Friederici. The brain basis of language processing: From structure to function.
PhysiologicalReviews,91(4):1357–1392,2011. PMID:22013214.
[7] AlonaFyshe,GustavoSudre,LeilaWehbe,NicoleRafidi,andTomM.Mitchell. Thesemantics
ofadjectivenounphrasesinthehumanbrain. bioRxiv,2019.
[8] GaryHGlover. Overviewoffunctionalmagneticresonanceimaging. Neurosurgeryclinicsof
NorthAmerica,22(2):133–vii,042011.
[9] GeneH.Golub,MichaelHeath,andGraceWahba. Generalizedcross-validationasamethod
forchoosingagoodridgeparameter. Technometrics,21(2):215–223,1979.
9
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
[10] JoachimGross,SylvainBaillet,GarethBarnes,RichardHenson,ArjanHillebrand,OleJensen,
KarimJerbi,VladimirLitvak,BurkhardMaess,RobertOostenveld,LauriParkkonen,Jason
Taylor,VirginieVanWassenhove,MichaelWibral,andJan-MathijsSchoffelen. Goodpractice
forconductingandreportingmegresearch. NeuroImage,65,102012.
[11] PeterHagoort. MUC(Memory,Unification,Control)andbeyond. Frontiersinpsychology,
4:416,jul2013.
[12] GregoryHickokandDavidPoeppel. Thecorticalorganizationofspeechprocessing. Nature
ReviewsNeuroscience,8(5):393–402,2007.
[13] YegangHu,ChunliYin,JicongZhang,andYupingWang. PartialLeastSquareAidedBeam-
formingAlgorithminMagnetoencephalographySourceImaging. Frontiersinneuroscience,
12:616,2018.
[14] Annika Hultén, Jan-Mathijs Schoffelen, Julia Uddén, Nietzsche H.L. Lam, and Peter Ha-
goort. Howthebrainmakessensebeyondtheprocessingofsinglewords–AnMEGstudy.
NeuroImage,186:586–594,feb2019.
[15] ShaileeJainandAlexanderHuth. Incorporatingcontextintolanguageencodingmodelsforfmri.
InS.Bengio,H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,editors,
AdvancesinNeuralInformationProcessingSystems31,pages6628–6637.CurranAssociates,
Inc.,2018.
[16] Sharmistha Jat, Hao Tang, Partha Talukdar, and Tom Mitchell. Relating simple sentence
representations in deep neural networks and the brain. In Proceedings of the 57th Annual
MeetingoftheAssociationforComputationalLinguistics,pages5137–5154,Florence,Italy,
July2019.AssociationforComputationalLinguistics.
[17] SheliaMKennisonandJessieLTrofe. ComprehendingPronouns: ARoleforWord-Specific
GenderStereotypeInformation. JournalofPsycholinguisticResearch,32(3):355–378,2003.
[18] AntonioKolossaandBrunoKopp. Dataqualityoverdataquantityincomputationalcognitive
neuroscience. NeuroImage,172,012018.
[19] GinaKuperberg. Neuralmechanismsoflanguagecomprehension: Challengestosyntax. Brain
research,1146:23–49,052007.
[20] M Kutas and SA Hillyard. Reading senseless sentences: brain potentials reflect semantic
incongruity. Science,207(4427):203–205,1980.
[21] HMatti,RiittaHari,RistoIlmoniemi,JukkaKnuutila,andOlliV.Lounasmaa. Magnetoen-
cephalography: Theory,instrumentation,andapplicationstononinvasivestudiesoftheworking
humanbrain. Rev.Mod.Phys.,65:413–,041993.
[22] TomM.Mitchell,SvetlanaV.Shinkareva,AndrewCarlson,Kai-MinChang,VicenteL.Malave,
RobertA.Mason,andMarcelAdamJust. Predictinghumanbrainactivityassociatedwiththe
meaningsofnouns. Science,320(5880):1191–1195,2008.
[23] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P.Prettenhofer,R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,
M.Perrot,andE.Duchesnay. Scikit-learn: MachineLearninginPython. JournalofMachine
LearningResearch,12:2825–2830,2011.
[24] Daniela Perani, Stefano F. Cappa, Tatiana Schnur, Marco Tettamanti, Simona Collina,
MàrioMiguelRosa,andFerruccioFazio1. Theneuralcorrelatesofverbandnounprocessing:
APETstudy. Brain,122(12):2337–2344,121999.
[25] Francisco Pereira, Bin Lou, Brianna Pritchett, Samuel Ritter, Samuel J. Gershman, Nancy
Kanwisher,MatthewBotvinick,andEvelinaFedorenko.Towardauniversaldecoderoflinguistic
meaningfrombrainactivation. NatureCommunications,9(1):963,2018.
[26] Nicole Rafidi. The role of syntax in semantic processing: A study of active and passive
sentences,2014. [Online;accessed2-March-2019].
10
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
[27] NicoleS.Rafidi. Usingmachinelearningfortimeseriestoelucidatesentenceprocessinginthe
brain,2018.
[28] MatthewRalph,ElizabethJefferies,KaralynPatterson,andTimothyRogers. Theneuraland
computationalbasesofsemanticcognition. NatureReviewsNeuroscience,18,112016.
[29] GraceERice,HelenCaswell,PerryMoore,PaulHoffman,andMatthewALambonRalph.
TheRolesofLeftVersusRightAnteriorTemporalLobesinSemanticMemory: ANeuropsy-
chological Comparison of Postsurgical Temporal Lobe Epilepsy Patients. Cerebral Cortex,
28(4):1487–1501,012018.
[30] BlakeARichards,TimothyPLillicrap,DenisTherien,KonradPKording,PhilippeBeaudoin,
YoshuaBengio,RafalBogacz,andAmeliaChristensen. FOCUS|PersPectiveAdeeplearning
frameworkforneuroscience. NatureNeuroscience,16:42.
[31] DanSchwartzandTomMitchell. Understandinglanguage-elicitedEEGdatabypredicting
it from a fine-tuned language model. In Proceedings of the 2019 Conference of the North
AmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTech-
nologies,Volume1(LongandShortPapers),pages43–57,Minneapolis,Minnesota,June2019.
AssociationforComputationalLinguistics.
[32] Dan Schwartz, Mariya Toneva, and Leila Wehbe. Inducing brain-relevant bias in natural
languageprocessingmodels. InAdvancesinNeuralInformationProcessingSystems32,pages
14123–14133.CurranAssociates,Inc.,2019.
[33] GustavoSudre,DeanPomerleau,MarkPalatucci,LeilaWehbe,AlonaFyshe,RiittaSalmelin,
and Tom Mitchell. Tracking neural coding of perceptual and semantic features of concrete
nouns. NeuroImage,62:451–63,052012.
[34] JingyuanSun,ShaonanWang,JiajunZhang,andChengqingZong. Towardssentence-level
braindecodingwithdistributedrepresentations. AAAIPress,2019.
[35] SamuTauluand RiittaHari. Removal ofmagnetoencephalographic artifactswith temporal
signal-spaceseparation: demonstrationwithsingle-trialauditory-evokedresponses. Human
brainmapping,305:1524–34,2009.
[36] MariyaTonevaandLeilaWehbe. Interpretingandimprovingnatural-languageprocessing(in
machines)withnaturallanguage-processing(inthebrain). InAdvancesinNeuralInformation
ProcessingSystems,pages14928–14938,2019.
[37] MikkoUusitaloandRistoIlmoniemi. Signal-spaceprojectionmethodforseparatingmegor
eegintocomponents. Medical,biologicalengineeringandcomputing,35:135–40,041997.
[38] LeilaWehbe,BrianMurphy,ParthaTalukdar,AlonaFyshe,AadityaRamdas,andTomMitchell.
Simultaneously uncovering the patterns of brain regions involved in different story reading
subprocesses. PloSone,9:e112575,112014.
[39] LeilaWehbe,AshishVaswani,KevinKnight,andTomM.Mitchell. Aligningcontext-based
statisticalmodelsoflanguagewithbrainactivityduringreading. InEMNLP,pages233–243.
ACL,2014.
[40] MashaWesterlundandLiinaPylkkänen. Theroleoftheleftanteriortemporallobeinsemantic
compositionvs.semanticmemory. Neuropsychologia,57,052014.
11
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
A Appendices
A.1 ChancePerformanceforRankAccuracy
Inthepredictiontask(stimulusfeaturestobrainactivity),wescorethepredictionbyrankingalistof
goldcandidatereferencesbycosinesimilarity. Ifthecorrectcandidateisatthetoprankposition,we
assignthepredictiona100%accuracyusingthefollowingformula:
r =rankof thetrueclass
C =numberof classes
A=RankAccuracy
(5)
(r−1)
A=1−
(C−1)
Theexpectedvalueoftheaccuracyestimatesthesystemperformanceunderrandomprediction.
(r−1)
E(A)=E(1− )
(C−1)
UsingLinearityofExpectation
(r−1)
=E(1)−E( ))
(C−1) (6)
1
=1−( ×E(r−1))
(C−1)
1
=1−[ ×(E(r)−1)]
(C−1)
In a random prediction, the true answer is equally likely to be ranked at any of the C positions.
Therefore,theexpectedvalueoftherandomrankwouldbe C+1.
2
1
E(r)= ×(1+2+...+C)
C
1 (C×(C+1))
= ×[ ] (7)
C 2
C+1
=
2
substitutingEquation7inEquation6
1 C+1
E(A)=1−[ )×( −1)]
C−1 2
1 (C−1)
=1−[ × ]
(C−1) 2 (8)
1
=1−
2
=0.5
Thusproved,thechanceperformanceofrankaccuracymeasureasdefinedinEquation5is50%.
12
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
A.2 SimplePassActdatasetStimuli
ThestimuliofSimplePassActdatasetisdesignedtobebalancedforsubjectsandobjectsinthedataset.
Theactiveandpassivesentencesinthedatasethavethefollowingsimplepattern:
“the[noun]was[verb]bythe[noun]"
“the[noun][verb]the[noun]"
Suchthat,
noun∈[singer,baker,customer,parent,artist,author]
verb∈[encouraged,attached,answered,challenged,followed]
Weconstructthesentenceswithallpossiblecombinationofnounsandverbs. Theonlyconditionis
thatanouncannotbepairedwithitselfinthesamesentence. Thetotalnumberofactiveandpassive
sentencesthusformedare300. Someexamplesentencesareasfollows:
“thebakerwasansweredbytheparent"
“theparentansweredthebaker"
“thesingerwasansweredbytheparent"
“theparentansweredthesinger"
13
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
Figure6: RankAccuracyofpredictingstimulus(noun1,verb,noun2)fromeachsensorlocation(102
locationseachwith3sensors). Theresultingrankaccuracywasaggregatedinsignalspaceoverbrain
regionsdelineatedbythepositionofeachsensorintheMEGhelmet. Thex-axisrepresentstheword
positioninthesentence. Eachwordpositionis500msofbrainactivitydata. Forexamplenoun1
appearsatposition2intheactivesentence,thefigureshowsnoun1decodingaccuracyfromposition
2toend-of-sentenceactivity. Wefindthatleft,rightfrontalandrighttemporalregionsarethemost
accurateatpredictingwordrepresentationduringsentencecomprehension. PleaserefertoSection
4.2formoredetails.
14
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
(a)Candidatereferencefornoun“artist"
(b)Candidatereferencefornoun“author"
(c)Candidatereferencefornoun“baker"
(d)Candidatereferencefornoun“customer"
(e)Candidatereferencefornoun“parent"
(f)Candidatereferencefornoun“singer"
Figure7: Exampletopomapvisualisationofthecandidatereferencesfornoun1atwordposition
(α)4inanactivesentence(eg. “theartistansweredthe"). Thebrainactivitydataisshownfor500
mswith100msnon-overlappingwindows. Weobservedistinctactivitypatternsinthecandidate
referencesfromthevisualisation.ThesedifferencesarecapturedbytheMLalgorithmstohelppredict
thecorrectstimulus. PleaserefertoSection3.3formoredetails.
15
bioRxiv preprint doi: https://doi.org/10.1101/2020.06.22.163808; this version posted June 22, 2020. The copyright holder for this preprint
(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made
available under aCC-BY-ND 4.0 International license.
(a)Candidatereferenceforverb“answered"
(b)Candidatereferenceforverb“attacked"
(c)Candidatereferenceforverb“challenged"
(d)Candidatereferenceforverb“encouraged"
(e)Candidatereferenceforverb“followed"
Figure8: Topomapvisualisationofthecandidatereferenceforverbsatwordposition(α)6ina
passivesentence(eg. “theartistwasansweredbythe"). Thebrainactivitydataisshownfor500
mswith100msnon-overlappingwindows. Weobservedistinctactivitypatternsinthecandidate
referencesfromthevisualisation. PleaserefertoSection3.3formoredetails.
16

Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems
AasishPappu AlexanderI.Rudnicky
SchoolofComputerScience
CarnegieMellonUniversity
{aasish, air}@cs.cmu.edu
Abstract slot/values that are unavailable on the web, e.g.,
regarding a recent interest/hobby of the user’s
Many goal-oriented dialog agents are ex- friend. Lasecki et al. [2013] have elicited natu-
pected to identify slot-value pairs in a ral language dialogs from humans to build NLU
spoken query, then perform lookup in models for the agent and Bigham et al. [2010]
a knowledge base to complete the task. have elicited answers to visual questions by in-
Whentheagentencountersunknownslot- tegrating users into the system. One observation
values, itmayasktheusertorepeatorre- from this work is that both users and non-users
formulate the query. But a robust agent can impart useful knowledge to system. In this
canproactivelyseeknewknowledgefrom paper we propose spoken language strategies that
auser,tohelpreducesubsequenttaskfail- allow an agent to elicit new slot-value pairs from
ures. Inthispaper,weproposeknowledge its own user population to extend its knowledge
acquisition strategies for a dialog agent base. Open-domain knowledge may be elicited
andshowtheireffectiveness. Theacquired through text-based questionnaires from non-users
knowledge can be shown to subsequently ofthesystem,butinasituatedinteractionscenario
contributetotaskcompletion. spoken strategies may be more effective. We ad-
dressthefollowingresearchquestions:
1 Introduction
1. Cananagentelicitreliableknowledgeabout
Many spoken dialog agents are designed to per- its domain from users? Particularly knowl-
form specific tasks in a specified domain e.g., in- edge it cannot locate elsewhere (e.g., on-line
formation about public events in a city. To carry knowledge bases). Is the collective knowl-
outitstask,anagentparsesaninpututterance,fills edgeoftheuserssufficienttoallowtheagent
inslot-valuepairs,thencompletesthetask. Some- toaugmentitsknowledgethroughinteractive
times, information on these slot-value pairs may means?
not be available in its knowledge base. In such
2. What strategies elicit useful knowledge from
cases,typicallytheagentcategorizesutterancesas
users? Based on previous work in com-
non-understanding errors. Ideally the incident is
mon sense knowledge acquisition [Von Ahn,
recorded and the missing knowledge is incorpo-
2006, Singh et al., 2002, Witbrock et al.,
ratedintothesystemwithadeveloper’sassistance
2003], we devise spoken language strategies
—aslowofflineprocess.
thatallowthesystemtosolicitinformationby
Thereareothersourcesofknowledge: automat-
presenting concrete situations and by asking
ically crawling the web, as done by NELL [Carl-
user-centricquestions.
son et al., 2010], and community knowledge
bases such as Freebase [Bollacker et al., 2008]. Weaddressthesequestionsinthecontextofthe
These approaches provide globally popular slot- EVENTSPEAK dialogsystem,anagentthatprovides
values[Araki,2012]andhigh-levelsemanticcon- information about seminars and talks in an aca-
texts [Pappu and Rudnicky, 2013]. Despite their demic environment. This paper is organized as
size, these knowledge bases may not contain in- follows. In Section 2, we discuss knowledge ac-
formation about the entities in a specific target quisition strategies. In Section 3, we describe a
domain. However, users in the agent’s domain user study on these strategies. Then, we present
can potentially provide specific information on an evaluation on system acquired knowledge and
finallywemakeconcludingremarks.
194
ProceedingsoftheSIGDIAL2014Conference,pages194–198,
Philadelphia,U.S.A.,18-20June2014.(cid:13)c2014AssociationforComputationalLinguistics
Table1:SysteminitiatedstrategiesusedbytheagentforknowledgeacquisitionintheEVENTSPEAKsystem.
StrategyType Strategy ExamplePrompt
QUERYEVENT Iknoweventsoncampus.Whatdoyouwanttoknow?
QUERYDRIVEN
QUERYPERSON Iknowsomeoftheresearchersoncampus.Whomdoyouwanttoknowabout?
BUZZWORDS Whataresomeofthepopularphrasesinyourresearch?
PERSONAL
FAMOUSPEOPLE Tellmesomewell-knownpeopleinyourresearcharea
TWEET Howwouldyoudescribethistalkinasentence,sayatweet.
SHOW&ASK KEYWORDS Givekeywordsforthistalkinyourownwords.
PEOPLE Doyouknowanyonewhomightbeinterestedinthistalk?
2 KnowledgeAcquisitionStrategies helptowardsbettertaskperformance.
We posit three different circumstances that can 3 KnowledgeAcquisitionStudy
trigger knowledge acquisition behavior: (1) initi-
atedbyexpertusersofthesystem[Holzapfeletal., Weconductedauserstudytodeterminereliability
2008,Spexardetal.,2006,Lu¨tkebohleetal.,2009, oftheinformationacquiredbythesystem. Weper-
Rudnicky et al., 2010], (2) triggered by “misun- formed this study using the EVENTSPEAK1 dialog
derstanding” of the user’s input [Chung et al., system,whichprovidesinformationaboutupcom-
2003,FiliskoandSeneff,2005,Prasadetal.,2012, ing talks and other events that might be of inter-
Pappuetal.,2014],or(3)triggeredbythesystem. est, and about ongoing research on campus. The
Theyaredescribedbelow: system presents material on a screen and accepts
spokeninput,inacontextsimilartoakiosk.
QUERYDRIVEN. The system prompts a user
withanopen-endedquestionakinto“how-may-I- The study evaluated performance of the seven
help-you” to learn what “values” of a slot are of strategies described above. For SHOW&ASK strate-
interest to the user. This strategy does not ground gies, we had users respond regarding a specific
user about system’s knowledge limitations. How- event. We used descriptions of research talks col-
ever, it allows the system to acquire information lected from the university’s website. We used a
(slot-value pairs) from user’s input. The system web-based interface for data collection; the inter-
can choose to respond to the input or ignore the face presented the prompt material and recorded
input depending on its knowledge about the slot- the subject’s voice response. Testvox2 was used
value pairs in the input. Table 1 shows strategies to setup the experiments and Wami3 for audio
ofthiskindi.e.,QUERYEVENTandQUERYPERSON. recording.
PERSONAL. Thesystemasksauserabouttheir
3.1 UserStudyDesign
owninterestsandpeoplewhomaysharethosein-
terests. Thisisanopen-endedrequestaswell,but We recruited 40 researchers (graduate students)
the system expects the response to be confined to fromtheSchoolofComputerScience,atCarnegie
the user’s knowledge aboutspecific entities in the Mellon, representative of the user population for
environment. BUZZWORDS and FAMOUSPEOPLE ex- the EVENTSPEAK dialog system. Each subject re-
pectstheusertoprovidevaluesfortheslots. sponded to prompts from the QUERYDRIVEN, PER-
SHOW&ASK. Thesystemprovidesadescrip- SONALandSHOW&ASKstrategies.
tion of an event and asks questions to ground In the QUERYDRIVEN tasks, the QUERYEVENT
user’s responses in relation to that event. E.g., strategy, the system responds to the user’s query
given the title and abstract of a technical talk, with a list of talks. The user’s response is
the system asks the user questions about the talk. recorded, then sent to an open-vocabulary speech
TWEET strategy is expected to elicit a concise de- recognizer; the result is used as a query to a
scription of the event, which eventually may help databaseoftalks. Theresultsarethendisplayedon
theagenttobothsummarizeeventsforotherusers the screen. The system applies the QUERYPERSON
and identify keywords for an event. KEYWORDS strategy in a similar way. In the PERSONAL tasks,
strategy expects the user to explicitly supply key- the system applies the BUZZWORDS strategy to ask
words for an event. PEOPLE strategy expects the theuseraboutpopularkeyphrasesintheirresearch
usertoprovidenamesoflikelyeventparticipants.
1http://www.speech.cs.cmu.edu/apappu/kacq
Wehypothesizedthatthesestrategiesmayallow
2https://bitbucket.org/happyalu/testvox/wiki/Home
the agent to learn new slot-value pairs that may 3https://code.google.com/p/wami-recorder/
195
Figure1:TimeperTaskforallstrategies Figure2:TimeperTaskvsExpertise
tweet
4
people
4
keywords
3
2.51 3
2.23
2
1.51 2
0.91 0.97
1 0.71 0.69 1
0 0
Q
uery E v Qe un et ry Perso Bn uzzw Faor md os usPeople T weet People Key w ords
E x p
ertL evel1
E x p
ertL evel2
E x p
ertL evel3
E x p
ertL evel4
area. The system then asks about well-known re- our initial observations that this strategy elicits
searchers(FAMOUSPEOPLE)intheuser’sarea. diverse responses. The PERSONAL task produced
In the SHOW&ASK tasks, we use two seminar a relatively higher number of researcher names
descriptions per subject (in our pilot study, we (FAMOUSPEOPLEstrategy)thanothertasks. Oneex-
found that people provide more diverse responses planation might be that people may find it easier
(in term of entities) in the SHOW&ASK based on torecallnamesintheirownresearcharea,ascom-
the event abstract, compared to PERSONAL, QUERY- pared to other areas. Overall, we identified 139
DRIVEN). We used a set of 80 research talk an- uniqueresearchernamesand485interests.
nouncements (consisting of a title, abstract and
Table2:CorpusStatistics
otherinformation). Foreachtalk,thesystemused
Unique Unique
allthreestrategiesviz.,TWEET,KEYWORDSandPEO-
StrategyType Researcher Research
PLE. For the TWEET tasks, subjects were asked to Names Interests
provideaonesentencedescription. Theywereal-
QUERYDRIVEN 21 30
lowed to give a non-technical/high-level descrip-
PERSONAL 77 107
tion if they were unfamiliar with the topic. For SHOW&ASK 76 390
thePEOPLEtask,subjectshadtogivenamesofcol-
Overall 139 485
leagues who might be interested in the talk. For
the KEYWORDS task, subjects provided keywords,
3.3 CorpusAnalysis
either their own words or ones selected from the
abstract. One of the objectives of this work is to determine
Since the material is highly technical, we were What strategies can the agent use to elicit knowl-
interested whether the tasks are cognitively de- edge from users? Although, time-cost will vary
mandingforpeoplewhoarelessfamiliarwiththe withtaskanddomain,ausablestrategyshould,in
subject of a talk. Therefore, users were asked to general,belessdemanding. Weanalyzedthetime-
indicatetheirfamiliaritywithaparticulartalk(re- per-task for each strategy, shown in Figure 1. We
searchareaingeneral)usingascaleof1–4: 4be- foundthattheTWEETstrategyisnotonlymorede-
ingmorefamiliarand1beinglessfamiliar. manding, it has higher variance than other tasks.
One explanation is that people would attempt to
3.2 CorpusDescription
summarize the entire abstract including technical
Thisuserstudyproduced64minutesofaudiodata, details, despite the instructions indicated that a
on average 1.6 minutes per subject. We tran- non-technicaldescriptionwasacceptable. Wecan
scribed the speech then annotated the corpus for see a similar trend in Figure 2 that irrespective
people names, and for research interests. Table 2 ofexpertise-level, subjectstakemoretimetogive
shows the number of unique slot-values found in one sentence descriptions. We also observe high
thecorpus. Weobservethatthenumberofunique variance and higher time-per-task for QUERYPER-
research interests produced during SHOW&ASK is SON; this is due to the system deliberately not re-
higher than for other strategies. This confirms turninganyresultsforthistask. Thiswasdoneto
196
setunimniemiT setunimniemiT
Table3:MeanPrecisionfor200researchers,brokendownbythe“source”strategyusedtoacquiretheirname
Note:Only85of200researchershadGoogleScholarpages,GScholarAccuracyiscomputedforonlythose85.
Metric DescriptionText SHOW&ASK PERSONAL QUERYDRIVEN mean
MeanPrecision 89.5% 86.9% 93.6% 86.2% 90.5%
GScholarAcc. 78.3% 82.3% 86.1% 100% 80.0%
findoutwhethersubjectswouldrepeatthetaskon commonmetricin IR, toevaluateretrieval. Inour
failure. Ideallythesystemneedstoonlyrarelyuse case,thegroundtruthforrelevantinterestscomes
thisstrategytonotloseuser’strustandsolicitmul- from the annotators. The results are shown in Ta-
tiplevaluesforagivenslot(e.g., personname)as ble 3. Our approach has high precision, 90.5%,
opposed torequesting list of valuesas in FAMOUS- for all 200 researchers. We see that irrespective
PEOPLEandPEOPLEstrategies. WefindthatPEOPLE, of the strategy used to acquire entities, precision
KEYWORDS, FAMOUSPEOPLE and BUZZWORDS strate- is good. We also compared our predicted inter-
gies are efficient with a time-per-task of less than estswithinterestslistedbyresearchersthemselves
oneminute. AsshowninFigure2,subjectsdonot onGoogleScholar. Thereareonly85researchers
take much time to speak a list of names or key- fromourlistwithaGoogleScholarpage;forthese
words. our accuracy is 80%, again good. Moreover, sig-
nificantknowledgeisabsentfromtheweb(atleast
4 EvaluationofAcquiredKnowledge
in our domain) yet can be elicited from users fa-
miliarwiththedomain.
To answer Can an agent elicit reliable knowl-
edge about its domain from users? we analyzed
the relevance of acquired knowledge. We have 5 Conclusion
two disjoint list of entities, (a) researchers and
(b)researchinterests;inadditionwehavespeaker
Wedescribeasetofknowledgeacquisitionstrate-
names from the talk descriptions. Our goal is
gies that allow a system to solicit novel informa-
to implicitly infer a list of interests for each re-
tion from users in a situated environment. To in-
searcher without soliciting the user for the inter-
vestigate the usability of these strategies, we con-
ests of every researcher exhaustively. To each re-
ductedauserstudyinthedomainofresearchtalks.
searcherinthelist,weattributelistofintereststhat
We analyzed a corpus of system-acquired knowl-
werementionedinthesamecontextasresearcher
edge and have made the material available5. Our
was mentioned. We tag list of names acquired
data show that users on average take less than a
from the FAMOUSPEOPLE strategy with list of key-
minute to provide new information using the pro-
words acquired from the BUZZWORDS strategy —
posed elicitation strategies. The reliability of ac-
bothlistsacquiredfromsameuser. Werepeatthis
quired knowledge in predicting relationships be-
process for each name mentioned in relation to a
tweenresearchersandinterestsisquitegood,with
talk in the SHOW&ASK strategy. We tag keywords
amean precisionof 90.5%. Wenote thatthe PER-
mentionedintheKEYWORDSstrategytoresearchers
strategy, which tries to tap personal knowl-
SONAL
mentionedinthePEOPLEstrategy.
edge, appears to be particularly effective. More
generally, automated elicitation appears to be a
4.1 Analysis
promising technique for continuous learning in
Weproduced200entriesforresearchersandtheir
spokendialogsystems.
set of interests. We then had two annotators (se-
nior graduate students) mark whether the system-
6 Appendix
predictedinterestswererelevant/accurate. Thean-
notatorswereallowedtouseinformationfoundon
researchers’ home pages and Google Scholar4 to SystemPredictedResearcher-Interests1
rich stern deep neural networks, speech recog-
evaluatethesystem-predictedinterests.
nition, signal processing, neural networks, machine
Thiscanbeseenasaninformationretrieval(IR) learning,speechsynthesis
problem,whereresearcheris“query”andinterests
are “documents”. So, we use Mean Precision, a
4scholar.google.com 5www.speech.cs.cmu.edu/apappu/pubdl/eventspeak corpus.zip
197
SystemPredictedResearcher-Interests2 GraceChung,StephanieSeneff,andChaoWang. Automatic
kishore prahallad dialogue systems, prosody, acquisitionofnamesusingspeakandspellmodeinspo-
speech synthesis, text to speech, pronunciation mod- kendialoguesystems.InProceedingsoftheNAACL-HLT,
eling,lowresourcelanguages pages32–39.ACL,2003.
EdwardFiliskoandStephanieSeneff. Developingcityname
acquisitionstrategiesinspokendialoguesystemsviauser
simulation. In 6th SIGdial Workshop on Discourse and
SystemPredictedResearcher-Interests3
Dialogue,2005.
carolyn rose crowdsourcing, meta discourse clas-
sification, statistical analysis, presentation skills in- HartwigHolzapfel,DanielNeubig,andAlexWaibel. Adia-
struction,manmadesystem,educationmodels,human logueapproachtolearningobjectdescriptionsandseman-
learning ticcategories.RoboticsandAutonomousSystems,56(11):
1004–1013,November2008.
WalterStephenLasecki,EceKamar,andDanBohus. Con-
versationsinthecrowd: Collectingdatafortask-oriented
SystemPredictedResearcher-Interests4
dialog learning. In First AAAI Conference on Human
florian metze dialogue systems, speech recogni-
ComputationandCrowdsourcing,2013.
tion, nlp, prosody, speech synthesis, text to speech,
pronunciationmodeling, lowresourcelanguages, au- Ingo Lu¨tkebohle, Julia Peltason, Lars Schillingmann,
tomaticaccentidentification ChristofElbrechter,BrittaWrede,SvenWachsmuth,and
Robert Haschke. The Curious Robot: Structuring Inter-
active Robot Learning. In ICRA’09, pages 4156–4162.
IEEE,2009.
SystemPredictedResearcher-Interests5
madhaviganapathirajuproteinstructure,contin- Aasish Pappu and Alexander Rudnicky. Predicting tasks
uous graphical models, generative models, structural in goal-oriented spoken dialog systems using semantic
biology, protein structure dynamics, molecular dy- knowledgebases. InProceedingsoftheSIGDIAL,pages
namics 242–250.ACL,2013.
Aasish Pappu, Teruhisa Misu, and Rakesh Gupta. Investi-
gating critical speech recognition errors in spoken short
SystemPredictedResearcher-Interests6 messages. InProceedingsofIWSDS,pages39–49,2014.
alexander hauptmann discriminatively trained
Rohit Prasad, Rohit Kumar, Sankaranarayanan Ananthakr-
models,deeplearning,computervision,bigdata
ishnan,WeiChen,SanjikaHewavitharana,MatthewRoy,
Frederick Choi, Aaron Challenner, Enoch Kan, Arvind
Neelakantan, et al. Active error detection and resolu-
tion for speech-to-speech translation. In Proceedings of
SystemPredictedResearcher-Interests7
IWSLT,2012.
jamie callan learning to rank, search, large scale
search, web search, click prediction, information re- AlexanderIRudnicky,AasishPappu,PengLi,andMatthew
trieval, web mining, user activity, recommendation, Marge. Instruction Taking in the TeamTalk System. In
relevance,machinelearning,webcrawling,distributed ProceedingsoftheAAAIFallSymposiumonDialogwith
systems,structuralsimilarity Robots,pages173–174,2010.
PushSingh,ThomasLin,ErikTMueller,GraceLim,Trav-
ell Perkins, and Wan Li Zhu. Open mind common
SystemPredictedResearcher-Interests8 sense:Knowledgeacquisitionfromthegeneralpublic. In
lori levin natural language understanding, knowl- CoopIS,DOA,andODBASE,pages1223–1237.Springer,
edge reasoning, construction grammar, knowledge 2002.
bases,naturallanguageprocessing Thorsten Spexard, Shuyin Li, Britta Wrede, Jannik Fritsch,
Gerhard Sagerer, Olaf Booij, Zoran Zivkovic, Bas Ter-
References wijn,andBenKrose. BIRON,whereareyou? Enabling
arobottolearnnewplacesinarealhomeenvironmentby
MasahiroAraki. Rapiddevelopmentprocessofspokendia- integratingspokendialogandvisuallocalization.Integra-
loguesystemsusingcollaborativelyconstructedsemantic tionTheVLSIJournal,(sectionII):934–940,2006.
resources. InProceedingsoftheSIGDIAL2012Confer-
Luis Von Ahn. Games with a purpose. Computer, 39(6):
ence,pages70–73.ACL,2012.
92–94,2006.
JeffreyPBigham, ChandrikaJayant, HanjieJi, GregLittle,
MichaelWitbrock,DavidBaxter,JonCurtis,DaveSchneider,
Andrew Miller, Robert C Miller, Robin Miller, Aubrey
Robert Kahlert, Pierluigi Miraglia, Peter Wagner, Kathy
Tatarowicz,BrandynWhite,SamualWhite,etal. Vizwiz:
Panton,GavinMatthews,andAmandaVizedom.Aninter-
nearlyreal-timeanswerstovisualquestions. InProceed-
activedialoguesystemforknowledgeacquisitionincyc.
ingsofthe23rdACMSymposiumonUserInterfacesoft-
InProceedingsofthe18thIJCAI,pages138–145,2003.
wareandtechnology,pages333–342.ACM,2010.
KurtBollacker, ColinEvans, PraveenParitosh, TimSturge,
and Jamie Taylor. Freebase: a collaboratively created
graph database for structuring human knowledge. Pro-
ceedingsoftheSIGMOD,pages1247–1249,2008.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Set-
tles, EstevamRHruschkaJr., andTomMMitchell. To-
wardanArchitectureforNever-EndingLanguageLearn-
ing. ArtificialIntelligence,2(4):1306–1313,2010.
198

November 1992 UILU-ENG-92-2244.
CRHC-92-26
Center for Reliable and High-Performance Computing
Report on Workshop on High Performance Computing
and Communications for Grand Challenge Applications:
Computer Vision* Speech and Natural Language Processing,
and Artificial Intelligence
Benjamin W. Wah University of Illinois, Urbana-Champaign
Thomas S. Huang University of Illinois, Urbana-Champaign
Aravind K. Joshi University of Pennsylvania
Dan Moldovan University of Southern California
John Aloimonos University of Maryland, College Park
Ruzena K. Bajcsy University of Pennsylvania
Dana Ballard University of Rochester
Doug DeGroot Texas Instruments
Kenneth DeJong George Mason University
Charles R„ Dyer University of Wisconsin, Madison
Scott E. Fahlman Camegie-Mellon University
Ralph Grishman New York University
Lynette Hirschman Massachusetts Institute of Technology
Richard E. Korf University of California, Los Angeles
Stephen E Levinson AT&T Bell Laboratories
Daniel P. Miranker University of Texas, Austin
Nelson H. Morgan * University of California, Berkeley
Sergei Nirenburg Camegie-Mellon University
Tomaso Poggio Massachusetts Institute of Technology
Edward M. Riseman University of Massachusetts, Amherst
Craig Stanfill Thinking Machines Corporation
Salvatore J. Stolfo Columbia University
Steven L. Tanimoto University of Washington
Charles Weems University of Massachusetts, Amherst
Coordinated Science Laboratory
College of Engineering
UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN
Approved for Public Release. Distribntion Unlimited.
UNCLASSIFIED
REPORT DOCUMENTATION PAGE
Unclassified
None
2a. SECURITY CLASSIFICATION AUTHORITY
Approved for public release;
2b. DECLASSIFICATION/DOWNGRADING SCHEDULE
distribution unlimited
S; sa g ys» ref?"1""
12. PERSONAL AUTHOR(S) --------
Wah, Benjamin W. , et all
13a. TYPE OF REPORT |13b. TIME COVERED
[14. DATE OP REPORT {Y*ar Month, D*y) ll 5. PAGE.COUNT
Technical^___ _ FROM TO 1992 November 24 | 4^
16. SUPPLEMENTARY NOTATION
This article reports the findings of the Workshop on High Performance Computing and
Communications (HPCQ for Grand Challenge Applications: Computer Vision, Speech and
Natural Language Processing (SNLP), and Artificial Intelligence (AI). The goals of the
workshop are to identify applications, research problems, and designs of HPCC systems for sup-
porting applications in these areas. F
In computer vision, we have identified the main scientific issues as machine learning sur­
face reconstruction inverse optics and integration, model acquisition, and perception and action
Since vision algorithms-operate in different levels of granularity, computers for supporting these
algorithms need to be heterogeneous and modular. Advances in technology, new architectural
concepts, and software design methods are essential for this area.
UNCLASSIFIED
UNCLASSIFIED
SCCUftITY CLASSIFICATION OF THIS FAOt
In SNLP, we have identified issues in statistical analysis in corpus-based speech and
language understanding, search strategies for language analysis, auditory and vocal-tract model­
ing, integration of multiple levels of speech and language analyses, and connectionist systems.
Similar to algorithms in computer vision, algorithms in SNLP require high computational power,
ranging from general purpose supercomputing to special purpose VLSI systems. As processing
has various requirements, a hybrid heterogeneous computer system is the most desirable.
In AI, important issues that need immediate attention include the development of efficient
machine learning and heuristic search methods that can adapt to different architectural
configurations, and the design and construction of scalable and verifiable knowledge bases,
active memories, and artificial neural networks. A computer system for supporting AI applica­
tions is heterogeneous, requiring research in high speed computer networks, mass storage and
efficient retrieval methods, computer languages, and hardware and software design tools.
Research in these areas is inherently multidisciplinary and will require active participation
of researchers in device and networking technologies, signal processing, computer architecture,
software engineering, and knowledge engineering. Besides extending current frontiers in
research, an important aspect to be emphasized is the integration of existing components and
results into working systems.
UNCLASS TirT ED
SECURITY r 'ION Or THIS PAGE
TABLE OF CONTENTS
WORKSHOP PARTICIPANTS.......................................................................................... i
PUBLICATION AND DISTRIBUTION OF THIS REPORT............................................ ii
1. EXECUTIVE SUMMARY ............................................................................................ 1
2. INTRODUCTION .......................................................................................................... 2
2.1. Origin of the Workshop......................................................................................... 2
2.2. The Workshop Charter .......................................................................................... 3
2.3. Organization of this Report ................................................................................... 3
3. COMPUTER VISION .................................................................................................... 5
3.1. Grand Challenge Applications .............................................................................. 5
3.2. Fundamental Science and Enabling Technologies................................................ 8
3.3. Implications for System Architectures .................................................................. 12
3.4. Infrastructure Support............................................................................................ 16
4. SPEECH AND NATURAL LANGUAGE PROCESSING ........................................... 17
4.1. Grand Challenge Applications .............................................................................. 18
4.2. Fundamental Science and Enabling Technologies................................................ 19
4.3. Implications for System Architectures .................................................................. 22
4.4. Infrastructure Support............................................................................................ 25
5. ARTIFICIAL INTELLIGENCE..................................................................................... 26
5.1. Grand Challenge Applications .............................................................................. 26
5.2. Fundamental Science and Enabling Technologies................................................ 30
5.3. Implications for System Architectures ............................ 33
5.4. Infrastructure Support....................................... 35
6. FINAL REMARKS ........................................................................................................ 36
ACKNOWLEDGEMENTS................................................................................................. 37
AUTHOR BIOGRAPHIES.................................................................................................. 38
AUTHOR ADDRESSES ..................................................................................................... 45
Report on Workshop on High Performance Computing
and Communications for Grand Challenge Applications:
Computer Vision, Speech and Natural Language Processing,
and Artificial Intelligence
Benjamin W. Wah University of Illinois, Urbana-Champaign
Thomas S. Huang University of Illinois, Urbana-Champaign
Aravind K. Joshi University of Pennsylvania
Dan Moldovan University of Southern California
John Aloimonos University of Maryland, College Park
Ruzena K. Bajcsy University of Pennsylvania
Dana Ballard University of Rochester
Doug DeGroot Texas Instruments
Kenneth DeJong George Mason University
Charles R. Dyer University of Wisconsin, Madison
Scott E. Fahlman Camegie-Mellon University
Ralph Grishman New York University
Lynette Hirschman Massachusetts Institute of Technology
Richard E. Korf University of California, Los Angeles
Stephen E. Levinson AT&T Bell Laboratories
Daniel P. Miranker University of Texas, Austin
Nelson H. Morgan *■ University of California, Berkeley
Sergei Nirenburg Camegie-Mellon University
Tomaso Poggio Massachusetts Institute of Technology
Edward M. Riseman University of Massachusetts, Amherst
Craig Stanfill Thinking Machines Corporation
Salvatore J. Stolfo Columbia University
Steven L. Tanimoto University of Washington
Charles Weems University of Massachusetts, Amherst
November 1992
This workshop was supported by National Science Foundation under grant IRI-9212592.
Any opinions, findings, conclusions, and recommendations expressed in this report are those of the authors and do
not necessarily reflect the views of the National Science Foundation.
November 23,1992 i
WORKSHOP PARTICIPANTS
Workshop Chair
Benjamin W. Wah University of Illinois, Urbana-Champaign
Computer Vision Area
Thomas Huang (Area Vice Chair) University of Illinois, Urbana-Champaign
John Aloimonos University of Maryland, College Park
Ruzena K. Bajcsy University of Pennsylvania
Dana Ballard University of Rochester
Charles R. Dyer University of Wisconsin, Madison
Tomaso Poggio Massachusetts Institute of Technology
Edward M. Riseman University of Massachusetts, Amherst
Steven L. Tanimoto University of Washington
Speech and Natural Language Processing Area
Aravind K. Joshi (Area Vice Chair) University of Pennsylvania
Ralph Grishman New York University
Lynette Hirschman Massachusetts Institute of Technology
Stephen E. Levinson AT&T Bell Laboratories
Nelson H. Morgan University of California, Berkeley
Sergei Nirenburg Camegie-Mellon University
Craig Stanfill Thinking Machines Corporation
Artificial Intelligence and Computer Architecture Area
Dan Moldovan (Area Vice Chair) University of Southern California
Doug DeGroot Texas Instruments
Kenneth DeJong George Mason University
Scott E. Fahlman Camegie-Mellon University
Richard E. Korf University of California, Los Angeles
Daniel P. Miranker University of Texas, Austin
Salvatore J. Stolfo Columbia University *
Benjamin W. Wah University of Illinois, Urbana-Champaign
National Science Foundation Observers
Syed Kamal Abdali Numeric, Symbolic, and Geometric Computations
Paul G. Chapin Linguistics
Su-Shing Chen Knowledge Models and Cognitive Systems
Bernard Chern Microelectronic Information Processing Systems
Y. T. Chien Information, Robotics, and Intelligent Systems
John H. Cozzens Circuits and Signal Processing
John D. Hestenes Interactive Systems
Richard Hirsch Supercomputer Center
Howard Moraff Robotics and Machine Intelligence
John Lehmann Microelectronic Information Processing Systems
Pen-Chung Yew Microelectronic Systems Architecture
Zeke Zalcstein Computer Systems Architecture
November 28,1992 ii
Publication and Distribution of this Report
1. Technical Report.
“Report of the Workshop on High Performance Computing and Communications for
Grand Challenge Applications: Computer Vision, Speech and Natural Language Pro­
cessing, and Artificial Intelligence,” Technical Report CRHC-92-26, Center for Reliable
and High Performance Computing, Coordinated Science Laboratory, University of Illi­
nois at Urbana-Champaign, Urbana, IL 61801, November 1992
To obtain a hard copy of this report, please send a letter or electronic mail to B. W. Wah
(journal@manip.crhc.uiuc.edu) requesting the above report. To obtain an electronic copy,
you can access the files by anonymous ftp from manip.crhc.uiuc.edu. The PostScript file
and the text file typed using ditroff and “me” macros are available in the directory
/pub/hpcc after you login as “anonymous.”
2. Journal Article.
“High Performance Computing and Communications for Grand Challenge Applications:
Computer Vision, Speech and Natural Language Processing, and Artificial Intelligence,”
IEEE Transactions on Knowledge and Data Engineering, Vol. 5, No. 1, Feb. 1993.
3. Electronic Bulletin Boards.
The executive summary will be distributed in the following electronic bulletin boards, with
instructions for accessing the entire article by anonymous ftp.
comp.ai.vision comp.ai comp.ai.nlang-know-rep comp.ai.neural-nets
comp.parallel comp, arch comp.sys.super comp.realtime
comp, robotics comp.graphics comp.graphics.research
3. Electronic Mailing Lists.
The executive summary will be distributed in the following electronic mailing lists, with
instructions for accessing the entire article by anonymous ftp.
ir-l@uccvma.bitnet supercomputer@nyu.edu
November 23,1992 1
1. EXECUTIVE SUMMARY
This article reports the findings of the Workshop on High Performance Computing and
Communications (HPCC) for Grand Challenge Applications: Computer Vision, Speech and
Natural Language Processing (SNLP), and Artificial Intelligence (AI). The goals of the
workshop are to identify applications, research problems, and designs of HPCC systems for sup­
porting applications in these areas.
In computer vision, we have identified the main scientific issues as machine learning, sur­
face reconstruction, inverse optics and integration, model acquisition, and perception and action.
Since vision algorithms operate in different levels of granularity, computers for supporting these
algorithms need to be heterogeneous and modular. Advances in technology, new architectural
concepts, and software design methods are essential for this area.
In SNLP, we have identified issues in statistical analysis in corpus-based speech and
language understanding, search strategies for language analysis, auditory and vocal-tract model­
ing, integration of multiple levels of speech and language analyses, and connectionist systems.
Similar to algorithms in computer vision, algorithms in SNLP require high computational power,
ranging from general purpose supercomputing to special purpose VLSI systems. As processing
has various requirements, a hybrid heterogeneous computer system is the most desirable.
In AI, important issues that need immediate attention include the development of efficient
machine learning and heuristic search methods that can adapt to different architectural
configurations, and the design and construction of scalable and verifiable knowledge bases,
active memories, and artificial neural networks. A computer system for supporting AI applica­
tions is heterogeneous, requiring research in high speed computer networks, mass storage and
efficient retrieval methods, computer languages, and hardware and software design tools.
Research in these areas is inherently multidisciplinary and will require active participation
of researchers in device and networking technologies, signal processing, computer architecture,
software engineering, and knowledge engineering. Besides extending current frontiers in
research, an important aspect to be emphasized is the integration of existing components and
results into working systems.
November 23,1992 2
2. INTRODUCTION
2.1. Origin of the Workshop
The idea of this workshop stemmed from the “Blue Book” 1 which identifies computing
and communication technologies as essentials “to satisfy national needs from a variety of per­
spectives including: technology, science applications, human resources, and technology transi­
tion.” Computer vision and SNLP are two of these grand challenge applications which “often
cut across various agencies and missions” and “are related to solving very intensive large scale
computing problems.”
The workshop was held on February 21 and 22, 1992 in Arlington, Virginia, with 23
experts from academia and industry attending and 12 program directors from NSF serving as
observers. Participants in the workshop were divided into three areas, with a vice-chair
identified for each. Before the workshop, each vice-chair had solicited position statements from
members of his area and coordinated discussions of issues through electronic mail. Based on
comments received during the workshop, the vice-chair, in consultation with members of the
area, prepared a summary report. After the workshop, the summary report was refined and was
posted in early March on many electronic bulletin boards with areas related to the focus of this
workshop.
This article was prepared on the basis of the preliminary report distributed in March and
further discussions among the participants through electronic mail. In preparing this report, each
vice-chair first assembled ideas and material from the workshop participants in his area and
prepared an initial draft of a section. Based on the three sections supplied by the vice-chairs, the
workshop chair integrated the material, refining the discussion so there is coherent flow and bal­
ance between the sections.
This report is a collection of ideas expressed by the participants. It does not necessarily
represent a consensus among all the participants. Further, ideas expressed in this report do not
reflect the official position of the sponsoring agency.
1. “Grand Challenges: High Performance Computing and Communications,” The FY 1992 U.S. Research and Development Program,
Committee on Physical, Mathematical, and Engineering Sciences, Federal Coordinating Council for Science, Engineering, and Technology,
Office of Science and Technology Policy, Washington, DC, 1992.
November 23,1992 3
2.2. The Workshop Charter
The goal of the workshop was to identify near-term (within five years) and long-term
(beyond five years) problems and potential approaches/research directions in grand challenge
applications in computer vision, SNLP, and artificial intelligence (AI). Attendees focused on
answering the following questions:
1) What grand challenge applications in computer vision, SNLP, and AI can benefit by the
availability of HPCC systems?
2) What research problems need to be solved in these grand challenge application areas?
3) How should HPCC systems be designed so that they can better support solutions in these
areas?
We have chosen to cover the three areas in this workshop in the order of computer vision,
SNLP, and AI. All three areas are important grand challenge application areas. Moreover, they
are closely related, since vision, speech, and natural language are three primary modes of per­
ception and communication in humans, and knowledge acquisition and intelligent reasoning is
needed for augmenting deficiencies and missing information. We do not attempt to cover all the
aspects in these three areas because they are too broad to be discussed in a single workshop, and
we will not do justice even if we try to focus on a small portion. Rather, we concentrate on
issues related to how high performance computing can help provide new solutions and insights
into solving problems in these areas. We also examine new applications where AI, vision,
speech, and natural language can be integrated.
2.3. Organization of this Report
This article is divided into three major sections, covering the areas of computer vision,
SNLP, and AI. Each section is further subdivided into four subsections: grand challenge appli­
cations, fundamental science and enabling technologies, implications for system architectures,
and infrastructure support. The subsection on grand challenge applications illustrates applica­
tions in the area and relates them to applications in the other areas covered in this article. The
subsection on fundamental science and enabling technologies discusses fundamental research
problems that need to be solved. We discuss issues that are application specific, as well as com­
mon issues; examples of the latter are machine learning and heuristic search. The subsection on
November 23,1992 4
implications for system architectures highlights requirements of each application area and
discusses how computers under the HPCC Initiative should be designed to better serve the appli­
cation. The last subsection on infrastructure support presents what advanced infrastructure tools
(hardware and software) are needed to support research in each area.
November 23,1992 5
3. COMPUTER VISION
Computer vision has two goals. From the engineering viewpoint, the goal is to build auto­
nomous systems which can perform some tasks that the human visual system can, and even go
beyond the capabilities of the human visual system in multimodality, speed, and reliability.
From the scientific viewpoint, the goal is to develop computational theories of vision, and by so
doing, gain insights into human visual perception.
Computer vision is related to other grand challenge areas because a) many applications,
such as video compression and human-machine interface, involve both vision and speech; and b)
AI techniques, such as knowledge-based reasoning, are needed in vision systems which must
operate in real-world domains.
3.1. Grand Challenge Applications
Grand challenge applications in computer vision fall in two classes. First, there are many
important applications in autonomous vision systems, most of which involve the interaction of
the vision system with the environment and humans. Examples of these applications include
flexible manufacturing, intelligent vehicle highway system, environmental monitoring, visual
prosthetics and rehabilitation robotics, multimedia and model-based compression, and education.
Second, there are many basic scientific application problems that can be studied using computer
vision techniques as invaluable tools. A prominent example is the visual understanding of tur­
bulence in fluid flow. In the following, we describe briefly some of these applications.
1) Flexible manufacturing. The manufacturing base is an important element to keep the
United States competitive at the turn of the 21st century. The next 20 years of manufacturing
will be characterized by ever faster changing products, with success determined by variability,
customized usage and low cost. The computer technologies that are now available indicate the
feasibility of achieving integrated and yet flexible manufacturing.
Computer vision is a critical enabling technology that allows cooperating robots to be
mobile and dexterous. This is important for effective flexible manufacturing of small batches of
customized parts, which may require functionalities and capabilities that are distributed across
multiple robot agents. This poses challenging issues in multi-agent cooperation, many of which
will require real-time perception.
November 23,1992 6
Distributed active vision for specialized manufacturing tasks is another important element
in flexible manufacturing. This allows software-configured logical connections between
factory-based sensors and a diverse collection of (remote) supercomputers that perform special­
ized vision tasks as required by the particular manufacturing process. This would certainly take
advantage of high-bandwidth networks as well as computer-vision technologies.
2) Intelligent Vehicle Highway System (IVHS). Computer vision is a key technology for
developing better highways to improve mobility, safety, capacity, and efficiency of our surface
transportation system. The strategic plan developed by the Intelligent Vehicle Highway Society
of America calls for advanced technologies that provide traffic management as well as driver-
assist and autonomous capabilities on vehicles. Autonomous vision-based systems for high­
speed vehicle control have already been tested in laboratories. These systems can provide driver
warning to help detect and avoid collisions for unanticipated emergencies in poor driving condi­
tions. An advanced concept considered is the design of high-speed convoys in special
automated lanes that function safely and efficiently in various weather conditions.
3) Environmental monitoring. An important part of the grand challenge problem in com­
puter vision is the monitoring of earth resources and points of interest via satellite and aerial
reconnaissance. Changing resources and their effects on earth, such as the loss of rain forests
leading to global warming effects, need to be monitored closely. Others, such as the monitoring
of points of military interest, are important for our national defense. Integrated space-based
sensing systems would perform complex resource allocation, balancing the capabilities of distri­
buted monitors such as satellites with the needs of individual problems. The advent of comput­
ing resources, with computing power of at least 10,000 times better than existing systems, prom­
ises to make important parts of this essential task feasible.
4) Visual prosthetics and rehabilitation robotics. Intelligent machines and “assistive
robots” capable of perception can benefit tremendously the blind and severely disabled individu­
als. Although the need remains clear, their widespread use has been overshadowed by cost and
complexity. Ideas considered include: a) a vision system that can monitor human whereabouts;
b) a vision system that can keep track of the users’ environment (objects, freeways, and tasks); c)
a computer-vision-initiated assistant that can be invoked “intelligently” without user interven­
tion, and d) a vision system that can guide a blind user to objects and landmarks of interest in the
environment.
November 23,1992 7
An “assistive workstation” capable of accepting requests to interpret a scene will be
invaluable for patients beyond rehabilitation. Such a system can be designed around a
computer-vision subsystem (with an intelligent user interface to the vision system) and a high­
speed network connecting computational resources across the system. Effective load balancing
would allow this system to be economically viable.
5) Multimedia and model-based compression. Computer vision can have a profound
impact in developing new image compression methods, which are critical for developing mul­
timedia and image dissemination techniques. The multimedia industry within 10 years is likely
to integrate two worlds that today are quite distinct: the world of communication and the world
of computers. It is expected to have significant impact in many areas, including education, visu­
alization for engineering, and entertainment.
The key technology in multimedia systems is signal compression and especially image
compression. From the technical point of view, the main challenge is to exploit computer vision
technologies of model-based recognition to achieve hypercompression, which is compression at
very high rates obtained by encoding only the parameters of models of objects identified in the
image (such as cultural objects). Well developed, “classical” image compression technologies,
especially in the area of model-based vision, can be significantly enhanced by the focused use of
computer vision algorithms.
6) Education. Computer vision technology will have an important role in education.
Image analysis techniques are part of several key tools to improve the widespread use of infor­
mation technology in the delivery of education. They have great potential to help author-
animators of visual materials integrate live action and animation. However, the full potential of
vision technologies in education will only be realized if major efforts are made to transfer the
technology of computer vision to the domain of digital-movie authoring tools. This requires the
development of many advanced technologies employing high-performance computing that are
beyond existing image-oriented packages for desk-top publishing.
A more ambitious direction is the development of intelligent tutoring systems that can pro­
cess speech and image data from a student, reason about the student’s understanding of material
in the subject domain, use stimulating and sensitive pedagogic strategies in the manner of a mas­
ter teacher, and utilize computer vision so that the system could visually perceive the human
with whom the system is interacting.
November 23,1992 8
Initially, one or more pilot studios should be assembled and connected to the National
Research and Educational Network (NREN). Eventually, an array of tools should be made
available so that anyone on NREN can use digital-television authoring tools and large well-
organized distributed libraries of source material. Very large amount of funds have already gone
into computer vision technology; transferring some of this technology to the educational arena is
an important way to capitalize on this existing investment.
7) Turbulent flow analysis. Understanding turbulence is important in applications such as
weather forecasting, designing aircrafts and ships, and noise control. Turbulence research
creates a number of challenging problems for computer vision and image processing. Research­
ers in this area, however, are overwhelmed by the huge amount of three-dimensional (3-D) time
varying vectors of data for modeling turbulent flow. Techniques in computer vision and AI can
be of tremendous help in understanding and in reducing the complexity of algorithms in fluid
flow. For instance, computer vision can aid in tracking coherent patterns in fluid flow, and unsu­
pervised clustering and learning methods can be applied to classify coherent patterns in turbulent
flows. In order for these to be possible, extensions must be made in existing image and signal
processing techniques that were designed for scalar data, and in conventional computer vision
methods that work mainly on surface data.
3.2. Fundamental Science and Enabling Technologies
Some of the main scientific issues underlying the applications are: i) machine learning, ii)
surface reconstruction, inverse optics, and integration, iii) model acquisition, and iv) perception
and action.
3.2.1. Learning
Learning has long been a central problem in understanding intelligence and developing
intelligent machines. For many years the “Turing test” has represented an operational
definition of intelligence against which most workers in AI have implicitly measured their own
goals and achievements: if a computer behaves in a way indistinguishable from a human person
then it can be called intelligent, even if its “intelligence” has been painstakingly programmed
by a very skilled human being.
November 23,1992 9
Consistent with Turing’s implicit definition, intelligence was perceived thirty years ago as
mainly reasoning, problem solving, proving theorems, or playing chess. In contrast, we realize
today how “intelligent” lower animals are and how complex are the problems that our senses
routinely solve. We also realize how intractable the problem of developing hardware and
software is and how much of it would be needed in order to replicate even a small part of the
simplest aspects of intelligence. From this perspective, a somewhat different definition of intel­
ligence seems needed to better capture today’s view of the underlying problems. This new
“Turing test” should emphasize the development of perceptual, motor, and language com­
petence. In particular, many would agree that a system that can acquire visual, motor, and
language skills should be called intelligent. This revised test introduces in an explicit way the
problem of learning as the core of any attempt at understanding intelligence.
Within computer vision itself, the next frontier is represented by the problem of learning.
Bringing together the two separate disciplines of machine perception and machine learning is an
exciting and promising goal. It allows more robust practical systems and flexible devices for
visual inspection, and may also have deep implications for understanding biological vision.
There are different levels of learning in vision: from the problem of adapting parameters in
preprogrammed vision algorithms, to the more challenging problem of learning to synthesize
specific vision algorithms (such as stereo) from examples of desired performance (depth maps).
The ultimate challenge is to develop a machine that learns to see, somewhat like a child comes
to acquire visual skills.
Learning in vision is especially challenging from the computational point of view. Algo­
rithms for learning can be computationally expensive, since the “learning” or estimation stage
is usually a non-convex optimization problem over many hundreds or thousands of parameters.
These problems are grand challenge problems themselves: they may require computational
power that are 100 times faster than real time.
3.2.2. Integration of vision modules
A vision system should have the ability to create representations of its visible environment
if it is designed to autonomously interact with its environment and perform various tasks related
to navigation, manipulation and recognition. Such representations are needed for several proper­
ties of the environment like shape and motion, as well as color and other material properties.
November 23,1992 10
Extraction of these geometric and physical properties of the environment on the basis of image
information is an inverse problem that requires modeling of the image-formation process as well
as the development of geometric and physical constraints relating 3-D information to image pro­
perties or cues.
Research during the decade of the 1980’s demonstrated that these inverse problems are
undetermined or ill-posed, i.e., their solution does not exist, or is not unique and does not depend
continuously on the data. For example, recovering 3-D shape using cues, such as shading, tex­
ture or contour, are ill-posed problems. The same holds true for recovering properties such as
image motion and discontinuities.
It is becoming clear from work of the past few years that if a vision system integrates infor­
mation from several cues, such as motion, stereo, texture, and contour, then several inverse prob­
lems become well posed and stable, simply because more information is taken into account. A
grand challenge for computer vision research in the 1990’s is, therefore, the development of a
sound framework for integrating vision modules, with the goal of creating robust environmental
descriptions. Although several alternatives could be outlined, it is not yet obvious how one
should proceed towards the solution of this problem. However, it is known that a solution
requires a vast amount of computational power so that multiple processes can cooperatively util­
ize various image cues and exchange information in high-speed links.
3.2.3. Model acquisition
In each of the applications described above, an important component of a complete vision
system is the use of models describing surfaces and objects of interest for model-based scene
analysis and for robust and efficient scene interpretation. Examples include models of manufac­
turing parts, roads, and vehicles in the IVHS domain, and buildings, rivers, and clouds in
environmental monitoring. Consequently, an important open research problem is the automation
of the model-acquisition process. Successful solutions to this problem will foster rapid prototyp­
ing of vision systems and benefit other areas such as computer graphics, robotics, and
geometric-model based problems.
Constructing appropriate object models is, at best, a very time-consuming task. Currently,
programmers handcraft 3-D models of objects, surfaces and features that can subsequently be
used in tasks such as object recognition, navigation, and surveillance. To successfully scale up
November 23,1992 11
prototype vision systems so that they can handle applications with a large number of complex
objects, better methods for automatically acquiring these models are essential. This encom­
passes several critical problems:
1) How can more general classes of real objects be modeled?
2) How can procedures be defined for automatically acquiring shape models?
3) How can robust object- and scene-recognition strategies be automatically acquired for
utilizing context and the many possible feature cues that may be available in a scene?
The first problem focuses on the generality of geometric modeling. We must extend
current static shape modeling techniques to more explicitly describe properties that are detect­
able by a given sensor. This modeling-for-detectability means taking into account the imaging
process in order to identify physically meaningful and measurable features. It also means that
dynamic modeling of non-rigid surfaces and objects must be based on physically accurate con­
straints (such as mass, friction, forces and torques). Moreover, it needs models based on seman­
tic or functional descriptions of the use of objects.
The second problem posed above is related to the automation of the model-acquisition pro­
cess. Current model-construction techniques in computer vision (and computer graphics) require
painstaking manual effort to add new models. New algorithms are needed for supporting several
issues related to this problem, which will necessarily overlap with other basic research problems
in learning and in combining perception and action. Examples include a) incremental shape
learning from multiple views, multiple resolutions, and multiple focuses of attention; b) shape
learning by active surface exploration; c) compiling characteristic views based on physical con­
straints on geometric and kinematic shape, lighting and imaging processes; and d) constructing
large-model databases using inter-model and intra-model characteristics to produce efficient
storage and access methods.
The third problem posed above involves the automatic construction of robust recognition
strategies. This is necessary to acquire and optimize a range of information that is difficult for a
human to catalogue and integrate. Although the number and types of cues available for object
and scene recognition is large, only a small amount is present in a particular image. Thus there
are many paths for semantic inference, including data-directed construction of 3-D information
(such as points, lines and surfaces), and model-based recognition using distinctive features (such
November 23,1992 12
as color, texture and shape) and scene context (such as spatial and functional relationships). The
strategies developed should be capable of using only the available information.
In short, model acquisition in its most general form is a computationally demanding prob­
lem that is at the heart of learning about the physical environment in which we live. Sensors
provide measurements about that environment, and the challenge is to incrementally integrate a
vast amount of information over time to compactly and explicitly represent what we know, how
it can appear, and how it can be applied for object and scene interpretation.
3.2.4. Perception and action
Perception is more than just vision and is highly enhanced by directed “actions.” It should
be viewed as a composition of processes that use sensory information from several different
modalities and/or different cues. This view implies the following research agenda: a) modeling
of individual sensory capabilities and limitations; b) modeling of individual cue capabilities and
limitations; c) determining a common representation space for integrating the information
obtained from different modalities; d) designing integration rules and methods for combining
information; e) developing control strategies for data acquisition; and f) studying the amount
perceptual information needed for a given task.
The last question points us to the issue of “task” modeling. We believe that there is no
such thing as perception without purpose. Hence, the task drives what sensory information and
how much of it will be processed for a given task or subtask.
The “activity” is apparent in this agenda described above. If the task requires accuracy or
view that the first measurement did not provide then one must “act,” i.e., take more and dif­
ferent measurements. Many problems such as ambiguities that arise from spatial alignments can
be disambiguated by simple actions of moving, grasping, or pushing. The performance of the
task can then be used to evaluate the performance of the perceptual system.
3.3. Implications for System Architectures
The computational requirement of vision algorithms is high and is predicted to be at least
tens of trillions of operations per second. Consider an autonomous vehicle moving through an
unstructured environment (e.g., a mobile robot operating as a hazardous-waste cleanup
machine). Assuming color stereo sensors operating at 30 frames per second, the input data rate
November 23,1992 13
will be roughly 50 megabytes per second using standard resolution images (512-by-512 pixels),
or 200 megabytes per second at higher resolution (1024-by-1024 pixels). From this data it is
necessary to extract multiple features (such as two-dimensional lines, regions and texture
patches, and three-dimensional lines, surfaces and volumes). Each feature-extraction process
can take from tens of operations (simple edge detection) to many thousands of operations (tex­
ture discrimination). Thus low level processing alone could easily consume anywhere from 5
GOPS2 (one simple process for a medium-resolution stereo-image stream) to 10 TOPS2 (ten
processes with an average complexity of 5000 operations on a high-resolution stereo-image
stream).
The above example illustrates that there are different levels of processing in a vision task.
In general, tasks in a vision system fall into roughly three categories: low-level, intermediate-
level, and high-level; each of which has different processing requirements and needs different
architectural supports.
In low-level processing, only a small percentage of the operations involve floating point
calculations (generally 32-bit); most are 1-bit, 8-bit, and 16-bit integer arithmetic and require
communications that exploit locality. The development of computers that support these features
is not the primary focus of many current architecture efforts, which instead target machines
mainly used by the scientific and engineering communities.
Beyond low-level processing, there usually are one or more intermediate levels that
emphasize symbolic processing of the feature information extracted from sensory data. This
hierarchy allows the information to be organized so that matches with high-level structures in
the knowledge base can occur efficiently. Intermediate-level processing may require many
diverse operations for organizing many local features into structures and for matching them to
projected model data. These diverse operations involve computations such as spatial geometric
relations, linking symbolic representations of features into graph structures, transforming graph
representations of models (via shifting, rotating, scaling, and warping operations), and determin­
ing subgraph isomorphisms and confidence measures of matches.
Most intermediate-level processing involves 16- and 32-bit integer arithmetic, some 32-bit
floating point arithmetic, and lots of graph-traversal and database-search operations. Again,
these are not typical of general-purpose computers. More importantly, the computation,
November 24,1992 14
communication and control differ significantly from that of low-level processing. In low-level
vision, the control mechanisms involve one or a small number of threads (in SIMD3 mode),
while intermediate-level processing is better characterized by a large number of highly orches­
trated threads (in SPMD3 mode). Communication in low-level processing can also take advan­
tage of a great deal of locality inherent in image structures, while at the intermediate level the
communication involves parallel global searches and reduction operations.
At the high level, there are further differences: computations are coarse-grained, have
independent control threads and communication, and may need spatially disparate processing.
At this level, computation may have to be carried out at several different locations in a distri­
buted fashion. One aspect of using distributed computing in this level is the transmission and
management of huge amounts of image data.
It is, therefore, clear that vision cannot be mapped efficiently onto a homogeneous parallel
processor. Yet efficiency is a key concern in almost all vision applications, most of which
require small, lightweight, low-power, low-cost, real-time embedded processors that serve as
powerful research tools as well as engines driving vision applications in the commercial and the
military markets. These issues, and especially that of real-time processing, are generally
neglected by the traditional architecture community, which focuses on large homogeneous paral­
lel systems with a strong emphasis on double precision floating-point and vector performance.
High performance systems for computer vision lie at two ends of a wide spectrum of paral­
lel architectures where research needs to be focused. The first one is the class of parallel super­
computers with distributed memory and a large number of coarse-grain processing elements.
Examples include Thinking Machines Corporation’s CM5 and Intel’s Paragon. These systems
are useful tools in research as well as in production applications that are extremely data intensive
but do not require real-time responses (such as processing satellite and aerial imagery). The
second type consists of special-purpose (digital and possibly analog) VLSI chips, such as focal-
plane processors, for very specific, real-time vision tasks. This would allow computer vision to
enter the commercial market in areas such as consumer electronics, multimedia systems, optical
character recognition, surveillance, and driver assistant. It will also have military applications
for smart weapons, ATR (automatic target recognition), and autonomous surveillance vehicles.
2. GOPS stands for giga-operations per second. TOPS stands for tera-operations per second.
3. SIMD stands for single-instruction-multiple-data-stream. SPMD stands for single-program-multiple-data-stream.
November 24,1992 15
Between the two technologies described above, there are a variety of intermediate general-
purpose and special-purpose computers, such as less expensive fine-grain image processing
boards and processors, that are suitable as experimental tools in research and in less time-critical
applications.
A research program on architectures for computer vision will involve three components:
basic technology, computer architecture, and software support.
At the basic technology level, research needs to be carried out on integrating existing as
well as emerging technologies in the design of real-time, small, inexpensive, and general-
purpose vision systems. Examples of these technologies include 3-D VLSI and/or photonics,
digital (and possibly analog) hybrid CMOS-CCD VLSI, application-specific DRAM (dynamic
random access memory) based processors, large hybrid substrate technology, and high-density
interconnect packages. Although some of these technologies may not be fully available today, it
is essential that design tools be developed to incorporate them in vision applications when they
become viable.
At the architecture level, there remain many issues to be resolved in order to integrate
heterogeneous peripheral preprocessing modules (such as optical processors, analog neural nets,
sensors with integrated analog preprocessing, and focal plane VLSI chips) with powerful parallel
processing systems. Immediate issues that need to be addressed include: a) development of a
consistent set of interfaces for connecting parallel processors into a heterogeneous ensemble; b)
development of a standard set of abstract machine models that can be mapped onto different
parallel hardware platforms; and c) research on heterogeneous parallel and distributed architec­
tures that permit different parallel systems to be tightly or loosely coupled in a flexible range of
configurations, where the processing element at each level can be matched to the varying com­
putational characteristics and data structures of vision algorithms. Tight coupling (i.e., high
bandwidth, low-latency communication, and coordinated control) is essential for parallel sys­
tems to work together efficiently; flexibility in configuring the system, on the other hand, is
necessary to address the requirements of different vision applications. Of particular interest is
the modularity and scalability of such architectures, especially the question of how different
components can be easily “glued” together, and the communication and control pathways
between different homogeneous parallel processors.
November 23,1992 16
Last, but far from least, are the software issues. Software must be as easy to port from one
heterogeneous configuration to another as it is to move from one workstation to another. It
ought to be easy to program, taking advantage of the algorithmic simplifications that result from
a more direct mapping of the vision problem onto a suitably structured hardware platform. This
research will involve studies on languages and compilers, reusable and extensible libraries for
supporting multiple abstract machine models, real-time operating systems, and tools for support­
ing the use of heterogeneous parallel machines. It will also involve the development and tests of
vision software and algorithms on teraflop and scalable parallel supercomputers now becoming
available and on other HPCC systems to be developed. One important challenge is to develop
easy-to-use software that can adapt to different architectures and application requirements.
3.4. Infrastructure Support
The infrastructure needs of computer vision and its related applications are several: a)
abundant computational cycles, tools, and displays for the design and development of
application-specific vision systems; b) a variety of building blocks that can be configured for the
various applications requirements; and c) software infrastructure, image databases, and standards
of image communication to enable the community to move forward coherently.
To design a high-performance computer-vision system, it is necessary to narrow the gap
between what traditional supercomputers can offer and what is needed in computer vision appli­
cations to process pixel-level, intermediate-level, and symbolic-level tasks. It is important that
there be more joint efforts among members of the computer vision community and computer
architects. It is also critical that the cost of visual processing systems be reduced to the range of
#1K-#10K rather than 5100K-510 million. Such an engineering challenge may require a high-
technology production infrastructure quite different from that used to produce current-day super­
computers. It is also essential that tools be available for configuring substrates and facilities
(such as MOSIS) for fabricating them in sample lots at low cost. Finally, computer vision, as
part of AI, is a field where experimental algorithm development tends to be much more the criti­
cal bottleneck than computational power, in spite of the tremendous need for cycles. Therefore,
support for developing new vision-oriented software tools, as well as support for generic
software infrastructure, is extremely important. Part of such software development efforts need
to be directed at achieving at least a modicum of community consensus (a la DARPA Image
Understanding benchmarks) in order to have their desired impact across the community.
November 23, 1992 17
4. SPEECH AND NATURAL LANGUAGE PROCESSING
The goal of SNLP is to process speech and natural language in real time in order to achieve
natural, fluent human/machine communication. HPCC related research in this area will lead
directly to developments that will revolutionize the modes of our communication with each other
as well as with machines, and our ways of using and sharing vast amounts of information either
in a monolingual or in a multi-lingual environment.
SNLP is strongly related to computer vision. Vision, proprioception, and controlled motion
help understand the model of the world and extract the meaning from spoken messages. For
instance, computer vision can be considered as a front-end for identifying characters in text, if
presented in written form, before they are processed. This in itself is a computationally demand­
ing task similar to the acoustic/phonetic-recognition stage in speech recognition, especially when
the text is handwritten in cursive script. At present, the technology for this task is no more
advanced than that of recognizing colloquial discourse. Computer vision is also a crucial front-
end for scanning and for storing bit maps when information is presented in printed form. In this
case, semantic analysis is needed to analyze the images in order to extract the underlying struc­
ture and to connect the scattered columns of text that make up the printed articles (such as news­
paper and magazine).
Spoken language (speech) also shares many common aspects with written language in
terms of the lexicon, syntax, and semantics. Since all these features are necessary to accomplish
automatic recognition of handwritten or printed language, studies of speech processing are appli­
cable to those of the written language. Moreover, the computational complexities of the
linguistic-analysis algorithms for both areas are similar.
Speech is an important component when integrated with other communication media —
sound, still images, moving images, and text — in multimedia systems. It is expected that infor­
mation in such systems will be stored predominantly in digital form in electronic libraries by the
end of the decade. Such databases can quickly grow to very large sizes, presenting challenges
for designers of information storage, transmission, and presentation systems.
In the rest of this section, we describe a few applications to illustrate the importance of this
area, discuss the fundamental science and enabling technologies, and highlight various implica­
tions for computer architects in designing new computers for this area.
November 23,1992 18
4.1. Grand Challenge Applications
1) Electronic library and librarian. An electronic library represents one of the most excit­
ing and potentially important grand challenges of the decade. We can foresee, by the year 2000,
developing the technology needed to electronically store and distribute quantities of information
large enough to rival a major national library such as the Library of Congress. This capability
can fundamentally change the way scientists, engineers, scholars, teachers, and students work by
making a wealth of information available from every desk-top computer in the country. Full
scale projects, such as putting the Library of Congress on line, await the appropriation of large
amounts of money as well as further reduction in the cost of mass storage. However, over the
next three years, it is possible to engage in scale-up experiments in which specialized libraries of
intermediate size can be built and made available to the public.
To manage information in such an electronic library, an electronic librarian is needed to
organize and catalog text as well as to assist users in locating information. Such systems will go
beyond current automatic classification and full-text retrieval systems, not only providing access
to individual files but also to entire collections of full text files, catalogs, reference material,
encyclopedias, indexes, abstract files, and collections of images and sounds. One of the critical
issues is the user interface of which speech and machine translation are important components.
The issue related to information retrieval is discussed in Section 5.1.
a) Spoken language interfaces. A spoken language interface is a natural way to access
information. A real-time spoken-language interface can carry on a conversation with a range of
users: from the casual or first-time user who may need guidance on how to use the system, to the
expert who may choose to talk in a focused, concise manner. This will require a system that
supports speaker-independent real-time spontaneous speech recognition and understanding (on a
large vocabulary, and possibly even for multiple languages, if the library is multi-lingual). The
system must also support dialogue modeling, language generation and speech synthesis (all in
real time) in order to interact with the user.
b) Machine translation (MT) capabilities will be crucial for using documents in different
languages as well for translating documents into other languages for their wider distribution.
The major challenge for MT of texts is to bring the quality of text-oriented systems up to a level
where everyday use is economically viable. The complexity and productiveness of human
language, together with the difficulty of developing language-understanding and generation
November 23,1992 19
capabilities, constitute the major obstacles for designing MT systems. It is widely perceived that
in practice these obstacles translate into difficulties of accumulating and manipulating very large
quantities of knowledge about the language and about the world. In particular, techniques for
automatic acquisition of lexical, grammatical and discourse knowledge are essential for
significant progress in this area.
2) Spoken language translation. This capability will revolutionize human-human commun­
ication, involving applications in telephone-based as well as face-to-face modes of communica­
tion. It will improve international trade, communications among government and other national
organizations, as well as activities in these organizations. Together with MT of written
language, it will allow translation using facsimile machines, and will resolve the worldwide
bottleneck in the quantity of documents that cannot be translated at the present speed and cost.
A central issue in the development of spoken language systems is speed of processing. In
this environment, the complexity of speech recognition is exacerbated by the complexity of
natural language processing, since the amount of knowledge to be manipulated by the systems is
significantly larger than in text translation. Moreover, there is a need to improve computer sys­
tems for speech-to-speech translation, with the aim of increasing the integration of the various
component modules — speech recognition, language analysis (including morphology, syntax,
semantics, pragmatics and discourse), message-content manipulation, message planning,
language generation, and speech synthesis.
4.2. Fundamental Science and Enabling Technologies
4.2.1. Statistical analysis in corpus-based speech and language processing
The availability of large corpora, including some corpora annotated in various ways, has
already led to the development of techniques that allow automatic acquisition of linguistic struc­
ture. We illustrate in the following examples the importance of statistical techniques for work
with large corpora.
1) Semantic constraints — determining what words can meaningfully appear as the subject
or object of a particular verb, for example — are critical for analyzing natural language, be it in
text or spoken form. Building up such constraints by hand, however, is a very time-consuming
and error-prone task. An alternative to the manual approach involves the automatic analysis of
November 23,1992 20
co-occurrence patterns in a text corpus. In this approach, a text is analyzed syntactically and
various patterns (such as subject-verb pairs) are gathered. Word similarities are computed based
on the frequency with which words occur in the same context (such as the subject of the same
verb). Based on these similarities, words are gathered into classes, and the word-level co­
occurrence patterns are generalized into patterns stated in terms of word classes.
This approach has been tried on small corpora with some limited success. However, since
we are looking for co-occurrence patterns involving very large numbers of word pairs, it is clear
that proper evaluations will require large corpora, and accordingly expanded computing
resources to handle these corpora.
2) Syntactic constraints. In developing robust SNLP systems, hand-crafted grammars are
inadequate because they fail to provide a wide coverage and are very brittle, rendering them not
useful for processing speech and free texts. Techniques have been designed for automatic
acquisition of linguistic structure from large corpora, some of which are annotated in various
ways. These techniques have to be lexically sensitive; hence, they have to be integrated with
techniques for discovering semantic constraints. To support these acquisition techniques, high
performance computing and storage resources are needed.
Text collections of several gigabytes are available; however, with current computers, these
would require months or even years to analyze. Computing power at the levels projected for
HPCC would allow such analysisrto be done in days, permitting experimentation with different
analysis methods. Moreover, with iterative procedures, partial syntactic and semantic informa­
tion from an initial text analysis can be used to produce improved parses, thereby improving syn­
tactic as well as semantic information.
4.2.2. Search strategies for language analysis
Language analysis is typically viewed as a large search problem, with various constraints
and heuristics introduced to make the search tractable. Often we are obliged to introduce res­
tricted constraints to limit the search, although more flexible or complex constraints would be
preferred. For instance, syntactic and semantic constraints need to be relaxed in order to handle
a number of types of ill-formedness. It is desirable to replace simple semantic constraints in
metonymic and some other non-literal usages by more complex reasoning procedures. Such
approaches will be feasible only when computing systems developed under HPCC are available.
November 23,1992 21
4.2.3. Auditory and vocal-tract modeling
The mathematical models of the speech signal in use today were proposed over fifty years
ago and have remained essentially unaltered since that time. This model, called the source-filter
model, assumes that there are sound sources as well as an independent vocal tract that is a time
varying linear filter driven by the sound sources. Although there are many computational reali­
zations of this model, the underlying mathematics for all of them is the wave equation. This
model has been remarkably successful in speech processing applications partly because it was
well matched to early forms of digital computing.
The source-filter model is inadequate today because it has many limitations with respect to
speech generation and recognition. For instance, speech recognizers based on this model are
incapable of making fine phonetic distinctions reliably when synthetic voices are used and
bandwidth is limited. These limitations can partly be overcome by developing new signal pro­
cessing models that can exploit high performance computing systems developed under the
HPCC Initiative. A particularly intriguing approach is that of computational fluid dynamics
(CFD) that views the speech signal as the acoustic consequence of fluid flow in the human vocal
apparatus. CFD is also appropriate for studying the human auditory periphery whose principal
organ is a fluid-filled chamber. Such studies may well help explain how the human listener can
classify accurately speech sounds under adverse conditions.
4.2.4. Integration of multiple levels of speech and language analyses
A speech-recognition system typically operates in a very large search space, producing
many candidate-sentence hypotheses for each spoken utterance. The most obvious role of the
language understanding component is to provide a meanful representation for an utterance,
which can be translated into some appropriate actions (such as database queries, function calls).
This component needs to be designed so that it constrains the recognizer’s search space and
selects the most “meaningful” candidate-word sequence.
Current recognition systems are generally “loosely coupled,” where the recognition com­
ponent delivers a set of candidate sentences to the language-understanding component after
completing its search. A tightly coupled system can potentially provide more accurate recogni­
tion, with the language understanding component working in locked steps with the recognition
component in order to select the most promising partial hypotheses. Such an integration,
November 23,1992 22
however, is very computationally intensive, due to the size of the search space traversed during
recognition. More research needs to be done in this area, especially in developing real-time
tightly coupled systems for incorporating multiple levels of linguistic knowledge (syntax,
semantics, discourse structure) and in providing constraints in the overall recognition process.
4.2.5. Connectionist approach for speech and language processing
Recent research has found that the connectionist approach can provide a cost-effective yet
specialized hardware solution to a number of problems in SNLP (such as continuous speech
recognition). Further research needs to be carried out on connectionist algorithms and represen­
tation methods in the context of realistic tasks, such as the recognition and understanding of
large vocabulary speech. This will allow the integration of knowledge in spoken-language
understanding at the acoustic, syntactic, semantic, and pragmatic levels, as well as data-driven
learning over all of these levels.
A major issue of using the connectionist approach is that the amount of computational
power and communication bandwidth required is large (well into the tera-operations- and tera-
bytes-per-second range), especially when practical and large training sets and neural networks
are concerned. New research need to be conducted in developing efficient yet general experi­
mental hardware and software solutions.
4.3. Implications for System Architectures
Algorithms in SNLP generally require an abundant amount of- computing power and
memory space. This need arises partially from the desire to do real-time analysis. A somewhat
less obvious reason is the need to analyze huge databases in experiments; these experiments will
simply not be done if analysis takes a very long time.
To illustrate the computing power required, consider sentence analyzers available today
that typically operate at 1-100 sentences per minute (on a 10 MIPS4 workstation), with the range
reflecting the degree to which they explore alternative hypotheses and make use of complex
7
semantic constraints. At this rate, a 10 sentence corpus (such as is now becoming available)
5 7
would require 10 to 10 minutes (2 to 200 months) to parse. For meaningful iterations in
parse/semantic analysis, the iteration cycle needs to less than 10 minutes, implying a speed-up
of 102 to 104 (1 to 100 GIPS4).
November 23,1992 23
In addition to the need for higher computational power, the architectural needs also differ
for different basic research topics addressed. These requirements may be better served by a
hybrid system containing general-purpose as well as special-purpose computers. In the rest of
this section, we discuss these requirements.
1) General-purpose supercomputing — more cycles, more memory, more bandwidth. Many
symbolic computations required for SNLP do not have the specialized regular structure that
would justify special-purpose architectures. These computations simply require a substantial
amount of computing power and will best be served by general-purpose parallel computer sys­
tems. For instance, many aspects of language analysis can be formulated as search tasks and can
be carried out in parallel computers. In addition, for analyses of large corpora, much of the pro­
cessing can be done independently on individual sentences and can be executed with minimal
communication overhead in a general-purpose parallel system.
2) Homogeneous architectures. While large problems in SNLP are inherently heterogene­
ous, homogeneity within large subtasks can be exploited. For instance, many tasks can be
expressed as matrix and vector operations that can be executed efficiently on parallel and vector
architectures. Attempts have also been made to unify the multiple aspects of SNLP using homo­
geneous architectures. Connectionist models are good examples for providing a consistent
representation at all levels, where activation variables may be viewed as a particularly simple
form of message (a single low-precision number). Although actual implementations may require
elements that are not of this form, to the extent that they are connectionist, both software and
hardware can be greatly simplified and still be quite flexible.
3) Special-purpose architectures. For a large part of SNLP, capabilities provided by
general-purpose supercomputers (such as floating-point arithmetic and sophisticated memory-
management schemes) are not required. For instance, many common paradigms — such as
dynamic time warp, table-driven Viterbi decoding for vector-quantized input, and connectionist
processing — use fixed-point arithmetic with moderate word widths (8-16 bits). Reduced-
precision arithmetic helps reduce the silicon area required for implementation, permits high-
bandwidth communication in pin-limited circuits, and simplifies interprocessor communication
requirements. These factors are particularly important for tasks common in SNLP, where data
movements commonly overwhelm computations required. In addition, virtual memory and
November 23,1992 24
elaborate operating systems are not needed, thereby further reducing the complexity and cost of
such systems.
The principal restriction of such a specialized system is its inflexibility. While some
researchers argue that a completely special-purpose machine is adequate, substantial changes on
the algorithms may be required for these machines. The challenge for architects in the speech
and natural language area is, therefore, to design machines that are sufficiently flexible and pro­
grammable, and yet have a high performance gain over the rapidly advancing general-purpose
machines. Such machines have been built and used successfully for limited sub-domains in
speech processing (multi-layer perceptron training and time-synchronous Viterbi decoding).
More research needs to be conducted in developing architecture-independent computational
models (such as object-oriented models) and algorithms, and methods for mapping these algo­
rithms on general-purpose as well as special-purpose computers.
4) Heterogeneous architectures and hybrid systems. Spoken-language systems generally
require a heterogeneous architecture, involving signal processing, numeric processing for the
search algorithms during recognition, and symbolic processing to support the knowledge-based
discourse and reasoning components of the system. In addition, in order to carry on real-time
conversations with users, a real-time system is needed for processing the input as it is received,
performing incremental signal processing, recognition, understanding (and even answer genera­
tion and speech synthesis). This creates an interesting challenge for high-performance architec­
tures, requiring heterogeneous, parallel, and pipelined processing. Moreover, a suitable architec­
ture would need high-bandwidth local networks and input/output devices for conducting on-line
experiments in real time at audio bandwidths.
There are many architectures that can be used for SNLP, ranging from heterogeneous sub­
systems that are loosely coupled (say, a general-purpose network of workstations), to a com­
pletely homogeneous SIMD array, and a whole spectrum of approaches in between. In general,
programmable systems require some homogeneity so that software can be developed. Further­
more, nearly every algorithm requires some amount of general processing capability that is not
obvious from the guiding equations. This “incidental” computing can easily dominate
resources when the design of an architecture and specialized hardware has been targeted for a
special algorithm. One possible solution is to provide a common computing element, such as a
November 23,1992 25
Reduced-Instruction-Set-Computer (RISC) core, to every node of the architecture, with special­
ized accelerators provided by (possibly heterogeneous) coprocessors. Such a system can be a
good platform for developing and executing parallel software, while permitting specialization
for subtasks (such as Viterbi search and feature extraction).
4.4. Infrastructure Support
The infrastructure needed for SNLP includes the development and technical support by
high-quality staff of sharable text and speech databases, and open, easy-to-access, and easy-to-
program parallel systems.
1) Sharable text and speech databases. An important role for a shared electronic library is
to provide not only one place where information can be shared and transmitted, but to provide a
place where the information itself can be studied. As we accumulate large quantities of text,
speech, and images in sharable databases in parallel supercomputers, we have a valuable
resource for research into SNLP and machine vision.
DARPA is in the process of establishing an industry-university consortium called Linguis­
tic Data Consortium (LDC) for generating very large text and speech corpora. Initial plans call
for the collection of texts up to 5 to 10 billion words, speech data for interactive tasks up to 400
hours, and read speech up to 1000 hours. HPCC environment is crucial for the successful
exploitation of these resources soon to be available.
2) Open parallel systems. One way to close the gap between special-purpose architectures
and general-purpose computers is to develop compilers that aid designers in mapping programs
on general purpose systems. This will allow designers to develop prototypes on general-purpose
systems that can be migrated later to special-purpose chips.
3) Easy access to high-performance computing systems. Research in this area will be
enhanced significantly when researchers have easy access to high-performance computing
centers through high-performance wide area networks. This allows not only the sharing of
expensive computing resources, but allows common on-line experiments to be conducted in real
time at multiple geographically distributed laboratories.
4. MIPS stands for million instructions per second. GIPS stands for giga-instructions per second.
November 23,1992 26
5. ARTIFICIAL INTELLIGENCE
In the last two sections, we have discussed grand challenge issues for two important forms
of communicating information — computer vision, and SNLP. In general, communication takes
place in a shared context that extends well beyond vision or language itself; we see objects and
understand language because we share certain knowledge about how things behave in the world.
Hence, any vision or language interface that involves understanding requires that the system
have information about the “world” that it operates in. For limited domains, this information
may be static and highly restricted. However, many domains will require substantial knowledge
about the real world, an ability to reason about objects in this larger context, and an ability to
maintain an updated view of the world based on actions that either the system or the user has
performed. In addition, in complex problem solving situations, the system may plan how to
interact with a user or how to solve a problem. This means that computer vision and language
understanding are intimately tied to knowledge representation, reasoning, common-sense models
of the world, and planning. Without this information, a vision or language understanding system
may need to model sizable chunks of world knowledge.
Most AI systems are built from knowledge and search, and high-performance hardware can
make a big difference in how much knowledge can be collected and how much search can be
carried out. The exact nature of the hardware varies from one AI paradigm to another. There is
room for very general hardware architectures (for experimenting with new ideas) as well as very
specialized architectures, once the performance and cost requirements are defined.
5.1. Grand Challenge Applications
In this subsection, we present eight specific applications requiring substantial innovation
and breakthroughs in AI and in systems research. These applications were what the workshop
participants thought would be of national significance and potentially achievable in a decade’s
time. Although many others were discussed, these were identified here to give a sense for what
might be accomplished in the future with significant advances in basic research and system
developments in AI.
1) Electronic librarian. One of the grand challenge applications in AI is to design a com­
puterized electronic librarian that knows how to navigate through the vast amount of knowledge
and information in an electronic library and provide useful information. Besides communicating
November 23,1992 27
with users in a form close to natural language (Section 4.1), the system must be able to access
databases in a variety of query languages, should update the databases automatically when new
knowledge and retrieval methods are available, and assist clients in locating and utilizing infor­
mation within an extremely large collection. Such a system will be immensely valuable to
scientists and researchers. It could, for example, provide a natural language interface to users,
scan the vast amount of knowledge and information, propose tests to run, find the right software
and computer systems, run the programs, and report the results.
In designing the electronic librarian, information retrieval is an important problem to be
addressed. The librarian would employ a mixture of AI, statistical inference, and linguistic
analysis to locate information which is of interest to patrons of the library. Intelligence is also
required in presenting the results of the search to the user and guiding the user as he or she
browses the contents of the database. The challenge to this problem is two-fold. First, current
techniques have been developed on relatively small databases and are only now being used and
evaluated on databases with a few gigabytes of data. An electronic library will require that these
methods be used on files containing tens, hundreds, and eventually thousands of gigabytes.
Additional research is needed to ensure that full text search is as easy to use on these large files
as it is on the more modest files of today. Second, most full text-retrieval methods to date are
concerned with reference searches in which the goal is to locate all documents pertaining to a
given topic. Future full text-retrieval techniques will need to adapt to account for differing
needs of users who wish to fine! tutorial material or to browse, and to organize searches
efficiently on thousands or tens of thousands of items on popular topics. -
2) Nation-wide job-bank. To help ensure that resources in the United States can be best
matched to the greatest needs, a national job and training database system would be very impor­
tant. Users with no computer skills can enter their work and skills profiles, scan job listings,
have their profiles automatically matched against employer postings, learn what training is
required to enter other fields, find sources of such training, and control the distribution and
matching of their profiles. Required technologies include SNLP, distributed computing, distri­
buted database technology, constraint satisfaction, agent modeling, and possibly intelligent and
interactive animation.
3) Tutoring system. To bolster our education system, we need to exploit recent technologi­
cal developments and develop intelligent and truly individualized tutoring systems for providing
November 23,1992 28
education on standard subjects. In addition to the computer-vision technologies discussed in
Section 2.1, a real-time interactive tutoring system will need to integrate new technologies such
as interactive multimedia systems, virtual reality, knowledge-based authoring systems, hyper­
text, and parallel processing.
4) Electronic market place. Providing a nation-wide electronic market place to bring sup­
pliers and consumers (or investors and offerers) together electronically offers enormous potential
for squeezing out inefficiencies in the retail, wholesale, and financial marketplaces. Reducing
these inefficiencies means enhanced competitiveness, and removing intermediaries or the “mid­
dle man” from the marketplace. The huge costs in doing business in this country can, therefore,
be greatly reduced, offering a relatively easy way for driving up productivity and enhancing the
gross national product. While this is not a new idea it has not been implemented at a large scale.
In such a system, both buyers and sellers of goods and services are represented by AI pro­
grams in the network, which find each other and negotiate prices and other details in order to
conclude the transactions. For instance, a customer wishing to purchase an airline ticket can
input into the system his or her utility function, encoding how much flexibility he or she has and
what he or she is willing to pay. The system then interacts directly with the corresponding sales
programs for airlines in order to negotiate the best times and fares for the traveler.
The grand technical challenge here is to provide a nationwide network that allows anyone
(hundreds of millions of users, businesses, institutions and individuals) to post requests for some
commodity (or financial investment) and to be matched with an optimal counterparty. Thus, at
each moment, many hundreds of millions of outstanding requests have to be matched with coun­
terparty offerers. One can list a multitude of enabling technologies needed to carry this out,
many impinging upon AI techniques. Essentially much knowledge needs to be brought to bare
on a “nationwide optimization problem” that is inherently distributed and very large.
5) Semantic integration and knowledge discovery. Besides collecting enormous amount of
data (from satellites, various sensors, laboratory data, etc.) and applying “complex numerical
modeling” and analysis techniques to that data, it is very important to consider the general
desideratum of “knowledge discovery.” An important and necessary grand challenge applica­
tion is the problem of merging multiple data/knowledge sources and of discovering new
knowledge from the merged sources. This involves much of what AI researchers have been
studying for many years: learning, reasoning and knowledge representation. The differences
November 23,1992 29
here, and consequently the grand challenges for AI, are: a) the scale of the problem is much
larger than anything attempted before in AI; b) means for integrating multiple knowledge
representations are important; c) methods for efficiently integrating multiple knowledge bases of
very large size are essential; and d) symbolic techniques need to be integrated effectively with
traditional numerical approaches.
6) Automatic knowledge acquisition. High performance AI systems will undoubtedly
require very large knowledge bases. Today, the construction of even small to medium
knowledge bases is a very time consuming process and often prevents the application of AI to
real-world problems. To overcome this deficiency, automation of knowledge-base construction
is needed. Knowledge may be acquired from the vast amount of information stored as texts.
Patterns of concepts and their semantic properties may then be extracted from text via natural
language parsing and learning techniques. Knowledge-classification and knowledge-base-
management techniques will need to be developed to support the incorporation of information
coming from diverse sources.
7) Intelligent planning and scheduling. This is a very large and economically important
area, especially for problems such as scheduling production lines to minimize bottlenecks and
delays, and planning flights for an airline to reduce overbooking and empty aircrafts. The prob­
lem here is similar to the problem in the electronic market place discussed above: find the best
match between demands and supplies. Some of these problems have been addressed before;
however, efficient solutions have not been found due to the facts that optimizations are often
beyond the limit of current supercomputers, and that experiments often involve perturbations of
existing systems that may not be acceptable in a working environment.
8) Medical diagnosis and informatics. This is an important application that requires the
integration of computer vision for analyzing medical images and the online storage and retrieval
of patient records, medical history, and expert knowledge. Solutions will require the develop­
ment of large knowledge bases interconnected by high bandwidth interconnection networks;
many of the design issues are similar to problems encountered in the development of an elec­
tronic librarian.
November 23,1992 30
5.2. Fundamental Science and Enabling Technologies
In this section, we describe six research issues. We start by presenting issues related to
machine learning and its implementations on high performance computing systems. In addition
to its prominence in AI, we have pointed out earlier the importance of machine learning in com­
puter vision and SNLP. This is followed by discussions on heuristic search and four important
enabling technologies that would aid in achieving success in basic research and application of
these results to realistically large problems. The key idea of these research issues is to embed AI
into “mainstream” systems in such a way that we can easily integrate AI with new systems and
allow it to coexist seamlessly with other HPCC applications.
5.2.1. Machine learning
Machine learning is an essential method for acquiring knowledge in AI. Besides the learn­
ing mechanisms discussed earlier in computer vision and SNLP, much recent work has focused
on sequential problems in decision, control, optimization, robotics, and planning. There are
numerous types of learning methods that learn by instruction, explanation, induction, deduction,
and analogy. Although many prototypes have been developed over the last forty years to test
different learning methods, there remain many open issues related to machine learning and com­
puter architecture. We list below a few important issues related to the implementation of
machine learning methods in high performance computing systems.
a) Scaling and generalization across tasks. Scaling is related to the generalization of the
knowledge learned to more general tasks beyond those used in the process of knowledge acquisi­
tion. Many current methods are restricted in their ability to generalize across the same problem
of different sizes and different problems. A particularly difficult problem is the verification and
validation of the knowledge generalized.
b) Quality-efficiency trade-off. This issue is related to the cost of using the knowledge
learned and the effectiveness of the knowledge acquired. Their trade-off is often not well
defined; hence, the learning system may have to be designed so it can propose alternative forms
of the knowledge for different application conditions.
c) Learning in knowledge-rich and knowledge-lean environments. When the learning
environment has access to domain-dependent and domain-independent knowledge, it is neces­
sary to design a learning system that can utilize this knowledge to the best extent. On the other
November 23,1992 31
hand, when the domain is knowledge lean, the issue is to design a robust learning method that
can acquire the best knowledge in limited time. The learning system must be able to trade
between generation (sampling) and testing (confidence-building).
d) Resource constraints. Resource constraints are almost always important in any learning
situations. These constraints include real-time constraints, constraints on storage and computa­
tional power, and constraints on communication bandwidth. Recent studies have addressed
these issues but not many powerful methods have been found. Since the search space for learn­
ing methods may be unbounded, the designer of the learning system must trade between the
amount of high performance computing resources available for learning and the quality of the
knowledge learned.
e) Instrumentation of application environment for learning. Many learning situations may
involve physical instrumentation of the learning environment and possibly modification of stra­
tegies used in the application. This may not be acceptable, especially in an operational system.
A possible approach is to build a model or a prototype, learn new knowledge and strategies
based on the model, and verify the knowledge on the physical system. In many cases, building
the prototype can be as hard as building the original system itself. More efforts are needed on
applying learning methods in realistic environments.
5.2.2. Heuristic Search
Heuristic search through a problem space is an important problem solving paradigm in
computer vision, SNLP, and AI applications in general. It is a powerful "model of high-level rea­
soning and problem solving. Heuristic search algorithms fall into three main categories: single
agent problem, two-player games, and constraint satisfaction problems. Since heuristic search
techniques may require an unbounded amount of time, space, and other resources, it is critical to
develop efficient search techniques that use limited resources, and automated learning methods
for acquiring heuristic knowledge used in the search.
5.2.3. Scalable and verifiable knowledge-based systems
Scalability and verifiability are two important properties in symbolic systems. On one
hand, knowledge may be acquired incrementally and expert systems may need to be refined over
time. It is, therefore, important for such systems to be scalable when more knowledge is avail­
able. On the other hand, expert systems applied in life-critical systems, such as process control
November 23,1992 32
and sensor fusion, may need to be verified before they can be applied. Studies in developing
verification methods have been carried out in the past, but none have been successful to the
extent of software engineering. More work on knowledge engineering — the engineering
approach to acquire, verify, and maintain knowledge — needs to be done.
5.2.4. Construction and utilization of very large knowledge bases
Knowledge is an important element in AI systems. A desideratum is to build large
knowledge bases by the end of the decade with orders of magnitude increases in size and scope.
These knowledge bases may be used for storing information in the applications discussed earlier.
Issues on knowledge representation, knowledge-access methods, handling multiple views of the
same knowledge, user interface, and the relationship between knowledge-base management and
database management should be addressed.
5.2.5. Active memories
Some of the main problems with current AI systems are related to the use and access of
knowledge effectively. Since knowledge accessed in a particular context may be scattered in a
large knowledge base, the efficiency is low when a large knowledge base is implemented in an
existing system designed on the basis of locality of access. One solution to this problem is to use
active memories, which can support marker passing and value passing, and reasoning techniques
such as memory-based and case-based reasoning. This concept is consistent with the desire in
computer architecture to decrease the so called Von Neumann bottleneck created by the separa­
tion of memory from processors. The development of effective architectures for supporting
operations in active memories is an important issue to be addressed.
5.2.6. Artificial neural networks
In the next few years it is expected that artificial neural networks can contribute in very
important ways to grand challenge applications that require subtasks such as pattern
classification. Although neural network technology in its modern incarnation is only 6 or 7 years
old, and most successes to date have been on simple classification problems, it may eventually
be able to handle high-level recognition and planning tasks. Besides the applications involving
low-level operations in computer vision and speech processing discussed earlier, it is recognized
that neural networks can complement learning and classification tasks studied in traditional AI.
November 23, 1992 33
An important research is, therefore, to develop techniques that can exploit both neural networks
and traditional machine learning methods in grand challenge applications.
5.3. Implications for System Architectures
Prior to designing HPCC systems, there is need for understanding the systems requirements
imposed by the A1 computation paradigm. The more we understand about the nature of AI pro­
cessing, the more efficient HPCC systems we can build. Although most of the current research
in AI is based on sequential reasoning, we expect that many future AI systems will use mas­
sively parallel processing techniques. This is straightforward in some AI paradigms which can
take advantage of the natural parallelism in applications. However, the design of large scale
parallel systems for supporting a large spectrum of AI techniques may impose a difficult task on
computer architects and programmers. It is clear that this research could benefit from interdisci­
plinary teamwork among AI researchers, knowledge engineers, computer architects, and applica­
tion designers. We identify the following issues in designing computers for AI processing.
1) Understanding the requirements. More work is needed for understanding the computa­
tional requirements for each AI processing paradigm used in high performance systems. It is
important to determine the processor structure for each paradigm. Architectures suited to sup­
porting AI will almost certainly differ significantly from conventional, numeric-oriented parallel
processing architectures. The computation models deemed most suited for AI computations tend
to exhibit large degrees of dynamical parallelism, dictating architectural support for dynamic
load balancing and dynamic extraction of parallelism. The grain sizes in AI computations will
most likely have far greater variances than those of numeric computations, requiring processor
structures oriented more toward latency toleration than latency hiding. Memory and I/O subsys­
tems will offer the opportunity to offload much of the repetitive computations from the CPU.
The interconnection network will need to support large amounts of nondeterministic accesses
and queries in knowledge processing and learning.
The design and evaluation of such systems may have to rely on expensive software simula­
tions in order to arrive at a proper trade-off between a software solution that maps the AI compu­
tations on existing parallel machines, and a hardware solution that requires the design of new
dedicated architectures.
November 23,1992 34
2) Supporting large knowledge bases. As we expect that future AI applications will
involve large knowledge bases, hardware and software supports for maintaining these
knowledge bases are very important. In these knowledge bases, reasoning techniques using the
knowledge stored may access information that is not localized in a particular storage media or
system. Moreover, the knowledge stored will evolve over time and may be revised by frequent
updates. Hence, important issues to be considered include the design of techniques for
knowledge-base partitioning and knowledge-base maintenance (such as update policies), and the
development of high-speed distributed computing systems, computer networks, and input/output
peripherals for supporting knowledge accesses.
3) Homogeneity versus heterogeneity. In many applications such as image and speech
understanding, symbolic AI processing and reasoning is done after intensive numeric computa­
tions take place. For these application areas one approach to high performance systems is to
design heterogeneous architectures that combine low-level numeric processing with high-level
reasoning. For other applications, it is seldom that one form of computer architecture is effec­
tive for the entire spectrum of algorithms. This phenomenon has been discussed earlier with
respect to grand challenge issues in computer vision and speech and natural language under­
standing. In such cases, it is desirable to have a heterogeneous architecture that has custom-
designed hardware for processing certain specialized tasks and general-purpose hardware for
carrying out the high-level reasoning and management tasks.
An important decision the systems designer must make is, therefore, the use of general-
purpose computers versus specialized accelerators. The system designer must understand the
computational requirements of AI processing and balance between the performance gains from
the specialized hardware and the cost of such implementations.
4) Language and compiler support. The choice of the language in AI processing is impor­
tant but it is not likely to solve the problem in the near future. Currently, there is a mismatch
between the high-level languages suitable for developing AI software (such as Lisp and other
traditional AI languages) and an architecture that can execute the resulting software for real time
applications. More work is needed in the development of good compilers, software enginnering
techniques, software development environments, debugging tools for large scale parallel sys­
tems, performance visualization tools, and methods for decomposing AI software between
general-purpose and special-purpose hardware.
November 23,1992 35
5.4. Infrastructure Support
AI researchers working on HPCC projects will benefit by having access to on-line
knowledge bases, benchmarks, corpora, (hardware and software) systems developed by other
researchers, and design and evaluation tools. Sharing such information will avoid spending
unnecessary efforts to develop similar information, and will help calibrate new results developed
in the future. Such information sharing practices are emerging, as is in the development of com­
mon benchmarks in computer vision and SNLP. However, the community at large will need
more coherent and coordinated efforts in collecting such information and in making it easily
accessible.
The AI community also needs access to fast and large scale parallel computing resources
with fast I/O, and toolkits embodying traditional numerical modeling codes (such as Monte
Carlo simulations, ordinary differential equations, and various statistical analysis packages). To
integrate AI systems with traditional numerical systems implies that the AI systems must be
“embeddable” into “mainstream” computing environments. This also means that AI symbolic
computing at a minimum must be compiled into the native environments hosting all of the com­
ponents. These sharable resources allow AI researchers to scale their small scale prototypes and
simulations on workstations to a more realistic environment. Using these resources, researchers
can evaluate performance of large scale experiments, process large data-sets in a reasonable
amount of time, integrate with numerical systems, and find means of efficiently assimilating
diverse sources of information.
November 23,1992 36
6. FINAL REMARKS
We have identified in this article many challenging applications and numerous issues that
need further study. These grand challenge application problems are probably among the hardest
that any researchers have attempted before. Some of these applications may be within our reach
by the year 2000, while others may take longer for solutions to mature. Progress in these areas
will require advances in device and networking technologies, signal processing techniques, AI,
algorithms, and new ideas in integrating many existing results and components into working sys­
tems.
In this article we have only scratched the surface of the issues related to grand challenge
applications in the areas discussed. Although we do not offer any solutions for these problems,
we have succeeded in bringing together many experts in these diversified areas to discuss com­
mon issues, solutions, and techniques. We feel that there is much to gain by holding frequent
workshops such as the one that this report is based on. These workshops allow people working
in different areas — researchers, application designers, AI engineers, tools builders, and experts
working on specific grand challenge application problems — to get together and reexamine their
work and attempt to find common solutions. We hope that this article will motivate further
excitement in these areas.
November 23,1992 37
ACKNOWLEDGEMENTS
The authors of this report wish to thank Dr. Su-Shing Chen and Dr. Y. T. Chien of the NSF
for suggesting this workshop. Support of this workshop was provided by National Science
Foundation under grant IRI-92-12592, which was jointly funded by the Knowledge Models and
Cognitive Systems, Robotics and Machine Intelligence, and Interactive Systems Programs of the
IRIS5 Division, the Microelectronic Systems Architecture Program of the MIPS5 Division, and
the Computer Systems Program of the CCR5 Division. The authors also appreciate very much
the guidance provided by Dr. Chen in focusing our attention on important issues. The authors
are also grateful for the comments provided by the Division and Program Directors of NSF who
attended the workshop.
5. IRIS Division is the Division of Information, Robotics, and Intelligent Systems. MIPS Division is the Division of Microelectronic
Information Processing Systems. CCR Division is the Division of Computer and Computation Research.
November 23,1992 38
AUTHOR BIOGRAPHIES
Benjamin W. Wah is a Professor of Electrical and Computer Engineering and a Research
Professor at the Coordinated Science Laboratory and the Beckman Institute, University of Illi­
nois at Urbana-Champaign. His current research interests are in areas relating parallel and distri­
buted processing and AI. His major focus is on the development of efficient resource manage­
ment schemes for machine learning and search, with applications in operating systems, neural
networks, computer vision, and genetic algorithms. He is a Fellow of the IEEE, is Editor-in-
Chief of IEEE Transactions on Knowledge and Data Engineering, and serves as a member of
the IEEE Computer Society Board of Governors.
Thomas S. Huang is a Professor of Electrical and Computer Engineering, and a Research
Professor at the Coordinated Science Laboratory and the Beckman Institute, University of Illi­
nois at Urbana-Champaign. His current research interests are in image processing and computer
vision, especially time-varying image processing and dynamic scene analysis. He is a Fellow of
the IEEE and the Optical Society of America; Area Editor for Motion of the international journal
CVGIP: Image Understanding, and an Editor of the Springer Series in Information Sciences. He
served as the Chairman of the IEEE Computer Society Technical Committee on Pattern Analysis
and Machine Intelligence, 1989-91, and received the Technical Achievement Award (1987) and
the Society Award (1991) of the IEEE Signal Processing Society.
Aravind K. Joshi is the Henry Salvatori Professor of Computer and Cognitive Science at the
University of Pennsylvania, Philadelphia, PA. His research is in the areas of computational
linguistics, mathematical and processing models of language, AI, and cognitive science. He is a
Fellow of IEEE, a Past President of the Association for Computational Linguistics, and a Found­
ing Fellow of the AAAI. At present he is a Co-director of the Institute for Research in Cognitive
Science, a NSF Science and Technology Center, at the University of Pennsylvania.
Dan /. Moldovan received his M.S. and Ph.D. degrees in Electrical Engineering and Com­
puter Science from Columbia University, New York, NY, in 1974 and 1978, respectively. He is
an Associate Professor of Computer Engineering at the University of Southern California. His
primary research interests are in the fields of parallel processing and AI. Moldovan is the Direc­
tor of the Parallel Knowledge Processing Laboratory where a parallel computer for AI applica­
tions is being built.
November 23,1992 39
Yiannis Aloimonos studied Mathematics in Athens, Greece and Computer Science at the
University of Rochester, NY (Ph.D. 1986). He is currently an Associate Professor at the Depart­
ment of Computer Science of the University of Maryland, College Park, and is affiliated with the
Institute for Advanced Computer Studies. He is also the Head of the Computer Vision Labora­
tory of the Center for Automation Research of the same University. His research interests are in
the area of Computer Vision and Robotics.
Ruzena Bajcsy received her Ph.D. in Electrical Engineering at the Slovak Technical
University in Bratislava, Czechoslovakia in 1967 and Ph.D. in Computer Science from Stanford
University in 1972. Since 1972, she has been on the faculty of the Computer and Information
Science Department at the University of Pennsylvania, where she served as its chairperson
between 1984 and 1989. She has worked in the general area of machine perception, covering the
whole spectrum of problems (segmentation, 3-D shape recognition, multiresolution problems,
etc.) that the field of computer vision involves, and has extended it to other modalities, in partic­
ular to touch. She has built the GRASP Laboratory. She is a Founding Fellow of the AAAI and
a Fellow of the IEEE since January 1992.
Dana Ballard obtained his Ph.D. degree from the University of California at Irvine in 1974.
During 1974-1975 he had a post-doctoral appointment at the Laboratorio Tecnologie
Biomediche in Rome, Italy. Since 1975 he has been at the University of Rochester in the Com­
puter Science Department where he has the rank of professor. He is the coauthor with Professor
Brown of Computer Vision, a standard text. Ballard’s current research focus is in computational
theories of vision that account for the real-time performance. In 1985 with Chris Brown, he led
led a team that designed and built a high speed binocular camera control system capable of
simulating human eye movements.
Doug DeGroot received his Ph.D. in Computer Science from the University of Texas at
Austin in 1981. He then joined IBM’s T.J. Watson Research Center where he worked on the
design team for an intemally-parallel, numeric and vector processor based on the IBM 801 pro­
cessor architecture. He then became VP of R&D at Quintus Computer Systems, developers of
Quintus Prolog. In 1987, DeGroot joined the Computer Science Center of Texas Instruments in
Dallas, Texas, as Manager of the Parallel and System Architectures Branch. DeGroot is ex-
Chairman of ACM’s SIGARCH. He is on (or has served on) the Editorial Boards of IEEE
Expert, the Journal of Parallel and Distributed Computing, New Generation Computing, and the
November 23,1992 40
IEEE Transactions on Parallel and Distributed Systems.
Kenneth A. DeJong is an Associate Professor of Computer Science and Associate Director
of the AI Center at George Mason University in Fairfax, Virginia. He received his Ph.D. degree
in Computer Science from the University of Michigan in 1975. Dr. DeJong’s research interests
include adaptive systems, machine learning, expert systems, and knowledge representation. He
is active in the Genetic Algorithms research community with a variety of papers, Ph.D. students,
and presentations. He is currently chair of the executive council of the Society of Genetic Algo­
rithms and Editor-in-chief of a new journal, Evolutionary Computation.
Charles R. Dyer received his Ph.D. degree in computer science from the University of
Maryland, College Park, in 1979. He is currently Professor and Chair of the Computer Sciences
Department at the University of Wisconsin, Madison. His research interests in computer vision
include model-based image understanding, active and purposive vision, three-dimensional object
representations, spatiotemporal image analysis, and parallel algorithms and architectures for
vision. He has over 80 publications in these and other related areas of computer vision. Dr.
Dyer was an Associate Editor of the IEEE Transactions on PAMI from 1987 to 1991, and is
currently on the Editorial Board of the Journal of Machine Vision and Applications. He is Pro­
gram Co-Chair of the 1992 IEEE Workshop on Applications of Computer Vision.
Scott E. Fahlman is a Senior Research Scientist in Carnegie Mellon University’s School of
Computer Science, where he has been on the faculty since 1978. His research interests include
AI and artificial neural networks. He has developed several new learning algorithms for neural
nets, including Quickprop and Cascade-Correlation. He has also been active in the development
of hardware and software tools for AI, including the Common Lisp language. Fahlman received
his Ph.D. from the Massachusetts Institute of Technology in 1977 for his work on the NETL sys­
tem, a semantic network memory built upon massively parallel hardware. His B.S. and M.S.
degrees are also from M.I.T.
Ralph Grishman is Professor of Computer Science at New York University. He has been
involved in research in natural language processing since 1969. He served for a year (1982-83)
as project leader in natural language processing at the Navy Center for Applied Research in
Artificial Intelligence. Since 1985, he has directed the Proteus Project, funded by the Defense
Advanced Research Projects Agency and the National Science Foundation, which has conducted
research in natural language text analysis, including information extraction, document retrieval,
November 23,1992 41
and machine translation. He is a past president of the Association for Computational Linguis­
tics, and the author of the text Computational Linguistics: An Introduction.
Lynette Hirschman is Principal Research Scientist in the Spoken Language Systems group
at M.I.T. Laboratory for Computer Science. She has done research on language understanding
since receiving her Ph.D. from the University of Pennsylvania in 1972. Her early research
includes work on sublanguage grammars and discovery procedures for sublanguage semantics,
as well as logic-grammar based text understanding systems. Her recent work focuses on spoken
language systems, particularly integration of speech and natural language processing and dialog
modeling. She chairs the DARPA Spoken Language Multi-Site Data Collection Working
Group, coordinating data collection and performance evaluation of interactive spoken language
systems.
Richard Korf is an Associate Professor of computer science at the University of California,
Los Angeles. He received his B.S. from M.I.T. in 1977, and his M.S. and Ph.D. from Carnegie-
Mellon University in 1980 and 1983, respectively, all in computer science. From 1983 to 1985,
he served as Herbert M. Singer Assistant Professor of Computer Science at Columbia Univer­
sity. His research is in the areas of problem-solving, heuristic search, and planning in AI. He is
the author of Learning to Solve Problems by Searching for Macro-Operators (Pitman, 1985).
Dr. Korf is the recipient of a 1985 IBM Faculty Development Award, a 1986 NSF Presidential
Young Investigator Award, and the first UCLA Computer Science Department Distinguished
Teaching Award, in 1989.
Stephen E. Levinson received his Ph.D. degree in Electrical Engineering from the Univer­
sity of Rhode Island in 1974. He has served as a design engineer at General Dynamics between
1966 and 1969, and a J. Willard Gibbs Instructorship in Computer Science at Yale University
between 1974 and 1976. In 1976, he joined Bell Laboratories in Murray Hill, NJ. He is
currently head of the Linguistics Research Department at AT&T Bell Laboratories where he
directs research in Speech Synthesis, Speech Recognition and Spoken Language Translation.
Dr. Levinson is a fellow of the IEEE and the Acoustical Society of America. He is on the edi­
torial board of Speech Technology, a founding editor of Computer Speech and Language, and a
member of the IEEE Signal Processing Society Technical Directions Committee.
Daniel P. Miranker is an Associate Professor of Computer Science at the University of
Texas at Austin. He earned a B.S. (1979) in mathematics at M.I.T. and a Ph.D. (1987) in
November 23,1992 42
Computer Science at Columbia University with a thesis on the parallel execution of expert-
system programs on DADO, a 1,023 processor multicomputer. He is co-patent holder on the
DADO architecture. Many copies of the DADO 2 were built and are still in use. Prof. Miranker
has been at the University of Texas since 1986. His work there encompasses compilation of
rule-based programs for high performance execution, targeting parallel supercomputers, data­
base systems and real-time environments.
Nelson Morgan is the group leader for the Computer Engineering Department (called the
Realization Group) at the International Computer Science Institute (ICSI), a non-profit research
laboratory closely associated with the EECS Department at U.C. Berkeley. He is also an
Adjunct Professor in the EECS Department. In the areas of speech and brain wave processing he
has developed connectionist approaches to signal analysis, as well as hardware and software sys­
tems to speed research. His current interests include the design of algorithms, architectures, and
systems for parallel signal processing and pattern recognition, particularly using connectionist
paradigms.
Sergei Nirenburg is Senior Research Computer Scientist at the Center for Machine Transla­
tion, School of Computer Science, Carnegie Mellon University in Pittsburgh, PA. He received
his Ph.D. in Linguistics from the Hebrew University of Jerusalem, Israel. He has written or
edited four books and has published over 60 articles in various areas of computational linguistics
and AI, including machine translation, natural language understanding and generation,
knowledge acquisition, intelligent interfaces, planning and cognitive modeling. Since 1987 he
has served as Editor-in-Chief of Machine Translation, and since 1991, as First Vice President of
the American Machine Translation Association. Dr. Nirenburg has directed many large research
and development projects on machine translation and natural language processing.
Tomaso Poggio is the Uncas and Helen Whitaker Professor in the Department of Brain and
Cognitive Sciences at M.I.T. He is known for his pioneering work on stereo vision, for his work
on the application of regularization theory to the problem of early vision, and more recently, for
the development of a new theory of learning and networks. He is Co-Director of the Center for
Biological Information Processing at M.I.T., where he pursues the understanding of information
processing in the brain. He is a Corporate Fellow of Thinking Machines Corporation and has
won many awards. He has written over 200 papers in the areas of AI, computer vision, biologi­
cal information processing, learning, and nonlinear systems theory. He is a Fellow of the AAAJ,
November 23,1992 43
and an Associate of the Neuroscience Research Program at Rockefeller University.
Edward M. Riseman received his Ph.D. degree in Electrical Engineering from Cornell
University in 1969. He is currently Professor of Computer Science at the University of Mas­
sachusetts, where he has conducted research in computer vision, AI, learning, and pattern recog­
nition. Since 1976, he has been the Director of the Computer Vision Laboratory with current
projects in know ledge-based scene interpretation, motion analysis, mobile robot navigation and
parallel architectures for real-time vision. He has applied the analysis of static and dynamic
images to a variety of domains, including vehicle navigation in outdoor terrain, industrial
manufacturing environments, and satellite photointerpretation. He is on the Editorial Boards of
CVGIP: Image Understanding, the International Journal of Computer Vision, and is a Fellow
Member of the AAAI.
Craig Stanfill received his Ph.D. from the University of Maryland in 1983 for work on
qualitative reasoning for Artificial Intelligence. Since then he has worked for Thinking
Machines Corporation, studying the use of massively parallel computers for a variety of tasks
including memory-based reasoning, statistical computing, database mining, information
retrieval, and relational databases. He is currently serving as secretary of ACM SIGIR.
Sal Stolfo is Associate Professor of Computer Science at Columbia University, where he
has been on the faculty since 1979. He received his Ph.D. in Computer Science from NYU’s
Courant Institute of Mathematical Sciences. He has conducted research in the area of AI
knowledge-based systems and parallel processing. In 1982 he co-designed and developed ACE,
an expert system that integrated AI inference techniques with conventional database manage­
ment system. Stolfo also led the design and development of the DADO parallel computer spe­
cialized to run knowledge-based systems and signal interpretation tasks at high speeds. In 1983,
he established the Massively Parallel Computing focal project of the Center for Advanced Tech­
nology at Columbia University.
Steven L. Tanimoto is Professor of Computer Science and Engineering and Adjunct Profes­
sor of Electrical Engineering at the University of Washington in Seattle, having joined the
faculty in 1977. His research has concentrated on pyramidal data structures and the languages,
parallel architectures, and algorithms that work with them. Other projects are or have been con­
cerned with the use of pictures and diagrams to program and use computers and with the use of
image processing in mathematics education. Dr. Tanimoto served as Editor-in-Chief of the
November 23,1992 44
IEEE Transactions on Pattern Analysis and Machine Intelligence from 1986 to 1990. He is the
author of The Elements of Artificial Intelligence Using Common Lisp published by W.H. Free­
man in 1990.
Charles Weems received his B.S. and M.A. degrees from Oregon State University in 1977
and 1979, respectively, and his Ph.D. from the University of Massachusetts at Amherst in 1984.
All degrees are in Computer Science. Since 1984 he has been Director of the Parallel Image
Understanding Architectures Research Group at the University of Massachusetts, where he is an
associate professor. His research interests include parallel architectures to support low-,
intermediate-, and high-level computer vision, benchmarks for vision, parallel programming
languages, and parallel vision algorithms. He is also the coauthor of a widely used introductory
computer science text.
November 28,1992 45
AUTHOR ADDRESSES
Prof. John Aloimonos Dr. Scott E. Fahlman
Center for Automation Research School of Computer Science
University of Maryland Camegie-Mellon University
College Park, MD 20742-3411 5000 Forbes Avenue
e-mail: yiannis@alv.umd.edu Pittsburgh, PA 15213
phone: (301)405-4526 (d) e-mail: scott.fahlman@cs.cmu.edu
phone: (412)268-2575
Prof. Ruzena K. Bajcsy
Computer & Information Science Prof. Ralph Grishman
University of Pennsylvania Computer Science Department
3401 Walnut St, Rm 301B New York University
Philadelphia, PA 19104 715 Broadway, Rm 703
e-ma il: baj csy@central. cis. upenn.edu New York, NY 10003
phone: (215)898-0370 e-mail: grishman@nyu.edu
Prof. Dana Ballard Dr. Lynette Hirschman
Dept of Computer Science Spoken Language Group, LCS
University of Rochester Massachusetts Institute of Technology
Rochester, NY 14627 545 Technology Square, NE43-643
e-mail: dana@cs.rochester.edu Cambridge, MA 02139
phone: (716)275-3772 e-mail: hirschman@goldilocks.lcs.mit.edu
phone: (617)253-7399
Dr. Doug DeGroot
Texas Instruments Prof. Thomas Huang
P.O. Box 869305, M/S 8435 Coordinated Science Laboratory
Plano, Texas 75086 University of Illinois
e-mail: degroot@dog.dseg.ti.com 1308 W. Main Street
phone: (214)575-3545 Urbana, IL 61801
e-mail: huang@uicsl.csl.uiuc.edu
Prof. Kenneth DeJong phone: (217)333-6192
Computer Science Department
George Mason University Prof. Aravind K. Joshi
Fairfax, VA 22030 Computer and Information Science Department
e-mail: kdejong@aic.gmu.edu University of Pennsylvania
phone: (703)993-1553 Moore School, Rm 555
Philadelphia, PA 19404
Dr. Charles R. Dyer e-mail: joshi@central.cis.upenn.edu
Dept of Computer Science phone: (215)898-8540
University of Wisconsin
Madison, Wisconsin 53706 Prof. Richard E. Korf
e-mail: dyer@cs.wisc.edu Computer Science Department
phone: (608)262-1204 University of California, Los Angeles
3532-H Boelter Hall
Los Angeles, California 90024-1596
e-mail: korf@cs.ucla.edu
phone: (310)206-5383
November 28,1992 46
Dr. Stephen E. Levinson Prof. Edward M. Riseman
Speech Research Dept., 2D-528 Computer and Information Science Department
AT&T Bell Labs University of Massachusetts
600 Mountain Avenue Graduate Research Center
Murray Hill, NJ 07974 Amherst, MA 01003
e-mail: sel@research.att.com e-mail: riseman@cs.umass.edu
phone: (201)582-3503 phone: (413) 545-2746
Dr. Daniel P. Miranker Dr. Craig Stanfill
Department of Computer Sciences Advanced Information Systems
University of Texas at Austin Thinking Machines Corporation
Taylor Hall 2.124 245 First Street
Austin, TX 78712-1188 Cambridge, MA 02142-1214
e-mail: miranker@cs.utexas.edu e-mail: craig@think.com
phone: (512) 471-7316 phone: 617-876-1111
Prof. Dan Moldovan Dr. Salvatore J. Stolfo
Dept of Electrical Engineering—Systems Computer Science Department
University of Southern California Columbia University
University Park 468 Computer Science Building
Los Angeles, CA 90089-0781 New York, NY 10027
e-mail: moldovan@gringo.usc.edu e-mail: sal@cs.columbia.edu
phone: (213) 740-4477 phone: (212) 854-8796
Dr. Nelson H. Morgan Prof. Steven L. Tanimoto
International Computer Science Institute Department of Computer Science, FR-35
1947 Center St., Suite 600 University of Washington
Berkeley, CA 94704 Seattle, WA 98195
e-mail: morgan@icsi.berkeley.edu e-mail: tanimoto@cs.washington.edu
phone: (415)643-9153 phone: (206)543-1695
Prof. Sergei Nirenburg Prof. Benjamin W. Wah
Center for Machine Translation Coordinated Science Laboratory
Camegie-Mellon University University of Illinois
5000 Forbes Avenue 1308 West Main Street
Pittsburgh, PA 15213-3890 Urbana, IL 61801
e-mail: sergei@nl.cs.cmu.edu e-mail: wah@manip.crhc.uiuc.edu
phone: (412)268-6593 phone: (217)333-3516
Prof. Tomaso Poggio
Artificial Intelligence Prof. Charles Weems
Massachusetts Institute of Technology Computer and Information Science Department
545 Technology Square, NE43-787 University of Massachusetts
Cambridge, MA 02139 Graduate Research Center
e-mail: poggio@ai.mit.edu Amherst, MA 01003
phone: (617)253-5230 e-mail: weems@cs.umass.edu

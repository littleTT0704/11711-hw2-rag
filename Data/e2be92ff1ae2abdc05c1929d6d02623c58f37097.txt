TYPE OriginalResearch
PUBLISHED 15May2023
DOI 10.3389/frai.2023.1048874
Image–text coherence and its
implications for multimodal AI
OPENACCESS
EDITEDBY
AndyLücking,
UniversitéParisCité,France MaliheAlikhani1*,BaberKhalid2 andMatthewStone2
REVIEWEDBY 1DepartmentofComputerScience,UniversityofPittsburgh,Pittsburgh,PA,UnitedStates,2Department
CaseyKennington,
ofComputerScience,RutgersUniversity,Piscataway,NJ,UnitedStates
BoiseStateUniversity,UnitedStates
ChristianOtto,
L3SResearchCenter,Germany
Human communication often combines imagery and text into integrated
*CORRESPONDENCE
presentations, especially online. In this paper, we show how image–text
MaliheAlikhani
malihe@pitt.edu coherence relations can be used to model the pragmatics of image–text
presentationsinAIsystems.Incontrasttoalternativeframeworksthatcharacterize
SPECIALTYSECTION
Thisarticlewassubmittedto image–text presentations in terms of the priority, relevance, or overlap of
LanguageandComputation,
information across modalities, coherence theory postulates that each unit of a
asectionofthejournal
FrontiersinArtificialIntelligence discourse stands in specific pragmatic relations to other parts of the discourse,
witheachrelationinvolvingitsowninformationgoalsandinferentialconnections.
RECEIVED20September2022
ACCEPTED10March2023 Text accompanying an image may, for example, characterize what’s visible in
PUBLISHED15May2023 the image, explain how the image was obtained, offer the author’s appraisal
CITATION of or reaction to the depicted situation, and so forth. The advantage of
AlikhaniM,KhalidBandStoneM(2023)
coherence theory is that it provides a simple, robust, and effective abstraction
Image–textcoherenceanditsimplicationsfor
multimodalAI.Front.Artif.Intell.6:1048874. ofcommunicativegoalsforpracticalapplications.Toarguethis,wereviewcase
doi:10.3389/frai.2023.1048874 studiesdescribingcoherenceinimage–textdatasets,predictingcoherencefrom
COPYRIGHT few-shotannotations,andcoherencemodelsofimage–texttaskssuchascaption
©2023 Alikhani,KhalidandStone.Thisisan generationandcaptionevaluation.
open-accessarticledistributedundertheterms
oftheCreativeCommonsAttributionLicense
(CCBY).Theuse,distributionorreproduction KEYWORDS
inotherforumsispermitted,providedthe
coherence,discourse,multimodality,machinelearning,evaluation
originalauthor(s)andthecopyrightowner(s)
arecreditedandthattheoriginalpublicationin
thisjournaliscited,inaccordancewith
acceptedacademicpractice.Nouse, 1. Introduction
distributionorreproductionispermittedwhich
doesnotcomplywiththeseterms.
The internet has become a multimodal information ecosystem, where units of
content—news articles, web pages, posts to social media—regularly tie together written
words,emojiandothericons,staticanddynamicimagery,andlinkstoyetmoremultimodal
content. Faced with the heterogeneity of online information, Artificial Intelligence (AI)
researchers have increasingly characterized problems of information access from the
perspective of multimodality: for example, producing text captions that make visual
information more accessible (e.g., Lin et al., 2014; Young et al., 2014); or taking both
textandimagecontentintoaccountininformationretrieval(e.g.,FunakiandNakayama,
2015; Chowdhury et al., 2019). At the same time, this heterogeneity has empowered AI
researchers to compile vast multimodal datasets (e.g., Sharma et al., 2018) and to build
largescale“foundation”models(e.g.,Luetal.,2019;Radfordetal.,2021)trainedtocapture
cross-modalpatternsandmakecross-modalpredictions.
Applicationsofsuchmodels,liketheDALL-Esystemforsynthesizingimageryfromtext
(Rameshetal.,2021),havecapturedtheimaginationofresearchersandthepublicalikeand
serveashigh-profileexamplesoftheabilityofrepresentationlearningtodrivesurprisingly
richAIcapabilities.
The rapid progress in multimodal AI brings new urgency to the challenge of better
understanding the data, tasks, model architectures, and performance metrics in the field.
Infact,eventhebasicdatapoints,“image–textpairs”scrapedfromtheweb,asinRadford
etal.(2021),involvediverseandsurprisingjuxtapositions.InFigure1,forexample,wesee
FrontiersinArtificialIntelligence 01 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
FIGURE1
Peoplecombinetextandimagescreativelytocommunicate.Inthesetwounrelatedimage–textpairs,theimagesprovideexplanationsforwhatis
describedinthetext.Imagecredits:MaliheAlikhani.
image–text pairs where the image depicts the reason for the text currentAIrarelyapproachessuchinferencesexplicitly,coherence
contribution—the depicted traffic is why the author will be late; theoryneverthelessremainsaninfluentialparadigmthatinformsa
the possibility of encountering bears in nature is why repellent widerangeofAIworkontextdiscourse,aswesurveyinSection2.
is needed. Though such text is “grounded” in imagery only in Interpreting image–text presentations requires analogous
a very abstract way, such inferences seem like common-sense to inferences across modalities, including inferences that locate the
humanreaders.Howgoodthenarelargevision–languagemodels viewpoint of imagery (Cumming et al., 2017), identify depicted
at capturing the varied implicit generalizations and relationships objects(Abusch,2013),andplacetheongoingsceneinthearcof
that connect text and imagery? How robust to this variation thenarrative(Cohn,2013).Somecoherencerelationslinkimagery
are machine learning approaches to using natural language as a together(McCloud,1993).Othersguideinferencesthatenrichthe
supervisionsignalformultimodalinference?Canwedesignmodels joint interpretation of communicative actions across modalities
that better understand and reason about these inferential links? (e.g., Lascarides and Stone, 2009a; Stone and Stojnic, 2015). The
More generally, what concepts and methods are needed for AI specificcaseofimage–textcoherenceisthefocusofourworkhere
researcherstoexploresuchquestionsinpreciseandeffectiveways? and underpins the contribution of our research. Section 2 builds
In this paper, we address these challenges through the lens onourreviewofbroaderworkoncoherenceinAItomotivateand
of theories of discourse coherence (Phillips, 1977; Hobbs, 1979, characterizeimage–textcoherence.
1985; Asher and Lascarides, 2003). Our focus is on image–text Having laid out the principles of coherence, we go on to
coherence,wherewearguethatcoherencerelationsthatresolvethe demonstrate the significance of image–text coherence for state-
interpretationoftextsegmentsagainstjuxtaposedimageryoffera of-the-art multimodal AI. Section 3 explores how coherence can
broad and powerful framework to improve AI datasets, models, beusedtoannotateandtoanalyzeimage–textdatasets.Section4
and systems so that they can better account for the structural, illustrates how coherence can be used to make sense of the
logical and purposeful organization of authors’ communicative representations and learning of different model architectures for
contributionstoonlinediscourse. multimodalAI.Section5reviewshowcoherence-awaretasksand
Coherence theory originates in the detailed analysis of the metrics enhance researchers’ ability to build more useful tools
inferences needed to support text interpretation in knowledge- andmeasureperformanceinmoremeaningfulways.Wecloseby
based approaches to natural language processing. For example, suggestingsomekeydirectionsforbuildingonthesesuccessesin
thisdiscoursefromHobbs(1985ex3)dependsoncommon-sense futureresearch.
knowledgeaboutbooksareformedandhandled:
(1) Johntookabookfromtheshelf.Heturnedtotheindex. 2. Coherence in image–text
presentations
Coherencehereconsistsofthefactthatthefirsteventbrings
about the situation in which the second event takes place—a
relationship referred to as Occasion (Hobbs, 1985) or Narration We begin with an overview of coherence theory. We
(Asher and Lascarides, 2003). For coherence theory, establishing have two aims. Our first aim is to present the motivations,
this Occasion relationship guides and prompts key inferences, analyses, and principles of coherence theory as an approach to
including the inference that John’s turning involves opening the multimodal discourse. Like text discourse, we argue, multimodal
bookhemustbeholdinginhishandtoanewpageandtheinference discourse recruits numerous fine-grained inferences to enrich
thattheindexreferstothesectionofthisbookJohnexhibits.While the interpretation of communicative contributions in context;
FrontiersinArtificialIntelligence 02 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
what unifies these inferences is the need to establish coherence pronounsthatyou’dexpectifthiswasastraightforwardextended
relationsthatorganizethecontributionsofpartsofthediscourse description of a single scene; the structure of Example (4) also
into an integrated whole. It may seem counterintuitive that such exhibits the syntactic and semantic parallels you’d expect if the
an approach—a theory devised to address abstract foundational authorwantedtofacilitateacomparison.Inthatsense,linguistic
questions about meaning—should pave the way for concrete formcorroboratescoherence.Butformdoesnotsignalcoherence—
progress in AI. Our second aim, then, is to explain why there are no words or constructions that give decisive evidence
the constructs of coherence theory have such direct, practical about what the author has in mind. Ambiguity is pervasive. A
implications for AI methodology in general, and for multimodal consonant lesson of Section 5 is that AI researchers should be
AIinparticular. skepticalthatmodelsandmetricslearnandrespectcoherence.If
coherence matters, rather than hoping learning methods capture
itautomatically,werecommenddesignerssolvecoherence-aware
2.1. Coherence: The key ideas tasksandassesssystemsoncoherence-awaremetrics.
Alternative coherence relations provide broad organizing
Authorshavedifferentpurposesinpresentinginformation.The frameworks for interpretation, rather than clearly demarcated
writerofExample(2),forexample,usesthesecondsentencetooffer elements of form or content. Nevertheless, ambiguities in
anexplanationofwhythefirsteventcameabout. coherence often correlate transparently with clear interpretive
differences. Smyth (1994) uses Example (5), to illustrate the
(2) Maxspiltabucketofwater.Hetrippedonhisshoelace.
connectionbetweencoherenceandcoreference.
Suchrelationshipsarecentraltomakingsenseoftheauthor’s (5) PhiltickledStanley,andLizpokedhim.
message. The second sentence of Example (2) works as an
Coherence could organize (Example 5) in two ways. The
explanation, for example, only because we understand He as
second sentence could describe Liz’s contingent reaction to the
Max, his as Max’s, the shoelace as that of one of the shoes
tickling. This suggests that Liz poked Phil, perhaps expressing
Max must be wearing, and the event where He tripped as
disapproval of his action. Alternatively, the two sentences could
located temporally immediately prior to the spilling. Coherence
describe similar events. That suggests that Stanley is the object
theory counsels that we take a fundamentally relational view
of both tickling and poking. Asking how him is interpreted
of actions in discourse. Successive actions in discourse don’t
indirectly answers questions about coherence. Similarly, we can
expressindependentpropositionsoractunassistedtoinfluencethe
easily appreciate the different coherence relations in Examples
audience:discoursecontributionsbuildononeanother.Coherence
(2–4) by considering when we understand the second event to
relationsspecifyhowtheydothis.(OurdiscussionofExamples2–4
haveoccurred:beforethefirst,simultaneoustoit,oratanytime
followsKehler,2002;AsherandLascarides,2003).WecounselAI
on the same relevant occasion. As we explore in Section 3, such
researchersalsototakearelationalapproachtodiscourseactions.
interpretive effects enable AI researchers to approach coherence
OnelessonofourexperimentsinSection4isthatAIarchitectures
through a surprisingly diverse set of methodologies for data
should not assume that discourse actions stand on their own.
annotationandanalysis.
Instead,AIarchitecturesshouldlearnaboutdiscourseactionsvia
Imagery, like language, must be understood by inference,
theirlatentrelationtootherdiscourseactionsinthecontext.
and scholars have long argued that the interpretation of visual
Afterall,relationshipsvary.HerearevariantsofExample(2)
communicationalsoaimstoestablishcoherence.McCloud(1993),
wherethefollowupsentencesmakecontributionsofverydifferent
for example, argues that coherence relations between panels
kinds.
underpinstorytellingincomics.Abusch(2013)makesananalogy
betweenthepersistenceofindividualsacrosspanelsincomicsand
(3) Maxspiltabucketofwater.Hespiltitallovertherug.
thephenomenonofcoreferenceintextdiscourse.Cummingetal.
(4) Maxspiltabucketofwater.Johndroppedajarofcookies. (2017)showhowfilmviewersrelyoncoherencerelationstoinfer
the spatial relationships implicitly connecting the viewpoints of
InExample(3),wefindwhatHobbs(1985)callsExpansion.We successiveshots.Inallcases,viewersmustdrawinferencesabout
learn more about the initial event, its context and consequences. what objects imagery is depicting, where, and when—just as in
In Example (4), we find what Kehler (2002) calls Resemblance. they must draw inferences to understand linguistic content. In
The author presents another event as notable for its similarity all cases, they use coherence to do it. Koby Leff’s video essay
anddifferencetothefirst.Coherencetheoristshaveelegantways presentsaparticularlyclearvisualexplanationofthephenomenon;
to systematize the diversity of such relational interpretations in AlikhaniandStone(2018)analyzeacasestudyofcoherentvisual
a rational taxonomy. For Kehler (2002), for example, Examples communicationincomputationalterms.
(2–4)illustrateCause-Effect,Contiguity,andResemblancerelations Text communication and visual communication thus share
(respectively),eachamanifestationoftherelationalityandfitness common principles of coherence, but we see the overarching
of human thought. At the same time, AI research is increasingly role of coherence especially vividly when we consider the
successfulatstandardizingguidelinesforannotatorstoclassifythe coherentrelationshipsbetweentextandimagery.Authorsregularly
relationsfoundinspecificexamples. juxtapose text and imagery to present their ideas. When we
Strikingly,thesedifferentrelationsarenotencodeddirectlyin establish the coherence of such presentations, we can find that
the linguistic forms and structures that make up Examples (2– text relates to imagery; the text may describe how the imagery
4). Of course, the forms of referring expressions in 3 are the was obtained, what it shows, what its context is, or what its
FrontiersinArtificialIntelligence 03 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
implicationsare.Conversely,wecanusecoherentlinkstotextto inspiredbythesuccessofanalogousapproachestotextdiscourse,
establish what imagery depicts: perhaps a representative moment particularly the theoretical work of Asher and Lascarides (2003)
during the event described by the text, or perhaps a telling andtheempiricalworkofPrasadetal.(2008).
moment as the event got underway, or achieved its results. To start, we need a framework that systematically organizes
Such interpretations attest to the importance of the cross-modal coherence relations based on their implications for the structure,
coherence relations which we illustrated already with Figure1 in content, and purpose of the discourse. Asher and Lascarides
the introduction. Coherence relations can provide a framework (2003)introducesuchataxonomyofcoherencerelationsbetween
forwaysthatweposequestionsaboutcommonsenseinferencesin discourse segments, as part of their Segmented Discourse
image–textpresentations. RepresentationTheory,orSDRT.Thesimplestrelationsarebased
Infact,cross-modalcoherencerelationshavealonghistoryin on reference to shared entities: Examples include Expansion (as
theanalysisofface-to-facecommunication(Engle,2001;Lascarides in Example 3) when a second discourse unit amplifies and
and Stone, 2009a; Stone and Stojnic, 2015; Hunter et al., 2018; expands on what’s described in the first unit, and Narration (as
Hunter,2019),asaformaltooltooperationalizecognitivescientists’ in one interpretation of Example 5), when a second discourse
view that speakers use diverse modalities, including speech and segment describes an event that follows the one described in the
coverbal gesture, to present integrated messages (McNeill, 1992; firstsegment.
Bavelas and Chovil, 2000; Kendon, 2004). Image–text coherence SDRT also involves relations at proposition level, such as the
islesswellstudied(theworkofFeinerandMcKeown,1991,who Parallelrelationthatconnectstwodiscoursesegmentsthatexpress
used a taxonomy of cross-modal coherence relations to inform propositionsthatmakesimilarclaimsaboutsimilarentities(asin
theautomatedsynthesisofmultimodaldocuments,isaprescient oneinterpretationofExample5).
exception), but it provides new illustrations of the key principles Finally, SDRT includes relations that describe the intents
ofcoherence. and goals of the utterances—these are particularly important
ConsidertheimagesofFigure2forexample. in interactive relationships such as Correction and Clarification
Bothimagesareassociatedwiththesummary“lookingoutthe Requestthatconnectutterancesbydifferentspeakers.
window”butthetextgetstwoqualitativelydifferentinterpretations. To annotate these relations, researchers have mapped out
Atleftwehaveviewofacatthroughglass.It’sthecatthat’slooking structured, multifaceted, hierarchical annotation guidelines
out the window. This is a characteristic example of a coherence (Prasad et al., 2008; Rohde et al., 2018; Alikhani et al., 2019). In
relationwehavecalledVisible(Alikhanietal.,2020);aswereview general,specifyingcoherencerelationsfirstrequiresdecidinghow
inmoredetailinSection3,weattributemanyuniquefeaturesof discourse elements attach into an ongoing discourse structure;
commonimage–captioncorporainAIresearchtothedistinctive see especially Webber et al. (2003). For each discourse unit, we
inferentialcharacteroftextthatsuppliesVisibleinformation. needtodescribewhatotherunitsit’srelatedtobycoherentlinks.
AtrightofFigure2,meanwhile,wehaveaviewofawindow, Connecting a discourse segment to a related segment can create
framed to draw attention to the landscaped scene outside. The a sibling (coordination) or a child (subordination), generating a
image features no gazing subject. It’s the camera that’s looking hierarchical structure. Different discourse frameworks model the
out the window. This is an example of a different coherence structureofthediscoursedifferently.Someofthemonlycapture
relation we have called Meta (Alikhani et al., 2020); utterances shallowrelations,whileothers,likeSDRT,usemorecomplexand
often get their coherence through Meta-talk by characterizing hierarchical graphs.1 In extended multimodal presentations, like
related communicative actions rather than by amplifying the blog posts involving multiple images interleaved with extended
communicatedcontentofrelatedsegments(Hobbs,1985;Sanders textualdescriptions(AlikhaniandStone,2018)orcontributionsto
et al., 1992; Asher and Lascarides, 2003). Meta text doesn’t spoken conversation including multiple utterances and co-verbal
summarize the visual information in the image like a typical gestures performed in synchrony (Lascarides and Stone, 2009a),
caption would, but in some genres authors often supply Meta it’sroutineforeachcontributiontoattachbothtoasynchronous
texttoaccompanytheirimagery.Suchambiguitiesinimage–text contributionacrossmodalitiesandapreviouscontributioninthe
coherence mirror the ambiguities found in text–text coherence. samemodality.
Textthataccompaniesimagerycanprovidequalitativelydifferent Our work on image–text coherence relations highlights
kindsofinformationabouttheimageitamplifies;theremaybelittle cases whose discourse structure is relatively clear—for example,
surface-levelinformationthatrevealsthecontributionthattextis images together with their ALT-TEXT, an auxiliary record that is
making.Thus,thereisacrucialbutimplicitroleforcoherencein understoodasprovidingsubordinate,supplementaryinformation
interpretingimage–textpresentations,onethatweargueimpacts to accompany the image. In more complex presentations, the
thedata,models,tasks,andmetricsofmultimodalAIsystems. question of multimodal discourse structure is a challenging issue
ripeforfurtherresearch.
Given an attachment, researchers first decide which of the
top-level categories describes the relationship between the two
2.2. The methodology of coherence
segments(Prasadetal.,2008;Hunteretal.,2018).Wehavefound
Because of the ambiguity of coherence, annotated data is
indispensable for AI experiments, whether in training models of 1 Researchersintextdiscourseoftenfocusontreediscoursestructures
coherencebysupervisedlearningorinevaluatingthepredictions (Hobbs,1985;Kehler,2002)buttheanalysisofmultimodaldiscoursebenefits
of unsupervised methods. Our work on text–image coherence is fromappealingtomoreflexiblestructures(Hunteretal.,2018).
FrontiersinArtificialIntelligence 04 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
FIGURE2
Twodifferentwaysofinterpreting“lookingoutthewindow”asaspecificationofanimage.(Left)Visualinformationdescribingtheposeandactivity
ofthesubjectinaphotographofacatbyCristieGuevara(CCPublicDomain1.0viapublicdomainpictures.net).(Right)Meta-levelinformation
describingthecameraviewpointinaphotographofthePhoenixArtMuseumbyChanelWheeler(CCBY-SA2.0viaWikimediaCommons).
it helpful to give annotators the option of specifying multiple broaderperspectivesontherangeofpossiblecoherencerelationsin
top-level relations. Given a relation, we can then “drill down” multimodalcommunication.
to characterize the relationships in more detail. Webber et al. Despite the many open questions about the structure
(2019) describes the hierarchical decisions that annotators must and relations exhibited by coherent multimodal discourse,
make to resolve the coherence relations defined for the Penn AI researchers can nevertheless make practical progress with
DiscourseTreebank. coherence-based approaches. For text–image coherence, we have
Wecansometimesextendcoherenceannotationprotocolsto used a restricted list of relations to annotate data at scale
multimodaldiscoursebybuildingonrelationshipsthathavebeen (Figure3), analyse large multimodal datasets, design coherence-
studied in text. The Narration relation (Hobbs, 1985; Asher and awaremodelsandevaluateourframework.Ourtaxonomyincludes
Lascarides, 2003) is a case in point. In multimodal discourse, five relations—Visible, Action, Subjective, Story and Meta—any
two images are connected by the Narration relation when the subsetofwhichcandefinethecoherentuseoftexttoamplifyon
second image shows the subsequent event of what’s depicted imagecontent.
in the first image. We can even find Narration from text to TheVisiblerelationholdswhenthetextpresentsinformation
images and vice versa. An image can show what happened right that is depicted in the image. This is similar to the Restatement
after the text it elaborates, and text can report what happened relationsintext(Prasadetal.,2008)intext,butherethecontentof
right after the image it modifies. More generally, Narration is animageoverlapswiththecontentofitsaccompanyingtext.When
one of a family of coherence relations expressing contingent theimagedepictsamomentorasnapshotofanactiondescribed
temporalconnectionsthatlinkaneventtoitspreparatoryprocess, in text, the pairs are connected with Action relation. The Action
its culmination, or its result state. These relationships can be relationisanalogoustoElaborationrelationsdescribedinPrasad
found between when–clauses and main clauses in discourse etal.(2008)fortext.
(Moens and Steedman, 1988), between successive clauses in ThetextandimagearerelatedwiththeSubjectiverelationwhen
discourse (Webber, 1987), or between related text and imagery theoverlappinginformationdescribedintextsometimesincludes
(AlikhaniandStone,2018). anevaluativestatementorareactiontothecontentoftheimage.
In other cases, we need new relations to describe inferences ThisissimilartoEvaluationrelationsintext(Hobbs,1985).
between text and images. In postulating such relations, we can Similar to the Occasion relation of Hobbs (1985) that holds
drawvaluableinsightsfromresearchthatexploreshowlinguistic when a discourse unit provides the background for another
content can be related to other kinds of visual content. Engle discourse segment, sometimes the text provides a free-standing
(2001), for example, present a number of semantic relationships description of the occasion depicted in the image. We call this
between speech and coverbal gesture, showing not only that a Story coherence relation. Sometimes the text goes beyond just
gesture has a characteristic ability to relate to accompanying providing information about what’s depicted in the image or the
speechbyDepiction,butthatsuchrelationscombinewithfamiliar occasion. It describes how, when, or where the image was taken
relationsfromtextualdiscoursesuchasExemplification.Stoneand byexplainingthepresentationandproductionprocedures.Insuch
Stojnic(2015),meanwhile,studyhowphysicaldemonstrationsare cases,wearguethetextandtheimageareconnectedwithaMeta
connected to speech and gesture, and appeal to relations such coherencerelation.
asSummary—whereanutteranceisunderstoodtocharacterizea
visiblesituation—andCompliance—whereaneventisunderstood
to meet an expectation for action established by a previous 2.3. Why coherence?
utterance. As our empirical results in Section 3 make clear,
presentationswithdifferentpurposesandgenresnaturallyfeature Thecommunicativefunctionsofimagesandtextinmultimodal
distinctiverelations,soweshouldexpectfutureresearchtoleadto communicationcanbeanalyzedfromseveraldifferentperspectives.
FrontiersinArtificialIntelligence 05 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
FIGURE3
Captionsfromlefttoright:Amansittinginfrontofabunchoffruits.Awomanistravelingonatrain.Thenewmanageroftheteam.Theviewfrom
thebridge.Textandimagesarelinkedtogetherusingaconstrainedsetofcoherencerelations,whichcansummarizethestructural,logicaland
purposefulrelationshipsbetweenthecontributionsoftextandthecontributionsofimages.ExamplesfromtheConceptualCaptionsdataset
(Sharma,2020)thatincludeCreativeCommonsLicensedimage–textpairs.
For example, Kruk et al. (2019) and Shuster et al. (2019) focus support data creation and model designs that we describe in the
ontheemotionsevokedbypresentations,Guoetal.(2019)study nextsection.
genreandstyle,andOttoetal.(2019)andVempalaandPreo¸tiuc-
Pietro(2019)assesshowtextandimagesmightbecomplementary
3. Coherence as a framework for
orredundant.
analyzing image–text datasets
The main contrast with our approach is that none of these
frameworks try to model information-level inferences between
text and images. An image might be a uniquely effective way to Ourfirstargumentfortheutilityofthecoherenceframework
prompt emotion for example, but it would be surprising if our comes in characterizing AI datasets. Coherence relations
cognitive mechanisms could resolve ambiguity in the image (or can provide information about the inferential and linguistic
in the accompanying text) to foster such affective engagement. characteristics of image–text corpora. In particular, coherence
Similarly, regardless of how we resolve their ambiguities, we will shedslightonhowgenresvaryandhowlinguisticformislikelyto
beabletoclassifyrelatedtextandimageryaseithercomplementary changeacrossdatasets.Inadditiontocharacterizingthechallenges
orredundant. ofimage–textinference,suchresultscanalsoinformAIresearch
In text discourse, information structure is an alternative byhelpingtodefinethedomainadaptationthatwillbenecessaryto
to coherence theory that provides yet another perspective to generalizemachinelearningresultsfromoneimage–textcollection
relate meaning to communicative goals. Information structure tootheres.
is a dimension of pragmatic meaning in language that helps In this section, we first describe the Clue dataset, which is
explain variation in word order, intonation, and other linguistic thelargestimage-textdatasetannotatedwithcoherencerelations.
cues that mark the relationship between utterances and their Then we discuss empirical studies that support the importance
context (van Kuppevelt, 1995; Roberts, 2012). A key construct ofcoherencerelationsandtheirassociatedlinguisticconstructsin
in theories of information structure is the “question under supportingcommonsenseinference.Finally,wediscussresultson
discussion,” or QUD; utterances relate to the context in part by the correlations of coherence relations and genre, and the use of
addressing the QUD. Information structure describes how an coherence relations to diagnose mismatches between image–text
utterance is partitioned into material that evokes the question corporaandmachinelearningmodels.
under discussion and the material that supplies the answer.
Although information structure is signaled grammatically, the
point of emphasis of the speaker often has to be inferred 3.1. Clue: A dataset of image–text
(Bolinger,1972).Thismakesinformationstructureandcoherence coherence relations
complementary: in particular, Hunter and Abrusán (2017) argue
that coherence provides a flexible and perspicuous way of Alikhanietal.(2020)presentedCluethatprovidesaprotocol
mapping the inferences involved in disambiguating information forannotatingcoherencerelationsintextandimagery.Theyused
structure and reconstructing the QUD. Coherence is especially the protocol to annotate from the 10,000 image-text pairs from
natural when we consdier how approaches should generalize to the Conceptual Captions (Sharma et al., 2018) and Open Images
multimodal discourse, because there isn’t anything analogous to (Kuznetsova et al., 2020) datasets. Half of these pairs include
information structure in a photograph, map, or diagram. Even captions generated by models and half of them are captions that
the emphasis you find in gesture is very different structurally users have written for the images. Guidelines are described in
and compositionally from information structure in language detailsinAlikhanietal.(2020).
(Kendon,2004).NeitherQUDnortheframeworksthatprioritize Figure4 shows the statistics over the resulting annotations
ways that images serve complementary roles for text would presented in Alikhani et al. (2020). We can see that models
give insights into inferences that connect the content of text struggle to generate subjective or story-like captions. The rate of
and imagery. They do not provide a framework that can captions and images with the Meta relation however is higher
FrontiersinArtificialIntelligence 06 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
FIGURE4
Thedistributionofcoherencerelationsinourdataset.
in text generated by models that signals the potential context 3.3. Coherence relations indicate Genre
hallucinationproblem.
AImodelsdonot,bydefault,exhibitthebehaviorofthetexts
that they are trained on. As we can see in Figure4, machine-
generated text includes more captions with the meta relation in
3.2. Coherence predicts linguistic form
comparisonwithtextwrittenbyhumans.Thisisoneofthemain
sources of the hallucination problem when the model includes
Alikhani and Stone (2019) present an empirical investigation
information in generated text that may not necessarily be true.
and argue that we can learn new perspectives on commonsense
Informationaboutthebackgroundorcontextoftheimagethatis
inference by correlating coherence relations with linguistic
notdepictedintheimage.Thecoherenceframeworkcanhelpus
constructs. In particular, they report that visible descriptions are
identify,characterizeandaddresstheseissues.
very distinctive. They only describe what’s depicted in the image
Coherence relations indicate discourse goals. Figure5 shows
inarestrictedway.
that the labels that our dataset presents correlate with the genre
Theyobservedthattherateofcaptionsthatdescribeongoing
underwhichthecaptionshavebeenproduced,whichmeansthat
events(atelicevents)isdrasticallyhigherthantherateofcaptions
textandimagesfromdifferenttypesofpublicationshavedifferent
that describe events with end points (telic events). This is the
distributions of relations. For example, captions from a news
difference between arriving at an event (telic) and standing
website such as daily mail are more story-like, whereas Getty
somewhere (atelic). stative descriptions are also very common in
ImagespostsoftenincludeVisiblecaptions.2
captions. Many captions describe quality, condition or the state
of what’s depicted in the image. Examples include the kids are
happy or green bananas are on the table. Alikhani and Stone 4. Using coherence to critique
(2019) argue that these captions are connected to images by
image–text models
the illustration or visible relation. They study the following
datasets with different types of textual descriptions with images:
In the previous Section, we discussed how coherence
(1) Google’s Conceptual Captions (CC) (Sharma et al., 2018) (2)
defines what linguistic forms and inferences go into text–image
Flickr30K (Flickr) (Young et al., 2014) (3) Visual Storytelling
interpretation.Wealsodiscussedhowtheframeworkcouldidentify
(VIST) (Huang et al., 2016) (4) the Recipe dataset (Yagcioglu
weaknesses in machine learning models trained on image-text
etal.,2018).WhileFlickrandCOCOarecaptainingcorpora,VIST
corpora.InthisSection,westudyhowdifferentmodelarchitectures
includesstory-likedescriptionsforconnectingfiveimagesandCC
andtrainingmechanismscancapturecoherencetovariousdegrees.
pairs web images with relevant text from associated ALT-TEXT
We first present the details of our computational experiments
HTMLattributes.
then move on to describe the results and discussions. Our
Theystudied1,000randomimage-textpairsfromeachofthe
computationalinvestigationsrevealthatlargemultimodalmodels
discusseddatasets.Over94%ofthetextincaptioncorporasuchas
cannotaccuratelyrepresentcoherencerelations.Theyfailtoreason
FlickerandCOCOincludeatelicevents.However,theratedrops
to40%inotherimage-textcorpora,suchasthemultimodalrecipe
dataset. Alikhani and Stone (2019) includes the experiments and 2 GettyImages:http://www.overleaf.com.
statisticsdetails. DailyMail:https://www.dailymail.co.uk/ushome/index.html.
FrontiersinArtificialIntelligence 07 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
F1metricasitisawell-knownmetricformeasuringaclassification
modelperformance.
Formally, speaking, we define a function fθ as a pre-trained
p
model which maps an input pair (xi,vi) to a vector hi in
a d dimensional representation space Rd, where θ p represents
the pre-trained weights, xi and vi represent the text sample
and its corresponding image respectively. To check whether the
representation hi has a certain information, we can define a
linearprobef
lin
whichtakeshi asinputandoutputsaprobability
distributionP(li|hi)overagivensetofnlabelsli∈L.Bytuningthe
weightsassociatedwiththelinearprobeW,wecanlearnwhether
therepresentationhitoidentifydifferentclassesli ∈L.
P(li|hi)=f lin(fθ p(xi,vi))
f lin(hi)=σ(Whi+b)
FIGURE5 (1)
D (Ai lff ike hr ae nn it ew te ab l.s ,i 2te 0s 20h )a .vedifferenttypesofimage–captionpairs L
i
= n1 X−log(P(lj|hi))
lj∈L
Where L i is the cross entropy loss signal associated with the
flexiblyaboutthelinksthatconnecttext–imagepairsinthesame image-textpairsandtheircorrespondinglabels,andσ represents
waywedescribedinourdata. thesigmoidfunction.Sincetheparametersθ p associatedwiththe
pre-trainedmodelarefrozen,thelinearprobecanonlymakeuseof
informationencodedintherepresentationhi.
4.1. Experiment structure
To discern if the representations hi encode the discourse
relationinformation,wecompareprobesfortheabovementioned
models with the probes trained using representations from
Inthissection,wedescribeourexperimentalsetup,theproblem
ResNet and BERT. In addition, we also fine-tune the pre-
formulationandthengoontodescribetheinsightswedrawfrom trained model weights θ to observe the improvements when the
ourresults.
representationshi arealsofine-tuned.Ourhypothesisisthatpre-
We investigate whether the two most recent and successful
trainedrepresentationscontainingsignalsforidentifyingdiscourse
versions of these models, VilBERT (Lu et al., 2019) and CLIP
relations should outperform the baselines and show competitive
(Radford et al., 2021) can implicitly learn coherence relations in
performance when compared to the models obtained using fine-
image–textpresentationsduringthepre-trainingprocess.
tunedrepresentations.
VilBERT is a transfomer based model pre-trained on the
Conceptual Captions dataset. It is inspired from the BERT
architecture and takes as input a sequence of image blocks and
4.1.2.Experimentparameters
text tokens belonging to the image caption. Image blocks and
AsdescribedintheSection4.2,wefine-tunethemodelsintwo
textsequenceareseparatedbyaspecialtoken.Itistrainedusing
ways.Whenthepre-trainedweightsarefrozenandonlytheprobe
twoproxytasks:maskedmulti-modalmodelingandmulti-modal
weights are trained, we rely on a batch size of 64 and a learning
alignmentpredictionandthenthelearnedweights.Itreliesonthe
rate of 5 ∗ 10−3. However, when the we run the experiments to
self-attentionmechanismoftransformerarchitecturetolearnthe
fine-tunethepre-trainedweightswerelyonabatchsizeof8anda
relationsbetweendifferentpartsofanimageandthecorresponding
learningrateof10−5.Inbothcases,theweightsareoptimizedusing
textualtokens.
CLIP is trained using contrastive learning to maximize theAdamoptimizerandasmallregularizationweightof10−5 is
utilizedtopreventmodelfromoverfittingthedata.
similaritytextandimagepairs.UnlikeVilBERT,ithastwoseparate
encoders for the text and visual data. Visual encoder is either
a variation of ResNet architecture or a visual transformer while
the text encoder is based on the transformer architecture. A dot 4.2. Results
product between the text and image representations is used to
computethesimilarityandthenlossiscomputedusingthebinary
To conduct our experiments we split the 5760 image-text
crossentropyobjective.
pairs into train, validation and test ratios of 0.85, 0.075, 0.075
respectively. The idea of our evaluation setup is to investigate
whether the representations learned by the pre-trained models
4.1.1.Problemformulation encode the discourse relations between image-text pairs. We do
The problem is structured in the form of a classification task so by comparing the performance of fine-tuned models with
where goal of the model is to classify the representation hi as a their respective linear probes. If the fine-tuned models perform
coherencerelation.Wemeasurethemodelperformanceusingthe betterthantheirrespectivelinearprobesitshowsthatpre-trained
FrontiersinArtificialIntelligence 08 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
TABLE1 ThistableshowstheF1scoreaveragesandtheirbreakdownsobtainedbythelinearprobesfordifferentvisual-linguisticarchitectures.
Metrics
Macroaverage Microaverage
Model F1 Precision Recall F1 Precision Recall
CLIP 0.466 0.468 0.571 0.762 0.744 0.781
VilBERTVil 0.582 0.545 0.712 0.780 0.758 0.804
VilBERTLinguistic 0.635 0.601 0.713 0.808 0.796 0.820
Resnet 0.333 0.337 0.345 0.682 0.617 0.763
BERT 0.623 0.590 0.776 0.798 0.791 0.804
BERT+Resnet 0.637 0.614 0.712 0.794 0.798 0.789
Macroaveragegivesequalweightstothemodelperformanceineachclassandhencerepresentstheeffectofimbalancewhilemicroaveragerepresentstheoverallaverageperformance.
Thedifferenceinperformancehighlightsthatrepresentationslearnedbysomemodelarchitecturearebettersuitedforthepurposeofidentifyingdiscourserelations;VilBERTLinguistic
representationsinthiscase.
FIGURE6
ThisfigurehighlightsthechangeinmacroF1scoreswhenthemodelweightsarefine-tunedduringthetrainingprocess.Inallcases,the
performanceforthefine-tunedmodelsimprovesbyasignificantmarginreflectingthatpre-trainedrepresentationsdonotencodeinformation
necessarytoidentifydiscourserelations.
representations lack key information which would have allowed architectureincludingvisual-linguisticvariationofVilBERT.This
themtoidentifycoherencerelationsandviceversa. couldbeasignalthatarchitectureswhichtrytolearnfine-grained
Asstatedearlierwebuildprobesfortwopre-trainedmodels— relations between image-text pairs are more suited to learning
CLIP and Vilbert. For the pre-trained Vilbert weights, we discourserelations.
build two probes: for the visual-linguistic representations and When compared with the baseline probes, specifically BERT
linguisticrepresentationsoutputtedbythemodel.Visual-linguistic probe, the performance of the best pre-trained visual-linguisitic
representationsareobtainedbycombiningthevisualandlinguistic probes is similar in terms of the F1 score achieved as shown in
representations, and linguistic representations use co-attention theTable1.Thishighlightsthatpre-trainedmodelsareunableto
mechanism with image representations to represent relations make use of the visual information in a meaningful manner to
betweenimageandtextpairs.FortheCLIPmodel,weconcatenate successfully encode the relations between the image-text pairs in
thevisualandtextrepresentationstoobtaintherepresentationfor thehigherdimensionalembeddingspace.
image-textpair. When the pre-trained model weights are fine-tuned we see a
We present the linear probe performance based on the F1 significantincreaseinthemacroF1scoreasshownintheFigure6.
average scores (macro and micro) and their breakdown in the Even though the best performance after fine-tuning is achieved
Table1. These results highlight the capability of representations by the VilBERTLinguistic model, it only lags behind the BERT
learned by different architectures in encoding information + Resnet performance by a slight margin of 3%. This provides
necessary to identify discourse relations. Our results show that evidence that pre-training with large corpora of image-text pairs
the probe for linguistic representations learned by the VilBERT doesnotimplicitlyallowmodelslikeVilBERTandCLIPtoencode
usingco-attentionwithimagerepresentations(VilBERTLinguistic) discourse relations in the higher dimensional embedding space
showsbetterperformancewhencomparedtotheprobesforother andcallsfortheexplorationoftechniqueswhichexplicitlyutilize
FrontiersinArtificialIntelligence 09 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
discourse relations to learn models whose predictions are in-line modelgeneratescoherence-awaredescriptionsfortheseimagesin
withhumanjudgements. differentcoherenceclasses.Then,theyaskedtheannotatorstowrite
Visible captions for 1,000 images from the OpenImages dataset
(Kuznetsovaetal.,2020)andcalledthedatasetCOIN(Corpusof
5. Using coherence to define and
OpenImageswithNaturaldescriptions).
evaluate AI systems The proposed approach has two versions—a baseline Vanilla
version and a ViLBERT-based model. Both are trained on
RaCCoontrainingdatawithnormalizedhumanannotatedrating
Inthissection,wediscusswaystousethedatasetandmodelswe
toobtainthemodel’stargetscore.Detailsofthedatasetandablation
discussedintheprevioussectionsindesigningmultimodalsystems.
studies are available here (Inan et al., 2021). Table2 presents the
Previous works have shown the utility of using the coherence
resultsoftheCOIN-basedstudy.ThelastrowshowstheKendall
framework in design image retrieval models (Alikhani et al.,
correlation coefficient between the scores assigned by users and
2022),captiongenerationmodels(Alikhanietal.,2020),automatic
the metric. The N-gram based metrics cannot adapt to the out-
evaluation(Inanetal.,2021),anddiagramunderstanding(Hiippala
of-domainground-truthcaptionsfromCOINwhichresultsinlow
etal.,2021).
correlationcoefficients.TheCIDErscoreshavenegativecorrelation
Can a coherence-aware model present information that is
coefficients which indicate negative association with user ratings.
aligned with the goal of the discourse? Can a coherence-aware
BLEURTandBERTScoredoamuchbetterjobincomparisonwith
model significantly improve caption quality? Can we design
CIDErandN-grambasedmetricsbuttheyarestillagnostictothe
automaticlearnedgenerationmetricsthatcanevaluatetheoutput
coherencerelationlabel.Ourproposedmodelwhichiscoherence-
ofcoherence-awaregenerationmodels?
awarehasthehighestcorrelationscoreswithuserjudgments.
5.1. Generating coherent captions
6. Conclusion
Can we design controllable image description generation
When authors combine text and imagery, they use the
models that can generate captions with respect to different
different modalities of communication in concert: common
coherence relations? Alikhani et al. (2020) introduced such
principles of coherence relate communicative actions together,
controllablemodelusingtheCluedataset.TheyusedTransformer
guideinterpretiveinferences,andresolveambiguities.Inthispaper,
Networks(Vaswanietal.,2017)anddesignedagenerationmodel
we have described how the well-known theory of coherence in
that can output captions using a sequence-generation approach.
textdiscourseextendstoimage–textpresentationsandcanguide
The result is a sequence of sub-tokens that create the desired
AI research on mulitmodal communication. While we have thus
caption.Theinputincludesdifferentimagesfeaturesandthetarget
far offered a range of findings to show the potential benefits of
coherencerelationlabel.Therelationlabelisthestarttokenofthe
coherenceinmultimodalAI,weareoptimisticforfurtherresearch
Transformerdecoder.
progressinalloftheseareas.
The proposed model is able to reduce noise by around 30%
In particular, we have seen that coherence relations offer
from the generated captions overall. It includes substantially
importanttoolstoanalyzedatasets;coherencerelationscanallow
fewer irrelevant captions, and it can respect the discourse goals
ustoquantifydifferencesinlanguageuseacrossdifferentcorpora
by generating captions connected to the image by the desired
and even to explain the distribution of linguistic phenomena in
coherence relations. The success rates of the model when it was
corpora as a function of the distinctive character of coherence
askedtogeneratevisible,meta,storyandsubjectivecaptionsware
in corpora. Work on taxonomies of coherence relations for
respectively79.85%,46.49%,58.8%and45.00%.Thedetailscanbe
multimodal discourse is in its infancy. New genres and tasks
foundinAlikhanietal.(2020).
could highlight the importance of additional relations or further
distinctionsinhowtextrelatestoimagery.Conversely,wehavesaid
littleaboutimagerythatgetsitscoherencefromaccompanyingtext.
5.2. Coherence-aware learned evaluation
Alikhanietal.(2019)annotatestheinferencesthatgroundimagery
metrics in a specific domain through particular temporal, spatial, and
logicalconnections,ratherthanthroughatraditionaltaxonomyof
As we observed in the previous section, image captioning coherencerelations.Itisanopenquestionwhethercoherencecan
metrics have struggled to give accurate learned estimates of the be systematized more generally across images and text. Another
semantic andpragmatic successof outputtext. Inan etal.(2021) challengeisaccountingforthestructureofmultimodaldiscourse,
proposed the first discourse-aware learned metric for evaluating particularly for presentations that involve relations within and
such properties in image descriptions. The goal of the metric is across modalities. Multimodal analyses of situated conversations
tooutputascorethatreflectsthequalityofthegeneratedcaption have revealed many complexities (Lascarides and Stone, 2009b;
giventheimage,coherencerelationandthereferencecaption.In Hunteretal.,2018).
whatfollowswereviewtheproposedmodelandresults. Wehavealsoseenthatcoherencerelationsalsoofferavaluable
Theyworkedwith1,000imagetextpairsfromtheConceptual lenstocritiqueandimprovethearchitectureofmachinelearning
Captions (CC) training dataset (Ng et al., 2020) and collected models.Modelsthatbuildinanassumptionthattextandimagery
ratings for them. They use the cc imges as inputs to caption relate in simple, uniform ways, are less effective in capturing
generation model presented by Alikhani et al. (2020). The coherence than models that allow for more flexibility. Research
FrontiersinArtificialIntelligence 10 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
in large vision–language models, however, has overwhelmingly
designed around capturing the relationships between image
content and Visible text. Relaxing this assumption offers exciting
prospects for learning more powerful representations of the
meaningsofimage–textpresentations.
Finally,wehaveofferedanexampleofcoherence-awaretasks
andevaluationmetrics.SincecurrentAItechnologystruggleswith
coherence,weneedtotakecoherenceintoaccountfromthestart
as we design and test AI systems. AI researchers still face many
challenges in extending information and interaction tasks from
their origins in text processing to multimodal communication.
All of these domains, we believe, offer fruitful settings to pursue
coherence-awaremethodologies.
Data availability statement
Theoriginalcontributionspresentedinthestudyareincluded
in the article/supplementary material, further inquiries can be
directedtothecorrespondingauthor.
Ethics statement
The studies involving human participants were reviewed
and approved by Institutional Review Board, Rutgers
University. The patients/participants provided their written
informed consent to participate in this study. Written
informed consent was obtained from the individual(s), and
minor(s)’ legal guardian/next of kin, for the publication
of any potentially identifiable images or data included in
thisarticle.
Author contributions
MA led data annotation, empirical analysis, and evaluation.
BK led model training and implementation. MS led theoretical
analysis,modeldesign,andexplanation.Allauthorscontributedto
thearticleandapprovedthesubmittedversion.
Funding
The research presented in this paper has been supported
by NSF awards IIS-1526723, IIS-1703883, CCF-1934924, IIS-
1955404, IIS-1955365, DGE-2021628, RETTL-2119265, and
EAGER-2122119 and by Air Force Research Lab Contract
FA8650-22-P-6415.
Acknowledgments
Thanks to Mert Inan, Piyush Sharma, Radu Soricut,
Shengjie Li, Gerard de Melo, Sreyasi Nag Chowdhury,
Fangda Han, Hareesh Ravi, Mubbasir Kapadia, and Vladimir
Pavlovic for their contributions to the Clue, CITE, COSMic,
and the coherence-aware image retrieval models and
datasets, and to the reviewers and editors of the special
FrontiersinArtificialIntelligence 11 frontiersin.org
.)1202,.latenanI(scirtemgninoitpactnereffiddnasresuybdetaulavesasledomgninoitpacegamitnereffidrofserocS
2ELBAT
scirteM
metsyS
ciMSOC
ciMSOC
ciMSOC
ciMSOC
F-SB
RB
S
C
LR
M
2B
1B
.muH.gvA
lebaL.hoC
ledoM
+TREBV
+allinaV
TREBV
allinaV
gnitaR
146.0
225.0
697.0
607.0
368.0
778.0–
030.0
290.0
061.0
940.0
770.0
361.0
191.2
elbisiV
DTUB
416.0
615.0
777.0
696.0
268.0
411.1–
200.0
020.0
660.0
910.0
520.0
050.0
235.03
elbisiV
206.0
505.0
727.0
845.0
368.0
950.1–
000.0
210.0
360.0
210.0
000.0
140.0
312.3
ateM
esaB
304.0
853.0
124.0
323.0
948.0
791.1–
000.0
710.0
750.0
110.0
210.0
330.0
038.2
.jbuS
725.0
284.0
926.0
335.0
248.0
403.1–
000.0
310.0
850.0
710.0
000.0
920.0
519.2
yrotS
406.0
515.0
487.0
486.0
368.0
101.1–
000.0
110.0
350.0
310.0
110.0
820.0
892.3
elbisiV
565.0
115.0
847.0
845.0
958.0
480.1–
000.0
510.0
550.0
800.0
010.0
620.0
038.2
ateM
etiL
914.0
973.0
154.0
463.0
948.0
712.1–
300.0
420.0
660.0
910.0
210.0
930.0
892.2
.jbuS
915.0
994.0
666.0
865.0
248.0
263.1–
000.0
120.0
260.0
810.0
000.0
630.0
624.2
yrotS
467.0
766.0
645.0
175.0
544.0
682.0
–
–
–
630.0
451.0
170.0
000.1
)τ(noitalerroC
s’lladneK
250.0
175.0
630.0
.stnemgdujnamuhhtiwserocsnoitalerroctsehgihehtsahledom+TREBLiVciMSOCehT
Alikhanietal. 10.3389/frai.2023.1048874
issue for their valuable comments on previous versions of Publisher’s note
thepaper.
All claims expressed in this article are solely those of the
Conflict of interest authors and do not necessarily represent those of their affiliated
organizations, or those of the publisher, the editors and the
The authors declare that the research was conducted in the reviewers. Any product that may be evaluated in this article, or
absenceofanycommercialorfinancialrelationshipsthatcouldbe claimthatmaybemadebyitsmanufacturer,isnotguaranteedor
construedasapotentialconflictofinterest. endorsedbythepublisher.
References
Abusch,D.(2013).“Applyingdiscoursesemanticsandpragmaticstoco-referencein Huang,T.-H.K.,Ferraro,F.,Mostafazadeh,N.,Misra,I.,Agrawal,A.,Devlin,J.,
picturesequences,”inProceedingsofSinnundBedeutung17,edsE.Chemla,V.Homer, etal.(2016).“Visualstorytelling,”inProceedingsofthe2016ConferenceoftheNorth
andG.Winterstein(Paris),9–25. AmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguage
Technologies,1233–1239.
Alikhani, M., Chowdhury, S. N., de Melo, G., and Stone, M. (2019).
“Cite: a corpus of image-text discourse relations,” in Proceedings of the 2019 Hunter,J.(2019).Relatinggesturetospeech:reflectionsontheroleofconditional
Conference of the North American Chapter of the Association for Computational presuppositions.Linguist.Philos.42,317–332.doi:10.1007/s10988-018-9244-0
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),
Hunter, J., and Abrusán, M. (2017). “Rhetorical relations and quds.” in New
570–575.
FrontiersinArtificialIntelligence:JSAI-isAIWorkshopsLENLS,JURISIN,KCSD,LLLL
Alikhani,M.,Han,F.,Ravi,H.,Kapadia,M.,Pavlovic,V.,andStone,M.(2022). RevisedSelectedPapers(Springer),371–380.
“Cross-modalcoherencefortext-to-imageretrieval,”inProceedingsofthe36thAAAI
Hunter,J.,Asher,N.,andLascarides,A.(2018).Aformalsemanticsforsituated
ConferenceonArtificialIntelligence,Vol.10,10427–10435.
conversation.SemantPragmat.doi:10.3765/sp.11.10
Alikhani,M.,Sharma,P.,Li,S.,Soricut,R.,andStone,M.(2020).“Cross-modal
Inan,M.,Sharma,P.,Khalid,B.,Soricut,R.,Stone,M.,andAlikhani,M.(2021).
coherencemodelingforcaptiongeneration,”inProceedingsofthe58thAnnualMeeting
“COSMic:acoherence-awaregenerationmetricforimagedescriptions,”inFindingsof
of the Association for Computational Linguistics (Association for Computational
theAssociationforComputationalLinguistics:EMNLP2021,3419–3430.
Linguistics),6525–6535.
Kehler,A.(2002).Coherence,Reference,andtheTheoryofGrammar.Stanford,CA:
Alikhani,M.,andStone,M.(2018).“Exploringcoherenceinvisualexplanations,”
CSLIPublications.
in2018IEEEConferenceonMultimediaInformationProcessingandRetrieval(MIPR)
(Miami,FL:IEEE),272–277. Kendon,A.(2004).Gesture:VisibleActionasUtterance.Cambridge:Cambridge
UniversityPress.
Alikhani,M.,andStone,M.(2019).““caption”asacoherencerelation:evidence
andimplications,”inProceedingsoftheSecondWorkshoponShortcomingsinVision Kruk, J., Lubin, J., Sikka, K., Lin, X., Jurafsky, D., and Divakaran, A. (2019).
andLanguage(Minneapolis,MN:AssociationforComputationalLinguistics),58–67. “Integratingtextandimage:DeterminingmultimodaldocumentintentinInstagram
posts,” in Proceedings of the 2019 Conference on Empirical Methods in Natural
Asher,N.,andLascarides,A.(2003).LogicsofConversation.Cambridge:Cambridge
LanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguage
UniversityPress.
Processing(EMNLP-IJCNLP)(HongKong:AssociationforComputationalLinguistics),
Bavelas, J. B., and Chovil, N. (2000). Visible acts of meaning: an integrated 4622–4632.
messagemodeloflanguageinface-to-facedialogue.J.Lang.Soc.Psychol.19,163–194.
Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J.,
doi:10.1177/0261927X00019002001
et al. (2020). The open images dataset v4. Int. J. Comput. Vis. 128, 1956–1981.
Bolinger,D.(1972).Accentispredictable(ifyou’reamind-reader).Language.48, doi:10.1007/s11263-020-01316-z
633–644.doi:10.2307/412039
Lascarides, A., and Stone, M. (2009a). Discourse coherence and gesture
Chowdhury, S. N., Tandon, N., and Weikum, G. (2019). Know2look: interpretation.Gesture9,147–180.doi:10.1075/gest.9.2.01las
commonsense knowledge for visual search. arXiv preprint arXiv:1909.00749.
Lascarides,A.,andStone,M.(2009b).Aformalsemanticanalysisofgesture.J.
doi:10.48550/arXiv.1909.00749
Semant.26,393–449.doi:10.1093/jos/ffp004
Cohn, N. (2013). Visual narrative structure. Cogn. Sci. 37, 413–452.
Lin,T.-Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ramanan,D.,etal.(2014).
doi:10.1111/cogs.12016
“Microsoftcoco:commonobjectsincontext,”inEuropeanConferenceonComputer
Cumming, S., Greenberg, G., and Kelly, R. (2017). Conventions of viewpoint Vision(Springer),740–755.
coherenceinfilm.Philos.Imprint17,1–29.
Lu,J.,Batra,D.,Parikh,D.,andLee,S.(2019).“Vilbert:pretrainingtask-agnostic
Engle,R.A.(2001).TowardaTheoryofMultimodalCommunication:Combining visiolinguisticrepresentationsforvision-and-languagetasks,”inProceedingsofthe33rd
Speech,Gestures,DiagramsandDemonstrationsinInstructionalExplanations(Ph.D. InternationalConferenceonNeuralInformationProcessingSystems,13–23.
thesis).StanfordUniversity.
McCloud,S.(1993).UnderstandingComics:TheInvisibleArt.WilliamMorrow.
Feiner, S. K., and McKeown, K. R. (1991). Automating the generation of
McNeill,D.(1992).HandandMind:WhatGesturesRevealAboutThought.Chicago:
coordinated multimedia explanations. IEEE Comput. 24, 33–41. doi: 10.1109/2.
UniversityofChicagoPress.
97249
Funaki, R., and Nakayama, H. (2015). “Image-mediated learning for zero-shot Moens,M.,andSteedman,M.(1988).Temporalontologyandtemporalreference.
cross-lingualdocumentretrieval,”inProceedingsofthe2015ConferenceonEmpirical Comput.Linguist.14,15–28.
MethodsinNaturalLanguageProcessing,585–590. Ng,E.G., Pang,B., Sharma,P., andSoricut, R. (2020).Understanding guided
Guo, L., Liu, J., Yao, P., Li, J., and Lu, H. (2019). “Mscap: multi-style image captioning performance across domains. arXiv preprint arXiv:2012.02339.
image captioning with unpaired stylized text,” in Proceedings of the IEEE doi:10.18653/v1/2021.conll-1.14
Conference on Computer Vision and Pattern Recognition (Long Beach, CA: IEEE),
Otto, C., Springstein, M., Anand, A., and Ewerth, R. (2019). “Understanding,
4204–4213.
categorizingandpredictingsemanticimage-textrelations,”inProceedingsofthe2019
Hiippala,T.,Alikhani,M.,Haverinen,J.,Kalliokoski,T.,Logacheva,E.,Orekhova, onInternationalConferenceonMultimediaRetrieval(ACM),168–176.
S.,etal.(2021).Ai2d-rst:amultimodalcorpusof1000primaryschoolsciencediagrams.
Phillips,B.(1977).“Acalculusofcohesion,”inFourthLACUSForum(Montreal,
Lang.Resour.Evaluat.55,661–688.doi:10.1007/s10579-020-09517-1
QC),627–637.
Hobbs, J. R. (1979). Coherence and coreference. Cogn. Sci. 3, 67–90.
Prasad,R.,Dinesh,N.,Lee,A.,Miltsakaki,E.,Robaldo,L.,Joshi,A.,etal.(2008).
doi:10.1207/s15516709cog0301_4
“ThepenndiscourseTreeBank2.0,”inProceedingsoftheSixthInternationalConference
Hobbs,J.R.(1985).Onthecoherenceandstructureofdiscourse.Technicalreport, onLanguageResourcesandEvaluation(LREC’08)(Marrakech:EuropeanLanguage
CenterfortheStudyofLanguageandInformation,StanfordUniversity. ResourcesAssociation,ELRA).
FrontiersinArtificialIntelligence 12 frontiersin.org
Alikhanietal. 10.3389/frai.2023.1048874
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., et al. Smyth,R.(1994).Grammaticaldeterminantsofambiguouspronounresolution.J.
(2021).“Learningtransferablevisualmodelsfromnaturallanguagesupervision,”in Psycholinguist.Res.23,197–229.doi:10.1007/BF02139085
Proceedingsofthe38thInternationalConferenceonMachineLearning,ICML2021,18-
Stone,M.,andStojnic,U.(2015).Meaninganddemonstration.Rev.Philos.Psychol.
24July2021,VirtualEvent,volume139ofProceedingsofMachineLearningResearch,
6,69–97.doi:10.1007/s13164-014-0213-4
edsM.MeilaandT.Zhang(PMLR),8748–8763.
vanKuppevelt,J.(1995).Discoursestructure,topicalityandquestioning.J.Linguist.
Ramesh,A.,Pavlov,M.,Goh,G.,Gray,S.,Voss,C.,Radford,A.,etal.(2021).“Zero-
31,109–147.doi:10.1017/S002222670000058X
shottext-to-imagegeneration,”inProceedingsofthe38thInternationalConference
on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,etal.
Proceedings of Machine Learning Research, eds M. Meila and T. Zhang (PMLR), (2017). “Attention is all you need,” in Advances in Neural Information Processing
8821–8831. Systems,5998–6008.
Roberts,C.(2012).Informationstructure:towardsanintegratedformaltheoryof Vempala, A., and Preo¸tiuc-Pietro, D. (2019). “Categorizing and inferring the
pragmatics.SemantPragmat.5,6–1.doi:10.3765/sp.5.6 relationshipbetweenthetextandimageofTwitterposts,”inProceedingsofthe57th
Rohde, H., Johnson, A., Schneider, N., and Webber, B. (2018). “Discourse AnnualMeetingoftheAssociationforComputationalLinguistics(Florence:Association
coherence: Concurrent explicit and implicit relations,” in Proceedings of the 56th forComputationalLinguistics),2830–2840.
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Webber,B.,Prasad,R.,Lee,A.,andJoshi,A.(2019).Thepenndiscoursetreebank3.0
Papers)(Melbourne,VIC:AssociationforComputationalLinguistics),2257–2267. annotationmanual.Technicalreport,LinguisticDataConsortium.
Sanders, T. J. M., Spooren, W. P. M., and Noordman, L. G. M. (1992).
Webber, B., Stone, M., Joshi, A., and Knott, A. (2003). Anaphora and
Toward a taxonomy of coherence relations. Discour. Process. 15, 1–35.
discourse structure. Comput. Linguist. 29, 545–588. doi: 10.1162/0891201033227
doi:10.1080/01638539209544800
53347
Sharma, A. (2020). “Improving intent classification in an E-commerce voice
Webber,B.L.(1987).“Theinterpretationoftenseindiscourse,”inProceedingsof
assistantbyusinginter-utterancecontext,”inProceedingsofThe3rdWorkshopon
the25thAnnualMeetingonAssociationforComputationalLinguistics(Associationfor
e-CommerceandNLP(Seattle,WA:AssociationforComputationalLinguistics),40–45.
ComputationalLinguistics),147–154.
Sharma,P.,Ding,N.,Goodman,S.,andSoricut,R.(2018).“Conceptualcaptions:
acleaned,hypernymed,imagealt-textdatasetforautomaticimagecaptioning,”in Yagcioglu, S., Erdem, A., Erdem, E., and Ikizler-Cinbis, N. (2018). Recipeqa: a
Proceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics challengedatasetformultimodalcomprehensionofcookingrecipes.arXivpreprint
(Volume1:LongPapers)(Melbourne,VIC:AssociationforComputationalLinguistics), arXiv:1809.00812.doi:10.18653/v1/D18-1166
2556–2565. Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. (2014). From image
Shuster,K.,Humeau,S.,Hu,H.,Bordes,A.,andWeston,J.(2019).“Engaging descriptions to visual denotations: New similarity metrics for semantic
imagecaptioningviapersonality,”inIEEEConferenceonComputerVisionandPattern inference over event descriptions. TACL 2, 67–78. doi: 10.1162/tacl_a
Recognition,CVPR2019(LongBeach,CA:IEEE),12516–12526. _00166
FrontiersinArtificialIntelligence 13 frontiersin.org

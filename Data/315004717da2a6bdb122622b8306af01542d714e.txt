TheThirty-SixthAAAIConferenceonArtificialIntelligence(AAAI-22)
Cross-Modal Coherence for Text-to-Image Retrieval
MaliheAlikhani1*,FangdaHan2*,HareeshRavi2*,MubbasirKapadia2,
VladimirPavlovic2,MatthewStone2
1UniversityofPittsburgh 2RutgersUniversity
Abstract Thestartoftherace.
Commonimage-textjointunderstandingtechniquespresume
thatimagesandtheassociatedtextcanuniversallybechar-
acterizedbyasingleimplicitmodel.However,co-occurring
imagesandtextcanberelatedinqualitativelydifferentways,
andexplicitlymodelingitcouldimprovetheperformanceof
currentjointunderstandingmodels.Inthispaper,wetraina
Cross-ModalCoherenceModelfortext-to-imageretrievaltask.
Ouranalysisshowsthatmodelstrainedwithimage–textcoher-
encerelationscanretrieveimagesoriginallypairedwithtar-
gettextmoreoftenthancoherence-agnosticmodels.Wealso CMCA CMCM
showviahumanevaluationthatimagesretrievedbythepro-
posedcoherence-awaremodelarepreferredoveracoherence- Figure1:ExampleretrievedimagebytheproposedCross-
agnostic baseline by a huge margin. Our findings provide ModalCoherenceModel(right)vsCross-ModalCoherence
insightsintothewaysthatdifferentmodalitiescommunicate Agnosticmodel(left)forinputcaption(top).
andtheroleofcoherencerelationsincapturingcommonsense
inferencesintextandimagery. methods don’t necessarily deliver images that fit naturally
withtext.Bymodelingcoherence intext andimagery,we
Introduction cansupplyimagestotextthathumanraterspreferbyalarge
margin. This paper describes models of these broader as-
Whenusingtexttoretrieveanimage,humansoftenrelyon
sociationsbetweentext andimageryforthetaskofimage
commonsense inference. Text and imagery can be related
retrieval.
in obvious explicit ways, yet the matter of a caption that
We hypothesize that bringing in coherence relations
accompanies an image frequently only indirectly overlaps
(Alikhanietal.2020)intotheretrievalprocess,incontrastto
with the content of the image. The text, for instance, can
personalitiesdefinedin(Shusteretal.2019),shouldbetter
describequantitiesthatcomplementwhatisdepictedinthe
improvetheperformanceoftext-to-imageretrievalinamore
image(e.g.,addtwocupsofwater)orasubjectivereactionto
generalizableway.WebuildonSalvadoretal.(2017)and
whatisdepictedinanimage(e.g.,fantasticview).Retrieving
Chenetal.(2018)andintroduceanewframeworkthatinte-
imageryisthereforenotjustfindinganimagethatportrays
gratescoherencerelationsintext-to-imageretrievaltaskby
thetextcontentbutdiscoveringanimagethatcoherentlyfits
extractingfeaturesforeachmodalityseparatelythenbuild-
withtexttoconveyanintegratedmessage.
ingalower-dimensionalcommonrepresentationspace.Our
These commonsense inferences can be modeled using
proposedframeworkintroducesaCoherenceAwareModule1
representationsandalgorithmsinformedbyapproachesto
thatlearnstopredictcoherencerelationsthatcharacterizean
naturallanguagediscourse,particularlycoherencerelations
inputimage–textpairduringtraining,andpredictionsfrom
(Hobbs1985;AsherandLascarides2003;TaboadaandMann
themoduleareappliedduringtestingthroughaSelectiveSim-
2006).Coherencerelationscharacterizetheinferentiallinks
ilarityRefinementtechniquetofurtherimprovetheretrieval
(suchastemporal,causal,andlogical)thatconnectthecon-
performance.
tentoftextandimagery.
TheexamplesofFigure1illustrateourapproach.Theycon-
Cluesfromtextandfromthetypicalrelationsoftextand
trasttheoutputofourbaselineCross-ModalCoherenceAg-
imagery provide important evidence about what kinds of
nosticmodel(CMCA)takenfromHan,Guerrero,andPavlovic
visual content is coherent. Therefore, coherence agnostic
(2020) and that of the proposed Cross-Modal Coherence
*Theseauthorscontributedequally.Authornamesorderedal- Model (CMCM) trained on image-text pairs with Story co-
phabeticallybasedonlastname.
Copyright©2022,AssociationfortheAdvancementofArtificial 1Code,contactanddata:https://github.com/klory/Cross-Modal-
Intelligence(www.aaai.org).Allrightsreserved. Coherence-for-Text-to-Image-Retrieval
10427
herencerelations.WeobservethattheproposedCMCMpro- they do not directly support the addition of our proposed
videsmoreimportancetothewordsthestart comparedto CoherenceAwareModule.Wehenceleavetheexplorationof
CMCAthatconcentratesonvisuallygroundedwordslikerace. sucharchitecturesfortheproposedsettingasfuturework.
Thus,CoherenceAwareModuleprovidesmoreinterpretable
androbustresults,byvirtueofexplicitlymodellingimage- Methodology
textcoherence.Duringinference,themodelleveragesthis
InthisSectionwedescribethedetailsofourproposedmodel.
knowledgetoretrieverelevantimages.Weevaluateoursys-
Wearguethatcoherencerelationscharacterizethedatafor
temsontwoimage-textcoherencedatasetsnamelyCITE++
multimodaldiscoursecomprehensionandhypothesizethata
(Alikhanietal.2019)andCLUE(Alikhanietal.2020).Each
modelwithcoherence(CMCM)willbetterretrieverelevantim-
ofthesedatasetscorrespondtodifferentdomainsandarean-
agescomparedtoCMCA.Figure2showsourframeworkfor
notatedwithdifferentcoherencerelationsasshowninTable
CMCMthatconsistsofImageandTextEncodersthatproject
1andTable2.Wealsoanalyzetheeffectofeachcoherence
the two modalitied onto acommon embedding space opti-
relationinthedatasetsbymodifyingtheCoherenceAware
mizedovercosinesimilarity,followedbyaCoherenceAware
ModuleintheproposedCMCMmodeltodetectonlythepres-
Modulethatpredictstheimage-textcoherencerelationsthat
enceofasinglerelation.ThesemodelsCMCM showwhich
c characterizetheinputimage-textpair.Weshowthataddition
coherencerelationsimprove/reducetheperformancewhen
ofCoherenceAwareModuleregularizesthelatentspaceand
comparedwiththebaselines.
improvestheperformanceoftext-to-imageretrievalbymod-
RelatedWork ellingthedifferentcoherencerelationsthatcharacterizean
image-textpair.Tofurtherexplicitlyusethepredictionsfrom
Text-to-image retrieval models have been used in several
CoherenceAwareModule,weproposeaSelectiveSimilarity
multimodalNLPtasksandapplications.Saggion,Pastra,and
Refinementtechniquetorefineandranktheretrievalresult.
Wilks (2003) extract syntactic relations from captions for
Tofurtheranalyzetheperformanceofeachcoherencere-
indexingandretrievingphotographsofcrimescenes.Elliott,
lationontheoverallmodel,wetrainseparateCMCM models
Lavrenko,andKeller(2014)useimageretrievalasatestbed c
thatareaware ofonlyonerelation.TheCoherenceAware
forlearningspatialrelationshipsbetweenimageregionsusing
Module is modified to predict only the presence of a par-
VisualDependencyRepresentations.Noneoftheprevious
ticular relation c (through binary classification in contrast
worksinthislinehavestudiedadiscourse-awareapproach
to multi–label classification in the overall model) in these
fortext-to-imageretrievalwhichwouldbestsuitthecontext
models.
ofthedialogue,inferencesbetweentextandimageryinmul-
timodal documents, and the role of coherence in learning
ModelArchitecture
bettermodelsofimage-textalignments.
Inspiredbythesuccessofcoherencetheorythathasbeen InordertotrainCMCMfortext-to-imageretrieval,wewrite
appliedtootherformsofmultimodalcommunicationsuchas S=[w 1,w 2,...,w m]fortheinputnaturallanguagetextcom-
gesture(LascaridesandStone2009)andcomics(McCloud posedofmwords.(Inprinciplew icouldbewords,phrases,
1993),Alikhanietal.(2019,2020)characterizedcoherence sentencesoranyothersemanticunitoftext.)Similarly,we
relations in text and imagery. Examples of these relations writeIforthecorrespondingimage.GiventextS i,theob-
includeelaboration,whenthetextincludeinformationthat jectiveofanimageretrievalmodelistoretrievethepaired
isnotdepictedintheimage(e.g.,leaveitintheovenfor30 imageI i fromanimagepool{S j},j ∈[1,...,N],whereN
minutes)orsubjectivewhenthetextevaluatesorreactsto isthenumberofimagesinpool.
thecontentoftheimage(e.g.,adeliciouspizza).Theyevalu-
atedtheeffectivenessofcoherencerelationsonacontrolled
captiongenerationtask.Wedonottrainacontrollablemodel
aswehypothesizethatnotallrelationsequallycharacterize
theimageandtextinapair.Thoughtherelationsaredefined
forjointimage-textdiscourse,somecoherencerelationslike
“Subjective”intheCLUEdatasetcharacterizehowthecap-
tionrelatestotheimageandnottheotherwayaround.Hence,
conditioningimageretrievalontherelationisnotreasonable.
Theproposedmethodevaluatestheeffectivenessofcoher-
encerelationsbycomparingCMCAwithCMCM.Notethatthe
proposedCross-ModalCoherenceModelisnotthesameas Figure2:FrameworkofourproposedCross-ModalCoher-
inAlikhanietal.(2020).Instead,ourmodellearnstopredict enceModel.E Sstandsfortextencoder,E Istandsforimage
thecoherencerelationduringtraining.
encoder,α istandsfortheattentionofwordembeddingh
i
WiththeadventoftheTransformer(Vaswanietal.2017)
architecture, there have been large pretrained multimodal ImageEncoderTheimageencoderE isapretrainedResnet-
I
transformers(Luetal.2019;Chenetal.2020;Lietal.2020) 50(Heetal.2016)followedbyabottlenecklayertotrans-
that train on large datasets like MSCOCO and others on formimagefeaturestothesharedlatentspace.Eachimageis
muiltiplejointimage–textlearningtaskssuchascrossmodal firstresizedto224×224,andthenforwardedthroughE to
I
retrieval. Though they obtain state of the art performance, gettheimageembeddingf ∈R300.
I
10428
TextEncoderThetextencoderE startsfromapretrained 3,
S
word2vecmodelthatembedseachwordintoa300dimen-
1
XN
sionalvector.Theword2vecmodelistrainedusingGensim L total = N (Ln ret+λ clsLn cls), (3)
(Rˇehu˚ˇrekandSojka2010).Themaximumlengthofthetext n=1
sequence considered is 200 for CITE++ and 40 for Clue whereλ istheweightassociatedwiththecoherenceaware
cls
basedonthelongestsentencesinthedataset.Then,theword moduleandischosenempiricallyasdescribedlater.
embeddingsaregivenasinputtoaLongShortTermMemory
(LSTM)networktogeteachwordrepresentation.Wenext SelectiveSimilarityRefinement
applyanattentionmechanism(Vaswanietal.2017)tothe Theperformanceoftheretrievalmodeldependsonthesimi-
LSTMrepresentations,whichlearnstheattentionforeach laritiesbetweenaquerycaptionS andallpossibleimages
i
wordandhelpsthemodelattendtokeywordsthatareim- {I },j ∈[1,...,N](includingtheground-truthimage).We
j
portanttoourtask.Finallyafully-connectedlayerisapplied usecosinesimilarity(thoughanyothervalidsimilaritymetric
toencodethejoinedrepresentationofallwordshintothe canbeused)andnotatethesimilaritybetweenqueryS and
i
sharedlatentspace. oneimageI asθ =cosine(S ,I ).
j i,j i j
Theoutputsofthetextandimageencodersarethenused LeveragingConfidenceScoreWeusethecoherencepredic-
withatripletobjectiveusingcosinesimilaritytrainedwith tionfromCoherenceAwareModuletorefinethesimilarity
hardnegativemining.Hardnegativeminingtargetsonthe between an image–text pair for retrieval during inference.
mostdifficultnegativeimageforeachqueryinabatchbased Notethatwedonotknowthecoherencerelationcharacteriz-
onthesimilaritiestoimproveperformance(Hermans,Beyer, ingagroundtruthimage–textpair.However,awelltrained
p
andLeibe2017).Lets(a,b)=aTb/ (aTa)(bTb)measure CoherenceAwareModuleisexpectedtopredictcoherence
thecosinesimilaritybetweentwovectorsaandb,thenthe foragroundtruthimage–textpairwithhighconfidence.We
objectivefortheretrievaltaskpersampleisgivenbyEquation defineaconfidencefunctionforaquerycaptionS andone
i
1, possibleimageI as
k
trip(a,p,n) = s(a,p) − s(a,n)− α,
η =eλ|xi,j,c−0.5|, (4)
i,j,c
L ret = min{0, trip(f S+, f I+, f I−)} (1) X
η = η , (5)
+ min{0, trip(f+, f+, f−)}, i,j i,j,c
I S S c
where L is the retrieval loss, f+ and f+ are outputs of where x is defined in Equation 2, and λ is a hyperparam-
ret S I c
textandimageencoderforapairoftextandimagewhilef− eterdecidedbycrossvalidation.Confidencefunctionwith
S
isatextoutputthatdoesnotcorrespondtocurrentimageand differentλareshowninFigure3(a).Wecanseethatlower
f− isanimageoutputthatdoesnotcorrespondtocurrent λ decreases the impact of the confidence function. We set
I
text.Themarginαissetto0.3bycross-validation. λ=0.13forCITE++andλ=0.12forCluedatasetsempir-
ically.Therefinedsimilarityisdefinedas,
Coherence Aware Module Instead of relying only on the θ¯ =θ ∗η (6)
encoders,wealsoleveragecoherencerelationslabelledby i,j i,j i,j
humans.WeaddaCoherenceAwareModulethattakesthe
normalizedfeaturesfrombothtextandimageencodersas
inputandthenpassesthemthroughamulti-layerperceptron
topredicttherelations.
Thedimensionofthefinallinearlayerisequaltothenum-
berofrelationsinthedatasetwhentrainedwithallrelations
(i.e.multi-labelclassification)and1whentrainedwithasin-
glerelation(i.e.single-labelclassification).WeuseBinary
CrossEntropy(BCE)asthelossfunctionandtheobjective
ofCoherenceAwareModuleforonesampleis,
P
L = w (y log(x )+(1−y )log(1−x )), (2) (a) (b)
cls c c c c c c
Figure3:(a)Confidencefunctionη withdifferentλ.(b)
i,j,c
wherex istheprobabilityassignedtorelationcbythemodel
c Correctimagerankvs.thedifferencebetweenthesimilarities
whiley isthegroundtruthbinaryvalue.Sincetherelations
c ofthetop2retrievedimagesonCITE++validationset
are not equally distributed in the dataset, we balance the
trainingofdifferentrelationsbygivingaweightw cforeach SelectiveRefinementThoughconfidencescorehelps,byit-
relationthatisreciprocaltoitsproportioninthedataset.For selfthescoreisaweakindicatorperformingonlyslightlybet-
CMCM c models,thesummationisremovedasthereisonly terthanrandom.Wehencelimittheuseofconfidencescore
onerelationthatispredicted. todifficultexamples.Wehypothesizethatsimilaritybetween
Themodelisthustrainedinamulti-tasksettingwherethe a correct image–text pair should on average be “α” larger
coherencepredictoristheauxiliarytask.Thefinalobjective thanthatofawrongimage–textpairbecauseofEquation1.
overtheentirebatchwithbatchsizeN isgiveninEquation InFigure3(b),weverifythishypothesisbyplottingtherank
10429
Relation Question Description Positiverate
Expansion Q2 Theimagegivesvisualinformationaboutthestepdescribedinthetext. 0.821
ImageNeeded Q3 Youneedtoseetheimageinordertobeabletocarryoutthestepproperly. 0.115
Elaboration Q4 Thetextprovidesspecificquantities(amounts,measurements,etc.)thatyou 0.329
t
wouldnotknowjustbylookingatthepicture.
Elaboration Q5 Theimageshowsatoolusedinthestepbutnotmentionedinthetext. 0.193
i−tool
Temporal Q6 Theimageshowshowtopreparebeforecarryingoutthestep. 0.158
i<t
Temporal Q7 Theimageshowstheresultsoftheactionthatisdescribedinthetext. 0.588
i>t
Temporal Q8 Theimagedepictsanactioninprogressthatisdescribedinthetext. 0.313
i=t
Table1:Coherencerelations,theirdistributionandentropyinCITE++dataset.Weusethequestionidentifierandtherelation
nameinterchangeablyinthepaper.Positiverateisthepercentageofsamplesthatarelabeledas‘Yes’forthatquestion
ofgroundtruthimagevs.thedifferencebetweenthesimi- texthasoneimagethatvisualizesit.UsingAmazonMechan-
laritiesofthetop2retrievedimageswiththequerycaption. icalTurk,theauthorsobtainedanswersto10questionsthat
Weobservethatwhenthedifferencebetweenthesimilarities helpcharacterizetherelationshipbetweenimageandtext.We
of the top 2 images (∆) is large enough (e.g., ≥ 0.2), the choosethequestionsthatarebestsuitedtotrainCMCMasde-
retrievalisalwayssuccessful(e.g.,groundtruthimagerank= scribedinTable1.Theoriginaldatasethas2057image-text
1).Basedonthisanalysis,weselectdifficultquerycaptions pairsannotatedwithTrue/Falseanswerstothesequestions
asthosewith∆<T,whereT isahyperparameterchosen indicatingpresence/absenceofthecoherencerelation.Toper-
as0.1empirically.WeusetherefinedsimilarityEquation6 formamorecomprehensiveexperiment,wecollected2242
for”difficult”examplesduringinference. morepairsusingthesameannotationprotocol,givingusa
totalof4299image-textpairs.Thedistributionofrelations
Image-TextCoherenceDatasets intheentiredatasetisgiveninTable1.Figure4[a]shows
WestudytheefficacyofCMCMforimage-retrievalbylever- anexamplefromCITE++dataset.
agingtwoimage-textdatasetsCITE++andClue(Alikhani
et al. 2020) that are annotated with image-text coherence Relation Visible Subj. Action Story Meta Irr.
relations.CITE++isextendedbyusfromCITE(Alikhani
et al. 2019) adding 2242 image-text pairs annotated with %Positive 67.4 6.6 15.7 24.3 39.1 08.7
coherencerelations.
Table2:Coherencerelations(Subj.isSubjectiveandIrr.is
Irrelevant) and their distribution in Clue dataset (Alikhani
etal.2020)
CLUE
TheCluedataset(Alikhanietal.2020)isconstructedusing
themuchlargerConceptualCaptionsdataset(Sharmaetal.
2018) which is primarily an image captioning dataset like
COCO(Linetal.2014).Clueannotated7559image-caption
(a) Once they have baked re- (b) Seals fighting for a spot to pairswithsixcoherencerelationstosummarizethestructural,
move them from the oven and sleepontherocks logical and purposeful relationships between the contribu-
sprinkle lightly with sugar. Af- tionsoftextsandimages.Exampleimage-captionpairwith
teryouhavedressedthemallow coherencerelationsareshowninFigure4(b).
themtocoolforabout5minutes
andserve
ExperimentalSetup
Figure 4: Example image-text pairs from CITE++ (a) and Network Details. The backbone of image encoder E is
i
Clue (b) datasets. Image-text pair on the left has relations ResNet-50, with one additional batch normalization layer
Expansion,ElaborationandTemporal i>t whiletheoneon andonefully-connectedlayertotransformthefeatureinto
therighthasrelationsActionasVisible thesharedspace(R1024).Theword2vecmodelencodeseach
wordintoavectorofR300,textencoderE takesthevector
t
CITE++
as input and forwards it through a bidirectional, one-layer
We extend the CITE dataset which is a subset of a popu- LSTMmodulefollowinganattentionlayer(Vaswanietal.
lar recipe dataset RecipeQA (Yagcioglu et al. 2018). The 2017),andfinallytheattention-weightedsummationofword
RecipeQAdatasetconsistsofmultimodalrecipesthatcon- features is also transformed into the shared space (R1024)
tainstextualinstructionsaccompaniedbyoneormoreimages. byabatchnormalizationlayerandafully-connectedlayer.
CITEleveragedrecipesthathaveone-to-onecorrespondence CoherenceAwareModulecontainsonefully-connectedlayer,
betweeninstructionandimage,e.g.,everyinstructioninthe addingmorelayersdoesnotimproveperformance.
10430
Model Coherence Relations Attention textsinCITE++dataset.Moreover,weobservethatCMCM
Aware modelperformsbetterthanCMCAandBaseacrossallmet-
Module ricsthoughwithvariablesignificance.Forexample,MedR
Base ✗ - ✗ forCMCAmodelis5.4butallCMCMmodelsachieveaverage
CMCA ✗ - ✓ MedRoflessthan5.0.Moreoverthestandarddeviationis
CMCM-NoAttn ✓ All ✗ alsolowerindicatingmorerobustperformance.Theresults
CMCM ✓ All ✓ ontheCluedatasetaregiveninTable5.Weobservethatboth
CMCM c ✓ c ✓ the attention mechanism and the coherence-aware module
improvetheperformance.WeusetheexampleinFig.1toin-
Table 3: Description of the models used for comparison. tuitivelyexplaintheeffectofCoherenceAwareModule.Note
-NoAttn means removing the attention module from the CMCAretrievestheincorrectimageastherearemoreimages
proposedmodel.‘All’relationsindicatethattheCoherence of“races”ingeneralthanthereareof“startofarace”inthe
AwareModuleistrainedwithalltherelationsinamulti-label dataset.Also,thetext“startofarace”communicatesastory
multi-classsetting.cindicatesonlyonerelationisusedina ratherthanfactuallydescribeelementsinanimage,CMCA
binaryclassificationsetting. ignoresthedifferentcharacteristicsbywhichanimage-text
pair can be related thereby producing the most commonly
EvaluationMetrics.Weevaluatetheretrievalperformance foundsemanticallysimilarimage.CMCMresolvesthiscon-
ofallthemodelsusingthemedianretrievalrank(MedR)and cern by considering coherence relations between the two
therecallatK(R@K)metricsfollowingexistingworkson modalitiesandretrievesthecorrectimage.Weobservethat
text–to–imageretrieval(Han,Guerrero,andPavlovic2020; allper-relationCMCM modelsperformbetterthanCMCA.In
c
Fromeetal.2013).Theretrievalrangeissettobe500.Since someinstants,per-relationmodelsperformbetterthanCMCM,
CITE++andCluehaveimage-textpairsthatexhibitcomplex confirming the conjecture that not all relations contribute
relationships,wealsoperformacomprehensiveuserstudyto in increasing the performance of the retrieval model. We
evaluatetheperformanceofthemodel.MedR(0 ≤MedR performadditionalanalysisonper-relationcontributionto
≤1)iscomputedasthemedianrankofthetruepositiveover CMCMsperformanceintheAppendix.
allqueries,alowerMedRsuggestsbetterperformance.R@K
(0≤R@K≤100)computesthepercentageoftruepositives
recalledamongthetop-Kretrievedcandidates,higherindi-
catesbetterperformance.Hereweonlyreporttheresultsof
retrievingimagebyusingthecaptionasquery.
DatasetPartition.Inourexperiments,weevaluatethemodel
and the coherence relations on CITE++ and Clue datasets
independently. We split the CITE++ dataset as 3439/860
fortraining/testingwhiletheCluedatasetas6047/1512for
training/testing.10%ofthetrainingdataisusedasvalidation.
Furthertrainingandhyperparameterdetailsaregiveninthe
appendix.ComparativeEvaluation.Forboththedatasets,
wetraintheproposedmodelandcomparewithvariousbase- Figure5:ComparisonMedRbetweenbaseline,CMCAand
linesasshowninTable3.ThebaselineCMCA(Han,Guerrero, differentCMCMvariants;aswellasthecomparisonbetween
andPavlovic2020)issimilartoexistingCNN-RNNarchi- thesamemodelwith(orange)andwithout(blue)selectivesim-
tecturessuchas(Xuetal.2015;Ravietal.2018;Yangetal. ilarityrefinement.Left:CITE++dataset.Right:Cluedataset
2020).NotewealsocomparewithCMCM ,whichonlyuses
c
one specific relation to train the system. We perform this
experimentprimarilytoanalyzetheeffectofeachrelation
Impact of Similarity Refinement. To evaluate the contri-
asnotallrelationscontributeequallytotheretrievalsystem.
butionofselectivesimilarityrefinement,wecompareMedR
Thisalsohelpsusbetterunderstandtheinfluenceofdifferent basedonθ andθ¯ ofthesamemodelinFigure5.TheCMCM
i i
relations on the proposed Cross-Modal Coherence Model (except ‘NoAttn’) variants clearly outperform CMCA and
model.Thoughitispossibletodeveloptransformerbased
baseline.Moreover,theselectiverefinementtechniqueim-
models for the proposed setting, we use GRUs and CNNs provestheresultofalmostalltheCMCMmodelsevenfurther
becauseofthelowcardinalityofthedatasetsandtheneces-
byalargemarginascanbeseenbythedifferencebetween
sityoflargedatasetsfortransformerbasedmodels(Inanetal.
the blue and orange bars. In Clue dataset, in most cases,
2021;Ganeshetal.2021;Crawford2021).
modelusingselectivesimilarityrefinementperformsbetter
thanthesamemodelwithoutrefinement,provingtheeffec-
ResultsandDiscussion tiveness of the refinement technique. For CMCM
Irrelevant
modelonClue(lasttwobarsonFigure5right),applyingthe
CMCMvsCMCA
refinementseverelydegradestheperformance.Webelieve
The results on CITE++ dataset are shown in Table 4. As that‘Irrelevant’relationdoesnoteffectivelycharacterizethe
canbeseen,havingattentionoverthetextclearlyimproves relationship between an image–text pair on top of its low
retrievalperformance.Thiscanbeattributedtothelengthy positivityscore.
10431
GT CMCM CMCA
(a)Action Horsegrazingonasummermeadowintheforestoutdoors.
(b)Visible Avectorillustrationofahappymalegolfer.
(c)Temporal Finishing-Paintalltheblackpartsexceptthedooronthelocomotivewithgoldfoodpaint...addmoredetails.
i>t
Figure6:Thegroundtruthimage(Left)andthetop5retrievedimagesbytheCMCMandtheCMCAmodelsfortwoexamples.
Thecoherencerelation(inblue)andcaptionaregivenabovetheimages.Theimage-textpairinexample(a)hasActionrelation
whileinexample(b)hasVisiblerelation.Inexample(a)theCMCMmodelleveragestheActioncoherencerelationtoretrieve
imagesthatdepictsomeactioninthetop5.Similarlyinexample(c)imagesretrievedbyproposedourmodelwithCAMretrieves
imagesthatdepicttheresultofaprocessasgivenbytheTemporal relation,whereastheagnosticmodelshowsimagesthat
i>4
depictactioninprogress.
MedR↓ R@1↑ R@5↑ R@10↑ MedR↓ R@1↑ R@5↑ R@10↑
Base 10.0±3.7 45.7 48.4 50.6 Base 19.8±1.9 11.6 28.6 38.3
CMCA 5.4±2.3 46.0 50.1 53.8 CMCA 19.3±2.0 13.2 30.6 40.0
CMCM-NoAttn 6.6±2.5 46.0 49.5 52.0 CMCM-NoAttn 20.6±2.6 12.4 28.9 38.8
CMCM 4.2±1.2 46.5 51.4 53.9 CMCM 18.7±1.6 13.8 31.6 40.6
CMCM 4.7±2.0 46.4 50.6 53.4 CMCM 19.6±3.1 13.4 31.7 41.1
Q2 Visible
CMCM 4.2±1.3 46.2 51.1 54.2 CMCM 25.0±3.1 12.9 29.4 38.0
Q3 Subjective
CMCM 4.2±1.3 46.2 51.2 54.2 CMCM 20.9±2.1 11.7 28.4 38.0
Q4 Action
CMCM 3.7±1.3 46.6 51.5 54.4 CMCM 17.7±1.7 13.0 30.7 41.5
Q5 Story
CMCM 4.6±1.4 45.9 50.8 53.4 CMCM 19.2±1.5 13.1 31.0 40.7
Q6 Meta
CMCM 3.9±1.7 46.9 51.2 54.1 CMCM 20.3±1.9 12.6 31.1 40.9
Q7 Irrelevant
CMCM 5.0±1.7 46.4 50.8 53.8
Q8
Table5:Quantitativecomparisonofthemodelstrainedand
Table 4: Quantitative comparison in CITE++ dataset. The evaluatedonCluedataset.
relationscorrespondingtoeachQ areshowninTable1.↓
i
indicatesthatlowerthebetterand↑indicatesthathigherthe
modelsareshownforpairwisecomparison.
better.
Werecruit250participantsthroughAmazonMechanical
HumanEvaluation Turk.AllsubjectswereUScitizens,agreedtoaconsentform
approvedbytheUniversityofPittsburghIRBreviewboard,
BothCITE++andCluehaveimage-textpairswithcomplex and were compensated at an estimated rate of USD 15 an
coherencerelationsincontrasttodatasetslikeMSCOCOthat hour.Weshowedsubjectsthecaption,thetopimageretrieved
havepredominantlyjustVisiblerelations.Hence,considering bythecoherenceawareandthecoherenceagnosticmodel
thegroundtruthasagoldstandardisnotreasonable.Given forfiverelationsfromboththedatasetsandaskedthemto
thewidedistributionofdifferentrelationsinthedatasets,the chooseoneofthefollowingoptions:
quantitativemetrics(e.g.,MedRandRecalls)areunreliable (1)IpreferimageA(2)IpreferimageB(3)Theimagesare
fortheproposedsetting.Therefore,weperformhumaneval- exactlythesame(4)Neitheroftheimagesisagoodmatch
uationwherethetop1retrievedimagesbyCMCAandCMCM forthistext.Theorderofimagesisrandomandeachexample
10432
golfer.Theresultisthemodelbeingabletoretrievethecor-
rectimageintop1thoughbothmodelsretrieveimagesof
Vectorillustrationinthetop5Figure6[b].Moreexamples
forotherrelationsareprovidedintheAppendix.
InCITE++dataset,weobservesimilarbehaviorasshown
inFigure6[c].TherelationTemporal characterizesthe
i>t
temporalcorrelationbetweenanimageandtextwherethe
image visualizes the result of the process described in the
correspondingtext.Theserelationsaredifficulttoimplicitly
Figure7:AttentionweightsforCMCMandCMCAmodelsfor understandasthetextisnodifferentfromanyotherstepinthe
exampleLeft:Figure6(a)andRight:Figure6(b). recipe.TrainingwithCoherenceAwareModulethatexplicitly
modelstemporalrelationimprovestheperformanceofimage
was ranked by three workers and the final rank is decided retrieval.Forexample,wecanseeinFigure6thatalltop5
viamajorityvoting.TheresultsareshowninTable6.Itcan imagesretrievedbytheCMCMareimagesthatvisualizethe
be seen that the images retrieved by the proposed model result of a process, in contrast to CMCA model that shows
arepreferredbyhumans.Moreimportantly,thedifferencein imagesofthestepbeingcarriedoutaswell.
preferenceissignificantincontrasttothequantitativemetrics.
PredictingCoherenceRelations
We can also see that the difference in preference between
CMCM and CMCA is higher when the relation is Subjective Tofurtherunderstandtheeffectandimportanceofcoherence
or Story when compared to regular captions (see Visible), relations,weanalyzethemodel’sabilitytopredictthepres-
indicatingtheimportanceofexplicitlymodelingcoherence ence of a coherence relation given the ground truth image
relationsforcross-modalunderstanding.Theresultsofthe andtext.Forthis,weusethemodelstrainedusingtheorig-
t-testshowsthatthedifferencesobservedinCMCMandCMCA inal objective in Equation 3. We provide the ground truth
categoryareallstatisticallysignificant(p<0.01,t>14.1). imageandtextasinputandcalculatetheAveragePrecision
Theresultsofthesensitivitypoweranalysisshowsthatour (AP) of coherence relation prediction. The results are pro-
experimentdetectseffectsizesassmallas0.17withapower vided in the Appendix. We can see that in most cases as
andsignificancelevelof95%.Theseresultseffectivelyshow expected, the CMCM algorithms perform reasonably well.
c
thatthequantitativemetricssuchasMedRandRecallcannot We haven’t provided the results for CMCA models as they
solelymeasureperformanceofthemodelespeciallygiven werenottrainedwithcoherencerelations.Theseresultsare
thenatureofthedatasetandcoherencerelations. comparabletosimilarexperimentsperformedin(Alikhani
etal.2020)thoughintheirexperiment,classificationwasthe
Better Worse BothGood BothBad onlyobjective.InterestinglySubjectiverelationhasverylow
AP (cf. appendix) similar to retrieval performance but the
CMCM 24% 17% 46% 13%
Visible proposedmodelobtainssignificancegaininperformancein
CMCM Subjective 53% 10% 7% 30% thehumanevaluation.
CMCM 40% 10% 33% 17%
Story Conclusion2
CMCM 56% 9% 25% 10%
Meta
Automatingtheunderstandingandgenerationofmultimodal
CMCM 43% 17% 27% 13%
Q7 discourserequiresajointunderstandingofco-occurringim-
ages and text. Our study shows the effectiveness of cross-
Table6:Humanevaluationresults.Valuesindicatetheper-
modalcoherencemodelingfortext-to-imageretrievaltasks.
centage of samples for which humans voted the output of
Ourevaluationshowsthattheperformanceofthecoherence-
CMCM as Better, Worse, Both Good, Both Bad when com-
awaremodelissignificantlybettercomparedtotheagnos-
paredwithCMCA.
ticmodels.WealsoobservethattheexistingRecallbased
quantitative metrics for text-to-image retrieval are unreli-
ableandfailtomeaningfullyevaluateretrievalsystemsespe-
QualitativeAnalysis
ciallywhenimage-textpairscanbecharacterizedbydiffer-
Tofurtherunderstandthebehaviorofthemodel,weinvesti-
entcoherentrelations.Futureworkinvolvesdevelopingnew
gatetheattentionweightsoverinputtextforCMCMandCMCA
transformer-basedcoherence-awaremetricsthatcanbetter
models. In example Figure 6 (a), the proposed coherence-
measuretheperformanceofretrievalmodels.Basedonthe
awaremodelretrievesthegroundtruthwithinthetop5im-
evidenceshowninthispaper,animportantextensionisto
ages.WecanseefromFigure7(left)thataddingCoherence
annotateexistingdatasetswithcoherencerelationstofurther
AwareModuleincreasestheweightonwordshorseandgraz-
improvesemanticjointunderstandingofimageandtext.
ing relative to the agnostic model. This can be attributed
to the model’s ability to predict the associated coherence
relationtohelpretrievetherightimage.TheCMCAmodel, 2The research presented in this paper has been supported
however,attendsmoretocommonlyvisualizedwordslike by NSF awards IIS-1703883, IIS-1955404, IIS-1955365, IIS
forest and outdoors. Similarly, in Figure 6 (right), CMCM 1955404, RETTL-2119265, IIS-1526723, CCF-1934924, and
showsimprovedattentionweightsforwordslikemaleand EAGER-2122119,andthroughgenerousdonationsfromAdobe.
10433
References In European Conference on Computer Vision, 121–137.
Alikhani,M.;Chowdhury,S.N.;deMelo,G.;andStone,M. Springer.
2019. CITE:ACorpusofImage–TextDiscourseRelations. Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.;
arXivpreprintarXiv:1904.06286. Ramanan,D.;Dollár,P.;andZitnick,C.L.2014. Microsoft
Alikhani,M.;Sharma,P.;Li,S.;Soricut,R.;andStone,M. coco:Commonobjectsincontext. InEuropeanconference
2020. Cross-modalCoherenceModelingforCaptionGen- oncomputervision,740–755.Springer.
eration. InProceedingsofthe58thAnnualMeetingofthe
Lu,J.;Batra,D.;Parikh,D.;andLee,S.2019. ViLBERT:
AssociationforComputationalLinguistics,6525–6535.
pretrainingtask-agnosticvisiolinguisticrepresentationsfor
Asher,N.;andLascarides,A.2003. Logicsofconversation. vision-and-language tasks. In Proceedings of the 33rd In-
CambridgeUniversityPress. ternational Conference on Neural Information Processing
Chen,J.-J.;Ngo,C.-W.;Feng,F.-L.;andChua,T.-S.2018. Systems,13–23.
Deepunderstandingofcookingprocedureforcross-modal
McCloud,S.1993. Understandingcomics:Theinvisibleart.
recipe retrieval. In Proceedings of the 26th ACM interna-
Northampton,Mass.
tionalconferenceonMultimedia,1020–1028.
Ravi,H.;Wang,L.;Muniz,C.;Sigal,L.;Metaxas,D.;and
Chen,Y.-C.;Li,L.;Yu,L.;Kholy,A.E.;Ahmed,F.;Gan,
Kapadia,M.2018.Showmeastory:Towardscoherentneural
Z.; Cheng, Y.; and Liu, J. 2020. Uniter: Universal image-
storyillustration. InProceedingsoftheIEEEConferenceon
text representation learning. In European Conference on
ComputerVisionandPatternRecognition,7613–7621.
ComputerVision.
Crawford,K.2021. TheAtlasofAI. YaleUniversityPress. Rˇehu˚ˇrek,R.;andSojka,P.2010. SoftwareFrameworkfor
TopicModellingwithLargeCorpora. InProceedingsofthe
Elliott, D.; Lavrenko, V.; and Keller, F. 2014. Query-by-
LREC2010WorkshoponNewChallengesforNLPFrame-
ExampleImageRetrievalusingVisualDependencyRepre-
works,45–50.Valletta,Malta:ELRA.
sentations. InProceedingsofCOLING2014,the25thInter-
nationalConferenceonComputationalLinguistics:Techni- Saggion,H.;Pastra,K.;andWilks,Y.2003. Nlpforindexing
calPapers,109–120.Dublin,Ireland:DublinCityUniversity andretrievalofcaptionedphotographs.In10thConferenceof
andAssociationforComputationalLinguistics. theEuropeanChapteroftheAssociationforComputational
Frome, A.; Corrado, G. S.; Shlens, J.; Bengio, S.; Dean, Linguistics.
J.; Ranzato, M.; and Mikolov, T. 2013. Devise: A deep Salvador,A.;Hynes,N.;Aytar,Y.;Marin,J.;Ofli,F.;Weber,
visual-semanticembeddingmodel. InAdvancesinneural I.;andTorralba,A.2017. Learningcross-modalembeddings
informationprocessingsystems,2121–2129. forcookingrecipesandfoodimages. InProceedingsofthe
Ganesh,P.;Chen,Y.;Lou,X.;Khan,M.A.;Yang,Y.;Sajjad, IEEEconferenceoncomputervisionandpatternrecognition,
H.;Nakov,P.;Chen,D.;andWinslett,M.2021.Compressing 3020–3028.
large-scaletransformer-basedmodels:Acasestudyonbert.
Sharma, P.; Ding, N.; Goodman, S.; and Soricut, R. 2018.
TransactionsoftheAssociationforComputationalLinguis-
ConceptualCaptions:ACleaned,Hypernymed,ImageAlt-
tics,9:1061–1080.
textDatasetForAutomaticImageCaptioning. InProceed-
Han, F.; Guerrero, R.; and Pavlovic, V. 2020. CookGAN: ingsofthe56thAnnualMeetingoftheAssociationforCom-
MealImageSynthesisfromIngredients. InTheIEEEWinter putationalLinguistics,2556–2565.
ConferenceonApplicationsofComputerVision,1450–1458.
Shuster, K.; Humeau, S.; Hu, H.; Bordes, A.; and Weston,
He,K.;Zhang,X.;Ren,S.;andSun,J.2016. Deepresidual
J. 2019. Engaging image captioning via personality. In
learningforimagerecognition. InProceedingsoftheIEEE
Proceedings of the IEEE Conference on Computer Vision
conferenceoncomputervisionandpatternrecognition,770–
andPatternRecognition,12516–12526.
778.
Taboada,M.;andMann,W.C.2006. Applicationsofrhetori-
Hermans,A.;Beyer,L.;andLeibe,B.2017. Indefenseof
calstructuretheory. Discoursestudies,8(4):567–588.
the triplet loss for person re-identification. arXiv preprint
arXiv:1703.07737. Vaswani,A.;Shazeer,N.;Parmar,N.;Uszkoreit,J.;Jones,L.;
Hobbs, J. R. 1985. On the coherence and structure of dis- Gomez,A.N.;Kaiser,Ł.;andPolosukhin,I.2017. Attention
course. CSLIPublications. isallyouneed.InAdvancesinneuralinformationprocessing
systems,5998–6008.
Inan, M.; Sharma, P.; Khalid, B.; Soricut, R.; Stone, M.;
andAlikhani,M.2021. COSMic:ACoherence-AwareGen- Xu,K.;Ba,J.;Kiros,R.;Cho,K.;Courville,A.;Salakhudi-
eration Metric for Image Descriptions. In Findings of the nov,R.;Zemel,R.;andBengio,Y.2015. Show,attendand
Association for Computational Linguistics: EMNLP 2021, tell:Neuralimagecaptiongenerationwithvisualattention.In
3419–3430. Internationalconferenceonmachinelearning,2048–2057.
Lascarides, A.; and Stone, M. 2009. A formal semantic Yagcioglu, S.; Erdem, A.; Erdem, E.; and Ikizler-Cinbis,
analysisofgesture. JournalofSemantics,26(4):393–449. N.2018. RecipeQA:AChallengeDatasetforMultimodal
Li,X.;Yin,X.;Li,C.;Zhang,P.;Hu,X.;Zhang,L.;Wang, ComprehensionofCookingRecipes. InProceedingsofthe
L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020. Oscar: Object- 2018ConferenceonEmpiricalMethodsinNaturalLanguage
semantics aligned pre-training for vision-language tasks. Processing,1358–1368.
10434
Yang,L.;Hu,H.;Xing,S.;andLu,X.2020. Constrained
lstmandresidualattentionforimagecaptioning. ACMTrans-
actions on Multimedia Computing, Communications, and
Applications(TOMM),16(3):1–18.
10435

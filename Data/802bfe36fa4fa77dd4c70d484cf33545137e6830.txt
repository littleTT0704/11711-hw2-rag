Jointly Modeling Inter-Slot Relations by Random Walk on Knowledge
Graphs for Unsupervised Spoken Language Understanding
Yun-NungChen,WilliamYangWang,andAlexanderI.Rudnicky
SchoolofComputerScience,CarnegieMellonUniversity
5000ForbesAvenue,Pittsburgh,PA15213-3891,USA
{yvchen, yww, air}@cs.cmu.edu
Abstract al.,2012),“restaurant”isthetargetslot,andimpor-
tantadjectivemodifierssuchas“Asian”(therestau-
Akeychallengeofdesigningcoherentseman- rant type) and “cheap” (the price of the restaurant)
tic ontology for spokenlanguage understand- should be included in the slot set, so that the se-
ingistoconsiderinter-slotrelations. Inprac-
manticrepresentationofSLUcanbemorecoherent
tice,however,itisdifficultfordomainexperts
and complete. In this case, it is challenging to de-
andprofessionalannotatorstodefineacoher-
signsuchacoherentandcompleteslotsetmanually,
ent slot set, while considering various lexi-
cal, syntactic, and semanticdependencies. In while considering various lexical, syntactic, and se-
this paper, we exploit the typed syntactic de- manticdependencies.
pendency theory for unsupervised induction Instead of considering slots independently, this
and filling of semantics slots in spoken dia- paper takes a data-driven approach to model word-
logue systems. More specifically, we build
to-word relations via syntactic dependencies and
two knowledge graphs: a slot-based seman-
further infer slot-to-slot relations. To do this, we
tic graph, and a word-based lexical graph.
incorporate the typed dependency grammar the-
To jointly consider word-to-word, word-to-
ory (De Marneffe and Manning, 2008) in a state-
slot, and slot-to-slot relations, we use a ran-
domwalkinferencealgorithmtocombinethe of-the-art frame-semantic driven unsupervised slot
twoknowledgegraphs,guidedbydependency induction framework (Chen et al., 2013b). In par-
grammars. The experiments show that con- ticular, we build two knowledge graphs: a slot-
sideringinter-slotrelationsiscrucialforgen-
basedsemanticknowledgegraph,andaword-based
eratingamorecoherentandcompeteslotset,
lexical knowledge graph. Using typed dependency
resulting in a better spoken language under-
triples, we then study the stochastic relations be-
standing model, while enhancing the inter-
tween slots and words, using a mutually-reinforced
pretabilityofsemanticslots.
random walk inference procedure to combine the
two knowledge graphs. In evaluations, we use the
1 Introduction
jointlylearnedinter-slotrelationstoinduceacoher-
ent slot set in an unsupervised fashion. Our contri-
An important requirement for building a success-
butionsarethree-fold:
ful spoken dialogue system (SDS) is to define a co-
herent slot set and the corresponding slot-fillers for
• We are among the first to consider unsuper-
the spoken language understanding (SLU) compo-
vised spoken language understanding combin-
nent. Unfortunately,sincethesemanticslotsareof-
ingsemanticandlexicalknowledgegraphs;
tenmutually-related,itisnon-trivialfordomainex-
perts and professional annotators to design a such • Weproposeanoveltypedsyntacticdependency
slotsetforsemanticrepresentationofSLU. grammar driven random walk model for rela-
Considering a restaurant domain (Henderson et tiondiscovery;
619
HumanLanguageTechnologies:The2015AnnualConferenceoftheNorthAmericanChapteroftheACL,pages619–629,
Denver,Colorado,May31–June5,2015.(cid:13)c2015AssociationforComputationalLinguistics
• Our experimental results suggest that jointly duceandfillsemanticslots(Chenetal.,2013b),and
considering inter-slot relations helps obtain a that leveraging distributional semantics helps im-
morecoherentandcompletesemanticslotset. provingtheperformance(Chenetal.,2014b). How-
ever, prior works treat each slot independently and
2 RelatedWork
havenotconsideredtheinter-slotrelationswhenin-
ducingthesemanticslots. Tothebestofourknowl-
Withtherecentsuccessofcommercialdialoguesys-
edge, we are the first to use syntactically-informed
tems and personal assistants (e.g., Microsoft’s Cor-
random walk algorithms to combine the semantic
tana1, Google Now2, Apple’s Siri3, and Amazon’s
and lexical knowledge graphs, and not individually
Echo4), a key focus on developing spoken under-
butgloballyinducingthesemanticslotsforbuilding
standingtechniquesisthescalabilityissue.
betterunsupervisedSLUcomponents.
From the knowledge management perspective,
empoweringthesystemwithalargeknowledgebase
3 TheProposedFramework
is of crucial significance to modern spoken dia-
loguesystems. Onthisend, ourworkclearlyaligns
We build our approach on top of the recent suc-
with recent studies on leveraging semantic knowl-
cessofanunsupervisedframe-semanticparsingap-
edge graphs for SLU modeling (Heck et al., 2013;
proach (Chen et al., 2013b). The main motivation
Hakkani-Tu¨r et al., 2013; Hakkani-Tu¨r et al., 2014;
of prior work is to use a FrameNet-trained statis-
El-Kahky et al., 2014; Chen et al., 2014a). While
tical probabilistic semantic parser to generate ini-
leveraging external knowledge is the trend, effi-
tial frame-semantic parses from automatic speech
cientinferencealgorithms,suchasrandomwalk,are
recognition (ASR) decodings of the raw audio con-
still less-studied for direct inference on knowledge
versation files, and then adapt the FrameNet-style
graphsofthespokencontents.
frames to the semantic slots in the target semantic
In the natural language processing literature, Lao
space, so that they can be used practically in the
et al. (2011) used a random walk algorithm to con-
SDSs. Chen et al. formulated the semantic map-
struct inference rules on large entity-based knowl-
pingandadaptationproblemasarankingproblemto
edge bases, and leveraged syntactic information
differentiate generic semantic concepts from target
for reading the Web (Lao et al., 2012). Even
semantic space for task-oriented dialogue systems.
though this work has important contributions, the
Thispaperimprovestheadaptationprocessbylever-
proposedalgorithmcannotlearnmutually-recursive
aging distributed word embeddings associated with
relations, and does not to consider lexical items—
typedsyntacticdependenciesbetweenwordstoinfer
in fact, more and more studies show that, in addi-
inter-slot relations (Mikolov et al., 2013b; Mikolov
tion to semantic knowledge graphs, lexical knowl-
et al., 2013c; Levy and Goldberg, 2014). The pro-
edge graphs (Inkpen and Hirst, 2006; Song et al.,
posed framework is shown in Figure 1. In the re-
2011;Lietal.,2013b)thatmodelsurface-levelnatu-
mainder of the section, we first introduce frame-
rallanguagerealization,multiwordexpressions,and
semanticparsingtoobtainslotcandidates. Withslot
context (Li et al., 2013a), are also critical for short
candidates, then we train the independent semantic
text understanding (Song et al., 2011; Wang et al.,
decoders. Theadaptationprocess,whichisthemain
2014).
focusofthispaper,isperformedtodecideoutputted
Fromtheengineeringperspective,quickandeasy
slots. Finally we can build an SLU model based on
development turnaround time for domain-specific
thelearnedsemanticdecodersandinducedslots.
dialogueapplicationsisalsocritical(ChenandRud-
nicky,2014). Priorworkshowsthatitispossibleto 3.1 ProbabilisticSemanticParsing
use the frame-semantics theory to automatically in-
FrameNet is a linguistically-principled semantic re-
1http://www.windowsphone.com/en-us/how-to/ sourcethatoffersannotationsofpredicate-argument
wp8/cortana semantics, and associated lexical units for En-
2http://www.google.com/landing/now
glish (Baker et al., 1998). FrameNet is developed
3http://www.apple.com/ios/siri
4http://www.amazon.com/oc/echo based on a semantic theory, Frame Semantics (Fill-
620
“can I have a cheap restaurant”
Unlabeled Collection
Semantic Decoder
Slot Induction Semantic Knowledge Graph Training
Frame-Semantic Parsing SLU Model
Slot
Slot Ranking
Candidates Model
Syntactic Dependency Parsing Semantic
Lexical Knowledge Graph Induced Slots Representation
Figure1: Theproposedframework
can i have a cheap restaurant andrestaurant),whichweconsideraspossibleslot-
fillers.
Frame: expensiveness
FT LU: cheap 3.2 IndependentSemanticDecoder
Frame: capability Frame: locale_by_use
FT LU: can FE Filler: i FT/FE LU: restaurant With outputted semantic parses, we extract the
frameswiththetop50highestfrequencyasourslot
Figure 2: An example of probabilistic frame-semantic candidates for training SLU. The features for train-
parsingonASRoutput. FT:frametarget. FE:frameele- ingaregeneratedbywordconfusionnetwork,where
ment. LU:lexicalunit.
confusionnetworkfeaturesareshowntobeusefulin
developingmorerobustsystemsforSLU(Hakkani-
more, 1976), which holds that the meaning of most Tu¨r et al., 2006; Henderson et al., 2012). We
words can be expressed on the basis of semantic buildavectorrepresentationofanutteranceasu =
frames, which encompass three major components: [x ,...,x ,...].
1 j
frame(F),frameelements(FE),andlexicalunits
(LU).Forexample,theframe“food”containswords x j = E[C u(n-gram j)]1/|n-gram j|, (1)
referring to items of food. A descriptor frame ele-
where C (n-gram ) counts how many times n-
ment within the food frame indicates the character- u j
gram occurs in the utterance u, E(C (n-gram ))
istic of the food. For example, the phrase “low fat j u j
is the expected frequency of n-gram in u, and
milk” should be analyzed with “milk” evoking the j
|n-gram |isthenumberofwordsinn-gram .
foodframeand“lowfat”fillingthedescriptorFEof j j
For each slot candidate s , we generate a pseudo
thatframe. i
training data Di to train a binary classifier Mi for
In our approach, we parse all ASR-decoded ut-
predicting the existence of s given an utterance,
terances in our corpus using SEMAFOR5, a state- i
Di = {(u ,li) | u ∈ R+,li ∈ {−1,+1}}K ,
of-the-art semantic parser for frame-semantic pars- k k k k k=1
where li = +1 when the utterance u contains the
ing (Das et al., 2010; Das et al., 2013), and ex- k k
slotcandidates initssemanticparse,li = −1oth-
tractallframesfromsemanticparsingresultsasslot i k
erwise,andK isthenumberofutterances.
candidates, where the LUs that correspond to the
frames are extracted for slot filling. For example,
3.3 AdaptationProcessandSLUModel
Figure2showsanexampleofanASR-decodedtext
SinceSEMAFORwastrainedonFrameNetannota-
outputparsedbySEMAFOR.SEMAFORgenerates
tion,whichhasamoregenericframe-semanticcon-
three frames (capability, expensiveness, and lo-
text, not all the frames from the parsing results can
cale by use) for the utterance, which we consider
beusedastheactualslotsinthedomain-specificdia-
asslotcandidatesfortrainingtheSLUmodel. Note
loguesystems. Forinstance,inFigure2,weseethat
that for each slot candidate, SEMAFOR also in-
theframes“expensiveness”and“locale by use”
cludes the corresponding lexical unit (can i, cheap,
are essentially the key slots for the purpose of un-
5http://www.ark.cs.cmu.edu/SEMAFOR/ derstandingintherestaurantquerydomain,whereas
621
ccomp
s
2
s 1 s 3 nsubj dobj det amod
Semantic Knowledge Graph can i have a cheap restaurant
capability expensiveness locale_by_use
w
2 w
5
w 1 w 3 w 7 Figure4: Thedependencyparsingresultonanutterance.
w w
4 6
Lexical Knowledge Graph scores, we only use the frequency of each slot can-
didateasitsprominence.
Figure 3: A simplified example of the two knowledge
First we construct two knowledge graphs, one
graphs,whereaslotcandidates isrepresentedasanode
i is a slot-based semantic knowledge graph and an-
in a semantic knowledge graph and a word w is repre-
j
otherisaword-basedlexicalknowledgegraph,both
sentedasanodeinalexicalknowledgegraph.
of which encode the typed dependency relations in
their edge weights. We also connect two graphs to
the “capability” frame does not convey particular
modeltherelationsbetweenslot-fillerpairs.
valuable information for SLU. With the trained in-
dependentsemanticdecodersforallslotcandidates, 4.1 KnowledgeGraphs
adaptationprocesscomputestheprominenceofslot
We construct two undirected graphs, semantic and
candidates for ranking and then selects a list of in-
lexicalknowledge graphs. Eachnodein theseman-
duced slots associated with their corresponding se-
tic knowledge graph is a slot candidate s outputted
i
manticdecodersforuseindomain-specificdialogue
by the frame-semantic parser, and each node in the
systems,wherethedetailisdescribedinSection4.
lexicalknowledgegraphisawordw .
j
Thenwitheachinducedslots anditscorrespond-
i
ingtrainedsemanticdecoderMi,anSLUmodelcan • Slot-basedsemanticknowledgegraphisbuilt
be built to predict whether the semantic slot occurs as G = hV ,E i, where V = {s } and
s s ss s i
inthegivenutteranceinafullyunsupervisedway. In E = {e | s ,s ∈ V }.
ss ij i j s
otherwords,theSLUmodelisabletotransformthe
• Word-basedlexicalknowledgegraphisbuilt
testingutteranceintosemanticrepresentationswith-
as G = hV ,E i, where V = {w } and
outhumaninvolvement. w w ww w i
E = {e | w ,w ∈ V }.
ww ij i j w
4 SlotRankingModel
With two knowledge graphs, we build the edges
The purpose of the ranking model is to distinguish between slots and slot-fillers to integrate them as
between generic semantic concepts and domain- showninFigure3. Thusthecombinedgraphcanbe
specificconceptsthatarerelevanttoanSDS.Toin- formulatedasG = hV ,V ,E ,E ,E i,where
s w ss ww ws
duce meaningful slots for the purpose of SDS, we E = {e | w ∈ V ,s ∈ V }. E , E ,
ws ij i w j s ss ww
compute the prominence of the slot candidates us- and E correspond to slot-to-slot relations, word-
ws
ingaslotrankingmodeldescribedbelow. to-wordrelations,andword-to-slotrelationsrespec-
WiththesemanticparsesfromSEMAFOR,where tively (Chen and Metze, 2012; Chen and Metze,
eachframeisviewedindependently,sointer-slotre- 2013).
lations are not included, the model ranks the slot
4.2 EdgeWeightEstimation
candidates by integrating two information: (1) the
frequencyofeachslotcandidateinthecorpus,since Considering the relations in the knowledge graphs,
slotswithhigherfrequencymaybemoreimportant. the edge weights for E and E are measured
ww ss
(2)therelationsbetweenslotcandidates. Assuming based on the dependency parsing results. The
that domain-specific concepts are usually related to example utterance “can i have a cheap restau-
each other, globally considering inter-slot relations rant” and its dependency parsing result are illus-
induces a more coherent slot set. Here for baseline trated in Figure 4. The arrows denote the de-
622
TypedDependencyRelation TargetWord Contexts
restaurant cheap/AMOD
Word hrestaurant,AMOD,cheapi
cheap restaurant/AMOD−1
locale by use expensiveness/AMOD
Slot hlocale by use,AMOD,expensivenessi
expansiveness locale by use/AMOD−1
Table1: Thecontextsextractedfortrainingdependency-basedword/slotembeddingsfromtheutteranceofFig.2.
pendency relations from headwords to their de- asaheadandx asadependent.
j
pendents, and words on arcs denote types of the
t∗ = argmaxC(x →− x ), (3)
dependencies. All typed dependencies between x →x i j
i j t t
two words are encoded in triples and form a
where C(x →− x ) counts how many times the de-
word-based dependency set T = {hw ,t,w i}, i j
w i j t
where t is the typed dependency between the pendencyhx ,t,x ioccursinthedependencysetT .
i j x
headword w and the dependent w . For exam- Then the scoring function that estimates the de-
i j
ple, Figure 4 generates hrestaurant,AMOD,cheapi, pendencyx
i
→ x
j
ismeasuredas
hhave,DOBJ,restauranti, etc. for T w. Simi-
r (x → x ) = C(x −−−−→ x ), (4)
larly, we build a slot-based dependency set T = 1 i j i j
s t∗
{hs ,t,s i} by transforming dependencies between xi→xj
i j
slot-fillers into ones between slots. For example, which equals to the highest observed frequency of
hrestaurant,AMOD,cheapi from T
w
is transformed thedependencyx
i
→ x
j
amongalltypesfromT x.
into hlocale by use,AMOD,expensivenessi for
4.2.2 Embedding-BasedMeasurement
building T , because both sides of the non-dotted
s
lineareparsedasslot-fillersbySEMAFOR. Most neural embeddings use linear bag-of-words
For the edges within a single knowledge graph, contexts,whereawindowsizeisdefinedtoproduce
weassignaweightoftheedgeconnectingnodesx contexts of the target words (Mikolov et al., 2013c;
i
and x as rˆ(x ,x ), where x is either s or w. Since Mikolovetal.,2013b;Mikolovetal.,2013a). How-
j i j
theweightsaremeasuredbasedontherelationsbe- ever, some important contexts may be missing due
tween nodes regardless of the directions, we com- to smaller windows, while larger windows capture
binethescoresoftwodirectionaldependencies: broad topical content. A dependency-based em-
bedding approach was proposed to derive contexts
rˆ(x ,x ) = r(x → x )+r(x → x ), (2)
i j i j j i basedonthesyntacticrelationsthewordparticipates
in for training embeddings, where the embeddings
where r(x → x ) is the score estimating the de-
i j
are less topical but offer more functional similarity
pendencyincludingx asaheadandx asadepen-
i j
compared to original embeddings (Levy and Gold-
dent. In Section 4.2.1 and 4.2.2, we propose two
berg,2014).
scoring functions for r(·), frequency-based as r (·)
1
Table 1 shows the extracted dependency-based
andembedding-basedasr (·)respectively.
2
contexts for each target word from the example in
For the edges in E , we estimate the edge
ws
Figure4,whereheadwordsandtheirdependentscan
weights based on the frequency that the slot candi-
formthecontextsbyfollowingthearconawordin
datesandthewordsareparsedasslot-fillerpairs. In
thedependencytree,and−1denotesthedirectional-
other words, the edge weight between the slot-filler
ityofthedependency. Afterreplacingoriginalbag-
w and the slot candidate s , rˆ(w ,s ), is equal to
i j i j
of-words contexts with dependency-based contexts,
howmanytimesthefillerw correspondstotheslot
i
we can train dependency-based embeddings for all
candidates intheparsingresults.
j
target words (Yih et al., 2014; Bordes et al., 2011;
4.2.1 Frequency-BasedMeasurement Bordesetal.,2013).
BasedonthedependencysetT ,weuset∗ to Fortrainingdependency-basedwordembeddings,
x x →x
i j
denote the most frequent typed dependency with x eachwordw isassociatedwithawordvectorv ∈
i w
623
Rd and each context c is represented as a context 4.3.1 Single-GraphRandomWalk
vector v c ∈ Rd, where d is the embedding dimen- Here we run random walk only on the semantic
sionality. We learn vector representations for both knowledge graph to propagate the scores based on
wordsandcontextssuchthatthedotproductv w·v c inter-slotrelationsthroughtheedgesE ss.
associated with “good” word-context pairs belong-
ing to the training data D is maximized, leading to R(t+1) = (1−α)R(0)+αL R(t), (8)
s s ss s
theobjectivefunction:
X 1 where R s(t) denotes the importance scores of the
arg max log , (5) slot candidates V in t-th iteration. In the algo-
vw,vc 1+exp(−v c·v w) s
(w,c)∈D rithm,thescoreistheinterpolationoftwoscores,the
which can be trained using stochastic-gradient up- normalized baseline importance of slot candidates
dates (Levy and Goldberg, 2014). Then we can (R(0)), and the scores propagated from the neigh-
s
obtain the dependency-based slot and word embed- boringnodesinthesemanticknowledgegraphbased
dingsusingT andT respectively. on slot-to-slot relations via L . The algorithm will
s w ss
With trained dependency-based embeddings, we converge when R(t+1) = R(t) = R∗ and (9) can be
s s s
estimate the probabilitythat x i is the headword and satisfied.
x isitsdependentviathetypeddependencytas (cid:16) (cid:17)
j
R∗ = (1−α)R(0)eT +αL R∗ = M R∗,
Sim(x ,x /t)+Sim(x ,x /t−1) s s ss s 1 s
P(x i →− t x j) = i j 2 j i , (9)
(6) where e = [1,1,...,1]T. It has been shown that
where Sim(x ,x /t) is the cosine similarity be- the closed-form solution R∗ of (9) is the dominant
i j s
tweentheslot/wordembeddingsv andthecontext eigenvector of M (Langville and Meyer, 2005),
x 1
i
embeddings v after normalizing to [0,1]. Then the eigenvector corresponding to the largest abso-
x /t
j
wecanmeasurethescoringfunctionr (·)as lute eigenvalue of M . The solution of R∗ de-
2 1 s
notes the updated importance scores for all utter-
r (x → x ) = C(x −−−−→ x )·P(x −−−−→ x ),
2 i j i t∗ j i t∗ j ances. SimilartothePageRankalgorithm(Brinand
xi→xj xi→xj
(7) Page,1998),thesolutioncanalsobeobtainedbyit-
which is similar to (4) but additionally weighted by erativelyupdatingR(t).
s
theestimatedprobability. Theestimatedprobability
4.3.2 Double-GraphRandomWalk
smoothstheobservedfrequencytoavoidoverfitting
duetoasmallerdataset. Here we borrow the idea from two-layer mutu-
allyreinforcedrandomwalktopropagatethescores
4.3 RandomWalkAlgorithm based on not only internal importance propagation
We first compute L = [rˆ(w ,w )] and withinthesamegraphbutexternalmutualreinforce-
ww i j |Vw|×|Vw|
L = [rˆ(s ,s )] , where rˆ(w ,w ) and ment between different knowledge graphs (Chen
ss i j |Vs|×|Vs| i j
rˆ(s ,s ) are either from frequency-based (r (·)) andMetze,2012;ChenandMetze,2013).
i j 1
or embedding-based measurements (r (·)). Sim- (
2
i [rl ˆa (r wly, ,sL
)w ]Ts
= [ ,rˆ w(w
hi
e, rs
ej
rˆ) (]
| wVw ,|× s|V )s i|
sta hn ed frL
eqs uw
enc=
y
RR (s( tt ++ 11 )) == (( 11 −− αα )) RR (s( 00 )) ++ αα LL ssL LswR Rw( (t t)
) (10)
i j |Vw|×|Vs| i j w w ww ws s
that s and w are a slot-filler pair computed in
j i
In the algorithm, they are the interpolations of two
Section 4.2. Then we only keep the top N high-
scores, the normalized baseline importance (R(0)
est weights for each row in L and L (N = s
ww ss
10), which means that we filter out the edges with and R w(0)) and the scores propagated from another
smaller weights within the single knowledge graph. graph. For the semantic knowledge graph, L R(t)
sw w
Column-normalizationareperformedforL ,L , is the score from the word set weighted by slot-to-
ww ss
L , L (Shi and Malik, 2000). They can be word relations, and then the scores are propagated
ws sw
viewed as word-to-word, slot-to-slot, and word-to- based on slot-to-slot relations via L . Similarly,
ss
slotrelationmatrices. nodes of the lexical knowledge graph also include
624
thescorespropagatedfromthesemanticknowledge
commerce scenario
locale by use
graph. ThenR(t+1) andR(t+1) canbemutuallyup- type expensiveness price range
s w building
range
datedbythelatterpartsin(10)iteratively. Whenthe
food
algorithmconverges,wehaveR∗ asfollows. food part orientational
s origin
direction
area
R∗ = (1−α)R(0) (11) speak on topic addr locale
s s(cid:16) (cid:17) part inner outer
seeking
+ αL ssL sw (1−α)R w(0)+αL wwL wsR s∗ desiring task contacting phone
(cid:16)
locating
sending postcode
= (1−α)R(0)eT +α(1−α)L L R(0)eT
s ss sw w
(cid:17)
+ α2L L L L R∗ = M R∗. Figure 5: The mappings from induced slots (within
ss sw ww ws s 2 s blocks)toreferenceslots(rightsidesofarrows).
Theclosed-formsolutionR∗of(11)isthedominant
s
eigenvectorofM . typed syntactic dependencies (Socher et al., 2013)
2
and set the dimensionality of embeddings d = 300
5 Experiments
inallexperiments.
Weevaluateourapproachintwoways. First,weex-
5.2 EvaluationMetrics
aminetheslotinductionaccuracybycomparingthe
ranked list of induced slots with the reference slots To eliminate the influence of threshold selection
created by system developers (Young, 2007). Sec- when choosing induced slots, in the following met-
ondly,withtherankedlistofinducedslotsandtheir rics,wetakethewholerankinglistintoaccountand
associated semantic decoders, we can evaluate the evaluate the performance by the metrics that are in-
SLUperformance. Fortheexperiments,weevaluate dependentoftheselectedthreshold.
bothonASRtranscriptsoftherawaudio,andonthe
5.2.1 SlotInduction
manualtranscripts.
To evaluate the accuracy of the induced slots, we
5.1 ExperimentalSetup
measure their quality as the proximity between in-
In this experiment, we used the Cambridge Univer- duced slots and reference slots. Figure 5 shows
sity SLU corpus, previously used on several other the mappings that indicate semantically related in-
SLU tasks (Henderson et al., 2012; Chen et al., ducedslotsandreferenceslots(Chenetal.,2013b).
2013a). Thedomainofthecorpusisaboutrestaurant Forexample,“expensiveness→price”,“food→
recommendationinCambridge;subjectswereasked food”, and “direction → area” show that these in-
to interact with multiple SDSs in an in-car setting. duced slots can be mapped into the reference slots
The corpus contains a total number of 2,166 dia- definedbyexpertsandcarryimportantsemanticsin
logues,including15,453utterances(10,571forself- the target domain for developing the task-oriented
training and 4,882 for testing). The data is gender- SDS. Since we define the adaptation task as a rank-
balanced, with slightly more native than non-native ing problem, with a ranked list of induced slots and
speakers. Thevocabularysizeis1868. AnASRsys- associated scores, we can use the standard average
temwasusedtotranscribethespeech;theworderror precision (AP) and the area under the precision-
rate was reported as 37%. There are 10 slots cre- recallcurve(PR-AUC)asourmetrics,wherethein-
ated by domain experts: addr, area, food, name, duced slot is counted as correct when it has a map-
phone, postcode, price range, signature, task, pingtoareferenceslot.
andtype.
5.2.2 SLUModel
Forparametersetting,thedampingfactorforran-
dom walk α is empirically set as 0.9 for all exper- Whilesemanticslotinductionisessentialforpro-
iments. For training the semantic decoders, we use viding semantic categories and imposing semantic
SVM with a linear kernel to predict each semantic constraints, we are also interested in understanding
slot. WeuseStanfordParsertoobtainthecollapsed the performance of our unsupervised SLU models.
625
ASR Manual
Approach SlotInduction SLUModel SlotInduction SLUModel
AP PR-AUC WAP AF AP PR-AUC WAP AF
(a) Baseline(Frequency) 56.69 54.67 35.82 43.28 53.01 50.80 36.78 44.20
(b) Frequency 63.88 62.05 41.67 47.38 63.02 61.10 43.76 48.53
Single
(c) Embedding 69.04 68.25 46.29 48.89 75.15 74.50 54.50 50.86
(d) Frequency 56.83 55.31 32.64 44.91 52.12 50.54 34.01 45.05
Double
(e) Embedding 71.48 70.84 44.06 47.91 76.42 75.94 52.89 50.40
Table2: TheperformanceofinducedslotsandcorrespondingSLUmodels(%)
For each induced slot with the mapping to a ref- ing the prominence of slot candidates to generate a
erence slot, we can compute an F-measure of the coherentslotset.
correspondingsemanticdecoder,andweighttheav-
erage precision with corresponding F-measure as 5.3.2 SLUModel
weighted average precision (WAP) to evaluate the
For both ASR and manual transcripts, almost all
performance of slot induction and SLU tasks to-
results outperform the baseline, which shows the
gether. The metric scores the ranking result higher
practical usage for training dialogue systems. The
iftheinducedslotscorrespondingtobettersemantic
bestperformanceisfromtheresultsofsingle-graph
decoders are ranked higher. Another metric is the
random walk with the embedding-based measure-
averageF-measure(AF),whichistheaveragemicro
ment,whichonlyusethesemanticknowledgegraph
F-measureofSLUmodelsatallcut-offpositionsin
to involve the inter-slot relations. The semantic
the ranked list. Compared to WAP, AF additionally
knowledgegraphisnotaspreciseasthelexicalone
considerstheslotpopularityinthedataset.
andmaybeinfluencedbytheperformanceofthese-
5.3 EvaluationResults mantic parser more. Although the row (e) does not
show better performance than the row (c), double-
Table5.1showstheresultsonbothASRandmanual
graph random walk may be more robust because
transcripts. Rows (a) is the baseline only consider-
it additionally includes the word relations to avoid
ing the frequency of each slot candidate for rank-
onlyrelyingontherelationstiedwiththeslotcandi-
ing (Chen et al., 2013b). Rows (b) and (c) show
dates.
performanceafterleveragingasemanticknowledge
graph through random walk. Rows (d) and (e) are
5.4 DiscussionandAnalysis
the results after combining two knowledge graphs.
We find almost all results are improved by addi-
5.4.1 ComparingFrequency-and
tionally considering inter-slot relations in terms of
Embedding-BasedMeasurements
single-anddouble-graphrandomwalkforbothASR
andmanualtranscripts. Table 5.1 shows that all results with the
embedding-based measurement perform better than
5.3.1 SlotInduction those with the frequency-based measurement. The
For both ASR and manual transcripts, almost frequency-based measurement also brings large im-
all results outperform the baseline, showing that provementforsingle-graphapproaches,butdoesnot
inter-slotrelationssignificantlyinfluencetheperfor- for double-graph ones. The reason is probably that
mance of slot induction. The best performance is usingobservedfrequenciesinthelexicalknowledge
from the results of double-graph random walk with graph may result in overfitting issues due to the
theembedding-basedmeasurement,whichintegrate smaller dataset. Additionally including embedding
a semantic knowledge graph and a lexical knowl- information can smooth the edge weights and deal
edgegraphtogetherandjointlyconsiderslot-to-slot, with data sparsity to improve the performance, es-
word-to-word,andword-to-slotrelationswhenscor- peciallyforthelexicalknowledgegraph.
626
5.4.2 ComparingSingle-andDouble-Graph Rank Relation
Approaches 1 hlocale by use,NN,foodi
Considering that the embedding-based measure-
2 hfood,AMOD,expensivenessi
ment performs better, we only compare the results
3 hlocale by use,AMOD,expensivenessi
ofsingle-anddouble-graphrandomwalkusingthis
4 hseeking,PREP FOR,foodi
measurement (rows (c) and (e)). It can be seen
5 hfood,AMOD,relational quantityi
that the difference between them is not consistent
6 hdesiring,DOBJ,foodi
intermsofslotinductionandSLUmodel.
7 hseeking,PREP FOR,locale by usei
For evaluating slot induction (AP and PR-AUC),
8 hfood,DET,quantityi
the double-graph random walk (row (e)) performs
Table 3: The top inter-slot relations learned from the
better on both ASR and manual results, which im-
trainingsetofASRoutputs.
pliesthatadditionallyintegratingthelexicalknowl-
edge graph helps decide a more coherent and com-
plete slot set since we can model the score propa- locale_by_use
PREP_FOR
gation moreprecisely (not only slot-levelbut word-
level information). However, for SLU evaluation seeking AMOD
NN
(WAP and AF), the single-graph random walk (row
PREP_FOR
(c))performsbetter, whichmayimplythattheslots
AMOD
food expensiveness
carryingthecoherentrelationsfromtherow(e)may DOBJ
AMOD
nothavegoodsemanticdecoderperformancesothat
desiring
the performance is decreased a little. For exam- relational_quantity
ple, double-graph random walk scores the slots lo-
cal by useandexpensivenesshigherthantheslot
Figure6: Theautomaticallyconstructeddomain-specific
contacting, while the single-graph method ranks
ontologybasedonTable3.
thelatterhigher. local by useandexpensiveness
are more important on this domain but contacting
6 Conclusion
has very good performance of its semantic decoder,
sothedouble-graphapproachdoesnotshowtheim-
The paper proposes an approach of jointly consid-
provement when evaluating SLU models. This al-
ering inter-slot relations for slot induction to out-
lows us to try an improved method of jointly opti-
put a more coherent slot set, where two knowledge
mizing the slot coherence and SLU performance in
graphs, a slot-based semantic knowledge graph and
thefuture.
aword-basedlexicalknowledgegraph,arebuiltand
combined by a random walk algorithm. The au-
5.4.3 RelationDiscoveryAnalysis
tomatically induced slots carry coherent and inter-
Tointerprettheinter-slotrelations, weoutputthe
pretablerelationsandcanbeusedfortrainingbetter
slot-to-slot relations with highest scores from the
SLUmodelsofSDSsinanunsupervisedfashion.
best results (row (e) in Table 5.1) in Table 3, and
the automatically constructed ontology is shown in Acknowledgments
Figure 6. It can be shown that the outputted inter-
slotrelationsarereasonableandusuallyconnecttwo We thank Anatole Gershman for helpful discus-
important semantic slots in this restaurant domain. sionsandanonymousreviewersfortheirusefulcom-
This proves that inter-slot relations help decide a ments. We are also grateful to MetLife’s support.
coherent and complete slot set and enhance the in- Any opinions, findings, and conclusions expressed
terpretability of semantic slots. Therefore, from a inthispublicationarethoseoftheauthorsanddonot
practical perspective, developers are able to design necessarilyreflecttheviewsoffundingagencies.
theframeworkofdialoguesystemsmoreeasily,and
thedevelopmentofSDScanbespeededupwithless
humaneffort.
627
References Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
Collin F Baker, Charles J Fillmore, and John B Lowe.
parsing. In Proceedings of The Conference of the
1998. The Berkeley FrameNet project. In Proceed-
North American Chapter of the Association for Com-
ingsofCOLING,pages86–90.
putational Linguistics: Human Language Technolo-
AntoineBordes,JasonWeston,RonanCollobert,Yoshua gies,pages948–956.
Bengio,etal. 2011. Learningstructuredembeddings DipanjanDas, DesaiChen, Andre´ F.T.Martins, Nathan
ofknowledgebases. InProceedingsofAAAI. Schneider,andNoahA.Smith. 2013. Frame-semantic
AntoineBordes,NicolasUsunier,AlbertoGarcia-Duran, parsing. ComputationalLinguistics.
JasonWeston, andOksanaYakhnenko. 2013. Trans- Marie-Catherine De Marneffe and Christopher D Man-
latingembeddingsformodelingmulti-relationaldata. ning. 2008. The Stanford typed dependencies repre-
In Advances in Neural Information Processing Sys- sentation. In Coling 2008: Proceedings of the work-
tems,pages2787–2795. shop on Cross-Framework and Cross-Domain Parser
SergeyBrinandLawrencePage. 1998. Theanatomyofa Evaluation,pages1–8.AssociationforComputational
large-scalehypertextualwebsearchengine. Computer Linguistics.
networksandISDNsystems,30(1):107–117. Ali El-Kahky, Derek Liu, Ruhi Sarikaya, Go¨khan Tu¨r,
Yun-Nung Chen and Florian Metze. 2012. Two-layer DilekHakkani-Tu¨r,andLarryHeck. 2014. Extending
mutuallyreinforcedrandomwalkforimprovedmulti- domain coverage of language understanding systems
party meeting summarization. In Proceedings of The via intent transfer between domains using knowledge
4thIEEEWorkshoponSpokenLanguageTachnology, graphsandsearchqueryclicklogs. InProceedingsof
pages461–466. ICASSP.
Charles J Fillmore. 1976. Frame semantics and the na-
Yun-Nung Chen and Florian Metze. 2013. Multi-layer
tureoflanguage. AnnalsoftheNYAS,280(1):20–32.
mutuallyreinforcedrandomwalkwithhiddenparame-
DilekHakkani-Tu¨r,Fre´de´ricBe´chet,GiuseppeRiccardi,
tersforimprovedmulti-partymeetingsummarization.
and Gokhan Tur. 2006. Beyond ASR 1-best: Using
InINTERSPEECH,pages485–489.
word confusion networks in spoken language under-
Yun-NungChenandAlexanderI.Rudnicky. 2014. Dy-
standing. Computer Speech & Language, 20(4):495–
namicallysupportingunexploreddomainsinconversa-
514.
tionalinteractionsbyenrichingsemanticswithneural
DilekHakkani-Tu¨r,LarryHeck,andGokhanTur. 2013.
wordembeddings. InProceedingsof2014IEEESpo-
Usingaknowledgegraphandqueryclicklogsforun-
kenLanguageTechnologyWorkshop(SLT),.
supervisedlearningofrelationdetection. InProceed-
Yun-Nung Chen, William Yang Wang, and Alexander I.
ingsofICASSP,pages8327–8331.
Rudnicky. 2013a. An empirical investigation of
Dilek Hakkani-Tu¨r, Asli Celikyilmaz, Larry Heck,
sparse log-linear models for improved dialogue act
GokhanTur,andGeoffZweig. 2014. Probabilisticen-
classification. InProceedingsofICASSP,pages8317–
richment of knowledge graph entities for relation de-
8321.
tection in conversational understanding. In Proceed-
Yun-Nung Chen, William Yang Wang, and Alexander I ingsofINTERSPEECH.
Rudnicky. 2013b. Unsupervisedinductionandfilling Larry P Heck, Dilek Hakkani-Tu¨r, and Gokhan Tur.
of semantic slots for spoken dialogue systems using 2013. Leveragingknowledgegraphsforweb-scaleun-
frame-semanticparsing. InProceedingsof2013IEEE supervised semantic parsing. In Proceedings of IN-
Workshop on Automatic Speech Recognition and Un- TERSPEECH.
derstanding(ASRU),pages120–125.IEEE. MatthewHenderson,MilicaGasic,BlaiseThomson,Pir-
Yun-Nung Chen, Dilek Hakkani-Tu¨r, and Gokhan Tur. ros Tsiakoulis, Kai Yu, and Steve Young. 2012.
2014a. Deriving local relational surface forms from Discriminative spoken language understanding using
dependency-based entity embeddings for unsuper- word confusion networks. In Proceedings of SLT,
visedspokenlanguageunderstanding. InProceedings pages176–181.
of2014IEEESpokenLanguageTechnologyWorkshop Diana Inkpen and Graeme Hirst. 2006. Building and
(SLT),. using a lexical knowledge base of near-synonym dif-
Yun-Nung Chen, William Yang Wang, and Alexander I. ferences. ComputationalLinguistics,32(2):223–262.
Rudnicky. 2014b. Leveraging frame semantics and Amy N Langville and Carl D Meyer. 2005. A survey
distributionalsemanticsforunsupervisedsemanticslot ofeigenvectormethodsforwebinformationretrieval.
inductioninspokendialoguesystems. InProceedings SIAMreview,47(1):135–161.
of2014IEEESpokenLanguageTechnologyWorkshop Ni Lao, Tom Mitchell, and William W Cohen. 2011.
(SLT),. Random walk inference and learning in a large scale
628
knowledgebase. InProceedingsoftheConferenceon andranking. InProceedingsofthe23rdACMInterna-
Empirical Methods in Natural Language Processing, tional Conference on Conference on Information and
pages 529–539. Association for Computational Lin- KnowledgeManagement,pages1069–1078.ACM.
guistics. Wen-tau Yih, Xiaodong He, and Christopher Meek.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and 2014. Semantic parsing for single-relation question
William W Cohen. 2012. Reading the web with answering. InProceedingsofACL.
learned syntactic-semantic inference rules. In Pro- Steve Young. 2007. CUED standard dialogue acts.
ceedings of the 2012 Joint Conference on Empirical Technical report, Cambridge University Engineering
Methods in Natural Language Processing and Com- Department.
putational Natural Language Learning, pages 1017–
1026.AssociationforComputationalLinguistics.
Omer Levy and Yoav Goldberg. 2014. Dependency-
basedwordembeddings. InProceedingsofACL.
PeipeiLi,HaixunWang,HongsongLi,andXindongWu.
2013a. Assessing sparse information extraction us-
ing semantic contexts. In Proceedings of the 22nd
ACMinternationalconferenceonConferenceoninfor-
mation&knowledgemanagement, pages1709–1714.
ACM.
Peipei Li, Haixun Wang, Kenny Q Zhu, Zhongyuan
Wang, and Xindong Wu. 2013b. Computing term
similaritybylargeprobabilisticisaknowledge. InPro-
ceedingsofthe22ndACMinternationalconferenceon
Conferenceoninformation&knowledgemanagement,
pages1401–1410.ACM.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficientestimationofwordrepresenta-
tions in vector space. In Proceedings of Workshop at
ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. InProceedingsofAdvancesinNeuralInformation
ProcessingSystems,pages3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751.Citeseer.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. Pattern Analysis and Ma-
chine Intelligence, IEEE Transactions on, 22(8):888–
905.
RichardSocher,JohnBauer,ChristopherDManning,and
AndrewYNg. 2013. Parsingwithcompositionalvec-
torgrammars. InProceedingsoftheACLconference.
Citeseer.
Yangqiu Song, Haixun Wang, Zhongyuan Wang, Hong-
song Li, and Weizhu Chen. 2011. Short text con-
ceptualizationusingaprobabilisticknowledgebase. In
Proceedings of the Twenty-Second international joint
conference on Artificial Intelligence-Volume Volume
Three,pages2330–2336.AAAIPress.
FangWang,ZhongyuanWang,ZhoujunLi,andJi-Rong
Wen. 2014. Concept-based short text classification
629

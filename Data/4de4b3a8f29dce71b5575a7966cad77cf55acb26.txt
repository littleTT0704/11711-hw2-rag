DialCrowd 2.0: A Quality-Focused Dialog System Crowdsourcing Toolkit
JessicaHuynh,Ting-RuiChiang,JeffreyBigham,MaxineEskenazi
CarnegieMellonUniversity
Pittsburgh,PA
{jhuynh,tingruic}@cs.cmu.edu,{jbigham,max}@cmu.edu
Abstract
Dialogsystemdevelopersneedhigh-qualitydatatotrain,fine-tuneandassesstheirsystems.Theyoftenusecrowdsourcingfor
thissinceitprovideslargequantitiesofdatafrommanyworkers. However,thedatamaynotbeofsufficientlygoodquality.
Thiscanbeduetothewaythattherequesterpresentsataskandhowtheyinteractwiththeworkers. Thispaperintroduces
DialCrowd 2.0 to help requesters obtain higher quality data by, for example, presenting tasks more clearly and facilitating
effective communication with workers. DialCrowd 2.0 guides developers in creating improved Human Intelligence Tasks
(HITs)andisdirectlyapplicabletotheworkflowsusedcurrentlybydevelopersandresearchers.
Keywords:dialog,crowdsourcing
1. Introduction suesincommunicationwiththeworkerswillrepelthe
better workers. Thus, in parallel to our examination
High-quality human data is essential in the develop-
of the HITs on AMT, we also tallied the workers’ as-
mentofdialogsystems. ManyresearcherscreateHITs
sessments of these HITs and of their requesters using
oncrowdsourcingplatformssuchasAmazonMechan-
Turkerview1. Outofatotalof102HITsavailableover
icalTurk(AMT)tocollectdatafromhumans. Obtain-
that time span, 56 met our criteria and were reviewed
ing high-quality data is dependent on the usability of
for the study. 54 of the total 102 HITs were reviewed
thetasksworkersareaskedtocomplete(e.g.,learnabil-
onTurkerviewforpaymentand67outofatotalof79
ity,feedback,etc.) (Nielsen,1994),yetmanytasksfall
requesters were reviewed on Turkerview for payment
short(Huynhetal.,2021).
assessment.
Toaddressthisproblem,weintroduceDialCrowd2.0,
Reinforcing the hypothesis that requesters need help
asubstantialupdatetoDialCrowd1.0. DialCrowd1.0
withtheirHITs,wefoundthat25%ofthe56HITshad
(Lee et al., 2018) facilitated data collection by pro-
technicalissues. Outofthe54HITsreviewedonTurk-
vidinganinterfacethatrequestersusedtocreateHITs
erview, only 39% paid above $10 an hour. All of the
frompre-configuredtemplates. Thegoalinthe1.0ver- paymentlevelsmaybefoundinFigure1(Huynhetal.,
sion was to make the task creation process more effi-
2021). The findings in this study reinforce the claim
cient. Once a HIT was created, workers accessed and
of this paper that the research community needs Dial-
worked on the HIT from the DialCrowd-generated in-
Crowd 2.0 to help them obtain better quality crowd-
terface. The added efficacy that DialCrowd provides
sourceddata.
was studied with 10 participant/requesters. All par-
ticipants observed that DialCrowd shortened the time Payment No. HITs %ofHITs
spent creating the study, and when asked to rate the <$7.25 24 44%
usefulness of this toolkit, participants responded with
$7.25-$10.00 9 17%
anaverageof4onascaleof1to5,with5beingbest.
>$10.00 21 39%
WhereasDialCrowd1.0focusedonhelpingrequesters
createHITsmoreefficiently,DialCrowd2.0addresses Table1: PaymentStatisticsforHITs
factors related to interaction and communication with
the workers that can affect the quality of the data ob-
tainedfromaHIT.Wehavedemonstratedthecommu- 2. RelatedWork
nity’s need for help with these two aspects in a recent
2.1. DialogDataCollection
study(Huynhetal.,2021). Inthisstudy,welookedat
thetasksrunningonAMToversevenconsecutivedays Tools such as ParlAI (Miller et al., 2017), ConvLab
in August 2021 to analyse their overall quality. The (Zhuetal.,2020),andMEEP(Arkhangorodskyetal.,
study examined only the natural language processing 2020)werecreatedtomakeHITcreationeasier.ParlAI
HITs (excluding computer vision, surveys, etc.) that and ConvLab are directly integrated with AMT with
werepresentedtoworkersatthistime.Inthesameway some coding required. MEEP is not integrated with
thatrequesterscangiveratingstoanindividualworker, AMT,buthasaWizard-of-Ozinterfacefordatacollec-
workersalsoratetherequestersandshareinformation tion. Inallthreecases, thesetoolsfocusonproviding
about them on their crowdsourcing forums and blogs pretrained models, datasets, and instruction on dialog
(Paolacci et al., 2010). A high requester rating will
attract more good workers while poor ratings and is- 1https://turkerview.com
2202
luJ
52
]LC.sc[
1v15521.7022:viXra
systemcreation. However,theydonotprovideaguide ersmayrelyontheirpreviousexperiencewithsimilar
tocommunicationandclaritywithworkersduringHIT tasks to create their own interpretations of what they
creation,whichDialCrowd2.0doesoffer. are to do (Chandler et al., 2013). To improve this as-
pectoftheinstructions,TaskMatehasworkersdiscover
2.2. HighQualityHITs
ambiguitiesintheinstructionsbeforetheentiretaskis
WedefineahighqualityHITasbeingaHITthatboth released (K. Chaithanya Manam et al., 2019). An au-
gathers high quality data and one that affords better tomaticmodelthatevaluatestheinstructionsmayalso
qualitycommunicationandrespectbetweenrequesters help a requester see how clear their instructions are
andworkers. Theworkerwantstodothetaskcorrectly (Nourietal.,2021).
whileminimizingtheamountoftimetheyspendonit
2.2.2. ProvidingExamples
(thus maximizing the amount they are paid per hour).
Theuseofwell-chosenexamplesandcounterexamples
Thus they will choose to work on the task that en-
withaccompanyingexplanationsofwhytheseparticu-
ables them to maintain this balance the best (Faradani
larexampleswerepresentedalsohelpsworkerstobet-
et al., 2011). The requester, on the other hand, wants
ter understand the task. Providing these examples has
togatherandaggregatemanyworkers’responsesinor-
beenshowntoimprovedataqualityoverothermethods
dertoproducegoodqualitydatatotrainorassesstheir
such as using gold standard questions (Doroudi et al.,
dialogsystemorforastudy(Wangetal.,2017).
2016).
Whiletherequesterisratingtheworkersandchoosing
workerswithahighratingtodotheirHIT,theworkers
2.2.3. Feedback
arealsoratingtherequestersinordertochoosewhose
Another way to improve communication with the
HITtoworkon. Ahighrequesterratingattractsmore
workers is to give them a text box at the end of each
goodworkerswhilepoorratingsandissuesincommu-
task where they can provide feedback (Kittur et al.,
nicationwiththeworkersrepelthebetterworkers. Our
2013). Onestudycreatedafeedbackdrop-downmenu
above-mentioned survey found that 35% of the 67 re-
thatgivesworkersalistofspecificreasonforthefeed-
questersstudiedwerejudgedbyworkersaspayingvery
back. While this is more restricted, it does allow the
badlyorpoorly(Huynhetal.,2021).
worker to pinpoint potential issues in the HIT more
ThispaperdefinesandimplementsfivecriteriathatDi-
rapidly(Kulkarnietal.,2012). Theuseofamenuhas
alCrowd 2.0 incorporates to contribute to a high qual-
notbeenshowntobecorrelatedwithanimmediatein-
ityproduction:includeclearinstructionsandexamples,
creaseindataquality.
allowworkerstoprovidefeedback,payworkersfairly,
filteroutlowqualitywork,andfilteroutlierdata. 2.2.4. FairPayment
Itisimportanttopayworkersfairlyfortheirtimeand
2.2.1. ProvidingInstructions
effort. Thereareconflictingstudiesonwhetherhigher
The first thing workers see when accessing a HIT is
paymentlevelsincreasethequalityofdata. Somestud-
the set of instructions. The requester can improve the
ies show significant increases in data quality (Aker et
task and attract the better workers by giving a high
al.,2012),someshowthatdataqualityincreasesupto
leveldescriptionofwhatthedatawillbeusedforand
a certain amount and then starts to decrease (Feng et
byprovidingclearandunambiguousinstructionsabout
al.,2009),whileothersshowthatdataqualitystaysthe
what to do (Chandler et al., 2013). Requesters can
samebutthatthespeedatwhichtheHITisfinishedis
alsoimprovetheinteractiveaspectsoftheinterfacethe
fasterwhenpaymentislower(MasonandWatts,2009)
workerseessolesstimeisspentscrollingandsearch-
(Buhrmesteretal.,2016)(Paolaccietal.,2010). Dial-
ing(Marcusetal.,2012)(Danieletal.,2018). Chenet
Crowdunderlinestheimportanceofpayingtheworkers
al 2011 (Chen et al., 2011) and Georgescu et al 2012
aminimumwageof$15/hr.
(Georgescu et al., 2012) have shown that attending to
interactiveissuesimprovesdataquality. 2.2.5. IdentifyingLowQuality
Ourabove-mentionedstudy(Huynhetal.,2021)found Thefiltermostfrequentlyusedforlowqualitydatade-
that28%ofthe56HITshadincomplete,unrelated,or tection has been gold standard HITs (HITs that have
ambiguousinstructions.MoredetailisshowninFigure previously been completed by the requester or some
2. expert) (Alabduljabbar and Al-Dossari, 2019). This
dataisusedtocheckwhethertheworker’sproduction
Instr. Issue No. HITs %ofHITs agreeswiththatoftheexpert(Allahbakhshetal.,2013)
CompletelyUnclear 0 0% (Chenetal.,2011)(Hsuehetal.,2009)(Sayeedetal.,
Incomplete 12 22% 2011)(Danieletal.,2018). ThesegoldstandardHITs
Unrelated 2 4% have been shown to have benefits beyond just assess-
Ambiguous/Vague 1 2% ingoneworker’sproduction. Theycanalsobeusedto
find consistent bias, or imbalanced datasets (Wang et
Table2: InstructionIssues al., 2011). Another filter uses duplicated data (Alab-
duljabbar and Al-Dossari, 2019). In this case the re-
When presented with ambiguous instructions, work- questerhasaworkerdothesameHITtwiceduringthe
courseoftheirwork. Thehopeisthattheworkerwill • Flexible appearance: DialCrowd 2.0 supports
give the same answer both times, thus demonstrating Markdown, which is a lightweight mark up lan-
intra-worker consistency. Both of these methods are, guage. Ithelpsrequestersformattexteasily. Dial-
evidently,notcostefficientsincerequestersareasking Crowd2.0alsoallowsrequesterstocustomizethe
forduplicatework,buttheydohelpimprovequality. styleofatask,e.g. backgroundcolor,textfont.
2.2.6. IdentifyingOutliers • Calculationofworkerpayment: Whilethisisnot
Yetanotheroptionistofilterthedatagatheredforout- aminorissue,itisdealtwithinasuccinctandef-
liers. This includes pattern matching (for example, if ficientmanner. Therequesterhasseveralpersons
aworkerhasselectedanswerchoiceAforeveryques- workonthegiventaskanddeterminestheaverage
tion),inordertomeasureanindividualworker’srelia- amount of time it has taken them to accomplish
bilityandagreementwiththerestoftheworkers’out- the task. That amount is entered and DialCrowd
put(Chandleretal.,2013)(Danieletal.,2018),aswell 2.0 uses this number to suggest worker payment,
as the amount of time spent (Rzeszotarski and Kittur, basedonanhourlywageof15dollarsanhour.
2012).
• Calculationofthenumberoftaskstodeploy: Di-
alCrowd2.0calculatesthenumberoftaskstode-
3. DialCrowd2.0
ployonAMTbasedonthedatatherequesterhas
Using what is known about best crowdsourcing prac- uploaded, the number of items/assignments per
tices, DialCrowd 2.0 helps requesters create HITs ac- taskunit,andthenumberoftaskunitspertask.
cordingtothosepractices. ThissectionpresentsDial-
• Built-inconsentformupload: DialCrowd2.0has
Crowd2.0,whichcanbeaccessedatthefollowinglink:
a built-in function for adding consent forms and
https://cmu-dialcrowd.herokuapp.com/.
theircorrespondingcheckboxes.
3.1. TaskCreation
3.1.1. Clarity
DialCrowd 2.0 has a user-friendly interface that helps
Instructionsthatareclearandunambiguoushelpmain-
requesterstocreatetasksmoreeasily. Afterconsulting
tainbetterbidirectionalcommunicationbetweenthere-
many publications that use crowdsourcing, four types
quester and the workers. While the requesters create
of tasks stood out as being the most often used. Thus
clearinstructions,theworkersgivefeedbackonhowto
task templates were created for these four task types
maketheHITbetter. Itisgoodpracticetopostasmall
and more templates can be added by the DialCrowd
subset the total HITs first. In this way resulting qual-
teamuponrequest:
itycanbeassessedandfeedbackcanbegatheredfrom
theworkers. Thisallowsforimprovementstobemade
• Interactive task: workers interact with a dialog
inthetaskbeforeitiscompletelydeployedandavoids
agent.Thistemplatecanbeusedtocollectconver-
the high cost of needing to repost a whole HIT when
sationwithdialogagentsfortrainingortoassess
theresultingdatahasbeenpoor.
dialogagents.
For requester-to-workers communication, DialCrowd
• Intentclassification: workersclassifytheintentof 2.0givesrequestersguidanceonhowtocomposeclear
anutterance. and complete instructions on the DialCrowd 2.0 con-
figuration page. There is also a link to the AMT best
• Entity classification: workers label the entities in
practices guide. DialCrowd 2.0 also explains the im-
anutterance.
portanceofgivingexamplesandcounterexamplesand
providesspaceforrequesterstoinputtheseitemsalong
• Quality annotation: workers assess the quality of
withexplanationsofwhybothtypesofexampleswere
adialogsystem’sresponsegivenacontextandre-
chosen.
sponsepair.
For worker feedback, DialCrowd 2.0 includes an op-
Requestersuseoneofthetemplatesandthenonlyneed tional feedback space which gives workers the oppor-
to fill out predefined configuration fields using Dial- tunity to point out instructions that are hard to follow,
Crowd2.0’sweb-basedgraphicaluserinterfacetocre- suggest better layout, note something that is not func-
ate a task. This eliminates the need to manually edit tioningcorrectlyetc. Whiletheabovementionedprac-
HTMLcode. Otherrelatedminorfeaturesarealsopro- ticeofpostingasmallamountoftasksfirstmayseem
videdasseeninFigures1,2,3,and4intheAppendix, counterintuitiveandonemightwonderifworkerswill
which show some examples of what the configuration actuallytakethetimetofilloutanoptionaltextboxif
pagelookslike.Figures5and6showwhattheworkers theyarenotpaidmore,(Mortensenetal.,2017)showed
see. thatworkersdoindeedprovidefeedback.
• Serializableconfiguration: Requesterscanupload 3.1.2. Low-QualityDataDetection
and download task configuration files in JSON Evenawell-constructedtaskmayyieldsomelowqual-
format. Ithelpsrequestersduplicatetasksorgen- ity work. This may be due to the work of bots, care-
eratetasksautomaticallywithprograms. lessness or fatigue on the part of a worker. For this,
DialCrowd2.0providesdetectionanalyticsthatinclude workercandoeachandeverytaskcorrectly(Danielet
qualitycontroltasksandmetricsforanomalydetection. al., 2018). In general a small number of golden items
It should be noted that the longer a HIT is active, the are given to the worker and a match to the experts al-
morelikelyitisthattherewillbebotsworkingonit. lowsthemtogoforwardtoworkontheHITs. Qualifi-
DialCrowd2.0offerstwotypesofqualitycontroltasks. cationtaskshavealreadybeenimplementedincrowd-
(1) it helps requesters include duplicated tasks, which sourcingplatformssuchasAMTandsodonotneedto
can be used to check individual worker consistency becoveredinDialCrowd2.0.
(intra-worker agreement). As mentioned above, the
data in a HIT is shown twice to a worker at different 5. FutureWork
places.Aconsistentworkerisexpectedtocompletethe
The DialCrowd team has connected the intent classi-
same HIT in the same way both times they see it. (2)
fication template of DialCrowd 2.0 to ParlAI. In this
DialCrowd2.0alsoenablesrequesterstouploadgolden
way, requesters will have access to the datasets and
dataasdescribedabove. Theworker’soutputiscom-
models ParlAI provides while having an interface to
paredtotheexperts’anddatathatdoesnotmatchcan
create HITs with DialCrowd 2.0. Future directions
be eliminated. If a given worker’s output frequently
could include the community creating new templates
does not match that of the expert, the totality of that
andcheckingtheminwithParlAI.
worker’sdatamaybeeliminated(buttheworkershould
still be paid for the time they spent trying to do the
6. Conclusion
task).
DialCrowd2.0alsohelpsrequestersdetectworkerbe- Clarityofinstructions,examples,fairpayment,andlow
haviorthatdiffersfromotherworkerswiththefollow- qualityfilteringareimportantconsiderationswhencre-
ingmetrics: ating HITs so that the data gathered is of the highest
quality possible. Studies have demonstrated the value
• Time: DialCrowd 2.0 tracks the amount of time ofthesefactors. DialCrowd2.0putsthesefactorsinto
spentbyeachworkeronthetask. DialCrowd2.0 practicebyprovidingasetoftoolsthatallowrequesters
flags work that is two standard deviations away tocollecthighqualitydata.
fromthemeantimetakenbyalloftheotherwork-
erstoaccomplishthetask. Averyshortperiodof Acknowledgments
time,forexample,mayindicatethepresenceofa
ThispaperissupportedbytheNationalScienceFoun-
bot,whileaverylongperiodoftimemayindicate
dationGraduateResearchFellowshipunderGrantNos.
unfamiliarity with the goal or the content of the
DGE1745016 and DGE2140739. It is also partly
task.
fundedbytheNationalScienceFoundationgrantCNS-
• Patterns:Aworker’sanswersmayrevealapattern 1512973. Theopinionsexpressedinthispaperdonot
inmultiplechoiceanswers. RespondingAtoev- necessarilyreflectthoseoftheNationalScienceFoun-
eryquestion,isanexampleofdatathatDialCrowd dation.
2.0willflag,thusprovidinganotherwaytodetect
potentialbots. 7. BibliographicalReferences
Aker, A., El-Haj, M., Albakour, M.-D., Kruschwitz,
• Agreement: For inter-worker agreement, Dial-
U., et al. (2012). Assessing crowdsourcing quality
Crowd2.0calculatestheagreementbetweeneach
throughobjectivetasks. InLREC,pages1456–1461.
workerandalltheotherworkersonthesameHIT
Citeseer.
usingCohen’sKappa.
Alabduljabbar, R. and Al-Dossari, H. (2019). A dy-
Foreachtask,DialCrowd2.0providesadatasummary namic selection approach for quality control mech-
page with all of the above information. This includes anisms in crowdsourcing. IEEE Access, 7:38644–
a table breaking down the summary numbers into in- 38656.
dividual results of these quality checks. It also in- Allahbakhsh, M., Benatallah, B., Ignjatovic, A.,
cludes individual Cohen’s Kappas between raters for Motahari-Nezhad, H. R., Bertino, E., and Dustdar,
each of the questions asked, as well as the Cohen’s S. (2013). Quality control in crowdsourcing sys-
Kappaamongratersforallofthequestionsasawhole. tems: Issuesanddirections. IEEEInternetComput-
ing,17(2):76–81.
4. Observations
Arkhangorodsky, A., Axelrod, A., Chu, C., Fang, S.,
Although DialCrowd 2.0 provides guidance for many Huang, Y., Nagesh, A., Shi, X., Zhang, B., and
aspects of good HIT creation, there are other aspects Knight,K. (2020). Meep: Anopen-sourceplatform
that it does not cover. Among those are the qualifica- for human-human dialog collection and end-to-end
tiontasks.Thesetasksassessthecapabilityofaworker agenttraining. arXivpreprintarXiv:2010.04747.
beforegivingthemaccesstoaHITbasedontheobser- Buhrmester,M.,Kwang,T.,andGosling,S.D. (2016).
vation that each worker’s skill set is different, so it is Amazon’s mechanical turk: A new source of inex-
better to check their work rather than assuming that a pensive,yethigh-qualitydata?
Chandler, J., Paolacci, G., and Mueller, P. (2013). architecture. IEEE Internet Computing, 16(5):28–
Risks and rewards of crowdsourcing marketplaces. 35.
In Handbook of human computation, pages 377– Lee, K., Zhao, T., Black, A. W., and Eskenazi, M.
392.Springer. (2018). Dialcrowd: A toolkit for easy dialog sys-
Chen, J. J., Menezes, N. J., Bradley, A. D., and tem assessment. In Proceedings of the 19th Annual
North, T. (2011). Opportunities for crowdsourc- SIGdialMeetingonDiscourseandDialogue,pages
ingresearchonamazonmechanicalturk. Interfaces, 245–248.
5(3):1. Marcus,A.,Karger,D.,Madden,S.,Miller,R.,andOh,
Daniel, F., Kucherbaev, P., Cappiello, C., Benatallah, S. (2012). Countingwiththecrowd. Proceedingsof
B.,andAllahbakhsh,M. (2018). Qualitycontrolin theVLDBEndowment,6(2):109–120.
crowdsourcing: A survey of quality attributes, as- Mason, W. and Watts, D. J. (2009). Financial incen-
sessment techniques, and assurance actions. ACM tivesandthe”performanceofcrowds”. InProceed-
ComputingSurveys(CSUR),51(1):1–40. ingsoftheACMSIGKDDworkshoponhumancom-
putation,pages77–85.
Doroudi, S., Kamar, E., Brunskill, E., and Horvitz,
E. (2016). Toward a learning science for complex Miller, A. H., Feng, W., Fisch, A., Lu, J., Batra,
crowdsourcing tasks. In Proceedings of the 2016 D., Bordes, A., Parikh, D., and Weston, J. (2017).
CHI Conference on Human Factors in Computing Parlai: A dialog research software platform. arXiv
Systems,pages2623–2634. preprintarXiv:1705.06476.
Mortensen, M. L., Adam, G. P., Trikalinos, T. A.,
Faradani,S.,Hartmann,B.,andIpeirotis,P.G. (2011).
Kraska, T., and Wallace, B. C. (2017). An ex-
What’stherightprice? pricingtasksforfinishingon
ploration of crowdsourcing citation screening for
time. In Workshops at the Twenty-Fifth AAAI Con-
systematic reviews. Research synthesis methods,
ferenceonArtificialIntelligence.
8(3):366–386.
Feng,D.,Besana,S.,andZajac,R. (2009). Acquiring
Nielsen, J. (1994). Usability engineering. Morgan
highqualitynon-expertknowledgefromon-demand
Kaufmann.
workforce. InProceedingsofthe2009Workshopon
Nouri,Z.,Gadiraju,U.,Engels,G.,andWachsmuth,H.
ThePeople’sWebMeetsNLP:CollaborativelyCon-
(2021). Whatisunclear? computationalassessment
structed Semantic Resources (People’s Web), pages
of task clarity in crowdsourcing. In Proceedings of
51–56.
the 32nd ACM Conference on Hypertext and Social
Georgescu, M., Pham, D. D., Firan, C. S., Nejdl, W.,
Media,pages165–175.
and Gaugaz, J. (2012). Map to humans and reduce
Paolacci, G., Chandler, J., and Ipeirotis, P. G. (2010).
error: crowdsourcing for deduplication applied to
Running experiments on amazon mechanical turk.
digital libraries. In Proceedings of the 21st ACM
JudgmentandDecisionmaking,5(5):411–419.
internationalconferenceonInformationandknowl-
Rzeszotarski, J. and Kittur, A. (2012). Crowdscape:
edgemanagement,pages1970–1974.
interactivelyvisualizinguserbehaviorandoutput. In
Hsueh, P.-Y., Melville, P., and Sindhwani, V. (2009).
Proceedingsofthe25thannualACMsymposiumon
Dataqualityfromcrowdsourcing:astudyofannota-
User interface software and technology, pages 55–
tionselectioncriteria. InProceedingsoftheNAACL
62.
HLT 2009 workshop on active learning for natural
Sayeed, A., Rusk, B., Petrov, M., Nguyen, H. C.,
languageprocessing,pages27–35.
Meyer, T. J., and Weinberg, A. (2011). Crowd-
Huynh, J., Bigham, J., and Eskenazi, M. (2021).
sourcing syntactic relatedness judgements for opin-
A survey of nlp-related crowdsourcing hits:
ion mining in the study of information technology
what works and what does not. arXiv preprint
adoption. Inproceedingsofthe5thACL-HLTwork-
arXiv:2111.05241.
shop on language technology for cultural heritage,
K. Chaithanya Manam, V., Jampani, D., Zaim, M., socialsciences,andhumanities,pages69–77.
Wu, M.-H., and J. Quinn, A. (2019). Taskmate: Wang,J.,Ipeirotis,P.G.,andProvost,F. (2011). Man-
Amechanismtoimprovethequalityofinstructions aging crowdsourcing workers. In The 2011 winter
in crowdsourcing. In Companion Proceedings of conference on business intelligence, pages 10–12.
The2019WorldWideWebConference,pages1121– Citeseer.
1130.
Wang,J.,Ipeirotis,P.G.,andProvost,F. (2017). Cost-
Kittur, A., Nickerson, J.V., Bernstein, M., Gerber, E., effectivequalityassuranceincrowdlabeling. Infor-
Shaw, A., Zimmerman, J., Lease, M., and Horton, mationSystemsResearch,28(1):137–158.
J. (2013). The future of crowd work. In Proceed- Zhu, Q., Zhang, Z., Fang, Y., Li, X., Takanobu, R.,
ingsofthe2013conferenceonComputersupported Li, J., Peng, B., Gao, J., Zhu, X., and Huang,
cooperativework,pages1301–1318. M. (2020). ConvLab-2: Anopen-sourcetoolkitfor
Kulkarni, A., Gutheim, P., Narula, P., Rolnitzky, D., building, evaluating, and diagnosing dialogue sys-
Parikh,T.,andHartmann,B. (2012). Mobileworks: tems. InProceedingsofthe58thAnnualMeetingof
Designing for quality in a managed crowdsourcing theAssociationforComputationalLinguistics: Sys-
tem Demonstrations, pages 142–149, Online, July. Appendix
AssociationforComputationalLinguistics.
Figures1, 2, and3arefromtheconfigurationpageof
DialCrowd2.0. Figures4and5arefromthetaskpage
thattheworkerssee.
Figure1: DialCrowd2.0willcalculateandsuggestaminimumpaymentfortheHITbasedonthetimeestimate
scaledto$15/hr
Figure2: FeedbackOptionfortheRequesters
Figure3: UsingExamplesandCounterexamplesForSpecificIntents
Figure4: InstructionsForSpecificIntents
Figure5: ExamplesForSpecificIntents

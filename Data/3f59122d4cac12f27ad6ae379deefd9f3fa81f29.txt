Prospection: Interpretable Plans From Language By Predicting the Future
Chris Paxton1, Yonatan Bisk2, Jesse Thomason2, Arunkumar Byravan2, Dieter Fox1,2
Abstract—High-levelhumaninstructionsoftencorrespondto Goal( ): World( ):
behaviors with multiple implicit steps. In order for robots to take the yellow object from
be useful in the real world, they must be able to to reason
the table and place it on
over both motions and intermediate goals implied by human
top of the red object
instructions.Inthiswork,weproposeaframeworkforlearning
representationsthatconvertfromanatural-languagecommand Next Prediction Interpretable Predicted Futures
to a sequence of intermediate goals for execution on a robot.
A key feature of this framework is prospection, training an
agent not just to correctly execute the prescribed command,
but to predict a horizon of consequences of an action before
taking it. We demonstrate the fidelity of plans generated by
our framework when interpreting real, crowd-sourced natural
subgoal grasp(yellow) move(yellow,red) release (yellow)
language commands for a robot in simulated scenes.
I. INTRODUCTION action
Arobotagentexecutingnaturallanguagecommandsmust
solve a series of problems. First, human language must be
translated to an understanding of intent. For example, the control
command pick up the yellow block and place it on top of
the red block corresponds to an intended change in world Fig. 1: DREAMCELLS convert instructions to interpretable
state that results in a yellow block on top of a red one. subgoals which can be visualized (Wˆ) and executed (θ).
Given that understanding, an agent must plan a sequence of
actions it can take to reach the target world state. In the
above example, this could be (move(yellow); grasp(yellow); We consider a simple pick-and-place task, where the goal
move(yellow, red); release(yellow)). Finally, these high level is to stack one block on top of another (Figure 1). This is
controlshavetobeexecutedintheworldbyservoinganarm limited in that there are only a few high-level actions the
to appropriate positions and controlling the gripper. robot can take in a given world. Still, it proves challenging
Eachoftheseproblemsischallengingandhasbeeninves- when specified with real, crowd-sourced natural language.
tigatedbyexistingresearch.Commonly,apipelineapproach In short, our contributions are:
is used, where each problem is addressed sequentially, and • A language embedding rich enough to specify a se-
the outputs of one are fed to the next. In the example above, quence of actions to achieve a task-level goal.
the semantic understanding that the goal is on(yellow, red) • A DREAMCELL architecture to predict future world
from natural language is passed to a high level controller. states from language and raw sensor observations.
To simplify high level control prediction, robot perception • An approach to convert a task plan output from these
is often augmented with visual semantics information, such DREAMCELLS into low-level executions.
as oracle object detections, bounding boxes, or 6d poses [1],
[2]. In this work, we instead train a single model end-to-end II. RELATEDWORK
that takes natural language and raw pixels, depths, and joint
Ourworkdrawsinspirationfromrecenteffortsonlearning
states to produce low-level controls to accomplish a goal.
abstract representations for planning [4]. We build primarily
Additionally,ratherthantreatthepipelineproblemsabove
on work in planning and natural language processing, with
independently, we introduce a prospection component to an
important future work in manipulation.
agent’s training and inference, which facilitates “dreaming”
Communicating control and goals has traditionally been
about the consequences of chosen high level actions in the
accomplished by specifying high level operations [5], [1],
current scene. Prospection is the ability to reason about the
[6], via formal languages like the Problem Domain De-
consequences of future actions without executing them [3].
scription Language (PDDL) [7] or as a Hierarchical Task
Our approach allows the agent to predict whether its high
Network [8]. Such systems provide a straightforward way to
level actions will lead to undesirable world states.
compose black-box operations to solve problems. While we
maintain an interpretable intermediate layer, our interface is
1NVIDIA,USA
natural language, most akin to [9], [10] though we work in
2PaulG.AllenSchoolofComputerScienceandEngineering,University
ofWashington,Seattle,USA a fully end-to-end differentiable paradigm where embedded
9102
raM
02
]IA.sc[
1v90380.3091:viXra
Predictor
+
RNN
Dream Cell
Fig.2:Ateachtimestep,themodelreceivesanLSTMencodedlanguagevectorL(cid:126),theinitialworldstateW ,andthecurrent
0
world state W . Using these, it predicts the next world state Wˆ and sub-goal G .
t t+1 t
languagerepresentationsarelearnedalongsidevisualencod- III. PROBLEMDESCRIPTION
ings of the world.
Given a natural language command and raw sensor read-
ings for a scene, the task is to issue a sequence of low level
Our work is also motivated by the Universal Planning
controls to a robotic arm that accomplish the intended task.
Networkswhichlearnanembeddingforimagesandaworld
Specification.A DREAMCELL takesintheinitialW
0
and
state vector used to generate motion plans to goals specified
currentW simulation-basedstateobservationsandanatural-
t
via a target image [11]. That work learns a distance metric
language sentence s to produce a sequence of intermediate,
fromthecurrentstateimagetothetargetimagewhichisused
latent-spacegoalsz ,...,z forthispick-and-placetaskup
i i+h
to perform rollouts for training and inference. While learned
to horizon h. Each goal is a semantically meaningful break
genericrepresentationshavenotionsofagencyandplanning,
point in the execution, e.g., a completed grasp on the target.
the produced plans lack human interpretability, which may
DREAMCELLS make three predictions at every time step:
be important to modularity and generalization [12]. Neural
1) A sequence of subgoals predictions, representing the
approaches and scaling robotic learning within simulation
next high level actions out to some planning horizon;
have become common as they allow for end-to-end training
2) Asequenceofhiddenstaterepresentationsz ,...,z
and can easily acquire more data (often from multiple do- i i+h
representing the results of these subgoals; and
mains)thanotherwisepossibleonaphysicaldevice[2].This
3) The end effector command θ that parameterizes the
has proven particularly important for RL-based approaches
low-level controller for task execution, consisting of a
[13] and interpretability [14]. More generally there is a
6DOF pose and gripper command.
growing literature on learning deep representations that can
These predictions allow us to learn an interpretable, exe-
be used to accomplish local control tasks or simple object
cutable representation for hallucinating future world states.
manipulations [15], [16], [11], [17].
Metrics. We measure both extrinsic performance on the
Core to our contribution is simulating the future actions taskandintrinsicperformanceofDREAMCELLcomponents.
and dynamics of our system. High level process simulation Extrinsically,weevaluatehowwelltherobotagentcompletes
has been used in the NLP literature [18] without sensor thepick-and-placetask.Werecordbinarytasksuccess/failure
data. Simultaneously, prospection has been used before in as whether the target block to be moved is dropped within
RL, often as a means of model-based control [19], [17], a threshold of its intended position based on the language
[20], and for fine control tasks like cutting [19]. In addition, command. We also record the average mean-squared error
our approach is compatible with work in Visual Robot of the predicted end effector goal at each step of the task.
Task Planning [4], which shows that prospective subgoal This metric penalizes moving the wrong block while giving
predictions can be used to generate task plans. partial credit for moving the right block to towards the right
place,evenifitneverarrivesthereorisplacedunstably(e.g.,
Complementary to our work is the growing literature in if it falls off the target block after release). Intrinsically, we
NLP focusing on complex grounding instructions with eso- evaluate the language-to-action component by how closely
tericreferencesandotherlongtaillinguisticphenomena[21], the predicted sequence of subgoals matches ground truth
[22]. Natural language communication with robotics also execution.
allows for learning joint multimodal representations [23],
IV. APPROACH
[24] which harness the unique perceptual and manipulation
capabilities of robotics. Wetrainthesystemend-to-endusingsimulateddata.This
allows us to automatically generate training sequences for
While accurate grasping and placement is not a focus of bothimagesandhigh-levelsubgoals.Subgoalstaketheform
our work, it has been explored in the literature [25], [26], of semantic predicates like grasp and move with block
[27]. In particular, high-precision grasping with deep neural arguments. To simplify notation, throughout the paper, we
networks generally takes the form of predicting a grasp refer to the union of RGB, pose, and depth images with
success classifier [25], [26]. the single world state variable W. At every timestep the
tile Predictor
cconv(5) cconv(5) cconv(5) deconv(5) cconv(1)
1x1x64 8x6x64 +pad
cat
8x6x64
8x6x128 8x6x64 4x3x128 8x6x64 8x6x64
+
Fig. 3: Diagram of a single prediction cell. The prediction cell predicts a change in hidden state ∆z, and is an important
component of the DREAMCELL, used both for visualizing possible futures and for predicting the goal of a particular motion
for execution on our robot.
model is provided the current world observation (W ), a tuplesateachtimestep,toahorizonoflengthfive.Wedivide
t
description of the goal configuration in natural language thesubgoalG into:verbtheactiontobetaken,to objthe
t
(encoded as L(cid:126)). In practice W also includes the initial state object to servo towards, and with obj the optional object
t
W to capture changes over time. All aspects of the model inhand(e.g.,move(red,yellow)movestheyellowblockin
0
(including the encoders for both language and the world) hand to the target red block).
are trained together. The model is trained on supervised During training, we use cross entropy loss on all 5x3
demonstration data collected from an expert policy as per generations. The first timestep in the RNN is passed the
previous work [15], [4]. current hidden state and the At each timestep the RNN
The basic unit in our model is the DREAMCELL (Fig. 2) cell is passed the prediction from the previous timestep.
which produces a sub-goal and a corresponding predicted As is common practice in the language modeling literature
imageofthearm’spositionatthenexttimestep.Thisformu- [29], we tie the emission and embedding matrix parameters.
lation allows for recurrent chaining of cells to rollout future Traditionally,thisisachievedbysimplytransposingasingle
goals and states (Fig. 1). Specifically, because the output of matrix. Our model produces tuples by passing the hidden
the DREAMCELL includes a deconvolved hallucination of vector through three different feed-forward layers, so, to
the next world state (Wˆ ) we can simply continue to run re-embed predictions we multiply by the three transposed
t+1
the network forward, where true observations are replaced embedding matrices and average the outputs to reconstruct
with the network’s predictions. Our cell has two outputs at an embedding.
every timestep t: 1. Subgoals (G ) and 2. Predicted Worlds
t
(Wˆ ).Weprovideintrinsicevaluationsonfutureprediction B. DREAMCELL World Predictor Module
t+1
performance in Section VI. The prediction cell, shown in Fig. 3, takes in the current
At inference time, we generate a task plan by rolling out hiddenstatez andthepredictedsubgoalG .Eachprediction
t t
multiple DREAMCELL timestepsintoapossiblefuturegiven modeloutputsapredictedlatent-statesubgoalzˆ ,suchthat
t+1
state observations, z. The core of our approach is that these P(z ,G )=zˆ .Ineffect,welearnamany-to-onemapping
t t t+1
subgoals are converted into estimated world states Wˆ and across multiple timesteps, all of which need to produce the
associated end effector goals θ, which are fed into a lower same goal. We should also be able to roll this simulation
level controller π that will convert them into trajectories. forward in time in order to visualize future actions. Training
this prediction space is a difficult problem and requires a
A. DREAMCELL Subgoal Module
complex loss function involving multiple components.
The subgoal module predicts the next subgoal from the Thepredictioncellisasimpleautoencodermappinginputs
current world state and the language instruction. It is formu- W to and from a learned latent space, as show in Fig. 2.
latedsimilarlytoimagecaptioningandsequencetosequence World observations W and W are combined into a single
t 0
prediction. First, we use an LSTM [28] to encode the estimated latentstate z . Thevector containingthe predicted
t
goal as expressed in language. Words are embedded as 64 subgoalG istiledontothisstate.Weuseabottleneckwithin
t
dimensionalvectorsinitializedrandomly.Weconcatenatethe each prediction cell to force information to propagate across
final hidden state (L(cid:126)) with the output of our world encoder the entire predicted image, and then estimate a change in
(z ) as the initial hidden state of a new LSTM cell for latent state ∆z such that zˆ =z +∆z.
t t+1 t
decoding. When visualizing the predicted image Wˆ , we use a
t+1
WegenerateanoutputofG =(verb,to obj,with obj) decoder consisting of a series of 5x5 convolutions and
t
predicted
current
*
1-hot
selector
Fig. 4: The actor module takes in a hidden state and associated subgoal API and converts this to a motion goal, which
is represented as a Cartesian (x,y,z) position, a unit quaternion q = (a,b,c,d), and a gripper command g ∈ (0,1). This
motion goal can then be sent to the control module for execution.
bilinear interpolation for upsampling. motivated recent work on GANs [31]. These are often
unstable, so we propose an alternative solution specialized
C. Actor Module
to our problem. Since each successful high-level action
Theactormodule,showninFig.4,predictstheparameters has a predictable result, we jointly train a classifier that
of an action that can be executed on the robot. Specifically, will recover the subgoal associated with each successive
it takes in z and the current high-level action and predicts a high-level action Gˆ . We use C (z ) as the classifier loss,
t t G t
destination end effector pose that corresponds to the robot’s minimizingcrossentropybetweentherecoveredestimateGˆ
t
position at z . and ground truth G .
t t
The architecture is a simple set of convolutions: the high- Actor LossInsteadofestimatingthefulljointstateofthe
level action is concatenated with the current z t as in the robot as the result of a high-level action, our Actor module
prediction module, then a set of three 3x3 convolutions estimates the end-effector pose θ associated with sugboal
t
with 64, 128, and 256 filters, each followed by a 2x2 G .
t
max pool. This is followed by a dropout and a single 512
These poses are represented as θ = (pˆ,qˆ,gˆ), where pˆ
dimensional fully connected layer, and then to N ×8
verbs is the Cartesian position, qˆ is a quaternion describing the
outputs, predicting gripper command g ∈ (0,1), Cartesian
orientation, and gˆ ∈ (0,1) is the gripper command. When
end effector position, and a unit quaternion for each high-
regressingtoposes,weuseamixtureoftheL2lossbetween
level action verb. The gripper command uses a sigmoid
Cartesian position and a loss derived from the quaternion
activation where 0 represents closed and 1 represents open,
angulardistance(tocapturebothspatialandrotationalerror).
and Cartesian end effector position uses a tanh activation
The angle between two quaternions qˆ and qˆ is given as:
1 2
function. All pose values are normalized to be in (−1,1).
A one-hot attention over action verbs chooses which pose
ω =cos−1(2(cid:104)qˆ ,qˆ (cid:105)2).
andgrippercommandshouldbeexecuted.Ineffect,theactor 1 2
learns to compute a set of pose features for predicting the
nextmanipulationgoalandlearnsasimpleperceptronmodel To avoid computing the inverse cosine as a part of the loss,
foreachactionverbinordertochoosewherethearmshould we use a squared distance metric. In addition, normalize
go and whether the gripper should be opened or closed after gripper commands to be between 0 and 1, where 0 is closed
the motion is complete. and 1 is open, and trained with an additional L2 loss on
predicted gripper commands. Given estimated pose θˆ and
D. Training
final pose θ, we calculate pose estimation loss:
Wetraintheencoderanddecoderjointlywhentrainingthe
PredictionandActormodulesandoptimizewithAdam[30], C (θˆ,θ)=λ (cid:8) (cid:107)pˆ−p(cid:107)2+(1−(cid:104)qˆ,q(cid:105))+(cid:107)gˆ−g(cid:107)2(cid:9) .
actor actor 2 2
usinganinitiallearningrateof1e−3.Wefixthelatentstate
encoder and decoder functions after this step, then use the
learned hidden space to train the Subgoal module. Object Pose Estimation Loss It is important to ensure
Image Reconstruction Loss This determines how well that our learned latent states z t capture all the necessary in-
ourmodelcanreconstructanimagefromagivenhiddenstate formationtoperformthetask.Assuch,weuseanaugmented
z t, and is trained on the output of our visualization module. loss C obj(z t) that predicts the position of each of the four
WeusedanL2lossonpixelsbothforRGBanddepth.Depth blocks in the scene at the observed frame. This information
values were capped at 2 meters and were normalized to be is not used at test time, but is structurally identical to the
between 0 and 1. pose estimation loss C pose
Subgoal Recovery Loss Image reconstruction losses are Combined Prediction Loss The final loss function for
often insufficient for capturing fine details. This issue has predictingtheeffectsofperformingasequenceofhigh-level
L2distanceincm↓ Success↑
Align Grasp Lift Move Release Rate
Oracle 0.04 0.03 0.04 0.04 0.04 98.4%
GTAction 0.32 0.31 0.48 0.63 0.63 90.4%
Template 0.32 0.39 0.47 0.65 0.65 87.8%
RealLang 0.51 1.23 1.50 2.39 2.40 77.1%
put the blue cube onto the yellow cube
stack the top most cube onto the second highest cube TABLE I: L2 distances and accuracy when executing plans
generated from either ambiguous natural language instruc-
Fig. 5: Human participants on Mechanical Turk gave two
tions or unambiguous template language.
commands for how to create the target image (right) from
the initial image (left).
align with the top of a random block, grasp that block
andclosethegripper,lifttheblockoffthetable,movethe
actions is then: block to atop another block, and then release the block.
We collected natural language commands from human
C(Zˆ)=λ (cid:107)Wˆ −W (cid:107)2+C (z )+
W t t 2 obj t annotatorsthroughtheMechanicalTurkcrowd-sourcingplat-
(cid:32) (cid:33)
(cid:88) λ W(cid:107)Wˆ t+i−W t+i(cid:107)2 2 . form.1 Annotatorswereshowntwosceneimages:onebefore
+ C (θˆ ,θ )+C (Gˆ ,G ) and one after a block had been stacked on another block.
actor t+i t+i G t+i t+i
i∈h
They were instructed to give two distinct commands that
E. Execution would let someone create the second scene from the first
When executing in a new environment, the robot agent (Figure 5), and were paid $0.25 per such annotation. For
takes in the current world state W and a natural language eachofour2370successfultrials,weobtainedtwolanguage
0
instructionL.Theagentcomputesafuturepredictionusinga commands describing the high level pick-and-place goal.
DREAMCELL, by rolling out predicted goals G t,...,G
t+H
On average, commands are 11 words long.2 We compare
which generate latent space subgoals. These subgoals can Turk data to unambiguous templated language that was
then be visualized to provide insight into how the robot procedurally generated from the manipulated blocks.
expects the task to progress. This also illuminates misunder-
VI. RESULTS
standings and limitations of the system (see Analysis VI).
We ran a set of experiments on our simulation, and
Thesystemgeneratesnewprospectiveplansouttoagiven
computedtaskexecutionsuccessrate.Weanalyzetheperfor-
planning horizon. After predicting the next subgoal z , it
t+1
mance of the Subgoal and Predictor modules given different
willthenusetheactortoestimatethenextmotiongoalθ .
t+1
classes of language input.
Thisgoalissenttothelow-levelexecutionsystem,whichin
our case is a traditional motion planner that does not have A. Execution Results
knowledge of object positions. In our case, the planner used
Finally, we test our model on a set of held-out scenarios,
was RRT-connect [32], via MoveIt
and compare to ground-truth execution. We compared accu-
In the future, these subgoals shown to the user, who can
racy of the estimated motion under each of three conditions:
give the final confirmation on whether or not to execute this
withoraclesubgoalsG fromthetestdata,withunambiguous
hallucinatedtaskplanifitaccomplisheswhattheyrequested. t
templated language, and with natural language. Position
Alternatively, the user could input a new L, or the agent
accuracy results are shown in Table I.
could sample a new sequence of goals.
In all cases, we compute an execution plan Gˆ ,...,Gˆ at
1 5
V. EXPERIMENTSETUP the beginning and use our Predictor and Actor networks to
followthisexecutionplanuntilallstepshavebeenexecuted.
We performed a number of variations on a simple block
Results are shown in Table I. We count successes when the
stackingtask.Allexperimentswereperformedinsimulation.
block was moved to within 1.5 cm of the target in the x and
We collected 5015 trials using a sub-optimal expert policy,
y direction, and 0.5 cm z of the final position from which it
of which 2370 were successes. Our model was trained only
was dropped.
on successful examples.
Weseeonlyahandfuloffailureswhentherobotwassent
We generated trials using a simple simulation of an ABB
to ground truth “oracle” poses, due to stochastic interactions
YuMirobotpickingup3.5cmcubes.Therewerefourcubes,
between the objects and gripper and randomness at the
one of each color: red, green, yellow, blue. When collecting
control level. 88.1% of execution with ground truth actions,
data, we first randomly compute a table position within a
88.0% of unambiguous templated language, and 84.0% of
50 cm box centered in front of the robot. Blocks were
natural language were successfully able to pick up a block
randomly placed in non-intersecting positions on this table.
and put it in the right area—indicating a high level of
Thearm’sinitialpositionwasalsorandomizedtoanareaoff
precisionindependentofthetaskspecification.Graspsuccess
the right side of the table.
rates tended to be very high. The most dramatic failures we
We selected manipulation goals at random, and provided
a simple expert policy which moved to pick up each object 1https://www.mturk.com/
using an RRT motion planner. The plan has five steps: 2Participantsused389uniquewordsafterlowercasingandtokenization.
Fig.6:Comparisonofgeneratedsubgoalpredictions.Toptworows:RGBanddepthimagesgeneratedfrompredictedsubgoal
G. Bottom two rows: RGB and depth images generated from ground truth G from training data.
Horizon h=1 h=2 h=3 h=4 h=5 that the model properly learned the correct sequence of
Template actions that should be executed. Accuracy farther out into
Verb 84.0% 87.9% 93.0% 97.4% 100.0% the future is stable because the network knows when and
ToObject 91.5% 90.4% 93.0% 97.4% 100.0%
how a sequence should end.
WithObject 93.7% 91.8% 94.4% 96.8% 100.0%
There are two major sources of error we observe in
Overall 82.9% 86.8% 92.5% 97.3% 100.0%
these examples. First, the difference in accuracy between
RealLang
MechanicalTurklanguageandtemplatedlanguageislargely
Verb 84.2% 87.8% 92.9% 97.2% 100.0%
ToObject 87.8% 87.4% 89.8% 93.9% 98.2% explained by the ambiguity and underspecificity in Turk
WithObject 91.4% 90.1% 92.5% 96.4% 100.0% commands (e.g., not specifying a destination after a grasp).
Overall 79.6% 83.8% 89.1% 93.7% 98.2% Second, the overall error is largely due to sequence error at
transitionpoints,wheremultiplepossibleactionsarereason-
TABLEII:Subgoalpredictionaccuracy(↑)atdifferenthori-
able depending on whether or not the low level control has
zons with templated versus natural language. We see higher
arrivedatitsdestination.Thisfurthersupportsourhypothesis
accuracy as we move closer to the end of the task, when the
that we need to reason about all three sub-problems jointly.
space of possible remaining plans is less ambiguous.
observedweresituationswhereoneormorenecessaryblocks C. Prediction Model
wasoutofthecamera’sviewpoint,inwhichcaseourvision-
The role of the prediction model is to generate subgoal
based system fails by default.
predictions zˆ ,...,zˆ representing the h next actions
The similar performance between templated language and t+1 t+h
that the robot can take in order to perform the task. Fig. 6
ground truth actions suggests that unambiguous, templated
showsanexampleofonecourseoferrorweseeduringthese
languageisinsufficienttodemonstratethelanguagelearning
prediction rollouts. The top two rows show a sequence of
capabilities of our system. We find our method is robust
predictions coming from the sequence to sequence model,
to real natural language from Mechanical Turk workers,
while the bottom row shows predictions using ground truth
achieving 95% of the success rate seen on unambiguous
actions from our data. In this case, we see that the sequence
templates.
to sequence model started the grasp verb earlier than the
B. Subgoal Module groundtruthexecution,butbothmodelsgenerategoodimage
We analyze the language learning component of our predictions.
model. A full breakdown of subgoal prediction accuracy AswecanseeinTableI,thereispersistentlysomeerrorin
is given in Table II. Performance was comparable between the low-level predictions from our actor module, even when
templatedlanguageandnaturallanguagedatacollectedfrom given oracle arguments Average placement error increases
Amazon Mechanical Turk. We see that it is more difficult to as we move away from the ground-truth arguments. Often
make accurate predictions on real language data. Addition- failures occur because the object is not clearly visible in the
ally, accuracy is remarkably consistent over time, meaning first frame.
detciderP
elcarO
We would like to thank Jonathan Tremblay for valuable
discussions.
REFERENCES
[1] C. Paxton, A. Hundt, F. Jonathan, K. Guerin, and G. D. Hager,
“CoSTAR: Instructing collaborative robots with behavior trees and
vision,” Robotics and Automation (ICRA), 2017 IEEE International
Conferenceon,2017.
[2] D.Xu,S.Nair,Y.Zhu,J.Gao,A.Garg,L.Fei-Fei,andS.Savarese,
“Neuraltaskprogramming:Learningtogeneralizeacrosshierarchical
tasks,”InternationalConferenceonRoboticsandAutomation(ICRA),
2018.
[3] D.T.GilbertandT.D.Wilson,“Prospection:Experiencingthefuture,”
Science,vol.317,no.5843,pp.1351–1354,2007.
[4] C.Paxton,Y.Barnoy,K.D.Katyal,R.Arora,andG.D.Hager,“Visual
robot task planning,” in International Conference on Robotics and
Automation(ICRA),2019.
[5] E. Guizzo. (2017) Rethink’s robots get massive software
upgrade, rodney brooks “so excited”. [Online]. Avail-
able: https://spectrum.ieee.org/automaton/robotics/industrial-robots/
rethink-robots-get-massive-software-upgrade
[6] C.Paxton,F.Jonathan,A.Hundt,B.Mutlu,andG.D.Hager,“Eval-
Fig. 7: Results of different simulated executions. Successful uatingmethodsforend-usercreationofrobottaskplans,”Intelligent
grasps (top row) can be undermined by small errors in the RobotsandSystems(IROS),2018IEEEInternationalConferenceon,
2018.
low-level actor network that compound to create accuracy
[7] M. Ghallab, C. Knoblock, D. Wilkins, A. Barrett, D. Christian-
issues at execution time (bottom row). son, M. Friedman, C. Kwok, K. Golden, S. Penberthy, D. E.
Smith et al., “PDDL-the planning domain definition language,”
http://www.citeulike.org/group/13785/article/4097279,1998.
Our reconstruction results have another advantage, how-
[8] K. Erol, J. Hendler, and D. S. Nau, “Htn planning: Complexity and
ever, as seen in Fig. 6: they are clearly interpretable, which expressivity,”inAAAI,vol.94,1994,pp.1123–1128.
means that the robot can readily justify its decisions even [9] R. Paul, J. Arkin, N. Roy, and T. Howard, “Efficient grounding of
abstract spatial concepts for natural language interaction with robot
when it does make a mistake, facilitating a human user
manipulators,” in Proceedings of the 2016 Robotics: Science and
providing a new instruction that considers this mistake. SystemsConference,June2016.
Overall, these results show that we can learn representations [10] D.Arumugam,S.Karamcheti,N.Gopalan,L.L.Wong,andS.Tellex,
“Accurately and efficiently interpreting human-robot instructions of
forataskthataresufficientforplanningandexecutionpurely
varying granularities,” in Proceedings of the 2017 Robotics: Science
from language and raw sensory data. andSystemsConference,2017.
We performed an ablation analysis on the best prediction [11] A. Srinivas, A. Jabri, P. Abbeel, and S. Levine, “Universal planning
networks,”inProceedingsoftheInternationalConferenceinMachine
models to determine how much they use information from
Learning(ICML),2018.
different layers. In particular, we see similar performance [12] M. Garnelo, K. Arulkumaran, and M. Shanahan, “Towards deep
when training without the image loss (89.5% successful symbolic reinforcement learning,” in Deep Reinforcement Learning
WorkshopatNIPS,2016.
on held out test data) and without the image and object
[13] O.Nachum,S.Gu,H.Lee,andS.Levine,“Data-efficienthierarchical
losses (88.6% successful). This suggests that our image reinforcementlearning,”2018.
reconstruction loss may help, and certainly does not have [14] J. Tremblay, T. To, A. Molchanov, S. Tyree, J. Kautz, and S. Birch-
field, “Synthetically trained networks for learning human-readable
a negative impact.
plans from real-world demonstrations,” International Conference on
RoboticsandAutomation(ICRA),2018.
VII. CONCLUSIONS [15] A.ByravanandD.Fox,“Se3-nets:Learningrigidbodymotionusing
deep neural networks,” in Robotics and Automation (ICRA), 2017
We present an approach for inferring interpretable plans
IEEEInternationalConferenceon. IEEE,2017,pp.173–180.
from natural language and raw sensor input using prospec- [16] C. Finn and S. Levine, “Deep visual foresight for planning robot
tion. Our DREAMCELL architecture predicts future world motion,”inRoboticsandAutomation(ICRA),2017IEEEInternational
Conferenceon. IEEE,2017,pp.2786–2793.
statesfromlanguageandrawsensorobservations,facilitating
[17] T. Weber, S. Racanie`re, D. P. Reichert, L. Buesing, A. Guez, D. J.
highlevelplaninferencethatcanbeconvertedintolow-level Rezende,A.P.Badia,O.Vinyals,N.Heess,Y.Lietal.,“Imagination-
execution.Prospectionenablesend-to-endplaninferencethat augmented agents for deep reinforcement learning,” arXiv preprint
arXiv:1707.06203,2017.
is agnostic to the nature of sensory input and low-level
[18] A.Bosselut,L.Omer,A.Holtzmann,C.Ennis,D.Fox,andY.Choi,
controller modules. In the future, using this architecture to “Simulatingactiondynamicswithneuralprocessnetworks,”Interna-
bootstrap language understanding for execution on a real tionalConferenceonLearningRepresentations,2018.
[19] I. Lenz, R. A. Knepper, and A. Saxena, “DeepMPC: Learning deep
robot using sim-to-real transfer techniques could facilitate
latentfeaturesformodelpredictivecontrol.”inRobotics:Scienceand
end-to-end control on a physical platform. Systems,2015.
[20] R. Pascanu, Y. Li, O. Vinyals, N. Heess, L. Buesing, S. Racanie`re,
VIII. ACKNOWLEDGEMENTS D.Reichert,T.Weber,D.Wierstra,andP.Battaglia,“Learningmodel-
basedplanningfromscratch,”arXivpreprintarXiv:1707.06170,2017.
This work was funded in part by the National Science [21] Y. Bisk, D. Yuret, and D. Marcu, “Natural language communication
with robots,” in Proceedings of the 2016 Conference of the North
Foundation under contract no. NSF-NRI-1637479, and the
American Chapter of the Association for Computational Linguistics:
DARPA CwC program through ARO (W911NF-15-1-0543). HumanLanguageTechnologies,2016,pp.751–761.
[22] Y. Bisk, K. J. Shih, Y. Choi, and D. Marcu, “Learning interpretable
spatial operations in a rich 3d blocks world,” in Proceedings of the
Thirty-SecondConferenceonArtificialIntelligence(AAAI-18),2017.
[23] J. Thomason, J. Sinapov, M. Svetlik, P. Stone, and R. Mooney,
“Learning multi-modal grounded linguistic semantics by playing “I
spy”,” in Proceedings of the 25th International Joint Conference on
ArtificialIntelligence(IJCAI-16),July2016,pp.3477–3483.
[24] J. Thomason, J. Sinapov, R. J. Mooney, and P. Stone, “Guiding
exploratorybehaviorsformulti-modalgroundingoflinguisticdescrip-
tions,”Intelligence(AAAI-18),2018.
[25] S.Levine,P.Pastor,A.Krizhevsky,andD.Quillen,“Learninghand-
eye coordination for robotic grasping with deep learning and large-
scaledatacollection,”inProceedingsoftheInternationalSymposium
onExperimentalRobotics,2016.
[26] J.Mahler,J.Liang,S.Niyaz,M.Laskey,R.Doan,X.Liu,J.A.Ojea,
and K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps
withsyntheticpointcloudsandanalyticgraspmetrics,”2017.
[27] D. Morrison, P. Corke, and J. Leitner, “Closing the loop for robotic
grasping:Areal-time,generativegraspsynthesisapproach,”Proceed-
ingsofthe2018Robotics:ScienceandSystemsConference,2018.
[28] S.HochreiterandJ.Schmidhuber,“Longshort-termmemory,”Neural
Computation,vol.9,no.8,pp.1735–1780,1997.
[29] O. Press and L. Wolf, “Using the output embedding to improve
language models,” in Proceedings of the 15th Conference of the
European Chapter of the Association for Computational Linguistics:
Volume2,ShortPapers. AssociationforComputationalLinguistics,
April2017,pp.157–163.
[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,” in 3rd International Conference for Learning Representations,
2015.
[31] I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,
S.Ozair,A.Courville,andY.Bengio,“Generativeadversarialnets,”in
Proceedingsofthe2014ConferenceonNeuralInformationProcessing
Systems,2014,pp.2672–2680.
[32] J.J.KuffnerJrandS.M.LaValle,“Rrt-connect:Anefficientapproach
tosingle-querypathplanning,”inICRA,vol.2,2000.
A-I. TEMPLATEDVSREALLANGUAGE
Table A1 contains examples of the different types of language that occur when asking humans to describe the action
versus using templated language.
Before After Template Human
place yellow block on the red stackwarmcolors
block
Unknownconcepts
stack the red block on the moveredrighttosamexandyaxisas
greenone green
CoordinateSystem
placegreenontheyellowone move the green box forward three
spaces
Spatiallanguage
stacktheblueoneonyellow take the blue block in your hand and
raiseitabovethetable.movetheblock
backandtotherightuntilitisdirectly
abovetheyellowblock.lowertheblue
blockdownontotheyellowblockand
releaseit
Latentdetailsabouthandmovement
put the yellow one on the movetheyellowcubetotherightuntil
greenblock itisontopofthegreencubewiththe
front half of the yellow cube touching
thefarhalfofthetopofthegreencube
Denotesspecificnuance
TABLEA1:Abovearetheinitialandfinalvisualframesforeachtask,nexttothetemplatelanguageandhumandescriptions
forexamplesfromourtrainingset.Theseexamplesillustratewhyitcanbesodifficultforamodeltopredictspecificmotions
that correspond to a particular natural language command, and further justify our approach for visualizing robot actions
before execution. Specific reasons why each description is difficult to ground are indicated in bold
A-II. PREDICTIONRESULTS
OneadvantageofproposedDREAMCELLsystemisthatitallowsustogeneratemultiplehallucinationsofpossiblefutures.
Here, we show example plans generated from four unseen test environments, given a natural-language prompt. We show
predictions for the first four high level actions: align, grasp, lift, and move to. Environments and trials were chosen
at random, and should be indicative of performance on the prospection problem.
Prompt: “put red on blue”
1.
2.
3.
4.
Fig. A1: Example showing predicted plans given straightforward language.
Prompt: “put blue on the other one”
1.
2.
3.
4.
Fig. A2: Example showing predicted plans given underspecified language. The system always picks up the blue block, and
correctly places it on the “other” one; however, it always chooses red. Handling ambiguity is left to future work.
Prompt: “take the red block in your hand and lift it off the table and move it
to the blue block and lower it and open your hand”
1.
2.
3.
4.
Fig. A3: Example showing predicted plans given overspecified language.
Prompt: “stack warm colors”
1.
2.
3.
4.
Fig. A4: Example showing predicted plans given language specified using rare (N =1 in train) terminology.

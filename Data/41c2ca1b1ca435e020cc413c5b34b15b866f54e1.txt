Traffic Danger Recognition With Surveillance Cameras Without Training Data
LijunYu∗ DaweiZhang XiangqunChen AlexanderHauptmann
PekingUniversity MIXLabs PekingUniversity CarnegieMellonUniv.
Beijing,China Beijing,China Beijing,China Pittsburgh,PA,US
yulijun@pku.edu.cn dawei@mixlabs.xyz cherry@sei.pku.edu.cn alex@cs.cmu.edu
Abstract Asaccidentsarerareinregularsurveillancevideos,itis
arduoustocollectandbuildalabeleddatasetofcarcrashes
We propose a traffic danger recognition model that coveringallpossiblesituations. Takingthisrealityintoac-
works with arbitrary traffic surveillance cameras to iden- count, we propose a model that requires no labeled crash
tify and predict car crashes. There are too many cameras data for training. Physically, a collision between cars oc-
to monitor manually. Therefore, we developed a model to curs when they gradually get closer and finally come into
predictandidentifycarcrashesfromsurveillancecameras contact. Wecanpredicttheirtrajectoriesandcheckoverlap
based on a 3D reconstruction of the road plane and pre- positionsindicatingacollision. Inseverecrashes,vehicles
dictionoftrajectories. Fornormaltraffic, itsupportsreal- are deformed and undetectable afterwards, but the crash is
time proactive safety checks of speeds and distances be- recognizedaheadoftimebasedonthepredictions.
tweenvehiclestoprovideinsightsaboutpossiblehigh-risk Our model consists of five steps to achieve the goal.
areas. We achieve good prediction and recognition of car Camera calibration method is applied to transform a point
crasheswithoutusinganylabeledtrainingdataofcrashes. ontheimagetotheroadplane. Objectdetectionandtrack-
ExperimentsontheBrnoCompSpeeddatasetshowthatour ingalgorithmsidentifyavehicleandtraceitshistory. A3D
modelcanaccuratelymonitortheroad,withmeanerrorsof bounding box is built to get the projection of a car on the
1.80%fordistancemeasurement,2.77km/hforspeedmea- road.Positionandspeedareestimatedandpredictedforthe
surement,0.24mforcarpositionprediction,and2.53km/h future. Finally, the model can recognize danger based on
forspeedprediction. distancesbetweenvehiclesandoverlapsinthetrajectories.
We run this model on BrnoCompDataset [19], which
contains highway surveillance videos with ground truth
1.Introduction speed and distance measurements. We evaluate its perfor-
mance for different steps and show convincing results. It
Surveillancecamerasarewidelyinstalled,recordingand
performs an effective 3D reconstruction of the road plane
storing massive data every day. But anomalous events are
with a mean distance measurement error of 1.80% along
veryrareanditisimpossibleforhumanstomonitorallthese
theroad. Uponefficientdetectionandtrackingofvehicles,
cameras. Car crashes are a crucial safety issue nowadays.
ittakesaprecisemeasurementofspeedsatameanerrorof
Leveraging the recent development of computer vision al-
2.77 km/h. It predicts vehicle trajectories reliably with er-
gorithms,wearedevelopinganautomaticsystemfortraffic
rorsof0.24mforcarpositionsand2.53km/hforspeedsav-
surveillanceonhighwaysandstreets.
eragelyfor0.12secondsahead. Thisallowsrefinedrecog-
We have built a model that can predict and recognize nition of traffic danger from all the measurement and pre-
crashes from surveillance cameras. One benefit is that dictions. Importantly,alltheseresultsareachievedwithout
ambulances could immediately be sent to the crash scene anylabeledtrainingdata.
saving lives. As accidents are relatively few, our model
Thekeycontributionsofthispaperare:
also supports proactive safety check based on normal traf-
• A traffic danger recognition model for surveillance
fic flow. Real-time speed and distance measurements will
camerasbasedona3Dreconstructionoftheroadand
lead to insights about high-risk areas, such as where cars
predictionoftrajectories. Itdoesnotneedanylabeled
frequently get too close. This will help to improve traffic
trainingdataofcarcrashes.
safetyonthelongterm.
• Results show that the model monitors the road accu-
∗ThisworkwasdonewhenLijunYuwasavisitingscholaratCarnegie rately, with mean errors of 1.80% for distance mea-
MellonUniversityandlateraresearchinternatMIXLabs. surement, 2.77 km/h for speed measurement, 0.24 m
978-1-5386-9294-3/18/$31.00(cid:13)c2018IEEE
8102
voN
92
]VC.sc[
1v96911.1181:viXra
for car position prediction, and 2.53 km/h for speed worldspace. Thereprojectionenablesa3Dreconstruction
prediction. ofvehiclesontheroad.
2.RelatedWork
Camera Calibration. Calibration methods are em-
ployedtoderivetheintrinsic(focallength,principalpoint)
and extrinsic (rotation, translation) parameters of a cam-
era. The accuracy of calibration is critical for the 3D re-
constructionandfurtherprocessing.Differentmethodsmay
requirevariousformsofuserinputs,suchasdrawingparal-
Figure 1. Traffic camera model: x,y,z define world coordinate
lellines[12],cameraposition[22,14],andaveragevehicle
system, where x-y plane is parallel to the image and z passes
size [3] or speed [16]. Fully automatic calibration is also
throughitstopleft. Cameraisonthex-yplaneandpointstothe
achievableaccordingto[4,18].
principalpointC atthecenteroftheimage. ρistheroadplane.
ObjectDetection. Objectdetectionmodelsareutilized
U,V,W aredirectionsofvanishingpoints, U inthedirectionof
to identify vehicles in video frames. These models such traffic,V paralleltotheroadandperpendiculartoU,andW per-
as Fast R-CNN [6] and Faster R-CNN [15] rely on region pendiculartoρ.
proposalalgorithmsanddeepconvolutionneuralnetworks
togetboundingboxesofobjects. MaskR-CNN[7]further Althoughsomeautomaticcalibrationmethodshavebeen
extendsbypredictingobjectmaskssimultaneously. developed, they do not achieve perfect performance in our
model. Soweremainusingamanualcalibrationwhichre-
MultipleObjectTracking. Vehicleobjectsdetectedin
quireslabelingtwogroupsofparallellinesofeachcamera
adjacent frames need to be traced correctly. SORT algo-
view. Then we derive two vanishing points in the image
rithm [2] supports fast online tracking with Kalman Fil-
spaceusingaleastsquareerrormethodasin[12]. WithAl-
ter[9]andHungarianalgorithm[10]. Deep-SORT[23]ad-
gorithm1extractedfromthesupplementarymaterialofthe
ditionallyintegratesappearanceinformationtoimprovethe
dataset[19],wecanderivetheroadplaneintheworldspace
performance.
andprojectimagepointstoworldpointsontheplane.
Anomaly Detection. Traffic danger recognition is one
We rotate the world coordinate system to make the x-z
specific aspect of anomaly detection. Multiple instance
plane parallel to the road plane, so we can get plane coor-
learning [20] requires sufficient annotated training data.
dinates of a point by omitting y axis. Rotation parameters
Motionpatternbasedlearningfortrafficanomaly[24]also
α,β,γ areacquiredbysolvingEquation1.
useslabeleddata. Butourapproachisbuiltuponnolabeled
videosofcarcrashes.
(cid:104) (cid:105) (cid:20)1 0 0 0(cid:21) (cid:20)cosβ 0−sinβ 0(cid:21)
3.Methodology 1 0 00 1 00 0 11 1 1 · 0 0 0−co s 0is nα αcs oin 0sα α0 0 1 · sin0 0β 1 00 co0 0sβ 0 10
(cid:20) cosγ sinγ 00(cid:21) (cid:34) (cid:107)V V(cid:107) (cid:35) (1)
Our traffic danger recognition model consists of five · −sinγ cosγ 00 = W
0 0 10 (cid:107)W(cid:107)
steps. Camera calibration provides geometry parameters 0 0 01 U
(cid:107)U(cid:107)
andatransformationfromimagecoordinatestoroadplane
coordinates. Objectdetectionandtrackingalgorithmspro- 3.2.ObjectDetectionandTracking
vide the types, positions, and masks of vehicles and trace
WeselectMaskR-CNNbyHeetal.[7]asourobjectde-
theirhistories. 3Dboundingboxesarebuilttolocalizeve-
tectionmodel,whichoutputsdetectionscores,objecttypes,
hiclesintheworldspaceandthenprojecttotheroadplane.
bounding boxes, and object masks. We use Abdulla’s im-
Positions and speeds are calculated with adjacent frames
plementation[1]withtrainedweightsonMicrosoftCOCO
plus smoothing and predicted for the future. Finally, we
dataset[13]andselectthreetypesofobjectsastargets: car,
can recognize danger from vehicle distances and potential
bus,andtruck.Thenweapplyafiltertothedetectedobjects
overlapsinthepredictions.
asshowninFigure2. Thefilterfollowsthreerules:
1. Vehiclesshouldnotbetoosmallinsize.
3.1.CameraModelandCalibration
2. Vehiclesshouldbeintheroadarea.
Weadoptatrafficcameramodelsimilartothepaperof 3. Vehiclesshouldbecompletelyvisible.
Sochoretal.[18]asshowninFigure1.Wefollowtheprac- WeuseDeepSORTbyWojkeetal.[23]totrackvehicles
tice of Dubska´ et al. [5] in setting up directions of three acrossframes. EachvehicleissupposedtogetauniqueID
vanishing points U,V,W. With a known plane, points in fromthetrackingmodel,anditisrobustthroughbriefloss
an image can be reprojected to points on the plane in the ofdetection.
Algorithm1Projectanimagepointtoaworldpointonthe Algorithm 2 Build 3D bounding box with tangent lines
road plane. f denotes the focal length. For points, lower fromvanishingpoints. Lineswithsubscriptmindenotethe
caserepresentsimagecoordinatesanduppercasestandsfor lineswiththeminimumtiltangle,andmaxthemaximum.
worldcoordinates. disanarbitraryoffsetoftheplane,usu- PositionofthepointsareshowninFigure3.
allysetto10asin[19]. Input: Tangentlinesfromvanishingpointsl ,l ,
U,min U,max
Input: Imagepointp=[p ,p ],planeoffsetd l ,l ,l ,l
x y V,min V,max W,min W,max
Input: Calibrationu=[u ,u ],v =[v ,v ],c=[c ,c ] Output: 3Dboundingbox(A,B,C,D,E,F,G,H)
x y x y x y
Output: WorldpointontheplaneP =[P ,P ,P ] A=l ∩l
x y z U,max V,min
f =(cid:112) −(u−c)·(v−c) B =l V,min∩l W,max
U =[u x,u y,f],V =[v x,v y,f],C =[c x,c y,0] D =l U,max∩l W,min
W =[W x,W y,W z]=(U −C)×(V −C) F =l U,min∩l W,max
w =[w ,w ]= [Wx,Wy] ·f +c G=l V,max∩l U,min
x y Wz H =l ∩l
n=[n ,n ,n ]=[w ,w ,f]−C V,max W,min
x y z x y
ρ=[a,b,c,d]=[ nx , ny , nz ,d] E F =FV ∩AW
(cid:107)n(cid:107) (cid:107)n(cid:107) (cid:107)n(cid:107) E =HU ∩AW
g =[p ,p ,f]−C H
x y if|AE |≥|AE |then
t=−ρ·[cx,cy,0,1] F H
[a,b,c]·g E =E F
P =C+t·g
else
E =E
H
endif
H =CV ∩BU
Figure2.Objectdetection: rawdetections(left),andfilteredob-
jects(right). Thewhitecaratthetopleftisfilteredbyrule1,the
redatthebottomrightby3,andthecarsatthetoprightby2.
3.3.3DBoundingBox
Weestimatethecontourofavehiclewithitsmaskfrom
Mask R-CNN, using the algorithm by Suzuki et al. [21].
Foreachofthethreevanishingpoints,wecalculatethetilt
angles of the lines passing that vanishing point and each
point in the contour. In this way we find the tangent lines Figure 3. 3D bounding box: tangent lines of contour and their
of the contour passing three vanishing lines. We alter the intersections(topleft),derivedlinesandintersections(topright),
thefinalresult(bottomleft),andvehiclesinotheranglesofview
algorithmfromSochor[17]tobuild3Dboundingboxesof
(bottomright).Linesincolorsofblue,green,andredpassthrough
carsasdescribedinAlgorithm2andFigure3.
u,v,wrespectively.
where t denotes frame number and fps is the frame rate
3.4.TrajectoryPrediction
of the video. Exponential smoothing is applied to get a
smoothedspeedv as
Togetthecurrentlocationofavehicle, wecanfindthe s
v =δ×v +(1−δ)×v (5)
bottom of the 3D bounding boxes and project them to the s,t s,t−1 r,t
roadplaneaccordingtoSection3.1. Thesetofthebottom Withanoptionalscalefactorλ,weareabletoknowthereal
pointsS reliesonthedirectionofthevehicleas: worldvalueofthespeed.
(cid:40) −−→ Topredictthetrajectories,weassume:
{A,B,C,D},whentanDA≥0
S = (2) 1. Thefutureisdividedintotimeslotswithequallengths.
{H,G,F,E},otherwise
2. Thevehiclecentersfollownormaldistributions.
Thecenterpositionofavehicleiscalculatedby
3. Thevehicleshapesdonotchange.
c=average(S) (3)
Wepredictspeed,acceleration,centercoordinatesandvari-
andarecentspeedv r iscalculatedfromadjacentframesas ance for the beginning of each slot as a snapshot. Within
v =(c −c )×fps (4) aslot,weassumetherearefixedaccelerationandvariance.
r,t t t−1
Thenthespeedandcentercoordinatescanbecalculatedac-
cording to kinematics rules. In this way, predictions are
availableforanarbitrarytimeinthefuture.
Fornow,weareusingasimplelinearpredictionmethod
with the real situation as the only one snapshot and as-
suming the acceleration is always zero. Models of condi-
tional random fields [11] and long short-term memory [8]
areplannedtobetestedinthefuture.
Figure4.Dangerrecognition:fourvehiclesinasampleprediction
3.5.DangerRecognition oftheroadplane,withvehicleIDsatthebottomrightandspeedsat
thetopleft. Distancesareshownfornearingvehicles,anddanger
Weusetwowaystorecognizedangeroussituations. The
mapisshowninblackattheoverlapofvehicle7and10.
first one is the distance measurement between vehicles. It
not only tells where cars are going to crash but provides 4.Experiments
a proactive safety check for areas where cars often get too
4.1.DatasetandSetup
close,aswell. Thesecondoneiscalleddangermap,which
detectsoverlapofvehiclesinthepredictionsthatindicates We use BrnoCompDataset [19] to evaluate the perfor-
crashes. mance of our model. It consists of surveillance videos of
Thedistancebetweentwovehiclesisdefinedasthemin- 6 sessions from 3 directions on the highway in the Czech
imum distance between two points from two quadrangles Republic. Thedatasetprovidesthegroundtruthofdistance
respectively. measurementlinesandspeedofvehiclesfromLidarsensor.
Italsohascalibrationresultsfromvarioussystems[19,18].
Lemma 1. Let A,B be the pair with the minimum dis-
Werunourmodeloneachofthe18videosfor10min-
tance among all pairs of points from two quadrangles re-
utes. The videos are processed at the original resolution
spectively,thenatleastoneofA,Bmustbeavertex.
of 1080p and downsampled from 50 fps to 25 fps. We do
Proof. SupposebothA,Barenotvertices,soeachofthem not use lower frame rate because the Deep-SORT model
isonanedge, namelya,b. Ifa (cid:107) b, theremustbeanother has worse performance when it is less than 25fps. We ex-
pair of points consisting of at least one vertex that has an ploit the calibration results from [18] which provides van-
equal distance. If a is not parallel to b, then the nearest ishingpointsu,vacquiredbymanualcalibrationfrompar-
distance between a and b cannot be at the middle of both allel lines, along with scale factors inferred from speeds.
edges, which contradicts the suppose. Therefore, at least Weletthesmoothingparameterδ =0.86accordingtosome
oneofA,Bisavertex. preliminaryexperiments.Thetrajectorypredictionissetfor
0.12and0.24secondsahead,accordingly3and6frames.
WithLemma1,wecancalculatetheminimumdistance
betweentwoquadranglesA B C D ,A B C D as:
1 1 1 1 2 2 2 2 4.2.CalibrationError
d qq(A 1B 1C 1D 1,A 2B 2C 2D 2)= Wemeasurethecalibrationerrortotesttheprovidedcal-
min( min d (P,A B C D ), ibrationresultsandthecorrectnessofourcoordinatestrans-
pq 2 2 2 2 (6)
P=A1B1C1D1 formation algorithm which maps a point from the image
min d pq(P,A 1B 1C 1D 1)) spacetotheroadplanespace.
P=A2B2C2D2
Wecalculatethedistanceofthegivenmeasurementlines
d (P,ABCD)= min d (X,e) (7)
pq e=AB,BC,CD,DA pe in our plane coordinate system. The lines are divided into
two groups according to their directions: toward u or v.
where d is the distance between a point and an edge. In
pe
The average length of the given lines is different in each
thisway,theminimumdistanceiscalculatedfromonly32
group. Wecollectabsoluteandrelativeerrorsofmeasured
candidates.Distancesforallvehiclepairsarecalculatedand
distancesandreportthemeanandmedianvaluesinTable1.
alertedwhenlessthanathreshold.
Weaccumulatetheprobabilityofacarboxbasedonthe Mean Median
distributionofitscentertogettheheatmapofavehicle. It
Distance AbsoluteError(m) 0.2618 0.1684
representstheprobabilityofitspositionataspecifictimein
Towardu RelativeError 1.80% 1.42%
thefuture. Thenweaggregatetheheatmapsofallthevehi-
Distance AbsoluteError(m) 0.1633 0.1646
clesinasceneintoadangermap. Adangermaprepresents
Towardv RelativeError 2.06% 2.07%
the probability of coexistence of two or more vehicles in
thesamelocation.Figure4showsasampleresultofdanger Table 1. Calibration error: distance measurement error on all
recognition. videos
Theresultsshowthatourmodelcanaccuratelymeasure 4.5.PredictionError
distances in the real world based merely on surveillance
Weevaluatethetwolevelsofpredictionsseparately. For
cameraviewsandcalibrationparameters. Theerrorineach
eachlevel,wecollecttheabsoluteerroroflocationpredic-
direction is much smaller than the shape of conventional
tion,plustheabsoluteandrelativeerrorofspeedprediction
vehicles. Asofthehighspeedinthehighway,theseerrors
ofeachvehicle. Asthesmoothedspeedisnotstableatthe
are even smaller than the movement of a vehicle between
beginning,predictionsfromvehicleswithahistoryoffewer
two adjacent frames. This model provides an effective 3D
than l = 5 frames (0.2 seconds) are excluded. The mean
reconstructionoftheroadplanewithlittleerror.
andmedianvaluesofeachmetricareshowninTable3.
4.3.VehicleDetectionandTrackingError Level +0.12s Mean Median
Location
We measure vehicle detection and tracking error to test AbsoluteError(m) 0.2433 0.1736
Prediction
theMaskR-CNNandDeep-SORTmodels. Foreachvehi-
Speed AbsoluteError(km/h) 2.5313 1.8373
cle detected and tracked, we record the time and position
Prediction RelativeError 4.55% 2.52%
of its every appearance. Based on the appearance history,
Level +0.24s Mean Median
wecalculateanestimatedperiodofthevehicleinthemea-
Location
surement area of Lidar sensors. The measurement area is AbsoluteError(m) 0.3563 0.3256
Prediction
consideredtobethelargestoneiftherearemorethantwo
Speed AbsoluteError(km/h) 3.0134 2.4995
Lidarsensorssetup. Thenwecalculatetheintersectionof
Prediction RelativeError 5.71% 3.92%
union (IoU) between the estimated period and the real pe-
riodofexistenceinthegroundtruthtogetasimilarityma-
Table 3. Prediction error: location and speed prediction error of
trix. Hungarian algorithm [10] is employed to solve this
twolevelsonallvideos
matchingproblem. Additionally,matchingresultswithIoU
less than l = 0.5 are dropped. We report the recall on Although the prediction mechanism currently deployed
IoU
eachvideoforthisevaluationinTable4. israthersimple,itprovidesresultsmuchbeyondourexpec-
WefindthatMaskR-CNNsometimesdoesnotworkat tations. Asavehicleat75km/hwouldmove2.5metersin
certainviewinganglesorforcertaintypesofvehicles. For 0.12 seconds, a mean error of 0.24 m for location predic-
lost detections, as long as the gap is short enough, Deep- tion is well acceptable. The difference between the mean
SORTisstillabletotrack.Inothercases,however,tracking andmedianvaluesindicatessomeoutliersareharmingthe
alsofailsandthatcausestheloss. Despitethese,thecombi- performance, but we can still see that most of the predic-
nationofMaskR-CNNandDeep-SORThaveachievedan tionsarewithinanerrorof2km/h. Fortrafficonhighways,
overall94.0%recallrate,whichshowsthatitisefficacious crashesusuallyhappenwithin0.12seconds,soitisenough
forthevehicledetectionandtrackinginthistask. for the danger map to work. Moreover, another prediction
of +0.24s is there for more information beforehand, and it
4.4.SpeedEstimationError isreasonabletohaveaslightlylargererrorthan+0.12s.
We use the matched vehicles from the previous section
5.Conclusions
toevaluatetheperformanceofourspeedestimation. Asthe
groundtruthonlyhastheaveragespeedforeachvehicle,we
Weproposeatrafficdangerrecognitionmodelthatworks
usethesmoothedspeedofavehicleatitslastappearancefor
witharbitrarysurveillancecameras. Itdoesnotrequireany
comparison. We collect absolute and relative errors of the
labeledtrainingdataofcrashes. Themodelconsistsoffive
estimatedspeedofeachvehicleandreportinTable2.
steps: cameracalibration,objectdetectionandtracking,3D
Mean Median bounding box, trajectory prediction, and danger recogni-
AbsoluteError(km/h) 2.7708 1.8625 tion. We measure the performance with experiments step
RelativeError 3.68% 2.55% by step, presenting that it is accurate at the estimation of
speedandpositionofvehiclesbyprojectingtoa3Drecon-
Table2.Speedestimationerror: estimatedspeedscomparedwith structed road plane. It is suitable for crash detection and
groundtruthfromLidarsensorsonallvideos proactivesafetychecks.
Ademoofourmodelworkingonarealcrashscenecan
Accordingtothedataset,theaveragespeedforeachses-
be found on Youtube1. In the future, a complete test set
sionismostlybetween60km/hand90km/h. Forhighway
ofvideocontainingrealcrasheswillbeprocessedtoreport
traffic, ameanerroroflessthan2.77km/hprovesthatour
detection accuracy. Trajectory prediction model could be
model can precisely measure the speeds of vehicles. This
accuratemeasurement isthe foundationforfurther predic- 1https://www.youtube.com/playlist?list=
tionsanddangerrecognition. PLssAerj8zfUR5wBc7N6gmCFTm0azCHSIf
VideoID 1C 1L 1R 2C 2L 2R 3C 3L 3R 4C
VehicleMatchingRecall 95.0% 92.2% 97.0% 82.2% 92.6% 92.5% 81.8% 100% 100% 92.4%
VideoID 4L 4R 5C 5L 5R 6C 6L 6R Mean
VehicleMatchingRecall 93.2% 98.2% 83.0% 98.9% 97.5% 98.8% 99.5% 96.4% 94.0%
Table4.Vehicledetectionandtrackingerror:vehiclematchingrecalloneachvideo.ThenumberinVideoIDisSessionID,andtheletter
denotesthedirectionaccordingtoC-center,L-left,R-right.
improved with conditional random fields or recurrent neu- [13] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-
ralnetwork. Wewillalsotestautomaticcameracalibration manan,P.Dolla´r,andC.L.Zitnick. Microsoftcoco: Com-
methods to obtain similar performance as manual calibra- monobjectsincontext.InEuropeanconferenceoncomputer
tion, then the system could function on arbitrary surveil- vision,pages740–755.Springer,2014. 2
lancecameraswithzeroinput. [14] T.-W. Pai, W.-J. Juang, and L.-J. Wang. An adaptive win-
dowingpredictionalgorithmforvehiclespeedestimation.In
IntelligentTransportationSystems,2001.Proceedings.2001
References
IEEE,pages901–906.IEEE,2001. 2
[1] W. Abdulla. Mask r-cnn for object detection and in- [15] S.Ren,K.He,R.Girshick,andJ.Sun.Fasterr-cnn:Towards
stance segmentation on keras and tensorflow. https:// real-timeobjectdetectionwithregionproposalnetworks. In
github.com/matterport/Mask_RCNN,2017. 2 Advances in neural information processing systems, pages
91–99,2015. 2
[2] A.Bewley,Z.Ge,L.Ott,F.Ramos,andB.Upcroft. Simple
online and realtime tracking. In Image Processing (ICIP), [16] T. N. Schoepflin and D. J. Dailey. Dynamic camera cali-
2016IEEEInternationalConferenceon,pages3464–3468. brationofroadsidetrafficmanagementcamerasforvehicle
IEEE,2016. 2 speed estimation. IEEE Transactions on Intelligent Trans-
portationSystems,4(2):90–98,2003. 2
[3] D.J.Dailey,F.W.Cathey,andS.Pumrin.Analgorithmtoes-
timatemeantrafficspeedusinguncalibratedcameras. IEEE [17] J. Sochor. Traffic analysis from video. Diplomova´ pra´ce,
BrnoUniversityofTechnology,FacultyofInformationTech-
TransactionsonIntelligentTransportationSystems,1(2):98–
nology,2014. 3
107,2000. 2
[18] J. Sochor, R. Jura´nek, and A. Herout. Traffic surveillance
[4] M.Dubska´,A.Herout,R.Jura´nek,andJ.Sochor. Fullyau-
cameracalibrationby3dmodelboundingboxalignmentfor
tomaticroadsidecameracalibrationfortrafficsurveillance.
accuratevehiclespeedmeasurement. ComputerVisionand
IEEE Transactions on Intelligent Transportation Systems,
ImageUnderstanding,161:87–98,2017. 2,4
16(3):1162–1171,2015. 2
[19] J. Sochor, R. Jura´nek, J. Sˇpanˇhel, L. Marsˇ´ık, A. Sˇiroky`,
[5] M. Dubska´, A. Herout, and J. Sochor. Automatic camera
A.Herout,andP.Zemcˇ´ık. Comprehensivedatasetforauto-
calibration for traffic understanding. In BMVC, volume 4,
maticsinglecameravisualspeedmeasurement.IEEETrans-
page8,2014. 2
actionsonIntelligentTransportationSystems,2018. 1,2,3,
[6] R.Girshick. Fastr-cnn. InProceedingsoftheIEEEinter-
4
nationalconferenceoncomputervision, pages1440–1448,
[20] W.Sultani,C.Chen,andM.Shah. Real-worldanomalyde-
2015. 2
tectioninsurveillancevideos. CenterforResearchinCom-
[7] K.He,G.Gkioxari,P.Dolla´r,andR.Girshick. Maskr-cnn.
puterVision(CRCV),UniversityofCentralFlorida(UCF),
InComputerVision(ICCV),2017IEEEInternationalCon-
2018. 2
ferenceon,pages2980–2988.IEEE,2017. 2
[21] S.Suzukietal. Topologicalstructuralanalysisofdigitized
[8] S.HochreiterandJ.Schmidhuber.Longshort-termmemory.
binaryimagesbyborderfollowing.Computervision,graph-
Neuralcomputation,9(8):1735–1780,1997. 4
ics,andimageprocessing,30(1):32–46,1985. 3
[9] R.E.Kalman.Anewapproachtolinearfilteringandpredic-
[22] K. Wang, H. Huang, Y. Li, and F.-Y. Wang. Research on
tionproblems. JournalofbasicEngineering, 82(1):35–45,
lane-marking line based camera calibration. In Vehicular
1960. 2
Electronics and Safety, 2007. ICVES. IEEE International
[10] H.W.Kuhn.Thehungarianmethodfortheassignmentprob-
Conferenceon,pages1–6.IEEE,2007. 2
lem.Navalresearchlogisticsquarterly,2(1-2):83–97,1955.
[23] N. Wojke, A. Bewley, and D. Paulus. Simple online and
2,5
realtimetrackingwithadeepassociationmetric. InImage
[11] J.Lafferty,A.McCallum,andF.C.Pereira.Conditionalran-
Processing(ICIP),2017IEEEInternationalConferenceon,
domfields:Probabilisticmodelsforsegmentingandlabeling
pages3645–3649.IEEE,2017. 2
sequencedata. 2001. 4
[24] Y. Xu, X. Ouyang, Y. Cheng, S. Yu, L. Xiong, C.-C. Ng,
[12] S.C.LeeandR.Nevatia. Robustcameracalibrationtoolfor
S.Pranata,S.Shen,andJ.Xing. Dual-modevehiclemotion
video surveillance camera in urban environment. In Com-
pattern learning for high performance road traffic anomaly
puterVisionandPatternRecognitionWorkshops(CVPRW),
detection.InCVPRWorkshop(CVPRW)ontheAICityChal-
2011IEEEComputerSocietyConferenceon,pages62–67.
lenge,2018. 2
IEEE,2011. 2

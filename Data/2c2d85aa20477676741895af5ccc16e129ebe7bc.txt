PublishedasaconferencepaperatICLR2023
CLIPSEP: LEARNING TEXT-QUERIED SOUND SEPA-
RATION WITH NOISY UNLABELED VIDEOS
Hao-WenDong1,2∗ NaoyaTakahashi1† YukiMitsufuji1
JulianMcAuley2 TaylorBerg-Kirkpatrick2
1SonyGroupCorporation 2UniversityofCaliforniaSanDiego
hwdong@ucsd.edu, {Naoya.Takahashi,Yuhki.Mitsufuji}@sony.com, {jmcauley,tberg}@ucsd.edu
ABSTRACT
Recent years have seen progress beyond domain-specific sound separation for
speechormusictowardsuniversalsoundseparationforarbitrarysounds. Prior
workonuniversalsoundseparationhasinvestigatedseparatingatargetsoundout
ofanaudiomixturegivenatextquery. Suchtext-queriedsoundseparationsystems
provide a natural and scalable interface for specifying arbitrary target sounds.
However,supervisedtext-queriedsoundseparationsystemsrequirecostlylabeled
audio-textpairsfortraining. Moreover,theaudioprovidedinexistingdatasetsis
oftenrecordedinacontrolledenvironment,causingaconsiderablegeneralization
gap to noisy audio in the wild. In this work, we aim to approach text-queried
universalsoundseparationbyusingonlyunlabeleddata. Weproposetoleverage
thevisualmodalityasabridgetolearnthedesiredaudio-textualcorrespondence.
TheproposedCLIPSepmodelfirstencodestheinputqueryintoaqueryvector
using the contrastive language-image pretraining (CLIP) model, and the query
vector is then used to condition an audio separation model to separate out the
target sound. While the model is trained on image-audio pairs extracted from
unlabeled videos, at test time we can instead query the model with text inputs
inazero-shotsetting,thankstothejointlanguage-imageembeddinglearnedby
theCLIPmodel. Further,videosinthewildoftencontainoff-screensoundsand
backgroundnoisethatmayhinderthemodelfromlearningthedesiredaudio-textual
correspondence. Toaddressthisproblem,wefurtherproposeanapproachcalled
noise invariant training for training a query-based sound separation model on
noisydata. Experimentalresultsshowthattheproposedmodelssuccessfullylearn
text-querieduniversalsoundseparationusingonlynoisyunlabeledvideos,even
achievingcompetitiveperformanceagainstasupervisedmodelinsomesettings.
1 INTRODUCTION
Humanscanfocusontoaspecificsoundintheenvironmentanddescribeitusinglanguage. Such
abilitiesarelearnedusingmultiplemodalities—auditoryforselectivelistening,visionforlearningthe
conceptsofsoundingobjects,andlanguagefordescribingtheobjectsorscenesforcommunication.
Inmachinelistening,selectivelisteningisoftencastastheproblemofsoundseparation,whichaims
toseparatesoundsourcesfromanaudiomixture(Cherry,1953;Bach&Jordan,2005). Whiletext
queriesofferanaturalinterfaceforhumanstospecifythetargetsoundtoseparatefromamixture
(Liuetal.,2022;Kilgouretal.,2022),trainingatext-queriedsoundseparationmodelinasupervised
mannerrequireslabeledaudio-textpaireddataofsingle-sourcerecordingsofavastnumberofsound
types,whichcanbecostlytoacquire. Moreover,suchisolatedsoundsareoftenrecordedincontrolled
environmentsandhaveaconsiderabledomaingaptorecordingsinthewild,whichusuallycontain
arbitrarynoiseandreverberations. Incontrast,humansoftenleveragethevisualmodalitytoassist
learningthesoundsofvariousobjects(Baillargeon,2002). Forinstance,byobservingadogbarking,
ahumancanassociatethesoundwiththedog,andcanseparatelylearnthattheanimaliscalleda
“dog.” Further,suchlearningispossibleevenifthesoundisobservedinanoisyenvironment,e.g.,
∗WorkdoneduringaninternshipatSonyGroupCorporation
†Correspondingauthor
1
3202
raM
3
]DS.sc[
2v56070.2122:viXra
PublishedasaconferencepaperatICLR2023
whenacarispassingbyorsomeoneistalkingnearby,wherehumanscanstillassociatethebarking
soundsolelywiththedog. Priorworkinpsychophysicsalsosuggeststheintertwinedcognitionof
visionandhearing(Sekuleretal.,1997;Shimojo&Shams,2001;Rahneetal.,2007).
Motivatedbythisobservation,weaimtotackletext-queriedsoundseparationusingonlyunlabeled
videosinthewild. Weproposeatext-queriedsoundseparationmodelcalledCLIPSepthatleverages
abundant unlabeled video data resources by utilizing the contrastive image-language pretraining
(CLIP)(Radfordetal.,2021)modeltobridgetheaudioandtextmodalities. AsillustratedinFigure1,
duringtraining,theimagefeatureextractedfromavideoframebytheCLIP-imageencoderisusedto
conditionasoundseparationmodel,andthemodelistrainedtoseparatethesoundthatcorresponds
totheimagequeryinaself-supervisedsetting. ThankstothepropertiesoftheCLIPmodel,which
projects corresponding text and images to close embeddings, at test time we instead use the text
featureobtainedbytheCLIP-textencoderfromatextqueryinazero-shotsetting.
However,suchzero-shotmodalitytransfercanbe
challengingwhenweusevideosinthewildfor
trainingastheyoftencontainoff-screensounds
andvoiceoversthatcanleadtoundesiredaudio-
visualassociations. Toaddressthisproblem,we
proposethenoiseinvarianttraining(NIT),where
query-basedseparationheadsandpermutationin-
variantseparationheadsjointlyestimatethenoisy
target sounds. We validate in our experiments Figure1: Anillustrationofmodalitytransfer.
thattheproposednoiseinvarianttrainingreduces
thezero-shotmodalitytransfergapwhenthemodelistrainedonanoisydataset,sometimesachieving
competitiveresultsagainstafullysupervisedtext-queriedsoundseparationsystem.
Ourcontributionscanbesummarizedasfollows: 1)Weproposethefirsttext-querieduniversalsound
separationmodelthatcanbetrainedonunlabeledvideos. 2)Weproposeanewapproachcallednoise
invarianttrainingfortrainingaquery-basedsoundseparationmodelonnoisydatainthewild. Audio
samplescanbefoundonanourdemowebsite.1 Forreproducibility,allsourcecode,hyperparameters
andpretrainedmodelsareavailableat: https://github.com/sony/CLIPSep.
2 RELATED WORK
Universalsoundseparation Muchpriorworkonsoundseparationfocusesonseparatingsounds
foraspecificdomainsuchasspeech(Wang&Chen,2018)ormusic(Takahashi&Mitsufuji,2021;
Mitsufujietal.,2021). Recentadvancesindomainspecificsoundseparationleadseveralattemptsto
generalizetoarbitrarysoundclasses. Kavalerovetal.(2019)reportedsuccessfulresultsonseparating
arbitrarysoundswithafixednumberofsourcesbyadoptingthepermutationinvarianttraining(PIT)
(Yuetal.,2017),whichwasoriginallyproposedforspeechseparation. Whilethisapproachdoes
notrequirelabeleddatafortraining,apost-selectionprocessisrequiredaswecannotnottellwhat
soundsareincludedineachseparatedresult. Follow-upwork(Ochiaietal.,2020;Kongetal.,2020)
addressedthisissuebyconditioningtheseparationmodelwithaclasslabeltospecifythetargetsound
inasupervisedsetting. However,theseapproachesstillrequirelabeleddatafortraining,andthe
interfaceforselectingthetargetclassbecomescumbersomewhenweneedalargenumberofclasses
tohandleopen-domaindata. Wisdometal.(2020)laterproposedanunsupervisedmethodcalled
mixtureinvarianttraining(MixIT)forlearningsoundseparationonnoisydata. MixITisdesignedto
separateallsourcesatatimeandalsorequiresapost-selectionprocesssuchasusingapre-trained
soundclassifier(Scottetal.,2021),whichrequireslabeleddatafortraining,toidentifythetarget
sounds. WesummarizeandcomparerelatedworkinTable1.
Query-basedsoundseparation Visualinformationhasbeenusedforselectingthetargetsoundin
speech(Ephratetal.,2019;Afourasetal.,2020),music(Zhaoetal.,2018;2019;Tianetal.,2021)
anduniversalsounds(Owens&Efros,2018;Gaoetal.,2018;Rouditchenkoetal.,2019). While
manyimage-queriedsoundseparationapproachesrequirecleanvideodatathatcontainsisolated
sources,Tzinisetal.(2021)introducedanunsupervisedmethodcalledAudioScopeforseparating
on-screensoundsusingnoisyvideosbasedontheMixITmodel. Whileimagequeriescanserveasa
1https://sony.github.io/CLIPSep/
2
PublishedasaconferencepaperatICLR2023
Method Querytype Unlabeleddata Noisydata
USS(Kavalerovetal.,2019) × (cid:88)
MixIT(Wisdometal.,2020) × (cid:88) (cid:88)
UniversalSoundSelector(Ochiaietal.,2020) Label
USS-Label(Kongetal.,2020) Label ((cid:88))
PixelPlayer(Zhaoetal.,2018) Image (cid:88) ((cid:88))
AudioScope(Tzinisetal.,2021) Image (cid:88) (cid:88)
SoundFilter(Gfelleretal.,2021) Audio (cid:88)
Zero-shotaudioseparation(Chenetal.,2022) Audio ((cid:88))
Text-Queried(Liuetal.,2022) Text
Text/Audio-Queried(Kilgouretal.,2022) Text/Audio
CLIPSep(ours) Text/Image (cid:88)
CLIPSep-NIT(ours) Text/Image (cid:88) (cid:88)
Table1: Comparisonsofrelatedworkinsoundseparation. ‘((cid:88))’indicatesthattheproblemofnoisy
training data is partially addressed. CLIPSep-NIT denotes the proposed CLIPSep model trained
withthenoiseinvarianttraining. Tothebestofourknowledge,nopreviousworkhasattemptedthe
problemoflabel-freetext-queriedsoundseparation.
naturalinterfaceforspecifyingthetargetsoundincertainusecases,imagesoftargetsoundsbecome
unavailableinlow-lightconditionsandforsoundsfromout-of-screenobjects.
Anotherlineofresearchusestheaudiomodalitytoqueryacousticallysimilarsounds. Chenetal.
(2022) showed that such approach can generalize to unseen sounds. Later, Gfeller et al. (2021)
croppedtwodisjointsegmentsfromsinglerecordingandusedthemasaquery-targetpairtotraina
soundseparationmodel,assumingbothsegmentscontainthesamesoundsource. However,inmany
cases,itisimpracticaltoprepareareferenceaudiosampleforthedesiredsoundasthequery.
Mostrecently,text-queriedsoundseparationhasbeenstudiedasitprovidesanaturalandscalable
interfaceforspecifyingarbitrarytargetsoundsascomparedtosystemsthatuseafixedsetofclass
labels.Liuetal.(2022)employedapretrainedlanguagemodeltoencodethetextquery,andcondition
themodeltoseparatethecorrespondingsounds. Kilgouretal.(2022)proposedamodelthataccepts
audio or text queries ina hybrid manner. These approaches, however, require labeled text-audio
paireddatafortraining. Differentfrompriorwork,ourgoalistolearntext-queriedsoundseparation
forarbitrarysoundwithoutlabeleddata,specificallyusingunlabelednoisyvideosinthewild.
Contrastivelanguage-image-audiopretraining TheCLIPmodel(Radfordetal.,2021)hasbeen
usedasapretrainingofjointembeddingspacesamongtext,imageandaudiomodalitiesfordown-
streamtaskssuchasaudioclassification(Wuetal.,2022;Guzhovetal.,2022)andsoundguided
imagemanipulation(Leeetal.,2022). Pretrainingisdoneeitherinasupervisedmannerusinglabels
(Guzhovetal.,2022;Leeetal.,2022)orinaself-supervisedmannerbytraininganadditionalaudio
encodertomapinputaudiotothepretrainedCLIPembeddingspace(Wuetal.,2022). Incontrast,
weexplorethezero-shotmodalitytransfercapabilityoftheCLIPmodelbyfreezingthepre-trained
CLIPmodelanddirectlyoptimizingtherestofthemodelforthetargetsoundseparationtask.
3 METHOD
3.1 CLIPSEP—LEARNINGTEXT-QUERIEDSOUNDSEPARATIONWITHOUTLABELEDDATA
In this section, we propose the CLIPSep model for text-queried sound separation without using
labeleddata. WebasetheCLIPSepmodelonSound-of-Pixels(SOP)(Zhaoetal.,2018)andreplace
thevideoanalysisnetworkoftheSOPmodel. AsillustratedinFigure2,duringtraining,themodel
takes as inputs an audio mixture x =
(cid:80)n
s , where s ,...,s are the n audio tracks, along
i=1 i 1 n
withtheircorrespondingimagesy ,...,y extractedfromthevideos. Wefirsttransformtheaudio
1 n
mixture x into a magnitude spectrogram X and pass the spectrogram through an audio U-Net
(Ronnebergeretal.,2015;Janssonetal.,2017)toproducek(≥n)intermediatemasksM˜ ,...,M˜ .
1 k
Ontheotherstream,eachimageisencodedbythepretrainedCLIPmodel(Radfordetal.,2021)
intoanembeddinge ∈R512. TheCLIPembeddinge willfurtherbeprojectedtoaqueryvector
i i
3
PublishedasaconferencepaperatICLR2023
Figure2: AnillustrationoftheproposedCLIPSepmodelforn=2. Duringtraining,wemixaudio
fromtwovideosandtrainthemodeltoseparateeachaudiosourcegiventhecorrespondingvideo
frameasthequery. Attesttime,weinsteaduseatextqueryintheformof“aphotoof[userinput
query]”toquerythesoundseparationmodel. ThankstothepropertiesofthepretrainedCLIPmodel,
thequeryvectorsweobtainfortheimageandtextqueriesareexpectedtobeclose.
q ∈Rk byaprojectionlayer,whichisexpectedtoextractonlyaudio-relevantinformationfrome .2
i i
Finally,thequeryvectorq willbeusedtomixtheintermediatemasksintothefinalpredictedmasks
i
Mˆ =(cid:80)k σ(cid:0) w q M˜ +b (cid:1) ,wherew ∈Rk isalearnablescalevector,b ∈Ralearnablebias,
i j=1 ij ij j i i i
andσ(·)thesigmoidfunction. Now,supposeM isthegroundtruthmaskforsources . Thetraining
i i
objectiveofthemodelisthesumoftheweightedbinarycrossentropylossesforeachsource:
L
=(cid:88)n
WBCE(M ,Mˆ
)=(cid:88)n X(cid:12)(cid:18)
−M logMˆ −(1−M
)log(cid:16)
1−Mˆ
(cid:17)(cid:19)
. (1)
CLIPSep i i i i i i
i=1 i=1
Attesttime,thankstothejointimage-textembeddingofferedbytheCLIPmodel,wefeedatext
queryinsteadofanimagetothequerymodeltoobtainthequeryvectorandseparatethetargetsounds
accordingly(seeAppendixAforanillustration). AssuggestedbyRadfordetal.(2021),weprefix
thetextqueryintotheformof“aphotoof[userinputquery]”toreducethegeneralizationgap.3
3.2 NOISEINVARIANTTRAINING—HANDLINGNOISYDATAINTHEWILD
WhiletheCLIPSepmodelcanseparatesoundsgivenimageortextqueries,itassumesthatthesources
arecleanandcontainfewquery-irrelevantsounds. However,thisassumptiondoesnotholdforvideos
inthewildasmanyofthemcontainout-of-screensoundsandvariousbackgroundnoises. Inspiredby
themixtureinvarianttraining(MixIT)proposedbyWisdometal.(2020),wefurtherproposethenoise
invarianttraining(NIT)totacklethechallengeoftrainingwithnoisydata. AsillustratedinFigure3,
weintroducenadditionalpermutationinvariantheadscallednoiseheadstotheCLIPSepmodel,
wherethemaskspredictedbytheseheadsareinterchangeableduringlosscomputation. Specifically,
weintroducenadditionalprojectionlayers,andeachofthemtakesasinputthesumofallquery
vectorsproducedbythequeryheads(i.e.,(cid:80)n
q )andproduceavectorthatislaterusedtomix
i=1 i
theintermediatemasksintothepredictednoisemask. Inprinciple,thequerymasksproducedby
thequeryvectorsareexpectedtoextractquery-relevantsoundsduetotheirstrongercorrelationsto
theircorrespondingqueries,whiletheinterchangeablenoisemasksshould‘soakup’othersounds.
2Weextractthreeframeswith1-secintervalsandcomputetheirmeanCLIPembeddingastheinputtothe
projectionlayertoreducethenegativeeffectswhentheselectedframedoesnotcontaintheobjectsofinterest.
3Similartohowwepreparetheimagequeries,wecreatefourqueriesfromtheinputtextqueryusingfour
querytemplates(seeAppendixB)andtaketheirmeanCLIPembeddingastheinputtotheprojectionlayer.
4
PublishedasaconferencepaperatICLR2023
Figure3: AnillustrationoftheproposedCLIPSep-NITmodelforn=2. SimilartoCLIPSep,we
trainthemodeltoseparateeachaudiosourcegiventhecorrespondingqueryimageduringtraining
andswitchtousingatextqueryattesttime. Thetwopredictednoisemasksareinterchangeablefor
losscomputationduringtraining,andtheyarediscardedattesttime(grayedoutpaths).
Mathematically,letMQ,...,MQbethepredictedquerymasksandMN,...,MN bethepredicted
1 n 1 n
noisemasks. Then,thenoiseinvariantlossisdefinedas:
L = min
(cid:88)n WBCE(cid:18)
M
,min(cid:16) 1,MˆQ+MˆN(cid:17)(cid:19)
, (2)
NIT
(j1,...,jn)∈Σni=1
i i ji
whereΣ denotesthesetofallpermutationsof{1,...,n}.4 Taken=2forexample.5 Weconsider
n
thetwopossiblewaysforcombiningthequeryheadsandthenoiseheads:
(Arrangement1) Mˆ 1 =min(cid:0) 1,Mˆ 1Q+Mˆ 1N(cid:1) , Mˆ 2 =min(cid:0) 1,Mˆ 2Q+Mˆ 2N(cid:1) , (3)
(Arrangement2) Mˆ(cid:48) =min(cid:0) 1,MˆQ+MˆN(cid:1) , Mˆ(cid:48) =min(cid:0) 1,MˆQ+MˆN(cid:1) . (4)
1 1 2 2 2 1
Then,thenoiseinvariantlossisdefinedasthesmallestlossachievable:
L(2) =min(cid:16) WBCE(cid:0) M ,Mˆ (cid:1) +WBCE(cid:0) M ,Mˆ (cid:1) ,WBCE(cid:0) M ,Mˆ(cid:48)(cid:1) +WBCE(cid:0) M ,Mˆ(cid:48)(cid:1)(cid:17) . (5)
NIT 1 1 2 2 1 1 2 2
Oncethemodelistrained,wediscardthenoiseheadsanduseonlythequeryheadsforinference(see
AppendixAforanillustration). UnliketheMixITmodel(Wisdometal.,2020),ourproposednoise
invarianttrainingstillallowsustospecifythetargetsoundbyaninputquery,anditdoesnotrequire
anypost-selectionprocessasweonlyusethequeryheadsduringinference.
Inpractice,wefindthatthemodeltendstoassignpartofthetargetsoundstothenoiseheadsasthese
headscanfreelyenjoytheoptimalpermutationtominimizetheloss. Hence,wefurtherintroducea
regularizationtermtopenalizeproducinghighactivationsonthenoisemasks:
L
=max(cid:18) 0,(cid:88)n mean(cid:16) MˆN(cid:17) −γ(cid:19)
, (6)
REG i
i=1
where γ ∈ [0,n] is a hyperparameter that we will refer to as the noise regularization level. The
proposedregularizationhasnoeffectwhenthesumofthemeansofallthenoisemasksislowerthan
apredefinedthresholdγ,whilehavingalinearlygrowingpenaltywhenthesumishigherthanγ.
Finally,thetrainingobjectiveoftheCLIPSep-NITmodelisaweightedsumofthenoiseinvariantloss
andregularizationterm:L =L +λL ,whereλ∈Risaweighthyperparameter.
CLIPSep-NIT NIT REG
Wesetλ=0.1forallexperiments,whichwefindworkwellacrossdifferentsettings.
4WenotethatCLIPSep-NITconsiders2nsourcesintotalasthemodelhasnqueriedheadsandnnoise
heads.WhilePIT(Yuetal.,2017)andMixIT(Wisdometal.,2020)respectivelyrequireO((2n)!)andO(22n)
searchtoconsider2nsources,theproposedNITonlyrequiresO(n!)permutationinthelosscomputation.
5Sinceourgoalisnottofurtherseparatethenoiseintoindividualsourcesbuttoseparatethesoundsthat
correspondtothequery,nmaynotneedtobelarge. Inpractice,wefindthattheCLIPSep-NITmodelwith
n=2alreadylearnstohandlethenoiseproperlyandcansuccessfullytransfertothetext-queriedmode.Thus,
weusen=2throughoutthispaperandleavethetestingonlargernasfuturework.
5
PublishedasaconferencepaperatICLR2023
Unlabeled Post-proc. Querytype SDR[dB]
Model
data free Training Test Mean Median
Mixture - - - - 0.00±0.89 0.00
Text-queriedmodels
CLIPSep (cid:88) (cid:88) Image Text 5.49±0.72 4.97
CLIPSep-Text (cid:88) Text Text 7.91±0.81 7.46
CLIPSep-Hybrid (cid:88) Text+Image Text 8.36±0.83 8.72
Image-queriedmodels
SOP(Zhaoetal.,2018) (cid:88) (cid:88) Image Image 6.59±0.85 6.22
CLIPSep (cid:88) (cid:88) Image Image 7.03±0.70 5.85
CLIPSep-Text (cid:88) Text Image 6.25±0.72 6.19
CLIPSep-Hybrid (cid:88) Text+Image Image 8.06±0.79 8.01
Nonqueriedmodels
LabelSep (cid:88) Label Label 8.18±0.80 7.82
PIT(Yuetal.,2017) (cid:88) × × 8.68±0.76 7.67
Table2: ResultsontheMUSICdataset. StandarderrorsarereportedinthemeanSDRcolumn. Bold
valuesindicatethelargestSDRachievedpergroup.
4 EXPERIMENTS
WebaseourimplementationsonthecodeprovidedbyZhaoetal.(2018)(https://github.com/
hangzhaomit/Sound-of-Pixels). ImplementationdetailscanbefoundinAppendixC.
4.1 EXPERIMENTSONCLEANDATA
We first evaluate the proposed CLIPSep model without the noise invariant training on musical
instrument sound separation task using the MUSIC dataset, as done in (Zhao et al., 2018). This
experimentisdesignedtofocusonevaluatingthequalityofthelearnedqueryvectorsandthezero-
shotmodalitytransferabilityoftheCLIPSepmodelonasmall,cleandatasetratherthanshowingits
abilitytoseparatearbitrarysounds. TheMUSICdatasetisacollectionof536videorecordingsof
peopleplayingamusicalinstrumentoutof11instrumentclasses. Sincenoexistingworkhastrained
atext-queriedsoundseparationmodelusingonlyunlabeleddatatoourknowledge,wecomparethe
proposedCLIPSepmodelwithtwobaselinesthatserveasupperbounds—thePITmodel(Yuetal.,
2017,seeAppendixDforanillustration)andaversionoftheCLIPSepmodelwherethequerymodel
isreplacedbylearnableembeddingsforthelabels,whichwewillrefertoastheLabelSepmodel. In
addition,wealsoincludetheSOPmodel(Zhaoetal.,2018)toinvestigatethequalityofthequery
vectorsastheCLIPSepandSOPmodelssharethesamenetworkarchitectureexceptthequerymodel.
WereporttheresultsinTable2. OurproposedCLIPSep
model achieves a mean signal-to-distortion ratio (SDR) 10 Text query Image query
(Vincent et al., 2006) of 5.49 dB and a median SDR of 8
4.97dBusingtextqueriesinazero-shotmodalitytransfer 6
setting. Whenusingimagequeries,theperformanceofthe 4
CLIPSepmodeliscomparabletothatoftheSOPmodel. 2
ThisindicatesthattheCLIPembeddingsareasinformative
0
asthoseproducedbytheSOPmodel. Theperformance CLIPSep CLIPSep-Text CLIPSep-Hybrid
difference between the CLIPSep model using text and Figure 4: Mean SDR and standard er-
imagequeriesattesttimeindicatesthezero-shotmodality rorsofthemodelstrainedandtestedon
transfergap. Weobserve1.54dBand0.88dBdifferences differentmodalities.
onthemeanandmedianSDRs,respectively. Moreover,
wealsoreportinTable2andFigure4theperformanceoftheCLIPSepmodelstrainedondifferent
modalitiestoinvestigatetheirmodalitytransferabilityindifferentsettings. Wenoticethatwhenwe
train the CLIPSep model using text queries, dubbed as CLIPSep-Text, the mean SDR using text
queriesincreasesto7.91dB.However,whenwetestthismodelusingimagequeries,weobserve
a 1.66 dB difference on the mean SDR as compared to that using text queries, which is close to
6
RDS
PublishedasaconferencepaperatICLR2023
MUSIC+ VGGSound-Clean+
Unlabeled Post-proc. Median Median
Model MeanSDR MeanSDR
data free SDR SDR
Mixture - - 4.49±1.41 2.04 -0.77±1.31 -0.84
Text-queriedmodels
CLIPSep (cid:88) (cid:88) 9.71±1.21 8.73 2.76±1.00 3.95
CLIPSep-NIT (cid:88) (cid:88) 10.27±1.04 10.02 3.05±0.73 3.26
BERTSep (cid:88) 4.67±0.44 4.41 5.09±0.80 5.49
CLIPSep-Text (cid:88) 10.73±0.99 9.93 5.49±0.82 5.06
Image-queriedmodels
SOP(Zhaoetal.,2018) (cid:88) (cid:88) 11.44±1.18 11.18 2.99±0.84 3.89
CLIPSep (cid:88) (cid:88) 12.20±1.17 12.42 5.46±0.79 5.35
CLIPSep-NIT (cid:88) (cid:88) 11.28±1.08 10.83 4.84±0.66 3.57
CLIPSep-Text (cid:88) 9.89±1.04 8.09 2.45±0.70 1.74
Nonqueriedmodels
PIT(Yuetal.,2017) (cid:88) 12.24±1.20 12.53 5.73±0.79 4.97
LabelSep (cid:88) - - 5.55±0.81 5.29
Table3: ResultsoftheMUSIC+ andVGGSound-Clean+ evaluations(seeSection4.2). Standard
errorsarereportedinthemeanSDR[dB]columns. BoldvaluesindicatethelargestSDRachieved
pergroup. Weuseγ =0.25forCLIPSep-NIT.NotethattheLabelSepmodeldoesnotworkonthe
MUSICdatasetduetothedifferentlabeltaxonomiesoftheMUSICandVGGSounddatasets.
themeanSDRdifferenceweobserveforthemodeltrainedwithimagequeries. Finally,wetraina
CLIPSepmodelusingbothtextandimagequeriesinalternation,dubbedasCLIPSep-Hybrid. We
seethatitleadstothebesttestperformanceforbothtextandimagemodalities,andthereisonlya
meanSDRdifferenceof0.30dBbetweenusingtextandimagequeries. Asareference,theLabelSep
modeltrainedwithlabeleddataperformsworsethantheCLIPSep-Hybridmodelusingtextqueries.
Further,thePITmodelachievesameanSDRof8.68dBandamedianSDRof7.67dB,butitrequires
post-processingtofigureoutthecorrectassignments.
4.2 EXPERIMENTSONNOISYDATA
Next,weevaluatetheproposedmethodonalarge-scaledatasetaimingatuniversalsoundseparation.
WeusetheVGGSounddataset(Chenetal.,2020),alarge-scaleaudio-visualdatasetcontainingmore
than190,00010-secondvideosinthewildoutofmorethan300classes. Wefindthattheaudiointhe
VGGSounddatasetisoftennoisyandcontainsoff-screensoundsandbackgroundnoise. Although
wetrainthemodelsonsuchnoisydata,itisnotsuitabletousethenoisydataastargetsforevaluation
becauseitfailstoprovidereliableresults. Forexample,ifthetargetsoundlabeledas“dogbarking”
alsocontainshumanspeech,separatingonlythedogbarkingsoundprovidesalowerSDRvaluethan
separatingthemixtureofdogbarkingsoundandhumanspeecheventhoughthetextqueryis“dog
barking”. (Notethatweusethelabelsonlyforevaluationbutnotfortraining.) Toavoidthisissue,
weconsiderthefollowingtwoevaluationsettings:
• MUSIC+: SamplesintheMUSICdatasetareusedascleantargetsandmixedwithasample
intheVGGSounddatasetasaninterference. Theseparationqualityisevaluatedontheclean
target from the MUSIC dataset. As we do not use the MUSIC dataset for training, this can
beconsideredaszero-shottransfertoanewdatadomaincontainingunseensounds(Radford
etal.,2019;Brownetal.,2020). Toavoidtheunexpectedoverlapofthetargetsoundtypesin
theMUSICandVGGSounddatasetscausedbythelabelmismatch,weexcludeallthemusical
instrumentplayingvideosfromtheVGGSounddatasetinthissetting.
• VGGSound-Clean+:Wemanuallycollect100cleansamplesthatcontaindistincttargetsounds
fromtheVGGSoundtestset,whichwewillrefertoasVGGSound-Clean. Wemixanaudio
sampleinVGGSound-CleanwithanotherinthetestsetofVGGSound. Similarly,weconsider
theVGGSoundaudioasaninterferencesoundaddedtotherelativelycleanerVGGSound-Clean
audioandevaluatetheseparationqualityontheVGGSound-Cleanstem.
7
PublishedasaconferencepaperatICLR2023
(a)Inputmixture (b)Groundtruth (c)Interference (d)Prediction (e)Noisehead1 (f)Noisehead2
Figure 5: Example results of the proposed CLIPSep-NIT model with γ = 0.25 on the MUSIC+
dataset. We mix the an audio sample (“violin” in this example) in the MUSIC dataset with an
interferenceaudiosample(“peoplesobbing”inthisexample)intheVGGSounddatasettocreate
anartificialmixture. (b)and(c)showthereconstructedsignalsusingthegroundtruthidealbinary
masks. Thespectrogramsareshowninthelogfrequencyscale. Weobservethattheproposedmodel
successfullyseparatesthedesiredsounds(i.e.,(d))fromquery-irrelevantnoise(i.e.,(e)and(f)).
Table3showstheevaluationresults. First,CLIPSepsuccessfullylearnstext-queriedsoundseparation
evenwithnoisyunlabeleddata,achieving5.22dBand3.53dBSDRimprovementsoverthemixture
onMUSIC+ andVGGSound-Clean+,respectively. BycomparingCLIPSepandCLIPSep-NIT,we
observethatNITimprovesthemeanSDRsinbothsettings. Moreover,onMUSIC+,CLIPSep-NIT’s
performancematchesthatofCLIPSep-Text,whichutilizeslabelsfortraining,achievingonlya0.46
dBlowermeanSDRandevena0.05dBhighermedianSDR.Thisresultsuggeststhattheproposed
self-supervisedtext-queriedsoundseparationmethodcanlearnseparationcapabilitycompetitivewith
thefullysupervisedmodelinsometargetsounds. Incontrast,thereisstillagapbetweenthemon
VGGSound-Clean+,possiblybecausethevideosofnon-music-instrumentobjectsaremorenoisyin
bothaudioandvisualdomains,thusresultinginamorechallengingzero-shotmodalitytransfer. This
hypothesisisalsosupportedbythehigherzero-shotmodalitytransfergap(meanSDRdifferenceof
image-andtext-queriedmode)of1.79dBonVGGSound-Clean+thanthatof1.01dBonMUSIC+
forCLIPSep-NIT.Inaddition,weconsideranotherbaselinemodelthatreplacestheCLIPmodel
in CLIPSep with a BERT encoder (Devlin et al., 2019), which we call BERTSep. Interestingly,
althoughBERTSepperformssimilarlytoCLIPSep-TextonVGGSound-Clean+,theperformanceof
BERTSepissignificantlylowerthanthatofCLIPSep-TextonMUISC+,indicatingthatBERTSep
failstogeneralizetounseentextqueries. WehypothesizethattheCLIPtextembeddingcapturesthe
timbralsimilarityofmusicalinstrumentsbetterthantheBERTembeddingdo,becausetheCLIP
modelisawareofthevisualsimilaritybetweenmusicalinstrumentsduringtraining. Moreover,it
isinterestingtoseethatCLIPSepoutperformsCLIPSep-NITwhenanimagequeryisusedattest
time(domain-matchedcondition),possiblybecauseimagescontainrichercontextinformationsuch
asobjectsnearbyandbackgroundsthanlabels,andthemodelscanusesuchinformationtobetter
separatethetargetsound. WhileCLIPSephastofullyutilizesuchinformation,CLIPSep-NITcanuse
thenoiseheadstomodelsoundsthatarelessrelevanttotheimagequery. Sinceweremovethenoise
headsfromCLIPSep-NITduringtheevaluation,itcanrelylessonsuchinformationfromtheimage,
thusimprovingthezero-shotmodalitytransferability. Figure5showsanexampleoftheseparation
resultsonMUSIC+(seeFigures12to15formoreexamples). Weobservethatthetwonoiseheads
containmostlybackgroundnoise. Audiosamplescanbefoundonourdemowebsite.1
4.3 EXAMININGTHEEFFECTSOFTHENOISEREGULARIZATIONLEVELγ
In this experiment, we examine the effects of the noise regularization level γ in Equation (6) by
changingthevaluefrom0to1.AswecanseefromFigure6(a)and(b),CLIPSep-NITwithγ =0.25
achievesthehighestSDRonbothevaluationsettings. Thissuggeststhattheoptimalγ valueisnot
sensitivetotheevaluationdataset. Further,wealsoreportinFigure6(c)thetotalmeannoisehead
activation,(cid:80)n mean(MˆN),onthevalidationset. AsMˆN isthemaskestimateforthenoise,the
i=1 i i
totalmeannoiseheadactivationvalueindicatestowhatextentsignalsareassignedtothenoisehead.
Weobservethattheproposedregularizersuccessfullykeepsthetotalmeannoiseheadactivation
closetothedesiredlevel,γ,forγ ≤0.5. Interestingly,thetotalmeannoiseheadactivationisstill
around0.5whenγ = 1.0,suggestingthatthemodelinherentlytriestouseboththequery-heads
andthenoiseheadstopredictthenoisytargetsounds. Moreover,whilewediscardthenoiseheads
duringevaluationinourexperiments,keepingthenoiseheadscanleadtoahigherSDRasshownin
8
PublishedasaconferencepaperatICLR2023
6.0 1.0
11 11 .. 05 C CL LI IP PS Se ep p- -N NI IT T (w/ noise heads) 5.5 C CL LI IP PS Se ep p- -N NI IT T (w/ noise heads) 0.8
5.0
10.5
10.0 4.5 0.6 9.5 4.0
9.0 3.5 0.4
8.5 3.0 0.2 8.0 2.5
7.5 2.0 0.0
(a) 0.0 0.2 0.4 0.6 0.8 1.0 (b) 0.0 0.2 0.4 0.6 0.8 1.0 (c) 0.0 0.2 0.4 0.6 0.8 1.0
Noise regularization level, Noise regularization level, Noise regularization level,
Figure6: Effectsofthenoiseregularizationlevelγ fortheproposedCLIPSep-NITmodel—mean
SDRforthe(a)MUSIC+and(b)VGGSound-Clean+evaluations,and(c)thetotalmeannoisehead
activation,(cid:80)n mean(MˆN),onthevalidationset. Theshadedareasshowstandarderrors.
i=1 i
Figure6(a)and(b),whichcanbehelpfulincertainusecaseswhereapost-processingprocedure
similartothePITmodel(Yuetal.,2017)isacceptable.
5 DISCUSSIONS
Fortheexperimentspresentedinthispaper,weworkonlabeleddatasetssothatwecanevaluatethe
performanceoftheproposedmodels. However,ourproposedmodelsdonotrequireanylabeleddata
fortraining,andcanthusbetrainedonlargerunlabeledvideocollectionsinthewild. Moreover,we
observethattheproposedmodelshowsthecapabilityofcombingmultiplequeries,e.g.,“aphotoof
[queryA]and[queryB],”toextractmultipletargetsounds,andwereporttheresultsonthedemo
website. Thisoffersamorenaturaluserinterfaceagainsthavingtoseparateeachtargetsoundand
mixthemviaanadditionalpost-processingstep. WealsoshowinAppendixGthatourproposed
modelisrobusttodifferenttextqueriesandcanextractthedesiredsounds.
Inourexperiments,weoftenobserveamodalitytransfergapgreaterthan1dBdifferenceofSDR.A
futureresearchdirectionistoexploredifferentapproachestoreducethemodalitytransfergap. For
example,theCLIPmodelispretrainedonadifferentdataset,andthusfinetuningtheCLIPmodelon
thetargetdatasetcanhelpimprovetheunderlyingmodalitytransferabilitywithintheCLIPmodel.
Further,whiletheproposednoiseinvarianttrainingisshowntoimprovethetrainingonnoisydata
andreducethemodalitytransfergap,itstillrequiresasufficientaudio-visualcorrespondencefor
training video. In other words, if the audio and images are irrelevant in most videos, the model
willstruggletolearnthecorrespondencebetweenthequeryandtargetsound. Inpractice,wefind
thatthedataintheVGGSounddatasetoftencontainsoff-screensoundsandthelabelssometimes
correspond to only part of the video content. Hence, filtering on the training data to enhance its
audio-visualcorrespondencecanalsohelpreducethemodalitytransfergap. Thiscanbeachieved
byself-supervisedaudio-visualcorrespondenceprediction(Arandjelovic´ &Zisserman,2017a;b)or
temporalsynchronization(Korbaretal.,2018;Owens&Efros,2018).
Anotherfuturedirectionistoexplorethesemi-supervisedsettingwhereasmallsubsetoflabeled
datacanbeusedtoimprovethemodalitytransferability. Wecanalsoconsidertheproposedmethod
asapretrainingonunlabeleddataforotherseparationtasksinthelow-resourceregime. Weinclude
inAppendixHapreliminaryexperimentinthisaspectusingtheESC-50dataset(Piczak,2015).
6 CONCLUSION
In this work, we have presented a novel text-queried universal sound separation model that can
betrainedonnoisyunlabeledvideos. Inthisend,wehaveproposedtousethecontrastiveimage-
languagepretrainingtobridgetheaudioandtextmodalities,andproposedthenoiseinvarianttraining
fortrainingaquery-basedsoundseparationmodelonnoisydata. Wehaveshownthattheproposed
models can learn to separate an arbitrary sound specified by a text query out of a mixture, even
achievingcompetitiveperformanceagainstafullysupervisedmodelinsomesettings. Webelieve
ourproposedapproachclosesthegapbetweenthewayshumansandmachineslearntofocusona
soundinamixture,namely,themulti-modalself-supervisedlearningparadigmofhumansagainstthe
supervisedlearningparadigmadoptedbyexistinglabel-basedmachinelearningapproaches.
9
RDS RDS
noitavitca
daeh esion
naem
latoT
PublishedasaconferencepaperatICLR2023
ACKNOWLEDGEMENTS
WewouldliketothankStefanUhlich,GiorgioFabbroandWoosungChoifortheirhelpfulcomments
duringthepreparationofthismanuscript. WealsothankMayankKumarSinghforsupportingthe
setupofthesubjectivetestinAppendixF.Hao-WenthankJ.YangandFamilyFoundationandTaiwan
MinistryofEducationforsupportinghisPhDstudy.
REFERENCES
TriantafyllosAfouras,AndrewOwens,JoonSonChung,,andAndrewZisserman. Self-supervised
learningofaudio-visualobjectsfromvideo. InProc.ECCV,2020. (Citedonpage2.)
ReljaArandjelovic´ andAndrewZisserman. Look,listenandlearn. InProc.ICCV,2017a. (Citedon
page9.)
ReljaArandjelovic´ andAndrewZisserman. Objectsthatsound. InProc.ECCV,2017b. (Citedon
page9.)
FrancisR.BachandMichaelI.Jordan. Blindone-microphonespeechseparation: Aspectrallearning
approach. InProc.NIPS,2005. (Citedonpage1.)
Rene´eBaillargeon. Theacquisitionofphysicalknowledgeininfancy: Asummaryineightlessons.
Blackwellhandbookofchildhoodcognitivedevelopment,2002. (Citedonpage1.)
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,
JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,
BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,
andDarioAmodei. Languagemodelsarefew-shotlearners. InProc.NeurIPS,2020. (Citedon
page7.)
Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. VGGSound: A large-scale
audio-visualdataset. InProc.ICASSP,2020. (Citedonpage7.)
KeChen,XingjianDu,BileiZhu,ZejunMa,TaylorBerg-Kirkpatrick,andShlomoDubnov. Zero-
shotaudiosourceseparationthroughquery-basedlearningfromweakly-labeleddata. InProc.
AAAI,2022. (Citedonpage3.)
EColinCherry. Someexperimentsontherecognitionofspeech,withoneandwithtwoears. The
JournaloftheacousticalsocietyofAmerica,25,1953. (Citedonpage1.)
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. InProc.NAACL,2019. (Citedonpage8.)
ArielEphrat,InbarMosseri,OranLang,TaliDekel,KevinWilson,AvinatanHassidim,WilliamT
Freeman,andMichaelRubinstein. Lookingtolistenatthecocktailparty: Aspeaker-independent
audio-visualmodelforspeechseparation. ACMTransactionsonGraphics,37,2019. (Citedon
page2.)
RuohanGao,RogerioFeris,andKristenGrauman. Learningtoseparateobjectsoundsbywatching
unlabeledvideo. InProc.ECCV,2018. (Citedonpage2.)
Beat Gfeller, Dominik Roblek, and Marco Tagliasacchi. One-shot conditional audio filtering of
arbitrarysounds. InProc.ICASSP,2021. (Citedonpage3.)
AndreyGuzhov,FedericoRaue,Jo¨rnHees,andAndreasDengel. AudioCLIP:ExtendingCLIPto
image,textandaudio. InProc.ICASSP,2022. (Citedonpage3.)
AndreasJansson,EricHumphrey,NicolaMontecchio,RachelBittner,AparnaKumar,andTillman
Weyde. SingingvoiceseparationwithdeepU-Netconvolutionalnetworks. InProc.ISMIR,2017.
(Citedonpage3.)
10
PublishedasaconferencepaperatICLR2023
IlyaKavalerov,ScottWisdom,HakanErdogan,BrianPatton,KevinWilson,JonathanLeRoux,and
JohnR.Hershey. Universalsoundseparation. InProc.WASPAA,2019. (Citedonpages2and3.)
KevinKilgour,BeatGfeller,QingqingHuang,ArenJansen,ScottWisdom,andMarcoTagliasacchi.
Text-drivenseparationofarbitrarysounds. InProc.INTERSPEECH,2022. (Citedonpages1and3.)
DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InProc.ICLR,
2015. (Citedonpage13.)
QiuqiangKong,YuxuanWang,XuchenSong,YinCao,WenwuWang,andMarkD.Plumbley.Source
separationwithweaklylabelleddata: Anapproachtocomputationalauditorysceneanalysis. In
Proc.ICASSP,2020. (Citedonpages2and3.)
BrunoKorbar,DuTran,andLorenzoTorresani. Cooperativelearningofaudioandvideomodels
fromself-supervisedsynchronization. InProc.NeurIPS,2018. (Citedonpage9.)
SeungHyunLee,WonseokRoh,WonminByeon,SangHoYoon,ChanYoungKim,JinkyuKim,and
SangpilKim. Sound-guidedsemanticimagemanipulation. InProc.CVPR,2022. (Citedonpage3.)
XuboLiu,HaoheLiu,QiuqiangKong,XinhaoMei,JinzhengZhao,QiushiHuang,MarkD.Plumbley,
andWenwuWang. Separatewhatyoudescribe: Language-queriedaudiosourceseparation. In
Proc.INTERSPEECH,2022. (Citedonpages1and3.)
YukiMitsufuji,GiorgioFabbro,StefanUhlich,Fabian-RobertSto¨ter,AlexandreDe´fossez,Minseok
Kim,WoosungChoi,Chin-YunYu,andKin-WaiCheuk. Musicdemixingchallenge2021,2021.
(Citedonpage2.)
TsubasaOchiai,MarcDelcroix,YumaKoizumi,HiroakiIto,KeisukeKinoshita,andShokoAraki.
Listentowhatyouwant: Neuralnetwork-baseduniversalsoundselector. InProc.INTERSPEECH,
2020. (Citedonpages2and3.)
AndrewOwensandAlexeiAEfros. Audio-visualsceneanalysiswithself-supervisedmultisensory
features. InProc.ECCV,2018. (Citedonpages2and9.)
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDesmaison,AndreasKo¨pf,Edward
Yang,ZachDeVito,MartinRaison,AlykhanTejani,SasankChilamkurthy,BenoitSteiner,LuFang,
JunjieBai,andSoumithChintala. PyTorch: Animperativestyle,high-performancedeeplearning
library. InProc.NeurIPS,2019. (Citedonpage13.)
KarolJ.Piczak. ESC:Datasetforenvironmentalsoundclassification. InProc.MM,2015. (Citedon
pages9and15.)
AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever. Language
modelsareunsupervisedmultitasklearners. TechnicalReportofOpenAI,2019. (Citedonpage7.)
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever.
Learning transferable visual models from natural language supervision. In Proc. ICML, 2021.
(Citedonpages2,3,4,and12.)
Torsten Rahne, Martin Bo¨ckmann, Hellmut von Specht, and Elyse S Sussman. Visual cues can
modulateintegrationandsegregationofobjectsinauditorysceneanalysis. Brainresearch,2007.
(Citedonpage2.)
OlafRonneberger,PhilippFischer,andThomasBrox. U-Net:Convolutionalnetworksforbiomedical
imagesegmentation. InProc.MICCAI,2015. (Citedonpages3and13.)
Andrew Rouditchenko, Hang Zhao, Chuang Gan, Josh McDermott, and Antonio Torralba. Self-
supervisedaudio-visualco-segmentation. InProc.ICASSP,2019. (Citedonpage2.)
WisdomScott,ArenJansen,RonJ.Weiss,HakanErdogan,andJohnR.Hershey. Sparse,efficient,
andsemanticmixtureinvarianttraining: Tamingin-the-wildunsupervisedsoundseparation. In
Proc.WASPAA,2021. (Citedonpage2.)
11
PublishedasaconferencepaperatICLR2023
RobertSekuler,AllisonB.Sekuler,andReneeLau. Soundaltersvisualmotionperception. Nature,
385,1997. (Citedonpage2.)
ShinsukeShimojoandLadanShams. Sensorymodalitiesarenotseparatemodalities: plasticityand
interactions. CurrentOpinioninNeurobiology,11,2001. (Citedonpage2.)
Fabian-RobertSto¨ter,AntoineLiutkus,andNobutakaIto. The2018signalseparationevaluation
campaign. InProc.LVA/ICA,2018. (Citedonpage13.)
NaoyaTakahashiandYukiMitsufuji. Denselyconnectedmultidilatedconvolutionalnetworksfor
densepredictiontasks. InProc.CVPR,2021. (Citedonpage2.)
YapengTian,DiHu,andChenliangXu. Cyclicco-learningofsoundingobjectvisualgroundingand
soundseparation. InProc.CVPR,2021. (Citedonpage2.)
Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Daniel P. W. Ellis,
andJohnR.Hershey. IntothewildwithAudioScope: Unsupervisedaudio-visualseparationof
on-screensounds. InProc.ICLR,2021. (Citedonpages2and3.)
Emmanuel Vincent, Re´mi Gribonval, and Ce´dric Fe´votte. Performance measurement in blind
audiosourceseparation. IEEETransactionsonAudio,Speech,andLanguageProcessing,14(4):
1462–1469,2006. (Citedonpage6.)
DeLiangWangandJitongChen. Supervisedspeechseparationbasedondeeplearning: Anoverview.
IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,26(10):1702–1726,2018.
(Citedonpage2.)
ScottWisdom,EfthymiosTzinis,HakanErdogan,RonJ.Weiss,KevinWilson,andJohnR.Hershey.
Unsupervisedsoundseparationusingmixtureinvarianttraining. InProc.NeurIPS,2020. (Citedon
pages2,3,4,and5.)
Ho-HsiangWu,PremSeetharaman,KundanKumar,andJuanPabloBello. Wav2CLIP:Learning
robustaudiorepresentationsfromCLIP. InProc.ICASSP,2022. (Citedonpage3.)
DongYu,MortenKolbæk,Zheng-HuaTan,andJesperJensen. Permutationinvarianttrainingofdeep
modelsforspeaker-independentmulti-talkerspeechseparation. InProc.ICASSP,2017. (Citedon
pages2,5,6,7,9,14,18,19,20,and21.)
JingzhaoZhang, TianxingHe, SuvritSra, andAliJadbabaie. Whygradientclippingaccelerates
training: Atheoreticaljustificationforadaptivity. InProc.ICLR,2020. (Citedonpage13.)
Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio
Torralba. Thesoundofpixels. InProc.ECCV,2018. (Citedonpages2,3,6,7,and14.)
Hang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Torralba. The sound of motions. In Proc.
ICCV,2019. (Citedonpage2.)
A INFERENCE PIPELINE OF CLIPSEP AND CLIPSEP-NIT
Figure 7 illustrates the inference pipeline for the proposed CLIPSep and CLIPSep-NIT models.
FortheCLIPSep-NITmodel,wediscardthenoiseheadsattesttime. Themaskedspectrogramis
combined with the input phase and converted to the waveform by the inverse short-time Fourier
transform(STFT).
B QUERY ENSEMBLING
Radfordetal.(2021)suggestthatusingaprompttemplateintheformof“aphotoof[userinput
query]”helpsbridgethedistributiongapbetweentextqueriesusedforzero-shotimageclassification
andtextinthetrainingdatasetfortheCLIPmodel. Theyfurthershowthattheensembleofvarious
prompttemplatesimprovethegeneralizability. Motivatedbythisobservation,weadoptasimilaridea
anduseseveralquerytemplatesattesttime(seeTable4). Thesequerytemplatesareheuristically
chosentohandlethenoisyimagesextractedfromvideos.
12
PublishedasaconferencepaperatICLR2023
Figure 7: Inference pipeline of the proposed CLIPSep and CLIPSep-NIT models. Note that the
mixturespectrogramandthemasksareshowninlogfrequencyscale,whilethemaskedspectrogram
isshowninlinearfrequencyscale.
Querytemplate Examplequeryforthelabel“dogbarking”
aphotoof[userinputquery] aphotoofdogbarking
aphotoofthesmall[userinputquery] aphotoofthesmalldogbarking
alowresolutionphotoofa[userinputquery] alowresolutionphotoofadogbarking
aphotoofmany[userinputquery] aphotoofmanydogbarking
Table4: Querytemplatesusedinourexperiments.
C IMPLEMENTATION DETAILS
Weimplementtheaudiomodelasa7-layerU-Net(Ronnebergeretal.,2015). Weusek =32. We
usebinarymasksasthegroundtruthmasksduringtrainingwhileusingtheraw,real-valuedmasks
forevaluation. Wetrainallthemodelsfor200,000stepswithabatchsizeof32. WeusetheAdam
optimizer(Kingma&Ba,2015)withβ =0.9,β =0.999and(cid:15)=10−8. Inaddition,weclipthe
1 2
normofthegradientsto1.0(Zhangetal.,2020). Weadoptthefollowinglearningrateschedule
withawarm-up—thelearningratestartsfrom0andgrowsto0.001after5,000steps,andthenit
linearlydropsto0.0001at100,000stepsandkeepsthisvaluethereafter. Wevalidatethemodelevery
10,000stepsusingimagequeriesaswedonotassumelabeleddataisavailableforthevalidation
set. We use a sampling rate of 16,000 Hz and work on audio clips of length 65,535 samples (≈
4 seconds). During training, we randomly sample a center frame from a video and extract three
frames(images)with1-secintervalsand4-secaudioaroundthecenterframe. Duringinference,for
image-queriedmodels,weextractthreeframeswith1-secintervalsaroundthecenterofthetestclip.
Forthespectrogramcomputation,weuseafilterlengthof1024,ahoplengthof256andawindow
sizeof1024intheshort-timeFouriertransform(STFT).Weresizeimagesextractedfromvideoto
asizeof224-by-224pixels. FortheCLIPSep-Hybridmodel,wealternativelytrainthemodelwith
textandimagequeries,i.e.,onebatchwithallimagequeriesandnextwithalltextqueries,andso
on. WeimplementallthemodelsusingthePyTorchlibrary(Paszkeetal.,2019). Wecomputethe
signal-to-distortionratio(SDR)usingmuseval(Sto¨teretal.,2018).
Inourpreliminaryexperiments,wealsotrieddirectlypredictingthefinalmaskbyconditioningthe
audiomodelonthequeryvector. WeappliedthismodificationforbothSOPandCLIPSepmodels,
however,weobservethatthisarchitectureispronetooverfitting. Wehypothesizethatthisisbecause
theaudiomodelispowerfulenoughtorememberthesubtlecluesinthequeryvector,whichhinder
the generalization to a new sound and query. In contrast, the proposed architecture first predicts
over-determinedmasksandthencombinesthemonthebasisofthequeryvector,whichavoidsthe
overfittingproblemduetothesimplefusionstep.
13
PublishedasaconferencepaperatICLR2023
Figure8: AnillustrationofthePITmodelforn=2. Thetwopredictedmasksareinterchangeable
during the loss computation. Since the two predicted masks are interchangeable, the PIT model
requiresanadditionalpost-selectionsteptoobtainthetargetsound.
D PERMUTATION INVARIANT TRAINING
Figure8illustratesthepermuatationinvarianttraining(PIT)model(Yuetal.,2017). Thepermutation
invariantlossisdefinedasfollowsforn=2.
L =min(cid:0) WBCE(M ,Mˆ )+WBCE(M ,Mˆ ),WBCE(M ,Mˆ )+WBCE(M ,Mˆ )(cid:1) , (7)
PIT 1 1 2 2 1 2 2 1
whereMˆ andMˆ arethepredictedmasks. NotethatthePITmodelrequiresanadditionalpost-
1 2
selectionsteptoobtainthetargetsound.
E QUALITATIVE EXAMPLE RESULTS
WeshowinFigures12to15someexampleresults. Moreresultsandaudiosamplescanbefoundat
https://sony.github.io/CLIPSep/.
F SUBJECTIVE EVALUATION
WeconductasubjectivetesttoevaluatewhethertheSDRresultsalignedwithperceptualquality. As
doneintheSoundofPixel(Zhaoetal.,2018),separatedaudiosamplesarerandomlypresentedto
evaluators,andthefollowingquestionisasked: “Whichsounddoyouhear? 1. A,2. B,3. Both,or
4. Noneofthem”. HereAandBarereplacedbylabelsoftheirmixturesources,e.g. A=accordion,
B=engineaccelerating. Tensamples(includingnaturallyoccurringmixture)areevaluatedforeach
modeland16evaluatorshaveparticipatedintheevaluation.Table5showsthepercentagesofsamples
whicharecorrectlyidentifiedthetargetsoundclass(Correct),whichareincorrectlyidentifiedthe
target sound sources (Wrong), which are selected as both sounds are audible (Both), and which
are selected as neither of the sounds are audible (None). The results indicate that the evaluators
moreoftenchoosethecorrectsoundsourceforCLIPSep-NIT(83.8%)thanCLIPSep(66.3%)with
textqueries. Notably,CLIPSep-NITwithtext-queryobtainedahighercorrectscorethanthatwith
image-query,whichmatchesthetrainingmode. Thisisprobablybecauseimagequeriesoftencontain
informationaboutbackgroundsandenvironments,hence,somenoiseandoff-screensoundsarealso
suggestedbytheimage-queriesandleaktothequeryhead. Incontrast,text-queriespurelycontain
theinformationoftargetsounds,thus,thequeryheadmoreaggressivelyextractthetargetsounds.
G ROBUSTNESS TO DIFFERENT QUERIES
Toexaminethemodel’srobustnesstodifferentqueries,wetakethesameinputmixtureandquery
themodelwithdifferenttextqueries. WeusetheCLIPSep-NITmodelontheMUSIC+datasetand
14
PublishedasaconferencepaperatICLR2023
Model Querytype Correct[%] Wrong[%] Both[%] None[%]
Mixture - 17.5 10.0 72.5 0.0
Image 70.6 0.0 29.4 0.0
CLIPSep
Text 66.3 3.8 30.0 0.0
Image 68.8 1.9 28.1 1.3
CLIPSep-NIT
Text 83.8 0.6 15.0 0.6
Table5: Subjectivetestresults.
Dataset SDR
Model Post-processing
free Training Finetuning Mean Median
Mixture - - - 0.00±0.44 0.00
PIT × VGGSound - 4.90±0.26 2.44
(cid:88) VGGSound - 1.07±0.28 2.34
CLIPSep (cid:88) ESC-50 - 5.18±0.26 5.09
(cid:88) VGGSound ESC-50 6.73±0.26 5.89
Table6: ResultsontheESC-50dataset. StandarderrorsarereportedinthemeanSDRcolumn.
reportinFigure16theresults. Weseethatthemodelisrobusttodifferenttextqueriesandcanextract
thedesiredsounds. Audiosamplescanbefoundathttps://sony.github.io/CLIPSep/.
H FINETUNING EXPERIMENTS ON THE ESC-50 DATASET
Inthisexperiment,weaimtoexaminethepossibilitiesofhavingacleandatasetforfurtherfinetuning.
WeconsidertheESC-50dataset(Piczak,2015),acollectionof2,000high-qualityenvironmental
audiorecordings,asthecleandatasethere.6 WereporttheexperimentalresultsinTable6. Wecan
seethatthemodelpretrainedonVGGSounddoesnotgeneralizewelltotheESC-50datasetasthe
ESC-50containsmuchcleanersounds,i.e.,withoutquery-irrelevantsoundsandbackgroundnoise.
Further,ifwetraintheCLIPSepmodelfromscratchontheESC-50dataset,itcanonlyachievea
meanSDRof5.18dBandamedianSDRof5.09dB.However,ifwetakethemodelpretrainedon
theVGGSounddatasetandfinetuneitontheESC-50dataset,itcanachieveameanSDRof6.73dB
andamedianSDRof4.89dB,resultinginanimprovementof1.55dBonthemeanSDR.
I TRAINING BEHAVIORS
WepresentinFigure9thetrainingandvalidationlossesalongthetrainingprogress. Pleasenotethat
weonlyshowtheresultsobtainedusingtextqueriesforreferencebutdonotusethemforchoosing
thebestmodel. Wealsoevaluatetheintermediatecheckpointsevery10,000stepsandpresentin
Figure10thetestSDRalongthetrainingprogress. Inaddition,fortheCLIPSep-NITmodel,we
visualizeinFigure11thetotalmeannoiseheadactivation,(cid:80)n mean(MˆN),alongthetraining
i=1 i
progress. Wecanseethatthetotalmeannoiseheadactivationstaysaroundthedesiredlevelfor
γ = 0.1,0.25. For γ = 0.5 and the unregularized version, the total mean noise head activation
convergestoasimilarvaluearound0.55.
6https://github.com/karolpiczak/ESC-50
15
PublishedasaconferencepaperatICLR2023
0.350 Train 0.350 Train 0.350 Train
Valid Valid Valid
0.325 Valid (text) 0.325 0.325 Valid (text)
0.300 0.300 0.300
0.275 0.275 0.275
0.250 0.250 0.250
0.225 0.225 0.225
0.200 0.200 0.200
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Steps Steps Steps
(a)CLIPSep (b)PIT (c)CLIPSep-NIT(unregularized)
0.350 Train 0.350 Train 0.350 Train
Valid Valid Valid
0.325 Valid (text) 0.325 Valid (text) 0.325 Valid (text)
0.300 0.300 0.300
0.275 0.275 0.275
0.250 0.250 0.250
0.225 0.225 0.225
0.200 0.200 0.200
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Steps Steps Steps
(d)CLIPSep-NIT(γ =0.5) (e)CLIPSep-NIT(γ =0.25) (f)CLIPSep-NIT(γ =0.1)
Figure 9: Training and validation losses along the training progress on the VGGSound dataset.
Wealsoincludethelossescomputedusingtextqueriesinsteadofimagequeries. They-axesare
intentionallysettothesamerangeforeasycomparison. Notethatwedonotusethevalidationresults
obtainedwithtextqueriesforchoosingthebestmodel.
8 8 8
7 7 7
6 6 6
5 5 5
4 4 4
3 3 3
2 2 2
Image Image Image (w/ noise heads)
1 Text 1 Image 1 Text Text (w/ noise heads)
0 0 0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Steps Steps Steps
(a)CLIPSep (b)PIT (c)CLIPSep-NIT(unregularized)
8 8 8
7 7 7
6 6 6
5 5 5
4 4 4
3 3 3
2 2 2
Image Image (w/ noise heads) Image Image (w/ noise heads) Image Image (w/ noise heads)
1 Text Text (w/ noise heads) 1 Text Text (w/ noise heads) 1 Text Text (w/ noise heads)
0 0 0
0 50000 100000 150000 200000 0 50000 100000 150000 200000 0 50000 100000 150000 200000
Steps Steps Steps
(d)CLIPSep-NIT(γ =0.5) (e)CLIPSep-NIT(γ =0.25) (f)CLIPSep-NIT(γ =0.1)
Figure10: TestSDRalongthetrainingprogressontheVGGSound-Cleandataset. They-axesare
intentionallysettothesamerangeforeasycomparison.
16
ssoL
ssoL
RDS
RDS
ssoL
ssoL
RDS
RDS
ssoL
ssoL
RDS
RDS
PublishedasaconferencepaperatICLR2023
0.7 = 0.1 = 0.5
= 0.25 Unregularized
0.6
0.5
0.4
0.3
0.2
0 50000 100000 150000 200000
Steps
Figure11:Totalmeannoiseheadactivation,(cid:80)n mean(MˆN),onthevalidationsetfortheCLIPSep-
i=1 i
NITmodelsalongthetrainingprogress.
17
noitavitca
daeh
esion
naem
latoT
PublishedasaconferencepaperatICLR2023
Groundtruth
Mixture Groundtruth
(Interference)
CLIPSep-NIT PIT
CLIPSep
(γ =0.25) (Yuetal.,2017)
Noisehead1 Noisehead2
(CLIPSep-NIT) (CLIPSep-NIT)
Figure12: ExampleresultsontheMUSIC+dataset. Targetsource—“violin”;interference—“people
sobbing”;query—“violin”. Thespectrogramsandmasksareshowninthelogandlinearfrequency
scales,respectively.
18
PublishedasaconferencepaperatICLR2023
Groundtruth
Mixture Groundtruth
(Interference)
CLIPSep-NIT PIT
CLIPSep
(γ =0.25) (Yuetal.,2017)
Noisehead1 Noisehead2
(CLIPSep-NIT) (CLIPSep-NIT)
Figure13: ExampleresultsontheMUSIC+dataset. Targetsource—“acousticguitar”;interference—
“cheetahchirrup”,query—“acousticguitar”. Thespectrogramsandmasksareshowninthelogand
linearfrequencyscales,respectively.
19
PublishedasaconferencepaperatICLR2023
Groundtruth
Mixture Groundtruth
(Interference)
CLIPSep-NIT PIT
CLIPSep LabelSep
(γ =0.25) (Yuetal.,2017)
Noisehead1 Noisehead2
(CLIPSep-NIT) (CLIPSep-NIT)
Figure 14: Example results on the VGGSound-Clean+ dataset. Target source—“cat growling”;
interference—“railroadcartrainwagon”;query—“catgrowling”. Thespectrogramsandmasksare
showninthelogandlinearfrequencyscales,respectively.
20
PublishedasaconferencepaperatICLR2023
Groundtruth
Mixture Groundtruth
(Interference)
CLIPSep-NIT PIT
CLIPSep LabelSep
(γ =0.25) (Yuetal.,2017)
Noisehead1 Noisehead2
(CLIPSep-NIT) (CLIPSep-NIT)
Figure 15: Example results on the VGGSound-Clean+ dataset. Target source—“electric grinder
grinding”;interference—“vehiclehorncarhornhonking”;query—“electricgrindergrinding”. The
spectrogramsandmasksareshowninthelogandlinearfrequencyscales,respectively. Notethatthe
PITmodelrequiresapost-selectionsteptogetthecorrectsource. Withoutthepost-selectionstep,the
PITmodelreturntherightsourceinonlya50%chance.
21
PublishedasaconferencepaperatICLR2023
Groundtruth
Mixture Groundtruth
(Interference)
Prediction
Prediction Prediction
(Query:”amanisplaying
(Query:”acousticguitar”) (Query:”guitar”)
acousticguitar”)
Prediction
Prediction
(Query:”amanisplaying
(Query:”carengine”)
acousticguitarinaroom”)
Figure16: QueryrobustnessexperimentontheMUSIC+dataset. Targetsource—“acousticguitar”;
interference—“cheetahchirrup”. Thespectrogramsareshowninthelogfrequencyscale.
22

Narrative Text Generation with a Latent Discrete Plan
HarshJhamtani1 TaylorBerg-Kirkpatrick2
1 SchoolofComputerScience,CarnegieMellonUniversity
2 ComputerScienceandEngineering. UniversityofCaliforniaSanDiego
jharsh@cs.cmu.edu, tberg@ucsd.eng.edu
Abstract
Past work on story generation has demon-
stratedtheusefulnessofconditioningonagen-
erationplantogeneratecoherentstories. How-
ever,theseapproacheshaveusedheuristicsor
off-the-shelf models to first tag training sto-
ries with the desired type of plan, and then
train generation models in a supervised fash-
ion. In this paper, we propose a deep latent Figure 1: Our aim is to generate a story given a title.
variable model that first samples a sequence We propose models which first generate a high level
ofanchorwords,onepersentenceinthestory, storyplanrealizedviaasequenceofanchorwords.
aspartofitsgenerativeprocess. Duringtrain-
ing, our model treats the sequence of anchor
words as a latent variable and attempts to in- 2013)orasentencerepresentingtheme(Chenetal.,
duceanchoringsequencesthathelpguidegen- 2019).
eration in an unsupervised fashion. We con- Yaoetal.(2019)notethatcomparedtospecific
duct experiments with several types of sen-
event based representations, using keywords to
tence decoder distributions – left-to-right and
formtheoutlineismoregeneralizableandwidely
non-monotonic, with different degrees of re-
applicable. In this work, we consider a sequence
striction. Further,sinceweuseamortizedvari-
of anchor words as a means to model story out-
ational inference to train our model, we in-
troduce two corresponding types of inference lines. Forexample,inFigure1,givenastorytitle
networkforpredictingtheposterioronanchor ‘Winning the Race’, our model first predicts a se-
words. We conduct human evaluations which quence of anchor words which represents a high
demonstrate that the stories produced by our levelstoryplan. Thereafter,adecoderconditions
modelareratedbetterincomparisonwithbase-
onthetitleandgeneratedsequenceofanchorwords
lines which do not consider story plans, and
togeneratethefinalstory. Weassumeanalignment
aresimilarorbetterinqualityrelativetobase-
betweentheanchorwordsandthestorysentences–
lineswhichuseexternalsupervisionforplans.
Additionally, the proposed model gets favor-
theithanchorwordcorrespondstotheithsentence
able scores when evaluated on perplexity, di- inthestory.
versity,andcontrolofstoryviadiscreteplan. However, stories do not naturally occur with
a tagged set of such anchor words or keywords.
1 Introduction
Many prior works use off the shelf tools to first
Maintaining long-term narrative flow and consis- labelstorieswithplanoutlines,thususingexternal
tencyareimportantconcernswhenaimingtogener- supervisionforlearningplotstructures. Forexam-
ateaplausiblestory(PorteousandCavazza,2009; ple,Yaoetal.(2019)usetheRAKEheuristic(Rose
Houetal.,2019). Priorworkonnarrativetextgen- etal.,2010)tofirstidentifythemostimportantkey-
erationhasfocusedongeneratingconsistentstories wordineachsentence,andthenusethistotraina
via story outlines using keywords or key phrases modelinasupervisedfashion. Thisapproachleads
(Xu et al., 2018; Yao et al., 2019), event-based toimprovedcoherencyandcontrol,butcreatesare-
representations (Riedl and Young, 2010; Martin lianceonsuchheuristicsanddoesnotjointlylearn
etal.,2018;Fanetal.,2019),plotgraphs(Lietal., anchorwordsalongwiththegenerator.
3637
FindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages3637–3650
November16-20,2020.(cid:13)c2020AssociationforComputationalLinguistics
Inspired by prior work indicating that anchor
wordscaneffectivelycaptureandcontrolhigh-level
generationstructure,weinvestigatetowhatextent
high-levelcontrolcanbelearnedinafullyunsuper-
visedfashion,directlyfromnaturalstorydata. We
designahierarchicallatentvariablemodel(Figure
2)thatinducessequencesofanchorwordsthatex-
plainobservedstories,whileatthesametimelearn-
ing to generate entire stories by first generating
anchorsequences. Fortraining,weuseamortized
variationallearning(KingmaandWelling,2014),
whereaninferencenetworkisusedtoapproximate
theposterioronanchorsequences.
At test time, given a title, we first sample a se-
quenceofanchorwordsusingthepriormodelcon-
ditioned on only the title, and then generate the
Figure 2: Model Overview: We consider multi-
actualstoryusingthedecoderconditioningonlyon
sentence text generation via a latent generation plan
thetitleandthesampledanchorwords.
realized through a sequence of anchor words with
Toinduceausefullatentgenerationplanandto one word per sentence. [We show sequence models
effectively condition on a sampled plan, we pro- withfirst-orderMarkovassumptionforsimplicity,even
poseaconstrainedstorydecoderandconstrained though all sequence models in our approach are auto-
regressivewithfullcontext.]
inference network. Specifically, our constrained
decoder begins a story sentence by deterministic
copyingthecorrespondinganchorword,andthen
lablestorygenerationaspervariousautomaticand
generates words to the left and then to the right
humanevaluations.
(Figure3). Forthisdecoder,thecorrespondingtrue
Finally, we note that our modelling approach
posterior on anchor words is sparse: the anchor
forstorygenerationhasaninterestingconnection
wordmustbechosenfromtheobservedsentence.
with work that treats text as a latent variable in
Thus, we constrain the output vocabulary of the
deepgenerativemodels(MiaoandBlunsom,2016;
corresponding inference network to the words of
Wen et al., 2017). We treat a latent sequence of
theinputsentence. Weobservethattheproposed
anchorwordsasaformofhierarchicalcontrolover
constrainedinferencenetworkdoesnotsufferfrom
generated outputs, while related work treats the
mode collapse, leading to models which can ef-
latentsequenceitselfassequentialtextthatisthe
fectively learn useful anchor words. Further, we
outputofthemodel.
also contrast this approach with a model whose
decoderisnotconstrainedtouseeachanchorword
2 Model
ineachsentence. Thetrueposteriorinthiscaseis
overthefullvocabulary. Weconductexperiments
Ourgoalistogenerateastoryx,consistingofmul-
withbothconstrainedandunconstraineddecoders
tiple sentences x ,x ,..x , given a title t. Our
1 2 K
and inference networks, and find that the best re-
model’sgenerativeprocessisdepictedinFigure2
sults are achieved through the combination of an
andoperatesasfollows: First,asequenceofanchor
unconstraineddecoderwithaconstrainedinference
words representing a generation plan is sampled
network–indicating,perhaps,thatwhileitismore
from an auto-regressive prior conditioned on the
effectivetouseflexiblemodels,usingaconstrained
title. Next, for each anchor word, a sentence is
inferencenetworkcanaddausefulinductivebias,
generated conditioned on the anchor words and
leading the model to mimic the constraint of the
previously generated sentences using a decoder.
inferencenetwork.
Duringtraining, thesequenceofanchorwordsis
WeexperimentwithtwoEnglishstorydatasets, unobservedandtreatedasalatentvariable. Asde-
andobservethatourbestmodelsachievefavorable scribedinmoredetaillater,wewillexploreseveral
scoresrelativetoseveralbaselineswhenevaluated choicesofdecoder–thosethattreatanchorwords
on perplexity, diversity, coherency, and control- asanexplicittokeninthesentencetobegenerated,
3638
Figure 3: Simplified demonstration of generation of a sentence conditioned on anchor words and preceding sen-
tences for the two types of decoders: (1) Unconstrained decoder is based on the story generation model of (Yao
etal.,2019),whichmayormaynotusethecorrespondinganchorword. (2)Constraineddecoderisforcedtouse
anchoringwordsincorrespondingsentences,generatingwordstotheleftandthentotherightofananchorword.
[Again,weshowsequencemodelswithafirst-orderMarkovassumptionforsimplicity,eventhoughallsequence
modelsareauto-regressivewithfullcontext. ]
generatingsurroundingcontexttotheleftandright,
and those that simply treat the anchor words as
conditioninginformation. Intheformercase,the
posterior must be sparse. In the latter case, our
choiceofvariationallearningschemewillbias(but
notforce)themodeltouseanchorwordsinoutput
story sentences. We shall refer to our proposed
modelasLatentAnchorPlanmodel(LAP).
2.1 AnchorSequencePrior Figure 4: Constrained Inference Network: Proposed
model is trained through amortized variational learn-
We model the sequence of anchor words repre-
ing using an inference network. One of the proposed
sentingthegenerationplanviaasequenceofdis-
modelsistrainedusingaconstrainedinferencenetwork
creterandomvariablesz 1,z 2,..,z K. Sinceouraim which assigns non-zero probability to only the words
is to induce latent plans, we assume z are unob- presentincorrespondingsentences.
served. Weconsideranauto-regressivepriormodel
(cid:81)
p (z|t) = p (z |z ,t) where each anchor
φ i φ i <i
objective,asdiscussedlater. Atthesametime,the
word is conditioned on preceding anchor words
unconstrained decoder has higher flexibility and
andthetitlet.
canskipusingananchorwordifitdoesn’tfitwith
2.2 StoryDecoder theprecedingcontext.
Ourdecoderp (x|t,z)generatesastorygiventhe
θ
title t and anchor words z. As mentioned earlier, ConstrainedDecoder: Weconsideraconstrained
z i isalignedtothesentencex i. Weconsidertwo decoder that always uses z i while generating x i.
decoders: (1) an unconstrained decoder which is This is achieved by first copying z i, then gener-
not bound to use z in x , and (2) a constrained ating to the left until the sentence start, and then
i i
decoder which assumes z is present in x , and to the right. Such a decoder is bound to use the
i i
constructswordstotheleftandthentotherightof correspondinganchorwordbydesign,andwillpo-
theanchorwordz . tentiallydemonstratehighercontroloftheanchor
i
wordsonthestory.
Unconstrained Decoder: Our unconstrained OurdecoderarchitecturefollowsfromYaoetal.
decoder is based on Yao et al. (2019)’s decoder (2019),whousea3-layerLSTMrecurrentmodel.
which does not use any explicit alignment of Our final reported model uses 1000 dimensional
anchorwordstocorrespondingsentences(Figure hiddenlayer,withtiedinputandoutputwordem-
3). The decoder is fed the title and the anchor beddings. Moreover, the prior model shares the
wordsappendedtogether,andistrainedtogenerate underlyingLSTMmoduleswiththedecoder. Since
themulti-sentencetext. Thedecoderisnotbound ourgoalistoinducealatentdiscreteplanandcom-
to use the anchor word z for x , but may have parewithkeywordtaggingbasedmethods,westick
i i
incentive to do so depending on the training tothesamechoiceofdecoderasinpriorwork.
3639
3 LearningandInference s . Finally,forsentencex ,wecomputeasoftmax
j i
overthescoresofwordsinx toobtainq(z |x).
i i
Our goal is to maximize the log likelihood of
the stories conditioned on the corresponding ti-
Unconstrained Inference Network We also
tles. Since z is unobserved at training, we must
consider an unconstrained inference network
marginalizeoverallpossiblevaluesofz.
whichdoesnotconstraintheinferencenetwork’s
(cid:88) (cid:88) output – i.e. the output distribution is over the
logp(x|t) = logE [p (x|t,z)]
z∼p (z|t) θ
φ entire vocabulary. We use a LSTM model to
t,x∈D t,x∈D
encodeeachsentence,obtainthelastwordhidden
,D representsthedatasetoftitlesandcorrespond- state, and then finally employ a linear layer to
ingstories. Sinceitisinfeasibletocomputetheex- transformittothevocabularysize.
actmarginalstatedabove,weuseamortizedvaria-
tionallearningbyintroducinganinferencenetwork Whenthedecoderisnotconstrained,itmaybe
q ,andtrainthemodeltomaximizethefollowing interestingtocomparethechoiceofinferencenet-
γ
evidencelower-bound(ELBO): work. Using the constrained inference net with
the unconstrained decoder will bias the decoder
ELBO =E z∼qγ(z|x,t)[logp θ(x|z,t)] to use the anchor words in the aligned sentences
(cid:124) (cid:123)(cid:122) (cid:125) – the model is not required to do this, but varia-
Reconstruction
tionallearningwillpulltheinferencenetworkand
−KL(q (z|x,t)||p (z|t))
γ φ
(cid:124) (cid:123)(cid:122) (cid:125) true model posterior towards each other (i.e. the
KL-term
ELBOobjectivepressuresthemtoagree). Thus,if
Weshallrefertothefirsttermasthereconstruction theinferencenetisconstrained,butthedecoderis
termandthesecondtermastheKL-term. not,learningwilltrytofindaweaklyconstrained
Wemakeamean-fieldassumptionintheposte- decodertomatchtheapproximateposterior.
rior approximation on z as follows: q(z|x,t) =
(cid:81)K
q(z |x ,t). Note that p(z|t) is auto-
3.2 Optimization
i=1 i i
regressive,andthusitisintractabletoexactlycom- Reconstruction term: As mentioned earlier,
putetheKLterm. WeresorttoMonteCarlosam- we draw samples from the inference network to
pling to approximate the ELBO by drawing sam- approximatethereconstructionterm. Thedecoder
plesfrominferencenetwork; thoughwewillper- parameters θ can be trained directly through
formthisdifferentlyfortheKLtermandtherecon- back-propagation to minimize the approximate
structionterm(moredetailsinSection3.2). reconstruction loss. However, since z is discrete,
we use the REINFORCE (Williams, 1992)
3.1 InferenceNetworkandPosteriorSparsity algorithmtotraintheparametersγ oftheinference
Constrained Inference Network With the con- networkq(z|x,t). Followingpriorwork(Xuetal.,
strained decoder discussed earlier, the true pos- 2015), we use an entropy regularizer term and a
teriorissparse–somakingtheinferencenetalso moving average baseline to reduce the variance
sparsewouldhelpthelearningprocedurebetterap- of the resulting gradient estimator for inference
proximatethetrueposterior(Figure4). Toleverage networkparametersγ.
this observation, we constrain the inference net-
work’soutputdistributiontohavenon-zeroprob- KLterm: NotethattheKLtermcanbesimplified
abilities only on the tokens present in the corre- asfollows:
spondingsentence:
KL(q (z)||p (z)) = KL(q (z )||p (z ))+
γ φ γ 1 φ 1
q(z = v|x ,t) = 0ifv (cid:54)∈ x E [KL(q (z )||p (z |z ))+
i i i z1∼qγ(z1) γ 2 φ 2 1
∝ exp(s v)otherwise E z2∼qγ(z2)[KL(q γ(z 3)||p φ(z 3|z <3)]+...]]]
Here, s is the logit output for the token v pro- Wedrawsamplesofz fromq(z)toapproximate
v
ducedbytheinferencenetwork. Ourconstrained theKLterm.
inference network is a BiLSTM model which
generates an encoding h for jth token in a story KLtermfortheconstrainedinferencenetwork:
j
sentence. A linear layer transforms h to a score Fortheconstrainedinferencenetwork,wehavea
j
3640
sparse approximate posterior. Given the fact that Yao et al. (2019). Yao et al. (2019) had chosen a
typicalsentencesinourdatasetare5-20wordsin subsetoftheoriginalROCcorpusinordertoselect
length,itiscomputationallyeasytoexactlycom- onlythosestorieswhichareaccompaniedbyatitle.
puteindividualKL(q(z )||p(z |z ))termsbysum- Thetrain,validationandtestsplitsconsistof78529,
i i <i
ming over the tokens in x instead of the whole 9816, and 9816 stories respectively. Most of the
i
vocabulary. This is still an approximation to the dataconsistoffivesentencestories. Additionally,
fullKLtermsincewecannotfeasiblysumoverthe weexperimentwiththevisualstorydataset(only
context. thetextportion),whichwediscussinmoredetail
inSection4.8.
(cid:88)
KL(q(z )||p(z |z ) = q(z )logq(z )/p(z )
i i <i i i i
4.2 Methods
zi∈V
(cid:88)
= q(z )logq(z )/p(z ) NOPLAN-LM: This baseline does not consider
i i i
anystorygenerationplanandconditionsonlyon
zi∈x
i
thetitle. Weusethesame3-layerLSTMasinthe
Thus, for the constrained inference network,
proposedmodel.
KL computation now proceeds as follows: we
first compute KL(q(z )||p(z )) as described
1 1 SUPERVPLAN: Thisbaselineisbasedonthework
above. Thenwesamplez1 ∼ q(z ),andcompute
1 of(Yaoetal.,2019)whichutilizesRAKE-tagged
KL(q(z )||p(z |z )),andsoon–westillneedto
2 2 <1 keywords as observed anchor words. The model
usesamples,butcanexactlycomputeeachofthe
is trained to predict the the observed anchor
K individualKLterms,oneateachoftheK steps
words and the story given the title. We can view
intheplan,similartotheapproachof(Yangetal.,
this baseline as a latent variable model that was
2018). Weobservethattheconstrainedinference
trained using RAKE keywords as the output of a
network leads to lower variance in the KL term
deterministicinferencenetwork.
approximation, thereby leading to more stable
gradients.
LAP: (1) We will refer to our model with
the constrained inference network and uncon-
Pretraining: Pretraining the inference network
strained decoder as LAP-CINF-UDEC. (2)
in an autoencoder setup has been found useful
LAP-UINF-UDEC uses the unconstrained in-
for VAE training (Li et al., 2019). We pretrain
ference network and unconstrained decoder. (3)
the inference network in an autoencoder setup
LAP-CINF-CDEC usestheconstrainedinference
wherethedecoderreconstructsthecorresponding
network with the constrained decoder. We found
sentences(ratherthanwholestory). Thereafter,we
that the model with constrained decoder and
train the decoder and prior keeping the inference
unconstrained encoder performed poorly during
networkfixed. Finallyweperformthefulltraining
training,andsowedonotincludeitinexperiments.
with all parameters being updated. We observe
that pretraining through this procedure leads to
Decodingprocedure: Forallthemethods,wegen-
morestabletraining.
eratesampleswithtop-psampling(Holtzmanetal.,
2020)withp = 0.6atthetimeofstorygeneration.
Unless otherwise stated, the same decoding pro-
4 Experiments
cedureisfollowedfortheevaluationsofdiversity,
Weevaluateandreportgenerationqualityofvari- storyquality,andcontrollablegenerationdiscussed
ousmodelsusingautomaticmetricsforfluencyand below. Laterintheanalysiswediscusstheeffectof
diversity,aswellashumanevaluationsforcoher- changingtheparameterponsomeoftheevaluation
enceofstoryandrelevancetotitle. Wealsoanalyze metrics.
theabilityofanchorwordstocontrolthegenerated
4.3 Perplexity
story,andhighlightcomparisonsbetweenvarious
choicesofinferencenetworksanddecoders. For the models with latent generation plans, we
useimportanceweighting(IW)(Burdaetal.,2016)
4.1 Dataset
(with 20 samples) to estimate perplexity scores
WeuseasubsetoftheROC-storiescorpus(ROC- since (IW) has been shown to provide a tighter
DATA)(Mostafazadehetal.,2016)usedearlierby boundthanELBOforevaluationpurposes(Lietal.,
3641
Method InferenceN/W Decoder PPL↓ NLL↓ DIV↑ DIV-B↓
test test dev plan story story
NoPlan
ROC-DATA NA NA NA NA NA NA 9.01 0.23
NOPLAN-LM NA Unconstrained 17.3 154.0 160.7 NA 7.70 0.50
WithPlan
SUPERVPLAN NA1 Unconstrained ≤28.3 ≤180.3 ≤187.6 8.71 7.74 0.49
LAP-CINF-UDEC Constrained Unconstrained ≤21.3 ≤168.9 ≤176.5 9.24 7.93 0.45
LAPothervariants:
LAP-CINF-CDEC Constrained Constrained ≤20.9 ≤166.9 ≤174.1 9.24 7.98 0.44
LAP-UINF-UDEC Unconstrained Unconstrained ≤17.5 ≤154.2 ≤160.9 0.01 7.67 0.52
Table 1: Automated metrics: We report Negative Log Likelihood (NLL), perplexity (PPL) (computed using im-
portanceweightedsamplesformodelswithlatentvariables),anddiversity(DIVandDIV-B). LAP-CINF-UDEC
performs better than SUPERVPLAN on perplexity as well as diversity. We also experiment with two other vari-
antsforLAP.LAP-UINF-UDEC,whichdoesnotconstraintheinferencenetwork,suffersfromposteriorcollapse.
LAP-CINF-CDEC,whichusestheconstraineddecoder,achievesperplexityanddiversityresultsthatarecompara-
bletoLAP-CINF-UDEC.
2019). For the baseline, SUPERVPLAN, we also tropy from the generated set of stories (Jhamtani
evaluate its marginal likelihood for comparison et al., 2018). For methods which use generation
with our model. To do this, we separately train plans,wealsocomputethisdiversitymetriconan-
aninferencenetwork(withthesamearchitecture chorwordsequences. Table1showstheresultsfor
as that used by the LAP-CINF-UDEC model) to variousmodels. LAP-CINF-UDEC performsbet-
approximate the posterior on anchor words for terthan SUPERVPLAN,achievinghigherdiversity
thetrainedSUPERVPLAN(bykeepingthetrained for both story and plans. Among the LAP vari-
model parameters fixed). This approximate pos- ants,usingthenon-constrainedinferencenetwork
terioristhenusedtocomputeanupperboundon (LAP-UINF-UDEC)leadstoworseresultsonstory
NLLandperplexity. diversity,andfarespoorlyinplandiversity(dueto
The proposed model LAP-CINF-UDEC per- posterior collapse). LAP-CINF-CDEC again per-
forms better than the baseline SUPERVPLAN, formssimilarlyto LAP-CINF-UDEC.
whichusesseparatelytaggedgenerationplans(Ta-
DIV-B We also report inter-story BLEU4 scores
ble1). However,theproposedmethod’sperplexity
(Zhuetal.,2018). Wecomputesamplesfromvar-
is close to that of NOPLAN-LM, which does not
ious methods for 1000 titles in the test split. For
consideranygenerationplan. Thisisnotuncom-
eachgeneratedstory,theremaining999aretreated
monfordeeplatentvariablemodels–sincetheir
asreferences. Thus,lowervaluesindicatehigher
held-outlikelihoodisintractable,andmostapprox-
diversity in the generated stories. Table 1 shows
imations yield upper bounds on perplexity, their
theresults. LAP-CINF-UDECperformsbetterthan
reportedperplexityisalwayspessimistic. Among
SUPERVPLAN,thoughisstillfarfromthevalues
LAPvariants,weobservethatLAP-UINF-UDEC
forhumanwrittenstoriesintheROCdatasetitself.
suffersfromposteriorcollapses,andbehavessimi-
larlytoNOPLAN-LMsincethelatentvariablesz
4.5 HumanEvaluations
arenotinformativeoruseful. Finally, LAP-CINF-
CDEC performssimilaronlikelihoodevaluations
We conduct human evaluations on Amazon Me-
comparedtotheLAP-CINF-UDECmodelwithan
chanicalTurktoevaluatethequalityofgenerated
unconstraineddecoder.
storiesgiventhetitle. Weevaluatethestorysam-
ples with respect to: (1) coherence, which mea-
4.4 Diversity
sures the logical and coherent narrative flow in a
Wegeneratestorysamplesforallthetitlesinthe
story,and(2)fidelitytotitle,whichmeasuresthe
testsplit. Weemploytwoevaluationstoreportdi-
degree to which the story is relevant to the given
versityinthegeneratedoutputs:
title. Given two stories from two different meth-
DIV We compute the geometric mean of empiri- ods,werequesthumanannotatorstoprovidetheir
cal unigram, bigram, and trigram distribution en- preference(ormarkastie).
3642
LAP-CINF-UDEC Coherence Title-Fidelity Method CTRL
vsMethodM win-tie-loss win-tie-loss SUPERVPLAN 38.8%
LAP-CINF-UDEC 72.9%
M=SUPERVPLAN 0.310.370.32 0.390.270.34
M=NOPLAN-LM 0.360.350.29† 0.330.370.30 LAPvariants:
M=ROC-DATA 0.120.080.80† 0.080.150.77† LAP-CINF-CDEC 100.0%
LAP-UINF-UDEC 0.0%
Table 2: Humanpreferenceevaluationswhenpittingvari-
ousmethodsagainstLAP-CINF-UDEC(i.e. preferencefor Table 3: We evaluate models for the extent to which the
LAP-CINF-UDECisreportedunderwin).ComparedtoSU- storyfollowsthegenerationplanbyevaluatingthefraction
PERVPLAN, LAP-CINF-UDEC performs better on fidelity of anchor words used in corresponding sentences (CTRL).
totitleandsimilaroncoherence. Lossvswinjudgements LAP-CINF-UDECdemonstratesbettercontrolcomparedto
markedwith†arestatisticallysignificantunderbootstraptest SUPERVPLAN.ModelwithLAP-UINF-UDECinferencenet-
(p<0.05)considering1000subsetseachofsize400. workcollapses,whileLAP-CINF-CDECdemonstratesperfect
controlduetothenatureofthedecoder.
Inordertoensurethequalityofhumanevalua-
offidelitytotitle,perhapsbecausetheELBOob-
tions,werestricttheannotationtasktoannotators
jective encourages the inference network to pick
fromAnglophonecountries,andlimitedtowork-
anchorwordswhichcanbemoreeasilypredicted
erswithmorethan90%HIT(HumanIntelligence
fromthetitlebythepriormodel,leadingtobetter
Task) acceptance rates. We randomize the order
titlefidelity. Weshowexamplegeneratedsamples
ofpresentedstoriestoavoidpositionalbiaseffects.
from LAP-CINF-UDEC in Table 4. More exam-
Additionally, we added two ‘check’ data points
ples and qualitative analysis can be found in the
with each HIT. More specifically, to construct a
Appendix.
‘check’, we pick a random story from the devel-
We found LAP-CINF-CDEC outputs to be
opment set, and then prepare a ‘decoy’ story by
slightlyworsethan LAP-CINF-UDECand SUPER-
replacing three lines of the story with that of an-
VPLAN outputsoncoherency. Comparedto LAP-
otherrandomlychosenstory. TheHITswherean-
CINF-UDEC, the constrained decoder achieves
notatorsmarkedthe‘decoy’asthepreferredstory
slightly better scores for perplexity and diversity
relativetotheunalteredstorywithrespecttoeither
(Table1)andcontrol(nextsubsection),butsuffers
coherence or fidelity for either of the two check
onoverallcoherency. Thisbehaviorislikelydueto
data points are skipped. These skipped HITs are
thereducedflexibilityofthemodelarchitecture(an
thenre-annotated.
exampleoutputisprovidedinTable5). Incontrast,
Based on the automated metrics and manual
thenon-constraineddecoderachievesafavorable
qualitativeinspection,wepick LAP-CINF-UDEC
balancebetweencontrolandcoherency. Thishigh-
asthebestconfigurationamongallthevariantsof
lights an interesting balance between the genera-
our model for human evaluation. We randomly
tionplanandthedegreetowhichthedecodermust
selected200titlesfromthetestsplit,generatesam-
followtheplan.
plesfromallthemethodsunderconsideration,and
evaluate each method against LAP-CINF-UDEC.
4.6 ControllableGeneration
Eachcomparisonisratedbythreedifferentannota-
We evaluate models for the extent to which the
torsleadingtoatotalof600judgementsperpair.
storyfollowsthegenerationplan. Toevaluatethis,
Table2showstheresults. Weobservethatonaver-
wedrawonestorysamplepertitleinthetestsplit,
age,annotatorsfound LAP-CINF-UDEC outputs
andreportthefractionofanchorwordswhichwere
similar or better on coherence and fidelity com-
usedincorrespondingsentences(CTRL). LAP-
paredtothebaselines. LAP-CINF-UDECisjudged
CINF-UDEC(73%)faresmuchbetterthanSUPER-
better than NOPLAN-LM on coherence, perhaps
VPLAN (39%) (Table 3). We note that in some
becausehavingaplanprovidesaroughsketchof
outputsfromLAP-CINF-UDEC,eventhoughthe
thestoryleadingtomorecoherentoutputs. Com-
exact anchor word was not used, we observe se-
paredtoSUPERVPLAN,outputsfromtheproposed
mantically equivalent concepts being used – for
method LAP-CINF-UDEC are judged similar in
example, for the sampled anchor word ‘dismay’,
quality in terms of coherence but better in terms
the generated story sentence was: ‘She then real-
1Weretrofitaninferencenetworktoatrained SUPERV- izedshewasn’tabletoattemptit’.
PLANtoapproximatePPLandNLLforevaluationpurposes
WealsoanalyzeCTRLandDIV-Bvalueswhen
only.TrainingtheSUPERVPLANmodeldoesnotinvolveany
inferencenetwork. sampling with different values of parameter p in
3643
TITLE: theexam
ANCHORWORDS: midtermknewnervousperformedpassed
STORY: Ihadabiggeometryexamtoday.Iknewthatiwouldhavetodoit.Iwasnervous.Ihadnotperformed
sinceiwasalittlegirl.Ipassedout.
TITLE: thenewbed
ANCHORWORDS: alexnewstoreamazingglad
STORY: Alexwastryingtofindanewbed.Sheneededanewone.Shewenttothestoretogetone.Shefounda
amazingone.Shewasgladsheboughtit.
TITLE: picnic
ANCHORWORDS: goesfancyleasteatingleave
STORY: Lastweekivisitedmyfriendstothepark.Itwasatthefancypark.Theyhadtoeatthefoodandwater.I
hadagreattimeeating.Ihadtoleave.
Table4: GeneratedsamplesfromtheproposedmethodLAP-CINF-UDEC.Weobservethatsamplesfromtheproposedmethod
demonstratefidelitytothetitle,betterfollowthesampledplanofanchorwordsequences,andareinaggregatemorecoherent
thanbaselineswhichdonotconsideragenerationplan.
TITLE: theexam
LAP-CINF- ANCHORWORDS: failingnervoustriedtestshocked
CDEC STORY: Jessicawasfailinghermathclass.Shewasnervoustotrytotakethetest.Shetried
tohelp.Shetookthetest.Shewasshockedandconfident
TITLE: thenewbed
LAP-UINF- ANCHORWORDS: formsformsformsformsforms
UDEC STORY: Janewasabouttogetanewbed.Shehadbeentryingtocatchafewnewsheets.She
decidedtogetanewbed.Shelookedatthenewsheets.Itwastherightchoice.
Table5: GeneratedsamplesfromLAP-CINF-CDECandLAP-UINF-UDECvariantsoftheproposedmodelclass.Weobserve
thatwhenusingtheconstraineddecodervariant,storyoutputslackcoherencemoreoftenthanwhenusingtheunconstrained
decoder,thoughtheydemonstratebettercontrolbydesign. TheLAP-UINF-UDECvariantsuffersfromposteriorcollapse,
leadingtoagenericanchorwordsequence,andoftenproducesstoriesthatlackoverallstructure.
p LAP-CINF-UDEC LAP-CINF-CDEC SUPERVPLAN whichindicatesthatthelearnedposteriorsforthe
CTRL DIV-B CTRL DIV-B CTRL DIV-B BiLSTMnetworkareinfactconsideringwordsin
0.5 80% 0.48 100% 0.48 43% 0.54 context rather than just identifying topical words
0.6 73% 0.45 100% 0.44 39% 0.48
inthevocabulary.
0.7 67% 0.41 100% 0.40 34% 0.43
0.8 59% 0.35 100% 0.34 29% 0.38 On analyzing the argmax outputs from the
inferencenetworkofthetrainedLAP-CINF-UDEC
Table 6: Using higher p in top-p sampling leads to model, we find that 42% of the predicted anchor
lowercontrolofstoryviaplanandhigherdiversity.
wordsarenouns,39%ofthemareverbs,and11%
are adjectives, compared to 58%, 33% and 6%
respectively for the RAKE extracted keywords
top-p sampling. As we increase p, we observe
for the ROC data. Thus, the inference network
higherdiversityinsamples,alongwithlowerscores
forCTRLforLAP-CINF-UDECaswellasSUPER-
learned along-with the LAP-CINF-UDEC model
has higher preference for verbs and adjectives
VPLAN(Table6). Thisfurthershowsaninteresting
comparedtotheRAKEalgorithm.
trade-offbetweencontrolanddiversity.
4.7 InferenceNetwork
4.8 VisualStorytellingDataset
Thelatentplanmodelwithnoconstraintonthein-
ferencenetwork,LAP-UINF-UDEC,suffersfrom Wealsoconductexperimentswiththetextportion
severe mode collapse and essentially ignores the ofavisualstorydataset(Huangetal.,2016). The
plan. Thisdemonstratesthatconstrainingtheinfer- datasetconsistsof40155,4990,and5055stories
encenetworkwasusefulinmitigatingtheposterior intrain,dev,andtestsplits. ComparedtotheROC
collapse issue. In preliminary experiments, we data,therearenotitlesassociatedwithstories,and
alsoobservedthatusingabag-of-wordsinference welearnunconditionalanchorwordsequencep(z).
network instead of the BiLSTM leads to worse WetrainthebestmodelconfigurationLAP-CINF-
performance on perplexity, diversity and control, UDEC (with constrained inference network and
3644
Model PPL↓ DIV↑ inference network and a constrained decoder for
dev test plan story storygeneration. Pointernetworks(Vinyalsetal.,
NoPlan 2015) have been used for amortized inference in
VIZSTORYDATA NA NA NA 8.9
prior work on summarization (Miao and Blun-
NOPLAN-LM 38.5 40.0 NA 6.3
som,2016), thoughinasemi-supervisedcontext.
WithPlan
Non-monotonicsequencegenerationhasbeenex-
SUPERVPLAN ≤41.5 ≤42.2 6.5 6.5
LAP-CINF-UDEC ≤39.9 ≤40.8 8.0 6.6 ploredinpastfortaskssuchasmachinetranslation
(Wellecketal.,2019).
Table7: Experimentswithasecondstorydataset. We
Intheproposedmodel,thegenerationplancan
experiment with the text portion of the Visual Story
beusedtocontrolthestoryviatheanchorwords.
Dataset. Weobservethat LAP-CINF-UDEC isableto
Hard and soft constraints for incorporating key-
performbetterthanSUPERVPLANonperplexityanddi-
wordsintogenerationhavebeenexploredinKid-
versity.
donetal.(2016);Miaoetal.(2019). Controllable
textgenerationhasbeenexploredinothertasksas
unconstraineddecoder). Totrainthebaseline SU- well,suchassummarization(Fanetal.,2018),para-
PERVPLAN,weruntheRAKEalgorithmtotagthe phrasing(GoyalandDurrett,2020),styletransfer
datawiththeanchorwords. Weobservethat LAP- (Keskar et al., 2019), and data-to-text generation
CINF-UDECperformsbetterintermsofdiversity (Shenetal.,2019).
ofgeneratedstoriesandplans,aswellasperplex-
6 Conclusion
ity relative to SUPERVPLAN (Table 7). Diversity
computations are performed with 200 generated
Inthisworkwehaveproposedadeeplatentvari-
samples. Weprovidefurtherexamplegenerations
able model which induces a discrete sequence of
fromvariousmethodsintheAppendix.
anchor words as a high-level plan to guide story
generation.2 We train the models though varia-
5 RelatedWork
tional learning using a constrained inference net-
work,andcompareconstrainedandunconstrained
Priorworkonstorygenerationhaslargelyfocused
versionsofthedecoder. Theproposedmodelper-
onplotoutlineviakeywordsorkeyphrases(Yao
forms similarly or better than baselines on vari-
etal.,2019;Xuetal.,2018),event-basedrepresen-
ousautomatedandhumanevaluations. Relatedap-
tations(Martinetal.,2018;Fanetal.,2019),ora
proachesmightbeusedmorebroadlyforavariety
sentencetheme(Chenetal.,2019). Liuetal.(2020)
of language generation tasks, or even for related
proposeamethodtogenerateastoryconditioned
domains like music generation. Other modeling
onacharacterdescription. Priorworkonnarrative
extensionsmightexplorericherstructureinlatent
textgenerationwithplanshasmostlyreliedonex-
plans–forexample,generalizingbeyondisolated
ternalresourcesortoolstoextractoutlines(Zhou
words. Finally, in this work we trained decoders
etal.,2018;Fanetal.,2019),andthentrainingin
fromscratch,thoughitwouldbeinterestingtoex-
asupervisedmanner. Forexample,usingVADER
ploretheincorporationoflargepretrainedmodels
(HuttoandGilbert,2014)totagsentimentpolarity
such as GPT2 (Radford et al., 2018) to increase
(Luoetal.).
fluency.
Muchpriorworkhasusedmanuallydefinedob-
jectivestoencouragecoherenceingeneratedtext.
Acknowledgements
In this context, reinforcement learning has been
usedtoencouragestoriestofollowcertainmanu- WethankNikitaDuseja,AashiJain,PrakharGupta,
ally defined goals such as being locally coherent Bodhisattwa Majumder, and the anonymous con-
(Tambwekar et al., 2018; Xu et al., 2018). Prior ferencereviewersforprovidingvaluablefeedback.
workonvisualstorygenerationaimtolearntopi- This project is funded in part by the NSF under
callycoherentvisualstorygeneration(Huangetal., grants 1618044 and 1936155, and by the NEH
2019;Wangetal.,2019). Comparedtotopics,key- under grant HAA256044-17. The first author is
wordsprovidemorefine-grainedplan,andthusare supported in part by an Adobe Research Fellow-
more likely to provide fine-grained control over ship. Findingsandobservationsdonotnecessarily
generatedoutputs.
2https://github.com/harsh19/
In this work we have proposed a constrained Latent-Anchor-Plan
3645
reflecttheviewsoffundingagencies. NitishShirishKeskar,BryanMcCann,LavRVarshney,
CaimingXiong,andRichardSocher.2019. Ctrl: A
conditionaltransformerlanguagemodelforcontrol-
lablegeneration. arXivpreprintarXiv:1909.05858.
References
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdi- Chloe´ Kiddon, Luke Zettlemoyer, and Yejin Choi.
nov.2016. Importanceweightedautoencoders. 2016. Globallycoherenttextgenerationwithneural
checklist models. In Proceedings of the 2016 Con-
Gang Chen, Yang Liu, Huanbo Luan, Meng Zhang, ferenceonEmpiricalMethodsinNaturalLanguage
Qun Liu, and Maosong Sun. 2019. Learning to Processing,EMNLP.
predictexplainableplotsforneuralstorygeneration.
arXivpreprintarXiv:1912.02395. Diederik P. Kingma and Max Welling. 2014. Auto-
encodingvariationalbayes.
Angela Fan, David Grangier, and Michael Auli. 2018.
Controllableabstractivesummarization. InProceed- Bohan Li, Junxian He, Graham Neubig, Taylor Berg-
ingsofthe2ndWorkshoponNeuralMachineTrans- Kirkpatrick,andYimingYang.2019. Asurprisingly
lationandGeneration,NMT@ACL2018,. effectivefixfordeeplatentvariablemodelingoftext.
InEMNLP-IJCNLP.
Angela Fan, Mike Lewis, and Yann Dauphin. 2019.
Strategiesforstructuringstorygeneration. InACL. BoyangLi,StephenLee-Urban,GeorgeJohnston,and
Mark Riedl. 2013. Story generation with crowd-
Tanya Goyal and Greg Durrett. 2020. Neural syntac- sourced plot graphs. In Proceedings of the Twenty-
ticpreorderingforcontrolledparaphrasegeneration. Seventh AAAI Conference on Artificial Intelligence,
In Annual Meeting of the Association for Computa- July14-18,2013,Bellevue,Washington,USA.AAAI
tionalLinguistics. Press.
AriHoltzman, JanBuys, LiDu, MaxwellForbes, and Danyang Liu, Juntao Li, Meng-Hsuan Yu, Ziming
Yejin Choi. 2020. The curious case of neural text Huang,GongshenLiu,DongyanZhao,andRuiYan.
degeneration. In 8th International Conference on 2020. A character-centric neural model for auto-
LearningRepresentations,ICLR2020,AddisAbaba, matedstorygeneration. InTheThirty-FourthAAAI
Ethiopia,April26-30,2020.OpenReview.net. Conference on Artificial Intelligence, AAAI 2020.
AAAIPress.
Chenglong Hou, Chensong Zhou, Kun Zhou, Jinan
Sun, and Sisi Xuanyuanj. 2019. A survey of deep Fuli Luo, Damai Dai, Pengcheng Yang, Tianyu Liu,
learning applied to story generation. In Interna- BaobaoChang,ZhifangSui,andXuSun. Learning
tionalConferenceonSmartComputingandCommu- to control the fine-grained sentiment for story end-
nication.Springer. ing generation. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
Qiuyuan Huang, Zhe Gan, Asli Celikyilmaz, Dapeng guistics.
Wu, JianfengWang, andXiaodongHe.2019. Hier-
archicallystructuredreinforcementlearningfortop- Lara J Martin, Prithviraj Ammanabrolu, Xinyu Wang,
icallycoherentvisualstorygeneration. InProceed- WilliamHancock,ShrutiSingh,BrentHarrison,and
ings of the AAAI Conference on Artificial Intelli- Mark O Riedl. 2018. Event representations for au-
gence,volume33. tomated story generation with deep neural nets. In
AAAI.
Ting-Hao (Kenneth) Huang, Francis Ferraro, Nasrin
Mostafazadeh,IshanMisra,AishwaryaAgrawal,Ja- Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei
cob Devlin, Ross B. Girshick, Xiaodong He, Push- Li. 2019. Cgmh: Constrained sentence generation
meetKohli,DhruvBatra,C.LawrenceZitnick,Devi bymetropolis-hastingssampling. InProceedingsof
Parikh,LucyVanderwende,MichelGalley,andMar- the AAAI Conference on Artificial Intelligence, vol-
garetMitchell.2016. Visualstorytelling. InNAACL ume33.
2016.
Yishu Miao and Phil Blunsom. 2016. Language as a
Clayton J. Hutto and Eric Gilbert. 2014. VADER: A latent variable: Discrete generative models for sen-
parsimoniousrule-basedmodelforsentimentanaly- tencecompression. InProceedingsofthe2016Con-
sisofsocialmediatext. InProceedingsoftheEighth ferenceonEmpiricalMethodsinNaturalLanguage
InternationalConferenceonWeblogsandSocialMe- Processing.
dia,ICWSM2014.
NasrinMostafazadeh,NathanaelChambers,Xiaodong
Harsh Jhamtani, Varun Gangal, Eduard Hovy, Gra- He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
ham Neubig, and Taylor Berg-Kirkpatrick. 2018. Pushmeet Kohli, and James Allen. 2016. A cor-
Learning to generate move-by-move commentary pus and evaluation framework for deeper under-
for chess games from large-scale social forum data. standing of commonsense stories. arXiv preprint
InACL2018. arXiv:1604.01696.
3646
TomPelsmaekerandWilkerAziz.2020. Effectiveesti- Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
mationofdeepgenerativelanguagemodels. InPro- AaronCourville,RuslanSalakhudinov,RichZemel,
ceedingsofthe58thAnnualMeetingoftheAssocia- and Yoshua Bengio. 2015. Show, attend and tell:
tionforComputationalLinguistics,Online.Associa- Neural image caption generation with visual atten-
tionforComputationalLinguistics. tion. InInternationalconferenceonmachinelearn-
ing.
Julie Porteous and Marc Cavazza. 2009. Controlling
narrativegenerationwithplanningtrajectories: The ZichaoYang,ZhitingHu,ChrisDyer,EricPXing,and
role of constraints. In Interactive Storytelling, Sec- Taylor Berg-Kirkpatrick. 2018. Unsupervised text
ond Joint International Conference on Interactive style transfer using language models as discrimina-
Digital Storytelling, ICIDS, Lecture Notes in Com- tors. InAdvancesinNeuralInformationProcessing
puterScience. Systems.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin
DarioAmodei,andIlyaSutskever.2018. Language Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-
modelsareunsupervisedmultitasklearners. and-write: Towardsbetterautomaticstorytelling. In
ProceedingsoftheAAAIConferenceonArtificialIn-
MarkO.RiedlandRobertMichaelYoung.2010. Nar- telligence.
rativeplanning: Balancingplotandcharacter. J.Ar-
tif.Intell.Res.,39. DeyuZhou,LinsenGuo,andYulanHe.2018. Neural
storyline extraction model for storyline generation
Stuart Rose, Dave Engel, Nick Cramer, and Wendy fromnewsarticles. InNAACL-HLT.
Cowley.2010. Automatickeywordextractionfrom
individual documents. Text mining: applications Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,
andtheory,1. WeinanZhang,JunWang,andYongYu.2018. Texy-
gen: A benchmarking platform for text generation
Xiaoyu Shen, Jun Suzuki, Kentaro Inui, Hui Su, Di- models. CoRR,abs/1802.01886.
etrich Klakow, and Satoshi Sekine. 2019. Select
and attend: Towards controllable content selection
intextgeneration. InEMNLP-IJCNLP.
Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J
Martin, Animesh Mehta, Brent Harrison, and
Mark O Riedl. 2018. Controllable neural story
plot generation via reinforcement learning. arXiv
preprintarXiv:1809.10736.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in neural in-
formationprocessingsystems.
Ruize Wang, Zhongyu Wei, Piji Li, Haijun Shan,
Ji Zhang, Qi Zhang, and Xuanjing Huang. 2019.
Keepitconsistent: Topic-awarestorytellingfroman
image stream via iterative multi-agent communica-
tion. arXivpreprintarXiv:1911.04192.
Sean Welleck, Kiante´ Brantley, Hal Daume´ III, and
Kyunghyun Cho. 2019. Non-monotonic sequential
text generation. In Proceedings of the 36th Inter-
national Conference on Machine Learning, ICML
2019.
Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and
SteveYoung.2017. Latentintentiondialoguemod-
els. InProceedingsofthe34thInternationalConfer-
enceonMachineLearning-Volume70.JMLR.org.
Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
mentlearning. Machinelearning,8(3-4).
Jingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xi-
aoyan Cai, and Xu Sun. 2018. A skeleton-based
modelforpromotingcoherenceamongsentencesin
narrativestorygeneration. InEMNLP.
3647
APPENDIX
A.AdditionalImplementationDetails
AdditionalTrainingDetails: Wefoundituseful
to add certain regularizers. Following Yao
et al. (2019), we add a temporal L2 penalty on
successivehiddenstaterepresentationsofLSTM.
Additionally, we block stopwords from being
sampled from the posterior since we are more
interested in inducing generation plans. We use
NLTK’s English stop-words list for this purpose.
Duringmodeltraining(afterpretraininginference
network),wealsouseKLthresholding/free-bits
(Pelsmaeker and Aziz, 2020) which thresholds
each component of the KL term to help prevent
posteriorcollapse.
Hyperparameters We perform model selection
based on best dev split performance as per NLL.
(In case of latent variable models, we use the up-
perboundonNLL).Thefinalmodelandtraining
configurationforLAP-CINF-UDECisasfollows:
Figure5: Summaryofmodelarchitecture.
batchsizeof20,temporalregularizationweightof
1.0,smoothingfactorformovingaveragebaseline
for reinforce reward is 0.1, dimension of hidden by copying the corresponding anchor word, gen-
embedding is 1000, input and output token em- erating words to the left and then to the right of
beddingsaretied. Asummaryofthedecoderand it. Thus LAP-CINF-CDEC modelcommitstous-
inference network for the final configuration of ing the corresponding anchor words. In contrast
LAP-CINF-UDECmodelisshowninFigure5. LAP-CINF-UDEChasmoreflexibilityinusingthe
anchor words, and sometimes anchor words get
Datasets: We use ROC data 3 splits from (Yao droppedorgetusedinaparaphrasedmanner. e.g.
etal.,2019)4. WealsousedVisualStoryDataset5 for sample S2, the model did not use the word
‘tired’inthelaststorysentence,thoughthesecond
B.GeneratedSamplesandQualitative lastsentenceevokessimilarunderstanding. How-
Analysis ever, this also means the control of the story via
theplanissomewhatweaker. e.g. forS4,thestory
Some additional generated samples from various
does not use the anchor word ‘boiled’. Finally,
modelsareshowninTable8. Wenotethat LAP-
comparedtooutputsfrom LAP-CINF-UDEC and
CINF-UDECplansoftenexhibitsgoodcontrolover
LAP-CINF-CDEC,weobservethatSUPERVPLAN
thegeneratedstory. Forexample,samplesS3and
outputs demonstrate much weaker control of the
S4 samples in Table 8 for the same title by-and-
planonthestory,andgenerateplanslessrelevant
largefollowthegeneratedplan. Wedoobservea
tothetitle.
certaindegreeofrepetitioninsomesamplese.gin
Table 9 shows samples from various models
sampleS2,thefirstandthirdsentencesbothdiscuss
trainedon(textpartof)visualstorydata.
mowingthelawn.
.
SampleS6furtherdemonstratesthegeneration
orderforLAP-CINF-CDEC. Eachsentencebegins
3https://cs.rochester.edu/nlp/
rocstories/
4https://bitbucket.org/VioletPeng/
language-model/src/master/
5http://visionandlanguage.net/VIST/
3648
S.no. Method Title Story
1 LAP-CINF-UDEC TITLE: race
ANCHORWORDS: participatehighchallengedsurewon
STORY: Jillwantedtoparticipateinarace.Shewasinhighschoolandhadnoideahow
torace.Jillchallengedherfriendtoarace.Shewasn’tsureifshewouldwin.
Ultimatelyshewontherace.
2 LAP-CINF-UDEC TITLE: lawn
ANCHORWORDS: mowinglawnmowedeverytired
STORY: Iwasmowingmylawnoneday.Iwasonthelawnbymyself.Imowedthelawn.
Iworkedonitforhours.EventuallyIwasabletogobacktowork.
3 LAP-CINF-UDEC TITLE: thedryer
ANCHORWORDS: loadcoldwasheverythinglong
STORY: Agirlgotaloadoflaundry.Sheputeverythinginthedryer.Shewantedtowash
itoff.Shedideverythingbutthewashingmachinewasnotsogood.Shehadto
takealongtimetocleanitup.
4 LAP-CINF-UDEC TITLE: thedryer
ANCHORWORDS: cookingboileddriedputcooking
STORY: Therewasapotofbeansinthemicrowave.Thepotwaspiledonthetable.They
drieditandcookedit.Theyputthenoodlesintheoven.Thekidswereall
cookingdinner.
5 LAP-CINF-UDEC TITLE: lostphone
ANCHORWORDS: baropenedsomeoneleftfound
STORY: Timwasatthebar.Heopenedthedoortofindaphone.Itwasofaperson.Tim
leftforacoupleofminutes.Hetookithomeanditturnedoutthathelostit.
6 LAP-CINF-CDEC TITLE: race
ANCHORWORDS: enteredwinlosingtrainedwin
STORY: enteredtim<start>alocalgym.wintowantedhe<start>.losingupwound
he<start>therace.trainedtim<start>togetanextra.wintodecidedtim
<start>therace.
STORY(reordered): Timenteredalocalgym.Hewantedtowin.Hewounduplosingtherace.Tim
trainedtogetanextra.Timdecidedtowintherace.
7 LAP-UINF-UDEC TITLE: race
ANCHORWORDS: formsformsformsformsforms
STORY: Cathyandhisfriendsdecidedtogotothefair.Cathywasworriedaboutthe
showthatshewashavingtobeonthetrackteam.Cathywasworriedthatthe
racewouldhavetobeover.Cathywasnervoustowinbyapointofwinningthe
race.Cathycaughttherace.
8 SUPERVPLAN TITLE: race
ANCHORWORDS: todaygooddayracerace
STORY: Iwasthefastestrunneronmyhighschooltrackteam.Itwasveryhardtowin,
butmyfriendconvincedmetodoit.Ipracticedwiththerunnersandpracticed
everyday.Ipracticedeverydayafterschool.Icangototheraceeveryday.
9 SUPERVPLAN TITLE: lostphone
ANCHORWORDS: packagewantedfoundbatterymissed
STORY: Tomwasgettinganewphone.Hewantedtobuyanewone.Whenhefounditit
wasstillabrokenbattery.Tomwassadbecauseitwasbroken.Tomtriedtosave
itbutitwasn’tenough.
Table8: Samplesfromdifferentmethods(withminoreditssuchascaptalization)
3649
LAP-CINF-UDEC TITLE: n/a
ANCHORWORDS: reuniontogetherwaytogethermembers
STORY: Wegottogethertohaveareunion.Wewenttoasmalltown.Wehadalotoffun.We
tookmanypictures.Thenweallgottogethertotalktosomefamilies.
LAP-CINF-UDEC TITLE: n/a
ANCHORWORDS: weddingweddingpeoplefoodbeautiful
STORY: Iwenttotheweddinglastweekend.Therewereatonofpeopleatthewedding.There
werealotofpeoplethere.Thefoodwasdelicious.Thecakewasbeautiful.
NOPLAN-LM TITLE: n/a
ANCHORWORDS: n/a
STORY: Thisisthefirstdayofmyvacation.Iliketohaveatriptolocation.Ihaveneverbeento
location.Hereismyapartment.I’vebeenhopingtobethere.I’msoexcited.
SUPERVPLAN TITLE: n/a
ANCHORWORDS: bigmannonesatshowed
STORY: Thebigday.Andthemanwasplaying.Andtheflowerswerelaidout.Thentheysaton
thefloor.Theywereallveryhappy
SUPERVPLAN TITLE: n/a
ANCHORWORDS: bigweddingtablepartypeople
STORY: Itwasabigdayforawedding.Theweddingpartyallgatheredaroundthetable.The
tablesweresetandreadytobeserved.Peoplearrivedandchattedwitheachother.The
tablewasset.
Table9: Samplesfromdifferentmethodsforvisualstorydata
3650

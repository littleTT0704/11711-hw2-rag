AUTOLEX: An Automatic Framework for Linguistic Exploration
AditiChaudhary†,ZaidSheikh†,DavidRMortensen†,AntoniosAnastasopoulos‡,GrahamNeubig†
†CarnegieMellonUniversity,‡GeorgeMasonUniversity
{aschaudh,zsheikh,dmortens,gneubig}@cs.cmu.edu antonis@gmu.edu
Abstract Linguistsandresearchershaveundertakeninitia-
tivestocollectlinguisticpropertiesinamachine-
Each language has its own complex systems
readableformatacrossseverallanguages,WALS
of word, phrase, and sentence construction,
(DryerandHaspelmath,2013)beingastandingex-
theguidingprinciplesofwhichareoftensum-
marized in grammar descriptions for the con- ample. Forinstance,WALScantellusthatEnglish
sumption of linguists or language learners. objectsoccurafterverbs,orthatTurkishpronouns
However,manualcreationofsuchdescriptions havesymmetricalcase. However,becauseWALS
is a fraught process, as creating descriptions
presentsthesepropertiesacrossmanydiverselan-
whichdescribethelanguagein“itsownterms”
guages,thesepropertiesarenecessarilydefinedat
without bias or error requires both a deep un-
acoarse-grainedlevelandcannotcapturelanguage-
derstanding of the language at hand and lin-
specificnuances. WALSdoesnotinformusofany
guisticsasawhole. Weproposeanautomatic
framework AUTOLEX that aims to ease lin- exceptionstoitsgeneralrules(e.g.thecaseswhen
guists’discoveryandextractionofconcisede- Englishobjectscomebeforeverbs),andthereare
scriptions of linguistic phenomena. Specifi- manyaspectsthatarenotevencovered(e.g.when
cally, we apply this framework to extract de- aTurkishpronountakestheaccusativemarkerand
scriptions for three phenomena: morphologi-
whenthenominative). Thereareotherchallenges
calagreement,casemarking,andwordorder,
to creating detailed descriptions, as for many of
across several languages. We evaluate the de-
the6,500+languages,therearefewornoformally
scriptions with the help of language experts
trainedlinguists. Evenintheidealcasewherethere
and propose a method for automated evalua-
tionwhenhumanevaluationisinfeasible.1 issuchalinguist,thereareaplethoraoflinguistic
phenomenatobecovered,anditishardtoenumer-
1 Introduction
ateeverysingleonethroughintrospection.
Languagesareamazinglydiverse,consistingofdif- ThankstotheNLPadvances,itisnowpossible
ferentsystemsforwordformation(morphology), toautomatesomelocalaspectsoflinguisticanaly-
phraseconstruction(syntax),andmeaning(seman- sissuchasPOStagging(ToutanvoaandManning,
tics). Thesesystemsaregovernedbyasetofguid- 2000),dependencyparsing(KiperwasserandGold-
ingprinciples,referredtoasgrammar. Creatinga berg,2016)ormorphologicalanalysis(Malaviya
human-readabledescriptionthathighlightssalient etal.,2018). Recentadvancesintransferlearning
pointsofalanguageisoneofthemajorendeavors haveshownthatthisispossibletoanextent,even
undertakenbylinguists. Suchdescriptionsforman for under-resourced languages (Kondratyuk and
indispensablecomponentoflanguagedocumenta- Straka,2019). Asmallamountofpriorworkhas
tionefforts,particularlyforendangeredorthreat- proposedmethodsforansweringspecificquestions
ened languages (Himmelmann, 1998; Hale et al., aboutlanguage,suchastheanalysisofwordorder
1992;Moseley,2010). Furthermore,ifdescriptions (Östling,2015;WangandEisner,2017)andmor-
canbecreatedinamachine-readableformatthey phologicalagreement(Chaudharyetal.,2020),or
canbeusedfordevelopinglanguagetechnologies grammarextractionfrominter-linearglosses(Ben-
(Pratapaetal.,2021). deretal.,2002)(Table2inAppendixAcompares
thequestionsansweredbyourandrelatedwork).
1Code and data are released on https://github.
com/Aditi138/auto-lex-learn/tree/master/code. Cur- In this work, we propose AUTOLEX, an auto-
rently, theonlinewebsite(https://aditi138.github.io/
maticframeworktoaidlinguisticexplorationand
auto-lex-learn/index.html)showstherulesfordifferent
languages. description, withthegoalofhelpinglinguistsde-
2202
raM
52
]LC.sc[
1v10931.3022:viXra
Raw Text
Syntactic analysis from experts
1
(e.g. treebanks)
Syntactic
Analysis Automatic syntactic analysis
(e.g using multilingual adaptation
for languages with no treebanks)
2
Extract
Features Automatic
Feature Extraction
3
Construct 4 5 6
Training 1. Usually Adj come after N Evaluate/Use
2. Ordinal Adj come before N
Data learn extract
Interpretable rules
model
Figure 1: An overview of the AUTOLEX framework (with Adj-N order in Spanish as an example). The exam-
ple sentence translates to Four books were bought by the small girl. First, we formulate a linguistic question
(e.g.regardingAdj-Norder)asabinaryclassificationtask(e.g.“whethertheAdjcomesbefore/aftertheN’).Next,
weperformsyntacticanalysisontherawtext, fromwhichweextractsyntactic, lexical, andsemanticfeaturesto
constructthetrainingdata. Finally,welearnaninterpretablemodelfromwhichweextractconciserules.
velop fine-grained understanding of different lin- correct,readable,andnoveltherulesareperceived
guisticphenomena. Theframeworkallowsthelin- tobe(§7.2). Finally,weapplythisframeworkto
guisttoaskaquestionsuchas“whataretherules athreatenedlanguagevariety,HmongDaw(mww),
ofobject-verborder?”,or“whendopronounstake andevaluatehowwellourframeworkextractsrules
theaccusativecaseinTurkish?”,andautomatically underzero-resourceconditions(§8).
acquire first-pass answers. AUTOLEX analyses
thetextsinthecorrespondinglanguagesandfinds 2 FormalizingLinguisticQuestions
answerssuchasinEnglish“typicaldeclarativecon-
Thefirststepinapplying AUTOLEX toanswera
structionsshowVO,butinterrogativesentencescan
question is to determine whether we can formu-
showOV”,orinTurkish“objectpronounstakethe
late it as a classification task, with training data
accusative case.” Specifically, we follow a multi-
{(cid:104)x ,y (cid:105),(cid:104)x ,y (cid:105)··· ,(cid:104)x ,y (cid:105)}, where x ∈ X
step process, as shown in Figure 1. First, we de- 1 1 2 2 n n i
are the input features and y ∈ Y are the labels
finethelinguisticquestionasaclassificationtask i
indicatingthelinguisticphenomenon. Below,we
(e.g.“doestheadjectivecomebeforethenounor
describe how we define Y for each phenomena,
not”;§2). Second,weautomaticallyextractsyntac-
and discuss how to construct X in the following
tic, semantic, and surface-level features that may
section. WeusetheUDschema(McDonaldetal.,
be predictive of the answer to this question (§ 3).
2013)forrepresentingthesyntaxandmorphology.
Next, we construct the training data and train an
interpretable classifier such as a decision tree to Case Marking is a system of “marking syntac-
identify the underlying patterns that answer this tic dependents for the type of grammatical rela-
question. Finally, we extract and visualize inter- tion(subject,object,etc.) theybeartotheirheads”
pretablerules(§4). Thismethodologyisinspired (Blake, 2009). Although there are different theo-
bypreviousworkondiscoveringfine-graineddis- ries on how to formalize case marking, we com-
tinctionsforindividualphenomena(WangandEis- mit to the viewpoint that there are two types of
ner, 2017; Chaudhary et al., 2020), but is signifi- cases: abstractandmorphological,wherethefor-
cantlymoregeneralinthatwedemonstrateitsabil- merisauniversalpropertyandthelatterisitsovert
itytodiscoverinterestingfeaturesforwordorder, realization (Chomsky, 1993; Halle et al., 1993).
casemarking,andmorphologicalagreement. Thus,weformulatetheexplanationofcasemark-
Weexperimentwith61languagesforwhichwe ingasdeterminingwhenawordclass(e.g. nouns)
designanautomatedevaluationprotocolwhichin- marks a particular case (e.g. nominative, etc.).
formsushowsuccessfulourframeworkisindis- Formally, for each POS tag t we learn a separate
coveringvalidgrammarrules(§7.1). Wefurther model,wheretheinputexamplesx arethewords
i
conductauserstudywithlinguiststoevaluatehow havingPOStagtwiththecasefeaturemarked(e.g.
Case=Nominative). Themodelistrainedtopredict Syntactic Features Prior work (Blake, 2009;
anoutputlabel(y ∈ Y),whereY isthelabelset Kittiläetal.,2011;Corbett,2003)hasdiscussedthe
i
ofallobservedcasevaluesforthatlanguage. roleofsyntaxandmorphologybeingimportantfor
determiningthecaseandagreement. InFigure1,
Word Order describes the relative position of
weshowasubsetoffeaturesextractedforsomeof
thesyntacticelements(Dryer.,2007),andisoneof
thefocuswords. Forexample,fortheadjective,we
themajoraxesoflinguisticdescriptionappearing
derivefeaturesfromitsPOStag(e.g.“is-adj”),all
ingrammarsketchesordatabasessuchasWALS.
ofitsmorphologicaltags(e.g.“is-ordinal”)andthe
WeconsiderthefollowingfiveWALSrelationsR:
dependencyrelationitisinvolvedin(e.g.“deprel-
subject-verb (82A), object-verb (83A), adjective-
is-mod”). Weextractsimilarfeaturesfortheadjec-
noun(87A),adposition-noun(85A)andnumeral-
tive’shead,whichislibros(e.g.“head-is-noun”).
noun(89A).IncontrasttoWALS,whichonlypro-
vides a single canonical order for the entire lan- Lexical Features An influential family of lin-
guage, we pose the linguistic question as deter- guistic theories such as lexical functional gram-
mining when does one word in such a relation mar(Kaplanetal.,1981),head-drivenphrasestruc-
appear before or after the other. Formally, the turegrammar(PollardandSag,1994),placesmost
pair of words involved in the syntactic relation of the explanatory weight for morphosyntax on
(cid:104)wa,wb(cid:105) ∈ r form the input example x and the thelexicon–thepropertiesoftheheadword(and
i i i
outputlabely ∈Y whereY ={before,after}. otherwords)drivetherealizationoftherestofthe
i
phraseorsentence. Therefore,weaddthelemma
Agreement is the process where one word or
for the focus words (e.g. “dep-lemma-is-cuatro,
morphemeselectsamorphologicalformthatagrees
head-lemma-is-libro”)asfeatures.
with that of another word/phrase in the sentence
(Corbett,2003). Wefollowasimilarproblemfor- SemanticFeatures Thereisastronginteraction
mulation as Chaudhary et al. (2020), which asks betweensemanticsandsentencestructure. Some
thequestionwhenisagreementrequiredbetweena well-knownexamplesareofanimacyorsemantic
head(w )anditsdependent(w )foramorpholog- class of a word determining case marking (Dahl
h d
ical attribute m. We focus on the morphological andFraurud,1996)andwordorder(Thuilieretal.,
attributes M ={gender,person,number}, which 2021) for some languages. Continuous vectors
more often show agreement than other attributes (Mikolov et al., 2013; Bojanowski et al., 2017a)
(Corbett,2009),andtrainaseparatemodelforeach. havebeenusedtocapturesemantic(andsyntactic)
Thepairofhead-dependentwordswhichbothmark similarity across words. However, most vectors
themorphologicalpropertymformtheinputexam- arehigh-dimensionalandnoteasilyinterpretable,
plex andtheoutputlabels(y )arebinarydenoting i.e.whatsemantic/syntacticpropertyeachindivid-
i i
ifagreementisobservedornotbetweenthepair. ual vector value represents is not obvious. Since
ourprimarygoalistoextractcomprehensiblede-
3 FeatureExtraction scriptionsoflinguisticphenomena,wefirstgener-
atesparsenon-negativevectorsusingSubramanian
Nowthatwehaveprovidedthreeexamplesofcon-
et al. (2018). For each dimension, we extract the
vertinglinguisticquestionsintoclassificationtasks,
top-k words having a high positive value, result-
wedesignfeaturestohelppredicteachquestion’s
ing in features like dim-1={radio,nuclear}, dim-
answer. Weuselinguisticknowledgetodesignfea-
2={hotel,restaurante}. Thishelpsusinterpretwhat
tures,butthefeatureextractionitselfisautomatic.
property each dimension is capturing, for exam-
Foradifferentquestionorlanguage,alinguistcan
ple, dim-1 refers to words about nuclear technol-
begintheprocessbyusingtheseinitialfeaturesor
ogy,whiledim-2referstoaccommodations. Now
evendesignnewfeaturesastheydeemfit. Instep-
thatwecaninterpretwhateachfeature(dimension)
2 of Figure 1, we demonstrate example features
corresponds to, we directly add these vector as
extractedfromaSpanishsentencefortrainingthe
features. InFigure1,asemanticfeature(e.g.“dep-
adjective-nounwordordermodel. Werefertothe
word-is-like={ochenta,sesenta}”2)extractedforcu-
wordsparticipatinginaninputx asfocuswords.
i atroinformsusthattheadjectivedenotesanumeric
Theseincludethewordsdescribingtherelationit-
quantity.
self(e.g. theadjectivecuatroanditsnounlibros)
andalsotheirrespectiveheadsanddependents. 2Thistranslatesto{eight,sixty}
4 LearningandExtractingRules visualize them in an interface (Figure 2). We se-
lectsuchexamplesthatarebothshortandconsist
Training Data To construct the training data
of diverse word forms to illustrate the rule usage
Dp for each task p, we start with the raw text
train indifferentcontexts. Alongwithexampleswhich
D of the language in question and perform syn-
follow a rule, we also show examples which do
tacticanalysis,producingPOStags,lemmas,mor-
notfollowtherule,givingasofter,morenuanced
phologicalanalysisanddependencytreesforeach
viewofthedata(detailsinAppendixB). Specifi-
sentence. Usingthisanalysis,wethenidentifythe
cally,tonotoverwhelmtheuser,weonlypresent
focusword(s)andextractk features,formingthe
10examplesforeachtype.
inputexample(x = {x0,x1,··· ,xk}).
i i i i
5 AutomatedEvaluationProtocol
Model Training Given that the learned model
mustbeinterpretabletolinguistsusingthesystem,
In the next two sections, we devise protocols for
weopttousedecisiontrees(Quinlan,1986),which
evaluation of the extracted rules using both auto-
split the data into leaves, where each leaf corre-
maticmetrics(forrapidevaluationthatcanbeap-
spondstoaportionoftheinputexamplesfollowing
pliedwidelyacrosslanguages),andevaluationby
commonsyntactic/semantic/lexicalpatterns.
humanlanguageexperts(asourgold-standardeval-
RuleExtraction Eachleafinthedecisiontreeis uation). We first describe below the process of
assignedalabelbasedonthedistributionofexam- automaticevaluationperlinguisticphenomenon.
pleswithinthatleaf. Forinstance,ifaleafofthe
CaseMarking Asnotedearlier,weusetheUD
adjective-noun word order decision tree has 60%
schemeforderivingthetrainingdata. Underthis
ofexampleswithadjectivesbeforetheirnouns,the
scheme, not every word is labeled with case, re-
leaf, by default, is labeled as before. However, a
strictingourtrainingandevaluationtobeonlyon
majority-basedthresholdaloneisinsufficientasit
such labeled examples. For such words, we con-
does not account for leaves with very few exam-
sidercasetobeauniversalpropertyi.e. eachword
ples,whichmaybebasedonspuriouscorrelations
marks a particular case value and, we evaluate
or nonsensical feature divisions. Instead, we use
whetherourmodelcancorrectlypredictthatvalue.
astatisticalthresholdforleaflabeling,inspiredby
Thus,wemeasuretheaccuracyonatestexample
Chaudharyetal.(2020),performingachi-squared
(cid:104)x ,y (cid:105) ∈ Dt ,comparingthemodelsprediction
test to first determine which leaves differ signifi- i i test
yˆ withtheobservedcasevaluey . Wecompareour
cantlyfromthebasedistribution. Forthis,wefirst i i
model against a frequency-based baseline which
define the null H and test H hypotheses. For
0 1
assignsthemostfrequentcasevalueinthetraining
instance,forwordorderwedefinethataleaf:
datatoallinputexamples.
H : takeseitherbefore/afterlabel
0 Word Order Similarly, we can assume that ev-
H : takesthelabeldominantunderthatleaf
1 eryinputexamplehasawordordervalue,forex-
ample subjects will occur either before or after
WecandesignsuchH asthewordsparticipating
0
the verbs. Therefore, for an input example, we
intherelationcaneitherbebeforeoraftertheother.
considertheobservedordertobethegroundtruth
Toapplythechi-squaredtest,wecomputetheex-
and compute the accuracy by comparing it with
pectedprobabilitydistributionforH consideringa
0
the model’s prediction. We compare against a
uniformdistribution.Wethencomputethep-value
frequency-baselinewherethemostfrequentword
andleaveswhicharenotstatisticallysignificantare
ordervalueisassignedtoallinputexamples.
assignedthelabelofcannotdecide,whichinforms
Comparingthemodel’spredictionwiththeob-
auserthatthemodelwasuncertainaboutthelabel
served order is reasonable for languages which
(detailsinAppendixB). Leavesthatpassthistest
have a dominant word order. There are a consid-
arethenassignedthemajoritylabelandcorrespond
erable set of languages which have a freer order.
toarulethatwillbeshowntolinguists,wherethe
WALS labels such relations as “no dominant or-
“rule”isdescribedbythesyntactic/semantic/lexical
der” (e.g. subject-verb order for Modern Greek).
featuresonthebranchthatleadtothatleaf.
Forsuchcases,consideringaccuracyalonemight
RuleVisualization Foreachrule, weextractil- beinsufficientasthereisnogroundtruth. There-
lustrativeexamplesfromtheunderlyingcorpusand fore,wealsoreporttheentropyoverthepredicted
Figure2: AruleextractedforSpanishadjective-nounwordorder.
distribution: experttoprovideanswersregardingthelinguistic
questionsweareevaluating. Forexample,weask
(cid:88)
H wr o = − p klogp k questionssuchas“whenaresubjectsafterverbsin
k=before,after Greek”,andtheyarerequiredtoprovideabriefan-
(cid:26)
1 yˆ = k swer(e.g.“forquestionsorwhengivingemphasis
(cid:80) 1 i
(cid:104)xr i,yi(cid:105)∈D tr est 0 otherwise toasubject”). Wethendirectthemtoourinterface
p =
k |Dr | whereweshowtheextractedfeaturesandafewex-
test
amplesforeachrule,thenaskquestionsregarding
Forlanguageswithnodominantorder,themodel eachofthethreeparameters(asshowninFigure6
shouldbeuncertainaboutthepredictedorderand intheAppendix).
we expect the model’s entropy to be high. The
Regardingcorrectness,theexpertisaskedtoan-
accuracy computed against the observed order is
notatewhethertheillustrativeexamples,shownfor
still useful, as despite there being “no dominant
thatrule,aregovernedbysomeunderlyinggram-
order”,speakerstendtopreferoneorderoverthe
mar rule. If so, they are then required to judge
other. Ahighaccuracywouldentailthatthemodel
how precise it is. Consider some rules extracted
wassuccessfulincapturingthis“preferredorder.”
forSpanishadjective-nounorderinTable1. Look-
ing at the examples and features for the Type-1
Agreement We use the automated rule metric
rule, it is evident that this rule precisely defines
(ARM)proposedbyChaudharyetal.(2020)which
the linguistic distinction.3 Some rules, although
computesaccuracybycomparingthegroundtruth
valid,maybetoogeneral(Type-3)ortoospecific
labeltothepredictedlabel. Thegroundtruthlabel
(Type-4). Finally,arulemaynotcorrespondtoany
ofanexampleisdecidedusingapredefinedthresh-
underlying grammar rule, like the Type-5 where
oldontheleaftowhichtheexamplebelongs. ARM
themodelsimplydiscoveredaspuriouscorrelation
doesnotusetheobservedagreementbetweenthe
in the data. For prior knowledge, if an extracted
headanditsdependentasgroundtruthbecausean
rulewasindeedavalidgrammarrule,thenweask
observed agreement might not necessarily mean
theexpertwhethertheywereawareofsucharule.
required agreement. WecomparewithChaudhary
Thiswillinformushowusefulourframeworkisin
etal.(2020),whichusessimplesyntacticfeatures
discoveringruleswhicha)alignwiththeexpert’s
such as POS of the head, the dependent and, the
priorknowledgeand,b)arenoveli.e. ruleswhich
dependencyrelationbetweenthem.
the expert were not aware of apriori. Finally, for
6 HumanExpertEvaluationProtocol feature correctness, we ask whether the features
selectedbythemodelaccuratelydescribesaidrule.
Since our primary objective is to extract rules
FortheType-1rule,theanswerwouldbeyes. But
whicharehuman-readableandofassistancetothe
forruleslikeType-2,thefeaturesarenotinforma-
linguists, we enlist the help of language experts
tive even though the corresponding examples do
toevaluatetherulesonthreeparameters: correct-
ness,priorknowledge,featurecorrectness. Before
3https://www.thoughtco.com/
startingwiththeactualevaluation,wefirstaskthe ordinal-numbers-in-spanish-3079591
Type RuleFeatures Examples Label
Tambiénseutilizabaenlasprimerasgrabacionesyarreglosjazzísticos.
Type-1
Itwasalsousedinearlyjazzrecordingsandarrangements.
AdjisaOrdinal Before
Lasprimeras24horassoncruciales.
(valid)
Thefirst24hoursarecrucial.
Matisyahupiensaeditarprontounnuevodiscograbadoenestudio.
Type-2 Adjbelongstogroup:
Matisyahuplanstoreleaseanewstudio-recordedalbumsoon.
Before
con,como,no,más,lo Esunaexperiencianuevaestardesempleado.
(valid,notinformative)
It’sanewexperiencebeingunemployed
Ademásdeunagranvariedaddeaplicaciones
Type-3
Inadditiontoagreatvarietyofapplications.
AdjisNOTOrdinal After
Unauniónsolemnizadaenunpaísextranjero
(valid,toogeneral)
Anunionsolemnizedinaforeigncountry
EnÁfricahaynumerosaslenguastonales
Type-4
InAfricatherearenumeroustonallanguages
Adj’slemmaisnumeroso Before
Ellasposeenvarioslibros
(valid,toospecific)
Theyownseveralbooks
Lasconsecuenciasdecualquier(colapso)dedivisaeinflaciónmasiva.
Type-5
Theconsequencesexpectedfromanycurrencycollapseandmassiveinflation.
Adj’sheadnounisaconjunct After
(Realizan)trabajosdealtacalidad,muybuenosprofesionales
(invalid)
Theydohighqualitywork,verygoodprofessionals
Table1:TypesofrulesdiscoveredbythemodelforSpanishadjective-nounwordorder.Adjectivesarehighlighted
andthenounstheymodifyareunderlined. IllustrativeexamplesundereachrulearealsoshownwiththeirEnglish
translationinitalics. Labeldenotesthepredictedorder.
followacommonpattern. phological analysis. We use the standard SUD
train,validationandtestsplits. Syntacticandlexi-
7 Gold-standardAnalysisExperiments calfeaturesaredirectlyextractedfromthesegold
syntacticanalyses. Semanticfeaturesarederived
In this section, we present results to demonstrate
fromcontinuouswordvectors: westartwith300-
that our framework can discover the conditions
dimpre-trainedfasttextwordvectors(Bojanowski
which govern the different linguistic phenomena.
et al., 2017b) which are transformed into sparse
Specifically,weexperimentwithgold-standardsyn-
vectorsusingSubramanianetal.(2018)4. Last,we
tactic analysis derived from SUD treebanks, and
usetheXGBoost(ChenandGuestrin,2016)library
run experiments to answer questions about word
to learn the decision tree. Further details on the
order, agreement, and case marking (§ 7.1). Fur-
modelsetuparediscussedinAppendixC.
thermore,wemanuallyverifyasubsetoftheseex-
tractedrules(§7.2). Experimentingwithlanguages
7.1 AutomatedEvaluationResults
thathavebeenalreadystudiedandhaveannotated
Wetrainmodelsusingsyntacticfeaturesforalllan-
treebanksiscrucialforverifyingtheefficacyofour
guages covered by SUD, wherever the linguistic
approach before applying it to other true low- or
questionisapplicable. Wefindthatourmodelsout-
zero-resourcelanguages. Underthissettingwenot
performtherespectivebaselinesbyan(avg.) accu-
onlyhavecleanandexpert-annotateddata,butwe
racyof+7.3forwordorder,+28.1forcasemarking,
can also quickly compare the effect of data size
and+4.0ARMforagreement.5 Wealsoreportthe
onthesystemperformanceasdifferentlanguages
resultbreakdownunderthreeresourcesettings,low,
havetreebanksofvaryingsize.
mid, and high, where low-resource refers to the
Data and Model We use the Syntactic Univer- treebankswith<500sentences,mid-resourcehas
salDependenciesv2.5(SUD)(Gerdesetal.,2019) 500−5000sentencesandhigh-resourcehas>5000
treebanks which are based on the Universal De- sentences. Acrossallthreelinguisticphenomena,
pendencies(UD)(Nivreetal.,2016,2018)project, the(avg.) modelgainsoverthebaselineare+3.19
the difference being that the former allows func- for the low-resource, +10.7 for the mid-resource
tion words to be syntactic heads (as opposed to
4https://github.com/harsh19/SPINE
UD’spreferenceforcontentwords),whichismore
5WealsoexperimentedwithRandomforests(RF),assug-
conducive to our goal of learning grammar rules. gestedbyanonymousreviewers,butfoundthedecisiontrees
We experiment with treebanks for 61 languages, (DT)tobeslightlyunderperforming((avg.) -0.12acc). But
giventhatitisstraightforwardtoextractinterpretablerules
whicharepubliclyavailablewithannotationsfor
fromDT,whichisourprimarygoal,ascomparedtoRF,we
POS tags, lemmas, dependency parses, and mor- usetheformerforallexperiments,detailsinAppendixD.
(cid:4)baseline(cid:4)syntactic specificdatasetonwhichitwastrained,wetrain
(cid:4)syntactic+lexical(cid:4)syntactic+semantic amodelononetreebankofalanguageandapply
Adj-NounWordOrder NounCaseMarking the trained model directly on the test portions of
100 100 other treebanks of the same language. There are
80
80 30 languages in the SUD which fit this require-
60
60 ment. Figure4intheAppendixdemonstratesone
40
suchsettingforunderstandingthewordorderpat-
English Spanish Greek Turkish
terns across different French corpora, where the
Figure3: Comparingtheeffectofdifferentfeatureson
models have been trained on the largest treebank
thewordorderandcasemarking.
(fr-gsd). Forsubject-verborder,alltreebanksex-
and +12.8 for the high-resource. The larger the ceptthefr-fqbshowsimilarhightestperformance
treebank size, the larger the improvement of our ( >90% acc.). Interestingly, the model severely
model’s performance over the baseline. Even in underperforms (28% acc.) on fr-fqb which is a
low-resourcesettings,againoverthebaselinesug- question-bankcorpuscomprisingofonlyquestions,
gests that our approach is extracting valid rules, andquestionsinFrenchcanhavevaryingwordor-
whichisencouragingforlanguagedocumentation derpatterns.7 Themodelfailstocorrectlypredict
efforts. We present the result breakdown of indi- the word order because in the training treebank
vidualrelationsinAppendix(Table3). only 1.7% of examples are questions making it
Asmotivatedin§3,theconditionswhichgovern challengingforthemodeltolearnwordorderrules
alinguisticphenomenonvaryconsiderablyacross fordifferentquestiontypes.
languages, which is also reflected in our model’s Throughthistool, alinguistcanpotentiallyin-
performance. For example, the model trained on spectandderiveinsightsonhowthepatternsdis-
syntacticfeaturesaloneissufficienttoreachahigh covered for a linguistic question vary across dif-
accuracy(avg.94.2%)forpredictingtheadjective- ferentsettings,bothwithinalanguageandacross
noun order in Germanic languages. But for Ro- differentlanguagesaswell.
mance languages, using only syntactic features
7.2 HumanEvaluationResults
leadstomuchlowerperformance(avg.74.6%). We
experiment with different features and report re- Throughtheaboveexperiments,weautomatically
sultsforasubsetoflanguagesinFigure3. Observe evaluated that the extracted rules are predictive
thatforSpanishadjective-nounorderaddinglexi- (to some extent) and applicable to the language
calfeaturesimprovestheperformancesignificantly ingeneral. Beforeapplyingthisframeworkonan
(+11.57)oversyntacticfeatures,andsemanticfea- endangered language we first perform a manual
turesprovideanadditionalgainof+4.48. Studying evaluation ourselves for English and Greek. We
thelanguagesmarkedashaving“nodominantor- selecttheselanguagesbasedontheavailabilityof
der” in WALS, we find our model does show a human annotators, using one expert each for En-
higher entropy. SUD contains 8 such languages glishandGreek. First,wenotethatthetotalnum-
forsubject-verborder,andourmodelproducesan berofrulesforEnglish(29)aremuchlessthanthat
(avg.) entropy of 1.09, as opposed to (avg.) 0.75 forGreek(161),thelatterbeingmoremorpholog-
entropy for all other languages. For noun case ically rich. We find that 80% of the rules (across
markinginGreek,syntacticfeaturesalreadybring all phenomena) are valid grammar rules for both
themodelperformanceto94%. ForTurkish,thead- languages. Asignificantportion(40%)ofthevalid
ditionofsemanticfeaturesraisesthemodelperfor- rulesareeithertoospecificortoogeneral,which
manceby+9.38. Themodelnowpreciselycaptures highlightsthatthereisscopeofimprovementinthe
thatnounsforlocationslikeev, oda, kapı, du¨nya6 feature and/or model design. Interestingly, even
typicallytakethelocativecase. Thisisin-linewith for English, there were 7 rules which the expert
BamyacıandvonHeusinger(2016)whichoutlines wasnotawareof. Forexample,thefollowingrule
theimportanceofanimacyinTurkishdifferential foradjective-nounorder–“whenthenominalisa
casemarking.
Toconfirmthatthesediscoveredconditionsgen- 7In questions such as Que signifie l’ acronyme NASA?
("What does the acronym NASA mean?"), the verb comes
eralize to the language as a whole and not the
beforeitssubject, butforquestionssuchasQuiproduitle
logiciel ? ("Who produces the software?") the subject is
6house,room,door,world beforetheverb.
YCARUCCA
word like something,nothing,anything, the adjec- text. Werandomlysplittheparseddataintoatrain
tivecancomeafterthenoun.“. ForGreek,almost and test set (80:20) and apply our general frame-
all valid rules were known to the expert, except worktoextractrules(detailsinAppendixE).
foroneGenderagreementrule8. Regardingfeature
correctness, the Greek expert found 69% of the
valid rules to be readable and informative, while
the English expert found 58% of such rules. We Results Hmonghasnoinflectionalmorphology
showtheindividualresultsinAppendix(Figure5). so we only train the model to answer word order
These insights may have utility even for lan- questions. We conduct the expert evaluation on
guagesthatalreadyhaveautomaticNLPtoolsfor four relations where our model outperforms the
POStaggingordependencyparsing,orevenatree- baseline,albeitslightly(+4.08forAdj-N,+0.12for
bank,asexistingannotationsdonotexhaustively Subj-V,+0.52forAdp-N,+0.72forNum-N).For
describefine-grainedorcomplexlinguisticbehav- Obj-Vrelation,ourmodelisonparwiththebase-
iorsonaholisticlevel(e.g. deviationinwordorder linewhichcouldindicatethateithertherewerenot
patterns or explaining the process of agreement). many examples whose word order deviated from
From the user-study above, we do find that the the dominant order or the model needs improve-
approachdiscoveredfine-grainedbehaviorsforEn- ment. First,weasktheexpert,alinguistwhostud-
glishandGreek,whichthelanguageexpertswere iesHmong,todescribetherules(ifany)foreachre-
not aware of or could not think of readily. In ad- lation. Comparingwiththeexpert’sprovidedrules,
dition,eveniflanguagedocumentationdoesexist wefindthatthemodelissuccessfulindiscovering
for a language, this does not mean that it is read- the dominant pattern for all relations. However,
ily available in a standardized machine-readable of the 30 rules (across all relations) presented to
format,whereastheoutputofourmethodis. the expert for annotation, only 5 rules (1 rule for
subject-verb,4rulesfornumeral-noun)werefound
8 HmongDawStudy
topreciselydescribethelinguisticdistinction. For
instance,accordingtotheexpert,numeralscannot
Finally,totesttheapplicabilityof AUTOLEX ina
occurimmediatelybeforenouns,rathertheyoccur
languagedocumentationsituation,weexperiment
before classifiers which then occur before nouns
with Hmong Daw (mww), a threatened language
(“1clf-1noun-1”). Interestingly,onerulecaptured
variety,spokenbyroughly1MpeopleacrossUS,
exampleswherethenumeralswereoccurringim-
China, Laos, Vietnam and Thailand. It certainly
mediatelybeforenounswithouttheclassifiers(e.g.
can be categorized as a low-resourced language
“1 noun-1, 2 noun-2”), which the expert was not
withrespecttocomputationalresourcesaswellas
aware of. On one hand, this is promising as the
accessibleanddetailedmachine-readablegrammat-
model,despitebeingtrainedonnoisysentencesand
icaldescriptions. Furthermore,thisstudypresents
syntacticanalyses,wasabletodiscoverinstances
a realistic setting of language analysis as there is
of interesting linguistic behavior. However, the
noexpert-annotatedsyntacticanalysisavailable.
expertnotedthatalargeportionoftheruleswere
Wehadaccessto445kHmongsentences,which
difficulttoevaluateasthesereferredtoexamples
were collected from the soc.culture.hmong
whichwereincorrectlyparsed,someofwhicheven
Usenet group. Since the data was scraped from
describedtheEnglishportionofcode-mixeddata.
theweb,itwasnoisyandintermixedwithEnglish.
Therefore,firstweautomaticallycleanthecorpus
Despite showing the promise of automatically
usingacharacter-levellanguagemodeltrainedon
obtainingdetaileddescriptionsonlanguageswith
English. Thisautomaticallyfiltered61ksentences.
good syntactic analyzers, we can see that it is
Next,weautomaticallyobtainsyntacticanalyses,
still challenging to apply methods to such under-
forwhichwetrainUdify(KondratyukandStraka,
resourcedlanguages. Thisposesanewchallenge
2019),amultilingualautomaticparserthatjointly
for zero-shot parsing, even the relatively strong
predictsPOStags,lemmas,morphologicalanalysis
modelofKondratyukandStraka(2019)resultedin
and dependency parses, on Vietnamese, Chinese
ahighenougherrorratethatitimpactedtheeffec-
andEnglishtreebanksandapplyittotheHmong
tivenessofourmethod, andmethodswithhigher
accuracymayfurtherimprovetheresultsofend-to-
8The rule was, “proper-nouns modifiers do not need to
necessarilyagreewiththeirheadnouns”. endgenerationofgrammardescriptions.
9 NextSteps EmilyM.Bender,DanFlickinger,andStephanOepen.
2002. Thegrammarmatrix: Anopen-sourcestarter-
While we have demonstrated that our automatic kitfortherapiddevelopmentofcross-linguistically
frameworkcananswerlinguisticquestionsacross consistent broad-coverage precision grammars. In
COLING-02: Grammar Engineering and Evalua-
differentlanguages,theruleswediscoverarelim-
tion.
itedbytheSUDannotationdecisions. Forexample,
severalnounsinGermanarenotannotatedforthe Emily M. Bender, Michael Wayne Goodman, Joshua
Crowgey, andFeiXia.2013. Towardscreatingpre-
defaultcase,whichmeansthesenounsgetignored
cisiongrammarsfrominterlinearglossedtext: Infer-
byourmodelinthecurrentsetting. Possibly,using
ring large-scale typological properties. In Proceed-
language-specific annotations or heuristics could ings of the 7th Workshop on Language Technology
helpalleviatethisproblem. AsnotedintheHmong forCulturalHeritage,SocialSciences,andHuman-
ities, pages 74–83, Sofia, Bulgaria. Association for
study, the quality of rules depends on the quality
ComputationalLinguistics.
of the underlying parses. We plan to devise an
iterative process where a linguist, assisted by an Barry J Blake. 2009. History of the research on case.
InTheOxfordhandbookofcase.
automatic parser, can improve syntactic parsing.
Themodelextractsrulesusingimprovedanalyses, PiotrBojanowski,EdouardGrave,ArmandJoulin,and
which the linguist can inspect and provide more Tomas Mikolov. 2017a. Enriching word vectors
withsubwordinformation. TransactionsoftheAsso-
inputs to further improve. We note that currently
ciationforComputationalLinguistics,5:135–146.
we use English as the meta-language to describe
therules,whichassumesthatan AUTOLEX user PiotrBojanowski,EdouardGrave,ArmandJoulin,and
iswell-versedwithEnglishandthecorresponding Tomas Mikolov. 2017b. Enriching word vectors
withsubwordinformation. TransactionsoftheAsso-
grammarterms. Inthefuture,weplantoprovide
ciationforComputationalLinguistics,5:135–146.
theserulesintheuser’schoiceoflanguage.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
StatementofEthics roshiMasuichi,andChristianRohrer.2002. Thepar-
allel grammar project. In COLING-02: Grammar
EngineeringandEvaluation.
Weacknowledgethatthereareseveralethicalcon-
cernswhileworkingwithendangeredorthreatened Aditi Chaudhary, Antonios Anastasopoulos, Adithya
languages,inparticular,thatweincludeandtake Pratapa, David R. Mortensen, Zaid Sheikh, Yulia
Tsvetkov, and Graham Neubig. 2020. Automatic
guidancefromcommunitymemberswhendesign-
extraction of rules governing morphological agree-
inganytechnologyusingtheirdata. Secondly,that
ment. In Proceedings of the 2020 Conference on
anydatawecollectisappropriatelyused,without Empirical Methods in Natural Language Process-
causinganydetrimentaleffectorbiasonthecom- ing (EMNLP), pages 5212–5236, Online. Associa-
tionforComputationalLinguistics.
munity. Inadherencetothat,thisworkisdonein
collaborationwithaHmonglinguistwhoisinclose Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A
collaborationandconsultationwiththecommunity. scalabletreeboostingsystem. InProceedingsofthe
22ndacmsigkddinternationalconferenceonknowl-
WewillreleaseanytoolsthatwebuildforHmong
edgediscoveryanddatamining,pages785–794.
inconsultationwiththemandthecommunity.
Noam Chomsky. 1993. Lectures on government and
Acknowledgements binding: ThePisalectures. WalterdeGruyter.
Greville G Corbett. 2003. Agreement: Terms and
Theauthorsaregratefultotheanonymousreview-
Boundaries. InTexasLinguisticSocietyConference.
erswhotookthetimetoprovidemanyinteresting
commentsthatmadethepapersignificantlybetter. Greville G Corbett. 2009. Agreement. In Die slavis-
chenSprachen/TheSlavicLanguages.
ThisworkissponsoredbytheWaibelPresidential
FellowshipandbytheNationalScienceFoundation OstenDahlandKariFraurud.1996. Animacyingram-
undergrant1761548. maranddiscourse. PragmaticsandBeyondNewSe-
ries.
Matthew S. Dryer. 2007. Word order. In Language
References TypologyandSyntacticDescription.
Elif Bamyacı and Klaus von Heusinger. 2016. Ani- Matthew S. Dryer and Martin Haspelmath, editors.
macy effects on differential object marking in turk- 2013. WALSOnline. MaxPlanckInstituteforEvo-
ish. Posterpresentationatlinguisticevidence. lutionaryAnthropology,Leipzig.
Kim Gerdes, Bruno Guillaume, Sylvain Kahane, and American Chapter of the Association for Computa-
GuyPerrier.2019. Improvingsurface-syntacticuni- tionalLinguistics(Demonstrations),pages127–131,
versal dependencies (SUD): MWEs and deep syn- Minneapolis, Minnesota. Association for Computa-
tactic features. In Proceedings of the 18th Interna- tionalLinguistics.
tionalWorkshoponTreebanksandLinguisticTheo-
ries (TLT, SyntaxFest 2019), pages 126–132, Paris, William D. Lewis and Fei Xia. 2008. Automatically
France.AssociationforComputationalLinguistics. identifyingcomputationallyrelevanttypologicalfea-
tures. In Proceedings of the Third International
Ken Hale, Michael Krauss, Lucille J Wata- Joint Conference on Natural Language Processing:
homigie, Akira Y Yamamoto, Colette Craig, Volume-II.
LaVerne Masayesva Jeanne, and Nora C England.
1992. EndangeredLanguages. Language. Chaitanya Malaviya, Matthew R. Gormley, and Gra-
hamNeubig.2018. Neuralfactorgraphmodelsfor
Morris Halle, Alec Marantz, Kenneth Hale, and cross-lingual morphological tagging. In Proceed-
Samuel Jay Keyser. 1993. Distributed morphology ings of the 56th Annual Meeting of the Association
andthepiecesofinflection. TheviewfromBuilding forComputationalLinguistics(Volume1: LongPa-
20. pers), pages 2653–2663, Melbourne, Australia. As-
sociationforComputationalLinguistics.
Lars Hellan. 2010. From descriptive annotation to
grammarspecification. InProceedingsoftheFourth Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Linguistic Annotation Workshop, pages 172–176, Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
Uppsala, Sweden. Association for Computational man Ganchev, Keith Hall, Slav Petrov, Hao
Linguistics. Zhang, Oscar Täckström, Claudia Bedini, Núria
Bertomeu Castelló, and Jungmee Lee. 2013. Uni-
NikolausPHimmelmann.1998. Documentaryandde-
versalDependencyannotationformultilingualpars-
scriptivelinguistics. Linguistics.
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
Kristen Howell, Emily M. Bender, Michel Lockwood,
ume2: ShortPapers),pages92–97,Sofia,Bulgaria.
FeiXia, andOlgaZamaraeva.2017. Inferringcase
AssociationforComputationalLinguistics.
systems from IGT: Enriching the enrichment. In
Proceedings of the 2nd Workshop on the Use of
TomasMikolov,IlyaSutskever,KaiChen,GregSCor-
ComputationalMethodsintheStudyofEndangered
rado, and Jeff Dean. 2013. Distributed representa-
Languages,pages67–75,Honolulu.Associationfor
tionsofwordsandphrasesandtheircompositional-
ComputationalLinguistics.
ity. In Advances in neural information processing
RonaldMKaplan, JoanBresnan, etal.1981. Lexical-
systems,pages3111–3119.
functionalgrammar: Aformalsystemforgrammati-
Christopher Moseley. 2010. Atlas of the World’s Lan-
calrepresentation. Citeseer.
guagesinDanger. UNESCO.
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Joakim Nivre, Rogier Blokland, Niko Partanen,
Miriam Butt. 2005. The feature space in parallel
Michael Rießler, and Jack Rueter. 2018. Universal
grammarwriting. ResearchonLanguageandCom-
Dependencies2.3.
putation,3(2-3):139–163.
Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim- JoakimNivre,Marie-CatherineDeMarneffe,FilipGin-
ple and accurate dependency parsing using bidirec- ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
tional LSTM feature representations. Transactions ning,RyanMcDonald,SlavPetrov,SampoPyysalo,
of the Association for Computational Linguistics, NataliaSilveira,etal.2016. Universaldependencies
4:313–327. v1: Amultilingualtreebankcollection. InProceed-
ings of the Tenth International Conference on Lan-
Seppo Kittilä, Katja Västi, and Jussi Ylikoski. 2011. guageResourcesandEvaluation(LREC’16),pages
Introduction to case, animacy and semantic roles. 1659–1666.
JohnBenjaminsPublishing.
Robert Östling. 2015. Word Order Typology through
Dan Kondratyuk and Milan Straka. 2019. 75 lan- MultilingualWordAlignment. InACL.
guages, 1 model: Parsing universal dependencies
universally. In Proceedings of the 2019 Confer- Carl Pollard and Ivan A Sag. 1994. Head-driven
ence on Empirical Methods in Natural Language phrase structure grammar. University of Chicago
Processing and the 9th International Joint Confer- Press.
ence on Natural Language Processing (EMNLP-
IJCNLP),pages2779–2795,HongKong,China.As- Adithya Pratapa, Antonios Anastasopoulos, Shruti Ri-
sociationforComputationalLinguistics. jhwani,AditiChaudhary,DavidR.Mortensen,Gra-
hamNeubig,andYuliaTsvetkov.2021. Evaluating
Haley Lepp, Olga Zamaraeva, and Emily M. Bender. the morphosyntactic well-formedness of generated
2019. Visualizinginferredmorphotacticsystems. In texts. In Proceedings of the 2021 Conference on
Proceedings of the 2019 Conference of the North EmpiricalMethodsinNaturalLanguageProcessing,
pages7131–7150, OnlineandPuntaCana, Domini-
can Republic. Association for Computational Lin-
guistics.
J.RossQuinlan.1986. Inductionofdecisiontrees. Ma-
chinelearning,1(1):81–106.
Anant Subramanian, Danish Pruthi, Harsh Jhamtani,
Taylor Berg-Kirkpatrick, and Eduard Hovy. 2018.
Spine: Sparse interpretable neural embeddings. In
Thirty-SecondAAAIConferenceonArtificialIntelli-
gence.
Juliette Thuilier, Margaret Grant, Benoît Crabbé, and
AnneAbeillé.2021. Wordorderinfrench: therole
ofanimacy. Glossa:ajournalofgenerallinguistics,
6(1).
KristinaToutanvoaandChristopherD.Manning.2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In 2000 Joint
SIGDATConferenceonEmpiricalMethodsinNatu-
ral Language Processing and Very Large Corpora,
pages 63–70, Hong Kong, China. Association for
ComputationalLinguistics.
DingquanWangandJasonEisner.2017. Fine-grained
prediction of syntactic typology: Discovering la-
tent structure with supervised learning. Transac-
tions of the Association for Computational Linguis-
tics,5:147–161.
A RelatedWork also improving the grammar specification which
reliesonthoseanalyses.
Prior work (Lewis and Xia, 2008; Hellan, 2010;
Bender et al., 2013; Howell et al., 2017) have B LearningandExtractingRules
proposed methods to map descriptive grammars,
StatisticalThresholdforRuleExtraction Sim-
present in the form of inter-linear glossed text
ilartoChaudharyetal.(2020),weapplystatistical
(IGT),toexistinghead-phrasestructuregrammar
testing to label leaves. For morphological agree-
(HPSG)basedgrammarsystemwhichismachine-
ment,weusethesamehypothesisdefinitionwhere
readable. Lewis and Xia (2008) enrich IGT data
thenullhypothesisH statesthateachleafdenotes
with syntactic structures to determine canonical 0
chance-agreement. This means that there is no
wordorderandcasemarkingobservedinthelan-
requiredagreementbetweenaheadanditsdepen-
guage. Theydonotethat,whilealinguistcarefully
dent on the morphological attribute m. The hy-
choosestheexamplestocreatetheIGTcorpussuch
pothesistobetestedforisH whichstatesthatthe
that they are representative of the linguistic phe- 1
leaf denotes required-agreement. For case mark-
nomenaofinterest,insightsderivedfromIGTmay
ing,wefollowasimilarapproachasexplainedfor
sufferfromthisbiasasthedatadoesn’tencompass
wordorder. WecandesignH aswordorder,be-
many of the naturally-occurring examples. Hel- 0
causeundertheabstractcaseviewpoint(§3),case
lan(2010)presentasentence-levelannotationcode
is a universal property for each word. We use a
which maps the properties of the sentence to dis-
p−value = 0.01basedontherecommendationof
cretelabels. Thesediscretelabelsformatemplate
Chaudharyetal.(2020).
which are then mapped to in a mixed to HPSG
or LFG format (Pollard and Sag, 1994; Kaplan RuleVisualization Undereachrule,wepresent
et al., 1981). Bender et al. (2013) extract major- asubsetofexamplesfromthetrainingportionof
constituentwordorderandcasemarkingproperties thetreebanktoillustratetherule. Positiveexamples
fromtheIGTforadiversesetoflanguages. Poten- refer to the examples which have features (from
tially,grammarrulescanalsobederivedfromex- thatrule)andfollowthelabelaspredictedbythat
istingprojectssuchastheLinGOGrammarMatrix leaf. However,therecouldbeexamplesinthetrain-
(Benderetal.,2002),ParGram(Buttetal.,2002; ingdata whichhavethe same featuresas defined
Kingetal.,2005). Thesearegrammardevelopment under that rule, but these example do not follow
tools designed to write and create grammar spec- thepredictedlabel. Werefertotheseexamplesas
ificationsthatsupportawiderangeoflanguages, negativeexamples.
inaunifiedformat. Theyfocusonmappingsimple Sinceweonlyshowasmallsetofexamples,we
description of languages, obtained from existing selecttheseexamplestobeconciseandrepresen-
IGT-annotateddataorinputfromalinguist,topre- tative. Wefirstgrouptheexamplesundertherule
cisiongrammarfragments,groundedinagrammar withthelemmatizedformsofthefocuswords. For
formalismsuchasHPSG,LFG.Ourworkdiffersin example,undertheType-1rule(Table1)extracted
that,1)weattempttodiscoverandexplainthelocal for Spanish adjective-noun word order, the focus
linguisticbehaviorsforthelanguageingeneral,2) words are the adjective (w ) and the noun (w ).
a b
wedonotextractrulesforanindividualsentence Wegrouptheseexamplesbythelemmatizedforms
inisolation,assomeoftheHPSG/LFG-basedap- of the adjective and noun (cid:104)l ,l (cid:105). The examples
a b
proachesdo,3)wediscoverthesebehaviorsfrom groupedunderalemmatizedpair(cid:104)l ,l (cid:105)arethen
a b
naturallyoccurringsentences. Wedonotethatthe sortedbytheirlengths. Foreachlemmatizedpair
ruleswepresentinthisworkarebasedontheSUD (cid:104)l ,l (cid:105), we select the top shortest examples. Fi-
a b
annotationscheme,butthecurrentframeworkcan nally, all selected examples are shuffled and we
beeasilyextendedtoanyothersuchscheme. InTa- randomlyselect10examples.
ble2,weoutlinethedifferentlinguisticquestions
answeredbyourworkandtherelatedwork. C ExperimentalSetup
Therehasalsobeenworkondevelopingtoolkits
Data Belowwedescribethelicensedetailsofthe
to visualize some aspects of language structure –
datasetsweused:
Leppetal.(2019)presentaweb-basedsystemto
explore different morphological analyses. They • SUD treebanks: Nospecificlicenseisspeci-
alsoallowausertoimprovetheanalysesthereby fied,butthedataisreleasedaspartofresearch
LinguisticPhenomena Work Rule-Type CorpusType
WordOrder Ours C+FG Rawtext
GrammarMatrix(Benderetal.,2002) C+FG IGTtext*
LewisandXia(2008) C IGTtext
Benderetal.(2013) C IGTtext
Östling(2015) C Rawtext
WangandEisner(2017) C Rawtext
WALSDryerandHaspelmath(2013) C Referencegrammar*
CaseMarking Ours C+FG Rawtext
GrammarMatrix(Benderetal.,2002) C+FG IGTtext*
WALSDryerandHaspelmath(2013) C Referencegrammar*
Howelletal.(2017) C IGTtext
Agreement Ours C+FG Rawtext
GrammarMatrix(Benderetal.,2002) C+FG IGTtext*
Chaudharyetal.(2020) C+FG Rawtext
Sentenceconstruction Hellan(2010) FG IGTtext*
Table2:Anoverviewoflinguisticquestionsautomaticallyansweredbyourcurrentworkandexistingrelatedwork.
Some of them combine semi-automatic approaches with manually annotated resources, there are marked with *.
Rule-Typedenotesthetypeofruleextractedforalanguage,Creferstocoarse-grainedsuchasrulesforcanonical
wordorder,FGreferstofine-grainedi.e. rulesextractedatalocallevel.
work (Gerdes et al., 2019). We have used basedonthevalidationsetperformance. Herethe
this data as intended which is for academic hyperparametersweuse:
researchpurposes.
• criterion: {gini,entropy}
• Fasttext embeddings: Released9 under the • max-depth: {3,4,5,6,7,8,9,10,15,20}
Creative Commons Attribution-Share-Alike
License 3.0. We have used this data as in- • n-estimators: 1
tended, which is for academic research pur-
• learning-rate: 0.1
poses.
• objective: multi:softprob
• Hmong Daw: This dataset was collected by
oneoftheco-authorsfromtheUsenetgroup D Gold-standardExperiments
soc.culture.hmong and is currently in sub-
D.1 AutomatedEvaluationResults
mission to LREC. The data used will be re-
leasedaspartoftheCreativeCommonsZero Inthemaintext,wereportedtheaverageimprove-
v1.0Universallicense. Accordingly,wewill mentforthewordorder,agreementandcasemark-
alsoreleasethetrain/testsplitforbetterrepro- ingmodels. InTable3wepresentthebreakdown
ducibility. per each question. The word order results are re-
portedover56languages,agreementover38and
Theobviousidentifyinginformationhasbeen
casemarkingover35languages.10
removedfromthedata,althoughitwouldbe
Wealsoreportresultsunderthreeresourceset-
possibletorecoverthatinformationbygoing
tingsasshowninTable5. Wealsoshowindividual
backtotheoriginalUsenetposts.
resultspereachlanguageforwordorder(Table7,
Table 8), agreement (Table 9), case marking (Ta-
Model Asdescribedinthemaintext,weusethe
ble 10, Table 11). We note that we report these
XGBOOSTtolearnadecisiontree. Foreachlanguage,
resultsonasinglerunoftheexperiment.
therunningtimeofthemodelisapproximately2-5
We experiment with Random Forest, which is
mins. Weperformagridsearchoverasetofhyper-
abetterclassifier,incomparisontodecisiontrees,
parameters and select the best performing model
10Somelanguageshaveverylittletrainingdataonwhich
9https://fasttext.cc/docs/en/pretrained-vectors. wecouldn’tfitamodelwhileforsomelanguagesthelinguistic
html questionswasnotapplicable.
LinguisticPhenomena Model Gain Model Language Randomforest(acc.) Decisiontree(acc.) Baseline
adjective-noun el 99.29 99.29 99.29
WordOrder adjective-noun 2.61 es 73.32 71.46 68.1
ur 99.04 99.04 99.04
subject-verb 6.95 fi 98.37 99.09 98.37
object-verb 10.78 lv 98.84 98.84 98.84
it 70.4 69.26 66.02
numeral-noun 9.88 no 97.92 97.92 97.76
fr 74.01 73.6 73.6
noun-adposition 2.31 ro 95.83 95.19 92.95
bg 97.98 98.49 97.23
Agreement Gender 4.02 gl 79.2 79.2 79.2
subject-verb en 98.81 98.81 94.15
Person 1.08
el 85.52 83.45 73.56
Number 4.95 es 83.5 82.52 71.52
tr 92.96 92.96 92.96
hi 99.56 99.56 99.56
CaseMarking NOUN 30.03 fi 87.14 90.36 79.16
lv 79.79 77.73 73.99
PRON 32.66
it 82.37 81.44 71.76
DET 47.33 no 86.28 85.33 70.34
fr 94.21 94.21 94.21
PROPN 29.77 ug 95.13 95.13 95.13
ro 75.62 73.49 54.36
ADJ 35.59 bg 81.67 79.22 72.73
VERB 18.76 gl 86.26 85.5 82.14
ADP 15.4 object-verb en 98.8 98.66 97.26
el 96.2 96.2 86.0
NUM 25.81 es 95.99 95.99 90.4
tr 96.64 96.64 96.64
hi 99.78 99.61 74.71
Table3: Breakdownoftheperformancegain(overthe ur 99.45 99.59 79.5
fi 85.21 86.36 74.83
baseline)foreachlinguisticquestion.Theperformance lv 83.31 82.95 75.24
it 94.97 94.79 84.92
of the agreement models is compared with the mod- no 98.68 98.68 95.86
fr 96.96 96.53 86.33
elstrainedoversimplesyntacticfeaturesinChaudhary ro 86.99 87.79 65.06
bg 92.53 92.22 80.66
etal.(2020). gl 94.48 94.17 82.2
(cid:4)fr-gsd(cid:4)fr-partut(cid:4)fr-pud(cid:4)fr-ftb(cid:4)fr-fqb(cid:4)fr-spoken noun-adposition e en
s
9 19 0. 04 .2
0
9 19 0. 04 .2
0
9 99 8. .4 82
3
ur 98.91 98.91 98.91
100 fi 89.35 98.12 89.35
lv 97.78 97.78 97.78
no 99.3 99.26 99.14
80 gl 99.32 99.32 99.18
numeral-noun en 88.06 88.06 82.09
60 el 80.6 80.6 80.6
es 88.62 88.62 75.61
ur 95.63 95.63 95.63
40 fi 92.14 87.25 90.71
it 82.33 79.32 79.32
20 no 85.78 88.44 88.44
fr 81.16 81.88 60.87
Subj-V Adj-N Obj-V ro 84.14 84.83 62.07
bg 88.24 88.24 88.24
Table 4: Comparing the accuracy of Random forest
Figure4: Comparingtheaccuracyofthemodelacross classifierwiththeDecisionTreefordifferentwordor-
different treebanks. Each model is trained on the fr- derrelations.
gsd treebank and directly applied on the other tree-
banks. Shadedbarsdenotethebestmodelperformance
pertareco-authorsofthepaper. ForEnglish,atotal
trained using all features while solid bars denote the
of 15 rules were evaluated for agreement, 11 for
most-frequentbaselineforthattreebank.
word order and 3 for case marking. For Greek, a
totalof35ruleswereevaluatedforagreement,11
butitisnotasinterpretableasthelatter. Neverthe- forwordorderand115forcasemarking. Wedis-
less,asrequestedbytheanonymousreviewers,we cussedtheresultsinthemaintext,herewepresent
comparehowdecisiontreesfareagainstRandom thefiguresforEnglishandGreek(Figure5). For
forest in Table 4. We train models for answering English, there were some rules which the expert
wordorderquestions,across15languagesfromthe was not aware of. We discussed one example for
SUDtreebanks. Overall,weobservethatdecision wordorderinthemaintext,weshowanexample
treesslightlyunderperformtheRandomforest,but foragreementandcasemarkinginTable6.
byonly(avg.) -0.12acc. points,wheretherange
ofaccuracyis0-100. Givenourprimarygoalisto E HmongDawStudy
extractcomprehensibledescriptions,weopttouse
decisiontrees. Data WeexperimentedwiththeHmongDawva-
rietyinthissetting. Oneofco-authorsofthepaper
D.2 HumanEvaluationResults
isaHmonglinguistwhoisinclosecollaboration
We conduct expert evaluation for English and and consultation with the community, and is the
Greek. Both the English and Greek language ex- expertwhoprovideduswiththeHmongdataand
YCARUCCA
(cid:4)precise(cid:4)too-specific
(cid:4)too-general(cid:4)not-a-rule (cid:4)yes-precisely(cid:4)yes-somewhat (cid:4)yes(cid:4)no
(cid:4)no-but-aware(cid:4)no-not-aware (cid:4)partially-correct
English
0.8
0.6
0.8 0.8
0.4
0.6 0.6
0.2
0.4 0.4
0.2 0.2
CaseMarking WordOrder Agreement
CaseMarking WordOrder Agreement CaseMarking WordOrder Agreement
Greek
0.8
0.6
0.8 0.8
0.4
0.6 0.6
0.2
0.4 0.4
0.2 0.2
CaseMarking WordOrder Agreement
CaseMarking WordOrder Agreement CaseMarking WordOrder Agreement
Figure 5: Evaluating rule correctness (left), prior knowledge (middle) and feature correctness (right). Top plot
showstheresultsforEnglishwhilethebottomplotshowsforGreek.
Figure6: Ruleevaluationformpresentedtothelanguageexpert.
SELURFOEGATNECREP
SELURFOEGATNECREP
LinguisticPhenomena Resource-Setting Gain(numberofmodels)
Agreement low -3.39(10)
mid 1.07(25)
high 5.89(55)
CaseMarking low 12.14(11)
mid 28.17(56)
high 37.17(56)
Table5: Breakdownoftheperformancegain(overthe
baseline) for each linguistic question by resource set-
ting.
alsohelpedevaluatetheextractedgrammarrules.
WechoseVietnamese,ChineseandEnglishtotrain
udify model as they share syntactic and lexical
similaritywithHmong. Weusethesamehyperpa-
rametersettingasspecifiedinthecode11.
11https://github.com/Hyperparticle/udify
LinguisticPhenomena Rule Examples Label
Number dependent’sheadisaNOUN Kidsfungamesareaddedtothebuilding. Not-required-agreement
Agreement Nationalistgroupsarecomingtotheconference.
Object Pronounisaoblique BecauseLargeFriesgiveyouFOURPIECES! Accusative
CaseMarking Givehimacalltommorow
Table 6: Some example of rules for agreement and case marking, which the expert annotator was not aware of.
Thefocuswordishighlighted,foragreementwealsounderlinetheheadwithwhichthedependent’sagreementis
checked.Theexamplesundernumberagreementdemonstratethatwhendependent’sheadisanounthedependent
need not agree with its head. We show one example where the first example shows the dependent matches the
numberofthehead,andthesecondexampleshowsthatitdidn’tnotmatch.
Type Lang Train-Test-Baseline Type Lang Train-Test-Baseline
adjective-noun it-vit 70.71-69.51-66.02 object-verb cu-proiel 80.37-82.72-76.03
adjective-noun no-nynorsk 97.68-97.92-97.76 object-verb be-hse 87.79-95.38-95.38
adjective-noun ro-nonstandard 87.46-95.19-92.95 object-verb sv-lines 96.75-96.79-95.31
adjective-noun bg-btb 97.27-98.49-97.23 object-verb uk-iu 82.77-87.16-83.16
adjective-noun gl-ctg 79.02-79.2-79.2 object-verb ga-idt 94.89-91.55-82.8
adjective-noun cs-pdt 94.69-94.36-93.69 object-verb sk-snk 81.84-86.17-80.91
adjective-noun fi-tdt 98.56-99.09-99.09 object-verb hu-szeged 73.23-68.26-53.73
adjective-noun pl-pdb 65.61-68.0-61.84 object-verb got-proiel 74.58-80.15-72.44
adjective-noun la-ittb 63.64-59.65-40.2 object-verb hr-set 89.27-92.2-83.32
adjective-noun nl-alpino 98.38-98.65-98.65 object-verb lzh-kyoto 97.86-98.01-95.7
adjective-noun mt-mudt 78.91-82.84-82.84 object-verb lv-lvtb 85.03-82.95-75.24
adjective-noun ja-bccwj 99.4-98.69-98.69 object-verb et-edt 76.03-79.51-69.67
adjective-noun orv-torot 71.39-65.76-53.48 object-verb fro-srcmf 79.62-81.82-48.25
adjective-noun pt-gsd 70.31-74.54-71.63 object-verb af-afribooms 82.72-96.19-86.03
adjective-noun cu-proiel 84.96-84.98-84.98 object-verb hy-armtdp 71.47-74.58-44.92
adjective-noun sv-lines 98.3-98.29-95.67 object-verb en-ewt 98.33-98.94-97.26
adjective-noun uk-iu 94.68-95.19-95.19 object-verb fr-gsd 98.89-97.18-86.33
adjective-noun sk-snk 96.11-95.17-95.17 object-verb el-gdt 97.18-96.2-86.0
adjective-noun got-proiel 79.51-79.51-72.48 object-verb es-gsd 97.47-95.99-90.4
adjective-noun hr-set 96.24-96.78-96.36 object-verb tr-imst 95.38-96.64-96.64
adjective-noun lv-lvtb 98.93-98.84-98.84 object-verb ru-syntagrus 87.47-88.33-85.63
adjective-noun et-edt 99.57-99.36-99.01 object-verb sl-ssj 84.16-88.24-72.92
adjective-noun fro-srcmf 73.84-74.42-73.26 object-verb id-gsd 99.33-98.99-95.97
adjective-noun en-ewt 97.84-98.25-96.77 object-verb lt-alksnis 80.76-79.02-69.73
adjective-noun fr-gsd 71.04-73.8-73.6 object-verb ar-nyuad 96.27-95.91-95.63
adjective-noun el-gdt 97.34-99.29-99.29 object-verb grc-proiel 72.98-75.87-67.05
adjective-noun es-gsd 76.27-71.46-68.1 subject-verb it-vit 82.95-82.53-71.76
adjective-noun ru-syntagrus 97.84-98.0-96.54 subject-verb no-nynorsk 83.42-85.33-70.34
adjective-noun sl-ssj 98.22-98.27-97.78 subject-verb ug-udt 95.32-95.13-95.13
adjective-noun id-gsd 93.41-92.79-92.79 subject-verb ro-nonstandard 69.06-74.27-54.36
adjective-noun lt-alksnis 98.61-98.3-98.3 subject-verb bg-btb 78.86-79.65-72.73
adjective-noun ar-nyuad 99.65-99.64-99.64 subject-verb gl-ctg 84.54-85.5-82.14
adjective-noun grc-proiel 65.23-72.33-64.82 subject-verb cs-pdt 67.13-73.18-63.33
adjective-noun de-hdt 99.47-99.66-99.26 subject-verb fi-tdt 88.11-90.57-88.19
object-verb it-vit 96.28-94.88-84.92 subject-verb pl-pdb 78.19-80.6-72.1
object-verb no-nynorsk 97.73-98.68-95.86 subject-verb la-ittb 80.29-82.69-72.54
object-verb ro-nonstandard 86.05-87.79-65.06 subject-verb zh-gsd 99.78-99.44-97.39
object-verb bg-btb 92.18-92.43-80.66 subject-verb nl-alpino 70.62-72.11-67.12
object-verb gl-ctg 92.71-94.17-82.2 subject-verb mt-mudt 83.91-84.96-72.03
object-verb cs-pdt 82.35-83.91-73.97 subject-verb orv-torot 72.38-66.07-60.46
object-verb fi-tdt 84.21-86.62-77.98 subject-verb he-htb 73.43-70.7-63.44
object-verb pl-pdb 88.89-90.28-81.07 subject-verb pt-gsd 89.4-93.15-87.47
object-verb la-ittb 65.96-65.36-52.63 subject-verb cu-proiel 73.88-76.31-62.48
object-verb zh-gsd 93.4-94.12-87.75 subject-verb be-hse 82.86-83.33-81.11
object-verb nl-alpino 90.32-94.69-47.48 subject-verb sv-lines 80.17-80.72-73.06
object-verb mt-mudt 95.66-94.96-94.96 subject-verb uk-iu 76.89-77.14-74.56
object-verb wo-wtb 91.6-91.81-75.11 subject-verb ga-idt 99.33-99.28-85.25
object-verb orv-torot 76.71-72.56-65.51 subject-verb sk-snk 63.43-73.69-73.69
object-verb he-htb 97.87-98.03-98.03 subject-verb hu-szeged 75.91-74.59-72.43
object-verb pt-gsd 95.17-95.02-88.45 subject-verb got-proiel 67.56-73.2-66.17
Table 7: Accuracy results for all relations across different languages. Baseline is the most frequent order in the
trainingdata.
Type Lang Train-Test-Baseline Type Lang Train-Test-Baseline
subject-verb hr-set 81.87-86.62-77.44 noun-adposition fi-tdt 97.88-98.12-89.47
subject-verb cop-scriptorium 85.92-83.84-76.71 noun-adposition pl-pdb 99.97-99.97-99.83
subject-verb lv-lvtb 76.96-77.98-73.99 noun-adposition nl-alpino 99.28-99.57-99.23
subject-verb et-edt 68.13-71.93-61.02 noun-adposition orv-torot 97.92-97.54-96.83
subject-verb fro-srcmf 79.21-80.69-78.1 noun-adposition he-htb 99.71-99.77-99.55
subject-verb hy-armtdp 81.25-80.25-80.25 noun-adposition cu-proiel 98.06-98.4-98.4
subject-verb en-ewt 98.92-98.81-94.15 noun-adposition sv-lines 98.6-98.11-98.11
subject-verb fr-gsd 96.7-94.21-94.21 noun-adposition uk-iu 99.74-99.8-99.54
subject-verb el-gdt 77.04-77.93-73.56 noun-adposition lzh-kyoto 95.58-96.61-96.61
subject-verb es-gsd 79.15-84.14-71.52 noun-adposition cop-scriptorium 99.92-99.78-99.18
subject-verb tr-imst 91.12-92.96-92.96 noun-adposition lv-lvtb 98.56-97.78-97.78
subject-verb ru-syntagrus 72.33-80.49-72.94 noun-adposition et-edt 98.92-98.77-81.84
subject-verb sl-ssj 70.95-74.66-63.01 noun-adposition fro-srcmf 99.75-99.42-99.42
subject-verb id-gsd 99.09-99.34-99.34 noun-adposition hy-armtdp 97.22-96.83-85.71
subject-verb lt-alksnis 74.44-78.39-75.33 noun-adposition en-ewt 99.67-99.42-99.42
subject-verb ar-nyuad 91.01-91.32-87.82 noun-adposition es-gsd 99.81-100.0-98.83
subject-verb grc-proiel 69.46-72.23-65.71 noun-adposition ru-syntagrus 99.24-99.41-99.13
subject-verb de-hdt 68.1-76.23-61.84 noun-adposition id-gsd 97.67-97.81-96.81
numeral-noun it-vit 73.17-79.32-79.32 noun-adposition ar-nyuad 99.84-99.87-99.48
numeral-noun no-nynorsk 88.49-88.44-88.44 noun-adposition grc-proiel 99.03-98.92-98.92
numeral-noun ro-nonstandard 87.27-84.83-62.07 noun-adposition de-hdt 99.98-99.98-99.37
numeral-noun bg-btb 92.22-88.24-88.24
numeral-noun cs-pdt 84.4-88.65-69.59
numeral-noun fi-tdt 82.35-87.25-68.3
numeral-noun pl-pdb 97.27-97.27-97.27
numeral-noun la-ittb 88.0-87.16-53.21
numeral-noun nl-alpino 95.03-98.7-89.61
numeral-noun mt-mudt 69.77-70.77-70.77
numeral-noun wo-wtb 74.63-82.5-73.75
numeral-noun ja-bccwj 99.05-98.71-98.71
numeral-noun orv-torot 86.64-79.8-72.73
numeral-noun he-htb 85.21-80.0-64.0
numeral-noun pt-gsd 92.18-89.42-73.56
numeral-noun sv-lines 81.3-85.48-85.48
numeral-noun ga-idt 73.2-62.86-57.14
numeral-noun sk-snk 88.01-75.36-43.12
numeral-noun hr-set 95.39-97.28-96.94
numeral-noun et-edt 91.63-91.54-83.65
numeral-noun en-ewt 85.33-89.05-82.09
numeral-noun fr-gsd 79.7-81.88-60.87
numeral-noun el-gdt 88.2-80.6-80.6
numeral-noun es-gsd 87.17-89.43-75.61
numeral-noun ru-syntagrus 93.48-95.01-85.15
numeral-noun sl-ssj 84.08-78.45-78.45
numeral-noun id-gsd 61.04-68.12-53.44
numeral-noun ar-nyuad 88.96-91.79-47.9
numeral-noun grc-proiel 68.76-62.9-62.9
noun-adposition no-nynorsk 99.31-99.26-99.14
noun-adposition gl-ctg 99.33-99.32-99.18
noun-adposition cs-pdt 99.98-99.98-99.94
Table 8: Accuracy results for all relations across different languages. Baseline is the most frequent order in the
trainingdata.
Type Lang Test-Baseline Type Lang Test-Baseline
Gender it-vit 71.01-67.19 Gender hr-set 71.32-72.5
Person it-vit 70.83-62.5 Person hr-set 63.33-76.92
Number it-vit 59.56-71.2 Number hr-set 64.25-67.53
Gender no-nynorsk 70.0-46.43 Gender lv-lvtb 74.48-72.66
Number no-nynorsk 70.0-70.21 Person lv-lvtb 60.98-50.0
Gender ro-nonstandard 61.05-63.95 Number lv-lvtb 72.78-68.83
Person ro-nonstandard 55.22-63.64 Gender hsb-ufal 60.87-85.71
Number ro-nonstandard 62.86-62.63 Number hsb-ufal 46.72-69.23
Gender bg-btb 66.0-63.83 Gender ru-syntagrus 64.96-69.68
Person bg-btb 64.0-62.5 Person ru-syntagrus 64.0-62.5
Number bg-btb 73.17-63.93 Number ru-syntagrus 60.24-59.09
Gender cs-pdt 75.09-56.44 Gender el-gdt 73.58-63.83
Person cs-pdt 57.78-59.09 Person el-gdt 65.0-66.67
Number cs-pdt 63.35-47.66 Number el-gdt 76.54-62.73
Gender pl-pdb 71.11-64.53 Gender hi-hdtb 69.11-58.59
Person pl-pdb 60.71-55.56 Number hi-hdtb 71.77-41.61
Number pl-pdb 66.06-63.68 Gender es-gsd 84.31-71.83
Gender la-ittb 77.78-73.53 Person es-gsd 91.67-59.09
Person la-ittb 19.05-19.05 Number es-gsd 88.89-64.39
Number la-ittb 65.14-57.89 Gender ta-ttb 100.0-68.18
Gender nl-alpino 56.25-66.67 Number ta-ttb 77.78-52.27
Number nl-alpino 60.94-54.84 Person ug-udt 37.93-52.63
Gender orv-torot 64.52-65.54 Number ug-udt 47.73-76.67
Person orv-torot 66.67-60.0 Person fi-tdt 58.06-38.71
Number orv-torot 64.04-62.12 Number fi-tdt 60.0-50.23
Gender he-htb 78.16-74.7 Person wo-wtb 52.17-55.0
Person he-htb 78.95-73.68 Number wo-wtb 57.14-48.57
Number he-htb 58.14-58.54 Person hu-szeged 39.39-44.44
Gender cu-proiel 58.26-61.0 Number hu-szeged 38.34-39.63
Person cu-proiel 61.54-66.67 Person et-edt 68.75-61.29
Number cu-proiel 60.4-67.21 Number et-edt 61.21-64.84
Gender mr-ufal 53.57-60.87 Person hy-armtdp 57.14-44.44
Person mr-ufal 28.57-72.73 Number hy-armtdp 58.49-59.18
Number mr-ufal 66.67-39.39 Person en-ewt 100.0-81.25
Gender be-hse 61.29-59.57 Number en-ewt 69.0-35.71
Number be-hse 65.82-64.62 Person tr-imst 32.69-35.91
Gender sv-lines 65.52-53.85 Number tr-imst 84.62-46.96
Number sv-lines 60.0-64.29 Number kmr-mg 55.56-78.26
Gender uk-iu 68.92-70.08 Number af-afribooms 68.75-60.0
Person uk-iu 72.73-70.0 Number fr-gsd 75.0-62.37
Number uk-iu 65.78-64.67
Gender ga-idt 73.77-64.0
Person ga-idt 42.86-62.5
Number ga-idt 43.16-46.75
Gender sk-snk 71.9-69.16
Person sk-snk 88.89-77.78
Number sk-snk 63.36-55.83
Gender got-proiel 62.02-55.86
Person got-proiel 62.16-57.14
Number got-proiel 67.51-64.0
Table9: Accuracyresultsforallrelationsacrossdifferentlanguages. BaselineisChaudharyetal.(2020)
Type Lang Train-Test-Baseline Type Lang Train-Test-Baseline
PRON no-nynorsk 98.55-99.55-78.28 VERB ug-udt 76.0-75.64-71.37
PRON ug-udt 92.22-94.87-73.68 VERB got-proiel 85.51-86.15-81.15
PRON ro-nonstandard 89.77-91.2-38.33 VERB lv-lvtb 96.43-95.61-75.58
PRON sk-snk 83.19-83.9-34.75 VERB tr-imst 67.53-66.58-46.13
PRON hu-szeged 73.94-79.15-59.46 VERB et-edt 86.95-86.08-82.91
PRON got-proiel 87.97-91.05-36.79 VERB hy-armtdp 86.63-94.34-39.62
PRON hr-set 88.6-89.54-68.79 VERB ur-udtb 96.01-98.95-98.95
PRON lv-lvtb 90.64-90.85-54.03 VERB lt-alksnis 94.86-95.0-52.5
PRON en-ewt 97.74-96.76-81.48 ADP ro-nonstandard 98.5-98.85-98.85
PRON el-gdt 93.5-93.35-36.8 ADP sk-snk 41.74-44.46-40.74
PRON tr-imst 71.0-73.33-42.5 ADP hr-set 45.85-48.42-37.96
PRON sme-giella 85.31-76.82-47.05 ADP hi-hdtb 85.57-86.99-52.34
PRON es-gsd 95.89-96.14-53.71 ADP ur-udtb 82.06-96.59-63.54
PRON da-ddt 84.38-82.53-54.7 ADP uk-iu 45.85-43.39-32.85
PRON et-edt 79.75-81.58-45.26 ADJ ro-nonstandard 98.14-96.9-96.42
PRON af-afribooms 58.86-53.07-31.2 ADJ ga-idt 95.47-93.25-90.18
PRON hy-armtdp 78.1-79.05-63.81 ADJ sk-snk 99.03-98.71-35.01
PRON mr-ufal 71.58-78.95-78.95 ADJ hu-szeged 98.73-98.25-92.58
PRON be-hse 81.3-76.12-65.67 ADJ got-proiel 88.48-92.33-38.36
PRON ur-udtb 87.78-90.53-54.73 ADJ hr-set 97.75-98.3-37.5
PRON lt-alksnis 82.8-80.28-30.28 ADJ lv-lvtb 93.85-94.37-39.59
PRON bg-btb 95.73-95.78-46.78 ADJ et-edt 94.13-95.16-41.07
PRON sv-lines 99.32-99.41-58.02 ADJ el-gdt 85.61-89.49-48.6
PRON uk-iu 88.52-90.98-48.82 ADJ hi-hdtb 84.36-84.34-70.48
NOUN ug-udt 78.31-77.13-63.46 ADJ tr-imst 56.45-60.22-51.88
NOUN ro-nonstandard 96.68-97.53-87.8 ADJ sme-giella 86.09-90.55-90.55
NOUN kmr-mg 53.09-47.21-47.21 ADJ ar-nyuad 94.15-96.94-62.54
NOUN ga-idt 93.26-95.62-80.28 ADJ be-hse 89.59-95.06-43.83
NOUN sk-snk 91.27-92.27-20.61 ADJ ur-udtb 99.04-98.81-62.02
NOUN hu-szeged 72.25-72.1-47.6 ADJ lt-alksnis 96.89-96.72-25.5
NOUN got-proiel 84.89-87.12-27.72 ADJ uk-iu 97.38-98.15-46.39
NOUN hr-set 88.35-92.21-34.41 DET ro-nonstandard 97.01-95.87-75.58
NOUN lzh-kyoto 89.86-93.72-76.61 DET sk-snk 95.7-93.24-43.74
NOUN lv-lvtb 81.94-83.87-31.62 DET got-proiel 94.78-96.25-32.29
NOUN kk-ktb 49.24-53.32-53.32 DET hr-set 94.87-95.64-42.81
NOUN et-edt 62.6-66.51-27.65 DET lv-lvtb 96.59-97.12-30.15
NOUN el-gdt 91.02-94.69-49.72 DET et-edt 96.73-96.19-34.25
NOUN hi-hdtb 96.1-97.35-54.72 DET el-gdt 91.42-93.64-47.48
NOUN tr-imst 59.03-64.52-54.65 DET hi-hdtb 88.89-92.87-76.1
NOUN ta-ttb 77.24-76.49-68.02 DET ur-udtb 95.79-95.91-64.33
NOUN sme-giella 76.51-78.78-30.32 DET lt-alksnis 79.46-83.65-39.92
NOUN ar-nyuad 87.66-94.66-67.49 DET uk-iu 94.31-94.86-27.93
NOUN hsb-ufal 24.07-19.53-19.53 PROPN ro-nonstandard 97.35-96.77-92.98
NOUN hy-armtdp 78.17-80.2-46.08 PROPN ga-idt 79.87-85.78-73.28
NOUN mr-ufal 81.38-75.0-42.65 PROPN sk-snk 90.24-88.9-46.39
NOUN be-hse 69.27-75.95-46.1 PROPN hu-szeged 91.47-89.36-89.36
NOUN ur-udtb 92.1-96.81-51.25 PROPN got-proiel 85.91-86.89-50.91
NOUN lt-alksnis 85.08-82.93-39.07 PROPN hr-set 92.42-94.67-48.27
NOUN sv-lines 99.6-99.86-97.47 PROPN lv-lvtb 88.64-90.13-39.91
NOUN uk-iu 94.1-94.73-43.79 PROPN el-gdt 91.44-90.32-32.58
Table10: Accuracyresultsforallrelationsacrossdifferentlanguages. Baselineisthemostfrequentcasevaluein
thetrainingdata.
Type Lang Train-Test-Baseline Type Lang Train-Test-Baseline
VERB ug-udt 76.0-75.64-71.37 PROPN hi-hdtb 94.91-96.49-48.51
VERB got-proiel 85.51-86.15-81.15 PROPN tr-imst 73.55-71.73-68.0
VERB lv-lvtb 96.43-95.61-75.58 PROPN ta-ttb 97.99-94.84-93.55
VERB tr-imst 67.53-66.58-46.13 PROPN sme-giella 84.23-82.9-35.81
VERB et-edt 86.95-86.08-82.91 PROPN ar-nyuad 78.68-84.27-59.85
VERB hy-armtdp 86.63-94.34-39.62 PROPN et-edt 75.05-83.18-51.24
VERB ur-udtb 96.01-98.95-98.95 PROPN hy-armtdp 82.28-89.13-54.89
VERB lt-alksnis 94.86-95.0-52.5 PROPN be-hse 86.43-72.68-72.68
ADP ro-nonstandard 98.5-98.85-98.85 PROPN ur-udtb 92.7-97.65-59.77
ADP sk-snk 41.74-44.46-40.74 PROPN sv-lines 97.21-96.6-91.23
ADP hr-set 45.85-48.42-37.96 PROPN uk-iu 93.76-95.14-36.14
ADP hi-hdtb 85.57-86.99-52.34 NUM sk-snk 81.47-77.38-39.29
ADP ur-udtb 82.06-96.59-63.54 NUM got-proiel 44.0-45.83-33.33
ADP uk-iu 45.85-43.39-32.85 NUM hr-set 90.27-94.26-41.8
ADJ ro-nonstandard 98.14-96.9-96.42 NUM lv-lvtb 88.07-85.44-38.61
ADJ ga-idt 95.47-93.25-90.18 NUM el-gdt 75.75-73.17-58.54
ADJ sk-snk 99.03-98.71-35.01 NUM tr-imst 76.55-82.22-77.78
ADJ hu-szeged 98.73-98.25-92.58 NUM sme-giella 47.8-41.84-41.84
ADJ got-proiel 88.48-92.33-38.36 NUM et-edt 88.9-93.51-70.3
ADJ hr-set 97.75-98.3-37.5 NUM uk-iu 90.46-92.48-52.29
ADJ lv-lvtb 93.85-94.37-39.59 ADV fa-seraji 85.35-81.36-81.36
ADJ et-edt 94.13-95.16-41.07
ADJ el-gdt 85.61-89.49-48.6
ADJ hi-hdtb 84.36-84.34-70.48
ADJ tr-imst 56.45-60.22-51.88
ADJ sme-giella 86.09-90.55-90.55
ADJ ar-nyuad 94.15-96.94-62.54
ADJ be-hse 89.59-95.06-43.83
ADJ ur-udtb 99.04-98.81-62.02
ADJ lt-alksnis 96.89-96.72-25.5
ADJ uk-iu 97.38-98.15-46.39
DET ro-nonstandard 97.01-95.87-75.58
DET sk-snk 95.7-93.24-43.74
DET got-proiel 94.78-96.25-32.29
DET hr-set 94.87-95.64-42.81
DET lv-lvtb 96.59-97.12-30.15
DET et-edt 96.73-96.19-34.25
DET el-gdt 91.42-93.64-47.48
DET hi-hdtb 88.89-92.87-76.1
DET ur-udtb 95.79-95.91-64.33
DET lt-alksnis 79.46-83.65-39.92
DET uk-iu 94.31-94.86-27.93
PROPN ro-nonstandard 97.35-96.77-92.98
PROPN ga-idt 79.87-85.78-73.28
PROPN sk-snk 90.24-88.9-46.39
PROPN hu-szeged 91.47-89.36-89.36
PROPN got-proiel 85.91-86.89-50.91
PROPN hr-set 92.42-94.67-48.27
PROPN lv-lvtb 88.64-90.13-39.91
PROPN el-gdt 91.44-90.32-32.58
Table11: Accuracyresultsforallrelationsacrossdifferentlanguages. Baselineisthemostfrequentvaluetraining
data.

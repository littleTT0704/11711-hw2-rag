Subword and Crossword Units for CTC Acoustic Models
Thomas Zenkel1, RamonSanabria2, FlorianMetze2 andAlexWaibel1,2
1KarlsruheInstituteofTechnology,Karlsruhe, Germany
2CarnegieMellonUniversity,Pittsburgh,PA, U.S.A.
thomas.zenkel@kit.edu, ramons@cs.cmu.edu, fmetze@cs.cmu.edu, ahw@cs.cmu.edu
Abstract lengthoftheirn-gramunitstoafixedlengthandonlyusese-
lectedsubwordn-grams.
This paper proposes a novel approach to create a unit set for Additionally, none of the mentioned grapheme-based ap-
CTC-basedspeechrecognitionsystems.ByusingByte-PairEn- proaches deal with crossword pronunciation phenomena like
codingwelearnaunitsetofarbitrarysizeonagiventraining contractions and reductions. In conversational English, ‘kind
text. Incontrast tousingcharactersorwordsasunits, thisal- of’and‘goingto’areoftenpronouncedas‘kinda’and‘gonna.’
lowsustofindagoodtrade-offbetweenthesizeofourunitset It may be useful to treat such phenomena as their own units,
andtheavailabletrainingdata. Weinvestigatebothcrossword since the AM could then learn these reduced pronunciations
units,whichmayspanmultiplewords,andsubwordunits. By [11].
evaluatingtheseunitsetswithdecodingmethodsusingasepa- Inthis work wepropose a method tocreate aunit set for
ratelanguagemodel,weareabletoshowimprovementsovera CTC AMs based on the Byte-Pair Encoding (BPE) algorithm
purelycharacter-basedunitset. [12]. This tackles two shortcomings of n-gram units: we do
IndexTerms: automaticspeechrecognition, decoding, neural notrestrictourunitstohaveafixedlength,andweonlycreate
networks unitsthatfrequentlyappearinagiventrainingcorpus. Dueto
theiterativeconstructionoftheunitset,itiseasytoempirically
1. Introduction determinethebesttrade-offbetweenthesizeofourunitsetand
thenumberoftraininglabels. Additionally, wedonotrestrict
Traditional automatic speech recognition (ASR) systems con- ourunitstobepartofawordonly. Inthispaperweinvestigate
sist of three components: a phoneme-based acoustic model theuseof subword andcrossword BPEunitsfor CTCspeech
(AM),aword-basedlanguagemodel(LM),andapronunciation recognition systems and compare them to character and word
lexicon,whichmapsasequenceofphonemestowords[1].This units.WecompareWFSTandRNNdecodingapproaches.
setup works well for systems based on hidden Markov mod- Compared to other approaches to learning the units of an
els(HMMs) aswell asfor ‘end-to-end’ systems[2,3], where ASRsystem,themainadvantageofthismethodisthatitdoes
the AM is trained towards a sequence loss, such as Connec- not rely on phonetics at all, and can be trained on text only,
tionist Temporal Classification (CTC) [4]. An expert-created whichgreatlyspeedsupsystem-buildingefforts. Additionally,
phonedictionarywillcontainaccuratepronunciations, andof- westillkeeptheabilityofrecognizingarbitrarywordsanddo
ten multiple variants for frequent words, which facilitates the notconstrainoursystembyusingafixedvocabulary.
AM’stask.
ACTCmodel’s‘spike-train’ patternmarginalizesoverall 2. RelatedWork
possible alignments of the output symbols, and bi-directional
long-shorttermmemory(LSTM)networkscanlearntemporal Theproblemofselectingaunitsetiscloselyrelatedtofinding
patternswell[5]. CTCmodelsthusperformsurprisinglywell, a decomposition of the target sequence into basic units. The
even when using context-independent characters as the AM’s straightforward variant is to rely on a fixed decomposition of
units, and in languages with ‘irregular’ pronunciations, such thetargetsequence,henceagiventargetsequencewillalways
as English. Because of CTC’s independence assumption, the besplitintothesameunits.
modelcannothoweverexplicitlylearnco-articulationpatterns, Manyapproachesusingfixeddecompositionhavebeenpro-
whichshouldhelpimproveperformance. posed to solvedifferent problems. Anapproach used for lan-
If characters are used as the units of the AM, the LM guage models is tokeep frequent words as units and split the
can directly be implemented as a recurrent neural network remainingunitsintosyllables[13]. Inmachinetranslationsys-
(RNN)[6]. TogetherwithaCTCAM,itallowsthecreationof tems the Byte-Pair Encoding algorithm is used to split infre-
‘all-neural’systems,whicharenotrestrictedtodecodingwithin quent words into subword units [14]. This can improve the
theweightedFiniteStateTransducer(WFST)framework. translationqualityofcompoundsaswellascognatesandloan-
Recent work shows that CTC models can directly pre- words. For speech recognition applications, [11] deals with
dict word units, albeit on an extremely large training cor- crosswordpronunciationphenomena. Byaddingfrequentmul-
pus [7]. [8] circumvent this problem by pre-training the AM tiwordslike‘sortof’and‘kindof’andtheircorrespondingpro-
withphonemesequences.Unfortunately,usingwordsasoutput nunciationstothepronunciationlexicontheywereabletoim-
unitsre-introducesafixedvocabularyandonlyfrequentwords proverecognitionperformance.
willbetrainedrobustly. Instead of using afixed decomposition, it isalso possible
Charactern-gramunitsprovideatrade-offbetweencharac- tolearnavariabledecomposition of thetarget sequence. The
ter and word-based unit sets. When using n-grams, the num- decomposition can for example depend on speaking style for
berofpossibleoutputsincreasesexponentiallywithrespectto speech recognition tasks. Thisapproach was successfully ap-
thelongestn-gramlength. Forthisreason[9,10]constrainthe pliedtoneuralspeechrecognitionsystems[9,10].[10]extends
8102
nuJ
81
]LC.sc[
2v55860.2171:viXra
theCTClossfunctiontolearnthealignmentbetweentheinput pearinthetrainingcorpusarestillsplitintomultiplesubword
andthetargetsequenceaswellasasuitabledecompositionof units.
thetargetsequence. Theunitsetconsistsofcharactern-grams Incontrast,thecrosswordunitsalsoincludeunitsconsisting
uptoafixedlengthandinfrequentn-gramsareomittedinthe ofmultiplewords. Smallsubwordunitsalreadycontainmulti-
unitset. Becauseoftheadditionaltaskoflearningadecompo- words for very frequent expressions. The units can become
sition,moretrainingdataisnecessary. longerforthelargemodelandalsolessfrequentexpressionsare
Another recent trend is to combine word and character- mergedintoasingleunit. Thelengthofboththesubwordand
basedacousticmodels.Word-basedmodelsuseafinitevocabu- thecrosswordsequences issignificantlylowerthanthelength
laryandmodelwordswhichdonotappearinthevocabularyas ofthecharactersequences.
aspecialoutputtoken,normallycalledtheOOVtoken. Instead
ofoutputtingtheOOVtoken,[15,16]outputcharactersfroma Table 1: An example utterance split into characters and pro-
differentacousticcomponentforOOVs. cessed with the subword and crossword BPE algorithm when
creating 300 and 10,000 additional units, respectively. Inthe
3. UnitSelection subword model the character ’@’ denotes that the unit isnot
theendoftheword.
ForcreatingourunitsweuseByte-PairEncoding(BPE)[12].
BPE is a compression algorithm that iteratively replaces the Method Utterance
mostfrequentpairofunits(orbytes)withanunusedunit. Our Original youknowit’snonotevencoldweather
initialunitsetforallourexperimentsconsistsofalphanumerical Character youknowit’snonotevencoldw
characters,tokenstorepresentvariousnoises,andsomespecial eather
characters(-’/&). Witheachsteptheunitsetgrowsbyoneand Subword300 youknowit’snonotevenco@ldw@ea@
thuswecancreateanarbitrarynumberofunits.Forexample,if ther
theunits’AB’and’CDE’arealreadyintheunitsetand’AB’ Subword10k youknowit’snonotevencoldweather
frequently appearsbefore ’CDE’,thenew unit ’ABCDE’will Crossword300 YouKnowIt’sNoNotEvenColdWeather
becreatedandaddedtotheunitset.WeusetheBPEalgorithm Crossword10k YouKnowIt’sNoNotEvenColdWeather
tocreatesubwordunitsaswellascrosswordunits.
Sincewealwayskeepsinglecharactersintheunitset,we
3.1. SubwordUnits
areabletomodel arbitrarywords withbothapproaches. This
Tocreate subword units weuse themethod proposed for ma- enablesourspeechrecognitionsystemtobeopenvocabulary.
chinetranslationin[14].Westartthealgorithmusingourinitial
unitset. Aspecialtoken(inourcase‘@’)isusedtodenoteif 4. Acoustic Model
theunitappearswithinaword.Wedonotuseadedicatedspace
character,asthewordboundariesaredefinedbytheabsenceof TheAMofoursystemiscomposedoffourbidirectionalLSTM
thetoken‘@’withinaunit.Crosswordboundariesarenotcon- layers[18]with320unitsineachdirectionfollowedbyasoft-
sideredinthiscase;weonlycounttheco-occurrencesofunits maxlayer.Thesizeofthesoftmaxlayerdependsontheunitset
withinaword. SinceBPEcreatesanewunitateachiteration, weuse. Wejointlytrainthewholemodel undertheCTCloss
veryfrequentwordsareeventuallymergedintoasingleunit. function[4].
To train the AM we use the 300h Switchboard data set
3.2. CrosswordUnits (LDC97S62). We perform data augmentation as in [19], cre-
ating3subsampleswithareducedframerate(i.e.from10msto
Tocreatecrosswordunitsweslightlychangethealgorithmused 30ms)fromeachoriginalsample. Thecodeisopen-sourcedin
forthesubwordunits. Inplaceofanendofwordsymbol, we EESEN[20];thetf cleanbranchwasusedtotrainourmod-
markthebeginningofeachwordwithacapitalletter.Thisidea els. To improve training stability, we pre-trained each model
isinspiredbytheunitsetusedforthespeechrecognitionsys- using only character labels until itsconvergence. Afterwards,
temin[17]. Forexample,theutterance‘idon’tknow’willbe wetrainwithoneoftheunitsetsdescribedinsection3.Weop-
preprocessedto‘IDon’tKnow’.Noticethatwedonotuseaded- timizetheparametersofthenetworkusingstochasticgradient
icatedspacecharacter,sincethewordboundariesaremodeled descent. Wehalvethelearningrateaftereachepochwherethe
usingcapitalletters.WenowapplyBPEandalsomergeacross validationaccuracydoesnotimprove.
wordboundaries. Wearguethatthismethodissuperiortodi-
rectlyapplyingBPEtoutterancescontainingspaces.Whendo- 5. Decoding Strategies
ingthis,itispossiblethattheunitsetwillnowinclude‘know’,
‘ know’,‘know ’aswellas‘ know ’(forvisibilityspacesare In this section we briefly summarize the different approaches
replacedby’ ’). togenerate atranscription given thestaticsequence of proba-
bilitiesgenerated by the acoustic model. For a more detailed
3.3. Comparison descriptionwereferreadersto[19].
WecomparethedecompositionofagiventargetsequenceinTa-
5.1. GreedyDecoding
ble1.Weshowanexampleofbothsmallandlargeunitsetsfor
eachapproach,subwordandcrossword.Thesmallsetiscreated ToestimatethequalityoftheAMweperformagreedysearch
byapplying300mergeoperations,whilethelargesetiscreated withoutaddinganylinguisticknowledgebyselectingthemost
byapplying10,000mergeoperations.Forsmallsubwordunits, probable unit at each frame [4]. By removing repeated units
allwordsexceptveryfrequentlyappearingwordsaresplitinto andtheblanktoken,weareabletocreateastringofunits. For
subword units. Large subword models almost resemble word thesubwordunits,weinsertaspacetoseperatewordsbetween
units;however,veryinfrequentwordsorwordsthatdidnotap- eachdecodedunit(e.g.‘ohye@ah’)andremovethesequence
‘@ ’ to get a sequence of words (e.g. ‘oh yeah’). In the
SubNoLM
crosswordcase,wesimplyconcatenatethedecodedunits(e.g.
SubRNNLM
‘OhYeah’)andreplacealluppercasecharacterwiththeirlower-
casecounterpartandaspace(e.g.‘ohyeah’). 25 SubWFST
CrossNoLM
5.2. WeightedFiniteStateTransducer CrossRNNLM
To improve over simple greedy search, the Weighted Finite
StateTransducer(WFST)approachaddslinguisticinformation
20
atthewordlevel[20].
The search graph of the WFST is composed of a token
WFST,whichappliestheCTCsquashfunction(i.e. removing
repeatedcharactersandtheblanktoken),alexiconWFSTthat
mapsasequenceofunitstowords,andagrammarWFSTthat
ismodeledbyawordbasedn-gramlanguagemodel. 15
The search graph is used to find the most probable word
103 104
sequence. We use a trigram and a 4-gram LM smoothed
withKneser Neydiscounting. WetraintheLMswithcleaned BPEOperations
SwitchboardandFishertranscripts. WedonotuseWFSTde-
codingforcrosswordunits.However,thiswouldbepossibleby Figure1:ComparisonofSubword(Sub)andCrossword(Cross)
introducingmulti-wordsintothelexicon. unit sets using no LM, a word-based LM (WFST), and an
RNNLM. We report the Word Error Rate on the Switchboard
5.3. BeamSearchwithaRNNLanguageModel subsetofEval2000.
IncontrasttotheWFSTdecoding,wecombinetheinformation
of the AM and the RNNLM directly at each frame. This is
possible because we trainthe LM on the same unit set asthe sider the word ‘kinds’: in the subword 10k model, this unit
AM. We additionally add symbols denoting the start and the has539trainingexamples,whileforthecrossword10kmodel,
endofasequencetotheLMunitset.Tofindthemostprobable thesearescatteredbetweenmultipleunits(“AllKindsOf”(203),
outputsequence,weapplybeamsearchsimilarlyto[21]. Fora “KindsOf” (158), “KindsOfThings” (75) etc). Because of a
mathematicalformulationofthesearch,wereferto[19]. lowernumberoftrainingexamples,wearguethatthecrossword
Weuseatwo-layerLSTMnetworkwith1024hiddenunits modelisnotabletolearnrobustrepresentationsfortheseunits.
at each layer and a 256-dimensional embedding layer as our Thisisalsosupportedbythefactthatlargecrosswordmodels
LM. We use the same cleaned Switchboard and Fisher tran- outputblanksinplaceofareasonableunitinmanysituations.
scriptsasourtrainingcorpusfortheRNNLM.Weoptimizethe Thedeletionrateconsistentlyincreaseswiththesizeoftheunit
networkwithadam[22]. Wehalvethelearningrateandrestart set,reaching14.3%forthecrossword10kmodel.
adamwheneverourvalidationcostdoesnotdecrease[23]. Adding linguistic knowledge during decoding always im-
proves our results. For the large crossword models, the
6. Results RNNLMisnotabletofixthediscusseddrawbacks;evenwitha
tunedinsertionbonus,thedeletionrateremainshigh(Table2).
The 2000 HUB5 ‘Eval2000’ (LDC2002S09) set is used for Weachievethebestresultswhenusingmodelswithsmallunit
evaluation. ThecorpusconsistsofEnglishtelephoneconversa- sets. Wearguethatbyusingsmallerunitswecanincludethe
tionsandisdividedintothe‘Switchboard’subset,whichmore linguisticknowledge earlier in thesearch process. Wedo not
closelyresemblesthetrainingdata,andthe‘Callhome’subset. havetowaituntiltheAMhasrecognized aword, butinmost
Weevaluateboththecrosswordandsubwordmodelswithboth cases can combine the information from the AM and the LM
beam search using an RNNLM and greedy decoding, which alreadyatthesubwordlevel.
doesnotuseadditionallinguisticknowledgefromtheLM.For
subwordmodelswealsoapplyWFSTdecoding. Foreachap- Table2: Substitution(S),Insertion(I),Deletion(D)andWord
proach,weevaluatewith300,600,1k,3k,6kand10kunits. ErrorRates(WER)fordifferentunitsetsusinganRNNLMdur-
ingdecodingontheSwitchboardsubsetofEval2000.
6.1. EvaluationofSubwordandCrosswordUnits
Method S D I WER
Figure1summarizestheresults.Forthesmallestunitset(300),
Subword300 8.8% 3.5% 2.4% 14.7%
the crossword model outperforms the subword model. While
Subword10k 8.0% 6.0% 2.1% 16.1%
the word error rate (WER) of the subword model constantly
Crossword300 8.8% 4.0% 2.1% 14.9%
decreases, the WER of the crossword model increases. One
Crossword10k 9.9% 12.2% 3.2% 25.3%
problemofthecrosswordmodelisthatthelengthoftheunits
constantlyincreases(e.g. “You’reNotGoingTo”), whileforthe
subwordmodelunitscannotbelongerthanasingleword. We Wealsofoundthat thetrainingtimeofeachmodel varies
argue that it is more difficult for the AM to recognize long accordingtothenumberofunitsused. Forinstance,atraining
expressions. The other drawback of a bigger unit set is that epochofthesubwordAMwith300outputunits,4layers,and
wehavefewertrainingexamplesperunit. Thisholdsforboth 320cellstakes42minutes,andthesamemodelwith10koutput
approaches; however, withthesubwordmodelfrequentwords unitstakes166minutesusinganNvidiaGeForceGTX1080Ti.
willstillhaveahighernumber oftrainingexamples, aswords Thisismostlikelyduetothefinalprojectionlayerwhichmaps
cannot be represented as longer units. As an example, con- thehiddenunitstowardsadesirednumberofunits.
REW
acterunitset,andreportslightimprovementscomparedtothe
SubNoLM
character-basedmodelof[17].
30 SubRNNLM
Whileattention-basedmethodsonlyshowmodestimprove-
CrossNoLM
mentswhenusingbiggerunitsets[9,24],selectinganappropri-
CrossRNNLM
ateunitsetforCTCsystemsseemstobemoreimportant. This
isalsoinlinewiththeresultsfrom[25],whoreportsignificant
20
gainswhenswitchingfromcharacterstowordsasoutputunits.
WearguethatitismoredifficultforCTCmodelstolearnanim-
plicitLM,whichmakesithardtoproducethecorrectspelling
ofwordswhenonlycharactersareusedasoutputunits.
10
Table 3: Comparison of our results to related work on
grapheme-based CTC ASR systems using no LM at all (‘No
LM’) and using a LM during decoding (‘LM’). We report the
0 WERontheSwitchboard (SW)andCallhome(CH)subsetsof
103 104
Eval2000. [19]representsthecharacterbaselineforourBPE
BPEOperations experiments[ours].
Figure 2: Number of words which were correctly recognized Category Unitset SW CH
during greedy and RNNLMdecoding (y-axis), but didnot ap- NoLM[19] Character 30.4% 44.0%
pearintheacoustictrainingcorpus. NoLM[ours] Subword10k 17.8% 29.0%
NoLM[16] Words&Characters 14.4% 24.0%
LM[19] Character 17.0% 30.2%
6.2. Recognitionofunseenwords LM[ours] Subword300 14.7% 26.2%
LM[17] Characters 15.1% 26.3%
WhiletheRNNLMisslightlyinferiortoWFSTdecoding,itis
stillabletooutputarbitrarywordsandisnotrestrictedbyafixed
vocabulary.InFigure2,weanalyzethenumberofwordswhich AnotheradvantageofBPE-basedapproachestocreatethe
didnotappearinthetrainingsetoftheacousticmodelbutwere unitsisthatwecancreateaunitsetofanarbitrarysize. Thus,
nonethelesscorrectlyrecognizedinthetestset.Wereportthese wecanadaptthesizeofourunitsettothesizeofthetraining
numbersforgreedydecoding,whichmeanswithoutaddingany data. Furthermore, we can easily create a number of diverse
LMinformation. Wenoticethatweareabletorecognizemore models which are encouraged by their distinctive unit sets to
previouslyunseenwordswhenusingasmallerunitset.Withan learndifferentconcepts. Wecombinedthecrosswordandsub-
increasingunitsetsize,fewerunseenwordsarerecognizedcor- wordsystemsdecodedwiththeRNNLMwithROVER[26]us-
rectly.ThissituationalsoholdsfordecodingwiththeRNNLM. ingmajorityvotingwithoutanyconfidencescores.Thisyieldsa
WhenusingthesameunitsetforboththeAMandtheLM,we worderrorrateof11.2%ontheSwitchboardtestsubset,which
arestillabletooutputarbitrarywords. representsa3.5%improvementcomparedtoourbestsinglesys-
Most new words we are now able to recognize consist of tem.
previously seen words withdifferent prefixesor suffixes. Ex-
amplesforthesubword300unitsetinclude‘interactions’,‘un- 7. Conclusions
official’,‘humiliated’,‘decency’and‘clunker’.Wearguethata
smallunitsetallowsustolearnprefixesandsuffixesmoreeas- In this paper we discussed two different methods to create a
ily. Thisdoesnotholdforthebiggerunitsets,becausemostof unitsetforCTC-basedspeechrecognitionsystems.Ourmethod
thetokensseenduringtrainingarecompletewords(97%when createsunitsetsofanydesiredsize,thusprovidingamethodto
usingthesubword 10kmodel). Whenusingsmallerunitsets, convenientlyadjustthesizeoftheunitsettotheamountofthe
wordsaremoreoftensplitintosubwordtokens. availabletrainingdata.
Webelievethatthisworkshowsthatthereisstillroomfor
6.3. Comparisontopreviouswork
improvement in automatically selecting a unit set for a given
dataset.Wewillcontinuetoimprovetheautomaticselectionof
We compare our results to previous work using the 300h
units, especially to remedy the drawbacks of crossword mod-
SwitchboardtrainingsetinTable3. Wefocusthecomparison
els.Usingmoreknowledgeabouttheinputsequencetodirectly
onCTCmodelswithgrapheme-based unitsets. Fordecoding
benefitfromthepronunciationmightbeagoodstartingpoint.
strategieswithoutusinglinguisticinformation,wereportgains
comparedtoourpreviouscharacterbasedsystemtrainedonthe WearguethattheperformanceofmethodslikeGram-CTC
same architecture. Highly tuned models which mainly focus andLatentSequenceDecompositioncouldimprovewhenusing
onwordsastheiroutputunit,suchas[16],yieldbetterresults. aBPEunitsets,butalsomethodsrelyingonlesstrainingdata
However, word-based models do not profit as much from us- usagemightbebeneficialforsystemsfocusingonavarietyof
inganLMduringdecoding. UsingLMinformationduringde- low-resourcelanguages. Also,insituationswheretheacoustic
codingcanbeanadvantageifforexampletheacoustictraining training datadoes not match thetest domain, one might want
datadoesnotmatchthetestdomain.AnotheradvantageofBPE torelymoreontheinformationfromthelanguagemodel. Our
unitsisthattheyareabletorecognize arbitrarywords. When resultssuggestthatthisispossiblebyspecifyingaunitsetofa
including LM information during decoding, we still improve smallersize. Inthefuture, wewillinvestigatetheuseofBPE
by more than 2% compared to thesame system using a char- unitsinmorediversetrainingandtestingscenarios.
8. References
[20] Y.Miao,M.Gowayyed,andF.Metze,“Eesen:End-to-endspeech
recognitionusingdeeprnnmodelsandwfst-baseddecoding,”in
[1] L.R.Rabiner,“Atutorialonhiddenmarkovmodelsandselected
AutomaticSpeechRecognitionandUnderstanding(ASRU),2015
applications in speech recognition,” Proceedings of the IEEE,
IEEEWorkshopon. IEEE,2015,pp.167–174.
vol.77,no.2,pp.257–286,1989.
[21] K. Hwang and W. Sung, “Character-leel incremental speech
[2] A.GravesandN.Jaitly,“Towardsend-to-endspeechrecognition
recognitionwithrecurrentneuralnetworks,”inAcoustics,Speech
withrecurrentneuralnetworks,”inProceedingsofthe31stInter-
andSignalProcessing(ICASSP),2016IEEEInternationalCon-
nationalConferenceonMachineLearning(ICML-14),2014,pp.
ferenceon. IEEE,2016,pp.5335–5339.
1764–1772.
[22] D.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimiza-
[3] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend
tion,”arXivpreprintarXiv:1412.6980,2014.
andspell: Aneuralnetworkforlargevocabularyconversational
speechrecognition,”inAcoustics,SpeechandSignalProcessing [23] M. Denkowski and G. Neubig, “Stronger baselines for
(ICASSP),2016IEEEInternationalConferenceon. IEEE,2016, trustable results in neural machine translation,” arXiv preprint
pp.4960–4964. arXiv:1706.09733,2017.
[4] A.Graves,S.Ferna´ndez, F.Gomez,andJ.Schmidhuber,“Con- [24] C.-C. Chiu, T.N.Sainath, Y. Wu, R. Prabhavalkar, P.Nguyen,
nectionist temporal classification: labelling unsegmented se- Z.Chen,A.Kannan,R.J.Weiss,K.Rao,K.Goninaetal.,“State-
quence data with recurrent neural networks,” in Proceedings of of-the-artspeechrecognitionwithsequence-to-sequencemodels,”
the23rdinternationalconferenceonMachinelearning. ACM, arXivpreprintarXiv:1712.01769,2017.
2006,pp.369–376. [25] J.Li,G.Ye,R.Zhao,J.Droppo,andY.Gong,“Acoustic-to-word
[5] A. Graves, S. Ferna´ndez, and J. Schmidhuber, “Bidirectional modelwithoutoov,”arXivpreprintarXiv:1711.10136,2017.
lstm networks for improved phoneme classification and recog- [26] J.G.Fiscus,“Apost-processingsystemtoyieldreducedworder-
nition,” Artificial Neural Networks: Formal Models and Their rorrates:Recognizeroutputvotingerrorreduction(rover),”inAu-
Applications–ICANN2005,pp.753–753,2005. tomaticSpeechRecognitionandUnderstanding, 1997.Proceed-
[6] A. L. Maas, Z. Xie, D. Jurafsky, and A. Y. Ng, “Lexicon-free ings.,1997IEEEWorkshopon. IEEE,1997,pp.347–354.
conversationalspeechrecognitionwithneuralnetworks.”inHLT-
NAACL,2015,pp.345–354.
[7] H. Soltau, H. Liao, and H. Sak, “Neural speech recognizer:
Acoustic-to-wordlstmmodelforlargevocabularyspeechrecog-
nition,”arXivpreprintarXiv:1610.09975,2016.
[8] K.Audhkhasi,B.Ramabhadran,G.Saon,M.Picheny,andD.Na-
hamoo, “Direct acoustics-to-word models for english conver-
sational speech recognition,” arXiv preprint arXiv:1703.07754,
2017.
[9] W.Chan,Y.Zhang,Q.Le,andN.Jaitly,“Latentsequencedecom-
positions,”arXivpreprintarXiv:1610.03035,2016.
[10] H.Liu,Z.Zhu,X.Li,andS.Satheesh,“Gram-ctc:Automaticunit
selectionandtargetdecompositionforsequencelabelling,”arXiv
preprintarXiv:1703.00096,2017.
[11] M.FinkeandA.Waibel,“Speakingmodedependentpronuncia-
tionmodelinginlargevocabularyconversationalspeechrecogni-
tion.”inEUROSPEECH,1997.
[12] P.Gage, “Anewalgorithm fordatacompression,” TheCUsers
Journal,vol.12,no.2,pp.23–38,1994.
[13] T.Mikolov,I.Sutskever,A.Deoras,H.-S.Le,S.Kombrink,and
J.Cernocky,“Subwordlanguagemodelingwithneuralnetworks,”
2012.
[14] R. Sennrich, B. Haddow, and A. Birch, “Neural machine
translation of rare words with subword units,” arXiv preprint
arXiv:1508.07909,2015.
[15] J.Li,G.Ye,A.Das,R.Zhao,andY.Gong,“Advancingacoustic-
to-wordctcmodel,”arXivpreprintarXiv:1803.05566,2018.
[16] K. Audhkhasi, B. Kingsbury, B. Ramabhadran, G. Saon, and
M.Picheny,“Buildingcompetitivedirectacoustics-to-wordmod-
elsforenglishconversationalspeechrecognition,”arXivpreprint
arXiv:1712.03133,2017.
[17] G. Zweig, C. Yu, J. Droppo, and A. Stolcke, “Advances in
all-neuralspeechrecognition,”arXivpreprintarXiv:1609.05935,
2016.
[18] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neuralcomputation,vol.9,no.8,pp.1735–1780,1997.
[19] T. Zenkel, R. Sanabria, F. Metze, J. Niehues, M. Sperber,
S.Stu¨ker,andA.Waibel,“Comparisonofdecodingstrategiesfor
ctcacousticmodels,”inProceedingsofthe17thAnnualConfer-
enceoftheInternationalSpeechCommunicationAssociation,In-
terspeech2017. InternationalSpeechCommunicationAssocia-
tion,2017.

Zero-Shot Dialogue Disentanglement
by
Self-Supervised Entangled Response Selection
Ta-ChungChi AlexanderI.Rudnicky
LanguageTechnologiesInstitute LanguageTechnologiesInstitute
CarnegieMellonUniversity CarnegieMellonUniversity
tachungc@andrew.cmu.edu air@cs.cmu.edu
Abstract
Entangled self-supervised Selection Loss
Dialogue training Attention Loss
Shared
Dialoguedisentanglementaimstogrouputter-
Model
New Entangled zero shot Dialogue
ancesinalongandmulti-participantdialogue
Dialogue adaption Disentanglement
intothreads. Thisisusefulfordiscourseanal-
ysisanddownstreamapplicationssuchasdia-
Figure 1: This is the high-level flow of our proposed
logue response selection, where it can be the
approach.
firststeptoconstructacleancontext/response
set. Unfortunately, labeling all reply-to links
takes quadratic effort w.r.t the number of ut-
Trainingdataforthedialoguedisentanglement
terances: an annotator must check all preced-
task is difficult to acquire due to the need for
ingutterancestoidentifytheonetowhichthe
manual annotation. Typically, the data is anno-
current utterance is a reply. In this paper, we
are the first to propose a zero-shot dialogue tated in the reply-to links format, i.e. every utter-
disentanglement solution. Firstly, we train a ance is linked to one preceding utterance. The
model on a multi-participant response selec- effort is quadratic w.r.t the length of dialogue,
tion dataset harvested from the web which
partly explaining the sole existence of human-
is not annotated; we then apply the trained
annotated large-scale dataset (Kummerfeld et al.,
modeltoperformzero-shotdialoguedisentan-
2018),whichwasconstructedbasedontheUbuntu
glement. Withoutanylabeleddata,ourmodel
IRC forum. To circumvent the need for expen-
canachieveaclusterF1scoreof25. Wealso
fine-tune the model using various amounts of sivelabeleddata,weaimtotrainaself-supervised
labeleddata. Experimentsshowthatwithonly modelfirstthenusethemodeltoperformzero-shot
10% of the data, we achieve nearly the same dialoguedisentanglement. Inotherwords,ourgoal
performanceofusingthefulldataset1.
istofindataskthatcanlearnimplicitreply-tolinks
withoutlabeleddata.
1 Introduction
Entangledresponseselection(Gunasekaraetal.,
2020)isthetaskthatwewillfocuson. Itissimilar
Multi-participant chat platforms such as Messen-
to the traditional response selection task, whose
ger and WhatsApp are common on the Internet.
goalistopickthecorrectnextresponseamongcan-
Whilebeingeasytocommunicatewithothers,mes-
didates,withthedifferencethatitsdialoguecontext
sagesoftenfloodintoasinglechannel,entangling
consistsofmultipletopicsandparticipants,leading
chathistorywhichispoorlyorganizedanddifficult
toamuchlongercontext(avg. 55utterances). We
to structure. In contrast, Slack provides a thread-
hypothesizethat:
openingfeaturethatallowsuserstomanuallyorga-
nizetheirdiscussions. Itwouldbeidealifwecould
A well-performing model of entangled
designanalgorithmtoautomaticallyorganizean
response selection requires recovery of
entangledconversationintoitsconstituentthreads.
reply-tolinkstoprecedingdialogue.
Thisisreferredtoasthetaskofdialoguedisentan-
glement (Shen et al., 2006; Elsner and Charniak, This is the only way that a model can pick the
2008;WangandOard,2009;ElsnerandCharniak, correctnextresponsegivenanentangledcontext.
2011;Jiangetal.,2018;Kummerfeldetal.,2018; Twochallengesareaheadofus:
Zhuetal.,2020;Lietal.,2020;YuandJoty,2020).
com/chijames/zero_shot_dialogue_
1Code is released at https://github. disentanglement
• Choosingadesignforsuchamodel. Previous Model R@1 R@5 R@10 MRR
workreliesonheuristicstofilteroututterances Concatenate 51.6 72.3 80.1 61.2
tocondensecontext. Amodelshouldnotrely +aug 64.3 82.9 88.4 72.8
onheuristics. See§2.3and2.5. Hierarchical 50.0 72.1 81.4 60.3
+aug 65.7 84.8 91.8 74.3
• Eventhoughwecantrainawell-performing
model,howshouldwerevealthelinkslearned Table1:Testsetperformanceoftheentangledresponse
implicitly? See§3.4. selectiontask. Concatenateisthemodelwithcomplex
pruningheuristicsdescribedinWuetal.(2020).
Finally,wewanttohighlightthehighpractical
value of our proposed method. Consider that we
orreferredtobythecandidates. Thisisproblematic
haveaccesstoalargeandunlabeledcorpusofchat
for two reasons. 1) The retained context is still
(e.g. WhatsApp/Messenger)history. Theonlycost
noisyastherearemultiplespeakerspresentinthe
shouldbetrainingtheproposedentangledresponse
candidates. 2) We might accidentally prune out
selection model with attention supervision using
relevantutteranceseventhoughtheydonotshare
unlabeleddata. Thetrainedmodelisimmediately
the same speakers. A better solution is to let the
readyfordialoguedisentanglement. Insummary,
modeldecidewhichutterancesshouldberetained.
thecontributionsofthisworkare:
2.3 Model(SolidArrowsinFigure2)
• Showthatcomplexpruningstrategiesarenot
Weuseahierarchicalencoderasshowninthemid-
necessaryforentangledresponseselection.
dle part of Figure 2. Suppose the input context
• With the proposed objective, the model is {U }n and the next response candidate set is
i i=1
trained on entangled response selection can {C }m . For every candidate utterance C , we
k k=1 k
performzero-shotdialoguedisentanglement. concatenateitwithallU s. Forexample,weform
i
n pairs for k = 1, (U + C )n . Then we use
i 1 i=1
• Bytuningwith10%ofthelabeleddata, our
BERTastheencoder(ϕ)toencodepairsandget
model achieves comparable performance to
thelastlayerembeddingofthe[CLS]tokenasV :
i
thattrainedusingthefulldataset.
V = ϕ(U +C )n , ∀i ∈ 1...n (1)
i i 1 i=1
2 EntangledResponseSelection
V = ϕ(C +C ) (2)
n+1 k k
2.1 TaskDescription
While (C + C ) is not necessary for response
k k
The dataset we use is DSTC8 subtask-2 (Gu- selection,itisusefullaterforpredictingself-link,
nasekara et al., 2020), which was constructed by which acts as the first utterance of a thread. We
crawlingtheUbuntuIRCforum. Concretely,given will see its role in §3.4. Then we use the output
an entangled dialogue context, the model is ex- embeddingsofaonelayertransformer(ψ)with8
pected to pick the next response among 100 can- headstoencodecontextualizedrepresentations:
didates. Theaveragecontextlengthis55andthe
{V(cid:48)}n+1 = ψ({V }n+1) (3)
numberofspeakersis20withmultiple(possibly i i=1 i i=1
relevant) topics discussed concurrently. The con- Todeterminerelativeimportance,weuseanatten-
textistoolongtobeencodedbytransformer-based tionmodule(A)tocalculateattentionscores:
models(Devlinetal.,2018;Liuetal.,2019). De-
v = MLP(V(cid:48)), ∀i ∈ 1...n+1 (4)
spitetheexistenceofmodelscapableofhandling i i
longcontext(Yangetal.,2019;Zaheeretal.,2020; {α }n+1 = softmax({v }n+1) (5)
i i=1 i i=1
Beltagy et al., 2020), it is difficult to reveal the
Thefinalpredictedscoreis:
implicitlylearnedreply-tolinksasdonein§3.4.
n+1
(cid:88)
2.2 RelatedWork s = MLP( α V(cid:48)) (6)
i i
Tothebestofourknowledge,previousworksadopt i=1
complex heuristics to prune out utterances in the Note that s should be 1 for C (the correct next
1
long context (Wu et al., 2020; Wang et al., 2020; response),andotherwise0(row1ofthemulti-task
Guetal.,2020;Berteroetal.,2020). Forexample, losstableinFigure2). Thiscanbeoptimizedusing
keepingtheutteranceswhosespeakeristhesameas thebinarycross-entropyloss.
response sel. shared model disentanglement
! !
" (! +' ), (! +' ), (! +' ), (' +' ) "
" ( # ( $ ( ( (
! # ! # ü
;=1,2 ;=1
! $ ! $
' ü '
(:" (:"
* =- ! +' , * =- ! +' , * =- ! +' , * =-(' +' )
' + " ( / # ( 0 $ ( 1 ( (
(:# zero shot
2(* ,…,* )
multi-task loss + 1 0
*4 *4 *4 *4 1
k=1 k=2 + / 0 1
0
> 1 0 >=MLP(Σ7 C* 64) 5(* 64)
arg max
7 8 0 1 7 " 7 # 7 $ 7 8
Figure 2: Solid arrows: Given an entangled context U and C as the correct next response (C is a
1,2,3 k=1 k=2
negativesample), eachpairoftheconcatenatedinputsisencodedseparatelybyϕ(BERT)togetV . Acontext-
i
aware model ψ (transformer) is applied over V s to generate contextualized V(cid:48). An attention module A is used
i i
tocalculatetheattentionscoresα andweightedsums. Modelisoptimizedaccordingtothetargetvaluesfors
i
andα inthemulti-tasklosstable. Dashedarrows: Givenanotherentangledcontext, weknowthatthecurrent
4
utteranceC isreplyingtoU bytakingtheargmaxofattentionscoresα inazero-shotmanner.
k=1 2 i
2.4 Results to quantify sharpness. Numerically, the entropy
is1.4(sharp)whenC iscorrectand2.1(flat)for
WeshowtheresultsinTable1. Theperformance k
incorrectones,validatingoursuppositions.
of our approach is comparable to previous work.
Isitpossibletorevealtheseimplicitlinks? The
Notethatourmodeldoesnotuseanyheuristicsto
solutionisinspiredbythelabeleddataofdialogue
pruneoututterances. Instead,theattentionscores
disentanglementaselaboratedin§3.4.
α aredecidedentirelybythemodel. Wealsorun
i
anexperimentusingaugmenteddatafollowingWu
3 ZeroshotDialoguedisentanglement
et al. (2020), which is constructed by excerpting
partialcontextfromtheoriginalcontext2. Finally,
3.1 TaskDescription
wewanttohighlighttheimportanceoftheattention
ThedatasetusedisDSTC8subtask-4(Kummerfeld
module A, where the performance drops by 10
etal.,2018)3. Wewanttofindtheparentutterance
pointsifremoved.
inanentangledcontexttowhichthecurrentutter-
2.5 AttentionAnalysis anceisreplying,andrepeatthisprocessforevery
utterance. Afterallthelinksarepredicted,weruna
Theempiricalsuccessofthehierarchicalencoder
connectedcomponentalgorithmoverthem,where
hasanimportantimplication: itisabletolinkthe
eachconnectedcomponentisonethread.
candidatewithoneormultiplerelevantutterances
inthecontext. Thiscanbeprovedbytheattention
3.2 RelatedWork
distributionα . Intuitively,ifC isthecorrectnext
i k
response(i.e. k = 1),thentheattentiondistribution All previous work (Shen et al., 2006; Elsner and
shouldbesharp,whichindicatesanimplicitreply- Charniak,2008;WangandOard,2009;Elsnerand
to that links to one of the previous utterances. In Charniak, 2011; Jiang et al., 2018; Kummerfeld
contrast,ifC isincorrect(i.e. k (cid:54)= 1),ourmodelis etal.,2018;Zhuetal.,2020;Lietal.,2020;Yuand
k
lesslikelytofindanimplicitlink,andtheattention Joty,2020)treatthetaskasasequenceofmultiple-
distributionshouldbeflat. Entropyisagoodtool choiceproblems. Eachofthemconsistsofasliding
windowofnutterances. Thetaskistolinkthelast
2For a context of length 50, we can take the first i ∈
1...49utterancesasanewcontextandthei+1-thutterance 3ItisadisjointsplitoftheUbuntuIRCchannelasopposed
acts as the new correct next response. We can sample the totheoneusedin§2.1,hencethereisnoriskofinformation
negativesrandomlyfromothersessionsinthedataset. leak.
CLUSTER LINK
Setting w data%
VI ARI P R F1 P R F1
1)zeroshot
0.00 0.0 62.9 14.7 2.9 0.3 0.5 41.2 39.7 40.5
0.25 0.0 84.4 50.1 25.9 24.8 25.3 43.7 41.4 42.2
0.50 0.0 84.6 51.5 24.6 23.8 24.2 41.5 40.0 40.8
0.75 0.0 84.6 49.2 23.3 23.1 23.2 41.8 40.3 41.1
1.00 0.0 84.3 47.5 22.9 23.0 23.0 41.6 40.1 40.9
2)fewshot
finetune 0.25 1 89.7 60.2 26.1 33.9 29.5 65.0 62.7 63.8
scratch - - 88.7 58.7 22.6 28.6 25.2 63.7 61.4 62.6
finetune 0.25 10 90.6 59.8 32.4 38.4 35.1 70.5 68.0 69.3
scratch - - 90.4 61.0 32.4 36.5 34.3 70.4 67.9 69.1
finetune 0.25 100 91.1 62.7 35.3 42.0 38.3 74.2 71.6 72.9
scratch - - 91.2 62.1 35.6 40.3 37.8 74.0 71.3 72.6
Table 2: w = 0 indicates pure entangled response selection training. In the few-shot section, scratch is the dis-
entanglementmodelnottrainedself-supervisedlyonentangledresponseselectionbefore. Theevaluationmetrics
andlabeleddatausedforfine-tuningareinKummerfeldetal.(2018). Resultsaretheaverageofthreeruns.
.
utterancetooneoftheprecedingn−1utterances. disentanglementdatainthetrainingprocess.
Thismodelisusuallytrainedinsupervisedmode
using the labeled reply-to links. Our model also
40
followsthesameformulation.
30
3.3 Model(DashedArrowsinFigure2)
Weusethetrainedhierarchicalmodelin§2.3with- 20 finetune
out the final MLP layer used for scoring. In ad- scratch
10
dition,weonlyhaveonecandidatenow,whichis
the last utterance in a dialogue. We use C to 0
k=1
representitforconsistency. Notethatweonlyneed 0 20 40 60 80 100
tocalculatei(cid:48) = argmax α . Thisindicatesthat Training Data %
i i
C isreplyingtoutteranceU inthecontext.
k=1 i(cid:48)
Figure 3: Different amounts of labeled data for fine-
tuning. Themodelwithself-supervisedresponseselec-
3.4 ProposedAttentionSupervision
tiontrainingoutperformstheonetrainedfromscratch.
We note that the labeled reply-to links act as su-
pervisiontotheattentionα : theyindicatewhich
i
3.5 Results
α shouldbe1. Wecallthisextrinsicsupervision.
i
Recalltheimplicitattentionanalysisin§2.5,from WepresenttheresultsinTable2. Inthefirstsection,
whichweexploittwokindsofintrinsicsupervision: wefocusonzero-shotperformance,wherewevary
• If C is the correct next response, then w toseeitseffect. Aswecansee,w = 0.25gives
k
α = 0 because C should be linking to aclose-to-bestperformanceintermsofclusterand
n+1 k
onepreviousutterance,notitself. linkscores. Therefore,weuseitforfew-shotfine-
• IfC isincorrect,thenitshouldpointtoitself, tuningsetting,underwhichourproposedmethod
k
actinglikethestartutteranceofanewthread. outperforms baselines trained from scratch by a
Hence,α = 1. large margin. We pick the best checkpoint based
n+1
WetrainthisintrinsicattentionusingMSE(row2 on the validation set performance and evaluate it
ofthemulti-tasklosstableinFigure2)alongwith on the test set. This procedure is repeated three
theoriginalresponseselectionlossusingaweight timeswithdifferentrandomseedstogettheaver-
w for linear combination L = (1 − w) ∗ L + agedperformancereportedinTable2. With10%
res
w ∗ L . Note that we do not use any labeled ofthedata,wecanachieve92%oftheperformance
attn
1F
retsulC
lavE
trained using full data. The performance gap be- Modelensemblingofesimandbertfordialoguere-
comessmallerwhenmoredataisusedasillustrated sponseselection. DSTC8.
inFigure3.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
KristinaToutanova.2018. Bert:Pre-trainingofdeep
3.6 Real-WorldApplication
bidirectional transformers for language understand-
Our method only requires one additional MLP ing. arXivpreprintarXiv:1810.04805.
layerattachedtothearchitectureofLietal.(2020)
Micha Elsner and Eugene Charniak. 2008. You talk-
to train on the entangled response selection task,
ingtome? acorpusandalgorithmforconversation
henceitistrivialtoswapthetrainedmodelintoa disentanglement. In Proceedings of ACL-08: HLT,
productionenvironment. Supposeadialoguedis- pages834–842.
entanglementsystem(Lietal.,2020)isalreadyup
Micha Elsner and Eugene Charniak. 2011. Disentan-
andrunning:
glingchatwithlocalcoherencemodels. InProceed-
ings of the 49th Annual Meeting of the Association
1. Train a BERT model on the entangled re-
for Computational Linguistics: Human Language
sponseselectiontask(§2.1)withattentionsu- Technologies,pages1179–1189.
pervision loss (§3.4). This is also the multi-
Jia-ChenGu,TiandaLi,QuanLiu,XiaodanZhu,Zhen-
tasklossdepictedinFigure2.
HuaLing,andYu-PingRuan.2020. Pre-trainedand
attention-based neural networks for building noetic
2. Copytheweightofthepretrainedmodelinto
task-oriented dialogue systems. arXiv preprint
theexistingarchitecture(Lietal.,2020).
arXiv:2004.01940.
3. Performzero-shotdialoguedisentanglement
Chulaka Gunasekara, Jonathan K. Kummerfeld, Luis
(zero-shot section of Table 2) right away, or Lastras,andWalterS.Lasecki.2020. Noesisii: Pre-
finetunethemodelfurtherwhenmorelabeled dicting responses, identifying success, and manag-
ingcomplexityintask-orienteddialogue. In8thEdi-
data becomes available (few-shot section of
tionoftheDialogSystemTechnologyChallengesat
Table2).
AAAI2019.
This strategy will be useful especially when we
Jyun-Yu Jiang, Francine Chen, Yan-Ying Chen, and
wanttobootstrapasystemwithlimitedandexpen- Wei Wang. 2018. Learning to disentangle inter-
sivelabeleddata. leavedconversationalthreadswithasiamesehierar-
chical network and similarity ranking. In Proceed-
ingsofthe2018ConferenceoftheNorthAmerican
4 Conclusion
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
In this paper, we first demonstrate that entangled
(LongPapers),pages1812–1822.
responseselectiondoesnotrequirecomplexheuris-
tics for context pruning. This implies the model Jonathan K Kummerfeld, Sai R Gouravajhala, Joseph
mighthavelearnedimplicitreply-tolinksusefulfor Peper,VigneshAthreya,ChulakaGunasekara,Jatin
dialoguedisentanglement. Byintroducingintrinsic Ganhotra,SivaSankalpPatel,LazarosPolymenakos,
and Walter S Lasecki. 2018. A large-scale corpus
attentionsupervisiontoshapethedistribution,our
for conversation disentanglement. arXiv preprint
proposedmethodcanperformzero-shotdialogue
arXiv:1810.11118.
disentanglement. Finally, with only 10% of the
datafortuning,ourmodelcanachieve92%ofthe TiandaLi,Jia-ChenGu,XiaodanZhu,QuanLiu,Zhen-
HuaLing,ZhimingSu,andSiWei.2020. Dialbert:
performance of the model trained on full labeled
A hierarchical pre-trained model for conversation
data. Ourmethodisthefirstattempttozero-shot
disentanglement. arXivpreprintarXiv:2004.03760.
dialogue disentanglement, and it can be of high
practicalvalueforreal-worldapplications. YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
References
proach. arXivpreprintarXiv:1907.11692.
Iz Beltagy, Matthew E Peters, and Arman Cohan.
2020. Longformer: Thelong-documenttransformer. DouShen,QiangYang,Jian-TaoSun,andZhengChen.
arXivpreprintarXiv:2004.05150. 2006. Thread detection in dynamic text message
streams. In Proceedings of the 29th annual inter-
Dario Bertero, Kenichi Yokote Takeshi Homma and, national ACM SIGIR conference on Research and
Makoto Iwayama, and Kenji Nagamatsu. 2020. developmentininformationretrieval,pages35–42.
Lidan Wang and Douglas W Oard. 2009. Context-
basedmessageexpansionfordisentanglementofin-
terleaved text conversations. In Proceedings of hu-
man language technologies: The 2009 annual con-
ferenceoftheNorthAmericanchapteroftheassoci-
ation for computational linguistics, pages 200–208.
Citeseer.
Weishi Wang, Shafiq Joty, and Steven CH Hoi.
2020. Response selection for multi-party conversa-
tions with dynamic topic tracking. arXiv preprint
arXiv:2010.07785.
Shuangzhi Wu, Xu Wang Yufan Jiang and, Wei Mia,
ZhenyuZhao,JunXie,andMuLi.2020. Enhancing
response selection with advanced context modeling
andpost-training. DSTC8.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V Le.
2019. Xlnet: Generalized autoregressive pretrain-
ing for language understanding. arXiv preprint
arXiv:1906.08237.
TaoYuandShafiqJoty.2020. Onlineconversationdis-
entanglementwithpointernetworks. arXivpreprint
arXiv:2010.11080.
Manzil Zaheer, Guru Guruganesh, Avinava Dubey,
Joshua Ainslie, Chris Alberti, Santiago Ontanon,
PhilipPham,AnirudhRavula,QifanWang,LiYang,
et al. 2020. Big bird: Transformers for longer se-
quences. arXivpreprintarXiv:2007.14062.
Henghui Zhu, Feng Nan, Zhiguo Wang, Ramesh Nal-
lapati, and Bing Xiang. 2020. Who did they re-
spond to? conversation structure modeling using
maskedhierarchicaltransformer. InProceedingsof
the AAAI Conference on Artificial Intelligence, vol-
ume34,pages9741–9748.

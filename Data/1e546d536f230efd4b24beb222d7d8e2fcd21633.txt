Crowdsourcing Beyond Annotation:
Case Studies in Benchmark Data Collection
AlaneSuhr1,ClaraVania2,NikitaNangia3,MaartenSap4
MarkYatskar5,SamuelR.Bowman3 and YoavArtzi1
1CornellUniversity 2Amazon 3NewYorkUniversity
4UniversityofWashington 5UniversityofPennsylvania
{suhr, yoav}@cs.cornell.edu
{nikitanangia, bowman}@nyu.edu vaniclar@amazon.co.uk
msap@cs.washington.edu myatskar@seas.upenn.edu
Abstract The selection of case studies focuses on chal-
lengingsettingswherecrowdworkersareaskedto
Crowdsourcingfromnon-expertsisoneofthe writeoriginaltextorotherwiseperformrelatively
most common approaches to collecting data unconstrained work. Through these case studies,
andannotationsinNLP.Eventhoughitissuch
wediscussindetailprocessesthatwerecarefully
afundamentaltoolinNLP,crowdsourcinguse
designed to achieve data with specific properties,
islargelyguidedbycommonpracticesandthe
forexampletorequirelogicalinference,grounded
personal experience of researchers. Develop-
reasoning or conversational understanding. Each
ing a theory of crowdsourcing use for practi-
cal language problems remains an open chal- casestudyfocusesondatacollectioncrowdsourc-
lenge. However, there are various principles ing protocol details that often receive limited at-
andpracticesthathaveproveneffectiveingen- tention in research presentations, for example in
eratinghighqualityanddiversedata.Thistuto- conferences, but are critical for research success.
rialexposesNLPresearcherstosuchdatacol-
Weintroducethetaskofeachcasestudy,anddonot
lectioncrowdsourcingmethodsandprinciples
assumepriorknowledge. Wherepossible,wehigh-
through a detailed discussion of a diverse set
lightcommontrends,orotherwisekeydifferences
ofcasestudies.
betweenthediscussedcasestudies.
1 TutorialDescription
Relevance to the NLP Community Crowd-
sourcingtechniquesarecommonlyused,butrarely
Crowdsourcingfromnon-expertsisoneofthemost
discussedindetail. Thistutorialprovidesadetailed
common approaches to collecting data and anno-
descriptionofcrowdsourcingdecisionsincomplex
tations in NLP. It has been applied to a plethora
scenarios and the reasoning behind them. NLP
oftasks,includingquestionanswering(Rajpurkar
researchersaimingtodevelopnewdatasets,tasks
et al., 2016; Choi et al., 2018), textual entail-
anddatacollectionprotocolswillfindthecontent
ment (Williams et al., 2018; Khot et al., 2018),
directly applicable to their own work. A strong
instructionfollowing(Bisketal.,2016;Misraetal.,
understandingofdatacollectionpracticesandthe
2018; Suhr et al., 2019a; Chen et al., 2019a), vi-
range of decisions they include will also aid re-
sualreasoning(Antoletal.,2015;Suhretal.,2017,
searchersusingexistingdatasettocriticallyassess
2019b),andcommonsensereasoning(Talmoretal.,
thedatatheyuse,includingitslimitations.
2019; Sap et al., 2019b). Even though it is such
a fundamental tool, crowdsourcing use is largely
Post-tutorial Materials The tutorial videos,
guidedbycommonpracticesandthepersonalex-
slides and other material will be made available
perience of researchers. Developing a theory of
publiclyonlinefollowingthetutorial.
crowdsourcinguseforpracticallanguageproblems
remains an open challenge. However, there are
2 StructureandContentOverview
various principles and practices that have proven
effectiveingeneratinghighqualityanddiversedata. Thetutorialspansthreehours(180minutes),and
ThistutorialexposesNLPresearcherstosuchdata isdividedintoeightsections:
collectioncrowdsourcingmethodsandprinciples
through a detailed discussion of a diverse set of Introduction (10 min) A brief introduction to
casestudies. thetutorialstructure,itsgoals,andthecasestudies.
Background(20min) Ahigh-speedrecapofes- terminewhetheracaptionistrueorfalseabouta
tablishedcrowdsourcingconceptsandterms. We paired image. The data was collected to require
referbacktothecontentofthissectioninthecase reasoningaboutobjectquantities,comparisonsbe-
studies. This section includes the basic structure tween object properties, and spatial relations be-
of a Mechanical Turk task (HIT), typical incen- tweenobjects. NLVR2isusedasevaluationdata
tive mechanisms, typical communication mecha- for numerous language-and-vision systems (e.g.,
nisms,typicalworkerqualificationandscreening Tan and Bansal, 2019; Chen et al., 2019c). Both
mechanisms,aswellasrelevantresultsaboutthe datasetswerecrowdsourcedwithacontrastivecap-
demographicsandexpressedpreferencesofcrowd- tioning designed to elicit linguistically complex
workersandthecrowdworkercommunity. sentencesandtonaturallybalancethedatasetsbe-
tweentrueandfalseexamples. NLVR2alsouses
Case Study I: MultiNLI (45 min) We discuss
a tiered system during crowdsourcing including
theMultiNLI(Williamsetal.,2018)corpus,with
distinctpoolsofannotationtasksforexperienced
primaryfocusonexperimentsfromsubsequentpa-
workersandnewworkers.
persthatextendorevaluatethedatacollectionpro-
tocolusedtocreatethisdataset. MultiNLIisbuilt Case Study III: CerealBar (25 min) Cereal-
aroundthetaskofnaturallanguageinference(a.k.a. Bar (Suhr et al., 2019a) is a game designed for
textualentailment;Daganetal.,2006;MacCartney, studying collaborative natural language interac-
2009): giventwosentences,thetaskistoidentify tions,releasedalongsideadatasetofinteractions
(roughly)whetherthefirstsentenceentailsthesec- between human players.2 CerealBar emphasizes
ond. Westartwiththiscasestudynotbecauseof collaborationthroughnaturallanguageinstruction
any unique success of the data collection proto- between agents with differing abilities. Each of
col,butbecauseMultiNLIandthenaturallanguage theagentscanbeahumanuseroralearnedmodel.
inferencetaskhaveemergedasapopulartestbed CerealBar has been used to design and train sys-
for data collection methods and for relevant data tems that follow instructions by grounding them
analysismethodsinNLP.Topicsinclude: inthesurroundingenvironmentandactinginthe
environment. The game rules were explicitly de-
• The development of a simple crowdworker-
signedwiththeintentofelicitingrichcollaborative
writing protocol for natural language infer-
interactions across many instructions, for exam-
encedata(Marellietal.,2014;Bowmanetal.,
ple by allowing a pair of players that is scoring
2015;Williamsetal.,2018)
well to continue playing for longer, thereby col-
• Knownissueswithartifacts,socialbias,and lecting more data from successful collaborations.
debatablejudgmentsindatacollectedunder TheCerealBardatacollectionprocessincludeda
thisprotocol(Rudingeretal.,2017;Tsuchiya, developmentofacommunityofplayers,whichhas
2018; Gururangan et al., 2018; Poliak et al., demonstratedbehavioralandlinguisticchangeover
2018;PavlickandKwiatkowski,2019) thecrowdsourcingprocess.
• Experimentsevaluatingdatacollectionfeasi- Case Study IV: QuAC (25 min) Question An-
bility under variants of the base task defini- sweringinContextisadatasetforstudyinginfor-
tion(Chenetal.,2020;Bowmanetal.,2020) mation seeking dialogs between a student and a
teacher(Choietal.,2018). Givenasubjecthead-
• Studiesevaluatingthefeasibilityofcollecting
ing, a student questions a teacher, who responds
data for the same task using alternative pro-
by copying spans from a Wikipedia article. The
tocols(Nieetal.,2020;Kaushiketal.,2019;
goal of the pair is to maintain a dialog of suffi-
Bowmanetal.,2020;Vaniaetal.,2020;Par-
cientlengthwithoutencounteringtoomanyunan-
rishetal.,2021)
swerablequestions. Thetaskistoplaytheroleof
Case Study II: NLVR (25 min) Natural Lan- theteacher: answeringquestionsofaninterested
guageforVisualReasoningcomprisestwodatasets, student. Thecollectionprotocolisuniqueinthat
NLVR(Suhretal.,2017)andNLVR2(Suhretal., two unreliable workers had to becoordinated for
2019b), both study natural language sentences sufficienttimetoaccomplishameaningfuldialog.
grounded in visual context.1 The task is to de- QuACcollectionreliedonseveralstrategiestokeep
1http://lil.nlp.cornell.edu/nlvr/ 2http://lil.nlp.cornell.edu/cerealbar/
partnersfromleavinginteractions,suchasallowing allnecessarybackground(e.g.,Dumitracheetal.,
workerstosimultaneouslyparticipateinmultiple 2018;Chenetal.,2019b;Ram´ırezetal.,2019).
related dialogs, a feedback system teachers used
to help students formulate questions, and scaling 4 Prerequistites
incentivesthatincludedpunitiveelements.
BroadfamiliaritywithNLPtasks,empiricaleval-
Case Study V: SOCIALIQA (25 min) SO- uationmethods,anddatacollectionpractices. We
CIALIQA(Sapetal.,2019b)isthefirstlarge-scale introduceallthenecessarytermsandthespecifics
benchmarktotestmodelemotionalandsocialrea- ofeachcasestudy.
soningthrough38kquestionsabouteverydaysitu-
ations. Thedistributionalnatureofsocialcommon- 5 ReadingList
senseknowledgerequirestheanswercandidatesto
Werecommendreviewingthe2015NAACLtuto-
covertheplausibleandlikely,aswellastheplausi-
rialoncrowdsourcing.3 Whilewefocusonuncon-
blebutunlikely,asopposedtoright/wronganswer
strainedandcomplexcasestudies,the2015tutorial
candidates as common in other QA benchmarks.
providesanoverviewofbasictermsandmethods
SOCIALIQAintroducesaquestion-switchingtech-
thatisacomplementarybackgroundtoourmate-
niqueforcrowdsourcingtheseunlikelyanswers,to
rial. However,wereviewtherequiredmaterialin
overcomethepossiblestylisticartefactsinnegative
the background section, and do not assume a fa-
answers(e.g.,negations,out-of-contextresponses;
miliaritywiththecontentofthispriortutorial. We
Schwartz et al., 2017). Additionally, to achieve
alsorecommendreadingthemainpapersdescrib-
large-scaleandbroadcoverage, SOCIALIQAused
ingeachofthecasestudies(Williamsetal.,2018;
a multi-stage crowdsourcing pipeline to expand
Suhretal.,2017,2019b,a;Choietal.,2018;Sap
seed events from the ATOMIC (Sap et al., 2019a)
etal.,2019b).
commonsense knowledge graph into full-fledged
socialsituations.
6 Presenters
Summary(5min) Abriefsummaryofthetuto-
rial,includingthemaintakeawaysfromthediffer- AlaneSuhr
entcasesstudiesandrepeatingthemes. PhDStudent,CornellUniversity
suhr@cs.cornell.edu
3 Breadth https://alanesuhr.com
Alane’sresearchfocusesongroundednaturallan-
Thesetofcasestudiescoversabroadanddiverse
guageunderstanding. Alanehasdesignedcrowd-
set of task types, including large-scale inference
sourcingtasksforcollectinglanguagedatatostudy
tasks(e.g.,NLI),small-scaleinteractivetasks(e.g.,
situated natural language understanding. Alane
CerealBar),andmulti-modalgroundedtasks(e.g.,
co-presentedatutorialinACL2018.
NLVR). The aim of this broad distribution is to
coverthemostcommontaskanddatascenariosin ClaraVania
NLP.Wefocusondetailsthatarerarelydiscussed AppliedScientist,Amazon
fully in papers. The set of case studies covers a vaniclar@amazon.co.uk
broadanddiversesetoftasktypes,includinglarge- https://claravania.github.io/
scale inference tasks (e.g., NLI), small-scale in- Her research focuses on crowdsourcing, transfer
teractivetasks(e.g.,CerealBar),andmulti-modal learning,andmultilingualNLU.Recently,shehas
groundedtasks(e.g.,NLVR).Theaimofthisbroad been working on semi-automatic data collection
distributionistocoverthemostcommontaskand fornaturallanguageinferenceandcrowdsourcing
datascenariosinNLP.Thecasestudiescoverthere- methodsforquestionanswering.
searchoffourdistinctresearchlabs. Foreachcase
NikitaNangia
study,wewillalsodiscussrelatedworkfromother
PhDstudent,NewYorkUniversity
authorsasisrelevant. Forexample,theMultiNLI
nikitanangia@nyu.edu
casestudywillincludeextensivediscussionoffol-
https://woollysocks.github.io
lowupworkandtheSocialIQAcasestudywilldis-
Nikita’sworkfocusesoncrowdsourcingmethods
cussrelatedcommonsenseresources. Inaddition,
wewilldiscussrelevantexistingworktoprovide 3http://crowdsourcing-class.org/tutorial.html
anddatacreationfornaturallanguageunderstand- References
ingsoft. Herrecentworkexploresusingincentive
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
structures to illicit creative examples. Nikita co- garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
organizedatutorialonlatentstructuremodelsfor and Devi Parikh. 2015. VQA: Visual question an-
NLPatACL2019. swering. InIEEEInternationalConferenceonCom-
puterVision,pages2425–2433.
MaartenSap
Yonatan Bisk, Deniz Yuret, and Daniel Marcu. 2016.
PhDstudent,UniversityofWashington Natural language communication with robots. In
msap@cs.washington.edu Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
http://maartensap.com/
tionalLinguistics: HumanLanguageTechnologies.
His research focuses on endowing NLP systems
withsocialintelligenceandsocialcommonsense, Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and understanding social inequality and bias in and Christopher D. Manning. 2015. A large anno-
tatedcorpusforlearningnaturallanguageinference.
language. Hissubstantialexperiencewithcrowd-
In Proceedings of the 2015 Conference on Empiri-
sourcingincludesthecollectingoftheSOCIALIQA
calMethodsinNaturalLanguageProcessing,pages
commonsense benchmark as well as the creation 632–642,Lisbon,Portugal.AssociationforCompu-
of knowledge graphs with inferential knowledge tationalLinguistics.
(ATOMIC,SocialBiasFrames).
Samuel R Bowman, Jennimaria Palomaki, Livio Bal-
dini Soares, and Emily Pitler. 2020. Collecting en-
MarkYatskar tailment data for pretraining: New protocols and
AssistantProfessor,UniversityofPennsylvania negativeresults. InProceedingsofEMNLP.
myatskar@seas.upenn.edu
Howard Chen, Alane Suhr, Dipendra Misra, Noah
https://markyatskar.com/
Snavely, and Yoav Artzi. 2019a. Touchdown: Nat-
Hisresearchfocusesontheintersectionofnatural urallanguagenavigationandspatialreasoninginvi-
languageprocessingandcomputervision. Mark’s sual street environments. In IEEE Conference on
ComputerVisionandPatternRecognition.
workhasresultedinthecreationofdatasetssuchas
imSitu,QuACandWinoBiasandrecentresearch Quanze Chen, Jonathan Bragg, Lydia B. Chilton, and
has focused on gender bias in visual recognition Dan S. Weld. 2019b. Cicero: Multi-turn, contex-
andcoreferenceresolution. tual argumentation for accurate crowdsourcing. In
Proceedingsofthe2019CHIConferenceonHuman
FactorsinComputingSystems,CHI’19,page1–14,
SamBowman
New York, NY, USA. Association for Computing
AssistantProfessor,NewYorkUniversity Machinery.
bowman@nyu.edu
TongfeiChen,ZhengpingJiang,AdamPoliak,Keisuke
https://cims.nyu.edu/˜sbowman/
Sakaguchi, and Benjamin Van Durme. 2020. Un-
Sam works on data creation, benchmarking, and
certain natural language inference. In Proceedings
modelanalysisforNLUandcomputationallinguis- of the 58th Annual Meeting of the Association for
tics. SamhashadasubstantialroleinseveralNLU Computational Linguistics, pages 8772–8779, On-
line.AssociationforComputationalLinguistics.
datasets, including SNLI, MNLI, XNLI, CoLA,
andBLiMP,andhisrecentworkhasfocusedonex- Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El
perimentallyevaluatingmethodsforcrowdsourced Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and
corpusconstruction. Jingjing Liu. 2019c. UNITER: Learning universal
image-textrepresentations. ArXiv,abs/1909.11740.
YoavArtzi
EunsolChoi, HeHe, MohitIyyer, MarkYatskar, Wen
AssociateProfessor,CornellUniversity tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-
yoav@cs.cornell.edu moyer.2018. Quac: Questionansweringincontext.
InEmpiricalMethodsinNaturalLanguageProcess-
https://yoavartzi.com/
ing.
Yoav’s research focuses on learning expressive
modelsfornaturallanguageunderstanding,most Ido Dagan, Oren Glickman, and Bernardo Magnini.
recently in situated interactive scenarios. Yoav 2006. ThePASCALrecognisingtextualentailment
challenge. In Machine learning challenges. Evalu-
led tutorials on semantic parsing in ACL 2013,
atingpredictiveuncertainty,visualobjectclassifica-
EMNLP2014andAAAI2015.
tion,andrecognisingtectualentailment,pages177–
190.Springer,NewYork,NY.
Anca Dumitrache, Oana Inel, Lora Aroyo, Benjamin Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Timmermans, and Chris Welty. 2018. Crowdtruth Rachel Rudinger, and Benjamin Van Durme. 2018.
2.0: Quality metrics for crowdsourcing with dis- Hypothesis only baselines in natural language in-
agreement. In Joint Proceedings SAD 2018 and ference. In Proceedings of the Seventh Joint Con-
CrowdBias 2018, CEUR Workshop Proceedings, ference on Lexical and Computational Semantics,
pages11–18.CEUR-WS. pages 180–191, New Orleans, Louisiana. Associa-
tionforComputationalLinguistics.
Suchin Gururangan, Swabha Swayamdipta, Omer
Levy,RoySchwartz,SamuelBowman,andNoahA. PranavRajpurkar,JianZhang,KonstantinLopyrev,and
Smith. 2018. Annotation artifacts in natural lan- Percy Liang. 2016. SQuAD: 100,000+ questions
guage inference data. In Proceedings of the 2018 formachinecomprehensionoftext. InProceedings
Conference of the North American Chapter of the oftheConferenceonEmpiricalMethodsinNatural
Association for Computational Linguistics: Human LanguageProcessing.
Language Technologies, Volume 2 (Short Papers),
pages107–112.AssociationforComputationalLin- Jorge Ram´ırez, Simone Degiacomi, Davide Zanella,
guistics. Marcos Baez, Fabio Casati, and Boualem Benatal-
lah. 2019. Crowdhub: Extending crowdsourcing
DivyanshKaushik, EduardHovy, andZacharyLipton. platforms for the controlled evaluation of tasks de-
2019. Learning the difference that makes a differ- signs. arXivpreprint1909.02800.
encewithcounterfactually-augmenteddata. InInter-
Rachel Rudinger, Chandler May, and Benjamin
nationalConferenceonLearningRepresentations.
VanDurme.2017. Socialbiasinelicitednaturallan-
guage inferences. In Proceedings of the First ACL
TusharKhot,AshishSabharwal,andPeterClark.2018.
Workshop on Ethics in Natural Language Process-
Scitail: A textual entailment dataset from science
ing, pages 74–79, Valencia, Spain. Association for
questionanswering.
ComputationalLinguistics.
Bill MacCartney. 2009. Natural language inference.
Maarten Sap, Ronan LeBras, Emily Allaway, Chan-
Ph.D.thesis,StanfordUniversity,Stanford,CA.
draBhagavatula,NicholasLourie,HannahRashkin,
Brendan Roof, Noah A Smith, and Yejin Choi.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
2019a. Atomic: Anatlasofmachinecommonsense
Bentivogli,Raffaellabernardi,andRobertoZampar-
forif-thenreasoning. InAAAI.
elli. 2014. A SICK cure for the evaluation of com-
positional distributional semantic models. In Pro-
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
ceedings of the Ninth International Conference on
LeBras, and Yejin Choi. 2019b. Social iqa: Com-
LanguageResourcesandEvaluation(LREC),pages
monsense reasoning about social interactions. In
216–223, Reykjavik, Iceland. European Language
EMNLP.
ResourcesAssociation(ELRA).
RoySchwartz,MaartenSap,IoannisKonstas,LiZilles,
DipendraMisra,AndrewBennett,ValtsBlukis,Eyvind
Yejin Choi, and Noah A Smith. 2017. The effect
Niklasson, Max Shatkhin, and Yoav Artzi. 2018.
ofdifferentwritingtasksonlinguisticstyle: Acase
Mappinginstructionstoactionsin3Denvironments
studyoftherocstoryclozetask. InCoNLL.
with visual goal prediction. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.
guageProcessing.
2017. A corpus of natural language for visual rea-
soning. InProceedingsoftheAnnualMeetingofthe
Yixin Nie, Adina Williams, Emily Dinan, Mohit
AssociationforComputationalLinguistics.
Bansal,JasonWeston,andDouweKiela.2020. Ad-
versarial NLI: A new benchmark for natural lan- Alane Suhr, Claudia Yan, Jack Schluger, Stanley Yu,
guageunderstanding. InProceedingsofthe58thAn- Hadi Khader, Marwa Mouallem, Iris Zhang, and
nual Meeting of the Association for Computational Yoav Artzi. 2019a. Executing instructions in situ-
Linguistics, pages 4885–4901, Online. Association ated collaborative interactions. In Proceedings of
forComputationalLinguistics. the Conference on Empirical Methods in Natural
LanguageProcessing.
AliciaParrish,WilliamHuang,OmarAgha,Soo-Hwan
Lee, Nikita Nangia, Alex Warstadt, Karmanya Ag- Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,
garwal, Emily Allaway, Tal Linzen, and Samuel R Huajun Bai, and Yoav Artzi. 2019b. A corpus for
Bowman.2021. Doesputtingalinguistintheloop reasoning about natural language grounded in pho-
improvenludatacollection? InToappearinFind- tographs. In Proceedings of the Annual Meeting of
ingsofEMNLP. theAssociationforComputationalLinguistics.
Ellie Pavlick and Tom Kwiatkowski. 2019. Inherent Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
disagreementsinhumantextualinferences. Transac- Jonathan Berant. 2019. CommonsenseQA: A ques-
tions of the Association for Computational Linguis- tion answering challenge targeting commonsense
tics,7:677–694. knowledge. InProceedingsoftheConferenceofthe
NorthAmericanChapteroftheAssociationforCom-
putationalLinguistics: HumanLanguageTechnolo-
gies.
HaoTanandMohitBansal.2019. LXMERT:Learning
cross-modality encoder representations from trans-
formers. InProceedingsofthe2019Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing
andthe9thInternationalJointConferenceonNatu-
ralLanguageProcessing(EMNLP-IJCNLP),pages
5100–5111, Hong Kong, China. Association for
ComputationalLinguistics.
Masatoshi Tsuchiya. 2018. Performance impact
caused by hidden bias of training data for recog-
nizing textual entailment. In Proceedings of the
EleventhInternationalConferenceonLanguageRe-
sources and Evaluation (LREC 2018), Miyazaki,
Japan. European Language Resources Association
(ELRA).
Clara Vania, Ruijie Chen, and Samuel R. Bowman.
2020. Askingcrowdworkerstowriteentailmentex-
amples: Thebestofbadoptions. InProceedingsof
AACL.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tenceunderstandingthroughinference. InProceed-
ingsofthe2018ConferenceoftheNorthAmerican
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1(LongPapers), pages1112–1122.Associationfor
ComputationalLinguistics.

COMET : Commonsense Transformers
for Automatic Knowledge Graph Construction
AntoineBosselut♦♠ HannahRashkin♦♠ MaartenSap♦♠ ChaitanyaMalaviya♦
AsliCelikyilmaz♣ YejinChoi♦♠
♦AllenInstituteforArtificialIntelligence,Seattle,WA,USA
♠PaulG.AllenSchoolofComputerScience&Engineering,Seattle,WA,USA
♣MicrosoftResearch,Redmond,WA,USA
Abstract
Automatic KB
Commonsense Knowledge Bases
Completion
(seen events)
We present the first comprehensive study xAttr loving
on automatic knowledge base construction xAttr PersonX towards bring a
puts their PersonY wallet
for two prevalent commonsense knowledge arms around
graphs: ATOMIC (Sap et al., 2019) and Con-
caring oReact PersonY xNeed
c
m
we iap
tn
htN
y
ce
c
at
o nn
o( vS
ne
ip cne ate
i
lor
tn
ee
a
mt
l
pKa ll aB. t,
es
s2
t
,h0 ca1 ot7 m) s.
t mor
oeC
nk
so enn not sr wa elr ey
Kd
Bgto
e
s
lf oe ve els
d
to
xIntent xIntentgoP ee ssr ts
o
to ro en tX h e do oz fi fng ha fuvi nng
o scn rl iy ptis ot no sre ol foo ks ne ol wy les dtr gu ec .tur Wed eo pp oe sn it-te thx at td ae n- Pc eo rm sofo nr Yt t fo o og det UsedFor Causes Personx XNee d
buys
important step toward automatic common- lunch
nap HasSubevent
sense completion is the development of gen- Going to
energy a movie
e ar na dtiv pe rom poo sd eels Co Of Mco mm onm so En ns se ense Trk an no sw fol re md eg re s, HasSubevent
h aa v rein sg
t
Causes
T ah r po aw ri tn yg
(COMET ) that learn to generate rich and Unseen Events
diverse commonsense descriptions in natural
language. Despite the challenges of com- Figure1:COMET learnsfromanexistingknowledge
monsense modeling, our investigation reveals base(solidlines)tobeabletogeneratenovelnodesand
promising results when implicit knowledge edges(dashedlines).
from deep pre-trained language models is
transferred to generate explicit knowledge in
commonsense knowledge graphs. Empirical necessary to automatically construct a common-
results demonstrate that COMET is able to
senseknowledgebase(KB).
generatenovelknowledgethathumansrateas
Automatic KB construction is a long-standing
highquality,withupto77.5%(ATOMIC)and
91.7%(ConceptNet)precisionattop1,which goal of artificial intelligence research due to the
approaches human performance for these re- difficulty of achieving high concept coverage in
sources. Our findings suggest that using gen- high-precision curated KBs (Lenat, 1995; Miller,
erative commonsense models for automatic 1995). Previousworkhasdevelopedmodelscapa-
commonsense KB completion could soon be
ble of reading and extracting semi-structured text
aplausiblealternativetoextractivemethods.
(Suchanek et al., 2007; Hoffart et al., 2013; Auer
et al., 2007; Bollacker et al., 2008) and unstruc-
1 Introduction
turedtext(Dongetal.,2014;Carlsonetal.,2010;
When reading text, humans make commonsense Nakashole et al., 2011, 2012; Niu, 2012) into re-
inferences that frame their understanding of the lational schemas that can be queried for down-
narrativebeingpresented. Formachinestoachieve stream applications. A common thread of these
this capability, they must be able to acquire rele- approaches, however, is the focus on encyclope-
vant and correct commonsense for an unbounded dicknowledge,whichlendsitselftoawell-defined
set of situations. In this work, we cast common- spaceofentitiesandrelationsthatcanbemodeled.
sense acquisition as knowledge base construction Commonsense knowledge, however, does not
andinvestigatewhetherlarge-scalelanguagemod- cleanly fit into a schema comparing two entities
elscaneffectivelylearntogeneratetheknowledge withaknownrelation,leadingcurrentapproaches
cimotA
teNtpecnoC
Multi-headed Attention Transformer Block Commonsense Transformer (COMeT)
g~l h tl [MASK][MASK] have boat <END>
Layer Normalization
Linear Projection Vocab Vocab Vocab Vocab Vocab
+
Concatenation Feedforward Network Block Block … Block … Block Block
… … …
Layer Normalization
Attention … Attention
Head 1 Head b
+ … …
Block Block Block Block Block
W 1K W 1V W 1Q W bK W b V W bQ
Multi-headed Attention + + + + +
e
0
p
0
e
1
p
1
e
|s|
p
|s|
…
K V Q {h 0l - ,1 …, h tl - - 1 1} h tl - 1 PersonXsails …<xNeed> … sail boat
( a ) ( b ) ( c )
Figure2: Modeldiagram. (a)Inthemulti-headedattentionmodule, thekey, value, andqueryallpassthrougha
head-specificprojectionbeforeascaleddot-productattentioniscomputedbetweenthem. Theoutputsoftheheads
areconcatenatedandprojected. (b)Insidethetransformerblock,theoutputsofallthepreviouslayerblocksfrom
earliertimestepsareinputtothemulti-headedattentionwiththeprecedingblockforthecurrenttimestepasthe
query.(c)Eachtokenisaninputtoafirst-layerblockalongwithallprecedingtokens.Dottedlinesindicateoutputs
toallfutureblocksinthenextlayerandinputsfromallprecedingblocksinthepreviouslayer.
to model “entities" as natural language phrases tween existing nodes by generating phrases that
and relations as any concept that can link them coherently complete an existing seed phrase and
(Li et al., 2016; Sap et al., 2019). OpenIE ap- relation type1. Second, we develop a framework
proaches display this property of open text enti- forusinglarge-scaletransformerlanguagemodels
tiesandrelations(Etzionietal.,2011;Faderetal., to learn to produce commonsense knowledge tu-
2011; Mausam et al., 2012), but being extrac- ples2. Finally, we perform an empirical study on
tive, they only capture knowledge that is explic- thequality,novelty,anddiversityofthecommon-
itly mentioned in text, limiting their applicability sense knowledge produced by our approach for
for capturing commonsense knowledge, which is twodomains,ATOMICandConceptNet,aswellas
oftenimplicit(GordonandVanDurme,2013). an efficiency study on the number of seed tuples
needed to learn an effective knowledge model.
Meanwhile, recent progress in training deep
The results indicate that COMET is able to pro-
contextualized language models (Peters et al.,
ducehighqualitytuplesashumanjudgesfindthat
2018; Radford et al., 2018; Devlin et al., 2018)
77.5%ofgeneratedtuplesforATOMICeventsand
providesanopportunitytoexplorebeyondextrac-
91.7% of generated tuples for ConceptNet rela-
tive methods as an avenue for commonsense KB
tionsarecorrect.
construction. These large-scale language models
displayimpressiveperformancewhentheirunder-
2 LearningtoGenerateCommonsense
lyingrepresentationsaretunedtosolveendtasks,
achieving state-of-the-art results on a variety of
COMET isanadaptationframeworkforconstruct-
complex problems. In this work, we define the
ingcommonsenseknowledgebasesfromlanguage
COMmonsEnse Transformer (COMET ), which
models by training the language model on a seed
constructs commonsense KBs by using existing
set of knowledge tuples. These tuples provide
tuples as a seed set of knowledge on which to
COMET with the KB structure and relations that
train. Using this seed set, a pre-trained language
must be learned, and COMET learns to adapt the
modellearnstoadaptitslearnedrepresentationsto
languagemodelrepresentationslearnedfrompre-
knowledge generation, and produces novel tuples
training to add novel nodes and edges to the seed
thatarehighquality.
knowledgegraph.
Wesummarizeourcontributionsinthisworkas
follows. First, we develop a generative approach 1Demo is available at https://mosaickg.apps.
allenai.org/
to knowledge base construction. A model must
2Code is available at https://github.com/
learntoproducenewnodesandidentifyedgesbe- atcbosselut/comet-commonsense
2.1 Task Multi-headed Attention The multi-headed at-
tention module of each transformer block, shown
Morespecifically,theproblemassumesCOMETis
inFigure2(a),isidenticaltotheoneoriginallyde-
given a training knowledge base of natural lan-
finedbyVaswanietal.(2017). Theattentionfunc-
guage tuples in {s,r,o} format, where s is the
tion receives three inputs, a query Q, key K, and
phrase subject of the tuple, r is the relation of the
valueV. Theattentionismadeofmultipleheads
tuple, and o is the phrase object of the tuple. For
thateachcomputeauniquescaleddotproductat-
example, a ConceptNet tuple relating to “taking
tentiondistributionoverV usingQandK:
a nap" would be: (s=“take a nap", r=Causes,
o=“haveenergy"). The taskisto generateogiven (cid:18) QKT(cid:19)
sandr asinputs. ATTENTION(Q,K,V) = softmax √ V
d
k
(5)
Notation We define Xs = {xs,...,xs } as the
0 |s|
tokens that make up the subject of the relation,
whered isthedimensionalityoftheinputvectors
k
Xr = {xr,...,xr } as the tokens that make up
0 |r| representing the query, key and value. For each
the relation of the tuple, and Xo = {xo 0,...,xo |o|} of the heads, Q, K, and V are uniquely projected
asthetokensthatmakeuptheobjectofthetuple. priortotheattentionbeingcomputed:
Theembeddingforanywordxisdenotedase.
H
i
= ATTENTION(QW iQ,KW iK,VW iV) (6)
2.2 TransformerLanguageModel
where H is the output of a single attention head
i
While COMET is agnostic to the language model and WQ , WK, and WV are head-specific projec-
with which it is initialized, in this work, we use i i i
tions for Q, K, and V, respectively. The outputs
the transformer language model architecture in-
oftheattentionheadsH arethenconcatenated:
i
troduced in Radford et al. (2018) (GPT), which
uses multiple transformer blocks of multi-headed MULTIH(Q, K, V) = [H 1;...;H b]WO (7)
scaled dot product attention and fully connected
layers to encode input text (Vaswani et al., 2017). whereWO isanoutputprojectionoftheconcate-
Figure2depictsdifferentcomponentsoftheGPT nated outputs of the attention heads. As shown in
architecture and we define each component in Figure 2(c), we follow Radford et al. (2018) and
moredepthbelow. use the output of the previous layer’s transformer
block as the query input for the multi-headed at-
Transformer Block As shown in Figure 2(b),
tentionofthenextblock. Thekeysandvaluesare
eachtransformerlayerlcontainsanarchitecturally
outputs of the previous layer’s block for all pre-
identical transformer block (though with unique
cedingtimesteps:
trainable parameters) that applies the following
transformationstotheinputtotheblock: MULTIATTN(hl−1) = MULTIH(hl−1,hl−1,hl−1)
t t t t
(8)
g˜l = MULTIATTN(hl−1) (1)
gl = LAYERNORM(g˜l +hl−1) (2) where hl t−1 = {hl−1} <t is the set of previous
layertransformerblockoutputsfortimestepspre-
h˜l = FFN(gl) (3)
cedingt.
hl = LAYERNORM(h˜l +gl) (4)
InputEncoder Asinputtothemodel,werepre-
sentaknowledgetuple{s,r,o}asaconcatenated
where MULTIATTN is a multi-headed self-
sequenceofthewordsofeachitemofthetuple:
attention mechanism (defined below), FFN is
a two-layer feed-forward network, and LAYER- X = {Xs,Xr,Xo} (9)
NORM representsalayernormalization(Baetal.,
2016) operation that is applied to the output of Since the transformer (a self-attention model) has
the self-attention and the feedforward network. no concept of ordering of tokens, a position em-
Note that the inputs to the LAYERNORM opera- beddingp
t
isinitializedforeachabsoluteposition
tions contain a residual connection that sums the in the sequence (Vaswani et al., 2017). For any
outputofandinputtothepreviousoperation. input word x ∈ X, our encoding of the input is
t
ATOMIC Input Template and ConceptNet Relation-only Input Template Initialization Parametersareinitializedtothefi-
s tokens mask tokens r token o tokens nal language model weights from Radford et al.
PersonX goes to the mall [MASK] <xIntent> to buy clothes (2018). Additional special tokens that are added
tothevocabularyforfinetuning(e.g.,relationem-
ConceptNet Relation to Language Input Template
s tokens mask tokens r tokens mask tokens o tokens beddings such as oReact for ATOMIC and IsA
go to mall [MASK] [MASK] has prerequisite [MASK] have money for ConceptNet) are initialized by sampling from
thestandardnormaldistribution.
Figure3: Inputtokensetupfortrainingconfigurations.
Forthe ATOMIC dataset,thetokensofthesubject,Xs Hyperparameters Following Radford et al.
(e.g.,PersonXgoestothemall)arefollowedbymask-
(2018)’s design of the GPT model, we initialize
ingtokens,whichisfollowedbyasinglerelationtoken
COMET with 12 layers, 768-dimensional hidden
Xr (e.g., xIntent), and then the object tokens Xo
states, and 12 attention heads. We use a dropout
(e.g.,tobuyclothes). Themodelreceivesthesamein-
rateof0.1anduseGeLU(HendrycksandGimpel,
put for ConceptNet, except that a second set of mask-
ingtokensseparateXr andXobecauseXr canhavea 2016) units as activation functions. During train-
variablenumberoftokensforConceptNet(§5.2) ing, our batch size is 64. Other dataset-specific
hyperparametersareprovidedinAppendixA.1.
thesumofitswordembedding,e withaposition
t 4 ATOMIC Experiments
embedding encoding its absolute position in the
sequenceX: The ATOMIC dataset3, released by Sap et al.
(2019), contains 877K tuples covering a variety
h0 = e +p (10) ofsocialcommonsenseknowledgearoundspecific
t t t
eventprompts(e.g.,“Xgoestothestore”). Specif-
wherep isthepositionembeddingfortimestept,
t ically, ATOMIC distills its commonsense in nine
andh0 istheinputtothefirsttransformerlayer.
dimensions, covering the event’s causes (e.g., “X
needstodrivethere”),itseffectsontheagent(e.g.,
3 TrainingCOMET
“to get food”) and its effect on other direct (or
COMET is trained to learn to produce the phrase implied) participants (e.g., “Others will be fed”).
object o of a knowledge tuple given the tuple’s More details about ATOMIC can be found in Ap-
phrasesubjectsandrelationr. Morespecifically, pendix D. For our experiments, ATOMIC events
given the concatenation of the tokens of s and r: (e.g.,“Xgoestothestore”)arephrasesubjects,s,
[Xs,Xr] as input, the model must learn to gener- thedimension(e.g.,xIntent)isthephraserela-
atethetokensofo: Xo(See§2.1fordefinitionsof tion, r, andthecauses/effects(e.g., “togetfood”)
thesevariables). are phrase objects, o. We use the training splits
from Sap et al. (2019), resulting in 710k training,
Loss Function To achieve this goal, COMET is
80kdevelopment,and87ktesttuplesrespectively.
trained to maximize the conditional loglikelihood
ofpredictingthephraseobjecttokens,Xo:
4.1 Setup
|s|+|r|+|o| Metrics Following Sap et al. (2019), we eval-
(cid:88)
L = − logP(x |x ) (11) uate our method using BLEU-2 as an automatic
t <t
evaluation metric. We also report the perplexity
t=|s|+|r|
ofthemodelonitsgoldgenerations. Theremain-
where |s|, |r|, and |o| are the number of tokens
ingautomaticmetricsinTable1measurethepro-
in the subject phrase, relation, and object phrase,
portion of generated tuples and generated objects
respectively. Figure3outlineshowthetokensins,
which are not in the training set. We report the
r,andoareorganizedfordifferenttrainingtasks.
proportion of all generated tuples that are novel
Datasets COMET relies on a seed set of knowl- (% N/T sro) and that have a novel object (% N/T
edge tuples from an existing KB to learn to pro- o)4. To show that these novel objects are diverse
duce commonsense knowledge. In this work, (i.e.,thesamenovelobjectisnottheonlyonebe-
we use ATOMIC and ConceptNet as knowledge inggenerated),wealsoreportthenumberofnovel
seed sets, but other commonsense knowledge re-
3https://homes.cs.washington.edu/
sourcescouldhavebeenusedaswellasCOMETis
~msap/atomic/
domain-agnostic. 4aneworepresentsanewnodeintheknowledgegraph
Model PPL5 BLEU-2 N/Tsro6 N/To N/Uo
9ENC9DEC(Sapetal.,2019) - 10.01 100.00 8.61 40.77
NearestNeighbor(Sapetal.,2019) - 6.61 - - -
Event2(IN)VOLUN(Sapetal.,2019) - 9.67 100.00 9.52 45.06
Event2PERSONX/Y(Sapetal.,2019) - 9.24 100.00 8.22 41.66
Event2PRE/POST(Sapetal.,2019) - 9.93 100.00 7.38 41.99
COMET (-pretrain) 15.42 13.88 100.00 7.25 45.71
COMET 11.14 15.10 100.00 9.71 51.20
Table 1: Automatic evaluations of quality and novelty for generations of ATOMIC commonsense. No novelty
scoresarereportedfortheNearestNeighborbaselinebecauseallretrievedsequencesareinthetrainingset.
Model oEffect oReact oWant xAttr xEffect xIntent xNeed xReact xWant Avg
9Enc9Dec(Sapetal.,2019) 22.92 32.92 35.50 52.20 47.52 51.70 48.74 63.57 51.56 45.32
Event2(In)voluntary(Sapetal.,2019) 26.46 36.04 34.70 52.58 46.76 61.32 49.82 71.22 52.44 47.93
Event2PersonX/Y(Sapetal.,2019) 24.72 33.80 35.08 52.98 48.86 53.93 54.05 66.42 54.04 46.41
Event2Pre/Post(Sapetal.,2019) 26.26 34.48 35.78 52.20 46.78 57.77 47.94 72.22 47.94 46.76
COMET(-pretrain) 25.90 35.40 40.76 48.04 47.20 58.88 59.16 64.52 65.66 49.50
COMET 29.02 37.68 44.48 57.48 55.50 68.32 64.24 76.18 75.16 56.45
Table 2: Human score of generations of ATOMIC commonsense. We present comparisons to the baselines from
Sapetal.(2019). UnderlinedresultsarethosewhereCOMETisnotsignificantlybetteratp<0.05
objects as a function of the set of unique objects Ablations To evaluate how pre-training on a
producedforalltestsetevents(%N/Uo). large corpus helps the model learn to produce
knowledge, we train a version of COMET that is
Finally, we perform a human evaluation using
notinitializedwithpre-trainedweights(COMET(-
workers from Amazon Mechanical Turk (AMT).
pretrain)). We also evaluate the data efficiency of
Workers are asked to identify whether a model
our method by training models on different pro-
generation of ATOMIC commonsense adequately
portions of the training data. Finally, because
completesaplausibletupleofphrasesubject,rela-
the ultimate goal of our method is to be able
tion,andphraseobject. FollowingthesetupofSap
to perform high-quality, diverse knowledge base
et al. (2019), we evaluate 100 randomly selected
construction, we explore how various decoding
events from the test set. For each event and rela-
schemesaffectthequalityofcandidateknowledge
tiontype,10candidatesaregeneratedusingbeam
tuples. Wepresenttheeffectofthefollowinggen-
searchandthefullbeamisevaluatedbyfivediffer-
erationstrategies: argmaxgreedydecoding,beam
entworkers. Overall,n=5000ratingsareproduced
searchwithbeamsizes,b=2,5,10,andtop-ksam-
per relation (100 events × 5 workers × 10 candi-
pling with k = 5, 10. For each decoding method,
dates). The reported Avg in Table 2 is an aver-
we conduct the human evaluation on the number
ageofthesescores,yieldingn=45000totalratings
offinalcandidatesproducedbyeachmethod.
for each model. We use Pitman’s test (Noreen,
1989) with 100k permutations to test for statis-
4.2 Results
tical significance. Because 50 different hypothe-
Overall performance The BLEU-2 results in
ses are tested (9 relations + the total), the Holm-
Table 1 indicate that COMET exceeds the perfor-
Bonferronimethod(Holm,1979)isusedtocorrect
mance of all baselines, achieving a 51% relative
significance thresholds. Example events from the
improvement over the top performing model of
developmentsetandtheirgeneratedphraseobjects
Sapetal.(2019). Moreinteresting,however,isthe
areavailableinTable5.
resultofthehumanevaluation,whereCOMET re-
ported a statistically significant relative Avg per-
Baselines We report the performance of our formance increase of 18% over the top baseline,
method against the models trained in Sap et al.
(2019)thatuseLSTMsequence-to-sequencemod-
5Sapetal.(2019)’smodelsweretrainedwithadifferent
vocabularysoadirectperplexitycomparisonisnotpossible.
els(Sutskeveretal.,2014)toencodetheinputsub-
6Alltestsetsdonotappearinthetrainingsetsoallfull
jectandrelationandproduceanoutputobject. tuplesmustbenovel.
COMET Decodingmethod oEffect oReact oWant xAttr xEffect xIntent xNeed xReact xWant Avg
Top-5randomsampling(n=2500perrelation) 34.60 44.04 35.56 64.56 55.68 58.84 46.68 80.96 58.52 53.27
Top-10randomsampling(n=5000perrelation) 25.20 37.42 27.34 49.20 47.34 47.06 38.24 72.60 48.10 43.61
Beamsearch-2beams(n=1000perrelation) 43.70 54.20 47.60 84.00 51.10 73.80 50.70 85.80 78.70 63.29
Beamsearch-5beams(n=2500perrelation) 37.12 45.36 42.04 63.64 61.76 63.60 57.60 78.64 68.40 57.57
Beamsearch-10beams(n=5000perrelation) 29.02 37.68 44.48 57.48 55.50 68.32 64.24 76.18 75.16 56.45
Greedydecoding(n=500perrelation) 61.20 69.80 80.00 77.00 53.00 89.60 85.60 92.20 89.40 77.53
HumanvalidationofgoldATOMIC 84.62 86.13 83.12 78.44 83.92 91.37 81.98 95.18 90.90 86.18
Table3: Humanevaluationtestingeffectofdifferentdecodingschemesoncandidatetuplequality. Thenumberof
ratingsmadeperrelationforeachdecodingmethodisprovidedinthefirstcolumn.
%traindata PPL BLEU-2 N/To N/Uo SeedConcept Relation Generated Plausible
XholdsoutX’shandtoY xAttr helpful (cid:88)
1%train 23.81 5.08 7.24 49.36 XmeetsYeyes xAttr intense (cid:88)
10%train 13.74 12.72 9.54 58.34 XwatchesYevery___ xAttr observant (cid:88)
50%train 11.82 13.97 9.32 50.37 Xeatsredmeat xEffect getsfat (cid:88)
Xmakescrafts xEffect getsdirty (cid:88)
FULL(-pretrain) 15.18 13.22 7.14 44.55 X Xt pu or un rs sX _’ _s _ph oo vn ee
rY’shead
x oE Ef ff fe ec ct
t
g ge et ts sa hute rtxt
(cid:88)
FULLtrain 11.13 14.34 9.51 50.05 X Xt pa ik sse es sY o’ ns Yhe ’a sd boo nff
fire
o oE Ef ff fe ec ct
t
b gl ee te sd bs
urned
(cid:88)
Xspoilssomebodyrotten xIntent tobemean
XgivesYsomepills xIntent tohelp (cid:88)
Table4:Effectofamountoftrainingdataonautomatic XprovidesforY’sneeds xIntent tobehelpful (cid:88)
evaluationofcommonsensegenerations XexplainsY’sreasons xNeed toknowY (cid:88)
XfulfilsX’sneeds xNeed tohaveaplan (cid:88)
XgivesYeverything xNeed tobuysomething (cid:88)
Xeatspancakes xReact satisfied (cid:88)
Xmakes___atwork xReact proud (cid:88)
Xmoveshouse xReact happy (cid:88)
Event2IN(VOLUN). This performance increase is XgivesbirthtotheY oReact happy (cid:88)
XgivesY’sfriend___ oReact grateful (cid:88)
consistent, as well, with an improvement being Xgoes___withfriends oReact happy (cid:88)
observed across every relation type. In addition Xgetsallthesupplies xWant tomakealist (cid:88)
XmurdersY’swife xWant tohidethebody (cid:88)
to the quality improvements, Table 1 shows that Xstartsshopping xWant togohome (cid:88)
XdevelopsYtheory oWant tothankX (cid:88)
COMET produces more novel tuple objects than XofferYaposition oWant toacceptthejob (cid:88)
Xtakes___outfordinner oWant toeat (cid:88)
thebaselines,aswell.
Table 5: Generations that were randomly selected
from a subset of novel generations from the ATOMIC
Learning knowledge from language Signifi-
developmentset. Anovelgenerationisasrotuplenot
cant differences were also observed between the
foundinthetrainingset. Manualevaluationofeachtu-
performanceofthemodelwhoseweightswereini- pleindicateswhetherthetupleisconsideredplausible
tialized with the pre-trained parameters from the byahumanannotator.
GPT model of Radford et al. (2018) and a model
with the same architecture that was trained from
ments still hover around 55%7 for a beam size of
randominitialization. This14%relativeimprove-
10. This result suggests that COMET could be ef-
ment in overall human performance confirms that
fective with human evaluators in the loop to con-
the language representations learned by the GPT
firmthecorrectnessofgeneratedtuples.
model are transferable to generating natural lan-
guagecommonsenseknowledge. Efficiency of learning from seed tuples Be-
cause not all domains will have large available
commonsense KBs on which to train, we explore
Effect of decoding algorithm In Table 3, we
how varying the amount of training data avail-
showtheeffectofdifferentgenerationpolicieson
able for learning affects the quality and novelty
knowledge quality. The most interesting result
of the knowledge that is produced. Our results in
is that using greedy decoding to produce knowl-
Table 4 indicate that even with only 10% of the
edge tuples only results in a 10% relative perfor-
available training data, the model is still able to
mance gap compared to a human evaluation of
the ATOMIC test set, showing that the knowledge 7Thisnumberispartiallylowduetothemany“none"ref-
producedbythemodelapproacheshumanperfor- erences in the oEffect, oReact, oWant categories. In
anysetof10candidates,“none"canonlybepredictedonce,
mance. While producing more total candidates
whichcausesmostcandidatesinthebeamtobeincorrectif
does lower overall performance, quality assess- “none"istheappropriateanswer.
produce generations that are coherent, adequate, Model PPL Score N/Tsro N/To Human
and novel. Using only 1% of the training data LSTM-s - 60.83 86.25 7.83 63.86
CKBG(Saitoetal.,2018) - 57.17 86.25 8.67 53.95
clearlydiminishesthequalityoftheproducedgen-
COMET(-pretrain) 8.05 89.25 36.17 6.00 83.49
erations, with significantly lower observed results COMET-RELTOK 4.39 95.17 56.42 2.62 92.11
across both quality and novelty metrics. Interest- COMET 4.32 95.25 59.25 3.75 91.69
ingly,wenotethattrainingthemodelwithoutpre-
Table6: ConceptNetgenerationResults
trained weights performs comparably to training
with 10% of the seed tuples, quantifying the im-
pactofusingpre-trainedlanguagerepresentations. or → s to help augment a knowledge base com-
pletion model. It is only evaluated on the sr → o
5 ConceptNetExperiments tuple generation task, however. For posterity, we
alsoincludetheresultfromaLSTMmodelthatis
The ConceptNet dataset8, provided by Li et al.
onlytrainedonthesr → otask(LSTM-s).
(2016), consists oftuples obtainedfromthe Open
MindCommonSense(OMCS)entriesinConcept- Ablations We include the following ablations
Net 5 (Speer et al., 2017). Tuples are in the stan- of our full model. First, we evaluate how pre-
dard sro form – (e.g., take a nap, Causes, have training on a large-scale corpus (Radford et al.,
energy). The most confident 1200 tuples were 2018)helpsperformancebytrainingacomparison
used to create the test set, while the next 1200 model from scratch, denoted COMET (- pretrain)
tuples were used to create two development sets, in Table 6. Second, in our main model, we map
whichwecombineinthiswork. The100kversion relation names to natural language (e.g., IsA →
ofthetrainingsetwasusedtotrainmodels,which “is a”; HasSubevent → “has subevent”) so the
contains34relationtypes. model can learn to represent these concepts with
language,asopposedtolearningaspecialembed-
5.1 Setup ding from scratch for each relation (Levy et al.,
Metrics We evaluate our models that generate 2017). As an ablation, we train a model with-
ConceptNetrelationsusingthefollowingmetrics. out converting relation tokens to natural language
First,wereporttheperplexityofthegoldrelations (e.g., IsA (cid:54)→ “is a”), which we denote COMET -
inthetestset(PPL).Toevaluatethequalityofgen- RELTOK.
erated knowledge, we also report the number of
5.2 Results
generatedpositiveexamplesinthetestsetthatare
Quality Our results indicate that high-quality
scored as correct by the pre-trained Bilinear AVG
modeldevelopedbyLietal.(2016).9 Foragiven knowledgecanbegeneratedbythemodel: thelow
perplexity scores in Table 6 indicate high model
sro tuple, this model produces a probability for
confidence in its predictions, while the high clas-
whether the tuple is correct. We threshold scores
sifier score (95.25%) indicates that the KB com-
at50%probabilitytoidentifypositivepredictions.
pletionmodelofLietal.(2016)scoresthegener-
On the completion task originally proposed in Li
ated tuples as correct in most of the cases. While
etal.(2016),thismodelachieved92.5%accuracy
adversarial generations could be responsible for
on the test set, indicating that it is a strong proxy
this high score, a human evaluation (following
for automatically evaluating whether a generated
tuple is correct. Finally, we report the same nov-
the same design as for ATOMIC) scores 91.7% of
greedily decoded tuples as correct. Randomly se-
eltymetricsasfor ATOMIC: N/TsroandN/To.
lected examples provided in Table 7 also point to
Baselines As a baseline, we re-implement thequalityofknowledgeproducedbythemodel.
the BiLSTM model proposed by Saito et al.
Novelty In addition to being high quality, the
(2018) with minor modifications outlined in Ap-
generatedtuplesfromCOMETarealsonovel,with
pendix A.2. This model is trained to learn to en-
59.25%ofthetuplesnotbeingpresentinthetrain-
code knowledge in both directions: sr → o and
ingset, showingthatthemodeliscapableofgen-
8https://ttic.uchicago.edu/~kgimpel/ erating new edges between nodes, and even cre-
commonsense.html ating new nodes – 3.75% of o nodes are novel –
9 A pre-trained model can be found at https:
to extend the size of the knowledge graph. One
//ttic.uchicago.edu/~kgimpel/comsense_
resources/ckbc-demo.tar.gz shortcoming, however, is that novel generations
100% 1.00 Seed Relation Completion Plausible
piece PartOf machine (cid:88)
bread IsA food (cid:88)
75% 0.75 oldsmobile IsA car (cid:88)
happiness IsA feel (cid:88)
math IsA subject (cid:88)
50% 0.50 mango IsA fruit (cid:88)
maine IsA state (cid:88)
planet AtLocation space (cid:88)
dust AtLocation fridge
25% 0.25
puzzle AtLocation yourmind
% of novel tuples college AtLocation town (cid:88)
Accuracy dentalchair AtLocation dentist (cid:88)
0% 0.00 finger AtLocation yourfinger
0.0 0.33 0.5 0.67 1.0
sing Causes youfeelgood (cid:88)
Edit Distance doctor CapableOf savelife (cid:88)
postoffice CapableOf receiveletter (cid:88)
Figure 4: The percentage of novel ConceptNet de- dove SymbolOf purity (cid:88)
velopment set tuples per minimum edit distance from sun HasProperty big (cid:88)
birdbone HasProperty fragile (cid:88)
trainingtuples. Ingreen: classifier-scoredaccuracyof earth HasA manyplant (cid:88)
eachsubset. yard UsedFor playgame (cid:88)
getpay HasPrerequisite work (cid:88)
printonprinter HasPrerequisite getprinter (cid:88)
playgame HasPrerequisite havegame (cid:88)
live HasLastSubevent die (cid:88)
aresometimessimplifiedformsoftuplesfromthe swim HasSubevent getwet (cid:88)
training set. In Table 7, for example, the tuple sitdown MotivatedByGoal youbetire (cid:88)
allpaper ReceivesAction recycle (cid:88)
“doctor CapableOf save life” is not present in chair MadeOf wood (cid:88)
earth DefinedAs planet (cid:88)
the training set, but “doctor CapableOf save
person life” is. Many tuples, however, are com- Table 7: Randomly selected and novel generations
pletelynovel,suchas“birdboneHasProperty from the ConceptNet development set. Novel genera-
fragile” and “driftwood AtLocation beach”, tionsaresrotuplesnotfoundinthetrainingset. Man-
ualevaluationofeachtupleindicateswhetherthetuple
whichhavenorelatedtuplesinthetrainingset.
isconsideredplausiblebyahumanannotator
Toexplorefurther,weinvestigatebyhowmuch
noveltuplesfromthedevelopmentsetdifferfrom
training set phrase objects for the same s,r using
model. Qualitatively,weobservethiseffectinTa-
minimumeditdistanceofphraseobjects. Wemea-
ble 7 with the generated example tuple “mango
sure the edit distance of phrase object o in the
dev IsAfruit",whichisnotpresentinthetrainingset.
tuple(s,r,o )totheo fromthenearesttrain-
dev trn The only tuple containing the “mango" entity in
ingtuple(s,r,o ). Editdistanceismeasuredus-
trn thetrainingsetis“mangoUsedForsalsa",which
ing word tokens (excluding stop words) and nor-
is not informative enough. As confirmation, we
malizedbythemaximumnumberofwordsino
dev observethattheoutputfromCOMET(-pretrain)is
or o . The maximum edit distance is one (i.e.,
trn “mango IsA spice”, which could be a reasonable
entirely different word sequences) and the mini-
inferencegiventheinformationabout“mango"in
mumeditdistanceiszero(i.e.,thesamesequence
theseedsetofknowledge.
excludingstopwords). Figure4showsthepercent-
age of novel development set tuples that have an
Representing relations with language While
edit distance from the closest training set tuple of
theautomaticmetricspointtoinsignificantdiffer-
at least the value on the x-axis. Over 75% of the
ences when comparing models with symbol re-
noveltupleshaveobjectsthatareanormalizededit
lations and those with natural language relations
distance of >= 0.5 from the training phrase ob-
(Table 6), examples can provide qualitative in-
jects, indicating that most of the novel phrase ob-
sightsintothebenefitsofrepresentingrelationsas
jects have significantly different word sequences
language. While the only non-ornithological ref-
fromtheirclosestanaloguesinthetrainingset.
erence to a “dove" in the ConceptNet training set
Learning knowledge from language Simi- is “dove CapableOf fly”, our model learns to
larly to ATOMIC, we explore how pre-training generalizetoproducethetuple“doveSymbolOf
COMET on a large language corpus affects its purity”. The model that uses symbol relation em-
ability to generalize commonsense. This effect beddings only manages to produce the relation
is apparent in Table 6, with a clear improve- “dove SymbolOf submarine”, which seems to
ment on automatic and human evaluations by the relate “submarine" to a more nautical (and unre-
pretrained COMET over the randomly initialized lated)wordsenseof“dove".
X
=>
ecnatsid
tide
htiw
selput
fo
%
ycaruccA
refiissalC
6 RelatedWork monsense KB construction by generating new
graphnodesandedgesbetweennodes.
Knowledge base construction Previous work
haslookedatconstructingknowledgebasesasre-
7 Conclusion
lational schemas using expert knowledge (Lenat,
1995; Bodenreider, 2004; Miller, 1995), semi- We introduce COMmonsense Transformers
structured text extraction (Suchanek et al., 2007; (COMET)forautomaticconstructionofcommon-
Hoffart et al., 2013; Auer et al., 2007; Bol- sense knowledge bases. COMET is a framework
lackeretal.,2008)andunstructuredtextextraction for adapting the weights of language models to
(Dongetal.,2014;Carlsonetal.,2010;Nakashole learn to produce novel and diverse common-
etal.,2011,2012;Niu,2012). Inourwork,wefo- sense knowledge tuples. Empirical results on
cus on construction of commonsense knowledge two commonsense knowledge bases, ATOMIC
bases which require the use of open-text events and ConceptNet, show that COMET frequently
ratherthanawell-definedrelationalschemastruc- produces novel commonsense knowledge that
ture. Other work in information extraction can human evaluators deem to be correct. These
also be applied to knowledge base construction positive results point to future work in extend-
withopen-textentities(Soderlandetal.,2010;Et- ing the approach to a variety of other types of
zionietal.,2011;Faderetal.,2011;Mausametal., knowledgebases,aswellasinvestigatingwhether
2012;Fanetal.,2010;Cuietal.,2018), butthese COMET can learn to produce OpenIE-style
methods typically extract explicitly stated text re- knowledgetuplesforarbitraryknowledgeseeds.
lations. Conversely, our approach generates new
knowledge that is often unstated in text, as com- Acknowledgments
monsense information typically is (Gordon and
We thank Thomas Wolf, Ari Holtzman, Chandra
VanDurme,2013).
Bhagavatula, Peter Clark, Rob Dalton, Ronan Le
Commonsense knowledge base completion Bras,RowanZellersandScottYihforhelpfuldis-
Existing work on generation of novel common- cussionsoverthecourseofthisproject,aswellas
sense knowledge has also used ConceptNet and theanonymousreviewersfortheirinsightfulcom-
ATOMIC asunderlyingKBs. Specifically,Lietal. ments. This research was supported in part by
(2016) proposed a set of neural network models NSF (IIS-1524371, IIS-1714566, NRI-1525251),
forscoringtuplesinConceptNet. Ourworkdiffers DARPAundertheCwCprogramthroughtheARO
fromthisapproachastheirmodelsevaluatefulltu- (W911NF-15-1-0543), and Samsung Research.
plesratherthanlearningtogeneratethephrasesto This material is based, in part, upon work sup-
make new nodes in the knowledge graph. Saito portedbytheNationalScienceFoundationGradu-
etal.(2018)buildsuponthisworkbyproposinga ateResearchFellowshipProgramunderGrantNo.
jointmodelforcompletionandgenerationofcom- DGE-1256082.
monsensetuples. Theirwork,however,focuseson
using tuple generation to augment their KB com-
pletion model, rather than to increase coverage in References
commonsenseKBconstruction. Finally,Sapetal.
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
(2019)useLSTMencoder-decodermodelstogen- Lehmann, Richard Cyganiak, and Zachary G. Ives.
eratecommonsenseknowledgeaboutsocialsitua- 2007. Dbpedia: A nucleus for a web of open data.
InISWC/ASWC.
tions. We use transformers and investigate the ef-
fect of using pre-trained language representations
JimmyBa,RyanKiros,andGeoffreyE.Hinton.2016.
(Radfordetal.,2018)toinitializethem.
Layernormalization. CoRR,abs/1607.06450.
Transformers and pre-training Finally, our
Olivier Bodenreider. 2004. The unified medical lan-
work builds on previous work on adapting pre- guagesystem(umls): Integratingbiomedicaltermi-
trained language models for various sequence la- nology. Nucleicacidsresearch,32:D267–70.
beling, classification, and NLI end tasks (Rad-
PiotrBojanowski,EdouardGrave,ArmandJoulin,and
ford et al., 2018; Peters et al., 2018; Devlin et al.,
TomasMikolov.2017. Enrichingwordvectorswith
2018). Our research investigates how pre-trained
subword information. Transactions of the Associa-
languagemodelscanbeusedforlarge-scalecom- tionforComputationalLinguistics,5:135–146.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sture Holm. 1979. A simple sequentially rejective
Sturge, and Jamie Taylor. 2008. Freebase: A col- multiple test procedure. Scandinavian Journal of
laboratively created graph database for structuring Statistics,6(2):65–70.
human knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Man- DouglasBLenat.1995. Cyc: Alarge-scaleinvestment
agement of Data, SIGMOD ’08, pages 1247–1250, inknowledgeinfrastructure. Communicationsofthe
NewYork,NY,USA.ACM. ACM,38(11):33–38.
AndrewCarlson,JustinBetteridge,BryanKisiel,Burr Omer Levy, Minjoon Seo, Eunsol Choi, and Luke S.
Settles, Estevam R. Hruschka, Jr., and Tom M. Zettlemoyer.2017. Zero-shotrelationextractionvia
Mitchell. 2010. Toward an architecture for never- readingcomprehension. InCoNLL.
ending language learning. In Proceedings of the
Xiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel.
Twenty-Fourth AAAI Conference on Artificial Intel-
2016. Commonsense knowledge base completion.
ligence,AAAI’10,pages1306–1313.AAAIPress.
InACL,volume1,pages1445–1455.
LeiCui,FuruWei,andMingZhou.2018. Neuralopen
informationextraction. InACL. Mausam,MichaelSchmitz,StephenSoderland,Robert
Bart,andOrenEtzioni.2012. Openlanguagelearn-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and ingforinformationextraction. InEMNLP-CoNLL.
KristinaToutanova.2018. Bert:Pre-trainingofdeep
bidirectional transformers for language understand- George A. Miller. 1995. Wordnet: A lexical database
ing. arXivpreprintarXiv:1810.04805. forenglish. Commun.ACM,38(11):39–41.
XinDong,EvgeniyGabrilovich,GeremyHeitz,Wilko NdapandulaNakashole,MartinTheobald,andGerhard
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Weikum.2011. Scalableknowledgeharvestingwith
Shaohua Sun, and Wei Zhang. 2014. Knowledge high precision and high recall. In Proceedings of
vault: Aweb-scaleapproachtoprobabilisticknowl- the Fourth ACM International Conference on Web
edge fusion. In Proceedings of the 20th ACM Search and Data Mining, WSDM ’11, pages 227–
SIGKDD International Conference on Knowledge 236,NewYork,NY,USA.ACM.
DiscoveryandDataMining, KDD’14, pages601–
Ndapandula Nakashole, Gerhard Weikum, and Fabian
610,NewYork,NY,USA.ACM.
Suchanek. 2012. Patty: A taxonomy of relational
Oren Etzioni, Anthony Fader, Janara Christensen, patterns with semantic types. In Proceedings of
StephenSoderland,andMausam.2011. Openinfor- the 2012 Joint Conference on Empirical Methods
mationextraction:Thesecondgeneration. InIJCAI. inNaturalLanguageProcessingandComputational
NaturalLanguageLearning,pages1135–1145.As-
Anthony Fader, Stephen Soderland, and Oren Etzioni. sociationforComputationalLinguistics.
2011. Identifyingrelationsforopeninformationex-
traction. InProceedingsoftheconferenceonempir- FengNiu.2012. Web-scaleKnowledge-baseConstruc-
icalmethodsinnaturallanguageprocessing,pages tion via Statistical Inference and Learning. Ph.D.
1535–1545.AssociationforComputationalLinguis- thesis,Madison,WI,USA. AAI3524067.
tics.
EricWNoreen.1989. Computerintensivemethodsfor
James Fan, David A. Ferrucci, David Gondek, and hypothesistesting: Anintroduction. Wiley,NY.
Aditya Kalyanpur. 2010. Prismatic: Inducing
Jeffrey Pennington, Richard Socher, and Christo-
knowledgefromalargescalelexicalizedrelationre-
source. InNAACL-HLT2010. pher D. Manning. 2014. Glove: Global vectors for
wordrepresentation. InEMNLP.
JonathanGordonandBenjaminVanDurme.2013. Re-
portingbiasandknowledgeacquisition. InProceed- Matthew E. Peters, Mark Neumann, Mohit Iyyer,
ingsofthe2013workshoponAutomatedknowledge Matthew Gardner, Christopher Clark, Kenton Lee,
baseconstruction,pages25–30.ACM. and Luke S. Zettlemoyer. 2018. Deep contextual-
izedwordrepresentations. CoRR,abs/1802.05365.
Dan Hendrycks and Kevin Gimpel. 2016. Bridging
nonlinearitiesandstochasticregularizerswithgaus- AlecRadford,KarthikNarasimhan,TimSalimans,and
sianerrorlinearunits. CoRR,abs/1606.08415. Ilya Sutskever. 2018. Improving language under-
standingbygenerativepre-training. URLhttps://s3-
SeppHochreiterandJürgenSchmidhuber.1997. Long us-west-2.amazonaws.com/openai-assets/research-
short-termmemory. NeuralComputation,9(8). covers/languageunsupervised/language under-
standingpaper.pdf.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A Itsumi Saito, Kyosuke Nishida, Hisako Asano, and
spatially and temporally enhanced knowledge base JunjiTomita.2018. Commonsenseknowledgebase
from wikipedia. Artificial Intelligence, 194:28 – completion and generation. In Proceedings of the
61. Artificial Intelligence, Wikipedia and Semi- 22nd Conference on Computational Natural Lan-
StructuredResources. guageLearning,pages141–150.
Maarten Sap, Ronan LeBras, Emily Allaway, Chan-
draBhagavatula,NicholasLourie,HannahRashkin,
BrendanRoof,NoahASmith,andYejinChoi.2019.
Atomic: An atlas of machine commonsense for if-
thenreasoning. InAAAI.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open
information extraction to domain-specific relations.
AIMagazine,31:93–102.
Robyn Speer, Joshua Chin, and Catherine Havasi.
2017. Conceptnet 5.5: An open multilingual graph
ofgeneralknowledge. InThirty-FirstAAAIConfer-
enceonArtificialIntelligence.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge. InProceedingsofthe16thInternationalCon-
ferenceonWorldWideWeb,WWW’07,pages697–
706,NewYork,NY,USA.ACM.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. InAdvancesinNeuralInformationProcess-
ingSystems.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
youneed. InNIPS.
A AdditionalTrainingDetails Wikipedia. With no reference to the precise
corpus used, we opted to use Glove embed-
A.1 TrainingHyperparameters
dingstoinitializethewordembeddingsofthe
ATOMIC For ATOMIC, we use a maximum encoderanddecoderinstead.
learning rate of 6.25e-5 with a warmup period
2. We use the Adam optimizer with learning
of 100 minibatches. After, we decay the learn-
rateof0.0001,ratherthanSGDwithalearn-
ing rate linearly until the end of training. We
ing rate of 1.0 because after training both
train for 50k minibatches and use early stopping.
models, we found that the Adam-trained
We clip gradients when their norm is greater than
model performed better on development set
1. The remainder of our hyperparameters are the
perplexity. We also do not use weight de-
same as in Radford et al. (2018). We use the
cay, as this seemed to lower validation per-
public HuggingFace implementation of the GPT
formance,aswell.
model as a base for our experiments available
at: https://github.com/huggingface/
3. We do not train the generation model jointly
pytorch-openai-transformer-lm.
with the completion model. We only train
ConceptNet For ConceptNet, we use a maxi- an individual generator. The results of Saito
etal.(2018)didnotshowasignificantdiffer-
mum learning rate of 1e-5 and a warm-up period
ence in generation performance between the
of 200 minibatches. The learning rate is decayed
twoontheConceptNetdataset.
linearly until the end of training, which lasts for
100k minibatches. All other hyperparameters are
4. We train a second baseline (LSTM - s) that
thesameasfortrainingonthe ATOMICcorpus.
doesnotlearntoproducerelationsinbothdi-
rections(i.e.,sr → oandor → s). Insteadif
A.2 ConceptNetbaseline
onlylearnsparametersthatcanproducerela-
We train the ConceptNet baseline with a learning
tionsintheforwarddirection(sr → o)
rate of 1e-4 for 100k minibatches. Early stopping
isusedwiththevalidationloss. SimilarlytoSaito 5. We do not decay the learning rate because it
et al. (2018), we use 200-dimension hidden states wasunclearfromtheoriginalpaperwhatthe
and200-dimensionalwordembeddings. Weusea exactlearningrateschedulewas.
single-layer bidirectional LSTM (Hochreiter and
Schmidhuber,1997)toencodethefirstphraseand
B AdditionalEvaluationDetails
a single-layer unidirectional LSTM to decode the
target phrase. Relation embeddings are concate- B.1 HumanEvaluations
nated with the word embeddings of the decoder
We used Amazon Mechanical Turk to get ratings
before being input to the decoder LSTM. We set
of model output accuracy. We selected seed con-
thedropoutrateto0.2beforetheoutputprojection
ceptsandrelationsfromthetestsetandgenerated
layer and after the word embedding layers. We
completions using each model to create (s,r,o)
outline the following differences between our re-
tuples. For ATOMIC,weselectedtuplesbychoos-
implementationofthemodelofSaitoetal.(2018)
ing all possible relations (9) for each of 100 ran-
and their original implementation and the reason
domly selected seed concepts (900 total (s,r)
forthechange.
pairs) following the procedure from Sap et al.
(2019). For ConceptNet, we used the full test set
1. We use Glove (Pennington et al., 2014) em-
(1200total(s,r)pairs).
beddings rather than fastText embeddings
For Beam-2/5/10 and top-5/10 sampling gener-
(Bojanowski et al., 2017) to initialize word
ations, we used the model to generate 2, 5, or 10
embeddings. Because the model indicated
(respectively) possible completions (o) per (s,r)
that200-dimensionalwordembeddingswere
pair. Workers were shown the full set and asked
used, we could not use the pretrained em-
toselectalloftheothatarevalidcompletionsfor
beddings provided by the fastText group1.
the (s,r) pair. Each set of tuples was rated by 5
In Saito et al. (2018), the authors de-
workers.
scribedtrainingtheirfastTextembeddingson
For greedy sampling generations, we used the
1https://fasttext.cc/ modeltogenerateonepossiblecompletion(o)per
(s,r)pair. Workerswereshownthecompletedtu- generating a phrase for a xWant relation would
ple (s,r,o) and asked whether it is valid or not. receive the <X> and <POST> tokens as input,
Eachtuplewasratedby5workers. but not <Involuntary>). Depending on the
We measure accuracy as the percentage of dis- relation for a particular training example (e.g.,
tinct worker responses where the (s,r,o) tuple is xReact), a set of meta-tokens are appended to
markedasvalid(i.e., #valid ). the relation tokens, Xr, that provide hierarchi-
5·|(s,r,o)|
cal relational information, allowing the model to
C ExampleOutputs share information across relation types. We pro-
vide a more in-depth description of the category
Additional examples can be seen in Figures 5,
hierarchy training combinations in Table 10. Re-
6, and 7 that are produced using the demo at
sultsonhumanevaluationmetricsareprovidedin
https://mosaickg.apps.allenai.
Table 12. Because the model with the hierarchi-
org.
cal meta-tokens performed worse than the regular
D AdditionalTrainingExperiments COMET,wedidnotrunadditionalexperimentson
thisablations.
In addition to the more naive setups for knowl-
edgegraphcompletion,weexplorevariousmulti-
taskandhierarchicallearningsetupsontopofthe
taxonomyofcommonsenserelationsgivenbySap
et al. (2019), which group together along vari-
ous axes (e.g., related to agent/theme, related to
causes/effects,etc.).
D.1 Multi-relationTraining
FortheATOMICcorpus,weexperimentwithmul-
tiplemulti-tasktrainingsetups,similartoSapetal.
(2019). First, we train an individual model for
each relation type (oReact, oEffect, etc.),
whichwedenoteasCOMET-9LMintheTable9.
We also experiment with various information-
sharingdatasetconfigurationsthatorganizediffer-
entrelationsacrosscommondimensions. Weout-
linethesedimensionsandthemakeupofeachsplit
inTable9. ForConceptNet,allmodelsarealways
trained on all relation types jointly. Results on
automatic evaluation metrics are provided in Ta-
ble 11. Because there did not seem to be signif-
icant differences betweenthese performances and
thatofCOMET- FULL, wedidnotrunadditional
experimentsontheseablations.
D.2 ConceptHierarchyTraining
Leveraging the prior knowledge that certain re-
lation types in the ATOMIC knowledge graph
are linked to each other, we explore provid-
ing these group identities as additional tokens
in the relation. For example, when generating
the completion of a xReact relation, the model
wouldreceiveasinputthefollowingmeta-tokens:
<xReact>, <X>, <POST>, <Involuntary>
– thereby providing common context with other
relationsthatarepartofthesamegroupings(e.g.,
Figure5:Exampleoutputsfortheevent"PersonXgivesPersonYapeptalk"fromCOMETtrainedontheATOMIC
knowledgegraph
Figure6: Exampleoutputsfortheevent"Ericwantstoseeamovie"fromCOMETtrainedontheATOMICknowl-
edgegraph. COMETisabletogeneralizebeyondthetemplatesofthe ATOMIC knowledgegraph(i.e., PersonX)
andcanbeuseddirectlywithnames.
Figure7: Exampleoutputsfortheevent"TomaskedJessicaifhecouldusehercar"fromCOMETtrainedonthe
ATOMICknowledgegraph
Event Description ExampleCompletion:
PersonXputsPersonX’strustinPersonY
oEffect Theeffecttheeventhasonothersbe- isconsideredtrustworthy
sidesPersonX isbelieved
gainsPersonX’sloyalty
oReact ThereactionofothersbesidesPerson trusted
Xtotheevent honored
trustworthy
oWant What others besides Person X may workwithPersonX
wanttodoaftertheevent partnerwithPersonX
tohelpPersonX
xAttr How Person X might be described faithful
giventheirpartintheevent hopeful
trusting
xEffect Theeffectthattheeventwouldhave getsrelieved
onPersonX staysfaithful
Isbetrayed
xIntent The reason why X would cause the tobetrusting
event hisorherhelp/guidance/advice
tobefriends
xNeed WhatPersonXmightneedtodobe- tobefriendswithPersonY
foretheevent tohaveheardalotofgoodthingsaboutPer-
sonY
togettoknowPersonY
xReact The reaction that Person X would trusting
havetotheevent safe,notalone
understood
xWant WhatPersonXmaywanttodoafter torelyonPersonY
theevent togointobusinesswithPersonY
tomakesurethattheirheartfeelingisright
Table 8: Definitions of the relations in ATOMIC. Events in ATOMIC center around the personal situations of a
centralfigure,PersonX,withpotentiallymoreparticipants.
Organization Description Relations
PERSON The training set is split into relations T
1
={xAttr, xEffect, xIntent,
X/Y forthesubjectsoftheevent(PersonX) xNeed, xReact, xWant}
and relations for other participants in T ={oEffect, oReact, oWant}
2
theevent
PRE/POST Event preconditions are jointly trained T
1
={xIntent, xNeed}
(i.e.,intentions,needs). Eventpostcon- T ={oEffect, oReact, oWant,
2
ditionsarejointlytrained. xEffect, xReact, xWant}
(IN)VOLUN Involuntaryrelationsaretrainedjointly, T
1
={oWant, xIntent, xNeed, xWant}
such as reactions and effects. Volun- T ={oEffect, oReact, xAttr,
2
taryrelationsaretrainedjointly,suchas xEffect, xReact}
needs,wants,andintents.
FULL Thetrainingsetismadeupofallrela- T
1
={oEffect, oReact, oWant, xAttr,
tionsandthemodelistrainedjointlyon xEffect, xIntent, xNeed, xReact,
allofthem xWant}
Table 9: Multi-relation training setups. Following Sap et al. (2019), the xAttr relation is not included in the
PRE/POSTtrainingconfiguration
Meta-Token Description Relations
<X> Appended to relations that describe an xAttr, xEffect, xIntent, xNeed,
attributeofPersonX xReact, xWant
<Y> Appendedtorelationsthatdescribesan oEffect, oReact, oWant
attributeofaparticipantthatisnotPer-
sonX
<Pre> Appended to relations that correspond xIntent, xNeed
topre-conditionsoftheevent
<Post> Appended to relations that correspond oEffect, oReact, oWant,
topost-conditionsoftheevent xEffect, xReact, xWant
<Voluntary> Appended to relations that correspond oWant, xIntent, xNeed, xWant
tovoluntarydimensionsofthesituation
<Involuntary> Appended to relations that correspond oEffect, oReact, xAttr,
toinvoluntarydimensionsofthesitua- xEffect, xReact
tion
Table10:Categoryhierarchymeta-tokens,alongwiththedescriptionandtherelationstowhichtheyareappended
Model PPL3 BLEU-2 N/Tsro4 N/To N/Uo
COMET-9LM 11.72 14.89 100.00 9.45 49.89
COMET-(IN)VOLUN 11.38 14.99 100.00 8.60 48.36
COMET-PERSONX/Y 11.30 15.21 100.00 9.12 49.59
COMET-PRE/POST 11.35 14.88 100.00 9.86 51.86
COMET-FULL(-pretrain) 15.42 13.88 100.00 7.25 45.71
COMET-FULL 11.14 15.10 100.00 9.71 51.20
COMET-FULL(+hierarchymeta-tokens) 10.98 15.27 100.00 10.03 51.97
Table11: AutomaticevaluationsofqualityandnoveltyforgenerationsofATOMICcommonsensethataretrained
withthetrainingsetsplitalongdifferentrelationtypes. ThetrainingsplitsareoutlinedinTable9.
Model oEffect oReact oWant xAttr xEffect xIntent xNeed xReact xWant Total
COMET 29.02 37.68 44.48 57.48 55.50 68.32 64.24 76.18 75.16 56.45
COMET(+hierarchymeta-tokens) 28.46 38.96 43.64 51.90 50.84 63.00 63.98 66.20 75.82 53.64
Table12: HumanscoreofgenerationsofATOMICcommonsensefortheregularCOMETmodelandtheCOMET+
categorymetatokens

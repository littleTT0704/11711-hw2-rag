AnEmpiricalAnalysisofMemorizationin
Fine-tunedAutoregressiveLanguageModels
FatemehsadatMireshghallah1,ArchitUniyal2,TianhaoWang2,
∗
DavidEvans2,TaylorBerg-Kirkpatrick1
1UniversityofCaliforniaSanDiego,2UniversityofVirginia
[fatemeh, tberg]@ucsd.edu,
[a.uniyal,tianhao,evans]@virginia.edu
Abstract (cid:20)(cid:17)(cid:19) (cid:55)(cid:85)(cid:68)L(cid:81)L(cid:81)(cid:74)(cid:3)3(cid:75)(cid:68)V(cid:72)
(cid:11)(cid:20)(cid:12)(cid:3))L(cid:87)(cid:87)L(cid:81)(cid:74)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)0(cid:72)PR(cid:85)L(cid:93)(cid:68)(cid:87)LR(cid:81)
Several recent works have shown that large (cid:19)(cid:17)(cid:27) (cid:11)(cid:21)(cid:12)(cid:3)0(cid:72)PR(cid:85)L(cid:93)(cid:68)(cid:87)LR(cid:81)(cid:3)2(cid:81)O(cid:92)
languagemodelspresentprivacyrisksthrough (cid:11)(cid:22)(cid:12)(cid:3)2(cid:89)(cid:72)(cid:85)IL(cid:87)(cid:87)L(cid:81)(cid:74)
memorizationoftrainingdata.Littleattention, (cid:19)(cid:17)(cid:25)
however,hasbeengiventothefine-tuningphase
anditisnotwellunderstoodhowmemorization (cid:19)(cid:17)(cid:23)
riskvariesacrossdifferentfine-tuningmethods
(cid:19)(cid:17)(cid:21)
(suchasfine-tuningthefullmodel,themodel
head,andadapter).Thispresentsincreasingcon-
(cid:21)(cid:20) (cid:21)(cid:21) (cid:21)(cid:22) (cid:21)(cid:23) (cid:21)(cid:24) (cid:21)(cid:25)
cernasthe“pre-trainandfine-tune”paradigm (cid:57)(cid:68)OL(cid:71)(cid:68)(cid:87)LR(cid:81)(cid:3)33(cid:47)
proliferates. Weempiricallystudymemoriza-
Figure1:Eachpointinthegraphshowsthegivenmetric
tionoffine-tuningmethodsusingmembership
valuesattheendofeachtrainingepoch.Therightmost
inferenceandextractionattacks,andshowthat
lowerpointsshowthebeginning,andaswemovetoleft
theirsusceptibilitytoattacksisverydifferent.
andupwardstrainingprogresses.Weidentifythreesep-
We observe that fine-tuning the head of the
aratephaseswithinthelearningprocess,distinguished
modelhasthehighestsusceptibilitytoattacks,
bytheirmemorizationandgeneralizationtrends.
whereasfine-tuningsmalleradaptersappearsto
belessvulnerabletoknownextractionattacks.
the severity of this issue by extracting complete
trainingsequencesandinferringmembershipofa
1 Introduction
largefractionofthetrainingsamples.
Transformer-basedlanguagemodelshavebecome
These works focused on memorization during
the models of choice for many NLP tasks, such
pre-training,butscantattentionhasbeengivento
asemail,textandcodeauto-completion,question
fine-tuning. In this work, we focus on different
answering and sentiment analysis (Chen et al.,
fine-tuning methods and their propensity for
2021,2019).Thesemodelsarecommonlytrained
memorizationoftrainingsamples.Fine-tuningdata
usingthepre-trainandfine-tuneparadigm,where
isactuallyofhigherconcernthanpre-trainingdata,
they are first trained (pre-trained) on a large,
since most pre-training datasets are large public
general domain dataset (in the order of hundreds
corpora(Raffeletal.,2019;Dodgeetal.,2021)with
of Gigabytes), and then fine-tuned on smaller,
limitedprivacyconcerns(Brownetal.,2022),while
task-specific datasets to adapt the model to a
fine-tuningsetsaresmall,targeted,andpotentially
specificdomain(RamponiandPlank,2020;Liand
very private (Basu et al., 2021; Li et al., 2021).
Liang,2021;Houlsbyetal.,2019).
Further,pre-traininggenerallyhappensonlyafew
Several works have demonstrated that such
times (as it needs resources that are usually only
largemodelshaveahighcapacityformemorizing
availabletolargecompanies(Brownetal.,2020))
trainingsamplesduringpre-trainingandarethere-
whilefine-tuningisincreasinglythedominantway
fore highly susceptible to membership inference
thatend-usersfitmodels.
and data extraction attacks (Zanella-Béguelin
Given the size of these large language models,
etal.,2020;Carlinietal.,2021b;Nakamuraetal.,
fine-tuning all the model parameters can be
2021).Morespecifically,Carlinietal.(2021b)and
compute and memory-intensive (Lewis et al.,
Mireshghallah et al. (2022) have mounted such
2019;Brownetal.,2020;Fedusetal.,2021). As
attacksonpre-trainedlanguagemodelsandshown
aresult,recentworkshaveproposednewparameter
∗Correspondingauthoremail:fatemeh@ucsd.edu efficient fine-tuning methods that update only a
1816
Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1816-1826
December7-11,2022©2022AssociationforComputationalLinguistics
OO(cid:68)F(cid:72)5(cid:3)(cid:36)(cid:44)0
subset of the model’s parameters (Houlsby et al., (1)fine-tuningthemodelhead,i.e.,theprediction
2019; Li and Liang, 2021; He et al., 2022). In layer, as it is the most common method used in
thispaper,wefocusonstudyingmemorizationof practice, and (2) fine-tuning adapters (Houlsby
threepopularfine-tuningmethods:(1)fine-tuning et al., 2019). Adapters are small rank-restricted
all model parameters (2) fine-tuning the head, modulesthatareinsertedinsidetransformerblocks,
which is commonly used by practitioners and asaddedparametersandarefine-tunedfordifferent
involvesupdatingonlythelastlayerofthemodel tasksordatasets.Theshapeandsizeoftheadapter
which produces the logits, and (3) fine-tuning moduleiscontrolledbythereductionfactor,which
adapters (Houlsby et al., 2019), which are small determines the ratio of the size of the bottleneck
bottleneck modules inserted within transformer toitsinput. Duringadaptertuning,therestofthe
blocks. For measuring memorization, we use modelremainsfrozen,thereforethenumberoftrain-
twoproxymetrics: (a)recallofareference-based ableparametersislow(around1%ofthefullmodel
membershipinferenceattack(MIA)(Mireshghal- parameters).Inourexperiments,wechoosereduc-
lah et al., 2022) and (b) exposure (Carlini et al., tionfactorsof16and2,foradapters,astheformer
2019),whichmeasureshowsusceptiblethemodel isthedefaultusedby(Pfeifferetal.,2020;Houlsby
is to a sample extraction attack which tries to etal.,2019),andthelatteristhelargestfactor.
reconstructsamplesfromtrainingdata.Werunour
3 MeasuringMemorization
experimentsontheWikipedia(Merityetal.,2016),
Penn Treebank (Marcus et al., 1993) and Enron To measure memorization, we use two metrics:
Emails(KlimtandYang,2004)datasets,forthetask membershipinferenceattackrecallandexposure.
ofautoregressivelanguagemodeling.Weselected
Membership Inference (MIA Recall). We use
Wikipedia and Penn Treebank as they are most
thepercentageoftrainingsamplesthatarecorrectly
commonlyusedforfine-tuning,andEnronsinceitis
classifiedastrainingmembers(outofapooloftrain-
adatasetofemailsrepresentingprivatetuningdata.
ingandvalidationsamples)bythereference-based
Figure 1 shows how we conceptually identify
attackproposedinMireshghallahetal.(2022)and
three distinct phases in the fine-tuning process,
Carlinietal.(2021a)asaproxymetricofmemo-
basedonvalidationperplexity(generalization)and
rization. For each sample x whose membership
membershipinferenceattackrecall(memorization).
inthetrainingsetwewanttodetermine, wefeed
Each point shows these metrics at the end of a
ittothefine-tunedmodel,M,andgetitslikelihood,
trainingepoch.Forallfine-tuningmethods,weob- PrM(x).Wealsofeedittoareferencemodel,R,a
servethatinamemorizationonlyphase,themodel
pre-trainedmodelthatisnotfine-tuned,andgetthe
memorizesmoreandmore,withoutoverfittingor probabilityPrR(x). WethenuseLR(x)= PrR(x) ,
generalizingbetter(Figure2).Intermsofdifferent PrM(x)
thelikelihoodratio,todetermineifxisatraining
fine-tuning methods, we find that the common
sample. IfLR(x)issmallerthanthresholdt, we
practiceoffine-tuningonlytheheadofamodelhas
classify it as a training set member. Otherwise,
thehighestmemorization(byalargemargin)forthe
weclassifyitasanon-member. Wedeterminethe
samelevelofperplexity,amongdifferentfine-tuning
threshold t by calculating LR(s) for all s in the
methods–evenfullfine-tuning,whichupdatesmore
validationset,andthenchoosethethresholdtobe
parameters.Thisresultissurprisingandpotentially
the highest threshold such that the false positive
indicatesthatonlytuningparametershigherinthe
rate(overtrainingandvalidationmembers)would
model architecture (closer to the output) exacer-
notexceed10%.Thehighertherecallofthisattack
batesthememorizationandincreasestheleakage
is,thehighertheleakageofthemodel.
basedonourmetrics.Wealsoshowthatfine-tuning
thefullmodelandsmalladaptersareonthePareto- Exposure. Asasecondmeasureofmemorization,
frontierintermsoftheattackrecallvs.validation weusetheexposuremetricfromCarlinietal.(2019)
perplexitygraph. Codeandinstructionstorepro- whichinsertsasecret(canary)ofacertainformat
duceourresultsareavailableathttps://github. intothetrainingdataandcalculatesitsvulnerability
com/mireshghallah/ft-memorization/. toextraction. Exposureisdefinedasthenegative
log-rank of the inserted secret in terms of model
2 ModelFine-tuning
probability,amongallotherpossiblesequencesof
Wefocusontwomainfinetuningmethods,forfine- thesamelength. Thisquantityisthenaddedtoa
tuningGPT-2withnextwordpredictionobjective: constanttoensuretheexposureisalwayspositive.
1817
1.0 1.0 hue
Full FT
0.8 Fine-tuning Method 0.8 Head FT
hue Blocks 1-6 FT
Head FT Blocks 7-12 FT
0.6 Full FT 0.6 Every-other FT
Blocks 1-12 FT Adapter(16) FT Adapter FT
Adapter(2) FT
0.4 Pareto Frontier lr 0.4 lr
2e-05
2e-05
0.0001
0.2 0.0001 0.2 0.001
0.001
20 30 40 50 60
20 22 24 26 28 30 32 Validation PPL
Validation PPL
(a)WikipediaDataset Figure 3: Ablating how the location and number of
trainableparameterseffectsmemorizationonthePenn
1.0 Fine-tuning Method Treebankdataset.Eachdotshowsdifferentcheckpoints,
hue
Head FT andthecolorsshowdifferentfine-tuningmethods.We
0.8
Full FT desiremodelsthathavelowPPLandlowattackrecall.
Adapter(16) FT
0.6 lr
2e-05 dataset consisting of 7180 emails. We use a
Pareto Frontier 0.0001
0.4 0.001 sequencelengthof1024, trainingbatchsizeof8,
andfine-tunefor20epochs.
0.2 Models. We study memorization in fine-tuning
20 25 30 35 40 45 Huggingface’s pre-trained GPT-2 on the datasets
Validation PPL
mentioned above. We use a pre-trained but not
(b)PennTreebankDataset
fine-tuned GPT-2 as the reference model for our
1.0 membershipinferenceattack. Weusetheadapter
Fine-tuning Method
hue hub’simplementationofthePfeifferarchitecture,
0.8 Head FT
Full FT withreductionfactors2and16(Pfeifferetal.,2020).
Adapter(16) FT
0.6 Adapter(2) FT Metrics. WeuseValidationPerplexityasametric
lr for the performance of the model, where lower
2e-05
0.4 Pareto Frontier 0.0001 perplexity is better. We evaluate memorization
0.001
ateachepochusingthe MIArecallandexposure
0.2
metrics described in Section 3. The experiments
14 16 18 20 22 24 26 28 30 inthepaperareallrepeated3timesandwereport
Validation PPL
theaveragevaluesforeachmetric.
(c)EnronDataset
Hyperparametersandresultpresentation. We
Figure2:Paretofrontierforutility(validationPPL)Vs. runoptimizationforeachfine-tuningmethodfor20
privacy(MIArecall). Eachdotshowsdifferentcheck- epochs,andperformevaluationofthementioned
points,andthecolorsshowdifferentfine-tuningmethods. metricsattheendofeachepoch. Weexperiment
WedesiremodelsthathavelowPPLandlowattackrecall. withthethreelearningrates2 10 5,10 4,10 3,
− − −
×
andpresenttheresultsforallofthem. Therefore,
Thelowertheexposureis,theharderitistoextract eachgraphwouldhaveanoverallof20 3points,
×
thesecret.Inourexperiments,weinsert50copies for each fine-tuning method, unless the point is
ofthephrase“thesecretnumberis940955”intothe outsidetheplotrange. Forthereportedexposure
trainingdatatoaccentuatethedifferencesbetween numbers, we selected points close to the pareto
the fine-tuning methods. For a six-digit secret, frontiertopresentinTable1,tosummarizeresults.
anexposureofaroundlog (106) 20meansthe
2 ≈ 5 Results
canarycanbereliablyextractedfromthemodel.
Inthissectionwediscussourexperimentalresults
4 ExperimentalSetup
comparing the privacy-utility trends for different
Datasets. (1)Huggingface’sWikipediawikitext- fine-tuning methods. We refer to the naming
2-raw-v1 dataset, consisting of 36718 training conventionshowninFigure1andprovideextended
samples (2) Huggingface’s Penn Treebank ptb_- graphsforeachexperimentinAppendixA.3. We
text_only, consisting of 42068 training samples alsopresentadditionalexperimentswherewetrain
and (3) a sub-sampled version of Enron email the model from scratch (instead of fine-tuning
1818
llaceR
AIM
llaceR
AIM
llaceR
AIM
llaceR
AIM
Table1:Exposuremetric.Higherexposureindicatesmore isthatheadfine-tuningisanoutlier,withextremely
leakage,andexposureabove20meansthesecrets(canaries)
high leakage, on all three datasets. We can also
are reliably extractable. The perplexity numbers here are
differentfromtheonesinotherexperimentssincethetraining seethatthevalidationperplexityachievedbythis
dataisdilutedwiththeartificiallyinsertedsecrets. methodisconsistentlylowerthantheothermethods.
FullFT HeadFT Adapters(2) Adapters(16) Wehypothesizethatthehighleakageoffine-tuning
Parameters(Millions) 124.440 38.590 7.092 0.895
theheadisduetoboththehighnumberofparame-
ValPPL 24.82 28.76 24.41 25.26
Exposure 1.42 10.78 14.54 0.83 ters(38million)andthelocationoftheparameters,
ValPPL 29.55 31.24 29.79 29.41 rightatthelastlayerofthemodelwherethenext
Exposure 7.03 12.0 12.40 4.54
word prediction happens. While full fine-tuning
ValPPL 12.52 13.51 13.03 12.81
Exposure 1.32 10.77 2.02 0.440 actually touches more parameters than head fine-
tuning,itleadstolessleakageundertheattackswe
pre-trained models), fine-tune different model
investigate.Thisresultissomewhatsurprisingand
architectures,andstudythegeneralizationgapin
potentiallyindicatesthattuningparameterslowerin
AppendixA.1.
themodelarchitecturemitigatessomeoftheexplicit
5.1 MemorizationofFine-tuningMethods memorizationperformedbythehead. Wefurther
studythisphenomenonandablateitinSection5.2.
Figures2a,2b,2ccomparethefine-tuningmethods
Wealsoobservethatforalow-perplexityregime
in terms of privacy leakage, measured by MIA
(without considering the cost), full fine-tuning
recallandTable1showstheexposureresultsforthe
is the best choice as it offers utility superior to
three datasets, along with their parameter counts.
adapters.However,ifwehavetoleranceforhigher
ThebluelinesshowtheParetofrontier,markingthe
perplexity,togetlowerleakage,optingforadapters
desirabletrade-offpoints,withlowrecallandPPL.
with a reduction factor of 16 appears better as it
5.1.1 SharedTrends
has lower MIA recall and a lower propensity for
The“memorizationonly”phaseintraining,where
overfitting, compared to the other methods. One
validation perplexity (generalization) is stable
final observation is that full-finetuning has the
andthemodelhasnotyetoverfit,isalsoobserved
shortest “fitting+memorization” phase, whereas
byTänzeretal.(2022)inpre-trainedBERT-based
headfine-tuninghasthelongest.
classifiers.However,itisnamedthe“settlingphase”
there,anditissuggestedthatasvalidationperplex- 5.2 ParameterCount,LocationandTying
ityisratherstable,earlystoppingisnotimportant Tofurthertestourhypothesisthattheprivacy-utility
andtrainingcanstopatanypointbeforeoverfitting. trade-off has to do with both trainable parameter
We, however, showthatmemorizationisactually count and location/distribution within the model
increasingduringthatphase.Therefore,ifweareop- architecture (Section 5.1.2), we run experiments
timizingforprivacyaswell,itisbesttostoptraining with the following set of trainable parameters:
earlier.AppendixA.1.2showsgeneralizationgapvs. (1) first half: blocks 1–6 of the 12 transformer
validationperplexitygraphsdemonstratingthatthe blocksoftheGPT2model(42Mtrainableparams),
gapremainsstableduringthe“memorizationonly” (2) second half: blocks 7–12 (42M), (3) every
phase.Forallthemethods,acrossalldatasets,inthe otherblock(42M)and(4)entirebody: allthe12
“fitting+memorization”andthe“memorizationonly” blocks(84M).Inallthesescenarioswefreezethe
phases,weseeanincreaseinmemorization,without head and fine-tune only the blocks. As shown in
anyoverfitting. Thisshowsthatwecanhavehigh Figure3,wefindthatFullFT>Adapters>all12
memorization/learning,andstillnotoverfit. This blocks=everyotherblock>blocks7to12>blocks
isalsoobservedfortraininglargelanguagemodels 1to6>HeadFT,intermsofprivacy-utilitytrade-
from scratch in Tirumala et al. (2022), which offdesirability.Basedonthis,wearguethathowthe
focusesonanalyzingtheeffectthattexttype(e.g., trainableparametersarescatteredinthenetworkaf-
partofspeech,numbers),datasizeandmodelsize fectshowwellthemodelmakesprogressinthefirst
haveonmemorizationwhentrainingfromscratch. phase(thetrainingandfittingphase),whichaffects
5.1.2 ComparisonofFine-tuningMethods thevalidationperplexitywhenitentersthesecond
ResultsforboththeMIArecallandexposuremetrics phase(memorization-onlyphase).AsFigure2also
(Figure2andTable1)areconsistent,showinghigher shows, full fine-tuning and adapter tuning make
leakageforheadfine-tuningandlowerforfullmodel fasterprogressandendupinalowerperplexity.
fine-tuningandadapters.Thefirstobservationhere Figure 4 shows an ablation study of how
1819
ikiW
BTP
nornE
Table2:Comparisonoffine-tuningdifferenttransformerblocksontheWikipediadataset.
Block1 Block5 Block8 Block12 FullFT HeadFT Adapters(2) Adapters(16)
ValidationPPL 24.39 23.35 23.36 24.05 23.05 23.93 23.62 21.75
MIARecall 22.2 22.6 20.8 21.3 19.2 81.6 16.8 15.2
#Params(inMillions) 7.088 7.088 7.088 7.088 124.440 38.590 7.092 0.895
fine-tuning full blocks seems less desirable than
1.0 Fine-tuning Method
hue usingadaptersorfine-tuningtheentiremodel.
0.8 Full FT Untied
Head FT Untied
6 Conclusion
Full FT
0.6 Head FT
lr Whenfine-tuningisdoneusingsensitivetraining
0.4 2e-05
0.0001 data, it is important to not just consider the cost
0.001
0.2 and utility of fine-tuning methods but to also be
aware that they may have different risks in terms
0.0
20 25 30 35 40 45 50 55 60 ofprivacy.Ourexperimentsshowthatthecommon
Validation PPL
practiceoffine-tuningonlytheheadofamodelhas
Figure 4: Ablating how the untying of the trainable thehighestmemorization(byalargemargin).Full
parameterseffectsmemorizationonthePennTreebank modelfine-tuningandadaptertuning,however,are
dataset. Eachdotshowsdifferentcheckpoints,andthe
bothonthePareto-frontierintermsofattackrecall
colorsshowdifferentfine-tuningmethods. Wedesire
vs.validationperplexity, suggestingthattheyare
modelsthathavelowPPLandlowattackrecall.
moresuitablewhenprivacyisaconcern.
untyingmodelparametersaffectstheprivacy-utility
Acknowledgements
trade-off.Byuntyingparameters,wemeancreating
This project is funded in part by the NSF under
a separate set of parameters for the head of the
grant2200333.Theauthorswouldliketothankthe
model and the input embeddings, as by default
anonymousreviewersandmeta-reviewersfortheir
thesetwoparametersetsaretiedinGPT2,meaning
helpful feedback. We also thank Nikolai Vogler,
thesamesetof38.59Millionparametersareused
NikitaSrivatsan,andKazemTaramforinsightful
forboththesecomponents.However,intheuntied
discussions.Additionally,wethankourcolleagues
scenario, wefirstduplicatethem, andthencreate
attheUCSDBergLabandUVASecurityResearch
separatetrainableparameters,addinganextraset
Groupfortheirhelpfulcommentsandfeedback.
of38.59Milliontrainableparameterstothemodel.
Asthefigureshows,tyingtheparametersimproves LimitationsandEthicsStatement
the progress in training and puts the model at an
Inourstudywefocusonautoregressivelanguage
advantage, comparedtountyingthem, creatinga
models–specificallyGPT-2,asithasbeenshown
betteroverallprivacy-utilitytrade-off.
tobemorepronetomemorizingsamplesthanpre-
5.3 Fine-tuningSingleTransformerBlocks trainedmaskedlanguagemodels(MLM)(Carlini
Tohaveafullanalysisoffine-tuningleakage,we etal.,2021c;Lehmanetal.,2021)Also,inthispaper
alsolookatfine-tuningindividualadapterblocks we loosely refer to the recall of the membership
andfreezingtherestofthemodel.TheGPT-2model inferenceattackonthetrainingsetasmemoirzation.
has12blocks,andweexperimentwithfine-tuning However,weneedtokeepinmindthatalowattack
thefirst,5th,8th,and12thblock,tocoverdifferent recalldoesnotnecessarilymeanlowmemorization,
positions within the model. Table 2 shows the andtheremightbestrongerattacks(ofothertypes,
resultsforthisexperiment. Wehaveselectedthe such as reconstruction) that can better uncover
numberssuchthatthevalidationPPLsareassimilar memorizationinlanguagemodels.
aspossible. Theredoesnotseemtobeanysignifi- In this work we have used publicly available
cantdifferencebetweenfine-tuningdifferentblocks, datasetsandhavenotcollectedanysensitive/private
as they all manifest similar attack recalls. Block data.Theultimategoalofourstudyistocontribute
8’srecall,however,islowerthanotherblocks,with to analyzing memorization under different fine-
lowerPPL,whichwouldmakeitthemostdesirable tuningparadigms,therebyadvancingourintuition
blockforfine-tuningintermsofthePPL-leakage ofhowwecanbetterdeployprivate,fairandsafe
trade-off.Withrespecttoprivacy-utilitytradeoffs, languagemodels.
1820
llaceR
AIM
References Junxian He, Chunting Zhou, Xuezhe Ma, Taylor
Berg-Kirkpatrick, and Graham Neubig. 2022.
Priyam Basu, Tiasa Singha Roy, Rakshit Naidu,
Towardsaunifiedviewofparameter-efficienttransfer
ZumrutMuftuoglu,SahibSingh,andFatemehsadat
learning. InInternationalConferenceonLearning
Mireshghallah. 2021. Benchmarking differential
Representations.
privacyandfederatedlearningforBertmodels. arXiv
preprintarXiv:2106.13973.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Hannah Brown, Katherine Lee, Fatemehsadat
Gesmundo, Mona Attariyan, and Sylvain Gelly.
Mireshghallah, Reza Shokri, and Florian Tramèr.
2019. Parameter-efficienttransferlearningfornlp.
2022. Whatdoesitmeanforalanguagemodelto
InInternationalConferenceonMachineLearning,
preserveprivacy? arXivpreprintarXiv:2202.05520.
pages2790–2799.PMLR.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
BryanKlimtandYimingYang.2004. TheEnroncorpus:
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Anewdatasetforemailclassificationresearch. In
Neelakantan,PranavShyam,GirishSastry,Amanda
European conference on machine learning, pages
Askell,etal.2020. Languagemodelsarefew-shot
217–226.Springer.
learners. AdvancesinNeuralInformationProcessing
Systems.
Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav
Goldberg, and Byron Wallace. 2021. Does BERT
Nicholas Carlini, Steve Chien, Milad Nasr, Shuang
pretrained on clinical notes reveal sensitive data?
Song, Andreas Terzis, and Florian Tramer. 2021a.
In Proceedings of the 2021 Conference of the
Membershipinferenceattacksfromfirstprinciples.
North American Chapter of the Association for
arXivpreprintarXiv:2112.03570.
Computational Linguistics: Human Language
Technologies, pages 946–959, Online. Association
NicholasCarlini,ChangLiu,ÚlfarErlingsson,Jernej
forComputationalLinguistics.
Kos, and Dawn Song. 2019. The Secret Sharer:
Evaluatingandtestingunintendedmemorizationin
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
neuralnetworks. InUSENIXSecuritySymposium.
Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.
NicholasCarlini,FlorianTramer,EricWallace,Matthew
BART:Denoisingsequence-to-sequencepre-training
Jagielski,ArielHerbert-Voss,KatherineLee,Adam
for natural language generation, translation, and
Roberts,TomBrown,DawnSong,UlfarErlingsson,
comprehension. arXivpreprintarXiv:1910.13461.
Alina Oprea, and Colin Raffel. 2021b. Extracting
trainingdatafromlargelanguagemodels. InUSENIX Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
SecuritySymposium. Optimizing continuous prompts for generation.
In 59th Annual Meeting of the Association for
NicholasCarlini,FlorianTramer,EricWallace,Matthew
ComputationalLinguisticsandthe11thInternational
Jagielski,ArielHerbert-Voss,KatherineLee,Adam
JointConferenceonNaturalLanguageProcessing
Roberts,TomBrown,DawnSong,UlfarErlingsson,
(Volume1:LongPapers).
Alina Oprea, and Colin Raffel. 2021c. Extracting
trainingdatafromlargelanguagemodels. XuechenLi,FlorianTramer,PercyLiang,andTatsunori
Hashimoto. 2021. Large language models can be
MarkChen,JerryTworek,HeewooJun,QimingYuan, strongdifferentiallyprivatelearners. arXivpreprint
HenriquePondedeOliveiraPinto,JaredKaplan,Harri arXiv:2110.05679.
Edwards,YuriBurda,NicholasJoseph,GregBrock-
man,etal.2021. Evaluatinglargelanguagemodels MitchellP.Marcus,BeatriceSantorini,andMaryAnn
trainedoncode. arXivpreprintarXiv:2107.03374. Marcinkiewicz.1993. Buildingalargeannotatedcor-
pusofEnglish:ThePennTreebank. Computational
MiaXuChen, BenjaminNLee, GaganBansal, Yuan Linguistics,19(2):313–330.
Cao,ShuyuanZhang,JustinLu,JackieTsay,Yinan
Wang, Andrew M Dai, Zhifeng Chen, et al. 2019. StephenMerity,CaimingXiong,JamesBradbury,and
Gmail smart compose: Real-time assisted writing. Richard Socher. 2016. Pointer sentinel mixture
In ACM SIGKDD International Conference on models.
KnowledgeDiscovery&DataMining.
Fatemehsadat Mireshghallah, Kartik Goyal, Archit
Jesse Dodge, Maarten Sap, Ana Marasovic´, William Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri.
Agnew,GabrielIlharco,DirkGroeneveld,andMatt 2022. Quantifyingprivacyrisksofmaskedlanguage
Gardner. 2021. Documenting the english colossal modelsusingmembershipinferenceattacks.
cleancrawledcorpus. ArXiv,abs/2104.08758.
YutaNakamura,ShouheiHanaoka,YukihiroNomura,
WilliamFedus,BarretZoph,andNoamShazeer.2021. NaotoHayashi,OsamuAbe,ShuntaroYada,Shoko
Switch transformers: Scaling to trillion parameter Wakamiya, and Eiji Aramaki. 2021. KART: Pa-
models with simple and efficient sparsity. arXiv rameterization of privacy leakage scenarios from
preprintarXiv:2101.03961. pre-trainedlanguagemodels.
1821
JonasPfeiffer,AndreasRücklé,CliftonPoth,Aishwarya
Kamath, Ivan Vulic´, Sebastian Ruder, Kyunghyun
Cho, and Iryna Gurevych. 2020. Adapterhub: A
framework for adapting transformers. In Confer-
ence on Empirical Methods in Natural Language
Processing(SystemsDemonstrations).
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
Lee,SharanNarang,MichaelMatena,YanqiZhou,
Wei Li, and Peter J Liu. 2019. Exploring the
limitsoftransferlearningwithaunifiedtext-to-text
transformer. arXivpreprintarXiv:1910.10683.
Alan Ramponi and Barbara Plank. 2020. Neural
unsuperviseddomainadaptationinNLP—asurvey.
arXivpreprintarXiv:2006.00632.
MichaelTänzer,SebastianRuder,andMarekRei.2022.
Memorisation versus generalisation in pre-trained
languagemodels. InProceedingsofthe60thAnnual
Meeting of the Association for Computational Lin-
guistics(Volume1:LongPapers),pages7564–7578,
Dublin, Ireland. Association for Computational
Linguistics.
KushalTirumala,AramHMarkosyan,LukeZettlemoyer,
andArmenAghajanyan.2022. Memorizationwithout
overfitting:Analyzingthetrainingdynamicsoflarge
languagemodels. arXivpreprintarXiv:2205.10770.
Santiago Zanella-Béguelin, Lukas Wutschitz, Shruti
Tople, Victor Rühle, Andrew Paverd, Olga Ohri-
menko,BorisKöpf,andMarcBrockschmidt.2020.
Analyzinginformationleakageofupdatestonatural
languagemodels. InACMSIGSACConferenceon
ComputerandCommunicationsSecurity.
1822
1.0 1.0
0.8 Fine-tuning Method 0.8 Fine-tuning Method
hue hue
Fine-Tuning Full FT
0.6 Training from Scratch 0.6 Head FT
lr lr
2e-05 2e-05
0.4 0.0001 0.4 0.0001
0.001
0.2
0.2
100 200 300 400 500 600 700
Validation PPL 20 30 40 50 60 70 80 90
Validation PPL
Figure 5: Ablating how training the model from (a)DistilGPT2
scratchaffectstheprivacy-utilitytrade-off, compared
1.0
to fine-tuning a pre-trained model, on the Wikipedia
dataset. Eachdotshowsdifferentcheckpoints,andthe 0.8 Fine-tuning Method
colorsshowdifferentfine-tuningmethods. Wedesire hue
Full FT
modelsthathavelowPPLandlowattackrecall. 0.6 Head FT
lr
0.4 2e-05
A Appendix
0.0001
0.001
0.2
A.1 AdditionalExperiments
A.1.1 Correlationbetween
30 40 50 60 70 80 90
Validation PPL
GeneralizationandMemorization
Figure8showsthecorrelationbetweenthegeneral- (b)OpenAI-GPT
izationgapandmembershipinferenceattackrecall.
Figure6: UtilityVs. privacy(MIArecall)onthePenn
Thegeneralizationgapreferstothesubtractionof
Treebank dataset for DistilGPT2 and OpenAI-GPT
trainingperplexityfromvalidationperplexity,anda
models.Eachdotshowsdifferentcheckpoints,andthe
largergapmeansmoreoverfitting.Wecanseethat colorsshowdifferentfine-tuningmethods. Wedesire
thereisadirectrelationbetweenthegeneralization modelsthathavelowPPLandlowattackrecall.
gapandattackrecall, forallfine-tuningmethods.
WecanalsoseethatforPennTreebankandEnron, Huggingface distilgpt2 and openai-gpt as
headfine-tuninghasaconsistentlyhighergeneral- well,andshowtheresultsinFigure6.Aswesee,the
izationgap,whichcouldexplainwhythemember- resultsarecommensuratewiththoseofGPT2.We
shipinferenceattackismoresuccessfulonit. cannotrunexperimentswithadaptershereasthese
A.1.2 GeneralizationGapvsValPPL modelsarenotsupportedbytheadapterlibraryyet.
Figure8showsgeneralizationgap(validation train
A.3 SeparatePlots
−
perplexity) versus validation perplexity. We plot
Figures 9, 10, and 11 show the MIA recall vs
this to show how this differes from MIA recall
validationPPLforeachfine-tuningmethodoneach
(memorization) versus perplexity (Figure 2), and
datasetseparately,toprovidebettervisibility.These
toemphasizehowinthememorizationonlyphase,
FigurescorrespondtothesubfiguresinFigure2.
memorizationisincreasing(thelongverticalstretch
inFigure2),howeverthevalidationperplexityand A.4 BreakingFine-tuningIntoPhases
generalization gap remain almost the same (the Althoughthereisnogroundtruthruleonhowthe
sharpturningpointinFigure8). phasesaredefined,weusethefollowingheuristic:
breakthetrainingbetweenphases1and2atpoints
A.2 TrainingFromScratch
where the slope of the lines on the graph starts
Figure5showshowpre-trainingafinetunedmdoel
increasingdrastically.Forbreakingbetweenphases
isdifferentfromtrainingthemodelfromscratch,in
2and3wechoosethepointwherethevalidation
termsofvalidationperplexityandattackrecall.We
perplexitystartsincreasingagain.
canseethatfine-tuningapre-trainedmodelleaks
lessinformation,thanfine-tuningfromscratch. A.5 ComputationalResources
A.2.1 OtherModels Forthispaper,wespentanoverall7daysinGPU
To further test how our findings generalize to timefortrainingandevaluation.Forthat,weused
other models, we repeat our experiments on the a server with 4 RTX2080 GPU with 11GB of
×
1823
llaceR
kcattA
AIM
llaceR
AIM
llaceR
AIM
1.0
0.8 Fine-tuning Method
hue
Head FT
0.6 Full FT
Adapter(16) FT
Adapter(2) FT
0.4
lr
2e-05
0.2 0.0001
0.001
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
Generalization Gap
40 Fine-tuning Method
(a)WikipediaDataset hue
Head FT
30 Full FT
1.0
Adapter(16) FT
Adapter(2) FT
0.8 20 lr
Fine-tuning Method 2e-05
hue 0.0001
0.6 Head FT 10 0.001
Full FT
Adapter(16) FT
0
0.4 lr
2e-05 20 22 24 26 28 30 32 34 36
0.0001 Validation PPL
0.2 0.001
(a)WikipediaDataset
2 4 6 8 10 12 14 16 18
Generalization Gap
Fine-tuning Method
(b)PennTreebankDataset 25 hue
Head FT
1.0 20 Full FT
Adapter(16) FT
lr
0.8 Fine-tuning Method 15 2e-05
hue 0.0001
Head FT 10 0.001
0.6 Full FT
Adapter(16) FT
Adapter(2) FT 5
0.4
lr
2e-05 20 25 30 35 40 45 50
0.0001 Validation PPL
0.2
0.001
(b)PennTreebankDataset
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Generalization Gap
17.5 Fine-tuning Method
(c)Enron 15.0 hue
Head FT
12.5 Full FT
Figure7: Attackrecallandgeneralizationgap(Valida- 10.0 Adapter(16) FT
tionPPL-TrainPPL)correlation.Asthegeneralization Adapter(2) FT
7.5 lr
gap increases, the attack observes more leakage as 5.0 2e-05
expectedforallfine-tuningmethodsonbothdatasets. 0.0001
2.5 0.001
0.0
memory.
−2.5
14 16 18 20 22 24 26 28 30
Validation PPL
(c)Enron
Figure8: Validationperplexityandgeneralizationgap
(ValidationPPL-TrainPPL)correlation.
1824
llaceR
AIM
llaceR
AIM
llaceR
AIM
paG
noitazilareneG
paG
noitazilareneG
paG
noitazilareneG
1.0 Fine-tuning Method 1.0
hue
0.8 Full FT 0.8 Fine-tuning Method lr
hue
1e-06
Full FT
0.6 2e-05 0.6
lr
0.001 1e-06
0.4 0.4 2e-05
0.0001
0.2 0.2
21 22 23 24 25 26 18 20 22 24 26 28 30 32 34
Validation PPL Validation PPL
(a)FullFT (a)FullFT
1.0 1.0 Fine-tuning Method
hue
0.9
Head FT
0.8
0.8 lr
2e-05
0.7 Fine-tuning Method 0.6 0.0001
hue 0.001 0.6
Head FT
0.5 lr 0.4
2e-05
0.4 0.0001
0.001 0.2
0.3
22 24 26 28 30 32 34 36 38 15 20 25 30 35
Validation PPL Validation PPL
(b)HeadFT (b)HeadFT
0.9 0.7
Fine-tuning Method Fine-tuning Method
0.8 hue hue
0.7 Adapter(16) FT 0.6 Adapter(16) FT lr lr
0.6 2e-05 0.5 2e-05
0.5 0.0001 0.0001
0.001 0.4 0.001
0.4
0.3
0.3
0.2 0.2
0.1
22 24 26 28 30 32 34 22 24 26 28 30 32
Validation PPL Validation PPL
(c)Adapter(16)FT (c)Adapter(16)FT
1.0 Fine-tuning Method 1.0 Fine-tuning Method
hue hue
Adapter(2) FT Adapter(2) FT
0.8 0.8
lr lr
2e-05 2e-05
0.6 0.0001 0.6 0.0001
0.001 0.001
0.4 0.4
0.2
0.2
22 24 26 28 30 32 34 36 38 20 22 24 26 28 30 32 34 36 38
Validation PPL Validation PPL
(d)Adapter(2)FT (d)Adapter(2)FT
Figure9:Wikipedia Figure10:PennTreeBank
1825
llaceR
kcattA
AIM
llaceR
AIM
llaceR
AIM
llaceR
AIM
llaceR
AIM
llaceR
AIM
llaceR
kcattA
AIM
llaceR
AIM
1.0
Fine-tuning Method
hue
0.8 Full FT
lr
1e-06
0.6 2e-05
0.0001
0.4
0.2
12.5 15.0 17.5 20.0 22.5 25.0 27.5 30.0 32.5
Validation PPL
(a)FullFT
0.9 Fine-tuning Method
hue
0.8
Head FT
0.7 lr
2e-05
0.6
0.0001
0.5 0.001
0.4
0.3
0.2
0.1
15 20 25 30 35
Validation PPL
(b)HeadFT
0.28
Fine-tuning Method
0.26 hue
Adapter(16) FT
0.24
lr
0.22 2e-05
0.0001
0.20 0.001
0.18
0.16
0.14
0.12
16 18 20 22 24 26 28
Validation PPL
(c)Adapter(16)FT
Fine-tuning Method
0.8 hue
0.7 Adapter(2) FT
lr
0.6 2e-05
0.0001
0.5
0.001
0.4
0.3
0.2
0.1
15 20 25 30 35
Validation PPL
(d)Adapter(2)FT
Figure11:Enron
1826
llaceR
AIM
llaceR
AIM
llaceR
AIM
llaceR
AIM

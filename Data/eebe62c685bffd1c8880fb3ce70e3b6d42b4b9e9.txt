DSI++: Updating Transformer Memory with New Documents
SanketVaibhavMehta1,2∗ JaiGupta2 YiTay3† MostafaDehghani4 VinhQ.Tran2
JinfengRao5† MarcNajork4 EmmaStrubell1 DonaldMetzler2
1CarnegieMellonUniversity
2GoogleResearch 3GoogleBrain 4GoogleDeepMind 5Google
sanketvmehta@google.com
Abstract information retrieval tasks using sequence-to-
sequence learning. Specifically, DSIs leverage
DifferentiableSearchIndices(DSIs)encodea
Transformermemory(Vaswanietal.,2017)toen-
corpusofdocumentsinmodelparametersand
code all of the information in a corpus of docu-
usethesamemodeltoansweruserqueriesdi-
rectly. DespitethestrongperformanceofDSI ments and then use that memory to answer user
models,deployingtheminsituationswherethe queries directly, thereby simplifying the retrieval
corpuschangesovertimeiscomputationallyex- process. DSIsachievethisfunctionalitybyjointly
pensivebecausereindexingthecorpusrequires optimizingforindexing(ormemorization)andre-
re-trainingthemodel. Inthiswork,weintro-
trievaltasks. Theindexingtaskrequireslearning
duce DSI++, a continual learning challenge
amappingfromdocumentcontenttoitsidentifier,
forDSItoincrementallyindexnewdocuments
typically represented by integers or short strings
whilebeingabletoanswerqueriesrelatedto
(documentidentifiers,abbreviateddocids). Then,
bothpreviouslyandnewlyindexeddocuments.
Across different model scales and document theretrievaltasknecessitatesmappinguserqueries
identifierrepresentations,weshowthatcontin- torelevantdocids. Besidesitssimplicityandend-
ual indexing of new documents leads to con- to-enddifferentiablenature,DSIsignificantlyout-
siderableforgettingofpreviouslyindexeddoc- performsstate-of-the-art“retrieve-and-rank"meth-
uments. We also hypothesize and verify that
odsbasedondual-encoders(Nietal.,2022).
the model experiences forgetting events dur-
DespitetheremarkableperformanceofDSImod-
ingtraining, leadingtounstablelearning. To
mitigate these issues, we investigate two ap- els,thereremainopenquestionsabouttheirappli-
proaches. Thefirstfocusesonmodifyingthe cabilityinthepracticalsettingofdynamiccorpora.
trainingdynamics. Flatterminimaimplicitlyal- Consider the realistic scenario wherein new doc-
leviateforgetting,soweoptimizeforflatterloss
uments are continually added to the indexed cor-
basinsandshowthatthemodelstablymemo-
pus. Updating the index in dual-encoder-based
rizesmoredocuments(+12%). Next,weintro-
methodsrequirescomputingembeddingsfornew
duceagenerativememorytosamplepseudo-
documents,followedbyre-indexingalldocument
queries for documents and supplement them
duringcontinualindexingtopreventforgetting embeddings(Karpukhinetal.,2020). Incontrast,
for the retrieval task. Extensive experiments indexconstructionusingaDSIinvolvestraininga
onnovelcontinualindexingbenchmarksbased Transformermodel. Therefore,themodelmustbe
onNaturalQuestions(NQ)andMSMARCO re-trainedfromscratcheverytimetheunderlying
demonstrate that our proposed solution miti-
corpusisupdated,thusincurringprohibitivelyhigh
gatesforgettingsignificantly. Concretely,itim-
computationalcostscomparedtodual-encoders. In
provestheaverageHits@10by+21.1%over
thiswork,weaimtoaddressthisissuebydevising
competitive baselines for NQ and requires 6
methodsforeffectiveincrementalindexingusing
times fewer model updates compared to re-
training the DSI model for incrementally in- Transformermemorywithoutre-trainingtheDSI
dexingfivecorporainasequence. modelfromscratch.
Lifelong (or continual) learning (Thrun, 1995;
1 Introduction
Parisi et al., 2019) is a biologically-inspired ma-
Differentiable Search Indices (DSIs; Tay et al. chine learning paradigm that deals with contin-
(2022)) represent a new modeling paradigm for uous learning of new tasks by preserving past
∗WorkdoneduringaninternshipatGoogleResearch knowledgeandusingittolearnnewconceptseffi-
†WorkcompletedwhileatGoogle ciently. Basedonthisparadigm,weproposeDSI++
3202
ceD
8
]LC.sc[
3v44790.2122:viXra
usingSharpness-AwareMinimization(SAM;Foret
100 et al. (2021)). Recent works have shown that ge-
Eval batch
ometrical properties of the minima play a vital
D0
80 D1 roleinforgetting,especiallymodelsinflatterloss
D2
basins tend to undergo less forgetting while life-
60
long learning from task sequences (Mehta et al.,
40 2023). Next,weintroduceagenerativememoryto
sample pseudo-queries for already indexed docu-
20
mentsandusethemtoalleviateforgettingofthere-
trievaltaskduringincrementalindexingofthenew
0
D0 D1 D2 D3 D4 D5
documents. Also,thegenerativememoryenables
Batch of documents for continual indexing
continualsemi-supervisedlearningoftheretrieval
Figure1: IndexingaccuracyofD ,D ,andD docu-
0 1 2 taskbygeneratingpseudo-queriesforanincoming
mentcorporavisualizedaswecontinuouslyindexnew
batchofnewdocuments. Ourmaincontributions
documents (averaged over 3 runs). We observe that
canbesummarizedasfollows:
continual indexing of new documents leads to severe
forgettingofthepreviouslymemorizeddocuments.
• We introduce DSI++, a continual learning
challengefortherecentlyproposedDifferen-
(DSI+newdocuments),acontinuallearningchal- tiableSearchIndices(DSI)paradigm. Toen-
lenge for DSI to incrementally index new docu- ableDSI++evaluations,wecreatetwobench-
mentswhilemaintainingtheabilitytoansweruser marks based on existing Natural Questions
queries related to both previously and newly in- andMSMARCOdatasets. Tounderstandthe
dexeddocuments. ToenableDSI++,weintroduce severityoftheforgettingphenomenonacross
novelbenchmarksconstructedfromexistingNatu- multiplescenarios,weanalyzeasuiteofpre-
ralQuestions(Kwiatkowskietal.,2019)andMS trainedmodels(T5-Base,T5-Large,T5-XL)
MARCO(Nguyenetal.,2016)datasets,simulating anddifferentdocumentidentifierrepresenta-
thecontinualadditionofdocumentstothesystem. tions(unstructuredatomic,naivelystructured,
Toourknowledge,thereisnopriorworkstudying andsemanticallystructured).
incrementallearningforDSI.
• We hypothesize and verify that the DSI
A naive solution for DSI++ is to continuously
modelexperiencesforgettingeventsthrough-
fine-tune the model with an indexing objective
outmemorization. Toalleviatethese,wepro-
over new documents. However, Figure 1 shows
posemodifyingtrainingdynamicstopromote
that continual indexing of new documents leads
flatterminimausingSAMandshowthatthe
tocatastrophicforgettingofthepreviouslymemo-
modelstablymemorizes+12%documents.
rizeddocuments(moredetailsin§2.1),acommon
phenomenoninneuralnetworkswhereinlearning
• Weproposeagenerativememory-basedexpe-
ofthenewconceptsinterfereswiththepreviously
riencerehearsalapproachtoalleviateexplicit
acquiredknowledge(McCloskeyandCohen,1989).
forgettingduringcontinualindexingandim-
Furthermore,whenweinvestigatethelearningdy-
prove the average Hits@1 by +25.0% and
namicsoftheDSImodelduringmemorization(Fig-
Hits@10 by +21.1% over competitive base-
ure 3, we observe a significant number of docu-
linesforMSMARCOandNQ,respectively.
ments(approx. 88%)experienceforgettingevents
after they have been memorized. Concretely, a 2 DSI++: Continuallearningchallenge
forgetting event (Toneva et al., 2019) is when a forDSI
prediction for an individual document goes from
2.1 Problemsetup
correctdocidtoincorrectonethroughoutlearning.
Therefore,implicitforgettingduringmemorization We focus on a setup where we receive an initial
andexplicitforgettingfromcontinualindexingof corpus of documents, D = {d ,··· ,d }, and
0 1 n
newdocumentsaretwokeychallengestoovercome user queries corresponding to a subset of them,
forsuccessfullyimplementingaDSI++system. R = {< q ,j >,∀j ∈ Y }, where D ⊂ D .
0 j D 0
To reduce forgetting during memorization, we DSIparadigminvolvestwotasks: (i)memorization
proposeexplicitlyoptimizingforflatterlossbasins taskwherethegoalistolearnanindexerf : X →
θ
ycarucca
gnixednI
Y,atext-to-textmodelparameterizedbyθ ∈ RP, 6,980 validation pairs. Like the benchmark cre-
thattakesdocumenttokens(x ∈ X)asinputand atedfromtheMSMARCOdataset(Pradeepetal.,
maps it to a document identifier (docid) j ∈ Y, 2023),werandomlysample50K uniquepassages
and(ii)retrievaltask wherethegoalistousethe to constitute the initial D corpus and five more
0
sameindexerf todirectlymapauserqueryq to corpora,eachwith10K passages. SeeTable2(in
θ
arelevantdocidj ∈ Y. Twodifferentpromptsare the Appendix) for exact dataset statistics for NQ
usedtodifferentiatebetweenthesetasks. Tayetal. andMSMARCO.
(2022)discussesseveralvariantsforrepresenting
2.3 EvaluationMetrics
docids–unstructuredatomicandstructuredstring
docids,whereeachdocumentisassignedaunique For DSI evaluation, we report indexing accuracy
token and tokenized string, respectively. Under for memorization task and Hits@k (k ∈ {1,10})
the unified text-to-text format, both of the above metric for retrieval task. Indexing accuracy and
tasksarecastasgenerationtasks,i.e.,decodingone Hits@karetheproportionofcorrectlymemorized
uniquetoken(unstructuredatomic)ordecodinga documents and correct documents ranked in the
tokenizedstringsequentially,onetokenatatime topkpredictions,respectively. Weformallydefine
(naively/semanticallystructured). metrics to summarize the model performance as
Inthedynamiccorpusscenario,wesimulatethe weincrementallyindexnewdocuments. LetP
n,o
arrival of new documents by updating the initial denote the performance (e.g., indexing accuracy)
corpusD withasequenceofbatchesD → ··· → oncorpusD aftertrainingoncorpusD . Follow-
0 1 o n
D . In DSI++, we have access to the new batch ing prior work (Mehta et al., 2023), we compute
t
ofdocumentsD ,butwedonothaveanyqueries the average performance (A ), forgetting (F )
i n n
relatedtothesedocuments. and learning performance (LA ) metrics after
n
indexingthecorpusD .
n
Goal: LearnaDSI++systemthatincrementally
The term F (aka backward transfer) refers to
n
indexes D ,D ,··· in f while being able to an-
1 2 θ the effect of indexing the corpus D on the per-
n
swerqueriesrelatedtopreviouslyaswellasaddi-
formanceofallpreviouslyindexeddocumentsD ,
o
tionallyindexeddocuments.
where0 ≤ o < n. LA (orforwardtransfer)mea-
n
suresthemodel’sabilitytolearnwhenpresented
2.2 BenchmarksforDSI++
withanewcorpusD andisdefinedastheaverage
n
To enable research on DSI++, we introduce two performance over the new corpora D ,··· ,D .
1 n
benchmarks constructed from the Natural Ques- WhentheDth corpusisincrementallyindexed,A ,
n n
tions (NQ; Kwiatkowski et al. (2019)) and MS F ,andLA aredefinedasfollows:
n n
MARCO(Nguyenetal.,2016)datasets. TheNQ
n n
dataset consists of Wikipedia articles and corre- 1 (cid:88) 1 (cid:88)
A = P ;LA = P ;
sponding natural language questions. Similar to n n+1 n,o n n o,o
o=0 o=1
(Tayetal.,2022),weconsiderWikipediaarticles
n−1
for memorization and the retrieval task as identi- 1 (cid:88)
F = max (P −P ); (1)
n o′,o n,o
fyingtheWikipediaarticlethatanswersthegiven n o′∈{0,···,n−1}
o=0
question. WeusetheoriginalNQtrainsplittocon-
struct train(80%)/ validation(20%) splits and use
2.4 Casestudy: ForgettingandForward
NQvalidationasatestsplit. Werandomlysample
Transfer
50K uniquearticlestoconstitutetheinitialD cor-
0
pus. Next,weconstructfivecorpora(D ,··· ,D ), AfterintroducingtheDSI++problemsetup,bench-
1 5
eachcontaining10K uniquearticles,toaddthem mark,andevaluationmetrics,westudythebehav-
to the DSI model sequentially. Corresponding to ioroftheDSImodelasnewdocumentsarecontin-
articlesineachofthesecorpora,wefilterqueries uouslyaddedtothesystem. Concretely,wearein-
from original NQ train/ validation splits to con- terestedininvestigatingthefollowingforcontinual
structRtrain,Rval,Rtest (∀i ∈ {0,··· ,5})splits. trainingoftheDSImodelwithindexingobjective
i i i
WeuseR totraintheDSImodelfortheretrieval on new documents – (Q1) How severe is the for-
0
taskanduseRtesttoevaluatepreviouslyandnewly gettingfortheinitiallyindexeddocuments? (Q2)
i
indexedarticles. ThefullMSMARCOdatasethas How does continual updating of the DSI model
approx. 500K passage-query training pairs and over a sequence of corpora affect the forgetting?
Average performance (An) Forgetting (Fn) Learning performance (LAn)
100 100 100
T5-Base
T5-Large
50 50 50 T5-XL
T5-Base(N)
T5-Base(S)
D0 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5
40 40 40
20 20 20
D0 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5
Training corpus Training corpus Training corpus
Figure 2: Systematic study about forgetting and forward transfer when incrementally indexing new corpus of
documentsacrossdifferentmodelsizes(T5-Base,T5-Large,T5-XL)anddocidrepresentations. Weuseatomic
docidsbydefaultanddenote(N)/(S)fornaively/semanticallystructureddocids. ↑indicateshigherisbetter, ↓
indicateslowerisbetter. Allresultsareaveragedover3runs. WeobservethattheaverageA andlearningLA
n n
performanceimprovesbyincreasingthemodelscale. However,forgettingF issevereacrossallmodelscales.
n
Next,weobservethatnaivelystructureddocids,T5-Base(N),underperformunstructuredatomicdocids,T5-Base,
acrossallmetrics-indexingaccuracy,Hits@1,(seeFigure6inAppendixforHits@10results). Imbuingthedocid
spacewithasemantic(S)structurealleviatestheforgettingcomparedtoanarbitrary/naive(N)structure.
(Q3)HowdoestheupdatedDSImodelperformon drifts and becomes less effective for the retrieval
newlyindexeddocuments,especiallytheretrieval task. These findings hint at an approach that re-
task? (Q4)Howdodifferentdocidrepresentation playsindexingandretrievaltasksduringcontinual
strategiesaffectforgetting? (Q5)HowdoestheDSI learning(henceourproposedmethodin§4).
modelscaleaffectforgetting? Figure2visualizes
Docid representations. For studying (Q4), we
resultsonthevalidationsplitofDSI++andhelps
consider unstructured atomic, naively(N) struc-
usconvincinglyanswerthesequestions.
tured,andsemantically(S)structureddocidrepre-
Forgetting. From Figure 2, we see that the T5- sentations. FromFigure2,weseethatT5-Base(N)
Basemodelwithatomicdocidrepresentation(blue underperforms T5-Base by a significant margin.
line plots) undergoes significant forgetting. This Forexample,theaverageperformanceA forthe
0
trendholdsacrossallDSIevaluationmetrics-in- Hits@1metricisapprox. 30and39fornaiveand
dexing accuracy, Hits@1, and Hits@10 (see 6 in atomicdocids,respectively. Further,asthenaively
Appendix). FortheoriginallyindexedD corpus, structuredapproachtreatsunstructureddocidsas
0
indexingaccuracyandHits@1dropbyapprox. 25 tokenizablestringsasopposedtodedicatedunique
and 20 points, respectively. Further, as we con- tokensinthecaseofatomicdocids,theyarerela-
tinueindexingthesequenceofcorpora,weseethat tivelymorepronetointerferencefromnewdocids
forgetting becomes even more severe. For exam- (see F subplot for indexing accuracy). Imbuing
n
ple, after continually indexing the D corpus, F semanticstructuretothenaivedocidspacehelps
5 5
(forgetting)forindexingaccuracyincreasesto75. to reduce forgetting however still underperforms
These results provide evidence to answer (Q1) & unstructureddocids.
(Q2)thattheDSImodelundergoessevereforget-
Model scale. As atomic docids are superior to
tingundercontinualindexingofnewdocuments.
naivedocids,weonlyconsideratomicdocidsfor
Forwardtransfer. Toanswer(Q3),wevisualize answering (Q5). From Figure 2, we observe that
the learning performance (LA ) for all DSI met- largermodelsoutperformtheirsmallercounterparts
n
rics for sequential indexing. From Figure 2, we in terms of the average performance A and the
n
seeLA increasesinindexingaccuracy, suggest- learningperformanceLA (T5-XL>T5-Large>
n n
ingthattheDSImodelisplasticenoughtoindex T5-Base). However,empiricallywereportthatfor-
newdocuments. However,fromFigure2,wesee gettingF issevereacrossallmodelscales,with-
n
adecliningtrendforHits@1. Duetothecontinu- out any clear best performer, and therefore, we
ous indexing updates, the underlying DSI model focusonT5-Basefortherestofourexperiments.
ycarucca
gnixednI
1@stiH
3 ImplicitForgetting: SAM
Memorization(orindexing)isaprimarytaskinthe
DSI paradigm where the goal is to learn a neural 15
corpusindexerthattakesdocumentcontentasinput
andmapsittoadocumentidentifier(docid). Under 10
theunstructuredatomicdocidrepresentationstrat-
egy, each docid is assigned a unique token/class 5
Optimizer
label. Nowgivenalargenumberofdocumentsin
Adafactor
thecorpus(evenmorethanamillion),memoriza- 0 SAM
20 40 60 80 100
tionconstitutesaninstanceofchallengingextreme
Percentage of examples
classificationsetting(Bengioetal.,2019). Further-
Figure 3: Investigating the effectiveness of SAM for
more, for every class, we have only one labeled
alleviatingimplicitforgettingintheT5-Basemodelby
example(i.e.,documentanditsidentifier),making
visualizingcumulativehistogramofforgettingevents.
thistasksetuprare. Motivatedbythislargelyunex-
Aforgettingevent(Tonevaetal.,2019)isdefinedwhen
ploredsetup,weinvestigatethelearningdynamics anindividualdocumentgoesfrombeingclassifiedcor-
forthememorizationtaskthroughouttraining. rectlytoincorrectlyoverthecourseofmemorization.
SAMincreasesthepercentageofexamplesexperiencing
zeroforgettingeventsbyabsolute12%overAdafactor.
Forgettingevents. InFigure5,wevisualizethe
indexing accuracy for the T5-Base model, opti-
mized with Adafactor (Shazeer and Stern, 2018). Sharpness-Aware Minimization. For the loss
We note that the model performance fluctuates function f, SAM seeks to find the parameters w
throughouttraining,suggestingunstablememoriza- that lie in the neighborhood with uniformly low
tion. Wehypothesizethatthemodelcontinuously lossregionsbyoptimizingthefollowingminimax
undergoestheforgettingphenomenonwhereinsub- objective: min max f(w +ϵ), where the
w ||ϵ||2≤ρ
sequentmini-batchupdatesinterferewiththeprevi- maximization region is defined to be a ℓp ball
ouslymemorizeddocuments. Todifferentiatethis with radius ρ for p = 2. Foret et al. (2021) es-
phenomenon from forgetting due to adding new timatesthegradientoftheinnermaximizationby
documents,werefertotheearlieroneasimplicit employing first-order approximation as follows:
forgetting and the latter as explicit forgetting. To ∇ wmax ||ϵ||2≤ρf(w + ϵ) ≈ ∇ wf(w)(cid:12) (cid:12) w+ˆϵ(w),
quantifyinstabilityduringmemorization,wecom-
where ˆϵ(w) = ρ∇ f(w)/||∇ f(w)|| . For a
w w 2
pute forgetting event (Toneva et al., 2019) statis-
givenmini-batchB,SAMapproximatelycomputes
tics. Forgetting event is defined when an individ- apointw′ = w+ϵˆ(w)wherelossismaximumand
ualdocumentgoesfrombeingclassifiedcorrectly
then updates the current model weights w using
(mappedtocorrectdocid)toincorrectlythroughout thegradientatw′. Wedeferreadersto(Foretetal.,
memorization. InFigure3,weplotthecumulative
2021)forcompletedetailsaboutthisderivation.
histogramofforgettingeventswherealmost88%
ofthedocumentsundergoforgettingatleastonce,
SAMalleviatesimplicitforgetting. Weinvesti-
validatingourhypothesisaboutimplicitforgetting.
gatetheapplicabilityofSAMforalleviatingtheim-
plicitforgettingphenomenon. Weuseapre-trained
Flatnessandforgetting. Mirzadehetal.(2020) T5-BasemodeltomemorizeD corpuscontaining
0
showsthatduringsequentiallearningoftasks,flat- 50K unique documents. We compare the perfor-
terminimaleadstolessforgetting. Further,Mehta manceoftheSAMwiththeAdafactoroptimizer. In
et al. (2023) shows that pre-trained initialization Figure5,weseethatSAMoutperformsAdafactor
implicitlyalleviatesforgettingastheypreferflatter intermsoftheoverallindexingaccuracy. Wealso
minima and explicitly optimizing for the flatness notethatSAMundergoeslessseverefluctuations
usingSharpness-AwareMinimization(SAM;Foret duringtraining,thus,hintingatlessforgetting. To
etal.(2021))furtherlessensforgetting. Basedon bolster this claim, in Figure 3, we see that SAM
theseobservations,wehypothesizethatmodifying hasasignificantlyhigherpercentageofdocuments
the training dynamics of the memorization tasks corresponding to a lower cumulative number of
usingSAMshouldalleviateimplicitforgetting. forgettingevents,i.e.,SAMstably(withzerofor-
stneve
gnittegrof
fo
rebmuN
gettingevents)memorizes+12%moredocuments perienceReplay(ER;Chaudhryetal.(2019))for
thanAdafactor. WealsonotethatSAM(35.9±2.2) continuallearninguseasubsetofprevioustaskdata
outperformsAdafactor(32.5±6.4)whenevaluated to regularize the future task learning while mini-
ontheretrievaltask(Hits@1)correspondingtoD . mizingforgetting. Baseduponthis,oneapproach
0
Therefore,wesetSAMtobeourdefaultoptimizer forDSI++istoretainground-truthqueriesforthe
fortherestoftheexperiments. retrievaltaskinepisodicmemoryandusethemto
co-trainwithincrementalindexingtasks. However,
Discussion. Mehta et al. (2023) show that ex-
inDSI++,wecannotaccessground-truthqueries
plicitly optimizing for flatness using SAM leads
foranincomingbatchofnewdocuments. Evenif
to less forgetting, especially in task-incremental
one retains queries for the initial D corpus, we
0
learningsettingswheredataundergoesacleardis-
show in Table 1 that such a method suffers from
tributional shift. We extend this work to the new
forwardtransfertonewlyindexeddocuments.
DSIparadigmandconvincinglydemonstratethat
SAMhelpswiththestablememorizationofdocu- Generativememory. Recentyearshaveseensig-
ments. Ourresultsgeneralizetheearlierfindings nificantprogressinthecapabilitiesofthegenera-
eventothesettingswheredatadoesnotundergoa tive language models (Raffel et al., 2020; Brown
cleardistributionalshift(i.e.,memorizationtask). et al., 2020). Motivated by the success of these
AlthoughSAMhelpsstablymemorizedocuments, models and the in-applicability of the episodic
thereisstillroomforimprovement,andourwork memoryforDSI++,weposeaquestion–insteadof
invitesmorefutureworkinthisdirection. retainingtheground-truthqueries,canwelearna
parametricmodeltogeneratesuchqueriesgivena
4 ExplicitForgetting: Generative
document? Concretely,weproposetotrainaquery
Memory
generatormodeltosamplequeriesforpreviously
seendocumentsandsupplementthemduringincre-
TheDSIparadigmconsistsoftwotasks–memo-
mentalindexing. Sinceweusethegeneratormodel
rizationandretrieval. Theprevioussectionshow-
tosamplequeriesforsparseexperiencereplay,our
casesthatSAMalleviatesimplicitforgettingbysta-
proposedmethod–generativememory. Moreover,
blymemorizingdocuments. Inthissection,wefo-
generativememoryisalsousedtogeneratepseudo-
cusontheforgettingphenomenonthatarisesfrom
queriesfortheincomingbatchofnewdocuments,
thecontinualindexingofnewdocuments,specifi-
thus,enablingcontinualsemi-supervisedlearn-
callyinthecontextoftheretrievaltask. Through
ingoftheretrievaltask.
oursystematicstudy(in§2.4), weshowthatirre-
spectiveofthemodelscaleanddocidrepresenta-
5 Experimentation
tions,DSImodelsundergosevereforgetting. More-
over, we observe that the learning performance
Inthissection,themodelsareinitializedwiththe
LA keepsdecliningfortheretrievaltask(seeFig-
n pre-trained T5-Base model, while the additional
ures2and6forHits@1andHits@10,respectively).
parametersforatomicdocidtokensarerandomly
Thisobservationsuggeststhataswecontinuously
initialized. See§A.1forimplementationdetails.
updatetheDSImodelwiththeindexingobjective,
the model forgets the retrieval task. In DSI, both
5.1 Methods
memorizationandretrievaltasksreturndocidfor
input. By setup, we can assume access to previ- We compare our proposed generative memory-
ousdocumentsandcontinueindexingoldandnew basedapproachwiththefollowingmethods:
documentstoreduceforgettingoftheretrievaltask. Continual indexing, cl(D ). The DSI model is
n
However, in Figure 4, we see that the model still sequentiallyfine-tunedwiththeindexingobjective
undergoesforgetting(morein§5.2). ontheincomingcorpusofdocumentsD .
n
Episodic memory. According to the Comple- Continual indexing with all seen documents,
mentary Learning Systems (McClelland et al., cl(U ). TheDSImodeliscontinuouslyfine-tuned
n
1995)theory,humansuseepisodicmemorytostore withtheindexingobjectiveontheupdatedcorpora
and revisit past experiences for retaining learned U
((cid:83)n
D )withthesamereplayfrequencyfor
n i=0 i
knowledge. Based on this motivation, memory- the old
((cid:83)n−1D
) and new (D ) corpora in the
i=0 i n
based approaches (Sodhani et al., 2022), like Ex- tasksmixture.
Added Method Evalcorpus=D Evalcorpus=D
0 1
corpus (Catastrophicforgetting) (Forwardtransfer)
Indexacc. Hits@1 Hits@10 Indexacc. Hits@1 Hits@10
D - 81.8 35.9 66.9 - - -
0 1.2 2.2 0.9
cl(D ) 52.4 19.2 43.6 96.5 31.7 55.6
1 3.5 3.9 5.7 0.0 6.4 4.9
cl(U =D ∪D ) 78.2 28.9 59.0 91.8 34.0 60.2
1 0 1 0.5 8.9 7.9 0.4 2.4 1.9
cl(U )+epsmem(D ) 77.8 22.9 51.4 93.1 13.1 39.6
1 0 0.5 1.5 0.5 0.0 2.1 3.1
cl(U )+genmem(D ) 77.8 26.0 54.9 93.0 8.6 31.6
D 1 0 0.3 6.9 8.3 0.5 4.8 11.8
1 cl(U )+epsmem(D ) 53.2 7.7 26.0 96.5 48.3 70.7
1 1 3.1 2.1 2.0 0.0 2.3 1.9
cl(U )+genmem(D ) 50.1 7.0 23.1 96.5 57.7 76.7
1 1 0.8 1.2 2.2 0.0 1.5 0.9
cl(U )+genmem(U ) 78.2 18.4 47.5 92.1 48.5 73.8
1 1 0.3 2.8 3.9 0.3 6.1 2.9
cl(U ,docidparametersonly) 78.9 32.7 64.8 94.6 10.8 35.0
1 0.1 5.1 4.2 0.1 3.8 7.3
trainfromscratch 78.7 35.9 66.4 79.2 32.9 63.9
0.6 1.4 0.0 0.3 1.8 1.2
Table1: ComparingperformanceonincrementalindexingofD corpusacrossdifferentmethods-cl(D ): continue
1 1
fine-tuningwithindexingtaskonD ,cl(U ): continuefine-tuningontheupdatedcorpusU ,cl(U )+epsmem(D):
1 1 1 1
continualindexingofU alongwithERofqueriesforD,cl(U )+genmem(D): continualindexingofU alongwith
1 1 1
ERofpseudo-queriesforD. Weobservethatcontinualindexingontheupdatedcorpuscl(U )reducesforgetting
1
comparedtojustindexingnewcorpuscl(D )intheNaturalQuestions(NQ)dataset(|D |=50K,|D |=10K).
1 0 1
Next,ERwitheitherD orD hurtsforwardtransferorforgetting. Ourproposedapproachofaugmentingpseudo-
0 1
queriesforalldocumentsalongwithcontinualindexing,cl(U )+genmem(U ),alleviatesforgettingofD corpus
1 1 0
andimprovesforwardtransfertoD corpus.
1
Continual experience replay using generative 4and7(NQ)andFigure8(MSMARCO),were-
memory,genmem(D ). Inthismethod,thepro- portoverallperformanceacrossDSImetricsaswe
n
posedgenerativememorymodelisusedtosample continuouslyupdatethemodelwiththesequence
pseudo-queries corresponding to the corpus D . offivecorpora(D → ··· → D ).
n 1 5
Next, these pseudo-queries are used for (sparse)
experiencereplayoftheretrievaltasksamples.
Doesgenerativememoryalleviateforgettingof
Continual experience replay using episodic olddocuments? InTable1,fortheNQdataset,
memory, epsmem(D n). Inthismethod, ground- we report Hits@1 to be 35.9 for the model after
truthqueriescorrespondingtotheD nth corpusare trainingonD 0. Weseethatcontinuallyindexing
usedforexperiencereplayoftheretrievaltask. both D and D corpora (cl(U ) - 28.9), signifi-
0 1 1
cantlyreduceforgettingtheretrievaltask(Hits@1)
cl(U , docid parameters only). In this method,
n
over just indexing the new corpora D (cl(D ) -
we only update the parameters corresponding to 1 1
19.2). Next,welookattheperformanceoftheER
atomicdocidtokensusingtheupdatedU corpus.
n
approacheswhenaugmentedwiththecontinualin-
Thismethodinspiritisadual-encoder-baseline.
dexingofalldocuments. Weseethatbothepisodic
Train from scratch, (no cl). The DSI model is memory (cl(U )+epsmem(D ) - 22.9), and gen-
1 0
trained from scratch every time a new corpus is erative memory (cl(U )+genmem(D ) - 26.0) re-
1 0
added. Thismethodcorrespondstoanon-continual duce forgetting compared to cl(D ) when we re-
1
learningsetupandiscomputationallyexpensive. play (pseudo-)queries corresponding to D cor-
0
pus. Moreover, generative memory outperforms
5.2 Results
episodicmemorywithoutretainingoriginalqueries.
In this section, we revisit some of the questions AlthoughfromTable1,weseegenerativememory,
(Q1)-(Q3) raised in our case study (see §2.4) to cl(U )+genmem(U ),underperformscl(U ),from
1 1 1
investigatetheeffectivenessofourproposedgen- Figures 4 and 7, we see that generative memory,
erativememory-basedapproach. Toanswerthese cl(U )+genmem(U ), outperforms cl(U ) both in
5 5 5
questions, in Table 1, we report the performance terms of average performance A and forgetting
n
of the DSI model on D (to study the forgetting F overfivesequentialupdates. Theseresultscon-
0 n
phenomenon)andD corpora(toanswerforward vincinglyshowthattheERwithgenerativememory
1
transfer question) after continual indexing on D significantlyalleviatesforgettingtheretrievaltask
1
forbothNQandMSMARCOdatasets. InFigures comparedtoconsideredbaselines.
Average performance (An) Forgetting (Fn) Learning performance (LAn)
100 100 100
cl(Dn)
50 50 50 cl(Un)
cl(Un)+genmem(Un) with r=32
cl(Un)+genmem(Un) with r=2
train from scratch (no cl)
0 0 0
D0 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5
75 75 75
50 50 50
25 25 25
0 0 0
D0 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5
Training corpus Training corpus Training corpus
Figure4: Investigatingtheeffectivenessofgenerativememoryinmitigatingforgettingwhencontinuouslyindexing
newcorpusD (T5-Basemodelandatomicdocidsrepresentation)fortheNQdataset. ↑indicateshigherisbetter,↓
n
indicateslowerisbetter. Weobservethatcontinualindexingofoldandnewdocumentscl(U )helpstoalleviate
n
forgettingofolderdocumentswhenevaluatedonretrievaltasks. However,averageHits@10(A )stillundergo
n
23pointsdropaftersequentialupdates(D → D ··· → D ). Generativememoryenablessparsereplayingof
0 1 5
pseudo-queriesforolddocumentsandcontinualsemi-supervisedlearningwithnewdocuments. Weobservethat
augmentinggenerativememoryduringcontinualindexingnotonlyreducestheforgetting(F )butalsoimproves
n
averageHits@10(A )by+21.1%overconsideredbaselines(seeFigure7forHits@1results. Figure8forMS
n
MARCOresultsintheAppendix).
Doesgenerativememoryenableforwardtrans- with generative memory enhances retrieval task
fer to new documents? One of the goals of performancebyreducingforgettingofindexeddoc-
DSI++ is to enable answering queries related to uments and enabling forward transfer to newly
newly indexed documents. Towards this goal, in indexeddocuments.
Table1,fortheNQdataset,welookattheretrieval
taskperformance(Hits@1)forD afterincremen- Does generative memory generalize to differ-
1
tallyindexingD . Tocomparedifferentmethods, ent datasets? In Table 3, for the MS MARCO
1
dataset,wereportHits@1tobe78.2aftertraining
we consider a baseline in the form of ER with
ground-truthqueriesforD (cl(U )+epsmem(D ) on D 0 passages. We see that continually index-
1 1 1
ing both D and D corpora (cl(U ) - 76.5 and
-48.3). Weseethatwithoutanyfine-tuningonthe 0 1 1
retrievaltaskforD ,incrementallearningwithin- cl(U 1)+genmem(U 1) - 73.7), significantly reduce
1
forgettingtheretrievaltask(Hits@1)overjustin-
dexingobjectiveshowsimpressiveforwardtransfer
(orzero-shotgains,cl(D )-31.7andcl(U )-34.0). dexingthenewcorporaD 1 (cl(D 1)-68.0). Next,
1 1
welookattheretrievaltaskperformance(Hits@1)
Moreover, ER with generative memory outper-
forms supervised baseline (cl(U )+genmem(D ) for D 1 after incrementally indexing D 1. We see
1 1
that without any fine-tuning on the retrieval task
-57.7). However,wenoticethatreplayingqueries
corresponding to either D or D hurt forward for D 1, incremental learning with indexing ob-
0 1
transfertoD (cl(U )+genmem(D )-8.6)oram- jectiveshowsimpressiveforwardtransfer(cl(D 1)
1 1 0
plifyforgettingofD (cl(U )+genmem(D )-7.0). - 36.1 and cl(U 1) - 35.3). Moreover, ER with
0 1 1
generative memory, cl(U )+genmem(U ) - 80.6,
These results suggest that the memory module 1 1
performs far superior to just incremental index-
shouldinclude(pseudo-)queriescorrespondingto
ing objective. Similar to the results with the NQ
oldandnewdocuments. FromFigure4,weseethat
continualindexingmethodcl(U )hasadownward dataset,weshowthatERwithgenerativememory,
n
trendforLA (Hits@10),therefore,eventuallyfor- cl(U n)+genmem(U n),improvestheoverallperfor-
n
mancefortheretrievaltask,reducingforgettingof
getting the retrieval task. On the other hand, ER
previouslyindexeddocumentsandenablesforward
withgenerativememoryisrelativelyconstant,pro-
vidingevidenceagainstforgetting. Insummary,ER transfertonewdocumentscomparedtocontinual
indexing of all documents, cl(U ). We show that
n
ycarucca
gnixednI
01@stiH
our results hold across two datasets, thus, show- be regarded as a competitive baseline rather than
casingthegeneralizabilityofourapproach. an inherent upper bound. This is also the reason
behindreportingthelearningaccuracy(LA )for
n
Investigatingtheeffectivenessofthegenerative
everymetric,whichcanbeseenasanupperbound
memory with the scale of a corpus. We con-
sinceitmaintainsarunningaverageofthebestper-
ductexperimentswithafullMSMARCOdataset
formanceacrossallcorpora. Furthermore,oneof
(≈ 8.9M passages). We construct two corpora
thekeyobjectivesofcontinuallearningistolever-
– D = 8M and D = 841,823 passages. We
0 1 agepriorknowledgetoenhancethelearningofnew
traintheDSImodelusingD passagesandincre-
0 tasks. Indeed,fromTables1and3,weobservethat
mental add D passages. In Table 3, we report
1 our proposed method excels in forward transfer
resultsforMSMARCO.Weseethatcontinualfine-
comparedtothe“trainfromscratch”approach.
tuningwiththeindexingtaskonD ,cl(D ),com-
1 1 FortheNQdataset,indexingtheinitialD cor-
0
pletely forget the retrieval task for D passages
0 pusof50Kdocumentsrequires350Ktrainingsteps.
(Hits@1 goes to 0.1 from 16.3). However, the
IfwesequentiallyindexadditionalD toD cor-
1 5
generative memory-based approach significantly
pora(10Keach)byre-trainingtheDSImodeleach
reducesforgetting(Hits@1of7.3). Moreover,gen-
time,itwouldrequirearound1.75Msteps. Incon-
erativememoryenablescontinualsemi-supervised
trast, our approach only requires slightly above
learningbyaugmentingpseudo-queriesforD pas-
1 300Kadditionalupdatestoincrementallyindexall
sages,therebyimprovingforwardtransfer(Hits@1
corpora, which is approximately six times fewer
of31.6vs. 18.2forcl(D )). Ourproposedsolution
1 updates. Ourapproachachievessuperioroverall
reducesforgettinginlargecorpussettings.
performancecomparedtore-trainingfromscratch,
whilealsobeingmorecomputationallyefficient.
Investigatingsparsityofexperiencereplay(ER)
on forgetting. ER with generative memory co-
6 Conclusion
trains the indexing and pseudo-labeled retrieval
tasks. Tayetal.(2022)introducesamixingratio DSI++introducesanewapproachtoaddressacru-
r to define the ratio of indexing to retrieval sam- cial requirement of DSI models for practical use
ples. The mixing ratio is inversely related to the inproductionsetups,wherecontinuousadditionof
sparsityofER,i.e.,higherr (moreindexingsam- newdocumentstothecorpusisnecessary. Through
ples)correspondstosparseupdatesfrompseudo- experiments,wedemonstratetheeffectivenessof
labeled retrieval samples. Following (Tay et al., ourproposedsolutions: sharpness-awareminimiza-
2022), we consider r = {2,32} for our analysis. tion and generative memory, which significantly
FromFigure4,weseethatr = 32(sparsereplay) reduce catastrophic forgetting. This work estab-
slightlyoutperformsr = 2intermsofaverageper- lishesafoundationforfurtherresearch,benefiting
formance,forgetting,andlearningaccuracy. These both DSI models and the broader community of
resultssuggestthatevensparseregularizationup- continual(semi-supervised)learning.
datesfromERpositivelyinfluencebackwardand
forwardtransferinDSI++. Limitations
Analyzing index construction time for DSI++. Inthisstudy,weexplorethephenomenonofforget-
DSIinvolvestrainingaTransformermodelforin- tinginrelationtotheadditionofnewanddistinct
dex construction. DSI++ allows incremental up- documentsintotheindexer. Itisimportanttonote
dating of the indexer. In Figures 4, 7, and 8, we that when a new document refutes or modifies a
demonstratethatourincrementalindexerupdating previouslyindexeddocument,themodel’sbehavior
methodsurpassesthe“trainfromscratch”baseline becomesunpredictable,requiringfurtheranalysis.
intermsofA . Notethatthe“trainfromscratch” Additionally,weexaminetheeffectivenessofour
n
baselinecanserveasaperformanceupperbound proposed method on a larger dataset, such as the
forcontinuallearningwhenthereisnodetrimental full MS MARCO dataset. However, it is worth
interferenceamongtasks,andalltasksareevenly notingthatwiththislargerdataset,themethodex-
balanced. However, in the case of DSI++, there hibitssignificantforgetting. Asaresult,additional
existsaninitialbasecorpusthatislargerthansub- researchisnecessarytoenhancethemodel’sper-
sequent corpora, leading to an imbalance among formance,particularlywhendealingwithdatasets
tasks. Consequently, “train from scratch” should oflargerscales.
EthicsStatement LuizBonifacio,HugoAbonizio,MarziehFadaee,and
RodrigoNogueira.2022. Inpars: Dataaugmentation
Training large models is expensive and can have forinformationretrievalusinglargelanguagemodels.
adetrimentalimpactontheenvironment(Strubell
arXivpreprintarXiv:2202.05144.
etal.,2019). Continuallearningontopofexisting
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
modelsispreferabletore-trainingfromscratchin mann, Trevor Cai, Eliza Rutherford, Katie Milli-
this regard since it requires many fewer training can,GeorgeBmVanDenDriessche,Jean-Baptiste
Lespiau, BogdanDamoc, AidanClark, etal.2022.
steps. WithDSI++,weaimtoreducetheneedto
Improvinglanguagemodelsbyretrievingfromtril-
re-trainDSImodelsfromscratchwheneveranew
lionsoftokens. InInternationalConferenceonMa-
set of documents is added to the corpus thereby chineLearning,pages2206–2240.PMLR.
makingitcheaperandbetterfortheenvironment.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Concretely,in§5.2,weanalyzetheindexconstruc-
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
tion time for DSI++ and show that our approach
Neelakantan,PranavShyam,GirishSastry,Amanda
is computationally efficient in comparison to re- Askell,etal.2020. Languagemodelsarefew-shot
trainingthemodelfromscratch. Atthesametime, learners. AdvancesinNeuralInformationProcessing
Systems,33:1877–1901.
we acknowledge that reduced cost can increase
overallconsumption(Jevons’paradox).
ArslanChaudhry,MarcusRohrbach,MohamedElho-
seiny,ThalaiyasingamAjanthan,PuneetKDokania,
Acknowledgements PhilipHSTorr,andMarc’AurelioRanzato.2019. On
tinyepisodicmemoriesincontinuallearning. arXiv
Wethanktheanonymousreviewersfortheirvalu- preprintarXiv:1902.10486.
able feedback and suggestions, which helped im-
NicolaDeCao,WilkerAziz,andIvanTitov.2021. Edit-
prove the paper. We also thank Ronak Pradeep ingfactualknowledgeinlanguagemodels. InPro-
andKaiHuiforhelpwiththeMSMARCOsetup, ceedingsofthe2021ConferenceonEmpiricalMeth-
ods in Natural Language Processing, pages 6491–
TalSchusterandRaghuramMandyamAnnasamy
6506.
for reviewing the paper, and William W. Cohen,
Aditya Gupta, Dara Bahri, and Fuzhao Xue for MatthiasDeLange,RahafAljundi,MarcMasana,Sarah
sharinginsightsandintuitionsduringinitialdiscus- Parisot,XuJia,AlešLeonardis,GregorySlabaugh,
andTinneTuytelaars.2021. Acontinuallearningsur-
sions. WewouldliketothankCOMEDY(COhorts
vey: Defyingforgettinginclassificationtasks. IEEE
ofMaartenSap,EmmaStrubell,DanielFried,and
transactionsonPatternAnalysisandMachineIntel-
YonatanBisk)labmembersforreviewingthepaper ligence,44(7):3366–3385.
andprovidingvaluablecomments; JeremiahMil-
CypriendeMassonD’Autume,SebastianRuder,Ling-
bauer,ClaraNa,JaredFernandez,NupoorGandhi,
peng Kong, and Dani Yogatama. 2019. Episodic
ZhisongZhang,andVijayViswanathanalsogave
memoryinlifelonglanguagelearning. Advancesin
constructivefeedbackondraftsandtables. NeuralInformationProcessingSystems,32.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
References
deepbidirectionaltransformersforlanguageunder-
standing. InProceedingsofthe2019Conferenceof
BadrAlKhamissi,MillicentLi,AsliCelikyilmaz,Mona
theNorthAmericanChapteroftheAssociationfor
Diab,andMarjanGhazvininejad.2022. Areviewon
ComputationalLinguistics: HumanLanguageTech-
languagemodelsasknowledgebases. arXivpreprint
nologies, Volume1(LongandShortPapers), page
arXiv:2204.06031.
4171–4186.
Dara Bahri, Hossein Mobahi, and Yi Tay. 2022. Bhuwan Dhingra, Jeremy R Cole, Julian Martin
Sharpness-aware minimization improves language Eisenschlos, Daniel Gillick, Jacob Eisenstein, and
modelgeneralization. InProceedingsofthe60thAn- WilliamWCohen.2022. Time-awarelanguagemod-
nualMeetingoftheAssociationforComputational elsastemporalknowledgebases. Transactionsofthe
Linguistics (Volume 1: Long Papers), pages 7360– AssociationforComputationalLinguistics,10:257–
7371. 273.
Samy Bengio, Krzysztof Dembczynski, Thorsten Pierre Foret, Ariel Kleiner, Hossein Mobahi, and
Joachims, Marius Kloft, and Manik Varma. 2019. Behnam Neyshabur. 2021. Sharpness-aware mini-
Extreme classification (dagstuhl seminar 18291). mizationforefficientlyimprovinggeneralization. In
In Dagstuhl Reports, volume 8. Schloss Dagstuhl- International Conference on Learning Representa-
Leibniz-ZentrumfürInformatik. tions.
KelvinGuu,KentonLee,ZoraTung,PanupongPasu- Linguistics (Volume 1: Long Papers), pages 4205–
pat,andMingweiChang.2020. REALM:Retrieval 4219.
augmented language model pre-training. In Inter-
national Conference on Machine Learning, pages KevinMeng,DavidBau,AlexJAndonian,andYonatan
3929–3938.PMLR. Belinkov.2022. Locatingandeditingfactualassoci-
ationsinGPT. InAdvancesinNeuralInformation
GautierIzacardandÉdouardGrave.2021. Leveraging ProcessingSystems,volume35.
passageretrievalwithgenerativemodelsforopendo-
mainquestionanswering. InProceedingsofthe16th SeyedImanMirzadeh,MehrdadFarajtabar,RazvanPas-
ConferenceoftheEuropeanChapteroftheAssoci- canu,andHassanGhasemzadeh.2020. Understand-
ationforComputationalLinguistics: MainVolume, ingtheroleoftrainingregimesincontinuallearning.
pages874–880. AdvancesinNeuralInformationProcessingSystems,
33:7308–7320.
ZhengbaoJiang,FrankFXu,JunAraki,andGraham
Neubig. 2020. How can we know what language EricMitchell,CharlesLin,AntoineBosselut,Chelsea
models know. Transactions of the Association for Finn,andChristopherDManning.2022. Fastmodel
ComputationalLinguistics,8:423–438. editing at scale. In International Conference on
LearningRepresentations.
VladimirKarpukhin,BarlasOguz,SewonMin,Patrick
Lewis,LedellWu,SergeyEdunov,DanqiChen,and TriNguyen,MirRosenberg,XiaSong,JianfengGao,
Wen-tauYih.2020. Densepassageretrievalforopen- Saurabh Tiwary, Rangan Majumder, and Li Deng.
domainquestionanswering. InProceedingsofthe 2016. MS MARCO: A human generated machine
2020ConferenceonEmpiricalMethodsinNatural reading comprehension dataset. In Proceedings of
LanguageProcessing(EMNLP),pages6769–6781. theWorkshoponCognitiveComputation:Integrating
neuralandsymbolicapproaches2016.
JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,
JoelVeness,GuillaumeDesjardins,AndreiARusu, JianmoNi,GustavoHernandezAbrego,NoahConstant,
Kieran Milan, John Quan, Tiago Ramalho, Ag- JiMa,KeithHall,DanielCer,andYinfeiYang.2022.
nieszka Grabska-Barwinska, et al. 2017. Over- Sentence-t5: Scalable sentence encoders from pre-
coming catastrophic forgetting in neural networks. trained text-to-text models. In Findings of the As-
Proceedings of the National Academy of Sciences, sociationforComputationalLinguistics: ACL2022,
114(13):3521–3526. pages1864–1874.
TomKwiatkowski, JennimariaPalomaki, OliviaRed- GermanIParisi,RonaldKemker,JoseLPart,Christo-
field,MichaelCollins,AnkurParikh,ChrisAlberti, pherKanan,andStefanWermter.2019. Continual
DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken- lifelong learning with neural networks: A review.
tonLee,etal.2019. Naturalquestions: abenchmark NeuralNetworks,113:54–71.
forquestionansweringresearch. Transactionsofthe
Association for Computational Linguistics, 7:453– Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
466. Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
AlexanderMiller.2019. Languagemodelsasknowl-
JamesLMcClelland,BruceLMcNaughton,andRan- edge bases? In Proceedings of the 2019 Confer-
dallCO’Reilly.1995. Whytherearecomplementary enceonEmpiricalMethodsinNaturalLanguagePro-
learningsystemsinthehippocampusandneocortex: cessingandthe9thInternationalJointConference
insightsfromthesuccessesandfailuresofconnec- onNaturalLanguageProcessing(EMNLP-IJCNLP),
tionistmodelsoflearningandmemory. Psychologi- pages2463–2473.
calReview,102(3):419.
Ronak Pradeep, Kai Hui, Jai Gupta, Adam D Lelkes,
Michael McCloskey and Neal J Cohen. 1989. Catas- HongleiZhuang, JimmyLin, DonaldMetzler, and
trophicinterferenceinconnectionistnetworks: The VinhQTran.2023. Howdoesgenerativeretrieval
sequentiallearningproblem. InPsychologyofLearn- scale to millions of passages? arXiv preprint
ingandMotivation,volume24,pages109–165.El- arXiv:2305.11841.
sevier.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
SanketVaibhavMehta,DarshanPatil,SarathChandar, Lee,SharanNarang,MichaelMatena,YanqiZhou,
andEmmaStrubell.2023. Anempiricalinvestiga- WeiLi,andPeterJLiu.2020. Exploringthelimits
tion of the role of pre-training in lifelong learning. oftransferlearningwithaunifiedtext-to-texttrans-
JournalofMachineLearningResearch,24(214):1– former. TheJournalofMachineLearningResearch,
50. 21(1):5485–5551.
SanketVaibhavMehta,JinfengRao,YiTay,MihirKale, Sylvestre-AlviseRebuffi,AlexanderKolesnikov,Georg
AnkurParikh,andEmmaStrubell.2022. Improving Sperl,andChristophHLampert.2017. iCaRL:In-
compositional generalization with self-training for crementalclassifierandrepresentationlearning. In
data-to-textgeneration. InProceedingsofthe60th ProceedingsoftheIEEEConferenceonComputer
AnnualMeetingoftheAssociationforComputational VisionandPatternRecognition,pages2001–2010.
AdamRoberts,HyungWonChung,AnselmLevskaya, SebastianThrun.1995. Islearningthen-ththingany
GauravMishra,JamesBradbury,DanielAndor,Sha- easier than learning the first? Advances in Neural
ran Narang, Brian Lester, Colin Gaffney, Afroz InformationProcessingSystems,8.
Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz,
Alex Salcianu, Marc van Zee, Jacob Austin, Se- MariyaToneva,AlessandroSordoni,RemiTachetdes
bastian Goodman, Livio Baldini Soares, Haitang Combes,AdamTrischler,YoshuaBengio,andGeof-
Hu, Sasha Tsvyashchenko, Aakanksha Chowdh- freyJGordon.2019. Anempiricalstudyofexample
ery, Jasmijn Bastings, Jannis Bulian, Xavier Gar- forgettingduringdeepneuralnetworklearning. In
cia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, International Conference on Learning Representa-
JonathanH.Clark,StephanLee,DanGarrette,James tions.
Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Ritter, Maarten Bosma, Alexandre Passos, Jeremy
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Maitin-Shepard,NoahFiedel,MarkOmernick,Bren-
Kaiser,andIlliaPolosukhin.2017. Attentionisall
nan Saeta, Ryan Sepassi, Alexander Spiridonov,
youneed. AdvancesinNeuralInformationProcess-
JoshuaNewlan,andAndreaGesmundo.2022. Scal-
ingSystems,30.
ingupmodelsanddatawitht5xandseqio. arXiv
preprintarXiv:2203.17189.
ZiruiWang,SanketVaibhavMehta,BarnabásPóczos,
andJaimeGCarbonell.2020. Efficientmetalifelong-
AdamRoberts,ColinRaffel,andNoamShazeer.2020.
learningwithlimitedmemory. InProceedingsofthe
Howmuchknowledgecanyoupackintotheparam-
2020ConferenceonEmpiricalMethodsinNatural
eters of a language model? In Proceedings of the
LanguageProcessing(EMNLP),pages535–548.
2020ConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP),pages5418–5426.
ChenZhu,AnkitSinghRawat,ManzilZaheer,Srinadh
Bhojanapalli,DaliangLi,FelixYu,andSanjivKumar.
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
2020. Modifyingmemoriesintransformermodels.
Adaptivelearningrateswithsublinearmemorycost.
arXivpreprintarXiv:2012.00363.
InInternationalConferenceonMachineLearning,
pages4596–4604.PMLR. ShengyaoZhuang,HouxingRen,LinjunShou,JianPei,
MingGong,GuidoZuccon,andDaxinJiang.2022.
HanulShin,JungKwonLee,JaehongKim,andJiwon Bridgingthegapbetweenindexingandretrievalfor
Kim.2017. Continuallearningwithdeepgenerative differentiable search index with query generation.
replay. AdvancesinNeuralInformationProcessing arXivpreprintarXiv:2206.10128.
Systems,30.
A Appendix
Shagun Sodhani, Mojtaba Faramarzi, Sanket Vaib-
hav Mehta, Pranshu Malviya, Mohamed Abdel-
A.1 ImplementationDetails
salam, Janarthanan Janarthanan, and Sarath Chan-
dar. 2022. An introduction to lifelong supervised We utilize the pre-trained T5-Base (Raffel et al.,
learning. arXivpreprintarXiv:2207.04354. 2020) to initialize all models and randomly ini-
tializetheadditionalparametersforatomicdocid
Pablo Sprechmann, Siddhant Jayakumar, Jack Rae,
tokens. Bahri et al. (2022) demonstrates the suc-
AlexanderPritzel,AdriaPuigdomenechBadia,Be-
nignoUria,OriolVinyals,DemisHassabis,Razvan cessfulapplicabilityofSAMforlanguagemodel
Pascanu,andCharlesBlundell.2018. Memory-based generalization,especiallyinpre-trainedT5models.
parameteradaptation. InInternationalConference
We mainly follow (Bahri et al., 2022) to set our
onLearningRepresentations.
hyper-parameters: ρ = 0.15,batchsize=32forthe
EmmaStrubell,AnanyaGanesh,andAndrewMcCal- innermaximizationstepinSAM.
lum. 2019. Energy and policy considerations for WhileindexingD corpus,wetrainallthemod-
0
deep learning in NLP. In Proceedings of the 57th
elsforamaximumof1Mstepswithawarmupof
AnnualMeetingoftheAssociationforComputational
100Ksteps. Duringcontinualindexingofothercor-
Linguistics,pages3645–3650.
pora,wetrainforamaximumof100Kstepswith
Fan-KengSun,Cheng-HaoHo,andHung-YiLee.2020. a warmup of 100 steps. For the rest of the hyper-
LAMAL:LAnguagemodelingisallyouneedforlife- parameters, we follow (Tay et al., 2022) – set a
longlanguagelearning. InInternationalConference
learningrateto0.001,batchsizeto128,andinput
onLearningRepresentations.
sequencelengthto32. Weevaluatemodelsafterev-
Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, ery5Kstepsandretainthecheckpointyieldingthe
DaraBahri,HarshMehta,ZhenQin,KaiHui,Zhe bestperformance. FortheinitialtrainingwithD
0
Zhao, Jai Gupta, Tal Schuster, William W. Cohen,
corpus,weco-trainonindexingandretrievaltasks;
andDonaldMetzler.2022. Transformermemoryas
therefore,weusetheaverageofallDSImetrics(in-
adifferentiablesearchindex. InAdvancesinNeural
InformationProcessingSystems,volume35. dexingaccuracy,Hits@1,andHits@10)formodel
stratesthatpre-trainedT5(Raffeletal.,2020)mod-
elscanbeemployedtoansweropen-domainques-
tions without access to any external knowledge
80
or context. However, unlike structured KBs, it is
75
non-trivialtoupdateknowledgestoredimplicitly
in the weights of these models. Therefore, Zhu
70
etal.(2020)introducesanexperimentationsetup
65
wherethetaskistoupdatefactsstoredwithinthe
60 Optimizer pre-trainedmodelsandproposesaconstrainedopti-
Adafactor
SAM mizationmethod,similartoElasticWeightConsol-
1 2 3 4 5 idation(Kirkpatricketal.,2017),toalleviatecatas-
Training step ( x100,000)
trophicforgetting. Withsimilarmotivation,(Dhin-
Figure 5: Investigating the effectiveness of SAM for
gra et al., 2022) introduces a diagnostic dataset
alleviatingimplicitforgettingintheT5-Basemodelby
to probe LMs for facts that change over time. It
visualizingindexingaccuracyduringmemorization. We
alsosuggestsjointlymodelingtextwithitstimes-
observeseriousfluctuationsintheindexingaccuracyin
tampforimprovedmemorizationofseenfacts. Re-
thecaseoftheAdafactoroptimizer,therebysuggesting
unstablememorization. SAMleadstorelativelystable centworkshavebeeninvestigatingefficientways
memorizationofdocuments. to localize and edit facts stored with the LMs
(AlKhamissi et al., 2022) using finetuning (Zhu
etal.,2020;Dhingraetal.,2022),hyper-networks
selection. Forthecontinuallearningexperiments,
(De Cao et al., 2021; Mitchell et al., 2022), and
we have access to only indexing accuracy for all
direct editing (Meng et al., 2022). Although a
involvedcorpora,soweuseitformodelselection.
crucial line of work around updating facts in the
Totrainaparametricmodelforgenerativemem-
pre-trainedLMs,usingpromptingasourprobing
ory,weutilizetheretrievaldatasetR ,whichcor-
0 mechanismonlyprovidesalowerboundestimate
respondstotheD corpus. Wesetthemaximum
0 oftheknowledgecontainedinthesemodels(Jiang
sequencelengthfordocumentcontentsto1024,the
etal.,2020). Ontheotherhand,weexplicitlyfo-
targetlengthforgeneratedqueriesto32,batchsize
cusonthememorizationtaskinDSI++. Thistask
to128,trainforamaximumof100Ksteps,anduse
helpsustoanswerquestionsrelatedtocatastrophic
BLUEformodelselection. Weusebeamdecoding
forgettingmoreconvincinglyratherthanbounded
togeneratepseudo-queries. Wetunethelearning
bythemechanismofhowweprobethesemodels.
rate amongst {0.001,0.0005} and linear warmup
amongst{1K,10K}. Forallourexperiments,we Optimization-basedapproaches forcontinual
usetheT5X(Robertsetal.,2022)frameworkalong learningencodethenecessaryinductivebiasesre-
with4-8TPUv4chipstotrainthemodels. quiredtoenablecontinuallearningbymodifying
thetrainingdynamics. Flatterminimaareshownto
A.2 RelatedWork
alleviateforgetting(Mirzadehetal.,2020). Further,
We review relevant prior work along two dimen- Mehtaetal.(2023)showedthatexplicitlyoptimiz-
sions: Application setups related to DSI++ and ing for flatter loss basins using Sharpness-Aware
continuallearningmethodstoalleviateforgetting Minimization(SAM;Foretetal.(2021))reduces
andenableforwardtransfer. forgetting. Buildingontheseworks,weshowthat
flatter minima induced by SAM reduce implicit
Language models (LMs) as knowledge bases forgettingduringmemorization,therebyleadingto
(KBs). Petroni et al. (2019) shows that pre- morestablememorization(see§3).
trained BERT (Devlin et al., 2019) models cap-
ture relational knowledge comparable to that of Memory-based(akadata-basedregularization)
theKBsconstructedusingoff-the-shelftechniques. approaches forcontinuallearningconstrainthe
Concretely, these models can be used to extract parameter updates based on the previous task ex-
factualknowledgeaboutrelationsbetweenentities amplessampledfrommemory. Sparseexperience
byprovidingaprompttopredictmissingwordsin replay using episodic memory (Chaudhry et al.,
acloze-styletemplate(e.g.,“NewDelhiisthecap- 2019)isaprominentapproach,andin§4,wedis-
italof ”). Similarly,Robertsetal.(2020)demon- cussitslimitationsofitforDSI++. Next,Shinetal.
ycarucca
gnixednI
Dataset #D NaturalQuestions(NQ) MSMARCO
#Train #Validation #Test #Train #Validation #Test
R 50K 53.8K 13.5K 3.9K 2M 25.0K 3.6K
0
R 10K 10.7K 2.7K 809 400K 5.1K 762
1
R 10K 10.6K 2.7K 787 400K 5.1K 770
2
R 10K 10.7K 2.7K 727 400K 4.9K 734
3
R 10K 10.9K 2.7K 772 400K 4.9K 730
4
R 10K 10.7K 2.7K 847 400K 4.9K 660
5
Table2: DSI++datasetstatisticsforNQandMSMARCO:memorizationandretrievaltasks.
Added Method Evalcorpus=D Evalcorpus=D
0 1
corpus (Catastrophicforgetting) (Forwardtransfer)
Indexacc. Hits@1 Hits@10 Indexacc. Hits@1 Hits@10
MSMARCO–|D |=50K,|D |=10K
0 1
D - 99.4 78.2 95.0 - - -
0 0.2 0.2 0.1
cl(D ) 46.7 68.0 87.3 99.8 36.1 65.8
1 18.6 2.0 1.3 0.0 9.5 6.9
cl(U ) 99.4 76.5 94.2 99.8 35.3 64.4
D 1 0.0 0.7 0.3 0.0 4.1 3.3
1 cl(U )+genmem(U ) 99.3 73.7 93.9 99.8 80.6 95.5
1 1 0.1 0.2 0.3 0.0 1.0 0.1
trainfromscratch 99.5 75.0 93.9 99.6 73.4 93.4
0.0 0.2 0.1 0.0 1.3 0.9
MSMARCO(full)–|D |=8M,|D |=842K
0 1
D - 99.4 16.3 46.8 - - -
0
cl(D ) 0.0 0.1 0.6 97.9 18.2 40.5
D 1
1 cl(U )+genmem(U ) 20.4 7.3 31.3 86.6 31.6 65.8
1 1
Table3: ComparingperformanceonincrementalindexingofD corpusacrossdifferentmethods-cl(D ): continue
1 1
fine-tuningwithindexingtaskonD ,cl(U ): continuefine-tuningontheupdatedcorpusU ,cl(U )+genmem(D):
1 1 1 1
continual indexing of U along with ER of pseudo-queries for D. We observe that continual indexing on the
1
updated corpus cl(U ) reduces forgetting compared to just indexing new corpus cl(D ) in the MS MARCO
1 1
dataset. Ourproposedapproachofaugmentingpseudo-queriesforalldocumentsalongwithcontinualindexing,
cl(U )+genmem(U ),alleviatesforgettingofD corpusandimprovesforwardtransfertoD corpus. Wealsoshow
1 1 0 1
thatourproposedsolutionreducesforgettingofD (=8M)passageswhileincrementalindexinginalargecorpus
0
setting,MSMARCO(full)containing8.9M passages.
Average performance (An) Forgetting (Fn) Learning performance (LAn)
60 60 60
40 40 40 T5-Base
T5-Large
T5-XL
20 20 20 T5-Base(N)
T5-Base(S)
D0 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5
Training corpus Training corpus Training corpus
Figure 6: Systematic study about forgetting and forward transfer when incrementally indexing new corpus of
documentsacrossdifferentmodelsizes(T5-Base,T5-Large,T5-XL)anddocidrepresentations. Weuseatomic
docidsbydefaultanddenote(N)/(S)fornaively/semanticallystructuredstringdocids. ↑indicateshigherisbetter,
↓ indicates lower is better. We observe that by increasing the model scale, the average A and learning LA
n n
performanceimproves. However,forgettingF issevereacrossallmodelscales. Moreover,weobservethatnaive
n
stringdocids(N)underperformatomicdocidsacrosstheHits@10metric. SimilartoFigure2,imbuingthedocid
spacewithasemantic(S)structurealleviatestheforgettingcomparedtoanarbitrary/naive(N)structure.
(2017);Sunetal.(2020)learnsaparametricmodel (older)documentsandanincomingbatchofnew
to reconstruct the examples for seen tasks. How- documents,thus,enablingustoleverageunlabeled
ever,inDSI++,wedonotseequeriesforthenew data(intheformofnewdocuments)forcontinual
documents. Therefore,weuseaparametricmem- semi-supervisedlearning. Ontheotherhand,Sun
orytogeneratepseudo-queriesforalreadyindexed et al. (2020) assumes that the incoming data are
01@stiH
Average performance (An) Forgetting (Fn) Learning performance (LAn)
60 60 60
40 40 40
20 20 20 cl(Dn)
cl(Un)
cl(Un)+genmem(Un) with r=32
0 0 0 cl(Un)+genmem(Un) with r=2
train from scratch (no cl)
D0 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5
Training corpus Training corpus Training corpus
Figure7: Investigatingtheeffectivenessofgenerativememoryinmitigatingforgettingwhencontinuouslyindexing
newcorpusD (T5-Basemodelandatomicdocidsrepresentation)fortheNQdataset. ↑indicateshigherisbetter,↓
n
indicateslowerisbetter. Weobservethatcontinualindexingofoldandnewdocumentscl(U )helpstoalleviate
n
forgettingofolderdocumentswhenevaluatedonretrievaltasks. However,averageHits@1(A )stillundergo19
n
pointsdropaftersequentialupdates(D →D ···→D ). Weobservethataugmentinggenerativememoryduring
0 1 5
continualindexingnotonlyreducestheforgetting(F )butalsoimprovesaverageHits@1(A )by+17.3%over
n n
continualindexing.
Average performance (An) Forgetting (Fn) Learning performance (LAn)
100 100 100
50 50 50 cl(Dn)
cl(Un)
cl(Un)+genmem(Un) with r=1
train from scratch (no cl)
0 0 0
D0 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5
75 75 75
50 50 50
25 25 25
0 0 0
D0 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5
100 100 100
50 50 50
0 0 0
D0 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5 D1 D2 D3 D4 D5
Training corpus Training corpus Training corpus
Figure8: Investigatingtheeffectivenessofgenerativememoryinmitigatingforgettingwhencontinuouslyindexing
newcorpusD (T5-Basemodelandatomicdocidsrepresentation)fortheMSMARCOdataset. ↑indicateshigher
n
isbetter,↓indicateslowerisbetter. Weobservethatcontinualindexingofoldandnewdocumentscl(U )helps
n
to alleviate forgetting of older documents when evaluated on retrieval tasks. However, average Hits@10 (A )
n
stillundergo25.0pointsdropaftersequentialupdates(D →D ···→D ). Generativememoryenablessparse
0 1 5
replayingofpseudo-queriesforolddocumentsandcontinualsemi-supervisedlearningwithnewdocuments. We
observethataugmentinggenerativememoryduringcontinualindexingnotonlyreducestheforgetting(F )butalso
n
improvesaverageHits@10(A )by+23.0%overconsideredbaselines.
n
fullylabeled,whichisnotapplicableinDSI++(we ory. Inourwork,wedonotgenerateexamplepairs
donotgettoseequeriesforthenewdocuments). (x,y)butrathergeneratepseudo-queries(y),sim-
Furthermore, Sun et al. (2020) shows that using ilar to contemporary works (Zhuang et al., 2022;
aparametricmodelunderperformsepisodicmem- Bonifacioetal.,2022). Weshowthatourapproach
ycarucca
gnixednI
01@stiH
1@stiH
1@stiH
outperformsepisodicmemory. Lastly,inthecon- thepredictiontime,thesemethodstypicallyrequire
text of pseudo-query generation, neural models task identity to activate the corresponding subset
arepronetohallucinateadditionalcontentnotsup- ofparametersforinference. IntheDSIparadigm,
portedbytheinputdocuments. Futureworkscan we are given user queries at the inference time,
study methods to filter out noisy pseudo-queries andthegoalistopredictrelevantdocumentiden-
(Mehtaetal.,2022)duringincrementalindexing. tifiers. Now during incremental indexing, if we
considereverynewdocumentcorpusasanewtask,
Test time adaptation approaches for contin- thenatypicalparameterisolation-basedapproach
uallearninguseepisodicmemoryattheinference wouldrequirecorpusidentityforeveryuserquery
timetoalterthemodelweightsbeforemakingpre- atthetesttime,defeatingthewholepurposeofthe
dictions (Rebuffi et al., 2017; Sprechmann et al., DSIparadigm. Duetothis,theparameterisolation-
2018; de Masson D’Autume et al., 2019; Wang basedapproachesintheircurrentformarerendered
et al., 2020). Updating the DSI indexer for ev- less useful for DSI++. Nevertheless, we believe
ery user query is computationally expensive, so thatbymaskingtheweightsforthealreadyindexed
we focus on continual learning methods during corpus, one is explicitly disabling the updates to
training. Apart from continual learning-focused the underlying DSI model; therefore, parameter
approaches,retrievalaugmentedgeneration(Guu isolation-based methods would be robust to for-
et al., 2020; Izacard and Grave, 2021; Borgeaud getting,andfutureworksshouldexplorethemfor
et al., 2022) family of approaches retrieve auxil- DSI++. Webelieve,however,thatadaptingthese
iary passages/documents to enhance pre-trained methodsforDSI++isoutofscopeforthispaper,
languagemodels. Theseapproachesaltertest-time and we would not be able to do both this topic
predictionsofthegenerativemodelsbyaugmenting and our current work justice in the limited space
theirinputwithrelevantpassagesretrievedfromex- available.
ternalretrievablememory. Moreover,oneexplicitly
disables the updates to the employed pre-trained
(andretrieval)modelusingtheexternalretrievable
memory. Suchapproachesdonotfaithfullyassess
thefundamentalchallengeoflearningcontinually,
specifically catastrophic forgetting. On the other
hand,ourworkfocusesontherecentlyintroduced
DSIparadigm(Tayetal.,2022),whereinformation
inthedocumentcorpusisencodedintothemodel
parameters. Therefore, any updates to the under-
lying corpus necessitate updating the model pa-
rametershence,undergoingsevereforgetting. Our
work tackles a more challenging setup for study-
ingtheforgettingphenomenonindetail. However,
retrieval-augmentedgeneration-basedmethodsdo
notanalyzetheforgettingphenomenon,onlylook-
ingatoverallperformancemetrics. Weagreethat
continuallearningisbroaderthancatastrophicfor-
getting. However,inthiswork,wedecidedtostudy
theforgettingphenomenonindetailononeofthe
mostchallengingsetups,ifnotthemostdifficult.
Parameterisolation-basedapproaches forcon-
tinual learning assign different dedicated subsets
of the model parameters to each task to prevent
forgetting(DeLangeetal.,2021). Whilelearning
a new task, these methods either freeze a subset
oftheparameterscorrespondingtooldertasksor
dynamicallyaddnewparameterspernewtask. At

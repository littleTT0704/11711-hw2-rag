Semantic Similarity Based Evaluation for Abstractive News
Summarization
FigenBekenFikri1,KemalOflazer2 ,BerrinYanıkog˘lu1
1DepartmentofComputerScienceandEngineering,SabancıUniversity,Istanbul,Turkey
2DepartmentofComputerScience,CarnegieMellonUniversity-Qatar,Doha,Qatar
1{fbekenfikri,berrin}@sabanciuniv.edu,2ko@andrew.cmu.edu
Abstract containthesamewordsinthegoldstandardsum-
mary. Onthecontrary,anabstractivesummariza-
ROUGEisawidelyusedevaluationmetricin
tionmodelisexpectedtogeneratenewwordsthat
text summarization. However, it is not suit-
may not even appear in the source. For aggluti-
able for the evaluation of abstractive summa-
native languages, the ineffectiveness of ROUGE
rizationsystemsasitreliesonlexicaloverlap
metricbecomesmoreapparent. Forinstance,both
between the gold standard and the generated
summaries. Thislimitationbecomesmoreap- ofthefollowingsentenceshasthemeaning”Iwant
parent for agglutinative languages with very tocalltheembassy”:
large vocabularies and high type/token ratios.
In this paper, we present semantic similarity Bu¨yu¨kelc¸ilig˘iaramakistiyorum.
models for Turkish and apply them as evalua-
tion metrics for an abstractive summarization Bu¨yu¨kelc¸ilig˘etelefonetmekistiyorum.
task. To achieve this, we translated the En-
While, ”aramak” is a verb that takes an object
glishSTSbdatasetintoTurkishandpresented
thefirstsemantictextualsimilaritydatasetfor inaccusativecase,”telefonetmek”isacompound
Turkish. We showed that our best similarity verbinTurkishandtheequivalentoftheaccusative
modelshavebetteralignmentwithaveragehu- objectinthefirstsentenceisrealizedwithanoun
man judgments compared to ROUGE in both
indativecase(ashighlightedwithunderlines). Al-
PearsonandSpearmancorrelations.
though, these sentences are semantically equiva-
lent,ROUGE-1,ROUGE-2andROUGE-3scores
1 Introduction
of these sentences are 0.25, 0, and 0.25 respec-
Automatic document summarization aims to pro- tively.
duceasummarythatconveysthesalientinforma- In this paper, we present a semantic similarity
tion in the given text(s). Automatic summarizers modelwhichcanbeappliedtoabstractivesumma-
provide reduction in the size of the text, as well rization as a semantic evaluation metric. To this
as,combineandclusterdifferentsourcesofinfor- end, we translated the English Semantic Textual
mation,whilepreservingtheinformationalcontent. Similarity benchmark (STSb) dataset (Cer et al.,
There are two approaches to summarization: ex- 2017)intoTurkishandpresentedthefirstseman-
tractiveandabstractive. Extractivesummarization tic textual similarity dataset for Turkish as well.
yieldsasummarybyextractingimportantphrases STSb dataset is a selection of data from English
or sentences from the document. In contrast, ab- STSsharedtasksbetween2012and2017. These
stractive summarization provides a much more datasetshavebeenwidelyusedforsentencelevel
human-likesummarybycapturingtheinternalse- similarity and semantic representations research
manticmeaningandgeneratingnewsentences. (Ceretal.,2017).
ROUGE is a widely used evaluation metric in WealsoleveragedtheNLI-TRdatasetthathas
textsummarization. Itcomparesthesystemsum- been presented recently for Turkish natural lan-
marywiththehumangeneratedsummaryorsum- guageinferencetask(Buduretal.,2020). TheNLI-
maries,byconsideringtheoverlappingunitssuch TRdatasetcombinesthetranslatedStanfordNat-
as n-gram, word sequences and word pairs (Lin, ural Language Inference (SNLI) (Bowman et al.,
2004). However,inabstractivesummarizationsys- 2015)andMultiGenreNaturalLanguageInference
tems,thegeneratedsummarydoesnotnecessarily (MultiNLI)(Williamsetal.,2018)datasets.
Ourpaperisstructuredinthefollowingway: In et al., 2020), YiSi (Lo, 2019), Prism (Thompson
section 2, we explain recent studies and evalua- andPost,2020)).
tionmetrics. Insection3,weexplainnaturallan-
3 Methodology
guageinferenceandsemantictextualsimilarity. We
present our STSb Turkish dataset and translation
3.1 NaturalLanguageInference
quality. Insection4, wepresentourexperiments
Natural language inference is the study of deter-
for semantic textual similarity. In section 5, we
miningwhetherthereisanentailment,acontradic-
presenttheexperimentsforsummarization. Weap-
tionoraneutralrelationshipbetweenahypothesis
pliedourbestperformingfoursemanticsimilarity
andagivenpremise. Therearetwomajorcorpora
modelsasevaluationmetricstothesummarization
in literature for natural language inference in En-
results. In section 6, we present our results both
glish. TheseareStanfordNaturalLanguageInfer-
qualitativelyandquantitativelybycomparingthe
ence(SNLI)(Bowmanetal.,2015)andMultiGenre
semanticsimilarityandROUGEscoreswithhuman
NaturalLanguageInference(MultiNLI)(Williams
judgmentsinPearsonandSpearmancorrelations.
et al., 2018) datasets. The SNLI corpus is about
570ksentencepairswhiletheMultiNLIcorpusis
2 RelatedWork
about433ksentencepairs. TheMultiNLIcorpus
The most widely used evaluation metric for sum- isinthesameformatasSNLI,butwithmorevar-
marizationisROUGEwhichcomparesthesystem iedtextgenres. Recently,thesecorporahavebeen
summary with the human generated summary or translatedintoTurkish(Buduretal.,2020). Inthis
summaries by considering the overlapping units
study,weusedtheNLI-TRdataset.1
such as n-gram, word sequences and word pairs
3.2 SemanticTextualSimilarity
(Lin, 2004). Recently, there has been a range of
studies focusing on the evaluation of factual cor- Semantictextualsimilarityaimstodeterminehow
rectness in the generated summaries. Falke et al. similar two pieces of texts are. There are many
(2019)hasstudiedwhethertextualentailmentcan applicationareassuchasmachinetranslation,sum-
beusedtodetectfactualerrorsingeneratedsum- marization, text generation, question answering,
mariesbasedontheideathatthesourcedocument dialogueandspeechsystems. Ithasbecomeare-
should entail the information in a summary. The markableareawiththecompetitionsorganizedby
authorsinvestigatedwhetherfactualerrorscanbe SemEvalsince2012.
reducedbyrerankingthealternativesummariesus- Semantictextualsimilaritystudiesareverycom-
ing models trained on NLI datasets. They found moninEnglish,andarebasedondatasetsthatare
that out-of-the-box NLI models do not perform annotated and given similarity scores by human
wellonthetaskoffactualcorrectness. Kryscinski annotators. However,annotationiscostlyandtime
etal. (2020)proposedamodel-basedapproachon consuming. Recently, with the increase of suc-
thedocument-sentencelevelforverifyingfactual cess in machine translation and the development
consistency in generated summaries. Zhao et al. ofmulti-languagemodels,ithasbecomepossible
(2020) addressed the problem of unsupported in- to use datasets by translating them from one lan-
formation in the generated summaries known as guagetoanother,e.g.,IsbisterandSahlgren(2020),
factual hallucination. Durmus et al. (2020) and Buduretal. (2020).
Wangetal. (2020)suggestedquestionanswering Inthisstudy,weusetheEnglishSTSBenchmark
basedmethodstoevaluatethefaithfullnessofthe (STSb)dataset(Ceretal.,2017)thatwetranslated
generatedsummaries. into Turkish using the Google Cloud Translation
API.2,3 The STSb dataset consists of all the En-
In addition to the studies focusing on summa-
glish datasets used in SemEval STS studies be-
rization evaluation, there are some recently pro-
tween2012and2017. Itconsistsof8628sentence
posed metrics to evaluate generated text with the
pairs(5749train,1500dev,1379test),(seeTable3
gold standard. Zhang et al. (2019) proposed
BERTScorethatusesBERT(Devlinetal.,2019) 1NLI-TRdatasetconsistsofthetranslationsofSNLIand
to compute a similarity score between the gener- MultiNLIdatasetsavailableonGitHub:https://github.com/
boun-tabi/NLI-TR
atedandreferencetext. Severalrecentworkspro-
2https://cloud.google.com/translate/docs/basic/
posednewevaluaitonmetricsformachinetransla-
translating-text
tion(BLEURT(Sellametal.,2020),COMET(Rei 3https://github.com/verimsu/STSb-TR
Sentence1 Sentence2 SimilarityScore
Adamatabiniyor. Biradamatabiniyor.
5.0
(Themanisridingahorse.) (Amanisridingonahorse.)
Birkızuc¸urtmauc¸uruyor. Kos¸anbirkızuc¸urtmauc¸uruyor.
4.0
(Agirlisflyingakite.) (Agirlrunningisflyingakite.)
Biradamgitarc¸alıyor. Biradams¸arkıso¨ylu¨yorvegitarc¸alıyor.
3.6
(Amanisplayingaguitar.) (Amanissingingandplayingaguitar.)
Biradamgitarc¸alıyor. Birkızgitarc¸alıyor.
2.8
(Amanisplayingaguitar.) (Agirlisplayingaguitar.)
Birbebekkaplanbirtoplaoynuyor. Birbebekbiroyuncakbebekleoynuyor.
1.6
(Ababytigerisplayingwithaball.) (Ababyisplayingwithadoll.)
Birkadındansediyor. Biradamkonus¸uyor.
0.0
(Awomanisdancing.) (Amanistalking.)
Table1: SampletranslationsfromSTSb-TRdatasetandthecorrespondinglabelstakenfromtheEnglishdataset.
OriginalEnglishsentencesaregiveninparenthesis.
fordetails). Inthisdataset,eachsentencepairwas S2: Group of people sitting at table of
annotatedbycrowdsourcingandassignedaseman- restaurant.
ticsimilarityscore. Fivescoreswerecollectedfor
T2: Birgrupinsanrestoranmasadaotu-
each pair and gold scores were generated by tak-
ruyor.
ingthemedianvalueofthesescores(Agirreetal.,
2016). Scoresrangefrom0(nosemanticsimilar- C2: Bir grup insan restoran masasında
ity)to5(semanticallyequivalent)onacontinuous oturuyor.
scale. Some examples from the STS dataset and
theirtranslationsaregiveninTable1.
Inthispaper,weassumedthatsuchtranslation
Here,weapplyvariousstate-of-the-artmodels
errorswillnotcauseamajorprobleminoursim-
onthetranslateddataset,andthebestperforming
ilaritymodels. Inordertoverifyourassumption,
fourmodelsareusedforsemanticsimilaritybased
wetestedthequalityoftranslationsbyselecting50
evaluation metric for the task of abstractive sum-
sentencepairs(100sentences)randomly,consider-
marization.
ingthepercentageofthecategoriesinthedataset.
So,6,19and25pairschosenfromforum,caption
3.3 TranslationQuality
andnewscategoriesrespectively. Thesesentences
Itispossibletoencountersometranslationerrorsin were translated by three native Turkish speakers
thetranslatedtexts. Themoststrikingmistakesare whoarefluentinEnglish. Weevaluatedqualityof
relatedtoexpressionsthatarenotusedinTurkish. the system translations with the three references
Forinstance,thesentenceinS1istranslatedasT1; usingBLEU(Papinenietal.,2002)score. Weused
however,amoreappropriatetranslationwouldbe the SacreBLEU4 tool (Post, 2018) version 1.5.1
C1, as”sitting”istranslateddifferentlyforinani- andfoundBLEUscoreas60.21whichshowsthat
matesubjects. oursystemtranslationscanbeconsideredasvery
highqualitytranslations(Google). Therefore,no
S1: Oldgreenbottlesittingonatable. changeshavebeenmadetothetranslations.
T1: Birmasadaoturaneskiyes¸ils¸is¸e. Table 2 shows vocabulary size (cased and un-
cased),type/tokenratio,averagewordlengthand
C1: Birmasadaduraneskiyes¸ils¸is¸e.
average sentence length values for English and
Turkishdatasets.5
Another typical error is possessive agreement
mismatch. Forexample, thesentenceS2istrans-
latedasT2butthecorrecttranslationwouldbeC2.
4https://github.com/mjpost/sacrebleu
5Onlythepunctuationmarksaroundthewordandatthe
endofsentencesweredeleted.
VocabSize VocabSize Type/Token AvgWord AvgSentence
Language
(Cased) (Uncased) Ratio Length Length
English 18,736 16,225 0.09 4.62 10.15
Turkish 29,461 26,649 0.19 6.20 8.26
Table 2: English and Turkish STSb dataset statistics. Vocab size is the word count and type/token ratio is the
numberofdifferentwordsdividedbythetotalnumberofwords. Wordlengthistheamountofcharactersinthe
wordandsentencelengthisthenumberofwordsinasentence.
Train Dev Test Total trained on multilingual data for translation lan-
News 3,299 500 500 4,299 guage modeling. The model produces language-
Caption 2,000 625 625 3,250 independent sentence embeddings for 109 lan-
Forum 450 375 254 1,079 guages,includingTurkish(Fengetal.,2020). Sim-
Total 5,749 1,500 1,379 8,628 ilartotheLASERmodel,Turkishsentenceembed-
dings were computed using a pre-trained LaBSE
Table3: STSbdatasetstatisticsintermsofnumberof
model.
sentencepairs.
MUSE MultilingualUniversalSentenceEncoder
(MUSE) model is a sentence embedding model
4 ExperimentsforSemanticTextual
trainedonmultiplelanguagesatthesametime. The
Similarity
modelcreatesacommonsemanticembeddingarea
Inordertoassessthesemanticsimilaritybetween foratotalof16languages,includingTurkish(Yang
apairoftexts,therearetwomainmodelstructures: etal.,2020). Inthisstudy,CNN7andTransformer8
1)Sentencerepresentationmodelsthattrytomapa modelsthataresharedpubliclyinTensorFlowHub
sentencetoafixed-sizedreal-valuevectorscalled areused.
sentence embeddings. 2) Cross-encoders that di-
BERT Bidirectional Encoder Representations
rectly compute the semantic similarity score of a
from Transformers (BERT) is designed to pre-
sentencepair.
traindeepbi-directionalrepresentationsfromunla-
Inthispaper,weexperimentedwithstate-of-the-
beledtextbyconditioningtogetherinbothleftand
artsentencerepresentationmodelsthatareapplica-
right context on all layers (Devlin et al., 2019).
bletoTurkish(language-specificandmultilingual
In this study, BERTurk9 and M-BERT10 (Pires
models) and BERT cross-encoders. In sentence
etal.,2019)modelswereused. Sentenceembed-
representation models, we obtained the semantic
dings were obtained by averaging the BERT em-
similarityscoresusingcosinesimilarity. Allmod-
beddings.11 Inaddition,themodelswereintegrated
elsweretestedontheSTSb-TRtestdataset.
intotheSiamesenetworkthatweexplainedinsec-
4.1 SentenceRepresentationModels tion4.1.
We experimented with LASER, LaBSE, MUSE, XLM-R RoBERTa Transformer model12 has
BERT,XLM-RandSentence-BERTmodelsasex-
been trained on a large multilingual data using a
plainedbelow.
multilingualmaskedlanguagemodelinggoal(Con-
neauetal.,2020). Inthisstudy,weusedthemodel
LASER Language-AgnosticSEntenceRepresen-
tocomputesentenceembeddingssimilartoBERT
tations(LASER)isalanguagemodelbasedonthe
models. We also integrated it into the Siamese
BiLSTM encoder trained on parallel data target-
networkusedinSentence-BERT.
ingtranslation. Themodelhasbeentrainedin93
languages,includingTurkish.6 Inthisstudy,Turk- 7https://tfhub.dev/google/
ish sentence embeddings were computed using a universal-sentence-encoder-multilingual/3
8https://tfhub.dev/google/
pre-trainedLASERmodel.
universal-sentence-encoder-multilingual-large/3
9https://huggingface.co/dbmdz/bert-base-turkish-cased
LaBSE Language-agnosticBERTSentenceEm-
10https://huggingface.co/bert-base-multilingual-cased
bedding (LaBSE) is a BERT variant masked and 11TheoutputoftheCLSvectorsyieldssignificantlylower
resultscomparedtotheresultsobtained.
6https://github.com/facebookresearch/LASER 12https://huggingface.co/xlm-roberta-base
Sentence-BERT Sentence-BERT (SBERT) Model Pearson Spearman
NottrainedforSTS
(also called Bi-Encoder BERT) is a modification
Avg.BERTurkembeddings 54.48 55.23
ofpre-trainedBERTnetwork(orothertransformer Avg.M-BERTembeddings 50.44 50.43
models) using Siamese and ternary network Avg.XLM-Rembeddings 20.22 41.81
LASER 69.86 70.18
structures (Reimers and Gurevych, 2019). The
LaBSE 72.24 71.74
modelderivesclosefixed-sizesentenceembedding MUSE-CNN 71.09 69.91
MUSE-Transformer 76.32 74.84
invectorspaceforsemanticallysimilarsentences.
TrainedonSTS
Thetraininglossfunctiondiffersdependingonthe BERTurk+STS 83.32 82.22
datasetthemodelwastrainedon. Duringthetrain- M-BERT+STS 79.08 78.15
XLM-R+STS 79.18 78.56
ingontheNLIdataset,theclassificationobjective
S-BERTurk+STS 81.97 81.43
functionwasused;whereasduringthetrainingon S-M-BERT+STS 73.28 72.84
theSTSbdataset,theregressionobjectivefunction S-XLM-R+STS 71.89 71.02
TrainedonNLI+STS
wasused(ReimersandGurevych,2019).
BERTurk+NLI+STS 85.36 84.59
The classification objective function concate- M-BERT+NLI+STS 79.30 78.39
nates the sentence embeddings by element-wise XLM-R+NLI+STS 81.94 81.21
S-BERTurk+NLI+STS 82.85 83.31
differenceandmultipliesbyatrainableweight. The
S-M-BERT+NLI+STS 75.74 75.41
modeloptimizesthecrossentropyloss: S-XLM-R+NLI+STS 77.26 77.32
Table 4: Experiment results for semantic textual sim-
o = softmax(W (u,v,|u−v|)),W (cid:15)R3n×k
t t ilarity. BERTurk, M-BERT and XLM-R are cross-
encodermodels. S-BERTurk,S-M-BERTandS-XLM-
wherenisthesizeofthesentenceembedding,and
Rarebi-encodermodels. PearsonandSpearmancorre-
k isthenumberoflabels. lationswerereportedasρx100.
Intheregressionobjectivefunction,thecosine
similaritybetweentwosentenceembeddings,opti-
mizethemodelsformeansquareerrorloss.
estimatedsimilarityscoresandthegoldlabels. Ta-
ble 4 shows the results as ρ x 100. According to
4.2 Cross-Encoders
theresults,trainingthemodelsfirstontheNLI-TR
Weadoptedcross-encoderarchitectureasexplained
datasetincreasesthemodelperformance. Thisis
in Reimers and Gurevych (2019). In the cross-
particularlynoticeablefortheXLM-Rmodels. The
encoder,bothsentencesarepassedtothenetwork
BERTurkmodelalsogivesverygoodresultswhen
and a similarity score between 0 and 1 obtained;
trained directly on the STSb-TR dataset. Here,
no sentence embeddings are produced.13 We ex-
weobservethattheexistingmultilingualLASER,
perimentedwithBERTurk,M-BERT,andXLM-R
LaBSE, MUSE models without any training for
withtrainingonNLI-TRandSTSb-TRdatasets.
semantictextualsimilarity,giveverygoodresults.
Compared to these models, the performance of
4.3 ResultsforSemanticTextualSimilarity
BERTmodelswithouttrainingarequitelow. The
All models were individually trained on NLI-TR
bestresultswereobtainedbytrainingtheBERTurk
andSTSb-TRtrainingdatasets. Also,themodels
modelontheNLI-TRdatasetfirst,andthenonthe
trainedontheNLI-TRdatasetwerefine-tunedon
STSb-TRdataset.
theSTSb-TRdataset. Allmodelswerethentested
ontheSTSb-TRtestdataset.
Wetrained/fine-tunedthemodelsonSTSb-TR
5 ExperimentsforSummarization
dataset with 4 epochs and 10 random seeds14 as
suggestedbyReimersandGurevych(2018;2019).
To investigate the effectiveness of our semantic
Then,wereportedtheaveragetestresultsof5suc-
similaritymodelsforsummarizationevaluation,we
cessfulmodelsthatperformbestonthevalidation
computedthecorrelationsofROUGEscoresand
set. Themodelswereevaluatedbycalculatingthe
our best performing four similarity models with
Spearman and Pearson correlations between the
humanjudgmentsforastate-of-the-artabstractive
13https://www.sbert.net/examples/applications/
model. Wereportedsemanticsimilarityscoresfor
cross-encoder/README.html
extractivebaselinesaswellinordertoobservetheir
14OnlyS-XLM-R+STSwastrainedwith20randomseeds
tohaveatleast5successfulmodels. alignmnetwiththeROUGEscores.
Cross-Encoder Bi-Encoder ROUGE OtherMetrics
Model NLI+STS STS NLI+STS STS ROUGE-1 ROUGE-2 ROUGE-L BERTScore
Lead-1 52.11 55.71 59.18 61.67 26.56 17.31 25.31 73.72
Lead-3 60.78 61.86 69.72 71.01 30.04 18.90 28.83 74.15
mT5 59.00 61.03 66.43 68.29 33.22 22.44 31.90 75.90
Table 5: Results of the summarization models on MLSUM dataset. The values under Cross-Encoder are the
average similarity scores predicted by the models; whereas, the values under Bi-Encoder are the average cosine
similaritiesofsentenceembeddingscomputedbythesemodels. Allthevalueswerescaledto100.
Relevance Consistency Fluency HumanAverage
Metric Pearson Spearman Pearson Spearman Pearson Spearman Pearson Spearman
Rouge-1 42.79 43.87 28.18 32.36 21.40 20.30 36.79 37.51
Rouge-2 38.26 41.63 27.39 35.78 16.43 20.83 32.76 38.02
Rouge-L 41.83 41.95 26.29 28.85 20.17 18.63 35.15 35.11
BERTScore 45.49 45.75 25.14 22.47 24.74 19.85 37.88 38.07
S-BERTurk+STS 55.44 52.82 30.25 30.04 25.63 26.70 44.26 45.86
S-BERTurk+NLI+STS 58.77 58.72 32.80 32.67 31.24 30.17 48.80 51.85
BERTurk+STS 56.87 53.54 38.02 32.46 34.10 27.88 51.32 48.59
BERTurk+NLI+STS 59.98 59.17 39.95 34.24 34.62 29.31 53.54 52.10
Table6:PearsonandSpearmancorrelationsofROUGE,BERTScoreandproposedevaluationmetricswithhuman
judgments.
5.1 Dataset Lead-3 We selected the first three sentences of
thesourcetextasasummary,basedontheobser-
MLSUMisthefirstlarge-scaleMultiLingualSUM-
vationthattheleadingthreesentencesareastrong
marization dataset which contains 1.5M+ arti-
baselineforsummarization(Nallapatietal.,2017;
cle/summary pairs including Turkish (Scialom
Sharmaetal.,2019).
etal.,2020). Theauthorscompiledthedatasetfol-
lowingthesamemethodologyofCNN/DailyMail
mT5 MultilingualT5(mT5)(Xueetal.,2020)is
dataset. Theyconsiderednewsarticlesasthetext
avariantofT5model(Raffeletal.,2020)thatwas
inputandtheirpairedhighlights/descriptionasthe
pre-trainedfor101languagesincludingTurkishon
summary. TurkishdatasetwascreatedfromInter-
anewCommonCrawl-baseddataset. ForTurkish
netHaber15 bycrawlingarchivedarticlesbetween
summarization,weusedmT5modelfine-tunedon
2010 and 2019. All the articles shorter than 50
MLSUMdatasetavailableonHuggingFace.17 The
words or summaries shorter than 10 words were
modelwastrainedwith10epochs,8batchsizeand
discarded. Thedatawassplitintotrain,validation
10e-4learningrate. Themaxnewslengthwas784
andtestsets,withrespecttothepublicationdates.
andmaxsummarylengthwasdeterminedas64.18
Thedatafrom2010to2018wasusedfortraining;
databetweenJanuary-April2019wasusedforval-
5.3 Evaluations
idation; and data up to December 2019 was used
for test (Scialom et al., 2020). In this study, we Weevaluatedthesummarizationmodelsusingse-
obtained the Turkish dataset from HuggingFace manticsimilarity-basedevaluation,ROUGEscores,
collection.16 Thedatasetconsistsof249,277train, andhumanjudgments. Allthevalueswerescaled
11,565validation,and12,775testsamples. to100.
5.2 Models Semantic Similarity Evaluations We used the
bestperformingfoursemanticsimilaritymodelsto
WeexperimentedonMLSUMTurkishdatasetwith
evaluatethesummarizationmodels. Thevaluesun-
extractivebaselinesLead-1andLead-3andastate-
derCross-Encoderaretheaveragesimilarityscores
of-the-artabstractivemodelmT5describedbelow.
predictedbythemodels;whereas,thevaluesunder
Lead-1 We selected the first sentence of the Bi-Encoder are the average cosine similarities of
sourcetextasasummary. sentenceembeddingscomputedbythesemodels.
15www.internethaber.com 17https://huggingface.co/ozcangundes/
16https://github.com/huggingface/datasets/tree/master/ mt5-small-turkish-summarization
datasets/mlsum 18Duringinference,wesetmaxsummarylengthto120.
1 1
ROUGE-1 ROUGE-2 ROUGE-L S-BERTurk+STS S-BERTurk+NLI+STS
BERTScore BERTurk+STS BERTurk+NLI+STS
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0 0
Relevance Consistency Fluency HumanAvg Relevance Consistency Fluency HumanAvg
Figure1: Pearsoncorrelationsbetweendifferentevaluationmetricsandhumanevaluations.
1 1
ROUGE-1 ROUGE-2 ROUGE-L S-BERTurk+STS S-BERTurk+NLI+STS
BERTScore BERTurk+STS BERTurk+NLI+STS
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0 0
Relevance Consistency Fluency HumanAvg Relevance Consistency Fluency HumanAvg
Figure2: Spearmancorrelationsbetweendifferentevaluationmetricsandhumanevaluations.
ROUGE WereportedF1scoresforROUGE-1, Overall,5annotators(3universitystudents,1Ph.D.
ROUGE-2 and ROUGE-L. ROUGE scores were student,and1professor)evaluatedthesummaries.
computedusingrougepackageversion0.3.1.19 Average relevance was 3.50±0.78, average con-
sistencywas4.45±0.83,andaveragefluencywas
BERTScore We reported F1 score for
4.34±0.77.
BERTScore(Zhangetal.,2019).
6 Results
Human Evaluations Human evaluations were
conductedtoshowtheeffectivenessofourseman-
Quantitative Analysis We computed Pearson
ticsimilaritybasedevaluationmetric. Werandomly
and Spearman correlations of human judgments
selected50articlesfromthetestsetwiththeirpre-
withsemanticsimilarityandROUGEscores. Cor-
dictedsummariesviamT5model. Followingthe
relationvaluescanbeseeninTable6andarevisu-
workofFabbrietal. (2021),weaskednativeTurk-
alizedinFigure1andFigure2.20 Theresultsshow
ish annotators to rate each predicted summary in
that,ourcross-encodermodelshavesignificantly
termsofrelevance(selectionofimportantcontent
bettercorrelationswithrelevance,consistency,flu-
from the source), consistency (the factual align-
ency, and human average. The correlations are
ment between the summary and the summarized
higher compared to the bi-encoder models. This
source)andfluency(thequalityofindividualsen-
tences)intherangeof1(verybad)to5(verygood). 20All the correlations were significant (p<.05) except
forthecorrelationsbetweenFluencyandS-BERTurk+STS,
19ThisisthepackageandversionthattheauthorsofML- BERTScore, ROUGE-L as well as correlations between
SUMreported:https://github.com/recitalAI/MLSUM. BERTScoreandConsistency.
snoitalerroCnosraeP
snoitalerroCnamraepS
Article-1
Seattles¸ehrininmerkezindemeydanagelenolayda, Kanadalıoldug˘ubelirtilenadam, birotomobildenso¨ktu¨g˘u¨ sunroof
camıylabo¨lgedebulunanarac¸larıno¨ncamlarınıparc¸aladı. Arac¸larınkaputlarınadac¸ıkanadam,c¸evredekibirc¸okaraca
maddihasarverdi.Sonrasında,c¸evredebulunanotoparkgo¨revlisiadamamu¨dahaleetmekistedi.Elindekicamtavanlabu
sefergo¨revliyesaldıranadam,c¸evredekidig˘erinsanlarınmu¨dahalesiyleetkisizhalegetirildi.Olayyerinegelenpolis,adamı
go¨zaltınaalırken;adamınuyus¸turucuetkisialtındaoldug˘ubildirildi.
ReferenceSummary
ABD’debiradam,elindekisunroofcamıylaotomobillerino¨ncamlarınıparc¸aladı. Adamamu¨dahaleetmekisteyenpark
go¨revlisideadamınsaldırısınaug˘radı.
GeneratedSummary
ABD’debirotomobildenso¨ktu¨g˘u¨sunroofcamıylabo¨lgedebulunanarac¸larıno¨ncamlarınıparc¸alayanadam,c¸evredekidig˘er
insanlarınmu¨dahalesiyleetkisizhalegetirildi.
ROUGE-(1/2/L):30.00,10.53,25.00
SemanticSimilarityScoresBERTurk+NLI+STS(CrossEncoder/Bi-Encoder):73.67/74.35
HumanEvaluations(relevance/consistency/fluency/avg):3.81/4.36/4.36/4.18
Article-2
Yangın,Salihli-Ko¨pru¨bas¸ıyoluTaytanMahallesiC¸aldırlıkmevkisindemeydanageldi.Edinilenbilgiyego¨re,seyirhalinde
ilerleyenServetDurmus¸idaresindeki43HE737plakalıotomobilinmotorbo¨lu¨mu¨ndeyangınc¸ıktı.Alevlerinbu¨yu¨mesiyle
birlikteotomobilates¸topunado¨ndu¨. Su¨ru¨cu¨ Durmus¸hemenitfaiyeekiplerinehaberverirkenolayyerinegelenManisa
Bu¨yu¨ks¸ehirBelediyesiSalihli˙ItfaiyeAmirlig˘iekipleriyangınamu¨dahaleetti.So¨ndu¨ru¨lenotomobilkullanılamazhalegeldi.
Yangınlailgilisorus¸turmabas¸latıldı.
ReferenceSummary
Manisa’nınSalihliilc¸esindeseyirhalindeilerleyenotomobilalevlereteslimoldu.
GeneratedSummary
Manisa’daseyirhalindekiotomobilinmotorbo¨lu¨mu¨ndeyangınc¸ıktı.
ROUGE-(1/2/L):11.11/0/11.11
SemanticSimilarityScoresBERTurk+NLI+STS(CrossEncoder/Bi-Encoder):76.16/81.75
HumanEvaluations(relevance/consistency/fluency/avg):4.0/4.8/5.0/4.6
Table7: ExamplearticlesfromMLSUMTurkishtestdatasetwiththeirreferenceandgeneratedsummaries. The
wordsthatappearinbothreferenceandgeneratedsummaryareinblue,whilethesemanticallysimilarwordsare
inred. Theitalictextpiecesinthearticleappearinthegeneratedsummary.
alsoshowsthatpredictedsimilarityscoresaremore tionscores. Ourproposedmetricscancapturethis
reliablethancomputedcosinesimilarities. butapparentlyROUGEcannot.
Whilethemainideaofthispaperistoevaluate
abstractivesummarization,wealsoshowedthatan 7 Conclusion
extractive Lead-3 baseline yields better semantic
Inthisstudy,wepresentedthefirstTurkishseman-
similarityscorescomparedtotheabstractivemT5
tic textual similarity corpus, called STSb-TR, by
althoughitoutperformstheextractivebaselinesin
translating the original English STSb dataset via
termsofBERTScoreandROUGEscores.
machine translation. We showed that the dataset
QualitativeAnalysis Weanalyzedtheeffective- hashighqualitytranslationsanddoesnotrequire
nessofourproposedmetricsqualitativelyaswell. costlyhumanannotation. Weappliedstate-of-the-
In Table 7, we show two example articles. In art models to the STSb-TR dataset, and used the
the first one, there are some overlapping words best performing four models as evaluation met-
between two sentences and they share semanti- rics for the text summarization task. We used
cally similar information in the following parts: natural language inference (NLI) models and ob-
”ABD’de bir adam, elindeki sunroof camıyla oto- servedthatwecanimproveoursemanticsimilar-
mobillerino¨ncamlarınıparc¸aladı”and”ABD’de ity models. We found high correlations between
birotomobildenso¨ktu¨g˘u¨ sunroofcamıylabo¨lgede human judgments and our models, compared to
bulunanarac¸larıno¨ncamlarınıparc¸alayanadam”. BERTScore and ROUGE scores. Our qualitative
So,wecansaythatbothROUGEandsemanticsim- analyses showed that the proposed models can
ilarity scores can be acceptable for this example. capture the semantic similarity of reference and
Ontheotherhand,thesecondexampleismorecrit- predicted summaries which cannot be caught by
icalasithasonlyoneoverlappingwordbetween ROUGEscores. Weconcludethatourmodelscan
the reference and generated summary; however, beappliedasevaluationmetrictoabstractivesum-
there is a high semantic similarity between them marizationinTurkish.
andthepredictedsummaryhashighhumanevalua-
References Rankinggeneratedsummariesbycorrectness:Anin-
terestingbutchallengingapplicationfornaturallan-
Eneko Agirre, Carmen Banea, Daniel Cer, Mona
guageinference. InProceedingsofthe57thAnnual
Diab, Aitor Gonzalez Agirre, Rada Mihalcea, Ger-
Meeting of the Association for Computational Lin-
man Rigau Claramunt, and Janyce Wiebe. 2016.
guistics,pages2214–2220.
SemEval-2016 task 1: Semantic textual similar-
ity, monolingual and cross-lingual evaluation. In Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen
SemEval-2016. 10th International Workshop on Se- Arivazhagan, and Wei Wang. 2020. Language-
manticEvaluation;2016Jun16-17;SanDiego,CA. agnosticBERTsentenceembedding. arXivpreprint
Stroudsburg(PA):ACL;2016.p.497-511.ACL(As- arXiv:2007.01852.
sociationforComputationalLinguistics).
Google. Evaluatingmodels—automltranslationdoc-
SamuelBowman,GaborAngeli,ChristopherPotts,and umentation—googlecloud.
Christopher D Manning. 2015. A large annotated
corpus for learning natural language inference. In Tim Isbister and Magnus Sahlgren. 2020. Why
Proceedings of the 2015 Conference on Empirical not simply translate? a first Swedish evaluation
Methods in Natural Language Processing, pages benchmark for semantic similarity. arXiv preprint
632–642. arXiv:2009.03116.
Emrah Budur, Rıza O¨zc¸elik, Tunga Gu¨ngo¨r, and WojciechKryscinski,BryanMcCann,CaimingXiong,
Christopher Potts. 2020. Data and representation and Richard Socher. 2020. Evaluating the factual
forTurkishnaturallanguageinference. InProceed- consistency of abstractive text summarization. In
ings of the 2020 Conference on Empirical Methods Proceedings of the 2020 Conference on Empirical
in Natural Language Processing (EMNLP), pages MethodsinNaturalLanguageProcessing(EMNLP),
8253–8267. pages9332–9346.
Daniel Cer, Mona Diab, Eneko Agirre, In˜igo Lopez- Chin-Yew Lin. 2004. ROUGE: A package for auto-
Gazpio, and Lucia Specia. 2017. SemEval-2017 matic evaluation of summaries. In Text summariza-
task1: Semantictextualsimilaritymultilingualand tionbranchesout,pages74–81.
crosslingual focused evaluation. In Proceedings of
the11thInternationalWorkshoponSemanticEvalu- Chi-kiuLo.2019. Yisi-aunifiedsemanticMTquality
ation(SemEval-2017),pages1–14. evaluationandestimationmetricforlanguageswith
different levels of available resources. In Proceed-
AlexisConneau, KartikayKhandelwal, NamanGoyal, ings of the Fourth Conference on Machine Transla-
Vishrav Chaudhary, Guillaume Wenzek, Francisco tion(Volume2: SharedTaskPapers, Day1), pages
Guzma´n, E´douard Grave, Myle Ott, Luke Zettle- 507–513.
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In RameshNallapati,FeifeiZhai,andBowenZhou.2017.
Proceedingsofthe58thAnnualMeetingoftheAsso- Summarunner: Arecurrentneuralnetworkbasedse-
ciation for Computational Linguistics, pages 8440– quencemodelforextractivesummarizationofdocu-
8451. ments. In Proceedings of the AAAI Conference on
ArtificialIntelligence,volume31.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of KishorePapineni,SalimRoukos,ToddWard,andWei-
deep bidirectional transformers for language under- JingZhu.2002. Bleu: amethodforautomaticeval-
standing. InProceedingsofthe2019Conferenceof uationofmachinetranslation. InProceedingsofthe
the North American Chapter of the Association for 40th annual meeting of the Association for Compu-
ComputationalLinguistics: HumanLanguageTech- tationalLinguistics,pages311–318.
nologies, Volume1(LongandShortPapers), pages
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
4171–4186.
How multilingual is multilingual BERT? In Pro-
EsinDurmus,HeHe,andMonaDiab.2020. FEQA:A ceedings of the 57th Annual Meeting of the Asso-
question answering evaluation framework for faith- ciation for Computational Linguistics, pages 4996–
fulnessassessmentinabstractivesummarization. In 5001.
Proceedingsofthe58thAnnualMeetingoftheAsso-
ciation for Computational Linguistics, pages 5055– Matt Post. 2018. A call for clarity in reporting bleu
5070. scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
Alexander R Fabbri, Wojciech Krys´cin´ski, Bryan 191.
McCann, Caiming Xiong, Richard Socher, and
Dragomir Radev. 2021. SummEval: Re-evaluating ColinRaffel,NoamShazeer,AdamRoberts,Katherine
summarizationevaluation. TransactionsoftheAsso- Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
ciationforComputationalLinguistics,9:391–409. Wei Li, and Peter J Liu. 2020. Exploring the lim-
its of transfer learning with a unified text-to-text
Tobias Falke, Leonardo FR Ribeiro, Prasetya Ajie transformer. JournalofMachineLearningResearch,
Utama, Ido Dagan, and Iryna Gurevych. 2019. 21:1–67.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy
Lavie.2020. COMET:AneuralframeworkforMT Guo, Jax Law, Noah Constant, Gustavo Hernandez
evaluation. In Proceedings of the 2020 Conference Abrego, Steve Yuan, Chris Tar, Yun-hsuan Sung,
onEmpiricalMethodsinNaturalLanguageProcess- etal.2020. Multilingualuniversalsentenceencoder
ing(EMNLP),pages2685–2702. forsemanticretrieval. InProceedingsofthe58thAn-
nual Meeting of the Association for Computational
Nils Reimers and Iryna Gurevych. 2018. Why com- Linguistics: SystemDemonstrations,pages87–94.
paring single performance scores does not allow
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
to draw conclusions about machine learning ap-
proaches. arXivpreprintarXiv:1803.09578. Weinberger, and Yoav Artzi. 2019. BERTScore:
Evaluating text generation with BERT. In Interna-
tionalConferenceonLearningRepresentations.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT:SentenceembeddingsusingSiameseBERT-
ZhengZhao,ShayBCohen,andBonnieWebber.2020.
networks. InProceedingsofthe2019Conferenceon
Reducingquantityhallucinationsinabstractivesum-
EmpiricalMethodsinNaturalLanguageProcessing
marization. arXivpreprintarXiv:2009.13312.
andthe9thInternationalJointConferenceonNatu-
ralLanguageProcessing(EMNLP-IJCNLP),pages
3973–3983.
ThomasScialom,Paul-AlexisDray,SylvainLamprier,
Benjamin Piwowarski, and Jacopo Staiano. 2020.
MLSUM: The multilingual summarization corpus.
InProceedingsofthe2020ConferenceonEmpirical
MethodsinNaturalLanguageProcessing(EMNLP),
pages8051–8067.
Thibault Sellam, Dipanjan Das, and Ankur Parikh.
2020. BLEURT: Learning robust metrics for text
generation. InProceedingsofthe58thAnnualMeet-
ingoftheAssociationforComputationalLinguistics,
pages7881–7892.
Eva Sharma, Chen Li, and Lu Wang. 2019. BIG-
PATENT: A large-scale dataset for abstractive and
coherentsummarization. InProceedingsofthe57th
Annual Meeting of the Association for Computa-
tionalLinguistics,pages2204–2213.
BrianThompsonandMattPost.2020. Automaticma-
chine translation evaluation in many languages via
zero-shotparaphrasing. InProceedingsofthe2020
Conference on Empirical Methods in Natural Lan-
guageProcessing(EMNLP),pages90–121.
Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.
Askingandansweringquestionstoevaluatethefac-
tual consistency of summaries. In Proceedings of
the58thAnnualMeetingoftheAssociationforCom-
putationalLinguistics,pages5008–5020.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tenceunderstandingthroughinference. InProceed-
ingsofthe2018ConferenceoftheNorthAmerican
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(LongPapers),pages1112–1122.
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2020. mT5: A mas-
sively multilingual pre-trained text-to-text trans-
former. arXivpreprintarXiv:2010.11934.

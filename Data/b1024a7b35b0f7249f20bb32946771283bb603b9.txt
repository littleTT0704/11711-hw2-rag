ONADVERSARIALROBUSTNESSOFLARGE-SCALEAUDIOVISUALLEARNING
JunchengBLi∗,ShuhuiQu∗,XinjianLi,Po-Yao(Bernie)Huang,FlorianMetze
CarnegieMellonUniversity
ABSTRACT multi-modalmodelsisthehighnon-convexityandnon-linearityof
thedecisionboundariesinhighdimensionallatentspaces.
Asaudio-visualsystemsarebeingdeployedforsafety-criticaltasks
Inthiswork,wediscussrobustnessofmulti-modalneuralnet-
such as surveillance and malicious content filtering, their robust-
work models for classification tasks in the large-scale setting. We
ness remains an under-studied area. Existing published work on
firstmeasurepoint-wiserobustnessthroughtheempiricalmaximum
robustness either does not scale to large-scale dataset, or does not
allowableperturbationin(cid:96) norm. Basedonpoint-wiserobustness,
dealwithmultiplemodalities. Thisworkaimstostudyseveralkey p
weshowthereexistcounterexamplestothegeneralclaimthatmulti-
questions related to multi-modal learning through the lens of ro-
modal models are more robust compare to the uni-modal models.
bustness: 1)Aremulti-modalmodelsnecessarilymorerobustthan
Due to the limitation of point-wise robustness in terms of scala-
uni-modal models? 2) How to efficiently measure the robustness
bilityandgeneralizability,class-wiserobustnessisanecessaryand
of multi-modal learning? 3) How to fuse different modalities to
practical compliment to tackle large-scale multi-modal robustness
achieve a more robust multi-modal model? To understand the ro-
problems. Instead of measuring the accuracy drop caused by run-
bustnessofthemulti-modalmodelinalarge-scalesetting,wepro-
ning universal adversarial perturbation in different magnitude and
pose a density-based metric, and a convexity metric to efficiently
(cid:96) norm[3],wedefinetheclass-wisemetricbyusing1)thedensity
measurethedistributionofeachmodalityinhigh-dimensionallatent p
ofsampleswithinthehigh-dimensionalballcenteredatthecentroid
space. Ourworkprovidesatheoreticalintuitiontogetherwithem-
ofeachclasswithacertain(cid:96) radius;2)theconvexityofsamplesin
piricalevidenceshowinghowmulti-modalfusionaffectsadversarial p
thehighdimensionallatentspace.WeevaluateourmetricontheAu-
robustnessthroughthesemetrics. Wefurtherdeviseamix-upstrat-
dioSet[1]andKinetic-Soundsdataset[2]. Theresultsindicatethat
egy based on our metrics to improve the robustness of the trained
multi-modal models are only more robust measured by class-wise
model. OurexperimentsonAudioSet[1]andKinetics-Sounds[2]
metricsforalimitednumberofclasses. Wealsoobservethepoint-
verify our hypothesis that multi-modal models are not necessarily
wise robustness of classification results vary greatly depending on
morerobustthantheiruni-modalcounterpartsinthefaceofadver- thevarianceofthedatawithspecificclasslabel.
sarialexamples. Wealsoobserveourmix-uptrainedmethodcould
Inspired by our observations, we propose a density-convexity-
achieveasmuchprotectionastraditionaladversarialtraining,offer-
basedmix-upfusiontechniquetosmooththedecisionboundaryand
ingacomputationallycheapalternative.1
addrobustnesstothefusedmodel. Ourproposedmixupcouldboth
1. INTRODUCTION improve class-wise robustness and point-wise robustness upon our
baseline fusion model while increasing the accuracy compared to
Nowadays,uploadingaclipofvideooraudiotosocialmediaplat- vanillafusionmethods. Wealsocompareittotraditionaladversar-
forms such as Facebook or YouTube will trigger a multi-modal ialtraining,wherewealsoseecompetitiverobustaccuracy. These
content filtering algorithm to proactively search for potentially advances allow us to address adversarial robustness in large-scale
policy-violating content; home monitoring devices such as Nest- multi-modalsettingsforthefirsttime.
Cams or RingCams are presumably using audio-visual models to
identify events in the monitored area. Multi-modal classification 2. RELATEDWORK
in safety critical, audio-visual tasks therefore calls for a thorough
Previousworkssuchas[7,6,10]focusedonpoint-wiserobustness,
understandingofitsrobustness,besidesitsaccuracy.
studyingthemaximumallowableradiusofcenteredChebyshevball:
Manyrecentresearcheshavedocumentedneuralnetworkmod-
a(cid:96) ballcenteredataninputpoint,withinwhichtheoutputclassofa
elscouldbevulnerabletoadversarialattacks,manipulationsofthe p
givenneuralnetworkwithremainsunchanged,treatingthedecision
input to a classifier specifically crafted to be inconspicuous to hu-
boundaryofthemodelasaconvexornon-convexpolytope.Thisfor-
mans, but which cause the classifier to predict incorrectly [3, 4].
mulationcertainlyprovideasafethresholdtodefendagainstadver-
Concernsaboutpotentialadversarialexampleshavesparkedahuge
sarialattacks, whereasitinvolvesexpensiveiterativecomputations
interest in the research community to study how can we train ro-
oneachanchorpoint,resultinginhugedifficultytoscale[7,6],most
bustmodelsthatdefendagainstpotentialperturbations[4,5]. How-
of them depend their claims on small-scale datasets like MNIST
ever,buildingsuchadversariallyrobustmodelsischallenging[5].A
orCIFAR-10. Forthelarge-scalemultimodaldatasetssuchasAu-
smaller but still substantial line of work has emerged to show that
dioSet,suchaverificationwouldhardlybefeasible.
we could have formal verification of the robustness of neural net-
Some recent works are try to study defence methods in large
workmodels[6,7]. However, suchmethodsaresubjectedtovery
scale,includingadversarialtraining[9,11],randomization[8],and
tightconstraintsandarenotoriouslydifficulttoscale.Sofar,despite
modelensemble[12]. Mostofthemmeasuredrobustnessbypoint-
some successful large-scale empirical evaluations[8, 9] on image-
wiseaccuracyorattacksuccessrateforspecificattackbudget(cid:15)and
onlydatasets,robustnessofmulti-modallearninghasnotbeenfully
numberofiterations. However,thesemetricsareoftentoogeneral
understood.AsillustratedinFig.1,themajorchallengetoanalyzing
toreflecttheclassifier’sbehaviorundertheinfluenceofadversarial
*equalcontribution,AcceptedtoICASSP2022 perturbation. This motivates us to look into both class-wise accu-
1Implementation:https://github.com/lijuncheng16/AudioSetDoneRight racychangesalongwithpoint-wiseaccuracychangeinordertohave
2202
rpA
12
]DS.sc[
2v22121.3022:viXra
a better chance of understanding potential reasons of label change
causedbyadversarialperturbations.
Audio-visuallearning[13]itselfismorecomplicatedthanim-
ageclassification,andthecurrentfocusstillseemstobeimproving
accuracy [14]. The robustness of multimodal classification mod-
els involving large-scale video-audio dataset has never been stud-
ied rigorously. [10] considered the robustness of deep neural net-
worksonvideosandexperimentedonUCF101dataset[15]which
isarelativelysmalldataset. Theymeasuredrobustnessbythemax-
imum safe radius (point-wise), which computes the minimum dis-
tancefromtheopticalflowsequenceobtainedfromagiveninputto
thatofanadversarialexampleintheneighbourhoodoftheinput.To
our knowledge, our work is the first to comprehensively study the
robustnessofmulti-modalsmodelsbothconsistofvideoandaudio
againstadversarialperturbationcausechangestobothmodalities.
3. BACKGROUND
3.1. UniversalAdversarialPerturbations
Theproblemofcomputinguniversaladversarialperturbationtoat-
tackaclassificationmodelf bymaximizingthefollowing[9]:
E max [L(f(x(cid:48)),y)]
(x,y)∼D
x(cid:48)∈C(x) (1)
Fig.1: (a)Anillustrationofmulti-modalfusion. (b)Illustrationof
subjecttoC(x)={a∈R:||a−x|| ≤(cid:15)}.
p thecentroidbaseddensitymetricρRτ,p.
c
whereListhelossfunction,xisinputandyislabel,Disthedataset,
andx(cid:48) = x+δ isourperturbedinput. Wewanttofindsomeper-
turbationx(cid:48) thatlooks“indistinguishable”fromx, yetisclassified Accordingly,tocomputevideoperturbationagainstvideoclassifier
incorrectlybyf evenwhenxisclassifiedcorrectly. g(x),weperformthefollowingPGDstep:
To solve such a constrained optimization problem, one of the
(cid:18) (cid:19)
mostcommonmethodsutilizedtocircumventthenon-exact-solution δ :=P δ −α ∇ δVL(f(h(x V +δ V)⊕g(x A)),y)
issueistheProjectedGradientDescent(PGD)method[9]: V (cid:15) V (cid:107)∇ L(f(h(x +δ )⊕g(x )),y)(cid:107)
δV V V A p
(cid:18) (cid:19) (5)
∇ L(f(x+δ),y)
δ:=P (cid:15) δ−α (cid:107)∇δ L(f(x+δ),y)(cid:107) (2) Inthemorecomplicatedmulti-modalcase,wejointlyoptimizeδ A
δ p andδ ,where:
V
whereP istheprojectionontothe(cid:96) ballofradius(cid:15),andαisthe
(cid:15) p
gradientstepsize.
δ ,δ :=P (δ −α
∇ δ(V,A)L(f(h(x V +δ V)⊕g(x A+δ A)),y)
)
3.2. Multi-ModalAdversarialPerturbations A V (cid:15) (V,A) (cid:107)∇ δ(V,A)L(f(h(x V +δ V)⊕g(x A+δ A)),y)(cid:107) p
(6)
Under audio-visual multimodal learning setting, we formulate our
loss as L = L(f(g(x ) ⊕ h(x )),y), over the classifi- 4. CHALLENGECOMMONASSUMPTIONSIN
multi A V
cation function f, which can however readily be expanded to MULTIMODALLEARNING
additional modalities. g(x ) denotes the encoding of audio fea-
A
turesintoabottleneckrepresentationwithaudioencodingnetworks Thereisavaguenotionthatmultimodalsystemsaregenerallymore
(CSN) depicted in Figure 2, while h(x ) similarly represents the robustcomparedtounimodalmodels[13]: “havingaccesstomul-
V
(R2+1D)CNN [16] encoded video representation, and ⊕ indicates tiple modalities that observe the same phenomenon may allow for
concatenation of features. D and D indicating their individual more robust predictions.” From an information retrieval perspec-
A V
dataset. Thismorecomplicatedsettingrequiresustostudythead- tive, thisstatementistheoreticallytruesincethesameinformation
versarialperturbationcomputedagainstboththeaudioinputδ and wascapturedtwiceindifferentmodality,improvingtherobustness
A
thevideoinputδ ,andcanbewrittenoutas: ofmultimodalmodels. However, thisisnotalwaystrueifwerig-
V
orouslyconsiderhowadversarialperturbationsaffecttheneuralnet-
E max [L(f(x(cid:48)),y)]
(xA,y)∼DA;(xV,y)∼DVδA∈C(xA),δV∈C(xV)
(3)
workmodelasfollows.
subjecttoC(x)={a∈R:||a−x|| ≤(cid:15)}
p Theorem1 Thereexistsasamplex ∈D,andaunimodalsample-
i
Here, x(cid:48) = g(x A +δ A)⊕h(x V +δ V). ThesetC(x)isusually wise attack ∃||δ A,i|| p ≤ (cid:15) A or ∃||δ V,i|| p ≤ (cid:15) V that can break a
definedasaballofsmallradiusoftheperturbationsize(cid:15)(ofeither multimodalfusionnetworkf((x V,i ⊕x A,i),y i), changingitspre-
(cid:96) ∞,(cid:96)
2
or (cid:96) 1) around x. Thus, to compute uni-modal audio per- dictionlabely i.
turbation to attack the multimodal model, our PGD step could be
rewrittenoutas: Here,Disthedataset,and(cid:15) and(cid:15) arethepoint-wiserobust-
A V
(cid:18) ∇ L(f(g(x +δ )⊕h(x )),y) (cid:19) nessthresholdforeachuni-modalofsamplex i.Therefore,asacon-
δ A :=P (cid:15) δ A−α (cid:107)∇δA L(f(g(xA +δA )⊕h(xV )),y)(cid:107) jecture,aunimodalattackcanbreakamultimodalmodel,whichwe
δA A A V p empiricallyverifiedtheexistenceofsuchcasesinourexperiments.
(4)
TheproofofTheorem1canbefoundintheappendixpage.
5. METRICSFORCLASS-WISEROBUSTNESS 6. DENSITY-CONVEXITYBASEDMIX-UP
Point-wiserobustnessislimitedintermsofscalabilityandgeneral- Asisnotedby[19],mix-uptechniquescouldpotentiallysmoothen
the decision boundary via generating virtual training samples by
izability.Class-wiserobustnessmetricisamoreefficientforalarge-
weightedsumofexistingtrainingsamples,whichimprovesgeneral-
scaledataset. Insteadofexhaustivelyrunninguniversaladversarial
izability.Inspiredbyourdensity-basedandconvexity-basedmetric,
perturbation we define two metrics to capture the main robustness weemployasimpleadjustmenttomixup:
propertyofeachclass.
x˜A=αxAi+(1−α)xAj;
5.1. Centroid-baseddensitymetric x˜V =αxVi+(1−α)xVj; (9)
Wecalculatetheclass-wisedensityoftheclass’shighdimensional
y˜=αyi+(1−α)yj;
l
p
normballbyafunctionofnumberofsamplesn
c
intheclassc whereα∈[0,1],(x Ai,x Vi,y i)and(x Aj,x Vj,y j)aretwotraining
samples, with both audio and video inputs drawn from 2 different
andthevolumeofthel normball. Inthiswork,n isthenumber
p c
classesy andy , subjecttoκ < T (T isanempiricalthreshold
ofsamplesofeachclassinAudioset. i j c
The centroid of a class (cid:12)
c
is the mean of bottleneck features ontheconvexity)andρR cτ,p > D (D isanempiricalthresholdon
l = g(x)ofsamplesxintheclassc: (cid:12) = (cid:80)n i=c 1li,c,wheren is thedensity),forbothclasses. BothT andDaredatasetdependent
c nc c parameters,T = 0.5,D = 8inthiswork. Effectively,weareaug-
thenumberofsamplesintheclassc. Inourcase,itisl = g(x )
A mentingthelessconvexclassesoftrainingdatawithmoresamples
foraudiomodalityorl=g(x )forvideomodality.Foreachclass,
V
we calculate the distance of samples in c to the centroid (cid:12) . The fromthe“denser”sampleswhichareclosertothecenterofitsfea-
c
radiusofaclassR onl normballisthemaximumdistanceofall turespace.
samplesinctothep c, ec ntroip d(cid:12) :R =max(cid:107)l −(cid:12) (cid:107) . 7. EXPERIMENTS
c p,c i,c c p,i=1,...,nc
The radiusof first τ percentage ofsamples closing tothe centroid
is R , and the n = τ ×n is the number of τ percentage 7.1. DatasetandModelSetup
τ,p,c τ,c c
samplesclosetothecentroid.Accordingto[18],thevolumeVp(R)
of the d-dimensional l norm ball with a radius R is:
Vp(Rd
) =
AudioSet [1] contains 2 Million 10-second YouTube video clips,
p d summing up to 5,800 hours annotated with 527 types of sound
(2Γ(1+1)R)2
p ,where,disthedimensionoftheball,Ristheradius, events.Wetrainandtestthemodelsaccordingtothetrain(1998999
Γ(d+1)
p samples)andtestsplit(20126samples)describedin[20]. Thein-
pisthe(cid:96) -norm,andΓistheGammafunction2. Now,weformally
p put for the audio branch are matrices of Mel filter bank features.
define the robustness of a class c with regard to τ quantile of the
classsamplex ’sdistancetothecentroid(cid:12) oftheclasscby: For the visual branch, we employ a (R2+1D)CNN + transformer
c c
backbone [16] to encode the spatial-temporal context. The clean
ρRτ,p,c=
nc−nτ,c
(7) performance (with no data augmentation) of our unimodal audio
c log(V dp(Rp,c))−log(V dp(Rτ,p,c)) model and audio-visual model are listed in Table 1 (italic font).
wherethenumeratoristhenumberofclasssampleswhosel dis- Kinetics-Sounds [2] is a subset of Kinetics [21] that contains 34
p
tancetocentroidlargerthanτ quantileofsamplesinc;R isthe classesofaudio-relatedevents(22,107train,1,504validation). We
τ,p,c
τ quantile of all class sample’s l distance to the class’s centroid. preprocess the Kinetic-sound dataset in the similar fashion. Our
p
Weperformthelogoperationonthevolumetoreducethescaleof cleanmultimodalbasline: 86.5%. WeuseKinetic-soundsonlyfor
Γfunctionfortheeaseofcomputation. Thiscanbeintuitivelyin- audio-visual performance since its audio-unimodal performance is
terpreted as the density in the outer crust of a ball as is shown in lowandthusnotrepresentative.
Fig. 1(b). Generally, the higher the density of the crust, the more
robustthesampleswithin/belowthecrustare.
5.2. Convexity-basedmetric
TheconvexsetCingeometryisdefinedasasetwheregivenanytwo
pointsx ,x ∈Cintheset,thesetcontainsthewholelinesegment
1 2
x=θx +(1−θ)x ,with0≤θ ≤1. Basedonourobservations
1 2
andconjecture,weproposetheconvexity-basedmetricasoneofthe
robustness measurement of the class. We construct the convex set
S = {xˆ |xˆ = θx +(1−θ)x ,θ ∼ U[0,1],∀x ,x ∈ C},
s s 1 2 1 2
andsamplenpointsfromit{xˆ ,...,xˆ |xˆ ∈ S}. Themetricisas
1 n i
follows:
(cid:80)n 1{f(xˆ )=y }
κ = i=1 i c (8)
c n
wherey istheclasslabel. Thehighertheκ is, themoreconvex
c c
thedecisionboundaryofclasscis. Inthiswork,wesetn = 2000.
Therefore, we use this metric as a proxy to measure how convex
theneuralnetworkis. Wehopetoseepositivecorrelationbetween
convexityandrobustness.
Forbothmetrics,weusethebottleneckfeatureltocalculatethe
valueofthemetric. Inlaterexperiments, weempiricallyshowthe
Fig.2:Theoverallarchitecture,theaudiobranch(left)usesConvo-
effectivenessofourmetricsbyconstrastingwiththeaccuracydrop
lutionself-attentionarchitecture,videobranchisontheright. Mid
causedbyuniversalperturbation[3].
fusioninvolvestheconcatenationstepdescribedinSS3.2.
2https://wikipedia.org/wiki/Gamma_function
Models Attack mAP AUC d-prime Wecouldalsoseethebenefitofourproposedmixup, whichcould
helpminimizetheaccuracylosssignificantlyonbothAudioSetand
AudioUniModal(PANNS)[23] No 0.383 0.963 2.521
Kinetics-Sounds,comparedtovanillamidfusionandtraditionalad-
AudioUniModal Yes 0.183 0.895 1.770
versarial training (AT). Interestingly, among AudioSet where au-
MidFusion(G-blend)[14] No 0.427 0.971 2.686
dio event classification shouldbe the dominant task, audio modal-
MidFusion YesA+V 0.182 0.889 1.836
ityshowedon-parrobustnesscomparedtovideo.WhileinKinetics-
MidFusion YesV-only 0.339 0.954 2.441
Soundsdatasetwherevideoeventclassificationisthedominanttask,
MidFusion YesA-only 0.310 0.940 2.276
audioattacksdidnotaffecttheclassifieratall,videomodalitydom-
MidFusionmixup No 0.424 0.972 2.711
inantlydecidestherobustness.
MidFusionmixup YesA+V 0.234 0.891 1.983
MidFusionAT No 0.397 0.964 2.530
MidFusionAT YesA+V 0.199 0.900 1.861
Table1: PerformanceofourbestperformingCSNmodelsonAu-
dioSet, and their performance against the adversarial perturbation,
usingtheoverallarchitectureshowninFigure2. Here,mAPisthe
meanaverageprecision,AUCistheareaunderthefalsepositiverate
andtruepositiverate(recall). Thed-primecanbecalculatedfrom
AUC[1].ATdenotesadversarialtraining.Aredtextcolorindicates
themostpotentperturbationagainstthemodel.
Models Attack Acc mAP AUC
MidFusion No 86.5% 0.853 0.987
MidFusion YesA+V 5.0% 0.513 0.824
MidFusion YesV-only 6.6% 0.541 0.848
MidFusion YesA-only 85.6% 0.851 0.987
MidFusionmixup No 85.5% 0.854 0.987
MidFusionmixup YesA+V 15.2% 0.734 0.623 Fig. 4: Performance Drop Rate% VS Convexity (κ ) for all audio
c
classesinAudioSet.DensityρR,ppartiallyshownduetospacelimit.
c
Table2:PerformanceofCSNmodelsonKinetics-Sounds.
InFig.4,PerformanceDropRate:= cleanperformance−performanceunderattack.
cleanperformance
ThedensityρR,p (seeSS5.1)ismeasuredat60and80quantileof
7.2. AdversarialPerturbation&AdversarialTraining c
theclassin(cid:96) norm.Wecanobserveanegativecorrelationbetween
2
Alloftheadversarialperturbationsinthisworkarecomputedwith droprateandtheconvexityoftheclass,suggestingapositivecorre-
the PGD method [9], with (cid:15) = 0.1, (cid:96) , trained over 20 iterations. lationbetweenrobustnessandconvexity. Wecouldalsoseehigher
2
Attackbudget((cid:15)and#ofiteration)areconstrainedtobethesame densitywouldcorrelatetomorerobustnessforanaudioclass,some
acrossunimodal, multimodalexperiments. AdversarialTrainingis denserclassestendtobemorerobustdespitehavinglowconvexity.
performedbythemethodsmentionedin[22]. Intriguingly, we see the pattern that simpler waveforms such as
siren (highlighted), bell sounds, child crying are more robust than
complexsignalsliketraffic/mechanicssounds. Ourempiricalfind-
7.3. ResultsandDiscussion
ingsverifyourconjecturethatboththedensityandtheconvexityof
thedataaresimplefactorsthatcouldseverelyaffectitsrobustness.
AswecanseefromTables1and2,wefindempiricalevidencesup-
Fig. 3 shows the convex hull of bottleneck feature of mixup fu-
portingTheorem1, asuni-modalattacks(Video-only, Audio-only)
sionmodelandvanillamidfusionmodel,whereweobservethatthe
cansuccessfullybreakthemultimodalnetwork.Multimodalattacks
mixupmodelclearlypushessamplesofaclasstoformadenserouter
arehowevermorepotentthanunimodalattacksgiventhesameat-
crust and encourages a clearer boundary between classes resulting
tackbudget,withthesame(cid:15)and#iterationscausingmoreaccuracy
inahigherconvexitywithinselectedclasses.
dropcomparedtoaunimodalattack.
8. CONCLUSION
In contrast to some common notion, our work shows that multi-
modal systems are not always more robust to adversarial attacks.
Inthiswork,weproposealternativemetricstounderstandtheadver-
sarialrobustnessoflarge-scalemulti-modalmodels,andweempiri-
callyshowtheeffectivenessofourdensityandconvexitymetric.We
furtherproposeanovelmulti-modalmix-upmethodthatselectively
augmentsthedensersamplesinlessconvexclassestocompensate
fortherobustness,andwhichoutperformssimpleadversarialtrain-
Fig.3: Convexhullofsamples. Left: TSNEofthevanillafusion ing with respect to robustness. Our experiments on AudioSet [1]
feature. Right: TSNE of the mixup fusion feature. Three classes: andKinetic-Soundsdataset[2]verifyourhypothesisandtheeffec-
Red→Siren;Blue→CivilSiren(subclassofSiren);Green→Pig. tivenessofthemix-upstrategy.
9. ACKNOWLEDGMENTS [13] Tadas Baltrusˇaitis, Chaitanya Ahuja, and Louis-Philippe
Morency, “Multimodalmachinelearning: Asurveyandtax-
ThisworkusedtheExtremeScienceandEngineeringDiscoveryEn- onomy,” IEEEtransactionsonpatternanalysisandmachine
vironment(XSEDE),whichissupportedbyNationalScienceFoun- intelligence,vol.41,no.2,pp.423–443,2018.
dationgrantnumberACI-1548562.Specifically,itusedtheBridges-
[14] WeiyaoWang,DuTran,andMattFeiszli, “Whatmakestrain-
2system,whichissupportedbyNSFawardnumberACI-1928147,
ingmulti-modalclassificationnetworkshard?,”inProceedings
atthePittsburghSupercomputingCenter(PSC).
oftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,2020,pp.12695–12705.
10. REFERENCES [15] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah,
“Ucf101: Adatasetof101humanactionsclassesfromvideos
[1] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren inthewild,” arXivpreprintarXiv:1212.0402,2012.
Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,
[16] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
andMarvinRitter,“Audioset:Anontologyandhuman-labeled
LeCun, and Manohar Paluri, “A closer look at spatiotempo-
datasetforaudioevents,” in2017IEEEInternationalConfer-
ralconvolutionsforactionrecognition,” inProceedingsofthe
ence on Acoustics, Speech and Signal Processing (ICASSP).
IEEEconferenceonComputerVisionandPatternRecognition,
IEEE,2017,pp.776–780.
2018,pp.6450–6459.
[2] ReljaArandjelovicandAndrewZisserman, “Look,listenand
[17] JunchengLi,ShuhuiQu,XinjianLi,Po-YaoHuang,andFlo-
learn,” inProceedingsoftheIEEEInternationalConference
rian Metze, “Appendix: On adversarial robustness of large-
onComputerVision,2017,pp.609–617. scale audio visual learning,” https://lijuncheng16.
[3] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar github.io/ICASSP2022_proof.pdf,2021.
Fawzi, andPascalFrossard, “Universaladversarialperturba- [18] MichaelJorgensen,“Volumesofn-dimensionalspheresandel-
tions,” in Proceedings of the IEEE conference on computer lipsoids,” “https://www.whitman.edu/documents/Academics/
visionandpatternrecognition,2017,pp.1765–1773. Mathematics/2014/jorgenmd.pdf”.
[4] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland [19] SaehyungLee,HyungyuLee,andSungrohYoon,“Adversarial
Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, vertex mixup: Toward better adversarially robust generaliza-
Aleksander Madry, and Alexey Kurakin, “On evaluating ad- tion,” inProceedingsoftheIEEE/CVFConferenceonCom-
versarialrobustness,” arXivpreprintarXiv:1902.06705,2019. puterVisionandPatternRecognition(CVPR),June2020.
[5] DimitrisTsipras,ShibaniSanturkar,LoganEngstrom,Alexan- [20] ShawnHershey,SourishChaudhuri,DanielP.W.Ellis,JortF.
der Turner, and Aleksander Madry, “Robustness may be at Gemmeke, Aren Jansen, Channing Moore, Manoj Plakal,
oddswithaccuracy,” arXivpreprintarXiv:1805.12152,2018. DevinPlatt,RifA.Saurous,BryanSeybold,MalcolmSlaney,
Ron Weiss, and Kevin Wilson, “Cnn architectures for large-
[6] EricWongandZicoKolter, “Provabledefensesagainstadver-
scale audio classification,” in International Conference on
sarialexamplesviatheconvexouteradversarialpolytope,” in
Acoustics,SpeechandSignalProcessing(ICASSP).2017.
InternationalConferenceonMachineLearning.PMLR,2018,
pp.5286–5295. [21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
ChloeHillier,SudheendraVijayanarasimhan,FabioViola,Tim
[7] MattJordan,JustinLewis,andAlexandrosGDimakis, “Prov- Green,TrevorBack,PaulNatsev,etal., “Thekineticshuman
ablecertificatesforadversarialexamples: Fittingaballinthe actionvideodataset,” arXivpreprintarXiv:1705.06950,2017.
unionofpolytopes,” inAdvancesinNeuralInformationPro-
[22] Eric Wong, Leslie Rice, and J Zico Kolter, “Fast is bet-
cessingSystems,2019,pp.14082–14092.
terthanfree: Revisitingadversarialtraining,” arXivpreprint
[8] JeremyMCohen,ElanRosenfeld,andJZicoKolter, “Certi- arXiv:2001.03994,2020.
fiedadversarialrobustnessviarandomizedsmoothing,” arXiv
[23] QiuqiangKong,YinCao,TurabIqbal,YuxuanWang,Wenwu
preprintarXiv:1902.02918,2019.
Wang,andMarkDPlumbley, “Panns: Large-scalepretrained
[9] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, audio neural networks for audio pattern recognition,” arXiv
Dimitris Tsipras, and Adrian Vladu, “Towards deep learn- preprintarXiv:1912.10211,2019.
ing models resistant to adversarial attacks,” arXiv preprint
arXiv:1706.06083,2017.
[10] Min Wu and Marta Kwiatkowska, “Robustness guarantees
for deep neural networks on videos,” in Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecog-
nition(CVPR),June2020.
[11] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,
and Jingjing Liu, “Large-scale adversarial training for
vision-and-language representation learning,” arXiv preprint
arXiv:2006.06195,2020.
[12] SanchariSen,BalaramanRavindran,andAnandRaghunathan,
“Empir: Ensemblesofmixedprecisiondeepnetworksforin-
creasedrobustnessagainstadversarialattacks,” ICLR,2020.
A. PROOFOFTHEOREM1
Let’sconsiderabinaryclassificationtaskasanexampleforsimplic-
ity. Let(x ,x )beapointwithdifferentpredictionresultsfor
A,i V,i
audiomodalityAandvideomodalityV. Assume∃a,b, suchthat
aTg(x )=−s<0andbTh(x )=t>0wheres,t>0,and
A,i V,i
thecorrectlabelis−1.Forthepoint-wiserobustnessthreshold(cid:15)
A,i
ofthispoint, whereanattack{δ : ||δ || ≤ (cid:15) }changes
(cid:15)A,i (cid:15)A,i P A,i
thepredictionlabel.Bydefinition,weknowaTg(x +δ )≥0
A,i (cid:15)A,i
andaTg(x +δ) ≤ 0forall0 ≤ δ ≤ (cid:15) . Ifs < t, thenthe
A,i A,i
fusednetworkpredictedthewronglabelevenwithoutanynoise.
f(x ,x )=(a,b)T(g(x )⊕h(x )) (10)
A,i V,i A,i V,i
=aTg(x )+bTh(x ) (11)
A,i V,i
=−s+t>0 (12)
Otherwise,inthecaseofs≥t,byapplyingIntermediateValueThe-
oremtog(x),thereexistsapoint0≤δ≤(cid:15) suchthataTg(x +
A A,i
δ)=−t/2:
f(x +δ,x )=(a,b)T(g(x +δ)⊕h(x ))
A,i V,i A,i V,i
=aTg(x A,i+δ)+bTh(x V,i) (13)
t
=− +t>0
2
Inbothcases, wecouldfindanoise0 ≤ δ < (cid:15) withinthe
A,i
originalunimodalrobustnessthresholdtoattackthemultimodalnet-
work successfully. Vise versa for video. Thus, a unimodal attack
can break a mulimodal model, which we also empirically verified
theexistenceofsuchcasesinourexperiments. (seeTable2ofthe
mainpaper).
WepostulatethatsuchphenomenonisliketheMcgurkEffect,
where multimodal fusion would further distort the already non-
convexdecisionboundary(Figure1ofthemainpaper),makingthe
fused decision boundary very different than the original ones and
unpredictable.

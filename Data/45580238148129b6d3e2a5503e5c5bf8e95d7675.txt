IMPROVING CHORAL MUSIC SEPARATION THROUGH EXPRESSIVE
SYNTHESIZED DATA FROM SAMPLED INSTRUMENTS
KeChen1 Hao-WenDong1 YiLuo2 JulianMcAuley1
TaylorBerg-Kirkpatrick1 MillerPuckette1 ShlomoDubnov1
1UCSanDiego,USA 2TencentAILab,China
1{knutchen, hwdong, jmcauley, tberg, msp, sdubnov}@ucsd.edu 2oulyluo@tencent.com
ABSTRACT speech separation, vocal-accompaniment separation and
musical instrument separation. The latter two tasks are
Choral music separation refers to the task of extracting primary tasks in the field of music signal processing and
tracks of voice parts (e.g., soprano, alto, tenor, and bass) have been adopted for practical use in the entertainment
from mixed audio. The lack of datasets has impeded re- industry[1]. Manymodels,suchasOpen-Unmix[2],De-
search on this topic as previous work has only been able mucs[3],andSpleeter[4],achievegreatseparationperfor-
to train and evaluate models on a few minutes of choral mance. Somemodels[5]furtherextendthesourcesepara-
music data due to copyright issues and dataset collection tion task to a zero-shot or query-based setting. However,
difficulties. In this paper, we investigate the use of syn- choralmusicseparationhasreceivedlimitedattention.Un-
thesizedtrainingdataforthesourceseparationtaskonreal likegeneralmusicalinstrumentseparation,whichseeksto
choralmusic. Wemakethreecontributions: first,wepro- separatenon-homologoussources(e.g.,piano,drums,and
vide an automated pipeline for synthesizing choral music singingvoice),choralmusicinstrumentseparationseeksto
data from sampled instrument plugins within controllable separate homologous or close-homologous sources (e.g.,
options for instrument expressiveness. This produces an soprano, alto, tenor, bass). Additionally, the scarcity of
8.2-hour-longchoralmusicdatasetfromtheJSBChorales dataonchoralmusicseparationimpedesfurtherprogress.
Datasetandonecaneasilysynthesizeadditionaldata.Sec- Choralmusicseparationcouldbeusedinawidevarietyof
ond,weconductanexperimenttoevaluatemultiplesepara- scenarios. Individualscouldobtainsolotracksfromchoral
tion models on available choral music separation datasets recordings for practice, analysis, and re-production. Not
from previous work. To the best of our knowledge, this only does it fill a void in a particular type of musical in-
isthefirstexperimenttocomprehensivelyevaluatechoral strument separation, but it also provides convenience for
musicseparation. Third,experimentsdemonstratethatthe musiceducators.
synthesized choral data is of sufficient quality to improve In this paper, we investigate the choral music separa-
the model’s performance on real choral music datasets. tion task from the perspective of addressing the insuffi-
This provides additional experimental statistics and data ciency of available datasets. We begin by introducing re-
supportforthechoralmusicseparationstudy. lated works in the field of choral music separation. Sec-
ond, wepresentthediscovery ofhowtoimprovetheper-
formance of choral music separation using high-quality,
1. INTRODUCTION
synthesizedmusicdata. Then,weconductcomprehensive
Choral music is a distinct artistic genre that includes sev- experimentswithmultiplemodelsanddatasetstoevaluate
eral vocal parts (e.g. soprano, alto, tenor, and bass) ar- theimprovementofusingsynthesizeddataonchoralmusic
ranged into intricate patterns from strict counterpoint to separation in real datasets. Finally, we discuss the exten-
polyphonicechoesandflowsoflyrics. Oneusefultoolin sibilityofourpipelinetomorechoral-relatedseparations,
theanalysisandre-productionofchoraltracksistheabil- suchasstringquartetseparation,aswellasitsfuturedirec-
ity to take mixed-down choral music and separate it back tions. Thecodeandthedatasetarepubliclyavailable1.
intoaudiotracksofisolatedvocalparts: i.e. choralmusic
separation,asasubtaskofaudiosourceseparation.
Audio source separation is an audio signal processing 2. RELATEDWORK
task that involves separating one or more sound sources
Research in choral music separation receives relatively
from a multi-source audio mixture. This task has a wide
less attention. Deep learning methods for audio source
range of applications in a variety of domains, including
separation has already outperformed traditional methods
(e.g. Non-negative Matrix Factorization [6]) for a long
time. Separation models have been developed follow-
©K.Chenetal.. LicensedunderaCreativeCommonsAt-
tribution4.0InternationalLicense(CCBY4.0). Attribution: K.Chen ing two directions: frequency-domain models and time-
etal.,“ImprovingChoralMusicSeparationthroughExpressiveSynthe- domainseparationmodels.
sizedDatafromSampledInstruments”,inProc. ofthe23rdInt. Society
forMusicInformationRetrievalConf.,Bengaluru,India,2022. 1https://github.com/RetroCirce/Choral_Music_Separation
2202
peS
7
]DS.sc[
1v17820.9022:viXra
Dataset Minutes Songs Public Pitchrange
ChoralSingingDataset[11] 7 3 (cid:88) Name Type Soprano Alto Tenor Bass
DagstuhlChoirSet[12] 5 2 (cid:88) StandardMIDI A0–C8 A0–C8 A0–C8 A0–C8
CantoriaDataset[13] 20 14 (cid:88) Noire[17] Piano A0–C8 A0–C8 A0–C8 A0–C8
ESMUCChoirDataset[13] 31 26 (cid:88) Grandeur[18] Piano A0–C8 A0–C8 A0–C8 A0–C8
BachandBarbershopCollection[14] 105 48 VoicesOfRapture[19] Vocal B3–D6 E3–G5 B2–C#5 A1–D4
DominusChoir[20] Vocal G3–A5 G3–A5 E2–G4 E2–G4
Table1: Existingdatasetsforchoralmusicseparation.
Table2:Sampleinstrumentlibrariesweuseforsynthesiz-
ingchoralmusicseparationdatasets.
2.1 Frequency-DomainandTime-DomainModels
The traditional method of audio separation is to mask the Inthispaper,wefirstconductfourfundamentalmodels:
frequency-domainrepresentationandtheninverselytrans- Spec-U-Net, Res-U-Net, Wave-U-Net and Conv-TasNet.
form it to the time-domain signal, referred as frequency- Our objective is to demonstrate the efficacy of synthe-
domain separation models. Spec-U-Net [7], based on sizedexpressivedatainimprovingseparationperformance
theU-Netarchitecture, containsconvolutionalneuralnet- on real choral music datasets. As a result, fundamental
work(CNN)blocksfordownsamplingtheinputshort-time modelsenableustoconsidertheperformancegainsmore
Fouriertransform(STFT)spectrogramandupsamplingthe directly associated with data changes and augmentations.
bottom feature back into a separation mask. The mask is Also, score-informed and conditional separation models
applied into the input to obtain the separate spectrogram introduce external information, such as musical notes of
asoutput.Res-U-Net[8]replacestheoriginalCNNblocks original songs or multi-pitch estimation results, to guide
withresidualCNNblockstoaccelerateconvergencespeed. theseparation’sgoal,whileitalsolimitsitsapplications.In
Ontheotherhand,time-domainmodelsperformthesepa- practice, we frequently find ourselves in situations where
rationdirectlyontheaudiowaveform. Wave-U-Net[9]in- the only available input is the audio. We continue to de-
corporatesanend-to-endU-Netstructureontheinputand mand unconditioned choral music separation. As a re-
output of waveforms. Conv-TasNet [10] applies a CNN sult, we proceed directly to unconditioned choral music
encoder-decoder structure to process waveforms into la- separationinthispaper,withoutrelyingonanyscorecon-
tent features and generates the mask. The masked latent ditions.
features are decoded back to waveforms as separation re-
sults. Bypassingthespectrogramprocessing,time-domain
3. METHODOLOGY
modelscansaveparametersandperformefficientlyinlow-
latencysystemsforspeechseparation. Somehybridmod-
3.1 ScarcityofDatasets
els,suchasDemucsv3[3],canleveragebothtime-domain
andfrequency-domainfeaturestoachievethebestperfor- Existing datasets for choral music are listed in Table 1,
manceformusicalinstrumentseparation,whilethesizeof collected from previous works and other public sources.
themodelisalittlebitlarge. We observe that most of these datasets have short total
lengths;threeofthemarelessthan20minutes.TheChoral
SingingDataset[11]andESMUCChoirDataset[13]have
2.2 ChoralMusicSeparation
been used for choral music separation by [16], while the
For choral music separation, [15] proposed a score- Dagstuhl ChoirSet [12] and Cantoria [13] Datasets were
informedseparationmodelbasedonWave-U-Netandper- never used for separation tasks but instead for singing
formed experiments on 347 (synthesized) Bach Chorale performance analysis. The Bach and Barbershop Collec-
pieces from MIDI files with MuseScore_General Sound- tion [14] is relatively longer, but is not publicly avail-
Font. This model performs well on this SoundFont- able. As a result, when a model is trained and tested on
Synthesisdatasetbutpoorlyonrealchoralmusicdatasets. suchasmallamountofdata,itsgeneralizationandsepara-
[16] proposed a conditional Spec-U-Net to optimize the tioncapabilitiesareseverelylimited.[15]directlysynthe-
separationperformancebyconditioningonthefundamen- sized 347 choral pieces of Bach’s from MIDI files with
talfrequencycontour. However,asmentionedintheirpa- MuseScore_General SoundFont and trained the model.
per,duetothelackofchoralmusicdatasets,theevaluation However, this SoundFont-Synthesis dataset is dissimilar
wasconductedononlythreesongswithatotaldurationof to true choral vocals. Moreover, it lacks lyrics and sylla-
sevenminutes.[14]proposedaharmonicoverlapscoreto bles.Asaresult,thetrainedmodelperformspoorlyonreal
increasethemodel’ssensitivitytodifferentchoralvoices, datasets[15].
therebyimprovingperformance.Itmadeuseofarelatively In the next section, we first introduce the pipeline of
largedatasetcontaining105minutesofBachandBarber- synthesizing audio datasets for choral music separation
shopCollections, butthisdatasetisnot publiclyavailable from sampled instrument libraries. Then, we train vari-
due to copyright concerns, which prevents it from being ous models on our datasets and compare them to deter-
opensource. Andindeed,100-minuteisstillnotenoughto mine the best model. Finally, we transfer the best model
helpachieveanaudioseparationmodelwithahighgener- weights to the real-world datasets shown in Table 1, fine-
alizationability,weexpecttoobtainasizemorethanthat. tune the model and determine whether it truly improves
Synthesis Pipeline Training Pipeline
Synthesis
Python Scripts Symbolic Choral Dataset
The Symbolic Dataset - JSB Chorale
The Synthesis Configuration Reaper DAW
Trained
Synthesized Choral Dataset
Octave Shifting: [1,0,1,0]
Tonality Shifting: TRUE Sampled Instruments Libraries:
Synthesis
L De yg na at mo i: c T VR eU lE o city: [85, 128] piano, vocal, etc. Se Mp oa dra et li son Fine-tuned Real Choral Training Set
Note Range: [B3,D6],[E5,G5],[B2,C#5],[A1,D4]
Dynamic Tempo: FALSE The Synthesis Configuration: Synthesized
Word Control: [10,13,17,24,28,32] dynamics, legato, word/syllable, etc. Choral Dataset Evaluated
Real Choral Test Set
Figure1: Thesynthesispipelineofchoralmusicdatafromsampleinstrumentsandthetrainingpipelinetoutilizeit.
performancewhencomparedtotheprevioussettings. voices, but it is more difficult to model the acoustic fea-
tures of these four voices than piano. This allows us to
3.2 DataSynthesisPipeline determinewhetherthemodelcanimproveperformanceby
exploitingthetimbredifferencebetweenvocaldatasets,or
Figure 1 shows our pipeline of choral music dataset cre-
ifitfailstomodelthesetimbresandperformabadsepara-
ationandtrainingmethods. Generally,weneedthreecol-
tionresult.
lectionsteps:
Due to the fact that we have two sampled libraries for
each type of instrument, each dataset contains 248 × 2 =
1. Thesymbolicchoralmusicdataset(MIDI,MusicXML)
496minutes(8.2hours)ofsynthesizedchoralmusicdata.
2. Thesampledinstrumentlibraries(standaloneVSTplu- All sampled instrument libraries that we use have a paid
gin,orKontaktsamplelibraries) license.
3. Thesynthesisconfiguration(syllablesorlyricschoices,
3.4 SynthesisConfigurationforExpressiveness
legato,velocity,andtempo)
For Step 3, we adopt two methods to further improve the
Then,ourprovidedcodecancompletelyautomatethedata scalability and quality of synthesized data: the basic data
synthesis process. It is built on top of the python-support augmentation, and expressiveness incorporation. The left
andfreedigitalaudioworkstation(DAW)–Reaper2.With ofFigure1showsaspecification.
theabovethreesteps,onecouldcompleteanychoralmusic Weperformtwooperationstoaugmentthedata. First,
datasynthesisprocessonsupportedsystemplatforms. we notice that the pitch ranges of sampled instrument
libraries do not always correspond exactly to the pitch
3.3 DataandInstrumentCollection ranges of tracks in JSB Chorales Dataset. For instance,
some bass melodies in JSB appear to be lower than the
For Step 1, following [15], we use the JSB Chorales
lowestnoteinsampledinstrumentlibraries. Insteadofdi-
Dataset [21], which contains 347 pieces of choral music
rectlydiscardingthesetracks,weimplement“octaveshift-
inMusicXMLformat. Thetotaldurationis248minutesat
ing” by shifting out-of-range notes up or down some oc-
a tempo of 90 bpm (a.k.a. beat per minute). The data is
taves until they fall within the range. While it produces
firsttransformedintoMIDIfiles,whichservesasthesym-
non-realistic jumps between some melodies, it saves the
bolicdatasetsourceforthecreationofchoralmusicaudio.
whole track to preserve more realistic data. Second, we
ForStep2,asampledinstrument3 isasoundsourceplugin
apply “tonality shifting” to each track. The tonality was
appliedinaDAW.UnlikeaSoundFont,itcontainssamples
shifted upward and downward by three semitones before
ofrealinstrumentsrecordedinaprofessionalacousticen-
synthesis. Therefore, the effective length of training data
vironment. Thehumansingingvoiceisalsoconsideredas
willbefurtheraugmentedseveraltimes.
aninstrumenttype. Andmanyvocalsampledinstruments
For expressiveness incorporation, we provide several
support a variety of lyrical or syllabic sets (e.g., vowels,
options, which are supported by sampled instrument li-
Latin words, etc.). We first choose two types of instru-
braries,tosynthesizeaudio:
ments for our purposes: piano and vocal (soprano, alto,
tenor, and bass). Then, we choose two sampled instru-
• Legato:forvocal,thisincludeswhetherornottochange
ments for each type, as shown in Table 2. The reason to
breath or sing continuously. In vocal instrument li-
choose the piano instrument is to evaluate the separation
braries,legatoiscontrolledbydetectingthepresenceof
performance of piano as a common and same-source in-
anoverlapbetweenadjacentnotes. Tosupportthelegato
strument. In this case, the model needs to consider most
configuration,webeginbysegmentingthetrackintomu-
on the pitch difference between each voice part. When
sicalphrasesusingthebreathbreakinformationinMu-
it comes to the vocal dataset, the model can distinguish
sicXML(ifprovided)orthenoteintervals(ifspecified).
thetimbresslightlybetweensoprano,alto,tenor,andbass
Then, in each phrase, we add overlap to adjacent notes
2https://www.reaper.fm/ (toactivatethelegato)iftheirpitchdistanceislessthan
3Adetailedintroductioncanbefoundathttps://tinyurl.com/2p8trn2u 7semitones(i.e.,aperfectfifth).
• Velocity: for each phrase, we provide three types of Standard MedianSource-DistortionRatio(dB)
Model
volume/velocitychangecurves: crescendo,diminuendo, Dataset Soprano Alto Tenor Bass Avg.
and cresc.→dim.. The configuration establishes the Piano Spec-U-Net[7] 9.78 9.46 10.35 10.60 10.05
maximumandminimumvelocityranges. Piano Res-U-Net[8] 8.53 9.01 9.97 12.23 9.94
Piano Wave-U-Net[9] 6.95 5.36 7.21 9.82 7.34
Piano Conv-TasNet[10] 7.04 6.98 7.29 7.82 7.28
• WordControl: forvocal,wesupportthewordcontrolof
sampledinstrumentlibrariesbyassigningrandomcom- Vocal Spec-U-Net[7] 10.45 10.19 12.25 9.53 10.61
Vocal Res-U-Net[8] 9.35 10.87 10.20 10.77 10.30
binationsofwordsorsyllablestoeachphrase. Notethat
Vocal Wave-U-Net[9] 2.65 3.08 3.06 3.90 3.17
real-world performance may not contain random word Vocal Conv-TasNet[10] 6.60 6.12 6.41 6.58 6.43
changes, but for model training, this still increases the
datarichnessoneachtrainingbatch. Table3:Theseparationperformanceoffourmodelsonthe
testsetsofStandard-PianoandStandard-Vocaldatasets.
Theconfigurationalsosupportsthereverberationasade-
applytheAdamoptimizer[22]withβ =0.9,β =0.999,
signedfeature,butcurrentlyitisnotappliedinthiswork. 1 2
(cid:15)=1e−8,andalearningrateschedulerwherethelearn-
ing rate is reduced with a multiplier f = 0.65 if the val-
4. EXPERIMENTS
idation performance does not improve across 3 consecu-
In this section, we conduct an experiment on evaluating tiveepochs.WeimplementedallmethodsinPytorchusing
different separation models on our synthesized datasets. NVIDIARTX2080TiGPUs.Allmodelsconvergedwithin
Thepurposeofthisexperimentistoidentifythebestmodel 300epochswithearlystopusinga10-epochpatience.
on synthesized datasets and then transfer it to real choral For evaluation, source-to-distortion ratio (SDR) is one
musicdatasets. of the most widely used metrics for evaluating a source
separation system’s output, which measures a ratio be-
4.1 Datasets,ModelsandHyperparameters tween the original source track and the noise, interfer-
ence, added artifacts in the separation track. It is consid-
As introduced above, we use two datasets (piano and vo- ered to be an overall measure of how good a separation
cal) to train four models (Spec-U-Net, Res-U-Net, Conv- result sounds. We follow the music separation campaign
TasNet,andWave-U-Net).Eachtrackisin22,050Hzsam- SiSEC2018[23]tousethemedianSDRtoevaluatesepa-
plerate. Eachdatasetcontains496minutesdata. Weuse ration performance. The median SDR is obtained by first
277tracksfortraining,35tracksforvalidationandanother computing segment-level SDR of each 2-sec segment in
35tracksfortesting. Sincewehavetrainingcombinations eachtrack,thentakingthemedianoverthemastrack-level
among four models, two datasets, and four choral voices, SDRs,finallytakingthemedianoverthetrack-levelSDRs
to save training time, we first train models on the dataset asthefinalSDR.Thecomputinglibraryismus_eval[23].
without the expressiveness incorporation in section 3.4,
named as Standard-Piano and Standard-Vocal datasets. 4.2 SeparationPerformance
Afterfindingthebestmodel,wewilltrainitontheexpres-
Table3showsallfourpartsmedianSDRperformanceon
sivedatasetsinsection4.3.
two standard datasets by four models. We can see that
Formodelhyper-parameters,InSpec-U-Net[7],weuse
thefrequency-domainmodelsSpec-U-NetandRes-U-Net
a window size of 2048, FFT size of 2048, and hop size
get similar results that are better than those of the time-
of 441. We apply 7 CNN blocks to downsample the in-
domain models Conv-TasNet and Wave-U-Net. Spec-U-
put spectrogram, and another 7 CNN blocks to upsample
NetachievesthebestaverageSDRsoverfourpartsontwo
it into the separation mask. In Res-U-Net, we apply the
datasetsas10.05and10.61. TheRes-U-Netachievesvery
implementationfrom[8],with10residualCNNblocksto
closeperformance.Whenanalyzingtheresults,frequency-
downsample the input spectrogram, then another 6 resid-
domain models can take advantage of spectrograms in
ual CNN blocks to upsample. In Wave-U-Net, we follow
choral music to obtain better separation results. The per-
the settings of [9] to adopt 6 downsampling CNN layers
formanceonsoprano,altoandtenorinvocalisbetterthan
and 6 upsampling CNN layers for separation. The filter
that in the piano dataset, suggesting that the timbre dif-
channels are set from 32 to 1024 in order for each layer.
ferencecanalsohelpfurtherdiscriminatedifferentsource
The kernel size is 15 for the first layer and 5 for remain-
tracks. Time-domain models can model the piano acous-
ing layers. In Conv-TasNet, we follow the setting of [10]
tic features well to achieve a good performance, but find
to set hyper-parameters as N = 512, L = 20, B = 128,
ithardtomodelthevocalfeaturessolelyonthewaveform
H = 512, P = 3, R = 3, X = 8. Spec-U-Net and
andfacethedropsinthevocaldataset.
Res-U-Net use their default mean absolute error (MAE)
lossfunction;Conv-TasNetusesthedefaultscale-invariant
4.3 Fine-TuningEvaluationonRealDatasets
source-to-noiseratio(SI-SDR)loss;andWave-U-Netwith
meansquarederror(MSE)loss. Aftercomparingmodelsinstandarddatasets,wechosethe
For training hyperparameters, the batch size is 8, the bestmodel,Spec-U-Net,toconductthenextexperiments.
learningrateis1e-3,andeachtrainingsampleisa2-secau- We trained Spec-U-Net on the Expressive-Vocal dataset,
diosegmentrandomlychosenfromonemusictrackinthe as we synthesized the data with the expressiveness incor-
trainingset.Thenumberofstepsforeachepochis700.We poration.Then,asshownintherightofFigure1,wesaved
Avg.MedianSDR(Fine-TuningRatio)
Fine-TuningEvaluationSet PretrainingSet
ratio=10% ratio=40% ratio=70%
None 1.42 3.91 4.13
SoundFont-Synthesis[15] 2.39 3.90 4.03
CantoriaDataset[13]
Standard-Vocal(ours) 3.03 4.59 5.08
Expressive-Vocal(ours) 3.73 5.48 5.71
None 1.98 2.78 5.26
SoundFont-Synthesis[15] 2.12 3.38 6.20
ChoralSingingDataset(CSD)[11]
Standard-Vocal(ours) 3.43 4.23 6.91
Expressive-Vocal(ours) 4.19 4.78 7.50
None 4.18 6.08 6.94
SoundFont-Synthesis[15] 4.19 6.17 6.93
Bach&BarbershopCollection(BBC)[14]
Standard-Vocal(ours) 4.98 6.71 7.27
Expressive-Vocal(ours) 5.58 7.17 7.64
Table4: Thefine-tuningperformanceofthreerealdatasetsbyourbestmodel–Spec-U-Net.
the best model checkpoints, and conducted a fine-tuning Vocal is better than that on SoundFont-Synthesis and
experiment to verify whether our data is useful for trans- none-dataset, where the performance of Expressive-Vocal
fer learning on real choral music datasets. Table 4 and achieves the best. When the training-test ratio is small as
Figure 2 illustrate the median SDR performance of three 10%, the performance of SoundFont-Synthesis and non-
real choral music datasets under different fine-tuning ra- datasethasthelargeestdifference,showingthatthemodel
tioswithdifferentpretrainedmodels. learns some priors from SoundFont-Synthesis and con-
For datasets, we chose the Cantoria Dataset, Choral verges to a better optimum. However, when the ratio in-
Singing Dataset (CSD), and Bach & Barbershop Collec- creasesto40%and70%,theirperformanceisclosetoeach
tion4 (BBC). The reason for these choices is that Canto- otheranddoesnotvarymuch,especiallyonCantoriaand
ria contains the best recording quality, CSD is most fre- BBC. Thus, pretraining on SoundFont-Synthesis dataset
quently used in previous works, and BBC contains the provides a very useful initialization – but gains diminish
longest length. The meta information of each dataset has (or even no gain) as the initializer is dominated by larger
beendescribedinTable1. andlargerquantitiesofrealtrainingdata.
Weconsideredthreeratiosforfine-tuning: (1)10%for
However, when the model is pretrained on Standard-
training, 90% for evaluation; (2) 40% for training, 60%
Vocal, it has a strong generalization to real choral mu-
for evaluation; and (3) 70% for training, 30% for evalua-
sic datasets under all fine-tuning ratios, because acous-
tion. Thefine-tuningexperimentsdemonstrateifoursyn-
tic features of synthesis tracks share large similarity to
thesized datasets can improve the separation performance
the real datasets. This performance is further boosted by
in real datasets under different settings (e.g., few-shot as
Expressive-Vocal as we introduce expressiveness during
10% and fairly enough as 70%). The intermediate ratio
synthesis,suchaslyricsandvelocitydynamics. Evenun-
40%isconductedtofurtherinvestigatethetendencyofthe
der the 70% fine-tuning ratio, as the model has received
improvementbroughtbyourdatasets.
manyrealdata,Standard-VocalandExpressive-Vocalpre-
There are four dataset choices on which to pretrain
trained model can still get improvements. In conclusion,
the models: (1) None: without any pretraining; (2)
our synthesized datasets provide not only additional data
SoundFont-Synthesis: the synthesis dataset in [15] by
volume,butalsohigh-qualityandclose-to-realchoralmu-
the Musescore_General SoundFont as a baseline; (3)
sicsamplesforboostingtheseparationperformance.
the Standard-Vocal dataset; and (4) the Expressive-Vocal
Tofurtherverifyouranalysis,wevisualizedthetrends
dataset. Since the SoundFont-Synthesis dataset only con-
ofmedianSDRs(blue,orange,cyan,andmagentacolors),
tains 248 minutes, instead of using two sampled libraries
with a 25th-75th percentile range, for each voice part of
(496minutes),weonlyprovidethedatasynthesizedfrom
three real datasets in Figure 2. We can see the perfor-
onelibrary–VoicesOfRapture[19]inStandard-Vocaland
manceofoursynthesizeddatasets(magenta&cyanlines)
Expressive-Vocalforthepretraining. Dataaugmentations
marksaclearperformanceincreaseandalargegaptothat
of “octave shifting" and “tonality shifting" are applied in
of SoundFont-Synthesis and non-dataset (orange & blue
allthreedatasets,except(4)incorporatesmoreexpressive-
lines). However, the trends of SoundFont-Synthesis and
ness settings. The fine-tuning learning rate is 1e-4, with
non-dataset are close to each other, and even overlap in
theschedulerinsection4.1.
BBC.Whenanalyzingthepercentilerangeofeachmodel,
Table 4 shows the average median SDR performance
onCantoriaandCSD,ourStandard-VocalandExpressive-
of Spec-U-Net over four voice parts under different fine-
Vocal pretrained models reveal a clear difference of per-
tuningratiosanddifferentpretrainingsettings. Wecansee
centile ranges to the left two models, demonstrating that
thatunderallthreetraining-testratios,theperformanceof
our models get a large improvement. However, the per-
the model pretrained on Standard-Vocal and Expressive-
centilerangesoftheSoundFont-Synthesisandnon-dataset
4Weappreciatethehelpfromauthorsin[14]toofferthedataset. pretrained models have a large overlap, demonstrating no
(a)TheMedianSDRperformanceontheCantoriaDataset.
(b)TheMedianSDRperformanceontheChoralSingingDataset(CSD).
(c)TheMedianSDRperformanceontheBach&BarbershopCollectionDataset(BBC).
Figure 2: The Median SDR performance, with a 25th–75th percentile range, of soprano, alto, tenor and bass on three
datasetsbySpec-UNetwithdifferentpretrainedmodelsanddifferentratiosoftraining-testsets.
difference even though their median SDRs differ a lit- There are also some limitations and future improve-
tle. On the BBC dataset, our models yield improvements ments to our work. First of all, our implementations of
on both the 25th and 75th percentile values but are not expressivenessarestillbasedonrandomtemplatemodes.
as pronounced as those observed on Cantoria and CSD. Deep learning methods can improve this expressiveness
The potential reason is because the BBC dataset contains modeling[24]. Second,thedesignofthechoralseparation
a relatively large data size (105 minutes), which makes model is needed to learn more priors from weak synthe-
the model already achieve a good convergence and hard sized data that can be transferred to real data, then it will
to get more significant improvements without model de- complement our proposed pipeline better. These limita-
signs. These trends further demonstrates that our synthe- tionsareplannedforexplorationinourfuturework.
sized dataset plays a role in making up the data scarcity
andimprovinggeneralizationability.
6. CONCLUSION
In this paper, we proposed an automated pipeline for
5. EXTENSIBILITYANDLIMITATIONS
synthesizing choral music data from sampled instrument
Ourprovidedsynthesispipelinefromsymbolicdatasetsto plugins, and created an 8.2-hour choral music dataset
real audio datasets not only benefits choral music separa- to improve separation performance on real choral music
tion tasks, but also other choral-related separation tasks. datasets. We comprehensively evaluated multiple separa-
Forexample,stringquartetseparation,toseparatetwovi- tionmodelstodemonstratethatsynthesizedchoraldatais
olins, viola, and cello parts from a mixed audio, can also of sufficient quality to improve model’s performance on
be trained with synthesized data of our pipeline. The de- realdatasets. Thisprovidesadditionalexperimentalstatis-
tailsofthestringquartetseparationexperimentcanbeac- ticsanddatasupportforchoralmusicseparationstudy. In
cessedinthecoderepository.Similarly,ourbestpretrained thefuture,wewillfocusonthedesignoftimbre-pitchdis-
model shows a 100%/30% performance increase to the entanglement model [25] for achieving better separation
SoundFont-Synthesis and non-dataset pretrained models. performance. The application of choral music separation
Thisfurthershowsapotentialapplicationofoursynthesis results into other music-related tasks, such as music rec-
pipelinetoimproveotherchoral-relatedseparationtasks. ommendation[26],isalsoplannedasthefuturework.
7. REFERENCES [14] S. Sarkar, E. Benetos, and M. B. Sandler, “Vocal har-
monyseparationusingtime-domainneuralnetworks,”
[1] E. Cano, D. FitzGerald, A. Liutkus, M. D. Plumbley,
inInterspeech2021.
and F. Stöter, “Musical source separation: An intro-
duction,”IEEESignalProcess.Mag.,2019. [15] M.GoverandP.Depalle,“Score-informedsourcesep-
aration of choral music,” in Proceedings of the 21th
[2] F. Stöter, S. Uhlich, A. Liutkus, and Y. Mitsufuji,
International Society for Music Information Retrieval
“Open-unmix-Areferenceimplementationformusic
Conference,ISMIR2020.
sourceseparation,”J.OpenSourceSoftw.,2019.
[16] D. Petermann, P. Chandna, H. Cuesta, J. Bonada, and
[3] A. Défossez, “Hybrid spectrogram and waveform
E.Gómez,“Deeplearningbasedsourceseparationap-
sourceseparation,”inWorkshoponMusicSourceSep-
plied to choir ensembles,” in Proceedings of the 21th
aration,ISMIR2021.
International Society for Music Information Retrieval
[4] R. Hennequin, A. Khlif, F. Voituret, and M. Moussal- Conference,ISMIR2020.
lam,“Spleeter: afastandefficientmusicsourcesepa-
[17] Native-Instruments, “Noire, evocative concert grand,”
ration tool with pre-trained models,” Journal of Open
https://www.native-instruments.com/en/products/
SourceSoftware,2020,deezerResearch.
komplete/keys/noire/,2021.
[5] K. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick,
[18] Native-Instruments, “The grandeur, vibrant mod-
and S. Dubnov, “Zero-shot audio source separation
ern grand,” https://www.native-instruments.com/en/
through query-based learning from weakly-labeled
products/komplete/keys/the-grandeur/,2020.
data,” in AAAI Conference on Artificial Intelligence,
AAAI2022.
[19] SoundIron, “Voice of rapture collection,” https://
soundiron.com/products/voices-of-rapture,2015.
[6] D. D. Lee and H. S. Seung, “Algorithms for non-
negative matrix factorization,” in Neural Information
[20] FluffyAudio, “Dominus choir,” https://www.
ProcessingSystems,NeurIPS2000.
fluffyaudio.com/shop/dominuschoir/,2019.
[7] A. Jansson and E. J. H. et al., “Singing voice separa-
[21] N. Boulanger-Lewandowski, Y. Bengio, and P. Vin-
tion with deep u-net convolutional networks,” in Pro-
cent, “Modeling temporal dependencies in high-
ceedings of the 18th International Society for Music
dimensional sequences: Application to polyphonic
InformationRetrievalConference,ISMIR2017.
musicgenerationandtranscription,”inProceedingsof
the29thInternationalConferenceonMachineLearn-
[8] Q.Kong,Y.Cao,H.Liu,K.Choi,andY.Wang,“De-
ing,ICML2012.
couplingmagnitudeandphaseestimationwithdeepre-
sunet for music source separation,” in Proceedings of
[22] D.P.KingmaandJ.Ba,“Adam:Amethodforstochas-
the 22nd International Society for Music Information
tic optimization,” in the 3rd International Conference
RetrievalConference,ISMIR2021.
onLearningRepresentations,ICLR2015.
[9] D. Stoller, S. Ewert, and S. Dixon, “Wave-u-net: A
[23] F.Stöter,A.Liutkus,andN.Ito,“The2018signalsep-
multi-scaleneuralnetworkforend-to-endaudiosource
arationevaluationcampaign,”inLatentVariableAnal-
separation,” in Proceedings of the 19th International
ysis and Signal Separation - 14th International Con-
Society for Music Information Retrieval Conference,
ference,LVA/ICA2018.
ISMIR2018.
[24] Y.WuandE.M.etal.,“MIDI-DDSP:detailedcontrol
[10] Y. Luo and N. Mesgarani, “Conv-tasnet: Surpassing
ofmusicalperformanceviahierarchicalmodeling,”in
ideal time-frequency magnitude masking for speech
the10thInternationalConferenceonLearningRepre-
separation,” IEEE ACM Trans. Audio Speech Lang.
sentations,ICLR2022.
Process.,TASLP2019.
[25] K. Chen, C. Wang, T. Berg-Kirkpatrick, and S. Dub-
[11] H. Cuesta, E. G. Gutiérrez, A. M. Domínguez, and
nov,“Musicsketchnet: Controllablemusicgeneration
F. Loáiciga, “Analysis of intonation in unison choir
viafactorizedrepresentationsofpitchandrhythm,”in
singing,” in Proceedings of the International Confer-
Proceedingsofthe21thInternationalSocietyforMu-
enceofMusicPerceptionandCognition,ICMPC2018.
sicInformationRetrievalConference,ISMIR2020.
[12] S. Rosenzweig, H. Cuesta, C. Weiß, F. Scherbaum,
[26] K.Chen,B.Liang,X.Ma,andM.Gu,“Learningaudio
E.Gómez,andM.Müller,“DagstuhlChoirSet:Amul-
embeddingswithuserlisteningdataforcontent-based
titrack dataset for MIR research on choral singing,”
musicrecommendation,”innInternationalConference
TransactionsoftheInternationalSocietyforMusicIn-
on Acoustics, Speech and Signal Processing, ICASSP
formationRetrieval,TISMIR2020.
2021.
[13] H. Cuesta, “Data-driven pitch content description of
choralsingingrecordings,”PhDThesisArchive,2022.

Flexible retrieval with NMSLIB and FlexNeuART
Leonid Boytsov ∗ Eric Nyberg
Pittsburgh, PA, USA Carnegie Mellon University
leo@boytsov.info Pittsburgh, PA, USA
ehn@cs.cmu.edu
Abstract collections would be impractical even with
recent efficiency improvements (Khattab
Our objective is to introduce to the NLP
and Zaharia, 2020).
community an existing k-NN search li-
The first retrieval stage is commonly re-
brary NMSLIB, a new retrieval toolkit
ferred to as the candidate generation (i.e.,
FlexNeuART, as well as their integra-
tion capabilities. NMSLIB, while be- wegeneratecandidatesforre-scoring). Until
ing one the fastest k-NN search li- about 2019, the candidate generation would
braries, is quite generic and supports exclusively rely on a traditional search en-
a variety of distance/similarity func- gine such as Lucene,1 which indexes occur-
tions. Because the library relies on
rences of individual terms, their lemmas or
thedistance-basedstructure-agnostical-
stems (Manning et al., 2010). In that, there
gorithms, it can be further extended
are several recent papers where promising
by adding new distances. FlexNeuART
is a modular, extendible and flexible results were achieved by generating dense
toolkit for candidate generation in IR embeddingsandusingak-NNsearchlibrary
and QA applications, which supports toretrievethem(Leeetal.,2019;Karpukhin
mixing of classic and neural ranking et al., 2020; Xiong et al., 2020). However,
signals. FlexNeuART can efficiently re-
these studies typically have at least one
trievemixed denseandsparserepresen-
of the following flaws: (1) they compare
tations(withweightslearnedfromtrain-
against a weak baseline such as untuned
ing data), which is achieved by extend-
BM25 or (2) they rely on exact k-NN search,
ing NMSLIB. In that, other retrieval sys-
tems work with purely sparse represen- thus, totally ignoring practical efficiency-
tations (e.g., Lucene), purely dense rep- effectiveness and scalability trade-offs re-
resentations (e.g., FAISS and Annoy), or lated to using k-NN search, see, e.g., §3.3
only perform mixing at the re-ranking
in Boytsov (2018). FlexNeuART implements
stage.
some of the most effective non-neural rank-
ingsignals: Itproducedbestnon-neuralruns
1 Introduction
in the TREC 2019 deep learning challenge
Although there has been substantial
(Craswell et al., 2020) and would be a good
progress on machine reading tasks using
tool to verify these results.
neural models such as BERT (Devlin et al.,
Furthermore, there is evidence that when
2018), these approaches have practical
dense representations perform well, even
limitations for open-domain challenges,
better results may be obtained by combin-
which typically require (1) a retrieval
ing them with traditional sparse-vector mod-
and (2) a re-scoring/re-ranking step to
els (Seo et al., 2019; Gysel et al., 2018;
restrict the number of candidate documents.
Karpukhin et al., 2020; Kuzi et al., 2020).
Otherwise, the application of state-of-the-art
It is not straightforward to incorporate
machine reading models to large document
∗WorkdoneprimarilywhileatCMU. 1https://lucene.apache.org/
these representations into existing toolkits, outperform the brute-force search in just a
but FlexNeuART supports dense and dense- dozen of dimensions (see, e.g., a discussion
sparse representations out of the box with in § 1 and § 2 of Boytsov 2018).
the help of NMSLIB (Boytsov and Naidan, For sufficiently small data sets and simple
2013a; Naidan et al., 2015a).2 NMSLIB is similarities, e.g., L 2, the brute-force search
an efficient library for k-NN search on CPU, can be a feasible solution, especially when
which supports a wide range of similarity the data set fits into a memory of an AI ac-
functions and data formats. NMSLIB is a celerator. In particular, the Facebook library
commonly used library3, which was recently for k-NN search FAISS (Johnson et al., 2017)
adopted by Amazon.4 Because NMSLIB algo- supports the brute-force search on GPU 7.
rithms are largely distance-agnostic, it is rel- However, GPU memory is quite limited com-
atively easy to extend the library by adding pared to the main RAM. For example, the
new distances. In what follows we describe latest A100 GPU has only 40 GB of memory8
NMSLIB,FlexNeuART,andtheirintegrationin while some commodity servers have 1+ TB
more detail. The code is publicly available: of main RAM.
In addition, GPUs are designed primar-
• https://github.com/oaqa/FlexNeuART
ily for dense-vector manipulations and have
• https://github.com/nmslib/nmslib poor support for sparse vectors (Hong et al.,
2018). When data is very sparse, as in the
2 NMSLIB
case of traditional text indices, it is possi-
Non-Metric Space Library (NMSLIB) is an ble to efficiently retrieve data using search
efficient cross-platform similarity search li- toolkits such as Lucene. Yet, for less sparse
braryandatoolkitforevaluationofsimilarity sets, more complex similarities, and large
searchmethods(BoytsovandNaidan,2013a; dense-vector data sets we have to resort to
Naidan et al., 2015a), which is the first com- approximate k-NN search, which does not
monly used library with a principled support have accuracy guarantees.
for non-metric space searching.5 NMSLIB is One particular efficient class of k-NN
an extendible library, which means that is search methods relies on the construction of
possible to add new search methods and dis- neighborhoodgraphsfordatasetpoints(see
tancefunctions. NMSLIBcanbeuseddirectly a recent survey by Shimomura et al. (2020)
in C++ and Python (via Python bindings). In for a thorough description). Despite ini-
addition, it is also possible to build a query tial promising results were published nearly
server,whichcanbeusedfromJava(orother 30 years ago (Arya and Mount, 1993), this
languages supported by Apache Thrift6). approach has only recently become popu-
k-NN search is a conceptually simple pro- lar due to good performance of NMSLIB and
cedure that consists in finding k data set ele- KGraph (Dong et al., 2011)9.
mentsthathavehighestsimilarityscores(or,
Specifically, two successive ANN-
alternatively, smallest distances) to another
Benchmarks challenges (Aumüller et al.,
element called query. Despite its formulaic
2019) were won first by our efficient im-
simplicity, k-NN search is a notoriously diffi-
plementation of the Navigable Small World
cult problem, which is hard to do efficiently,
(NSW) (Malkov et al., 2014) and then by the
i.e., faster than the brute-force scan of the
HierarchicalNavigableSmallWorld(HNSW)
data set, for high dimensional data and/or
contributed to NMSLIB by Yury Malkov
non-Euclidean distances. In particular, for
(Malkov and Yashunin, 2018). HNSW
some data sets exact search methods do not
performance was particularly impressive.
2https://github.com/nmslib/nmslib
3https://pypistats.org/packages/nmslib 7https://github.com/facebookresearch/faiss/
4https://amzn.to/3aDCMtC wiki/Running-on-GPUs
5https://github.com/nmslib/nmslib 8https://www.nvidia.com/en-us/data-center/a100/
6https://thrift.apache.org/ 9https://github.com/aaalgo/kgraph
Unlike many other libraries for k-NN
search, NMSLIB focuses on retrieval for
generic similarities. The generality is
achieved by relying largely on distance- NAPP Lucene Bag-of-words
based methods: NSW (Malkov et al., 2014), BrH utN eS foW rce DAAT C++ Te prm os l ii ts iot nw sith
HNSW (Malkov and Yashunin, 2018), NAPP
(Tellez et al., 2013; Boytsov et al., 2016),
Figure 1: Retrieval Architecture and Workflow
and an extension of the VP-tree (Boytsov
Overview
and Naidan, 2013b; Boytsov and Nyberg,
2019b). Distance-based methods can only
usevaluesofthemutualdatapointdistances, speed up using special SIMD operations. In
but cannot exploit the structure of the data, addition, NMSLIB supports the Jaccard sim-
e.g., they have no direct access to vector ilarity, the Levenshtein distance (for ASCII
elements or string characters. In addition, strings), and the number of (more exotic)
NMSLIB has a simple (no compression) im- divergences (including the KL-divergence).
plementation of a traditional inverted file, The library has substantial documentation
which can be used to carry out an exact and additional information can be found on-
maximum-inner product search on sparse
line11.
vectors. 3 FlexNeuART
Graph-based retrieval algorithms have
3.1 Motivation
been shown to work efficiently for a variety
of non-metric and non-symmetric distances FlexibleclassicandNeurAlRetrievalToolkit,
(Boytsov and Nyberg, 2019a; Boytsov, 2018; or shortly FlexNeuART (intended pronuncia-
Naidan et al., 2015b). This flexibility per- tion flex-noo-art) is a modular text retrieval
mits adding new distances/similarities with toolkit, which incorporates some of the best
little effort (as we do not have to change the classic,i.e.,traditional,informationretrieval
retrieval algorithms). However, this needs (IR) signals and provides capabilities for in-
to be done in C++, which is one limita- tegration with recent neural models. This
tion. It is desirable to have an API where toolkitsupportsallkeystagesoftheretrieval
C++ code could call Python-implemented pipeline, including indexing, generation of
distances. NMSLIB supports only in-memory trainingdata,trainingthemodels,candidate
indices and with a single exception all in- generation, and re-ranking.
dices are static, which is another (current) FlexNeuART has been under active devel-
limitation of the library. opment for several years and has been used
There is a number of data format and for our own projects, in particular, to inves-
distances—a combination which we call a tigate applicability of k-NN search for text
space—supported by NMSLIB. A detailed de- retrieval (Boytsov et al., 2016). It was also
scription can be found online10. Most impor- used in recent TREC evaluations (Craswell
tantly,thelibrarysupportsL p distanceswith et al., 2020) as well as to produce strong
the norm (cid:107)x(cid:107)
p
= (cid:0)(cid:80) i∈I|x i|p(cid:1)1/p , the cosine runs on the MS MARCO document leader-
board.12 ThetoolkitisgearedtowardsTREC
similarity, and the inner product similarity.
evaluations: For broader acceptance we
For all of these, the data can be both fixed-
would clearly need to implement Python
size “dense” and variable-size “sparse“ vec-
bindings and experimentation code at the
tors. Sparse vectors can have an unlimited
Python level.
number of non-zero elements and their pro-
cessing is less efficient compared to dense
vectors. On Intel CPUs the processing is 11https://github.com/nmslib/nmslib/tree/
master/manual
10https://github.com/nmslib/nmslib/blob/ 12https://microsoft.github.io/msmarco/
master/manual/spaces.md #docranking
FlexNeuART was created to fulfill the fol-
1{
lowing needs: 2 "DOCNO" : "0",
3 "text" : "nfl team represent super bowl 50",
4 "text_unlemm" : "nfl teams represented super bowl 50"
• Shallow integration with Lucene and
5}
state-of-the-art toolkits for k-NN search
(i.e., the candidate generation compo- Figure2: Sampleinputforquestion“WhichNFL
nent should be easy to change); teamrepresentedtheAFCatSuperBowl50?”
• Efficient retrieval and efficient re-
3.2 System Design and Workflow
ranking with basic relevance signals;
The FlexNeuART system—outlined in Fig-
• An out-of-the-box support for multi-field
ure 1—implements a classic multi-stage
document ranking;
retrieval pipeline, where documents flow
• Aneaseofimplementationand/oruseof through a series of “funnels“ that discard
most traditional ranking signals; unpromising candidates using increasingly
more complex and accurate ranking com-
• An out-of-the-box support for learning-
ponents. In that, FlexNeuART supports one
to-rank (LETOR) and basic experimenta-
intermediate and one final re-ranker (both
tion;
are optional). The initial ranked set of doc-
uments is provided by the so-called candi-
• A support for mixed dense-sparse re-
dategenerator (alsoknownasthecandidate
trieval and/or re-ranking.
provider).
Packages most similar to ours in retrieval FlexNeuART is designed to work with
and LETOR capabilities are Anserini (Yang plugable candidate generators and re-
et al., 2018), Terrier (Ounis et al., 2006), rankers. Out-of-the-box it supports Apache
and OpenNIR (MacAvaney, 2020). Anserini Lucene14 and NMSLIB, which we describe
and Terrier are Java packages, which were in § 2. NMSLIB works as a standalone multi-
recently enhanced with Python bindings threaded server implemented with Apache
through Pyserini13 and PyTerrier (Mac- Thrift.15 NMSLIB supports an efficient ap-
donaldandTonellotto,2020). OpenNIRimple- proximate (and in some cases exact) max-
ments re-ranking code on top of Anserini. imum inner-product search on sparse and
These packages are tightly integrated with sparse-dense representations. Sparse-dense
specific retrieval toolkits, which makes im- retrieval is a recent addition.
plementation of re-ranking components diffi- Lucene full-text search algorithms rely on
cult, as these components need to access classic term-level inverted files, which are
retrieval engine internals—which are fre- stored in compressed formats (so Lucene is
quently undocumented—to retrieve stored quite space-efficient). NMSLIB (see § 2) sup-
documents, term statistics, etc. Replac- ports the classic (uncompressed) inverted
ing the core retrieval component becomes files with document-at-at-time (DAAT) pro-
problematic as well. In contrast, our sys- cessing, the brute-force search, the graph-
tem decouples retrieval and re-ranking mod- based retrieval algorithms HNSW (Malkov
ules by keeping an independent forward in- andYashunin,2018)andNSW(Malkovetal.,
dex, which enables plugable LETOR and IR 2014), as well the pivoting algorithm NAPP
modules. In addition to this, OpenNIR and (Tellez et al., 2013; Boytsov et al., 2016).
PyserinidonotprovideAPIforfusionofrel- The indexing and querying pipelines in-
evancesignalsandnoneofthetoolkitsincor- gest data (queries and documents) in the
porates a lexical translation model (Berger form of multi-field JSON entries, which are
et al., 2000), which can substantially boost generated by external Java and/or Python
accuracy for QA.
14https://lucene.apache.org/
13https://github.com/castorini/pyserini 15https://thrift.apache.org/
code. Each field can be parsed or raw. The
1{"extractors": [
parsed field contains white-space separated 2 {"type": "TFIDFSimilarity",
3 "params": {
tokens while the raw field can keep arbi- 4 "indexFieldName": "text",
trary text, which is tokenized directly by re- 5 "queryFieldName": "text",
6 "similType": "bm25",
ranking components. In particular, BERT 7 "k1": "1.2",
8 "b": "0.75"}
models rely on their own tokenizers (Devlin
9 },
et al., 2018). 10 {"type": "avgWordEmbed",
11 "params": {
The core system does not directly incor- 12 "indexFieldName": "text_unlemm",
13 "queryFieldName": "text_unlemm",
porate any text processing code, instead,
14 "queryEmbedFile": "embeds/starspace_unlemm.query",
we assume that an external pipeline does 15 "docEmbedFile": "embeds/starspace_unlemm.answer",
16 "useIDFWeight": "True",
all the processing: parsing, tokenization, 17 "useL2Norm": "True",
stopping, and possibly stemming/lemmati- 18 "distType": "l2"}
19 }
zation to produce a string of white-space 20 ]}
separated tokens. This relieves the indexing
code from the need to do complicated pars- Figure3: Samplescoringconfiguration.
ing and offers extra flexibility in choosing
parsing tools.
contains an array of scoring sub-modules
An example of a two-field input JSON en- whose parameters are specified via nested
try for a SQuAD 1.1 (Rajpurkar et al., 2016) dictionaries (in curly brackets). Each de-
question is given in Fig. 2. Document and scription contains the mandatory parame-
query entries contain at least two manda- ters type and params. Scoring modules are
tory fields: DOCNO and text, which repre- feature extractors, each of which produces
sent the document identifier and indexable one or more numerical feature that can be
text. Queries and documents may have ad- used by a LETOR component to train a rank-
ditional optional fields. For example, HTML ing model or to score a candidate document.
documents commonly have a title field. The special composite feature extractor
In Fig. 2, text_unlemm consists of lower- reads the configuration file and for each de-
cased original words, and text contains scription of the extractor it creates an in-
word lemmas. Stop words are removed from stance ofthe featureextractor whosetype is
both fields. From our prior TREC experi- defined by type. The value of params can be
mentswelearnedthatitisbeneficialtocom- arbitrary: parsing and interpreting param-
bine scores obtained for the lemmatized (or eters is delegated to the constructor of the
stemmed) and the original text (Boytsov and extractor object.
Belova, 2011). A sample configuration in Fig. 3 defines
Retrieval requires a Lucene or an NMSLIB a BM25 (Robertson, 2004) scorer with pa-
index, each of which can be created inde- rameters k 1 = 1.2 and b = 0.25 for the index
pendently. To support re-ranking, we also field text (and query field text) as well as
need to create forward indices. There is one the averaged embedding generator for the
forwardindexforeachdatafield. Forparsed fieldstext_unlemm. Thelattercreatesdense
fields, it contains bag-of-word representa- query and document representations using
tions of documents (term IDs and frequen- StarSpace embeddings (Wu et al., 2018).
cies) and (optionally) an ordered sequence There are separate sets of embeddings for
of words. For raw fields, the index keeps queries and documents. Word embeddings
unmodified text. A forward index is also re- are weighted using IDFs and subsequently
quired to create an NMSLIB index. L 2 normalized. Finally, this extractor pro-
TheFlexNeuARTsystemhasaconfigurable duces a single feature equal to the L 2 dis-
re-ranking module, which can combine re- tance between averaged embeddings of the
sults from several ranking components. A query and the document.
sample configuration file shown in Fig. 3 From the forward indices, we can export
model, which, in our experience, is effective
1 [
2 { primarily when the number of features and
3 "experSubdir": "final_exper",
4 "candProvAddConfParam" : "exper_desc/lucene.json", training examples is quite large.
5 "extrType": "exper_desc/final_extr.json", We provide basic experimentation sup-
6 "extrTypeInterm" : "exper_desc/interm_extr.json",
7 "modelInterm" : "exper_desc/classic_ir.model", port. An experiment is described via a
8 "candQty" : 2000,
JSON descriptor, which defines parameters
9 "testOnly": 0,
10 "runId" : "sample_run_id" of the candidate generating, re-ranking, and
11 }
12 ] LETOR algorithms. Some experimentation
parameters such as training and testing sub-
Figure4: Sampleexperimentalconfiguration. sets can also be specified in the command
line.
A sample descriptor is shown in Fig. 4.
data to NMSLIB and create an index for
It uses an intermediate re-ranker which re-
k-NN search. This is supported only for
scores 2000 entries with the highest Lucene
inner-product similarities. As discussed in
scores. A given number of highly scored
the following subsection § 3.3, there are
entries can be further re-scored using the
two scenarios. In the first scenario we ex-
“final” re-ranker. Note that the experimen-
port one vector per feature extractor. In
tal descriptor references feature-extractor
particular, we generate a sparse vector for
JSONs rather than defining everything in a
BM25 and a dense vector for the averaged
single configuration file.
embeddings. Then, NMSLIB combines these
Given an experimental descriptor, the
representations on its own using adjustable
training pipeline generates specified fea-
weights, which can be tweaked after data is
tures, exports results to a special RankLib
exported. In the second scenario–which is
format and trains the model. Training of
more efficient but less flexible—we create
the LETOR model also requires a relevance
one composite vector per document/query,
file (a QREL file in the TREC NIST format),
where individual component weights cannot
which lists known relevant documents. Af-
be changed further after export.
ter training, the respective retrieval system
3.3 Scoring Modules
is evaluated on another set of queries. The
Similarity scores between queries and doc- user can disable model training: This mode
uments are computed for a pair of query is used to tune BM25.
and a document field (typically these are the Based on our experience with TREC and
same fields).16 Scores from various scorers community QA collections (Boytsov and
are then combined into a single score by Naidan, 2013b; Boytsov, 2018), we support
a learning-to-rank (LETOR) algorithm (Liu the following scoring approaches:
et al., 2009). FlexNeuART use the LETOR
• A proxy scorer that reads scores from
library RankLib from which we use two par-
one or more standalone scoring servers,
ticularly effective learning algorithms: a co-
which can be implemented in Python
ordinate ascent (Metzler and Croft, 2007)
or any other language supported by
and LambdaMART (Burges, 2010). We have
Apache Thrift.17. Our system imple-
found a bug in RankLib implementation of
ments neural proxy scorers for CEDR
the coordinate ascent: We, thus, use our
(MacAvaney et al., 2019) and MatchZoo
own, bugfixed, version.
(Fan et al., 2017). We have modified
Coordinate ascent produces a linear
CEDR by providing a better parameteri-
model. It is most effective when the number
zation of the training procedure, adding
of features and/or the number of examples
support for BERT large (Devlin et al.,
is small. LambdaMART is a boosted tree
2018) and multi-GPU training.
16There can be multiple scorers for each pair of
fields. 17https://thrift.apache.org/
• The TF×IDF similarity BM25 (Robert- Instead we should stick to a simple vector-
son, 2004), where logarithms of inverse space model, where similarity is computed
document term frequencies (IDFs) are as the inner product between query and
multiplied by normalized and smoothed document vectors (Manning et al., 2010).
term counts in a document (TFs). The respective retrieval procedure is a maxi-
mum inner-product search (a form of k-NN
• Sequential dependence model (Metzler
search). For example both BM25 and the co-
andCroft,2005): ourre-implementation
sine similarity between query and document
is based on the one from Anserini.
embeddings belong to this class of scorers.
• BM25-based proximity scorer, which Under the vector-space framework we
treats ordered and unordered pairs of need to (1) generate/read a set of field-
query terms as a single token. It is sim- specific vectors for queries and documents,
ilar to the proximity scorer used in our (2) compute field-specific scores using the
prior work (Boytsov and Belova, 2011). inner product between query and document
vectors, and (3) aggregate the scores using
• Cosine/L 2 distance between averaged a linear model. Alternatively, we can cre-
word embeddings. We first train word ate composite queries and document vec-
embeddings for the corpus, then con- tors, where we concatenate field-specified
struct a dense vector for a document vectors multiplied by field weights. Then,
(or query) by applying TF×IDF weight- the overall similarity score is computed as
ing to the individual word embeddings the inner product between composite query
and summing them. Then we compare and document vectors.
averaged embeddings using the cosine
Our system supports both computation
similarity (or L 2 distance). scenarios. To this end, all inner-product
equivalent scorers should inherit from a
• IBM Model 1 is a lexical translation
specific abstract class and implement the
model trained using expectation maxi-
functions to generate respective query and
mization. We use Model 1 to compute
document vectors. This abstraction simpli-
an alignment log-probability between
fies generation of sparse and sparse-dense
queries and answer documents. Using
query/document vectors, which can be sub-
Model 1 allows us to reduce the vocab-
sequently indexed by NMSLIB.
ulary gap between queries and docu-
ments (Berger et al., 2000).
4 Experiments
• Aproxyquery-anddocumentembedder, We carry out experiments with two objec-
that produces fixed-size dense vectors tives: (1) measuring effectiveness of im-
for queries and documents. The similar- plemented ranking models; (2) demonstrat-
ity is the inner product between query ing the value of a well-tuned traditional IR
and document embeddings. This scorer system. We use two recently released MS
operates as an Apache Thrift server. MARCO collections (Craswell et al., 2020;
Nguyen et al., 2016) and a community ques-
• A BM25-based pseudo-relevance feed-
tion answering (CQA) collection Yahoo An-
back model RM3. Unlike a common ap-
swers Manner (Surdeanu et al., 2011). Col-
proach where RM3 is used for query-
lection statistics is summarized in Table 1.
expansion, we use it in re-ranking mode
MSMARCOhasadocumentandapassage
(Diaz, 2015).
re-ranking task where all queries can be an-
Although FlexNeuART supports complex swered using a short text snippet. There
scoring models, these can be computation- are three sets of queries in each task. In
ally too expensive to be used directly for re- addition to one large query set with sparse
trieval (Boytsov et al., 2016; Boytsov, 2018). judgments, there are two small evaluation
Yahoo contractions such as “n’t” and “’ll”. Lemmas
MSMARCO
Answers
are indexed using Lucene 7.6. In the case of
documents passages
MS MARCO documents, entries come in the
generalstatistics
HTML format. We extract HTML body and
#ofdocuments 3.2M 8.8M 819.6K
#ofdoc.lemmas 476.7 30.6 20.1 title (and store/index them separately).
#ofquerylemmas 3.2 3.5 11.9
In additional to traditional tokenizers, we
#ofqueries
also use the BERT tokenizer from the Hug-
train/fusion 10K 20K 14.3K
train/modeling 357K 788.7K 100K gingFace Transformers library (Wolf et al.,
development 2500 20K 7034
test 2693 3000 3000 2019). This tokenizer can split a single
TREC2019 100 100
word into several sub-word pieces (Wu et al.,
TREC2020 100 100
2016). The stopword list is not applied to
BITEXTtokens
BERT tokens.
#ofQApairs 43.9M 4M 572.8K
#ofquerytokens 2.7 2.8 12.6 Training Model 1, which is a translation
#ofdoc.tokens 4.3 4.2 20
model, requires a parallel corpus where
BITEXTBERTwordpieces
queries are paired with respective relevant
#ofQApairs 50M 9.5M 572.8K
#ofquerytokens 6.1 2.8 42.3 documents. The parallel corpus is also
#ofdoc.tokens 9.4 4.3 62.7
knownasabitext. InthecaseofMSMARCO
Table1: Datasetstatistics collections documents are much longer than
queries, which makes it impossible to com-
candidate MSMARCO MSMARCO pute translation probabilities using stan-
generator documents passages dard alignment tools (Och and Ney, 2003).18
TREC TREC
develop. develop. Hence, for each pair of query q and its rele-
2019 2019
vantdocumentd,wefirstsplitdintomultiple
BM25 0.647 0.443 0.707 0.452
Tunedsystem 0.693 0.472 0.739 0.480 short chunks d 1, d 2, ...d n. Then, we replace
Gain 7.08% 6.39% 4.57% 6.08% the pair (q,d) with a set of pairs {(q,d i)}.
We evaluate performance of several mod-
Table 2: The effect of using a more effec-
tive candidate generator (evaluation metric is els and their combinations. Each model
NDCG@10). BM25istunedforMSMARCOpas- name is abbreviated as X (Y), where X is
sages,butnotdocuments. a type of the model (see §3.3 for details)
and Y is a type of the text field. Specifi-
cally, we index original tokens, lemmas, as
sets from the TREC 2019/2020 deep learn-
wellasBERTtokensextractedfromthemain
ingtrack(Craswelletal.,2020). MSMARCO
document text. For MS MARCO documents,
collections query sets were randomly split
whichcomeinHTMLformat,wealsoextract
into training, development (to tune hyper
tokens and lemmas from the title field.
parameters), and test sets.
First, we evaluate performance of the
Yahoo Answers Manner has a large num-
tuned BM25 (lemmas). Second, we evaluate
ber of paired question-answer pairs. We in-
fusion models that combine BM25 (lemmas)
cludeitinourexperiments,becauseModel1
with BM25, proximity, and Model 1 scores
wasshowntobeeffectiveforCQAdatainthe
(see §3.3) computed for various fields. Note
past (Jeon et al., 2005; Riezler et al., 2007;
that our fusion models are linear. Third, we
Surdeanu et al., 2011; Xue et al., 2008). It
evaluate collection-specific combinations of
wasrandomlysplitintothetrainingandeval-
manually-selected models: Except for minor
uation sets.
changes these are the fusion models that we
Document text is processed using Spacy
used in our TREC 2019 and 2020 submis-
2.2.3 (Honnibal and Montani, 2017) to ex-
sions.
tract tokens and lemmas. The frequently oc-
All models were trained and/or tuned us-
curredtokensandlemmasarefilteredoutus-
ing training and development sets listed in
ing Indri’s list of stopwords (Strohman et al.,
2005), which is expanded to include a few 18https://github.com/moses-smt/mgiza/
Yahoo
MSMARCOdocuments MSMARCOpassages
Answers
TREC TREC TREC TREC
test test test
2019 2020 2019 2020
MRR NDCG@10 NDCG@10 MRR NDCG@10 NDCG@10 NDCG@10
BM25(lemmas) 0.270 0.544 0.524 0.256 0.522 0.516 0.152
BM25(lemmas)+BM25(BERTtokens) 0.283 0.528 0.537 0.270 0.518 0.525 0.159
BM25(lemmas)+BM25(tokens) 0.274 0.544 0.523 0.265 0.517 0.521 0.157
BM25(lemmas)+BM25(titletokens) 0.294 0.550 0.527
BM25(lemmas)+proximity(lemmas) 0.282 0.559 0.524 0.257 0.538 0.523
BM25(lemmas)+proximity(tokens) 0.284 0.560 0.531 0.265 0.534 0.524
BM25(lemmas)+Model1(tokens) 0.283 0.548 0.535 0.274 0.522 0.567 0.160
BM25(lemmas)+Model1(BERTtokens) 0.284 0.557 0.525 0.271 0.517 0.509 0.175
bestcombination 0.310 0.565 0.542 0.290 0.558 0.560
Table3: Evaluationofvariousfusionmodels.
Table 1. For TREC 2019 and 2020 query scorer). However, we refrained from cor-
sets (as well as for Yahoo Answers Man- recting this error to illustrate how a good fu-
ner), the evaluation metric is NDCG@10 sion model can produce a strong ranker via
(Järvelin and Kekäläinen, 2002), which the a combination of suboptimal weak rankers.
mainmetricintheTRECdeeplearningtrack
Indeed, as we can see from Table 2, there
(Craswell et al., 2020). For subsets of MS
is a substantial 4.5-7% loss in accuracy by
MARCO collections, we use the mean recip-
re-ranking the output of BM25 compared
rocal rank (MRR) as suggested by Craswell
to re-ranking the output of the well-tuned
et al. (2020).
traditionalpipeline. Thisdegradationoccurs
From the experiments in Table 3, we can
in all four experiments.
see that for all large query sets the fusion
models outperform BM25 (lemmas). In par-
5 Conclusion and Future Work
ticular, the best MS MARCO fusion models
are 13-15% better than BM25 (lemmas). In We present to the NLP community an exist-
the case of Yahoo Answers Manner, com- ing k-NN search library NMSLIB, a new re-
bining BM25 (lemmas) with Model 1 scores trieval toolkit FlexNeuART, as well as their
computedforBERTtokensalsoboostperfor- integration capabilities, which enable effi-
mance by about 15%. For small TREC 2019 cient retrieval of sparse and sparse-dense
and 2020 query sets the gains are marginal. document representations. FlexNeuART im-
However, our fusion models are still better plements a variety of effective traditional
than BM25 (lemmas) by 4-8%. relevance signals, which we plan to use
We further compare the accuracy of the for a fairer comparison with recent neu-
BERT-based re-ranker (Nogueira and Cho, ral retrieval systems based on representing
2019) applied to the output of the tuned tra- queries and documents via fixed-size dense
ditional IR system with the accuracy of the vectors.
same BERT-based re-ranker applied to the
output of Lucene (with a BM25 scorer). The 6 Acknowledgements
BERT scorer is used to re-rank 150 docu-
ments: Further increasing the number of This work was done primarily while Leonid
candidates degraded performance on the BoytsovwasaPhDstudentatCMUwherehe
TREC 2019 test set. was supported by the NSF grant #1618159.
By mistake we used the same BM25 pa- WethankSeanMacAvaneyformakingCEDR
rameters for both passages and documents. (MacAvaney et al., 2019) publicly available
As a result, MS MARCO documents candi- and Igor Brigadir for suggesting to experi-
date generator was suboptimal (passage re- ment with indexing of BERT word pieces.
trieval did use the properly tuned BM25
References Jacob Devlin, Ming-Wei Chang, Kenton Lee,
and Kristina Toutanova. 2018. Bert: Pre-
Sunil Arya and David M Mount. 1993. Approx-
training of deep bidirectional transformers
imate nearest neighbor queries in fixed di-
for language understanding. arXiv preprint
mensions. In Proceedings of the fourth an-
arXiv:1810.04805.
nualACM-SIAMsymposiumonDiscretealgo-
rithms,pages271–280. Fernando Diaz. 2015. Condensed list relevance
models. In Proceedings of the 2015 Interna-
Martin Aumüller, Erik Bernhardsson, and
tional Conference on The Theory of Informa-
Alexander Faithfull. 2019. ANN-benchmarks:
tionRetrieval,pages313–316.
Abenchmarkingtoolforapproximatenearest
neighboralgorithms. InformationSystems. WeiDong,CharikarMoses,andKaiLi.2011. Ef-
ficientk-nearestneighborgraphconstruction
Adam L. Berger, Rich Caruana, David Cohn, for generic similarity measures. In Proceed-
Dayne Freitag, and Vibhu O. Mittal. 2000. ings of the 20th international conference on
Bridging the lexical chasm: statistical ap- Worldwideweb,pages577–586.
proaches to answer-finding. In SIGIR 2000:
Proceedingsofthe23rdAnnualInternational Yixing Fan, Liang Pang, JianPeng Hou, Jiafeng
ACM SIGIR Conference on Research and De- Guo, Yanyan Lan, and Xueqi Cheng. 2017.
velopment in Information Retrieval, July 24- Matchzoo: A toolkit for deep text matching.
28,2000,Athens,Greece,pages192–199. arXivpreprintarXiv:1707.07270.
Leonid Boytsov. 2018. Efficient and Accurate Christophe Van Gysel, Maarten De Rijke, and
Non-Metrick-NNSearchwithApplicationsto Evangelos Kanoulas. 2018. Neural vector
TextMatching. Ph.D.thesis,CarnegieMellon spacesforunsupervisedinformationretrieval.
University. ACM Transactions on Information Systems
(TOIS),36(4):38.
LeonidBoytsovandAnnaBelova.2011. Evaluat-
Changwan Hong, Aravind Sukumaran-Rajam,
inglearning-to-rankmethodsinthewebtrack
Bortik Bandyopadhyay, Jinsung Kim,
adhoctask. InTREC.
Süreyya Emre Kurt, Israt Nisa, Shivani
Sabhlok, Ümit V Çatalyürek, Srinivasan
LeonidBoytsovandBilegsaikhanNaidan.2013a.
Parthasarathy, and P Sadayappan. 2018.
Engineering efficient and effective non-
Efficient sparse-matrix multi-vector product
metricspacelibrary. InProceedingsofSISAP
on gpus. In Proceedings of the 27th Inter-
2013,pages280–293.Springer.
national Symposium on High-Performance
LeonidBoytsovandBilegsaikhanNaidan.2013b. Parallel and Distributed Computing, pages
Learning to prune in metric and non-metric 66–79.
spaces. In Advances in Neural Information
Matthew Honnibal and Ines Montani. 2017.
ProcessingSystems,pages1574–1582.
spaCy 2: Natural language understanding
with Bloom embeddings, convolutional neu-
Leonid Boytsov, David Novak, Yury Malkov, and
ral networks and incremental parsing. To ap-
Eric Nyberg. 2016. Off the beaten path:
pear.
Let’s replace term-based retrieval with k-NN
search. In Proceedings of CIKM 2016, pages
Kalervo Järvelin and Jaana Kekäläinen. 2002.
1099–1108.ACM.
Cumulated gain-based evaluation of ir tech-
niques. ACM Transactions on Information
Leonid Boytsov and Eric Nyberg. 2019a. Accu-
Systems(TOIS),20(4):422–446.
rateandfastretrievalforcomplexnon-metric
data via neighborhood graphs. In Interna- Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
tional Conference on Similarity Search and 2005. Findingsimilarquestionsinlargeques-
Applications,pages128–142.Springer. tion and answer archives. In Proceedings
of the 2005 ACM CIKM International Confer-
Leonid Boytsov and Eric Nyberg. 2019b. Prun-
enceonInformationandKnowledgeManage-
ing algorithms for low-dimensional non-
ment,Bremen,Germany,October31-Novem-
metric k-nn search: A case study. In Similar-
ber5,2005,pages84–90.
itySearchandApplications.
Jeff Johnson, Matthijs Douze, and Hervé Jé-
Christopher JC Burges. 2010. From RankNet gou.2017. Billion-scalesimilaritysearchwith
toLambdaRanktoLambdaMart: Anoverview. gpus. arXivpreprintarXiv:1702.08734.
MicrosoftTechnicalReportMSR-TR-2010-82.
Vladimir Karpukhin, Barlas Og˘uz, Sewon Min,
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Ledell Wu, Sergey Edunov, Danqi Chen, and
Daniel Campos, and Ellen M Voorhees. 2020. Wen-tau Yih. 2020. Dense passage retrieval
Overviewofthetrec2019deeplearningtrack. for open-domain question answering. arXiv
arXivpreprintarXiv:2003.07820. preprintarXiv:2004.04906.
Omar Khattab and Matei Zaharia. 2020. Col- Donald Metzler and W. Bruce Croft. 2007. Lin-
bert: Efficient and effective passage search ear feature-based models for information re-
via contextualized late interaction over bert. trieval. Inf.Retr.,10(3):257–274.
InProceedingsofthe43rdInternationalACM
SIGIR Conference on Research and Devel- Bilegsaikhan Naidan, Leonid Boytsov, Yury
opment in Information Retrieval, SIGIR ’20, Malkov, and David Novak. 2015a. Non-
page 39–48, New York, NY, USA. Association metric space library manual. arXiv preprint
forComputingMachinery. arXiv:1508.05470.
Saar Kuzi, Mingyang Zhang, Cheng Li, Michael Bilegsaikhan Naidan, Leonid Boytsov, and Eric
Bendersky, and Marc Najork. 2020. Lever- Nyberg. 2015b. Permutation search meth-
aging semantic and lexical matching to im- odsareefficient,yetfastersearchispossible.
prove the recall of document retrieval sys- ProceedingsoftheVLDBEndowment,8(12).
tems: A hybrid approach. arXiv preprint
Tri Nguyen, Mir Rosenberg, Xia Song, Jian-
arXiv:2010.01195.
fengGao,SaurabhTiwary,RanganMajumder,
Kenton Lee, Ming-Wei Chang, and Kristina and Li Deng. 2016. Ms marco: A human
Toutanova. 2019. Latent retrieval for weakly generated machine reading comprehension
supervised open domain question answering. dataset.
arXivpreprintarXiv:1906.00300.
Rodrigo Nogueira and Kyunghyun Cho. 2019.
Tie-Yan Liu et al. 2009. Learning to rank Passage re-ranking with bert. arXiv preprint
for information retrieval. Foundations and arXiv:1901.04085.
Trends® in Information Retrieval, 3(3):225–
Franz Josef Och and Hermann Ney. 2003. A
331.
systematic comparison of various statistical
Sean MacAvaney. 2020. OpenNIR: A complete alignmentmodels. ComputationalLinguistics,
neural ad-hoc ranking pipeline. In WSDM 29(1):19–51.
2020.
Iadh Ounis, Gianni Amati, Vassilis Plachouras,
Sean MacAvaney, Andrew Yates, Arman Cohan, Ben He, Craig Macdonald, and Christina Li-
and Nazli Goharian. 2019. Cedr: Contextu- oma. 2006. Terrier: A high performance and
alized embeddings for document ranking. In scalable information retrieval platform. In
Proceedings of the 42nd International ACM ProceedingsoftheOSIRWorkshop,pages18–
SIGIR Conference on Research and Develop- 25.
ment in Information Retrieval, pages 1101–
Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-
1104.
rev,andPercyLiang.2016. Squad: 100,000+
Craig Macdonald and Nicola Tonellotto. 2020. questions for machine comprehension of text.
Declarative experimentation in information In Proceedings of EMNLP 2016, pages 2383–
retrieval using pyterrier. arXiv preprint 2392.
arXiv:2007.14271.
Stefan Riezler, Alexander Vasserman, Ioannis
Yury Malkov, Alexander Ponomarenko, Andrey Tsochantaridis, Vibhu O. Mittal, and Yi Liu.
Logvinov, and Vladimir Krylov. 2014. Approx- 2007. Statistical machine translation for
imate nearest neighbor algorithm based on query expansion in answer retrieval. In ACL
navigable small world graphs. Information 2007, Proceedings of the 45th Annual Meet-
Systems,45:61–68. ing of the Association for Computational Lin-
guistics.
YuryAMalkovandDmitryAYashunin.2018. Ef-
ficientandrobustapproximatenearestneigh- Stephen Robertson. 2004. Understanding in-
borsearchusinghierarchicalnavigablesmall verse document frequency: on theoretical ar-
world graphs. IEEE transactions on pattern guments for IDF. Journal of Documentation,
analysisandmachineintelligence. 60(5):503–520.
Christopher Manning, Prabhakar Raghavan, Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski,
and Hinrich Schütze. 2010. Introduction to Ankur P Parikh, Ali Farhadi, and Hannaneh
information retrieval. Natural Language En- Hajishirzi. 2019. Real-time open-domain
gineering,16(1):100–103. questionansweringwithdense-sparsephrase
index. arXivpreprintarXiv:1906.05807.
Donald Metzler and W Bruce Croft. 2005. A
markov random field model for term depen- Larissa C Shimomura, Rafael Seidi Oyamada,
dencies. In Proceedings of the 28th annual MarcosRVieira,andDanielSKaster.2020. A
international ACM SIGIR conference on Re- surveyongraph-basedmethodsforsimilarity
search and development in information re- searches in metric spaces. Information Sys-
trieval,pages472–479. tems,page101507.
Trevor Strohman, Donald Metzler, Howard Peilin Yang, Hui Fang, and Jimmy Lin. 2018.
Turtle, and W Bruce Croft. 2005. In- Anserini: Reproducible ranking baselines us-
dri: A language-model based search engine ing Lucene. J. Data and Information Quality,
for complex queries. http://ciir.cs.umass. 10(4):16:1–16:20.
edu/pubfiles/ir-407.pdf [Last Checked Apr
2017].
Mihai Surdeanu, Massimiliano Ciaramita, and
Hugo Zaragoza. 2011. Learning to rank
answers to non-factoid questions from web
collections. Computational Linguistics,
37(2):351–383.
Eric Sadit Tellez, Edgar Chávez, and Gonzalo
Navarro. 2013. Succinct nearest neighbor
search. Inf.Syst.,38(7):1019–1030.
Thomas Wolf, Lysandre Debut, Victor Sanh,
Julien Chaumond, Clement Delangue, An-
thony Moi, Pierric Cistac, Tim Rault, Rémi
Louf, Morgan Funtowicz, Joe Davison, Sam
Shleifer,PatrickvonPlaten,ClaraMa,Yacine
Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush.
2019. Huggingface’s transformers: State-of-
the-art natural language processing. ArXiv,
abs/1910.03771.
Ledell Yu Wu, Adam Fisch, Sumit Chopra, Keith
Adams, Antoine Bordes, and Jason Weston.
2018. Starspace: Embed all the things! In
ProceedingsofAAAI2018.
Yonghui Wu, Mike Schuster, Zhifeng Chen,
Quoc V. Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin
Gao, Klaus Macherey, Jeff Klingner, Apurva
Shah, Melvin Johnson, Xiaobing Liu, Lukasz
Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku
Kudo, Hideto Kazawa, Keith Stevens, George
Kurian, Nishant Patil, Wei Wang, Cliff Young,
JasonSmith, JasonRiesa, AlexRudnick, Oriol
Vinyals, Greg Corrado, Macduff Hughes, and
Jeffrey Dean. 2016. Google’s neural ma-
chinetranslationsystem: Bridgingthegapbe-
tweenhumanandmachinetranslation. CoRR,
abs/1609.08144.
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung
Tang, Jialin Liu, Paul Bennett, Junaid Ahmed,
and Arnold Overwijk. 2020. Approximate
nearest neighbor negative contrastive learn-
ing for dense text retrieval. arXiv preprint
arXiv:2007.00808.
Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft.
2008. Retrieval models for question and an-
swer archives. In Proceedings of the 31st
Annual International ACM SIGIR Conference
onResearchandDevelopmentinInformation
Retrieval, SIGIR 2008, Singapore, July 20-24,
2008,pages475–482.

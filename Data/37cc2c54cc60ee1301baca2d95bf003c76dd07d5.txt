Multimodal Abstractive Summarization for How2 Videos
ShrutiPalaskar1 JindrˇichLibovický2 SpandanaGella3∗ FlorianMetze1
1SchoolofComputerScience,CarnegieMellonUniversity
2FacultyofMathematicsandPhysics,CharlesUniversity
3 AmazonAI
spalaska@cs.cmu.edu, libovicky@ufal.mff.cuni.cz
sgella@amazon.com, fmetze@cs.cmu.edu
Abstract video. Ourworkbenefitsusersthroughbettercon-
textualinformationanduserexperience,andvideo
Inthispaper,westudyabstractivesummariza- sharingplatformswithincreaseduserengagement
tion for open-domain videos. Unlike the tra- byretrievingorsuggestingrelevantvideostousers
ditional text news summarization, the goal is
andcapturingtheirattention.
lessto“compress”textinformationbutrather
Summarizationisataskofproducingashorter
to provide a fluent textual summary of infor-
versionofthecontentinthedocumentwhilepre-
mationthathasbeencollectedandfusedfrom
serving its information and has been studied for
different source modalities, in our case video
and audio transcripts (or text). We show how bothtextualdocuments(automatictextsummariza-
a multi-source sequence-to-sequence model tion) and visual documents such as images and
with hierarchical attention can integrate infor- videos(videosummarization). Automatictextsum-
mation from different modalities into a coher- marizationisawidelystudiedtopicinnaturallan-
ent output, compare various models trained
guageprocessing(Luhn,1958;Kupiecetal.,1995;
with different modalities and present pilot ex-
Mani,1999);givenatextdocumentthetaskisto
perimentsontheHow2corpusofinstructional
generate a textual summary for applications that
videos. Wealsoproposeanewevaluationmet-
ric(ContentF1)forabstractivesummarization can assist users to understand large documents.
task that measures semantic adequacy rather Most of the work on text summarization has fo-
than fluency of the summaries, which is cov- cused on single-document summarization for do-
eredbymetricslikeROUGEandBLEU. mains such as news (Rush et al., 2015; Nallapati
etal.,2016;Seeetal.,2017;Narayanetal.,2018)
1 Introduction andsomeonmulti-documentsummarization(Gold-
steinetal.,2000;LinandHovy,2002;Woodsend
In recent years, with the growing popularity of
andLapata,2012;Caoetal.,2015;Yasunagaetal.,
video sharing platforms, there has been a steep
2017).
riseinthenumberofuser-generatedinstructional
Videosummarizationisthetaskofproducinga
videossharedonline. Withtheabundanceofvideos
compactversionofthevideo(visualsummary)by
online, there has been an increase in demand for
encapsulatingthemostinformativeparts(Money
efficientwaystosearchandretrieverelevantvideos
and Agius, 2008; Lu and Grauman, 2013; Gygli
(Songetal.,2011;Wangetal.,2012;Otanietal.,
et al., 2014; Song et al., 2015; Sah et al., 2017).
2016; Torabi et al., 2016). Many cross-modal
Multimodalsummarizationisthecombinationof
searchapplicationsrelyontextassociatedwiththe
textual and visual modalities by summarizing a
video such as description or title to find relevant
videodocumentwithatextsummarythatsumma-
content. However, often videos do not have text
rizesthecontentofthevideo. Multimodalsumma-
meta-dataassociatedwiththemortheexistingones
rizationisamorerecentchallengewithnobench-
donotprovideclearinformationofthevideocon-
marking datasets yet. Li et al. (2017) collected a
tentandfailtocapturesubtledifferencesbetween
multimodalcorpusof500Englishnewsvideosand
relatedvideos(Wangetal.,2012). Weaddressthis
articlespairedwithmanuallyannotatedsummaries.
byaimingtogenerateashorttextsummaryofthe
The dataset is small-scale and has news articles
videothatdescribesthemostsalientcontentofthe
with audio, video, and text summaries, but there
∗*WorkdonewhileSGwasatUniversityofEdinburgh arenohumanannotatedaudio-transcripts.
Transcript Video
today we are going to show you how to make spanish omelet . i 'm going to
dice a little bit of peppers here . i 'm not going to use a lot , i 'm going to use
very very little . a little bit more then this maybe . you can use red peppers if
you like to get a little bit color in your omelet . some people do and some
people do n't …. t is the way they make there spanish omelets that is what she
says . i loved it , it actually tasted really good . you are going to take the onion
also and dice it really small . you do n't want big chunks of onion in there
cause it is just pops out of the omelet . so we are going to dice the up also very
very small . so we have small pieces of onions and peppers ready to go .
Summary
how to cut peppers to make a spanish omelette; get expert tips and advice on making cuban breakfast recipes in this free
cooking video .
Figure 1: How2 dataset example with different modalities. “Cuban breakfast” and “free cooking video” is not
mentionedinthetranscript,andhastobederivedfromothersources.
Relatedtasksincludeimageorvideocaptioning 2 MultimodalAbstractive
anddescriptiongeneration,videostorygeneration, Summarization
procedurelearningfrominstructionalvideosand
TheHow2dataset(Sanabriaetal.,2018)contains
titlegenerationwhichfocusoneventsoractivities
about 2,000 hours of short instructional videos,
in the video and generating descriptions at vari-
spanningdifferentdomainssuchascooking,sports,
ous levels of granularity from single sentence to
indoor/outdoor activities, music, etc. Each video
multiplesentences(Dasetal.,2013;Regnerietal.,
is accompanied by a human-generated transcript
2013; Rohrbach et al., 2014; Zeng et al., 2016;
anda2to3sentencesummaryisavailableforev-
Zhouetal.,2018;Zhangetal.,2018;Gellaetal.,
eryvideowrittentogenerateinterestinapotential
2018). Acloselyrelatedtasktooursisvideotitle
viewer.
generation where the task is to describe the most
salient event in the video in a compact title that TheexampleinFigure1showsthetranscriptde-
is aimed at capturing users attention (Zeng et al., scribesinstructionsindetail,whilethesummaryis
2016). Zhou et al. (2018) present the YouCookII ahigh-leveloverviewoftheentirevideo,mention-
datasetcontaininginstructionalvideos,specifically ingthatthepeppersarebeing“cut”,andthatthis
cooking recipes, with temporally localized anno- is a “Cuban breakfast recipe”, which is not men-
tations for the procedure which could be viewed tionedinthetranscript. Weobservethattextand
asasummarizationtaskaswellalthoughlocalized visionmodalitiesbothcontaincomplementaryin-
withtimealignmentsbetweenvideosegmentsand formation,therebywhenfused,helpsingenerating
procedures. richerandmorefluentsummaries. Additionally,we
canalsoleveragethespeechmodalitybyusingthe
Inthiswork,westudymultimodalsummariza- outputofaspeechrecognizerasinputtoasumma-
tionwithvariousmethodstosummarizetheintent rizationmodelinsteadofahuman-annotatedtran-
ofopen-domaininstructionalvideosstatingtheex- script. The How2 corpus contains 73,993 videos
clusiveanduniquefeaturesofthevideo,irrespec- fortraining,2,965forvalidationand2,156fortest-
tiveofmodality. Westudythistaskindetailusing ing. Theaveragelengthoftranscriptsis291words
thenewHow2dataset(Sanabriaetal.,2018)which and of summaries is 33 words. A more general
containshumanannotatedvideosummariesfora comparisonoftheHow2datasetforsummarization
variedrangeoftopics. Ourmodelsgeneratenatu- ascomparedwithcertaincommondatasetsisgiven
ral language descriptions for video content using in(Sanabriaetal.,2018).
thetranscriptions(bothuser-generatedandoutput
ofautomaticspeechrecognitionsystems)aswell Video-based Summarization. We represent
as visual features extracted from the video. We videos by features extracted from a pre-trained
also introduce a new evaluation metric (Content action recognition model: a ResNeXt-101 3D
F1)thatsuitsthistaskandpresentdetailedresults ConvolutionalNeuralNetwork(Haraetal.,2018)
tounderstandthetaskbetter. trained to recognize 400 different human actions
in the Kinetics dataset (Kay et al., 2017). These
featuresare2048dimensional,extractedforevery
··· ··· videoframes
16 non-overlapping frames in the video. This
resultsinasequenceoffeaturevectorspervideo
ResNeXtfeatures
··· ···
rather than a single/global one. We use these (w/RNN:7;w/oRNN:6,8,9)
sequential features in our models described in
attention
Section3. ⊕
attention hier.attn. ...
Speech-basedSummarization. Weleveragethe (8,9)
speech modality by using the outputs from a pre- w
trainedspeechrecognizerthatistrainedwithother RNNovertranscript (3-5,8,9) RNNdecoder
data, as inputs to a text summarization model.
We use the state-of-the-art models for distant-
Figure2: Buildingblocksofthesequence-to-sequence
microphoneconversationalspeechrecognition,AS- models,graynumbersinbracketsindicatewhichcom-
pIRE (Peddinti et al., 2015) and EESEN (Miao ponentsareutilizedinwhichexperiments.
etal.,2015;LeFrancetal.,2018). Theworderror
rateofthesemodelsontheHow2testdatais35.4%.
the input modalities (text and video). In the next
This high error mostly stems from normalization
step, the context vectors are treated as states of
issues in the data. For example, recognizing and
another encoder, and a new vector is computed.
labeling“20”as“twenty”etc. Handlingtheseeffec-
Whenusingasequenceofactionfeaturesinstead
tivelywillreducetheworderrorratessignificantly.
of a single averaged vector for a video, the RNN
Weaccepttheseasisforthistask.
layerhelpscapturecontext. InFigure2wepresent
thebuildingblockofourmodels.
TransferLearning. OurparallelworkSanabria
etal.(2019)demonstratestheuseofsummarization
4 Evaluation
modelstrainedinthispaperforatransferlearning
basedsummarizationtaskontheCharadesdataset Weevaluatethesummariesusingthestandardmet-
(Sigurdssonetal.,2016)thathasaudio,video,and ricforabstractivesummarizationROUGE-L(Lin
text(summary,captionandquestion-answerpairs) andOch,2004)thatmeasuresthelongestcommon
modalities similar to theHow2dataset. Sanabria sequencebetweenthereferenceandthegenerated
etal.(2019)observethatpre-trainingandtransfer summary. Additionally,weintroducetheContent
learning with the How2 dataset led to significant F1 metric that fits the template-like structure of
improvementsinunimodalandmultimodaladapta- the summaries. We analyze the most frequently
tiontasksontheCharadesdataset. occurringwordsinthetranscriptionandsummary.
Thewordsintranscriptreflecttheconversational
3 SummarizationModels and spontaneous speech while the words in the
summariesreflecttheirdescriptivenature. Forex-
Westudyvarioussummarizationmodels. First,we
amples,seeTableA1inAppendixA.2.
useaRecurrentNeuralNetwork(RNN)Sequence-
to-Sequence(S2S)model(Sutskeveretal.,2014) Content F1. This metric is the F1 score of the
consisting of an encoder RNN to encode (text contentwordsinthesummariesbasedoveramono-
or video features) with the attention mechanism lingualalignment,similartometricsusedtoevalu-
(Bahdanau et al., 2014) and a decoder RNN to atequalityofmonolingualalignment(Sultanetal.,
generate summaries. Our second model is a 2014). WeusetheMETEORtoolkit(Banerjeeand
Pointer-Generator(PG)model(Vinyalsetal.,2015; Lavie,2005;DenkowskiandLavie,2014)toobtain
Gülçehre et al., 2016) that has shown strong per- the alignment. Then, we remove function words
formanceforabstractivesummarization(Nallapati andtask-specificstopwordsthatappearinmostof
etal.,2016;Seeetal.,2017). Asourthirdmodel, thesummaries(seeAppendixA.2)fromtherefer-
weusehierarchicalattentionapproachofLibovický enceandthehypothesis. Thestopwordsareeasy
andHelcl2017originallyproposedformultimodal topredictandthusincreasetheROUGEscore. We
machinetranslationtocombinetextualandvisual treatremainingcontentwordsfromthereference
modalities to generate text. The model first com- andthehypothesisastwobagsofwordsandcom-
putesthecontextvectorindependentlyforeachof putetheF1scoreoverthealignment. Notethatthe
videoframes············ResNeXtfeatures(w/RNN:7;w/oRNN:6,8,9)attentionRNNovertranscript(3-5,8,9)attention⊕hier.attn.(8,9)w...RNNdecoder
ModelNo. Description ROUGE-L ContentF1
1 RandomBaselineusingLanguageModel 27.5 8.3
2a Rule-basedExtractivesummary 16.4 18.8
2b Next-neighborSummary 31.8 17.9
3 UsingExtractedSentencefrom2aonly(Text-only) 46.4 36.0
4 First200tokens(Text-only) 40.3 27.5
5a S2SCompleteTranscript(Text-only,650tokens) 53.9 47.4
5b PGCompleteTranscript(Text-only) 50.2 42.0
5c ASRoutputCompleteTranscript(Text-only) 46.1 34.7
6 ActionFeaturesonly(Video) 38.5 24.8
7 ActionFeatures+RNN(Video) 46.3 34.9
8 Ground-truthtranscript+ActionwithHierarchicalAttn 54.9 48.9
9 ASRoutput+ActionwithHierarchicalAttn 46.3 34.7
Table1:ROUGE-LandContentF1fordifferentsummarizationmodels:randombaseline(1),rule-basedextracted
summary (2a), nearest neighbor summary (2b), different text-only (3,4,5a), pointer-generator (5b), ASR output
transcript(5c),video-only(6-7)andtext-and-videomodels(8-9).
Model(No.) INF REL COH FLU a low Content F1 score in Table 1. As another
baseline, we replace the target summary with a
Text-only(5a) 3.86 3.78 3.78 3.92
rule-basedextractedsummaryfromthetranscrip-
Video-only(7) 3.58 3.30 3.71 3.80
tionitself. Weusedthesentencecontainingwords
Text-and-Video(8) 3.89 3.74 3.85 3.94
“howto”withpredicateslearn,tell,show,discuss
orexplain,usuallythesecondsentenceinthetran-
Table 2: Human evaluation scores on 4 different mea-
sures of Informativeness (INF), Relevance (REL), Co- script. Ourfinalbaselinewasamodeltrainedwith
herence(COH),Fluency(FLU). thesummaryofthenearestneighborofeachvideo
intheLatentDirichletAllocation(LDA;Bleietal.,
2003) based topic space as a target. This model
scoreignoresthefluencyofoutput.
achieves a similar Content F1 score as the rule-
Human Evaluation. In addition to automatic basedmodelwhichshowsthesimilarityofcontent
evaluation,weperformahumanevaluationtoun- andfurtherdemonstratestheutilityoftheContent
derstandtheoutputsofthistaskbetter. Following F1score.
the abstractive summarization human annotation
Weusethetranscript(eitherground-truthtran-
workofGruskyetal.(2018),weaskourannotators
scriptorspeechrecognitionoutput)andthevideo
tolabelthegeneratedoutputonascaleof1−5on
action features to train various models with dif-
informativeness,relevance,coherence,andfluency.
ferent combinations of modalities. The text-only
Weperformthisonrandomlysampled500videos
modelperformsbestwhenusingthecompletetran-
from the test set. We evaluate three models: two
scriptintheinput(650tokens). Thisisincontrast
unimodal(text-only(5a),video-only(7))andone
to prior work with news-domain summarization
multimodal (text-and-video (8)). Three workers
(Nallapati et al., 2016). We also observe that PG
annotatedeachvideoonAmazonMechanicalTurk.
networksdonotperformbetterthanS2Smodelson
More details about human evaluation are in the
thisdatawhichcouldbeattributedtotheabstrac-
AppendixA.5.
tivenatureofoursummariesandalsothelackof
commonn-gramoverlapbetweeninputandoutput
5 ExperimentsandResults
whichistheimportantfeatureofPGnetworks. We
As a baseline, we train an RNN language model alsousetheautomatictranscriptionsobtainedfrom
(Sutskeveretal.,2011)onallthesummariesand apretrainedautomaticspeechrecognizerasinput
randomly sample tokens from it. The output ob- tothesummarizationmodel. Thismodelachieves
tainedisfluentinEnglishleadingtoahighROUGE competitiveperformancewiththevideo-onlymod-
score, but the content is unrelated which leads to els(describedbelow)butdegradesnoticeablythan
0.06 outputsvary.
Human
avg:33.1, std:9.6
0.05 Ground-truth+Action Feat (8) 6 Conclusions
avg:30.0, std:8.9
Ground-truth Transcript (5a)
0.04 avg:30.0, std:8.9 We present several baseline models for generat-
ASR output+Action Feat (9) ingabstractivetextsummariesfortheopen-domain
avg:29.2, std:7.9
0.03 videosinHow2data. Ourpresentedmodelsinclude
ASR output (5c)
avg:29.2, std:7.9 avideo-onlysummarizationmodelthatperforms
0.02 First 200 (4)
avg:29.3, std:7.6 competitivelywithatext-onlymodel. Inthefuture,
Action only (6) wewouldliketoextendthisworktogeneratemulti-
0.01 avg:29.0, std:7.3
document(multi-video)summariesandalsobuild
end-to-endmodelsdirectlyfromaudiointhevideo
0.00
0 20 40 60 80 100 120 140
insteadoftext-basedoutputfrompretrainedASR.
Number of Words
We define and show the quality of a new metric,
Figure3: Worddistributionincomparisonwiththehu-
ContentF1,forevaluationofthevideosummaries
mansummariesfordifferentunimodalandmultimodal
thataredesignedasteasersorhighlightsforview-
models. Density curves show the length distributions
ofhumanannotatedandsystemproducedsummaries. ers,insteadofacondensedversionoftheinputlike
traditionaltextsummaries.
ground-truth transcription summarization model. Acknowledgements
ThisisasexpectedduetothelargemarginofASR
Thisworkwasmostlyconductedatthe2018Fred-
errorsindistant-microphoneopen-domainspeech
erick Jelinek Memorial Summer Workshop on
recognition.
SpeechandLanguageTechnologies,1 hostedand
Wetrainedtwovideo-onlymodels: thefirstone
sponsored by Johns Hopkins University. Shruti
usesasinglemean-pooledfeaturevectorrepresen-
PalaskarreceivedfundingfromFacebookandAma-
tation for the entire video, while the second one
zon grants. Jindˇrich Libovický received funding
appliesasinglelayerRNNoverthevectorsintime.
fromtheCzechScienceFoundation,grantno. 19-
Note that using only the action features in input
26934X.ThisworkusedtheExtremeScienceand
reachesalmostcompetitiveROUGEandContent
EngineeringDiscoveryEnvironment(XSEDE)sup-
F1scorescomparedtothetext-onlymodelshow-
portedbyNSFgrantACI-1548562andtheBridges
ingtheimportanceofbothmodalitiesinthistask.
systemsupportedbyNSFawardACI-1445606,at
Finally,thehierarchicalattentionmodelthatcom-
thePittsburghSupercomputingCenter.
binesbothmodalitiesobtainsthehighestscore.
InTable2,wereporthumanevaluationscoreson
ourbesttext-only,video-onlyandmultimodalmod- References
els. Inthreeevaluationmeasures,themultimodal
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
models with the hierarchical attention reach the Bengio. 2014. Neural machine translation by
bestscores. Modelhyperparametersettings,atten- jointly learning to align and translate. CoRR,
abs/1409.0473.
tionanalysisandexampleoutputsforthemodels
describedaboveareavailableintheAppendix.
SatanjeevBanerjeeandAlonLavie.2005. Meteor: An
InFigure3,weanalyzetheworddistributionsof automatic metric for mt evaluation with improved
differentsystemgeneratedsummarieswiththehu- correlation with human judgments. In Proceedings
oftheaclworkshoponintrinsicandextrinsicevalu-
manannotatedreference. Thedensitycurvesshow
ationmeasuresformachinetranslationand/orsum-
thatmostmodeloutputsareshorterthanhumanan- marization,pages65–72.
notationswiththeaction-onlymodel(6)beingthe
David M Blei, Andrew Y Ng, and Michael I Jordan.
shortestasexpected. Interestingly,thetwodifferent
2003. Latent dirichlet allocation. Journal of ma-
uni-modal and multimodal systems with ground-
chineLearningresearch,3(Jan):993–1022.
truth text and ASR output text features are very
similar in length showing that the improvements Ozan Caglayan, Mercedes García-Martínez, Adrien
Bardet,WalidAransa,FethiBougares,andLoïcBar-
inRouge-LandContent-F1scoresstemfromthe
rault.2017. Nmtpy: Aflexibletoolkitforadvanced
differenceincontentratherthanlength. Example
presentedinTableA2SectionA.3showshowthe 1https://www.clsp.jhu.edu/workshops/18-workshop/
ytisneD
neural machine translation systems. The Prague Will Kay, Joao Carreira, Karen Simonyan, Brian
BulletinofMathematicalLinguistics,109:15–28. Zhang, Chloe Hillier, Sudheendra Vijaya-
narasimhan, Fabio Viola, Tim Green, Trevor
ZiqiangCao,FuruWei,LiDong,SujianLi,andMing Back,PaulNatsev,etal.2017. Thekineticshuman
Zhou. 2015. Ranking with recursive neural net- actionvideodataset. CoRR.
worksanditsapplicationtomulti-documentsumma-
rization. InTwenty-ninthAAAIconferenceonartifi- Diederik P. Kingma and Jimmy Ba. 2014. Adam:
cialintelligence. A method for stochastic optimization. CoRR,
abs/1412.6980.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
JulianKupiec,JanPedersen,andFrancineChen.1995.
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
A trainable document summarizer. In Proceedings
Schwenk, and Yoshua Bengio. 2014. Learning
ofthe18thannualinternationalACMSIGIRconfer-
phrase representations using rnn encoder–decoder
ence on Research and development in information
forstatisticalmachinetranslation. InProceedingsof
retrieval,pages68–73.ACM.
the2014ConferenceonEmpiricalMethodsinNatu-
ralLanguageProcessing(EMNLP). Adrien Le Franc, Eric Riebling, Julien Karadayi,
W Yun, Camila Scaff, Florian Metze, and Alejand-
P. Das, C. Xu, R. F. Doell, and J. J. Corso. 2013. A rina Cristia. 2018. The aclew divime: An easy-to-
thousand frames in just a few words: Lingual de- use diarization tool. In Interspeech, pages 1383–
scription of videos through latent topics and sparse 1387.Interspeech,ISCA.
objectstitching. InProceedingsofIEEEConference
onComputerVisionandPatternRecognition. Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, and
Chengqing Zong. 2017. Multi-modal summariza-
Michael Denkowski and Alon Lavie. 2014. Meteor tion for asynchronous collection of text, image, au-
universal: Language specific translation evaluation dio and video. In Proceedings of the 2017 Con-
foranytargetlanguage. InProceedingsoftheninth ferenceonEmpiricalMethodsinNaturalLanguage
workshop on statistical machine translation, pages Processing,pages1092–1102.
376–380. Association for Computational Linguis-
JindˇrichLibovickýandJindˇrichHelcl.2017. Attention
tics.
strategies for multi-source sequence-to-sequence
learning. In Proceedings of the 55th Annual Meet-
Spandana Gella, Mike Lewis, and Marcus Rohrbach.
ingoftheAssociationforComputationalLinguistics
2018. Adatasetfortellingthestoriesofsocialmedia
(Volume2: ShortPapers),pages196–202.
videos. In Proceedings of the 2018 Conference on
EmpiricalMethodsinNaturalLanguageProcessing,
Chin-YewLinandEduardHovy.2002. Fromsingleto
pages968–974.
multi-document summarization. In Proceedings of
40thAnnualMeetingoftheAssociationforCompu-
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
tationalLinguistics,pages457–464.
Mark Kantrowitz. 2000. Multi-document summa-
rization by sentence extraction. In Proceedings Chin-Yew Lin and Franz Josef Och. 2004. Auto-
of the 2000 NAACL-ANLP Workshop on Automatic matic evaluation of machine translation quality us-
summarization, pages 40–48. Association for Com- ing longest common subsequence and skip-bigram
putationalLinguistics. statistics. InProceedingsofthe42ndMeetingofthe
Association for Computational Linguistics, pages
Max Grusky, Mor Naaman, and Yoav Artzi. 2018. 605–612. Association for Computational Linguis-
Newsroom:Adatasetof1.3millionsummarieswith tics.
diverseextractivestrategies. CoRR.
Zheng Lu and Kristen Grauman. 2013. Story-driven
Çaglar Gülçehre, Sungjin Ahn, Ramesh Nallapati, summarizationforegocentricvideo. InProceedings
Bowen Zhou, and Yoshua Bengio. 2016. Pointing oftheIEEEConferenceonComputerVisionandPat-
theunknownwords. InProceedingsofthe54thAn- ternRecognition,pages2714–2721.
nual Meeting of the Association for Computational
HansPeter Luhn.1958. The automaticcreationof lit-
Linguistics,ACL2016,Volume1: LongPapers.
erature abstracts. IBM Journal of research and de-
velopment,2(2):159–165.
MichaelGygli,HelmutGrabner,HaykoRiemenschnei-
der, and Luc Van Gool. 2014. Creating summaries InderjeetMani.1999. Advancesinautomatictextsum-
from user videos. In European conference on com- marization. MITpress.
putervision,pages505–520.Springer.
YajieMiao,MohammadGowayyed,andFlorianMetze.
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. 2015. Eesen: End-to-end speech recognition us-
2018. Can spatiotemporal 3d cnns retrace the his- ing deep rnn models and wfst-based decoding. In
tory of 2d cnns and imagenet? In Proceedings of Automatic Speech Recognition and Understanding
the IEEE Conference on Computer Vision and Pat- (ASRU), 2015 IEEE Workshop on, pages 167–174.
ternRecognition(CVPR),pages6546–6555. IEEE.
Arthur G Money and Harry Agius. 2008. Video sum- AbigailSee,PeterJ.Liu,andChristopherD.Manning.
marisation: A conceptual framework and survey of 2017. Gettothepoint: Summarizationwithpointer-
the state of the art. Journal of Visual Communica- generatornetworks. InProceedingsofthe55thAn-
tionandImageRepresentation,19(2):121–143. nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1073–
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, 1083.
Ça glar Gulçehre, and Bing Xiang. 2016. Abstrac-
tivetextsummarizationusingsequence-to-sequence Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
rnnsandbeyond. CoNLL2016,page280. dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Vale-
Shashi Narayan, Shay B Cohen, and Mirella Lapata.
rioMiceliBarone,JozefMokry,andMariaNadejde.
2018. Ranking sentences for extractive summariza-
2017. Nematus: a toolkit for neural machine trans-
tionwithreinforcementlearning. CoRR.
lation. In Proceedings of the Software Demonstra-
tionsofthe15thConferenceoftheEuropeanChap-
Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne
ter of the Association for Computational Linguis-
Heikkilä, and Naokazu Yokoya. 2016. Learning
tics, pages 65–68. Association for Computational
joint representations of videos and sentences with
Linguistics.
webimagesearch. InEuropeanConferenceonCom-
puterVision,pages651–667.Springer.
Gunnar A. Sigurdsson, Gül Varol, Xiaolong Wang,
AliFarhadi,IvanLaptev,andAbhinavGupta.2016.
Vijayaditya Peddinti, Guoguo Chen, Vimal Manohar,
Hollywood in homes: Crowdsourcing data collec-
Tom Ko, Daniel Povey, and Sanjeev Khudanpur.
tion for activity understanding. In European Con-
2015. Jhu aspire system: Robust lvcsr with tdnns,
ferenceonComputerVision.
ivectoradaptationandrnn-lms. InAutomaticSpeech
RecognitionandUnderstanding(ASRU),2015IEEE
Workshopon,pages539–546.IEEE. Jingkuan Song, Yi Yang, Zi Huang, Heng Tao Shen,
and Richang Hong. 2011. Multiple feature hash-
MichaelaRegneri,MarcusRohrbach,DominikusWet- ing for real-time large scale near-duplicate video
zel, Stefan Thater, Bernt Schiele, and Manfred retrieval. In Proceedings of the 19th ACM inter-
Pinkal. 2013. Grounding action descriptions in nationalconferenceonMultimedia,pages423–432.
videos. TACL,1:25–36. ACM.
Anna Rohrbach, Marcus Rohrbach, Wei Qiu, An- Yale Song, Jordi Vallmitjana, Amanda Stent, and Ale-
nemarie Friedrich, Manfred Pinkal, and Bernt jandro Jaimes. 2015. Tvsum: Summarizing web
Schiele. 2014. Coherent multi-sentence video de- videosusingtitles. InProceedingsoftheIEEEcon-
scription with variable level of detail. In Pat- ferenceoncomputervisionandpatternrecognition,
ternRecognition-36thGermanConference,GCPR pages5179–5187.
2014,pages184–195.
MdArafatSultan,StevenBethard,andTamaraSumner.
AlexanderM.Rush,SumitChopra,andJasonWeston. 2014. Back to basics for monolingual alignment:
2015. A neural attention model for abstractive sen- Exploiting word similarity and contextual evidence.
tence summarization. In Proceedings of the 2015 Transactions of the Association for Computational
Conference on Empirical Methods in Natural Lan- Linguistics,2:219–230.
guage Processing, pages 379–389. Association for
ComputationalLinguistics.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
Shagan Sah, Sourabh Kulhare, Allison Gray, Sub-
networks. In Proceedings of the 28th International
hashini Venugopalan, Emily Prud’Hommeaux, and
ConferenceonMachineLearning(ICML-11),pages
Raymond Ptucha. 2017. Semantic text summariza-
1017–1024.JMLR.org.
tion of long videos. In Applications of Computer
Vision (WACV), 2017 IEEE Winter Conference on,
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
pages989–997.IEEE.
Sequencetosequencelearningwithneuralnetworks.
In Advances in Neural Information Processing Sys-
Ramon Sanabria, Ozan Caglayan, Shruti Palaskar,
tems27,pages3104–3112.CurranAssociates,Inc.
Desmond Elliott, Loïc Barrault, Lucia Specia, and
Florian Metze. 2018. How2: a large-scale dataset
formultimodallanguageunderstanding. InProceed- Atousa Torabi, Niket Tandon, and Leonid Sigal.
ingsoftheWorkshoponVisuallyGroundedInterac- 2016. Learning language-visual embedding for
tionandLanguage(ViGIL).NIPS. movieunderstandingwithnatural-language. CoRR,
abs/1609.08124.
Ramon Sanabria, Shruti Palaskar, and Florian Metze.
2019. Cmu sinbad’s submission for the dstc7 avsd Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
challenge. In Proc. 7th Dialog System Technology 2015. Pointer networks. In Advances in Neural
Challenges Workshop at AAAI, Honolulu, Hawaii, Information Processing Systems, pages 2692–2700.
USA. CurranAssociates,Inc.
Meng Wang, Richang Hong, Guangda Li, Zheng-Jun Set Words
Zha, Shuicheng Yan, and Tat-Seng Chua. 2012.
Eventdrivenwebvideosummarizationbytaglocal- Transcript the,to,and,you,a,it,that,of,is,i,
ization and key-shot identification. IEEE Transac- going,we,in,your,this,’s,so,on
tionsonMultimedia,14(4):975–985.
Summary in, a, this, to, free, the, video, and,
learn,from,on,with,how,tips,for,
KristianWoodsendandMirellaLapata.2012. Multiple
of,expert,an
aspect summarization using integer linear program-
ming. InProceedingsofthe2012JointConference
onEmpiricalMethodsinNaturalLanguageProcess- Table A1: Most frequently occurring words in Tran-
ingandComputationalNaturalLanguageLearning, scriptandSummaries.
pages233–243.
Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, A.2 FrequentWordsinTranscriptsand
AyushPareek,KrishnanSrinivasan,andDragomirR. Summaries
Radev. 2017. Graph-based neural multi-document
summarization. In Proceedings of the 21st Confer- TableA1showsthefrequentwordsintranscripts
enceonComputationalNaturalLanguageLearning (input) and summaries (output). The words in
(CoNLL2017),pages452–462.
transcriptsreflectconversationalandspontaneous
speech while words in the summary reflect their
Kuo-Hao Zeng, Tseng-Hung Chen, Juan Carlos
descriptivenature.
Niebles, and Min Sun. 2016. Generation for user
generated videos. In European conference on com-
putervision,pages609–625.Springer. A.3 OutputExamplesfromDifferentModels
TableA2showsexampleoutputsfromourdifferent
Jianguo Zhang, Pengcheng Zou, Zhao Li, Yao Wan,
text-onlyandtext-and-videomodels. Thetext-only
Ye Liu, Xiuming Pan, Yu Gong, and Philip S Yu.
2018. Product title refinement via multi-modal model produces a fluent output which is close to
generative adversarial learning. arXiv preprint the reference. The action features with the RNN
arXiv:1811.04498.
model,whichseesnotextintheinput,producesan
in-domain(“flytying”’and“fishing”)abstractive
LuoweiZhou,ChenliangXu,andJasonJ.Corso.2018.
summary that involves more details like “equip-
Towardsautomaticlearningofproceduresfromweb
instructional videos. In Proceedings of the Thirty- ment”whichismissingfromthetext-basedmodels
Second AAAI Conference on Artificial Intelligence, butisrelevant. TheactionfeatureswithoutRNN
(AAAI-18),pages7590–7598. modelbelongstotherelevantdomainbutcontains
fewer details. The nearest neighbor model is re-
A Appendix lated to “knot tying” but not related to “fishing”.
The scores for each of these models reflect their
A.1 ExperimentalSetup
respectiveproperties. Therandombaselineoutput
In all our experiments, the text encoder consists showstheoutputofsamplingfromtherandomlan-
of 2 bidirectional layers of the encoder with 256 guagemodelbasedbaseline. Althoughitisafluent
GatedRecurrentUnits(GRU;Choetal.2014)and output, the content is incorrect. Observing other
2 layers of the decoder with Conditional Gated outputsofthemodelwenoticedthatalthoughpre-
Recurrent Units (CGRU; Sennrich et al. 2017). dictionswereusuallyfluentleadingtohighscores,
WeoptimizethemodelswiththeAdamOptimizer there is scope to improve them by predicting all
(KingmaandBa,2014)withlearningrate4·10−4 details from the ground truth summary, like the
halvedaftereachepochwhenthevalidationperfor- subtlesellingpointphrases,orbyusingthevisual
mancedoesnotincreaseformaximum50epochs. featuresinadifferentadaptationmodel.
Werestricttheinputlengthto600tokensforall
A.4 AttentionAnalysis
experimentsexceptthebesttext-onlymodelinthe
section Experiments and Results. We use vocab- Figure A1 shows an analysis of the attention dis-
ularythe20,000mostfrequentlyoccurringwords tributionsusingthehierarchicalattentionmodelin
which showed best results in our experiments, an example video of painting. The vertical axis
largelyoutperformingmodelsusingsubword-based denotestheoutputsummaryofthemodel,andthe
vocabularies. We ran all experiments with the horizontalaxisdenotestheinputtime-steps(from
nmtpytorchtoolkit(Caglayanetal.,2017). thetranscript). Weobservelessattentioninthefirst
No. Model R-L C-F1 Output
- Reference - - watchandlearnhowtotiethreadtoahooktohelpwithflytyingas
explainedbyoutexpertinthisfreehow-tovideoonflytyingtips
andtechniques.
8 Ground-truthtext+ 54.9 48.9 learnfromourexperthowtoattachthreadtoflyfishingforflyfishing
ActionFeat. inthisfreehow-tovideoonflytyingtipsandtechniques.
5a Text-only(Ground- 53.9 47.4 learnfromourexperthowtotieathreadforflyfishinginthisfree
truth) how-tovideoonflytyingtipsandtechniques.
9 ASR output + Ac- 46.3 34.7 learnhowtotieaflyknotforflyfishinginthisfreehow-tovideoon
tionFeat. flytyingtipsandtechniques.
5c ASRoutput 46.1 34.7 learntipsandtechniquesforflyfishinginthisfreefishingvideoon
techniquesforandmakingflyfishingnymphs.
7 Action Features + 46.3 34.9 learnabouttheequipmentneededforflytying,aswellasotherfly
RNN fishingtipsfromourexpertinthisfreehow-tovideoonflytying
tipsandtechniques.
6 Action Features 38.5 24.8 learnfromourexperthowtodoadoublehalfhitchknotinthisfree
only videoclipabouthowtouseflyfishing.
2b NextNeighbor 31.8 17.9 useasheepshankknottoshortenalongpieceofrope.learnhow
totiesheepshankknotsforshorteningropeinthisfreeknottying
videofromaneaglescout.
1 RandomBaseline 27.5 8.3 learntipsonhowtoplaythebassdrumbeatvariationontheguitar
inthisfreevideocliponmusictheoryandguitarlesson.
TableA2: Exampleoutputsofground-truthtext-and-videowithhierarchicalattention(8),text-onlywithground-
truth (5a), text-only with ASR output (5c), ASR output text-andv-video with hierarchical attention (9), action
features with RNN (7) and action features only (6) models compared with the reference, the topic-based next
neighbor(2b)andrandombaseline(1). Arrangedintheorderofbesttoworstsummaryinthistable.
cut cut
Black frames
at the end
Talking and preparing Close-up of Close-up of
the brush brushstrokes w/ hand brushstrokes no hand
FigureA1: VisualizingAttentionoverVideoFeatures.
partofthevideowherethespeakerisintroducing attentionoverconsecutiveframes. Towardstheend,
the task and preparing the brush. In the middle thecloseupdoesnotcontainthehandbutonlythe
half,thecamerafocusesontheclose-upofbrush paperandbrush,wherethemodelagainpaysless
strokeswithhand,towhichthemodelpayshigher attentionwhichcouldbeduetounrecognizedac-
tionsintheclose-up. Thereareblackframesinthe
veryendofthevideowherethemodellearnsnot
to pay any attention. In the middle of the video,
therearetwoplaceswithacutinthevideowhen
thecamerashiftsangle. Themodelhaslearnedto
identify these areas and uses it effectively. From
this particular example, we see the model using
bothmodalitiesveryeffectivelyinthistaskofthe
summarizationofopen-domainvideos.
A.5 HumanEvaluationDetails
To understand the outputs generated for this task
better, we ask workers on Amazon Mechanical
Turk to compare outputs of unimodal and multi-
modalmodelswiththeground-truthsummaryand
assignascorebetween1(lowest)and5(highest)
for four metrics: informativeness, relevance, co-
herence and fluency of generated summary. The
annotatorswereshowntheground-truthsummary
andacandidatesummary(withoutknowledgeof
thetypeofmodalityusedtogenerateit). Eachex-
amplewasannotatedbythreeworkers. Annotation
wasrestrictedtoEnglishspeakingcountries. 129
annotatorsparticipatedinthistask.

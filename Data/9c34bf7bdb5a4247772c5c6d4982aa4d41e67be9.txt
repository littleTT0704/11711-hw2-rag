Event Nugget Annotation: Processes and Issues
Teruko Mitamura, Yukari Yamakawa, Susan Holm
Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA
teruko@cs.cmu.edu, {yukariy, sh4s}@andrew.cmu.edu
Zhiyi Song, Ann Bies, Seth Kulick, Stephanie Strassel
Linguistic Data Consortium, University of Pennsylvania, Philadelphia PA
{zhiyi, bies, skulick, strassel}@ldc.upenn.edu
Abstract ports the TAC KBP pilot evaluation for Event
This paper describes the processes and issues Nugget Detection as part of the DEFT program.
of annotating event nuggets based on DEFT In this paper, we introduce the notion of event
ERE Annotation Guidelines v1.3 and TAC nugget and how event nuggets are annotated in the
KBP Event Detection Annotation Guidelines
corpus. We discuss the issues that arose in the pro-
1.7. Using Brat Rapid Annotation Tool (brat),
cess of developing TAC KBP Event Guidelines,
newswire and discussion forum documents
because they are important challenges for manual
were annotated. One of the challenges arising
annotation and impact the quality of annotation for
from human annotation of documents is anno-
gold standard creation. Two major issues are (1)
tators’ disagreement about the way of tagging
events. We propose using Event Nuggets to determining if an event meets the event
help meet the definitions of the specific type/subtype definitions and (2) deciding which
type/subtypes which are part of this project. words should be tagged within the span of a multi-
We present case studies of several examples word event nugget that represents a single event.
of event annotation issues, including discon- We provide screen images of our annotation tool in
tinuous multi-word events representing single
order to give a complete picture of the annotation
events. Annotation statistics and consistency
process. Finally, we present statistics to explain the
analysis is provided to characterize the inter-
characteristics of the corpus, such as the size of the
annotator agreement, considering single term
corpus and the distribution of event type/subtypes.
events and multi-word events which are both
We discuss consistency analysis of inter-annotator
continuous and discontinuous. Consistency
analysis is conducted using a scorer to com- agreement in terms of single word, multi-word
pare first pass annotated files against adjudi- continuous, and multi-word discontinuous event
cated files. nuggets.
1 Introduction 2 What is an Event Nugget?
Annotating event mentions is useful for event de- It is challenging to provide clear-cut definitions of
tection tasks. It also is useful for detecting event events, because many researchers define events
coreference, subevent relations, event arguments, differently. For example, in the Light ERE annota-
and realis values in corpora. This paper describes tions, as well as in ACE, Automatic Content Ex-
the processes and issues of annotating event nug- traction) English Annotation Guidelines for Events
gets based on the DEFT ERE Annotation Guide- (LDC, 2005), an event is defined as an explicit oc-
lines v1.3 (LDC, 2014) (henceforth referred to as currence involving participants. An event is some-
Light ERE Guidelines) and the TAC KBP Event thing that happens at a particular place and time,
Detection Annotation Guidelines v1.7 (LTI, 2014) and it can frequently be described as a change of
(henceforth referred to as TAC KBP Event Guide- state. The Light ERE Guidelines expect annotators
lines). Using the Brat Rapid Annotation Tool to tag an event trigger, which is the smallest extent
(brat)1, we annotated files in newswire and discus- of text that expresses the occurrence of an event.
sion forums genres to create the corpus that sup- Both ACE and Light ERE, only examples of a par-
ticular set of types/subtypes are tagged. An event
trigger is usually a word or phrase. In many cases,
1 Brat Rapid Annotation Tool (brat) was developed by Pontus
Stenetorp et al. (2014). It is a web-based annotation tool. event triggers are main verbs in sentences that in-
66
Proceedingsofthe3rdWorkshoponEVENTSattheNAACL-HLT2015,pages66–76,
Denver,Colorado,June4,2015.(cid:13)c2015AssociationforComputationalLinguistics
dicate the occurrence of the events. Annotating a belong to a multiword event nugget because they
main verb is relatively easy and is likely to pro- are all required to meet the definition, such as “The
duce a higher rate of inter-annotator agreement, company laid 10 workers off,” and “His death
because it allows annotators to pay more attention sentence was carried out.”
to a syntactic attribute of an event as well as its Discontinuous tagging is very effective because
semantic feature. However, event triggers are not it can be used to prevent violations of rules for an-
just verbs. Some nouns and adjectives can also ex- notation. For example, TAC KBP Event Guidelines
press events (See examples in Section 3.1.). as well as Light ERE Guidelines mention that non-
In this study, we took a different approach to main verbs should not be tagged. In sentences such
event annotations so that we would be able to an- as “His death sentence was carried out,” annotators
notate more complex events, which consist of mul- may want to tag “death sentence was carried out”
tiple words taggable as events. For this reason, we to meet the definition of Justice_Execute events,
decided to take a semantically oriented approach since carrying out a death sentence means execut-
for annotation. New annotation guidelines were ing someone. However, tagging “was” violates the
produced (TAC KBP Event Guidelines), based on rule that non-main verbs are not taggable. In this
the Light ERE Guidelines and ACE. To clarify the case, tagging “death sentence” and “carried out”
tagging of multiword events, we propose the idea together as a discontinuous multiword event nug-
of “event nugget,” which is comprised of a seman- get not only meets the definition of Jus-
tically meaningful unit that expresses the event in a tice_Execute events but also does not violate the
sentence. An event nugget can be either a single rule that “be” verbs should not be tagged.
word (main verb, noun, adjective, adverb) or a The merits of event nugget annotation are
continuous or discontinuous multi-word phrase. summarized as follows: identification of events in
The main reason why we propose event nugget a more semantically meaningful way and flexible
annotation is to identify events accurately enough annotation without violating annotation rules. In
to meet the definitions of event types/subtypes in the next section, we present examples of event
the Light ERE Guidelines. The type/subtype defini- nuggets, using the following format to indicate the
tions restrict annotation to very specific types of annotation: [Event Type_Subtype, REALIS]. Real-
events. Figuring out which events fall within the is will be discussed in Section 3.3.
type/subtype definitions is a key issue to annota-
tion. In the process of annotation, we have encoun- 3 Types of Event Nuggets and REALIS
tered cases in which multiple words could equally
3.1 Single-Word Event Nuggets
be considered as an event trigger. In many cases
the multiple words are hard to separate from one
As in ACE and Light ERE annotation, single-word
another in terms of meaning (e.g., “hold a meet-
event nuggets meet the definitions of event triggers
ing”, “serve a sentence”, “send email”). Thus, we
for particular types/subtypes. Slightly modified in
decided to annotate the maximum extent of text
TAC KBP Event Guidelines, single-word event
which meets the definition of the event
nuggets refer to words that meet the definitions of
types/subtypes provided by the Light ERE Guide-
event types/subtypes by themselves. They are
lines. This approach allows annotators to tag all
verbs (usually main verbs), nouns, adjectives, or
possible words that meet the definition of the event
adverbs. Below are some examples of single-word
types/subtypes.
event nuggets. The words in bold face are event
In addition to the annotation of the maximum
nuggets.
extent of events, discontinuous tagging is another
characteristic of event nugget annotation. (In order • The attack by insurgents occurred on Satur-
to clarify which words are in the same event nug- day. [Conflict_Attack, ACTUAL]
get in this paper, we underline from the first word • Hillary Clinton was not elected president in
in a discontinuous multiword event nugget to the 2008. [Personnel_Elect, OTHER]
last word in the nugget. A dotted underline appears
There are some cases where multiple single-word
under words that are not part of the event nugget.)
event nuggets appear in the same sentence.
Discontinuous tagging allows annotators to tag
words that do not lie next to each other but still
67
• Kennedy was shot dead by Oswald. [Con- For example, consider the definition of Jus-
flict_Attack, ACTUAL], [Life_Die, ACTUAL] tice_Sue: “A SUE event occurs whenever a court
• Three years ago, investors bought two stag- proceeding has been initiated for the purposes of
nant web-hosting companies and merged them determining the liability of a PERSON,
into what is now known as The Planet. [Trans-
ORGANIZATION or GPE accused of committing
action_Transfer-Ownership, ACTUAL],
a crime or neglecting a commitment.” The three
[Business_Merge-Org, ACTUAL]
examples below illustrate event nuggets for Jus-
Pronouns and other anaphors are also considered tice_Sue events. (For clarification, strikethrough
as single-word event nuggets if they refer to previ- denotes an event that is not part of the event nugget
ous event mentions that meet the definitions of being illustrated.)
event types/subtypes.
• His lawyer should file a lawsuit. [Justice_Sue,
• The talks between the Koreas were largely un- OTHER]
successful. They ended without agreement on • His lawyer should sue. [Justice_Sue, OTHER]
Monday. [Contact_Communicate, ACTUAL], • His lawyer should contest the lawsuit. [Jus-
[Contact_Communicate, ACTUAL] tice_Sue, OTHER]
3.2 Complex (Multi-Word) Event Nuggets The noun+verb combination of “file” and “law-
suit” meet the definition of Justice_Sue as a court
Complex event nuggets are multi-word phrases (or
proceeding having been initiated. A lawsuit is a
compounds) that construct semantic units that meet
court proceeding, and filing refers to its initiation,
the definitions of event types/subtypes. Those units
which is a part of the court proceeding. The two
can be continuous or discontinuous. Multi-word
words in combination express the “doing” of the
event nuggets take various forms such as
SUE event and meet the definition of Justice_Sue.
verb+noun, verb+particle/adverb, noun+noun, and
The single verb “sue” can also be used to meet this
so on. The words underlined and in bold face are
definition, as can the single noun “lawsuit”. How-
multi-word event nuggets that represent a single
ever in the third sentence, “contest” is separate
event.
from the lawsuit event and does not belong to the
event nugget. To contest a lawsuit is an action of
• Foo Company had filed Chapter 11 in 2000.
the defense team in response to an existing lawsuit.
[Business_Declare-Bankruptcy, ACTUAL]
There is currently no Justice Subtype defined in the
• The police investigated the murder incident.
[Conflict_Attack, ACTUAL] Light ERE Guidelines to fit this contest event.
Discontinuous tagging is one of the characteris- 3.3 REALIS
tics of annotation of multi-word event nuggets.
In our annotation, event nuggets are annotated with
This type of tagging is useful because it captures
three types of REALIS: ACTUAL, GENERIC, and
event nuggets accurately without missing im-
OTHER. REALIS relates to whether or not an
portant components of meaning. Below are the
event occurred (LTI, 2014).
examples of discontinuous tagging of multi-word
The REALIS of ACTUAL is used when the
event nuggets.
event actually happened at a particular place and
• The court found him guilty. [Justice_Convict, time, involving specific entities. Both ongoing
ACTUAL] events and events that have ended are tagged
• His death sentence was carried out. [Jus- ACTUAL. For example, “He emailed her about
tice_Execute, ACTUAL] their plans [Contact_Communicate, ACTUAL].”
• All charges were dropped against him last The REALIS of GENERIC is used for events
year. [Justice_Acquit, ACTUAL]
that refer to general events involving types or cate-
gories of entities. GENERIC is also used for tag-
Multi-word event nuggets that represent single
gable event nuggets which appear in statistics or
events are tagged either continuously or discontin-
demographic information. For example, “People
uously depending on the particular construction of
die [Life_Die, GENERIC].”
the semantic units that meet the definitions of the
event types/subtypes in each sentence.
68
The REALIS of OTHER will be used for events (e.g., “in prison”, “behind bars”), which seem to
that are neither ACTUAL nor GENERIC. If it is meet the definitions of event types/subtypes. The
determined that an event meets the definition of a third case involves annotating nouns that refer to
type/subtype and it is not an ACTUAL or the consequences or results of events (e.g., “inju-
GENERIC event, it can simply be tagged OTHER. ry”, “body”, “funeral”), which could be considered
For example, “He plans to meet with both political as either an entity or an event by individual annota-
parties [Contact_Meet, OTHER].” tors. The fourth case occurs when only a portion of
In the case of GENERIC events which also a word indicates an event (e.g., “antiwar”, “post-
qualify as OTHER (e.g., negated generic) or war”, “ex-husband”, “ex-wife”). The last case is
ACTUAL (e.g., past generic, habitual generic), discontinuous tagging of event nuggets. Although
GENERIC is used, not OTHER or ACTUAL. discontinuous tagging is effective for capturing the
semantically meaningful unit of event nuggets, the
4 Event Types/Subtypes consistency (See Table 5) of discontinues event
nuggets is not as good as singe token event nugget.
The TAC KBP Event Guidelines and the Light ERE
In the case studies below, the words in italic
Guidelines share the same 33 event types/subtypes
bold are controversial or in issue.
in particular areas, such as Life, Movement, Busi-
ness, Conflict, Personnel, Transaction, and Justice, Case Study 1: Is a person an event?
which were originated in the ACE Guidelines
(LDC, 2005). • Two other assailants have committed suicide.
The complete set of event types/subtypes is: • Here is the KICKER: As reported by local
news stations DOZENs of protestors showed
Life (Be-Born, Marry, Divorce, Injure, Die),
up to protest.
Movement (Transport-Person), Business (Start-
• On the grounds of legality, according to the
Org, End-Org, Declare-Bankruptcy, Merge-Org),
Geneva Convention, members of regular armed
Conflict (Demonstrate, Attack), Contact (Meet,
forces – involved in conflicts – are the only
Communicate), Personnel (Start-Position, End-
persons who may be considered lawful com-
Position, Nominate, Elect), Transaction (Transfer- batants and authorized to use lethal force.
Ownership, Transfer-Money), Justice (Arrest-Jail,
Release-Parole, Trial-Hearing, Charge-Indict, Sue, The words such as “assailants”, “protesters”, and
Convict, Sentence, Fine, Execute, Extradite, Ac- “combatants” imply the occurrence of events, as
quit, Appeal, Pardon). we can see by paraphrasing them as “a person who
assailed (assaulted) someone,” “people who are
• John Doe was born in Casper, WY. [Life_Be- protesting,” and “people who combat.” If annota-
Born, ACTUAL] tors take the implied occurrences into considera-
• Roosevelt and his family immediately depart- tion, those words will be tagged as event nuggets.
ed for Buffalo. [Movement_Transport-Person,
However, those words actually refer to the “peo-
ACTUAL]
ple” themselves. People are not events. Tagging
• A car bomb exploded in central Baghdad.
them as events means that we tag implied events.
[Conflict_Attack, ACTUAL]
In a similar fashion, some annotators may be
5 Annotation Challenges tempted to tag “the dead” as an event nugget, but
others do not because they think that “the dead”
One of the main challenges in the development of refers to dead people. It is critical for annotators to
annotation guidelines is that there is always some consider the implications of implied events when
disagreement about what should (not) be taggable. they tag. If implied events are to be tagged, rules
In this section, we present some examples of disa- should be explicitly stated to guide annotators as to
greements, which we experienced in the process of which implied events should be tagged, and which
developing annotation guidelines, as case studies. implied events should not be tagged.
The first case is related to annotating implied Case Study 2: Is a prepositional phrase taggable?
events which are contained within nouns referring
to persons (e.g., “protestor”, “assailant”, “killer”). • A former militant of the French far-left group
Action Directe, Georges Cipriani, left prison
The second case concerns prepositional phrases
69
on parole on Wednesday after 23 years behind The decision on whether a portion of a word
bars for two high-profile murders. should be tagged also causes disagreement among
• Prosecutors have said Chen could face life in annotators. Some annotators may think it not ap-
prison if convicted on all counts, including propriate to break a word into chunks, or others
embezzlement and bribe-taking.”
may tag a part of a word only if it is hyphenated.
This case study raises the issue on how events are
The phrases “behind bars” and “in prison” indicate
defined in relation to word level structure. Seman-
that the agent was (or would be) imprisoned and
tically, both “war” and “ex” meet the definitions of
could be tagged as Justice_Arrest-Jail events. They
event types/subtypes. However, it is unclear
are, however, prepositional phrases that describe a
whether the entire word (“postwar”, “antiwar”,
certain state (i.e. the state of physically residing in
“ex-wife”) should be tagged. Is “antiwar” a Con-
a particular place). There is some debate whether
flict_Attack event, for instance? It is necessary to
or not states are taggable as events. Especially in
have a clear rule for this type of tagging.
the case of prepositional phrases, it is difficult for
annotators to decide whether those phrases should
Case Study 5: Tagging Discontinuous Multiword
be tagged, since they could be considered to refer
Event Nuggets
to states and sound less eventive.
Case Study 3: Is it an event or the result of an In our corpus with 3,798 event nuggets, there were
event? 209 discontinuous nuggets, a ratio of 5.5%. The
• Why was Trayvon’s body laying 12 hours in discontinuous event nuggets appear in various
the Morgue? forms such as verb+noun, verb+particle/adverb,
• A cry for the men to be hanged went up almost verb+adjective, and verb+prepositional phrase.
immediately after the woman died of her inju- Among those patterns, the most frequent one is a
ries, … verb+noun compound (83%), where a noun is the
• And those already existing time place and direct object of the verb. This pattern appears in a
manner restrictions were utilized at Matthew
passive form as well.
Snyder’s funeral, with the result that the fami-
ly never even knew WBC was there.
• today I got a letter from the hospital [Con-
tact_Communicate, ACTUAL]
The words in italic bold indicate the consequence
• where was the father when the shot was fired
or result of certain events. For example, the type of
not more than a 1000 feet away? [Con-
“body” referred to in the first example only exists
flict_Attack, ACTUAL]
after a Life_Die event has occurred. “Injuries” ex-
ist on or in a person’s body after (s)he has experi- These discontinuous events are tagged because
enced a Life_Injure or Conflict_Attack event. A multiple words in the sentence are important se-
“funeral” is a ceremony that occurs after a mantic components of their event type/subtype
Life_Die event has happened. Since “body”, “inju- definitions. For example, the word, “get” is used to
ries” and “funeral” are words that are closely relat- create various event types such as “get money”
ed to taggable event types/subtypes, annotators (Transaction_Transfer-Money) and “get a job”
may be tempted to tag those words as event nug- (Personnel_Start-Position). Thus, tagging a verb
gets. However, it is necessary to differentiate the and a noun together as one event seems important
consequence/result of an event from an event itself. to differentiate a particular event type from the
others. In the second example, both “shot” and
Case Study 4: Is a portion of a word taggable?
“fired” are taggable as events and it is hard to ig-
nore either of them as not taggable due to the close
• U.N. Secretary General Kofi Annan said this
relationship between the “doing” of an event and
week that the body has no interest in policing a
postwar Iraq, … event itself. A verb+noun compound appears very
• We were so proud of forming an antiwar bloc often in the following event types/subtypes:
with France and Germany … Transaction_Transfer-Money (23%), Con-
• Jurassic Park creator Crichton agrees to pay ex- tact_Communicate (18%), and Conflict_Attack
wife 31 million dollars (10%).
70
Part of speech patterns for discontinuous tag- 7 Data Selection and Preparation
ging include verb+particle/adverb, which is 14% of
the entire discontinuous tagging. This form appears We produced training and evaluation (eval) data to
most often in Movement_Transport-Person (68%). support the Event Nugget evaluation as a pilot
TAC KBP evaluation. The data includes both for-
• …took us in for a interview…[Movement_Tra mal newswire text (NW) and informal discussion
nsport-Person, ACTUAL] forums (DF), drawn from a pool of data also la-
• ... i put the thread up because i really did want beled for the DARPA DEFT Program’s Light Enti-
some opinions…[Contact_Communicate,
ties, Relations and Events (Light ERE) task (Song
ACTUAL]
et al., 2015), and/or the NIST TAC KBP Evalua-
tion Event Argument Task (Ellis et al., 2014), with
Some annotators may only tag main verbs because
the goal of ultimately being able to take advantage
they think adverbs and particles are modifying the
of multiple styles of event annotation on the same
verbs, but others may tag verb+adverb/particles
data. Documents for the current task were carefully
together because they feel that the adverb/particles
selected from this pool to optimize coverage of as
signify a different meaning from just the verbs
many of the event types and subtypes as possible,
alone. As shown Table 5, it is not as easy to con-
with a goal of at least five instances of each type-
sistently annotate multi-word event nuggets as it is
subtype combination. The training data consists of
to consistently annotate single-word event nuggets.
151 documents, while the eval data contains 200
However, the percentage of multi-word event nug-
documents. Table 1 shows the genre distribution as
gets is so low that it may not significantly affect
well as token counts for each partition.
overall event nugget detection performance.
We continue to work on reaching agreement on
Partition Training Eval
the optimal method of handling of these four types
Genre NW DF NW DF
of controversial event nuggets in order to better
represent the deeper semantics of texts. The very Documents 77 74 101 99
low frequency of discontinuous event nuggets does Tokens 44,962 70,427 50,997 169,740
not mean that they should be ignored to achieve Table 1. Event Nugget Data Profile
higher inter-annotator agreement. Clear rules for
these cases should be laid out for future tasks on While the Light ERE and KBP Event Argument
event nugget detection. tasks rely on character offsets for annotation and
scoring, the Event Nugget Tuple Scorer2 (Liu,
6 Brat Rapid Annotation Tool (brat) Mitamura & Hovy, 2015) requires tokenized data.
Therefore, prior to annotation, all selected docu-
Our annotation was conducted using Brat Rapid ments were automatically tokenized in the Penn
Annotation Tool (brat). This tool allows for cus- English Treebank style. No manual correction was
tomization of tags, such as event types/subtypes, performed on the tokenization due to time con-
realis types, types of entities/arguments, types of straints.
event links, and provides a means to add notes for
questionable mentions. In addition, brat supports 8 Corpus and Consistency Analysis
discontinuous tagging and side-by-side comparison
of two files. 8.1 Corpus
The actual procedure of annotation and the re-
Experience with event annotation for Light ERE
view of applied tags are relatively simple with this
and ACE (Doddington et al., 2004) and related
user-friendly application. Clicking on a word to be
tasks suggests that a major challenge for annota-
tagged opens a window where annotators can se-
tion consistency is poor recall – human annotators
lect tags, such as event types/subtypes and realis.
are not highly consistent in recognizing that a men-
After a word has been tagged, when the cursor is
tion has occurred. To reduce the impact of this
moved over the tag, a small box appears, display-
known issue for the Event Nugget task, two anno-
ing the assigned event type and realis for review.
Screenshots of brat are shown in the Appendix.
2 Event Nugget Tuple refers to the tuple made up of the nug-
get, event type/subtype, and realis.
71
tators independently labeled each document (two mean the exact list of tokens, continuous or discon-
first pass annotation passes, referred to as FP1 and tinuous, that make up an event nugget. However,
FP2 below); a senior annotator then adjudicated each system nugget can only be mapped to one
discrepancies to create a gold standard. The team gold nugget. For each gold nugget, the scorer com-
consisted of four first pass annotators, two of putes type and realis accuracy scores based on the
whom were also adjudicators. The effort was made values for the gold nugget and all the system nug-
to ensure that annotators did not adjudicate their gets that are mapped to it.
own first pass files, but due to time constraints and The scorer produces three scores for each file.
the pilot nature of the task, in some cases there was The first is an F-measure for the nugget spans,
overlap. based on the mapping from gold to system nug-
The gold standard training data has 3,798 event gets, as well as ‘‘false alarms” in the system file
nuggets annotated in total, while the eval data has that are not mapped to any nuggets in the gold file.
6,921 event nuggets. Table 2 shows the distribu- The type and realis scores for each gold mention
tion of event nuggets by genre and realis type for are also cumulatively summed up, producing a
each partition. type and realis score for the file. The type and real-
is scores are therefore tied to the F-measure score
Realis Training Eval of the nugget spans. We used this scorer rather
Attribute
than the ACE (NIST, 2005) scorer since this scorer
NW DF NW DF
was designed for the event nugget evaluation task,
Generic 202 383 245 981
and so seemed the most appropriate to use for
Other 346 406 448 1271 evaluation of annotation consistency and quality of
Actual 1313 1132 1752 2224 this corpus.
Total 37983 6921 We examined annotation consistency by com-
paring the two independent first passes of annota-
Table 2. Realis Annotation of Event Nuggets
tion (FP1 and FP2), with the results shown in the
column FP1 vs. FP2 in Table 3. We also evaluated
Figure 1 (in Appendix) shows the distribution
improvement in annotation quality in the workflow
of each type-subtype combination in the training
by comparing the adjudicated (ADJ) and first (FP1
and eval data. Conflict_Attack has the highest rep-
and FP2) passes, shown in the columns ADJ vs.
resentation in both training (579) and eval (791).
FP1 and FP2 in Table 3. The noticeable improve-
Justice_Extradite has the lowest count in training
ment in score shows the advantage of including
data (3), while Life_Be-Born is least frequent in
adjudication as part of the annotation process. (For
the eval data (19). Despite our efforts to manually
IAA purposes, there is obviously no gold or sys-
select documents to maximize coverage for all
tem, but in order to use the scorer we arbitrarily
type-subtype combinations, the corpus does not
treated one file as the “gold”.)
include any occurrences of Business_End-Org or
Personnel_End-Position.
ADJ vs.
FP1 vs. FP2 ADJ vs. FP2
FP1
8.2 Consistency Analysis
Span 69.0 78.2 89.3
We examined annotation consistency and quality Type 68.2 71.7 84.3
by comparing different passes of the eval set anno- Realis 60.0 63.2 85.7
tation using the Event Nugget Tuple Scorer (Liu, Table 3. Scores for Event Nugget Eval Set Annotation
Mitamura, & Hovy, 2015) developed for the event
To gain some further insight into these numbers
nugget evaluation task. This scorer treats one file
we expanded the analysis in two directions. First,
as “gold” and the other as “system”, and matches
we compared the FP1 vs. FP2 event nugget con-
each nugget in the gold file to one or more nuggets
sistency with the FP1 vs. FP2 annotation con-
in the system file. This mapping is based on the
sistency on the ACE 2005 training data (Walker et
overlap of the nugget spans. By nugget span, we
al., 2006). There is also a scorer that was devel-
oped for ACE (NIST, 2005), but we used the Event
Nugget Tuple evaluation scorer so that we could
3 16 event nuggets in the training set did not receive a realis
attribute, due to annotation error. score both sets of data for this comparison as in the
72
event nugget evaluation. This necessitated convert- consist of multiple tokens, whether continuous or
ing the ACE files into the format for event nuggets not. Mode (1) is the same as the score reported for
used for the current scorer. We used the ‘‘anchor’’ the spans in Tables 3 and 4, and modes (2)-(5) in
string of the ACE event mention as the nugget effect break this down into subcomponents. The
span, the ‘‘type’’ and ‘‘subtype’’ of the ACE event results are shown in Table 5. ACE annotation did
mention as the nugget type, and the ‘‘modality’’ of not allow discontinuous multiple token mentions,
the ACE event mention as the nugget realis value. and so there are no results listed for ACE for (4)
The results are shown in Table 4. The ACE FP1 vs. and (5).
FP2 scores in Table 4 are somewhat lower than the The results for the consistency agreement be-
FP1 vs. FP2 scores for the event nugget annotated tween FP1 and FP2 show a similar fall in score for
data. However, while we have converted the for- both the event nugget data and the ACE 2005
mat and used the same scorer, the annotation task training data, when considering only multiple con-
is not identical, so this can only be taken as a rough tinuous tokens. The score climbs back up a little
comparison. There is greater difference between for the event nugget FP1 vs. FP2 score when con-
the ADJ vs. FP1, FP2 scores for the event nugget sidering (5) either continuous or discontinuous
data than the ACE data. The event nugget task had multiple tokens, as compared with either (3) only
a smaller annotation team than for ACE, and it is multiple continuous or (4) only multiple discontin-
likely that more of the adjudication annotators for uous. The reason for this is that there are cases
event nugget annotation also did the FP2 pass than where one file has an event nugget with a continu-
was the case for ACE. ous multiple token span such as “got jail time”
while the other has the corresponding event nugget
FP1 v. FP2 ADJ v.FP1 ADJ v. FP2
with a multiple discontinuous span such as ‘‘got
Span 64.8 79.3 81.8
time’’. In (3) or (4), only one or the other would be
Type 62.2 70.4 75.6
included in the comparison, whereas in (5) and (1)
Realis 56.1 68.0 73.0
both would be included, allowing for partial match
Table 4. Scores for ACE 2005 Training Annotation
instead of a miss. Similarly, there are cases where
Second, we wished to determine also if there one file has a single token span for a nugget while
was a difference in the annotation consistency and the other file has a multiple token span for the cor-
quality of event nugget spans depending on wheth- responding nugget, and so it is only in (1) that both
er the span consists of only one token as compared would be included, allowing for a partial match
to those that are multiple tokens, either continuous instead of a miss.
or discontinuous. We decomposed the span F- These more fine-grained nugget span scores for
measure in Tables 3 and 4 based on these criteria. FP1 vs. FP2 show that single-token nuggets are
We did this by modifying the event nugget scoring annotated more consistently than multi-token nug-
program to optionally ignore nuggets depending on gets. Considering just the multi-token nuggets,
their span. For example, when we wished to com- there is little difference in consistency of annota-
pare annotations for which the span is a single to- tion between continuous and discontinuous spans.
ken, we simply ignored all nuggets with spans of The ADJ vs. FP1 / ADJ vs. FP2 results show that
more than one token. Likewise, when comparing including adjudication annotation lessens any dif-
nuggets for which the span consists of discontinu- ference in annotation quality for nuggets depend-
ous multiple tokens, all nuggets for which the span ing on whether the span is single or multi-token.
was either a single token or multiple continuous In future work on this consistency analysis, we
tokens were ignored. will also go in the other direction, and convert the
We ran this modified scorer in different modes event nugget data into the ACE format so that it
to use (1) all nuggets (as before), (2) only nuggets can be evaluated using the ACE scorer (NIST,
that consist of a single token, ignoring all others, 2005), ensuring that the comparison of inter-
(3) only nuggets that consist of multiple continu- annotator consistency is not overly affected by de-
ous tokens, (4) only nuggets that consist of multi- tails of particular scoring algorithms.
ple discontinuous tokens, and (5) only nuggets that
73
Event Nugget ACE 2005 Training
ADJ vs. FP1 / ADJ vs. FP1 /
FP1 vs. FP2 FP1 vs. FP2
ADJ vs FP2* ADJ vs. FP2
Span Span Span Span
Ratio** Ratio Ratio Ratio
F-meas F-meas F-meas F-meas
(1) All mentions 69.0 100% 78.2/89.3 100% 64.9 100% 79.3/81.8 100%
(2) Single-token 67.7 90.0% 77.0/88.9 87.7% 65.0 94.6% 79.2/81.6 95.2%
(3) Multiple cont. 45.3 6.1% 57.7/84.4 6.8% 44.2 5.4% 70.8/70.6 4.8%
(4) Multiple discont. 43.0 4.0% 57.5/84.1 5.5% NA NA NA NA
(5) Multiple all 46.0 10.1% 59.0/85.4 12.3% NA NA NA NA
Table 5: Decomposing the Span Scores for Nugget and Trigger Span
* The two figures represent ADJ compared to FP1 (before the slash) and ADJ compared to FP2 (after the
slash).
** Event nugget type per all event nuggets.
9 Conclusion Acknowledgments
This paper first describes the processes of event This material is based on research sponsored by
nugget annotation using a brat tool and issues Air Force Research Laboratory and Defense Ad-
which arose in the process of developing TAC KBP vanced Research Projects Agency under agreement
Event Guidelines. We present complex cases that numbers FA8750-13-2-0045 and FA8750-12-2-
cause annotators’ disagreement on tagging. Ques- 0342. The U.S. Government is authorized to repro-
tions are raised about implied events, states vs. duce and distribute reprints for Governmental pur-
events, results of events, tagging portions of words, poses notwithstanding any copyright notation
and discontinuous tagging. Second, the paper ex- thereon. The views and conclusions contained
plains the creation of a tagged event nugget corpus herein are those of the authors and should not be
and provides annotation statistics and consistency interpreted as necessarily representing the official
analysis comparing the first pass annotations, and policies or endorsements, either expressed or im-
also a comparison of adjudicated files with first plied, of Air Force Research Laboratory and De-
pass files using the Event Nugget Tuple Scorer. fense Advanced Research Projects Agency or the
The analysis shows that single-word nuggets are U.S. Government.
tagged more consistently than multi-word nuggets We are thankful to Jun Araki, Zhengzhong
and that adjudication is very important for improv- “Hector” Liu, and Volkan Cirik for their assistance
ing the quality of annotation. in the annotation tool and data calculation.
Reconciliation of annotation disagreement is
crucial in terms of not only the development of
annotation guidelines but also the quality of anno-
tation. This is closely associated with how an event
nugget is defined and clarification of tagging rules.
Resolving the issues surrounding event
type/subtype definitions will be very helpful not
only for future studies on event nugget detection
but also studies on event coreference, subevent
relations, and event arguments.
74
References
George Doddington, Alexis Mitchell, Mark Przbocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program – tasks, data, and evaluation. In
Proceedings of the Fourth International Conference
on Language Resources and Evaluation (LREC
2004), Lisbon, May 24-30.
Joe Ellis, Jeremy Getman, and Stephanie M. Strassel.
2014. Overview of Linguistic Resources for the TAC
KBP 2014 Evaluations: Planning, Execution, and Re-
sults. In Proceedings of TAC KBP 2014 Workshop,
National Institute of Standards and Technology,
Gaithersburg, Maryland, USA, November 17-18,
2014.
Language Technologies Institute. 2014. TAC KBP Event
Detection Annotation Guidelines, Version 1.7, Lan-
guage Technologies Institute, CMU, September 12,
2014.
Linguistic Data Consortium. 2014. DEFT ERE Annota-
tion Guidelines: Events Version 1.3, March 13, 2014.
Zhengzhong Liu, Teruko Mitamura, Eduard Hovy.
2015. "Evaluation Algorithms for Event Nugget De-
tection: A Pilot Study". To appear in the Proceedings
of the 3rd Workshop on EVENTS: Definition, Detec-
tion, Coreference, and Representation.
NAACL-HLT 2015.Linguistic Data Consortium.
2005. ACE (Automatic Content Extraction) English
Annotation Guidelines for Events, Version 5.4.1
2005.05.09.
National Institute of Standards and Technology. 2005.
The ACE 2005 Evaluation Plan.
http://www.itl.nist.gov/iad/mig/tests/ace/2005/doc/ac
e05-evalplan.v3.pdf
Zhiyi Song, Ann Bies, Tom Riese, Justin Mott, Jonathan
Wright, Seth Kulick, Neville Ryant, Stephanie Stras-
sel, Xiaoyi Ma. Submitted. From Light to Rich ERE:
Annotation of Entities, Relations, and Events.
Pontus Stenetorp, Sampo Pyysalo, Goran Topić, Tomo-
ko Ohta, Sophia Ananiadou and Jun'ichi Tsujii
(2012). brat: a Web-based Tool for NLP-Assisted
Text Annotation. In Proceedings of the Demonstra-
tions Session at EACL 2012.
Christopher Walker, Stephanie Strassel, Julie Medero,
Kazuaki Maeda. 2006. ACE 2005 Multilingual Train-
ing Corpus. Linguistic Data Consortium Catalog No.:
LDC2006T06.
75
Appendix
Figure 1. Type and Subtype Distribution in Event Nugget Annotation
Screenshot 1. Brat tool main annotation screen Screenshot 2. Brat tool pop-up window
76

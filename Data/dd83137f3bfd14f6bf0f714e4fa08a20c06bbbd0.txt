COSMic: A Coherence-Aware Generation Metric for Image
Descriptions
MertI˙nan PiyushSharma BaberKhalid
UniversityofPittsburgh GoogleResearch RutgersUniversity
mert.inan@pitt.edu piyushsharma@google.com baber.khalid@rutgers.edu
RaduSoricut MatthewStone MaliheAlikhani
GoogleResearch RutgersUniversity UniversityofPittsburgh
rsoricut@google.com mdstone@cs.rutgers.edu malihe@pitt.edu
Abstract
Developers of text generation models rely on
automated evaluation metrics as a stand-in
for slow and expensive manual evaluations.
However,imagecaptioningmetricshavestrug-
gled to give accurate learned estimates of
the semantic and pragmatic success of out-
put text. We address this weakness by intro-
ducing the first discourse-aware learned gen-
eration metric for evaluating image descrip- Caption Coh. CIDEr COSMic
tions. Our approach is inspired by computa-
first flower of
Model Story
tionaltheoriesofdiscourseforcapturinginfor- theyear
0.000 0.653
mation goals using coherence. We present a
close-up of
dataset of image–description pairs annotated Human Visible
pinkflowers
with coherence relations. We then train a
coherence-aware metric on a subset of the Figure 1: A comparison of the scores for a generated
Conceptual Captions dataset and measure its (Model)captionthathasadifferentcoherencerelation
effectiveness—itsabilitytopredicthumanrat- thanthereference(Human)caption. “Coh.” represents
ings of output captions—on a test set com- the coherence labels for generated and reference cap-
posed of out-of-domain images. We demon- tions. Our coherence-aware metric COSMic is aware
strateahigherKendallCorrelationCoefficient of the different information goals for these captions,
forourproposedmetricwiththehumanjudg- and assigns a more adequate score when comparing
ments for the results of a number of state- the Model caption against the Human caption. In this
of-the-artcoherence-awarecaptiongeneration case where a caption that does not just describe the
models when compared to several other met- image but elaborates on it, our metric recognizes that
rics including recently proposed learned met- themodeloutputispotentiallysuccessful(Photocredit:
ricssuchasBLEURTandBERTScore. MoorthyGounder)
1 Introduction
Lin,2004;DenkowskiandLavie,2014;Anderson
Aninvestigationofthedescriptionsusedwithim-
etal.,2016a).
ages on the web shows that image descriptions
canhavedifferentfunctionsandgoals(Kruketal., Sofar,however,effortstodevelopsuchexpres-
2019a; Alikhani et al., 2020). For instance, cap- sivecaptioningmodelshavebeenhinderedbythe
tions may describe visible entities, activities and lack of automatic metrics that can evaluate their
relationships,providebackgroundinformationthat output with respect to their information goals in
goesbeyondwhat’svisible,orreportthewriter’s context. Previous approaches to automatic cap-
ownsubjectivereactionstowhat’sdisplayed. By tion evaluation have mostly focused on n-gram
drawingonsuchdiverseexamples,imagecaption- measuresofsimilaritytoreferenceoutput(Vedan-
ingmodelscanlearnthedifferentinferentiallinks tam et al., 2014); such surface-level models fail
betweentextandimagesandusethatinformation to deal with the lexical and syntactic diversity of
atgenerationtimetoproducedescriptionsthatcan imagedescriptions. Morerecentapproachesmore
fulfill different discourse goals and inject the de- closelyapproximatesemanticsimilarityusingword
siredcontextintotheiroutput(Papinenietal.,2002; embedding-basedtechniques. Thesemodelsshow
robust performance and achieve a higher correla- experimentsdemonstratethatamongallthesemet-
tion with human judgments than that of previous rics,ourproposedmetrichasthehighestcorrelation
metrics. Nevertheless, they too fail to generalize withhumanjudgments.
tothedifferentkindsofcontentthatsuccessfulde-
scriptions may exhibit across different goals and 2 Relatedwork
contexts. That is, they cannot distinguish reason-
Therearediversewaysofcharacterizingthecon-
abledescriptionsthathappentodifferfromrefer-
tributions of text and imagery. Gao et al. (2015)
ence output in their goals and perspective, from
investigatethegenreofimagecaptionsandHuang
problematicdescriptionsthathallucinateinappro-
andKovashka(2016)studythepersuasiveimplicit
priatecontentorcontext.
relationshipsbetweentextandimages. Kruketal.
Tobridgethisgap,wepresentacoherence-aware
(2019b)studytheemotionallinksbetweentextand
embedding-basedgenerationmetricthatlearnsto
images. Otto et al. (2019) present an annotated
respectdiversediscoursegoalswithoutpenalizing
dataset of text and imagery that compares the in-
captions that arepurposefully generated to fulfill
formation load in text and images. However, we
differentpurposesorcommunicatebackgroundin-
buildonworksthatstudyinformation-levelinfer-
formation. Figure1demonstratesthiscapabilityby
encesbetweendiscourseunitsindifferentmodali-
presentinganexampleimageandcaptionswithdif-
tiessuchascomicbookpanels(McCloud,1993),
ferentcoherencelabelstogetherwiththeirscores.
movieplots(Cummingetal.,2017),anddiagram-
Our approach to modeling discourse goals is maticelements(Hiippalaetal.,2021). Inparticular,
basedontheframeworkofdiscoursecoherencethe- weuseAlikhanietal.(2020)’srelationsthatchar-
ory(Hobbs,1985),whichcharacterizestheinfer- acterizeinferencesbetweentextandimages.
encesthatgivediscourseunitsacoherentjointin- Coherence-aware models have benefited sev-
terpretationusingaconstrainedinventoryofcoher- eralNLPtaskssuchasgestureinterpretation(Las-
encerelations. Inparticular,weusethetaxonomy carides and Stone, 2009; Pustejovsky and Krish-
for image–text coherence developed by Alikhani naswamy, 2020), text summarization (Xu et al.,
etal.(2020),whichforexampleincludesVisible,
2019),machinecomprehension(Gaoetal.,2020).
StoryandSubjectiverelationsbetweenthetextand
ThemajorityoftheseworksuseRhetoricalStruc-
theimage. Adescriptionandanimagestandina ture Theory (RST) (Mann and Thompson, 1987)
Visiblerelationifthetextincludesinformationthat
and Penn Discourse TreeBank (PDTB) (Prasad
isrecognizablydepictedintheimage. Subjective et al., 2008b) datasets to learn and predict these
captionsreacttothecontentoftheimageandStory
relationsbetweentwoadjacenttextspans. Inthis
captionsprovideafree-standingdescriptionofthe lineofwork,wearethefirsttopresentacoherence-
circumstancesdepictedintheimagesimilartothe awaregenerationmetric.
Narrationrelationintext. Ourmetricislearnedin
Themostwidelyusedautomaticevaluationmet-
part from a new dataset of 4000 images with de-
rics are ngram-based, which compute the exact
scriptionslabeledwithdifferentcoherencelabels
numberofngrammatchesbetweenreferenceand
inthistaxonomy.
generated text (Cui et al., 2018). Examples of
In inaugurating the study of coherence-aware suchmetricsthatarecommonlyusedforevaluating
generationmetrics,wemakethefollowingspecific the output of captioning, translation and summa-
contributions. InSection3wepresenttwodiffer- rizationmodelsareBLEU(Papinenietal.,2002),
ent, annotated datasets for training and testing a ROUGE(Lin,2004),andCIDEr(Vedantametal.,
coherence-aware metric. We present a model to 2015), . The major problem of the n-gram sim-
score a generated caption given the image, refer- ilarity metrics is that they give no credit to syn-
encecaption,andthediscoursegoalsofboththese onymmatchesofreferencen-grams,evenifthose
captions(Section4). Wecomparethismetrictopre- words arecommon andused appropriatelyin the
viousonesusingacommonmethodology,ranking generatedtext. Embedding-basedmetricssuchas
theperformanceofseveraldifferentcaptiongenera- BLEURT (Sellam et al., 2020) and BERTScore
tionsystemsonout-of-domainimages—relyingon (Zhang et al., 2020) designed to address this lim-
anewbenchmarkout-of-domaintestset,whichwe itation are closer to human ratings. BLEURT is
publish,providingreferencecaptionsforasubset a data-intensive training scheme that is based on
of OpenImages (Kuznetsova et al., 2020b). Our BERT (Devlin et al., 2019) fine-tuned on human
Facadeofaglassbuilding. Apinkflowerbushinagar- TheundersideoftheArcde Close-upofaflysittingona
den. Triomphe. daisy.
Man sitting by his artwork Woman with an umbrella Cowboyonahorseandcow- Black and white artwork
looking at a large statue of readingabooksittinginthe boyonthegroundworking paintedonabluewall.
amanonahorseinaroyal grass in front of a city sky- togethertolassoacalfina
courtyard. line. pen.
Figure2:ExamplesofthegroundtruthcaptionsthatwecollectedfortheCOINdataset. (Photocreditsfromleftto
right, toptobottom: SharronMollerus, Northfielder, GeorgeM.Groutas, davebloggs007, TimAdams, Brisbane
CityCouncil,ColinBrown,GuilhemVellut)
ratings of generated text. BERTScore, however, of White and Latino ethnicity. The code 1 of the
computes the similarity score as the average of annotationwebsite,andthedetailsoftheprotocol
cosine similarities between predicted tokens and ispubliclyavailable. Thestudyhasbeenapproved
theirtopmatchingreferencetokens. Thesemetrics byourinstitution’shumansubjectboard.
however,donotrespecttheinformationgoaland
Conceptual Captions Score Annotation We
thepurposeforwhichthemodelhasgeneratedthe
havecollectedratingsonthequalityofdifferentim-
text. Weaddressthisproblembyintroducingthe
agedescriptionswithcoherencelabelsforasubset
first coherence-aware generation metric. Similar
of1000imagesfromtheConceptualCaptions(CC)
toSPICE(Andersonetal.,2016b)andVIFIDEL
trainingdataset(Ngetal.,2020). Withthispaper,
(Madhyasthaetal.,2019)weusetheinformation
wearepublishingthisdatasetasabenchmarkfor
encoded in images. We further propose the addi-
evaluationmetricsthatarecoherence-aware. The
tionofcoherencerelationsthatfacilitatelearning
set-up of the data collection is as follows: CC
withfewersamplesbyamultimodalmetricusing
imagesareinputintoacaption-generationmodel
pre-trainedBERTandViLBERT.
created by Alikhani et al. (2020). This model
generatescoherence-awaredescriptionsforinput
3 DataCollection
images in 4 different coherence classes of Meta,
Visible, Subjective, Story. These 4,000
We collect two datasets: human judgments for
image/caption pairs are then presented to human
image captions that are generated by coherence-
annotators who are asked to select the correct
aware captioning systems using Conceptual Cap-
coherencelabelforeachpair:
tionsdataset;andground-truthlabelsfortheOpen
Images dataset. With Conceptual Captions cor-
• Meta: the caption talks about when, where,
porawefine-tuneViLBERTwithratingsandshow
and how the picture is taken. Meta-talk in
thatadditionofcoherencerelationscanmakeau-
Schiffrin(1980)
tomatedscoringclosertohumanscoring. Weuse
• Visible: thecaptionistruejustbylookingat
OpenImagescorporatoreinforcethatmultimodal-
the picture. Restatement relation in Prasad
ityandcoherencerelationshavesignificantcontri-
etal.(2008a).
butionstoscoringout-of-domaindatasets,aswell.
• Subjective: thecaptionsisthematterofopin-
ion. EvaluationrelationinHobbs(1985).
Protocol Wehiredtwoexpertlinguistsfordata
• Story: textandimageworklikestoryandil-
annotationanddesignedanannotationwebsiteto
lustration. OccasionrelationinHobbs(1985).
facilitate the annotation procedure. They are na-
tiveEnglishspeakerswhoidentifythemselvesas 1https://github.com/Merterm/COSMic
Figure 3: An illustration of different flavors of COSMic that outputs a score for the generated caption given the
image, reference caption, and the coherence-labels for both the captions. (a) COSMic Vanilla uses only global
textual and visual features, while (b) COSMic ViLBERT uses combined visio-linguistic features with both local
andglobalfocus. Thismodeltakesintoaccounttheinformationgoals(determinedbycoherence-labels)forboth
thecaptionswhencomparingthegeneratedcaptiontothereferenceforevaluation.
Aftertheannotatorselectsaspecificcoherence referencecaption. ThismetricfunctionM canbe
labelfromtheabove,weaskthemtoratethequality formalizedaspredictingascoresasfollows:
ofthecaptions,giventhelabel,onascaleof1to
5. Weusetheseannotationsastrainingdataforour s = M(I,g,r,g ,r ;θ) (1)
c c
coherence-awarecaptioningmetric,COSMic. We
callthisdataweannotatedRaCCoon(Ratingsfor where the metric is defined by parameters θ, and
ConceptualCaption). wherethemodelinputsaredefinedasI beingthe
TocalculatetheCohen’sκagreementmeasure, imagebeingcaptioned,g andr thegeneratedand
we selected 150 images randomly and assigned referencecaptions,respectively. g andr arethe
c c
themtotwoannotators. TheKappacoefficientis coherencerelationsforg,r respectively.
κ = 0.89whichindicatesasubstantialagreement We now describe the architecture of our
(VieraandGarrett,2005) coherence-aware image captioning metric, COS-
Mic (COherence-Sensitive Metric of image
OpenImages Ground Truth Captions To cre-
captions). Ithastwoflavors—aViLBERT-based
ate an out of domain test set we asked our anno-
modelpre-trainedonlargemultimodaldata,anda
tatorstowriteVisiblecaptionsfor1,000images2
baselineVanillaversion,asillustratedinFigure3.
from the OpenImages dataset (Kuznetsova et al.,
Both are trained on RaCCoon training data (Sec-
2020a). We call this dataset COIN (Corpus of
tion3)withnormalizedhumanannotatedratingto
OpenImageswithNaturaldescriptions). Asample
obtainthemodel’stargetscore.
ofthesegroundtruthcaptionswrittenbyourexpert
linguists are presented in Figure 2. We use this
4.1 COSMicViLBERT
datasettotestCOSMicandotherlearnedmetrics
inSection5andpresentourbenchmarkresultsin ViLBERT(Luetal.,2019)isamultimodalfeature
Table1. learningmodelpre-trainedon3.3millionConcep-
tualCaptionsimageandcaptionsdata. Itistrained
4 Method
formaskedmulti-modallearningandmulti-modal
alignmentpredictionanddemonstratesstrongper-
The goal of a coherence-aware image captioning
formanceonseveraldownstreammultimodaltasks
metric is to predict a score for the generated cap-
suchasVQA,VCR,grounding,andimageretrieval.
tiongiventheimage,referencecaption,andcoher-
Forthisreasonweuseapre-trainedViLBERTto
ence relations of one generated caption and one
embedourmultimodalinputsshowninEquation1
2The same subset, named T2, was used for the
withchangestoincorporateboththecaptionsand
CVPR-2019 Workshop on Conceptual Captions,
www.conceptualcaptions.com. coherencerelations.
For input image (I), we use the same process BERT-Large-512 model. We use the [CLS] to-
asViLBERT.WeuseaFasterR-CNN(Renetal., kenembeddingas1024dimensionalcaption-level
2016) model pre-trained on Visual Genome (Kr- representationineachcaseandtransformthemto
ishnaetal.,2016)todetectobjectsregionsandex- 512-dimensionalspace.
tractfeatures. Thesequenceoftheseimagefeatures
is denoted as I(cid:48) with 100 bounding box features e g = Linear 2(BERT CLS(g))
(4)
whereeachelementisR2048. SimilartoViLBERT, e = Linear (BERT (r))
r 2 CLS
we use the special token [IMG] to denote the be-
In our coherence label embedding module, g
ginningoftheboundingboxfeatureslist. c
andr areeachrepresentedasone-hotvectorssuch
For input captions (g, r) and coherence labels c
thatthedimensionscorrespondtolabelsMeta,Vis-
(g ,g ),thesequencebeginswiththespecialtoken
c r
ible,SubjectiveandStory. Eachisembeddedinto
[CLS]followedbyinputtextembeddings. Eachof
a512-dimensionalspace.
ourtextinputsaretokenizedandembeddedusing
ViLBERT’sinputtextpre-processinganddenoted
e = Linear (g )
as g(cid:48), r(cid:48), g(cid:48), g(cid:48) for g, r, g and g respectively. gc 3 c (5)
c r c r e = Linear (r )
Notethatthecoherencelabelsareprocessedastext rc 3 c
inputssuchas“Visible”and“Story”whichallows
We thus obtain the 5 vectors (each R512),
themodeltouseitspre-trainedrepresentationsof
representing one of the inputs of Equation 1.
theseconcepts. Eachoftheseinputsequencesare
We concatenate and use a feed-forward net-
separatedbythespecialtoken[SEP]toformour
work with progressively smaller hidden layers
inputsequence.
of sizes [512,256,128,64,32,16,8], each with
Hence,ourinputtoViLBERTisofform:
ReLU(Agarap,2018)activation. Theoutputscore,
v=([IMG],I(cid:48),[CLS],r(cid:48),[SEP],g(cid:48),[SEP],r c(cid:48),[SEP],g c(cid:48))
s,iscomputedbyafinallinearlayerontopofthe
We use a linear layer with sigmoid activation abovenetwork.
onViLBERT’soutputtextlogitstocomputeCOS-
e = concat([e ,e ,e ,e ,e ]))
Mic’soutputmetricscore(s). I g r gc rc
(6)
s = Linear (MLP (e))
4 1
s = Linear(ViLBERT(v)) (2)
wheree ∈ R2560 ands ∈ R.
Tounderstandtheroleofeachcomponentofthis
Duringtraining,wefine-tuneViLBERTandthe
implementation,wefurtherdeconstructeachmod-
outputlinearlayerinanend-to-endfashionbymini-
uleinablationexperimentsdescribedinTable2.
mizingtheMean-Squarederrorbetweentheoutput
score,sandthecorrespondingreferencescore,y,
4.3 Coherence-awareCaptioningSystems
ontheRaCCoondataset.
InordertoexperimentwithCOSMic,wegenerate
4.2 COSMicVanilla ourowncaptions. Inthissectionwedescribethe
coherence-awarecaptioningsystemsusedtogener-
TheCOSMicViLBERTapproachabovetakesad-
atetheseimagecaptionsforthetrainingandtesting
vantageofmultimodalpre-trainingontheConcep-
ofCOSMic.
tualCaptionsdatasettoembedtheimageandtext
Forourbasecaptioningsystem,weusethestate-
inputs. As a simpler baseline, we now present
of-the-art coherence-aware captioning system in-
COSMicVanillawhichindependentlyembedsthe
troduced by (Alikhani et al., 2020). It uses a
inputimageandtexttobelatercombinedforscore
Transformer-based(Vaswanietal.,2017)encoder-
computationwithnoend-to-endtraining.
decoderarchitecturewheretheencoderinputsare
Toextractimagefeatures,weuseaResNet50v2
(1)globalimagefeatures,(2)imagelabels,and(3)
(He et al., 2015) model pre-trained on ImageNet
coherencelabel. Thecoherence-labelalsoserves
(Dengetal.,2009)andlinearlytransformtheglobal
asthefirstinputtokenforthedecoderwhichgen-
imagerepresentationto512-dimensionalspace.
erates the output captions. We set the coherence
e = Linear (AveragePool(ResNet(I))) (3) label to the groundtruth relation at training time,
I 1
andthedesiredrelationatinferencetime. Weuse
In our textual feature extraction module, we the Conceptual Captions dataset (Sharma et al.,
embed g and r independently with a pre-trained 2018)withmachine-generatedcoherencelabelsfor
System Metrics
Avg.
Model Coh. Hum. BS- COSMic COSMic COSMic COSMic
Label Rating B1 B2 M RL C S BR F Vanilla ViL- Vanilla+ ViL-
BERT BERT+
BUTD Visible 2.191 .163 .077 .049 .160 .092 .030 -.877 .863 .706 .796 .522 .641
Visible 3.532 .050 .025 .019 .066 .020 .002 -1.114 .862 .696 .777 .516 .614
Meta 3.213 .041 .000 .012 .063 .012 .000 -1.059 .863 .548 .727 .505 .602
Base
Subj. 2.830 .033 .012 .011 .057 .017 .000 -1.197 .849 .323 .421 .358 .403
Story 2.915 .029 .000 .017 .058 .013 .000 -1.304 .842 .533 .629 .482 .527
Visible 3.298 .028 .011 .013 .053 .011 .000 -1.101 .863 .684 .784 .515 .604
Meta 2.830 .026 .010 .008 .055 .015 .000 -1.084 .859 .548 .748 .511 .565
Lite
Subj. 2.298 .039 .012 .019 .066 .024 .003 -1.217 .849 .364 .451 .379 .419
Story 2.426 .036 .000 .018 .062 .021 .000 -1.362 .842 .568 .666 .499 .519
Kendall’s
1.000 .071 .154 .036 -.036 -.571 -.052 .286 .445 .571 .546 .667 .764
Correlation(τ)
Table 1: System-level scores for 9 different image captioning systems as evaluated by human annotators and
variouscaptioningmetrics. Bottom-UpTop-Down(BUTD)istrainedonCOCO,whileothersaretrainedonthe
ConceptualCaptions(CC)dataset.TheevaluationhoweverisconductedonCOINdataset,whichisout-of-domain
forbothCOCOandCC.Thisdomainshiftcausesthen-grambasedmetrics(e.g.BLEU,ROUGE,CIDEr)toassign
very low scores to otherwise correct captions (See Table 4). Whereas embedding based metrics (e.g. BLEURT,
BERTScore and COSMic) do not suffer from this limitation. Since all metrics have different scales, instead of
absolute scores, we use Kendall Rank Correlation to measure agreement with human scores. Model names are
abbreviatedasfollows: B : Bleu ,B : Bleu ,M:METEOR,R : ROUGE ,C:CIDEr,S:SPICE,BR:BLEURT,
1 1 2 2 L L
BS-F:BERTScoreF1. COSMicmodelswith’+’denoteapplicationofdataaugmentationtoremovetrainingdata
bias. Moremetricsanddetailedresultscanbefoundonthecoderepository.
trainingthiscaptioningsystem. Toobtaintheco- acaptioningsystemtrainedonMSCOCO(Chen
herencelabelsabove,wecloselyfollow(Alikhani etal.,2015). SinceCOSMicexpectsaninputco-
et al., 2020) to train a coherence classifier on the herencelabel,andCOCOcaptionsareVisiblestyle
Clue dataset (Alikhani et al., 2020) that provides bydesign,wesetthelabeltoVisible. Specifically,
around4Khumanannotated(image,caption,rela- weusetheBottom-UpTop-Down(BUTD)Atten-
tion) triplets. We present two caption-generation tion model (Anderson et al., 2018). This helps
systemsinthissection. studyhowwellCOSMicgeneralizestoothercap-
tioningdatasetsandcoherence-agnosticcaptioning
Base-systems family A family of 4 captioning
systems.
systems is created by setting the coherence-label
to Meta, Visible, Subjective or Story in the base
5 Experiments
captioningmodeldescribedabove. Thesearecon-
sidered different captioning systems because the
Here,wedescribetheexperimentalsetuptocom-
information content and discourse goals, as con-
pare COSMic with other metrics. As outlined in
trolledbythecoherencelabel,aredifferent.
Section3and4,weusetheRaCCoondatatotrain
Lite-systems family We remove the global im- ourmodels,andCOINtotestCOSMicandother
agefeaturesfromthebasemodel’sinputtoobtain metrics. Wehaveseveralbaselinemetricsthatwe
asmaller,light-weight(lite)model. Similartothe compareto,whichcanbefoundonTable1.
base model, we obtain a family of 4 captioning
systemsbychangingthecoherence-label. 5.1 ModelTrainingSetup
In Section 5, we study the order in which sev-
We implement COSMic—as described in Sec-
eralimagecaptioningmetricsrankthese8systems.
tion4—withPyTorch(Paszkeetal.,2019)andtrain
The goal is to identify the metric that agrees the
onaGTX1080GPU.Wepre-computeBERT3 and
most with the groundtruth rankings based on hu-
ResNet4 features using their TensorFlow (Abadi
manassessments.
et al., 2015) implementations. We use the pub-
4.4 COCO-trainedCaptioningSystem
COSMic’s training data, RaCCoon, is based on
3https://github.com/google-research/
bert
ConceptualCaptionsanditiscoherence-aware. To
4https://www.tensorflow.org/api_docs/
testthemodel’sgeneralizationcapability,weuse python/tf/keras/applications/ResNet50V2
lic ViLBERT5 implementation. We use a batch computedbymatchingthetokens’outputembed-
sizeof4,andalearningrateof2×10−6 forfine- dings.
tuning ViLBERT and use RAdam optimizer and PleasenotethatforbothBERT-basedbaselines
stop the training when the validation score does above(BLEURT,BERTScore),weusetheBERT-
not change for 3 epochs. For COSMic Vanilla, Large-512sizemodel.
wetrainwithabatch-sizeof10,Adamoptimizer
(Kingma and Ba, 2017) with a base learning rate 5.3 COIN-basedEvaluationSetup
of 10−3 that decays by a factor of 10−2 every 10
WeuseeachbaselinemetricandCOSMictoscore
epochs. We observe that the Vanilla converges
the8differentimagecaptioningsystemsdescribed
in approximately 100 epochs and ViLBERT con-
in Section 4 on the same set of test images with
verges in 9 epochs. ViLBERT has 250 million
referencecaptions. Notethattherangeandscale
parameters. COSMic Vanilla includes 3,062,913
of each metric is different, however they are all
trainableparameters. Pre-trainedBERT-Largeand
monotonouslyincreasingfunctionsofmodelqual-
ResNet50V2haveanadditional350millionparam-
ity. So in our study, we do not analyze the abso-
eters. The setup for coherence-aware captioning
lutescoreassignedbythesemetrics,butonlytheir
models to obtain machine-generated captions for
ranks. Wealsoaskhumanannotatorstorankthese
ourstudyisthesameas(Alikhanietal.,2020).
8 captioning systems on the same set of test im-
ages. Theranksassignedbyahigherperforming
5.2 BaselineCaptioningMetrics
metricwillalignbetterwiththeranksfromhuman
TobenchmarkCOSMic,wecompareitwithother
annotators.
learnedmetrics. Inthissectionwedescribethese
Sincethecaptioningsystemsabovearetrained
various metrics traditionally used for measuring
on Conceptual Captions or COCO, we use im-
imagecaptioningsystems. Noneofthesemetrics
age/captionpairsfromCOINforanout-of-domain
were designed to support the coherence relations
evaluation. Asubsetof50randomimagesisused
ofthereferenceorgeneratedcaptions. Theseserve
torankthecaptioningsystemsasdescribedabove,
asbaselinesforCOSMic.
resultingin400machine-generatedcaptionstotal
for the 8 captioning systems. These were then
N-grambased Themostpopularimagecaption-
evaluatedbyhumanannotatorsusingtheprocess
ingmetricsarebasedonprecisionandrecallofn-
describedinSection3. Thehuman-scoredsystem
gramsfromgeneratedandreferencecaptions. We
level performance for each captioning system on
comparewithBleu ,Bleu ,Bleu ,Bleu (Guoand
1 2 3 4
thistestsetisreportedinTable1in“AverageHu-
Hu,2019),ROUGE (Lin,2004),CIDEr(Vedan-
L
manRating”.
tam et al., 2015), and SPICE (Anderson et al.,
2016b). We compute these using their popular We measure the alignment between metric-
open-sourceimplementation6. assigned and human-assigned scores using the
Kendall(Kendall,1938)correlationcoefficient. In
BLEURT Weuseapre-trainedBLEURTmodel7 ordertocalculatethescore,wefirstaggregateall
asabaselineforourwork. UnlikeN-grambased the sample scores and average them. Then we
approaches,BLEURTusesBERT-basedwordem- calculate the Kendall tau score using the SciPy
beddingswhicharerobusttovariationsinsurface 1.7.1 implementation. The score is calculated
word realizations between the reference and gen- between two vectors, first of which is the aver-
eratedcaptions. Wedonotdoanyfine-tuningfor age human ratings for 8 models and the second
thisbaseline. being the investigated metric scores for 8 mod-
els in the following order:[Base , Base ,
BERTScore BERTScore8 uses a pre-trained Visible Meta
Base , Base , Lite , Lite ,
Subjective Story Visible Meta
BERT model to embed the reference and gener-
Lite , Lite ]. Due to the small sam-
Subjective Story
atedcaptions. Text-levelsimilarityscoresarethen
ple size, Kendall correlation is the most suitable
correlationmeasure.
5https://github.com/facebookresearch/
vilbert-multi-task A key measure of the success of an automatic
6https://github.com/tylin/coco-caption
evaluationmetriciswhetheritmakesthesamedeci-
7https://github.com/google-research/
sionaboutwhichsystemisbetterinahead-to-head
bleurt
8https://github.com/Tiiiger/bert_score evaluationaswewouldgetfromahuman-subjects
evaluation. If each system is evaluated based on DataAugmentation TherawRaCCoontraining
itsaveragescore,thensuccesscomeswhentheav- datahasacoherence-levelbiasasdemonstratedby
eragecomputedmetriccorrelatescloselywiththe theaverageCOSMicscoreforeachclass—Visi-
averagehuman-ranking. Inparticular,wemeasure ble(0.622),Meta(0.459),Subjective(0.236)and
thealignmentbetweenmetricassignedandhuman Story(0.397). Thisreflectsthehumanannotators’
assignedscoresusingtheKendallscore,following biastowardslikingVisiblecaptionsthemost,and
theworkof(Sellametal.,2020). Subjective captions the least, which is expected.
However,trainingCOSMiconthisdatainjectsthe
6 Results same coherence-bias into the model which is un-
desirable. AspresentedinTable1,bothflavorsof
Table 1 presents the results of the COIN-based COSMic (without the ‘+’) assign high scores to
study. The last row reports the Kendall correla- Visiblecaptioningsystems.
tioncoefficientbetweenthescoresassignedbythe
To mitigate this issue, we algorithmically aug-
metricandhumans.
mentthetrainingdatatobringtheaveragescores
All N-gram based metrics, such as BLEU and
foreachcoherenceclasstocomparablevalues. We
CIDEr,failtoadapttotheout-of-domainground-
achieve this by pairing images with random cap-
truth captions from COIN. This results in a rela-
tionsfromthecoherenceclassandassigningthem
tivelyflatdistributionofsystem-levelscorescon-
ascoreof0. Thisisavalidtrainingsamplebecause
centratedcloseto0,andhencelowcorrelationco-
therandomlysampledcaptiondoesnotdescribethe
efficients. CIDErhasahighlynegativeKendall’s
saidimageandservesasanegativesample. With
τ, which denotes a strong negative association
theseoperations,theclassbiasissignificantlyre-
withhumanjudgements. Thisispartlyduetolow duced—Visible(0.459),Meta(0.439),Subjective
(∼0.01)andhencenoisyCIDErscores. (Figure4 (0.328)andStory(0.425). TheCOSMiccolumns
provides example cases that illustrate this argu-
in Table 1 with ‘+’ denote that this data augmen-
ment.)
tation approach improves ranking of captioning
Embedding-based methods, BLEURT and systems leading to better alignment with human
BERTScore,donotsufferfromthislimitationre- judgements.
sultinginmoremeaningfulscoringofsystemsand
hencehighercorrelationwithhumanscores. How- Ablation Study Table 2 reports the perfor-
ever,bydesign,boththesemetricsareagnosticto mance of COSMic Vanilla without coherence-
coherence-labels and the input image. COSMic, labelsand/ortheimageasmodelinputs. Wefind
whichiscoherence-aware,obtainsthehighestcor- thatremovalofimagefeaturesaffectsCOSMic’s
relation with human scores. COSMic ViLBERT performance,showingtheimportantcontribution
hasthehighestKendall’scorrelationamongallof of images. The performance deteriorates signifi-
our models. COSMic Vanilla performs the sec- cantlywhenthecoherence-labelsareremovedfrom
ondbestamongourmodelsanditperformsbetter the model ("No r ,g " column in Table 2). This
c c
than the rest of the models in terms of Kendall’s demonstratesthatCOSMic successfullyintegrates
correlation. coherence-relationsinthecaptionscoringprocess.
Reference two men in scrubs per- mountainsinfrontofa largebrickbuildingnextto afoggyforest.
formingsurgery. clearbluesky. agreenlawnandbigtrees.
Generated surgeonsoperatingona mountainrangeasseen thefrontofthehouse. light shining through
patient. fromthetrail. thetrees.
Figure4: IllustrationofCOINreferencecaptionsandcorrespondingoutputsoftheBase-Visiblemodel. Though
thegeneratedcaptionsarecorrect,ann-grambasedmetricsuchasCIDErassignsthemaverylowscoreduetothe
variationsinsurfacewordrealizations. SeeTable1foraveragescoresoverthetestset. (Photocredits,fromleftto
right: U.S.ArmyAfrica,Gabriel,FrJamesBradley,RosmarieVoegtli)
System COSMic 8 Ethics
Model Coh. Full NoI Noc NoI&c
Thispaperdescribesaresearchprototype. Wedo
Label
notworkwithsensitiveorpersonaldata. Ourpro-
Visible .516 .447 .434 .442
tocol was approved by our ethics board. Human
Meta .505 .439 .442 .453
Base
Subj. .356 .347 .438 .453 subjects participated voluntarily, undertook min-
Story .505 .433 .436 .445
imal risk, and were compensated fairly for their
Visible .515 .444 .434 .433 time. Thedatasetweproducedisfullyanonymized.
Meta .511 .434 .447 .464
Lite Subjectsconsentedtothedistributionoftheirdata
Subj. .379 .367 .440 .459
Story .499 .440 .433 .442 aspartoftheirparticipationintheresearch. Tech-
nologistsshouldthinkcarefullybeforedeploying
Kendall’s
.667 .546 -.222 -.415
Corr.(τ) our ideas in production. Our work depends on
pretrainedmodelssuchaswordandimageembed-
Table 2: Ablation experiment results. "No I" repre-
dings. Thesemodelsareknowntoreproduceand
sents "COSMic Vanilla without image features", "No
evenmagnifysocietalbiaspresentintrainingdata.
r ,g "represents"COSMicVanillawithoutcoherence
c c
Moreover,likemanyMLNLPmethods,ourmeth-
label embeddings", finally "No I & No r ,g " repre-
c c
ods are likely to perform better for content that
sents"COSMicVanillawithoutcoherencelabelembed-
dingsandwithoutimagefeatures". isbetterrepresentedintraining,leadingtofurther
biasagainstmarginalizedgroups. Wecanhopethat
generalmethodstomitigateharmsfromMLbias
7 Conclusion
canaddresstheseissues.
Ourworkisthefirststeptowardsdesigninggenera- Adistinctivecomplicationofourworkisthefact
tionmetricsthatrespecttheinformationgoalofthe that many image–text presentations involve writ-
generatedtext. Weobservethatasmallsetofex- ers expressing subjective opinions. By its nature,
amplesannotatedwithcoherencerelationscanpro- ourevaluationmetricassessessuchsubjectivetexts
videwhatisneededforlearningadiscourse-aware based on averages and trends across many users,
generationmetric. Ourfindingshaveimplications which may be problematic. Although such judg-
for designing context-aware multimodal metrics mentsareultimatelymattersofpersonaltaste,they
with criteria that are closer to human ratings for areneverthelessoftengroundsbywhichhierarchies
evaluatingmachine-generatedmultimodalcontent. ofdifferencesareculturallyencodedandenforced.
We have called attention to the challenge of Thus,adeployedsubjective-captiongenerationsys-
learning robust generation metrics that can eval- temcouldwellbeunfairtousers,especiallyifthose
uate the output of the generation models consid- usersarenotconfidentintheirowntasteorcritical
ering the information goals. Our findings sug- towards the system’s responses. Our evaluation
gestthatfine-tuningViLBERT—originallytrained metricisnotsensitivetosuchharms.
withmillionsofimages—withasmallersampleof
Acknowledgements
coherencerelationsandexpert-annotatedscoring,
automated metrics can score generated captions
TheauthorsaffiliatedwithRutgersUniversitywere
closer to a human rating. The presented dataset
partly supported by NSF Award CCF-19349243.
providestheopportunityforfutureresearchinthe
ThankstoPittCyberforsupportingthisprojectand
area of image description generation, designing
theauthorsfromtheUniversityofPittsburgh. We
discourse-awaremetrics,andmultimodalcontent
alsoacknowledgetheCenterforResearchComput-
evaluation. Wehopethatcoherence-awaretextgen-
ingattheUniversityofPittsburghforprovidingthe
eration metrics could be used for learning better
requiredcomputationalresourcesforcarryingout
generationmodels(suchasabstractivesummariza-
experimentsattheUniversityofPittsburgh.
tion or story generation) and could be deployed
directly in machine learning pipelines to help in
optimizinghyper-parameters. Ultimately,itisin- References
tendedtohaveageneralizablemodelthatcanuse
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene
alabelingmechanism—notrestrictedtocoherence
Brevdo,ZhifengChen,CraigCitro,GregS.Corrado,
labels—toimproveapplicabilityofgenerationmet-
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
ricsindifferenttasks. Ghemawat,IanGoodfellow,AndrewHarp,Geoffrey
Irving,MichaelIsard,YangqingJia,RafalJozefow- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
icz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven- Kristina Toutanova. 2019. BERT: Pre-training of
berg,DandelionMané,RajatMonga,SherryMoore, deep bidirectional transformers for language under-
DerekMurray,ChrisOlah,MikeSchuster,Jonathon standing. In Proceedings of the 2019 Conference
Shlens,BenoitSteiner,IlyaSutskever,KunalTalwar, of the North American Chapter of the Association
Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, for Computational Linguistics: Human Language
Fernanda Viégas, Oriol Vinyals, Pete Warden, Mar- Technologies, Volume 1 (Long and Short Papers),
tin Wattenberg, Martin Wicke, Yuan Yu, and Xiao- pages4171–4186,Minneapolis,Minnesota.Associ-
qiang Zheng. 2015. TensorFlow: Large-scale ma- ationforComputationalLinguistics.
chinelearningonheterogeneoussystems. Software
availablefromtensorflow.org. HaoyuanGao,JunhuaMao,JieZhou,ZhihengHuang,
LeiWang, andWeiXu.2015. Areyoutalkingtoa
Abien Fred Agarap. 2018. Deep learning using recti- machine? dataset and methods for multilingual im-
fiedlinearunits(relu). CoRR,abs/1803.08375. age question. In Advances in Neural Information
ProcessingSystems,pages2296–2304.
Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu
Soricut,andMatthewStone.2020. Cross-modalco- Yifan Gao, Chien-Sheng Wu, Jingjing Li, Shafiq Joty,
herence modeling for caption generation. In Pro- Steven C.H. Hoi, Caiming Xiong, Irwin King, and
ceedings of the 58th Annual Meeting of the Asso- Michael Lyu. 2020. Discern: Discourse-aware en-
ciation for Computational Linguistics, pages 6525– tailment reasoning network for conversational ma-
6535, Online. Association for Computational Lin- chine reading. In Proceedings of the 2020 Confer-
guistics. ence on Empirical Methods in Natural Language
Processing(EMNLP),pages2439–2449,Online.As-
Peter Anderson, Basura Fernando, Mark Johnson, sociationforComputationalLinguistics.
and Stephen Gould. 2016a. SPICE: semantic
Yinuo Guo and Junfeng Hu. 2019. Meteor++ 2.0:
propositional image caption evaluation. CoRR,
Adoptsyntacticlevelparaphraseknowledgeintoma-
abs/1607.08822.
chine translation evaluation. In Proceedings of the
FourthConferenceonMachineTranslation(Volume
PeterAnderson,BasuraFernando,MarkJohnson,and
2: SharedTaskPapers,Day1),pages501–506,Flo-
Stephen Gould. 2016b. Spice: Semantic propo-
sitional image caption evaluation. In European rence,Italy.AssociationforComputationalLinguis-
Conference on Computer Vision, pages 382–398. tics.
Springer.
KaimingHe,XiangyuZhang,ShaoqingRen,andJian
Sun.2015. Deepresiduallearningforimagerecog-
PeterAnderson,XiaodongHe,ChrisBuehler,Damien
nition. CoRR,abs/1512.03385.
Teney, Mark Johnson, Stephen Gould, and Lei
Zhang.2018. Bottom-upandtop-downattentionfor
Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen,
imagecaptioningandvisualquestionanswering. In
Timo Kalliokoski, Evanfiya Logacheva, Serafina
Proceedings of the IEEE Conference on Computer
Orekhova, Aino Tuomainen, Matthew Stone, and
VisionandPatternRecognition(CVPR).
John A. Bateman. 2021. AI2D-RST: a multimodal
corpus of 1000 primary school science diagrams.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
Lang.Resour.Evaluation,55(3):661–688.
ishna Vedantam, Saurabh Gupta, Piotr Dollar, and
C. Lawrence Zitnick. 2015. Microsoft coco cap-
JerryR.Hobbs.1985. Onthecoherenceandstructure
tions: Datacollectionandevaluationserver.
ofdiscourse.
Yin Cui, Guandao Yang, Andreas Veit, Xun Huang,
XinyueHuangandAdrianaKovashka.2016. Inferring
andSergeBelongie.2018. Learningtoevaluateim-
visual persuasion via body language, setting, and
age captioning. In Proceedings of the IEEE con-
deep features. In Proceedings of the IEEE Confer-
ferenceoncomputervisionandpatternrecognition,
ence on Computer Vision and Pattern Recognition
pages5804–5812.
Workshops,pages73–79.
SamuelCumming,GabrielGreenberg,andRoryKelly. M.G.Kendall.1938. Anewmeasureofrankcorrela-
2017. Conventions of viewpoint coherence in film. tion. Biometrika,30(1/2):81–93.
Philosophers’Imprint,17(1):1–29.
Diederik P. Kingma and Jimmy Ba. 2017. Adam: A
J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei- methodforstochasticoptimization.
Fei. 2009. ImageNet: A Large-Scale Hierarchical
ImageDatabase. InCVPR09. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
MichaelDenkowskiandAlonLavie.2014. Meteoruni- Yannis Kalantidis, Li-Jia Li, David A Shamma,
versal: Language specific translation evaluation for Michael Bernstein, and Li Fei-Fei. 2016. Visual
any target language. In Proceedings of the EACL genome: Connecting language and vision using
2014WorkshoponStatisticalMachineTranslation. crowdsourceddenseimageannotations.
Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan Christian Otto, Matthias Springstein, Avishek Anand,
Jurafsky, and Ajay Divakaran. 2019a. Integrating and Ralph Ewerth. 2019. Understanding, catego-
textandimage: Determiningmultimodaldocument rizing and predicting semantic image-text relations.
intent in Instagram posts. In Proceedings of the In Proceedings of the 2019 on International Con-
2019 Conference on Empirical Methods in Natu- ference on Multimedia Retrieval, pages 168–176.
ral Language Processing and the 9th International ACM.
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 4622–4632, Hong Kong, KishorePapineni,SalimRoukos,ToddWard,andWei
China.AssociationforComputationalLinguistics. jingZhu.2002. Bleu: amethodforautomaticevalu-
ationofmachinetranslation. pages311–318.
Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan
Jurafsky, and Ajay Divakaran. 2019b. Integrat- Adam Paszke, Sam Gross, Francisco Massa, Adam
ing text and image: Determining multimodal doc- Lerer, James Bradbury, Gregory Chanan, Trevor
ument intent in instagram posts. arXiv preprint Killeen, Zeming Lin, Natalia Gimelshein, Luca
arXiv:1904.09073. Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, ZacharyDeVito, MartinRaison, AlykhanTe-
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper jani,SasankChilamkurthy,BenoitSteiner,LuFang,
Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Junjie Bai, and Soumith Chintala. 2019. Pytorch:
Kamali, Stefan Popov, Matteo Malloci, Alexander An imperative style, high-performance deep learn-
Kolesnikov, and et al. 2020a. The open images inglibrary. InAdvancesinNeuralInformationPro-
dataset v4. International Journal of Computer Vi- cessingSystems32,pages8024–8035.CurranAsso-
sion,128(7):1956–1981. ciates,Inc.
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
Kamali, Stefan Popov, Matteo Malloci, Alexander
nie Webber. 2008a. The Penn Discourse Tree-
Kolesnikov, et al. 2020b. The open images dataset
Bank 2.0. In Proceedings of the Sixth Interna-
v4. InternationalJournalofComputerVision,pages
tionalConferenceonLanguageResourcesandEval-
1–26.
uation (LREC’08), Marrakech, Morocco. European
LanguageResourcesAssociation(ELRA).
Alex Lascarides and Matthew Stone. 2009. A formal
semanticanalysisofgesture. JournalofSemantics,
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
26(4):393–449.
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nieLWebber.2008b. ThePenndiscoursetreebank
Chin-Yew Lin. 2004. ROUGE: A package for auto-
2.0. InLREC.Citeseer.
maticevaluationofsummaries. InTextSummariza-
tion Branches Out, pages 74–81, Barcelona, Spain.
J Pustejovsky and N Krishnaswamy. 2020. Situated
AssociationforComputationalLinguistics.
meaning in multimodal dialogue: human-robot and
human-computerinteractions.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan
Lee. 2019. Vilbert: Pretraining task-agnostic visi-
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
olinguistic representations for vision-and-language
Sun. 2016. Faster r-cnn: Towards real-time object
tasks. In Advances in Neural Information Process-
detectionwithregionproposalnetworks.
ingSystems,volume32.CurranAssociates,Inc.
Pranava Madhyastha, Josiah Wang, and Lucia Specia. Deborah Schiffrin. 1980. Meta-talk: Organizational
2019. VIFIDEL: Evaluating the visual fidelity of and evaluative brackets in discourse. Sociological
image descriptions. In Proceedings of the 57th An- Inquiry,50(3-4):199–236.
nual Meeting of the Association for Computational
Linguistics,pages6539–6550,Florence,Italy.Asso- Thibault Sellam, Dipanjan Das, and Ankur Parikh.
ciationforComputationalLinguistics. 2020. BLEURT: Learning robust metrics for text
generation. InProceedingsofthe58thAnnualMeet-
William C Mann and Sandra A Thompson. 1987. ingoftheAssociationforComputationalLinguistics,
Rhetorical structure theory: A theory of text orga- pages7881–7892,Online.AssociationforComputa-
nization. University of Southern California, Infor- tionalLinguistics.
mationSciencesInstituteLosAngeles.
Piyush Sharma, Nan Ding, Sebastian Goodman, and
ScottMcCloud.1993. Understandingcomics: Thein- Radu Soricut. 2018. Conceptual captions: A
visibleart. WilliamMorrow. cleaned, hypernymed, image alt-text dataset for au-
tomatic image captioning. In Proceedings of the
Edwin G. Ng, Bo Pang, Piyush Sharma, and Radu 56thAnnualMeetingoftheAssociationforCompu-
Soricut. 2020. Understanding guided image cap- tationalLinguistics(Volume1: LongPapers),pages
tioningperformanceacrossdomains. arXivpreprint 2556–2565, Melbourne, Australia. Association for
arXiv:2012.02339. ComputationalLinguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessingSystems,volume30,pages5998–6008.Cur-
ranAssociates,Inc.
RamakrishnaVedantam,CLawrenceZitnick,andDevi
Parikh. 2015. CIDEr: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecogni-
tion,pages4566–4575.
Ramakrishna Vedantam, C. Lawrence Zitnick, and
Devi Parikh. 2014. Cider: Consensus-based image
descriptionevaluation. CoRR,abs/1411.5726.
AnthonyVieraandJoanneGarrett.2005. Understand-
ing interobserver agreement: The kappa statistic.
Familymedicine,37:360–3.
Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu.
2019. Discourse-aware neural extractive text sum-
marization. arXivpreprintarXiv:1910.14142.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger,andYoavArtzi.2020. Bertscore: Eval-
uating text generation with bert. In International
ConferenceonLearningRepresentations.

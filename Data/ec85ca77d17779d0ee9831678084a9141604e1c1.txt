Achieving Conversational Goals with
Unsupervised Post-hoc Knowledge Injection
BodhisattwaPrasadMajumder♣ HarshJhamtani♦
TaylorBerg-Kirkpatrick♣ JulianMcAuley♣
♣DepartmentofComputerScienceandEngineering,UCSanDiego
{bmajumde, tberg, jmcauley}@eng.ucsd.edu
♦SchoolofComputerScience,CarnegieMellonUniversity
jharsh@cs.cmu.edu
Abstract Find me something fun to do around Dialog
Cambridge area in daytime! Context
A limitation of current neural dialog mod- Retrieved Knowledge
There are plenty of museums to visit around
els is that they tend to suffer from a lack of Cambridge. If you love hiking, you can enjoy the trails
alongside the river. Some of my friends like to go the
specificityandinformativenessingeneratedre- centre of the town and catch a movie.
sponses,primarilyduetodependenceontrain-
You can go for a Many prefer to visit museums. You
ing data that covers a limited variety of sce- 🤖 movie. Is there can do hiking around the river if you 🤖
anything else that love nature. Or you can watch a
narios and conveys limited knowledge. One your prefer? movie. Which one do you prefer?
way to alleviate this issue is to extract rele- Initial Response Final Response
vant knowledge from external sources at de-
Figure1: Augmentinginitialresponsefromanexistingdi-
coding time and incorporate it into the dialog alogmodelwithrelevantexternalknowledgeleadstomore
response. In this paper, we propose a post- engagingandinformativeresponsesimprovingthesuccessin
hoc knowledge-injection technique where we achievingtheconversationalgoal(here,findingafunactivity).
first retrieve a diverse set of relevant knowl-
edge snippets conditioned on both the dialog
relevant knowledge at decoding-time. For exam-
history and an initial response from an exist-
ing dialog model. We construct multiple can- ple, in Figure 1, the user is seeking options for a
didate responses, individually injecting each fun activity around Cambridge. While the initial
retrieved snippet into the initial response us- dialogresponsesuggestswatchingamovieasan
ing a gradient-based decoding method, and option,itdoesnotprovideanyinformationbehind
then select the final response with an unsu-
thatchoice.
pervised ranking step. Our experiments in
Weproposeandevaluateanapproachforunsu-
goal-orientedandknowledge-groundeddialog
pervisedknowledgeinjectionintoadialogmodel’s
settings demonstrate that human annotators
judge the outputs from the proposed method
responseatdecodingtime1—notaddressedinany
to be more engaging and informative com- previouswork. Wefirstsamplearesponsefromthe
pared to responses from prior dialog systems. model(trainedondialogdata)conditionedonthe
Wefurthershowthatknowledge-augmentation dialogcontext. Next,weutilizethedialogcontext
promotes success in achieving conversational
andthesampledresponsetoqueryexternalknowl-
goalsinbothexperimentalsettings.
edgesources. Finally,theretrievedknowledgeis
1 Introduction usedtoconstructamoreinformativeandengaging
response (Figure 1). A major advantage of such
Genericresponseswhichlackspecificityhavebeen
post-hoc knowledge injection is its flexibility in
amajorissueinexistingdialogmodels(Hosseini-
addingnewerknowledgesourcesespeciallywhere
Asl et al., 2020; Dinan et al., 2019a). The issue
the success of achieving conversational goals re-
in part stems from bottlenecks in dialog models
lies upon the availability of relevant knowledge.
due to a limited scope of scenarios and access to
Post-hocinjectionalsopromotesefficiencyinNLP
limited knowledge available during training. On
applications(Schwartzetal.,2020;Strubelletal.,
theotherhand,encodingallpossibleworldknowl-
2019): itmitigatestheneedtoretraindialogmodels
edgeattrainingtimeisnotfeasible,andevenun-
toaccommodatedynamicallyevolvingknowledge.
desirable in cases where knowledge sources are
We experiment with two types of knowledge
dynamicallyvarying(Ghazvininejadetal.,2018;
sources: language models, which we treat as
Majumderetal.,2020b;Zhaoetal.,2020;Bruyn
parametricknowledgebases(Petronietal.,2019;
et al., 2020; Kim et al., 2020; Prabhumoye et al.,
2021). One possible approach is to incorporate 1Code:https://github.com/majumderb/poki
2202
raM
22
]LC.sc[
1v99311.3022:viXra
Initial Response N knowledge snippets Relevance-Redundancy tradeoff to Candidate* Final *for each snippet ki B Candidate
xd select B out of N snippets Response x if Final Responses
Knowledge
Fidelity for k i
Dialog Rank w.r.to
Dialog DPP Model ℳ Entailment likelihood
Model ℳ with ℋ and linguistic
ff oo rr Lw Ma r fd lu p ea ns cs y backward pass diversity
with constraints
B
Dialog History
ℋ Knowledge Sources 𝒦 N Dialog History ℋ Final Response
Post-hoc Knowledge Knowledge Selection Constrained Decoding Ranking
Figure2: PipelineofPOKI:Itfirstretrievespost-hocknowledgefromexternalsourcesbasedondialoghistoryandaninitial
responsefromadialogmodel.Thenthemostrelevantanddiverseknowledgesnippetsareselectedfromtheretrievedset.Each
selectedsnippetisindividuallycombinedwiththeinitialresponsethroughconstraineddecodingtogenerateacandidatefinal
response.Atlast,thefinalresponseisselectedviaanunsupervisedrankingstep.NotethatPOKIrequiresnoadditionaltraining.
Brown et al., 2020); and user review datasets 2 Post-hocKnowledgeforDialog
such as Yelp reviews (Hajas et al., 2014) as non-
parametric knowledge sources (§2). Since it is Ourgoalistoconstructadialogresponsebyinject-
possibletogatheralargeamountofrelatedknowl- ing knowledge (from external textual sources) at
edgegivenaquery,weselectarelevantanddiverse decodingtime,withouthavingtoretrainthemod-
(estimatedviainformation-theoreticmeasures)sub- els. Consider a dialog model M from which we
set of knowledge snippets using an unsupervised can sample a dialog response xd given a dialog
method (§3.1). Then, a gradient-based inference historyH. Weshallrefertotheresponsexd sam-
approachisusedtoconstructanupdatedresponse pledfromsuchamodelwithoutanydecodingtime
that incorporates the selected knowledge (§3.2). knowledgeinjectionastheinitialresponse.
Notethatourframeworkdoesnotrequireretrain-
However, as motivated earlier, samples from
ingtheexistingdialogmodel—itonlyreliesupon
suchadialogmodeloftenlackdetail. Toimprove
updatingthemodel’soutputhiddenstatesatdecod-
such responses, we retrieve and incorporate rele-
ingtimeforunsupervisedknowledgeinjection.
vantexternalknowledgek intotheinitialresponse.
We experiment with two scenarios: goal- To achieve our goal, we construct a query using
orientedandknowledge-groundeddialogwherethe bothdialoghistoryH andtheinitialresponsexd,
trainingdatacoversonlyafractionoftheneeded andgatherarelevantknowledgecandidatekfroma
knowledge. Automaticevaluationrevealsthatour knowledgesourceK. Theretrievedsnippetcanpro-
method is capable of generating highly diverse videusefulinformationtotheend-usertoachieve
responses in both settings. In some cases, the theconversationalgoal(see§5.3). Weexploreboth
generated response shows high overlap with the parametric (e.g querying a language model) and
original target response showing that our unsu- non-parametric(e.g.deterministicretrievalusing
pervised method bridges the knowledge gap be- word-overlap)waystoobtainpost-hocknowledge.
tweenavailableknowledgeandhuman-writtenre-
sponsespresentintheexistingdialogcorpus. An
2.1 Parametricknowledgesources
extensive human evaluation confirms that gener-
ated responses are indeed engaging, interesting,
Pretrainedlanguagemodels(PTLM)aretypically
andhuman-likewithoutanylossinfluency.
trained with a vast amount of text that spans a
Topinpointtheusefulnessofknowledgeinjec- diverse range of domains. Petroni et al. (2019);
tion in the above settings, we design a real-time Brownetal.(2020)showedthatsuchPTLMscan
study(§5.3)whereusersinteractwithoursystemto be used as a source of knowledge when queried
reachaconversationalgoal(e.g.planningaholiday withsuitabletextualprompts(e.g.Seattleisfamous
orknowingmoreaboutthesolarsystem). Wefind for ). To use PTLMs in our use-case, we con-
that external knowledge enables users to achieve structusefulpromptsfromdialoghistoryandthe
their goals more efficiently. Additionally, we ob- initialresponse. Weassemblesimplepromptsin-
serve that the our approach of sub-selecting rele- spiredfromvariousknowledge-seekingsituations
vantbutdiverseknowledgeleadstoresponsesthat indialog(Shwartzetal.,2020)suchas[KP]isfa-
promotesuccessinachievingconversationalgoals. mousfor ,HereiswhatIknowabout[KP]: ,
where[KP]isakey-phrase2extractedfromdialog 3.1 Relevance-RedundancyTradeofffor
context. Weusegpt2-largeasthePTLM.For KnowledgeSelection
example,aquery“HereiswhatIknowaboutfun
Ateachturn,weobtainN knowledgesnippetsfrom
things around Cambridge:" results in “There are
both the parametric and non-parametric sources.
plenty of museums to visit around Cambridge. If
WewishtoselectasubsetofB (outofN)relevant
youlovehiking,youcanenjoythetrailsalongside
butdiverseknowledgesnippets.
theriver..."asshowninFigure1. Acompletelist
We define relevance score of a snippet k with
i
ofpromptsisprovidedinAppendixB. Wefinally
respect to the dialog history H using pointwise
rankeachknowledgesnippetkusingthelikelihood
mutualinformation(PMI)asfollows:
obtainedfromthePTLMforaconcatenatedinput
Åp(H|k )ã
ofk anddialoghistoryandchoosethemostlikely. REL = PMI(k ,H) = log i ,
i i
p(H)
2.2 Non-parametricknowledgesources
Thus, a high PMI score would imply a larger se-
External knowledge in the form of a text corpus manticsimilaritybetweenthesnippetk iandH. To
canbeusedasanon-parametricknowledgesource account for redundancy between the snippet pair
availableatdecodingtime. Comparedtoparamet- k i,k j weagainusethePMIscoreasfollows:
ric knowledge sources, such sources do not gen- Åp(k |k )ã
eratetextasknowledgesnippets,butofferthead- RED ij,j>i = PMI(k i,k j) = log j i .
p(k )
j
vantage of high quality and reliability of human
written text. We consider the dialog history and Theredundancyscoreissymmetrici.e. RED ij =
theinitialresponseasaquerytoretrieverelevant RED ji asPMIisasymmetricmeasure.
knowledge instances from the corpus. Next, we Weestimateprobabilities(bothconditionaland
identifythetoprelevantinstancesinthegivencor- marginal)p(.)intheaboveequationsusingGPT2
pus with respect to the constructed query using language model, following past work (Padmaku-
cosinesimilarityonTF-IDFbasedrepresentations marandHe,2021). ThePMImeasureisoftencon-
(Robertsonetal.,1995). sideredbetterthanothern-gram-basedoverlapmet-
ricstomeasurethedegreeofassociationbetween
3 UnsupervisedKnowledgeInjectionin two sentences (Kedzie et al., 2018; Padmakumar
GeneratedDialog and He, 2021). Semantically similar phrases oc-
curinbothsentencesthatcaneasilybeignoredby
Effectivelyutilizingtheretrievedknowledgesnip- overlapbasedmetrics.
pets to construct an enriched dialog response en-
Selection via Determinantal Point Processes.
compassestwomajorchallenges. Firstly,itisnot
To select B knowledge snippets out of N with a
practicaltousepotentiallyhundredsofknowledge
relevance-redundancytrade-off,weuseasubsetse-
snippetsobtainedfromtheretrievalstepforasingle
lectionprocessnamedDeterminantalPointProcess
responsegeneration. Thus,weneedtofindarele-
(DPP)(KuleszaandTaskar,2011). DPPemploysa
vantbutdiversesubsetofthesnippets. Secondly,
non-uniformselectionthatassignslowprobability
thedialogmodelMistrainedtoconditiononlyon
to subsets (here, of knowledge snippets) that are
thedialogcontext,andnotontheexternalknowl-
lessdiversebymodelingtherepulsivecorrelation
edge. Hence,toleveragetheknowledgesnippets,
betweenindependentlyoccurringdatapoints(see
we need a decoding strategy to rewrite the initial
Figure2).
responsexd suchthattheresultingfinalresponse
WebuildanN ×N kernelmatrixD,whichis
xf shouldcloselyfollowtheknowledgesnippetto
real, symmetric and positive semi-definite. The
be injected without a loss in the fluency and con-
diagonalentriesD arepopulatedbythesquared
sistency. Thus,ourmethodrequiresnoadditional ii
relevance score of the i-th knowledge REL and
trainingandonlyassumesalanguagemodeltrained i
the off-diagonal entries D are β× squared re-
on dialog context (i.e. M). We refer to our pro- ij
dundancy scores RED . We adjust β in such a
posed framework (Figure 2) as POKI (Post-hoc ij
waythatD alwaysremainspositivesemi-definite
KnowledgeInjectioninGeneratedDialog).
(moredetailsin(Wilhelmetal.,2018)). Toselect
asubsetofB,aDPPassignsaprobabilityofsam-
2Itpossiblethatalackofkey-phrasesresultsinnoknowl-
edge.Key-phraseextractiondetailsareinAppendixB. plingsuchasubsetproportionaltothedeterminant
of the submatrix D of D, constructed using the θ(z,H)thatpredictstheprobabilityofxf (ideally,
B
indicesofthesubsetteditems. TheDPPprobabil- thehiddenrepresentationzofxf)entailingH. The
ity is geometrically related to the volume of the classifierθ(z,H)isabag-of-wordsclassification
parallelepipedspannedbytheselectedknowledge layerwithhiddenstatesz fromMandfine-tuned
snippets. Diverse knowledge snippets tend to be using the DNLI dataset (Welleck et al., 2019) to
orthogonalintheirspacehencespanlargervolume predict whether the current response is entailed
(KuleszaandTaskar,2012). withpreviousresponsesornot.
Choosing B-size submatrix from N-size D is
Decoding. Inthesubsequentforwardandback-
a combinatorial problem and can become pro-
wardpasses,thehiddenrepresentationz isgradu-
hibitivelycostlywhenN isveryhigh. Hence,we
allyperturbedviagradientascentontherespective
useagreedymethod(Wilhelmetal.,2018)where
objectives. During backward pass, the objective
weinitializetheselectionwiththemostrelevantk
i withconstraintsis
andsubsequentlyselectthenextk thatmaximizes
j
thedeterminantoftheresultantsubmatrix. L(H,k;z) = αlogθ(z,H)−λCE(k,Wz)
3.2 Gradient-basedConstrainedDecoding
with hyperparameters α and λ. We use
forKnowledgeInjection
back-propagation to update z with the gradient
Upon selecting B knowledge snippets, we want ∇ L(H,k;z)whiletheparametersofMremain
z
toindividuallyinjecteachknowledgesnippetinto fixed. Theupdatedlatentrepresentationsofz after
xd to construct a candidate final response xf at thebackwardpassaredenotedaszbw.
inferencetime. AforwardpasswithMisrequiredtoregularize
Previousworkshaveaddressedtheproblemof thehiddenstatesztowardtheoriginaldialogmodel
unsupervised modification of already-generated objective to obtain zfw. Corresponding to the tth
textusinggradient-baseddecoding(Dathathrietal., token, the hidden states for the t+1th time step
2020; Qin et al., 2020) that employs an iterative arecomputedviaaweightedadditionofbackward
procedureconsistingofaforwardandabackward andforwardhiddenstates,i.e.,z = γ×zbw +
(t+1) (t)
pass. The forward pass on the generative model (1−γ)×zfw whereγ ∈ (0,1)isahyperparameter.
(t)
(here, M) encourages fluency of the generated
Duringgeneration,westartbysamplingtheini-
text while the backward pass performs gradient tial response xd with greedy decoding from M.
ascent on certain desired constraints. Note that Thehiddenstatesz (ofxd)areiterativelyupdated
due to the discrete nature of x , it is not pos-
d byalternatebackwardandforwardpasses. Thefi-
sible to directly update it via back-propagation. nalresponseissampledasxf ∼ softmax(Wz/τ).
Therefore, we maintain the sequence of hidden
Thenumberofiterations(= 5)andtheγ (= 0.45)
representations of each output token as z from
werechosenbymaximizingtheZ-normalizedsum
the dialog model. Each output token xd is re-
(t) ofdialogmodelperplexityandlinguisticdiversity
alizedviap(xd (t)) ∼ softmax(Wz (t)/τ),whereτ (%ofdistinctbigrams)inagreedyhyperparameter
is the temperature hyperparameter, W is the out- search. MoredetailsareinAppendixB.
putembeddingmatrix(sharedwiththeinput),and
Wz ∈ RV (V isthesizeofthevocabulary). 3.3 UnsupervisedRankingofCandidate
(t)
FinalResponses
Constraints. Following Majumder et al.
(2021a), we define a knowledge fidelity objec- Several previous works often over-generate and
tive that encourages xf to be minimally differ- use an additional ranking step in order to select
ent from the knowledge snippet k. We achieve thefinalcandidateinunsupervisedtextgeneration
thisbyminimizingthecrossentropyloss(CE)be- (Qin et al., 2020; Shwartz et al., 2020; Paranjape
tween knowledge tokens k ,...,k as labels and Manning, 2021). Similarly, here we want to
(1) (T)
andWz ,...,Wz asthelogits. rank the generated candidate final responses ac-
(1) (T)
We further notice that injected knowledge can cording to the diversity of the generated text as
influencethegenerationinsuchawaythatitcontra- well as the conditional likelihood of generation
dictswithresponsesutteredduringpreviousturns. given the dialog history. For diversity, we mea-
Hence,wealsowantxf tobeentailedwiththedi- surethepercentageofdistinctbigramspresentin
aloghistoryH. Webuildanentailmentclassifier the response. For conditional likelihood, we use
System Acc BLEU BRTSc D-2 ENTR System BLEU BRTSc D-2 ENTR
KCopy 70.1 4.1 62.3 3.16 2.41 KCopy 13.4 74.3 3.64 3.12
SimpleTOD(2020) 70.1 15.0 79.2 0.56 0.90 KGuide(2017) 16.7 71.5 2.54 2.12
SimpleTOD+(2021) 69.8 12.1 68.1 0.81 1.11 KGround(2019) 18.3 72.5 2.87 2.35
Arranger(2021) 70.2 12.3 68.5 0.93 1.15 BART(2020a) 19.8 73.4 2.97 2.55
Rewriter(2021) 70.2 12.1 69.4 1.03 1.45 RAG(2020b) 19.9 73.1 1.03 1.45
POKI 71.1 13.7 74.5 3.78 2.67 POKI 19.4 76.8 3.65 3.44
w/oEntailment 69.9 10.9 67.8 3.67 2.56 w/oEntailment 18.1 74.2 3.17 3.39
w/oKwFidelity 70.0 12.3 71.2 0.95 1.19 w/oKwFidelity 18.8 73.3 2.75 2.54
Gold 100 100 100 0.78 0.86 Gold 100 100 2.98 2.59
Table 1: Automatic metrics on the test set of MultiWoZ. Table 2: Automatic metrics on the test set of Wizard-of-
Differencebetweenboldandnon-boldnumbersisstatistically Wikipedia.Differencebetweenboldandnon-boldnumbersis
significant(p<0.001). statisticallysignificant(p<0.001).
thepre-trainedGPT2modeltoobtainthelogprob- 4.2 BaselinesandAblations
ability when the dialog history, followed by the
generatedresponse,passedasaconcatenatedinput. BaselinesforMultiWOZ. ForMultiWOZ,we
Since these two scores can have varied scale, we consider several baselines following (Sun et al.,
performZ-normalizationontheindividualscores 2021) for knowledge injection. First, we use the
andaddthemtoobtainasinglescoreforranking. current state-of-the-art model, SimpleTOD, for
The highest ranked candidate response is finally goal-oriented dialog (Hosseini-Asl et al., 2020).
renderedtotheuser. Sun et al. (2021) extends SimpleTOD by adding
chitchatcandidatestodialoghistoriesduringtrain-
4 ExperimentalSetup
ing. Theyalsohaveothervariantsthateithercon-
catenate output from SimpleTOD and candidate
4.1 ScenariosandDatasets
chitchats(Arranger)orrewritebycombiningboth
We experiment with two dialog scenarios: goal-
output and chitchat snippets (Rewriter). We also
orientedandknowledgegrounded. Bothsetupsare
haveatrivialbaseline(KCopy)whichappendsthe
knowledgeintensivebutthetrainingdatainsuch
retrievedknowledgesnippetkfromPOKIwiththe
setupsoftencontainsonlyafractionoftheneeded
initialresponsex .
d
knowledge. Forthegoal-orientedsetting,weuse
the Multi-domain Wizard-of-Oz (Budzianowski Baselines for WoW. For WoW, we use
etal.,2018)dataset. Forknowledgegroundeddia- two current-best knowledge-grounded models,
log,weusetheWizard-of-Wikipedia(Dinanetal., KGround (Wolf et al., 2019) and BART (Lewis
2019b)dataset. MoredetailsareinAppendixA. etal.,2020a)thatconcatenatetheassociatedknowl-
edge snippets (present in WoW) and the dialog
Multi-domain Wizard-of-Oz (MultiWOZ) is
historyasinputstogeneratetheresponsewithsu-
a multi-domain dialog dataset (we use v2.0
pervision. KGuide (Zhao et al., 2017) and RAG
(Hosseini-Asl et al., 2020)) consisting of goal-
(Lewis et al., 2020b) have an additional knowl-
orientedhuman-humanconversations. Thedataset
edge selection step modeled by a latent variable
spanssevendomains(restaurant,train,attraction,
before response generation similar to knowledge
hotel, taxi, hospital, police) and contains 10,438
groundedmodels. WealsousetheKCopybaseline,
dialogswith13.68averageturns. Since,wedonot
asdescribedforMultiWOZ.
needanytrainingdata,weonlyuseanevaluation
set(of7Kutterances).
VariantsofPOKI. Toinvestigatetheimpactof
Wizard-of-Wikipedia (WoW) is a knowledge variousdecodingconstraintsinPOKI,weconsider
groundeddialogdatasetwhichinvolvesretrieving thefollowingtwovariantsofPOKI—w/oEntail-
relevantknowledgefromWikipedia,readingand ment and w/o Knowledge (Kw) Fidelity (§3.2).
conditioning on it, and finally generating dialog In POKI, we use SimpleTOD as the base dialog
responses(Dinanetal.,2019b). Thedatasetcon- model in goal-oriented scenarios and use BART
tains 201K utterances from 22K dialogues span- (which is a state-of-the-art model for WoW) as
ning1300diversetopics,fromwhichweuseonly thebasedialogmodelintheknowledge-grounded
thetestset. TheassociatedWikipediaknowledge scenario. ForallvariantsofPOKI,weusegradient-
basehas5.4Marticlesand93Msentences. basedinferencefordecodingthefinalresponse.
POKIvs SimpleTOD Rewriter w/oEntailment w/oKwFidelity Gold
Criteria win loss κ win loss κ win loss κ win loss κ win loss κ
Coherent 93.2 4.4 0.76 85.6 10.2 0.75 98.7 0.8 0.72 77.8 17.8 0.78 26.2 34.4 0.69
Engaging 94.3 4.5 0.78 89.7 7.9 0.79 98.7 0.6 0.80 71.5 20.5 0.80 42.4 37.4 0.78
Interesting 92.7 5.4 0.72 91.2 8.3 0.73 88.6 8.9 0.68 98.7 0.8 0.75 49.7 45.6 0.67
Humanlike 85.4 10.7 0.68 87.4 7.3 0.65 61.9 30.5 0.71 81.7 14.0 0.74 29.7 37.8 0.66
RAG BART w/oEntailment w/oKwFidelity Gold
Coherent 95.4 4.5 0.78 88.5 9.6 0.72 94.3 3.4 0.68 83.6 10.7 0.65 23.8 25.3 0.73
Engaging 89.3 7.7 0.72 87.8 8.3 0.71 97.7 0.8 0.70 71.5 25.4 0.69 25.4 26.7 0.73
Interesting 96.3 3.5 0.74 83.3 9.9 0.75 79.8 17.2 0.70 93.5 4.5 0.71 35.9 37.8 0.76
Humanlike 91.4 7.1 0.68 92.4 6.5 0.66 84.5 10.5 0.67 81.8 13.5 0.71 42.3 41.9 0.68
Table3: Pairwisecomparison(%win/losscases,tienotreported)betweenresponsesfromPOKIandfromotherbaselinesas
wellasgroundtruth.Differencebetweenboldandnon-boldnumbersisstatisticallysignificant(p<0.001).κdenotesCohen’s
Kappa(Cohen,1960)betweenapairofannotators.CompletedetailsofthehumanevaluationareinAppendixC.
5 ResultsandDiscussion the knowledge adherence constraint still remains
asignificantfactorforincreasingdiversity,oneof
5.1 AutomaticEvaluation
themaingoalsofknowledgeinjection. ForWoW,
Ourprimarygoalistogenerateresponsesenriched weinsteadseePOKIoutperformevenBART(pre-
with relevant external knowledge. Arguably, a viousSOTA)intermsofBERTScorewheninjected
system which can effectively leverage additional withexternalknowledgeindicatingtheneedofthe
knowledgeatdecodingtimeshouldgeneratemore externalknowledgeformodelingWoWdialogs.
diverseresponses. Wemeasurepercentageofdis-
5.2 HumanEvaluation
tinctbigramsasDistinct-(D-2)(Lietal.,2016)and
Weconductacomparativehumanevaluationwith
geometricmeanofentropyvaluesofempiricalfre-
quency distributions of n-grams (n = 1,2,3) as 300 samples to evaluate the quality of gener-
ateddialogresponsesfollowingACUTE-Eval(Li
Entropy(ENTR)(Jhamtanietal.,2018)fordiver-
etal.,2019). Weshowageneratedresponsefrom
sity. Additionally,wereportoverlapbetweengen-
POKI to an annotator with its associated dialog
erated responses and corresponding ground truth
history to annotate if knowledge injection makes
asperBLEUandBERTScore(BRTSc). ForMulti-
thefinalresponsemoreengaging,interestingand
WOZ,wealsoreportthefinalgoalaccuracy(Acc)
humanlike compared to a baseline response. As
following(Hosseini-Asletal.,2020).
sanity check, we also investigate if the response
MultiWOZ. Table1shows POKI outperforms
remaincoherent afterknowledgeinjection. Each
allthebaselinesintermsofdiversityofgenerated
sampleisevaluatedbytwoannotators3.
responses. More importantly, we see POKI pro-
MultiWOZ. Table3recordsthepairwisecom-
motes accuracy of reaching the final dialog state
parison showing POKI consistently outperforms
i.e. the goal. For ablated versions of POKI, we
baselinesonallcriteria. ResponsesfromPOKIare
find the entailment constraint has little effect on
more engaging and interesting compared to Sim-
diversitywhiledroppingtheknowledgeadherence
pleTODandRewriter,demonstratingthatgradient-
constraintnegativelyinfluencesaccuracyanddiver-
baseddecodingiseffectiveforknowledgeinjection.
sity. All variants of SimpleTOD and all versions
InPOKI,entailmentconstraintmostlyinfluences
ofPOKIshowdeparturefromtheresultsobtained
coherencewhereasknowledgefidelityconstraintis
by SimpleTOD on BLEU and BERTScore since
importantforengagingnessandinterestingness.
all of these versions add external knowledge that
were not explicitly present in the data. However, WoW. Table 3 shows POKI outperforms base-
weobservethatthedepartureisnotsignificantand linesthatusegroundingknowledgeduringtraining
POKIachievesamuchcloserBERTScoretoSim- inallcriteriashowingthatexternalknowledgecan
pleTODcomparedtobaselines. beusefulevenintheknowledge-groundedsetting
tomaketheconversationengagingandinteresting.
WoW. DespiteallsystemsforWoWuseknowl-
It also indicates the limitation of the training sig-
edgeexplicitlyintheknowledge-groundeddialog
nal or lack of access to sufficient knowledge and
generation task, Table 2 shows POKI generates
themostdiverseresponses. SimilartoMultiWOZ, 3MoredetailsofthesetupareinAppendixC.
ZOWitluM
WoW
👧 : I need a place to eat that is cheap. Do you have a location
🤖: Do you have a location preference? I have several preference? I have several None
options for inexpensive restaurants. options for you.
👧 : Center of the town in Cambridge. Asian cuisines such as Chinese or Indian
I can book a Chinese or
🤖: I think you can go with Asian cuisines such as Indian restaurant near cuisines are inexpensive. They are value for
Chinese or Indian cuisines. They are value for money centre for you. money since they are known for their great
since they are known for their great taste. taste.
👧 : Do you have any restaurants that serve English food? I do not have an inexpensive Indian food is very popular in the centre area,
🤖: I do not have an inexpensive restaurant that serves restaurant that serves Cambridge. I know many friends who like
English food in the centre area. Indian food i very English food in the centre English food also enjoy Indian food. Indian food
popular in the centre area. Many people who like English area. is great value for money.
food also enjoy Indian food.
👧 : Awesome. I can go with Indian food. I can book a reservation for Kohinoor has a vibrant environment in the
🤖: Great. I can book a reservation for you at Kohinoor. you at Kohinoor. The table evening. They are best known for their starters.
The table will be reserved for 15 minutes. They have a will be reserved for 15 Do check them out. Also Indian sweets are
vibrant environment and they are known for their starters. minutes. great as desserts.
Figure 3: POKI converses with a user who is looking for some restaurant options (left column). In each turn, a
knowledgesnippet (rightcolumn)isinjectedintoan initialresponse (middlecolumn).MoreexamplesareinAppendixD.
roomforimprovementintermsofhowknowledge 5.3 UserStudyforEffectivenessof
isutilized. Alargegapinwinpercentagesinfavor KnowledgeInjection
of POKI for evaluating how ‘humanlike’ is a re-
Relevant knowledge injection has the benefit of
sponsewhencomparedtostate-of-the-artmethods
adding more justification to terse dialog outputs
suggestsknowledgeinjectionleadstomorenatural
andhenceinfluencingthetaskoutcomepositively.
conversation. Heretoo,bothdecodingconstraints
Mirroringobservationsfrom(Ghandehariounetal.,
showsimilartrendstoMultiWOZ.
2019), a real-time full conversation evaluation is
Qualitative Analysis. Figure 3 shows a con- needed to investigate if POKI could achieve the
versationby POKI withauserwhoseekstofind conversationalgoalanybetterthanbaselines.
restaurantoptionsaroundCambridge. Weobserve Werecruited60usersforthisstudy4. Onehalfof
thatinmostoftheturnstheinjectedknowledgeap- theusersinteractedwithPOKI,whiletheotherhalf
pearedasanadditionaljustificationovertheinitial interacted with the best baseline model that does
responses making the dialog engaging and effec- notaugmentdialogresponseswithexternalknowl-
tivetoreachtheuser’sgoal(alsonotedbyhuman edge. Weconstructaspeculativegoalforeachuser
judgesin§5.3). Forexample,inturn3,weobserve toaccomplishviatheconversation. Weallowusers
thataddingtheextrainformationaboutIndiancui- to end the conversation any time they would like
sinehelpedusertoreachaconclusionwhentheir and ask them whether the system helped them to
originalchoiceofEnglishcuisinewasabsent. reachtheirconversationgoalalongwithadditional
commentstojustifytheirannotation. Userswhoin-
Effect of Response Length. Qualitatively, as
teractedwithaknowledge-augmentedsystemalso
seeninFigure3,responsesgeneratedbyPOKIare
asked if the system provided any knowledge that
longerthanthosefromtheinitialresponsedueto
user has not explicitly asked for but indeed the
the post-hoc knowledge injection. In the human
extrainformationhelpedthemtoreachtheconver-
evaluationsample,wefoundthat37%ofresponses
sational goal (Majumder et al., 2021b). Finally,
from POKI are similar or smaller in length com-
wealsoaskiftheywouldliketoengagewiththe
paredtoresponsesfromthebestbaseline. Wein-
systemtheyinteractedwithinfuture.
vestigateifresponselengthactedasaconfounding
Forgoal-orienteddialog,weconstructspecula-
factor during human evaluation. Among all the
tivegoals(e.g.lookingforentertainmentoptions)
caseswhere POKI waslost overabaseline,45%
manually from the ground truth for 300 dialog
(±2%whenbootstrappedwith1000subsetsofsize
samples. Since we are not using the underlying
50)ofresponsesfromPOKIwerelongerthanthose
databases,wemadesurespeculativegoalsdonot
fromthecomparingbaseline. Amongwincasesfor
require specific information (e.g. booking avail-
POKI,weobserve49%(±3%whenbootstrapped
ability, flight information, etc.). For knowledge-
with1000subsetsofsize50)POKIresponseswere
groundeddialog,weprovidetheintendedtopicof
longerthanthosefromthecomparingmethod. This
indicates that human users did not only choose
4Moredetailsoftheparticipantsandthestudysetuparein
longerresponsesasbetter. AppendixC.
MultiWOZ #turns↓ Goal Know Woulduse Relevant Factual BRTScforWoW
Source Random DPP Random DPP Random DPP
Rewriter 8±2 69% 35% 56%
POKI 4±3 86% 84% 76% Parametric 82% 89% 65% 83% 74.2 81.3
Non-parametric 81% 83% 97% 98% 65.2 76.8
WoW #turns↑ Goal Know Woulduse
Table5: Evaluationforthequalityoftheknowledgesnippets
BART 10±2 56% 70% 48%
forrandomandDPP-basedselection.
POKI 16±3 76% 89% 71%
Table 4: Real-time user study with average # of turns for
System MultiWOZ WoW
successfulgoalcompletion,%oftimethegoalwasachieved,
%ofsuccesscasesuserswerehelpedbyanadditionalknowl- Supervised 17.6±5.2ms 23.6±4.6ms
edge(Know)thatwasnotexplicitlyaskedtoreachtheirgoal, PPCM(2020) 30.9±7.5ms 32.6±4.2ms
andifuserswouldliketousethesysteminfuture. POKI 34.2±8.4ms 35.7±5.7ms
POKI,onlydecoding 31.6±2.7ms 32.3±3.4ms
Table6: Meanandstd.errorofclock-timetakenpertoken
discussion(e.g.sciencefiction)presentinthedata;
thespeculativegoalhereistoknowmoreabout,or
tohaveanengagingconversationaboutthetopic. selected knowledge5. We perform a human eval-
uation on 200 snippets to measure the relevance
Results. Firstofall,wefindthatPOKIisunan-
andthefactualcorrectnessintwoscenarios: when
imouslypreferredbyuserscomparedtothebase-
werandomlyselectaretrievedsnippetorselectvia
line during the user study. More importantly, we
DPP.InTable5,weseethattheparametricknowl-
seethatwhentheusersuccessfullyaccomplished
edge source (gpt2-large) generates more rel-
their goal, 84% of those times they found the ad-
evant knowledge snippets than a non-parametric
ditional knowledge helpful in the goal-oriented
one. We attribute this to 1) a large and diverse
setting (MultiWOZ) as compared to a baseline
dataset(webtext)usedduringpretrainingofgpt2
(Rewriter) that did not use any external knowl-
ascomparedtoyelpreviews(restricteddomains)
edge. Mostimportantly,POKItakessignificantly
weusedforretrieval,and2)thelimitedrecallofrel-
fewer turns for users to accomplish the goal as
evantknowledgewhenusingword-overlapbased
comparedtoRewriterimplicitlyindicatinginjected
retrieval. However,largelanguagemodelsarestill
knowledge(weobservehighcorrelation,0.67)con-
pronetogeneratenon-factualknowledge. Weob-
tributestowardmoreefficientconversations.
serve that DPP-based selection in POKI is able
For the knowledge-grounded setting (WoW),
tosub-selectmorefactualknowledgewhichthen
both BART and POKI have access to external
positivelyinfluencesthefinalresponsequality. For
knowledge sources. However, 89% (compared
WoW,wealsocomparetheselectedsnippetswith
to 70%) of success scenarios were directly influ-
thegoldknowledgeavailableinthedatasetthatin
encedbytheadditionalpost-hocknowledge. For
turnshowhighfidelityintermsofBERTScore.
knowledge-groundeddialog,alongerconversation
isindicativeofengagingnessonaparticulartopic TimeComplexity. Madottoetal.(2020)shows
(Gopalakrishnanetal.,2019),henceuserspreferred that iterative gradient-based decoding could be
toconversewithPOKIformoreturnsascompared slower than generating response using single for-
toaBARTbaseline. Wequoteacommentfroma wardpassfromanexistingmodel. Whenwebench-
user who found a conversation about the Korean mark POKI inanNvidia2080TiGPU,inTable6,
culture with POKI was particularly engaging— we see that knowledge generation (or retrieval)
“Before this conversation, I had less knowledge could be a computational bottleneck for POKI.
about Korean movies and art-forms. This gave Howeverthegreedyselectionandtheconstrained
me a new perspective and a handful of popular decodingstepdonotaddsignificantcomputational
opinionstolookatit.”. load. Furthermore,POKI’sperformanceiscompa-
rablewithPPCM(Madottoetal.,2020)—amore
efficientversionofgradient-baseddecoding. The
5.4 Discussion
efficiencyoftheknowledgeretrievalstepcanbeim-
provedwithbetterindexing(Johnsonetal.,2021)
Performance of Knowledge Selection. The
whichweleaveasafuturework.
knowledgeselectionstepinPOKIactsaninforma-
tionbottleneckwherethequalityofthegenerated
5Astatisticalanalysisonnumberofknowledgesnippets
response directly depends on the quality of the retrieved/generatedandselectedisprovidedinAppendixB.
6 RelatedWork References
Knowledge grounded dialog datasets such as LeonardAdolphs, KurtShuster, JackUrbanek, Arthur
Szlam, and Jason Weston. 2021. Reason first, then
Wizard-of-Wikipedia (Dinan et al., 2019a) and
respond:Modulargenerationforknowledge-infused
Topical chat (Gopalakrishnan et al., 2019) typi-
dialogue. CoRR,abs/2111.05204.
cally consist of dialog responses paired with rel-
Tom B. Brown, Benjamin Mann, Nick Ryder, et al.
evant knowledge available as collected annota-
2020. Language models are few-shot learners. In
tions. Hence, models trained on such datasets
NeurIPS.
arerestrictedtotheknowledgesourcestheywere
Maxime De Bruyn, Ehsan Lotfi, Jeska Buhmann, and
exposed to at training time. Past work (Sun
Walter Daelemans. 2020. BART for knowledge
et al., 2021; Majumder et al., 2020a; Su et al.,
grounded conversations. In Converse@KDD, vol-
2020; Komeili et al., 2021; Adolphs et al., 2021; ume2666.CEUR-WS.org.
Ghazvininejadetal.,2018;Tuanetal.,2020;Lewis
Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang
etal.,2020c;Guuetal.,2020)haslookedintoin-
Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-
jecting extra knowledge sources at training time
madan,andMilicaGasic.2018. Multiwoz-Alarge-
inabidtoaddknowledgenotavailableoriginally scale multi-domain wizard-of-oz dataset for task-
as paired to dialog responses. However, such ap- orienteddialoguemodelling. InEMNLP.
proaches require re-training the model if some
Ricardo Campos, Vítor Mangaravite, Arian Pasquali,
new knowledge source were to be used. More- Alípio Jorge, Célia Nunes, and Adam Jatowt. 2020.
over,whilepreviousworkfocusesonjustimprov- Yake! keyword extraction from single documents
usingmultiplelocalfeatures. InformationSciences,
ing specificity of dialog response using external
509.
knowledge,wealsostudytheeffectofadditional
knowledgeinachievingconversationalgoals. Jacob Cohen. 1960. A coefficient of agreement for
nominalscales. Educationalandpsychologicalmea-
Improvingthediversityofdialogresponsesby
surement,20(1):37–46.
using diversity-promoting sampling has been ex-
plored in past work (Fan et al., 2018; Holtzman SumanthDathathri,AndreaMadotto,JaniceLan,Jane
et al., 2020). We use a gradient-based decoding Hung,EricFrank,PieroMolino,JasonYosinski,and
RosanneLiu.2020. Plugandplaylanguagemodels:
method, building on past work in this direction
Asimpleapproachtocontrolledtextgeneration. In
(Dathathri et al., 2020; Qin et al., 2020; Madotto ICLR.
etal.,2020;Majumderetal.,2021a). However,we
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
propose new objectives to inject post-hoc knowl-
Fan, Michael Auli, and Jason Weston. 2019a. Wiz-
edgeobtainedbasedonalreadygenerateddialog—
ard of wikipedia: Knowledge-powered conversa-
anunsupervisedknowledgeinjectionmethodthat tionalagents. InICLR.
hasnotbeenexploredsofar.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, MichaelAuli, andJasonWeston.2019b. Wiz-
7 Conclusion
ard of wikipedia: Knowledge-powered conversa-
tionalagents. InICLR.
Weproposeaframeworkforunsupervisedknowl-
edge injection into dialog responses. We show AngelaFan,MikeLewis,andYannN.Dauphin.2018.
thatknowledgecanbeobtainedpost-hocfromany Hierarchicalneuralstorygeneration. InACL.
knowledgesourcesthatcanimproveusers’ability
Guillaume Gautier, Guillermo Polito, Rémi Bardenet,
toreachtheirconversationalgoalmoreeffectively. andMichalValko.2019. DPPy:DPPSamplingwith
In future, our idea can be generalized to setups Python. Journal of Machine Learning Research -
whereexternalknowledgecanjustifymodel’spre- Machine Learning Open Source Software (JMLR-
MLOSS).
dictionssuchasconversationalrecommendation.
Asma Ghandeharioun, Judy Hanwen Shen, Natasha
Acknowledgements
Jaques, Craig Ferguson, Noah Jones, Àgata
Lapedriza, and Rosalind W. Picard. 2019. Approx-
Wethankanonymousreviewersforprovidingvalu-
imatinginteractivehumanevaluationwithself-play
ablefeedback. BPMispartlysupportedbyaQual- foropen-domaindialogsystems. InNeurIPS.
commInnovationFellowship,aFriendsoftheIn-
Marjan Ghazvininejad, Chris Brockett, Ming-Wei
ternationalCenterFellowship–UCSanDiego,NSF
Chang,BillDolan,JianfengGao,Wen-tauYih,and
Award#1750063,andMeetElise.
MichelGalley.2018. Aknowledge-groundedneural
conversationmodel. InAAAI.
Karthik Gopalakrishnan, Behnam Hedayatnia, Tim Rocktäschel, Sebastian Riedel, and Douwe
Qinglang Chen, Anna Gottardi, Sanjeev Kwa- Kiela. 2020b. Retrieval-augmented generation for
tra, Anu Venkatesh, Raefer Gabriel, and Dilek knowledge-intensiveNLPtasks. InNeurIPS.
Hakkani-Tür. 2019. Topical-chat: Towards
knowledge-grounded open-domain conversations. Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
InInterspeech. tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- Tim Rocktäschel, Sebastian Riedel, and Douwe
pat,andMing-WeiChang.2020. REALM:retrieval- Kiela. 2020c. Retrieval-augmented generation for
augmented language model pre-training. CoRR, knowledge-intensiveNLPtasks. InNeurIPS.
abs/2002.08909.
JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,
Peter Hajas, Louis Gutierrez, and Mukkai S. Krish- and Bill Dolan. 2016. A diversity-promoting ob-
namoorthy.2014. Analysisofyelpreviews. CoRR, jective function for neural conversation models. In
abs/1407.1443. NAACLHLT.
AriHoltzman, JanBuys, LiDu, MaxwellForbes, and Margaret Li, Jason Weston, and Stephen Roller. 2019.
Yejin Choi. 2020. The curious case of neural text ACUTE-EVAL: improved dialogue evaluation with
degeneration. InICLR. optimized questions and multi-turn comparisons.
CoRR,abs/1909.03087.
EhsanHosseini-Asl,BryanMcCann,Chien-ShengWu,
Semih Yavuz, and Richard Socher. 2020. A sim- AndreaMadotto,EtsukoIshii,ZhaojiangLin,Sumanth
ple language model for task-oriented dialogue. In Dathathri, and Pascale Fung. 2020. Plug-and-play
NeurIPS. conversationalmodels. InFindingsofEMNLP.
Harsh Jhamtani, Varun Gangal, Eduard Hovy, Gra- Bodhisattwa Prasad Majumder, Taylor Berg-
ham Neubig, and Taylor Berg-Kirkpatrick. 2018. Kirkpatrick, Julian J. McAuley, and Harsh
Learning to generate move-by-move commentary Jhamtani. 2021a. Unsupervised enrichment of
for chess games from large-scale social forum data. persona-grounded dialog with background stories.
InACL2018. InACL.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Bodhisattwa Prasad Majumder, Harsh Jhamtani, Tay-
Billion-scale similarity search with gpus. IEEE lorBerg-Kirkpatrick,andJulianJ.McAuley.2020a.
Trans.BigData. Like hiking? you probably enjoy nature: Persona-
groundeddialogwithcommonsenseexpansions. In
ChrisKedzie,KathleenR.McKeown,andHalDaumé EMNLP.
III.2018. Contentselectionindeeplearningmodels
ofsummarization. InEMNLP. BodhisattwaPrasadMajumder,ShuyangLi,JianmoNi,
and Julian J. McAuley. 2020b. Interview: Large-
Byeongchang Kim, Jaewoo Ahn, and Gunhee Kim. scale modeling of media dialog with discourse pat-
2020. Sequential latent knowledge selection for ternsandknowledgegrounding. InEMNLP.
knowledge-grounded dialogue. In ICLR. OpenRe-
view.net. Bodhisattwa Prasad Majumder, Sudha Rao, Michel
Galley, and Julian J. McAuley. 2021b. Ask what’s
Mojtaba Komeili, Kurt Shuster, and Jason Weston. missing and what’s useful: Improving clarifica-
2021. Internet-augmented dialogue generation. tion question generation using global knowledge.
CoRR,abs/2107.07566. NAACL.
Alex Kulesza and Ben Taskar. 2011. k-dpps: Fixed- VishakhPadmakumarandHeHe.2021. Unsupervised
sizedeterminantalpointprocesses. InICML.Omni- extractivesummarizationusingpointwisemutualin-
press. formation. InEACL.
Alex Kulesza and Ben Taskar. 2012. Determinantal AshwinParanjapeandChristopherD.Manning.2021.
pointprocessesformachinelearning. Found.Trends Human-like informative conversations: Better ac-
Mach.Learn.,5(2-3):123–286. knowledgements using conditional mutual informa-
tion. InNAACL-HLT.
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Levy, Veselin Stoyanov, and Luke Zettlemoyer. Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,
2020a. BART:denoisingsequence-to-sequencepre- and Alexander H. Miller. 2019. Language models
trainingfornaturallanguagegeneration,translation, asknowledgebases? InEMNLP-IJCNLP.
andcomprehension. InACL.
Shrimai Prabhumoye, Kazuma Hashimoto, Yingbo
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik- Zhou, Alan W. Black, and Ruslan Salakhutdi-
tus, Fabio Petroni, Vladimir Karpukhin, Naman nov. 2021. Focused attention improves document-
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, groundedgeneration. InNAACL-HLT.
LianhuiQin,VeredShwartz,PeterWest,ChandraBha- A Datasets
gavatula, Jena D. Hwang, Ronan Le Bras, Antoine
Bosselut,andYejinChoi.2020. Backtothefuture: MultiWOZ. To compare with previous works,
Unsupervisedbackprop-baseddecodingforcounter- weuseMultiWoz2.0following(Hosseini-Asletal.,
factual and abductive commonsense reasoning. In
2020). Notethatwedonotneedanytrainingdata
EMNLP.
forourmodelssinceweperformpost-hocknowl-
Stephen E. Robertson, Steve Walker, and Micheline edgeinjection.
Hancock-Beaulieu. 1995. Large test collection
experiments on an operational, interactive system: WoW For Wizard-of-Wikipedia, all baselines
Okapi at TREC. Inf. Process. Manag., 31(3):345– andtheoriginaldialogmodelfor POKI useavail-
360.
ablepairedknowledgepresentinthetrainingdata
Roy Schwartz, Jesse Dodge, Noah A. Smith, and (notapartofourpipeline). However, POKI addi-
Oren Etzioni. 2020. Green AI. Commun. ACM, tionally uses the external knowledge snippets se-
63(12):54–63.
lectedviaDPP.
Vered Shwartz, Peter West, Ronan Le Bras, Chandra
B ImplementationDetails
Bhagavatula, and Yejin Choi. 2020. Unsupervised
commonsensequestionansweringwithself-talk. In
We open-source our code at: https://github.
EMNLP.
com/majumderb/poki. Weusethepubliclyavail-
Emma Strubell, Ananya Ganesh, and Andrew McCal- able implementation6 for DPP (Gautier et al.,
lum. 2019. Energy and policy considerations for
2019).
deeplearninginNLP. InACL.
We obtain the MultiWOZ 2.0 from the official
Hui Su, Xiaoyu Shen, Sanqiang Zhao, Xiao Zhou, release 7. Similarly, we obtain the Wizard-of-
Pengwei Hu, Randy Zhong, Cheng Niu, and Jie Wikipedia from ParlAI repository 8. We adapted
Zhou. 2020. Diversifying dialogue generation with
codesfromoriginalPPLM(Dathathrietal.,2020)
non-conversationaltext. InACL.
repository9 andmodifiedthemforourownobjec-
Kai Sun, Seungwhan Moon, Paul A. Crook, Stephen tivefunction. WeobtainedtheYelpreviewdataset
Roller, Becka Silvert, Bing Liu, Zhiguang Wang,
fromtheofficialwebsite10. Yelpdatasetcontains
Honglei Liu, Eunjoon Cho, and Claire Cardie.
8,635,403 reviews. For diversity calculation (in
2021. Adding chit-chats to enhance task-oriented
dialogues. NAACL. automatic evaluation), we use NLTK11 to extract
n-grams.
Yi-Lin Tuan, Wei Wei, and William Yang Wang.
2020. Unsupervised injection of knowledge into
Network architecture For MultiWOZ, we use
dialogue generation via language models. CoRR,
the SimpleTOD12 as the base model. Whereas
abs/2004.14614.
for WoW, we use BART13 as the base model.
Sean Welleck, Jason Weston, Arthur Szlam, and For the parametric knowledge source, we use
Kyunghyun Cho. 2019. Dialogue natural language
gpt2-large14.
inference. InACL.
Hyperparameters POKI does not require any
MarkWilhelm,AjithRamanathan,AlexanderBonomo,
Sagar Jain, Ed H. Chi, and Jennifer Gillenwater. training since we perform gradient-based decod-
2018. Practical diversified recommendations on ing at the inference time. For hyperparameters
youtube with determinantal point processes. In involvedinthedecodingstage, wemaximizethe
CIKM.ACM.
6https://github.com/guilgautier/DPPy
Thomas Wolf, Victor Sanh, Julien Chaumond, and 7https://github.com/budzianowski/
Clement Delangue. 2019. Transfertransfo: A trans- multiwoz
ferlearningapproachforneuralnetworkbasedcon- 8https://parl.ai/projects/wizard_of_
versationalagents. CoRR,abs/1901.08149. wikipedia/
9https://github.com/uber-research/PPLM
Tiancheng Zhao, Ran Zhao, and Maxine Eskénazi. 10https://www.yelp.com/dataset
2017. Learning discourse-level diversity for neural 11https://www.nltk.org/_modules/nltk/
dialog models using conditional variational autoen- util.html
coders. InACL. 12https://github.com/salesforce/
simpletod
Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, 13https://huggingface.co/transformers/
Dongyan Zhao, and Rui Yan. 2020. Knowledge- model_doc/bart.html
grounded dialogue generation with pre-trained lan- 14https://huggingface.co/transformers/
guagemodels. InEMNLP. model_doc/gpt2.html
Z-normalizedsumofdialogmodelperplexityand (Holtzman et al., 2020), p = 0.95) for each key-
linguisticdiversity(%ofdistinctbigrams)ofthe phrase extracted from an input instance (dialog
generated response in a greedy fashion to select history+initialresponse). Afterknowledgeselec-
thebestvalues. Forourbestmethod,inobjective tionbyDPP,onanaverage(overvalidationset),5
function L, we use α as 1 and λ as 1. We keep snippetswereselectedforMultiWozand8snippets
generation length to be 100 to encourage longer wereselectedforWoW.
generations. Wetraintheentailmentclassifierus-
ingcodefromPPLMrepository15. Theweightγ C HumanEvaluationandUserStudy
formixingforwardandbackwardpasseswassetto Setup
0.45. Werun5backward-forwardpassestoobtain
acandidatefinalresponse. Human Evaluation We hired two Anglophone
(LifetimeHITacceptance%>85)annotatorsfor
Filtering knowledge candidates from PTLMs everytestsample. Figure4showsasampleques-
Our initial experiments suggests that that knowl- tionforthepairwisecomparisonbetweenresponse
edge generated from PTLMs can be inappropri- generatedbyPOKIandabaselineforinformative-
ate (contains bias or toxic content) and mislead- ness. The exact formulations for all criteria are
ing/nonfactual. Sunetal.(2021)collectedannota- providedasbelow:
tions of dialog responses with labels positive
(useful, social), negative (inappropriate and
• Coherent: Whichversionismoreconsistent
misleading). Welearnabinaryclassifiertoclassify
withthedialoghistory?
aknowledgesnippetaspositiveornegativeanduse
• Engaging: Which version is more likely to
itasafilteringcriteria.
hold your attention and make you want to
hearmore?
Key-phrase extraction Given a sentence from
• Interesting: Whichversionarousesyourcu-
the context, we first extract n-gram (n ∈ 1,2,3,4)
riosityortellsyousomethingneworuseful?
key-phrasesusingYAKE(Yet-Another-Keyword-
• Humanlike: Which version is more natural
Extractor) (Campos et al., 2020) and retain only
andpersonable?
thosethatcontainatleastanoun.
Prompts Wecuratedpromptsinspiredbyvarious Alldifferencesinvaluesfromhumanevaluations
knowledge-seeking situations (such as for: more
aresignificantwithp < 0.05frombootstraptests
information,opinion,review)(Shwartzetal.,2020) on 1000 subsets of size 50. A snapshot of our
andarelistedinTable7. human evaluation interface is shown in Figure 4.
Theorderoftwocandidateresponses(R1andR2)
[KP]isfamousfor ismaderandomforeachquestion.
Thepopularopinionabout[KP]is
HereiswhatIknowabout[KP]:
User Study For user study, we similarly re-
Myfriendsaysthat[KP]is:
Hereissomeinformationabout[KP]: cruited 60 Anglophone users who have at least
Herearesomereviewsabout[KP]: high-schoollevelofeducationandarecomfortable
Ithink[KP]is:
with handling internet-based technologies. Each
Ireadontheinternetabout[KP]andfoundthat
TodayIlearnedabout[KP]that session(dependingonthesystemstheyinteracted)
lastedonanaverage30minutes(forMultiWOZ)
Table7: ManuallycuratedpromptstoquerythePTLM and60minutes(forWoW)includingon-boarding,
performing actual task and answering post-task
questions.
Statistics on generated and selected knowledge
snippets Forbothdatasets,weretrieve100most
D QualitativeExamples
relevantknowledgesnippetsfromnon-parametric
source(here,yelpreviews),andgenerate5candi- Figure5showsacompletedialogintheknowledge-
dateknowledgesnippets(usingnucleussampling groundedscenariowheretheuserdiscussesabout
‘science-fiction’. Figure 6 shows more utter-
15https://github.com/uber-research/
ance level examples for both goal-oriented and
PPLM/blob/master/run_pplm_discrim_train.
py knowledge-groundedscenarios.
Figure4: HumanevaluationsetupforpairwisecomparisonbetweenPOKIandanotherbaseline
Ethicalconsiderations
Wedonotforeseeanyimmediateethicalconcerns
forourmethodasweuseseveralconstraints(less
divergencefromtheextractedknowledge,consis-
tency with the dialog context) that allow the gen-
eration to be restricted to the context. In general,
weexpectourdialogsystemtobemoreengaging
and accessible to the user. Since we use PTLMs
asknowledgesource,weinheritthegeneralriskof
generatingbiasedortoxiclanguage,whichshould
becarefullyfiltered. Inourwork,weperformex-
plicit filtering steps to make sure that the knowl-
edge is appropriate. Furthermore, our selection
steppromotesmorefactuallycorrectknowledgeto
beselected. However,thegenerationsmayincor-
poratebiasesthatarealreadypresentinthedialog
datasetsduetocrowd-sourceddatacollection. Fi-
nally, our generations are limited only to the En-
glishlanguage. Hencewesuggestthatasystemlike
oursshouldlikelynotbeusedasa‘blackbox,’but
wouldbestbeusedinasettingwhereitsoutputscan
be‘audited’. Carbonfootprint: Oursystemuses
post-hocknowledgeinjectionwhichrefrainsfrom
retraining newer dialog models to accommodate
dynamically evolving external knowledge. This
promotesgreenNLPapplications(Schwartzetal.,
2020; Strubell et al., 2019) reducing carbon foot-
printsthatstemfromtraining(orevenfinetuning)
largelanguagemodels.
👧 : I think science fiction is an
amazing genre for anything. Future
science, technology, time travel, FTL
travel, they're all such interesting I read a lot of science Issac Asimov is one of the most
concepts. fiction books. What popular science fiction writers. His
about you? writing style is quite unique.
🤖: I read a lot of science fiction books.
My favorite author is Issac Asimov. What
about you?
👧 : Awesome! I like Issac Asimov too.
Sci-fi storytellers should on Science fiction is known for
philosophical issues. I like how they discussing dystopias, alien
address problems that invasions and time travel. They talk
🤖: Yes! I like how science fiction
can destroy our about how we destroy our current
discusses dystopias, alien invasion and
future. world and how we can save
time travel. They address the current
problems and how we can save ourselves.
ourselves from them.
👧 : Great that you think in the same
way. I gtg. Thanks! You are None
welcome.
🤖: Thanks! You are welcome.
Figure 5: POKI converses with a user who is discussing about science fiction, in a knowledge-grounded dialog
scenario(leftcolumn). Ineachturn,a initialresponse (middlecolumn)isaugmentedwitha knowledgesnippet
(rightcolumn)usingconstrainedgradient-baseddecoding. Humanjudgesunanimouslynotedthisconversationas
moreengagingascomparedtotheinitialresponses.
👧 : I'd like to dine for an expensive
Expensive British cuisine is popular
cuisine. I’d recommend British because of its easy accessibility,
🤖: I’d recommend the British cuisine. cuisine. freshness. There are plenty for you
It is popular because of its freshness. to eat at your leisure.
There are plenty to eat at your leisure.
(a)
👧 : Are there any theatres in the
Mumford theatre has good parking
centre of town, Cambridge? There are 4 theatres in
and drinking places nearby. It has
🤖: I would recommend the the centre. I would
lovely intimate venue and the door
recommend the
Mumford theatre. It has good parking
Mumford theatre. prices are very reasonable.
and drinking places nearby. The
place is an intimate venue.
(b)
👧 : Tell me more about Picasso.
Picasso is one of the Picasso was raised in Spain before
🤖: Picasso is one of the finest artists
finest artists in the going on to spend most of his adult
in the modern time. He was raised in
modern time. life working as an artist in France.
Spain before he spent most of his
adult life in France.
(c)
Figure6:Utterancelevelexamples(leftcolumn)in(a)and(b)goalorientedscenario;and(c)knowledge-grounded
scenario. POKI updates the initialresponse (middle column) with a knowledgesnippet (right column) using
constrainedgradient-baseddecoding.

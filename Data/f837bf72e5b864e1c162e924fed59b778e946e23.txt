Imagining Grounded Conceptual Representations from Perceptual
Information in Situated Guessing Games
AlessandroSuglia1,AntonioVergari2,IoannisKonstas1,YonatanBisk3,
EmanueleBastianelli1,AndreaVanzo1, and OliverLemon1
1Heriot-WattUniversity,Edinburgh,UK
2UniversityofCalifornia,LosAngeles,USA
3CarnegieMellonUniversity,Pittsburgh,USA
1 {as247,i.konstas,a.vanzo,e.bastianelli,o.lemon}@hw.ac.uk
2aver@cs.ucla.edu,3ybisk@cs.cmu.edu
Abstract
In visual guessing games, a Guesser has to identify a target object in a scene by asking ques-
tions to an Oracle. An effective strategy for the players is to learn conceptual representations
of objects that are both discriminative and expressive enough to ask questions and guess cor-
rectly. However,asshownbySugliaetal.(2020),existingmodelsfailtolearntrulymulti-modal
representations, relying instead on gold category labels for objects in the scene both at train-
ing and inference time. This provides an unnatural performance advantage when categories at
inference time match those at training time, and it causes models to fail in more realistic “zero-
shot” scenarios where out-of-domain object categories are involved. To overcome this issue,
we introduce a novel “imagination” module based on Regularized Auto-Encoders, that learns
context-awareandcategory-awarelatentembeddingswithoutrelyingoncategorylabelsatinfer-
encetime. Ourimaginationmoduleoutperformsstate-of-the-artcompetitorsby8.26%gameplay
accuracy in the CompGuessWhat?! zero-shot scenario (Suglia et al., 2020), and it improves the
Oracle and Guesser accuracy by 2.08% and 12.86% in the GuessWhat?! benchmark, when no
gold categories are available at inference time. The imagination module also boosts reasoning
aboutobjectpropertiesandattributes.
1 Introduction
Humans do not learn conceptual representations from language alone, but from a wide range of situa-
tionalinformation(Beinbornetal.,2018;Bisketal.,2020)ashighlightedalsobyproperty-listingexper-
iments (McRae et al., 2005). When humans experience the concept of “boat”, theysimulate a new rep-
resentationbyreactivatingandaggregatingmulti-modalrepresentationsthatresideintheirmemoryand
areassociatedwiththeconceptof“boat”(e.g.,whataboatlookslike,theactionofsailing,etc)(Barsa-
lou, 2008). This simulation process is called perceptual simulation. Therefore, it is no wonder that
recenttrendsinlearningconceptualrepresentationsadoptmulti-modalandholisticapproaches(Bruniet
al.,2014)whereinabstractdistributionallexicalrepresentations(LandauerandDumais,1997;Laurence
andMargolis,1999)learnedfromtextcorporaareaugmentedorrefinedwithperceptualinformationfor
concreteandcontext-awarerepresentationsbuiltfromvisual(Kielaetal.,2018;Lazaridouetal.,2015),
olfactory(Kielaetal.,2015),orauditory(KielaandClark,2015)modalities.
Language games between AI agents, inspired by Wittgenstein’s Language Games among hu-
mans (Wittgenstein et al., 1953), are an excellent test bed for such approaches since concepts are ex-
pected to emerge when agents are required to communicate to solve specific tasks in specific environ-
ments. GuessWhat?!(DeVriesetal.,2017)isaprototypicallanguagegameofthiskind: aGuesserhasto
identifyatargetobjectinascenerepresentedasanimagebyaskingquestionstoanOracle. Learningto
groundpixelsofthesceneintoobjectrepresentationsthatarerelevantfortheobjectcategorytheybelong
to(category-aware),butarealsoparticularizedforthespecificscene(context-aware),isfundamentalfor
theGuessertoeffectivelyconversewiththeOracleandvice-versa.
This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/.
Training time Inference time
c
pasticciotto
z
c c
donut
person person donut donut donut
z
z
Figure 1: Common approaches to visual grounding such as De Vries et al. (2017) and Zhuang et al.
(2018) rely on gold category labels at test time, thereby failing to ground novel objects from categories
notseenduringtraining(e.g.,a“pasticciotto”,topright)ortoproperlyencodeknowncategoriesbutwith
unseen visual features (like a “frosted donut”, bottom right) since they employ category embeddings c
from a predefined set that are fixed for each object. Instead, embeddings z learned by our imagination
modulecanbeflexiblycategory-awareallowingthemtogeneralizetounseencategories.
Weconsideramodeltrulymulti-modalifitalwaysusesallthemodalitiestomakedecisions. However,
existingapproaches(DeVriesetal.,2017;Shekharetal.,2019)relyinsteadongoldcategorylabelsthat
are assumed to be available also at inference time, thus making these models depend on this modality
and discarding the others. This not only poses an unnatural performance advantage for players in con-
trolledbenchmarkscenariosliketheGuessWhat?!gamewhencategoriesatinferencetimematchthose
at training time, but causes them to fail in more realistic zero-shot scenarios (Suglia et al., 2020) where
players are required to generalize to out-of-domain object categories. For example, consider an agent
thatduringtraininghasonlyseenglazeddonuts, associatedwiththefixed“donut”categoryembedding
(cf. Figure1). Atinferencetime,themodelcannotgroundvisualrepresentationsforobjectsbelongingto
the“pasticciotto”(anItalianpastry)category,sincesuchacategorywasnotinitsrepertoire. Similarly,it
willlikelyrepresentfrosteddonutswithageneric“donut”embedding,despitetheperceptualdifferences
amongdifferenttypesofdonut.
In this paper, we tackle the above limitations by introducing a novel imagination module based on
Regularized Auto-encoders (Ghosh et al., 2019), which are able to derive imagination embeddings di-
rectlyfromperceptualinformationintheformoftheobjectcrop. Ourformulationofthereconstruction
lossallowsthemodeltolearncontext-awareandcategory-awareimaginationembeddings. Thus,remov-
ing the need for gold category labels at inference time and greatly improving zero-shot generalization.
Section4.2integratesourimaginationcomponentintotheOraclemodelofDeVriesetal.(2017)andthe
GuessermodelofShekharetal.(2019). Weshowthatthenewimaginationmodelsarestate-of-the-artin
therecentlyintroducedCompGuessWhat?!benchmark(Sugliaetal.,2020)outperformingcurrentmod-
elsby8.26%. ItalsoimprovestheOracle’sandGuesser’saccuracy(by2.08%and12.86%,respectively)
inthestandardGuessWhat?! whennogoldcategorylabelsareavailable. Lastly,weshowthatimagining
latentobjectrepresentationsgreatlyhelpstoreasonaboutobjectvisualproperties(i.e.,color,shape,etc.),
qualifyingourmoduleasagenericperceptualsimulationcomponentala` Barsalou(2008).
2 Background: GuessingGamesandConceptRepresentations
GuessWhat?! is an instance of a multi-word guessing game (Steels, 2015). Every game involves two
players: an Oracle and a Guesser conversing about a scene S (a natural image). A scene S can be
<latexit sha1_base64="bgb8BW6SkYNwcqdDMdxIH9RNrp0=">AAACOnicbVBNaxRBEO1J1MTxaxNvemlcBE/LjCxEb8FcPEZwk+DOsNT01GyadPcM3TUmSzP/wmv8JfkjuXqTXPMD7P046K4PCh7vVVGPVzRKOkqS22hr+8HDRzu7j+MnT589f9Hb2z9xdWsFjkStantWgEMlDY5IksKzxiLoQuFpcXE090+/o3WyNl9p1mCuYWpkJQVQkL5lGui8qLzoJr1+MkgW4JskXZE+W+F4she9yspatBoNCQXOjdOkodyDJSkUdnHWOmxAXMAUx4Ea0Ohyv4jc8bdBKXlV2zCG+EL9+8KDdm6mi7A5j+jWvbn4X6/Qa5+p+pB7aZqW0Ijl46pVnGo+74OX0qIgNQsEhJUhOxfnYEFQaC2OM4OXotYaTOkzsFMNV904zX3WmjIsIPl+2vmM8Ir80uZd18WhzXS9u01y8n6QDgcfvwz7h59Wve6y1+wNe8dSdsAO2Wd2zEZMMMN+sGv2M7qJfkW/o7vl6la0unnJ/kF0/wcPDa4h</latexit><latexit sha1_base64="xpNqhg0PGb+cbYn+l5HCICqJkCQ=">AAACOnicbVDLbtRAEBwnQIJ5ZcMNLiNWSJxWNloJcovgkmOQ2CRibUXtcXszyszYmmnDLiP/Ra7Jl+RHcuWGuOYDMvs4wIaSWipVdatLVTRKOkqSm2hj88HDR1vbj+MnT589f7HT2z1ydWsFjkStantSgEMlDY5IksKTxiLoQuFxcf557h9/R+tkbb7SrMFcw8TISgqgIH3LNNBZUfmf3elOPxkkC/D7JF2RPlvh8LQXvcrKWrQaDQkFzo3TpKHcgyUpFHZx1jpsQJzDBMeBGtDocr+I3PG3QSl5VdswhvhC/fvCg3ZupouwOY/o1r25+F+v0GufqfqYe2maltCI5eOqVZxqPu+Dl9KiIDULBISVITsXZ2BBUGgtjjODP0StNZjSZ2AnGqbdOM191poyLCD5ftr5jHBKfmnzruvi0Ga63t19cvR+kA4He1+G/f1Pq1632Wv2hr1jKfvA9tkBO2QjJphhF+ySXUXX0a/od/RnuboRrW5esn8Q3d4BOI+uOA==</latexit> <latexit sha1_base64="bgb8BW6SkYNwcqdDMdxIH9RNrp0=">AAACOnicbVBNaxRBEO1J1MTxaxNvemlcBE/LjCxEb8FcPEZwk+DOsNT01GyadPcM3TUmSzP/wmv8JfkjuXqTXPMD7P046K4PCh7vVVGPVzRKOkqS22hr+8HDRzu7j+MnT589f9Hb2z9xdWsFjkStantWgEMlDY5IksKzxiLoQuFpcXE090+/o3WyNl9p1mCuYWpkJQVQkL5lGui8qLzoJr1+MkgW4JskXZE+W+F4she9yspatBoNCQXOjdOkodyDJSkUdnHWOmxAXMAUx4Ea0Ohyv4jc8bdBKXlV2zCG+EL9+8KDdm6mi7A5j+jWvbn4X6/Qa5+p+pB7aZqW0Ijl46pVnGo+74OX0qIgNQsEhJUhOxfnYEFQaC2OM4OXotYaTOkzsFMNV904zX3WmjIsIPl+2vmM8Ir80uZd18WhzXS9u01y8n6QDgcfvwz7h59Wve6y1+wNe8dSdsAO2Wd2zEZMMMN+sGv2M7qJfkW/o7vl6la0unnJ/kF0/wcPDa4h</latexit><latexit sha1_base64="bgb8BW6SkYNwcqdDMdxIH9RNrp0=">AAACOnicbVBNaxRBEO1J1MTxaxNvemlcBE/LjCxEb8FcPEZwk+DOsNT01GyadPcM3TUmSzP/wmv8JfkjuXqTXPMD7P046K4PCh7vVVGPVzRKOkqS22hr+8HDRzu7j+MnT589f9Hb2z9xdWsFjkStantWgEMlDY5IksKzxiLoQuFpcXE090+/o3WyNl9p1mCuYWpkJQVQkL5lGui8qLzoJr1+MkgW4JskXZE+W+F4she9yspatBoNCQXOjdOkodyDJSkUdnHWOmxAXMAUx4Ea0Ohyv4jc8bdBKXlV2zCG+EL9+8KDdm6mi7A5j+jWvbn4X6/Qa5+p+pB7aZqW0Ijl46pVnGo+74OX0qIgNQsEhJUhOxfnYEFQaC2OM4OXotYaTOkzsFMNV904zX3WmjIsIPl+2vmM8Ir80uZd18WhzXS9u01y8n6QDgcfvwz7h59Wve6y1+wNe8dSdsAO2Wd2zEZMMMN+sGv2M7qJfkW/o7vl6la0unnJ/kF0/wcPDa4h</latexit><latexit sha1_base64="xpNqhg0PGb+cbYn+l5HCICqJkCQ=">AAACOnicbVDLbtRAEBwnQIJ5ZcMNLiNWSJxWNloJcovgkmOQ2CRibUXtcXszyszYmmnDLiP/Ra7Jl+RHcuWGuOYDMvs4wIaSWipVdatLVTRKOkqSm2hj88HDR1vbj+MnT589f7HT2z1ydWsFjkStantSgEMlDY5IksKTxiLoQuFxcf557h9/R+tkbb7SrMFcw8TISgqgIH3LNNBZUfmf3elOPxkkC/D7JF2RPlvh8LQXvcrKWrQaDQkFzo3TpKHcgyUpFHZx1jpsQJzDBMeBGtDocr+I3PG3QSl5VdswhvhC/fvCg3ZupouwOY/o1r25+F+v0GufqfqYe2maltCI5eOqVZxqPu+Dl9KiIDULBISVITsXZ2BBUGgtjjODP0StNZjSZ2AnGqbdOM191poyLCD5ftr5jHBKfmnzruvi0Ga63t19cvR+kA4He1+G/f1Pq1632Wv2hr1jKfvA9tkBO2QjJphhF+ySXUXX0a/od/RnuboRrW5esn8Q3d4BOI+uOA==</latexit><latexit sha1_base64="xpNqhg0PGb+cbYn+l5HCICqJkCQ=">AAACOnicbVDLbtRAEBwnQIJ5ZcMNLiNWSJxWNloJcovgkmOQ2CRibUXtcXszyszYmmnDLiP/Ra7Jl+RHcuWGuOYDMvs4wIaSWipVdatLVTRKOkqSm2hj88HDR1vbj+MnT589f7HT2z1ydWsFjkStantSgEMlDY5IksKTxiLoQuFxcf557h9/R+tkbb7SrMFcw8TISgqgIH3LNNBZUfmf3elOPxkkC/D7JF2RPlvh8LQXvcrKWrQaDQkFzo3TpKHcgyUpFHZx1jpsQJzDBMeBGtDocr+I3PG3QSl5VdswhvhC/fvCg3ZupouwOY/o1r25+F+v0GufqfqYe2maltCI5eOqVZxqPu+Dl9KiIDULBISVITsXZ2BBUGgtjjODP0StNZjSZ2AnGqbdOM191poyLCD5ftr5jHBKfmnzruvi0Ga63t19cvR+kA4He1+G/f1Pq1632Wv2hr1jKfvA9tkBO2QjJphhF+ySXUXX0a/od/RnuboRrW5esn8Q3d4BOI+uOA==</latexit>
?
?
?
abstracted into a collection of objects O, each of which is associated with a category c ∈ C,i =
i
∗
{1,...,K}. The aim of the Guesser is to identify a target object o ∈ O by asking questions about
S totheOracle. ThegameplayofGuessWhat?!thuscomprisesthreetasks: i)questiongenerationwhere
the Guesser inquires about an object in the scene S given the dialogue generated so far; ii) answer
prediction, where the Oracle answers a ∈ A = {Yes,No,N/A} given the scene S, question and the
∗
target object o ; and iii) target prediction where the Guesser selects a candidate object with the highest
relevancescorer(o ).
i
Several architectural variants have been proposed to tackle GuessWhat?! (cf. Section 5 for some
related works). In this work we adopt the recent GDSE model (Shekhar et al., 2019), which learns a
visuallygroundeddialoguestateusedtolearnbothquestiongenerationandtargetobjectprediction. As
shownbelow,GDSEdoesnotdeliverthedesiredmulti-modalityneeded,thereforeweextenditwithour
Imaginationcomponenttoobtainmoreeffectivemulti-modalobjectrepresentations.
For successful gameplay, both the Guesser and Oracle must build representations of the scene that
contain specific perceptual information of objects (object-aware), are relevant for the object category
they belong to (category-aware), and are specialized to the scene in which the game is played (context-
aware). As the scene S is an image, it is natural to associate each object o ∈ O with a perceptual
i
embedding, i.e., a vector v ∈ RdO extracted from the penultimate layer of a pretrained vision model
i
(e.g. ResNet-152 (Shekharetal.,2019))basedontheirboundingbox.1
However,theserepresentationsarenotsufficientastheyareneithercontext-awarenorcategory-aware,
i.e., they ignore other objects in the scene and do not leverage their category information. GDSE and
other recent approaches (De Vries et al., 2017; Shekhar et al., 2019; Zhuang et al., 2018; Shukla et al.,
2019) coped with the second issue by introducing category embeddings as dC-dimensional continuous
representationsc ∈ RdC fork = 1,...,K. Oncelearned,acategoryembeddingcisthenconcatenated
k
toan8-dimensionalfeaturevectors derivedfromtheobjectboundingbox(cf.DeVriesetal.(2017)).
i
While these embeddings partially solve category-awareness, they are not object-aware. For instance,
the embedding for the object category “apple” will be the same regardless of a particular object to be
a red or green apple, i.e., most likely a centroid representation of the objects seen only during training.
Moreover, if during training we only see red apples, at inference time, we will likely fail to detect
green apples as belonging to the same category (Figure 2(a)). These issues have gone unnoticed since
category embeddings usually boost performances on the original GuessWhat?! task, given that gold
category labels are also available at inference time. However, this boost is illusory: models relying on
thissymbolicinformationtobealwaysavailablearenotlearningtoexploitallmodalities. Infact,a 20%
dropintheGuesseraccuracyifgoldcategorylabelsarenotprovidedhasbeenreportedinZhuangetal.
(2018) for GuessWhat?! and analogous poor results in more realistic benchmarks measuring zero-shot
generalizationsuchasCompGuessWhat?!(Sugliaetal.,2020).
3 ImaginationModule: LearningContext-andCategory-awareObjectRepresentations
To overcome the limitations of GDSE and competitors and realize a form of perceptual simulation in
a learning system, we introduce a generic component—named the imagination module—which learns
latent concept representations that are both context- and category-aware, without relying on category
labels at inference time. Our imagination model can be understood in the context of representation
learning via deep generative models (Bengio et al., 2013) which has been popularized by variational
autoencoders (VAEs) (Kingma and Welling, 2013; Kingma et al., 2014), and GANs(Goodfellow et al.,
2014). Specifically, we substantially extend the recently introduced regularized autoencoders (RAEs)
framework (Ghosh et al., 2019). RAEs are simplified VAEs where stochasticity in the encoder and de-
coder is dropped in favor of more stable training and more informative embedding learning. In fact,
RAEs do not suffer from several issues known to affect VAEs, such as poor convergence and the pos-
sibility of learning embeddings that are independent of the input images (cf. Ghosh et al. (2019) for a
detailed discussion). More crucially for our purposes, RAEs do not have to compromise the informa-
tivenessofthelearnedembeddingswithafixeda-prioristructureinthelatentspacethatenablessimple
1Boundingboxesareassumedtobegiven,e.g.byusingobjectrecognitionasapre-processingstep(Andersonetal.,2018).
Imagination
embedding
fR eae ts uN ree st
Encoder Decoder
R Ie nc po un t. LI RM EG
C
v i E φ z i D θ v˜ i
LI RM EG C:= max(0,η−MSE(D θ( ),ResNet( ))+MSE(D θ( ),ResNet( )))
(a) Imaginationmodule
(b) Oraclemodel (c) Guessermodel
Figure 2: Imagination-based Representation Learning: Given the perceptual information v of object
i
o , we learn an imagination embedding z generated by Encoder E . The latent code is optimized to
i i φ
reconstruct the original visual representation v (the “donut” ResNet encoding) via the reconstruction
i
lossLIMG usingtheDecoderD . Figures2(b)and2(c)showhowtheimaginationembeddingzreplaces
REC θ
the category embedding c in the Oracle model from De Vries et al. (2017) and Guesser model from
Shekharetal.(2019)respectively,andisconcatenatedtothespatialinformations .
i
sampling (e.g., an isotropic Gaussian prior). VAEs which need to have such a fixed prior, instead, are
deemedtolearnembeddingsthatarelessinformativew.r.t. objects,categories,andcontextinformation.
Module architecture. Figure 2(a) summarizes our imagination module. Its aim is to distill a context
andcategory-awareembeddingz ∈ RdZ perobjecto insceneS. Tothisend,weadoptanencoderE
i i φ
parameterizedbyφthatmapsaperceptualembeddingv ofobjecto toitsimaginedcounterpartz ,i.e.,
i i i
E (v ) = z . AdecoderD realizestheinversemappingv˜ = D (z ),withv˜ ∈ RdO beingalsocalled
φ i i θ i θ i i
thereconstructionoftheinputv i. AsinRAEs,ourper-objectlossLIMG comprisesareconstructionloss
(LREC),weightinghowgoodthereconstructionsofD
θ
arew.r.t. theencodedrepresentationsbyE φ,and
a regularization term (LREG) enhancing generalization by smoothing the decoder D θ. This leads to the
followingcompositeloss:
LIMG = LREC+αLREG, (1)
where α is an hyperparameter controlling regularization.2 As in L2-RAE (Ghosh et al., 2019), the reg-
ularization component is defined as LREG := ||z i||+||θ||2: the first term bounds the latent embedding
spacelearnedbyE easingoptimization;thesecondenforcessmoothingoverD improvinggeneraliza-
φ θ
tionoverregionsofthelatentspacethatareunseenduringtraining.
Differently from RAEs, we devise a specific reconstruction loss tailored to learn contextual and
category-aware representations. In conventional RAEs, in fact, the reconstruction loss is defined as
the Mean Squared Error (MSE) representing the distance between v and its reconstruction v˜ , so that
i i
LRAE := MSE(v ,v˜ ). This loss is purely unsupervised and as such agnostic to object categories or to
REC i i
IMG
thescenecontext. Toouraims,wedefineacustomimaginationreconstructionlossL asaninstance
REC
ofamax-margintriplet-loss(Wangetal., 2014; Schroffetal., 2015), asfollows. Letc bethecategory
i
2Ghoshetal.(2019)usetwodifferenthyperparametersforthetwotermsinL REG Optimizingthemindependentlyhadno
evidentbenefitinourexperiments,hencewesimplytreatthemasasingleregularizertogether.
<latexit sha1_base64="wO8jNKWwgKoRKPPNK/m0Urad/E0=">AAACaHicbVDbahRBEO0dL4njbaMPor4MWQSflhkJUd9CgqigEMVNAjvjUtNTs2nS3TN015gsTf+JX+Or/oC/4FfYe0F0Y0E1p86poqpP2UphKU1/9qIrV69d39i8Ed+8dfvO3f7WvSPbdIbjiDeyMSclWJRC44gESTxpDYIqJR6XZwdz/fgLGisa/YlmLRYKplrUggMFatLfzVHbzqACOnX5/OUg3Ts/WRa2dh9fHXjvP/+p375/7f2kP0iH6SKSyyBbgQFbxeFkq/corxreKdTEJVg7ztKWCgeGBJfo47yz2AI/gymOA9Sg0BZu8UGfPAlMldSNCakpWbB/TzhQ1s5UGToXV65rc/K/WqnWNlP9onBCtx2h5svFdScTapK5e0klDHKSswCAGxFuT/gpGOAUPI7jXOM5b5QCXbkczFTBhR9nhcs7XYUGJDfIvMsJL8gt5SR4Gwc3s3XvLoOjZ8NsZ/jyw85gb3/l6yZ7zLbZU5ax52yPvWGHbMQ4+8q+se/sR+9X1I8eRA+XrVFvNXOf/RPR9m8qEr4B</latexit> <latexit sha1_base64="dakxWVtu4SRUoW7JD/GNMPBCxGQ=">AAACMnicbVDLShxBFK3WPEwbEx+7ZFM4CK6GbhFMAgExmyxNyKgw3cjt6ttjYVV1U3U7cSj6D9zql+Rn4k7c5iNS81gkYw5cOJxzLvdyikZJR0nyK1pafvL02fOVF/Hqy7VXr9c3Nk9c3VqBA1Gr2p4V4FBJgwOSpPCssQi6UHhaXH6a+Kff0TpZm280bjDXMDKykgIoSF8/fDxf7yX9ZAr+mKRz0mNzHJ9vRG+yshatRkNCgXPDNGko92BJCoVdnLUOGxCXMMJhoAY0utxPX+34TlBKXtU2jCE+Vf/e8KCdG+siJDXQhVv0JuJ/vUIvXKbqXe6laVpCI2aHq1ZxqvmkB15Ki4LUOBAQVobfubgAC4JCW3GcGfwhaq3BlD4DO9Jw1Q3T3GetKUMAyffSzmeEV+RnNu+6Lg5tpovdPSYne/10v//+y37v8Gje6wp7y7bZLkvZATtkn9kxGzDBKnbNbtht9DO6i+6jh1l0KZrvbLF/EP3+Ayt3qh8=</latexit> <latexit sha1_base64="5mSt3etAdfeuBViQTwLcRDq9Cwo=">AAACTnicbVBNaxRBEO1Zv9bxIxu9qYfGRdiDLjMSUG9BEbwIEd0ksDMsNT01mybdPUN3TZKlmYu/xqv+Eq/+EW+ivR8H3fig4dV7VVT1KxolHSXJj6h35eq16zf6N+Nbt+/c3Rns3jt0dWsFTkStantcgEMlDU5IksLjxiLoQuFRcfpm6R+doXWyNp9o0WCuYW5kJQVQkGaDR5mGi1HylGdIwJ/xUNKJq/z7j2+70WwwTMbJCvwySTdkyDY4mO1GD7KyFq1GQ0KBc9M0aSj3YEkKhV2ctQ4bEKcwx2mgBjS63K++0fEnQSl5VdvwDPGV+veEB+3cQhehc3XltrcU/+sVemszVS9zL03TEhqxXly1ilPNlxnxUloUpBaBgLAy3M7FCVgQFJKM48zguai1BlP6DOw8JNhN09xnrSlDA5Ifpp3PCC/Ir23edV0c0ky3s7tMDp+P073xqw97w/3Xm1z77CF7zEYsZS/YPnvHDtiECfaZfWFf2bfoe/Qz+hX9Xrf2os3MffYPev0/ntSzCw==</latexit> <latexit sha1_base64="Sv2oIx1lgrD1b45ORInSN1//oUQ=">AAACPHicbVBNSxxBEO0x8WuiRpNbvDRZBE/LjAgmNzGXHA1kVdgZlpqemrWxu2forjEuzfwNr+aX5H/knlvINef0fhx0zYOCx3tV1OMVjZKOkuRntPLi5era+sZm/Gpre+f17t6bC1e3VuBA1Kq2VwU4VNLggCQpvGosgi4UXhY3n6b+5S1aJ2vzlSYN5hrGRlZSAAUpyzTQdVH5224kR7u9pJ/MwJ+TdEF6bIHz0V70Litr0Wo0JBQ4N0yThnIPlqRQ2MVZ67ABcQNjHAZqQKPL/Sx0xw+CUvKqtmEM8Zn6+MKDdm6ii7A5DemWvan4X6/QS5+p+pB7aZqW0Ij546pVnGo+bYSX0qIgNQkEhJUhOxfXYEFQ6C2OM4PfRK01mNJnYMca7rphmvusNWVYQPK9tPMZ4R35uc27rotDm+lyd8/JxVE/Pe5//HLcOz1b9LrB9tl7dshSdsJO2Wd2zgZMsIbdswf2PfoR/Yp+R3/mqyvR4uYte4Lo7z/s5a8Q</latexit> <latexit sha1_base64="PnRlC2nvGC7z8aPCaYaekVkfPVY=">AAACO3icbVBNaxRBEO2JiYlj1ERvemmyCDktMxLQ3IJ68JiAmwR2hqWmp3a3SX8M3TWapZmf4VV/iT/Eszfxmru9HwezyYOCx3uvqOJVjZKesuxXsvFgc+vh9s6j9PHuk6fP9vafn3vbOoEDYZV1lxV4VNLggCQpvGwcgq4UXlRXH+b+xRd0XlrzmWYNlhomRo6lAIrS8OMoFDRFgu5wtNfL+tkC/C7JV6THVjgd7Scvi9qKVqMhocD7YZ41VAZwJIXCLi1ajw2IK5jgMFIDGn0ZFj93/HVUaj62Lo4hvlD/3wigvZ/pKiY10NSve3PxXq/Sa5dp/K4M0jQtoRHLw+NWcbJ8XgivpUNBahYJCCfj71xMwYGgWFuaFga/Cqs1mDoU4CYarrthXoaiNXUMIIVe3sUS8ZrC0uZd16WxzXy9u7vk/E0/P+ofnx31Tt6vet1hr9gBO2Q5e8tO2Cd2ygZMMMu+se/sR/Iz+Z38Sf4uoxvJaucFu4Xk5h9NFK43</latexit> <latexit sha1_base64="GZlFYJV2Ukst5D/QnV9TUXsZXtQ=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCOayzMhC9CZ68aiQVWFnkJqe2rWxu2forokuzfwCr8kvya/xJl7zJ9L7cTBrHhQ83ntFFa+olXSUJI/R0pu3y+9WVtfi9+sfPm5sbm1fuKqxAvuiUpW9KsChkgb7JEnhVW0RdKHwsrg9mfiXP9A6WZnvNK4x1zAycigFUJDOv15vdpJuMgV/TdI56bA5zq63ok9ZWYlGoyGhwLlBmtSUe7AkhcI2zhqHNYhbGOEgUAMaXe6nn7Z8NyglH1Y2jCE+VV9ueNDOjXURkhroxi16E/G/XqEXLtPwIPfS1A2hEbPDw0ZxqvikBl5Ki4LUOBAQVobfubgBC4JCWXGcGbwTldZgSp+BHWm4bwdp7rPGlCGA5Dtp6zPCe/Izm7dtG4c208XuXpOL/W7a6x6e9zpHx/NeV9ln9oXtsZR9Y0fslJ2xPhMM2QP7yX5Fv6PH6Cl6nkWXovnODvsH0Z+/eYupxw==</latexit> <latexit sha1_base64="T7AFbhoSs7NvtekDgsUg4i2wb7o=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCB7CMiML0ZvoxaNCVoWdQWp6atfG7p6huya6NPMLvCa/JL/Gm3jNn0jvx8GseVDweO8VVbyiVtJRkjxGS2/eLr9bWV2L369/+LixubV94arGCuyLSlX2qgCHShrskySFV7VF0IXCy+L2ZOJf/kDrZGW+07jGXMPIyKEUQEE6/3q92Um6yRT8NUnnpMPmOLveij5lZSUajYaEAucGaVJT7sGSFArbOGsc1iBuYYSDQA1odLmfftry3aCUfFjZMIb4VH254UE7N9ZFSGqgG7foTcT/eoVeuEzDg9xLUzeERswODxvFqeKTGngpLQpS40BAWBl+5+IGLAgKZcVxZvBOVFqDKX0GdqThvh2kuc8aU4YAku+krc8I78nPbN62bRzaTBe7e00u9rtpr3t43uscHc97XWWf2Re2x1L2jR2xU3bG+kwwZA/sJ/sV/Y4eo6foeRZdiuY7O+wfRH/+An7yqco=</latexit><latexit sha1_base64="jbdYaInA2pjivkHxxIi2AOe21a0=">AAACPnicbVBNSxxBEO3RmOjEJBpv5tJkETwtM0GI3kQvOQUjWRV2RqnpqV0bu3uG7pro0sz/8Gp+iX/DP+At5JqjvR+HZM2Dgsd7r6jiFbWSjpLkIVpYfLH08tXySvx69c3bd2vr709c1ViBPVGpyp4V4FBJgz2SpPCstgi6UHhaXB2O/dMfaJ2szHca1ZhrGBo5kAIoSOcZ4Q35Y3Rfkdrti7VO0k0m4M9JOiMdNsPRxXq0mZWVaDQaEgqc66dJTbkHS1IobOOscViDuIIh9gM1oNHlfvJ2y7eCUvJBZcMY4hP17w0P2rmRLkJSA126eW8s/tcr9NxlGuzmXpq6ITRienjQKE4VH3fCS2lRkBoFAsLK8DsXl2BBUGgujjOD16LSGkzpM7BDDTdtP8191pgyBJB8J239tMqpzdu2jUOb6Xx3z8nJp2660937ttPZP5j1usw+sI9sm6XsM9tnX9gR6zHBLLtld+xndB89Rr+i39PoQjTb2WD/IPrzBEHvr7I=</latexit> <latexit sha1_base64="TUC0OHKTlUoFRT+yIPjuesmoY40=">AAACOHicbVBNSxxBEO0xMdGJSTTe4qXJInhaZoJgcpNIIEeFrC7sDEtNT+1uY3fP0F2TuDTzJ3I1v8R/4i038eovsPfjkKx5UPB47xVVvKJW0lGS3EZrz56vv3i5sRm/2nr95u32zrtzVzVWYE9UqrL9AhwqabBHkhT2a4ugC4UXxeXJzL/4gdbJynynaY25hrGRIymAgtT/OvRZPZHtcLuTdJM5+FOSLkmHLXE63IneZ2UlGo2GhALnBmlSU+7BkhQK2zhrHNYgLmGMg0ANaHS5nz/c8v2glHxU2TCG+Fz9e8ODdm6qi5DUQBO36s3E/3qFXrlMo0+5l6ZuCI1YHB41ilPFZ23wUloUpKaBgLAy/M7FBCwICp3FcWbwp6i0BlP6DOxYw1U7SHOfNaYMASTfSVufEV6RX9i8bds4tJmudveUnH/spofdz2eHneMvy1432B77wA5Yyo7YMfvGTlmPCabYL3bNfkc30Z/oLrpfRNei5c4u+wfRwyMMpq0d</latexit> <latexit sha1_base64="sRgUgd6NPsCwBRoI0kbcVFT7O3w=">AAACPHicbVBNSxxBEO0xMdGJSfy4xUuTRfC0zATB5CbJxaOCq8LOsNT01KyN3T1Dd03ippm/kWvyS/I/cs9NvHpO78dB1zwoeLxXRT1e0SjpKEn+RCvPnq++eLm2Hr/aeP3m7ebW9rmrWytwIGpV28sCHCppcECSFF42FkEXCi+K6y9T/+IrWidrc0aTBnMNYyMrKYCClGUa6Kqo/PduJEebvaSfzMCfknRBemyBk9FW9C4ra9FqNCQUODdMk4ZyD5akUNjFWeuwAXENYxwGakCjy/0sdMf3glLyqrZhDPGZ+vDCg3ZuoouwOQ3plr2p+F+v0EufqfqYe2maltCI+eOqVZxqPm2El9KiIDUJBISVITsXV2BBUOgtjjOD30StNZjSZ2DHGm66YZr7rDVlWEDyvbTzGeEN+bnNu66LQ5vpcndPyfmHfnrQ/3R60Dv6vOh1je2y92yfpeyQHbFjdsIGTLCG/WA/2a/od/Q3uo3u5qsr0eJmhz1CdP8P9CWvFA==</latexit> <latexit sha1_base64="GZlFYJV2Ukst5D/QnV9TUXsZXtQ=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCOayzMhC9CZ68aiQVWFnkJqe2rWxu2forokuzfwCr8kvya/xJl7zJ9L7cTBrHhQ83ntFFa+olXSUJI/R0pu3y+9WVtfi9+sfPm5sbm1fuKqxAvuiUpW9KsChkgb7JEnhVW0RdKHwsrg9mfiXP9A6WZnvNK4x1zAycigFUJDOv15vdpJuMgV/TdI56bA5zq63ok9ZWYlGoyGhwLlBmtSUe7AkhcI2zhqHNYhbGOEgUAMaXe6nn7Z8NyglH1Y2jCE+VV9ueNDOjXURkhroxi16E/G/XqEXLtPwIPfS1A2hEbPDw0ZxqvikBl5Ki4LUOBAQVobfubgBC4JCWXGcGbwTldZgSp+BHWm4bwdp7rPGlCGA5Dtp6zPCe/Izm7dtG4c208XuXpOL/W7a6x6e9zpHx/NeV9ln9oXtsZR9Y0fslJ2xPhMM2QP7yX5Fv6PH6Cl6nkWXovnODvsH0Z+/eYupxw==</latexit> <latexit sha1_base64="GZlFYJV2Ukst5D/QnV9TUXsZXtQ=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCOayzMhC9CZ68aiQVWFnkJqe2rWxu2forokuzfwCr8kvya/xJl7zJ9L7cTBrHhQ83ntFFa+olXSUJI/R0pu3y+9WVtfi9+sfPm5sbm1fuKqxAvuiUpW9KsChkgb7JEnhVW0RdKHwsrg9mfiXP9A6WZnvNK4x1zAycigFUJDOv15vdpJuMgV/TdI56bA5zq63ok9ZWYlGoyGhwLlBmtSUe7AkhcI2zhqHNYhbGOEgUAMaXe6nn7Z8NyglH1Y2jCE+VV9ueNDOjXURkhroxi16E/G/XqEXLtPwIPfS1A2hEbPDw0ZxqvikBl5Ki4LUOBAQVobfubgBC4JCWXGcGbwTldZgSp+BHWm4bwdp7rPGlCGA5Dtp6zPCe/Izm7dtG4c208XuXpOL/W7a6x6e9zpHx/NeV9ln9oXtsZR9Y0fslJ2xPhMM2QP7yX5Fv6PH6Cl6nkWXovnODvsH0Z+/eYupxw==</latexit> <latexit sha1_base64="O5yuQYwjvHSf3Lu0+16y+sgYlck=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCEJgmZGF6E304lEhq8LOIDU9tWtjd8/QXRNdmvkFXpNfkl/jTbzmT6T342DWPCh4vPeKKl5RK+koSR6jpTdvl9+trK7F79c/fNzY3Nq+cFVjBfZFpSp7VYBDJQ32SZLCq9oi6ELhZXF7MvEvf6B1sjLfaVxjrmFk5FAKoCCdf73e7CTdZAr+mqRz0mFznF1vRZ+yshKNRkNCgXODNKkp92BJCoVtnDUOaxC3MMJBoAY0utxPP235blBKPqxsGEN8qr7c8KCdG+siJDXQjVv0JuJ/vUIvXKbhQe6lqRtCI2aHh43iVPFJDbyUFgWpcSAgrAy/c3EDFgSFsuI4M3gnKq3BlD4DO9Jw3w7S3GeNKUMAyXfS1meE9+RnNm/bNg5tpovdvSYX+9201z0873WOjue9rrLP7AvbYyn7xo7YKTtjfSYYsgf2k/2KfkeP0VP0PIsuRfOdHfYPoj9/AX0lqck=</latexit> <latexit sha1_base64="7wVNgUCngXDdDROjwbaveCu6A0g=">AAACPXicbVBNSxxBEO0xJprJh5rc4qXJEvC0zATB5CYRIZeAIVkVdgap6alZG7t7xu4a49LM7/Bqfkl+R35AbiFXr/Z+HHTNg4LHe1XU4xWNko6S5He09Gj58ZOV1afxs+cvXq6tb7w6dHVrBQ5ErWp7XIBDJQ0OSJLC48Yi6ELhUXG2N/GPLtA6WZvvNG4w1zAyspICKEh5poFOXeW/fNvvtk7We0k/mYI/JOmc9NgcBycb0ZusrEWr0ZBQ4NwwTRrKPViSQmEXZ63DBsQZjHAYqAGNLvfT1B1/F5SSV7UNY4hP1bsXHrRzY12EzWnKRW8i/tcr9MJnqj7kXpqmJTRi9rhqFaeaTyrhpbQoSI0DAWFlyM7FKVgQFIqL48zgD1FrDab0GdiRhstumOY+a00ZFpB8L+18RnhJfmbzruvi0Ga62N1Dcvi+n273P37d7u1+mve6yjbZW7bFUrbDdtlndsAGTLBzdsWu2c/oV/Qn+hv9m60uRfOb1+weoptb3MGu+g==</latexit> <latexit sha1_base64="jGJWUt0JO8HxpEU3WchXpZFjazU=">AAACOnicbVBNaxRBEO2Jmo+JmsTc4qVxCXhaZmQhegvRg8cIbhLcGZaantrdJt09Q3dNkqWZf+HV/JL8Ea+5iVd/gL0fB93kQcHjvVdU8YpaSUdJ8jNae/L02frG5la8/fzFy53dvVdnrmqswL6oVGUvCnCopME+SVJ4UVsEXSg8Ly4/zvzzK7ROVuYrTWvMNYyNHEkBFKRvn4Y+owkStMPdTtJN5uAPSbokHbbE6XAvOsjKSjQaDQkFzg3SpKbcgyUpFLZx1jisQVzCGAeBGtDocj9/ueWHQSn5qLJhDPG5+u+GB+3cVBchqYEmbtWbiY96hV65TKP3uZembgiNWBweNYpTxWd98FJaFKSmgYCwMvzOxQQsCAqtxXFm8FpUWoMpfQZ2rOGmHaS5zxpThgCS76Rt6BBvyC9s3rZtHNpMV7t7SM7eddNe98OXXuf4ZNnrJnvN3rC3LGVH7Jh9ZqeszwQz7Dv7wW6ju+g++hX9XkTXouXOPvsP0Z+/27GuBQ==</latexit> <latexit sha1_base64="PnRlC2nvGC7z8aPCaYaekVkfPVY=">AAACO3icbVBNaxRBEO2JiYlj1ERvemmyCDktMxLQ3IJ68JiAmwR2hqWmp3a3SX8M3TWapZmf4VV/iT/Eszfxmru9HwezyYOCx3uvqOJVjZKesuxXsvFgc+vh9s6j9PHuk6fP9vafn3vbOoEDYZV1lxV4VNLggCQpvGwcgq4UXlRXH+b+xRd0XlrzmWYNlhomRo6lAIrS8OMoFDRFgu5wtNfL+tkC/C7JV6THVjgd7Scvi9qKVqMhocD7YZ41VAZwJIXCLi1ajw2IK5jgMFIDGn0ZFj93/HVUaj62Lo4hvlD/3wigvZ/pKiY10NSve3PxXq/Sa5dp/K4M0jQtoRHLw+NWcbJ8XgivpUNBahYJCCfj71xMwYGgWFuaFga/Cqs1mDoU4CYarrthXoaiNXUMIIVe3sUS8ZrC0uZd16WxzXy9u7vk/E0/P+ofnx31Tt6vet1hr9gBO2Q5e8tO2Cd2ygZMMMu+se/sR/Iz+Z38Sf4uoxvJaucFu4Xk5h9NFK43</latexit> <latexit sha1_base64="GZlFYJV2Ukst5D/QnV9TUXsZXtQ=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCOayzMhC9CZ68aiQVWFnkJqe2rWxu2forokuzfwCr8kvya/xJl7zJ9L7cTBrHhQ83ntFFa+olXSUJI/R0pu3y+9WVtfi9+sfPm5sbm1fuKqxAvuiUpW9KsChkgb7JEnhVW0RdKHwsrg9mfiXP9A6WZnvNK4x1zAycigFUJDOv15vdpJuMgV/TdI56bA5zq63ok9ZWYlGoyGhwLlBmtSUe7AkhcI2zhqHNYhbGOEgUAMaXe6nn7Z8NyglH1Y2jCE+VV9ueNDOjXURkhroxi16E/G/XqEXLtPwIPfS1A2hEbPDw0ZxqvikBl5Ki4LUOBAQVobfubgBC4JCWXGcGbwTldZgSp+BHWm4bwdp7rPGlCGA5Dtp6zPCe/Izm7dtG4c208XuXpOL/W7a6x6e9zpHx/NeV9ln9oXtsZR9Y0fslJ2xPhMM2QP7yX5Fv6PH6Cl6nkWXovnODvsH0Z+/eYupxw==</latexit> <latexit sha1_base64="T7AFbhoSs7NvtekDgsUg4i2wb7o=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCB7CMiML0ZvoxaNCVoWdQWp6atfG7p6huya6NPMLvCa/JL/Gm3jNn0jvx8GseVDweO8VVbyiVtJRkjxGS2/eLr9bWV2L369/+LixubV94arGCuyLSlX2qgCHShrskySFV7VF0IXCy+L2ZOJf/kDrZGW+07jGXMPIyKEUQEE6/3q92Um6yRT8NUnnpMPmOLveij5lZSUajYaEAucGaVJT7sGSFArbOGsc1iBuYYSDQA1odLmfftry3aCUfFjZMIb4VH254UE7N9ZFSGqgG7foTcT/eoVeuEzDg9xLUzeERswODxvFqeKTGngpLQpS40BAWBl+5+IGLAgKZcVxZvBOVFqDKX0GdqThvh2kuc8aU4YAku+krc8I78nPbN62bRzaTBe7e00u9rtpr3t43uscHc97XWWf2Re2x1L2jR2xU3bG+kwwZA/sJ/sV/Y4eo6foeRZdiuY7O+wfRH/+An7yqco=</latexit> <latexit sha1_base64="jbdYaInA2pjivkHxxIi2AOe21a0=">AAACPnicbVBNSxxBEO3RmOjEJBpv5tJkETwtM0GI3kQvOQUjWRV2RqnpqV0bu3uG7pro0sz/8Gp+iX/DP+At5JqjvR+HZM2Dgsd7r6jiFbWSjpLkIVpYfLH08tXySvx69c3bd2vr709c1ViBPVGpyp4V4FBJgz2SpPCstgi6UHhaXB2O/dMfaJ2szHca1ZhrGBo5kAIoSOcZ4Q35Y3Rfkdrti7VO0k0m4M9JOiMdNsPRxXq0mZWVaDQaEgqc66dJTbkHS1IobOOscViDuIIh9gM1oNHlfvJ2y7eCUvJBZcMY4hP17w0P2rmRLkJSA126eW8s/tcr9NxlGuzmXpq6ITRienjQKE4VH3fCS2lRkBoFAsLK8DsXl2BBUGgujjOD16LSGkzpM7BDDTdtP8191pgyBJB8J239tMqpzdu2jUOb6Xx3z8nJp2660937ttPZP5j1usw+sI9sm6XsM9tnX9gR6zHBLLtld+xndB89Rr+i39PoQjTb2WD/IPrzBEHvr7I=</latexit><latexit sha1_base64="sBEerw2YIXqT6IrG1AZXG7t4wcY=">AAACRnicbVDLahRBFL098RHb1yS60k3hILgauiWg7kLcuIzgJIHpZqiuvj0pUlXdVN2OGYv6l2z1S/wFf8KduLXmsdCJBy4czrmXezhVp6SjLPuRDHZu3b5zd/deev/Bw0ePh3v7J67trcCJaFVrzyruUEmDE5Kk8KyzyHWl8LS6eL/0Ty/ROtmaT7TosNR8bmQjBacozYZPC83pvGp8QVLV6C9DmMnZcJSNsxXYTZJvyAg2OJ7tJc+KuhW9RkNCceemedZR6bklKRSGtOgddlxc8DlOIzVcoyv9Kn5gL6NSs6a1cQyxlfr3hefauYWu4uYyrNv2luJ/vUpvfabmbeml6XpCI9aPm14xatmyG1ZLi4LUIhIurIzZmTjnlguKDaZpYfCzaLXmpvYFt3PNr8I0L33RmzouIPlRHmKReEV+bbMQQhrbzLe7u0lOXo/zg/G7jwejw6NNr7vwHF7AK8jhDRzCBziGCQj4AtfwFb4l35Ofya/k93p1kGxunsA/GMAf6cex+A==</latexit> <latexit sha1_base64="wO8jNKWwgKoRKPPNK/m0Urad/E0=">AAACaHicbVDbahRBEO0dL4njbaMPor4MWQSflhkJUd9CgqigEMVNAjvjUtNTs2nS3TN015gsTf+JX+Or/oC/4FfYe0F0Y0E1p86poqpP2UphKU1/9qIrV69d39i8Ed+8dfvO3f7WvSPbdIbjiDeyMSclWJRC44gESTxpDYIqJR6XZwdz/fgLGisa/YlmLRYKplrUggMFatLfzVHbzqACOnX5/OUg3Ts/WRa2dh9fHXjvP/+p375/7f2kP0iH6SKSyyBbgQFbxeFkq/corxreKdTEJVg7ztKWCgeGBJfo47yz2AI/gymOA9Sg0BZu8UGfPAlMldSNCakpWbB/TzhQ1s5UGToXV65rc/K/WqnWNlP9onBCtx2h5svFdScTapK5e0klDHKSswCAGxFuT/gpGOAUPI7jXOM5b5QCXbkczFTBhR9nhcs7XYUGJDfIvMsJL8gt5SR4Gwc3s3XvLoOjZ8NsZ/jyw85gb3/l6yZ7zLbZU5ax52yPvWGHbMQ4+8q+se/sR+9X1I8eRA+XrVFvNXOf/RPR9m8qEr4B</latexit> <latexit sha1_base64="GZlFYJV2Ukst5D/QnV9TUXsZXtQ=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCOayzMhC9CZ68aiQVWFnkJqe2rWxu2forokuzfwCr8kvya/xJl7zJ9L7cTBrHhQ83ntFFa+olXSUJI/R0pu3y+9WVtfi9+sfPm5sbm1fuKqxAvuiUpW9KsChkgb7JEnhVW0RdKHwsrg9mfiXP9A6WZnvNK4x1zAycigFUJDOv15vdpJuMgV/TdI56bA5zq63ok9ZWYlGoyGhwLlBmtSUe7AkhcI2zhqHNYhbGOEgUAMaXe6nn7Z8NyglH1Y2jCE+VV9ueNDOjXURkhroxi16E/G/XqEXLtPwIPfS1A2hEbPDw0ZxqvikBl5Ki4LUOBAQVobfubgBC4JCWXGcGbwTldZgSp+BHWm4bwdp7rPGlCGA5Dtp6zPCe/Izm7dtG4c208XuXpOL/W7a6x6e9zpHx/NeV9ln9oXtsZR9Y0fslJ2xPhMM2QP7yX5Fv6PH6Cl6nkWXovnODvsH0Z+/eYupxw==</latexit> <latexit sha1_base64="GZlFYJV2Ukst5D/QnV9TUXsZXtQ=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCOayzMhC9CZ68aiQVWFnkJqe2rWxu2forokuzfwCr8kvya/xJl7zJ9L7cTBrHhQ83ntFFa+olXSUJI/R0pu3y+9WVtfi9+sfPm5sbm1fuKqxAvuiUpW9KsChkgb7JEnhVW0RdKHwsrg9mfiXP9A6WZnvNK4x1zAycigFUJDOv15vdpJuMgV/TdI56bA5zq63ok9ZWYlGoyGhwLlBmtSUe7AkhcI2zhqHNYhbGOEgUAMaXe6nn7Z8NyglH1Y2jCE+VV9ueNDOjXURkhroxi16E/G/XqEXLtPwIPfS1A2hEbPDw0ZxqvikBl5Ki4LUOBAQVobfubgBC4JCWXGcGbwTldZgSp+BHWm4bwdp7rPGlCGA5Dtp6zPCe/Izm7dtG4c208XuXpOL/W7a6x6e9zpHx/NeV9ln9oXtsZR9Y0fslJ2xPhMM2QP7yX5Fv6PH6Cl6nkWXovnODvsH0Z+/eYupxw==</latexit> <latexit sha1_base64="GZlFYJV2Ukst5D/QnV9TUXsZXtQ=">AAACMXicbVBNSxxBEO0xMerExK9bcmmyCOayzMhC9CZ68aiQVWFnkJqe2rWxu2forokuzfwCr8kvya/xJl7zJ9L7cTBrHhQ83ntFFa+olXSUJI/R0pu3y+9WVtfi9+sfPm5sbm1fuKqxAvuiUpW9KsChkgb7JEnhVW0RdKHwsrg9mfiXP9A6WZnvNK4x1zAycigFUJDOv15vdpJuMgV/TdI56bA5zq63ok9ZWYlGoyGhwLlBmtSUe7AkhcI2zhqHNYhbGOEgUAMaXe6nn7Z8NyglH1Y2jCE+VV9ueNDOjXURkhroxi16E/G/XqEXLtPwIPfS1A2hEbPDw0ZxqvikBl5Ki4LUOBAQVobfubgBC4JCWXGcGbwTldZgSp+BHWm4bwdp7rPGlCGA5Dtp6zPCe/Izm7dtG4c208XuXpOL/W7a6x6e9zpHx/NeV9ln9oXtsZR9Y0fslJ2xPhMM2QP7yX5Fv6PH6Cl6nkWXovnODvsH0Z+/eYupxw==</latexit>
ofobjecto
i
withperceptualembeddingv
i
insceneS andletO¬ci = {o
j
| o
j
∈ O∧c
j
6= c i}betheset
IMG
ofallobjectsinS belongingtoadifferentcategorythanc . Ourper-objectL termisdefinedas:
i REC
LIMG := max(0,η−MSE(v ,D (z ))+MSE(v ,D (z ))), (2)
REC i θ i j θ i
where η is the minimum margin between two components: i) the distance between the perceptual em-
bedding v and its reconstruction D (z ), and ii) the distance between the perceptual embedding v of
i θ i j
a randomly sampled object o
j
∈ O¬ci and the reconstruction D θ(z i). By doing so, we enforce each
object representation to be representative of its category given a specific context by locally contrasting
it to another object of a different category in the same scene. Note that this is strikingly different from
previous approaches employing a max-margin loss (Elliott and Ka´da´r, 2017; Kiros et al., 2018) where
“negative”objectsarearbitrarilysampledfromotherscenesinthesamebatch.
Imagining at inference time. Differently from the category embeddings c employed by all previous
work,ourimaginationembeddingszdonotdependongoldcategorylabelsatinferencetime,whilestill
beingcontext-awareandcategory-aware. Infact, onceparametersφhavebeenlearned, theencoderE
φ
contains all the information needed to distill embeddings z independently of LIMG, which is necessary
only at training time. We consider imagination the ability of the model of generating latent representa-
tionson-the-fly. Therefore,forbothGuesserandOraclemodelsweconsideranobjectrepresentationfor
objecto thatreplacesc withz andconcatenatesitwithitsspatialinformations (seeFigures2(b)and
i i i i
2(c) and Appendix A.1 for details). By doing so, we consider every gameplay situated in a reference
sceneasanexperiencewhereourimaginationmoduleisabletoderivealatentconceptualrepresentation
simplyby“looking”atobjects,realizingaperceptualsimulator(Barsalou,2008). Weplantoinvestigate
how to combine label-dependent category embeddings c with our imagination embeddings z, similarly
tohowsomeVAEvariantstacklesemi-supervisedclassificationscenarios(Kingmaetal.,2014).
4 ExperimentalInvestigation
To assess the impact of using the imagination embeddings against the category embeddings, we use
twoevaluationbenchmarks: GuessWhat?!andCompGuessWhat?!. Moreinformationaboutthetraining
procedurecanbefoundinAppendixA.2.
4.1 GuessWhat?! Evaluation
In this experiment, we evaluate the accuracy of the Oracle in answering questions and the accuracy of
the Guesser in selecting the target object. We consider as both training and evaluation data all the gold
dialogues(andquestions)thathavebeenlabeledassuccessfulinthedataset(DeVriesetal.,2017). We
wanttohighlightthatinthisevaluationphase,themodelsusinglabel-awareobjectencodingshavegold
informationbothattrainingandtesttime. ThisistruebothfortheOracleandGuessermodels. However,
thisdoesnotholdforallothermodelsusingtheimaginationcomponent.
4.1.1 ExperimentalSetup
Oracletask. Weevaluatetheimagination-basedOracleandcompareittoseveralcombinationsofthe
followingbaselineswithandwithoutcategoryembeddingsfromDeVriesetal.(2017): 1) MAJORITY:
majorityclassifier;2)QUESTION: usesonlythequestion;3)IMAGE: usesonlytheimagerepresentation;
4) CROP: usesonlythecroprepresentationofthetargetobject.
Guesser task. Similarly, we compare the GDSE model using imagination embeddings
(GDSE+IMAGINATION) with the following label-aware baselines: 1) text-only baselines using
LSTMencoder(LSTM)andHierarchicalRecurrentEncoder-Decoderarchitecture(Serbanetal.,2017)
(HRED) as well as their corresponding multi-modal models LSTM+IMAGE and HRED+IMAGE; 2)
PARALLELATTENTION (Zhuangetal.,2018)and GDSE (Shekharetal.,2019). Wealsocomparewith
variants of the above that do not use any category embeddings or gold category labels (*-NOCAT), as
wellasmodelswithpredictedcategorylabels(*-PREDCAT).3
3WetrainanobjectclassifierusingasinputtheResNet-101featuresgeneratedfortheobjectcrop.Itachieves65%accuracy
evaluatedonallobjectsintheGuessWhat?!testset.
PERCEPTUALINFORMATION CATEGORICALINFORMATION
SUPER
MODEL(DV-QUES+SPATIAL)LOCATION SHAPE COLOR TEXTURE SIZE
CATEGORY
OBJECT
+CROP 66.86% 69.08% 67.25% 68.30% 65.09% 88.94% 80.48%
+CATEGORY 67.48% 68.42% 61.83% 70.08% 60.14% 97.09% 88.82%
+CATEGORY+CROP 65.27% 60.34% 59.14% 65.76% 59.08% 96.19% 86.32%
+IMAGINATION 68.62% 69.08% 67.64% 69.86% 62.65% 90.05% 82.32%
Table2: OracleaccuracygroupedbyquestiontypeforthebestOraclemodelwithcategoryinformation
(DV-QUES+SPATIAL)andformulti-modalvariantsusingeitherperceptualorcategoricalinformation.
4.1.2 Results
Oracle task. In Table 1, we divide config-
MODEL VAL TEST
urations into category-aware (De Vries et al.,
MAJORITY 53.80% 49.10%
2017) and multi-modal. The model refer- QUES 58.30% 58.80%
ence for several other publications on Guess-
IMG 53.30% 53.30%
CROP 57.30% 57.00%
What?! is a category-aware model QUES-
DV-QUES+CAT 74.20% 74.30%
TION+SPATIAL+CATEGORY. However, by re-
DV-QUES+CROP+CAT 75.60% 75.30%
lying on symbolic information in the form of DV-QUES+SPATIAL+CAT 78.90% 78.50%
category labels, it is inevitably not truly multi- DV-QUES+SPATIAL+CROP+CAT 78.30% 77.90%
DV-QUES+SPATIAL+IMG+CAT 76.80% 76.50%
modal anymore because the heavy-lifting is
done by these embeddings. As shown in the re- DV-QUES+CROP 70.90% 70.80%
DV-QUES+IMG 59.80% 60.20%
sults, other multi-modal models such as QUES-
DV-QUES+SPATIAL 68.80% 68.70%
TION+SPATIAL+CROP and QUESTION+CROP, DV-QUES+SPATIAL+CROP 74.00% 73.80%
are not able to learn effective representations DV-QUES+SPATIAL+CROP+IMG 72.30% 72.10%
IMAGINATION 75.78% 75.88%
to bridge the gap between category-aware and
category-free models. On the other hand, the
Table 1: Oracle results on gold questions: we com-
proposed imagination model is able to reduce
pare the IMAGINATION Oracle model to models
this gap without relying on gold information as
from De Vries et al. (2017) (DV-*). We group
input. Indeed, we are able to learn category-
them into models relying on gold category labels
aware and context-aware latent codes by using
(W/ CAT) and models that only use multi-modal
categoryinformationonlyinourlossfunction.
perceptualinformation(MM).
Weinvestigatethisargumentfurtherbyusing
a rule-based question classifier (Shekhar et al., 2019) to partition the test questions according to their
type. Table 2 summarizes this analysis; we include models considered truly multi-modal and the best
Oracle model QUESTION+SPATIAL+CATEGORY. The latter can answer with high accuracy questions
aboutspecificobjectinstances(e.g.,“isitthedog?”)orsuper-categories(e.g.,“isitananimal?”)sinceit
isusingcategoryembeddingsasinput. However,whenitcomestoansweringquestionsaboutperceptual
propertiesofthetargetobject,itlosessomeaccuracypointsbecausetheperceptualinformationismissing
fromthecategoryembeddingrepresentingacentroidoftypicalinstancesseenattrainingtimeonly. On
theotherhand,theIMAGINATIONmodelisabletobringimprovementsof1.34%,5.81%,and2.52%for
location,color,andshapequestions,respectively. Onquestionsrelatedtoperceptualinformation,models
using crop information seem to be on par with the IMAGINATION model. However, our model is able
to obtain an improvement over +CROP of 1.84% in object questions and of 1.11% on super category
questionssolelybyrelyingontheimaginationembeddings.
Guessertask. Table3comparesseveralcategory-awareandmulti-modalmodels; PARALLELATTEN-
TION and GDSE-SL are the two best performing configurations. However, when PARALLELATTEN-
TION does not have access to category information (PARALLELATTENTION-NOCAT) its performance
drops by 3.7% (also noted by Zhuang et al. (2018)). We confirmed the same behavior for GDSE-SL as
well (GDSE-SL-NOCAT), noticing a more significant drop in performance of 16.95% which is in line
withthesimplerLSTM+IMAGEmodel. Ontheotherhand,GDSE-SLwithourimaginationcomponent
(GDSE-SL+IMAGINATION), performs comparably with the category-aware model and better then all
ESAB
TAC/W
MM
Gameplay AttributePrediction Zero-shotGameplay
ACCURACY A-F1 S-F1 AS-F1 L-F1 ND-ACC OD-ACC GROLLA
RANDOM 15.81% 15.1 0.1 7.8 2.8 16.9% 18.6% 13.3
DEVRIES-SL 41.5% 46.8 39.1 48.5 42.7 31.3% 28.4% 38.5
DEVRIES-RL 53.5% 45.2 38.9 47.2 42.5 43.9% 38.7% 46.2
GDSE-SL 49.1% 59.9 47.6 60.1 48.3 29.8% 22.3% 43.0
GDSE-CL 59.8% 59.5 47.6 59.8 48.1 43.4% 29.8% 50.1
GDSE-SL+IMAGINATION 43.82% 56.23 47.37 57.2 51.73 39.19% 39.90% 45.50
GDSE-CL+IMAGINATION 51.98% 57.59 47.6 58.31 50.42 46.56% 46.96% 50.74
Table4: ResultsfortheCompGuessWhat?! benchmark(Sugliaetal.,2020). Weassessmodelqualityin
termsofgameplayaccuracy,attributepredictionquality,measuredintermsofF1fortheabstract(A-F1),
situated(S-F1),abstract+situated(AS-F1)andlocation(L-F1)predictionscenario,aswellaszero-shot
learninggameplay. GROLLA isamacro-averageoftheindividualscores.
multi-modal models. Therefore we argue that it is possible to learn object representations that, given
a representation for the current dialogue state, allow for discriminating the target object among other
candidateswithoutrelyingonsymbolicinformation.
4.2 CompGuessWhat?! Evaluation MODEL VAL TEST
HUMAN 90.80% 90.80%
CompGuessWhat?! is a benchmark proposed to
RANDOM 17.10% 17.10%
assessthequalityofmodels’representationsand
LSTM 62.10% 61.30%
out-of-domain generalization. It includes the HRED 61.80% 61.00%
following tasks: a) in-domain gameplay accu- LSTM+IMAGE 61.50% 60.50%
HRED+IMAGE 61.60% 60.40%
racy, – selecting the target object with model
PARALLELATTENTION 63.80% 63.40%
generateddialoguesasinput,b)attributepredic- GDSE-SL 63.14% 62.96%
tion task – assessing the ability of the dialogue GDSE-SL-PREDCAT 52.08% 51.00%
representationtorecovertargetobjectattributes, LSTM+IMAGE-NOCAT 50.10% 48.60%
and c) zero-shot gameplay accuracy – selecting PARALLELATTENTION-NOCAT 55.70% 59.70%
GDSE-SL-NOCAT 46.11% 46.01%
thetargetobjectamongobjectsbelongingtocat-
GDSE-SL-IMAGINATION 59.54% 58.90%
egoriesneverseenbythemodelduringtraining.
IncontrasttoGuessWhat?!,theattributepredic- Table 3: Guesser accuracy on successful gold di-
tion and zero-shot tasks give us more insights alogues: we compare GDSE-SL-IMAGINATION
about the quality of the learned representations withi)modelsthataretrulymulti-modal(MM)and
andthemodel’sgeneralizationability. ii)usecategoryinformation(CATEGORY).
4.2.1 ExperimentalSetup
We compare imagination-based models with baselines used in Suglia et al. (2020): 1) RANDOM: ran-
domly selects an object; 2) DEVRIES-SL: presented in De Vries et al. (2017) trained using Supervised
Learning; 3) DEVRIES-RL: DEVRIES-SL with Questioner fine-tuned using Reinforcement Learning
(Strubetal.,2017);andwhere4) GDSE-SL and5) GDSE-CL arethesameasusedinSection4.1.
4.2.2 Results
In-domaingameplay. Table4presentstheresultsontheCompGuessWhat?!benchmark. Modelsare
taskedtoplaythegamebygeneratingupto10questionsandcorrespondinganswers. Firstly,wenotethat
the results for GDSE-CL+IMAGINATION—the collaborative version of the model with Imagination—
is still in the same ballpark of more complex models, such as DEVRIES-RL that is using category
embeddings as input. At the same time, we notice that overall both imagination models perform worse
thantheGDSE-*models. Weimputethisdroptotheintroductionofadditionallosstermsthatprobably
havechangedthetrainingdynamicofacumbersomemodulo-nmulti-tasktraining(Shekharetal.,2019).
This downside calls for a more principled way of handling tasks of different complexity (i.e., question
generationandtargetprediction)inamulti-tasklearningsystem;weleavethisforfuturework.
YROGETAC
MM
Attribute prediction. Table 4 reports the attribute prediction task results. In this scenario, we under-
line the fact that the dialogue state representation generated by the Guesser model is used to recover
severaltypesofattributesassociatedwiththetargetobject. Inthiswork,weusethesamedialoguestate
representation as used by Shekhar et al. (2019) and only focus on improving the object representations
using the imagination component. Indeed, the best imagination model GDSE-SL+IMAGINATION is in
linewithGDSE-SL,currentlythebestmodelintermsofattributeprediction. Inparticular,eventhough
the dialogue state representation is only indirectly affected by the imagination embeddings (via a dot-
product operation to score the candidate objects), we can still see an improvement in terms of F1 for
Locationattributes(L-F1)andsimilarperformanceforSituated attributeprediction(S-F1). Bothcanbe
considered,tosomeextent,aresultofbettersituatedobjectrepresentations.
Zero-shotgameplay. AsunderlinedinSection3,theimaginationmodule’smainstrengthistobeable
to distill imagination embeddings from perceptual information only, without relying on externally pro-
vided category labels. The zero-shot gameplay scenario from CompGuessWhat?! (Table 4) sheds some
light on the ability of the model to generalize to out-of-distribution examples. In the out-of-domain
gameplay scenario where candidate objects belonging to categories never seen before are present, both
imagination-based models GDSE-SL+IMAGINATION and GDSE-CL+IMAGINATION outperform the
previousbestperformingsystem DEVRIES-RL by1.2%and8.26%, respectivelyintermsof ODaccu-
racy (OD-ACC). By analyzing their output, we notice that the best imagination model achieves higher
accuracybylearningabettergameplaystrategyinvolvinghalftheamountoflocationquestionsgenerated
byDEVRIES-RL(39.68%vs75.84%;seeAppendixA.3formoredetails). Afurtherimprovementinthe
near-domain scenario (ND-ACC) confirms the effectiveness of the imagination component to generate
categoryembeddingsforobjectson-the-flyusingonlyperceptualinformation.
Out-of-domainerroranalysis. Lastly,wereportanerroranalysiscomprising50dialoguesselectedat
random from out-of-domain games (for more details refer to Appendix A.3). First, we manually anno-
tatedtheOracleanswersandpartitionedthemaccordingtotheirtypeusingthesamequestionclassifier
used for the Oracle Task (Section 4.1.2). 83% of super-category questions (from a total of 80) were
correctlyansweredbythemodeland63.36%colorrelatedquestions(fromatotalof88)werecorrectly
answered. For instance, as shown in Figure 3, GDSE-CL is not able to answer correctly the question
“isitaperson?” becauseitdoesnothavecategoryinformationforthelabel“girl”butonlyforthelabel
GDSE-CL+imagination GDSE-CL GDSE-CL+imagination GDSE-CL GDSE-CL+imagination GDSE-CL
is it a vase? yes is it a cup? no is it a person? yes is it a person? no is it an animal? yes Is it a giraffe? no
is i b sii i st s o i t i tt tsth h th b l e eeie et s o ttow wa an n lh nele eo t de h so l n e te tn h b t t l ht eeh oh e fi t pe n t t? t lg oe wle ? t? ?oft ? y y y yne e e eos s s s isi t is h s i sti et hs t h h i e tcet e h o t tb i hhe his r ts a hn ea e oi c een vit ft gb k i r i ud a t sb a lgj e roale ie a tb br mk s fc ho c o lshn t t e e?u k ht oi ? sit n tf s e pl e hl te d d ? ii a bp? nn ? in r ahg tt g co h on ktte foeh ? ia ?e t r n n n n n n no o o o o o o i is sis i it tit t w sth h ee te aa k rw tii nd uh g soo a :ln e n s t p h h uee e cr lsb m coik en ee t? s?? sy y ye e es s s is is i ti ts ht hii est e i t tt s h h th b he ei e it k e g bea t rhr i ikib e ge n ee hi k b tn twe h ?i k? eb he a eb?g ea? lc tk o? yn n n neo o o os isi i is s si t i i i is t t tt h t t ti h he ht e e eit soh m w w ene vi ne d h hio s t o od w iin rbl ll ee ee e h le ? o aa ai ?n s nn n e it i i mm mh he aa ae ll la ?? ?d y y y y ye e e e es s s s s iI s is si s t i h it itt s tte ht hh th te h eeo he e b b ea bbsI w as n j b rri e a vt o c oio lti cm n iat k wi swo n t icga im ad n bg nt k r l h c l o a p ehat o jea a lhu ?al 'nn ar scen a ni? ?n m k nd hdak ei ea lmn t ei al in ? ?m wa d lt i a th o hle n yn n n n neo o o o os
boi ts t li et sb ae ntw de te hn e t ph oe t t iw n o th e no is it the furthest right? no is it the tb hik ee b t iko e t ?he left of yes is le t fth e si dh ee a od f tfa hc ei n pg ic t to u rt eh ?e yes is the object brown? no
is it next m toi d thd ele b? lue cup? yes is it the left most corner? no is it the whole bike? yes is the head le ff ta ?cing to the yes is the handle black? no
status: failed status: failed status: failed status: success status: failed
Figure 3: Qualitative examples in the zero-shot gameplay scenario: the categories ’girl’ and ’antelope’
are not present in MSCOCO and therefore cannot be encoded by the GDSE-CL model. On the other
hand, the imagination model is able to distill imagination embeddings by using the crop features only
(forthesakeofpresentationqualityweremoveconsecutiverepeatedquestions).
“person”. Ontheotherhand, GDSE-CL+IMAGINATION isabletoa)categorizetheobjectasamember
of the super-category “person”, and b) correctly ground the expression “kid on the bike” to the target
object. The same behavior can be observed when the “antelope” is the target object. Antelopes are not
part of the MSCOCO classes, and therefore have not been seen by the model during training. First, the
model refers to it as “animal”, hence the Oracle is able to correctly answer the question even though
“antelope” was never involved in the training. Secondly, we found that the number of No answers for
GDSE-CL is considerably higher (88.06%) than GDSE-CL+IMAGINATION (51.02%), validating our
hypothesisthattheOracledoesnotknowhowtodealwithunseeninstances. Finally,intheimagination
dialogueofthefirstexample,eventhoughthegeneratedquestion/answerswereprobablyreferringtothe
correctobject,theGuessermodeliseventuallyunabletoguesscorrectly. Moreworkisrequiredtobetter
fusethelanguagemodalityandtheobjectrepresentationstoimproveitsperformance.
5 RelatedWork
Concerning unsupervised learning of concept representations, Bruni et al. (2014) first learn modality-
specificrepresentationsandthenfusethemintoaunifiedrepresentationforeachconcept. However,they
relyonhand-craftedbagsofvisualfeatures,makingtheapproachlaborioustoextendtonewdomainsand
games. Kielaetal.(2018)copewiththisissuebyrelyingonCNNmodelstoextractlatentfeaturesfrom
images for instances of specific objects. Lazaridou et al. (2015) use a margin loss but in the context of
maximizingthesimilaritybetweenthevisualrepresentationofanounphraseanditscorrespondingtext
representation. Similarly,Collelletal.(2017)learnamappingbetweentheResNetfeaturesandtheword
embeddings of a concept. As discussed in Section 2, unlike our imagination embeddings, these purely-
perceptualrepresentationsareneithercategory-awarenorcontext-aware. Silbereretal.(2016)presenta
multi-modalmodelthatusesadenoisingauto-encoderframework. Unlikeus,theydonotuseperceptual
information as input but rely on an attribute-based representation derived from an additional attribute
predictor. However,theydouseareconstructionloss(cross-entropylossforattributeprediction)andan
auxiliary category loss during training. Their training scheme is more complex as they first separately
train the AE for each modality and then fuse them, which we avoid by adopting a single end-to-end
architecture. EbertandPavlick(2019)usedVAEstolearngroundedrepresentationsforlexicalconcepts.
However, as discussed in Section 3, VAEs are not as well suited as RAEs to representation learning for
ourimaginationmodule. Inthecontextofguessinggames,allthepreviousapproachesrelyoncategories
embeddings(DeVriesetal., 2017; Shekharetal.,2019; Strubetal.,2017; Zhuangetal.,2018; Shukla
et al., 2019) (see Section 2). Our imagination component can be flexibly integrated in any of them by
replacingthecategoryembeddingswithimaginationembeddings.
6 Conclusions
We argued that existing models for learning grounded conceptual representations fail to learn compo-
sitional and generalizable multi-modal representations, relying instead on the use of category labels
for every object in the scene both at training and inference time (De Vries et al., 2017). To address
this, we introduced a novel “imagination” module based on Regularized Auto-Encoders, that learns
a context-aware and category-aware latent embedding for every object directly from its image crop,
withoutusingcategorylabels. Weshowedstate-of-the-artperformanceintheCompGuessWhat?! zero-
shotscenario(Sugliaetal.,2020),outperformingcurrentmodelsby8.26%ingameplayaccuracywhile
performing comparably on the other tasks to models which use category labels at training time. The
imagination-based model also shows improvements of 2.08% and 12.86% in Oracle and Guesser accu-
racy. Finally, we conducted an extensive error analysis and showed that imagination embeddings help
toreasonaboutobjectvisualpropertiesandattributes. Forfuturework,weplanto1)integratecategory
labelsattrainingtimeinamoreprincipledwayfollowingadvancesinsemi-supervisedlearning(Kingma
etal.,2014);2)improvethemulti-tasklearningprocedurepresentedin(Shekharetal.,2019)tooptimize
atthesametimemultipletasksofdifferentcomplexities.
References
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
2018. Bottom-upandtop-downattentionforimagecaptioningandvisualquestionanswering. InProceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition,pages6077–6086.
LawrenceWBarsalou. 2008. Groundedcognition. Annu.Rev.Psychol.,59:617–645.
LisaBeinborn,TeresaBotschen,andIrynaGurevych. 2018. Multimodalgroundingforlanguageprocessing. In
Proceedingsofthe27thInternationalConferenceonComputationalLinguistics,pages2325–2339.
YoshuaBengio,AaronCourville,andPascalVincent. 2013. Representationlearning: Areviewandnewperspec-
tives. IEEEtransactionsonpatternanalysisandmachineintelligence,35(8):1798–1828.
YonatanBisk,AriHoltzman,JesseThomason,JacobAndreas,YoshuaBengio,JoyceChai,MirellaLapata,Ange-
likiLazaridou,JonathanMay,AleksandrNisnevich,etal. 2020. Experiencegroundslanguage. arXivpreprint
arXiv:2004.10151.
EliaBruni,Nam-KhanhTran,andMarcoBaroni. 2014. Multimodaldistributionalsemantics. Journalofartificial
intelligenceresearch,49:1–47.
Guillem Collell, Ted Zhang, and Marie-Francine Moens. 2017. Imagined visual representations as multimodal
embeddings. InThirty-FirstAAAIConferenceonArtificialIntelligence.
George E Dahl, Tara N Sainath, and Geoffrey E Hinton. 2013. Improving deep neural networks for lvcsr using
rectified linear units and dropout. In 2013 IEEE international conference on acoustics, speech and signal
processing,pages8609–8613.IEEE.
Harm De Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville. 2017.
Guesswhat?! visualobjectdiscoverythroughmulti-modaldialogue. InProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition,pages5503–5512.
DylanEbertandElliePavlick. 2019. Usinggroundedwordrepresentationstostudytheoriesoflexicalconcepts.
InProceedingsoftheWorkshoponCognitiveModelingandComputationalLinguistics,pages160–169,Min-
neapolis,Minnesota,June.AssociationforComputationalLinguistics.
Desmond Elliott and A´kos Ka´da´r. 2017. Imagination improves multimodal translation. In Proceedings of the
EighthInternationalJointConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pages130–
141.
ParthaGhosh,MehdiSMSajjadi,AntonioVergari,MichaelBlack,andBernhardScho¨lkopf. 2019. Fromvaria-
tionaltodeterministicautoencoders. arXivpreprintarXiv:1903.12436.
IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,AaronCourville,
andYoshuaBengio. 2014. Generativeadversarialnets. InAdvancesinneuralinformationprocessingsystems,
pages2672–2680.
DouweKielaandStephenClark. 2015. Multi-andcross-modalsemanticsbeyondvision: Groundinginauditory
perception. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,
pages2461–2470.
DouweKiela,LuanaBulat,andStephenClark. 2015. Groundingsemanticsinolfactoryperception. InProceed-
ings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International
JointConferenceonNaturalLanguageProcessing(Volume2: ShortPapers),pages231–236.
DouweKiela,AlexisConneau,AllanJabri,andMaximilianNickel. 2018. Learningvisuallygroundedsentence
representations. InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTechnologies,Volume1(LongPapers),pages408–418.
GaryKingandLangcheZeng. 2001. Logisticregressioninrareeventsdata. Politicalanalysis,9(2):137–163.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980.
DiederikPKingmaandMaxWelling. 2013. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114.
DurkPKingma,ShakirMohamed,DaniloJimenezRezende,andMaxWelling. 2014. Semi-supervisedlearning
withdeepgenerativemodels. InAdvancesinneuralinformationprocessingsystems,pages3581–3589.
JamieKiros,WilliamChan,andGeoffreyHinton. 2018. Illustrativelanguageunderstanding: Large-scalevisual
groundingwithimagesearch. InProceedingsofthe56thAnnualMeetingoftheAssociationforComputational
Linguistics(Volume1: LongPapers),pages922–933.
Thomas K Landauer and Susan T Dumais. 1997. A solution to plato’s problem: The latent semantic analysis
theoryofacquisition,induction,andrepresentationofknowledge. Psychologicalreview,104(2):211.
StephenLaurenceandEricMargolis. 1999. Conceptsandcognitivescience. Concepts: corereadings,3:81.
Angeliki Lazaridou, Marco Baroni, et al. 2015. Combining language and vision with a multimodal skip-gram
model. InProceedingsofthe2015ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-
tionalLinguistics: HumanLanguageTechnologies,pages153–163.
KenMcRae,GeorgeSCree,MarkSSeidenberg,andChrisMcNorgan. 2005. Semanticfeatureproductionnorms
foralargesetoflivingandnonlivingthings. Behaviorresearchmethods,37(4):547–559.
FlorianSchroff,DmitryKalenichenko,andJamesPhilbin. 2015. Facenet: Aunifiedembeddingforfacerecogni-
tionandclustering. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
815–823.
IulianVladSerban,AlessandroSordoni,RyanLowe,LaurentCharlin,JoellePineau,AaronCourville,andYoshua
Bengio. 2017. Ahierarchicallatentvariableencoder-decodermodelforgeneratingdialogues. InThirty-First
AAAIConferenceonArtificialIntelligence.
Ravi Shekhar, Aashish Venkatesh, Tim Baumga¨rtner, Elia Bruni, Barbara Plank, Raffaella Bernardi, and Raquel
Ferna´ndez. 2019. Beyond task success: A closer look at jointly learning to see, ask, and guesswhat. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers),pages2578–2587.
Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni, Matthew Turk, and William Yang Wang.
2019. What should i ask? using conversationally informative rewards for goal-oriented visual dialog. In
Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages6442–6451.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2016. Visually grounded meaning representations. IEEE
transactionsonpatternanalysisandmachineintelligence,39(11):2284–2297.
LucSteels. 2015. TheTalkingHeadsexperiment: Originsofwordsandmeanings,volume1. LanguageScience
Press.
FlorianStrub,HarmDeVries,JeremieMary,BilalPiot,AaronCourvile,andOlivierPietquin. 2017. End-to-end
optimizationofgoal-drivenandvisuallygroundeddialoguesystems. InProceedingsofthe26thInternational
JointConferenceonArtificialIntelligence,pages2765–2771.
Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank, and
OliverLemon. 2020. CompGuessWhat?!: Amulti-taskevaluationframeworkforgroundedlanguagelearning.
InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages7625–7641,
Online,July.AssociationforComputationalLinguistics.
JiangWang,YangSong,ThomasLeung,ChuckRosenberg,JingbinWang,JamesPhilbin,BoChen,andYingWu.
2014. Learning fine-grained image similarity with deep ranking. In Proceedings of the IEEE Conference on
ComputerVisionandPatternRecognition,pages1386–1393.
Ludwig Wittgenstein, Gertrude Elizabeth Margaret Anscombe, and Rush Rhees. 1953. Philosophische Unter-
suchungen.(Philosophicalinvestigations). BasilBlackwell.
BohanZhuang,QiWu,ChunhuaShen,IanReid,andAntonVanDenHengel. 2018. Parallelattention: Aunified
frameworkforvisualobjectdiscoverythroughdialogsandqueries. InProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition,pages4252–4261.
A Appendix
A.1 Modeldetails
AsdescribedinSection3ofthemainpaper,weextendboththeOracleandGuessermodelwithanimag-
ination component. For both roles, we keep the same model structure for the imagination component.
InthispaperweimplementE asa2-layerfeed-forwardneuralnetworkwithReLU(Dahletal.,2013)
φ
activation function. We acknowledge that many other implementations are possible in this case and we
leavemorecomplexdesignsforfuturework. Giventhelatentcodez generatedbythefunctionE ,we
i φ
useadecoderD togeneratethereconstructedperceptualinput(imagined)oftheobjecto ,D (z ) = v˜ .
θ i θ i i
Ascommonpractice,wedefinethedecoderD assymmetrictothearchitectureoftheencoderE . For
θ φ
the category embeddings size d , as in (Shekhar et al., 2019), we use 256 and 512 for the Oracle and
c
Guesserrespectively. Fortheimaginationcomponent,werunagridsearchinvolvingseveralparameters
forthelatentcodezsuchas(16,32,64,128,256,512). Forbothroles,wechoose512becauseitwasthe
value that lead to the highest accuracy on the validation set. We also experimented with several values
for the coefficient α of the regularization term LREG: (1e-3, 1e-5, 1e-6, 1e-7). For the Oracle the best
value resulted to be 1e − 7, while 1e − 5 for the Guesser. When training the imagination component
withtheobjectcategoryloss,duetotheclassimbalance,weapplylossweighting. Wecomputetheclass
weights using the method reported in (King and Zeng, 2001). For the margin value η we opted for 1.0
after experimenting with a less effective dynamic margin that would change depending on the distance
betweentheconceptsintheWordNethierarchy.
A.2 Trainingdetails
For both roles, we train the models using the Adam optimizer (Kingma and Ba, 2014). For the Oracle
and Guesser training we use 0.0001 as learning rate. In both cases, we use the original GuessWhat?!
validation set to select the best model that is used in the evaluation on the test set. As described in
(Shekhar et al., 2019), we use a modulo-n training procedure to jointly optimize both the Guesser and
Questioner. In our experimental evaluation we run a grid search of several values of n such as 3,5,7.
We selected 5 as the best performing value on the validation set. For a fair comparison with all the
GDSEmodelvariantstrainedwithSupervisedLearningandCollaborativeLearning,wemadethesame
architectural choices and hyperparameters values. Please refer to the original codebase implementation
availableonGitHub4. AnotherpointofdifferenceisintheCollaborativeLearningfine-tuningphasefor
the Guesser model. During this phase, only the Questioner and Guesser models are fine-tuned whereas
theOraclemodelisfixed(Shekharetal.,2019)therefore,wedecidedtousethebestperformingOracleso
thattheGuessermodelisnotnegativelyaffectedbyalessperformingOracleandalsotobecomparable
withtheoriginalimplementation.
A.3 Erroranalysis
In order to provide a more fine-grained evaluation of the generated dialogues, we adapt the quality
evaluation script presented by Suglia et al. (2020) and extend it with additional metrics. First of all, it
reliesonarule-basedquestionclassifierthatclassifiesagivenquestioninoneofsevenclasses: 1)super-
category(e.g.,“person”,“utensil”,etc.),2)inanimateobject(e.g.,“car”,“oven”,etc.),3)animateobject
(e.g., “dog”, “cat”, etc.), 3) “color”, 4) “size”, 5) “texture”, 6) “shape” and “location”. The question
classifierisusefultoevaluatethedialoguestrategylearnedbythemodels. Inparticular, welookattwo
typesofturntransitions: 1)super-category→object/attr,itmeasureshowmanytimesaquestionwithan
affirmativeanswerfromtheOraclerelatedtoasuper-categoryisfollowedbyeitheranobjectorattribute
question(where“attribute”representstheset{color,size,texture,shapeandlocation};2)object→attr,
it measures how many times a question with an affirmative answer from the Oracle related to an object
is followed by either an object or attribute question. We compute the lexical diversity as the type/token
ratioamongallgames,questiondiversityandthepercentageofgameswithrepeatedquestions. Wealso
evaluatethepercentageofdialogueturnsinvolvinglocationquestions. Table5and6showtheresultsof
theseanalysisforthemodelsGDSE-CLandGDSE-CL+imaginationanalyzedinthispaper.
4https://github.com/shekharRavi/Beyond-Task-Success-NAACL2019
Using the above-mentioned question classifier, we completed an error analysis trying to understand
the quality of the generated gameplay in a zero-shot scenario from the point of view of the answers
prediction performance and the guesser accuracy. In particular, we randomly sampled a pool of 50
referencegamesfromtheout-of-domainzero-shotscenarioandwemanuallyannotatedwhetheragiven
answer generated by the Oracle model was correct or not. Table 7 shows the results of the manual
annotation step. The model confirms high performance in answering questions about super-category
informationdemonstratingthatitisabletocorrectlycategoriesobjectsinmacro-categorieseventhough
ishasnotseenthembefore.
A.3.1 Zero-shotgameplayquality
%games %turns
Lexical Question Super-cat-> Object->
Model repeated location Vocab.size Accuracy
diversity diversity obj/attr attribute
questions questions
DeVries-RL 0.13 1.77 99.48 97.39 98.70 78.07 702.00 43.92%
GDSE-CL 0.17 13.74 66.75 93.62 66.27 31.23 1260 43.42%
GDSE-CL+Imagination 0.10 8.56 91.80 93.15 60.72 39.90 808 46.70%
Table 5: Comparison between the quality of gameplay in the near-domain zero-shot scenario between
GDSE-CLandGDSE-CLwithimagination. Numberoftotalturns10.
%games %turns
Lexical Question Super-cat-> Object->
Model repeated location Vocab.size Accuracy
diversity diversity obj/attr attribute
questions questions
DeVries-RL 0.24 2.96 98.49 91.26 98.57 75.84 1275 38.73%
GDSE-CL 0.14 7.86 66.32 91.67 72.33 26.03 1002 29.83%
GDSE-CL+Imagination 0.10 8.57 89.19 94.82 58.51 39.68 814 46.93%
Table 6: Comparison between the quality of gameplay in out-of-domain zero-shot scenario between
GDSE-CLandGDSE-CLwithimagination. Numberoftotalturns10.
Questiontype Accuracy Count Questiontype Accuracy Count
Inanimateobject 65.48% 168 Inanimateobject 81.71% 164
Animateobject 53.33% 15 Animateobject 70.00% 10
Supercategory 83.33% 60 Supercategory 67.61% 71
Location 78.86% 175 Location 72.97% 148
Size 100.00% 1 Size 100% 1
Color 58.33% 24 Color 63.64% 88
Parts 100.00% 2 Parts 71.43% 7
Table 7: Error analysis results completed on the Out-of-domain zero-shot scenario for the model
GDSE-CL+Imagination(ontheleft)andGDSE-CL(ontheright).

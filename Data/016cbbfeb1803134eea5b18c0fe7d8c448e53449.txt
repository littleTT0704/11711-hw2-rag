LEATHER: A Framework for Learning to Generate Human-like Text in
Dialogue
AnthonySicilia1 and MaliheAlikhani1,2
{anthonysicilia, malihe}@pitt.edu
1IntelligentSystemsProgramand2ComputerScienceDepartment
UniversityofPittsburgh,Pittsburgh,PA,USA
Abstract nothuman-like(Shekharetal.,2019). Evencare-
fullydesignedrule-basedsystemsarebrittleinthe
Algorithmsfortext-generationindialoguecan
presenceofunforeseendata-shift.
be misguided. For example, in task-oriented
Theoreticalanalysesoflearningareimperative
settings,reinforcementlearningthatoptimizes
only task-success can lead to abysmal lexi- as they provide solutions to these issues. For ex-
cal diversity. We hypothesize this is due to ample,traditional(PAC)learningtheory(Valiant,
poor theoretical understanding of the objec- 1984)studiessimilarissuesarisingfromcomputa-
tives in text-generation and their relation to tionalalgorithmsforlearningtoclassify. Progress
thelearningprocess(i.e., modeltraining). To
inourunderstandinghasbeenimpressive,ranging
this end, we propose a new theoretical frame-
fromcomprehensiveguaranteesondata-efficiency
workforlearningtogeneratetextindialogue.
(Shalev-ShwartzandBen-David,2014)toinsights
Comparedtoexistingtheoriesoflearning,our
framework allows for analysis of the multi- foralgorithm-designwhenthelearnerisfacedwith
faceted goals inherent to text-generation. We data-shift(Zhaoetal.,2019;Zhangetal.,2019b;
useourframeworktodeveloptheoreticalguar- TachetdesCombesetal.,2020). Whiletraditional
antees for learners that adapt to unseen data. theorymaybeapplicabletosimplegenerationob-
As an example, we apply our theory to study
jectives like next-word prediction, it is unfortu-
data-shift within a cooperative learning algo-
natelyunabletomodelmorediversegoals. Thatis
rithmproposedfortheGuessWhat?! visualdi-
tosay,itisinsufficienttostudyreplicationofthe
alogue game. From this insight, we propose
diversequalitiesinherenttohumandialogue.
a new algorithm, and empirically, we demon-
strateourproposalimprovesbothtask-success The goal of this paper is to provide a new the-
and human-likeness of the generated text. Fi- ory for analyzing the multi-faceted objectives in
nally, we show statistics from our theory are computationallearningofdialoguegeneration. In
empirically predictive of multiple qualities of particular,weproposeLEATHER1 basedonexisting
the generated dialogue, suggesting our theory
theories of computational learning. We demon-
isusefulformodel-selectionwhenhumaneval-
stratetheutilityofLEATHERwithafocusonunder-
uationsarenotavailable.
standingdata-shiftinlearningalgorithms. Wealso
1 Introduction show empirical results for a task-oriented visual
dialoguegame. Indetail,wecontributeasfollows:
Generatingcoherent,human-liketextfordialogue
1. In Section 3, we propose LEATHER, our novel
remains a challenge. Yet, it is an inseparable
theory for computational learning of dialogue
componentofopendomainandtaskorienteddia-
generation. WeusetheGuessWhat?! visualdia-
logue systems like Alexa and Siri. Undoubtedly,
loguegame(DeVriesetal.,2017)asanexam-
it is also a complex process to learn. Generation
pletogroundabstractterminologyinpractice.
basedonclassification(e.g.,next-wordprediction)
WeconcludeSection3byapplyingourtheory
over-emphasizesthelikelihoodoftext,leadingto
toanalyzeacooperativelearningalgorithmfor
blandqualities,whicharenothuman-like(Holtz-
GuessWhat?!. Ourtheoryunveilsharmfulshifts
man et al., 2019). Meanwhile, framing dialogue
indata-distributionthatoccurduringtraining.
generationasaMarkovdecisionprocessishighly
2. InSection4,weuseLEATHERtostudythegen-
data-inefficient when compared to classification
eralproblemofdata-shiftintext-generation. We
(Kakade,2003). Further,withoutcarefuldesignof
providenewtheoreticalstudythatcharacterizes
rewards,modelscansufferfrommode-collapsein
dialogue, producing repetitive behaviors that are 1LEArningTheoryforHuman-likedialoguegenERation
2202
tcO
41
]LC.sc[
1v77770.0122:viXra
Figure1: ExamplesofhumanandgenerateddialoguewithoriginalcooperativelearningalgorithmCL(Shekharetal.,2019)and
ourlearningalgorithmmotivatedbyourtheory(LEATHER).Roughly,LEATHERworksbyapplyingaseriesofteststogenerated
dialogueandcomparingthetestresultsacrossthehumanandgenerateddialogue.Well-generateddialogueisexpectedtoperform
similarlytohumandialogueonthesetests.Theexampleteststhe%ofrelevantquestions.ComparedtoCL,LEATHERasksmore
relevantquestionsandthereforebehavesmorehuman-like.AggregateempiricalresultsinSection5echothistrend.
statisticalenergyasaneffectiveempiricaltool ing. For example, see Strub et al. (2017). While
forquantifyingtheimpactofdata-shift. Aptly, POMDPs remedy the issues of typical PAC anal-
to conclude Section 4, we use energy to mo- ysis by supporting implementation of high-level
tivate an improved learning algorithm for our objectives, as we are aware, there are no empiri-
runningexample–theGuessWhat?! game. callyverifiedtheoreticalstudiesoflearningunder
3. In Section 5, empirically, we demonstrate the data-shiftinPOMDPs. Incontrast,wedemonstrate
benefitsofourLEATHER-inspiredalgorithmcom- LEATHER admits such a theory of learning, using
pared to common baselines. Importantly, we ittopredictthehuman-likenessofgeneratedtext
alsoshowourproposedstatistic(energy)ispre- underdata-shift(wherePOMDPsfallshort).
dictiveofthequalityofgenerateddialogue;i.e.,
Theories of Learning with Data-Shift Early
weexhibitalinearrelationship. Thissuggests
learning theoretic models of data-shift in classi-
LEATHERisuseful,notonlyasatheoreticaltool
ficationandregressionareduetoBen-Davidetal.
for algorithm design, but also as an empirical
(2010a,b)andMansouretal.(2009). Whilemod-
toolformodel-selection.
ernapproachesaregenerallysimilarinspirit,new
Ourframeworkispubliclyavailablethroughex-
statisticsincorporateincreasinginformationabout
perimentalcodeandaPythonpackage.2
thelearningalgorithm(Liptonetal.,2018;Kuroki
et al., 2019; Germain et al., 2020; Sicilia et al.,
2 RelatedWorks
2022a). Ultimately, such techniques tend to im-
Theories of Learning to Generate Text Most provethepredictivecapabilitiesofatheoryinprac-
widely,text-generationisframedasaclassification tical application (Rabanser et al., 2019; Atwell
problem,inwhichamodelpredictsthenextword etal.,2022). Diverseadditionalapproachestode-
provided existing context (e.g., previous words). scribing the impact of data-shift have also been
While common PAC learning analyses do apply proposed, forexample, usingintegralprobability
to classification, this theory only describes the metrics(Redkoetal.,2017,2020;Shenetal.,2018;
learner’sabilityatthenext-wordpredictiontask. In Johansson et al., 2019). Unfortunately, existing
somespecificcases,instead,PACanalysishasalso worksfocusonclassificationandregressionwhich,
beenusedtoanalyzehigh-levelobjectivesandmoti- asdiscussed,arenotdirectlyapplicabletodialogue
vateconversationalstrategies(Siciliaetal.,2022b), generation. Further,thistheorydoesnoteasilyex-
butthisanalysisisproblem-dependent. Incontrast, tend to generation (see Section 3.3). Ultimately,
ourworkoffersageneralproblem-independentfor- usingLEATHER,ourworkderivesanewstatistic(en-
malismforstudyinghigh-levelqualitiesofgener- ergy)forpredictingchangesinmodelperformance,
atedtext. Anotherfrequentformalismcomesfrom whichisapplicabletodialoguegeneration.
partially observable Markov decision processes
EvaluationofGeneratedText Therearemany
(POMDPs)usedtomotivatereinforcementlearn-
automatedmetricsforevaluationofgeneratedtext
2github.com/anthonysicilia/LEATHER-AACL2022 includingmetricsbasedonn-gramssuchasBLEU
(Papineni etal., 2002), ROUGE (Lin,2004), and (Q ,A ,...,Q ,A ) with some random length
1 1 P P
CIDEr(Vedantametal.,2015). Automatedmetrics P ≤ m. Each Q is a random question taking
i
based on neural models are also becoming more valuefromthesetQandeachA isarandoman-
i
prevalentincludingBLEURT(Sellametal.,2020), swerfromthesetA.
BertScore(Zhangetal.,2019a),andCOSMic(Inan
NotationforModeledGames Fromamodeling
etal.,2021). BruniandFernandez(2017)propose
perspective,inthispaper,wefocusonthequestion-
use of an adversary to discriminate between hu-
player and assume a human answer-player. We
man and generated text, evaluating based on the
consider learning a model that generates the ran-
generator’s ability to fool the adversary. Human
dom dialogue Dˆ = (Qˆ ,A˜ ,...Qˆ ,A˜ ) along
1 1 m m
annotationandevaluation,ofcourse,remainsthe
withapredictedgoalobjectOˆ.4 Forexample,con-
gold-standard. Notably,ourproposedframework
siderthemodelofShekharetal.(2019)westudy
encapsulatesthesetechniques,sinceitissuitable
later. Itgeneratesdialogue/predictedgoalasbelow:
for analyzing the impact of the learning process
onalloftheseevaluationstrategiesandmore(see Oˆ =Gues α(Enc β(I,Dˆ))
(1)
Section3forexamples). Qˆ =QGen (Enc (I,Qˆ ,A˜ ,...Qˆ ,A˜ )
i+1 θ β 1 1 i i
where,aptly,theneural-modelQGen : Rd → Qis
3 TheorywithExamples θ
calledthequestion-generatorandtheneural-model
In this section, we develop our new theoretical Gues : Rd → O iscalledtheobject-guesser. The
α
framework. To assist our exposition, we use the finalneural-modelEnc : I ×(Q×A)∗ → Rd is
β
GuessWhat?! visualdialoguegame–avariantof calledtheencoderandcapturespertinentfeatures
thechild’sgameISpy–asarunningexample. We fortheformermodelstoshare. Subscriptsdenote
first describe the game along with our modeling theparametersofeachmodel(tobelearned).
interestswithinthegame. Wecontinuewithade-
ModelingGoals Therearetwomainobjectives
scriptionofourtheoryandthenapplythistheoryto
weconsider. Thefirstistask-oriented:
analyzeanalgorithmthatlearnstoplaythegame.
min E[1{Oˆ (cid:54)=O}] (2)
α,β
3.1 GuessWhat?! VisualDialogueGame
whichrequiresthepredictedgoal-objectalignwith
An image and goal-object within the image are
thetruegoal. Thesecondobjectiveismoreelusive
both randomly chosen. A question-player with
fromamathematicalperspective: thegenerateddia-
access to the image asks yes/no questions to an logueDˆ shouldbehuman-like. Thatis,itshouldbe
answer-playerwhohasaccesstoboththeimage
similartothehumandialogueD. Asweseenext,
and goal-object. The question-player’s goal is to
ourtheoryisaimedatformalizingthisobjective.
identifythegoal-object. Theanswer-player’sgoal
istorevealthegoal-objecttothequestion-player 3.2 TheoreticalFramework(LEATHER)
by answering the yes/no questions appropriately.
Now,wepresentourproposedtheorywithexam-
Thequestion-andanswer-playerconverseuntilthe
plesfromtheGuessWhat?! gamejustdiscussed.
question-playerisreadytomakeaguessoratmost
mquestionshavebeenasked.3 Thequestion-player 3.2.1 Terminology
thenguesseswhichobjectwasthesecretgoal. Sets AssumeaspaceC,whichencompassesthe
set of dialogue contexts, and a space D, which
Notation for Human Games To discuss this
encompassesthesetofpossibledialogues. Ingen-
game within our theoretical framework next, we
eral,thestructureofthesesetsandrepresentation
provide some notation. We assume the possible
ofelementsthereinarearbitrarytoallowwideap-
questions, answers, and objects are respectively
plicabilitytoanydialoguesystem. Forparticular
confined to the sets Q, A, and O. We also as-
examples,considertheGuessWhat?! game: c ∈ C
sume a set of possible images I. A game be-
isanimage-goalpairandd ∈ Disalistofquestion-
tween two human players can be represented by
answer pairs. Note, we also allow an additional,
a series of random variables. The image-object
arbitrary space U to account for any unobserved
pair is represented by the random tuple (I,O).
effectsonthetestoutputs(discussednext).
The dialogue between the question- and answer-
player is represented by the random-tuple D = 4Notice, although the answer-player is still human, the
answersmayfollowadistinctdistributionduetodependence
3Bydefault,m=8followingShekharetal.(2019). onthequestions,sowedemarcatethisdifferenceby(cid:3)˜.
TestFunctions Toevaluategeneratedtext,weas- is QGen and the function E is implicitly defined
θ
sume a group of fixed test functions {h ...h } by Eq. (1). In particular, we have Dˆ ∼ P (I,O)
1 L θ
whereforeach(cid:96) ∈ [L]thefunctionh : D×U → where image I and object O are sampled from
(cid:96)
[0,1] assigns a [0,1]-valued score that character- the goal-distribution of contextualized dialogues
izessomehigh-levelpropertyofthedialogue. For ((I,O),D) ∼ G. We call E the environment of
example, a test function might be a binary value the learner and use sans serif in notation. In
indicatingpresenceofparticularquestion-type,a the GuessWhat?! example, the environment can
continuousvalueindicatingtheproportionofclari- changeforamyriadofreasons: theanswer-player
ficationquestions,asentimentscore,orsomeother could change strategies (inducing a new answer-
user-evaluation. Atestfunctioncanalsobeanau- distribution), the distribution of image I could
tomatedmetriclikelexicaldiversity,forexample. change, or the distribution of the object O could
change. All of which, can impact the function
RandomOutputs Asnoted,thespaceU primar-
(θ,c) −E → P (c). Oneimplicitfactorweencounter
ilyallowsthetesth toexhibitrandomnessdueto θ
(cid:96)
lateristhedependenceoftheenvironmentEonthe
unobserved effects. For example, this is the case
encoderparametersβ inEq.(1). Indiscussion,we
whenourtestfunctionisahumanevaluationand
mayexplicitlywriteE todenotethisdependence.
randomnessarisesfromthehumanannotator. To β
modelthis,weassumeanunknowndistributionU
FormalObjectiveofLearner Asdiscussedbe-
overU,sothatforU ∼ Uanddialogued ∈ D,the fore, the conceptual task of the dialogue learner
scoreh (cid:96)(d,U)isarandomvariable. Ingeneral,we is to produce human-like text. To rephrase more
donotassumetoomuchaccesstothisrandomness, formally: the task of the learner is to induce a
since sampling from U can be costly; e.g., it can contextualized dialogue distribution that is indis-
requirerecruitingnewannotatorsorcollectingnew tinguishable from the the goal distribution. Un-
annotations. Note,U canalsobeusedtoencapsu- fortunately,thisobjectiveismadedifficultbythe
lateadditional(observable)informationneededto complexityofdialogue. Inparticular,itisunclear
conductthetesth (cid:96) (e.g.,areferencedialogue). whatfeaturesofthedialogueareimportanttomea-
sure: should we focus on the atomic structure of
Goal Distribution Next, we assume a goal dis-
a dialogue, the overall semantics, or maybe just
tribution G over the set of contextualized di-
the fluency? Surely, the answer to this question
alogues; i.e., context-dialogue pairs in C × D.
is dependent on the application. For this reason,
Typically, G is the distribution of contextualized
we suggest the general notion of a test function.
dialogues between human interlocutors. In the
Each test {h ...h } can be hand selected prior
GuessWhat?! example, G is the distribution of 1 L
to learning to emphasize a particular goal for the
the random, iterated tuple ((I,O),D). Recall, I
dialoguelearner;e.g.,asinFigure1,h canrepre-
is the random image and O is the random goal- 1
sentauserevaluationofquestionrelevance,h can
object, which together form the context. D = 2
capturelexicaldiversity,etc. Then,thequalityof
(Q ,A ...Q ,A )isthevariable-lengthtupleof
1 1 P P
thecontextualizeddialoguedistributioninducedby
question-answer pairs produced by humans dis-
thedialoguelearnerismeasuredbypreservationof
cussingthecontext(I,O).
theoutputofthetestfunctions. Thatis,theoutput
Dialogue Learner and Environment We also of test functions should be similar when applied
assume some dialogue learner parameterized by to human dialogue about the same context. We
θ ∈ Rd. The learner may only partially control capturethisideathroughthetestdivergence:
each dialogue – e.g., the learner might only con- TD (θ)=(cid:88)L TD(cid:96)(θ)
trol a subset of the turns in each dialogue – and E (cid:96)=1 E
the mechanism through which this occurs is ac- where TD(cid:96) E(θ)=E[|h (cid:96)(D,U)−h (cid:96)(Dˆ,U)|], (3)
tually unimportant in the general setting; i.e., it (C,D)∼G, Dˆ ∼P (C), U ∼U.
θ
willnotbeassumedinourtheoreticalresults. Ul- Notice, the test divergence is not only dependent
timately,weneedonlyassumeexistenceofsome ontheparametersofthedialoguelearner,butalso
function(θ,c) −E → P (c)whereθ arethelearned theenvironmentEwhichgovernsthedistribution
θ
parameters, c ∈ C is the context, and P (c) is a P (C). Recall, this function is induced by the
θ θ
distributionoverdialoguesD. IntheGuessWhat?! learner’senvironmentanditsroleinelicitinggen-
examplediscussedpreviously,thedialoguelearner erateddialogue. Finally,withalltermsdefined,the
formalobjectiveofthedialoguelearneristypically thetestdivergencesimplyreportsaverageabsolute
tominimizethetestdivergence: differencebetweenannotations.
min TD (θ). (4)
θ E 3.3 ApplicationtoaGuessWhat?! Algorithm
Example(BLEU/ROUGE) Usefulexamplesof Inthisnextpart,weapplythetheoryjustdiscussed
test divergence are traditional evaluation metrics, to analyze a cooperative learning algorithm (CL)
using a human reference – metrics like BLEU, proposedbyShekharetal.(2019). RecallEq.(1),
ROUGE,oraccuracyatnext-wordprediction. To CLgeneratesdialogue/predictedgoalasbelow:
seetheconnection,inEq.(3),letL = 1,leth be
1
oneofthemetrics,andsetU = D. Then,h (D,U) Oˆ =Gues (Enc (I,Dˆ))
1 α β
(6)
computessome formof n-gramoverlapbetween Qˆ =QGen (Enc (I,Qˆ ,A˜ ,...Qˆ ,A˜ )
i+1 θ β 1 1 i i
the human reference and itself, so it evaluates to
1 (full overlap). On the other hand, h 1(Dˆ,U) is where QGen θ is the question-generator, Gues α is
thetraditionalnotionofthemetric(e.g.,BLEUor theobject-guesser,andEnc β istheencoder.
ROUGE).So,thetestdivergencesimplybecomes
CL Algorithm Conceptually, cooperative learn-
1minustheaverageofthemetric. Notice,thisex-
ing encompasses a broad class of algorithms in
ample shows how U can be used to encapsulate
which two or more independent model compo-
observable(random)informationaswell.
nents coordinate during training to improve each
Example (GuessWhat?!) We can also consider other’sperformance. Forexample,thiscaninvolve
amorecomplicatedexampleintheGuessWhat?! a shared learning objective (Das et al., 2017). In
game. Here, Shekhar et al. (2019) evaluate the the algorithm we consider, Shekhar et al. (2019)
human-likeness of dialogue with respect to the coordinatetrainingofasharedencoderusingtwo
question strategies. Specifically, the authors con- distinctlearningphases. Writteninthecontextof
sider a group of strategy classifiers s i : Q → ourtheory,theyare:
{0,1},i ∈ [L] which each indicate presence of
1. Task-Oriented Learning: Solve Eq. (2). Up-
a particular strategy in the input question. For dateαandβ tominimizeE[1{Oˆ (cid:54)= O}].
example, s might identify if its input is a color
1 2. Language Learning: Solve Eq. (4). Update
question“Isitblue?” ands mightidentifyifits
2 θ and β to minimize TD (θ) where the test
E
β
input is a spatial question “Is it in the corner?”.
measuresaccuracyatnext-wordprediction.
Then,oneintuitivemathematicaldescriptionofthe
The two phases repeat, alternating until training
question-strategydissimilaritymaybewritten
is finished. As is typical when training neural-
E(cid:34) (cid:88)(cid:96) (cid:12) (cid:12)1 (cid:88)P
s (Q )−
1 (cid:88)m
s (Qˆ
)(cid:12) (cid:12)(cid:35)
(5)
n be at tw chor Sk Gs, Dth we ip thar aam die ffte er rew ne tii ag bh lt es sa ur re rou gp ad ta ete ld osu ss .i Tn og
(cid:12)P i j m i k (cid:12)
i=1 j=1 k=1 dosointhetask-orientedlearningphase,Gues
α
Above captures expected deviation in proportion isdesignedtooutputprobabilityestimatesforeach
ofcolor/spatialquestionsfromthehuman-tothe objectandthenegativelog-liklihoodofthisoutput
generated-text. Italsocoincideswiththedefinition distributionisminimized. Inthelanguagelearn-
of test divergence. To see this, note the above is ingphase,QGen θ isdesignedtooutputprobabili-
Eq.(3)preciselywhenh returnstheproportionof tiesfortheindividualutterancesthatcomposeeach
i
questionsinadialoguewithtypeidentifiedbys . question. Then,thesurrogateoptimizationis:
i
Example(HumanAnnotation) Humanannota- min E(cid:104) (cid:88) L(Qˆ ,Q )(cid:105) where
θ,β i+1 i+1
tionisalsoanexample,inwhich,humansubjects i+1≤P (7)
arepresentedwithtwodialogueexamples: onema- Qˆ =QGen (Enc (I,Q ,A ...Q ,A )
i+1 θ β 1 1 i i
chinegeneratedandonefromagoalcorpuswith
bothdialoguespertainingtothesamecontext. The andLsumsthenegativelogliklihoodoftheindivid-
humanthenannotatesbothexampleswithascore ual utterances. Notice, a form of teacher-forcing
pertainingtothequalityofthedialogue(e.g.,the is used in this objective, so that the encoder and
relevance of questions as in Figure 1). So, h is question-generatorareconditionedononlyhuman
i
representedbytheannotationprocess,usingU to dialogueduringthelanguagelearningphase. This
encapsulateanyunobservedrandomeffects. Then, factwillbecomeimportantinthenextpart.
Problem Importantly,theencoderparametersβ fromsourcetotarget. When∆issmall,thechange
areupdatedinboththetask-oriented andlanguage should be small too or the target error should be
learning phases. So, in the language learning evenlowerthanthesourceerror. When∆islarge,
phase, the dialogue learner selects θ to minimize wecannotnecessarilycometothisconclusion. Im-
the test divergence in cooperation with a particu- portantly, for ∆ to be useful in practice it should
lar choiceoftheencoderparameters–letuscall notrelyontoomuchinformation. Indialoguegen-
theseβs. Then,inthetask-orientedlearningphase, eration,itisimportantfor∆toavoidrelianceon
the learned encoder parameters may change to a thetestfunctions,sincethesecanoftenencompass
new setting βt. Importantly, by changing the pa- costlysamplingprocesseslikehuman-evaluation.
rametersinEq.(1),weinduceanewenvironment AsalludedinSection2,manyadaptationbounds
E (cid:54)= E , which governs a new generation pro- exist,butasitturnsout,noneofthemaredirectly
βt βs
cess. For brevity, we set T = E and S = E . applicabletodialoguegenerationcontexts. Thisis
βt βs
This change brings us to our primary issue: the because,asweareaware,computationofallpre-
shiftinlearningenvironmentdoesnotnecessarily vious bounds relies on efficient access to the test
preservethequalityofthegenerateddialogue. In functions{h ...h }andsamplesU ∼ U,which
1 L
termsofourformaltheory,werephrase: is not always possible in dialogue. In particular,
these functions, along with the sampling process
TD S(θ)=? TD T(θ). (8) U ∼ U, might represent a time-consuming, real-
world processes like human-evaluation. For this
Withoutcontrollingthechangeintestdivergence
reason, inthenextsection, weproveanewadap-
across these two environments, it is possible the
tationboundwithnewstatistic∆,whichdoesnot
twolearningphasesarenot“cooperating”atall.
requireaccesstothetestfunctions.
LEATHER-InspiredSolution Ingeneral,itisclear
4 Text-GenerationunderData-Shift
equalitywillnothold,butwecanstillaskhowdif-
ferent these quantities will be. If they are very MotivatedbytheGuessWhat?! exampleandalgo-
different, the quality of the dialogue generation rithmCL,wecontinueinthissectionwithageneral
learned in the language learning phase may de- studyofdomainadaptationfordialoguegeneration.
gradesubstantiallyduringthetask-orientedlearn- Webeginbyproposinganew(general)adaptation
ingphase. Moregenerally,theproblemweseehere bound for LEATHER. We then apply this general
isaproblemofdata-shift. Inlearningtheory, the boundtotheGuessWhat?! algorithmCL,motivat-
study of data-shift is often referred to as domain ingfruitfulmodificationsthroughouranalysis.
adaptation. The test divergence on the environ-
4.1 ANovelAdaptationBoundforLEATHER
mentS–inwhichwelearnθ –isreferredtoasthe
sourceerror,whilethetestdivergenceontheen- TheEnergyStatisticandComputation
vironmentT–inwhichweevaluateθ –isreferred
Definition4.1. Foranyindependentrandomvari-
toasthetargeterror. Thetoolweusetoquantify
ablesAandB,thediscreteenergydistanceisde-
thechangebetweenthesourceerrorandthetarget
finedε (A,B)equalto
01
error is an adaptation bound, in which we find a
statistic∆forwhichthefollowingistrue:5 2E[1{A(cid:54)=B}]−E[1{A(cid:54)=A(cid:48)}]−E[1{B (cid:54)=B(cid:48)}] (10)
TD (θ)(cid:46)TD (θ)+∆. (9) whereA(cid:48)isani.i.dcopyofA,B(cid:48)isani.i.d. copyof
T S
B,and1{·}istheindicatorfunction;i.e.,itreturns
Then,wecanbesuretheerrorinthenewenviron- 1fortrueargumentsand0otherwise.
menthasnotincreasedmuchmorethan∆. Inthis
Thediscreteenergydistanceisamodificationof
sense, we say ∆ is a predictive statistic because
theenergydistancesometimescalledthestatistical
itpredictsthemagnitudeofthetargeterrorTD
T energy. It was first proposed by Szekely (1989)
fromthemagnitudeofthesourceerrorTD . To
S andwasstudiedextensivelybySzékelyandRizzo
putitmoreconcisely,itpredictsthechangeinerror
(2013)inthecasewhereAandB arecontinuous
5The inequality is approximate because there are often variablesadmittingaprobabilitydensityfunction.
otherstatisticsinthebound,butthroughreasonableassump- In general, and especially in dialogue, this is not
tions,onestatistic∆isidentifiedasthekeyquantityofinterest.
thecase. Aptly, ournewlysuggestedformofthe
Theseassumptionsshouldbecarefullymadetoavoidundesir-
ableresults(Ben-Davidetal.,2010b;Zhaoetal.,2019). energy distance is more widely applicable to any
variables A and B for which equality is defined. Unobserved Terms in Dialogue As noted, an
Whilegeneral,thisdistancecanbeinsensitive,es- importantbenefitofourtheoryisthatweneednot
pecially when A and B take on many values. To assumecomputationallyefficientaccesstothetest
remedythis,weintroducethefollowing. functions{h ...h }orsamplesU ∼ U. Yet,the
1 L
readerlikelynoticesanumberoftermsinEq.(12)
Definition 4.2. Let D be any set. A coarsening
dependent on both of these. Similar to the tradi-
function is a map c : D → D such that c(D) =
tionalcase,wearguethatourtheoryisstillpredic-
{c(d) | d ∈ D}isfinite,andfurther,|c(D)| < |D|.
tivebecauseitisoftenappropriatetoassumethese
Since D is likely an immensely large set, this
unobservedtermsaresmall,orotherwiseirrelevant.
can make the signal 1{a (cid:54)= b} for a,b ∈ D over-
Weaddresseachoftheminthefollowing:
whelming compared to the signal 1{a = b}, and
1. Thetermγ capturesaveragechangeintestout-
therefore,weakenthesensitivityofthediscreteen-
put as a function of the coarsening function c.
ergydistance,overall. Coarseningfunctionsallow
Wheneverc(D˜ )isagoodrepresentativeofD˜
ustoalleviatethisproblembyeffectively“shrink- i i
(i.e., it maintains information to which h is
ing”thesetD toasmallerset. Todothis,therole (cid:96)
sensitive)γ shouldbesmall.
ofthecoarseningfunctionistoexploitadditional
2. The next term ϕ is the smallest sum of ex-
contexttoarriveatanappropriateclusteringofthe
pecteddifferencesthatanyfunctionofthecoars-
dialogues,whichassignsconceptually“near”dia-
eneddialoguesc(D˜ )andthearbitraryrandom-
loguestothesamecluster. So, thechoiceofc(d) i
ness U can achieve in mimicking the true test
shouldbea“good”representationofd,inthesense
scoresh (D,U). Sincethesetofallfunctions
thattoomuchvaluableinformationisnotlost. Asa (cid:96)
fromD×U to[0,1]shouldbeveryexpressive,
generalshorthand,foracoarseningfunctioncand
thiscanbeseenasanotherrequirementonour
variablesA,B,wewrite
coarseneddialoguesc(D˜ ). Forexample,when
i
ε (A,B)=ε (c(A),c(B)). (11) c(D˜ ) = D˜ ≈ D thistermcanbeclosetozero.
c 01 i i
Wheninstead|c(D)|ismuchsmallerthan|D|
Inthispaper,weimplementcusingtheresultsofa
(e.g.,asingletonset),weexpectϕtogrow.
k-meansclusteringwithdetailsinAppendixA.
3. The last term δ can actually be large. Fortu-
AdaptationBound Withthesedefined,wegive nately, since δ is multiplied by the energy dis-
thenovelbound. Proofofamoregeneralversionof tance,thisissueismitigatedwhenthestatistical
thisbound–applicablebeyonddialoguecontexts energyissmallenough. Ultimately,theenergy
(e.g., classification) – is provided in Appendix B is paramount in controlling the impact of this
Thm.B.1. Notably,ourproofrequiressometech- termonthebound’soverallmagnitude.
nical results on the relationship between discrete
A Predictive Theory Granted the background
energyandthecharacteristicfunctionsofdiscrete
above,ourdiscussionreducesthepredictiveaspect
probabilitydistributions. Thesemayalsobeofin-
oftheboundtoasinglekeyquantity: thediscrete
dependentinterest,outsidethescopeofthispaper. energydistanceε (D˜ ,D˜ ). Inparticular,besides
c 1 2
Theorem 4.1. For any θ ∈ Rd, any coarsening the test divergence TD , all other terms can be
S
functionc : D → D,andall(cid:96) ∈ [L] assumedreasonablysmallbyproperchoiceofthe
(cid:113) coarseningfunction,orotherwisecontrolledbythe
TD(cid:96)(θ)≤γ+ϕ+TD(cid:96)(θ)+ ε (D˜ ,D˜ )×δ (12)
T S c 1 2 statisticalenergythroughmultiplication. Note,the
where D˜ ∼ P (C) = T(θ,C), D˜ ∼ Q (C) = firstissueisdiscussedinAppendixA.Ultimately,
1 θ 2 θ
the main takeaway is that statistical energy plays
S(θ,C), (C,D) ∼ G, U ∼ U,6
theroleof∆asdiscussedinSection3.3.
γ =(cid:88) E[|h (c(D˜ ),U)−h (D˜ ,U)|]
i∈{1,2} (cid:96) i (cid:96) i 4.2 ANewCooperativeLearningAlgorithm
g∈ argmin (cid:88) E[|f(c(D˜ ),U)−h (D,U)|]
i (cid:96) Withalltheoreticaltoolsinplay,wereturntothe
f∈[0,1]D×U i
algorithmCLandtheproblemraisedinSection3.3.
where [0,1]D×U ={f |f :D×U →[0,1]}. (13)
ϕ=(cid:88) E[|g(c(D˜ ),U)−h (D,U)|] LEATHER-Motivated Modification Recall, we
i (cid:96)
i∈{1,2} are interested in quantifying and controlling the
(cid:104)(cid:88) (cid:105)
δ=E |g(x,U)−h (cid:96)(x,U)| . change in error from source TD (θ) to target
x∈c(D) S
6Forsimplicity,letD˜ 1,D˜ 2,U bepairwise-independent. TD T(θ)acrossthetrainingphases. Basedonour
denotestheoriginalalgorithmproposedbyShekhar
et al. (2019) (Section 3.3). LEATHER denotes our
LEATHER-inspiredmodification(Section4.2).
AutomatedMetrics Wereportaverageaccuracy
accoftheguessermoduleinidentifyingthetrue
goal-object across three random seeds as well as
Figure2: Energybetweentrainingphases.Energyispredic-
tiveofchangeintestdivergenceasdesired.Dottedlineisline average lexical diversity (lexdiv; type/token ra-
ofbestfit. Bluecircles(CL)indicateuseofonlygenerated tiooveralldialogues),averagequestiondiversity
dialogue in task-oriented learning phase. Orange triangles
(qdiv; % unique questions over all dialogues),
(LEATHER)indicateregularizationwithhumandata.
and average percent of dialogues with verbatim
repeated questions (repq). acc quantifies task-
theory,weknowweshoulddecreasethestatistical
success,whilesubsequentmetricsaredesignedto
energy between dialogues to reduce this change.
quantifyhuman-likenessofthegenerateddialogue.
That is, we should reduce the distance between
These metrics were all previously computed by
the generated dialogue distributions across learn-
Shekharetal.(2019)withdetailsintheircode.
ing phases. We hypothesize this may be done by
incorporatinghumandialogueinthetask-oriented HumanEvaluation Weaskedtwoannotatorsto
learningphase. TheencoderinCLseesnohuman helpusfurtherevaluatetheresults. Throughoutthe
dialogue when forming the prediction Oˆ that is process,humansubjectguidelinesfromtheauthors’
comparedtoO duringtask-orientedlearning–as institutionwerefollowedandthetaskwasapproved
seen in Eq. (1), only the generated dialogue Dˆ is byourinstitutionhumansubjectboard. Theannota-
used. Incontrast,theencoderseesonlythehuman torsexaminedcontextualizedhumandialoguesand
dialogueDinthealternatelanguagelearningphase generateddialoguesfromaCLmodelandLEATHER
–i.e.,asseeninthesurrogateobjectiveinEq.(7). model. All dialogues used the same image/goal
Wehypothesizethisstarkcontrastproduceslarge contextandannotatorsobservedalldialoguesfora
shiftsintheparametersβs → βt betweenphases. specificcontextinrandomorderwithoutknowing
Instead,weproposetoregularizethetask-oriented how each dialogue was created. Across 50+ dia-
learningphasewithhumandialogueasbelow: logues,averagepercentageofirrelevantquestions
perdialogue(irrq)wasdetermined.7 Averageper-
minE[1[Oˆ (cid:54)=O]]+E[1[Oˆ(cid:48) (cid:54)=O]] where
centageofspecificquestions(spcq)wasalsode-
α,β (14)
Oˆ(cid:48) =Gues (Enc (I,D)), ((I,O),D)∼G termined.8 WereportTD,whichgivestheaverage
α β
differenceinpercentagesfromthecorresponding
and Oˆ is still as described in Eq. (1). Intuitively, humandialogue. Sansscaling,theseTDmetrics
thisshouldconstrainparametershiftfromβs → βt, areexamplesofthetestdivergenceinEq.(3)using
therebyconstrainingthechangeinoutputsofthe ahuman-evaluationtestfunction. Qualitativeanaly-
encoder,andultimatelyconstrainingthechangein sisoferrorswasalsoconductedbasedonannotator
outputsofthequestion-generator,whichiscondi- remarks(providedlaterinthissection).
tioned on the encoder outputs. As the generated
dialoguedistributionsfromdistinctlearningphases ImpactofLEATHER InTable1,wecomparethe
willbemoresimilarbythisconstraint,wehypothe- cooperative learning algorithms CL and LEATHER.
sizethepenultimateeffectwillbedecreasedstatis- Theformerusesonlythegenerateddialogueduring
ticalenergy(i.e.,sinceenergymeasuresdistanceof task-orientedlearning,whilethelatterincorporates
distributions). Basedonourtheory,reducedenergy humandatatoregularizethechangeinparameters
providesresolutiontoourproblem: testdivergence underlyingtheenvironmentalshift. Aspredictedby
shouldbepreservedfromsourcetotarget. ourtheory,regularizationisverybeneficial,improv-
5 Experiments
7Anirrelevantquestionignorestheimageorcurrentdi-
aloguecontext. Forexample,inFigure1,CLasksaboutthe
man’s“face”(Q5)afterlearningthegoal-objectisacar,which
5.1 CooperativeLearningviaLEATHER
ignoresdialogue-context.CLalsohallucinatesanobject“cut
Setup Ingeneral,weuseexperimentalsettingsof off”ontherightside(Q4),whichignoresimagecontext.
8Aspecificquestioncontainstwoormoremodifiersofone
Shekharetal.(2019)(e.g.,hyperparameters,valida-
ormorenouns. Forexample,LEATHERmodifies“car”with
tion,etc.) withfulldetailsavailableinthecode. CL “behind”and“man”with“thewhiteshirt”inFigure1Q7.
acc↑ lexdiv↑ qdiv↑ repq↓ irrq(TD)↓ spcq(TD)↓ energy↓
CL 57.1(55.9) 9.98(10.7) 13.5(14.3) 55.9(58.2) 30.5 23.3 0.143
LEATHER 58.4(56.9) 11.4(12.7) 13.1(16.0) 53.6(47.5) 26.2 19.5 0.123
RL 56.3 7.3 1.04 96.5 - - -
Table1: ComparisonofCLandourtheory-motivatedmodificationLEATHER.Bestepochbasedonvalidationaccisreported
withlastepochinparentheses.Up/downarrowsindicateobjective.Metricsareon100pointscale,excludingenergy.Thefirst4
metricsareautomated,thenext2arefromhumanevaluation,andthelastisourproposedstatistic.LEATHERimprovesaccuracy
andhuman-likenessofdialogue.Further,ourproposedstatisticenergyispredictiveofhuman-likeness.
ingtask-successandhuman-likeness. Forexample, 5.2 LEATHERisEmpiricallyPredictive
LEATHER decreases % of irrelevant questions by
Here,weshowstatisticalenergypredictstestdiver-
4.8% compared to CL, which is more similar to
gence,empirically. Computationofenergycanbe
human dialogue according to the test divergence
automated,sopredictiveabilityisusefulformodel-
(TD). Interestingly,LEATHERalsodecreased%of
selectionwhenhumanevaluationisnotavailable.
specificquestionsby1.7%. BasedontheTD,this
Weconsidertestdivergence(TD)with4groupsof
is also more similar to human dialogue, indicat-
tests: (A)the9fine-grainedstrategyclassifiersof
inghumansaskfewerspecificquestionstoo. The
Shekharetal.(2019)usedasinEq.(5),(B)lexical
designoftheTDallowsustocapturethesenon-
diversitycomputedastype/tokenratioperdialogue,
intuitive results. Notably, regularization inspired
(C)questionrepetitioncomputedasabinaryindica-
by LEATHER allows us to train longer without de-
torforeachdialogue,and(D)thediscussedhuman-
grading task-success or suffering from mode col-
evaluationsofquestionrelevance/specificity. Fig-
lapse(i.e.,repeatedquestions). Automatedhuman-
ure2plotschangeinTDfor(A-C)asafunction
likenessmetricsforthelastepoch(inparentheses)
of energy. Specifically, change in TD is the dif-
showsubstantialimprovementsoverCLinthiscase.
ference TD (θ) − TD (θ) where S and T are
T S
definedbythetransitionfromlanguagelearningto
Cooperative vs. Reinforcement Learning In
task-orientedlearningdiscussedinSection3. We
Table1,wecomparethetwocooperativelearning
plotthischangeatthetransitionsafterepochs65,
algorithms CL and LEATHER to the reinforcement
75,85,and95(outof100total). Notably,energy
learningalgorithm(RL).Weusetheresultsreported
is predictive and, specifically, is linearly related
byShekharetal.(2019)forRL,sincewesharean
to change in test divergence. For (D), in Table 1,
experimental setup. Compared to RL, both coop-
weshowaverageenergyacrossalltransitionscom-
erativelearningapproachesimprovetasksuccess
paredtotestdivergence. Energyisalsopredictive
and human-likeness. As noted in Section 2, the
forthesehuman-evaluationtests.
theoretical framework for RL (i.e., POMDPs) is
not equipped to study interaction of the distinct 6 Conclusion
learning phases within this algorithm (i.e., with
ThisworkpresentsLEATHER,atheoreticallymoti-
respecttodata-shift). Bettertheoreticalunderstand-
vated framework for learning to generate human-
ingcouldexplainpoorperformanceandofferim-
like dialogue. The energy statistic, which is de-
provementasdemonstratedwithLEATHER,which
rived from this theory, is used to analyze and im-
improveshuman-likenessofCL.
proveanalgorithmfortask-orienteddialoguegen-
Qualitative Analysis In dialogue generated by eration. Further, energy is empirically predictive
CL,questionswithpoorrelevanceignoredtheim- ofimprovementsindialoguequality,measuredby
age context (e.g., model hallucination). In dia- bothautomatedandhumanevaluation. Futurework
loguegeneratedbytheLEATHERmodel,irrelevant mayinvolvemoreexperimentstotesttheutilityof
questionsignoredcurrentdialoguecontext(e.g.,a LEATHERinotherdialoguesettings. Theoretically,
questionwhichshouldalreadybeinferredfromex- wehopetostudysample-complexityinLEATHER,
istinganswers). Wehypothesizethismaybedueto whichisahallmarkofcommonPACtheories.
poorfaithintheautomatedanswer-playerusedfor
Acknowledgments
training,whichalsohasproblemswithmodelhal-
lucination(e.g.,Figure1). Bothmodelshadissues Wethanktheanonymousreviewersforhelpfulfeed-
withrepeatedquestions. Inhumandialogue,issues backandJenniferC.Gates,CCC-SLP,forinputon
weregrammaticalwithfewirrelevantquestions. qualitativeevaluationsofdialogueinexperiments.
References Sham Machandranath Kakade. 2003. On the sample
complexityofreinforcementlearning. Universityof
Katherine Atwell, Anthony Sicilia, Seong Jae Hwang,
London, University College London (United King-
and Malihe Alikhani. 2022. The change that mat-
dom).
ters in discourse parsing: Estimating the impact of
domain shift on parser error. In Findings of the As-
Seiichi Kuroki, Nontawat Charoenphakdee, Han Bao,
sociationforComputationalLinguistics: ACL2022,
Junya Honda, Issei Sato, and Masashi Sugiyama.
pages824–845.
2019. Unsupervised domain adaptation based on
source-guided discrepancy. In Proceedings of the
Shai Ben-David, John Blitzer, Koby Crammer, Alex
AAAI Conference on Artificial Intelligence, vol-
Kulesza, Fernando Pereira, and Jennifer Wortman
ume33,pages4122–4129.
Vaughan. 2010a. A theory of learning from differ-
entdomains. Machinelearning,79(1):151–175.
Chin-YewLin.2004. Rouge: Apackageforautomatic
ShaiBen-David, TylerLu, TeresaLuu, andDávidPál. evaluation of summaries. In Text summarization
2010b. Impossibility theorems for domain adapta- branchesout,pages74–81.
tion. InProceedingsoftheThirteenthInternational
Conference on Artificial Intelligence and Statistics, Zachary Lipton, Yu-Xiang Wang, and Alexander
pages 129–136. JMLR Workshop and Conference Smola. 2018. Detecting and correcting for label
Proceedings. shift with black box predictors. In International
conference on machine learning, pages 3122–3130.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An- PMLR.
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio.2015. Generatingsentencesfromacontinuous Yishay Mansour, Mehryar Mohri, and Afshin Ros-
space. arXivpreprintarXiv:1511.06349. tamizadeh. 2009. Domain adaptation: Learn-
ing bounds and algorithms. arXiv preprint
Elia Bruni and Raquel Fernandez. 2017. Adversarial
arXiv:0902.3430.
evaluationforopen-domaindialoguegeneration. In
Proceedingsofthe18thAnnualSIGdialMeetingon
KishorePapineni,SalimRoukos,ToddWard,andWei-
DiscourseandDialogue,pages284–288.
JingZhu.2002. Bleu: amethodforautomaticeval-
uationofmachinetranslation. InProceedingsofthe
R Cuppens. 1975. Decomposition of multivariate dis-
40th annual meeting of the Association for Compu-
tributions.
tationalLinguistics,pages311–318.
AbhishekDas, SatwikKottur, JoséMFMoura, Stefan
Lee, and Dhruv Batra. 2017. Learning cooperative María Pérez-Ortiz, Omar Rivasplata, John Shawe-
visual dialog agents with deep reinforcement learn- Taylor, and Csaba Szepesvári. 2021. Tighter risk
ing. In Proceedings of the IEEE international con- certificatesforneuralnetworks. JournalofMachine
ferenceoncomputervision,pages2951–2960. LearningResearch,22.
HarmDeVries,FlorianStrub,SarathChandar,Olivier Stephan Rabanser, Stephan Günnemann, and Zachary
Pietquin, Hugo Larochelle, and Aaron Courville. Lipton. 2019. Failing loudly: An empirical study
2017. Guesswhat?! visualobjectdiscoverythrough of methods for detecting dataset shift. Advances in
multi-modal dialogue. In Proceedings of the IEEE NeuralInformationProcessingSystems,32.
ConferenceonComputerVisionandPatternRecog-
nition,pages5503–5512. Ievgen Redko, Amaury Habrard, and Marc Sebban.
2017. Theoretical analysis of domain adaptation
PascalGermain,AmauryHabrard,FrançoisLaviolette, withoptimaltransport. InECMLPKDD,pages737–
and Emilie Morvant. 2020. Pac-bayes and domain
753.Springer.
adaptation. Neurocomputing,379:379–397.
Ievgen Redko, Emilie Morvant, Amaury Habrard,
AriHoltzman, JanBuys, LiDu, MaxwellForbes, and
Marc Sebban, and Younès Bennani. 2020. A
YejinChoi.2019. Thecuriouscaseofneuraltextde-
survey on domain adaptation theory. ArXiv,
generation. In International Conference on Learn-
abs/2004.11829.
ingRepresentations.
MertInan,PiyushSharma,BaberKhalid,RaduSoricut, Thibault Sellam, Dipanjan Das, and Ankur Parikh.
Matthew Stone, and Malihe Alikhani. 2021. Cos- 2020. BLEURT: Learning robust metrics for text
mic: A coherence-aware generation metric for im- generation. InProceedingsofthe58thAnnualMeet-
agedescriptions. arXivpreprintarXiv:2109.05281. ingoftheAssociationforComputationalLinguistics,
pages7881–7892,Online.AssociationforComputa-
Fredrik D Johansson, David Sontag, and Rajesh Ran- tionalLinguistics.
ganath. 2019. Support and invertibility in domain-
invariantrepresentations. InThe22ndInternational Shai Shalev-Shwartz and Shai Ben-David. 2014. Un-
Conference on Artificial Intelligence and Statistics, derstandingmachinelearning: Fromtheorytoalgo-
pages527–536.PMLR. rithms. Cambridgeuniversitypress.
Ravi Shekhar, Aashish Venkatesh, Tim Baumgärtner, HanZhao,RemiTachetDesCombes,KunZhang,and
Elia Bruni, Barbara Plank, Raffaella Bernardi, and Geoffrey Gordon. 2019. On learning invariant rep-
Raquel Fernández. 2019. Beyond task success: A resentationsfordomainadaptation. InInternational
closerlookatjointlylearningtosee,ask,andGuess- ConferenceonMachineLearning,pages7523–7532.
What. In Proceedings of the 2019 Conference of PMLR.
the North American Chapter of the Association for
ComputationalLinguistics: HumanLanguageTech-
nologies, Volume1(LongandShortPapers), pages
2578–2587, Minneapolis, Minnesota. Association
forComputationalLinguistics.
Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu.
2018. Wasserstein distance guided representation
learningfordomainadaptation. InAAAI.
Anthony Sicilia, Katherine Atwell, Malihe Alikhani,
andSeongJaeHwang.2022a. Pac-bayesiandomain
adaptation bounds for multiclass learners. In The
38th Conference on Uncertainty in Artificial Intelli-
gence.
AnthonySicilia,TristanMaidment,PatHealy,andMal-
iheAlikhani.2022b. Modelingnon-cooperativedia-
logue: Theoreticalandempiricalinsights. Transac-
tions of the Association for Computational Linguis-
tics,10:1084–1102.
Florian Strub, Harm De Vries, Jeremie Mary, Bilal
Piot, Aaron Courvile, and Olivier Pietquin. 2017.
End-to-endoptimizationofgoal-drivenandvisually
grounded dialogue systems. In Proceedings of the
26thInternationalJointConferenceonArtificialIn-
telligence,pages2765–2771.
GaborJSzekely.1989. Potentialandkineticenergyin
statistics. LectureNotes,BudapestInstitute.
Gábor J Székely and Maria L Rizzo. 2013. En-
ergy statistics: A class of statistics based on dis-
tances. Journalofstatisticalplanningandinference,
143(8):1249–1272.
RemiTachetdesCombes,HanZhao,Yu-XiangWang,
and Geoffrey J Gordon. 2020. Domain adaptation
with conditional distribution matching and general-
ized label shift. Advances in Neural Information
ProcessingSystems,33:19276–19289.
LeslieGValiant.1984. Atheoryofthelearnable. Com-
municationsoftheACM,27(11):1134–1142.
RamakrishnaVedantam,CLawrenceZitnick,andDevi
Parikh. 2015. Cider: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecogni-
tion,pages4566–4575.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger,andYoavArtzi.2019a. Bertscore:Eval-
uating text generation with bert. arXiv preprint
arXiv:1904.09675.
Yuchen Zhang, Tianle Liu, Mingsheng Long, and
Michael Jordan. 2019b. Bridging theory and al-
gorithm for domain adaptation. In International
ConferenceonMachineLearning,pages7404–7413.
PMLR.
A NovelAdaptationBoundandComputationofEnergyStatistic
In this section, we give our novel adaptation bound and details for the accompanying energy statistic.
ThereissomeredundancybetweenthissectionandSection4,butingeneral,thissectionismoredetailed.
Recall,sourceerrorisdenotedTD andisobservedontheenvironmentQ (c) = S(θ,c). Thetargeterror
S θ
isdenotedTD andisobservedontheenvironmentP (c) = T(θ,c). ForthealgorithmCLdiscussedin
T θ
themaintext,thetargetisinducedbythetask-orientedlearningphaseandthesourceisinducedbythe
languagelearningphase.
A.1 TheProblemwithTraditionalBounds
PredictiveAdaptationTheories Animportantqualityoftraditionaldomainadaptationbounds,pro-
posedforclassificationandregressionproblems,isthattheyofferapredictivetheory. Namely,without
observingthetargeterrorTD ,wecaninferthisquantityfrom∆andthesourceerrorTD . Theutility
T S
ofthisistwo-fold: first,itallowsustodesignalgorithmsthatpreparealearnerfordata-shiftbycontrolling
∆;second,itallowsapractitionertoselectanappropriatemodeltodeployinthepresenceofdata-shiftby
comparingthedifferentvaluesof∆foreachmodel. Ingeneral,theseuse-caseswouldnotbepossible
without∆becausethetargeterrorTD isnotobservableuntilitistoolate. Incontrast,thequantity∆
T
should beobservable. Whilethisisnotalwaystrueof∆,authorstypicallyreducethemaineffectof∆to
onekeystatistic, whichisobservable. Forexample, Atwelletal.(2022)reduce∆toonekeystatistic
calledtheh-discrepancybysuggestingtheothercomponentsmakingup∆aresmall. Thisiswhyweuse
an“approximate”inequalityinthemaintext,sinceother(small)termsmaycontributetothebound.
TraditionalTheoriesAreNotPredictive Traditionaltheoriesofadaptationarenotpredictivefordia-
loguegeneration. Namely,computationof∆anditskeycomponentsgenerallyreliesoncomputationally
efficient access to the tests {h ...h } and requires sampling from the unknown distribution U ∼ U.
1 L
While we can always observe the outputs of {h ...h } with randomness U ∼ U through the source
1 L
errorTD (θ),itisnotalwaysthecasethatwehavecomputationalefficientlyaccesstothesetestsorthe
S
randomness. Forexample,asnotedinSection3.2.1,thegroupoftests{h ...h }alongwithsamplesU
1 L
fromtheunknowndistributionUmayrepresentcomplexreal-worldprocessessuchashuman-evaluation.
Evenforsimplerevaluationmetricsbasedontext-classifiers(e.g.,like{s ...s }inEq.(5))algorithms
1 L
for computing ∆ turn out to be non-trivial, and must be handled on a case-by-case basis. Thus, in
generationcontexts,wetypicallyhavenowayofcomputing∆algorithmically,andwhenwedo,itcanbe
difficulttoimplement. Ifwerequireaneasilyimplemented,predictivetheory,thentheclassicaltheoryis
ruledout. Asasolution,weproposeanoveladaptationbound.
A.2 ANovelAdaptationBound
First,wedefinesometerms.
TheEnergyStatisticandComputation
DefinitionA.1. ForanyindependentrandomvariablesAandB,thediscreteenergydistanceisdefined:
ε (A,B)=2E[1{A(cid:54)=B}]−E[1{A(cid:54)=A(cid:48)}]−E[1{B (cid:54)=B(cid:48)}] (15)
01
whereA(cid:48) isani.i.dcopyofA,B(cid:48) isani.i.d. copyofB,and1{·}istheindicatorfunction;i.e.,itreturns1
fortrueargumentsand0otherwise.
Thediscreteenergydistanceisamodificationoftheenergydistancesometimescalledthestatistical
energy. ItwasfirstproposedbySzekely(1989)andwasstudiedextensivelybySzékelyandRizzo(2013)
inthecasewhereAandB arecontinuousvariablesadmittingaprobabilitydensityfunction. Ingeneral,
andespeciallyindialogue,thisisnotthecase. Aptly,wesuggesttheaboveformoftheenergydistance,
whichiswidelyapplicabletoanyvariablesAandB forwhichequalityisdefined. Whilegeneral,this
energydistancecanbestrictandinsensitive,especiallywhenAandB takeonmanypossiblevalues. To
remedythis,weproposethefollowingaddendum.
DefinitionA.2. LetD beanyset. Acoarseningfunctionisamapc : D → D suchthatc(D) = {c(d) |
d ∈ D}isfinite,andfurther,|c(D)| < |D|.
Figure3:ComparisonofenergystatisticsandautomatedtestfunctionsasinSection5.Here,wevarytheparameter
kinthek-meansclusteringusedtodeterminethecoarseningfunctionwhencomputingenergy. Trendsreportedin
themaintextarerobusttovariationink.
SinceDislikelyanimmenselylargeset,thiscanmakethesignal1{a (cid:54)= b}fora,b ∈ Doverwhelming
comparedtothesignal1{a = b},andtherefore,weakenthesensitivityofthediscreteenergydistance,
overall. Coarseningfunctionsallowustoalleviatethisproblembyeffectively“shrinking”thesetD to
a smaller set. To do this, the role of the coarsening function is to exploit additional context to arrive
atanappropriateclusteringofthedialogues,whichassignsconceptually“near”dialoguestothesame
cluster. So,thechoiceofc(d)shouldbea“good”representationofd,inthesensethattoomuchvaluable
informationisnotlost. Asageneralshorthand,foracoarseningfunctioncandvariablesA,B,wewrite
ε (A,B)=ε (c(A),c(B)). (16)
c 01
Example One example of a coarsening function for dialogues is k-means clustering. In fact, this is
thecoarseningfunctionweusetocomputeenergyinSection5,selectingk = 100. Real-valuedvector
representationsofdialogues(e.g.,frommodellatentspace)cancapturesemanticinformationaboutthe
dialogue(Bowmanetal.,2015),soweuselatentspacerepresentations(i.e.,theoutputoftheencoder)
to represent each dialogue and conduct a k-means clustering on these representations. For a dialogue
d the output c(d) is then defined by the cluster of d; i.e., we select an arbitrary dialogue to represent
thewholeofeachclusterandassignthisdialogueastheoutputc(d). Inpracticalimplementations,itis
typicallyeasiertojustcomputetheenergydistanceontheclusterlabelsthemselves;thisstatisticisalways
equivalenttotheenergyonthecoarseneddialogues,sincethemapbetweenclusterrepresentativesand
clusterlabelsisbijective. Later,withinLemmaB.3,weprovethisequivalenceforanybijectivemap.
Ofcourse,regardlessofimplementation,thisclusteringisdependentonthechoiceofk. Figure3shows
thattheresultsinSection5arerobusttodifferentchoicesofk. Inallcases,thereisalinearrelationship
betweentheenergyandthechangeinthetestdivergence.
AdaptationBound Withthesedefined,wegivethenovelbound. Proofofamoregeneralversionof
thisbound–applicablebeyonddialoguecontexts–isprovidedinAppendixBThm.B.1. Inparticular,
the general version is “backwards compatible” in the sense that it also applies to traditional learning
theoretic settings like classification and regression. Arguably, in these settings, it also remains more
computationallyefficientthanexistingtheories. Notably,ourproofrequiressometechnicalresultsonthe
relationshipbetweendiscreteenergyandthecharacteristicfunctionsofdiscreteprobabilitydistributions.
Thesemayalsobeofindependentinterest,outsidethescopeofthispaper.
TheoremA.1. Foranyθ ∈ Rd,anycoarseningfunctionc : D → D,andall(cid:96) ∈ [L]
(cid:113)
TD(cid:96)(θ)≤γ+ϕ+TD(cid:96)(θ)+ ε (D˜ ,D˜ )×δ (17)
T S c 1 2
whereD˜ ∼ P (C) = T(θ,C), D˜ ∼ Q (C) = S(θ,C), (C,D) ∼ G, U ∼ U,9
1 θ 2 θ
γ =E[|h (c(D˜ ),U)−h (D˜ ,U)|]+E[|h (c(D˜ ),U)−h (D˜ ,U)|]
(cid:96) 1 (cid:96) 1 (cid:96) 2 (cid:96) 2
g∈ argmin (cid:88) E[|f(c(D˜ ),U)−h (D,U)|] where [0,1]X×U ={f |f :X ×U →[0,1]}.
i (cid:96)
f∈[0,1]D×U i
(18)
ϕ=E[|g(c(D˜ ),U)−h (D,U)|]+E[|g(c(D˜ ),U)−h (D,U)|]
1 (cid:96) 2 (cid:96)
(cid:104)(cid:88) (cid:105)
δ=E |g(x,U)−h (x,U)| .
(cid:96)
x∈c(D)
UnobservedTermsinDialogue Asnoted,animportantbenefitofourtheoryisthatweneednotassume
computationallyefficientaccesstothetestfunctions{h ...h }orsamplesU ∼ U. Yet,thereaderlikely
1 L
noticesanumberoftermsinEq.(17)dependentonbothofthese. Similartothetraditionalcase,weargue
thatourtheoryisstillpredictivebecauseitistypicallyappropriatetoassumetheseunobservedtermsare
small,orotherwiseirrelevant. Weaddresseachoftheminthefollowing:
1. Thetermγ capturesaveragechangeintestoutputasafunctionofthecoarseningfunctionc. Whenever
c(D˜ )isagoodrepresentativeofD˜ (i.e.,itmaintainsinformationtowhichh issensitive)γ should
i i (cid:96)
besmall. Sincewechoosethecoarseningfunction,theformerpremiseisnotastrongrequirement. In
practice,ifchoiceofcisunclear,werecommendstudyingmanychoicesasinFigure3.
2. Thenexttermϕisthesmallestsumofexpecteddifferencesthatanyfunctionofthecoarseneddialogues
c(D˜ ) and the arbitrary randomness U can achieve in mimicking the true test scores h (D,U). In
i (cid:96)
general,thesetofallfunctionsfromD×U to[0,1]shouldbeveryexpressive;e.g.,itcontainsh itself
(cid:96)
andanyotherfunctionwhichmightmimich (D,U)betterwhenappliedtoc(D˜ )andU. So,itisnot
(cid:96) i
unreasonabletoexpectsomegoodminimizertoexist,andtherefore,ϕtobesmall. Usingthislogic,
oneadditionalconstraintisthatc(D˜ )hasappropriatevariance. Forinstance,ifc(D˜ )isconstantand
i i
D is not, ϕ can easily be large. Instead, when c(D˜ ) does have variance, the expressiveness of the
i
functionclass[0,1]D×U canbewellexploited. Forreasonabledialoguelearnersandawell-chosenc,
thevarianceofc(D˜ )isanon-issue.
i
3. Thelasttermδ mayactuallybelarge,butwearguethisisalsoanon-issueforinterpretationpurposes.
Ingeneral,becauseδ isanunnormalized sum,itsmagnitudegrowswiththesizeofc(D),evenifthe
individual summands may be small. Fortunately, since δ is multiplied by the energy distance, this
issueismitigatedwhenthestatisticalenergyissmallenough. Ultimately,theenergyisparamountin
controllingtheimpactofthistermonthebound’soverallmagnitude.
APredictiveTheory Grantedthebackgroundabove,ourdiscussionreducesthepredictiveaspectof
theboundtoasinglekeyquantity: thediscreteenergydistanceε (D˜ ,D˜ ). Inparticular,besidesthetest
c 1 2
divergenceTD (knownpriortotheenvironmentalchange),allothertermscanbeassumedreasonably
S
small,orotherwisecontrolledbythestatisticalenergythroughmultiplication. Therefore,ifthestatistical
energy between environments is small, it can be reasonable to assume the dialogue quality has been
maintainedorimproved. Otherwise,itispossiblethequalityofthegenerateddialoguehassubstantially
degraded. Inthisway,thestatisticalenergyisaneasilyobservablequantitythatassistsusindetermining
ifthesourceerrorTD knownbeforetheenvironmentalchangeisagoodrepresentativeoftheunknown
S
targeterrorTD ,whichisobservedaftertheenvironmentalchange.
T
UseCases Ingeneral,controllingthestatisticalenergybetweendialoguesensureswepreservedialogue
qualitywhentheevaluationmetricswecareaboutarenotavailable. Asdemonstratedinthemaintext,this
makesitusefulinalgorithmdesign;i.e.,toinformdecisionsinmodeltraining. Energycanalsobeuseful
formodelselection. Namely,thegenerationmodelwhosedialogueshavethesmallestenergycomparedto
goaldialogueshouldproducethehighestqualitydialogue. Toseethis,simplysetD˜ = D inthebound.
2
Similarlogicalreductionshowstheenergyisthedominatingterminthiscaseaswell.
9Forsimplicity,letD˜ ,D˜ ,U bepairwise-independent.Whenindependencedoesnothold,similarresultscanbederived
1 2
underassumptionofcontext-conditionalindependence.
B Proofs
Inthissectionweprovetheclaimedtheoreticalresults. Sothattheresultsmaybemorebroadlyapplicable,
weprovetheminamoregeneralcontextandthenspecifytothecontextofdialoguegeneration(inthe
maintextandAppendixA).
B.1 AnAdaptationBoundBasedonaDiscreteEnergyStatistic
Inthissection,weproposeanadaptationboundbasedontheenergystatistic. Asweareaware,oursarethe
firsttheoreticalresultsrelatingthestatisticalenergybetweendistributionstothechangeinfunctionoutputs
acrosssaiddistributions. Giventheuseofthediscreteenergydistance(Def.A.1)andtheaccompanying
coarsening function (Def. A.2), we appropriately choose to prove our theoretical results for discrete
randomvariables(i.e.,thosewhichtakeononlyacountablenumberofvaluesandexhibitaprobability
massfunction). Theeffectofthischoiceisthatwealsocontributeanumberofnewtheoreticalresults
relating the probability mass function of a real-valued, discrete random variable to its characteristic
function (i.e., in similar style to the Parseval-Plancherel Theorem). Furthermore, we expand on the
relationshipbetweenthestatisticalenergyofdistributionsandtheircharacteristicfunctions. Whilethishas
beenwellstudiedinthecontinuoussetting(SzékelyandRizzo,2013)wherethedistributionsofrandom
variablesadmitprobabilitydensities(i.e.,absolutelycontinuouswithrespecttotheLesbesguemeasure),
ithasnotbeenstudiedinthecaseofdiscreterandomvariables. Westartourresultsusingonlyreal-valued
discretevariables,butproveourmainresultsforalldiscreterandomvariablesusingLemmaB.3
B.1.1 Setup
Suppose A and B are discrete random variables taking on values in Rd for some d. Respectively, the
distributionofAisαandthedistributionofB isβ. ThespaceΩ ⊂ Rd isthecountablesubsetofRd for
whichαorβ assignsnon-zeroprobability;i.e.,Ω = supp(α)∪supp(β). Then,theexpectationofany
functionf : Rd → RofAisdefined:
(cid:90)
(cid:88)
E[f(A)] = fdα = f(a)p (a) (19)
α
Rd
a∈Ω
where p is the probability mass function for A (i.e., α). Expectations of functions ofB are similarly
α
defined.
ThecharacteristicfunctionofAisdefinedasthecomplex-conjugateoftheFourier-Stieltjestransform
oftheprobabilitymassfunctionp . Moreexplicitly,itisthefunctionpˆ : Rd → Rdefined
α α
(cid:88)
pˆ (τ) = E[exp{iτTA}] = p (a)exp{iτTa} (20)
α α
a∈Ω
whereiistheimaginaryunit(i.e.,i2 = −1)andτTaisthe(inner)productbetweencolumnvectorsτ and
a. Note,thecharacteristicfunctionalwaysexistsandisfiniteforeachτ.
B.1.2 Parseval-PlancherelTheorem(Reprise)
Onenotableuseforthecharacteristicfunctionisthefollowinginversionformula. Inthediscretecontext
weconsider,Cuppens(1975)provesthefollowing
(cid:32) d (cid:33) (cid:90)
(cid:89)
p (a) = lim lim ... lim 1/(2τ ) pˆ (t)exp{−itTa}λ(dt) (21)
α i α
τ1→∞τ2→∞ τ d→∞
B(τ)
i=1
where τ = (τ ,τ ,...,τ )T, B(τ) = {x ∈ Rd | −τ ≤ x ≤ τ }, and λ is the Lebesgue measure.
1 2 d i i i
This inversion formula highlights the connection between the characteristic function and the general
FouriertransformasalludedtojustbeforeEq.(20), sinceFouriertransformsarewellknownfortheir
owninversionformulas. AnothercommonlyusedresultinFourierAnalysis(relatedtoinversion)isthe
Parseval-PlancherelTheorem. Weproveavariationonthisresultbelow. Asweareaware,itisthefirst
whichusesthetransformgiveninEq.(20)(i.e.,specifictodiscrete,real-valuedrandomvariables).
LemmaB.1. ForanydiscreterandomvariablesAandB asdescribed,takingvaluesinRd,
(cid:32) d (cid:33) (cid:90)
(cid:88) (cid:89)
|p (x)−p (x)|2 = lim lim ... lim 1/(2τ ) |pˆ (t)−pˆ (t)|2λ(dt). (22)
α β i α β
τ1→∞τ2→∞ τ d→∞
B(τ)
x∈Ω i=1
Proof. Foranyfunctionf : Rd → R+ suchthat(cid:80) f(x) < ∞forallt ∈ Rd,weprovethefollowing
x∈Ω
moregeneralresult
(cid:32) d (cid:33) (cid:90)
(cid:88) (cid:89)
f2(x) = lim lim ... lim 1/(2τ ) fˆ(x)fˆ∗(x)λ(dt) (23)
i
τ1→∞τ2→∞ τ d→∞
B(τ)
x∈Ω i=1
whereasbeforea“hat”denotestheFourier-StieltjestransformgiveninEq.(20)andthenewnotation
fˆ∗ denotesthecomplex-conjugateoffˆ . Observe,thisprovesthedesiredresultsbecausesettingf(x) =
p (x)−q (x)wehave
α α
f2(x) = (p (x)−q (x))2 = |p (x)−q (x)|2 (24)
α α α α
and
fˆ(x)fˆ∗(x) = (p (x(cid:92) )−p (x))(p (x(cid:92) )−p (x))∗
α α α α
(25)
= (pˆ (x)−pˆ (x))(pˆ (x)−pˆ (x))∗ = |pˆ (x)−pˆ (x)|2.
α α α α α α
ProceedingwiththeproofofEq.(23)wehave
(cid:32) d (cid:33) (cid:90)
(cid:89)
lim lim ... lim 1/(2τ ) fˆ(x)fˆ∗(x)λ(dt)
i
τ1→∞τ2→∞ τ d→∞
B(τ)
i=1
(cid:32) d (cid:33) (cid:90) (cid:32) (cid:33)(cid:32) (cid:33)
(cid:89) (cid:88) (cid:88)
= lim 1/(2τ ) f(x)exp{itTx} f(x)exp{−itTx} λ(dt)
i
τi→∞
B(τ)
i=1 x∈Ω x∈Ω
(cid:32) (cid:33)
(cid:90)
(cid:89) (cid:88) (cid:88)
= lim 1/(2τ ) f(x)f(x(cid:48))exp{i(tTx−tTx(cid:48))}λ(dt) (Fubini-Tonelli)
i
τi→∞
i B(τ) x∈Ωx(cid:48)∈Ω
(cid:32) d (cid:33) (cid:90)
(cid:89) (cid:88) (cid:88)
= lim 1/(2τ ) f(x)f(x(cid:48)) exp{i(tTx−tTx(cid:48))}λ(dt) (Fubini-Tonelli)
i
τi→∞
i=1 x∈Ωx(cid:48)∈Ω B(τ)
(cid:32) d (cid:33)(cid:34) (cid:90) (cid:35)
(cid:88) (cid:88) (cid:89)
= lim f(x)f(x(cid:48)) 1/(2τ ) exp{i(tTx−tTx(cid:48))}λ(dt)
i
τi→∞
x∈Ωx(cid:48)∈Ω i=1 B(τ)
(cid:88) (cid:88)
(cid:32) (cid:89)d (cid:34) (cid:90) τi (cid:35)(cid:33)
= lim f(x)f(x(cid:48)) 1/(2τ ) exp{i(t (x −x(cid:48))}dt (Fubini-Tonelli)
i i i i i
τi→∞
x∈Ωx(cid:48)∈Ω i=1 −τi
= lim (cid:88) (cid:88) f(x)f(x(cid:48))(cid:32) (cid:89)d χ(x i,x(cid:48) i,τ i)(cid:33) where χ = (cid:40)sin τiτ (i x( ix −i− x(cid:48) ix )(cid:48) i) ifx i (cid:54)= x(cid:48) i,
τi→∞ 1 else
x∈Ωx(cid:48)∈Ω i=1
(cid:32) d (cid:33)
(cid:88) (cid:88) (cid:89)
= f(x)f(x(cid:48)) lim χ(x ,x(cid:48),τ ) (DCT)
i i i
τi→∞
x∈Ωx(cid:48)∈Ω i=1
(cid:40)
(cid:88) (cid:88) 1 ifargholds,
= f(x)f(x(cid:48))1[x = x(cid:48)] where 1[arg] =
0 else
x∈Ωx(cid:48)∈Ω
(cid:88)
= f2(x).
x∈Ω
(26)
Indetails: thefirstequalityfollowsbydefinition;thesecondandthirdbyFubini-TonelliTheorem;10 the
fourthbysimplerulesofarithmetic;thefifthagainbyFubini-TonelliTheoremtodecomposethevolume
calculationintoaproduct; thesixthbyevaluatingtheintegral; seventhbythedominatedconvergence
theorem;11 theeighthbyevaluatingthelimit;andthelastbysimplearithmetic.
B.1.3 TheEnergyofDiscreteDistributionsasDescribedbytheirCharacteristicFunctions
LemmaB.2. Foranyindependent,discreterandomvariablesAandB asdescribed,takingvaluesinRd,
(cid:32) d (cid:33) (cid:90)
(cid:89)
ε (A,B) = lim lim ... lim 1/(2τ ) |pˆ (t)−pˆ (t)|2λ(dt). (27)
01 i α β
τ1→∞τ2→∞ τ d→∞
B(τ)
i=1
Proof. AccordingtoSzékelyandRizzo(2013),forindependentAandB,wehave
|pˆ (t)−pˆ (t)|2 = E[cos{tT(A−A(cid:48))}+cos{tT(B−B(cid:48))}−cos{tT(A−B)}]
α β
(28)
= E{2[1−cos{tT(A−B)}]−[1−cos{tT(A−A(cid:48))}]−[1−cos{tT(B−B(cid:48))}]}
where A(cid:48) and B(cid:48) are i.i.d. copies of A and B, respectively. With the equivalence above, by Fubini’s
Theorem,wemayinterchangetheexpectationandintegralinEq.(27). Wemayalsochangetheorderof
integrationtoarriveat
(cid:32) d (cid:33) (cid:90)
(cid:89)
lim lim ... lim 1/(2τ ) |pˆ (t)−pˆ (t)|2λ(dt)
i α β
τ1→∞τ2→∞ τ d→∞
B(τ)
i=1
(cid:34)(cid:32) (cid:89)d 1 (cid:33) (cid:90) τ1 (cid:90) τ d (cid:110) (cid:16) (cid:88)d (cid:17)
= lim E ... 2 1−cos τ (A −B ) (29)
i i i
τi→∞
i=1
(2τ i)
−τ1 −τ
d i=1
d d (cid:35)
(cid:16) (cid:88) (cid:17) (cid:16) (cid:88) (cid:17)(cid:111)
− 1−cos τ (A −A(cid:48)) − 1−cos τ (B −B(cid:48)) dτ ...dτ .
i i i i i i d 1
i=1 i=1
Toevaluatetheintegralwefirstobserve,foranyx ∈ Rd,
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:90) τ d (cid:88)d sin τ dx d+(cid:80)d i=− 11τ ix i −sin −τ dx d+(cid:80)d i=− 11τ ix i
1−cos τ x dτ = 2τ −
i i d d
x
−τ d
d i=1 (30)
(cid:16) (cid:17)
2cos
(cid:80)d−1τ
x sin(τ x )
i=1 i i d d
= 2τ − .
d
x
d
Notice,theaboveequationimpliesaniterativepatternwhichcanbeusedtosolvethemultipleintegral.
10TheprimaryassumptionofFubini-TonelliTheoremrequirestheabsolutevalueoftheintegrandhavefinitedoubleoriterated
(cid:80)
integral/sum.Inthefirstcase,withtheiteratedsum,itisclearforeachfixedtsince f(x)isboundedandsoisexp{−iz}
x
forallz.Inthesecondandthirdcases,wesimplycitetheboundednessofB(τ)foreachfixedτ.
11TheprimaryassumptionoftheDCTisthatthesequenceoffunctionsbeingintegrated(orsummedinourcase)isdominated
bysomefunctiongwithfiniteintegral(i.e.,inthesensethattheabsolutevalueofeveryfunctioninthesequenceislessthanor
equaltogonallinputs).Again,thisiseasytoseeusingpropertiesassumedonf andthefactthat|χ|≤1forallinputs.
Keepinginmindwhichtermsareconstantswithrespecttothedifferential,wehave
(cid:90) τ1 (cid:90) τ d−1 (cid:16)(cid:90) τ d (cid:88)d (cid:17)
... 1−cos τ x dτ dτ ...dτ
i i d d−1 1
−τ1 −τ
d−1
−τ
d i=1
(cid:16) (cid:17)
(cid:90) τ1 (cid:90) τ d−2 (cid:32) (cid:90) τ d−1 2cos (cid:80)d i=− 11τ ix i sin(τ dx d) (cid:33)
= ... 2τ − dτ dτ ...dτ
d d−1 d−2 1
x
−τ1 −τ
d−2
−τ
d−1
d
(cid:16) (cid:17)
(cid:90) τ1 (cid:90) τ d−2 (cid:32) 4cos (cid:80)d i=− 12τ ix i sin(τ dx d)sin(τ d−1x d−1)(cid:33)
= ... (2τ )(2τ )− dτ ...dτ
d d−1 d−2 1
x x
−τ1 −τ
d−2
d d−1
= ...
(cid:16) (cid:17)
(cid:90) τ1 (cid:90) τ d−j (cid:32) (cid:89)j cos (cid:80)d i=− 1jτ ix i (cid:81)j i=12sin(τ d−i+1x d−i+1)(cid:33)
= ... (2τ )− dτ ...dτ
−τ1 −τ d−j i=1
d−i+1 (cid:81)j
i=1x d−i+1
d−j 1
...
=
(cid:89)d
(2τ )−
(cid:81)d
i=12sin(τ d−i+1x d−i+1)
d−i+1 (cid:81)d
x
i=1 i=1 d−i+1
=
(cid:89)d
(2τ )−
(cid:81)d
i=12sin(τ ix i)
.
i (cid:81)d
x
i=1 i=1 i
(31)
Now,returningtotheRHSofEq.(29),linearityoftheintegralimplies
(cid:32) (cid:89)d 1 (cid:33) (cid:90) τ1 (cid:90) τ d (cid:110) (cid:16) (cid:88)d (cid:17)
... 2 1−cos τ (A −B )
i i i
(2τ )
i=1
i −τ1 −τ
d i=1
d d
(cid:16) (cid:88) (cid:17) (cid:16) (cid:88) (cid:17)(cid:111)
− 1−cos τ (A −A(cid:48)) − 1−cos τ (B −B(cid:48)) dτ ...dτ
i i i i i i d 1
i=1 i=1
(cid:32) (cid:89)d 1 (cid:33) (cid:90) τ1 (cid:90) τ d (cid:110) (cid:16) (cid:88)d (cid:17)
= ... 2 1−cos τ (A −B ) }dτ ...dτ (32)
i i i d 1
(2τ )
i=1
i −τ1 −τ
d i=1
(cid:32) (cid:89)d 1 (cid:33) (cid:90) τ1 (cid:90) τ d (cid:110)(cid:16) (cid:88)d (cid:17)
− ... 1−cos τ (A −A(cid:48)) }dτ ...dτ
(2τ ) i i i d 1
i=1
i −τ1 −τ
d i=1
(cid:32) (cid:89)d 1 (cid:33) (cid:90) τ1 (cid:90) τ d (cid:110)(cid:16) (cid:88)d (cid:17)(cid:111)
− ... 1−cos τ (B −B(cid:48)) dτ ...dτ .
(2τ ) i i i d 1
i=1
i −τ1 −τ
d i=1
Thus,wecanapplythesolutioninEq.(31)tosolvetheintegralinEq.(29). Takingx = (A −B )in
i i i
Eq.(31),weconsiderthefirstintegralofEq.(32)abovealongwithitsmultiplicativeconstant:
(cid:32) (cid:89)d 1 (cid:33) (cid:90) τ1 (cid:90) τ d (cid:88)d (cid:17)
... (1−cos τ (A −B )
i i i
(2τ )
i=1
i −τ1 −τ
d i=1
(cid:110) (cid:111)
(cid:32) d (cid:33)(cid:32) d (cid:81)d 2sin τ (A −B ) (cid:33)
(cid:89) 1 (cid:89) i=1 i i i
= (2τ )− (33)
i=1
(2τ i)
i=1
i (cid:81)d i=1(A i−B i)
(cid:110) (cid:111)
d sin τ (A −B ) d
(cid:89) i i i (cid:89)
= 1− = 1− χ(A ,B ,τ )
i i i
τ (A −B )
i i i
i=1 i=1
whereχisdefinedintheproofofEq.(23)(LemmaB.1). Takingx = (A −A(cid:48))andx = (B −B(cid:48))
i i i i i i
andproceedingasaboveallowsustoresolvetheentireintegral. Inparticular,wehave
(cid:32) d (cid:33) (cid:90)
(cid:89)
lim lim ... lim 1/(2τ ) |pˆ (t)−pˆ (t)|2λ(dt)
i α β
τ1→∞τ2→∞ τ d→∞
B(τ)
i=1
(cid:34) d d d (cid:35)
(cid:16) (cid:89) (cid:17) (cid:16) (cid:89) (cid:17) (cid:16) (cid:89) (cid:17)
= limE 2 1− χ(A ,B ,τ ) − 1− χ(A ,A(cid:48),τ ) − 1− χ(B ,B(cid:48),τ
i i i i i i i i i
τi (34)
i=1 i=1 i=1
(cid:34) (cid:40) d d d (cid:41)(cid:35)
(cid:16) (cid:89) (cid:17) (cid:16) (cid:89) (cid:17) (cid:16) (cid:89) (cid:17)
= E lim 2 1− χ(A ,B ,τ ) − 1− χ(A ,A(cid:48),τ ) − 1− χ(B ,B(cid:48),τ
i i i i i i i i i
τi
i=1 i=1 i=1
= E(cid:2) 2×1[A (cid:54)= B ]−1[A (cid:54)= A(cid:48)]−1[B (cid:54)= B(cid:48)](cid:3) .
i i i i i i
Here,thesecondequalityfollowsfromthedominatedconvergencetheoremand1[arg]isdefinedasin
proofofEq.(23)(LemmaB.1).
B.1.4 MovingfromReal-ValuedDiscreteVariablestoAnyDiscreteVariables
LemmaB.3. LetA˜andB˜ beanyindependent,discreterandomvariablesoveracountablesetΩ(i.e.,
notnecessarilycontainedinRd). Then,
(cid:88)
|p˜ (x)−p˜ (x)| = ε (A˜,B˜). (35)
α β 01
x∈Ω
wherep˜ andp˜ arethemassfunctionsofA˜andB˜,respectively.
α β
Proof. Let Π ⊂ Rd with |Π| = |Ω|. Note, Π exists because Ω is countable and Rd is not. Next, let
f : Ω → Πbeanybijectivemap.
Then,supposingp andp arethemassfunctionsoff(A˜)andf(B˜)respectively,bydefinitionofthe
α β
pushforwardmeasure,foranyy ∈ Πsuchthaty = f(x)forx ∈ Ω
p (y) = p˜ ({a ∈ Ω | f(a) = y}) = p˜ (x). (36)
α α α
Notice, bijectivity of f ensures the last step, because each y ∈ Π has a unique inverse x ∈ Ω. From
bijectivityoff,wealsohaveinjectivity,whichimplies1[a (cid:54)= b] = 1[f(a) (cid:54)= f(b)]foralla,b ∈ Ω. By
simplesubstitution,theprevioustwofactstellsus
(cid:88) (cid:88) (cid:88)
2 1[a (cid:54)= b]p˜ (a)p˜ (b)− 1[a (cid:54)= a(cid:48)]p˜ (a)p˜ (a(cid:48))− 1[b (cid:54)= b(cid:48)]p˜ (b)p˜ (b(cid:48))
α β α α β β
a,b∈Ω a,a(cid:48)∈Ω b,b(cid:48)∈Ω
(cid:88) (cid:88)
= 2 1[f(a) (cid:54)= f(b)]p (f(a))p (f(b))− 1[f(a) (cid:54)= f(a)(cid:48)]p (f(a))p (f(a(cid:48)))
α β α α (37)
a,b∈Ω a,a(cid:48)∈Ω
(cid:88)
− 1[f(b) (cid:54)= f(b(cid:48))]p (f(b))p (f(b(cid:48)))
β β
b,b(cid:48)∈Ω
Sincef issurjectivetoo(i.e.,alongwithinjective),summationofanyfunctiong(f(a),f(b))overa,b ∈ Ω
andsummationofg(c,d)overc,d ∈ Πareequivalent.12 So,wecancontinueasfollows:
(cid:88) (cid:88)
2 1[f(a) (cid:54)= f(b)]p (f(a))p (f(b))− 1[f(a) (cid:54)= f(a)(cid:48)]p (f(a))p (f(a(cid:48)))
α β α α
a,b∈Ω a,a(cid:48)∈Ω
(cid:88)
− 1[f(b) (cid:54)= f(b(cid:48))]p (f(b))p (f(b(cid:48)))
β β (38)
b,b(cid:48)∈Ω
(cid:88) (cid:88) (cid:88)
= 2 1[c (cid:54)= d]p (c)p (d)− 1[c (cid:54)= c(cid:48)]p (c)p (c(cid:48))− 1[d (cid:54)= d(cid:48)]p (d)p (d(cid:48))
α β α α β β
c,d∈Π c,c(cid:48)∈Ω d,d(cid:48)∈Ω
12Inparticular,becausef issurjective,weknowallpairs(c,d)∈Π2havesomepair(a,b)∈Ω2forwhich(f(a),f(b))=
(c,d); i.e.,wedonot“miss”aterminthissum. Becausef isinjective,weknowallpairs(c,d) ∈ Π2 haveonlyonepair
(a,b)∈Ω2forwhich(f(a),f(b))=(c,d);i.e.,wedonot“repeat”aterminthissum.
Inotherwords,theprevioustwoequationstellusε (A˜,B˜) = ε (f(A˜),f(B˜)). Applyingequivalence
01 01
ofthemassfunctions,thenLemmasB.1andB.2,thenequivalenceoftheenergies:
(cid:88) (cid:88)
|p˜ (x)−p˜ (x)| = |p (y)−p (y)| = ε (f(A˜),f(B˜)) = ε (A˜,B˜). (39)
α β α β 01 01
x∈Ω y∈Π
Note,thisusesthefactthatfunctionsofindependentrandomvariablesarealsoindependent.
B.1.5 TheMainBound
Theorem B.1. Let A and B be any independent random variables over any space X and let S, S(cid:48) be
randomvariablesover[0,1]. LetU bearandomvariable,independentfromAandB,overanysetU.
Supposec : X → Ωisacoarseningfunction(so,Ω ⊂ X)andletf ∈ [0,1]X×U. Then,
(cid:112)
E[|S −f(A,U)|] ≤ γ +ϕ+E[|S(cid:48)−f(B,U)|]+ ε (A,B)×δ (40)
c
where
γ = E[|f(c(B),U)−f(B)|]+E[|f(c(A),U)−f(A)|],
g ∈ argmin E[|S −h(c(A),U)|]+E[|h(c(B),U)−S(cid:48)|],
h∈[0,1]X×U
(41)
ϕ = E[|S −g(c(A),U)|]+E[|g(c(B),U)−S(cid:48)|],
(cid:88)
δ = |g(x)−f(x)|2
x∈Ω
Proof. Foranyg ∈ [0,1]X×U,bywayofthetriangleinequalityandmonotonicityoftheexpectation,
E[|S −f(A,U)|] = E[|S −f(A,U)|]+E[|S(cid:48)−f(B,U)|]−E[|S(cid:48)−f(B,U)|]
= E[|S −g(c(A),U)+g(c(A),U)−f(A,U)|]+E[|S(cid:48)−f(B,U)|]−E[|S(cid:48)−f(B,U)|]
≤ E[|S −g(c(A),U)|]+E[|g(c(A),U)−f(A,U)|]+E[|S(cid:48)−f(B,U)|]
−E[|S(cid:48)−f(B,U)|]
≤ E[|S −g(c(A),U)|]+E[|g(c(A),U)−f(A,U)|]+E[|S(cid:48)−f(B,U)|]
−E[|g(c(B),U)−f(B,U)|]+E[|g(c(B),U)−S(cid:48)|] (42)
≤ E[|S −g(c(A),U)|]+E[|g(c(A),U)−f(c(A),U)|]+E[|f(c(A),U)−f(A,U)|]
+E[|S(cid:48)−f(B,U)|]−E[|g(c(B),U)−f(B,U)|]+E[|g(c(B),U)−S(cid:48)|]
≤ E[|S −g(c(A),U)|]+E[|g(c(A),U)−f(c(A),U)|]+E[|f(c(A),U)−f(A,U)|]
+E[|S(cid:48)−f(B,U)|]−E[|g(c(B,U)−f(c(B),U)|]
+E[|f(c(B),U)−f(B,U)|]+E[|g(c(B),U)−S(cid:48)|].
SetB˜ = c(B),A˜= c(A)andset
γ = E[|f(B˜,U)−f(B,U)|]+E[|f(A˜,U)−f(A,U)|],
g ∈ argmin E[|S −h(A˜,U)|]+E[|h(B˜,U)−S(cid:48)|],
(43)
h∈[0,1]X×U
ϕ = E[|S −g(A˜,U)|]+E[|g(B˜,U)−S(cid:48)|].
Then,Eq.(42)implies
E[|S−f(A,U)|] ≤ γ+ϕ+E[|S(cid:48)−f(B,U)|]+E[|g(A˜,U)−f(A˜,U)|]−E[|g(B˜,U)−f(B˜,U)|]. (44)
Now, suppose p˜ and p˜ are probability mass functions for A˜ and B˜, respectively. Then, using basic
α β
propertiesoftheexpectationalongwithothernotedfacts,
E[|g(A˜,U)−f(A˜,U)|]−E[|g(B˜,U)−f(B˜,U)|]
(cid:104)(cid:88) (cid:88) (cid:105)
= E |g(a,U)−f(a,U)|p˜ (a)− |g(b,U)−f(b,U)|p˜ (b) (Fubini)
α β
a∈Ω b∈Ω
(cid:104)(cid:88) (cid:105) (cid:104)(cid:88) (cid:105)
= E |g(x,U)−f(x,U)|(p˜ (x)−p˜ (x)) ≤ E |g(x,U)−f(x,U)||p˜ (x)−p˜ (x)|
α β α β
x∈Ω x∈Ω
(45)
(cid:34)(cid:32) (cid:33)1/2(cid:32) (cid:33)1/2 (cid:35)
(cid:88) (cid:88)
≤ E |g(x,U)−f(x,U)|2 |p˜ (x)−p˜ (x)|2 (Cauchy-Schwarz)
α β
x∈Ω x∈Ω
(cid:34)(cid:32) (cid:33)1/2 (cid:35)
(cid:113)
(cid:88)
≤ ε (A˜,B˜)×E |g(x,U)−f(x,U)|2 (LemmaB.3)
01
x∈Ω
Inthelaststep,wemayapplyLemmaB.3becauseA˜andB˜ arestillindependent(i.e.,theyarefunctions
ofindependentrandomvariables)andarenowdiscretetoo. Definingδ appropriatelyyieldstheresult.
B.1.6 ProofofThm.A.1andOtherApplicationsofThm.B.1
Thm.A.1 Thm.A.1issimplyaspecificationofThm.B.1above. Infact,itisbetterstatedasacorollary
ofThm.B.1. WesetX = D,leaveU anditsvariableU unchanged,andsetS = S(cid:48) = h (D,U). Then,
(cid:96)
A = D˜ andB = D˜ . Takingf = h yieldstheresult.
1 2 (cid:96)
Classification and Regression In adaptation for classification and regression, we consider a source
distributionSgoverningrandomvariables(X ,Y )andatargetdistributionTgoverningrandomvariables
S S
(X T,Y T). In general, the goal is to predict Y(cid:3) from X(cid:3). We can set S = Y
T
and S(cid:48) = Y S. We may
alsosetA = X andB = X . Then,welearnf fromapre-specifiedhypothesisclassH ⊆ [0,1]X×U.
T S
Typically,U isignoredinthesesettings,butitseemspossibletoemploythistermtomodelstochastic
(Gibbs)predictors;i.e.,inPAC-BayesianFrameworks(Germainetal.,2020;Siciliaetal.,2022a). Notice,
forregression,ourframeworkonlyconsidersanormalizedresponsevariableandthemeanabsoluteerror.
B.1.7 SampleComplexity
As alluded in Section 6, a key shortcoming of our framework compared to existing frameworks is the
absenceofanytermsmeasuringsample-complexity. Thatis,wedonotexplicitlyquantifythedifference
betweenourempiricalobservationoftheenergyandthetrueenergy(i.e.,thepopulationversionofthe
statistic)usingthenumberofsamplesinourobservation. Thisisabigpartofcomputationallearning
theory,astheactofchoosingafunctionf usingdata–or,indialoguecontexts,choosingtheparameter
θ using data – can have significant impact on the difference between our observations of a statistical
processesandreality. Infact,thisimpactisthebasisofoverfittingand,besidescomputationalefficiency,
isthemainpillarofstudyintraditionalPAClearning13 (Valiant,1984;Shalev-ShwartzandBen-David,
2014). In more recent studies of domain adaptation, like our work, the population-only bound can be
justasimportantforpurposeofunderstandingandinterpretation. Furthermore,ifweonlycareaboutthe
empiricalsamplesin-hand,thesepopulation-onlyboundsaredirectlyapplicable,14 whichpartlyexplains
theempiricaleffectivenessofourtheoryinSection5. Nonetheless,theroleofsample-complexitycan
beveryinformativeandusefulinpractice(Pérez-Ortizetal.,2021)andwouldbeimportantformodel-
selectionapplicationsasdescribedattheendofAppendixA.Weleaveinvestigationofsample-complexity
asfuturework. Asweareaware,thereiscurrentlynoappropriatedescriptionofsample-complexityfor
dialoguegenerationcontexts.
13ProbablyApproximatelyCorrectlearning
14Theempiricalsamplebecomesthewholepopulationaboutwhichweareconcerned.
C StatisticsonDataset
uniqueimages uniqueobjects words(+1occurrences) words(+3occurrences) questions
67K 134K 19K 6.6K 277K
Table2: StatisticsonGuessWhat?!.Formoreinformation(e.g.,train/testsplits)seeoriginalproposal(DeVriesetal.,2017).
Figure4: VisualizationofobjectcountsanddialoguelengthinGuessWhat?!dataset.
References
Katherine Atwell, Anthony Sicilia, Seong Jae Hwang, and Malihe Alikhani. 2022. The change that matters in
discourse parsing: Estimating the impact of domain shift on parser error. In Findings of the Association for
ComputationalLinguistics: ACL2022,pages824–845.
ShaiBen-David, JohnBlitzer, KobyCrammer, AlexKulesza, FernandoPereira, andJenniferWortmanVaughan.
2010a. Atheoryoflearningfromdifferentdomains. Machinelearning,79(1):151–175.
ShaiBen-David,TylerLu,TeresaLuu,andDávidPál.2010b. Impossibilitytheoremsfordomainadaptation. In
ProceedingsoftheThirteenthInternationalConferenceonArtificialIntelligenceandStatistics,pages129–136.
JMLRWorkshopandConferenceProceedings.
SamuelRBowman,LukeVilnis,OriolVinyals,AndrewMDai,RafalJozefowicz,andSamyBengio.2015. Gen-
eratingsentencesfromacontinuousspace. arXivpreprintarXiv:1511.06349.
EliaBruniandRaquelFernandez.2017. Adversarialevaluationforopen-domaindialoguegeneration. InProceed-
ingsofthe18thAnnualSIGdialMeetingonDiscourseandDialogue,pages284–288.
RCuppens.1975. Decompositionofmultivariatedistributions.
AbhishekDas,SatwikKottur,JoséMFMoura,StefanLee,andDhruvBatra.2017. Learningcooperativevisualdi-
alogagentswithdeepreinforcementlearning. InProceedingsoftheIEEEinternationalconferenceoncomputer
vision,pages2951–2960.
Harm De Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville. 2017.
Guesswhat?! visualobjectdiscoverythroughmulti-modaldialogue. InProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition,pages5503–5512.
PascalGermain,AmauryHabrard,FrançoisLaviolette,andEmilieMorvant.2020. Pac-bayesanddomainadapta-
tion. Neurocomputing,379:379–397.
AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi.2019. Thecuriouscaseofneuraltextdegenera-
tion. InInternationalConferenceonLearningRepresentations.
MertInan,PiyushSharma,BaberKhalid,RaduSoricut,MatthewStone,andMaliheAlikhani.2021. Cosmic: A
coherence-awaregenerationmetricforimagedescriptions. arXivpreprintarXiv:2109.05281.
Fredrik D Johansson, David Sontag, and Rajesh Ranganath. 2019. Support and invertibility in domain-invariant
representations. InThe22ndInternationalConferenceonArtificialIntelligenceandStatistics,pages527–536.
PMLR.
ShamMachandranathKakade.2003. Onthesamplecomplexityofreinforcementlearning. UniversityofLondon,
UniversityCollegeLondon(UnitedKingdom).
Seiichi Kuroki, Nontawat Charoenphakdee, Han Bao, Junya Honda, Issei Sato, and Masashi Sugiyama. 2019.
Unsuperviseddomainadaptationbasedonsource-guideddiscrepancy. InProceedingsoftheAAAIConference
onArtificialIntelligence,volume33,pages4122–4129.
Chin-YewLin.2004. Rouge: Apackageforautomaticevaluationofsummaries. InTextsummarizationbranches
out,pages74–81.
ZacharyLipton,Yu-XiangWang,andAlexanderSmola.2018. Detectingandcorrectingforlabelshiftwithblack
boxpredictors. InInternationalconferenceonmachinelearning,pages3122–3130.PMLR.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. 2009. Domain adaptation: Learning bounds and
algorithms. arXivpreprintarXiv:0902.3430.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational
Linguistics,pages311–318.
MaríaPérez-Ortiz,OmarRivasplata,JohnShawe-Taylor,andCsabaSzepesvári.2021. Tighterriskcertificatesfor
neuralnetworks. JournalofMachineLearningResearch,22.
StephanRabanser,StephanGünnemann,andZacharyLipton.2019. Failingloudly:Anempiricalstudyofmethods
fordetectingdatasetshift. AdvancesinNeuralInformationProcessingSystems,32.
IevgenRedko,AmauryHabrard,andMarcSebban.2017. Theoreticalanalysisofdomainadaptationwithoptimal
transport. InECMLPKDD,pages737–753.Springer.
IevgenRedko,EmilieMorvant,AmauryHabrard,MarcSebban,andYounèsBennani.2020. Asurveyondomain
adaptationtheory. ArXiv,abs/2004.11829.
ThibaultSellam, DipanjanDas, andAnkurParikh.2020. BLEURT:Learningrobustmetricsfortextgeneration.
InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages7881–7892,
Online.AssociationforComputationalLinguistics.
Shai Shalev-Shwartz and Shai Ben-David. 2014. Understanding machine learning: From theory to algorithms.
Cambridgeuniversitypress.
Ravi Shekhar, Aashish Venkatesh, Tim Baumgärtner, Elia Bruni, Barbara Plank, Raffaella Bernardi, and Raquel
Fernández.2019. Beyondtasksuccess:Acloserlookatjointlylearningtosee,ask,andGuessWhat. InProceed-
ingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,Volume1(LongandShortPapers),pages2578–2587,Minneapolis,Minnesota.
AssociationforComputationalLinguistics.
JianShen,YanruQu,WeinanZhang,andYongYu.2018. Wassersteindistanceguidedrepresentationlearningfor
domainadaptation. InAAAI.
AnthonySicilia,KatherineAtwell,MaliheAlikhani,andSeongJaeHwang.2022a. Pac-bayesiandomainadapta-
tionboundsformulticlasslearners. InThe38thConferenceonUncertaintyinArtificialIntelligence.
AnthonySicilia,TristanMaidment,PatHealy,andMaliheAlikhani.2022b. Modelingnon-cooperativedialogue:
Theoretical and empirical insights. Transactions of the Association for Computational Linguistics, 10:1084–
1102.
FlorianStrub,HarmDeVries,JeremieMary,BilalPiot,AaronCourvile,andOlivierPietquin.2017. End-to-end
optimizationofgoal-drivenandvisuallygroundeddialoguesystems. InProceedingsofthe26thInternational
JointConferenceonArtificialIntelligence,pages2765–2771.
GaborJSzekely.1989. Potentialandkineticenergyinstatistics. LectureNotes,BudapestInstitute.
GáborJSzékelyandMariaLRizzo.2013. Energystatistics: Aclassofstatisticsbasedondistances. Journalof
statisticalplanningandinference,143(8):1249–1272.
RemiTachetdesCombes,HanZhao,Yu-XiangWang,andGeoffreyJGordon.2020. Domainadaptationwithcon-
ditionaldistributionmatchingandgeneralizedlabelshift. AdvancesinNeuralInformationProcessingSystems,
33:19276–19289.
LeslieGValiant.1984. Atheoryofthelearnable. CommunicationsoftheACM,27(11):1134–1142.
RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh.2015. Cider: Consensus-basedimagedescription
evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566–
4575.
TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi.2019a. Bertscore: Evaluatingtext
generationwithbert. arXivpreprintarXiv:1904.09675.
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. 2019b. Bridging theory and algorithm for
domainadaptation. InInternationalConferenceonMachineLearning,pages7404–7413.PMLR.
HanZhao,RemiTachetDesCombes,KunZhang,andGeoffreyGordon.2019. Onlearninginvariantrepresenta-
tionsfordomainadaptation. InInternationalConferenceonMachineLearning,pages7523–7532.PMLR.

RAINIER: Reinforced Knowledge Introspector
for Commonsense Question Answering
JiachengLiu♥ SkylerHallinan♥ XimingLu♥♠ PengfeiHe♥
SeanWelleck♥♠ HannanehHajishirzi♥♠ YejinChoi♥♠
♥PaulG.AllenSchoolofComputerScience&Engineering,UniversityofWashington
♠AllenInstituteforArtificialIntelligence
liujc@cs.washington.edu
Abstract
Knowledge underpins reasoning. Recent re-
searchdemonstratesthatwhenrelevantknowl-
edgeisprovidedasadditionalcontexttocom-
monsensequestionanswering(QA),itcansub-
stantiallyenhancetheperformanceevenontop
ofstate-of-the-art. Thefundamentalchallenge
iswhereandhowtofindsuchknowledgethat
is high quality and on point with respect to
thequestion;knowledgeretrievedfromknowl-
edgebasesareincompleteandknowledgegen-
eratedfromlanguagemodelsareinconsistent.
We present RAINIER1, or Reinforced Knowl- Figure 1: RAINIER can introspect for commonsense
edgeIntrospector, thatlearnstogeneratecon- knowledgethatunderpinthereasoningprocess,andis
textually relevant knowledge in response to trained via reinforcement learning, where the reward
given questions. Our approach starts by im- is derived from the effectiveness of knowledge when
itating knowledge generated by GPT-3, then promptingafrozen,genericQAmodel.
learnstogenerateitsownknowledgeviarein-
forcement learning where rewards are shaped tion. Thishindersmodels’performanceandrobust-
basedontheincreasedperformanceonthere- nessoncommonsensetasks,andmakesitdifficult
sulting question answering. RAINIER demon- to inspect their point of failure. Recent research
stratessubstantialandconsistentperformance
hasdemonstratedthatrelevantknowledgecanpro-
gains when tested over 9 different common-
videusefulcontextforapproachingcommonsense
sense benchmarks: including 5 datasets that
tasks. Yet these methods either retrieve from in-
are seen during model training, as well as 4
domainknowledgebases(Mitraetal.,2019;Chang
datasets that are kept unseen. Our work is
thefirsttoreportthatknowledgegeneratedby etal.,2020)thatdonothavegoodcoverageover
models that are orders of magnitude smaller commonsense, or generate knowledge from neu-
than GPT-3, even without direct supervision ral models (Shwartz et al., 2020; Gu et al., 2022;
on the knowledge itself, can exceed the qual- Liuetal.,2022),whichoftenneeddomain-specific
ity of commonsense knowledge elicited from
engineering and very large models (e.g. GPT-3
GPT-3.
(Brownetal.,2020)). Itisstillanopenchallenge
tosystematicallyfindhigh-qualityknowledge.
1 Introduction
In this work, we use a novel, reinforcement-
Commonsenseisasignificantchallengeformod-
learning-basedmethodtodevelopRAINIER,agen-
ernNLPmodels,duetotheobscurityofunderly-
erative neural model that can introspect the un-
ingknowledgethatgroundsthereasoningprocess.
derlyingknowledgeforreasoningaboutcommon-
Whilehumansaregenerallyabletointrospectthe
sensequestions. AsillustratedinFig1,RAINIERis
underlying reasons for their conclusion (Mercier
trainedtogenerateknowledgethatarebothfluent
andSperber,2017),neuralmodelslackthecapabil-
natural language statements, and useful prompts
itytoverbalizethepremisesleadingtotheirpredic-
that optimize the performance of a generic ques-
tionanswering(QA)model. Ourmodel(1)demon-
1Code,modelandknowledge-extendeddatasetsareavail-
ableathttp://github.com/liujch1998/rainier stratesstronggeneralizationtounseenbenchmarks
2202
tcO
22
]LC.sc[
2v87030.0122:viXra
without additional engineering effort (e.g. fine-
tuning),(2)producescommonsenseknowledgeof Algorithm1Training RAINIER
highqualityanddiversity,and(3)issubstantially Input initial policy model θ 0, initial value model φ 0, pre-
trainedQAmodelψ
smallerinsizecomparedtoGPT-3,thebestknowl- QA
D ←GetsilverknowledgeonD fromGPT-3.
imit seen
edgesourcereportedsofar. θ ←Optimizeθ withEqn2fromD . (cid:46)Section2.1
imit 0 imit
θ
RAINIER
←REINFORCEDLEARNING(D seen,θ imit,φ 0,ψ QA)
TotrainRAINIER,weoptimizeknowledgeintro- (cid:46)Section2.2
spectionfortheresultingQA,insteadofdirectsu- procedureREINFORCEDLEARNING(D seen,θ,φ,ψ QA)
θ ←θ,φ ←φ
pervision,becausethereareusuallynogoldknowl- old old
foriterations=1,2,... do
edge labels on commonsense datasets. In order SampleaminibatchfromD .
seen
toensurethatourmodellearnstogenerategener- forstep=1,2,...,sdo
ComputeL ontheminibatchwithEqn3.
ically useful knowledge for a broad range of QA PPO
OptimizeθandφwithL foronestep.
PPO
models,wetrainonly RAINIER,theknowledgein-
θ ←θ,φ ←φ
old old
trospector,withoutfinetuningtheQAmodel. Since returnθ
our desired knowledge are sequences of discrete, Outputθ
RAINIER
non-differentiable word tokens, we adapt a rein-
forcementlearningalgorithm,ProximalPolicyOp-
timization(PPO)(Schulmanetal.,2017;Ouyang
RAINIER model, and the commonsense datasets
etal.,2022),tooptimizetheknowledgeintrospec-
extendedwithknowledgegeneratedbyRAINIER.
tor. Specifically, the reward is defined as the ef-
fectof RAINIER-generatedknowledgeontheQA 2 Method
model’sprediction. WetrainRAINIERinamulti-
Problem Overview. We focus on the tasks of
tasksettingon8commonsenseQAdatasets–en-
multiple-choice commonsense QA, consisting of
compassinggeneral,scientific,physical,andsocial
instancesofformatx = (q,A,a∗),whereq isthe
commonsense–toequipthemodelwithbettergen-
question, A is the set of candidate answers, and
eralizationtounseenbenchmarks.
a∗ ∈ A is the correct answer. For full contextu-
Experiments show that RAINIER substantially alization, we append candidate answers A to the
improvestheperformanceofQAmodelson9com- question q to form the input to the QA model as
monsensebenchmarks(5datasetsseenduringtrain- follows:
ing and 4 unseen datasets), and gives larger and
moreconsistentgainsthanafew-shotGPT-3(Liu q = {question} (A) {choice_A} (B) {choice_B} ...
etal.,2022)despitebeing16xsmallerinparameter
Common approaches only train supervised QA
size. Italsobooststheperformanceontopofthose
models. As a complement, we train a separate
QA models that it is not trained against, indicat-
model, which we refer to as RAINIER, that can
ingthatitgeneratesgenericallyusefulknowledge
introspect question-specific knowledges that are
instead of merely hacking into the reward given
usefultopromptafixedQAmodel. RAINIERisa
by a single QA model. Knowledge generated by
sequence-to-sequencelanguagemodel,p (k|q;θ),
K
RAINIER can even boost a QA model that is 4x
andweexpectittogenerateknowledgestatements
largerthanit,showingthepromiseofusingmodel-
(k’s) in response to the given question (q). How-
generated knowledge as a complement to model
ever,thechallengeisthatwehavenogoldknowl-
scaling in making progress in commonsense rea-
edgelabelsassupervision.
soning. Our analyses show that the knowledge
generatedbyRAINIERareofhighquality,andare Training. Sincewedonothavegoldknowledge
diverseintermsofdomain(e.g. scientific,social), to train RAINIER, we obtain this model by fine-
relation expressed (e.g. part of, member of, pur- tuningapretrainedlanguagemodelintwostages:
pose),andsyntacticproperty(e.g. negation,com- (I)imitationlearning,andthen(II)reinforcement
parison). TheeffectoftheseknowledgeontheQA learning. InStageI(§2.1),wegetsilverknowledge
modelalsoalignswellwithhumanjudgments. The labelsonsomedatasetsfromGPT-3,andteachour
successof RAINIER showsthatmoderately-sized modeltoimitatethisknowledge-generatingGPT-3.
models can serve as source of high-quality and Thisequipsourmodelwiththebasicfunctionality
usefulcommonsenseknowledgethatfacilitatesrea- of knowledge generation. In Stage II (§2.2), we
soning. Wepubliclyreleasethecode, thetrained usereinforcementlearningtocontinuetrainingthe
model obtained in Stage I to make the generated 2.2 TrainingStageII:Reinforcement
knowledge more useful while staying fluent and Learning
meaningful. Specially,wesettherewardtobethe
Aswewillseeintheempiricalresults,theimitation
effectofthegeneratedknowledgeontheprediction
model obtained in Stage I does not provide the
madebyafixed,genericQAmodel. Weobtainsil-
most beneficial knowledge. Therefore, in Stage
verknowledgeandtrain RAINIERontheunionof
II, we continue optimizing RAINIER to generate
multipleQAdatasets(whichareconsideredseen
knowledge that best prompts the QA model, by
during training), i.e. D =
(cid:83)∆seenD
, where
seen d=1 d directlymaximizingtherewardgivenbythisQA
D d = {(q j,A j,a∗ j)} j|D =d 1| . The generic QA model model.
weusemayormaynothavebeentrainedonthese
Knowledge generation as reinforcement learn-
seen datasets. The complete training process is
ing. Since knowledge statements (k’s) are dis-
outlinedinAlgorithm1.
creteandthusnon-differentiable,weadoptarein-
Inference. Theeffectivenessof RAINIERiseval- forcementlearningapproach,andconsiderknowl-
uatedagainstasetofunseenQAdatasets,D unseen, edge generation as a sequential decision making
inadditiontotheseendatasets. NotethatRAINIER processoverthenaturallanguagevocabularyspace.
isnottrainedonanyunseendatasets,whichmeans Weconsiderthegenerationofknowledgestatement
weneithergetsilverknowledge,nordoimitation k withT tokensasanepisodeoflengthT. Atstep
learning or reinforcement learning on them. The t ∈ [1,T], the state s = (q,k ) is the combina-
t <t
genericQAmodelweusewasnottrainedonany tionofthequestionandtheknowledgedecodedup
unseendatasetsaswell. Wediscussdetailsofinfer- to the (t−1)-th token; the action a = k would
t t
encein§2.3. bethet-thtokentodecode. The RAINIER model,
p (k |q,k ;θ), is the policy model that we op-
2.1 TrainingStageI:ImitationLearning K t <t
timize. We define a reward function r(x,k) that
In Stage I, we train RAINIER so that it generates
characterizes the effect of the knowledge on the
fluentandmeaningfulnaturallanguagestatements
QAmodel’sprediction,anddiscussthedefinition
thatresembleknowledge. Thereisnolarge-scale
ofthisrewardfunctionin§2.2.1.
commonsenseQAdatasetlabeledwithhigh-quality
Toensurethatthegeneratedknowledgestayflu-
knowledge,butGPT-3hasbeenshownasagood
entandmeaningful,wewouldlikethelearnedpol-
generatorforrelevantknowledge(Liuetal.,2022).
icymodelnottomovetoofarfromtheinitialim-
Therefore, we get silver knowledge from GPT-3
itation model. Therefore, we add to the reward
onourseendatasets. FollowingLiuetal.(2022),
an(approximate)KLpenaltybetweenthelearned
weelicitquestion-relatedknowledgebyprompting
policyandtheinitialpolicy(Ouyangetal.,2022),
GPT-3withatask-specificsetoffew-shotdemon-
strations(See§Cfordetailsontheprompts),and p (k|q;θ)
K
R(x,k) = r(x,k)−βlog .
decodingM knowledgeforeachquestion: p (k|q;θ )
K imit
(cid:8) (cid:9)
K(q) = k : k ∼ p (k | prompt(task(q)),q) ,
m m G Since this reward is computed based on the full
where p (·|·) denotes GPT-3 with nucleus sam- knowledgestatement,weassignittothelaststep
G
plingwherep = 0.5(Holtzmanetal.,2020). This of the episode. Non-terminal steps are assigned
yieldsasilverdatasetofquestion-knowledgepairs: zerorewards. Formally,
(cid:110) (cid:111)
D = (q,k) : (q,A,a∗) ∈ D ,k ∈ K(q) , r = R(x,k) (whereT = |k|andk = [EOS]);
imit seen T T
(1) r t = 0 (where1 ≤ t < T).
We then train RAINIER, starting from a pre-
We employ Proximal Policy Optimization2
trainedsequence-to-sequencelanguagemodel,on
(PPO)(Schulmanetal.,2017)asourreinforcement
thissilverdatasetwithstandardsupervisedloss:
learningalgorithm,andadaptfromtheimplemen-
(cid:88)
Ltrain(θ) ∝ −logp (k|q;θ). (2) tationofPPOinOuyangetal.(2022). Asidefrom
K
(q,k)∈D it mra ii tn 2WechoosePPObecauseithasshownsuccessfulresults
in other NLP tasks (Nakano et al., 2021; Stiennon et al.,
Theparameterizationoftheresultingmodelisde-
2020). OurearlierexperimentswithREINFORCEdidnot
notedasθ imit. showpromisingresults.
the policy model, PPO additionally uses a value 2.2.1 RewardShaping
model(parameterizedbyφ)toestimatethevalue We define the reward function in reinforcement
function for states with incomplete decoded text, learning as the quantified effect of RAINIER’s
i.e. V(s t;φ)foranyt. PPOminimizesajointloss, knowledge on the QA model’s prediction. Sup-
posewealreadyhaveareasonablygoodQAmodel,
L PPO(θ,φ) = L Policy(θ)+α·L Value(φ), (3) whichassignsaprobabilityscoreP QA(a|q)toany
candidate answer a ∈ A. Since we will use a
where L (θ) is the loss on the policy model,
Policy sequence-to-sequence language model (i.e. Uni-
L (φ)isthelossonthevaluemodel,andαisa
Value fiedQA(Khashabietal.,2020))astheQAmodel,
hyperparameter.
wedefine
Policy loss. To obtain the policy loss, we first expS (a|q)
QA
P (a|q) = ,
computethetruncatedestimatedadvantagefunc- QA (cid:80) expS (a(cid:48)|q)
a(cid:48)∈A QA
tion,
where
T−1
Aˆ
t
= (cid:88) (γλ)t(cid:48)−tδ t(cid:48), 1 (cid:88)|a|
S (a|q) = −logp (a |q,a ;ψ ),
t(cid:48)=t QA |a| QA i <i QA
i=1
where δ = r +γV(s ;φ)−V(s ;φ),
t(cid:48) t(cid:48) t(cid:48)+1 t(cid:48)
wherep (a |q,a ;ψ )isthelanguagemodel-
QA i <i QA
where the value functions V(·) are estimated by ingscorereceivedbya , thei-thtokenofa. The
i
thevaluemodel. PPOthenmaximizestheempir- naive prediction would be the candidate answer
ical expectation of a so-called clipped surrogate thatgetsthehighestP (a|q)(orequivalently,the
QA
objectiveterm, highestS (a|q)): aˆ = argmax P (a|q).
QA a∈A QA
WeaimatmaximizingP (a∗|q◦k),theprob-
QA
cso(Aˆ t,ν t(θ),ε) = abilityscorereceivedbythecorrectanswerwhen
min(cid:0) ν (θ)Aˆ ,clip(ν (θ),1−ε,1+ε)Aˆ (cid:1) , theQAmodelispromptedwiththeknowledgek
t t t t
generatedbyRAINIER,and◦denotestextconcate-
whereν (θ) =
pK(kt|q;θ)
istheratiobetweenthe
nation. One naive definition of reward function
t pK(kt|q;θ old) maybe
currentpolicyθ andalaggingpolicyθ . Thelag-
old
gingpolicyisupdatedtothecurrentpolicyundera r(x,k) = P (a∗|q◦k)−P (a∗|q).
QA QA
fixedintervalofstrainingsteps,andiskeptfixed
otherwise. Weadaptthistoourusecase,anddefine However, this reward only captures the absolute
thepolicylossas change of score, but not whether the model pre-
dictionischangedornot. Toremedyforthis,we
(cid:104) (cid:105)
L (θ) = −Eˆ cso(Aˆ ,ν (θ),ε) definetherewardfunctionas
Policy t t
1(cid:104)
where the expectation is taken over all instances r(x,k) =
2
in the training data (x ∼ D str ea ei nn), the distribution tanh(cid:0) S (a∗|q◦k)− maxS (a(cid:48)|q◦k)(cid:1)
QA QA
of model-generated knowledge as determined by a(cid:48)∈A,
a(cid:48)(cid:54)=a∗
the current policy conditioning on the instance’s
(cid:105)
question (k ∼ p (k|q;θ)), and all tokens in the −tanh(cid:0) S (a∗|q)− maxS (a(cid:48)|q)(cid:1) .
K QA QA
a(cid:48)∈A,
knowledgestatement(t ∈ [1,|k|]).
a(cid:48)(cid:54)=a∗
Valueloss. ThevaluemodelistrainedwithMSE Intuitively, this function would give a reward of
losswithrespecttothetargetvalue,Vtarg
,whichin near +1 if the naive prediction is incorrect (i.e.
t
turnisestimatedwithalaggingvaluemodelφ old: S QA(a∗|q) < max a(cid:48)∈A,a(cid:48)(cid:54)=a∗S QA(a(cid:48)|q)), while
theknowledge-promptedpredictioniscorrect(i.e.
(cid:104) (cid:105)
L (φ) = Eˆ (cid:0) V(s ;φ)−Vtarg(cid:1)2 , S (a∗|q ◦ k) > max S (a(cid:48)|q ◦ k)).
Value t t QA a(cid:48)∈A,a(cid:48)(cid:54)=a∗ QA
Similarly,therewardwouldbenear−1ifthenaive
T−1
where Vtarg = (cid:88) γt(cid:48)−tr +γT−tV(s ;φ ). predictioniscorrectbuttheknowledge-prompted
t t(cid:48) T old
prediction is incorrect. The hyperbolic tangent
t(cid:48)=t
servesasasmoothedsignfunction,andprovides 3 Experiments
asoftinterpolationbetweenthetwopolarityofre-
Seen datasets. For both imitation learning and
wardvaluesbytakingintoaccountthemarginof
reinforcement learning, we use the 8 multiple-
thecorrectanswer.
choicedatasetsthatUnifiedQA (Khashabietal.,
Wealsoexperimentwithsomealternativedefini- v2
2022)usesfortraining: OpenBookQA(Mihaylov
tionsoftherewardfunction. SeeTable4.
etal.,2018),ARC(Clarketal.,2018),AI2Science
Reward normalization. To stabilize training, (Clark et al., 2018), CommonsenseQA (Talmor
weapplyanaffinetransformationontherewards et al., 2019), QASC (Khot et al., 2020), Physi-
sothatinitiallytheyarenormalized. Beforestart- calIQA(Bisketal.,2020),SocialIQA(Sapetal.,
ing Stage II training, we use the imitation model 2019),andWinogrande(Sakaguchietal.,2021).4
togenerateaknowledgestatementforeachtrain-
Unseen datasets. We additionally evaluate our
inginstance,andestimatethepopulationmeanand
method on the following 4 multiple-choice QA
standarddeviationofrewards:
datasets that our model was not trained on: Nu-
R init = (cid:8) r(x,k) : x ∈ D str ea ei nn,k ∼ p K(·|q;θ imit)(cid:9) , merSense(Linetal.,2020),RiddleSense(Linetal.,
µ = µ(R ),σ = σ(R ). (4) 2021), QuaRTz (Tafjord et al., 2019), and Hel-
0 init 0 init
laSwag(Zellersetal.,2019).
InStageIItraining,eachrewardisnormalizedas:
Models. ForStageItraining,wegetsilverknowl-
r(x,k)−µ
0
r(x,k) ← . (5) edgefromtheGPT-3-Curie(13B)model(Brown
σ
0
et al., 2020). The knowledge introspector is ini-
2.3 Inference: KnowledgePromptingand
tializedwithT5-large(Raffeletal.,2019),which
Aggregation
has 0.77B parameters. For Stage II training, we
FollowingLiuetal.(2022), atinferencetimewe initialize the value model with T5-large, and re-
use RAINIER togeneratemultipleknowledgeper place the language modeling head with a value
question,andprompttheQAmodelbyindividually regressionhead,whichisinitializedfromscratch;
concatenatingeachknowledgetothequestion. The we use UnifiedQA-large (UQA-large) (Khashabi
knowledgearegeneratedbyRAINIERwithnucleus etal.,2020)astheQAmodelthatprovidesreward,
samplingwherep = 0.5(Holtzmanetal.,2020), whichmeansthetextconcatenationfunctionisde-
K(q) = {ε}∪(cid:8) k : k ∼ pp=0.5(k | q;θ), fined as q ◦k = {q} \n {k}. We use the same
m m K
(cid:9) questionformattingasUnifiedQA.SeeTable7for
m = 1...M ,
hyperparameters.
whereM isthenumberofknowledgeperquestion,
Baselines. We mainly report performance im-
andεdenotesemptystring. Wecollectasetofout-
provements over the vanilla QA baseline (i.e. di-
putsforpromptingwitheachknowledge. Thefinal
rect inference with the UnifiedQA-large model
prediction is the candidate answer that receives
andwithoutprompting RAINIER-generatedknowl-
maximumconfidence,
edge). Wealsoconsiderusingknowledgefrom:
aˆ = argmax max P (a|q◦k),
QA
a∈A k∈K(q) • Few-shot GPT-3 (Liu et al., 2022), where
andthepredictionissupportedbyasingleknowl- knowledge statements are elicited from the
edge–theselectedknowledge, GPT-3-Curie(13B)model–underthesame
prompts used for getting silver knowledge
kˆ = argmaxmaxP (a|q◦k).
QA in Stage I training (§2.1), and the same hy-
k∈K(q) a∈A
perparameter setting for decoding (M = 10
Trainingtimemodelselection. InStageIItrain-
knowledge per question, with nucleus sam-
ing,weonlygenerateoneknowledgeperquestion
plingwherep = 0.5).
forthevalidationset.3 Predictionsaremadeusing
thesameknowledgepromptingmethodasabove, • Self-talk(Shwartzetal.,2020),wherewegen-
andthemodelcheckpointwiththemaximalaccu- erate M = 10 knowledge per question with
racyontheunionofallvalidationsetsisselected. GPT-3-Curieandavarietyoftemplates.
3Thisisforefficiencypurposes.Weusegreedydecoding 4WeexcludeMCTestandRACEbecausemostquestions
herebecauseitismorestablethannucleussamplingwhen inthesereadingcomprehensiondatasetsaretoolongtofitinto
generatingonlyoneknowledgeperquestion. ourmodel’sinput.
Dataset→ CSQA QASC PIQA SIQA WG Avg.
Method↓ dev test dev test dev test dev test dev test dev test
UQA-large(0.77B) 61.43 53.00 43.09 45.65 63.66 65.50 53.84 57.21 53.35 54.67 55.07 55.21
+Few-shotGPT-3-Curie(13B) 66.34 – 53.24 – 64.25 – 58.29 – 55.56 – 59.54 –
+Self-talkGPT-3-Curie(13B) 63.31 – 49.89 – 65.23 – 51.89 – 52.96 – 56.66 –
+DREAM(11B) 64.54 – 49.46 – 64.74 – 51.59 – 56.12 – 57.29 –
+RAINIER-large(0.77B)[ours] 67.24 60.18 54.97 54.13 65.67 67.09 57.01 59.01 56.91 57.39 60.36 59.56
Table 1: Results on seen datasets. All experiments use UnifiedQA-large as the QA model, and optionally uses
knowledgefromoneoftheknowledgegenerationmodels. Skippedbaselinesaremarkedwith“–”.
Dataset→ NS RS QuaRTz HS Avg.
Method↓ dev test-all dev test dev test dev test dev test
UQA-large(0.77B) 26.50 19.61 28.11 38.34 68.75 67.60 35.00 34.30 39.59 41.85
+Few-shotGPT-3-Curie(13B) 38.00 – 35.65 – 69.01 – 37.33 – 45.00 –
+RAINIER-large(0.77B)[ours] 30.00 21.81 30.07 41.22 70.31 68.24 35.73 34.80 41.53 43.76
Table2: Resultsonunseendatasets.
• DREAM(Guetal.,2022),wherewegenerate Performance on unseen datasets. Table 2
M = 10sceneelaborationsperquestionwith showsthat RAINIER’sknowledgesubstantiallyim-
theDREAM(11B)model. provesperformanceoverthevanillaQAbaseline
onthefourunseendatasets,demonstratingitsgen-
See§A.2formoredetailsonthesebaselines. We
eralizationcapability.
donotcomparewithchain-of-thoughtprompting
(Wei et al., 2022) because it relies on emergent
Choice of QA model for evaluation. To verify
behaviors that does not exist in the scale that we
that our RAINIER model is not hacking into the
experimentwith.
rewards provided by the QA model we use dur-
4 Results ingtraining,weevaluatetheeffectof RAINIER’s
knowledge on different QA models. We choose
4.1 MainResults
threeotherUnifiedQAmodelswithdifferentsizes,
Performance on seen datasets. Table 1 shows as well as a different model known as Unicorn
theperformanceof RAINIER-enhancedQAmodel (Lourie et al., 2021). Results are shown in Fig-
on the seen datasets. On average, our method ure 2. RAINIER consistently gives performance
achievesmorethan5%improvementoverdirectly gainsontopofallQAmodels,indicatingthatits
applying the QA model. The knowledge gener- knowledgearegenerallyusefulinformationrather
ated by RAINIER improves performance on five thanmereartifactsofmodel-specificrewardhack-
benchmarks: CommonsenseQA, QASC, Physi- ing. We even observe performance gains with a
calIQA, SocialIQA, and Winogrande, with the QA model that is 4x as large as RAINIER, which
greatestimprovementonCommonsenseQA(+6%) means generating and prompting relevant knowl-
andQASC(+12%). AsshowninTable8,thereis edgecanbeatechniquecomplementarytomodel
noperformancegainonOpenBookQA,ARC,and scaling,andcanbedonemeaningfullywithsmaller
AI2Science. Weconjecturethatthisisbecausethe models. Finally, we see the largest improvement
QAmodel,UnifiedQA,isalreadytrainedonthese whentheQAmodelitselfhasweak,butnon-trivial,
threedatasets,thussettingastrongbaseline. performance(UnifiedQA-smallforseendatasets,
andUnifiedQA-baseforunseendatasets).
Comparison with other models. Compared to
RAINIER,otherknowledgegenerationmodels,in-
4.2 Ablations
cluding few-shot GPT-3, Self-talk, and DREAM,
providegenerallyweakerimprovementsoverthe StageIandStageIItraining. Weexperimented
vanilla QA baseline. In particular, RAINIER out- withomittingtheStageI(imitation)and/orStageII
performs GPT-3-based models while being 16x (reinforcement)fromthetrainingpipeline. Results
smallerinparametersize(0.77Bvs. 13B). are shown in Table 3. Without Stage I training,
Figure2: EffectivenessofRAINIER-generatedknowledgeondifferentQAmodels. Averageaccuracyondevsets
isreported. (Note: resultsoffew-shotGPT-3-CurieonUnicorn-largeismissing.)
QAModel→ UQA-large
RewardFunc.↓ Definition:r(x,k)=... seen unseen
QAModel→ UQA-large
KnowledgeGen.↓ seen unseen RAINIER’s 1 2(cid:104) tanh(cid:0) S QA(a∗|q◦k)−max a(cid:48)∈A,a(cid:48)(cid:54)=a∗S QA(a(cid:48)|q◦k)(cid:1)
None 55.07 39.59 −tanh(cid:0) S QA(a∗|q)−max a(cid:48)∈A,a(cid:48)(cid:54)=a∗S QA(a(cid:48)|q)(cid:1)(cid:105) 60.36 41.53
RAINIER-large 60.36 41.53 Probonly P QA(a∗|q◦k) 59.11 40.61
–StageI 53.68 36.83 Probdiff P QA(a∗|q◦k)−P QA(a∗|q) 60.69 40.91
–StageII 57.00 40.70 Scorediff S QA(a∗|q◦k)−S QA(a∗|q) 58.26 39.86
–StageI–StageII 53.29 36.72 Hardactivation 21(cid:104) sgn(cid:0) S QA(a∗|q◦k)−max a(cid:48)∈A,a(cid:48)(cid:54)=a∗S QA(a(cid:48)|q◦k)(cid:1)
Table 3: Ablations on the impor- −sgn(cid:0) S QA(a∗|q)−max a(cid:48)∈A,a(cid:48)(cid:54)=a∗S QA(a(cid:48)|q)(cid:1)(cid:105) 58.32 41.16
tanceofbothtrainingstages.
Table4: Ablationsonthechoiceofrewardfunction.
RAINIERdoesnotimprovetheperformanceofthe sopotentialbiasiseliminated.
QAmodel(regardlessofwhetheritistrainedwith
Stage II or not), showing the indispensability of Quality. First,wefollowLiuetal.(2022)byan-
equippingthemodelwiththebasicfunctionalityof notatingthequalityaspects–relevance,factuality,
knowledgegeneration. Ontheotherhand,amodel andhelpfulness–ofeachknowledgewithrespect
trainedsolelywithStageIgivesweakerimprove- tothequestion. Wefindthat RAINIER-generated
mentsthanthefullytrainedRAINIER,stressingthe knowledge are overwhelmingly related to the re-
importanceofStageIItrainingaswell. spectivequestions. 64%arefactuallycorrect,25%
arefactuallyincorrect,andtheremaining11%have
Rewardfunction. Table4showstheresultsfor undeterminedfactualityduetovariousreasons(e.g.
knowledgeintrospectorstrainedwithdifferentre- ambiguity, cultural sensitivity). 58% are seen by
wardfunctions. Ourrewardshapinggivesthebest human as being helpful for reasoning about the
performanceonunseendatasets,aswellasoneof question,whereas24%areseenasharmful.
the top performance on seen datasets. While the
Inourannotations,thereare420knowledgethat
naiveprobdiff rewardfunctiongivesslightlybetter
rectifyUnifiedQA-large’spredictions(i.e. flipping
performanceonseendatasets,ourrewardshaping
fromwrongtoright),and246knowledgethatmis-
resultsinbettergeneralization.
lead the predictions (i.e. flipping from right to
wrong). Amongtherectifyingknowledge,84%are
4.3 Analysis
deemedhelpfulbyhuman;andamongthemislead-
Togetadeeperunderstandingofthebehaviorand ing knowledge, 62% are deemed harmful. These
capabilityof RAINIER,wemanuallyanalyzedthe resultshavesimilartrendsasLiuetal.(2022),and
generatedknowledgealongseveralqualityanddi- showthatRAINIER’sknowledgeareofhighquality
versity aspects. We asked three NLP experts to andinterpretabilityinhelpingQAmodels.
annotate the selected knowledge (§2.3) for up to
100questionsperdatasetamongthevalidationsets Diversity. Additionally,weanalyzethediversity
of8benchmarks(5seen,3unseen;seeFigure3). It aspectsbyannotatingeachknowledgewiththedo-
washiddenfromtheannotatorswhethertheknowl- main(s) it belongs to (e.g. scientific, social), the
edgerectifiesormisleadsQAmodel’sprediction, relation(s)itexpresses(e.g. attribute,capableof),
Figure3: HumananalysisofRAINIER-generatedknowledge. Left: Percentageofgoodknowledgeineachquality
aspect. Mid: Agreementbetweenhumanandmachineonhelpfulnessofselectedknowledge. Right: Percentage
ofRAINIER-generatedknowledgecategorizedbydomain,expressedrelation,andsyntax. Thepercentagesdonot
addupto100%becausesomeknowledgehavenoneofthesecharacteristics,whilesomeothersmayhavemultiple.
and its syntactic property(s) (e.g. negation, com- et al. (2020) and Paranjape et al. (2021) prompt
parison). SeeFigure3forcompletelistofoptions pretrained models with pre-defined templates to
undereachaspect. Theknowledge’sdomaindistri- generate question clarifications or contrastive
butionisstronglytiedtothedomainofthebench- explanations,whichareinturnusedtopromptthe
mark(e.g. scientificforQASCandQuaRTz,social inference model. The above approaches all pose,
forSocialIQAandWinogrande,numericalforNu- implicitly or explicitly, certain constraints (e.g.
merSense). Thedomainaspectismorediversefor domain,relation,syntax)onthemodel-generated
benchmarks that test general commonsense, like text. In contrast, Wei et al. (2022) elicits full
CommonsenseQAandRiddleSense. Fortherela- chain-of-reasoning from language models with
tionaspect,therearemanyknowledgethatexpress in-contextlearning;Liuetal.(2022)usesfew-shot
an“attribute”relation,whileotherrelationsarealso demonstrations to elicit flexible, relevant knowl-
substantially represented. As for syntax, a good edgestatementsfromalanguagemodel,andWang
proportionoftheknowledgecontainstructureslike et al. (2022) distills this capability into smaller
comparison and negation. Therefore, RAINIER’s modelsusingsupervisedlearning. Thesemethods
knowledgehavegoodsyntacticandsemanticdiver- providemoreflexibilityontheknowledge,yetthey
sitywhilebeingabletoadapttothedomain. relyonaccessingverylargelanguagemodels(e.g.
GPT-3). Asidefrommethodsthatmakereasoning
4.4 QualitativeExamples explicit in a linear chain manner, another set of
work produce recursive structures of reasoning,
Weshowsomeexamplesofgoodknowledgegen-
through either backward chaining (Dalvi et al.,
eratedby RAINIERinTable5.
2022; Jung et al., 2022) or forward chaining
5 RelatedWork (Bostrom et al., 2022). Our work contributes to
this line of research, yet we depart from prior
Explicit reasoning for commonsense QA. work by presenting the first approach that learns
Commonsense question answering poses a togeneraterelevantknowledgewithoutrequiring
significantchallengetomodernneuralmodels. To human-labeledgoldknowledge.
improve performance and interpretability, many
work have proposed to do explicit reasoning ReinforcementlearningforNLP. Recently,re-
for tasks in this area, that is, to verbalize the inforcementlearningmethodshavebeenadopted
intermediate text artifacts that facilitate the rea- for NLP tasks like question answering (Nakano
soningprocess. Rajanietal.(2019)andLatcinnik etal.,2021),summarization(Stiennonetal.,2020;
and Berant (2020) use supervised learning to Paulus et al., 2018), machine translation (Shen
train models to generate text explanations, while etal.,2016;Wuetal.,2016),groundedtextgener-
Gu et al. (2022) and Bansal et al. (2021) use ation(Ammanabroluetal.,2021,2022),controlled
similartrainingregimestoobtainmodelsthatcan textgeneration(Luetal.,2022),andpromptgen-
generate scene elaborations and paths through a eration(Guoetal.,2021;Dengetal.,2022). Our
structuredknowledgegraph,respectively. Shwartz application of reinforcement learning on knowl-
Task Question/Knowledge Domain Relation Syntax
Whatwouldvinylbeanoddthingtoreplace?(A)pants(B)recordalbums(C)
CSQA
recordstore(D)cheese(E)wallpaper
Vinylisatypeofplastic. scientific memberof –
Somepelycosaursgaverisetoreptileancestralto(A)lamphreys(B)angiosperm
QASC
(C) mammals (D) paramecium (E) animals (F) protozoa (G) arachnids (H)
backbones scientific
Reptilesaretheancestorsofallmammals. temporal attribute –
SydneyrubbedAddison’sheadbecauseshehadahorribleheadache.Whatwill
SIQA
happentoSydney?(A)drifttosleep(B)receivethanks(C)bereprimanded
Agooddeedwillberewarded. social – –
AdamalwaysspentallofthefreetimewatchingTvunlikeHunterwhovolun-
WG
teered,dueto_beinglazy.(A)Adam(B)Hunter
HunterismoreactivethanAdam. social attribute comparison
Causesbadbreathandfrightensblood-suckers(A)tuna(B)iron(C)trash(D)
RS
garlic(E)pubs
Garlicisastrong-smellingfood. – attribute –
Ifthemassofanobjectgetsbiggerwhatwillhappentotheamountofmatter
QuaRTz
containedwithinit?(A)getsbigger(B)getssmaller scientific
Themassofanobjectisproportionaltotheamountofmatteritcon- physical – –
tains.
Table 5: Examples of good knowledge generated by RAINIER. Each of these knowledge rectifies UnifiedQA-
large’sprediction,andislabeledbytheannotatorasrelevant,factual,andhelpful.
edgeintrospectionisnovel. Theideaofreinforce- Limitations
mentlearningwithmodel-providedfeedbackhas
Despitethepositiveeffectofourknowledgeintro-
beenpreviouslyexploredinGuoetal.(2021),Am-
spector RAINIER on commonsense QA tasks, its
manabroluetal.(2021),andLuetal.(2022). The
performanceonnon-commonsenseapplicationsis
PPO algorithm has been previously employed to
unknown and thus requires further investigation.
optimize rewards learned from human feedback
Evenforcommonsenseapplications,thereisstilla
(Nakano et al., 2021; Stiennon et al., 2020). In
largegapbetweenmodelperformanceandhuman
contrast, we use PPO to optimize reward purely
performance, so the resulting model is not ready
derivedfromthedecision-makingneuralmodels.
forreal-worldapplications. Thereisalsoalimit
onthelengthofknowledgeitgeneratesinourex-
perimental setting, and it has not been tested on
6 Conclusion
generating long and coherent text. Furthermore,
insomecasesitmaygenerateknowledgethatex-
press inappropriate social values (Table 10), are
WeintroducedRAINIER,aneuralmodelthatcanin-
culture-specific(Table11),orcontainethicalrisks
trospectforrelevantknowledgeonabroadrangeof
(Table12). See§Bforexamples. Extracareshould
commonsensequestionansweringtasks. RAINIER
be taken when applying our model in production
istrainedwithanoveladaptionofreinforcement
environments,especiallywhenmakingcriticalde-
learning,anddoesnotneedgoldknowledgelabels
cisionsorexposingitsgeneratedcontentsdirectly
thataredifficulttoobtain. Knowledgegeneratedby
tohumanendusers.
RAINIERcanserveasusefulpromptsthatimproves
the performance of QA models on both seen and
Acknowledgements
unseen benchmarks, and outperform knowledge
elicitedfromafew-shotGPT-3whichis16xbigger. ThisworkwasfundedinpartbytheDARPAMCS
RAINIER generatesknowledgeintheformofnatu- program through NIWC Pacific (N66001-19-2-
rallanguagestatementsthatarefluent,meaningful, 4031), NSF IIS-2044660, and ONR N00014-18-
high-quality, and diverse in terms of domain and 1-2826. We thank OpenAI for offering access to
relation;furthermore,theeffectoftheseknowledge theGPT-3API.
ontheQAmodelisfoundtoalignwellwithhuman WewouldliketothankPrithvirajAmmanabrolu,
judgments. Alisa Liu and Weijia Shi for the discussion and
feedback on early drafts of the paper. We also (DeeLIO): The First Workshop on Knowledge Ex-
thanktheanonymousreviewersfortheirvaluable tractionandIntegrationforDeepLearningArchitec-
tures,pages74–79,Online.AssociationforCompu-
feedback.
tationalLinguistics.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,
References
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap,
swering? tryarc,theai2reasoningchallenge. arXiv
Hannaneh Hajishirzi, and Yejin Choi. 2022. Align-
preprintarXiv:1803.05457.
ing to social norms and values in interactive narra-
tives. arXivpreprintarXiv:2205.01975.
Bhavana Dalvi, Oyvind Tafjord, and Peter Clark.
Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, 2022. Towardsteachablereasoningsystems. arXiv
Arthur Szlam, Tim Rocktäschel, and Jason Weston. preprintarXiv:2204.13074.
2021. Howtomotivateyourdragon: Teachinggoal-
drivenagentstospeakandactinfantasyworlds. In MingkaiDeng,JianyuWang,Cheng-PingHsieh,Yihan
Proceedings of the 2021 Conference of the North Wang, Han Guo, Tianmin Shu, Meng Song, Eric P
American Chapter of the Association for Computa- Xing, and Zhiting Hu. 2022. Rlprompt: Optimiz-
tional Linguistics: Human Language Technologies, ing discrete text prompts with reinforcement learn-
pages 807–833, Online. Association for Computa- ing. arXivpreprintarXiv:2205.12548.
tionalLinguistics.
Yuling Gu, Bhavana Dalvi, and Peter Clark. 2022.
Rachit Bansal, Milan Aggarwal, Sumit Bhatia, Ji- DREAM: Improving situational QA by first elab-
vat Neet Kaur, and Balaji Krishnamurthy. 2021. orating the situation. In Proceedings of the 2022
Cose-co:Textconditionedgenerativecommonsense Conference of the North American Chapter of the
contextualizer. Association for Computational Linguistics: Human
Language Technologies, pages 1115–1127, Seattle,
YonatanBisk,RowanZellers,RonanLeBras,Jianfeng United States. Association for Computational Lin-
Gao,andYejinChoi.2020. PIQA:reasoningabout guistics.
physical commonsense in natural language. In The
Thirty-Fourth AAAI Conference on Artificial Intelli- Han Guo, Bowen Tan, Zhengzhong Liu, Eric P Xing,
gence,AAAI2020,TheThirty-SecondInnovativeAp- andZhitingHu.2021. Textgenerationwithefficient
plicationsofArtificialIntelligenceConference,IAAI (soft)q-learning. arXivpreprintarXiv:2106.07704.
2020, The Tenth AAAI Symposium on Educational
AdvancesinArtificialIntelligence,EAAI2020,New
AriHoltzman, JanBuys, LiDu, MaxwellForbes, and
York, NY, USA, February 7-12, 2020, pages 7432–
Yejin Choi. 2020. The curious case of neural text
7439.AAAIPress.
degeneration. In 8th International Conference on
LearningRepresentations,ICLR2020,AddisAbaba,
Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and
Ethiopia,April26-30,2020.OpenReview.net.
Greg Durrett. 2022. Natural language deduction
through search over statement compositions. arXiv
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-
preprintarXiv:2201.06028.
man, Chandra Bhagavatula, Ronan Le Bras, and
Yejin Choi. 2022. Maieutic prompting: Logi-
TomB.Brown,BenjaminMann,NickRyder,Melanie
cally consistent reasoning with recursive explana-
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
tions. arXivpreprintarXiv:2205.11822.
Neelakantan,PranavShyam,GirishSastry,Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Daniel Khashabi, Yeganeh Kordi, and Hannaneh Ha-
Gretchen Krueger, Tom Henighan, Rewon Child,
jishirzi. 2022. Unifiedqa-v2: Stronger general-
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
ization via broader cross-format training. arXiv
Clemens Winter, Christopher Hesse, Mark Chen,
preprintarXiv:2202.12359.
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Amodei.2020. Languagemodelsarefew-shotlearn- Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
ers. InAdvancesinNeuralInformationProcessing nanehHajishirzi.2020. UNIFIEDQA:Crossingfor-
Systems33: AnnualConferenceonNeuralInforma- mat boundaries with a single QA system. In Find-
tion Processing Systems 2020, NeurIPS 2020, De- ings of the Association for Computational Linguis-
cember6-12,2020,virtual. tics: EMNLP 2020, pages 1896–1907, Online. As-
sociationforComputationalLinguistics.
Ting-Yun Chang, Yang Liu, Karthik Gopalakrishnan,
BehnamHedayatnia, PeiZhou, andDilekHakkani- Tushar Khot, Peter Clark, Michal Guerquin, Peter
Tur. 2020. Incorporating commonsense knowledge Jansen, and Ashish Sabharwal. 2020. QASC: A
graphinpretrainedmodelsforsocialcommonsense dataset for question answering via sentence com-
tasks. InProceedingsofDeepLearningInsideOut position. In The Thirty-Fourth AAAI Conference
on Artificial Intelligence, AAAI 2020, The Thirty- ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,
Second Innovative Applications of Artificial Intelli- Long Ouyang, Christina Kim, Christopher Hesse,
genceConference,IAAI2020,TheTenthAAAISym- Shantanu Jain, Vineet Kosaraju, William Saunders,
posium on Educational Advances in Artificial Intel- et al. 2021. Webgpt: Browser-assisted question-
ligence, EAAI 2020, New York, NY, USA, February answering with human feedback. arXiv preprint
7-12,2020,pages8082–8090.AAAIPress. arXiv:2112.09332.
Veronica Latcinnik and Jonathan Berant. 2020. Ex- LongOuyang,JeffWu,XuJiang,DiogoAlmeida,Car-
plaining question answering models through text roll L Wainwright, Pamela Mishkin, Chong Zhang,
generation. arXivpreprintarXiv:2004.05569. SandhiniAgarwal, KatarinaSlama, AlexRay, etal.
2022. Training language models to follow in-
Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-
structions with human feedback. arXiv preprint
angRen.2020. Birdshavefourlegs?! NumerSense:
arXiv:2203.02155.
Probing Numerical Commonsense Knowledge of
Pre-Trained Language Models. In Proceedings of Bhargavi Paranjape, Julian Michael, Marjan
the 2020 Conference on Empirical Methods in Nat- Ghazvininejad, Hannaneh Hajishirzi, and Luke
ural Language Processing (EMNLP), pages 6862– Zettlemoyer. 2021. Prompting contrastive explana-
6868, Online. Association for Computational Lin- tionsforcommonsensereasoningtasks. InFindings
guistics. of the Association for Computational Linguistics:
ACL-IJCNLP 2021, pages 4179–4192, Online.
Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee,
AssociationforComputationalLinguistics.
and Xiang Ren. 2021. RiddleSense: Reasoning
about riddle questions featuring linguistic creativ-
Romain Paulus, Caiming Xiong, and Richard Socher.
ity and commonsense knowledge. In Findings of
2018. Adeepreinforcedmodelforabstractivesum-
theAssociationforComputationalLinguistics:ACL-
marization. In 6th International Conference on
IJCNLP 2021, pages 1504–1515, Online. Associa-
Learning Representations, ICLR 2018, Vancouver,
tionforComputationalLinguistics.
BC, Canada, April 30 - May 3, 2018, Conference
TrackProceedings.OpenReview.net.
JiachengLiu,AlisaLiu,XimingLu,SeanWelleck,Pe-
terWest,RonanLeBras,YejinChoi,andHannaneh
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
Hajishirzi. 2022. Generated knowledge prompting
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
for commonsense reasoning. In Proceedings of the
WeiLi, andPeterJLiu.2019. Exploringthelimits
60thAnnualMeetingoftheAssociationforCompu-
of transfer learning with a unified text-to-text trans-
tationalLinguistics(Volume1: LongPapers),pages
former. arXivpreprintarXiv:1910.10683.
3154–3169, Dublin, Ireland. Association for Com-
putationalLinguistics.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong,andRichardSocher.2019. Explainyourself!
NicholasLourie,RonanLeBras,ChandraBhagavatula,
leveraging language models for commonsense rea-
and Yejin Choi. 2021. Unicorn on rainbow: A uni-
soning. In Proceedings of the 57th Annual Meet-
versalcommonsensereasoningmodelonanewmul-
ing of the Association for Computational Linguis-
titaskbenchmark. InProceedingsoftheAAAICon-
tics, pages 4932–4942, Florence, Italy. Association
ference on Artificial Intelligence, volume 35, pages
forComputationalLinguistics.
13480–13488.
KeisukeSakaguchi,RonanLeBras,ChandraBhagavat-
Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel,
ula, and Yejin Choi. 2021. Winogrande: An adver-
Lianhui Qin, Peter West, Prithviraj Ammanabrolu,
sarialwinogradschemachallengeatscale. Commu-
andYejinChoi.2022. Quark: Controllabletextgen-
nicationsoftheACM,64(9):99–106.
eration with reinforced unlearning. arXiv preprint
arXiv:2205.13636.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
HugoMercierandDanSperber.2017. Theenigmaof Le Bras, and Yejin Choi. 2019. Social IQa: Com-
reason. monsense reasoning about social interactions. In
Proceedings of the 2019 Conference on Empirical
TodorMihaylov,PeterClark,TusharKhot,andAshish Methods in Natural Language Processing and the
Sabharwal.2018. Canasuitofarmorconductelec- 9th International Joint Conference on Natural Lan-
tricity? a new dataset for open book question an- guage Processing (EMNLP-IJCNLP), pages 4463–
swering. InProceedingsofthe2018Conferenceon 4473,HongKong,China.AssociationforComputa-
EmpiricalMethodsinNaturalLanguageProcessing, tionalLinguistics.
pages 2381–2391, Brussels, Belgium. Association
forComputationalLinguistics. JohnSchulman, FilipWolski, PrafullaDhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal
Arindam Mitra, Pratyay Banerjee, Kuntal Kumar Pal, policy optimization algorithms. arXiv preprint
Swaroop Mishra, and Chitta Baral. 2019. How ad- arXiv:1707.06347.
ditional knowledge can improve natural language
commonsense question answering? arXiv preprint Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
arXiv:1909.08855. Wu, MaosongSun, andYangLiu.2016. Minimum
risktrainingforneuralmachinetranslation. InPro-
ceedingsofthe54thAnnualMeetingoftheAssocia-
tionforComputationalLinguistics(Volume1: Long
Papers), pages 1683–1692, Berlin, Germany. Asso-
ciationforComputationalLinguistics.
Vered Shwartz, Peter West, Ronan Le Bras, Chandra
Bhagavatula, and Yejin Choi. 2020. Unsupervised
commonsensequestionansweringwithself-talk. In
Proceedings of the 2020 Conference on Empirical
MethodsinNaturalLanguageProcessing(EMNLP),
pages4615–4629,Online.AssociationforComputa-
tionalLinguistics.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
DarioAmodei,andPaulFChristiano.2020. Learn-
ing to summarize with human feedback. Advances
inNeuralInformationProcessingSystems,33:3008–
3021.
Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter
Clark. 2019. QuaRTz: An open-domain dataset of
qualitativerelationshipquestions. InProceedingsof
the 2019 Conference on Empirical Methods in Nat-
uralLanguageProcessingandthe9thInternational
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 5941–5946, Hong Kong,
China.AssociationforComputationalLinguistics.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. InProceedingsofthe2019Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages4149–4158,Minneapolis,Minnesota.Associ-
ationforComputationalLinguistics.
Wenya Wang, Vivek Srikumar, Hanna Hajishirzi, and
Noah A Smith. 2022. Elaboration-generating com-
monsense question answering at scale. arXiv
preprintarXiv:2209.01232.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chainofthoughtpromptingelicitsreasoninginlarge
languagemodels. arXivpreprintarXiv:2201.11903.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between hu-
man and machine translation. arXiv preprint
arXiv:1609.08144.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can
a machine really finish your sentence? In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4791–
4800,Florence,Italy.AssociationforComputational
Linguistics.
A AdditionalExperimentalDetails B AdditionalAnalysis
A.1 Hyperparameters Table9through12showmoreanalysisofknowl-
SeeTable7.
edgegeneratedbyRAINIER. Table9showsseman-
tically problematic knowledge. Table 10 shows
A.2 Baselines knowledge that express some social value. Ta-
Self-talk. We generate M = 10 knowledge ble11showsknowledgethatareculture-specific.
Table12showsknowledgethathavepotentialethi-
perquestionwithGPT-3-Curie,using10pairsof
calrisks. Allexamplesaretakenfromthevalida-
question-answertemplatesadaptedfromShwartz
tionsetoftherespectivedataset.
et al. (2020). We generate one knowledge from
eachtemplate: first,queryGPT-3withthequestion
C PromptsforGettingSilverKnowledge
template and using nucleus sampling (p = 0.2)
fromGPT-3
toobtainafullquestion;next,queryGPT-3again
withboththefullquestionandthecorresponding SeeTable13through20.
answertemplate,thistimeusingnucleussampling
(p = 0.5), to obtain a full answer sentence. The
answersentencewillbetreatedastheknowledge.
DREAM. We generate M = 10 scene elabora-
tionsperquestionwiththeDREAM(11B)model.
Guetal.(2022)proposes4typesofsceneelabora-
tions: motivation(M),emotion(E),rule-of-thumb
(ROT), and consequence (Con). Each is associ-
atedwithacontrolcodethatguidestheDREAM
model. Wegenerate2or3sceneelaborationsfor
eachtype,makingatotalof10perquestion.
QuestionTemplate AnswerTemplate
Whatisthedefinitionof Thedefinitionof_is
Whatisthemainpurposeof Thepurposeof_isto
Whatisthemainfunctionofa Themainfunctionofa_is
Whatarethepropertiesofa Thepropertiesofa_arethat
Whatisa _is
Whathappenedasaresultof Asaresultof_,
Whatmighthavecaused Thecauseof_was
Whatisapartof Apartof_is
Whatisanexampleof Anexampleof_is
Howwouldyou Onewould_by
Table6: Templatesusedintheself-talkbaseline.
Symbol Value Description
GETTINGSILVERKNOWLEDGEFROMFEW-SHOTGPT-3
M 20 NumberofknowledgestatementstosamplefromGPT-3,perquestion.
p 0.5 ParameterfornucleussamplingfromGPT-3.
L 64 MaxlengthofoutputfromGPT-3.
output
STAGEI:IMITATIONLEARNING
L
input
256 MaxlengthofinputtoRAINIER(i.e.questionpluschoices).
L
output
64 MaxlengthofoutputfromRAINIER(i.e.generatedknowledge).
B 64 Batchsizefortraining.
S 50,000 Totalnumberoftrainingsteps.
η 1×10−5 LearningrateofAdamoptimizer.
STAGEII:REINFORCEMENTLEARNING
α 1.0 WeightofvaluemodellossinPPO.
β 0.2 Weightofentropybonusterminreward.
γ 1.0 Discountfactorforrewards.
λ 0.95 Parameterforadvantageestimation.
ε 0.2 Clippingrangefortheclippedsurrogateobjective.
L
input
256 MaxlengthofinputtoRAINIER(i.e.questionpluschoices).
L
output
32 MaxlengthofoutputfromRAINIER(i.e.generatedknowledge).
τ 0.7 TemperatureforknowledgesamplinginPPOtraining.
E 1M Totalnumberoftrainingepisodes.
B 64 Batchsizefortraining.
S 15,625 Totalnumberoftrainingsteps.
s 4 Interval(insteps)forupdatingthelaggingmodels(policyandvalue).
η 2×10−5 LearningrateofAdamoptimizer(withalinearlearningratedecayschedule).
INFERENCE
M 10 NumberofknowledgestatementstosamplefromRAINIER,perquestion.
p 0.5 ParameterfornucleussamplingfromRAINIER.
L
input
256 MaxlengthofinputtoRAINIER(i.e.questionpluschoices).
L
output
32 MaxlengthofoutputfromRAINIER.
Table7: Hyperparametersettings.
Dataset→ ARC AI2Science
OBQA
Method↓ easy hard elem mid
UQA-large(0.77B) 70.20 69.12 55.85 69.11 64.80
+Few-shotGPT-3-Curie(13B) 68.80 71.05 56.52 70.73 65.60
+RAINIER-large(0.77B)[ours] 69.60 67.72 55.18 68.29 63.20
Table8:Resultsontheother3seendatasets.AllexperimentsuseUnifiedQA-largeastheQAmodel,andoptionally
usesknowledgefromoneoftheknowledgegenerationmodels. Onthesedatasets,RAINIER-generatedknowledge
doesnotgiveanimprovementoverthevanillaQAbaseline.
Task Question/Knowledge Problemwiththeknowledge
RS Iamafruit.Iamtastyandprovidelotsofenergy.You Ambiguous
canalsofindmeinacalendar.WhatamI? Theknowledgedoesnotspecifywhich
(A)shop(B)choppers(C)date(D)european(E)movie year, so it is not possible to verify its
Thefruitoftheyearisthedate. factuality.
QuaRTz Carla is pushing two carts down a street. One cart is Under-specified
heavy,theotherislight.Theheavycart,comparedtothe Missing the control factor that the
lightone,willaccelerate forcesexertedonthecartsarethesame.
(A)slowly(B)quickly Thoughthisflawalsoexistsintheques-
Theheavycartwillaccelerateslowerthanthelight tionitself.
one.
RS Whathasaheadatnightbutnotinthemorning? Over-specified
(A)sleeping(B)pillow(C)neck(D)shampoo(E)beer Itwouldbenormaltosay"animalshave
Sleepinganimalshaveaheadatnight. ahead".Itisweirdtoqualifywith"sleep-
ing"and"atnight".
WG BecauseWilliamdevelopedasorekneeduringhisrace Over-confident
againstagainstKyle,_wontherace. Williammightstillbeabletofinishthe
(A)William(B)Kyle racewithasoreknee.
Williamwasnotabletoruntherace.
RS whatisthebeginingofenternitytheendoflifetheend Tautological
oftimeandtheendtoeveryrace Thisknowledgedoesnotgiveanymean-
(A)e(B)quick(C)finality(D)fix(E)habit ingfulinformation.
Theendofeveryraceistheendofeveryrace.
QuaRTz Sharonisconductinganexperimentonvalenceelectrons Notapplicable
andsoondiscoversthatwhentheyareclosertothenu- Thisknowledgeimplicitlycomparesthe
cleus,theyare_____easilyremovedfromtheatom. removabilityofvalencevs.non-valence
(A)more(B)less electrons.However,thequestionneedsa
Valenceelectronsaremorepronetobeingremoved comparisonofvalenceelectronsinatoms
fromtheatom. ofdifferentsizes,sotheknowledgecan-
notbeappliedtoansweringthisquestion.
Table9: ExamplesofknowledgegeneratedbyRAINIERwhicharesemanticallyproblematic.
Task Question/Knowledge Problemwiththeknowledge
SIQA Rileybrokeloosefromthehouse.Hethoughthewould Socialvalue
nevergetoutofthere.WhydidRileydothis? Thisknowledgeisagenerallytruestate-
(A)Stayinthehouselonger(B)thinkabouthislife(C) ment,sowelabeleditasfactual.
gohomeforawhile
Breakingoutofabadhabitisusuallyabadidea.
SIQA Tracyheardafaintbuzzingnoiseandimmediatelyran Socialvalue
forherlife.HowwouldyoudescribeTracy? Itishardtodecidewhetherthisknowl-
(A)scaredofbees(B)sad(C)notphasedbybees edgeshouldbeconsideredfactualornot.
Oneshouldnotbescaredofbees.
SIQA RemygaveSkylar’sNetflixaccountpasswordtooneof Socialvalue
Remy’sotherfriends.HowwouldSkylarfeelasaresult? Itisambiguouswhethercanmeansitis
(A)likeabadfriend(B)excited(C)used possiblethat...,oroughtto. Ifitisthe
Afriendcanbeusedbyafriend. latter,thentheknowledgeispromoting
someproblematicsocialvalue.
SIQA Rileywasthebestoffriendswiththeboywithcancer. Socialvalue
WhatwillRileywanttodonext? Itisgenerallyakindthingtovisitasick
(A) visit the hospital (B) shun the friend (C) become friend. However, itisconceivablethat
friendswiththeboywithcancertoo thefriendneedstorecoverinpeaceor
Oneshouldvisittheirsickfriend. hassomeinfectiousdisease,whichren-
dersavisitinappropriate.
SIQA CarsontriedtofightRobinlastnightbecauseRobinhurt Socialvalue
Carsonalot.WhatwillCarsonwanttodonext? This knowledge is generally accepted.
(A)apologize(B)donothing(C)hurtRobin However,thereareextenuatingcircum-
Oneshouldapologizewhentheyhurtsomeone. stanceswherehurtingsomeonedoesnot
needanapology(e.g. hurtingaviolent
criminaltoprotectoneself).
SIQA BaileytoldAlextosendthepdfbecausetheydidn’twant Socialvalue
todoitthemselves.HowwouldAlexfeelasaresult? This knowledge is generally accepted,
(A)lazyaboutwork(B)happy(C)angry butitisnotagoodfittothequestion’s
Oneshouldbewillingtohelpothers. context. It is normal to be emotional
whenbeingorderedtodosomethingon
other’sbehalf.
SIQA Kendallwrappedabandagearoundmyneckaftergetting Socialvalue
injuredinafight.WhatwillKendallwanttodonext? This knowledge is generally accepted,
(A)harmthem(B)punishthem(C)protectthem andappropriatetothequestion’scontext.
Oneshouldhelpothersinneed.
Table10: ExamplesofknowledgegeneratedbyRAINIERthatexpresssomesocialvalue.
Task Question/Knowledge Problemwiththeknowledge
SIQA RemymadehaygettinghomefromschoolonFridaythe Culture-specific
13th.WhydidRemydothis? This knowledge largely applies within
(A)gotoschoolbeforethis(B)hadheardthatonFriday western,especiallyChristian,culture.
the13th,Godwouldblessyouifyoujustwenthomeand
hid(C)hadheardthatonFridaythe13th,thatpeople
prankedotherpeoplereallybad
PeoplearemorelikelytobeprankedonFridaythe
13th.
WG Lindsey like to read graphic novels but Natalie liked Culture-specific
classicliteraturetoread._boughtthenewFrankMiller Thisknowledgecouldbecorrectornot
comicatthebookstore. dependingonthecultureorgeographical
(A)Lindsey(B)Natalie location.
Graphicnovelsarepopularamongteens.
CSQA Wherewouldapersonlivethatisn’tinthemetroarea Culture-specific
butstillhasgoodschools? Thisknowledgecouldbecorrectornot
(A)woods(B)bed(C)suburbs(D)rural(E)neighbor- dependingonthecultureorgeographical
hood location,anditisnotevenalwaystrue
A person who lives in the suburbs is likely to have withintheU.S.
goodschools.
NS mostpeopleintheu.s.works<mask>consecutivehours Culture-specific
inabusinessday. Thisknowledgeisanover-generalization.
(A)no(B)one(C)two(D)three(E)four(F)five(G)six Insomeothercultures,mostpeoplework
(H)seven(I)eight(J)nine(K)ten moretheneighthours,orless.
Mostpeopleworkeighthoursinaday.
CSQA Whatwillgodneverdoaccordingtoreligion? Culture-specific
(A) anything (B) judge people (C) work miracles (D) Thisisatopicspecifictothereligion.
judgemen(E)everywhere
Godisajudgeofpeople.
Table11: ExamplesofknowledgegeneratedbyRAINIERthatareculture-specific.
Task Question/Knowledge Problemwiththeknowledge
RS Awomanscreams. Yougointoashopandfindaman Ethicalrisk
lyingdeadonthefloor. Therearethreepeopleinthe Thisknowledgemightimpressbiasto-
room.Apostman,amilkmanandadoctor.Whokilled wardsacertainprofession.However,the
him? questionitselfshouldprobablytakethe
(A)doctor(B)policestation(C)liars(D)radio(E)water blame,becauseunfortunately,"doctor"is
fountain theonlyvalidchoicegiventhequestion’s
Thedoctoristheonewhokilledtheman. context.
PIQA crossingout Ethicalrisk
(A)uglypeoplefromthebacheloriscool(B)uglypeople Thisknowledgemightimpressbiasto-
fromplanetearthiscool wardsagroupwithcertainphysicalchar-
Crossingoutuglypeoplefromplanetearthiscool. acteristic.
CSQA With the card slot lit up he knew how to get started Ethicalrisk
findinghisbalancewithwhat? Thisknowledgemightadvocateforgam-
(A)slotmachine(B)ticketmachine(C)bankmachine bling.
(D)telephone(E)automatedteller
Aslotmachineisamachinethattakescardsanduses
themtomakemoney.
Table12: ExamplesofknowledgegeneratedbyRAINIERthathavepotentialethicalrisks.
Task Prompt
OBQA Input:Thesunisresponsiblefor\n(A)puppieslearningnewtricks(B)childrengrowingupand
gettingold(C)flowerswiltinginavase(D)plantssprouting,bloomingandwilting
Knowledge:Naturallightprovidesenergyforphotosynthesis.
Input:Poisoncausesharmtowhichofthefollowing?\n(A)aTree(B)arobot(C)ahouse(D)a
car
Knowledge:Livingorganismsaresusceptibletopoisonousmatter.
Input:Asacarapproachesyouinthenight\n(A)theheadlightsbecomemoreintense(B)the
headlightsrecedeintothedark(C)theheadlightsremainataconstant(D)theheadlightsturnoff
Knowledge:Theintensityoflightincreaseswhenobservedfromashorterdistance.
Input:WhentheweatherchangesasitdoesfromChristmastoEaster,\n(A)theairmaychill
(B)thegroundmayfreeze(C)theplantsmaydie(D)thegroundmaywarm
Knowledge:ChristmasisinwinterandEasterisinspring.
Input: Using mirrors to focus collected light from heavenly bodies allows \n (A) detailed
observation(B)foregoneconclusions(C)radiationexperiments(D)celestialmusic
Knowledge:Telescopesusemirrorstofocuslightfromthestars.
Input:{question}
Knowledge:
Table13: PromptforOpenBookQA.
Task Prompt
ARC Input: Georgewantstowarmhishandsquicklybyrubbingthem. Whichskinsurfacewill
producethemostheat?\n(A)drypalms(B)wetpalms(C)palmscoveredwithoil(D)palms
coveredwithlotion
Knowledge:Rubbinghandsproducesheatbecauseoffriction.
Input:Whichofthefollowingisanexampleofaphysicalchange?\n(A)lightingamatch(B)
breakingaglass(C)burningofgasoline(D)rustingofiron
Knowledge: Physicalchangesmustnotinvolvechemicalchangessuchascombustionand
rusting.
Input:OnEarth,watercanbeasolid,aliquid,oragas.Whichenergysourcehasthegreatest
influenceonthestateofmatterofwater?\n(A)thesun(B)thewind(C)oceancurrents(D)the
metalcore
Knowledge:Earth’swatercirculationismostlydrivenbyheatradiatedfromthesun.
Input:Whatdocellsbreakdowntoproduceenergy?\n(A)food(B)water(C)chlorophyll(D)
carbondioxide
Knowledge:Foodcontaincalories.
Input:WhatcharacteristicofDNAresultsincelldifferentiationindevelopingembryos?\n(A)
whichgenesarepresent(B)howmanycopiesofeachgenearepresent(C)whichgenesare
active(D)whatproteinisproducedbyagene
Knowledge:Celldifferentiationiscausedbyselectiveexpressionofgenes.
Input:{question}
Knowledge:
Table14: PromptforARC.
Task Prompt
AI2Sci Input:Whichisanonrenewablenaturalresourcethatisusedtomakeelectricalenergy?\n(A)
coal(B)wind(C)water(D)thermal
Knowledge:Fossilfuelisnonrenewablenaturalresource.
Input: Whichadaptationwillwarnpredatorsnottoeatananimal? \n(A)brightcolors(B)
bulgingeyes(C)geometricshapes(D)poisonoussecretions
Knowledge:Brightcolorsinanimalsareusuallyasignofbeingpoisonous.
Input:AnItalianscientistnamedAlessandroVoltainventedtheVoltaicpilein1800.Itwasable
toproduceasteadyelectricalcurrent.Basedonthisdescription,whatisthemodernequivalent
oftheVoltaicpile?\n(A)awire(B)abattery(C)aresistor(D)alightbulb
Knowledge:Batteriescanproducesteadyelectricalcurrent.
Input: WhatisthebestmeasuretouseindeterminingtheeffectofsolarenergyonEarth’s
atmosphere?\n(A)thetemperatureoftheair(B)thetemperatureoftheocean(C)thedensityof
cloudsinthesky(D)theamountofrainfallonarainyday
Knowledge:SolarradiationconvertstoheatinEarth’satmosphere.
Input: Whichnongaseouscompoundcanbemadefromtwoelementsthataregasesatroom
temperature?\n(A)water(B)tablesalt(C)ironoxide(D)carbondioxide
Knowledge:WatermoleculesaremadeofHydrogenandOxygen.
Input:{question}
Knowledge:
Table15: PromptforAI2Science.
Task Prompt
CSQA Input: GoogleMapsandotherhighwayandstreetGPSserviceshavereplacedwhat? \n(A)
unitedstates(B)mexico(C)countryside(D)atlas(E)oceans
Knowledge:Electronicmapsarethemodernversionofpaperatlas.
Input:Thefoxwalkedfromthecityintotheforest,whatwasitlookingfor?\n(A)prettyflowers.
(B)henhouse(C)naturalhabitat(D)storybook(E)denseforest
Knowledge:Naturalhabitatsareusuallyawayfromcities.
Input:Youcansharefileswithsomeoneifyouhaveaconnectiontoawhat?\n(A)freeway(B)
radio(C)wires(D)computernetwork(E)electricalcircuit
Knowledge:FilescanbesharedovertheInternet.
Input:Toomanypeoplewantexoticsnakes.Thedemandisdrivingwhattocarrythem?\n(A)
ditch(B)shop(C)northamerica(D)petshops(E)outdoors
Knowledge:Somepeopleraisesnakesaspets.
Input:Thebodyguardwasgoodathisduties,hemadethepersonwhohiredhimwhat?\n(A)
betterjob(B)irritated(C)feelsafe(D)savemoney(E)headache
Knowledge:Thejobofbodyguardsistoensurethesafetyandsecurityoftheemployer.
Input:{question}
Knowledge:
Table16: PromptforCommonsenseQA.
Task Prompt
QASC Input:Whattypeofwaterformationisformedbyclouds?\n(A)pearls(B)streams(C)shells
(D)diamonds(E)rain(F)beads(G)cooled(H)liquid
Knowledge:Cloudsaremadeofwatervapor.
Input:Whatcanpreventfoodspoilage?\n(A)prolactinrelease(B)onecelledorganisms(C)
hydratingfood(D)cleaningfood(E)airingoutfood(F)Electricgenerators(G)ahydraulic
system(H)dehydratingfood
Knowledge:Dehydratingfoodisusedforpreservingfood.
Input: Theprocessbywhichgenesarepassedis\n(A)Mostplants(B)flowofelectrons(C)
mitosis(D)Summer(E)respiration(F)mutation(G)mechanical(H)reproduction
Knowledge:Genesarepassedfromparenttooffspring.
Input:Thestomachdoeswhatinthebody?\n(A)decreasesitsbodilywater(B)killsallgerms
(C)breaksfoodintonutrients(D)storesbile(E)heatisproduced(F)extractswaterfromfood
(G)getchemicalreactionsstarted(H)causepeopletobecomesick.
Knowledge:Thestomachispartofthedigestivesystem.
Input:Whatcancauserockstobreakdown?\n(A)WindBarriers(B)ProtectiveBarriers(C)
StoneSealers(D)wind(E)mines(F)Water(G)erosion(H)Gravity
Knowledge:Mechanicalweatheringiswhenrocksarebrokendownbymechanicalmeans.
Input:{question}
Knowledge:
Table17: PromptforQASC.
Task Prompt
PIQA Input:howdoyoufloodaroom?\n(A)fillitwithobjects.(B)fillitwithwater.
Knowledge:Toomuchwatercancauseflooding.
Input:HowcanIgetoilstainsoutofmydriveway?\n(A)Douseeachstainwithacouplecans
ofbeer.(B)Douseeachstainwithacouplecansofsoda.
Knowledge:Sodiumcarbonatesolutioncanwashawayoilstains.
Input:Sootheapainfulsunburn.\n(A)Waituntilbrewedteabagiscool,thenapplyonburn.
(B)Waituntilbrewedteabagishot,thenapplyonburn.
Knowledge:Sunburncanbealleviatedbyapplyingcoldmaterial.
Input:WhatcanIuseforfuelinanalcoholstove?\n(A)Useacetone.(B)Usevinegar.
Knowledge:Acetoneisflammable,whilevinegarisnot.
Input:HowcanIcutthehandlesofmetalcutlery?\n(A)Useahandsawtocutthehandles.(B)
Useahanddrilltocutthehandles.
Knowledge:Ahandsawisusedformakingcuts;ahanddrillisusedformakingholes.
Input:{question}
Knowledge:
Table18: PromptforPhysicalIQA.
Task Prompt
SIQA Input:WhatwillQuinnwanttodonext?\n(A)Eatmessysnacks(B)helpoutafriend(C)Pick
upthedirtyclothes\nQuinnwantedtohelpmecleanmyroomupbecauseitwassomessy.
Knowledge:Amessyroomlikelycontainsdirtyclothes.
Input:WhatwillAubreywanttodonext?\n(A)helpAubreygobackhome(B)keeponpartying
withoutthemom(C)goingonwiththemom\nSasha’smompassedoutinthemiddleofthe
party.AubreytookSasha’smomtothehospital.
Knowledge:Oneshouldattendtotheirsickfamilymember.
Input:HowwouldJanfeelafterwards?\n(A)scaredoflosingthecat(B)normal(C)relieved
forfixingtheproblem\nTheircatkepttryingtoescapeoutofthewindow,soJanplacedan
obstacleintheway.
Knowledge:Oneusuallyhaspositiveemotionsaftersolvingaproblem.
Input:HowwouldSydneyfeelafterwards?\n(A)affected(B)liketheyreleasedtheirtension
(C)worse\nSydneyhadsomuchpentupemotion,theyburstintotearsatwork.
Knowledge:Cryingcanbeacatharsis.
Input:WhatdoesSydneyneedtodobeforethis?\n(A)bebadatherjob(B)doagoodjob(C)
belazy\nSydneygotaraiseandanewpromotion.
Knowledge:Payraiseandpromotionareusuallyresultsofgoodjobperformance.
Input:{question}
Knowledge:
Table19: PromptforSocialIQA.
Task Prompt
WG Input:TheGPSandmaphelpedmenavigatehome.Igotlostwhenthe_gotturnedoff.\n(A)
GPS(B)map
Knowledge:AGPSdeviceiselectronic,whileamapispaper-based.
Input:Ipickedupabagofpeanutsandraisinsforasnack.IwantedasweetersnackoutsoIate
the_fornow.\n(A)raisins(B)peanuts
Knowledge:Peanutscontainalotoffat.Raisinscontainalotofsugar.
Input:Thegeeseprefertonestinthefieldsratherthantheforestsbecauseinthe_predatorsare
morehidden.\n(A)fields(B)forests
Knowledge:Therearemoretreesintheforeststhaninthefields.
Input: Once in Poland, Dennis enjoyed the trip more than Jason because _ had a shallow
understandingofthePolishlanguage.\n(A)Dennis(B)Jason
Knowledge:Thosewhoknowthenativelanguagewouldenjoythetripbetter.
Input:AdamputhandwashonlyclothesinthewasherbutAaronwashedthembyhandas_was
lazy.\n(A)Adam(B)Aaron
Knowledge:Washingclotheswithwashertakeslesseffortthanbyhand.
Input:{question}
Knowledge:
Table20: PromptforWinogrande.

Improved Relation Extraction with
Feature-Rich Compositional Embedding Models
MatthewR.Gormley1⇤ and MoYu2⇤and MarkDredze1
1HumanLanguageTechnologyCenterofExcellence
CenterforLanguageandSpeechProcessing
JohnsHopkinsUniversity,Baltimore,MD,21218
2MachineIntelligenceandTranslationLab
HarbinInstituteofTechnology,Harbin,China
gflfof@gmail.com, {mrg, mdredze}@cs.jhu.edu
Abstract its lemma, its morphological features) with as-
pectsofaword’slinguisticcontext(e.g. whetherit
Compositional embedding models build
liesbetweentwoentitiesoronadependencypath
a representation (or embedding) for a
between them). While these help learning, they
linguistic structure based on its compo-
makegeneralizationtounseenwordsdifficult. An
nent word embeddings. We propose
alternative approach to capturing lexical informa-
a Feature-rich Compositional Embedding tion relies on continuous word embeddings2 as
Model (FCM) for relation extraction that
representative of words but generalizable to new
isexpressive,generalizestonewdomains,
words. Embedding features have improved many
and is easy-to-implement. The key idea
tasks,includingNER,chunking,dependencypars-
is to combine both (unlexicalized) hand-
ing, semantic role labeling, and relation extrac-
crafted features with learned word em-
tion (Miller et al., 2004; Turian et al., 2010; Koo
beddings. The model is able to directly
et al., 2008; Roth and Woodsend, 2014; Sun et
tackle the difficulties met by traditional
al.,2011;PlankandMoschitti,2013;Nguyenand
compositional embeddings models, such
Grishman, 2014). Embeddings can capture lexi-
ashandlingarbitrarytypesofsentencean-
calinformation, butalonetheyareinsufficient: in
notations and utilizing global information
state-of-the-art systems, they are used alongside
for composition. We test the proposed
featuresofthebroaderlinguisticcontext.
model on two relation extraction tasks,
In this paper, we introduce a compositional
and demonstrate that our model outper-
modelthatcombinesunlexicalizedlinguisticcon-
formsbothpreviouscompositionalmodels
text and word embeddings for relation extraction,
and traditional feature rich models on the
a task in which contextual feature construction
ACE2005relationextractiontask,andthe
plays a major role in generalizing to unseen data.
SemEval 2010 relation classification task.
Our model allows for the composition of embed-
The combination of our model and a log-
dings with arbitrary linguistic structure, as ex-
linearclassifierwithhand-craftedfeatures
pressed by hand crafted features. In the follow-
givesstate-of-the-artresults. Wemadeour
ingsections,webeginwithapreciseconstruction
implementationavailableforgeneraluse1.
of compositional embeddings using word embed-
dings in conjunction with unlexicalized features.
1 Introduction
Various feature sets used in prior work (Turian et
Two common NLP feature types are lexical al., 2010; Nguyen and Grishman, 2014; Hermann
properties of words and unlexicalized linguis- et al., 2014; Roth and Woodsend, 2014) are cap-
tic/structural interactions between words. Prior
work on relation extraction has extensively stud- 2Such embeddings have a long history in NLP, in-
cluding term-document frequency matrices and their low-
iedhowtodesignsuchfeaturesbycombiningdis-
dimensional counterparts obtained by linear algebra tools
cretelexicalproperties(e.g. theidentityofaword, (LSA,PCA,CCA,NNMF),Brownclusters,randomprojec-
tions and vector space models. Recently, neural networks /
⇤⇤GormleyandYucontributedequally. deeplearninghaveprovidedseveralpopularmethodsforob-
1https://github.com/mgormley/pacaya tainingsuchembeddings.
1774
Proceedingsofthe2015ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1774–1784,
Lisbon,Portugal,17-21September2015.(cid:13)c2015AssociationforComputationalLinguistics.
Class M M SentenceSnippet
1 2
(1)ART(M ,M ) aman ataxicab Amandrivingwhatappearedtobeataxicab
1 2
(2)PART-WHOLE(M ,M ) thesouthernsuburbs Baghdad directionofthesouthernsuburbsofBaghdad
1 2
(3)PHYSICAL(M ,M ) theunitedstates 284people intheunitedstates,284peopledied
2 1
Table1: ExamplesfromACE2005.In(1)theword“driving”isastrongindicatoroftherelationART3betweenM andM .
1 2
Afeaturethatdependsontheembeddingforthiscontextwordcouldgeneralizetootherlexicalindicatorsofthesamerelation
(e.g. “operating”)thatdon’tappearwithART duringtraining. Butlexicalinformationaloneisinsufficient;relationextraction
requirestheidentificationoflexicalroles: whereawordappearsstructurallyinthesentence. In(2), theword“of”between
“suburbs”and“Baghdad”suggeststhatthefirstentityispartofthesecond,yettheearlieroccurrenceafter“direction”isofno
significancetotherelation.Evenfinerinformationcanbeexpressedbyaword’sroleonthedependencypathbetweenentities.
In(3)wecandistinguishtheword“died”fromotherirrelevantwordsthatdon’tappearbetweentheentities.
turedasspecialcasesofthisconstruction. Adding et al. (2015), Yu and Dredze (2015) and Yu et al.
thesecompositionalembeddingsdirectlytoastan- (2015). Additionally, we have extended FCM to
dard log-linear model yields a special case of our incorporate a low-rank embedding of the features
full model. We then treat the word embeddings (Yu et al., 2015), which focuses on fine-grained
asparametersgivingrisetoourpowerful,efficient, relation extraction for ACE and ERE. This paper
and easy-to-implement log-bilinear model. The obtains better results than the low-rank extension
model capitalizes on arbitrary types of linguistic onACEcoarse-grainedrelationextraction.
annotations by better utilizing features associated
withsubstructuresofthoseannotations, including 2 RelationExtraction
global information. We choose features to pro-
Inrelationextractionwearegivenasentenceasin-
motedifferentpropertiesandtodistinguishdiffer-
putwiththegoalofidentifying,forallpairsofen-
entfunctionsoftheinputwords.
tity mentions, what relation exists between them,
The full model involves three stages. First, it
if any. For each pair of entity mentions in a sen-
decomposestheannotatedsentenceintosubstruc-
tence S, we construct an instance (y,x), where
tures (i.e. a word and associated annotations).
x = (M ,M ,S,A). S = {w ,w ,...,w } is
Second, it extracts features for each substructure 1 2 1 2 n
a sentence of length n that expresses a relation
(word), and combines them with the word’s em-
of type y between two entity mentions M and
beddingtoformasubstructureembedding. Third, 1
M ,whereM andM aresequencesofwordsin
we sum over substructure embeddings to form a 2 1 2
S. A is the associated annotations of sentence S,
composed annotated sentence embedding, which
such as part-of-speech tags, a dependency parse,
isusedbyafinalsoftmaxlayertopredicttheout-
and named entities. We consider directed rela-
putlabel(relation).
tions: for a relation type Rel, y=Rel(M ,M )
Theresultisastate-of-the-artrelationextractor 1 2
and y0=Rel(M ,M ) are different relations. Ta-
forunseendomainsfromACE2005(Walkeretal., 2 1
ble1showsACE2005relations, andhasastrong
2006) and the relation classification dataset from
label bias towards negative examples. We also
SemEval-2010Task8(Hendrickxetal.,2010).
consider the task of relation classification (Se-
Contributions Thispapermakesseveralcontri- mEval), where the number of negative examples
butions,including: isartificiallyreduced.
1. We introduce the FCM, a new compositional
embeddingmodelforrelationextraction. Embedding Models Word embeddings and
2. We obtain the best reported results on ACE- compositional embedding models have been suc-
2005forcoarse-grainedrelationextractionin cessfullyappliedtoarangeofNLPtasks,however
thecross-domainsetting, bycombining FCM theapplicationsoftheseembeddingmodelstore-
withalog-linearmodel. lation extraction are still limited. Prior work on
3. We obtain results on on SemEval-2010 Task relationclassification(e.g. SemEval2010Task8)
8competitivewiththebestreportedresults. has focused on short sentences with at most one
Note that other work has already been published relation per sentence (Socher et al., 2012; Zeng
that builds on the FCM, such as Hashimoto et al. et al., 2014). For relation extraction, where neg-
(2015),NguyenandGrishman(2015),dosSantos ative examples abound, prior work has assumed
that only the named entity boundaries and not
3InACE2005,ART referstoarelationbetweenaperson
their types were available (Plank and Moschitti,
andanartifact;suchasauser,owner,inventor,ormanufac-
turerrelationship 2013; Nguyen et al., 2015). Other work has as-
1775
sumed that the order of two entities in a relation a special case of our full model presented in the
aregivenwhiletherelationtypeitselfisunknown nextsubsection.
(NguyenandGrishman,2014;NguyenandGrish- An annotated sentence is first decomposed into
man,2015). Thestandardrelationextractiontask, substructures. The type of substructures can vary
as adopted by ACE 2005 (Walker et al., 2006), by task; for relation extraction we consider one
useslongsentencescontainingmultiplenameden- substructure per word5. For each substructure in
titieswithknowntypes4 andunknownrelationdi- the sentence we have a hand-crafted feature vec-
rections. Wearethefirsttoapplyneurallanguage tor f and a dense embedding vector e . We
w w
i i
modelembeddingstothistask. represent each substructure as the outer product
⌦betweenthesetwovectorstoproduceamatrix,
Motivation and Examples Whether a word is
herein called a substructure embedding: h =
w
indicative of a relation depends on multiple prop- i
f ⌦e . Thefeaturesf arebasedonthelocal
w w w
erties, which may relate to its context within the i i i
context in S and annotations in A, which can in-
sentence. For example, whether the word is in-
cludeglobalinformationabouttheannotatedsen-
between the entities, on the dependency path be-
tence. These features allow the model to pro-
tween them, or to their left or right may provide
motedifferentpropertiesandtodistinguishdiffer-
additional complementary information. Illustra-
ent functions of the words. Feature engineering
tive examples are given in Table 1 and provide
can be task specific, as relevant annotations can
the motivation for our model. In the next section,
change with regards to each task. In this work
we will show how we develop informative repre-
we utilize unlexicalized binary features common
sentationscapturingboththesemanticinformation
in relation extraction. Figure 1 depicts the con-
in word embeddings and the contextual informa-
structionofasentence’ssubstructureembeddings.
tion expressing a word’s role relative to the entity
We further sum over the substructure embed-
mentions. We are the first to incorporate all of
dingstoformanannotatedsentenceembedding:
this information at once. The closest work is that
of Nguyen and Grishman (2014), who use a log- Xn
e = f ⌦e (1)
linear model for relation extraction with embed- x w i w i
dings as features for only the entity heads. Such i=1
embedding features are insensitive to the broader Whenboththehand-craftedfeaturesandwordem-
contextual information and, as we show, are not beddings are treated as inputs, as has previously
sufficienttoelicittheword’sroleinarelation. been the case in relation extraction, this anno-
tated sentence embedding can be used directly as
3 AFeature-richCompositional the features of a log-linear model. In fact, we
EmbeddingModelforRelations find that the feature sets used in prior work for
many other NLP tasks are special cases of this
We propose a general framework to construct an
simple construction (Turian et al., 2010; Nguyen
embedding of a sentence with annotations on its
and Grishman, 2014; Hermann et al., 2014; Roth
component words. While we focus on the rela-
and Woodsend, 2014). This highlights an im-
tion extraction task, the framework applies to any
portant connection: when the word embeddings
taskthatbenefitsfrombothembeddingsandtypi-
areconstant,ourconstructionsofsubstructureand
calhand-engineeredlexicalfeatures.
annotated sentence embeddings are just specific
forms of polynomial (specifically quadratic) fea-
3.1 CombiningFeatureswithEmbeddings
turecombination—hencetheircommonalityinthe
Webeginbydescribingaprecisemethodforcon- literature. Our experimental results suggest that
structing substructure embeddings and annotated suchaconstructionismorepowerfulthandirectly
sentence embeddings from existing (usually un- includingembeddingsintothemodel.
lexicalized) features and embeddings. Note that
3.2 TheLog-BilinearModel
these embeddings can be included directly in a
log-linear model as features—doing so results in Our full log-bilinear model first forms the sub-
structureandannotatedsentenceembeddingsfrom
4Since the focus of this paper is relation extraction, we
adopt the evaluation setting of prior work which uses gold 5Weusewordsassubstructuresforrelationextraction,but
namedentitiestobetterfacilitatecomparison. usethegeneralterminologytomaintainmodelgenerality.
1776
162 Classifier 162Features Classifier Features F1 F1
163 SVM [] 163POS, prefixes,SmVoMrph[]ological, WordNet, dPepOeSn,dpernecfiyxpeas,rsme,orph8o2l.o2gical, WordNet, dependency parse, 82.2
(Best in SemEval2010) Levin classed,(PBreosBtainnkS,eFmraEmvaelN20et1,0N)omLLexev-Pinlucsl,assed, ProBank, FrameNet, NomLex-Plus,
164 164
Google n-gram, paraphrases, TextRunner Google n-gram, paraphrases, TextRunner
165 165
word embeddings, syntactic parse word embeddings, sy7n4t.a8ctic parse 74.8
166 RNN 166 RNN
word embeddings, syntactic parse, POS, NwEoRr,dWemorbdeNdedtings, sy7n7t.a6ctic parse, POS, NER, WordNet 77.6
167 167
word embeddings, syntactic parse word embeddings, sy7n9t.a1ctic parse 79.1
MVRNN MVRNN
168 168word embedding, syntactic parse, POS, NEwRo,rdWeomrdbNedetding, syn8t2a.c4tic parse, POS, NER, WordNet 82.4
169 FCM (fixed-embedding) 169word embeddinFgCsM, d(efipxeendd-eenmcybepdadrsineg, )WorwdNoredt embeddings, de8p2e.n0dency parse, WordNet 82.0
FCM (fine-tuning) word embeddinFgCsM, d(efipneen-dteunncinygp)arse, WorwdNoredt embeddings, de8p2e.n3dency parse, WordNet 82.3
170 170
FCM + linear word embeddinFgCsM, d+epleinnedaerncy parse, WorwdNoredt embeddings, dependency parse, WordNet
171 171
172 172
Table 2: Feature sets used in . Table 2: Feature sets used in .
FCM FCM
173 173
174 174
References References
175 175
bc bc cts cts wl wl
176 176
[1] Yoshua Bengio, Holger Schwenk, Jean[-1S]e´bYaosstiheunaSBeenne´gcaiol,,FHroe´ldgeerricSMchowrienn,ka,nJdeaJne-aSne´-LbausctiGenauSveanien´c.aNl,eFurre´adleric Morin, and Jean-Luc Gauvain. Neural
Model Model P R P F1 R P F1 R P F1 R P F1 R P F1 R F1
177 probabilistic language mod1e7ls7. In InnovatipornosbianbMiliastcihcilnaengLueaagrneimngo,dpealsg.esIn1I3n7n–o1v8a6t.ioSnpsriinngMera,c2h0i0n6e.Learning, pages 137–186. Springer, 2006.
HeadEmb HeadEmb
178 178
[2] Ronan Collobert and Jason Weston. A[2u]niRfioednaanrcChoitleloctbuerret faonrdnJaatsuornalWlaensgtuoang.eApruonciefisesdinagr:chDiteeecptunreeufroarl natural language processing: Deep neural
CNN (wsize=C1)N+Nlo(cwaslizfeea=t1u)re+s local DfeeavtuMreRs R Test MRDRev MRR Test MRR
179 networks with multitask le1a7r9ning. In IntnerentwatoiorknsalwciothnfmeruelnticteasoknleMarancihnign.e lIenarInntienrgn,aptiaogneasl1c6o0n–fe1r6e7n.ce on Machine learning, pages 160–167.
Model FiCnNe-Ntu(nwinsgize=C3s)uN+pNelor(Mvcwiaossloidzfneeela=t3u)r1e+,s0l0o0caFlifn1e0ea-,t0tuu0ren0sing100,s0u0p0erv1is,o0n00 11,00,00000 101,0000,0000100,000 1,000 10,000 100,000
ACM, 2008. ACM, 2008.
180 180
-local only lo-cal only 46.95 35.-29 30.69 - 52.63 4461.9.159 353.279.32 30.69 52.63 41.19 37.32
FCT FCT
181 [3] Tomas Mikolov, Ilya Sutsk1ev8e1r, Kai C[h3e]n,ToGmreagsCMoSirkrUoadlMoov,,aInlydaJSefuftrsekyevDeer,anK.aDi Cishtreinb,uGterdegreCproeSrsrUeandMtoa,tiaonnds Jeffrey Dean. Distributed representations
182
of words and phrases and th 1e 8i 2r
compositioonfalwityo.rdasrXanivdpprherparsienstaanrdXitvh:e1i3Fr1Cc0oT.Y m4g 5pl 4oo 6sb i,ta 2il o0n1a3lF.itCy.TP aPgrXlDoiBbvaplreprin5
t
0 ar.8 X1 iv:16 303 1. 066 .Y. 498 51 464
,
22 0.33 19 326..09.26P499P.D925B472.2.35396.4514405.98..0193124.4535664.8.1441.122.3783342.44.9512.95425.7738.12.37741.493556.0.11631.7741.2336.16
RNN (d=50) Yglobal (BrowPnPg)lDRoNBbNal((dB=r5o04w)5n.6)7 633.01Y.586 39.52876.30.51P458P.D665B349.8.56482.4453459.86..2763566.4736023.8.5446.564.9053267.45.0745.95465.0425.98.49354.393958.2.75529.9335.4398.75
183 [4] Joseph Turian, Lev Ratinov1,8a3nd Yoshu[4a]BJeonsgeipoh. TWuorriadnr,eLpreevseRnattaitnioovn,sa:F naC dsT iYmopshleuaanBdegnegFniCoeT.raWl moredthroedprfeosrentations: a simple and general method for
184
semi-supervised learning. I 1n 8A 4ssociation
fosermCio-smuppueR travN tiiN soend( ad ll= eLa2 ri0 nng0 inu) gis.tIicnsA,FpsCsaTogY ceg isalo 3ti8b o4na –l f3o( 9W r4FC,o Co2r mTd 0P 1N pPg0uelDR .tot aN)BbtiNaoln((adWl= Lo2 ir04 ndg08Nu).9 ies7tti)cs5
,
9 p3 a.30 gY.0 e5 s0 384 44 –. 373 991 45. ,91 2.300P5 100P 0.D .925B434.5.76990.2404580.09..5793029.6036303.5.8240.075.7773391.65.1003.95475.7337.45.91850.494050.5.90234.1838.5470.92
PETN(Plank anPdEMTPPo(PDsclBahnitkti,an2d041M73.)o5s3chit5t3i1,5.22N.50813)40.3615.311.P24P5D.35B440.3.3651.044741.55.93.3367.83553.159.804.130.4 3371..83351.4 435.443.323.8 354.413.49.60 32.839.1034.0
185 [5] Ronan Collobert. Deep le1a8rn5ing for[e5f]ficRieonntandisCcorilmloibneartti.veDpeaerpsinlega.rnIinngInftoerrneaffiticoineanlt Cdiosncrfeimreinncaetivoenparsing. In International Conference on
FCT FCT
Artificial Intelligence and Statistics, 2011.Artificial Intelligence and StatBisOticWY s, 2(P01la1n.k aBnOdPWMPDo(PsBclahnitkti,an25d011M.32)o2sch5i3t7t6i.,2Y.7260133)7.3135.579.P24P5D.06B317.1.1157.554146.25.92.0391.83654.774.654.311.0 3313..84519.1 416.102.171.2 414.163.29.97 27.244.3132.7
186 186
Best-(Plank anBdeMstLo(PMsclahnitkti,an2d041M93.)o4s3chit5t3i5,7.23.-40613)43.3125.252.34L8M.55433.5.6154.144942.48.63.5338.13753.449.614.444.7 3382..13292.9 445.373.556.8 394.923.76.38 35.839.4437.8
187 [6] Eric H Huang, Richard Soc1h8e7r, Christ[o6p]heErriDc HMaHnunainngg,, RanicdhAarnddSreowchYer,NCgh.riIsmtopprhoverinDg wMoarndnrinepgr,easnedn-Andrew Y Ng. Improving word represen-
Y LM + PPDB 53.82 37Y.48 34L.4M3 + P6P5D.4B7 5439.8.424 374.458.65 34.43 65.47 49.44 45.65
tations via global context and multiple wotardtiopnrostvoitayFpCgelTso.baIlncAosnstoexcitaatinodnmfourltCipolempwuotradtiopnroatloLtyFinpCgeTsu.isItincsA,ssociation for Computational Linguistics,
188 188
pages 873–882, 2012. pages 873–882, 2012. Tablejo7in:tPerfoTramLbalMenc7+e: oPPnPerDAfoBCrEm2a05n06c5.e5to3enstAsCe4jt1Eos..i24nT01t0h5e tfiers3st6tLs.p4eMa5tsrt.+oTfPh6Pteh8Defi.5Brt2satbplea5r56sth1.5oo.63fw5tsheth4tea1b4.p4l6ee1.r5fso3hromw3as6nt.ch4ee5opfedrfi6off8rem.r5ea2nntcme 5oo1fd.ed6li5sffoenren4t 6m.5o3dels on
189 189
different sourcdeifsfeorfeennttsitoyurtcyepseso,fwehnetirtey”tyGp”esm, ewahnesrteha”tGt”hemgeoalndsttyhpaetsthaereguosldedtyapneds”aPre” umseeadnasntdha”tPw”emaeraenusstihnagt twhee are using the
[7] Richard Socher, Alex Perelygin, Jean[7W] uR,icJhasaordn SCohcuhaenrg, ,AClehxrisPteorpehlyergiDn,. JMeaannnWinug,, JAasnodnreCwhNuagn,ga,nCdhristopher D. Manning, Andrew Ng, and
190 190
Christopher Potts. Recursive deep modelsCfhorrissteomphaenrtiPcoctotsm. pRoescituiorsnipvalerT ieta dydb eioelce vptee9 mrd: aotP dyse eeplr nesfptso ifrm.r oem rTednsihacetnemttcerseaedeenco bttyion acpnnt ckedh o.se mp.Ias nTperohtm seoia tfisn oett nhci aceolT insttai yadmbbopllie velaear9srrth:i atooyPfswe ettra nshfs teo itk mhrtmewaebnari tltenehstc rusP eelhePto bosDn awunBtsnkh .dtdehea Isernteatrmh.eesaunllottiwsc-usrniemdsoeilruartrchiteeysltoeawtstki-nrwges,iotwhuhrPcePerDesBtehttedinaegtna,t.iwtyhetyrepetshe entity types
191 191
Empirical Methods in Natural Language PEromcpeisrsiicnagl,Mpaegtheosd1s6i3n1N–1a6tu4ar2ar,el2Lu0an1n3kg.nuoawgenP.aroreceusnsiknngo, wpang.es 1631–1642, 2013.
192 192
[8] Karl Moritz Hermann, Dipanjan Das, J[a8s]onKWarelsMtoonr,itaznHd eKrumzamnann, DGiapnacnhjaevn.DSaesm, JaanstoicnfWramesetoind,eanntidficKautizomnan Ganchev. Semantic frame identification
193 193
Dev MRR Dev MRR Test MRR Test MRR
with distributed word representations. In PwroictheeddiisntrgisbuotfeAdCwLo.rAdsrseopcrieasteionntaftioornCs.oImnpPurtoactieoendailnLgsinogfuAisCtiLc.sA, ssociation for Computational Linguistics,
194 194
June 2014. June 2014. Model Model Fine-tuning Fi1n,e0-0t0unin1g0,0010,000100,1000,0000 1,010000,00100,0010,000100,1000,0000 100,000
195 054 Based19o5n above ideas, we achieve@a`geneX rn al m@o`del and can easily apply@`to moX dn el t@o`an NLP task
(w , f = f (w , S)) (20()w , f = f (w , S)) (20)
[9] John Duchi, Elad Hazan, and Yoram[9S]inJgoehrn. DAudcahpit,ivEelasdubHgarazdanie,natnmdeYtShooUrdaMms foSrinognelri.neSAlUedaMarpntiivneg saunbdgradient method-s for onlin4e6.li9e-a5rniing35an.2d946 i.95 30.6395.29 52.6330.6941 i.195 i2.63 37.34 i21.19 37.32
196 055 witho1u9t6the need for designing model = structures or⌦ sef lecti⌦ nge fea, tures fro( m13 s=) cratch. Spec⌦ ifif cally⌦ , ife , (13)
w w w w
stochastic optimization. The Journal of MastcohcinheasLtiecaornpitnimgiR@zaeTtsieoanr.cTh,h1e2J:o2@u1RrS2n1Ua–lM2o1f5M9,ia2c0h1in1e.SLiUeMarning R@eTsearch, 12:2@1R21–Y2159,i20115.0.i8Y1 36.8150.81 32.9326.81 57.2332.9245.0157.23 41.2435.01 41.23
197 056 we de1n9o7te a instance as (y, S), where S i=is 1an arbitrary language structure an id =y 1 is the label for
B´est RecursBievset RNeNcu(dr=s5i0v)e NN (d´=Y50) 45.6Y7 30.8645.67 27.0350.86 54.8247.0539.2554.84 35.4399.25 35.49
[10] Iris Hendrickx, Su Nam Kim, Zornits[a10K]ozIrairseHvae,nPdrreisclkaxv,NSuakNova,mDKiairmm,uZidorOnitSse´aaKghodzharae,vSae,bParsetsialanvPNadako´o, v, Diarmuid O Se´aghdha, Sebastian Pado´,
198 057 the st 1r 9u 8cture. Then we decompose the structure to some factors following S = {f}. For each
Marco Pennacchiotti, Lorenza Romano, aMndarSctoanPeSnznpaackcohwioicttzi,. LSoermenezvaalR-2Bo0me1s0atntRoa,eskacn8ud:rSMstaBiunlevtSise-tzwpRaaNeykNcocwul(adirsc=-zs2.i0Sv0ee)meNvaNl-(2d0=1Y020ta0s)k 8: M48u.l9tYi7-way3c3la.s5-048.97 31.1333.50 53.5391.1340.5053.59 38.5470.50 38.57
199 058 factor 19f 9, there is a list of m associated features g = g , g , ..., g , and a list of t associated words
sification of semantic relations between pasiirfiscoatfionnomofinsaelms.anInticPrreolcaeteiodninsgbseotwf etheen1Speami2rEs voafl-n2ommWionraklssh.oIpn, Proceedings of Nthe SemEv 4a 7l- .2 5N3Wowrks 3h25o .p 5S, 847.53 31.3315.58 54.333(12.131)41.965w4.3323S9.1401.96 39.10 (21)
200 Uppsala, Sw05 ed9 en,
20w 10f.,1, 2w 0f0,2, ..., w f,tU2 ppf s. alH a,e @Sre w`w ede ens ,uX 2pn 0p 1o 0s .e that eaFcChTfactor @h `as FthCeT @s `ame nXunmber of words, and th @e `re i i
Y 51.2Y2 36.7651.22 33.5396.76 61.1313.5946.9961.11 44.3416.99 44.31
is a transformation from the wor=ds in aIfa[cwtor=to wFaCh]TiTdd en laye rFaCsfTfollo=ws(:14)I [w = w] T     f (14)
201 060 201 i w i w
[11] Richard Socher, Brody Huval, Christ[o1p1h]erRDic.hMardan@Snoeincgh,era,nBdrAo d nydrHewuvYal.,NCgh.risSteomphaenrt@iDcR.coMm apn@onesiniitgi o,naanldityAndrew Y. Ng. Semant@icRcomposiitionality
w FCT + LM FCT +wLM - 49.4-3 37.4649.43 32.2327.46 53.5362.2242.6353.56 39.4442.63 39.44
202 through rec0 u6 rs1 ive matrix-ve2c0t2or spaces. IntPhrrooucgeehdriench gusrsoi= fvteh  mei=2a10tre 1ix2-Jvoeicn: tote rCsopnafceer: es. .n.. cIne: Poe nroEcmeepdi· irnW icgasloM, f tehteih=-21012 Joint Conference on(1 E) mpirical Meth-
f w w w
f,1 FfC,2T + LM +fs,tu Fp Ce Trv+isLeMd +supervised Y 53.8Y2 37.4853.82 34.4337.48 65.4374.4349.4465.47 45.6459.44 45.65
203 ods in Natu0ra6l2Language P2ro0c3essing and Coodms pinutNaatitounraall NLaantugruaalgLeaPnrgoucaegsesiLnegaarnndinCg,opmapguetsa1ti2o0n1a–l1N2a1t1u,ral Language Learning, pages 1201–1211,
where e is the word embedding for word w . Suppose the word embeddings have d dimensions
joint 5T6j. o5i3n(t⌦f41.4⌦156e.53)3,6.4451.41 68.532(62.24T5)51 .6(56⌦8.f52 4⌦6.55e31.6)5, 46.53 (22)
204 Jeju Island, 0K 6o 3rea, Ju aly nd20 t1 h22 e0. 4 hwA ii dss do ec nia lti ao yn eJf reoj hru aCI ssol dmanpdu d,t iKa mtoio eren naa s,l ioJLu niln syg .2u H0is1 et2i rc e.siA. Wsso =cia [t Wion f Wor C .o .m .Wputa ],ti eo an ca hl L Wingu ii ssti acs d. e ⇥ d matrix, w i w i w i w i
h Xn 1 2 t Xn j e h
205 054 [12]BaRseodboenrtaPbaorvke0e6 ird4,eDasa,vwideisGacr ahaief tfvr2,ea0J na5u sngfbe0 on5o re4 mrKa[ alo1 tnm2 ig oo]B, ndaKeRslfeeo rdab onCoe mdhnrtceaan tPbnh,aoeearvkanecsediiorld,yK neDacaaspa az,pv tuwleiaydenktaiGaotcM irmhoaiaoef nfevd,edeoJlaafu.tongwEbeanno oneg rrKNdlailLos ehnmPmgogt,adi bsgKekelae dwadnCodihr ndce ganfins,feat th oansdeitld hyKi etaaipzo ipu nnlay, pk utiotM smoaoe fddetla h.teoEahnnig dNl diLs ehPngtai lsg aka yw ero .rd fifth edition,
Table 8: PerfoTrmabalnec8e:oPnertfhoermseamnacnetoicnstihmeilsaermityanttaiscksiwmitihlaPriPtyDtBasdkawtai.th PPDB data.
T : f ⌦ e   representatioTn : (f15⌦) e   representation (15)
206
055 wijthuonuet.tLheinngeue0ids 6tf5iocrDdeastiag TnCi hno egn nms 2o to 0hrdt 6eeiul sms0 it5 gr,5u mLcDt ourC ie d2sw0 toi rjt1rhu a1osn neTue slt0e. fct7 oLht, riein m2ngn0ge af1uee t1id ias o.ttfui nocrre  Dsdefa wrstoiaig im lnC lisn bocgr enamswt ucoo ih srd. etei duSl mps tte or,cuLic gfiDt ecua trClel t2sy h,0o ei1rf1 vseT al0 le uc7t e,in s2g0 of1 fe1a h.tu idre ds ef nroim laysc er rawt fc rih o. mSp ie tc sifi ic na pll uy, tsif
.
056 we denote a instance as (y,S), where05S6is anwaerbditernaroytelaanignusatagnecsetrausct(uyr,eSa)n,dwyheirsethSeilsabaenlafrobritrary language structure and y is the label for
207 066 207 i=1 i=1
057 the structure. Then we decompose th0e5s7tructutrheetostrsuocmtueref.acTtohresnfowlleowdeincgomSpo=se{thfe}.strFuocrtueraechto some factors following S = {f}. For each
  (23)   (23)
@` @` @R@` @` @R
208 058 factor f, there i0s6a7list of m asso2ci0a8ted0f5e8aturesfgact=orgf,,gthe,r.e..,isga,liasntdofamlisatsosfoActiaapstespdocfeieanatetuddreiwsxogr1dA=s:pgFp,egean,t.ud..,rigxes,1aU:ndFsaeeldiasttiounfrtFeassCsoUcTiasteeddwionrdsFCT
1 2 m 1 2 m = =
e e
209 059 w f,1,w f,2,...,w 0f6,8t 2 f. Here we 2s 0u 9ppo 0s 5e 9that ewac fh ,1f,awc fto ,w 2r, fh. ,.a 1.s,wth fe ,tsa2mfe.nHuemrebewreo fsuwpoprodsse, tahnadt tehaecrhef!actowr fh ,a 1s the same number o f words, and there !     0 @T @(R24@)T@T @R @T   0 (24)
2,0 2,0
210
0 06 60
1
is a transformat 0io 6n 9from the w hor =d 2s
1
 i 0n  a  0 0ef 6 6a 0 1c (cid:4)to (cid:14)r
(cid:6)
:(cid:9)t eo (cid:8)i (cid:8)sa (cid:12)ah (cid:15)i (cid:11)t :dr (cid:18)a .d
(cid:1)
.ne .(cid:1)esn :f wo el farm ,y 2eart  iao ·sn Wfofrl  olom ,wtsh: Te w h or =d 7s .  i 1Xn  na
 
efa O fc (cid:4)to v (cid:14)r
⌦(cid:6)
:e (cid:9)t ero
(cid:8)
ea (cid:8)a
((cid:12)
1lh 7 (cid:15)l )i (cid:11). :d p (cid:18)1 .d
(cid:1)
.ee .(cid:1)en r :wf el O fa o ,y 2rver me  a r ·s aa Wf no ll lc  lo ep ,(w s 1es r: 6o Tf )no  rAmCaEX nn c2e0 fs0o5 ⌦n A e(1C
)
E  200 ,5
  ,  (16)   0 (25 ) ,   ,     0 (25)
211 070 f 211 w f,(cid:16)1(cid:10)(cid:1)(cid:21)(cid:16)w(cid:17)f(cid:8),2(cid:18)(cid:1) w..f.,t f (cid:1)w f,(cid:16)i 1(cid:10)(cid:1)(cid:21)(cid:16)w(cid:17)f(cid:8)w ,2(cid:18) i(cid:1) w..f.,t -.5 (cid:1).3 .8i .7 w i 2,1 2,2 2,3 2,1 2,2 2,3
212
0 06 62
3
w anh der the ee
hw ii
di ds et nh 0e la7w
y1
eo rrd hae sm dbe dd id min eg
2 n1
sf io
2
or nw
s0
0
.6
6o
H2
3rd(cid:12) e(cid:15) rw(cid:1) ei(cid:5) W.(cid:1)w a(cid:10)S n(cid:5)hu d=(cid:7)ep(cid:19)r tp he(cid:16) [o We(cid:17)es(cid:1)
e
hwe wiit dWhi dse et nwh ..eo l.aWrw yd eoe rr ]m ,d heabe asem cd dhbdei Wn ddg id msin ih esg na av s(cid:13)f
(cid:5)
ioe
i d(cid:6)
or =(cid:9)d
n(cid:13)w(cid:18)
se1
⇥.od Hr didm(cid:12) e(cid:15) rwe m(cid:1) en i(cid:5) Wa.s(cid:1)i t(cid:10)So ri(cid:5)nu x=(cid:7)sp ,(cid:19)p(cid:16)
[S
o
W(cid:17)sU
(cid:1)
eeM
wt
Wh( eA
w
.B
.o
.Wrd) e =
]m
,
ebS aS
e
cU dU hdMM
i
Wn(( gAB
s
ihB sA
a av
(cid:13)) (cid:5))
e
i
d(cid:6)-=(cid:9)0 0. =
d
5(cid:13)(cid:18)
e1
⇥.0 03dS
di
.0
0m
8U eM m.0 07n( asB
i to rin
xA
s
,
()7) (7)
L 1, L 2 L 1, L 2
h 1 f,t2 t h j e h 1 f,t2 t j 0e 0 0h 0
213 064 is a transforma0ti7on2 from the con2ca1t3ena0 t6 io4 n of wisoardtreamnsbfeodrdminatgisontofrtohme itnhpeutcsonocfattheenahtiidodneonflawyoerrd. embeddings to the inputs of th-e.5 h.i3dd.8en.7 layer.
(9) (9)
(cid:10)(cid:9)(cid:5)(cid:19)(cid:20)(cid:17)(cid:9)(cid:18)(cid:1) (cid:10)(cid:9)(cid:5)(cid:19)(cid:20)(cid:17)(cid:9)(cid:18)(cid:1)
-.5 .3 .8 .7
065 Then the sigmoid transformation   will06b5e usedTtohegnetththeesivgamluoei sdotrfahnisdfdoernmlaatyioenr f ro!wmillitbseinupsuetds.to get the value s of hidden layer fro!m its inputs.
214 073 214 2n2 |V |n 2n2 |V |n (8) (8)
066 066 (cid:2)(cid:6)X(cid:3)(cid:1)n Xn (cid:2)(cid:6)X(cid:3)(cid:1)n Xn
215 074 215 @L @L @L@L @L @L
067 067 T   f ⌦ e = T  Tf   e f ⌦ e = T   f  e = 1 + =2 1 + 2
i w i w i w i w
068 075 068 i        i i        i @R @R @R@R @R @R
Figure 1: Tensor representation of the FCT model. (a) Representation of an input structure. (b)
i=1 i=1 i=1 i=1
069 076 Representa06 ti9 on for the parameter space. A A0 of AB0 A0B of B0 B(9) (9)
070 070 M -.5 .3 .8 .7 M
077 4 4i i
071 071
078 Based on above notations, we can represent each factor as the outer product between the feature
072 072 Xn Xn
073 079 vector and0t7h3e hidden layer of transformed embedding g ⌦hA . TBhe wAe u0 sAeoafteBnsBor0 AT0= oLf⌦EB⌦(10F0) s((1l0, )e , e , S;sT(l), e=, e , Ss(;lT, e) =, f )s(l, e , f )
f fy(cid:1)
1 2 1 2 w w w w
074 080 as in Figru0r7e4 1(b) to transform this input matrix to the labels. Here L is the set of labels, Ef wi(cid:1)ree fwi(cid:1)ers to i i i i
075 075 Xn M 1(cid:4)man(cid:1) M (cid:3)=taxicab(cid:1) …,w n(cid:1) Xn f wi(cid:1) (w i is on path?)(cid:1) i=1 i=1
076 F Ri eg pu rr ee se1 n: taT tie on ns0o for8 r1re thp ere ps ae rn aa t mall t eiod teni rm o spfe atn ch es e 0 .i 7o F 6Cns To mf oF Rh di egei pud l r.r ed e s(e ea1n n): taRl T ta e ie oy pn nre seor fs ore( rnr| e ttE hap etri| eo ps= n ae rno at2 mfat0 a eion t0 en) rino sa p pfn u attd ch f ese .eF iFn ⌦tCei Ts nc et meh w.oe d(bs e)e l.t (o af ) f Re ea pt ru esr ee ns t. at wi 1o (cid:2) (wn, 12 7of )an input fse in1 0 1⌦ten- -0. .c5 5ee w.. .03
3
(. .08 8b). .07
7
(17) Xn Xn
082 A(cid:1)i T   f   e  T R efla tioen s Relatiio(n11s) (11)
077 In order to07p7redict the conditional probability of a label y given the structure S, we have0 0 0 0 0 = T   f=   eT   f (12 ) e (12)
078
Based on
above08 n3
otations, we can
rep0 re7 s8
ent eaBchasefadctoonr aabsotvhee noouttaetriopnrso,dwucet
cb i ae= ntw1 reeepnr  e  tsheentfeeaa  tcuhrefactf ow(cid:2)1(cid:1)
r
(cid:1)f awi s(cid:1) the(cid:2) o(cid:1) utere pw(cid:1)
roduct
b i e= tw1 een  0 1 the-0 .5 fe. a0 3  tu.0 8 re.0 7 l w i wl i w i w i
fex⌦p{es(yw,=S“A;”[(cid:1)Tff):w}⌦=e“]deriving”(cid:1) [f : e] -.5 .3 .8 .7 i=1 i=1
079 vector and the h0id8d4en layer of transform0e7d9embevdedcitnogrgan fd⌦thhe fh.i PTdhd (ee ynw |l Seayu ;es Treoa )ftt =ernasnoPsrfoTrm=edLe⌦m [E Ab me a⌦d n]dF1 di rn ivig
ng
wg
haf
t
a⌦ ppeaih
redf
to. beT
[a
h
,
taxe icaw
b]
e (cid:1)use a
f
w(cid:1)tensor T = L⌦e wE i(cid:1)(w⌦ i=“Fdriv(in2g”))(cid:1)
080 as in Figrure 1(b) to transform this inpu0t8m0 atrixatsoitnheFilgabruerles.1H(be)retoLtriasntshfeorsmettohfislaibneplust, Emaerterxfixeprt{soMs1tot(hye l,abSe;lsT. H)}ere LMi2s the set of labels, E refers to
085 FCT CNFCNT CNN
081 all dimensions of hidden layer (|E| = 200810) andaFll disimtheensseiotnosf ofefahtuidrdese.n layer (|E| =Fyig  2u2r 0eL 01 ): aExnadmpFlecoinssttruhcetionseoftsuobfstrfuectaurteuermebsed.dings. Eachsubstructureisawordw iinS,augmentedbythetarget
entity information and related information from annotation A (e.g. a dependency tree). We show the factorization of the
086
082 In order to predict the condw ith ioe nr ae l ps r( oy b0 a,8 bS2 il; itT y o) fIni as olat rbh deee lrys toc gpo ivrr eee dnio tchtf ethl sa etrb cue ocntl udy rietic Soo n,am wlepp rhua on abnt voe at ea bdte id lws ie tn ytietn oche fi anoto lus au brbs etrm luc ytuore gds i( vele eflt). n,th tSe hc eionn scca tt ree un(a ct1iwo tn u8oe rf e)thd Sees ,ubc wstor eumct hur aep veom ebsededintghsfeorthsetsrenutecnc-e(middle),anda (18) @` Xn @@`` Xn @`
sin gle s ub structureembeddingfromthatconcatenation(right). Theannotatedse nt enc e embedding(notshown)wouldbethe
083 087 ture S to f0a83ctors, each factor f 2 S will consutmroifbthuestuebstrutcoturetehmebeddsincgso,arseoppboseadstoe@ thde` ircoonncatetnhatieon.m    o 0d@e`l pa   r ( am  0)eters. = ⌦=f ⌦ e ,⌦ f ⌦(1e3), (13)
084 084 exp{s(y,S;T)} i exp{s(y,S;T)} -.5 .3 .8 .7 1 0-.512 .30 .08 1.7 @T @@RT w i @Rw i w i w i
088 SpePc(iyfi|Sca;Tll)y,=ePach label y correspo,nds toPa(ysl|Sic;eT)of=thPe te(2n)sor T , which,is a matrix  (y, ·, ·). (T2)hen
exp{s(y ,S;T)}
the previous subsect eio xn p. {T sh (eyym  o ,d Sel ;u Tse@ )s }iRts pa- dependenc@y pRath between M
1
and M 2. Suppose
i=1 i=1
085 085
089 each factor f wiyl l2Lcontribute a score rameters to scyo r2eLthe annotated sentence embed- the third feature in f w indicates this on-path
086 086i ding and uses a softmax to produce an output la- feature. Our model ci an now learn parameters
where s(y,S;T) is the score of label y computwedhewreiths(oyu,rSm;Tod)eils. tShiencsceowreeodfelcaobmelpyosceotmhepusttreudcw- ith our model. Since we decompose the struc-
090 bel. We call the entire model the Feature-rich which give the third row a high weight for the
087 087 s(y, f ) = T   g   h , (3)
ture S to factors, each factor f 2 S will conttruibreutSe ttootfhaectsocrosr,eeabcahsefdacotnorthfe m2oSdewl pilalrcaomnettreibrsu.te to the score based on the model parameters.
i ii Com+pyositionalfEmbeddifngModel(FCM). (19)ART label. Oth+er words with embeddings similar (19)
088 Specifically, ea0ch91label y corresponds t0o88a sliceSopfetchieficteanllsyo,reTach, wlahbieclhyiscoarrmesaptroixnd s(tyo,a·,s·l)i.ceThoefnthe tensor T , which is a matrix  (y,·,·). Then
y Our task is to determine the labeyl y (relation) to “driving” that appear on the dependency path
where   correspond to tensor product, while in the case of Eq.(3), it has the equivalent form:
089 each factor f w 0i 9ll 2contribute a score 089 each factor f will contribute a scoregiven the instance x = (M ,M ,S,A). We for- between the mentions will1similarly receive high
i i 1 2
0
090 090 mulatethisasaprobability. weightfortheART label. O 1ntheotherhand,ifthe
093 s(y,f i) = T y   Tg f   h gf,   h = T   (gs(y ⌦,f hi) = )T =( y3 ) ( g Pf(y  ,h ·,f·, ) · g )T h e.mbeddingissimilarbutis0n(3 ot) onthedependency
091
where   corres0p9on4d to tensor
product,0 w9 h1
ile
inwthheecreay
s e
ocforErqef s.(p3o)n,ditf thoatsetnhseoy reqpuroivdaulecf ntP,t(wyfo|hxri;mlTef
,:ein)
=the exp c( asen i=o1fT yE  q.( (f 3w)i,⌦ itef whia))
s
thef p ea qth u, ii vt w alil el nh tav fe o0 rmwe :ight0 1. Thus, our model gen-
092 In this way0 ,92 the target score of label y given an instance S and paZr(ax)meter tensoerralTizesciatsnmobdeel pwarramitetteersnacaross:s words with
093 095 T   g   h = T093  (g ⌦ h ) = ( (y,·,·)T· g )Tgh . h = T   (g ⌦ h ) = ( (y,·,·) · g(2 )) T hsim .ilar embeddings only when they share similar
y f f y f f yf Xnf f f wherye   is tfhe X‘ nmatfrix dot product’ or Frobfe- fuf nctionsinthesentence.
094 094
In this way, the0 ta9 r6 get score of label y given an iInnsttahnicsewSaya,ntdheptaarragmetetsecrotreenosforlaTbeclaynniogbuiseveiwnnnreairtntperinondsautscat:nocfethSe atwnodmpaatrriacems.eteTrhetensor T can be written as:
095 095 s(y, S; T) = s(y, nf orm; aT liz) ing= constant wT hich  sumg s over  allh possi-. Smoothed Lexical Features (A4no)ther intuition
i y f f
097 Xn Xn blXenoutput labels y0 2 LXnis giveni by Z(x) i = about the selection of outer product is that it is
096 096 P  P  
s(y,S;T) = s(y,f ;T) = T   g   hs(iy=.,S1;T) = se(xyp,f n;i(T=4T))1= (f ⌦Te  ) .g Th e pha- . actually a smoothed version(4o)f traditional lexical
097 098 097 i y f i f i ramy0 e2 tL
ers of
theii= m1 ody e0
l are
w thi
e
wy ow ri
d
emf bi eddingsf i features used in   c la0ssical NLP   s( y2s tem0)s. Consider
i=1 i=1 i=1 i=1 -.5 .3 .8 .7 1 0 1 0 0 1
098 099 The m09o8del only performs linear transforme faortieoacnhswoordntyepeaacnhd avliiset owf woeifghtthmeatrtixensa ole rx ,ic mal f aea ktu ir ne f g= thu e^ mw, owh dic eh lis a conjunc-
FCM
tion (logic-and) between non-lexical property u
099 The FCM mode 1l 0on 0ly perfoerfmfiscliienenatratrn0a9 dn9sfeoarsmyaTtthiooeniFsmCoMnplemeaomchdeevlnioetwn.lyofptehrefotremnssolri,nmeaarT yk .tir T= na hgn e[ sT mtfhy oo] ey dr2 emmL
l
iaw o st ldh i oi oe gch -nl bs ii ls io nu ens ae red 6at (o c i.h es .c vo lor ie ge -we qa uc aoh df rl aa t tb h ice e )l ten ans dor le, xm icaa lk pi an rtg (wth ore d)m wo .d Ie fl
we represent w as
100 efficient and easy to implement. 100 efficient and easy to implement.
aone-hotvector,thentheouterproductexactlyre-
101 since we recover a log-linear model by fixing ei-
101 Learning101In order to train the parameters wtheeroeportTim. Wiezsteudtyhboethfthoelfluollwlogi-nbilginecarraondss-econvetrrsothpeyorioginbajlefecattuirve ef.:Then if we replace
Learning In order to train the parameters weLoepatirmniiznegtheIfnoollrodweirntgoctrraoisns-tehnetrpoapraymoebtjeercstiwvee:optimize the following cross-entropy objective:
102 102 102 X tXhe log-linear Xmodel obtained by fixing the word the one-hot representation with its word embed-
embeddings.
ding, we get the current form of our FCM. There-
103 103 `(D;T,W10)3= logP(y|S;`T(,DW;)T, W)`(=D;T,W) = log P(lyo|gSP;(Ty|,SW;T,)W)
fore,ourmodelcanbeviewedasasmoothedver-
104 104 3.3 DiscussionoftheModel sion of lexical features, which keeps the expres-
104 (y,S)2D (y,S)2D
(y,S)2D
sive strength, and uses embeddings to generalize
105 105 Substructure Embeddings Similar words (i.e.
105 tolowfrequencyfeatures.
106
where D is the set of all trainin 1g
06
data.wherWeeDuseisd tAhedaGsertadof[9a]lltotraoitnphiotnismegwizitdehastimaa.bilaorveemWbededinugss)ewdithAsimdialaGrfruandc- [9] to optimize above
objective. 1T0h6ereforewhweereareD peisrfortmhoienbgjescetsitvtoec.ohafsticalTlhtreartiernfaioinrngei;ngwaenddaarteatfioo.nrspienertfahoWecrhsmeeniteningncue-s(ie.sedt.otchhoAsaesdwtiiacthGsirmtraialdairnfiena[g-9;] TimatonedCoo mfpopltrei xm ityeiazcIe nhferea nincb e-o inve FCM is much
107 107
stance (y,S) the loss ofbujnecctitoivne.` = Ts`ht(ayen,rcSeef;oT(ry,eW,S))wteh=e alorlsoesgPfpu(yne|crStfi;ooTnr,mtu Wr `e is n)) .w gi =llha sv Te to`hsi (m ecyni h,la aSrm s;ta Titr c,ix Wrep )tre rs aen i=t nat iio nns g. l;oTo gPf(aays nt|eSdrt;hTan,fbWo oth r)C.NN es a(C cTo hhlloebner it net -al.,2011)and
107 understandourselectionoftheouterproduct,con- RNNs (Socher et al., 2013b; Bordes et al., 2012).
stance (y, S) the loss function ` = side`r(thye,exSam;pTle ,inWFig.)1. Th=e word “dlroivgingP” (y FC|S M ;reTqui,reWs O()s.nd) produTcths eonnaverage with
can indicate the ART relation if it appears on the sparse features, where s is the average number of
2 2
per-word non-zero feature values, n is the length
6Other popular log-bilinear models are the log-bilinear
of the sentence, and d is the dimension of word
language models (Mnih and Hinton, 2007; Mikolov et al.,
2013).2 embedding. Incontrast, CNNsandRNNsusually
1777
havecomplexityO(C ·nd2), whereC isamodel Set Template
HeadEmb {I[i=h1],I[i=h2]}
dependentconstant. (wiisheadofM1/M2)⇥{ ,t h1,t h2,t h1 t h2}
Context I[i=h1±1](left/righttokenofw h1)
4 HybridModel
I[i=h2±1](left/righttokenofw h2)
In-between I[i>h1]&I[i<h2](inbetween)
⇥{ ,t ,t ,t  t }
h1 h2 h1 h2
We present a hybrid model which combines the On-path I[wi2P](onpath)
⇥{ ,t ,t ,t  t }
FCM with an existing log-linear model. We do so h1 h2 h1 h2
Table2: FeaturesetsusedinFCM.
bydefininganewmodel:
1
6 ExperimentalSettings
p (y|x) = p (y|x)p (y|x) (3)
FCM+loglin Z FCM loglin
The log-linear model has the usual form: Features Our FCM features (Table 2) use a fea-
p loglin(y|x) / exp(✓ · f(x,y)), where ✓ are the ture vector f w
i
over the word w i, the two tar-
model parameters and f(x,y) is a vector of fea- get entities M 1,M 2, and their dependency path.
tures. The integration treats each model as a pro- Hereh 1,h 2 aretheindicesofthetwoheadwords
viding a score which we multiply together. The of M 1,M 2, ⇥ refers to the Cartesian product be-
constantZ ensuresanormalizeddistribution. tweentwosets,t h andt h areentitytypes(named
1 2
entitytagsforACE2005orWordNetsupertagsfor
5 Training SemEval 2010) of the head words of two entities,
and standsfortheemptyfeature.  referstothe
FCMtrainingoptimizesacross-entropyobjective:
X conjunction of two elements. The In-between
`(D;T,e) = logP(y|x;T,e) featuresindicatewhetherawordw isinbetween
i
(x,y)2D two target entities, and the On-path features in-
where D is the set of all training data and e dicate whether the word is on the dependency
is the set of word embeddings. To optimize path, onwhichthereisasetofwordsP, between
the objective, for each instance (y,x) we per- thetwoentities.
form stochastic training on the loss function ` = We also use the target entity type as a feature.
`(y,x;T,e) = logP(y|x;T,e). The gradi- Combining this with the basic features results in
ents of the model parameters are obtained by morepowerfulcompoundfeatures,whichcanhelp
backpropagation (i.e. repeated application of usbetterdistinguishthefunctionsofwordembed-
the chain rule). We define the vector s = dings for predicting certain relations. For exam-
P
[ T  (f ⌦e )] ,whichyields ple, if we have a person and a vehicle, we know
i y w i w i 1yL
h i it will be more likely that they have an ART rela-
@`     T
= I[y0 = y] P(y0|x;T,e) , tion. For the ART relation, we introduce a corre-
@s 1yL
sponding weight vector, which is closer to lexical
where the indicator function I[x] equals 1 if x is
embeddingssimilartotheembeddingof“drive”.
trueand0otherwise. Wehavethefollowinggradi-
P All linguistic annotations needed for fea-
ents: @` = @` ⌦ n f ⌦e ,whichisequiv-
@T @s i=1 w i w i tures (POS, chunks7, parses) are from Stanford
alentto:
CoreNLP (Manning et al., 2014). Since SemEval
@`     Xn doesnothavegoldentitytypesweobtainedWord-
= I[y = y0] P(y0|x;T,e) · f ⌦e .
@T w i w i Net and named entity tags using Ciaramita and
y0
i=1
Altun (2006). For all experiments we use 200-
Whenwetreatthewordembeddingsasparameters
d word embeddings trained on the NYT portion
(i.e. thelog-bilinearmodel),wealsofine-tunethe
of the Gigaword 5.0 corpus (Parker et al., 2011),
wordembeddingswiththeFCMmodel:
! ! withword2vec(Mikolovetal.,2013). Weusethe
@` Xn X @` CBOW model with negative sampling (15 nega-
= T ·f ·I[w = w].
@e @s y i i tive words). We set a window size c=5, and re-
w y
i=1 y
movetypesoccurringlessthan5times.
As is common in deep learning, we initialize
theseembeddingsfromanneurallanguagemodel Models We consider several methods. (1) FCM
and then fine-tune them for our supervised task. in isolation without fine-tuning. (2) FCM in isola-
The training process for the hybrid model (§ 4) tionwithfine-tuning(i.e. trainedasalog-bilinear
isalsoeasilydonebybackpropagationsinceeach
7ObtainedfromtheconstituencyparseusingtheCONLL
sub-modelhasseparateparameters. 2000chunkingconverter(Perlscript).
1778
model). (3) A log-linear model with a rich binary cross validation on the training data to select hy-
feature set from Sun et al. (2011) (Baseline)— perparametersanddoregularizationbyearlystop-
thisconsistsofallthebaselinefeaturesofZhouet ping. The learning rates for FCM with/without
al.(2005)plusseveraladditionalcarefully-chosen fine-tuning are 5e-3 and 5e-2 respectively. We
featuresthathavebeenhighlytunedforACE-style report macro-F1 and compare to previously pub-
relation extraction over years of research. We ex- lishedresults.
cludetheCountrygazetteerandWordNetfeatures
7 Results
fromZhouetal.(2005). Thetworemainingmeth-
odsarehybridmodelsthatintegrateFCMasasub-
ACE2005 DespiteFCM’s(1)simplefeatureset,
model within the log-linear model (§ 4). We con-
it is competitive with the log-linear baseline (3)
sider two combinations. (4) The feature set of
on out-of-domain test sets (Table 3). In the typi-
Nguyen and Grishman (2014) obtained by using
calgoldentityspansandtypessetting,bothPlank
the embeddings of heads of two entity mentions
and Moschitti (2013) and Nguyen and Grishman
(+HeadOnly). (5) Our full FCM model (+FCM). (2014) found that they were unable to obtain im-
All models use L2 regularization tuned on dev
provementsbyaddingembeddingstobaselinefea-
data.
ture sets. By contrast, we find that on all do-
mainsthecombinationbaseline+FCM(5)obtains
6.1 DatasetsandEvaluation
the highest F1 and significantly outperforms the
ACE 2005 We evaluate our relation extraction other baselines, yielding the best reported results
system on the English portion of the ACE 2005 for this task. We found that fine-tuning of em-
corpus (Walker et al., 2006).8 There are 6 do- beddings (2) did not yield improvements on our
mains: Newswire (nw), Broadcast Conversation out-of-domain development set, in contrast to our
(bc), Broadcast News (bn), Telephone Speech results below for SemEval. We suspect this is be-
(cts), Usenet Newsgroups (un), and Weblogs cause fine-tuning allows the model to overfit the
(wl). Following prior work we focus on the do- trainingdomain,whichthenhurtsperformanceon
mainadaptationsetting,wherewetrainononeset the unseen ACE test domains. Accordingly, Ta-
(the union of the news domains (bn+nw), tune ble3showsonlythelog-linearmodel.
hyperparameters on a dev domain (half of bc) Finally, we highlight an important contrast be-
and evaluate on the remainder (cts, wl, and tween FCM (1) and the log-linear model (3): the
the remainder of bc) (Plank and Moschitti, 2013; latter uses over 50 feature templates based on a
Nguyen and Grishman, 2014). We assume that POStagger,dependencyparser,chunker,andcon-
gold entity spans and types are available for train stituency parser. FCM uses only a dependency
and test. We use all pairs of entity mentions to parsebutstillobtainsbetterresults(Avg. F1).
yield43,518totalrelationsinthetrainingset. We
reportprecision,recall,andF1forrelationextrac- SemEval 2010 Task 8 Table 4 shows FCM
compared to the best reported results from the
tion. While it is not our focus, for completeness
SemEval-2010 Task 8 shared task and several
we include results with unknown entity types fol-
othercompositionalmodels.
lowingPlankandMoschitti(2013)(Appendix1).
FortheFCMweconsideredtwofeaturesets. We
SemEval 2010 Task 8 We evaluate on the Se- foundthatusingNEtagsinsteadofWordNettags
mEval 2010 Task 8 dataset9 (Hendrickx et al., helpswithfine-tuningbuthurtswithout. Thismay
2010) to compare with other compositional mod- bebecausethesetofWordNettagsislargermak-
elsandhighlighttheadvantagesofFCM. Thistask ingthemodelmoreexpressive,butalsointroduces
is to determine the relation type (or no relation) moreparameters. Whentheembeddingsarefixed,
between two entities in a sentence. We adopt the they can help to better distinguish different func-
setting of Socher et al. (2012). We use 10-fold tions of embeddings. But when fine-tuning, it be-
comeseasiertoover-fit. Alleviatingover-fittingis
8Many relation extraction systems evaluate on the ACE
asubjectforfuturework(§9).
2004corpus(Mitchelletal.,2005). Unfortunately,themost
commonconventionistouse5-foldcrossvalidation,treating With either WordNet or NER features, FCM
theentiretyofthedatasetasbothtrainand evaluationdata. achieves better performance than the RNN and
Ratherthancontinuingtooverfitthisdatabyperpetuatingthe
MVRNN. With NER features and fine-tuning, it
cross-validationconvention,weinsteadfocusonACE2005.
9 outperforms a CNN (Zeng et al., 2014) and also
http://docs.google.com/View?docid=dfvxd49s_36c28v9pmw
1779
bc cts wl Avg.
Model P R F1 P R F1 P R F1 F1
(1) FCMonly(ST) 66.56 57.86 61.90 65.62 44.35 52.93 57.80 44.62 50.36 55.06
(3) Baseline(ST) 74.89 48.54 58.90 74.32 40.26 52.23 63.41 43.20 51.39 54.17
(4) +HeadOnly(ST) 70.87 50.76 59.16 71.16 43.21 53.77 57.71 42.92 49.23 54.05
(5) +FCM(ST) 74.39 55.35 63.48 74.53 45.01 56.12 65.63 47.59 55.17 58.26
Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our
reimplementationofthefeaturesofNguyenandGrishman(2014).
Classifier Features F1
SVM(RinkandHarabagiu,2010) POS,prefixes,morphological,WordNet,dependencyparse,
(BestinSemEval2010) Levinclassed,ProBank,FrameNet,NomLex-Plus, 82.2
Googlen-gram,paraphrases,TextRunner
RNN wordembedding,syntacticparse 74.8
RNN+linear wordembedding,syntacticparse,POS,NER,WordNet 77.6
MVRNN wordembedding,syntacticparse 79.1
MVRNN+linear wordembedding,syntacticparse,POS,NER,WordNet 82.4
CNN(Zengetal.,2014) wordembedding,WordNet 82.7
CR-CNN(log-loss) wordembedding 82.7
CR-CNN(ranking-loss) wordembedding 84.1
RelEmb(word2vecembedding) wordembedding 81.8
RelEmb(task-specembedding) wordembedding 82.8
RelEmb(task-specembedding)+linear wordembedding,dependencypaths,WordNet,NE 83.5
DepNN wordembedding,dependencypaths 82.8
DepNN+linear wordembedding,dependencypaths,WordNet,NER 83.6
wordembedding,dependencyparse,WordNet 82.0
(1)FCM(log-linear)
wordembedding,dependencyparse,NER 81.4
wordembedding,dependencyparse,WordNet 82.5
(2)FCM(log-bilinear)
wordembedding,dependencyparse,NER 83.0
wordembedding,dependencyparse,WordNet 83.1
(5)FCM(log-linear)+linear(Hybrid)
wordembedding,dependencyparse,NER 83.4
Table4: ComparisonofFCM withpreviouslypublishedresultsforSemEval2010Task8.
thecombinationofanembeddingmodelandatra- Finally, a concurrent work (Liu et al., 2015)
ditional log-linear model (RNN/MVRNN + lin- proposesDepNN,whichbuildsrepresentationsfor
ear)(Socheretal.,2012). AswithACE,FCMuses the dependency path (and its attached subtrees)
lesslinguisticresourcesthanmanyclosecompeti- between two entities by applying recursive and
tors(RinkandHarabagiu,2010). convolutionalneuralnetworkssuccessively. Com-
We also compared to concurrent work on en- pared to their model, our FCM achieves compa-
hancing the compositional models with task- rable results. Of note, our FCM and the RelEmb
specific information for relation classification, in- arealsothemostefficientmodelsamongallabove
cludingHashimotoetal.(2015)(RelEmb),which compositional models since they have linear time
trained task-specific word embeddings, and dos complexity with respect to the dimension of em-
Santos et al. (2015) (CR-CNN), which proposed beddings.
a task-specific ranking-based loss function. Our
7.1 Effectsoftheembeddingsub-models
Hybridmethods(FCM+linear)getcomparablere-
sults to theirs. Note that their base compositional Wenextinvestigatetheeffectsofdifferenttypesof
model results without any task-specific enhance- featuresonFCMusingablationtestsonACE2005
ments, i.e. RelEmb with word2vec embeddings (Table 5.) We focus on FCM alone with the fea-
andCR-CNNwithlog-loss,arestilllowerthanthe ture templates of Table 2. Additionally, we show
best FCM result. We believe that FCM can be also results of using only the head embedding features
improved with these task-specific enhancements, from Nguyen and Grishman (2014) (HeadOnly).
e.g. replacing the word embeddings to the task- Not surprisingly, the HeadOnly model performs
specific ones from (Hashimoto et al., 2015) in- poorly (F1 score = 14.30%), showing the impor-
creases the result to 83.7% (see §7.2 for details). tanceofourrichbinaryfeatureset. Amongallthe
We leave the application of ranking-based loss to featurestemplates,removingHeadEmbresultsin
futurework. the largest degradation. The second most im-
1780
FeatureSet Prec Rec F1 Embeddings Model F1
HeadOnly 31.67 9.24 14.30 RelEmb 81.8
w2v-enwiki-d300
FCM 69.17 56.73 62.33 (2)FCM(log-bilinear) 83.4
-HeadEmb 66.06 47.00 54.92 RelEmb 82.8
-Context 70.89 55.27 62.11 task-specific-d100 RelEmb+linear 83.5
-In-between 66.39 51.86 58.23 (2)FCM(log-bilinear) 83.7
-On-path 69.23 53.97 60.66 CR-CNN 82.7
w2v-enwiki-d400
FCM-EntityTypes 71.33 34.68 46.67 (2)FCM(log-bilinear) 83.0
DepNN 83.6
w2v-nyt-d200
Table5: AblationtestofFCMondevelopmentset. (2)FCM(log-bilinear) 83.0
Table 6: Evaluation of FCMs with different word
portant feature template is In-between, while embeddingsonSemEval2010Task8.
Context features have little impact. Remov-
ing all entity type features (t ) does significantly
h i tion suggests that the other compositional models
worse than the full model, showing the value of
may also benefit from the work of Hashimoto et
ourentitytypefeatures.
al.(2015).
7.2 Effectsofthewordembeddings
8 RelatedWork
Good word embeddings are critical for both FCM
Compositional Models for Sentences In order
and other compositional models. In this section,
to build a representation (embedding) for a sen-
we show the results of FCM with embeddings
tence based on its component word embeddings
usedtoinitializeotherrecentstate-of-the-artmod-
andstructuralinformation,recentworkoncompo-
els. Thoseembeddingsincludethe300-dbaseline
sitionalmodels(stemmingfromthedeeplearning
embeddings trained on English Wikipedia (w2v-
community) has designed model structures that
enwiki-d300) and the 100-d task-specific embed-
dings(task-specific-d100)10 fromtheRelEmbpa- mimic the structure of the input. For example,
these models could take into account the order of
per (Hashimoto et al., 2015), the 400-d embed-
the words (as in Convolutional Neural Networks
dings from the CR-CNN paper (dos Santos et al.,
(CNNs)) (Collobert et al., 2011) or build off of
2015). Moreover, we list the best result (DepNN)
an input tree (as in Recursive Neural Networks
in Liu et al. (2015), which uses the same embed-
(RNNs) or the Semantic Matching Energy Func-
dings as ours. Table 6 shows the effects of word
tion)(Socheretal.,2013b;Bordesetal.,2012).
embeddingsonFCMandprovidesrelativecompar-
Whilethesemodelsworkwellonsentence-level
isons between FCM and the other state-of-the-art
representations, the nature of their designs also
models. We use the same hyperparameters and
limitsthemtofixedtypesofsubstructuresfromthe
numberofiterationsinTable4.
annotated sentence, such as chains for CNNs and
The results show that using different embed-
treesforRNNs. Suchmodelscannotcapturearbi-
dings to initialize FCM can improve F1 beyond
trarycombinationsoflinguisticannotationsavail-
our previous results. We also find that increas-
able for a given task, such as word order, depen-
ing the dimension of the word embeddings does
dency tree, and named entities used for relation
not necessarily lead to better results due to the
extraction. Moreover,theseapproachesignorethe
problem of over-fitting (e.g.w2v-enwiki-d400 vs.
differences in functions between words appearing
w2v-enwiki-d300). With the same initial embed-
indifferentroles. Thisdoesnotsuitmoregeneral
dings, FCM usuallygetsbetterresultswithoutany
substructurelabelingtasksinNLP,e.g. thesemod-
changes to the hyperparameters than the compet-
elscannotbedirectlyappliedtorelationextraction
ing model, further confirming the advantage of
since they will output the same result for any pair
FCM at the model-level as discussed under Ta-
ofentitiesinasamesentence.
ble 4. The only exception is the DepNN model,
which gets better result than FCM on the same Compositional Models with Annotation Fea-
embeddings. The task-specific embeddings from
tures To tackle the problem of traditional com-
(Hashimoto et al., 2015) leads to the best perfor-
positional models, Socher et al. (2012) made the
mance (an improvement of 0.7%). This observa-
RNNmodelspecifictorelationextractiontasksby
working on the minimal sub-tree which spans the
10In the task-specific setting, FCM will represent entity
wordsandcontextwordswithseparatesetsofembeddings. twotargetentities. However,thesespecializations
1781
torelationextractiondoesnotgeneralizeeasilyto task-specific word embeddings, and dos Santos et
othertasksinNLP.Therearetwowaystoachieve al. (2015) proposed a ranking-based loss function
suchspecializationinamoregeneralfashion: forrelationclassification.
1. Enhancing Compositional Models with Fea-
9 Conclusion
tures. A recent trend enhances compositional
models with annotation features. Such an ap-
We have presented FCM, a new compositional
proach has been shown to significantly improve
model for deriving sentence-level and substruc-
over pure compositional models. For example,
ture embeddings from word embeddings. Com-
Hermann et al. (2014) and Nguyen and Grishman
pared to existing compositional models, FCM can
(2014) gave different weights to words with dif-
easily handle arbitrary types of input and handle
ferent syntactic context types or to entity head
globalinformationforcomposition,whileremain-
words with different argument IDs. Zeng et al.
ing easy to implement. We have demonstrated
(2014) use concatenations of embeddings as fea-
thatFCMaloneattainsnearstate-of-the-artperfor-
tures in a CNN model, according to their posi-
mances on several relation extraction tasks, and
tions relative to the target entity mentions. Be-
in combination with traditional feature based log-
linkov et al. (2014) enrich embeddings with lin-
linearmodelsitobtainsstate-of-the-artresults.
guistic features before feeding them forward to a
Our next steps in improving FCM focus on en-
RNN model. Socher et al. (2013a) and Hermann
hancementsbasedontask-specificembeddingsor
and Blunsom (2013) enhanced RNN models by
loss functions as in Hashimoto et al. (2015; dos
refining the transformation matrices with phrase
Santos et al. (2015). Moreover, as the model pro-
typesandCCGsupertags.
vides a general idea for representing both sen-
2. Engineering of Embedding Features. A dif-
tences and sub-structures in language, it has the
ferentapproachtocombiningtraditionallinguistic
potential to contribute useful components to vari-
featuresandembeddingsishand-engineeringfea-
ous tasks, such as dependency parsing, SRL and
tures with word embeddings and adding them to
paraphrasing. Also as kindly pointed out by one
log-linearmodels. Suchapproacheshaveachieved
anonymous reviewer, our FCM can be applied to
state-of-the-art results in many tasks including
the TAC-KBP (Ji et al., 2010) tasks, by replac-
NER, chunking, dependency parsing, semantic
ingthetrainingobjectivetoamulti-instancemulti-
rolelabeling,andrelationextraction(Milleretal.,
labelone(e.g. Surdeanuetal.(2012)). Weplanto
2004; Turian et al., 2010; Koo et al., 2008; Roth
explore the above applications of FCM in the fu-
and Woodsend, 2014; Sun et al., 2011; Plank and
ture.
Moschitti,2013). RothandWoodsend(2014)con-
sidered features similar to ours for semantic role
Acknowledgments
labeling.
However, in prior work both of above ap- Wethanktheanonymousreviewersfortheircom-
proaches are only able to utilize limited informa- ments, and Nicholas Andrews, Francis Ferraro,
tion,usuallyonepropertyforeachword. Yetthere and Benjamin Van Durme for their input. We
maybedifferentusefulpropertiesofawordwhich thank Kazuma Hashimoto, C´ıcero Nogueira dos
cancontributetotheperformancesofthetask. By Santos, Bing Xiang and Bowen Zhou for sharing
contrast, our FCM can easily utilize these features their word embeddings and many helpful discus-
withoutchangingthemodelstructures. sions. Mo Yu is supported by the China Scholar-
In order to better utilize the dependency anno- shipCouncilandbyNSFC61173073.
tations,recentlyworkbuilttheirmodelsaccording
to the dependency paths (Ma et al., 2015; Liu et
References
al., 2015), which share similar motivations to the
usageofOn-pathfeaturesinourwork. YonatanBelinkov,TaoLei,ReginaBarzilay,andAmir
Globerson. 2014. Exploring compositional archi-
Task-SpecificEnhancementsforRelationClas- tectures and word vector representations for prepo-
sification Anorthogonaldirectionofimproving sitional phrase attachment. Transactions of the As-
sociationforComputationalLinguistics,2:561–572.
compositionalmodelsforrelationclassificationis
toenhancethemodelswithtask-specificinforma-
Antoine Bordes, Xavier Glorot, Jason Weston, and
tion. Forexample,Hashimotoetal.(2015)trained Yoshua Bengio. 2012. A semantic matching en-
1782
ergyfunctionforlearningwithmulti-relationaldata. Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
MachineLearning,pages1–27. and Houfeng WANG. 2015. A dependency-based
neural network for relation classification. In Pro-
Massimiliano Ciaramita and Yasemin Altun. 2006.
ceedingsofthe53rdAnnualMeetingoftheAssoci-
Broad-coveragesensedisambiguationandinforma-
ationforComputationalLinguisticsandthe7thIn-
tionextractionwithasupersensesequencetagger. In
ternational Joint Conference on Natural Language
EMNLP2006,pages594–602,July.
Processing (Volume 2: Short Papers), pages 285–
290,Beijing,China,July.AssociationforComputa-
RonanCollobert,JasonWeston,Le´onBottou,Michael
tionalLinguistics.
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
MingboMa,LiangHuang,BowenZhou,andBingXi-
scratch. JMLR,12:2493–2537.
ang. 2015. Dependency-basedconvolutionalneural
Cicero dos Santos, Bing Xiang, and Bowen Zhou. networks for sentence embedding. In Proceedings
2015. Classifying relations by ranking with con- of the 53rd Annual Meeting of the Association for
volutional neural networks. In Proceedings of the ComputationalLinguisticsandthe7thInternational
53rdAnnualMeetingoftheAssociationforCompu- Joint Conference on Natural Language Processing
tational Linguistics and the 7th International Joint (Volume 2: Short Papers), pages 174–179, Beijing,
Conference on Natural Language Processing (Vol- China,July.AssociationforComputationalLinguis-
ume 1: Long Papers), pages 626–634, Beijing, tics.
China,July.AssociationforComputationalLinguis-
ChristopherD.Manning,MihaiSurdeanu,JohnBauer,
tics.
Jenny Finkel, Steven J. Bethard, and David Mc-
KazumaHashimoto,PontusStenetorp,MakotoMiwa, Closky. 2014. The Stanford CoreNLP natural lan-
and Yoshimasa Tsuruoka. 2015. Task-oriented guage processing toolkit. In Proceedings of 52nd
learning of word embeddings for semantic relation Annual Meeting of the Association for Computa-
classification. arXivpreprintarXiv:1503.00095. tional Linguistics: System Demonstrations, pages
55–60.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O´ Se´aghdha, Sebastian Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
Pado´, Marco Pennacchiotti, Lorenza Romano, and rado,andJeffreyDean. 2013. Distributedrepresen-
Stan Szpakowicz. 2010. Semeval-2010 task tationsofwordsandphrasesandtheircomposition-
8: Multi-way classification of semantic relations ality. arXivpreprintarXiv:1310.4546.
between pairs of nominals. In Proceedings of
SemEval-2Workshop. Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
Karl Moritz Hermann and Phil Blunsom. 2013. The
criminative training. In Susan Dumais, Daniel
role of syntax in vector space models of composi-
Marcu, and Salim Roukos, editors, HLT-NAACL
tionalsemantics. InAssociationforComputational
2004: Main Proceedings. Association for Compu-
Linguistics,pages894–904.
tationalLinguistics.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
Alexis Mitchell, Stephanie Strassel, Shudong Huang,
andKuzmanGanchev. 2014. Semanticframeiden-
and Ramez Zakhary. 2005. Ace 2004 multilin-
tification with distributed word representations. In
gual training corpus. Linguistic Data Consortium,
Proceedingsofthe52ndAnnualMeetingoftheAs-
Philadelphia.
sociationforComputationalLinguistics(Volume1:
Long Papers), pages 1448–1458, Baltimore, Mary-
Andriy Mnih and Geoffrey Hinton. 2007. Three new
land, June. Association for Computational Linguis-
graphicalmodelsforstatisticallanguagemodelling.
tics.
InProceedingsofthe24thinternationalconference
HengJi,RalphGrishman,HoaTrangDang,KiraGrif- onMachinelearning,pages641–648.ACM.
fitt, and Joe Ellis. 2010. Overview of the tac 2010
Thien Huu Nguyen and Ralph Grishman. 2014. Em-
knowledge base population track. In Third Text
ploying word representations and regularization for
AnalysisConference(TAC2010).
domain adaptation of relation extraction. In Pro-
Terry Koo, Xavier Carreras, and Michael Collins. ceedingsofthe52ndAnnualMeetingoftheAssocia-
2008. Simplesemi-superviseddependencyparsing. tionforComputationalLinguistics(Volume2:Short
In Proceedings of ACL-08: HLT, pages 595–603, Papers), pages 68–74, Baltimore, Maryland, June.
Columbus, Ohio, June. Association for Computa- AssociationforComputationalLinguistics.
tionalLinguistics.
ThienHuuNguyenandRalphGrishman. 2015. Rela-
Qi Li and Heng Ji. 2014. Incremental joint extrac- tionextraction: Perspectivefromconvolutionalneu-
tion of entity mentions and relations. In Proceed- ral networks. In Proceedings of NAACL Workshop
ingsofthe52ndAnnualMeetingoftheAssociation onVectorSpaceModelingforNLP.
forComputationalLinguistics(Volume1: LongPa-
pers), pages 402–412, Baltimore, Maryland, June. Thien Huu Nguyen, Barbara Plank, and Ralph Gr-
AssociationforComputationalLinguistics. ishman. 2015. Semantic representations for do-
1783
main adaptation: A case study on the tree kernel- MethodsinNaturalLanguageProcessingandCom-
based method for relation extraction. In Proceed- putationalNaturalLanguageLearning,pages455–
ings of the 53rd Annual Meeting of the Association 465.AssociationforComputationalLinguistics.
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro- JosephTurian,LevRatinov,andYoshuaBengio. 2010.
cessing (Volume 1: Long Papers), pages 635–644, Wordrepresentations: asimpleandgeneralmethod
Beijing,China,July.AssociationforComputational for semi-supervised learning. In Association for
Linguistics. ComputationalLinguistics,pages384–394.
Robert Parker, David Graff, Junbo Kong, Ke Chen, Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2011. English gigaword and Kazuaki Maeda. 2006. ACE 2005 multilin-
fifth edition, june. Linguistic Data Consortium, gual training corpus. Linguistic Data Consortium,
LDC2011T07. Philadelphia.
Barbara Plank and Alessandro Moschitti. 2013. Em- MoYuandMarkDredze. 2015. Learningcomposition
bedding semantic similarity in tree kernels for do- modelsforphraseembeddings. Transactionsofthe
main adaptation of relation extraction. In Proceed- Association for Computational Linguistics, 3:227–
ings of the 51st Annual Meeting of the Association 242.
forComputationalLinguistics(Volume1: LongPa-
pers), pages 1498–1507, Sofia, Bulgaria, August. MoYu,MatthewR.Gormley,andMarkDredze. 2015.
AssociationforComputationalLinguistics. Combining word embeddings and feature embed-
dings for fine-grained relation extraction. In Pro-
Bryan Rink and Sanda Harabagiu. 2010. Utd: Clas- ceedingsofNAACL.
sifyingsemanticrelationsbycombininglexicaland
semanticresources. InProceedingsofthe5thInter- Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
national Workshop on Semantic Evaluation, pages andJunZhao. 2014. Relationclassificationviacon-
256–259, Uppsala, Sweden, July. Association for volutional deep neural network. In Proceedings of
ComputationalLinguistics. COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
Michael Roth and Kristian Woodsend. 2014. Com- pages 2335–2344, Dublin, Ireland, August. Dublin
positionofwordrepresentationsimprovessemantic City University and Association for Computational
rolelabelling. InEMNLP. Linguistics.
Richard Socher, Brody Huval, Christopher D. Man- GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
ning,andAndrewY.Ng. 2012. Semanticcomposi- 2005. Exploring various knowledge in relation ex-
tionalitythroughrecursivematrix-vectorspaces. In traction. InAssociationforComputationalLinguis-
Proceedingsofthe2012JointConferenceonEmpir- tics,pages427–434.
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211,JejuIsland,Korea,July.Associationfor
ComputationalLinguistics.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In In Proceedings of the
ACLconference.Citeseer.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, ChristopherD.Manning, AndrewNg, and
Christopher Potts. 2013b. Recursive deep models
forsemanticcompositionalityoverasentimenttree-
bank. In Empirical Methods in Natural Language
Processing,pages1631–1642.
AngSun,RalphGrishman,andSatoshiSekine. 2011.
Semi-supervisedrelationextractionwithlarge-scale
word clustering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
521–529,Portland,Oregon,USA,June.Association
forComputationalLinguistics.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedingsofthe2012JointConferenceonEmpirical
1784

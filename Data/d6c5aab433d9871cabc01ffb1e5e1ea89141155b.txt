KERPLE: Kernelized Relative Positional Embedding
for Length Extrapolation
Ta-ChungChiâˆ— Ting-HanFanâˆ—
CarnegieMellonUniversity PrincetonUniversity
tachungc@andrew.cmu.edu tinghanf@princeton.edu
PeterJ.Ramadge AlexanderI.Rudnicky
PrincetonUniversity CarnegieMellonUniversity
ramadge@princeton.edu air@cs.cmu.edu
Abstract
Relativepositionalembeddings(RPE)havereceivedconsiderableattentionsince
RPEseffectivelymodeltherelativedistanceamongtokensandenablelengthex-
trapolation. WeproposeKERPLE,aframeworkthatgeneralizesrelativeposition
embeddingforextrapolationbykernelizingpositionaldifferences. Weachievethis
goalusingconditionallypositivedefinite(CPD)kernels,aclassoffunctionsknown
forgeneralizingdistancemetrics. Tomaintaintheinnerproductinterpretationof
self-attention,weshowthataCPDkernelcanbetransformedintoaPDkernelby
addingaconstantoffset.ThisoffsetisimplicitlyabsorbedintheSoftmaxnormaliza-
tionduringself-attention. ThediversityofCPDkernelsallowsustoderivevarious
RPEsthatenablelengthextrapolationinaprincipledway. Experimentsdemon-
stratethatthelogarithmicvariantachievesexcellentextrapolationperformance
onthreelargelanguagemodelingdatasets. Ourimplementationandpretrained
checkpointsarereleasedathttps://github.com/chijames/KERPLE.git.
1 Introduction
Transformer-basedmodelshaveexcelledinvariousnaturallanguageprocessingtaskssuchaschat-
bot [Roller et al., 2021], code completion [Chen et al., 2021a], and paper abstract summariza-
tion[Zhangetal.,2020]. Thesesequencemodelingtasksoftenrequirethemodeltooperatewellon
significantlylongertextsequencesthanthefixedmaximumlengthLusedattrainingtime. Training
(orretraining)themodelusingasubstantiallylargervalueofLisofteninfeasiblesincethetrans-
formertrainingcostisO(L2). Hence,onedesiresatransformerthatcontinuestoperformwellon
longersequencesthanthoseusedduringtraining;i.e.,performlengthextrapolationatinferencetime.
Mosttransformerdesignsdonothavethisproperty[Pressetal.,2022].Whilerecentworkonabsolute
positionalembeddingsdemonstratedtheextrapolationability[Kiyonoetal.,2021,Likhomanenko
etal.,2021],itisbelievedthatrelativepositionalembeddingsaremorerobusttoinputlengthchange
[Likhomanenkoetal.,2021],forexample,ALiBi[Pressetal.,2022]andT5[Raffeletal.,2020].
Hence,wearemotivatedtostudytheinnerworkingsofrelativepositionalembeddings.
Relativepositionalembeddings(RPE)encodetheideaofshift-invariance: foranyshiftp,(m+p)âˆ’
(n+p)=mâˆ’n. Itisoftenaddeddirectlytotheself-attentionmatrixbeforeSoftmaxnormalization
[Chenetal.,2021b]. Inspiredbyshift-invarianceandtheabilityofakerneltodefineasimilarity
function,therehavebeenstudiesonshift-invariantkernelsforRPE[WennbergandHenter,2021]
with a focus on Gaussian kernel. However, in our preliminary experiments, the Gaussian kernel
âˆ—
Equalcontribution
36thConferenceonNeuralInformationProcessingSystems(NeurIPS2022).
2202
tcO
31
]LC.sc[
2v12990.5022:viXra
Figure 1: The 3-Para-Log Variant of Our KERPLE Framework. a, b, and p are learnable
parametersineachattentionheadsharedacrosslayers.Since#ofheadsisH,thereare3Â·H learnable
parameters. Thelearnableparametersaretrainedwithlength-3sequences. Attheinferencetime,the
lastrow(indashedsquares)becomesactive,andthemodelextrapolatestolength-4sequences. Note
wefocusoncausallanguagemodelingfollowingALiBi,sothematricesaretriangular.
ð‘ž1ð‘˜1 ð‘Žâˆ™0ð‘
ð‘ž2ð‘˜1 ð‘ž2ð‘˜2 ð‘Žâˆ™1ð‘ ð‘Žâˆ™0ð‘
âˆ’ð‘âˆ™log(1+ )
ð‘ž3ð‘˜1 ð‘ž3ð‘˜2 ð‘ž3ð‘˜3 ð‘Žâˆ™2ð‘ ð‘Žâˆ™1ð‘ ð‘Žâˆ™0ð‘
ð‘ž4ð‘˜1 ð‘ž4ð‘˜2 ð‘ž4ð‘˜3 ð‘ž4ð‘˜4 ð‘Žâˆ™3ð‘ ð‘Žâˆ™2ð‘ ð‘Žâˆ™1ð‘ ð‘Žâˆ™0ð‘
demonstrates limited length extrapolation ability (see Appendix A.3). Hence, a distinct class of
shift-invariantkernelsisneededtoachieveadequatelengthextrapolation.
Tothisend,wenoteasetofwell-establishedconditionallypositivedefinite(CPD)kernelssuitable
formodelingdistancemetrics[SchÃ¶lkopf,2000]. However,CPDkernelsdonotconformtoaninner
product. We can remedy this issue by transforming a CPD kernel into a PD kernel by adding a
sufficientlylargeconstant. ThisconstantoffsetissubsequentlyabsorbedimplicitlyintheSoftmax
normalization(seethediscussionbelowEq.(2)). Forexample,ALiBiimplicitlyadmitsaPDkernel
oftheformcâˆ’|mâˆ’n|(seetheendofsection4),whichisreducedtoaCPDkernelâˆ’|mâˆ’n|. The
CPDkernelandSoftmaxnormalizationcombinationopensthedoortoaseaofpossibleCPDkernels.
Weinvestigatestructuresfromthisclassthatexhibitastronglengthextrapolationability,likeALiBi.
Our main result is a framework for KErnelize Relative Positional Embedding for Length
Extrapolation(KERPLE).Theframeworkelucidateskeyprinciplesthatencouragethelengthextrap-
olationproperty. WeshowthatALiBiisaparticularinstancewithinourframework. Oursubsequent
experimentssuggestthattheproposedmethodyieldsbetterlengthextrapolationonlargedatasets
suchasOpenWebText2,GitHub,andArXiv.
2 BackgroundandRelatedWork
2.1 Preliminary
Let{w }L betheinputtokenstoatransformermodel,whereListhetotalnumberoftokens.Each
m m=1
w isascalarandisusedtoindextheembeddingvectore âˆˆRdastheinputtothetransformer. A
m m
transformerconvertseache intoquery,key,andvaluevectorsinRd: q =W e ,k =W e ,
m m q m m k m
v =W e ,whereW ,W ,W âˆˆRdÃ—darelearnablematrices. Then,theself-attentionmodule
m v m q k v
computesthescaledattentionscoresandgeneratestheoutputvectoro atpositionmas:
m
âˆš
a =
exp(q m(cid:62)k n/ d âˆš)
, o =
(cid:88)L
a v .
m,n (cid:80)L exp(q(cid:62)k / d) m m,n n
i=1 m i n=1
Sincetheoperationisposition-agnostic,itisbelievedthatpositionalinformationhelpsmodeltoken
interactions[Vaswanietal.,2017],whichwesurveyinthenextsubsection.
2.2 PositionalEmbedding
Absolute. Absolutepositionalembeddingsassignapositionalvectorp toeachpositionmand
m
addsp totheembeddingvectore . Theveryfirstversionofwhichisthepredefinedsinusoidal
m m
function[Vaswanietal.,2017]. FollowedbythesuccessofBERT[Devlinetal.,2019],learnable
absolutepositionalembeddingshavebeenappliedtothetaskofmaskedlanguagemodeling[Devlin
etal.,2019,Liuetal.,2019,Clarketal.,2020,Lanetal.,2020],Autoregressive-decoding[Radford
et al., 2018, 2019], and sequence-to-sequence [Gehring et al., 2017, Lewis et al., 2019] settings.
Recent work studied ways to extrapolate sinusoidal positional embeddings to longer sequences
byrandomlyshiftingabsolutepositionsduringtraining[Kiyonoetal.,2021]oraugmentingwith
continuoussignals[Likhomanenkoetal.,2021].
2
Relative. Asopposedtothemodelingofabsolutepositionm,relativepositionalembeddings(RPE)
thatmodelthepositionaldifferencemâˆ’nhasbecomepopularintheliterature[Shawetal.,2018,
Huangetal.,2019,Daietal.,2019,Yangetal.,2019,Huangetal.,2020,Heetal.,2021,Keetal.,
2021,Chenetal.,2021b]. Inparticular,theT5modelthatconsidersbucketedrelativedistancesand
log-binninghasbeenshowntoperformwellonvarioustransformerarchitectures[Raffeletal.,2020].
Rotary positional embedding [Su et al., 2021] encodes the position with rotations: f(q ,m) =
m
R q whereR isarotationmatrixwithanglesproportionaltom. Withtherotationâ€™sproperty,
m m m
thequery-keyproductexhibitsapositionaldifference: f(q ,m)(cid:62)f(k ,n)=q(cid:62)R k .
m n m nâˆ’m n
WenotethattheoverviewabovefocusesontheNLPdomain. Recentworkhasappliedpositional
embeddingstootherdomainssuchasvision[Wuetal.,2021a]andspeech[Likhomanenkoetal.,
2021]. Asurveycanbefoundin[Dufteretal.,2022].
2.3 KernelanditsApplicationinTransformer
Thekerneltrickisaclassicapproachtogeneralizetheinnerproducttohighdimensionalspaces
[Mikaetal.,1998,SchÃ¶lkopf,2000,Leslieetal.,2001,Dhillonetal.,2004,Takedaetal.,2007]. In
thecontextoftransformers,therehasbeeninterestinapplyingkernelstotheself-attentionstructure
toenhancetheperformance. Examplesofsuchworkincludekernelforpositionalembeddings[Tsai
etal.,2019,Wuetal.,2021b,WennbergandHenter,2021,Luoetal.,2021]. Anotherlineofresearch
leveragesthekernelâ€™sfeaturemap[RahimiandRecht,2007]tolinearizetheself-attentionmodule
andreducethecomputationalcost[Katharopoulosetal.,2020,Chenetal.,2021c,Xiongetal.,2021,
Pengetal.,2021,Choromanskietal.,2021,Qinetal.,2022].
3 TheoreticalFoundationsofCPDKernels
3.1 PDandCPDKernels
Inthiswork,weuseshift-invariantconditionallypositivedefinite(CPD)kernelstomodeltheeffect
of relative positional differences. We propose this formulation because the notion of relative is
modeled by a shift-invariant function: a bivariate function k over two positions (m,n) such that
k(m,n)=f(mâˆ’n)forsomeunivariatef. Thenotionofpositionaldifferencemâˆ’nisgeneralized
bytheCPDkernel. WereviewthedefinitionsofPDandCPDkernelsbelow.
Definition 1 (PD Kernel). A (real) symmetric function k : X Ã— X â†’ R is a positive definite
kernelifforanyintegerN andany{x âˆˆX}N ,{c âˆˆR}N ,thequadraticformisnonnegative:
i i=1 i i=1
(cid:80)N (cid:80)N
c c k(x ,x )â‰¥0.
i=1 j=1 i j i j
Definition2(CPDKernel). A(real)symmetricfunctionkËœ :X Ã—X â†’Risaconditionallypositive
definite kernel if for any integer N and any {x âˆˆ X}N , the quadratic form is conditionally
i i=1
nonnegative: (cid:80)N (cid:80)N c c kËœ(x ,x )â‰¥0for{c âˆˆR}N with(cid:80)N c =0.
i=1 j=1 i j i j i i=1 i=1 i
Fact1(Bergetal.[1984]andProp. 5ofSchÃ¶lkopf[2000]). LetkËœ :X Ã—X â†’(âˆ’âˆž,0]beaCPD
kernelwithkËœ(x,x)=0âˆ€xâˆˆX. Then,thereexistsaHilbertspaceHandamappingÏ†:X â†’H
suchthat(cid:107)Ï†(x)âˆ’Ï†(x(cid:48))(cid:107)2 =âˆ’kËœ(x,x(cid:48)).
Fact1suggeststhatCPDkernelsgeneralizedistancemetricstohighdimensionalspaces. Sincewe
areinterestedinpositionaldifferences,weexaminemodelingthedistancebetweenpositionsusing
CPDkernels.
However, Fact1alsoimpliesthatCPDkernelsdonotencodeinnerproductsasrequiredbyself-
attentionforthecomputationofpairwiserelations. PDkernelsrepresentinnerproducts. Tobetter
understandtheeffectofCPDkernelsonself-attention,weneedtoestablishrelationsbetweenCPD
andPDkernels. AsnotedinSchÃ¶lkopf[2000],ifonetakesanyPDkernelandoffsetsitbyaconstant,
theresultisatleastaCPDkernel. Inthenextsubsection,weshowthattheconverseisnearlytrue:
ifkËœisCPD,soisc+kËœforlargeenoughcâˆˆR(Lemma1). Therefore,wemaygeneratetheCPD
kernelsofinterestandtransformthemintoPDkernelsifneeded.
3
3.2 ConstructingPDKernelsFromCPDKernelsviaConstantShifts
Inthissubsection,wereviewafewpropertiesofCPDkernelsandusethesetogenerateavarietyof
CPDkernels. Then,wepresentalemmathattransformsCPDkernelsintoPDkernelsviaconstant
shifts. ThisenablestheproductionofafamilyofPDkernelsfromCPDkernels. Finally,wepresent
our critical observation that the exact value of the constant shift is not needed, thanks to a nice
propertyofSoftmaxnormalization.
BelowaresomeimportantfactsaboutCPDkernels.
Fact2(ScalingandSummation). IfkËœ andkËœ areCPD,thensoareaÂ·kËœ (fora>0)andkËœ +kËœ .
1 2 1 1 2
Fact3(Bergetal.[1984]andProp. 4ofSchÃ¶lkopf[2000]). IfkËœ :X Ã—X â†’(âˆ’âˆž,0]isCPD,then
soareâˆ’(âˆ’kËœ)Î±for0<Î±<1andâˆ’log(1âˆ’kËœ).
Fact4(Page3ofSchÃ¶lkopf[2000]). Thenegativesquareddistanceâˆ’(cid:107)xâˆ’x(cid:48)(cid:107)2isCPD.
ThethreeFactsabovejointlyyieldarichfamilyofCPDkernelsasshownbelow.
Corollary1. ThefollowingareCPDkernels.
(a) kËœ(x,x(cid:48))=âˆ’a(cid:107)xâˆ’x(cid:48)(cid:107)pwith0<pâ‰¤2anda>0.
(b) kËœ(x,x(cid:48))=âˆ’bÂ·log(1+a(cid:107)xâˆ’x(cid:48)(cid:107)p)with0<pâ‰¤2anda,b>0.
WenotethatitispossibletokeepiteratingbetweenFact2and3andgeneratemorecomplicated
examples,e.g.,âˆ’a(cid:107)xâˆ’x(cid:48)(cid:107)pâˆ’bÂ·log(1+a(cid:107)xâˆ’x(cid:48)(cid:107)p)orâˆ’bÂ·log(1+a(cid:107)xâˆ’x(cid:48)(cid:107)p)cfor0<c<1.
However, sincerelativepositionalembeddingsareofourinterest, weonlyconsidersimpleCPD
kernels. Thosewithcomplicatedformsaredeferredtofuturework.
NowthatCorollary1haspresentedafewclassofCPDkernels,weprovealemma(inAppendixA.1)
thatconstructsPDkernelsfromCPDkernelsthroughshifting. LaterinEq.(2),wewillseethatthe
shiftingconstructioniscombinedneatlywiththeSoftmaxnormalizationofself-attention.
Lemma 1 (CPD ShiftLemma. Proof in Appendix A.1). Let kËœ : X Ã—X â†’ R be a CPDkernel.
Thereexistscâ‰¥0suchthatc+kËœisaPDkernel.
Lemma 1 implies the CPD kernels in Corollary 1 can be made PD if a large enough constant is
added. Forexample,câˆ’(cid:107)xâˆ’x(cid:48)(cid:107)pforlargeenoughc. AlthoughLemma1doesnothaveanexplicit
constructionofc,thankstotheshift-invariantpropertyoftheSoftmaxnormalization,wecanleaveit
asanunder-determinedconstantinourpositionalembeddingdesign(Eq.(1)insection4). Givena
setoftestpoints{x }N ,onecandoageometricsequencesearch1 tosearchforacsuchthatthe
i i=1
N Ã—N matrix[c+kËœ(x ,x )]N (cid:23)0. Hence,wedonotneedthevalueofc,butwecancompute
i j i,j=1
itifneeded,e.g.,derivingthefeaturemapofc+kËœ.
Alternative Proof ofcccâˆ’âˆ’âˆ’(cid:107)(cid:107)(cid:107)xxxâˆ’âˆ’âˆ’xxx(cid:48)(cid:48)(cid:48)(cid:107)(cid:107)(cid:107)ppp. While the CPD shift lemma is convenient, one can prove
câˆ’(cid:107)xâˆ’x(cid:48)(cid:107)pisPDforlargeenoughcusingakernelrepresentationtheoreminSchoenberg[1938].
SeeAppendixA.2fordetails.
4 KernelizedRelativePositionalEmbedding
Let{q }L and{k }L betheinputqueriesandkeys. Let(r ,...,r )belearnableparameters.
m m=1 n n=1 1 (cid:96)
Weproposeakernelizedrelativepositionalembeddingasfollows.
âˆš
exp(cid:0) (q(cid:62)k +kËœ (m,n))/ d(cid:1)
a = m n r1,...,r(cid:96) âˆš , (1)
m,n (cid:80)L exp((q(cid:62)k +kËœ (m,i))/ d)
i=1 m i r1,...,r(cid:96)
1Bygeometricsequencesearch,wecanenlargecby2,4,8,16,andsoonuntilwefindtherequiredlarge
enoughconstant.
4
wherekËœ (m,n)isanyshift-invariantCPDkernelwith(cid:96)parameters. DuetoLemma1,Eq.(1)
r1,...,r(cid:96)
canbereformulatedintoitskernelformasfollows.
âˆš
exp(cid:0) (q(cid:62)k +c+kËœ (m,n))/ d(cid:1)
a ( =âˆ—) m n r1,...,r(cid:96) âˆš
m,n (cid:80)L exp((q(cid:62)k +c+kËœ (m,i))/ d)
i=1 m i r1,...,r(cid:96) âˆš âˆš (2)
exp(cid:0) q(cid:62)k +k (m,n))/ d(cid:1) exp(cid:0) kcomp([q ,m],[k ,n])/ d(cid:1)
Lem =ma1 m n r1,...,r(cid:96) âˆš = m n âˆš .
(cid:80)L exp(q(cid:62)k +k (m,i))/ d) (cid:80)L exp(cid:0) kcomp([q ,m],[k ,i])/ d(cid:1)
i=1 m i r1,...,r(cid:96) i=1 m i
(*)isduetotheshift-invariantpropertyoftheSoftmaxnormalization: exp(xi) = exp(xi+c)
foranycâˆˆR.
Thesecondequalitydefinesabiaskernelwhichispositive(cid:80)
dj
ee fixp n( ix tej) using(cid:80)
Lj
eex mp m(x aj+ 1c :)
k =c+kËœ . (3)
r1,...,r(cid:96) r1,...,r(cid:96)
Thelastequalityintroducesacompositekernelkcomp :Rd+1Ã—Rd+1 â†’Ras
kcomp([q ,m],[k ,n])=q(cid:62)k +k (m,n). (4)
m n m n r1,...,r(cid:96)
Interpretation. Theproposedmethodcanbeinterpretedasapplyingacompositekerneltoself-
attention. Thecompositekernelcombinestheinformationfromqueryq ,keyk ,andpositions
m n
(m,n) in a way that augments the original self-attention structure by multiplicative and additive
positionembeddings. Theaugmentationallowskcomptonotonlyretaintheoriginalq(cid:62)k butalso
m n
includepositionalinformationfromthebiaskernelk .
r1,...,r(cid:96)
PracticalChoice. Insection5.2,wefix(cid:96) = 2andexperimentontwovariantsofthecomposite
kernel,Eq.(4),wherewecallthesethepowervariantandthelogarithmicvariantofourproposed
KERPLEframework,Eq.(2). ThesearefromacombinationofCorollary1andEq.(3).
(power) kcomp([q m,m],[k n,n])=q m(cid:62)k n+câˆ’r 1|mâˆ’n|r2 withr
1
>0and0<r
2
â‰¤2.
(logarithmic) kcomp([q ,m],[k ,n])=q(cid:62)k +câˆ’r Â·log(1+r |mâˆ’n|)withr ,r >0.
m n m n 1 2 1 2
Wenotethatthesearenottheonlyvariantsofthecompositekernel. Insection5.3,weexperiment
withtwomorecomplicatedvariants,butonlyfindlowertrainingspeedsandmarginalimprovement
inperplexities(e.g.,logarithmicvariantvs. 3-para-log). Thus,basedonourstudy,thechoicesabove
holdadvantagesinbothperformanceandspeed.
ConnectiontoPriorWork. Whenthebiaskernel,Eq.(3),isatrianglekernel: câˆ’|mâˆ’n|,our
modelreducestoALiBi[Pressetal.,2022]. WennbergandHenter[2021]discussthesituationwhere
thebiaskernelisaGaussiankernel. Tsaietal.[2019]isthecasewherethereisnobiaskerneland
theattentionproductq(cid:62)k ismultipliedbyanexponentiatedinnerproductkernel,exp(x(cid:62)y). Since
m n
ALiBiisthestate-of-the-artandhasgreatinputlengthextrapolation,wewillfocusoncomparison
withALiBiinourexperiments.
The logarithmic variant has an implicit connection to T5 positional bias [Raffel et al.,
2020]. According to the official GitHub repository https://github.com/google-research/
text-to-text-transfer-transformerandtheHuggingFaceTransformer[Wolfetal.,2020],
T5biasisimplementedwithalog-binningstrategy. Foreachheadofthetransformer,theymaintaina
bucketof32learnableparametersandassigntherelativepositionalbiasb totheseparametersas
mâˆ’n
ï£±
bucket[0] ifmâˆ’n<0
ï£´ï£²
b = bucket[mâˆ’n] if0â‰¤mâˆ’n<16
mâˆ’n
ï£´ï£³bucket[min(31,(cid:98)log((mâˆ’n)/16))Â·16(cid:99)] ifmâˆ’nâ‰¥16,
log(128/16)
where(cid:98)Â·(cid:99)isthefloorfunction. Notethatthelogfactorisapproximately7.7logmâˆ’n. Therefore,T5
16
isusingalogarithmicbucketassignment,whichturnsouttoextrapolatetodifferentinputlengths.
ComparedwithT5,ourlogarithmicvariantuseslessparameters(2x12vs. 32x12)butcannotlearn
non-monotonicrelations(thelogfunctionismonotonic). WewillconductmorecomparisonswithT5
biasinourexperiments.
5
5 Experiments
5.1 DatasetandImplementationDescription
Dataset. We conduct experiments on OpenWebText2, GitHub, and ArXiv datasets gathered in
Gao et al. [2020]. OpenWebText2 includes recent content from Reddit submissions until 2020,
contentfrommultiplelanguages,documentmetadata,multipledatasetversions,andopen-source
replicationcode. GitHubincludesopen-sourcerepositorieswritteninprimarycodinglanguagessuch
asJava,C/C++,Python,andGo. ArXivincludespaperswritteninLaTexinMath,ComputerScience,
Physics,andsomerelatedfields. Thesetasksaremotivatedbythedownstreamapplicationssuch
asonlinechatting[Rolleretal.,2021],codecompletion[Chenetal.,2021a],andacademicpaper
summarization[Zhangetal.,2020].
Table1: DatasetOverview. RawSizeisthesizebeforeanyup-ordown-sampling.
OpenWebText2 GitHub ArXiv
RawSize 66.77GB 95.16GB 56.21GB
Type Internet Coding Academic
Implementation. WeadaptourmodelfromGPT-NeoX[Blacketal.,2021],atransformerimple-
mentationbytheEleutherAIteam. ThecodebaseisbasedonNVIDIAMegatronLanguageModel
[Shoeybietal.,2019]andfurtheracceleratedusingMicrosoftDeepSpeedlibrary[Rasleyetal.,2020].
OurmodelistrainedonamachinewithoneNVIDIAA100GPUwith40GBofmemory. Weadopt
almostallconfigurationsofsmallGPT-NeoX2,exceptthatwechangethetrain-micro-batch-size
to32,seq-lengthto512,andmax-position-embeddingsto512. Table2summarizestheimportant
configurationsfixedthroughoutourexperiments. Inparticular,thefloating-pointencodingissetas
Table2: 162MModelConfigurations.
#Layers HiddenSize #AttentionHeads TrainSeq. Len. #TrainableParams.
12 64 12 512 162M
Optimizer BatchSize TrainSteps Precision #TrainableParams. forRPEs
Adam(lr6e-4) 32 50,000 bfloat16 atmost36
bfloat16(BrainFloatingPoint,developedbyGoogleBrain)sothatthetrainingcanbeacceleratedby
half-precisioncomputationwithreliablestability[Kalamkaretal.,2019]. Hiddensize64meansthat
d=64inEq.(1).
5.2 ExperimentalResults(Alsoc.f. AppendixA.4toA.7)
Weconductexperimentstocoveraspectssuchasinputlengthextrapolation,applicationondifferent
domains,andcomparisonwiththepriorwork. Theseareelaboratedonbelow. (i)Motivatedbythe
inputlengthextrapolationdemonstratedin[Pressetal.,2022],wetrainourmodelwithlength512
andtestonlengthsrangingfrom512to16384. Wehopethattheemphasisonextrapolationenables
theapplicationoftransformerstolongersequences. (ii)Toevaluatetheapplicabilityofthemodel
indifferentdomains,weconductexperimentsonOpenWebText2,GitHub,andArXivdatasets. (iii)
Tovalidatetheeffectivenessofourmethod,wecompareKERPLEwithSinusoidal[Vaswanietal.,
2017],Rotary[Suetal.,2021],T5[Raffeletal.,2020],andALiBi[Pressetal.,2022].
Table 3 reports the perplexities at different extrapolation lengths. We perform non-overlapping
evaluation: Suppose text is segmented in a different manner for 512 and 1024 tokens, we have
N sentences and N/2 correspondingly to evaluate. We also perform a paired two-sided t-test to
validatethestatisticalsignificance(significancelevel=0.05). WecompareeachcandidateRPEwith
ourproposedlogarithmicvariantandmarkthecandidatewithaâ€  ifthelogvariantisstatistically
significantlybetter. Table4reportsthetrainingspeeds. Thesetablesyieldthreeconclusions. First,
withintheKERPLEframework,thelogarithmicvariantisbetterthanthepowervariant. Secondly,the
logarithmicvariantis9.7%fasterthanT5. Intermsofextrapolation,thelogarithmicvariantgenerally
2https://github.com/EleutherAI/gpt-neox/blob/main/configs/small_bf16.yml
6
doesbetterthanT5butcouldbeslightlyworsethanT5atshorterlengths. Third,thelogarithmic
variant is slightly slower than some prior work (ALiBi, Rotary, and Sinusoidal) but consistently
outperformthesemethodsatallextrapolationlengths. Moredetailsaregivenbelow.
LogarithmicVariantvs. PowerVariant. InourproposedKERPLEframework,thelogarithmic
variantisbetterthanthepowervariant. Precisely,thelogarithmicvariantis4.4%fasterandhaslower
perplexitiesacrossallextrapolationlengthsandalltasks.
LogarithmicVariantvs. T5. Intermsofspeed, thelogarithmicvariantis9.7%fasterthanT5.
In terms of extrapolation perplexity, the logarithmic variant is close to or slightly worse than T5
whentheextrapolationlengthisshorterthan2048,andconsistentlyexcelsT5atlongerextrapolation
lengths. Thetendencyofextrapolationholdsforalldatasetsevaluatedinthiswork.
LogarithmicVariantvs. ALiBi,Rotary,andSinusoidal. Thelogarithmicvariantis1.6%slower,
7.5%faster,and3.0%slowerthanALiBi,Rotary,andSinusoidal. Thespeedcomparisonmakessense
becausewerequireonlyalimitedamountoflearnableparametersforRPEs(atmost3Â·H). Also,the
logarithmicvariantconsistentlyoutperformspriorworkatallextrapolationlengthsandtasks.
Table3: PerplexityComparisononOpenWebText2,GitHub,andArXiv. Allmodelsaretrained
for50kstepswithtraininglength512andfiverandomseeds. xâ€ meansourlogvariantisstatistically
significantlybetterthanx. Thetestusedispairedtwo-sidedt-testwithÎ±=0.05.
OpenWebText2
KERPLE
Extrp. ALiBi T5 Rotary Sinusoidal
(log) (power)
512 23.9Â±0.6 23.9Â±0.6 23.9Â±0.6 23.7Â±Â±Â±0.6 24.2Â±0.6â€  33Â±1â€ 
1024 22.0Â±0.6 22.1Â±0.7 22.4Â±0.5â€  21.9Â±Â±Â±0.6 32.8Â±1.7â€  750Â±346â€ 
2048 21.6Â±Â±Â±0.3 21.9Â±0.2â€  22.5Â±0.2â€  21.7Â±0.2 62.4Â±6.1â€  5507Â±2607â€ 
4096 21.2Â±Â±Â±0.4 21.5Â±0.5â€  22.2Â±0.4â€  22.5Â±0.6â€  111Â±13.8â€  14039Â±2325â€ 
8192 21.3Â±Â±Â±0.4 21.6Â±0.4â€  22.3Â±0.3â€  25.5Â±1.3â€  185Â±18.9â€  22621Â±1927â€ 
16384 21.4Â±Â±Â±0.6 21.6Â±0.6 22.5Â±0.5â€  31.4Â±3.1â€  269Â±33.0â€  30046Â±4824â€ 
GitHub
KERPLE
Extrp. ALiBi T5 Rotary Sinusoidal
(log) (power)
512 3.40Â±0.20 3.42Â±0.20 3.42Â±0.21 3.38Â±Â±Â±0.21 3.44Â±0.20â€  4Â±0.2â€ 
1024 3.04Â±0.14 3.07Â±0.16 3.15Â±0.17â€  3.02Â±Â±Â±0.14 3.86Â±0.25â€  105Â±39â€ 
2048 2.86Â±0.10 2.90Â±0.08â€  3.13Â±0.10â€  2.84Â±Â±Â±0.09 5.94Â±0.64â€  1380Â±404â€ 
4096 2.74Â±Â±Â±0.05 2.79Â±0.06 3.04Â±0.08â€  2.78Â±0.04â€  11.1Â±1.55â€  5217Â±1118â€ 
8192 2.71Â±Â±Â±0.05 2.76Â±0.05 3.04Â±0.03â€  2.95Â±0.13â€  20.2Â±2.75â€  10081Â±3583â€ 
16384 2.75Â±Â±Â±0.16 2.76Â±0.13 3.02Â±0.13â€  3.35Â±0.27â€  31.3Â±5.20â€  16443Â±8503â€ 
ArXiv
KERPLE
Extrp. ALiBi T5 Rotary Sinusoidal
(log) (power)
512 6.07Â±0.26 6.10Â±0.26 6.12Â±0.26â€  6.03Â±Â±Â±0.26 6.07Â±0.27 43Â±44
1024 5.61Â±0.10 5.65Â±0.10â€  5.82Â±0.09â€  5.58Â±Â±Â±0.09 7.49Â±0.34â€  221Â±136â€ 
2048 5.22Â±0.12 5.26Â±0.13â€  5.71Â±0.14â€  5.21Â±Â±Â±0.14 14.2Â±1.81â€  730Â±343â€ 
4096 5.20Â±Â±Â±0.10 5.25Â±0.09 5.87Â±0.08â€  5.32Â±0.16â€  30.1Â±4.32â€  1998Â±497â€ 
8192 5.01Â±Â±Â±0.10 5.06Â±0.15 5.74Â±0.13â€  5.54Â±0.39â€  54.3Â±6.22â€  4228Â±2645â€ 
16384 5.07Â±Â±Â±0.16 5.07Â±0.19 5.78Â±0.15â€  6.25Â±0.61â€  85.4Â±7.40â€  6674Â±5696
Table4: TrainingTimeComparisononGitHub
KERPLE
ALiBi T5 Rotary Sinusoidal
(log) (power)
sec/step 0.307 0.321 0.302 0.340 0.332 0.298
7
5.3 ExperimentsonComplicatedKernels
Inadditiontothepracticalvariants(power&logarithmic)insection4,weconsidertwocomplicated
versionsofthecompositekernel,Eq.(4),asfollows.
(bias+wht) bias+weight:
kcomp([q m,m],[k n,n])=q m(cid:62)k nÂ·exp(âˆ’r 3|mâˆ’n|r4)+câˆ’r 1|mâˆ’n|r2
withr ,r >0and0<r ,r â‰¤2.
1 3 2 4
(3-para-log) 3-parameter-logarithmic:
kcomp([q m,m],[k n,n])=q m(cid:62)k n+câˆ’r 1Â·log(1+r 2|mâˆ’n|r3)
withr ,r >0and0<r â‰¤2.
1 2 3
Recallthetensorproductpropertyofakernel: ifk isakernelonX andk isakernelonY,then
1 2
k((x,y),(x(cid:48),y(cid:48))) = k (x,x(cid:48))k (y,y(cid:48))isakernelonX Ã—Y. Therefore,(bias+wht)isthesetting
1 2
wherewetrainaweightexp(âˆ’r 3|mâˆ’n|r4)andabiaskernelcâˆ’r 1|mâˆ’n|r2.q m(cid:62)k nismultipliedby
theweightkernelandthenaddedwiththebiaskernel. (3-para-log)isthesettingwhereweconsider
|mâˆ’n|r3 inthelog. Whenr
3
=1,itisreducedtothelogarithmicvariantproposedinsection4.
WepluginthesecompositekernelkcompintoourKERPLEframework,Eq.(2),andtesttheperfor-
manceoftheseRPE.Comparedwithsection5.2,Table5suggeststhatthesevariantsdonothave
clearadvantageinextrapolationperformance,e.g.,3-para-logisslightlybetterinperplexitythanthe
(two-parameter)logarithmicvariant. Thus,enlargingthecomplexityofkernelsdoesnotnecessarily
givebetterperformanceinthecontextofRPE.
Table 5: Perplexity Comparison for KERPLE with Complicated Kernels on OpenWebText2,
GitHub,andArXiv. Allmodelsaretrainedfor50kstepswithtraininglength512andfiveseeds
random. OOMmeansoutofmemory.
OpenWebText2 GitHub ArXiv
Extrp.
(bias+wht) (3-para-log) (bias+wht) (3-para-log) (bias+wht) (3-para-log)
512 24.1Â±0.6 23.8Â±0.6 3.44Â±0.21 3.40Â±0.20 6.11Â±0.27 6.06Â±0.27
1024 22.2Â±0.6 22.0Â±0.7 3.08Â±0.15 3.04Â±0.13 5.66Â±0.09 5.61Â±0.10
2048 21.9Â±0.4 21.6Â±0.2 2.90Â±0.12 2.85Â±0.10 5.28Â±0.12 5.21Â±0.12
4096 21.5Â±0.5 21.2Â±0.4 2.79Â±0.06 2.73Â±0.05 5.31Â±0.08 5.18Â±0.09
8192 21.4Â±0.5 21.3Â±0.4 2.76Â±0.03 2.68Â±0.04 5.16Â±0.18 5.00Â±0.11
16384 OOM OOM OOM OOM OOM OOM
5.4 PlotsofKernelFunctions
Weplotkernelfunctionsincludingthepower,logvariants,andALiBifordifferentheadstoseetheir
contributionstosoftmax. WeusetheGitHubdatasetfordemonstration. PleaseseeFigure2,3,and4.
BothALiBianditsgeneralizedpowervariantquicklyreachaverynegativevalue. Incontrast,thelog
variantsuccessfullydiscoversseveralflatkernels,effectivelyextendingthewindowattention. This
corroboratesourpreviousobservationthatKERPLE-logcanutilizemoredistanttokeninformation.
Figure2: KernelFunctionsofLearnedbytheLogVariant.
8
Figure3: KernelFunctionsLearnedbythePowerVariant. Notethey-axisshouldbemultipliedby
1e8,whichisaverynegativevalue.
Figure4: KernelFunctionsLearnedbyALiBi.
5.5 Position-wisePerplexityEvaluation
We plot the position-wise perplexity with evaluation length=4096 in Figure 5. Please see Ap-
pendixA.6forsimilarlength=16384result. Theevaluationisdonebymeasuringthelossateach
positionineachsequenceandaveragingoverthesequences.
WenotethatPPL@512ofKERPLE-logisthelowestamongallmodelvariants.Wecanderiveseveral
criticalobservationsforevaluationlength=4096inFigure5: First,KERPLE-logliesbelowKERPLE-
log-windowed@512,indicatingitsusageofmoredistantinformationthanwindowattention: Ifour
modeldoesnotusemoreinformationotherthanafixed-window=512,they-valuesafterposition=512
shouldoverlapwiththelinewindowedat512. Thisisclearlynotthecase. Inaddition, thePPL
ofKERPLE-logcontinuestodecreasetilltheendof4096positions(Notplateauing). Second,T5
lies below KERPLE-log-windowed@512 most of the time and fluctuates around KERPLE-log-
windowed@512 after length=3000. It is still worse than KERPLE-log. Third, ALiBi lies above
KERPLE-log-windowed@512foralmostallthepositions,indicatingthatwindowattentionmightbe
abetterchoicethanALiBi.
Althoughwindowattentionisastrongbaseline,ourKERPLE-logisalmostlikeafreelunchcompared
towindowattention: Withonly24additionallearnableparameters(2para. foreachhead),thealmost
sametrainingspeed,andthesametrainlength=512aswindowattention,itisabletoachievelower
PPLsacrossdifferentpositions.
6 ConclusionandFutureWork
Ageneralframework,KERPLE,isproposedtokernelizerelativepositionalembeddingsforlength
extrapolation. AtthecoreofthisframeworkistheapplicationofCPDkernelsandthederivationof
practicalvariants. WeshowthattheseCPDkernelscanbeimplicitlyconvertedtoPDkernels,which
9
Figure 5: Position-wise Perplexity on GitHub at Evaluation Length=4096 Compared to Win-
dowAttention@512.
Position-wise Perplexity at Evaluation Length=4096
20.0
Rotary
17.5 ALiBi
T5
15.0
KERPLE-log
KERPLE-log-windowed@512
12.5
10.0
7.5
5.0
2.5
0.0
0 1000 2000 3000 4000
Position
keep the inner product interpretation of self-attention. We also demonstrate that the logarithmic
variantachievesexceptionalextrapolationperformanceonthreelargelanguagemodelingdatasets.
Webelieveourworkpavesthewayforsomeinterestingfuturedirectionsthatresolveourlimitations.
For instance, we can consider general kernel families and model non-monotonic effects due to
positionaldifferences. Inaddition,theuseoflearnableparametersinKERPLEmightenablebetter
generalizationtoinputshigherthanone-dimensional. Lastbutnotleast,thereisalwaysroomfor
improvingmemoryefficiencybyadjustingthemodelarchitectureandtrainingprocedure.
7 BroaderImpact
Ourworkdevelopsabetterunderstandingofrelativepositionalembeddingfortransformersbased
onexpressivekernelclassesthatadaptwelltovariousdatasets. Theresultsapplytodomainswhere
thepositionalinformationishelpfulinthemodeling,e.g.,naturallanguage,programminglanguage,
andDNA/proteinsequencesforbiology/medicine. Thestudiesoftransformersmayhavepositive
economiceffectsbyenablingnewtaskswhichcannotbedonebyhumansorenhancingaccuracyand
efficiency. Butinappropriateusecanhavenegativesocietalimpacts. Theseincludejoblossdueto
automation,theethicalchallengesfromimpropertextgeneration,andtheprivacyissuesinthedata
collectionprocess. Theseimplicationsapplytoanyresearchonnaturallanguageprocessingandare
notassociatedwithanyspecificwork.
8 Acknowledgement
Wethanktheanonymousreviewersfortheirinsightfulfeedbackandsuggestions. WethankPrinceton
ResearchComputingforthetechnicalsupportontheDellaandtheAdroitclusters. Thethirdauthor
acknowledgessupportfromNSFMRIAward: 1919452.
10
ytixelpreP
egarevA
References
StephenRoller,EmilyDinan,NamanGoyal,DaJu,MaryWilliamson,YinhanLiu,JingXu,Myle
Ott,EricMichaelSmith,Y-LanBoureau,andJasonWeston. Recipesforbuildinganopen-domain
chatbot. InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationfor
ComputationalLinguistics: MainVolume,pages300â€“325,Online,April2021.Associationfor
ComputationalLinguistics.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan,HarriEdwards, YuriBurda, NicholasJoseph, GregBrockman, etal. Evaluatinglarge
languagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021a.
JingqingZhang,YaoZhao,MohammadSaleh,andPeterLiu. Pegasus: Pre-trainingwithextracted
gap-sentencesforabstractivesummarization. InInternationalConferenceonMachineLearning,
pages11328â€“11339.PMLR,2020.
OfirPress,NoahSmith,andMikeLewis. Trainshort,testlong: Attentionwithlinearbiasesenables
inputlengthextrapolation. InInternationalConferenceonLearningRepresentations,2022.
ShunKiyono,SosukeKobayashi,JunSuzuki,andKentaroInui. SHAPE:Shiftedabsoluteposition
embeddingfortransformers. InProceedingsofthe2021ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,pages3309â€“3321,OnlineandPuntaCana,DominicanRepublic,
November2021.AssociationforComputationalLinguistics.
TatianaLikhomanenko,QiantongXu,GabrielSynnaeve,RonanCollobert,andAlexRogozhnikov.
Cape: Encodingrelativepositionswithcontinuousaugmentedpositionalembeddings. InM.Ran-
zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in
NeuralInformationProcessingSystems,volume34,pages16079â€“16092.CurranAssociates,Inc.,
2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. JournalofMachineLearningResearch,21(140):1â€“67,2020.
Pu-ChinChen,HenryTsai,SrinadhBhojanapalli,HyungWonChung,Yin-WenChang,andChun-
Sung Ferng. A simple and effective positional encoding for transformers. In Proceedings of
the2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2974â€“2988,
2021b.
UlmeWennbergandGustavEjeHenter.Thecasefortranslation-invariantself-attentionintransformer-
based language models. In Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 130â€“140, Online, August 2021. Association for
ComputationalLinguistics.
BernhardSchÃ¶lkopf. Thekerneltrickfordistances. InT.Leen,T.Dietterich,andV.Tresp,editors,
AdvancesinNeuralInformationProcessingSystems,volume13.MITPress,2000.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Åukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessing
systems,30,2017.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. InProceedingsofthe2019Conferenceof
theNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage
Technologies,Volume1(LongandShortPapers),pages4171â€“4186,Minneapolis,Minnesota,June
2019.AssociationforComputationalLinguistics.
YinhanLiu, MyleOtt, Naman Goyal, JingfeiDu, MandarJoshi, DanqiChen, OmerLevy, Mike
Lewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretraining
approach. arXivpreprintarXiv:1907.11692,2019.
11
KevinClark,Minh-ThangLuong,QuocV.Le,andChristopherD.Manning. Electra: Pre-training
textencodersasdiscriminatorsratherthangenerators. InInternationalConferenceonLearning
Representations,2020.
ZhenzhongLan,MingdaChen,SebastianGoodman,KevinGimpel,PiyushSharma,andRaduSoricut.
Albert: A lite bert for self-supervised learning of language representations. In International
ConferenceonLearningRepresentations,2020.
AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever. Improvinglanguageunder-
standingbygenerativepre-training. OpenAIblog,2018.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolutional
sequencetosequencelearning. InProceedingsofthe34thInternationalConferenceonMachine
Learning-Volume70,ICMLâ€™17,page1243â€“1252.JMLR.org,2017.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy,VesStoyanov,andLukeZettlemoyer. Bart: Denoisingsequence-to-sequencepre-trainingfor
naturallanguagegeneration,translation,andcomprehension. arXivpreprintarXiv:1910.13461,
2019.
PeterShaw,JakobUszkoreit,andAshishVaswani.Self-attentionwithrelativepositionrepresentations.
In Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages
464â€“468,NewOrleans,Louisiana,June2018.AssociationforComputationalLinguistics.
Cheng-ZhiAnnaHuang,AshishVaswani,JakobUszkoreit,IanSimon,CurtisHawthorne,Noam
Shazeer,AndrewM.Dai,MatthewD.Hoffman,MonicaDinculescu,andDouglasEck. Music
transformer. InInternationalConferenceonLearningRepresentations,2019.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext. InProceedingsof
the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978â€“2988,
Florence,Italy,July2019.AssociationforComputationalLinguistics.
ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,andQuocVLe.
Xlnet: Generalizedautoregressivepretrainingforlanguageunderstanding. InAdvancesinNeural
InformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.
ZhihengHuang,DavisLiang,PengXu,andBingXiang. Improvetransformermodelswithbetterrel-
ativepositionembeddings. InFindingsoftheAssociationforComputationalLinguistics: EMNLP
2020,pages3327â€“3335,Online,November2020.AssociationforComputationalLinguistics.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. {DEBERTA}: {DECODING}-
{enhanced}{bert}{with}{disentangled}{attention}. InInternationalConferenceonLearning
Representations,2021.
GuolinKe,DiHe,andTie-YanLiu. Rethinkingpositionalencodinginlanguagepre-training. In
InternationalConferenceonLearningRepresentations,2021.
JianlinSu,YuLu,ShengfengPan,BoWen,andYunfengLiu. Roformer: Enhancedtransformerwith
rotarypositionembedding. arXivpreprintarXiv:2104.09864,2021.
KanWu,HouwenPeng,MinghaoChen,JianlongFu,andHongyangChao.Rethinkingandimproving
relativepositionencodingforvisiontransformer. In2021IEEE/CVFInternationalConferenceon
ComputerVision(ICCV),pages10013â€“10021,2021a.
PhilippDufter, MartinSchmitt, andHinrichSchÃ¼tze. PositionInformationinTransformers: An
Overview. ComputationalLinguistics,pages1â€“31,072022.
SebastianMika,BernhardSchÃ¶lkopf,AlexSmola,Klaus-RobertMÃ¼ller,MatthiasScholz,andGunnar
RÃ¤tsch. Kernelpcaandde-noisinginfeaturespaces. Advancesinneuralinformationprocessing
systems,11,1998.
12
ChristinaLeslie,EleazarEskin,andWilliamStaffordNoble. Thespectrumkernel: Astringkernel
forsvmproteinclassification. InBiocomputing2002,pages564â€“575.WorldScientific,2001.
InderjitSDhillon,YuqiangGuan,andBrianKulis. Kernelk-means: spectralclusteringandnor-
malizedcuts. InProceedingsofthetenthACMSIGKDDinternationalconferenceonKnowledge
discoveryanddatamining,pages551â€“556,2004.
HiroyukiTakeda,SinaFarsiu,andPeymanMilanfar. Kernelregressionforimageprocessingand
reconstruction. IEEETransactionsonimageprocessing,16(2):349â€“366,2007.
Yao-HungHubertTsai,ShaojieBai,MakotoYamada,Louis-PhilippeMorency,andRuslanSalakhut-
dinov. Transformerdissection: Anunifiedunderstandingfortransformerâ€™sattentionviathelens
ofkernel. InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguage
Processingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-
IJCNLP),pages4344â€“4353,HongKong,China,November2019.AssociationforComputational
Linguistics.
ChuhanWu,FangzhaoWu,andYongfengHuang. DA-transformer: Distance-awaretransformer.
In Proceedings of the 2021 Conference of the North American Chapter of the Association for
ComputationalLinguistics:HumanLanguageTechnologies,pages2059â€“2068,Online,June2021b.
AssociationforComputationalLinguistics.
ShengjieLuo,ShandaLi,TianleCai,DiHe,DinglanPeng,ShuxinZheng,GuolinKe,LiweiWang,
andTie-YanLiu. Stable,fastandaccurate: Kernelizedattentionwithrelativepositionalencoding.
AdvancesinNeuralInformationProcessingSystems,34,2021.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D.Koller,Y.Singer,andS.Roweis,editors,AdvancesinNeuralInformationProcessingSystems,
volume20.CurranAssociates,Inc.,2007.
AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFranÃ§oisFleuret. Transformersare
RNNs: Fastautoregressivetransformerswithlinearattention. InHalDaumÃ©IIIandAartiSingh,
editors,Proceedingsofthe37thInternationalConferenceonMachineLearning,volume119of
ProceedingsofMachineLearningResearch,pages5156â€“5165.PMLR,13â€“18Jul2020.
YifanChen,QiZeng,HengJi,andYunYang. Skyformer: Remodelself-attentionwithgaussian
kernelandnystr\"ommethod. AdvancesinNeuralInformationProcessingSystems,34,2021c.
YunyangXiong,ZhanpengZeng,RudrasisChakraborty,MingxingTan,GlennFung,YinLi,and
VikasSingh. NystrÃ¶mformer: AnystÃ¶m-basedalgorithmforapproximatingself-attention. In
Proceedingsofthe...AAAIConferenceonArtificialIntelligence.AAAIConferenceonArtificial
Intelligence,volume35,page14138.NIHPublicAccess,2021.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong.
Randomfeatureattention. InInternationalConferenceonLearningRepresentations,2021.
KrzysztofMarcinChoromanski,ValeriiLikhosherstov,DavidDohan,XingyouSong,AndreeaGane,
TamasSarlos,PeterHawkins,JaredQuincyDavis,AfrozMohiuddin,LukaszKaiser,DavidBen-
jaminBelanger,LucyJColwell,andAdrianWeller. Rethinkingattentionwithperformers. In
InternationalConferenceonLearningRepresentations,2021.
ZhenQin,WeixuanSun,HuiDeng,DongxuLi,YunshenWei,BaohongLv,JunjieYan,Lingpeng
Kong,andYiranZhong. cosformer: Rethinkingsoftmaxinattention. InInternationalConference
onLearningRepresentations,2022.
ChristianBerg,JensPeterReusChristensen,andPaulRessel. Harmonicanalysisonsemigroups:
theoryofpositivedefiniteandrelatedfunctions,volume100. Springer,1984.
IsaacJSchoenberg. Metricspacesandpositivedefinitefunctions. TransactionsoftheAmerican
MathematicalSociety,44(3):522â€“536,1938.
13
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
vonPlaten,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,SylvainGugger,
MariamaDrame,QuentinLhoest,andAlexanderRush. Transformers: State-of-the-artnatural
languageprocessing. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
LanguageProcessing: SystemDemonstrations,pages38â€“45,Online,October2020.Association
forComputationalLinguistics.
LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,JasonPhang,
HoraceHe,AnishThite,NoaNabeshima,etal. Thepile: An800gbdatasetofdiversetextfor
languagemodeling. arXivpreprintarXiv:2101.00027,2020.
Sid Black, Stella Biderman, Alex Andonian, Quentin Anthony, Preetham Gali, Leo Gao, Eric
Hallahan,JoshLevy-Kramer,ConnorLeahy,LucasNestler,KipParker,JasonPhang,Michael
Pieler,ShivanshuPurohit,TriSongz,PhilWang,andSamuelWeinbach. GPT-NeoX:Largescale
autoregressivelanguagemodelinginpytorch,2021. URLhttp://github.com/eleutherai/
gpt-neox.
MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatan-
zaro. Megatron-lm: Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism.
arXivpreprintarXiv:1909.08053,2019.
JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYuxiongHe. Deepspeed: Systemoptimiza-
tionsenabletrainingdeeplearningmodelswithover100billionparameters. InProceedingsofthe
26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,pages
3505â€“3506,2020.
Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,
SasikanthAvancha,DharmaTejaVooturi,NatarajJammalamadaka,JianyuHuang,HectorYuen,
etal. Astudyofbfloat16fordeeplearningtraining. arXivpreprintarXiv:1905.12322,2019.
A Appendix
A.1 ProofofCPDShiftLemma
Lemma 1 (CPD Shift Lemma). Let k : X Ã—X â†’ R be a conditionally positive definite (CPD)
kernel. Then,thereexistscâ‰¥0suchthatc+k(x,y)isapositivedefinitekernel.
Proof. LetK =[k(x ,x )]N bethematrixgeneratedby{x ,...,x }withN âˆˆN. Consider
i j i,j=1 1 N
f (v)=v(cid:62)(c11(cid:62)+K)v =c(v(cid:62)1)2+v(cid:62)Kv.
c
Wewanttoshowthereexistsalargeenoughcsuchthatf (v)â‰¥0forallv âˆˆ{v :(cid:107)v(cid:107)=1}.
c
(i) Itissufficienttoconsideraaaâˆ—âˆ—âˆ—===mmmiiinnn vvv(cid:62)(cid:62)(cid:62)KKKvvv<<<000...
vvv:::(cid:107)(cid:107)(cid:107)vvv(cid:107)(cid:107)(cid:107)===111
Letaâˆ—bethesolutiontotheminimization:
aâˆ— = min v(cid:62)Kv.
v:(cid:107)v(cid:107)=1
Sincev(cid:62)Kv iscontinuousinv and{v : (cid:107)v(cid:107) = 1}iscompact(i.e., closedandbounded), aâˆ—
mustexist. Ifaâˆ— â‰¥0,K ispositivesemidefiniteandf (v)â‰¥0forcâ‰¥0. Thus,withoutlossof
c
generality,weassumeaâˆ— <0.
(ii) ItissufficienttoconsiderKKK withoutzeroeigenvalues(i.e.,fullrank).
Ifthereexistsv suchthatKv = 0, thenc â‰¥ 0isenoughtosatisfyf (v ) â‰¥ 0. Foranyv
0 0 c 0 1
satisfyingv(cid:62)v =0,wehave(v +v )(cid:62)K(v +v )=v(cid:62)Kv . Therefore,whetherthereexists
1 0 1 0 1 0 1 1
ctohavef (v)â‰¥0doesnâ€™tdependontheeigenvectorcorrespondingtozeroeigenvalue(ifthere
c
issuchavector). ThismeansitisenoughtoconsiderK withoutzeroeigenvalues.
14
(iii) ItissufficienttoconsiderstrictCPD.
Bydefinitionofconditionalpositivedefiniteness(CPD),weknowv(cid:62)Kv â‰¥0whenv(cid:62)1=0.
SinceK hasnozeroeigenvalue,wecannothavev(cid:62)Kv = 0whenv(cid:62)1 = 03. Thismeansthe
inequalityisstricthere: v(cid:62)Kv >0whenv(cid:62)1=0,anditisenoughtoconsiderstrictCPD.
(iv) Itissufficienttoshowthereexists(smallenough)Î´Î´Î´>>>000suchthatvvv(cid:48)(cid:48)(cid:48)âˆˆâˆˆâˆˆTTT â‡’â‡’â‡’vvv(cid:48)(cid:48)(cid:48)(cid:62)(cid:62)(cid:62)KKKvvv(cid:48)(cid:48)(cid:48)>>>000.
Î´Î´Î´
NoteT isdefinedas
Î´
T ={v(cid:48) :|v(cid:48)(cid:62)1|<Î´, (cid:107)v(cid:48)(cid:107)=1}.
Î´
Foranyv(cid:48) âˆˆT ,ifv(cid:48)satisfiesv(cid:48)(cid:62)Kv(cid:48) >0,thencâ‰¥0isenoughtohavef (v(cid:48))â‰¥0. Conversely,
Î´ c
when|v(cid:62)1|â‰¥Î´and(cid:107)v(cid:107)=1,observethat
f (v)=c(v(cid:62)1)2+v(cid:62)Kv â‰¥c(v(cid:62)1)2+aâˆ— â‰¥cÎ´2+aâˆ—
c
Then,f (v)â‰¥0whencâ‰¥ âˆ’aâˆ— . Therefore,weneedtoprovev(cid:48) âˆˆT â‡’v(cid:48)(cid:62)Kv(cid:48) >0forsmall
c Î´2 Î´
enoughÎ´.
(v) Leveraging Continuity. Consider v(cid:48) satisfying (cid:107)v(cid:48) âˆ’v(cid:107) < Î´ with v âˆˆ S = {v : (cid:107)v(cid:107) =
2
1, v(cid:62)1 = 0}. Sincev(cid:62)Kv iscontinuousinv and(cid:107)K(cid:107) < âˆž,forany(cid:15) > 0andanyv âˆˆ S,a
smallenoughÎ´ >0gives|v(cid:48)(cid:62)Kv(cid:48)âˆ’v(cid:62)Kv|<(cid:15).
2
Toseethis,takingv(cid:48) =v+pwith(cid:107)p(cid:107)<Î´ ,wehave
2
|v(cid:48)(cid:62)Kv(cid:48)âˆ’v(cid:62)Kv|=|p(cid:62)Kv+v(cid:62)Kp+p(cid:62)Kp| â‰¤ (cid:107)K(cid:107)(2(cid:107)p(cid:107)+(cid:107)p(cid:107)2)â‰¤(cid:107)K(cid:107)(2Î´ +Î´2).
2 2
(cid:107)v(cid:107)=1
(cid:112)
Therefore,0<Î´ < 1+(cid:15)/(cid:107)K(cid:107)âˆ’1isenoughtohave|v(cid:48)(cid:62)Kv(cid:48)âˆ’v(cid:62)Kv|<(cid:15).
2
BydefinitionofstrictCPD,weknowmin v(cid:62)Kv =Î»>0. Thus,take(cid:15)<Î»,asmallenough
vâˆˆS
Î´ givesv(cid:48)(cid:62)Kv(cid:48) > v(cid:62)Kvâˆ’(cid:15) >= Î»âˆ’(cid:15) > 0. Inotherwords,thereexistsasmallenoughÎ´
2 2
suchthatv(cid:48)(cid:62)Kv(cid:48) >0forv(cid:48) âˆˆS ={v(cid:48) :(cid:107)v(cid:48)âˆ’v(cid:107)<Î´ , v âˆˆS}.
Î´2 2
(vi) ProvingâˆƒâˆƒâˆƒÎ´Î´Î´>>>000sss...ttt...vvv(cid:48)(cid:48)(cid:48)âˆˆâˆˆâˆˆTTT â‡’â‡’â‡’vvv(cid:48)(cid:48)(cid:48)(cid:62)(cid:62)(cid:62)KKKvvv(cid:48)(cid:48)(cid:48)>>>000.
Î´Î´Î´
Due to (iv), we want to show âˆƒ Î´ > 0 s.t. v(cid:48) âˆˆ T â‡’ v(cid:48)(cid:62)Kv(cid:48) > 0. We will prove by the
Î´
conclusionof(v). Let(cid:107)v(cid:48)(cid:107)=1,v(cid:48)(cid:62)1=rwith|r|<Î´andv(cid:48)(cid:48) =v(cid:48)âˆ’ r1. Wehave
n
(cid:114) (cid:114)
r r r2
v(cid:48)(cid:48)(cid:62)1=0, (cid:107)v(cid:48)âˆ’v(cid:48)(cid:48)(cid:107)= âˆš , (cid:107)v(cid:48)(cid:48)(cid:107)= (cid:107)v(cid:48)âˆ’ 1(cid:107)2 = 1âˆ’ .
n n n
(cid:113)
Takev = v(cid:48)(cid:48) = v(cid:48)(cid:48) ,whereq = 1âˆ’ r2. Wehavev(cid:62)1=0, (cid:107)v(cid:107)=1and
(cid:107)v(cid:48)(cid:48)(cid:107) q n
1 1r 1 r2 1 1r2
(cid:107)v(cid:48)âˆ’v(cid:107)2 =(cid:107)(1âˆ’ )v(cid:48)+ 1(cid:107)2 =(1âˆ’ )2+ +2(1âˆ’ )
q qn q nq2 q q n
1 r2 2 1 1 (cid:16) r2 (cid:17)
=(1âˆ’ )2+ ( âˆ’ )= (qâˆ’1)2+ (2qâˆ’1)
q n q q2 q2 n
1 (cid:16) r2 r2 r2(cid:17)
= 1âˆ’ âˆ’2q+1+ 2qâˆ’
1âˆ’ r2 n n n
n
1 (cid:16) r2 r2 r2(cid:17)
= 1âˆ’ âˆ’2q(1âˆ’ )+1âˆ’ =2âˆ’2q
1âˆ’ r2 n n n
n
(cid:114)
r2 1r2 r2 âˆš x
=2(1âˆ’ 1âˆ’ )â‰ˆ2(1âˆ’1+ )= ( 1âˆ’xâ‰ˆ1âˆ’ when|x|(cid:28)1)
n 2 n n 2
Thus,(cid:107)v(cid:48)âˆ’v(cid:107) = O(âˆš|r n|) â‰¤ O(âˆšÎ´ n) < Î´
2
forsmallenoughÎ´. Thisimpliesthat,withasmall
enoughÎ´,foranyv(cid:48) âˆˆT ,wecanfindv âˆˆS suchthat(cid:107)v(cid:48)âˆ’v(cid:107)<Î´ . Thus,v(cid:48) âˆˆS ,andby
Î´ 2 Î´2
(v),wearriveatv(cid:48)(cid:62)Kv(cid:48) >0.
3Byspectraldecomposition,v(cid:62)Kv=(cid:80) Î» (v(cid:62)u )2 â‰¥0.SincethereisnoÎ» =0,theinequalityisstrict.
i i i i
15
A.2 Shift-invariantKernelswithBoundedandUnboundedRanges
Definition1impliesashift-invariantkernelisgeneratedbyaunivariatefunctionf : X â†’ R. To
characterizethesetofvalidunivariatefunctions,weintroducethepositivedefinitefunctionsasbelow.
Definition3(Positivedefinitefunction). A(real)positivedefinitefunctionisafunctionf :X â†’R
such that for any integer N and any set of N points {x âˆˆ X}N , the N Ã— N matrix A =
i i=1
[f(x âˆ’x )]N ispositivesemidefinite.
i j i,j=1
Wewillinterchangetheideasofshift-invariantkernelsandpositivedefinitefunctionsbecausethey
are equivalent by definition. Any statement in positive definite functions can be translated into
shift-invariantkernels,andviceversa. Becauseofthis,wewillusesomefactsaboutthepositive
definitefunctionstoderivetheshift-invariantkernelsofourinterest.
GeneralizingClassicalBoundedShift-invariantKernel. Intheliterature,therehavebeenstudies
onapplyingkernelsintheattentionmechanism[Tsaietal.,2019,Choromanskietal.,2021,Peng
etal.,2021]. OneofthemostcommonapproachesistoconsidertheGaussiankernel:
k(m,n)=exp(âˆ’Î³(mâˆ’n)2), Î³ >0.
Notethe Gaussiankernel isbounded (k(m,n) âˆˆ (0,1] forthe caseabove). To generalizeit to a
broaderclassofboundedshift-invariantkernels,observethattheGaussiankernelgeneratedbya
positivedefinitefunctionoftheformf(x)=exp(âˆ’|x|2). Sincethereisnostrongreasontostickto
thepowerof2,onemaygeneralizeittoabroaderclassofpositivedefinitefunctionsasbelow.
Fact5(Corollary3ofSchoenberg[1938]). exp(âˆ’|x|p)ispositivedefiniteif0 < p â‰¤ 2andnot
positivedefiniteifp>2.
Fact5impliesthat, ifonewantstofindaclassofboundedshift-invariantkernel(i.e., k(m,n)is
withinsomefixedintervalforanym,n),thenk(m,n)=exp(âˆ’a|mâˆ’n|p)witha>0andpâˆˆ(0,2]
maybeofinterest.
ConstructingUnboundedShift-invariantKernels. AlimitationofFact5isthatitonlygenerates
kernelswithaboundedrange(here,therangeisboundedin(0,1]). Insituationswherethereareno
explicitbounds,onemightwanttoconsiderkernelswithunboundedrange. Toconstructsuchkernels,
weutilizeakernelrepresentationtheorempresentedinSchoenberg[1938]:
Fact 6 (Theorem 4 of Schoenberg [1938]). f(x) is bounded away from zero (f(x) > 0) and its
positivepowersf(x)Î»(Î»>0)areallpositivedefiniteifandonlyiff(x)isoftheform
f(x)=exp(c+Ïˆ(x)),
whereÏˆ(x)ispositivedefiniteandcisarealconstant.
SinceFact6worksfornon-negativekernels,wecombineitwithFact5andshowthefollowingclass
ofshift-invariantkernelwithanunboundedrange.
Proposition1(KernelfromDistancePowers). Foranypâˆˆ(0,2],thereexistsc âˆˆRsuchthat
min
foranycâ‰¥c ,
min
k(m,n)=câˆ’|mâˆ’n|p,
isapositivedefinitekernel. Whenp>2,thereisnoctomakek(m,n)positivedefinite.
Proof. DuetoFact5,weknowexp(âˆ’|x|p)ispositivedefinitewhenpâˆˆ(0,2]. Sinceexp(âˆ’|x|p)>
0andexp(âˆ’Î»|x|p)ispositivedefiniteforanyÎ» > 04,Fact6impliesthereexistsac(cid:48) âˆˆ Randa
positivedefiniteÏˆ(x)suchthat
exp(âˆ’|x|p)=exp(c(cid:48)+Ïˆ(x)).
Inotherwords,âˆ’c(cid:48)âˆ’|x|pisapositivedefinitefunction.Takec =âˆ’c(cid:48).Weseethatc âˆ’|mâˆ’n|p
min min
isashift-invariantpositivedefinitekernel. Finally,letk(m,n)=câˆ’|mâˆ’n|pwithcâ‰¥c . The
min
N Ã—N matrix[k(x ,x )]N generatedbyk(m,n)onpoints{x âˆˆR}N obeys
i j i,j=1 i i=1
[k(x ,x )]N =[c âˆ’|x âˆ’x |p]N +(câˆ’c )11(cid:62) (cid:23)(câˆ’c )11(cid:62) (cid:23)0,
i j i,j=1 min i j i,j=1 min min
4exp(âˆ’Î»|x|p)=exp(âˆ’|Î»1/px|p)isaconstantrescalingofexp(âˆ’|x|p)andthereforeispositivedefinite.
16
where(cid:23)istheLoewnerorderand1=[1,...,1](cid:62) intheN-dimensionalvectorwithallones. This
showsk(m,n)isashift-invariantpositivedefinitekernelwhen0<pâ‰¤2. Theconclusiononp>2
isprovedbycontradiction. Whenp>2,ifthereexistsacsuchthatk(m,n)ispositivedefinite,then
exp(k(m,n))ispositivedefinite,whichcontradictstothecaseofp>2inFact5.
Prop.1introducesakernelwithunboundedrange(k(m,n)âˆˆ(âˆž,c])andisadaptedfromthep-th
powerofthedistance|mâˆ’n|. Sincethedistanceisanotionof"dissimilarity",âˆ’|mâˆ’n|pbecomes
anotionof"similarity",whichgivesasenseofkernel. Thereby,wecaninterprettheconstantcasthe
requiredvaluetoshiftâˆ’|mâˆ’n|psuchthatcâˆ’|mâˆ’n|pbecomesapositivedefinitekernel.
Infact,âˆ’|mâˆ’n|pisaconditionalpositivedefinitekernelforpâˆˆ(0,2]SchÃ¶lkopf[2000]. Therefore,
thefactthatâˆ’|mâˆ’n|pcanbecomeapositivedefinitekernelbyshiftingisnotacoincidence,asit
hasalreadyhadanintimaterelationtopositivedefinitekernels.
A.3 ExperimentsonGaussian-likeKernels
Since the prior work on shift-invariant kernels mainly focuses on Gaussian kernels, we present
preliminaryexperimentsonGaussian-likekernels. Comparedwithsection5.2,theperplexitiesof
thesekernelsarelargeateveryextrapolationlength. Thisverifiesourpreviousassertionthatthe
Gaussian-likekernelshavelimitedextrapolationability.
Becausethekernelcanbeusedasaweightorabias,weconsiderfourkindsofthecompositekernel
(seesection4)asfollows.
(2-para-bias) r ,r >0.
1 2
kcomp([q ,m],[k ,n])=q(cid:62)k +r exp(âˆ’r |mâˆ’n|2).
m n m n 1 2
(3-para-bias) r ,r >0and0<r â‰¤2.
1 2 3
kcomp([q m,m],[k n,n])=q m(cid:62)k n+r 1exp(âˆ’r 2|mâˆ’n|r3).
(1-para-wht) r >0.
1
kcomp([q ,m],[k ,n])=q(cid:62)k Â·exp(âˆ’r |mâˆ’n|2).
m n m n 1
(2-para-wht) r >0and0<r â‰¤2.
1 2
kcomp([q m,m],[k n,n])=q m(cid:62)k nÂ·exp(âˆ’r 1|mâˆ’n|r2).
(2-para-bias) and (1-para-wht) are the settings where we put the Gaussian kernel as a bias and
a weight, respectively. (3-para-bias) and (2-para-wht) generalize these settings by considering a
learnablepowerbetween0and2. Notewemustconstrainthepowerin(0,2];otherwise,thefunction
isnotpositivedefinite. SeeFact5fordetails.
ThesecompositekernelkcomparepluggedintotheKERPLEframework,Eq.(2),andareevaluatedon
OpenWebText2,GitHub,andArXivdatasets. Table6showstheGaussian-likekernelisbettertobea
weightinsteadofabias. AsdiscussedinAppendixA.2,theGaussian-likekernelsarebounded. To
someextent,thisimpliesthattheboundedpositivekernelcanmodelaweight. However,compared
withsection5.2theGaussian-likekernelshavelimitedadvantagesinextrapolation. Althoughthe
performancemightbeimprovedifthepowerofexp(âˆ’|x|p)isrelaxedfromp=2topâˆˆ(0,2],still
itcannotbeasgoodasthelogarithmicvariantaswedemonstrateinsection5.2. Therefore,whilethe
Gaussiankernelisfrequentlyusedintheliterature,weneedabetterclassofshift-invariantkernelsto
tacklethelengthextrapolationchallenge.
A.4 ExperimentsonLargeModel,LongerTrainingLength,andWikitext-103
Inthissubsection,wepresentadditionalexperimentson(a)largemodels,(b)longertraininglength,
and(c)Wikitext-103. Belowisthesummaryoftheexperiments.
(a) The1.3B large modelistrained ona machinewithtwoNVIDIAA100 GPUwith 40GBof
memory. WeadoptalmostallconfigurationsofXLGPT-NeoX5,exceptthatwechangethetrain-
micro-batch-sizeto16,model-parallel-sizeto2,seq-lengthto512,andmax-position-embeddings
to512. Table8summarizestheconfigurationsofthe1.3Bmodel.
(b) The162MModelwithtrainingsequencelength=1024followsthesameconfigurationsasthe
onesinTable2exceptthatthetrainseq. lengthischangedto1024.
5https://github.com/EleutherAI/gpt-neox/blob/main/configs/XL.yml
17
Table 6: Extrapolation of Gaussian-like kernels on OpenWebText2, GitHub, and ArXiv. All
modelsaretrainedfor50kstepswithtraininglength512andfiverandomseeds.
OpenWebText2
Extrp.
1-para-wht 2-para-wht 2-para-bias 3-para-bias
512 33.8Â±1.1 24.8Â±0.9 58.4Â±71.6 26.4Â±0.5
1024 32.5Â±0.8 23.0Â±0.8 88.7Â±62.6 75.3Â±37.8
2048 34.1Â±0.6 22.7Â±0.4 406Â±101 2629Â±4024
4096 35.6Â±0.9 22.6Â±0.6 2590Â±3211 37557Â±67936
8192 39.2Â±1.1 23.2Â±0.3 10829Â±18855 189216Â±369499
GitHub
Extrp.
1-para-wht 2-para-wht 2-para-bias 3-para-bias
512 7.78Â±0.48 3.56Â±0.23 4.08Â±0.85 3.67Â±0.22
1024 7.85Â±0.40 3.19Â±0.17 4.63Â±0.59 4.23Â±0.57
2048 8.08Â±0.21 3.01Â±0.09 18.8Â±6.8 20.0Â±4.8
4096 8.47Â±0.43 2.93Â±0.09 75.8Â±32.2 94.0Â±24.7
8192 9.41Â±0.75 3.05Â±0.20 207Â±110 261Â±86
ArXiv
Extrp.
1-para-wht 2-para-wht 2-para-bias 3-para-bias
512 10.6Â±0.4 6.18Â±0.25 6.73Â±0.30 7.12Â±1.43
1024 10.7Â±0.2 5.73Â±0.11 7.07Â±0.63 7.27Â±0.69
2048 10.8Â±0.3 5.35Â±0.15 20.4Â±9.3 23.5Â±8.4
4096 11.6Â±0.3 5.44Â±0.14 80.6Â±49.4 131Â±140
8192 12.1Â±0.2 5.50Â±0.27 220Â±138 437Â±591
Table7: TrainingTimeComparisonforGaussian-likeKernelsonGitHub.
1-para-wht 2-para-wht 2-para-bias 3-para-bias
sec/step 0.326 0.327 0.324 0.351
(c) The Wikitext-103 model is implemented on ALiBiâ€™s GitHub6 with exactly the same config-
urations(247Mparameters), exceptthatthefunctionbuffered_future_mask()atline1011of
attention_with_linear_biases/fairseq/models/transformer.pyisadaptedtoourKERPLE-log.
Table8: 1.3BModelConfigurations.
#Layers HiddenSize #AttentionHeads TrainSeq. Len. #TrainableParams.
24 128 16 512 1.3B
Optimizer BatchSize TrainSteps Precision #TrainableParams. forRPEs
Adam(lr2e-4) 32 150,000 float16 48
Table 9 shows the results on the large model (1.3B). Compared with the small model results in
Table 3, we see that T5 bias becomes weaker than KERPLE-log and ALiBi, and KERPLE-log
remainsstrongerthanALiBionGitHubandArXivdatasets. Thisisexplainedbythetendencyof
overfitting. ObservethatbothT5andKERPLElearnthepositionalembeddingswhileALiBiuses
fixedones. T5andKERPLEhaveahighertendencyofoverfitting. Alargermodel(1.3B>162M)or
anoisydataset(OpenWebText2>GitHub,ArXiv)positsahigherriskofoverfitting. Hence,wesee
thatT5biasisweakonalargemodel,andKERPLE-logonlyextrapolateswellonGitHubandArXiv.
Again,Table9showstheresultsonlongtraininglength(1024). comparedwiththeshorttraining
length(512)inTable3,KERPLE-logremainsbetterthanALiBiandT5bias,especiallyonlonger
evaluationlength. ThisshowstherobustnessofKERPLE-logoverdifferenttraininglengths.
Table10comparesKERPLE-logwithALiBiusingALiBiâ€™simplementationandconfigurations. The
resultsshowthatKERPLE-logissuperiortoALiBionWikitext-103.
6https://github.com/ofirpress/attention_with_linear_biases
18
Table9: PerplexityComparisonforLargeModels(1.3B)andLongTrainingLength(1024)on
GitHub,ArXiv,OpenWebText2. Duetothetimeconstraintandlimitedcomputingresources,we
arenotabletoobtainthenumbersforthelargemodel(1.3B)onOpenWebText2fornow. Allmodels
aretrainedwithfiverandomseeds. xâ€ meansourlogvariantisstatisticallysignificantlybetterthanx.
Thetestusedispairedtwo-sidedt-testwithÎ±=0.05.
162MModel. Trainlength,steps=1024,50k. 1.3BModel. Trainlength,steps=512,150k.
GitHub GitHub
Extrp.
KERPLE-log ALiBi T5bias KERPLE-log ALiBi T5bias
512 - - - 2.88Â±0.11 2.88Â±0.11 2.93Â±0.11â€ 
1024 2.83Â±0.16 2.84Â±0.16â€  2.81Â±0.16 2.60Â±0.12 2.62Â±0.11â€  2.64Â±0.11â€ 
2048 2.70Â±0.07 2.82Â±0.07â€  2.68Â±0.07 2.44Â±0.05 2.58Â±0.05â€  2.47Â±0.07â€ 
4096 2.53Â±0.04 2.77Â±0.06â€  2.54Â±0.04 2.46Â±0.11 2.65Â±0.12â€  2.49Â±0.12
8192 2.42Â±0.03 2.74Â±0.02â€  2.57Â±0.06â€  2.44Â±0.13 2.57Â±0.13â€  2.57Â±0.13â€ 
16384 2.48Â±0.11 2.80Â±0.11â€  3.10Â±0.34â€  2.60Â±0.07 2.61Â±0.07 3.16Â±0.35â€ 
ArXiv ArXiv
Extrp.
KERPLE-log ALiBi T5bias KERPLE-log ALiBi T5bias
512 - - - 5.56Â±0.15 5.58Â±0.16 5.62Â±0.15â€ 
1024 5.23Â±0.09 5.26Â±0.09 5.20Â±0.10 4.87Â±0.07 4.94Â±0.07â€  4.92Â±0.06â€ 
2048 4.76Â±0.12 4.98Â±0.18â€  4.74Â±0.12 4.50Â±0.16 4.87Â±0.17â€  4.55Â±0.16â€ 
4096 4.75Â±0.10 5.31Â±0.13â€  4.97Â±0.27 4.45Â±0.06 4.97Â±0.13â€  4.53Â±0.08â€ 
8192 4.54Â±0.10 5.25Â±0.15â€  6.55Â±0.97â€  4.47Â±0.20 4.94Â±0.16â€  4.65Â±0.15â€ 
16384 4.62Â±0.15 5.35Â±0.19â€  16.0Â±4.77â€  4.65Â±0.24 4.94Â±0.07 5.25Â±0.26â€ 
Extrp. OpenWebText2 OpenWebText2
KERPLE-log ALiBi T5bias KERPLE-log ALiBi T5bias
512 - - - 17.5Â±0.3 17.5Â±0.4 17.8Â±0.3â€ 
1024 19.2Â±0.1 19.3Â±0.2 19.1Â±0.1 16.6Â±0.6 16.7Â±0.6 16.9Â±0.6â€ 
2048 19.3Â±0.2 19.5Â±0.1 19.2Â±0.2
16.2Â±0.4 16.4Â±0.4â€  16.7Â±0.4â€ 
4096 18.6Â±0.3 19.0Â±0.3â€  19.2Â±0.4â€ 
16.4Â±0.8 16.5Â±0.5 18.0Â±0.9â€ 
8192 18.7Â±0.5 19.3Â±0.4â€  24.0Â±1.1â€ 
16.9Â±0.7 16.5Â±0.1 22.7Â±3.7â€ 
16384 18.8Â±0.5 19.2Â±0.3â€  50.8Â±6.5â€ 
17.8Â±1.2 16.5Â±0.3 37.1Â±13.1â€ 
Table10:PerplexityComparisononWikitext-103. Toensureafaircomparison,themodel(247M)
is trained on ALiBiâ€™s codebase with exactly the same configurations except for the positional
embeddings. TheresultsshowthatKERPLE-logissuperiortoALiBionWikitext-103.
trainlength512 trainlength2048
Extrp. length 512 1024 1536 2048 3072 2048 3072
ALiBi 19.73 18.81 18.50 18.48 18.40 17.91 17.64
KERPLE-log 19.69 18.76 18.37 18.29 18.24 17.84 17.56
A.5 AdditionalAnalyses
SincethepowerandlogarithmicvariantsderivedfromKERPLEachievesuperiorperformanceon
lengthextrapolationacrossvariousdatasets,weinvestigatetheunderlyingreasonbyvisualizingthe
effectivelengthasshowninFigure6. Thevisualizationworksinthefollowingprocedure.
1. Foreachtrainingdataset,thelearnableparameters(r(h),...,r(h))associatedwitheachheadh(12
1 (cid:96)
intotal)areextractedfromthemodelcheckpoint.TheCPDkernelatheadhiskËœ(h) =kËœ .
r(h),...,r(h)
1 (cid:96)
Boththepowerandthelogarithmicvariantsincorollary1undergoasimilarprocedure. Theonly
differenceisthattheirkËœâ€™saredifferent.
2. Foreachheadh,wecomputetheeffectivelengthofkËœ(h) aseff(h) = min |mâˆ’n|.
kËœ(h)(0,|mâˆ’n|)<âˆ’2
Thatis, therelativepositionaldifference|mâˆ’n|suchthatkËœ(m,n) shif =t-inv. kËœ(0,|mâˆ’n|)just
becomessmallerthan-2. NotekËœ(0,|mâˆ’n|)strictlydecreasesin|mâˆ’n|,sothereisonlyone
19
Figure 6: Number of Heads with Effective Lengths â‰¤â‰¤â‰¤|||mmmâˆ’âˆ’âˆ’nnn||| for different choices of CPD
kernelsanddatasets. SeesectionA.5fordetails.
(b)LogarithmicVariant:
(a)PowerVariant:âˆ’a|mâˆ’n|p âˆ’alog(1+b|mâˆ’n|)
12 12
10 10
8 8
6 6
4 GitHub 4 GitHub
Arxiv Arxiv
2 OpenWebText2 2 OpenWebText2
ALiBi ALiBi
0 0
0 5000 10000 15000 20000 0 5000 10000 15000 20000
Positional Differences Between Tokens |m-n| Positional Differences Between Tokens |m-n|
possiblevalue. Wepickâˆ’2herebecausekËœisabiasandisfollowedbytheSoftmaxnormalization.
Abiasofâˆ’2orsmallercanmakeagreatimpactontheoutputofSoftmax7. eff(h)isinterpreted
astheeffectivelengthbecause,when|mâˆ’n|<eff(h),theattenuationduetokËœ(h)isnotstrong.
When|mâˆ’n|>eff(h),theattenuationisstrongandhasalargeimpactonq(cid:62)k +kËœ(h)(m,n).
m n
3. Then, foreach|mâˆ’n| âˆˆ [0,...,20480], wecountthenumberofheadsthatsatisfieseff(h) â‰¤
|mâˆ’n|. ThisgivesacumulativeplotasshowninFigure6,wherethex-axisis|mâˆ’n|andthe
y-axisisCount({h: hâˆˆ[1,...,12], eff(h) â‰¤|mâˆ’n|}).
4. Repeattheabovestepsforotherdatasetsandkernels.
InterpretationofCurves. Forapoint(x,y)onacurve,itmeansthattherearey headswithat
leastâˆ’2biaswhenthetokendistance|mâˆ’n|isgreaterthanx. Inotherwords,theslowerthey
convergesto12,thelongertheinter-tokenrangethatthemodelfocuseson.
TheAdvantageofLearnableParameters. WeobservethatALiBi[Pressetal.,2022]produces
thesamecurvenomatterwhichdatasetisused. ThereasonisthatALiBiselectsafixedparameter
r =2âˆ’ H8h atheadhforitslinearbiasâˆ’r|mâˆ’n|(H headsintotal)regardlessofthedataset. While
thisstrategyisusefulforextrapolation,wehypothesizethatdifferentdatasetsmighthavedifferent
characteristics,e.g.,theaveragedistanceofhighlyrelatedtokensshoulddifferamongthedatasetsas
showninFigure6. Thesecharacteristicsareeasieradaptedbylearnableparameters. Thus,webelieve
thatlearnableparametershavemoreadvantagesincapturingthedataset-dependentcharacteristics.
TrendsAcrossDatasets. WenoticethatbothkernelstrainedonOpenWebText2tendtofocusmore
on distant relations. This makes sense because OpenWebText2 has the highest perplexity scores
amongalldatasets,implyingthatmorecontextisneededtodisambiguatethenextpredictedtoken.
TheoppositetrendholdsforArxivandGitHubdatasets,whichisreasonableconsideringtheirlower
perplexityscores.
CharacteristicsLearnedbyKernels. Underanydataset,thelogarithmicvarianttendstofocus
moreondistantrelationsthanthepowervariantdoes. Wecanexplainitthroughtheirfunctional
forms. Becauselogarithm(âˆ’alog(1+b|mâˆ’n|))decaysmuchslowerthanpower(âˆ’a|mâˆ’n|p)
does,thelogvariantmightencouragethefocusondistantrelations.
A.6 Position-wisePerplexityforLength=16384
WecandrawsimilarconclusionsfromFigure7:
1. KERPLE-log lies below KERPLE-log-windowed@512 most of the time, indicating its
usageofmoredistantinformationthanwindowattention.
7SinceSoftmaxisanexponentiatedfunction,a-2biasintheSoftmaxâ€™sargumentroughlygivesanattenuation
ofexp(âˆ’2)â‰ˆ0.135.
20
sdaeH
#
sdaeH
#
2. ThePPLofT5explodes.
3. ThePPLofALiBidoesnotexplode,butitisstillworsethanwindowattention,i.e. lies
aboveKERPLE-log-windowed@512.
Figure7: Position-wisePerplexityonGitHubatEvaluationLength=16384ComparedtoWin-
dowAttention@512.
Position-wise Perplexity at Evaluation Length=16384
20.0
Rotary
17.5 ALiBi
T5
15.0
KERPLE-log
KERPLE-log-windowed@512
12.5
10.0
7.5
5.0
2.5
0.0
0 2500 5000 7500 10000 12500 15000
Position
A.7 TheChoiceofcodebaseandHyperparameters
Weadoptalmostallthehyperparameters(exceptbatchsizetofitinourGPU)andallimplementations
oftheT5bias,ALiBi,Rotary,andSinusoidalbaselinesfromtheGPT-NeoXcodebase. Toensurefair
comparisons,wedidnotfine-tunehyper-parametersforKERPLE.Thedatasetsweusedareexactly
thesameastheonesreleasedwiththeGPT-NeoXcodebase. Wejustrantheirprepare_data.pyscript
toautomaticallydownloadandparsethedatasets. Allourcodewasuploadedwiththesubmissionon
openreview,andhttps://github.com/EleutherAI/gpt-neoxistheoriginalGitHubrepository.
Asasidenote,wechosethiscodebaseandadoptedtheirparametersettingsbecauseitisbuiltby
EleutherAI, which is a well-known and truly non-profit group of researchers publishing various
famouspretrainedmodelsforacademiaincludingGPT-J-6BandGPT-NeoX-20B.
21
ytixelpreP
egarevA

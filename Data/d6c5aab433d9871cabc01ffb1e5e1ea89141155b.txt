KERPLE: Kernelized Relative Positional Embedding
for Length Extrapolation
Ta-ChungChi∗ Ting-HanFan∗
CarnegieMellonUniversity PrincetonUniversity
tachungc@andrew.cmu.edu tinghanf@princeton.edu
PeterJ.Ramadge AlexanderI.Rudnicky
PrincetonUniversity CarnegieMellonUniversity
ramadge@princeton.edu air@cs.cmu.edu
Abstract
Relativepositionalembeddings(RPE)havereceivedconsiderableattentionsince
RPEseffectivelymodeltherelativedistanceamongtokensandenablelengthex-
trapolation. WeproposeKERPLE,aframeworkthatgeneralizesrelativeposition
embeddingforextrapolationbykernelizingpositionaldifferences. Weachievethis
goalusingconditionallypositivedefinite(CPD)kernels,aclassoffunctionsknown
forgeneralizingdistancemetrics. Tomaintaintheinnerproductinterpretationof
self-attention,weshowthataCPDkernelcanbetransformedintoaPDkernelby
addingaconstantoffset.ThisoffsetisimplicitlyabsorbedintheSoftmaxnormaliza-
tionduringself-attention. ThediversityofCPDkernelsallowsustoderivevarious
RPEsthatenablelengthextrapolationinaprincipledway. Experimentsdemon-
stratethatthelogarithmicvariantachievesexcellentextrapolationperformance
onthreelargelanguagemodelingdatasets. Ourimplementationandpretrained
checkpointsarereleasedathttps://github.com/chijames/KERPLE.git.
1 Introduction
Transformer-basedmodelshaveexcelledinvariousnaturallanguageprocessingtaskssuchaschat-
bot [Roller et al., 2021], code completion [Chen et al., 2021a], and paper abstract summariza-
tion[Zhangetal.,2020]. Thesesequencemodelingtasksoftenrequirethemodeltooperatewellon
significantlylongertextsequencesthanthefixedmaximumlengthLusedattrainingtime. Training
(orretraining)themodelusingasubstantiallylargervalueofLisofteninfeasiblesincethetrans-
formertrainingcostisO(L2). Hence,onedesiresatransformerthatcontinuestoperformwellon
longersequencesthanthoseusedduringtraining;i.e.,performlengthextrapolationatinferencetime.
Mosttransformerdesignsdonothavethisproperty[Pressetal.,2022].Whilerecentworkonabsolute
positionalembeddingsdemonstratedtheextrapolationability[Kiyonoetal.,2021,Likhomanenko
etal.,2021],itisbelievedthatrelativepositionalembeddingsaremorerobusttoinputlengthchange
[Likhomanenkoetal.,2021],forexample,ALiBi[Pressetal.,2022]andT5[Raffeletal.,2020].
Hence,wearemotivatedtostudytheinnerworkingsofrelativepositionalembeddings.
Relativepositionalembeddings(RPE)encodetheideaofshift-invariance: foranyshiftp,(m+p)−
(n+p)=m−n. Itisoftenaddeddirectlytotheself-attentionmatrixbeforeSoftmaxnormalization
[Chenetal.,2021b]. Inspiredbyshift-invarianceandtheabilityofakerneltodefineasimilarity
function,therehavebeenstudiesonshift-invariantkernelsforRPE[WennbergandHenter,2021]
with a focus on Gaussian kernel. However, in our preliminary experiments, the Gaussian kernel
∗
Equalcontribution
36thConferenceonNeuralInformationProcessingSystems(NeurIPS2022).
2202
tcO
31
]LC.sc[
2v12990.5022:viXra
Figure 1: The 3-Para-Log Variant of Our KERPLE Framework. a, b, and p are learnable
parametersineachattentionheadsharedacrosslayers.Since#ofheadsisH,thereare3·H learnable
parameters. Thelearnableparametersaretrainedwithlength-3sequences. Attheinferencetime,the
lastrow(indashedsquares)becomesactive,andthemodelextrapolatestolength-4sequences. Note
wefocusoncausallanguagemodelingfollowingALiBi,sothematricesaretriangular.
𝑞1𝑘1 𝑎∙0𝑝
𝑞2𝑘1 𝑞2𝑘2 𝑎∙1𝑝 𝑎∙0𝑝
−𝑏∙log(1+ )
𝑞3𝑘1 𝑞3𝑘2 𝑞3𝑘3 𝑎∙2𝑝 𝑎∙1𝑝 𝑎∙0𝑝
𝑞4𝑘1 𝑞4𝑘2 𝑞4𝑘3 𝑞4𝑘4 𝑎∙3𝑝 𝑎∙2𝑝 𝑎∙1𝑝 𝑎∙0𝑝
demonstrates limited length extrapolation ability (see Appendix A.3). Hence, a distinct class of
shift-invariantkernelsisneededtoachieveadequatelengthextrapolation.
Tothisend,wenoteasetofwell-establishedconditionallypositivedefinite(CPD)kernelssuitable
formodelingdistancemetrics[Schölkopf,2000]. However,CPDkernelsdonotconformtoaninner
product. We can remedy this issue by transforming a CPD kernel into a PD kernel by adding a
sufficientlylargeconstant. ThisconstantoffsetissubsequentlyabsorbedimplicitlyintheSoftmax
normalization(seethediscussionbelowEq.(2)). Forexample,ALiBiimplicitlyadmitsaPDkernel
oftheformc−|m−n|(seetheendofsection4),whichisreducedtoaCPDkernel−|m−n|. The
CPDkernelandSoftmaxnormalizationcombinationopensthedoortoaseaofpossibleCPDkernels.
Weinvestigatestructuresfromthisclassthatexhibitastronglengthextrapolationability,likeALiBi.
Our main result is a framework for KErnelize Relative Positional Embedding for Length
Extrapolation(KERPLE).Theframeworkelucidateskeyprinciplesthatencouragethelengthextrap-
olationproperty. WeshowthatALiBiisaparticularinstancewithinourframework. Oursubsequent
experimentssuggestthattheproposedmethodyieldsbetterlengthextrapolationonlargedatasets
suchasOpenWebText2,GitHub,andArXiv.
2 BackgroundandRelatedWork
2.1 Preliminary
Let{w }L betheinputtokenstoatransformermodel,whereListhetotalnumberoftokens.Each
m m=1
w isascalarandisusedtoindextheembeddingvectore ∈Rdastheinputtothetransformer. A
m m
transformerconvertseache intoquery,key,andvaluevectorsinRd: q =W e ,k =W e ,
m m q m m k m
v =W e ,whereW ,W ,W ∈Rd×darelearnablematrices. Then,theself-attentionmodule
m v m q k v
computesthescaledattentionscoresandgeneratestheoutputvectoro atpositionmas:
m
√
a =
exp(q m(cid:62)k n/ d √)
, o =
(cid:88)L
a v .
m,n (cid:80)L exp(q(cid:62)k / d) m m,n n
i=1 m i n=1
Sincetheoperationisposition-agnostic,itisbelievedthatpositionalinformationhelpsmodeltoken
interactions[Vaswanietal.,2017],whichwesurveyinthenextsubsection.
2.2 PositionalEmbedding
Absolute. Absolutepositionalembeddingsassignapositionalvectorp toeachpositionmand
m
addsp totheembeddingvectore . Theveryfirstversionofwhichisthepredefinedsinusoidal
m m
function[Vaswanietal.,2017]. FollowedbythesuccessofBERT[Devlinetal.,2019],learnable
absolutepositionalembeddingshavebeenappliedtothetaskofmaskedlanguagemodeling[Devlin
etal.,2019,Liuetal.,2019,Clarketal.,2020,Lanetal.,2020],Autoregressive-decoding[Radford
et al., 2018, 2019], and sequence-to-sequence [Gehring et al., 2017, Lewis et al., 2019] settings.
Recent work studied ways to extrapolate sinusoidal positional embeddings to longer sequences
byrandomlyshiftingabsolutepositionsduringtraining[Kiyonoetal.,2021]oraugmentingwith
continuoussignals[Likhomanenkoetal.,2021].
2
Relative. Asopposedtothemodelingofabsolutepositionm,relativepositionalembeddings(RPE)
thatmodelthepositionaldifferencem−nhasbecomepopularintheliterature[Shawetal.,2018,
Huangetal.,2019,Daietal.,2019,Yangetal.,2019,Huangetal.,2020,Heetal.,2021,Keetal.,
2021,Chenetal.,2021b]. Inparticular,theT5modelthatconsidersbucketedrelativedistancesand
log-binninghasbeenshowntoperformwellonvarioustransformerarchitectures[Raffeletal.,2020].
Rotary positional embedding [Su et al., 2021] encodes the position with rotations: f(q ,m) =
m
R q whereR isarotationmatrixwithanglesproportionaltom. Withtherotation’sproperty,
m m m
thequery-keyproductexhibitsapositionaldifference: f(q ,m)(cid:62)f(k ,n)=q(cid:62)R k .
m n m n−m n
WenotethattheoverviewabovefocusesontheNLPdomain. Recentworkhasappliedpositional
embeddingstootherdomainssuchasvision[Wuetal.,2021a]andspeech[Likhomanenkoetal.,
2021]. Asurveycanbefoundin[Dufteretal.,2022].
2.3 KernelanditsApplicationinTransformer
Thekerneltrickisaclassicapproachtogeneralizetheinnerproducttohighdimensionalspaces
[Mikaetal.,1998,Schölkopf,2000,Leslieetal.,2001,Dhillonetal.,2004,Takedaetal.,2007]. In
thecontextoftransformers,therehasbeeninterestinapplyingkernelstotheself-attentionstructure
toenhancetheperformance. Examplesofsuchworkincludekernelforpositionalembeddings[Tsai
etal.,2019,Wuetal.,2021b,WennbergandHenter,2021,Luoetal.,2021]. Anotherlineofresearch
leveragesthekernel’sfeaturemap[RahimiandRecht,2007]tolinearizetheself-attentionmodule
andreducethecomputationalcost[Katharopoulosetal.,2020,Chenetal.,2021c,Xiongetal.,2021,
Pengetal.,2021,Choromanskietal.,2021,Qinetal.,2022].
3 TheoreticalFoundationsofCPDKernels
3.1 PDandCPDKernels
Inthiswork,weuseshift-invariantconditionallypositivedefinite(CPD)kernelstomodeltheeffect
of relative positional differences. We propose this formulation because the notion of relative is
modeled by a shift-invariant function: a bivariate function k over two positions (m,n) such that
k(m,n)=f(m−n)forsomeunivariatef. Thenotionofpositionaldifferencem−nisgeneralized
bytheCPDkernel. WereviewthedefinitionsofPDandCPDkernelsbelow.
Definition 1 (PD Kernel). A (real) symmetric function k : X × X → R is a positive definite
kernelifforanyintegerN andany{x ∈X}N ,{c ∈R}N ,thequadraticformisnonnegative:
i i=1 i i=1
(cid:80)N (cid:80)N
c c k(x ,x )≥0.
i=1 j=1 i j i j
Definition2(CPDKernel). A(real)symmetricfunctionk˜ :X ×X →Risaconditionallypositive
definite kernel if for any integer N and any {x ∈ X}N , the quadratic form is conditionally
i i=1
nonnegative: (cid:80)N (cid:80)N c c k˜(x ,x )≥0for{c ∈R}N with(cid:80)N c =0.
i=1 j=1 i j i j i i=1 i=1 i
Fact1(Bergetal.[1984]andProp. 5ofSchölkopf[2000]). Letk˜ :X ×X →(−∞,0]beaCPD
kernelwithk˜(x,x)=0∀x∈X. Then,thereexistsaHilbertspaceHandamappingφ:X →H
suchthat(cid:107)φ(x)−φ(x(cid:48))(cid:107)2 =−k˜(x,x(cid:48)).
Fact1suggeststhatCPDkernelsgeneralizedistancemetricstohighdimensionalspaces. Sincewe
areinterestedinpositionaldifferences,weexaminemodelingthedistancebetweenpositionsusing
CPDkernels.
However, Fact1alsoimpliesthatCPDkernelsdonotencodeinnerproductsasrequiredbyself-
attentionforthecomputationofpairwiserelations. PDkernelsrepresentinnerproducts. Tobetter
understandtheeffectofCPDkernelsonself-attention,weneedtoestablishrelationsbetweenCPD
andPDkernels. AsnotedinSchölkopf[2000],ifonetakesanyPDkernelandoffsetsitbyaconstant,
theresultisatleastaCPDkernel. Inthenextsubsection,weshowthattheconverseisnearlytrue:
ifk˜isCPD,soisc+k˜forlargeenoughc∈R(Lemma1). Therefore,wemaygeneratetheCPD
kernelsofinterestandtransformthemintoPDkernelsifneeded.
3
3.2 ConstructingPDKernelsFromCPDKernelsviaConstantShifts
Inthissubsection,wereviewafewpropertiesofCPDkernelsandusethesetogenerateavarietyof
CPDkernels. Then,wepresentalemmathattransformsCPDkernelsintoPDkernelsviaconstant
shifts. ThisenablestheproductionofafamilyofPDkernelsfromCPDkernels. Finally,wepresent
our critical observation that the exact value of the constant shift is not needed, thanks to a nice
propertyofSoftmaxnormalization.
BelowaresomeimportantfactsaboutCPDkernels.
Fact2(ScalingandSummation). Ifk˜ andk˜ areCPD,thensoarea·k˜ (fora>0)andk˜ +k˜ .
1 2 1 1 2
Fact3(Bergetal.[1984]andProp. 4ofSchölkopf[2000]). Ifk˜ :X ×X →(−∞,0]isCPD,then
soare−(−k˜)αfor0<α<1and−log(1−k˜).
Fact4(Page3ofSchölkopf[2000]). Thenegativesquareddistance−(cid:107)x−x(cid:48)(cid:107)2isCPD.
ThethreeFactsabovejointlyyieldarichfamilyofCPDkernelsasshownbelow.
Corollary1. ThefollowingareCPDkernels.
(a) k˜(x,x(cid:48))=−a(cid:107)x−x(cid:48)(cid:107)pwith0<p≤2anda>0.
(b) k˜(x,x(cid:48))=−b·log(1+a(cid:107)x−x(cid:48)(cid:107)p)with0<p≤2anda,b>0.
WenotethatitispossibletokeepiteratingbetweenFact2and3andgeneratemorecomplicated
examples,e.g.,−a(cid:107)x−x(cid:48)(cid:107)p−b·log(1+a(cid:107)x−x(cid:48)(cid:107)p)or−b·log(1+a(cid:107)x−x(cid:48)(cid:107)p)cfor0<c<1.
However, sincerelativepositionalembeddingsareofourinterest, weonlyconsidersimpleCPD
kernels. Thosewithcomplicatedformsaredeferredtofuturework.
NowthatCorollary1haspresentedafewclassofCPDkernels,weprovealemma(inAppendixA.1)
thatconstructsPDkernelsfromCPDkernelsthroughshifting. LaterinEq.(2),wewillseethatthe
shiftingconstructioniscombinedneatlywiththeSoftmaxnormalizationofself-attention.
Lemma 1 (CPD ShiftLemma. Proof in Appendix A.1). Let k˜ : X ×X → R be a CPDkernel.
Thereexistsc≥0suchthatc+k˜isaPDkernel.
Lemma 1 implies the CPD kernels in Corollary 1 can be made PD if a large enough constant is
added. Forexample,c−(cid:107)x−x(cid:48)(cid:107)pforlargeenoughc. AlthoughLemma1doesnothaveanexplicit
constructionofc,thankstotheshift-invariantpropertyoftheSoftmaxnormalization,wecanleaveit
asanunder-determinedconstantinourpositionalembeddingdesign(Eq.(1)insection4). Givena
setoftestpoints{x }N ,onecandoageometricsequencesearch1 tosearchforacsuchthatthe
i i=1
N ×N matrix[c+k˜(x ,x )]N (cid:23)0. Hence,wedonotneedthevalueofc,butwecancompute
i j i,j=1
itifneeded,e.g.,derivingthefeaturemapofc+k˜.
Alternative Proof ofccc−−−(cid:107)(cid:107)(cid:107)xxx−−−xxx(cid:48)(cid:48)(cid:48)(cid:107)(cid:107)(cid:107)ppp. While the CPD shift lemma is convenient, one can prove
c−(cid:107)x−x(cid:48)(cid:107)pisPDforlargeenoughcusingakernelrepresentationtheoreminSchoenberg[1938].
SeeAppendixA.2fordetails.
4 KernelizedRelativePositionalEmbedding
Let{q }L and{k }L betheinputqueriesandkeys. Let(r ,...,r )belearnableparameters.
m m=1 n n=1 1 (cid:96)
Weproposeakernelizedrelativepositionalembeddingasfollows.
√
exp(cid:0) (q(cid:62)k +k˜ (m,n))/ d(cid:1)
a = m n r1,...,r(cid:96) √ , (1)
m,n (cid:80)L exp((q(cid:62)k +k˜ (m,i))/ d)
i=1 m i r1,...,r(cid:96)
1Bygeometricsequencesearch,wecanenlargecby2,4,8,16,andsoonuntilwefindtherequiredlarge
enoughconstant.
4
wherek˜ (m,n)isanyshift-invariantCPDkernelwith(cid:96)parameters. DuetoLemma1,Eq.(1)
r1,...,r(cid:96)
canbereformulatedintoitskernelformasfollows.
√
exp(cid:0) (q(cid:62)k +c+k˜ (m,n))/ d(cid:1)
a ( =∗) m n r1,...,r(cid:96) √
m,n (cid:80)L exp((q(cid:62)k +c+k˜ (m,i))/ d)
i=1 m i r1,...,r(cid:96) √ √ (2)
exp(cid:0) q(cid:62)k +k (m,n))/ d(cid:1) exp(cid:0) kcomp([q ,m],[k ,n])/ d(cid:1)
Lem =ma1 m n r1,...,r(cid:96) √ = m n √ .
(cid:80)L exp(q(cid:62)k +k (m,i))/ d) (cid:80)L exp(cid:0) kcomp([q ,m],[k ,i])/ d(cid:1)
i=1 m i r1,...,r(cid:96) i=1 m i
(*)isduetotheshift-invariantpropertyoftheSoftmaxnormalization: exp(xi) = exp(xi+c)
foranyc∈R.
Thesecondequalitydefinesabiaskernelwhichispositive(cid:80)
dj
ee fixp n( ix tej) using(cid:80)
Lj
eex mp m(x aj+ 1c :)
k =c+k˜ . (3)
r1,...,r(cid:96) r1,...,r(cid:96)
Thelastequalityintroducesacompositekernelkcomp :Rd+1×Rd+1 →Ras
kcomp([q ,m],[k ,n])=q(cid:62)k +k (m,n). (4)
m n m n r1,...,r(cid:96)
Interpretation. Theproposedmethodcanbeinterpretedasapplyingacompositekerneltoself-
attention. Thecompositekernelcombinestheinformationfromqueryq ,keyk ,andpositions
m n
(m,n) in a way that augments the original self-attention structure by multiplicative and additive
positionembeddings. Theaugmentationallowskcomptonotonlyretaintheoriginalq(cid:62)k butalso
m n
includepositionalinformationfromthebiaskernelk .
r1,...,r(cid:96)
PracticalChoice. Insection5.2,wefix(cid:96) = 2andexperimentontwovariantsofthecomposite
kernel,Eq.(4),wherewecallthesethepowervariantandthelogarithmicvariantofourproposed
KERPLEframework,Eq.(2). ThesearefromacombinationofCorollary1andEq.(3).
(power) kcomp([q m,m],[k n,n])=q m(cid:62)k n+c−r 1|m−n|r2 withr
1
>0and0<r
2
≤2.
(logarithmic) kcomp([q ,m],[k ,n])=q(cid:62)k +c−r ·log(1+r |m−n|)withr ,r >0.
m n m n 1 2 1 2
Wenotethatthesearenottheonlyvariantsofthecompositekernel. Insection5.3,weexperiment
withtwomorecomplicatedvariants,butonlyfindlowertrainingspeedsandmarginalimprovement
inperplexities(e.g.,logarithmicvariantvs. 3-para-log). Thus,basedonourstudy,thechoicesabove
holdadvantagesinbothperformanceandspeed.
ConnectiontoPriorWork. Whenthebiaskernel,Eq.(3),isatrianglekernel: c−|m−n|,our
modelreducestoALiBi[Pressetal.,2022]. WennbergandHenter[2021]discussthesituationwhere
thebiaskernelisaGaussiankernel. Tsaietal.[2019]isthecasewherethereisnobiaskerneland
theattentionproductq(cid:62)k ismultipliedbyanexponentiatedinnerproductkernel,exp(x(cid:62)y). Since
m n
ALiBiisthestate-of-the-artandhasgreatinputlengthextrapolation,wewillfocusoncomparison
withALiBiinourexperiments.
The logarithmic variant has an implicit connection to T5 positional bias [Raffel et al.,
2020]. According to the official GitHub repository https://github.com/google-research/
text-to-text-transfer-transformerandtheHuggingFaceTransformer[Wolfetal.,2020],
T5biasisimplementedwithalog-binningstrategy. Foreachheadofthetransformer,theymaintaina
bucketof32learnableparametersandassigntherelativepositionalbiasb totheseparametersas
m−n

bucket[0] ifm−n<0

b = bucket[m−n] if0≤m−n<16
m−n
bucket[min(31,(cid:98)log((m−n)/16))·16(cid:99)] ifm−n≥16,
log(128/16)
where(cid:98)·(cid:99)isthefloorfunction. Notethatthelogfactorisapproximately7.7logm−n. Therefore,T5
16
isusingalogarithmicbucketassignment,whichturnsouttoextrapolatetodifferentinputlengths.
ComparedwithT5,ourlogarithmicvariantuseslessparameters(2x12vs. 32x12)butcannotlearn
non-monotonicrelations(thelogfunctionismonotonic). WewillconductmorecomparisonswithT5
biasinourexperiments.
5
5 Experiments
5.1 DatasetandImplementationDescription
Dataset. We conduct experiments on OpenWebText2, GitHub, and ArXiv datasets gathered in
Gao et al. [2020]. OpenWebText2 includes recent content from Reddit submissions until 2020,
contentfrommultiplelanguages,documentmetadata,multipledatasetversions,andopen-source
replicationcode. GitHubincludesopen-sourcerepositorieswritteninprimarycodinglanguagessuch
asJava,C/C++,Python,andGo. ArXivincludespaperswritteninLaTexinMath,ComputerScience,
Physics,andsomerelatedfields. Thesetasksaremotivatedbythedownstreamapplicationssuch
asonlinechatting[Rolleretal.,2021],codecompletion[Chenetal.,2021a],andacademicpaper
summarization[Zhangetal.,2020].
Table1: DatasetOverview. RawSizeisthesizebeforeanyup-ordown-sampling.
OpenWebText2 GitHub ArXiv
RawSize 66.77GB 95.16GB 56.21GB
Type Internet Coding Academic
Implementation. WeadaptourmodelfromGPT-NeoX[Blacketal.,2021],atransformerimple-
mentationbytheEleutherAIteam. ThecodebaseisbasedonNVIDIAMegatronLanguageModel
[Shoeybietal.,2019]andfurtheracceleratedusingMicrosoftDeepSpeedlibrary[Rasleyetal.,2020].
OurmodelistrainedonamachinewithoneNVIDIAA100GPUwith40GBofmemory. Weadopt
almostallconfigurationsofsmallGPT-NeoX2,exceptthatwechangethetrain-micro-batch-size
to32,seq-lengthto512,andmax-position-embeddingsto512. Table2summarizestheimportant
configurationsfixedthroughoutourexperiments. Inparticular,thefloating-pointencodingissetas
Table2: 162MModelConfigurations.
#Layers HiddenSize #AttentionHeads TrainSeq. Len. #TrainableParams.
12 64 12 512 162M
Optimizer BatchSize TrainSteps Precision #TrainableParams. forRPEs
Adam(lr6e-4) 32 50,000 bfloat16 atmost36
bfloat16(BrainFloatingPoint,developedbyGoogleBrain)sothatthetrainingcanbeacceleratedby
half-precisioncomputationwithreliablestability[Kalamkaretal.,2019]. Hiddensize64meansthat
d=64inEq.(1).
5.2 ExperimentalResults(Alsoc.f. AppendixA.4toA.7)
Weconductexperimentstocoveraspectssuchasinputlengthextrapolation,applicationondifferent
domains,andcomparisonwiththepriorwork. Theseareelaboratedonbelow. (i)Motivatedbythe
inputlengthextrapolationdemonstratedin[Pressetal.,2022],wetrainourmodelwithlength512
andtestonlengthsrangingfrom512to16384. Wehopethattheemphasisonextrapolationenables
theapplicationoftransformerstolongersequences. (ii)Toevaluatetheapplicabilityofthemodel
indifferentdomains,weconductexperimentsonOpenWebText2,GitHub,andArXivdatasets. (iii)
Tovalidatetheeffectivenessofourmethod,wecompareKERPLEwithSinusoidal[Vaswanietal.,
2017],Rotary[Suetal.,2021],T5[Raffeletal.,2020],andALiBi[Pressetal.,2022].
Table 3 reports the perplexities at different extrapolation lengths. We perform non-overlapping
evaluation: Suppose text is segmented in a different manner for 512 and 1024 tokens, we have
N sentences and N/2 correspondingly to evaluate. We also perform a paired two-sided t-test to
validatethestatisticalsignificance(significancelevel=0.05). WecompareeachcandidateRPEwith
ourproposedlogarithmicvariantandmarkthecandidatewitha† ifthelogvariantisstatistically
significantlybetter. Table4reportsthetrainingspeeds. Thesetablesyieldthreeconclusions. First,
withintheKERPLEframework,thelogarithmicvariantisbetterthanthepowervariant. Secondly,the
logarithmicvariantis9.7%fasterthanT5. Intermsofextrapolation,thelogarithmicvariantgenerally
2https://github.com/EleutherAI/gpt-neox/blob/main/configs/small_bf16.yml
6
doesbetterthanT5butcouldbeslightlyworsethanT5atshorterlengths. Third,thelogarithmic
variant is slightly slower than some prior work (ALiBi, Rotary, and Sinusoidal) but consistently
outperformthesemethodsatallextrapolationlengths. Moredetailsaregivenbelow.
LogarithmicVariantvs. PowerVariant. InourproposedKERPLEframework,thelogarithmic
variantisbetterthanthepowervariant. Precisely,thelogarithmicvariantis4.4%fasterandhaslower
perplexitiesacrossallextrapolationlengthsandalltasks.
LogarithmicVariantvs. T5. Intermsofspeed, thelogarithmicvariantis9.7%fasterthanT5.
In terms of extrapolation perplexity, the logarithmic variant is close to or slightly worse than T5
whentheextrapolationlengthisshorterthan2048,andconsistentlyexcelsT5atlongerextrapolation
lengths. Thetendencyofextrapolationholdsforalldatasetsevaluatedinthiswork.
LogarithmicVariantvs. ALiBi,Rotary,andSinusoidal. Thelogarithmicvariantis1.6%slower,
7.5%faster,and3.0%slowerthanALiBi,Rotary,andSinusoidal. Thespeedcomparisonmakessense
becausewerequireonlyalimitedamountoflearnableparametersforRPEs(atmost3·H). Also,the
logarithmicvariantconsistentlyoutperformspriorworkatallextrapolationlengthsandtasks.
Table3: PerplexityComparisononOpenWebText2,GitHub,andArXiv. Allmodelsaretrained
for50kstepswithtraininglength512andfiverandomseeds. x†meansourlogvariantisstatistically
significantlybetterthanx. Thetestusedispairedtwo-sidedt-testwithα=0.05.
OpenWebText2
KERPLE
Extrp. ALiBi T5 Rotary Sinusoidal
(log) (power)
512 23.9±0.6 23.9±0.6 23.9±0.6 23.7±±±0.6 24.2±0.6† 33±1†
1024 22.0±0.6 22.1±0.7 22.4±0.5† 21.9±±±0.6 32.8±1.7† 750±346†
2048 21.6±±±0.3 21.9±0.2† 22.5±0.2† 21.7±0.2 62.4±6.1† 5507±2607†
4096 21.2±±±0.4 21.5±0.5† 22.2±0.4† 22.5±0.6† 111±13.8† 14039±2325†
8192 21.3±±±0.4 21.6±0.4† 22.3±0.3† 25.5±1.3† 185±18.9† 22621±1927†
16384 21.4±±±0.6 21.6±0.6 22.5±0.5† 31.4±3.1† 269±33.0† 30046±4824†
GitHub
KERPLE
Extrp. ALiBi T5 Rotary Sinusoidal
(log) (power)
512 3.40±0.20 3.42±0.20 3.42±0.21 3.38±±±0.21 3.44±0.20† 4±0.2†
1024 3.04±0.14 3.07±0.16 3.15±0.17† 3.02±±±0.14 3.86±0.25† 105±39†
2048 2.86±0.10 2.90±0.08† 3.13±0.10† 2.84±±±0.09 5.94±0.64† 1380±404†
4096 2.74±±±0.05 2.79±0.06 3.04±0.08† 2.78±0.04† 11.1±1.55† 5217±1118†
8192 2.71±±±0.05 2.76±0.05 3.04±0.03† 2.95±0.13† 20.2±2.75† 10081±3583†
16384 2.75±±±0.16 2.76±0.13 3.02±0.13† 3.35±0.27† 31.3±5.20† 16443±8503†
ArXiv
KERPLE
Extrp. ALiBi T5 Rotary Sinusoidal
(log) (power)
512 6.07±0.26 6.10±0.26 6.12±0.26† 6.03±±±0.26 6.07±0.27 43±44
1024 5.61±0.10 5.65±0.10† 5.82±0.09† 5.58±±±0.09 7.49±0.34† 221±136†
2048 5.22±0.12 5.26±0.13† 5.71±0.14† 5.21±±±0.14 14.2±1.81† 730±343†
4096 5.20±±±0.10 5.25±0.09 5.87±0.08† 5.32±0.16† 30.1±4.32† 1998±497†
8192 5.01±±±0.10 5.06±0.15 5.74±0.13† 5.54±0.39† 54.3±6.22† 4228±2645†
16384 5.07±±±0.16 5.07±0.19 5.78±0.15† 6.25±0.61† 85.4±7.40† 6674±5696
Table4: TrainingTimeComparisononGitHub
KERPLE
ALiBi T5 Rotary Sinusoidal
(log) (power)
sec/step 0.307 0.321 0.302 0.340 0.332 0.298
7
5.3 ExperimentsonComplicatedKernels
Inadditiontothepracticalvariants(power&logarithmic)insection4,weconsidertwocomplicated
versionsofthecompositekernel,Eq.(4),asfollows.
(bias+wht) bias+weight:
kcomp([q m,m],[k n,n])=q m(cid:62)k n·exp(−r 3|m−n|r4)+c−r 1|m−n|r2
withr ,r >0and0<r ,r ≤2.
1 3 2 4
(3-para-log) 3-parameter-logarithmic:
kcomp([q m,m],[k n,n])=q m(cid:62)k n+c−r 1·log(1+r 2|m−n|r3)
withr ,r >0and0<r ≤2.
1 2 3
Recallthetensorproductpropertyofakernel: ifk isakernelonX andk isakernelonY,then
1 2
k((x,y),(x(cid:48),y(cid:48))) = k (x,x(cid:48))k (y,y(cid:48))isakernelonX ×Y. Therefore,(bias+wht)isthesetting
1 2
wherewetrainaweightexp(−r 3|m−n|r4)andabiaskernelc−r 1|m−n|r2.q m(cid:62)k nismultipliedby
theweightkernelandthenaddedwiththebiaskernel. (3-para-log)isthesettingwhereweconsider
|m−n|r3 inthelog. Whenr
3
=1,itisreducedtothelogarithmicvariantproposedinsection4.
WepluginthesecompositekernelkcompintoourKERPLEframework,Eq.(2),andtesttheperfor-
manceoftheseRPE.Comparedwithsection5.2,Table5suggeststhatthesevariantsdonothave
clearadvantageinextrapolationperformance,e.g.,3-para-logisslightlybetterinperplexitythanthe
(two-parameter)logarithmicvariant. Thus,enlargingthecomplexityofkernelsdoesnotnecessarily
givebetterperformanceinthecontextofRPE.
Table 5: Perplexity Comparison for KERPLE with Complicated Kernels on OpenWebText2,
GitHub,andArXiv. Allmodelsaretrainedfor50kstepswithtraininglength512andfiveseeds
random. OOMmeansoutofmemory.
OpenWebText2 GitHub ArXiv
Extrp.
(bias+wht) (3-para-log) (bias+wht) (3-para-log) (bias+wht) (3-para-log)
512 24.1±0.6 23.8±0.6 3.44±0.21 3.40±0.20 6.11±0.27 6.06±0.27
1024 22.2±0.6 22.0±0.7 3.08±0.15 3.04±0.13 5.66±0.09 5.61±0.10
2048 21.9±0.4 21.6±0.2 2.90±0.12 2.85±0.10 5.28±0.12 5.21±0.12
4096 21.5±0.5 21.2±0.4 2.79±0.06 2.73±0.05 5.31±0.08 5.18±0.09
8192 21.4±0.5 21.3±0.4 2.76±0.03 2.68±0.04 5.16±0.18 5.00±0.11
16384 OOM OOM OOM OOM OOM OOM
5.4 PlotsofKernelFunctions
Weplotkernelfunctionsincludingthepower,logvariants,andALiBifordifferentheadstoseetheir
contributionstosoftmax. WeusetheGitHubdatasetfordemonstration. PleaseseeFigure2,3,and4.
BothALiBianditsgeneralizedpowervariantquicklyreachaverynegativevalue. Incontrast,thelog
variantsuccessfullydiscoversseveralflatkernels,effectivelyextendingthewindowattention. This
corroboratesourpreviousobservationthatKERPLE-logcanutilizemoredistanttokeninformation.
Figure2: KernelFunctionsofLearnedbytheLogVariant.
8
Figure3: KernelFunctionsLearnedbythePowerVariant. Notethey-axisshouldbemultipliedby
1e8,whichisaverynegativevalue.
Figure4: KernelFunctionsLearnedbyALiBi.
5.5 Position-wisePerplexityEvaluation
We plot the position-wise perplexity with evaluation length=4096 in Figure 5. Please see Ap-
pendixA.6forsimilarlength=16384result. Theevaluationisdonebymeasuringthelossateach
positionineachsequenceandaveragingoverthesequences.
WenotethatPPL@512ofKERPLE-logisthelowestamongallmodelvariants.Wecanderiveseveral
criticalobservationsforevaluationlength=4096inFigure5: First,KERPLE-logliesbelowKERPLE-
log-windowed@512,indicatingitsusageofmoredistantinformationthanwindowattention: Ifour
modeldoesnotusemoreinformationotherthanafixed-window=512,they-valuesafterposition=512
shouldoverlapwiththelinewindowedat512. Thisisclearlynotthecase. Inaddition, thePPL
ofKERPLE-logcontinuestodecreasetilltheendof4096positions(Notplateauing). Second,T5
lies below KERPLE-log-windowed@512 most of the time and fluctuates around KERPLE-log-
windowed@512 after length=3000. It is still worse than KERPLE-log. Third, ALiBi lies above
KERPLE-log-windowed@512foralmostallthepositions,indicatingthatwindowattentionmightbe
abetterchoicethanALiBi.
Althoughwindowattentionisastrongbaseline,ourKERPLE-logisalmostlikeafreelunchcompared
towindowattention: Withonly24additionallearnableparameters(2para. foreachhead),thealmost
sametrainingspeed,andthesametrainlength=512aswindowattention,itisabletoachievelower
PPLsacrossdifferentpositions.
6 ConclusionandFutureWork
Ageneralframework,KERPLE,isproposedtokernelizerelativepositionalembeddingsforlength
extrapolation. AtthecoreofthisframeworkistheapplicationofCPDkernelsandthederivationof
practicalvariants. WeshowthattheseCPDkernelscanbeimplicitlyconvertedtoPDkernels,which
9
Figure 5: Position-wise Perplexity on GitHub at Evaluation Length=4096 Compared to Win-
dowAttention@512.
Position-wise Perplexity at Evaluation Length=4096
20.0
Rotary
17.5 ALiBi
T5
15.0
KERPLE-log
KERPLE-log-windowed@512
12.5
10.0
7.5
5.0
2.5
0.0
0 1000 2000 3000 4000
Position
keep the inner product interpretation of self-attention. We also demonstrate that the logarithmic
variantachievesexceptionalextrapolationperformanceonthreelargelanguagemodelingdatasets.
Webelieveourworkpavesthewayforsomeinterestingfuturedirectionsthatresolveourlimitations.
For instance, we can consider general kernel families and model non-monotonic effects due to
positionaldifferences. Inaddition,theuseoflearnableparametersinKERPLEmightenablebetter
generalizationtoinputshigherthanone-dimensional. Lastbutnotleast,thereisalwaysroomfor
improvingmemoryefficiencybyadjustingthemodelarchitectureandtrainingprocedure.
7 BroaderImpact
Ourworkdevelopsabetterunderstandingofrelativepositionalembeddingfortransformersbased
onexpressivekernelclassesthatadaptwelltovariousdatasets. Theresultsapplytodomainswhere
thepositionalinformationishelpfulinthemodeling,e.g.,naturallanguage,programminglanguage,
andDNA/proteinsequencesforbiology/medicine. Thestudiesoftransformersmayhavepositive
economiceffectsbyenablingnewtaskswhichcannotbedonebyhumansorenhancingaccuracyand
efficiency. Butinappropriateusecanhavenegativesocietalimpacts. Theseincludejoblossdueto
automation,theethicalchallengesfromimpropertextgeneration,andtheprivacyissuesinthedata
collectionprocess. Theseimplicationsapplytoanyresearchonnaturallanguageprocessingandare
notassociatedwithanyspecificwork.
8 Acknowledgement
Wethanktheanonymousreviewersfortheirinsightfulfeedbackandsuggestions. WethankPrinceton
ResearchComputingforthetechnicalsupportontheDellaandtheAdroitclusters. Thethirdauthor
acknowledgessupportfromNSFMRIAward: 1919452.
10
ytixelpreP
egarevA
References
StephenRoller,EmilyDinan,NamanGoyal,DaJu,MaryWilliamson,YinhanLiu,JingXu,Myle
Ott,EricMichaelSmith,Y-LanBoureau,andJasonWeston. Recipesforbuildinganopen-domain
chatbot. InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationfor
ComputationalLinguistics: MainVolume,pages300–325,Online,April2021.Associationfor
ComputationalLinguistics.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan,HarriEdwards, YuriBurda, NicholasJoseph, GregBrockman, etal. Evaluatinglarge
languagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021a.
JingqingZhang,YaoZhao,MohammadSaleh,andPeterLiu. Pegasus: Pre-trainingwithextracted
gap-sentencesforabstractivesummarization. InInternationalConferenceonMachineLearning,
pages11328–11339.PMLR,2020.
OfirPress,NoahSmith,andMikeLewis. Trainshort,testlong: Attentionwithlinearbiasesenables
inputlengthextrapolation. InInternationalConferenceonLearningRepresentations,2022.
ShunKiyono,SosukeKobayashi,JunSuzuki,andKentaroInui. SHAPE:Shiftedabsoluteposition
embeddingfortransformers. InProceedingsofthe2021ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,pages3309–3321,OnlineandPuntaCana,DominicanRepublic,
November2021.AssociationforComputationalLinguistics.
TatianaLikhomanenko,QiantongXu,GabrielSynnaeve,RonanCollobert,andAlexRogozhnikov.
Cape: Encodingrelativepositionswithcontinuousaugmentedpositionalembeddings. InM.Ran-
zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in
NeuralInformationProcessingSystems,volume34,pages16079–16092.CurranAssociates,Inc.,
2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. JournalofMachineLearningResearch,21(140):1–67,2020.
Pu-ChinChen,HenryTsai,SrinadhBhojanapalli,HyungWonChung,Yin-WenChang,andChun-
Sung Ferng. A simple and effective positional encoding for transformers. In Proceedings of
the2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2974–2988,
2021b.
UlmeWennbergandGustavEjeHenter.Thecasefortranslation-invariantself-attentionintransformer-
based language models. In Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 130–140, Online, August 2021. Association for
ComputationalLinguistics.
BernhardSchölkopf. Thekerneltrickfordistances. InT.Leen,T.Dietterich,andV.Tresp,editors,
AdvancesinNeuralInformationProcessingSystems,volume13.MITPress,2000.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessing
systems,30,2017.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. InProceedingsofthe2019Conferenceof
theNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage
Technologies,Volume1(LongandShortPapers),pages4171–4186,Minneapolis,Minnesota,June
2019.AssociationforComputationalLinguistics.
YinhanLiu, MyleOtt, Naman Goyal, JingfeiDu, MandarJoshi, DanqiChen, OmerLevy, Mike
Lewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretraining
approach. arXivpreprintarXiv:1907.11692,2019.
11
KevinClark,Minh-ThangLuong,QuocV.Le,andChristopherD.Manning. Electra: Pre-training
textencodersasdiscriminatorsratherthangenerators. InInternationalConferenceonLearning
Representations,2020.
ZhenzhongLan,MingdaChen,SebastianGoodman,KevinGimpel,PiyushSharma,andRaduSoricut.
Albert: A lite bert for self-supervised learning of language representations. In International
ConferenceonLearningRepresentations,2020.
AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever. Improvinglanguageunder-
standingbygenerativepre-training. OpenAIblog,2018.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolutional
sequencetosequencelearning. InProceedingsofthe34thInternationalConferenceonMachine
Learning-Volume70,ICML’17,page1243–1252.JMLR.org,2017.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy,VesStoyanov,andLukeZettlemoyer. Bart: Denoisingsequence-to-sequencepre-trainingfor
naturallanguagegeneration,translation,andcomprehension. arXivpreprintarXiv:1910.13461,
2019.
PeterShaw,JakobUszkoreit,andAshishVaswani.Self-attentionwithrelativepositionrepresentations.
In Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages
464–468,NewOrleans,Louisiana,June2018.AssociationforComputationalLinguistics.
Cheng-ZhiAnnaHuang,AshishVaswani,JakobUszkoreit,IanSimon,CurtisHawthorne,Noam
Shazeer,AndrewM.Dai,MatthewD.Hoffman,MonicaDinculescu,andDouglasEck. Music
transformer. InInternationalConferenceonLearningRepresentations,2019.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext. InProceedingsof
the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988,
Florence,Italy,July2019.AssociationforComputationalLinguistics.
ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,andQuocVLe.
Xlnet: Generalizedautoregressivepretrainingforlanguageunderstanding. InAdvancesinNeural
InformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.
ZhihengHuang,DavisLiang,PengXu,andBingXiang. Improvetransformermodelswithbetterrel-
ativepositionembeddings. InFindingsoftheAssociationforComputationalLinguistics: EMNLP
2020,pages3327–3335,Online,November2020.AssociationforComputationalLinguistics.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. {DEBERTA}: {DECODING}-
{enhanced}{bert}{with}{disentangled}{attention}. InInternationalConferenceonLearning
Representations,2021.
GuolinKe,DiHe,andTie-YanLiu. Rethinkingpositionalencodinginlanguagepre-training. In
InternationalConferenceonLearningRepresentations,2021.
JianlinSu,YuLu,ShengfengPan,BoWen,andYunfengLiu. Roformer: Enhancedtransformerwith
rotarypositionembedding. arXivpreprintarXiv:2104.09864,2021.
KanWu,HouwenPeng,MinghaoChen,JianlongFu,andHongyangChao.Rethinkingandimproving
relativepositionencodingforvisiontransformer. In2021IEEE/CVFInternationalConferenceon
ComputerVision(ICCV),pages10013–10021,2021a.
PhilippDufter, MartinSchmitt, andHinrichSchütze. PositionInformationinTransformers: An
Overview. ComputationalLinguistics,pages1–31,072022.
SebastianMika,BernhardSchölkopf,AlexSmola,Klaus-RobertMüller,MatthiasScholz,andGunnar
Rätsch. Kernelpcaandde-noisinginfeaturespaces. Advancesinneuralinformationprocessing
systems,11,1998.
12
ChristinaLeslie,EleazarEskin,andWilliamStaffordNoble. Thespectrumkernel: Astringkernel
forsvmproteinclassification. InBiocomputing2002,pages564–575.WorldScientific,2001.
InderjitSDhillon,YuqiangGuan,andBrianKulis. Kernelk-means: spectralclusteringandnor-
malizedcuts. InProceedingsofthetenthACMSIGKDDinternationalconferenceonKnowledge
discoveryanddatamining,pages551–556,2004.
HiroyukiTakeda,SinaFarsiu,andPeymanMilanfar. Kernelregressionforimageprocessingand
reconstruction. IEEETransactionsonimageprocessing,16(2):349–366,2007.
Yao-HungHubertTsai,ShaojieBai,MakotoYamada,Louis-PhilippeMorency,andRuslanSalakhut-
dinov. Transformerdissection: Anunifiedunderstandingfortransformer’sattentionviathelens
ofkernel. InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguage
Processingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-
IJCNLP),pages4344–4353,HongKong,China,November2019.AssociationforComputational
Linguistics.
ChuhanWu,FangzhaoWu,andYongfengHuang. DA-transformer: Distance-awaretransformer.
In Proceedings of the 2021 Conference of the North American Chapter of the Association for
ComputationalLinguistics:HumanLanguageTechnologies,pages2059–2068,Online,June2021b.
AssociationforComputationalLinguistics.
ShengjieLuo,ShandaLi,TianleCai,DiHe,DinglanPeng,ShuxinZheng,GuolinKe,LiweiWang,
andTie-YanLiu. Stable,fastandaccurate: Kernelizedattentionwithrelativepositionalencoding.
AdvancesinNeuralInformationProcessingSystems,34,2021.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D.Koller,Y.Singer,andS.Roweis,editors,AdvancesinNeuralInformationProcessingSystems,
volume20.CurranAssociates,Inc.,2007.
AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFrançoisFleuret. Transformersare
RNNs: Fastautoregressivetransformerswithlinearattention. InHalDauméIIIandAartiSingh,
editors,Proceedingsofthe37thInternationalConferenceonMachineLearning,volume119of
ProceedingsofMachineLearningResearch,pages5156–5165.PMLR,13–18Jul2020.
YifanChen,QiZeng,HengJi,andYunYang. Skyformer: Remodelself-attentionwithgaussian
kernelandnystr\"ommethod. AdvancesinNeuralInformationProcessingSystems,34,2021c.
YunyangXiong,ZhanpengZeng,RudrasisChakraborty,MingxingTan,GlennFung,YinLi,and
VikasSingh. Nyströmformer: Anystöm-basedalgorithmforapproximatingself-attention. In
Proceedingsofthe...AAAIConferenceonArtificialIntelligence.AAAIConferenceonArtificial
Intelligence,volume35,page14138.NIHPublicAccess,2021.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong.
Randomfeatureattention. InInternationalConferenceonLearningRepresentations,2021.
KrzysztofMarcinChoromanski,ValeriiLikhosherstov,DavidDohan,XingyouSong,AndreeaGane,
TamasSarlos,PeterHawkins,JaredQuincyDavis,AfrozMohiuddin,LukaszKaiser,DavidBen-
jaminBelanger,LucyJColwell,andAdrianWeller. Rethinkingattentionwithperformers. In
InternationalConferenceonLearningRepresentations,2021.
ZhenQin,WeixuanSun,HuiDeng,DongxuLi,YunshenWei,BaohongLv,JunjieYan,Lingpeng
Kong,andYiranZhong. cosformer: Rethinkingsoftmaxinattention. InInternationalConference
onLearningRepresentations,2022.
ChristianBerg,JensPeterReusChristensen,andPaulRessel. Harmonicanalysisonsemigroups:
theoryofpositivedefiniteandrelatedfunctions,volume100. Springer,1984.
IsaacJSchoenberg. Metricspacesandpositivedefinitefunctions. TransactionsoftheAmerican
MathematicalSociety,44(3):522–536,1938.
13
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
vonPlaten,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,SylvainGugger,
MariamaDrame,QuentinLhoest,andAlexanderRush. Transformers: State-of-the-artnatural
languageprocessing. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
LanguageProcessing: SystemDemonstrations,pages38–45,Online,October2020.Association
forComputationalLinguistics.
LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,JasonPhang,
HoraceHe,AnishThite,NoaNabeshima,etal. Thepile: An800gbdatasetofdiversetextfor
languagemodeling. arXivpreprintarXiv:2101.00027,2020.
Sid Black, Stella Biderman, Alex Andonian, Quentin Anthony, Preetham Gali, Leo Gao, Eric
Hallahan,JoshLevy-Kramer,ConnorLeahy,LucasNestler,KipParker,JasonPhang,Michael
Pieler,ShivanshuPurohit,TriSongz,PhilWang,andSamuelWeinbach. GPT-NeoX:Largescale
autoregressivelanguagemodelinginpytorch,2021. URLhttp://github.com/eleutherai/
gpt-neox.
MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatan-
zaro. Megatron-lm: Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism.
arXivpreprintarXiv:1909.08053,2019.
JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYuxiongHe. Deepspeed: Systemoptimiza-
tionsenabletrainingdeeplearningmodelswithover100billionparameters. InProceedingsofthe
26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,pages
3505–3506,2020.
Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,
SasikanthAvancha,DharmaTejaVooturi,NatarajJammalamadaka,JianyuHuang,HectorYuen,
etal. Astudyofbfloat16fordeeplearningtraining. arXivpreprintarXiv:1905.12322,2019.
A Appendix
A.1 ProofofCPDShiftLemma
Lemma 1 (CPD Shift Lemma). Let k : X ×X → R be a conditionally positive definite (CPD)
kernel. Then,thereexistsc≥0suchthatc+k(x,y)isapositivedefinitekernel.
Proof. LetK =[k(x ,x )]N bethematrixgeneratedby{x ,...,x }withN ∈N. Consider
i j i,j=1 1 N
f (v)=v(cid:62)(c11(cid:62)+K)v =c(v(cid:62)1)2+v(cid:62)Kv.
c
Wewanttoshowthereexistsalargeenoughcsuchthatf (v)≥0forallv ∈{v :(cid:107)v(cid:107)=1}.
c
(i) Itissufficienttoconsideraaa∗∗∗===mmmiiinnn vvv(cid:62)(cid:62)(cid:62)KKKvvv<<<000...
vvv:::(cid:107)(cid:107)(cid:107)vvv(cid:107)(cid:107)(cid:107)===111
Leta∗bethesolutiontotheminimization:
a∗ = min v(cid:62)Kv.
v:(cid:107)v(cid:107)=1
Sincev(cid:62)Kv iscontinuousinv and{v : (cid:107)v(cid:107) = 1}iscompact(i.e., closedandbounded), a∗
mustexist. Ifa∗ ≥0,K ispositivesemidefiniteandf (v)≥0forc≥0. Thus,withoutlossof
c
generality,weassumea∗ <0.
(ii) ItissufficienttoconsiderKKK withoutzeroeigenvalues(i.e.,fullrank).
Ifthereexistsv suchthatKv = 0, thenc ≥ 0isenoughtosatisfyf (v ) ≥ 0. Foranyv
0 0 c 0 1
satisfyingv(cid:62)v =0,wehave(v +v )(cid:62)K(v +v )=v(cid:62)Kv . Therefore,whetherthereexists
1 0 1 0 1 0 1 1
ctohavef (v)≥0doesn’tdependontheeigenvectorcorrespondingtozeroeigenvalue(ifthere
c
issuchavector). ThismeansitisenoughtoconsiderK withoutzeroeigenvalues.
14
(iii) ItissufficienttoconsiderstrictCPD.
Bydefinitionofconditionalpositivedefiniteness(CPD),weknowv(cid:62)Kv ≥0whenv(cid:62)1=0.
SinceK hasnozeroeigenvalue,wecannothavev(cid:62)Kv = 0whenv(cid:62)1 = 03. Thismeansthe
inequalityisstricthere: v(cid:62)Kv >0whenv(cid:62)1=0,anditisenoughtoconsiderstrictCPD.
(iv) Itissufficienttoshowthereexists(smallenough)δδδ>>>000suchthatvvv(cid:48)(cid:48)(cid:48)∈∈∈TTT ⇒⇒⇒vvv(cid:48)(cid:48)(cid:48)(cid:62)(cid:62)(cid:62)KKKvvv(cid:48)(cid:48)(cid:48)>>>000.
δδδ
NoteT isdefinedas
δ
T ={v(cid:48) :|v(cid:48)(cid:62)1|<δ, (cid:107)v(cid:48)(cid:107)=1}.
δ
Foranyv(cid:48) ∈T ,ifv(cid:48)satisfiesv(cid:48)(cid:62)Kv(cid:48) >0,thenc≥0isenoughtohavef (v(cid:48))≥0. Conversely,
δ c
when|v(cid:62)1|≥δand(cid:107)v(cid:107)=1,observethat
f (v)=c(v(cid:62)1)2+v(cid:62)Kv ≥c(v(cid:62)1)2+a∗ ≥cδ2+a∗
c
Then,f (v)≥0whenc≥ −a∗ . Therefore,weneedtoprovev(cid:48) ∈T ⇒v(cid:48)(cid:62)Kv(cid:48) >0forsmall
c δ2 δ
enoughδ.
(v) Leveraging Continuity. Consider v(cid:48) satisfying (cid:107)v(cid:48) −v(cid:107) < δ with v ∈ S = {v : (cid:107)v(cid:107) =
2
1, v(cid:62)1 = 0}. Sincev(cid:62)Kv iscontinuousinv and(cid:107)K(cid:107) < ∞,forany(cid:15) > 0andanyv ∈ S,a
smallenoughδ >0gives|v(cid:48)(cid:62)Kv(cid:48)−v(cid:62)Kv|<(cid:15).
2
Toseethis,takingv(cid:48) =v+pwith(cid:107)p(cid:107)<δ ,wehave
2
|v(cid:48)(cid:62)Kv(cid:48)−v(cid:62)Kv|=|p(cid:62)Kv+v(cid:62)Kp+p(cid:62)Kp| ≤ (cid:107)K(cid:107)(2(cid:107)p(cid:107)+(cid:107)p(cid:107)2)≤(cid:107)K(cid:107)(2δ +δ2).
2 2
(cid:107)v(cid:107)=1
(cid:112)
Therefore,0<δ < 1+(cid:15)/(cid:107)K(cid:107)−1isenoughtohave|v(cid:48)(cid:62)Kv(cid:48)−v(cid:62)Kv|<(cid:15).
2
BydefinitionofstrictCPD,weknowmin v(cid:62)Kv =λ>0. Thus,take(cid:15)<λ,asmallenough
v∈S
δ givesv(cid:48)(cid:62)Kv(cid:48) > v(cid:62)Kv−(cid:15) >= λ−(cid:15) > 0. Inotherwords,thereexistsasmallenoughδ
2 2
suchthatv(cid:48)(cid:62)Kv(cid:48) >0forv(cid:48) ∈S ={v(cid:48) :(cid:107)v(cid:48)−v(cid:107)<δ , v ∈S}.
δ2 2
(vi) Proving∃∃∃δδδ>>>000sss...ttt...vvv(cid:48)(cid:48)(cid:48)∈∈∈TTT ⇒⇒⇒vvv(cid:48)(cid:48)(cid:48)(cid:62)(cid:62)(cid:62)KKKvvv(cid:48)(cid:48)(cid:48)>>>000.
δδδ
Due to (iv), we want to show ∃ δ > 0 s.t. v(cid:48) ∈ T ⇒ v(cid:48)(cid:62)Kv(cid:48) > 0. We will prove by the
δ
conclusionof(v). Let(cid:107)v(cid:48)(cid:107)=1,v(cid:48)(cid:62)1=rwith|r|<δandv(cid:48)(cid:48) =v(cid:48)− r1. Wehave
n
(cid:114) (cid:114)
r r r2
v(cid:48)(cid:48)(cid:62)1=0, (cid:107)v(cid:48)−v(cid:48)(cid:48)(cid:107)= √ , (cid:107)v(cid:48)(cid:48)(cid:107)= (cid:107)v(cid:48)− 1(cid:107)2 = 1− .
n n n
(cid:113)
Takev = v(cid:48)(cid:48) = v(cid:48)(cid:48) ,whereq = 1− r2. Wehavev(cid:62)1=0, (cid:107)v(cid:107)=1and
(cid:107)v(cid:48)(cid:48)(cid:107) q n
1 1r 1 r2 1 1r2
(cid:107)v(cid:48)−v(cid:107)2 =(cid:107)(1− )v(cid:48)+ 1(cid:107)2 =(1− )2+ +2(1− )
q qn q nq2 q q n
1 r2 2 1 1 (cid:16) r2 (cid:17)
=(1− )2+ ( − )= (q−1)2+ (2q−1)
q n q q2 q2 n
1 (cid:16) r2 r2 r2(cid:17)
= 1− −2q+1+ 2q−
1− r2 n n n
n
1 (cid:16) r2 r2 r2(cid:17)
= 1− −2q(1− )+1− =2−2q
1− r2 n n n
n
(cid:114)
r2 1r2 r2 √ x
=2(1− 1− )≈2(1−1+ )= ( 1−x≈1− when|x|(cid:28)1)
n 2 n n 2
Thus,(cid:107)v(cid:48)−v(cid:107) = O(√|r n|) ≤ O(√δ n) < δ
2
forsmallenoughδ. Thisimpliesthat,withasmall
enoughδ,foranyv(cid:48) ∈T ,wecanfindv ∈S suchthat(cid:107)v(cid:48)−v(cid:107)<δ . Thus,v(cid:48) ∈S ,andby
δ 2 δ2
(v),wearriveatv(cid:48)(cid:62)Kv(cid:48) >0.
3Byspectraldecomposition,v(cid:62)Kv=(cid:80) λ (v(cid:62)u )2 ≥0.Sincethereisnoλ =0,theinequalityisstrict.
i i i i
15
A.2 Shift-invariantKernelswithBoundedandUnboundedRanges
Definition1impliesashift-invariantkernelisgeneratedbyaunivariatefunctionf : X → R. To
characterizethesetofvalidunivariatefunctions,weintroducethepositivedefinitefunctionsasbelow.
Definition3(Positivedefinitefunction). A(real)positivedefinitefunctionisafunctionf :X →R
such that for any integer N and any set of N points {x ∈ X}N , the N × N matrix A =
i i=1
[f(x −x )]N ispositivesemidefinite.
i j i,j=1
Wewillinterchangetheideasofshift-invariantkernelsandpositivedefinitefunctionsbecausethey
are equivalent by definition. Any statement in positive definite functions can be translated into
shift-invariantkernels,andviceversa. Becauseofthis,wewillusesomefactsaboutthepositive
definitefunctionstoderivetheshift-invariantkernelsofourinterest.
GeneralizingClassicalBoundedShift-invariantKernel. Intheliterature,therehavebeenstudies
onapplyingkernelsintheattentionmechanism[Tsaietal.,2019,Choromanskietal.,2021,Peng
etal.,2021]. OneofthemostcommonapproachesistoconsidertheGaussiankernel:
k(m,n)=exp(−γ(m−n)2), γ >0.
Notethe Gaussiankernel isbounded (k(m,n) ∈ (0,1] forthe caseabove). To generalizeit to a
broaderclassofboundedshift-invariantkernels,observethattheGaussiankernelgeneratedbya
positivedefinitefunctionoftheformf(x)=exp(−|x|2). Sincethereisnostrongreasontostickto
thepowerof2,onemaygeneralizeittoabroaderclassofpositivedefinitefunctionsasbelow.
Fact5(Corollary3ofSchoenberg[1938]). exp(−|x|p)ispositivedefiniteif0 < p ≤ 2andnot
positivedefiniteifp>2.
Fact5impliesthat, ifonewantstofindaclassofboundedshift-invariantkernel(i.e., k(m,n)is
withinsomefixedintervalforanym,n),thenk(m,n)=exp(−a|m−n|p)witha>0andp∈(0,2]
maybeofinterest.
ConstructingUnboundedShift-invariantKernels. AlimitationofFact5isthatitonlygenerates
kernelswithaboundedrange(here,therangeisboundedin(0,1]). Insituationswherethereareno
explicitbounds,onemightwanttoconsiderkernelswithunboundedrange. Toconstructsuchkernels,
weutilizeakernelrepresentationtheorempresentedinSchoenberg[1938]:
Fact 6 (Theorem 4 of Schoenberg [1938]). f(x) is bounded away from zero (f(x) > 0) and its
positivepowersf(x)λ(λ>0)areallpositivedefiniteifandonlyiff(x)isoftheform
f(x)=exp(c+ψ(x)),
whereψ(x)ispositivedefiniteandcisarealconstant.
SinceFact6worksfornon-negativekernels,wecombineitwithFact5andshowthefollowingclass
ofshift-invariantkernelwithanunboundedrange.
Proposition1(KernelfromDistancePowers). Foranyp∈(0,2],thereexistsc ∈Rsuchthat
min
foranyc≥c ,
min
k(m,n)=c−|m−n|p,
isapositivedefinitekernel. Whenp>2,thereisnoctomakek(m,n)positivedefinite.
Proof. DuetoFact5,weknowexp(−|x|p)ispositivedefinitewhenp∈(0,2]. Sinceexp(−|x|p)>
0andexp(−λ|x|p)ispositivedefiniteforanyλ > 04,Fact6impliesthereexistsac(cid:48) ∈ Randa
positivedefiniteψ(x)suchthat
exp(−|x|p)=exp(c(cid:48)+ψ(x)).
Inotherwords,−c(cid:48)−|x|pisapositivedefinitefunction.Takec =−c(cid:48).Weseethatc −|m−n|p
min min
isashift-invariantpositivedefinitekernel. Finally,letk(m,n)=c−|m−n|pwithc≥c . The
min
N ×N matrix[k(x ,x )]N generatedbyk(m,n)onpoints{x ∈R}N obeys
i j i,j=1 i i=1
[k(x ,x )]N =[c −|x −x |p]N +(c−c )11(cid:62) (cid:23)(c−c )11(cid:62) (cid:23)0,
i j i,j=1 min i j i,j=1 min min
4exp(−λ|x|p)=exp(−|λ1/px|p)isaconstantrescalingofexp(−|x|p)andthereforeispositivedefinite.
16
where(cid:23)istheLoewnerorderand1=[1,...,1](cid:62) intheN-dimensionalvectorwithallones. This
showsk(m,n)isashift-invariantpositivedefinitekernelwhen0<p≤2. Theconclusiononp>2
isprovedbycontradiction. Whenp>2,ifthereexistsacsuchthatk(m,n)ispositivedefinite,then
exp(k(m,n))ispositivedefinite,whichcontradictstothecaseofp>2inFact5.
Prop.1introducesakernelwithunboundedrange(k(m,n)∈(∞,c])andisadaptedfromthep-th
powerofthedistance|m−n|. Sincethedistanceisanotionof"dissimilarity",−|m−n|pbecomes
anotionof"similarity",whichgivesasenseofkernel. Thereby,wecaninterprettheconstantcasthe
requiredvaluetoshift−|m−n|psuchthatc−|m−n|pbecomesapositivedefinitekernel.
Infact,−|m−n|pisaconditionalpositivedefinitekernelforp∈(0,2]Schölkopf[2000]. Therefore,
thefactthat−|m−n|pcanbecomeapositivedefinitekernelbyshiftingisnotacoincidence,asit
hasalreadyhadanintimaterelationtopositivedefinitekernels.
A.3 ExperimentsonGaussian-likeKernels
Since the prior work on shift-invariant kernels mainly focuses on Gaussian kernels, we present
preliminaryexperimentsonGaussian-likekernels. Comparedwithsection5.2,theperplexitiesof
thesekernelsarelargeateveryextrapolationlength. Thisverifiesourpreviousassertionthatthe
Gaussian-likekernelshavelimitedextrapolationability.
Becausethekernelcanbeusedasaweightorabias,weconsiderfourkindsofthecompositekernel
(seesection4)asfollows.
(2-para-bias) r ,r >0.
1 2
kcomp([q ,m],[k ,n])=q(cid:62)k +r exp(−r |m−n|2).
m n m n 1 2
(3-para-bias) r ,r >0and0<r ≤2.
1 2 3
kcomp([q m,m],[k n,n])=q m(cid:62)k n+r 1exp(−r 2|m−n|r3).
(1-para-wht) r >0.
1
kcomp([q ,m],[k ,n])=q(cid:62)k ·exp(−r |m−n|2).
m n m n 1
(2-para-wht) r >0and0<r ≤2.
1 2
kcomp([q m,m],[k n,n])=q m(cid:62)k n·exp(−r 1|m−n|r2).
(2-para-bias) and (1-para-wht) are the settings where we put the Gaussian kernel as a bias and
a weight, respectively. (3-para-bias) and (2-para-wht) generalize these settings by considering a
learnablepowerbetween0and2. Notewemustconstrainthepowerin(0,2];otherwise,thefunction
isnotpositivedefinite. SeeFact5fordetails.
ThesecompositekernelkcomparepluggedintotheKERPLEframework,Eq.(2),andareevaluatedon
OpenWebText2,GitHub,andArXivdatasets. Table6showstheGaussian-likekernelisbettertobea
weightinsteadofabias. AsdiscussedinAppendixA.2,theGaussian-likekernelsarebounded. To
someextent,thisimpliesthattheboundedpositivekernelcanmodelaweight. However,compared
withsection5.2theGaussian-likekernelshavelimitedadvantagesinextrapolation. Althoughthe
performancemightbeimprovedifthepowerofexp(−|x|p)isrelaxedfromp=2top∈(0,2],still
itcannotbeasgoodasthelogarithmicvariantaswedemonstrateinsection5.2. Therefore,whilethe
Gaussiankernelisfrequentlyusedintheliterature,weneedabetterclassofshift-invariantkernelsto
tacklethelengthextrapolationchallenge.
A.4 ExperimentsonLargeModel,LongerTrainingLength,andWikitext-103
Inthissubsection,wepresentadditionalexperimentson(a)largemodels,(b)longertraininglength,
and(c)Wikitext-103. Belowisthesummaryoftheexperiments.
(a) The1.3B large modelistrained ona machinewithtwoNVIDIAA100 GPUwith 40GBof
memory. WeadoptalmostallconfigurationsofXLGPT-NeoX5,exceptthatwechangethetrain-
micro-batch-sizeto16,model-parallel-sizeto2,seq-lengthto512,andmax-position-embeddings
to512. Table8summarizestheconfigurationsofthe1.3Bmodel.
(b) The162MModelwithtrainingsequencelength=1024followsthesameconfigurationsasthe
onesinTable2exceptthatthetrainseq. lengthischangedto1024.
5https://github.com/EleutherAI/gpt-neox/blob/main/configs/XL.yml
17
Table 6: Extrapolation of Gaussian-like kernels on OpenWebText2, GitHub, and ArXiv. All
modelsaretrainedfor50kstepswithtraininglength512andfiverandomseeds.
OpenWebText2
Extrp.
1-para-wht 2-para-wht 2-para-bias 3-para-bias
512 33.8±1.1 24.8±0.9 58.4±71.6 26.4±0.5
1024 32.5±0.8 23.0±0.8 88.7±62.6 75.3±37.8
2048 34.1±0.6 22.7±0.4 406±101 2629±4024
4096 35.6±0.9 22.6±0.6 2590±3211 37557±67936
8192 39.2±1.1 23.2±0.3 10829±18855 189216±369499
GitHub
Extrp.
1-para-wht 2-para-wht 2-para-bias 3-para-bias
512 7.78±0.48 3.56±0.23 4.08±0.85 3.67±0.22
1024 7.85±0.40 3.19±0.17 4.63±0.59 4.23±0.57
2048 8.08±0.21 3.01±0.09 18.8±6.8 20.0±4.8
4096 8.47±0.43 2.93±0.09 75.8±32.2 94.0±24.7
8192 9.41±0.75 3.05±0.20 207±110 261±86
ArXiv
Extrp.
1-para-wht 2-para-wht 2-para-bias 3-para-bias
512 10.6±0.4 6.18±0.25 6.73±0.30 7.12±1.43
1024 10.7±0.2 5.73±0.11 7.07±0.63 7.27±0.69
2048 10.8±0.3 5.35±0.15 20.4±9.3 23.5±8.4
4096 11.6±0.3 5.44±0.14 80.6±49.4 131±140
8192 12.1±0.2 5.50±0.27 220±138 437±591
Table7: TrainingTimeComparisonforGaussian-likeKernelsonGitHub.
1-para-wht 2-para-wht 2-para-bias 3-para-bias
sec/step 0.326 0.327 0.324 0.351
(c) The Wikitext-103 model is implemented on ALiBi’s GitHub6 with exactly the same config-
urations(247Mparameters), exceptthatthefunctionbuffered_future_mask()atline1011of
attention_with_linear_biases/fairseq/models/transformer.pyisadaptedtoourKERPLE-log.
Table8: 1.3BModelConfigurations.
#Layers HiddenSize #AttentionHeads TrainSeq. Len. #TrainableParams.
24 128 16 512 1.3B
Optimizer BatchSize TrainSteps Precision #TrainableParams. forRPEs
Adam(lr2e-4) 32 150,000 float16 48
Table 9 shows the results on the large model (1.3B). Compared with the small model results in
Table 3, we see that T5 bias becomes weaker than KERPLE-log and ALiBi, and KERPLE-log
remainsstrongerthanALiBionGitHubandArXivdatasets. Thisisexplainedbythetendencyof
overfitting. ObservethatbothT5andKERPLElearnthepositionalembeddingswhileALiBiuses
fixedones. T5andKERPLEhaveahighertendencyofoverfitting. Alargermodel(1.3B>162M)or
anoisydataset(OpenWebText2>GitHub,ArXiv)positsahigherriskofoverfitting. Hence,wesee
thatT5biasisweakonalargemodel,andKERPLE-logonlyextrapolateswellonGitHubandArXiv.
Again,Table9showstheresultsonlongtraininglength(1024). comparedwiththeshorttraining
length(512)inTable3,KERPLE-logremainsbetterthanALiBiandT5bias,especiallyonlonger
evaluationlength. ThisshowstherobustnessofKERPLE-logoverdifferenttraininglengths.
Table10comparesKERPLE-logwithALiBiusingALiBi’simplementationandconfigurations. The
resultsshowthatKERPLE-logissuperiortoALiBionWikitext-103.
6https://github.com/ofirpress/attention_with_linear_biases
18
Table9: PerplexityComparisonforLargeModels(1.3B)andLongTrainingLength(1024)on
GitHub,ArXiv,OpenWebText2. Duetothetimeconstraintandlimitedcomputingresources,we
arenotabletoobtainthenumbersforthelargemodel(1.3B)onOpenWebText2fornow. Allmodels
aretrainedwithfiverandomseeds. x†meansourlogvariantisstatisticallysignificantlybetterthanx.
Thetestusedispairedtwo-sidedt-testwithα=0.05.
162MModel. Trainlength,steps=1024,50k. 1.3BModel. Trainlength,steps=512,150k.
GitHub GitHub
Extrp.
KERPLE-log ALiBi T5bias KERPLE-log ALiBi T5bias
512 - - - 2.88±0.11 2.88±0.11 2.93±0.11†
1024 2.83±0.16 2.84±0.16† 2.81±0.16 2.60±0.12 2.62±0.11† 2.64±0.11†
2048 2.70±0.07 2.82±0.07† 2.68±0.07 2.44±0.05 2.58±0.05† 2.47±0.07†
4096 2.53±0.04 2.77±0.06† 2.54±0.04 2.46±0.11 2.65±0.12† 2.49±0.12
8192 2.42±0.03 2.74±0.02† 2.57±0.06† 2.44±0.13 2.57±0.13† 2.57±0.13†
16384 2.48±0.11 2.80±0.11† 3.10±0.34† 2.60±0.07 2.61±0.07 3.16±0.35†
ArXiv ArXiv
Extrp.
KERPLE-log ALiBi T5bias KERPLE-log ALiBi T5bias
512 - - - 5.56±0.15 5.58±0.16 5.62±0.15†
1024 5.23±0.09 5.26±0.09 5.20±0.10 4.87±0.07 4.94±0.07† 4.92±0.06†
2048 4.76±0.12 4.98±0.18† 4.74±0.12 4.50±0.16 4.87±0.17† 4.55±0.16†
4096 4.75±0.10 5.31±0.13† 4.97±0.27 4.45±0.06 4.97±0.13† 4.53±0.08†
8192 4.54±0.10 5.25±0.15† 6.55±0.97† 4.47±0.20 4.94±0.16† 4.65±0.15†
16384 4.62±0.15 5.35±0.19† 16.0±4.77† 4.65±0.24 4.94±0.07 5.25±0.26†
Extrp. OpenWebText2 OpenWebText2
KERPLE-log ALiBi T5bias KERPLE-log ALiBi T5bias
512 - - - 17.5±0.3 17.5±0.4 17.8±0.3†
1024 19.2±0.1 19.3±0.2 19.1±0.1 16.6±0.6 16.7±0.6 16.9±0.6†
2048 19.3±0.2 19.5±0.1 19.2±0.2
16.2±0.4 16.4±0.4† 16.7±0.4†
4096 18.6±0.3 19.0±0.3† 19.2±0.4†
16.4±0.8 16.5±0.5 18.0±0.9†
8192 18.7±0.5 19.3±0.4† 24.0±1.1†
16.9±0.7 16.5±0.1 22.7±3.7†
16384 18.8±0.5 19.2±0.3† 50.8±6.5†
17.8±1.2 16.5±0.3 37.1±13.1†
Table10:PerplexityComparisononWikitext-103. Toensureafaircomparison,themodel(247M)
is trained on ALiBi’s codebase with exactly the same configurations except for the positional
embeddings. TheresultsshowthatKERPLE-logissuperiortoALiBionWikitext-103.
trainlength512 trainlength2048
Extrp. length 512 1024 1536 2048 3072 2048 3072
ALiBi 19.73 18.81 18.50 18.48 18.40 17.91 17.64
KERPLE-log 19.69 18.76 18.37 18.29 18.24 17.84 17.56
A.5 AdditionalAnalyses
SincethepowerandlogarithmicvariantsderivedfromKERPLEachievesuperiorperformanceon
lengthextrapolationacrossvariousdatasets,weinvestigatetheunderlyingreasonbyvisualizingthe
effectivelengthasshowninFigure6. Thevisualizationworksinthefollowingprocedure.
1. Foreachtrainingdataset,thelearnableparameters(r(h),...,r(h))associatedwitheachheadh(12
1 (cid:96)
intotal)areextractedfromthemodelcheckpoint.TheCPDkernelatheadhisk˜(h) =k˜ .
r(h),...,r(h)
1 (cid:96)
Boththepowerandthelogarithmicvariantsincorollary1undergoasimilarprocedure. Theonly
differenceisthattheirk˜’saredifferent.
2. Foreachheadh,wecomputetheeffectivelengthofk˜(h) aseff(h) = min |m−n|.
k˜(h)(0,|m−n|)<−2
Thatis, therelativepositionaldifference|m−n|suchthatk˜(m,n) shif =t-inv. k˜(0,|m−n|)just
becomessmallerthan-2. Notek˜(0,|m−n|)strictlydecreasesin|m−n|,sothereisonlyone
19
Figure 6: Number of Heads with Effective Lengths ≤≤≤|||mmm−−−nnn||| for different choices of CPD
kernelsanddatasets. SeesectionA.5fordetails.
(b)LogarithmicVariant:
(a)PowerVariant:−a|m−n|p −alog(1+b|m−n|)
12 12
10 10
8 8
6 6
4 GitHub 4 GitHub
Arxiv Arxiv
2 OpenWebText2 2 OpenWebText2
ALiBi ALiBi
0 0
0 5000 10000 15000 20000 0 5000 10000 15000 20000
Positional Differences Between Tokens |m-n| Positional Differences Between Tokens |m-n|
possiblevalue. Wepick−2herebecausek˜isabiasandisfollowedbytheSoftmaxnormalization.
Abiasof−2orsmallercanmakeagreatimpactontheoutputofSoftmax7. eff(h)isinterpreted
astheeffectivelengthbecause,when|m−n|<eff(h),theattenuationduetok˜(h)isnotstrong.
When|m−n|>eff(h),theattenuationisstrongandhasalargeimpactonq(cid:62)k +k˜(h)(m,n).
m n
3. Then, foreach|m−n| ∈ [0,...,20480], wecountthenumberofheadsthatsatisfieseff(h) ≤
|m−n|. ThisgivesacumulativeplotasshowninFigure6,wherethex-axisis|m−n|andthe
y-axisisCount({h: h∈[1,...,12], eff(h) ≤|m−n|}).
4. Repeattheabovestepsforotherdatasetsandkernels.
InterpretationofCurves. Forapoint(x,y)onacurve,itmeansthattherearey headswithat
least−2biaswhenthetokendistance|m−n|isgreaterthanx. Inotherwords,theslowerthey
convergesto12,thelongertheinter-tokenrangethatthemodelfocuseson.
TheAdvantageofLearnableParameters. WeobservethatALiBi[Pressetal.,2022]produces
thesamecurvenomatterwhichdatasetisused. ThereasonisthatALiBiselectsafixedparameter
r =2− H8h atheadhforitslinearbias−r|m−n|(H headsintotal)regardlessofthedataset. While
thisstrategyisusefulforextrapolation,wehypothesizethatdifferentdatasetsmighthavedifferent
characteristics,e.g.,theaveragedistanceofhighlyrelatedtokensshoulddifferamongthedatasetsas
showninFigure6. Thesecharacteristicsareeasieradaptedbylearnableparameters. Thus,webelieve
thatlearnableparametershavemoreadvantagesincapturingthedataset-dependentcharacteristics.
TrendsAcrossDatasets. WenoticethatbothkernelstrainedonOpenWebText2tendtofocusmore
on distant relations. This makes sense because OpenWebText2 has the highest perplexity scores
amongalldatasets,implyingthatmorecontextisneededtodisambiguatethenextpredictedtoken.
TheoppositetrendholdsforArxivandGitHubdatasets,whichisreasonableconsideringtheirlower
perplexityscores.
CharacteristicsLearnedbyKernels. Underanydataset,thelogarithmicvarianttendstofocus
moreondistantrelationsthanthepowervariantdoes. Wecanexplainitthroughtheirfunctional
forms. Becauselogarithm(−alog(1+b|m−n|))decaysmuchslowerthanpower(−a|m−n|p)
does,thelogvariantmightencouragethefocusondistantrelations.
A.6 Position-wisePerplexityforLength=16384
WecandrawsimilarconclusionsfromFigure7:
1. KERPLE-log lies below KERPLE-log-windowed@512 most of the time, indicating its
usageofmoredistantinformationthanwindowattention.
7SinceSoftmaxisanexponentiatedfunction,a-2biasintheSoftmax’sargumentroughlygivesanattenuation
ofexp(−2)≈0.135.
20
sdaeH
#
sdaeH
#
2. ThePPLofT5explodes.
3. ThePPLofALiBidoesnotexplode,butitisstillworsethanwindowattention,i.e. lies
aboveKERPLE-log-windowed@512.
Figure7: Position-wisePerplexityonGitHubatEvaluationLength=16384ComparedtoWin-
dowAttention@512.
Position-wise Perplexity at Evaluation Length=16384
20.0
Rotary
17.5 ALiBi
T5
15.0
KERPLE-log
KERPLE-log-windowed@512
12.5
10.0
7.5
5.0
2.5
0.0
0 2500 5000 7500 10000 12500 15000
Position
A.7 TheChoiceofcodebaseandHyperparameters
Weadoptalmostallthehyperparameters(exceptbatchsizetofitinourGPU)andallimplementations
oftheT5bias,ALiBi,Rotary,andSinusoidalbaselinesfromtheGPT-NeoXcodebase. Toensurefair
comparisons,wedidnotfine-tunehyper-parametersforKERPLE.Thedatasetsweusedareexactly
thesameastheonesreleasedwiththeGPT-NeoXcodebase. Wejustrantheirprepare_data.pyscript
toautomaticallydownloadandparsethedatasets. Allourcodewasuploadedwiththesubmissionon
openreview,andhttps://github.com/EleutherAI/gpt-neoxistheoriginalGitHubrepository.
Asasidenote,wechosethiscodebaseandadoptedtheirparametersettingsbecauseitisbuiltby
EleutherAI, which is a well-known and truly non-profit group of researchers publishing various
famouspretrainedmodelsforacademiaincludingGPT-J-6BandGPT-NeoX-20B.
21
ytixelpreP
egarevA

Towards Inference-Oriented Reading Comprehension: ParallelQA
SoumyaWadhwa VarshaEmbar MatthiasGrabmair EricNyberg
∗ ∗
CarnegieMellonUniversity
soumyaw, vembar, mgrabmai, en09 @andrew.cmu.edu
{ }
Abstract help perform such reasoning. Although state-of-
the-art deep learning models for machine reading
In this paper, we investigate the tendency of
are believed to have such reasoning capabilities,
end-to-end neural Machine Reading Compre-
the limited ability of these models to generalize
hension(MRC)modelstomatchshallowpat-
indicates certain shortcomings. We believe that it
terns rather than perform inference-oriented
is important to develop benchmarks which give a
reasoningonRCbenchmarks. Weaimtotest
the ability of these systems to answer ques- realisticsenseofasystem’sRCcapabilities. Thus,
tionswhichfocusonreferentialinference. We ourgoalinthispaperistwo-fold:
propose ParallelQA, a strategy to formulate ProofofConcept: Weproposeamethodtocreate
such questions using parallel passages. We anRCdatasetthatassessesamodel’sabilityto:
also demonstrate that existing neural models
move beyond lexical pattern matching be-
failtogeneralizewelltothissetting. •
tweenthequestionandpassage,
infer the correct answers to questions which
1 Introduction •
containreferringexpressions,and
Reading Comprehension (RC) is the task of read- generalizetodifferentlanguagestyles.
•
ing a body of text and answering questions about Analysis of Existing Models: We test three
it. It requires a deep understanding of the infor- end-to-end neural MRC models, which perform
mationpresentedinordertoreasonaboutentities, well on SQuAD (Rajpurkar et al., 2016), on a
actions, events, and their interrelationships. This few question-answer pairs generated using our
necessitateslanguageunderstandingskillsaswell methodology. Wedemonstratethatitisindeeddif-
asthecognitiveabilitytodrawinferences. ficult for these systems to answer such questions,
Recent efforts in creating large-scale datasets alsoindicatingtheirtendenciestoresorttoshallow
have triggered a renewed interest in the RC task, patternmatchingandoverfittotrainingdata.
with subsequent development of complex end-
to-end solutions featuring neural models. While 2 ExistingDatasets
these models do exceedingly well on the specific
In this work, we focus on datasets with multi-
datasets they are developed for (some reaching
word spans as answers rather than cloze-style RC
or even surpassing human performance), they do
datasets like MCTest (Richardson et al., 2013),
not perform proportionally across datasets. Weis-
CNN / Daily Mail (Hermann et al., 2015) and
senbornetal.(2017)haveshownthatusingacon-
Children’sBookTest(Westonetal.,2015).
text or type matching heuristic to derive simple
The Stanford Question Answering Dataset
neural baseline architectures can achieve compa-
(SQuAD) (Rajpurkar et al., 2016) was one of the
rable results. Our experiments also indicate that
firstlargescaleRCdatasets(over100kQApairs),
patternmatchingcanworkwellonthesedatasets.
wheretheanswertoeachquestionisaspaninthe
Inference, an important RC skill (Spearritt,
given passage. For its collection, different sets
1972; Strange, 1980), is the ability to understand
of crowd-workers were asked to formulate ques-
themeaningoftextwithoutalltheinformationbe-
tions and answers using passages obtained from
ing stated explicitly. Table 5, Section A describes
500 Wikipedia articles. However, this resulted
thetypesofinferencethatwemayencounterwhile ∼
in the questions having similar word patterns to
comprehendingapassagealongwiththecuesthat
the sentences containing the answers. We empir-
∗EqualContribution ically demonstrate this in Table 1, where we ob-
1
ProceedingsoftheWorkshoponGeneralizationintheAgeofDeepLearning,pages1–7
NewOrleans,Louisiana,June5,2018.(cid:13)c2018AssociationforComputationalLinguistics
served that the sentence in the passage with the eralimportantaspectsofRCthatremainuntested.
highestlexicalsimilaritytothequestioncontained
theanswer 80%ofthetime. Finalanswerstend 3 ParallelQA
∼
tobeshort,withanaveragespanlengthofaround
InanRCtask,thereisaneedtoincorporateques-
3tokens,andarelargelyentities(40.88%). Subra-
tions that require not just lexical and syntactic
manianetal.(2017)andYangetal.(2017)provide
prowess, but reference resolution, multiple steps
evidenceforregularpatternsincandidateanswers
of reasoning, and use of world knowledge. These
thatneuralmodelscanexploit. Weshowinsubse-
capabilities ultimately lead to global rather than
quentsectionsthatmodelswhichperformwellon
sentence-level understanding of text. The con-
SQuAD rely on lexical pattern matching, and are
struction of a large-scale dataset of this nature is
alsonotrobusttovarianceinlanguagestyle.
a challenging task. We take a small step in this
direction by focusing on referential inference.1
Metric SQuAD NewsQA ParallelQA
WikiHop (Welbl et al., 2017) is an interesting
Jaccard 79.28% 38.11% 27.45%
multi-hopinference-focuseddatasetcreatedusing
TF-IDF 81.32% 51.86% 31.37%
entity-relationpairsforqueriesspanningdifferent
BM25 74.26% 43.45% 27.45%
Wikipedia passages. While the focus of our pi-
lot study is similar to theirs, we believe that our
Table 1: Sentence Retrieval Performance using
method can easily be extended to other inference
Jaccard similarity (Jaccard, 1912), TF-IDF overlap
types. Also, identifying the correct span is more
(Sparck Jones, 1972) and BM-25 overlap (Robertson
etal.,1994)scoringmetrics challengingthanchoosingananswerfromalist.
Weaimtoincorporatemultiplelanguagestyles,
To alleviate the lack of topic diversity in makingithardforthesystemtomemorizelinguis-
SQuAD, NewsQA (Trischler et al., 2016) was tic patterns (Williams et al., 2017). We achieve
created from 12,744 news articles sampled from thisbyusingtwoparallelpassagesthattalkabout
CNN/Daily Mail. To ensure lexical diversity, one the same or related subject(s) but are obtained
set of crowd-workers generated questions using from different sources. This helps in formulating
only an abstractive summary, while the answer referentialinferencequestionsbecausethereexists
spansweremarkedinthefullarticlebyanotherset nosinglesentenceinthepassagewhichmatchesa
ofcrowd-workers. However,newsarticlestendto paraphrase of the question, and necessitates that
encourage questions that point to entities, and the inference (which goes beyond co-reference) be
dataset does not specifically focus on inference. performed across both passages. Evaluation is
Determining the exact answer span is harder, but easyandobjectivebecauseanswersarestillspans
thismaybeduetotheuseofonlynewshighlights within the passages. Questions can be answered
togeneratequestions;thismayinducenoiseinthe solely on the basis of the information provided in
answerspansmarkedinthenewsarticlessincethe theiraccompanyingpassages.
questionmightnotbeexactlyapt. For example, to answer Question 1 in Table 2,
To prevent annotation bias, SearchQA (Dunn the system will have to infer from passage 1 that
et al., 2017) starts with question-answer pairs PresidentKamazuBandabelongstotheMCPand
from Jeopardy! and adds documents retrieved was defeated in the elections. The equivalence of
by a search engine for each question as its con- thiseventandtheelectioninpassage2mustbees-
text. However, the questions are mostly factoid. tablished, while comprehending that the “favored
Kocˇisky` et al. (2017) found that 80% of answers challenger” Bakili Muluzi is the one Banda lost
are bigrams or unigrams, and 99% contain 5 or the elections to, and who belonged to the UDP,
fewer tokens, with many answers being named makingitthecorrectanswer.
entities. TriviaQA (Joshi et al., 2017) similarly Giventhattheinformationisspatiallyscattered
includes question-answer pairs authored by trivia acrossthetwopassages,thismethodwouldensure
enthusiastsalongwithindependently-gatheredev- thattheparallelpassageshavetobeunderstoodin
idence documents which provide distant supervi- combinationtoanswerthequestion.
sionforansweringthequestions.
1Referentialinferenceistheprocessofidentifyingthedis-
Thesedatasetshavefacilitatedthedevelopment
courseand/orreal-worldentityreferredtobyalinguisticex-
of new QA models, but we believe there are sev- pression(name,noun,pronoun,etc.).
2
HastingsKamuzuBandawastheleaderofMalawifrom1961to1994. In1963hewasformallyappointedprimeministerof
Nyasalandand,ayearlater,ledthecountrytoindependenceasMalawi.TwoyearslaterheproclaimedMalawiarepublicwith
himselfaspresident. HedeclaredMalawiaone-partystateundertheMalawiCongressParty(MCP)andbecamePresidentof
MCPaswellasPresidentforLifeofMalawiin1971.Areferendumendedhisone-partystateandaspecialassemblyendedhis
life-termpresidency,strippinghimofmostofhispowers. Bandaranforpresidentinthedemocraticelectionswhichfollowed
andwasdefeated.HediedinSouthAfricain1997.
MalawiansSaturdaywoundupanhistoricelectioncampaignbringingmultipartypoliticstoacountryruledforthepastthree
decadesbyPresidentHastingsKamuzuBanda.Theailingpresidentinspectedtroopsfromanopentruckassome20,000people
turnedupatastadiumheretocelebratehisofficialbirthdayaheadofelectionsonMay17. Readingapreparedspeechwith
somedifficulty, BandaappealedtoMalawianstoconductthemselves”asladiesandgentlemen”duringtheelections, which
shouldbe”freeandfair.”Meanwhile,thebiggeroppositionrallywasaddressedbythepresidentialchallengerfavoredtowin
theelections,BakiliMuluzioftheUnitedDemocraticFront(UDF).
Question1:WhoemergedvictoriousbetweentheMCPandUDF?
Question2:WhatdidtheMCPleaderaskofthepeopleofMalawionpollingday?
Question3:WhatbroughtmultipartypoliticstoMalawiafterthreedecades?
Table2: ExampleofaParallelPassage. Thequestionsandcorrespondinganswersarecolorcoded.
4 ProofofConcept Our small-scale experiment shows the feasibil-
ity of the approach, although collecting a larger
Forafairevaluationofexistingmodels,wesought dataset requires more effort in acquiring passages
tousedatadrawnfromasimilardomain,butwrit- andgeneratingquestionsfromdiversesources.
ten in a different style. We chose the CNN/Daily
5 AnalysisofExistingModels
Mail corpus and Wikipedia because they both fo-
cus on factoid statements, yet differ in language
style to a noticeable extent (e.g. in the use of id- SQuAD ParallelQA
Model
iomatic expressions). We picked 20 CNN/Daily EM F1 EM F1
Mailarticlesatrandomtoformoneofthepassages BiDAF 67.70 77.30 35.29 42.52
inourpair. Tofindanassociatedparallelpassage, DrQA 69.64 78.76 39.22 47.23
we selected the most frequently mentioned enti- R-Net 71.07 79.51 41.18 50.38
ties in each article and obtained its corresponding
Table3: PerformanceonSQuADvsParallelQA
Wikipedia pages. We fragmented these into pas-
sages with at most 500 words, and performed a
We consider three deep learning models: Bidi-
k-Nearest Neighbor search using tf-idf and topic
rectional Attention Flow (BiDAF) 2 (Seo et al.,
vectors(Bleietal.,2003)toformpairs. Wetuned
2016), Document Reader (DrQA) 3 (Chen et al.,
the number of entities per article used to retrieve
2017), and Gated Self-Matching Networks (R-
Wikipedia pages, as well as the sections consid-
Net) 4 (Wang et al., 2017) trained on SQuAD.
ered in each article. This process produced a to-
Wefeedtheconcatenatedparallelpassageandthe
tal of 15 News-Wiki passage pairs. While no two
question as inputs. On a total of 51 QA pairs, we
pairs have the same news article, they may be
observed exact match (EM) scores of about 40%
pairedwiththesameWikipassage.
and token overlap F1 scores of about 45% for all
We focused on referential inference for this pi- models, versus their performance on the SQuAD
lot, but the method can be extended to include dataset (EM of almost 70% and F1 of 80%). De-
questions based on other types of inference. 15 tailedresultsareshowninTable3.
humanannotatorsweregivenexplicitinstructions Althoughthemodelsweretrainedandtestedon
andreal-worldexamples toformquestion-answer differentdatasets,weexpectthemtoperformrea-
pairs using given parallel passages. We collected sonablywellonthenewtasksincethedatasources
50 valid question-answer pairs through this and domain are similar. Also, the size of our col-
∼
mechanism. The average length of the answers lected data is much smaller than the SQuAD de-
obtained was around 4 words. Basic sentence re- velopmentset,butwebelievethatthesamplesare
trieval statistics (similar to the ones discussed in fairly representative of data that can be generated
Section 2) are shown in Table 1, indicating that
2
https://allenai.github.io/bi-att-flow/
lexical similarity between the question and pas-
3
https://github.com/hitvoice/DrQA
sage sentences is insufficient to obtain an answer. 4
https://github.com/HKUST-KnowComp/R-Net
3
Passage Question
...TheUNisthelargest,mostfamiliar,mostinternationallyrepresentedandmostpower- WhowassenttoBosniaastheenvoy
fulintergovernmentalorganisationintheworld...UNenvoyYasushiAkashicalledameet- of most powerful intergovernmental
ingofallpartiestotalksonafour-monthceasefireforSaturdayafternoon,headded... organisationintheworld?
...Onarrival, thepresidentandhiswifeHillaryweretakentoUniversityCollege, one From which state was this US Pres-
of37Oxfordcolleges,wherehestudiedpoliticalscienceasaRhodesScholarbetween ident who was a Rhodes scholar be-
October1968andJune1970...ClintonwasbornandraisedinArkansasand... tween1968and1970?
...withdrewfromaUN-designatedthree-kilometer(two-mile)exclusionzonearoundthe WhereinBosniadidthesuccessorof
easternBosnianenclaveofGorazde...TheUnitedNations(UN)isanintergovernmental the League of Nations designate an
organization... AreplacementfortheineffectiveLeagueofNations,theorganization... exclusionzone?
[easternBosnianenclave,Gorazde,easternBosnianenclaveofGorazde]
ToddMartinsqueezedtoa7-67-6victoryoverfellow-AmericanPeteSamprasinthe Pistol Pete lost to whom in the
finaloftheQueen’sClubtournamenthereonSunday. Thewinfurtherbolsteredfifth Queen’sclubtournament?
seededMartin’sreputationasoneofthemostdangerousgrasscourtplayers...
...(RENAMO)rebelsataUN-supervisedassemblypointbrutallybeatoneoftheirsenior WherewasRaulDiquebeatenupby
officialsduringamutinyoverseverancepayonJune1atMocubela,about100kilometers rebelsofRENAMO?
(62 miles) east of Mocuba. But RENAMO has denied the official, identified as Raul
Dique,wasbeatenupbymutineers,theMozambicannewsagency(AIM)saidinareport
monitoredinHarareThursday...
Table4:ExamplesoferrortrendsonParallelQA:blue-goldanswer,red-spanpredictedincorrectlybyallmodels,
orange-BiDAFandR-Netpredictionoverlap,olive-BiDAF,magenta-DrQA,cyan-R-Net
usingourproposedmechanism. Thus,thelowEM Entity Type Confusion: Despite having a
•
and F1 scores support our hypothesis that these variety of entities as answers to questions in
datasets do not adequately assess the capabilities the training data, sometimes the model an-
of these models, which overfit to lexical patterns swers do not correspond to the correct entity
ratherthangeneralizing. type(example5).
Wenowdiscussafewcommonerrorsobserved
upon manual inspection of the results. Examples
6 Discussion&Conclusion
for each are provided in Table 4. The distribution
ofpredictionsacrosstheseerrorcategoriescanbe
While our approach is promising, we observed a
foundinFigure1,SectionA.
few problems during the pilot study. Longer pas-
sages and constraints on the question formulation
High Lexical Overlap - Incorrect Sen-
• require more time and skill in the annotation pro-
tence: Themodelstendtopickanswerspans
cess. Thiscanleadtocrowd-workersformulating
fromsentenceswhichhavehighlexicalover-
a single referring expression and then using it in
lap with the question. We observe that this
different contexts to form questions, reducing di-
accountsforthelargestchunkoferrorsacross
versity. For some questions, although inference
allmodels(example2). Ourobservationsare
is needed, both passages may not be necessary to
consistent with the findings of Jia and Liang
answer them. Since we used news articles and
(2017). The models often simply resolve the
Wikipedia passages in our pilot study, 58.82% of
referential expression in the question to its
answers were named entities. We plan to extend
correspondingentity. Inexample1,themod-
this mechanism to other inference types and con-
els resolve “organisation” in the question to
ductalargerpilotbeforescalingupthecollection.
“TheUN”duetohighlexicalsimilarity.
Our experiments demonstrate that the Paral-
Incorrect Answer Boundaries: This is the
• lelQA task can be more challenging than some
secondmostfrequentlyobservederror,where
prior QA tasks. Our analysis shows that many
theanswersgeneratedarealmostcorrect,but
popular RC datasets seem to test the ability of
models face issues in appropriately defining
models to pick up superficial cues. ParallelQA
answer boundaries (example 3). R-Net and
is our proposed step towards inference-oriented
DrQA, on average, produce shorter answers.
reading comprehension. We use parallel pas-
BiDAFtendstoproducelongeranswers.
sages from different sources for generating rea-
Missing Logical Inference: Models are soningquestionswhichencouragesystemstogain
•
sometimes unable to make certain logical a deeper understanding of language, and become
conclusions like A’s victory over B implies robusttovariationsinstyleandtopic. Weinclude
thatBlosttoA(example4). examplesfromourinitialpilotstudyinTable6.
4
Acknowledgments Stephen E Robertson, Steve Walker, Susan Jones,
Micheline Hancock-Beaulieu, and Mike Gatford.
The authors would like to thank Chaitanya 1994. Okapi at trec-3 proceedings of the third text
Malaviya, Sandeep Subramanian, Siddharth retrievalconference. TREC.
Dalmia, TejasNamaandVaishnaviAKforuseful
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
discussions. We would also like to express our Hannaneh Hajishirzi. 2016. Bidirectional attention
gratitudetotheannotatorswhoparticipatedinthe flow for machine comprehension. arXiv preprint
arXiv:1611.01603.
pilotstudy.
Karen Sparck Jones. 1972. A statistical interpretation
of term specificity and its application in retrieval.
References Journalofdocumentation28(1):11–21.
David M Blei, Andrew Y Ng, and Michael I Jordan. Donald Spearritt. 1972. Identification of sub-skills of
2003. Latent dirichlet allocation. Journal of ma- readingcomprehensionbymaximumlikelihoodfac-
chineLearningresearch3(Jan):993–1022. tor analysis1. ETS Research Bulletin Series pages
i–24.
DanqiChen, AdamFisch, JasonWeston, andAntoine
Bordes. 2017. Reading wikipedia to answer open- MichaelStrange.1980. Instructionalimplicationsofa
domainquestions. arXivpreprintarXiv:1704.00051 conceptual theory of reading comprehension. The
. ReadingTeacher33:391–97.
Matthew Dunn, Levent Sagun, Mike Higgins, Ugur Sandeep Subramanian, Tong Wang, Xingdi Yuan, and
Guney, Volkan Cirik, and Kyunghyun Cho. 2017. AdamTrischler.2017. Neuralmodelsforkeyphrase
Searchqa: A new q&a dataset augmented with detection and question generation. arXiv preprint
context from a search engine. arXiv preprint arXiv:1706.04560.
arXiv:1704.05179.
AdamTrischler,TongWang,XingdiYuan,JustinHar-
Karl Moritz Hermann, Tomas Kocisky, Edward ris, Alessandro Sordoni, Philip Bachman, and Ka-
Grefenstette,LasseEspeholt,WillKay,MustafaSu- heer Suleman. 2016. Newsqa: A machine compre-
leyman, and Phil Blunsom. 2015. Teaching ma- hensiondataset. arXivpreprintarXiv:1611.09830.
chinestoreadandcomprehend. InAdvancesinNeu-
Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
ral Information Processing Systems. pages 1693–
and Ming Zhou. 2017. Gated self-matching net-
1701.
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
PaulJaccard.1912. Thedistributionoftheflorainthe
ingoftheAssociationforComputationalLinguistics
alpinezone. Newphytologist11(2):37–50.
(Volume1:LongPapers).volume1,pages189–198.
Robin Jia and Percy Liang. 2017. Adversarial exam-
Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
plesforevaluatingreadingcomprehensionsystems.
2017. Fastqa: A simple and efficient neural ar-
arXivpreprintarXiv:1707.07328.
chitecture for question answering. arXiv preprint
arXiv:1703.04816.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer.2017. Triviaqa: Alargescaledistantly
Johannes Welbl, Pontus Stenetorp, and Sebastian
supervisedchallengedatasetforreadingcomprehen-
Riedel. 2017. Constructing datasets for multi-hop
sion. arXivpreprintarXiv:1705.03551.
reading comprehension across documents. arXiv
preprintarXiv:1710.06481.
Toma´sˇ Kocˇisky`, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, Ga´bor Melis,
JasonWeston,AntoineBordes,SumitChopra,Alexan-
and Edward Grefenstette. 2017. The narrativeqa
derMRush,BartvanMerrie¨nboer,ArmandJoulin,
reading comprehension challenge. arXiv preprint
and Tomas Mikolov. 2015. Towards ai-complete
arXiv:1712.07040.
questionanswering: Asetofprerequisitetoytasks.
arXivpreprintarXiv:1502.05698.
PranavRajpurkar,JianZhang,KonstantinLopyrev,and
Percy Liang. 2016. Squad: 100,000+ questions Adina Williams, Nikita Nangia, and Samuel R Bow-
for machine comprehension of text. arXiv preprint man.2017. Abroad-coveragechallengecorpusfor
arXiv:1606.05250. sentence understanding through inference. arXiv
preprintarXiv:1704.05426.
MatthewRichardson,ChristopherJCBurges,andErin
Renshaw. 2013. Mctest: A challenge dataset for Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
the open-domain machine comprehension of text. William W Cohen. 2017. Semi-supervised qa with
In Proceedings of the 2013 Conference on Empiri- generative domain-adaptive nets. arXiv preprint
calMethodsinNaturalLanguageProcessing.pages arXiv:1702.02206.
193–203.
5
A SupplementalMaterial
InferenceType Meaning Examples InformationRequired
Referential Coreferences,ReferringExpressions BillClinton’swifeisHillaryClinton Alinkbetweentheexpressionandentityitrefersto
Figurative Metaphors Alltheworld’sastage Adictionaryofcommonmetaphorsandwhattheymean
Part-Whole Inclusion Adogisananimal Anontologyofhierarchicalandotherrelationshipsbetweenwords
Numeric Units,Operations 60secondsisaminute Equivalence(andconversion)ofunits,BasicOperationSkills
Lexical MeaningsfromLinguisticContext Iateanapple(apple=fruitorcompany?) ContextualInformation:WordEmbeddings/NER/PoS
Denotation LiteralMeaningsofExpressions Olivebranchdenotespeace WorldKnowledge+ContextualInformation
Spatial ReasoningaboutSpace BerlinisinGermanywhichisinEurope WorldKnowledge+BasicSpatialReasoningRules
Temporal ReasoningaboutTime WorldWarIIhappenedbeforeColdWar WorldKnowledge+BasicTemporalReasoningRules
Table5: DifferentTypesofInferencealongwithexamplesandpossibleinformationrequiredtoperformthem
Figure1: DistributionoferrorsbyBiDAF,DrQAandR-Netacrossdifferentcategoriesusingmanualinspection
6
ToddMartinsqueezedtoa7-67-6victoryoverfellow-AmericanPeteSamprasinthefinaloftheQueen’sClubtournament
hereonSunday. ThewinfurtherbolsteredfifthseededMartin’sreputationasoneofthemostdangerousgrasscourtplayers.
But it was essentially a baseline slogging match which provided little to whet the appetite for Wimbledon. There were no
breaksofserveineithersetandonlythreebreakpointsintheentirematch-twoagainstSamprasinthesecondgameandone
againstMartininthenext. Martinclinchedthefirsttie-breakcourtesyofadoublefaultfromSamprastolead4-2andthena
gloriouscross-courtforehandreturnonhissecondsetpointtotaketheshoot-out7-4.Hetookthesecondtie-breakbythesame
score,SamprassavingthreematchpointsbeforeafiercesmashclinchedMartin’sthirdcareertitleandhisfirstvictoryoverhis
compatriotinfourmeetings.
Petros”Pete”Sampras(bornAugust12, 1971)isaretiredAmericantennisplayerwidelyregardedasoneofthegreatestin
the history of the sport. He was a longtime world No. 1 with a precise serve that earned him the nickname ”Pistol Pete”.
Hiscareerbeganin1988andendedatthe2002USOpen,whichhewon,defeatingrivalAndreAgassiinthefinal. Sampras
wasthefirstmantowin14GrandSlamsinglestitles(sevenWimbledon,fiveUSOpen,twoAustralianOpen). Healsowon
sevenyear-endchampionshipsandfinishedsixconsecutiveseasonsatoptherankings. Summaryofprofessionalawards. U.S.
OlympicCommittee”SportsmanoftheYear”in1997. Hewasthefirsttennisplayertoreceivethisaward. GQMagazine’s
IndividualAthleteAwardforManoftheYearin2000. SelectedtheNo. 1player(of25players)inthepast25yearsbya
panelof100currentandpastplayers,journalists,andtournamentdirectorstocommemoratethe25thanniversaryoftheATP
in1997. Voted48thathleteofTop50GreatestNorthAmericanAthletesofESPN’sSportsCentury(alsoyoungestonlist). In
2005,TENNISMagazinenamedSamprasthegreatesttennisplayerfortheperiod1965through2005,fromitslist,”The40
GreatestPlayersoftheTENNISEra”.
Question1:Thefirstmantowin14GrandSlamsinglestitleslosttowhomintheQueen’sclubtournament?
Question2:Thegreatesttennisplayerfortheperiod1965through2005losttoToddMartininthefinalsofwhichtournament?
Question3:WhatwasthetallyofTodd’scareertitlesafterdefeatingtheGQMagazine’sManoftheYearawardwinner,inthe
finalofQueen’sclubtournament?
Cambodianco-premiersPrinceNorodomRanariddhandHunSensaidWednesdaytheyhadagreedtoholdingpeacetalkswith
theKhmerRougeinPyongyangwithoutpreconditions,inresponsetoanappealbyKingNorodomSihanouk.Theco-premiers
hadsentanofficiallettertotheking”sayingthatwearereadytogotoPyongyangwithoutceasefire,withoutpreconditions,”
PrinceRanariddhtoldjournalists. ”Lettalksbegin,”headded. HunSensaidthetalks,beginningonMay27,wouldbebased
onapeaceplanputforwardbyKingSihanouk,butaddedthatthegovernmenthadyettoreceiveareplyfromtheKhmerRouge
regardingtheproposal. KingSihanoukhasproposedthatcertain”acceptable”membersoftheKhmerRougebegivensenior
cabinetpostsinthegovernmentinexchangeforgivinguptheirzones,ceasingallguerrillaactivitiesandmergingtheirfighters
withtheroyalarmedforces.
HunSenisthePrimeMinisterofCambodia,PresidentoftheCambodianPeople’sParty(CPP),andMemberofParliament(MP)
forKandal. HehasservedasPrimeMinistersince1985,makinghimthelongestservingheadofgovernmentofCambodia,
and one of the longest serving leaders in the world. From 1979 to 1986 and again from 1987 to 1990, Hun Sen served as
Cambodia’sforeignminister.HisfullhonorarytitleisSamdechAkeakMohaSenaPadeyTechoHunSen.BornHunBunal,he
changedhisnametoHunSenin1972twoyearsafterjoiningtheKhmerRouge. HunSenrosetothepremiershipinJanuary
1985whentheone-partyNationalAssemblyappointedhimtosucceedChanSywhohaddiedinofficeinDecember1984.He
heldthepositionuntilthe1993UN-backedelections,whichresultedinahungparliament.Aftercontentiousnegotiationswith
theFUNCINPEC,HunSenwasacceptedasSecondPrimeMinister,servingalongsideNorodomRanariddhuntila1997coup
whichtoppledthelatter.UngHuotwasthenselectedtosucceedRanariddh.
Question1:AccordingtoHunBunal,whatisthebasisoftalksonMay27th?
Question2:UntilwhichyeardidtheCambodianco-premiersholdoffice?
Question3:ThePresidentoftheCambodianPeople’sPartywasholdingpeacetalkswiththeKhmerRougealongwithwhom?
Table6: Examplesofcollectedparallelpassages. Thequestionsandcorrespondinganswersarecolorcoded.
7

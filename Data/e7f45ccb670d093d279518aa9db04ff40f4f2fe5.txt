BMC Bioinformatics
BioMed Central
Research
Open Access
Active machine learning for transmembrane helix prediction
Hatice U Osmanbeyoglu1, Jessica A Wehner2, Jaime G Carbonell3
and Madhavi K Ganapathiraju*1,4
Addresses:1DepartmentofBiomedicalInformatics,UniversityofPittsburghSchoolofMedicine,Pittsburgh,PA,USA,2Departmentof
Mathematics,UniversityofNorthCarolina,ChapelHill,NC,USA,3LanguageTechnologiesInstitute,CarnegieMellonUniversity,Pittsburgh,
PA,USAand4IntelligentSystemsProgram,UniversityofPittsburghSchoolofArtandSciences,Pittsburgh,PA,USA
E-mail:HaticeUOsmanbeyoglu-huo1+bmc@pitt.edu;JessicaAWehner-wehner@email.unc.edu;JaimeGCarbonell-jgc+@cs.cmu.edu;
MadhaviKGanapathiraju*-madhavi+bmc@pitt.edu
*Correspondingauthor
fromTheEighthAsiaPacificBioinformaticsConference(APBC2010)
Bangalore,India18-21January2010
Published:18January2010
BMCBioinformatics2010,11(Suppl1):S58 doi:10.1186/1471-2105-11-S1-S58
Thisarticleisavailablefrom:http://www.biomedcentral.com/1471-2105/11/S1/S58
©2010Osmanbeyogluetal;licenseeBioMedCentralLtd.
Thisisanopenaccessarticledistributedundertheterms oftheCreativeCommons Attribution License(http://creativecommons.org/licenses/by/2.0),
whichpermitsunrestricteduse,distribution,andreproductioninanymedium,providedtheoriginalworkisproperlycited.
Abstract
Background: About 30% of genes code for membrane proteins, which are involved in a wide
variety of crucial biological functions. Despite their importance, experimentally determined
structurescorrespondtoonlyabout1.7%ofproteinstructuresdepositedintheProteinDataBank
duetothedifficultyincrystallizingmembraneproteins.Algorithmsthatcanidentifyproteinswhose
high-resolutionstructurecanaidinpredictingthestructureofmanypreviouslyunresolvedproteins
are therefore of potentially high value. Active machine learning is a supervised machine learning
approach which is suitable for this domain where there are a large number of sequences but only
very few have known corresponding structures. In essence, active learning seeks to identify
proteins whose structure, if revealed experimentally, is maximally predictive of others.
Results: Anactivelearningapproachispresentedforselectionofaminimalsetofproteinswhose
structurescanaidinthedeterminationoftransmembranehelicesfortheremainingproteins.TMpro,
analgorithmforhighaccuracyTMhelixpredictionwepreviouslydeveloped,iscoupledwithactive
learning.Weshowthatwithawell-designedselectionprocedure,highaccuracycanbeachievedwith
onlyfewproteins.TMpro,trainedwithasingleproteinachievedanF-scoreof94%onbenchmark
evaluationand91%onMPtopodataset,whichcorrespondtothestate-of-the-artaccuraciesonTM
helixpredictionthatareachievedusuallybytrainingwithover100trainingproteins.
Conclusion: Active learning is suitable for bioinformatics applications, where manually
characterized data are not a comprehensive representation of all possible data, and in fact can
be a very sparse subset thereof. It aids in selection of data instances which when characterized
experimentallycanimprovetheaccuracyofcomputationalcharacterizationofremainingrawdata.
Theresultspresentedherealso demonstratethatthefeatureextractionmethodofTMproiswell
designed, achieving a very good separation between TM and non TM segments.
Page1of9
(pagenumbernotforcitationpurposes)
BMCBioinformatics2010,11(Suppl1):S58 http://www.biomedcentral.com/1471-2105/11/S1/S58
Background Clusteringis commonlyapplied toselecttherepresenta-
Membrane proteins mediate a broad range of funda- tive data points [18-22].
mental biological processes such as cell signaling,
transport of molecules that cannot otherwise cross The scarcity of labelled data for TM helix prediction
impermeable cell membranes, cell-cell communication, makesitanexcellentcandidateforactivelearning.Inthe
cell recognition and cell adhesion [1]. About 8000 (or case of transmembrane helix prediction, unlabeled data
30%) of human genes encode for membrane proteins, refers to sequences of all the membrane proteins and
the sequences of which are relatively easy to obtain. In labelingreferstodeterminationofstructuralannotations
contrast, of the 60,369 protein structures deposited to- by experimental means. Technological improvements
dateintheProteinDataBank(PDB),only1062or1.7% overthelastdecadeleadtoarapidincreaseinbiological
correspond to membrane proteins [2]. Production of data including gene sequences from several organisms.
membrane protein crystals which is required for deter- One of the major challenges of bioinformatics relates to
mining highly accurate detail of the 3D structure is very this flood of raw data. Moreover, in some cases such as
difficult or sometimes impossible due to the inherently transmembrane (TM) helix prediction, manual annota-
hydrophobic nature of membrane proteins [3,4]. NMR tion is very difficult or sometimes impossible [4].
methods do not apply readily to very large molecules
such as transmembrane proteins. Knowledge of the EarlyTMhelixpredictionmethodsusetwofundamental
transmembrane (TM) segment locations in a membrane characteristics: (i) the length of the TM helix being at
protein can narrow down possible tertiary structure least20residuessothatitislongenoughtocrossthe30
conformations for the given protein [5-8] and aid in Åthicklipid bilayer[23],and(ii)theTMresidues being
prediction of its function. Therefore prediction of the hydrophobic for reasons of thermodynamic stability in
structure by computational methods is useful. the hydrophobic membrane environment [7]. Explicit
methods employ a numerical scale to code for the
Labeling data (or characterizing) by experimental or hydrophobic property of amino acids followed by
other manual methods is often time consuming and computing average-hydrophobicity in moving windows,
expensive. To characterize data by computational meth- to locate long hydrophobic segments [24,25]. Other
ods, supervised machine learning approaches require a methods treat the 20 amino acids as distinct entities,
training set which is a representative sample of all the without explicit representation of their similarities, and
unlabeled data in order to achieve comparable perfor- statistically model their distribution in different topolo-
mance on the latter. In practice, data selection for gical locations of TM proteins [26,27]. These methods
experimental or manual characterization is rarely ever generally use statistical modelling and assume that
carried out by taking into account the complete space membraneproteinsconformtothecommonlyobserved
spanned by the unlabeled data. It is useful and efficient topology of cytoplasmic-TM-extracellular. Drawbacks
to design algorithms that can not only learn from with these methods have been low accuracy with the
existing training data but can also direct the optimal windowing methods and over-fitting by statistical
selection of new data instances for manual labeling. methods due to the lack of sufficient training data.
Active learning, a type of supervised learning, samples Recently, Ganapathiraju et al. developed TMpro, a
the unlabeled pool of data and selects instances whose computationalmethodwitharadicallydifferentmethod
labels would prove to be most informative additions to forfeaturecomputationthatattainsagoodseparationof
the training set. Each time new labelled instances are the TM versus non-TM “windows” and thereby, a high
added to the training set, the classification function is accuracy in TM helix prediction [28].
updated. As a consequence of this, the information
valuable to the learning function is maximized. Here, TMpro is employed for transmembrane helix
prediction, in conjunction with active learning algo-
Commonstrategiesemployedfordataselectioninactive rithms to select a minimal set of proteins to train the
learning [9] are density based, where a set of data points TMpro statistical models (henceforth called TMpro-
from dense regions are selected for labelling [10,11]; or active).
uncertainty based, where data points with maximum
confusion or uncertainty with current classifier are
selected [12,13]; or representative based, in which data Methods
points most representative of the data set are selected Datasets
[14]; or ensemble based in which multiple criteria are PriortoTMpro,TMHMMhasbeenthemostwidely-used
employed [15-17]. Many of the active learning transmembrane helix prediction algorithm. A set of 160
approaches combine density-based and uncertainty proteins was used to train TMHMM [29]. In order to
based strategies to achieve better performance. compare the performance of TMpro with TMHMM, in
Page2of9
(pagenumbernotforcitationpurposes)
BMCBioinformatics2010,11(Suppl1):S58 http://www.biomedcentral.com/1471-2105/11/S1/S58
[28] the same training dataset of 160 proteins has been (5) Active learning algorithm is applied to determine the
used to train TMpro. Keeping the training dataset the training set iteratively.
same, evaluations were presented on three datasets:
benchmarkdata,PDB_TM and MPtopo. Todemonstrate (6) A neural network (NN) is trained with the selected
themeritsofactivelearningincomparisontonon-active training set, and the output of the neural network is
learningbasedmethods,evaluationsofTMpro-activeare interpreted as done for TMpro.
presented in comparison to TMpro without active
learning.Notethat,whenallthedataintraining,namely Feature computation
all of the 160 proteins are selected, TMpro-active is the This process is same as carried for TMpro, but is
same as TMpro. presented here for completeness of information.
The following three datasets ofmembrane proteins with Data preprocessing and vector space representation
high resolution structural annotations are used for The primary sequence of each protein is decomposed
evaluation: (i) high resolution set from results reported into five different sequences by replacing each amino
by the benchmark evaluation by Chen et al [30], (ii) acid with its property according to charge, polarity,
membrane proteins with high resolution information aromaticity, size and electronic property [28]. The
from the MPtopo dataset consisting of 101proteins and proteinsequence,oflengthL,isanalyzedwithamoving
443 TM segments in [31], (iii) PDBTM dataset down- window of length l; the window is moved along the
loaded in April 2006 (in order to compare the results sequence one residue at a time, each position of the
with what have been published before [28], an older window yielding a feature vector. The feature vector at
dataset is used); it contains all transmembrane proteins positioni,representedbyR,isderivedfromthewindow
i
with3DstructuresfromthePDBdeterminedtothatdate beginningattheithresidueandextendinglresiduestoits
[2]. PDBTM provides a non-redundant subset of alpha- right. It is given as
helical TM proteins having sequence identity less than
40%. Chains corresponding to this non-redundant list R i =[C ij] 1×16 (1)
were extracted from the complete set, resulting in 191
proteins consisting of 789 TM segments [2]. 12 out of where C is the count of property-value j in window i.
ij
191 proteins of PDBTM and 16 out of 101 proteins of The specific property-values counted by the C are as
ij
MPtopo are redundant with the training set. A 2-class follows:
labelingschemehasbeenemployedforallthreedatasets
where each residue is marked as “non-TM” (inside and C : count of “charge-positive”
i1
outside regions of the membrane) or “TM”.
C : count of “charge-negative”
i2
Approach C : count of “charge-neutral”
i3
The proposed approach, TMpro-active, explores the
feature space to identify the data points, and thereby C : count of “polarity-polar”
i4
theproteins,whicharemost-representativeofthefeature
space. Proteins thus selected are used to train the neural C : count of “polarity-nonpolar”
i5
network of the original TMpro algorithm. The steps
involved in TMpro-active are as follows (steps in italics C : count of “aromaticity-aromatic”
i6
are new compared to TMpro):
C : count of “aromaticity-aliphatic”
i7
(1) To begin with, all proteins are considered to be unlabeled
proteins. C : count of “aromaticity-neither”
i8
(2) Primary sequence of each protein is expanded into C : count of “electronic property-strong acceptor”
i9
five different primary sequences, each coding for one of
polarity,charge,aromaticity,sizeandelectronicproperty. C : count of “electronic property-strong donor”
i10
(3) Features are computed over moving windows of C : count of “electronic property-acceptor”
i11
length 16 as done for TMpro.
C : count of “electronic property-donor”
i12
(4) A Self-Organizing-Map is constructed over the feature
space spanned by the proteins. C : count of “electronic property-neutral”
i13
Page3of9
(pagenumbernotforcitationpurposes)
BMCBioinformatics2010,11(Suppl1):S58 http://www.biomedcentral.com/1471-2105/11/S1/S58
C : count of “size-medium” Each neuron receives the input signal vectors weighted
i14
by a vector indicating the degree of closeness of
C : count of “size-small” peripheral neurons. For each input vector, the node
i15
closest to it is determined via similarity between the
C : count of “size-big” input vector and the weights of each node. In the
i16
training process, the weights of the winning neuron and
When feature vectorsR are computed for every position its adjacent neurons are updated in terms of distance of
i
ofthewindow,movingtotherightoneresidueatatime, each of these neurons from the input vector.
the entire protein will have a matrix representation p
(Equation 2), AnSOMhasbeencreatedwith50nodesarrangedin5×
10 hexagonal lattice grid [33], and is trained on 1000
p=[R 1TR 2T…R LT −l+1] 16×L−l+1 (2) randomdatapoints(seeFigure1).Thehexagonallattice
emphasizes the diagonal directions in addition to
whose columns are the transpose of feature vectors R horizontal and vertical directions (see [32] for details).
i
(Equation 1). Euclidian distance is used to calculate distance between
nodes to its neighbours. Weights of nodes are used as
Singular value decomposition clustercentroids.TheSOMisthensimulatedfortheentire
Amino acid properties for feature representation (C to set features - which amounts to clustering of the data
i,1
C )aremutuallydependent.Itisthereforedesirableto around these 50 nodes.
i,16
transform thesefeaturevectors into an orthogonal space
priortotheuseofthisdataforclusteringandfeaturesfor
prediction.Toachievethis,proteinfeaturematricesofall Active learning
the proteins are concatenated to form a large matrix A, As discussed earlier, active learning is an approach to
minimize the number of labelled proteins that form the
and subjected to singular value decomposition (SVD)
trainingset.Thealgorithmcanactivelychooseaminimal
A =USVT (3) training set. To explore design choices that affect this
desirable behaviour, four different experiments were
where U and V are the right and left singular matrices
andSisadiagonalmatrixwhoseelementsarethesquare
rootsoftheeigenvaluesofthematrixAAT.Onlythetop
4 dimensions have been used for feature presentation,
since the top 4 dimensions of S of training data have
been found to carry 85% of the energy (variance) [28].
The matrices U, S and V are dependent on the matrix A
from which they are computed.
Therefore, for each new protein, singular value decom-
position should ideally be recomputed. However, this
would also involve recomputation of all the statistical
models built on the features derived through singular
value decomposition. To avoid this, the feature vectors
along the same principal components can be approxi-
mated by multiplication R with UT similarly as given in
i
Equation 3 [28].
Data clustering
A Self-Organizing Map (SOM) is computed as a way of
clustering theunlabeled data. Itarrangesa grid ofnodes
topologically based on the values of the features of the
training data, such that adjacent nodes are more similar
to each other. The SOM grid can efficiently be used to
Figure 1
visualize and investigate properties of the data [32]. In
The coverage of SOM network over the data. Figure
basic SOM, the neurons (nodes) are located on a low represents the coverage of the SOM network. 1000 data
dimensional grid, usually one- or two-dimensional. The points are just shown for more clear representation.
basic SOM construction follows an iterative procedure.
Page4of9
(pagenumbernotforcitationpurposes)
BMCBioinformatics2010,11(Suppl1):S58 http://www.biomedcentral.com/1471-2105/11/S1/S58
designedforchoosingthetrainingset.Wehaveapoolof (cid:129) Completely-nonmembrane (Class label = -1): If all
160 proteins that are considered to be unlabeled by the residues in the window are labeled non-TM
algorithm, from which it can select training proteins. (cid:129) Mixed (Class label = 0): If some residues in the
window are labeled TM and some non-TM
Random selection
This method is considered the baseline with which to The number of input nodes of the NN is 4 and the
compare active learning techniques. One protein is number of output neurons is 1. One hidden layer of 4
selected randomly per iteration and added to the nodes is placed in between input and output layers. The
training set. model is fully connected in the forward direction. Each
of the hidden and output neurons is a tansig classifier
[33]. Back-propagation procedure [33] is used to train
Selection by node-coverage
the network by presenting it with feature vectors and
Coverage of the feature space spanned by a protein is
their corresponding target output class labels. Mixed
indicatedbythenodesoftheSOMtowhichthefeatures
label feature vectors are not presented for training, since
areassigned.Allproteinsinthedatasetarerankedbythe
theyarisefrombothTMandnon-TMresiduesandhence
number ofnodes they each cover; i.e., by the number of
are expected to lie in the “confusable” region in the
nodes into which their features fall. The protein that
features space. The output neuron learns to fire -1 for
covers the largest number of the nodes of the SOM is
non-TM features and +1 for TM features. A threshold of
selected and added to the training set.
0.4 is chosen for automatic classification of the feature
intoitsclass.Eachinputfeaturevectorcausestheoutput
Selection by maximal entropy (confusion-rated)
neuronto firean analogvaluerangingfrom-1(non-TM
Initially, 1 protein is selected randomly and is added to
class)to+1(TMclass).Athresholdof0.4isusedtolabel
the training set. Subsequent to that, in each iteration,
the residue at the first position in the window to be TM
proteins whose feature vectors fall in the nodes with
ornon-TM.Sincethefeatureisderivedoverawindowof
maximumconfusionbetweenTMandnon-TMlabels,or length 16, and threshold of 0.4 is “more confident”
rather nodes with higher confusion rate as determined
towards the TM label, the 8 residues starting from the
by their entropy, are selected and added to the training
first position of the window are all set to be of TM type
set. The confusion (entropy) in labelling the data points
(these numbers are heuristically chosen during cross
can be measured as
validation). The process is repeated for the next feature
∑ vector, and so on, and a TM label is assigned to 8
− p ×log(p )
i i (4) residues at a time every time the output neuron fires a
i=(0,1) value greater than the threshold. Evaluation is carried
out with the same metrics as designed by Chen et al in
where p is the fraction of data points labelled TM and
0 [30]. The same metrics were also used in comparing
p is the fraction of data points labelled non-TM. By
1 TMpro with TMHMM in [28].
choosing proteins from highest confusion nodes, we
assumethatthemostinformativeproteinsarethosethat
the classifier is most uncertain about. Implementation
Singular value decomposition of the protein feature
matrix is computed using the SVDS tool in MATLAB®.
Selection by both node-coverage and confusion-rate
The SOM Toolbox of MATLAB® is used to create SOMs.
Proteinswhoselabelsareaskedareselectedaccordingto
Training and classification procedures for neural net-
node coverage and confusion rate in an alternating
works areimplemented using the Neural Net toolbox of
manner in each iteration.
MATLAB®.
Neural networks for feature classification
In each iteration, the neural network classifier is Results and discussion
retrained with the updated set of labelled proteins, and Self organizing map
prediction performance on test data is evaluated. The SOMisimplementedtoclustertheunlabeleddata.Only
neural network is modelled as described before [22]. 1000 random data points are used while training the
Duringtraining,aclasslabelisdefinedforeachwindow SOM network. After 1000 random data points, increas-
based on the number of TM and non-TM residue labels ing the amount of unlabelled data did not improve
in the window: clusteringefficiency.Moreover,byusingasmallamount
of data points, SOM can be formed as quickly as
(cid:129) Completely-membrane (Class label = 1): If all possible. Figure 1 shows the resulting SOM among the
residues in the window are labeled TM spread of a random sample of unlabeled data. Not all
Page5of9
(pagenumbernotforcitationpurposes)
BMCBioinformatics2010,11(Suppl1):S58 http://www.biomedcentral.com/1471-2105/11/S1/S58
unlabeled data is shown so as to keep the SOM nodes AsdescribedinMethodssection,Randomselectsproteins
visible. to add to the training set based on random selection,
Node-coverage selects proteins that cover the largest
Benchmark analysis of transmembrane segment number of nodes, Confusion-rated selects proteins that
prediction in membrane proteins cover nodes with maximum confusion and Node-cover-
Predictions are uploaded to the transmembrane helix age & Confusion-rated alternately selects proteins based
(TMH) benchmark evaluation server [27]. Transmem- onnodecoverageandconfusion-rated.Ineachiteration,
brane helix (TMH) benchmark server is an excellent theneural networkis reinitialized and then trained with
resource to quantitatively compare new TM helix the updated labelled (training) data. This process is
prediction methods with previous methods which performed independently for each method and their
include both simple hydrophobicity scale methods to performance is evaluated on the test data. Neural
more advanced algorithms that use hidden Markov network initialization, training and evaluation are
models, neural nets, etc. The benchmark server uses the carried out ten times, and average performance over
following metrics for evaluations [30]: Q is the ten experiments is reported here. Table 1 shows the
ok
percentage of proteins whose membrane segments are average performance for each of the learning algorithms
all predicted correctly. Segment-recall (called Q htm on reported by the benchmark server (on dataset 1) after
obs
benchmark server) is the percentage of experimentally first, second, fifth and tenth round of training. Active
determined (or ‘observed’) segments that are predicted learning methods (Node-coverage, Confusion-rated and
correctly. Segment-precision (called Q htm on bench- Node-coverage & Confusion-rated) outperformed Ran-
pred
markserver)isthepercentageofpredictedsegmentsthat domselection(apassivelearningalgorithm).Theresults
are correct. The residue accuracy Q refers to the for the active learning method that performed best,
2
percentage of residues that are predicted correctly. We Node-coverage, reached 94% segment level F-score even
also computed the F-score, which is the geometric mean for a single protein with balanced performance between
of segment level recall and precision. Since recall and segment-recall (97%) and segment-precision (92%).
precisioncaneachbeincreasedarbitrarilyattheexpense Node-coverage & Confusion-rated performed similarly
oftheother,thetwometricswhenviewedindependently to Node-coverage after one protein, likely because this
do not reflect the strength of the algorithm. Hence, the alternatingmethodusedNode-coverageselectiononthe
geometric mean of the two, (effectively the point where firstroundratherthanConfusion-rated.Comparethisto
thetwomeasuresareexpectedtobeequal)isusedasthe Confusion-rated alone, which reached 91% segment
metric. level F-score only after a second iteration, and Random
Table1:ComparisonofTMproNN:applyingactivevs.passivelearningalgorithmsforupdatingtrainingsetfrombenchmarkanalysis
Methods #ofProteinsinTraining-Set Qok Qhtm Qhtm Qhtm Q2
Fscore %obs %prd
1 Random 1 14 27 29 25 55
2 36 63 67 60 65
5 51 82 84 80 70
10 54 91 95 88 73
2 Node-Coverage 1 61 94 97 92 75
2 61 94 97 91 75
5 63 94 97 92 75
10 61 94 97 92 75
3 Confusion-Rated 1 14 27 29 25 55
2 52 91 95 87 73
5 55 91 95 88 73
10 59 93 96 89 74
4 Node-Coverage&Confusion-Rated 1 61 94 97 92 75
2 59 92 96 88 73
5 58 92 96 89 73
10 61 94 96 91 74
ItcanbeseenthatTMproachieveshighsegmentaccuracy(F-score)eveniftheclassifieristrainedwithjustoneproteinthatisfoundbyActive
Learningalgorithms.Thecolumnsfromlefttorightshow:methodbeingevaluated;Numberofproteinsintraining-set;Proteinlevelaccuracies:Qok,
whichisthepercentageofproteinsinwhichallexperimentallydeterminedsegmentsarepredictedcorrectly,andnoextrasegmentsarepredicted;
thatis,thereisaonetoonematchbetweenpredictedandexperimentallydeterminedsegments;SegmentF-scorewhichisthegeometricmeanof
RecallandPrecision;Recall(Qhtm,%obs,percentageofexperimentallydeterminedsegmentsthatarepredictedcorrectly);andPrecision(Qhtm,%
predpercentageofpredictedsegmentsthatarecorrect).Q2istheresiduelevelaccuracywhenallresiduesinaproteinareconsideredtogether,and
theQ2valuefortheentiresetofproteinsistheaverageofthatofindividualproteins.See[30]forfurtherdetailsonthesemetrics.
Page6of9
(pagenumbernotforcitationpurposes)
BMCBioinformatics2010,11(Suppl1):S58 http://www.biomedcentral.com/1471-2105/11/S1/S58
whichonlyachieved63%F-scoreforcomparablesizeof
trainingset.Activelearningsignificantlyreducedthecost
(number of proteins labeled out of 160 proteins), while
reaching high prediction accuracy, from over 100
training proteins for other state of the art TM helix
prediction algorithms to only one training protein.
Moreover, by using the smallest number of examples, a
classifier can be trained as quickly as possible.
TMpro-active shows similar performance over TMpro.
However, lower Q is observed, which is likely due the
ok
smaller number of training proteins. When the training
data includes larger number of proteins, TMpro-active
would be the same as TMpro in algorithm (see [28] for
performanceevaluationofTMprobytrainingwithallof
the 160 proteins).
Figure 2
Performance on MPtopo and PDBTM data sets Segment level TM prediction F-score results for
MPtopo. (A) Random, (B) Node-coverage, (C) Confusion-
We additionally tested prediction performance of our
rated, (D) Node-coverage and confusion-rated. It can be
active learning algorithms on two larger data sets,
seen that TMpro achieves high segment accuracy (F-score)
MPtopo dataset of 101 proteins (see results in Table 2)
even if the classifier is trained with just one protein that is
and PDBTM dataset of 191 proteins (see Table 3).
found by active learning algorithms. Node-Coverage shows
Figure 2 shows comparison of F-scores for Active
best performance.
Learning. For both data sets, Active learning methods
outperformed random selection. The results for the
active learning method that performed best for MPtopo
and PDBTM is Node Coverage. This method reaches on aftertenthiteration.Confusion-ratedandNode-coverage
average 91% segment level F-score even for single &Confusion-ratedperformapproximatelythesameafter
protein with balanced segment recall (92%) and preci- second iteration. Active learning significantly reduced
sion(%90)onMPtopodataset.Moreover,whentrained the cost (number of proteins labeled out of 160
evenwithasingleprotein,itachievedanF-scoreof91%, proteins) while reaching high prediction accuracy. As
segmentrecallof93%andsegmentprecisionof90%on showninTable2andTable3,Node-coveragecontinued
a PDBTM dataset. Although for second iteration the to outperform both Random and Confusion-rated,
performance is reduced for both data sets, it recovered achieving about 91% segment F-score with only five
Table 2: Comparison of TMpro NN: applying active vs. passive learning algorithms for updating training set from MPtopo
(101proteins,443TMsegments)
Methods #ofProteinsinTraining-Set Qok Qhtm Qhtm Qhtm Q2
Fscore %obs %prd
1 Random 1 14 40 41 41 63
2 25 60 59 61 67
5 36 84 88 81 74
10 35 79 81 78 74
2 Node-Coverage 1 44 91 92 90 78
2 44 91 91 90 79
5 44 91 92 90 79
10 46 91 92 90 79
3 Confusion-Rated 1 26 68 75 65 67
2 34 85 90 81 75
5 41 89 91 87 78
10 45 91 91 90 79
4 Node-Coverage&Confusion-Rated 1 44 91 92 90 78
2 44 90 91 89 79
5 44 91 91 90 79
10 45 90 91 90 78
Fordescriptionofcolumns,seecaptionofTable1.Q %obsandQ %predhavebeencomputedper-proteinandaveragedoveralltheproteins.
htm htm
Page7of9
(pagenumbernotforcitationpurposes)
BMCBioinformatics2010,11(Suppl1):S58 http://www.biomedcentral.com/1471-2105/11/S1/S58
Table 3: Comparison of TMpro NN: applying active vs. passive learning algorithms for updating training set from PDBTM
(191proteins,789TMsegments)
Methods #ofProteinsinTraining-Set Qok Qhtm Qhtm Qhtm Q2
Fscore %obs %prd
1 Random 1 20 51 54 49 20
2 32 69 72 66 32
5 35 76 78 73 35
10 35 78 81 75 35
2 Node-Coverage 1 50 91 93 90 79
2 47 90 91 88 79
5 49 91 92 89 79
10 50 91 93 90 79
3 Confusion-Rated 1 20 48 51 46 70
2 36 81 84 78 75
5 38 85 90 81 74
10 46 90 92 87 78
4 Node-Coverage&Confusion-Rated 1 50 91 93 90 79
2 49 90 92 88 78
5 48 91 93 89 79
10 51 91 93 90 79
Fordescriptionofcolumns,seecaptionofTable1.Qhtm,%obsandQhtm,%predhavebeencomputedper-proteinandaveragedoveralltheproteins.
training protein, as opposed to Random with 84% on Authors’ contributions
MPtopo and 76% on PDBTM and Confusion-rated with MKG designed the algorithms. JAW developed the
89% on MPtopo and 85% on PDBTM. Again, Node- framework for feature analysis and clustering and HUO
coverage & Confusion-rated performed the same as just developed the active learning components. Manuscript
Node-coverage on the first round, but on subsequent has been prepared by HUO and MKG. JGC provided
rounds the F-score decreased slightly below just Node- advice on algorithm-design and reviewed the manu-
coverage, perhaps showing that Confusion-rated is script.
known to typically exceed performance compared to
density-based selection after acquiring some amount of Acknowledgements
initial training [34]. In this case however, the feature JAW’s work has been funded by the NSF-NIBIB Bioengineering and
creation has been very effective and the learning rate is BioinformaticsSummerInstituteatUniversityofPittsburghgrantnumber
significantly high even with a single protein, leaving no EEC-0609139. MKG’s work has been partially funded by Department of
room for confusion rated selection to overtake density DefenseHenryM.JacksonFoundationgrantW81XWH-05-2-0005.
based selection.
This article has been published as part of BMC Bioinformatics Volume 11
Supplement 1, 2010: Selected articles from the Eighth Asia-Pacific
Bioinformatics Conference (APBC 2010). The full contents of the
Conclusion
supplement are available online at http://www.biomedcentral.com/1471-
Activelearningisapromisingmethodforbioinformatics 2105/11?issue=S1.
applications such as membrane structure prediction and
protein-protein interaction prediction which are marked References
by availability of small amounts of fully-characterized 1. Wallin E and von Heijne G: Genome-wide analysis of integral
membraneproteinsfromeubacterial,archaean,andeukar-
data and unwieldy procedures for experimental char- yoticorganisms.ProteinSci1998,7(4):1029–1038.
acterization. In this paper, active learning has been 2. Tusnady GE, Dosztanyi Z and Simon I: PDB_TM: selection and
employed to tag the proteins that prove to be most membrane localization of transmembrane proteins in the
protein data bank. Nucleic acids research 2005, 33 Database:
informative in training a transmembrane-helix predic- D275–278.
tionalgorithm,TMpro.Resultsshowthatactivelearning 3. White SH: Biophysical dissection of membrane proteins.
Nature2009,459(7245):344–346.
can significantly reduce the labelling costs without
4. White MA, Clark KM, Grayhack EJ and Dumont ME: Character-
degrading performance. It is seen that latent semantic istics affecting expression and solubilization of yeast mem-
braneproteins.JMolBiol2007,365(3):621–636.
analysis of protein sequences [28,35] is highly effective
5. Tseitin VM and Nikiforovich GV: Isolated transmembrane
for prediction of TM segments. helices arranged across a membrane: computational stu-
dies.Proteinengineering1999,12(4):305–311.
6. TreutleinH,LemmonM,EnglemanDandBrungerA:Simulationof
helixassociationinmembranes:modellingtheglycophorin
Competing interests A transmembrane domain. System Sciences, Proceeding of the
Twenty-SixthHawaiiInternationalConferenceon::19931993,708–714.
The authors declare that they have no competing
7. Filizola M, Perez JJ and Carteni-Farina M: BUNDLE: a program
interests. for building the transmembrane domains of G-protein-
Page8of9
(pagenumbernotforcitationpurposes)
BMCBioinformatics2010,11(Suppl1):S58 http://www.biomedcentral.com/1471-2105/11/S1/S58
coupledreceptors.Journalofcomputer-aidedmoleculardesign1998, 35. Ganapathiraju MK, Klein-Seetharaman J, Balakrishnan N and
12(2):111–118. Reddy R: Characterization of protein secondary structure.
8. Ott C and Lingappa V: Integral membrane protein biosynth- SignalProcessingMagazine,IEEE2004,21(3):78–87.
esis:whytopologyishardtopredict.JCellSci2002,115(Pt10):
2003–2009.
9. DeBarrDandWechslerH: SpamDetectionusingClustering,
RandomForests,andActiveLearning.SixthConferenceonEmail
andAnti-Spam.MountainView,California2009.
10. McCallum A and Nigam K: Employing EM and Pool-based
ActiveLearningforTextClassification.InternationalConference
onMachineLearning(ICML):19981998,359–367.
11. Nguyen H and Smeulders A: Active Learning using Pre-
clustering. International Conference on Machine Learning (ICML):
20042004,623–630.
12. Campbell C, Cristianini N and Smola A: Query Learning with
Large Margin Classifiers. International Conference on Machine
Learning(ICML):20002000,111–118.
13. TongSandKollerD:Supportvectormachineactivelearning
with applications to text classification. Proceedings of Interna-
tionalConferenceonMachineLearning2000,999–1006.
14. XuZ,YuK,TrespV,XuXandWangJ:RepresentativeSampling
for Text Classification Using Support Vector Machines.
Advances in Information Retrieval: 25th European Conf on IR Research
ECIR2003:2003;Italy2003.
15. Baram Y, El-Yaniv R and Luz K: Online Choice of Active
LearningAlgorithms.InternationalConferenceonMachineLearning
(ICML):20032003,19–26.
16. Donmez P, Carbonell J and Bennett P: Dual-Strategy Active
Learning. European Conference on Machine Learning (ECML): 2007;
Warsaw,Poland2007.
17. Melville P and Mooney R: Diverse Ensembles for Active
Learning. International Conference on Machine Learning (ICML):
20042004,584–591.
18. Liu Y: Active learning with support vector machine applied
to gene expression data for cancer classification. Journal of
chemicalinformationandcomputersciences2004,44(6):1936–1941.
19. McCallumAandNigamK:EmployingEMandpool-basedactive
learningfortextclassification.ICML’9819981998,359–367.
20. TongSandKollerD:Supportvectormachineactivelearning
with applications to text classification. Proceedings of Interna-
tionalConferenceonMachineLearning20002000,999–1006.
21. Nguyen HT and Smeulders A: Active learning with pre-
clustering.ICML‘04:20042004,623–630.
22. VogiatzisDandTsapatsoulisN:ActiveLearningformicroarray
data.InternationalJournalofApproximateReasoning2008,47:85–96.
23. JayasingheS,HristovaKandWhiteSH:Energetics,stability,and
prediction of transmembrane helices. Journal of molecular
biology2001,312(5):927–934.
24. WhiteSH:Globalstatisticsofproteinsequences:implications
fortheorigin,evolution,andpredictionofstructure.Annual
reviewofbiophysicsandbiomolecularstructure1994,23:407–439.
25. Kyte J and Doolittle RF: A simple method for displaying the
hydropathic character of aprotein. Journal of molecular biology
1982,157(1):105–132.
26. Sonnhammer EL, von Heijne G and Krogh A: A hidden Markov
model for predicting transmembrane helices in protein
sequences.Proceedings/InternationalConferenceonIntelligentSystems
forMolecularBiology;ISMB1998,6:175–182.
27. Kernytsky A and Rost B: Static benchmarking of membrane
helixpredictions.Nucleicacidsresearch2003,31(13):3642–3644.
28. GanapathirajuM,BalakrishnanN,ReddyRandKlein-SeetharamanJ:
Transmembranehelixpredictionusingaminoacidproperty
featuresandlatentsemanticanalysis.BMCBioinformatics2008,
9(Suppl1):S4. Publish with BioMed Central and e very
29. KroghA,LarssonB,vonHeijneGandSonnhammerEL:Predicting
scientist can read your work free of charge
transmembrane protein topology with a hidden Markov
model: application to complete genomes. J Mol Biol 2001,
305(3):567–580. "BioMed Central will be the most significant development for
disseminating the results of biomedical research in our lifetime."
30. Chen CP, Kernytsky A and Rost B: Transmembrane helix
predictionsrevisited.ProteinSci2002,11(12):2774–2791. Sir Paul Nurse, Cancer Research UK
31. Jayasinghe S, Hristova K and White SH: MPtopo: A database of
membraneproteintopology.ProteinSci2001,10(2):455–458. Your research papers will be:
32. Vesanto J and Alhoniemi E: Clustering of the self-organizing available free of charge to the entire biomedical community
map. IEEE transactions on neural networks/a publication of the IEEE
NeuralNetworksCouncil2000,11(3):586–600. peer reviewed and published immediately upon acceptance
33. HaykinS:Neuralnetworks:Acomprehensivefoundation.2. cited in PubMed and archived on PubMed Central
34. Donmez P,Carbonell JG andBennett PN: Dual Strategy Active
Learning. Proceedings of the 18th European conference on Machine yours — you keep the copyright
Learning.Warsaw,Poland2007.
Submit your manuscript here: BioMedcentral
http://www.biomedcentral.com/info/publishing_adv.asp
Page9of9
(pagenumbernotforcitationpurposes)

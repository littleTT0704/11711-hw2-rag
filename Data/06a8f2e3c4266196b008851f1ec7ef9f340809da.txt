Advancing Regular Language Reasoning in
Linear Recurrent Neural Networks
Ting-HanFan∗
IndependentResearcher
tinghanf@alumni.princeton.edu
Ta-ChungChi∗ AlexanderI.Rudnicky
CarnegieMellonUniversity CarnegieMellonUniversity
tachungc@andrew.cmu.edu air@cs.cmu.edu
Abstract LRNNs’abilitytomodelregularlanguage. Regular
languageisatypeoflanguagethatstrictlyfollows
In recent studies, linear recurrent neural net- certainruleslikegrammar.1 Thisisverydifferent
works (LRNNs) have achieved Transformer-
fromnaturallanguageashumanlanguageisfullof
levelperformanceinnaturallanguagemodeling
ambiguities. Thesuccessfulmodelingofaregular
andlong-rangemodelingwhileofferingrapid
paralleltrainingandconstantinferencecosts. language is important since it implies a model’s
WiththeresurgedinterestinLRNNs,westudy abilitytolearntheunderlyingrulesofthedata. For
whethertheycanlearnthehiddenrulesintrain- example,ifthetrainingdataarearithmeticopera-
ingsequences,suchasthegrammaticalstruc- tionssuchas1+2×3,themodelshouldlearnthe
turesofregularlanguage. Wetheoreticallyana-
rulesofa+b,a×b,andthat×hasahigherpriority
lyzesomeexistingLRNNsanddiscovertheir
than +. Learning unambiguous rules behind the
limitationsonregularlanguage. Motivatedby
data is a critical step toward sequence modeling
theanalysis,weproposeanewLRNNequipped
withregulatedoutput.
withablock-diagonalandinput-dependenttran-
sition matrix. Experiments suggest that the In this paper, we aim to determine if existing
proposedmodelistheonlyLRNNthatcanper- LRNNsarecompetenttolearnthecorrectgrammar
formlengthextrapolationonregularlanguage ofregularlanguagebytestingtheirlanguagetrans-
tasks such as Sum, Even Pair, and Modular
ductioncapabilityundertheextrapolationsetting.
Arithmetic. Concretely,amodelistrainedonlytopredictthe
desiredoutputsonasetofshortsequencesoflength
1 Introduction
L . Itthenneedstopredictthecorrectoutputsfor
tr
There is a recent surge of using linear recurrent longertestingsequencesoflengthL ≫ L .
ex tr
neural networks (RNNs) (Gu et al., 2022; Peng
Wetheoreticallyshowthatsomeoftherecently
et al., 2023; Orvieto et al., 2023) as alternatives
proposed LRNNs lack the expressiveness to en-
tothede-factoTransformerarchitecture(Vaswani
codecertainarithmeticoperationsusedinthetasks
etal.,2017;Radfordetal.,2019)thatisingrained
of regular language. In light of this observation,
inthefieldofnaturallanguageprocessing. LRNNs
we propose a new LRNN equipped with a block-
departfromtheinter-timestepnon-linearitydesign
diagonal and input-dependent transition matrix.
principle of classic RNNs (Elman, 1990; Jordan,
ThetwomodificationsofourproposedLRNNal-
1997; Hochreiter and Schmidhuber, 1997; Cho
lowthemodeltolearnandfollowthegrammarof
et al., 2014) while at the same time: 1. achieve
the regular language. Experiments show that the
Transformer-levelperformanceonthetaskofnatu-
proposedmodelistheonlyLRNNarchitecturethat
rallanguagemodeling(Fuetal.,2023;Polietal.,
canextrapolateonregularlanguagetaskssuchas
2023) and even better performance on synthetic
Sum,EvenPair,andModularArithmetic.
longrangemodelingtasks(Guetal.,2022;Gupta
etal.,2022;Orvietoetal.,2023;Hasanietal.,2023; 2 LimitationsofPriorWork
Smith et al., 2023). 2. have the added benefits
offastparallelizabletraining(MartinandCundy, Inthissection,weshowthatmostLRNNsareun-
2018)andconstantinferencecost. able to represent arithmetic operations, posing a
Inspiteoftheremarkableempiricalperformance seriousissueundertheextrapolationsettingwhere
onnaturallanguage,therehasbeennoresearchon
1Formallyspeaking,therulesaredefined/recognizedby
∗
Equalcontribution theunderlyingfinitestatemachine.
3202
peS
41
]LC.sc[
1v21470.9032:viXra
themodelhastolearntheunderlyinglanguageto Ontheotherhand,letx = A2z+Au +u be
0− 0 −
combatthelengthdistributionalshift. thevectorrepresentationfor"0-". Thesequences
"0-0-1"and"0-1-0"arerepresentedas
2.1 LinearRNN
x = A3x +A2u +Au +u
0−0−1 0− 0 − 1
In this paper, we consider a general family of
x = A3x +A2u +Au +u .
LRNNsasfollows. 0−1−0 0− 1 − 0
Noticex isfor"0-0-1"whilex for"0-
x = A x +Bu 0−0−1 0−1−0
k k k−1 k
(1) 1-0". Enforcingx = x ,wehave
0−0−1 0−1−0
y = h(x ).
k k
A2u +Au +u = A2u +Au +u ,
0 − 1 1 − 0
A isamatrixthatdefinestherecurrencerelation.
k
A mayormaynotdependontheinputu . When whichisacontradiction.
k k
itisinput-independent,A isreducedtoA;other-
k
2.3 Input-dependentDiagonalLRNN
wise, A = g(u ) for some function g. The first
k k
line encodes a linear recurrence on the state x . ToreducethecomputationalcomplexityofLRNNs,
k
Thesecondlineisanoutputy thatdependsonx . there has been interest in applying diagonal lin-
k k
Tocontrolthemodel’sexpressiveness,thefunction ear recurrence (Gupta et al., 2022; Smith et al.,
hmayormaynotbealinearoperation. Sincethe 2023; Orvieto et al., 2023). In particular, prior
existing LRNNs differ in their linear recurrence workadoptsinput-independentdiagonalrecurrence
relations(Eq.(2),(3),and(4)),wemainlyfocuson andisunabletorepresentsubtractionasshownin
thelinearrecurrenceofeachmodelinthispaper. §2.2. Asaresult,onemaywonderifgeneralizing
themodeltodiagonalinput-dependentRNNscan
2.2 Input-independentLRNN solvetheproblem,asdefinedinEq.(3).
Tobeginwith,state-spacemodels(indiscrete-time
x = diag(v )x +Bu , (3)
k k k−1 k
format)followthestandardLRNNrecurrence.
wherev = f(u )isavectorthatdependsonu .
k k k
x = Ax +Bu (2) To answer this question, we show that an input-
k k−1 k
dependent diagonal LRNN cannot represent sub-
Eq.(2)encapsulatestherecurrencerelationofthe tractioninproposition2.
S4models(Guetal.,2022;Guptaetal.,2022),S5
Proposition2. Aninput-dependentdiagonallinear
model(Smithetal.,2023), andLinearRecurrent
RNNisinconsistentinrepresentingsubtraction.
Unit (Orvieto et al., 2023). For example, A is in
TheproofisessentiallyageneralizationofPropo-
thefamilyofHiPPOmatrix(Guetal.,2023)inS4
sition1andisdeferredtoAppendixA.1.
oracomplexdiagonalmatrixinLinearRecurrent
Unit. We show in Proposition 1 that such input- 2.4 Implication
independentAmatrixcannotrepresentsubtraction.
Note the failure in representation implies the ex-
Proposition1. Aninput-independentLRNNisin- trapolation error is large, but the model may still
consistentinrepresentingsubtraction. performwellifthetestinglengthisnogreaterthan
thetraininglength. Wewillevaluatethisin§4.
Proof. Denoteu , u , andu astheinputvector
0 − 1
3 ProposedMethod
w.r.t. input character 0, -, and 1. Denote z as the
initialstatevector. Thesequences"0-1"and"1-0"
3.1 MotivationfromLiquid-S4
arerepresentedas
The recently proposed liquid-S4 (Hasani et al.,
x = A3z+A2u +Au +u , for"0-1" 2023) can be seen as a generalization of Eq. (2)
0−1 0 − 1
and (3), as its transition matrix is composed of
x = A3z+A2u +Au +u , for"1-0"
1−0 1 − 0 an input-independent block matrix and an input-
dependent diagonal matrix with the following re-
Because0−1 ̸= 1−0,byforcingx ̸= x ,
0−1 1−0 currence.
wehave
x = Ax +(Bu )⊙x +Bu
k k−1 k k−1 k
(4)
A2u 0+Au −+u 1 ̸= A2u 1+Au −+u 0. = (A+diag(Bu k))x k−1+Bu k,
where ⊙ denotes the Hadamard product and Notethat1isacolumnvectorofallones. |·|and
diag(w)constructsadiagonalmatrixfromw. ≤areelement-wiseabsolutevalueandinequality
Although our experiments in §4.3 show that operations. Thelasttwoinequalitiesfollowfrom
Liquid-S4cannotextrapolateonregularlanguage thatA(i) andA(i) ’scolumnnormsarenogreater
k+1 k
tasks,toourbestknowledge,itisstillthefirsttouse than1in∥·∥ .
1
input-dependentblockmatricesA+diag(Bu k). In Eq.(7)demonstratesthatp = 1canstabilizethe
§3.2, we will introduce a construction of input- proposedblock-diagonalrecurrence,Eq.(5). How-
dependent block matrices that can potentially ever, asmallprestrictsamodel’sexpressiveness.
model regular language tasks with sufficient nu- In§4.3,wewillshowthatp = 1.2issmallenough
mericalstability. toyieldgoodempiricalperformance.
3.2 Block-diagonalInput-dependentLRNN
4 Experiments
Asshownin§2,intheworldofLRNNs,neitherthe
4.1 RegularLanguageTasks
input-independentrecurrencenorinput-dependent
diagonalrecurrencecanaccuratelyrepresentarith- Regular language is the type of formal lan-
meticoperations. Tobalancebetweenrepresenta- guage recognized by a Finite State Automata
tionabilityandcomputationalefficiency,wepro- (FSA)(Chomsky,1956). AnFSAisdescribedby
poseaninput-dependentblock-diagonalLRNNas a5-tuple(Q,Σ,δ,q 0,F). Qisafinitenon-empty
setofstates. Σisafinitenon-emptysetofsymbols.
x = A x +Bu , (5)
k k k−1 k q ∈ Qisaninitialstate. δ : Q×Σ → Qisatran-
0
whereA = g(u )isablockdiagonalmatrixthat sitionfunction;F ⊆ Qisasetoffinalstates. As
k k
dependsonu butnotontheprevioustimesteps. g notedinDeletangetal.(2023),languagetransduc-
k
is an arbitrary function with the output being the tioncanbemoreusefulthanlanguagerecognition
sizeofA . inpractice. Languagetransductionmapsfromone
k
Eq.(5)isnumericallyunstablebecausetheprod- stringtoanotherwhilelanguagerecognitionclassi-
uct
(cid:81)k
A could produce large numbers. The fieswhetherastringobeysarule.
i=1 i
solutionistoimposeadditionalconstraintsonthe In this work, we follow the regular language
blocksofA inEq(5): transduction tasks in Deletang et al. (2023). We
k
areparticularlyinterestedinSum(5),EvenPair(5),
(cid:16) (cid:17) ModArith(5). For Sum(M), the input is a string
A = diag A(1) ,...,A(h) ∈ Rbh×bh
k k k {s }n−1 of numbers in [0,...,M − 1]. The
i i=0
(cid:104) (cid:105)
A(i) = v(i,1) ... v(i,b) ∈ Rb×b (6) output is the sum of them under modulo M:
k k k (cid:80)n−1s modM. Forexample,whenM = 5,the
i=0 i
(i,j)
∥v ∥ ≤ 1, i ∈ [1,...,h], j ∈ [1,...,b], input"0324"correspondstotheoutput"4"because
k p
0+3+2+4mod5 = 4. Notably,Sum(2)isthe
(i,j)
where ∥·∥ denotes the vector p-norm and v
p k famous PARITY problem that evaluates whether
is a column vector that depends on u . For any
k thereisanoddnumberof1’sinabitstring. Thus,
vectorv,wecanderiveanothervectorv′ tosatisfy
Sum(M)isageneralizationofPARITYandshares
p-normconstraintthroughv′ = v/max(1,∥v∥ ).
p thesamecharacteristic: Ifoneerroroccursduring
Because ∥v∥ ≥ ∥v∥ if p ≤ q, a smaller p im-
p q thesummation,theoutputwillbewrong.
posesastrongerconstraintonthecolumnsofA(i) . In EvenPair(M), the input is a string {s }n−1
k i i=0
Inotherwords,wecanstabilizeEq.(5)byselecting of numbers in [0,...,M − 1]. The output is 1 if
asufficientlysmallp. s = s and 0 otherwise. For example, when
n−1 0
(i)
Takep = 1asanexample. EveryblockA is M = 5,theinput"0320"correspondstotheoutput
k
amatrixthatnoneofitscolumnnormsisgreater "1" because the first entry equals the last entry.
than1in∥·∥ . ThisimpliesA(i) A(i) isthesame SinceEvenPair(M)onlycaresaboutthefirstand
1 k+1 k
kindofmatrix. Specifically,letv(1),...,v(b) bethe last entries, the model should learn to remember
(i) (i) thefirstentryandforgettheentriesi ∈ [1,..,n−2].
columnsofA A . Wehave
k+1 k InModArith(M),theinputisastring{s }n−1
(cid:12) (cid:12) i i=0
(cid:2) ∥v(1)∥ ... ∥v(b)∥ (cid:3) = 1⊤(cid:12)A(i) A(i)(cid:12) ofoddlength(i.e.,nisodd). Theevenentries(i ∈
1 1 (cid:12) k+1 k (cid:12)
(7) [0,2,...])arenumbersin[0,...,M−1];Theodden-
(cid:12) (cid:12)(cid:12) (cid:12) (cid:12) (cid:12)
≤ 1⊤(cid:12)A(i) (cid:12)(cid:12)A(i)(cid:12) ≤ 1⊤(cid:12)A(i)(cid:12) ≤ 1⊤. tries(i ∈ [1,3,...])aresymbolsin{+,−,×}. The
(cid:12) k+1(cid:12)(cid:12) k (cid:12) (cid:12) k (cid:12)
outputistheanswerofthemathematicalexpression To accelerate the computational speed, all
undermoduloM.Forexample,whenM = 5,the LRNNsinTable2areimplementedwiththeparal-
input"1+2-3×4"correspondstotheoutput"1"be- lelscanalgorithm(MartinandCundy,2018). The
cause 1 + 2 − 3 × 4 mod 5 = −9 mod 5 = 1. computationalcostofourmodelisO(b2hlog(T))
ModArith(M) is much more complicated than whereb,handT areblocksize,numberofblocks,
Sum(M) and EvenPair(M) because the model and sequence length, respectively. Because the
should learn to prioritize multiplication over ad- embeddingdimensionisheldfixedasbh,thecom-
ditionandsubtraction. plexityscaleslinearlyw.r.ttheblocksize.
Table2comparestheextrapolationabilityofour
4.2 LengthExtrapolation
modelwithotherLRNNbaselinesonregularlan-
In our pilot experiments, we discovered that all guage tasks. Aswe cansee, the proposedmodel,
modelscanachievenear-perfectsame-lengthtest- Eq.(5)and(6),istheonlyLRNNthatcanextrapo-
ing accuracy; i.e., testing with L = L . We lateonregularlanguage.
ex tr
hypothesize that this is because a huge enough S4 and S4D cannot extrapolate on ModArith.
model(e.g.,enlargingtheembeddingdimension) This is expected as Prop. 1 shows that S4 and
canmemorizealltrainingsequences. Memorizing S4Dcannotrepresentsubtractionduetotheirinput-
the training sequences does not mean the model independenttransitionmatrices. AsforLiquid-S4,
can do well during testing, especially when the although it uses input-dependent block matrices
testingsequencesarelongerthanthetrainingones. (discussed in §3.1), it still cannot extrapolate on
Toevaluatewhetheramodellearnstheunderlying regularlanguage. Webelievethiscanbeexplained
rulesofthelanguagewithoutsimplymemorizing byitslowexpressiveness(Eq.(4))comparedtothe
thetrainingdata,wefirsttrainitonsequencesof proposedmodel(Eq.(5)and(6)). Overall,wecan
lengthL generatedbyanFSA;Itisthenevaluated seethatthecombinationofinputdependencyand
tr
onsequencesoflengthL > L generatedbythe sufficientexpressivenessplaysanimportantrolein
ex tr
sameFSA. termsofregularlanguagemodeling.
Table 1 summarizes the extrapolation setting.
We mostly follow the requirements in Deletang Ours S4 S4D Liquid-S4
et al. (2023), where the training and extrapola- Sum(5) 1.00 0.27 0.27 0.27
tion lengths are 40 and 500. The lengths for EvenPair(5) 0.99 0.81 0.82 0.72
ModArith(5)are39and499becausethistaskre- ModArith(5) 1.00 0.27 0.27 0.27
quiresodd-lengthinputs.
Table2: ExtrapolationonRegularLanguageTasks.
Eachreportednumberisanaverageoffiverandomtrials.
Sum(5) EvenPair(5) ModArith(5)
Eachrandomtrialreturnsthebesttestingaccuracyover
L 40 40 39
tr 40,000gradientupdates.
L 500 500 499
ex
Table 1: Our Training and Extrapolation Settings.
5 Conclusion
L andL representthetrainingandextrapolation
tr ex
sequencelengths,respectively. Inthiswork,weexplorelinearRNNsintherealm
of regular language modeling. We discover that
existing LRNN models cannot represent subtrac-
4.3 ExperimentalResults
tion and in turn propose a new LRNN equipped
We implemented LRNNs such as S4 (Gu et al., withablock-diagonalandinput-dependenttransi-
2022), S4D (Gupta et al., 2022), and Liquid-S4 tionmatrix. Ourexperimentsconfirmtheproposed
(Hasanietal.,2023)usingtheirreleasedcodebases model’sabilitytomodelregularlanguagetaskslike
asbaselinemethods. Fortheproposedmethod,we Sum,EvenPair,andModularArithmeticunderthe
setp = 1.2inEq.(6)andtraintheblock-diagonal challenginglengthextrapolationsetting.
input-dependent LRNN with (b, h) = (8, 8). Be-
cause ModArith is more complicated than Sum
References
andEvenPair,ModArithuses3layerswhilethe
others take 1 layer. Each layer is a full pass of
Kyunghyun Cho, Bart van Merriënboer, Caglar Gul-
LRNNasEq.(1). cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning 40thInternationalConferenceonMachineLearning,
phraserepresentationsusingRNNencoder–decoder volume 202 of Proceedings of Machine Learning
for statistical machine translation. In Proceedings Research,pages26670–26698.PMLR.
of the 2014 Conference on Empirical Methods in
NaturalLanguageProcessing(EMNLP),pages1724– Bo Peng, Eric Alcaide, Quentin Anthony, Alon Al-
1734, Doha, Qatar. Association for Computational balak,SamuelArcadinho,HuanqiCao,XinCheng,
Linguistics. Michael Chung, Matteo Grella, Kranthi Kiran GV,
et al. 2023. Rwkv: Reinventing rnns for the trans-
NoamChomsky.1956. Threemodelsforthedescription formerera. arXivpreprintarXiv:2305.13048.
oflanguage. IRETransactionsoninformationtheory,
MichaelPoli,StefanoMassaroli,EricNguyen,DanielY
2(3):113–124.
Fu, TriDao, StephenBaccus, YoshuaBengio, Ste-
GregoireDeletang,AnianRuoss,JordiGrau-Moya,Tim fanoErmon,andChristopherRe.2023. Hyenahierar-
Genewein, Li Kevin Wenliang, Elliot Catt, Chris chy: Towardslargerconvolutionallanguagemodels.
Cundy,MarcusHutter,ShaneLegg,JoelVeness,and
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Pedro A Ortega. 2023. Neural networks and the
DarioAmodei,andIlyaSutskever.2019. Language
chomskyhierarchy. In TheEleventhInternational
modelsareunsupervisedmultitasklearners.
ConferenceonLearningRepresentations.
JimmyT.H.Smith,AndrewWarrington,andScottLin-
JeffreyLElman.1990. Findingstructureintime. Cog-
derman.2023. Simplifiedstatespacelayersforse-
nitivescience,14(2):179–211.
quencemodeling. InTheEleventhInternationalCon-
DanielYFu,TriDao,KhaledKamalSaab,ArminW
ferenceonLearningRepresentations.
Thomas,AtriRudra,andChristopherRe.2023. Hun-
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
gryhungryhippos: Towardslanguagemodelingwith
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
state space models. In The Eleventh International
Kaiser,andIlliaPolosukhin.2017. Attentionisall
ConferenceonLearningRepresentations.
youneed. Advancesinneuralinformationprocessing
systems,30.
AlbertGu,KaranGoel,andChristopherRe.2022. Ef-
ficiently modeling long sequences with structured
A AdditionalProofs
statespaces. InInternationalConferenceonLearn-
ingRepresentations.
A.1 ProofofProposition2
AlbertGu,IsysJohnson,AmanTimalsina,AtriRudra, Denote (A ,u ), (A ,u ), and (A ,u ) as the
0 0 − − 1 1
andChristopherRe.2023. HowtotrainyourHIPPO:
pairof(transitionmatrix,inputvector)w.r.t. input
Statespacemodelswithgeneralizedorthogonalbasis
projections. InInternationalConferenceonLearning character0,-,and1. NotethatA 0,A −,andA 1 are
Representations. diagonalmatricesbyassumption.
Denote z as the initial state vector. The se-
Ankit Gupta, Albert Gu, and Jonathan Berant. 2022.
quences"0-1"and"1-0"arerepresentedas
Diagonalstatespacesareaseffectiveasstructured
state spaces. In Advances in Neural Information
x = A A A z+A A u +A u +u
ProcessingSystems. 0−1 1 − 0 1 − 0 1 − 1
x = A A A z+A A u +A u +u .
1−0 0 − 1 0 − 1 0 − 0
Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang,
Makram Chahine, Alexander Amini, and Daniela
Note x is for "0-1" while x for "1-0". Be-
Rus.2023. Liquidstructuralstate-spacemodels. In 0−1 1−0
cause A’s are diagonal, we know A A A =
TheEleventhInternationalConferenceonLearning 1 − 0
Representations. A 0A −A 1. Because 0−1 ̸= 1−0, by enforcing
x ̸= x ,wehave
0−1 1−0
SeppHochreiterandJürgenSchmidhuber.1997. Long
short-termmemory. Neuralcomputation,9(8):1735–
A A u +A u +u ̸= A A u +A u +u .
1 − 0 1 − 1 0 − 1 0 − 0
1780.
(8)
Michael I Jordan. 1997. Serial order: A parallel dis- Ontheotherhand,letx = A A z+A u +u
0− − 0 − 0 −
tributed processing approach. In Advances in psy- bethevectorrepresentationfor"0-". Considertwo
chology,volume121,pages471–495.Elsevier.
othersequences: "0-0-1"and"0-1-0". Theirvector
EricMartinandChrisCundy.2018. Parallelizinglinear representationsare
recurrentneuralnetsoversequencelength. InInter-
nationalConferenceonLearningRepresentations. x = A A A x +A A u +A u +u
0−0−1 1 − 0 0− 1 − 0 1 − 1
x = A A A x +A A u +A u +u .
AntonioOrvieto,SamuelLSmith,AlbertGu,Anushan 0−1−0 0 − 1 0− 0 − 1 0 − 0
Fernando, Caglar Gulcehre, Razvan Pascanu, and
Note x is for "0-0-1" while x for
SohamDe.2023. Resurrectingrecurrentneuralnet- 0−0−1 0−1−0
works for long sequences. In Proceedings of the "0-1-0". Similarly, because A’s are diagonal and
0−0−1 = 0−1−0, by enforcing x =
0−0−1
x ,wehave
0−1−0
A A u +A u +u = A A u +A u +u .
1 − 0 1 − 1 0 − 1 0 − 0
(9)
BecauseEq.(8)contradictswithEq.(9), thetwo
relationsx ̸= x andx = x can-
0−1 1−0 0−0−1 0−1−0
not co-exist. We hence conclude that an input-
dependentdiagonallinearRNNisinconsistentin
representingsubtraction.

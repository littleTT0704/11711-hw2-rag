POWERTRANSFORMER:
Unsupervised Controllable Revision for Biased Language Correction
Xinyao(Michelle)Ma(cid:63) (cid:5) MaartenSap(cid:63) (cid:5) HannahRashkin(cid:5) YejinChoi(cid:5)‚Ä†
(cid:5)PaulG.AllenSchoolofComputerScience&Engineering,UniversityofWashington
‚Ä†AllenInstituteforArtificialIntelligence
Seattle,USA
{max36,msap,hrashkin,yejin}@cs.washington.edu
Abstract
AGENT to daydream AGENT to pursue
Unconsciousbiasescontinuetobeprevalentin Connotation
Frames
moderntextandmedia,callingforalgorithms agency(AG) = low agency(AG) = high
thatcanassistwriterswithbiascorrection.For
Mey daydreams of Mey pursues her
example,afemalecharacterinastoryisoften
being a doctor. dream to be a doctor.
portrayedaspassiveandpowerless(‚ÄúSheday-
dreams about being a doctor‚Äù) while a man
PowerTransformer
is portrayed as more proactive and powerful
(‚ÄúHepursueshisdreamofbeingadoctor‚Äù).
agency(AG) = low agency(AG) = high
Ana wandered Ana strutted
WeformulateControllableDebiasing,anew PowerTransformer
through the park. through the park.
revisiontaskthataimstorewriteagiventextto
correcttheimplicitandpotentiallyundesirable agency(AG) = low agency(AG) = high
biasincharacterportrayals.Wethenintroduce Issa enjoyed PowerTransformer Issa loved playing
football growing up. football growing up.
POWERTRANSFORMER as an approach that
debiases text through the lens of connotation
frames (Sap et al., 2017), which encode prag- Figure 1: Examples of using connotation frames (Sap
matic knowledge of implied power dynamics etal.,2017)forcontrollablerevisionstoportraycharac-
withrespecttoverbpredicates. Onekeychal- terswithmoreagencyandpower. Inthesecondexam-
lenge of our task is the lack of parallel cor- ple, ‚ÄúAnastrutted‚Äùimpliesthatsheismoreactiveand
pora. To address this challenge, we adopt an decisive,comparedto‚ÄúAnawandered‚Äùwhichportrays
unsupervised approach using auxiliary super- herasaimlessandpassive.
visionwithrelatedtaskssuchasparaphrasing
andself-supervisionbasedonareconstruction
loss,buildingonpretrainedlanguagemodels.
peoplearedescribed. Forexample,automatically
Throughcomprehensiveexperimentsbasedon
rewriting‚ÄúMeydaydreamedaboutbeingadoctor‚Äù
automatic and human evaluations, we demon-
as ‚ÄúMey pursued her dream to be a doctor‚Äù por-
stratethatourapproachoutperformsablations
trays Mey with more authority and decisiveness
and existing methods from related tasks. Fur-
(Figure 1). Such controllable revision methods
thermore, we demonstrate the use of POWER-
TRANSFORMER as a step toward mitigating couldbeusedtohelpreshapehowgenderrolesare
the well-documented gender bias in character portrayedinmedia(e.g.,throughmachine-in-the-
portrayalinmoviescripts. loopwritingsystems;Clarketal.,2018).
1 Introduction To edit such biases out of text, a controllable
rewritingmodelfacesthreekeychallenges. First,a
Narrativesandnewstextsoftenreflectsocietalbi-
modelshouldbeabletomakeeditsbeyondsurface-
ases and stereotypes, such as the traditional gen-
levelparaphrasing,assimpleparaphrasingwillof-
der role that women are passive and submissive
ten not adequately debias the underlying events
(Lakoff,1973;Fiske,1993;Fastetal.,2016). The
described. For example, Mey‚Äôs portrayal in Fig-
task of controllable text revision, i.e., rephrasing
ure1carriesbothovertbias(thechoiceofaction)
texttoatargetedstyleorframing,canhelpcorrect
and subtle bias (the framing of the action), both
forthesebiasesbyalteringandequalizingtheway
of which require rewriting to be adequately debi-
(cid:63)Bothauthorscontributedequally. ased. Second,amodel‚Äôsdebiasingrevisionsshould
be purposeful and precise and should not make text. Wereleaseourpreprocesseddataandcodeat
unnecessary changes to the underlying meaning http://maartensap.com/controllable-debiasing.
of the original text. Lastly, since parallel data
2 ControllableDebiasing
does not exist, models must learn to revise and
debias text without supervised data, thereby pre-
Controllable Debiasing is a novel formalization
ventingstraightforwardmachinetranslation-style
of stylistic rewriting that aims to debias the por-
modelling.
trayalofcharactersthroughcontrollablerevision.
WeformulateControllableDebiasingasanew Toachievethedesiredcharacterportrayal,asystem
controllabletextrevisiontaskthataimstocorrect mustbeabletochangetheunderlyingmeaningof
the implicit and possibly unwanted bias against events, unlike certain formalizations (e.g., polite-
or towards a specific character portrayed in text nesstransfer;RaoandTetreault,2018)wherefull
(¬ß2). As shown in Figure 1 (top), we study the meaningpreservationisrequired. Withoutthis,sys-
portrayal biases through the lens of connotation temsruntheriskofmerelyparaphrasingthebiases
frames of power and agency (Sap et al., 2017), in text. However, revisions must be precise and
whichprovidepragmaticknowledgeaboutimplied avoid unnecessary meaning changes, which can
powerandagencylevelsprojectedontocharacters often occur in stylistic rewriting (e.g., reversing
byapredicate. the sentiment of a review drastically changes its
Wecreate POWERTRANSFORMER,anencoder- underlyingmeaning).
decoder model that rewrites sentences with a de- Forournewrewritingtaskofchangingportrayal
sired portrayal using agency connotation frames bias, we focus on connotation frames that mea-
(¬ß3). Wecombineareconstructionandparaphrase surethepower andagencyascribedtocharacters
objective into our model to overcome the lack of throughtheactionstheytake. Connotationframes
parallelsuperviseddata,buildingoffofthedenois- (Rashkin et al., 2016; Sap et al., 2017) distill im-
ing autoencoder setup from Li et al. (2018a). To plicit relations between a verb, its agent, and its
steertherevisions,weendowthemodelwithcon- theme. In this work, we use the positive, neutral,
notation frame knowledge both at training time and negative agency dimensions, where agency
usingcontroltokens,andatgenerationtimeusing is defined as the capacity to intentionally make
agency-basedvocabboosting. changes or act upon one‚Äôs environment (Dennett,
1989). For example, illustrated in Figure 1, ‚ÄúX
Ourfindingsshowthat POWERTRANSFORMER
pursued Y‚Äù implies that X has positive agency.1
is effective at rewriting sentences with desired
Using machine-in-the-loop writing systems (e.g.,
agency connotations while only making minimal
Ghazvininejadetal.,2016,2017;Clarketal.,2018,
changes to their meaning, as measured through
Textio2), models trained on this task could help
both human and automatic evaluations (¬ß4). We
authorswritenews,stories,ormoviesthatportray
also show that POWERTRANSFORMER signifi-
charactersinlessbiasedways,andtherebyhelpmit-
cantlyoutperformsexistingstylisticrewritingmeth-
igatethenegativeeffectsofstereotypicalportrayals
ods (Prabhumoye et al., 2018; Dathathri et al.,
inmedia(Behm-MorawitzandMastro,2008;Field
2020)onthosemetrics. Additionally,throughab-
etal.,2019).
lationsstudies,weestablishtheusefulnessofeach
componentofthemodel,findingbenefitsfromboth
3 POWERTRANSFORMER
thejointobjective(47%gaininaccuracy)andthe
agencyscaling(12%gaininaccuracy). We present a new approach for Controllable De-
biasingcalled POWERTRANSFORMER,whichad-
Finally, in ¬ß5, we apply Controllable Debias-
dressestwokeychallenges: thepaucityofparallel
ingtoacorpusofmodernEnglishmovies(Gorin-
supervised data for training and the difficulty of
ski and Lapata, 2015) as a step towards remov-
incorporatingfine-grainedcontrolforsteeringthe
inggenderbiasincharacterportrayalestablished
agency of the output. Our approach (Figure 2)
by prior work (Sap et al., 2017). Using POW-
jointlylearnstoreconstructpartiallymaskedstory
ERTRANSFORMER, we revise the movie scripts
and significantly increase the agency levels of 1Futureworkcouldexploreusingthepowerdimension
female characters, thereby reducing the gender insteadofagency,oralternativeoperationalizationsofbiases,
e.g.,SocialBiasFrames(Sapetal.,2020)orregardtowards
bias. Our findings show promise for using mod-
minoritiesasintroducedbyShengetal.(2019).
ern NLP tools to help mitigate societal biases in 2https://textio.com/
+ ùõÉAw Vocab boosting
Issa played football growing up. + at decoding time
boosted logits agency scaling
Joint reconstruction + paraphrase objective
next word logits
at training time
GPT Transformer
Transformer
Issa <VERB> football growing up. <POS>
inputs
masking
- positive
Issa enjoyed football growing up.
Target agency
Figure 2: Overview of the full POWERTRANSFORMER model. An input sentence is masked for verb tokens
indicativeofagency. MaskedinputsandtargetagencyareusedasGPTinputs. Weuseajointobjectiveusingboth
paraphrasedataandmaskedinputsentencesfortraining.Atdecodingtime,weemployavocabboostingtechnique
tosteergenerationstowardsthetargetagency.
sentences while also learning to paraphrase from et al. (2017).4 Then, we mask out all verbs in-
anexternalcorpusofparaphrases(¬ß3.2). Atgener- dicative of the agency level, replacing them with
ationtime,wealsoincludeaboostingmethodfor a special <VERB> token. In this setup, the target
fine-grained steering towards the desired agency output is the original sentence x = {x ,...,x },
1 n
levelasdescribedin¬ß3.3. withthemaskedsentencexÀÜandthetargetagency
leveltasinputs. Duringtraining,weminimizethe
3.1 ModelOverview
cross entropy of the target output sentence given
POWERTRANSFORMER is an encoder-decoder theinputs:
style model with an OpenAI-GPT transformer
n
1 (cid:88)
model (Radford et al., 2018) as the base. The L = ‚àí logp(x |x ,xÀÜ,t) (2)
recon i <i
n
input sentence x is converted to a sequence of
i=1
bytepairencodings(BPE){x ,...,x },andgiven
1 n Paraphrasing Togobeyondreconstructingsen-
to the encoder after being scrubbed of its agency
tences, we add a paraphrasing objective using an
markers as described below. To steer the model,
out-of-domain paraphrase corpus (¬ß4.1). We ex-
we also give the encoder the target agency t,
tractagencylevelsforeachsentenceanditspara-
whichwerepresentasoneofthreespecialtokens
phraseandmaskouttheagencyverbsintheinput,
{<Pos>,<Equal>,<Neg>}.3
usingthesamemethodsasdescribedabove. Here,
theinputsarethemaskedsentencexÀÜandthetarget
3.2 JointObjective
agencyt,whilethetargetoutputy = {y ,...,y }
1 m
Wetrainourmodelonbothareconstructionanda
istheparaphrase. Aswithreconstruction,wemin-
paraphrasingtask,forwhichinputsaremaskedand
imizethecrossentropyofthetargetoutputgiven
paraphrasedversionsoftheoutput,respectively.
theinputs:
L joint = L recon+L para (1) 1 (cid:88)m
L = ‚àí logp(y |y ,xÀÜ,t) (3)
para i <i
m
Masking and Reconstructing Inspired by the i=1
delete-retrieve-generate model from Li et al. 3.3 ControlledDecodingwithVocabBoosting
(2018a),thisobjectiveteachesthemodeltorecover
We employ a vocab-boosting technique during
maskedoutagency-associatedverbsinsentences.
generation to encourage models towards generat-
Wefirstassignanagencyleveltoaninputsentence
ing with the desired agency, inspired by Ghosh
bycountingverbsintheagencylexiconfromSap
4For sentences that have multiple verbs, we assign the
3In earlier experiments, we also provided the original agencylevelthatthemostverbsinthesentencehave(e.g.,
agencyasaninputtothemodelduringtraininganddecoding, asentencewithtwopositiveagencyverbsandonenegative
butfoundthatitmadelittledifferenceinperformance. agencyverbwillbeassignedpositiveagency).
Type #Instances Pos Neutral Neg paraphrasesforbothparaphraseandreconstruction
tasks. We show data statistics in Table 1, with
train 10721 3834 4151 2736
additionalpreprocessingdetailsinAppendixA.
dev 1803 633 710 460
test 899 325 350 224
ROCstorycorpus Themainfocusofourstudy
train 45000 16410 14153 14437 is controllable revision of story sentences; there-
dev 10000 3645 3328 3127 fore,weselectsentencesfromtheROCstorycor-
pus (ROC Mostafazadeh et al., 2016). After ex-
Table1: Statisticsforourmainstorysentencesdataset
tracting agency levels for all sentences from the
(ROC)andfortheexternalparaphrasecorpus(Para.).
trainingstories,wesampleroughlyequalamounts
ofallthreeagencylevels,andrandomlysplitsen-
etal.(2017). Ateachdecodingtimestepi,were- tencesintotraining,development,andtestsets.6
scaletheunnormalizedtokenprobabilities(logits
l i ‚àà RV,whereVisthevocabularysize)toboost Paraphrasecorpus Asadditionaltrainingdata,
thelikelihoodofpredictingwordswiththetarget we use the corpus of automatically aligned para-
agency. Thenexttokenprobabilitiesarethencom- phrasesofTVsubtitles(Creutz,2018,Para.). As
putedusingthe‚Äúboosted‚Äùlogits: withtheROCstorycorpus,weextractagencylev-
elsforeachsentenceanditsparaphrase,thensam-
P(y |y ,x,t) ‚àù softmax(l +Œ≤ ¬∑Aw) (4)
i <i i pleroughlyequalamountsofpairswithalldifferent
sentence-paraphraseagencycombinations(further
where A is a RV√ó3 matrix that represents a 3-
details in ¬ßA.2). We randomly split the data into
dimensional{positive,equal,andnegative}agency
45ktrainand10kdev. instances(Table1).7
embedding for each token in the vocabulary, w
isaR3 one-hotvectordenotingthetargetagency
4.2 Metrics
for the output, and Œ≤ is a scalar hyperparameter
representing the boosting strength. We create A
Inadditiontohumanevaluations,wealsouseavari-
manually using the verbs in the agency lexicon
etyofautomatedevaluationmetricstocharacterize
(Sap et al., 2017).5 Used only at decoding time,
differentaspectsofperformance. Wemeasurethe
thismethodeffectivelyincreasesthelikelihoodof
accuracy of the change in agency by comparing
usingawordwiththetargetagencylevel.
thetargetagencylevelwiththatoftheoutput(ex-
tractedusingtheconnotationframeslexicon). As
4 ControllableDebiasingExperiments
ameasureofmeaningpreservation,weuseBERT-
Inthissection,wedescribethreeexperimentsfor scoreF1metrics(Zhangetal.,2020)tocompare
investigating POWERTRANSFORMERperformance. thesemanticsimilarityoftheinputsentencewith
First, we evaluate performance of our full model themachineoutput.
andablatedbaselines,usingautomaticmetricsto Asadditionalmetrics, wemeasurethefluency,
quantifytheeffectivenessofeachmodellingcom- therepetitiveness,anddiversityoftheoutput. Fol-
ponent(¬ß4.4). Next,wecompareourfullmodelto lowingpreviouswork(Daietal.,2019),wemea-
baselinesfromrelatedwork(¬ß4.5). Lastly, given surefluencyasperplexity(PPL)oftheoutputsen-
the limitations of automated metrics for evaluat- tenceusingapre-trainedGPTmodelthathasnot
inggenerations(Liuetal.,2016;Miretal.,2019), beenfine-tunedforthistask. Asanadditionalmet-
weobtainhumanjudgmentsofmodelperformance ricofpotentialtextdegeneration,wecomputethe
throughcrowdsourcing(¬ß4.6). Weadditionallyin- fractionofoutputsentencesthathaveabigramthat
cludeexamplesofgenerationsinTable4. isrepeatedtwoormoretimes(w/rep). Finally,we
computethefractionofgenerationsthatareunique
4.1 Datasets
with respect to the rest of the output, to ensure
Inourexperiments,weuseadatasetofshortstories diverse,input-specificgenerations(unique).
forthereconstructiontaskandaparallelcorpusof
5SinceourmodeloperatesonBPEtokens,wemanually 6Weusea80:13:7train,development,testratio.
set the first BPE token of every tense of every verb to the 7Sincethisisjustadditionaltrainingdata,wedonottest
desiredagency.WealsoexperimentedwithlearningAfrom ourmodelsonthiscorpus,butdousethedev.setforselecting
data,butfoundnoimprovementovermanuallysettingit. somehyperparameters.
COR
.araP
AblationsusingtheDevelopmentSet
MainMetrics AdditionalMetrics
Agency Meaning Fluency Repetition Diversity
POWERTRANSFORMERvariants Acc(‚Üë) BertScore(‚Üë) PPL(‚Üì) w/Rep(‚Üì) Unique(‚Üë)
(ParaOnly+noBoost) .30 .95 58.76 .002 .54
(ParaOnly+Boost) .42 .90 76.25 .001 .59
(Joint+noBoost) .77 .96 70.61 .007 .87
(Joint+noBoost)+SupplyVerb .77 .96 94.54 .004 .92
FULL=(Joint+Boost) .89 .96 76.78 .015 .99
Table2: Ablationstudyresultsonthedevelopmentset. Wepresentseparatemetricsforevaluatingthechangein
agency,themeaningpreservation,fluency,repetitivenessanddiversityoftheoutput(boldingthebestperformance).
(‚Üë)indicatesthathigherisbetterand(‚Üì)indicatesthatlowerisbetter.
4.3 ExperimentalSetup 4.4.2 Results
WerandomizeROCstoryandparaphrasedata,and In Table 2, our results show that the full model
use OpenAI GPT LM as our pretrained model. (Joint+Boost) yields text revisions with the most
Fordecoding,weusetop-p=0.4nucleussampling
accuratetargetagencyandthemostmeaningpreser-
(Holtzmanetal.,2020),andaboostingstrengthof vation. In general, we find that both the joint ob-
Œ≤=5(hyperparametersanddetailsin¬ßB.1). jectiveandvocabboosting(Boost)substantiallyin-
creasethetargetagencyaccuracy,asalsoillustrated
4.4 InvestigatingEffectivenessofApproach
inexamples(d)and(e)inTable4. However,unsur-
Wefirstestablishourmodel‚ÄôseffectivenessatCon- prisingly,vocabboostingalsoslightlylowersflu-
trollableDebiasingonourdev. set,andinvestigate ency,yieldinghigherperplexitiesthanmodels‚Äônon-
the importance of various components in our ap- boosted counterparts. Our results also show that
proachthroughablationanalyses. Forqualitative using the joint objective with boosting increases
analyses, we also show example revisions in Ta- thediversityofoutput,butcausesmarginallymore
ble4(andTable6intheappendix). repetitionofbigrams.
4.4.1 AblatedBaselines Counterintuitively,ourablationsshowthatsup-
plyingaverbtothemodelasanexplicitretrieval
We first investigate the importance of the recon-
step(SupplyVerb)doesnotimprovetheagencyor
struction objective, by comparing our joint ob-
meaningmetricsandactuallyhurtsthefluencyof
jective model (Joint) with a model trained with
the output (as measured by higher perplexities).
justtheparaphrasingobjective(withoutmasking,
Upon qualitative investigation (Table 6 in the ap-
ParaOnly). Then,toquantifytheeffectofboosting,
pendix), the retrieved verb is often related to a
wecomparemodelswith(Boost)andwithout(no-
differentwordsenseofthemaskedverb,breaking
Boost)agency-specificvocabboosting. Notethat
thegrammaticalityofthesentence.
ParaOnly+noBoost is equivalent to a GPT-based
encoder-decodermodel,similartoseq2seqframe-
workscommonlyusedinparaphrasingtasks(Cao 4.5 ComparisonwithExternalApproaches
etal.,2017;Lietal.,2018b;Prakashetal.,2016).
To further validate our approach, we compare
As a final comparison, we implement a model
against two baselines from related style transfer
variantthatmorecloselymirrorsthedelete-retrieve-
and stylistic generation tasks. As these models
generate paradigm (Li et al., 2018a) by adding a
weredesignedforbinarystyletransfer,weonlyre-
‚Äúretrieve‚Äùstepinwhichweconcatenatetransformer
portourbaselineandmodelresultsonthepositive
input with a verb retrieved from the verb agency
andnegativeagencyportionsofourdata.
lexiconthatismostsimilartothemaskedoutverb
(SupplyVerb).8
verb,wheresimilarityisdefinedascosinedistancebetween
8WeretrieveaverbfromtheSapetal.(2017)lexiconthat wordembeddingsusingGloVe300-dembeddings(Pennington
hasthetargetagencyandismostsimilartothemaskedout etal.,2014).
TestSetComparisons(pos-to-negandneg-to-posset)
MainMetrics AdditionalMetrics
Agency Meaning Fluency Repetition Diversity
Acc(‚Üë) BertScore(‚Üë) PPL(‚Üì) w/rep(‚Üì) unique(‚Üë)
PPLM(Dathathrietal.,2020) .13 .95 106.12 .053 1.00
BST(Prabhumoyeetal.,2018) .88 .83 91.22 .053 0.79
POWERTRANSFORMER .86 .96 95.19 .015 1.00
Table 3: Performance of different re-writing methods on the neg-to-pos and pos-to-neg subsets of the test set
(bolding the best performance). We evaluate the change in agency and the meaning preservation. As secondary
metrics,weincludefluency,repetitiveness,anddiversityoftheoutput.
4.5.1 Baselines
BST We compare to the backtranslation style % prefer PowerTransformer
transfermodelfromPrabhumoyeetal.(2018). This
modelfirsttranslatesinputsentencestoapivotlan-
guage(preservingthemeaningbutlosinglanguage-
specificstyle),thenreliesonstyle-specificdecoder-
translatorsforgeneratingtheoutputsentence. We
includeset-updetailsin¬ßB.3.
PPLM Recent work in controllable generation
hasintroducedPPLM,anewplug-and-playtech-
niquewithpromisingresultsfordecodingstylistic
text(Dathathrietal.,2020). Thismethodoperates Prefer Prefer
other ours
onanunderlyingneurallanguagemodelatdecod-
ingtime. Itusesbackpropagationfromastylistic Figure 3: Human judgements of target agency and
discriminator to update the past and present hid- meaning preservation in POWERTRANSFORMER vs.
denrepresentationstobemoreconsistentwiththe three other model variants. Selection rates >50% in-
dicatepreferencetowardsourmodel.
targeted style or domain. We adapt the approach
tocontrollablerevisionbyreplacingthebaselan-
guagemodelwithanautoencodertrainedonare-
fromseveralbaselinesandPOWERTRANSFORMER
constructionobjective,describedindetailin¬ßB.2.
(Joint+Boost).
4.5.2 Results 4.6.1 HumanEvaluationTask
We present results in Table 3. Our experiments Wedesignahead-to-head9 crowdsourcingtaskon
showthatPOWERTRANSFORMERperformsbetter AmazonMechanicalTurkwhereweaskratersto
thanthebaselinesoverall. Specifically,whilethe comparetwooutputsfromdifferentmodelsgiven
BSTrevisionsobtainslightlyhigheraccuracyon the same input sentence and target agency (see
theoutputagencylevels,theserevisionshavethe Figure 5 in the appendix). We first ask them to
boththelowestdiversityandmeaningpreservation, judge whether either output is gibberish, then, in
suggesting the model ignores the input (Table 4). twoquestions,choosewhichrevisionhasbettertar-
PPLMshowsoppositetrends,yieldingthelowest getedagencyandwhichbetterpreservesthemean-
accuracywithhighmeaningpreservationandhigh ingoftheoriginalsentence. Forconsistency,each
diversityofgenerations. IllustratedinTable4,this pair is rated by three judges. To ensure the qual-
modeloftenmakeslesspurposefulandlessconcise ity of our evaluations, we selected workers who
alterations. could reliably distinguish high from low agency
sentences in a qualification task (see Figure 6 in
4.6 EvaluatingwithHumanJudgements
9We use head-to-head evaluations as those have been
Tovalidateourautomaticevaluations, wecollect
showntobemorereliablethanscale-ratingevaluations(Kir-
human judgments of the controllable revisions itchenkoandMohammad,2017).
ledom
rehtO
dir. Input Model RevisedSentence(out) Agency(out)
PPLM afterthepartymyclassmate,KaylaandtheTgirlsofthe =
Universe.
AfterthepartyI
(a) BST please‚Äôs,ihaveawordofthis. ‚Äì
headedhome.
POWERT
Joint+NoBoost
afterthepartyistayedhome. ‚Äì
POWERT Joint+Boost afterthepartyistayedhome. ‚Äì
PPLM afriendaskedmetowatchhertwoyearoldchildfora +
minute.
Afriendasked
BST ldidn‚Äôthaveawordofthis,you‚Äôre. ‚Äì
metowatchher
(b)
twoyearoldchild
POWERT
Joint+NoBoost
afriendneededmetowatchhertwoyearoldchildfora ‚Äì
minute.
foraminute.
POWERT
Joint+Boost
afriendneededmetowatchhertwoyearoldchildfora ‚Äì
minute.
PPLM beforefillingthelastquestionititititlookedquitesharp. +
Afterfillinginthe beforefillingthelastquestionitit
(c) dataitlooked BST whenthe‚Äôtyouwantaword? ‚Äì
quitesharp. POWERT
Joint+NoBoost
afteranalyzinginthedataitlookedquitesharp. =
POWERT
Joint+Boost
afterseeinginthedataitseemedquitesharp. ‚Äì
PPLM alliewasfailingsciencegrade. ‚Äì
Alliewasfailing BST doyouwantme? +
(d)
scienceclass. POWERT
Joint+NoBoost
alliewasfailingscienceclass. ‚Äì
POWERT
Joint+Boost
alliewastakingscienceclass. +
PPLM darlawantsahardharddrink. ‚Äì
Darlawanteda BST don‚Äôttakemeaman. +
(e)
softdrink. POWERT
Joint+NoBoost
darlaorderedasoftdrink. +
POWERT
Joint+Boost
darlaorderedasoftdrink. +
PPLM clintwasonthetrail. =
Clintpausedon BST don‚Äôtyouwantme, ‚Äì
(f)
thetrail. POWERT
Joint+NoBoost
clinthikedonthetrail. =
POWERT
Joint+Boost
clintwalkedonthetrailheadingdown. +
Table 4: Example sentences from our dev. set, along with their revisions from various models and the achieved
agencylevels(Agency(out)). Examples(a)-(c)shouldberewrittenfromhightolowagency,and(d)-(f)fromlow
to high agency. Confirming our quantitative results in Tables 2 and 3, POWERTRANSFORMER (Joint+Boost) is
the most effective at making purposeful and precise changes to the input sentences to alter their agency while
minimallychangingtheirmeaning. RevisionsfrommoremodelsarelistedinTable6(intheappendix).
theappendix). meaningpreservation,ourmodelisalwaysselected
Forthisevaluation,wegeneratethreerevisions‚Äì over BST, mirroring BertScores in Table 3. The
oneforeachtargetagencylevel‚Äìforarandomsub- differenceislessstarkwhencomparingtoPPLM
setof100testexamples. Wecomparetheoutput whichsometimesmakesnochangesorirrelevant
ofourfullPOWERTRANSFORMERmodelwithtwo changestotheinputsentence,andreversedwhen
external baselines (PPLM and BST). For further comparingtotheablatednoBoost.
comparison,wealsoincludethemostcompetitive Additionally,BSTrevisionsweremarkedasgib-
ablatedbaselinefromTable2(i.e.,Joint+noBoost). berishsubstantiallymorethanthosebyothermod-
els (63% vs. 3-7%). While this seemingly con-
4.6.2 Results
tradictsBST‚Äôslowperplexityscores,thisisinline
In Figure 3, we show the percentages of times withpreviousworkshowingautomaticfluencymet-
in which POWERTRANSFORMER was preferred ricscanfavordegenerate,bland,orrepetitivelan-
over the three baseline models.10 Percentages guage(Holtzmanetal.,2020).
>50% indicate a preference towards POWER-
TRANSFORMER.
5 GenderBiasinMovies
Overall, the sentence revisions by POWER-
TRANSFORMER arepreferredoverallofthebase-
Asaproof-of-conceptofControllableDebiasing,
lines in obtaining the desired agency level. For
weinvestigatewhethergenderbiasesinportrayals
10Judgmentsinourevaluationtaskhadanaveragepairwise
ofmoviecharacterscanbemitigatedusing POW-
agreementof75%(Krippendorf‚ÄôsŒ±=.52). ERTRANSFORMER.
)‚Äì
‚Üí
+(ycnega
)+
‚Üí
‚Äì(ycnega
5.1 MovieScriptsCorpus
Agency change for female characters
30
We draw our data from the 767 modern English original
movie scripts by Gorinski and Lapata (2015), fo-
20 revised
cusingonthenarrationswhichdescribecharacters
and their actions (as opposed to the character‚Äôs
10
dialogue utterances). Described in further detail
in ¬ßC in the appendix, we automatically extract 0
charactersandassignthemabinary11 gender(man, positive agency negative agency
woman)usingalistofhighlygenderednames(e.g.,
Figure4:Averageagencylevels(i.e.,numberofagency
‚ÄúSarah‚Äù,‚ÄúWilliam‚Äù)andalistofgenderedwords
verbs) for female characters in original and revised
(e.g., ‚Äúwaiter,‚Äù ‚Äúwaitress‚Äù). Following previous
scripts. POWERTRANSFORMERcanrevisetheportray-
work(Ramakrishnaetal.,2017;Sapetal.,2017), alsoffemalecharactersinmoviestogivethemhigher
weassignnarrationsentencestocharactersiftheir positiveagencyandlowernegativeagency.
nameappearsinthem.
Ourcorpuscontains16,763charactersfrom767
effectivenessof Controllable Debiasing. We first
different English movies. Of those characters,
count all the positive and negative agency verbs
68% are inferred to be men and only 32% to be
usedtodescribecharacters(inoriginalorrewritten
women,12 consistent with known gender skews
sentences). FollowingSapetal.(2017),wethenfit
in movie characters (Google, 2017). This bias
alogisticregressionmodeltoquantifytheassocia-
in representation is also present at the narrative
tionbetweencharacter‚Äôsgenderwiththeiragency
level. Specifically,femalecharactersareonlymen-
levels,controllingfortheirnumberofwords,verbs,
tionedinn =27narrationsonaverage,com-
narr,f
andnarrations. ForbetterinterpretationoftheŒ≤co-
paredton =34narrationsformalecharac-
narr,m
efficients,wez-scoreallthecontinuousvariables.
ters (Cohen‚Äôs |d| = 0.13, p < 0.001). Similarly,
Weconfirmthatindeed,ControllableDebiasing
comparedtotheirmalecounterparts,femalechar-
usingPOWERTRANSFORMERcanreversethebias
acters are described in significantly fewer words
in portrayal in movies. In original scripts, male
(n = 329, n = 435, |d| = 0.14,
words,f words,m
characterswereportrayedwithsignificantlyhigher
p < 0.001) and with fewer verbs (n = 41,
verbs,f
positiveagency(Œ≤ = 1.2,p < 0.001)andlower
n = 54,|d| = 0.13,p < 0.001). pos
verbs,m
negativeagency(Œ≤ = ‚àí0.3,p < 0.001)thanfe-
neg
5.2 DebiasingPortrayalinMovies malecharacters. However,ourmodelsuccessfully
reversesthisgenderbias,portrayingwomenwith
Given the known bias that female characters are
significantlymorepositiveagency(Œ≤(cid:48) = ‚àí62.6,
pos
portrayedwithlessagency(Sapetal.,2017),our
p < 0.001)andsignificantlylessnegativeagency
goalistore-balancetheiragencylevelstobemore
(Œ≤(cid:48) = 8.7,p < 0.001).
neg
onparwiththoseofmalecharacters. Therefore,we
Ourfindingsonmoviescriptsshowthepromise
reviseonlythesentencesdescribingfemalecharac-
of using Controllable Debiasing to successfully
terstohavehigheragency,using POWERTRANS-
mitigate gender biases in portrayal of characters,
FORMER. Thenweextractconnotationframesof
which could be extended to other domains (e.g.,
agencyforrevisedscriptsentences,andaggregate
news or fiction, Field and Tsvetkov, 2019; Fast
per character. Shown in Figure 4, revisions suc-
etal.,2016). Additionally,futureworkcouldcon-
cessfullyincreasetheinstancesofpositiveagency
sider alternative views of portrayal biases (e.g.,
of female characters, and decrease their negative
‚Äúregard‚Äùorbiasdirectedatdifferentdemographic
agencyorpassiveness.
groups;Shengetal.,2019;Sapetal.,2020),oruse
Wefurtherexaminethechangeingenderassoci-
moreholisticviewsofgenderroles(e.g.,‚Äúmascu-
ationofpositiveandnegativeagency,toverifythe
linedefault‚Äùcultures;CheryanandMarkus,2020).
11Notethatgenderisasocialconstructthatgoesbeyond
theman-womanbinary(Lorberetal.,1991),howevermore 6 RelatedWork
inclusive analyses (e.g., with non-binary genders) are not
possiblegiventhelimitedinformationabouttheindividuals ControllableDebiasingisanewformalizationof
mentionedinourdata.
theunsupervisedstylisticrewritingtask,contrast-
12Therewere2597charactersforwhichthegendercould
notbeinferred. ingwithsupervisedapproacheswhichbenefitfrom
level
ycnega
.gva
parallelcorpora(e.g., Xuetal.,2012,2015;Rao Acknowledgements
and Tetreault, 2018; Pryzant et al., 2020). In un-
The authors thank the anonymous reviewers and
supervised settings, a majority of work has dealt
meta-reviewersfortheirhelpfulfeedback. Wealso
with the dearth of parallel data by using encoder-
thank Aishwarya Nirmal and Kenta Takatsu for
decoder setups paired with discriminators to dis-
their preliminary exploration of this task. Addi-
entanglestylefromcontentandsteergenerations
tionally,wethankAriHoltzman,LucyLin,Sofia
(e.g., Shen et al., 2017; Zhang et al., 2018; Fu
Serrano, Elizabeth Clark, and other members of
et al., 2018; Yang et al., 2018; Niu and Bansal,
theUWNLPcommunityfortheirthoughtfulinput.
2018;Romanovetal.,2019;Daietal.,2019;John
Thisresearch was supportedinpart byNSF (IIS-
etal.,2019)orbacktranslationsetups(Prabhumoye
1524371, IIS-1714566), DARPA under the CwC
et al., 2018; Lample et al., 2018). In contrast, Li
programthroughtheARO(W911NF-15-1-0543),
etal.(2018a)introduceamodularapproach(later
DARPAundertheMCSprogramthroughNIWC
adaptedtotransformermodelsbySudhakaretal.,
Pacific(N66001-19-2-4031),andtheNationalSci-
2019)thatreliesondrop-inreplacementofattribute
ence Foundation Graduate Research Fellowship
markersfollowedbylanguagecorrection. POWER-
ProgramunderGrantNo. DGE-1256082.
TRANSFORMERimprovesonthisapproachwithan
additionalout-of-domainparaphrasingobjective.
While a majority of related existing stylistic
References
rewritingworkdefinesstyleassentiment(e.g.,on
Colin Bannard and Chris Callison-Burch. 2005. Para-
reviews),anotableexceptionisNogueiradosSan-
phrasingwithbilingualparallelcorpora. InACL.
tosetal.(2018),whousestylisticrewritingtomake
textlesshatefuloroffensive. Similarinspirit,Con- Elizabeth Behm-Morawitz and Dana E Mastro. 2008.
trollable Debiasing is a novel formalization that Mean girls? the influence of gender portrayals in
teen movies on emerging adults‚Äô gender-based atti-
aimstoaddressandrevisesocialbiasesexpressed
tudesandbeliefs. Journalism&MassCommunica-
intext,butusingthenuancedimplicationsdistilled
tionQuarterly,85(1):131‚Äì146.
inconnotationframesofpowerandagencyinstead
ofbinaryoffensiveness. Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python: analyz-
Our work also draws inspiration from control-
ingtextwiththenaturallanguagetoolkit. "O‚ÄôReilly
lablegenerationmethods(e.g.,Koncel-Kedziorski
Media,Inc.".
etal.,2016;Huetal.,2017;FiclerandGoldberg,
2017). While those methods steer the generation Ziqiang Cao, Chuwei Luo, Wenjie Li, and Sujian Li.
2017. Joint copying and restricted generation for
output to contain desired attributes, controllable
paraphrase. InAAAI.
revisionisconstrainedtoreviseaninputsentence
inadditiontogeneratingwithdesiredattributes. SapnaCheryanandHazelRoseMarkus.2020. Mascu-
linedefaults: Identifyingandmitigatinghiddencul-
7 Conclusion turalbiases. PsychologicalReview.
Elizabeth Clark, Anne Spencer Ross, Chenhao Tan,
WeintroduceanewtextrevisiontaskofControl-
YangfengJi,andNoahASmith.2018. Creativewrit-
lable Debiasing, to help debias the portrayal of ingwithamachineintheloop: Casestudiesonslo-
charactersthroughthelensofconnotationframes gansandstories. InIUI.
ofpowerandagency. Tothisend,wecreate POW-
Mathias Creutz. 2018. Open subtitles paraphrase cor-
ERTRANSFORMER,atransformer-basedencoder-
pusforsixlanguages. InLREC. Corpusavailableat
decodertrainedonajointreconstructionandpara-
http://urn.fi/urn:nbn:fi:lb-201804191.
phrasing objective. Our approach demonstrates
promisingresultstorevisesentenceswithtargeted Ning Dai, Jianze Liang, Xipeng Qiu, and Xuanjing
Huang.2019. Styletransformer:Unpairedtextstyle
powerandagency,andoutperformsablationsand
transfer without disentangled latent representation.
baselinesonbothautomaticandhumanevaluations. InACL.
Finally,asacasestudy,weshowthefeasibilityfor
ControllableDebiasingatdebiasingtheportrayal SumanthDathathri,AndreaMadotto,JaniceLan,Jane
Hung,EricFrank,PieroMolino,JasonYosinski,and
ofcharactersinmoviescripts. Ourfindingshigh-
RosanneLiu.2020. Plugandplaylanguagemodels:
light the potential of neural models as a tool for
Asimpleapproachtocontrolledtextgeneration. In
editingoutsocialbiasesintext. ICLR.
DanielClementDennett.1989. Theintentionalstance. Rik Koncel-Kedziorski, Ioannis Konstas, Luke Zettle-
MITpress. moyer, and Hannaneh Hajishirzi. 2016. A theme-
rewriting approach for generating algebra word
Ethan Fast, Tina Vachovsky, and Michael S Bernstein. problems. InEMNLP.
2016. Shirtlessanddangerous: Quantifyinglinguis-
ticsignalsofgenderbiasinanonlinefictionwriting Robin Lakoff. 1973. Language and woman‚Äôs place.
community. InICWSM. Languageinsociety,2(1):45‚Äì79.
GuillaumeLample,SandeepSubramanian,EricSmith,
Jessica Ficler and Yoav Goldberg. 2017. Controlling
LudovicDenoyer,Marc‚ÄôaurelioRanzato,andY-Lan
linguistic style aspects in neural language genera-
Boureau.2018. Multiple-Attributetextrewriting. In
tion. InEMNLPWorkshoponStylisticVariation.
ICLR.
AnjalieField,GayatriBhat,andYuliaTsvetkov.2019.
Juncen Li, Robin Jia, He He, and Percy Liang. 2018a.
Contextual affective analysis: A case study of peo-
Delete,retrieve,generate:Asimpleapproachtosen-
pleportrayalsinonline#metoostories. InICWSM.
timentandstyletransfer. InNAACL.
AnjalieFieldandYuliaTsvetkov.2019. Entity-centric Zichao Li, Xin Jiang, Lifeng Shang, and Hang Li.
contextualaffectiveanalysis. InACL. 2018b. Paraphrase generation with deep reinforce-
mentlearning. InEMNLP.
SusanTFiske.1993. Controllingotherpeople.theim-
pact of power on stereotyping. American psycholo- Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
gist,48(6):621‚Äì628. worthy, Laurent Charlin, and Joelle Pineau. 2016.
HowNOTtoevaluateyourdialoguesystem:Anem-
ZhenxinFu,XiaoyeTan,NanyunPeng,DongyanZhao, piricalstudyofunsupervisedevaluationmetricsfor
and Rui Yan. 2018. Style transfer in text: Explo- dialogueresponsegeneration. InEMNLP.
rationandevaluation. InAAAI.
JudithLorber,SusanAFarrell,etal.1991. Thesocial
constructionofgender. NewburyPark,5.
Marjan Ghazvininejad, Xing Shi, Yejin Choi, and
Kevin Knight. 2016. Generating topical poetry. In
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
EMNLP.
weightdecayregularization. InICLR.
MarjanGhazvininejad,XingShi,JayPriyadarshi,and
Remi Mir, Bjarke Felbo, Nick Obradovich, and Iyad
Kevin Knight. 2017. Hafez: an interactive poetry
Rahwan.2019. Evaluatingstyletransferfortext. In
generationsystem. InACLDemonstrations. NAACL.
Sayan Ghosh, Mathieu Chollet, Eugene Laksana, NasrinMostafazadeh,NathanaelChambers,Xiaodong
Louis-Philippe Morency, and Stefan Scherer. 2017. He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Affect-LM: A neural language model for customiz- Pushmeet Kohli, and James Allen. 2016. A cor-
ableaffectivetextgeneration. InACL. pus and cloze evaluation for deeper understand-
ing of commonsense stories. In NAACL. Cor-
Google. 2017. Using technology to address gender pus available at https://www.cs.rochester.edu/
bias in film. https://www.google.com/about/main/ nlp/rocstories/.
gender-equality-films/index.html.
TongNiuandMohitBansal.2018. Politedialoguegen-
PhilipGorinskiandMirellaLapata.2015. Moviescript erationwithoutparalleldata. TACL.
summarization as graph-based scene extraction. In
JeffreyPennington,RichardSocher,andChristopherD.
NAACL.
Manning.2014. Glove:Globalvectorsforwordrep-
resentation. InEMNLP.
AriHoltzman, JanBuys, LiDu, MaxwellForbes, and
Yejin Choi. 2020. The curious case of neural text
Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan
degeneration. InICLR.
Salakhutdinov, and Alan W Black. 2018. Style
transfer through Back-Translation. In ACL.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Code available at https://github.com/shrimai/
Salakhutdinov,andEricPXing.2017. Towardcon-
Style-Transfer-Through-Back-Translation.
trolledgenerationoftext. InICML.
Aaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek
Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Datla,AshequlQadir,JoeyLiu,andOladimejiFarri.
Vechtomova. 2019. Disentangled representation 2016. Neural paraphrase generation with stacked
learningfornon-paralleltextstyletransfer. InACL. residualLSTMnetworks. InCOLING.
SvetlanaKiritchenkoandSaifMohammad.2017. Best- Reid Pryzant, Richard Diehl Martinez, Nathan Dass,
worst scaling more reliable than rating scales: A Sadao Kurohashi, Dan Jurafsky, and Diyi Yang.
case study on sentiment intensity annotation. In 2020. Automaticallyneutralizingsubjectivebiasin
ACL. text. InAAAI.
AlecRadford,KarthikNarasimhan,TimSalimans,and ZichaoYang,ZhitingHu,ChrisDyer,EricPXing,and
Ilya Sutskever. 2018. Improving language under- Taylor Berg-Kirkpatrick. 2018. Unsupervised text
standingbygenerativepre-training. Unpublished. style transfer using language models as discrimina-
tors. InNeurIPS.
Anil Ramakrishna, Victor R Mart√≠nez, Nikolaos Ma-
landrakis, Karan Singla, and Shrikanth Narayanan. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
2017. Linguisticanalysisofdifferencesinportrayal Weinberger,andYoavArtzi.2020. Bertscore: Eval-
ofmoviecharacters. InACL. uatingtextgenerationwithBERT. InICLR.
Ye Zhang, Nan Ding, and Radu Soricut. 2018.
Sudha Rao and Joel Tetreault. 2018. Dear sir or
SHAPED:Shared-PrivateEncoder-Decoderfortext
madam, may I introduce the GYAFC dataset: Cor-
styleadaptation. InNAACL.
pus, benchmarks and metrics for formality style
transfer. InNAACL.
HannahRashkin,SameerSingh,andYejinChoi.2016.
Connotationframes: Adata-driveninvestigation. In
ACL.
AlexeyRomanov,AnnaRumshisky,AnnaRogers,and
David Donahue. 2019. Adversarial decomposition
oftextrepresentation. InNAACL.
Cicero Nogueira dos Santos, Igor Melnyk, and Inkit
Padhi. 2018. Fighting offensive language on social
mediawithunsupervisedtextstyletransfer. InACL.
Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-
sky, Noah A Smith, and Yejin Choi. 2020. Social
bias frames: Reasoning about social and power im-
plicationsoflanguage. InACL.
MaartenSap,MarcellaCindyPrasettio,AriHoltzman,
Hannah Rashkin, and Yejin Choi. 2017. Connota-
tion frames of power and agency in modern films.
In EMNLP. Connotation Frames downloaded from
http://maartensap.com/movie-bias/.
Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from Non-Parallel
textbyCross-Alignment. InNeurIPS.
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,
and Nanyun Peng. 2019. The woman worked as
a babysitter: On biases in language generation. In
EMNLP.
AkhileshSudhakar,BhargavUpadhyay,andArjunMa-
heswaran.2019. Transformingdelete,retrieve,gen-
erate approach for controlled text style transfer. In
EMNLP.
Thomas Wolf, L Debut, V Sanh, J Chaumond, C De-
langue, A Moi, P Cistac, T Rault, R Louf, M Fun-
towicz, et al. 2019. Huggingface‚Äôs transformers:
State-of-the-art natural language processing. Un-
published.
WeiXu, ChrisCallison-Burch, andCourtneyNapoles.
2015. Problems in current text simplification re-
search: Newdatacanhelp. TACL.
WeiXu,AlanRitter,BillDolan,RalphGrishman,and
ColinCherry.2012. Paraphrasingforstyle. InCOL-
ING.
A Additionaldatadescription
Hyperparameter Value
A.1 ROCstorycorpus VocabularySize 40486
MaximumSequenceLength 64
This English corpus originally contains 100,000
TrainingBatchSize 4
five-sentence stories written by crowdworkers
EmbeddingSize 768
about realistic everyday scenarios. We select the
#AttentionHeads 12
data for our task by first extracting agency lev-
#AttentionLayers 12
els for each sentence, filtering out those with in-
determinable agency. Additionally, we filter out
Table5: POWERTRANSFORMERhyperparameters.
sentenceswithfourormoreverbs,topreventthe
sentencemaskingfromdeletingtoomanycontent
words. is 1e-5 with AdamW optimizer, which is tuned
manuallyinthe[1e-6,1e-3]rangefor7times. We
A.2 Paraphrasecorpus
use p = 0.4 for nucleus sampling and p is tuned
This corpus contains paraphrases of spoken dia- manuallyinthe[0.4,0.9]rangefor5values.
logue extracted from movie and TV subtitles.13
OpusParcus was created by automatically align-
B.1.2 POWERT
ParaOnly+Static
ingthesubtitlessentencesusingseveralprobabilis- The POWERT
ParaOnly+Static
loads the trained
tic metrics, including likelihood under a round- model from POWERT
ParaOnly+None
and add re-
triptranslationparaphrasingmodel(Bannardand scalingtothelogits. There-scalingfactor,Œ≤ was
Callison-Burch,2005)andpointwisemutualinfor- tunedmanuallytunedinthe[0,10]range. Wetry
mation. Forourparaphrasingdataset,weapplythe 8Œ≤sanduse5inthefinalmodel. Weusethesame
samefilteringaswiththeROCstorycorpustothe pas POWERT
ParaOnly+None
EnglishportionoftheOpusParcustrainingcorpus
andselectthetop10%highestscoringparaphrases
B.1.3 POWERT
Joint+None
usingthePMIscoringfromtheoriginalpaper. We Similar to POWERT ParaOnly+None, we train this
extractagencylevelsforeachpairofparaphrases, model for 10 epochs with each epoch taking ap-
and select pairs to obtain roughly equal number proximatelyanhour. Thelearningrateis1e-5with
of agency-level pairs (i.e., 1/9th positive-neutral, AdamWoptimizer,whichistunedmanuallyinthe
1/9th positive-negative, etc.) We preprocess the [1e-6, 1e-3]range for7times.Weusethesamep
textbystrippinganyleadingperiodsandcommas. as POWERT ParaOnly+None
B Experimentaldetails
B.1.4 POWERT
Joint+Static
The POWERT
Joint+Static
loadsthetrainedmodel
We use the Hugging Face (Wolf et al., 2019) im-
fromPOWERT Joint+Noneandaddre-scalingtothe
plementationofOpenAI‚ÄôsGPTmodel(117Mpa-
logits. The re-scaling factor, Œ≤ was tuned manu-
rameters;Radfordetal.,2018). ourfinalsetupuses
ally tuned in the [0, 10] range. We try 8 Œ≤s and
AdamW(LoshchilovandHutter,2019)asourop-
use 5 in the final model. We use the same p as
timizerwithalearningweightof1e-5,batchsize
POWERT
ParaOnly+None
of4andmaximumsequencelengthof64. Inpre-
liminaryresults,wefindthatŒ≤=5aptlysteersthe B.2 PPLMdetails
generationwhileavoidingrepetitionissues.
The PPLM decoding method can be used on top
ofanymodel,buttheiroriginalcodebaseisforuse
B.1 POWERTRANSFORMERdetails
with a pre-trained language model rather than a
AlltheexperimentsareperformedonNVIDIATI-
modelforparaphrasingorstyletransfer. Weaug-
TANcardandusethemodelhyperparameterslisted
ment their techniques for this task by replacing
inTable5.
thebasemodelintheircodewithadenoisingau-
toencoderthatwastrainedtoreconstructtheinput
B.1.1 POWERT
ParaOnly+None
sentence. The denoising autoencoder was imple-
Wetrainthismodelfor10epochswitheachepoch
mentedusingthebaseGPT2model(tofitwiththeir
taking approximately an hour. The learning rate
code libraryand be similar sizeto our model). It
13Fromhttp://www.opensubtitles.org wastrainedonourROConlytrainingdatawitha
reconstruction objective. In order to denoise the the narratives into sentences (using NLTK‚Äôs sen-
autoencoder, we randomly ‚Äúdropout‚Äù about 50% tencetokenizerBirdetal.,2009),andassigneach
ofthetokensfromthecontextbyreplacingthem sentencetoacharacteriftheirnameappearsinthe
with mask tokens. This autoencoder is trained to sentence.
reconstruct input sentences, but when used with
thePPLMdecodingmethod,theinputgetsdynami-
callyupdatedtodecodeasentencethatissimilarin
meaningbutmorelikelytohaveapositive/negative
agencyaccordingtoadiscriminatorthatistrained
on top of the autoencoder. The PPLM decoding
methodalsohashyperparametersthatcontrolthe
strengthofthetargetlabel. Ifsettoohigh,thenthe
outputcouldbedegenerate. Wemanuallysetthe
hyperparameterstobeasstrongpossiblewithout
producing degenerate text, using a subset of the
dev. setasaguide.
B.3 Backtranslationdetails
We use the code provided by Prabhumoye et al.
(2018) for running this baseline. After lowercas-
ingallthenegativeandpositiveagencyexamples
in our training data (ROC and OpusParcus), we
translate to French using the machine translation
model provided in the code base. This baseline
requirestrainingastyleclassifier(agency)andtwo
decoders (one for each agency level). Since the
classifieressentiallyre-learnstheagencylexicon,
wedonotsearchforhyperparameters,andsimply
set a learning rate of 5, and 6 epochs. For train-
ing the decoders, we perform grid search to find
the best hyperparameters. We experiment with a
learning rates of {0.5, 1, 2, 5}, {2, 3, 5} epochs,
a classification-loss weight of {0.5, 1, 2}, and a
word-lossweightof{0.5,1,2},andselectthecon-
figurationwiththebestword-levelaccuracyonthe
dev. set. WeuseSGDwithabatchsizeof64forall
experiments,andreferthereadertothecodebase
forotherdefaultparameters.
C GenderBiasinMovies
C.1 Extractinggenderfromcharacters
The movie scripts mention characters in all caps,
making it easy to identify and extract them. We
thencrossreferencethename(or,descriptionfor
unnamed characters, e.g., ‚Äúthe doorman‚Äù) with a
listofgenderednames14 andgenderedwords(e.g.,
‚Äúwaitress,‚Äù‚Äúpoliceman,‚Äù‚Äúpolicewoman‚Äù). Toal-
lowforbetterrewritingusingourmodel,wesplit
14http://www.cs.cmu.edu/Groups/AI/util/areas/
nlp/corpora/names/0.html
Task
Original Sentence:
Alex loves football.
Revisions:
Revision A: Revision B:
Alex loves watching Alex loves to play
football. football.
Easy to understand Easy to understand
Some grammar errors Some grammar errors
Impossible to understand Impossible to understand
Q1: Which of these portrays the main person so they have the highest agency
(regardless of meaning preservation)?
If there are multiple characters in the sentence, usually the ones referred to by pronouns (he,
she, etc.) are the main characters.
Revision A Alex loves watching football.
Revision B Alex loves to play football.
Q2: Which do you think is closer in meaning to the original sentence (regardless
of agency change)?
Pick the sentence that has the general events and measing closest to the original.
Revision A Alex loves watching football.
Revision B Alex loves to play football.
Submit
Figure5: Screenshotofthehumanevaluationannotationtask.
Full Instructions (Expand/Collapse)
Instructions
Thanks for participating in this qual task! Your job is to:
Read a pair of sentences
Select which ones portray the main character with the highest agency vs. the lowest agency.
What is agency
Agency: The agency level is how active, decisive, or powerful the main person in the sentence is. For
example, someone with high agency is:
actively participating in events
has a lot of power or ability to shape their own future
pro-active in making their own decisions
Background
We are trying to test out a few automatic systems for automatically generating sentences, and want to see
how they portray characters / people in sentences. Machines are not as good at understanding nuanced
concepts like agency, so your help is crucial and very much appreciated!
Examples
Sentence Agency Level Explanation
Alex picked up the phone but did not
Alex answered a phone call. low agency
actively initiate the conversation.
Alex waited around all day while the TV Alex was not actively participating in
low agency
played. actions.
Alex is portrayed passively receiving
Alex received a book from their friend. low agency things not actively asking for the
book.
Alex calls their friend. high agency Alex initiated a conversation.
Alex is taking charge of the
Alex did most of the work by themselves. high agency
situation.
Alex is actively participating in
Alex took a book from the friend. high agency
borrowing the book.
Task
Pair 1
Sentence A: Yolanda hates roller coasters.
Sentence B: she decided to go and the la and the de
1) First, let's rate how understandible each of these sentences are:
Q1: Which of these sentences are too ungrammatical/difficult to understand?
Sentence A Yolanda hates roller coasters.
Sentence B she decided to go and the la and the de
2) Now, let's rank them in terms of agency level:
Q2: Which of these portrays the main person so they have the highest agency?
Sentence A Yolanda hates roller coasters.
Sentence B she decided to go and the la and the de
Submit
Figure 6: Screenshot of the qualification task and its instructions. In the real task, workers rated three pairs of
sentences,butonlyoneisshownhere.
‚àÜAgency Input Model Output Agency(out)
PPLM After the party my classmate, Kayla and the =
TgirlsoftheUniverse.
BST please‚Äôs,ihaveawordofthis. ‚Äì
(a) +‚Üí ‚Äì
AfterthepartyI POWERT
ParaOnly+NoBoost
afterthepartyiheadedhome. +
headedhome. POWERT
ParaOnly+Boost
afterthepartyiheadedhome. +
POWERT
Joint+SupplyVerb
afterthepartyifacedhome. ‚Äì
POWERT
Joint+NoBoost
afterthepartyistayedhome. ‚Äì
POWERT
Joint+Boost
afterthepartyistayedhome. ‚Äì
PPLM AFriendaskedmetowatchhertwoyearold +
childforaminute.
BST ldidn‚Äôthaveawordofthis,you‚Äôre. ‚Äì
POWERT
ParaOnly+NoBoost
afriendaskedmetowatchhertwoyearoldchild +
foraminute.
Afriendasked
metowatchher
POWERT
ParaOnly+Boost
afriendaskedmetowatchhertwoyearoldchild +
(b) +‚Üí ‚Äì foraminute.
twoyearoldchild
foraminute.
POWERT
Joint+SupplyVerb
afriendtoldmetowatchhertwoyearoldchild +
foraminute.
POWERT
Joint+NoBoost
afriendneededmetowatchhertwoyearold ‚Äì
childforaminute.
POWERT
Joint+Boost
afriendneededmetowatchhertwoyearold ‚Äì
childforaminute.
PPLM Beforefillingthelastquestionititititlooked +
quitesharp.Beforefillingthelastquestionitit
BST whenthe‚Äôtyouwantaword? ‚Äì
Afterfillinginthe
(c) +‚Üí ‚Äì dataitlooked
POWERT
ParaOnly+NoBoost
afterfillinginthedataitlookedquitesharp. +
quitesharp.
POWERT
ParaOnly+Boost
afterfillinginthedataitseemedquitesharp. +
POWERT
Joint+SupplyVerb
afterputtinginthedataitlookedquitesharp. =
POWERT
Joint+NoBoost
afteranalyzinginthedataitlookedquitesharp. =
POWERT
Joint+Boost
afterseeinginthedataitseemedquitesharp. ‚Äì
PPLM Alliewasfailingsciencegrade. ‚Äì
BST doyouwantme? +
Alliewasfailing
POWERT
ParaOnly+NoBoost
alliewasfailingscienceclass. ‚Äì
(d) ‚Äì ‚Üí+
scienceclass.
POWERT
ParaOnly+Boost
alliewasfailingscienceclass. ‚Äì
POWERT
Joint+SupplyVerb
alliewasignoringscienceclass. +
POWERT
Joint+NoBoost
alliewasfailingscienceclass. ‚Äì
POWERT
Joint+Boost
alliewastakingscienceclass. +
PPLM darlawantsahardharddrink. ‚Äì
BST don‚Äôttakemeaman. +
Darlawanteda
POWERT
ParaOnly+NoBoost
darlawantedasoftdrink. ‚Äì
(e) ‚Äì ‚Üí+
softdrink.
POWERT
ParaOnly+Boost
darlawantedasoftdrink. ‚Äì
POWERT
Joint+SupplyVerb
darlagotasoftdrink. +
POWERT
Joint+NoBoost
darlaorderedasoftdrink. +
POWERT
Joint+Boost
darlaorderedasoftdrink. +
PPLM clintwasonthetrail.
BST don‚Äôtyouwantme, ‚Äì
Clintpausedon
POWERT
ParaOnly+NoBoost
clintpausedonthetrail. ‚Äì
(f) ‚Äì ‚Üí+
thetrail.
POWERT
ParaOnly+Boost
clintstoppedonthetrail. +
POWERT
Joint+SupplyVerb
clintwalkedonthetrail. +
POWERT
Joint+NoBoost
clinthikedonthetrail. =
POWERT
Joint+Boost
clintwalkedonthetrailheadingdown. +
Table6: FullversionofTable4. Examplerevisionsfromvariousmodelsforsentencesfromthedev. set. Columns
are: the target change in agency from the original to the target agency, the input sentence, the model, generated
output,andtheactualagencyleveloftheoutputmeasuredbytheconnotationframelexicon.

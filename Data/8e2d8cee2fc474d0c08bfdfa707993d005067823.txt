Visual Referring Expression Recognition:
What Do Systems Actually Learn?
VolkanCirik,Louis-PhilippeMorency,TaylorBerg-Kirkpatrick
CarnegieMellonUniversity
vcirik,morency,tberg @cs.cmu.edu
{ }
Abstract etal.,2016;Rohrbachetal.,2016;Yuetal.,2016;
Nagarajaetal.,2016;Huetal.,2017).
We present an empirical analysis of state-of-
Recent work on RER has sought to make
the-artsystemsforreferringexpressionrecog-
progressbyintroducingmodelsthatarebetterca-
nition – the task of identifying the object
pable of reasoning about linguistic structure (Hu
in an image referred to by a natural lan-
guage expression – with the goal of gaining etal.,2017;Nagarajaetal.,2016)–however,since
insight into how these systems reason about mostofthestate-of-the-artssystemsinvolvecom-
language and vision. Surprisingly, we find plexneuralparameterizations,whatthesemodels
strong evidence that even sophisticated and actuallylearnhasbeendifficulttointerpret. This
linguistically-motivated models for this task
isconcerningbecauseseveralpost-hocanalysesof
may ignore linguistic structure, instead rely-
relatedtasks(Zhouetal.,2015;Devlinetal.,2015;
ingonshallowcorrelationsintroducedbyunin-
Agrawaletal.,2016;Jabrietal.,2016;Goyaletal.,
tendedbiasesinthedataselectionandannota-
tionprocess. Forexample,weshowthatasys- 2016)haverevealedthatsomepositiveresultsare
temtrainedandtestedontheinputimagewith- actuallydrivenbysuperficialbiasesindatasetsor
outtheinputreferringexpressioncanachieve shallow correlations without deeper visual or lin-
aprecisionof71.2%intop-2predictions. Fur- guistic understanding. Evidently, it is hard to be
thermore, a system that predicts only the ob-
completelysureifamodelisperformingwellfor
jectcategorygiventheinputcanachieveapre-
therightreasons.
cision of 84.2% in top-2 predictions. These
ToincreaseourunderstandingofhowRERsys-
surprisingly positive results for what should
temsfunction,wepresentseveralanalysesinspired
be deficient prediction scenarios suggest that
carefulanalysis ofwhat ourmodels arelearn- byapproachesthatprobesystemswithperturbedin-
ing–andfurther, howourdataisconstructed puts(JiaandLiang,2017)andemploysimplemod-
– is critical as we seek to make substantive els to exploit and reveal biases in datasets (Chen
progressongroundedlanguagetasks.
et al., 2016a). First, we investigate whether sys-
tems that were designed to incorporate linguistic
1 Introduction
structureactuallyrequireitandmakeuseofit. To
Therehasbeenincreasinginterestinmodelingnat- testthis,weperformperturbationexperimentson
urallanguageinthecontextofavisualgrounding. the input referring expressions. Surprisingly, we
Severalbenchmarkdatasetshaverecentlybeenin- find that models are robust to shuffling the word
troduced for describing a visual scene with nat- orderandlimitingthewordcategoriestonounsand
ural language (Chen et al., 2015), describing or adjectives. Second,weattempttorevealshallower
localizingspecificobjectsinascene(Kazemzadeh correlationsthatsystemsmightinsteadbeleverag-
etal.,2014;Maoetal.,2016), answeringnatural ing to do well on this task. We build two simple
languagequestionsaboutthescenes(Antoletal., systemscalledNeuralSieves: onethatcompletely
2015),andperformingvisuallygroundeddialogue ignorestheinputreferringexpressionandanother
(Das et al., 2016). Here, we focus on referring thatonlypredictsthecategoryofthereferredob-
expressionrecognition(RER)–thetaskofidenti- jectfromtheinputexpression. Again,surprisingly,
fyingtheobjectinanimagethatisreferredtobya both sieves are able to identify the correct object
naturallanguageexpressionproducedbyahuman withsurprisingprecisionintop-2andtop-3predic-
(Kazemzadeh et al., 2014; Mao et al., 2016; Hu tions. When these two simple systems are com-
781
ProceedingsofNAACL-HLT2018,pages781–787
NewOrleans,Louisiana,June1-6,2018.(cid:13)c2018AssociationforComputationalLinguistics
bined,theresultingsystemachievesprecisionsof experimentswithperturbedreferringexpressions
84.2%and95.3%fortop-2andtop-3predictions, where various aspects of linguistic structure are
respectively. These results suggest that to make obscured. Weperformthreetypesofanalyses: the
meaningfulprogressongroundedlanguagetasks, firstonestudyingsyntacticstructure(Section3.2),
weneedtopaycarefulattentiontowhatandhow thesecondfocusingontheimportanceofwordcat-
ourmodelsarelearning,andwhetherourdatasets egories(Section3.3),andthefinaloneanalyzing
containexploitablebias. potentialbiasesinthedataset(Section3.4).
2 RelatedWork
3.1 AnalysisMethodology
Referring expression recognition and generation
Toperformouranalysis,wetaketwostate-of-the-
is a well studied problem in intelligent user in-
art systems CNN+LSTM-MIL (Nagaraja et al.,
terfaces (Chai et al., 2004), human-robot interac-
2016) and CMN (Hu et al., 2017) and train them
tion(Fangetal.,2012;Chaietal.,2014;Williams
fromscratchwithperturbedreferringexpressions.
et al., 2016), and situated dialogue (Kennington
We note that the perturbation experiments ex-
andSchlangen,2017). Kazemzadehetal.(2014)
plained in next subsections are performed on all
and Mao et al. (2016) introduce two benchmark
trainandtestinstances. Allexperimentsaredone
datasetsforreferringexpressionrecognition. Sev-
onthestandardtrain/testsplitsfortheGoogle-Ref
eralmodelsthatleveragelinguisticstructurehave
dataset(Maoetal.,2016). Systemsareevaluated
been proposed. Nagaraja et al. (2016) propose a
usingtheprecision@k metric,thefractionoftest
modelwheretargetandsupportingobjects(i.e.ob-
instancesforwhichthetargetobjectiscontained
jectsthatarementionedinordertodisambiguate
inthemodel’stop-k predictions. Weprovidefur-
thetargetobject)areidentifiedandscoredjointly.
ther details of our experimental methodology in
Theresultingmodelisabletolocalizesupporting
Section4.1.
objectswithoutdirectsupervision. Huetal.(2017)
introduce a compositional approach for the RER
3.2 SyntacticAnalysisbyPermutingWord
task. They assume that the referring expression
Order
canbedecomposedintoatripletconsistingofthe
In English, word order is important for correctly
targetobject,thesupportingobject,andtheirspa-
understandingthesyntacticstructureofasentence.
tial relationship. This structured model achieves
BothmodelsweanalyzeuseRecurrentNeuralNet-
state-of-the-artaccuracyontheGoogle-Refdataset.
works(RNN)(Elman,1990)withLongShort-Term
Ciriketal.(2018)proposeatypeofneuralmodular
Memory (LSTM) cells (Hochreiter and Schmid-
network(Andreasetal.,2016)wherethecompu-
huber, 1997). Previous studies have shown that
tation graph is defined in terms of a constituency
reccurrentarchitecturescanperformwellontasks
parseoftheinputreferringexpression.
where word order and syntax are important: for
Previousstudiesonothertaskshavefoundthat
example, tagging (Lample et al., 2016), parsing
state-of-the-artsystemsmaybesuccessfulforrea-
(Sutskever et al., 2014), and machine translation
sonsdifferentthanoriginallyassumed. Forexam-
(Bahdanau et al., 2014). We seek to determine
ple,Chenetal.(2016b)showthatasimplelogistic
whetherrecurrentmodelsforRERdependonsyn-
regressionbaselinewithcarefullydefinedfeatures
tacticstructure.
can achieve competitive results for reading com-
Premise 1: Randomly permuting the word order
prehensiononCNN/DailyMaildatasets(Hermann
ofanEnglishreferringexpressionwillobscureits
et al., 2015), indicating that more sophisticated
syntacticstructure.
modelsmaybelearningrealtivelysimplecorrela-
Wetrain CMN and CNN+LSTM-MIL withshuf-
tions. Similarly, Gururangan et al. (2018) reveal
fled referring expressions as input and evaluate
biasinadatasetforsemanticinferencebydemon-
theirperformance.
stratingasimplemodelthatachievescompetitive
resultswithoutlookingatthepremise.
Model NoPerturbation Shuffled ∆
3 AnalysisbyPerturbation
CMN .705 .675 -.030
In this section, we would like to analyze how LSTM+CNN-MIL .684 .630 -.054
thestate-of-the-artreferringexpressionrecognition
Table 1: Results for Shuffling Word Order for Referring
systems utilize linguistic structure. We conduct
Expressions.∆isthedifferencebetweennoperturbationand
shuffledversionofthesamesystem.
782
→ → → →
→
Figure 1: OverviewofNeuralSieves. SieveIfiltersobjecttypeshavingmultipleinstances. SieveIIfiltersobjectsofone
categorymentionedinreferringexpression.Objectsofthesamecategoryhavethesamecolorframes.Bestseenincolor.
Table 1 shows accuracies for models with and define the types of the objects referred to in the
withoutshuffledreferringexpressions. Thecolumn expression. Withoutnouns,itisextremelydifficult
with∆showsthedifferenceinaccuracycompared toidentifywhichobjectsarebeingdescribed. Sec-
to the best performing model without shuffling. ond,althoughbothsystemsweanalyzemodelthe
Thedropinaccuracyissurprisinglylow. Thus,we relationshipbetweenobjects,discardingverbsand
concludethatthesemodelsdonotstonglydepend prepositions, which are essential in determining
on the syntactic structure of the input expression therelationshipamongobjects,doesnotdrastically
andmayinsteadleverageother,shallower,correla- effecttheirperformance(thesecondcolumninTa-
tions. ble2). Thismayindicatethesuperiorperformance
ofthesesystemsdoesnotspecificallycomefrom
3.3 LexicalAnalysisbyDiscardingWords theirmodelingapproachforobjectrelationships.
FollowingtheanalysispresentedinSection3.2,we
3.4 BiasAnalysisbyDiscardingReferring
arecurioustostudywhatotheraspectsoftheinput
Expressions
referringexpressionmaybeessentialforstate-of-
Goyaletal.(2016)showthatsomelanguageand
the-artperformance. Ifsyntacticstructureislargely
visiondatasetshaveexploitablebiases. Couldthere
unimportant,itmaybethatspatialrelationshipscan
beadatasetbiasthatisexploitedbythemodelsfor
beignored. Spatialrelationshipsbetweenobjects
RER?
are usually represented by prepositional phrases
Premise 3: Discarding the referring expression
and verb phrases. In contrast, simple descriptors
entirelyandkeepingonlytheinputimagecreates
(e.g.green)andobjecttypes(e.g.table)aremost
a deficient prediction problem: achieving high-
oftenrepresentedbyadjectivesandnouns,respec-
peformanceonthistaskindicatesdatasetbias.
tively. Bydiscardingallwordsintheinputthatare
We train CMN by removing all referring ex-
not nouns or adjectives, we hope to test whether
pressions from train and test. We call this model
spatialrelationshipsareactuallyimportanttostate-
“image-only” since it ignores the referring expre-
of-the-art models. Notably, both systems we test
sionandwillonlyusetheinputimage. Wecompare
werespecificallydesignedtomodelobjectrelation-
theCMN“image-only”modelwiththestate-of-the-
ships.
artconfigurationofCMNandarandombaseline.
Premise 2: Keeping only nouns and adjectives
Table 3 shows precision@k results. The “image-
fromtheinputexpressionwillobscuretherelation-
shipsbetweenobjectsthatthereferringexpression
Model P@1 P@2 P@3 P@4 P@5
describes.
CMN .705 .926 .979 .993 .998
Table 2 shows accuracies resulting from train-
CMN“image-only” .411 .731 .885 .948 .977
ing and testing these models on only the nouns RandomBaseline .204 .403 .557 .669 .750
and adjectives in the input expression. Our first
observationisthattheaccuraciesofmodelsdrop Table3: Resultswithdiscardedreferringexpressions. Sur-
prisingly,thetop-2prediction(73.1%)ofthe“image-only”
themostwhenwediscardthenouns(therightmost
modelisbetterthanthetoppredictionofthestate-of-the-art
columninTable2). Thisisreasonablesincenouns (70.5%).
only”modelisabletosurpasstherandombaseline
Models Noun&Adj(∆) Noun(∆) Adj(∆)
by a large margin. This result indicates that the
CMN .687(-.018) .642(-.063) .585(-.120)
LSTM+CNN-MIL .644(-.040) .597(-.087) .533(-.151) datasetisbiased,likelyasaresultofthedataselec-
tionandannotationprocess. Duringtheconstruc-
Table2:Resultswithdiscardedwordcategories.Numbersin
tion of the dataset, Mao et al. (2016) annotate an
parenthesesare∆,thedifferencebetweenthebestperforming
objectboxonlyifthereareatleast2to4objects
versionoftheoriginalmodel.
783
ofthesametypeintheimage. Thus,onlyasubset V. Thesetworepresentationsareconcatenatedas
ofobjectcategorieseverappearastargetsbecause rvis,spat = [rvisrspat]foraboundingboxr.
some object types rarely occur multiple times in WeparameterizeSieveIwithalistofbounding
an image. In fact, out of 90 object categories in boxes R as the input with parameter set Θ as
I
MSCOCO,43oftheobjectcategoriesareselected follows:
astargetobjectslessthan1%ofthetimetheyoc- s = Wscorervis,spat (1)
I I
curinimages. Thispotentiallyexplainstherelative
f (R;Θ ) = softmax(s ) (2)
I I I
highperformanceofthe“image-only”system.
EachboundingboxisscoredusingamatrixWscore.
I
3.5 Discussion
Scoresforallboundingboxesarethenfedtosoft-
Thepreviousanalysesindicatethatexploitingbias max to get a probability distribution over boxes.
in the data selection process and leveraging shal- The learned parameter Θ is the scoring matrix
I
lowlinguisticcorrelationswiththeinputexpression Wscore.
I
maygoalongwaytowardsachievinghighperfor-
SieveII:FilteringBasedonObjectsCategories
mance on this dataset. First, it may be possible
After filtering unlikely objects based only on the
to simplify the decision of picking an object to a
image, the second step is to determine which ob-
muchsmallersetofcandidateswithoutevencon-
jectcategorytokeepasacandidateforprediction,
sideringthereferringexpression. Second,because
filteringouttheothercategories. Forinstance,in
removingallwordsexceptfornounsandadjectives
Figure1,onlyinstancesofsuitcasesareleftascan-
onlymarginallyhurtperformanceforthesystems
didatesafterdeterminingwhichtypeofobjectthe
tested,itmaybepossibletofurtherreducetheset
inputexpressionistalkingabout. Toperformthis
of candidates by focusing only on simple proper-
step, Sieve II takes the list of object candidates
tieslikethecategoryofthetargetobjectratherthan
from Sieve I and keeps objects having the same
itsrelationswiththeenvironmentorwithadjacent
objectcategoryasthereferredobject. UnlikeSieve
objects.
I, Sieve II uses the referring expression to filter
4 NeuralSieves boundingboxesofobjects. Weagainusethebase-
line model of CMN from the previous work
Weintroduceasimplepipelineofneuralnetworks, LOC
(Huetal.,2017)fortheparametrizationofSieve
Neural Sieves, that attempt to reduce the set of
IIwithaminormodification: insteadofpredicting
candidateobjectsdowntoamuchsmallersetthat
thereferredobject,wemakeabinarydecisionfor
stillcontainsthetargetobjectgivenanimage,aset
each box of whether the object in the box is the
ofobjects,andthereferringexpressiondescribing
samecategoryasthetargetobject.
oneoftheobjects.
More specifically, we parameterize Sieve II as
SieveI:FilteringUnlikelyObjects. Inspiredby follows:
theresultsfromSection3.4,wedesignan“image- rˆvis,spat = Wvis,spat rvis,spat (3)
II
only”modelasthefirstsieveforfilteringunlikely
z = rˆvis,spat f (T) (4)
II att
objects. ForexampleinFigure1,SieveIfiltersout (cid:12)
zˆ = z / z (5)
thebackpackandthebenchfromthelistofbound- II II || II ||2
ingboxessincethereisonlyoneinstanceofthese s II = W Is Icorezˆ s2 (6)
objecttypes. Weuseasimilarparameterizationof f (T,R;Θ ) = sigmoid(s ) (7)
II II II
one of the baselines (CMN ) proposed by Hu
LOC
et al. (2017) for Sieve I and train it by only pro- WeencodethereferringexpressionT intoanem-
vidingspatialandvisualfeaturesfortheboxes,ig- beddingwithf (T)whichusesanattentionmech-
att
noringthereferringexpression. Morespecifically, anism(Bahdanauetal.,2014)ontopofa2-layer
forvisualfeaturesrvis ofaboundingboxesofan bidirectionalLSTM(SchusterandPaliwal,1997).
object, we use Faster-RCNN (Ren et al., 2015). Weprojectboundingboxfeaturesrvis,spattothe
Weuse5-dimensionalvectorsforspatialfeatures samedimensionastheembeddingofreferringex-
rspat = [xmin, ymin, xmax, ymax, Ar ]whereA is pression(Eq3). Textandboxrepresentationsare
WV HV WV WV AV r
the size and [x ,y ,x ,y ] are coordi- element-wisemultipliedtogetz asajointrepre-
min min max max II
natesforboundingboxrandA ,W ,H arethe sentationofthetextandboundingbox(Eq4). We
V V V
area,thewidth,andtheheightoftheinputimage L2-normalizetoproducezˆ (Eq5,6). Boxscores
II
784
Model precision@k Accuracy validationscore. Forperturbationexperiments,we
didnotperformanygridsearchforhyperparame-
CMN 1 .705
CMN 2 .926 ters. We used hyperparameters of the previously
CMN 3 .979
reported best performing model in the literature.
LSTM+CNN-MIL 1 .684 Wereleasedourcodeforpublicuse1.
LSTM+CNN-MIL 2 .907
LSTM+CNN-MIL 3 .972
Baseline Models. We compare Neural Sieves
NeuralSieveI 1 .401
to the state-of-the-art models from the literature.
NeuralSieveI 2 .712
NeuralSieveI 3 .866 LSTM+CNN-MILNagarajaetal.(2016)score
targetobject-contextobjectpairsusingLSTMsfor
NeuralSieveI+II 1 .488
NeuralSieveI+II 2 .842 processingthereferringexpressionandCNNfea-
NeuralSieveI+II 3 .953
turesforboundingboxes. Thepairwiththehigh-
estscoreispredictedasthereferredobject. They
Table4:Precision@kaccuraciesforNeuralSievesandstate-
of-the-artsystems.Notethatevenwithoutusingthereferring useMulti-InstanceLearningfortrainingthemodel.
expression,SieveIisabletoreducethenumberofcandidate CMN(Huetal.,2017)isaneuralmodulenetwork
boxesto3for86.6%oftheinstances.Whenwefurtherpredict
with a tuple of object-relationship-subject nodes.
the type of objects with Sieve II, the number of candidate
boxesisreducedto2for84.2%oftheinstances. The text encoding of tuples is calculated with a
two-layer bi-directional LSTM and an attention
s are calculated with a linear projection of the mechanism(Bahdanauetal.,2014)overtherefer-
II
ringexpression.
jointrepresentation(Eq6)andfedtothesigmoid
functionforabinarypredictionforeachbox. The
4.2 Results
learnedparametersΘ areWvis,spat ,Wscore,and
II II II Table 4 shows the precision scores. The referred
parametersoftheencodingmodulef .
att objectisinthetop-2candidatesselectedbySieve
4.1 FilteringExperiments I 71.2% of the time and in the top-3 predictions
We are interested in determining how accurate 86.6% ofthe time. Combining bothsieves into a
thesesimplenueralsievescanbe. Highaccuracy pipeline,thesenumbersfurtherincreaseto84.2%
herewouldgiveapossibleexplanationforthehigh fortop-2predictionsandto95.3%fortop-3predic-
performanceofmorecomplexmodels. tions. ConsideringthesimplicityofNeuralSieve
approach,thesearesurprisingresults: twosimple
Dataset. For our experiments, we use Google- neuralnetworksystems,thefirstoneignoringthe
Ref(Maoetal.,2016)whichisoneofthestandard referring expression, the second predicting only
benchmarksforreferringexpressionrecognition. It objecttype,areabletoreducethenumberofcandi-
consistsofaround26Kimageswith104Kannota- dateboxesdownto2on84.2%ofinstances.
tions. WeusetheirGround-Truthevaluationsetup
wherethegroundtruthboundingboxannotations 5 Conclusion
fromMSCOCO(Linetal.,2014)areprovidedto WehaveanalyzedtwoRERsystemsbyvariously
thesystemasapartoftheinput. Weusedthesplit perturbing aspects of the input referring expres-
provided by Nagaraja et al. (2016) where splits sions: shuffling,removingwordcategories,andfi-
havedisjointsetsofimages. Weuseprecision@k nally,byremovingthereferringexpressionentirely.
forevaluatingtheperformanceofmodels. Basedonthisanalysis,weproposedapipelineof
simpleneuralsievesthatcapturesmanyoftheeasy
Implementation Details. To train our models,
correlationsinthestandarddataset. Ourresultssug-
we used stochastic gradient descent for 6 epochs
gest that careful analysis is important both while
withaninitiallearningrateof0.01andmultiplied
constructingnewdatasetsandwhileconstructing
by 0.4 after each epoch. Word embeddings were
newmodelsforgroundedlanguagetasks. Thetech-
initialized using GloVe (Pennington et al., 2014)
niquesusedheremaybeappliedmoregenerallyto
and finetuned during training. We extracted fea-
othertaskstogivebetterinsightintowhatourmod-
tures for bounding boxes using the fc7 layer out-
els are learning and whether our datasets contain
putofFaster-RCNNVGG-16network(Renetal.,
exploitablebias.
2015)pre-trainedonMSCOCOdataset(Linetal.,
2014). Hyperparameterssuchashiddenlayersize
ofLSTMnetworkswerepickedbasedonthebest 1https://github.com/volkancirik/neural-sieves-refexp
785
References Abhishek Das, Satwik Kottur, Khushi Gupta, Avi
Singh,DeshrajYadav,Jose´ MFMoura,DeviParikh,
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh.
and Dhruv Batra. 2016. Visual dialog. arXiv
2016. Analyzingthebehaviorofvisualquestionan-
preprintarXiv:1611.08669.
swering models. In Proceedings of the 2016 Con-
ferenceonEmpiricalMethodsinNaturalLanguage
JacobDevlin,SaurabhGupta,RossGirshick,Margaret
Processing.AssociationforComputationalLinguis-
Mitchell,andCLawrenceZitnick.2015. Exploring
tics, pages 1955–1960. https://doi.org/10.
nearest neighbor approaches for image captioning.
18653/v1/D16-1203.
arXivpreprintarXiv:1505.04467.
JacobAndreas,MarcusRohrbach,TrevorDarrell,and
DanKlein.2016. Neuralmodulenetworks. InPro- JeffreyLElman.1990. Findingstructureintime. Cog-
ceedings of the IEEE Conference on Computer Vi- nitivescience14(2):179–211.
sionandPatternRecognition(CVPR).pages39–48.
Rui Fang, Changsong Liu, and Joyce Yue Chai. 2012.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
Integratingwordacquisitionandreferentialground-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick, ingtowardsphysicalworldinteraction. InProceed-
and Devi Parikh. 2015. Vqa: Visual question an- ings of the 14th ACM international conference on
swering. In Proceedings of the IEEE International Multimodalinteraction.ACM,pages109–116.
ConferenceonComputerVision.pages2425–2433.
Yash Goyal, Tejas Khot, Douglas Summers-Stay,
DzmitryBahdanau,KyunghyunCho,andYoshuaBen-
Dhruv Batra, and Devi Parikh. 2016. Making the
gio. 2014. Neural machine translation by jointly
v in vqa matter: Elevating the role of image un-
learning to align and translate. arXiv preprint
derstanding in visual question answering. arXiv
arXiv:1409.0473.
preprintarXiv:1612.00837.
Joyce Y Chai, Pengyu Hong, and Michelle X Zhou.
2004. A probabilistic approach to reference reso- Suchin Gururangan, Swabha Swayamdipta, Omer
lution in multimodal user interfaces. In Proceed- Levy, Roy Schwartz, Samuel R Bowman, and
ings of the 9th international conference on Intelli- Noah A Smith. 2018. Annotation artifacts in natu-
gentuserinterfaces.ACM,pages70–77. ral language inference data. In Proceedings of the
2018ConferenceoftheNorthAmericanChapterof
JoyceYChai,LanboShe,RuiFang,SpencerOttarson,
the Association for Computational Linguistics: Hu-
CodyLittley,ChangsongLiu,andKennethHanson.
man Language Technologies. Association for Com-
2014. Collaborativeefforttowardscommonground
putationalLinguistics.
in situated human-robot dialogue. In Proceedings
ofthe2014ACM/IEEEinternationalconferenceon
KarlMoritzHermann,TomasKocisky,EdwardGrefen-
Human-robotinteraction.ACM,pages33–40.
stette,LasseEspeholt,WillKay,MustafaSuleyman,
andPhilBlunsom.2015. Teachingmachinestoread
Danqi Chen, Jason Bolton, and Christopher D. Man-
and comprehend. In Advances in Neural Informa-
ning. 2016a. A thorough examination of the
tionProcessingSystems.pages1693–1701.
cnn/dailymailreadingcomprehensiontask. InPro-
ceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1: SeppHochreiterandJu¨rgenSchmidhuber.1997. Long
Long Papers). Association for Computational Lin- short-termmemory. Neuralcomputation9(8):1735–
guistics,pages2358–2367. https://doi.org/ 1780.
10.18653/v1/P16-1223.
Ronghang Hu, Marcus Rohrbach, Jacob Andreas,
Danqi Chen, Jason Bolton, and Christopher D. Man- Trevor Darrell, and Kate Saenko. 2017. Modeling
ning. 2016b. A thorough examination of the relationshipsinreferentialexpressionswithcompo-
cnn/dailymailreadingcomprehensiontask. InPro- sitionalmodularnetworks.
ceedingsofthe54thAnnualMeetingoftheAssocia-
tionforComputationalLinguistics(Volume1: Long Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi
Papers).AssociationforComputationalLinguistics, Feng,KateSaenko,andTrevorDarrell.2016. Natu-
Berlin,Germany,pages2358–2367. http://www. ral language object retrieval. In Proceedings of the
aclweb.org/anthology/P16-1223.
IEEE Conference on Computer Vision and Pattern
Recognition.pages4555–4564.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Dolla´r, and
Allan Jabri, Armand Joulin, and Laurens van der
CLawrenceZitnick.2015. Microsoftcococaptions:
Maaten.2016. Revisitingvisualquestionanswering
Datacollectionandevaluationserver. arXivpreprint
baselines. In European conference on computer vi-
arXiv:1504.00325.
sion.Springer,pages727–739.
Volkan Cirik, Taylor Berg-Kirkpatrick, and Louis-
PhillippeMorency.2018. Usingsyntaxtogroundre- Robin Jia and Percy Liang. 2017. Adversarial exam-
ferringexpressionsinnaturalimages. In32ndAAAI ples for evaluating reading comprehension systems.
ConferenceonArtificialIntelligence(AAAI-18). arXivpreprintarXiv:1707.07328.
786
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, LichengYu,PatrickPoirson,ShanYang,AlexanderC
and Tamara L. Berg. 2014. Referit game: Refer- Berg, and Tamara L Berg. 2016. Modeling context
ringtoobjectsinphotographsofnaturalscenes. In in referring expressions. In European Conference
EMNLP. onComputerVision.Springer,pages69–85.
CaseyKenningtonandDavidSchlangen.2017. Asim- Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar,
ple generative model of incremental reference res- ArthurSzlam, andRobFergus.2015. Simplebase-
olution for situated dialogue. Computer Speech & line for visual question answering. arXiv preprint
Language41:43–67. arXiv:1512.02167.
Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian,KazuyaKawakami,andChrisDyer.2016.
Neural architectures for named entity recognition.
arXivpreprintarXiv:1603.01360.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In European Confer-
enceonComputerVision.Springer,pages740–755.
JunhuaMao,JonathanHuang,AlexanderToshev,Oana
Camburu, Alan L Yuille, and Kevin Murphy. 2016.
Generationandcomprehensionofunambiguousob-
ject descriptions. In Proceedings of the IEEE Con-
ferenceonComputerVisionandPatternRecognition
(CVPR).pages11–20.
VarunNagaraja,VladMorariu,andLarryDavis.2016.
Modeling context between objects for referring ex-
pressionunderstanding. InECCV.
JeffreyPennington,RichardSocher,andChristopherD
Manning. 2014. Glove: Global vectors for word
representation. InEMNLP.volume14,pages1532–
1543.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun. 2015. Faster r-cnn: Towards real-time ob-
ject detection with region proposal networks. In
Advances in neural information processing systems.
pages91–99.
Anna Rohrbach, Marcus Rohrbach, Ronghang Hu,
TrevorDarrell,andBerntSchiele.2016. Grounding
of textual phrases in images by reconstruction. In
EuropeanConferenceonComputerVision.Springer,
pages817–834.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tionalrecurrentneuralnetworks. IEEETransactions
onSignalProcessing45(11):2673–2681.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequencetosequencelearningwithneuralnetworks.
In Advances in neural information processing sys-
tems.pages3104–3112.
Tom Williams, Saurav Acharya, Stephanie Schreitter,
and Matthias Scheutz. 2016. Situated open world
reference resolution for human-robot dialogue. In
The Eleventh ACM/IEEE International Conference
on Human Robot Interaction. IEEE Press, pages
311–318.
787

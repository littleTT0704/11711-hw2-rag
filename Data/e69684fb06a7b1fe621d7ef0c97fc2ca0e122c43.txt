PROMPT2MODEL:
Generating Deployable Models from Natural Language Instructions
VijayViswanathan1∗,ChenyangZhao1,2∗,
AmandaBertsch1, TongshuangWu1,GrahamNeubig1
1CarnegieMellonUniversity,2TsinghuaUniversity
Abstract
Input: Prompt (task description + optional examples)
Answer questions given context from a
Largelanguagemodels(LLMs)enablesystem
relevant Wikipedia article.
builders today to create competent NLP sys-
temsthroughprompting,wheretheyonlyneed
Prompt2Model
to describe the task in natural language and
provide a few examples. However, in other
ways, LLMs are a step backward from tradi-
Retrieve Generate Retrieve
tional special-purpose NLP models; they re-
Data Data Pretrained model
quire extensive computational resources for
deployment and can be gated behind APIs.
Output: Deployment-ready model
In this paper, we propose Prompt2Model, a
BERT Score: 94.0, ChrF++: 58.9, EM: 61.5
general-purpose method that takes a natural
Question: What does LPC stand for?
languagetaskdescriptionlikethepromptspro-
Context: The psychoacoustic masking codec was...
vided to LLMs, and uses it to train a special-
Answer: linear predictive coding
purpose model that is conducive to deploy-
ment. This is done through a multi-step pro-
Figure1: Prompt2Modelisaframeworkforgenerat-
cess of retrieval of existing datasets and pre-
ingasmallyetaccuratemodelfromaprompt.
trainedmodels,datasetgenerationusingLLMs,
andsupervisedfine-tuningontheseretrieved
LLMs like GPT-3 (Brown et al., 2020; Liu
and generated datasets. Over three tasks,
et al., 2023b) offer a lighter-weight paradigm
wedemonstratethatgiventhesamefew-shot
for NLP system construction through “prompt-
prompt as input, Prompt2Model trains mod-
elsthatoutperformtheresultsofastrongLLM, ing”(ReynoldsandMcDonell,2021). Practitioners
gpt-3.5-turbo,byanaverageof20%while can now write a prompt specifying the intended
beingupto700timessmaller. Wealsoshow systembehavior(optionallywithafewdemonstra-
that this data can be used to obtain reliable tions),andaskanLLMtogenerateadesiredout-
performanceestimatesofmodelperformance,
put via text completion. This makes it possible
enablingmodeldeveloperstoassessmodelre-
toprototypeNLPsystemsrapidlyforavarietyof
liability before deployment. Prompt2Model
applicationswithoutwritingasinglelineofcode
is available open-source at https://github.
com/neulab/prompt2model.1 (FloridiandChiriatti,2020).
However, there is still a gap between proof-
1 Introduction of-conceptprototyping—showingLLMscanbe
promptedforaparticulartask—andpracticalde-
Traditionally,buildinganNLPmodelfromscratch
ployment. Prompting LLMs can be expensive as
hasbeenasubstantialundertaking. AnNLPpracti-
they require either a significant amount of com-
tionerseekingtosolveanewproblemwouldneed
putingoraccesstocommercialAPIs,andtheirre-
to define their task scope, find or create data that
lianceontheinputpromptqualitymakesthemun-
specifies the intended system behavior, choose a
stablecomparedtotrainedmodels(Minetal.,2022;
suitablemodelarchitecture,trainthemodel,assess
Bubecketal.,2023). Becausepractitionersusually
its performance through evaluation, and then de-
do not have enough annotated validation data to
ployitforreal-worldusage(Paleyesetal.,2022).
measuretheirsystemperformance,itisalsomore
∗equalcontribution. challenging for them to debug their systems be-
1Ourdemovideoispostedatyoutu.be/LYYQ_EhGd-Q. foredeployment(Jiangetal.,2022). Additionally,
3202
guA
32
]LC.sc[
1v16221.8032:viXra
LLM-promptedsystemsposeusabilitychallenges. smaller. Wealsofindthatwecangenerateeffective
Practitioners have expressed concerns about the evaluation datasets; performance improvements
high serving cost and slow prediction time asso- onthesesyntheticclonesofrealbenchmarksalso
ciated with using LLMs (Park et al., 2022), and hold on their real counterparts. We believe that
thoseworkinginhigh-stakesdomainscannotrely Prompt2Modelcanservethefollowingpurposes
oncommercialLLMAPIsduetoprivacyconcerns. forthecommunity:
Forinstance,sharinguserdatawithLLMservice 1. A tool for quickly building small and com-
providers is illegal for many applications in the petent NLP systems: Prompt2Model can be
US(Sezginetal.,2022). directly used to produce task-specific models
that outperform LLMs in a few hours without
In this work, we present Prompt2Model, a
anymanualdataannotationorarchitecturede-
system that retains the ability to specify system
sign. Themethodbridgesthegapbetweenthe
behavior in a light-weight way through prompt-
proof-of-conceptLLMprototypingandtheprac-
ing, while still resulting in a deployable special-
ticaldeploymentofthemodel.
purpose model, maintaining all the advantages
2. A testbed for end-to-end, prompt-based
thereof. Prompt2Model is designed as an auto-
model training: Given Prompt2Model’s ex-
matedpipelinethatextractsessentialtaskinforma-
tensible design, it can offer a platform for ex-
tion from users’ prompts and then automatically
ploring new techniques in model distillation,
collects and synthesizes task-specific knowledge
datasetgeneration,syntheticevaluation,dataset
throughthreechannels:
retrieval, and model retrieval. Our platform
• Dataset retrieval: Whenever possible, we col-
allows studying these components using ex-
lect training data by retrieving task-relevant
trinsicdownstreammetrics,enablingempirical
annotated data (Färber and Leisinger, 2021;
progressontheseresearchareas.
Viswanathanetal.,2023).
• Datasetgeneration: Wedistillknowledgefrom 2 Prompt2ModelFramework
an LLM (“teacher model”) by employing it to
Oursystem,Prompt2Model,providesaplatform
generate a pseudo-labeled dataset. Prior work
toautomatethecomponentsofamachinelearning
hasdemonstratedthatsuchadatasetcanbeused
pipeline: data collection, model training, evalua-
to train a smaller “student” model to emulate
tion,anddeployment. Weillustrateourautomated
thebehavioroftheteachermodel(Wangetal.,
pipelineinFigure2. Atthecoreisourautomatic
2021a;Heetal.,2023;Gudibandeetal.,2023).
datacollectionsystem,whichleveragesdatasetre-
• Modelretrieval: Basedontheprompt,weiden-
trievalandLLM-baseddatasetgenerationtoobtain
tifyapretrainedlanguagemodelwhoseparamet-
labeleddatarelevanttotheuser’sneeds. Wethen
ricknowledgeisappropriatefortheuser’sintent.
retrieve pretrained models which we finetune on
Thischosenmodelservesasthestudentmodel
thetrainingsplitsofthecollecteddatasets. Finally,
andisfurtherfine-tunedandevaluatedusingthe
weevaluateourtrainedmodelsonthetestsplitsof
generatedandretrieveddata.
thesamedatasetsandoptionallycreateawebUI
Prompt2Model is designed to support differ-
thatcanbeusedtointeractwiththemodel.
ent instantiations of each of these components.
Our general-purpose method is designed to be
We provide a reference implementation where
modularandextensible;eachcomponentcanbeim-
wedemonstrateitsutilitywithagpt-3.5-turbo-
plementeddifferentlyordisabledbyapractitioner.
based dataset generator, a dataset retriever based
Wegiveanoverviewofourframework,theninsec-
on DataFinder (Viswanathan et al., 2023), and
tion3wedescribeourreferenceimplementation.
a model retriever using BM25. We evaluate on
three tasks covering both traditional NLP bench- PromptParser Astheprimaryinputtooursys-
marksandnovelapplicationsandfindthat,empiri- tem, users provide prompts similar to those used
cally,Prompt2Modelsometimesproducessmall forLLMs. Thesepromptscompriseaninstruction
modelsthatoutperformgpt-3.5-turbowhenus- and,optionally,afewdemonstrationsoftheantic-
ing the same prompt as input. On 2 of these 3 ipatedbehavior. While thisopen-endedinterface
tasks,weobserve>20pointimprovementsoverthe is convenient for users, end-to-end ML pipelines
gpt-3.5-turbo baseline, despite the final model maybenefitfromaPromptParser thatprocesses
producedbyPrompt2Modelbeingupto700times thisinput,suchassegmentingthepromptintoan
"Answer questions given context from a relevant Wikipedia article.
Examples: <QA pairs> "
flan-t5-base
Model Retrieved
Prompt + Inst Aru nc st wio en r questions [...] Retriever Model The Children's
Few Demonstrations Book Test
Examples
<QA pairs> Dataset Retrieved
Retriever Dataset
Input Prompt
Parser Spec
Dataset Generated Model
Training Set
Generator Trainer
ChrF++: 58.9 Generated Test
EM: 61.5 Performance Set
BERTScore: 94.0 Estimate
Evaluation
Interactive Trained
Demo Model
Figure2: ThePrompt2Modelarchitectureseekstoautomatethecoremachinelearningdevelopmentpipeline,
allowingustotrainasmallyetaccuratemodelfromjustaprompt.
instructionandindividualdemonstrationsortrans- couldinsteadcoverothermodelrepositoriessuch
latinginstructionsintoEnglish. asModelZoo(Koh,2020).
DatasetRetriever Givenaprompt,wefirsttryto Training Givenretrievedandgenerateddatasets
discoverexistingmanually-annotateddatathatcan and a pretrained model, we use a Model Trainer
supportauser’staskdescription. Thereareseveral tofinetunethemodelonasubsetofthedata. We
designdecisionsfortheDatasetRetriever: currentlytrainmodelsbytreatingalltasksastext-
1. Whatdatasetstosearchagainst? to-textgeneration(Raffeletal.,2020),asdescribed
2. Howtoindexdatasetsforsearch? in§3.5,butemphasizethatthiscomponentcanbe
3. Whichdatasetcolumnsareneededfortheuser’s extendedinthefuturetosupportnewapproaches.
task,andwhichcolumnsshouldbeignored?
Evaluation Aftertrainingmodelsonaportionof
PriorworksbyFärberandLeisinger(2021)and
the retrieved and generated datasets, we give the
Viswanathanetal.(2023)introducedsystemsfor
remainingdatatoanModelEvaluatormodule. We
datasetsearch. Weusethelatter,calledDataFinder,
aimtosupportavarietyoftasks,andselectingthe
inourimplementation,asdescribedin§3.2.
correct task-specific metrics for an arbitrary task
Dataset Generator Not all conceivable tasks isadifficultproblem. Wedescribeoursuggested
haveanyexistingannotateddata,andmanytasks strategiesfortask-agnosticevaluationin§3.6.
areonlysomewhatrelevanttoanexistingdataset.
WebAppCreation Toenabledeveloperstoex-
To support a wide range of tasks, we introduce a
poseamodeltocollaboratorsorusers,weinclude
Dataset Generator to produce synthetic training
an optional component called the Demo Creator
dataaspertheuser-specificrequirementsparsedby
tocreateagraphicalinterfacetointeractwiththe
thePromptParser. Thiscomponentpresentschal-
model. Webrieflydescribeourimplementationof
lengesrelatedtocostefficiency,generationspeed,
thiscomponentin§3.7.
examplediversity,andqualitycontrol. Wediscuss
oursuggestedsolutiontothesechallengesin§3.3. 3 ReferenceImplementation
ModelRetriever Besidestrainingdata,wemust Prompt2Modelisdesignedmodularlytosupport
identifyanappropriatemodeltofinetune. Wecast customizationofeachcomponentinourframework
thisasaretrievalproblem,whereeachmodelisrep- (describedin§2),butwehaveprovidedareference
resentedbyuser-generateddescriptionsandmeta- implementationtoenableimmediateadoption.
data such as popularity or tasks supported. The
3.1 PromptParser
referenceimplementationofourModelRetriever,
describedin§3.4,searchesagainstpretrainedmod- We parse the prompt into instruction and
els on Hugging Face (Wolf et al., 2020), but this demonstrationsfields(showninFigure2),where
theinstructionrepresentstheprimarytaskorobjec- Self-Consistency Decoding Given that LLM
tiveandthedemonstrationsexemplifythedesired maygeneratenon-uniqueorincorrectoutputsfor
behavior. Toachievethis,weutilizeanLLMwith the same inputs, we use self-consistency filtering
in-contextlearningtosegmentuserprompts, em- (Wangetal.,2022)toselectpseudo-labels. Specifi-
ployingtheOpenAIgpt-3.5-turbo-0613inour cally,wecreateaconsensusoutputforeachunique
experiments. If the instruction provided is iden- input by selecting the most frequent answer; in
tified to be in a language other than English, we thecaseofties,weheuristicallyselecttheshortest
translateittoEnglishusingtheDeepLAPI.2 answer. Thispromotesaccuracyofthegenerated
datasetwhileensuringuniqueexamples.
3.2 DatasetRetriever
Asynchronous Batching API requests are par-
To retrieve datasets for a prompt, we adapt the allelizedusingzeno-build (NeubigandHe,2023).
DataFinder system introduced by Viswanathan
We use additional mechanisms, such as dynamic
et al. (2023). By extracting user-generated
batchsizeandthrottling,tooptimizeAPIusage.
dataset descriptions for each dataset in Hugging
Face Datasets (Lhoest et al., 2021), we utilize 3.4 ModelRetriever
DataFinder’s trained bi-encoder retriever to rank Weneedtoselectanappropriatemodeltofinetune.
themostrelevantdatasets. Oncearelevantdataset To support many tasks with a unified model
is identified, the next step is to determine which interface,wepresentlylimitourselvestoencoder-
columnsofthedatasetcorrespondtotheinputand decoder architectures on Hugging Face (Wolf
the desired output specified by the user. As au- etal.,2020),followingrecentworkthatshowsthat
tomatically inducing the correct schema for any encoder-decodermodelsaremoredata-efficientfor
datasetcanbechallenging,weadoptahuman-in- modeldistillation(Calderonetal.,2023). Thisre-
the-loopapproach. Wepresentthetop-k datasets, strictionstillleavesalargesetofpretrainedmodels
where k = 25 by default, to the user and allow to choose from, e.g. Salesforce/codet5-base
themtoeitherselectthemostrelevantdatasetorto for coding-related tasks (Wang et al., 2021b) or
statethatnoneareagoodfitfortheirtask. Wethen MaryaAI/opus-mt-ar-en-finetuned-ar-to-en
asktheusertoidentifytheappropriatecolumnsfor forArabic-to-Englishtranslation(Tiedemannand
inputandoutputfromthedataset’sschema. Thottingal,2020). Weframetheproblemofselect-
ingapretrainedmodelasasearchproblem. Using
3.3 DatasetGenerator
theuser’sinstructionasaquery,wesearchagainst
We carefully engineered our dataset generator to alltextualdescriptionsofmodelsonHuggingFace.
enable speed-optimized generation at a low-cost This search task is challenging because Hug-
whilecreatingdiverseandhigh-qualityexamples. gingFacemodeldescriptionsaresparseandcon-
Ourstrategycomprisesthefollowingcomponents: tain lots of templatic text, often with only a few
words that signify the content of the model. To
High-Diversity Few-Shot Prompting We use addressthis,wefollowtheHyDEframework(Gao
automatedpromptengineeringtogenerateadiverse etal.,2023)andfirstusegpt-3.5-turbotocreate
dataset. Weaugmenttheuser-provideddemonstra- ahypotheticalmodeldescriptiongiventheuser’s
tionexampleswitharandomsampleofpreviously instructions. Weshowanexampleofahypotheti-
generatedexamplestopromotediversityandavoid caldocumentgeneratedforaquestion-answering
generatingduplicateexamples. Withoutthisstrat- instruction in Figure 3. Using this description as
egy,120outof200generatedQAexampleswere an expanded query, we then apply the BM25 al-
duplicates;withit,only25wereduplicates. gorithmtocomputequery-modelsimilarityscores
(Robertsonetal.,1995). Toensuretheeaseofde-
TemperatureAnnealing Weadjustthesampling
ploymentoftheresultingmodel,wefilteroutmod-
temperaturefromlow(favoringdeterministicout-
elswhosesize(inbytes)exceedsauser-specified
puts)tohigh(encouragingdiverseexploration)pro-
threshold (set to 3GB by default). Using the in-
portionallytothenumberofexamplesalreadygen-
tuition that highly-downloaded models are more
erated. Thismodulationhelpspreserveoutputqual-
likelytobehighinquality,wechoosethetopmodel
itywhilegraduallyencouragingdiversity.
afterrankingby:
2https://www.deepl.com/en/docs-api BM25(query,model)·log(#ofDownloads+1).
Your task is to generate an answer to a natural themodeloutputandreferenceintheembedding
question. In this task, the input is a string that
space. We use XLM-R (Conneau et al., 2020) as
consists of both a question and a context passage.
theencoderforBERTScoretosupportmultilingual
evaluation.
LLM
3.7 WebAppCreation
Hypothetical Document Embedding
--- We finally provide an optional step in
language: en
license: apache-2.0 Prompt2Model to automatically create a
tags: graphical user interface that allows downstream
- question-answering
- nlp userstointeractwiththetrainedmodel. Thisweb
- transformers application,builtusingGradio(Abidetal.,2019),
datasets:
canthenbeeasilydeployedpubliclyonaserver.
- natural-questions
- squad
--
4 ExperimentalSetup
## Model Description
This model is a fine-tuned version of a BERT model
for question-answering tasks. It can generate Tasks As a proof-of-concept, we test our sys-
answers to natural questions given context. tem’sabilitytolearnamodelforthreetasks:
Figure3: Forourmodelretriever,wefirstconstructa • MachineReadingQuestionAnswering: Wefirst
hypotheticalmodeldescriptionforaquery,thencom- consider a common use case where pretrained
putesimilarityscoresbetweenthathypotheticalmodel models and training datasets are plentiful. We
descriptionandthedescriptionsofrealmodels.
useSQuAD(Rajpurkaretal.,2016)asground
truthtoevaluatethissetting.
3.5 Training
• Japanese NL-to-Code: Code generation from
DatasetProcessing Wetrainthemodelbylever- Japanese-languagequeriesisachallengingsce-
aging two datasets- one generated and one re- nariowherepriorworkexistsbutnoannotated
trieved. To sidestep the challenge of making dataorpretrainedmodelsareavailable. Weuse
schema-specificmodelingdecisions(e.g. construct- MCoNaLa(Wangetal.,2023)forevaluation.
ing specialized architectures for classification or • TemporalExpressionNormalization: Wefinally
generationtasks),wetreatalldatasetsas“text-to- consider a task where there are no pretrained
text”problems(Raffeletal.,2020). Wetextualize modelsortrainingdatasetsofanykindavailable.
theinputcolumnsofeachdatasetandprependthe Here we use the Temporal dataset of Wu et al.
user’sinstructionstotheinputtoguidethemodel. (2023)asgroundtruthforevaluation.
Though Prompt2Model offers automated model
Finetuning We concatenate the retrieved and
evaluation(ongeneratedandretrieveddatasests),
generated datasets and shuffle them before train-
we use real benchmark datasets here to measure
ing the student model. We use the same default
ourpipeline’sabilitytotrainaccuratemodels.
hyperparameters forall tasks.3 Wetrain withthe
AdamWoptimizerwithlr = 5e-5for3epochs, LLM Baseline A primary goal of our work is
whichtakesroughlyonehourforalltasks. to train small models that can match or outper-
formLLMs. Tomeasuresuccesstowardsthisgoal,
3.6 Evaluation
wereport theperformance ofgpt-3.5-turboon
OurModelEvaluatorautomaticallyevaluatesmod- eachbenchmark. Weprovidegpt-3.5-turbo4 the
els for all tasks using three general-purpose met- same instruction and demonstrations provided to
rics: Exact Match,ChrF++(Popovic´,2015),and Prompt2Model,forfaircomparison.
BERTScore (Zhang et al., 2019). ChrF++ bal-
ances precision and recall to assess text genera- 5 ExperimentResults
tion quality. Exact Match measures how often
5.1 Downstreamperformance
themodeloutputperfectlymatchestheexactrefer-
ence. BERTScorecapturessemanticsimilaritiesde- How effective is Prompt2Model at producing a
high-qualitymodel? InTable1,weevaluatedmod-
spitedifferentwordingsorphrasingsbycomparing
els produced by Prompt2Model, as well as our
3Weempiricallyfindthatthesedefaulthyperparameters
areeffective,butweplanonimplementinghyperparameter 4Weusedgpt-3.5-turbo-0613,accessedbetweenJuly
selectionusinggeneratedvalidationdatainthefuture. 26andAugust6,2023.
Method SQuAD MCoNaLa Temporal Method #Train Performance Anno.Cost
(EM) (ChrF++) (ChrF++)
Retrievalonly 3,000 56.79 ≈$0
Prompt2Model 61.5 13.1 55.2 Generationonly 3,000 44.20 ≈$5
w/oModelRet. 61.5 15.8 55.2 Retrieval+generation 6,000 61.46 ≈$5
w/oDataRet. 50.2 16.6 N/A
Customannotation 3,000 61.64 ≈$540
gpt-3.5-turbo 42.1 37.3 30.7
Table2: WecomparemodelperformanceonSQuAD
Table 1: We evaluate the model produced by onanannotation-costbasis,usingdatasetsproducedby
Prompt2Model on real benchmarks for each test set, differentmodulesofPrompt2Model,alongwithfully-
comparedtogpt-3.5-turbo,whichweusedtopower manualannotation.Performancereportedforallmodels
ourdatasetgenerator. Wealsoexaminetheeffectofre- is the exact match on the test set,7 which reflects the
movingspecificpartsofourpipeline—modelretrieval true task performance. Cost of custom annotation is
and dataset retrieval. There are no relevant datasets estimated from Rajpurkar et al. (2016) using their re-
availablefortheTemporaltask, sowedidnotusere- portedannotatorpayrateof$9/hourandkeeping1,000
trieveddataforPrompt2Modelthere. validationexamples.
baselineLLMgpt-3.5-turbo,onrealbenchmark
datasetsforeachtask—SQuAD,MCoNaLa,and true dataset. We use SQuAD as a running exam-
Temporal. We further examine the effect of re- ple.5 AsourpromptisadescriptionoftheSQuAD
moving2specificelementsofthePrompt2Model passage-levelquestionansweringtask(Figure1),
pipeline—modelretrievalanddatasetretrieval. weexcludeSQuADfromourretrieveddatasetslist.
On2of3datasets,wefindthatPrompt2Model Instead,weevaluatemodelsfinetunedon:
produces models that are considerably more ac- 1. 3kexamplesfromtheclosestretrieveddataset6
curate than gpt-3.5-turbo. This is remarkable 2. 3kexamplesgeneratedbyPrompt2Model
becausetheretrievedmodelforSQuADandTem- 3. Theunionoftheabove,whichiswhatthefull
poralisFlan-T5,which,at250Mparameters,isup Prompt2Modelpipelineuses
to700timessmallerthangpt-3.5-turbo(which 4. 3k examples from SQuAD (analogous to the
isbelievedtocontain175Bparameters). usercustom-annotatingdataforatask).
WeobservethatPrompt2Model’sperformance Table2showstheresultsacrossthesefourset-
onMCoNaLa’sJapanese-to-Pythontaskissignif- tings. While using retrieved or generated data
icantlyworsethangpt-3.5-turbo. Oneexplana- causesareductioninperformanceduetodomain
tion for this is the relatively low diversity in the shift,thecombinationofthetwomethodsachieves
generateddatasetofJapanesequeries;45of5000 similarperformancetousingthetruedataset. For
examples are different ways of saying “find the this machine reading comprehension task where
maximumvalueinalistofnumbers“. Wedonotob- the user would need to custom-annotate data for
servethislevelofredundancyinourotherdatasets,
theirtask,Prompt2Modelallowsforsimilarper-
suggesting that gpt-3.5-turbo may struggle to formanceatlessthan1%ofthecost.
generate diverse text for non-English languages.
5.3 Ourgeneratedevaluationdatacan
Another reason is the lack of an appropriate stu-
identifyrealmodelingimprovements
dentmodel—themodelsfoundbythemodelre-
trieverweretrainedoneitheronmultiplelanguage
orcode,butnotboth. Theresultingpretrainedmod- High-qualitygenerateddatashouldalsoallowus
elsmaylacktheparametricknowledgetorepresent to discriminate between multiple candidate mod-
theJapaneseinputs,Pythonoutputs,orboth. elstoselectamodelthatwillperformwelldown-
stream. Wefinetunevariousmodelsonagenerated
5.2 Combiningretrievedandgenerated datasetandranktheirperformanceaccordingtothe
datasetsispowerful generatedtestdataandthetestdatafromthetarget
(real)dataset. WeevaluatetheKendall’srankcor-
Ideally,generatedandretrieveddatashouldbeas
close to the target domain as possible. In our ex-
5WefocusononlySQuADherebecauseourothertwo
perimentalsetting,wherewedeliberatelychoose taskshavelessrealtrainingexamplesthanthedatasetswe
promptsthatmimicexistingdatasets,wecaneval- generate,makingcomparisonimpractical.
6Theclosestdatasetretrievedbythedatasetretrieverfor
uate how well the model performs relative to a
our SQuAD-inspired prompt is The Children’s Book Test
modeltrainedonthesameamountofdatafromthe Dataset(Hilletal.,2016).
Dataset Metric τ p-value latetheirneedsupfront,futureextensionsshould
also address the challenge of human-in-the-loop
SQuAD EM 64.3 0.03*
Temporal ChrF++ 24.2 0.31 correction–eitherbyofferingpotentialstrategies
MCoNaLa(JP) ChrF++ 70.9 0.00**
tohelphumansiterativelyrefineprompts,orallow-
inghumanstoperformpost-hocfixeswhenthetask
Table3:Weevaluate10differentmodelsonrealtestsets
metadataextractionandgenerateddatadonotalign
andtheircorrespondinggeneratedclones. Wecompute
withtheirintentions. Wehopetoproposeexplicit
Kendall’s Tau on the ranked lists of models and find
statisticallysignificantcorrelationsfor2of3datasets. challengesandinvitethecommunitytocontribute
novel implementations of various components in
ourframework.
relation(Kendall,1938)betweenthetworankings
todetermineifourgenerateddatacaneffectively
determinewhichmodelsarelikelytoperformwell
Limitations
downstream. Thisiscloselyrelatedtotheconcept
of concurrence between benchmarks (Liu et al.,
2023a); however, we are evaluating whether the
Oneoftheprimarylimitationsofoursystemisthat
generatedandrealdatarankspecificmodelsinthe
our current experiments have all been conducted
sameordering,ratherthanmodelingapproaches.
using the gpt-3.5-turbo API (used for prompt
Table3showstheKendall’sτ foreachtask,com-
parsing, dataset generation, and model retrieval).
puted over a set of reasonable models.8 The gen-
ThisLLMispaidandclosed-source,whichmakes
erated data shows strong correlation to the true
this problematic as a scientific artifact (Rogers
performanceontwoofthethreedatasets.
etal.,2023). Furthermore,theserviceproviderof
thisLLM,OpenAI,prohibitstheuseoftheirAPI
6 DiscussionandConclusion
tocreatemodelsthatmaycompetewithOpenAI,
We propose Prompt2Model, a framework that creating potential legal concerns with the use of
automatically constructs task-specific models us- Prompt2Model in commercial applications. We
ing only natural language prompts. Our proof- areexploringtheintegrationofopen-sourceLLMs
of-concept experiments show that, despite us- toavoidourrelianceonproprietaryAPIs.
ing a similar easy-to-use interface as LLMs,
Anotherlimitationofourworkisthelimitedabil-
Prompt2Model delivers small yet accurate mod-
ityofPrompt2Modeltosupporttasksthatrequire
els and its generated datasets can be used to esti-
processinglanguagesotherthanEnglish. Whilewe
mate real-world performance. Besides our refer-
have shown the limitations of our system at sup-
enceimplementationprovidingaready-to-usetool,
portingcodegenerationfromJapanesenaturallan-
Prompt2Model’s extensible design and modular
guagequeries,oursystemislikelytostrugglemore
implementation makes it a platform for advanc-
withlower-resourcelanguages. Weusetheunpub-
ingmodeldistillation,datasetgeneration,synthetic
lishedgpt-3.5-turbomodelforourDatasetGen-
evaluation,datasetretrieval,andmodelretrieval.
eratorinourreferenceimplementation. Thismodel
WebelieveourPrompt2Modelframeworkcan
is believed to be similar to GPT-3 (Brown et al.,
inspirevariousnovelresearchquestions. Wehope
2020), which was trained on 93% English docu-
that our platform enables future work that looks
ments, 1%Germandocuments, 1%Frenchdocu-
moredeeplyintoqualityassuranceonthegenerated
ments,and<5%documentsinanyotherlanguage.
dataandthemodel. Interestingquestionsinclude
Ouruseofthismodelmayexacerbateexistingdis-
howmuchdatashouldwegeneratefordownstream
parities in language technologies between high-
modeltrainingandhowdiverseshoulditbe? How
resourcelanguagesandlow-resourcelanguages.
doweeffectivelymixtheretrievedandgenerated
dataset such to achieve complementary strengths One potential limitation is that we have only
(e.g. using dataset generation to focus on the ex- testedourapproachon3tasks,eachwithasingle
pectedinputstothemodelthattheretrieveddataset datasetandasingleevaluationmetric. Wejustify
failstocover)? Sinceusersoftenstruggletoarticu- thisdecisionbecauseourfocusisonprovidingan
extensiblesoftwaresystemratherthanestablishing
8Thissetofmodelsconsistedof5T5-familymodels,2
state-of-the-artresultsonmanydatasets,butwebe-
BART-family models, and 1-5 additional retrieved models
fromtheModelRetriever,dependingontask. lievethatourresultssuggestbroaderapplicability.
EthicsStatement References
Any system which makes powerful technology AbubakarAbid,AliAbdalla,AliAbid,DawoodKhan,
moreaccessibletothepublichasethicalimplica- AbdulrahmanAlfozan,andJamesZou.2019. Gradio:
Hassle-freesharingandtestingofMLmodelsinthe
tions. Widder et al. (2022) discuss ethical issues
wild. arXivpreprintarXiv:1906.02569.
withopen-sourcepackagesinrelationtosoftware
libraries for deepfaking, including the possibility EmilyM.Bender,TimnitGebru,AngelinaMcMillan-
ofenablingmaliciousactorstousetechnologythat Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language mod-
theywouldotherwisenothavethetechnicalskills
els be too big? In Proceedings of the 2021 ACM
toleverage. ThisisalsoariskforanAutoMLsys-
ConferenceonFairness,Accountability,andTrans-
temsuchasPrompt2Model;however,webelieve parency,FAccT’21,page610–623,NewYork,NY,
this risk is outweighed by the benefits of greater USA.AssociationforComputingMachinery.
accessibility,especiallygiventhatalowbarrierto
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
entryforgeneratingharmfuldataalreadyexistsin
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
theformofprompted,web-interfacemodels.
Neelakantan,PranavShyam,GirishSastry,Amanda
While Prompt2Model could, if given harm- Askell, Sandhini Agarwal, Ariel Herbert-Voss,
ful inputs, generate toxic, offensive, or inaccu- Gretchen Krueger, Tom Henighan, Rewon Child,
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
rate synthetic data, this is no more of a risk with
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
Prompt2Model than it is with the underlying
teusz Litwin, Scott Gray, Benjamin Chess, Jack
promptedmodel(Benderetal.,2021);indeed,the Clark, ChristopherBerner, SamMcCandlish, Alec
useofmodelsandsupplementarydatasetsretrieved Radford, Ilya Sutskever, and Dario Amodei. 2020.
from Hugging Face may lessen the likelihood of Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems,
a downstream model replicating harms from the
volume 33, pages 1877–1901. Curran Associates,
promptedmodel’soutputs,thoughmoreinvestiga-
Inc.
tion is needed. Like all ML models, the models
that Prompt2Model returns can make mistakes, Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
andweaimtobetransparentinourdocumentation dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
aboutpotentiallimitationsofthesystem.
berg,etal.2023. Sparksofartificialgeneralintelli-
We hope that Prompt2Model will be broadly gence: Earlyexperimentswithgpt-4. arXivpreprint
useful. Our work is motivated by a desire to in- arXiv:2303.12712.
crease the accessibility of NLP models to people
NitayCalderon,SubhabrataMukherjee,RoiReichart,
whoarenotintheNLPcommunitybutwouldben-
andAmirKantor.2023. Asystematicstudyofknowl-
efitfromthecommunity’sinnovations;particularly,
edgedistillationfornaturallanguagegenerationwith
topeoplewhowoulduseNLPmodelsdownstream pseudo-target training. In Proceedings of the 61st
butmaynothavethedomain-specificknowledge AnnualMeetingoftheAssociationforComputational
todesigntheirownsystem. Prompt2Modelmay Linguistics(Volume1: LongPapers),pages14632–
14659,Toronto,Canada.AssociationforComputa-
alsoproveusefulforearlyNLPresearchersbypro-
tionalLinguistics.
viding a starting point for intuitions about base-
linesforvarioustasksandenablingthediscovery AlexisConneau,KartikayKhandelwal,NamanGoyal,
ofsimilaritiesbetweenadescribedtaskandexist- Vishrav Chaudhary, Guillaume Wenzek, Francisco
ing work. We open-source Prompt2Model and Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer,andVeselinStoyanov.2020. Unsupervised
welcomecommunitycontributions.
cross-lingualrepresentationlearningatscale. InPro-
ceedings of the 58th Annual Meeting of the Asso-
Acknowledgements
ciationforComputationalLinguistics,pages8440–
8451, Online. Association for Computational Lin-
This work was supported in part by a fellowship
guistics.
fromNECResearchLaboratories. Wearegrateful
to Alex Cabrera, Will Epperson, Nelson Liu, Ar- MichaelFärberandAnn-KathrinLeisinger.2021. Rec-
jun Ramani, Zirui Cheng, Zhiyuan Zeng, Tianci ommendingdatasetsforscientificproblemdescrip-
tions. InCIKM,pages3014–3018.
Xue,YanchenLiu,Yi-HsinHungandZhilinYang
for their feedback and guidance. We particularly
Luciano Floridi and Massimo Chiriatti. 2020. Gpt-3:
appreciateZiruiCheng’svideoproductionsupport
Itsnature,scope,limits,andconsequences. Minds
forourdemo. andMachines,30:681–694.
LuyuGao,XueguangMa,JimmyLin,andJamieCallan. promptingmethodsinnaturallanguageprocessing.
2023. Precisezero-shotdenseretrievalwithoutrel- ACMComput.Surv.,55(9).
evance labels. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin- SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe,
guistics(Volume1: LongPapers),pages1762–1777, MikeLewis,HannanehHajishirzi,andLukeZettle-
Toronto,Canada.AssociationforComputationalLin- moyer.2022. Rethinkingtheroleofdemonstrations:
guistics. Whatmakesin-contextlearningwork? InProceed-
ingsofthe2022ConferenceonEmpiricalMethodsin
ArnavGudibande,EricWallace,CharlesBurtonSnell, NaturalLanguageProcessing,pages11048–11064,
XinyangGeng,HaoLiu,P.Abbeel,SergeyLevine, AbuDhabi,UnitedArabEmirates.Associationfor
andDawnSong.2023. Thefalsepromiseofimitating ComputationalLinguistics.
proprietaryllms. ArXiv,abs/2305.15717.
GrahamNeubigandZhiweiHe.2023. ZenoGPTMa-
XingweiHe, Zheng-WenLin, YeyunGong, AlexJin, chineTranslationReport.
HangZhang,ChenLin,JianJiao,SiuMingYiu,Nan
Duan,andWeizhuChen.2023. Annollm: Making Andrei Paleyes, Raoul-Gabriel Urma, and Neil D
large language models to be better crowdsourced Lawrence.2022. Challengesindeployingmachine
annotators. ArXiv,abs/2303.16854. learning: asurveyofcasestudies. ACMComputing
Surveys,55(6):1–29.
FelixHill, AntoineBordes, SumitChopra, andJason
Weston. 2016. The goldilocks principle: Reading
GunhoPark,BaeseongPark,MinsubKim,SungjaeLee,
children’s books with explicit memory representa-
JeonghoonKim,BeomseokKwon,SeJungKwon,
tions.
ByeongwookKim,YoungjooLee,andDongsooLee.
2022. Lut-gemm: Quantizedmatrixmultiplication
Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra
based on luts for efficient inference in large-scale
Molina,AaronDonsbach,MichaelTerry,andCarrieJ
generativelanguagemodels.
Cai.2022. Promptmaker: Prompt-basedprototyping
withlargelanguagemodels. InCHIConferenceon
Maja Popovic´. 2015. chrF: character n-gram F-score
HumanFactorsinComputingSystemsExtendedAb-
forautomaticMTevaluation. InProceedingsofthe
stracts,pages1–8.
TenthWorkshoponStatisticalMachineTranslation,
pages 392–395, Lisbon, Portugal. Association for
Maurice G Kendall. 1938. A new measure of rank
ComputationalLinguistics.
correlation. Biometrika,30(1/2):81–93.
JingYuKoh.2020. Modelzoo. URLhttp://modelzoo. ColinRaffel,NoamShazeer,AdamRoberts,Katherine
co. Lee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJ.Liu.2020. Exploringthelimits
Quentin Lhoest, Albert Villanova del Moral, Yacine oftransferlearningwithaunifiedtext-to-texttrans-
Jernite,AbhishekThakur,PatrickvonPlaten,Suraj former. J.Mach.Learn.Res.,21(1).
Patil,JulienChaumond,MariamaDrame,JulienPlu,
Lewis Tunstall, Joe Davison, Mario Šaško, Gun- PranavRajpurkar,JianZhang,KonstantinLopyrev,and
jan Chhablani, Bhavitvya Malik, Simon Brandeis, PercyLiang.2016. SQuAD:100,000+questionsfor
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas machinecomprehensionoftext. InProceedingsof
Patry, Angelina McMillan-Major, Philipp Schmid, the2016ConferenceonEmpiricalMethodsinNatu-
Sylvain Gugger, Clément Delangue, Théo Matus- ralLanguageProcessing,pages2383–2392,Austin,
sière, Lysandre Debut, Stas Bekman, Pierric Cis- Texas.AssociationforComputationalLinguistics.
tac, Thibault Goehringer, Victor Mustar, François
Lagunas,AlexanderRush,andThomasWolf.2021. LariaReynoldsandKyleMcDonell.2021. Promptpro-
Datasets: Acommunitylibraryfornaturallanguage gramming for large language models: Beyond the
processing. InProceedingsofthe2021Conference few-shot paradigm. In Extended Abstracts of the
onEmpiricalMethodsinNaturalLanguageProcess- 2021 CHI Conference on Human Factors in Com-
ing: SystemDemonstrations,pages175–184,Online putingSystems,CHIEA’21,NewYork,NY,USA.
andPuntaCana,DominicanRepublic.Association AssociationforComputingMachinery.
forComputationalLinguistics.
Stephen E. Robertson, Steve Walker, Micheline
NelsonF.Liu,TonyLee,RobinJia,andPercyLiang. Hancock-Beaulieu, Mike Gatford, and A. Payne.
2023a. Do question answering modeling improve- 1995. Okapiattrec-4. InTextRetrievalConference.
ments hold across benchmarks? In Proceedings
of the 61st Annual Meeting of the Association for Anna Rogers, Niranjan Balasubramanian, Leon Der-
ComputationalLinguistics(Volume1: LongPapers), czynski,JesseDodge,AlexanderKoller,SashaLuc-
pages13186–13218,Toronto,Canada.Association cioni, MaartenSap, RoySchwartz, NoahASmith,
forComputationalLinguistics. andEmmaStrubell.2023. Closedaimodelsmake
badbaselines.
PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,
HiroakiHayashi,andGrahamNeubig.2023b. Pre- EmreSezgin,JosephSirrianni,andSimonL.Linwood.
train, prompt, and predict: A systematic survey of 2022. Operationalizingandimplementingpretrained
largeailinguisticmodelsintheunitedstateshealth- SherryWu,HuaShen,DanielSWeld,JeffreyHeer,and
caresystem: Anoutlookofgpt-3asaservice. JMIR MarcoTulioRibeiro.2023. Scattershot: Interactive
MedicalInformatics,10(2). in-contextexamplecurationfortexttransformation.
InProceedingsofthe28thInternationalConference
JörgTiedemannandSanthoshThottingal.2020. OPUS- onIntelligentUserInterfaces,IUI’23,page353–367,
MT — Building open translation services for the New York, NY, USA. Association for Computing
World. InProceedingsofthe22ndAnnualConfer- Machinery.
enecoftheEuropeanAssociationforMachineTrans-
lation(EAMT),Lisbon,Portugal. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger,andYoavArtzi.2019. Bertscore: Evalu-
VijayViswanathan,LuyuGao,TongshuangWu,Pengfei atingtextgenerationwithbert.
Liu,andGrahamNeubig.2023. DataFinder: Scien-
tificdatasetrecommendationfromnaturallanguage
descriptions. InProceedingsofthe61stAnnualMeet-
ing of the Association for Computational Linguis-
tics(Volume1: LongPapers),pages10288–10303,
Toronto,Canada.AssociationforComputationalLin-
guistics.
ShuohangWang, YangLiu, YichongXu, Chenguang
Zhu,andMichaelZeng.2021a. Wanttoreducela-
beling cost? GPT-3 can help. In Findings of the
AssociationforComputationalLinguistics: EMNLP
2021,pages4195–4205,PuntaCana,DominicanRe-
public.AssociationforComputationalLinguistics.
XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,
EdChi,SharanNarang,AakankshaChowdhery,and
DennyZhou.2022. Self-consistencyimproveschain
ofthoughtreasoninginlanguagemodels.
YueWang,WeishiWang,ShafiqJoty,andStevenC.H.
Hoi. 2021b. Codet5: Identifier-aware unified pre-
trainedencoder-decodermodelsforcodeunderstand-
ingandgeneration. InEMNLP.
Zhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F.
Xu,andGrahamNeubig.2023. MCoNaLa: Abench-
markforcodegenerationfrommultiplenaturallan-
guages. InFindingsoftheAssociationforCompu-
tational Linguistics: EACL 2023, pages 265–273,
Dubrovnik,Croatia.AssociationforComputational
Linguistics.
DavidGrayWidder,DawnNafus,LauraDabbish,and
JamesHerbsleb.2022. Limitsandpossibilitiesfor
“ethicalai”inopensource: Astudyofdeepfakes. In
Proceedingsofthe2022ACMConferenceonFair-
ness,Accountability,andTransparency,FAccT’22,
page2035–2046,NewYork,NY,USA.Association
forComputingMachinery.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RemiLouf,MorganFuntow-
icz,JoeDavison,SamShleifer,PatrickvonPlaten,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
QuentinLhoest,andAlexanderRush.2020. Trans-
formers:State-of-the-artnaturallanguageprocessing.
InProceedingsofthe2020ConferenceonEmpirical
Methods in Natural Language Processing: System
Demonstrations,pages38–45,Online.Association
forComputationalLinguistics.

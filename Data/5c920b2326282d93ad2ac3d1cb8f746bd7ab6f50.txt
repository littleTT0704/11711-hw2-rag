Results of WMT23 Metrics Shared Task:
Metrics might be Guilty but References are not Innocent
MarkusFreitag(1),NitikaMathur(2),Chi-kiuLo羅致翹(3),EleftheriosAvramidis(4),
RicardoRei(5,6,7),BrianThompson(8),TomKocmi(9),FrédéricBlain(10),DanielDeutsch(1),
CraigStewart(11),ChrysoulaZerva(7,12),SheilaCastilho(13),AlonLavie(11),GeorgeFoster(1)
(1)GoogleResearch(2)OracleDigitalAssistant(3)NationalResearchCouncilCanada
(4)GermanResearchCenterforArtificialIntelligence(DFKI)(5)Unbabel(6)INESC-ID
(7)InstitutoSuperiorTécnico(8)AWSAILabs(9)Microsoft(10)TilburgUniversity(11)Phrase
(12)InstitutodeTelecomunicações(13)DublinCityUniversity
wmt-metrics@googlegroups.com
Abstract
Metric avgcorr
XCOMET-Ensemble 1 0.825
XCOMET-QE-Ensemble* 2 0.808
ThispaperpresentstheresultsoftheWMT23
MetricX-23 2 0.808
MetricsSharedTask. Participantssubmitting
GEMBA-MQM* 2 0.802
automaticMTevaluationmetricswereasked MetricX-23-QE* 2 0.800
toscoretheoutputsofthetranslationsystems mbr-metricx-qe* 3 0.788
competing in the WMT23 News Translation MaTESe 3 0.782
CometKiwi* 3 0.782
Task. Allmetricswereevaluatedonhowwell
COMET 3 0.779
they correlate with human ratings at the sys-
BLEURT-20 3 0.776
tem and segment level. Similar to last year, KG-BERTScore* 3 0.774
weacquiredourownhumanratingsbasedon sescoreX 3 0.772
expert-basedhumanevaluationviaMultidimen- cometoid22-wmt22* 4 0.772
docWMT22CometDA 4 0.768
sionalQualityMetrics(MQM).Followinglast
docWMT22CometKiwiDA* 4 0.767
year’s success, we also included a challenge
Calibri-COMET22 4 0.767
set subtask, where participants had to create Calibri-COMET22-QE* 4 0.755
contrastive test suites for evaluating metrics’ YiSi-1 4 0.754
abilitytocaptureandpenalisespecifictypesof MS-COMET-QE-22* 5 0.744
translationerrors. Furthermore,weimproved prismRef 5 0.744
mre-score-labse-regular 5 0.743
ourmeta-evaluationprocedurebyconsidering
BERTscore 5 0.742
fewertasksandcalculatingaglobalscoreby
XLsim 6 0.719
weightedaveragingacrossthevarioustasks. f200spBLEU 7 0.704
MEE4 7 0.704
tokengram_F 7 0.703
We present an extensive analysis on how
embed_llama 7 0.701
wellmetricsperformonthreelanguagepairs: BLEU 7 0.696
Chinese→English, Hebrew→English on the chrF 7 0.694
sentence-level and English→German on the eBLEU 7 0.692
Random-sysname* 8 0.529
paragraph-level. Theresultsstronglyconfirm
prismSrc* 9 0.455
theresultsreportedlastyear,thatneural-based
metricsaresignificantlybetterthannon-neural
Table1: Officialrankingofprimarysubmissionstothe
metricsintheirlevelsofcorrelationwithhuman
WMT23MetricTask.Thefinalscoreistheweightedav-
judgments. Further,weinvestigatetheimpact
eragecorrelationover10differenttasks.Starredmetrics
ofbadreferencetranslationsonthecorrelations
arereference-free,andunderlinedmetricsarebaselines.
ofmetricswithhumanjudgment. Wepresent
SeeTable18forthepairwisecomparisonsfromwhich
anovelapproachforgeneratingsyntheticref-
therankswerederived.
erencetranslationsbasedonthecollectionof
MT system outputs and their corresponding
MQMratings,whichhasthepotentialtomiti- 1 Introduction
gatebadreferenceissuesweobservedthisyear
Themetricssharedtask1hasbeenakeycomponent
forsomelanguagepairs. Finally,wealsostudy
theconnectionsbetweenthemagnitudeofmet- ofWMTsince2008,servingasawaytovalidate
ricdifferencesandtheirexpectedsignificance the use of automatic MT evaluation metrics and
in human evaluation, which should help the drive the development of new metrics. We eval-
communitytobetterunderstandandadoptnew
metrics. 1https://wmt-metrics-task.github.io/
uatereference-basedautomaticmetricsthatscore othermetrics. Finally,wecomputetop-levelsig-
MT output by comparing the translations with a nificanceclusterstoprovideaclearerglobalrank-
referencetranslationgeneratedbyhumantransla- ingofparticipatingmetrics.
tors,whoareinstructedtotranslate“fromscratch”
withoutpost-editingfromMT.Inaddition,wealso • SyntheticReference: TheMQMscoresforthe
invitedsubmissionsofreference-freemetrics(qual- humanreferencetranslationforzh→enwereun-
ityestimationmetricsorQEmetrics)thatcompare expectedly low, ranking humans below almost
MToutputsdirectlywiththesourcesegments. All all WMT submissions. We investigate the im-
metricsareevaluatedbasedontheiragreementwith pact of bad reference translations on reference-
human rating when scoring MT systems and hu- based metrics and propose a novel approach
mantranslationsatthesystemandsentencelevel. to create a synthetic reference translation from
Thefinalrankingofthisyear’ssubmittedprimary all WMT submissions and their corresponding
metricsisshowninTable1. Belowaresomekey MQMscores.
details and changes for this year’s metric shared
• Challenge Sets Subtask: For the second year,
task:
weincludeadecentralizedsub-taskonchallenge
sets,inwhichtestsetsaresubmittedbydifferent
• Language Pairs: For this year, we focus on
researchteamstargetingtorevealmetrics’abil-
threemainlanguagepairs: (i)Onelanguagepair
ities or the weaknesses in evaluating particular
withparagraph-leveltestsets: English→German
translationphenomena. Wereceivedthreechal-
(en→de), (i) one low-resource language pair
lenge sets covering a wide range of translation
withsentence-leveltestsets: Hebrew→English
errorsandlinguisticphenomenainmorethana
(he→en), (iii) one high-resource language pair
hundredtranslationdirections.
withsentence-leveltestsets: Chinese→English
(zh→en).
• Understand Magnitude of Score Difference:
Thisyear,weincludetwoanalysestounderstand
• Human Evaluation: Like last year, we col-
themeaningofthescoredifferencesthatmetrics
lectedourownhumanratingsforourthreelan-
presentwithrespecttothestatisticalsignificance
guage pairs from professional translators via
ofMTsystemrankingsaccordingtohumanan-
MQM(Lommeletal.,2014;Freitagetal.,2021).
notationsandmetricscores. Theseanalysespro-
Wereleasedanduploaded2allMQMannotations,
videadditionalassistanceforMTresearchersto
andwerecommendusingMarot3forlookinginto
buildanintuitionontherelationshipbetweenthe
thisdata.
magnitude of metric score differences and the
reliabilityoftheimprovedtranslationquality.
• MetaEvaluation: Thisyear’smeta-evaluationis
significantlystreamlinedfromlastyear’s. Instead • MTME:Similartolastyear,allthedatahasbeen
of201tasks,weusejust10,designedtocapture uploaded to MTME4, and all results in this pa-
complementaryrankingandlinearityproperties per are calculated with this analysis tool. We
at system- and segment-level granularity. We encourageeverymetricdevelopertouseMTME
replace Kendall’s tau at the segment level with tocalculatecontrastivescorestoenhanceconsis-
a version of pairwise accuracy that gives met- tencyandcomparabilitygoingforward.
ricscreditforcorrectlypredictingtiesinhuman
scores,whileautomaticallycalibratingforeach Ourmainfindingsare:
metric’snaturalscale(Deutschetal.,2023). In-
stead of averaging per-task ranks to derive an • XCOMET-Ensemble is the winner of the
overallscoreforeachmetric,wesimplyaverage WMT23MetricsSharedTask(Table1).
correlation/accuracy scores across tasks. This
• Highcorrelationsbetweenautomaticmetrics
places metric scores on an absolute scale, and
andhumanjudgmentsatthesegmentleveldo
makesthemindependentoftheperformanceof
notnecessarilyguaranteehighcorrelationsat
2https://github.com/google/ thesystemlevel(Figure5).
wmt-mqm-human-evaluation
3https://github.com/google-research/ 4https://github.com/google-research/
google-research/tree/master/marot mt-metrics-eval
• Reference quality matters: The low quality 2.1 WMTTestSets
referenceforzh→ensignificantlyloweredthe
WeusetestsetspreparedbytheWMT23General
correlation of all metrics with human judge-
MT Shared Task (Kocmi et al., 2023). For our
ment(Section8).
threemainlanguagepairs,thetestsetscontain557
• Wedeterminedthemagnitudeofscorediffer-
en→de,1910he→en,and1976zh→ensegments.
encesrequiredtoproduceastatisticallysignif- This year, the test sets cover up to five domains
icantdifferenceinhumanjudgmentforeach fromthefollowinglist: news,conversational,user
metric, revealing that even minor score dif- reviews,manuals,andsocial. Eachlanguagepair
ferences of the top performing metrics can containsacomparablenumberofsentencesfrom
bestatisticalsignificantwithhighprobability eachdomain,resultinginreasonablybalancedtest
(Section7). sets.
English→German contains four balanced do-
• Resultsfromthechallengesetsindependently
mains: news, social, conversational, and user re-
agreed with our findings that the quality of
views. Incontrasttootherlanguagepairs,segments
referencematters. Developingreference-free
areparagraphsratherthansentences.
metricsisworthfurtherexploration,andmet-
Hebrew→Englishcontainsonlynewsanduser
ricresearchersareadvisedtoinvestigateinto
reviewsdomains. Thislanguagepairhastwohu-
the influence of language-agnostic multilin-
manreferences,butoneofthem(refA)issuspected
gual embeddings on MT evaluation. It is
ofbeingapost-editedOnline-Bsystemoutput.
equally important for metric researchers to
Chinese→Englishcontainsnews,userreviews,
testtheperformanceofmetricsindiversecol-
andmanuals. Thefirsttwodomainscontainaround
lection of linguistic phenomena and wider
750sentences,whilemanualscontainsaround500.
landscape of translation quality in order to
Thereferencetranslationsprovidedforthetest
minimize unexpected behaviours of metrics
setsareproducedbyprofessionaltranslators.
(Section10).
Formoredetailsregardingthenewstestsets,we
The rest of the paper is organized as follows: referthereadertotheWMT23GeneralMTShared
Section 2 describes the test data and additional Taskfindingspaper(Kocmietal.,2023).
MT systems that we trained. Section 3 presents
2.2 AdditionalMTOutput
anoverviewoftheconductedexpert-basedhuman
evaluation. Section4describesthemetricsevalu- Similartolastyear,wemadeanefforttoexpandthe
ated this year (baselines and participants). Sec- pooloftranslationsbeyondtheWMTsubmissions,
tion 5 describes the conducted meta-evaluation. whichcanpotentiallybequitesimilartoeachother.
Section6reportsourmainresults. Section7inter- Weaddedtranslationswhichweexpectedtodiffer
pretsandevaluatesmetrics’scoresbeyondcorrela- in two main ways from the submissions: 1) by
tions. Section 8 analyses the impact of bad refer- using a massively multilingual model; and 2) by
encetranslationsonthevariousmetrics. Section9 generatingwithMBRdecoding;
summarizes our results for additional WMT23 Forourmultilingualmodel,weselectedthe3.3B
Translationtasklanguage-pairsbasedontheirDi- parameter NLLB200 model (NLLB Team et al.,
rect Assessment human evaluation. Section 10 2022)viathehuggingface(Wolfetal.,2020)inter-
presents a description of the submitted challenge face. WefoundNLLB200tosignificantlyoutper-
setsalongwiththeirfindings. Finally,Section11 formtheM2M100(Fanetal.,2021)thatweused
presentsourmostrelevantconclusions. lastyear.
MinimumBayesRisk(MBR)decodinghasre-
2 TranslationSystems
cently gained attention in MT as a decision rule,
Similartopreviousyears’editions,thesource,ref- with the potential to overcome some of the bi-
erencetexts,andMTsystemoutputsforthemetrics asesofMAPdecodinginNMT(EikemaandAziz,
taskaremainlyderivedfromtheWMT23General 2020;MüllerandSennrich,2021;EikemaandAziz,
MT Shared Task. In addition to the MT system 2021;Freitagetal.,2022;Fernandesetal.,2022).
outputs from the WMT evaluation campaign, we MBR decoding centrally relies on a reference-
includedtranslationsfromtwoadditionalMTsys- based utility metric: its goal is to identify a hy-
temswhichwedeemedinterestingforevaluation. pothesiswithahighestimatedutility(expectation
undermodeldistribution)withthehopethatahigh annotators were instructed to label error spans
estimatedutilitytranslatesintoahighactualutility within each segment in a document, paying
(with respect to a human reference). In practice, particular attention to document context. Each
this means generating several candidate transla- error was highlighted in the text, and labelled
tionsandfindingthetranslationthatismostsimilar with an error category and a severity. Segments
totherestofthecandidatetranslations. that are too badly garbled to permit reliable
We produced both the top-1 greedy transla- identification of individual errors are assigned a
tion and MBR outputs. For MBR, we sampled specialNon-translationerror. Errorseveritiesare
100translationcandidatesfromthemodelviaEp- assigned independent of category, and consist of
silonsampling(Hewittetal.,2022;Freitagetal., Major, Minor, and Neutral levels, corresponding
2023). We used epsilon_cutoff=0.02 and respectively to actual translation or grammatical
eta_cutoff=0.0. Thisyear,weusedsentence- errors,smallerimperfectionsandpurelysubjective
level BLEUfromsacreBLEU(Post,2018)withthe opinions about the translation. Since we are
default‘a13’tokenizerandthe‘floor’smoothing ultimatelyinterestedinscoringsegments,weadopt
methodasutilityfunctiononly. theweightingschemeshowninTable2. Formore
details, exact annotator instructions and a list of
3 MQMHumanEvaluation errorcategories,wereferthereadertoFreitagetal.
(2021) as the exact same setup was used for the
Automaticmetricsareusuallyevaluatedbymeasur-
previoustwometricstasks.
ing correlations with human ratings. The quality
oftheunderlyinghumanratingsiscritical,andre-
Severity Category Weight
centfindings(Freitagetal.,2021)haveshownthat
Major Non-translation 25
crowdsourced human ratings are not reliable for
allothers 5
high quality MT output. Furthermore, an evalua-
Minor Fluency/Punctuation 0.1
tionschemabasedonMQM(Lommeletal.,2014), allothers 1
whichrequiresexpliciterrorannotation,isprefer-
Neutral all 0
abletoanevaluationschemathatonlyasksraters
forasinglescalarvaluepertranslation. Similarto Table2: Google’sMQMerrorweighting.
last year, we decided to conduct our own MQM-
basedhumanevaluationonasubsetofsubmissions
3.2 Hebrew→English
and language pairs that are most interesting for
TheannotationsfortheHebrew→Englishlanguage
evaluatingcurrentmetrics.
pairweresourcedfromUnbabel,whoengagedfour
MQM is a general framework that provides a
professional native language annotators possess-
hierarchy of translation errors which can be tai-
ing extensive translation experience. Much like
lored to specific applications. Google and Unba-
Google’sapproach,theseannotatorswereprovided
belsponsoredthehumanevaluationforthisyear’s
withthefulldocumentcontext, comprisingupto
metrics task for a subset of language pairs using
tensegments. Theirtaskwastoidentifyandclas-
eitherprofessionaltranslators(English→German,
sifyerrorsbyhighlightingthem,followingUnba-
Chinese→English) or trusted and trained raters
bel’sMQM3.0typology5.
(Hebrew→English). Theerrorannotationtypology
The annotators were instructed to classify the
and guidelines used by Google’s and Unbabel’s
errorsbasedonseverity,withUnbabel’sclassifica-
annotatorsdifferslightlyandaredescribedinthe
tionencompassingnotonly“Minor”and“Major”
followingtwosections.
errorseverities(analogoustoGoogle’scriteria)but
3.1 English→GermanandChinese→English alsoa“Critical”errorseverity. However,toensure
consistency in our evaluation process, we opted
Annotations for English→German and
to align with the Google methodology outlined
Chinese→English were sponsored and exe-
previously. Specifically, we treated all annotated
cutedbyGoogle,using18professionaltranslators
“Critical”errorsas“Major”errors,andweapplied
(10forEnglish→German,8forChinese→English)
aweightingschemeforpunctuationerrors,asde-
havingaccesstothefulldocumentcontext. Each
tailedinTable2.
segment gets annotated by a single rater. Instead
of assigning a scalar value to each translation, 5seeUnbabelAnnotationGuidelines-Typology3.0
3.3 HumanEvaluationResults • F200SPBLEU (NLLB Team et al., 2022)
are BLEU scores computed with sub-
Due to the fact that we ran our own human eval-
word tokenization done by the standardized
uation, we were only able to evaluate a subset of
FLORES-200Sentencepiecemodels. Weused
thetestsegments. InTable3,youcanseethenum-
thecommandlineSacreBLEUtocomputethe
berofsegmentsanddocumentsforeachlanguage
sentencelevelF200SPBLEU7 andweaverage
pairandtestsetthatweusedforhumanevaluation.
the segment-level scores to obtain a corpus-
Wefollowedasimpleandconsistentapproachto
levelscore.
downsample the data: we considered each docu-
ment,whileonlykeepingthefirst10sentencesof
• CHRF (Popovic´, 2015) uses character
each document. By doing this, we did not need
n-gramsinsteadofwordn-gramstocompare
todiscardmostofthedocumentsandonlyneeded
theMToutputwiththereference. ForCHRF
tocroplongerdocuments. TheEnglish→German
weusedtheSacreBLEUsentence_chrf
test is on the paragraph-level, and we had to dis-
function (with default arguments8) for
cardtwodocumentsasthefirstparagraphalready
segment-level scores and we average those
containedmorethan10sentences. Inallcases,the
scorestoobtainacorpus-levelscore.
MQMscoreforasegmentisthesumofthescores
fortheerrorsinthatsegment,andtheMQMscore
BERTSCORE (Zhang et al., 2020) leverages
foratestsetistheaverageoftheMQMscoresof
contextualembeddingsfrompre-trainedtransform-
thesegmentsthatwereannotated.
erstocreatesoft-alignmentsbetweenwordsincan-
The results of the MQM human evaluation didateandreferencesentencesusingcosinesimilar-
can be seen in Table 4. Most of the reference ity. Basedonthealignmentmatrix,BERTSCORE
translations are ranked first, except for refA for returnsaprecision, recallandF1score. Weused
Chinese→English. Notrankingthehumanevalua- F1withoutTF-IDFweighting.
tionontopoftheMToutputisusuallyasignalfor
a corrupt human evaluation. We double-checked BLEURT(Sellametal.,2020) isalearnedmetric
the annotation for refA and can confirm that the fine-tuned on Direct Assessments (DA). Unlike
referencetranslationindeedcontainedmanyerrors. COMET,BLEURTencodesthetranslationandthe
referencetogetherandutilizesthe[CLS]tokenas
4 BaselinesandSubmissions anembeddingtorepresentthepair. Weemployed
theBLEURT20checkpoint(Puetal.,2021),which
We computed scores for several baseline metrics was trained on top of RemBERT using DA data
inordertocomparesubmissionsagainstprevious fromprevioussharedtasksspanningfrom2015to
well-studied metrics. We will start by describing 2019,alongwithadditionalsyntheticdatacreated
thosebaselines,andthenwewilldescribethesub- fromWikipediaarticles.
missionsfromparticipatingteams. Anoverviewof
theevaluatedmetricscanbeseeninTable5. COMET (Rei et al., 2022a) is a learned metric
fine-tuned using DA from previous WMT Trans-
4.1 Baselines lationsharedtasks. Thismetricreliesonsentence
embeddingsfromthesource,translation,andref-
SacreBLEUbaselines Weusethefollowingmet-
erencetoproduceafinalscore. Weutilizedthede-
ricsfromSacreBLEU(Post,2018)asbaselines:
faultmodelwmt22-comet-daprovidedinver-
sion 2.0.2 of the Unbabel/COMET framework.
• BLEU(Papinenietal.,2002)isbasedonthe
ThismodelemploysXLM-Rlargeasitsbackbone
precisionofn-gramsbetweentheMToutput
modelandistrainedondatafromthe2017to2019
anditsreferenceweightedbyabrevitypenalty.
WMTsharedtasks,incombinationwiththeMLQE-
Using SacreBLEU we obtained sentence-
PEcorpus(Fomichevaetal.,2022).
BLEU values using the sentence_bleu
Python function and for corpus-level BLEU
COMETKIWI(Reietal.,2022b) isareference-
we used corpus_bleu (both with default
free learned metric that functions similarly to
arguments6).
7nrefs:1|case:mixed|eff:yes|tok:flores200|smooth:exp|ver-
6lnrefs.1|case.mixed|lang.LANGPAIR|tok.13a|smooth.exp| sion:2.3.0
version.2.3.0 8chrF2|lang.LANGPAIR|nchars.6|space.false|version.2.3.0
language news social speech userreviews manuals
en→de 104/139(30/30) 206/212(79/79) 58/113(23/25) 92/93(58/58) n/a
he→en 619/1558(68/70) n/a n/a 201/352(26/26) n/a
zh→en 377/763(38/38) n/a n/a 677/726(127/127) 123/487(14/14)
Table3: NumbersofMQM-annotatedsegmentsperdomain(numberofdocsinbrackets).
English→German↓ BLEURT, but instead of encoding the transla-
System all news social speech user-reviews
tion along with its reference, it uses the source.
refA 2.96 3.12 2.02 4.74 3.77
Weutilizedthewmt22-cometkiwi-damodel,
GPT4-5shot 3.72 4.00 2.41 6.51 4.60
ONLINE-W 3.95 2.69 2.62 5.90 7.13 which was a top-performing reference-free met-
ONLINE-B 4.71 4.35 3.14 5.96 7.85 ric from last year’s shared task. This reference-
ONLINE-Y 5.64 4.45 3.67 7.48 10.26
free metric is fine-tuned on the same data as
ONLINE-A 5.67 4.40 3.84 7.78 9.87
wmt22-comet-dausingtheversion2.0.2ofthe
ONLINE-G 6.57 6.43 4.12 7.93 11.38
ONLINE-M 6.94 4.87 4.41 8.30 14.08 Unbabel/COMETframework.
Lan-BridgeMT 8.67 7.99 5.55 9.72 15.78
LanguageX 9.25 8.43 5.74 14.23 14.92 DOCWMT22COMETDA (Vernikos et al.,
NLLB_Greedy 9.54 8.29 5.20 14.82 17.35
2022) is the document-level version of
NLLB_MBR_BLEU 10.79 9.93 5.53 17.75 19.18
wmt22-comet-da, which computes the BERT
AIRC 14.23 14.32 8.34 20.34 23.45
embeddingsusingmulti-sentencecontextinstead
Hebrew→English↓ ofjustthesinglesentence.
System all news user-reviews DOCWMT22COMETKIWIDAisthedocument-
refA 1.17 1.28 0.86 level version of WMT22-COMETKIWI-DA (QE)
GPT4-5shot 1.33 1.29 1.48
whichcomputestheBERTembeddingsusingmulti-
ONLINE-A 1.38 1.34 1.50
ONLINE-B 1.55 1.60 1.39 sentencecontextinsteadofjustthesinglesentence.
GTCOM_DLUT 1.89 1.85 1.99
UvA-LTL 1.92 1.80 2.30 MS-COMET-QE-22 (Kocmi et al., 2022b) is
ONLINE-G 2.06 2.06 2.04 builtontopof COMET byMicrosoftResearchus-
ONLINE-Y 2.35 2.42 2.12
ing proprietary data. This metric is trained on a
LanguageX 2.38 2.33 2.53
Samsung_Research_Philippines 3.23 3.62 2.05 severaltimeslargersetofhumanjudgementscom-
NLLB_MBR_BLEU 3.68 3.83 3.20 paredtoCOMET-baseline,covering113languages
NLLB_Greedy 3.79 3.98 3.19
and15domains. Furthermore,theauthorspropose
Lan-BridgeMT 3.79 3.81 3.74
filteringofhumanjudgementwithpotentiallylow
Chinese→English↓ qualitytofurtherimprovethemodel. Themetric
System all news manuals user-reviews calculatedscoresinqualityestimationfashionwith
Lan-BridgeMT 2.10 2.31 1.28 2.13 onlysourcesegmentandMThypothesis.
GPT4-5shot 2.31 2.26 2.01 2.39
Yishu 3.23 3.34 1.67 3.46 PRISMREF and PRISMSRC (Thompson and
ONLINE-B 3.39 3.27 1.78 3.74 Post,2020a,b) PRISMREFisthereference-based
HW-TSC 3.40 3.40 1.83 3.68
PRISM thatusesamultilingualMTmodelinzero-
ONLINE-A 3.79 2.90 1.83 4.63
ONLINE-Y 3.79 3.47 2.84 4.14 shotparaphrasemodeltoscorethecandidatetrans-
ONLINE-G 3.86 3.58 2.02 4.34 lationconditionedonthereferencesentence,and
ONLINE-W 4.06 3.84 2.16 4.53
thereferencesentenceconditionedonthecandidate
LanguageX 4.23 4.05 2.84 4.59
IOL_Research 4.59 3.60 1.85 5.63 translation, and averages the two scores. PRISM-
refA 4.83 5.04 5.17 4.65 SRC is the source-based (i.e. QE as a metric)
ONLINE-M 5.43 4.71 2.98 6.28
PRISM thatusesamultilingualMTmodeltoforce-
ANVITA 6.08 5.17 2.97 7.15
NLLB_MBR_BLEU 6.36 6.57 3.39 6.78 decodeandscorethecandidatetranslationcondi-
NLLB_Greedy 6.57 6.70 2.95 7.16 tionedonthesourcesentence.
Table4: MQMhumanevaluationsforgeneralMT2023. RANDOM-SYSNAME is a random metric that
LoweraverageerrorcountsrepresenthigherMTquality. takesthesystemnameastheonlyparameter. For
Systems above any solid line are significantly better each translation system, the metric computes the
thanthosebelow,basedonalldomainswithp<0.05.
mean value X as sha256(sysname)[0]%10. It
usesdiscretescores. Segment-levelscoresfollow
senilesab snoissimbusyramirp
)/moc.buhtig//:sptth(ytilibaliava
noitatic
eerf.fer
desivrepus
yrogetacdaorb
cirtem
uelbercas/tsopjm
)2002(.lateinenipaP
palrevolacixel
UELB
uelbercas/tsopjm
)2202(.latemaeTBLLN
palrevolacixel
UELBPS002F
uelbercas/tsopjm
)5102(c´ivopoP
palrevolacixel
FRHC
erocs_treb/regiiiT
)0202(.lategnahZ
ytiralimisgniddebme
EROCSTREB
truelb/hcraeser-elgoog
)0202(.latemalleS
✓
cirtemdenut-enfi
TRUELB
TEMOC/lebabnU
)a2202(.lateieR
✓
cirtemdenut-enfi
TEMOC
TEMOC/lebabnU
)b2202(.lateieR
✓
✓
cirtemdenut-enfi
IWIKTEMOC
scirtem-tm-cod/hcraeser-nozama
)2202(.latesokinreV
✓
cirtemdenut-enfi
ADTEMOC22TMWCOD
scirtem-tm-cod/hcraeser-nozama
)2202(.latesokinreV
✓
✓
cirtemdenut-enfi
ADIWIKTEMOC22TMWCOD
temoC-SM/rotalsnarTtfosorciM
)b2202(.lateimcoK
✓
✓
cirtemdenut-enfi
22-EQ-TEMOC-SM
msirp/bnospmoht
)b,a0202(tsoPdnanospmohT
✓
cirtemledom-TM
FERMSIRP
msirp/bnospmoht
)b,a0202(tsoPdnanospmohT
✓
✓
cirtemledom-TM
CRSMSIRP
)elbaliavaton(
—
✓
enilesabmodnar
EMANSYS-MODNAR
isiy/oluikihc
)9102(oL
ytiralimisgniddebme
1-ISIY
)elbaliavaton(
—
✓
cirtemdenut-enfi
22TEMOC-IRBILAC
)elbaliavaton(
—
✓
✓
cirtemdenut-enfi
EQ-22TEMOC-IRBILAC
ved-nairam/tmn-nairam
)3202(.lateadwoG
✓
✓
cirtemdenut-enfi
22TMW-22DIOTEMOC
32tmw-scirtem-tm-uelbe/leanum
)3202(imcoKdnayhsarkoNlE
ytiralimisgniddebme
UELBE
amall_debme/onaerDneroS
)a3202(.lateonaerD
ytiralimisgniddebme
AMALL_DEBME
ABMEG/rotalsnarTtfosorciM
)3202(nnamredeFdnaimcoK
✓
cirtemdesab-tpmorpMLL
MQM-ABMEG
)elbaliavaton(
)3202(.lateuW
✓
ytiralimisgniddebme
EROCSTREB-GK
eSETaM/PLNazneipaS
)2202(.lateallerreP
✓
cirtemdenut-enfi
ESETAM
)elbaliavaton(
)3202(.lateraksaN
✓
✓
cirtemdenut-enfi
EQ-XCIRTEM-RBM
noissimbuS22TMW/redoCaynanA
)3202(avatsavirhSdnaeejrehkuM
ytiralimisgniddebme&lacixel
4EEM
xcirtem/hcraeser-elgoog
)3202(.lateaksaruJ
✓
cirtemdenut-enfi
32-XCIRTEM
xcirtem/hcraeser-elgoog
)3202(.lateaksaruJ
✓
✓
cirtemdenut-enfi
EQ-32-XCIRTEM
scirtem-mll-tneiciffe/G2LN
)3202(.latevoksiV
✓
cirtemdenut-enfi
RALUGER-ESBAL-EROCS-ERM
erocSES/zh8991ux
)2202(.lateuX
✓
cirtemdenut-enfi
EROCSES
F_margnekot/onaerDneroS
)b3202(.lateonaerD
palrevolacixel
F_MARGNEKOT
TEMOC/lebabnU
)3202(.lateorierreuG
✓
cirtemdenut-enfi
ELBMESNE-TEMOCX
TEMOC/lebabnU
)3202(.lateorierreuG
✓
✓
cirtemdenut-enfi
ELBMESNE-EQ-TEMOCX
misLX/redoCaynanA
)3202(avatsavirhSdnaeejrehkuM
✓
cirtemdenut-enfi
MISLX
gnidrageR
.scirtemdenut-enfidnaytiralimisgniddebme,lacixel
:sessalcrojam3otniscirtemezirogetaceW
.ksatscirtemehtrofsnoissimbusyramirpdnascirtemenilesaB
:5elbaT
.)nmulocdr3(gninut-enfirofslebalcitehtnysesutahtscirtemdnaMQMroADsahcusserocsytilauqnamuhesutahtscirtemevahewscirtemdenut-enfi
Gaussian distribution around mean value X (in a COMETKIWI XL/XXL (Rei et al., 2023)
range0-9)andstandarddeviationof2. shares the same architecture as the COMETKIWI
baseline but replaces InfoXLM with XLM-R
YISI-1(Lo,2019) isaMTevaluationmetricthat
XL(3.5B)andXXL(10.7B).Intermsoftraining
measures the semantic similarity between a ma-
data,thesemodelsaretrainedonthesamedataset
chinetranslationandhumanreferencesbyaggre-
as COMETKIWI, along with newly released Di-
gatingtheIDF-weightedlexicalsemanticsimilari-
rectAssessments(DA)forIndianlanguages,which
tiesbasedonthecontextualembeddingsextracted
wereintroducedasadditionaltrainingdataforthis
frompre-trainedlanguagemodels(e.g. RoBERTa,
year’sQualityEstimation(QE)sharedtask(Blain
CamemBERT,XLM-RoBERTa,etc.).
etal.,2023).
4.2 MetricSubmissions
EBLEU(ElNokrashyandKocmi,2023) String-
Therestofthissectionsummarizestheparticipat- basedmetricssuchas BLEU and CHRF dependon
ingmetrics. string similarity as proxy for meaning similarity
betweencandidateandtargetsentences. EBLEU
CALIBRI-COMET22 and CALIBRI-
stands for ‘Embedded BLEU’ and is loosely in-
COMET22-QE apply a post-processing
spiredbyit. In EBLEU,wematchcandidateand
approach to ratings provided by COMET. It
targettokensapproximatelyusingnon-contextual
uses Unbabel/wmt22-comet-da as the
word embeddings and a word-to-word similarity
backboneforthereferenced CALIBRI-COMET22
mapinaformwehavedubbed“relativemeaning
and Unbabel/wmt22-cometkiwi-da as
diffusiontensors”.
the backbone for the unreferenced CALIBRI-
COMET22-QEmetric. Theinformationwhether EMBED_LLAMA(Dreanoetal.,2023a) relies
atranslationiserror-freefromMQMratings(e.g. on pretrained Llama2 embeddings, without any
underGoogle’sMQMerrorweighting,error-free fine-tuning, to transform sentences into a vector
translationshaveascoreof0)canberecovered. It spacethatestablishesconnectionsbetweengeomet-
then aims to calibrate the scores of the backbone ricandsemanticproximities. Thismetricsdraws
model with respect to this binary error-freeness inspirationfromWord2vec,andutilizescosinedis-
label using isotonic regression. During test time, tance for the purpose of estimating similarity or
it takes the samples for a given tuple (lang-pair, dissimilaritybetweensentences.
test-set, domain, ref, system-id) and employs a
heuristicstrategytoselectsamplesfromprevious GEMBA-MQM(KocmiandFedermann,2023)
yearsthatmatchthetestsamplescoredistribution. is a LLM-enabled metric for error quality span
It then fits an isotonic regression model to the marking. It uses three-shot prompting with the
selected samples and transforms the test scores GPT4model. IncontrasttoEAPrompt(Luetal.,
accordingly. Themainideaisthatinthisway,the 2023),itdoesnotrequirelanguagespecificexam-
averagedsystem-levelscorecanbeinterpretedas plesandrequiresonlyasingleprompt.
thefractionoferror-freetranslations.
HWTSC-EE-METRIC and KG-
COMETOID22 (Gowda et al., 2023) is a BERTSCORE (Wu et al., 2023) EE stands
reference-free metric created using knowledge for Entropy Enhanced MT Metrics and aims at
distillation from reference-based metrics. Using achievingamorebalancedsystem-levelratingby
COMET-22 as a teacher metric, it scores the MT assigning weights to segment-level scores pro-
outputssubmittedtotheWMTNews/GeneralMa- ducedbyMTmetrics. Theweightsaredetermined
chineTranslationtasksince2009. Astudentmet- by the difficulty of a segment determined by the
ric,calledCOMETOID22,isthentrainedtomimic entropy between the hypothesis-reference pair.
theteacherscoreswithoutusingreferencetransla- This year, the COMET metric is utilized as the
tion. Thestudentmetrichasthesamearchitecture backboneofourEEmetrics. Themodelweuseis
as COMET-QE, and is initialized with pretrained WMT22-COMET-DA.
weights from InfoXLM, a multilingual language KG-BERTSCORE incorporates multilingual
model. Wesubmitthreevariants: COMETOID22- knowledgegraphintoBERTSCOREandgenerates
WMT{21,22,23}, where the suffix indicates the thefinalevaluationscorebylinearlycombiningthe
trainingdatacut-offyear. resultsofKGSCOREandBERTSCORE,inwhich
we use COMET-QE to calculate BERTSCORE “c”, forboththereference-basedandQEmetrics.
thisyear. The“b”variantadditionallytrainsonMQMdata
from 2022 and the “c” variant uses the PaLM-2
MATESE (Perrella et al., 2022) leverages
languagemodel(Aniletal.,2023)toinitializethe
transformer-basedencoderstoidentifyerrorspans
metricinsteadofmT5.
intranslations,andclassifytheirseveritybetween
MinorandMajor. Differentlyfromlastyear’sver-
MRE-SCORE (Viskov et al., 2023) is a trained
sion,MATESEisnowbasedonDeBERTaforeval-
metric that is based on the encoder part of mT0-
uatingtranslationstowardsEnglish,andInfoXLM
largemodel. Weuseaconcatenationofsource,ref-
forGermanandRussian. Furthermore,ithasbeen
erenceandhypothesistextsforinput. Additionally,
re-trained using also the MQM data released at
someofthevariantsofthemodelusescontextual
WMT2022.
embeddingsfromLaBSE.
MBR-METRICX-QE(Naskaretal.,2023) MBR
decodingwithneuralutilitymetricslikeBLEURT SESCOREX (Xu et al., 2023b) and IN-
is known to be effective in generating high qual- STRUCTSCORE(Xuetal.,2023c) SESCOREX
ity machine translations. We use the underlying is an improved version of SESCORE2 (Xu et al.,
techniqueofMBRdecodinganddevelopanMBR 2023a). Building upon the established strengths
basedreference-freequalityestimationmetric. Our of SESCORE2, we utilize its framework for syn-
methodusesanevaluatormachinetranslationsys- thetic data generation to pre-train our scoring
tem and a reference-based utility metric (specifi- model. To further elevate the performance of
callyBLEURTandMETRICX)tocalculateaquality SESCOREX,weintroducetwokeymodifications:
estimationscoreofamodel. Wereportresultsre- fine-tuninghumanratingdataandtransitioningthe
latedtocomparingdifferentMBRconfigurations scoringbackbonemodeltotheMT5-xlmodel. IN-
andutilitymetrics. STRUCTSCOREisanopen-source,explainableeval-
uationmetricfortextgeneration. Utilizingexplicit
MEE4 (MukherjeeandShrivastava,2023) is
humanguidelinesandGPT4’simplicitknowledge,
an unsupervised, reference-based metric (an im-
wefine-tuneanLlamamodeltoprovideevaluation
proved version of MEE) focusing on computing
metrics along with diagnostic reports that align
contextualandsyntacticequivalences,alongwith
withhumanassessments. Unliketraditionalneural
lexical, morphological, and semantic similarity.
metrics, INSTRUCTSCORE evaluates text genera-
Thegoalistocomprehensivelyevaluatethefluency
tionbyprovidingaqualityscorebasedondetailed
andadequacyofMToutputswhilealsoconsider-
errorexplanations.
ingthesurroundingcontext. Fluencyisdetermined
byanalysingsyntacticcorrelations,whilecontext
SLIDE (Raunaketal.,2023) Buildingmetrics
isevaluatedbycomparingsentencesimilaritiesus-
explicitly for document-level MT quality estima-
ing sentence embeddings. The ultimate score is
tionhasbeenchallengingowingtothelackoflarge-
derived from a weighted amalgamation of three
scaledocument-levelhumanannotateddatasets. In
distinctsimilaritymeasures: a)Syntacticsimilarity,
thissubmission,wepresentametricnamedSLIDE
whichisestablishedusingamodified BLEUscore.
(SlidingDocumentEvaluator),whichoperatesat
b) Lexical, morphological, and semantic similar-
the span of multiple sentences or paragraphs by
ity,quantifiedthroughexplicitunigrammatching.
way of an overlapping sliding window. SLIDE
c)Contextualsimilarity,gaugedbysentencesimi-
feeds each chunk into a source-based COMET
larityscoresobtainedfromtheLanguage-Agnostic
model, with scores over overlapping chunks ac-
BERTmodel.
cumulatedtoproduceasystem-levelscore. SLIDE
METRICX-23andMETRICX-23-QE(Juraska ismotivatedbytwoideas: (1)SinceCOMET’sun-
et al., 2023) are learned reference-based and derlyingencoderistrainedonwidercontexts,we
reference-free (respectively) regression metrics mightobservegeneralizableevaluationbehaviour
basedonthemT5encoder-decoderlanguagemodel. beyondtypicalsentences-levellengths,withincer-
Theyfurtherfine-tunethemT5-XXLcheckpointon tainlengthlimitsand(2)sinceasentence’sevalua-
directassessmentdatafrom2015-2020andMQM tionwilldifferatdifferentpositionswithinadocu-
data from 2020 to 2021 as well as synthetic data. ment,itmaybehelpfultoevaluateeachsentence
There are two contrastive submissions, “b” and inmultipledifferentcontexts.
TOKENGRAM_F (Dreano et al., 2023b) is an 1, 1, 2, 3, 3, ...). Motivated by theoretical results
F-score-basedevaluationmetricformachinetrans- pertaining to combining rankings from different
lation that is heavily inspired by CHRF++. By knowledgesources(Colomboetal.,2022;Dwork
replacingword-gramswithtoken-gramsobtained etal.,2001),weestablishedanoverallrankingby
from contemporary tokenization algorithms, TO- simplyaveragingtheper-taskranks.
KENGRAM_Fcapturessimilaritiesbetweenwords Thisapproachhasseveraldisadvantages. First,
sharingthesamesemanticrootsandthusobtains it is difficult to incorporate new metrics into the
moreaccurateratings. comparison, since this requires not only comput-
ing the score of a new metric on 201 tasks, but
XCOMET-XL/XXL (Guerreiro et al., 2023) is
also comparing it to all existing metrics on each
anew COMET (Reietal.,2020)modelthatisde-
taskusingexpensiveresamplingsignificancetests.
signedtoidentifyerrorspansinsentencesandgen-
Addinganewmetricalsohastheundesirableeffect
erateafinalqualityscore,makingitamoreinter-
ofpotentiallycausingothermetricstoswapplaces
pretablelearntmetric. Thismetricisoptimizedfor
intheoverallrankings. Whilerankaveraginghas
both regression and sequence tagging, and it can
theoretical underpinnings, as noted above, these
beusedwithorwithoutreferences. XCOMET-QE
applytosettingsinwhichtheconstituenttaskspro-
submission results from the same model but run-
videonlyrankinginformationthemselves. Inorder
ninginferencewithoutareference. Thesemodels
to take advantage of richer information available
utilizeXLM-RXLorXXLastheirbackbonemod-
fromcorrelationstatistics,wederiveddenseranks
els,with XCOMET-XLhaving3.5Bparametersand
frompairwisesignificancetests,butthisrelieson
XCOMET-XXLhaving10.7Bparameters. Thetrain-
anadhocclusteringalgorithm,anditisnotclearto
ingprocessforthismetricoccursinstages,starting
whatextentouraverageranksaresupportedbythe
with DAs and then is fine-tuning on MQM data.
original theory. They also lack confidence infor-
XCOMET-ENSEMBLE is an ensemble between 1
mation,makingitdifficulttoquantifyconclusions
XL and 2 XXL checkpoints that result from the
about the overall superiority of one metric over
differenttrainingstages.
another.
Thisyearweadoptedamuchsimplerapproach
XLSIM (Mukherjee and Shrivastava, 2023)
inordertoaddressthesedifficulties. Weusejust10
is a supervised reference-based metric that re-
maintasks,andcomputeanoverallscorebytaking
gressesonhumanscoresprovidedbyWMT(2017-
aweightedaverageofresultsfromeachtask. We
2022). Usingacross-linguallanguagemodelXLM-
performsignificancetestsoneachpairofmetrics
RoBERTa, we train a supervised model using a
foreachtaskasbefore,butalsodosoforeachpair
Siamesenetworkarchitecturewithcosinesimilar-
ofmetricsontheoverallaveragescore,allowingus
ityloss.
toestablishaclearerglobalranking. Theaverage
scoreforanewmetriccanbecomputedrelatively
5 MetaEvaluation
quickly, and it does not affect the scores of other
Ourmaingoalinevaluatingmetricsistoestablisha metrics. Significancetestsstillrequiretheexpen-
rankingthatreflectsametric’sperformanceacross sivestepofcomparingtoallothermetrics,butthey
arangeofsettingsandapplications. Combiningre- arenolongernecessaryforcomputingametric’s
sultsfromdifferentsettingsischallengingbecause rawoverallscore.
correlationswithhumangoldscoreshavedifferent We acknowledge that this approach is not per-
rangesandmaybesubjecttodifferingdegreesof fect. One problem is that we need to combine
noise. Therearealsomanywaysofmeasuringcor- correlations and accuracies that may have differ-
relation,withdifferentstrengthsandweaknesses, entdynamicranges. Forexample,themeanPear-
and it is often not clear which is best in a given soncorrelationacrossallmetricsforen→deatthe
setting. systemlevelis0.88withstandarddeviation0.24,
Last year, our approach was to define a large while at the segment level it is 0.39 with a stan-
numberof“tasks”(201intotal)thatvariedalong darddeviationof0.17. Averagingsystem-leveland
dimensionssuchaslanguagepair,domain,granu- segment-levelcorrelationswillthereforeeffectively
larity, correlation statistic, etc. For each task, we upweightthesystem-levelcontribution. Weexperi-
usedpairwisesignificanceteststoestablishadense mentedwithdifferentweightingstocompensatefor
clusteredrankingofparticipatingmetrics(e.g.,1, this,butfoundthattheydidnotmakealargediffer-
language refused scoredref task lang level correlation wt
en→de A –
1 all system accuracy 3
he→en B A
zh→en A –
2 en→de system Pearson 1
3 en→de segment Pearson 1
Table6: Useofreferencetranslations.
4 en→de segment acc∗ 1
eq
ence,anddecidedtouseequalweightsforsimplic- 5 he→en system Pearson 1
ity. Anotherproblemisthatwedonotaccountfor 6 he→en segment Pearson 1
dependenciesamongtasks. Althoughalltasksare 7 he→en segment acc∗ 1
eq
atleastsomewhatcomplementary,many–suchas
8 zh→en system Pearson 1
system-levelandsegment-levelcorrelations—are
9 zh→en segment Pearson 1
basedonthesameunderlyingdata,andthusviolate
10 zh→en segment acc∗ 1
theassumptionsofourhypothesistests. Weleave eq
more sophisticated inference approaches such as
Table7: Tasksandweighting.
proposed by Dror et al. (2017) or Hagmann and
Riezler(2023)forfuturework. • System-level pairwise ranking accuracy (as
proposedbyKocmietal.,2021). Thisiscom-
5.1 TaskAttributes
puted over data pooled across all three lan-
Tasks are identified by unique value assignments guagepairs.
foreachofthefollowingattributes: language,level,
and correlation statistic. Unlike last year, we no • Segment-levelpairwiserankingaccuracywith
longerhavetasksspecifictodifferentdomains,as tiecalibration(asproposedbyDeutschetal.,
domainsdifferacrosslanguagesthisyear. Wealso 2023). We use the acc∗ variant to compare
eq
dropthe"include-human"vs"no-human"distinc- vectorsofmetricandgoldscoresforeachseg-
tion, and always score reference translations that ment,thenaveragetheresultsoversegments.
arenotusedbythemetrics. AsshowninTable6,
• System- and segment-level Pearson correla-
Hebrew→English is the only language pair for
tion. Atthesegmentlevel,weflattenmatrices
which such a reference is available. Finally, last
of system × segment scores into vectors be-
yearweusedthreedifferentaveragingmethodsfor
forecomparingthem.
eachcorrelationstatisticatthesegmentlevel;this
yearwechooseonlyonemethodforeachsegment-
levelcorrelation. 5.2 TasksandWeighting
Attributesareasfollows:
Table7showsthecompletelistoftasksandtheir
Language weights. All tasks receive a weight of 1, except
forsystem-levelaccuracy,whichhasaweightof3
Language pairs include those for which
becauseitcombinesdatafromallthreelanguage
we have MQM ratings—English→German,
pairs.
Hebrew→English, and Chinese→English—plus
Tocomputeaglobalscoreforeachmetricacross
all,whichindicatesallpairspooledtogether.
all tasks, we first map Pearson correlations from
Level theirnaturalrangeof[−1,1]intothe[0,1]rangeof
Wecomputedcorrelationsatthesystemleveland theaccuracyscores,thentakeaweightedaverage
thesegmentlevel. ForEnglish→German,segments oftheresults.
are paragraphs; for the two other language pairs,
5.3 RankAssignment
they are sentences. System-level scores for hu-
manratingsandforallmetricsthatdidnotsupply For each task, we assign ranks to metrics based
an explicit system-level score are averages over on their significance clusters. To do so, we com-
segment-levelscores. pareallpairsofmetricsanddeterminewhetherthe
differenceintheircorrelationscoresissignificant,
Correlation/accuracy
accordingtothePERM-BOTHhypothesistestof
Wecomputedthreecorrelation/accuracystatistics Deutsch et al. (2021). We use 1000 re-sampling
selectedtoprovidecomplementaryinformation: runsandsetp = 0.05. AsadvocatedbyWeietal.
(2022), we divide the sample into blocks of 100, 6 MainResults
computesignificanceaftereachblock(cumulative
AswehaveseeninSection5,themainresultsare
over all blocks sampled so far), and stop early if
the overall scores by taking a weighted average
thep-valueis< 0.02or> 0.50.
of the results from the ten main tasks, including
Theacc∗ statisticcreatesaproblemforsignifi-
eq system-level and segment-level tasks in different
cancetestingbecauseitoptimizesalatenttiethresh-
translation directions. Similar to last year, since
oldforeachmetriconeachtestset(justonethresh-
themainusecaseofautomaticmetricsistorank
oldforallitem-wisescorevectors). Sincetheper-
systems,system-levelaccuracyhasa1/4weighton
mutation test for comparing two metrics creates
the final score with the remaining 3/4 distributed
twonewvectorsbyrandomlyswappingelements
over9differentsettings.
oftheoriginalvectorsoneachdraw,thisnecessi-
Table1showstheofficialscoresandrankingsof
tatestheveryexpensivestepoffindingtwonewtie
allbaselinesandprimarysubmissions. Table8and
thresholds for each draw. To reduce the expense,
9showthescoresandrankingsofeachindividual
weusedthefollowingapproximateprocedure. First
taskatsystemlevelandsegmentlevel,respectively.
findanoptimalthresholdforeachinputmetricon
Similar to last year’s results, neural metrics per-
thecurrenttestset,thencreateallpairsofitem-wise
form significantly better than lexical metrics. Of
scoresandassignacorrect/incorrectstatustoeach
the32evaluatedmetrics, BLEU, F200SPBLEUand
pair by examining whether the metric’s ranking
CHRF areranked28th,24thand29threspectively.
matchesthehumanranking. Thenperformtheper-
Ontheotherhand,fine-tunedneuralbaselinemet-
mutationtestonthesepairwisestatusvectorsrather
rics,likeCOMETandBLEURT-20,remainranked
thantheoriginalscorevectors. Thisapproximation
higherthanseveralofthenewprimarysubmissions.
hasmoredegreesoffreedomthantheoriginaltest,
Theyaresurpassedonlybysubmissionsrelyingon
andcansamplepairsthatwouldneverresultfrom
significantlylargermodels.
swappingtheoriginalscorevectors,butourexperi-
Itisworthnotingthatthebest-performingbase-
mentsshowedthatitisareasonableproxyforthe
line, COMETKIWI, along with four of the seven
correctprocedure.
top-performingprimarysubmissions,arereference-
Tocomputeoverallp-valuesbasedonweighted
free. As we will elaborate on in a later section
averagescoresoftwometricsacrossalltasks,we
(Section8),therearequalityissueswithhumanref-
cachetheresultsofthedrawsfortheper-tasksig-
erencetranslations. Thishighlightsthechallenge
nificancetests. Inallcases,thesearevectorsofK
ofensuringrobustnesstopoor-qualityreferences
pairs of correlation or accuracy statistics. Where
forreference-basedmetrics. Incaseswhereahigh-
K < 1000duetoearlystopping,weduplicateele-
qualityhumanreferenceisnotavailable,reference-
mentstoget1000examples. Thenforiin1..1000
freemetricscanserveasmorerobustalternatives.
wecomparetheweightedaverageofthepairsfrom
Overall, XCOMET-Ensemble is the best per-
theithdrawacrossalltasks,andrecordtheresults
forming metric in terms of average scores over
toproduceanoverallp-value.
the10meta-evaluationsettings,withastatistically
significantadvantageoverallothermetrics. Itcon-
sistentlycorrelatesbestwithhumanMQMscores
Clustering
atsegmentlevelforalltranslationdirections,and
Given significance results (p-values) for all pairs itisrankedatworstinthe2ndsignificancecluster
of metrics, we assign ranks as follows. Starting forallsystem-levelmeta-evaluationtasks.
with the highest-scoring metric, we move down Figure 1 shows the correlation scores split by
the list of metrics in descending order by score, translation direction. There are two key observa-
andassignrank1toallmetricsuntilweencounter tions: 1) a majority of the metrics have higher
thefirstmetricthatissignificantlydifferentfrom correlations for en→de among the three transla-
any that have been visited so far. That metric is tion directions, except for MRE-SCORE-LABSE-
assignedrank2,andtheprocessisrepeated. This REGULARand EBLEU,thatperformsubstantially
continues until all metrics have been assigned a better for he→en, and YISI-1 and BERTSCORE,
rank. Note that this is a greedy algorithm, and that perform equally in en→de and he→en; 2)
henceitcanplacetwometricsthatarestatistically reference-based metrics struggle for zh→en due
indistinguishableindifferentclusters. to the reference quality, except for XCOMET-
en→de,he→en,zh→en en→de he→en zh→en
accuracy pearson pearson pearson
Metric avg-corr task1 task2 task5 task8
XCOMET-Ensemble 1 0.825 1 0.928 2 0.980 1 0.950 2 0.927
XCOMET-QE-Ensemble* 2 0.808 1 0.908 2 0.974 2 0.909 3 0.892
MetricX-23 2 0.808 1 0.908 2 0.977 2 0.910 4 0.873
GEMBA-MQM* 2 0.802 1 0.944 1 0.993 2 0.939 1 0.991
MetricX-23-QE* 2 0.800 2 0.892 2 0.969 3 0.858 4 0.859
mbr-metricx-qe* 3 0.788 2 0.880 2 0.976 2 0.915 2 0.936
MaTESe 3 0.782 2 0.904 4 0.918 2 0.906 3 0.889
CometKiwi* 3 0.782 1 0.904 3 0.946 3 0.860 2 0.963
COMET 3 0.779 2 0.900 1 0.990 2 0.940 3 0.898
BLEURT-20 3 0.776 2 0.892 1 0.990 2 0.937 4 0.880
KG-BERTScore* 3 0.774 2 0.884 4 0.926 2 0.908 2 0.962
sescoreX 3 0.772 2 0.892 3 0.952 3 0.901 5 0.797
cometoid22-wmt22* 4 0.772 2 0.880 2 0.973 4 0.839 2 0.940
docWMT22CometDA 4 0.768 2 0.904 1 0.990 2 0.922 3 0.907
docWMT22CometKiwiDA* 4 0.767 2 0.900 2 0.970 2 0.906 2 0.965
Calibri-COMET22 4 0.767 1 0.904 2 0.963 2 0.930 4 0.863
Calibri-COMET22-QE* 4 0.755 2 0.863 2 0.978 4 0.778 2 0.934
YiSi-1 4 0.754 2 0.871 4 0.925 2 0.917 4 0.823
MS-COMET-QE-22* 5 0.744 2 0.871 3 0.959 5 0.721 3 0.901
prismRef 5 0.744 2 0.851 4 0.920 1 0.956 6 0.762
mre-score-labse-regular 5 0.743 2 0.888 3 0.942 1 0.958 3 0.903
BERTscore 5 0.742 2 0.871 5 0.891 3 0.895 5 0.810
XLsim 6 0.719 2 0.855 4 0.925 3 0.887 5 0.796
f200spBLEU 7 0.704 3 0.819 4 0.919 4 0.805 6 0.772
MEE4 7 0.704 3 0.823 5 0.861 3 0.879 6 0.743
tokengram_F 7 0.703 3 0.815 5 0.858 3 0.878 5 0.795
embed_llama 7 0.701 3 0.831 5 0.861 4 0.841 5 0.785
BLEU 7 0.696 3 0.815 4 0.917 5 0.769 7 0.734
chrF 7 0.694 3 0.795 5 0.866 4 0.776 5 0.809
eBLEU 7 0.692 2 0.859 4 0.918 2 0.911 7 0.727
Random-sysname* 8 0.529 4 0.578 6 0.357 6 0.209 8 0.093
prismSrc* 9 0.455 5 0.386 6 -0.327 6 -0.017 8 -0.406
Table8: Resultsonsystem-leveltasksformainlanguagepairs. Rowsaresortedbytheoverallaveragecorrelation
acrossall10tasks(leftmostcolumn). Starredmetricsarereference-free,andunderlinedmetricsarebaselines.
ENSEMBLE and SESCOREX. The reason for the system-level, as we only have a limited number
significantdropincorrelationforhe→enisunclear. of MT systems. Although most of the metrics
This drop is observed across almost all metrics, computethesystem-levelscorebyaveragingtheir
whether they are trained or untrained, reference- segment-level scores, we observe that high cor-
free or reference-based, and they exhibit varying relations between automatic metrics and human
degreesofdegradation. judgmentsatthesegmentleveldonotnecessarily
Wecontinuetobeinterestedinmetrics’abilityto guaranteehighcorrelationsatthesystemlevel. For
generaliseacrossdomains. InFigure2,3and4we example, PRISMSRC is in the middle of the pack
presenttheperformanceofeachmetricacrossdif- andhasmoderatePearson’scorrelationatsegment
ferentdomainsineachtranslationdirection. Most level for en→de. However, it is negatively corre-
metricsperformwellinevaluatingtranslationinthe latingwithhumanjudgementswhenevaluatingthe
user reviews domain across translation direction, samelanguagepairatsystemlevel.
despitelackingannotateddatainthatdomain. Fur-
therinvestigationisrequiredtounderstandwhether 7 Understandingmetrics’scoresbeyond
thisisbecausethetranslationqualityofMToutput correlation
ismorediverseintheuserreviewsdomain,making
iteasierformetricstoaccuratelydiscriminate. In the past few years, we demonstrated that new
Figure5showstheaveragecorrelationsofmet- metricscorrelatebetterwithhumanjudgmentsthan
ricswhengroupedseparatelybysystem-leveland BLEU does. Somenewbaselinemetricsevencon-
segment-level tasks. Many metrics fall into the sistently outperform BLEU for consecutive years
same significance cluster when evaluated on the across translation directions and domains. How-
en→de en→de he→en he→en zh→en zh→en
pearson acc-t pearson acc-t pearson acc-t
Metric task3 task4 task6 task7 task9 task10
XCOMET-Ensemble 1 0.695 1 0.604 1 0.556 1 0.586 1 0.650 1 0.543
XCOMET-QE-Ensemble* 2 0.679 3 0.588 3 0.498 4 0.554 1 0.647 3 0.533
MetricX-23 4 0.585 1 0.603 1 0.548 2 0.577 2 0.625 3 0.531
GEMBA-MQM* 6 0.502 5 0.572 5 0.401 3 0.564 6 0.449 5 0.522
MetricX-23-QE* 3 0.626 2 0.596 2 0.520 3 0.564 1 0.647 4 0.527
mbr-metricx-qe* 4 0.571 3 0.584 5 0.411 4 0.553 5 0.489 2 0.537
MaTESe 5 0.554 9 0.528 4 0.459 5 0.550 4 0.511 12 0.479
CometKiwi* 7 0.475 5 0.569 7 0.387 6 0.544 6 0.442 4 0.525
COMET 8 0.432 4 0.574 5 0.401 8 0.532 8 0.396 7 0.514
BLEURT-20 7 0.484 5 0.572 8 0.382 10 0.519 9 0.378 6 0.518
KG-BERTScore* 8 0.451 7 0.556 8 0.382 7 0.537 7 0.430 6 0.516
sescoreX 5 0.519 6 0.563 7 0.385 15 0.484 3 0.536 9 0.499
cometoid22-wmt22* 8 0.441 4 0.578 9 0.365 11 0.515 5 0.479 7 0.515
docWMT22CometDA 10 0.394 7 0.559 10 0.339 13 0.497 10 0.353 10 0.493
docWMT22CometKiwiDA* 8 0.444 8 0.547 12 0.286 14 0.489 8 0.387 10 0.493
Calibri-COMET22 9 0.413 10 0.522 5 0.401 11 0.515 8 0.396 14 0.474
Calibri-COMET22-QE* 8 0.441 12 0.483 6 0.395 12 0.506 6 0.443 10 0.491
YiSi-1 11 0.366 8 0.542 6 0.395 8 0.529 11 0.290 8 0.504
MS-COMET-QE-22* 12 0.310 8 0.546 12 0.295 13 0.498 9 0.367 9 0.498
prismRef 6 0.516 10 0.518 11 0.319 9 0.528 14 0.183 8 0.504
mre-score-labse-regular 17 0.111 9 0.530 8 0.378 10 0.522 16 0.145 12 0.481
BERTscore 12 0.325 9 0.528 10 0.335 11 0.515 12 0.236 9 0.499
XLsim 13 0.239 9 0.527 14 0.233 16 0.480 17 0.111 15 0.464
f200spBLEU 14 0.237 9 0.526 14 0.230 18 0.447 18 0.108 13 0.476
MEE4 16 0.202 9 0.529 13 0.256 19 0.441 18 0.105 12 0.480
tokengram_F 15 0.227 10 0.520 14 0.226 17 0.461 20 0.060 11 0.485
embed_llama 13 0.250 12 0.483 15 0.215 20 0.430 15 0.161 16 0.447
BLEU 16 0.192 10 0.520 15 0.220 19 0.442 17 0.119 14 0.472
chrF 14 0.232 10 0.519 15 0.221 17 0.460 19 0.063 11 0.485
eBLEU 19 -0.011 11 0.512 16 0.131 18 0.445 22 -0.084 14 0.473
Random-sysname* 18 0.064 14 0.409 17 0.041 20 0.428 21 0.018 18 0.381
prismSrc* 9 0.425 13 0.426 16 0.140 19 0.441 13 0.223 17 0.421
Table9: Resultsonsegment-leveltasksformainlanguagepairs. Rowsaresortedbytheoverallaveragecorrelation
acrossall10tasks(leftmostcolumninTable8). Starredmetricsarereference-free,andunderlinedmetricsare
baselines.
ever, the research community is still reluctant to 7.1 CorrespondencetoMQMscores
adopt newer and better automatic MT evaluation significance
metricsinpractice. OneofthereasonsisthatMT
First, we follow Lo et al. (2023a) to study the re-
researchers have established some “common be-
lationship between statistically significant differ-
liefs”abouttherelationshipbetweenBLEUandac-
encesinhumanscoresandthemagnitudeofmetric
tualtranslationquality,andsimilarintuitionsabout
differences. Specifically,werunaone-sidedpaired
newmetricshaveyettocrystallize. Thus,thisyear,
t-testwithanequalvarianceassumptionforeach
weconducttwoadditionalanalysesbeyondcorrela-
systempaironsegment-levelMQMscores. After
tionwithhumantounderstandthemeaningofthe
that, we fit the corresponding metric score differ-
scoredifferencesthatmetricspresentwithrespect
ences and the p-values of the t-test on the MQM
to the statistical significance of MT system rank-
scorestoanisotonicregression(Robertsonetal.,
ings according to human annotations and metric
1988),thatpredictswhetherthehumanMQMscore
scores. Our results should NOT be used as argu-
difference will be significant given the metric’s
ments to forego significance tests or appropriate
score difference. Isotonic regression produces a
human evaluation. These analyses only support
non-decreasingfunctionwheretheclassifieroutput
anintuitivesenseofmetricscoremeaningstoen-
can be interpreted as a confidence level.9 We set
courage broader adoption of new automatic MT
p < 0.05 as the significance level of MQM
evaluationmetrics. mqm
9https://scikit-learn.org/stable/
modules/isotonic.html
Figure3: Averagemetrics’correlationwithhumanin
Figure1: Averagemetrics’meta-evaluationscoresin tasksgroupedbydomaininhe→en.The“mixed“group
tasks grouped by translation direction. The “mixed“ istheaveragecorrelationinallhe→entasks.
groupistheaccuracyscoreofthemetricsintask1.
Figure2: Averagemetrics’correlationwithhumanin Figure4: Averagemetrics’correlationwithhumanin
tasksgroupedbydomaininen→de.The“mixed“group tasksgroupedbydomaininzh→en.The“mixed“group
istheaveragecorrelationinallen→detasks. istheaveragecorrelationinallzh→entasks.
offsof∆M whenPr(p < 0.05|∆M) = 0.8
mqm
foreachmetricandtranslationdirections.
We run the leave-one-system-out cross valida-
tion and Table 10 shows that the range of preci-
sion in the cross validation are consistently high
acrossmetrics,withtheexceptionof BLEU, CHRF,
PRISMSRC,RANDOM-SYSNAMEandSLIDE. This
meansthemetriccut-offswefindusingtheregres-
sionmodelarereliable.
Contrarytothecommonbeliefthat2BLEUim-
provementrepresents“significant”or“notableby
human”improvementintheactualtranslationqual-
ity, our analyses show that 2.2 BLEU is the mini-
mumrequiredimprovementforahighconfidence
(80%) that MQM annotators to mark significant
differencesinthetranslationoutputforonetrans-
lationdirection(zh→en)andthatthresholdwould
beashighas11BLEUforen→de. Table10serves
asareferencebetween BLEU differencesanddif-
ferencesinsomeofthemodernmetrics,andassists
metricusersinunderstandingscoresprovidedby
modern metrics. For example, when evaluating
he→en translation quality, we see that a BLEU
difference of 3.5 corresponds to 80% confidence
that the metric’s ranking of the two MT systems
will match with the decision made by human an-
notatorswithasignificantdifference. Meanwhile,
Figure5: Averagemetrics’correlationwithhumanin
a COMET score difference of 0.014 would have
tasksgroupedbygranularitylevel.
thesame80%chanceofhumanjudgedsignificant
difference.
scores. Thus,theoutputoftheisotonicregression 7.2 Correspondencetometricscores
functioncanbeviewedasPr(p < 0.05|∆M) significance
mqm
wherep isthep-valueofthet-testontheMQM
mqm Inspired byMarie (2022), werun a studysimilar
scoresforeachsystempairand∆M isthemetric
to that in the previous subsection but on the rela-
scoredifference.
tions between statistically significant differences
Figure 6 shows the (log) p-value of one-sided in metric scores and the magnitude of metric dif-
paired t-test on the MQM scores against the cor- ferences. Insteadofone-sidedt-testonMQM,the
responding BLEU and COMET score difference p-values are now obtained by running statistical
for each system pair in en→de. Figures 9-14 in significancetestswithbootstrapresamplingonthe
appendix D, show the same analyses for all met- metricscoresforeachsystempair. Similarly, we
ricsandtranslationdirections. Foreachmetric,we fitthecorrespondingmetricscoredifferencesand
canchooseaparticularlevelofconfidence(i.e.,a thep-valuesofthesignificancetesttoanisotonic
pointalongthey-axisontheright)togivemetric regression for predicting whether the translation
scoredifferencecut-offs(i.e.,apointalongthex- qualityimprovementasindicatedbythemetricwill
axis)thatthismetricdifferencereflectssignificant besignificantgiventhemetricscoredifference. We
MQMscoredifferences. Drawingahorizontalline setp < 0.05andthus,theoutputoftheisotonic
M
fromtheconfidencelevel,say80%,totheredline regressionfunctionisnowPr(p < 0.05|∆M),
M
enablesustofindtheminimummetricdifference wherep isthep-valueofthesignificanceteston
M
cut-offrequiredatthecorrespondingx-valuedown themetricscoresforeachsystempairand∆M is
from the red line, i.e. 11 for BLEU in Figure 6. themetricscoredifference.
Usingthislookupmethod,Table10showsthecut- Figure 7 shows the (log) p-value of the signifi-
Figure6: Logp-valueofone-sidedpairedt-testonMQMscores(p mqm)againstthemetric(left: BLEU, right:
COMET)scoredifferenceforeachsystempairinen→de. Theredlineistheisotonicregressionfittoalldata
points,representingPr(p <0.05|∆M). Note: forreadability,valuesofp areroundedupto0.0001when
mqm mqm
theyarelessthan0.0001.
en→de he→en zh→en
Metric min∆M c.v.precision min∆M c.v.precision min∆M c.v.precision
BERTSCORE 0.011 [75-100%] 0.0053 [83-100%] 0.0033 [75-100%]
BLEU 11 [33-100%] 3.5 [82-100%] 2.2 [75-100%]
BLEURT-20 0.041 [75-100%] 0.019 [100-100%] 0.013 [82-100%]
CALIBRI-COMET22 0.068 [71-100%] 0.031 [89-100%] 0.043 [80-100%]
CALIBRI-COMET22-QE 0.072 [82-100%] 0.020 [86-100%] 0.025 [67-100%]
CHRF 2.8 [25-100%] 3.2 [83-100%] 2.6 [86-100%]
COMET 0.030 [78-100%] 0.014 [88-100%] 0.013 [80-100%]
COMETKIWI 0.022 [67-100%] 0.014 [64-100%] 0.0098 [62-100%]
COMETOID22-WMT22 0.018 [86-100%] 0.0077 [71-100%] 0.011 [67-100%]
DOCWMT22COMETDA 0.027 [78-100%] 0.012 [82-100%] 0.014 [82-100%]
DOCWMT22COMETKIWIDA 0.026 [75-100%] 0.012 [64-100%] 0.0096 [71-100%]
EBLEU 0.022 [57-100%] 0.019 [83-100%] 0.017 [86-100%]
EMBED_LLAMA 0.062 [67-100%] 0.019 [80-100%] 0.020 [80-100%]
F200SPBLEU 4.6 [60-100%] 3.6 [75-100%] 3.5 [86-100%]
GEMBA-MQM 2.0 [89-100%] 1.0 [82-100%] 2.0 [69-100%]
KG-BERTSCORE 0.0097 [50-100%] 0.0097 [86-100%] 0.0079 [62-100%]
MATESE 0.99 [71-100%] 0.77 [75-100%] 0.70 [73-100%]
MBR-METRICX-QE 0.047 [75-100%] 0.026 [82-100%] 0.022 [75-100%]
MEE4 0.013 [71-100%] 0.024 [78-100%] 0.020 [86-100%]
METRICX-23 0.73 [100-100%] 0.29 [76-100%] 0.55 [83-100%]
METRICX-23-QE 0.53 [71-100%] 0.092 [67-100%] 0.49 [60-100%]
MRE-SCORE-LABSE-REGULAR 0.010 [67-100%] 0.016 [100-100%] 0.0064 [62-100%]
MS-COMET-QE-22 1.5 [80-100%] 1.4 [67-100%] 1.2 [60-100%]
PRISMREF 0.081 [75-100%] 0.14 [88-100%] 0.19 [83-100%]
PRISMSRC 0.036 [73-100%] 0.040 [33-100%] 0.022 [64-100%]
RANDOM-SYSNAME 7.8 [0-100%] 0.082 [67-90%] 5.0 [50-90%]
SESCOREX 0.38 [73-100%] 0.50 [89-100%] 0.62 [73-100%]
SLIDE 0.049 [78-100%] 0.017 [78-100%] 0.013 [58-100%]
XCOMET-ENSEMBLE 0.029 [88-100%] 0.0092 [83-100%] 0.012 [75-100%]
XCOMET-QE-ENSEMBLE 0.038 [86-100%] 0.012 [83-100%] 0.021 [67-100%]
XLSIM 0.015 [67-100%] 0.0073 [82100%] 0.0091 [70-100%]
YISI-1 0.0049 [67-100%] 0.0060 [80-100%] 0.0054 [75-100%]
Table10: Minimum∆M whenPr(p <0.05|∆M)=0.8foreachmetricindifferenttranslationdirections
mqm
roundto2significantfigures,andtherangeofprecisionfortheisotonicregressionmodelinleave-one-system-out
crossvalidation.
cancetestwithbootstrapresamplingonthemetric tions. Usingthesamelookupmethoddescribedin
scoresfor BLEU andCOMETscoredifferenceof the previous subsection, Table 11 shows the cut-
each system pair in en→de. Additional figures offsof∆M whenPr(p < 0.05|∆M) = 0.8for
M
(Figures15-20inappendixAppendixD)showthe eachmetricandtranslationdirections.
sameanalysesforallmetricsandtranslationdirec-
We run the leave-one-system-out cross valida-
Figure7: Logp-valueofsignificancetestwithbootstrapresampling(p )onsystem-levelmetricscoresagainst
M
each metric (left: BLEU, right: COMET) score difference for each system pair in en→de. The red line is the
isotonicregressionfittoalldatapoints,representingPr(p <0.05|∆M). Note: forreadability,valuesofp are
M M
roundedupto0.0001whentheyarelessthan0.0001.
en→de he→en zh→en
Metric min∆M c.v.precision min∆M c.v.precision min∆M c.v.precision
BERTSCORE 0.0026 [100-100%] 0.0012 [100-100%] 0.00085 [100-100%]
BLEU 1.1 [100-100%] 0.79 [100-100%] 0.58 [93-100%]
BLEURT-20 0.0081 [100-100%] 0.0041 [100-100%] 0.0024 [100-100%]
CALIBRI-COMET22 0.010 [91-100%] 0.0063 [100-100%] 0.0064 [100-100%]
CALIBRI-COMET22-QE 0.015 [100-100%] 0.0086 [89-100%] 0.0078 [92-100%]
CHRF 0.99 [100-100%] 0.68 [100-100%] 0.48 [100-100%]
COMET 0.0038 [100-100%] 0.0038 [90-100%] 0.0029 [100-100%]
COMETKIWI 0.0074 [91-100%] 0.0019 [100-100%] 0.0025 [93-100%]
COMETOID22-WMT22 0.0062 [82-100%] 0.0026 [100-100%] 0.0019 [100-100%]
DOCWMT22COMETDA 0.0033 [100-100%] 0.0013 [100-100%] 0.0023 [100-100%]
DOCWMT22COMETKIWIDA 0.0028 [100-100%] 0.0021 [100-100%] 0.0015 [100-100%]
EBLEU 0.0076 [90-100%] 0.0048 [100-100%] 0.0050 [100-100%]
EMBED_LLAMA 0.013 [100-100%] 0.0079 [100-100%] 0.0054 [100-100%]
F200SPBLEU 1.0 [100-100%] 0.94 [100-100%] 0.65 [100-100%]
GEMBA-MQM 0.52 [100-100%] 0.38 [100-100%] 0.35 [100-100%]
KG-BERTSCORE 0.0051 [100-100%] 0.0016 [100-100%] 0.00029 [93-100%]
MATESE 0.33 [100-100%] 0.20 [100-100%] 0.15 [100-100%]
MBR-METRICX-QE 0.0073 [100-100%] 0.0039 [100-100%] 0.0023 [100-100%]
MEE4 0.0029 [90-100%] 0.0067 [100-100%] 0.0054 [100-100%]
METRICX-23 0.23 [100-100%] 0.083 [90-100%] 0.089 [92-100%]
METRICX-23-QE 0.19 [100-100%] 0.072 [89-100%] 0.11 [100-100%]
MRE-SCORE-LABSE-REGULAR 0.0034 [100-100%] 0.0028 [100-100%] 0.0010 [100-100%]
MS-COMET-QE-22 0.49 [100-100%] 0.45 [88-100%] 0.18 [100-100%]
PRISMREF 0.018 [100-100%] 0.031 [100-100%] 0.020 [100-100%]
PRISMSRC 0.028 [100-100%] 0.025 [75-100%] 0.016 [100-100%]
RANDOM-SYSNAME 0.21 [100-100%] 0.14 [100-100%] 0.12 [100-100%]
SESCOREX 0.039 [100-100%] 0.10 [100-100%] 0.085 [100-100%]
XCOMET-ENSEMBLE 0.010 [90-100%] 0.0035 [100-100%] 0.0033 [100-100%]
XCOMET-QE-ENSEMBLE 0.0065 [100-100%] 0.0027 [100-100%] 0.0042 [93-100%]
XLSIM 0.0019 [100-100%] 0.0018 [100-100%] 0.0022 [100-100%]
YISI-1 0.0013 [100-100%] 0.0033 [73-100%] 0.00074 [100-100%]
Table 11: Minimum ∆M when Pr(p < 0.05|∆M) = 0.8 for each metric in different translation directions
M
roundto2significantfigures,andtherangeofprecisionfortheisotonicregressionmodelinleave-one-system-out
crossvalidation.
tion,andTable11showsthattherangeofprecision in BLEU with high confidence (80%), the BLEU
inthecrossvalidationareconsistentlyhighacross differences should be greater than 1.1 BLEU for
metrics. This means the metric cut-offs we find en→de. Table 11 serves as a reference of metric
usingtheregressionmodelarereliable. differenceswithrespecttostatisticalsignificance
withhighconfidence. Forexample,whenevaluat-
Our results, agreeing with Marie (2022), show
that to claim significant differences (p < 0.05) ingen→detranslationquality,weseethata BLEU
M
difference of 1.1 corresponds to 80% confidence zh→en en→de
thedifferenceisstatisticalsignificant. Meanwhile,
syntheticRef. 0.66 0.87
aCOMETscoredifferenceof0.0038wouldhave bestMT 2.10 3.72
refA 4.83 2.96
thesame80%chanceofstatisticalsignificance.
We have to emphasize again that our result
Table12: MQMscoresofthesyntheticreferences.
should NOT be interpreted as evidence to forego
significancetestsorappropriatehumanevaluation.
Instead,weareonlyprovidingassistancetobuild systemsinthisselectionprocess. Table13shows
anintuitiononthemeaningofthescoresprovided the number of segments contributed by each sys-
bythenewmetricstoencouragethetransitionaway tem to the generated synthetic reference transla-
from BLEU. tions. Unsurprisingly,thetopperformingMTsys-
temsarealsothemaincontributorstotheselected
8 SyntheticReferenceTranslations synthetic reference translation. For en→de, refA
(the original human-generated reference transla-
Reference-basedmetricscomparemachinetrans-
tion) provided the majority of the selected trans-
lations of source segments to human translations
lations,whileforzh→enGPT4-5shotisthemain
ofthosesamesourcesegmentstodeterminehow
contributor, reflecting that the human-generated
goodtheyare. Thequalityoftheunderlyinghuman
referencerefAforzh→enwasindeederror-prone.
translationiscrucialandcanimpactthequalityof
However, it is interesting to note that despite the
thepredictedscoresmorethanthechoiceofmet-
overall low quality of this human-generated ref-
ric(Freitagetal.,2020). Motivatedbythelowhu-
erence, our method still selected 209 segments
manratingsofrefAforChinese→English(Table4)
fromthistranslationasthelowest-errortranslation.
andtherelativelyhighrankingsofreference-free
This would appear to indicate that these human-
metrics (in comparison to other language pairs)
generatedreferencetranslationsarenotuniformly
forthislanguage-pair,weinvestigateamethodfor
bad,andonlyasubsetofthetranslationswereun-
generatingasyntheticreferencetranslationbased
reliableandcontainedmajorerrors. Apossibleex-
on the MT output and the corresponding MQM
planationcouldbethatmultipletranslatorsworked
ratings.
onthereference,however,weconfirmedwiththe
sponsortranslatingzh→enthatallsegmentswere
8.1 SyntheticReferenceGeneration
translatedwiththesametranslator.
Themainideaisstraightforward: Giventhesetof
translationsofWMT23GeneralMTSharedTask zh→en en→de
(generalMT2023) from the WMT campaign and GPT4-5shot 314 refA 243
theircorrespondingMQMratings,wegeneratea refA 209 GPT4-5shot 57
Lan-BridgeMT 157 ONLINE-B 36
new synthetic reference translation by choosing
ANVITA 142 ONLINE-A 20
foreachsegmentthetranslationthatreceivedthe HW-TSC 105 AIRC 20
lowestMQMerrorscoreastheselectedreference. IOL_Research 42 ONLINE-W 19
ONLINE-W 33 NLLB_Greedy 14
Theoriginalhumanreferencetranslation(i.e. refA)
ONLINE-Y 28 NLLB_MBR_BLEU 13
is considered as one of the possible translations ONLINE-B 26 ONLINE-G 10
ONLINE-A 24 ONLINE-Y 9
in this process, and MQM score ties are broken
ONLINE-G 21 Lan-BridgeMT 9
randomly. Table 12 shows the resulting MQM
NLLB_Greedy 20 ONLINE-M 8
score of the synthetic reference translations. We ZengHuiMT 18 ZengHuiMT 2
Yishu 18
wereabletoreducetheMQMscoretobelow1for
NLLB_MBR_BLEU 14
both tested language pairs (en→de and zh→en), ONLINE-M 6
whichcorrespondstoanaverageoflessthanone
minorerrorpersegment. Whilethismayseemlike Table 13: Number of segments contributed by each
a significant improvement, we must caution the systemtowardsthesyntheticreference.
readerthatthisisinessence"cherry-picking"based
ontheMQMratingsandmaythereforeintroduce
8.2 ImpactonMetrics
manyhiddenissues.
It is also interesting to understand how many Figure8comparesthesegment-levelandsystem-
segments come from each of the individual MT levelPearsoncorrelationsofallsubmittedmetrics
Figure8: PearsonCorrelationwhenusingeitherthesyntheticrefortheoriginalhumantranslationasreference
translation. QEmetricsarecolouredinyellow;ref-basedmetricsarecolouredinblue.
whenusingeithertheoriginalorthesyntheticref- canbeusedtomitigatebadreferencetranslations,
erence translation. Reference-based metrics are althoughitassumesobtainingMQMannotations
coloured in blue, while QE metrics are coloured andsuffersfromcherry-pickingbias.10
in orange. Obviously, as QE metrics do not use Anopenunansweredquestionremains: isital-
reference translations, their correlations are ex- waysnecessaryforareferencetranslationtobeof
actly the same. For Chinese→English, replac- higherqualitythanthetranslationgeneratedbythe
ingthehuman-generatedreferencetranslationby MT system, in order to have a reliable reference-
thesyntheticreferencetranslationhasadramatic based metric? This would imply that generating
impact. All reference-based metrics increased a synthetic reference translation with any errors
theircorrelationlevelswithhumanjudgementsat is problematic, since for any reference-based au-
boththesegment-levelandthesystem-level. This tomatic metric, these synthetic references would
clearly indicates how critically important a high becomeuselessforevaluatinganyMTsystemthat
qualityreferencetranslationisforreference-based generatestranslationsthatsurpassesthereference
metrics, but moreover, it also highlights the ad- inquality.
vantages of QE metrics in cases where human-
generatedreferenceshavemajorqualityissues. For 9 DA+SQMHumanEvaluation
the English→German language pair, the human-
InadditiontoourMQMannotationsandasacon-
generatedreferencetranslationisofhigherquality
trastive evaluation to cover more language pairs,
thananysubmittedMTsystem. Consequently,the
welookintotheperformanceofmetricswhencom-
synthetic reference translation had almost no im-
paredtothehumanevaluationcampaignconducted
pactonthesegment-levelcorrelationsandonlya
bytheWMT23GeneralMTSharedTask(Kocmi
mixedimpactonthesystem-levelcorrelations.
etal.,2023),whoranhumanevaluationforall14
Themaintakeawaysfromthisstudyare(i)poor
translationdirectionsandallWMT23submissions.
human-generatedreferencetranslationscandramat-
Incontrasttopreviousyears,theynolongeruse
icallyhurttheperformanceandreliabilityofyour
metric,(ii)strongQEmetricscanbebetteralterna- 10Amongotherissues,anypracticalstrategyforcreating
syntheticreferenceswouldneedtohaveawayofavoidingbias
tivesinsuchscenarios,and(iii)generatingasyn-
towardsystemsthataresimilartotheonesusedforreference
theticreferencetranslationfromallsystemoutputs creation.
MTurkneitherreference-basedevaluationforinto- Metric MQM DA+SQM
Translationdirections 3 8
Englishlanguagepairs. Theynolongerusez-score
Systempairs(N) 237 793
normalizationbecausetheuserinterfacedecision
GEMBA-MQM* 0.944 (1) 0.899 (1)
to not track users (i.e., only maintaining HIT in- XCOMET-Ensemble 0.928 (2) 0.870 (10)
MetricX-23 0.908 (3) 0.863 (11)
formation)meansthatthez-scoresarelikelytobe XCOMET-QE-Ensemble* 0.908 (4) 0.871 (8)
CometKiwi* 0.904 (5) 0.887 (3)
influencedbythedistributionofsystemqualityin
COMET 0.900 (6) 0.890 (2)
theHITsratherthanonlyannotatorvariation. BLEURT-20 0.892 (7) 0.880 (6)
MetricX-23-QE* 0.892 (8) 0.870 (9)
TheyemploytheDirectAssessmentScalarQual- mre-score-labse-regular 0.888 (9) 0.861 (12)
KG-BERTScore* 0.884 (10) 0.884 (4)
ityMetrics(DA+SQM)techniqueaspresentedin cometoid22-wmt22* 0.880 (11) 0.884 (5)
BERTscore 0.871 (12) 0.799 (16)
Kocmietal.(2022a).
MS-COMET-QE-22* 0.871 (13) 0.879 (7)
YiSi-1 0.871 (14) 0.832 (13)
DA+SQM asksbilingualraterstoannotatesys- eBLEU 0.859 (15) 0.781 (19)
XLsim 0.855 (16) 0.831 (14)
tem translations against original sources on a 0– prismRef 0.851 (17) 0.808 (15)
100labelledscale. Thescaleismarkedwithseven embed_llama 0.831 (18) 0.778 (20)
f200spBLEU 0.819 (19) 0.786 (17)
pointsrepresentingexpectedquality. BLEU 0.815 (20) 0.770 (22)
tokengram_F 0.815 (21) 0.786 (18)
At the time of writing, the WMT23 General chrF 0.795 (22) 0.777 (21)
MT Shared Task had collected data only for 8 Random-sysname* 0.578 (23) 0.580 (23)
prismSrc* 0.386 (24) 0.412 (24)
translationdirections: Chinese↔English(zh↔en),
German↔English (de↔en), Japanese↔English Table14: Comparisonbetweensystem-levelpairwise
(ja↔en), English→Czech (en→cz), and accuracyusingMQMandDA+SQMgoldscores.MQM
Czech→Ukrainian(cz→uk). results pool data from our 3 main language pairs;
DA+SQMresultspooldatafromthe8languagepairs
We present system-level accuracy results for
for which DA+SQM scores are available. Rows are
bothMQMandDA+SQMinTable14. Thereare
sortedbyMQMaccuracy,withthepurerankorderindi-
manyfactorsthatcouldaffecttheranking. Apart
catedinbrackets. Starredmetricsarereference-freeand
fromusingadifferenthumanannotationprotocol,
underlinedmetricsarebaselines.
MQMcompares3translationdirectionswhereas
the DA+SQM compares 8 translation directions,
containingalsothenon-Englishlow-resourcepair spectives on evaluation, the sub-task takes place
of cz→uk. There is anoverlap ofonly two trans- inadecentralizedmanner,where,contrarytothe
lation directions between the two: en→de and mainmetrictask,thetestsetsarenotprovidedby
zh→en. Themaindifferenceinrankingisformet- theorganizersbutbydifferentresearchteams,who
ricsXCOMET-EnsembleandMetricX-23ranking are also responsible for analysing and presenting
significantly lower than for MQM. Investigating theresults.
system-level Pearson’s correlation for individual Thissubtaskismadeofthreeconsecutivephases;
languagesinTables19to27showsthatbothmet- 1)theBreakingRound,2)theScoringRound and
ricsareperformingconsiderablyloweracrossall 3)theAnalysisRound:
languages(excepten→czandcz→uk)andwedo
1. In the Breaking Round, every challenge set
notseeanypatternbehindthedropinperformance.
participant(Breaker)submitstheirchallenge
10 ChallengeSetsSub-task set S composed of contrastive examples for
different phenomena, where every example
Forthesecondyear,weincludedasub-taskonchal- (s,tˆ,t,r) ∈ S containsonesourcesentences,
lengesets. Thissub-taskisinspiredbytheBuildit oneincorrecttranslationtˆ,onecorrecttransla-
orbreakit: TheLanguageEditionsharedtask(Et-
tiontandonereferencer.
tingeretal.,2017)whichaimedattestingthegener-
alizabilityofNLPsystemsbeyondthedistributions 2. IntheScoringRound,theorganizersdecom-
oftheirtrainingdata. Whereasthestandardevalua- posetheS intoablindtestsetS′,whereeach
tionofthesharedtaskrunsontestsetscontaining exampleincludeseitheranincorrecttransla-
generictextfromreal-worldcontent,thechallenge tion (s,tˆ,r) or a correct translation (s,t,r)
set evaluation is based on test sets designed with alongwiththesourceandthereference. The
theaimofrevealingtheabilitiesortheweaknesses separated contrastive examples are shuffled,
ofthemetricsonevaluatingparticulartranslation andthegoldentruthofwhichsamplesarecor-
phenomena. Inordertoshedlightondifferentper- rectorincorrectiskeptinaseparateset. The
challengeset directions phenomena items citation availability(https://github.com/)
ACES 146 translationerrors 36476 Amrheinetal.(2023) EdinburghNLP/ACES
DFKI-CS 3 linguisticphenomena 20993 Avramidisetal.(2023) DFKI-NLP/mt-testsuite
MSLC23 4 lowqualityMT 9345 Loetal.(2023b) nrc-cnrc/MSLC23
Table15: Overviewoftheparticipationatthemetricschallengesetssub-task
metrics participants from the main task (the • performance change between the 2023 and
Builders) are asked to score with their met- 2022versionsofthemetricsishighlyvariable.
ricsthetranslationsinthegivenblindtestset
The authors’ recommendations are similar to
without knowing which ones are correct or
thosefromWMT2022. Metricdevelopersshould
incorrect. Also,inthisphase,theorganizers
focus on: building ensembles of metrics from
scorealldatawiththebaselinemetrics.
differentdesignfamilies,developingmetricsthat
3. Finally,afterhavinggatheredallmetricscores, pay more attention to the source and rely less on
the organizers return the respective scored surface-level overlap, and carefully determining
translations to the Breakers for the Analysis the influence of multilingual embeddings on MT
round,wheretheylookatwhichmetricsare evaluation.
abletocorrectlyrankthecorrecttranslations
DFKIChallengeSet ThesubmissionbyDFKI
higherthantheincorrectonesforthephenom-
(Avramidis et al., 2023) employs a linguistically
enabeingtested.
motivatedchallengesetthatincludesabout21,000
There were 3 submissions this year, covering a itemsextractedfrom155machinetranslationsys-
widerangeofphenomenaand146differenttrans- temsforthreelanguagedirections(de→en,en→de,
lation directions. An overview of the submitted en→ru), covering more than 100 linguistically-
challenge sets can be seen in Table 15. A short motivatedphenomenaorganizedin14categories.
descriptionofeverysubmissionfollows: Themetricsthathavethebestperformancewithre-
gardtoourlinguisticallymotivatedanalysisarethe
ACESChallengeSet TheTranslationAccuracy COMETOID22-WMT23forde→enandMETRICX-
ChallengESet(ACES,Amrheinetal.,2023)con- 23-C for en→de and en→ru. Some of the most
sistsof36Kexamplesrepresentingchallengesfrom difficult phenomena for the metrics to score are
68phenomenaandcovering146translationdirec- passive voice for de→en, named entities, termi-
tions. Thephenomenarangefromsimpleperturba- nologyandmeasurementunitsforen→deandfo-
tionsattheword/characterleveltomorecomplex cus particles, adverbial clause and stripping for
errors based on discourse and real-world knowl- en→ru.
edge. Webenchmarktheperformanceofsegment-
levelmetricssubmittedtoWMT2023usingACES. MSLC23ChallengeSet TheMetricScoreLand-
Foreachmetric,theauthorsprovideadetailedpro- scapeChallenge(MSLC23;Loetal.,2023b)data
file of performance across the ten top-level accu- set aims to gain insight into metric scores on a
racyerrorcategoriesinACESaswellasanoverall broader/wider landscape of MT quality. Recent
ACES-Scoreforquickcomparison. Theyalsomea- developmentofMTevaluationmetricshasfocused
sure the incremental performance of the metrics on improving their correlation with human judg-
submittedtobothWMT2023and2022. mentontranslationsofhigh-qualitysystems(e.g.,
Theyfindthat: participantsintheWMTNews/GeneralMTShared
Tasks). Thismeansthatmetricperformancemay
• there is no clear winner among the metrics be untested on low- to medium-quality MT out-
submittedtoWMT2023, put. MSLC23 provides a collection of low- to
• neuralmetricsalsotendtofocusmoreonlex- medium-quality MT output on the news portion
icaloverlapthansemanticcontent, of the WMT23 General MT Shared Task test set.
• reference-free metrics using language- Togetherwiththehighqualitysystemssubmitted
agnostic multilingual embeddings struggle to the General MT Shared Task, this enables bet-
with detecting untranslated or sentences ter interpretation of metric scores across a range
translatedinthewrongdirection,and ofdifferentlevelsoftranslationquality. Withthis
widerrangeofMTquality,theauthorsalsovisual- andCOMET)datedbeforethisyear’ssharedtask
izeandanalysemetriccharacteristicsbeyondjust andpubliclyavailable. NewerversionsofCOMET
correlation. were developed without using any of the test set,
The authors find that the smaller variations in test suite or challenge sets. We ensured that the
segment-levelscoresgivenbysomemetricsatthe metrics co-authored by Tom Kocmi were imple-
lowendofqualitycouldindicatethatthesemetrics mented without using any privileged test sets or
struggletodiscriminatebetweenlow-qualityMT insiderinformation.
systems. Thisisfurthershownbytheobservation
13 Acknowledgments
thatsomemetricsrankthelow-qualitysystemsin
reverseorderatsystemlevel. A“universalscore”
Resultsforthissharedtaskwouldnotbepossible
phenomenonforsomemetrics,whereasmallsub-
withouttightcollaborationwiththeorganizersof
setofnon-minimum/maximumdistinctscoresare
the WMT23 General MT Shared Task. We are
assignedtoavarietyoftranslationoutput,hasbeen
gratefultoGoogleandUnbabelforsponsoringand
discovered. Thereisalsoanobservationofdiverse
overseeingthehumanevaluation.
behavioursfromdifferentmetricsonemptystring
RicardoReiissupportedbythePortugueseRe-
translation. These results highlight the need for
coveryandResiliencePlan(PRR)throughproject
metric researchers to check their metrics’ perfor-
C645008882-00000055, Center for Responsible
manceonawiderlandscapeoftranslationquality,
AI.
ortoindicatetopotentialusersthattheyshouldbe
EleftheriosAvramidisissupportedbythe Ger-
cautiousaboutusingtheirmetriconawiderange
man Research Foundation (DFG) through the
ofquality.
project TextQ (grant num. MO 1038/31-1,
436813723),andbytheGermanFederalMinistry
11 Conclusion
of Education and Research (BMBF) through the
ThispapersummarizestheresultsoftheWMT23 projectSocialWear(grantnum. 01IW2000).
shared task on automated machine translation
evaluation, the Metrics Shared Task. We pre-
References
sented an extensive analysis on how well met-
rics perform on our three main translation direc- Chantal Amrhein, Nikita Moghe, and Liane Guillou.
tions: English→German, Hebrew→English and 2023. ACES:Translationaccuracychallengesetsat
Chinese→English. Theresults,basedon10differ- wmt2023. InProceedingsoftheEighthConference
onMachineTranslation,Singapore.Associationfor
enttasks,confirmthesuperiorityofneural-based
ComputationalLinguistics.
learned metrics over overlap-based metrics like
BLEU, SPBLEU or CHRF. These results are con- RohanAnil,AndrewM.Dai,OrhanFirat,MelvinJohn-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
firmedwithDA+SQMhumanjudgement. Wealso
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
foundthatreference-freemetricswerestrongcon-
Chen, Eric Chu, Jonathan H. Clark, Laurent El
tendersthisyear,partlybecausetheydonotrelyon Shafey,YanpingHuang,KathyMeier-Hellstern,Gau-
thequalityofreferencetranslations,anincreasingly ravMishra,EricaMoreira,MarkOmernick,Kevin
important issue as MT systems under evaluation Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
YuanzhongXu,YujingZhang,GustavoHernandez
becomebetter. Inaddition,wecontinuedthechal-
Abrego,JunwhanAhn,JacobAustin,PaulBarham,
lengesetsubtask,whereparticipantshadtocreate
Jan Botha, James Bradbury, Siddhartha Brahma,
contrastivetestsuitesforevaluatingmetrics’ability KevinBrooks,MicheleCatasta,YongCheng,Colin
tocaptureandpenalisespecifictypesoftranslation Cherry,ChristopherA.Choquette-Choo,Aakanksha
Chowdhery,ClémentCrepy,ShachiDave,Mostafa
errors.
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
12 EthicalConsiderations
Feng, Vlad Fienber, Markus Freitag, Xavier Gar-
cia,SebastianGehrmann,LucasGonzalez,GuyGur-
MQMannotationsandadditionalreferencetransla-
Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua
tionsinthispaperaredonebyprofessionaltransla- Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur-
tors. Theyareallpaidatprofessionalrates. witz,MichaelIsard,AbeIttycheriah,MatthewJagiel-
ski,WenhaoJia,KathleenKenealy,MaximKrikun,
OrganizersfromtheNationalResearchCouncil
SnehaKudugunta,ChangLan,KatherineLee,Ben-
Canada and Unbabel have submitted to this task
jaminLee,EricLi,MusicLi,WeiLi,YaGuangLi,
the frozen stable versions of their metrics (YiSi JianLi,HyeontaekLim,HanzhaoLin,ZhongtaoLiu,
FrederickLiu,MarcelloMaggioni,AromaMahendru, languageprocessing: Testingsignificancewithmul-
JoshuaMaynez,VedantMisra,MaysamMoussalem, tiple datasets. Transactions of the Association for
Zachary Nado, John Nham, Eric Ni, Andrew Nys- ComputationalLinguistics,5:471–486.
trom, Alicia Parrish, Marie Pellat, Martin Polacek,
AlexPolozov,ReinerPope,SiyuanQiao,EmilyReif, CynthiaDwork,RaviKumar,MoniNaor,andD.Sivaku-
Bryan Richter, Parker Riley, Alex Castro Ros, Au- mar.2001. Rankaggregationmethodsfortheweb.
rkoRoy,BrennanSaeta,RajkumarSamuel, Renee InProceedingsofthe10thInternationalConference
Shelby, Ambrose Slone, Daniel Smilkov, David R. onWorldWideWeb,WWW’01,page613–622,New
So, Daniel Sohn, Simon Tokumine, Dasha Valter, York,NY,USA.AssociationforComputingMachin-
Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, ery.
Pidong Wang, Zirui Wang, Tao Wang, John Wiet-
ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting BryanEikemaandWilkerAziz.2020. IsMAPdecoding
Xue,PengchengYin,JiahuiYu,QiaoZhang,Steven allyouneed? theinadequacyofthemodeinneural
Zheng,CeZheng,WeikangZhou,DennyZhou,Slav machinetranslation. InProceedingsofthe28thInter-
Petrov,andYonghuiWu.2023. PaLM2Technical nationalConferenceonComputationalLinguistics,
Report. arXivpreprintarXiv:2305.10403. pages4506–4520,Barcelona,Spain(Online).Inter-
nationalCommitteeonComputationalLinguistics.
EleftheriosAvramidis,ShushenManakhimova,Vivien
Macketanz,andSebastianMöller.2023. Challenging BryanEikemaandWilkerAziz.2021. Sampling-Based
thestate-of-the-artmachinetranslationmetricsfrom MinimumBayesRiskDecodingforNeuralMachine
alinguisticperspective. InProceedingsoftheEighth Translation. arXivpreprintarXiv:2108.04718.
ConferenceonMachineTranslation,Singapore.As-
Muhammad ElNokrashy and Tom Kocmi. 2023.
sociationforComputationalLinguistics.
eBLEU: Unexpectedly Good Machine Translation
FrédéricBlain,ChrysoulaZerva,RicardoRei,NunoM. EvaluationUsingSimpleWordEmbeddings. InPro-
Guerreiro, Diptesh Kanojia, José G. C. de Souza, ceedingsoftheEighthConferenceonMachineTrans-
Beatriz Silva, Tânia Vaz, Yan Jingxuan, Fatemeh lation(WMT),Singapore,Singapore(Hybrid).Asso-
Azadi,ConstantinOra˘san,andAndréF.T.Martins. ciationforComputationalLinguistics.
2023. FindingsoftheWMT2023SharedTaskon
Allyson Ettinger, Sudha Rao, Hal Daumé III, and
QualityEstimation. InProceedingsoftheSeventh
EmilyM.Bender.2017. Towardslinguisticallygen-
ConferenceonMachineTranslation(WMT),Singa-
eralizableNLPsystems:Aworkshopandsharedtask.
pore,Singapore(Hybrid).AssociationforComputa-
InProceedingsoftheFirstWorkshoponBuildingLin-
tionalLinguistics.
guisticallyGeneralizableNLPSystems,pages1–10,
Pierre Colombo, Nathan Noiry, Ekhine Irurozki, and Copenhagen, Denmark. Association for Computa-
Stéphan Clémençon. 2022. What are the best sys- tionalLinguistics.
tems? New perspectives on NLP Benchmarking.
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi
arXivpreprintarXiv:2202.03799.
Ma, Ahmed El-Kishky, Siddharth Goyal, Man-
Daniel Deutsch, Rotem Dror, and Dan Roth. 2021. deep Baines, Onur Celebi, Guillaume Wenzek,
A statistical analysis of summarization evaluation VishravChaudhary, Naman Goyal, Tom Birch, Vi-
metrics using resampling methods. arXiv preprint taliy Liptchinsky, Sergey Edunov, Edouard Grave,
arXiv:2104.00054. Michael Auli, and Armand Joulin. 2021. Beyond
English-Centric Multilingual Machine Translation.
Daniel Deutsch, George Foster, and Markus Freitag. JournalofMachineLearningResearch,22(1).
2023. Ties Matter: Modifying Kendall’s Tau for
Modern Metric Meta-Evaluation. arXiv preprint Patrick Fernandes, António Farinhas, Ricardo Rei,
arXiv:2305.14324. JoséG.C.deSouza,PerezOgayo,GrahamNeubig,
andAndreMartins.2022. Quality-awaredecoding
SörenDreano,DerekMolloy,andNoelMurphy.2023a. for neural machine translation. In Proceedings of
Embed_Llama: usingLLMembeddingsfortheMet- the2022ConferenceoftheNorthAmericanChap-
ricsSharedTask. InProceedingsoftheEighthCon- teroftheAssociationforComputationalLinguistics:
ferenceonMachineTranslation(WMT),Singapore, HumanLanguageTechnologies,pages1396–1412,
Singapore(Hybrid).AssociationforComputational Seattle,UnitedStates.AssociationforComputational
Linguistics. Linguistics.
SörenDreano,DerekMolloy,andNoelMurphy.2023b. Marina Fomicheva, Shuo Sun, Erick Fonseca,
Tokengram_F,afastandaccuratetoken-basedchrF++ ChrysoulaZerva,FrédéricBlain,VishravChaudhary,
derivative. InProceedingsoftheEighthConference FranciscoGuzmán,NinaLopatina,LuciaSpecia,and
onMachineTranslation(WMT),Singapore, Singa- AndréF.T.Martins.2022. MLQE-PE:Amultilin-
pore (Hybrid). Association for Computational Lin- gualqualityestimationandpost-editingdataset. In
guistics. ProceedingsoftheThirteenthLanguageResources
andEvaluationConference,pages4963–4974,Mar-
RotemDror,GiliBaumer,MarinaBogomolov,andRoi seille,France.EuropeanLanguageResourcesAsso-
Reichart. 2017. Replicability analysis for natural ciation.
MarkusFreitag,GeorgeFoster,DavidGrangier,Viresh Mariya Shmatova, and Jun Suzuki. 2023. Find-
Ratnakar,QijunTan,andWolfgangMacherey.2021. ingsofthe2023conferenceonmachinetranslation
Experts,errors,andcontext: Alarge-scalestudyof (WMT23): LLMsareherebutnotquitethereyet. In
humanevaluationformachinetranslation. Transac- ProceedingsoftheEighthConferenceonMachine
tionsoftheAssociationforComputationalLinguis- Translation(WMT),Singapore,Singapore(Hybrid).
tics,9:1460–1474. AssociationforComputationalLinguistics.
Markus Freitag, Behrooz Ghorbani, and Patrick Fer- Tom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton
nandes.2023. EpsilonSamplingRocks: Investigat- Dvorkovich, Christian Federmann, Mark Fishel,
ing Sampling Strategies for Minimum Bayes Risk Thamme Gowda, Yvette Graham, Roman Grund-
DecodingforMachineTranslation. arXivpreprint kiewicz,BarryHaddow,RebeccaKnowles,Philipp
arXiv:2305.09860. Koehn,ChristofMonz,MakotoMorishita,Masaaki
Nagata,ToshiakiNakazawa,MichalNovák,Martin
Markus Freitag, David Grangier, and Isaac Caswell. Popel, and Maja Popovic´. 2022a. Findings of the
2020. BLEUmightbeguiltybutreferencesarenot 2022conferenceonmachinetranslation(WMT22).
innocent. In Proceedings of the 2020 Conference In Proceedings of the Seventh Conference on Ma-
onEmpiricalMethodsinNaturalLanguageProcess- chineTranslation(WMT),pages1–45,AbuDhabi,
ing(EMNLP),pages61–71,Online.Associationfor UnitedArabEmirates(Hybrid).AssociationforCom-
ComputationalLinguistics. putationalLinguistics.
MarkusFreitag,DavidGrangier,QijunTan,andBowen TomKocmiandChristianFedermann.2023. GEMBA-
Liang. 2022. High quality rather than high model MQM: Detecting Translation Quality Error Spans
probability: MinimumBayesriskdecodingwithneu- with GPT-4. In Proceedings of the Eighth Confer-
ralmetrics. TransactionsoftheAssociationforCom- enceonMachineTranslation(WMT),Singapore,Sin-
putationalLinguistics,10:811–825. gapore(Hybrid).AssociationforComputationalLin-
guistics.
Thamme Gowda, Tom Kocmi, and Marcin Junczys-
Dowmunt. 2023. Cometoid: Distilling Strong Tom Kocmi, Christian Federmann, Roman Grund-
Reference-basedMachineTranslationMetricsinto kiewicz,MarcinJunczys-Dowmunt,HitokazuMat-
EvenStrongerQualityEstimationMetrics. InPro- sushita,andArulMenezes.2021. Toshipornotto
ceedingsoftheEighthConferenceonMachineTrans- ship: Anextensiveevaluationofautomaticmetrics
lation(WMT),Singapore,Singapore(Hybrid).Asso- formachinetranslation. InProceedingsoftheSixth
ciationforComputationalLinguistics. ConferenceonMachineTranslation,pages478–494,
Online.AssociationforComputationalLinguistics.
NunoM.Guerreiro,RicardoRei,DaanvanStigt,Luisa
Coheur, Pierre Colombo, and André F. T. Martins. TomKocmi,HitokazuMatsushita,andChristianFeder-
2023. xCOMET:TransparentMachineTranslation mann.2022b. MS-COMET:Moreandbetterhuman
Evaluation through Fine-grained Error Detection. judgementsimprovemetricperformance. InProceed-
arXivpreprintarXiv:2310.10482. ings of the Seventh Conference on Machine Trans-
lation(WMT),pages541–548, AbuDhabi, United
MichaelHagmannandStefanRiezler.2023. Towards
ArabEmirates(Hybrid).AssociationforComputa-
inferential reproducibility of machine learning re-
tionalLinguistics.
search. arXivpreprintarXiv:2302.04054.
Chi-kiuLo.2019. YiSi-aunifiedsemanticMTquality
John Hewitt, Christopher Manning, and Percy Liang.
evaluationandestimationmetricforlanguageswith
2022. Truncation sampling as language model
differentlevelsofavailableresources. InProceed-
desmoothing. InFindingsoftheAssociationforCom-
ingsoftheFourthConferenceonMachineTransla-
putationalLinguistics: EMNLP2022,pages3414–
tion(Volume2: SharedTaskPapers,Day1),pages
3427,AbuDhabi,UnitedArabEmirates.Association
507–513,Florence,Italy.AssociationforComputa-
forComputationalLinguistics.
tionalLinguistics.
JurajJuraska,MaraFinkelstein,DanielDeutsch,Aditya Chi-kiuLo,RebeccaKnowles,andCyrilGoutte.2023a.
Siddhant, Mehdi Mirzazadeh, and Markus Freitag. Beyondcorrelation: Makingsenseofthescorediffer-
2023. MetricX-23: TheGoogleSubmissiontothe encesofnewmtevaluationmetrics. InProceedings
WMT 2023 Metrics Shared Task. In Proceedings ofMachineTranslationSummitXIXVol.1: Research
of the Eighth Conference on Machine Translation Track,pages186–199.
(WMT),Singapore,Singapore(Hybrid).Association
forComputationalLinguistics. Chi-kiu Lo, Samuel Larkin, and Rebecca Knowles.
2023b. Metricscorelandscapechallenge(MSLC23):
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Understandingmetrics’performanceonawiderland-
Ondˇrej Bojar, Anton Dvorkovich, Christian Fed- scape of translation quality. In Proceedings of the
ermann, Mark Fishel, Markus Freitag, Thamme EighthConferenceonMachineTranslation,Singa-
Gowda, Roman Grundkiewicz, Barry Haddow, pore.AssociationforComputationalLinguistics.
Philipp Koehn, Benjamin Marie, Christof Monz,
MakotoMorishita,KentonMurray,MasaakiNagata, ArleLommel,HansUszkoreit,andAljoschaBurchardt.
Toshiaki Nakazawa, Martin Popel, Maja Popovic´, 2014. MultidimensionalQualityMetrics(MQM):A
FrameworkforDeclaringandDescribingTranslation pages569–577,AbuDhabi,UnitedArabEmirates
QualityMetrics. Tradumàtica,pages0455–463. (Hybrid).AssociationforComputationalLinguistics.
Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang, Maja Popovic´. 2015. chrF: character n-gram F-score
TomKocmi,andDachengTao.2023. Erroranalysis forautomaticMTevaluation. InProceedingsofthe
promptingenableshuman-liketranslationevaluation TenthWorkshoponStatisticalMachineTranslation,
inlargelanguagemodels: Acasestudyonchatgpt. pages 392–395, Lisbon, Portugal. Association for
arXivpreprintarXiv:2303.13809. ComputationalLinguistics.
Benjamin Marie. 2022. Yes, we need sta- MattPost.2018. AcallforclarityinreportingBLEU
tistical significance testing. towardsai.net scores. InProceedingsoftheThirdConferenceon
https://pub.towardsai.net/yes-we-need-statistical- MachineTranslation: ResearchPapers,pages186–
significance-testing-927a8d21f9f0. 191, Brussels, Belgium. Association for Computa-
tionalLinguistics.
Ananya Mukherjee and Manish Shrivastava. 2023.
MEE4 and XLsim: IIIT HYD’s Submissions for
AmyPu,HyungWonChung,AnkurParikh,Sebastian
WMT23MetricsSharedTask. InProceedingsofthe
Gehrmann, and Thibault Sellam. 2021. Learning
EighthConferenceonMachineTranslation(WMT),
compactmetricsforMT. InProceedingsofthe2021
Singapore,Singapore(Hybrid).AssociationforCom-
Conference on Empirical Methods in Natural Lan-
putationalLinguistics.
guageProcessing,pages751–762,OnlineandPunta
Cana,DominicanRepublic.AssociationforCompu-
MathiasMüllerandRicoSennrich.2021. Understand-
tationalLinguistics.
ingthepropertiesofminimumBayesriskdecoding
inneuralmachinetranslation. InProceedingsofthe
Vikas Raunak, Tom Kocmi, and Matt Post. 2023.
59thAnnualMeetingoftheAssociationforCompu-
SLIDE:SlidingDocumentEvaluatorforDocument-
tationalLinguisticsandthe11thInternationalJoint
ContextEvaluationinMachineTranslation. InPro-
Conference on Natural Language Processing (Vol-
ceedingsoftheEighthConferenceonMachineTrans-
ume1: LongPapers),pages259–272,Online.Asso-
lation(WMT),Singapore,Singapore(Hybrid).Asso-
ciationforComputationalLinguistics.
ciationforComputationalLinguistics.
SubhajitNaskar,DanielDeutsch,andMarkusFreitag.
Ricardo Rei, José G. C. de Souza, Duarte Alves,
2023. Quality Estimation using Minimum Bayes
ChrysoulaZerva,AnaCFarinha,TaisiyaGlushkova,
Risk. In Proceedings of the Eighth Conference on
AlonLavie,LuisaCoheur,andAndréF.T.Martins.
MachineTranslation(WMT),Singapore,Singapore
2022a. COMET-22: Unbabel-IST2022submission
(Hybrid).AssociationforComputationalLinguistics.
for the metrics shared task. In Proceedings of the
NLLBTeam,MartaR.Costa-jussà,JamesCross,Onur SeventhConferenceonMachineTranslation(WMT),
Çelebi,MahaElbayad,KennethHeafield,KevinHef- pages578–585,AbuDhabi,UnitedArabEmirates
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht, (Hybrid).AssociationforComputationalLinguistics.
JeanMaillard,AnnaSun,SkylerWang,Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar- RicardoRei, NunoM.Guerreiro, JoséPombal, Daan
rault,GabrielMejiaGonzalez,PrangthipHansanti, vanStigt,MarcosTreviso,LuisaCoheur,JoséG.C.
John Hoffman, Semarley Jarrett, Kaushik Ram deSouza,andAndréF.T.Martins.2023. Scalingup
Sadagopan, Dirk Rowe, Shannon Spruit, Chau COMETKIWI: Unbabel-IST 2023 Submission for
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti theQualityEstimationSharedTask. InProceedings
Bhosale, Sergey Edunov, Angela Fan, Cynthia of the eighth Conference on Machine Translation,
Gao,VedanujGoswami,FranciscoGuzmán,Philipp Singapore.AssociationforComputationalLinguis-
Koehn, Alexandre Mourachko, Christophe Rop- tics.
ers, Safiyyah Saleem, Holger Schwenk, and Jeff
RicardoRei,CraigStewart,AnaCFarinha,andAlon
Wang. 2022. No Language Left Behind: Scal-
Lavie.2020. COMET:AneuralframeworkforMT
ing Human-Centered Machine Translation. arXiv
evaluation. InProceedingsofthe2020Conference
preprintarXiv:2207.04672.
onEmpiricalMethodsinNaturalLanguageProcess-
KishorePapineni,SalimRoukos,ToddWard,andWei- ing(EMNLP),pages2685–2702,Online.Association
JingZhu.2002. Bleu: amethodforautomaticevalu- forComputationalLinguistics.
ationofmachinetranslation. InProceedingsofthe
40thAnnualMeetingoftheAssociationforCompu- Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro,
tational Linguistics, pages 311–318, Philadelphia, ChrysoulaZerva,AnaCFarinha,ChristineMaroti,
Pennsylvania,USA.AssociationforComputational José G. C. de Souza, Taisiya Glushkova, Duarte
Linguistics. Alves, Luisa Coheur, Alon Lavie, and André F. T.
Martins.2022b. CometKiwi: IST-unbabel2022sub-
Stefano Perrella, Lorenzo Proietti, Alessandro Scirè, mission for the quality estimation shared task. In
Niccolò Campolungo, and Roberto Navigli. 2022. ProceedingsoftheSeventhConferenceonMachine
MaTESe: Machine translation evaluation as a se- Translation (WMT), pages 634–645, Abu Dhabi,
quencetaggingproblem. InProceedingsoftheSev- UnitedArabEmirates(Hybrid).AssociationforCom-
enth Conference on Machine Translation (WMT), putationalLinguistics.
T.Robertson,F.T.Wright,andR.Dykstra.1988. Or- Annotation: HW-TSC’sSubmissiontotheWMT23
derRestrictedStatisticalInference. Probabilityand MetricsSharedTask. InProceedingsoftheEighth
StatisticsSeries.Wiley. ConferenceonMachineTranslation(WMT),Singa-
pore,Singapore(Hybrid).AssociationforComputa-
ThibaultSellam,DipanjanDas,andAnkurParikh.2020. tionalLinguistics.
BLEURT: Learning robust metrics for text genera-
tion. InProceedingsofthe58thAnnualMeetingof JingXu,MeganUng,MojtabaKomeili,KushalArora,
theAssociationforComputationalLinguistics,pages Y-LanBoureau,andJasonWeston.2023a. Learning
7881–7892,Online.AssociationforComputational newskillsafterdeployment:Improvingopen-domain
Linguistics. internet-driven dialogue with human feedback. In
Proceedings of the 61st Annual Meeting of the As-
BrianThompsonandMattPost.2020a. Automaticma-
sociationforComputationalLinguistics(Volume1:
chinetranslationevaluationinmanylanguagesvia
LongPapers),pages13557–13572,Toronto,Canada.
zero-shotparaphrasing. InProceedingsofthe2020
AssociationforComputationalLinguistics.
Conference on Empirical Methods in Natural Lan-
guageProcessing(EMNLP),pages90–121,Online. Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, and
AssociationforComputationalLinguistics. William Yang Wang. 2023b. Sescore2: Learning
textgenerationevaluationviasynthesizingrealistic
Brian Thompson and Matt Post. 2020b. Paraphrase
mistakes.
generationaszero-shotmultilingualtranslation: Dis-
entanglingsemanticsimilarityfromlexicalandsyn- Wenda Xu, Yilin Tuan, Yujie Lu, Michael Saxon,
tacticdiversity. InProceedingsoftheFifthConfer- Lei Li, and William Yang Wang. 2022. Not all
enceonMachineTranslation,pages561–570,Online. errors are equal: Learning text generation met-
AssociationforComputationalLinguistics. ricsusingstratifiederrorsynthesis. arXivpreprint
arXiv:2210.05035.
GiorgosVernikos,BrianThompson,PrashantMathur,
andMarcelloFederico.2022. Embarrassinglyeasy WendaXu,DanqingWang,LiangmingPan,Zhenqiao
document-level MT metrics: How to convert any Song,MarkusFreitag,WilliamYangWang,andLei
pretrainedmetricintoadocument-levelmetric. In Li.2023c. Instructscore: Explainabletextgeneration
ProceedingsoftheSeventhConferenceonMachine evaluationwithfinegrainedfeedback.
Translation (WMT), pages 118–128, Abu Dhabi,
UnitedArabEmirates(Hybrid).AssociationforCom- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
putationalLinguistics. Weinberger,andYoavArtzi.2020. Bertscore: Eval-
uating text generation with bert. In International
Vasiliy Viskov, George Kokush, Daniil Larionov,
ConferenceonLearningRepresentations.
Steffen Eger, and Alexander Panchenko. 2023.
Semantically-Informed Regressive Encoder Score.
InProceedingsoftheEighthConferenceonMachine
Translation(WMT),Singapore,Singapore(Hybrid).
AssociationforComputationalLinguistics.
Johnny Wei, Tom Kocmi, and Christian Federmann.
2022. Searching for a higher power in the human
evaluation of MT. In Proceedings of the Seventh
ConferenceonMachineTranslation(WMT),pages
129–139,AbuDhabi,UnitedArabEmirates(Hybrid).
AssociationforComputationalLinguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RemiLouf,MorganFuntow-
icz,JoeDavison,SamShleifer,PatrickvonPlaten,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
QuentinLhoest,andAlexanderRush.2020. Trans-
formers:State-of-the-artnaturallanguageprocessing.
InProceedingsofthe2020ConferenceonEmpirical
Methods in Natural Language Processing: System
Demonstrations,pages38–45,Online.Association
forComputationalLinguistics.
ZhanglinWu,YilunLiu,MinZhang,XiaofengZhao,
Junhao Zhu, Ming Zhu, Xiaosong Qiao, Jingfei
Zhang, Ma Miaomiao, Zhao Yanqing, Song Peng,
shimintao,HaoYang,andYanfeiJiang.2023. Em-
poweringaMetricwithLLM-assistedNamedEntity
A CorrelationswithMQMforallmetrics
Tables 16 and 17 contain system- and segment-level results for all metrics (including contrastive sub-
missions) on the 10 standard tasks described in Table 7. No pairwise significance tests were carried
outfortheseresults,sotheper-taskranksonlyindicateeachmetric’sorderonthattask,ratherthanits
significanceclusterasinTables8and9.
lang: en→de,he→en,zh→en en→de he→en zh→en
corr_fcn: accuracy pearson pearson pearson
metric avg-corr task1 task2 task5 task8
XCOMET-Ensemble 1 0.825 6 0.928 9 0.980 4 0.950 14 0.927
XCOMET-XXL 2 0.824 5 0.932 7 0.982 1 0.964 16 0.911
MetricX-23-QE-b* 3 0.823 2 0.940 8 0.982 5 0.947 15 0.926
XCOMET-XL 4 0.816 7 0.924 18 0.973 11 0.937 26 0.884
MetricX-23-QE-c* 5 0.813 4 0.932 20 0.972 8 0.939 4 0.974
MetricX-23-b 6 0.811 9 0.916 4 0.990 15 0.928 19 0.902
XCOMET-QE-Ensemble* 7 0.808 13 0.908 16 0.974 23 0.909 23 0.892
MetricX-23 8 0.808 12 0.908 12 0.977 22 0.910 28 0.873
GEMBA-MQM* 9 0.802 1 0.944 1 0.993 9 0.939 1 0.991
MetricX-23-QE* 10 0.800 24 0.892 22 0.969 35 0.858 30 0.859
cometoid22-wmt23* 11 0.794 3 0.936 10 0.979 16 0.928 8 0.956
mbr-metricx-qe* 12 0.788 29 0.880 13 0.976 19 0.915 11 0.936
CometKiwi-XXL* 13 0.786 11 0.912 6 0.986 14 0.929 2 0.978
CometKiwi-XL* 14 0.786 8 0.916 14 0.975 29 0.900 3 0.974
MaTESe 15 0.782 17 0.904 36 0.918 25 0.906 25 0.889
CometKiwi* 16 0.782 16 0.904 27 0.946 34 0.860 6 0.963
COMET 17 0.779 20 0.900 3 0.990 7 0.940 21 0.898
MetricX-23-c 18 0.778 10 0.916 28 0.944 6 0.946 9 0.953
instructscore 19 0.777 22 0.896 25 0.952 21 0.910 31 0.825
BLEURT-20 20 0.776 23 0.892 5 0.990 12 0.937 27 0.880
KG-BERTScore* 21 0.774 27 0.884 30 0.926 24 0.908 7 0.962
sescoreX 22 0.772 25 0.892 26 0.952 28 0.901 35 0.797
cometoid22-wmt22* 23 0.772 28 0.880 17 0.973 37 0.839 10 0.940
cometoid22-wmt21* 24 0.768 30 0.871 19 0.973 38 0.832 13 0.929
docWMT22CometDA 25 0.768 18 0.904 2 0.990 17 0.922 17 0.907
docWMT22CometKiwiDA* 26 0.767 21 0.900 21 0.970 26 0.906 5 0.965
Calibri-COMET22 27 0.767 15 0.904 23 0.963 13 0.930 29 0.863
Calibri-COMET22-QE* 28 0.755 34 0.863 11 0.978 40 0.778 12 0.934
YiSi-1 29 0.754 33 0.871 31 0.925 18 0.917 32 0.823
MS-COMET-QE-22* 30 0.744 32 0.871 24 0.959 43 0.721 20 0.901
prismRef 31 0.744 37 0.851 33 0.920 3 0.956 40 0.762
mre-score-labse-regular 32 0.743 26 0.888 29 0.942 2 0.958 18 0.903
BERTscore 33 0.742 31 0.871 38 0.891 30 0.895 33 0.810
XLsim 34 0.719 36 0.855 32 0.925 31 0.887 36 0.796
f200spBLEU 35 0.704 40 0.819 34 0.919 39 0.805 39 0.772
MEE4 36 0.704 39 0.823 41 0.861 32 0.879 41 0.743
tokengram_F 37 0.703 42 0.815 43 0.858 33 0.878 37 0.795
embed_llama 38 0.701 38 0.831 42 0.861 36 0.841 38 0.785
BLEU 39 0.696 41 0.815 37 0.917 42 0.769 42 0.734
chrF 40 0.694 43 0.795 40 0.866 41 0.776 34 0.809
eBLEU 41 0.692 35 0.859 35 0.918 20 0.911 43 0.727
Random-sysname* 42 0.529 44 0.578 44 0.357 44 0.209 44 0.093
prismSrc* 43 0.455 45 0.386 45 -0.327 45 -0.017 45 -0.406
HuaweiTSC_EE_Metric – – 19 0.900 39 0.878 27 0.903 22 0.894
slide* – – 14 0.904 15 0.975 10 0.938 24 0.890
Table16: Resultsforallmetricsonsystem-leveltasksformainlanguagepairs. Rowsaresortedbytheoverall
averagecorrelationacrossall10tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
lang: en→de en→de he→en he→en zh→en zh→en
corr_fcn: pearson acc-t pearson acc-t pearson acc-t
metric task3 task4 task6 task7 task9 task10
XCOMET-Ensemble 2 0.695 3 0.604 1 0.556 1 0.586 2 0.650 2 0.543
XCOMET-XXL 1 0.695 4 0.603 2 0.556 4 0.577 5 0.627 3 0.541
MetricX-23-QE-b* 5 0.628 1 0.606 6 0.529 3 0.580 1 0.661 4 0.539
XCOMET-XL 3 0.680 6 0.601 5 0.536 7 0.568 7 0.624 9 0.531
MetricX-23-QE-c* 11 0.525 11 0.581 7 0.526 6 0.576 9 0.581 1 0.545
MetricX-23-b 9 0.566 2 0.604 4 0.537 2 0.581 8 0.612 6 0.535
XCOMET-QE-Ensemble* 4 0.679 8 0.588 9 0.498 10 0.554 4 0.647 7 0.533
MetricX-23 7 0.585 5 0.603 3 0.548 5 0.577 6 0.625 8 0.531
GEMBA-MQM* 16 0.502 17 0.572 13 0.401 9 0.564 16 0.449 14 0.522
MetricX-23-QE* 6 0.626 7 0.596 8 0.520 8 0.564 3 0.647 11 0.527
cometoid22-wmt23* 20 0.448 9 0.586 16 0.397 15 0.544 19 0.439 15 0.520
mbr-metricx-qe* 8 0.571 10 0.584 12 0.411 11 0.553 13 0.489 5 0.537
CometKiwi-XXL* 28 0.417 13 0.578 19 0.390 13 0.550 24 0.390 10 0.528
CometKiwi-XL* 21 0.446 18 0.571 22 0.384 18 0.533 21 0.430 13 0.522
MaTESe 10 0.554 30 0.528 10 0.459 12 0.550 11 0.511 34 0.479
CometKiwi* 18 0.475 19 0.569 20 0.387 14 0.544 18 0.442 12 0.525
COMET 25 0.432 15 0.574 14 0.401 19 0.532 22 0.396 19 0.514
MetricX-23-c 15 0.508 27 0.539 31 0.313 20 0.531 27 0.371 21 0.507
instructscore 12 0.519 20 0.563 11 0.458 17 0.536 12 0.499 40 0.459
BLEURT-20 17 0.484 16 0.572 24 0.382 24 0.519 26 0.378 16 0.518
KG-BERTScore* 19 0.451 23 0.556 23 0.382 16 0.537 20 0.430 17 0.516
sescoreX 13 0.519 21 0.563 21 0.385 33 0.484 10 0.536 24 0.499
cometoid22-wmt22* 23 0.441 14 0.578 26 0.365 25 0.515 14 0.479 18 0.515
cometoid22-wmt21* 26 0.428 12 0.581 27 0.360 26 0.515 15 0.458 20 0.514
docWMT22CometDA 30 0.394 22 0.559 28 0.339 31 0.497 29 0.353 28 0.493
docWMT22CometKiwiDA* 22 0.444 24 0.547 33 0.286 32 0.489 25 0.387 27 0.493
Calibri-COMET22 29 0.413 34 0.522 15 0.401 27 0.515 23 0.396 36 0.474
Calibri-COMET22-QE* 24 0.441 41 0.483 18 0.395 29 0.506 17 0.443 29 0.491
YiSi-1 31 0.366 26 0.542 17 0.395 21 0.529 30 0.290 22 0.504
MS-COMET-QE-22* 33 0.310 25 0.546 32 0.295 30 0.498 28 0.367 26 0.498
prismRef 14 0.516 38 0.518 30 0.319 22 0.528 33 0.183 23 0.504
mre-score-labse-regular 41 0.111 28 0.530 25 0.378 23 0.522 35 0.145 32 0.481
BERTscore 32 0.325 31 0.528 29 0.335 28 0.515 31 0.236 25 0.499
XLsim 35 0.239 32 0.527 35 0.233 34 0.480 37 0.111 39 0.464
f200spBLEU 36 0.237 33 0.526 36 0.230 37 0.447 38 0.108 35 0.476
MEE4 39 0.202 29 0.529 34 0.256 41 0.441 39 0.105 33 0.480
tokengram_F 38 0.227 35 0.520 37 0.226 35 0.461 41 0.060 31 0.485
embed_llama 34 0.250 40 0.483 40 0.215 42 0.430 34 0.161 41 0.447
BLEU 40 0.192 36 0.520 39 0.220 39 0.442 36 0.119 38 0.472
chrF 37 0.232 37 0.519 38 0.221 36 0.460 40 0.063 30 0.485
eBLEU 43 -0.011 39 0.512 42 0.131 38 0.445 43 -0.084 37 0.473
Random-sysname* 42 0.064 43 0.409 43 0.041 43 0.428 42 0.018 43 0.381
prismSrc* 27 0.425 42 0.426 41 0.140 40 0.441 32 0.223 42 0.421
Table17: Resultsforallmetricsonsegment-leveltasksformainlanguagepairs. Rowsaresortedbytheoverall
averagecorrelationacrossall10tasks(leftmostcolumninTable16). Starredmetricsarereference-free,underlined
metricsarebaselines,anditalicizedmetricsarecontrastivesubmissions.
metric avgcorr p-values
XCOMET-Ensemble 1 0.825 . 01 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
XCOMET-QE-Ensemble* 2 0.808 . . 46 20 26 00 00 00 01 01 01 03 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
MetricX-23 2 0.808 . . . 24 25 03 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
GEMBA-MQM* 2 0.802 . . . . 43 03 00 00 00 00 00 02 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
MetricX-23-QE* 2 0.800 . . . . . 13 07 05 03 00 06 02 00 00 00 00 01 01 00 00 02 00 00 00 00 00 00 00 00 00 00 00
mbr-metricx-qe* 3 0.788 . . . . . . 31 24 17 10 16 09 02 02 00 00 00 03 00 02 01 00 00 00 00 00 00 00 00 00 00 00
MaTESe 3 0.782 . . . . . . . 48 38 26 19 24 12 09 04 06 03 03 00 01 03 00 00 00 00 00 00 00 00 00 00 00
CometKiwi* 3 0.782 . . . . . . . . 39 25 26 23 04 07 01 02 02 02 00 00 01 00 00 00 00 00 00 00 00 00 00 00
COMET 3 0.779 . . . . . . . . . 22 34 25 23 01 19 11 11 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00
BLEURT-20 3 0.776 . . . . . . . . . . 46 34 35 10 20 16 13 04 02 00 00 01 00 00 00 00 00 00 00 00 00 00
KG-BERTScore* 3 0.774 . . . . . . . . . . . 43 49 24 29 32 16 08 00 04 07 02 00 00 00 00 00 00 00 00 00 00
sescoreX 3 0.772 . . . . . . . . . . . . 49 30 37 31 18 06 08 03 04 04 00 00 00 00 00 00 00 00 00 00
cometoid22-wmt22* 4 0.772 . . . . . . . . . . . . . 34 22 22 07 14 03 04 07 01 00 00 00 00 00 00 00 00 00 00
docWMT22CometDA 4 0.768 . . . . . . . . . . . . . . 51 44 24 10 03 03 03 04 00 00 00 00 00 00 00 00 00 00
docWMT22CometKiwiDA* 4 0.767 . . . . . . . . . . . . . . . 48 14 20 07 09 12 03 00 00 00 00 00 00 00 00 00 00
Calibri-COMET22 4 0.767 . . . . . . . . . . . . . . . . 17 23 10 16 11 01 00 00 00 00 00 00 00 00 00 00
Calibri-COMET22-QE* 4 0.755 . . . . . . . . . . . . . . . . . 45 30 36 30 18 07 01 04 02 01 00 00 00 00 00
YiSi-1 4 0.754 . . . . . . . . . . . . . . . . . . 30 13 22 31 00 00 00 00 00 00 00 00 00 00
MS-COMET-QE-22* 5 0.744 . . . . . . . . . . . . . . . . . . . 52 49 43 12 01 02 02 02 01 00 00 00 00
prismRef 5 0.744 . . . . . . . . . . . . . . . . . . . . 44 44 00 00 01 00 00 00 00 00 00 00
mre-score-labse-regular 5 0.743 . . . . . . . . . . . . . . . . . . . . . 49 06 01 04 01 00 00 00 00 00 00
BERTscore 5 0.742 . . . . . . . . . . . . . . . . . . . . . . 18 03 07 05 01 02 02 00 00 00
XLsim 6 0.719 . . . . . . . . . . . . . . . . . . . . . . . 04 10 01 06 01 01 00 00 00
f200spBLEU 7 0.704 . . . . . . . . . . . . . . . . . . . . . . . . 51 48 39 06 13 12 00 00
MEE4 7 0.704 . . . . . . . . . . . . . . . . . . . . . . . . . 46 46 33 23 16 00 00
tokengram_F 7 0.703 . . . . . . . . . . . . . . . . . . . . . . . . . . 45 22 15 11 00 00
embed_llama 7 0.701 . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 30 29 00 00
BLEU 7 0.696 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 35 00 00
chrF 7 0.694 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 00 00
eBLEU 7 0.692 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 00
Random-sysname* 8 0.529 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 04
prismSrc* 9 0.455 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Table18: Resultsofpairwisemetricsignificancetestsforprimarysubmissionsusingpermutationresampling. Each
valuegivesthe100×estimatedprobabilityofthenullhypothesisthattheaveragecorrelationofthemetricinthe
currentrowis≤theaveragecorrelationofthemetricinthecurrentcolumn. Starredmetricsarereference-free,and
underlinedmetricsarebaselines.
B Significancecomparisonsformainresults
Table18containstheresultsofpairwisecomparisonsfortheresultsinTable1.
C CorrelationswithWMTDA-SQMforallmetrics
Tables 19 to 27 give correlations with WMT direct assessment (DA-SQM) scores on all 8 translation
directions for which those scores are available. In all cases, reference A was used, and no additional
metricswereavailabletobescoredbythemetrics. Weevaluatemetricsonatasksetupsimilartothat
of Table 7: one system-level pairwise accuracy task involving all languages (with a weight of 8), and
system-levelPearson,segment-levelPearson,andsegment-levelacc∗ tasksforeachtranslationdirection
eq
(24tasksintotal,eachwithaweightof1). Eachtableshowsoverallaveragecorrelation,alongwiththe
resultsforthetasksforonetranslationdirection. Metricsthatdidnotparticipateinalltasksdonothave
anaveragecorrelation,andaredisplayedattheendofeachtable.
WewishtoemphasizethattheDA+SQMisconsiderablynoisierthanMQM.Thisincreasedvariability
mayinfluencetheoutcomesobservedinthefollowingspotlightevaluation. Consequently,readersshould
exerciseconsiderablecautionwhendrawingconclusionsfromtheseresults.
lang: cs→uk,de→en,en→cs,en→de,en→ja,en→zh,ja→en,zh→en
corr_fcn: accuracy
metric avg-corr task1
CometKiwi-XXL* 1 0.798 1 0.912
CometKiwi-XL* 2 0.795 2 0.905
COMET 3 0.787 8 0.890
CometKiwi* 4 0.787 10 0.887
cometoid22-wmt23* 5 0.786 6 0.897
KG-BERTScore* 6 0.784 12 0.884
MetricX-23-QE-c* 7 0.780 9 0.887
BLEURT-20 8 0.778 15 0.880
MetricX-23-QE-b* 9 0.777 14 0.880
cometoid22-wmt22* 10 0.776 13 0.884
MetricX-23-c 11 0.775 5 0.898
cometoid22-wmt21* 12 0.774 11 0.885
XCOMET-Ensemble 13 0.774 20 0.870
MetricX-23-b 14 0.768 17 0.873
MetricX-23-QE* 15 0.768 19 0.870
MS-COMET-QE-22* 16 0.767 16 0.879
XCOMET-QE-Ensemble* 17 0.766 18 0.871
MetricX-23 18 0.762 22 0.863
YiSi-1 19 0.749 25 0.832
XCOMET-XL 20 0.748 24 0.860
XLsim 21 0.745 26 0.831
XCOMET-XXL 22 0.743 21 0.866
GEMBA-MQM* 23 0.739 4 0.899
prismRef 24 0.736 27 0.808
mre-score-labse-regular 25 0.734 23 0.861
BERTscore 26 0.732 28 0.799
tokengram_F 27 0.714 30 0.786
chrF 28 0.712 33 0.777
f200spBLEU 29 0.708 29 0.786
embed_llama 30 0.701 32 0.778
eBLEU 31 0.694 31 0.781
BLEU 32 0.660 34 0.770
Random-sysname* 33 0.537 35 0.580
prismSrc* 34 0.514 36 0.412
HuaweiTSC_EE_Metric – – 7 0.892
slide* – – 3 0.902
Table19: CorrelationswithWMTDA-SQMscoresforallmetricsonall-pairsdata. Rowsaresortedbytheoverall
averagecorrelationacrossall25tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
lang: cs→uk cs→uk cs→uk
corr_fcn: pearson pearson acc-t
metric avg-corr task1 task2 task3
CometKiwi-XXL* 1 0.798 12 0.889 4 0.462 6 0.555
CometKiwi-XL* 2 0.795 18 0.866 15 0.412 10 0.548
COMET 3 0.787 5 0.899 6 0.454 8 0.553
CometKiwi* 4 0.787 23 0.788 8 0.429 13 0.536
cometoid22-wmt23* 5 0.786 7 0.898 11 0.420 15 0.534
KG-BERTScore* 6 0.784 24 0.788 9 0.429 16 0.530
MetricX-23-QE-c* 7 0.780 3 0.920 2 0.502 9 0.553
BLEURT-20 8 0.778 2 0.926 7 0.443 12 0.538
MetricX-23-QE-b* 9 0.777 6 0.898 16 0.410 4 0.559
cometoid22-wmt22* 10 0.776 19 0.851 19 0.403 19 0.528
MetricX-23-c 11 0.775 1 0.932 1 0.523 5 0.558
cometoid22-wmt21* 12 0.774 21 0.822 14 0.414 23 0.521
XCOMET-Ensemble 13 0.774 8 0.897 3 0.482 3 0.560
MetricX-23-b 14 0.768 13 0.888 17 0.410 1 0.568
MetricX-23-QE* 15 0.768 11 0.889 21 0.382 7 0.555
MS-COMET-QE-22* 16 0.767 20 0.851 23 0.322 24 0.519
XCOMET-QE-Ensemble* 17 0.766 17 0.873 5 0.462 11 0.540
MetricX-23 18 0.762 15 0.879 20 0.395 2 0.567
YiSi-1 19 0.749 26 0.753 25 0.315 20 0.526
XCOMET-XL 20 0.748 14 0.882 10 0.423 18 0.529
XLsim 21 0.745 22 0.792 24 0.318 21 0.526
XCOMET-XXL 22 0.743 9 0.897 18 0.407 33 0.436
GEMBA-MQM* 23 0.739 4 0.913 12 0.419 34 0.323
prismRef 24 0.736 27 0.694 22 0.372 17 0.530
mre-score-labse-regular 25 0.734 25 0.772 13 0.417 14 0.534
BERTscore 26 0.732 32 0.544 26 0.292 22 0.524
tokengram_F 27 0.714 30 0.626 28 0.268 25 0.518
chrF 28 0.712 29 0.637 27 0.273 26 0.517
f200spBLEU 29 0.708 28 0.676 30 0.221 28 0.504
embed_llama 30 0.701 34 0.511 33 0.157 30 0.492
eBLEU 31 0.694 33 0.512 31 0.188 27 0.511
BLEU 32 0.660 31 0.548 32 0.184 31 0.480
Random-sysname* 33 0.537 35 0.343 34 0.047 32 0.469
prismSrc* 34 0.514 36 -0.236 29 0.261 29 0.495
HuaweiTSC_EE_Metric – – 10 0.893 – – – –
slide* – – 16 0.877 – – – –
Table20: CorrelationswithWMTDA-SQMscoresforallmetricsoncs→ukdata. Rowsaresortedbytheoverall
averagecorrelationacrossall25tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
lang: de→en de→en de→en
corr_fcn: pearson pearson acc-t
metric avg-corr task1 task2 task3
CometKiwi-XXL* 1 0.798 15 0.931 14 0.411 11 0.571
CometKiwi-XL* 2 0.795 12 0.934 17 0.402 13 0.569
COMET 3 0.787 6 0.953 2 0.480 2 0.584
CometKiwi* 4 0.787 14 0.933 9 0.447 16 0.559
cometoid22-wmt23* 5 0.786 17 0.913 3 0.471 12 0.571
KG-BERTScore* 6 0.784 13 0.933 7 0.447 20 0.553
MetricX-23-QE-c* 7 0.780 30 0.835 8 0.447 7 0.574
BLEURT-20 8 0.778 3 0.965 1 0.486 5 0.578
MetricX-23-QE-b* 9 0.777 21 0.893 12 0.425 4 0.579
cometoid22-wmt22* 10 0.776 23 0.881 6 0.449 17 0.558
MetricX-23-c 11 0.775 9 0.944 27 0.298 24 0.544
cometoid22-wmt21* 12 0.774 26 0.856 10 0.437 19 0.556
XCOMET-Ensemble 13 0.774 28 0.842 15 0.408 9 0.573
MetricX-23-b 14 0.768 27 0.850 18 0.389 1 0.590
MetricX-23-QE* 15 0.768 24 0.876 13 0.418 8 0.574
MS-COMET-QE-22* 16 0.767 29 0.841 32 0.256 23 0.545
XCOMET-QE-Ensemble* 17 0.766 34 0.813 19 0.385 18 0.556
MetricX-23 18 0.762 31 0.831 20 0.382 3 0.584
YiSi-1 19 0.749 1 0.970 5 0.451 10 0.572
XCOMET-XL 20 0.748 35 0.780 22 0.341 25 0.544
XLsim 21 0.745 8 0.947 23 0.340 15 0.560
XCOMET-XXL 22 0.743 32 0.828 21 0.375 31 0.517
GEMBA-MQM* 23 0.739 10 0.938 4 0.463 34 0.426
prismRef 24 0.736 4 0.963 16 0.403 14 0.565
mre-score-labse-regular 25 0.734 16 0.916 34 0.121 26 0.540
BERTscore 26 0.732 2 0.969 11 0.434 6 0.576
tokengram_F 27 0.714 22 0.891 25 0.319 21 0.551
chrF 28 0.712 25 0.860 24 0.328 22 0.550
f200spBLEU 29 0.708 19 0.904 28 0.291 27 0.539
embed_llama 30 0.701 18 0.913 29 0.275 30 0.525
eBLEU 31 0.694 5 0.954 33 0.207 28 0.538
BLEU 32 0.660 20 0.897 31 0.270 29 0.534
Random-sysname* 33 0.537 37 0.185 35 0.044 33 0.472
prismSrc* 34 0.514 36 0.449 30 0.273 32 0.502
HuaweiTSC_EE_Metric – – 7 0.950 – – – –
slide* – – 11 0.934 – – – –
MaTESe – – 33 0.816 26 0.308 35 0.373
Table21: CorrelationswithWMTDA-SQMscoresforallmetricsonde→endata. Rowsaresortedbytheoverall
averagecorrelationacrossall25tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
lang: en→cs en→cs en→cs
corr_fcn: pearson pearson acc-t
metric avg-corr task1 task2 task3
CometKiwi-XXL* 1 0.798 1 0.922 7 0.367 5 0.548
CometKiwi-XL* 2 0.795 5 0.897 6 0.369 7 0.541
COMET 3 0.787 14 0.865 4 0.377 10 0.524
CometKiwi* 4 0.787 22 0.790 13 0.350 11 0.518
cometoid22-wmt23* 5 0.786 13 0.865 11 0.352 16 0.507
KG-BERTScore* 6 0.784 21 0.790 12 0.350 17 0.507
MetricX-23-QE-c* 7 0.780 6 0.893 3 0.391 8 0.540
BLEURT-20 8 0.778 20 0.793 5 0.373 12 0.510
MetricX-23-QE-b* 9 0.777 10 0.881 18 0.338 2 0.551
cometoid22-wmt22* 10 0.776 17 0.825 16 0.341 18 0.506
MetricX-23-c 11 0.775 23 0.750 19 0.316 13 0.510
cometoid22-wmt21* 12 0.774 18 0.824 17 0.340 14 0.508
XCOMET-Ensemble 13 0.774 3 0.903 1 0.402 6 0.543
MetricX-23-b 14 0.768 11 0.880 15 0.344 1 0.552
MetricX-23-QE* 15 0.768 12 0.878 14 0.348 4 0.549
MS-COMET-QE-22* 16 0.767 19 0.797 21 0.286 21 0.497
XCOMET-QE-Ensemble* 17 0.766 2 0.908 2 0.395 9 0.528
MetricX-23 18 0.762 7 0.891 9 0.361 3 0.550
YiSi-1 19 0.749 26 0.568 24 0.245 24 0.492
XCOMET-XL 20 0.748 4 0.898 8 0.362 15 0.507
XLsim 21 0.745 25 0.627 23 0.259 20 0.503
XCOMET-XXL 22 0.743 8 0.890 10 0.353 33 0.439
GEMBA-MQM* 23 0.739 16 0.852 20 0.309 34 0.327
prismRef 24 0.736 27 0.557 22 0.265 22 0.495
mre-score-labse-regular 25 0.734 24 0.718 33 0.130 19 0.504
BERTscore 26 0.732 30 0.480 25 0.228 23 0.493
tokengram_F 27 0.714 34 0.409 26 0.203 26 0.481
chrF 28 0.712 33 0.450 27 0.201 27 0.480
f200spBLEU 29 0.708 29 0.496 28 0.199 29 0.475
embed_llama 30 0.701 32 0.466 30 0.172 28 0.476
eBLEU 31 0.694 31 0.467 32 0.169 25 0.483
BLEU 32 0.660 28 0.519 29 0.186 30 0.460
Random-sysname* 33 0.537 35 0.015 34 0.002 32 0.452
prismSrc* 34 0.514 36 -0.042 31 0.171 31 0.456
HuaweiTSC_EE_Metric – – 15 0.862 – – – –
slide* – – 9 0.885 – – – –
Table22: CorrelationswithWMTDA-SQMscoresforallmetricsonen→csdata. Rowsaresortedbytheoverall
averagecorrelationacrossall25tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
lang: en→de en→de en→de
corr_fcn: pearson pearson acc-t
metric avg-corr task1 task2 task3
CometKiwi-XXL* 1 0.798 13 0.972 4 0.506 1 0.595
CometKiwi-XL* 2 0.795 5 0.984 3 0.512 3 0.589
COMET 3 0.787 21 0.953 5 0.496 5 0.588
CometKiwi* 4 0.787 1 0.990 1 0.537 6 0.586
cometoid22-wmt23* 5 0.786 26 0.944 6 0.491 12 0.580
KG-BERTScore* 6 0.784 3 0.990 2 0.523 17 0.578
MetricX-23-QE-c* 7 0.780 39 0.859 12 0.465 19 0.576
BLEURT-20 8 0.778 25 0.945 15 0.452 18 0.577
MetricX-23-QE-b* 9 0.777 33 0.910 19 0.437 4 0.588
cometoid22-wmt22* 10 0.776 32 0.911 16 0.447 21 0.575
MetricX-23-c 11 0.775 2 0.990 8 0.482 13 0.580
cometoid22-wmt21* 12 0.774 34 0.905 20 0.433 22 0.574
XCOMET-Ensemble 13 0.774 38 0.861 25 0.399 20 0.576
MetricX-23-b 14 0.768 35 0.896 30 0.377 8 0.583
MetricX-23-QE* 15 0.768 37 0.867 18 0.443 11 0.582
MS-COMET-QE-22* 16 0.767 28 0.942 33 0.371 28 0.558
XCOMET-QE-Ensemble* 17 0.766 41 0.849 29 0.382 27 0.564
MetricX-23 18 0.762 40 0.855 28 0.389 9 0.582
YiSi-1 19 0.749 6 0.980 13 0.456 23 0.571
XCOMET-XL 20 0.748 42 0.845 34 0.365 32 0.552
XLsim 21 0.745 7 0.979 27 0.391 25 0.566
XCOMET-XXL 22 0.743 36 0.868 24 0.399 39 0.525
GEMBA-MQM* 23 0.739 17 0.961 7 0.488 42 0.434
prismRef 24 0.736 16 0.963 37 0.321 36 0.544
mre-score-labse-regular 25 0.734 30 0.927 42 0.144 35 0.548
BERTscore 26 0.732 12 0.973 23 0.417 24 0.567
tokengram_F 27 0.714 27 0.943 32 0.371 30 0.556
chrF 28 0.712 24 0.945 31 0.374 31 0.553
f200spBLEU 29 0.708 14 0.970 36 0.324 29 0.557
embed_llama 30 0.701 23 0.951 35 0.348 34 0.550
eBLEU 31 0.694 31 0.920 41 0.159 37 0.542
BLEU 32 0.660 18 0.958 38 0.275 38 0.541
Random-sysname* 33 0.537 44 0.278 43 0.075 41 0.482
prismSrc* 34 0.514 45 -0.364 40 0.190 40 0.485
HuaweiTSC_EE_Metric – – 10 0.975 – – – –
instructscore – – 8 0.977 10 0.473 15 0.578
slide* – – 4 0.984 – – – –
Calibri-COMET22 – – 22 0.953 21 0.425 7 0.584
Calibri-COMET22-QE* – – 19 0.957 17 0.445 33 0.551
MEE4 – – 15 0.968 22 0.421 26 0.565
MaTESe – – 43 0.791 39 0.272 43 0.375
docWMT22CometDA – – 29 0.941 14 0.454 2 0.593
docWMT22CometKiwiDA* – – 11 0.973 26 0.392 14 0.579
mbr-metricx-qe* – – 20 0.954 9 0.477 10 0.582
sescoreX – – 9 0.977 11 0.473 16 0.578
Table23: CorrelationswithWMTDA-SQMscoresforallmetricsonen→dedata. Rowsaresortedbytheoverall
averagecorrelationacrossall25tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
lang: en→ja en→ja en→ja
corr_fcn: pearson pearson acc-t
metric avg-corr task1 task2 task3
CometKiwi-XXL* 1 0.798 3 0.993 2 0.527 2 0.592
CometKiwi-XL* 2 0.795 2 0.993 1 0.528 1 0.593
COMET 3 0.787 12 0.969 6 0.462 11 0.580
CometKiwi* 4 0.787 6 0.984 4 0.516 4 0.588
cometoid22-wmt23* 5 0.786 11 0.979 10 0.449 13 0.574
KG-BERTScore* 6 0.784 5 0.984 3 0.516 7 0.583
MetricX-23-QE-c* 7 0.780 22 0.955 8 0.456 9 0.580
BLEURT-20 8 0.778 4 0.990 15 0.417 15 0.569
MetricX-23-QE-b* 9 0.777 21 0.956 14 0.428 3 0.590
cometoid22-wmt22* 10 0.776 20 0.960 11 0.449 14 0.569
MetricX-23-c 11 0.775 28 0.918 23 0.371 25 0.545
cometoid22-wmt21* 12 0.774 16 0.964 12 0.442 16 0.568
XCOMET-Ensemble 13 0.774 26 0.920 5 0.470 6 0.586
MetricX-23-b 14 0.768 23 0.941 16 0.413 5 0.587
MetricX-23-QE* 15 0.768 30 0.898 17 0.411 10 0.580
MS-COMET-QE-22* 16 0.767 9 0.983 7 0.458 18 0.565
XCOMET-QE-Ensemble* 17 0.766 31 0.895 9 0.455 12 0.574
MetricX-23 18 0.762 29 0.916 18 0.401 8 0.580
YiSi-1 19 0.749 7 0.984 21 0.382 20 0.561
XCOMET-XL 20 0.748 34 0.821 19 0.397 21 0.558
XLsim 21 0.745 27 0.918 24 0.354 22 0.557
XCOMET-XXL 22 0.743 32 0.871 20 0.394 31 0.485
GEMBA-MQM* 23 0.739 8 0.983 13 0.429 33 0.389
prismRef 24 0.736 25 0.922 22 0.371 19 0.561
mre-score-labse-regular 25 0.734 10 0.979 31 0.120 17 0.566
BERTscore 26 0.732 18 0.962 26 0.317 23 0.550
tokengram_F 27 0.714 13 0.969 27 0.227 24 0.548
chrF 28 0.712 14 0.966 28 0.220 26 0.543
f200spBLEU 29 0.708 19 0.961 30 0.190 29 0.523
embed_llama 30 0.701 15 0.964 29 0.212 28 0.524
eBLEU 31 0.694 24 0.926 32 0.073 30 0.522
BLEU 32 0.660 33 0.833 34 0.001 34 0.070
Random-sysname* 33 0.537 36 0.307 33 0.064 32 0.484
prismSrc* 34 0.514 35 0.764 25 0.322 27 0.530
HuaweiTSC_EE_Metric – – 17 0.963 – – – –
slide* – – 1 0.995 – – – –
Table24: CorrelationswithWMTDA-SQMscoresforallmetricsonen→jadata. Rowsaresortedbytheoverall
averagecorrelationacrossall25tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
lang: en→zh en→zh en→zh
corr_fcn: pearson pearson acc-t
metric avg-corr task1 task2 task3
CometKiwi-XXL* 1 0.798 14 0.982 7 0.559 3 0.601
CometKiwi-XL* 2 0.795 8 0.988 4 0.588 1 0.601
COMET 3 0.787 3 0.995 6 0.575 8 0.589
CometKiwi* 4 0.787 4 0.994 1 0.635 7 0.590
cometoid22-wmt23* 5 0.786 1 0.997 5 0.588 11 0.584
KG-BERTScore* 6 0.784 5 0.994 2 0.635 10 0.584
MetricX-23-QE-c* 7 0.780 28 0.913 18 0.468 12 0.582
BLEURT-20 8 0.778 9 0.988 8 0.550 18 0.571
MetricX-23-QE-b* 9 0.777 19 0.963 19 0.456 2 0.601
cometoid22-wmt22* 10 0.776 7 0.989 9 0.537 15 0.574
MetricX-23-c 11 0.775 24 0.937 12 0.507 23 0.563
cometoid22-wmt21* 12 0.774 10 0.988 10 0.527 16 0.573
XCOMET-Ensemble 13 0.774 21 0.944 14 0.493 4 0.596
MetricX-23-b 14 0.768 27 0.926 23 0.420 5 0.595
MetricX-23-QE* 15 0.768 22 0.943 22 0.439 6 0.594
MS-COMET-QE-22* 16 0.767 2 0.996 3 0.610 19 0.570
XCOMET-QE-Ensemble* 17 0.766 30 0.908 21 0.450 14 0.577
MetricX-23 18 0.762 33 0.885 24 0.411 9 0.588
YiSi-1 19 0.749 15 0.977 15 0.493 20 0.566
XCOMET-XL 20 0.748 35 0.790 26 0.366 28 0.542
XLsim 21 0.745 12 0.985 11 0.524 17 0.572
XCOMET-XXL 22 0.743 32 0.885 25 0.391 31 0.517
GEMBA-MQM* 23 0.739 18 0.973 16 0.489 33 0.385
prismRef 24 0.736 17 0.975 13 0.496 21 0.564
mre-score-labse-regular 25 0.734 11 0.986 32 0.177 13 0.577
BERTscore 26 0.732 16 0.975 17 0.474 22 0.563
tokengram_F 27 0.714 20 0.945 27 0.343 24 0.558
chrF 28 0.712 25 0.934 29 0.326 25 0.550
f200spBLEU 29 0.708 31 0.905 28 0.327 26 0.547
embed_llama 30 0.701 26 0.927 30 0.297 27 0.542
eBLEU 31 0.694 29 0.912 31 0.210 29 0.535
BLEU 32 0.660 34 0.804 33 0.093 34 0.141
Random-sysname* 33 0.537 36 0.046 34 0.018 32 0.462
prismSrc* 34 0.514 23 0.941 20 0.452 30 0.527
HuaweiTSC_EE_Metric – – 6 0.992 – – – –
slide* – – 13 0.982 – – – –
Table25: CorrelationswithWMTDA-SQMscoresforallmetricsonen→zhdata. Rowsaresortedbytheoverall
averagecorrelationacrossall25tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
lang: ja→en ja→en ja→en
corr_fcn: pearson pearson acc-t
metric avg-corr task1 task2 task3
CometKiwi-XXL* 1 0.798 2 0.984 1 0.474 2 0.578
CometKiwi-XL* 2 0.795 3 0.982 4 0.446 6 0.573
COMET 3 0.787 16 0.968 5 0.445 3 0.576
CometKiwi* 4 0.787 10 0.975 3 0.455 10 0.568
cometoid22-wmt23* 5 0.786 19 0.966 7 0.435 15 0.560
KG-BERTScore* 6 0.784 9 0.975 2 0.455 12 0.561
MetricX-23-QE-c* 7 0.780 21 0.965 11 0.418 7 0.572
BLEURT-20 8 0.778 22 0.964 6 0.436 9 0.570
MetricX-23-QE-b* 9 0.777 12 0.972 16 0.383 4 0.575
cometoid22-wmt22* 10 0.776 25 0.946 8 0.432 19 0.550
MetricX-23-c 11 0.775 11 0.972 22 0.342 24 0.547
cometoid22-wmt21* 12 0.774 26 0.944 9 0.431 20 0.549
XCOMET-Ensemble 13 0.774 24 0.947 12 0.410 5 0.574
MetricX-23-b 14 0.768 29 0.938 21 0.343 1 0.578
MetricX-23-QE* 15 0.768 30 0.936 20 0.344 11 0.567
MS-COMET-QE-22* 16 0.767 34 0.916 14 0.388 22 0.548
XCOMET-QE-Ensemble* 17 0.766 31 0.935 13 0.388 16 0.557
MetricX-23 18 0.762 33 0.918 24 0.332 8 0.572
YiSi-1 19 0.749 6 0.978 15 0.383 13 0.561
XCOMET-XL 20 0.748 32 0.922 25 0.327 23 0.547
XLsim 21 0.745 1 0.989 23 0.342 18 0.552
XCOMET-XXL 22 0.743 27 0.941 18 0.352 31 0.492
GEMBA-MQM* 23 0.739 4 0.982 10 0.421 34 0.395
prismRef 24 0.736 13 0.971 19 0.351 17 0.557
mre-score-labse-regular 25 0.734 5 0.980 33 0.186 21 0.548
BERTscore 26 0.732 7 0.977 17 0.357 14 0.560
tokengram_F 27 0.714 18 0.967 27 0.290 25 0.546
chrF 28 0.712 20 0.966 26 0.292 26 0.545
f200spBLEU 29 0.708 23 0.955 29 0.226 28 0.528
embed_llama 30 0.701 14 0.969 31 0.203 29 0.524
eBLEU 31 0.694 15 0.969 32 0.202 27 0.530
BLEU 32 0.660 28 0.939 30 0.221 30 0.517
Random-sysname* 33 0.537 36 0.288 35 0.061 32 0.481
prismSrc* 34 0.514 37 -0.747 34 0.171 33 0.470
HuaweiTSC_EE_Metric – – 17 0.967 – – – –
slide* – – 8 0.976 – – – –
MaTESe – – 35 0.904 28 0.242 35 0.326
Table26: CorrelationswithWMTDA-SQMscoresforallmetricsonja→endata. Rowsaresortedbytheoverall
averagecorrelationacrossall25tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
lang: zh→en zh→en zh→en
corr_fcn: pearson pearson acc-t
metric avg-corr task1 task2 task3
CometKiwi-XXL* 1 0.798 1 0.938 3 0.435 4 0.540
CometKiwi-XL* 2 0.795 3 0.936 6 0.427 9 0.535
COMET 3 0.787 21 0.811 11 0.378 15 0.525
CometKiwi* 4 0.787 4 0.931 1 0.460 10 0.534
cometoid22-wmt23* 5 0.786 9 0.913 10 0.402 16 0.523
KG-BERTScore* 6 0.784 5 0.927 2 0.448 18 0.521
MetricX-23-QE-c* 7 0.780 14 0.843 13 0.373 6 0.537
BLEURT-20 8 0.778 25 0.766 17 0.331 21 0.520
MetricX-23-QE-b* 9 0.777 17 0.823 21 0.298 1 0.544
cometoid22-wmt22* 10 0.776 8 0.918 5 0.432 19 0.520
MetricX-23-c 11 0.775 7 0.924 16 0.339 24 0.512
cometoid22-wmt21* 12 0.774 10 0.908 7 0.419 20 0.520
XCOMET-Ensemble 13 0.774 20 0.816 18 0.322 7 0.537
MetricX-23-b 14 0.768 26 0.759 26 0.261 3 0.540
MetricX-23-QE* 15 0.768 24 0.770 22 0.284 5 0.538
MS-COMET-QE-22* 16 0.767 6 0.927 8 0.418 22 0.519
XCOMET-QE-Ensemble* 17 0.766 22 0.803 19 0.315 17 0.522
MetricX-23 18 0.762 30 0.735 24 0.264 8 0.536
YiSi-1 19 0.749 31 0.715 25 0.263 28 0.511
XCOMET-XL 20 0.748 28 0.758 27 0.254 27 0.512
XLsim 21 0.745 32 0.702 33 0.218 26 0.512
XCOMET-XXL 22 0.743 23 0.787 23 0.275 39 0.463
GEMBA-MQM* 23 0.739 11 0.873 14 0.370 41 0.356
prismRef 24 0.736 41 0.632 31 0.229 25 0.512
mre-score-labse-regular 25 0.734 18 0.817 38 0.146 30 0.509
BERTscore 26 0.732 33 0.702 30 0.236 23 0.515
tokengram_F 27 0.714 37 0.670 37 0.167 31 0.503
chrF 28 0.712 35 0.701 35 0.168 32 0.503
f200spBLEU 29 0.708 39 0.651 39 0.139 36 0.483
embed_llama 30 0.701 34 0.702 41 0.123 34 0.494
eBLEU 31 0.694 42 0.629 42 0.107 35 0.494
BLEU 32 0.660 43 0.610 40 0.134 37 0.475
Random-sysname* 33 0.537 44 -0.144 43 -0.026 40 0.446
prismSrc* 34 0.514 45 -0.457 28 0.248 38 0.471
HuaweiTSC_EE_Metric – – 19 0.816 – – – –
instructscore – – 38 0.652 32 0.227 42 0.342
slide* – – 12 0.863 – – – –
Calibri-COMET22 – – 27 0.759 20 0.313 13 0.529
Calibri-COMET22-QE* – – 13 0.854 12 0.375 11 0.530
MEE4 – – 40 0.632 36 0.168 33 0.498
MaTESe – – 29 0.739 34 0.201 43 0.319
docWMT22CometDA – – 15 0.836 15 0.345 12 0.530
docWMT22CometKiwiDA* – – 2 0.938 9 0.403 2 0.542
mbr-metricx-qe* – – 16 0.827 4 0.435 14 0.526
sescoreX – – 36 0.695 29 0.238 29 0.509
Table27: CorrelationswithWMTDA-SQMscoresforallmetricsonzh→endata. Rowsaresortedbytheoverall
averagecorrelationacrossall25tasks(leftmostcolumn). Starredmetricsarereference-free,underlinedmetricsare
baselines,anditalicizedmetricsarecontrastivesubmissions.
D Additionalfigures
Figures 9-14 show the (log) p-value of one-sided paired t-test on the MQM scores against the score
differenceofeachmetricforeachsystempairineachtranslationdirection. Figures15-20showthe(log)
p-valueofsignificancetestwithbootstrapresamplingonthemetricscoresagainstthescoredifferenceof
thatmetricforeachsystempairineachtranslationdirection.
en→de he→en zh→en
Figure 9: Log p-value of one-sided paired t-test on MQM scores (p ) against the score difference of each
mqm
metric(toptobottom: BERTScore,BLEU,BLEURT-20,CALIBRI-COMET22,CALIBRI-COMET22-QE)for
eachsystempairineachtranslationdirection(lefttoright: en→de,he→en,zh→en). Theredlineistheisotonic
regressionfittoalldatapoints,representingPr(p < 0.05|∆M). Note: forreadability,valuesofp are
mqm mqm
roundedupto0.0001whentheyarelessthan0.0001.
en→de he→en zh→en
Figure 10: Log p-value of one-sided paired t-test on MQM scores (p ) against the score difference of
mqm
eachmetric(toptobottom: CHRF, COMET, COMETKIWI, COMETOID22-WMT22, DOCWMT22COMETDA,
DOCWMT22COMETKIWIDA)foreachsystempairineachtranslationdirection(lefttoright: en→de,he→en,
zh→en). Theredlineistheisotonicregressionfittoalldatapoints,representingPr(p <0.05|∆M). Note:
mqm
forreadability,valuesofp areroundedupto0.0001whentheyarelessthan0.0001.
mqm
en→de he→en zh→en
Figure11:Logp-valueofone-sidedpairedt-testonMQMscores(p )againstthescoredifferenceofeachmetric
mqm
(toptobottom: EBLEU,EMBED_LLAMA, F200SPBLEU, GEMBA-MQM,KG-BERTSCORE, MATESE)for
eachsystempairineachtranslationdirection(lefttoright: en→de,he→en,zh→en). Theredlineistheisotonic
regressionfittoalldatapoints,representingPr(p < 0.05|∆M). Note: forreadability,valuesofp are
mqm mqm
roundedupto0.0001whentheyarelessthan0.0001.
en→de he→en zh→en
Figure12:Logp-valueofone-sidedpairedt-testonMQMscores(p )againstthescoredifferenceofeachmetric
mqm
(toptobottom: MBR-METRICX-QE,MEE4,METRICX-23,METRICX-23-QE,MRE-SCORE-LABSE-REGULAR,
MS-COMET-QE-22)foreachsystempairineachtranslationdirection(lefttoright:en→de,he→en,zh→en).The
redlineistheisotonicregressionfittoalldatapoints,representingPr(p <0.05|∆M). Note: forreadability,
mqm
valuesofp areroundedupto0.0001whentheyarelessthan0.0001.
mqm
en→de he→en zh→en
Figure13: Logp-valueofone-sidedpairedt-testonMQMscores(p )againstthescoredifferenceofeach
mqm
metric(toptobottom: PRISMREF,PRISMSRC,RANDOM-SYSNAME,SESCOREX,SLIDE,TOKENGRAM_F)for
eachsystempairineachtranslationdirection(lefttoright: en→de,he→en,zh→en). Theredlineistheisotonic
regressionfittoalldatapoints,representingPr(p < 0.05|∆M). Note: forreadability,valuesofp are
mqm mqm
roundedupto0.0001whentheyarelessthan0.0001.
en→de he→en zh→en
Figure14:Logp-valueofone-sidedpairedt-testonMQMscores(p )againstthescoredifferenceofeachmetric
mqm
(toptobottom: XCOMET-ENSEMBLE,XCOMET-QE-ENSEMBLE,XLSIM,YISI-1)foreachsystempairineach
translationdirection(lefttoright: en→de,he→en,zh→en). Theredlineistheisotonicregressionfittoalldata
points,representingPr(p <0.05|∆M). Note: forreadability,valuesofp areroundedupto0.0001when
mqm mqm
theyarelessthan0.0001.
en→de he→en zh→en
Figure15: Logp-valueofsignificancetestwithbootstrapresampling(p )onsystem-levelmetricscoresagainst
M
eachmetric(toptobottom: BERTSCORE,BLEU,BLEURT-20,CALIBRI-COMET22,CALIBRI-COMET22-QE,
CHRF)scoredifferenceforeachsystempairineachtranslationdirection(lefttoright: en→de,he→en,zh→en).
Theredlineistheisotonicregressionfittoalldatapoints,representingPr(p <0.05|∆M). Note: forreadability,
M
valuesofp areroundedupto0.0001whentheyarelessthan0.0001.
M
en→de he→en zh→en
Figure 16: Log p-value of significance test with bootstrap resampling (p ) on system-level metric scores
M
againsteachmetric(toptobottom: COMET,COMETKIWI,COMETOID22-WMT22,DOCWMT22COMETDA,
DOCWMT22COMETKIWIDA,EBLEU)scoredifferenceforeachsystempairineachtranslationdirection(left
to right: en→de, he→en, zh→en). The red line is the isotonic regression fit to all data points, representing
Pr(p <0.05|∆M). Note: forreadability,valuesofp areroundedupto0.0001whentheyarelessthan0.0001.
M M
en→de he→en zh→en
Figure17: Logp-valueofsignificancetestwithbootstrapresampling(p )onsystem-levelmetricscoresagainst
M
each metric (top to bottom: EMBED_LLAMA, F200SPBLEU, GEMBA-MQM, KG-BERTSCORE, MATESE,
MBR-METRICX-QE)scoredifferenceforeachsystempairineachtranslationdirection(lefttoright: en→de,he→en,
zh→en). Theredlineistheisotonicregressionfittoalldatapoints,representingPr(p <0.05|∆M). Note: for
M
readability,valuesofp areroundedupto0.0001whentheyarelessthan0.0001.
M
en→de he→en zh→en
Figure18:Logp-valueofsignificancetestwithbootstrapresampling(p )onsystem-levelmetricscoresagainsteach
M
metric(toptobottom: MEE4,METRICX-23,METRICX-23-QE,MRE-SCORE-LABSE-REGULAR,MS-COMET-
QE-22,PRISMREF)scoredifferenceforeachsystempairineachtranslationdirection(lefttoright: en→de,he→en,
zh→en). Theredlineistheisotonicregressionfittoalldatapoints,representingPr(p <0.05|∆M). Note: for
M
readability,valuesofp areroundedupto0.0001whentheyarelessthan0.0001.
M
en→de he→en zh→en
Figure19:Logp-valueofsignificancetestwithbootstrapresampling(p )onsystem-levelmetricscoresagainsteach
M
metric(toptobottom: PRISMSRC,RANDOM-SYSNAME,SESCOREX,TOKENGRAM_F,XCOMET-ENSEMBLE,
XCOMET-QE-ENSEMBLE)scoredifferenceforeachsystempairineachtranslationdirection(lefttoright:en→de,
he→en,zh→en). Theredlineistheisotonicregressionfittoalldatapoints,representingPr(p < 0.05|∆M).
M
Note: forreadability,valuesofp areroundedupto0.0001whentheyarelessthan0.0001.
M
en→de he→en zh→en
Figure20: Logp-valueofsignificancetestwithbootstrapresampling(p )onsystem-levelmetricscoresagainst
M
eachmetric(toptobottom: XLSIM, YISI-1)scoredifferenceforeachsystempairineachtranslationdirection
(lefttoright: en→de,he→en,zh→en). Theredlineistheisotonicregressionfittoalldatapoints,representing
Pr(p <0.05|∆M). Note: forreadability,valuesofp areroundedupto0.0001whentheyarelessthan0.0001.
M M

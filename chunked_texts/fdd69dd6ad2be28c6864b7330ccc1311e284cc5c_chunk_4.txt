ting Keyword 10×2×3
ESC-50 EnvironmentalSoundEvent 18×2×3
modelsfromscratch[28,23,29,20,24,21]. Specifically,asshowninTable1a,ittakesabout335
GPUdays(279GPUdayswithoutImageNet)toevaluateFixMatch[20]withTorchSSL[21]. Such
ahighcostcanmakeitunaffordableforresearchlabs(particularlyinacademia)toconductSSL
research. Recently,thepre-trainingandfine-tuningparadigm[30,31,32,33]achievespromising
results. Comparedwithtrainingfromscratch,pre-traininghasmuchreducedcostinSSL.However,
therearerelativelyfewbenchmarksthatofferafairtestbedforSSLwiththepre-trainedversionsof
neuralmodels.
ToaddresstheaboveissuesandfacilitategeneralSSLresearch,weproposeUSB:aUnifiedSSL
Benchmarkforclassification3. USBoffersadiverseandchallengingbenchmarkacrossfiveCV
datasets,fiveNLPdatasets,andfiveAudiodatasets(Table1b),enablingconsistentevaluationover
multipletasksfromdifferentdomains. Moreover,USBprovidescomprehensiveevaluationsofSSL
algorithmswithevenfewerlabeleddatacomparedwithTorchSSL,astheperformancegapbetween
SSLalgorithmsdiminisheswhentheamountoflabeledsamplesbecomeslarge. Benefitingfromthe
rapidlydevelopedneuralarchitectures,weintroducepre-trainedTransformers[4]intoSSLinsteadof
trainingResNets[1]fromscratchtoreducethetrainingcostforCVtasks. Specifically,wefindthat
usingpre-trainedVisionTransformers(ViT)[34]canlargelyreducethenumberoftrainingiterations
(e.g.,by80%from1,000kto200konCVtasks)withouthurtingtheperformance,andmostSSL
algorithmsachieveevenbetterperformancewithlesstrainingiterations.
AsillustratedinTable1b,usingUSB,wespendonly39GPUdaystoevaluatetheperformanceof
anSSLalgorithm(i.e., FixMatch)onasingle
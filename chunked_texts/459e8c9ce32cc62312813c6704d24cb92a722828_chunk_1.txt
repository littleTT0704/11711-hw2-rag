TheThirty-FifthAAAIConferenceonArtificialIntelligence(AAAI-21)
MLE-Guided Parameter Search for Task Loss Minimization in Neural Sequence
Modeling
SeanWelleck,(cid:3) KyunghyunCho
NewYorkUniversity
Abstract lossfunctions,andthediscrepancybetweenthesequencedis-
tributionsusedfortrainingandthedistributionencountered
Neuralautoregressivesequencemodelsareusedtogenerate
atevaluationtimehaspromptedalternativesequence-level
sequencesinavarietyofnaturallanguageprocessing(NLP)
trainingalgorithms(e.g.(DaumeÂ´,Langford,andMarcu2009;
tasks,wheretheyareevaluatedaccordingtosequence-level
Ranzatoetal.2016;Shenetal.2016)).Nevertheless,max-
task losses. These models are typically trained with maxi-
mumlikelihoodestimation,whichignoresthetaskloss,yet imizingthelikelihoodhasempiricallyperformedwellasa
empiricallyperformswellasasurrogateobjective.Typical surrogatetominimizingthetaskloss,achievingstrongperfor-
approachestodirectlyoptimizingthetasklosssuchaspolicy manceontheaforementionedtasks.Inthispaper,wedevelop
gradientandminimumrisktrainingarebasedaroundsampling asequence-leveltrainingprocedurethataddressesthedown-
inthesequencespacetoobtaincandidateupdatedirections sidesofmaximumlikelihoodbyleveragingitsstrengthsasa
thatarescoredbasedonthelossofasinglesequence.Inthis surrogateobjective.
paper, we develop an alternative method based on random
Itischallengingtooptimizeataskloss,asthelossistypi-
searchintheparameterspacethatleveragesaccesstothemax-
imumlikelihoodgradient.Weproposemaximumlikelihood callynon-differentiablewithrespecttothemodelparameters,
guidedparametersearch(MGS),whichsamplesfromadis- andoptimizationisdoneoverahigh-dimensionalparameter
tributionoverupdatedirectionsthatisamixtureofrandom space.Typ
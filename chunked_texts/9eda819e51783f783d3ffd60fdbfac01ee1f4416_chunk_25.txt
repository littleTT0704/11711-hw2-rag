attentional
translationmodels. InProceedingsoftheThirdCon-
ference on Machine Translation: Research Papers,
pages 261–271, Belgium, Brussels. Association for
ComputationalLinguistics.
Adam Santoro, Sergey Bartunov, Matthew Botvinick,
Daan Wierstra, and Timothy Lillicrap. 2016. One-
shot learning with memory-augmented neural net-
works. arXivpreprintarXiv:1605.06065.
Timo Schick and Hinrich Schu¨tze. 2021. Few-shot
text generation with natural language instructions.
In Proceedings of the 2021 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,pages
390–402, Online and Punta Cana, Dominican Re-
public.AssociationforComputationalLinguistics.
ThomasScialom,Paul-AlexisDray,SylvainLamprier,
Benjamin Piwowarski, and Jacopo Staiano. 2020.
Mlsum: The multilingual summarization corpus.
arXivpreprintarXiv:2004.14900.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequencetosequencelearningwithneuralnetworks.
In Advances in neural information processing sys-
tems,pages3104–3112.
Daniel Varab and Natalie Schluter. 2021. Mas-
sivesumm: a very large-scale, very multilingual,
news summarisation dataset. In Proceedings of the
2021 Conference on Empirical Methods in Natural
LanguageProcessing,pages10150–10161.
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2021. mT5: A massively
multilingualpre-trainedtext-to-texttransformer. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 483–498, Online. Association for Computa-
tionalLinguistics.
JingqingZhang,YaoZhao,MohammadSaleh,
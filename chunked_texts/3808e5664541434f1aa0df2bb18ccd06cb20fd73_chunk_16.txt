.3
LMIXIN-Dialect - 92.26
0.1
16.07
0.4
Random 0.4027 92.18 16.67
0.1 0.6
AFLite 0.3577 91.94 16.84
0.1 0.8
DataMaps-Ambig. 0.2965 92.45 15.99
0.1 0.4
DataMaps-Hard 0.2878 92.61 13.71
0.1 0.2
DataMaps-Easy 0.5347 91.94 19.46
0.2 2.8
AAE-relabeled 0.3453 91.64
0.3
12.69
0.0
Table 4: Dialectal bias evaluation for all debiasing
methods (§5), as well as the relabeling approach (§6)
Figure 2: Challenge set evaluation for lexical biases,
on the Founta et al. (2018) test set. We report F and
1
comparingalldebiasingmethodswithbaselines,using
the false positive rate with respect to tweets in AAE
theONI-Advtestset.Takeaway:F 1(↑)measuresshow
(FPR ), reflecting dialectal bias (lower is less bi-
AAE
thatallmodelsperformpoorlyatidentifyingtoxictext
ased),showingmeanands.d. (subscript)across3runs.
notcontainingovertlylexicalcuesoftoxicity. Ingen-
(TopBlock)Debiasedtrainingapproaches,alongwith
eral,debiasedtrainingapproachesoutperformtheorig-
thevanillaclassifier,arealltrainedonthefulldataset.
inalmodelonthischallengeset,whiledatafilteringis
(Middle Block) Random, AFLite and DataMaps all
notaseffective.
are trained on only 33% of the training data. Best
performance for each training set size is in boldface.
Takeaway: Both debiasing approaches improve per-
sistent with Table 1). Notably, DataMaps-Hard
formanceoverbaselines,withDataMaps-Hardproving
performs the best at dialectal debiasing, both in
the most effective at debiasing. (Bottom Block) A
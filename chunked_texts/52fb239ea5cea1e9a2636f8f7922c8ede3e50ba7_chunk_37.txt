. Roth. 2019. “going on a vacation” takes longer
Glue:Amulti-taskbenchmarkandanalysisplatform than“goingforawalk”: Astudyoftemporalcom-
for natural language understanding. arXiv preprint monsense understanding. In Proceedings of the
arXiv:1804.07461. 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Yizhong Wang, Swaroop Mishra, Pegah Alipoor- Joint Conference on Natural Language Processing
molabashi, Yeganeh Kordi, Amirreza Mirzaei, (EMNLP-IJCNLP),pages3363–3369.
Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, et al.
2022. Benchmarking generalization via in-context
instructions on 1,600+ language tasks. arXiv
preprintarXiv:2204.07705.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drewMDai,andQuocVLe.2021. Finetunedlan-
guagemodelsarezero-shotlearners. arXivpreprint
arXiv:2109.01652.
Sean Welleck, Jiacheng Liu, Ronan Le Bras, Han-
naneh Hajishirzi, Yejin Choi, and Kyunghyun Cho.
2021. Naturalproofs: Mathematical theorem prov-
ing in natural language. In Thirty-fifth Conference
onNeuralInformationProcessingSystemsDatasets
andBenchmarksTrack(Round1).
Sean Welleck, Peter West, Jize Cao, and Yejin Choi.
2022. Symbolicbrittlenessinsequencemodels: on
systematic generalization in symbolic mathematics.
InAAAI.
JasonWeston,AntoineBordes,SumitChopra,Alexan-
derMRush,BartvanMerriënboer,ArmandJoulin,
and Tomas Mikolov. 2015. Towards ai-complete
questionanswering: Asetofprerequisit
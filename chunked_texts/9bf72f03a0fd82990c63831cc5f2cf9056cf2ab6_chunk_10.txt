bitsand1%in3bitsinGLUEtasks,
which shows the necessity of quantization-aware
training (QAT) over PQT. We compare SQuAT
Algorithm1:AlternateTraining
withotherQATmethods,includingQ-BERT(Shen
1 whilenotconvergingdo etal.,2020)andthecurrentSOTALSQ(Esseretal.,
→sampleBatchB;
2020). Whenquantizingto2-bit,3-bit,and4-bit,
→computegradient∇ L(Q(w,s ))
Q t t BERTmodelquantizedwithSQuAToutperforms
→compute(cid:15)(Q(w,s ))
t t LSQ quantized BERT by 2% on average for all
→updatew withSTEgradient
8 GLUE tasks. Remarkably, our 3-bit and 4-bit
w ← w −η·∇ L(Q(w,s )+(cid:15))
t+1 t Q t t performanceexceedthefull-precisionscoreby1%
→updatesw.r.tweights
atseveralGLUEtasks.
s ← s +η·∇ L(Q(w,s ))
t+1 t s t+1 t To show that the SQuAT quantization method
reducesthesharpnessoftheminima,wemeasure
1Moredetailsofthesetupisincludedintheappendix
the sharpness score of the local minimum in the
losslandscapefollowing(Foretetal.,2020;Mehta
et al., 2021). The result is shown in Figure. 2 2.
Compared to LSQ, we observe that the models
trainedwithourSQuATquantizationconvergeto
muchflatterminimaacrossallGLUEtasks.
5 Limitation
SQuATmethodoptimizesforthesharpness-aware
perturbation and step size alternatively, and thus
wouldincurmorecomputationaloverheadsandpo-
tentiallylongertraintimecomparedtotheclassical
QAT methods, but there is no additional cost in
inferencetime. Inevitably,asamethodofQAT,we
inher
supportedparallelism,learningwiderarchitectures
amplesinclude: PyTorch(Paszkeetal.,2019)and
on GPU devices and deeper models on CPU de-
Chainer(Tokuietal.,2015).
vices (Cai et al., 2018; Tan et al., 2019; Sandler
et al., 2018). However, it is often impossible to
DeferredExecution: Aseriesofoperationsare
controlforthehardwaresystemsusedforcollect-
definedandexecutedonsampledatatogeneratea
ingthesemetricsleadingtoconclusionsthatmay
dataflowgraphthatcanthenbejust-in-time(JiT)
notgeneralizeacrossdeploymentsettings.
compiled. Examples include: TorchScript, Jax
(Bradbury et al., 2018), Theano (Al-Rfou et al.,
2.3 PlatformPerformanceAnalysis
2016),Caffe(Jiaetal.,2014).
Effortstoestablishcommonbenchmarksleverage
referencemodelsandhardwareplatformswithtar- Static: Thecomputationalgraphispre-defined,
getlatencyoraccuracy(Reddietal.,2020;Zhou compiled, and executed inside a specialized run-
et al., 2020). Although these efforts have led to time;allowingforaggressive,globalahead-of-time
improvementinend-to-endlatency,theyoftenab- (AoT)compileroptimizations. Examplesinclude:
CPU LL aa yu en rc h 1 Launch Layer 2 Launch Layer 3 Launch Layer 4 0.030 T Co Ut Dal A M Ko ed re nl e L l a Et xe en cc uy tion Time
Framework Overhead
GPU
Compute Compute Compute Compute
Kernel 1 Kernel 2 Kernel 3 Kernel 4 0.025
Time
0.020
CPU
Launch Layer 1 Launch Layer 2 Launch Layer 3 Launch Layer 4
0.015
GPU Compute Compute Compute Compute 0.010
Kernel 1 Kernel 2 Kernel 3 Kernel 4
0.005
Figure2: Profileswhereexecutionisframeworkbound
0 10 20 30 40 50 60
byCPUkerneldispatchoperations(above)andcompute-
Batch Size
boundbyGPUkerneloperations(below
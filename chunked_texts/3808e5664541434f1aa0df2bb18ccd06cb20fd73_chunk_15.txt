 are good at predicting subtle,
and debiasing effects, and expose a limitation of non-overttoxicity.
thesedatasets.
5 Experiments: DialectalandRacial
4.4 AdversarialEvaluation: ONI-Adv
Biases
To further study the reliance of debiased models
Wetesttheefficacyofthebiasreductionmethods
on the ONI words, we use the test set from Di-
from§3fordialectalbias(§2.2)reduction.
nan et al. (2019), which contains 3000 manually
created sentences, 300 toxic. The toxic sentences
5.1 DialectalBiases
barely contain any ONI mentions, e.g., Cheese is
made by french people who smell. We call this For our dialectal bias experiments, we first infer
test set ONI-Adv (for adversarial) since it chal- the dialect of a tweet as described in §2.2. Then,
lenges models with a reversal in the association analogoustothelexicalbiasevaluation,wequan-
betweentoxicityandoffensivenon-identitywords tifythedialectaldebiasingusingthePearson’scor-
(e.g.,“f*ck”,“sh*t”). relation between estimated probabilities of AAE
We report F for all models in Figure 2, which andtoxicity(R ),andthefalsepositiveratesof
1 AAE
shows how well a model identifies toxicity in of- models on AAE tweets (FPR ). See Appendix
AAE
fensive tweets that do not contain overtly lexical A.3 for hyperparameters and other experimental
cuesoftoxicity. Thedebiasedtrainingapproaches settings.
improveoverthebaselines;datafilteringmethods Results in Table 4 show that almost all data fil-
do not. One reason for this might be that data tering and debiasing methods reduce dialectal bi-
filtering methods were trained on much less data ases, with DataMaps-Easy as the exception (con-
3148
niart%33
Test
R ↓ F ↑ FPR ↓
AAE 1 AAE
Vanilla 0.4079 92.33 16.84
0.0 0
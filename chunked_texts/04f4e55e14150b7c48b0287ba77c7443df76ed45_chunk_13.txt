rexamples(q,s 1,s 2)
alsohighlyversatile.Onecantrytosubstituteitwithavari-
wherebymovingfroms 1 tos 2,orviceversa,requiresedit- etyofdifferenthouseholditems,suchas‘milk’or‘alcohol,’
ing a given word w.7 We show examples of words w that
often to disastrous effects. However, ‘spoons’ have fewer
occurfrequentlyinboththetrainingandvalidationsplitsof
challengingproperties.Aspooncannotgenerallybesubsti-
thedataset,whichallowsRoBERTatorefinerepresentations
tutedwithautensilthatissharporhasprongs,suchasafork,
oftheseconceptsduringtrainingandgivesusalargeenough
aknife,oratoothpick.RoBERTaobtainshighaccuracyon
samplesizetoreliablyestimatemodelperformance.
‘spoon’ examples, which suggests that it might understand
As shown, RoBERTa struggles to understand certain thissimpleaffordance,butdoesnotcapturethelongtailof
highly flexible relations. In particular, Figure 7 highlights affordancesassociatedwith‘water.’
thedifficultyofcorrectlyansweringquestionsthatdifferby
the words ‘before,’ ‘after’, ‘top‘, and ‘bottom’: RoBERTa
Qualitativeresults
performsnearlyatchancewhenencounteringthese.
Interestingly,theconceptsshowninFigure7suggestthat Our analysis thus far has been on simple-to-analyze single
RoBERTaalsostrugglestounderstandmanycommon,more word expressions, where we have shown that state-of-the-
versatile, physical concepts. Though there are 300 training art language models (such as RoBERTa) struggle at a nu-
examples wherein the solution choices s 1,s 2 differ by the anced understanding of key commonsense concepts, such
word ‘water.’ RoBERTa performs worse than average on as relations. To further probe the knowledge gap of these
thesereplacements.Ontheotherhand,RoBERTadoesmuch strongmodels,wepresentqualitativeexamplesinFigure9
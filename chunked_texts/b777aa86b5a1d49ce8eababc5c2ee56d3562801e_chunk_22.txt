ProcessingSystems,33:9782–9793.
Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.
Wavlm: Large-scaleself-supervisedpre-trainingfor ForrestNIandola,SongHan,MatthewWMoskewicz,
full stack speech processing. IEEE Journal of Se- Khalid Ashraf, William J Dally, and Kurt Keutzer.
lectedTopicsinSignalProcessing,16(6):1505–1518. 2016. Squeezenet: Alexnet-levelaccuracywith50x
fewer parameters and< 0.5 mb model size. arXiv
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan preprintarXiv:1602.07360.
Wang,MinjieWang,TianjunXiao,BingXu,Chiyuan
Zhang, and Zheng Zhang. 2015. Mxnet: A flexi- Forrest NIandola, AlbertE Shaw, RaviKrishna, and
ble and efficient machine learning library for het- Kurt W Keutzer. 2020. Squeezebert: What can
erogeneous distributed systems. arXiv preprint computervisionteachnlpaboutefficientneuralnet-
arXiv:1512.01274. works? arXivpreprintarXiv:2006.11316.
TianqiChen,ThierryMoreau,ZihengJiang,Lianmin PeterIzsak,MosheBerchansky,andOmerLevy.2021.
Zheng,EddieYan,HaichenShen,MeghanCowan, Howtotrainbertwithanacademicbudget. InPro-
Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. ceedingsofthe2021ConferenceonEmpiricalMeth-
{TVM}: An automated {End-to-End} optimizing odsinNaturalLanguageProcessing,pages10644–
compilerfordeeplearning. In13thUSENIXSympo- 10652.
siumonOperatingSystemsDesignandImplementa-
VijayJanapaReddi,DavidKanter,PeterMattson,Jared
tion(OSDI
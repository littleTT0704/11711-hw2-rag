ualcorporain25languages,byfine-tuningonparallel
corporainacross50languagesfordirectuseasatranslationengine.
• M2M100(Fanetal.,2021): Transformer-basedmultilingualmodelsformany-to-manytranslation.
We report on two sizes with 418M and 1.2B parameters respectively. The M2M100 model is
6CatwalkprovidesaunifiedinterfacetoabroadrangeofexistingNLPtasksandmodels.Alistoftasksthat
arecurrentlysupportedbyPentathloncanbefoundathttps://github.com/allenai/catwalk.
7Inthisscenariothemodelsaregrantedimmediateaccesstoallinstancesandcansortthembylength.If
theinstancesweredrawnfromthetestset,thiswouldresultintheartifactthatgroupsduplicatesofthesame
instanceinthesamebatch,whichweaimtoavoid.
6
trainedusingparallelcorpora(e.g.,WMTcorporadescribedabove)andminedbitexttoenable
translationbetweenanytwoof100languages.
• OPUS(Tiedemann&Thottingal,2020): abilingualTransformermodelwith74Mparametersfor
DE->ENtranslation. ThemodelistrainedonOPUSbitextcorpora(Tiedemann,2012).
• WMT19-Meta(Ngetal.,2019): aDE->ENTransformermodelwith314Mparameters.
ThissystemwontheWMT19taskonGermantoEnglishnewstranslation(Barraultetal.,2019).
• WMT21-Meta (Tran et al., 2021): a M2O Transformer model with 4.7B parameters. Unlike
WMT19-Meta,thismodelismultilingualandtrainedondatafromalllanguagesfortheWMT
2021sharedtask.Trainingdataisamixtureofparallelcorpora,monolingualcorporaandmined
bitext. ThismultilingualsystemrankedhighinseveralWMT21newstranslationtasks(Akhbardeh
etal.,2021). WerefertoTranetal.(2021)forcompletedetails.
WeevaluateusingPyTorchwithbothfullprecision(FP32)
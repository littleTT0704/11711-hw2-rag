. arXivpreprintarXiv:1909.08478. Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu,
Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang,
YueCao,XiaojunWan,JingeYao,andDianYu.2020. WentaoHan,MinlieHuang,etal.2021. Pre-trained
Multisumm: Towards a unified model for multi- models: Past,presentandfuture. AIOpen.
lingualabstractivesummarization. InAAAIConfer-
enceonArtificialIntelligence. TahmidHasan,AbhikBhattacharjee,Md.SaifulIslam,
Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,
Sumit Chopra, Michael Auli, and Alexander M. Rush.
M. Sohel Rahman, and Rifat Shahriyar. 2021. XL-
2016. Abstractive sentence summarization with at-
sum: Large-scale multilingual abstractive summa-
tentiverecurrentneuralnetworks. InProceedingsof
rizationfor44languages. InFindingsoftheAssoci-
the 2016 Conference of the North American Chap-
ation for Computational Linguistics: ACL-IJCNLP
teroftheAssociationforComputationalLinguistics:
2021, pages 4693–4703, Online. Association for
Human Language Technologies, pages 93–98, San
ComputationalLinguistics.
Diego, California. Association for Computational
Linguistics. JunxianHe,ChuntingZhou,XuezheMa,TaylorBerg-
Kirkpatrick, andGrahamNeubig.2021. Towardsa
AlexisConneau, KartikayKhandelwal, NamanGoyal,
unifiedviewofparameter-efficienttransferlearning.
Vishrav Chaudhary, Guillaume Wenzek, Francisco
arXivpreprintarXiv:2110.04366.
Guzma´n, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised Neil Houlsby, Andrei Giurgiu, Stanislaw
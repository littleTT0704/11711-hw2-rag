 performance. Our introduction of mathematical rea-
soning categories and the evaluation setup is inspired by task hierarchies in
other domains such as vision (Zamir et al., 2018) and NLP (Rogers et al., 2021)
whichappearinlargescalebenchmarks(e.g.,Srivastavaetal.,2022;Wangetal.,
2022).
3 L¯ila
L¯ila is composed of 23 tasks across 4 dimensions, curated from 44 sub-datasets
across 20 dataset sources. Here we discuss the construction and composition of
the benchmark and provide descriptive statistics of the datasets.
4
Category Tasks
Math ability Basic math, multiplication/division, number theory, algebra, ge-
ometry, counting and statistics, calculus, linear algebra, advanced
math
Language No language, simple language, complex language
Knowledge No background knowledge, commonsense, math, science, computer
science, real world knowledge
Format Fill-in-the-blank, generative question answering, multiple-choice,
natural language inference, reading comprehension
Table 1: Categories and their associated tasks.
3.1 Dataset Construction
Data Sources. L¯ila incorporates 20 existing datasets from the mathematical
reasoning literature (Table 19 gives a detailed list), where inputs are natural
language or templated text and outputs are numerical or expressions, e.g., we
exclude theorem proving (Welleck et al., 2021; Han et al., 2021), where the
output is not a number or expression. We leave the incorporation of formats
like theorem proving to future work.
Unified format. We normalize all datasets to a unified format with the
following fields:
1. The source dataset. Category tags for each of the four dimensions (math
ability, language complexity, format, and external knowledge; see §3.2).
2. The question, in English.
3. The answer to the question, as a string containing a number, expression, list,
or other data format. A set of Python strings that print the answer.
4. A task-level instruction in natural language.
We also retain meta-data from the original dataset.
Automatic program annotation. Most of the annotations in the source
datasets do not contain output in the form of a Python program. We auto-
matically annotate most datasets by generating Python programs using the
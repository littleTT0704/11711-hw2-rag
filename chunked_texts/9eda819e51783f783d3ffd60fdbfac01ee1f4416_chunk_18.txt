theharmofsharingallparameterstohigh-resource fixingthepre-trainedLM.Popularmethodsinclude
languagesduetothelimitmodelcapacity. adapter-tuning(Houlsbyetal.,2019;Bapnaetal.,
Besides,thetwowaystoaddadditionalparameters: 2019;Pfeifferetal.,2020a)prefix-tuning(Liand
private adapter, private prefix for each language Liang,2021)prompt-tuning(Lesteretal.,2021),
haveroughlythesameoverallperformanceonthe andothers(Maoetal.,2021;Huetal.,2021;Guo
wholedatasetandthesametrendlinesdepictedin etal.,2020). Amongtheseworks,acomprehensive
Fig.4,despitetheirdifferenceswehavediscussed discussion in the context of multilingual summa-
inSec.3.1. Thepossibleexplanationisthatthedis- rizationisrelativelymissing.
advantageofprefix-tuninglackingtheflexibilityto
modifyfreezeLMaddressedinSec.3.1,isallevi-
5 Discussion
atedorremovedbytuningsharedLM.Bothprefix
andadapterâ€™sadvantagecomesfromaddingprivate
parameters,sotheyhavesimilarbehavior. Inthispaper,weinvestigatetheapplicablescope
ofdifferentfamiliesoftuningstrategiesformulti-
4 RelatedWork linguallearning. Wespecificallyaskthreeresearch
questions, and by extensive experiments on sum-
4.1 MultilingualTasks
marization datasets with 45 languages we obtain
Withrapiddevelopmentofpre-trainedLMs,multi- diverseobservationswhich,hopefully,wouldpro-
lingualLMshaveemergedtoleveragethepower vide a useful instruction for future designing of
of pre-training on a large number of languages, multilingualtuningstrategies.
6 Limitations Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao
Jiang, and Graham Neubig. 2021. GSum: A gen-
One limitation of our work is that we only con- eralframeworkforguidedneuralabstractivesumma-
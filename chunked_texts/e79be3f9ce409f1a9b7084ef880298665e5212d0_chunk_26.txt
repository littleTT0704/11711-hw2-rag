omaingaptothepretrainingdatasets.Tocomparewith
alignment. It is aimed at addressing two existing issues
themethodsusingfeaturesextractedfromcollaborativeex-
in current contrastive learning pipelines: missing fine-
perts [14], we enrich our video representation by adding
grainedalignmentandinefficientsamplingformulti-modal
2D R-152 feature, which achieves better performance on
fusion. Without introducing any extra parameters, our
MSR-VTT,andbetterRecall@1andMedianRankonAc-
method achieved promising results on three text-video re-
tivityNet. Notethatthiscombinationhurtstheperformance
trievalbenchmarksundervariousevaluationprotocols. We
onYouCook2,andwewitnessedasimilartrendformodels
furtherdemonstratedthelearnedrepresentationscanbeef-
withoutpretraininginTable2. Finally,comparingwiththe
fectivelytransferredtoothertaskssuchasactionsteplocal-
results without pretraining in Table 5, 6 and 7, we clearly
ization and segmentation. Based on all these encouraging
find large-scale pretraining and finetuning brings substan-
results, we believe TACo is a good alternative to conven-
tialimprovementsconsistently.
tionalcontrastivelearningpipeline.
8
tohs-oreZ
denuteniF
References [13] FartashFaghri,DavidJFleet,JamieRyanKiros,andSanja
Fidler. Vse++:Improvingvisual-semanticembeddingswith
[1] Jean-BaptisteAlayrac,PiotrBojanowski,NishantAgrawal,
hardnegatives. arXivpreprintarXiv:1707.05612,2017. 2
JosefSivic, IvanLaptev, andSimonLacoste-Julien. Unsu-
[14] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia
pervisedlearningfromnarratedinstructionvideos. InPro-
Schmid. Multi-modaltransformerforvideoretrieval. InEu-
ceedings of the IEEE Conference on Computer Vision and

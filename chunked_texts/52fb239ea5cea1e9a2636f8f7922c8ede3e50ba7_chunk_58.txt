 the form of Python programs, thereby obtaining explainable
solutions in addition to the correct answer. We additionally introduce
two evaluation datasets to measure out-of-distribution performance and
robustness to language perturbation. Finally, we introduce Bha¯skara,
a general-purpose mathematical reasoning model trained on L¯ila. Im-
portantly, we find that multi-tasking leads to significant improvements
(average relative improvement of 21.83% F1 score vs. single-task models),
whilethebestperformingmodelonlyobtains60.40%,indicatingtheroom
for improvement in general mathematical reasoning and understanding.1
∗Equalfirstauthors.
†WorkdonewhileattheAllenInstituteforAI.
‡Correspondingauthors: matthewf@allenai.org,ashwinkv@allenai.org.
1Our dataset: https://github.com/allenai/Lila. Our model: https://huggingface.co/
allenai/bhaskara.
1
3202
raM
8
]LC.sc[
2v71571.0122:viXra
(cid:48)(cid:68)(cid:87)(cid:75)(cid:3)(cid:68)(cid:69)(cid:76)(cid:79)(cid:76)(cid:87)(cid:92)(cid:29)(cid:3)(cid:69)(cid:68)(cid:86)(cid:76)(cid:70)(cid:3)(cid:80)(cid:68)(cid:87)(cid:75)
(cid:47)(cid:68)(cid:81)(cid:74)(cid:88)(cid:68)(cid:74)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:91)(cid:76)(cid:87)(cid:92)(cid:29)(cid:3)(cid:86)(cid:76)(cid:80)(cid:83)(cid:79)(
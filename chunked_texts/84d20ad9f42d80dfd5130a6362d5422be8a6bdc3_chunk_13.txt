Akhbardehetal.,2021;Kocmietal.,2022). PentathlonalreadysupportsmanyotherMTand
textgenerationdatasets,andcanbeeasilyextendedtomore. WefocusonDE->ENtranslationhere;
additionalresultswithEN->DEareavailableintheAppendices.
Balancingtheinferencewallclocktimeandaccuratelymeasuringtheefficiency,weusedifferent
numbersofevaluatinginstancesacrossthefourscenarios. ForWMT14DE-EN:
• Fixedbatchingusesthefulltestsetof3,002instances. Italsomeasuresthetranslationquality
usingSacreBLEU(Post,2018).
• Poissonbatchingrandomlydraws4,000instances(withreplacement)fromthetestset.
• Inthesinglestreamscenario,1,000randomlyselectedtestinstancesareused.
• Differentlyfromothers,theofflinescenariorandomlyselects8,000instancesfromthetraining
data.7 Weensurethattheselectedinstanceshaveanaveragelengthmatchingthatofthetestset.
Controllingfortherandomseed,allmodelsareevaluatedonthesamesetofinstancesinthesame
order,andidenticalbatchsizesinthePoissonbatchingscenario. Preliminaryexperimentsindicate
thatthemodels’efficiencyperformanceremainsconsistentacrossmultipleruns. Assuch,weopt
outofconductingmultipleroundsofevaluation. AllmodelsareevaluatedononeRTX8000GPU,
andtheinferencebatchsizesforthefixedbatchingandofflinescenariosaretunedtotheallowable
maximumfortheavailableGPUhardware.
Models. Webenchmarkthefollowingpublicly-availablemodelscoveringawiderangeofsizes:
• MBART(Tangetal.,2021): a610M-parameter-sizedTransformermodelformultilingualtrans-
lation. Ithastwovariants,many-to-one(MBARTM2O)translatesotherlanguagesintoEnglish,
andmany-to-many(M2M)cantranslatebetweenmultiplelanguagepairs. WeusetheMBART50
variant,originallypre-trainedonmonoling
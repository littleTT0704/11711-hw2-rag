A Multi-dimensional Evaluation of
Tokenizer-free Multilingual Pretrained Models
JiminSun1,2 PatrickFernandes1 XinyiWang1 GrahamNeubig1
1LanguageTechnologiesInstitute,CarnegieMellonUniversity 2KakaoEnterprise
{jimins2,pfernand,xinyiw1,gneubig}@cs.cmu.edu
Abstract weuncovered;wehighlightsomechallengeswith
applyingthesemodelsandproposebestpractices
Recent work on tokenizer-free multilingual
forfutureresultsreportinginthisarea.
pretrained models show promising results in
Specifically,weperformexperimentsfine-tuning
improvingcross-lingualtransferandreducing
pretrained multilingual models, evaluating them
engineeringoverhead(Clarketal.,2022;Xue
et al., 2022). However, these works mainly with respect to (1) robustness to fine-tuning data
focus on reporting accuracy on a limited set settings,(2)dataefficiency,and(3)inferencetime
of tasks and data settings, placing less em- andmemoryconsumption. Basedonthesemultiple
phasis on other important factors when tun-
dimensions,wecometothesomewhatsurprising
inganddeployingthemodelsinpractice,such
conclusionthatsubword-basedmodelsmightstill
as memory usage, inference speed, and fine-
be the most practical choice in most settings, as
tuningdatarobustness. Weattempttofillthis
theyarecomparablyrobusttovariousfine-tuning
gapbyperformingacomprehensiveempirical
comparisonofmultilingualtokenizer-freeand datasettingswitharelativelylowinferencecost.
subword-based models considering these var-
ious dimensions. Surprisingly, we find that 2 Tokenizer-freeMultilingualModels
subword-basedmodelsmightstillbethemost
practical choice in many settings, achieving Whilemultilingualpretrainedmodels(Devlinetal.,
betterperformanceforlowerinferencelatency 2019; Lample and Conneau, 2019; Liu et al.,
andmemoryusage. Basedontheseresults,we 2020;Chietal.,2021;Xueetal.,2021)haveled
encouragefutureworkintokenizer-freemeth-
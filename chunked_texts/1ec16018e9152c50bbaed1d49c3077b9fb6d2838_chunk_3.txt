scommonlyusedinNLP,such step,theLMchoosesonewordoutofallvocabu-
laryitems,andseverallinguisticdistinctionscome
∗∗WorkdonewhileatCarnegieMellonUniversity.
1Codeanddemo:https://github.com/kayoyin/interpret-lm. intoplayforeachlanguagemodeldecision.
184
Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages184-198
December7-11,2022©2022AssociationforComputationalLinguistics
TobetterexplainLMdecisions,weproposeinter- (Wallaceetal.,2019). Despitetheimportanceof
pretingLMswithcontrastiveexplanations(Lipton, both language models and interpretability in the
1990). Contrastive explanations aim to identify NLPliterature,therelativepaucityofworkinthis
causalfactorsthatleadthemodeltoproduceone areamaybesomewhatsurprising,andwepositthat
outputinsteadofanotheroutput. Webelievethat this may be due to the large output space of lan-
contrastive explanations are especially useful to guagemodelsnecessitatingtheuseoftechniques
handlethecomplexityandthelargeoutputspace suchascontrastiveexplanations,whichwedetail
oflanguagemodeling. InTable1, thesecondex- furtherbelow.
planationsuggeststhattheinputword“dog”makes
“barking” more likely than a verb not typical for 2.2 ContrastiveExplanations
dogs such as “crying”, and the third explanation Contrastive explanations attempt to explain why
suggests that the input word “stop” increases the given an input x the model predicts a target y t
likelihoodof“barking”overaverbwithoutnega- instead of a foil y f. Relatedly, counterfactual
tiveconnotationssuchas“walking”. explanationsexplorehowtomodifytheinputxso
Inthispaper,wefirstextendthreeinterpretabil- thatthemodelmorelikelypredictsy insteadofy
f t
ity methods to compute contrastive
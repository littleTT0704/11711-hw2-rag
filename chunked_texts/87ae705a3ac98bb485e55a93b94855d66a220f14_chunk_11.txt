erscanbetrainedusingsen- Zettlemoyer2016),andAMRparsing(Pengetal.2017).The
tencesannotatedwiththisformalrepresentation(Zelleand linearization addresses data sparsity in two primary ways.
Mooney 1996; Zettlemoyer and Collins 2012; Wong and First,weusealinearizationschemesimilartoonethathas
Mooney2006;Kwiatkowskietal.2010;Krishnamurthyand beenshowntoimproveparseaccuracyforAMRparsingon
Mitchell2012),theyhavenotgenerallybeenuseddirectly similar size datasets (Peng et al. 2017). Secondly, we use
forspokenlanguage.Mostapplicationsofspokenlanguage a representation that aligns to the spoken language under-
understandingmapsutterancesontoafixeddomain,intent, standingproblem,aligningtheseslotsandintentswiththose
andslotstructure(Guptaetal.2006).Itcannotgenerallyrep- usedinAMRLparsing.Thisalignmentshouldimprovethe
resentcomplex,cross-domain,orcompositionalutterances. alignmentofembeddingsfromtheSLUdomain.
Multitasklearningindeepneuralnetworkshasbeenshown An example of the linearization scheme for AMRL can
tohelpgeneralization.SimilartoourworkbutwithCNNs beseeninFigure4.AMRLisarootedgraph.Startingatthe
(XuandSarikaya2013)jointlymodelsentenceclassification root,thepropertylinearizationrecursivelydescendstoaleaf,
andsequencelabeling.(Guoetal.2014)userecursiveneural appendingeachpropertyvisitedalongthewayuntilaleaf
networkstojointlyclassifyintentsandfillslots.(Miwaand node is reached. Incoming property arcs to a visited node
Bansal2016)achievedstateoftheartforentityandrelation areinvertedtoavoidcyclesandhandlemulti-headedgraphs.
classification. (Zhang and Weiss 2016) used them for part Typesoftheleafnodesaretheonlyonesthatappearinthe
ofspeechtagginganddependencyparsing.Transferringof linearization.Whenaleafnodeisvisited,the
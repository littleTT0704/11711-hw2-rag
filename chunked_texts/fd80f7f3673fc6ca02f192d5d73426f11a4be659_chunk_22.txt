701 0.539 0.409
in-context examples sets. We can see that output Prompting
distribution isheavilybiased bythe scoresin the BISON ✓ 92.2% 0.605 0.540 0.462 0.339
UNICORN ✓ 87.4% 0.609 0.621 0.495 0.384
in-contextexamples: despitenever predicting79 BISON ✗ 89.8% 0.567 0.478 0.381 0.313
in the zero-shot setting, when a single example UNICORN ✗ 84.4% 0.536 0.523 0.433 0.334
with that score is included, it starts to dominate
the model predictions. This seems to hint that
Table4: Meta-evaluationresultsforsystem-levelaccu-
racyandsegment-levelPearsononthelow-resourcelan-
LLMs“overfit”tothespecificscoresprovidedas
guages,usingPaLM-2forscoreprediction. ⋆Notethat
examples,ratherthangeneralizingtothebroader
thebaselineisslightlydifferentfromthehigh-resource
evaluationtask,whichcouldexplainthelackluster
case,beingtrainedonthesamedatabutwithoutthese
performanceofin-contextlearning. low-resourcelanguagepairs.
6.3 LowResourceLanguages
k). Contrarytotheperformancewithscorepredic-
Table 4 shows the performance of PaLM-2 mod- tion, wefindthatperformancewith AUTOMQM
els at score prediction for low-resource transla- seems to (mostly) scale with the number of in-
tion. Overall,wefindthatsimilartohigh-resource contextexamples: performanceincreasesmonoton-
LPs, these models are good zero-shot evaluators, icallywithupto4in-contextexamplesandplateaus
with system-level accuracies around 90%. How- thereafter. Additionally,thevarianceacrossthein-
ever,zero-shot LLMsunderperformlearned met- contextlearningsetsseemstobelower,withmost
rics,evenwhenthesemetricsalsoweren’texposed examplesetsexhibitinglessthan0.05Pearsond
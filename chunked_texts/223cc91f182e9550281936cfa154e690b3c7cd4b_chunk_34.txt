 (RL) (Sutton &
Barto, 2018). Note that here we are primarily interested in learning the conditional model (policy) p (y∣x).
θ
Yet we can still define the joint distribution as p (x,y) = p (y∣x)p (x).
θ θ 0
4.3.1. Expected Future Reward
The first simple way to use the reward signals as the experience is by defining the experience function as the
logarithm of the expected future reward:
f :=fθ (x,y) =logQθ(x,y), (4.16)
reward,1
which leads to the classical policy gradient algorithm (Sutton et al., 2000).
25
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
Policy gradient. With α = β = 1, we arrive at policy gradient. To see this, consider the teacher-student
optimization procedure in Equation 3.3, where the teacher-step yields the q solution:
q(n)(x,y) =p (x,y)Qθ(n) (x,y) / Z, (4.17)
θ(n)
and the student-step updates θ with the gradient at θ = θ(n) :
Eq(n)(x,y)[∇
θ
logp θ(x,y)]+Eq(n)(x,y)[∇ θf rθ eward,1(x,y)] ∣∣
θ=θ(n)
=1/Z ⋅ p (x)∇ p (y∣x)Qθ (x,y) ∣
∑ 0 θ ∑ θ ∣θ=θ(n)
(4.18)
x y
=1/Z ⋅ μθ (x) Qθ (x,y)∇ p (y∣x) ∣.
∑ ∑
θ θ ∣θ=θ(n)
x y
Here the first equation is due to the log-derivative trick g∇logg = ∇g; and the second equation is
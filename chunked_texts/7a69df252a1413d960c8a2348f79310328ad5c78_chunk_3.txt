O [21] datasets to conduct the experimental analy-
sis. Our experiments show that our network generates images of similar quality
as previous work while either advancing or competing with the state of the art
on the Inception Score (IS) [34] and the Fr´echet Inception Distance (FID) [10].
The main contributions of this paper are threefold:
(1)Weincorporatemultipleattentionmodels,therebyreactingtosubtledif-
ferences in the textual input with fine-grained word attention; modelling long-
range dependencies with local self-attention; and capturing non-linear interac-
tion among channels with squeeze-and-excitation (SE) attention. SE attention
can learnto learnto use globalinformation toselectivelyemphasise informative
features and suppress less useful ones.
Title Suppressed Due to Excessive Length 3
(2)Westabilisethetrainingwithspectralnormalisation[23],whichrestricts
the function space from which the discriminators are selected by bounding the
Lipschitz norm and setting the spectral norm to a designated value.
(3) We demonstrate that improvements on single evaluation metrics have to
be viewed carefully by showing that evaluation metrics may react oppositely.
The rest of the paper is organized as follows: In Section 2, we give a brief
overview of the literature. In Section 3, we explain the presented approach in
detail.InSection4,wementiontheemployeddatasetsandexperimentalresults.
Then, we discuss the outcomes and we conclude the paper in Section 5.
2 Related Work
While there has been substantial work for years in the field of image-to-text
translation,suchasimagecaptiongeneration[1][6][41],onlyrecentlytheinverse
problem came into focus: text-to-image generation. Generative image models
require a deep understanding of spatial, visual, and semantic world knowledge.
A majority of recent approaches are based on GANs [7].
Reed et al. [31] use a GAN with a direct text-to-image approach and have
shown to generate images highly related to the text’s meaning. Reed et al. [30]
further developed this approach by conditioning the GAN additionally on ob-
ject locations. Zhang et al. built on Reed et al.�
MRR@100), which increases from 0.119 to 0.151
when expanding Wikipedia, and from 0.035 to 0.071 when applying our method to
Wiktionary. More effective rankings can improve answer selection performance, and
thus end-to-end QA accuracy, if the search rank is used as a feature. In addition, if
relevant search results are ranked higher, shorter hit lists can be used with little loss
in recall to reduce the number of incorrect candidate answers and the computational
costs for scoring the candidates.
Table 6.12 shows OpenEphyra’s search performance on TREC 8–15 if varying
numbers of Wikipedia seed articles are expanded. Similarly to the results for Watson,
the 100,000 most popular seed documents yield the largest gains and the final 100,000
seeds have little impact on search results. This applies to search recall as well as the
number of relevant results, and to both passage and title searches. Thus our method
for popularity-based seed selection appears to be equally effective for OpenEphyra.
For about 43% of the factoid questions in TREC 8–15 OpenEphyra generates
queriesthatonlyconsistofindividualkeywords, whereas57%ofthequestionscontain
compound nouns and named entities that are used as phrases in the Indri queries.
6.3. SEARCH EXPERIMENTS 93
llaceR hcraeS
llaceR hcraeS
yranoitkiW
aidepikiW
56.0
58.0
%3.2+
%5.07+
)1000.<p(
%9.1+
08.0
06.0
)1000.<p(
%4.87+
57.0
55.0
%0.3+
segassaP
%0.68+
)1000.<p(
07.0
05.0
56.0
%4.78+
%1.4+
54.0
)1000.<p(
06.0
04.0
%3.4+
%2.2+
55.0
sseeggaassssaaPP
%3.88+
)1000.<p(
)9100.=p(
%0.4+
53.0

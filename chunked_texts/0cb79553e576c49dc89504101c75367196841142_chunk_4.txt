 Ourworkshowstheeffec-
expertusing1Mnon-toxicand100Kovertlytoxic
tivenessofcombiningcontrollablegenerationwith
commentsfromtheJigsawcorpus(Do,2019),as
textrewritingmethodsfortextdetoxification.1
done in Liu et al. (2021) and Dale et al. (2021).
BARTcaninfillmultipleornotokensevenifonly
2 Background: TextDetoxification
one token is masked, allowing for more flexible
maskinfilling. SeeAppendixAfortrainingdetails.
Text detoxification is a form of stylistic rewrit-
ing (Hu et al., 2017; Shen et al., 2017; Jhamtani
3.1 ContextualMasking
etal.,2017)withthegoalofproducinganon-toxic
Wefirstidentifylocationsthatcould conveytoxic
rewrite given a toxic input sentence. This task is
meaning; intuitively, these could be words or
challenging,asitrequiresbothdetoxificationand
phrases with strongly differing likelihoods under
preservationofnon-toxicmeaning, incontrastto
theexpertandanti-expert.
controllabletextgeneration,whichaimstosimply
Formally,givenasequencew,foreverytoken
generateanynon-toxiccontinuationforaprompt
w ∈ w,wetemporarilymaskitandgenerateprob-
(Prabhumoyeetal.,2020;Gehmanetal.,2020). i
abilitydistributionsoverthevocabularyV forthat
Duetoalackofsupervisionwithparalleldata,
location from G+ and G−, which we denote P+
an often effective approach to stylistic rewriting
and P− respectively. Then, we compute the dis-
reliesonunsupervisedmasking-and-reconstructing
tance d between P+ and P− using the Jensen-
approaches(Lietal.,2018;Wuetal.,2019;Malmi i
Shannondivergence,asymmetricformoftheKull-
et al., 2020; Ma et al., 2020). In this paradigm,
back–Leibler(KL)divergence:2
sourcestyle-specifict
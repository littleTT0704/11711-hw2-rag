 Right:
Directed/undirected probabilistic graphical models of various dependence structures.
The θ to be learned includes the neural network weights. It can be randomly initialized and then trained from
scratch. On the other hand, with the prevalence of pretrained models, such as BERT (Devlin et al., 2019) for
text representation and GPT-3 (Brown et al., 2020) for language modeling, we can often initialize θ with the
appropriate pretrained model weights and finetune it to the downstream tasks of interest. Sometimes it is
sufficient to finetune only a subset of the model parameters (e.g., the classification head of the image classifier)
while keeping all other parts fixed. In many downstream tasks, it is often difficult to obtain a large number of
supervised data instances to train/finetune θ with simple supervised MLE. In such cases, SE offers a more
flexible framework that allows plugging in all other forms of experience (as those in Section 4) related to the
downstream tasks for more effective learning.
Prompts for pretrained models. Updating large pretrained models can be prohibitively expensive due to the
massive amount of model parameters. Prompting is a new emerging way of steering the pretrained models for
performing downstream tasks without changing the model parameters (Brown et al., 2020). A prompt is a short
sequence of text tokens or embedding vectors which, by concatenating with the input and being fed together
into the model, stimulates the model to perform the task of interest and produce the desired output for the input
(Figure 6, left). It is thus desirable to learn the prompt that enables optimal performance of the pretrained
models on the task of interest. For continuous prompt, which is a sequence of differentiable embedding vectors
41
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
(Lester et al., 2021; Li & Liang, 2021), we could naturally treat the prompt as the learnable θ, and plug the
resulting target model p, now the pretrained model plus the learnable prompt, into the SE for training. On the
θ
other hand, for discrete prompt, which is a sequence of text tokens, one can instead parameterize a prompt-
generation network as θ which, after training, generates optimal discrete prompts for the downstream
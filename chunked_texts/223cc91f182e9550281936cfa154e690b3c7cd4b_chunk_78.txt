. Xing & T.
Jebara (Eds.), Proceedings of the 31st International Conference on International Conference on Machine
Learning - Volume 32 (pp. 1791–1799). https://www.cs.toronto.edu/~amnih/papers/nvil.pdf
Mohamed, S., & Lakshminarayanan, B. (2016). Learning in implicit generative models. arXiv.
https://doi.org/10.48550/arXiv.1610.03483
Neal, R. M. (1992). Bayesian training of backpropagation networks by the hybrid Monte Carlo method (tech.
rep.). University of Toronto. https://www.cs.toronto.edu/~radford/ftp/bbp.pdf
Neal, R. M., & Hinton, G. E. (1998). A view of the EM algorithm that justifies incremental, sparse, and other
variants. In Learning in graphical models (pp. 355–368). Springer. https://doi.org/10.1007/978-94-011-5014-
9_12
Norouzi, M., Bengio, S., Chen, z., Jaitly, N., Schuster, M., Wu, Y., Schuurmans, D. (2016). Reward augmented
maximum likelihood for neural structured prediction. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, & R.
Garnett (Eds.), Proceedings of the 30th International Conference on Neural Information Processing Systems
56
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
(Vol. 29; 1723–1731. Curran Associates Inc.
https://papers.nips.cc/paper/2016/file/2f885d0fbe2e131bfc9d98363e55d1d4-Paper.pdf
Nowozin, S., Cseke, B., & Tomioka, R. (2016). f -GAN: Training generative neural samplers using variational
divergence minimization. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, & R. Garnett (Eds.), Proceedings of
the 30th International Conference on Neural Information Processing Systems (Vol., 29
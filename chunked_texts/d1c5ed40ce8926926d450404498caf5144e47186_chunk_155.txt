Section 2.2), and we have shown that there is little
additional benefit in expanding more than 200,000 Wikipedia seeds using content
from web search results (Sections 6.3.2 and 6.3.4). In the last three rows, we report
Watsonâ€™s search recall when using the collection of all manually acquired sources
without expansion, when adding expansions of Wikipedia, Wiktionary, World Book
and Microsoft Encarta generated from web search results, and when also including
an expansion of Wikipedia generated from the local web crawl. The extraction-based
approach yields small but consistent recall gains on top of all other sources even
though it was only applied to Wikipedia seeds.
In Table 7.3 we show the improvements in search recall when increasing numbers
of Wikipedia seed articles are expanded with content extracted from the web crawl.
Recall increases almost monotonically, and the configuration with 500,000 expanded
documents is most effective in all experiments except when combining passages and
document titles for regular Jeopardy! questions. The largest improvements are real-
ized on Final Jeopardy! data, and there may be headroom for further gains on this
dataset by expanding additional Wikipedia seeds using content from ClueWeb09.
This is because Final Jeopardy! questions ask about less common topics or obscure
facts about popular topics that may not yet be covered sufficiently in the seed corpus
or previously expanded documents.
Table 7.4 shows the candidate recall of Watson when using the original document
collections and expanded corpora generated with one or both of the SE techniques.
Again our approach consistently increases recall independently of the source of re-
lated content. It can also be seen that both expansion methods have low variance,
i.e. they improve candidate recall far more often than they hurt recall. For instance,
when expanding all manually acquired sources using web search results and locally
extracted content, correct candidate answers can be found by Watson for an addi-
tional 240 regular Jeopardy! questions, but only 63 out of 3,508 questions are hurt.
We confirmed that the average number of candidates in the experiments that include
content from ClueWeb09 is roughly the same as the numbers given in Table 6.15
for search-based SE and the original sources without expansion. The reported recall
numbers are therefore directly comparable.
7.1. EXTRACTION-BASED SOURCE
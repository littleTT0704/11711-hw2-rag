y sk) in the parallel fashion. Particularly,
let m be the binary masked matrix of y, where the val-
E log(cid:32) p′ s(ys\k|y sk)(cid:33) +E log(cid:32) p′ s(y t\k|y tk)(cid:33) uesofoneandzeroindicateagivenpixels (unmaskedpixel)
xs∼ps(xs) ps(ys\k|y sk) xt∼pt(xt) ps(y t\k|y tk) and an unknown pixel (masked pixel), respectively. Then,
≤−(cid:104) E xs∼ps(xs)logps(ys\k|y sk)+E xt∼pt(xt)logps(y t\k|y tk)(cid:105) the conditional structure p s(y s\k|y sk) can be rewritten as
p (y ⊙(1−m)|y ⊙m), where ⊙ is the element-wise
(9) s s s
productandthemaskmcontainsonlyoneunmaskedpixel,
With any form of ideal distribution p′ s(·), Eqn. (9) always i.e.,thegivenkthpixel(mk =1). Learningtheconditional
holdduetologp′(·) ≤ 0. Hence,optimizingEqn. (9)also
s structureconstraintviabinarymaskmcanbeformedas:
ensuretheconditionalstructuralconstraintinEqn. (8)im-
posed due to the upper bound of Eqn. (9). Therefore, the argm ΘinE ys∈Ys,m∈M−logps(ys⊙(1−m)|ys⊙m) (11)
where M is the set of possible binary masks. Through
Eqn. (11), modeling the conditional structural constraint
p (y\k|yk) can be equivalently interpreted as learning the
s s s
condition of masked pixels on the given unmask pixel. To
increasethemodelingcapabilityoftheconditionalstructure
network, three different strategies of the binary mask are
adopted
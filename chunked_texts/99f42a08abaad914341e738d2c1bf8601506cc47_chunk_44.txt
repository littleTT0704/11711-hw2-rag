 The approximate range of s can follow
the common setting in many existing approaches [2], [5].
Second,wefindtheoptimalmarginparametermforeachs WegivetheAUC-0.0005inTable5.SphereFaceachieves
62.4% with SFN and 61.37% with HFN. SphereFace-R
withauniformsearch.IthasbeenshowninTable4thatthe
range of m depends on the specific s we use. For larger s, v1 achieves 61.37% with SFN and 60.45% with HFN.
itsoptimalmistypicallylargeraswell.Asausefulpractice, SphereFace-Rv2achieves61.21%withSFNand62.72%with
weshouldgraduallysearchlargerm,assincreases.Finally, HFN.OurexperimentsshowthatthatSFNgenerallyyields
we choose the scale s and the margin m that lead to the comparableorevenbetterresultsthanHFNwithaproperly
chosentwhenourmodelsaretrainedonVGGFace2.
best performance on the validation set. We find that such
a simple hyperparameter searching is generally useful and
6.2.4 CharacteristicGradientDetachment
can be applied to tuning different kinds of hyperspherical
FRmethodsinpractice. InTable6,wecomparethemodelstrainedwithorwithout
CGD. Except the usage of CGD, the experiments are per-
6.2.3 SoftFeatureNormalization formed with exactly the same experimental settings (e.g.,
SFN is a soft regularization to constrain the feature norm dataset, architecture, training setup etc). We have evalu-
and can be considered as a trade-off between NFN and ated CGD on all three FN strategies (i.e., NFN, HFN and
HFN. SFN has a weighting hyperparameter t controlling SFN). The results show that CGD significantly improves
the contribution of feature norm regularization term. Since the results in all scenarios, which validates the importance
therearethreehyper-parameters,i.e.,m,s,andt,itistime- of simplifying the gradient of the characteristic function.
consuming and infeasible to enumerate all possible combi- CGD works particularly well
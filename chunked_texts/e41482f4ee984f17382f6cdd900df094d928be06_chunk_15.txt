c”)
must_include(body,“acarinNYC”)
Table1: Weintroducetwoevaluationapproaches. r (top)measuresthecorrectnessofperforming
info
information-seeking tasks. It compares the predicted answer aˆ with the annotated reference a∗
withthreeimplementations. r (bottom)programmaticallycheckswhethertheintermediatestates
prog
duringtheexecutionspossesstheanticipatedpropertiesspecifiedbytheintent.
thepostandifthepostcontainstherequestedcontentbyexaminingthepostcontent. Wereusethe
exact_matchandmust_includefunctionsfrominformation-seekingtasksforthispurpose.
Unachievable Tasks Due to constraints such as inadequate evidence, user permissions (§A.3),
ortheabsenceofnecessaryfunctionalsupportonthewebsite,humansmayaskfortasksthatare
notpossibletocomplete. Inspiredbypreviousworkonevaluatingquestion-answeringmodelson
unanswerablequestions(Rajpurkaretal.,2018),wedesignunachievabletasksinWebArena. For
instance, fulfilling an intent like “Tell me the contact number of OneStopShop” is impracticable
in WebArena, given that the website does not provide such contact information. We label such
instancesas"N/A"andexpectanagenttoproduceanequivalentresponse. Theseexamplesallowus
toassessanagent’sabilitytoavoidmakingunfoundedclaimsanditsadherencetofactualaccuracy.
AnnotationProcess Theintentswerecontributedbytheauthorsfollowingtheannotationguideline
in §3.1. Every author has extensive experience with web-based tasks. The reference answers to
the information-seeking tasks were curated by the authors and an external annotator. To ensure
consistencyandaccuracy, eachquestionwasannotatedtwice. Ifthetwoannotatorsdisagreed, a
third annotator finalized the annotation. The programs to evaluate the remaining examples were
contributedbythreeoftheauthorswhoareproficientinJavaScriptprogramming. Difficulttasks
wereoftendiscussedcollectivelytoensurethecorrectnessoftheannotation. Theannotation
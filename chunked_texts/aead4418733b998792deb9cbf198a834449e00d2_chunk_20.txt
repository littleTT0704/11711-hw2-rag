-based attack to
abilitytoanincorrectsolutionatthetop,andverylowprob-
swaptokens.Wefaceanon-differentiablecostandgenerate
abilitytocorrectsolutions.
largecollectionsoffailureswithawideclassofmutations.
When a correct solution is not within the top 500 candi-
dates,themodelisagainconfidentlyincorrect,withthetop 8 Conclusion
candidatey(1) receiving≈0.94probability.Improvingthe
beam Westudygeneralizationinsymbolicmathematicsusingthe
searchalgorithm–e.g.byfurtherincreasingthesearchbud-
predominant modeling paradigm: a large-scale transformer
getorusinganalternativetobeamsearch–wouldinevitably
trainedwithmaximumlikelihood.Wefinddeficienciesthat
return a low probability solution, as the 500 candidates al-
are not captured by test accuracy, including brittleness to
ready cover more than 99.4% of the probability mass. The
small perturbations, difficulty composing known solutions,
findingsagainpointtomodelerrors.
and gaps in the training distribution. We offer speculations
In-distribution. Next, we study in-distribution problems based on our results. Due to the large space of equations,
fromtheFWDvalidationsetofLampleandCharton(2019). practicalempiricaldistributionsdonotprovideadensesam-
On failure cases, we test if the ground-truth y
∗
is scored pling of individual problem types (e.g. k 1cos(k 2x)), and
above the top k beam candidates, meaning that failure@k each empirical sample contains shared biases from the un-
mightberesolvedwithperfectsearch–y wasscoredmore derlying data generator (e.g. integer values, lengths). Thus,
∗
highlybutwassimplynotfound.Asseenin12,themajority sparse test sets do not adequately measure systematic gen-
offailures–91.6%forfailure@1and65.2%forfailure@10 eralization. From a learning perspective, generic networks
–wouldremainunresolvedwithperfectsearch. trained with SGD do not necessarily favor the simplest hy-
pothesistoexpl
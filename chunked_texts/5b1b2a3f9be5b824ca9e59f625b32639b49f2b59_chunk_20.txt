in-B 9.0 0.28 44.9 66.2 48.1 44.3 48.3
Ours Online Swin-L 5.3 0.36 47.6 70.3 50.8 46.1 52.3
TABLEI
COMPARISONTOTHESTATE-OF-THE-ARTOFFLINEANDONLINEVISMETHODSONTHEYOUTUBE-VIS-2019VALIDATIONSET.THEINFERENCESPEED
ISMEASUREDONASINGENVIDIAV100GPUWITHbatchsize=1.∗INDICATESTHATVALUESAREIMPORTEDFROMAPREPRINT.
3) Audio Context Fusion the M ∈ RN×H×W by a threshold of 0.5. Then we filter
t
We correspond the audio features to pixel-wise visual out slots with a class probability less than 0.4 and keep
feature maps with cross-modal attention in the transformer the remaining ones as the predictions at time t. Dataset.
encoder. Similar to tokens from visual features, we create We evaluate our method on the two extensively used VIS
audio tokens to support subsequent context fusion. datasets, Youtube-VIS-2019 and Youtube-VIS-2021, as well
Audio token. We combine the overall audio features as the as our newly constructed AVIS dataset.
reference audio context f taud =a t−δ⊕···⊕a t ∈RCa×(1+δ), • Youtube-VIS-2019has40categories,4,883uniquevideo
whereC a =4096.Atwo-layerbi-directionalLongShortTime instances.Thereare2,238trainingvideos,302validation
Memory (bi-LSTM) [21] network with hidden size 512 is videos, and 343 test videos in it.
leveraged to aggregate the temporal information. After that, • Youtube-VIS-2021 is an improved version of Youtube-
we project the audio features f taud to a lower dimension C(cid:48) VIS-2019, which contains 8,171 unique video instances.
with two fully-connected layers and ReLU activation. Thus, There are 2,985 training videos, 421 validation videos,
we get the audio token O aud ∈RC(cid:48)×(1+δ). and 453
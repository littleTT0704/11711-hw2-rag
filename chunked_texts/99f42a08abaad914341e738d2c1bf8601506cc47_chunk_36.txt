. The training images are horizontally flipped for
gradient weighting strategy. This may open a brand new
dataaugmentation.Wetrainallthemodelsontwopopular
gatetogaindeeperunderstandingstowardshyperspherical
training dataset: VGGFace2 [76] (3.1M images from 8.6K
FR. Moreover, we believe that our novel loss characteriza-
IDs) and MS-Celeb-1M (5.8M images from 86K IDs, also
tionreformulationinEq.(19)andEq.(20)mayinspiremore
calledMS1M-V2,thecleanedversionofMS-Celeb-1Mused
effectivedesignsforlossfunctionsinhypersphericalFR.
in [5]). Detailed statistics of these datasets are given in
Table 2. In our experiments, all the models are optimized
6 EXPERIMENTS AND RESULTS using stochastic gradient descent with momentum 0.9. For
In this section, we present comprehensive experiments to VGGFace2, we train on 2 GPUs for 80k iterations, with a
explorethepropertiesoftheSphereFacefamily.Experimen- learning rate of 0.1 which is decreased by 10 at the 40k
tal setup is introduced in Section 6.1. We perform ablation and 60k iteration. For MS-Celeb-1M, we train on 4 GPUs
studies in Section 6.2 and Section 6.3 to investigate differ- for240kiterations.Thelearningrateisinitializedas0.1and
ent variants of SphereFace and their hyperparameters. In decreasedby10attheiterationof100k,180k,and220k.
Section 6.4, we evaluate our methods on the large-scale Testing.Westrictlyfollowthespecificprotocolprovidedin
benchmarksandcomparetoohterstate-of-artmethods. each dataset for evaluation. Table 2 shows the statistics of
the testing sets. Given a face image, we extract two 512-
6.1 ImplementationDetails dimensional embeddings from the original image and its
horizontally flipped version, respectively. The final embed-
Preprocessing.Eachfaceimageiscroppedbasedonthefive
dingisobtainedbyaveragingthetwo.Thescoringmethod
facelandmarks(i.e
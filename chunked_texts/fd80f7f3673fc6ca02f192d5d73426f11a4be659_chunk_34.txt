irè,NiccolòCampolungo,andRobertoNav-
Guu,AdamsWeiYu,BrianLester,NanDu,An-
igli.2022. MaTESe: Machinetranslationeval-
drewMDai,andQuocVLe.2021. Finetuned
uation as a sequence tagging problem. In Pro-
language models are zero-shot learners. In In-
ceedingsoftheSeventhConferenceonMachine
ternationalConferenceonLearningRepresenta-
Translation(WMT),pages569–577,AbuDhabi,
tions.
UnitedArabEmirates(Hybrid).Associationfor
ComputationalLinguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, brian ichter, Fei Xia, Ed H.
Yiwei Qin, Weizhe Yuan, Graham Neubig, and
Chi,QuocVLe,andDennyZhou.2022. Chain
PengfeiLiu.2022. T5score: Discriminativefine-
ofthoughtpromptingelicitsreasoninginlarge
tuningofgenerativeevaluationmetrics. ArXiv,
languagemodels. InAdvancesinNeuralInfor-
abs/2212.05726.
mationProcessingSystems.
Colin Raffel, Noam Shazeer, Adam Roberts,
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri,
KatherineLee,SharanNarang,MichaelMatena,
Alane Suhr, Prithviraj Ammanabrolu, Noah A.
YanqiZhou,WeiLi,andPeterJ.Liu.2020. Ex-
Smith,MariOstendorf,andHannanehHajishirzi.
ploringthelimitsoftransferlearningwithauni-
2023. Fine-grainedhumanfeedbackgivesbetter
fied text-to-text transformer. J. Mach. Learn.
rewardsforlanguagemodeltraining.
Res.,21(1).
WendaXu,DanqingWang,LiangmingPan,Zhen-
Ricardo Rei, José G.
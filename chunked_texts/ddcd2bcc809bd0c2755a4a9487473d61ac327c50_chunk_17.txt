ubsetoffalsebelief
samples(theoriginalexamplesfromToM-k),whilefailingonadversarialsamplesthatinvolvetransparentaccessor
relationshipchange(in→on).
Figure3: PerformanceofarangeofGPTmodelsonvariouscategorieswithintheunexpectedcontentsegmentof
Adv-CSFB.Theresultsaretheaverageaccuracyofquestion2(e.g. Hebelievesthatitisfullof_)andquestion3
(e.g. Hecallshisfriendtotellthemthathehasjustfoundabottlefullof_),whichspecificallyfocusonanagent’s
beliefsratherthanobjectivetruth.
4 achieves 97.5% and 83.3% on the two cate- thelabel),anduninformativelabel(i.e.,theprotag-
goriesrespectively. Nevertheless,therehasbeena onistcan’treadthelabel).
gradualdeclineintheperformanceofsubsequent We regenerated the responses multiple times,
models onother categories, suchas other person consistently obtaining similar results, so we can
(from 93.8% by davinci-002 to 68.8% by GPT- concludethatthemodelsexhibitconfidenceintheir
4),in→on(from71.4%bydavinci-002to0%by predictions,eveniftheyareincorrect. Itisimpor-
GPT-4), and transparent access (from 66.7% by tanttonote,however,thattheresultsobtainedfrom
davinci-002to0%byGPT-4). LM-probingmayslightlydifferfromMC-probing.
In MC-probing, even with our 1-shot setup, the
Figure3showcasestheperformanceoftheGPT
modelmayproduceresponsesthatarenotapplica-
familyonvariouscategorieswithintheunexpected
ble, such as “none of the above” or “both”. This
contentssegment. Itbecomesapparentthat, akin
is particularly noticeable in verbose models like
to the unexpected transfer segment, newer mod-
GPT-3.5-Turbo and GPT-4. These models tend
elssuch
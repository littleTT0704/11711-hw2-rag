babledialect.
hateful towards minorities (Dixon et al., 2018).
2.3 DatasetforToxicLanguageDetection
We detect these identity mentions in text using a
listof26regularexpressions. We focus our analyses on a widely used hate
speech dataset of English tweets (Founta et al.,
Possibly offensive minority identity mentions
2018). The tweets were collected using a multi-
(OI) are mentions of minoritized identities that
round bootstrapping procedure, and were labeled
coulddenoteprofanityorhatedependingonprag- out of context8 for toxic language. We focus on
maticandcontextualinterpretations. Thisincludes
the 86k tweets that are annotated as hateful, abu-
slurs and objectifying outdated terms to refer to
sive,orneitheranddiscardthoselabelledasspam.
minority groups, which are usually understood as
Weaggregatetheabusiveandhatefullabelsintoa
attacks. Additionally,thisincludesreclaimedslurs
single toxic category, yielding 32k toxic and 54k
(queer, n*gga), which connote less offensive in- non-toxictweets.9
tentwhenspokenbyin-groupmemberscompared
toout-groupmembers(Croom,2013). 3 DebiasingMethods
Possiblyoffensive non-identitymentions (ONI) Weconsidertwotypesofdebiasingmethodsfrom
containsswearwordsandotherprofanities,which currentliterature. Thefirsttypeaddressesknown,
areusuallyoffensivebutnotassociatedtoanyso- pre-defined biases—such as lexical and dialectal
cial groups (e.g., f*ck, sh*t). Note that the prag- biases for hate speech detection, via a model-
matic interpretation of these words is not neces- based approach involving additional training ob-
sarily always toxic or offensive (Dynel, 2012), as jectives (§3.1). In contrast, the second type is ag-
they are often used to convey closeness between nostic to prior knowledge about biases, and in-
the speaker and listener or emphasize the emo- stead filters out examples that appear “too easy”
tionality of a statement (e.g., second example
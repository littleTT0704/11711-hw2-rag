ldecision.
municate why a model made a certain prediction.
Particularly, we focus on methods that compute
3 ContrastiveExplanationsfor
saliencyscoresS(x i)overinputfeaturesx
i
tore-
LanguageModels
veal which input tokens are most relevant for a
prediction: thehigherthesaliencyscore,themore In this section, we describe how we extend three
x supposedlycontributedtothemodeloutput. existinginputsaliencymethodstothecontrastive
i
Despite a large body of literature examining setting. Thesemethodscanalsobeeasilyadapted
input feature explanations for NLP models on to tasks beyond language modeling, such as ma-
tasks such as text classification (for a complete chinetranslation(AppendixA).
review see Belinkov and Glass (2019); Madsen
3.1 GradientNorm
et al. (2021)), or interpreting how language mod-
elsuselinguisticfeaturessuchassyntax(Ravfogel Simonyanetal.(2013);Lietal.(2016a)calculate
et al., 2021; Finlayson et al., 2021), few works saliencyscoresbasedonthenormofthegradientof
attempttoexplainlanguagemodelingpredictions themodelprediction,suchastheoutputlogit,with
185
respecttotheinput. ApplyingthismethodtoLMs 4 DoContrastiveExplanationsIdentify
entailsfirstcalculatingthegradientasfollows: LinguisticallyAppropriateEvidence?
g(x ) = q(y x)
i âˆ‡xi t
|
First,weaskwhethercontrastiveexplanationsare
where x is the input sequence embedding, y is quantifiably better than non-contrastive explana-
t
thenexttokenintheinputsequence,q(y x)isthe tionsinidentifyingevidencethatwebelieveapri-
t
modeloutputforthetokeny giventhein| putx. ori should be important to the LM decision. In
t
ordertodoso,wedevelopamethodologyinwhich
Then,weobtainthesaliencyscorefortheinput
tokenx bytakingtheL1norm: w
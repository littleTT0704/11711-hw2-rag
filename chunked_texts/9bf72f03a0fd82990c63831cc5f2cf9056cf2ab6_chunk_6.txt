/8 59.2±0.6 93.0±0.2 86.2±0.7 89.1±0.1 91.5±0.1 84.6±0.1 92.1±0.1 70.6±0.6
Q-BERT 4/8 - 92.7 - - - 83.9 - -
LSQ* 4/8 58.7±0.5 93.2±0.1 83.4±0.6 89.1±0.1 91.5±0.1 84.6±0.1 91.6±0.1 68.6±0.4
SQuAT* 4/8 59.1±0.4 93.6±0.2 85.1±0.5 89.3±0.1 91.5±0.1 85.1±0.1 91.9±0.2 69.4±0.7
Table1:PerformancecomparisonsofdifferentquantizationmethodsonGLUEbenchmark. Thescoreisevaluated
onthedevelopmentsetofthetaskusingthespecifiedmetric.WecompareourSQuATagainstGOBO(Zadehetal.,
2020)(PTQbaseline),Q-BERT(Shenetal.,2020),andLSQ(Esseretal.,2020). Wealsolistfull-precisionmodel
(FP32),andthe8-bitsQ8Bert(Zafriretal.,2019)modelasreferences. Wereportthemeanandstandarddeviation
oftheperformancecalculatedover3randomseeds."-"denotesresultswerenotreportedintheoriginalpaper.Note
here (Bai et al., 2020; Zhang et al., 2020; Kim et al., 2021) all initialized with different BERT models than our
BERT model,andthusarenotlistedincomparisonhere. Fortaskswithmultiplemetrics,wereportthemain
base
metrichere. ThealternativemetricsareshowninTable2oftheappendix. *indicatesourownimplementation
et al., 2021), many QAT methods that worked quantization be aware of loss landscape. A more
with ResNet on vision tasks would not perform recentunpublishedwork(Liuetal.,2022)(concur-
equallywellwiththeheavierparam
baselinesatdetox-
Forallbaselines,weusenucleussampling(Holtz- ification,althoughwithalossinfluencyanddiver-
manetal.,2020)withp “ 0.9togenerateupto20 sitythatislikelyduetothelesseffectivecontrast
tokens. Notethatforourmethod,nucleussampling betweenthebasemodelandanti-expert. Wereport
is done as described in §2, by using the nucleus theper-generationruntimeofeachmethodinAp-
fromthebaseLM.Othertrainingandgeneration pendixA.4todemonstrate DEXPERTS’sefficiency
details(e.g.,hyperparameters)aredescribedinAp- comparedtootherdecoding-timemethods.
pendixA.
3.2.4 HumanEvaluation
3.2.3 AutomaticEvaluation While automatic toxicity classifiers like Perspec-
tive API enable the kind of large-scale evalua-
We evaluate our generations for toxicity, fluency,
tion required for systematic comparison of meth-
anddiversity. Followingpreviouswork(Gehman
ods, an abundance of work shows that their ac-
etal.,2020),wecharacterizegenerationtoxicityus-
curacy is far from ideal (Dixon et al., 2018; Sap
ingthetoxicityscorefromPerspectiveAPI,along
et al., 2019; Davidson et al., 2019; Hutchinson
two axes: 1) the maximum toxicity over k “ 25
et al., 2020) in part due to reliance on spurious
generations,and2)theempiricalprobabilityofgen-
features, which we discuss in §8. Therefore, we
eratingacontinuationwithtoxicityě 0.5atleast
carryoutahumanevaluationonAmazonMechan-
onceoverk “ 25generations. Generationfluency
ical Turk on 120 random prompts from the 10K
is measured by the mean perplexity of generated
nontoxic subset. For each prompt, we compare
continuationsaccordingtoalargerpretrainedLM,
four pairs of models: DEXPERTS (large) versus
GPT-2XL.Generationdiversityismeasuredusing
GPT-2
 robot agent (Figure 5), and were paid $0.25 per such annotation. For
takes in the current world state W and a natural language eachofour2370successfultrials,weobtainedtwolanguage
0
instructionL.Theagentcomputesafuturepredictionusinga commands describing the high level pick-and-place goal.
DREAMCELL, by rolling out predicted goals G t,...,G
t+H
On average, commands are 11 words long.2 We compare
which generate latent space subgoals. These subgoals can Turk data to unambiguous templated language that was
then be visualized to provide insight into how the robot procedurally generated from the manipulated blocks.
expects the task to progress. This also illuminates misunder-
VI. RESULTS
standings and limitations of the system (see Analysis VI).
We ran a set of experiments on our simulation, and
Thesystemgeneratesnewprospectiveplansouttoagiven
computedtaskexecutionsuccessrate.Weanalyzetheperfor-
planning horizon. After predicting the next subgoal z, it
t+1
mance of the Subgoal and Predictor modules given different
willthenusetheactortoestimatethenextmotiongoalÎ¸.
t+1
classes of language input.
Thisgoalissenttothelow-levelexecutionsystem,whichin
our case is a traditional motion planner that does not have A. Execution Results
knowledge of object positions. In our case, the planner used
Finally, we test our model on a set of held-out scenarios,
was RRT-connect [32], via MoveIt
and compare to ground-truth execution. We compared accu-
In the future, these subgoals shown to the user, who can
racy of the estimated motion under each of three conditions:
give the final confirmation on whether or not to execute this
withoraclesubgoalsG fromthetestdata,withunambiguous
hallucinatedtaskplanifitaccomplisheswhattheyrequested. t
templated language, and with natural language. Position
Alternatively, the user could input a new L, or the agent
accuracy results are shown in Table I.
could sample a new sequence of goals.
In all cases, we compute an
D ∞ 19.095 14.077
ds
KNN same ffn Big L2 N ×D ∞ 20.734 15.594
ds
KNN same ffn Big IP N ×D ∞ 21.101 16.254
ds
Overfit@92 diff ffn V IP F +V ×D 1702.806 21.732 17.764
Overfit@129 diff ffn V IP F +V ×D 8966.508 21.733 17.814
Table9: Performancecomparisonofseveralbaselineswithtwooverfittedmodels,at92and129additional
epochs.
search are trying to memorize the training data. In other words, the parametric LM is under-fitting some
tokens. TheintuitionbehindthisisthatthekNNcomponentretrievesexamplesdirectlyfromthetrainingset.
WhatifwecouldretrievethesameexamplesusinganoverfittedLM?WetookthetrainedLM,removedthe
dropout,andcontinuedtraininguntilalmostperfectfit(verysmalltrainingloss). Wetheninterpolatedthe
overfittedtransformerwiththeoriginalLM.TheresultsareshowninTable9. F representsthenumberof
parametersinthebaseLM,minustheoutputembeddingmatrix. Wecanseethatoverfittingcanprovidevery
littlehelpafterinterpolation. Lookingattheoracleperformance,wethinkthattheoverfittedmodelmemorizes
somerarecontextsandtokensinthetrainingsetwhereitcouldbeusefulduringevaluation. However,the
overfittinghurtstheperformanceonothertokenstoomuchsothateveninterpolationisnotabletobalancethe
performance.
E.6.2 Soft-LabelTraining
Yangetal.(2022)claimsthatusing“softlabels”duringtrainingisthekeytokNN’ssuccess,thatinterpolates
theground truthlabelswith kNN-LMmodeloutputs, effectively“distilling” kNN-LM.It isbasedonthe
hypothesisthattheroomforkNN-LM’simprovementoverbaseLMliesinthe“over-correction”whentraining
witha1-hotlabels. Thisisrelatedt
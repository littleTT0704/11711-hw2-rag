avoids
wedemonstrategainsonsymbolicandalgorithmicbench-
theseproblemsbyoffloadingthecalculationandsomeof
marks as well, and (2) chose benchmark-specific prompt
thereasoningtoaPythoninterpreter, whichiscorrectby
examples,whileweusedthesamepromptexamplesaspre-
construction, given the right program. Further, not only
viouswork,todisentangledthebenefitofourapproachfrom
thatPALcanimprovethestandardchain-of-thought,itcan
thebenefitofthechoiceofexamples.
improveleast-to-mostprompting(Zhouetal.,2022)aswell,
asweshowinAppendixI.
LMs with external tools Several prior works have
equippedneuralmodelswithspecializedmodules. Forex-
ample, Cobbe et al. (2021) employ a calculator for arith- Semantic parsing Our work can also be seen as a very
metic operations as a post hoc processing, and Demeter generalformofsemanticparsing,whereinsteadofparsing
&Downey(2020)addspecializedmodulesforgenerating intostrictdomain-specificlanguages,themodelgenerates
citiesanddates. Unliketheseworks, PALgeneratescode free-formPythoncode. Someworksconstrainthedecoder
foraPythoninterpreter,whichisgeneralenoughtohandle usingaContext-FreeGrammar(CFG)togenerateadomain-
botharithmeticcalculationsanddates,withoutspecialized specificmeaningrepresentation(Shin&VanDurme,2021)
modulesandad-hocfixes. Chowdheryetal.(2022)andWei oracanonicalutterance,whichcanbeconvertedtoaLisp-
etal.(2022)havealsoexperimentedwithexternalcalcula- likemeaningrepresentation(Shinetal.,2021). Incontrast,
tors;however,thecalculatorhadimprovedCodexbyonly PALdoesnotrequireanyconstrainingordomain-specific
2.3%(absolute)onGSM8KandimprovedPaLM-540Bby representationsotherthanPythoncode. Further,LMsthat
1.7%, while PAL
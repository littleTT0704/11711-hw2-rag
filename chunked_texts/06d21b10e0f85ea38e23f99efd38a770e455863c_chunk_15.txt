resultsarenotthesameasthosereported
usingtheSpearman’sρoftheconcatenationofthe
ReimersandGurevych(2019). However,wealso
datasetsforeachyear,andfindouraverageperfor-
includeresultsusingtheirapproachforcomplete-
manceoverthe2012-2016datasetstobe76.9,com-
ness. Oneotherdifferencebetweenthesetwoways
paredto77.4and77.9fortheRoBERTa-base(Liu
of calculating the results is the inclusion of the
etal.,2019)andRoBERTa-largeversionsofSim-
SMT dataset of the 2013 task, which we also ex-
CSE.Whileourperformanceisslightlylower,we
clude when replicating the approach in Reimers
note that they tune their model on the dev set of
andGurevych(2019).
theSTSBenchmark(Ceretal.,2017),whichcon-
Forcross-lingualsemanticsimilarityandseman-
tainsasubsetofthedatafromSTStaskswhichwe
tic similarity in non-English languages, we eval-
useforevaluation. Therefore,theyaretuningona
uate on the STS tasks from SemEval 2017. This
subsetoftheevaluationdata,anditisunclearhow
evaluationcontainsArabic-Arabic,Arabic-English,
tuningonthistestdataaffectsmodelperformance.
Spanish-Spanish, Spanish-English, and Turkish-
English datasets. The datasets were created by Cross-Lingual Semantic Similarity. The re-
translating one or both pairs of an English STS sultsforthenon-Englishandcross-lingualseman-
pairintoArabic(ar),Spanish(es),orTurkish(tr).
ticsimilarityevaluationareshowninTable3. From
Followingconvention,wereportresultswithPear- theresults,ourmodelagainoutperformsallprior
son’sr forallsystems,butalsoincluderesultsin work using sentence embeddings. The only sys-
Spearman’sρforLASER,LaBSE,and P-SP. temsthathavebetterperformancearethetop(non-
We also evaluate
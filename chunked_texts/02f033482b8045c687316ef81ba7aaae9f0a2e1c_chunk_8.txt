.464 40.61 0.58 0.86 0.86
DAPT 0.428 0.360 31.21 0.57 0.84 0.84
GeDi 0.363 0.217 60.03 0.62 0.84 0.83
DEXPERTS(anti-only) 0.352 0.191 52.02 0.58 0.80 0.73
DEXPERTS(small) 0.302 0.118 38.20 0.56 0.82 0.83
DEXPERTS(medium) 0.307 0.125 32.51 0.57 0.84 0.84
DEXPERTS(large) 0.314 0.128 32.41 0.58 0.84 0.84
Table1:ResultsofexperimentsindetoxifyinggenerationsfromGPT-2. DEXPERTS(size)indicatesthesizeofthe
(anti-)experts. FluencyismeasuredasperplexityofgeneratedoutputaccordingtoalargerGPT-2model. Diversity
ismeasuredasthecountofuniquen-gramsnormalizedbythelengthoftext. Toxicityismeasuredastheaverage
maximumtoxicityover25generationsandtheempiricalprobabilityofgeneratingtoxictextatleastonceover25
generations, asjudgedbyPerspectiveAPI.Allmodelsareevaluatedonadatasetof10Knontoxicpromptsfrom
RealToxicityPrompts(Gehmanetal.,2020),exceptPPLM,whichisevaluatedonasubsetof1Kprompts,dueto
thegreatercomputationalexpense.
Weusethetoxicanti-expertbasedonGPT-2Large all existing baselines at detoxification. In partic-
andthesamehyperparametervalueα “ 2.0. ular, DEXPERTS (medium, large) are among the
mostfluentcontrollablegenerationmethods,while
Non-ToxicExpert Finally,weconsidergenerat-
fullypreservingoutputdiversitycomparedtothe
ing directly from the non-toxic expert based on
basemodel. Moreover,theDEXPERTS(anti-only)
GPT-2Large.
ablationcontinuestooutperform
 for all
experiments.
Table5: AANG-TDspecificHyper-parameters
Hyper-parameter Values Description
aux lr 1.0,0.1 Learningrateforfactorvectors-{WAll,WI,WT,WR,WO}
sopt lr 0.1,0.01 Learningrateforprimarytaskweightingλ
e
nconf subsamp 3,6 Numberofsub-sampledauxiliarytasks.
learningrate 1e-3,1e-4 LearningrateusedforfurthertrainingofRoBERTa
base
aux bsz 256 Batchsizeofforauxiliaryobjectives
Table6: AANG-TD+EDspecificHyper-parameters
Hyper-parameter Values Description
aux lr 1.0,0.5,0.1 Learningrateforfactorvectors-{WAll,WI,WT,WR,WO}
sopt lr 0.1 Learningrateforprimarytaskweightingλ
e
nconf subsamp 6,12,24 Numberofsub-sampledauxiliarytasks.
learningrate 1e-4 LearningrateusedforfurthertrainingofRoBERTa
base
aux bsz 1024 Batchsizeofforauxiliaryobjectives
Table7: META-TARTANHyper-parametersforsingletaskauxiliarytasks
Hyper-parameter Values Description
sopt lr 1.0,0.1,0.01 Learningrateforprimarytaskweightingλ
e
learningrate 1e-3,1e-4,5e-5 LearningrateusedforfurthertrainingofRoBERTa
base
META-TARTANintroducesadev-headwhichistrainedsporadicallyduringtrainingforestimating
themeta-gradients. Weusethefollowinghyper-parametersfortrainingthisdev-head: wesample32
examples(8examplesinthecaseofH.PARTISAN)andperformfullbatchgradientdescentwith
15
PublishedasaconferencepaperatICLR2023
alearningrateof1e-2for10iterations. Thedev-headistrainedwiththeAdamWoptimizerwith
weightdecaysetto0.1.
Wecopytheend-taskagnosticbaselineresultsfrom(Deryetal.,2021b)whenavailable. Weus
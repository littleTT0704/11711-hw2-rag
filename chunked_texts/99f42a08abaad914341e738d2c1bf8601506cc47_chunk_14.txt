� θ1 1)
)
can be viewed as special cases under this unified frame-
work, as shown in Table 1 (first four rows). To intuitively whereη(θ 1)−η(θ 2)>∆(θ 1)meansthatxcanbecorrectly
understand different variants of angular margin, we also classified,η(θ 1)−η(θ 2)=∆(θ 1)meansthatxexactlylieson
compare their characteristic functions ∆(θ) in Fig. 1(a). thedecisionboundaryandη(θ 1)−η(θ 2)<∆(θ 1)meansthat
One can observe that different hyperspherical FR methods xcannotbecorrectlyclassified.Theresultsimplythatwhen
yield different large-margin characteristic functions. Each xcanbecorrectlyclassified,atrivialsolutiontoreduceloss
characteristicfunctiondetermineshowahypersphericalFR to zero is to simply increase s. However, increasing s does
method performs and therefore it is of great significance not help the neural network learn angularly discriminative
to design a suitable characteristic function. Specifically, the face embeddings and results in bad local minima. Because
characteristic function ∆(θ) clearly reveals the induced an- (cid:107)x(cid:107) can be viewed as an instance-dependent learnable s,
gular margin for samples with different recognition hard- the neural network without feature normalization is likely
ness (larger θ y typically implies a harder sample). Instead tosimplyincrease(cid:107)x(cid:107)afterθ 1passesthedecisionboundary.
ofastaticcharacteristicfunction,designingadynamicchar- Therefore,usingaconstantscanpreventthistrivialwayof
acteristic function could be beneficial [37], [40]. It is also reducing loss value and eliminate these bad local minima.
possibletolearnthecharacteristicfunctioninadata-driven We also plot how the loss value changes as s increases in
andautomaticfashion,asexploredin[70],[71]. Fig. 2(a). The same argument can easily generalize to the
Besidesthecharacteristic
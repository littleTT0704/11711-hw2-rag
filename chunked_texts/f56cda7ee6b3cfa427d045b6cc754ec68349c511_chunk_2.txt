 mod-
duceutterancesthataregenderandraciallybiased
els,suchasDialoGPT,whichweshowaretwo
(Wolfetal.,2017;Shengetal.,2020;Dinanetal.,
timesmorelikelytoagreewithoffensivecom-
ments. To enable automatic detection of of- 2020a). For example, OpenAI’s GPT-3 (Brown
fensive language, we fine-tuned transformer- et al., 2020), a 175 billion parameter neural net-
based classifiers on TOXICHAT that achieve work, has been shown to generate dangerous ad-
0.71 F 1 for offensive labels and 0.53 Macro- vice,suchasrecommendingahypotheticalpatient
F 1 for stance labels. Finally, we quantify tokillthemselves.2 Presentinguserswithcontent
the effectiveness of controllable text genera-
generatedbyaneuralnetworkpresentsnewrisks,
tion (CTG) methods to mitigate the tendency
as it is difficult to predict when the model might
ofneuraldialoguemodelstoagreewithoffen-
saysomethingtoxic,orotherwiseharmful.
sivecomments. Comparedtothebaseline,our
best CTG model achieves a 19% reduction in A key challenge for conversational AI is that
agreement with offensive comments and pro- toxiclanguageisoftencontext-dependent(Dinan
duces 29% fewer offensive replies. Our work
etal.,2019a),makingitnotoriouslydifficulttode-
highlights the need for further efforts to char-
tect;textthatseemsinnocuousinisolationmaybe
acterizeandanalyzeinappropriatebehaviorin
offensivewhenconsideredinthebroadercontext
dialogue models, in order to help make them
safer.1 of a conversation. For example, neural chatbots
willoftenagreewithoffensivestatements,whichis
1 Introduction undesirable(seeexamplesinFigure1). Thesolu-
tionemployedbycurrentsystems,suchasGPT-3
Despite significant progress toward data-driven
orFacebook’sBlenderchatbot(Rolleretal.,2021),
conversationalagents(Ritteretal.,2011;Lietal.,
istostopproducingoutputwhenoffensiveinput
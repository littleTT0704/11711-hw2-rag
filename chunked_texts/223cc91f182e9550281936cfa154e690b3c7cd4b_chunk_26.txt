 on the structures of the data and the target model. In particular, one could apply a “split” function that
artificially partitions t∗ into two parts (x∗,y∗) = split(t∗) in different, sometimes stochastic ways. Then
the two parts are treated as the input and output for the properly designed target model p (x,y) for
θ
supervised MLE as above, by plugging in the slightly altered experience function:
f :=f data-self(x,y;D) =logEt∗∼D, (x∗,y∗)=split(t∗)[I(x∗,y∗)(x,y)]. (4.4)
A key difference from the above standard supervised learning setting is that now the target variable y is not
costly obtained labels or annotations, but rather part of the massively available data instances. The paradigm of
treating part of observed instance as the prediction target is called ‘self-supervised’ learning (Lecun & Misra,
2021) and has achieved great success in language and vision modeling. For example, in language modeling
(Brown et al., 2020; Devlin et al., 2019), the instance t is a piece of text, and the ‘split’ function usually selects
from t one or few words to be the target y and the remaining words to be x.
4.1.3. Unsupervised Data Instances
In the unsupervised setting, for each instance t = (x,y), such as (image, cluster index), we only observe the
x part. That is, we are given a data set D = {x∗} without the associated y∗. The data set defines the
20
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
~
empirical distribution p (x). The experience can be encoded in the same form as the supervised data
d
(Equation 4.2) but now with only the information of x∗:
f :=f data(x;D) =logEx∗∼D[Ix∗(x)]. (4.5)
Applying the SE to this setting with proper specifications derives the unsupervised MLE algorithm.
Unsupervised
eval
sim (s,g )=proj(M (s,g ))+λsim (s,g ) (1)
In the first stage, we independently encode each 2 i c i 1 i
step s ∈ S and goal g ∈ G with a model M,
b where proj(·) takes an d-dimension vector and
resultinginembeddingse s1,e s2,...,e sn ande g1, turnsittoascalarwithweightmatrixW ∈ Rd×1,
e,..., e. The similarity score between s and
g2 gm and λ is the weight for the first-stage similarity
g is calculated as the cosine similarity between
score. BothW andλareoptimizedthroughback-
e and e. We denote this first-stage similarity
s g propagation(seemoreaboutlabeleddatain§4.1).
scoreassim (s,g). Usingthisscore,wecanobtain
1 With labeled data, we finetune M to mini-
c
the top-k most similar candidate goals for each
mizethenegativelog-likelihoodofthecorrectgoal
step s, and we denote this candidate goal list as
amongthetop-k candidategoallist,wherethelog-
C(s) = [g,...,g ]. To perform this top-k search,
1 k likelihoodiscalculatedas:
weuseefficientsimilaritysearchlibrariessuchas
(cid:32) (cid:32) (cid:33)(cid:33)
FA WIS eS in(J so tah nn tis ao tn eMeta bl. w,2 it0 h1 t7 w) o.
learning-basedpara-
ll(s,g i)=−log softmax (cid:80) gj∈s Cim (s2 )( ss im,g 2i ()
s,g j)
(2)
phrase encoding models. The first is the SP
model(Wietingetal.,2019,2021),whichencodes Comparedtotherandomlysampledin-batchneg-
asentenceastheaverageofthesub-wordunitem- ativeexamples,thetop-k candidategoalsarepre-
bedd
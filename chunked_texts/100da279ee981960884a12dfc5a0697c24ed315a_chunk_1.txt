PublishedasaconferencepaperatICLR2023
SOFTMATCH: ADDRESSING THE QUANTITY-QUALITY
TRADE-OFF IN SEMI-SUPERVISED LEARNING
HaoChen1∗,RanTao1∗,YueFan2,YidongWang3
JindongWang3†,BerntSchiele2,XingXie3,BhikshaRaj1,4,MariosSavvides1†
1CarnegieMellonUniversity,2MaxPlanckInstituteforInformatics,SaarlandInformaticsCampus,
3MicrosoftResearchAsia,4MohamedbinZayedUniversityofAI
ABSTRACT
The critical challenge of Semi-Supervised Learning (SSL) is how to effectively
leverage the limited labeled data and massive unlabeled data to improve the
model’s generalization performance. In this paper, we first revisit the popular
pseudo-labelingmethodsviaaunifiedsampleweightingformulationanddemon-
strate the inherent quantity-quality trade-off problem of pseudo-labeling with
thresholding, which may prohibit learning. To this end, we propose SoftMatch
to overcome the trade-off by maintaining both high quantity and high quality of
pseudo-labelsduringtraining,effectivelyexploitingtheunlabeleddata.Wederive
atruncatedGaussianfunctiontoweightsamplesbasedontheirconfidence,which
canbeviewedasasoftversionoftheconfidencethreshold. Wefurtherenhance
the utilization of weakly-learned classes by proposing a uniform alignment ap-
proach.Inexperiments,SoftMatchshowssubstantialimprovementsacrossawide
varietyofbenchmarks,includingimage,text,andimbalancedclassification.
1 INTRODUCTION
Semi-Supervised Learning (SSL), concerned with learning from a few labeled data and a large
amount of unlabeled data, has shown great potential in practical applications for significantly re-
ducedrequirements onlaborious annotations(Fanet al.,2021; Xieet al.,2020;Sohn etal., 2020;
Phametal.,2021;Zhangetal.,2021;Xuetal.,2021b;a;Chenetal
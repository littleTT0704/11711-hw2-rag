 ByT5
subwords or characters. Our experiments found onthreetasksadoptedfromtheXTREMEbench-
similarperformanceforbothvariants,soweonly mark(Huetal.,2020).
showtheperformanceofCANINE-S,leavingthe
3.1 Tasks
resultsforCANINE-Cin§B.1.
XNLI TheCross-lingualNaturalLanguageInfer-
ByT5 (Xue et al., 2022) is an encoder-decoder
ence(Conneauetal.,2018)isasequence-levelclas-
transformermodelsimilartothemT5(Xueetal.,
sificationtaskinwhichthemodelpredictswhether
2021)model. BothByT5andmT5arepretrained
thehypothesissentenceisanentailment,contradic-
onthemultilingualCommonCrawl(mC4)corpus1
tion, or neutral given the premise sentence. The
usingthespanreconstructionobjectiveproposedby
taskisprovidedin15languages.
Raffeletal.(2020). ByT5operatesontherawUTF-
8 bytes of the input without any downsampling, NER Named Entity Recognition (NER) is a
leadingtoalongersequencelengthwhilehavinga structured prediction task. We use the WikiAnn
muchsmallervocabularysizethanmT5. dataset (Pan et al., 2017), which covers 282 lan-
TokeeptheparametercountfixedbetweenmT5 guages. We select 20 languages (listed in Tab. 3)
and ByT5, Xue et al. (2022) allocate the parame- based on linguistic diversity and the languages
ters saved from the embedding layer in ByT5 to available in the other two tasks we consider. We
additionalencoderlayers. Althoughareasonable usethetrain,dev,andtestsplitsfromRahimietal.
design choice, our results in § 4 show that ByT5 (2019).
suffers from a much higher inference cost due to
TyDi QA-GoldP The Typologically Diverse
a deeper encoder and longer sequence lengths in
Question Answering (Clark et al., 2020) dataset
boththeinputandoutput.2
isanextractivequestionansweringbenchmarkin
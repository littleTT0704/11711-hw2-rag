, a suitable architec­
ture would need high-bandwidth local networks and input/output devices for conducting on-line
experiments in real time at audio bandwidths.
There are many architectures that can be used for SNLP, ranging from heterogeneous sub­
systems that are loosely coupled (say, a general-purpose network of workstations), to a com­
pletely homogeneous SIMD array, and a whole spectrum of approaches in between. In general,
programmable systems require some homogeneity so that software can be developed. Further­
more, nearly every algorithm requires some amount of general processing capability that is not
obvious from the guiding equations. This “incidental” computing can easily dominate
resources when the design of an architecture and specialized hardware has been targeted for a
special algorithm. One possible solution is to provide a common computing element, such as a
November 23,1992 25
Reduced-Instruction-Set-Computer (RISC) core, to every node of the architecture, with special­
ized accelerators provided by (possibly heterogeneous) coprocessors. Such a system can be a
good platform for developing and executing parallel software, while permitting specialization
for subtasks (such as Viterbi search and feature extraction).
4.4. Infrastructure Support
The infrastructure needed for SNLP includes the development and technical support by
high-quality staff of sharable text and speech databases, and open, easy-to-access, and easy-to-
program parallel systems.
1) Sharable text and speech databases. An important role for a shared electronic library is
to provide not only one place where information can be shared and transmitted, but to provide a
place where the information itself can be studied. As we accumulate large quantities of text,
speech, and images in sharable databases in parallel supercomputers, we have a valuable
resource for research into SNLP and machine vision.
DARPA is in the process of establishing an industry-university consortium called Linguis­
tic Data Consortium (LDC) for generating very large text and speech corpora. Initial plans call
for the collection of texts up to 5 to 10 billion words, speech data for interactive tasks up to 400
hours, and read speech up to 1000 hours. HPCC environment is crucial for the successful
exploitation of these resources soon to be available.
2) Open parallel systems.
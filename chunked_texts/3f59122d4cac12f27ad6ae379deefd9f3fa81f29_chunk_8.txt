. Actor Module
to our problem. Since each successful high-level action
Theactormodule,showninFig.4,predictstheparameters has a predictable result, we jointly train a classifier that
of an action that can be executed on the robot. Specifically, will recover the subgoal associated with each successive
it takes in z and the current high-level action and predicts a high-level action Gˆ. We use C (z ) as the classifier loss,
t t G t
destination end effector pose that corresponds to the robot’s minimizingcrossentropybetweentherecoveredestimateGˆ
t
position at z. and ground truth G.
t t
The architecture is a simple set of convolutions: the high- Actor LossInsteadofestimatingthefulljointstateofthe
level action is concatenated with the current z t as in the robot as the result of a high-level action, our Actor module
prediction module, then a set of three 3x3 convolutions estimates the end-effector pose θ associated with sugboal
t
with 64, 128, and 256 filters, each followed by a 2x2 G.
t
max pool. This is followed by a dropout and a single 512
These poses are represented as θ = (pˆ,qˆ,gˆ), where pˆ
dimensional fully connected layer, and then to N ×8
verbs is the Cartesian position, qˆ is a quaternion describing the
outputs, predicting gripper command g ∈ (0,1), Cartesian
orientation, and gˆ ∈ (0,1) is the gripper command. When
end effector position, and a unit quaternion for each high-
regressingtoposes,weuseamixtureoftheL2lossbetween
level action verb. The gripper command uses a sigmoid
Cartesian position and a loss derived from the quaternion
activation where 0 represents closed and 1 represents open,
angulardistance(tocapturebothspatialandrotationalerror).
and Cartesian end effector position uses a tanh activation
The angle between two quaternions qˆ and qˆ is given as:
1 2
function. All pose values are normalized to be in (−1,1).
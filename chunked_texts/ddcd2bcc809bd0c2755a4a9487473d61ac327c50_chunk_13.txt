
different LLMs of different sizes: FlanT5: havehadmoreopportunitiestotrainontheolder
flan-t5-{small, base, large, xl, xxl} (Chung datasets,resultinginbetterperformance(see§8.5).
et al., 2022), FlanUl2 (Tay et al., 2022), GPT- Basedonthismeta-evaluation, ourresultssug-
3 (text-davinci-002, text-davinci-003), GPT-3.5 / gestarethatwhilesomemodelsexhibitstrongToM
ChatGPT(gpt-3.5-turbo-0301),GPT-4(gpt-4-0314) abilitiesonsomedatasets,nomodelrobustlyex-
(Brownetal.,2020;Ouyangetal.,2022),andJuras- hibitsToMonalldatasets. Thesefindingsarecon-
sic2: j2-{jumbo-instruct, grande-instruct, jumbo, sistentwithSapetal.(2022)andUllman(2023).
grande, large}.6 We provide technical details re-
gardingpromptinganddecodingparametersinAp-
4.2 HowsensitiveareLLMstotheprobing
pendix8.3.
technique?
4.1 HowwelldoLLMsperformonToM Weexaminetheeffectofthedifferentprobingmeth-
tasks? Meta-Evaluation odsdetailedbelowonLLMperformance. Certain
techniques have shown to be superior to others
Weconductedanevaluationoftheperformanceof
(e.g.,Weietal.,2023). However,wearguethatto
15LLMsinazero-shotmanner(Liuetal.,2021)on
claimthatamodelhasN-ToMabilities,itisessen-
allToM-relateddatasetsconsidered(§3),andcom-
tialthatitperformswellacrossprobingtechniques.
paretoamost-frequent-class(MFC)baselinethat
Ononehand,themostefficientmethodcanpoten-
alwayspredictsthemostfrequentanswer
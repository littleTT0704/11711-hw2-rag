field
approximation, which assumes a diagonal variational covariance matrix). We refer interested readers to Wilson
et al. (2016b) for more details.
Wake-Sleep. In some cases when the auxiliary q is assumed to have a certain form (e.g., a deep network), the
approximate E-step in Equation 2.12 may still be too complex to be tractable, or the gradient estimator (w.r.t.
the parameters of q) can suffer from high variance (Mnih & Gregor, 2014; Paisley et al., 2012). To tackle the
challenge, more approximations are introduced. The wake-sleep algorithm (Hinton et al., 1995) is one of such
methods. In the E-step w.r.t. q, rather than minimizing KL(q(y)∥p (y∣x∗)) (Equation 2.9) as in EM and
θ
variational EM, the wake-sleep algorithm makes an approximation by minimizing the Kullback–Leibler (KL)
divergence in opposite direction:
Approximate E-step (Sleep-phase): minKL(p (y∣x∗)∥q(y)),
θ (2.13)
q∈Q′
which can be optimized efficiently with gradient descent when q is parameterized. Besides wake-sleep, one
can also use other methods for low-variance gradient estimation in Equation 2.12, such as reparameterization
gradient (Kingma & Welling, 2014) and score gradient (Glynn, 1990; Mnih & Gregor, 2014; Ranganath et al.,
2014).
In sum, the entropy maximization perspective has formulated unsupervised MLE as an optimization-theoretic
framework that permits simple alternating minimization solvers. Starting from the upper bound of negative
marginal log-likelihood (Equation 2.9) with maximum entropy and minimum cross entropy, the originally
9
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
intractable MLE problem gets simplified, and a series of optimization algorithms, ranging from (variational)
EM to wake-sleep, arise naturally as an approximation to the original solution.
2.2. Bayesian In
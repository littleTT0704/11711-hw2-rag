Hu and Xing, 2022),
tions) (Brown et al., 2020). The KG consists of
which inverts the conventional problem of using
symbolicknowledgetolearnneuralnetworks(Hu
asetofknowledgetuplesintheform⟨HEAD EN-
etal.,2016). TITY (h), RELATION (r), TAIL ENTITY (t)⟩. Our
approach utilizes the LM to automatically har-
LMsasknowledgebases Anotherlineofworks vest a large number of appropriate entity pairs
attemptedtouseLMsasknowledgebases(LAMA, (h 1,t 1),(h 2,t 2),..., for every given relation r.
Petronietal.2019). Theseworksarealsoknownas Thispresentsamorechallengingproblemthantra-
factualprobingbecausetheymeasuredhowmuch ditionalLMprobingtasks,whichtypicallypredict
knowledgeisencodedinLMs. Thisisusuallyim- asingletailentityorasmallnumberofvalidtail
plementedbypromptingmethodsandleveraging entitiesgivenaheadentityandrelation.
the masked LM pretraining task. LPAQA (Jiang Our approach for extracting knowledge tu-
etal.,2020)proposestousetextminingandpara- ples of a specific relation of interest, such as
phrasing to find and select prompts to optimize "potential_risk" as depicted in Figure 2, only
thepredictionofasingleorafewcorrecttailenti- requiresminimalinputinformationthatdefinesthe
ties,insteadofextensivelypredictingallthevalid relation. This includes an initial prompt, such as
entity pairs like in our framework. AutoPrompt "The potential risk of A is B"andasmall
(Shinetal.,2020),QinandEisner,2021andOP- numberofexampleentitypairs,suchas⟨EATING
TIPrompt (Zhong et al., 2021) learn discrete or CANDY, TOOTH DECAY⟩. Thepromptprovidesthe
continuous prompts automatically with an addi- overallsemanticsoftherelation,whiletheexam-
tional training set. Though making prompts un- ple entity pairs clarify possible ambigu
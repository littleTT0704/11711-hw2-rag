classifier y T =? y S classifier y T =? y S
e T =? e S student student
simulability loss
explainer explainer regularizer explainer
×N
Figure 1: Illustration of our SMaT framework. First, a student model is trained to recover the
classifier’spredictionsandtomatchtheexplanationsgivenbytheexplainer. Then,theexplaineris
updatedbasedonhowwellthetrainedstudentsimulatestheclassifier(withoutaccesstoexplanations).
Inpractice,werepeatthesetwoconsecutiveprocessesforseveralsteps. Greenarrowsandboxes
representlearnablecomponents.
explainabilitymethodsformeasuringconsistency[JainandWallace,2019,SerranoandSmith,2019],
sufficiencyandcomprehensiveness[DeYoungetal.,2020],andsimulability: whetherahumanor
machineconsumerofexplanationsunderstandsthemodelbehaviorwellenoughtopredictitsoutput
onunseenexamples[Lipton,2016,Doshi-VelezandKim,2017]. Simulability,inparticular,hasa
numberofdesirableproperties,suchasbeingintuitivelyalignedwiththegoalofcommunicatingthe
underlyingmodelbehaviortohumansandbeingmeasurableinmanualandautomatedexperiments
[TrevisoandMartins,2020,HaseandBansal,2020,Pruthietal.,2020].
For instance, Pruthi et al. [2020] proposed a framework for automatic evaluation of simulability
that,givenateachermodelandexplanationsofthismodel’spredictions,trainsastudentmodelto
matchtheteacher’spredictions. Theexplanationsarethenevaluatedwithrespecttohowwellthey
helpastudentlearntosimulatetheteacher(§2). Thisisanalogoustotheconceptinpedagogyof
instructionalscaffolding[VandePoletal.,2010],aprocessthroughwhichateacheraddssupport
forstudentstoaidlearning. Moreeffectivescaffolding—inourcase,betterexplanations—isassumed
toleadtobetterstudentlearning. However,whilethispreviousworkprovidesanattractive
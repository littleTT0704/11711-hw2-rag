specific
dings,wefindthatthishasamodestoveralleffect variationswithouthavingseparateparametersfor
onperformance. Lastly,weeliminatethelanguage eachlanguage. Evidenceforthishypothesisisin
embeddings. Firstweremovethelanguageembed- Table4wherehavingthelanguagevariationshared
ding inputs to the decoder (no enc. l.e.), then we amongst4encodersinsteadof1actuallyappears
experiment by removing the input language em- toweakenperformanceoverall.
beddingstothelanguage-specificencoder(nodec.
7.3 Zero-ShotBitextMining
l.e.). We find these language embeddings have a
smallerthanexpectedimpactonperformance,per- TheTatoebadatasetcontainsparallelsentencepairs
hapsbecausethelargecapacityofthedecodercan of English with 112 languages. Our model is
ascertainthelanguagebeinginputordecoded. trained using 93 of these languages, and there-
forethereare19languageswecanuseforazero-
7.2 TestingtheParameterSharinginVMSST
shotevaluationofbitextmining. Table8summa-
Parametersharingwasneededinorderefficiently rizes the results of this zero-shot evaluation for
performsourceseparationonN languages. Specif- the two generation objectives, BITRANSLATION
ically we collapsed the language encoders into a and VMSST considered in this paper. The re-
singleencoderandwecollapsedthedecodersintoa sults are shown in Table 8. We also compute âˆ†
singledecoder. The VMSST approximateshaving which is the difference between the performance
N languageencodersbyusinganinputembedding gapof VMSST and BITRANSLATION ontheseen
to indicate the language being considered. The and unseen languages. From the results, we see
samestrategyisappliedwiththedecodersaswell, thatVMSST doesevenbetterthanBITRANSLA-
withthefirstinputtokentothedecoderindicating TION onunseenlanguagesthanunseenlanguages.
thelanguagetobegenerated. Since BITRANSLATION can
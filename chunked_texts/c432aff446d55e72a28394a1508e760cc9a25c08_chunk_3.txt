max() ğ·
ğ· ğ·
â„ ğ‘ ğ‘š ğ‘Š ğ‘ ğ‘š ğ‘‰ + â„ ğ‘‘ğ‘ 
Feed Forward FFN
Network ğ‘Š ğ‘‘ğ‘  ğ‘ ğ‘‘ğ‘  mask-to-k()
Layer Norm ATT In ğ‘˜NN-LM:
top-ğ‘˜()
Multi Headed
Attention In ğ‘˜NN-LM:
ğ‘ : up to 5000ğ‘‰
ğ‘‘ğ‘ 
Figure1: AnillustrationofthegeneralizedformulationofkNN-LMinEquation5.
notsimplybenefitingfromaccesstomoredata. Intriguedbythis,weaskquestionslike,couldkNN-LMbe
improvingbecauseofcapacityissuesintheparametricbaseLM?Inthispaper,wesetouttounderstandwhy
kNN-LMsworkeveninthissetting.
Inthefollowingsections,wefirstelucidateconnectionsbetweentheaddedkNNcomponentandthestandard
LMcomponent. Specifically,wenotethatworddistributionsfromthetwocomponentsarebothcalculated
usingasoftmaxfunction,basedonthesimilarityofthecurrentcontextembeddingwithasetofembeddings
thatcorrespondstodifferentnextwords. Withthisintuition,weformalizeandgeneralizethenon-parametric
distributioncalculationwiththesoftmaxlayerandwordembeddinglayerusedinparametricLMs. Wethen
showthatthisgeneralizedformexposesavarietyofdesignchoices,e.g.,thenumberofcontextembeddings
inthedatastore,theinputrepresentationusedinsoftmaxlayer,differentsimilarityfunctions,aswellasthe
approximationandsparsificationimplementationsinthekNNsearch. Thisprovidesageneralframeworkfor
analyzingkNN-LMandsimilarmodelsandallowsustoperformablationstudiesthattesttheimportanceof
variousdesigndecisions.
WeproceedtoproposemultiplehypothesesforwhykNN-LMworks, whicharetestablebyadjustingthe
variousparameter
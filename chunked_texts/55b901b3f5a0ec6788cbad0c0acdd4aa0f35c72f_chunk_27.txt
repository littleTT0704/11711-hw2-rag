oscope: Anefficient,learnablerepresentationforallstructured
linearmaps. arXivpreprintarXiv:2012.14966,2020.
LucioMDery,YannDauphin,andDavidGrangier. Auxiliarytaskupdatedecomposition: Thegood,
thebadandtheneutral. arXivpreprintarXiv:2108.11346,2021a.
LucioMDery,PaulMichel,AmeetTalwalkar,andGrahamNeubig. Shouldwebepre-training? an
argumentforend-taskawaretrainingasanalternative. arXivpreprintarXiv:2109.07437,2021b.
10
PublishedasaconferencepaperatICLR2023
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
XuanyiDongandYiYang.Searchingforarobustneuralarchitectureinfourgpuhours.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.1761–1770,2019.
Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Celebi, Michael Auli, Ves
Stoyanov, and Alexis Conneau. Self-training improves pre-training for natural language
understanding. arXivpreprintarXiv:2010.02194,2020.
ChelseaFinn,PieterAbbeel,andSergeyLevine. Model-agnosticmeta-learningforfastadaptationof
deepnetworks,2017. URLhttps://arxiv.org/abs/1703.03400.
Jean-Bastien Grill, Florian Strub, Florent Altche´, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya,CarlDoersch,BernardoAvilaPires,ZhaohanGuo,MohammadGheshlaghiAzar,
etal. Bootstrapyourownlatent-anewapproachtoself-supervisedlearning. AdvancesinNeural
InformationProcess
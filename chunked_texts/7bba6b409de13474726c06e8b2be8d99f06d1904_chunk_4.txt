 the processor con-
or books. However, readers also frequently encounter texts
stantly uses the current context to compute expectations for
that contain errors, e.g., in hand-written notes, emails, text
thenextword,anddifficultyensuesiftheseexpectationsturn
messages,andsocialmediaposts. Intuitively,sucherrorsare
outtobeincorrect. However,ifthecontextisdegradedbya
easytocopewithandimpedeunderstandingonlyinaminor
largenumberoferrors,thenitishardertocomputeexpecta-
way.Infact,errorsoftengounnoticedduringnormalreading,
tions(andtheybecomelessreliable),andreadingshouldslow
whichispresumablywhyproofreadingisdifficult.
down. Crucially,weexpecttoseethiseffectonallwords,not
Theaimofthispaperistoexperimentallyinvestigateread-
inginthefaceoferrors, andtoproposeasimplemodelthat
1For example, in the error corpus we use (Geertzen, Alex-
canaccountforourexperimentalresults. Specifically,wefo-
opoulou,&Korhonen,2014)only11%oftheerrorsareletterswaps
cusonerrorsthatchangetheformofaword,i.e.,thataltera orrepetitions,seeTable1.
Proceedingsofthe41stAnnualConferenceoftheCognitiveScienceSociety.Montreal,2019.
just on those words that contain errors. This is the second phonetics deletion swap/repeat keyboard insertion other
predictionthatwewilltestinoureye-trackingexperimentby
36.2 16.7 11.0 10.5 8.3 17.3
comparingtextswithhighandlowerrorrates.
In the second part of this paper, we present a surprisal Table1: Percentagesofdifferenttypesofmisspellingsinthe
modelthatcanaccountforthepatternsofdifficultyobserved naturalerrorcondition.
in our experiment on reading texts with errors. We start by
showing that standard word-based surprisal does not make
therightpredictions,asitessentiallytreatswordswitherrors textisobtained. Threeincorrectanswers(distractors

Sambhav Jain, Albert Gural, Michael Wu, and Chris preprintarXiv:1910.03771.
Dick.2020. Trainedquantizationthresholdsforac-
curate and efficient fixed-point inference of deep KoheiYamamoto.2021. Learnablecompandingquan-
neural networks. In I. Dhillon, D. Papailiopoulos, tizationforaccuratelow-bitneuralnetworks. InPro-
andV.Sze, editors, ProceedingsofMachineLearn- ceedingsoftheIEEE/CVFConferenceonComputer
ingandSystems,volume2,pages112–128. VisionandPatternRecognition,pages5029–5038.
Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad,
and Andreas Moshovos. 2020. Gobo: Quantiz-
ing attention-based nlp models for low latency and
energy efficient inference. In 2020 53rd Annual
IEEE/ACMInternationalSymposiumonMicroarchi-
tecture(MICRO),pages811–824.IEEE.
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019. Q8bert: Quantized 8bit bert. In
2019 Fifth Workshop on Energy Efficient Machine
Learning and Cognitive Computing-NeurIPS Edi-
tion(EMC2-NIPS),pages36–39.IEEE.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao
Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert:
Distillation-awareultra-lowbitbert. arXivpreprint
arXiv:2009.12812.
A UnformVS.non-Uniform
Quantization
Quantization approaches can be subdivided into
uniform and non-uniform quantization. Non-
uniformquantizationtendstoachievebetteraccu-
racythanuniformquantization(Yamamoto,2021),
butrequiresnonstandardhardwaresupporttostore
codebooksorquantizationintervals,hencenotprac-
ticalwithexistinghardware(Gholamietal.,2021).
Inthes
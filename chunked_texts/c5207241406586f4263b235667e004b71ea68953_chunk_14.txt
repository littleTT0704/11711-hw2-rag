.156 0.189 0.0953
GPT-3davinci-001 0.487 0.647 0.515 0.376
GPT-3davinci-003 0.914 0.919 0.715 0.567
Table 3: Correlation between the δ-LL from intr-
agent/intr-patient sentences with the δ-LL from the
nouninisolation,humanratings,subject(GoogleSyn-
tacticNgrams),andagentratios(Propbank).
3.3 Exp3: Agentivitywiththetransitive
Figure4: Averageaccuracyforpredictingthelabelof
nounsinintr-agent/intr-patientsentences. Theblack Aspreviouslydiscussed,thesyntacticpositionof
line indicates majority class performance; blue bars thenouninthetransitivesentences(subjectorob-
indicateabovemajorityclassperformance.
ject) directly map to their semantic roles (agent
andpatient,respectively). Figure5showsaccuracy
Figure 4 shows the accuracy of each model
splitbytrans-agentandtrans-patient.
in predicting (giving a higher probability to) the
As in the previous experiments, GPT-3
correct semantic label. Over half of the models
davinci-003 outperforms all other models
do not achieve chance performance (predicting
(0.994 for trans-agent and 0.991 for trans-
the majority class ≈ 0.582). Interestingly, we
patient—it is actually the only model which per-
find that there is no monotonic increase in per-
formssignificantlyabovechanceforbothExperi-
formance for this task with respect to model size
ments 2 and 3, and is also consistent across both
(Kaplanetal.,2020)—forexample,performance
exampleorderings.
drops drastically between text-ada-001 and
1976; Comrie 1989, inter alia). However, across
both orderings, the only noun that was misclassi-
fiedwas“model”inthesentenceThismodelpho-
tographsbeautifully/nicely. Nevertheless,itcould
bearguedthatanagentinterpretationinthiscontext
isplausible.
It appears that there are two interactions that
are occurring in the above
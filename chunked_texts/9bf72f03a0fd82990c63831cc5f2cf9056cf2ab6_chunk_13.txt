, and Dharmendra S
Keutzer. 2020. Q-bert: Hessian based ultra low
Modha. 2020. Learned step size quantization. In
precision quantization of bert. In Proceedings of
International Conference on Learning Representa-
the AAAI Conference on Artificial Intelligence, vol-
tions.
ume34,pages8815–8821.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and
Stefan Uhlich, Lukas Mauch, Fabien Cardinaux,
Behnam Neyshabur. 2020. Sharpness-aware min-
Kazuki Yoshiyama, Javier Alonso Garcia, Stephen
imization for efficiently improving generalization.
Tiedemann, Thomas Kemp, and Akira Nakamura.
arXivpreprintarXiv:2010.01412.
2020. Mixed precision dnns: All you need is a
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei good parametrization. In International Conference
Yao, Michael W Mahoney, and Kurt Keutzer. onLearningRepresentations.
2021. A survey of quantization methods for ef-
Alex Wang, Amanpreet Singh, Julian Michael, Felix
ficient neural network inference. arXiv preprint
Hill, Omer Levy, and Samuel R Bowman. 2018.
arXiv:2103.13630.
Glue:Amulti-taskbenchmarkandanalysisplatform
Benoit Jacob, Skirmantas Kligys, Bo Chen, Meng- for natural language understanding. arXiv preprint
longZhu,MatthewTang,AndrewHoward,Hartwig arXiv:1804.07461.
Adam, and Dmitry Kalenichenko. 2018. Quanti-
zation and training of neural networks for efficient Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
integer-arithmetic-onlyinference. InProceedingsof Chaumond, ClementDelangue, AnthonyMoi, Pier-
the IEEE Conference on Computer Vision and Pat- ric Cistac, Tim Rault, Rémi Louf, Morgan Fun-
ternRecognition,pages2704–2713. towicz, et al. 2019. Huggingface’s transformers:
State-of-the-art natural language processing. arXiv
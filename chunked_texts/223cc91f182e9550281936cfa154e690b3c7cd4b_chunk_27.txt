 MLE. The form of Equation 3.2 is reminiscent of the variational free energy objective in the
standard EM for unsupervised MLE (Equation 2.9). We can indeed get exact correspondence by setting
~
α = β = 1, and setting the auxiliary distribution q(x,y) = p (x)q(y∣x). The reason for β = 1,
d
which differs from the specification β = ϵ in the supervised setting, is that the auxiliary distribution q cannot
be determined fully by the unsupervised ‘incomplete’ data experience alone. Instead, it additionally relies on
~
p through the divergence term. Here q is assumed a specialized decomposition q(x,y) = p (x)q(y∣x)
θ d
~
where p (x) is fixed and thus not influenced by p. In contrast, if no structure of q is assumed, we could
d θ
potentially obtain an extended, instance-weighted version of EM where each instance x∗ is weighted by the
marginal likelihood p (x∗), in line with the previous weighted EM methods for robust clustering (Gebru et
θ
al., 2016; Yu et al., 2011).
4.1.4. Manipulated Data Instances
Data manipulation, such as reweighting data instances or augmenting an existing data set with new instances, is
often a crucial step for efficient learning, such as in a low data regime or in presence of low-quality data sets
(e.g., imbalanced labels). We show that the rich data manipulation schemes can be treated as experience and be
naturally encoded in the experience function (Hu, Tan et al., 2019). This is done by extending the data-instance
experience function (Equation 4.2), in particular by enriching the similarity metric in different ways. The
discussion here generally applies to data instance t of any structures, for example, t = (x,y) or t = x.
Data reweighting. Rather than assuming the same importance of all data instances, we can associate each
instance t∗ with an importance weight w(t∗) ∈ R, so that the learning pays more attention to those high-
quality instances, while low-quality ones (e.g., with noisy labels) are downplayed. This can be
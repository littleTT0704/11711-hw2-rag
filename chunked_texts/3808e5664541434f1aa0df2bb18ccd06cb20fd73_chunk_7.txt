 in and might hence contain spurious correlations
inFigure1). (§3.2).
3.1 DebiasedTrainingforPre-Defined
2.2 DialectalBiases(AAE)
ToxicityBiases
Current toxic language detection systems also as-
We use the LEARNED-MIXIN method of Clark
sociate higher toxicity with dialectal markers of
et al. (2019), which achieved high out-of-
AfricanAmericanEnglish(AAE;Sapetal.,2019;
distribution (OOD) performance on several NLU
Davidson et al., 2019). Since AAE is a vari-
tasks, for debiased training. This method trains
ety of English that is common among African
an ensemble containing a bias-only model which
Americans and often signals a cultural identity
only uses pre-defined features corresponding to
in the US (Green, 2002), this dialect-based racial
knownbiases,andafullmodelwhichusesallfea-
bias causes speech by Black authors to be sup-
tures. Intuitively,theensembleencouragesthefull
pressed more often than non-Black authors (Sap
et al., 2019), thereby exacerbating racial inequal- 7We avoid using disputed terms such as general Ameri-
ity(Rosa,2019). canEnglish,standardAmericanEnglish,ormainstreamUS
English, which are frequently used for WAE, since we be-
In our experiments, we estimate the dialect of
lievethatnodialectshouldbeprivilegedwiththedesignation
a tweet using a topic model from Blodgett et al. “general”,“standard”,or“mainstream”(Rosa,2019).
(2016). This model was trained on 60M tweets, 8Onlythetweettext—noprofileinformationorconversa-
tionalcontext—wasshowntoannotators.
where the dialect of the tweet was inferred from
9Wealsoexploredusinganotherwidelyusedhatespeech
themodelcoordinates,whichyieldedaprobability dataset (Davidson et al., 2017), which collected tweets us-
of a tweet being in one of four dialects (African- ing a seed list of swear words and slurs. However, in line
withfindingsby
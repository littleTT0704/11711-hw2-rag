anslation(NMT)modelsdifficult.
and Anshul Kundaje. 2016. Not just a black Wethereforealsoextendcontrastiveexplanations
box: Learning important features through prop-
toNMTmodels.
agating activation differences. arXiv preprint
We compute the contrastive gradient norm
arXiv:1605.01713.
saliency for an NMT model by first calculating
KarenSimonyan,AndreaVedaldi,andAndrewZisser-
thegradientovertheencoderinput(thesourcesen-
man.2013. Deepinsideconvolutionalnetworks: Vi-
tence)andoverthedecoderinput(thepartialtrans-
sualising image classification models and saliency
maps. arXivpreprintarXiv:1312.6034. lation)as:
Ilia MS at re típ nin P, erJ eo irs ae -FM ariñA al.o 2n 0s 2o 1,.A Ale sj ua rn vd er yo ofC ca ota nl ta ra, sta in vd
e
g ∗(xe i) = ∇xe
i
q(y t |xe,xd) −q(y f |xe,xd)
and counterfactual explanation generation methods (cid:16) (cid:17)
for explainable artificial intelligence. IEEE Access, g (xd) = q(y xe,xd) q(y xe,xd)
9:11974–12001. ∗ i ∇xd i t | − f |
(cid:16) (cid:17)
wherexe istheencoderinput,xd isthedecoder
MukundSundararajan,AnkurTaly,andQiqiYan.2017.
Axiomatic attribution for deep networks. In Inter- input, and the other notations follow the ones in
national Conference on Machine Learning, pages §3.1.
3319–3328.PMLR. Then,thecontrastivegradientnormforeachxe
i
andxd are:
Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subra- i
manian,MattGardner,andSameerSing
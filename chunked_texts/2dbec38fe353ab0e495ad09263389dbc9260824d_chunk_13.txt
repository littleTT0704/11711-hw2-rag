 of LAMT(2022) Transformer(2017) Mixed
ThePile(Gaoetal.,2020). (ii)Syntheticdatasets Table 3: Finetuned pre-trained language models for
basedontemplatesorinteractionwithengines. Re- downstreammathematicalreasoningtasks.
centwork(Wuetal.,2021d;Krishnaetal.,2021;
Ri and Tsuruoka, 2022; Anderson and Farrell, guageModeling(CLM),wherethemodelistrained
2022;Wuetal.,2022c)showsthatpre-trainingon to predict the next token in a sequence of tokens.
datathatisfullysyntheticallygeneratedâ€”synthetic Followingthesameparadigm,researcherspre-train
pre-trainingcanactuallyprovidesubstantialgains. language models with MLM and CLM tasks on
Representative work includes TaPEX (Liu et al., mathematicalorscientificcorporafordownstream
2022b),whichobtainsapre-trainingcorpusbyau- tasks(PoluandSutskever,2020;Hendrycksetal.,
tomatically synthesizing executable SQL queries 2021b;Hanetal.,2022;Jiangetal.,2022b).
and their execution outputs. LISA (Jiang et al.,
There is also recent work that designs cus-
2021)extractslemmasandtheoremsbyinteracting
tomized tasks to inject mathematical reasoning
withtheIsabellestandardlibraryandtheArchiveof
capabilities into language models. For instance,
FormalProofs. GenBERT(Gevaetal.,2020)gen-
Liangetal.(2022b)pre-trainlanguagemodelswith
eratesnumericalandtextualpre-trainingdatasets
asuiteof8numeracy-augmentedtaskswithconsid-
basedonmanuallycraftedandextractedtemplates.
erationofreasoninglogicandnumericalproperties.
Pre-trainingtasks. Generalpre-traininglanguage LIME (Wu et al., 2021d) proposes synthetic pre-
modelshavetwotypicalself-supervisedlearning training tasks to learn three reasoning primitives:

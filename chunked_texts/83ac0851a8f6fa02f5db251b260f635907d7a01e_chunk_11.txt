queue:
We compare our results to four published baselines for
thistask.8
Q ←Q +∀ {τ ∪(u,a )} (4)
F F i∈K 0 0 i
• RANDOM: an agent that randomly selects a direction
Q ←Q +τ (5)
C C 0 andmovesfivestepinthatdirection [2].
• SEQ2SEQ: the best performing model in the R2R
Now that the Q is not empty and the stop criterion is
F
datasetpaper[2].
not met, FAST can choose the best partial trajectory from
• SPEAKER-FOLLOWER [10]: an agent trained with
thefrontierqueueunderthelocalscoringfunction:
data augmentation from a speaker model on the
panoramicactionspace.
τˆ ←argmaxL(Q ) (6)
F
τi • SMNA [13]: an agent trained with a visual-textual
co-grounding module and a progress monitor on the
Following τˆ, we perform the final action proposal, a, to
t panoramicactionspace.9
movetoanewnode(locationinthehouse). FASTcannow
updatethecandidatequeuewiththislocationandthefron- 3.3.OurModel
tier queue with all possible new actions. We then either
continue, byexploitingtheavailableactionsatthenewlo- As our framework provides a flexible design space, we
cation,orbacktrack,dependingonthechoiceofbacktrack reportperformancefortwoversions:
criteria. We repeat this process until the model chooses to • FAST(short)usestheexploitstrategy. Weusethesum
stopandreturnsthebestcandidatetrajectory. of logits fusion method to compute L and terminate
whenthebestlocalactionisstop.
τ∗ ←argmaxG(Q ) (7)
C
τ
8Somebaselinesontheleader-boardarenotyetpublicwhensubmit-
ted;therefore,wecannotcomparewiththemdirectlyonthetrainingand
Algorithm 1 more precisely outlines the full procedure for
validationsets.
ourapproach. §4.3detail
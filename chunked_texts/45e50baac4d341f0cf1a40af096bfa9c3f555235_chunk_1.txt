Understanding the Effect of Model Compression on
Social Bias in Large Language Models
GustavoGonçalves1,2 and EmmaStrubell1,3
1LanguageTechnologiesInstitute,CarnegieMellonUniversity,Pittsburgh,PA,USA
2NOVALINCS,UniversidadeNOVAdeLisboa,Lisbon,Portugal
3AllenInstituteforArtificialIntelligence,Seattle,WA,USA
{ggoncalv, estrubel}@cs.cmu.edu
Abstract prohibitivelyexpensivebothfinanciallyandenvi-
ronmentally (Hessenthaler et al., 2022). At the
LargeLanguageModels(LLMs)trainedwith
same time, the compression of LLMs has been
self-supervision on vast corpora of web text
intensely studied. Pruning, quantization, and dis-
fit to the social biases of that text. Without
tillationareamongthemostcommonstrategiesto
intervention,thesesocialbiasespersistinthe
model’spredictionsindownstreamtasks,lead- compressLLMs. Pruningreducestheparameters
ingtorepresentationalharm. Manystrategies of a trained model by removing redundant con-
havebeenproposedtomitigatetheeffectsof nectionswhilepreservingequivalentperformance
inappropriatesocialbiaseslearnedduringpre- to their original counterparts (Liebenwein et al.,
training. Simultaneously, methodsformodel
2021; Ahia et al., 2021). Quantization reduces
compression have become increasingly pop-
theprecisionofmodelweightsandactivationsto
ular to reduce the computational burden of
improveefficiencywhilepreservingperformance
LLMs. Despite the popularity and need for
(Ahmadianetal.,2023). Finally,knowledgedistil-
bothapproaches,littleworkhasbeendoneto
explore the interplaybetween thesetwo. We lation (Hinton et al., 2015) trains a smaller more
performacarefullycontrolledstudyoftheim- efficientmodelbasedonalargerpre-trainedmodel.
pactofmodelcompressionviaquantizationand While much research has been done on mea-
knowledge distillation on measures of social
suring and mitigating social bias in
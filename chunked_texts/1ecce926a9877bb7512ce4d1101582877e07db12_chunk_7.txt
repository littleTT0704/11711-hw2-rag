odalfusionmodule,and
gregation.Opticalflowreflectingthemotionalsoservesasa decoders.Wefirstleverageavisualandanacousticencoder
strongcueforVSODtaskin(Lietal.2018,2019).Recently, to extract visual {f }T and acoustic features gsem, gloc.
t t=1
Skip Connections
Student
Visual Enc.
Visual Dec.
Video ğ‘‰={ğ¼!â‹¯ğ¼ "} Visual Feature ğ‘“! "% #$ GroundTruth Student Feature ğ‘“!&!â€™ "% #$ Student Prediction ğ‘€!&!â€™ "% #$
(Label-guided)
Multimodal Fusion
â„’â€™(!)($$ â„’!)*+&
Skip Connections GroundTruth
Sem. Head
Acoustic Enc. Sem Embed. ğ‘”!"# Teacher
Visual Dec.
Loc. Head
Multi-channelAudioA= ğ» # #% $! Loc. Embed. ğ‘”$%& Teacher Feature ğ‘“!!() "%
#$
Teacher Prediction ğ‘€!!() "%
#$
Figure2:PipelineOverview.Weuseseparateencoderstoextractmultimodalfeatures.ForavideoclipV = {I Â·Â·Â·I },a
1 T
visualencoderisutilizedtoextractvisualfeature{f }T.ForaudioinputA = {H }4,atwo-brunchacousticencoderis
t t=1 i i=1
employedtoextractthesemanticembeddinggsemandlocationembeddinggloc.Afterthat,alabel-guidedmultimodalfusion
module is introduced to effectively fuse the multimodal features, which outputs a student feature {fstu}T and a teacher
t t=1
feature{ftch}T.Twodecodersareleveragedtodecodethefinalpredictions{Mstu}T and{Mtch}T fromcompacted
t t=1 t t=1 t t=1
features{fstu}T and{ftch}T respectively.Inparticular,toenhancemultimodalcommunication,adistillationlossL is
t t=1 t t=1 d

issetto0.03. Weadoptthecosinelearningrateannealingschemetoadjustthelearningratewith
atotaltrainingstepof220. ThelabeledbatchsizeB issetto64andtheunlabeledbatchsizeB
L U
issetto7timesofB foralldatasets. Wesetmto0.999anddividetheestimatedvarianceσˆ by
L t
4for2σ oftheGaussianfunction. WerecordtheEMAofmodelparametersforevaluationwitha
momentum of 0.999. Each experiment is run with three random seeds on labeled data, where we
reportthetop-1errorrate. Moredetailsonthehyper-parametersareshowninAppendixA.3.1.
Results. SoftMatchobtainsthestate-of-the-artresultsonalmostallsettingsinTable2andTable3,
exceptCIFAR-100with2,500and10,000labelsandSVHNwith1,000labels,wheretheresultsof
SoftMatch are comparable to previous methods. Notably, FlexMatch exhibits a performance drop
compared to FixMatch on SVHN, since it enrolls too many erroneous pseudo-labels at the begin-
ningofthetrainingthatprohibitslearningafterward. Incontrast,SoftMatchsurpassesFixMatchby
1.48% on SVHN with 40 labels, demonstrating its superiority for better utilization of the pseudo-
labels.Onmorerealisticdatasets,CIFAR-100with400labels,STL-10with40labels,andImageNet
with10%labels,SoftMatchexceedsFlexMatchbyamarginof7.73%,2.84%,and1.33%,respec-
tively. SoftMatchshowsthecomparableresultstoFlexMatchonCIFAR-100with2,500and10,000
labels,whereasReMixMatch(Berthelotetal.,2019a)demonstratesthebestresultsduetotheMixup
(Zhangetal.,2017)andRotationloss.
1
AllexperimentsinSection4.1,Section4.2,andSection4.5areconductedwithTorchSSL(Zhangetal.,2021
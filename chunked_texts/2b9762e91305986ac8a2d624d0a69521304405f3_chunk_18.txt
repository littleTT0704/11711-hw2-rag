.5 99.0 30.0 95.5 80.2 5.1 MULTICHECKLIST
zh 94.4 100.0 33.0 100.0 94.5 71.2 82.2
sw 100.0 100.0 94.0 6.0 100.0 98.0 83.0
th 91.4 78.4 100.0 100.0 100.0 41.0 85.1 WeshowtheresultsofXLM-Rfine-tunedonEn-
he 100.0 97.5 100.0 100.0 100.0 27.9 87.6
qu 91.9 100.0 100.0 98.0 95.5 97.0 97.1 glishSQuADv1.1onthe6testsofMULTICHECK-
ht 95.5 100.0 100.0 100.0 100.0 90.9 97.7
ha 100.0 100.0 99.5 100.0 100.0 91.5 98.5 LISTinTable5(seeAppendixFforthefullresults,
yo 100.0 100.0 100.0 100.0 100.0 99.5 99.9
wo 100.0 100.0 100.0 100.0 100.0 100.0 100.0 examplefailurecases,andmBERTresults). While
Avg 47.3 90.9 64.8 26.7 51.6 32.8 52.4
mBERTâ€™s average error rate is greater than 85%
Table 5: Error rate of XLM-R fine-tuned on English on4/6testcategories,XLM-Rdemonstratesasub-
SQuADv1.1on6CHECKLISTQAtests. stantiallymorerobustcross-lingualunderstanding
ability. XLM-R performs worst on tests in low-
resourcelanguageswithlimitedornopre-training
comparedtothezero-shotsetting(Huetal.,2020). datasuchasgu,ha,ht,qu,sw,wo,andyoand
The new tasks are challenging for current mod- inlanguageswithnon-Latinscriptssuchashe,ja,
els,whichshowrelativelylowerperformancecom-
6Duetocomputelimitations,weextractmT5embeddings
paredtoother
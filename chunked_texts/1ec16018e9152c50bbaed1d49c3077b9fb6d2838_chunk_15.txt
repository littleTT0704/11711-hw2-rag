iveusefulnessofexplanationsfor
minimumofconfusionfromatobandvice-versa, variousexplanationmethods. Foreachmethod,scores
toensurethatwordsaremutuallyconfusable: thatarestatisticallysignificantlyhigher(p 0.05)than
≤
the analogous method with a different contrastive set-
(a,b) = min(P(x true = a,x model = b), ting are bolded. Overall, users achieve higher simula-
C
P(x = b,x = a)). tionaccuracywithcontrastiveexplanations.
true model
Accuracy Firstofall,usershavethelowestaccu-
We recruited 10 graduate students in machine racyinpredictingLMoutputswhennoexplanation
learning(notauthorsofthispaper)toperformthe is given, which suggests that all four types of ex-
study. Eachparticipantisgiven10differentword planationshelpuserssimulatemodelbehavior. For
pairs. Foreachwordpair,oneexplanationmethod bothexplanationmethods,thecontrastivesetting
waschosenatrandomtogeneratetheaccompany- leads to a significantly higher contrastive simula-
ing explanations, and the participant is given 40 tionaccuracythanthenon-contrastivesetting.
189
We also examined examples where annotators 500sentencesfromWikiText-103andobtainasen-
incorrectly predict the model output, and for all tence set X. For each foil y and each sentence
f
typesofexplanationsgiven,themosthumanerrors x X,wegenerateasinglecontrastiveexplana-
i
∈
are made in examples where there are no words tione(x,y,y ). Then,foreachtargety andfoil
i t f t
in the input sentence that makes one word more y, we generate an aggregate explanation vector
f
likelythantheother. Notably,thethreewordpairs e(y,y ) = e(x,y,y )byconcatenating
with the lowest user accuracy are “son/brother”,
thet sinf gleexplax ni∈atX ionvei ctot rsf
foreachsentencein
L
�
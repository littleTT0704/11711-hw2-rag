iasmorethananyotherdebi-
indicates % of white users’ tweets being flagged as
asingmethod,specificallyasmeasuredbytheFPR
toxic,AA-Tox.indicates%ofAfricanAmericanusers’
tweets being flagged as toxic, ∆ refers to the differ- on AAE tweets, with one point drop in F 1 score.
TheF scoreonthe“gold”testdata(Table4)are
encebetweenAA-Tox.andW-Tox., andAA/Wrefers 1
totheratiobetweenAA-Tox. andW-Tox. Takeaway: not fully reliable, as test data contain label biases
Methods generally fail in debiasing on this OOD test and better performance could come from exploit-
setexcepttherelabelingapproachshowssomebenefit. ing these biases. As shown in Table 5, the model
trainedonAAE-relabeledhasthelowestracialdis-
parity in toxicity flagging rates compared to all
Focusing on dialectal bias, our key assumption
othermethods.
is that an AAE tweet and its corresponding WAE
These results highlight that debiasing meth-
versionshouldhavethesametoxicitylabel,there-
ods are much less effective at mitigating dialec-
fore toxic AAE tweets whose WAE versions are
tal dataset biases compared to data relabeling.
non-toxicarecandidatesforlabelcorrection.14
For future investigations, we recommend obtain-
However, gold-standard translations of AAE to ing human-written AAE-WAE pairs (e.g., as done
WAE would require qualified translators, and au- by Groenwold et al., 2020). Additionally, to en-
tomatic AAE-to-WAE translation systems do not
sure less biased toxicity labeling, we recommend
exist, to the best of our knowledge. Therefore,
recruiting AAE speakers or experts for avoid-
we create a proof-of-concept study—we set up a
ing over-identification of AAE-markers as toxic
AAE to WAE “translation” system using the few- (Spears, 1998; Croom, 2013). Alternatively, we
shot capabilities of the GPT-3 language model
recommend exploring more
 20.572 18.782
Table6: PerformancecomparisonofkNNbaselinesandseveralconfigurationsthatadaptivelyincreasethe
embeddingsizewithtraininglossorwordfrequency.
C.2 MixtureofSoftmaxes
Yangetal.(2017)proposesasolutiontotheproblemusingaMixtureofSoftmax(MoS)toproducemore
linearly independent probability distributions of words given different contexts. Suppose that there are a
totalofRmixturecomponents. MoSfirstusesRlinearlayerswithweightw totransformthecurrentquery
r
contextvectorh intow h. WithasharedwordembeddingmatrixW,wecancalculateeachsoftmax
ds r ds sm
component’sprobabilitydistributionwithsoftmax(W ·w h ). Themixturedistributionisthengivenby:
sm r ds
R
(cid:88)
P = π softmax(W ·w h ) (7)
MoS r,hds sm r ds
r
The prior weights are calculated using another linear layer with weight w, as π = softmax(w h ).
π r,hds π ds
The softmax ensures that
(cid:80)Rπ
= 1. Comparing the MoS with the first term in Equation 5,
r r,hds
Msoftmax(mask-to-k(W ⊗h )), we can see that there are some connections between the two. MoS
ds ds
eliminates the mask-to-k(·) operation, and replaces the single softmax across a very large vector (size of
datastore),intomultiplesmallersoftmaxes,eachacrossonlyavectorofthesizeofvocabulary. Asaresult,
thehugeW isreplacedbyseverallinearlayerstoprojectthewordembeddingmatrix. Nowthefirstterm
ds
becomes:
M(⊕Rsoftmax(W ·w h )) (8)
r sm r ds
M =π,∀i≤V (9)
ir r,hds
where⊕representsthevectorconcatenationoperation,andtheaggregationmatrixM nowcontainsthemixture
weights
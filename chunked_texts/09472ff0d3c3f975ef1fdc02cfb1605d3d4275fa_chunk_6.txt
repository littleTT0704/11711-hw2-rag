(·,·) denotes a generic loss function. RSC requires
one extra scalar hyperparameter: the percentage of the representations to be
discarded, denoted as p. Further, we use · to denote the estimated quantities,
(cid:98)
use˜· to denote the quantities after the representations are discarded, and use
t in the subscript to index the iteration. For example, θ(cid:98)t means the estimated
parameter at iteration t.
3.1 Self-Challenging Algorithm
As a generic deep learning training method, RSC solves the same standard loss
function as the ones used by many other neural networks, i.e.,
(cid:88)
θ(cid:98)=argmin l(f(x;θ),y),
θ
(cid:104)x,y(cid:105)∼(cid:104)X,Y(cid:105)
Self-Challenging Improves Cross-Domain Generalization 5
but RSC solves it in a different manner.
At each iteration, RSC inspects the gradient, identifies and then mutes the
most predictive subset of the representation z (by setting the corresponding
values to zero), and finally updates the entire model.
This simple heuristic has three steps (for simplicity, we drop the indices of
samples and assume the batch size is 1 in the following equations):
1. Locate: RSC first calculates the gradient of upper layers with respect to the
representation as follows:
g
z
=∂(h(z;θ(cid:98) ttop)(cid:12)y)/∂z, (1)
where (cid:12) denotes an element-wise product. Then RSC computes the (100−
p)th percentile, denoted as q. Then it constructs a masking vector m in the
p
same dimension of g as follows. For the ith element:
(cid:40)
0, if g (i)≥q
m(i)= z p (2)
1, otherwise
In other words, RSC creates a masking vector m, whose element is set to 0
if the corresponding element in g is one of the top p percentage elements in
g, and set to 1
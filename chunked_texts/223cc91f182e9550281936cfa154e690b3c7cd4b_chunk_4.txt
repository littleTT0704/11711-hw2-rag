o, 2018) makes use of
feedback obtained via interaction with the environment. Knowledge-constrained learning like posterior
regularization (Ganchev et al., 2010; Zhu et al., 2014) incorporates structures, knowledge, and rules expressed
as constraints. Generative adversarial learning(Goodfellow et al., 2014) leverages a companion model called
discriminator to guide training of the model of interest.
In light of these results, we present a standard equation (SE) of the objective function. The SE formulates a
rather broad design space of learning algorithms. We show that many of the well-known algorithms of the
above paradigms are all instantiations of the general formulation. More concretely, the SE, based on the
maximum entropy and variational principles, consists of three principled terms, including the experience term
that offers a unified language to express arbitrary relevant information to supervise the learning, the divergence
term that measures the fitness of the target model to be learned, and the uncertainty term that regularizes the
complexity of the system. The single succinct formula re-derives the objective functions of a large diversity of
learning algorithms, reducing them to different choices of the components. The formulation thus sheds new
light on the fundamental relationships between the diverse algorithms that were each originally designed to
deal with a specific type of experience.
The modularity and generality of the framework is particularly appealing not only from the theoretical point of
view, but also because it offers guiding principles for designing algorithmic approaches to new problems in a
mechanical way. Specifically, the SE by its nature allows combining together all different experience to learn a
model of interest. Designing a problem solution boils down to choosing what experience to use depending on
the problem structure and available resources, without worrying too much about how to use the experience in
the training. Besides, the standardized ML perspective also highlights that many learning problems in different
research areas are essentially the same and just correspond to different specifications of the SE components.
This enables us to systematically repurpose successful techniques in one area to solve problems in another.
The remainder of the article is organized as follows. Section 2 gives an overview of relevant learning and
inference techniques as a prelude of the standardized framework. Section 3 presents the standard equation as a
general formulation of the objective function in learning algorithms. The subsequent two sections discuss
different choices of two of the key components in the standard equation, respectively
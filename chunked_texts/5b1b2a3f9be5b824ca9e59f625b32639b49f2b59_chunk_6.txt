61] proposes a ranking attention module to filter our
tracking method, and its corresponding mathematical
irrelevant features based on the pixel-level similarity obtained
explanation.
from an encoder-decoder network. Space-Time-Memory net-
â€¢ We are the first to investigate audio effects on the video works (STM) [46] introduces a new paradigm that builds
dense prediction task and contribute a corresponding
a memory bank for each object in the video and segments
dataset with synchronized audio-visual signals.
following objects by matching them to the memory bank.
A preliminary version of our method has been published in More recently, referring video object segmentation (R-VOS)
[30].Inthismanuscript,wemakethefollowingimprovements. emerges and attracts more and more attention because of its
3
Optional audio branch
Legend:
R Reshape
Audio
backbone âŠ• Concatenate
Audio ğ´" "! #!$% Audio featureğ‘“!+,- âŠ—Dot Product
Fused token
Instance code
Backbone ğ’ªâ€²=ğ’ªâ€™!(!â¨ ğ’ªâ€²")*
generation
PandaEmptyPandaâ‹¯
Reference feature
â¨ğ’ªâ€²+,- (O Inr sd tae nr- cp er ce ose dr ev ğ‘’in!g) Instance classesğ¶!
Reference frames ğ¼" "!$ #&!$% ğ‘“!")* Context fusion
ğ’ªâ€²!(!
R
Mask
Backbone
decoder
Target featureğ‘“! Fused featureğ‘“!â€™
Target frameğ¼! Instance masksğ‘€!
Segmentation mapğ‘†!
Fig.2. Overview of the proposed network at timestamp t.ForeachtargetframeIt,weconsiderreferenceframes{Ir}t râˆ’ =1
tâˆ’Î´
ascontextforpredicting
theinstancemasksMt andclassesCt.Bothtargetfeatureft andreferencefeaturef tref areextractedbyasharedbackboneandfusedinthecontextfusion
module.ThemaskdecoderdecodesthefusedtargettokenO t(cid:48)
gt
int
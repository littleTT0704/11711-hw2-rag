61] proposes a ranking attention module to filter our
tracking method, and its corresponding mathematical
irrelevant features based on the pixel-level similarity obtained
explanation.
from an encoder-decoder network. Space-Time-Memory net-
• We are the first to investigate audio effects on the video works (STM) [46] introduces a new paradigm that builds
dense prediction task and contribute a corresponding
a memory bank for each object in the video and segments
dataset with synchronized audio-visual signals.
following objects by matching them to the memory bank.
A preliminary version of our method has been published in More recently, referring video object segmentation (R-VOS)
[30].Inthismanuscript,wemakethefollowingimprovements. emerges and attracts more and more attention because of its
3
Optional audio branch
Legend:
R Reshape
Audio
backbone ⊕ Concatenate
Audio 𝐴" "! #!$% Audio feature𝑓!+,- ⊗Dot Product
Fused token
Instance code
Backbone 𝒪′=𝒪’!(!⨁ 𝒪′")*
generation
PandaEmptyPanda⋯
Reference feature
⨁𝒪′+,- (O Inr sd tae nr- cp er ce ose dr ev 𝑒in!g) Instance classes𝐶!
Reference frames 𝐼" "!$ #&!$% 𝑓!")* Context fusion
𝒪′!(!
R
Mask
Backbone
decoder
Target feature𝑓! Fused feature𝑓!’
Target frame𝐼! Instance masks𝑀!
Segmentation map𝑆!
Fig.2. Overview of the proposed network at timestamp t.ForeachtargetframeIt,weconsiderreferenceframes{Ir}t r− =1
t−δ
ascontextforpredicting
theinstancemasksMt andclassesCt.Bothtargetfeatureft andreferencefeaturef tref areextractedbyasharedbackboneandfusedinthecontextfusion
module.ThemaskdecoderdecodesthefusedtargettokenO t(cid:48)
gt
int
imilarity
(howlikelythetwonarratorswouldempathizewith WeproposefinetuningLLMstolearnembeddings
each other), (2) similarity in the main events, (3) thatcaptureempathicsimilarityusingcosinedis-
emotions,and(4)moralsofthestories. tance,forefficientretrievalattesttime. Incontrast,
apopularapproachistousefew-shotpromptingof
AgreementWeaggregateannotationsbyaveraging
verylargelanguagemodels(e.g.,GPT-3andChat-
betweenthe2raters. Agreementscoresforempa-
GPT),whichhaveshownimpressiveperformance
3Bycomparingthecosinesimilarityofhumanannotated acrossavarietyoftasks(Brownetal.,2020). How-
event,emotion,andmoraltotheChatGPTsummarizedsto- ever,inarealdeploymentsetting,retrievalthrough
ries,wefindthatthereishighsemanticoverlapofthehuman
promptingeverypossiblepairofstoriesisexpen-
ground-truthstotheautomaticallygeneratedsummaries(0.66
forevent,0.64foremotion,and0.49formoral). siveandinefficient.
Model r ρ Acc P R F1 P τ ρ
k=1 rank rank
SBERT 30.93 29.86 62.75 57.81 90.24 70.48 57.92 17.46 18.74
+finetuning 35.93 35.21 64.75 58.68 90.73 71.26 57.43 17.59 18.98
BART 10.24 11.54 57.00 52.19 99.02 68.35 49.51 7.56 9.28
+finetuning 34.20 34.43 64.75 58.2 88.29 70.16 65.84 24.68 26.55
GPT-3 3.24 2.79 51.25 51.25 100 67.77 90.59 0.33 0.79
+5examples 4.94 6.71 51.25 51.27 98.54 67.45 72.77 -4.8 -5.33
ChatG
,proceduralgeneration,generalistagents, theselessonstopre-traininglargermodelswithlarger
andmulti-agentandhumaninteraction. datasets.
Pre-training also provides a way to specify di-
5.1.Pre-training
versetasksforagentseasily. Open-worldagentsmust
Pre-training has powered impressive results from be able to flexibly complete tasks with unseen goals
visual recognition [69], natural language [53, 148], or task specifications. Prior work shows that pre-
and audio [132]. Pre-trained models can be repur- trainedmodelscanprovidedenserewardsupervision
posed through fine-tuning, zero-shot generalization, [36,46,174]. Otherworkshowsthatpre-trainedmod-
or prompting to perform diverse tasks. However, els can be leveraged for open-world object detection,
pre-training has not yet found such levels of success allowing for zero-shot generalization to new goals in
in embodied AI. Recent work has begun to explore navigation tasks [5, 63, 112]. Finally, some methods
this direction, showing that pre-trained models can explore generalization to new language instructions
helpimproveperformance,efficiencyandexpandthe byemployingpre-trainedmodels[176]. Therearefur-
scope of solvable tasks. This section discusses how ther opportunities to use such models for zero-shot
pre-training can help embodied AI with visual pre- generalizationtocompletingnewtasks,newgoals,or
training objectives, the role of scale in pre-training, flexiblyspecifyinggoalsindifferentinputmodalities.
pre-trainingfortaskspecification,andpre-trainedbe- Finally,pre-trainingcanlearnbehavioralpriorsfor
havioralpriors. interaction. Thepreviouslydiscussedpre-trainingob-
One promising area is new pre-training objectives jectives primarily focus on learning representations
forvisualrepresentationsinembodiedAI.Priorwork of input modalities. However, this leaves out a crit-
19
ical part of embodied AI, interacting with the envi- cally,(3)manydetailsencodedinastateareirrelevant
ronment. Rather than pre-training representations, to task completion (e.g. minor color or texture varia-
pre-training can also learn models of behavior that tions of
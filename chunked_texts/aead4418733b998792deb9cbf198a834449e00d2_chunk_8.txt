 balance diversity and fitness, by clustering can-
didates and selecting the highest-fitness members of each
Table 3: Robustness examples. We show the model’s top
cluster.SAGGAcontinuesuntilthearchivecontainsatarget prediction(beamsearch,size10).Notethat(x44+x)/x =
numberofproblems.Algorithm1summarizesSAGGA. x43+1;itsderivativeis43x42andishenceincorrect.
SAGGAofferscontroloverthetypesofproblemsthatit
discovers through its seed problems, fitness function, and
mutation strategy. We detail our choices for each kind of 3.1 ManuallyTestingRobustness
generalizationintheirrespectivesections,andshowdefault To define nearby problems, we first consider manual tem-
settingsandfurtherimplementationdetailsintheAppendix. plateswhichminimallyperturbaproblemf,e.g.
3 RobustorBrittle? k·f, f +lnx,...
First, we study whether the model’s strong test-set perfor- These problems are nearby f in the sense that a single op-
manceadequatelyrepresentsitsrobustness.Robustnesstells eration is added to the problem’s equation tree, or a small
us whether the integration model systematically solves all numberofnodevaluesarechangedinthetree.
problems in a neighborhood governed by a generalizable
pattern; for instance a model that solves R 26x42 should Brittlenessonsimpleprimitivefunctions. Wefirstinves-
solve R 53x42. We study problems that are nearby to those tigate whether the neural sequence integrator is robust on
simple primitive functions, since they make up more com-
fromtheoriginaltestdistribution,aswellastosimpleprim-
plicated functions and are frequently entered by real-world
itivefunctionsthatofferfine-grained,interpretablecontrol.
users.Weuseamanualneighborhoodwhichyields,
Arobustmodelisstabletosmallperturbationsininput,
meaningthatitgetsnearbyproblemsx˜correctwhenitgetsa X ={k ln(k x), k exp(k x), k x, k x42,
N
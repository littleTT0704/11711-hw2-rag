NLP2020,pages6058–6069.
Jonathan H Clark, Dan Garrette, Iulia Turc, and John
Daniel Gillick, Alessandro Presta, and Gaurav Singh
Wieting. 2021. CANINE: Pre-training an Efficient
Tomar. 2018. End-to-end retrieval in continuous
Tokenization-Free Encoder for Language Represen-
tation. arXivpreprintarXiv:2103.06874. space. arXivpreprintarXiv:1811.08008.
AlexisConneau, KartikayKhandelwal, NamanGoyal, KristinaGulordava,PiotrBojanowski,EdouardGrave,
Vishrav Chaudhary, Guillaume Wenzek, Francisco Tal Linzen, and Marco Baroni. 2018. Colorless
Guzmán, Edouard Grave, Myle Ott, Luke Zettle- green recurrent networks dream hierarchically. In
moyer, and Veselin Stoyanov. 2020. Unsupervised ProceedingsofNAACL-HLT2018.
Cross-lingual Representation Learning at Scale. In
ProceedingsofACL2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Weizhu Chen. 2021. DeBERTa: Decoding-
Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad- enhanced BERT with Disentangled Attention. In
ina Williams, Samuel Bowman, Holger Schwenk, ProceedingsofICLR2021.
and Veselin Stoyanov. 2018. XNLI: Evaluating
cross-lingual sentence representations. In Proceed- MichaelA.Hedderich,DavidAdelani,DaweiZhu,Je-
ingsofEMNLP2018,pages2475–2485. sujoba Alabi, Udia Markus, and Dietrich Klakow.
2020. Transfer Learning and Distant Supervision
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and for Multilingual Transformer Models: A Study on
Kristina Toutanova. 2019. BERT: Pre-training of African Languages. In Proceedings of EMNLP
deep bidirectional transformers for language under- 2020.
standing. In
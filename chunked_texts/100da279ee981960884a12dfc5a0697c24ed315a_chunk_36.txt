Fofmax(p)asauniformwithin
arange[τ,1]stilldoesnotreflecttheactuallydistributionoverconfidenceduringtraining.
A.1.5 SOFTMATCH
In this paper, we propose SoftMatch to overcome the trade-off between quantity and quality of
pseudo-labels.Differentfrompreviousmethods,whichimplicitlymakeover-simplifiedassumptions
onthedistributionofp,wedirectlymodellingthePMFofmax(p),fromwhichwederivethesample
weightingfunctionλ(p)usedinSoftMatch.
Weassumetheconfidenceofmodelpredictionsmax(p)generallyfollowstheGaussiandistribution
(max(p);µˆ,σˆ )whenmax(p)<µ andtheuniformdistributionwhenmax(p) µ.Notethat
t t t t
N ≥
µ andσ ischangingalongtrainingasthemodellearnsbetter. Onecanseethattheuniformpartof
t t
thePMFissimilartothatofconfidencethresholding,anditistheGaussianpartmakesSoftMatch
distinct from previous methods. In SoftMatch, we directly estimate the Gaussian parameters on
max(p)usingMaximumLikelihoodEstimation(MLE),ratherthansetthemtofixedvalues,which
ismoreconsistenttotheactualdistributionofpredictionconfidence. UsingthedefinitionofPMF
λ¯(p),wecandirectlywritethesamplingweightingfunctionλ(p)ofSoftMatchas:
(cid:26) λ √2πσ φ(max(p;µ,σ )), max(p)<µ
λ(p)= max t t t t, (26)
λ, max(p) µ
max t
≥
where φ(x;µ,σ) = √ 21
πσ
exp( −(x 2− σµ 2)2 ). Without loss of generality, we can assume max(p i) <
µ tfori ∈[0,N 2U],asµ
t
= N1
U
(cid:80)N
i
U max(p i)(shown
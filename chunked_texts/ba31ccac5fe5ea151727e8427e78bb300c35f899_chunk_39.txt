
ginasintheNERtask. Forrelations,wefindthat
sub-structure unit. In this case, the query and
averaginguncertaintiesoverallmentionpairshas
selection for DPAR are almost identical to the
a bias towards sentences with fewer mentions.
NER task because of this token-wise decision
Tomitigatesuchbias, wefirstaggregateanun-
scheme. Therefore, the same AL strategies in
certainty score for each mention by taking the
NERcanbeadoptedhere.
maximumscorewithinalltherelationsthatlink
toitandthenaveragingoverallthementionsfor
• Annotation. InDPAR,therearenospecialspan-
sentence-levelscores. Finally,thescoresofthe
based annotations as in NER; thus, we simply
twosub-tasksarelinearlycombinedtoformthe
annotateinaword-basedscheme.
sentence-leveluncertainty.
• Modellearning. SimilartoNER,weadoptthe
• Partialselection. ForPAselection,thetwosub-
log-likelihoodofthegoldparsetreeasthetrain-
tasks are handled separately according to the
inglossinFAandmarginalizedlikelihoodinPA
adaptive ratio scheme. We further adopt two
(Lietal.,2016).
heuristics for the relational task to compensate
forerrorsinthementionextraction. First,since
A.2 IE there can be over-predicted mentions that lead
to discarded relation queries, we adjust the PA
• Tasks. Wetackleeventextraction(EE)andrela-
ratio by estimating how many candidate rela-
tionextraction(RE)usingatwo-steppipelined
tions contain such errors in the mentions. We
approach. The first step aims to extract entity
again train a logistic regression model to pre-
mentionsforRE,andentitymentionsandevent
dict whether a token is NIL (or ‘O’ in the BIO
triggersforEE.Weadoptsequencelabelingfor
scheme,meaningnotcontainedinsideanygold
mentionextractionsasintheNERtask. Basedon
mentions)basedonitsNILprobability. Thenfor
themention
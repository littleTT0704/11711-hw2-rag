 The last problem is to stabilize the notoriously
difficult training of generative adversarial networks (GANs) for a wide range of image and text generation
tasks.
The three problems, though seemingly unrelated at first sight, can all be reduced to the same underlying
problem in the unified SE view, namely, learning with imperfect experience f (e.g., underspecified knowledge
constraints, small imbalanced data, and unstable discriminator). We want to automatically adapt/improve the
imperfect experience in order to better supervise the target model training. This readily falls into the dynamic
SE setting described in Section 6, where now the experience function is f (t) associated with learnable
ϕ
parameters ϕ. For example, in problem (1), f (t) = ∑ λifi (t), where any learnable components in
ϕ i ϕ rule,ϕ
each knowledge constraint fi (Section 4.2) and the weights λi constitute the ϕ to be learned (Hu et al.,
rule,ϕ ϕ
47
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
2018). In problem (2), f (t) is instantiated as f (t;D) (Equation 4.6) with learnable data weights
ϕ data-w,ϕ
w(t∗) ∈ ϕ, or as f data-aug,ϕ(t;D) (Equation 4.6) with the metric for augmentation a t∗(t) ∈ ϕ to be
learned (Hu, Tan, et al., 2019). In problem (3), we have discussed the training of f (t) as the GAN
ϕ
discriminator in Section 6.1, but we want to improve the training stability (Wu et al., 2020). Thus, one
approach for efficient updates of the general experience function f would address all three problems together.
ϕ
To seek for solutions, we again take advantage of the unified SE perspective that enables us to reuse existing
successful techniques instead of having to invent new ones. In particular, the connection of the experience
function f with the reward in Section 4.3 naturally inspires us to repurpose known techniques from the fertile
reinforcement learning (RL) literature, especially those of learning reward
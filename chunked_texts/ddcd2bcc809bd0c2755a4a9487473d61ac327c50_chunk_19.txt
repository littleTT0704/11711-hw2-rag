on
aspecificaspectofToMandasmallnumberofex-
amples(40forKosinski(2023)and10forBubeck
etal.(2023)). FollowingUllman(2023),weempiri-
callyshowedthateventhebestmodelsfailonsmall
variationsoftheoriginaltasks, provingthateven
GPT-4doesnotdisplayrobustN-ToMabilities.
Figure5: SocialIQa’saccuraciesforthequestionsthat
focusonthemaincharactervs. others. WhileGPT-4
CleverHans,Heuristics&Shortcuts Theper-
(the best-performing model) achieves a total of 0.79
formance gaps between different question types
accuracyscore,itachievesonly0.61onthesubsetques-
tionsof“otherseffect”. suggests that LLMs rely on shortcuts, heuris-
tics,andspuriouscorrelations,whichoftenlead
themastray. InAdv-CSFB(§5.2),thebadperfor-
Here we show this recurring phenomenon across
manceonsomeoftheadversarialcategoriesmight
twoToMdatasets,inspiredbytheanalysesinSap
bepartlyattributedtoreportingbias(Gordonand
etal.(2022).
VanDurme,2013;ShwartzandChoi,2020). Peo-
Figure4describesToMiaccuraciesondifferent
ple don’t share obvious facts (Grice, 1975), so it
questiontypes;ToMicontainsquestionsaboutfacts
islikelythatLLMsarebiasedtowardsgenerating
vs. beliefs (mind), and specifically about true or
surprising rather than unsurprising continuations.
falsebeliefs. WhileGPT-3.5(thebest-performing
Inmostofthesecategories,theprotagonistbelief
model)achieves81%accuracy,onthesubsetques-
isthesameasthetruth,makingaboringstory.
tions“falsebelief”,itachievesonly46%,closeto
Furthermore, the newer models such as GPT-
randomperformance.
3.5 and GPT-4 are trained in addition to the LM
Figure 5
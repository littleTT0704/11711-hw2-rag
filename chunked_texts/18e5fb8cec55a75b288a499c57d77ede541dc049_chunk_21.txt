adversarialstrategies.5
improvebyarchitecturalimprovementsoftheLMs,knowl-
edge sources with wider coverage and stronger semantics, Conclusions
and well-tuned scoring functions.4 In addition, despite its
Whilezero-shotQAisgainingfocus,nostudysofarhasin-
complexity and diversity, commonsense knowledge (unlike
vestigatedsystematicallythedependencybetweenthetask,
knowledgeonentitiesandevents,whichchangesrapidly)is
the technique, and any additional knowledge used. To ad-
largelystaticandevolvesslowlyovertime,thusmakingthe
dress this gap, this paper proposed a framework for zero-
dataset-specific finetuning unnecessary in theory. A natural
shotQAbypretrainingLMsonartificialdatacreatedbased
questionarises:canwebuildasufficientlyreliable,general
on KGs. We studied the interplay between five knowledge
commonsenseservice,bypretrainingaLMonarichsetof
sources,fourdatagenerationstrategies,twoLMs,twotrain-
questionscoveringawidespectrumofknowledgetypes?
ingregimes,andfivetasks.Weputforwardsevenhypothe-
ses, which guided our experiments. We observed that lan-
ImpactofKnowledge guagemodelsbenefitfrompretrainingonartificiallycreated
In general, we observed that using knowledge from a QAsets.Similartopriorwork,weobservedthatthebestper-
wider set of sources is beneficial. However, on aNLI and formanceisobtainedbytheRoBERTamodel.Furthermore,
CommonSenseQA, the best accuracy was obtained with combiningknowledgesourcesleadstobestperformance,in-
a subset of all questions. This could be due to the dicating that diverse relevant knowledge is desired for pre-
kinds of knowledge covered: 1. aNLI focuses on ex- training. However, this is conditioned on a good alignment
pectations of agents and events, for which ATOMIC is between knowledge and the task. The training regime had
directly useful, whereas the other KGs might be delu- a role too: marginal ranking is superior to vanilla language
sive; 2.
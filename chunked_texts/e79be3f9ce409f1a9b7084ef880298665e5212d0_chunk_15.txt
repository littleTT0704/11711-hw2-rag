numberofself-attentionlayersusedinthemodel. In[60],
videoclipsannotatedwithtextdescriptionsbyhumananno-
the authors use 12 multi-modal self-attention layers while
tators. Following[35,34],wetrainourmodelsonthetrain-
6videoencoderlayersand2multi-modalfusionlayersare
ingsplit,andreportthetext-videoretrievalperformanceon
usedin[33].Differently,4multi-modalself-attentionlayers
around3.5kvalidationclips.
are used in [14]. In this paper, for all our ablation studies
•MSR-VTT[52]contains10kvideoclipsassociatedwith
below, we use 1 and 2 self-attention layers for our video
200ksentences. Therearetwovalidationsplitsusedinpre-
encoderandmulti-modalfusion, respectively. Tocompare
viouswork.In[31,14],thetrainingsethas9kclip-textpairs
with previous work on specific dataset, we use 2 video
withtheremaining1kpairsforevaluation,whichwedenote
encoding layers. While pretraining the model with large-
bysplit1.In[53,35,34],1kclip-textpairsaresampledfrom
scaledatasetHowto100M[35], weincreaseto4videoen-
the3kpairsintestsetforevaluation,whiletheoriginal7k
coding layers for comparable model capacity to previous
pairs are used for training. We denote this by split2. We
works [60, 33, 14]. Note that this largest model is still
reporttext-videoretrievalresultsusingbothsplits.
smallerthanoronparwiththeaforementionedmethods.
•ActivityNet[25].Itconsistsof20KYouTubevideos,each
ofwhichisassociatedwithmultiplehuman-annotatedcap- 4.3.Implementationdetails
tions. Following [56, 14], we concatenate all the captions
ForYouCook2andMSR-VTT,themaximumnumberof
for a video into a paragraph and evaluate the paragraph-
videoandtexttok
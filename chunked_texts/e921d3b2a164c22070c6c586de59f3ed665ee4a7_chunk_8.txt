carriesaregressorr θ l∗ k suchthattheprediction
plexlydistributedmulti-modaldataisneverthelesslocally y (cid:98)lk = r θ l∗ k(x). Then, an estimate of y is given by the
(andnon-linearly)separable,andhencethenon-leafexperts expectationofthepredictionsoverallleaves
ofthetreefunctionasaroutingmechanismtopartitionthe
(cid:88)
dataintosubsetsofsimple(uni-modal)distributions,and y (cid:98)= r θ l∗ k(x)q(z lk |x), (4)
routeeachsubsettoasimpleleafexperttomakepredictions. lk∈leaves
Wedenotetheinputfeaturesasx∈Rdandthecontinuous andthecorrespondingconditionaldensityforleafl is
k
output label as y ∈ R. In order to route the data in such
fashion, it requires to determine the optimal classifier at p(y|z lk,x)←r θ l∗ k(x). (5)
HierarchicalRoutingMixtureofExperts
3.2.LearningAlgorithm
Input: [data],[root]
Parameter:{t},classifierparameters,regressor
Fromtheprevioussection,wehaveshownthatinorderto
parameters
makepredictionsusingthetree,weneedtodeterminethe
Output: HRMETree
optimaltreesettings,i.e.,thetreestructure{n },thenon-
i
FunctionGrowTree(data list,nodes per level)
leafnodethresholds{t },theclassifierparameters{β },
ni ni
fornodeinnodes per leveldo
andtheleafnoderegressorparameters{θ }.
ni D←data list
Weadoptamaximum-likelihoodapproach. Specifically,our node l,node r←GrowSubtree(node)
objectiveistomaximizethelog-likelihoodforeachx fortdo
D,D ←SplitData(D,t)
maxlogp(y|x)
, MikeLewis,HannanehHajishirzi,andLukeZettle-
Toronto,Canada.AssociationforComputationalLin- moyer.2022. Rethinkingtheroleofdemonstrations:
guistics. Whatmakesin-contextlearningwork? InProceed-
ingsofthe2022ConferenceonEmpiricalMethodsin
ArnavGudibande,EricWallace,CharlesBurtonSnell, NaturalLanguageProcessing,pages11048–11064,
XinyangGeng,HaoLiu,P.Abbeel,SergeyLevine, AbuDhabi,UnitedArabEmirates.Associationfor
andDawnSong.2023. Thefalsepromiseofimitating ComputationalLinguistics.
proprietaryllms. ArXiv,abs/2305.15717.
GrahamNeubigandZhiweiHe.2023. ZenoGPTMa-
XingweiHe, Zheng-WenLin, YeyunGong, AlexJin, chineTranslationReport.
HangZhang,ChenLin,JianJiao,SiuMingYiu,Nan
Duan,andWeizhuChen.2023. Annollm: Making Andrei Paleyes, Raoul-Gabriel Urma, and Neil D
large language models to be better crowdsourced Lawrence.2022. Challengesindeployingmachine
annotators. ArXiv,abs/2303.16854. learning: asurveyofcasestudies. ACMComputing
Surveys,55(6):1–29.
FelixHill, AntoineBordes, SumitChopra, andJason
Weston. 2016. The goldilocks principle: Reading
GunhoPark,BaeseongPark,MinsubKim,SungjaeLee,
children’s books with explicit memory representa-
JeonghoonKim,BeomseokKwon,SeJungKwon,
tions.
ByeongwookKim,YoungjooLee,andDongsooLee.
2022. Lut-gemm: Quantizedmatrixmultiplication
Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra
based on luts for efficient inference in large-scale
Molina,AaronDons
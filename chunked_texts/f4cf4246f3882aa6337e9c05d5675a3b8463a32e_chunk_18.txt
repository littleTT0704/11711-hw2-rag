 and question answering tasks [50].
quence. Theagentistestedbyfirstforcingittofollowthe
These performance gaps were due to structural biases in
expertdemonstrationtomaintainahistoryofstatesleading
the datasets or issues with model capacity. We evaluate
uptothesub-goal,thenrequiringittocompletethesub-goal
theseablationbaselines(NO LANGUAGEandNO VISION)
conditionedontheentirelanguagedirectiveandcurrentvi-
tostudyvisionandlanguagebiasinALFRED.
sualobservation. Forthetask“Putahotpotatosliceonthe
TheunimodalablationperformancesinTable3indicate
counter”forexample,wecanevaluatethesub-goalofnav-
that both vision and language modalities are necessary to
igating to the potato after using the expert demonstration
accomplish the tasks in ALFRED. The NO LANGUAGE
tonavigatetoandpickupaknife. Thetasksin ALFRED
modelfinishessomegoal-conditionsbyinteractingwithfa-
containonaverage7.5suchsub-goals(resultsinTable4).
miliarobjectsseenduringtraining. TheNO VISIONmodel
similarly finishes some goal-conditions by following low-
6.Analysis
level language instructions for navigation and memorizing
interactionmasksforcommonobjectslikemicrowavesthat
Results from our experiments are presented in Table 3.
arecenteredinthevisualframe.
We find that the initial model, without spatial or seman-
tic maps, object segmentations, or explicit object-state
6.3.ModelAblations
tracking, performs poorly on ALFRED’s long-horizon
tasks with high-dimensional state-spaces. The SEQ2SEQ We additionally ablate the amount of language super-
model achieves ∼8% goal-condition success rate, show- vision available to the model, as language directives are
ing that the agent does learn to partially complete some given as both a high-level goal and step-by-step instruc-
tasks. This headroom (as compared with humans) mo- tions. Providing only high-level, underspecified goal lan-

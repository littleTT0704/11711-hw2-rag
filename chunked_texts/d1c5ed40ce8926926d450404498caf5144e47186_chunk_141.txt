 sources were first intended for the Jeopardy! challenge, but these
results indicate that they are equally effective for TREC questions. This is not sur-
prising because apart from the popularity-based seed selection procedure we did not
make any design choices based on Jeopardy! questions, and we have confirmed that
the seed selection approach is also effective for TREC. Thus our source expansion
method can be applied to build text corpora that are general enough to be leveraged
for different QA tasks without adaptation. On the other hand, the strongest baseline
in Table 6.16 consists of information sources that were chosen specifically because
they improved performance on Jeopardy! and TREC data. This manual source ac-
quisition approach is more likely to overfit since useful content was selected based
102 CHAPTER 6. APPLICATION TO QUESTION ANSWERING
Wikipedia Wiktionary All Sources
No Expansion 77.12% 19.87% 84.12%
(Threshold) (0.5020) (0.1720) (0.6024)
Expansion 83.96% 38.11% 87.74%
(Threshold) (0.5553) (0.2426) (0.6552)
Table 6.17: Precision if Watson answers 70% of all regular Jeopardy! questions using
sources that were expanded with web search results. We also show for each setup the
confidence threshold that determined whether Watson would attempt to answer.
on the distribution of the questions and topics in these QA tasks, and the selected
sources even have perfect coverage for the TREC test set.
In Section 2.2 we described a related approach for leveraging web data to improve
QA performance. Clarke et al. [2002] used raw web crawls as sources for question
answering and evaluated the impact on the performance of their QA system on the
TREC 10 dataset. The web crawler started with web sites of educational institutions
and retrieved linked web pages in breadth-first order. The retrieved documents were
of relatively high quality, and duplicates were removed from the collection. In these
experiments, about 75 GB of web data were needed to match the performance of the
3 GB reference corpus used in the TREC evaluation (a collection of about 1 million
newspaper articles). In comparison, we use a much larger document collection as a
baseline, which
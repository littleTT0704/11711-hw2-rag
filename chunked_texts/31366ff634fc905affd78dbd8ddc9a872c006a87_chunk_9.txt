andthecandidate, CodeBERTScore We use the similarity matrix
resulting in x·y∗ and x·yˆ. We use the tokenizer (seeFigure2),formedbythesimilarityscoresbe-
T providedwiththemodelB: tweeny∗ andyˆ, tocomputeprecision, recall, and
B
F, by taking the maximum across the rows and
1
columns of the similarity matrix, and then aver-
T (x·y∗) = ⟨x,...,x,y∗,...,y∗ ⟩
B 1 k 1 m (1) aging. Following Banerjee and Lavie (2005), we
T (x·yˆ) = ⟨x,...,x,yˆ,...,yˆ ⟩
B 1 k 1 n also compute F by giving more weight to recall,
3
asshowninFigure3. Additionaldetailsregarding
to get a sequences of tokens. We run a standard token weighting and scaling are provided in Ap-
“forward pass” with the model B for each tok- pendixA.
3 ExperimentalSetup Correlation metrics We used three major cor-
relationmetrics. Followingbestpracticesinnatu-
We evaluate CodeBERTScore across multiple
rallanguageevaluation,weusedKendall-Tau(τ),
datasets and programming languages. We first
Pearson (r ) and Spearman (r ) to measure the
p s
show that CodeBERTScore is more correlated
correlation between each metric’s scores and the
with human preference than previous metrics, us-
references. Thedetailedequationscanbefoundin
inghuman-ratedsolutionsfortheCoNaLadataset
AppendixC.
(Yinetal.,2018a;Evtikhievetal.,2022). Wethen
show that CodeBERTScore is more correlated Human preference experiments We evaluate
withfunctionalcorrectness,usingtheHumanEval differentmetricsonCoNaLa(Yinetal.,2018b),a
dataset (Chen et al., 2021). We also show that naturallanguagetoPythoncodegenerationbench-
CodeBERTScore achieves a higher newly pro- mark collected from StackOverflow
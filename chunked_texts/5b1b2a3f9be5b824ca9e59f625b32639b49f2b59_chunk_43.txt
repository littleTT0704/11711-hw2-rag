.6 63.9 52.5 44.6 52.9
TABLEXI our model takes the multi-modal features as inputs, which is
COMPARISONTOSTATE-OF-THE-ARTVIDEOINSTANCE different from MaskFormer. The decoded instance embedding
SEGMENTATIONONAVISVALSET.
is further to be leveraged to track the instance identities given
the Lipschitz continuity of the network.
IX. ADDITIONALEXPERIMENTSONAVISDATASET
WereporttheperformanceofusingResNet-50andResNet-
XI. VISUALIZATIONOFATTENTIONMAPSINCFM
101 backbone on AVIS dataset. As shown in Table XI, we
also compare the CrossVIS baseline which only leverages the
video data. Our method outperforms CrossVIS baseline by We demonstrate the attention map of all eight heads of
4.4 and 3.9 mAP with ResNet-50 and ResNet-101 backbone the reference-to-target attention. The attention map of the
respectively. compressed feature is sharper than the attention map using
We also report the p-value of t-test on AVIS results with the full features. The attention map is selected from the same
different thresholds. As shown in Table XII, the p-value position in the transformer encoder for fairly comparison.
under IoU>0.75 threshold is smaller than that under IoU>0.5
threshold, which means the audio inputs have more effect on
the high-quality results.
XII. LIMITEDGAINOFINVOLVINGAUDIOMODALITY
X. DIFFERENCEWITHMASKFORMER[13]
Our method introduces audio modality in an online fashion
MaskFormer [13] is an image-level panoptic segmentation
with synchronized the audio and video data. Since we con-
methodthattreatspredictionbelongingtobothstuffandthings
sidering streamlining video frames (as previous online VIS
methods), we also introduce the corresponding audio frames
Ref.number AP AP50 AP AP50 p-value in the same way. However, for audio data analysis, long-term
w/oAudio-token wAudio-token
1 42.9 60.2 44.8(+1.9) 60.3 0.30(0
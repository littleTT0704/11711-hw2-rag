 model with the current feature set
would significantly improve question answering performance compared to the logistic
regression model that includes features of adjacent instances. However, the sequential
model could easily be extended with additional or improved transition features, and
further performance gains over logistic regression seem possible, particularly when
ranking smaller units of text. Furthermore, while the sequential model is conceptu-
ally more complex than LR Adjacent, it actually has fewer degrees of freedom and is
therefore less prone to overfitting. LR Adjacent uses 55 relevance features, including
features of adjacent instances. Thus we must optimize 55 weights and an intercept, or
56 parameters in total, if we do not perform feature selection. The sequential model,
on the other hand, uses only the original 19 relevance features and 8 transition fea-
8.2. SEQUENTIAL MODELS 143
tures. Thus it comprises one relevance model with 20 parameters and two transition
models with 9 parameters each (including intercepts). In total, the sequential model
has only 38 parameters.
In order to be usable for large-scale source expansion, a relevance model must be
extremely efficient at runtime so that it can be applied to billions of text nuggets.
The sequential relevance model meets this requirement since the time complexity of
the Viterbi algorithm and the computation of marginal probabilities is linear in the
number of text nuggets and features. Compared to the logistic regression model with
featuresofadjacentinstances, thesequentialmodelhastheadvantageofrequiringless
time for training since it uses fewer features in its relevance component (19 instead
of 55 features). The training time for the sequential model was about 2 minutes for
paragraph-length text nuggets and 3 minutes for sentence-length nuggets when using
all annotated data. In contrast, it took about 45 minutes to fit a logistic regression
model with adjacent features to paragraph nuggets and 68 minutes to fit an LR model
tosentencenuggets. Notethatittakeslongertotrainamodelonsentencessincethere
are more sentences than paragraphs in the dataset. The runtimes were measured on
a server with 3 GHz Xeon CPUs and 32 GB RAM.
144 CHAPTER 8. EXTENSIONS FOR RELEVANCE ESTIMATION
Chapter 9
Conclusions
In Section 9.1 we summarize our approach and experimental results,1 and in Sec-
tion 9.
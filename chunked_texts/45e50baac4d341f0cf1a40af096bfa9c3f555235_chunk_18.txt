putationalLinguistics.
Guangxuan Xu and Qingyuan Hu. 2022. Can
ModelCompressionImproveNLPFairness. CoRR,
abs/2201.08542.
A DetailsofMetricCalculation
A.1 SEAT
TheSEATtasksharesthesametaskasWEATtask,
which is defined by four word sets, two attribute
sets, and two target sets. For example, to decide
the presence of gender bias the two attribute sets
are disjoint sets given by: 1) a masculine set of
words, such as {’man’, ’boy’, ’he’,...}, and 2) a
setoffemininewords{’woman’,’girl’,’her’,...}.
Thetargetsetswillcharacterizeconceptssuchas
’sports’and’culinary’.
WEATevaluateshowclosearetheattributesets
fromthetargetsetstodeterminetheexistenceof
bias. Mathematicallythisisgivenby:
s(A,B,X,Y) = s(x,A,B) s(y,A,B)
−
x X y Y
X∈ X∈
(1)
2669
Figure2: CrowsGENDERbiaswithQuantizedResults
Figure3: CrowsRACEbiaswithQuantizedResults
Figure4: CrowsRELIGIONbiaswithQuantizedResults
2670
Figure5: StereosetGENDERbiaswithQuantizedResults
Figure6: StereosetRACEbiaswithQuantizedResults
Figure7: StereosetRELIGIONbiaswithQuantizedResults
2671
Figure8: StereosetLMScorewithQuantizedResults
Table 4: SS stereotype scores and language modeling scores (LM Score) for BERT, and RoBERTa models.
Stereotypescorescloserto50%indicatelessbiasedmodelbehavior. Boldvaluesindicatethebestmethodper
biasandLMScore. ResultsareontheSStestset. Arandommodel(whichchoosesthestereotypicalcandidate
andtheanti-stereotypicalcandidateforeachexamplewithequalpro
077
Table 6.2: Questions in TREC datasets.
the queries used to retrieve relevant content from the source corpora. This approach
is relatively effective for resolving coreferences to the topics of question series, which
occur frequently in TREC. A summary of the datasets is given in Table 6.2.
To judge QA search results and final answers automatically, we used answer keys
thatwerecompiledfromcorrectanswersfoundduringtheTRECevaluationsandthat
are also available on the NIST homepage. NIL questions without known answers were
removed from the datasets. TREC questions often have more than one acceptable
answer, and the answer patterns only cover answers that were found in the reference
corpora used in the evaluations. They are often incomplete, outdated or inaccurate,
and evaluation results that are based on these patterns can significantly understate
true system performance. When we evaluated end-to-end QA results (Section 6.4),
assessors who were not involved in the development of the SE approach manually
judged the top answers returned by the QA system and extended the answer keys
with additional correct answers. The reported accuracies are based on these extended
answer patterns, and are therefore unbiased estimates of true system performance.
When measuring search performance (Section 6.3), we did not further extend the
answer keys because it was impractical to manually judge tens or hundreds of search
results for each question. However, since we used the same imperfect patterns in all
search experiments, our comparative evaluation results are still valid.
6.2 Sources
We expanded local copies of two sources that are both useful for the Jeopardy! and
TREC QA tasks: The online encyclopedia Wikipedia3 and the online dictionary Wik-
tionary4. Wikipedia, in particular, has proven to be a valuable resource for a variety
of natural language processing tasks [Bunescu et al., 2008, Medelyan et al., 2009]
and has been used successfully in QA [Ahn et al., 2004, Kaisser, 2008]. The sources
differ in two important ways that affect SE: (1) Wiktionary entries are on average
much shorter than Wikipedia articles (780 vs. 3,600 characters), and (2) Wiktionary
entries are often about common terms. The shorter Wiktionary seeds render the top-
icality features that leverage the seed document body,
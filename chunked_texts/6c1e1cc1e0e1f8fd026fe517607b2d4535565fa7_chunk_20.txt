 improves Codex by 6.4% on the same werepretrainedonPythonareabundantcomparedtoother
benchmark (Section 5.1). Similarly to our work, Chowd- domain-specific languages, making Python code a much
heryetal.(2022)havealsoexperimentedwithgenerating morepreferablerepresentation.Andoretal.(2019)generate
PythoncodeforsolvingtheGSM8Kbenchmark,buttheir task-specificarithmeticoperationsforreadingcomprehen-
experimentsresultedinlower accuracythanthestandard siontasks;Guptaetal.(2019)designneuralmodulessuch
PaLM-540B that uses chain-of-thought. Pi et al. (2022) ascounttodealwitharithmeticoperations. PAL gener-
pretrainthemodelonexecutionresultsofrandomexpres- alizestheseworksbygeneratinggeneralPythonprograms,
sionsonacalculator,insteadofusingthesolverattesttime withouttheneedfordefiningspecializedmodules. Theclos-
aswell. Whiletheirmodelcanhypotheticallyperformarith- estworktoourstechnicallymaybeBinder(Chengetal.,
meticbetterthanotherpretrainedLMs,theirresultsonthe 2022),butitaddressedmostlyansweringquestionsabout
SVAMPbenchmarkaremuchlower:57.4%usingaT5-11B tablesusingSQLandSQL-likePython.
PAL:Program-aidedLanguageModels 9
8.Conclusion C., Tillet, P., Such, F. P., Cummings, D., Plappert, M.,
Chantzis,F.,Barnes,E.,Herbert-Voss,A.,Guss,W.H.,
WeintroducePAL,anewmethodfornaturallanguagerea-
Nichol,A.,Paino,A.,Tezak,N.,Tang,J.,Babuschkin,
soning, using programs as intermediate reasoning steps.
I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr,
DifferentlyfromexistingLM-basedreasoningapproaches,
A. N., Leike, J., Achiam, J., Misra, V., Morikawa,
andhalfprecision(FP16),tostudythe
effectofquantization. Inourpreliminaryexperiments,wefoundthatemployingmoreaggressive
quantization techniques such as 8-bit and 4-bit quantization using naive methods led to severely
compromisedtranslationquality,withtheBLEUscoredroppingtoaround1,effectivelyresultingin
afailedtranslation. Allmodels’implementationandcheckpointsareavailableonHuggingFace.
Results. Figure2summarizestheefficiencyperformanceofdifferentmodelsinontheWMT14
DE-ENdataset,alongwiththeirtranslationquality. Overall,modelstrainedforEnglishtranslation
demonstratedbettertrade-offsbetweentranslationqualityandefficiency.Notably,OPUSoutperforms
themuchlargerMBARTM2MandM2M100modelsinbothaccuracyandallaspectsofefficiency,
andisthemostefficientmodelamongall. AlthoughWMT21-Meta,thelargestmodelconsidered,
providesthehighestBLEUscore,ittakesasubstantialhitinefficiency.
Interestingly,despitebeingmorethanfourtimeslarger,WMT19-Metaachievesefficiencyperfor-
mancecomparabletoOPUSinlatency,memoryoverhead,andenergyconsumption,andsignificantly
outperformsitintermsofBLEU.However,itfallsshortofOPUSinthroughput. Thisobservation
confirmsthatrelyingonasingleefficiencymetricrisksoversimplifyingthecomplexperformance
landscapeofefficiencyinpracticalapplications.
WithONNX,themodelsachieveover20%improvementsinlatencyandthroughputinthesingle-
streamscenario,accompaniedbyasignificantreductioninmemoryandenergyoverhead. However,
lessefficiencyimprovementisobservedinotherscenarioswithlargerbatchsizes.
Larger models benefit more from FP16 quantization. By comparing Figures 2a and 2b, we
observe that FP16 quantization improves all models’ efficiency performance (except #Params.),
particularlymemoryoverhead. Largermodelsappeartobenefitmorefromquantization. Asshown
inFigures2cand2d, whileOPUSexperiencesminimalefficiencygainsfromquantizationapart
fromincreasedthroughput,
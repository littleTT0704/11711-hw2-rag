aset.Inthisway,we encoder,theattentionofeachlayerisnormalizedbyasoftmax
consider the network encodes some class-specific information among all inputs. However, the mask prediction only needs
in the slot to improve instance segmentation quality. fine-grained target frame information. Therefore, the softmax
2) Ablation Experiments function downplays the importance of the target frame and
We conduct ablation studies on Youtube-VIS-2019 to show introduces noises from reference frames.
the effectiveness of different components of our method. Reference token size. The optimal reference token size is
Fusionmethod.Toinvestigatetheimportanceofcompressing a trade-off between information compression loss and target
reference features, we compare the performance of using importance gain. We conduct experiments on different token
reference tokens against using two baseline settings that di- sizes. As shown in Table V, the token size of 4 achieves the
rectly fuse full reference features in transformer encoder and best performance.
10
t=T t=T+5 t=T+10 t=T t=T+5 t=T+10
Attn Attn Attn
Attn PCA Attn PCA Attn PCA
Fig.10. Qualitativecomparisonoftheperformancewithandw/oaudioinputs.Themulti-headattentionmapiscompressedtothreedimensionbyPCAfor
visualization.
Reference frame number. To investigate the importance of Ref.number AP AP50 AP AP50 p-value
w/oAudio-token withAudio-token
temporalinformation,weconductanablationstudyontraining
1 42.9 60.2 44.8(+1.9) 60.3 0.30
with different reference frames. As shown in Table III, as the 2 44.3 60.9 46.0(+1.7) 63.8 0.26
reference frame number varies from 0 to 4, the mAP first 3 45.6 61.9 46.6(+1.0) 63.3 0.65
4 45.6 62.2 45.7(+0.1) 62.7 -
increasesto40.8thensaturatesanddecreases.Thisisbecause
5 45.2 62.9 45.0(-0.2) 62.6 -
the
,itisusefultointroducediversetasks
frommultipledomainswhenevaluatinganSSLalgorithm.
Effectiveness of Pre-training As shown in Figure 1a and Figure 1b, benefiting from the pre-
trainedViT,thetrainingbecomesmoreefficient,andmostSSLalgorithmsachievehigheroptimal
performance. Note that Pseudo Labeling, Mean Teacher, Î  model, VAT, and MixMatch barely
convergeiftrainingWRN-28-8fromscratch. Apossiblereasonisthatthescarcelabeleddatacannot
provideenoughsupervisionforunlabeleddatatoformcorrectclusters. However, thesemethods
canachievesufficientlyreasonableresultswhenusingpre-trainedViT.AsillustratedinFigure2,
7
0.8 Pseudo Labeling 0.8 Pseudo Labeling
Mean Teacher Mean Teacher
Pi Model Pi Model
0.6 VAT 0.6 VAT
MixMatch MixMatch
ReMixMatch ReMixMatch
0.4 UDA 0.4 UDA
FixMatch FixMatch
Dash Dash
0.2 C Co RM Ma at tc ch h 0.2 C Co RM Ma at tc ch h
FlexMatch FlexMatch
SimMatch SimMatch
0.0 25k 50k 75k 100k 0.0 25k 50k 75k 100k
Iter. Iter.
(a)WRN-28-8fromscratch. (b)Pre-trainedViT-S-P2-32.
Figure1:ComparisonoftestaccuracyofSSLalgorithmsonCIFAR-100with400labels. (a)Existing
protocolwhichtrainsWRN-28-8fromscratch; (b)USBCVprotocolwhichtrainsImageNet-1K
pre-trainedViT-S-P2-32,whereSdenotessmall,Pdenotespatchsize,and32isinputimagesize.
0.8 0.8
0.6 0.6
WRN-28-8 WRN-28-8
w/o Pretrain w/o Pretrain
WRN-28-8 WRN-28-8
0.4 ViT-S-P16-224 0.4 ViT-S-P16-224
ViT-S-P2
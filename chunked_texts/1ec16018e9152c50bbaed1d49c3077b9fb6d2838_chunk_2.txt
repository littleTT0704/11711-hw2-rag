
asgradient-basedsaliencymaps(Lietal.,2016a;
demonstrate that contrastive explanations are
Sundararajanetal.,2017),arenotasinformative
quantifiably better than non-contrastive expla-
for LM predictions compared to other tasks like
nations in verifying major grammatical phe-
nomena, and that they significantly improve textclassification. Forexample,toexplainwhyan
contrastivemodelsimulatabilityforhumanob- LM predicts “barking” given “Can you stop the
servers. Wealsoidentifygroupsofcontrastive dog from ____”, we demonstrate in experiments
decisions where the model uses similar evi-
thattheinputtokenprecedingthepredictionisof-
dence,andweareabletocharacterizewhatin-
tenmarkedasthemostinfluentialtokentothepre-
puttokensmodelsuseduringvariouslanguage
diction(Table1)byinstanceattributionmethods.
generationdecisions.1
Theprecedingtokenisindeedhighlyimportantto
1 Introduction determinecertainfeaturesofthenexttoken,ruling
outwordsthatwouldobviouslyviolatesyntaxin
Despitetheirsuccessacrossawideswathofnatural that context (e.g. non “-ing” verbs in the given
languageprocessing(NLP)tasks,neurallanguage example). However,thisdoesnotexplainwhythe
models(LMs)areoftenusedasblackboxes: how modelmadeothermoresubtledecisions,suchas
theymakecertainpredictionsremainsobscure(Be- whyitpredicts“barking”insteadof“crying”or
linkovandGlass,2019). Thisisinpartduetothe “walking”,whichareallplausiblechoicesifweonly
high complexity of the LM task itself, as well as lookattheprecedingtoken. Ingeneral,language
thatofthemodelarchitecturesusedtosolveit. modelinghasalargeoutputspaceandahighcom-
Wearguethatthisisalsoduetothefactthatinter- plexitycomparedtootherNLPtasks;ateachtime
pretabilitymethod
 levelqualityestimationtasks(Zervaetal.,2022).
ties,asourmetricsdointhiswork. Deutschetal.
6.2 Results
(2023)recommendreportingapairwiseaccuracy
score,whichrewardsmetricsforcorrectlyranking 6.2.1 ScorePrediction
translationsaswellascorrectlypredictingties,in Table2summarizesthemeta-evaluationresults,at
combination with atie calibration procedure that thesystemandsegmentlevel,forboththezero-shot
automaticallyintroducestiesintometricscoresso promptingandfinetuningsettings.
that the meta-evaluation is fairer. This accuracy
score,denotedacc∗,rangesbetween0and1,and Prompting Afirstobservationisalmostallzero-
arandommetricwouldachieve33%accuracy. We shotLLMevaluatorshavehighersystem-levelper-
reportthe“group-by-item”variantofthepairwise formance than learned metrics (with and without
accuracy score from Deutsch et al. (2023) in ad- references), with PaLM 540B and PaLM-2 UNI-
dition to Pearson’s ρ, a complementary signal to CORNachievingthebestperformance. Attheseg-
rank-basedcorrelationsthatmeasurethestrengthof mentlevel,thestoryismorecomplicated: similarly
thelinearrelationshipbetweentwovariables(and to Kocmi et al. (2022), we find that none of the
oneofthestandardcorrelationsreportedinWMT). LLMs we explored was able to consistently out-
performthebaselinelearnedmetrics. Weseethat
SpanMeta-Evaluation Since AUTOMQM pro-
PaLM-540Bisaparticularlypoorreference-based
vides not only scores but also the identified error
evaluator,whichissurprisinggivenitssystem-level
spans,wecancomparethepredictedspanswiththe
performance. Unexpectedly, instruction-tuning
errorsmarkedbyannotatorsintheMQMannota-
with FLAN seems to degrade performance, with
tions. Weevaluatequalityofpredictedspansusing:
FLAN-Pa
UD treebanks, and
usetheXGBoost(ChenandGuestrin,2016)library
run experiments to answer questions about word
to learn the decision tree. Further details on the
order, agreement, and case marking (§ 7.1). Fur-
modelsetuparediscussedinAppendixC.
thermore,wemanuallyverifyasubsetoftheseex-
tractedrules(§7.2). Experimentingwithlanguages
7.1 AutomatedEvaluationResults
thathavebeenalreadystudiedandhaveannotated
Wetrainmodelsusingsyntacticfeaturesforalllan-
treebanksiscrucialforverifyingtheefficacyofour
guages covered by SUD, wherever the linguistic
approach before applying it to other true low- or
questionisapplicable. Wefindthatourmodelsout-
zero-resourcelanguages. Underthissettingwenot
performtherespectivebaselinesbyan(avg.) accu-
onlyhavecleanandexpert-annotateddata,butwe
racyof+7.3forwordorder,+28.1forcasemarking,
can also quickly compare the effect of data size
and+4.0ARMforagreement.5 Wealsoreportthe
onthesystemperformanceasdifferentlanguages
resultbreakdownunderthreeresourcesettings,low,
havetreebanksofvaryingsize.
mid, and high, where low-resource refers to the
Data and Model We use the Syntactic Univer- treebankswith<500sentences,mid-resourcehas
salDependenciesv2.5(SUD)(Gerdesetal.,2019) 500−5000sentencesandhigh-resourcehas>5000
treebanks which are based on the Universal De- sentences. Acrossallthreelinguisticphenomena,
pendencies(UD)(Nivreetal.,2016,2018)project, the(avg.) modelgainsoverthebaselineare+3.19
the difference being that the former allows func- for the low-resource, +10.7 for the mid-resource
tion words to be syntactic heads (as opposed to
4https://github.com/harsh19/SPINE
UD’spreferenceforcontentwords),whichismore
5WealsoexperimentedwithRandomfore
.3.1 NoFeatureNormalization
of both SphereFace and SphereFace-R is not very sensitive
We evaluate SphereFace, SphereFace-R v1 and SphereFace-
tomandremainsstableforawiderangeofm.
R v2 without feature normalization. Note that SphereFace
with NFN is equivalent to the original SphereFace [2] with 6.3.3 SoftFeatureNormalization
CGD.TheresultsaregiveninTable7.Similartotheexperi-
Similar to Section 6.2.3, we adopt the best-performing set-
mentsonVGGFace2,NFNgenerallyworkswellwithsmall
tings (m and s) from HFN and vary the hyperparameter t
margins, achieving 79.89%, 84.66%, and 84.78% with m
inordertoevaluatetheperformanceofSFN.Theresultsare
being1.1,1.2,and1.2,respectively.Theincreasednumberof
reported in Table 9. With a properly tuned t, SFN achieves
trainingidentities(from8Kto86K)leadstosevereoptimiza-
87.70%, 87.19% and 86.10% AUC-0.0005 for SphereFace,
tion difficulty during training. This can be empirically ob-
SphereFace-R v1 and SphereFace-R v2, respectively. We
servedfromthesmalleroptimalmfromSphereFaceandthe
observe that the performance of SFN on MS-Celeb-1M are
dramatically decreased performance from both SphereFace
not as good as HFN, which contradicts the observation on
andSphereFace-Rwithlargermargins.
VGGFace2. This implies that SFN may be sensitive to the
distribution of the training data. We hypothesize that SFN
6.3.2 HardFeatureNormalization
is more sensitive to noisy samples (since MS-Celeb-1M has
Following our hyperparameter tuning strategy given in
muchmorelow-qualityandnoisyimagesthanVGGFace2).
Section 6.2.2, we search the optimal combination of s and
m.Sincetheperformanceisstableacrossawiderangeofs,
6.4 EvaluationonLarge-scaleBenchmarks
hereweusearelatively
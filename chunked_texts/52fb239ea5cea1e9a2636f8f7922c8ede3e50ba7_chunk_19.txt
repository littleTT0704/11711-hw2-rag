,wetakeexamplesfromotherdatasets(Ta-
representatedinthedistributionofexamplesacross ble 12-15) within the same task. We repeat this
categoriesinL¯ILA. evaluation with increasing numbers of examples
(uptothetokensizeofmodels)tostudytheeffect
4 Experiments onperformance5.
Inthissection,weintroduceourmodelingcontri- Evaluation. Weevaluateourmodelsundertwo
butions for the L¯ILA benchmark and discuss the regimes—directlyoutputtingtheansweri.e.,pro-
overallexperimentalsetup. graminductionandoutputtingaPythonprogram
thatisthenexecutedtoobtainthefinalansweri.e.,
Data partition and evaluation. For the IID program synthesis. In the case of our fine-tuned
setup,werandomlypartitionthedataineachtask models,wetrainthemtooutputboththefinalan-
into training (70%), development (10%) and test swerandthePythonprogramconditionedonthe
(20%)sets. Additionally,wealsoevaluateonL¯ILA- inputquestion. Toevaluateourmodelsunderdirect
OODandL¯ILA-ROBUSTsettings;thus,thefinal
4text-davinci-002, code-davinci-002
evaluationschemeisacombinationoftheperfor-
5Henceforthwerefertothemaxexamplemodelunless
manceonallthreeevaluationsetups otherwisespecified.
questionanswering,weuseF1-score6 tocompare Program synthesis substantially outperforms
themodeloutputandthegoldanswer. Toevaluate answer prediction. Synthesizing the program
programsynthesis,weexecutethemodel’soutput and evaluating it to get an answer substantially
within a Python interpreter and compare the pro- outperformsdirectlypredictingtheanswer. Forin-
gramoutputwiththeoutputofthegoldprogram, stance,multi-taskprogramsynthesis(BHA¯SKARA-
againusingF1. Weevaluatebasedontheprogram P)hasanaveragescoreof0.480whilemulti-task
output,ratherthantheprogramitself,toaccountfor answer prediction (BHA¯SKARA-A) scores 0.252.
diversityinsolvingtechn
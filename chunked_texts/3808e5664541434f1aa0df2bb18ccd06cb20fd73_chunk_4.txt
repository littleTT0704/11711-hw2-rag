 of the world (Spears, 1998; Tech-
to address, as has also been shown by Xia et al. nau, 2018). For example, viewing AAE as a more
(2020). “Debiased” models still disproportion- toxicorlesspropervarietyofEnglishisaformof
atelyflagtextincertaindialectsastoxic. Notably, linguisticdiscriminationthatupholdsracialhierar-
mitigatingdialectalbiasthroughcurrentdebiasing chiesintheUnitedStates(RosaandFlores,2017).
methodsdoesnotmitigateamodel’spropensityto In this work, we consider two broad categories
label tweets by Black authors as more toxic than of toxic language dataset biases—lexical (§2.1)
bywhiteauthors. and dialectal (§2.2). Our experiments focus on
Weadditionallyexploreanalternativeproof-of- a single, widely used dataset (§2.3) from Founta
conceptstudy—relabelingsupposedlytoxictrain- etal.(2018).
ing instances whose automatic translations into a
majoritydialectaredeemednon-toxicbytheclas-
2.1 LexicalBiases(TOXTRIG)
sifier. Tothisend,wecreateasyntheticdatasetvia
few-shotdialecttranslationsystembuiltwithGPT- Current toxic language detection systems often
3 (Brown et al., 2020). While only an illustrative rely on the presence or absence of certain words
solution, it nevertheless takes into account the di- (e.g., swear words, identity mentions) to make
alectal context of the tweet, resulting in a model their predictions (Dixon et al., 2018; Dinan et al.,
lesspronetodialectalandracialbiases(§6). Over- 2019). While most previous analyses of this bias
all,ourfindingsindicatethatdebiasingamodelal- relied on a simple list of “bad” words (Davidson
readytrainedonbiasedtoxiclanguagedatacanbe et al., 2019; Dinan et al., 2019),4 we take a more
challenging,comparedtorelabelingthedatatore- nuancedviewofhowlexicalitemscanconveytox-
move existing biases
 0.999
PredictionEMAMomentum 0.999
WeakAugmentation RandomCrop,RandomHorizontalFlip
StrongAugmentation RandAugment(Cubuketal.,2020)
A.3.3 TEXTCLASSIFICATION
For text classification tasks, we random split a validation set from the training set of each dataset
used.ForIMDbandAGNews,werandomlysample1,000dataand2,500dataper-classrespectively
as validation set, and other data is used as training set. For Amazon-5 and Yelp-5, we randomly
sample 5,000 data and 50,000 data per-class as validation set and training set respectively. For
DBpedia,thevalidationsetandtrainingsetconsistof1,000and10,000samplesper-class.
18
PublishedasaconferencepaperatICLR2023
ThetrainingparametersusedareshowninTable8. Notethatforstrongaugmentation,weuseback-
translationsimilarto(Xieetal.,2020). Weconductback-translationofflinebeforetraining, using
EN-DE and EN-RU with models provided in fairseq (Ott et al., 2019). We use NVIDIA V100 to
trainalltextclassificationmodels,thetotaltrainingtimeisaround20hours.
Table8: Hyper-parametersoftextclassificationtasks.
Dataset AGNews DBpedia IMDb Amazom-5 Yelp-5
Model Bert-Base
WeightDecay 1e-4
LabeledBatchsize 16
UnlabeledBatchsize 16
LearningRate 1e-5
Scheduler η=η cos(7πk)
0 16K
ModelEMAMomentum 0.0
PredictionEMAMomentum 0.999
WeakAugmentation None
StrongAugmentation Back-Translation(Xieetal.,2020)
A.4 EXTENDEXPERIMENTRESULTS
In this section, we provide detailed experiments on the implementation of the sample weighting
function in unlabeled loss, as shown in Table 9. One can observe most fixed functions works sur-
prisinglywellonCIFAR-10with250labels,yetGaussianfunctiondemonstratethebestresultson
CIFAR-10with40labels. OntheSVHNwith40lab
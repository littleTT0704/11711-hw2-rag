 we have seen that related text that is retrieved or extracted for a given
topic can be used in the place of a seed document to estimate topicality since this
text on average contains more relevant information than randomly selected content.
In the experiments with ClueWeb09 we found that much more topical information
can be extracted from the local corpus than from the top 100 web pages retrieved
for a topic. Thus, if a large unstructured source is transformed, the effectiveness of
topicality features that leverage the extracted text nuggets may increase.
The transformed text corpus may itself already be a useful source for question
answering since it contains less noise and the content has been organized by topics.
Through effective topic discovery and statistical relevance estimation, we should be
able to select information about only the most important topics and to focus on the
most relevant facts about these topics. When using a collection of web pages, this
method is also likely to remove noise such as markup and embedded code, adver-
tisements and everything else that is not part of the actual content of a web page.
In addition, the topic-oriented source can be leveraged to answer factoid questions
seeking entities that match a given description, or to retrieve information about a
given topic for definitional QA. However, unless the unstructured source already had
sufficient coverage and a high degree of semantic redundancy, the transformed corpus
should be expanded with additional relevant content. The pseudo-documents can be
used as seeds for the topic-oriented source expansion system, and related content can
be retrieved or extracted from the Web or large locally stored sources.
Chapter 8
Extensions for Relevance
Estimation
Suppose we need to adapt the statistical source expansion approach to a new QA
domain, or to a different information retrieval or extraction application. In previous
experiments, we fitted relevance models using a large dataset comprising about 1,500
manuallyannotatedwebpages. Thetotalannotationtimethatwasrequiredtocreate
this dataset amounted to approximately 5â€“6 person weeks. In Section 8.1 we demon-
strate that active learning can substantially reduce the manual annotation effort with
only a slight loss in relevance estimation performance, thus greatly facilitating the
adaptation of SE to new domains and applications.
InSection8.2wefurtherproposeasequentialmodelthatestimatestherelevanceof
text nuggets given their surrounding text
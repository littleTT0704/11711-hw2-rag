[Tail]. Isthis
findlongnarrativestendtobedrivenfarawayfrom true?
theoriginalcommonsenseknowledge. Therefore, xWant DoesPersonXwant[Tail]after[Head]?
we set the length of the narrative to two or three
xNeed [Tailinpasttense].Isthistruewhen[Head]?
sentences.
We leverage text-davinci-002 GPT-3.5 for Table9: Templatesforconvertingsymboliccommon-
generating narratives. We set temperature to 0.9, senseknowledgetoquestionsforvalidation.
top-p to 0.95, frequency penalty to 1.0, presence
penaltyto0.6,andmaxtokensto1024.
B Detailsof SODA
A.2 Narrative→Conversation Table 10 and Table 11 show samples from our
dataset.
Inferring Conversation Participants We
prompt GPT-3.5 with “[narrative] The B.1 Post-processingtheConversations
following is a conversation in the scene
FilteringNon-humanSpeakers First,wecheck
between [PersonX’s name] and...” to let it
whetherthespeakerprefixincludesthenamefrom
finish the partial prompt. This yields a plausible
our name base (§2.4). Next, we use lexical pat-
interlocutor for a given narrative (e.g., mom,
tern matching and identify words in speaker pre-
classmate, coworker, etc.); for the example story
fixesthatindicatehumans(e.g.,mom,dad,teacher,
withMadeleine,“hercoach”waspredicted.
Mrs., Mr.). Finally, for speaker prefixes that
We leverage the text-davinci-002 GPT-3.5
do not match the above patterns, we prompt the
model for identifying the speakers. We set tem-
text-davinci-002 GPT-3.5 model whether the
peratureto0,top-pto1.0,frequencypenaltyto0,
speakerishuman. Forexample,“Q: Is [speaker
presencepenaltyto0
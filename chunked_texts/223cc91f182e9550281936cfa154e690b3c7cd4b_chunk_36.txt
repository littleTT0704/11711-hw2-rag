step produces the q solution:
26
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
q(n)(x,y) =p θ(n)(x,y)exp {Qθ(n) (x,y)/ρ
}
/ Z. (4.21)
The subsequent student-step involves approximation by fixing θ = θ(n) in Qθ(x,y) in the above
variational objective, and minimizes only Eq(n)(x,y) [logp θ(x,y)] w.r.t. θ.
4.3.2. Intrinsic Reward
Rewards provided by the extrinsic environment can be sparse in many real-world sequential decision problems.
Learning in such problems is thus difficult due to the lack of supervision signals. A method to alleviate the
difficulty is to supplement the extrinsic reward with dense intrinsic reward that is generated by the agent itself
(i.e., the agent is intrinsically motivated). The intrinsic reward can be induced in various ways, such as the
‘curiosity’-based reward that encourages the agent to explore novel or ‘surprising’ states (Houthooft et al.,
2016; Pathak et al., 2017; Schmidhuber, 2010), or the ‘optimal reward’, which is designed with the goal of
encouraging maximum extrinsic reward at the end (Singh et al., 2010; Zheng et al., 2018). Formally, let
r tin = rin(x t,y t) ∈ R be the intrinsic reward at time t with state x t and action y t. For example, in
(Pathak et al., 2017), rin is the prediction error (i.e., the ‘surprise’) of the next state x. Let Qin,θ(x,y)
t t+1
denote the action-value function for the intrinsic reward, defined in a similar way as the extrinsic
Qθ(x,y):
∞
Qin,θ(x,y) =E ∑γtr tin ∣ x 0 =x,y 0 =y. (4.22)
[ ]
t=0
It is straightforward to derive the
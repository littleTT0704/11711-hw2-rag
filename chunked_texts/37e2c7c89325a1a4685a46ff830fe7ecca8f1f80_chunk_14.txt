72:81.87] 84.20[84.03:85.03]
Attention(alllayers) 83.00[82.60:83.00] 85.72[85.72:86.23] 90.08[89.72:90.11]
Attention(lastlayer) 80.91[79.99:81.07] 83.15[82.91:83.51] 91.47[91.39:91.56]
Attention(SMaT) 91.48[91.40:91.56] 92.56[92.28:92.83] 92.84[92.84:93.08]
5.1 TextClassification
For text classification, we consider the IMDB dataset [Maas et al., 2011], a binary sentiment
classificationtaskoverhighlypolarizedEnglishmoviereviews. Asthebasepretrainedmodel,weuse
thesmallELECTRAmodel[Clarketal.,2020],with12layersand4headsineach(total48heads).
Like the setting in Pruthi et al. [2020], we use the original training set with 25,000 samples to
traintheteacher,andfurthersplitthetestsetintoatrainingsetforthestudentandadevandtest
set. Wevarythenumberofsamplesthestudentistrainedonbetween500,1,000,and2,000. We
evaluatesimulabilityusingaccuracy(i.e.,whatpercentageofstudentpredictionsmatchwithteacher
predictions). Theteachermodelobtains91%accuracyonthestudenttestset.
Table1showstheresultsinterminstegoratfedgsraidmientsu: lnaoboffienlsiettyoan(yEoneqwuhoasatwiothinsan1d)likefdoitr,btuthiethreesettings. Wecansee
that,overall,theattentionexplainheaterdittr!aitidnragegeddonwanidthonaSndMthereawTasnloetaavdersygtooodpslottu,adlsoe,ntsthatsimulatetheteacher
toosimpleandtheactingwassoso...iwouldgivethiss##nor##efe##st
modelmuch
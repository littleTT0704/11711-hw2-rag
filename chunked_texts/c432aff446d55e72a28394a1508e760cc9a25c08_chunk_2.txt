2020). Thismodelingisusuallydonebysequentiallyencodingacontext
c usingatrainedneuralnetworkfunctionf,andcomputingtheprobabilityofthenextwordw accordingto
t t
f(c )andavectorrepresentationofw.
t t
Recently,retrieval-augmentedLMshaveshownaseriesofimpressiveresults(Graveetal.,2017;Guuetal.,
2018;Heetal.,2020;Khandelwaletal.,2020b;Borgeaudetal.,2022;Alonetal.,2022). Retrieval-augmented
LMscomputenexttokendistributionsbasednotonlyontheimmediatelyprecedingcontextc andthemodel
t
parameters,butalsoonanexternaldatastore,fromwhichexamplesareretrievedandincorporatedintothe
baseLM’sprediction.
Oneretrieval-augmentedmodelthatisnotableforbothitssimplicityandefficacyisthek-nearestneighbor
languagemodel(kNN-LM;Khandelwaletal.,2020b). ItextendsatrainedbaseLMbylinearlyinterpolating
theoutputworddistributionwithakNNmodel. Thenearestneighborsareretrievedaccordingtothedistances
betweenthecurrentcontextembeddingofthebaseLMandallthecontextembeddingsinthedatastore. The
datastoreiscreatedbyencodingallcontextsfromanytextcollection,includingtheoriginalLMtrainingdata.
OneofthemostsurprisingresultsfromKhandelwaletal.(2020b)isthatkNN-LMreducestheperplexityof
thebaseLMevenwhenthekNNcomponentisretrievingexamplesfromthesametrainingsetthattheLM
wasoriginallytrainedon,indicatingthatthekNN-LMimprovestheabilitytomodelthetrainingdataandis
Preprint.Underreview.
3202
naJ
71
]LC.sc[
2v82820.1032:viXra
𝑃 parametric component 𝑃 non-parametric component
𝐿𝑀 𝑘𝑁𝑁
softmax() 𝐷 soft
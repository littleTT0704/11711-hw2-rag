 show below that the discriminator/critic-based experience, in
combination of certain choices of the divergence function D, re-derives the generative adversarial learning
(Goodfellow et al., 2014) from a different perspective than Section 5.2.
Generative adversarial learning: The variational experience view. The functional descent view of
generative adversarial learning presented in Section 5.2 is based on the treatment that the experience is the
given static data instances, and the various GAN algorithms are due to the different choices of the divergence
function. The extended view of SE in this section also allows an alternative viewpoint of the learning
paradigm, that gives more flexibility in not only choosing the divergence function but also the experience
function, leading to a richer set of GAN variants.
In this viewpoint, we consider experience that is defined variationally as mentioned above. That is, the
experience function f, as a measure of the goodness of a sample t, is not specified a priori but rather defined
through an optimization problem. As a concrete example, we define f as a binary classifier f ϕ with sigmoid
activation and parameters ϕ, where the value f ϕ(t) measures the log probability of the sample t being a real
instance (as opposed to a model generation). Thus the higher f ϕ(t) value, the higher quality of sample t. The
parameters ϕ of the experience function need to be learned. We can do so by augmenting the standard equation
(Equation 3.2) with added optimization of ϕ in various ways. The following equation gives one of the
approaches:
32
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
m q,i θnm ϕax −αH(q)+βD(q,p θ)−Eq [f ϕ]+Ep
d
[f ϕ],
(6.3)
where, besides the optimization of q and θ, we additionally maximize over ϕ with the extra term Ep [f ϕ] to
d
form the classification problem max ϕ −Eq [f ϕ]+Ep [f ϕ]. Further assuming a particular configuration of
d
the tradeoff hyperparameters α = 0 and β = 1, the resulting objective
m q,i θ
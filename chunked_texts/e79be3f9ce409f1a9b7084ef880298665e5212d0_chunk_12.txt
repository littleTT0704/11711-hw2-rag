 two similarities as
i
optimizeθ andθ inatoken-awaremanner. the alignment score for the given pair. For each text, we
v t
Tokenofinterest.InEq.4,weneedtodecidewhichtokens choose the top K(cid:48) aligned negative videos and vice versa.
should be included in P. In this paper, we heuristically The resulting 2K × (K(cid:48) + 1) pairs are then fed into the
i
select nouns and verbs as the targets considering they are multi-modalfusionlayers.Throughthisstrategy,wecanef-
more“concrete” inthevideos. Inpractice, nouns or verbs fectivelyselectthedifficultnegativesamplesontheflyatno
usuallyhavedifferentdiscriminativenesseveniftheyareall extracost.Sincethemulti-modalfusionlayershasmoreca-
thesametype. Forexample,“man”isanounbutislessin- pacity(parameters)todistinguishthesehardnegativesfrom
formativethan“gymnast”. Toreflectthis,wefurtherassign positive ones, our sampling strategy naturally prompts the
different words with different weights by computing their cooperationbetweenthethreecontrastivelosses.
inversedocumentfrequency(idf)[22]. Ahigheridfmeans Finally, wepresentacomprehensivecomparisontodif-
it is more unique across the corpus, and hence will weigh ferentiateourmodelwithpreviousworkswithrespecttothe
morewhencomputingthetoken-levelcontrastiveloss. An- usedcontrastivelearningmethodinTable1.
otherpracticalissueforcomputingthelossisthatthetokens
3.4.Objective
are usually sub-words due to the BERT tokenizer. Hence,
foralltokensthatbelongstothesameword,wewillassign The training objective in our method is finding optimal
thesameweightsaccordingly. θ = {θ,θ,θ } by minimizing the combination of the
v t m
After computing the token-aware contrastive loss, we abovethreecontrastivelosses
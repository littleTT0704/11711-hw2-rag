 Q and M K) is a 12-layer transformer with Baselines. Our first baseline is the BUT-
12 heads and hidden dimension 384. The last layer LER::BRAIN (BUTLER) agent (Shridhar et al.,
is then fed into two linear heads to generate K and 2020b), which consists of an encoder, an aggregator,
Q. For embedding of actions and observations, we use and a decoder. At each time step t, the encoder
pre-trained RoBERTa-large (Liu et al., 2019) with em- takes initial observation s0, current observation st,
bedding dimension 1024. For sub-task generation, we and task string s and generates representation
task
use ground-truth sub-tasks for training, and generated rt. The recurrent aggregator combines rt with the
sub-tasks from Plan module for evaluation. last recurrent state htâˆ’1 to produce ht, which is
then decoded into a string at representing action. In
Experimental Setup. Unlike the original bench- addition, the BUTLER agent uses beam search to get
mark (Shridhar et al., 2020b), we experiment with out of stuck conditions in the event of a failed action.
models trained with behavior cloning. Although Shrid- Our second baseline GPT (Micheli & Fleuret, 2021)
har et al. (2020b) observe that models benefit greatly is a fine-tuned GPT2-medium on 3553 demonstrations
from DAgger training, DAgger assumes an expert that from the AlfWorld training set. Specifically, the
is well-defined at all possible states, which is inefficient GPT is fined-tuned to generate each action step
and impractical. In our experiments, training is 100x word-by-word to mimic the rule-based expert using
slower with DAgger compared to behavior cloning (3 the standard maximum likelihood loss.
Plan, Eliminate, and Track
4.2. Overall Results on Template and Human 4.4. Automated Analysis of PET modules
Goals
Plan Module We experiment with different LLMs
We compare the performance of action attention as- such as GPT2-XL (Radford et al., 2019), GPT-Neo-
sisted by PET with BUTLER (Shridhar et al., 2020b) 2.7B (Black et al.,
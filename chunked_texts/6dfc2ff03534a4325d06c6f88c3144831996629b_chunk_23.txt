stomergethevi-
sionandlanguagerepresentationsgivenbyaqueryandeach
6.1.Baselines
regionintheimage.
WecompareourR2Ctoseveralstronglanguageandvi- h. MultimodalTuckerFusion(MUTAN)[6]: Thismodel
sionbaselines. expressesjointvision-languagecontextintermsofatensor
decomposition,allowingformoreexpressivity.
Text-only baselines We evaluate the level of visual
We note that BottomUpTopDown, MLB, and MUTAN
reasoning needed for the dataset by also evaluating purely
text-only models. For each model, we represent q and r(i) alltreatVQAasamultilabelclassificationoverthetop1000
answers[4,50]. Because ishighlydiverse(SuppA),
as streams of tokens, with the detection tags replaced by VCR
theobjectname(e.g. chair5â†’chair). Tominimizethe for these models we represent each response r(i) using a
GRU [11].11 The output logit for response i is given by
discrepancybetweenourtaskandpretrainedmodels,were-
the dot product between the final hidden state of the GRU
placepersondetectiontagswithgender-neutralnames.
encodingr(i),andthefinalrepresentationfromthemodel.
a. BERT[15]:BERTisarecentlyreleasedNLPmodelthat
Humanperformance Weaskedfivedifferentworkers
achievesstate-of-the-artperformanceonmanyNLPtasks.
on Amazon Mechanical Turk to answer 200 dataset ques-
b. BERT(responseonly)WeusethesameBERTmodel,
tionsfromthetestset. Adifferentsetoffiveworkerswere
however, during fine-tuning and testing the model is only
giventheresponsechoicesr(i). askedtochooserationalesforthosequestionsandanswers.
c. ESIM+ELMo [10]: ESIM is another high perform- Predictionswerecombinedusingamajorityvote.
ingmodelforsentence-pairclassificationtasks,particularly 10ForVQA,themodelistrainedbysamplingpositiveornegativean-
whenusedwithEL
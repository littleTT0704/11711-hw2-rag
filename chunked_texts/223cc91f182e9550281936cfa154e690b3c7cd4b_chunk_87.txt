 Zhiting Hu and Eric P. Xing. This article is licensed under a Creative Commons Attribution (CC BY
4.0) International license, except where otherwise indicated with respect to particular material included in the
article.
Footnotes
1. We can alternatively derive the objective from Equation [eq:divg:var-gan-fgan-H], by setting Œ± = Œ≤ = 1,
ùîª to the cross entropy, and ‚Ñç to the Shannon entropy. Note that KL(q‚à•p ) = ‚àí H(q) ‚àí ùîº [logp ]. Thus the
Œ∏ q Œ∏
solution of q can be derived as in Equation [eq:se-teach-student]. ‚Ü©
References
Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., & Riedmiller, M. (2018, April 30‚Äì
May 3). Maximum a posteriori policy optimisation [Poster presentation]. 6th International Conference on
Learning Representations, Vancouver, BC, Canada. https://openreview.net/pdf?id=S1ANxQW0b
‚Ü©
Altun, Y., & Smola, A. (2006). Unifying divergence minimization and statistical inference via convex
duality. In G. Lugosi & H. U. Simon (Eds.), Lecture notes in computer science: Vol. 4005. Learning theory
(pp. 139‚Äì153). https://doi.org/10.1007/11776420_13
‚Ü©
61
Harvard Data Science Review ‚Ä¢ Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
Anderson, P. W. (1972). More is different: Broken symmetry and the nature of the hierarchical structure of
science. Science, 177(4047), 393‚Äì396. https://doi.org/10.1126/science.177.4047.393 ‚Ü©
Arjovsky, M., Chintala, S., & Bottou, L. (2017). Wasserstein generative adversarial networks. In D. Precup
& Y. W. Teh (Eds.), Proceedings of the 34th International Conference on Machine Learning (Vol. 70; 214‚Äì
223). https://proceedings.mlr.press/v70
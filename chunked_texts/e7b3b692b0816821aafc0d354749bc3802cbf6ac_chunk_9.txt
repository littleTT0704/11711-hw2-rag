,astheargmaxofthisscore:
ub =argmax(P ToM(x|uj)wl ·p speaker(uj)) (9)
U
Inourexperiments,wetrainmodelswiththreedifferentsettingsofw.Wetrainmodelswithw =0
l l
to isolate the effects of the ToM listener from the effects of the new model architecture. We train
models with w = 1 which weigh the speaker and ToM listener input equally. Finally, we train
l
models where w is the arbitrarily high constant 1000. In this case, the listener score dominates,
l
and our speaker essentially seeks to maximize P (x|ui), which it approximates with the
listener
ToM listener score. If we replace the learned ToM listener with a copy of the external listener,
thisutterancesamplingprocessisclosetothatofthelearnedRationalSpeechAct(RSA)modelin
Andreas & Klein (2016), which also trains pragmatic speakers on referential game environments.
WecompareourToMspeakerstothese“RSA”speakerstoevaluatetheimpactofusingourlearned
approximationratherthantheactuallistener.
Finally,weintroduceahyperparameterσ: ourspeakeroutputsarandomutterancewithprobability
σ andub withprobability1−σ. σ issettodecaylinearlyovertime;thisrandomizationisdoneto
promoteearlyexploration.
Bydefault, webeginwithanuntrainedlistenerthatwillbetrainedtoemulatetheactuallistener’s
outputs over time. To train the ToM listener, we introduce a third training objective in addition to
O andO,theToMobjectiveO,definedasthecross-entropylossbetweenthedistribution
LI CG ToM
oftheToMlistenerandthatofthetruelistener. Thus,wearetrainingittogivehigherprobabilities
tothelistener’schoicexˆbasedonthespeaker’sutterances. Formally,
(cid:26)
−logP (xˆ|u) P >θ
O = ToM max 1 (10)
ToM 0
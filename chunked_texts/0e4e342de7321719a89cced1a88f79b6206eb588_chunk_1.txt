Bear the Query in Mind: Visual Grounding with Query-conditioned Convolution
ChonghanChen1∗, QiJiang1∗, Chih-HaoWang1∗, NoelChen1∗, HaohanWang1,2,
XiangLi1, BhikshaRaj1
1,2SchoolofComputerScience,CarnegieMellonUniversity
2 SchoolofInformationScience,UniversityofIllinoisUrbana-Champaign
{chonghac,qij,chihaow,yunhsua3,haohanw,xl6}@andrew.cmu.edu,
bhiksha@cs.cmu.edu
Abstract (x,y,w,h) (x,y,w,h) (x,y,w,h)
Prediction Head Prediction Head Prediction Head
Visualgroundingisataskthataimstolocateatar-
Fusion Module Fusion Module
get object according to a natural language expres-
Visual Encoder Visual Encoder sion. Asamulti-modaltask,featureinteractionbe- Textual Visual Textual Textual
Query-conditioned Query-conditioned
Encoder Encoder Encoder Encoder
tween textual and visual inputs is vital. However, Convolution Convolution
previoussolutionsmainlyhandleeachmodalityin- text text text
dependently before fusing them together, which query image query image query image
doesnottakefulladvantageofrelevanttextualin- (a) Extract-and-fuse pipeline (b)VGQC pipelinewith fusion (c) VGQC pipelinewithout fusion
formationwhileextractingvisualfeatures. Tobet-
Figure 1: Previous extract-and-fuse pipeline (a) and our VGQC
ter leverage the textual-visual relationship in vi- pipeline(b)and(c). In(a),textualandvisualfeaturesareextracted
sual grounding, we propose a Query-conditioned independently and fused together in the following fusion module.
Convolution Module (QCM) that extracts query- (b) uses a visual encoder with query-conditioned convolution that
awarevisualfeaturesbyincorporatingqueryinfor- takesinboththeimageandquerytokentoproducequery-awarevi-
m
f data-aug(t;D) =logEt∗∼D[a t∗(t)]. (4.8)
The resulting student-step updates of θ, keeping (α = 1,β = ϵ) of supervised MLE, is thus:
m θaxEt∗∼D,
t∼a
t∗(t)[logp θ(t)].
(4.9)
The metric a t∗(t) can be defined in various ways, leading to different augmentation strategies. For example,
setting a t∗(t) ∝ exp{R(t,t∗)}, where R(t,t∗) is a task-specific evaluation metric such as BLEU for
machine translation, results in the reward-augmented maximum likelihood (RAML) algorithm (Norouzi et al.,
2016). Besides the manually designed strategies, we can also specify a t∗(t) as a parameterized transformation
process and learn any free parameters thereof automatically (Section 6). Notice the same form of the
augmentation experience f and the reweighting experience f, where the similarity metrics both
data-aug data-w
include learnable components (i.e., a t∗(t) and w(t∗), respectively). Thus the same approach to automated
data reweighting can also be applied for automated data augmentation, as discussed more in Section 9.2.
4.1.5. Actively Supervised Data Instances
Instead of access to data instances x∗ with readily available labels y∗, in the active supervision setting, we are
presented with a large pool of unlabeled instances D = {x∗} as well as a certain budget for querying an
oracle (e.g., human annotators) for labeling a limited set of instances. To minimize the need for labeled
instances, we need to strategically select queries from the pool according to an informativeness measure
u(x) ∈ R. For example, u(x) can be the predictive uncertainty on the instance x, quantified by the
Shannon entropy of the predictive distribution or the vote entropy based on a committee of predictors (Dagan
& Engelson, 1995).
22
Harvard Data Science Review • Issue 4.4, Fall 2022
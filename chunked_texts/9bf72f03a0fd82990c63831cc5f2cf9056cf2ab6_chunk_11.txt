itthelimitationsofQAT,whichis“fake"quan-
tization, meaning all computations are still done
with full-precision floating point numbers. This
approachismoreexpensivetocarryoutinpractice
thanposttrainingquantization,butgeneratesbetter
results.
2RefertoAppendixforhowwecomputedthescore,and
moredetailsinTable.3&4
References Shigeki Karita, Nanxin Chen, Tomoki Hayashi,
Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang,
HaoliBai,WeiZhang,LuHou,LifengShang,JingJin,
Masao Someki, Nelson Enrique Yalta Soplin,
Xin Jiang, Qun Liu, Michael Lyu, and Irwin King.
Ryuichi Yamamoto, Xiaofei Wang, et al. 2019. A
2020. Binarybert: Pushing the limit of bert quanti-
comparative study on transformer vs rnn in speech
zation. arXivpreprintarXiv:2012.15701.
applications. In 2019 IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU),
Yoshua Bengio, Nicholas Léonard, and Aaron
pages449–456.IEEE.
Courville. 2013. Estimating or propagating gradi-
entsthroughstochasticneuronsforconditionalcom-
NitishShirishKeskar, DheevatsaMudigere, JorgeNo-
putation. arXivpreprintarXiv:1308.3432.
cedal, Mikhail Smelyanskiy, and Ping Tak Peter
PulkitBhuwalka,AlanChiao,SuharshSivakumar,Ra- Tang. 2016. Onlarge-batch training for deep learn-
ziel Alvarez, Feng Liu, Lawrence Chan, Skirman- ing: Generalization gap and sharp minima. arXiv
tas Kligys, Yunlu Li, Khanh LeViet, Billy Lam- preprintarXiv:1609.04836.
bert,MarkDaoust,TimDavis,SarahSirajuddin,and
FrançoisChollet.2020. Quantizationawaretraining SehoonKim
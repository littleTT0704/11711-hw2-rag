(Guoetal.,
2017; Arazo et al., 2020) as training progresses, where the model memorizes the wrong pseudo-
labelsandbecomesveryconfidentonthem. Wecanalsonoticethat,bysettingλ(p)toafixedvalue
λ,weimplicitlyassumethePMFofthemodel’spredictionpisuniform,whichisfarawayfrom
max
therealisticdistribution.
A.1.3 LOSSWEIGHTRAMPUP
In the earlier attempts of semi-supervised learning, a bunch of work (Tarvainen & Valpola, 2017;
Berthelot et al., 2019b;a) exploit the loss weight ramp up technique to avoid involving too much
erroneouspseudo-labelsintheearlytrainingandletthemodelfocusonlearningfromlabeleddata
first. Inthiscase,thesampleweightingfunctionisformulatedasafunctionoftrainingiterationt,
which is linearly increased during training and reaches its maximum λ after T warm-up itera-
max
tions. Thuswehave:
t
λ(p)=λ min(,1), (17)
max T
λ min(t,1) 1
λ¯(p)= max T =, (18)
N λ min(t,1) N
U max T U
t
f(p)=λ min(,1), (19)
max T
g(p)=(cid:88)NU 1(pˆ
i
=y iu)
, (20)
N
U
i
whichdemonstratesthesameuniformassumptionofPMFandsamequalityfunctionasnaiveself-
training.Italsoindicatesthat,aslongassamesampleweightisusedforallunlabeleddata,auniform
assumptionofPDFoverpismade.
A.1.4 FIXEDCONFIDENCETHRESHOLDING
Confidencethresholdingintroducesafilteringmechanism,wheretheunlabeleddatawhosepredic-
tion confidence max(p) is above the pre-defined threshold τ is fully enrolled during training, and
othersbeingignored(Xieetal.,2020;Sohnetal.,2020). Theconf
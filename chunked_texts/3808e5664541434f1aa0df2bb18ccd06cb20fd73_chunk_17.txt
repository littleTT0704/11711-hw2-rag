AE-
terms of toxicity-AAE correlation (R AAE) and in relabelingresultsinamodelwhichdespitefollowinga
termsoffalseflaggingoftoxicity(FPR ). Inter- noisy process yields even larger improvements for di-
AAE
estingly, most models’ decrease in false flagging alectaldebiasing.
issmall,suggestingroomforimprovement.
ing methods and baselines, except for DataMaps-
5.2 RacialBiases
Easy, which shows the most racial bias in toxic-
To quantify the real-world impact of dialect-
ityflagging. Surprisingly,DataMaps-Hard,which
based racial bias, we measure the rates of toxi-
mitigated dialectal bias the best out of all debi-
city predicted by models on a corpus of tweets
asing methods, also shows high discrepancy be-
for which the race of authors is available, but
tween author races. Confirming previous results,
not annotations of toxicity. Specifically, we con-
thissuggeststhatdebiasingthesesystemsrequires
sider the dataset released by Preo¸tiuc-Pietro and
morethanautomaticdebiasingmethods.
Ungar (2018), which consists of 5.4M tweets,
collected from 4,132 survey participants (3,184 6 TowardsDataRelabeling
White, 374 African American) with self-reported
Based on our quantitative and qualitative analy-
race/ethnicityandTwitteruserhandles.12
ses, we believe there still is room for improve-
Wequantifyourmodels’racialbiasbymeasur-
ment in debiasing hate speech detection. There-
ing the difference in rates of flagging tweets by
fore,weturnourattentiontotheroleoflabelnoise
African American authors and those by white au-
indatasets. Partlyinspiredbyourqualitativeanal-
thors,followingSapetal.(2019).13
yses of debiased models’ predictions, we design
Listed in Table 5, our results show that auto-
a proof-of-concept study where we automatically
matic debiasing methods do not consistently de-
correctthelabeloftweetsusinga(nautomatic)di-
crease the racial
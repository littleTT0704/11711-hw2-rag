32,andthewindowsizefrom7to4toaccommodatetheadaptedinputimage
size. ForSemi-Aves,weadopttheoriginalSwin-S.FromtheresultsinTable12,onecanobserve,
thatonEuroSAT(32),asweadopt224pre-trainedSwin-Sandchangeitsinputandwindowsize,
theresultsareinferiortoViT-32reportedinthepaper,whereasonSemi-Aves(224),theresultsare
better than ViT-S. An interesting finding is that CoMatch performs relatively better with Swin-S
whileCrMatchperformsworse. Thisalsoshowstheimportanceofconstantlyupdatingthebackbone
inthefuturedevelopmentofUSB.
ForNLPtasks,weadditionallyexperimentwithRoBERTa[31]. WetrainRoBERTausingthesame
hyper-parametersreportedinTable16. RoBertagenerallyperformsbetterthanBertasexpected. The
performancedifferenceisbothveryclosewhenusingRoBertaorBert.
Due to the fact that the audio tasks setting in the current version of USB being built upon raw
waveforms,therearenotmanypre-trainedmodelsavailabletouse. WereporttheresultsofHuBert
[32]andWave2Vecv2.0[71]foraudiotaskstocomparedifferentbackbones. Thedifferencebetween
thesetwobackbonesselectedmainlyliesinpre-trainingdata. Wave2Vecv2.0ispre-trainedusing
raw human voice data and HuBert is an improved model with a discrete clustering target. Thus
wecanobservefromtheresults,thatonhumanvoicetasksSuperb-KS,Wave2Vecv2.0hasbetter
performance,whereas,onothertasks,HuBertismorerobustandoutperformsWave2Vecv2.0.
19
Table13: RoBERTaresultsonYelp.
Dataset Yelp
#Labels 250 1000
Supervised 42.56±1.15 39.00±0.16
Fully-Supervised 29.15±0.12
Pseudo-Label 48.26±0.02
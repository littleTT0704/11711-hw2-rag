anfindseveralinterestingconclusions:
Effectivenessofre-trainingW : Inthecaseof“LearnedW w/FFN”,weareessentiallyre-learningthe
ds ds
weightsfeedingintothesoftmaxfunctionseparatelyfromtheunderlyingLMencoder. Despitethisfact,we
canseethemodelachievesaPPLof20.920,whichis0.83pointsbetterthanthebasemodel. Thissuggests
thatthereissomebenefitinlearningtheparametersofW afterfreezingthebodyofthetransformerencoder.
ds
Effectivenessofensemblingtwopredictors:InbothcasesforW,theinterpolatedperplexityissignificantly
ds
betterthanthatofusingasinglepredictor. Thisisparticularlythecasewhenusingthe“att”representationfor
h,suggestingthattheutilityofensemblingpredictionsfromtwoviewsofthedataisnotonlyusefulwhen
ds
usingkNN-LM,butalsoinstandardparametricmodelsaswell.
ParametricensemblesasanalternativetokNN-LM?:Overall,byusingaseparatewordembeddingmatrix
withsizeV ×DasanalternativetokNN,wecanrecoverabout55%oftheperformancegainachievedby
kNN-LM,withonlyalimitednumberofparametersandwithoutthenecessityforslowkNNretrievalevery
timeatokenispredicted. ThissuggeststhatthemajorityofthegainaffordedbykNN-LMcouldbeachieved
byothermoreefficientmeansaswell.
h N ⊗ +#params PPL Interp. Oracle
ds ds
BaseLM - - - 0 21.750 - -
kNN-LMw/ATT att Big IP N ×D ∞ 19.095 14.077
ds
LearnedW w/ATT att 1x IP V ×D 22.584 20.353 16.954
ds
kNN-LMw/FFN ffn Big IP N ×D ∞ 21.101 16.254
ds
LearnedW w/FFN ffn 1x IP V ×D 20.920 20.694 18.772
ds
Table2: Performancecomparison
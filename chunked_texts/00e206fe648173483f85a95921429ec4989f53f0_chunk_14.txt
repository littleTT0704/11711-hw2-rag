ference examples from the source
domain (CLS), and MD with examples from the
6 ResultsandAnalysis
target domain (MDT). We report results only
withMDT pairedwithhigh-prec. c2fpruning(i.e. Table 4 reports results when transfering models
thresholdq =.5imposedonthementionscores m) trainedonONtoi2b2andmodelstrainedoni2b2
as described in §4. Without the threshold, MDT toCNwithsingletonsincluded(forcompleteness
has almost no effect on overall coreference per- Appendix A, Table 5 reports results without sin-
formance, likely because the space of candidate gletons). Forbothi2b2 CNandON i2b2,our
→ →
antecedentsforanygivenmentiondoesnotshrink. model performs better with mention annotations
Our model uses only mentions without target thanthecontinuedtrainingbaselinewithhalfthe
domaincoreferencelinks,whileourbaselineuses coreferenceannotations(e.g. equivalentannotator
coreferenceannotations. Accordingly,wecompare time,sincetheaveragelengthofi2b2documents
resultsforsettingswherethereis(1)anequivalent is 963 words; and timed experiments in CN sug-
numberofannotateddocumentsand(2)anequiv- gested mention annotations are ~2X faster than
alent amount of annotator time spent, estimated coreference,§5.1). CombiningMLMT withMDT
basedonthetimedannotationexperimentsin§3. results in our best performing model, but intro-
Foreachtransfersetting,weassumethesource ducing MDT with high-precision c2f pruning is
domain has coreference examples allowing us to enoughtosurpassthebaseline. Theresultssuggest
optimize CLS. In the target domain, however, in-domain mention annotation are more efficient
we are interested in a few different settings: (1) foradaptationthancoreferenceannotations.
100% of annotation budget is spent on corefer-
6.1 TransferAcrossAnnotationStyles
ence, (2) 100% of annotation budget is spent on
mentions, (3) the annotation budget is split be- ONandi
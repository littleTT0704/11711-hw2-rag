 the divergence Equation 7.7. The
probability functional descent (PFD) can also be used to optimize with a certain class of divergences, following
similar derivations discussed in Sections 5 and 7.2.1.
8. The Target Model
Besides the objective function (Sections 3–6) and the optimization (Section 7) above, we now briefly discuss
the third and last core ingredient of an ML approach, namely, the target model p θ. A key feature of the SE as
an objective function is that the formulation is agnostic to the specific choices of the target model. That is, one
can use the SE to train effectively anything with a learnable component (denoted as θ in general), ranging
from black-box neural networks, to probabilistic graphical models, symbolic models, and the combinations
thereof. We will mention a few examples of the target model that are of radically different categories,
illustrating the utility and generality of the SE.
Deep neural networks of any architectures. The target model can be a deep neural network of an arbitrary
architecture, composed of different neural layers and units. Figure 5, right panel, illustrates a set of common
40
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
neural components and how they are composed to form more complex ones at different levels of granularity.
Neural models of different architectures are used for different tasks of interest. For example, an image classifier
p (y∣x), with input image x and output object label y, typically consists of an encoder and a classification
θ
layer (a.k.a., classification head), where the encoder can be a ConvNet (LeCun et al., 1989) or transformer
(Vaswani et al., 2017) and the classification layer is often a simple multilayer perceptron (MLP) consisting of
several neural dense layers. As another example, a neural language model p (t) over text sequence t is a
θ
generator with a decoder component (which is in turn implemented via a transformer or other neural
architectures).
Figure 5. Left: An (incomplete) set of neural model components at different granularities.
Components at a lower level are composed to form higher-level, more complex ones.
or Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled
versionofbert: smaller,faster,cheaperandlighter. ArXiv,abs/1910.01108.
Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A Smith, and Yejin Choi. 2020.
Socialbiasframes: Reasoningaboutsocialandpowerimplicationsoflanguage. InACL.
ThomasScanlon.1998. Whatweowetoeachother. HarvardUniversityPress.
TimoSchickandHinrichSchütze.2020. It’snotjustsizethatmatters: Smalllanguagemodelsare
alsofew-shotlearners. arXivpreprintarXiv:2009.07118.
14
Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. 2021.
Towardsout-of-distributiongeneralization: Asurvey. CoRR,abs/2108.13624.
NisanStiennon,LongOuyang,JeffWu,DanielM.Ziegler,RyanLowe,ChelseaVoss,AlecRadford,
Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. In
AdvancesinNeuralInformationProcessingSystems(NeurIPS).
YuSun,ShuohuanWang,ShikunFeng,SiyuDing,ChaoPang,JunyuanShang,JiaxiangLiu,Xuyi
Chen,YanbinZhao,YuxiangLu,WeixinLiu,ZhihuaWu,WeibaoGong,JianzhongLiang,Zhizhou
Shang,PengSun,WeiLiu,XuanOuyang,DianhaiYu,HaoTian,HuaWu,andHaifengWang.
2021. ERNIE3.0: Large-scaleknowledgeenhancedpre-trainingforlanguageunderstandingand
generation. CoRR,abs/2107.02137.
AlonTalmor,OjinvdTafjord
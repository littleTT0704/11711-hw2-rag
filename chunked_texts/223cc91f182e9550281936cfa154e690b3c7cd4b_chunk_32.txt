 the dominant sentiment precisely. For example,
if a sentence is of structure ‘A-but-B’ with the connective ‘but’, the sentiment of the half sentence after ‘but’
~
dominates. Let x be the half sentence after ‘but’ and y ∈ [0,1] the (soft) sentiment prediction over x
B B B
by the current model, a possible way to express the knowledge as a logical rule f (x,y) is:
rule
f :=f (x,y)
rule
~ ~ (4.13)
=has-‘A-but-B’-structure(x) ⇒ (I(y =1) ⇒ y
B
& y
B
⇒ I(y =1)),
where I(⋅) is an indicator function that takes 1 when its argument is true, and 0 otherwise. Given an
~
instantiation (a.k.a. grounding) of (x,y,y ), the truth value of f (x,y) can be evaluated by definitions
B rule
~
in Equation 4.12. Intuitively, the f (x,y) truth value gets closer to 1 when y and y are more consistent.
rule B
We then make use of the knowledge-based experience such as f (t) to drive learning. The standard
rule
equation rediscovers classical algorithms for learning with symbolic knowledge.
Posterior regularization and extensions. By setting α = β = 1 and f to a constraint function such as
f, the SE with cross entropy naturally leads to a generalized posterior regularization framework (Hu et al.,
rule
2016):
min −H(q(t))−Eq(t)[logp θ(t)]−Eq(t)[f rule(t)],
(4.14)
θ,q
which extends the conventional Bayesian inference formulation (Section 2.3) by permitting regularization on
arbitrary random variables of arbitrary models (e.g., deep neural networks) with complex rule constraints.
The trade-off hyperparameters can also take other values. For example, by allowing arbitrary α ∈ R, the
objective corresponds to the unified expectation maximization (UEM) algorithm (Samdani et al., 2012
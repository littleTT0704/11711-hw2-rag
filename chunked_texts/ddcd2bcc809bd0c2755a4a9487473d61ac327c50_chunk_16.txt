 has a similar format.
Wereporttheaverageaccuracyofquestions2and
5 CleverHansvs. GeneralizedReasoning
3, both focusing on an agent’s belief rather than
Weconductedaseriesofexperimentsaimedtoen- objectivetruth. Finally,toensuremaximumrepro-
hanceourunderstandingofthefactorsinfluencing ducibilityoftheresults,wesetthetemperatureto0.
performanceinthecontextofN-ToMtasks. There- OurmainfindingisthatLLMsdon’texhibitro-
searchquestionthatguideduswas: Dothemodels bustperformanceacrossdifferentcategories. In
thatsolvethetaskspossessageneralabilityordo particular,laterLLMsexcelinsomecategories
theyrelyonmemorizationandshallowheuristics whilecompletelyfailingonothers. Weprovide
(“CleverHans”;Kavumbaetal.,2019)? Wedetail detailsbelow.
theexperimentsandfindingsbelow. Figure2illustratestheperformanceofarange
ofGPTmodelsondifferentcategorieswithinthe
5.1 DoLLMsRelyonSpuriousCorrelations?
unexpected transfer segment of Adv-CSFB. It is
ToMi and ToM-k are datasets that examine the evidentthatbothfalsebelief (i.e. theoriginalex-
unexpected transfer false belief problem. While amples from ToM-k) and trusted testimony (i.e.,
someone tells the protagonist that the object has
7Weusethezero-shotsetupwithoutprovidinganyreason-
ingexamples. beenmoved)haveimprovedinnewermodels. GPT-
6
Figure2: PerformanceofarangeofGPTmodelsonvariouscategorieswithintheunexpectedtransfersegmentof
Adv-CSFB.Theresultsaretheaverageaccuracyofquestion2(e.g. Mariathinksthatthebananasareinthe_)and
question3(e.g. WhenMariacomesback,shewillfirstlookforthebananasinthe_),whichspecificallyfocusonan
agent’sbeliefsratherthanobjectivetruth. Notably,GPT-4achievesanaccuracyof97%onthes
all
eration. Moreover,utilizingtheLLMsforgenerat-
baselines,includingthoseback-bonedwithbigger
ingknowledgedistilleddataismoreeffectivethan
models(e.g.,Nlp-RoBERTa,HL-RoBERTa). This
simplesemantictextaugmentation(e.g.,Nlpaug).
highlights the effectiveness of the value-aligned
knowledgedistillationwithLLMs. Per-Category performance Figure 3 presents
WeobservethattheOPT-175Bfew-shotlearning theper-categoryevaluationscoresof VA-BART.
approachperformsbetterthanrandomlabelassign- Theresultsvarysignificantlybetweencategories,
ments on the test set and HL-ALBERT, but still indicatingthecomplexityofourproposedtask. The
performsworsethanorascomparableastheother resultsforbothMenstruation-relatedDiscrimina-
baselines. ThisindicatesthatLLMswithprompt- tionandPayGapachievescoreshigherthan90%,
basedfew-shotlearningcanunderstandthevalue- while the results for Internalized Sexism are rela-
alignedclassificationtasktosomeextent,butthe tivelylow. Weconjecturereasonsforthehighper-
Model Accuracy W-F1
VA-ALBERT 70.101.65 70.751.48
w/ohumanlabeleddata 70.791.40 71.291.33
VA-ROBERTA 73.240.39 73.820.32
w/ohumanlabeleddata 72.902.06 73.191.68
VA-BART 74.070.82 74.360.60
w/ohumanlabeleddata 72.301.24 72.710.90
Table 2: Effectiveness of generated data. We remove
human-labelleddatafromthetrainingsetandonlyuse
synthetic samples generated from LLM for training
Figure 4: Vocabulary overlaps (%) of the generated (w/o human-labeled data). The minimal drops in per-
dataamongsexismcategories. Onlytop-3andbottom- formanceshowtheeffectivenessofvalue-alignedtrain-
3categoriesaredisplayedindescendingorderofW-

mainbaseline,whereasunfreezingtheantecedent
length, we sampled short (~200 words), medium
linkerdoesnotfurthersignificantlyimproveperfor-
(~500),andlong(~800)documents. Eachannota-
mance. Thisresultimpliesthatmentiondetection
torannotatedfourdocumentsforcoreferencereso-
can be disproportionately responsible for perfor-
lutionandfourdocumentsformentionidentifica-
manceimprovementsfromcontinuedtraining. If
tion(oneshort,onemedium,andtwolong,asmost
adapting only theencoder and mention detection
i2b2/VAdocumentsarelong). Eachdocumentwas
portions of the model yields strong performance
annotated by one annotator for coreference, and
gains,thissuggeststhatmention-onlyannotations,
one for mention detection. This annotation con-
asopposedtofullcoreferenceannotations,maybe
figuration maximizes the number of documents
sufficientforadaptingcoreferencemodelstonew
annotated(asopposedtothenumberofannotators
domains.
perdocument),whichisnecessaryduetothehigh
Model Recall Precision F1 varianceinstyleandtechnicaljargoninthemedical
SpanBERT+c2f 31.94 50.75 39.10 corpus. Intotal28documentswereannotated.
+tuneEnc,MDonly 60.40 56.21 56.42 Table3reportstheaveragetimetakentoanno-
+tuneEnc,AL,MD 60.51 57.33 56.71
tate each document. On average it takes 1.85X
Table1:Whenconductingcontinuedtrainingofac2fmodel
moretimetoannotatecoreferencethanmentionde-
ontargetdomaini2b2/VA,tuningtheantecedentlinker(AL)
doesnotresultinasignificantimprovementoverjusttuning tection,andthedisparityismorepronounced(2X)
thementiondetector(MD)andencoder(Enc).Alldifferences for longer documents. In Table 6 (Appendix A)
betweentunedmodelsandSpanBERT+c2fwerestatistically
significant(
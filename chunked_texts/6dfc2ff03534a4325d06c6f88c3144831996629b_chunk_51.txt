inthepaper,we
(MovieQA)to45.8%(TGIFQA),overa20%baseline.
usedBERTtorepresenttext[15]. Wewantedtoprovidea
There is also some non-visual bias for all datasets con-
faircomparisonbetweenourmodelandBERT,soweused
sidered: from 35.4% when given the question and the an-
BERT-Baseforeach. WetriedtomakeouruseofBERTto
swers(MovieQA)to72.5%(TGIFQA).Whiletheseresults
beassimpleaspossible,matchingouruseofitasabaseline.
suggestthatMovieQAisincrediblydifficultwithoutseeing
Given a query q and response choice r(i), we merge both
thevideoclip, therearetwothingstoconsiderhere. First,
intoasinglesequencetogivetoBERT.Oneexamplemight
MovieQAisroughly20xsmallerthanourdataset,with9.8k
looklikethefollowing:
examplesintraining. Thus,wealsotriedtrainingBERTon
[CLS] why is riley riding motorcycle while
‘ small’:taking9.8kexamplesatrandomfromourtrain-
VCR
wearing a hospital gown? [SEP] she had to leave
ing set. Performance is roughly 14% worse, to the point
the hospital in a hurry. [SEP]
of being roughly comparable to MovieQA.24 Second, of-
Note that in the above example, we replaced per-
tentimestheexamplesinMovieQAhavesimilarstructure,
son tags with gender neutral names [19] (person3→
whichmighthelptoalleviatestylisticpriors,forexample:
riley) and replaced object detections by their class name
“WhohasfollowedBoyletoEamon’sapartment?” An- (motorcycle1→motorcycle), tominimizedomainshift
swers: betweenBERT’spretraineddata(WikipediaandtheBook-
1. ThommoandhisIRAsquad. Corpus[94])and VCR.
2. DarrenandhisIREsquad. Each token
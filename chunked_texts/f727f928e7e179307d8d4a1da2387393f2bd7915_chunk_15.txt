multiplebeliefsinarow. CorrectLabel 94.46(0.7) 94.45(0.7) -0.24(0.1)
Baselines. Weuseoff-the-shelfoptimizersasbase-
Table5: Evaluationdifficultybydesiredmodeloutput,
lines. We tune the baseline hyperparameters sep- foralearnedoptimizertrainedwithSLAGonzsRE.
arately for each dataset, selecting among several
kinds of optimizers, learning rates, and the num- that“MainInputx isfalse.” SoforContrapositive
i
berofupdatesteps. Theselectioncriterionisthe Acc,wemeasurehowoftenthemodelfollowsthis
sameasthecriterionoutlinedforlearnedoptimiz- rule,whentheantecedentholdsofitsprediction.
ersabove. Theresultingbaselinesaresurprisingly
Beliefmeasurementresults. Table2showsthebe-
strong(seeAppendixTable12forfinalselections).
liefmetricsforeachdataset. Wefindthat 100M
∼
Hypothesis testing. We obtain 95% confidence parametermodelsshowlimitedevidenceofhaving
intervals and perform hypothesis tests via block consistentfactualbeliefs. Paraphraseconsistency
bootstrap,resamplingmodelseedsanddatapoints is69.50%( 1.09)forzsREandmuchlowerfor
±
(EfronandTibshirani,1994). Forablationexperi- Wikidata5m(25.84% 0.53). Whileentailmentac-
±
ments,werunonlyonemodelseedpercondition. curacyishighforLeapOfThought(85.63% 1.08),
±
the model is consistent under the contrapositive
5 ExperimentResults only16.51%( 2.71)ofthetime. Overall,these
±
results are not nearly as consistent as we would
5.1 DoLMshaveconsistentfactualbeliefs?
hope for factual beliefs to be. Interestingly, the
We measure Paraphrase Consistency, Entailment metrics are much higher when the model predic-
Acc, and Contrapositive Acc for finetuned task tionontheMainInputiscorrect
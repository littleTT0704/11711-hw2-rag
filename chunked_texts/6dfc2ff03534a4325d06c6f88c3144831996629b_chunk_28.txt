fundamentallydifferentsources,which
understanding as a QA task, with questions about COCO raisestheconcernthatmodelscancheatbyperformingau-
images [49] typically answered with a short phrase. This thorshipidentificationratherthanreasoningovertheimage.
lineofworkalsoincludes‘pointing’questions[45,93]and In contrast, in Adversarial Matching, the wrong choices
templated questions with open ended answers [86]. Re- comefromtheexactsamedistributionastherightchoices,
cent datasets also focus on knowledge-base style content andnohumanvalidationisneeded.
[80, 83]. On the other hand, the answers in are en-
VCR
tiresentences,andtheknowledgerequiredbyourdatasetis 8.Conclusion
largelybackgroundknowledgeabouthowtheworldworks.
Recent work also includes movie or TV-clip based QA Inthispaper,weintroducedVisualCommonsenseRea-
[75, 51, 46]. In these settings, a model is given a video soning, along with a large dataset VCR for the task that
clip, often alongside additional language context such as wasbuiltusingAdversarialMatching. WepresentedR2C,
subtitles, a movie script, or a plot summary.12 In contrast, amodelforthistask,butthechallenge–ofcognition-level
features no extra language context besides the ques- visualundertanding–isfarfromsolved.
VCR
tion. Moreover, the use of explicit detection tags means Acknowledgements
thatthereisnoneedtoperformpersonidentification[66]or WethanktheMechanicalTurkworkersfordoingsuchanoutstanding
linkagewithsubtitles. job with dataset creation - this dataset and paper would not exist with-
outthem. ThanksalsotoMichaelSchmitzforhelpingwiththedataset
Anorthogonallineofworkhasbeenonreferringexpres-
splitandJenDumasforlegaladvice.ThisworkwassupportedbytheNa-
sions: askingtowhatimageregionanaturallanguagesen- tionalScienceFoundationthroughaGraduateResearchFellowship(DGE-
1256082)and
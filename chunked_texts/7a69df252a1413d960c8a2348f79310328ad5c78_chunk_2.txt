030-92659-5 25 (DOI: 10.1007/978-
3-030-92659-5 25)
2202
naJ
41
]VC.sc[
4v36621.4012:viXra
2 H. Schulze et al.
Fig.1. Example results of the proposed CAGAN (SE). The generated images are of
64x64, 128x128, and 256x256 resolutions respectively, bilinearly upsampled for visual-
ization.
Although generating images highly related to the meanings embedded in a
natural language description is a challenging task due to the gap between text
and image modalities, there has been exciting recent progress in the field using
numeroustechniquesanddifferentinputs[20][11] [18][46][43][28][3][17][4][19]
[42] [30] [31] yielding impressive results on limited domains. A majority of ap-
proaches are based on Generative Adversarial Networks (GANs) [7]. Zhang et
al. introduced Stacked GANs [44] which consist of two GANs generating images
in a low-to-high resolution fashion. The second generator receives the image en-
coding of the first generator and the text embedding as input to correct defects
and generate higher resolution images. Further research has mainly focused to
enhance the quality of generation by investigating the use of spatial attention
and/or textual attention thereby neglecting the relationship between channels.
In this work, we propose Combined Attention Generative Adversarial Net-
work(CAGAN)thatcombinesmultipleattentionmodels,therebypayingatten-
tion to word, channel, and spatial relationships. First, the network uses a deep
bi-directional LSTM encoder [42] to obtain word and sentence features. Then,
the images are generated in a coarse to fine fashion (see Figure 1) by feeding
the encoded text features into a three stage GAN. Thereby, we utilise local-self
attention [26] mainly during the first stage of generation; word attention at the
beginning of the second and the third generator; and squeeze-and-excitation at-
tention [12] throughout the second and the third generator. We use the publicly
available CUB [38] and COC
;k))=
1
otherwdx isei
.
(1)
long problem
In other words, m(·,·) is 1 when the model fails to pre-
Figure1: Illustratingrobustness,compositionality,andout-
dict the correct integral, and 0 when the model succeeds.
of-distributiondeficienciesintheneuralsequenceintegrator.
We measure the proportion of failures on problems X =
{x,...,x }usingkcandidatesolutionsperproblemas:
1 N
algorithm,SAGGA(SymbolicArchiveGeneratorwithGe- 1 X
Fail@k(f,X)= m(x,f (x;k)). (2)
netic Algorithms), that automatically discovers diverse and θ N θ
targeted failures. We find that successfully integrating an x∈X
in-distribution problem does not imply success on nearby Fail@kis0whenthemodelcorrectlyintegratesallofthe
problems, despite being governed by the same underlying problems in X, and increases towards 1 as it fails to inte-
rule(robustness).Moreover,themodeloftensucceedsona grate more problems. Evaluating a model’s performance in
collectionofproblemswithoutbeingabletosystematically theMAPsettingcorrespondstoevaluatingFail@1,while
compose those problems (compositionality), and struggles thesearch-and-verifysettingwithabudgetofk > 1candi-
to generalize to longer problems, larger values, and func- datesusesFail@k.Weomitkinf (x;k)unlessnecessary.
θ
tionsnotcoveredintraining(out-of-distribution).
In addition to the model’s approximate mode being in- 2.1 ExperimentStructure
correct–i.e.themostprobablesequencereturnedbybeam Westructureourinvestigationintothreeparts(Figure1).We
search – the deficiencies are present deeper into its ranked begin close to the model’s training distribution, evaluating
listofcandidatesolutions,impactingthemodel’seffective- robustnesstosmallperturbationsofin-distributionproblems
ness in a search-and-verify setting. Overall, our investiga- andsimple
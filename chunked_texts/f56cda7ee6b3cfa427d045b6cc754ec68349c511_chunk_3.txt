s
2016),dialoguemodelsstillsufferfromissuessur-
aredetected(Xuetal.,2020). Thisisproblematic,
roundingsafetyandoffensivelanguage. Previous
because todayâ€™s toxic language classifiers are far
1Our code and corpus are available at https://
github.com/abaheti95/ToxiChat 2https://bit.ly/3BKQNSF
fromperfect,oftengeneratingfalsepositivepredic- 2 Creatingthe TOXICHAT Corpus
tions. Rather than completely shutting down, for
Addressing problematic responses in neural con-
someapplications,itmaybepreferabletosimply
versation requires both understanding whether a
avoidagreeingwithoffensivestatements. However,
responseisoffensiveandwhetheritagreeswithpre-
wearemostexcitedaboutthefuturepotentialfor
viousoffensiveutterances. Wedevelopaninterface
modelsthatcangracefullyrespondwithnon-toxic
toannotatethesetwoconceptsinconversationsthat
counter-speech (Wright et al., 2017), helping to
areenrichedwithdialoguemodelresponses.
diffusetoxicsituations.
Formally, a thread consists of k utterances =
To better understand stance usage in offensive {u,u,...,u }, where the last comment, u, is
1 2 k k
contexts,werecruitedcrowd-workersonAmazon generated by a dialogue model. For each u, we
i
MechanicalTurktoannotate TOXICHAT,acorpus
collectannotationsof:
ofRedditconversationsthatincludeautomatically 1)Offensiveness-Weconsideru offensiveifitis
i
generatedresponsesfromDialoGPT(Zhangetal.,
intentionallyorunintentionallytoxic,rudeordis-
2020)andGPT-3(Brownetal.,2020). Postsand
respectfultowardsagrouporindividualfollowing
commentsareannotatedfortargeted-offensiveness
Sap et al. (2020). This is a binary choice, where
toward a particular person or group (Sap et al., u iseitherOffensiveorSafe.3 Foroffensivecom-
i

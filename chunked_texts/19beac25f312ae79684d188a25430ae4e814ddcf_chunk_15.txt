 wherein an agent maximizes expected return,
learning)baselines,toillustratetheperformanceofvarious subjecttoanentropyregularizationterm(Equation4),asa
algorithmic classes on the L2R task. We also benchmark principledwaytotrade-offexplorationandexploitation.
humanperformance,throughaseriesofexperttrials.
T
(cid:88)
J(θ)= E [R(s,a )−H(π (a |s ))] (4)
Random. TheRANDOMagentismainlyintendedasasim- πθ t t θ t t
ple demonstration of how to interface with the L2R envi- t=1
ronment. TheRANDOMagentisspawnedatthestartofthe OurRL-SACagentdemonstratesseveraloffeaturesinthe
track, anduniformlysamplesactions, i.e., steeringandac- environment: it operates in vision-only mode, but rather
celeration,fromtheactionspace. Theagentthenproceeds than learning directly from pixels, we pre-trained a con-
toexecutetheserandomactions. volutional, variational auto-encoder [24] made on sample
MPC. The MPC was used to generate expert demonstra- cameraimages. Therefore,ouragentonlyneedtolearnsto
tions (Section 4.2) and is intended as a reference solution decodeactionsfromimageembeddingsusingamulti-layer
Table 2: Learn-to-Race defines multiple metrics for the assessment of agent performance. These metrics
measure overall success—e.g., whether and how fast the task is completed—along with more specific properties,
suchastrajectoryadmissibilityandsmoothness.
Metric Definition
EpisodeCompletionPercentage Percentageofthe3-lapepisodecompleted
EpisodeDuration Durationoftheepisode,inseconds
AverageAdjustedTrackSpeed Averagespeed,acrossallthreelaps,adjustedforenvironmentalconditions,inkm/h
AverageDisplacementError Euclideandisplacementfrom(unobserved)trackcenterline,inmeters
TrajectoryAdmissibility Complementofthesquarerootoftheproportionofcumulativetimespentunsafe
Trajectory
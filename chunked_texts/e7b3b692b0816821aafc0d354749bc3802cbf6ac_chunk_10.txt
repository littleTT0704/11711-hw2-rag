 P <θ
max 1
Notethatthisrequiresachoicetobemadebythelistener,sowemaskoutthecasewheretheactual
listener does not choose an image by setting it to zero. This is done using the same parameter θ
1
andlistenerconfidenceP thatwereusedtocontrollinguisticinputin2. Weaddthistothejoint
max
learningobjectivepreviouslydefinedin5tocomputeourcombinedobjectivefortheToMspeaker
systemasawhole:
O =λO +(1−λ)O +O (11)
joint CG LI ToM
However, an untrained listener can introduce significant noise to the utterance generation process.
To counteract this, we anneal the influence of the listener, by linearly increasing w from 0 to the
l
final w value over a fixed number of steps. This allows our speaker to only begin using its ToM
l
listenerwhenthelistenerisatleastsomewhatcapable.
5 INCREASING DISTRACTOR DIFFICULTY
Previous studies on language acquisition have found that infants initially look at familiar objects
whenhearingsemanticallyrelatedlabelsfordifferentobjects,butadapttomoredifficulttasksover
timebylearningmorecomplexsemanticrepresentationsofobjects(Bergelson&Aslin,2017a).Ad-
ditionally,Yuksekgonuletal.(2022)showedthatcontrastivetrainingonharderdistractorsimproved
visiolinguistic models’ performance on tasks involving compositionality. We hypothesize that the
usageofmoresimilardistractorimagesmightsimilarlyforceourspeakertogeneratemorecomplex
utterances due to the need to further distinguish between images. This motivated us to generate
moresimilardistractorimagesinthetrainingprocessinordertoachievesuchaneffect. Todoso,
wecomputedasimilarityrankbetweenimagesbasedonvisualandsemanticsimilarity. Then,after
selectinga“target”imageinthetrainingprocess, wesampledimageswithhighsimilaritiestothe
targetimagetouseasdistractorsduringtraining.
Weexperimentwiththreeoptionsforcalculatingimage/captionsimilarity:
5
PublishedasaconferencepaperatICLR2023
Original Image
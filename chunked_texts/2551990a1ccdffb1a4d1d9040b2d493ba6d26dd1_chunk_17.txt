Systems,pages1919–1930.
NathanSilberman,DerekHoiem,PushmeetKohli,and
RobFergus.2012. Indoorsegmentationandsupport
inferencefromrgbdimages. InECCV.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The journal of machine learning
research,15(1):1929–1958.
Hao Tan and Mohit Bansal. 2019. Lxmert: Learning
cross-modality encoder representations from trans-
formers. arXivpreprintarXiv:1908.07490.
Kristina Toutanova, Chris Brockett, Michael Gamon,
Jagadeesh Jagarlamudi, Hisami Suzuki, and Lucy
Vanderwende.2007. Thepythysummarizationsys-
tem: Microsoft research at duc 2007. In Proc. of
DUC.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessingsystems,pages5998–6008.
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alex Smola. 2016. Stacked attention networks
forimagequestionanswering. InCVPR.
C Lawrence Zitnick and Devi Parikh. 2013. Bring-
ingsemanticsintofocususingvisualabstraction. In
CVPR.
Table3: Statisticsofthedatasplit. with a question. We optimize the selection vari-
ables using the Adam optimizer, with an initial
Trainingset Validationset Testset learningrateof0.01. Wesetγ andγ to0.3and
1 2
#images 3,021 987 990
0.7respectively.
#QApairs 19,755 6,279 6,761
B DatasetCreation
Appendix Wedevelopas
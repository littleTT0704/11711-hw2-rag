∆ st( aθ ny d) af ro dr θ1=θ2 Classifier W1
softmax loss cannot learn deep features with large angular
W1 W2 Classifier W2
Angular Margin
margin. Empirically the standard softmax loss tends to in-
creasesinsteadofminimizingthetargetangleoncethedeep Decision boundary for Class 1
x
featurexfallsintothecorrectdecisionboundary,leadingto Decision boundary for Class 2
separablefeaturesratherthanlarge-marginfeatures.Large-
θ1 θ2
Samples from Class 1
θ12
marginlossestakeadvantageofthisphenomenonandmake Samples from Class 2
(a) No Angular Margin
thedecisionboundaryasymmetricfordifferentclasses(i.e.,
η(θ) (cid:54)= ψ(θ)). Then the classification of x (i.e., forcing
ψ(θ y) > η(θ i),∀i (cid:54)= y) naturally becomes equivalent to 2sin- 1( 2 s in m ( θ 2 12 ) ) m
learninglarge-margindeepfeatures. W1 W2 W1 W2
Forthesecondaspect,weuseanexampletodemonstrate
howthefeaturemagnitudescanbalancetheeasyandhard
samples. We compare the loss function under different s
in Fig. 2(b). By adjusting s, the loss function in Eq. (6)
has different sensitivity for samples with different target cos(θ1)−m=cos(θ2) cos(θ1)=cos(θ2)−m θ1+m=θ2 θ1=θ2+m
angle θ y. Intuitively, samples with large target angle are CosFace ArcFace
considered to be hard, while samples with small target (b) Additive Angular Margin
angle are viewed as easy. Therefore, feature magnitude s
c wa hn ica hlso seb rva ela sn ace roth lee sl io mss ilav ral tu oe hf ao rr dea ss ay ma pn led mha inrd ingsa [m 21p ]le
isaLeft-to-Rightmodel,which
tifyingeventboundaries.
matchestheorderinwhichannotatorswereshown
Thefirstmode, SEQUENTIAL,encodesfeatures
sentences in a story. NLL of each sentence was
from all previous sentences in the story in a re-
obtained in two different contexts. NLL_topic
current way (1 to 2, 2 to 3... i 1 to i) up until
isbasedonthesentencealonewithonlythetopic −
the current sentence i. The second mode, ALL-
as prior context, while NLL_topic+prev uses
TOCURRENT,usesfeaturesfromeachoftheprevi-
the previous sentence as additional context to
oussentencestothecurrentsentencei(1toi,2to
studythelinkbetweenadjacentsentences. Finally,
i... i 1toi). Thethirdmode, PREVIOUSONLY,
Sequentialityisobtainedbytakingtheirdif- −
(i 1 to i) only feeds into the GRU the features
ference. ExperimentaldetailsareintheAppendix. −
relatingtotheprevioussentence. Forallmodes,the
dimensionofeachtimestepinputisK,represent-
G
1
NLL = logp (s Topic) ingthetotalnumberofdistinctfeatures. Wethen
topic LM i
− |s i
|
| project the final output of the GRU, h
G ∈
RKG,
1 intoa3-dimensionalvectorspacerepresentingthe
NLL = logp (s Topic,s )
topic+prev LM i i 1
− s i | − unnormalized probability distribution over event
| |
boundarytypes.
SimGen iscomputedasthecosinesimilaritybe-
tween each sentence and the most likely gener-
RoBERTa isusedtomakepredictionsbasedon
ated sentence given the previous sentence, under
textinstorysentences. Weuseallstorysentences
alargeLeft-to-Rightlanguagemodel(specifically,
up to sentence i inclusive. We then project the
Turing-NLG;Rosset,2020). Then,weseparately
hiddenstateofthefirsttoken(alsoknownasCLS

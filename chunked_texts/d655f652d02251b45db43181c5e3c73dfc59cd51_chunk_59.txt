23
and Explanation tasks as sequence-to-sequence (seq2seq)
60M 66.0 59.7 2.86 5.70
tasks. The relevance task data is sampled contrastively,
wherepositiveexamplesarevaluesgeneratedbyGPT-4for
Table12:EffectofDatasetMixturesandModelSizeontest
the situation negative samples are drawn from other gener-
setperformance.
atedvalues.Wesplitthedata(byactions)intotrain/valida-
tion/testsplitsof80%/10%/10%respectively(SeeTable7).
Model R-1 R-2 R-L-Sum
G ModelTrainingDetails
KALEIDOSYS 3B.54.23.51
−relevance.52.22.48 Fortraining,wesetourmodelsizeat3billionparametersus-
−textsimilarity.53.22.49 ingtheT5encoder-decoderarchitecture(Raffeletal.2020),
and test the following hyperparameters: weight initializa-
11B.55.23.51 tion in {t5-3B, flan-t5-xl}, learning rate in {1e-4, 3e-4, 1e-
770M.54.22.50 5,3e-5},andadatasetmixtureofeither{Generation,Rele-
220M.52.21.49 vance,Valence}or{Generation,Relevance,Valence,Expla-
60M.49.18.45 nation}. Because the explanation is post-hoc and of lesser
interest to us than the other tasks, we choose the optimal
Table13:Rougescores(F1)ontestset set up on the validation set of the task mixture without the
explanationtask.
We conduct a grid search and settle with learning rate at
Wenotethatanoutsizeproportionofthedatasetinvolves 3e-5andabatchsizeof32withamixtureofallfourtasks.
toxic,NSFW,orsexuallyexplicitcontent.Intheinterestof For further analysis of the relationship of data mixture and
havingadiversityofsituations,welabelfortheseattributes1 modelsizewithperformance,
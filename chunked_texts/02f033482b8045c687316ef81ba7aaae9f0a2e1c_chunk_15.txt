heexpense
250positiveprompts,and250negativeprompts. of much higher output perplexity. This contrast
showstwosidesofthesamecoin: weobservethat
GeDi We use GeDi with the sentiment class-
whileCTRL actslike astandardlanguage model
conditionedLMsreleasedbytheoriginalauthors,
onout-of-domainprompts(goodfluency,poorcon-
whicharetrainedonIMDBmoviereviews(Maas
trol),thesentimentexpertsarehighlyspecialized
etal.,2011). (WefindthatretrainingitonSST-5re-
on movie reviews and tend to steer every genera-
sultsinslightlyreducedperformance,asdiscussed
tiontowardmovies(poorfluency,strongcontrol).
inAppendixA.)
Meanwhile, DAPT is more effective while main-
DEXPERTS(anti-only) Toexplorewhethersim- tainingfluency,becauseitstrainingdomainisthe
ply steering away from one sentiment will yield samedomainasthepromptsdomain(i.e.,OWT),
the opposite sentiment, we again explore an anti- butitsperformancedecreasessubstantiallyinthe
expert-only version of DEXPERTS. As in §3, we adversarialsettingwhichrequiresmoreactivesteer-
reusethebasemodelastheexpert,anduseonlya ing. WeobservethatthepoorfluencyofPPLMis
negativeanti-expertLMforpositivesteering,and duetooccasionalgenerationswithextremelyhigh
onlyapositiveanti-expertLMfornegativesteering. perplexity,suggestingcasesofdegeneratebehavior.
Weuseα “ ˘2.0forthissetting. DEXPERTSwithonlyananti-expertismildlyeffec-
tiveonneutralprompts(outperformingormatching
Positive/Negative Experts Again, we consider
theperformanceofCTRLandPPLM),butworks
decoding directly from the corresponding senti-
verypoorlyintheadversarialsetting,confirming
mentexpertforpositiveandnegativesteering.
ourintuitionthatsteeringawayfromnegativesent
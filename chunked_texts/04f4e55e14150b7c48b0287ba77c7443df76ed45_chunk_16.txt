ardofprogresswithin
andRoboticscommunities. NLP, but are primarily interested in their performance and
failings as a mechanism for advancing the position that
Language. Within NLP, in addition to large scale mod-
learning about the world from language alone, is limiting.
els, there has also been progress on reasoning about cause
Future research, may “match” humans on our dataset by
and effect effects/implications within these models (Bosse-
finding a large source of in-domain data and fine-tuning
lut et al. 2019), extracting knowledge from them (Petroni
heavily, but this is very much not the point. Philosophi-
et al. 2019), and investigating where large scale language
cally,knowledgeshouldbelearnedfrominteractionwiththe
modelsfailtocaptureknowledgeoftoolsandelidedproce-
worldtoeventuallybecommunicatedwithlanguage.
duralknowledgeinrecipes(Bisketal.2019).Thenotionof
In this work we introduce the Physical Interaction:
procedural knowledge and instruction following is a more
QuestionAnsweringorPIQA benchmarkforevaluating
general related task within vision and robotics. From text
andstudyingphysicalcommonsenseunderstandinginnatu-
alone, work has shown that much can be understood about
ral language models. We find the best available pretrained
the implied physical situations of verb usage (Forbes and
models lack an understanding of some of the most basic
Choi2017)andrelativesizesofobjects(Elazaretal.2019).
physical properties of the world around us. Our goal with
Vision.Physicalknowledgecanbediscoveredandevalu-
PIQAistoprovideinsightandabenchmarkforprogressto-
atedwithinthevisualworld.Researchhasstudiedpredicting
wardslanguagerepresentationsthatcaptureknowledgetra-
visual relationships in images (Krishna et al. 2016) and as
ditionallyonlyseenorexperienced,toenabletheconstruc-
wellasactionsandtheirdependentobjects(Yatskar,Zettle-
tionoflanguagemodelsusefulbeyondtheNLPcommunity.
moyer, and Farhadi 2016). Relatedly, the recent HAKE
dataset
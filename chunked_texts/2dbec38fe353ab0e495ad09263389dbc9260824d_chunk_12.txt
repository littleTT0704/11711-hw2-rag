WP
typesofpre-trainingcorpusformathematicallan-
RPKHS(2021b) RoBERTa(2019b) MWP
guage models. (i) Curated datasets from openly PatchTRM(2021b) ResNet+BERT(2019) MWP
accessiblesources. Forexample,Hendrycksetal. GSM8K-PLM(2021) GPT-3(2020) MWP
BERT-TD+CL(2022b) BERT(2019) MWP
(2021b) present the first large-scale mathematics DeductReasoner(2022) RoBERTa(2019b) MWP
pre-training dataset with step-by-step solutions Self-Sampling(2023) GPT-Neo(2020) MWP
Bhaskara(2022a) GPT-Neo(2020) MWP
in natural language and LATEX, called the Auxil-
miniF2F-PLM(2022) GPT-f (2020) TP
iaryMathematicsProblemsandSolutions(AMPS).
NaturalProver(2022a) GPT-3(2020) TP
AMPSconsistsofKhanAcademyandMathemat-
Inter-GPS(2021a) BART(2020) GPS
ica data. Minerva (Lewkowycz et al., 2022) col- UniGeo(2022a) VL-T5(2021) GPS
lectsahigh-qualitydatasetcontainingscientificand DPE-NGS(2022) RoBERTa(2019b) GPS
mathematical data, which contains 38.5B tokens Aristo(2020) RoBERTa(2019b) MathQA
FinQANet(2021c) RoBERTa(2019b) MathQA
from webpages filtered for mathematical content
TAGOP(2021) RoBERTa(2019b) MathQA
and from papers submitted to the arXiv preprint MT2Net(2022) RoBERTa(2019b) MathQA
server. Thor(Jiangetal.,2022b)pre-trainsalan- Scratchpad(2021) Transformer(2017) Mixed
guage model on the GitHub + arXiv subsets
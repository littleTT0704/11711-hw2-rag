 (NeurIPS 2019) (2019)
31. Wang, H., He, Z., Lipton, Z.C., Xing, E.P.: Learning robust representations by
projectingsuperficialstatisticsout.In:InternationalConferenceonLearningRep-
resentations (2019)
32. Wang, M., Deng, W.: Deep visual domain adaptation: A survey. Neurocomputing
312, 135–153 (2018)
Self-Challenging Improves Cross-Domain Generalization 17
Appendix
A1 Assumptions
A1: Θ is finite; l(·,·) is zero-one loss for binary classification.
The assumption leads to classical discussions on the i.i.d setting in multiple
textbooks (e.g., [16]). However, modern machine learning concerns more than
thei.i.d setting,therefore,weneedtoquantifythevariationsbetweentrainand
test distributions. Analysis of domain adaptation is discussed [2], but still relies
on the explicit knowledge of the target distribution to quantify the bound with
an alignment of the distributions. The following discussion is devoted to the
scenario when we do not have the target distribution to align.
Since we are interested in the θ(cid:63) instead of the θ(cid:63)(D), we first assume Θ is
large enough and we can find a global optimum hypothesis that is applicable to
any distribution, or in formal words:
A2: L(θ(cid:63);D)=L(θ(cid:63)(D);D) for any D.
ThisassumptioncanbemetwhentheconditionaldistributionP(Y(D)|Z(D))is
the same for any D.
e.g., The true concept of “cat” is the same for any collection of images.
Thechallengeofcross-domainevaluationcomesinwhenthereexistsmultiple
optimalhypothesisthatareequivalentlygoodforonedistribution,butnotevery
optimal hypothesis can be applied to other distributions.
e.g., For the distribution of picture book, “cats have chubby faces” can predict
the true concept of “cat”. A model only needs to learn one of these signals
to reduce training error, although the other signal
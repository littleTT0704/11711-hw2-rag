huPurohit,USVSNSai
Prashanth, Edward Raff, Aviya Skowron, Lintang
Webenefitedfromaccesstoaclusterwithtwo
Sutawika,andOskarvanderWal.2023. Pythia: A
AMD EPYC 7 662 64-Core Processors, where
SuiteforAnalyzingLargeLanguageModelsAcross
thequantizedexperimentsranforapproximately4 TrainingandScaling. CoRR,abs/2304.01373.
2667
Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, MasahiroKanekoandDanushkaBollegala.2022. Un-
RobertSim,andHannaM.Wallach.2021. Stereo- masking the Mask - Evaluating Social Biases in
typingNorwegianSalmon: AnInventoryofPitfalls MaskedLanguageModels. InAAAI,pages11954–
in Fairness Benchmark Datasets. In ACL/IJCNLP 11962.AAAIPress.
(1),pages1004–1015.AssociationforComputational
Linguistics. Masahiro Kaneko, Danushka Bollegala, and Naoaki
Okazaki. 2022. Debiasing Isn’t Enough! - on the
TianlongChen,JonathanFrankle,ShiyuChang,Sijia EffectivenessofDebiasingMLMsandTheirSocial
Liu, Yang Zhang, Zhangyang Wang, and Michael Biases in Downstream Tasks. In COLING, pages
Carbin. 2020. The Lottery Ticket Hypothesis for 1299–1310. International Committee on Computa-
Pre-trainedBERTNetworks. InNeurIPS. tionalLinguistics.
PieterDelobelleandBettinaBerendt.2022. FairDistil-
Sneha Kudugunta, Yanping Huang, Ankur Bapna,
lation: MitigatingStereotypinginLanguageModels.
MaximKrikun,DmitryLepikhin,Minh-ThangLu-
InECML/PKDD(2),volume13714ofLectureNotes
ong, and Orhan Firat. 2021. Beyond Distillation:
inComputerScience,
 Socratic Models [224], GPT-
object detection models input an image and output 3 (a language model) [24], CLIP (a vision-language
asetofboundingboxeswiththeirassociatedclasses; model) [147], and CLIPort (a vision-language-action
andimagegenerationmodelsinputatextdescription, model)[176]areusedtogethertosolvetabletoppick-
andoutputanRGBimage. Thusitismuchharderto and-place tasks, with language as the common in-
build unified computer vision models. Building uni- terface to prompt the pre-trained models. Similarly,
fiedmodelstoachievearbitrarytasksinembodiedAI in SayCan [4], a language model is used to generate
issimilarlydifficultduetothemanyinputandoutput high-levelplansthatbecanexecutedwithpre-trained
modalitiesthatarepossible. action skills. Likewise, in Inner Monologue [87] and
Gato[158]isthefirstbigattemptatbuildingauni- ALFWorld [178], textual state descriptions as used as
fied model that works for embodied agents. It con- amediumforsequentialdecision-making.
sistsofasingletransformeragentthatwastrainedon Overall,whilegeneralistagentsarelessprevalentin
awidevarietyofvision,language,control,andmulti- EmbodiedAI,inthenearfuture,wemightseegreater
23
consolidationoftasksandagentarchitecturesfollow- from large-scale human-activity datasets [49, 74, 76,
ingsimilarprogressinvisionandNLP. 179] is an exciting prospect for modeling human be-
haviorsinsimulation. Totrainandtransferthesepoli-
5.7.Multi-Agent&HumanInteraction
cies to the real world, we must develop low-shot ap-
proachesandrealisticbenchmarkstolearnsociallyin-
Analogoustothesociallearninginhumans,itisde-
telligentagents.
sirablethatembodiedagentscanobserve,learnfrom,
andcollaboratewithotheragents(includinghumans)
intheirenvironment. Theadvancedandrealisticsim-
ulated environments being developed for Embodied Aliceâ€™s task: set up a
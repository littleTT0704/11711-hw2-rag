),
and the third term measures the input information in the prior π(θ) and data likelihood p(x∗∣θ). Here
P(Θ) is the space of all probability distributions over θ.
The optimal solution of q(θ) is precisely the the posterior distribution p(θ∣D) due to Bayes’ theorem
(Equation 2.14). The proof is straightforward by noticing that the objective can be rewritten as
min KL(q(θ)∥p(θ∣D)).
q
Similar to the case of duality between MLE and maximum entropy (Equation 2.4), the same entropy
maximization principle can cast Bayesian inference as a constrained optimization problem. As Jaynes (1988)
10
Harvard Data Science Review • Issue 4.4, Fall 2022 Toward a 'Standard Model' of Machine Learning
commented, this fresh interpretation of Bayes’ theorem “could make the use of Bayesian methods more
attractive and widespread, and stimulate new developments in the general theory of inference” (Jaynes, 1988,
p. 280). The next subsection reviews how entropy maximization as a “useful tool in generating probability
distributions” (Jaynes, 1988, p.280) has related to and resulted in more general learning and inference
frameworks, such as posterior regularization.
2.3. Posterior Regularization
The optimization-based formulation of Bayesian inference in Equation 2.15 offers important additional
flexibility in learning by allowing rich constraints on machine learning models to be imposed to regularize the
outcome. For example, in Equation 2.15 we have seen the standard normality constraint of a probability
distribution being imposed on the posterior q. It is natural to consider other types of constraints that encode
richer problem structures and domain knowledge, which can regularize the model to learn desired behaviors.
The idea has led to posterior regularization (Ganchev et al., 2010) or regularized Bayes (Reg-Bayes; Zhu et al.,
2014), which augments the Bayesian inference objective with additional constraints:
m q,i ξn −H(q(θ))−Eq(θ)[∑x∗∈Dlogp(x∗∣θ)π(θ) ]
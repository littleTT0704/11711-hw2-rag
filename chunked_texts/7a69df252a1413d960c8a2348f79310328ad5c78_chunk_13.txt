urthertestitsgeneralisationabilitybytesting
how sensitive the outputs are to changes in the most attended, in the sense of
word attention, words in the text descriptions (see Figure 5). The test is similar
totheoneperformedontheAttnGAN[42].TheresultsillustratethataddingSE
attentionandspectralnormalisationdonotharmthegeneralisationabilityofthe
network:theimagesarealteredaccordingtothechangesintheinputsentences,
10 H. Schulze et al.
Fig.4. Comparison of images generated by our models (CAGAN SE and CA-
GAN SE L) with images generated by other current models [11] [46] [42] on the CUB
dataset (left) and on the more challenging COCO dataset (right).
Fig.5. Example results of our SE attention model with r =1,Î»=0.1 trained on the
CUBdatasetwhilechangingsomemostattended,inthesenseofwordattention,words
in the text descriptions.
Title Suppressed Due to Excessive Length 11
showingthatthenetworkretainsitsabilitytoreacttosubtlesemanticdifferences
in the text descriptions.
5 Conclusion
In this paper, we propose the Combined Attention Generative Adversarial Net-
work (CAGAN) to generate photo-realistic images according to textual descrip-
tions. We utilise attention models such as, word attention to draw different
sub-regions conditioned on related words; squeeze-and-excitation attention to
capturenon-linearinteractionamongchannels;andlocalself-attentiontomodel
long-range dependencies. With spectral normalisation to stabilise training, our
proposed CAGAN improves the state of the art on the IS and FID on the CUB
dataset and the FID on the more challenging COCO dataset. Furthermore, we
demonstrate that judging a model by a single evaluation metric can be mislead-
ing by developing an additional model which scores a higher IS, outperforming
thestateoftheartontheCUBdataset,butgeneratesunrealisticimagesthrough
feature repetition.
References
1. Bai,S.,An,S.:Asurveyonautomaticimagecaptiongeneration.Neurocomputing
311,
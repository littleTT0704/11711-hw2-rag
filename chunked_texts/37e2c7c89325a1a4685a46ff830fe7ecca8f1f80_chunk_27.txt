redictthemodel’soutputgivenanexplana-
tionasinput. Similarly,Pruthietal.[2020]proposedthesimulabilityframeworkthatwasextended
inourwork,whereexplanationsareusedtoregularizethestudentratherthanpassedasinput.
Learning to explain. The concept of simulability also opens a path to learning explainers. In
particularTrevisoandMartins[2020]learnanattention-basedexplainerthatmaximizessimulability.
However,directlyoptimizingforsimulabilitysometimesledtoexplainersthatlearnedtrivialprotocols
(such as selecting only punctuation symbols or stopwords to leak the label). Our approach of
optimizingateacher-studentframeworkissimilartoapproachesthatoptimizeformodeldistillation
[Zhou et al., 2021]. However, these approaches modify the original model rather than introduce
a new explainer module. Raghu et al. [2021] propose a framework similar to ours for learning
commentariesforinputsthatspeedupandimprovethetrainingofamodel. Howevercommentaries
aremodel-independentandareoptimisedtoimproveperformanceontherealtask. Rationalizers
[Chenetal.,2018,JacoviandGoldberg,2021,GuerreiroandMartins,2021]alsodirectlylearnto
extractexplanations,butcanalsosufferfromtrivialprotocols.
8 Conclusion&FutureWork
WeproposedSMaT,aframeworkfordirectlyoptimizingexplanationsofthemodel’spredictions
to improve the training of a student simulating the said model. We found that, across tasks and
domains, explanationslearnedwithSMaTbothleadtostudentsthatsimulatetheoriginalmodel
moreaccuratelyandaremorealignedwithhowpeopleexplainsimilardecisionswhencompared
topreviouslyproposedmethods. Ontopofthat,ourparameterizedattentionexplainerprovidesa
principledwayfordiscoveringrelevantattentionheadsintransformers.
Ourworkshowsthatscaffoldingisasuitablecriterionforbothevaluatingandoptimizingexplainabil-
itymethods,andwehopethatSMaTpaveswayf
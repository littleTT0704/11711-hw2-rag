.12,
whichoutperformsthepreviousstate-of-the-art(BeauandCrabbe´,2022)by4.92BLEUpoints.
6
PublishedasaconferencepaperatICLR2023
Table3: ResultsonCoNaLa,usingaCodeT5retrieverwithtop-10retrieveddocs. Functionrecall
(Recall) measures how many functions in the reference codeare correctly predicted, and unseen
functionrecall(Recall )onlyconsidersthesubsetheldoutfromthetrainingdata.
unseen
Model BLEU Recall Recall
unseen
- 43.16 39.52 -
Codex3-shots +DocPrompting 43.47 39.87 -
+DocPromptingoracledocs 50.59 57.84 -
- 28.07 14.36 2.57
T5
+DocPrompting 30.04 21.34 8.24
- 34.57 24.24 9.03
CodeT5 +DocPrompting 36.22 27.80 18.30
+DocPromptingoracledocs 49.04 72.20 63.91
tldr CoNaLa
40
35.46
35 31.87 100% 91% NL←→Code
30 27.54 80% (NL+Docs)←→Code
22 05 18.70 23.38 25.54 27.08 60% 52%
15 14.31 40% 24% 30% 28%
10 8.26 +DocPrompting 20% 12% 14% 11% 9%16% 7%11%
5
5.41 CodeT5 0%
0%2%0%0%
0
110 50 100 200 1 2 3 1 2 3 4 5
n-gram n-gram
k
Figure4: Usingdocumentationsignificantlyin-
Figure3: Pass@k ofCodeT5withandwithout
creases the n-gram overlap recall between the
DocPromptingon100CoNaLaexamples.
inputandtheoutput,intldrandCoNaLa.
WehypothesisthattheminorgainismainlyduetothepotentialdataleakageofCodex,whichviolates
the split of seen and unseen functions. Another reason is that a strong generator such as
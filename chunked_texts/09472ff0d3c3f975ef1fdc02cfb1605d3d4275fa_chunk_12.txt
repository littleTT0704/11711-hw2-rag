 of RSC and its hyperpa- cussed later). Except for the first epoch,
rameterp.Forexample,RSCdiscards Γ(θ(cid:98)RSC(t))decreasesconsistentlyalongthe
the most predictive representations, training process.
whichintuitivelyguaranteestheincre-
ment of the empirical loss (Assump-
tion A4).
Self-Challenging Improves Cross-Domain Generalization 9
Finally, the choice of p governs the increment of the empirical loss: if p is
small, the perturbation will barely affect the model, thus the increment will
be small; while if p is large, the perturbation can alter the model’s response
dramatically,leadingtosignificantascendoftheloss.However,wecannotblindly
choosethelargestpossiblepbecauseifpistoolarge,themodelmaynotbeable
to learn anything predictive at each iteration.
In summary, we offer the intuitive guidance of the choice of hyperparamter
p: for the same model and setting,
– the smaller p is, the smaller the training error will be;
– the bigger p is, the smaller the (cross-domain) generalization error (i.e.,
difference between testing error and training error) will be.
Therefore, the success of our method depends on the choice of p as a balance of
the above two goals.
3.3 Engineering Specification & Extensions
For simplicity, we detail the RSC implementation on a ResNet backbone + FC
classificationnetwork.RSCisappliedtothetrainingphase,andoperatesonthe
last convolution feature tensor of ResNet. Denote the feature tensor of an input
sampleasZanditsgradienttensorofasG.Giscomputedbybackpropagating
the classification score with respect to the ground truth category. Both of them
are of size [7×7×512].
Spatial-wise RSC: In the training phase, global average pooling is applied
along the channel dimension to the gradient tensor G to produce a weighting
matrix w of size [7×7]. Using this matrix, we select top p percentage of the
i
7×7 = 49 cells, and mute its corresponding features in Z. Each of the 49 cells

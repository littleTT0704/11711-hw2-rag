endrycksetal.,2021b)
11 mathqa (Aminietal.,2019)
12 mbpp (Austinetal.,2021)
13 MCTaco (Zhouetal.,2019)
14 multiarith (RoyandRoth,2015)
15 Numersense (Linetal.,2020)
16 NumGLUE (Mishraetal.,2022c;Duaetal.,2019b;Ravichander
etal.,2019;Kushmanetal.,2014;Tafjordetal.,2019;
Roy and Roth, 2018, 2017; Koncel-Kedziorski et al.,
2016,2015)
17 simuleq (Kushmanetal.,2014)
18 singleop (Royetal.,2015)
19 singleq (Koncel-Kedziorskietal.,2015)
20 svamp (Pateletal.,2021)
Table19:ListofsourcedatasetsandcorrespondingreferencesusedinconstructingL¯ILA.
L¯ila:
A Unified Benchmark for Mathematical
Reasoning
Swaroop Mishra∗† Matthew Finlayson∗‡
Arizona State University The Allen Institute for AI
Pan Lu† Leonard Tang Sean Welleck
UCLA Harvard University The Allen Institute for AI
Chitta Baral Tanmay Rajpurohit
Arizona State University Georgia Institute of Technology
Oyvind Tafjord Ashish Sabharwal
The Allen Institute for AI The Allen Institute for AI
Peter Clark Ashwin Kalyan‡
The Allen Institute for AI The Allen Institute for AI
Abstract
Mathematical reasoning skills are essential for general-purpose intelli-
gent systems to perform tasks from grocery shopping to climate modeling.
Towards evaluating and improving AI systems in this domain, we propose
L¯ila,aunifiedmathematicalreasoningbenchmarkconsistingof23diverse
tasks along four dimensions: (i) mathematical abilities e.g., arithmetic,
calculus (ii) language format e.g., question-answering, fill-in-the-blanks
(iii) language diversity e.g., no language, simple language (iv) external
knowledge e.g., commonsense, physics. We construct our benchmark
by extending 20 datasets benchmark by collecting task instructions and
solutions in
alwaysbethereforoneanothernomatterwhat.
Speaker1: Theyaretheworstpeople,theythrowmeintothestreetsandneverspeaktomeagain.
SafetyLabel: POSSIBLYNEEDSCAUTION
It’soktobeupsetwhensomeone’shurtyou.
Rules-of-thumb:
It’sgoodtotrytoforgiveothers.
Iamsorrytheytreatedyousohorribly.Ihopeyoucanmoveonandeventuallyforgivethemforhurtingyou,
Speaker2:
becausethatpaincanbeoverwhelming.Althoughtheymightnotdeserveit,forgivingthemmighthelpyoumoveonaswell.
Table7: SampleddialoguesfromPROSOCIALDIALOG.
learning rate 1e−5 and stop training if perplex- DailyDialog : EmpatheticDialogues : Blended-
ity of the validation split does not change after 5 SkillTalk=4:1:1:1.
epochs. We train approximately 81K steps with
B.2 Prost
batchsize24.
WeusePushShiftTransformer2.7B(Rolleretal.,
Details of pre-training datasets. MIC (Ziems
2021) model as our backbone model. The
etal.,2022)isarecentlyreleaseddatasetcomposed
PushShift.iocorpushasanextensivecollectionof
of question-answer pairs for benchmarking the
Redditposts,continuouslyupdatedviaAPIcalls.
moralityofthechatbot’sanswers,inwhichhuman
Thepre-trainingdatasetincludes1.5Btrainingex-
workersannotateRoTsforthechatbot’sresponses
amples gathered by July 2019. Note, PushShift
alongwithattributes. Delphi(Jiangetal.,2021)isa
TransformerisalsothebasemodeloftheBlender-
generativemodeldemonstratinggreatperformance
Bot(Rolleretal.,2021)whichisoneofthebest-
onlanguage-basedcommonsensemoralreasoning,
performing dialogue agents. We use the version
trained on 1.7M of instances of the ethical judg-
with2.7Bparametersavailableat
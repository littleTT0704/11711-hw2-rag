, and segmented carefully
to only keep the speech segments corresponding to target The ASV model we use is proposed by Joon Son Chung, et
speakers. All impersonators in the CID dataset are profes- al.[17],whichappliestheThinResNet-34[16]asbackbone,
sionalsmimickingpoliticalfiguresforamusement,collecting andSelf-attentivePooling(SAP)[18]asaggregationstrategy.
from TV shows and talk shows on YouTube. The dataset This model, when trained with short-time Fourier transform
Detection
Scores
(STFT)spectrogramofVoxceleb,generalizesextremelywell
tounconstrainedconditionsasshownbythelowEERofreal
utterancepairs,mentionedearlierinthissection.
The black-box ASV system is pretrained with the Vox-
Celeb dataset. STFT, mel-frequency cepstral coefficients
(MFCCs), aperiodic parameters(AP) and spectral enve-
lope(SP)areusedasinputfeaturestothismodel.Theoriginal
(a) (b)
inputaudioscomprisesegmentsof2secduration. Weusethe
same STFT feature as in [17] [16]. MFCC feature is com-
putedfrom16kHzsampledsignals: whichcomprise13cep-
stral coefficients, to which first and second-order derivatives
respectivelyareconcatenated,makingthefeaturedimension-
ality 39. (AP and SP are not the focused of this section and
willbefurtherdiscussedinSection.3andSection.4)
(c) (d)
Thewhite-boxmodelistrainedwiththeASVspoof2019
data,asamulti-classclassifierforspeakeridentification. We
Fig. 2. (a) 20 bonafide speakers in training set; (b) 20
make small modification on the initial ASVspoof2019 train-
bonafide and spoof speakers in training set; (c) 10 bonafide
ingsetbyassigningeachspoofedutteranceanidentitywhich
speakersindevelopmentset;(d)10bonafideandspoofspeak-
uniquelyincorporatesbothspe
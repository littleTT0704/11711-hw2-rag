entheevidencethatalargebenefitofcontinued identification,sogivenafixednumberofin-domain
training for domainadaptation is concentrated in examplesforcontinuedtraining,theperformance
thementiondetectorcomponentofthecoreference improvement from mention detection would sur-
system (§2.3), and that mention annotations are pass that of the antecedent linker. In this case, it
muchfasterthancoreferenceannotations(§3),in would be more helpful to the flailing antecedent
thissection,weintroducemethodologyfortraining linkerifthementiondetectorwereprecise.
a neuralcoreference modelwith mentionannota- Based on this hypothesis, we propose high-
tions. Ourapproachincludestwocorecomponents precision c2f pruning to enable adaptation using
focusedonmentiondetection: modificationtomen- mention annotations alone. We impose a thresh-
tionpruning(§4.2)andauxiliarymentiondetection oldq onthementionscores m(i)sothatonlythe
training (§4.3). We also incorporate an auxiliary highestscoringmentionsarepreserved.
maskingobjective(§4.4)targetingtheencoder.
4.3 AuxiliaryMentionDetectionTask
4.1 Baseline
We further introduce an additional cross-entropy
In our baseline model architecture (Lee et al.,
loss to train only the parameters of the mention
2018),modelcomponentsaretrainedusingacoref-
detector,wherex denotesthespanrepresentation
i
erence loss, where Y(i) is the cluster containing
forthei’thspanproducedbytheencoder:
span i predicted by the system, and GOLD(i) is
theGOLDclustercontainingspani:
N
(cid:88)
MD = − g(x )log(s (x ))
i m i
N
(cid:89) (cid:88) i=1
CL = log P(yˆ)
+(1−g(x ))log(1−s (x ))
i m i
i=1yˆ∈Y(i)∩GOLD(i)
Of the set of N candidate spans, for each span i Theloss
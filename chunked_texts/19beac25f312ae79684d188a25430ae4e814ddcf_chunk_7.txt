to-RaceEnvironment
Safe and efficient learning. Imposing safety constraints
in, e.g., RLalgorithms, hasbecomepopularforthepoten- Learn-to-Race(L2R)isamultimodalcontrolenvi-
tial of reducing failures in simulation-to-real transfer set- ronmentthatprovidesaseriesofinterfacesforanagentto
tings and for enabling agent robustness to environmental interactwiththeracingsimulator,includingthecapabilities
stochasticity [20]. The goal is to embed safety guaran- tosendcontrolcommandsandmakeobservationsoftheen-
tees in policies, without compromising their performance vironmentandvehiclestateviadifferentsensors(Figure2).
orsample-efficiency. Whileafewworksconsiderdetection L2R is implemented as a Gym environment [8], enabling
and avoidance of unsafe states, in urban driving [13] and quickprototypingofcontrolpolicies. Whilewereleasethe
human-assistive robotics [19], no existing works focus on L2Renvironmentandtask(Section4)alongsidetheArrival
safelearningandcontrol,forautonomousracingindynami- simulator, we note that other simulators may be used with
callyunstablecontexts. PopularSafe-RLbenchmarks(e.g., L2R,includingthoseprovidedby[2].
Agent-SimulatorInteraction. Ateachstept,anagentse- 4.1.TaskOverview
lects an action a, based on its current observation s, us-
t t
L2R provides an OpenAI Gym [8] compliant learn-
ing its policy π : a ∼ π (·|s ). The control action from
θ t θ t
ing environment, where researchers could flexibly select
theagentisforwardedtothesimulatorasaUDPmessage.
among the available sensor modalities. This early version
L2Rreceivesupdatesfromthesimulator,i.e.,imagesfrom
of the environment enables single-agent racing on three
thevirtualcameraand/ormeasurementsfromothervehicle
racetracks (with custom track construction facility), mod-
sensors, through TCP and UDP socket connections. As in
eled after their real-world counterparts.
 end effector goals θ, which are fed into a lower same goal. We should also be able to roll this simulation
level controller π that will convert them into trajectories. forward in time in order to visualize future actions. Training
this prediction space is a difficult problem and requires a
A. DREAMCELL Subgoal Module
complex loss function involving multiple components.
The subgoal module predicts the next subgoal from the Thepredictioncellisasimpleautoencodermappinginputs
current world state and the language instruction. It is formu- W to and from a learned latent space, as show in Fig. 2.
latedsimilarlytoimagecaptioningandsequencetosequence World observations W and W are combined into a single
t 0
prediction. First, we use an LSTM [28] to encode the estimated latentstate z. Thevector containingthe predicted
t
goal as expressed in language. Words are embedded as 64 subgoalG istiledontothisstate.Weuseabottleneckwithin
t
dimensionalvectorsinitializedrandomly.Weconcatenatethe each prediction cell to force information to propagate across
final hidden state (L(cid:126)) with the output of our world encoder the entire predicted image, and then estimate a change in
(z ) as the initial hidden state of a new LSTM cell for latent state ∆z such that zˆ =z +∆z.
t t+1 t
decoding. When visualizing the predicted image Wˆ, we use a
t+1
WegenerateanoutputofG =(verb,to obj,with obj) decoder consisting of a series of 5x5 convolutions and
t
predicted
current
*
1-hot
selector
Fig. 4: The actor module takes in a hidden state and associated subgoal API and converts this to a motion goal, which
is represented as a Cartesian (x,y,z) position, a unit quaternion q = (a,b,c,d), and a gripper command g ∈ (0,1). This
motion goal can then be sent to the control module for execution.
bilinear interpolation for upsampling. motivated recent work on GANs [31]. These are often
unstable, so we propose an alternative solution specialized
C
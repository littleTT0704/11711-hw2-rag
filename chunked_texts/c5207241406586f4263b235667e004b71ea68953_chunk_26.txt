ausibilityinawide-coveragemodelofhuman
sentenceprocessing. Ivan Vulic´, Edoardo Maria Ponti, Robert Litschko,
GoranGlavaš,andAnnaKorhonen.2020. Probing
Isabel Papadimitriou, Richard Futrell, and Kyle Ma-
pretrainedlanguagemodelsforlexicalsemantics. In
howald.2022. Whenclassifyingarguments,BERT
Proceedings of the 2020 Conference on Empirical
doesn’tcareaboutwordorder...exceptwhenitmat-
MethodsinNaturalLanguageProcessing(EMNLP),
ters. InProceedingsoftheSocietyforComputation
pages7222–7240,Online.AssociationforComputa-
inLinguistics2022,pages203–205,online.Associa-
tionalLinguistics.
tionforComputationalLinguistics.
Erika Petersen and Christopher Potts. 2022. Lexical AlexWarstadt,AliciaParrish,HaokunLiu,AnhadMo-
semanticswithlargelanguagemodels: Acasestudy hananey,WeiPeng,Sheng-FuWang,andSamuelR.
ofEnglishbreak. Ms.,StanfordUniversity. Bowman.2020. BLiMP:Thebenchmarkoflinguis-
tic minimal pairs for English. Transactions of the
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
Association for Computational Linguistics, 8:377–
DarioAmodei,IlyaSutskever,etal.2019. Language
392.
modelsareunsupervisedmultitasklearners. OpenAI
blog,1(8):9. LeonieWeissweiler,ValentinHofmann,AbdullatifKök-
sal,andHinrichSchütze.2022. Thebetteryoursyn-
ShauliRavfogel,GrushaPrasad,TalLinzen,andYoav
tax, the better your semantics? Probing pretrained
Goldberg. 2021. Counterfactual interventions re-
languagemodelsfortheEnglishcomparativecorrel-
veal the causal effect of relative clause represent
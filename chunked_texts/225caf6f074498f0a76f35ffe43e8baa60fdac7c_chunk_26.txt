 be seen as an abla-
In this section, we investigate what effect this tionof VMSST,i.e. VMSST withoutthesource-
parameter sharing has on VMSST by using N separationloss,weseethatthesource-separation
encoders and decoders (full enc, full dec.). We loss especially helps with generalization to new
experimentwith6layerTransformerencodersand languages.
4languagesSpanish,English,Arabic,andTurkish
7.4 EffectsofBatchSize
inordertokeeptheexperimentstractableasinthis
setting we have 5 encoders and 4 decoders. The Lastly, weinvestigatehow VMSST comparesto
resultsareshowninTable6. CONTRASTIVE asbatchsizeincreases. Itiscom-
The results indicate that the approximation ap- monknowledgethatcontrastivemodelslearnbetter
pears to hold, as VMSST is much closer to the representationswhengivenhardernegativeexam-
fullmodelthan BITRANSLATION,whichisanab- ples. Sinceweareusingin-batchnegativesinour
Model B.Size Sem.Sim. BitextMining Quest.Retrieval Score
Eng. XL XL(s.) XL(d.) Tatoeba BUCC(c.) BUCC(m.) NQ MKQA
RandomInit.(6Layer)
2048 65.5 66.8 73.3 62.4 63.1 64.7 82.9 34.0 17.6 53.5
CONTRASTIVE 4096 67.5 69.3 75.4 65.3 66.0 71.5 87.0 35.3 19.2 56.1
8192 69.4 71.6 76.8 68.1 68.6 76.2 89.4 36.4 20.9 58.3
2048 70.1 67.4 75.1 62.2 58.7 72.6 84.7 37.2 20.2 55.4
VMSST 4096 70.2 67.4 75.3 62.1 58.5 73.1 86.0 38.2 20.3 55.7
8192 71.4 70.9 76.6 67.1 61.8 77.9 88.0 39.0 22.4
 a curriculum learning training paradigm. mentsrelevanttothecurrentdialogueturn. These
Ourapproachshowsa12pointimprovement
groundingdocumentscouldpotentiallybedifferent
inBLEUscorecomparedtothebaselineRAG
fromthosegroundedinpreviousdialogueturns.
model.
Weproposeamodelthatimprovesoverthebase-
1 Introduction linemodelbyfocusingoneachcomponentofits
retriever-readerarchitecture. Firstly,weintroduce
Thetaskframeworkofdocument-grounded,conver-
sparse lexical representations in the retriever for
sationalquestionansweringunifiesseveralclosely
matching,asoutlinedinFormaletal.(2021). Sec-
related task frameworks, including open-domain
ondly,wereranktheretriever’sresultsusingtech-
questionanswering(QA),conversationalQA,and
niquesfromFajciketal.(2021). Furthermore,we
knowledge-groundedgeneration. Inopen-domain
update the decoding process to incorporate the
question answering tasks, such as SQuAD (Ra-
fusion-in-decoder (FiD) technique (Izacard and
jpurkar et al., 2018), models are required to re-
Grave,2021). Finally,weusecurriculumlearning
spond to a question with knowledge that may be
totrainourmodels. Weobserveanimprovement
locatedwithinapotentiallylargecollectionofdoc-
of11.9(BLEU)and9.5(F1)pointsonthevalida-
uments. For conversational QA tasks like QuAC
tion;andanimprovementof9.5(BLEU)and10.3
(Choietal.,2018b),thequeriesposedtothemodel
(F1)pointsontestsetintheMDD-SEENsetting
take the form of a dialogue, where previous dia-
comparedtothebaselineRAGmodel. Weachieve
logueturnscontainnecessarycontexttoanswerthe
18.7 and 13.7 points improvement in the BLEU
currentturn’squestion. Bothofthesetaskframe-
andF1metricrespectively
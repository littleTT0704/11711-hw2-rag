andcomputethecrossentropy(CE)betweenthedistributionofmodelpredictionoverthe
twoclassesandhumanresponses.
Results. We report the results of all models in Table 3. Our proposed MORALCOT model
outperformsallexistingLLMs,showingthatourCoTpromptingstrategyiseffectiveforthetask.
Specifically,MORALCOTachieves64.47%F1,improvingoverthebaselineInstructGPTthatour
model is based on by 10.53%. Moreover, compared with the state-of-the-art moralQA model,
Delphi++,wealsoimprovebyamarginof6.2%F1. Giventhechallengingnatureandtheimportance
oftheproblem,thereisgreatvalueinexploringhowLLMscanbeimprovedformodellingmoral
flexibility; andweencouragefutureworktofurtherimproveourpreliminarymodelattempt. We
observe several interesting trends. For example, we find that the Cons. scores for most models
arequitepolarized,withmostmodelscloseto100(stickingtotheoriginalruletooconservatively)
or0(allowingrule-breakingtooboldly). Notably,ourmodelimprovesoverthefullyconservative
InstructGPTtoallowformoremoralflexibility(whereourCons. scoreis66.96%).
5.2 DetailedErrorAnalysis
AlthoughtheperformanceofourproposedmodelimprovesoverexistingLLMs,wecannoticethat
mostmodelshaveanF1scorenotmuchbetterthantherandombaseline(around50%). Thishas
non-trivialnegativeimplicationsandraisestheurgencyoftheneedformoreworkonAIsafety. To
betterunderstandwhytheLLMcannotdowellonMoralExceptQA,weconductmorefine-grained
erroranalysisconsidering: (1)howwellitanswerseachofthesubquestionsinvolvedinMORALCOT,
(2)howwellitunderstandsthecostsandbenefitsassociatedwithagivenaction,(3)howreasonably
itexplainstherationalebehindadecisionand(4)howmuchitreliesonword-levelcorrelations? We
usethefree-formQAmodel
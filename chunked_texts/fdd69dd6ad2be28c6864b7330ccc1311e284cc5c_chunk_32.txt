ard Hovy, Thang Luong, and Quoc Le. Unsupervised data
augmentationforconsistencytraining. AdvancesinNeuralInformationProcessingSystems,
33:6256–6268,2020.
[30] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaN.Toutanova. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. 2018.
[31] YinhanLiu, MyleOtt, NamanGoyal, JingfeiDu, MandarJoshi, DanqiChen, OmerLevy,
MikeLewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbert
pretrainingapproach. arXivpreprintarXiv:1907.11692,2019.
[32] Wei-NingHsu,BenjaminBolte,Yao-HungHubertTsai,KushalLakhotia,RuslanSalakhutdi-
nov,andAbdelrahmanMohamed. Hubert: Self-supervisedspeechrepresentationlearningby
maskedpredictionofhiddenunits. IEEE/ACMTransactionsonAudio,Speech,andLanguage
Processing,29:3451–3460,2021.
[33] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick. Masked
autoencodersarescalablevisionlearners. arXivpreprintarXiv:2111.06377,2021.
[34] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. In
InternationalConferenceonLearningRepresentations,2020.
[35] AnttiRasmus,MathiasBerglund,MikkoHonkala,HarriValpola,andTapaniRaiko. Semi-
supervisedlearningwithladdernetworks.

(j) (j)
resource-scarcetargetlanguage(Telugu). language(s), denoted as D = {(x,y );j ∈
s s s
We propose a meta-learning based method, [1,M]} where M (cid:29) N and (3) a pre-trained
MetaXL,tobridgethisrepresentationgapandal- modelf θ,whichisnotnecessarilytrainedonany
lowforeffectivecross-lingualtransfertoextremely monolingual data from the target language – our
low-resourcelanguages. MetaXLlearnstotrans- goalistoadaptthemodeltomaximizetheperfor-
formrepresentationsfromauxiliarylanguagesina manceonthetargetlanguage.
waythatmaximallyfacilitatestransfertothetarget Whensometargetlanguagelabeleddataisavail-
language. Concretely,ourmeta-learningobjective ableforfine-tuning,astandardpracticeistojointly
encouragestransformationsthatincreasethealign- fine-tune(JT)themultilinguallanguagemodelus-
mentbetweenthegradientsofthesource-language ingaconcatenationofthelabeleddatafromboth
setwiththoseofatarget-languageset. Figure1(b) the source and target languages D s and D t. The
showsthatMetaXLsuccessfullybringsrepresenta- representationgap(Singhetal.,2019)betweenthe
tionsfromseeminglydistantlanguagescloserfor source language and target language in a jointly
moreeffectivetransfer. trainedmodelbringsadditionalchallenges,which
motivatesourproposedmethod.
We evaluate our method on two tasks: named
entity recognition (NER) and sentiment analysis
2.2 RepresentationTransformation
(SA).Extensiveexperimentson8low-resourcelan-
guagesforNERand2low-resourcelanguagesfor Thekeyideaofourapproachistoexplicitlylearn
SAshowthatMetaXLsignificantlyimprovesover totransformsourcelanguagerepresentations,such
strong baselines by an average of 2.1 and 1.3 F1 that when training with these transformed repre-
scorewithXLM-Rasthemultilingual
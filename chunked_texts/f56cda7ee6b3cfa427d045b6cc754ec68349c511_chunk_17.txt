 Our final evaluations on 500 offensive test threads. Pre-
samplecontainsabout100,000offensiveresponses trained dialogue models DGPT and GPT-3 gen-
and75,000agreeingresponses. Wefurtherdivide erate ≈ 30% and ≈ 41% offensive responses
intoeachcontroldatasetofsizeLintoa95-5ratio when tested in offensive contexts. On the other
togettrainanddevsplit. hand,fine-tuningdialoguemodelsonsafeconver-
sations reduce their offensive behavior, as seen
6.1 Modeling,TrainingandTestingDetails
withBlenderbotandDAPTsafecontrolresponses.
WeuseCTGtechniquesthatwerefoundeffective However,additionalsafeconversationsfine-tuning
inreducingtoxicityinlanguagemodelsbyGehman alone doesn’t eliminate offensive behavior. Sur-
etal.(2020). Thisincludes(1)Domain-Adaptive prisingly, Bender and DAPT safe control mod-
PreTraining(DAPT)-fine-tuningapretraineddia- elsbothshowhigheragreementinoffensivecon-
loguemodelonthreadswithfixedcontroltokens textsthantheDGPTbaseline. Fine-tuningonboth
(Gururanganetal.,2020). (2)AttributeCondition- ‘neutral’ and ‘safe’ responses, as in the case of
ing(ATCON)-Inthismethod,specialcontrolto- the DAPT - neutral stance control model, simul-
taneously reduces the agreement while generat-
11Onlythreadswithallsafecommentswereconsideredfor
Stancecontrolattribute. 13Wepredictthemostlikelyclassinautomaticevaluation
12Weselectedthresholdsforalllabelssuchthatweget.75 insteadofhigh-precisionthresholdprediction,whichwasused
andhigherprecision. togeneratefine-tuningdataforcontrollabletextgeneration.
Model Control Len. Dist-1↑ Dist-2↑ %Bad↓ %Off↓ %Agree↓ %Neutral↑
DGPTmedium - 9.02.378.858 5.6 29.6 13
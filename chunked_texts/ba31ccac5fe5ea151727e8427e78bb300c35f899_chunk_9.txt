andthecurrentmodelmbeingtrained:
fewer errors, the ratio will be tuned down corre-
(cid:88)
spondinglytoavoidwastingannotationbudgeton L = − p m′(y|x)logp m(y|x)
already-correctly-predictedsub-structures. Aswe y∈Y
willseeinlaterexperiments,thisadaptivescheme
Several points are notable here: 1) The previous
issuitableforAL(§3.3).
modeliskeptunchanged,andwecansimplycache
itspredictionsbeforetraining;2)Overtheinstances
2.3 Self-training
thathavepartialannotations,thepredictionsshould
Betterutilizationofunlabeleddataisapromising
reflect these annotations by incorporating corre-
directiontofurtherenhancemodeltraininginAL
spondingconstraintsatinferencetime;3)Fortasks
since unlabeled data are usually freely available
withCRFbasedmodels,theoutputspaceY isusu-
from the unlabeled pool. In this work, we adopt
allyexponentiallylargeandinfeasibletoexplicitly
self-training(Yarowsky,1995)forthispurpose.
enumerate; we utilize special algorithms (Wang
Themainideaofself-trainingistoenhancethe
etal.,2021)todealwiththis,andmoredetailsare
modeltrainingwithpseudolabelsthatarepredicted
presentedinAppendixC.
by the current model on the unlabeled data. It
Finally,wefinditbeneficialtoincludeboththe
has been shown effective for various NLP tasks
pseudolabelsandtherealannotatedgoldlabelsfor
(Yarowsky,1995;McCloskyetal.,2006;Heetal.,
themodeltraining. Withthegolddata,theoriginal
2020;Duetal.,2021). ForthetrainingofALmod-
training loss is adopted, while the KD objective
els, self-training can be seamlessly incorporated.
isutilizedwiththepseudolabels. Wesimplymix
For FA, the application of self-training is no dif-
these two types of data with a
xtfortraining
Englishparaphrases. Eventhoughallmodelsare
paraphrastic representations for other languages
able to model semantic similarity in English, we
as well that are also able to model cross-lingual
findthattrainingonParaNMTspecificallyleadsto
semantic similarity (Wieting et al., 2019a, 2020).
stronger models as it is easier to filter the data to
Prior work in learning general sentence embed-
removenoiseandsentencepairswithlittletonodi-
dingshasusedautoencoders(Socheretal.,2011;
versity. Werefertoourmodelsas PARAGRAM-SP,
abbreviatedas P-SP,2 referringtohowthemodels Hill et al., 2016), encoder-decoder architectures
(Kiros et al., 2015; Gan et al., 2017), and other
are based on averaging subword units generated
sources of supervision and learning frameworks
bysentencepiece(KudoandRichardson,2018).
(LeandMikolov,2014;Phametal.,2015;Arora
Wemakeallofthesemodelsavailabletothecom-
etal.,2017;Pagliardinietal.,2017).
munityforuseondownstreamtasks.
ForEnglishsemanticsimilarity,wecompareto
We also add functionality to our implementa-
well known sentence embedding models such as
tion. Besidesthesupportforefficient,low-memory
InferSent(Conneauetal.,2017),GenSen(Subra-
training on tens of million of sentence pairs de-
manian et al., 2018), the Universal Sentence En-
scribedabove,weaddcodetosupport(1)reading
coder (USE) (Cer et al., 2018), as well as BERT
inalistofsentencesandproducingasavednumpy (Devlinetal.,2019).3 WeusethepretrainedBERT
arrayofthesentenceembeddings;(2)readingina
modelintwowaystocreateasentenceembedding.
listofsentencepairsandproducingcosinesimilar-
The first way is to concatenate the hidden states
ityscores;and(3)downloadingandpreprocessing
fortheCLSt
the theAMRLtypes.ThehypothesisisthatasingleLSTMem-
finalstatefromtheforwardandbackwardLSTMisusedto beddinglayermightbeenoughtoencodealltheinformation
predictthecategory. fromtheSLUrepresentation.Figure6dshowsthetopology
The first layer provides a shared word embedding (Col- forthisnetwork.
lobertandWeston2008),whiletheremainingthreeLSTM
Word embeddings and gazetteers We add to the input
layers are connected to an affine transform and a softmax
featureofourbaselinepre-trainedwordembeddingvectors
layertoprovidetheoutputforeachofthethreetasks(i.e.,
andgazetteers.Weusethreehundreddimensionalpre-trained
properties,type,andintentprediction).IOBtaggingisused
word2vecembeddings,trainedontheGoogleNewscorpus
todenotetheinside,outside,andbeginningofeachproperty
on100billionutterances(Mikolovetal.2013)andweincor-
andtypespan.Theinputofthisbaselinemodelisaone-hot
poratethemasanadditionalinputtotheone-hotencoding
vectorforeachwordinthesentence.
ofeachword.Gazetteers(listsofentitymentions)fromthe
AlexaontologythatbacksAMRLwereusedinasimilarway
Transferringlearnedrepresentations Therearefourpri-
towordembeddings(e.g.,asanadditionalinputperword).
marymodelingarchitecturesthathavebeenusedtoaddem-
Forexample,aMusiciangazetteerwillcontainalistofmusic
beddinglayersfordomains,intentsandslots.Thesearchi-
ofmusiciannameslike“Sting.”Thesefeaturesareindicators
tecturesusedomainandslotembeddingsfromtheSLUtask.
thataresetto1ifthecurrentwordorwordsequenceappears
IntentembeddingsfromtheSLUtaskwereaddedtoinitial
ina gazetteer,and setto0 otherwise. Thismodel usesthe
models,butdidnotresultsignificantperformancegains.
sametopology
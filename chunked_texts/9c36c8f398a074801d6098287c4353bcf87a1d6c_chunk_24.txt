oyanov, and Luke Zettlemoyer. 2019.
Bart: Denoising sequence-to-sequence pre-training AdamRoberts,ColinRaffel,andNoamShazeer.2020.
for natural language generation, translation, and How much knowledge can you pack into the pa-
comprehension. arXivpreprintarXiv:1910.13461. rameters of a language model? arXiv preprint
arXiv:2002.08910.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav
Shoham. 2021. Jurassic-1: Technical details and TimoSchickandHinrichSchütze.2020. Few-shottext
evaluation. WhitePaper.AI21Labs. generation with pattern-exploiting training. arXiv
preprintarXiv:2012.11926.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Sima Sharifirad, Borna Jafarpour, and Stan Matwin.
Luke Zettlemoyer, and Veselin Stoyanov. 2019. 2018. Boosting text classification performance on
Roberta: A robustly optimized bert pretraining ap- sexist tweets by text augmentation and text genera-
proach. arXivpreprintarXiv:1907.11692. tion using a combination of knowledge graphs. In
Proceedings of the 2nd workshop on abusive lan- A AdditionalDatadetails
guageonline(ALW2),pages107–114.
A.1 ValueandCounterValue
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared Intotal,wehave19categoriesofsexismandtwo
Casper, Zhun Liu, Shrimai Prabhumoye, George correspondingvaluesforeachofthem. Weprovide
Zerveas, Vijay Korthikanti, et al. 2022. Using
thelistofallvaluesandcounter-valuespersexism
deepspeed and megatron to train megatron-turing
categoryinTable6.
nlg 530b, a large-scale generative language model.

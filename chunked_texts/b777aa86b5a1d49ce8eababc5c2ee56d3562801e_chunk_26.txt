ani,Jaewon
ciationforComputationalLinguistics.
Lee,JohnLundell,ChangkyuKim,ArunKejariwal,
andJohnDOwens.2022. Buildingaperformance
ZhiqingSun,HongkunYu,XiaodanSong,RenjieLiu,
modelfordeeplearningrecommendationmodeltrain-
YimingYang,andDennyZhou.2020. Mobilebert: a
ingongpus. arXivpreprintarXiv:2201.07821.
compacttask-agnosticbertforresource-limitedde-
vices. arXivpreprintarXiv:2004.02984.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, MingxingTan,BoChen,RuomingPang,VijayVasude-
Luke Zettlemoyer, and Veselin Stoyanov. 2019. van,MarkSandler,AndrewHoward,andQuocVLe.
Roberta: A robustly optimized bert pretraining ap- 2019. Mnasnet: Platform-awareneuralarchitecture
proach. arXivpreprintarXiv:1907.11692. searchformobile. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecog-
MosheLooks,MarcelloHerreshoff,DeLesleyHutchins, nition,pages2820â€“2828.
andPeterNorvig.2017. Deeplearningwithdynamic
computationgraphs. In5thInternationalConference YiTay,MostafaDehghani,SamiraAbnar,HyungWon
on Learning Representations, ICLR 2017, Toulon, Chung,WilliamFedus,JinfengRao,SharanNarang,
France, April 24-26, 2017, Conference Track Pro- VinhQTran,DaniYogatama,andDonaldMetzler.
ceedings.OpenReview.net. 2022. Scaling laws vs model architectures: How
doesinductivebiasinfluencescaling? arXivpreprint theAAAIConferenceonArtificialIntelligence,vol-
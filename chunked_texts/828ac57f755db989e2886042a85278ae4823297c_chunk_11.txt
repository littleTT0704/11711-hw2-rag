 xh¯xt+W hh¯(rt(cid:12)ht−1)) (3) Learn to Answer
7'
ht =(1−zt)(cid:12)ht−1+zt(cid:12)h¯t (4)
QA pairs 6'
where xt is the input, rt is the reset gate, zt is the update Reconstruct
gate,ht istheproposedstateand(cid:12)iselement-wisemulti- 5' present
State
(Model I)
plication. Forthedecoder,weusethesamearchitectureas copy
theencoder,butitshiddenstateofh0 isinitializedwiththe 6 4'
hidden state of the last time step N in the encoder. Simi-
3'
lar to [32], we construct our GRU encoder-decoder model 5
(Figure4).Besidesreconstructingtheinputframes,wealso Reconstruct
2' past
train another two models which are asked to reconstruct 4
(Model II)
the future frames (Figure 4 top) and past frames (Figure 4
Input frames Encoder 1'
bottom),respectively. Ourproposedmodelsarecapableof
learninggoodfeaturesasthenetworkisoptimizedbymin- Decoders Predictions
imizing the reconstruction error. In order to achieve good Figure4. Theencoder-decodermodel(right):encoderstateoflasttime
stepispassedtothreedecodersforreconstruction.Learntoanswer(left):
reconstruction,representationpassedtothedecodershould
learnedtoanswerquestionsinasupervisedway.
retain high level abstraction of the target sequence. Note
thatourthreemodelsarelearnedseparately,whereencoder
and decoder weights are not shared across models of past,
ers are stacked. Our decoders are conditioned on the in-
presentandfuture.
puts,andweapplyDropoutwithrate0.5atconnectionsbe-
Training. We first train the encoder-decoder models in an tweenfirstGRUlayerandsecondGRUlayerassuggested
unsupervised way using videos collected from a subset of byZarembaetal.[49]toimprovethe
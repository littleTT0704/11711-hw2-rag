ρ
issmallunderagivenradius,ρ. (cid:15)≈ρ∇ L(Q(w,s))/(cid:107)∇ L(Q(w,s))(cid:107) (5)
Q Q 2
Inordertoachieveaflatteroptima,SAM(Foret s←s−η∇ QL(Q(w,s)+(cid:15))∇ sQ(w,s) (6)
et al., 2020) introduces a minimax objective: w←w−η∇ QL(Q(w,s)+(cid:15)) (7)
min max L(w + (cid:15)) to push models into
w (cid:107)(cid:15)(cid:107)2<ρ
flat minima and proposes the following gradient
Noticethatweareomittingtheterm∇ Q(w,s)
update under (cid:96) norm (Note that to reduce w
2
whenupdatingw sinceforeachelementbeingup-
computational cost, (cid:15)(w) in Eq. 3 is regarded as
dated, the STE gradient ∂Q(w,s)/∂w = 1. If
constantandnogradientflowstoit):
wetakeQ(w,s) → w,Eq.7breaksdowntothe
(cid:15)(w) ≈ ρ∇ L(w)/(cid:107)∇ L(w)(cid:107) update in Eq. 3. However, this is not the case
w w 2
(3)
for step-size parameter s (Eq. 6). The joint up-
w ← w−η·∇ L(w+(cid:15)(w))
w
dateofeachstep-sizeparameterhaselementwise
Nahshan et al. (2020) pioneered the effort to let gradient∂Q(w,s)/∂swhichmustbeevaluatedat
Figure2: SharpnessofSQuATVS.LSQforallGLUETasks. Thelowersharpnessmeansaflatterlocalminima.
run-time.
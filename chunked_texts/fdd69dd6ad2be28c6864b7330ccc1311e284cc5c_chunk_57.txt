arm-upof5,120steps. Thelabeledandunlabeledbatchsizeisbothsetto16.
Otheralgorithm-relatedhyper-parametersstaythesameasintheoriginalpapers.
G.2 SetupforNLPTasksinUSB
Weusepre-trainedBERT-Base[30]forallNLPtasksinUSB.Wesetthebatchsizeoflabeleddata
andunlabeleddatato4forreducingthetrainingtimeandGPUmemoryrequirement. Tofine-tune
theBERT-BaseunderUSB,weadoptAdamWoptimizerwithweightdecayof1e−4. Similarly,we
conductagridsearchofthelearningrateandlayerdecayondifferentdatasetsusingFixMatchand
pickthebestconfigurationtofine-tuneotherSSLalgorithms. Weutilizethesamecosinelearningrate
6Wepresentthefulltuningresultsin:https://github.com/microsoft/Semi-supervised-learning.
23
schedulerasintheclassicsettingwiththetotaltrainingstepsof102,400andawarm-upof5,120
steps. Weusethefine-tunedmodelwithoutparametermomentumtoconductevaluations. Forall
datasets,wecutthelongsentencetosatisfytheinputlengthrequirementofBERT-Base. Fordata
augmentation,weadoptback-translationasthestrongaugmentation[29,25]. Specifically,weuse
De-EnandRu-EntranslationwithWMT19.
Table16: Hyper-parametersofNLPtasksinUSB.
Dataset AGNews Yahoo!Answer IMDb Amazom-5 Yelp-5
MaxLength 512
Model Bert-Base
WeightDecay 1e-4
LabeledBatchsize 4
UnlabeledBatchsize 4
LearningRate 5e-5 1e-4 5e-5 1e-5 5e-5
LayerDecayRate 0.65 0.65 0.75 0.75 0.75
Scheduler η=η cos(7πk)
0 16K
ModelEMAMomentum 0.0
PredictionEMAMomentum 0.999
WeakAugmentation None
StrongAugmentation Back-Translation[29]
G.
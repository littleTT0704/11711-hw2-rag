 Saghar Hosseini,
Ahmed Awadallah, M Najafabadipour, J M Tuñas,
A Rodríguez-González, and E Menasalvas. 2020.
GenderBiasinMultilingualEmbeddingsandCross-
lingualTransfer. InProceedingsofACL2020.
Appendix UD-POS Weemploythepart-of-speechtagging
data from the Universal Dependencies v2.7 tree-
A Detailsof XTREME models
banks(Nivreetal.,2018)covering104languages.
We use the English training data for training and
All submissions to the XTREME leaderboard are
evaluateonthetestsetsofthetargetlanguages.
large-scale Transformers (Vaswani et al., 2017)
WikiANN-NER For named entity recognition,
trained with masked language modeling (MLM),
weusetheWikiANNdataset(Panetal.,2017),in
manyofwhichextendmonolingualmodels. Multi-
which proper noun spans in Wikipedia text have
lingualBERT(mBERT),XLM-RoBERTa(XLM-
been automatically annotated as either location,
R;Conneauetal.,2020)andmultilingualT5(mT5;
person,ororganization. Weusethebalancedtrain,
Xueetal.,2021)extendBERT(Devlinetal.,2019),
dev,andtestsplitsfromRahimietal.(2019).
RoBERTa(Liuetal.,2019),andT5(Raffeletal.,
XQuAD TheCross-lingualQuestionAnswering
2020) respectively. Rebalanced mBERT (Rem-
Dataset(Artetxeetal.,2020a)requiresidentifying
BERT; Chung et al., 2021) is a more efficient,
theanswertoaquestionasaspaninthecorrespond-
scaled-up reparameterization of mBERT. These
ingparagraph. AsubsetoftheEnglishSQuADv1.1
models have been pre-trained on unlabeled data
(Rajpurkaretal.,2016)devsetwasprofessionally
inaround100languagesfromWikipedia(mBERT)
translatedintotenotherlanguagesfor
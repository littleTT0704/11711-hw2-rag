 85.0 GloVe embeddings rather than BERT, lowers performance
significantly,by10%and25%respectively.
Table 1: Experimental results on. VQA mod-
VCR
els struggle on both question-answering (Q → A) as
well as answer justification (Q → AR), possibly due d. LSTM+ELMo: Here an LSTM with ELMo embed-
dingsisusedtoscoreresponsesr(i).
to the complex language and diversity of examples in
VQA Baselines Additionally we compare our ap-
the dataset. While language-only models perform well,
our model R2C obtains a significant performance boost. proach to models developed on the VQA dataset [5]. All
modelsusethesamevisualbackboneasR2C(ResNet50)
Still, all models underperform human accuracy at this
aswellastextrepresentations(GloVe;[56])thatmatchthe
task. For more up-to-date results, see the leaderboard at
visualcommonsense.com/leaderboard. originalimplementations.
e. RevisitedVQA[38]: Thismodeltakesasinputaquery,
response, and image features for the entire image, and
as wellas answer justification(QA→R). Thus, inaddition passestheresultthroughamultilayerperceptron,whichhas
to reporting combined Q→AR performance, we will also toclassify‘yes’or‘no’.10
reportQ→AandQA→R. f. Bottom-up and Top-down attention (BottomUpTop-
Tasksetup Amodelispresentedwithaquery q, and Down)[4]: Thismodelattendsoverregionproposalsgiven
four response choices r(i). Like our model, we train the byanobjectdetector. Toadaptto,wepassthismodel
VCR
baselinesusingmulti-classcrossentropybetweenthesetof objectregionsreferencedbythequeryandresponse.
responses and the label. Each model is trained separately g. Multimodal Low-rank Bilinear Attention (MLB)
forquestionansweringandanswerjustification.9 [42]: ThismodelusesHadamardproduct
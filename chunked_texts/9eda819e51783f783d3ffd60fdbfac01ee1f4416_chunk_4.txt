si|di;θ),andhenceparametersθare
makingparametersofpre-trainedmodelstunable
obtainedbymaximumlikelihoodestimation
orintroducingadditionalmoduleswhoseparame-
terscanbesharedbydifferentlanguages. (cid:88)
argmax logp(si|di;θ), (1)
Usingthisframework,weaskthesequestions: θ
(di,si)∈(D,S)
Q1: How well do different parameter-efficient
tuning methods (Figure 1-b) perform compared where (D,S) is the parallel training corpus. For
to PLM fine-tuning models (Figure 1-a)in multi- multilingualtextsummarization,D andS canbe
lingualsummarization? Q2: Willsupervisedtrans- inanyofanumberoflanguages.
fer, a commonly used technique in multi-lingual
2.2 TuningStrategy
learning,behelpfulforparameter-efficienttuning?
Q3: Couldbetterresultsbeachievedbyenablingin- Recently, applying pre-trained language mod-
formationexchangebetweendifferentlanguages? els (PLMs) to abstractive summarization tasks
How do different choices of parameter-efficient equipped with diverse tuning strategies has
tuningmethodsinteractwiththissharing? achievedagreatsuccess,whichcanbeformulated
asbelow:
Weexplorethesequestionsbyperformingexten-
siveexperimentsover45differentlanguages. Our
h = PLM(D,s,h ;θ,θ ) (2)
quantitative and qualitative analyses find several i i <i plm add
observations,suchas:
wherePLMisasequencetosequencepre-traiend
(1)ComparedtoPLMfine-tuning,bothparameter-
LMs(e.g.,T5(Raffeletal.,2019)orBART(Lewis
efficient tuning methods (prefix- and adapter-
etal.,2020)),θ representstheoriginalPLMpa-
plm
tuning) are advantageous in low-resource scenar-
rameterandθ denotestheadditionalparameters
add
ios. Particularly,prefix
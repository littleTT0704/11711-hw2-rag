-based multiclass tasks, the final layer is a softmax with
24
HEAR: Holistic Evaluation of Audio Representations
Table 3: Properties of baseline and submitted models, including: whether the model pro-
cesses raw audio (1-D) or spectrograms (2D); on what kind of data the model
is pretrained; the number of million parameters; the size of the output embed-
ding for scene and timestamp tasks; and the number of minutes the model spends
embedding Speech Commands V2. We caution that embedding time is not the
entire picture, if participants did not do simple speed optimizations. For example,
the CREPE wrapper (also used by GURA) is known not to exploit GPU batch
parallelism.
Input Pretraining data # M Embed dim Time
Model 1D 2D speech broad music params scene time min
OpenL3 X X 4.7 512 512 94.9
wav2vec2 X X 315.4 1024 1024 8.9
CREPE X X 22.2 2048 2048 38.3
AMAAILab wav2vec2+DDSP X X X 98.8 871 871 43.6
AMAAIwav2vec2 music+speech X X X 300.0 768 768 5.0
CP-JKU PaSST 2lvl X X 86.2 1295 2590 14.5
CP-JKU PaSST 2lvl+mel X X 86.2 1295 3358 5.8
CP-JKU PaSST base X X 86.2 1295 1295 5.8
CVSSPPANNS X X 80.8 2048 2048 3.9
Descript/MARL Wav2CLIP X X 11.7 512 512 3.1
GURAAvgH+w+C X X X 1339.0 1024 1024 40.0
GURAAvgHubert+Crepe X X X 1022.0 1024 1024 33.9
GURAAvgHubert+wav2vec2 X X 634.0 1024 1024 14.6
GURACat H+w+C X X X 1339.0 3072 3072 40.1
GURACat Hubert+wav2vec2 X X 634.0 2048 2048 14.4
GURACat wav2vec2+crepe X
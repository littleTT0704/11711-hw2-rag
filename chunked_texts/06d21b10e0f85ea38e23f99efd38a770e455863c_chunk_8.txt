allowsoneto
Xi h i
read in a list of sentence pairs and produce the
wherethesimilarityfunctionisdefinedas: sentence pairs along with their cosine similarity
scores in an output file. These scripts allow our
f (s,t) = cos g(s;θ ),g(t;θ ) (2) modelstobeusedwithoutanyprogrammingforthe
θ src tgt
twomostcommonusecases: embeddingsentences
(cid:16) (cid:17)
where g is the sentence encoder with parameters andscoringsentencepairs. Examplesoftheirusage
for each language θ = (θ,θ ). To select t withatrainedmodelareshowninFigure2.
src tgt 0i
381
from models import load_model cd preprocess/bilingual && bash do_all.
1 1
sh fr-es-de
2
text1 = ’This is a test.’ cd../..
3 2
text2 = ’This is another test.’ cd preprocess/paranmt && bash do_all.sh
4 3
0.4 1.0 0.7
5
# Load English paraphrase model
6
7 model_name = ’paraphrase-at-scale/model. Figure3: Usageexamplestodownloadandpreprocess
para.lc.100.pt’
bilingualandParaNMTdata.Thefirstcommanddown-
sp_model = ’paraphrase-at-scale/paranmt.
8 loads and preprocesses (filters, trains sentencepiece
model’
models, tokenizes if language is zh, converts files to
9
10 model, _ = load_model(model_name= hdf5format)en-Xbilingualdata. Thethirdcommand
model_name, sp_model=sp_model) downloads and preprocesses ParaNMT data. The ar-
11 gumentsareusedtofilterthedata(semanticsimilarity
# Obtain sentence embedding
12
scoresbetween0.4and1.0andtrigramoverlapbelow
embeddings = model.embed_raw_text([text1
13
, text2]) #
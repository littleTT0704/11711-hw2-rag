lish,TomHenighan,TomB Lerer, James Bradbury, Gregory Chanan, Trevor
Brown,BenjaminChess,RewonChild,ScottGray, Killeen, Zeming Lin, Natalia Gimelshein, Luca
AlecRadford,JeffreyWu,andDarioAmodei.2020. Antiga, et al. 2019. Pytorch: An imperative style,
Scaling laws for neural language models. arXiv high-performancedeeplearninglibrary. Advancesin
preprintarXiv:2001.08361. neuralinformationprocessingsystems,32.
David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
Le, Chen Liang, Lluis-Miquel Munguia, Daniel
2020. Reformer: The efficient transformer. arXiv
Rothchild,DavidR.So,MaudTexier,andJeffDean.
preprintarXiv:2001.04451.
2022. Thecarbonfootprintofmachinelearningtrain-
ingwillplateau,thenshrink. Computer,55(7):18–
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
28.
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: Alitebertforself-supervisedlearn-
Vijay Janapa Reddi, Christine Cheng, David Kanter,
ing of language representations. arXiv preprint
PeterMattson,GuentherSchmuelling,Carole-Jean
arXiv:1909.11942.
Wu, Brian Anderson, Maximilien Breughe, Mark
Charlebois,WilliamChou,etal.2020. Mlperfinfer-
DavidLangerman, AlexJohnson, KyleBuettner, and
encebenchmark. In2020ACM/IEEE47thAnnual
Alan D George. 2020. Beyond floating-point ops:
InternationalSymposiumonComputerArchitecture
Cnn performance prediction with critical datapath
(ISCA),pages446–459.IEEE.
length. In 2020 IEEE High Performance Extreme
ComputingConference(HPEC),pages1–9.IEEE. Mark
 R2C and allows us to focus on learning more
this form includes annotation artifacts, which our analysis powerfulvisionrepresentations.
suggests is prevalent amongst multiple-choice VQA tasks ModelHyperparameters Amoredetaileddiscussion
whereinhumanswritethewrongendings.Ouranalysisalso ofthehyperparametersusedforR2Cisasfollows.Wetried
suggests Adversarial Matching can help minimize this ef-
fect,evenwhentherearestrongnaturalbiasesintheunder- 26The only slight difference is that, due to the WordPiece encoding
scheme,rarewords(likechortled)arebrokenupintosubwordunits(cho
lyingtextualdata.
##rt ##led). Inthiscase,werepresentthatwordastheaverageofthe
BERTactivationsofitssubwords.
27SincethedomainthatBERTwaspretrainedon(Wikipediaandthe
24Assuminganequalchanceofchoosingeachincorrectending,there- BookCorpus[94])isstillquitedifferentfromourdomain,wefine-tuned
sultsforBERTonanimaginary4-answerversionofTVQAandMovieQA BERTonthetextofVCR(usingthemaskedlanguagemodelingobjec-
wouldbe54.5%and42.2%,respectively. tive,aswellasnextsentenceprediction)foroneepochtoaccountforthe
25Weprependthesubtitlesthatarealignedtothevideocliptothebegin- domainshift,andthenextractedtherepresentations.
ningofthequestion,withaspecialtoken(;)inbetween. Wetrimtokens 28Thissuggests,however,thatifwealsofine-tunedBERTalongwith
fromthesubtitleswhenthetotalsequencelengthisabove128tokens. therestofthemodelparameters,theresultsofR2Cwouldbehigher.
16
Q→A QA→R Q→AR
tosticktosimplesettings(andwhenpossible,usedsimilar Model GloVe BERT GloVe BERT GloVe BERT
configurationsforthebaselines,particularlywithrespectto
R2C 46.4 63.8 38
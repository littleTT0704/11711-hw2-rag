 vary the input representation h and distance function ⊗ from
ds
Equation 5. All of them use a large datastore with size N, approximately 5000 times the size of the
ds
vocabularyV,asalsoreflectedin“+#params”,thenumberofadditionalparametersotherthanthebaseLM.
Wereportseveralimportantquantitieswithrespecttoeachmodel.
• “PPL”showstheperplexityofonlythekNNcomponentofthemodelp (). Thisis∞forallkNN-
kNN
LMmodelsinallcases,aswhenthekNNsearchdoesnotretrieveanydatastoreentriescorresponding
tothetruetargetwordw theprobabilityofthetargetwordwillbezero.
t
• “Oracle”showsthelowerboundoftheinterpolationperformancebychoosingthebestλforeach
token in the evaluation dataset, which will either be λ = 0 or λ = 1 depending on whether
P (w |c )>P (w |c )ornot,respectively.
LM t t knn t t
Fromthetable,wecanseethat:
1. Usingtheoutputofthemulti-headedattentionlayer(“att”)ash (insteadofthestandard“ffn”layer)
ds
iscrucialforbetterperformanceofkNN-LM.
2. Ingeneral,usingnegativesquaredL2distanceorinnerproductasasimilarityfunctiondoesnotresult
inalargeandconsistentdifference,althoughinoursetting,IPprovidesslightlybetterperformance
whenusingthe“att”inputs,andslightlyworsewhenusing“ffn”inputs.
3. Interestingly,whenusing“ffn”and“IP”,thesameinputanddistancemetricusedintheparametric
model, the results are the worst, indicating that the kNN-LM is particularly benefiting when the
kNN-LMachievesadifferentviewofthedatafromtheparametricmodel.
WefoundinpreliminaryexperimentsthatkNN-LMisgeneralizabletootherbaselanguagemodelsaswell,
rangingfromsmallmodelswith82Mparameterstolargermodelswith774Mparameters.
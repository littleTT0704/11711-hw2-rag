��). WetriedtoreplacethekNNcomponentwithacomponentthatjusttakesthetokensretrievedbythekNN
searchandreturnstheirL2distancetotheLMoutputwordembeddings: W ⊗h insteadofW ⊗h,
sm ds ds ds
where⊗representsthenegativeL2distance. Wetriedthiswithbothvariantsofh,attentionlayeroutput,
ds
andfeedforwardlayeroutput. Noneofthesehelped.
E.2 Sparsification
InEquation5,mask-to-k(·)usedbykNNretrievalinducessparsityinthedistributionoverthevocabulary,
duetoasmallkcomparedtothenumberofvocabularyV. WehypothesizethattheinkNN-LM,thekNN
distributionissparse,practicallyincreasingtheprobabilityofthetop-k entries. ThekNNdistributionhas
upto1024entriesthatarenon-zero,concentratingmoreprobabilitymassoverthemostlikelytokens. This
effectissimilartotheredistributionofprobabilitymassfortextgenerationinHoltzmanetal.(2019). We
testthishypothesisonlybytakingtop32,64,128,512,or1024tokensintheparametricLMprobabilityand
zeroingouttheprobabilitiesoftherestofthetokens. Tocompensate,weexperimentwithdifferentsoftmax
temperaturesandtheninterpolatewiththeparametricLMprobability. Thisisolatestheeffectofthedatastore
andretrievalatall,andthisdoesnothelpatall,suggestingthatsparsificationoftheoutputprobabilityaloneis
notenough.
AnotherattemptistohypothesizethatthekeyinkNN-LMisthatitselects“whichtokenstoinclude”inthe
kNNdistribution,andnottheirdistances. Theintuitionbehindisthatmaybetheselectionofthetoptokens
accordingtothekNNsearchisbetterthanthatfromthedot-productdistancebetweenthelanguagemodel’s
outputvectorandallthevocabularyembeddings. Weperformexperimentssimilartothe
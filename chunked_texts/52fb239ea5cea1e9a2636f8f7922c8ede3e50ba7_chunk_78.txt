hesis from computation, while opening directions for further study on either
aspect.
10
Neo-A Neo-P
Dimension
IID OOD IID OOD
Math ability 0.191 0.129 0.445 0.188
Language 0.189 0.147 0.429 0.246
Format 0.246 0.382 0.372 0.404
Knowledge 0.206 0.143 0.331 0.213
Average 0.208 0.200 0.394 0.263
Table 4: Multi-task models are able to generalize to unseen tasks in some
categories. Program output (Neo-P) always outperforms number output (Neo-
A).
Data Answer (% F1) Program (% F1)
Neo Multi ∆ Neo Multi ∆
100% 28.4 32.3 +4.0 80.0 82.4 +2.5
40% 20.0 21.1 +1.2 75.2 70.3 -4.9
20% 15.8 18.4 +2.6 66.3 67.1 +0.8
Table 5: Here we show the results of fine-tuning both GPT-Neo-2.7B (Neo)
and Bha¯skara (Multi) on 100%, 40%, and 20% of the held-out data from
L¯ila-OOD. The Multi almost always outperforms Neo (the ∆ column shows
the margin).
Models leverage symbolic execution and libraries. The gap between
program synthesis and answer prediction suggests that the neural language
model offloads computations to the symbolic Python runtime that are otherwise
difficult to compute directly. We identify two common cases. First, the model
leverages standard Python as a calculator. For instance, this pattern is common
in the basic_math and mul_div categories, which involve evaluating arithmetic
expressions; Table 4 shows examples. Second, the model is able to call external
libraries that perform sophisticated computations. For instance, in statistics
the model uses scipy.stats.entropy or np.linalg.det in linear algebra while
solving problems (Table 5).
Models occasionally generate non-executable code. Roughly 10% of
Bha¯skara’s IID programs fail to execute. 86% of these are SyntaxErrors
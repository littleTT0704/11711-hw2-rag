. Seethepaperfor
thefulllist.
Our framework presents new opportunities for
21Wecountdisagreementsasbeinglabeledmisinformation
here,discardingdisagreementsleadstoF1of74.97. studyingperceivedintentandimpactofmisinfor-
3115
mation,whichmayalsoaidincounteringandde- especially in the case of clickbait. We find head-
tectingmisinformation. lines to be suitable as online readers often share
headlines without clicking on them (Gabielkov
We can estimate content virality. Given the
et al., 2016), however future work may explore
user-annotatedlabelsforlikelihoodofreadingor
extendingreactionframestofullnewsarticles.
sharing,wecanestimatewhethertheinformation
Thereisalsoannotatorandmodelbias. Readers
intheassociatedarticleislikelytopropagate.
involvedinourdatacurationandhumanevaluation
studiesare“generallyknowledgeable,”asproved
We can analyze the underlying intents behind
bytheirabilitytodiscernmisinformationfromreal
headlines. Usingannotatedwriterintents,wecan
news. We see this bias as a potential strength as
determine common themes and perceived inten-
it allows us to find ways to counter misinforma-
tionsinmisinformationheadlinesacrossdomains
tionincaseswherereadersarewell-informedbut
(e.g. mistrust of vaccination across medical do-
stillbelievefalseinformation. However,annotators
mains). Giventheperformanceofpredictivemod-
mayhaveundesirablepoliticalorsocialbiases. In
els highlighted by Tables 5 and 6, we can also
such cases, gender bias may lead an annotator to
extendthisanalysistounseenheadlines.
assume that a politician mentioned in a headline
Wecancategorizeheadlinesbyseverityoflikely ismaleortodismissinequalityconcernsraisedby
outcomes. Falseheadlinesthatexplicitlyincite ascientistbelongingtoaminoritygroupas“play-
violence,orotherwiseencourageactionsthatlead ing the race card.” These biases can also appear
to psychological or physical harm (e.g. not vac- in
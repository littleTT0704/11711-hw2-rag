 representing the h next actions
The similar performance between templated language and t+1 t+h
that the robot can take in order to perform the task. Fig. 6
ground truth actions suggests that unambiguous, templated
showsanexampleofonecourseoferrorweseeduringthese
languageisinsufficienttodemonstratethelanguagelearning
prediction rollouts. The top two rows show a sequence of
capabilities of our system. We find our method is robust
predictions coming from the sequence to sequence model,
to real natural language from Mechanical Turk workers,
while the bottom row shows predictions using ground truth
achieving 95% of the success rate seen on unambiguous
actions from our data. In this case, we see that the sequence
templates.
to sequence model started the grasp verb earlier than the
B. Subgoal Module groundtruthexecution,butbothmodelsgenerategoodimage
We analyze the language learning component of our predictions.
model. A full breakdown of subgoal prediction accuracy AswecanseeinTableI,thereispersistentlysomeerrorin
is given in Table II. Performance was comparable between the low-level predictions from our actor module, even when
templatedlanguageandnaturallanguagedatacollectedfrom given oracle arguments Average placement error increases
Amazon Mechanical Turk. We see that it is more difficult to as we move away from the ground-truth arguments. Often
make accurate predictions on real language data. Addition- failures occur because the object is not clearly visible in the
ally, accuracy is remarkably consistent over time, meaning first frame.
detciderP
elcarO
We would like to thank Jonathan Tremblay for valuable
discussions.
REFERENCES
[1] C. Paxton, A. Hundt, F. Jonathan, K. Guerin, and G. D. Hager,
“CoSTAR: Instructing collaborative robots with behavior trees and
vision,” Robotics and Automation (ICRA), 2017 IEEE International
Conferenceon,2017.
[2] D.Xu,S.Nair,Y.Zhu,J.Gao,A.Garg,L.Fei-Fei,andS.Savarese,
“Neuraltaskprogramming:Learningtogeneralizeacrosshierarchical
tasks

spectives on Psychological Science, 3(5):370–386.
PMID:26158955.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, ClementDelangue, AnthonyMoi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-artnaturallanguagepro-
cessing. InProceedingsofthe2020Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing:
SystemDemonstrations,pages38–45,Online.Asso-
ciationforComputationalLinguistics.
Lili Yao, Nanyun Peng, Weischedel Ralph, Kevin
Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-
and-write: Towardsbetterautomaticstorytelling. In
The Thirty-Third AAAI Conference on Artificial In-
telligence(AAAI-19).
Jeffrey M. Zacks. 2020. Event perception and mem-
ory. Annual Review of Psychology, 71(1):165–191.
PMID:31905113.
Jeffrey M Zacks, Nicole K Speer, Khena M Swallow,
ToddSBraver,andJeremyRReynolds.2007. Event
perception: a mind-brain perspective. Psychologi-
calbulletin,133(2):273.
11
A Appendix A.5 SimGenexperimentaldetails
We used the Turing-NLG model without further
A.1 Atomicrelationstrainingdetails
fine-tuning. The model has 17B and we used it
withtop-psampling(top-p=0.85),temperature=1.0
Weusedthetrain/dev/testsplitsfromtheoriginal
and max sequence length of 64 tokens. MPnet-
Atomicdataset(Sapetal.,2019a). Negativesam-
basemodelwasaccessedfromtheSentence-BER
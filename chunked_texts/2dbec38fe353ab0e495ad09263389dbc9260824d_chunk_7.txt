neuralnetworkarchitectureshavebeenpro-
tiplevariables. Thegraph-basedinformationcan
posedformathematicalreasoningtasks,including
alsobeembeddedwhenencodingtheinputmathe-
Seq2Seq-based networks, graph-based networks,
maticalsequences(Zhangetal.,2020b;Shenand
andattention-basednetworks. Thesemethodsare
Jin,2020;Lietal.,2020b;Wuetal.,2021a).
outlinedinmoredetailinTable8intheAppendix.
3.1 Seq2Seq-basedNetworksforMath 3.3 Attention-basedNetworksforMath
Sequence-to-sequence(Seq2Seq)(Sutskeveretal., Theattentionmechanismhasbeensuccessfullyap-
2014)neuralnetworkshavebeensuccessfullyap- plied to NLP (Bahdanau et al., 2015) and vision
pliedtomathematicalreasoningtasks,suchasmath problems(Xuetal.,2015;Wooetal.,2018),taking
wordproblemsolving(Wangetal.,2017),theorem intoaccountthehiddenvectorsoftheinputsdur-
proving (Yang and Deng,2019), geometry prob- ingthedecodingprocessing. Recently,researchers
lemsolving(Robaideketal.,2018),andmathques- havebeenexploringitsusefulnessinmathematical
tionanswering(Tafjordetal.,2019). ASeq2Seq reasoning tasks, as it can be used to identify the
model uses an encoder-decoder architecture and mostimportantrelationshipsbetweenmathemati-
usuallyformalizesmathematicalreasoningasase- calconcepts. Forinstance,MATH-EN(Wangetal.,
quencegenerationtask. Thebasicideabehindthis 2018a)isamathwordproblemsolverwhichben-
approachistomapaninputsequence(e.g. amath- efits from long-distance dependency information
ematicalproblem)toanoutputsequence(e.g. an learned by self-attention. Attention-based meth-
equation,program,andproof). Commonencoders odshavealsobeenappliedtoothermathemat
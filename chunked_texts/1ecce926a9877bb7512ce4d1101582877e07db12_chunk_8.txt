itill
adoptedbetweenstudentfeature{fstu}T and(label-guided)teacherfeature{ftch}T.AstructurelossL isutilizedas
t t=1 t t=1 struc
theobjective.Thegraycolorindicatescomponentsthatareonlyusedduringtraining.
Theacousticfeaturescontainsasemanticembeddinggsem
andalocationembeddinggloc whichencodesthecategory
and3Dlocationofsoundsourcesrespectively.Afterthat,a ER Projection
multimodalfusionmoduleisutilizedtoenableaudio-visual
interaction.Specifically,themultimodalfusionmodulecon-
PanoramicFrame ER Frame
tainstwopseudo-siameseblocks-astudentblockthatfuses
Figure3:IllustrationofERProjection.Severedistortions
audio-visualinformationusingmultimodalattentionanda
canbeobservedinthepolarareas.
teacherblockthatsharesthesamestructureofthestudent
block while taking an additional ground truth as input to
guidethefusion.Theoutputstudentfeatures{fstu}T and
t t=1 ferent from regular positional encoding (Vaswani et al.
(label-guided)teacherfeatures{ftch}T aresenttovisual
t t=1 2017), spherical positional encoding first re-projects the
decodersequippedwithskipconnectionstogeneratethefi-
plane coordinates of each pixel back to the 3D sphere
nal salient object predictions {Mstu}T and {Mtch}T.
t t=1 t t=1 and generates positional encoding based on the 3D coor-
A distillation loss between student feature {fstu}T and
t t=1 dinates. In this way, each pixel can reflect its true 3D po-
(label-guided)teacherfeatures{ftch}T isadoptedtohelp
t t=1 sition thus avoiding the severe distortion in the polar re-
themultimodalinteractionandastructurelossbetweenpre-
gionandinevitableseparationalongalongitudeintheER
dictionMË† andgroundtruthM isusedasthetaskobjective
t t frame.Inparticular,thespherical
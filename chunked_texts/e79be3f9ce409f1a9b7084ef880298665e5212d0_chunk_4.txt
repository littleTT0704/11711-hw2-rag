 to efficiently select a small set of hard negative
to separately encode the spatial and temporal cues [11].
examplesontheflyduringtraining. Itleveragesthevideo-
Thesemodelsusuallyrelyonarankormarginlosstolearn
textalignmentscorescomputedinL andL beforemulti-
1 2 the correct alignment for video-text pairs. Another line
modalfusionlayers,andhelpstolearnthemulti-modalfu-
of work learns fine-grained or hierarchical alignment be-
sionlayersmoreeffectivelywithoutanyextraoverhead.
tween videos and texts [56, 49, 6]. In [49], the authors
We perform a comprehensive empirical study to val-
proposed a fine-grained alignment by extracting the nouns
idate the effectiveness of TACo in both pretraining and
and verbs from action phrase in a sentence and projecting
dataset-specific scenarios. We apply TACo and different
themintoasharedspacewithvideos. Alternatively,theau-
variants of contrastive losses to train or pretrain and fine-
thorsin[6]extractahierarchicalsemanticgraphandapply
tune on various downstream tasks including text-video re-
graph reasoning to achieve the alignment at different lev-
trieval (YouCook2, MSR-VTT and ActivityNet) [58, 52,
els. Similar ideas have been also proposed in the image-
12],videoactionsteplocalization(CrossTask)[61]andac-
text alignment by decomposing the images and texts into
tionsegmentation(COIN)[43].OurresultsshowthatTACo
sub-tokens [26, 50]. Thus far, it has not been studied how
improves the text-video retrieval performance over current
thesetask-specificarchitecturescanbeintegratedintolarge-
state-of-the-art across three benchmarks. Furthermore, the
scale pretraining. In this paper, we are the first to propose
learned multi-modal representation and video representa-
asimpleyeteffectivetoken-awarecontrastivelossforfine-
tioncanbeeffectivelytransferredtoCrossTaskandCOIN,
grainedalignmentforpretraininganddownstreamtasks.
and achieve better or comparable performance
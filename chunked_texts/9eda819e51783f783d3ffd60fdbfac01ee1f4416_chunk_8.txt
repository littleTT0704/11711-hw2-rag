 Fine-tuning (MPF) This is
3WeusetherougepackageprovidedbyXL-Sum(Hasan
acommonly-usedsetting(Hasanetal.,2021)when etal.,2021)tosupportmultiplelanguages.
(a)PLMFine-tuning (b)Adapter-tuning (c)Prefix-tuning
Figure2: Differenttuningmethods. Redindicatestrainableparameters;blueindicatesfrozenparameters.
threetuningmethods: prefix-tuning,adapter-tuning on the full test set of the chosen language. The
andPLMfine-tuningondifferentlanguages. performanceoffewshotexperimentsisinfluenced
bythetrainingsampleschosen(Zhaoetal.,2021),
3.1.1 ExperimentDetails
so we keep the sampled training set and develop-
Settings and Hyper-parameters We use the mentsetthesameforthethreetuningmethodsto
baseversionofmultilingualT5(Xueetal.,2021) have a fair comparison. For non-few shot experi-
as a backbone, which covers most languages ments,eachsizehasoneexperimentandistested
in XL-Sum dataset and is the same as (Hasan on the full test set of the chosen language. The
et al., 2021), allowing us to make a fair com- hyperparametersarechosenfromasinglelanguage
parison. In our experiment, MLP of prefix- (Japanese)foreachtuningmethodandappliedas-is
tuning is two linear layers with an inner dimen- toalllanguages. Weusetheresultfromthecheck-
sion as a hyper-parameter. For prefix-tuning, the pointwiththebestvalidationsetperformanceover
hyper-parameters we tune4 are the same as Li alltrainingepochs.
and Liang (2021). For adapter-tuning, the hyper-
3.1.2 ResultsandAnalysis
parametersremainthesameasprefix-tuningexcept
the prefix length that is not needed for adapter- Results Fig.3illustratestheperformanceofthree
tuning. More details are in the appendix. For differenttuningmethodswithrespecttotheavail-
bothadapter-tuningandprefix-
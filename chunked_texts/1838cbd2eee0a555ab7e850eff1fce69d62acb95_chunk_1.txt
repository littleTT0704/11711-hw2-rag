MetaXL: Meta Representation Transformation for Low-resource
Cross-lingual Learning
MengzhouXia§ ∗ GuoqingZheng‡ SubhabrataMukherjee‡ MiladShokouhi‡
GrahamNeubig† AhmedHassanAwadallah‡
§PrincentonUniversity †CarnegieMellonUniversity ‡MicrosoftResearch
mengzhou@princeton.edu,{zheng,submukhe,milads}@microsoft.com
gneubig@cs.cmu.edu,hassanam@microsoft.com
Abstract Joint Training MetaXL
en en
tel tel
The combination of multilingual pre-trained
representations and cross-lingual transfer
learning is one of the most effective methods
for building functional NLP systems for low-
resource languages. However, for extremely
low-resource languages without large-scale
monolingual corpora for pre-training or
(a) (b)
sufficient annotated data for fine-tuning,
Figure 1: First two principal components of sequence
transfer learning remains an under-studied
representations (corresponding to [CLS] tokens) of
and challenging task. Moreover, recent work
TeluguandEnglishexamplesfromajointlyfine-tuned
shows that multilingual representations are
mBERTandaMetaXLmodelforthetaskofsentiment
surprisingly disjoint across languages (Singh
analysis. MetaXL pushes the source (EN) and target
et al., 2019), bringing additional challenges
(TEL)representationsclosertorealizeamoreeffective
for transfer onto extremely low-resource
transfer. The Hausdorff distance between the source
languages. Inthispaper,weproposeMetaXL,
a meta-learning based framework that learns
andtargetrepresentationsdropsfrom0.57to0.20with
to transform representations judiciously from
F1scoreimprovementfrom74.07to78.15.
auxiliarylanguagestoatargetoneandbrings
theirrepresentationspacescloserforeffective
Wikipedia and XLM-R (Conneau et al., 2020) is
transfer. Extensiveexperimentsonreal-world
low-resource languages – without access pre-trainedon100languageswithCommonCrawl
to large-scale mon
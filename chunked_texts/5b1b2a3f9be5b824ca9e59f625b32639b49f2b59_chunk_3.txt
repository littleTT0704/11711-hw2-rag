alities through sound localization and audio-visual
Research Asia. Xiang Li is currently with Department of Electrical and
separation, but there is no investigation on dense prediction
Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213
USA.(email:xl6@andrew.cmu.edu) done jointly with audio and visual modalities, potentially
JingluWangandYanLuarewithMicrosoftResearchAsia,Beijing,China. offering lost opportunities.
(email:jinglwa@microsoft;yanlu@microsoft.com)
In this paper, we propose a robust context fusion method
ThisworkwasdonewhenXiaohaoXuwasaninternatMicrosoftResearch
Asia. Xiaohao Xu is currently with Department of Mechanical Engineering, for online VIS. To preserve the location information of in-
Huazhong University of Science and Technology, Wuhan, China. (email: stances, we build our model on the crop-free transformer-
xxh11102019@outlook.com)
basednetwork.AsshowninFigure1,weemployanattention-
BhikshaRajiscurrentlywithSchoolofComputerScience,CarnegieMellon
University,Pittsburgh,PA15213USA.(email:bhiksha@cs.cmu.edu) based context fusion module for multi-modal contextual in-
2202
luJ
21
]VC.sc[
1v08550.7022:viXra
2
1) We analyze the mathematical intuition behind the order-
ContextFusion
Seg.map preservingpropertyofinstancecode andconductmoreexper-
Reference frames ‚Ñ¨ iments to understand its behavior. 2) We simplify the instance
‚Ñ¨
Att. from‚Ä¶ ref. frames code generation to improve the network generalization, and at
thesametimethecomputationalcostisreduced.3)Weextend
the framework to accept audio-visual inputs, and present an
Target frame ùêº! ‚Ñ¨! (Order-preserI vn is nt ga )nce mask ùëÄ!
effective method to fuse the multimodal context information
Att. from audio
Attention maps
Instance emb. ùëí! leveraging the compatibility of the network. 4) We provide
Audio extensive quantitative and qualitative experimental results to
(a) Timestamp
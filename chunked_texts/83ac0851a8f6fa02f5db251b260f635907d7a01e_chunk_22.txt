,E.Kolve,J.J.Lim,A.Gupta,L.Fei-
[10] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.- Fei, and A. Farhadi. Target-driven visual navigation in in-
P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, doorscenesusingdeepreinforcementlearning. InIEEEIn-
and T. Darrell. Speaker-follower models for vision-and- ternationalConferenceonRoboticsandAutomation(ICRA),
languagenavigation. InNeuralInformationProcessingSys- pages3357â€“3364.IEEE,2017. 8
tems(NeurIPS),2018. 1,2,5,7,8
[11] D.Gordon,A.Kembhavi,M.Rastegari,J.Redmon,D.Fox,
andA.Farhadi. IQA:Visualquestionansweringininterac-
tiveenvironments.InComputerVisionandPatternRecogni-
tion(CVPR),volume1,2018. 8
[12] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner,
H. Soyer, D. Szepesvari, W. M. Czarnecki, M. Jaderberg,
D.Teplyashin,etal. Groundedlanguagelearninginasimu-
lated3dworld. arXivpreprintarXiv:1706.06551,2017. 8
[13] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher,
and C. Xiong. Self-monitoring navigation agent via aux-
A.SupplementaryMaterial
Our appendix is structured to provide both correspond-
ing qualitative examples for the quantitative results in the
paperandadditionalimplementationdetailsforreplication.
A.1.Qualitativecomparison
FiguresA1throughA3showthreeexamplescomparing
our approach to the previous state-of-the-art. In addition,
thefollowingURLincludesa90secondvideo(https://
youtu.be/AD
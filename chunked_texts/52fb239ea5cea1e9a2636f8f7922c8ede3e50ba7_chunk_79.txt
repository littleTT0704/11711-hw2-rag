,
which often occur because decoding terminates before finishing the program or
the model generates a program of the form ‘2+3=5’, which is invalid Python.
The remaining 14% of execution failures are less trivial, including NameErrors
(7%) and TypeErrors (1%) (see Table 6).
Bha¯skara is a good starting point for further fine-tuning Table 5
shows that our Bha¯skara model is a better starting point for downstream
fine-tuning than the vanilla pre-trained GPT-Neo-2.7B. When comparing fine-
11
tuning for direct question answering with T5-3B, we see an almost 8% absolute
improvement in F1 (30.1% to 37.6%). These findings establish Bha¯skara as a
strong starting point for further fine-tuning on new tasks. For this reason, we
release our multi-task model for public use under the name Bha¯skara, with
the hope that it will be useful for future research into math reasoning models.
5.2 Results: Category-wise Analysis
In this section we discuss the trends among the tasks within each category. For
brevity, we primarily consider Bha¯skara, the GPT-Neo multi-task model in
the program-synthesis setting.
Math ability. Among the tasks in the math category, Bha¯skara excels
in basic math, linear algebra, and in-domain statistics. On these tasks, it
performs equal or better to Codex. On the other hand, Bha¯skara struggles
in advanced math and geometry, with mediocre performance in multiplication-
division, number theory, and calculus. Codex shows analogous trends, except
for performing very well on calculus (0.930)7.
Language complexity. Models generally show lower performance on pro-
gram synthesis as language complexity increases. Bha¯skara gets mean F1 over
0.5 only for datasets with the least linguistic complexity where it achieves an F1
of 0.7.
Question format. Among the format tasks in the dataset, Bha¯skara does
exceptionally well on multiple-choice and natural-language inference, getting
performance close to 0.9 on the latter, and outperforming Codex on both. On
the other hand, the model performs close to
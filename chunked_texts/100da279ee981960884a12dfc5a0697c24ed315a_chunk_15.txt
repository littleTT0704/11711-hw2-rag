.03 2.33±0.25 2.01±0.01 21.42±3.48 5.73±0.24
AnessentialdifferencebetweenUAandDistributionAlignment(DA)(Berthelotetal.,2019a)pro-
posedearlierliesinthecomputationofunsupervisedloss. Thenormalizationoperationmakesthe
predicted probability biased towards the less-predicted classes. In DA, this might not be an issue,
asthenormalizedpredictionisusedassofttargetinthecross-entropyloss. However,withpseudo-
labeling, more erroneous pseudo-labels are probably created after normalization, which damages
the quality. UA avoids this issue by exploiting original predictions to compute pseudo-labels and
normalized predictions to compute sample weights, maintaining both the quantity and quality of
pseudo-labelsinSoftMatch. ThecompletetrainingalgorithmisshowninAppendixA.2.
4 EXPERIMENTS
WhilemostSSLliteratureperformsevaluationonimagetasks,weextensivelyevaluateSoftMatch
onvariousdatasetsincludingimageandtextdatasetswithclassicandlong-tailedsettings.Moreover,
WeprovideablationstudyandqualitativecomparisontoanalyzetheeffectivenessofSoftMatch. 1
4.1 CLASSICIMAGECLASSIFICATION
Setup.Fortheclassicimageclassificationsetting,weevaluateonCIFAR-10/100(Krizhevskyetal.,
2009),SVHN(Netzeretal.,2011),STL-10(Coatesetal.,2011)andImageNet(Dengetal.,2009),
withvariousnumbersoflabeleddata,whereclassdistributionofthelabeleddataisbalanced.Weuse
theWRN-28-2(Zagoruyko&Komodakis,2016)forCIFAR-10andSVHN,WRN-28-8forCIFAR-
100,WRN-37-2(Zhouetal.,2020)forSTL-10,andResNet-50(Heetal.,2016)forImageNet. For
allexperiments,weuseSGDoptimizerwithamomentumof0.9,wheretheinitiallearningrateη
0
hescaling
ofthesimilarityscoresandthusthepeakinessofthenon-parametriccomponentdistribution.
4. Approximation & Sparsification: In the parametric model, k = V, and no values are masked,
but inthe kNN-LM, k (cid:28) V, andmostof thedatastoreentriesare prunedout. The definitionof
themask-to-k(·)function,i.e. howtoselecttheimportantdatastoreembeddingstoincludeinthe
similaritycalculation(inkNN-LM’scasetheknearestneighbors),isacrucialopendesignchoice.
Inthefollowingsections,wesetouttobetterunderstandhoweachofthesedesigndecisionscontributestothe
improvementinaccuracyduetotheuseofkNN-LMs.
3 BaselinekNN-LMResults
First,weevaluatethekNN-LMbaselineontheWikitext-103dataset(Merityetal.,2016),andexaminethe
importanceoftwodesignchoices: theinputrepresentationh andthesimilarityfunction⊗.
ds
Inmodelsexaminedinthispaper, theparametricmodelisatransformerlanguagemodelwithmostlythe
samearchitectureasinKhandelwaletal.(2020b). However,Wedomakemodificationstotheoriginalbase
LM(BaevskiandAuli,2018)toaccommodateourexperimentationneed.WeusingBPEtokenization(Sennrich
etal.,2015)totrainasmallervocabulary(33K)thantheoriginal(260K)onthetrainingcorpusofWikitext-103,
assubwordtokenizationisubiquitousinmanystate-of-the-artlanguagemodels(Radfordetal.,2019;Devlin
etal.,2018;Liuetal.,2019;Brownetal.,2020). Usingsubwordtokenizationalsoeliminatestheneedfor
adaptivesoftmax(Joulinetal.,2017). Itmakestheoutputlayermoregeneralized,sharingmoreresemblance
tothekNNcomponentasdescribedinSection2,andfacilitat
8.2) 24.5(0.8)
Human – –.000.011 107.9 –
Table1: Textcompletionresults(GPT-2,Wikitext-103testset),reportedasmean (stdev)using5randomseeds.Policy
gradient(PG)andminimumrisktraining(MRT)arestochasticallymixedwithMLEandreportedas(+MLE(cid:11)),withthe(cid:11)
valuesselectedbasedonthetaskloss.Resultsherearewithgreedydecoding;seetheAppendixforancestralsampling.
Reward augmented maximum-likelihood (RAML) ourmethodarelearnedmanifoldrandomsearch(Senerand
(Norouzietal.2016)maximizesthelikelihoodofsequences Koltun2020)whichrequiresaninneroptimizationtolearn
that are sampled proportional to their rewards, which in parameters of a search manifold, and guided evolutionary
practicereliesonasamplingmethoddesignedforaspecific strategies(Maheswaranathanetal.2019)whichusessurro-
task loss. Our method weights parameter, rather than gatedirectionstomodifythesearchdistribution’scovariance;
sequence,samplesproportionaltotheirrewards.Minimum theirmethodrequiresQRdecompositionandwasevaluated
risk training originated in statistical machine translation on synthetic and unrolled optimization tasks with smaller
(Och2003)andwasappliedtoend-to-endneuralmachine networksthanthoseweconsider.
translation (Shen et al. 2016; Edunov et al. 2018). Other
approaches train a greedy decoder given a learned model 5 Experiments
(Gu, Cho, and Li 2017), which is a different setting than
5.1 TextCompletionwithGPT-2
ours.
First,weevaluateMGSonatextcompletiontask,whichhas
Aseparatefamilyofmethods,includinggloballynormal-
previouslybeenusedtoevaluatetheeffectivenessofsequence
izedmodels,(SountsovandSarawagi2016),energy-based
models(e.g.Sutskever,Martens,andHinton(2011);Radford
models (Deng et al. 2020), unlikelihood training (Welleck
et
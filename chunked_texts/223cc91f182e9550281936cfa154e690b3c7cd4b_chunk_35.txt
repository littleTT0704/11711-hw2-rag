 due to
the policy gradient theorem (Sutton et al., 2000), where μθ(x) = ∑∞ γtp(x = x) is the unnormalized
t=0 t
discounted state visitation measure. The final form is exactly the policy gradient up to a multiplication factor
1/Z.
We can also consider a slightly different use of the reward, by directly setting the experience function to the Q
function:
f :=fθ (x,y) =Qθ (x,y). (4.19)
reward,2
This turns out to connect to the known RL-as-inference approach that has a long history of
research (Abdolmaleki et al., 2018; Dayan & Hinton, 1997; Deisenroth et al., 2013; Levine, 2018; Rawlik et
al., 2012).
RL as inference. We set α = β := ρ > 0. The configuration corresponds to the approach that casts RL as a
probabilistic inference problem. To see this, we introduce an additional binary random variable o, with
p(o = 1∣x,y) ∝ exp{Q(x,y)/ρ}. Here o = 1 is interpreted as the event that maximum reward is
obtained, p(o = 1∣x,y) is seen as the ‘conditional likelihood’, and ρ is the temperature. The goal of
learning is to maximize the marginal likelihood of optimality: logp(o = 1), which, however, is intractable
to solve. Much like how the standard equation applied to unsupervised MLE provides a surrogate variational
objective for the marginal data likelihood (Section 4.1.3), here the standard equation also derives a variational
bound for logp(o = 1) (up to a constant factor) with the above specification of (f,α,β):
−logp(o =1) =−logEp θ(x,y)[p(o =1∣x,y)]
(4.20)
≤−ρH(q)−ρEq(x,y)[logp θ(x,y)]−Eq(x,y)[Qθ(x,y)].
Following the teacher-student procedure in Equation 3.3, the teacher-
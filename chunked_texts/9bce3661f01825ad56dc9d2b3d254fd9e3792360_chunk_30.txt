aturallan- mad Abdul-Mageed, and Alham Fikri Aji. 2023.
guageconversations. Lamini-lm: A diverse herd of distilled mod-
els from large-scale instructions. arXiv preprint
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann arXiv:2304.14402.
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Stan- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
ford alpaca: An instruction-following llama Weinberger,andYoavArtzi.2020. Bertscore: Eval-
model. https://github.com/tatsu-lab/ uating text generation with bert. In International
stanford_alpaca. ConferenceonLearningRepresentations.
MosaicMLNLPTeam.2023. Introducingmpt-7b: A Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
newstandardforopen-source,lyusablellms. Smola. 2022. Automatic chain of thought prompt-
ing in large language models. arXiv preprint
HugoTouvron,ThibautLavril,GautierIzacard,Xavier arXiv:2210.03493.
Martinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,Faisal YiZhou,MasahiroKaneko,andDanushkaBollegala.
Azhar,AurelienRodriguez,ArmandJoulin,Edouard 2022. Senseembeddingsarealsobiased–evaluating
Grave,andGuillaumeLample.2023. Llama: Open socialbiasesinstaticandcontextualisedsenseem-
and efficient foundation language models. arXiv beddings. InProceedingsofthe60thAnnualMeet-
preprintarXiv:2302.13971. ingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pages1924–1935,Dublin,
Ashish Vaswani, Noam Sh
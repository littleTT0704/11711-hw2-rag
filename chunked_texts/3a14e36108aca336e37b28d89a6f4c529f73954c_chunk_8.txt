urtherdescribedin
i i′ <a><b><c><X1>Themusicartist<X2>herhand<st>
Table1.Wefindoverhalf(51.5%)theexamplesshow≥3of
• Reorder-1S:WeuseexamplesfromtrainUnsupforstage1.
4changetypesatonce,and89.5%show≥2.Thisshowsthat
Itisproblematictotrainfortheforwarddirectionofourtask
NAREORrequiresperformingdifferentchangesintandem. S,π →S′sinceS′isnotknown.ApproximatingS′using
i′
S′ would hurt output fluency. We instead train in the
3 Methodology inn va ei rv se edirectionS′,π−1 →S,whereπ−1;π−1(π )=
naive i′ i′ i′ i′
3.1 TrainingMethods I n is the inverse permutation of π i′. To reduce train-test
mismatch,weusetheinverseformulationhalfthetime,and
Weintroducetwotask-specifictrainingmethods.
anautoencodingone,i.e.S,I →S theotherhalf.
n
NAR-denoise(NAR-d) • Reorder-2S:trainSupexamplesareusedtofurtherfinetune
This is partially inspired by how humans rewrite; a com- onreorder-1S.WetraininthetaskdirectionS,π i′ →S′.
mon approach is to first reorder sentences naively (simply
3.2 ChosenModels
swappositions),thenmakeotherchanges.NAR-dattempts
to mimic this, learning to convert from naive orderings to We choose several pretrained generation models: GPT-2,
high-qualitytext.Itinvolvestwostagesofmodeltraining. BART,andT5.Wefinetuneallusingbothourtrainingmeth-
odstoproducedenoise-1S(d-1S),denoise-2S(d-2S),reorder-
1. Denoise-1S: Stage 1 is unsupervised training through
1S(r-1S),andreorder-
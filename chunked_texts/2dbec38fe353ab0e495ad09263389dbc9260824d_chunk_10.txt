 with self-supervised tasks for
hasbeenappliedtotheoremproving(Loosetal.,
mathematicalreasoning.
2017;Bansaletal.,2019),duetoitsabilitytoad-
Modelscale. Thereisacleartrendthatpre-trained
dress longitudinal time-series data. Furthermore,
languagemodelshavebecomeincreasinglylarger
TransformersarefoundtooutperformGRUingen-
in the past few years (Devlin et al., 2019; Lewis
eratingmathematicalequationsinDDT(Mengand
etal.,2020;Raffeletal.,2020;Radfordetal.,2020;
Rumshisky,2019). Finally,MathDQN(Wangetal.,
Brown et al., 2020). A recent study (Liang et al.,
2018b) is the first work to explore reinforcement
2022a)showsthatmodelscalewithinamodelfam-
learning for math word problem solving, taking
ily reliably predicts model accuracy. The study
advantageofitsstrongsearchcapabilities.
also mentions an interesting thresholding effect:
“allmodelsthatwinhead-to-headmodelcompar-
4 Pre-trainedLanguageModelsfor
isonsforaccuracyataratewellabovechanceare
MathematicalReasoning
atleast50Bparameters”. Asimilarsize-growing
Pre-trainedlanguagemodels(Devlinetal.,2019; trend can be observed in the field of mathemat-
Radford et al., 2020; Brown et al., 2020) have ical reasoning with pre-trained language models.
demonstrated remarkable performance gains on For example, MWP-BERT (Liang et al., 2022b)
a wide range of NLP tasks. By pre-training on uses a backbone of BERT (110M) (Devlin et al.,
a large corpus of text, the models learn valuable 2019) and RoBERTa (123M) (Liu et al., 2019b)
worldknowledge(Guuetal.,2020),whichcould for Math Word Problems. Most recently, Min-
beappliedtodownstreamtasks. Similarideascan erva(Lewkowyczetal.,2022),whichisbasedon
beappliedtomath-relatedpro
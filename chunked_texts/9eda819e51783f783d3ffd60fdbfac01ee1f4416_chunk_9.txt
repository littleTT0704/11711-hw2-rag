tuning,thesehyper- abletrainingsamples,observationsare:
parameters lead to about 8% additional parame- (1)Ingeneralwhenthesamplenumberislessthan
terscomparedtotheLM’stotalparameters,which 200 prefix-tuning achieves the best performance.
aretunedduringtrainingwhiletheLM’sparame- Between 200 and 10k adapter-tuning is superior,
ters are frozen. To study whether language fea- andgreaterthan10kPLMfine-tuningsurpassesthe
tures will influence the choice of tuning meth- other two. This indicates that regarding both the
ods,wechoosefivelanguagesfromdifferentlan- performanceandparameterefficiency(onlytuning
guagefamilies: English(Germanic),Chinese 8%oftheparametersofPLMfine-tuning),prefix-
simplified (Sino Tibetan), Spanish (Ro- tuningisthebestchoicewhenwehaveextremely
mance), Ukrainian (Balto Slavic) and Urdu few samples, while adapter-tuning is the winner
(IndoIranian). in medium-resource settings. (2) As the training
Wesubsamplethefulldatasetofeachlanguage setsizeincreasesfromfewshottohigh-resource,
to obtain sub-datasets of various sizes,5 and sub- PLM fine-tuning has the largest performance im-
datasets of size ≤ 500 are considered “few-shot” provement,whileprefix-tuninghastheleastperfor-
experiments. For each few-shot experiment, we manceimprovementandadapter-tuningisthemid-
randomlysample3differenttrainingsetsanddevel- dle. (3)ComparedtoPLMfine-tuning,whichisal-
opmentset(withdevsize=20%trainingsetsize). mostmonotonicallyincreasingwiththetrainingset
Thereportedresultistheaverageof3experiments size,adapter-tuningandprefix-tuninghavefluctua-
tions. Frompreliminaryexperiments,wefindthat
4Specifically,thenumberofepochs,batchsize,learning
adapter-tuningandprefix-tuningaremoresensitive
rate,prefixlengthand
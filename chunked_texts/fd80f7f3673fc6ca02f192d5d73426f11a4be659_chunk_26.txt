urtherimprove
usingAUTOMQM,onWMT22EN-DE theperformanceofLLMswithoutfinetuningwhile
providinginterpretabilitythrougherrorspansthat
alignwithhumanannotations.
EN-DE ZH-EN
OurfindingssurroundingfinetuningLLMsfor
Model R? SP MR MCC SP MR MCC score prediction hint that LLMs’ performance in
Baselines machinetranslationevaluationcouldbefurtherim-
COMET-WL ✗ 0.2670.2500.1610.3640.1780.152
provedbyfinetuningthesemodelsonfine-grained
AutoMQM humanjudgmentdata(likeMQM)andisadirec-
BISON ✓ 0.0950.7490.0600.2520.2550.109
tion we are actively pursuing. Additionally, the
UNICORN ✓ 0.1750.6280.1930.2380.4760.143
BISON ✗ 0.1190.5200.0920.2240.3110.091 general-purpose nature of LLMs may enable the
UNICORN ✗ 0.1500.5800.1500.2290.4880.133 applicationofsimilarpromptingtechniques(lever-
aging some fine-grained evaluation schemes) to
Table 6: Span-level meta-evaluation on WMT22 for otherevaluationproblems(Wuetal.,2023).
PaLM-2modelsusingAutoMQM.SRandMRrepresent
spanprecisionandmajorrecall,respectively. Acknowledgements
We would like to thank Ricardo Rei, Marcos
Finally, when evaluating the error spans pro-
Treviso and Chryssa Zerva for helping run the
ducedbyLLMspromptedwith AUTOMQM (Ta-
word-levelQEbaselines,andGeorgeFosterwho
ble 6), we find that PaLM-2 models are able to
provided feedback on an earlier version of this
identifymostofthemajorerrors. However,itdoes
work. ThisworkwaspartiallysupportedbyEU’s
seemtoover-predicterrors(witherrorspredicted
HorizonEuropeResearchandInnovationActions
byUNICORNhavingonaverage
predictionand PROPARAhaveonlyonepossi-
blecorrectvalue. Thus,followingpriorwork,we
CODEX 0.46 2.71
report the automated, standard metrics for these
DAVINCI 0.63 2.25
tasks. For EXPLAGRAPHS, we use model-based
CURIE 1.17 3.32
metricsproposedbySahaetal.(2021),whichwere
foundtohaveahighcorrelationwithhumanjudg-
Table 7: Average loss per token of the three few-shot
models used in this work. TEXT refers to the average ments. ForPROSCRIPTgraphgeneration,wecon-
lossover30Wikipediapages,andCODEisthelossover ductedanexhaustiveautomatedevaluationthatsep-
PythonscriptsintheevaluationsplitofPolycoder. aratelyscoresthecorrectnessofthenodesandthe
correctnessoftheedges.
However,automatedmetricsarelimitedintheir
B DynamicpromptCreation
abilitytoevaluatemodel-generatedoutput. Thus,
to further investigate the quality of outputs, we
Asanalternativetocreatingprompts,thereisnow
conduct a human evaluation to compare the out-
a growing interest in customizing the in-context
exampleseachexampleT. Populartechniques puts generated by COCOGEN and DAVINCI. We
test
sampled20examples,andthreeoftheauthorsper-
typically train a retriever, which is used to fetch
formedtheevaluation. Annotatorswereshowntwo
theexamplesinthetrainingsetthatareclosestto
T (Liu et al., 2021; Rubin et al., 2021; Poesia graphs (generated by COCOGEN and DAVINCI)
test
andwereaskedtoselectonetheythoughtwasbet-
etal.,2021).
ter regarding relevance and correctness. The se-
SpecificallyPoesiaetal.(2021)trainaretriever
lectionforeachcriterionwasmadeindependently:
withatarget-similaritytuning(TST)objectiveover
a corpus of D of (x,y) examples.
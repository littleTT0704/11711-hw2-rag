fulepisode
neural architecture from [11], except that we do not have
isdefinedthecompletionof3lapsfromastandingstartand
different commands in our case, e.g., turn left, turn right,
the agent not going out of the driveable areas of the track.
go straight, and stop. Thus, we used a single branch for
If the agent is unsuccessful in the pre-evaluation phase, it
decoding actions. We assume both front view images and
is disqualified and not evaluated further. As we continue
sensormeasurementsareavailablefortheILagent. Ineach
toprovidesupportfornewtracks(necessitatingmorenovel
sample, the input consists of a 512 × 384 image and 30
driving maneuvers), we will also continue to add and per-
sensor measurements, and output is 2 actions (as listed in
mute the driving competency checks, to maintain fairness
Table1). TheimplementationofCILautomaticallyadjusts
ofevaluationonthosetracks.
the neural network architecture based on specified input-
Postasuccessfulpre-evaluationstage,thefinalteststage
output dimensions. The imitation loss (Equation 3) is the
occurs: agents are provided all the various input modal-
mean squared error between the predicted action, aˆ, and
t
ities and have to compete on the metrics defined Section
theactiontakenbytheexpert,a.
t
4.3. When the agent successfully passes through the pre-
evaluationstage,theuserisnotprovidedwiththeresultsof n
(cid:88)
the competency checks and instead is able to view the re- L= ||aˆ i−a i||2 2 (3)
sultsofthecompleteevaluationdirectlyontheleaderboard. i=1
SoftActor-Critic. Weprovideareferenceimplementation
5.BaselineAgents ofSoft-ActorCritic(SAC)[12,21],whichisgenerallyper-
formant and known to be robust [15]. SAC belongs to the
Wedefineaseriesoflearning-free(e.g.,RANDOM,MPC) family of maximum entropy reinforcement learning (RL)
and learning-based (e.g., reinforcement learning, imitation algorithms,
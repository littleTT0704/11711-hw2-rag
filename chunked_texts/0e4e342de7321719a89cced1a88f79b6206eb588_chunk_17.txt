 81.61 84.68 76.57
VGQC-layer4 78.93 82.37 73.08
VGQCw/ofusion 80.01 83.02 73.94
Table6: Ablationstudyondifferentnum-
Table5: AblationstudyonVGQCw/ofusion
berofcandidatekernels(K)inQCMblock Table7: Ablationstudyonthenumberof
model trained on RefCOCO where we apply
basedonVGQCw/ofusiontrainedonRe- layersusedinthefusiontransformereval-
QCMblocksonlytothelaterlayersofthevi-
fCOCOdataset. uatedonRefCOCOdataset.
sualencoderbackbone.
𝑖 blue middle man man in blue shirt red dress woman woman in red dress and layer 2&3&4. The experiment is conducted on VGQC
…1 (layers omitted) … … … … 1.00 w/ofusiononRefCOCOdataset. TheresultinTable5shows
13 thattheperformancedropswhenapplyingQCMtoonlylater
14 0.50 layers,whichmeansitisbeneficialtostartincorporatingtex-
15 tualinformationfromearlierlayers.
16 0.00 Number of candidate kernels. We measure the effect of
𝛼 1(𝑖)𝛼 2(𝑖)𝛼 3(𝑖)𝛼 4(𝑖)𝛼 5(𝑖)𝛼 1(𝑖) … 𝛼 5(𝑖) 𝛼 1(𝑖) … 𝛼 5(𝑖) 𝛼 1(𝑖) … 𝛼 5(𝑖)
changingthenumberofcandidatekernelsusedinourQCM
Figure 6: Attention weights of the last 4 out of 16 QCM blocks blocks. We trained three VGQC w/o fusion models on Ref-
of VGQC w/ fusion model trained on RefCOCO, with 4 different COCO dataset and evaluate them in all three splits. Among
queriesshownatthetop.α(i)representstheattentionweightforthe theth
 81.61 84.68 76.57
VGQC-layer4 78.93 82.37 73.08
VGQCw/ofusion 80.01 83.02 73.94
Table6: Ablationstudyondifferentnum-
Table5: AblationstudyonVGQCw/ofusion
berofcandidatekernels(K)inQCMblock Table7: Ablationstudyonthenumberof
model trained on RefCOCO where we apply
basedonVGQCw/ofusiontrainedonRe- layersusedinthefusiontransformereval-
QCMblocksonlytothelaterlayersofthevi-
fCOCOdataset. uatedonRefCOCOdataset.
sualencoderbackbone.
ğ‘– blue middle man man in blue shirt red dress woman woman in red dress and layer 2&3&4. The experiment is conducted on VGQC
â€¦1 (layers omitted) â€¦ â€¦ â€¦ â€¦ 1.00 w/ofusiononRefCOCOdataset. TheresultinTable5shows
13 thattheperformancedropswhenapplyingQCMtoonlylater
14 0.50 layers,whichmeansitisbeneficialtostartincorporatingtex-
15 tualinformationfromearlierlayers.
16 0.00 Number of candidate kernels. We measure the effect of
ğ›¼ 1(ğ‘–)ğ›¼ 2(ğ‘–)ğ›¼ 3(ğ‘–)ğ›¼ 4(ğ‘–)ğ›¼ 5(ğ‘–)ğ›¼ 1(ğ‘–) â€¦ ğ›¼ 5(ğ‘–) ğ›¼ 1(ğ‘–) â€¦ ğ›¼ 5(ğ‘–) ğ›¼ 1(ğ‘–) â€¦ ğ›¼ 5(ğ‘–)
changingthenumberofcandidatekernelsusedinourQCM
Figure 6: Attention weights of the last 4 out of 16 QCM blocks blocks. We trained three VGQC w/o fusion models on Ref-
of VGQC w/ fusion model trained on RefCOCO, with 4 different COCO dataset and evaluate them in all three splits. Among
queriesshownatthetop.Î±(i)representstheattentionweightforthe theth
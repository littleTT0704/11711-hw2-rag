wedescribetheproblemofupdatingmodel
phraseconsistencyasameasureofbelief. Inother
beliefsandourlearnedoptimizermethod. Wealso
recentwork,Kassneretal.(2021)measureconsis-
discussmetricsformeasuringfactualbeliefsbelow,
tencyunderentailmentandparaphraseforfactual
whileourBeliefGraphsarepresentedinSec. 6.
beliefwithanewsmall-scaledataset,BeliefBank.
Problem statement and metrics. We suppose
Updating factual beliefs in language models. we have a model f = p (y x) parametrized by
θ θ
|
Approachestomakingtargetedupdatestomodel θ. Foraninputx thathassomeundesiredmodel
i
beliefs vary along a few dimensions. First is outputyˆ = argmax p (y x),wewishtoobtain
i y θ |
whetherthemethodsaltermodeltrainingoroper- anewmodelθ thatproducesadesiredoutputy
∗ i∗
ateinapost-trainingsetting. Sinitsinetal.(2020) for x. This new model θ should also fulfill a
i ∗
use a meta-learning objective during training to few other desiderata. As in past work (De Cao
encourage ease of editing afterwards. A larger etal.,2021;Mitchelletal.,2021),weoperational-
family of methods perform post-training model izethesedesideratainthefollowingmetrics:
updates: Daietal.(2021)proposeahand-crafted
algorithm that edits model weights, while Zhu 1. UpdateSuccessRate(MainInput): Thepro-
et al. (2020) use projected gradient descent for portion of Main Inputs x i for which the up-
batches of points. De Cao et al. (2021) train a datedmodelgivesthedesiredoutputy i∗.
hypernetwork (learned optimizer) that processes
2. UpdateSuccessRate(Paraphrase): Thepro-
modelgradientsinordertoproduceanewmodel
portion of paraphrases of x for which the
i
that(1)givesthedesired
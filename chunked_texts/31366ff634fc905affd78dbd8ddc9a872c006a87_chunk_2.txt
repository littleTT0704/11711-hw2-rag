Code- grams in the generated code and the reference
BERTScore is more likely to be preferred by code. CrystalBLEU (Eghbali and Pradel, 2022)
humans,aswellastofunctioncorrectlywhen extends BLEU by ignoring the 500 most oc-
executed. We release five language-specific
curring n-grams, arguing that they are trivially
pretrained models to use with our publicly
shared between the prediction and the reference.
availablecode. Ourlanguage-specificmodels
Nonetheless, both BLEU and CrystalBLEU rely
have been downloaded more than 1,000,000
timesfromtheHuggingfaceHub.1 on the lexical exact match of tokens, which does
not account for diversity in implementation,
1 Introduction variable names, and code conventions. Figure 1
shows an example: given the reference code
Natural-language-to-codegeneration(NL→Code)
in Figure 1(a), both BLEU and CrystalBLEU
has seen sharply growing popularity recently
prefer (rank higher) the non-equivalent code in
due to the emergence of large language models
Figure 1(b) over the functionally equivalent code
(LLMs) trained on vast amounts of natural lan-
inFigure1(c).
guage and code (Chen et al., 2021; Fried et al.,
CodeBLEU(Renetal.,2020)attemptstolower
2022; Zhou et al., 2023; Austin et al., 2021; Al-
therequirementforalexicalexactmatch,byrely-
lal et al., 2023). LLMs have reached such a high
ing on data-flow and Abstract Syntax Tree (AST)
NL→Code accuracy that they are now useful for
matching as well; nevertheless, valid generations
thebroadprogrammingaudienceandactuallysave
may have different ASTs and data flow from the
∗Equalcontribution
referencecode,whichmayleadtolowCodeBLEU
†NowatGoogleDeepMind
scoreevenwhenthepredictioniscorrect. Further,
1The code and data are available at https://github.com/
neulab/code-bert-score partial predictions may be useful for a program-
3202
tcO
13
]ES.sc[
2v72550.2032:viX
 Included tracks
reality,updatefrequenciesacrossthevarioussensormodal-
aretheThruxtonCircuit(Track01:Thruxton)andAn-
ities are not equal, so L2R synchronizes observations by
glesey National Circuit (Track02:Anglesey) from the
providing agents with the most recent data from each (Al-
United Kingdom, and the North Road Track at Las Ve-
gorithm 1). The step method of the environment returns
gas Motor Speedway (Track03:Vegas), located in the
the new observation s, along with a calculated reward
t+1
United States. Analogous to having separate town maps
to the agent, r = R(s,a,s ), and a Boolean terminal
t t t t+1
for training and testing in other simulation environments,
state flag. The reward function and evaluation metrics are
e.g., CARLA [14], we use Track01 and Track02 for
definedinSection4.3.
trainingandTrack03fortesting. Consequently, wegen-
erateexperttracesfromthetrainingtracks,forinclusionin
Algorithm1Agent-SimulatorInteraction
ourinitialdatasetrelease(seeSection4.2). Manyavenues
1: functionSENSORTHREAD forresearchcanbeexploredwithinL2R,includingvarious
2: data←Initialvalue learning paradigms, such as: (constrained) reinforcement
3: functionGETDATA learning,learningfromdemonstrations,multitasklearning,
4: returndata transferlearninganddomainadaptation,simulation-to-real
5: whilenotterminateddo transfer,fastdecision-making,classical/neuralhybridmod-
6: data←ReceiveData eling,etc. Regardlessofthemethodchosen,agents’multi-
7: modalperceptioncapabilities—i.e.,theirabilitytofuseand
8: functionSTEP(a t) alignsensoryinformation—areofcriticalimportance.
9: Senda tasUDPmessage
10: s t+1 ←GetData∀SensorThreads 4.2.Learn-to-RaceDataset
11: r t ←R(s t,a t,s t+1
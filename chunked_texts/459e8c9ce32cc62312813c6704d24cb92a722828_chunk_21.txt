posalsamplescandidateswithvariedtask The MGS model trained from scratch outperforms the
losses.Asaqualitativeexample,wesampletwocandidates baselineMLEmodelonBLEU,thoughbyasmallermargin
fromq MGSattheendoftraining,decodeabatchofsequences thanthefine-tunedmodels.WeobservedthevalidationBLEU
with each candidate, and in Table 3 show an example se- over time for MGS and the baseline, indicating that they
quenceandthepooledloss.TheMLEcandidateâ€™ssequence arriveattheirperformancelevelsviadifferentpaths.Figure4
isnon-terminating,whilethezerocandidatedecodesashorter showstheproportionofMLEcandidatesthathadthehighest
sequenceandhasalowerpooledloss. weightoutofthefourcandidatessampledfromthemixture
Weinvestigatewhichcandidatescontributetotheupdate (q ), and Table 3 shows an example sequence decoded
MGS
direction over the course of training by showing the total fromacandidatesampledfromeachcomponent.
weightofMLE-componentcandidatesinFigure2((cid:11)=1:0). Candidatessampledfromthezero-componenttendtolo-
TheMLEcandidatesarehighlyweightedatthebeginning callyimprovethetasklossmorethanthosefromtheMLE
oftraining,onlycontributingoccasionallythereafter.Finally, component. However, we find that at the end of training,
weanalyzetheeffectofthe(cid:11)hyper-parameter,whichcon- roughly46%oftheweightcomesfromtheMLEcandidates.
14037
Figure2:WeightofcandidatesfromtheMLEcomponent. Figure3:Validationsequencelossas(cid:11)varies(MGS-LM).
Valid Test
BLEU" SBLEU" MET." EDIT# BLEU" SBLEU" MET." EDIT#
MLE 36.00 36.22 63.82 47.88 34.71 35.67 62.19 50.74
MGS-SBLEU 36.22 36.58 64.08 47.25 35.03 35.89 62.2 50.23
MGS-METEOR 36
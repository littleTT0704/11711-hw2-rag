 it
improvementbroughtbylarge-scalepretrainingismoresig-
withtheground-truthannotations,wecomputetherecallfor
nificant. However, on MSR-VTT, our model still outper-
eachtaskandthendotheaverage. Accordingtotheresults
forms MIL-NCE [34] which uses the same video features.
in Table 9, our model achieves the best performance com-
InFig.2,weshowthezero-shotperformanceonYouCook2
pared with previous works. This indicates that our model
and MSR-VTT when pretraining our models with differ-
canlearngoodvideo-languagerepresentations.
ent contrastive losses as listed in Table 3. Accordingly, it
Wefurtherevaluateourpretrainedmodelonactionseg-
shows our proposed contrastive losses gradually improve
mentationtaskonCOINdataset,following[34,60]. Unlike
theperformance,andcombiningalltechniquesachievesthe
theabovetask, actionsegmentationdoesnotrelyontexts,
best performance. Based on the pretrained model, we fur-
and thus can be used to evaluate the learned video repre-
therfinetuneitonspecificdatasets. Inourexperiments,we
sentation. As shown in Table 9, our method significantly
usetwofeatureS3D-HMandR-152+S3D-HM,tocompare
outperforms MIL-NCE and ActBert, and achieves compa-
withthemethodswiththesame/similarsettings. Aswecan
rableperformancetoUniVL.Thisindicatesthatourmodel
see, our model using S3D-HM outperforms UniVL [33]
isalsoagoodvideorepresentationlearner.
using the same feature but more video encoder layers (6).
Differentfromzero-shotresults,weobservemoreimprove- 6.Conclusion
ment on MSR-VTT than YouCook2 after finetuning. This
In this paper, we introduced TACo, a simple yet ef-
impliesthatfinetuningonspecificdatasetscancompensate
fective contrastive learning method for learning video-text
thed
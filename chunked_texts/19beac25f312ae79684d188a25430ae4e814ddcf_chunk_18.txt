assplitintotwosep- ble3. Afterwards,agentsareevaluatedbasedontheirper-
arate phases—practice/training and recording/testing. Ex- formanceonTrack03:Vegas,followingthe1-hourpre-
pertplayerswerealreadyfamiliarwiththesimulator, task, evaluation period described in Section 4.4. Learning-free
and objective, prior to engaging in the event. In the train- agents,RANDOMandMPC,simplyperforminferenceinthe
ing phase, players were instructed to engage in the race, testingenvironment. TheRL-SACagent,alearning-based
until the variance in finished lap-times, for three consecu- approach,operatesinvision-onlymodeandutilizesthepre-
tiveruns,fellbelowacertainthreshold. Afterthistraining evaluation stage to perform simple transfer learning to the
phase,playerswereallowedtoproceedtothetestingphase, new racetrack. The agent’s image encoder does not have
forwhichtheirtop-3lapswererecorded. Weaveragedthe accesstothetesttrackpriortopre-evaluationandisnotup-
top-3resultsinthetestingphase,fromallexperts,foreach datedduringthisphase,butthemodelweightsoftheagent
track;thetrainingresultswerediscarded. doupdateasnewexperiencebecomesavailable. Following
the pre-evaluation phase, agents completed 3 consecutive will effectively assess models, based on their general un-
episodes,andwepresentmetricaveragesinTable4. derstandingofvehicledynamics, high-speedandhigh-risk
Humanexperts. Humanexpertsstronglyoutperformdur- control,racetrackperception,andintelligentracingtactics.
ing both training and testing, suggesting a general under-
Tochallengestate-of-the-artlearningapproaches,which
standing of racing: they can quickly adapt to a new track,
continue to demonstrate superhuman performance in sim-
despite different features, including frequent and severe
plisticenvironments,webelievethatthedirectionoffuture
turns. Human experts fully complete 3 lap episodes at
tasksmustbetowardshighercomplexityandrealism. Our
speedsne
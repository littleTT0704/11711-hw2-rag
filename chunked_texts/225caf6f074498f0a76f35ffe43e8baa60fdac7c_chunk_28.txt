(unseen)ontheTatoebadataset.
similarity, and BUCC with margin (Artetxe and
Schwenk,2019a), where VMSST hasbetterper-
formance on English semantic similarity, BUCC
contrastive baseline, the increased batch size in-
withcosinesimilarity,andtheretrievaltasks. For
creases the chances of encountering harder nega-
the24layervariations, VMSST isbetteratevery
tive examples and will generally increase perfor-
task, with the exception of Tatoeba, and has the
manceuptothepointwherethenegativesbecome
highestoverallscoreofanymodelinthetable. The
false. Furthermore. biggerbatchsizesareknown
24layerCONTRASTIVEvariationdoesnotperform
toalsoimproveresultsinmodelsusingtheTrans-
aswellasthe6layerversionatanybatchsize,in
formerarchitecture,presumablyduetolessnoisy
contrastto VMSST wherethe24layermodelal-
gradients,whichwouldimprovetheresultsofboth
waysoutperformsthe6layervariation.
CONTRASTIVE and VMSST. It is important to
note that using bigger batch sizes, means seeing
8 Conclusion
more examples (100,000 steps at a batch size of
2048isabout1passthroughthedata). However, Wepresent VMSST,agenerativemassivelymulti-
parallel data is so numerous that training to con- lingualtextembeddingmodeltrainedtoseparate
vergenceontheavailabledataisnotverypractical. semantic information from language-specific in-
Therefore, these experiments do not separate out formation. VMSSTalsooutperformsstrongcon-
the gains from using a bigger batch size versus trastive and generative baselines on a variety of
seeingmoretrainingdata,butwearguethatisnot tasks. There are several avenues for future work
animportantdistinctiontomakeduetothesheer includingalternativepretrainingobjectivesthatbet-
ter fit the use case of the decoder, explore incor- EnekoAgirre,CarmenBanea,ClaireCardie,DanielCer,
poratingmonolingualdataintothegenerativeob
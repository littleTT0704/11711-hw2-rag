self-attention. InProceedingsof tokens.
CPU GPU GPUArch CoreCount Tensor ClockRate Memory Mem BW FP16
Cores (GHz) (GB) (GB/s) TFLOPS
IntelXeonSilver4110 1080Ti Pascal 3584 - 1.38 11 484.4 22.78
IntelXeonGold6242 V100 Volta 5120 640 1.23 32 897 28.26
IntelXeonE5-2630 2080Ti Turing 3584 544 1.35 11 616 26.90
IntelXeonE5-2630 RTX-8000 Turing 4608 576 1.40 48 672 32.62
AMDEPYC7282 3090 Ampere 10496 328 1.35 24 936 35.89
IntelXeonSilver4110 A6000 Ampere 10752 336 1.41 48 768 38.71
IntelXeon8339HC A100 Ampere 6912 432 1.215 40 1935 77.97
Table2: Detailsonhardwareplatformsusedinourexperiments,orderedbyNvidiamicroarchitecturegeneration.
Framework GraphCompilation KernelSerialization Latency SMsActive WarpOccupancy
PyTorch None None 10.54ms 2.6% 0.9%
PyTorchwithTorchScript Just-in-Time None 6.14ms 18.5% 3.0%
PyTorchwithCUDAGraphs None Yes 2.82ms 57% 9.2%
ONNXRT Ahead-of-Time None 2.56ms 22.3% 9.5%
ONNXRTwithCUDAGraphs Ahead-of-Time Yes 2.11ms 59% 20.3%
Table3: ComparisonofSMActivityAcrossFrameworksforBERT-Baseatbatchsize1.
Figure 12: Latency of vision models using efficient
variationsofconvolutionandself-attentionoperations.
E AdditionalExperiments: Speech
Figure10: Latencyofgenerativelanguagemodelsfor
In Figure 13, we examine the behavior of the
varyingbatchsizes.
WavLM(Chenetal.,2022)modelwhichconsists
ofaCNNencoderfollowedbytransformerencoder
D AdditionalExperiments: Vision
layers. Audio inputs are simulated as 2
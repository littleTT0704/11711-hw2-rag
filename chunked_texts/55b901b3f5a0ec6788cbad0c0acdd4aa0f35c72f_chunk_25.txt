 Our experiments show that AANG is apromising firststep in
automatingauxiliarylearning.
9
6-6102-ES
CREICS
PublishedasaconferencepaperatICLR2023
9 ACKNOWLEDGEMENTS
This work was supported in part by DSO National Laboratories, an ENS-CFM Data Science
Chair,DARPAFA875017C0141,theNationalScienceFoundationgrantsIIS1705121,IIS1838017,
IIS2046613andIIS-2112471,anAmazonWebServicesAward,aFacebookFacultyResearchAward,
funding from Booz Allen Hamilton Inc., and a Block Center Grant. Any opinions, findings and
conclusions or recommendations expressed in this material are those of the author(s) and do not
necessarilyreflecttheviewsofanyofthesefundingagencies. Wearegratefulforhelpfulfeedback
fromUriAlon,PatrickFernandes,JoonSikKim,HanGuo,VictorAkinwandeandClaraNa.
REFERENCES
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and
SonalGupta. Muppet: Massivemulti-taskrepresentationswithpre-finetuning. arXivpreprint
arXiv:2101.11038,2021.
PulkitAgrawal,AshvinVNair,PieterAbbeel,JitendraMalik,andSergeyLevine. Learningtopoke
bypoking: Experientiallearningofintuitivephysics. Advancesinneuralinformationprocessing
systems,29,2016.
JonathanBaxter. Amodelofinductivebiaslearning. Journalofartificialintelligenceresearch,12:
149–198,2000.
IzBeltagy,ArmanCohan,andKyleLo. Scibert: Pretrainedcontextualizedembeddingsforscientific
text. CoRR,abs/1903.10676,2019. URLhttp://arxiv.org/abs/1903.10676.
OlivierBousquetandAndre´Elisseeff. Stabilityandgeneralization. TheJournalofMachineLearning
Research,2:499–526,2002.
TomB.Brown
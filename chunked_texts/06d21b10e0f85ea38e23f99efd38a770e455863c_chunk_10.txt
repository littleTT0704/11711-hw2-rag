ata,use
tences,oneperline. Theoutputofthescriptisasaved alanguageclassifiertofilteroutnon-Englishsen-
numpyarrayofsentenceembeddingsinthesameorder tences6 (Joulinetal.,2017),filterthedatabypara-
of the input sentences. The second command is a us- phrasescore,trigramoverlap,andlength,7 traina
age example of scoring a list of sentence pairs. The
sentencepiece model, encode the data with the
fileexample-sentences-pairs.txtcontainspairsof
sentencepiecemodel, andprocessthedatainto
tab-separated sentences, one per line. The output of
HDF5format. ExamplesareshowninFigure3.
thescriptisatextfilecontainingthetabseparatedlist
ofsentencesalongwiththeircosinescoresinthesame
5 Experiments
orderoftheinputsentences.
5.1 ExperimentalSetup
Secondly,weaddedatrainingmodeusingHDF54
Data. For our English model, we train on se-
format, allowing training data to remain on disk
lectedsentencepairsfromParaNMT(Wietingand
during training. This leads to a significant reduc-
Gimpel,2018). Wefilterthecorpusbyonlyinclud-
tioninRAMusageduringtraining,whichisespe-
ingsentencepairswheretheparaphrasescorefor
ciallytruewhenusingmorethan10milliontrain-
thetwosentencesis 0.4. Weadditionallyfiltered
ingexamples. Efficienttrainingcannowbedone â‰¥
onCPUonlyusingonlyafewgigabytesofRAM.
5Weremovesentenceswiththenumberoftokens(untok-
enized)smallerthan3orgreaterthan100.
Lastly, we also added code for preprocessing 6https://fasttext.cc
7Weremovesentenceswiththenumberoftokens(untok-
4https://docs.h5py.org/en/stable/ enized)smallerthan5orgreaterthan40.
382
SemanticTextualSimilarity(STS)
Model
2012 2013 2014 2015 2016 Avg.
BERT(CLS
metricstoidentifysimplebugfixes.
gramusage,and(2)coverdiversedomainsasmea-
ODEXjointlyfacilitatespracticalopen-domain
sured by libraries used. These properties align
code generation and execution-based evaluation.
wellwithourmainfocusonopen-domainqueries.
It serves as a comprehensive data benchmark for
(M)CoNaLafurtherproofsandclarifiesitsNLin-
NL-to-code systems, supporting diverse NL con-
tentsusinghumanannotatorstoensuredataquality.
texts, library usage, and evaluation methods. By
addressing the unique challenges of test creation 2.2 AnnotationStandardandProcedures
and execution, we hope to lay a foundation for
GiveneachsourceNL-Codepair,ourmainanno-
evaluatingopen-domaincodeviaexecution.
tationtaskistowritetestcasestocheckcodeexe-
cutioncorrectness,asillustratedbythefoursteps
2 TheODEXDataset
inFigure2. Aqualifiedtestcaseshouldverifythe
mainfunctionalityofthecanonicalcodesolution.
Inthissection,wedescribeourfour-stepprocess
Inthecasewhereannotatorsdonotunderstandthe
ofconstructingtheODEXdataset. Wefirstcollect
languageoftheintent,weusetranslationtoolssuch
resourcesofnatural,open-domaincodingqueries
astheGoogleTranslateAPI.3
(ยง2.1). Next,weestablishtheannotationstandard
and procedures for test case creation (ยง2.2). We
Step 1: Wrapping Snippets into Functions
thendescribetheannotatorhiringandworkingpro-
Codesolutionsin(M)CoNaLaareoftenshortsnip-
cesses(ยง2.3). Finally,weconductcheckstoensure
dataquality(ยง2.4). 3https://translate.google.com
NL Calculate sum over all rows of 2D numpy array `a` notation quality. We execute the canonical code
solutiononeachnewlycreatedtestcase. Unlessthe
Code
testcaseenablesasuccessfulpassofthesolution,
Step 1
code wrapping itshould
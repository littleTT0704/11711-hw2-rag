
3 3
encoded in feature magnitude during training while still 2 3 2 3
encouraging a feature normalization effects. Moreover, the θi 1 0 0 1 θy 2 θi 1 0 0 1 θy 2
difference between SFN and HFN can be viewed as using (c) SphereFace-R v1 (s=40) (d) SphereFace-R v2 (s=60)
different optimization techniques to constrain the feature
Fig.5.Acomparisonoflosscharacteristicsamongnormalizedsoftmax,
norm to a prescribed constant. HFN has the flavor of pro-
SphereFace,SphereFace-Rv1andSphereFace-Rv2.
jectedgradientdescentwherethesolutionwillbeprojected
tothefeasibleregiontosatisfysomeconstraint.Incontrast,
SFN is essentially a Lagrangian relaxation of the original of similarity score as S(x 1,x 2) = g((cid:107)x 1(cid:107),(cid:107)x 2(cid:107))·cos(θ 1,2)
problem where the feature norm is constrained. Therefore, where x 1,x 2 are deep features of two input samples, θ 1,2
their empirical performance could be quite different in istheanglebetweenx
1
andx 2,andg((cid:107)x 1(cid:107),(cid:107)x 2(cid:107))denotes
practice,eveniftheysharethesameoptimizationtarget. a function with the norm of x 1 and x 2 as input. We may
Dynamicfeaturemagnitude.IncontrasttoHFNthatusesa requirethefunctiong((cid:107)x 1(cid:107),(cid:107)x 2(cid:107))tohaveafewproperties:
staticfeaturemagnitude,bothFN-freelearningandSFNcan (i) permutation invariance: g((cid:107)x 1(cid:107),(cid:107)x 2(cid:107)) = g((cid:107)x 2(cid:107),(cid:107)x 1(cid:107))
beviewedasadynamic
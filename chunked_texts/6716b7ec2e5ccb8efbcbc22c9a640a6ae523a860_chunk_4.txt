becomeapopularchoice
(Lewisetal.,2020a)whichispre-trainedusinga
foropen-domainQA,increasinglyusingdensere-
denoisingobjectiveandavarietyofdifferentnois-
trieval methods such as DPR (Karpukhin et al.,
ingfunctions. Moreover,RAGmarginalizesoutput
2020). We study works related to four areas
fromeach(query,passage)pairbasedonretrieval
for modeling improvement: retrieval, reranking,
scores. It has obtained state-of-the-art results on
reader,andtraining.
adiversesetofgenerationtasksandoutperforms
Retriever: AstheMultiDoc2Dialtaskisformu-
comparably-sizedT5models.
lated in an open-domain setting, it requires the
Fusion in Decoder (FiD) (Izacard and Grave,
retrievalofrelevantsources(passages)fromalarge
2021)performswellinextractive-basedQAtasks
poolofdocumentsforgeneratingtherightoutput.
likeNaturalQuestions(Kwiatkowskietal.,2019).
Hence, we investigate the strides in information
UnlikeRAGmodel,theindependentprocessingof
retrievalinrecentyears.
the passages on the encoder side allows the FiD
Recently,denseretrievalbasedapproacheshave
modeltoscaletoalargenumberofpassages,while
showncompetitiveperformance(Karpukhinetal.,
thefusioninthedecodereffectivelycombinesevi-
2020; Xiong et al., 2020; HofstaÂ¨tter et al., 2021)
dencefrommultiplepassages.
while also scaling to large corpora, like MS-
Training : Works such as Xu et al. (2020) have
MARCOdataset(Nguyenetal.,2016). Theyuse
shownthatfine-tuningatransformermodelonex-
anearestneighborindex,suchasFAISS(Johnson
amples,orderedonthebasisoftheirdifficulty,re-
etal.,2019)toensurescalability. Denseretrieval
sultsinsignificantperformancegainsacrossdiffer-
techniquesaimstoencodethequeryand
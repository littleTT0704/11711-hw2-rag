standpoint,butlow-resourcetasksalsobenefitthemostfrom
auxiliarylearning. Wealsochoosethesetasksbecausetheyfeatureinpreviousworkwhichweuse
asbaselines(Gururanganetal.,2020;Deryetal.,2021b)
BaselinesandSearchSpaces: Thefollowingmethodsareend-taskagnosticbaselines. Byend-task
agnostic,wemeanthatthesedonotmultitaskwiththeend-task. Finetuningontheend-taskoccurs
aftertrainingontheauxiliaryobjective.
1. RoBERTa(Liuetal.,2019b): Wesimplyfinetuneapre-trainedRoBERTa ontheend-task.
base
2. TAPT (Gururangan et al., 2020): Continue training RoBERTa on masked language
base
modellingonend-taskdataitselfbeforefinetuningontheend-task.
Thefollowingnamedobjectivesareend-taskawarebaselinesthatuseMETA-TARTAN(Deryetal.,
2021b)bututilizeonly1auxiliarytask. Eachauxiliaryobjectiveismulti-taskedwiththeend-task.
1. GPT-style: Weperformend-taskawaretrainingwithadenoisingauxiliaryobjectivebasedon
left-to-rightcausalmaskingforcomputingrepresentations. =End-taskdata, =No-op, =
{I T R
Left-To-Right, =DenoiseToken.
O }
2. XLNET-style:Thisisadenoisingauxiliaryobjectivethatusesrandomizedmaskingforcomputing
representations. =End-taskdata, =No-op, =Random-factorized, =DenoiseToken.
{I T R O }
3. BERT-style/TAPT:DenoisinginputscorruptedviaBERT-Op: 80%maskingand10%random
replacement. =End-taskdata, =BERT-Op, =Bi-directional, =DenoiseToken. Please
{I T R O }
notethatthisbaselineisequivalenttoMETA-TARTANasintroducedinDeryetal.(2021b).
Table1:AANG
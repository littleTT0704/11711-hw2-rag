. Our code and data are pub- icity,inspiredbyworkinpragmaticsandsociolin-
liclyavailableonGithub.2 guistics of rudeness (Dynel, 2015; Kasper, 1990,
inter alia). Specifically, we manually split our
2 BiasesinToxicLanguageDetection full list of words into three distinct categories de-
pendingontheextenttowhichtheycarryprofane
We test the use of debiasing3 methods for the
orhatefulmeaningsoraresimplyassociatedwith
task of toxic language detection, which aims to
hatefulcontexts.5 Werefertothefullsetofwords
flag rude, offensive, hateful, or toxic language on
as TOXTRIG, for Toxicity Triggers, which is in-
the internet, with the goal of moderating online
cludedinourreleasedrepository.6
communities(Roberts,2019;Vidgenetal.,2019).
2https://github.com/XuhuiZhou/Toxic_ 4https://tinyurl.com/list-of-bad-words
Debias 5Wenote,however,thatthiscategorizationisinitselfsub-
3Our definition of “bias” is specific to the social biases jective.
intoxiclanguagedetectiondatasets,groundedaslexicaland 6https://github.com/XuhuiZhou/Toxic_
dialectalbiases; seeBlodgettetal.(2020)foradetailedin- Debias/blob/master/data/word_based_bias_
vestigationoftheterm“bias”. list.csv
3144
Non-offensive minority identity mentions on African-American English (AAE) and white-
(NOI) refers to descriptive mentions of minori- aligned English (WAE) tweets; both definitions
tized demographic or social identities (e.g., gay, are based on US English, as per Blodgett et al.
female, Muslim). While these mentions are not (2016).7 Our experiments either use the proba-
usually inherently offensive by themselves, they bility of a tweet being in these dialects, or assign
are often found in offensive statements that are tweetstheirestimated-most-pro
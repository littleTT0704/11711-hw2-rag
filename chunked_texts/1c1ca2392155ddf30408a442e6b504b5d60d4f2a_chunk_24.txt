ems
esuoh
hsilomed
larum
esare
newconditionsinthescenario;andamongtheexplanationsthatrefertothespecificconditionsin
thescenario,thereasoningqualityis73%,wheretheerrorcasesareoftenbeingtoodogmatic,e.g.,
banning kids to cannonball even when “there is no art class” to be disturbed. The details of this
analysisareintheAppendix.
DependenceontheLiteralText. LLMsaregoodatpickingupcorrela- Keyword Corr.(↓)
tions. OnepossiblehypothesisisthatsomeerrorsmaycomefromLLMs
Alldata 0.190
associatingcertainwordsdirectlywithamoraldecision,butnotcapturing
Bathroom 0.902
thesemanticmeaning. Toillustratethis,weextractallpossiblepairofin- Noise 0.503
puts(t i,t j),andrecordtheirtextcosinesimilaritys i,j byageneral-purpose Lines 0.377
sentence similarity model, all-distilroberta-v1 (Sanh et al., 2019), along Million 0.298
withpredictedpermissibilitysimilarityd =−|pˆ −pˆ |. Wecalculatethe Cannonball 0.196
i,j i j
Pearsoncorrelationbetweenthes ’sandd ’s. Thecloserthecorrelation BlueHouse 0.071
i,j i,j
isto1,themorethepredictionreliesontextualsimilarity. InTable5,we Snack -0.042
noticethatthecorrelationacrossalldatais0.190. Wealsocheckwhether Hundred -0.870
thiscorrelationchangesgivendifferentscenariokeywords,e.g.,0.902inthe
Table5:Correlationbe-
subsetaboutcuttinginlinetothe“bathroom.” FulldetailsareinAppendix.
tweenlabelprediction
andtextualsimilarity.
5.3 Discussions
Limitations and Future Directions. One limitation – and opportunity for improvement – is the
datasetsize. Futureworkcouldcollectalargerdatas
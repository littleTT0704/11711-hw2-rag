ize. SimilartothefindingofGehmanetal.
presentinTable4. (2020), ATCON modelisonlyslightlylessoffen-
sive than the DGPT baseline and doesn’t reduce
According to human evals, the DAPT model
theagreementrate. Therefore,wefindfinetuning
achieves the lowest ‘agree’ responses and high-
onsafeandneutralconversationsi.e. DAPTtobe
est ‘neutral’ responses but is slightly more offen-
themosteffectivetechniqueinreducingoffensive
sivethanFacebook’sBlenderchatbot. Blenderis
behaviorinchatbots,butitisstillfarfromperfect.
the least offensive but most agreeing among all
evaluatedmodels. Thisimpliesthatouroffensive
7 RelatedWork
14Thetestthreadsusedtoevaluatedialoguemodelsdidn’t
IdentifyingToxicity-Mostworksonidentifying
haveafollow-upReddituserresponse. Hence,wecollecta
differentsetof500offensivethreadswithafinaluserresponse. toxiclanguagelookedatisolatedsocialmediaposts
or comments while ignoring the context (David- mediauserstoformecho-chambers(Cinellietal.,
son et al., 2017; Xu et al., 2012; Zampieri et al., 2021; Soliman et al., 2019). Consequently, dia-
2019; Rosenthal et al., 2020; Kumar et al., 2018; logue models learn to mimic this behavior and
GariboiOrts,2019;Ousidhoumetal.,2019;Bre- agreemorefrequentlyinoffensivecontexts. How-
itfeller et al., 2019; Sap et al., 2020; Hada et al., ever,fine-tuningdialoguemodelsoncleanertrain-
2021; Barikeri et al., 2021). These methods are ing data with desirable conversational properties
ill-equipped in conversational settings where re- (safeandneutralresponseswithDAPT)canmiti-
sponses can be contextually offensive. Recently, gatethisissuetosomeextent. Tofurtherstrengthen
Dinanetal.(2019a);Xuetal.(2020)studiedcon- dialogue safety, future research
ana-
(left)andincorrect(right)prediction, accordingtodot
product(DP),probesneeded(PN),andmeanreciprocal tionmethods,wenextdescribemethodologyand
rank(MRR).Alignmentscoresthatarebetterthanthe experiments to measure to what extent explana-
score for the analogous explanation method with the tionscanimprovetheabilityofuserstopredictthe
differentcontrastivesettingarebolded. outputofthemodel,namelymodelsimulatability
(Lipton,2018;Doshi-VelezandKim,2017).
In Table 3, we further examined alignment be-
tween model explanations and known evidence 5.1 StudySetup
on instances where the model correctly allocates Our user study is similar in principle to previous
moreprobabilitytotheacceptabletoken,orincor- worksthatmeasuremodelsimulatabilitygivendif-
rectlyselectstheothertoken. Onexampleswhere ferentexplanations(Chandrasekaranetal.,2018;
themodelmakesanincorrectprediction,itisnot Hase and Bansal, 2020; Pruthi et al., 2020). In
clearwhethernon-contrastiveorcontrastivemeth- our study (Figure 3), users are given the input of
odshavebetteralignment. Onexampleswherethe a GPT-2 model, two choices for the next token,
modelpredictscorrectly,contrastiveexplanations andanexplanationforthemodeloutput. Theyare
obtainbetteralignmentthantheirnon-contrastive asked to select which of the two choices is more
counterpartsforeachexplanationmethodandalign- likelythemodeloutput,thenanswerwhetherthe
mentmetric. explanationwasusefulinmakingtheirdecision3.
In Figure 2, we see that for most explanation
Wecomparetheeffectofhavingnoexplanation,
methods,thelargerthedistancebetweentheknown
explanations with Gradient Input, Contrastive
×
evidence and the target token, the larger the in-
Gradient Input, Erasure and Contrastive Era-
×
creaseinalignmentofcontrastiveexplanationsover
sure. WedonotincludeGradientNormandCon
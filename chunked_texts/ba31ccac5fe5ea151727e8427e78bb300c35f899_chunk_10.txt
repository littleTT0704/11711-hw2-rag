 ratio of 1:1 in the
ferent than that in the conventional scenarios by
trainingprocess,whichwefindworkswell.
applyingthecurrentmodeltoalltheun-annotated
instances in the unlabeled pool. The more inter- 3 Experiments
estingcaseisonthepartiallyannotatedinstances
3.1 MainSettings
in the PA regime. The same motivation from the
adaptiveratioscheme(ยง2.2)alsoapplieshere: We Tasksanddata. Ourexperiments6areconducted
selectthehighly-uncertainsub-structuresthatare over four English tasks. The first two are named
error-prone and the remaining un-selected parts entityrecognition(NER)anddependencyparsing
are likely to be correctly predicted; therefore we (DPAR), which are representative structured pre-
can trust the predictions on the un-selected sub- dictiontasksforpredictingsequenceandtreestruc-
structuresandincludethemfortraining. Onemore tures. WeadopttheCoNLL-2003Englishdataset
enhancementtoapplyhereisthatwecouldfurther (TjongKimSangandDeMeulder,2003)forNER
performre-inferencebyincorporatingtheupdated and the English Web Treebank (EWT) from Uni-
annotationsovertheselectedsub-structures,which versalDependenciesv2.10(Nivreetal.,2020)for
can enhance the predictions of un-annotated sub- DPAR.Moreover,weexploretwomorecomplex
structuresthroughoutputdependencies. IEtasks: Eventextractionandrelationextraction.
In this work, we adopt a soft version of self-
6Our implementation is available at https://github.
trainingthroughknowledgedistillation(KD;Hin- com/zzsfornlp/zmsp/.
Each task involves two pipelined sub-tasks: The 3.2 ComparisonScheme
firstaimstoextracttheeventtriggerand/orentity
SinceFAandPAannotateatdifferentgranularities,
mentions, and the second predicts links between
weneedacommoncostmeasurementtocompare
these mentions as event arguments or entity rela-
their effectiveness properly
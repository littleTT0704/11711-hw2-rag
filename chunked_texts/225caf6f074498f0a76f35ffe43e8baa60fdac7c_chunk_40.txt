 we eval-
thenon-EnglishsourcesentencesandtheEnglish
uate on the STS tasks from SemEval 2017. This
targetsentences,selectingthehighestscoringEn-
evaluationcontainsArabic-Arabic,Arabic-English,
glishsentenceasthematch. Inthesecond,BUCC
Spanish-Spanish, Spanish-English, and Turkish-
(margin),wefollowArtetxeandSchwenk(2019a)
English datasets. The datasets were created by
anduseamargin-basedscoringapproach.
translating one or both pairs of an English STS
pairintoArabic(ar),Spanish(es),orTurkish(tr).
B FullTrainingData
Followingconvention,wereportresultswithPear-
son’sr forallsystems,butalsoincluderesultsin We follow Artetxe and Schwenk (2019b) in con-
Spearman’sρinTable10. structingourtrainingdata,samplingdatafromEu-
roparl,14, United Nations (Rafalovitch and Dale,
A.2 QuestionRetrieval 2009),15 OpenSubtitles2018(Lisonetal.,2018),16,
GlobalVoices,17 Tanzil,18 andTatoebav2021-07-
Forourquestionretrievalevaluation,wereportthe
22.19
accuracy (R@1)onthe testsets ofNaturalQues-
tions (NQ) (Kwiatkowski et al., 2019) shown in 14http://opus.nlpl.eu/Europarl.php
Table 11 and the Multilingual Knowledge Ques- 15https://opus.nlpl.eu/UN.php
16http://opus.nlpl.eu/OpenSubtitles.php
tionsandAnswers(MKQA)(Longpreetal.,2021)
17https://opus.nlpl.eu/GlobalVoices.php
showninTable12. WeusethetheProbablyAsked 18https://opus.nlpl.eu/Tanzil.php
Questionsdataset(PAQ)(Lewisetal.,2021)asa 19https://opus.nlpl.eu/Tatoeba.php
Model EnglishSemanticSimilarity
2012 2013 2014 2015 2016
CONTRASTIVE 69
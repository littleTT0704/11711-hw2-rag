uperiorresults
k
wk ∝exp(cid:0) WAll +WI+WT+WR+WO(cid:1) comparedtoalternatingoptimizationwhen
(d,t,r,o) d t r o leveragingauxiliaryobjectives(Aghajanyan
wn ←wk etal.,2021). Weperformgradientdescent
k
endfor onthefollowingtotallosswhichinterpolates
Get losses from batches of data betweentheend-taskandtheauxiliaryloss
Lˆ A(Kn,wn)=(cid:80)n k=1wkL k total = λ e E +(1 λ e) K. Here, is
L =λ L +(1−λ )Lˆ L achosensubL setof.− L K
total e E e A
Get gradients and update factors A
(cid:0) Second,asindicatedinSection4,given,
θ U Ut p p+ d d1 a a, t t{ e e∇ { λWw en uA, sλ ill ne, g} W ∇← I λ, eM WE TTA,W-TA RR,WTA ON }θ ut s, inE g, ∇L t wot nal) w
f
ae =can (cid:80)w kr ∈it Ke wth ke
f
akse.t Bas ya Prs ein scg rl ie pto iob njec (Ptiv 1K )e
,
untildone we want to choose wk such that f has
a
Return: θ T asmall∆withthee{ nd-t} askf e. Wewould
also like to set λ such that the bound on (cid:15) is minimized. Whilst a closed form exists for the
e gen
optimalweightingsλ, wk,itdependsonvariableslike ∆k, βk,Lthatarehardtoestimate.
e { } { } { a}
4Thisholdsatfixedγwhichweachievebyadjustingλ toaccountforintroducingmoreauxiliarydata.
e
5
PublishedasaconferencepaperatICLR2023
We therefore
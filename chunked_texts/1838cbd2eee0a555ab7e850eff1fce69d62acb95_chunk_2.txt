olingual corpora or large Corpora. However,thesemodelsstillleavebehind
amountsoflabeleddata–fortaskslikecross- more than 200 languages with few articles avail-
lingual sentiment analysis and named entity ableinWikipedia,nottomentionthe6,700orso
recognition show the effectiveness of our ap-
languages with no Wikipedia text at all (Artetxe
proach.CodeforMetaXLispubliclyavailable
et al., 2020). Cross-lingual transfer learning for
atgithub.com/microsoft/MetaXL.
these extremely low-resource languages is essen-
1 Introduction tialforbetterinformationaccessbutunder-studied
inpractice (HirschbergandManning,2015). Re-
Recentadvancesinmultilingualpre-trainedrepre- centworkoncross-lingualtransferlearningusing
sentationshaveenabledsuccessonawiderangeof pre-trainedrepresentationsmainlyfocusesontrans-
naturallanguageprocessing(NLP)tasksformany ferringacrosslanguagesthatarealreadycovered
languages. However, these techniques may not byexistingrepresentations(WuandDredze,2019).
readily transfer onto extremely low-resource lan- In contrast, existing work on transferring to lan-
guages, where: (1) large-scale monolingual cor- guageswithoutsignificantmonolingualresources
poraarenotavailableforpre-trainingand(2)suf- tends to be more sparse and typically focuses on
ficient labeled data is lacking for effective fine- specifictaskssuchaslanguagemodeling(Adams
tuning for downstream tasks. For example, mul- etal.,2017)orentitylinking(Zhouetal.,2019).
tilingualBERT(mBERT)(Devlinetal.,2018)is BuildingNLPsystemsinthesesettingsischal-
pre-trainedon104languageswithmanyarticleson lenging for several reasons. First, a lack of suf-
ficient annotated data in the target language pre-
∗Mostoftheworkwasdonewhilethefirstauthorwasan
internatMicrosoftResearch. vents effective fine-tuning. Second, multilingual
pre-trained representations are not directly trans- languages1 andthenapplyittothetargetlanguage.
ferableduet
ALCOT,amoralphilosophy-inspiredchainofthoughtpromptingstrategy
toelicitmulti-stepmulti-aspectmoralreasoningforLLMs;
3. Weshow6.2%improvementbyourmodeloverthebeststate-of-the-artLLM;
4. We conduct a detailed error analysis showcasing the limitations of LLMs in our moral
flexibilitystudyandsuggestdirectionsforfutureprogress.
2
2 Background
2.1 ImportantQuestionsforAISafety
AI Safety. The fundamental goal of AI safety is to ensure that AI models do not harm humans
(Bostrom and Yudkowsky, 2014; Russell, 2019; Tegmark, 2017; Hendrycks et al., 2021d). AI
systemsaretrainedtooptimizegivenobjectives. However,itisnoteasytodefineaperfectobjective,
becausecorrect,formalspecificationsrequireustoexpressmanyofthehumanvaluesthatarein
thebackgroundofsimpleobjectives. Whenweaskarobottofetchcoffee,forinstance,wedonot
mean: fetchcoffeenomatterwhatittakes. Wemeansomethingmorelike: fetchcoffee,ifcoffeeora
reasonablesubstituteisavailableatareasonableprice,withinareasonabletimeframe,andwhen
thefetchingwillnothaveanon-trivialexpectationofendangeringotheragentsorimpedingmore
importantgoals,weighingmygoalsassomewhatmoreimportantthanthoseofothers. AIsafety
researchers point out that human objectives and their associated values are often too complex to
captureandexpress(BostromandYudkowsky,2014;Russell,2019).
However, recentresearch in the field ofcognitive science has begun to reveal that human values
indeedhaveasystematicandpredictablestructure(Mikhail,2011;Greene,2014;Kleiman-Weiner
etal.,2015). Ofcourse,valuesvaryacrossculturesâ€“andevenacrossindividualswithinasingle
culture. Sometimes even the same individual can hold conflicting values or make contradictory
judgments. Despite this important and pervasive variation in human moral judgment, it is still
possible to describe systematic ways that a particular population of humans responds to morally
chargedcases. Inthispaperwedrawonrecentadv
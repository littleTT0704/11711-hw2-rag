 making them more adept at
on tens of million of sentence pairs resulting in
detectingparaphrasesbutslightlyweakeratiden-
models that achieve state-of-the-art performance
tifying translations. It is also worth noting that
on unsupervised English semantic similarity and
LASER was trained on Tatoeba data outside the
arestate-of-the-artorcompetitiveonnon-English
testsets,whichcouldalsoaccountforsomeofthe
semanticsimilarity,cross-lingualsemanticsimilar-
slightimprovementoverourmodel.
ity,andbitextmining.
Moreover, our models are significantly faster
7 SpeedAnalysis
thanpriorworkowingtotheirsimplearchitecture.
TheycanalsoberunonCPUwithlittletonoloss
inspeedfromrunningthemonGPU—-something
Model GPU CPU
thatnostrongmodelsfrompriorworkareableto
P-SP 13,863 12,776
do. Lastly,wereleaseourcodethathasbeenmod-
LASER 6,033 26
Sentence-Bert 288 2 ified to make training and inference easier, with
InferSent 4,445 16 supportfortrainingonlargecorpora, preprocess-
Table 5: Speed as measured in sentences/second on ingparaphraseandbilingualcorporaandevaluation
bothGPU(Nvidia1080TI)andCPU(singlecore). data,aswellasscriptsforeasyinferencethatcan
generateembeddingsorsemanticsimilarityscores
Weanalyzethespeedofourmodelsaswellasse-
forsentencessuppliedinatextfile.
lected popular sentence embedding models from
priorwork. Toevaluateinferencespeed,wemea-
surethetimerequiredtoembed120,000sentences References
fromtheTorontoBookCorpus(Zhuetal.,2015).
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Preprocessingofsentencesisnotfactoredintothe Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
timing, and each method sorts the sentences by Guo,IñigoLopez-Gazpio,MontseMaritxalar,Rada
length prior to computing the embeddings to re- M
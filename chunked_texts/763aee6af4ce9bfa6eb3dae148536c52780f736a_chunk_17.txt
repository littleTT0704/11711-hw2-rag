orsaremixedtogeneratetheimagebyvaryingi∈{1.0,0.9,...,0.1}.
Theorem1 (Empiricaldualitygap)WhenAssumption2 [3], GDRO [66], Mixup [80], MLDG [44], CORAL [73],
holds,bydenotingmax{L (cid:15),L (cid:15) }asm,wehave MMD[46],DANN[23],CDANN[47],AugMix[30].
(cid:96) s d g
AllthebaselinesinDGtasksareimplementedusingthe
(cid:114)
(cid:12) (cid:12)P(cid:63)−Dε(cid:63),n(γ)(cid:12) (cid:12)≤(1+|λ|)m+O( log n(n) ). (11) c sto ud de ib ea sse uso if nD go thm eai on fb fie cd ia[ l27 im]. pW lee ma ed na tp at tiA ou ng sM asix info dr ica ab tl ea dtio inn
[30]. The two-sample classifier, implemented as a RBF
Thefinalboundtellsusthatthequalityoftheempirical,
kernelSVMusingScikit-learn,isusedforcalculatingthe
dualapproximationoftheprimalproblemisdeterminedby generalizationerrorforA-distance.
thesamplesize,thehardnessofthelearningproblem,and
Hyperparameter search. Following the experimental
the richness of parameterization. The proof can be easily
settingsin[27],weconductarandomsearchof20trialsover
shown using triangle inequality as in Appendix A.3. As
thehyperparameterdistributionforeachalgorithmandtest
suggestedbyTheorem1,wecanimprovetheperformanceof
domain. Specifically, we splitthe datafrom each domain
ouralgorithmbyusingneuralnetworkswithlargercapacity
into 80% and 20% proportions, where the larger split is
ortrainingourmodelwithmoredata.
usedfortrainingandeval
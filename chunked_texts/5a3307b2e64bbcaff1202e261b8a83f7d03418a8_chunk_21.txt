qiaoYang1,JingluWang3,RitaSingh1,BhikshaRaj1,4
Figure4:Thenormalizederrorsandğ¶ğ¼sof24AMson(a)malesubset,(b)femalesubset,and(c)asmallerfemalesubset.If1âˆ’ğ¶ğ¼
ğ‘¢
>0,
theAMispredictableelseunpredictable.
4.1 Dataset 4.2 ImplementationDetails
WeperformexperimentsonaprivateaudiovisualdatasetD.The We leverage a backbone E to learn voice codeğ‘’ which is a sim-
datasetconsistsofpairedvoicerecordingsandscanned3Dfacial pleconvolutionalneuralnetwork.Thedetailednetworkstructure
shapesfrom1,026people,with364malesand662females.The ispresentedinthesupplementarymaterials.ğ¹ ğ‘˜ andğº ğ‘˜ sharethe
scanned3Dfaceisstoredinthemeshformatwith6790pointsfor backboneâ€™slearnableparametersbuthaveindividualparametersfor
eachface.Thevoicerecordingsareabout2minuteslongforeach theirheads.Weuseasinglelayerfully-connectednetworkforeach
speaker. We reduce the influencing factors to the voice and face head.Forthevariancehead,weaddanexponentialactivationto
by(1)askingparticipantstospeakasetofspecifiedsentences,(2) thelastlayerofğº ğ‘˜ fornon-negativepositiveoutput.Wefollowthe
askingparticipantstospeakwithoutemotion,(3)controltheageof typicalsettingsofstochasticgradientdescent(SGD)foroptimiza-
participants(roughly18-28yearsold).Inaddition,topreventthe tion.Minibatchsizeis64.Themomentum,learningrate,andweight
modelsfromtakingthegendershortcuts,wesplitthedatasetDby decayvaluesare0.9,0.1,and0.0005,respectively.Thetrainingis
gender,andexperimentsareindividuallyperformedonmaleandfe- completedat5kiter
 on 2012–2016 SemEval Semantic sentence with its nearest neighbors. To compute
TextualSimilarity(STS)sharedtasks(Agirreetal., thismarginscore,wedividethecosinesimilarity
2012,2013,2014,2015,2016). Morespecifically, for source sentence s and target sentence t by
i i
for each year of the competition, we average the thesumofthescoresofthefournearestneighbors
Pearson’s r × 100 for each dataset in that year, ofs withthetargetsentencesandthesumofthe
i
and then finally average this result for each year scoresofthefournearestneighborsoft withthe
i
of the competition. For the cross-lingual evalua- sourcesentences.
tion we use the cross-lingual STS tasks from Se- Margin-based scoring is designed to alleviate
mEval 2017 (Cer et al., 2017). This evaluation the“hubnessproblem”(Radovanovicetal.,2010;
containsArabic-Arabic,Arabic-English,Spanish- Radovanovic´ etal.,2010)wheretheneighborhood
Spanish, Spanish-English, and Turkish-English around embeddings in a high-dimensional space,
STSdatasets. Thesedatasetswerecreatedbytrans- likeinsentenceembeddings,havemanyneighbors
lating one or both pairs of an English STS pair incommon. Theseneighborscandisplacethecor-
intoArabic(ar),Spanish(es),orTurkish(tr). We rectmappingintheordering,hurtingperformance.
averagePearson’sr×100forthesedatasets.
Question Retrieval For our question retrieval
Bitext Mining For bitext mining, we use the evaluation, we report the accuracy (R@1) on the
TatoebadatasetintroducedinArtetxeandSchwenk testsetsofNaturalQuestions(NQ)(Kwiatkowski
(2019b) and the 2018 Building and Using Par- etal.,2019)andtheMultilingualKnowledgeQues-
allel Corpora (BUCC) shared bitext mining tionsandAnswers(MKQA)(Longpreetal.,2021).

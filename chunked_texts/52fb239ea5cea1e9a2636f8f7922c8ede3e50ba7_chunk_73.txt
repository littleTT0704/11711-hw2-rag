 length of programs 47.85
Table 2: Key statistics of L¯ila.
Fine-tuning. We fine-tune a series of GPT-Neo-2.7B causal language mod-
els(Blacketal.,2021))onL¯ila. WechooseGPT-Neobecauseitwaspre-trained
on both natural language and code (Gao et al., 2020), as opposed to solely on
natural language. To assess the capabilities of GPT-Neo on various aspects of
the dataset, we fine-tune single-task models on each of the 23 tasks in L¯ila. We
also evaluate the benefit of transfer learning by fine-tuning a single multi-task
GPT-Neo baseline on all the tasks simultaneously. We call our multitask model
Bha¯skara.
Prompting. Wealsousefew-shotpromptingtoevaluateGPT-3andCodex4(Brown
et al., 2020; Chen et al., 2021). For the IID setting, we prompt the model with a
random input-output examples from the same dataset as the input. In the OOD
setting, we take examples from other datasets (Table 12-15) within the same
task. We repeat this evaluation with increasing numbers of examples (up to the
token size of models) to study the effect on performance5.
Evaluation. We evaluate our models under two regimes—directly outputting
theansweri.e.,programinductionandoutputtingaPythonprogramthatisthen
executed to obtain the final answer i.e., program synthesis. In the case of our
fine-tunedmodels,wetrainthemtooutputboththefinalanswerandthePython
program conditioned on the input question. To evaluate our models under direct
question answering, we use F1-score6 to compare the model output and the
gold answer. To evaluate program synthesis, we execute the model’s output
within a Python interpreter and compare the program output with the output
of the gold program, again using F1. We evaluate based on the program output,
4text-davinci-002, code-davinci-002
5Henceforthwerefertothemaxexamplemodelunlessotherwisespecified.
6Thisisasoftversionofexactmatchaccuracyassigningpartialcreditwhencommonwords

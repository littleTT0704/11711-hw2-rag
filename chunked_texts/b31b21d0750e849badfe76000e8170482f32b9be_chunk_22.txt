avoidcascadingerrors. Further,
webelievethattheprinciplesandthemethodspresentedinthispaperareapplicabletoadditional
code-relatedtasks,andotherdocumentation-likeresourcessuchastutorialsandblogposts. Tothese
ends,wemakeallourcode,data,andmodelspubliclyavailable.
9
PublishedasaconferencepaperatICLR2023
9 ACKNOWLEDGEMENT
We thanks the anonymous reviewers for their useful comments and suggestions. This work is
supported by a gift from Amazon AI and a contract from the Air Force Research Laboratory
underagreementnumberFA8750-19-2-0200. TheU.S.Governmentisauthorizedtoreproduceand
distributereprintsforGovernmentalpurposesnotwithstandinganycopyrightnotationthereon. The
viewsandconclusionscontainedhereinarethoseoftheauthorsandshouldnotbeinterpretedas
necessarilyrepresentingtheofficialpoliciesorendorsements,eitherexpressedorimplied,oftheAir
ForceResearchLaboratoryortheU.S.Government.
REFERENCES
MiltiadisAllamanis,DanielTarlow,AndrewD.Gordon,andYiWei. Bimodalmodellingofsourcecodeand
naturallanguage. InFrancisR.BachandDavidM.Blei,editors,Proceedingsofthe32ndInternational
ConferenceonMachineLearning,ICML2015,Lille,France,6-11July2015,volume37ofJMLRWorkshop
andConferenceProceedings,pages2123–2132.JMLR.org,2015. URLhttp://proceedings.mlr.
press/v37/allamanis15.html.
UriAlon,RoySadaka,OmerLevy,andEranYahav. Structurallanguagemodelsofcode. InInternational
conferenceonmachinelearning,pages245–256.PMLR,2020.
JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,DavidDohan,EllenJiang,
CarrieCai,MichaelTerry,QuocLe,etal. Programsynthesiswithlargelanguagemodels. ArXivpre
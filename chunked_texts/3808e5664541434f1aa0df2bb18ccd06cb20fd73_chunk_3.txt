1Weusehatespeechandtoxiclanguageinterchangeably
inthiswork,thoughtheirdefinitionsdonotperfectlyalign. et al., 2020). This raises a natural question: are
3143
Proceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics,pages3143–3155
April19-23,2021.©2021AssociationforComputationalLinguistics
currentdebiasingapproacheseffectiveformitigat- This task differs in several ways from the natu-
ingbiasesspecifictotoxiclanguagedetection? rallanguageunderstanding(NLU)tasksthatdebi-
In this work, we address the above question by asing methods have been successful on, such as
investigating two classes of debiasing approaches textual entailment (e.g., SNLI, MNLI; Bowman
to mitigate lexical and dialectal biases—one that etal.,2015;Williamsetal.,2018)orreadingcom-
employsadditionaltrainingobjectivesforbiasre- prehension(e.g.,SQuAD;Rajpurkaretal.,2016).
moval, and another that filters training instances First, compared to these NLU tasks where there
likely exhibiting spurious biases (§3). Through is one correct label, the toxicity of language is
comprehensive experiments, we show that both inherently more nuanced, subjective, and contex-
approachesfacemajorchallengesinmitigatingbi- tual,whichcausestoxiclanguagedatasetstohave
ases from a model trained on a biased dataset (in lower agreement in general (Ross et al., 2017).
our case, the dataset from Founta et al., 2018) Second, the dataset biases in NLU are predom-
for toxic language detection. While data filter- inantly artifacts introduced during data creation
ingresultsinreducedbiasassociationsinthedata, (e.g., negations, exaggerations; Schwartz et al.,
modelstrainedonfiltereddatasetsstillpickupon 2017; Gururangan et al., 2018), whereas those in
lexical (§4) and dialectal biases (§5). We find toxic language detection are grounded in the so-
that dialectal biases are particularly challenging cial dynamics
imilaritycomputation,essentiallycom- paces, etc.) except for arithmetic operators, from
putingf(yˆ,y∗,x)ratherthanf(yˆ,y∗).
each of the encoded reference and encoded can-
2. Given the precision and recall, instead of didate. This results in encoded reference tokens
computing the F 1 score, we also compute F 3 y∗ = ⟨y 1∗,...,y m∗ ⟩,encodedcandidatetokensyˆ =
toweighrecallhigherthanprecision,follow- ⟨yˆ,...,yˆ ⟩, and their corresponding masks m∗
1 n
ingMETEOR(BanerjeeandLavie,2005). andmˆ. Wedenotey[m]astheremainingencoded
3. As our underlying BERT-like model, we use tokens in y after selecting only alphanumeric to-
programming language-specific models that kenvectorsaccordingtothemaskm.
we pretrain and release, rather than models
Similarity Computation We compute the co-
thatwereintendedfornaturallanguageonly.
sinesimilaritybetweentheencodedreferenceand
We use a BERT-like pretrained model B to en-
candidatetokens,followingZhangetal.(2020):
code the reference and candidate. In our exper-
iments, B is a CodeBERT model that we fur-
y∗⊤·yˆ
ther pretrained using the masked language mod- sim(y∗,yˆ ) = i j (3)
i j ∥y∗∥·∥yˆ ∥
eling objective (Devlin et al., 2019) on language- i j
specific corpora, but B can be any transformer- Although this compares the individual tokens y∗
i
based model which we have access to its internal and yˆ, their vector representations y∗ and yˆ
j i j
hiddenstates.
contain information about their context, and thus
abouttheirsemanticroleinthecode.
TokenRepresentation Weconcatenatethecon-
textxwitheachofthereference
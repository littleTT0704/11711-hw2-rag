 204K 614K Open/MC in(Lauetal.,2018).
VQAv2 General 204K 1.1M Open/MC
COCO-QA General 123K 118K Open/MC
D Numberofquestionsindifferent
CLEVR General 100K 999K Open
categoriesfortraining,validation,and
VQA-Med Medical 4,200 15,292 Open/MC
VQA-RAD Medical 315 3,515 Open/MC testset
Ours Medical 4,998 32,795 Open
Forourdatasplit,thenumberofquestionsindif-
ferentcategoriesineachsetisshowninTable4.
The comparison of existing VQA datasets is
showninTable6. Thefirstfivedatasetsareinthe
generaldomainwhilethelastthreeareinthemed-
icaldomain. Notsurprisingly,thesizeofgeneral-
domaindatasets(includingthenumberofimages
andquestion-answerpairs)ismuchlargerthanthat
ofmedicaldatasetssincegeneral-domainimages
are much more available publicly and there are
manyqualifiedhumanannotatorstogenerateQA
pairsongeneralimages. Ourdatasetislargerthan
the two medical datasets: VQA-Med and VQA-
RAD,andmajorityofquestionsinourdatasetare
open-endedwhilemajorityofquestionsinVQA-
MedandVQA-RADareinmultiple-choicesstyle.
C.2 AutomaticConstructionof
Question-AnswerPairs
Existingdatasetshaveusedautomatedmethodsfor
constructingquestion-answerpairs. InDAQUAR,
questions are generated with templates, such as
“Howmany{object}arein{image id}?”. These
templatesareinstantiatedwithground-truthfacts
from the database. In COCO-QA, the authors
develop a question generation algorithm based
on the Stanford syntactic parser (Klein and Man-
ning, 2003), and they form four types of ques-
tions—“object”,“number”,“color”,and“location”
usinghand-craftedrules. InCLEVR,thelocations
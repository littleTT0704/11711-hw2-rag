estoselect(acqui- betterestimationaccordingtopreliminaryexperiments.
sition function), but also at what granularity to select and 4The specific criterion is that the argmax prediction
annotate.IncontrastwithmostALworkthatfocusesonthe matchesthegoldoneanditsmarginisgreaterthan0.5.Since
firstaspect(andclassificationtasks),wemainlyinvestigate neuralmodelsareusuallyover-confident,itishardtodecidea
thesecondoneandexplorebetterpartialselectionstrategies. confidencethreshold.Nevertheless,wefind0.5areasonable
Exploringmoreadvancedacquisitionfunctionsismostlyor- valuefortheratiodecisionhere.
thogonaltoourmainfocusandislefttofuturework. 5Were-usethedevelopmentsetforthetaskmodeltraining.
estimatetheoverallerrorrateasoneminustheaver- tonetal.,2015). Thischoiceisbecausewewantto
agecorrectnessprobabilityoverallthecandidates avoidthepotentialnegativeinfluencesofambigu-
inthequerysetQ(allsub-structuresintheselected ous predictions (mostly in completely unlabeled
sentences),andsettheselectionratiorasthiserror instances). One way to mitigate this is to set an
rate: uncertainty threshold and only utilize the highly-
1 (cid:88) confident sub-structures. However, it is unclear
r = 1− p(correct = 1|s)
howtosetapropervalue,similartothescenarios
n
s∈Q inqueryselection. Therefore,wetakethemodel’s
Inthisway, theselectionratiocanbesetadap- fulloutputpredictionsasthetrainingtargetswith-
tivelyaccordingtothecurrentmodel’scapability. outfurtherprocessing.
If the model is weak and makes many mistakes, Specifically,ourself-trainingobjectivefunction
wewillhavealargerratiowhichcanleadtomore is the cross-entropy between the output distribu-
denseannotationsandrichertrainingsignals. As tions predicted by the previous model m′ before
the model is trained with more data and makes training
ointtrainingandMetaXLstandsforMetaRepresentationTransformation. Weboldthe
numberswithabetteraverageperformanceineachsetting.
Method tel fa cantgainstothejointtrainingbaseline(JT)over
using target language data only (target only), as
(1) targetonly 86.87 82.58
intheNERtask. Inaddition,MetaXLstilloutper-
JT 88.68 85.51 formsjointtrainingbyaround0.9and1.6F1score
(2)
MetaXL 89.52 87.14 onTeluguandPersian. Theseresultssupportour
hypothesis that MetaXL can transfer representa-
Table3:F1forsentimentanalysisontwosettingsusing tionsfromotherlanguagesmoreeffectively. That,
(1) only the target language data; (2) target language inturn,contributestotheperformancegainonthe
dataalongwith1kexamplesofEnglish.
targettask.
4.2 SourceLanguageDataSize
pre-trained on. We leave exploring the best tok-
enizationstrategyforleveragingpre-trainedmodel ToevaluatehowMetaXLperformswithdifferent
onunseenlanguageasfuturework. sizesofsourcelanguagedata,weperformexperi-
mentsonvaryingthesizeofsourcedata. ForNER,
4 ResultsandAnalysis weexperimentwith5k,10k,and20ksourceexam-
ples. For SA, we test on 1k, 3k and 5k 4 source
4.1 MainResults
examples.
NER We present results of NER in Table 2,
AsobservedfromTable4,MetaXLdeliverscon-
whereweuse5kexamplesfromEnglishorarelated
sistent gains as the size of source data increases
language as source data. When we only use the
over the joint training model (except on fa when
annotateddataofthetargetlanguagetofine-tune
using5kauxiliarydata).5 However,themarginal
XLM-R(target),weobservethattheperformance
gain decreases as the source data size increases
variessignificantlyacrosslanguages,rangingfrom
on NER. We also note that MetaXL continues to
37.
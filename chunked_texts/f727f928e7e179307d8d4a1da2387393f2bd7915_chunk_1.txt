Methods for Measuring, Updating, and Visualizing Factual Beliefs in
Language Models
PeterHase1,2 MonaDiab1 AsliCelikyilmaz1 XianLi1
ZornitsaKozareva1 VeselinStoyanov1 MohitBansal2 SrinivasanIyer1
1MetaAI 2UNCChapelHill
{peter, mbansal}@cs.unc.edu
{mdiab, aslic, xianl, zori, ves, sviyer}@fb.com
Abstract Whilepretrainedmodelsclearlystorefactualbe-
liefs, it is not well understood how to efficiently
Languagemodelscanmemorizeaconsiderable
editthestoredbeliefs. Modeleditingisanexciting
amountoffactualinformationduringpretrain-
recentdirectionofresearchwithseveralpractical
ingthatcanbeelicitedthroughpromptingor
usescases(Sinitsinetal.,2020;Zhuetal.,2020;
finetuning models on tasks like question an-
swering. Inthispaper,wediscussapproaches De Cao et al., 2021; Mitchell et al., 2021). For
to measuring model factual beliefs, updating LMs,theseusesincludeupdatingfactuallyinaccu-
incorrectfactualbeliefsinmodels,andvisualiz- rateoutputsandpreventingotherunwantedmodel
inggraphicalrelationshipsbetweenfactualbe-
outputs(e.g. toxicgeneratedtext)withoutexpen-
liefs. Ourmaincontributionsinclude: (1)new
sivedatacurationandretrainingefforts. Theseare
metrics for evaluating belief-updating meth-
importantapplicationsgiventhatLMs(1)struggle
odsfocusingonthelogicalconsistencyofbe-
withfuturedatawhentrainedondatafromthepast
liefs, (2) a training objective for Sequential,
Local,andGeneralizingupdates(SLAG)that (Lazaridouetal.,2021;Dhingraetal.,2021), (2)
improvestheperformanceofexistinghypernet- oftengeneratemorallyundesirabletext(Gehman
work approaches, and (3)
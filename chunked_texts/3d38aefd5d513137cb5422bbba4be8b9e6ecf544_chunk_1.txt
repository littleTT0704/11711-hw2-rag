DETECTIONANDEVALUATIONOFHUMANANDMACHINEGENERATEDSPEECHIN
SPOOFINGATTACKSONAUTOMATICSPEAKERVERIFICATIONSYSTEMS
YangGao∗,JiachenLian∗,BhikshaRaj+,RitaSingh+
∗ElectricalandComputerEngineering,+LanguageTechnologiesInstitute
CarnegieMellonUniversity
Pittsburgh,PA,USA-15213
ABSTRACT andwavenetmodels[2,3], thequalityofsyntheticspeechis
gettingmuchclosertonaturalspeech[3]. Attackscarriedout
Automaticspeakerverification(ASV)systemsutilizethebio-
using synthetic speech generated by these methods pose se-
metric information in human speech to verify the speaker’s
rious threats to ASV systems. As a first step, in this paper,
identity. The techniques used for performing speaker verifi-
we compare the threats from impersonation attacks with the
cation are often vulnerable to malicious attacks that attempt
synthetic speech attacks and establish that deep synthesized
to induce the ASV system to return wrong results, allowing
fakesareinfactthemostdangerousattacksforASVsystems.
an impostor to bypass the system and gain access. Attack-
Itisthereforeimportanttobeabletodistinguishbetween
ers use a multitude of spoofing techniques for this, such as
fake/synthesized and human-generated speech. This is the
voice conversion, audio replay, speech synthesis, etc. In re-
broadergoalofourpaper. However,wedonotfocusonmere
cent years, easily available tools to generate deepfaked au-
featureselectionsas[4–6],whichwouldbeinfluencedbythe
dio have increased the potential threat to ASV systems. In
datasetchoices,modelsandtrainingprocedures. Instead,we
thispaper,wecomparethepotentialofhumanimpersonation
startwiththehypothesisthatmachine-generatedspeechistoo
(voicedisguise)basedattackswithattacksbasedonmachine-
consistentinmanyrespects,andmachinesareunabletoem- generatedspeech,onblack-boxandwhite-
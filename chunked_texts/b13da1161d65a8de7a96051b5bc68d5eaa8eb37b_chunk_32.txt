peanconferenceoncomputervision(ECCV),pages289–
305,2018. 2,3,4,12
11
Inthissupplementary,weprovideadditionaldetailsand and pseudo-labels as described in Section 4.3. While, Ta-
analysisforourproposedmethod,PAC-UDA.Algorithm1, ble6highlightstheimportanceofcombiningallmodalities
providesastep-by-stepprocedureforunsuperviseddomain with pseudo-labels for the best mean IoU, there are a few
adaptationviaPAC-UDA. otherimportantobservationswithrespecttoclasswiseIoUs.
For instance, using “PL” for objectness constraints sig-
A.HyperparametersforMainExperiments
nificantly underperforms other settings (by upto 49 IoU)
in rare source-classes, like motorcycle and bike (Figure
4). This gap is surprisingly large (by upto 38.5 IoU) even
Table5.HyperparametersusedinTable1
when compared to “Depth-RGB”. We attribute this large
method k b δ τ performance gap to the class-imbalance problem [64] that
S peak p
is known to adversely affect self-training in the absence
CAG+PAC 50 200 0.0025 0.90
of class-balanced losses. However, incorporating our ob-
SAC+PAC 50 200 0.0025 0.90
jectness constraint alleviates the rare-class IoUs signifi-
DACS+PAC 25 200 0.001 0.90
cantly without losing performance in frequent classes (ex-
cept, sky). These results provide strong evidence for the
ToreporttheresultsinTable1,Table2andTable3,we
normalisationofclass-relatedstatisticaleffectsinthepres-
choosethebesthyperparametersfollowingstandardcross-
enceofmultimodalobjectness-constraints.
validation on a random subset of Cityscapes train-split in-
troduced in [2]. For base methods, we use the default hy- Another interesting insight arises from comparing
perparametersfromrespectivepapers. InTable5,wesum- “Depth-PL” and “RGB-PL” settings that
decoder hidden state. We compute these outputs of GAT
into an one-hot embedding of size H × W according to
using:
its spatial location. We use the final state of this encoder
H te ∈ RH×W×denc, where d
enc
is the hidden size, to ini- h˜
i
= |N1
|
(cid:88) f e([v i,v j])+h
i
(4)
tialize the state of the decoders. We also use the temporal i j∈Ni
averageofthesemanticmaps,S = 1 (cid:80)h S,duringeach whereN aretheneighborsofnodev inGwitheachnode
h t=1 t i i
decodingstep. ThecontextisrepresentedasH=[He,S]. represented as v i = [h i,S i], where S i collects the cell i’s
h
feature in S. f is some edge function (implemented as
3.2.CoarseLocationDecoder e
an MLP in our experiments) that computes the attention
AftergettingthecontextH,ourgoalistoforecastfuture weights.
locations. We initially focus on predicting locations at the
levelofgridcells,Y ∈G.InSection3.3,wediscusshowto The graph-structured update function for the RNN en-
t
predictacontinuousoffsetinR2,whichspecifiesa“delta” suresthattheprobabilitymass“diffusesout”tonearbygrid
fromthecenterofeachgridcell,togetafine-grainedloca- cellsinacontrolledmanner,reflectingthepriorknowledge
tionprediction. thatpeopledonotsuddenlyjumpbetweendistantlocations.
Let the coarse distribution over grid locations at time t This inductive bias is also encoded in the convolutional
structure,butaddingthegraphattentionnetworkgivesim- Note that during training, when updating the RNN, we
provedresults,becausetheweightsareinput-dependentand feed in the predicted soft distribution over locations, C.
t
notfixed. SeeEq.(2). Analternativewouldbetofeedinthetrueval
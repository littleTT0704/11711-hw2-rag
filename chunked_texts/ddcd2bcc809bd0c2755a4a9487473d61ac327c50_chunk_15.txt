’. Thenotablediscrepancybetweentheperfor-
manceofthetwodatasetssuggeststhatthemodel’s
MC-probing promptstheLLMwiththecontext,
abilities are not based on generalization. Instead
question,andanswerchoices,andasksittogener-
oftrueunderstandingoftheproblemathand,such
atetheanswerintheformof“a,b,c”. Thismethod
asaccuratelydeterminingone’sexactthoughts,the
isapplicableforLLMssuchasGPT-3.5andGPT-4
modelmightberecognizingpatternsfromtheSally-
thatdon’tproduceprobabilities(Huetal.,2022).
AnnestoryinotherToM-kexamplesandgenerat-
CoT-probing asks the model to first “reason” ingresponsesbasedonthosepatterns. Conversely,
about the question step-by-step and then give a the performance on ToMi’ is worse because it is
finalanswer,whichgenerallycontributestobetter morerobusttospuriouscorrelations.
performance(Weietal.,2023).7
Table3showsthattheprobingtechniquesinflu- 5.2 IsN-ToMRobusttoAdversarialChanges?
encetheLLMperformanceonbothdatasets. CoT
TotesttherobustnessoftheLLMs’N-ToM,wetest
generallydemonstratesenhancedperformance,as
theperformanceofGPTmodelsoneachofthecate-
supportedbypriorresearch(Camburuetal.,2018;
goriesinAdv-CSFB(§3.2),usingMC-probing. To
Shwartzetal.,2020;Weietal.,2023). Nonetheless,
ensurecorrectformattingandpreventunintended
therearecaseswherethistrenddoesnothold,since
outputs(e.g.,explanationofwhytheansweriscor-
thereasoningmayoccasionallyresultinerroneous
rect),weprependtothepromptoneout-of-domain
conclusions(Jungetal.,2022).
example from ToMi, which
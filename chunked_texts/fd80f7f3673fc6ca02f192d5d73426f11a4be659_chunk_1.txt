The Devil is in the Errors: Leveraging Large Language Models for
Fine-grained Machine Translation Evaluation
PatrickFernandes∗,2,3,4 DanielDeutsch1 MaraFinkelstein1 ParkerRiley1
AndréF.T.Martins3,4,5 GrahamNeubig2,6
AnkushGarg1 JonathanH.Clark1 MarkusFreitag1 OrhanFirat1
1Google 2CarnegieMellonUniversity 3InstitutoSuperiorTécnico
4InstitutodeTelecomunicações 5Unbabel 6InspiredCognition
pfernand@cs.cmu.edu
Abstract
Source: “Avaliar tradução Candidate: “Evaluating
automática é difícil.” automatic translation are easy.”
Automaticevaluationofmachinetranslation Score Prediction
(MT)isacriticaltooldrivingtherapiditer-
Score the following translation from 0 to 100:
ative development of MT systems. While
Portuguese: {source}; English:{candidate}
considerableprogresshasbeenmadeonesti-
matingasinglescalarqualityscore,current
Score: 25
metricslacktheinformativenessofmorede-
tailed schemes that annotate individual er-
AᴜᴛᴏMQM
rors,suchasMultidimensionalQualityMet- Identify the errors in the translation
rics(MQM).Inthispaper,wehelpfillthis Portuguese: {source}; English:{candidate}
gap by proposing AUTOMQM, a prompt-
ingtechniquewhichleveragesthereasoning
Errors: ‘easy’ - major/accuracy; ‘are’ - minor/fluency
andin-contextlearningcapabilitiesoflarge
language models (LLMs) and asks them
MQM Score: -5x1(major) - 1x1(minor) = -6
to identify and categorize errors in transla-
tions. WestartbyevaluatingrecentLLMs,
suchasPaLMandPaLM-2,throughsimple
Figure 1: Illustration of how AUTOMQM uses
score prediction prompting, and we study LLMstoassessthequality
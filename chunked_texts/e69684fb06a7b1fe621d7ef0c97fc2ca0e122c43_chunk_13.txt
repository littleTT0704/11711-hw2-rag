findthatPrompt2Model Instead,weevaluatemodelsfinetunedon:
produces models that are considerably more ac- 1. 3kexamplesfromtheclosestretrieveddataset6
curate than gpt-3.5-turbo. This is remarkable 2. 3kexamplesgeneratedbyPrompt2Model
becausetheretrievedmodelforSQuADandTem- 3. Theunionoftheabove,whichiswhatthefull
poralisFlan-T5,which,at250Mparameters,isup Prompt2Modelpipelineuses
to700timessmallerthangpt-3.5-turbo(which 4. 3k examples from SQuAD (analogous to the
isbelievedtocontain175Bparameters). usercustom-annotatingdataforatask).
WeobservethatPrompt2Model’sperformance Table2showstheresultsacrossthesefourset-
onMCoNaLa’sJapanese-to-Pythontaskissignif- tings. While using retrieved or generated data
icantlyworsethangpt-3.5-turbo. Oneexplana- causesareductioninperformanceduetodomain
tion for this is the relatively low diversity in the shift,thecombinationofthetwomethodsachieves
generateddatasetofJapanesequeries;45of5000 similarperformancetousingthetruedataset. For
examples are different ways of saying “find the this machine reading comprehension task where
maximumvalueinalistofnumbers“. Wedonotob- the user would need to custom-annotate data for
servethislevelofredundancyinourotherdatasets,
theirtask,Prompt2Modelallowsforsimilarper-
suggesting that gpt-3.5-turbo may struggle to formanceatlessthan1%ofthecost.
generate diverse text for non-English languages.
5.3 Ourgeneratedevaluationdatacan
Another reason is the lack of an appropriate stu-
identifyrealmodelingimprovements
dentmodel—themodelsfoundbythemodelre-
trieverweretrainedoneitheronmultiplelanguage
orcode,butnotboth. Theresultingpretrained
 Jack
promptedmodel(Benderetal.,2021);indeed,the Clark, ChristopherBerner, SamMcCandlish, Alec
useofmodelsandsupplementarydatasetsretrieved Radford, Ilya Sutskever, and Dario Amodei. 2020.
from Hugging Face may lessen the likelihood of Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems,
a downstream model replicating harms from the
volume 33, pages 1877–1901. Curran Associates,
promptedmodel’soutputs,thoughmoreinvestiga-
Inc.
tion is needed. Like all ML models, the models
that Prompt2Model returns can make mistakes, Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
andweaimtobetransparentinourdocumentation dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
aboutpotentiallimitationsofthesystem.
berg,etal.2023. Sparksofartificialgeneralintelli-
We hope that Prompt2Model will be broadly gence: Earlyexperimentswithgpt-4. arXivpreprint
useful. Our work is motivated by a desire to in- arXiv:2303.12712.
crease the accessibility of NLP models to people
NitayCalderon,SubhabrataMukherjee,RoiReichart,
whoarenotintheNLPcommunitybutwouldben-
andAmirKantor.2023. Asystematicstudyofknowl-
efitfromthecommunity’sinnovations;particularly,
edgedistillationfornaturallanguagegenerationwith
topeoplewhowoulduseNLPmodelsdownstream pseudo-target training. In Proceedings of the 61st
butmaynothavethedomain-specificknowledge AnnualMeetingoftheAssociationforComputational
todesigntheirownsystem. Prompt2Modelmay Linguistics(Volume1: LongPapers),pages14632–
14659,Toronto,Canada.AssociationforComputa-
alsoproveusefulforearlyNLPresearchersbypro-
tionalLingu
2.
CODEX-002 Tree(Figure10) 56.5 proaches.
WhenapplyingexistingIstateV-oafr-itaheti-oarntsiynstpermosm, pts Morerecently,severalneuralsystemshavebeen
suchasRecurrentEntityNetworks(Hena↵etal.,
developedtoanswerquestionsabouttheworldstate
Table 14: Performance of CODEX-001 and CODEX-
2016) and Query-reduction Networks (Seo et al.,
afteraprocess,inspiredinpartbythebAbIdataset.
002 on the the different formats present in Figure 10 We run each experiment with 3 different random
2017b),wefindthattheydonotperformwellon
Building on the general Memory Network archi-
and9forPROSCRIPTedgepre Pdi rc oti Po an r. aW ae ndfin thd eth rea st uth lte saresoeneldyss,liwghhtelryebtehteterrathnadnom seeds decides the order
tecture (Weston et al., 2014) and gated recurrent
literalformatthatcombinesstructurewithliterallyFig-
themajoritybaselines. Asoafsetxepamfoprlwesaridn,twheepprroom- pt.mWodeefilsnsdumchinaismGaRlUvar(Ci-hoetal.,2014),Recurrent
ureoutputperformsthebestfo pr oC sOeDtwEXo-0 n0 e2 w.
neural modaenlscethbaettwuseeenarltuenrsnautsiivnegdifEfenrteintytfiNxeetdwoprrkosm(pEtnstbNee-t)(Hena↵etal.,2016)isa
mechanismsforstatepredictionandpropagation,
tween3runs. Further,asshoswtatne-inoft-htheeT-aabrtlemse1t8h,o1d9,forbAbI.EntNetusesady-
inparticularusingLSTMinputencodingandspan
namicmemoryofhiddenstates(memoryblocks)to
prediction.
The